<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20260126.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Splat-Portrait: Generalizing Talking Heads with Gaussian Splatting", "author": "Tong Shi and Melonie de Almeida and Daniela Ivanova and Nicolas Pugeault and Paul Henderson", "abstract": "Talking Head Generation aims at synthesizing natural-looking talking videos from speech and a single portrait image. Previous 3D talking head generation methods have relied on domain-specific heuristics such as warping-based facial motion representation priors to animate talking motions, yet still produce inaccurate 3D avatar reconstructions, thus undermining the realism of generated animations. We introduce Splat-Portrait, a Gaussian-splatting-based method that addresses the challenges of 3D head reconstruction and lip motion synthesis. Our approach automatically learns to disentangle a single portrait image into a static 3D reconstruction represented as static Gaussian Splatting, and a predicted whole-image 2D background. It then generates natural lip motion conditioned on input audio, without any motion driven priors. Training is driven purely by 2D reconstruction and score-distillation losses, without 3D supervision nor landmarks. Experimental results demonstrate that Splat-Portrait exhibits superior performance on talking head generation and novel view synthesis, achieving better visual quality compared to previous works. Our project code and supplementary documents are public available at https://github.com/stonewalking/Splat-portrait.", "link": "http://arxiv.org/abs/2601.18633v1", "date": "2026-01-26", "relevancy": 3.3441, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6703}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6703}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6659}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Splat-Portrait%3A%20Generalizing%20Talking%20Heads%20with%20Gaussian%20Splatting&body=Title%3A%20Splat-Portrait%3A%20Generalizing%20Talking%20Heads%20with%20Gaussian%20Splatting%0AAuthor%3A%20Tong%20Shi%20and%20Melonie%20de%20Almeida%20and%20Daniela%20Ivanova%20and%20Nicolas%20Pugeault%20and%20Paul%20Henderson%0AAbstract%3A%20Talking%20Head%20Generation%20aims%20at%20synthesizing%20natural-looking%20talking%20videos%20from%20speech%20and%20a%20single%20portrait%20image.%20Previous%203D%20talking%20head%20generation%20methods%20have%20relied%20on%20domain-specific%20heuristics%20such%20as%20warping-based%20facial%20motion%20representation%20priors%20to%20animate%20talking%20motions%2C%20yet%20still%20produce%20inaccurate%203D%20avatar%20reconstructions%2C%20thus%20undermining%20the%20realism%20of%20generated%20animations.%20We%20introduce%20Splat-Portrait%2C%20a%20Gaussian-splatting-based%20method%20that%20addresses%20the%20challenges%20of%203D%20head%20reconstruction%20and%20lip%20motion%20synthesis.%20Our%20approach%20automatically%20learns%20to%20disentangle%20a%20single%20portrait%20image%20into%20a%20static%203D%20reconstruction%20represented%20as%20static%20Gaussian%20Splatting%2C%20and%20a%20predicted%20whole-image%202D%20background.%20It%20then%20generates%20natural%20lip%20motion%20conditioned%20on%20input%20audio%2C%20without%20any%20motion%20driven%20priors.%20Training%20is%20driven%20purely%20by%202D%20reconstruction%20and%20score-distillation%20losses%2C%20without%203D%20supervision%20nor%20landmarks.%20Experimental%20results%20demonstrate%20that%20Splat-Portrait%20exhibits%20superior%20performance%20on%20talking%20head%20generation%20and%20novel%20view%20synthesis%2C%20achieving%20better%20visual%20quality%20compared%20to%20previous%20works.%20Our%20project%20code%20and%20supplementary%20documents%20are%20public%20available%20at%20https%3A//github.com/stonewalking/Splat-portrait.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18633v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSplat-Portrait%253A%2520Generalizing%2520Talking%2520Heads%2520with%2520Gaussian%2520Splatting%26entry.906535625%3DTong%2520Shi%2520and%2520Melonie%2520de%2520Almeida%2520and%2520Daniela%2520Ivanova%2520and%2520Nicolas%2520Pugeault%2520and%2520Paul%2520Henderson%26entry.1292438233%3DTalking%2520Head%2520Generation%2520aims%2520at%2520synthesizing%2520natural-looking%2520talking%2520videos%2520from%2520speech%2520and%2520a%2520single%2520portrait%2520image.%2520Previous%25203D%2520talking%2520head%2520generation%2520methods%2520have%2520relied%2520on%2520domain-specific%2520heuristics%2520such%2520as%2520warping-based%2520facial%2520motion%2520representation%2520priors%2520to%2520animate%2520talking%2520motions%252C%2520yet%2520still%2520produce%2520inaccurate%25203D%2520avatar%2520reconstructions%252C%2520thus%2520undermining%2520the%2520realism%2520of%2520generated%2520animations.%2520We%2520introduce%2520Splat-Portrait%252C%2520a%2520Gaussian-splatting-based%2520method%2520that%2520addresses%2520the%2520challenges%2520of%25203D%2520head%2520reconstruction%2520and%2520lip%2520motion%2520synthesis.%2520Our%2520approach%2520automatically%2520learns%2520to%2520disentangle%2520a%2520single%2520portrait%2520image%2520into%2520a%2520static%25203D%2520reconstruction%2520represented%2520as%2520static%2520Gaussian%2520Splatting%252C%2520and%2520a%2520predicted%2520whole-image%25202D%2520background.%2520It%2520then%2520generates%2520natural%2520lip%2520motion%2520conditioned%2520on%2520input%2520audio%252C%2520without%2520any%2520motion%2520driven%2520priors.%2520Training%2520is%2520driven%2520purely%2520by%25202D%2520reconstruction%2520and%2520score-distillation%2520losses%252C%2520without%25203D%2520supervision%2520nor%2520landmarks.%2520Experimental%2520results%2520demonstrate%2520that%2520Splat-Portrait%2520exhibits%2520superior%2520performance%2520on%2520talking%2520head%2520generation%2520and%2520novel%2520view%2520synthesis%252C%2520achieving%2520better%2520visual%2520quality%2520compared%2520to%2520previous%2520works.%2520Our%2520project%2520code%2520and%2520supplementary%2520documents%2520are%2520public%2520available%2520at%2520https%253A//github.com/stonewalking/Splat-portrait.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18633v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Splat-Portrait%3A%20Generalizing%20Talking%20Heads%20with%20Gaussian%20Splatting&entry.906535625=Tong%20Shi%20and%20Melonie%20de%20Almeida%20and%20Daniela%20Ivanova%20and%20Nicolas%20Pugeault%20and%20Paul%20Henderson&entry.1292438233=Talking%20Head%20Generation%20aims%20at%20synthesizing%20natural-looking%20talking%20videos%20from%20speech%20and%20a%20single%20portrait%20image.%20Previous%203D%20talking%20head%20generation%20methods%20have%20relied%20on%20domain-specific%20heuristics%20such%20as%20warping-based%20facial%20motion%20representation%20priors%20to%20animate%20talking%20motions%2C%20yet%20still%20produce%20inaccurate%203D%20avatar%20reconstructions%2C%20thus%20undermining%20the%20realism%20of%20generated%20animations.%20We%20introduce%20Splat-Portrait%2C%20a%20Gaussian-splatting-based%20method%20that%20addresses%20the%20challenges%20of%203D%20head%20reconstruction%20and%20lip%20motion%20synthesis.%20Our%20approach%20automatically%20learns%20to%20disentangle%20a%20single%20portrait%20image%20into%20a%20static%203D%20reconstruction%20represented%20as%20static%20Gaussian%20Splatting%2C%20and%20a%20predicted%20whole-image%202D%20background.%20It%20then%20generates%20natural%20lip%20motion%20conditioned%20on%20input%20audio%2C%20without%20any%20motion%20driven%20priors.%20Training%20is%20driven%20purely%20by%202D%20reconstruction%20and%20score-distillation%20losses%2C%20without%203D%20supervision%20nor%20landmarks.%20Experimental%20results%20demonstrate%20that%20Splat-Portrait%20exhibits%20superior%20performance%20on%20talking%20head%20generation%20and%20novel%20view%20synthesis%2C%20achieving%20better%20visual%20quality%20compared%20to%20previous%20works.%20Our%20project%20code%20and%20supplementary%20documents%20are%20public%20available%20at%20https%3A//github.com/stonewalking/Splat-portrait.&entry.1838667208=http%3A//arxiv.org/abs/2601.18633v1&entry.124074799=Read"},
{"title": "LoD-Structured 3D Gaussian Splatting for Streaming Video Reconstruction", "author": "Xinhui Liu and Can Wang and Lei Liu and Zhenghao Chen and Wei Jiang and Wei Wang and Dong Xu", "abstract": "Free-Viewpoint Video (FVV) reconstruction enables photorealistic and interactive 3D scene visualization; however, real-time streaming is often bottlenecked by sparse-view inputs, prohibitive training costs, and bandwidth constraints. While recent 3D Gaussian Splatting (3DGS) has advanced FVV due to its superior rendering speed, Streaming Free-Viewpoint Video (SFVV) introduces additional demands for rapid optimization, high-fidelity reconstruction under sparse constraints, and minimal storage footprints. To bridge this gap, we propose StreamLoD-GS, an LoD-based Gaussian Splatting framework designed specifically for SFVV. Our approach integrates three core innovations: 1) an Anchor- and Octree-based LoD-structured 3DGS with a hierarchical Gaussian dropout technique to ensure efficient and stable optimization while maintaining high-quality rendering; 2) a GMM-based motion partitioning mechanism that separates dynamic and static content, refining dynamic regions while preserving background stability; and 3) a quantized residual refinement framework that significantly reduces storage requirements without compromising visual fidelity. Extensive experiments demonstrate that StreamLoD-GS achieves competitive or state-of-the-art performance in terms of quality, efficiency, and storage.", "link": "http://arxiv.org/abs/2601.18475v1", "date": "2026-01-26", "relevancy": 3.2967, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6986}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6706}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoD-Structured%203D%20Gaussian%20Splatting%20for%20Streaming%20Video%20Reconstruction&body=Title%3A%20LoD-Structured%203D%20Gaussian%20Splatting%20for%20Streaming%20Video%20Reconstruction%0AAuthor%3A%20Xinhui%20Liu%20and%20Can%20Wang%20and%20Lei%20Liu%20and%20Zhenghao%20Chen%20and%20Wei%20Jiang%20and%20Wei%20Wang%20and%20Dong%20Xu%0AAbstract%3A%20Free-Viewpoint%20Video%20%28FVV%29%20reconstruction%20enables%20photorealistic%20and%20interactive%203D%20scene%20visualization%3B%20however%2C%20real-time%20streaming%20is%20often%20bottlenecked%20by%20sparse-view%20inputs%2C%20prohibitive%20training%20costs%2C%20and%20bandwidth%20constraints.%20While%20recent%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20advanced%20FVV%20due%20to%20its%20superior%20rendering%20speed%2C%20Streaming%20Free-Viewpoint%20Video%20%28SFVV%29%20introduces%20additional%20demands%20for%20rapid%20optimization%2C%20high-fidelity%20reconstruction%20under%20sparse%20constraints%2C%20and%20minimal%20storage%20footprints.%20To%20bridge%20this%20gap%2C%20we%20propose%20StreamLoD-GS%2C%20an%20LoD-based%20Gaussian%20Splatting%20framework%20designed%20specifically%20for%20SFVV.%20Our%20approach%20integrates%20three%20core%20innovations%3A%201%29%20an%20Anchor-%20and%20Octree-based%20LoD-structured%203DGS%20with%20a%20hierarchical%20Gaussian%20dropout%20technique%20to%20ensure%20efficient%20and%20stable%20optimization%20while%20maintaining%20high-quality%20rendering%3B%202%29%20a%20GMM-based%20motion%20partitioning%20mechanism%20that%20separates%20dynamic%20and%20static%20content%2C%20refining%20dynamic%20regions%20while%20preserving%20background%20stability%3B%20and%203%29%20a%20quantized%20residual%20refinement%20framework%20that%20significantly%20reduces%20storage%20requirements%20without%20compromising%20visual%20fidelity.%20Extensive%20experiments%20demonstrate%20that%20StreamLoD-GS%20achieves%20competitive%20or%20state-of-the-art%20performance%20in%20terms%20of%20quality%2C%20efficiency%2C%20and%20storage.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18475v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoD-Structured%25203D%2520Gaussian%2520Splatting%2520for%2520Streaming%2520Video%2520Reconstruction%26entry.906535625%3DXinhui%2520Liu%2520and%2520Can%2520Wang%2520and%2520Lei%2520Liu%2520and%2520Zhenghao%2520Chen%2520and%2520Wei%2520Jiang%2520and%2520Wei%2520Wang%2520and%2520Dong%2520Xu%26entry.1292438233%3DFree-Viewpoint%2520Video%2520%2528FVV%2529%2520reconstruction%2520enables%2520photorealistic%2520and%2520interactive%25203D%2520scene%2520visualization%253B%2520however%252C%2520real-time%2520streaming%2520is%2520often%2520bottlenecked%2520by%2520sparse-view%2520inputs%252C%2520prohibitive%2520training%2520costs%252C%2520and%2520bandwidth%2520constraints.%2520While%2520recent%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520advanced%2520FVV%2520due%2520to%2520its%2520superior%2520rendering%2520speed%252C%2520Streaming%2520Free-Viewpoint%2520Video%2520%2528SFVV%2529%2520introduces%2520additional%2520demands%2520for%2520rapid%2520optimization%252C%2520high-fidelity%2520reconstruction%2520under%2520sparse%2520constraints%252C%2520and%2520minimal%2520storage%2520footprints.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520StreamLoD-GS%252C%2520an%2520LoD-based%2520Gaussian%2520Splatting%2520framework%2520designed%2520specifically%2520for%2520SFVV.%2520Our%2520approach%2520integrates%2520three%2520core%2520innovations%253A%25201%2529%2520an%2520Anchor-%2520and%2520Octree-based%2520LoD-structured%25203DGS%2520with%2520a%2520hierarchical%2520Gaussian%2520dropout%2520technique%2520to%2520ensure%2520efficient%2520and%2520stable%2520optimization%2520while%2520maintaining%2520high-quality%2520rendering%253B%25202%2529%2520a%2520GMM-based%2520motion%2520partitioning%2520mechanism%2520that%2520separates%2520dynamic%2520and%2520static%2520content%252C%2520refining%2520dynamic%2520regions%2520while%2520preserving%2520background%2520stability%253B%2520and%25203%2529%2520a%2520quantized%2520residual%2520refinement%2520framework%2520that%2520significantly%2520reduces%2520storage%2520requirements%2520without%2520compromising%2520visual%2520fidelity.%2520Extensive%2520experiments%2520demonstrate%2520that%2520StreamLoD-GS%2520achieves%2520competitive%2520or%2520state-of-the-art%2520performance%2520in%2520terms%2520of%2520quality%252C%2520efficiency%252C%2520and%2520storage.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18475v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoD-Structured%203D%20Gaussian%20Splatting%20for%20Streaming%20Video%20Reconstruction&entry.906535625=Xinhui%20Liu%20and%20Can%20Wang%20and%20Lei%20Liu%20and%20Zhenghao%20Chen%20and%20Wei%20Jiang%20and%20Wei%20Wang%20and%20Dong%20Xu&entry.1292438233=Free-Viewpoint%20Video%20%28FVV%29%20reconstruction%20enables%20photorealistic%20and%20interactive%203D%20scene%20visualization%3B%20however%2C%20real-time%20streaming%20is%20often%20bottlenecked%20by%20sparse-view%20inputs%2C%20prohibitive%20training%20costs%2C%20and%20bandwidth%20constraints.%20While%20recent%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20advanced%20FVV%20due%20to%20its%20superior%20rendering%20speed%2C%20Streaming%20Free-Viewpoint%20Video%20%28SFVV%29%20introduces%20additional%20demands%20for%20rapid%20optimization%2C%20high-fidelity%20reconstruction%20under%20sparse%20constraints%2C%20and%20minimal%20storage%20footprints.%20To%20bridge%20this%20gap%2C%20we%20propose%20StreamLoD-GS%2C%20an%20LoD-based%20Gaussian%20Splatting%20framework%20designed%20specifically%20for%20SFVV.%20Our%20approach%20integrates%20three%20core%20innovations%3A%201%29%20an%20Anchor-%20and%20Octree-based%20LoD-structured%203DGS%20with%20a%20hierarchical%20Gaussian%20dropout%20technique%20to%20ensure%20efficient%20and%20stable%20optimization%20while%20maintaining%20high-quality%20rendering%3B%202%29%20a%20GMM-based%20motion%20partitioning%20mechanism%20that%20separates%20dynamic%20and%20static%20content%2C%20refining%20dynamic%20regions%20while%20preserving%20background%20stability%3B%20and%203%29%20a%20quantized%20residual%20refinement%20framework%20that%20significantly%20reduces%20storage%20requirements%20without%20compromising%20visual%20fidelity.%20Extensive%20experiments%20demonstrate%20that%20StreamLoD-GS%20achieves%20competitive%20or%20state-of-the-art%20performance%20in%20terms%20of%20quality%2C%20efficiency%2C%20and%20storage.&entry.1838667208=http%3A//arxiv.org/abs/2601.18475v1&entry.124074799=Read"},
{"title": "GenAgent: Scaling Text-to-Image Generation via Agentic Multimodal Reasoning", "author": "Kaixun Jiang and Yuzheng Wang and Junjie Zhou and Pandeng Li and Zhihang Liu and Chen-Wei Xie and Zhaoyu Chen and Yun Zheng and Wenqiang Zhang", "abstract": "We introduce GenAgent, unifying visual understanding and generation through an agentic multimodal model. Unlike unified models that face expensive training costs and understanding-generation trade-offs, GenAgent decouples these capabilities through an agentic framework: understanding is handled by the multimodal model itself, while generation is achieved by treating image generation models as invokable tools. Crucially, unlike existing modular systems constrained by static pipelines, this design enables autonomous multi-turn interactions where the agent generates multimodal chains-of-thought encompassing reasoning, tool invocation, judgment, and reflection to iteratively refine outputs. We employ a two-stage training strategy: first, cold-start with supervised fine-tuning on high-quality tool invocation and reflection data to bootstrap agent behaviors; second, end-to-end agentic reinforcement learning combining pointwise rewards (final image quality) and pairwise rewards (reflection accuracy), with trajectory resampling for enhanced multi-turn exploration. GenAgent significantly boosts base generator(FLUX.1-dev) performance on GenEval++ (+23.6\\%) and WISE (+14\\%). Beyond performance gains, our framework demonstrates three key properties: 1) cross-tool generalization to generators with varying capabilities, 2) test-time scaling with consistent improvements across interaction rounds, and 3) task-adaptive reasoning that automatically adjusts to different tasks. Our code will be available at \\href{https://github.com/deep-kaixun/GenAgent}{this url}.", "link": "http://arxiv.org/abs/2601.18543v1", "date": "2026-01-26", "relevancy": 3.1824, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6923}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6153}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenAgent%3A%20Scaling%20Text-to-Image%20Generation%20via%20Agentic%20Multimodal%20Reasoning&body=Title%3A%20GenAgent%3A%20Scaling%20Text-to-Image%20Generation%20via%20Agentic%20Multimodal%20Reasoning%0AAuthor%3A%20Kaixun%20Jiang%20and%20Yuzheng%20Wang%20and%20Junjie%20Zhou%20and%20Pandeng%20Li%20and%20Zhihang%20Liu%20and%20Chen-Wei%20Xie%20and%20Zhaoyu%20Chen%20and%20Yun%20Zheng%20and%20Wenqiang%20Zhang%0AAbstract%3A%20We%20introduce%20GenAgent%2C%20unifying%20visual%20understanding%20and%20generation%20through%20an%20agentic%20multimodal%20model.%20Unlike%20unified%20models%20that%20face%20expensive%20training%20costs%20and%20understanding-generation%20trade-offs%2C%20GenAgent%20decouples%20these%20capabilities%20through%20an%20agentic%20framework%3A%20understanding%20is%20handled%20by%20the%20multimodal%20model%20itself%2C%20while%20generation%20is%20achieved%20by%20treating%20image%20generation%20models%20as%20invokable%20tools.%20Crucially%2C%20unlike%20existing%20modular%20systems%20constrained%20by%20static%20pipelines%2C%20this%20design%20enables%20autonomous%20multi-turn%20interactions%20where%20the%20agent%20generates%20multimodal%20chains-of-thought%20encompassing%20reasoning%2C%20tool%20invocation%2C%20judgment%2C%20and%20reflection%20to%20iteratively%20refine%20outputs.%20We%20employ%20a%20two-stage%20training%20strategy%3A%20first%2C%20cold-start%20with%20supervised%20fine-tuning%20on%20high-quality%20tool%20invocation%20and%20reflection%20data%20to%20bootstrap%20agent%20behaviors%3B%20second%2C%20end-to-end%20agentic%20reinforcement%20learning%20combining%20pointwise%20rewards%20%28final%20image%20quality%29%20and%20pairwise%20rewards%20%28reflection%20accuracy%29%2C%20with%20trajectory%20resampling%20for%20enhanced%20multi-turn%20exploration.%20GenAgent%20significantly%20boosts%20base%20generator%28FLUX.1-dev%29%20performance%20on%20GenEval%2B%2B%20%28%2B23.6%5C%25%29%20and%20WISE%20%28%2B14%5C%25%29.%20Beyond%20performance%20gains%2C%20our%20framework%20demonstrates%20three%20key%20properties%3A%201%29%20cross-tool%20generalization%20to%20generators%20with%20varying%20capabilities%2C%202%29%20test-time%20scaling%20with%20consistent%20improvements%20across%20interaction%20rounds%2C%20and%203%29%20task-adaptive%20reasoning%20that%20automatically%20adjusts%20to%20different%20tasks.%20Our%20code%20will%20be%20available%20at%20%5Chref%7Bhttps%3A//github.com/deep-kaixun/GenAgent%7D%7Bthis%20url%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18543v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenAgent%253A%2520Scaling%2520Text-to-Image%2520Generation%2520via%2520Agentic%2520Multimodal%2520Reasoning%26entry.906535625%3DKaixun%2520Jiang%2520and%2520Yuzheng%2520Wang%2520and%2520Junjie%2520Zhou%2520and%2520Pandeng%2520Li%2520and%2520Zhihang%2520Liu%2520and%2520Chen-Wei%2520Xie%2520and%2520Zhaoyu%2520Chen%2520and%2520Yun%2520Zheng%2520and%2520Wenqiang%2520Zhang%26entry.1292438233%3DWe%2520introduce%2520GenAgent%252C%2520unifying%2520visual%2520understanding%2520and%2520generation%2520through%2520an%2520agentic%2520multimodal%2520model.%2520Unlike%2520unified%2520models%2520that%2520face%2520expensive%2520training%2520costs%2520and%2520understanding-generation%2520trade-offs%252C%2520GenAgent%2520decouples%2520these%2520capabilities%2520through%2520an%2520agentic%2520framework%253A%2520understanding%2520is%2520handled%2520by%2520the%2520multimodal%2520model%2520itself%252C%2520while%2520generation%2520is%2520achieved%2520by%2520treating%2520image%2520generation%2520models%2520as%2520invokable%2520tools.%2520Crucially%252C%2520unlike%2520existing%2520modular%2520systems%2520constrained%2520by%2520static%2520pipelines%252C%2520this%2520design%2520enables%2520autonomous%2520multi-turn%2520interactions%2520where%2520the%2520agent%2520generates%2520multimodal%2520chains-of-thought%2520encompassing%2520reasoning%252C%2520tool%2520invocation%252C%2520judgment%252C%2520and%2520reflection%2520to%2520iteratively%2520refine%2520outputs.%2520We%2520employ%2520a%2520two-stage%2520training%2520strategy%253A%2520first%252C%2520cold-start%2520with%2520supervised%2520fine-tuning%2520on%2520high-quality%2520tool%2520invocation%2520and%2520reflection%2520data%2520to%2520bootstrap%2520agent%2520behaviors%253B%2520second%252C%2520end-to-end%2520agentic%2520reinforcement%2520learning%2520combining%2520pointwise%2520rewards%2520%2528final%2520image%2520quality%2529%2520and%2520pairwise%2520rewards%2520%2528reflection%2520accuracy%2529%252C%2520with%2520trajectory%2520resampling%2520for%2520enhanced%2520multi-turn%2520exploration.%2520GenAgent%2520significantly%2520boosts%2520base%2520generator%2528FLUX.1-dev%2529%2520performance%2520on%2520GenEval%252B%252B%2520%2528%252B23.6%255C%2525%2529%2520and%2520WISE%2520%2528%252B14%255C%2525%2529.%2520Beyond%2520performance%2520gains%252C%2520our%2520framework%2520demonstrates%2520three%2520key%2520properties%253A%25201%2529%2520cross-tool%2520generalization%2520to%2520generators%2520with%2520varying%2520capabilities%252C%25202%2529%2520test-time%2520scaling%2520with%2520consistent%2520improvements%2520across%2520interaction%2520rounds%252C%2520and%25203%2529%2520task-adaptive%2520reasoning%2520that%2520automatically%2520adjusts%2520to%2520different%2520tasks.%2520Our%2520code%2520will%2520be%2520available%2520at%2520%255Chref%257Bhttps%253A//github.com/deep-kaixun/GenAgent%257D%257Bthis%2520url%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18543v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenAgent%3A%20Scaling%20Text-to-Image%20Generation%20via%20Agentic%20Multimodal%20Reasoning&entry.906535625=Kaixun%20Jiang%20and%20Yuzheng%20Wang%20and%20Junjie%20Zhou%20and%20Pandeng%20Li%20and%20Zhihang%20Liu%20and%20Chen-Wei%20Xie%20and%20Zhaoyu%20Chen%20and%20Yun%20Zheng%20and%20Wenqiang%20Zhang&entry.1292438233=We%20introduce%20GenAgent%2C%20unifying%20visual%20understanding%20and%20generation%20through%20an%20agentic%20multimodal%20model.%20Unlike%20unified%20models%20that%20face%20expensive%20training%20costs%20and%20understanding-generation%20trade-offs%2C%20GenAgent%20decouples%20these%20capabilities%20through%20an%20agentic%20framework%3A%20understanding%20is%20handled%20by%20the%20multimodal%20model%20itself%2C%20while%20generation%20is%20achieved%20by%20treating%20image%20generation%20models%20as%20invokable%20tools.%20Crucially%2C%20unlike%20existing%20modular%20systems%20constrained%20by%20static%20pipelines%2C%20this%20design%20enables%20autonomous%20multi-turn%20interactions%20where%20the%20agent%20generates%20multimodal%20chains-of-thought%20encompassing%20reasoning%2C%20tool%20invocation%2C%20judgment%2C%20and%20reflection%20to%20iteratively%20refine%20outputs.%20We%20employ%20a%20two-stage%20training%20strategy%3A%20first%2C%20cold-start%20with%20supervised%20fine-tuning%20on%20high-quality%20tool%20invocation%20and%20reflection%20data%20to%20bootstrap%20agent%20behaviors%3B%20second%2C%20end-to-end%20agentic%20reinforcement%20learning%20combining%20pointwise%20rewards%20%28final%20image%20quality%29%20and%20pairwise%20rewards%20%28reflection%20accuracy%29%2C%20with%20trajectory%20resampling%20for%20enhanced%20multi-turn%20exploration.%20GenAgent%20significantly%20boosts%20base%20generator%28FLUX.1-dev%29%20performance%20on%20GenEval%2B%2B%20%28%2B23.6%5C%25%29%20and%20WISE%20%28%2B14%5C%25%29.%20Beyond%20performance%20gains%2C%20our%20framework%20demonstrates%20three%20key%20properties%3A%201%29%20cross-tool%20generalization%20to%20generators%20with%20varying%20capabilities%2C%202%29%20test-time%20scaling%20with%20consistent%20improvements%20across%20interaction%20rounds%2C%20and%203%29%20task-adaptive%20reasoning%20that%20automatically%20adjusts%20to%20different%20tasks.%20Our%20code%20will%20be%20available%20at%20%5Chref%7Bhttps%3A//github.com/deep-kaixun/GenAgent%7D%7Bthis%20url%7D.&entry.1838667208=http%3A//arxiv.org/abs/2601.18543v1&entry.124074799=Read"},
{"title": "GPA-VGGT:Adapting VGGT to Large Scale Localization by Self-Supervised Learning with Geometry and Physics Aware Loss", "author": "Yangfan Xu and Lilian Zhang and Xiaofeng He and Pengdong Wu and Wenqi Wu and Jun Mao", "abstract": "Transformer-based general visual geometry frameworks have shown promising performance in camera pose estimation and 3D scene understanding. Recent advancements in Visual Geometry Grounded Transformer (VGGT) models have shown great promise in camera pose estimation and 3D reconstruction. However, these models typically rely on ground truth labels for training, posing challenges when adapting to unlabeled and unseen scenes. In this paper, we propose a self-supervised framework to train VGGT with unlabeled data, thereby enhancing its localization capability in large-scale environments. To achieve this, we extend conventional pair-wise relations to sequence-wise geometric constraints for self-supervised learning. Specifically, in each sequence, we sample multiple source frames and geometrically project them onto different target frames, which improves temporal feature consistency. We formulate physical photometric consistency and geometric constraints as a joint optimization loss to circumvent the requirement for hard labels. By training the model with this proposed method, not only the local and global cross-view attention layers but also the camera and depth heads can effectively capture the underlying multi-view geometry. Experiments demonstrate that the model converges within hundreds of iterations and achieves significant improvements in large-scale localization. Our code will be released at https://github.com/X-yangfan/GPA-VGGT.", "link": "http://arxiv.org/abs/2601.16885v2", "date": "2026-01-26", "relevancy": 3.0849, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6274}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6264}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPA-VGGT%3AAdapting%20VGGT%20to%20Large%20Scale%20Localization%20by%20Self-Supervised%20Learning%20with%20Geometry%20and%20Physics%20Aware%20Loss&body=Title%3A%20GPA-VGGT%3AAdapting%20VGGT%20to%20Large%20Scale%20Localization%20by%20Self-Supervised%20Learning%20with%20Geometry%20and%20Physics%20Aware%20Loss%0AAuthor%3A%20Yangfan%20Xu%20and%20Lilian%20Zhang%20and%20Xiaofeng%20He%20and%20Pengdong%20Wu%20and%20Wenqi%20Wu%20and%20Jun%20Mao%0AAbstract%3A%20Transformer-based%20general%20visual%20geometry%20frameworks%20have%20shown%20promising%20performance%20in%20camera%20pose%20estimation%20and%203D%20scene%20understanding.%20Recent%20advancements%20in%20Visual%20Geometry%20Grounded%20Transformer%20%28VGGT%29%20models%20have%20shown%20great%20promise%20in%20camera%20pose%20estimation%20and%203D%20reconstruction.%20However%2C%20these%20models%20typically%20rely%20on%20ground%20truth%20labels%20for%20training%2C%20posing%20challenges%20when%20adapting%20to%20unlabeled%20and%20unseen%20scenes.%20In%20this%20paper%2C%20we%20propose%20a%20self-supervised%20framework%20to%20train%20VGGT%20with%20unlabeled%20data%2C%20thereby%20enhancing%20its%20localization%20capability%20in%20large-scale%20environments.%20To%20achieve%20this%2C%20we%20extend%20conventional%20pair-wise%20relations%20to%20sequence-wise%20geometric%20constraints%20for%20self-supervised%20learning.%20Specifically%2C%20in%20each%20sequence%2C%20we%20sample%20multiple%20source%20frames%20and%20geometrically%20project%20them%20onto%20different%20target%20frames%2C%20which%20improves%20temporal%20feature%20consistency.%20We%20formulate%20physical%20photometric%20consistency%20and%20geometric%20constraints%20as%20a%20joint%20optimization%20loss%20to%20circumvent%20the%20requirement%20for%20hard%20labels.%20By%20training%20the%20model%20with%20this%20proposed%20method%2C%20not%20only%20the%20local%20and%20global%20cross-view%20attention%20layers%20but%20also%20the%20camera%20and%20depth%20heads%20can%20effectively%20capture%20the%20underlying%20multi-view%20geometry.%20Experiments%20demonstrate%20that%20the%20model%20converges%20within%20hundreds%20of%20iterations%20and%20achieves%20significant%20improvements%20in%20large-scale%20localization.%20Our%20code%20will%20be%20released%20at%20https%3A//github.com/X-yangfan/GPA-VGGT.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16885v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPA-VGGT%253AAdapting%2520VGGT%2520to%2520Large%2520Scale%2520Localization%2520by%2520Self-Supervised%2520Learning%2520with%2520Geometry%2520and%2520Physics%2520Aware%2520Loss%26entry.906535625%3DYangfan%2520Xu%2520and%2520Lilian%2520Zhang%2520and%2520Xiaofeng%2520He%2520and%2520Pengdong%2520Wu%2520and%2520Wenqi%2520Wu%2520and%2520Jun%2520Mao%26entry.1292438233%3DTransformer-based%2520general%2520visual%2520geometry%2520frameworks%2520have%2520shown%2520promising%2520performance%2520in%2520camera%2520pose%2520estimation%2520and%25203D%2520scene%2520understanding.%2520Recent%2520advancements%2520in%2520Visual%2520Geometry%2520Grounded%2520Transformer%2520%2528VGGT%2529%2520models%2520have%2520shown%2520great%2520promise%2520in%2520camera%2520pose%2520estimation%2520and%25203D%2520reconstruction.%2520However%252C%2520these%2520models%2520typically%2520rely%2520on%2520ground%2520truth%2520labels%2520for%2520training%252C%2520posing%2520challenges%2520when%2520adapting%2520to%2520unlabeled%2520and%2520unseen%2520scenes.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520self-supervised%2520framework%2520to%2520train%2520VGGT%2520with%2520unlabeled%2520data%252C%2520thereby%2520enhancing%2520its%2520localization%2520capability%2520in%2520large-scale%2520environments.%2520To%2520achieve%2520this%252C%2520we%2520extend%2520conventional%2520pair-wise%2520relations%2520to%2520sequence-wise%2520geometric%2520constraints%2520for%2520self-supervised%2520learning.%2520Specifically%252C%2520in%2520each%2520sequence%252C%2520we%2520sample%2520multiple%2520source%2520frames%2520and%2520geometrically%2520project%2520them%2520onto%2520different%2520target%2520frames%252C%2520which%2520improves%2520temporal%2520feature%2520consistency.%2520We%2520formulate%2520physical%2520photometric%2520consistency%2520and%2520geometric%2520constraints%2520as%2520a%2520joint%2520optimization%2520loss%2520to%2520circumvent%2520the%2520requirement%2520for%2520hard%2520labels.%2520By%2520training%2520the%2520model%2520with%2520this%2520proposed%2520method%252C%2520not%2520only%2520the%2520local%2520and%2520global%2520cross-view%2520attention%2520layers%2520but%2520also%2520the%2520camera%2520and%2520depth%2520heads%2520can%2520effectively%2520capture%2520the%2520underlying%2520multi-view%2520geometry.%2520Experiments%2520demonstrate%2520that%2520the%2520model%2520converges%2520within%2520hundreds%2520of%2520iterations%2520and%2520achieves%2520significant%2520improvements%2520in%2520large-scale%2520localization.%2520Our%2520code%2520will%2520be%2520released%2520at%2520https%253A//github.com/X-yangfan/GPA-VGGT.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16885v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPA-VGGT%3AAdapting%20VGGT%20to%20Large%20Scale%20Localization%20by%20Self-Supervised%20Learning%20with%20Geometry%20and%20Physics%20Aware%20Loss&entry.906535625=Yangfan%20Xu%20and%20Lilian%20Zhang%20and%20Xiaofeng%20He%20and%20Pengdong%20Wu%20and%20Wenqi%20Wu%20and%20Jun%20Mao&entry.1292438233=Transformer-based%20general%20visual%20geometry%20frameworks%20have%20shown%20promising%20performance%20in%20camera%20pose%20estimation%20and%203D%20scene%20understanding.%20Recent%20advancements%20in%20Visual%20Geometry%20Grounded%20Transformer%20%28VGGT%29%20models%20have%20shown%20great%20promise%20in%20camera%20pose%20estimation%20and%203D%20reconstruction.%20However%2C%20these%20models%20typically%20rely%20on%20ground%20truth%20labels%20for%20training%2C%20posing%20challenges%20when%20adapting%20to%20unlabeled%20and%20unseen%20scenes.%20In%20this%20paper%2C%20we%20propose%20a%20self-supervised%20framework%20to%20train%20VGGT%20with%20unlabeled%20data%2C%20thereby%20enhancing%20its%20localization%20capability%20in%20large-scale%20environments.%20To%20achieve%20this%2C%20we%20extend%20conventional%20pair-wise%20relations%20to%20sequence-wise%20geometric%20constraints%20for%20self-supervised%20learning.%20Specifically%2C%20in%20each%20sequence%2C%20we%20sample%20multiple%20source%20frames%20and%20geometrically%20project%20them%20onto%20different%20target%20frames%2C%20which%20improves%20temporal%20feature%20consistency.%20We%20formulate%20physical%20photometric%20consistency%20and%20geometric%20constraints%20as%20a%20joint%20optimization%20loss%20to%20circumvent%20the%20requirement%20for%20hard%20labels.%20By%20training%20the%20model%20with%20this%20proposed%20method%2C%20not%20only%20the%20local%20and%20global%20cross-view%20attention%20layers%20but%20also%20the%20camera%20and%20depth%20heads%20can%20effectively%20capture%20the%20underlying%20multi-view%20geometry.%20Experiments%20demonstrate%20that%20the%20model%20converges%20within%20hundreds%20of%20iterations%20and%20achieves%20significant%20improvements%20in%20large-scale%20localization.%20Our%20code%20will%20be%20released%20at%20https%3A//github.com/X-yangfan/GPA-VGGT.&entry.1838667208=http%3A//arxiv.org/abs/2601.16885v2&entry.124074799=Read"},
{"title": "Coding the Visual World: From Image to Simulation Using Vision Language Models", "author": "Sagi Eppel", "abstract": "The ability to construct mental models of the world is a central aspect of understanding. Similarly, visual understanding can be viewed as the ability to construct a representative model of the system depicted in an image. This work explores the capacity of Vision Language Models (VLMs) to recognize and simulate the systems and mechanisms depicted in images using the Im2Sim methodology. The VLM is given a natural image of a real-world system (e.g., cities, clouds, vegetation) and is tasked with describing the system and writing code that simulates and generates it. This generative code is then executed to produce a synthetic image, which is compared against the original. This approach is tested on various complex emergent systems, ranging from physical systems (waves, lights, clouds) to vegetation, cities, materials, and geological formations. Through analysis of the models and images generated by the VLMs, we examine their understanding of the systems in images. The results show that leading VLMs (GPT, Gemini) have the ability to understand and model complex, multi-component systems across multiple layers of abstraction and a wide range of domains. At the same time, the VLMs exhibit limited ability to replicate fine details and low-level arrangements of patterns in the image. These findings reveal an interesting asymmetry: VLMs combine high-level, deep visual understanding of images with limited perception of fine details.", "link": "http://arxiv.org/abs/2601.05344v3", "date": "2026-01-26", "relevancy": 3.0146, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6256}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6256}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Coding%20the%20Visual%20World%3A%20From%20Image%20to%20Simulation%20Using%20Vision%20Language%20Models&body=Title%3A%20Coding%20the%20Visual%20World%3A%20From%20Image%20to%20Simulation%20Using%20Vision%20Language%20Models%0AAuthor%3A%20Sagi%20Eppel%0AAbstract%3A%20The%20ability%20to%20construct%20mental%20models%20of%20the%20world%20is%20a%20central%20aspect%20of%20understanding.%20Similarly%2C%20visual%20understanding%20can%20be%20viewed%20as%20the%20ability%20to%20construct%20a%20representative%20model%20of%20the%20system%20depicted%20in%20an%20image.%20This%20work%20explores%20the%20capacity%20of%20Vision%20Language%20Models%20%28VLMs%29%20to%20recognize%20and%20simulate%20the%20systems%20and%20mechanisms%20depicted%20in%20images%20using%20the%20Im2Sim%20methodology.%20The%20VLM%20is%20given%20a%20natural%20image%20of%20a%20real-world%20system%20%28e.g.%2C%20cities%2C%20clouds%2C%20vegetation%29%20and%20is%20tasked%20with%20describing%20the%20system%20and%20writing%20code%20that%20simulates%20and%20generates%20it.%20This%20generative%20code%20is%20then%20executed%20to%20produce%20a%20synthetic%20image%2C%20which%20is%20compared%20against%20the%20original.%20This%20approach%20is%20tested%20on%20various%20complex%20emergent%20systems%2C%20ranging%20from%20physical%20systems%20%28waves%2C%20lights%2C%20clouds%29%20to%20vegetation%2C%20cities%2C%20materials%2C%20and%20geological%20formations.%20Through%20analysis%20of%20the%20models%20and%20images%20generated%20by%20the%20VLMs%2C%20we%20examine%20their%20understanding%20of%20the%20systems%20in%20images.%20The%20results%20show%20that%20leading%20VLMs%20%28GPT%2C%20Gemini%29%20have%20the%20ability%20to%20understand%20and%20model%20complex%2C%20multi-component%20systems%20across%20multiple%20layers%20of%20abstraction%20and%20a%20wide%20range%20of%20domains.%20At%20the%20same%20time%2C%20the%20VLMs%20exhibit%20limited%20ability%20to%20replicate%20fine%20details%20and%20low-level%20arrangements%20of%20patterns%20in%20the%20image.%20These%20findings%20reveal%20an%20interesting%20asymmetry%3A%20VLMs%20combine%20high-level%2C%20deep%20visual%20understanding%20of%20images%20with%20limited%20perception%20of%20fine%20details.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05344v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoding%2520the%2520Visual%2520World%253A%2520From%2520Image%2520to%2520Simulation%2520Using%2520Vision%2520Language%2520Models%26entry.906535625%3DSagi%2520Eppel%26entry.1292438233%3DThe%2520ability%2520to%2520construct%2520mental%2520models%2520of%2520the%2520world%2520is%2520a%2520central%2520aspect%2520of%2520understanding.%2520Similarly%252C%2520visual%2520understanding%2520can%2520be%2520viewed%2520as%2520the%2520ability%2520to%2520construct%2520a%2520representative%2520model%2520of%2520the%2520system%2520depicted%2520in%2520an%2520image.%2520This%2520work%2520explores%2520the%2520capacity%2520of%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520to%2520recognize%2520and%2520simulate%2520the%2520systems%2520and%2520mechanisms%2520depicted%2520in%2520images%2520using%2520the%2520Im2Sim%2520methodology.%2520The%2520VLM%2520is%2520given%2520a%2520natural%2520image%2520of%2520a%2520real-world%2520system%2520%2528e.g.%252C%2520cities%252C%2520clouds%252C%2520vegetation%2529%2520and%2520is%2520tasked%2520with%2520describing%2520the%2520system%2520and%2520writing%2520code%2520that%2520simulates%2520and%2520generates%2520it.%2520This%2520generative%2520code%2520is%2520then%2520executed%2520to%2520produce%2520a%2520synthetic%2520image%252C%2520which%2520is%2520compared%2520against%2520the%2520original.%2520This%2520approach%2520is%2520tested%2520on%2520various%2520complex%2520emergent%2520systems%252C%2520ranging%2520from%2520physical%2520systems%2520%2528waves%252C%2520lights%252C%2520clouds%2529%2520to%2520vegetation%252C%2520cities%252C%2520materials%252C%2520and%2520geological%2520formations.%2520Through%2520analysis%2520of%2520the%2520models%2520and%2520images%2520generated%2520by%2520the%2520VLMs%252C%2520we%2520examine%2520their%2520understanding%2520of%2520the%2520systems%2520in%2520images.%2520The%2520results%2520show%2520that%2520leading%2520VLMs%2520%2528GPT%252C%2520Gemini%2529%2520have%2520the%2520ability%2520to%2520understand%2520and%2520model%2520complex%252C%2520multi-component%2520systems%2520across%2520multiple%2520layers%2520of%2520abstraction%2520and%2520a%2520wide%2520range%2520of%2520domains.%2520At%2520the%2520same%2520time%252C%2520the%2520VLMs%2520exhibit%2520limited%2520ability%2520to%2520replicate%2520fine%2520details%2520and%2520low-level%2520arrangements%2520of%2520patterns%2520in%2520the%2520image.%2520These%2520findings%2520reveal%2520an%2520interesting%2520asymmetry%253A%2520VLMs%2520combine%2520high-level%252C%2520deep%2520visual%2520understanding%2520of%2520images%2520with%2520limited%2520perception%2520of%2520fine%2520details.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05344v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coding%20the%20Visual%20World%3A%20From%20Image%20to%20Simulation%20Using%20Vision%20Language%20Models&entry.906535625=Sagi%20Eppel&entry.1292438233=The%20ability%20to%20construct%20mental%20models%20of%20the%20world%20is%20a%20central%20aspect%20of%20understanding.%20Similarly%2C%20visual%20understanding%20can%20be%20viewed%20as%20the%20ability%20to%20construct%20a%20representative%20model%20of%20the%20system%20depicted%20in%20an%20image.%20This%20work%20explores%20the%20capacity%20of%20Vision%20Language%20Models%20%28VLMs%29%20to%20recognize%20and%20simulate%20the%20systems%20and%20mechanisms%20depicted%20in%20images%20using%20the%20Im2Sim%20methodology.%20The%20VLM%20is%20given%20a%20natural%20image%20of%20a%20real-world%20system%20%28e.g.%2C%20cities%2C%20clouds%2C%20vegetation%29%20and%20is%20tasked%20with%20describing%20the%20system%20and%20writing%20code%20that%20simulates%20and%20generates%20it.%20This%20generative%20code%20is%20then%20executed%20to%20produce%20a%20synthetic%20image%2C%20which%20is%20compared%20against%20the%20original.%20This%20approach%20is%20tested%20on%20various%20complex%20emergent%20systems%2C%20ranging%20from%20physical%20systems%20%28waves%2C%20lights%2C%20clouds%29%20to%20vegetation%2C%20cities%2C%20materials%2C%20and%20geological%20formations.%20Through%20analysis%20of%20the%20models%20and%20images%20generated%20by%20the%20VLMs%2C%20we%20examine%20their%20understanding%20of%20the%20systems%20in%20images.%20The%20results%20show%20that%20leading%20VLMs%20%28GPT%2C%20Gemini%29%20have%20the%20ability%20to%20understand%20and%20model%20complex%2C%20multi-component%20systems%20across%20multiple%20layers%20of%20abstraction%20and%20a%20wide%20range%20of%20domains.%20At%20the%20same%20time%2C%20the%20VLMs%20exhibit%20limited%20ability%20to%20replicate%20fine%20details%20and%20low-level%20arrangements%20of%20patterns%20in%20the%20image.%20These%20findings%20reveal%20an%20interesting%20asymmetry%3A%20VLMs%20combine%20high-level%2C%20deep%20visual%20understanding%20of%20images%20with%20limited%20perception%20of%20fine%20details.&entry.1838667208=http%3A//arxiv.org/abs/2601.05344v3&entry.124074799=Read"},
{"title": "TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion", "author": "Weishi Mi and Yong Bao and Xiaowei Chi and Xiaozhu Ju and Zhiyuan Qin and Kuangzhi Ge and Kai Tang and Peidong Jia and Shanghang Zhang and Jian Tang", "abstract": "The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions.\n  To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool's imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control.\n  TC-IDM extracts the tool's point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals.\n  This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects.\n  In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models.", "link": "http://arxiv.org/abs/2601.18323v1", "date": "2026-01-26", "relevancy": 3.0068, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6141}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5968}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TC-IDM%3A%20Grounding%20Video%20Generation%20for%20Executable%20Zero-shot%20Robot%20Motion&body=Title%3A%20TC-IDM%3A%20Grounding%20Video%20Generation%20for%20Executable%20Zero-shot%20Robot%20Motion%0AAuthor%3A%20Weishi%20Mi%20and%20Yong%20Bao%20and%20Xiaowei%20Chi%20and%20Xiaozhu%20Ju%20and%20Zhiyuan%20Qin%20and%20Kuangzhi%20Ge%20and%20Kai%20Tang%20and%20Peidong%20Jia%20and%20Shanghang%20Zhang%20and%20Jian%20Tang%0AAbstract%3A%20The%20vision-language-action%20%28VLA%29%20paradigm%20has%20enabled%20powerful%20robotic%20control%20by%20leveraging%20vision-language%20models%2C%20but%20its%20reliance%20on%20large-scale%2C%20high-quality%20robot%20data%20limits%20its%20generalization.%20Generative%20world%20models%20offer%20a%20promising%20alternative%20for%20general-purpose%20embodied%20AI%2C%20yet%20a%20critical%20gap%20remains%20between%20their%20pixel-level%20plans%20and%20physically%20executable%20actions.%0A%20%20To%20this%20end%2C%20we%20propose%20the%20Tool-Centric%20Inverse%20Dynamics%20Model%20%28TC-IDM%29.%20By%20focusing%20on%20the%20tool%27s%20imagined%20trajectory%20as%20synthesized%20by%20the%20world%20model%2C%20TC-IDM%20establishes%20a%20robust%20intermediate%20representation%20that%20bridges%20the%20gap%20between%20visual%20planning%20and%20physical%20control.%0A%20%20TC-IDM%20extracts%20the%20tool%27s%20point%20cloud%20trajectories%20via%20segmentation%20and%203D%20motion%20estimation%20from%20generated%20videos.%20Considering%20diverse%20tool%20attributes%2C%20our%20architecture%20employs%20decoupled%20action%20heads%20to%20project%20these%20planned%20trajectories%20into%206-DoF%20end-effector%20motions%20and%20corresponding%20control%20signals.%0A%20%20This%20plan-and-translate%20paradigm%20not%20only%20supports%20a%20wide%20range%20of%20end-effectors%20but%20also%20significantly%20improves%20viewpoint%20invariance.%20Furthermore%2C%20it%20exhibits%20strong%20generalization%20capabilities%20across%20long-horizon%20and%20out-of-distribution%20tasks%2C%20including%20interacting%20with%20deformable%20objects.%0A%20%20In%20real-world%20evaluations%2C%20the%20world%20model%20with%20TC-IDM%20achieves%20an%20average%20success%20rate%20of%2061.11%20percent%2C%20with%2077.7%20percent%20on%20simple%20tasks%20and%2038.46%20percent%20on%20zero-shot%20deformable%20object%20tasks.%20It%20substantially%20outperforms%20end-to-end%20VLA-style%20baselines%20and%20other%20inverse%20dynamics%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18323v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTC-IDM%253A%2520Grounding%2520Video%2520Generation%2520for%2520Executable%2520Zero-shot%2520Robot%2520Motion%26entry.906535625%3DWeishi%2520Mi%2520and%2520Yong%2520Bao%2520and%2520Xiaowei%2520Chi%2520and%2520Xiaozhu%2520Ju%2520and%2520Zhiyuan%2520Qin%2520and%2520Kuangzhi%2520Ge%2520and%2520Kai%2520Tang%2520and%2520Peidong%2520Jia%2520and%2520Shanghang%2520Zhang%2520and%2520Jian%2520Tang%26entry.1292438233%3DThe%2520vision-language-action%2520%2528VLA%2529%2520paradigm%2520has%2520enabled%2520powerful%2520robotic%2520control%2520by%2520leveraging%2520vision-language%2520models%252C%2520but%2520its%2520reliance%2520on%2520large-scale%252C%2520high-quality%2520robot%2520data%2520limits%2520its%2520generalization.%2520Generative%2520world%2520models%2520offer%2520a%2520promising%2520alternative%2520for%2520general-purpose%2520embodied%2520AI%252C%2520yet%2520a%2520critical%2520gap%2520remains%2520between%2520their%2520pixel-level%2520plans%2520and%2520physically%2520executable%2520actions.%250A%2520%2520To%2520this%2520end%252C%2520we%2520propose%2520the%2520Tool-Centric%2520Inverse%2520Dynamics%2520Model%2520%2528TC-IDM%2529.%2520By%2520focusing%2520on%2520the%2520tool%2527s%2520imagined%2520trajectory%2520as%2520synthesized%2520by%2520the%2520world%2520model%252C%2520TC-IDM%2520establishes%2520a%2520robust%2520intermediate%2520representation%2520that%2520bridges%2520the%2520gap%2520between%2520visual%2520planning%2520and%2520physical%2520control.%250A%2520%2520TC-IDM%2520extracts%2520the%2520tool%2527s%2520point%2520cloud%2520trajectories%2520via%2520segmentation%2520and%25203D%2520motion%2520estimation%2520from%2520generated%2520videos.%2520Considering%2520diverse%2520tool%2520attributes%252C%2520our%2520architecture%2520employs%2520decoupled%2520action%2520heads%2520to%2520project%2520these%2520planned%2520trajectories%2520into%25206-DoF%2520end-effector%2520motions%2520and%2520corresponding%2520control%2520signals.%250A%2520%2520This%2520plan-and-translate%2520paradigm%2520not%2520only%2520supports%2520a%2520wide%2520range%2520of%2520end-effectors%2520but%2520also%2520significantly%2520improves%2520viewpoint%2520invariance.%2520Furthermore%252C%2520it%2520exhibits%2520strong%2520generalization%2520capabilities%2520across%2520long-horizon%2520and%2520out-of-distribution%2520tasks%252C%2520including%2520interacting%2520with%2520deformable%2520objects.%250A%2520%2520In%2520real-world%2520evaluations%252C%2520the%2520world%2520model%2520with%2520TC-IDM%2520achieves%2520an%2520average%2520success%2520rate%2520of%252061.11%2520percent%252C%2520with%252077.7%2520percent%2520on%2520simple%2520tasks%2520and%252038.46%2520percent%2520on%2520zero-shot%2520deformable%2520object%2520tasks.%2520It%2520substantially%2520outperforms%2520end-to-end%2520VLA-style%2520baselines%2520and%2520other%2520inverse%2520dynamics%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18323v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TC-IDM%3A%20Grounding%20Video%20Generation%20for%20Executable%20Zero-shot%20Robot%20Motion&entry.906535625=Weishi%20Mi%20and%20Yong%20Bao%20and%20Xiaowei%20Chi%20and%20Xiaozhu%20Ju%20and%20Zhiyuan%20Qin%20and%20Kuangzhi%20Ge%20and%20Kai%20Tang%20and%20Peidong%20Jia%20and%20Shanghang%20Zhang%20and%20Jian%20Tang&entry.1292438233=The%20vision-language-action%20%28VLA%29%20paradigm%20has%20enabled%20powerful%20robotic%20control%20by%20leveraging%20vision-language%20models%2C%20but%20its%20reliance%20on%20large-scale%2C%20high-quality%20robot%20data%20limits%20its%20generalization.%20Generative%20world%20models%20offer%20a%20promising%20alternative%20for%20general-purpose%20embodied%20AI%2C%20yet%20a%20critical%20gap%20remains%20between%20their%20pixel-level%20plans%20and%20physically%20executable%20actions.%0A%20%20To%20this%20end%2C%20we%20propose%20the%20Tool-Centric%20Inverse%20Dynamics%20Model%20%28TC-IDM%29.%20By%20focusing%20on%20the%20tool%27s%20imagined%20trajectory%20as%20synthesized%20by%20the%20world%20model%2C%20TC-IDM%20establishes%20a%20robust%20intermediate%20representation%20that%20bridges%20the%20gap%20between%20visual%20planning%20and%20physical%20control.%0A%20%20TC-IDM%20extracts%20the%20tool%27s%20point%20cloud%20trajectories%20via%20segmentation%20and%203D%20motion%20estimation%20from%20generated%20videos.%20Considering%20diverse%20tool%20attributes%2C%20our%20architecture%20employs%20decoupled%20action%20heads%20to%20project%20these%20planned%20trajectories%20into%206-DoF%20end-effector%20motions%20and%20corresponding%20control%20signals.%0A%20%20This%20plan-and-translate%20paradigm%20not%20only%20supports%20a%20wide%20range%20of%20end-effectors%20but%20also%20significantly%20improves%20viewpoint%20invariance.%20Furthermore%2C%20it%20exhibits%20strong%20generalization%20capabilities%20across%20long-horizon%20and%20out-of-distribution%20tasks%2C%20including%20interacting%20with%20deformable%20objects.%0A%20%20In%20real-world%20evaluations%2C%20the%20world%20model%20with%20TC-IDM%20achieves%20an%20average%20success%20rate%20of%2061.11%20percent%2C%20with%2077.7%20percent%20on%20simple%20tasks%20and%2038.46%20percent%20on%20zero-shot%20deformable%20object%20tasks.%20It%20substantially%20outperforms%20end-to-end%20VLA-style%20baselines%20and%20other%20inverse%20dynamics%20models.&entry.1838667208=http%3A//arxiv.org/abs/2601.18323v1&entry.124074799=Read"},
{"title": "Self-Refining Video Sampling", "author": "Sangwon Jang and Taekyung Ki and Jaehyeong Jo and Saining Xie and Jaehong Yoon and Sung Ju Hwang", "abstract": "Modern video generators still struggle with complex physical dynamics, often falling short of physical realism. Existing approaches address this using external verifiers or additional training on augmented data, which is computationally expensive and still limited in capturing fine-grained motion. In this work, we present self-refining video sampling, a simple method that uses a pre-trained video generator trained on large-scale datasets as its own self-refiner. By interpreting the generator as a denoising autoencoder, we enable iterative inner-loop refinement at inference time without any external verifier or additional training. We further introduce an uncertainty-aware refinement strategy that selectively refines regions based on self-consistency, which prevents artifacts caused by over-refinement. Experiments on state-of-the-art video generators demonstrate significant improvements in motion coherence and physics alignment, achieving over 70\\% human preference compared to the default sampler and guidance-based sampler.", "link": "http://arxiv.org/abs/2601.18577v1", "date": "2026-01-26", "relevancy": 3.0055, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.619}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6059}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5784}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Refining%20Video%20Sampling&body=Title%3A%20Self-Refining%20Video%20Sampling%0AAuthor%3A%20Sangwon%20Jang%20and%20Taekyung%20Ki%20and%20Jaehyeong%20Jo%20and%20Saining%20Xie%20and%20Jaehong%20Yoon%20and%20Sung%20Ju%20Hwang%0AAbstract%3A%20Modern%20video%20generators%20still%20struggle%20with%20complex%20physical%20dynamics%2C%20often%20falling%20short%20of%20physical%20realism.%20Existing%20approaches%20address%20this%20using%20external%20verifiers%20or%20additional%20training%20on%20augmented%20data%2C%20which%20is%20computationally%20expensive%20and%20still%20limited%20in%20capturing%20fine-grained%20motion.%20In%20this%20work%2C%20we%20present%20self-refining%20video%20sampling%2C%20a%20simple%20method%20that%20uses%20a%20pre-trained%20video%20generator%20trained%20on%20large-scale%20datasets%20as%20its%20own%20self-refiner.%20By%20interpreting%20the%20generator%20as%20a%20denoising%20autoencoder%2C%20we%20enable%20iterative%20inner-loop%20refinement%20at%20inference%20time%20without%20any%20external%20verifier%20or%20additional%20training.%20We%20further%20introduce%20an%20uncertainty-aware%20refinement%20strategy%20that%20selectively%20refines%20regions%20based%20on%20self-consistency%2C%20which%20prevents%20artifacts%20caused%20by%20over-refinement.%20Experiments%20on%20state-of-the-art%20video%20generators%20demonstrate%20significant%20improvements%20in%20motion%20coherence%20and%20physics%20alignment%2C%20achieving%20over%2070%5C%25%20human%20preference%20compared%20to%20the%20default%20sampler%20and%20guidance-based%20sampler.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18577v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Refining%2520Video%2520Sampling%26entry.906535625%3DSangwon%2520Jang%2520and%2520Taekyung%2520Ki%2520and%2520Jaehyeong%2520Jo%2520and%2520Saining%2520Xie%2520and%2520Jaehong%2520Yoon%2520and%2520Sung%2520Ju%2520Hwang%26entry.1292438233%3DModern%2520video%2520generators%2520still%2520struggle%2520with%2520complex%2520physical%2520dynamics%252C%2520often%2520falling%2520short%2520of%2520physical%2520realism.%2520Existing%2520approaches%2520address%2520this%2520using%2520external%2520verifiers%2520or%2520additional%2520training%2520on%2520augmented%2520data%252C%2520which%2520is%2520computationally%2520expensive%2520and%2520still%2520limited%2520in%2520capturing%2520fine-grained%2520motion.%2520In%2520this%2520work%252C%2520we%2520present%2520self-refining%2520video%2520sampling%252C%2520a%2520simple%2520method%2520that%2520uses%2520a%2520pre-trained%2520video%2520generator%2520trained%2520on%2520large-scale%2520datasets%2520as%2520its%2520own%2520self-refiner.%2520By%2520interpreting%2520the%2520generator%2520as%2520a%2520denoising%2520autoencoder%252C%2520we%2520enable%2520iterative%2520inner-loop%2520refinement%2520at%2520inference%2520time%2520without%2520any%2520external%2520verifier%2520or%2520additional%2520training.%2520We%2520further%2520introduce%2520an%2520uncertainty-aware%2520refinement%2520strategy%2520that%2520selectively%2520refines%2520regions%2520based%2520on%2520self-consistency%252C%2520which%2520prevents%2520artifacts%2520caused%2520by%2520over-refinement.%2520Experiments%2520on%2520state-of-the-art%2520video%2520generators%2520demonstrate%2520significant%2520improvements%2520in%2520motion%2520coherence%2520and%2520physics%2520alignment%252C%2520achieving%2520over%252070%255C%2525%2520human%2520preference%2520compared%2520to%2520the%2520default%2520sampler%2520and%2520guidance-based%2520sampler.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18577v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Refining%20Video%20Sampling&entry.906535625=Sangwon%20Jang%20and%20Taekyung%20Ki%20and%20Jaehyeong%20Jo%20and%20Saining%20Xie%20and%20Jaehong%20Yoon%20and%20Sung%20Ju%20Hwang&entry.1292438233=Modern%20video%20generators%20still%20struggle%20with%20complex%20physical%20dynamics%2C%20often%20falling%20short%20of%20physical%20realism.%20Existing%20approaches%20address%20this%20using%20external%20verifiers%20or%20additional%20training%20on%20augmented%20data%2C%20which%20is%20computationally%20expensive%20and%20still%20limited%20in%20capturing%20fine-grained%20motion.%20In%20this%20work%2C%20we%20present%20self-refining%20video%20sampling%2C%20a%20simple%20method%20that%20uses%20a%20pre-trained%20video%20generator%20trained%20on%20large-scale%20datasets%20as%20its%20own%20self-refiner.%20By%20interpreting%20the%20generator%20as%20a%20denoising%20autoencoder%2C%20we%20enable%20iterative%20inner-loop%20refinement%20at%20inference%20time%20without%20any%20external%20verifier%20or%20additional%20training.%20We%20further%20introduce%20an%20uncertainty-aware%20refinement%20strategy%20that%20selectively%20refines%20regions%20based%20on%20self-consistency%2C%20which%20prevents%20artifacts%20caused%20by%20over-refinement.%20Experiments%20on%20state-of-the-art%20video%20generators%20demonstrate%20significant%20improvements%20in%20motion%20coherence%20and%20physics%20alignment%2C%20achieving%20over%2070%5C%25%20human%20preference%20compared%20to%20the%20default%20sampler%20and%20guidance-based%20sampler.&entry.1838667208=http%3A//arxiv.org/abs/2601.18577v1&entry.124074799=Read"},
{"title": "DeltaDorsal: Enhancing Hand Pose Estimation with Dorsal Features in Egocentric Views", "author": "William Huang and Siyou Pei and Leyi Zou and Eric J. Gonzalez and Ishan Chatterjee and Yang Zhang", "abstract": "The proliferation of XR devices has made egocentric hand pose estimation a vital task, yet this perspective is inherently challenged by frequent finger occlusions. To address this, we propose a novel approach that leverages the rich information in dorsal hand skin deformation, unlocked by recent advances in dense visual featurizers. We introduce a dual-stream delta encoder that learns pose by contrasting features from a dynamic hand with a baseline relaxed position. Our evaluation demonstrates that, using only cropped dorsal images, our method reduces the Mean Per Joint Angle Error (MPJAE) by 18% in self-occluded scenarios (fingers >= 50% occluded) compared to state-of-the-art techniques that depend on the whole hand's geometry and large model backbones. Consequently, our method not only enhances the reliability of downstream tasks like index finger pinch and tap estimation in occluded scenarios but also unlocks new interaction paradigms, such as detecting isometric force for a surface \"click\" without visible movement while minimizing model size.", "link": "http://arxiv.org/abs/2601.15516v2", "date": "2026-01-26", "relevancy": 2.8489, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6048}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5619}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeltaDorsal%3A%20Enhancing%20Hand%20Pose%20Estimation%20with%20Dorsal%20Features%20in%20Egocentric%20Views&body=Title%3A%20DeltaDorsal%3A%20Enhancing%20Hand%20Pose%20Estimation%20with%20Dorsal%20Features%20in%20Egocentric%20Views%0AAuthor%3A%20William%20Huang%20and%20Siyou%20Pei%20and%20Leyi%20Zou%20and%20Eric%20J.%20Gonzalez%20and%20Ishan%20Chatterjee%20and%20Yang%20Zhang%0AAbstract%3A%20The%20proliferation%20of%20XR%20devices%20has%20made%20egocentric%20hand%20pose%20estimation%20a%20vital%20task%2C%20yet%20this%20perspective%20is%20inherently%20challenged%20by%20frequent%20finger%20occlusions.%20To%20address%20this%2C%20we%20propose%20a%20novel%20approach%20that%20leverages%20the%20rich%20information%20in%20dorsal%20hand%20skin%20deformation%2C%20unlocked%20by%20recent%20advances%20in%20dense%20visual%20featurizers.%20We%20introduce%20a%20dual-stream%20delta%20encoder%20that%20learns%20pose%20by%20contrasting%20features%20from%20a%20dynamic%20hand%20with%20a%20baseline%20relaxed%20position.%20Our%20evaluation%20demonstrates%20that%2C%20using%20only%20cropped%20dorsal%20images%2C%20our%20method%20reduces%20the%20Mean%20Per%20Joint%20Angle%20Error%20%28MPJAE%29%20by%2018%25%20in%20self-occluded%20scenarios%20%28fingers%20%3E%3D%2050%25%20occluded%29%20compared%20to%20state-of-the-art%20techniques%20that%20depend%20on%20the%20whole%20hand%27s%20geometry%20and%20large%20model%20backbones.%20Consequently%2C%20our%20method%20not%20only%20enhances%20the%20reliability%20of%20downstream%20tasks%20like%20index%20finger%20pinch%20and%20tap%20estimation%20in%20occluded%20scenarios%20but%20also%20unlocks%20new%20interaction%20paradigms%2C%20such%20as%20detecting%20isometric%20force%20for%20a%20surface%20%22click%22%20without%20visible%20movement%20while%20minimizing%20model%20size.%0ALink%3A%20http%3A//arxiv.org/abs/2601.15516v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeltaDorsal%253A%2520Enhancing%2520Hand%2520Pose%2520Estimation%2520with%2520Dorsal%2520Features%2520in%2520Egocentric%2520Views%26entry.906535625%3DWilliam%2520Huang%2520and%2520Siyou%2520Pei%2520and%2520Leyi%2520Zou%2520and%2520Eric%2520J.%2520Gonzalez%2520and%2520Ishan%2520Chatterjee%2520and%2520Yang%2520Zhang%26entry.1292438233%3DThe%2520proliferation%2520of%2520XR%2520devices%2520has%2520made%2520egocentric%2520hand%2520pose%2520estimation%2520a%2520vital%2520task%252C%2520yet%2520this%2520perspective%2520is%2520inherently%2520challenged%2520by%2520frequent%2520finger%2520occlusions.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520novel%2520approach%2520that%2520leverages%2520the%2520rich%2520information%2520in%2520dorsal%2520hand%2520skin%2520deformation%252C%2520unlocked%2520by%2520recent%2520advances%2520in%2520dense%2520visual%2520featurizers.%2520We%2520introduce%2520a%2520dual-stream%2520delta%2520encoder%2520that%2520learns%2520pose%2520by%2520contrasting%2520features%2520from%2520a%2520dynamic%2520hand%2520with%2520a%2520baseline%2520relaxed%2520position.%2520Our%2520evaluation%2520demonstrates%2520that%252C%2520using%2520only%2520cropped%2520dorsal%2520images%252C%2520our%2520method%2520reduces%2520the%2520Mean%2520Per%2520Joint%2520Angle%2520Error%2520%2528MPJAE%2529%2520by%252018%2525%2520in%2520self-occluded%2520scenarios%2520%2528fingers%2520%253E%253D%252050%2525%2520occluded%2529%2520compared%2520to%2520state-of-the-art%2520techniques%2520that%2520depend%2520on%2520the%2520whole%2520hand%2527s%2520geometry%2520and%2520large%2520model%2520backbones.%2520Consequently%252C%2520our%2520method%2520not%2520only%2520enhances%2520the%2520reliability%2520of%2520downstream%2520tasks%2520like%2520index%2520finger%2520pinch%2520and%2520tap%2520estimation%2520in%2520occluded%2520scenarios%2520but%2520also%2520unlocks%2520new%2520interaction%2520paradigms%252C%2520such%2520as%2520detecting%2520isometric%2520force%2520for%2520a%2520surface%2520%2522click%2522%2520without%2520visible%2520movement%2520while%2520minimizing%2520model%2520size.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.15516v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeltaDorsal%3A%20Enhancing%20Hand%20Pose%20Estimation%20with%20Dorsal%20Features%20in%20Egocentric%20Views&entry.906535625=William%20Huang%20and%20Siyou%20Pei%20and%20Leyi%20Zou%20and%20Eric%20J.%20Gonzalez%20and%20Ishan%20Chatterjee%20and%20Yang%20Zhang&entry.1292438233=The%20proliferation%20of%20XR%20devices%20has%20made%20egocentric%20hand%20pose%20estimation%20a%20vital%20task%2C%20yet%20this%20perspective%20is%20inherently%20challenged%20by%20frequent%20finger%20occlusions.%20To%20address%20this%2C%20we%20propose%20a%20novel%20approach%20that%20leverages%20the%20rich%20information%20in%20dorsal%20hand%20skin%20deformation%2C%20unlocked%20by%20recent%20advances%20in%20dense%20visual%20featurizers.%20We%20introduce%20a%20dual-stream%20delta%20encoder%20that%20learns%20pose%20by%20contrasting%20features%20from%20a%20dynamic%20hand%20with%20a%20baseline%20relaxed%20position.%20Our%20evaluation%20demonstrates%20that%2C%20using%20only%20cropped%20dorsal%20images%2C%20our%20method%20reduces%20the%20Mean%20Per%20Joint%20Angle%20Error%20%28MPJAE%29%20by%2018%25%20in%20self-occluded%20scenarios%20%28fingers%20%3E%3D%2050%25%20occluded%29%20compared%20to%20state-of-the-art%20techniques%20that%20depend%20on%20the%20whole%20hand%27s%20geometry%20and%20large%20model%20backbones.%20Consequently%2C%20our%20method%20not%20only%20enhances%20the%20reliability%20of%20downstream%20tasks%20like%20index%20finger%20pinch%20and%20tap%20estimation%20in%20occluded%20scenarios%20but%20also%20unlocks%20new%20interaction%20paradigms%2C%20such%20as%20detecting%20isometric%20force%20for%20a%20surface%20%22click%22%20without%20visible%20movement%20while%20minimizing%20model%20size.&entry.1838667208=http%3A//arxiv.org/abs/2601.15516v2&entry.124074799=Read"},
{"title": "3DGesPolicy: Phoneme-Aware Holistic Co-Speech Gesture Generation Based on Action Control", "author": "Xuanmeng Sha and Liyun Zhang and Tomohiro Mashita and Naoya Chiba and Yuki Uranishi", "abstract": "Generating holistic co-speech gestures that integrate full-body motion with facial expressions suffers from semantically incoherent coordination on body motion and spatially unstable meaningless movements due to existing part-decomposed or frame-level regression methods, We introduce 3DGesPolicy, a novel action-based framework that reformulates holistic gesture generation as a continuous trajectory control problem through diffusion policy from robotics. By modeling frame-to-frame variations as unified holistic actions, our method effectively learns inter-frame holistic gesture motion patterns and ensures both spatially and semantically coherent movement trajectories that adhere to realistic motion manifolds. To further bridge the gap in expressive alignment, we propose a Gesture-Audio-Phoneme (GAP) fusion module that can deeply integrate and refine multi-modal signals, ensuring structured and fine-grained alignment between speech semantics, body motion, and facial expressions. Extensive quantitative and qualitative experiments on the BEAT2 dataset demonstrate the effectiveness of our 3DGesPolicy across other state-of-the-art methods in generating natural, expressive, and highly speech-aligned holistic gestures.", "link": "http://arxiv.org/abs/2601.18451v1", "date": "2026-01-26", "relevancy": 2.8288, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6097}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5595}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DGesPolicy%3A%20Phoneme-Aware%20Holistic%20Co-Speech%20Gesture%20Generation%20Based%20on%20Action%20Control&body=Title%3A%203DGesPolicy%3A%20Phoneme-Aware%20Holistic%20Co-Speech%20Gesture%20Generation%20Based%20on%20Action%20Control%0AAuthor%3A%20Xuanmeng%20Sha%20and%20Liyun%20Zhang%20and%20Tomohiro%20Mashita%20and%20Naoya%20Chiba%20and%20Yuki%20Uranishi%0AAbstract%3A%20Generating%20holistic%20co-speech%20gestures%20that%20integrate%20full-body%20motion%20with%20facial%20expressions%20suffers%20from%20semantically%20incoherent%20coordination%20on%20body%20motion%20and%20spatially%20unstable%20meaningless%20movements%20due%20to%20existing%20part-decomposed%20or%20frame-level%20regression%20methods%2C%20We%20introduce%203DGesPolicy%2C%20a%20novel%20action-based%20framework%20that%20reformulates%20holistic%20gesture%20generation%20as%20a%20continuous%20trajectory%20control%20problem%20through%20diffusion%20policy%20from%20robotics.%20By%20modeling%20frame-to-frame%20variations%20as%20unified%20holistic%20actions%2C%20our%20method%20effectively%20learns%20inter-frame%20holistic%20gesture%20motion%20patterns%20and%20ensures%20both%20spatially%20and%20semantically%20coherent%20movement%20trajectories%20that%20adhere%20to%20realistic%20motion%20manifolds.%20To%20further%20bridge%20the%20gap%20in%20expressive%20alignment%2C%20we%20propose%20a%20Gesture-Audio-Phoneme%20%28GAP%29%20fusion%20module%20that%20can%20deeply%20integrate%20and%20refine%20multi-modal%20signals%2C%20ensuring%20structured%20and%20fine-grained%20alignment%20between%20speech%20semantics%2C%20body%20motion%2C%20and%20facial%20expressions.%20Extensive%20quantitative%20and%20qualitative%20experiments%20on%20the%20BEAT2%20dataset%20demonstrate%20the%20effectiveness%20of%20our%203DGesPolicy%20across%20other%20state-of-the-art%20methods%20in%20generating%20natural%2C%20expressive%2C%20and%20highly%20speech-aligned%20holistic%20gestures.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18451v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DGesPolicy%253A%2520Phoneme-Aware%2520Holistic%2520Co-Speech%2520Gesture%2520Generation%2520Based%2520on%2520Action%2520Control%26entry.906535625%3DXuanmeng%2520Sha%2520and%2520Liyun%2520Zhang%2520and%2520Tomohiro%2520Mashita%2520and%2520Naoya%2520Chiba%2520and%2520Yuki%2520Uranishi%26entry.1292438233%3DGenerating%2520holistic%2520co-speech%2520gestures%2520that%2520integrate%2520full-body%2520motion%2520with%2520facial%2520expressions%2520suffers%2520from%2520semantically%2520incoherent%2520coordination%2520on%2520body%2520motion%2520and%2520spatially%2520unstable%2520meaningless%2520movements%2520due%2520to%2520existing%2520part-decomposed%2520or%2520frame-level%2520regression%2520methods%252C%2520We%2520introduce%25203DGesPolicy%252C%2520a%2520novel%2520action-based%2520framework%2520that%2520reformulates%2520holistic%2520gesture%2520generation%2520as%2520a%2520continuous%2520trajectory%2520control%2520problem%2520through%2520diffusion%2520policy%2520from%2520robotics.%2520By%2520modeling%2520frame-to-frame%2520variations%2520as%2520unified%2520holistic%2520actions%252C%2520our%2520method%2520effectively%2520learns%2520inter-frame%2520holistic%2520gesture%2520motion%2520patterns%2520and%2520ensures%2520both%2520spatially%2520and%2520semantically%2520coherent%2520movement%2520trajectories%2520that%2520adhere%2520to%2520realistic%2520motion%2520manifolds.%2520To%2520further%2520bridge%2520the%2520gap%2520in%2520expressive%2520alignment%252C%2520we%2520propose%2520a%2520Gesture-Audio-Phoneme%2520%2528GAP%2529%2520fusion%2520module%2520that%2520can%2520deeply%2520integrate%2520and%2520refine%2520multi-modal%2520signals%252C%2520ensuring%2520structured%2520and%2520fine-grained%2520alignment%2520between%2520speech%2520semantics%252C%2520body%2520motion%252C%2520and%2520facial%2520expressions.%2520Extensive%2520quantitative%2520and%2520qualitative%2520experiments%2520on%2520the%2520BEAT2%2520dataset%2520demonstrate%2520the%2520effectiveness%2520of%2520our%25203DGesPolicy%2520across%2520other%2520state-of-the-art%2520methods%2520in%2520generating%2520natural%252C%2520expressive%252C%2520and%2520highly%2520speech-aligned%2520holistic%2520gestures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18451v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DGesPolicy%3A%20Phoneme-Aware%20Holistic%20Co-Speech%20Gesture%20Generation%20Based%20on%20Action%20Control&entry.906535625=Xuanmeng%20Sha%20and%20Liyun%20Zhang%20and%20Tomohiro%20Mashita%20and%20Naoya%20Chiba%20and%20Yuki%20Uranishi&entry.1292438233=Generating%20holistic%20co-speech%20gestures%20that%20integrate%20full-body%20motion%20with%20facial%20expressions%20suffers%20from%20semantically%20incoherent%20coordination%20on%20body%20motion%20and%20spatially%20unstable%20meaningless%20movements%20due%20to%20existing%20part-decomposed%20or%20frame-level%20regression%20methods%2C%20We%20introduce%203DGesPolicy%2C%20a%20novel%20action-based%20framework%20that%20reformulates%20holistic%20gesture%20generation%20as%20a%20continuous%20trajectory%20control%20problem%20through%20diffusion%20policy%20from%20robotics.%20By%20modeling%20frame-to-frame%20variations%20as%20unified%20holistic%20actions%2C%20our%20method%20effectively%20learns%20inter-frame%20holistic%20gesture%20motion%20patterns%20and%20ensures%20both%20spatially%20and%20semantically%20coherent%20movement%20trajectories%20that%20adhere%20to%20realistic%20motion%20manifolds.%20To%20further%20bridge%20the%20gap%20in%20expressive%20alignment%2C%20we%20propose%20a%20Gesture-Audio-Phoneme%20%28GAP%29%20fusion%20module%20that%20can%20deeply%20integrate%20and%20refine%20multi-modal%20signals%2C%20ensuring%20structured%20and%20fine-grained%20alignment%20between%20speech%20semantics%2C%20body%20motion%2C%20and%20facial%20expressions.%20Extensive%20quantitative%20and%20qualitative%20experiments%20on%20the%20BEAT2%20dataset%20demonstrate%20the%20effectiveness%20of%20our%203DGesPolicy%20across%20other%20state-of-the-art%20methods%20in%20generating%20natural%2C%20expressive%2C%20and%20highly%20speech-aligned%20holistic%20gestures.&entry.1838667208=http%3A//arxiv.org/abs/2601.18451v1&entry.124074799=Read"},
{"title": "CLIP's Visual Embedding Projector is a Few-shot Cornucopia", "author": "Mohammad Fahes and Tuan-Hung Vu and Andrei Bursuc and Patrick P\u00e9rez and Raoul de Charette", "abstract": "We introduce ProLIP, a simple and architecture-agnostic method for adapting contrastively pretrained vision-language models, such as CLIP, to few-shot classification. ProLIP fine-tunes the vision encoder's projection matrix with Frobenius norm regularization on its deviation from the pretrained weights. It achieves state-of-the-art performance on 11 few-shot classification benchmarks under both ``few-shot validation'' and ``validation-free'' settings. Moreover, by rethinking the non-linear CLIP-Adapter through ProLIP's lens, we design a Regularized Linear Adapter (RLA) that performs better, requires no hyperparameter tuning, is less sensitive to learning rate values, and offers an alternative to ProLIP in black-box scenarios where model weights are inaccessible. Beyond few-shot classification, ProLIP excels in cross-dataset transfer, domain generalization, base-to-new class generalization, and test-time adaptation--where it outperforms prompt tuning while being an order of magnitude faster to train. Code is available at https://github.com/astra-vision/ProLIP .", "link": "http://arxiv.org/abs/2410.05270v4", "date": "2026-01-26", "relevancy": 2.8103, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6053}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5404}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5404}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIP%27s%20Visual%20Embedding%20Projector%20is%20a%20Few-shot%20Cornucopia&body=Title%3A%20CLIP%27s%20Visual%20Embedding%20Projector%20is%20a%20Few-shot%20Cornucopia%0AAuthor%3A%20Mohammad%20Fahes%20and%20Tuan-Hung%20Vu%20and%20Andrei%20Bursuc%20and%20Patrick%20P%C3%A9rez%20and%20Raoul%20de%20Charette%0AAbstract%3A%20We%20introduce%20ProLIP%2C%20a%20simple%20and%20architecture-agnostic%20method%20for%20adapting%20contrastively%20pretrained%20vision-language%20models%2C%20such%20as%20CLIP%2C%20to%20few-shot%20classification.%20ProLIP%20fine-tunes%20the%20vision%20encoder%27s%20projection%20matrix%20with%20Frobenius%20norm%20regularization%20on%20its%20deviation%20from%20the%20pretrained%20weights.%20It%20achieves%20state-of-the-art%20performance%20on%2011%20few-shot%20classification%20benchmarks%20under%20both%20%60%60few-shot%20validation%27%27%20and%20%60%60validation-free%27%27%20settings.%20Moreover%2C%20by%20rethinking%20the%20non-linear%20CLIP-Adapter%20through%20ProLIP%27s%20lens%2C%20we%20design%20a%20Regularized%20Linear%20Adapter%20%28RLA%29%20that%20performs%20better%2C%20requires%20no%20hyperparameter%20tuning%2C%20is%20less%20sensitive%20to%20learning%20rate%20values%2C%20and%20offers%20an%20alternative%20to%20ProLIP%20in%20black-box%20scenarios%20where%20model%20weights%20are%20inaccessible.%20Beyond%20few-shot%20classification%2C%20ProLIP%20excels%20in%20cross-dataset%20transfer%2C%20domain%20generalization%2C%20base-to-new%20class%20generalization%2C%20and%20test-time%20adaptation--where%20it%20outperforms%20prompt%20tuning%20while%20being%20an%20order%20of%20magnitude%20faster%20to%20train.%20Code%20is%20available%20at%20https%3A//github.com/astra-vision/ProLIP%20.%0ALink%3A%20http%3A//arxiv.org/abs/2410.05270v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIP%2527s%2520Visual%2520Embedding%2520Projector%2520is%2520a%2520Few-shot%2520Cornucopia%26entry.906535625%3DMohammad%2520Fahes%2520and%2520Tuan-Hung%2520Vu%2520and%2520Andrei%2520Bursuc%2520and%2520Patrick%2520P%25C3%25A9rez%2520and%2520Raoul%2520de%2520Charette%26entry.1292438233%3DWe%2520introduce%2520ProLIP%252C%2520a%2520simple%2520and%2520architecture-agnostic%2520method%2520for%2520adapting%2520contrastively%2520pretrained%2520vision-language%2520models%252C%2520such%2520as%2520CLIP%252C%2520to%2520few-shot%2520classification.%2520ProLIP%2520fine-tunes%2520the%2520vision%2520encoder%2527s%2520projection%2520matrix%2520with%2520Frobenius%2520norm%2520regularization%2520on%2520its%2520deviation%2520from%2520the%2520pretrained%2520weights.%2520It%2520achieves%2520state-of-the-art%2520performance%2520on%252011%2520few-shot%2520classification%2520benchmarks%2520under%2520both%2520%2560%2560few-shot%2520validation%2527%2527%2520and%2520%2560%2560validation-free%2527%2527%2520settings.%2520Moreover%252C%2520by%2520rethinking%2520the%2520non-linear%2520CLIP-Adapter%2520through%2520ProLIP%2527s%2520lens%252C%2520we%2520design%2520a%2520Regularized%2520Linear%2520Adapter%2520%2528RLA%2529%2520that%2520performs%2520better%252C%2520requires%2520no%2520hyperparameter%2520tuning%252C%2520is%2520less%2520sensitive%2520to%2520learning%2520rate%2520values%252C%2520and%2520offers%2520an%2520alternative%2520to%2520ProLIP%2520in%2520black-box%2520scenarios%2520where%2520model%2520weights%2520are%2520inaccessible.%2520Beyond%2520few-shot%2520classification%252C%2520ProLIP%2520excels%2520in%2520cross-dataset%2520transfer%252C%2520domain%2520generalization%252C%2520base-to-new%2520class%2520generalization%252C%2520and%2520test-time%2520adaptation--where%2520it%2520outperforms%2520prompt%2520tuning%2520while%2520being%2520an%2520order%2520of%2520magnitude%2520faster%2520to%2520train.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/astra-vision/ProLIP%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05270v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP%27s%20Visual%20Embedding%20Projector%20is%20a%20Few-shot%20Cornucopia&entry.906535625=Mohammad%20Fahes%20and%20Tuan-Hung%20Vu%20and%20Andrei%20Bursuc%20and%20Patrick%20P%C3%A9rez%20and%20Raoul%20de%20Charette&entry.1292438233=We%20introduce%20ProLIP%2C%20a%20simple%20and%20architecture-agnostic%20method%20for%20adapting%20contrastively%20pretrained%20vision-language%20models%2C%20such%20as%20CLIP%2C%20to%20few-shot%20classification.%20ProLIP%20fine-tunes%20the%20vision%20encoder%27s%20projection%20matrix%20with%20Frobenius%20norm%20regularization%20on%20its%20deviation%20from%20the%20pretrained%20weights.%20It%20achieves%20state-of-the-art%20performance%20on%2011%20few-shot%20classification%20benchmarks%20under%20both%20%60%60few-shot%20validation%27%27%20and%20%60%60validation-free%27%27%20settings.%20Moreover%2C%20by%20rethinking%20the%20non-linear%20CLIP-Adapter%20through%20ProLIP%27s%20lens%2C%20we%20design%20a%20Regularized%20Linear%20Adapter%20%28RLA%29%20that%20performs%20better%2C%20requires%20no%20hyperparameter%20tuning%2C%20is%20less%20sensitive%20to%20learning%20rate%20values%2C%20and%20offers%20an%20alternative%20to%20ProLIP%20in%20black-box%20scenarios%20where%20model%20weights%20are%20inaccessible.%20Beyond%20few-shot%20classification%2C%20ProLIP%20excels%20in%20cross-dataset%20transfer%2C%20domain%20generalization%2C%20base-to-new%20class%20generalization%2C%20and%20test-time%20adaptation--where%20it%20outperforms%20prompt%20tuning%20while%20being%20an%20order%20of%20magnitude%20faster%20to%20train.%20Code%20is%20available%20at%20https%3A//github.com/astra-vision/ProLIP%20.&entry.1838667208=http%3A//arxiv.org/abs/2410.05270v4&entry.124074799=Read"},
{"title": "MARC: Memory-Augmented RL Token Compression for Efficient Video Understanding", "author": "Peiran Wu and Zhuorui Yu and Yunze Liu and Chi-Hao Wu and Enmin Zhou and Junxiao Shen", "abstract": "The rapid progress of large language models (LLMs) has laid the foundation for multimodal models. However, visual language models (VLMs) still face heavy computational costs when extended from images to videos due to high frame rates and long durations. Token compression is a promising solution, yet most existing training-free methods cause information loss and performance degradation. To overcome this, we propose \\textbf{Memory-Augmented Reinforcement Learning-based Token Compression (MARC)}, which integrates structured retrieval and RL-based distillation. MARC adopts a \\textit{retrieve-then-compress} strategy using a \\textbf{Visual Memory Retriever (VMR)} to select key clips and a \\textbf{Compression Group Relative Policy Optimization (C-GRPO)} framework to distil reasoning ability from a teacher to a student model. Experiments on six video benchmarks show that MARC achieves near-baseline accuracy using only one frame's tokens -- reducing visual tokens by \\textbf{95\\%}, GPU memory by \\textbf{72\\%}, and latency by \\textbf{23.9\\%}. This demonstrates its potential for efficient, real-time video understanding in resource-constrained settings such as video QA, surveillance, and autonomous driving.", "link": "http://arxiv.org/abs/2510.07915v2", "date": "2026-01-26", "relevancy": 2.7606, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5613}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5613}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MARC%3A%20Memory-Augmented%20RL%20Token%20Compression%20for%20Efficient%20Video%20Understanding&body=Title%3A%20MARC%3A%20Memory-Augmented%20RL%20Token%20Compression%20for%20Efficient%20Video%20Understanding%0AAuthor%3A%20Peiran%20Wu%20and%20Zhuorui%20Yu%20and%20Yunze%20Liu%20and%20Chi-Hao%20Wu%20and%20Enmin%20Zhou%20and%20Junxiao%20Shen%0AAbstract%3A%20The%20rapid%20progress%20of%20large%20language%20models%20%28LLMs%29%20has%20laid%20the%20foundation%20for%20multimodal%20models.%20However%2C%20visual%20language%20models%20%28VLMs%29%20still%20face%20heavy%20computational%20costs%20when%20extended%20from%20images%20to%20videos%20due%20to%20high%20frame%20rates%20and%20long%20durations.%20Token%20compression%20is%20a%20promising%20solution%2C%20yet%20most%20existing%20training-free%20methods%20cause%20information%20loss%20and%20performance%20degradation.%20To%20overcome%20this%2C%20we%20propose%20%5Ctextbf%7BMemory-Augmented%20Reinforcement%20Learning-based%20Token%20Compression%20%28MARC%29%7D%2C%20which%20integrates%20structured%20retrieval%20and%20RL-based%20distillation.%20MARC%20adopts%20a%20%5Ctextit%7Bretrieve-then-compress%7D%20strategy%20using%20a%20%5Ctextbf%7BVisual%20Memory%20Retriever%20%28VMR%29%7D%20to%20select%20key%20clips%20and%20a%20%5Ctextbf%7BCompression%20Group%20Relative%20Policy%20Optimization%20%28C-GRPO%29%7D%20framework%20to%20distil%20reasoning%20ability%20from%20a%20teacher%20to%20a%20student%20model.%20Experiments%20on%20six%20video%20benchmarks%20show%20that%20MARC%20achieves%20near-baseline%20accuracy%20using%20only%20one%20frame%27s%20tokens%20--%20reducing%20visual%20tokens%20by%20%5Ctextbf%7B95%5C%25%7D%2C%20GPU%20memory%20by%20%5Ctextbf%7B72%5C%25%7D%2C%20and%20latency%20by%20%5Ctextbf%7B23.9%5C%25%7D.%20This%20demonstrates%20its%20potential%20for%20efficient%2C%20real-time%20video%20understanding%20in%20resource-constrained%20settings%20such%20as%20video%20QA%2C%20surveillance%2C%20and%20autonomous%20driving.%0ALink%3A%20http%3A//arxiv.org/abs/2510.07915v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMARC%253A%2520Memory-Augmented%2520RL%2520Token%2520Compression%2520for%2520Efficient%2520Video%2520Understanding%26entry.906535625%3DPeiran%2520Wu%2520and%2520Zhuorui%2520Yu%2520and%2520Yunze%2520Liu%2520and%2520Chi-Hao%2520Wu%2520and%2520Enmin%2520Zhou%2520and%2520Junxiao%2520Shen%26entry.1292438233%3DThe%2520rapid%2520progress%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520laid%2520the%2520foundation%2520for%2520multimodal%2520models.%2520However%252C%2520visual%2520language%2520models%2520%2528VLMs%2529%2520still%2520face%2520heavy%2520computational%2520costs%2520when%2520extended%2520from%2520images%2520to%2520videos%2520due%2520to%2520high%2520frame%2520rates%2520and%2520long%2520durations.%2520Token%2520compression%2520is%2520a%2520promising%2520solution%252C%2520yet%2520most%2520existing%2520training-free%2520methods%2520cause%2520information%2520loss%2520and%2520performance%2520degradation.%2520To%2520overcome%2520this%252C%2520we%2520propose%2520%255Ctextbf%257BMemory-Augmented%2520Reinforcement%2520Learning-based%2520Token%2520Compression%2520%2528MARC%2529%257D%252C%2520which%2520integrates%2520structured%2520retrieval%2520and%2520RL-based%2520distillation.%2520MARC%2520adopts%2520a%2520%255Ctextit%257Bretrieve-then-compress%257D%2520strategy%2520using%2520a%2520%255Ctextbf%257BVisual%2520Memory%2520Retriever%2520%2528VMR%2529%257D%2520to%2520select%2520key%2520clips%2520and%2520a%2520%255Ctextbf%257BCompression%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528C-GRPO%2529%257D%2520framework%2520to%2520distil%2520reasoning%2520ability%2520from%2520a%2520teacher%2520to%2520a%2520student%2520model.%2520Experiments%2520on%2520six%2520video%2520benchmarks%2520show%2520that%2520MARC%2520achieves%2520near-baseline%2520accuracy%2520using%2520only%2520one%2520frame%2527s%2520tokens%2520--%2520reducing%2520visual%2520tokens%2520by%2520%255Ctextbf%257B95%255C%2525%257D%252C%2520GPU%2520memory%2520by%2520%255Ctextbf%257B72%255C%2525%257D%252C%2520and%2520latency%2520by%2520%255Ctextbf%257B23.9%255C%2525%257D.%2520This%2520demonstrates%2520its%2520potential%2520for%2520efficient%252C%2520real-time%2520video%2520understanding%2520in%2520resource-constrained%2520settings%2520such%2520as%2520video%2520QA%252C%2520surveillance%252C%2520and%2520autonomous%2520driving.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07915v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MARC%3A%20Memory-Augmented%20RL%20Token%20Compression%20for%20Efficient%20Video%20Understanding&entry.906535625=Peiran%20Wu%20and%20Zhuorui%20Yu%20and%20Yunze%20Liu%20and%20Chi-Hao%20Wu%20and%20Enmin%20Zhou%20and%20Junxiao%20Shen&entry.1292438233=The%20rapid%20progress%20of%20large%20language%20models%20%28LLMs%29%20has%20laid%20the%20foundation%20for%20multimodal%20models.%20However%2C%20visual%20language%20models%20%28VLMs%29%20still%20face%20heavy%20computational%20costs%20when%20extended%20from%20images%20to%20videos%20due%20to%20high%20frame%20rates%20and%20long%20durations.%20Token%20compression%20is%20a%20promising%20solution%2C%20yet%20most%20existing%20training-free%20methods%20cause%20information%20loss%20and%20performance%20degradation.%20To%20overcome%20this%2C%20we%20propose%20%5Ctextbf%7BMemory-Augmented%20Reinforcement%20Learning-based%20Token%20Compression%20%28MARC%29%7D%2C%20which%20integrates%20structured%20retrieval%20and%20RL-based%20distillation.%20MARC%20adopts%20a%20%5Ctextit%7Bretrieve-then-compress%7D%20strategy%20using%20a%20%5Ctextbf%7BVisual%20Memory%20Retriever%20%28VMR%29%7D%20to%20select%20key%20clips%20and%20a%20%5Ctextbf%7BCompression%20Group%20Relative%20Policy%20Optimization%20%28C-GRPO%29%7D%20framework%20to%20distil%20reasoning%20ability%20from%20a%20teacher%20to%20a%20student%20model.%20Experiments%20on%20six%20video%20benchmarks%20show%20that%20MARC%20achieves%20near-baseline%20accuracy%20using%20only%20one%20frame%27s%20tokens%20--%20reducing%20visual%20tokens%20by%20%5Ctextbf%7B95%5C%25%7D%2C%20GPU%20memory%20by%20%5Ctextbf%7B72%5C%25%7D%2C%20and%20latency%20by%20%5Ctextbf%7B23.9%5C%25%7D.%20This%20demonstrates%20its%20potential%20for%20efficient%2C%20real-time%20video%20understanding%20in%20resource-constrained%20settings%20such%20as%20video%20QA%2C%20surveillance%2C%20and%20autonomous%20driving.&entry.1838667208=http%3A//arxiv.org/abs/2510.07915v2&entry.124074799=Read"},
{"title": "DisasterInsight: A Multimodal Benchmark for Function-Aware and Grounded Disaster Assessment", "author": "Sara Tehrani and Yonghao Xu and Leif Haglund and Amanda Berg and Michael Felsberg", "abstract": "Timely interpretation of satellite imagery is critical for disaster response, yet existing vision-language benchmarks for remote sensing largely focus on coarse labels and image-level recognition, overlooking the functional understanding and instruction robustness required in real humanitarian workflows. We introduce DisasterInsight, a multimodal benchmark designed to evaluate vision-language models (VLMs) on realistic disaster analysis tasks. DisasterInsight restructures the xBD dataset into approximately 112K building-centered instances and supports instruction-diverse evaluation across multiple tasks, including building-function classification, damage-level and disaster-type classification, counting, and structured report generation aligned with humanitarian assessment guidelines.\n  To establish domain-adapted baselines, we propose DI-Chat, obtained by fine-tuning existing VLM backbones on disaster-specific instruction data using parameter-efficient Low-Rank Adaptation (LoRA). Extensive experiments on state-of-the-art generic and remote-sensing VLMs reveal substantial performance gaps across tasks, particularly in damage understanding and structured report generation. DI-Chat achieves significant improvements on damage-level and disaster-type classification as well as report generation quality, while building-function classification remains challenging for all evaluated models. DisasterInsight provides a unified benchmark for studying grounded multimodal reasoning in disaster imagery.", "link": "http://arxiv.org/abs/2601.18493v1", "date": "2026-01-26", "relevancy": 2.747, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5646}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5646}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5191}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DisasterInsight%3A%20A%20Multimodal%20Benchmark%20for%20Function-Aware%20and%20Grounded%20Disaster%20Assessment&body=Title%3A%20DisasterInsight%3A%20A%20Multimodal%20Benchmark%20for%20Function-Aware%20and%20Grounded%20Disaster%20Assessment%0AAuthor%3A%20Sara%20Tehrani%20and%20Yonghao%20Xu%20and%20Leif%20Haglund%20and%20Amanda%20Berg%20and%20Michael%20Felsberg%0AAbstract%3A%20Timely%20interpretation%20of%20satellite%20imagery%20is%20critical%20for%20disaster%20response%2C%20yet%20existing%20vision-language%20benchmarks%20for%20remote%20sensing%20largely%20focus%20on%20coarse%20labels%20and%20image-level%20recognition%2C%20overlooking%20the%20functional%20understanding%20and%20instruction%20robustness%20required%20in%20real%20humanitarian%20workflows.%20We%20introduce%20DisasterInsight%2C%20a%20multimodal%20benchmark%20designed%20to%20evaluate%20vision-language%20models%20%28VLMs%29%20on%20realistic%20disaster%20analysis%20tasks.%20DisasterInsight%20restructures%20the%20xBD%20dataset%20into%20approximately%20112K%20building-centered%20instances%20and%20supports%20instruction-diverse%20evaluation%20across%20multiple%20tasks%2C%20including%20building-function%20classification%2C%20damage-level%20and%20disaster-type%20classification%2C%20counting%2C%20and%20structured%20report%20generation%20aligned%20with%20humanitarian%20assessment%20guidelines.%0A%20%20To%20establish%20domain-adapted%20baselines%2C%20we%20propose%20DI-Chat%2C%20obtained%20by%20fine-tuning%20existing%20VLM%20backbones%20on%20disaster-specific%20instruction%20data%20using%20parameter-efficient%20Low-Rank%20Adaptation%20%28LoRA%29.%20Extensive%20experiments%20on%20state-of-the-art%20generic%20and%20remote-sensing%20VLMs%20reveal%20substantial%20performance%20gaps%20across%20tasks%2C%20particularly%20in%20damage%20understanding%20and%20structured%20report%20generation.%20DI-Chat%20achieves%20significant%20improvements%20on%20damage-level%20and%20disaster-type%20classification%20as%20well%20as%20report%20generation%20quality%2C%20while%20building-function%20classification%20remains%20challenging%20for%20all%20evaluated%20models.%20DisasterInsight%20provides%20a%20unified%20benchmark%20for%20studying%20grounded%20multimodal%20reasoning%20in%20disaster%20imagery.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18493v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisasterInsight%253A%2520A%2520Multimodal%2520Benchmark%2520for%2520Function-Aware%2520and%2520Grounded%2520Disaster%2520Assessment%26entry.906535625%3DSara%2520Tehrani%2520and%2520Yonghao%2520Xu%2520and%2520Leif%2520Haglund%2520and%2520Amanda%2520Berg%2520and%2520Michael%2520Felsberg%26entry.1292438233%3DTimely%2520interpretation%2520of%2520satellite%2520imagery%2520is%2520critical%2520for%2520disaster%2520response%252C%2520yet%2520existing%2520vision-language%2520benchmarks%2520for%2520remote%2520sensing%2520largely%2520focus%2520on%2520coarse%2520labels%2520and%2520image-level%2520recognition%252C%2520overlooking%2520the%2520functional%2520understanding%2520and%2520instruction%2520robustness%2520required%2520in%2520real%2520humanitarian%2520workflows.%2520We%2520introduce%2520DisasterInsight%252C%2520a%2520multimodal%2520benchmark%2520designed%2520to%2520evaluate%2520vision-language%2520models%2520%2528VLMs%2529%2520on%2520realistic%2520disaster%2520analysis%2520tasks.%2520DisasterInsight%2520restructures%2520the%2520xBD%2520dataset%2520into%2520approximately%2520112K%2520building-centered%2520instances%2520and%2520supports%2520instruction-diverse%2520evaluation%2520across%2520multiple%2520tasks%252C%2520including%2520building-function%2520classification%252C%2520damage-level%2520and%2520disaster-type%2520classification%252C%2520counting%252C%2520and%2520structured%2520report%2520generation%2520aligned%2520with%2520humanitarian%2520assessment%2520guidelines.%250A%2520%2520To%2520establish%2520domain-adapted%2520baselines%252C%2520we%2520propose%2520DI-Chat%252C%2520obtained%2520by%2520fine-tuning%2520existing%2520VLM%2520backbones%2520on%2520disaster-specific%2520instruction%2520data%2520using%2520parameter-efficient%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529.%2520Extensive%2520experiments%2520on%2520state-of-the-art%2520generic%2520and%2520remote-sensing%2520VLMs%2520reveal%2520substantial%2520performance%2520gaps%2520across%2520tasks%252C%2520particularly%2520in%2520damage%2520understanding%2520and%2520structured%2520report%2520generation.%2520DI-Chat%2520achieves%2520significant%2520improvements%2520on%2520damage-level%2520and%2520disaster-type%2520classification%2520as%2520well%2520as%2520report%2520generation%2520quality%252C%2520while%2520building-function%2520classification%2520remains%2520challenging%2520for%2520all%2520evaluated%2520models.%2520DisasterInsight%2520provides%2520a%2520unified%2520benchmark%2520for%2520studying%2520grounded%2520multimodal%2520reasoning%2520in%2520disaster%2520imagery.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18493v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DisasterInsight%3A%20A%20Multimodal%20Benchmark%20for%20Function-Aware%20and%20Grounded%20Disaster%20Assessment&entry.906535625=Sara%20Tehrani%20and%20Yonghao%20Xu%20and%20Leif%20Haglund%20and%20Amanda%20Berg%20and%20Michael%20Felsberg&entry.1292438233=Timely%20interpretation%20of%20satellite%20imagery%20is%20critical%20for%20disaster%20response%2C%20yet%20existing%20vision-language%20benchmarks%20for%20remote%20sensing%20largely%20focus%20on%20coarse%20labels%20and%20image-level%20recognition%2C%20overlooking%20the%20functional%20understanding%20and%20instruction%20robustness%20required%20in%20real%20humanitarian%20workflows.%20We%20introduce%20DisasterInsight%2C%20a%20multimodal%20benchmark%20designed%20to%20evaluate%20vision-language%20models%20%28VLMs%29%20on%20realistic%20disaster%20analysis%20tasks.%20DisasterInsight%20restructures%20the%20xBD%20dataset%20into%20approximately%20112K%20building-centered%20instances%20and%20supports%20instruction-diverse%20evaluation%20across%20multiple%20tasks%2C%20including%20building-function%20classification%2C%20damage-level%20and%20disaster-type%20classification%2C%20counting%2C%20and%20structured%20report%20generation%20aligned%20with%20humanitarian%20assessment%20guidelines.%0A%20%20To%20establish%20domain-adapted%20baselines%2C%20we%20propose%20DI-Chat%2C%20obtained%20by%20fine-tuning%20existing%20VLM%20backbones%20on%20disaster-specific%20instruction%20data%20using%20parameter-efficient%20Low-Rank%20Adaptation%20%28LoRA%29.%20Extensive%20experiments%20on%20state-of-the-art%20generic%20and%20remote-sensing%20VLMs%20reveal%20substantial%20performance%20gaps%20across%20tasks%2C%20particularly%20in%20damage%20understanding%20and%20structured%20report%20generation.%20DI-Chat%20achieves%20significant%20improvements%20on%20damage-level%20and%20disaster-type%20classification%20as%20well%20as%20report%20generation%20quality%2C%20while%20building-function%20classification%20remains%20challenging%20for%20all%20evaluated%20models.%20DisasterInsight%20provides%20a%20unified%20benchmark%20for%20studying%20grounded%20multimodal%20reasoning%20in%20disaster%20imagery.&entry.1838667208=http%3A//arxiv.org/abs/2601.18493v1&entry.124074799=Read"},
{"title": "daVinci-Dev: Agent-native Mid-training for Software Engineering", "author": "Ji Zeng and Dayuan Fu and Tiantian Mi and Yumin Zhuang and Yaxing Huang and Xuefeng Li and Lyumanshan Ye and Muhang Xie and Qishuo Hua and Zhen Huang and Mohan Jiang and Hanning Wang and Jifan Lin and Yang Xiao and Jie Sun and Yunze Wu and Pengfei Liu", "abstract": "Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, **agentic mid-training**-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is **agent-native data**-supervision comprising two complementary types of trajectories: **contextually-native trajectories** that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and **environmentally-native trajectories** collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model's agentic capabilities on `SWE-Bench Verified`. We demonstrate our superiority over the previous open software engineering mid-training recipe `Kimi-Dev` under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve **56.1%** and **58.5%** resolution rates, respectively, which are ...", "link": "http://arxiv.org/abs/2601.18418v1", "date": "2026-01-26", "relevancy": 2.7365, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.6154}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5216}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20daVinci-Dev%3A%20Agent-native%20Mid-training%20for%20Software%20Engineering&body=Title%3A%20daVinci-Dev%3A%20Agent-native%20Mid-training%20for%20Software%20Engineering%0AAuthor%3A%20Ji%20Zeng%20and%20Dayuan%20Fu%20and%20Tiantian%20Mi%20and%20Yumin%20Zhuang%20and%20Yaxing%20Huang%20and%20Xuefeng%20Li%20and%20Lyumanshan%20Ye%20and%20Muhang%20Xie%20and%20Qishuo%20Hua%20and%20Zhen%20Huang%20and%20Mohan%20Jiang%20and%20Hanning%20Wang%20and%20Jifan%20Lin%20and%20Yang%20Xiao%20and%20Jie%20Sun%20and%20Yunze%20Wu%20and%20Pengfei%20Liu%0AAbstract%3A%20Recently%2C%20the%20frontier%20of%20Large%20Language%20Model%20%28LLM%29%20capabilities%20has%20shifted%20from%20single-turn%20code%20generation%20to%20agentic%20software%20engineering-a%20paradigm%20where%20models%20autonomously%20navigate%2C%20edit%2C%20and%20test%20complex%20repositories.%20While%20post-training%20methods%20have%20become%20the%20de%20facto%20approach%20for%20code%20agents%2C%20%2A%2Aagentic%20mid-training%2A%2A-mid-training%20%28MT%29%20on%20large-scale%20data%20that%20mirrors%20authentic%20agentic%20workflows-remains%20critically%20underexplored%20due%20to%20substantial%20resource%20requirements%2C%20despite%20offering%20a%20more%20scalable%20path%20to%20instilling%20foundational%20agentic%20behaviors%20than%20relying%20solely%20on%20expensive%20reinforcement%20learning.%20A%20central%20challenge%20in%20realizing%20effective%20agentic%20mid-training%20is%20the%20distribution%20mismatch%20between%20static%20training%20data%20and%20the%20dynamic%2C%20feedback-rich%20environment%20of%20real%20development.%20To%20address%20this%2C%20we%20present%20a%20systematic%20study%20of%20agentic%20mid-training%2C%20establishing%20both%20the%20data%20synthesis%20principles%20and%20training%20methodology%20for%20effective%20agent%20development%20at%20scale.%20Central%20to%20our%20approach%20is%20%2A%2Aagent-native%20data%2A%2A-supervision%20comprising%20two%20complementary%20types%20of%20trajectories%3A%20%2A%2Acontextually-native%20trajectories%2A%2A%20that%20preserve%20the%20complete%20information%20flow%20an%20agent%20experiences%2C%20offering%20broad%20coverage%20and%20diversity%3B%20and%20%2A%2Aenvironmentally-native%20trajectories%2A%2A%20collected%20from%20executable%20repositories%20where%20observations%20stem%20from%20actual%20tool%20invocations%20and%20test%20executions%2C%20providing%20depth%20and%20interaction%20authenticity.%20We%20verify%20the%20model%27s%20agentic%20capabilities%20on%20%60SWE-Bench%20Verified%60.%20We%20demonstrate%20our%20superiority%20over%20the%20previous%20open%20software%20engineering%20mid-training%20recipe%20%60Kimi-Dev%60%20under%20two%20post-training%20settings%20with%20an%20aligned%20base%20model%20and%20agentic%20scaffold%2C%20while%20using%20less%20than%20half%20mid-training%20tokens%20%2873.1B%29.%20Besides%20relative%20advantage%2C%20our%20best%20performing%2032B%20and%2072B%20models%20achieve%20%2A%2A56.1%25%2A%2A%20and%20%2A%2A58.5%25%2A%2A%20resolution%20rates%2C%20respectively%2C%20which%20are%20...%0ALink%3A%20http%3A//arxiv.org/abs/2601.18418v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DdaVinci-Dev%253A%2520Agent-native%2520Mid-training%2520for%2520Software%2520Engineering%26entry.906535625%3DJi%2520Zeng%2520and%2520Dayuan%2520Fu%2520and%2520Tiantian%2520Mi%2520and%2520Yumin%2520Zhuang%2520and%2520Yaxing%2520Huang%2520and%2520Xuefeng%2520Li%2520and%2520Lyumanshan%2520Ye%2520and%2520Muhang%2520Xie%2520and%2520Qishuo%2520Hua%2520and%2520Zhen%2520Huang%2520and%2520Mohan%2520Jiang%2520and%2520Hanning%2520Wang%2520and%2520Jifan%2520Lin%2520and%2520Yang%2520Xiao%2520and%2520Jie%2520Sun%2520and%2520Yunze%2520Wu%2520and%2520Pengfei%2520Liu%26entry.1292438233%3DRecently%252C%2520the%2520frontier%2520of%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520capabilities%2520has%2520shifted%2520from%2520single-turn%2520code%2520generation%2520to%2520agentic%2520software%2520engineering-a%2520paradigm%2520where%2520models%2520autonomously%2520navigate%252C%2520edit%252C%2520and%2520test%2520complex%2520repositories.%2520While%2520post-training%2520methods%2520have%2520become%2520the%2520de%2520facto%2520approach%2520for%2520code%2520agents%252C%2520%252A%252Aagentic%2520mid-training%252A%252A-mid-training%2520%2528MT%2529%2520on%2520large-scale%2520data%2520that%2520mirrors%2520authentic%2520agentic%2520workflows-remains%2520critically%2520underexplored%2520due%2520to%2520substantial%2520resource%2520requirements%252C%2520despite%2520offering%2520a%2520more%2520scalable%2520path%2520to%2520instilling%2520foundational%2520agentic%2520behaviors%2520than%2520relying%2520solely%2520on%2520expensive%2520reinforcement%2520learning.%2520A%2520central%2520challenge%2520in%2520realizing%2520effective%2520agentic%2520mid-training%2520is%2520the%2520distribution%2520mismatch%2520between%2520static%2520training%2520data%2520and%2520the%2520dynamic%252C%2520feedback-rich%2520environment%2520of%2520real%2520development.%2520To%2520address%2520this%252C%2520we%2520present%2520a%2520systematic%2520study%2520of%2520agentic%2520mid-training%252C%2520establishing%2520both%2520the%2520data%2520synthesis%2520principles%2520and%2520training%2520methodology%2520for%2520effective%2520agent%2520development%2520at%2520scale.%2520Central%2520to%2520our%2520approach%2520is%2520%252A%252Aagent-native%2520data%252A%252A-supervision%2520comprising%2520two%2520complementary%2520types%2520of%2520trajectories%253A%2520%252A%252Acontextually-native%2520trajectories%252A%252A%2520that%2520preserve%2520the%2520complete%2520information%2520flow%2520an%2520agent%2520experiences%252C%2520offering%2520broad%2520coverage%2520and%2520diversity%253B%2520and%2520%252A%252Aenvironmentally-native%2520trajectories%252A%252A%2520collected%2520from%2520executable%2520repositories%2520where%2520observations%2520stem%2520from%2520actual%2520tool%2520invocations%2520and%2520test%2520executions%252C%2520providing%2520depth%2520and%2520interaction%2520authenticity.%2520We%2520verify%2520the%2520model%2527s%2520agentic%2520capabilities%2520on%2520%2560SWE-Bench%2520Verified%2560.%2520We%2520demonstrate%2520our%2520superiority%2520over%2520the%2520previous%2520open%2520software%2520engineering%2520mid-training%2520recipe%2520%2560Kimi-Dev%2560%2520under%2520two%2520post-training%2520settings%2520with%2520an%2520aligned%2520base%2520model%2520and%2520agentic%2520scaffold%252C%2520while%2520using%2520less%2520than%2520half%2520mid-training%2520tokens%2520%252873.1B%2529.%2520Besides%2520relative%2520advantage%252C%2520our%2520best%2520performing%252032B%2520and%252072B%2520models%2520achieve%2520%252A%252A56.1%2525%252A%252A%2520and%2520%252A%252A58.5%2525%252A%252A%2520resolution%2520rates%252C%2520respectively%252C%2520which%2520are%2520...%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18418v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=daVinci-Dev%3A%20Agent-native%20Mid-training%20for%20Software%20Engineering&entry.906535625=Ji%20Zeng%20and%20Dayuan%20Fu%20and%20Tiantian%20Mi%20and%20Yumin%20Zhuang%20and%20Yaxing%20Huang%20and%20Xuefeng%20Li%20and%20Lyumanshan%20Ye%20and%20Muhang%20Xie%20and%20Qishuo%20Hua%20and%20Zhen%20Huang%20and%20Mohan%20Jiang%20and%20Hanning%20Wang%20and%20Jifan%20Lin%20and%20Yang%20Xiao%20and%20Jie%20Sun%20and%20Yunze%20Wu%20and%20Pengfei%20Liu&entry.1292438233=Recently%2C%20the%20frontier%20of%20Large%20Language%20Model%20%28LLM%29%20capabilities%20has%20shifted%20from%20single-turn%20code%20generation%20to%20agentic%20software%20engineering-a%20paradigm%20where%20models%20autonomously%20navigate%2C%20edit%2C%20and%20test%20complex%20repositories.%20While%20post-training%20methods%20have%20become%20the%20de%20facto%20approach%20for%20code%20agents%2C%20%2A%2Aagentic%20mid-training%2A%2A-mid-training%20%28MT%29%20on%20large-scale%20data%20that%20mirrors%20authentic%20agentic%20workflows-remains%20critically%20underexplored%20due%20to%20substantial%20resource%20requirements%2C%20despite%20offering%20a%20more%20scalable%20path%20to%20instilling%20foundational%20agentic%20behaviors%20than%20relying%20solely%20on%20expensive%20reinforcement%20learning.%20A%20central%20challenge%20in%20realizing%20effective%20agentic%20mid-training%20is%20the%20distribution%20mismatch%20between%20static%20training%20data%20and%20the%20dynamic%2C%20feedback-rich%20environment%20of%20real%20development.%20To%20address%20this%2C%20we%20present%20a%20systematic%20study%20of%20agentic%20mid-training%2C%20establishing%20both%20the%20data%20synthesis%20principles%20and%20training%20methodology%20for%20effective%20agent%20development%20at%20scale.%20Central%20to%20our%20approach%20is%20%2A%2Aagent-native%20data%2A%2A-supervision%20comprising%20two%20complementary%20types%20of%20trajectories%3A%20%2A%2Acontextually-native%20trajectories%2A%2A%20that%20preserve%20the%20complete%20information%20flow%20an%20agent%20experiences%2C%20offering%20broad%20coverage%20and%20diversity%3B%20and%20%2A%2Aenvironmentally-native%20trajectories%2A%2A%20collected%20from%20executable%20repositories%20where%20observations%20stem%20from%20actual%20tool%20invocations%20and%20test%20executions%2C%20providing%20depth%20and%20interaction%20authenticity.%20We%20verify%20the%20model%27s%20agentic%20capabilities%20on%20%60SWE-Bench%20Verified%60.%20We%20demonstrate%20our%20superiority%20over%20the%20previous%20open%20software%20engineering%20mid-training%20recipe%20%60Kimi-Dev%60%20under%20two%20post-training%20settings%20with%20an%20aligned%20base%20model%20and%20agentic%20scaffold%2C%20while%20using%20less%20than%20half%20mid-training%20tokens%20%2873.1B%29.%20Besides%20relative%20advantage%2C%20our%20best%20performing%2032B%20and%2072B%20models%20achieve%20%2A%2A56.1%25%2A%2A%20and%20%2A%2A58.5%25%2A%2A%20resolution%20rates%2C%20respectively%2C%20which%20are%20...&entry.1838667208=http%3A//arxiv.org/abs/2601.18418v1&entry.124074799=Read"},
{"title": "MultiHateLoc: Towards Temporal Localisation of Multimodal Hate Content in Online Videos", "author": "Qiyue Sun and Tailin Chen and Yinghui Zhang and Yuchen Zhang and Jiangbei Yue and Jianbo Jiao and Zeyu Fu", "abstract": "The rapid growth of video content on platforms such as TikTok and YouTube has intensified the spread of multimodal hate speech, where harmful cues emerge subtly and asynchronously across visual, acoustic, and textual streams. Existing research primarily focuses on video-level classification, leaving the practically crucial task of temporal localisation, identifying when hateful segments occur, largely unaddressed. This challenge is even more noticeable under weak supervision, where only video-level labels are available, and static fusion or classification-based architectures struggle to capture cross-modal and temporal dynamics. To address these challenges, we propose MultiHateLoc, the first framework designed for weakly-supervised multimodal hate localisation. MultiHateLoc incorporates (1) modality-aware temporal encoders to model heterogeneous sequential patterns, including a tailored text-based preprocessing module for feature enhancement; (2) dynamic cross-modal fusion to adaptively emphasise the most informative modality at each moment and a cross-modal contrastive alignment strategy to enhance multimodal feature consistency; (3) a modality-aware MIL objective to identify discriminative segments under video-level supervision. Despite relying solely on coarse labels, MultiHateLoc produces fine-grained, interpretable frame-level predictions. Experiments on HateMM and MultiHateClip show that our method achieves state-of-the-art performance in the localisation task.", "link": "http://arxiv.org/abs/2512.10408v2", "date": "2026-01-26", "relevancy": 2.7277, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5551}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5531}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiHateLoc%3A%20Towards%20Temporal%20Localisation%20of%20Multimodal%20Hate%20Content%20in%20Online%20Videos&body=Title%3A%20MultiHateLoc%3A%20Towards%20Temporal%20Localisation%20of%20Multimodal%20Hate%20Content%20in%20Online%20Videos%0AAuthor%3A%20Qiyue%20Sun%20and%20Tailin%20Chen%20and%20Yinghui%20Zhang%20and%20Yuchen%20Zhang%20and%20Jiangbei%20Yue%20and%20Jianbo%20Jiao%20and%20Zeyu%20Fu%0AAbstract%3A%20The%20rapid%20growth%20of%20video%20content%20on%20platforms%20such%20as%20TikTok%20and%20YouTube%20has%20intensified%20the%20spread%20of%20multimodal%20hate%20speech%2C%20where%20harmful%20cues%20emerge%20subtly%20and%20asynchronously%20across%20visual%2C%20acoustic%2C%20and%20textual%20streams.%20Existing%20research%20primarily%20focuses%20on%20video-level%20classification%2C%20leaving%20the%20practically%20crucial%20task%20of%20temporal%20localisation%2C%20identifying%20when%20hateful%20segments%20occur%2C%20largely%20unaddressed.%20This%20challenge%20is%20even%20more%20noticeable%20under%20weak%20supervision%2C%20where%20only%20video-level%20labels%20are%20available%2C%20and%20static%20fusion%20or%20classification-based%20architectures%20struggle%20to%20capture%20cross-modal%20and%20temporal%20dynamics.%20To%20address%20these%20challenges%2C%20we%20propose%20MultiHateLoc%2C%20the%20first%20framework%20designed%20for%20weakly-supervised%20multimodal%20hate%20localisation.%20MultiHateLoc%20incorporates%20%281%29%20modality-aware%20temporal%20encoders%20to%20model%20heterogeneous%20sequential%20patterns%2C%20including%20a%20tailored%20text-based%20preprocessing%20module%20for%20feature%20enhancement%3B%20%282%29%20dynamic%20cross-modal%20fusion%20to%20adaptively%20emphasise%20the%20most%20informative%20modality%20at%20each%20moment%20and%20a%20cross-modal%20contrastive%20alignment%20strategy%20to%20enhance%20multimodal%20feature%20consistency%3B%20%283%29%20a%20modality-aware%20MIL%20objective%20to%20identify%20discriminative%20segments%20under%20video-level%20supervision.%20Despite%20relying%20solely%20on%20coarse%20labels%2C%20MultiHateLoc%20produces%20fine-grained%2C%20interpretable%20frame-level%20predictions.%20Experiments%20on%20HateMM%20and%20MultiHateClip%20show%20that%20our%20method%20achieves%20state-of-the-art%20performance%20in%20the%20localisation%20task.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10408v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiHateLoc%253A%2520Towards%2520Temporal%2520Localisation%2520of%2520Multimodal%2520Hate%2520Content%2520in%2520Online%2520Videos%26entry.906535625%3DQiyue%2520Sun%2520and%2520Tailin%2520Chen%2520and%2520Yinghui%2520Zhang%2520and%2520Yuchen%2520Zhang%2520and%2520Jiangbei%2520Yue%2520and%2520Jianbo%2520Jiao%2520and%2520Zeyu%2520Fu%26entry.1292438233%3DThe%2520rapid%2520growth%2520of%2520video%2520content%2520on%2520platforms%2520such%2520as%2520TikTok%2520and%2520YouTube%2520has%2520intensified%2520the%2520spread%2520of%2520multimodal%2520hate%2520speech%252C%2520where%2520harmful%2520cues%2520emerge%2520subtly%2520and%2520asynchronously%2520across%2520visual%252C%2520acoustic%252C%2520and%2520textual%2520streams.%2520Existing%2520research%2520primarily%2520focuses%2520on%2520video-level%2520classification%252C%2520leaving%2520the%2520practically%2520crucial%2520task%2520of%2520temporal%2520localisation%252C%2520identifying%2520when%2520hateful%2520segments%2520occur%252C%2520largely%2520unaddressed.%2520This%2520challenge%2520is%2520even%2520more%2520noticeable%2520under%2520weak%2520supervision%252C%2520where%2520only%2520video-level%2520labels%2520are%2520available%252C%2520and%2520static%2520fusion%2520or%2520classification-based%2520architectures%2520struggle%2520to%2520capture%2520cross-modal%2520and%2520temporal%2520dynamics.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520MultiHateLoc%252C%2520the%2520first%2520framework%2520designed%2520for%2520weakly-supervised%2520multimodal%2520hate%2520localisation.%2520MultiHateLoc%2520incorporates%2520%25281%2529%2520modality-aware%2520temporal%2520encoders%2520to%2520model%2520heterogeneous%2520sequential%2520patterns%252C%2520including%2520a%2520tailored%2520text-based%2520preprocessing%2520module%2520for%2520feature%2520enhancement%253B%2520%25282%2529%2520dynamic%2520cross-modal%2520fusion%2520to%2520adaptively%2520emphasise%2520the%2520most%2520informative%2520modality%2520at%2520each%2520moment%2520and%2520a%2520cross-modal%2520contrastive%2520alignment%2520strategy%2520to%2520enhance%2520multimodal%2520feature%2520consistency%253B%2520%25283%2529%2520a%2520modality-aware%2520MIL%2520objective%2520to%2520identify%2520discriminative%2520segments%2520under%2520video-level%2520supervision.%2520Despite%2520relying%2520solely%2520on%2520coarse%2520labels%252C%2520MultiHateLoc%2520produces%2520fine-grained%252C%2520interpretable%2520frame-level%2520predictions.%2520Experiments%2520on%2520HateMM%2520and%2520MultiHateClip%2520show%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance%2520in%2520the%2520localisation%2520task.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10408v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiHateLoc%3A%20Towards%20Temporal%20Localisation%20of%20Multimodal%20Hate%20Content%20in%20Online%20Videos&entry.906535625=Qiyue%20Sun%20and%20Tailin%20Chen%20and%20Yinghui%20Zhang%20and%20Yuchen%20Zhang%20and%20Jiangbei%20Yue%20and%20Jianbo%20Jiao%20and%20Zeyu%20Fu&entry.1292438233=The%20rapid%20growth%20of%20video%20content%20on%20platforms%20such%20as%20TikTok%20and%20YouTube%20has%20intensified%20the%20spread%20of%20multimodal%20hate%20speech%2C%20where%20harmful%20cues%20emerge%20subtly%20and%20asynchronously%20across%20visual%2C%20acoustic%2C%20and%20textual%20streams.%20Existing%20research%20primarily%20focuses%20on%20video-level%20classification%2C%20leaving%20the%20practically%20crucial%20task%20of%20temporal%20localisation%2C%20identifying%20when%20hateful%20segments%20occur%2C%20largely%20unaddressed.%20This%20challenge%20is%20even%20more%20noticeable%20under%20weak%20supervision%2C%20where%20only%20video-level%20labels%20are%20available%2C%20and%20static%20fusion%20or%20classification-based%20architectures%20struggle%20to%20capture%20cross-modal%20and%20temporal%20dynamics.%20To%20address%20these%20challenges%2C%20we%20propose%20MultiHateLoc%2C%20the%20first%20framework%20designed%20for%20weakly-supervised%20multimodal%20hate%20localisation.%20MultiHateLoc%20incorporates%20%281%29%20modality-aware%20temporal%20encoders%20to%20model%20heterogeneous%20sequential%20patterns%2C%20including%20a%20tailored%20text-based%20preprocessing%20module%20for%20feature%20enhancement%3B%20%282%29%20dynamic%20cross-modal%20fusion%20to%20adaptively%20emphasise%20the%20most%20informative%20modality%20at%20each%20moment%20and%20a%20cross-modal%20contrastive%20alignment%20strategy%20to%20enhance%20multimodal%20feature%20consistency%3B%20%283%29%20a%20modality-aware%20MIL%20objective%20to%20identify%20discriminative%20segments%20under%20video-level%20supervision.%20Despite%20relying%20solely%20on%20coarse%20labels%2C%20MultiHateLoc%20produces%20fine-grained%2C%20interpretable%20frame-level%20predictions.%20Experiments%20on%20HateMM%20and%20MultiHateClip%20show%20that%20our%20method%20achieves%20state-of-the-art%20performance%20in%20the%20localisation%20task.&entry.1838667208=http%3A//arxiv.org/abs/2512.10408v2&entry.124074799=Read"},
{"title": "ELIP: Efficient Discriminative Language-Image Pre-training with Fewer Vision Tokens", "author": "Yangyang Guo and Haoyu Zhang and Yongkang Wong and Liqiang Nie and Mohan Kankanhalli", "abstract": "Learning a versatile language-image model is computationally prohibitive under a limited computing budget. This paper delves into the \\emph{efficient language-image pre-training}, an area that has received relatively little attention despite its importance in reducing computational cost and footprint. To that end, we propose a vision token pruning and merging method ELIP, to remove less influential tokens based on the supervision of language outputs. Our method is designed with several strengths, such as being computation-efficient, memory-efficient, and trainable-parameter-free, and is distinguished from previous vision-only token pruning approaches by its alignment with task objectives. We implement this method in a progressively pruning manner using several sequential blocks. To evaluate its generalization performance, we apply ELIP to three commonly used language-image pre-training models and utilize public image-caption pairs with 4M images for pre-training. Our experiments demonstrate that with the removal of ~30$\\%$ vision tokens across 12 ViT layers, ELIP maintains significantly comparable performance with baselines ($\\sim$0.32 accuracy drop on average) over various downstream tasks including cross-modal retrieval, VQA, image captioning, \\emph{etc}. In addition, the spared GPU resources by our ELIP allow us to scale up with larger batch sizes, thereby accelerating model pre-training and even sometimes enhancing downstream model performance.", "link": "http://arxiv.org/abs/2309.16738v3", "date": "2026-01-26", "relevancy": 2.7114, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5571}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5349}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ELIP%3A%20Efficient%20Discriminative%20Language-Image%20Pre-training%20with%20Fewer%20Vision%20Tokens&body=Title%3A%20ELIP%3A%20Efficient%20Discriminative%20Language-Image%20Pre-training%20with%20Fewer%20Vision%20Tokens%0AAuthor%3A%20Yangyang%20Guo%20and%20Haoyu%20Zhang%20and%20Yongkang%20Wong%20and%20Liqiang%20Nie%20and%20Mohan%20Kankanhalli%0AAbstract%3A%20Learning%20a%20versatile%20language-image%20model%20is%20computationally%20prohibitive%20under%20a%20limited%20computing%20budget.%20This%20paper%20delves%20into%20the%20%5Cemph%7Befficient%20language-image%20pre-training%7D%2C%20an%20area%20that%20has%20received%20relatively%20little%20attention%20despite%20its%20importance%20in%20reducing%20computational%20cost%20and%20footprint.%20To%20that%20end%2C%20we%20propose%20a%20vision%20token%20pruning%20and%20merging%20method%20ELIP%2C%20to%20remove%20less%20influential%20tokens%20based%20on%20the%20supervision%20of%20language%20outputs.%20Our%20method%20is%20designed%20with%20several%20strengths%2C%20such%20as%20being%20computation-efficient%2C%20memory-efficient%2C%20and%20trainable-parameter-free%2C%20and%20is%20distinguished%20from%20previous%20vision-only%20token%20pruning%20approaches%20by%20its%20alignment%20with%20task%20objectives.%20We%20implement%20this%20method%20in%20a%20progressively%20pruning%20manner%20using%20several%20sequential%20blocks.%20To%20evaluate%20its%20generalization%20performance%2C%20we%20apply%20ELIP%20to%20three%20commonly%20used%20language-image%20pre-training%20models%20and%20utilize%20public%20image-caption%20pairs%20with%204M%20images%20for%20pre-training.%20Our%20experiments%20demonstrate%20that%20with%20the%20removal%20of%20~30%24%5C%25%24%20vision%20tokens%20across%2012%20ViT%20layers%2C%20ELIP%20maintains%20significantly%20comparable%20performance%20with%20baselines%20%28%24%5Csim%240.32%20accuracy%20drop%20on%20average%29%20over%20various%20downstream%20tasks%20including%20cross-modal%20retrieval%2C%20VQA%2C%20image%20captioning%2C%20%5Cemph%7Betc%7D.%20In%20addition%2C%20the%20spared%20GPU%20resources%20by%20our%20ELIP%20allow%20us%20to%20scale%20up%20with%20larger%20batch%20sizes%2C%20thereby%20accelerating%20model%20pre-training%20and%20even%20sometimes%20enhancing%20downstream%20model%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2309.16738v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DELIP%253A%2520Efficient%2520Discriminative%2520Language-Image%2520Pre-training%2520with%2520Fewer%2520Vision%2520Tokens%26entry.906535625%3DYangyang%2520Guo%2520and%2520Haoyu%2520Zhang%2520and%2520Yongkang%2520Wong%2520and%2520Liqiang%2520Nie%2520and%2520Mohan%2520Kankanhalli%26entry.1292438233%3DLearning%2520a%2520versatile%2520language-image%2520model%2520is%2520computationally%2520prohibitive%2520under%2520a%2520limited%2520computing%2520budget.%2520This%2520paper%2520delves%2520into%2520the%2520%255Cemph%257Befficient%2520language-image%2520pre-training%257D%252C%2520an%2520area%2520that%2520has%2520received%2520relatively%2520little%2520attention%2520despite%2520its%2520importance%2520in%2520reducing%2520computational%2520cost%2520and%2520footprint.%2520To%2520that%2520end%252C%2520we%2520propose%2520a%2520vision%2520token%2520pruning%2520and%2520merging%2520method%2520ELIP%252C%2520to%2520remove%2520less%2520influential%2520tokens%2520based%2520on%2520the%2520supervision%2520of%2520language%2520outputs.%2520Our%2520method%2520is%2520designed%2520with%2520several%2520strengths%252C%2520such%2520as%2520being%2520computation-efficient%252C%2520memory-efficient%252C%2520and%2520trainable-parameter-free%252C%2520and%2520is%2520distinguished%2520from%2520previous%2520vision-only%2520token%2520pruning%2520approaches%2520by%2520its%2520alignment%2520with%2520task%2520objectives.%2520We%2520implement%2520this%2520method%2520in%2520a%2520progressively%2520pruning%2520manner%2520using%2520several%2520sequential%2520blocks.%2520To%2520evaluate%2520its%2520generalization%2520performance%252C%2520we%2520apply%2520ELIP%2520to%2520three%2520commonly%2520used%2520language-image%2520pre-training%2520models%2520and%2520utilize%2520public%2520image-caption%2520pairs%2520with%25204M%2520images%2520for%2520pre-training.%2520Our%2520experiments%2520demonstrate%2520that%2520with%2520the%2520removal%2520of%2520~30%2524%255C%2525%2524%2520vision%2520tokens%2520across%252012%2520ViT%2520layers%252C%2520ELIP%2520maintains%2520significantly%2520comparable%2520performance%2520with%2520baselines%2520%2528%2524%255Csim%25240.32%2520accuracy%2520drop%2520on%2520average%2529%2520over%2520various%2520downstream%2520tasks%2520including%2520cross-modal%2520retrieval%252C%2520VQA%252C%2520image%2520captioning%252C%2520%255Cemph%257Betc%257D.%2520In%2520addition%252C%2520the%2520spared%2520GPU%2520resources%2520by%2520our%2520ELIP%2520allow%2520us%2520to%2520scale%2520up%2520with%2520larger%2520batch%2520sizes%252C%2520thereby%2520accelerating%2520model%2520pre-training%2520and%2520even%2520sometimes%2520enhancing%2520downstream%2520model%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.16738v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ELIP%3A%20Efficient%20Discriminative%20Language-Image%20Pre-training%20with%20Fewer%20Vision%20Tokens&entry.906535625=Yangyang%20Guo%20and%20Haoyu%20Zhang%20and%20Yongkang%20Wong%20and%20Liqiang%20Nie%20and%20Mohan%20Kankanhalli&entry.1292438233=Learning%20a%20versatile%20language-image%20model%20is%20computationally%20prohibitive%20under%20a%20limited%20computing%20budget.%20This%20paper%20delves%20into%20the%20%5Cemph%7Befficient%20language-image%20pre-training%7D%2C%20an%20area%20that%20has%20received%20relatively%20little%20attention%20despite%20its%20importance%20in%20reducing%20computational%20cost%20and%20footprint.%20To%20that%20end%2C%20we%20propose%20a%20vision%20token%20pruning%20and%20merging%20method%20ELIP%2C%20to%20remove%20less%20influential%20tokens%20based%20on%20the%20supervision%20of%20language%20outputs.%20Our%20method%20is%20designed%20with%20several%20strengths%2C%20such%20as%20being%20computation-efficient%2C%20memory-efficient%2C%20and%20trainable-parameter-free%2C%20and%20is%20distinguished%20from%20previous%20vision-only%20token%20pruning%20approaches%20by%20its%20alignment%20with%20task%20objectives.%20We%20implement%20this%20method%20in%20a%20progressively%20pruning%20manner%20using%20several%20sequential%20blocks.%20To%20evaluate%20its%20generalization%20performance%2C%20we%20apply%20ELIP%20to%20three%20commonly%20used%20language-image%20pre-training%20models%20and%20utilize%20public%20image-caption%20pairs%20with%204M%20images%20for%20pre-training.%20Our%20experiments%20demonstrate%20that%20with%20the%20removal%20of%20~30%24%5C%25%24%20vision%20tokens%20across%2012%20ViT%20layers%2C%20ELIP%20maintains%20significantly%20comparable%20performance%20with%20baselines%20%28%24%5Csim%240.32%20accuracy%20drop%20on%20average%29%20over%20various%20downstream%20tasks%20including%20cross-modal%20retrieval%2C%20VQA%2C%20image%20captioning%2C%20%5Cemph%7Betc%7D.%20In%20addition%2C%20the%20spared%20GPU%20resources%20by%20our%20ELIP%20allow%20us%20to%20scale%20up%20with%20larger%20batch%20sizes%2C%20thereby%20accelerating%20model%20pre-training%20and%20even%20sometimes%20enhancing%20downstream%20model%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2309.16738v3&entry.124074799=Read"},
{"title": "On Procrustes Contamination in Machine Learning Applications of Geometric Morphometrics", "author": "Lloyd Austin Courtenay", "abstract": "Geometric morphometrics (GMM) is widely used to quantify shape variation, more recently serving as input for machine learning (ML) analyses. Standard practice aligns all specimens via Generalized Procrustes Analysis (GPA) prior to splitting data into training and test sets, potentially introducing statistical dependence and contaminating downstream predictive models. Here, the effects of GPA-induced contamination are formally characterised using controlled 2D and 3D simulations across varying sample sizes, landmark densities, and allometric patterns. A novel realignment procedure is proposed, whereby test specimens are aligned to the training set prior to model fitting, eliminating cross-sample dependency. Simulations reveal a robust \"diagonal\" in sample-size vs. landmark-space, reflecting the scaling of RMSE under isotropic variation, with slopes analytically derived from the degrees of freedom in Procrustes tangent space. The importance of spatial autocorrelation among landmarks is further demonstrated using linear and convolutional regression models, highlighting performance degradation when landmark relationships are ignored. This work establishes the need for careful preprocessing in ML applications of GMM, provides practical guidelines for realignment, and clarifies fundamental statistical constraints inherent to Procrustes shape space.", "link": "http://arxiv.org/abs/2601.18448v1", "date": "2026-01-26", "relevancy": 2.6702, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.563}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5341}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Procrustes%20Contamination%20in%20Machine%20Learning%20Applications%20of%20Geometric%20Morphometrics&body=Title%3A%20On%20Procrustes%20Contamination%20in%20Machine%20Learning%20Applications%20of%20Geometric%20Morphometrics%0AAuthor%3A%20Lloyd%20Austin%20Courtenay%0AAbstract%3A%20Geometric%20morphometrics%20%28GMM%29%20is%20widely%20used%20to%20quantify%20shape%20variation%2C%20more%20recently%20serving%20as%20input%20for%20machine%20learning%20%28ML%29%20analyses.%20Standard%20practice%20aligns%20all%20specimens%20via%20Generalized%20Procrustes%20Analysis%20%28GPA%29%20prior%20to%20splitting%20data%20into%20training%20and%20test%20sets%2C%20potentially%20introducing%20statistical%20dependence%20and%20contaminating%20downstream%20predictive%20models.%20Here%2C%20the%20effects%20of%20GPA-induced%20contamination%20are%20formally%20characterised%20using%20controlled%202D%20and%203D%20simulations%20across%20varying%20sample%20sizes%2C%20landmark%20densities%2C%20and%20allometric%20patterns.%20A%20novel%20realignment%20procedure%20is%20proposed%2C%20whereby%20test%20specimens%20are%20aligned%20to%20the%20training%20set%20prior%20to%20model%20fitting%2C%20eliminating%20cross-sample%20dependency.%20Simulations%20reveal%20a%20robust%20%22diagonal%22%20in%20sample-size%20vs.%20landmark-space%2C%20reflecting%20the%20scaling%20of%20RMSE%20under%20isotropic%20variation%2C%20with%20slopes%20analytically%20derived%20from%20the%20degrees%20of%20freedom%20in%20Procrustes%20tangent%20space.%20The%20importance%20of%20spatial%20autocorrelation%20among%20landmarks%20is%20further%20demonstrated%20using%20linear%20and%20convolutional%20regression%20models%2C%20highlighting%20performance%20degradation%20when%20landmark%20relationships%20are%20ignored.%20This%20work%20establishes%20the%20need%20for%20careful%20preprocessing%20in%20ML%20applications%20of%20GMM%2C%20provides%20practical%20guidelines%20for%20realignment%2C%20and%20clarifies%20fundamental%20statistical%20constraints%20inherent%20to%20Procrustes%20shape%20space.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18448v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Procrustes%2520Contamination%2520in%2520Machine%2520Learning%2520Applications%2520of%2520Geometric%2520Morphometrics%26entry.906535625%3DLloyd%2520Austin%2520Courtenay%26entry.1292438233%3DGeometric%2520morphometrics%2520%2528GMM%2529%2520is%2520widely%2520used%2520to%2520quantify%2520shape%2520variation%252C%2520more%2520recently%2520serving%2520as%2520input%2520for%2520machine%2520learning%2520%2528ML%2529%2520analyses.%2520Standard%2520practice%2520aligns%2520all%2520specimens%2520via%2520Generalized%2520Procrustes%2520Analysis%2520%2528GPA%2529%2520prior%2520to%2520splitting%2520data%2520into%2520training%2520and%2520test%2520sets%252C%2520potentially%2520introducing%2520statistical%2520dependence%2520and%2520contaminating%2520downstream%2520predictive%2520models.%2520Here%252C%2520the%2520effects%2520of%2520GPA-induced%2520contamination%2520are%2520formally%2520characterised%2520using%2520controlled%25202D%2520and%25203D%2520simulations%2520across%2520varying%2520sample%2520sizes%252C%2520landmark%2520densities%252C%2520and%2520allometric%2520patterns.%2520A%2520novel%2520realignment%2520procedure%2520is%2520proposed%252C%2520whereby%2520test%2520specimens%2520are%2520aligned%2520to%2520the%2520training%2520set%2520prior%2520to%2520model%2520fitting%252C%2520eliminating%2520cross-sample%2520dependency.%2520Simulations%2520reveal%2520a%2520robust%2520%2522diagonal%2522%2520in%2520sample-size%2520vs.%2520landmark-space%252C%2520reflecting%2520the%2520scaling%2520of%2520RMSE%2520under%2520isotropic%2520variation%252C%2520with%2520slopes%2520analytically%2520derived%2520from%2520the%2520degrees%2520of%2520freedom%2520in%2520Procrustes%2520tangent%2520space.%2520The%2520importance%2520of%2520spatial%2520autocorrelation%2520among%2520landmarks%2520is%2520further%2520demonstrated%2520using%2520linear%2520and%2520convolutional%2520regression%2520models%252C%2520highlighting%2520performance%2520degradation%2520when%2520landmark%2520relationships%2520are%2520ignored.%2520This%2520work%2520establishes%2520the%2520need%2520for%2520careful%2520preprocessing%2520in%2520ML%2520applications%2520of%2520GMM%252C%2520provides%2520practical%2520guidelines%2520for%2520realignment%252C%2520and%2520clarifies%2520fundamental%2520statistical%2520constraints%2520inherent%2520to%2520Procrustes%2520shape%2520space.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18448v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Procrustes%20Contamination%20in%20Machine%20Learning%20Applications%20of%20Geometric%20Morphometrics&entry.906535625=Lloyd%20Austin%20Courtenay&entry.1292438233=Geometric%20morphometrics%20%28GMM%29%20is%20widely%20used%20to%20quantify%20shape%20variation%2C%20more%20recently%20serving%20as%20input%20for%20machine%20learning%20%28ML%29%20analyses.%20Standard%20practice%20aligns%20all%20specimens%20via%20Generalized%20Procrustes%20Analysis%20%28GPA%29%20prior%20to%20splitting%20data%20into%20training%20and%20test%20sets%2C%20potentially%20introducing%20statistical%20dependence%20and%20contaminating%20downstream%20predictive%20models.%20Here%2C%20the%20effects%20of%20GPA-induced%20contamination%20are%20formally%20characterised%20using%20controlled%202D%20and%203D%20simulations%20across%20varying%20sample%20sizes%2C%20landmark%20densities%2C%20and%20allometric%20patterns.%20A%20novel%20realignment%20procedure%20is%20proposed%2C%20whereby%20test%20specimens%20are%20aligned%20to%20the%20training%20set%20prior%20to%20model%20fitting%2C%20eliminating%20cross-sample%20dependency.%20Simulations%20reveal%20a%20robust%20%22diagonal%22%20in%20sample-size%20vs.%20landmark-space%2C%20reflecting%20the%20scaling%20of%20RMSE%20under%20isotropic%20variation%2C%20with%20slopes%20analytically%20derived%20from%20the%20degrees%20of%20freedom%20in%20Procrustes%20tangent%20space.%20The%20importance%20of%20spatial%20autocorrelation%20among%20landmarks%20is%20further%20demonstrated%20using%20linear%20and%20convolutional%20regression%20models%2C%20highlighting%20performance%20degradation%20when%20landmark%20relationships%20are%20ignored.%20This%20work%20establishes%20the%20need%20for%20careful%20preprocessing%20in%20ML%20applications%20of%20GMM%2C%20provides%20practical%20guidelines%20for%20realignment%2C%20and%20clarifies%20fundamental%20statistical%20constraints%20inherent%20to%20Procrustes%20shape%20space.&entry.1838667208=http%3A//arxiv.org/abs/2601.18448v1&entry.124074799=Read"},
{"title": "A Tumor Aware DenseNet Swin Hybrid Learning with Boosted and Hierarchical Feature Spaces for Large-Scale Brain MRI Classification", "author": "Muhammad Ali Shah and Muhammad Mansoor Alam and Saddam Hussain Khan", "abstract": "This study proposes an efficient Densely Swin Hybrid (EDSH) framework for brain tumor MRI analysis, designed to jointly capture fine grained texture patterns and long range contextual dependencies. Two tumor aware experimental setups are introduced to address class-specific diagnostic challenges. The first setup employs a Boosted Feature Space (BFS), where independently customized DenseNet and Swint branches learn complementary local and global representations that are dimension aligned, fused, and boosted, enabling highly sensitive detection of diffuse glioma patterns by successfully learning the features of irregular shape, poorly defined mass, and heterogeneous texture. The second setup adopts a hierarchical DenseNet Swint architecture with Deep Feature Extraction have Dual Residual connections (DFE and DR), in which DenseNet serves as a stem CNN for structured local feature learning, while Swin_t models global tumor morphology, effectively suppressing false negatives in meningioma and pituitary tumor classification by learning the features of well defined mass, location (outside brain) and enlargments in tumors (dural tail or upward extension). DenseNet is customized at the input level to match MRI spatial characteristics, leveraging dense residual connectivity to preserve texture information and mitigate vanishing-gradient effects. In parallel, Swint is tailored through task aligned patch embedding and shifted-window self attention to efficiently capture hierarchical global dependencies. Extensive evaluation on a large-scale MRI dataset (stringent 40,260 images across four tumor classes) demonstrates consistent superiority over standalone CNNs, Vision Transformers, and hybrids, achieving 98.50 accuracy and recall on the test unseen dataset.", "link": "http://arxiv.org/abs/2601.18330v1", "date": "2026-01-26", "relevancy": 2.6613, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5559}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5213}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5196}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Tumor%20Aware%20DenseNet%20Swin%20Hybrid%20Learning%20with%20Boosted%20and%20Hierarchical%20Feature%20Spaces%20for%20Large-Scale%20Brain%20MRI%20Classification&body=Title%3A%20A%20Tumor%20Aware%20DenseNet%20Swin%20Hybrid%20Learning%20with%20Boosted%20and%20Hierarchical%20Feature%20Spaces%20for%20Large-Scale%20Brain%20MRI%20Classification%0AAuthor%3A%20Muhammad%20Ali%20Shah%20and%20Muhammad%20Mansoor%20Alam%20and%20Saddam%20Hussain%20Khan%0AAbstract%3A%20This%20study%20proposes%20an%20efficient%20Densely%20Swin%20Hybrid%20%28EDSH%29%20framework%20for%20brain%20tumor%20MRI%20analysis%2C%20designed%20to%20jointly%20capture%20fine%20grained%20texture%20patterns%20and%20long%20range%20contextual%20dependencies.%20Two%20tumor%20aware%20experimental%20setups%20are%20introduced%20to%20address%20class-specific%20diagnostic%20challenges.%20The%20first%20setup%20employs%20a%20Boosted%20Feature%20Space%20%28BFS%29%2C%20where%20independently%20customized%20DenseNet%20and%20Swint%20branches%20learn%20complementary%20local%20and%20global%20representations%20that%20are%20dimension%20aligned%2C%20fused%2C%20and%20boosted%2C%20enabling%20highly%20sensitive%20detection%20of%20diffuse%20glioma%20patterns%20by%20successfully%20learning%20the%20features%20of%20irregular%20shape%2C%20poorly%20defined%20mass%2C%20and%20heterogeneous%20texture.%20The%20second%20setup%20adopts%20a%20hierarchical%20DenseNet%20Swint%20architecture%20with%20Deep%20Feature%20Extraction%20have%20Dual%20Residual%20connections%20%28DFE%20and%20DR%29%2C%20in%20which%20DenseNet%20serves%20as%20a%20stem%20CNN%20for%20structured%20local%20feature%20learning%2C%20while%20Swin_t%20models%20global%20tumor%20morphology%2C%20effectively%20suppressing%20false%20negatives%20in%20meningioma%20and%20pituitary%20tumor%20classification%20by%20learning%20the%20features%20of%20well%20defined%20mass%2C%20location%20%28outside%20brain%29%20and%20enlargments%20in%20tumors%20%28dural%20tail%20or%20upward%20extension%29.%20DenseNet%20is%20customized%20at%20the%20input%20level%20to%20match%20MRI%20spatial%20characteristics%2C%20leveraging%20dense%20residual%20connectivity%20to%20preserve%20texture%20information%20and%20mitigate%20vanishing-gradient%20effects.%20In%20parallel%2C%20Swint%20is%20tailored%20through%20task%20aligned%20patch%20embedding%20and%20shifted-window%20self%20attention%20to%20efficiently%20capture%20hierarchical%20global%20dependencies.%20Extensive%20evaluation%20on%20a%20large-scale%20MRI%20dataset%20%28stringent%2040%2C260%20images%20across%20four%20tumor%20classes%29%20demonstrates%20consistent%20superiority%20over%20standalone%20CNNs%2C%20Vision%20Transformers%2C%20and%20hybrids%2C%20achieving%2098.50%20accuracy%20and%20recall%20on%20the%20test%20unseen%20dataset.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18330v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Tumor%2520Aware%2520DenseNet%2520Swin%2520Hybrid%2520Learning%2520with%2520Boosted%2520and%2520Hierarchical%2520Feature%2520Spaces%2520for%2520Large-Scale%2520Brain%2520MRI%2520Classification%26entry.906535625%3DMuhammad%2520Ali%2520Shah%2520and%2520Muhammad%2520Mansoor%2520Alam%2520and%2520Saddam%2520Hussain%2520Khan%26entry.1292438233%3DThis%2520study%2520proposes%2520an%2520efficient%2520Densely%2520Swin%2520Hybrid%2520%2528EDSH%2529%2520framework%2520for%2520brain%2520tumor%2520MRI%2520analysis%252C%2520designed%2520to%2520jointly%2520capture%2520fine%2520grained%2520texture%2520patterns%2520and%2520long%2520range%2520contextual%2520dependencies.%2520Two%2520tumor%2520aware%2520experimental%2520setups%2520are%2520introduced%2520to%2520address%2520class-specific%2520diagnostic%2520challenges.%2520The%2520first%2520setup%2520employs%2520a%2520Boosted%2520Feature%2520Space%2520%2528BFS%2529%252C%2520where%2520independently%2520customized%2520DenseNet%2520and%2520Swint%2520branches%2520learn%2520complementary%2520local%2520and%2520global%2520representations%2520that%2520are%2520dimension%2520aligned%252C%2520fused%252C%2520and%2520boosted%252C%2520enabling%2520highly%2520sensitive%2520detection%2520of%2520diffuse%2520glioma%2520patterns%2520by%2520successfully%2520learning%2520the%2520features%2520of%2520irregular%2520shape%252C%2520poorly%2520defined%2520mass%252C%2520and%2520heterogeneous%2520texture.%2520The%2520second%2520setup%2520adopts%2520a%2520hierarchical%2520DenseNet%2520Swint%2520architecture%2520with%2520Deep%2520Feature%2520Extraction%2520have%2520Dual%2520Residual%2520connections%2520%2528DFE%2520and%2520DR%2529%252C%2520in%2520which%2520DenseNet%2520serves%2520as%2520a%2520stem%2520CNN%2520for%2520structured%2520local%2520feature%2520learning%252C%2520while%2520Swin_t%2520models%2520global%2520tumor%2520morphology%252C%2520effectively%2520suppressing%2520false%2520negatives%2520in%2520meningioma%2520and%2520pituitary%2520tumor%2520classification%2520by%2520learning%2520the%2520features%2520of%2520well%2520defined%2520mass%252C%2520location%2520%2528outside%2520brain%2529%2520and%2520enlargments%2520in%2520tumors%2520%2528dural%2520tail%2520or%2520upward%2520extension%2529.%2520DenseNet%2520is%2520customized%2520at%2520the%2520input%2520level%2520to%2520match%2520MRI%2520spatial%2520characteristics%252C%2520leveraging%2520dense%2520residual%2520connectivity%2520to%2520preserve%2520texture%2520information%2520and%2520mitigate%2520vanishing-gradient%2520effects.%2520In%2520parallel%252C%2520Swint%2520is%2520tailored%2520through%2520task%2520aligned%2520patch%2520embedding%2520and%2520shifted-window%2520self%2520attention%2520to%2520efficiently%2520capture%2520hierarchical%2520global%2520dependencies.%2520Extensive%2520evaluation%2520on%2520a%2520large-scale%2520MRI%2520dataset%2520%2528stringent%252040%252C260%2520images%2520across%2520four%2520tumor%2520classes%2529%2520demonstrates%2520consistent%2520superiority%2520over%2520standalone%2520CNNs%252C%2520Vision%2520Transformers%252C%2520and%2520hybrids%252C%2520achieving%252098.50%2520accuracy%2520and%2520recall%2520on%2520the%2520test%2520unseen%2520dataset.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18330v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Tumor%20Aware%20DenseNet%20Swin%20Hybrid%20Learning%20with%20Boosted%20and%20Hierarchical%20Feature%20Spaces%20for%20Large-Scale%20Brain%20MRI%20Classification&entry.906535625=Muhammad%20Ali%20Shah%20and%20Muhammad%20Mansoor%20Alam%20and%20Saddam%20Hussain%20Khan&entry.1292438233=This%20study%20proposes%20an%20efficient%20Densely%20Swin%20Hybrid%20%28EDSH%29%20framework%20for%20brain%20tumor%20MRI%20analysis%2C%20designed%20to%20jointly%20capture%20fine%20grained%20texture%20patterns%20and%20long%20range%20contextual%20dependencies.%20Two%20tumor%20aware%20experimental%20setups%20are%20introduced%20to%20address%20class-specific%20diagnostic%20challenges.%20The%20first%20setup%20employs%20a%20Boosted%20Feature%20Space%20%28BFS%29%2C%20where%20independently%20customized%20DenseNet%20and%20Swint%20branches%20learn%20complementary%20local%20and%20global%20representations%20that%20are%20dimension%20aligned%2C%20fused%2C%20and%20boosted%2C%20enabling%20highly%20sensitive%20detection%20of%20diffuse%20glioma%20patterns%20by%20successfully%20learning%20the%20features%20of%20irregular%20shape%2C%20poorly%20defined%20mass%2C%20and%20heterogeneous%20texture.%20The%20second%20setup%20adopts%20a%20hierarchical%20DenseNet%20Swint%20architecture%20with%20Deep%20Feature%20Extraction%20have%20Dual%20Residual%20connections%20%28DFE%20and%20DR%29%2C%20in%20which%20DenseNet%20serves%20as%20a%20stem%20CNN%20for%20structured%20local%20feature%20learning%2C%20while%20Swin_t%20models%20global%20tumor%20morphology%2C%20effectively%20suppressing%20false%20negatives%20in%20meningioma%20and%20pituitary%20tumor%20classification%20by%20learning%20the%20features%20of%20well%20defined%20mass%2C%20location%20%28outside%20brain%29%20and%20enlargments%20in%20tumors%20%28dural%20tail%20or%20upward%20extension%29.%20DenseNet%20is%20customized%20at%20the%20input%20level%20to%20match%20MRI%20spatial%20characteristics%2C%20leveraging%20dense%20residual%20connectivity%20to%20preserve%20texture%20information%20and%20mitigate%20vanishing-gradient%20effects.%20In%20parallel%2C%20Swint%20is%20tailored%20through%20task%20aligned%20patch%20embedding%20and%20shifted-window%20self%20attention%20to%20efficiently%20capture%20hierarchical%20global%20dependencies.%20Extensive%20evaluation%20on%20a%20large-scale%20MRI%20dataset%20%28stringent%2040%2C260%20images%20across%20four%20tumor%20classes%29%20demonstrates%20consistent%20superiority%20over%20standalone%20CNNs%2C%20Vision%20Transformers%2C%20and%20hybrids%2C%20achieving%2098.50%20accuracy%20and%20recall%20on%20the%20test%20unseen%20dataset.&entry.1838667208=http%3A//arxiv.org/abs/2601.18330v1&entry.124074799=Read"},
{"title": "Low Cost, High Efficiency: LiDAR Place Recognition in Vineyards with Matryoshka Representation Learning", "author": "Judith Vilella-Cantos and Mauro Martini and Marcello Chiaberge and M\u00f3nica Ballesta and David Valiente", "abstract": "Localization in agricultural environments is challenging due to their unstructured nature and lack of distinctive landmarks. Although agricultural settings have been studied in the context of object classification and segmentation, the place recognition task for mobile robots is not trivial in the current state of the art. In this study, we propose MinkUNeXt-VINE, a lightweight, deep-learning-based method that surpasses state-of-the-art methods in vineyard environments thanks to its pre-processing and Matryoshka Representation Learning multi-loss approach. Our method prioritizes enhanced performance with low-cost, sparse LiDAR inputs and lower-dimensionality outputs to ensure high efficiency in real-time scenarios. Additionally, we present a comprehensive ablation study of the results on various evaluation cases and two extensive long-term vineyard datasets employing different LiDAR sensors. The results demonstrate the efficiency of the trade-off output produced by this approach, as well as its robust performance on low-cost and low-resolution input data. The code is publicly available for reproduction.", "link": "http://arxiv.org/abs/2601.18714v1", "date": "2026-01-26", "relevancy": 2.6523, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5349}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5301}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Low%20Cost%2C%20High%20Efficiency%3A%20LiDAR%20Place%20Recognition%20in%20Vineyards%20with%20Matryoshka%20Representation%20Learning&body=Title%3A%20Low%20Cost%2C%20High%20Efficiency%3A%20LiDAR%20Place%20Recognition%20in%20Vineyards%20with%20Matryoshka%20Representation%20Learning%0AAuthor%3A%20Judith%20Vilella-Cantos%20and%20Mauro%20Martini%20and%20Marcello%20Chiaberge%20and%20M%C3%B3nica%20Ballesta%20and%20David%20Valiente%0AAbstract%3A%20Localization%20in%20agricultural%20environments%20is%20challenging%20due%20to%20their%20unstructured%20nature%20and%20lack%20of%20distinctive%20landmarks.%20Although%20agricultural%20settings%20have%20been%20studied%20in%20the%20context%20of%20object%20classification%20and%20segmentation%2C%20the%20place%20recognition%20task%20for%20mobile%20robots%20is%20not%20trivial%20in%20the%20current%20state%20of%20the%20art.%20In%20this%20study%2C%20we%20propose%20MinkUNeXt-VINE%2C%20a%20lightweight%2C%20deep-learning-based%20method%20that%20surpasses%20state-of-the-art%20methods%20in%20vineyard%20environments%20thanks%20to%20its%20pre-processing%20and%20Matryoshka%20Representation%20Learning%20multi-loss%20approach.%20Our%20method%20prioritizes%20enhanced%20performance%20with%20low-cost%2C%20sparse%20LiDAR%20inputs%20and%20lower-dimensionality%20outputs%20to%20ensure%20high%20efficiency%20in%20real-time%20scenarios.%20Additionally%2C%20we%20present%20a%20comprehensive%20ablation%20study%20of%20the%20results%20on%20various%20evaluation%20cases%20and%20two%20extensive%20long-term%20vineyard%20datasets%20employing%20different%20LiDAR%20sensors.%20The%20results%20demonstrate%20the%20efficiency%20of%20the%20trade-off%20output%20produced%20by%20this%20approach%2C%20as%20well%20as%20its%20robust%20performance%20on%20low-cost%20and%20low-resolution%20input%20data.%20The%20code%20is%20publicly%20available%20for%20reproduction.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18714v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLow%2520Cost%252C%2520High%2520Efficiency%253A%2520LiDAR%2520Place%2520Recognition%2520in%2520Vineyards%2520with%2520Matryoshka%2520Representation%2520Learning%26entry.906535625%3DJudith%2520Vilella-Cantos%2520and%2520Mauro%2520Martini%2520and%2520Marcello%2520Chiaberge%2520and%2520M%25C3%25B3nica%2520Ballesta%2520and%2520David%2520Valiente%26entry.1292438233%3DLocalization%2520in%2520agricultural%2520environments%2520is%2520challenging%2520due%2520to%2520their%2520unstructured%2520nature%2520and%2520lack%2520of%2520distinctive%2520landmarks.%2520Although%2520agricultural%2520settings%2520have%2520been%2520studied%2520in%2520the%2520context%2520of%2520object%2520classification%2520and%2520segmentation%252C%2520the%2520place%2520recognition%2520task%2520for%2520mobile%2520robots%2520is%2520not%2520trivial%2520in%2520the%2520current%2520state%2520of%2520the%2520art.%2520In%2520this%2520study%252C%2520we%2520propose%2520MinkUNeXt-VINE%252C%2520a%2520lightweight%252C%2520deep-learning-based%2520method%2520that%2520surpasses%2520state-of-the-art%2520methods%2520in%2520vineyard%2520environments%2520thanks%2520to%2520its%2520pre-processing%2520and%2520Matryoshka%2520Representation%2520Learning%2520multi-loss%2520approach.%2520Our%2520method%2520prioritizes%2520enhanced%2520performance%2520with%2520low-cost%252C%2520sparse%2520LiDAR%2520inputs%2520and%2520lower-dimensionality%2520outputs%2520to%2520ensure%2520high%2520efficiency%2520in%2520real-time%2520scenarios.%2520Additionally%252C%2520we%2520present%2520a%2520comprehensive%2520ablation%2520study%2520of%2520the%2520results%2520on%2520various%2520evaluation%2520cases%2520and%2520two%2520extensive%2520long-term%2520vineyard%2520datasets%2520employing%2520different%2520LiDAR%2520sensors.%2520The%2520results%2520demonstrate%2520the%2520efficiency%2520of%2520the%2520trade-off%2520output%2520produced%2520by%2520this%2520approach%252C%2520as%2520well%2520as%2520its%2520robust%2520performance%2520on%2520low-cost%2520and%2520low-resolution%2520input%2520data.%2520The%2520code%2520is%2520publicly%2520available%2520for%2520reproduction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18714v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low%20Cost%2C%20High%20Efficiency%3A%20LiDAR%20Place%20Recognition%20in%20Vineyards%20with%20Matryoshka%20Representation%20Learning&entry.906535625=Judith%20Vilella-Cantos%20and%20Mauro%20Martini%20and%20Marcello%20Chiaberge%20and%20M%C3%B3nica%20Ballesta%20and%20David%20Valiente&entry.1292438233=Localization%20in%20agricultural%20environments%20is%20challenging%20due%20to%20their%20unstructured%20nature%20and%20lack%20of%20distinctive%20landmarks.%20Although%20agricultural%20settings%20have%20been%20studied%20in%20the%20context%20of%20object%20classification%20and%20segmentation%2C%20the%20place%20recognition%20task%20for%20mobile%20robots%20is%20not%20trivial%20in%20the%20current%20state%20of%20the%20art.%20In%20this%20study%2C%20we%20propose%20MinkUNeXt-VINE%2C%20a%20lightweight%2C%20deep-learning-based%20method%20that%20surpasses%20state-of-the-art%20methods%20in%20vineyard%20environments%20thanks%20to%20its%20pre-processing%20and%20Matryoshka%20Representation%20Learning%20multi-loss%20approach.%20Our%20method%20prioritizes%20enhanced%20performance%20with%20low-cost%2C%20sparse%20LiDAR%20inputs%20and%20lower-dimensionality%20outputs%20to%20ensure%20high%20efficiency%20in%20real-time%20scenarios.%20Additionally%2C%20we%20present%20a%20comprehensive%20ablation%20study%20of%20the%20results%20on%20various%20evaluation%20cases%20and%20two%20extensive%20long-term%20vineyard%20datasets%20employing%20different%20LiDAR%20sensors.%20The%20results%20demonstrate%20the%20efficiency%20of%20the%20trade-off%20output%20produced%20by%20this%20approach%2C%20as%20well%20as%20its%20robust%20performance%20on%20low-cost%20and%20low-resolution%20input%20data.%20The%20code%20is%20publicly%20available%20for%20reproduction.&entry.1838667208=http%3A//arxiv.org/abs/2601.18714v1&entry.124074799=Read"},
{"title": "CONQUER: Context-Aware Representation with Query Enhancement for Text-Based Person Search", "author": "Zequn Xie", "abstract": "Text-Based Person Search (TBPS) aims to retrieve pedestrian images from large galleries using natural language descriptions. This task, essential for public safety applications, is hindered by cross-modal discrepancies and ambiguous user queries. We introduce CONQUER, a two-stage framework designed to address these challenges by enhancing cross-modal alignment during training and adaptively refining queries at inference. During training, CONQUER employs multi-granularity encoding, complementary pair mining, and context-guided optimal matching based on Optimal Transport to learn robust embeddings. At inference, a plug-and-play query enhancement module refines vague or incomplete queries via anchor selection and attribute-driven enrichment, without requiring retraining of the backbone. Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid demonstrate that CONQUER consistently outperforms strong baselines in both Rank-1 accuracy and mAP, yielding notable improvements in cross-domain and incomplete-query scenarios. These results highlight CONQUER as a practical and effective solution for real-world TBPS deployment. Source code is available at https://github.com/zqxie77/CONQUER.", "link": "http://arxiv.org/abs/2601.18625v1", "date": "2026-01-26", "relevancy": 2.648, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5357}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5265}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CONQUER%3A%20Context-Aware%20Representation%20with%20Query%20Enhancement%20for%20Text-Based%20Person%20Search&body=Title%3A%20CONQUER%3A%20Context-Aware%20Representation%20with%20Query%20Enhancement%20for%20Text-Based%20Person%20Search%0AAuthor%3A%20Zequn%20Xie%0AAbstract%3A%20Text-Based%20Person%20Search%20%28TBPS%29%20aims%20to%20retrieve%20pedestrian%20images%20from%20large%20galleries%20using%20natural%20language%20descriptions.%20This%20task%2C%20essential%20for%20public%20safety%20applications%2C%20is%20hindered%20by%20cross-modal%20discrepancies%20and%20ambiguous%20user%20queries.%20We%20introduce%20CONQUER%2C%20a%20two-stage%20framework%20designed%20to%20address%20these%20challenges%20by%20enhancing%20cross-modal%20alignment%20during%20training%20and%20adaptively%20refining%20queries%20at%20inference.%20During%20training%2C%20CONQUER%20employs%20multi-granularity%20encoding%2C%20complementary%20pair%20mining%2C%20and%20context-guided%20optimal%20matching%20based%20on%20Optimal%20Transport%20to%20learn%20robust%20embeddings.%20At%20inference%2C%20a%20plug-and-play%20query%20enhancement%20module%20refines%20vague%20or%20incomplete%20queries%20via%20anchor%20selection%20and%20attribute-driven%20enrichment%2C%20without%20requiring%20retraining%20of%20the%20backbone.%20Extensive%20experiments%20on%20CUHK-PEDES%2C%20ICFG-PEDES%2C%20and%20RSTPReid%20demonstrate%20that%20CONQUER%20consistently%20outperforms%20strong%20baselines%20in%20both%20Rank-1%20accuracy%20and%20mAP%2C%20yielding%20notable%20improvements%20in%20cross-domain%20and%20incomplete-query%20scenarios.%20These%20results%20highlight%20CONQUER%20as%20a%20practical%20and%20effective%20solution%20for%20real-world%20TBPS%20deployment.%20Source%20code%20is%20available%20at%20https%3A//github.com/zqxie77/CONQUER.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18625v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCONQUER%253A%2520Context-Aware%2520Representation%2520with%2520Query%2520Enhancement%2520for%2520Text-Based%2520Person%2520Search%26entry.906535625%3DZequn%2520Xie%26entry.1292438233%3DText-Based%2520Person%2520Search%2520%2528TBPS%2529%2520aims%2520to%2520retrieve%2520pedestrian%2520images%2520from%2520large%2520galleries%2520using%2520natural%2520language%2520descriptions.%2520This%2520task%252C%2520essential%2520for%2520public%2520safety%2520applications%252C%2520is%2520hindered%2520by%2520cross-modal%2520discrepancies%2520and%2520ambiguous%2520user%2520queries.%2520We%2520introduce%2520CONQUER%252C%2520a%2520two-stage%2520framework%2520designed%2520to%2520address%2520these%2520challenges%2520by%2520enhancing%2520cross-modal%2520alignment%2520during%2520training%2520and%2520adaptively%2520refining%2520queries%2520at%2520inference.%2520During%2520training%252C%2520CONQUER%2520employs%2520multi-granularity%2520encoding%252C%2520complementary%2520pair%2520mining%252C%2520and%2520context-guided%2520optimal%2520matching%2520based%2520on%2520Optimal%2520Transport%2520to%2520learn%2520robust%2520embeddings.%2520At%2520inference%252C%2520a%2520plug-and-play%2520query%2520enhancement%2520module%2520refines%2520vague%2520or%2520incomplete%2520queries%2520via%2520anchor%2520selection%2520and%2520attribute-driven%2520enrichment%252C%2520without%2520requiring%2520retraining%2520of%2520the%2520backbone.%2520Extensive%2520experiments%2520on%2520CUHK-PEDES%252C%2520ICFG-PEDES%252C%2520and%2520RSTPReid%2520demonstrate%2520that%2520CONQUER%2520consistently%2520outperforms%2520strong%2520baselines%2520in%2520both%2520Rank-1%2520accuracy%2520and%2520mAP%252C%2520yielding%2520notable%2520improvements%2520in%2520cross-domain%2520and%2520incomplete-query%2520scenarios.%2520These%2520results%2520highlight%2520CONQUER%2520as%2520a%2520practical%2520and%2520effective%2520solution%2520for%2520real-world%2520TBPS%2520deployment.%2520Source%2520code%2520is%2520available%2520at%2520https%253A//github.com/zqxie77/CONQUER.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18625v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CONQUER%3A%20Context-Aware%20Representation%20with%20Query%20Enhancement%20for%20Text-Based%20Person%20Search&entry.906535625=Zequn%20Xie&entry.1292438233=Text-Based%20Person%20Search%20%28TBPS%29%20aims%20to%20retrieve%20pedestrian%20images%20from%20large%20galleries%20using%20natural%20language%20descriptions.%20This%20task%2C%20essential%20for%20public%20safety%20applications%2C%20is%20hindered%20by%20cross-modal%20discrepancies%20and%20ambiguous%20user%20queries.%20We%20introduce%20CONQUER%2C%20a%20two-stage%20framework%20designed%20to%20address%20these%20challenges%20by%20enhancing%20cross-modal%20alignment%20during%20training%20and%20adaptively%20refining%20queries%20at%20inference.%20During%20training%2C%20CONQUER%20employs%20multi-granularity%20encoding%2C%20complementary%20pair%20mining%2C%20and%20context-guided%20optimal%20matching%20based%20on%20Optimal%20Transport%20to%20learn%20robust%20embeddings.%20At%20inference%2C%20a%20plug-and-play%20query%20enhancement%20module%20refines%20vague%20or%20incomplete%20queries%20via%20anchor%20selection%20and%20attribute-driven%20enrichment%2C%20without%20requiring%20retraining%20of%20the%20backbone.%20Extensive%20experiments%20on%20CUHK-PEDES%2C%20ICFG-PEDES%2C%20and%20RSTPReid%20demonstrate%20that%20CONQUER%20consistently%20outperforms%20strong%20baselines%20in%20both%20Rank-1%20accuracy%20and%20mAP%2C%20yielding%20notable%20improvements%20in%20cross-domain%20and%20incomplete-query%20scenarios.%20These%20results%20highlight%20CONQUER%20as%20a%20practical%20and%20effective%20solution%20for%20real-world%20TBPS%20deployment.%20Source%20code%20is%20available%20at%20https%3A//github.com/zqxie77/CONQUER.&entry.1838667208=http%3A//arxiv.org/abs/2601.18625v1&entry.124074799=Read"},
{"title": "Scale-Aware Self-Supervised Learning for Segmentation of Small and Sparse Structures", "author": "Jorge Quesada and Ghassan AlRegib", "abstract": "Self-supervised learning (SSL) has emerged as a powerful strategy for representation learning under limited annotation regimes, yet its effectiveness remains highly sensitive to many factors, especially the nature of the target task. In segmentation, existing pipelines are typically tuned to large, homogeneous regions, but their performance drops when objects are small, sparse, or locally irregular. In this work, we propose a scale-aware SSL adaptation that integrates small-window cropping into the augmentation pipeline, zooming in on fine-scale structures during pretraining. We evaluate this approach across two domains with markedly different data modalities: seismic imaging, where the goal is to segment sparse faults, and neuroimaging, where the task is to delineate small cellular structures. In both settings, our method yields consistent improvements over standard and state-of-the-art baselines under label constraints, improving accuracy by up to 13% for fault segmentation and 5% for cell delineation. In contrast, large-scale features such as seismic facies or tissue regions see little benefit, underscoring that the value of SSL depends critically on the scale of the target objects. Our findings highlight the need to align SSL design with object size and sparsity, offering a general principle for buil ding more effective representation learning pipelines across scientific imaging domains.", "link": "http://arxiv.org/abs/2601.18619v1", "date": "2026-01-26", "relevancy": 2.6388, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5554}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5276}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scale-Aware%20Self-Supervised%20Learning%20for%20Segmentation%20of%20Small%20and%20Sparse%20Structures&body=Title%3A%20Scale-Aware%20Self-Supervised%20Learning%20for%20Segmentation%20of%20Small%20and%20Sparse%20Structures%0AAuthor%3A%20Jorge%20Quesada%20and%20Ghassan%20AlRegib%0AAbstract%3A%20Self-supervised%20learning%20%28SSL%29%20has%20emerged%20as%20a%20powerful%20strategy%20for%20representation%20learning%20under%20limited%20annotation%20regimes%2C%20yet%20its%20effectiveness%20remains%20highly%20sensitive%20to%20many%20factors%2C%20especially%20the%20nature%20of%20the%20target%20task.%20In%20segmentation%2C%20existing%20pipelines%20are%20typically%20tuned%20to%20large%2C%20homogeneous%20regions%2C%20but%20their%20performance%20drops%20when%20objects%20are%20small%2C%20sparse%2C%20or%20locally%20irregular.%20In%20this%20work%2C%20we%20propose%20a%20scale-aware%20SSL%20adaptation%20that%20integrates%20small-window%20cropping%20into%20the%20augmentation%20pipeline%2C%20zooming%20in%20on%20fine-scale%20structures%20during%20pretraining.%20We%20evaluate%20this%20approach%20across%20two%20domains%20with%20markedly%20different%20data%20modalities%3A%20seismic%20imaging%2C%20where%20the%20goal%20is%20to%20segment%20sparse%20faults%2C%20and%20neuroimaging%2C%20where%20the%20task%20is%20to%20delineate%20small%20cellular%20structures.%20In%20both%20settings%2C%20our%20method%20yields%20consistent%20improvements%20over%20standard%20and%20state-of-the-art%20baselines%20under%20label%20constraints%2C%20improving%20accuracy%20by%20up%20to%2013%25%20for%20fault%20segmentation%20and%205%25%20for%20cell%20delineation.%20In%20contrast%2C%20large-scale%20features%20such%20as%20seismic%20facies%20or%20tissue%20regions%20see%20little%20benefit%2C%20underscoring%20that%20the%20value%20of%20SSL%20depends%20critically%20on%20the%20scale%20of%20the%20target%20objects.%20Our%20findings%20highlight%20the%20need%20to%20align%20SSL%20design%20with%20object%20size%20and%20sparsity%2C%20offering%20a%20general%20principle%20for%20buil%20ding%20more%20effective%20representation%20learning%20pipelines%20across%20scientific%20imaging%20domains.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18619v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScale-Aware%2520Self-Supervised%2520Learning%2520for%2520Segmentation%2520of%2520Small%2520and%2520Sparse%2520Structures%26entry.906535625%3DJorge%2520Quesada%2520and%2520Ghassan%2520AlRegib%26entry.1292438233%3DSelf-supervised%2520learning%2520%2528SSL%2529%2520has%2520emerged%2520as%2520a%2520powerful%2520strategy%2520for%2520representation%2520learning%2520under%2520limited%2520annotation%2520regimes%252C%2520yet%2520its%2520effectiveness%2520remains%2520highly%2520sensitive%2520to%2520many%2520factors%252C%2520especially%2520the%2520nature%2520of%2520the%2520target%2520task.%2520In%2520segmentation%252C%2520existing%2520pipelines%2520are%2520typically%2520tuned%2520to%2520large%252C%2520homogeneous%2520regions%252C%2520but%2520their%2520performance%2520drops%2520when%2520objects%2520are%2520small%252C%2520sparse%252C%2520or%2520locally%2520irregular.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520scale-aware%2520SSL%2520adaptation%2520that%2520integrates%2520small-window%2520cropping%2520into%2520the%2520augmentation%2520pipeline%252C%2520zooming%2520in%2520on%2520fine-scale%2520structures%2520during%2520pretraining.%2520We%2520evaluate%2520this%2520approach%2520across%2520two%2520domains%2520with%2520markedly%2520different%2520data%2520modalities%253A%2520seismic%2520imaging%252C%2520where%2520the%2520goal%2520is%2520to%2520segment%2520sparse%2520faults%252C%2520and%2520neuroimaging%252C%2520where%2520the%2520task%2520is%2520to%2520delineate%2520small%2520cellular%2520structures.%2520In%2520both%2520settings%252C%2520our%2520method%2520yields%2520consistent%2520improvements%2520over%2520standard%2520and%2520state-of-the-art%2520baselines%2520under%2520label%2520constraints%252C%2520improving%2520accuracy%2520by%2520up%2520to%252013%2525%2520for%2520fault%2520segmentation%2520and%25205%2525%2520for%2520cell%2520delineation.%2520In%2520contrast%252C%2520large-scale%2520features%2520such%2520as%2520seismic%2520facies%2520or%2520tissue%2520regions%2520see%2520little%2520benefit%252C%2520underscoring%2520that%2520the%2520value%2520of%2520SSL%2520depends%2520critically%2520on%2520the%2520scale%2520of%2520the%2520target%2520objects.%2520Our%2520findings%2520highlight%2520the%2520need%2520to%2520align%2520SSL%2520design%2520with%2520object%2520size%2520and%2520sparsity%252C%2520offering%2520a%2520general%2520principle%2520for%2520buil%2520ding%2520more%2520effective%2520representation%2520learning%2520pipelines%2520across%2520scientific%2520imaging%2520domains.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18619v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scale-Aware%20Self-Supervised%20Learning%20for%20Segmentation%20of%20Small%20and%20Sparse%20Structures&entry.906535625=Jorge%20Quesada%20and%20Ghassan%20AlRegib&entry.1292438233=Self-supervised%20learning%20%28SSL%29%20has%20emerged%20as%20a%20powerful%20strategy%20for%20representation%20learning%20under%20limited%20annotation%20regimes%2C%20yet%20its%20effectiveness%20remains%20highly%20sensitive%20to%20many%20factors%2C%20especially%20the%20nature%20of%20the%20target%20task.%20In%20segmentation%2C%20existing%20pipelines%20are%20typically%20tuned%20to%20large%2C%20homogeneous%20regions%2C%20but%20their%20performance%20drops%20when%20objects%20are%20small%2C%20sparse%2C%20or%20locally%20irregular.%20In%20this%20work%2C%20we%20propose%20a%20scale-aware%20SSL%20adaptation%20that%20integrates%20small-window%20cropping%20into%20the%20augmentation%20pipeline%2C%20zooming%20in%20on%20fine-scale%20structures%20during%20pretraining.%20We%20evaluate%20this%20approach%20across%20two%20domains%20with%20markedly%20different%20data%20modalities%3A%20seismic%20imaging%2C%20where%20the%20goal%20is%20to%20segment%20sparse%20faults%2C%20and%20neuroimaging%2C%20where%20the%20task%20is%20to%20delineate%20small%20cellular%20structures.%20In%20both%20settings%2C%20our%20method%20yields%20consistent%20improvements%20over%20standard%20and%20state-of-the-art%20baselines%20under%20label%20constraints%2C%20improving%20accuracy%20by%20up%20to%2013%25%20for%20fault%20segmentation%20and%205%25%20for%20cell%20delineation.%20In%20contrast%2C%20large-scale%20features%20such%20as%20seismic%20facies%20or%20tissue%20regions%20see%20little%20benefit%2C%20underscoring%20that%20the%20value%20of%20SSL%20depends%20critically%20on%20the%20scale%20of%20the%20target%20objects.%20Our%20findings%20highlight%20the%20need%20to%20align%20SSL%20design%20with%20object%20size%20and%20sparsity%2C%20offering%20a%20general%20principle%20for%20buil%20ding%20more%20effective%20representation%20learning%20pipelines%20across%20scientific%20imaging%20domains.&entry.1838667208=http%3A//arxiv.org/abs/2601.18619v1&entry.124074799=Read"},
{"title": "Prism: Towards Lowering User Cognitive Load in LLMs via Complex Intent Understanding", "author": "Zenghua Liao and Jinzhi Liao and Xiang Zhao", "abstract": "Large Language Models are rapidly emerging as web-native interfaces to social platforms. On the social web, users frequently have ambiguous and dynamic goals, making complex intent understanding-rather than single-turn execution-the cornerstone of effective human-LLM collaboration. Existing approaches attempt to clarify user intents through sequential or parallel questioning, yet they fall short of addressing the core challenge: modeling the logical dependencies among clarification questions. Inspired by the Cognitive Load Theory, we propose Prism, a novel framework for complex intent understanding that enables logically coherent and efficient intent clarification. Prism comprises four tailored modules: a complex intent decomposition module, which decomposes user intents into smaller, well-structured elements and identifies logical dependencies among them; a logical clarification generation module, which organizes clarification questions based on these dependencies to ensure coherent, low-friction interactions; an intent-aware reward module, which evaluates the quality of clarification trajectories via an intent-aware reward function and leverages Monte Carlo Sample to simulate user-LLM interactions for large-scale,high-quality training data generation; and a self-evolved intent tuning module, which iteratively refines the LLM's logical clarification capability through data-driven feedback and optimization. Prism consistently outperforms existing approaches across clarification interactions, intent execution, and cognitive load benchmarks. It achieves stateof-the-art logical consistency, reduces logical conflicts to 11.5%, increases user satisfaction by 14.4%, and decreases task completion time by 34.8%. All data and code are released.", "link": "http://arxiv.org/abs/2601.08653v2", "date": "2026-01-26", "relevancy": 2.6167, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5334}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5334}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prism%3A%20Towards%20Lowering%20User%20Cognitive%20Load%20in%20LLMs%20via%20Complex%20Intent%20Understanding&body=Title%3A%20Prism%3A%20Towards%20Lowering%20User%20Cognitive%20Load%20in%20LLMs%20via%20Complex%20Intent%20Understanding%0AAuthor%3A%20Zenghua%20Liao%20and%20Jinzhi%20Liao%20and%20Xiang%20Zhao%0AAbstract%3A%20Large%20Language%20Models%20are%20rapidly%20emerging%20as%20web-native%20interfaces%20to%20social%20platforms.%20On%20the%20social%20web%2C%20users%20frequently%20have%20ambiguous%20and%20dynamic%20goals%2C%20making%20complex%20intent%20understanding-rather%20than%20single-turn%20execution-the%20cornerstone%20of%20effective%20human-LLM%20collaboration.%20Existing%20approaches%20attempt%20to%20clarify%20user%20intents%20through%20sequential%20or%20parallel%20questioning%2C%20yet%20they%20fall%20short%20of%20addressing%20the%20core%20challenge%3A%20modeling%20the%20logical%20dependencies%20among%20clarification%20questions.%20Inspired%20by%20the%20Cognitive%20Load%20Theory%2C%20we%20propose%20Prism%2C%20a%20novel%20framework%20for%20complex%20intent%20understanding%20that%20enables%20logically%20coherent%20and%20efficient%20intent%20clarification.%20Prism%20comprises%20four%20tailored%20modules%3A%20a%20complex%20intent%20decomposition%20module%2C%20which%20decomposes%20user%20intents%20into%20smaller%2C%20well-structured%20elements%20and%20identifies%20logical%20dependencies%20among%20them%3B%20a%20logical%20clarification%20generation%20module%2C%20which%20organizes%20clarification%20questions%20based%20on%20these%20dependencies%20to%20ensure%20coherent%2C%20low-friction%20interactions%3B%20an%20intent-aware%20reward%20module%2C%20which%20evaluates%20the%20quality%20of%20clarification%20trajectories%20via%20an%20intent-aware%20reward%20function%20and%20leverages%20Monte%20Carlo%20Sample%20to%20simulate%20user-LLM%20interactions%20for%20large-scale%2Chigh-quality%20training%20data%20generation%3B%20and%20a%20self-evolved%20intent%20tuning%20module%2C%20which%20iteratively%20refines%20the%20LLM%27s%20logical%20clarification%20capability%20through%20data-driven%20feedback%20and%20optimization.%20Prism%20consistently%20outperforms%20existing%20approaches%20across%20clarification%20interactions%2C%20intent%20execution%2C%20and%20cognitive%20load%20benchmarks.%20It%20achieves%20stateof-the-art%20logical%20consistency%2C%20reduces%20logical%20conflicts%20to%2011.5%25%2C%20increases%20user%20satisfaction%20by%2014.4%25%2C%20and%20decreases%20task%20completion%20time%20by%2034.8%25.%20All%20data%20and%20code%20are%20released.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08653v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrism%253A%2520Towards%2520Lowering%2520User%2520Cognitive%2520Load%2520in%2520LLMs%2520via%2520Complex%2520Intent%2520Understanding%26entry.906535625%3DZenghua%2520Liao%2520and%2520Jinzhi%2520Liao%2520and%2520Xiang%2520Zhao%26entry.1292438233%3DLarge%2520Language%2520Models%2520are%2520rapidly%2520emerging%2520as%2520web-native%2520interfaces%2520to%2520social%2520platforms.%2520On%2520the%2520social%2520web%252C%2520users%2520frequently%2520have%2520ambiguous%2520and%2520dynamic%2520goals%252C%2520making%2520complex%2520intent%2520understanding-rather%2520than%2520single-turn%2520execution-the%2520cornerstone%2520of%2520effective%2520human-LLM%2520collaboration.%2520Existing%2520approaches%2520attempt%2520to%2520clarify%2520user%2520intents%2520through%2520sequential%2520or%2520parallel%2520questioning%252C%2520yet%2520they%2520fall%2520short%2520of%2520addressing%2520the%2520core%2520challenge%253A%2520modeling%2520the%2520logical%2520dependencies%2520among%2520clarification%2520questions.%2520Inspired%2520by%2520the%2520Cognitive%2520Load%2520Theory%252C%2520we%2520propose%2520Prism%252C%2520a%2520novel%2520framework%2520for%2520complex%2520intent%2520understanding%2520that%2520enables%2520logically%2520coherent%2520and%2520efficient%2520intent%2520clarification.%2520Prism%2520comprises%2520four%2520tailored%2520modules%253A%2520a%2520complex%2520intent%2520decomposition%2520module%252C%2520which%2520decomposes%2520user%2520intents%2520into%2520smaller%252C%2520well-structured%2520elements%2520and%2520identifies%2520logical%2520dependencies%2520among%2520them%253B%2520a%2520logical%2520clarification%2520generation%2520module%252C%2520which%2520organizes%2520clarification%2520questions%2520based%2520on%2520these%2520dependencies%2520to%2520ensure%2520coherent%252C%2520low-friction%2520interactions%253B%2520an%2520intent-aware%2520reward%2520module%252C%2520which%2520evaluates%2520the%2520quality%2520of%2520clarification%2520trajectories%2520via%2520an%2520intent-aware%2520reward%2520function%2520and%2520leverages%2520Monte%2520Carlo%2520Sample%2520to%2520simulate%2520user-LLM%2520interactions%2520for%2520large-scale%252Chigh-quality%2520training%2520data%2520generation%253B%2520and%2520a%2520self-evolved%2520intent%2520tuning%2520module%252C%2520which%2520iteratively%2520refines%2520the%2520LLM%2527s%2520logical%2520clarification%2520capability%2520through%2520data-driven%2520feedback%2520and%2520optimization.%2520Prism%2520consistently%2520outperforms%2520existing%2520approaches%2520across%2520clarification%2520interactions%252C%2520intent%2520execution%252C%2520and%2520cognitive%2520load%2520benchmarks.%2520It%2520achieves%2520stateof-the-art%2520logical%2520consistency%252C%2520reduces%2520logical%2520conflicts%2520to%252011.5%2525%252C%2520increases%2520user%2520satisfaction%2520by%252014.4%2525%252C%2520and%2520decreases%2520task%2520completion%2520time%2520by%252034.8%2525.%2520All%2520data%2520and%2520code%2520are%2520released.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08653v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prism%3A%20Towards%20Lowering%20User%20Cognitive%20Load%20in%20LLMs%20via%20Complex%20Intent%20Understanding&entry.906535625=Zenghua%20Liao%20and%20Jinzhi%20Liao%20and%20Xiang%20Zhao&entry.1292438233=Large%20Language%20Models%20are%20rapidly%20emerging%20as%20web-native%20interfaces%20to%20social%20platforms.%20On%20the%20social%20web%2C%20users%20frequently%20have%20ambiguous%20and%20dynamic%20goals%2C%20making%20complex%20intent%20understanding-rather%20than%20single-turn%20execution-the%20cornerstone%20of%20effective%20human-LLM%20collaboration.%20Existing%20approaches%20attempt%20to%20clarify%20user%20intents%20through%20sequential%20or%20parallel%20questioning%2C%20yet%20they%20fall%20short%20of%20addressing%20the%20core%20challenge%3A%20modeling%20the%20logical%20dependencies%20among%20clarification%20questions.%20Inspired%20by%20the%20Cognitive%20Load%20Theory%2C%20we%20propose%20Prism%2C%20a%20novel%20framework%20for%20complex%20intent%20understanding%20that%20enables%20logically%20coherent%20and%20efficient%20intent%20clarification.%20Prism%20comprises%20four%20tailored%20modules%3A%20a%20complex%20intent%20decomposition%20module%2C%20which%20decomposes%20user%20intents%20into%20smaller%2C%20well-structured%20elements%20and%20identifies%20logical%20dependencies%20among%20them%3B%20a%20logical%20clarification%20generation%20module%2C%20which%20organizes%20clarification%20questions%20based%20on%20these%20dependencies%20to%20ensure%20coherent%2C%20low-friction%20interactions%3B%20an%20intent-aware%20reward%20module%2C%20which%20evaluates%20the%20quality%20of%20clarification%20trajectories%20via%20an%20intent-aware%20reward%20function%20and%20leverages%20Monte%20Carlo%20Sample%20to%20simulate%20user-LLM%20interactions%20for%20large-scale%2Chigh-quality%20training%20data%20generation%3B%20and%20a%20self-evolved%20intent%20tuning%20module%2C%20which%20iteratively%20refines%20the%20LLM%27s%20logical%20clarification%20capability%20through%20data-driven%20feedback%20and%20optimization.%20Prism%20consistently%20outperforms%20existing%20approaches%20across%20clarification%20interactions%2C%20intent%20execution%2C%20and%20cognitive%20load%20benchmarks.%20It%20achieves%20stateof-the-art%20logical%20consistency%2C%20reduces%20logical%20conflicts%20to%2011.5%25%2C%20increases%20user%20satisfaction%20by%2014.4%25%2C%20and%20decreases%20task%20completion%20time%20by%2034.8%25.%20All%20data%20and%20code%20are%20released.&entry.1838667208=http%3A//arxiv.org/abs/2601.08653v2&entry.124074799=Read"},
{"title": "Exploration vs Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward", "author": "Peter Chen and Xiaopeng Li and Ziniu Li and Wotao Yin and Xi Chen and Tianyi Lin", "abstract": "This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.", "link": "http://arxiv.org/abs/2512.16912v3", "date": "2026-01-26", "relevancy": 2.5983, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5345}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5123}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploration%20vs%20Exploitation%3A%20Rethinking%20RLVR%20through%20Clipping%2C%20Entropy%2C%20and%20Spurious%20Reward&body=Title%3A%20Exploration%20vs%20Exploitation%3A%20Rethinking%20RLVR%20through%20Clipping%2C%20Entropy%2C%20and%20Spurious%20Reward%0AAuthor%3A%20Peter%20Chen%20and%20Xiaopeng%20Li%20and%20Ziniu%20Li%20and%20Wotao%20Yin%20and%20Xi%20Chen%20and%20Tianyi%20Lin%0AAbstract%3A%20This%20paper%20examines%20the%20exploration-exploitation%20trade-off%20in%20reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%2C%20a%20framework%20for%20improving%20the%20reasoning%20of%20Large%20Language%20Models%20%28LLMs%29.%20Recent%20studies%20suggest%20that%20RLVR%20can%20elicit%20strong%20mathematical%20reasoning%20in%20LLMs%20through%20two%20seemingly%20paradoxical%20mechanisms%3A%20spurious%20rewards%2C%20which%20suppress%20exploitation%20by%20rewarding%20outcomes%20unrelated%20to%20the%20ground%20truth%2C%20and%20entropy%20minimization%2C%20which%20suppresses%20exploration%20by%20pushing%20the%20model%20toward%20more%20confident%20and%20deterministic%20outputs%2C%20highlighting%20a%20puzzling%20dynamic%3A%20both%20discouraging%20exploitation%20and%20discouraging%20exploration%20improve%20reasoning%20performance%2C%20yet%20the%20underlying%20principles%20that%20reconcile%20these%20effects%20remain%20poorly%20understood.%20We%20focus%20on%20two%20fundamental%20questions%3A%20%28i%29%20how%20policy%20entropy%20relates%20to%20performance%2C%20and%20%28ii%29%20whether%20spurious%20rewards%20yield%20gains%2C%20potentially%20through%20the%20interplay%20of%20clipping%20bias%20and%20model%20contamination.%20Our%20results%20show%20that%20clipping%20bias%20under%20spurious%20rewards%20reduces%20policy%20entropy%2C%20leading%20to%20more%20confident%20and%20deterministic%20outputs%2C%20while%20entropy%20minimization%20alone%20is%20insufficient%20for%20improvement.%20We%20further%20propose%20a%20reward-misalignment%20model%20explaining%20why%20spurious%20rewards%20can%20enhance%20performance%20beyond%20contaminated%20settings.%20Our%20findings%20clarify%20the%20mechanisms%20behind%20spurious-reward%20benefits%20and%20provide%20principles%20for%20more%20effective%20RLVR%20training.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16912v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploration%2520vs%2520Exploitation%253A%2520Rethinking%2520RLVR%2520through%2520Clipping%252C%2520Entropy%252C%2520and%2520Spurious%2520Reward%26entry.906535625%3DPeter%2520Chen%2520and%2520Xiaopeng%2520Li%2520and%2520Ziniu%2520Li%2520and%2520Wotao%2520Yin%2520and%2520Xi%2520Chen%2520and%2520Tianyi%2520Lin%26entry.1292438233%3DThis%2520paper%2520examines%2520the%2520exploration-exploitation%2520trade-off%2520in%2520reinforcement%2520learning%2520with%2520verifiable%2520rewards%2520%2528RLVR%2529%252C%2520a%2520framework%2520for%2520improving%2520the%2520reasoning%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520Recent%2520studies%2520suggest%2520that%2520RLVR%2520can%2520elicit%2520strong%2520mathematical%2520reasoning%2520in%2520LLMs%2520through%2520two%2520seemingly%2520paradoxical%2520mechanisms%253A%2520spurious%2520rewards%252C%2520which%2520suppress%2520exploitation%2520by%2520rewarding%2520outcomes%2520unrelated%2520to%2520the%2520ground%2520truth%252C%2520and%2520entropy%2520minimization%252C%2520which%2520suppresses%2520exploration%2520by%2520pushing%2520the%2520model%2520toward%2520more%2520confident%2520and%2520deterministic%2520outputs%252C%2520highlighting%2520a%2520puzzling%2520dynamic%253A%2520both%2520discouraging%2520exploitation%2520and%2520discouraging%2520exploration%2520improve%2520reasoning%2520performance%252C%2520yet%2520the%2520underlying%2520principles%2520that%2520reconcile%2520these%2520effects%2520remain%2520poorly%2520understood.%2520We%2520focus%2520on%2520two%2520fundamental%2520questions%253A%2520%2528i%2529%2520how%2520policy%2520entropy%2520relates%2520to%2520performance%252C%2520and%2520%2528ii%2529%2520whether%2520spurious%2520rewards%2520yield%2520gains%252C%2520potentially%2520through%2520the%2520interplay%2520of%2520clipping%2520bias%2520and%2520model%2520contamination.%2520Our%2520results%2520show%2520that%2520clipping%2520bias%2520under%2520spurious%2520rewards%2520reduces%2520policy%2520entropy%252C%2520leading%2520to%2520more%2520confident%2520and%2520deterministic%2520outputs%252C%2520while%2520entropy%2520minimization%2520alone%2520is%2520insufficient%2520for%2520improvement.%2520We%2520further%2520propose%2520a%2520reward-misalignment%2520model%2520explaining%2520why%2520spurious%2520rewards%2520can%2520enhance%2520performance%2520beyond%2520contaminated%2520settings.%2520Our%2520findings%2520clarify%2520the%2520mechanisms%2520behind%2520spurious-reward%2520benefits%2520and%2520provide%2520principles%2520for%2520more%2520effective%2520RLVR%2520training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16912v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploration%20vs%20Exploitation%3A%20Rethinking%20RLVR%20through%20Clipping%2C%20Entropy%2C%20and%20Spurious%20Reward&entry.906535625=Peter%20Chen%20and%20Xiaopeng%20Li%20and%20Ziniu%20Li%20and%20Wotao%20Yin%20and%20Xi%20Chen%20and%20Tianyi%20Lin&entry.1292438233=This%20paper%20examines%20the%20exploration-exploitation%20trade-off%20in%20reinforcement%20learning%20with%20verifiable%20rewards%20%28RLVR%29%2C%20a%20framework%20for%20improving%20the%20reasoning%20of%20Large%20Language%20Models%20%28LLMs%29.%20Recent%20studies%20suggest%20that%20RLVR%20can%20elicit%20strong%20mathematical%20reasoning%20in%20LLMs%20through%20two%20seemingly%20paradoxical%20mechanisms%3A%20spurious%20rewards%2C%20which%20suppress%20exploitation%20by%20rewarding%20outcomes%20unrelated%20to%20the%20ground%20truth%2C%20and%20entropy%20minimization%2C%20which%20suppresses%20exploration%20by%20pushing%20the%20model%20toward%20more%20confident%20and%20deterministic%20outputs%2C%20highlighting%20a%20puzzling%20dynamic%3A%20both%20discouraging%20exploitation%20and%20discouraging%20exploration%20improve%20reasoning%20performance%2C%20yet%20the%20underlying%20principles%20that%20reconcile%20these%20effects%20remain%20poorly%20understood.%20We%20focus%20on%20two%20fundamental%20questions%3A%20%28i%29%20how%20policy%20entropy%20relates%20to%20performance%2C%20and%20%28ii%29%20whether%20spurious%20rewards%20yield%20gains%2C%20potentially%20through%20the%20interplay%20of%20clipping%20bias%20and%20model%20contamination.%20Our%20results%20show%20that%20clipping%20bias%20under%20spurious%20rewards%20reduces%20policy%20entropy%2C%20leading%20to%20more%20confident%20and%20deterministic%20outputs%2C%20while%20entropy%20minimization%20alone%20is%20insufficient%20for%20improvement.%20We%20further%20propose%20a%20reward-misalignment%20model%20explaining%20why%20spurious%20rewards%20can%20enhance%20performance%20beyond%20contaminated%20settings.%20Our%20findings%20clarify%20the%20mechanisms%20behind%20spurious-reward%20benefits%20and%20provide%20principles%20for%20more%20effective%20RLVR%20training.&entry.1838667208=http%3A//arxiv.org/abs/2512.16912v3&entry.124074799=Read"},
{"title": "HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding", "author": "Haowei Zhang and Shudong Yang and Jinlan Fu and See-Kiong Ng and Xipeng Qiu", "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant improvement in offline video understanding. However, extending these capabilities to streaming video inputs, remains challenging, as existing models struggle to simultaneously maintain stable understanding performance, real-time responses, and low GPU memory overhead. To address this challenge, we propose HERMES, a novel training-free architecture for real-time and accurate understanding of video streams. Based on a mechanistic attention investigation, we conceptualize KV cache as a hierarchical memory framework that encapsulates video information across multiple granularities. During inference, HERMES reuses a compact KV cache, enabling efficient streaming understanding under resource constraints. Notably, HERMES requires no auxiliary computations upon the arrival of user queries, thereby guaranteeing real-time responses for continuous video stream interactions, which achieves 10$\\times$ faster TTFT compared to prior SOTA. Even when reducing video tokens by up to 68% compared with uniform sampling, HERMES achieves superior or comparable accuracy across all benchmarks, with up to 11.4% gains on streaming datasets.", "link": "http://arxiv.org/abs/2601.14724v2", "date": "2026-01-26", "relevancy": 2.5842, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5184}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5184}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HERMES%3A%20KV%20Cache%20as%20Hierarchical%20Memory%20for%20Efficient%20Streaming%20Video%20Understanding&body=Title%3A%20HERMES%3A%20KV%20Cache%20as%20Hierarchical%20Memory%20for%20Efficient%20Streaming%20Video%20Understanding%0AAuthor%3A%20Haowei%20Zhang%20and%20Shudong%20Yang%20and%20Jinlan%20Fu%20and%20See-Kiong%20Ng%20and%20Xipeng%20Qiu%0AAbstract%3A%20Recent%20advancements%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20significant%20improvement%20in%20offline%20video%20understanding.%20However%2C%20extending%20these%20capabilities%20to%20streaming%20video%20inputs%2C%20remains%20challenging%2C%20as%20existing%20models%20struggle%20to%20simultaneously%20maintain%20stable%20understanding%20performance%2C%20real-time%20responses%2C%20and%20low%20GPU%20memory%20overhead.%20To%20address%20this%20challenge%2C%20we%20propose%20HERMES%2C%20a%20novel%20training-free%20architecture%20for%20real-time%20and%20accurate%20understanding%20of%20video%20streams.%20Based%20on%20a%20mechanistic%20attention%20investigation%2C%20we%20conceptualize%20KV%20cache%20as%20a%20hierarchical%20memory%20framework%20that%20encapsulates%20video%20information%20across%20multiple%20granularities.%20During%20inference%2C%20HERMES%20reuses%20a%20compact%20KV%20cache%2C%20enabling%20efficient%20streaming%20understanding%20under%20resource%20constraints.%20Notably%2C%20HERMES%20requires%20no%20auxiliary%20computations%20upon%20the%20arrival%20of%20user%20queries%2C%20thereby%20guaranteeing%20real-time%20responses%20for%20continuous%20video%20stream%20interactions%2C%20which%20achieves%2010%24%5Ctimes%24%20faster%20TTFT%20compared%20to%20prior%20SOTA.%20Even%20when%20reducing%20video%20tokens%20by%20up%20to%2068%25%20compared%20with%20uniform%20sampling%2C%20HERMES%20achieves%20superior%20or%20comparable%20accuracy%20across%20all%20benchmarks%2C%20with%20up%20to%2011.4%25%20gains%20on%20streaming%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14724v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHERMES%253A%2520KV%2520Cache%2520as%2520Hierarchical%2520Memory%2520for%2520Efficient%2520Streaming%2520Video%2520Understanding%26entry.906535625%3DHaowei%2520Zhang%2520and%2520Shudong%2520Yang%2520and%2520Jinlan%2520Fu%2520and%2520See-Kiong%2520Ng%2520and%2520Xipeng%2520Qiu%26entry.1292438233%3DRecent%2520advancements%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520significant%2520improvement%2520in%2520offline%2520video%2520understanding.%2520However%252C%2520extending%2520these%2520capabilities%2520to%2520streaming%2520video%2520inputs%252C%2520remains%2520challenging%252C%2520as%2520existing%2520models%2520struggle%2520to%2520simultaneously%2520maintain%2520stable%2520understanding%2520performance%252C%2520real-time%2520responses%252C%2520and%2520low%2520GPU%2520memory%2520overhead.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520HERMES%252C%2520a%2520novel%2520training-free%2520architecture%2520for%2520real-time%2520and%2520accurate%2520understanding%2520of%2520video%2520streams.%2520Based%2520on%2520a%2520mechanistic%2520attention%2520investigation%252C%2520we%2520conceptualize%2520KV%2520cache%2520as%2520a%2520hierarchical%2520memory%2520framework%2520that%2520encapsulates%2520video%2520information%2520across%2520multiple%2520granularities.%2520During%2520inference%252C%2520HERMES%2520reuses%2520a%2520compact%2520KV%2520cache%252C%2520enabling%2520efficient%2520streaming%2520understanding%2520under%2520resource%2520constraints.%2520Notably%252C%2520HERMES%2520requires%2520no%2520auxiliary%2520computations%2520upon%2520the%2520arrival%2520of%2520user%2520queries%252C%2520thereby%2520guaranteeing%2520real-time%2520responses%2520for%2520continuous%2520video%2520stream%2520interactions%252C%2520which%2520achieves%252010%2524%255Ctimes%2524%2520faster%2520TTFT%2520compared%2520to%2520prior%2520SOTA.%2520Even%2520when%2520reducing%2520video%2520tokens%2520by%2520up%2520to%252068%2525%2520compared%2520with%2520uniform%2520sampling%252C%2520HERMES%2520achieves%2520superior%2520or%2520comparable%2520accuracy%2520across%2520all%2520benchmarks%252C%2520with%2520up%2520to%252011.4%2525%2520gains%2520on%2520streaming%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14724v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HERMES%3A%20KV%20Cache%20as%20Hierarchical%20Memory%20for%20Efficient%20Streaming%20Video%20Understanding&entry.906535625=Haowei%20Zhang%20and%20Shudong%20Yang%20and%20Jinlan%20Fu%20and%20See-Kiong%20Ng%20and%20Xipeng%20Qiu&entry.1292438233=Recent%20advancements%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20significant%20improvement%20in%20offline%20video%20understanding.%20However%2C%20extending%20these%20capabilities%20to%20streaming%20video%20inputs%2C%20remains%20challenging%2C%20as%20existing%20models%20struggle%20to%20simultaneously%20maintain%20stable%20understanding%20performance%2C%20real-time%20responses%2C%20and%20low%20GPU%20memory%20overhead.%20To%20address%20this%20challenge%2C%20we%20propose%20HERMES%2C%20a%20novel%20training-free%20architecture%20for%20real-time%20and%20accurate%20understanding%20of%20video%20streams.%20Based%20on%20a%20mechanistic%20attention%20investigation%2C%20we%20conceptualize%20KV%20cache%20as%20a%20hierarchical%20memory%20framework%20that%20encapsulates%20video%20information%20across%20multiple%20granularities.%20During%20inference%2C%20HERMES%20reuses%20a%20compact%20KV%20cache%2C%20enabling%20efficient%20streaming%20understanding%20under%20resource%20constraints.%20Notably%2C%20HERMES%20requires%20no%20auxiliary%20computations%20upon%20the%20arrival%20of%20user%20queries%2C%20thereby%20guaranteeing%20real-time%20responses%20for%20continuous%20video%20stream%20interactions%2C%20which%20achieves%2010%24%5Ctimes%24%20faster%20TTFT%20compared%20to%20prior%20SOTA.%20Even%20when%20reducing%20video%20tokens%20by%20up%20to%2068%25%20compared%20with%20uniform%20sampling%2C%20HERMES%20achieves%20superior%20or%20comparable%20accuracy%20across%20all%20benchmarks%2C%20with%20up%20to%2011.4%25%20gains%20on%20streaming%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2601.14724v2&entry.124074799=Read"},
{"title": "An Unsupervised Tensor-Based Domain Alignment", "author": "Chong Hyun Lee and Kibae Lee and Hyun Hee Yim", "abstract": "We propose a tensor-based domain alignment (DA) algorithm designed to align source and target tensors within an invariant subspace through the use of alignment matrices. These matrices along with the subspace undergo iterative optimization of which constraint is on oblique manifold, which offers greater flexibility and adaptability compared to the traditional Stiefel manifold. Moreover, regularization terms defined to preserve the variance of both source and target tensors, ensures robust performance. Our framework is versatile, effectively generalizing existing tensor-based DA methods as special cases. Through extensive experiments, we demonstrate that our approach not only enhances DA conversion speed but also significantly boosts classification accuracy. This positions our method as superior to current state-of-the-art techniques, making it a preferable choice for complex domain adaptation tasks.", "link": "http://arxiv.org/abs/2601.18564v1", "date": "2026-01-26", "relevancy": 2.5786, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.55}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5098}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Unsupervised%20Tensor-Based%20Domain%20Alignment&body=Title%3A%20An%20Unsupervised%20Tensor-Based%20Domain%20Alignment%0AAuthor%3A%20Chong%20Hyun%20Lee%20and%20Kibae%20Lee%20and%20Hyun%20Hee%20Yim%0AAbstract%3A%20We%20propose%20a%20tensor-based%20domain%20alignment%20%28DA%29%20algorithm%20designed%20to%20align%20source%20and%20target%20tensors%20within%20an%20invariant%20subspace%20through%20the%20use%20of%20alignment%20matrices.%20These%20matrices%20along%20with%20the%20subspace%20undergo%20iterative%20optimization%20of%20which%20constraint%20is%20on%20oblique%20manifold%2C%20which%20offers%20greater%20flexibility%20and%20adaptability%20compared%20to%20the%20traditional%20Stiefel%20manifold.%20Moreover%2C%20regularization%20terms%20defined%20to%20preserve%20the%20variance%20of%20both%20source%20and%20target%20tensors%2C%20ensures%20robust%20performance.%20Our%20framework%20is%20versatile%2C%20effectively%20generalizing%20existing%20tensor-based%20DA%20methods%20as%20special%20cases.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20that%20our%20approach%20not%20only%20enhances%20DA%20conversion%20speed%20but%20also%20significantly%20boosts%20classification%20accuracy.%20This%20positions%20our%20method%20as%20superior%20to%20current%20state-of-the-art%20techniques%2C%20making%20it%20a%20preferable%20choice%20for%20complex%20domain%20adaptation%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Unsupervised%2520Tensor-Based%2520Domain%2520Alignment%26entry.906535625%3DChong%2520Hyun%2520Lee%2520and%2520Kibae%2520Lee%2520and%2520Hyun%2520Hee%2520Yim%26entry.1292438233%3DWe%2520propose%2520a%2520tensor-based%2520domain%2520alignment%2520%2528DA%2529%2520algorithm%2520designed%2520to%2520align%2520source%2520and%2520target%2520tensors%2520within%2520an%2520invariant%2520subspace%2520through%2520the%2520use%2520of%2520alignment%2520matrices.%2520These%2520matrices%2520along%2520with%2520the%2520subspace%2520undergo%2520iterative%2520optimization%2520of%2520which%2520constraint%2520is%2520on%2520oblique%2520manifold%252C%2520which%2520offers%2520greater%2520flexibility%2520and%2520adaptability%2520compared%2520to%2520the%2520traditional%2520Stiefel%2520manifold.%2520Moreover%252C%2520regularization%2520terms%2520defined%2520to%2520preserve%2520the%2520variance%2520of%2520both%2520source%2520and%2520target%2520tensors%252C%2520ensures%2520robust%2520performance.%2520Our%2520framework%2520is%2520versatile%252C%2520effectively%2520generalizing%2520existing%2520tensor-based%2520DA%2520methods%2520as%2520special%2520cases.%2520Through%2520extensive%2520experiments%252C%2520we%2520demonstrate%2520that%2520our%2520approach%2520not%2520only%2520enhances%2520DA%2520conversion%2520speed%2520but%2520also%2520significantly%2520boosts%2520classification%2520accuracy.%2520This%2520positions%2520our%2520method%2520as%2520superior%2520to%2520current%2520state-of-the-art%2520techniques%252C%2520making%2520it%2520a%2520preferable%2520choice%2520for%2520complex%2520domain%2520adaptation%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Unsupervised%20Tensor-Based%20Domain%20Alignment&entry.906535625=Chong%20Hyun%20Lee%20and%20Kibae%20Lee%20and%20Hyun%20Hee%20Yim&entry.1292438233=We%20propose%20a%20tensor-based%20domain%20alignment%20%28DA%29%20algorithm%20designed%20to%20align%20source%20and%20target%20tensors%20within%20an%20invariant%20subspace%20through%20the%20use%20of%20alignment%20matrices.%20These%20matrices%20along%20with%20the%20subspace%20undergo%20iterative%20optimization%20of%20which%20constraint%20is%20on%20oblique%20manifold%2C%20which%20offers%20greater%20flexibility%20and%20adaptability%20compared%20to%20the%20traditional%20Stiefel%20manifold.%20Moreover%2C%20regularization%20terms%20defined%20to%20preserve%20the%20variance%20of%20both%20source%20and%20target%20tensors%2C%20ensures%20robust%20performance.%20Our%20framework%20is%20versatile%2C%20effectively%20generalizing%20existing%20tensor-based%20DA%20methods%20as%20special%20cases.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20that%20our%20approach%20not%20only%20enhances%20DA%20conversion%20speed%20but%20also%20significantly%20boosts%20classification%20accuracy.%20This%20positions%20our%20method%20as%20superior%20to%20current%20state-of-the-art%20techniques%2C%20making%20it%20a%20preferable%20choice%20for%20complex%20domain%20adaptation%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2601.18564v1&entry.124074799=Read"},
{"title": "Gained in Translation: Privileged Pairwise Judges Enhance Multilingual Reasoning", "author": "Lintang Sutawika and Gokul Swamy and Zhiwei Steven Wu and Graham Neubig", "abstract": "When asked a question in a language less seen in its training data, current reasoning large language models (RLMs) often exhibit dramatically lower performance than when asked the same question in English. In response, we introduce \\texttt{SP3F} (Self-Play with Privileged Pairwise Feedback), a two-stage framework for enhancing multilingual reasoning without \\textit{any} data in the target language(s). First, we supervise fine-tune (SFT) on translated versions of English question-answer pairs to raise base model correctness. Second, we perform RL with feedback from a pairwise judge in a self-play fashion, with the judge receiving the English reference response as \\textit{privileged information}. Thus, even when none of the model's responses are completely correct, the privileged pairwise judge can still tell which response is better. End-to-end, \\texttt{SP3F} greatly improves base model performance, even outperforming fully post-trained models on multiple math and non-math tasks with less than\n  of the training data across the single-language, multilingual, and generalization to unseen language settings.", "link": "http://arxiv.org/abs/2601.18722v1", "date": "2026-01-26", "relevancy": 2.5614, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5239}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5239}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gained%20in%20Translation%3A%20Privileged%20Pairwise%20Judges%20Enhance%20Multilingual%20Reasoning&body=Title%3A%20Gained%20in%20Translation%3A%20Privileged%20Pairwise%20Judges%20Enhance%20Multilingual%20Reasoning%0AAuthor%3A%20Lintang%20Sutawika%20and%20Gokul%20Swamy%20and%20Zhiwei%20Steven%20Wu%20and%20Graham%20Neubig%0AAbstract%3A%20When%20asked%20a%20question%20in%20a%20language%20less%20seen%20in%20its%20training%20data%2C%20current%20reasoning%20large%20language%20models%20%28RLMs%29%20often%20exhibit%20dramatically%20lower%20performance%20than%20when%20asked%20the%20same%20question%20in%20English.%20In%20response%2C%20we%20introduce%20%5Ctexttt%7BSP3F%7D%20%28Self-Play%20with%20Privileged%20Pairwise%20Feedback%29%2C%20a%20two-stage%20framework%20for%20enhancing%20multilingual%20reasoning%20without%20%5Ctextit%7Bany%7D%20data%20in%20the%20target%20language%28s%29.%20First%2C%20we%20supervise%20fine-tune%20%28SFT%29%20on%20translated%20versions%20of%20English%20question-answer%20pairs%20to%20raise%20base%20model%20correctness.%20Second%2C%20we%20perform%20RL%20with%20feedback%20from%20a%20pairwise%20judge%20in%20a%20self-play%20fashion%2C%20with%20the%20judge%20receiving%20the%20English%20reference%20response%20as%20%5Ctextit%7Bprivileged%20information%7D.%20Thus%2C%20even%20when%20none%20of%20the%20model%27s%20responses%20are%20completely%20correct%2C%20the%20privileged%20pairwise%20judge%20can%20still%20tell%20which%20response%20is%20better.%20End-to-end%2C%20%5Ctexttt%7BSP3F%7D%20greatly%20improves%20base%20model%20performance%2C%20even%20outperforming%20fully%20post-trained%20models%20on%20multiple%20math%20and%20non-math%20tasks%20with%20less%20than%0A%20%20of%20the%20training%20data%20across%20the%20single-language%2C%20multilingual%2C%20and%20generalization%20to%20unseen%20language%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18722v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGained%2520in%2520Translation%253A%2520Privileged%2520Pairwise%2520Judges%2520Enhance%2520Multilingual%2520Reasoning%26entry.906535625%3DLintang%2520Sutawika%2520and%2520Gokul%2520Swamy%2520and%2520Zhiwei%2520Steven%2520Wu%2520and%2520Graham%2520Neubig%26entry.1292438233%3DWhen%2520asked%2520a%2520question%2520in%2520a%2520language%2520less%2520seen%2520in%2520its%2520training%2520data%252C%2520current%2520reasoning%2520large%2520language%2520models%2520%2528RLMs%2529%2520often%2520exhibit%2520dramatically%2520lower%2520performance%2520than%2520when%2520asked%2520the%2520same%2520question%2520in%2520English.%2520In%2520response%252C%2520we%2520introduce%2520%255Ctexttt%257BSP3F%257D%2520%2528Self-Play%2520with%2520Privileged%2520Pairwise%2520Feedback%2529%252C%2520a%2520two-stage%2520framework%2520for%2520enhancing%2520multilingual%2520reasoning%2520without%2520%255Ctextit%257Bany%257D%2520data%2520in%2520the%2520target%2520language%2528s%2529.%2520First%252C%2520we%2520supervise%2520fine-tune%2520%2528SFT%2529%2520on%2520translated%2520versions%2520of%2520English%2520question-answer%2520pairs%2520to%2520raise%2520base%2520model%2520correctness.%2520Second%252C%2520we%2520perform%2520RL%2520with%2520feedback%2520from%2520a%2520pairwise%2520judge%2520in%2520a%2520self-play%2520fashion%252C%2520with%2520the%2520judge%2520receiving%2520the%2520English%2520reference%2520response%2520as%2520%255Ctextit%257Bprivileged%2520information%257D.%2520Thus%252C%2520even%2520when%2520none%2520of%2520the%2520model%2527s%2520responses%2520are%2520completely%2520correct%252C%2520the%2520privileged%2520pairwise%2520judge%2520can%2520still%2520tell%2520which%2520response%2520is%2520better.%2520End-to-end%252C%2520%255Ctexttt%257BSP3F%257D%2520greatly%2520improves%2520base%2520model%2520performance%252C%2520even%2520outperforming%2520fully%2520post-trained%2520models%2520on%2520multiple%2520math%2520and%2520non-math%2520tasks%2520with%2520less%2520than%250A%2520%2520of%2520the%2520training%2520data%2520across%2520the%2520single-language%252C%2520multilingual%252C%2520and%2520generalization%2520to%2520unseen%2520language%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18722v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gained%20in%20Translation%3A%20Privileged%20Pairwise%20Judges%20Enhance%20Multilingual%20Reasoning&entry.906535625=Lintang%20Sutawika%20and%20Gokul%20Swamy%20and%20Zhiwei%20Steven%20Wu%20and%20Graham%20Neubig&entry.1292438233=When%20asked%20a%20question%20in%20a%20language%20less%20seen%20in%20its%20training%20data%2C%20current%20reasoning%20large%20language%20models%20%28RLMs%29%20often%20exhibit%20dramatically%20lower%20performance%20than%20when%20asked%20the%20same%20question%20in%20English.%20In%20response%2C%20we%20introduce%20%5Ctexttt%7BSP3F%7D%20%28Self-Play%20with%20Privileged%20Pairwise%20Feedback%29%2C%20a%20two-stage%20framework%20for%20enhancing%20multilingual%20reasoning%20without%20%5Ctextit%7Bany%7D%20data%20in%20the%20target%20language%28s%29.%20First%2C%20we%20supervise%20fine-tune%20%28SFT%29%20on%20translated%20versions%20of%20English%20question-answer%20pairs%20to%20raise%20base%20model%20correctness.%20Second%2C%20we%20perform%20RL%20with%20feedback%20from%20a%20pairwise%20judge%20in%20a%20self-play%20fashion%2C%20with%20the%20judge%20receiving%20the%20English%20reference%20response%20as%20%5Ctextit%7Bprivileged%20information%7D.%20Thus%2C%20even%20when%20none%20of%20the%20model%27s%20responses%20are%20completely%20correct%2C%20the%20privileged%20pairwise%20judge%20can%20still%20tell%20which%20response%20is%20better.%20End-to-end%2C%20%5Ctexttt%7BSP3F%7D%20greatly%20improves%20base%20model%20performance%2C%20even%20outperforming%20fully%20post-trained%20models%20on%20multiple%20math%20and%20non-math%20tasks%20with%20less%20than%0A%20%20of%20the%20training%20data%20across%20the%20single-language%2C%20multilingual%2C%20and%20generalization%20to%20unseen%20language%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2601.18722v1&entry.124074799=Read"},
{"title": "Whole Slide Concepts: A Supervised Foundation Model For Pathological Images", "author": "Till Nicke and Daniela Schacherer and Jan Raphael Sch\u00e4fer and Natalia Artysh and Antje Prasse and Andr\u00e9 Homeyer and Andrea Schenk and Henning H\u00f6fener and Johannes Lotz", "abstract": "Foundation models (FMs) are transforming computational pathology by offering new ways to analyze histopathology images. However, FMs typically require weeks of training on large databases, making their creation a resource-intensive process. In this paper, we present a training for foundation models from whole slide images using supervised, end-to-end, multitask learning on slide-level labels. Notably, it is the first model to incorporate cancer subtyping, risk estimation, and genetic mutation prediction into one model. The presented model outperforms self-supervised models on seven benchmark tasks while the training only required 5% of the computational resources. The results not only show that supervised training can outperform self-supervision with less data, but also offer a solution to annotation problems, as patient-based labels are widely available through routine clinical processes. Furthermore, an attention module provides a layer of explainability across different tasks and serves as a tumor detector for unseen cancer types. To address the issue of closed-source datasets, the model was fully trained on openly available data. The code and model weights are made available under https://github.com/FraunhoferMEVIS/MedicalMultitaskModeling.", "link": "http://arxiv.org/abs/2507.05742v3", "date": "2026-01-26", "relevancy": 2.5594, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5405}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4976}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4976}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Whole%20Slide%20Concepts%3A%20A%20Supervised%20Foundation%20Model%20For%20Pathological%20Images&body=Title%3A%20Whole%20Slide%20Concepts%3A%20A%20Supervised%20Foundation%20Model%20For%20Pathological%20Images%0AAuthor%3A%20Till%20Nicke%20and%20Daniela%20Schacherer%20and%20Jan%20Raphael%20Sch%C3%A4fer%20and%20Natalia%20Artysh%20and%20Antje%20Prasse%20and%20Andr%C3%A9%20Homeyer%20and%20Andrea%20Schenk%20and%20Henning%20H%C3%B6fener%20and%20Johannes%20Lotz%0AAbstract%3A%20Foundation%20models%20%28FMs%29%20are%20transforming%20computational%20pathology%20by%20offering%20new%20ways%20to%20analyze%20histopathology%20images.%20However%2C%20FMs%20typically%20require%20weeks%20of%20training%20on%20large%20databases%2C%20making%20their%20creation%20a%20resource-intensive%20process.%20In%20this%20paper%2C%20we%20present%20a%20training%20for%20foundation%20models%20from%20whole%20slide%20images%20using%20supervised%2C%20end-to-end%2C%20multitask%20learning%20on%20slide-level%20labels.%20Notably%2C%20it%20is%20the%20first%20model%20to%20incorporate%20cancer%20subtyping%2C%20risk%20estimation%2C%20and%20genetic%20mutation%20prediction%20into%20one%20model.%20The%20presented%20model%20outperforms%20self-supervised%20models%20on%20seven%20benchmark%20tasks%20while%20the%20training%20only%20required%205%25%20of%20the%20computational%20resources.%20The%20results%20not%20only%20show%20that%20supervised%20training%20can%20outperform%20self-supervision%20with%20less%20data%2C%20but%20also%20offer%20a%20solution%20to%20annotation%20problems%2C%20as%20patient-based%20labels%20are%20widely%20available%20through%20routine%20clinical%20processes.%20Furthermore%2C%20an%20attention%20module%20provides%20a%20layer%20of%20explainability%20across%20different%20tasks%20and%20serves%20as%20a%20tumor%20detector%20for%20unseen%20cancer%20types.%20To%20address%20the%20issue%20of%20closed-source%20datasets%2C%20the%20model%20was%20fully%20trained%20on%20openly%20available%20data.%20The%20code%20and%20model%20weights%20are%20made%20available%20under%20https%3A//github.com/FraunhoferMEVIS/MedicalMultitaskModeling.%0ALink%3A%20http%3A//arxiv.org/abs/2507.05742v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhole%2520Slide%2520Concepts%253A%2520A%2520Supervised%2520Foundation%2520Model%2520For%2520Pathological%2520Images%26entry.906535625%3DTill%2520Nicke%2520and%2520Daniela%2520Schacherer%2520and%2520Jan%2520Raphael%2520Sch%25C3%25A4fer%2520and%2520Natalia%2520Artysh%2520and%2520Antje%2520Prasse%2520and%2520Andr%25C3%25A9%2520Homeyer%2520and%2520Andrea%2520Schenk%2520and%2520Henning%2520H%25C3%25B6fener%2520and%2520Johannes%2520Lotz%26entry.1292438233%3DFoundation%2520models%2520%2528FMs%2529%2520are%2520transforming%2520computational%2520pathology%2520by%2520offering%2520new%2520ways%2520to%2520analyze%2520histopathology%2520images.%2520However%252C%2520FMs%2520typically%2520require%2520weeks%2520of%2520training%2520on%2520large%2520databases%252C%2520making%2520their%2520creation%2520a%2520resource-intensive%2520process.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520training%2520for%2520foundation%2520models%2520from%2520whole%2520slide%2520images%2520using%2520supervised%252C%2520end-to-end%252C%2520multitask%2520learning%2520on%2520slide-level%2520labels.%2520Notably%252C%2520it%2520is%2520the%2520first%2520model%2520to%2520incorporate%2520cancer%2520subtyping%252C%2520risk%2520estimation%252C%2520and%2520genetic%2520mutation%2520prediction%2520into%2520one%2520model.%2520The%2520presented%2520model%2520outperforms%2520self-supervised%2520models%2520on%2520seven%2520benchmark%2520tasks%2520while%2520the%2520training%2520only%2520required%25205%2525%2520of%2520the%2520computational%2520resources.%2520The%2520results%2520not%2520only%2520show%2520that%2520supervised%2520training%2520can%2520outperform%2520self-supervision%2520with%2520less%2520data%252C%2520but%2520also%2520offer%2520a%2520solution%2520to%2520annotation%2520problems%252C%2520as%2520patient-based%2520labels%2520are%2520widely%2520available%2520through%2520routine%2520clinical%2520processes.%2520Furthermore%252C%2520an%2520attention%2520module%2520provides%2520a%2520layer%2520of%2520explainability%2520across%2520different%2520tasks%2520and%2520serves%2520as%2520a%2520tumor%2520detector%2520for%2520unseen%2520cancer%2520types.%2520To%2520address%2520the%2520issue%2520of%2520closed-source%2520datasets%252C%2520the%2520model%2520was%2520fully%2520trained%2520on%2520openly%2520available%2520data.%2520The%2520code%2520and%2520model%2520weights%2520are%2520made%2520available%2520under%2520https%253A//github.com/FraunhoferMEVIS/MedicalMultitaskModeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.05742v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Whole%20Slide%20Concepts%3A%20A%20Supervised%20Foundation%20Model%20For%20Pathological%20Images&entry.906535625=Till%20Nicke%20and%20Daniela%20Schacherer%20and%20Jan%20Raphael%20Sch%C3%A4fer%20and%20Natalia%20Artysh%20and%20Antje%20Prasse%20and%20Andr%C3%A9%20Homeyer%20and%20Andrea%20Schenk%20and%20Henning%20H%C3%B6fener%20and%20Johannes%20Lotz&entry.1292438233=Foundation%20models%20%28FMs%29%20are%20transforming%20computational%20pathology%20by%20offering%20new%20ways%20to%20analyze%20histopathology%20images.%20However%2C%20FMs%20typically%20require%20weeks%20of%20training%20on%20large%20databases%2C%20making%20their%20creation%20a%20resource-intensive%20process.%20In%20this%20paper%2C%20we%20present%20a%20training%20for%20foundation%20models%20from%20whole%20slide%20images%20using%20supervised%2C%20end-to-end%2C%20multitask%20learning%20on%20slide-level%20labels.%20Notably%2C%20it%20is%20the%20first%20model%20to%20incorporate%20cancer%20subtyping%2C%20risk%20estimation%2C%20and%20genetic%20mutation%20prediction%20into%20one%20model.%20The%20presented%20model%20outperforms%20self-supervised%20models%20on%20seven%20benchmark%20tasks%20while%20the%20training%20only%20required%205%25%20of%20the%20computational%20resources.%20The%20results%20not%20only%20show%20that%20supervised%20training%20can%20outperform%20self-supervision%20with%20less%20data%2C%20but%20also%20offer%20a%20solution%20to%20annotation%20problems%2C%20as%20patient-based%20labels%20are%20widely%20available%20through%20routine%20clinical%20processes.%20Furthermore%2C%20an%20attention%20module%20provides%20a%20layer%20of%20explainability%20across%20different%20tasks%20and%20serves%20as%20a%20tumor%20detector%20for%20unseen%20cancer%20types.%20To%20address%20the%20issue%20of%20closed-source%20datasets%2C%20the%20model%20was%20fully%20trained%20on%20openly%20available%20data.%20The%20code%20and%20model%20weights%20are%20made%20available%20under%20https%3A//github.com/FraunhoferMEVIS/MedicalMultitaskModeling.&entry.1838667208=http%3A//arxiv.org/abs/2507.05742v3&entry.124074799=Read"},
{"title": "Dual Optimistic Ascent (PI Control) is the Augmented Lagrangian Method in Disguise", "author": "Juan Ramirez and Simon Lacoste-Julien", "abstract": "Constrained optimization is a powerful framework for enforcing requirements on neural networks. These constrained deep learning problems are typically solved using first-order methods on their min-max Lagrangian formulation, but such approaches often suffer from oscillations and can fail to find all local solutions. While the Augmented Lagrangian method (ALM) addresses these issues, practitioners often favor dual optimistic ascent schemes (PI control) on the standard Lagrangian, which perform well empirically but lack formal guarantees. In this paper, we establish a previously unknown equivalence between these approaches: dual optimistic ascent on the Lagrangian is equivalent to gradient descent-ascent on the Augmented Lagrangian. This finding allows us to transfer the robust theoretical guarantees of the ALM to the dual optimistic setting, proving it converges linearly to all local solutions. Furthermore, the equivalence provides principled guidance for tuning the optimism hyper-parameter. Our work closes a critical gap between the empirical success of dual optimistic methods and their theoretical foundation in the single-step, first-order regime commonly used in constrained deep learning.", "link": "http://arxiv.org/abs/2509.22500v2", "date": "2026-01-26", "relevancy": 2.5438, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5221}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5156}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual%20Optimistic%20Ascent%20%28PI%20Control%29%20is%20the%20Augmented%20Lagrangian%20Method%20in%20Disguise&body=Title%3A%20Dual%20Optimistic%20Ascent%20%28PI%20Control%29%20is%20the%20Augmented%20Lagrangian%20Method%20in%20Disguise%0AAuthor%3A%20Juan%20Ramirez%20and%20Simon%20Lacoste-Julien%0AAbstract%3A%20Constrained%20optimization%20is%20a%20powerful%20framework%20for%20enforcing%20requirements%20on%20neural%20networks.%20These%20constrained%20deep%20learning%20problems%20are%20typically%20solved%20using%20first-order%20methods%20on%20their%20min-max%20Lagrangian%20formulation%2C%20but%20such%20approaches%20often%20suffer%20from%20oscillations%20and%20can%20fail%20to%20find%20all%20local%20solutions.%20While%20the%20Augmented%20Lagrangian%20method%20%28ALM%29%20addresses%20these%20issues%2C%20practitioners%20often%20favor%20dual%20optimistic%20ascent%20schemes%20%28PI%20control%29%20on%20the%20standard%20Lagrangian%2C%20which%20perform%20well%20empirically%20but%20lack%20formal%20guarantees.%20In%20this%20paper%2C%20we%20establish%20a%20previously%20unknown%20equivalence%20between%20these%20approaches%3A%20dual%20optimistic%20ascent%20on%20the%20Lagrangian%20is%20equivalent%20to%20gradient%20descent-ascent%20on%20the%20Augmented%20Lagrangian.%20This%20finding%20allows%20us%20to%20transfer%20the%20robust%20theoretical%20guarantees%20of%20the%20ALM%20to%20the%20dual%20optimistic%20setting%2C%20proving%20it%20converges%20linearly%20to%20all%20local%20solutions.%20Furthermore%2C%20the%20equivalence%20provides%20principled%20guidance%20for%20tuning%20the%20optimism%20hyper-parameter.%20Our%20work%20closes%20a%20critical%20gap%20between%20the%20empirical%20success%20of%20dual%20optimistic%20methods%20and%20their%20theoretical%20foundation%20in%20the%20single-step%2C%20first-order%20regime%20commonly%20used%20in%20constrained%20deep%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2509.22500v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual%2520Optimistic%2520Ascent%2520%2528PI%2520Control%2529%2520is%2520the%2520Augmented%2520Lagrangian%2520Method%2520in%2520Disguise%26entry.906535625%3DJuan%2520Ramirez%2520and%2520Simon%2520Lacoste-Julien%26entry.1292438233%3DConstrained%2520optimization%2520is%2520a%2520powerful%2520framework%2520for%2520enforcing%2520requirements%2520on%2520neural%2520networks.%2520These%2520constrained%2520deep%2520learning%2520problems%2520are%2520typically%2520solved%2520using%2520first-order%2520methods%2520on%2520their%2520min-max%2520Lagrangian%2520formulation%252C%2520but%2520such%2520approaches%2520often%2520suffer%2520from%2520oscillations%2520and%2520can%2520fail%2520to%2520find%2520all%2520local%2520solutions.%2520While%2520the%2520Augmented%2520Lagrangian%2520method%2520%2528ALM%2529%2520addresses%2520these%2520issues%252C%2520practitioners%2520often%2520favor%2520dual%2520optimistic%2520ascent%2520schemes%2520%2528PI%2520control%2529%2520on%2520the%2520standard%2520Lagrangian%252C%2520which%2520perform%2520well%2520empirically%2520but%2520lack%2520formal%2520guarantees.%2520In%2520this%2520paper%252C%2520we%2520establish%2520a%2520previously%2520unknown%2520equivalence%2520between%2520these%2520approaches%253A%2520dual%2520optimistic%2520ascent%2520on%2520the%2520Lagrangian%2520is%2520equivalent%2520to%2520gradient%2520descent-ascent%2520on%2520the%2520Augmented%2520Lagrangian.%2520This%2520finding%2520allows%2520us%2520to%2520transfer%2520the%2520robust%2520theoretical%2520guarantees%2520of%2520the%2520ALM%2520to%2520the%2520dual%2520optimistic%2520setting%252C%2520proving%2520it%2520converges%2520linearly%2520to%2520all%2520local%2520solutions.%2520Furthermore%252C%2520the%2520equivalence%2520provides%2520principled%2520guidance%2520for%2520tuning%2520the%2520optimism%2520hyper-parameter.%2520Our%2520work%2520closes%2520a%2520critical%2520gap%2520between%2520the%2520empirical%2520success%2520of%2520dual%2520optimistic%2520methods%2520and%2520their%2520theoretical%2520foundation%2520in%2520the%2520single-step%252C%2520first-order%2520regime%2520commonly%2520used%2520in%2520constrained%2520deep%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22500v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual%20Optimistic%20Ascent%20%28PI%20Control%29%20is%20the%20Augmented%20Lagrangian%20Method%20in%20Disguise&entry.906535625=Juan%20Ramirez%20and%20Simon%20Lacoste-Julien&entry.1292438233=Constrained%20optimization%20is%20a%20powerful%20framework%20for%20enforcing%20requirements%20on%20neural%20networks.%20These%20constrained%20deep%20learning%20problems%20are%20typically%20solved%20using%20first-order%20methods%20on%20their%20min-max%20Lagrangian%20formulation%2C%20but%20such%20approaches%20often%20suffer%20from%20oscillations%20and%20can%20fail%20to%20find%20all%20local%20solutions.%20While%20the%20Augmented%20Lagrangian%20method%20%28ALM%29%20addresses%20these%20issues%2C%20practitioners%20often%20favor%20dual%20optimistic%20ascent%20schemes%20%28PI%20control%29%20on%20the%20standard%20Lagrangian%2C%20which%20perform%20well%20empirically%20but%20lack%20formal%20guarantees.%20In%20this%20paper%2C%20we%20establish%20a%20previously%20unknown%20equivalence%20between%20these%20approaches%3A%20dual%20optimistic%20ascent%20on%20the%20Lagrangian%20is%20equivalent%20to%20gradient%20descent-ascent%20on%20the%20Augmented%20Lagrangian.%20This%20finding%20allows%20us%20to%20transfer%20the%20robust%20theoretical%20guarantees%20of%20the%20ALM%20to%20the%20dual%20optimistic%20setting%2C%20proving%20it%20converges%20linearly%20to%20all%20local%20solutions.%20Furthermore%2C%20the%20equivalence%20provides%20principled%20guidance%20for%20tuning%20the%20optimism%20hyper-parameter.%20Our%20work%20closes%20a%20critical%20gap%20between%20the%20empirical%20success%20of%20dual%20optimistic%20methods%20and%20their%20theoretical%20foundation%20in%20the%20single-step%2C%20first-order%20regime%20commonly%20used%20in%20constrained%20deep%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2509.22500v2&entry.124074799=Read"},
{"title": "HeadLighter: Disentangling Illumination in Generative 3D Gaussian Heads via Lightstage Captures", "author": "Yating Wang and Yuan Sun and Xuan Wang and Ran Yi and Boyao Zhou and Yipengjing Sun and Hongyu Liu and Yinuo Wang and Lizhuang Ma", "abstract": "Recent 3D-aware head generative models based on 3D Gaussian Splatting achieve real-time, photorealistic and view-consistent head synthesis. However, a fundamental limitation persists: the deep entanglement of illumination and intrinsic appearance prevents controllable relighting. Existing disentanglement methods rely on strong assumptions to enable weakly supervised learning, which restricts their capacity for complex illumination. To address this challenge, we introduce HeadLighter, a novel supervised framework that learns a physically plausible decomposition of appearance and illumination in head generative models. Specifically, we design a dual-branch architecture that separately models lighting-invariant head attributes and physically grounded rendering components. A progressive disentanglement training is employed to gradually inject head appearance priors into the generative architecture, supervised by multi-view images captured under controlled light conditions with a light stage setup. We further introduce a distillation strategy to generate high-quality normals for realistic rendering. Experiments demonstrate that our method preserves high-quality generation and real-time rendering, while simultaneously supporting explicit lighting and viewpoint editing. We will publicly release our code and dataset.", "link": "http://arxiv.org/abs/2601.02103v2", "date": "2026-01-26", "relevancy": 2.5319, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6345}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6345}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HeadLighter%3A%20Disentangling%20Illumination%20in%20Generative%203D%20Gaussian%20Heads%20via%20Lightstage%20Captures&body=Title%3A%20HeadLighter%3A%20Disentangling%20Illumination%20in%20Generative%203D%20Gaussian%20Heads%20via%20Lightstage%20Captures%0AAuthor%3A%20Yating%20Wang%20and%20Yuan%20Sun%20and%20Xuan%20Wang%20and%20Ran%20Yi%20and%20Boyao%20Zhou%20and%20Yipengjing%20Sun%20and%20Hongyu%20Liu%20and%20Yinuo%20Wang%20and%20Lizhuang%20Ma%0AAbstract%3A%20Recent%203D-aware%20head%20generative%20models%20based%20on%203D%20Gaussian%20Splatting%20achieve%20real-time%2C%20photorealistic%20and%20view-consistent%20head%20synthesis.%20However%2C%20a%20fundamental%20limitation%20persists%3A%20the%20deep%20entanglement%20of%20illumination%20and%20intrinsic%20appearance%20prevents%20controllable%20relighting.%20Existing%20disentanglement%20methods%20rely%20on%20strong%20assumptions%20to%20enable%20weakly%20supervised%20learning%2C%20which%20restricts%20their%20capacity%20for%20complex%20illumination.%20To%20address%20this%20challenge%2C%20we%20introduce%20HeadLighter%2C%20a%20novel%20supervised%20framework%20that%20learns%20a%20physically%20plausible%20decomposition%20of%20appearance%20and%20illumination%20in%20head%20generative%20models.%20Specifically%2C%20we%20design%20a%20dual-branch%20architecture%20that%20separately%20models%20lighting-invariant%20head%20attributes%20and%20physically%20grounded%20rendering%20components.%20A%20progressive%20disentanglement%20training%20is%20employed%20to%20gradually%20inject%20head%20appearance%20priors%20into%20the%20generative%20architecture%2C%20supervised%20by%20multi-view%20images%20captured%20under%20controlled%20light%20conditions%20with%20a%20light%20stage%20setup.%20We%20further%20introduce%20a%20distillation%20strategy%20to%20generate%20high-quality%20normals%20for%20realistic%20rendering.%20Experiments%20demonstrate%20that%20our%20method%20preserves%20high-quality%20generation%20and%20real-time%20rendering%2C%20while%20simultaneously%20supporting%20explicit%20lighting%20and%20viewpoint%20editing.%20We%20will%20publicly%20release%20our%20code%20and%20dataset.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02103v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHeadLighter%253A%2520Disentangling%2520Illumination%2520in%2520Generative%25203D%2520Gaussian%2520Heads%2520via%2520Lightstage%2520Captures%26entry.906535625%3DYating%2520Wang%2520and%2520Yuan%2520Sun%2520and%2520Xuan%2520Wang%2520and%2520Ran%2520Yi%2520and%2520Boyao%2520Zhou%2520and%2520Yipengjing%2520Sun%2520and%2520Hongyu%2520Liu%2520and%2520Yinuo%2520Wang%2520and%2520Lizhuang%2520Ma%26entry.1292438233%3DRecent%25203D-aware%2520head%2520generative%2520models%2520based%2520on%25203D%2520Gaussian%2520Splatting%2520achieve%2520real-time%252C%2520photorealistic%2520and%2520view-consistent%2520head%2520synthesis.%2520However%252C%2520a%2520fundamental%2520limitation%2520persists%253A%2520the%2520deep%2520entanglement%2520of%2520illumination%2520and%2520intrinsic%2520appearance%2520prevents%2520controllable%2520relighting.%2520Existing%2520disentanglement%2520methods%2520rely%2520on%2520strong%2520assumptions%2520to%2520enable%2520weakly%2520supervised%2520learning%252C%2520which%2520restricts%2520their%2520capacity%2520for%2520complex%2520illumination.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520HeadLighter%252C%2520a%2520novel%2520supervised%2520framework%2520that%2520learns%2520a%2520physically%2520plausible%2520decomposition%2520of%2520appearance%2520and%2520illumination%2520in%2520head%2520generative%2520models.%2520Specifically%252C%2520we%2520design%2520a%2520dual-branch%2520architecture%2520that%2520separately%2520models%2520lighting-invariant%2520head%2520attributes%2520and%2520physically%2520grounded%2520rendering%2520components.%2520A%2520progressive%2520disentanglement%2520training%2520is%2520employed%2520to%2520gradually%2520inject%2520head%2520appearance%2520priors%2520into%2520the%2520generative%2520architecture%252C%2520supervised%2520by%2520multi-view%2520images%2520captured%2520under%2520controlled%2520light%2520conditions%2520with%2520a%2520light%2520stage%2520setup.%2520We%2520further%2520introduce%2520a%2520distillation%2520strategy%2520to%2520generate%2520high-quality%2520normals%2520for%2520realistic%2520rendering.%2520Experiments%2520demonstrate%2520that%2520our%2520method%2520preserves%2520high-quality%2520generation%2520and%2520real-time%2520rendering%252C%2520while%2520simultaneously%2520supporting%2520explicit%2520lighting%2520and%2520viewpoint%2520editing.%2520We%2520will%2520publicly%2520release%2520our%2520code%2520and%2520dataset.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02103v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HeadLighter%3A%20Disentangling%20Illumination%20in%20Generative%203D%20Gaussian%20Heads%20via%20Lightstage%20Captures&entry.906535625=Yating%20Wang%20and%20Yuan%20Sun%20and%20Xuan%20Wang%20and%20Ran%20Yi%20and%20Boyao%20Zhou%20and%20Yipengjing%20Sun%20and%20Hongyu%20Liu%20and%20Yinuo%20Wang%20and%20Lizhuang%20Ma&entry.1292438233=Recent%203D-aware%20head%20generative%20models%20based%20on%203D%20Gaussian%20Splatting%20achieve%20real-time%2C%20photorealistic%20and%20view-consistent%20head%20synthesis.%20However%2C%20a%20fundamental%20limitation%20persists%3A%20the%20deep%20entanglement%20of%20illumination%20and%20intrinsic%20appearance%20prevents%20controllable%20relighting.%20Existing%20disentanglement%20methods%20rely%20on%20strong%20assumptions%20to%20enable%20weakly%20supervised%20learning%2C%20which%20restricts%20their%20capacity%20for%20complex%20illumination.%20To%20address%20this%20challenge%2C%20we%20introduce%20HeadLighter%2C%20a%20novel%20supervised%20framework%20that%20learns%20a%20physically%20plausible%20decomposition%20of%20appearance%20and%20illumination%20in%20head%20generative%20models.%20Specifically%2C%20we%20design%20a%20dual-branch%20architecture%20that%20separately%20models%20lighting-invariant%20head%20attributes%20and%20physically%20grounded%20rendering%20components.%20A%20progressive%20disentanglement%20training%20is%20employed%20to%20gradually%20inject%20head%20appearance%20priors%20into%20the%20generative%20architecture%2C%20supervised%20by%20multi-view%20images%20captured%20under%20controlled%20light%20conditions%20with%20a%20light%20stage%20setup.%20We%20further%20introduce%20a%20distillation%20strategy%20to%20generate%20high-quality%20normals%20for%20realistic%20rendering.%20Experiments%20demonstrate%20that%20our%20method%20preserves%20high-quality%20generation%20and%20real-time%20rendering%2C%20while%20simultaneously%20supporting%20explicit%20lighting%20and%20viewpoint%20editing.%20We%20will%20publicly%20release%20our%20code%20and%20dataset.&entry.1838667208=http%3A//arxiv.org/abs/2601.02103v2&entry.124074799=Read"},
{"title": "ABCD-LINK: Annotation Bootstrapping for Cross-Document Fine-Grained Links", "author": "Serwar Basch and Ilia Kuznetsov and Tom Hope and Iryna Gurevych", "abstract": "Understanding fine-grained links between documents is crucial for many applications, yet progress is limited by the lack of efficient methods for data curation. To address this limitation, we introduce a domain-agnostic framework for bootstrapping sentence-level cross-document links from scratch. Our approach (1) generates and validates semi-synthetic datasets of linked documents, (2) uses these datasets to benchmark and shortlist the best-performing linking approaches, and (3) applies the shortlisted methods in large-scale human-in-the-loop annotation of natural text pairs. We apply the framework in two distinct domains -- peer review and news -- and show that combining retrieval models with LLMs achieves a 73% human approval rate for suggested links, more than doubling the acceptance of strong retrievers alone. Our framework allows users to produce novel datasets that enable systematic study of cross-document understanding, supporting downstream tasks such as media framing analysis and peer review assessment. All code, data, and annotation protocols are released to facilitate future research.", "link": "http://arxiv.org/abs/2509.01387v2", "date": "2026-01-26", "relevancy": 2.5237, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5066}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5066}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ABCD-LINK%3A%20Annotation%20Bootstrapping%20for%20Cross-Document%20Fine-Grained%20Links&body=Title%3A%20ABCD-LINK%3A%20Annotation%20Bootstrapping%20for%20Cross-Document%20Fine-Grained%20Links%0AAuthor%3A%20Serwar%20Basch%20and%20Ilia%20Kuznetsov%20and%20Tom%20Hope%20and%20Iryna%20Gurevych%0AAbstract%3A%20Understanding%20fine-grained%20links%20between%20documents%20is%20crucial%20for%20many%20applications%2C%20yet%20progress%20is%20limited%20by%20the%20lack%20of%20efficient%20methods%20for%20data%20curation.%20To%20address%20this%20limitation%2C%20we%20introduce%20a%20domain-agnostic%20framework%20for%20bootstrapping%20sentence-level%20cross-document%20links%20from%20scratch.%20Our%20approach%20%281%29%20generates%20and%20validates%20semi-synthetic%20datasets%20of%20linked%20documents%2C%20%282%29%20uses%20these%20datasets%20to%20benchmark%20and%20shortlist%20the%20best-performing%20linking%20approaches%2C%20and%20%283%29%20applies%20the%20shortlisted%20methods%20in%20large-scale%20human-in-the-loop%20annotation%20of%20natural%20text%20pairs.%20We%20apply%20the%20framework%20in%20two%20distinct%20domains%20--%20peer%20review%20and%20news%20--%20and%20show%20that%20combining%20retrieval%20models%20with%20LLMs%20achieves%20a%2073%25%20human%20approval%20rate%20for%20suggested%20links%2C%20more%20than%20doubling%20the%20acceptance%20of%20strong%20retrievers%20alone.%20Our%20framework%20allows%20users%20to%20produce%20novel%20datasets%20that%20enable%20systematic%20study%20of%20cross-document%20understanding%2C%20supporting%20downstream%20tasks%20such%20as%20media%20framing%20analysis%20and%20peer%20review%20assessment.%20All%20code%2C%20data%2C%20and%20annotation%20protocols%20are%20released%20to%20facilitate%20future%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2509.01387v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DABCD-LINK%253A%2520Annotation%2520Bootstrapping%2520for%2520Cross-Document%2520Fine-Grained%2520Links%26entry.906535625%3DSerwar%2520Basch%2520and%2520Ilia%2520Kuznetsov%2520and%2520Tom%2520Hope%2520and%2520Iryna%2520Gurevych%26entry.1292438233%3DUnderstanding%2520fine-grained%2520links%2520between%2520documents%2520is%2520crucial%2520for%2520many%2520applications%252C%2520yet%2520progress%2520is%2520limited%2520by%2520the%2520lack%2520of%2520efficient%2520methods%2520for%2520data%2520curation.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%2520a%2520domain-agnostic%2520framework%2520for%2520bootstrapping%2520sentence-level%2520cross-document%2520links%2520from%2520scratch.%2520Our%2520approach%2520%25281%2529%2520generates%2520and%2520validates%2520semi-synthetic%2520datasets%2520of%2520linked%2520documents%252C%2520%25282%2529%2520uses%2520these%2520datasets%2520to%2520benchmark%2520and%2520shortlist%2520the%2520best-performing%2520linking%2520approaches%252C%2520and%2520%25283%2529%2520applies%2520the%2520shortlisted%2520methods%2520in%2520large-scale%2520human-in-the-loop%2520annotation%2520of%2520natural%2520text%2520pairs.%2520We%2520apply%2520the%2520framework%2520in%2520two%2520distinct%2520domains%2520--%2520peer%2520review%2520and%2520news%2520--%2520and%2520show%2520that%2520combining%2520retrieval%2520models%2520with%2520LLMs%2520achieves%2520a%252073%2525%2520human%2520approval%2520rate%2520for%2520suggested%2520links%252C%2520more%2520than%2520doubling%2520the%2520acceptance%2520of%2520strong%2520retrievers%2520alone.%2520Our%2520framework%2520allows%2520users%2520to%2520produce%2520novel%2520datasets%2520that%2520enable%2520systematic%2520study%2520of%2520cross-document%2520understanding%252C%2520supporting%2520downstream%2520tasks%2520such%2520as%2520media%2520framing%2520analysis%2520and%2520peer%2520review%2520assessment.%2520All%2520code%252C%2520data%252C%2520and%2520annotation%2520protocols%2520are%2520released%2520to%2520facilitate%2520future%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.01387v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ABCD-LINK%3A%20Annotation%20Bootstrapping%20for%20Cross-Document%20Fine-Grained%20Links&entry.906535625=Serwar%20Basch%20and%20Ilia%20Kuznetsov%20and%20Tom%20Hope%20and%20Iryna%20Gurevych&entry.1292438233=Understanding%20fine-grained%20links%20between%20documents%20is%20crucial%20for%20many%20applications%2C%20yet%20progress%20is%20limited%20by%20the%20lack%20of%20efficient%20methods%20for%20data%20curation.%20To%20address%20this%20limitation%2C%20we%20introduce%20a%20domain-agnostic%20framework%20for%20bootstrapping%20sentence-level%20cross-document%20links%20from%20scratch.%20Our%20approach%20%281%29%20generates%20and%20validates%20semi-synthetic%20datasets%20of%20linked%20documents%2C%20%282%29%20uses%20these%20datasets%20to%20benchmark%20and%20shortlist%20the%20best-performing%20linking%20approaches%2C%20and%20%283%29%20applies%20the%20shortlisted%20methods%20in%20large-scale%20human-in-the-loop%20annotation%20of%20natural%20text%20pairs.%20We%20apply%20the%20framework%20in%20two%20distinct%20domains%20--%20peer%20review%20and%20news%20--%20and%20show%20that%20combining%20retrieval%20models%20with%20LLMs%20achieves%20a%2073%25%20human%20approval%20rate%20for%20suggested%20links%2C%20more%20than%20doubling%20the%20acceptance%20of%20strong%20retrievers%20alone.%20Our%20framework%20allows%20users%20to%20produce%20novel%20datasets%20that%20enable%20systematic%20study%20of%20cross-document%20understanding%2C%20supporting%20downstream%20tasks%20such%20as%20media%20framing%20analysis%20and%20peer%20review%20assessment.%20All%20code%2C%20data%2C%20and%20annotation%20protocols%20are%20released%20to%20facilitate%20future%20research.&entry.1838667208=http%3A//arxiv.org/abs/2509.01387v2&entry.124074799=Read"},
{"title": "Analytic Incremental Learning For Sound Source Localization With Imbalance Rectification", "author": "Zexia Fan and Yu Chen and Qiquan Zhang and Kainan Chen and Xinyuan Qian", "abstract": "Sound source localization (SSL) demonstrates remarkable results in controlled settings but struggles in real-world deployment due to dual imbalance challenges: intra-task imbalance arising from long-tailed direction-of-arrival (DoA) distributions, and inter-task imbalance induced by cross-task skews and overlaps. These often lead to catastrophic forgetting, significantly degrading the localization accuracy. To mitigate these issues, we propose a unified framework with two key innovations. Specifically, we design a GCC-PHAT-based data augmentation (GDA) method that leverages peak characteristics to alleviate intra-task distribution skews. We also propose an Analytic dynamic imbalance rectifier (ADIR) with task-adaption regularization, which enables analytic updates that adapt to inter-task dynamics. On the SSLR benchmark, our proposal achieves state-of-the-art (SoTA) results of 89.0% accuracy, 5.3\u00b0 mean absolute error, and 1.6 backward transfer, demonstrating robustness to evolving imbalances without exemplar storage.", "link": "http://arxiv.org/abs/2601.18335v1", "date": "2026-01-26", "relevancy": 2.5229, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5226}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5091}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4821}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analytic%20Incremental%20Learning%20For%20Sound%20Source%20Localization%20With%20Imbalance%20Rectification&body=Title%3A%20Analytic%20Incremental%20Learning%20For%20Sound%20Source%20Localization%20With%20Imbalance%20Rectification%0AAuthor%3A%20Zexia%20Fan%20and%20Yu%20Chen%20and%20Qiquan%20Zhang%20and%20Kainan%20Chen%20and%20Xinyuan%20Qian%0AAbstract%3A%20Sound%20source%20localization%20%28SSL%29%20demonstrates%20remarkable%20results%20in%20controlled%20settings%20but%20struggles%20in%20real-world%20deployment%20due%20to%20dual%20imbalance%20challenges%3A%20intra-task%20imbalance%20arising%20from%20long-tailed%20direction-of-arrival%20%28DoA%29%20distributions%2C%20and%20inter-task%20imbalance%20induced%20by%20cross-task%20skews%20and%20overlaps.%20These%20often%20lead%20to%20catastrophic%20forgetting%2C%20significantly%20degrading%20the%20localization%20accuracy.%20To%20mitigate%20these%20issues%2C%20we%20propose%20a%20unified%20framework%20with%20two%20key%20innovations.%20Specifically%2C%20we%20design%20a%20GCC-PHAT-based%20data%20augmentation%20%28GDA%29%20method%20that%20leverages%20peak%20characteristics%20to%20alleviate%20intra-task%20distribution%20skews.%20We%20also%20propose%20an%20Analytic%20dynamic%20imbalance%20rectifier%20%28ADIR%29%20with%20task-adaption%20regularization%2C%20which%20enables%20analytic%20updates%20that%20adapt%20to%20inter-task%20dynamics.%20On%20the%20SSLR%20benchmark%2C%20our%20proposal%20achieves%20state-of-the-art%20%28SoTA%29%20results%20of%2089.0%25%20accuracy%2C%205.3%C2%B0%20mean%20absolute%20error%2C%20and%201.6%20backward%20transfer%2C%20demonstrating%20robustness%20to%20evolving%20imbalances%20without%20exemplar%20storage.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18335v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalytic%2520Incremental%2520Learning%2520For%2520Sound%2520Source%2520Localization%2520With%2520Imbalance%2520Rectification%26entry.906535625%3DZexia%2520Fan%2520and%2520Yu%2520Chen%2520and%2520Qiquan%2520Zhang%2520and%2520Kainan%2520Chen%2520and%2520Xinyuan%2520Qian%26entry.1292438233%3DSound%2520source%2520localization%2520%2528SSL%2529%2520demonstrates%2520remarkable%2520results%2520in%2520controlled%2520settings%2520but%2520struggles%2520in%2520real-world%2520deployment%2520due%2520to%2520dual%2520imbalance%2520challenges%253A%2520intra-task%2520imbalance%2520arising%2520from%2520long-tailed%2520direction-of-arrival%2520%2528DoA%2529%2520distributions%252C%2520and%2520inter-task%2520imbalance%2520induced%2520by%2520cross-task%2520skews%2520and%2520overlaps.%2520These%2520often%2520lead%2520to%2520catastrophic%2520forgetting%252C%2520significantly%2520degrading%2520the%2520localization%2520accuracy.%2520To%2520mitigate%2520these%2520issues%252C%2520we%2520propose%2520a%2520unified%2520framework%2520with%2520two%2520key%2520innovations.%2520Specifically%252C%2520we%2520design%2520a%2520GCC-PHAT-based%2520data%2520augmentation%2520%2528GDA%2529%2520method%2520that%2520leverages%2520peak%2520characteristics%2520to%2520alleviate%2520intra-task%2520distribution%2520skews.%2520We%2520also%2520propose%2520an%2520Analytic%2520dynamic%2520imbalance%2520rectifier%2520%2528ADIR%2529%2520with%2520task-adaption%2520regularization%252C%2520which%2520enables%2520analytic%2520updates%2520that%2520adapt%2520to%2520inter-task%2520dynamics.%2520On%2520the%2520SSLR%2520benchmark%252C%2520our%2520proposal%2520achieves%2520state-of-the-art%2520%2528SoTA%2529%2520results%2520of%252089.0%2525%2520accuracy%252C%25205.3%25C2%25B0%2520mean%2520absolute%2520error%252C%2520and%25201.6%2520backward%2520transfer%252C%2520demonstrating%2520robustness%2520to%2520evolving%2520imbalances%2520without%2520exemplar%2520storage.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18335v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analytic%20Incremental%20Learning%20For%20Sound%20Source%20Localization%20With%20Imbalance%20Rectification&entry.906535625=Zexia%20Fan%20and%20Yu%20Chen%20and%20Qiquan%20Zhang%20and%20Kainan%20Chen%20and%20Xinyuan%20Qian&entry.1292438233=Sound%20source%20localization%20%28SSL%29%20demonstrates%20remarkable%20results%20in%20controlled%20settings%20but%20struggles%20in%20real-world%20deployment%20due%20to%20dual%20imbalance%20challenges%3A%20intra-task%20imbalance%20arising%20from%20long-tailed%20direction-of-arrival%20%28DoA%29%20distributions%2C%20and%20inter-task%20imbalance%20induced%20by%20cross-task%20skews%20and%20overlaps.%20These%20often%20lead%20to%20catastrophic%20forgetting%2C%20significantly%20degrading%20the%20localization%20accuracy.%20To%20mitigate%20these%20issues%2C%20we%20propose%20a%20unified%20framework%20with%20two%20key%20innovations.%20Specifically%2C%20we%20design%20a%20GCC-PHAT-based%20data%20augmentation%20%28GDA%29%20method%20that%20leverages%20peak%20characteristics%20to%20alleviate%20intra-task%20distribution%20skews.%20We%20also%20propose%20an%20Analytic%20dynamic%20imbalance%20rectifier%20%28ADIR%29%20with%20task-adaption%20regularization%2C%20which%20enables%20analytic%20updates%20that%20adapt%20to%20inter-task%20dynamics.%20On%20the%20SSLR%20benchmark%2C%20our%20proposal%20achieves%20state-of-the-art%20%28SoTA%29%20results%20of%2089.0%25%20accuracy%2C%205.3%C2%B0%20mean%20absolute%20error%2C%20and%201.6%20backward%20transfer%2C%20demonstrating%20robustness%20to%20evolving%20imbalances%20without%20exemplar%20storage.&entry.1838667208=http%3A//arxiv.org/abs/2601.18335v1&entry.124074799=Read"},
{"title": "ctELM: Decoding and Manipulating Embeddings of Clinical Trials with Embedding Language Models", "author": "Brian Ondov and Chia-Hsuan Chang and Yujia Zhou and Mauro Giuffr\u00e8 and Hua Xu", "abstract": "Text embeddings have become an essential part of a variety of language applications. However, methods for interpreting, exploring and reversing embedding spaces are limited, reducing transparency and precluding potentially valuable generative use cases. In this work, we align Large Language Models to embeddings of clinical trials using the recently reported Embedding Language Model (ELM) method. We develop an open-source, domain-agnostic ELM architecture and training framework, design training tasks for clinical trials, and introduce an expert-validated synthetic dataset. We then train a series of ELMs exploring the impact of tasks and training regimes. Our final model, ctELM, can accurately describe and compare unseen clinical trials from embeddings alone and produce plausible clinical trials from novel vectors. We further show that generated trial abstracts are responsive to moving embeddings along concept vectors for age and sex of study subjects. Our public ELM implementation and experimental results will aid the alignment of Large Language Models to embedding spaces in the biomedical domain and beyond.", "link": "http://arxiv.org/abs/2601.18796v1", "date": "2026-01-26", "relevancy": 2.5204, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5096}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5014}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ctELM%3A%20Decoding%20and%20Manipulating%20Embeddings%20of%20Clinical%20Trials%20with%20Embedding%20Language%20Models&body=Title%3A%20ctELM%3A%20Decoding%20and%20Manipulating%20Embeddings%20of%20Clinical%20Trials%20with%20Embedding%20Language%20Models%0AAuthor%3A%20Brian%20Ondov%20and%20Chia-Hsuan%20Chang%20and%20Yujia%20Zhou%20and%20Mauro%20Giuffr%C3%A8%20and%20Hua%20Xu%0AAbstract%3A%20Text%20embeddings%20have%20become%20an%20essential%20part%20of%20a%20variety%20of%20language%20applications.%20However%2C%20methods%20for%20interpreting%2C%20exploring%20and%20reversing%20embedding%20spaces%20are%20limited%2C%20reducing%20transparency%20and%20precluding%20potentially%20valuable%20generative%20use%20cases.%20In%20this%20work%2C%20we%20align%20Large%20Language%20Models%20to%20embeddings%20of%20clinical%20trials%20using%20the%20recently%20reported%20Embedding%20Language%20Model%20%28ELM%29%20method.%20We%20develop%20an%20open-source%2C%20domain-agnostic%20ELM%20architecture%20and%20training%20framework%2C%20design%20training%20tasks%20for%20clinical%20trials%2C%20and%20introduce%20an%20expert-validated%20synthetic%20dataset.%20We%20then%20train%20a%20series%20of%20ELMs%20exploring%20the%20impact%20of%20tasks%20and%20training%20regimes.%20Our%20final%20model%2C%20ctELM%2C%20can%20accurately%20describe%20and%20compare%20unseen%20clinical%20trials%20from%20embeddings%20alone%20and%20produce%20plausible%20clinical%20trials%20from%20novel%20vectors.%20We%20further%20show%20that%20generated%20trial%20abstracts%20are%20responsive%20to%20moving%20embeddings%20along%20concept%20vectors%20for%20age%20and%20sex%20of%20study%20subjects.%20Our%20public%20ELM%20implementation%20and%20experimental%20results%20will%20aid%20the%20alignment%20of%20Large%20Language%20Models%20to%20embedding%20spaces%20in%20the%20biomedical%20domain%20and%20beyond.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18796v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DctELM%253A%2520Decoding%2520and%2520Manipulating%2520Embeddings%2520of%2520Clinical%2520Trials%2520with%2520Embedding%2520Language%2520Models%26entry.906535625%3DBrian%2520Ondov%2520and%2520Chia-Hsuan%2520Chang%2520and%2520Yujia%2520Zhou%2520and%2520Mauro%2520Giuffr%25C3%25A8%2520and%2520Hua%2520Xu%26entry.1292438233%3DText%2520embeddings%2520have%2520become%2520an%2520essential%2520part%2520of%2520a%2520variety%2520of%2520language%2520applications.%2520However%252C%2520methods%2520for%2520interpreting%252C%2520exploring%2520and%2520reversing%2520embedding%2520spaces%2520are%2520limited%252C%2520reducing%2520transparency%2520and%2520precluding%2520potentially%2520valuable%2520generative%2520use%2520cases.%2520In%2520this%2520work%252C%2520we%2520align%2520Large%2520Language%2520Models%2520to%2520embeddings%2520of%2520clinical%2520trials%2520using%2520the%2520recently%2520reported%2520Embedding%2520Language%2520Model%2520%2528ELM%2529%2520method.%2520We%2520develop%2520an%2520open-source%252C%2520domain-agnostic%2520ELM%2520architecture%2520and%2520training%2520framework%252C%2520design%2520training%2520tasks%2520for%2520clinical%2520trials%252C%2520and%2520introduce%2520an%2520expert-validated%2520synthetic%2520dataset.%2520We%2520then%2520train%2520a%2520series%2520of%2520ELMs%2520exploring%2520the%2520impact%2520of%2520tasks%2520and%2520training%2520regimes.%2520Our%2520final%2520model%252C%2520ctELM%252C%2520can%2520accurately%2520describe%2520and%2520compare%2520unseen%2520clinical%2520trials%2520from%2520embeddings%2520alone%2520and%2520produce%2520plausible%2520clinical%2520trials%2520from%2520novel%2520vectors.%2520We%2520further%2520show%2520that%2520generated%2520trial%2520abstracts%2520are%2520responsive%2520to%2520moving%2520embeddings%2520along%2520concept%2520vectors%2520for%2520age%2520and%2520sex%2520of%2520study%2520subjects.%2520Our%2520public%2520ELM%2520implementation%2520and%2520experimental%2520results%2520will%2520aid%2520the%2520alignment%2520of%2520Large%2520Language%2520Models%2520to%2520embedding%2520spaces%2520in%2520the%2520biomedical%2520domain%2520and%2520beyond.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18796v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ctELM%3A%20Decoding%20and%20Manipulating%20Embeddings%20of%20Clinical%20Trials%20with%20Embedding%20Language%20Models&entry.906535625=Brian%20Ondov%20and%20Chia-Hsuan%20Chang%20and%20Yujia%20Zhou%20and%20Mauro%20Giuffr%C3%A8%20and%20Hua%20Xu&entry.1292438233=Text%20embeddings%20have%20become%20an%20essential%20part%20of%20a%20variety%20of%20language%20applications.%20However%2C%20methods%20for%20interpreting%2C%20exploring%20and%20reversing%20embedding%20spaces%20are%20limited%2C%20reducing%20transparency%20and%20precluding%20potentially%20valuable%20generative%20use%20cases.%20In%20this%20work%2C%20we%20align%20Large%20Language%20Models%20to%20embeddings%20of%20clinical%20trials%20using%20the%20recently%20reported%20Embedding%20Language%20Model%20%28ELM%29%20method.%20We%20develop%20an%20open-source%2C%20domain-agnostic%20ELM%20architecture%20and%20training%20framework%2C%20design%20training%20tasks%20for%20clinical%20trials%2C%20and%20introduce%20an%20expert-validated%20synthetic%20dataset.%20We%20then%20train%20a%20series%20of%20ELMs%20exploring%20the%20impact%20of%20tasks%20and%20training%20regimes.%20Our%20final%20model%2C%20ctELM%2C%20can%20accurately%20describe%20and%20compare%20unseen%20clinical%20trials%20from%20embeddings%20alone%20and%20produce%20plausible%20clinical%20trials%20from%20novel%20vectors.%20We%20further%20show%20that%20generated%20trial%20abstracts%20are%20responsive%20to%20moving%20embeddings%20along%20concept%20vectors%20for%20age%20and%20sex%20of%20study%20subjects.%20Our%20public%20ELM%20implementation%20and%20experimental%20results%20will%20aid%20the%20alignment%20of%20Large%20Language%20Models%20to%20embedding%20spaces%20in%20the%20biomedical%20domain%20and%20beyond.&entry.1838667208=http%3A//arxiv.org/abs/2601.18796v1&entry.124074799=Read"},
{"title": "Next Token Knowledge Tracing: Exploiting Pretrained LLM Representations to Decode Student Behaviour", "author": "Max Norris and Kobi Gal and Sahan Bulathwela", "abstract": "Modelling student knowledge is a key challenge when leveraging AI in education, with major implications for personalised learning. The Knowledge Tracing (KT) task aims to predict how students will respond to educational questions in learning environments, based on their prior interactions. Existing KT models typically use response correctness along with metadata like skill tags and timestamps, often overlooking the question text, which is an important source of pedagogical insight. This omission poses a lost opportunity while limiting predictive performance. We propose Next Token Knowledge Tracing (NTKT), a novel approach that reframes KT as a next-token prediction task using pretrained Large Language Models (LLMs). NTKT represents both student histories and question content as sequences of text, allowing LLMs to learn patterns in both behaviour and language. Our series of experiments significantly improves performance over state-of-the-art neural KT models and generalises much better to cold-start questions and users. These findings highlight the importance of question content in KT and demonstrate the benefits of leveraging pretrained representations of LLMs to model student learning more effectively.", "link": "http://arxiv.org/abs/2511.02599v2", "date": "2026-01-26", "relevancy": 2.4778, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4934}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Next%20Token%20Knowledge%20Tracing%3A%20Exploiting%20Pretrained%20LLM%20Representations%20to%20Decode%20Student%20Behaviour&body=Title%3A%20Next%20Token%20Knowledge%20Tracing%3A%20Exploiting%20Pretrained%20LLM%20Representations%20to%20Decode%20Student%20Behaviour%0AAuthor%3A%20Max%20Norris%20and%20Kobi%20Gal%20and%20Sahan%20Bulathwela%0AAbstract%3A%20Modelling%20student%20knowledge%20is%20a%20key%20challenge%20when%20leveraging%20AI%20in%20education%2C%20with%20major%20implications%20for%20personalised%20learning.%20The%20Knowledge%20Tracing%20%28KT%29%20task%20aims%20to%20predict%20how%20students%20will%20respond%20to%20educational%20questions%20in%20learning%20environments%2C%20based%20on%20their%20prior%20interactions.%20Existing%20KT%20models%20typically%20use%20response%20correctness%20along%20with%20metadata%20like%20skill%20tags%20and%20timestamps%2C%20often%20overlooking%20the%20question%20text%2C%20which%20is%20an%20important%20source%20of%20pedagogical%20insight.%20This%20omission%20poses%20a%20lost%20opportunity%20while%20limiting%20predictive%20performance.%20We%20propose%20Next%20Token%20Knowledge%20Tracing%20%28NTKT%29%2C%20a%20novel%20approach%20that%20reframes%20KT%20as%20a%20next-token%20prediction%20task%20using%20pretrained%20Large%20Language%20Models%20%28LLMs%29.%20NTKT%20represents%20both%20student%20histories%20and%20question%20content%20as%20sequences%20of%20text%2C%20allowing%20LLMs%20to%20learn%20patterns%20in%20both%20behaviour%20and%20language.%20Our%20series%20of%20experiments%20significantly%20improves%20performance%20over%20state-of-the-art%20neural%20KT%20models%20and%20generalises%20much%20better%20to%20cold-start%20questions%20and%20users.%20These%20findings%20highlight%20the%20importance%20of%20question%20content%20in%20KT%20and%20demonstrate%20the%20benefits%20of%20leveraging%20pretrained%20representations%20of%20LLMs%20to%20model%20student%20learning%20more%20effectively.%0ALink%3A%20http%3A//arxiv.org/abs/2511.02599v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNext%2520Token%2520Knowledge%2520Tracing%253A%2520Exploiting%2520Pretrained%2520LLM%2520Representations%2520to%2520Decode%2520Student%2520Behaviour%26entry.906535625%3DMax%2520Norris%2520and%2520Kobi%2520Gal%2520and%2520Sahan%2520Bulathwela%26entry.1292438233%3DModelling%2520student%2520knowledge%2520is%2520a%2520key%2520challenge%2520when%2520leveraging%2520AI%2520in%2520education%252C%2520with%2520major%2520implications%2520for%2520personalised%2520learning.%2520The%2520Knowledge%2520Tracing%2520%2528KT%2529%2520task%2520aims%2520to%2520predict%2520how%2520students%2520will%2520respond%2520to%2520educational%2520questions%2520in%2520learning%2520environments%252C%2520based%2520on%2520their%2520prior%2520interactions.%2520Existing%2520KT%2520models%2520typically%2520use%2520response%2520correctness%2520along%2520with%2520metadata%2520like%2520skill%2520tags%2520and%2520timestamps%252C%2520often%2520overlooking%2520the%2520question%2520text%252C%2520which%2520is%2520an%2520important%2520source%2520of%2520pedagogical%2520insight.%2520This%2520omission%2520poses%2520a%2520lost%2520opportunity%2520while%2520limiting%2520predictive%2520performance.%2520We%2520propose%2520Next%2520Token%2520Knowledge%2520Tracing%2520%2528NTKT%2529%252C%2520a%2520novel%2520approach%2520that%2520reframes%2520KT%2520as%2520a%2520next-token%2520prediction%2520task%2520using%2520pretrained%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520NTKT%2520represents%2520both%2520student%2520histories%2520and%2520question%2520content%2520as%2520sequences%2520of%2520text%252C%2520allowing%2520LLMs%2520to%2520learn%2520patterns%2520in%2520both%2520behaviour%2520and%2520language.%2520Our%2520series%2520of%2520experiments%2520significantly%2520improves%2520performance%2520over%2520state-of-the-art%2520neural%2520KT%2520models%2520and%2520generalises%2520much%2520better%2520to%2520cold-start%2520questions%2520and%2520users.%2520These%2520findings%2520highlight%2520the%2520importance%2520of%2520question%2520content%2520in%2520KT%2520and%2520demonstrate%2520the%2520benefits%2520of%2520leveraging%2520pretrained%2520representations%2520of%2520LLMs%2520to%2520model%2520student%2520learning%2520more%2520effectively.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.02599v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Next%20Token%20Knowledge%20Tracing%3A%20Exploiting%20Pretrained%20LLM%20Representations%20to%20Decode%20Student%20Behaviour&entry.906535625=Max%20Norris%20and%20Kobi%20Gal%20and%20Sahan%20Bulathwela&entry.1292438233=Modelling%20student%20knowledge%20is%20a%20key%20challenge%20when%20leveraging%20AI%20in%20education%2C%20with%20major%20implications%20for%20personalised%20learning.%20The%20Knowledge%20Tracing%20%28KT%29%20task%20aims%20to%20predict%20how%20students%20will%20respond%20to%20educational%20questions%20in%20learning%20environments%2C%20based%20on%20their%20prior%20interactions.%20Existing%20KT%20models%20typically%20use%20response%20correctness%20along%20with%20metadata%20like%20skill%20tags%20and%20timestamps%2C%20often%20overlooking%20the%20question%20text%2C%20which%20is%20an%20important%20source%20of%20pedagogical%20insight.%20This%20omission%20poses%20a%20lost%20opportunity%20while%20limiting%20predictive%20performance.%20We%20propose%20Next%20Token%20Knowledge%20Tracing%20%28NTKT%29%2C%20a%20novel%20approach%20that%20reframes%20KT%20as%20a%20next-token%20prediction%20task%20using%20pretrained%20Large%20Language%20Models%20%28LLMs%29.%20NTKT%20represents%20both%20student%20histories%20and%20question%20content%20as%20sequences%20of%20text%2C%20allowing%20LLMs%20to%20learn%20patterns%20in%20both%20behaviour%20and%20language.%20Our%20series%20of%20experiments%20significantly%20improves%20performance%20over%20state-of-the-art%20neural%20KT%20models%20and%20generalises%20much%20better%20to%20cold-start%20questions%20and%20users.%20These%20findings%20highlight%20the%20importance%20of%20question%20content%20in%20KT%20and%20demonstrate%20the%20benefits%20of%20leveraging%20pretrained%20representations%20of%20LLMs%20to%20model%20student%20learning%20more%20effectively.&entry.1838667208=http%3A//arxiv.org/abs/2511.02599v2&entry.124074799=Read"},
{"title": "ExoGS: A 4D Real-to-Sim-to-Real Framework for Scalable Manipulation Data Collection", "author": "Yiming Wang and Ruogu Zhang and Minyang Li and Hao Shi and Junbo Wang and Deyi Li and Jieji Ren and Wenhai Liu and Weiming Wang and Hao-Shu Fang", "abstract": "Real-to-Sim-to-Real technique is gaining increasing interest for robotic manipulation, as it can generate scalable data in simulation while having narrower sim-to-real gap. However, previous methods mainly focused on environment-level visual real-to-sim transfer, ignoring the transfer of interactions, which could be challenging and inefficient to obtain purely in simulation especially for contact-rich tasks. We propose ExoGS, a robot-free 4D Real-to-Sim-to-Real framework that captures both static environments and dynamic interactions in the real world and transfers them seamlessly to a simulated environment. It provides a new solution for scalable manipulation data collection and policy learning. ExoGS employs a self-designed robot-isomorphic passive exoskeleton AirExo-3 to capture kinematically consistent trajectories with millimeter-level accuracy and synchronized RGB observations during direct human demonstrations. The robot, objects, and environment are reconstructed as editable 3D Gaussian Splatting assets, enabling geometry-consistent replay and large-scale data augmentation. Additionally, a lightweight Mask Adapter injects instance-level semantics into the policy to enhance robustness under visual domain shifts. Real-world experiments demonstrate that ExoGS significantly improves data efficiency and policy generalization compared to teleoperation-based baselines. Code and hardware files have been released on https://github.com/zaixiabalala/ExoGS.", "link": "http://arxiv.org/abs/2601.18629v1", "date": "2026-01-26", "relevancy": 2.4729, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6784}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6129}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ExoGS%3A%20A%204D%20Real-to-Sim-to-Real%20Framework%20for%20Scalable%20Manipulation%20Data%20Collection&body=Title%3A%20ExoGS%3A%20A%204D%20Real-to-Sim-to-Real%20Framework%20for%20Scalable%20Manipulation%20Data%20Collection%0AAuthor%3A%20Yiming%20Wang%20and%20Ruogu%20Zhang%20and%20Minyang%20Li%20and%20Hao%20Shi%20and%20Junbo%20Wang%20and%20Deyi%20Li%20and%20Jieji%20Ren%20and%20Wenhai%20Liu%20and%20Weiming%20Wang%20and%20Hao-Shu%20Fang%0AAbstract%3A%20Real-to-Sim-to-Real%20technique%20is%20gaining%20increasing%20interest%20for%20robotic%20manipulation%2C%20as%20it%20can%20generate%20scalable%20data%20in%20simulation%20while%20having%20narrower%20sim-to-real%20gap.%20However%2C%20previous%20methods%20mainly%20focused%20on%20environment-level%20visual%20real-to-sim%20transfer%2C%20ignoring%20the%20transfer%20of%20interactions%2C%20which%20could%20be%20challenging%20and%20inefficient%20to%20obtain%20purely%20in%20simulation%20especially%20for%20contact-rich%20tasks.%20We%20propose%20ExoGS%2C%20a%20robot-free%204D%20Real-to-Sim-to-Real%20framework%20that%20captures%20both%20static%20environments%20and%20dynamic%20interactions%20in%20the%20real%20world%20and%20transfers%20them%20seamlessly%20to%20a%20simulated%20environment.%20It%20provides%20a%20new%20solution%20for%20scalable%20manipulation%20data%20collection%20and%20policy%20learning.%20ExoGS%20employs%20a%20self-designed%20robot-isomorphic%20passive%20exoskeleton%20AirExo-3%20to%20capture%20kinematically%20consistent%20trajectories%20with%20millimeter-level%20accuracy%20and%20synchronized%20RGB%20observations%20during%20direct%20human%20demonstrations.%20The%20robot%2C%20objects%2C%20and%20environment%20are%20reconstructed%20as%20editable%203D%20Gaussian%20Splatting%20assets%2C%20enabling%20geometry-consistent%20replay%20and%20large-scale%20data%20augmentation.%20Additionally%2C%20a%20lightweight%20Mask%20Adapter%20injects%20instance-level%20semantics%20into%20the%20policy%20to%20enhance%20robustness%20under%20visual%20domain%20shifts.%20Real-world%20experiments%20demonstrate%20that%20ExoGS%20significantly%20improves%20data%20efficiency%20and%20policy%20generalization%20compared%20to%20teleoperation-based%20baselines.%20Code%20and%20hardware%20files%20have%20been%20released%20on%20https%3A//github.com/zaixiabalala/ExoGS.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18629v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExoGS%253A%2520A%25204D%2520Real-to-Sim-to-Real%2520Framework%2520for%2520Scalable%2520Manipulation%2520Data%2520Collection%26entry.906535625%3DYiming%2520Wang%2520and%2520Ruogu%2520Zhang%2520and%2520Minyang%2520Li%2520and%2520Hao%2520Shi%2520and%2520Junbo%2520Wang%2520and%2520Deyi%2520Li%2520and%2520Jieji%2520Ren%2520and%2520Wenhai%2520Liu%2520and%2520Weiming%2520Wang%2520and%2520Hao-Shu%2520Fang%26entry.1292438233%3DReal-to-Sim-to-Real%2520technique%2520is%2520gaining%2520increasing%2520interest%2520for%2520robotic%2520manipulation%252C%2520as%2520it%2520can%2520generate%2520scalable%2520data%2520in%2520simulation%2520while%2520having%2520narrower%2520sim-to-real%2520gap.%2520However%252C%2520previous%2520methods%2520mainly%2520focused%2520on%2520environment-level%2520visual%2520real-to-sim%2520transfer%252C%2520ignoring%2520the%2520transfer%2520of%2520interactions%252C%2520which%2520could%2520be%2520challenging%2520and%2520inefficient%2520to%2520obtain%2520purely%2520in%2520simulation%2520especially%2520for%2520contact-rich%2520tasks.%2520We%2520propose%2520ExoGS%252C%2520a%2520robot-free%25204D%2520Real-to-Sim-to-Real%2520framework%2520that%2520captures%2520both%2520static%2520environments%2520and%2520dynamic%2520interactions%2520in%2520the%2520real%2520world%2520and%2520transfers%2520them%2520seamlessly%2520to%2520a%2520simulated%2520environment.%2520It%2520provides%2520a%2520new%2520solution%2520for%2520scalable%2520manipulation%2520data%2520collection%2520and%2520policy%2520learning.%2520ExoGS%2520employs%2520a%2520self-designed%2520robot-isomorphic%2520passive%2520exoskeleton%2520AirExo-3%2520to%2520capture%2520kinematically%2520consistent%2520trajectories%2520with%2520millimeter-level%2520accuracy%2520and%2520synchronized%2520RGB%2520observations%2520during%2520direct%2520human%2520demonstrations.%2520The%2520robot%252C%2520objects%252C%2520and%2520environment%2520are%2520reconstructed%2520as%2520editable%25203D%2520Gaussian%2520Splatting%2520assets%252C%2520enabling%2520geometry-consistent%2520replay%2520and%2520large-scale%2520data%2520augmentation.%2520Additionally%252C%2520a%2520lightweight%2520Mask%2520Adapter%2520injects%2520instance-level%2520semantics%2520into%2520the%2520policy%2520to%2520enhance%2520robustness%2520under%2520visual%2520domain%2520shifts.%2520Real-world%2520experiments%2520demonstrate%2520that%2520ExoGS%2520significantly%2520improves%2520data%2520efficiency%2520and%2520policy%2520generalization%2520compared%2520to%2520teleoperation-based%2520baselines.%2520Code%2520and%2520hardware%2520files%2520have%2520been%2520released%2520on%2520https%253A//github.com/zaixiabalala/ExoGS.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18629v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ExoGS%3A%20A%204D%20Real-to-Sim-to-Real%20Framework%20for%20Scalable%20Manipulation%20Data%20Collection&entry.906535625=Yiming%20Wang%20and%20Ruogu%20Zhang%20and%20Minyang%20Li%20and%20Hao%20Shi%20and%20Junbo%20Wang%20and%20Deyi%20Li%20and%20Jieji%20Ren%20and%20Wenhai%20Liu%20and%20Weiming%20Wang%20and%20Hao-Shu%20Fang&entry.1292438233=Real-to-Sim-to-Real%20technique%20is%20gaining%20increasing%20interest%20for%20robotic%20manipulation%2C%20as%20it%20can%20generate%20scalable%20data%20in%20simulation%20while%20having%20narrower%20sim-to-real%20gap.%20However%2C%20previous%20methods%20mainly%20focused%20on%20environment-level%20visual%20real-to-sim%20transfer%2C%20ignoring%20the%20transfer%20of%20interactions%2C%20which%20could%20be%20challenging%20and%20inefficient%20to%20obtain%20purely%20in%20simulation%20especially%20for%20contact-rich%20tasks.%20We%20propose%20ExoGS%2C%20a%20robot-free%204D%20Real-to-Sim-to-Real%20framework%20that%20captures%20both%20static%20environments%20and%20dynamic%20interactions%20in%20the%20real%20world%20and%20transfers%20them%20seamlessly%20to%20a%20simulated%20environment.%20It%20provides%20a%20new%20solution%20for%20scalable%20manipulation%20data%20collection%20and%20policy%20learning.%20ExoGS%20employs%20a%20self-designed%20robot-isomorphic%20passive%20exoskeleton%20AirExo-3%20to%20capture%20kinematically%20consistent%20trajectories%20with%20millimeter-level%20accuracy%20and%20synchronized%20RGB%20observations%20during%20direct%20human%20demonstrations.%20The%20robot%2C%20objects%2C%20and%20environment%20are%20reconstructed%20as%20editable%203D%20Gaussian%20Splatting%20assets%2C%20enabling%20geometry-consistent%20replay%20and%20large-scale%20data%20augmentation.%20Additionally%2C%20a%20lightweight%20Mask%20Adapter%20injects%20instance-level%20semantics%20into%20the%20policy%20to%20enhance%20robustness%20under%20visual%20domain%20shifts.%20Real-world%20experiments%20demonstrate%20that%20ExoGS%20significantly%20improves%20data%20efficiency%20and%20policy%20generalization%20compared%20to%20teleoperation-based%20baselines.%20Code%20and%20hardware%20files%20have%20been%20released%20on%20https%3A//github.com/zaixiabalala/ExoGS.&entry.1838667208=http%3A//arxiv.org/abs/2601.18629v1&entry.124074799=Read"},
{"title": "Harnessing the Universal Geometry of Embeddings", "author": "Rishi Jha and Collin Zhang and Vitaly Shmatikov and John X. Morris", "abstract": "We introduce the first method for translating text embeddings from one vector space to another without any paired data, encoders, or predefined sets of matches. Our unsupervised approach translates any embedding to and from a universal latent representation (i.e., a universal semantic structure conjectured by the Platonic Representation Hypothesis). Our translations achieve high cosine similarity across model pairs with different architectures, parameter counts, and training datasets.\n  The ability to translate unknown embeddings into a different space while preserving their geometry has serious implications for the security of vector databases. An adversary with access only to embedding vectors can extract sensitive information about the underlying documents, sufficient for classification and attribute inference.", "link": "http://arxiv.org/abs/2505.12540v4", "date": "2026-01-26", "relevancy": 2.4599, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4989}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4977}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4794}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harnessing%20the%20Universal%20Geometry%20of%20Embeddings&body=Title%3A%20Harnessing%20the%20Universal%20Geometry%20of%20Embeddings%0AAuthor%3A%20Rishi%20Jha%20and%20Collin%20Zhang%20and%20Vitaly%20Shmatikov%20and%20John%20X.%20Morris%0AAbstract%3A%20We%20introduce%20the%20first%20method%20for%20translating%20text%20embeddings%20from%20one%20vector%20space%20to%20another%20without%20any%20paired%20data%2C%20encoders%2C%20or%20predefined%20sets%20of%20matches.%20Our%20unsupervised%20approach%20translates%20any%20embedding%20to%20and%20from%20a%20universal%20latent%20representation%20%28i.e.%2C%20a%20universal%20semantic%20structure%20conjectured%20by%20the%20Platonic%20Representation%20Hypothesis%29.%20Our%20translations%20achieve%20high%20cosine%20similarity%20across%20model%20pairs%20with%20different%20architectures%2C%20parameter%20counts%2C%20and%20training%20datasets.%0A%20%20The%20ability%20to%20translate%20unknown%20embeddings%20into%20a%20different%20space%20while%20preserving%20their%20geometry%20has%20serious%20implications%20for%20the%20security%20of%20vector%20databases.%20An%20adversary%20with%20access%20only%20to%20embedding%20vectors%20can%20extract%20sensitive%20information%20about%20the%20underlying%20documents%2C%20sufficient%20for%20classification%20and%20attribute%20inference.%0ALink%3A%20http%3A//arxiv.org/abs/2505.12540v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarnessing%2520the%2520Universal%2520Geometry%2520of%2520Embeddings%26entry.906535625%3DRishi%2520Jha%2520and%2520Collin%2520Zhang%2520and%2520Vitaly%2520Shmatikov%2520and%2520John%2520X.%2520Morris%26entry.1292438233%3DWe%2520introduce%2520the%2520first%2520method%2520for%2520translating%2520text%2520embeddings%2520from%2520one%2520vector%2520space%2520to%2520another%2520without%2520any%2520paired%2520data%252C%2520encoders%252C%2520or%2520predefined%2520sets%2520of%2520matches.%2520Our%2520unsupervised%2520approach%2520translates%2520any%2520embedding%2520to%2520and%2520from%2520a%2520universal%2520latent%2520representation%2520%2528i.e.%252C%2520a%2520universal%2520semantic%2520structure%2520conjectured%2520by%2520the%2520Platonic%2520Representation%2520Hypothesis%2529.%2520Our%2520translations%2520achieve%2520high%2520cosine%2520similarity%2520across%2520model%2520pairs%2520with%2520different%2520architectures%252C%2520parameter%2520counts%252C%2520and%2520training%2520datasets.%250A%2520%2520The%2520ability%2520to%2520translate%2520unknown%2520embeddings%2520into%2520a%2520different%2520space%2520while%2520preserving%2520their%2520geometry%2520has%2520serious%2520implications%2520for%2520the%2520security%2520of%2520vector%2520databases.%2520An%2520adversary%2520with%2520access%2520only%2520to%2520embedding%2520vectors%2520can%2520extract%2520sensitive%2520information%2520about%2520the%2520underlying%2520documents%252C%2520sufficient%2520for%2520classification%2520and%2520attribute%2520inference.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12540v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20the%20Universal%20Geometry%20of%20Embeddings&entry.906535625=Rishi%20Jha%20and%20Collin%20Zhang%20and%20Vitaly%20Shmatikov%20and%20John%20X.%20Morris&entry.1292438233=We%20introduce%20the%20first%20method%20for%20translating%20text%20embeddings%20from%20one%20vector%20space%20to%20another%20without%20any%20paired%20data%2C%20encoders%2C%20or%20predefined%20sets%20of%20matches.%20Our%20unsupervised%20approach%20translates%20any%20embedding%20to%20and%20from%20a%20universal%20latent%20representation%20%28i.e.%2C%20a%20universal%20semantic%20structure%20conjectured%20by%20the%20Platonic%20Representation%20Hypothesis%29.%20Our%20translations%20achieve%20high%20cosine%20similarity%20across%20model%20pairs%20with%20different%20architectures%2C%20parameter%20counts%2C%20and%20training%20datasets.%0A%20%20The%20ability%20to%20translate%20unknown%20embeddings%20into%20a%20different%20space%20while%20preserving%20their%20geometry%20has%20serious%20implications%20for%20the%20security%20of%20vector%20databases.%20An%20adversary%20with%20access%20only%20to%20embedding%20vectors%20can%20extract%20sensitive%20information%20about%20the%20underlying%20documents%2C%20sufficient%20for%20classification%20and%20attribute%20inference.&entry.1838667208=http%3A//arxiv.org/abs/2505.12540v4&entry.124074799=Read"},
{"title": "Emergence of Quantised Representations Isolated to Anisotropic Functions", "author": "George Bird", "abstract": "Presented is a novel methodology for determining representational structure, which builds upon the existing Spotlight Resonance method. This new tool is used to gain insight into how discrete representations can emerge and organise in autoencoder models, through a controlled ablation study that alters only the activation function. Using this technique, the validity of whether function-driven symmetries can act as implicit inductive biases on representations is determined. Representations are found to tend to discretise when the activation functions are defined through a discrete algebraic permutation-equivariant symmetry. In contrast, they remain continuous under a continuous algebraic orthogonal-equivariant definition. This confirms the hypothesis that the symmetries of network primitives can carry unintended inductive biases, leading to task-independent artefactual structures in representations. The discrete symmetry of contemporary forms is shown to be a strong predictor for the production of symmetry-organised discrete representations emerging from otherwise continuous distributions -- a quantisation effect. This motivates further reassessment of functional forms in common usage due to such unintended consequences. Moreover, this supports a general causal model for a mode in which discrete representations may form, and could constitute a prerequisite for downstream interpretability phenomena, including grandmother neurons, discrete coding schemes, general linear features and a type of Superposition. Hence, this tool and proposed mechanism for the influence of functional form on representations may provide insights into interpretability research. Finally, preliminary results indicate that quantisation of representations correlates with a measurable increase in reconstruction error, reinforcing previous conjectures that this collapse can be detrimental.", "link": "http://arxiv.org/abs/2507.12070v4", "date": "2026-01-26", "relevancy": 2.4594, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4995}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4881}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4881}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emergence%20of%20Quantised%20Representations%20Isolated%20to%20Anisotropic%20Functions&body=Title%3A%20Emergence%20of%20Quantised%20Representations%20Isolated%20to%20Anisotropic%20Functions%0AAuthor%3A%20George%20Bird%0AAbstract%3A%20Presented%20is%20a%20novel%20methodology%20for%20determining%20representational%20structure%2C%20which%20builds%20upon%20the%20existing%20Spotlight%20Resonance%20method.%20This%20new%20tool%20is%20used%20to%20gain%20insight%20into%20how%20discrete%20representations%20can%20emerge%20and%20organise%20in%20autoencoder%20models%2C%20through%20a%20controlled%20ablation%20study%20that%20alters%20only%20the%20activation%20function.%20Using%20this%20technique%2C%20the%20validity%20of%20whether%20function-driven%20symmetries%20can%20act%20as%20implicit%20inductive%20biases%20on%20representations%20is%20determined.%20Representations%20are%20found%20to%20tend%20to%20discretise%20when%20the%20activation%20functions%20are%20defined%20through%20a%20discrete%20algebraic%20permutation-equivariant%20symmetry.%20In%20contrast%2C%20they%20remain%20continuous%20under%20a%20continuous%20algebraic%20orthogonal-equivariant%20definition.%20This%20confirms%20the%20hypothesis%20that%20the%20symmetries%20of%20network%20primitives%20can%20carry%20unintended%20inductive%20biases%2C%20leading%20to%20task-independent%20artefactual%20structures%20in%20representations.%20The%20discrete%20symmetry%20of%20contemporary%20forms%20is%20shown%20to%20be%20a%20strong%20predictor%20for%20the%20production%20of%20symmetry-organised%20discrete%20representations%20emerging%20from%20otherwise%20continuous%20distributions%20--%20a%20quantisation%20effect.%20This%20motivates%20further%20reassessment%20of%20functional%20forms%20in%20common%20usage%20due%20to%20such%20unintended%20consequences.%20Moreover%2C%20this%20supports%20a%20general%20causal%20model%20for%20a%20mode%20in%20which%20discrete%20representations%20may%20form%2C%20and%20could%20constitute%20a%20prerequisite%20for%20downstream%20interpretability%20phenomena%2C%20including%20grandmother%20neurons%2C%20discrete%20coding%20schemes%2C%20general%20linear%20features%20and%20a%20type%20of%20Superposition.%20Hence%2C%20this%20tool%20and%20proposed%20mechanism%20for%20the%20influence%20of%20functional%20form%20on%20representations%20may%20provide%20insights%20into%20interpretability%20research.%20Finally%2C%20preliminary%20results%20indicate%20that%20quantisation%20of%20representations%20correlates%20with%20a%20measurable%20increase%20in%20reconstruction%20error%2C%20reinforcing%20previous%20conjectures%20that%20this%20collapse%20can%20be%20detrimental.%0ALink%3A%20http%3A//arxiv.org/abs/2507.12070v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmergence%2520of%2520Quantised%2520Representations%2520Isolated%2520to%2520Anisotropic%2520Functions%26entry.906535625%3DGeorge%2520Bird%26entry.1292438233%3DPresented%2520is%2520a%2520novel%2520methodology%2520for%2520determining%2520representational%2520structure%252C%2520which%2520builds%2520upon%2520the%2520existing%2520Spotlight%2520Resonance%2520method.%2520This%2520new%2520tool%2520is%2520used%2520to%2520gain%2520insight%2520into%2520how%2520discrete%2520representations%2520can%2520emerge%2520and%2520organise%2520in%2520autoencoder%2520models%252C%2520through%2520a%2520controlled%2520ablation%2520study%2520that%2520alters%2520only%2520the%2520activation%2520function.%2520Using%2520this%2520technique%252C%2520the%2520validity%2520of%2520whether%2520function-driven%2520symmetries%2520can%2520act%2520as%2520implicit%2520inductive%2520biases%2520on%2520representations%2520is%2520determined.%2520Representations%2520are%2520found%2520to%2520tend%2520to%2520discretise%2520when%2520the%2520activation%2520functions%2520are%2520defined%2520through%2520a%2520discrete%2520algebraic%2520permutation-equivariant%2520symmetry.%2520In%2520contrast%252C%2520they%2520remain%2520continuous%2520under%2520a%2520continuous%2520algebraic%2520orthogonal-equivariant%2520definition.%2520This%2520confirms%2520the%2520hypothesis%2520that%2520the%2520symmetries%2520of%2520network%2520primitives%2520can%2520carry%2520unintended%2520inductive%2520biases%252C%2520leading%2520to%2520task-independent%2520artefactual%2520structures%2520in%2520representations.%2520The%2520discrete%2520symmetry%2520of%2520contemporary%2520forms%2520is%2520shown%2520to%2520be%2520a%2520strong%2520predictor%2520for%2520the%2520production%2520of%2520symmetry-organised%2520discrete%2520representations%2520emerging%2520from%2520otherwise%2520continuous%2520distributions%2520--%2520a%2520quantisation%2520effect.%2520This%2520motivates%2520further%2520reassessment%2520of%2520functional%2520forms%2520in%2520common%2520usage%2520due%2520to%2520such%2520unintended%2520consequences.%2520Moreover%252C%2520this%2520supports%2520a%2520general%2520causal%2520model%2520for%2520a%2520mode%2520in%2520which%2520discrete%2520representations%2520may%2520form%252C%2520and%2520could%2520constitute%2520a%2520prerequisite%2520for%2520downstream%2520interpretability%2520phenomena%252C%2520including%2520grandmother%2520neurons%252C%2520discrete%2520coding%2520schemes%252C%2520general%2520linear%2520features%2520and%2520a%2520type%2520of%2520Superposition.%2520Hence%252C%2520this%2520tool%2520and%2520proposed%2520mechanism%2520for%2520the%2520influence%2520of%2520functional%2520form%2520on%2520representations%2520may%2520provide%2520insights%2520into%2520interpretability%2520research.%2520Finally%252C%2520preliminary%2520results%2520indicate%2520that%2520quantisation%2520of%2520representations%2520correlates%2520with%2520a%2520measurable%2520increase%2520in%2520reconstruction%2520error%252C%2520reinforcing%2520previous%2520conjectures%2520that%2520this%2520collapse%2520can%2520be%2520detrimental.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.12070v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emergence%20of%20Quantised%20Representations%20Isolated%20to%20Anisotropic%20Functions&entry.906535625=George%20Bird&entry.1292438233=Presented%20is%20a%20novel%20methodology%20for%20determining%20representational%20structure%2C%20which%20builds%20upon%20the%20existing%20Spotlight%20Resonance%20method.%20This%20new%20tool%20is%20used%20to%20gain%20insight%20into%20how%20discrete%20representations%20can%20emerge%20and%20organise%20in%20autoencoder%20models%2C%20through%20a%20controlled%20ablation%20study%20that%20alters%20only%20the%20activation%20function.%20Using%20this%20technique%2C%20the%20validity%20of%20whether%20function-driven%20symmetries%20can%20act%20as%20implicit%20inductive%20biases%20on%20representations%20is%20determined.%20Representations%20are%20found%20to%20tend%20to%20discretise%20when%20the%20activation%20functions%20are%20defined%20through%20a%20discrete%20algebraic%20permutation-equivariant%20symmetry.%20In%20contrast%2C%20they%20remain%20continuous%20under%20a%20continuous%20algebraic%20orthogonal-equivariant%20definition.%20This%20confirms%20the%20hypothesis%20that%20the%20symmetries%20of%20network%20primitives%20can%20carry%20unintended%20inductive%20biases%2C%20leading%20to%20task-independent%20artefactual%20structures%20in%20representations.%20The%20discrete%20symmetry%20of%20contemporary%20forms%20is%20shown%20to%20be%20a%20strong%20predictor%20for%20the%20production%20of%20symmetry-organised%20discrete%20representations%20emerging%20from%20otherwise%20continuous%20distributions%20--%20a%20quantisation%20effect.%20This%20motivates%20further%20reassessment%20of%20functional%20forms%20in%20common%20usage%20due%20to%20such%20unintended%20consequences.%20Moreover%2C%20this%20supports%20a%20general%20causal%20model%20for%20a%20mode%20in%20which%20discrete%20representations%20may%20form%2C%20and%20could%20constitute%20a%20prerequisite%20for%20downstream%20interpretability%20phenomena%2C%20including%20grandmother%20neurons%2C%20discrete%20coding%20schemes%2C%20general%20linear%20features%20and%20a%20type%20of%20Superposition.%20Hence%2C%20this%20tool%20and%20proposed%20mechanism%20for%20the%20influence%20of%20functional%20form%20on%20representations%20may%20provide%20insights%20into%20interpretability%20research.%20Finally%2C%20preliminary%20results%20indicate%20that%20quantisation%20of%20representations%20correlates%20with%20a%20measurable%20increase%20in%20reconstruction%20error%2C%20reinforcing%20previous%20conjectures%20that%20this%20collapse%20can%20be%20detrimental.&entry.1838667208=http%3A//arxiv.org/abs/2507.12070v4&entry.124074799=Read"},
{"title": "Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability", "author": "Shobhita Sundaram and John Quan and Ariel Kwiatkowski and Kartik Ahuja and Yann Ollivier and Julia Kempe", "abstract": "Can a model learn to escape its own learning plateau? Reinforcement learning methods for finetuning large reasoning models stall on datasets with low initial success rates, and thus little training signal. We investigate a fundamental question: Can a pretrained LLM leverage latent knowledge to generate an automated curriculum for problems it cannot solve? To explore this, we design SOAR: A self-improvement framework designed to surface these pedagogical signals through meta-RL. A teacher copy of the model proposes synthetic problems for a student copy, and is rewarded with its improvement on a small subset of hard problems. Critically, SOAR grounds the curriculum in measured student progress rather than intrinsic proxy rewards. Our study on the hardest subsets of mathematical benchmarks (0/128 success) reveals three core findings. First, we show that it is possible to realize bi-level meta-RL that unlocks learning under sparse, binary rewards by sharpening a latent capacity of pretrained models to generate useful stepping stones. Second, grounded rewards outperform intrinsic reward schemes used in prior LLM self-play, reliably avoiding the instability and diversity collapse modes they typically exhibit. Third, analyzing the generated questions reveals that structural quality and well-posedness are more critical for learning progress than solution correctness. Our results suggest that the ability to generate useful stepping stones does not require the preexisting ability to actually solve the hard problems, paving a principled path to escape reasoning plateaus without additional curated data.", "link": "http://arxiv.org/abs/2601.18778v1", "date": "2026-01-26", "relevancy": 2.4559, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4939}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4939}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4857}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Teaching%20Models%20to%20Teach%20Themselves%3A%20Reasoning%20at%20the%20Edge%20of%20Learnability&body=Title%3A%20Teaching%20Models%20to%20Teach%20Themselves%3A%20Reasoning%20at%20the%20Edge%20of%20Learnability%0AAuthor%3A%20Shobhita%20Sundaram%20and%20John%20Quan%20and%20Ariel%20Kwiatkowski%20and%20Kartik%20Ahuja%20and%20Yann%20Ollivier%20and%20Julia%20Kempe%0AAbstract%3A%20Can%20a%20model%20learn%20to%20escape%20its%20own%20learning%20plateau%3F%20Reinforcement%20learning%20methods%20for%20finetuning%20large%20reasoning%20models%20stall%20on%20datasets%20with%20low%20initial%20success%20rates%2C%20and%20thus%20little%20training%20signal.%20We%20investigate%20a%20fundamental%20question%3A%20Can%20a%20pretrained%20LLM%20leverage%20latent%20knowledge%20to%20generate%20an%20automated%20curriculum%20for%20problems%20it%20cannot%20solve%3F%20To%20explore%20this%2C%20we%20design%20SOAR%3A%20A%20self-improvement%20framework%20designed%20to%20surface%20these%20pedagogical%20signals%20through%20meta-RL.%20A%20teacher%20copy%20of%20the%20model%20proposes%20synthetic%20problems%20for%20a%20student%20copy%2C%20and%20is%20rewarded%20with%20its%20improvement%20on%20a%20small%20subset%20of%20hard%20problems.%20Critically%2C%20SOAR%20grounds%20the%20curriculum%20in%20measured%20student%20progress%20rather%20than%20intrinsic%20proxy%20rewards.%20Our%20study%20on%20the%20hardest%20subsets%20of%20mathematical%20benchmarks%20%280/128%20success%29%20reveals%20three%20core%20findings.%20First%2C%20we%20show%20that%20it%20is%20possible%20to%20realize%20bi-level%20meta-RL%20that%20unlocks%20learning%20under%20sparse%2C%20binary%20rewards%20by%20sharpening%20a%20latent%20capacity%20of%20pretrained%20models%20to%20generate%20useful%20stepping%20stones.%20Second%2C%20grounded%20rewards%20outperform%20intrinsic%20reward%20schemes%20used%20in%20prior%20LLM%20self-play%2C%20reliably%20avoiding%20the%20instability%20and%20diversity%20collapse%20modes%20they%20typically%20exhibit.%20Third%2C%20analyzing%20the%20generated%20questions%20reveals%20that%20structural%20quality%20and%20well-posedness%20are%20more%20critical%20for%20learning%20progress%20than%20solution%20correctness.%20Our%20results%20suggest%20that%20the%20ability%20to%20generate%20useful%20stepping%20stones%20does%20not%20require%20the%20preexisting%20ability%20to%20actually%20solve%20the%20hard%20problems%2C%20paving%20a%20principled%20path%20to%20escape%20reasoning%20plateaus%20without%20additional%20curated%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18778v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeaching%2520Models%2520to%2520Teach%2520Themselves%253A%2520Reasoning%2520at%2520the%2520Edge%2520of%2520Learnability%26entry.906535625%3DShobhita%2520Sundaram%2520and%2520John%2520Quan%2520and%2520Ariel%2520Kwiatkowski%2520and%2520Kartik%2520Ahuja%2520and%2520Yann%2520Ollivier%2520and%2520Julia%2520Kempe%26entry.1292438233%3DCan%2520a%2520model%2520learn%2520to%2520escape%2520its%2520own%2520learning%2520plateau%253F%2520Reinforcement%2520learning%2520methods%2520for%2520finetuning%2520large%2520reasoning%2520models%2520stall%2520on%2520datasets%2520with%2520low%2520initial%2520success%2520rates%252C%2520and%2520thus%2520little%2520training%2520signal.%2520We%2520investigate%2520a%2520fundamental%2520question%253A%2520Can%2520a%2520pretrained%2520LLM%2520leverage%2520latent%2520knowledge%2520to%2520generate%2520an%2520automated%2520curriculum%2520for%2520problems%2520it%2520cannot%2520solve%253F%2520To%2520explore%2520this%252C%2520we%2520design%2520SOAR%253A%2520A%2520self-improvement%2520framework%2520designed%2520to%2520surface%2520these%2520pedagogical%2520signals%2520through%2520meta-RL.%2520A%2520teacher%2520copy%2520of%2520the%2520model%2520proposes%2520synthetic%2520problems%2520for%2520a%2520student%2520copy%252C%2520and%2520is%2520rewarded%2520with%2520its%2520improvement%2520on%2520a%2520small%2520subset%2520of%2520hard%2520problems.%2520Critically%252C%2520SOAR%2520grounds%2520the%2520curriculum%2520in%2520measured%2520student%2520progress%2520rather%2520than%2520intrinsic%2520proxy%2520rewards.%2520Our%2520study%2520on%2520the%2520hardest%2520subsets%2520of%2520mathematical%2520benchmarks%2520%25280/128%2520success%2529%2520reveals%2520three%2520core%2520findings.%2520First%252C%2520we%2520show%2520that%2520it%2520is%2520possible%2520to%2520realize%2520bi-level%2520meta-RL%2520that%2520unlocks%2520learning%2520under%2520sparse%252C%2520binary%2520rewards%2520by%2520sharpening%2520a%2520latent%2520capacity%2520of%2520pretrained%2520models%2520to%2520generate%2520useful%2520stepping%2520stones.%2520Second%252C%2520grounded%2520rewards%2520outperform%2520intrinsic%2520reward%2520schemes%2520used%2520in%2520prior%2520LLM%2520self-play%252C%2520reliably%2520avoiding%2520the%2520instability%2520and%2520diversity%2520collapse%2520modes%2520they%2520typically%2520exhibit.%2520Third%252C%2520analyzing%2520the%2520generated%2520questions%2520reveals%2520that%2520structural%2520quality%2520and%2520well-posedness%2520are%2520more%2520critical%2520for%2520learning%2520progress%2520than%2520solution%2520correctness.%2520Our%2520results%2520suggest%2520that%2520the%2520ability%2520to%2520generate%2520useful%2520stepping%2520stones%2520does%2520not%2520require%2520the%2520preexisting%2520ability%2520to%2520actually%2520solve%2520the%2520hard%2520problems%252C%2520paving%2520a%2520principled%2520path%2520to%2520escape%2520reasoning%2520plateaus%2520without%2520additional%2520curated%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18778v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Teaching%20Models%20to%20Teach%20Themselves%3A%20Reasoning%20at%20the%20Edge%20of%20Learnability&entry.906535625=Shobhita%20Sundaram%20and%20John%20Quan%20and%20Ariel%20Kwiatkowski%20and%20Kartik%20Ahuja%20and%20Yann%20Ollivier%20and%20Julia%20Kempe&entry.1292438233=Can%20a%20model%20learn%20to%20escape%20its%20own%20learning%20plateau%3F%20Reinforcement%20learning%20methods%20for%20finetuning%20large%20reasoning%20models%20stall%20on%20datasets%20with%20low%20initial%20success%20rates%2C%20and%20thus%20little%20training%20signal.%20We%20investigate%20a%20fundamental%20question%3A%20Can%20a%20pretrained%20LLM%20leverage%20latent%20knowledge%20to%20generate%20an%20automated%20curriculum%20for%20problems%20it%20cannot%20solve%3F%20To%20explore%20this%2C%20we%20design%20SOAR%3A%20A%20self-improvement%20framework%20designed%20to%20surface%20these%20pedagogical%20signals%20through%20meta-RL.%20A%20teacher%20copy%20of%20the%20model%20proposes%20synthetic%20problems%20for%20a%20student%20copy%2C%20and%20is%20rewarded%20with%20its%20improvement%20on%20a%20small%20subset%20of%20hard%20problems.%20Critically%2C%20SOAR%20grounds%20the%20curriculum%20in%20measured%20student%20progress%20rather%20than%20intrinsic%20proxy%20rewards.%20Our%20study%20on%20the%20hardest%20subsets%20of%20mathematical%20benchmarks%20%280/128%20success%29%20reveals%20three%20core%20findings.%20First%2C%20we%20show%20that%20it%20is%20possible%20to%20realize%20bi-level%20meta-RL%20that%20unlocks%20learning%20under%20sparse%2C%20binary%20rewards%20by%20sharpening%20a%20latent%20capacity%20of%20pretrained%20models%20to%20generate%20useful%20stepping%20stones.%20Second%2C%20grounded%20rewards%20outperform%20intrinsic%20reward%20schemes%20used%20in%20prior%20LLM%20self-play%2C%20reliably%20avoiding%20the%20instability%20and%20diversity%20collapse%20modes%20they%20typically%20exhibit.%20Third%2C%20analyzing%20the%20generated%20questions%20reveals%20that%20structural%20quality%20and%20well-posedness%20are%20more%20critical%20for%20learning%20progress%20than%20solution%20correctness.%20Our%20results%20suggest%20that%20the%20ability%20to%20generate%20useful%20stepping%20stones%20does%20not%20require%20the%20preexisting%20ability%20to%20actually%20solve%20the%20hard%20problems%2C%20paving%20a%20principled%20path%20to%20escape%20reasoning%20plateaus%20without%20additional%20curated%20data.&entry.1838667208=http%3A//arxiv.org/abs/2601.18778v1&entry.124074799=Read"},
{"title": "Is In-Context Learning Learning?", "author": "Adrian de Wynter", "abstract": "In-context learning (ICL) allows some autoregressive models to solve tasks via next-token prediction and without needing further training. This has led to claims about these model's ability to solve (learn) unseen tasks with only a few shots (exemplars) in the prompt. However, deduction does not always imply learning, as ICL does not explicitly encode a given observation. Instead, the models rely on their prior knowledge and the exemplars given, if any. We argue that, mathematically, ICL does constitute learning, but its full characterisation requires empirical work. We then carry out a large-scale analysis of ICL ablating out or accounting for memorisation, pretraining, distributional shifts, and prompting style and phrasing. We find that ICL is an effective learning paradigm, but limited in its ability to learn and generalise to unseen tasks. We note that, in the limit where exemplars become more numerous, accuracy is insensitive to exemplar distribution, model, prompt style, and the input's linguistic features. Instead, it deduces patterns from regularities in the prompt, which leads to distributional sensitivity, especially in prompting styles such as chain-of-thought. Given the varied accuracies on formally similar tasks, we conclude that autoregression's ad-hoc encoding is not a robust mechanism, and suggests limited all-purpose generalisability.", "link": "http://arxiv.org/abs/2509.10414v3", "date": "2026-01-26", "relevancy": 2.4536, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5048}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4837}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4837}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20In-Context%20Learning%20Learning%3F&body=Title%3A%20Is%20In-Context%20Learning%20Learning%3F%0AAuthor%3A%20Adrian%20de%20Wynter%0AAbstract%3A%20In-context%20learning%20%28ICL%29%20allows%20some%20autoregressive%20models%20to%20solve%20tasks%20via%20next-token%20prediction%20and%20without%20needing%20further%20training.%20This%20has%20led%20to%20claims%20about%20these%20model%27s%20ability%20to%20solve%20%28learn%29%20unseen%20tasks%20with%20only%20a%20few%20shots%20%28exemplars%29%20in%20the%20prompt.%20However%2C%20deduction%20does%20not%20always%20imply%20learning%2C%20as%20ICL%20does%20not%20explicitly%20encode%20a%20given%20observation.%20Instead%2C%20the%20models%20rely%20on%20their%20prior%20knowledge%20and%20the%20exemplars%20given%2C%20if%20any.%20We%20argue%20that%2C%20mathematically%2C%20ICL%20does%20constitute%20learning%2C%20but%20its%20full%20characterisation%20requires%20empirical%20work.%20We%20then%20carry%20out%20a%20large-scale%20analysis%20of%20ICL%20ablating%20out%20or%20accounting%20for%20memorisation%2C%20pretraining%2C%20distributional%20shifts%2C%20and%20prompting%20style%20and%20phrasing.%20We%20find%20that%20ICL%20is%20an%20effective%20learning%20paradigm%2C%20but%20limited%20in%20its%20ability%20to%20learn%20and%20generalise%20to%20unseen%20tasks.%20We%20note%20that%2C%20in%20the%20limit%20where%20exemplars%20become%20more%20numerous%2C%20accuracy%20is%20insensitive%20to%20exemplar%20distribution%2C%20model%2C%20prompt%20style%2C%20and%20the%20input%27s%20linguistic%20features.%20Instead%2C%20it%20deduces%20patterns%20from%20regularities%20in%20the%20prompt%2C%20which%20leads%20to%20distributional%20sensitivity%2C%20especially%20in%20prompting%20styles%20such%20as%20chain-of-thought.%20Given%20the%20varied%20accuracies%20on%20formally%20similar%20tasks%2C%20we%20conclude%20that%20autoregression%27s%20ad-hoc%20encoding%20is%20not%20a%20robust%20mechanism%2C%20and%20suggests%20limited%20all-purpose%20generalisability.%0ALink%3A%20http%3A//arxiv.org/abs/2509.10414v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520In-Context%2520Learning%2520Learning%253F%26entry.906535625%3DAdrian%2520de%2520Wynter%26entry.1292438233%3DIn-context%2520learning%2520%2528ICL%2529%2520allows%2520some%2520autoregressive%2520models%2520to%2520solve%2520tasks%2520via%2520next-token%2520prediction%2520and%2520without%2520needing%2520further%2520training.%2520This%2520has%2520led%2520to%2520claims%2520about%2520these%2520model%2527s%2520ability%2520to%2520solve%2520%2528learn%2529%2520unseen%2520tasks%2520with%2520only%2520a%2520few%2520shots%2520%2528exemplars%2529%2520in%2520the%2520prompt.%2520However%252C%2520deduction%2520does%2520not%2520always%2520imply%2520learning%252C%2520as%2520ICL%2520does%2520not%2520explicitly%2520encode%2520a%2520given%2520observation.%2520Instead%252C%2520the%2520models%2520rely%2520on%2520their%2520prior%2520knowledge%2520and%2520the%2520exemplars%2520given%252C%2520if%2520any.%2520We%2520argue%2520that%252C%2520mathematically%252C%2520ICL%2520does%2520constitute%2520learning%252C%2520but%2520its%2520full%2520characterisation%2520requires%2520empirical%2520work.%2520We%2520then%2520carry%2520out%2520a%2520large-scale%2520analysis%2520of%2520ICL%2520ablating%2520out%2520or%2520accounting%2520for%2520memorisation%252C%2520pretraining%252C%2520distributional%2520shifts%252C%2520and%2520prompting%2520style%2520and%2520phrasing.%2520We%2520find%2520that%2520ICL%2520is%2520an%2520effective%2520learning%2520paradigm%252C%2520but%2520limited%2520in%2520its%2520ability%2520to%2520learn%2520and%2520generalise%2520to%2520unseen%2520tasks.%2520We%2520note%2520that%252C%2520in%2520the%2520limit%2520where%2520exemplars%2520become%2520more%2520numerous%252C%2520accuracy%2520is%2520insensitive%2520to%2520exemplar%2520distribution%252C%2520model%252C%2520prompt%2520style%252C%2520and%2520the%2520input%2527s%2520linguistic%2520features.%2520Instead%252C%2520it%2520deduces%2520patterns%2520from%2520regularities%2520in%2520the%2520prompt%252C%2520which%2520leads%2520to%2520distributional%2520sensitivity%252C%2520especially%2520in%2520prompting%2520styles%2520such%2520as%2520chain-of-thought.%2520Given%2520the%2520varied%2520accuracies%2520on%2520formally%2520similar%2520tasks%252C%2520we%2520conclude%2520that%2520autoregression%2527s%2520ad-hoc%2520encoding%2520is%2520not%2520a%2520robust%2520mechanism%252C%2520and%2520suggests%2520limited%2520all-purpose%2520generalisability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.10414v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20In-Context%20Learning%20Learning%3F&entry.906535625=Adrian%20de%20Wynter&entry.1292438233=In-context%20learning%20%28ICL%29%20allows%20some%20autoregressive%20models%20to%20solve%20tasks%20via%20next-token%20prediction%20and%20without%20needing%20further%20training.%20This%20has%20led%20to%20claims%20about%20these%20model%27s%20ability%20to%20solve%20%28learn%29%20unseen%20tasks%20with%20only%20a%20few%20shots%20%28exemplars%29%20in%20the%20prompt.%20However%2C%20deduction%20does%20not%20always%20imply%20learning%2C%20as%20ICL%20does%20not%20explicitly%20encode%20a%20given%20observation.%20Instead%2C%20the%20models%20rely%20on%20their%20prior%20knowledge%20and%20the%20exemplars%20given%2C%20if%20any.%20We%20argue%20that%2C%20mathematically%2C%20ICL%20does%20constitute%20learning%2C%20but%20its%20full%20characterisation%20requires%20empirical%20work.%20We%20then%20carry%20out%20a%20large-scale%20analysis%20of%20ICL%20ablating%20out%20or%20accounting%20for%20memorisation%2C%20pretraining%2C%20distributional%20shifts%2C%20and%20prompting%20style%20and%20phrasing.%20We%20find%20that%20ICL%20is%20an%20effective%20learning%20paradigm%2C%20but%20limited%20in%20its%20ability%20to%20learn%20and%20generalise%20to%20unseen%20tasks.%20We%20note%20that%2C%20in%20the%20limit%20where%20exemplars%20become%20more%20numerous%2C%20accuracy%20is%20insensitive%20to%20exemplar%20distribution%2C%20model%2C%20prompt%20style%2C%20and%20the%20input%27s%20linguistic%20features.%20Instead%2C%20it%20deduces%20patterns%20from%20regularities%20in%20the%20prompt%2C%20which%20leads%20to%20distributional%20sensitivity%2C%20especially%20in%20prompting%20styles%20such%20as%20chain-of-thought.%20Given%20the%20varied%20accuracies%20on%20formally%20similar%20tasks%2C%20we%20conclude%20that%20autoregression%27s%20ad-hoc%20encoding%20is%20not%20a%20robust%20mechanism%2C%20and%20suggests%20limited%20all-purpose%20generalisability.&entry.1838667208=http%3A//arxiv.org/abs/2509.10414v3&entry.124074799=Read"},
{"title": "Seeing Beyond the Image: ECG and Anatomical Knowledge-Guided Myocardial Scar Segmentation from Late Gadolinium-Enhanced Images", "author": "Farheen Ramzan and Yusuf Kiberu and Nikesh Jathanna and Meryem Jabrane and Vicente Grau and Shahnaz Jamil-Copley and Richard H. Clayton and  Chen and  Chen", "abstract": "Accurate segmentation of myocardial scar from late gadolinium enhanced (LGE) cardiac MRI is essential for evaluating tissue viability, yet remains challenging due to variable contrast and imaging artifacts. Electrocardiogram (ECG) signals provide complementary physiological information, as conduction abnormalities can help localize or suggest scarred myocardial regions. In this work, we propose a novel multimodal framework that integrates ECG-derived electrophysiological information with anatomical priors from the AHA-17 atlas for physiologically consistent LGE-based scar segmentation. As ECGs and LGE-MRIs are not acquired simultaneously, we introduce a Temporal Aware Feature Fusion (TAFF) mechanism that dynamically weights and fuses features based on their acquisition time difference. Our method was evaluated on a clinical dataset and achieved substantial gains over the state-of-the-art image-only baseline (nnU-Net), increasing the average Dice score for scars from 0.6149 to 0.8463 and achieving high performance in both precision (0.9115) and sensitivity (0.9043). These results show that integrating physiological and anatomical knowledge allows the model to \"see beyond the image\", setting a new direction for robust and physiologically grounded cardiac scar segmentation.", "link": "http://arxiv.org/abs/2511.14702v2", "date": "2026-01-26", "relevancy": 2.4452, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4939}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4908}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20Beyond%20the%20Image%3A%20ECG%20and%20Anatomical%20Knowledge-Guided%20Myocardial%20Scar%20Segmentation%20from%20Late%20Gadolinium-Enhanced%20Images&body=Title%3A%20Seeing%20Beyond%20the%20Image%3A%20ECG%20and%20Anatomical%20Knowledge-Guided%20Myocardial%20Scar%20Segmentation%20from%20Late%20Gadolinium-Enhanced%20Images%0AAuthor%3A%20Farheen%20Ramzan%20and%20Yusuf%20Kiberu%20and%20Nikesh%20Jathanna%20and%20Meryem%20Jabrane%20and%20Vicente%20Grau%20and%20Shahnaz%20Jamil-Copley%20and%20Richard%20H.%20Clayton%20and%20%20Chen%20and%20%20Chen%0AAbstract%3A%20Accurate%20segmentation%20of%20myocardial%20scar%20from%20late%20gadolinium%20enhanced%20%28LGE%29%20cardiac%20MRI%20is%20essential%20for%20evaluating%20tissue%20viability%2C%20yet%20remains%20challenging%20due%20to%20variable%20contrast%20and%20imaging%20artifacts.%20Electrocardiogram%20%28ECG%29%20signals%20provide%20complementary%20physiological%20information%2C%20as%20conduction%20abnormalities%20can%20help%20localize%20or%20suggest%20scarred%20myocardial%20regions.%20In%20this%20work%2C%20we%20propose%20a%20novel%20multimodal%20framework%20that%20integrates%20ECG-derived%20electrophysiological%20information%20with%20anatomical%20priors%20from%20the%20AHA-17%20atlas%20for%20physiologically%20consistent%20LGE-based%20scar%20segmentation.%20As%20ECGs%20and%20LGE-MRIs%20are%20not%20acquired%20simultaneously%2C%20we%20introduce%20a%20Temporal%20Aware%20Feature%20Fusion%20%28TAFF%29%20mechanism%20that%20dynamically%20weights%20and%20fuses%20features%20based%20on%20their%20acquisition%20time%20difference.%20Our%20method%20was%20evaluated%20on%20a%20clinical%20dataset%20and%20achieved%20substantial%20gains%20over%20the%20state-of-the-art%20image-only%20baseline%20%28nnU-Net%29%2C%20increasing%20the%20average%20Dice%20score%20for%20scars%20from%200.6149%20to%200.8463%20and%20achieving%20high%20performance%20in%20both%20precision%20%280.9115%29%20and%20sensitivity%20%280.9043%29.%20These%20results%20show%20that%20integrating%20physiological%20and%20anatomical%20knowledge%20allows%20the%20model%20to%20%22see%20beyond%20the%20image%22%2C%20setting%20a%20new%20direction%20for%20robust%20and%20physiologically%20grounded%20cardiac%20scar%20segmentation.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14702v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520Beyond%2520the%2520Image%253A%2520ECG%2520and%2520Anatomical%2520Knowledge-Guided%2520Myocardial%2520Scar%2520Segmentation%2520from%2520Late%2520Gadolinium-Enhanced%2520Images%26entry.906535625%3DFarheen%2520Ramzan%2520and%2520Yusuf%2520Kiberu%2520and%2520Nikesh%2520Jathanna%2520and%2520Meryem%2520Jabrane%2520and%2520Vicente%2520Grau%2520and%2520Shahnaz%2520Jamil-Copley%2520and%2520Richard%2520H.%2520Clayton%2520and%2520%2520Chen%2520and%2520%2520Chen%26entry.1292438233%3DAccurate%2520segmentation%2520of%2520myocardial%2520scar%2520from%2520late%2520gadolinium%2520enhanced%2520%2528LGE%2529%2520cardiac%2520MRI%2520is%2520essential%2520for%2520evaluating%2520tissue%2520viability%252C%2520yet%2520remains%2520challenging%2520due%2520to%2520variable%2520contrast%2520and%2520imaging%2520artifacts.%2520Electrocardiogram%2520%2528ECG%2529%2520signals%2520provide%2520complementary%2520physiological%2520information%252C%2520as%2520conduction%2520abnormalities%2520can%2520help%2520localize%2520or%2520suggest%2520scarred%2520myocardial%2520regions.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520multimodal%2520framework%2520that%2520integrates%2520ECG-derived%2520electrophysiological%2520information%2520with%2520anatomical%2520priors%2520from%2520the%2520AHA-17%2520atlas%2520for%2520physiologically%2520consistent%2520LGE-based%2520scar%2520segmentation.%2520As%2520ECGs%2520and%2520LGE-MRIs%2520are%2520not%2520acquired%2520simultaneously%252C%2520we%2520introduce%2520a%2520Temporal%2520Aware%2520Feature%2520Fusion%2520%2528TAFF%2529%2520mechanism%2520that%2520dynamically%2520weights%2520and%2520fuses%2520features%2520based%2520on%2520their%2520acquisition%2520time%2520difference.%2520Our%2520method%2520was%2520evaluated%2520on%2520a%2520clinical%2520dataset%2520and%2520achieved%2520substantial%2520gains%2520over%2520the%2520state-of-the-art%2520image-only%2520baseline%2520%2528nnU-Net%2529%252C%2520increasing%2520the%2520average%2520Dice%2520score%2520for%2520scars%2520from%25200.6149%2520to%25200.8463%2520and%2520achieving%2520high%2520performance%2520in%2520both%2520precision%2520%25280.9115%2529%2520and%2520sensitivity%2520%25280.9043%2529.%2520These%2520results%2520show%2520that%2520integrating%2520physiological%2520and%2520anatomical%2520knowledge%2520allows%2520the%2520model%2520to%2520%2522see%2520beyond%2520the%2520image%2522%252C%2520setting%2520a%2520new%2520direction%2520for%2520robust%2520and%2520physiologically%2520grounded%2520cardiac%2520scar%2520segmentation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14702v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20Beyond%20the%20Image%3A%20ECG%20and%20Anatomical%20Knowledge-Guided%20Myocardial%20Scar%20Segmentation%20from%20Late%20Gadolinium-Enhanced%20Images&entry.906535625=Farheen%20Ramzan%20and%20Yusuf%20Kiberu%20and%20Nikesh%20Jathanna%20and%20Meryem%20Jabrane%20and%20Vicente%20Grau%20and%20Shahnaz%20Jamil-Copley%20and%20Richard%20H.%20Clayton%20and%20%20Chen%20and%20%20Chen&entry.1292438233=Accurate%20segmentation%20of%20myocardial%20scar%20from%20late%20gadolinium%20enhanced%20%28LGE%29%20cardiac%20MRI%20is%20essential%20for%20evaluating%20tissue%20viability%2C%20yet%20remains%20challenging%20due%20to%20variable%20contrast%20and%20imaging%20artifacts.%20Electrocardiogram%20%28ECG%29%20signals%20provide%20complementary%20physiological%20information%2C%20as%20conduction%20abnormalities%20can%20help%20localize%20or%20suggest%20scarred%20myocardial%20regions.%20In%20this%20work%2C%20we%20propose%20a%20novel%20multimodal%20framework%20that%20integrates%20ECG-derived%20electrophysiological%20information%20with%20anatomical%20priors%20from%20the%20AHA-17%20atlas%20for%20physiologically%20consistent%20LGE-based%20scar%20segmentation.%20As%20ECGs%20and%20LGE-MRIs%20are%20not%20acquired%20simultaneously%2C%20we%20introduce%20a%20Temporal%20Aware%20Feature%20Fusion%20%28TAFF%29%20mechanism%20that%20dynamically%20weights%20and%20fuses%20features%20based%20on%20their%20acquisition%20time%20difference.%20Our%20method%20was%20evaluated%20on%20a%20clinical%20dataset%20and%20achieved%20substantial%20gains%20over%20the%20state-of-the-art%20image-only%20baseline%20%28nnU-Net%29%2C%20increasing%20the%20average%20Dice%20score%20for%20scars%20from%200.6149%20to%200.8463%20and%20achieving%20high%20performance%20in%20both%20precision%20%280.9115%29%20and%20sensitivity%20%280.9043%29.%20These%20results%20show%20that%20integrating%20physiological%20and%20anatomical%20knowledge%20allows%20the%20model%20to%20%22see%20beyond%20the%20image%22%2C%20setting%20a%20new%20direction%20for%20robust%20and%20physiologically%20grounded%20cardiac%20scar%20segmentation.&entry.1838667208=http%3A//arxiv.org/abs/2511.14702v2&entry.124074799=Read"},
{"title": "Learning to Discover: A Generalized Framework for Raga Identification without Forgetting", "author": "Parampreet Singh and Somya Kumar and Chaitanya Shailendra Nitawe and Vipul Arora", "abstract": "Raga identification in Indian Art Music (IAM) remains challenging due to the presence of numerous rarely performed Ragas that are not represented in available training datasets. Traditional classification models struggle in this setting, as they assume a closed set of known categories and therefore fail to recognise or meaningfully group previously unseen Ragas. Recent works have tried categorizing unseen Ragas, but they run into a problem of catastrophic forgetting, where the knowledge of previously seen Ragas is diminished. To address this problem, we adopt a unified learning framework that leverages both labeled and unlabeled audio, enabling the model to discover coherent categories corresponding to the unseen Ragas, while retaining the knowledge of previously known ones. We test our model on benchmark Raga Identification datasets and demonstrate its performance in categorizing previously seen, unseen, and all Raga classes. The proposed approach surpasses the previous NCD-based pipeline even in discovering the unseen Raga categories, offering new insights into representation learning for IAM tasks.", "link": "http://arxiv.org/abs/2601.18766v1", "date": "2026-01-26", "relevancy": 2.4434, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5169}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4811}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Discover%3A%20A%20Generalized%20Framework%20for%20Raga%20Identification%20without%20Forgetting&body=Title%3A%20Learning%20to%20Discover%3A%20A%20Generalized%20Framework%20for%20Raga%20Identification%20without%20Forgetting%0AAuthor%3A%20Parampreet%20Singh%20and%20Somya%20Kumar%20and%20Chaitanya%20Shailendra%20Nitawe%20and%20Vipul%20Arora%0AAbstract%3A%20Raga%20identification%20in%20Indian%20Art%20Music%20%28IAM%29%20remains%20challenging%20due%20to%20the%20presence%20of%20numerous%20rarely%20performed%20Ragas%20that%20are%20not%20represented%20in%20available%20training%20datasets.%20Traditional%20classification%20models%20struggle%20in%20this%20setting%2C%20as%20they%20assume%20a%20closed%20set%20of%20known%20categories%20and%20therefore%20fail%20to%20recognise%20or%20meaningfully%20group%20previously%20unseen%20Ragas.%20Recent%20works%20have%20tried%20categorizing%20unseen%20Ragas%2C%20but%20they%20run%20into%20a%20problem%20of%20catastrophic%20forgetting%2C%20where%20the%20knowledge%20of%20previously%20seen%20Ragas%20is%20diminished.%20To%20address%20this%20problem%2C%20we%20adopt%20a%20unified%20learning%20framework%20that%20leverages%20both%20labeled%20and%20unlabeled%20audio%2C%20enabling%20the%20model%20to%20discover%20coherent%20categories%20corresponding%20to%20the%20unseen%20Ragas%2C%20while%20retaining%20the%20knowledge%20of%20previously%20known%20ones.%20We%20test%20our%20model%20on%20benchmark%20Raga%20Identification%20datasets%20and%20demonstrate%20its%20performance%20in%20categorizing%20previously%20seen%2C%20unseen%2C%20and%20all%20Raga%20classes.%20The%20proposed%20approach%20surpasses%20the%20previous%20NCD-based%20pipeline%20even%20in%20discovering%20the%20unseen%20Raga%20categories%2C%20offering%20new%20insights%20into%20representation%20learning%20for%20IAM%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18766v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Discover%253A%2520A%2520Generalized%2520Framework%2520for%2520Raga%2520Identification%2520without%2520Forgetting%26entry.906535625%3DParampreet%2520Singh%2520and%2520Somya%2520Kumar%2520and%2520Chaitanya%2520Shailendra%2520Nitawe%2520and%2520Vipul%2520Arora%26entry.1292438233%3DRaga%2520identification%2520in%2520Indian%2520Art%2520Music%2520%2528IAM%2529%2520remains%2520challenging%2520due%2520to%2520the%2520presence%2520of%2520numerous%2520rarely%2520performed%2520Ragas%2520that%2520are%2520not%2520represented%2520in%2520available%2520training%2520datasets.%2520Traditional%2520classification%2520models%2520struggle%2520in%2520this%2520setting%252C%2520as%2520they%2520assume%2520a%2520closed%2520set%2520of%2520known%2520categories%2520and%2520therefore%2520fail%2520to%2520recognise%2520or%2520meaningfully%2520group%2520previously%2520unseen%2520Ragas.%2520Recent%2520works%2520have%2520tried%2520categorizing%2520unseen%2520Ragas%252C%2520but%2520they%2520run%2520into%2520a%2520problem%2520of%2520catastrophic%2520forgetting%252C%2520where%2520the%2520knowledge%2520of%2520previously%2520seen%2520Ragas%2520is%2520diminished.%2520To%2520address%2520this%2520problem%252C%2520we%2520adopt%2520a%2520unified%2520learning%2520framework%2520that%2520leverages%2520both%2520labeled%2520and%2520unlabeled%2520audio%252C%2520enabling%2520the%2520model%2520to%2520discover%2520coherent%2520categories%2520corresponding%2520to%2520the%2520unseen%2520Ragas%252C%2520while%2520retaining%2520the%2520knowledge%2520of%2520previously%2520known%2520ones.%2520We%2520test%2520our%2520model%2520on%2520benchmark%2520Raga%2520Identification%2520datasets%2520and%2520demonstrate%2520its%2520performance%2520in%2520categorizing%2520previously%2520seen%252C%2520unseen%252C%2520and%2520all%2520Raga%2520classes.%2520The%2520proposed%2520approach%2520surpasses%2520the%2520previous%2520NCD-based%2520pipeline%2520even%2520in%2520discovering%2520the%2520unseen%2520Raga%2520categories%252C%2520offering%2520new%2520insights%2520into%2520representation%2520learning%2520for%2520IAM%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18766v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Discover%3A%20A%20Generalized%20Framework%20for%20Raga%20Identification%20without%20Forgetting&entry.906535625=Parampreet%20Singh%20and%20Somya%20Kumar%20and%20Chaitanya%20Shailendra%20Nitawe%20and%20Vipul%20Arora&entry.1292438233=Raga%20identification%20in%20Indian%20Art%20Music%20%28IAM%29%20remains%20challenging%20due%20to%20the%20presence%20of%20numerous%20rarely%20performed%20Ragas%20that%20are%20not%20represented%20in%20available%20training%20datasets.%20Traditional%20classification%20models%20struggle%20in%20this%20setting%2C%20as%20they%20assume%20a%20closed%20set%20of%20known%20categories%20and%20therefore%20fail%20to%20recognise%20or%20meaningfully%20group%20previously%20unseen%20Ragas.%20Recent%20works%20have%20tried%20categorizing%20unseen%20Ragas%2C%20but%20they%20run%20into%20a%20problem%20of%20catastrophic%20forgetting%2C%20where%20the%20knowledge%20of%20previously%20seen%20Ragas%20is%20diminished.%20To%20address%20this%20problem%2C%20we%20adopt%20a%20unified%20learning%20framework%20that%20leverages%20both%20labeled%20and%20unlabeled%20audio%2C%20enabling%20the%20model%20to%20discover%20coherent%20categories%20corresponding%20to%20the%20unseen%20Ragas%2C%20while%20retaining%20the%20knowledge%20of%20previously%20known%20ones.%20We%20test%20our%20model%20on%20benchmark%20Raga%20Identification%20datasets%20and%20demonstrate%20its%20performance%20in%20categorizing%20previously%20seen%2C%20unseen%2C%20and%20all%20Raga%20classes.%20The%20proposed%20approach%20surpasses%20the%20previous%20NCD-based%20pipeline%20even%20in%20discovering%20the%20unseen%20Raga%20categories%2C%20offering%20new%20insights%20into%20representation%20learning%20for%20IAM%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2601.18766v1&entry.124074799=Read"},
{"title": "Kinetic Mining in Context: Few-Shot Action Synthesis via Text-to-Motion Distillation", "author": "Luca Cazzola and Ahed Alboody", "abstract": "The acquisition cost for large, annotated motion datasets remains a critical bottleneck for skeletal-based Human Activity Recognition (HAR). Although Text-to-Motion (T2M) generative models offer a compelling, scalable source of synthetic data, their training objectives, which emphasize general artistic motion, and dataset structures fundamentally differ from HAR's requirements for kinematically precise, class-discriminative actions. This disparity creates a significant domain gap, making generalist T2M models ill-equipped for generating motions suitable for HAR classifiers. To address this challenge, we propose KineMIC (Kinetic Mining In Context), a transfer learning framework for few-shot action synthesis. KineMIC adapts a T2M diffusion model to an HAR domain by hypothesizing that semantic correspondences in the text encoding space can provide soft supervision for kinematic distillation. We operationalize this via a kinetic mining strategy that leverages CLIP text embeddings to establish correspondences between sparse HAR labels and T2M source data. This process guides fine-tuning, transforming the generalist T2M backbone into a specialized few-shot Action-to-Motion generator. We validate KineMIC using HumanML3D as the source T2M dataset and a subset of NTU RGB+D 120 as the target HAR domain, randomly selecting just 10 samples per action class. Our approach generates significantly more coherent motions, providing a robust data augmentation source that delivers a +23.1% accuracy points improvement. Animated illustrations and supplementary materials are available at https://lucazzola.github.io/publications/kinemic.", "link": "http://arxiv.org/abs/2512.11654v2", "date": "2026-01-26", "relevancy": 2.4416, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6449}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6221}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kinetic%20Mining%20in%20Context%3A%20Few-Shot%20Action%20Synthesis%20via%20Text-to-Motion%20Distillation&body=Title%3A%20Kinetic%20Mining%20in%20Context%3A%20Few-Shot%20Action%20Synthesis%20via%20Text-to-Motion%20Distillation%0AAuthor%3A%20Luca%20Cazzola%20and%20Ahed%20Alboody%0AAbstract%3A%20The%20acquisition%20cost%20for%20large%2C%20annotated%20motion%20datasets%20remains%20a%20critical%20bottleneck%20for%20skeletal-based%20Human%20Activity%20Recognition%20%28HAR%29.%20Although%20Text-to-Motion%20%28T2M%29%20generative%20models%20offer%20a%20compelling%2C%20scalable%20source%20of%20synthetic%20data%2C%20their%20training%20objectives%2C%20which%20emphasize%20general%20artistic%20motion%2C%20and%20dataset%20structures%20fundamentally%20differ%20from%20HAR%27s%20requirements%20for%20kinematically%20precise%2C%20class-discriminative%20actions.%20This%20disparity%20creates%20a%20significant%20domain%20gap%2C%20making%20generalist%20T2M%20models%20ill-equipped%20for%20generating%20motions%20suitable%20for%20HAR%20classifiers.%20To%20address%20this%20challenge%2C%20we%20propose%20KineMIC%20%28Kinetic%20Mining%20In%20Context%29%2C%20a%20transfer%20learning%20framework%20for%20few-shot%20action%20synthesis.%20KineMIC%20adapts%20a%20T2M%20diffusion%20model%20to%20an%20HAR%20domain%20by%20hypothesizing%20that%20semantic%20correspondences%20in%20the%20text%20encoding%20space%20can%20provide%20soft%20supervision%20for%20kinematic%20distillation.%20We%20operationalize%20this%20via%20a%20kinetic%20mining%20strategy%20that%20leverages%20CLIP%20text%20embeddings%20to%20establish%20correspondences%20between%20sparse%20HAR%20labels%20and%20T2M%20source%20data.%20This%20process%20guides%20fine-tuning%2C%20transforming%20the%20generalist%20T2M%20backbone%20into%20a%20specialized%20few-shot%20Action-to-Motion%20generator.%20We%20validate%20KineMIC%20using%20HumanML3D%20as%20the%20source%20T2M%20dataset%20and%20a%20subset%20of%20NTU%20RGB%2BD%20120%20as%20the%20target%20HAR%20domain%2C%20randomly%20selecting%20just%2010%20samples%20per%20action%20class.%20Our%20approach%20generates%20significantly%20more%20coherent%20motions%2C%20providing%20a%20robust%20data%20augmentation%20source%20that%20delivers%20a%20%2B23.1%25%20accuracy%20points%20improvement.%20Animated%20illustrations%20and%20supplementary%20materials%20are%20available%20at%20https%3A//lucazzola.github.io/publications/kinemic.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11654v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKinetic%2520Mining%2520in%2520Context%253A%2520Few-Shot%2520Action%2520Synthesis%2520via%2520Text-to-Motion%2520Distillation%26entry.906535625%3DLuca%2520Cazzola%2520and%2520Ahed%2520Alboody%26entry.1292438233%3DThe%2520acquisition%2520cost%2520for%2520large%252C%2520annotated%2520motion%2520datasets%2520remains%2520a%2520critical%2520bottleneck%2520for%2520skeletal-based%2520Human%2520Activity%2520Recognition%2520%2528HAR%2529.%2520Although%2520Text-to-Motion%2520%2528T2M%2529%2520generative%2520models%2520offer%2520a%2520compelling%252C%2520scalable%2520source%2520of%2520synthetic%2520data%252C%2520their%2520training%2520objectives%252C%2520which%2520emphasize%2520general%2520artistic%2520motion%252C%2520and%2520dataset%2520structures%2520fundamentally%2520differ%2520from%2520HAR%2527s%2520requirements%2520for%2520kinematically%2520precise%252C%2520class-discriminative%2520actions.%2520This%2520disparity%2520creates%2520a%2520significant%2520domain%2520gap%252C%2520making%2520generalist%2520T2M%2520models%2520ill-equipped%2520for%2520generating%2520motions%2520suitable%2520for%2520HAR%2520classifiers.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520KineMIC%2520%2528Kinetic%2520Mining%2520In%2520Context%2529%252C%2520a%2520transfer%2520learning%2520framework%2520for%2520few-shot%2520action%2520synthesis.%2520KineMIC%2520adapts%2520a%2520T2M%2520diffusion%2520model%2520to%2520an%2520HAR%2520domain%2520by%2520hypothesizing%2520that%2520semantic%2520correspondences%2520in%2520the%2520text%2520encoding%2520space%2520can%2520provide%2520soft%2520supervision%2520for%2520kinematic%2520distillation.%2520We%2520operationalize%2520this%2520via%2520a%2520kinetic%2520mining%2520strategy%2520that%2520leverages%2520CLIP%2520text%2520embeddings%2520to%2520establish%2520correspondences%2520between%2520sparse%2520HAR%2520labels%2520and%2520T2M%2520source%2520data.%2520This%2520process%2520guides%2520fine-tuning%252C%2520transforming%2520the%2520generalist%2520T2M%2520backbone%2520into%2520a%2520specialized%2520few-shot%2520Action-to-Motion%2520generator.%2520We%2520validate%2520KineMIC%2520using%2520HumanML3D%2520as%2520the%2520source%2520T2M%2520dataset%2520and%2520a%2520subset%2520of%2520NTU%2520RGB%252BD%2520120%2520as%2520the%2520target%2520HAR%2520domain%252C%2520randomly%2520selecting%2520just%252010%2520samples%2520per%2520action%2520class.%2520Our%2520approach%2520generates%2520significantly%2520more%2520coherent%2520motions%252C%2520providing%2520a%2520robust%2520data%2520augmentation%2520source%2520that%2520delivers%2520a%2520%252B23.1%2525%2520accuracy%2520points%2520improvement.%2520Animated%2520illustrations%2520and%2520supplementary%2520materials%2520are%2520available%2520at%2520https%253A//lucazzola.github.io/publications/kinemic.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11654v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kinetic%20Mining%20in%20Context%3A%20Few-Shot%20Action%20Synthesis%20via%20Text-to-Motion%20Distillation&entry.906535625=Luca%20Cazzola%20and%20Ahed%20Alboody&entry.1292438233=The%20acquisition%20cost%20for%20large%2C%20annotated%20motion%20datasets%20remains%20a%20critical%20bottleneck%20for%20skeletal-based%20Human%20Activity%20Recognition%20%28HAR%29.%20Although%20Text-to-Motion%20%28T2M%29%20generative%20models%20offer%20a%20compelling%2C%20scalable%20source%20of%20synthetic%20data%2C%20their%20training%20objectives%2C%20which%20emphasize%20general%20artistic%20motion%2C%20and%20dataset%20structures%20fundamentally%20differ%20from%20HAR%27s%20requirements%20for%20kinematically%20precise%2C%20class-discriminative%20actions.%20This%20disparity%20creates%20a%20significant%20domain%20gap%2C%20making%20generalist%20T2M%20models%20ill-equipped%20for%20generating%20motions%20suitable%20for%20HAR%20classifiers.%20To%20address%20this%20challenge%2C%20we%20propose%20KineMIC%20%28Kinetic%20Mining%20In%20Context%29%2C%20a%20transfer%20learning%20framework%20for%20few-shot%20action%20synthesis.%20KineMIC%20adapts%20a%20T2M%20diffusion%20model%20to%20an%20HAR%20domain%20by%20hypothesizing%20that%20semantic%20correspondences%20in%20the%20text%20encoding%20space%20can%20provide%20soft%20supervision%20for%20kinematic%20distillation.%20We%20operationalize%20this%20via%20a%20kinetic%20mining%20strategy%20that%20leverages%20CLIP%20text%20embeddings%20to%20establish%20correspondences%20between%20sparse%20HAR%20labels%20and%20T2M%20source%20data.%20This%20process%20guides%20fine-tuning%2C%20transforming%20the%20generalist%20T2M%20backbone%20into%20a%20specialized%20few-shot%20Action-to-Motion%20generator.%20We%20validate%20KineMIC%20using%20HumanML3D%20as%20the%20source%20T2M%20dataset%20and%20a%20subset%20of%20NTU%20RGB%2BD%20120%20as%20the%20target%20HAR%20domain%2C%20randomly%20selecting%20just%2010%20samples%20per%20action%20class.%20Our%20approach%20generates%20significantly%20more%20coherent%20motions%2C%20providing%20a%20robust%20data%20augmentation%20source%20that%20delivers%20a%20%2B23.1%25%20accuracy%20points%20improvement.%20Animated%20illustrations%20and%20supplementary%20materials%20are%20available%20at%20https%3A//lucazzola.github.io/publications/kinemic.&entry.1838667208=http%3A//arxiv.org/abs/2512.11654v2&entry.124074799=Read"},
{"title": "Fair-Eye Net: A Fair, Trustworthy, Multimodal Integrated Glaucoma Full Chain AI System", "author": "Wenbin Wei and Suyuan Yao and Cheng Huang and Xiangyu Gao", "abstract": "Glaucoma is a top cause of irreversible blindness globally, making early detection and longitudinal follow-up pivotal to preventing permanent vision loss. Current screening and progression assessment, however, rely on single tests or loosely linked examinations, introducing subjectivity and fragmented care. Limited access to high-quality imaging tools and specialist expertise further compromises consistency and equity in real-world use. To address these gaps, we developed Fair-Eye Net, a fair, reliable multimodal AI system closing the clinical loop from glaucoma screening to follow-up and risk alerting. It integrates fundus photos, OCT structural metrics, VF functional indices, and demographic factors via a dual-stream heterogeneous fusion architecture, with an uncertainty-aware hierarchical gating strategy for selective prediction and safe referral. A fairness constraint reduces missed diagnoses in disadvantaged subgroups. Experimental results show it achieved an AUC of 0.912 (96.7% specificity), cut racial false-negativity disparity by 73.4% (12.31% to 3.28%), maintained stable cross-domain performance, and enabled 3-12 months of early risk alerts (92% sensitivity, 88% specificity). Unlike post hoc fairness adjustments, Fair-Eye Net optimizes fairness as a primary goal with clinical reliability via multitask learning, offering a reproducible path for clinical translation and large-scale deployment to advance global eye health equity.", "link": "http://arxiv.org/abs/2601.18464v1", "date": "2026-01-26", "relevancy": 2.4389, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5001}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4835}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fair-Eye%20Net%3A%20A%20Fair%2C%20Trustworthy%2C%20Multimodal%20Integrated%20Glaucoma%20Full%20Chain%20AI%20System&body=Title%3A%20Fair-Eye%20Net%3A%20A%20Fair%2C%20Trustworthy%2C%20Multimodal%20Integrated%20Glaucoma%20Full%20Chain%20AI%20System%0AAuthor%3A%20Wenbin%20Wei%20and%20Suyuan%20Yao%20and%20Cheng%20Huang%20and%20Xiangyu%20Gao%0AAbstract%3A%20Glaucoma%20is%20a%20top%20cause%20of%20irreversible%20blindness%20globally%2C%20making%20early%20detection%20and%20longitudinal%20follow-up%20pivotal%20to%20preventing%20permanent%20vision%20loss.%20Current%20screening%20and%20progression%20assessment%2C%20however%2C%20rely%20on%20single%20tests%20or%20loosely%20linked%20examinations%2C%20introducing%20subjectivity%20and%20fragmented%20care.%20Limited%20access%20to%20high-quality%20imaging%20tools%20and%20specialist%20expertise%20further%20compromises%20consistency%20and%20equity%20in%20real-world%20use.%20To%20address%20these%20gaps%2C%20we%20developed%20Fair-Eye%20Net%2C%20a%20fair%2C%20reliable%20multimodal%20AI%20system%20closing%20the%20clinical%20loop%20from%20glaucoma%20screening%20to%20follow-up%20and%20risk%20alerting.%20It%20integrates%20fundus%20photos%2C%20OCT%20structural%20metrics%2C%20VF%20functional%20indices%2C%20and%20demographic%20factors%20via%20a%20dual-stream%20heterogeneous%20fusion%20architecture%2C%20with%20an%20uncertainty-aware%20hierarchical%20gating%20strategy%20for%20selective%20prediction%20and%20safe%20referral.%20A%20fairness%20constraint%20reduces%20missed%20diagnoses%20in%20disadvantaged%20subgroups.%20Experimental%20results%20show%20it%20achieved%20an%20AUC%20of%200.912%20%2896.7%25%20specificity%29%2C%20cut%20racial%20false-negativity%20disparity%20by%2073.4%25%20%2812.31%25%20to%203.28%25%29%2C%20maintained%20stable%20cross-domain%20performance%2C%20and%20enabled%203-12%20months%20of%20early%20risk%20alerts%20%2892%25%20sensitivity%2C%2088%25%20specificity%29.%20Unlike%20post%20hoc%20fairness%20adjustments%2C%20Fair-Eye%20Net%20optimizes%20fairness%20as%20a%20primary%20goal%20with%20clinical%20reliability%20via%20multitask%20learning%2C%20offering%20a%20reproducible%20path%20for%20clinical%20translation%20and%20large-scale%20deployment%20to%20advance%20global%20eye%20health%20equity.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18464v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFair-Eye%2520Net%253A%2520A%2520Fair%252C%2520Trustworthy%252C%2520Multimodal%2520Integrated%2520Glaucoma%2520Full%2520Chain%2520AI%2520System%26entry.906535625%3DWenbin%2520Wei%2520and%2520Suyuan%2520Yao%2520and%2520Cheng%2520Huang%2520and%2520Xiangyu%2520Gao%26entry.1292438233%3DGlaucoma%2520is%2520a%2520top%2520cause%2520of%2520irreversible%2520blindness%2520globally%252C%2520making%2520early%2520detection%2520and%2520longitudinal%2520follow-up%2520pivotal%2520to%2520preventing%2520permanent%2520vision%2520loss.%2520Current%2520screening%2520and%2520progression%2520assessment%252C%2520however%252C%2520rely%2520on%2520single%2520tests%2520or%2520loosely%2520linked%2520examinations%252C%2520introducing%2520subjectivity%2520and%2520fragmented%2520care.%2520Limited%2520access%2520to%2520high-quality%2520imaging%2520tools%2520and%2520specialist%2520expertise%2520further%2520compromises%2520consistency%2520and%2520equity%2520in%2520real-world%2520use.%2520To%2520address%2520these%2520gaps%252C%2520we%2520developed%2520Fair-Eye%2520Net%252C%2520a%2520fair%252C%2520reliable%2520multimodal%2520AI%2520system%2520closing%2520the%2520clinical%2520loop%2520from%2520glaucoma%2520screening%2520to%2520follow-up%2520and%2520risk%2520alerting.%2520It%2520integrates%2520fundus%2520photos%252C%2520OCT%2520structural%2520metrics%252C%2520VF%2520functional%2520indices%252C%2520and%2520demographic%2520factors%2520via%2520a%2520dual-stream%2520heterogeneous%2520fusion%2520architecture%252C%2520with%2520an%2520uncertainty-aware%2520hierarchical%2520gating%2520strategy%2520for%2520selective%2520prediction%2520and%2520safe%2520referral.%2520A%2520fairness%2520constraint%2520reduces%2520missed%2520diagnoses%2520in%2520disadvantaged%2520subgroups.%2520Experimental%2520results%2520show%2520it%2520achieved%2520an%2520AUC%2520of%25200.912%2520%252896.7%2525%2520specificity%2529%252C%2520cut%2520racial%2520false-negativity%2520disparity%2520by%252073.4%2525%2520%252812.31%2525%2520to%25203.28%2525%2529%252C%2520maintained%2520stable%2520cross-domain%2520performance%252C%2520and%2520enabled%25203-12%2520months%2520of%2520early%2520risk%2520alerts%2520%252892%2525%2520sensitivity%252C%252088%2525%2520specificity%2529.%2520Unlike%2520post%2520hoc%2520fairness%2520adjustments%252C%2520Fair-Eye%2520Net%2520optimizes%2520fairness%2520as%2520a%2520primary%2520goal%2520with%2520clinical%2520reliability%2520via%2520multitask%2520learning%252C%2520offering%2520a%2520reproducible%2520path%2520for%2520clinical%2520translation%2520and%2520large-scale%2520deployment%2520to%2520advance%2520global%2520eye%2520health%2520equity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18464v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fair-Eye%20Net%3A%20A%20Fair%2C%20Trustworthy%2C%20Multimodal%20Integrated%20Glaucoma%20Full%20Chain%20AI%20System&entry.906535625=Wenbin%20Wei%20and%20Suyuan%20Yao%20and%20Cheng%20Huang%20and%20Xiangyu%20Gao&entry.1292438233=Glaucoma%20is%20a%20top%20cause%20of%20irreversible%20blindness%20globally%2C%20making%20early%20detection%20and%20longitudinal%20follow-up%20pivotal%20to%20preventing%20permanent%20vision%20loss.%20Current%20screening%20and%20progression%20assessment%2C%20however%2C%20rely%20on%20single%20tests%20or%20loosely%20linked%20examinations%2C%20introducing%20subjectivity%20and%20fragmented%20care.%20Limited%20access%20to%20high-quality%20imaging%20tools%20and%20specialist%20expertise%20further%20compromises%20consistency%20and%20equity%20in%20real-world%20use.%20To%20address%20these%20gaps%2C%20we%20developed%20Fair-Eye%20Net%2C%20a%20fair%2C%20reliable%20multimodal%20AI%20system%20closing%20the%20clinical%20loop%20from%20glaucoma%20screening%20to%20follow-up%20and%20risk%20alerting.%20It%20integrates%20fundus%20photos%2C%20OCT%20structural%20metrics%2C%20VF%20functional%20indices%2C%20and%20demographic%20factors%20via%20a%20dual-stream%20heterogeneous%20fusion%20architecture%2C%20with%20an%20uncertainty-aware%20hierarchical%20gating%20strategy%20for%20selective%20prediction%20and%20safe%20referral.%20A%20fairness%20constraint%20reduces%20missed%20diagnoses%20in%20disadvantaged%20subgroups.%20Experimental%20results%20show%20it%20achieved%20an%20AUC%20of%200.912%20%2896.7%25%20specificity%29%2C%20cut%20racial%20false-negativity%20disparity%20by%2073.4%25%20%2812.31%25%20to%203.28%25%29%2C%20maintained%20stable%20cross-domain%20performance%2C%20and%20enabled%203-12%20months%20of%20early%20risk%20alerts%20%2892%25%20sensitivity%2C%2088%25%20specificity%29.%20Unlike%20post%20hoc%20fairness%20adjustments%2C%20Fair-Eye%20Net%20optimizes%20fairness%20as%20a%20primary%20goal%20with%20clinical%20reliability%20via%20multitask%20learning%2C%20offering%20a%20reproducible%20path%20for%20clinical%20translation%20and%20large-scale%20deployment%20to%20advance%20global%20eye%20health%20equity.&entry.1838667208=http%3A//arxiv.org/abs/2601.18464v1&entry.124074799=Read"},
{"title": "Funny or Persuasive, but Not Both: Evaluating Fine-Grained Multi-Concept Control in LLMs", "author": "Arya Labroo and Ivaxi Sheth and Vyas Raina and Amaani Ahmed and Mario Fritz", "abstract": "Large Language Models (LLMs) offer strong generative capabilities, but many applications require explicit and \\textit{fine-grained} control over specific textual concepts, such as humor, persuasiveness, or formality. Prior approaches in prompting and representation engineering can provide coarse or single-attribute control, but systematic evaluation of multi-attribute settings remains limited. We introduce an evaluation framework for fine-grained controllability for both single- and dual-concept scenarios, focusing on linguistically distinct concept pairs (e.g., persuasiveness vs.~humor). Surprisingly, across multiple LLMs and generative tasks, we find that performance often drops in the dual-concept setting, even though the chosen concepts should in principle be separable. This reveals a fundamental limitation of naive prompting-based control: models struggle with compositionality even when concepts are intuitively independent. Our framework provides systematic evidence of this gap and offers a principled approach for measuring the ability of future methods for multi-concept control.", "link": "http://arxiv.org/abs/2601.18483v1", "date": "2026-01-26", "relevancy": 2.4361, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4884}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4884}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Funny%20or%20Persuasive%2C%20but%20Not%20Both%3A%20Evaluating%20Fine-Grained%20Multi-Concept%20Control%20in%20LLMs&body=Title%3A%20Funny%20or%20Persuasive%2C%20but%20Not%20Both%3A%20Evaluating%20Fine-Grained%20Multi-Concept%20Control%20in%20LLMs%0AAuthor%3A%20Arya%20Labroo%20and%20Ivaxi%20Sheth%20and%20Vyas%20Raina%20and%20Amaani%20Ahmed%20and%20Mario%20Fritz%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20offer%20strong%20generative%20capabilities%2C%20but%20many%20applications%20require%20explicit%20and%20%5Ctextit%7Bfine-grained%7D%20control%20over%20specific%20textual%20concepts%2C%20such%20as%20humor%2C%20persuasiveness%2C%20or%20formality.%20Prior%20approaches%20in%20prompting%20and%20representation%20engineering%20can%20provide%20coarse%20or%20single-attribute%20control%2C%20but%20systematic%20evaluation%20of%20multi-attribute%20settings%20remains%20limited.%20We%20introduce%20an%20evaluation%20framework%20for%20fine-grained%20controllability%20for%20both%20single-%20and%20dual-concept%20scenarios%2C%20focusing%20on%20linguistically%20distinct%20concept%20pairs%20%28e.g.%2C%20persuasiveness%20vs.~humor%29.%20Surprisingly%2C%20across%20multiple%20LLMs%20and%20generative%20tasks%2C%20we%20find%20that%20performance%20often%20drops%20in%20the%20dual-concept%20setting%2C%20even%20though%20the%20chosen%20concepts%20should%20in%20principle%20be%20separable.%20This%20reveals%20a%20fundamental%20limitation%20of%20naive%20prompting-based%20control%3A%20models%20struggle%20with%20compositionality%20even%20when%20concepts%20are%20intuitively%20independent.%20Our%20framework%20provides%20systematic%20evidence%20of%20this%20gap%20and%20offers%20a%20principled%20approach%20for%20measuring%20the%20ability%20of%20future%20methods%20for%20multi-concept%20control.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18483v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFunny%2520or%2520Persuasive%252C%2520but%2520Not%2520Both%253A%2520Evaluating%2520Fine-Grained%2520Multi-Concept%2520Control%2520in%2520LLMs%26entry.906535625%3DArya%2520Labroo%2520and%2520Ivaxi%2520Sheth%2520and%2520Vyas%2520Raina%2520and%2520Amaani%2520Ahmed%2520and%2520Mario%2520Fritz%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520offer%2520strong%2520generative%2520capabilities%252C%2520but%2520many%2520applications%2520require%2520explicit%2520and%2520%255Ctextit%257Bfine-grained%257D%2520control%2520over%2520specific%2520textual%2520concepts%252C%2520such%2520as%2520humor%252C%2520persuasiveness%252C%2520or%2520formality.%2520Prior%2520approaches%2520in%2520prompting%2520and%2520representation%2520engineering%2520can%2520provide%2520coarse%2520or%2520single-attribute%2520control%252C%2520but%2520systematic%2520evaluation%2520of%2520multi-attribute%2520settings%2520remains%2520limited.%2520We%2520introduce%2520an%2520evaluation%2520framework%2520for%2520fine-grained%2520controllability%2520for%2520both%2520single-%2520and%2520dual-concept%2520scenarios%252C%2520focusing%2520on%2520linguistically%2520distinct%2520concept%2520pairs%2520%2528e.g.%252C%2520persuasiveness%2520vs.~humor%2529.%2520Surprisingly%252C%2520across%2520multiple%2520LLMs%2520and%2520generative%2520tasks%252C%2520we%2520find%2520that%2520performance%2520often%2520drops%2520in%2520the%2520dual-concept%2520setting%252C%2520even%2520though%2520the%2520chosen%2520concepts%2520should%2520in%2520principle%2520be%2520separable.%2520This%2520reveals%2520a%2520fundamental%2520limitation%2520of%2520naive%2520prompting-based%2520control%253A%2520models%2520struggle%2520with%2520compositionality%2520even%2520when%2520concepts%2520are%2520intuitively%2520independent.%2520Our%2520framework%2520provides%2520systematic%2520evidence%2520of%2520this%2520gap%2520and%2520offers%2520a%2520principled%2520approach%2520for%2520measuring%2520the%2520ability%2520of%2520future%2520methods%2520for%2520multi-concept%2520control.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18483v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Funny%20or%20Persuasive%2C%20but%20Not%20Both%3A%20Evaluating%20Fine-Grained%20Multi-Concept%20Control%20in%20LLMs&entry.906535625=Arya%20Labroo%20and%20Ivaxi%20Sheth%20and%20Vyas%20Raina%20and%20Amaani%20Ahmed%20and%20Mario%20Fritz&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20offer%20strong%20generative%20capabilities%2C%20but%20many%20applications%20require%20explicit%20and%20%5Ctextit%7Bfine-grained%7D%20control%20over%20specific%20textual%20concepts%2C%20such%20as%20humor%2C%20persuasiveness%2C%20or%20formality.%20Prior%20approaches%20in%20prompting%20and%20representation%20engineering%20can%20provide%20coarse%20or%20single-attribute%20control%2C%20but%20systematic%20evaluation%20of%20multi-attribute%20settings%20remains%20limited.%20We%20introduce%20an%20evaluation%20framework%20for%20fine-grained%20controllability%20for%20both%20single-%20and%20dual-concept%20scenarios%2C%20focusing%20on%20linguistically%20distinct%20concept%20pairs%20%28e.g.%2C%20persuasiveness%20vs.~humor%29.%20Surprisingly%2C%20across%20multiple%20LLMs%20and%20generative%20tasks%2C%20we%20find%20that%20performance%20often%20drops%20in%20the%20dual-concept%20setting%2C%20even%20though%20the%20chosen%20concepts%20should%20in%20principle%20be%20separable.%20This%20reveals%20a%20fundamental%20limitation%20of%20naive%20prompting-based%20control%3A%20models%20struggle%20with%20compositionality%20even%20when%20concepts%20are%20intuitively%20independent.%20Our%20framework%20provides%20systematic%20evidence%20of%20this%20gap%20and%20offers%20a%20principled%20approach%20for%20measuring%20the%20ability%20of%20future%20methods%20for%20multi-concept%20control.&entry.1838667208=http%3A//arxiv.org/abs/2601.18483v1&entry.124074799=Read"},
{"title": "Ask Me Again Differently: GRAS for Measuring Bias in Vision Language Models on Gender, Race, Age, and Skin Tone", "author": "Shaivi Malik and Hasnat Md Abdullah and Sriparna Saha and Amit Sheth", "abstract": "As Vision Language Models (VLMs) become integral to real-world applications, understanding their demographic biases is critical. We introduce GRAS, a benchmark for uncovering demographic biases in VLMs across gender, race, age, and skin tone, offering the most diverse coverage to date. We further propose the GRAS Bias Score, an interpretable metric for quantifying bias. We benchmark five state-of-the-art VLMs and reveal concerning bias levels, with the least biased model attaining a GRAS Bias Score of only 2 out of 100. Our findings also reveal a methodological insight: evaluating bias in VLMs with visual question answering (VQA) requires considering multiple formulations of a question. Our code, data, and evaluation results are publicly available.", "link": "http://arxiv.org/abs/2508.18989v2", "date": "2026-01-26", "relevancy": 2.428, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4913}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4913}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ask%20Me%20Again%20Differently%3A%20GRAS%20for%20Measuring%20Bias%20in%20Vision%20Language%20Models%20on%20Gender%2C%20Race%2C%20Age%2C%20and%20Skin%20Tone&body=Title%3A%20Ask%20Me%20Again%20Differently%3A%20GRAS%20for%20Measuring%20Bias%20in%20Vision%20Language%20Models%20on%20Gender%2C%20Race%2C%20Age%2C%20and%20Skin%20Tone%0AAuthor%3A%20Shaivi%20Malik%20and%20Hasnat%20Md%20Abdullah%20and%20Sriparna%20Saha%20and%20Amit%20Sheth%0AAbstract%3A%20As%20Vision%20Language%20Models%20%28VLMs%29%20become%20integral%20to%20real-world%20applications%2C%20understanding%20their%20demographic%20biases%20is%20critical.%20We%20introduce%20GRAS%2C%20a%20benchmark%20for%20uncovering%20demographic%20biases%20in%20VLMs%20across%20gender%2C%20race%2C%20age%2C%20and%20skin%20tone%2C%20offering%20the%20most%20diverse%20coverage%20to%20date.%20We%20further%20propose%20the%20GRAS%20Bias%20Score%2C%20an%20interpretable%20metric%20for%20quantifying%20bias.%20We%20benchmark%20five%20state-of-the-art%20VLMs%20and%20reveal%20concerning%20bias%20levels%2C%20with%20the%20least%20biased%20model%20attaining%20a%20GRAS%20Bias%20Score%20of%20only%202%20out%20of%20100.%20Our%20findings%20also%20reveal%20a%20methodological%20insight%3A%20evaluating%20bias%20in%20VLMs%20with%20visual%20question%20answering%20%28VQA%29%20requires%20considering%20multiple%20formulations%20of%20a%20question.%20Our%20code%2C%20data%2C%20and%20evaluation%20results%20are%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2508.18989v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAsk%2520Me%2520Again%2520Differently%253A%2520GRAS%2520for%2520Measuring%2520Bias%2520in%2520Vision%2520Language%2520Models%2520on%2520Gender%252C%2520Race%252C%2520Age%252C%2520and%2520Skin%2520Tone%26entry.906535625%3DShaivi%2520Malik%2520and%2520Hasnat%2520Md%2520Abdullah%2520and%2520Sriparna%2520Saha%2520and%2520Amit%2520Sheth%26entry.1292438233%3DAs%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520become%2520integral%2520to%2520real-world%2520applications%252C%2520understanding%2520their%2520demographic%2520biases%2520is%2520critical.%2520We%2520introduce%2520GRAS%252C%2520a%2520benchmark%2520for%2520uncovering%2520demographic%2520biases%2520in%2520VLMs%2520across%2520gender%252C%2520race%252C%2520age%252C%2520and%2520skin%2520tone%252C%2520offering%2520the%2520most%2520diverse%2520coverage%2520to%2520date.%2520We%2520further%2520propose%2520the%2520GRAS%2520Bias%2520Score%252C%2520an%2520interpretable%2520metric%2520for%2520quantifying%2520bias.%2520We%2520benchmark%2520five%2520state-of-the-art%2520VLMs%2520and%2520reveal%2520concerning%2520bias%2520levels%252C%2520with%2520the%2520least%2520biased%2520model%2520attaining%2520a%2520GRAS%2520Bias%2520Score%2520of%2520only%25202%2520out%2520of%2520100.%2520Our%2520findings%2520also%2520reveal%2520a%2520methodological%2520insight%253A%2520evaluating%2520bias%2520in%2520VLMs%2520with%2520visual%2520question%2520answering%2520%2528VQA%2529%2520requires%2520considering%2520multiple%2520formulations%2520of%2520a%2520question.%2520Our%2520code%252C%2520data%252C%2520and%2520evaluation%2520results%2520are%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18989v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ask%20Me%20Again%20Differently%3A%20GRAS%20for%20Measuring%20Bias%20in%20Vision%20Language%20Models%20on%20Gender%2C%20Race%2C%20Age%2C%20and%20Skin%20Tone&entry.906535625=Shaivi%20Malik%20and%20Hasnat%20Md%20Abdullah%20and%20Sriparna%20Saha%20and%20Amit%20Sheth&entry.1292438233=As%20Vision%20Language%20Models%20%28VLMs%29%20become%20integral%20to%20real-world%20applications%2C%20understanding%20their%20demographic%20biases%20is%20critical.%20We%20introduce%20GRAS%2C%20a%20benchmark%20for%20uncovering%20demographic%20biases%20in%20VLMs%20across%20gender%2C%20race%2C%20age%2C%20and%20skin%20tone%2C%20offering%20the%20most%20diverse%20coverage%20to%20date.%20We%20further%20propose%20the%20GRAS%20Bias%20Score%2C%20an%20interpretable%20metric%20for%20quantifying%20bias.%20We%20benchmark%20five%20state-of-the-art%20VLMs%20and%20reveal%20concerning%20bias%20levels%2C%20with%20the%20least%20biased%20model%20attaining%20a%20GRAS%20Bias%20Score%20of%20only%202%20out%20of%20100.%20Our%20findings%20also%20reveal%20a%20methodological%20insight%3A%20evaluating%20bias%20in%20VLMs%20with%20visual%20question%20answering%20%28VQA%29%20requires%20considering%20multiple%20formulations%20of%20a%20question.%20Our%20code%2C%20data%2C%20and%20evaluation%20results%20are%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2508.18989v2&entry.124074799=Read"},
{"title": "ARTI-6: Towards Six-dimensional Articulatory Speech Encoding", "author": "Jihwan Lee and Sean Foley and Thanathai Lertpetchpun and Kevin Huang and Yoonjeong Lee and Tiantian Feng and Louis Goldstein and Dani Byrd and Shrikanth Narayanan", "abstract": "We propose ARTI-6, a compact six-dimensional articulatory speech encoding framework derived from real-time MRI data that captures crucial vocal tract regions including the velum, tongue root, and larynx. ARTI-6 consists of three components: (1) a six-dimensional articulatory feature set representing key regions of the vocal tract; (2) an articulatory inversion model, which predicts articulatory features from speech acoustics leveraging speech foundation models, achieving a prediction correlation of 0.87; and (3) an articulatory synthesis model, which reconstructs intelligible speech directly from articulatory features, showing that even a low-dimensional representation can generate natural-sounding speech. Together, ARTI-6 provides an interpretable, computationally efficient, and physiologically grounded framework for advancing articulatory inversion, synthesis, and broader speech technology applications. The source code and speech samples are publicly available.", "link": "http://arxiv.org/abs/2509.21447v2", "date": "2026-01-26", "relevancy": 2.4099, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.487}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.487}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ARTI-6%3A%20Towards%20Six-dimensional%20Articulatory%20Speech%20Encoding&body=Title%3A%20ARTI-6%3A%20Towards%20Six-dimensional%20Articulatory%20Speech%20Encoding%0AAuthor%3A%20Jihwan%20Lee%20and%20Sean%20Foley%20and%20Thanathai%20Lertpetchpun%20and%20Kevin%20Huang%20and%20Yoonjeong%20Lee%20and%20Tiantian%20Feng%20and%20Louis%20Goldstein%20and%20Dani%20Byrd%20and%20Shrikanth%20Narayanan%0AAbstract%3A%20We%20propose%20ARTI-6%2C%20a%20compact%20six-dimensional%20articulatory%20speech%20encoding%20framework%20derived%20from%20real-time%20MRI%20data%20that%20captures%20crucial%20vocal%20tract%20regions%20including%20the%20velum%2C%20tongue%20root%2C%20and%20larynx.%20ARTI-6%20consists%20of%20three%20components%3A%20%281%29%20a%20six-dimensional%20articulatory%20feature%20set%20representing%20key%20regions%20of%20the%20vocal%20tract%3B%20%282%29%20an%20articulatory%20inversion%20model%2C%20which%20predicts%20articulatory%20features%20from%20speech%20acoustics%20leveraging%20speech%20foundation%20models%2C%20achieving%20a%20prediction%20correlation%20of%200.87%3B%20and%20%283%29%20an%20articulatory%20synthesis%20model%2C%20which%20reconstructs%20intelligible%20speech%20directly%20from%20articulatory%20features%2C%20showing%20that%20even%20a%20low-dimensional%20representation%20can%20generate%20natural-sounding%20speech.%20Together%2C%20ARTI-6%20provides%20an%20interpretable%2C%20computationally%20efficient%2C%20and%20physiologically%20grounded%20framework%20for%20advancing%20articulatory%20inversion%2C%20synthesis%2C%20and%20broader%20speech%20technology%20applications.%20The%20source%20code%20and%20speech%20samples%20are%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2509.21447v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DARTI-6%253A%2520Towards%2520Six-dimensional%2520Articulatory%2520Speech%2520Encoding%26entry.906535625%3DJihwan%2520Lee%2520and%2520Sean%2520Foley%2520and%2520Thanathai%2520Lertpetchpun%2520and%2520Kevin%2520Huang%2520and%2520Yoonjeong%2520Lee%2520and%2520Tiantian%2520Feng%2520and%2520Louis%2520Goldstein%2520and%2520Dani%2520Byrd%2520and%2520Shrikanth%2520Narayanan%26entry.1292438233%3DWe%2520propose%2520ARTI-6%252C%2520a%2520compact%2520six-dimensional%2520articulatory%2520speech%2520encoding%2520framework%2520derived%2520from%2520real-time%2520MRI%2520data%2520that%2520captures%2520crucial%2520vocal%2520tract%2520regions%2520including%2520the%2520velum%252C%2520tongue%2520root%252C%2520and%2520larynx.%2520ARTI-6%2520consists%2520of%2520three%2520components%253A%2520%25281%2529%2520a%2520six-dimensional%2520articulatory%2520feature%2520set%2520representing%2520key%2520regions%2520of%2520the%2520vocal%2520tract%253B%2520%25282%2529%2520an%2520articulatory%2520inversion%2520model%252C%2520which%2520predicts%2520articulatory%2520features%2520from%2520speech%2520acoustics%2520leveraging%2520speech%2520foundation%2520models%252C%2520achieving%2520a%2520prediction%2520correlation%2520of%25200.87%253B%2520and%2520%25283%2529%2520an%2520articulatory%2520synthesis%2520model%252C%2520which%2520reconstructs%2520intelligible%2520speech%2520directly%2520from%2520articulatory%2520features%252C%2520showing%2520that%2520even%2520a%2520low-dimensional%2520representation%2520can%2520generate%2520natural-sounding%2520speech.%2520Together%252C%2520ARTI-6%2520provides%2520an%2520interpretable%252C%2520computationally%2520efficient%252C%2520and%2520physiologically%2520grounded%2520framework%2520for%2520advancing%2520articulatory%2520inversion%252C%2520synthesis%252C%2520and%2520broader%2520speech%2520technology%2520applications.%2520The%2520source%2520code%2520and%2520speech%2520samples%2520are%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21447v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ARTI-6%3A%20Towards%20Six-dimensional%20Articulatory%20Speech%20Encoding&entry.906535625=Jihwan%20Lee%20and%20Sean%20Foley%20and%20Thanathai%20Lertpetchpun%20and%20Kevin%20Huang%20and%20Yoonjeong%20Lee%20and%20Tiantian%20Feng%20and%20Louis%20Goldstein%20and%20Dani%20Byrd%20and%20Shrikanth%20Narayanan&entry.1292438233=We%20propose%20ARTI-6%2C%20a%20compact%20six-dimensional%20articulatory%20speech%20encoding%20framework%20derived%20from%20real-time%20MRI%20data%20that%20captures%20crucial%20vocal%20tract%20regions%20including%20the%20velum%2C%20tongue%20root%2C%20and%20larynx.%20ARTI-6%20consists%20of%20three%20components%3A%20%281%29%20a%20six-dimensional%20articulatory%20feature%20set%20representing%20key%20regions%20of%20the%20vocal%20tract%3B%20%282%29%20an%20articulatory%20inversion%20model%2C%20which%20predicts%20articulatory%20features%20from%20speech%20acoustics%20leveraging%20speech%20foundation%20models%2C%20achieving%20a%20prediction%20correlation%20of%200.87%3B%20and%20%283%29%20an%20articulatory%20synthesis%20model%2C%20which%20reconstructs%20intelligible%20speech%20directly%20from%20articulatory%20features%2C%20showing%20that%20even%20a%20low-dimensional%20representation%20can%20generate%20natural-sounding%20speech.%20Together%2C%20ARTI-6%20provides%20an%20interpretable%2C%20computationally%20efficient%2C%20and%20physiologically%20grounded%20framework%20for%20advancing%20articulatory%20inversion%2C%20synthesis%2C%20and%20broader%20speech%20technology%20applications.%20The%20source%20code%20and%20speech%20samples%20are%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2509.21447v2&entry.124074799=Read"},
{"title": "A Master Class on Reproducibility: A Student Hackathon on Advanced MRI Reconstruction Methods", "author": "Lina Felsner and Sevgi G. Kafali and Hannah Eichhorn and Agnes A. J. Leth and Aidas Batvinskas and Andre Datchev and Fabian Klemm and Jan Aulich and Puntika Leepagorn and Ruben Klinger and Daniel Rueckert and Julia A. Schnabel", "abstract": "We report the design, protocol, and outcomes of a student reproducibility hackathon focused on replicating the results of three influential MRI reconstruction papers: (a) MoDL, an unrolled model-based network with learned denoising; (b) HUMUS-Net, a hybrid unrolled multiscale CNN+Transformer architecture; and (c) an untrained, physics-regularized dynamic MRI method that uses a quantitative MR model for early stopping. We describe the setup of the hackathon and present reproduction outcomes alongside additional experiments, and we detail fundamental practices for building reproducible codebases.", "link": "http://arxiv.org/abs/2601.18314v1", "date": "2026-01-26", "relevancy": 2.4028, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4836}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4836}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Master%20Class%20on%20Reproducibility%3A%20A%20Student%20Hackathon%20on%20Advanced%20MRI%20Reconstruction%20Methods&body=Title%3A%20A%20Master%20Class%20on%20Reproducibility%3A%20A%20Student%20Hackathon%20on%20Advanced%20MRI%20Reconstruction%20Methods%0AAuthor%3A%20Lina%20Felsner%20and%20Sevgi%20G.%20Kafali%20and%20Hannah%20Eichhorn%20and%20Agnes%20A.%20J.%20Leth%20and%20Aidas%20Batvinskas%20and%20Andre%20Datchev%20and%20Fabian%20Klemm%20and%20Jan%20Aulich%20and%20Puntika%20Leepagorn%20and%20Ruben%20Klinger%20and%20Daniel%20Rueckert%20and%20Julia%20A.%20Schnabel%0AAbstract%3A%20We%20report%20the%20design%2C%20protocol%2C%20and%20outcomes%20of%20a%20student%20reproducibility%20hackathon%20focused%20on%20replicating%20the%20results%20of%20three%20influential%20MRI%20reconstruction%20papers%3A%20%28a%29%20MoDL%2C%20an%20unrolled%20model-based%20network%20with%20learned%20denoising%3B%20%28b%29%20HUMUS-Net%2C%20a%20hybrid%20unrolled%20multiscale%20CNN%2BTransformer%20architecture%3B%20and%20%28c%29%20an%20untrained%2C%20physics-regularized%20dynamic%20MRI%20method%20that%20uses%20a%20quantitative%20MR%20model%20for%20early%20stopping.%20We%20describe%20the%20setup%20of%20the%20hackathon%20and%20present%20reproduction%20outcomes%20alongside%20additional%20experiments%2C%20and%20we%20detail%20fundamental%20practices%20for%20building%20reproducible%20codebases.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18314v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Master%2520Class%2520on%2520Reproducibility%253A%2520A%2520Student%2520Hackathon%2520on%2520Advanced%2520MRI%2520Reconstruction%2520Methods%26entry.906535625%3DLina%2520Felsner%2520and%2520Sevgi%2520G.%2520Kafali%2520and%2520Hannah%2520Eichhorn%2520and%2520Agnes%2520A.%2520J.%2520Leth%2520and%2520Aidas%2520Batvinskas%2520and%2520Andre%2520Datchev%2520and%2520Fabian%2520Klemm%2520and%2520Jan%2520Aulich%2520and%2520Puntika%2520Leepagorn%2520and%2520Ruben%2520Klinger%2520and%2520Daniel%2520Rueckert%2520and%2520Julia%2520A.%2520Schnabel%26entry.1292438233%3DWe%2520report%2520the%2520design%252C%2520protocol%252C%2520and%2520outcomes%2520of%2520a%2520student%2520reproducibility%2520hackathon%2520focused%2520on%2520replicating%2520the%2520results%2520of%2520three%2520influential%2520MRI%2520reconstruction%2520papers%253A%2520%2528a%2529%2520MoDL%252C%2520an%2520unrolled%2520model-based%2520network%2520with%2520learned%2520denoising%253B%2520%2528b%2529%2520HUMUS-Net%252C%2520a%2520hybrid%2520unrolled%2520multiscale%2520CNN%252BTransformer%2520architecture%253B%2520and%2520%2528c%2529%2520an%2520untrained%252C%2520physics-regularized%2520dynamic%2520MRI%2520method%2520that%2520uses%2520a%2520quantitative%2520MR%2520model%2520for%2520early%2520stopping.%2520We%2520describe%2520the%2520setup%2520of%2520the%2520hackathon%2520and%2520present%2520reproduction%2520outcomes%2520alongside%2520additional%2520experiments%252C%2520and%2520we%2520detail%2520fundamental%2520practices%2520for%2520building%2520reproducible%2520codebases.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18314v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Master%20Class%20on%20Reproducibility%3A%20A%20Student%20Hackathon%20on%20Advanced%20MRI%20Reconstruction%20Methods&entry.906535625=Lina%20Felsner%20and%20Sevgi%20G.%20Kafali%20and%20Hannah%20Eichhorn%20and%20Agnes%20A.%20J.%20Leth%20and%20Aidas%20Batvinskas%20and%20Andre%20Datchev%20and%20Fabian%20Klemm%20and%20Jan%20Aulich%20and%20Puntika%20Leepagorn%20and%20Ruben%20Klinger%20and%20Daniel%20Rueckert%20and%20Julia%20A.%20Schnabel&entry.1292438233=We%20report%20the%20design%2C%20protocol%2C%20and%20outcomes%20of%20a%20student%20reproducibility%20hackathon%20focused%20on%20replicating%20the%20results%20of%20three%20influential%20MRI%20reconstruction%20papers%3A%20%28a%29%20MoDL%2C%20an%20unrolled%20model-based%20network%20with%20learned%20denoising%3B%20%28b%29%20HUMUS-Net%2C%20a%20hybrid%20unrolled%20multiscale%20CNN%2BTransformer%20architecture%3B%20and%20%28c%29%20an%20untrained%2C%20physics-regularized%20dynamic%20MRI%20method%20that%20uses%20a%20quantitative%20MR%20model%20for%20early%20stopping.%20We%20describe%20the%20setup%20of%20the%20hackathon%20and%20present%20reproduction%20outcomes%20alongside%20additional%20experiments%2C%20and%20we%20detail%20fundamental%20practices%20for%20building%20reproducible%20codebases.&entry.1838667208=http%3A//arxiv.org/abs/2601.18314v1&entry.124074799=Read"},
{"title": "GlobalGeoTree: A Multi-Granular Vision-Language Dataset for Global Tree Species Classification", "author": "Yang Mu and Zhitong Xiong and Yi Wang and Muhammad Shahzad and Franz Essl and Holger Kreft and Mark van Kleunen and Xiao Xiang Zhu", "abstract": "Global tree species mapping using remote sensing data is vital for biodiversity monitoring, forest management, and ecological research. However, progress in this field has been constrained by the scarcity of large-scale, labeled datasets. To address this, we introduce GlobalGeoTree, a comprehensive global dataset for tree species classification. GlobalGeoTree comprises 6.3 million geolocated tree occurrences, spanning 275 families, 2,734 genera, and 21,001 species across the hierarchical taxonomic levels. Each sample is paired with Sentinel-2 image time series and 27 auxiliary environmental variables, encompassing bioclimatic, geographic, and soil data. The dataset is partitioned into GlobalGeoTree-6M for model pretraining and curated evaluation subsets, primarily GlobalGeoTree-10kEval for zero-shot and few-shot benchmarking. To demonstrate the utility of the dataset, we introduce a baseline model, GeoTreeCLIP, which leverages paired remote sensing data and taxonomic text labels within a vision-language framework pretrained on GlobalGeoTree-6M. Experimental results show that GeoTreeCLIP achieves substantial improvements in zero- and few-shot classification on GlobalGeoTree-10kEval over existing advanced models. By making the dataset, models, and code publicly available, we aim to establish a benchmark to advance tree species classification and foster innovation in biodiversity research and ecological applications.", "link": "http://arxiv.org/abs/2505.12513v3", "date": "2026-01-26", "relevancy": 2.3981, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4885}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4752}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GlobalGeoTree%3A%20A%20Multi-Granular%20Vision-Language%20Dataset%20for%20Global%20Tree%20Species%20Classification&body=Title%3A%20GlobalGeoTree%3A%20A%20Multi-Granular%20Vision-Language%20Dataset%20for%20Global%20Tree%20Species%20Classification%0AAuthor%3A%20Yang%20Mu%20and%20Zhitong%20Xiong%20and%20Yi%20Wang%20and%20Muhammad%20Shahzad%20and%20Franz%20Essl%20and%20Holger%20Kreft%20and%20Mark%20van%20Kleunen%20and%20Xiao%20Xiang%20Zhu%0AAbstract%3A%20Global%20tree%20species%20mapping%20using%20remote%20sensing%20data%20is%20vital%20for%20biodiversity%20monitoring%2C%20forest%20management%2C%20and%20ecological%20research.%20However%2C%20progress%20in%20this%20field%20has%20been%20constrained%20by%20the%20scarcity%20of%20large-scale%2C%20labeled%20datasets.%20To%20address%20this%2C%20we%20introduce%20GlobalGeoTree%2C%20a%20comprehensive%20global%20dataset%20for%20tree%20species%20classification.%20GlobalGeoTree%20comprises%206.3%20million%20geolocated%20tree%20occurrences%2C%20spanning%20275%20families%2C%202%2C734%20genera%2C%20and%2021%2C001%20species%20across%20the%20hierarchical%20taxonomic%20levels.%20Each%20sample%20is%20paired%20with%20Sentinel-2%20image%20time%20series%20and%2027%20auxiliary%20environmental%20variables%2C%20encompassing%20bioclimatic%2C%20geographic%2C%20and%20soil%20data.%20The%20dataset%20is%20partitioned%20into%20GlobalGeoTree-6M%20for%20model%20pretraining%20and%20curated%20evaluation%20subsets%2C%20primarily%20GlobalGeoTree-10kEval%20for%20zero-shot%20and%20few-shot%20benchmarking.%20To%20demonstrate%20the%20utility%20of%20the%20dataset%2C%20we%20introduce%20a%20baseline%20model%2C%20GeoTreeCLIP%2C%20which%20leverages%20paired%20remote%20sensing%20data%20and%20taxonomic%20text%20labels%20within%20a%20vision-language%20framework%20pretrained%20on%20GlobalGeoTree-6M.%20Experimental%20results%20show%20that%20GeoTreeCLIP%20achieves%20substantial%20improvements%20in%20zero-%20and%20few-shot%20classification%20on%20GlobalGeoTree-10kEval%20over%20existing%20advanced%20models.%20By%20making%20the%20dataset%2C%20models%2C%20and%20code%20publicly%20available%2C%20we%20aim%20to%20establish%20a%20benchmark%20to%20advance%20tree%20species%20classification%20and%20foster%20innovation%20in%20biodiversity%20research%20and%20ecological%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2505.12513v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobalGeoTree%253A%2520A%2520Multi-Granular%2520Vision-Language%2520Dataset%2520for%2520Global%2520Tree%2520Species%2520Classification%26entry.906535625%3DYang%2520Mu%2520and%2520Zhitong%2520Xiong%2520and%2520Yi%2520Wang%2520and%2520Muhammad%2520Shahzad%2520and%2520Franz%2520Essl%2520and%2520Holger%2520Kreft%2520and%2520Mark%2520van%2520Kleunen%2520and%2520Xiao%2520Xiang%2520Zhu%26entry.1292438233%3DGlobal%2520tree%2520species%2520mapping%2520using%2520remote%2520sensing%2520data%2520is%2520vital%2520for%2520biodiversity%2520monitoring%252C%2520forest%2520management%252C%2520and%2520ecological%2520research.%2520However%252C%2520progress%2520in%2520this%2520field%2520has%2520been%2520constrained%2520by%2520the%2520scarcity%2520of%2520large-scale%252C%2520labeled%2520datasets.%2520To%2520address%2520this%252C%2520we%2520introduce%2520GlobalGeoTree%252C%2520a%2520comprehensive%2520global%2520dataset%2520for%2520tree%2520species%2520classification.%2520GlobalGeoTree%2520comprises%25206.3%2520million%2520geolocated%2520tree%2520occurrences%252C%2520spanning%2520275%2520families%252C%25202%252C734%2520genera%252C%2520and%252021%252C001%2520species%2520across%2520the%2520hierarchical%2520taxonomic%2520levels.%2520Each%2520sample%2520is%2520paired%2520with%2520Sentinel-2%2520image%2520time%2520series%2520and%252027%2520auxiliary%2520environmental%2520variables%252C%2520encompassing%2520bioclimatic%252C%2520geographic%252C%2520and%2520soil%2520data.%2520The%2520dataset%2520is%2520partitioned%2520into%2520GlobalGeoTree-6M%2520for%2520model%2520pretraining%2520and%2520curated%2520evaluation%2520subsets%252C%2520primarily%2520GlobalGeoTree-10kEval%2520for%2520zero-shot%2520and%2520few-shot%2520benchmarking.%2520To%2520demonstrate%2520the%2520utility%2520of%2520the%2520dataset%252C%2520we%2520introduce%2520a%2520baseline%2520model%252C%2520GeoTreeCLIP%252C%2520which%2520leverages%2520paired%2520remote%2520sensing%2520data%2520and%2520taxonomic%2520text%2520labels%2520within%2520a%2520vision-language%2520framework%2520pretrained%2520on%2520GlobalGeoTree-6M.%2520Experimental%2520results%2520show%2520that%2520GeoTreeCLIP%2520achieves%2520substantial%2520improvements%2520in%2520zero-%2520and%2520few-shot%2520classification%2520on%2520GlobalGeoTree-10kEval%2520over%2520existing%2520advanced%2520models.%2520By%2520making%2520the%2520dataset%252C%2520models%252C%2520and%2520code%2520publicly%2520available%252C%2520we%2520aim%2520to%2520establish%2520a%2520benchmark%2520to%2520advance%2520tree%2520species%2520classification%2520and%2520foster%2520innovation%2520in%2520biodiversity%2520research%2520and%2520ecological%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12513v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GlobalGeoTree%3A%20A%20Multi-Granular%20Vision-Language%20Dataset%20for%20Global%20Tree%20Species%20Classification&entry.906535625=Yang%20Mu%20and%20Zhitong%20Xiong%20and%20Yi%20Wang%20and%20Muhammad%20Shahzad%20and%20Franz%20Essl%20and%20Holger%20Kreft%20and%20Mark%20van%20Kleunen%20and%20Xiao%20Xiang%20Zhu&entry.1292438233=Global%20tree%20species%20mapping%20using%20remote%20sensing%20data%20is%20vital%20for%20biodiversity%20monitoring%2C%20forest%20management%2C%20and%20ecological%20research.%20However%2C%20progress%20in%20this%20field%20has%20been%20constrained%20by%20the%20scarcity%20of%20large-scale%2C%20labeled%20datasets.%20To%20address%20this%2C%20we%20introduce%20GlobalGeoTree%2C%20a%20comprehensive%20global%20dataset%20for%20tree%20species%20classification.%20GlobalGeoTree%20comprises%206.3%20million%20geolocated%20tree%20occurrences%2C%20spanning%20275%20families%2C%202%2C734%20genera%2C%20and%2021%2C001%20species%20across%20the%20hierarchical%20taxonomic%20levels.%20Each%20sample%20is%20paired%20with%20Sentinel-2%20image%20time%20series%20and%2027%20auxiliary%20environmental%20variables%2C%20encompassing%20bioclimatic%2C%20geographic%2C%20and%20soil%20data.%20The%20dataset%20is%20partitioned%20into%20GlobalGeoTree-6M%20for%20model%20pretraining%20and%20curated%20evaluation%20subsets%2C%20primarily%20GlobalGeoTree-10kEval%20for%20zero-shot%20and%20few-shot%20benchmarking.%20To%20demonstrate%20the%20utility%20of%20the%20dataset%2C%20we%20introduce%20a%20baseline%20model%2C%20GeoTreeCLIP%2C%20which%20leverages%20paired%20remote%20sensing%20data%20and%20taxonomic%20text%20labels%20within%20a%20vision-language%20framework%20pretrained%20on%20GlobalGeoTree-6M.%20Experimental%20results%20show%20that%20GeoTreeCLIP%20achieves%20substantial%20improvements%20in%20zero-%20and%20few-shot%20classification%20on%20GlobalGeoTree-10kEval%20over%20existing%20advanced%20models.%20By%20making%20the%20dataset%2C%20models%2C%20and%20code%20publicly%20available%2C%20we%20aim%20to%20establish%20a%20benchmark%20to%20advance%20tree%20species%20classification%20and%20foster%20innovation%20in%20biodiversity%20research%20and%20ecological%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2505.12513v3&entry.124074799=Read"},
{"title": "Data Valuation for LLM Fine-Tuning: Efficient Shapley Value Approximation via Language Model Arithmetic", "author": "M\u00e9lissa Tamine and Otmane Sakhi and Benjamin Heymann", "abstract": "Data is a critical asset for training large language models (LLMs), alongside compute resources and skilled workers. While some training data is publicly available, substantial investment is required to generate proprietary datasets, such as human preference annotations or to curate new ones from existing sources. As larger datasets generally yield better model performance, two natural questions arise. First, how can data owners make informed decisions about curation strategies and data sources investment? Second, how can multiple data owners collaboratively pool their resources to train superior models while fairly distributing the benefits? This problem, data valuation, which is not specific to large language models, has been addressed by the machine learning community through the lens of cooperative game theory, with the Shapley value being the prevalent solution concept. However, computing Shapley values is notoriously expensive for data valuation, typically requiring numerous model retrainings, which can become prohibitive for large machine learning models. In this work, we demonstrate that this computational challenge is dramatically simplified for LLMs trained with Direct Preference Optimization (DPO). We show how the specific mathematical structure of DPO enables scalable Shapley value computation. We believe this observation unlocks many applications at the intersection of data valuation and large language models.", "link": "http://arxiv.org/abs/2512.15765v2", "date": "2026-01-26", "relevancy": 2.3979, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4832}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4832}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Valuation%20for%20LLM%20Fine-Tuning%3A%20Efficient%20Shapley%20Value%20Approximation%20via%20Language%20Model%20Arithmetic&body=Title%3A%20Data%20Valuation%20for%20LLM%20Fine-Tuning%3A%20Efficient%20Shapley%20Value%20Approximation%20via%20Language%20Model%20Arithmetic%0AAuthor%3A%20M%C3%A9lissa%20Tamine%20and%20Otmane%20Sakhi%20and%20Benjamin%20Heymann%0AAbstract%3A%20Data%20is%20a%20critical%20asset%20for%20training%20large%20language%20models%20%28LLMs%29%2C%20alongside%20compute%20resources%20and%20skilled%20workers.%20While%20some%20training%20data%20is%20publicly%20available%2C%20substantial%20investment%20is%20required%20to%20generate%20proprietary%20datasets%2C%20such%20as%20human%20preference%20annotations%20or%20to%20curate%20new%20ones%20from%20existing%20sources.%20As%20larger%20datasets%20generally%20yield%20better%20model%20performance%2C%20two%20natural%20questions%20arise.%20First%2C%20how%20can%20data%20owners%20make%20informed%20decisions%20about%20curation%20strategies%20and%20data%20sources%20investment%3F%20Second%2C%20how%20can%20multiple%20data%20owners%20collaboratively%20pool%20their%20resources%20to%20train%20superior%20models%20while%20fairly%20distributing%20the%20benefits%3F%20This%20problem%2C%20data%20valuation%2C%20which%20is%20not%20specific%20to%20large%20language%20models%2C%20has%20been%20addressed%20by%20the%20machine%20learning%20community%20through%20the%20lens%20of%20cooperative%20game%20theory%2C%20with%20the%20Shapley%20value%20being%20the%20prevalent%20solution%20concept.%20However%2C%20computing%20Shapley%20values%20is%20notoriously%20expensive%20for%20data%20valuation%2C%20typically%20requiring%20numerous%20model%20retrainings%2C%20which%20can%20become%20prohibitive%20for%20large%20machine%20learning%20models.%20In%20this%20work%2C%20we%20demonstrate%20that%20this%20computational%20challenge%20is%20dramatically%20simplified%20for%20LLMs%20trained%20with%20Direct%20Preference%20Optimization%20%28DPO%29.%20We%20show%20how%20the%20specific%20mathematical%20structure%20of%20DPO%20enables%20scalable%20Shapley%20value%20computation.%20We%20believe%20this%20observation%20unlocks%20many%20applications%20at%20the%20intersection%20of%20data%20valuation%20and%20large%20language%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15765v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Valuation%2520for%2520LLM%2520Fine-Tuning%253A%2520Efficient%2520Shapley%2520Value%2520Approximation%2520via%2520Language%2520Model%2520Arithmetic%26entry.906535625%3DM%25C3%25A9lissa%2520Tamine%2520and%2520Otmane%2520Sakhi%2520and%2520Benjamin%2520Heymann%26entry.1292438233%3DData%2520is%2520a%2520critical%2520asset%2520for%2520training%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520alongside%2520compute%2520resources%2520and%2520skilled%2520workers.%2520While%2520some%2520training%2520data%2520is%2520publicly%2520available%252C%2520substantial%2520investment%2520is%2520required%2520to%2520generate%2520proprietary%2520datasets%252C%2520such%2520as%2520human%2520preference%2520annotations%2520or%2520to%2520curate%2520new%2520ones%2520from%2520existing%2520sources.%2520As%2520larger%2520datasets%2520generally%2520yield%2520better%2520model%2520performance%252C%2520two%2520natural%2520questions%2520arise.%2520First%252C%2520how%2520can%2520data%2520owners%2520make%2520informed%2520decisions%2520about%2520curation%2520strategies%2520and%2520data%2520sources%2520investment%253F%2520Second%252C%2520how%2520can%2520multiple%2520data%2520owners%2520collaboratively%2520pool%2520their%2520resources%2520to%2520train%2520superior%2520models%2520while%2520fairly%2520distributing%2520the%2520benefits%253F%2520This%2520problem%252C%2520data%2520valuation%252C%2520which%2520is%2520not%2520specific%2520to%2520large%2520language%2520models%252C%2520has%2520been%2520addressed%2520by%2520the%2520machine%2520learning%2520community%2520through%2520the%2520lens%2520of%2520cooperative%2520game%2520theory%252C%2520with%2520the%2520Shapley%2520value%2520being%2520the%2520prevalent%2520solution%2520concept.%2520However%252C%2520computing%2520Shapley%2520values%2520is%2520notoriously%2520expensive%2520for%2520data%2520valuation%252C%2520typically%2520requiring%2520numerous%2520model%2520retrainings%252C%2520which%2520can%2520become%2520prohibitive%2520for%2520large%2520machine%2520learning%2520models.%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520that%2520this%2520computational%2520challenge%2520is%2520dramatically%2520simplified%2520for%2520LLMs%2520trained%2520with%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529.%2520We%2520show%2520how%2520the%2520specific%2520mathematical%2520structure%2520of%2520DPO%2520enables%2520scalable%2520Shapley%2520value%2520computation.%2520We%2520believe%2520this%2520observation%2520unlocks%2520many%2520applications%2520at%2520the%2520intersection%2520of%2520data%2520valuation%2520and%2520large%2520language%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15765v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Valuation%20for%20LLM%20Fine-Tuning%3A%20Efficient%20Shapley%20Value%20Approximation%20via%20Language%20Model%20Arithmetic&entry.906535625=M%C3%A9lissa%20Tamine%20and%20Otmane%20Sakhi%20and%20Benjamin%20Heymann&entry.1292438233=Data%20is%20a%20critical%20asset%20for%20training%20large%20language%20models%20%28LLMs%29%2C%20alongside%20compute%20resources%20and%20skilled%20workers.%20While%20some%20training%20data%20is%20publicly%20available%2C%20substantial%20investment%20is%20required%20to%20generate%20proprietary%20datasets%2C%20such%20as%20human%20preference%20annotations%20or%20to%20curate%20new%20ones%20from%20existing%20sources.%20As%20larger%20datasets%20generally%20yield%20better%20model%20performance%2C%20two%20natural%20questions%20arise.%20First%2C%20how%20can%20data%20owners%20make%20informed%20decisions%20about%20curation%20strategies%20and%20data%20sources%20investment%3F%20Second%2C%20how%20can%20multiple%20data%20owners%20collaboratively%20pool%20their%20resources%20to%20train%20superior%20models%20while%20fairly%20distributing%20the%20benefits%3F%20This%20problem%2C%20data%20valuation%2C%20which%20is%20not%20specific%20to%20large%20language%20models%2C%20has%20been%20addressed%20by%20the%20machine%20learning%20community%20through%20the%20lens%20of%20cooperative%20game%20theory%2C%20with%20the%20Shapley%20value%20being%20the%20prevalent%20solution%20concept.%20However%2C%20computing%20Shapley%20values%20is%20notoriously%20expensive%20for%20data%20valuation%2C%20typically%20requiring%20numerous%20model%20retrainings%2C%20which%20can%20become%20prohibitive%20for%20large%20machine%20learning%20models.%20In%20this%20work%2C%20we%20demonstrate%20that%20this%20computational%20challenge%20is%20dramatically%20simplified%20for%20LLMs%20trained%20with%20Direct%20Preference%20Optimization%20%28DPO%29.%20We%20show%20how%20the%20specific%20mathematical%20structure%20of%20DPO%20enables%20scalable%20Shapley%20value%20computation.%20We%20believe%20this%20observation%20unlocks%20many%20applications%20at%20the%20intersection%20of%20data%20valuation%20and%20large%20language%20models.&entry.1838667208=http%3A//arxiv.org/abs/2512.15765v2&entry.124074799=Read"},
{"title": "On the Failure of Latent State Persistence in Large Language Models", "author": "Jen-tse Huang and Kaiser Sun and Wenxuan Wang and Mark Dredze", "abstract": "While Large Language Models (LLMs) excel in reasoning, whether they can sustain persistent latent states remains under-explored. The capacity to maintain and manipulate unexpressed, internal representations-analogous to human working memory-is a cornerstone of complex reasoning. In this paper, we formalize and quantify the \"Latent State Persistence\" (LSP) gap through three novel experiments. First, we utilize a Number Guessing Game, demonstrating that across independent queries, LLMs fail to allocate probability mass to a singular hidden choice, violating a fundamental probabilistic principle. Second, we employ a Yes-No Game to show that as the number of questions increases, LLMs suffer from \"concept drift,\" leading to inevitable self-contradictions due to the lack of LSP. Finally, inspired by Mathematical Mentalism, we task models with tracking transformations on hidden variables, revealing a failure in variable binding and state evolution when the initial state is not explicitly present in the context. Collectively, these findings suggest that LLMs function as reactive post-hoc solvers rather than proactive planners with LSP. Our work provides a framework for evaluating the fidelity of internal representations and highlights a fundamental architectural divergence between autoregressive transformers and human-like cognition.", "link": "http://arxiv.org/abs/2505.10571v5", "date": "2026-01-26", "relevancy": 2.3948, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.483}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.483}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Failure%20of%20Latent%20State%20Persistence%20in%20Large%20Language%20Models&body=Title%3A%20On%20the%20Failure%20of%20Latent%20State%20Persistence%20in%20Large%20Language%20Models%0AAuthor%3A%20Jen-tse%20Huang%20and%20Kaiser%20Sun%20and%20Wenxuan%20Wang%20and%20Mark%20Dredze%0AAbstract%3A%20While%20Large%20Language%20Models%20%28LLMs%29%20excel%20in%20reasoning%2C%20whether%20they%20can%20sustain%20persistent%20latent%20states%20remains%20under-explored.%20The%20capacity%20to%20maintain%20and%20manipulate%20unexpressed%2C%20internal%20representations-analogous%20to%20human%20working%20memory-is%20a%20cornerstone%20of%20complex%20reasoning.%20In%20this%20paper%2C%20we%20formalize%20and%20quantify%20the%20%22Latent%20State%20Persistence%22%20%28LSP%29%20gap%20through%20three%20novel%20experiments.%20First%2C%20we%20utilize%20a%20Number%20Guessing%20Game%2C%20demonstrating%20that%20across%20independent%20queries%2C%20LLMs%20fail%20to%20allocate%20probability%20mass%20to%20a%20singular%20hidden%20choice%2C%20violating%20a%20fundamental%20probabilistic%20principle.%20Second%2C%20we%20employ%20a%20Yes-No%20Game%20to%20show%20that%20as%20the%20number%20of%20questions%20increases%2C%20LLMs%20suffer%20from%20%22concept%20drift%2C%22%20leading%20to%20inevitable%20self-contradictions%20due%20to%20the%20lack%20of%20LSP.%20Finally%2C%20inspired%20by%20Mathematical%20Mentalism%2C%20we%20task%20models%20with%20tracking%20transformations%20on%20hidden%20variables%2C%20revealing%20a%20failure%20in%20variable%20binding%20and%20state%20evolution%20when%20the%20initial%20state%20is%20not%20explicitly%20present%20in%20the%20context.%20Collectively%2C%20these%20findings%20suggest%20that%20LLMs%20function%20as%20reactive%20post-hoc%20solvers%20rather%20than%20proactive%20planners%20with%20LSP.%20Our%20work%20provides%20a%20framework%20for%20evaluating%20the%20fidelity%20of%20internal%20representations%20and%20highlights%20a%20fundamental%20architectural%20divergence%20between%20autoregressive%20transformers%20and%20human-like%20cognition.%0ALink%3A%20http%3A//arxiv.org/abs/2505.10571v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Failure%2520of%2520Latent%2520State%2520Persistence%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DJen-tse%2520Huang%2520and%2520Kaiser%2520Sun%2520and%2520Wenxuan%2520Wang%2520and%2520Mark%2520Dredze%26entry.1292438233%3DWhile%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520excel%2520in%2520reasoning%252C%2520whether%2520they%2520can%2520sustain%2520persistent%2520latent%2520states%2520remains%2520under-explored.%2520The%2520capacity%2520to%2520maintain%2520and%2520manipulate%2520unexpressed%252C%2520internal%2520representations-analogous%2520to%2520human%2520working%2520memory-is%2520a%2520cornerstone%2520of%2520complex%2520reasoning.%2520In%2520this%2520paper%252C%2520we%2520formalize%2520and%2520quantify%2520the%2520%2522Latent%2520State%2520Persistence%2522%2520%2528LSP%2529%2520gap%2520through%2520three%2520novel%2520experiments.%2520First%252C%2520we%2520utilize%2520a%2520Number%2520Guessing%2520Game%252C%2520demonstrating%2520that%2520across%2520independent%2520queries%252C%2520LLMs%2520fail%2520to%2520allocate%2520probability%2520mass%2520to%2520a%2520singular%2520hidden%2520choice%252C%2520violating%2520a%2520fundamental%2520probabilistic%2520principle.%2520Second%252C%2520we%2520employ%2520a%2520Yes-No%2520Game%2520to%2520show%2520that%2520as%2520the%2520number%2520of%2520questions%2520increases%252C%2520LLMs%2520suffer%2520from%2520%2522concept%2520drift%252C%2522%2520leading%2520to%2520inevitable%2520self-contradictions%2520due%2520to%2520the%2520lack%2520of%2520LSP.%2520Finally%252C%2520inspired%2520by%2520Mathematical%2520Mentalism%252C%2520we%2520task%2520models%2520with%2520tracking%2520transformations%2520on%2520hidden%2520variables%252C%2520revealing%2520a%2520failure%2520in%2520variable%2520binding%2520and%2520state%2520evolution%2520when%2520the%2520initial%2520state%2520is%2520not%2520explicitly%2520present%2520in%2520the%2520context.%2520Collectively%252C%2520these%2520findings%2520suggest%2520that%2520LLMs%2520function%2520as%2520reactive%2520post-hoc%2520solvers%2520rather%2520than%2520proactive%2520planners%2520with%2520LSP.%2520Our%2520work%2520provides%2520a%2520framework%2520for%2520evaluating%2520the%2520fidelity%2520of%2520internal%2520representations%2520and%2520highlights%2520a%2520fundamental%2520architectural%2520divergence%2520between%2520autoregressive%2520transformers%2520and%2520human-like%2520cognition.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10571v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Failure%20of%20Latent%20State%20Persistence%20in%20Large%20Language%20Models&entry.906535625=Jen-tse%20Huang%20and%20Kaiser%20Sun%20and%20Wenxuan%20Wang%20and%20Mark%20Dredze&entry.1292438233=While%20Large%20Language%20Models%20%28LLMs%29%20excel%20in%20reasoning%2C%20whether%20they%20can%20sustain%20persistent%20latent%20states%20remains%20under-explored.%20The%20capacity%20to%20maintain%20and%20manipulate%20unexpressed%2C%20internal%20representations-analogous%20to%20human%20working%20memory-is%20a%20cornerstone%20of%20complex%20reasoning.%20In%20this%20paper%2C%20we%20formalize%20and%20quantify%20the%20%22Latent%20State%20Persistence%22%20%28LSP%29%20gap%20through%20three%20novel%20experiments.%20First%2C%20we%20utilize%20a%20Number%20Guessing%20Game%2C%20demonstrating%20that%20across%20independent%20queries%2C%20LLMs%20fail%20to%20allocate%20probability%20mass%20to%20a%20singular%20hidden%20choice%2C%20violating%20a%20fundamental%20probabilistic%20principle.%20Second%2C%20we%20employ%20a%20Yes-No%20Game%20to%20show%20that%20as%20the%20number%20of%20questions%20increases%2C%20LLMs%20suffer%20from%20%22concept%20drift%2C%22%20leading%20to%20inevitable%20self-contradictions%20due%20to%20the%20lack%20of%20LSP.%20Finally%2C%20inspired%20by%20Mathematical%20Mentalism%2C%20we%20task%20models%20with%20tracking%20transformations%20on%20hidden%20variables%2C%20revealing%20a%20failure%20in%20variable%20binding%20and%20state%20evolution%20when%20the%20initial%20state%20is%20not%20explicitly%20present%20in%20the%20context.%20Collectively%2C%20these%20findings%20suggest%20that%20LLMs%20function%20as%20reactive%20post-hoc%20solvers%20rather%20than%20proactive%20planners%20with%20LSP.%20Our%20work%20provides%20a%20framework%20for%20evaluating%20the%20fidelity%20of%20internal%20representations%20and%20highlights%20a%20fundamental%20architectural%20divergence%20between%20autoregressive%20transformers%20and%20human-like%20cognition.&entry.1838667208=http%3A//arxiv.org/abs/2505.10571v5&entry.124074799=Read"},
{"title": "Superlinear Multi-Step Attention", "author": "Yufeng Huang", "abstract": "In this paper, we propose \\textbf{Superlinear attention}, a fully trainable multi-step attention architecture that achieves subquadratic complexity for long sequences while preserving \\textbf{random context access} (a.k.a.\\ structural non-exclusion): no eligible token position is structurally excluded from being selected for attention. Superlinear attention reformulates standard causal self-attention as a multi-step search problem with $N$ steps, yielding an overall complexity of $O(L^{1+\\frac{1}{N}})$. To illustrate the architecture, we present a baseline $N=2$ implementation, which is algorithmically analogous to standard jump search. In this $O(L^{3/2})$ instantiation, the first step performs $O(L^{3/2})$ span-search to select relevant spans of the sequence, and the second step applies $O(L^{3/2})$ span-attention (standard attention restricted to the selected spans). In an upscaled $O(L^{1.54})$ configuration for robustness, we achieve an average decoding throughput of 114 tokens/sec at 1M context length and 80 tokens/sec at 10M context in our implementation on a modified 30B hybrid MoE model on a single B200 GPU. With limited training, we also obtain strong performance on the NIAH (Needle In A Haystack) task up to 256K context length, demonstrating that the routed span selection is learnable end-to-end. This paper emphasizes architectural formulation, scaling analysis, and systems feasibility, and presents initial validation; comprehensive quality evaluations across diverse long-context tasks are left to future work.", "link": "http://arxiv.org/abs/2601.18401v1", "date": "2026-01-26", "relevancy": 2.3883, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5079}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4625}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Superlinear%20Multi-Step%20Attention&body=Title%3A%20Superlinear%20Multi-Step%20Attention%0AAuthor%3A%20Yufeng%20Huang%0AAbstract%3A%20In%20this%20paper%2C%20we%20propose%20%5Ctextbf%7BSuperlinear%20attention%7D%2C%20a%20fully%20trainable%20multi-step%20attention%20architecture%20that%20achieves%20subquadratic%20complexity%20for%20long%20sequences%20while%20preserving%20%5Ctextbf%7Brandom%20context%20access%7D%20%28a.k.a.%5C%20structural%20non-exclusion%29%3A%20no%20eligible%20token%20position%20is%20structurally%20excluded%20from%20being%20selected%20for%20attention.%20Superlinear%20attention%20reformulates%20standard%20causal%20self-attention%20as%20a%20multi-step%20search%20problem%20with%20%24N%24%20steps%2C%20yielding%20an%20overall%20complexity%20of%20%24O%28L%5E%7B1%2B%5Cfrac%7B1%7D%7BN%7D%7D%29%24.%20To%20illustrate%20the%20architecture%2C%20we%20present%20a%20baseline%20%24N%3D2%24%20implementation%2C%20which%20is%20algorithmically%20analogous%20to%20standard%20jump%20search.%20In%20this%20%24O%28L%5E%7B3/2%7D%29%24%20instantiation%2C%20the%20first%20step%20performs%20%24O%28L%5E%7B3/2%7D%29%24%20span-search%20to%20select%20relevant%20spans%20of%20the%20sequence%2C%20and%20the%20second%20step%20applies%20%24O%28L%5E%7B3/2%7D%29%24%20span-attention%20%28standard%20attention%20restricted%20to%20the%20selected%20spans%29.%20In%20an%20upscaled%20%24O%28L%5E%7B1.54%7D%29%24%20configuration%20for%20robustness%2C%20we%20achieve%20an%20average%20decoding%20throughput%20of%20114%20tokens/sec%20at%201M%20context%20length%20and%2080%20tokens/sec%20at%2010M%20context%20in%20our%20implementation%20on%20a%20modified%2030B%20hybrid%20MoE%20model%20on%20a%20single%20B200%20GPU.%20With%20limited%20training%2C%20we%20also%20obtain%20strong%20performance%20on%20the%20NIAH%20%28Needle%20In%20A%20Haystack%29%20task%20up%20to%20256K%20context%20length%2C%20demonstrating%20that%20the%20routed%20span%20selection%20is%20learnable%20end-to-end.%20This%20paper%20emphasizes%20architectural%20formulation%2C%20scaling%20analysis%2C%20and%20systems%20feasibility%2C%20and%20presents%20initial%20validation%3B%20comprehensive%20quality%20evaluations%20across%20diverse%20long-context%20tasks%20are%20left%20to%20future%20work.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18401v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuperlinear%2520Multi-Step%2520Attention%26entry.906535625%3DYufeng%2520Huang%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520propose%2520%255Ctextbf%257BSuperlinear%2520attention%257D%252C%2520a%2520fully%2520trainable%2520multi-step%2520attention%2520architecture%2520that%2520achieves%2520subquadratic%2520complexity%2520for%2520long%2520sequences%2520while%2520preserving%2520%255Ctextbf%257Brandom%2520context%2520access%257D%2520%2528a.k.a.%255C%2520structural%2520non-exclusion%2529%253A%2520no%2520eligible%2520token%2520position%2520is%2520structurally%2520excluded%2520from%2520being%2520selected%2520for%2520attention.%2520Superlinear%2520attention%2520reformulates%2520standard%2520causal%2520self-attention%2520as%2520a%2520multi-step%2520search%2520problem%2520with%2520%2524N%2524%2520steps%252C%2520yielding%2520an%2520overall%2520complexity%2520of%2520%2524O%2528L%255E%257B1%252B%255Cfrac%257B1%257D%257BN%257D%257D%2529%2524.%2520To%2520illustrate%2520the%2520architecture%252C%2520we%2520present%2520a%2520baseline%2520%2524N%253D2%2524%2520implementation%252C%2520which%2520is%2520algorithmically%2520analogous%2520to%2520standard%2520jump%2520search.%2520In%2520this%2520%2524O%2528L%255E%257B3/2%257D%2529%2524%2520instantiation%252C%2520the%2520first%2520step%2520performs%2520%2524O%2528L%255E%257B3/2%257D%2529%2524%2520span-search%2520to%2520select%2520relevant%2520spans%2520of%2520the%2520sequence%252C%2520and%2520the%2520second%2520step%2520applies%2520%2524O%2528L%255E%257B3/2%257D%2529%2524%2520span-attention%2520%2528standard%2520attention%2520restricted%2520to%2520the%2520selected%2520spans%2529.%2520In%2520an%2520upscaled%2520%2524O%2528L%255E%257B1.54%257D%2529%2524%2520configuration%2520for%2520robustness%252C%2520we%2520achieve%2520an%2520average%2520decoding%2520throughput%2520of%2520114%2520tokens/sec%2520at%25201M%2520context%2520length%2520and%252080%2520tokens/sec%2520at%252010M%2520context%2520in%2520our%2520implementation%2520on%2520a%2520modified%252030B%2520hybrid%2520MoE%2520model%2520on%2520a%2520single%2520B200%2520GPU.%2520With%2520limited%2520training%252C%2520we%2520also%2520obtain%2520strong%2520performance%2520on%2520the%2520NIAH%2520%2528Needle%2520In%2520A%2520Haystack%2529%2520task%2520up%2520to%2520256K%2520context%2520length%252C%2520demonstrating%2520that%2520the%2520routed%2520span%2520selection%2520is%2520learnable%2520end-to-end.%2520This%2520paper%2520emphasizes%2520architectural%2520formulation%252C%2520scaling%2520analysis%252C%2520and%2520systems%2520feasibility%252C%2520and%2520presents%2520initial%2520validation%253B%2520comprehensive%2520quality%2520evaluations%2520across%2520diverse%2520long-context%2520tasks%2520are%2520left%2520to%2520future%2520work.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18401v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Superlinear%20Multi-Step%20Attention&entry.906535625=Yufeng%20Huang&entry.1292438233=In%20this%20paper%2C%20we%20propose%20%5Ctextbf%7BSuperlinear%20attention%7D%2C%20a%20fully%20trainable%20multi-step%20attention%20architecture%20that%20achieves%20subquadratic%20complexity%20for%20long%20sequences%20while%20preserving%20%5Ctextbf%7Brandom%20context%20access%7D%20%28a.k.a.%5C%20structural%20non-exclusion%29%3A%20no%20eligible%20token%20position%20is%20structurally%20excluded%20from%20being%20selected%20for%20attention.%20Superlinear%20attention%20reformulates%20standard%20causal%20self-attention%20as%20a%20multi-step%20search%20problem%20with%20%24N%24%20steps%2C%20yielding%20an%20overall%20complexity%20of%20%24O%28L%5E%7B1%2B%5Cfrac%7B1%7D%7BN%7D%7D%29%24.%20To%20illustrate%20the%20architecture%2C%20we%20present%20a%20baseline%20%24N%3D2%24%20implementation%2C%20which%20is%20algorithmically%20analogous%20to%20standard%20jump%20search.%20In%20this%20%24O%28L%5E%7B3/2%7D%29%24%20instantiation%2C%20the%20first%20step%20performs%20%24O%28L%5E%7B3/2%7D%29%24%20span-search%20to%20select%20relevant%20spans%20of%20the%20sequence%2C%20and%20the%20second%20step%20applies%20%24O%28L%5E%7B3/2%7D%29%24%20span-attention%20%28standard%20attention%20restricted%20to%20the%20selected%20spans%29.%20In%20an%20upscaled%20%24O%28L%5E%7B1.54%7D%29%24%20configuration%20for%20robustness%2C%20we%20achieve%20an%20average%20decoding%20throughput%20of%20114%20tokens/sec%20at%201M%20context%20length%20and%2080%20tokens/sec%20at%2010M%20context%20in%20our%20implementation%20on%20a%20modified%2030B%20hybrid%20MoE%20model%20on%20a%20single%20B200%20GPU.%20With%20limited%20training%2C%20we%20also%20obtain%20strong%20performance%20on%20the%20NIAH%20%28Needle%20In%20A%20Haystack%29%20task%20up%20to%20256K%20context%20length%2C%20demonstrating%20that%20the%20routed%20span%20selection%20is%20learnable%20end-to-end.%20This%20paper%20emphasizes%20architectural%20formulation%2C%20scaling%20analysis%2C%20and%20systems%20feasibility%2C%20and%20presents%20initial%20validation%3B%20comprehensive%20quality%20evaluations%20across%20diverse%20long-context%20tasks%20are%20left%20to%20future%20work.&entry.1838667208=http%3A//arxiv.org/abs/2601.18401v1&entry.124074799=Read"},
{"title": "DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception", "author": "Tim Broedermannn and Christos Sakaridis and Luigi Piccinelli and Wim Abbeloos and Luc Van Gool", "abstract": "Robust semantic perception for autonomous vehicles relies on effectively combining multiple sensors with complementary strengths and weaknesses. State-of-the-art sensor fusion approaches to semantic perception often treat sensor data uniformly across the spatial extent of the input, which hinders performance when faced with challenging conditions. By contrast, we propose a novel depth-guided multimodal fusion method that upgrades condition-aware fusion by integrating depth information. Our network, DGFusion, poses multimodal segmentation as a multi-task problem, utilizing the lidar measurements, which are typically available in outdoor sensor suites, both as one of the model's inputs and as ground truth for learning depth. Our corresponding auxiliary depth head helps to learn depth-aware features, which are encoded into spatially varying local depth tokens that condition our attentive cross-modal fusion. Together with a global condition token, these local depth tokens dynamically adapt sensor fusion to the spatially varying reliability of each sensor across the scene, which largely depends on depth. In addition, we propose a robust loss for our depth, which is essential for learning from lidar inputs that are typically sparse and noisy in adverse conditions. Our method achieves state-of-the-art panoptic and semantic segmentation performance on the challenging MUSES and DeLiVER datasets. Code and models are available at https://github.com/timbroed/DGFusion", "link": "http://arxiv.org/abs/2509.09828v3", "date": "2026-01-26", "relevancy": 2.3877, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6321}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.601}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DGFusion%3A%20Depth-Guided%20Sensor%20Fusion%20for%20Robust%20Semantic%20Perception&body=Title%3A%20DGFusion%3A%20Depth-Guided%20Sensor%20Fusion%20for%20Robust%20Semantic%20Perception%0AAuthor%3A%20Tim%20Broedermannn%20and%20Christos%20Sakaridis%20and%20Luigi%20Piccinelli%20and%20Wim%20Abbeloos%20and%20Luc%20Van%20Gool%0AAbstract%3A%20Robust%20semantic%20perception%20for%20autonomous%20vehicles%20relies%20on%20effectively%20combining%20multiple%20sensors%20with%20complementary%20strengths%20and%20weaknesses.%20State-of-the-art%20sensor%20fusion%20approaches%20to%20semantic%20perception%20often%20treat%20sensor%20data%20uniformly%20across%20the%20spatial%20extent%20of%20the%20input%2C%20which%20hinders%20performance%20when%20faced%20with%20challenging%20conditions.%20By%20contrast%2C%20we%20propose%20a%20novel%20depth-guided%20multimodal%20fusion%20method%20that%20upgrades%20condition-aware%20fusion%20by%20integrating%20depth%20information.%20Our%20network%2C%20DGFusion%2C%20poses%20multimodal%20segmentation%20as%20a%20multi-task%20problem%2C%20utilizing%20the%20lidar%20measurements%2C%20which%20are%20typically%20available%20in%20outdoor%20sensor%20suites%2C%20both%20as%20one%20of%20the%20model%27s%20inputs%20and%20as%20ground%20truth%20for%20learning%20depth.%20Our%20corresponding%20auxiliary%20depth%20head%20helps%20to%20learn%20depth-aware%20features%2C%20which%20are%20encoded%20into%20spatially%20varying%20local%20depth%20tokens%20that%20condition%20our%20attentive%20cross-modal%20fusion.%20Together%20with%20a%20global%20condition%20token%2C%20these%20local%20depth%20tokens%20dynamically%20adapt%20sensor%20fusion%20to%20the%20spatially%20varying%20reliability%20of%20each%20sensor%20across%20the%20scene%2C%20which%20largely%20depends%20on%20depth.%20In%20addition%2C%20we%20propose%20a%20robust%20loss%20for%20our%20depth%2C%20which%20is%20essential%20for%20learning%20from%20lidar%20inputs%20that%20are%20typically%20sparse%20and%20noisy%20in%20adverse%20conditions.%20Our%20method%20achieves%20state-of-the-art%20panoptic%20and%20semantic%20segmentation%20performance%20on%20the%20challenging%20MUSES%20and%20DeLiVER%20datasets.%20Code%20and%20models%20are%20available%20at%20https%3A//github.com/timbroed/DGFusion%0ALink%3A%20http%3A//arxiv.org/abs/2509.09828v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDGFusion%253A%2520Depth-Guided%2520Sensor%2520Fusion%2520for%2520Robust%2520Semantic%2520Perception%26entry.906535625%3DTim%2520Broedermannn%2520and%2520Christos%2520Sakaridis%2520and%2520Luigi%2520Piccinelli%2520and%2520Wim%2520Abbeloos%2520and%2520Luc%2520Van%2520Gool%26entry.1292438233%3DRobust%2520semantic%2520perception%2520for%2520autonomous%2520vehicles%2520relies%2520on%2520effectively%2520combining%2520multiple%2520sensors%2520with%2520complementary%2520strengths%2520and%2520weaknesses.%2520State-of-the-art%2520sensor%2520fusion%2520approaches%2520to%2520semantic%2520perception%2520often%2520treat%2520sensor%2520data%2520uniformly%2520across%2520the%2520spatial%2520extent%2520of%2520the%2520input%252C%2520which%2520hinders%2520performance%2520when%2520faced%2520with%2520challenging%2520conditions.%2520By%2520contrast%252C%2520we%2520propose%2520a%2520novel%2520depth-guided%2520multimodal%2520fusion%2520method%2520that%2520upgrades%2520condition-aware%2520fusion%2520by%2520integrating%2520depth%2520information.%2520Our%2520network%252C%2520DGFusion%252C%2520poses%2520multimodal%2520segmentation%2520as%2520a%2520multi-task%2520problem%252C%2520utilizing%2520the%2520lidar%2520measurements%252C%2520which%2520are%2520typically%2520available%2520in%2520outdoor%2520sensor%2520suites%252C%2520both%2520as%2520one%2520of%2520the%2520model%2527s%2520inputs%2520and%2520as%2520ground%2520truth%2520for%2520learning%2520depth.%2520Our%2520corresponding%2520auxiliary%2520depth%2520head%2520helps%2520to%2520learn%2520depth-aware%2520features%252C%2520which%2520are%2520encoded%2520into%2520spatially%2520varying%2520local%2520depth%2520tokens%2520that%2520condition%2520our%2520attentive%2520cross-modal%2520fusion.%2520Together%2520with%2520a%2520global%2520condition%2520token%252C%2520these%2520local%2520depth%2520tokens%2520dynamically%2520adapt%2520sensor%2520fusion%2520to%2520the%2520spatially%2520varying%2520reliability%2520of%2520each%2520sensor%2520across%2520the%2520scene%252C%2520which%2520largely%2520depends%2520on%2520depth.%2520In%2520addition%252C%2520we%2520propose%2520a%2520robust%2520loss%2520for%2520our%2520depth%252C%2520which%2520is%2520essential%2520for%2520learning%2520from%2520lidar%2520inputs%2520that%2520are%2520typically%2520sparse%2520and%2520noisy%2520in%2520adverse%2520conditions.%2520Our%2520method%2520achieves%2520state-of-the-art%2520panoptic%2520and%2520semantic%2520segmentation%2520performance%2520on%2520the%2520challenging%2520MUSES%2520and%2520DeLiVER%2520datasets.%2520Code%2520and%2520models%2520are%2520available%2520at%2520https%253A//github.com/timbroed/DGFusion%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.09828v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DGFusion%3A%20Depth-Guided%20Sensor%20Fusion%20for%20Robust%20Semantic%20Perception&entry.906535625=Tim%20Broedermannn%20and%20Christos%20Sakaridis%20and%20Luigi%20Piccinelli%20and%20Wim%20Abbeloos%20and%20Luc%20Van%20Gool&entry.1292438233=Robust%20semantic%20perception%20for%20autonomous%20vehicles%20relies%20on%20effectively%20combining%20multiple%20sensors%20with%20complementary%20strengths%20and%20weaknesses.%20State-of-the-art%20sensor%20fusion%20approaches%20to%20semantic%20perception%20often%20treat%20sensor%20data%20uniformly%20across%20the%20spatial%20extent%20of%20the%20input%2C%20which%20hinders%20performance%20when%20faced%20with%20challenging%20conditions.%20By%20contrast%2C%20we%20propose%20a%20novel%20depth-guided%20multimodal%20fusion%20method%20that%20upgrades%20condition-aware%20fusion%20by%20integrating%20depth%20information.%20Our%20network%2C%20DGFusion%2C%20poses%20multimodal%20segmentation%20as%20a%20multi-task%20problem%2C%20utilizing%20the%20lidar%20measurements%2C%20which%20are%20typically%20available%20in%20outdoor%20sensor%20suites%2C%20both%20as%20one%20of%20the%20model%27s%20inputs%20and%20as%20ground%20truth%20for%20learning%20depth.%20Our%20corresponding%20auxiliary%20depth%20head%20helps%20to%20learn%20depth-aware%20features%2C%20which%20are%20encoded%20into%20spatially%20varying%20local%20depth%20tokens%20that%20condition%20our%20attentive%20cross-modal%20fusion.%20Together%20with%20a%20global%20condition%20token%2C%20these%20local%20depth%20tokens%20dynamically%20adapt%20sensor%20fusion%20to%20the%20spatially%20varying%20reliability%20of%20each%20sensor%20across%20the%20scene%2C%20which%20largely%20depends%20on%20depth.%20In%20addition%2C%20we%20propose%20a%20robust%20loss%20for%20our%20depth%2C%20which%20is%20essential%20for%20learning%20from%20lidar%20inputs%20that%20are%20typically%20sparse%20and%20noisy%20in%20adverse%20conditions.%20Our%20method%20achieves%20state-of-the-art%20panoptic%20and%20semantic%20segmentation%20performance%20on%20the%20challenging%20MUSES%20and%20DeLiVER%20datasets.%20Code%20and%20models%20are%20available%20at%20https%3A//github.com/timbroed/DGFusion&entry.1838667208=http%3A//arxiv.org/abs/2509.09828v3&entry.124074799=Read"},
{"title": "Dynamic Thinking-Token Selection for Efficient Reasoning in Large Reasoning Models", "author": "Zhenyuan Guo and Tong Chen and Wenlong Meng and Chen Gong and Xin Yu and Chengkun Wei and Wenzhi Chen", "abstract": "Large Reasoning Models (LRMs) excel at solving complex problems by explicitly generating a reasoning trace before deriving the final answer. However, these extended generations incur substantial memory footprint and computational overhead, bottlenecking LRMs' efficiency. This work uses attention maps to analyze the influence of reasoning traces and uncover an interesting phenomenon: only some decision-critical tokens in a reasoning trace steer the model toward the final answer, while the remaining tokens contribute negligibly. Building on this observation, we propose Dynamic Thinking-Token Selection (DynTS). This method identifies decision-critical tokens and retains only their associated Key-Value (KV) cache states during inference, evicting the remaining redundant entries to optimize efficiency.", "link": "http://arxiv.org/abs/2601.18383v1", "date": "2026-01-26", "relevancy": 2.3831, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5625}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4337}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4337}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Thinking-Token%20Selection%20for%20Efficient%20Reasoning%20in%20Large%20Reasoning%20Models&body=Title%3A%20Dynamic%20Thinking-Token%20Selection%20for%20Efficient%20Reasoning%20in%20Large%20Reasoning%20Models%0AAuthor%3A%20Zhenyuan%20Guo%20and%20Tong%20Chen%20and%20Wenlong%20Meng%20and%20Chen%20Gong%20and%20Xin%20Yu%20and%20Chengkun%20Wei%20and%20Wenzhi%20Chen%0AAbstract%3A%20Large%20Reasoning%20Models%20%28LRMs%29%20excel%20at%20solving%20complex%20problems%20by%20explicitly%20generating%20a%20reasoning%20trace%20before%20deriving%20the%20final%20answer.%20However%2C%20these%20extended%20generations%20incur%20substantial%20memory%20footprint%20and%20computational%20overhead%2C%20bottlenecking%20LRMs%27%20efficiency.%20This%20work%20uses%20attention%20maps%20to%20analyze%20the%20influence%20of%20reasoning%20traces%20and%20uncover%20an%20interesting%20phenomenon%3A%20only%20some%20decision-critical%20tokens%20in%20a%20reasoning%20trace%20steer%20the%20model%20toward%20the%20final%20answer%2C%20while%20the%20remaining%20tokens%20contribute%20negligibly.%20Building%20on%20this%20observation%2C%20we%20propose%20Dynamic%20Thinking-Token%20Selection%20%28DynTS%29.%20This%20method%20identifies%20decision-critical%20tokens%20and%20retains%20only%20their%20associated%20Key-Value%20%28KV%29%20cache%20states%20during%20inference%2C%20evicting%20the%20remaining%20redundant%20entries%20to%20optimize%20efficiency.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18383v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Thinking-Token%2520Selection%2520for%2520Efficient%2520Reasoning%2520in%2520Large%2520Reasoning%2520Models%26entry.906535625%3DZhenyuan%2520Guo%2520and%2520Tong%2520Chen%2520and%2520Wenlong%2520Meng%2520and%2520Chen%2520Gong%2520and%2520Xin%2520Yu%2520and%2520Chengkun%2520Wei%2520and%2520Wenzhi%2520Chen%26entry.1292438233%3DLarge%2520Reasoning%2520Models%2520%2528LRMs%2529%2520excel%2520at%2520solving%2520complex%2520problems%2520by%2520explicitly%2520generating%2520a%2520reasoning%2520trace%2520before%2520deriving%2520the%2520final%2520answer.%2520However%252C%2520these%2520extended%2520generations%2520incur%2520substantial%2520memory%2520footprint%2520and%2520computational%2520overhead%252C%2520bottlenecking%2520LRMs%2527%2520efficiency.%2520This%2520work%2520uses%2520attention%2520maps%2520to%2520analyze%2520the%2520influence%2520of%2520reasoning%2520traces%2520and%2520uncover%2520an%2520interesting%2520phenomenon%253A%2520only%2520some%2520decision-critical%2520tokens%2520in%2520a%2520reasoning%2520trace%2520steer%2520the%2520model%2520toward%2520the%2520final%2520answer%252C%2520while%2520the%2520remaining%2520tokens%2520contribute%2520negligibly.%2520Building%2520on%2520this%2520observation%252C%2520we%2520propose%2520Dynamic%2520Thinking-Token%2520Selection%2520%2528DynTS%2529.%2520This%2520method%2520identifies%2520decision-critical%2520tokens%2520and%2520retains%2520only%2520their%2520associated%2520Key-Value%2520%2528KV%2529%2520cache%2520states%2520during%2520inference%252C%2520evicting%2520the%2520remaining%2520redundant%2520entries%2520to%2520optimize%2520efficiency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18383v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Thinking-Token%20Selection%20for%20Efficient%20Reasoning%20in%20Large%20Reasoning%20Models&entry.906535625=Zhenyuan%20Guo%20and%20Tong%20Chen%20and%20Wenlong%20Meng%20and%20Chen%20Gong%20and%20Xin%20Yu%20and%20Chengkun%20Wei%20and%20Wenzhi%20Chen&entry.1292438233=Large%20Reasoning%20Models%20%28LRMs%29%20excel%20at%20solving%20complex%20problems%20by%20explicitly%20generating%20a%20reasoning%20trace%20before%20deriving%20the%20final%20answer.%20However%2C%20these%20extended%20generations%20incur%20substantial%20memory%20footprint%20and%20computational%20overhead%2C%20bottlenecking%20LRMs%27%20efficiency.%20This%20work%20uses%20attention%20maps%20to%20analyze%20the%20influence%20of%20reasoning%20traces%20and%20uncover%20an%20interesting%20phenomenon%3A%20only%20some%20decision-critical%20tokens%20in%20a%20reasoning%20trace%20steer%20the%20model%20toward%20the%20final%20answer%2C%20while%20the%20remaining%20tokens%20contribute%20negligibly.%20Building%20on%20this%20observation%2C%20we%20propose%20Dynamic%20Thinking-Token%20Selection%20%28DynTS%29.%20This%20method%20identifies%20decision-critical%20tokens%20and%20retains%20only%20their%20associated%20Key-Value%20%28KV%29%20cache%20states%20during%20inference%2C%20evicting%20the%20remaining%20redundant%20entries%20to%20optimize%20efficiency.&entry.1838667208=http%3A//arxiv.org/abs/2601.18383v1&entry.124074799=Read"},
{"title": "Reflect: Transparent Principle-Guided Reasoning for Constitutional Alignment at Scale", "author": "Henry Bell and Caroline Zhang and Mohammed Mobasserul Haque and Dhaval Potdar and Samia Zaman and Brandon Fain", "abstract": "The constitutional framework of alignment aims to align large language models (LLMs) with value-laden principles written in natural language (such as to avoid using biased language). Prior work has focused on parameter fine-tuning techniques, such as reinforcement learning from human feedback (RLHF), to instill these principles. However, these approaches are computationally demanding, require careful engineering and tuning, and often require difficult-to-obtain human annotation data. We propose \\textsc{reflect}, an inference-time framework for constitutional alignment that does not require any training or data, providing a plug-and-play approach for aligning an instruction-tuned model to a set of principles. \\textsc{reflect} operates entirely in-context, combining a (i) constitution-conditioned base response with post-generation (ii) self-evaluation, (iii)(a) self-critique, and (iii)(b) final revision. \\textsc{reflect}'s technique of explicit in-context reasoning over principles during post-generation outperforms standard few-shot prompting and provides transparent reasoning traces. Our results demonstrate that \\textsc{reflect} significantly improves LLM conformance to diverse and complex principles, including principles quite distinct from those emphasized in the model's original parameter fine-tuning, without sacrificing factual reasoning. \\textsc{reflect} is particularly effective at reducing the rate of rare but significant violations of principles, thereby improving safety and robustness in the tail end of the distribution of generations. Finally, we show that \\textsc{reflect} naturally generates useful training data for traditional parameter fine-tuning techniques, allowing for efficient scaling and the reduction of inference-time computational overhead in long-term deployment scenarios.", "link": "http://arxiv.org/abs/2601.18730v1", "date": "2026-01-26", "relevancy": 2.3822, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4808}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4808}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reflect%3A%20Transparent%20Principle-Guided%20Reasoning%20for%20Constitutional%20Alignment%20at%20Scale&body=Title%3A%20Reflect%3A%20Transparent%20Principle-Guided%20Reasoning%20for%20Constitutional%20Alignment%20at%20Scale%0AAuthor%3A%20Henry%20Bell%20and%20Caroline%20Zhang%20and%20Mohammed%20Mobasserul%20Haque%20and%20Dhaval%20Potdar%20and%20Samia%20Zaman%20and%20Brandon%20Fain%0AAbstract%3A%20The%20constitutional%20framework%20of%20alignment%20aims%20to%20align%20large%20language%20models%20%28LLMs%29%20with%20value-laden%20principles%20written%20in%20natural%20language%20%28such%20as%20to%20avoid%20using%20biased%20language%29.%20Prior%20work%20has%20focused%20on%20parameter%20fine-tuning%20techniques%2C%20such%20as%20reinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%2C%20to%20instill%20these%20principles.%20However%2C%20these%20approaches%20are%20computationally%20demanding%2C%20require%20careful%20engineering%20and%20tuning%2C%20and%20often%20require%20difficult-to-obtain%20human%20annotation%20data.%20We%20propose%20%5Ctextsc%7Breflect%7D%2C%20an%20inference-time%20framework%20for%20constitutional%20alignment%20that%20does%20not%20require%20any%20training%20or%20data%2C%20providing%20a%20plug-and-play%20approach%20for%20aligning%20an%20instruction-tuned%20model%20to%20a%20set%20of%20principles.%20%5Ctextsc%7Breflect%7D%20operates%20entirely%20in-context%2C%20combining%20a%20%28i%29%20constitution-conditioned%20base%20response%20with%20post-generation%20%28ii%29%20self-evaluation%2C%20%28iii%29%28a%29%20self-critique%2C%20and%20%28iii%29%28b%29%20final%20revision.%20%5Ctextsc%7Breflect%7D%27s%20technique%20of%20explicit%20in-context%20reasoning%20over%20principles%20during%20post-generation%20outperforms%20standard%20few-shot%20prompting%20and%20provides%20transparent%20reasoning%20traces.%20Our%20results%20demonstrate%20that%20%5Ctextsc%7Breflect%7D%20significantly%20improves%20LLM%20conformance%20to%20diverse%20and%20complex%20principles%2C%20including%20principles%20quite%20distinct%20from%20those%20emphasized%20in%20the%20model%27s%20original%20parameter%20fine-tuning%2C%20without%20sacrificing%20factual%20reasoning.%20%5Ctextsc%7Breflect%7D%20is%20particularly%20effective%20at%20reducing%20the%20rate%20of%20rare%20but%20significant%20violations%20of%20principles%2C%20thereby%20improving%20safety%20and%20robustness%20in%20the%20tail%20end%20of%20the%20distribution%20of%20generations.%20Finally%2C%20we%20show%20that%20%5Ctextsc%7Breflect%7D%20naturally%20generates%20useful%20training%20data%20for%20traditional%20parameter%20fine-tuning%20techniques%2C%20allowing%20for%20efficient%20scaling%20and%20the%20reduction%20of%20inference-time%20computational%20overhead%20in%20long-term%20deployment%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18730v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReflect%253A%2520Transparent%2520Principle-Guided%2520Reasoning%2520for%2520Constitutional%2520Alignment%2520at%2520Scale%26entry.906535625%3DHenry%2520Bell%2520and%2520Caroline%2520Zhang%2520and%2520Mohammed%2520Mobasserul%2520Haque%2520and%2520Dhaval%2520Potdar%2520and%2520Samia%2520Zaman%2520and%2520Brandon%2520Fain%26entry.1292438233%3DThe%2520constitutional%2520framework%2520of%2520alignment%2520aims%2520to%2520align%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520value-laden%2520principles%2520written%2520in%2520natural%2520language%2520%2528such%2520as%2520to%2520avoid%2520using%2520biased%2520language%2529.%2520Prior%2520work%2520has%2520focused%2520on%2520parameter%2520fine-tuning%2520techniques%252C%2520such%2520as%2520reinforcement%2520learning%2520from%2520human%2520feedback%2520%2528RLHF%2529%252C%2520to%2520instill%2520these%2520principles.%2520However%252C%2520these%2520approaches%2520are%2520computationally%2520demanding%252C%2520require%2520careful%2520engineering%2520and%2520tuning%252C%2520and%2520often%2520require%2520difficult-to-obtain%2520human%2520annotation%2520data.%2520We%2520propose%2520%255Ctextsc%257Breflect%257D%252C%2520an%2520inference-time%2520framework%2520for%2520constitutional%2520alignment%2520that%2520does%2520not%2520require%2520any%2520training%2520or%2520data%252C%2520providing%2520a%2520plug-and-play%2520approach%2520for%2520aligning%2520an%2520instruction-tuned%2520model%2520to%2520a%2520set%2520of%2520principles.%2520%255Ctextsc%257Breflect%257D%2520operates%2520entirely%2520in-context%252C%2520combining%2520a%2520%2528i%2529%2520constitution-conditioned%2520base%2520response%2520with%2520post-generation%2520%2528ii%2529%2520self-evaluation%252C%2520%2528iii%2529%2528a%2529%2520self-critique%252C%2520and%2520%2528iii%2529%2528b%2529%2520final%2520revision.%2520%255Ctextsc%257Breflect%257D%2527s%2520technique%2520of%2520explicit%2520in-context%2520reasoning%2520over%2520principles%2520during%2520post-generation%2520outperforms%2520standard%2520few-shot%2520prompting%2520and%2520provides%2520transparent%2520reasoning%2520traces.%2520Our%2520results%2520demonstrate%2520that%2520%255Ctextsc%257Breflect%257D%2520significantly%2520improves%2520LLM%2520conformance%2520to%2520diverse%2520and%2520complex%2520principles%252C%2520including%2520principles%2520quite%2520distinct%2520from%2520those%2520emphasized%2520in%2520the%2520model%2527s%2520original%2520parameter%2520fine-tuning%252C%2520without%2520sacrificing%2520factual%2520reasoning.%2520%255Ctextsc%257Breflect%257D%2520is%2520particularly%2520effective%2520at%2520reducing%2520the%2520rate%2520of%2520rare%2520but%2520significant%2520violations%2520of%2520principles%252C%2520thereby%2520improving%2520safety%2520and%2520robustness%2520in%2520the%2520tail%2520end%2520of%2520the%2520distribution%2520of%2520generations.%2520Finally%252C%2520we%2520show%2520that%2520%255Ctextsc%257Breflect%257D%2520naturally%2520generates%2520useful%2520training%2520data%2520for%2520traditional%2520parameter%2520fine-tuning%2520techniques%252C%2520allowing%2520for%2520efficient%2520scaling%2520and%2520the%2520reduction%2520of%2520inference-time%2520computational%2520overhead%2520in%2520long-term%2520deployment%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18730v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reflect%3A%20Transparent%20Principle-Guided%20Reasoning%20for%20Constitutional%20Alignment%20at%20Scale&entry.906535625=Henry%20Bell%20and%20Caroline%20Zhang%20and%20Mohammed%20Mobasserul%20Haque%20and%20Dhaval%20Potdar%20and%20Samia%20Zaman%20and%20Brandon%20Fain&entry.1292438233=The%20constitutional%20framework%20of%20alignment%20aims%20to%20align%20large%20language%20models%20%28LLMs%29%20with%20value-laden%20principles%20written%20in%20natural%20language%20%28such%20as%20to%20avoid%20using%20biased%20language%29.%20Prior%20work%20has%20focused%20on%20parameter%20fine-tuning%20techniques%2C%20such%20as%20reinforcement%20learning%20from%20human%20feedback%20%28RLHF%29%2C%20to%20instill%20these%20principles.%20However%2C%20these%20approaches%20are%20computationally%20demanding%2C%20require%20careful%20engineering%20and%20tuning%2C%20and%20often%20require%20difficult-to-obtain%20human%20annotation%20data.%20We%20propose%20%5Ctextsc%7Breflect%7D%2C%20an%20inference-time%20framework%20for%20constitutional%20alignment%20that%20does%20not%20require%20any%20training%20or%20data%2C%20providing%20a%20plug-and-play%20approach%20for%20aligning%20an%20instruction-tuned%20model%20to%20a%20set%20of%20principles.%20%5Ctextsc%7Breflect%7D%20operates%20entirely%20in-context%2C%20combining%20a%20%28i%29%20constitution-conditioned%20base%20response%20with%20post-generation%20%28ii%29%20self-evaluation%2C%20%28iii%29%28a%29%20self-critique%2C%20and%20%28iii%29%28b%29%20final%20revision.%20%5Ctextsc%7Breflect%7D%27s%20technique%20of%20explicit%20in-context%20reasoning%20over%20principles%20during%20post-generation%20outperforms%20standard%20few-shot%20prompting%20and%20provides%20transparent%20reasoning%20traces.%20Our%20results%20demonstrate%20that%20%5Ctextsc%7Breflect%7D%20significantly%20improves%20LLM%20conformance%20to%20diverse%20and%20complex%20principles%2C%20including%20principles%20quite%20distinct%20from%20those%20emphasized%20in%20the%20model%27s%20original%20parameter%20fine-tuning%2C%20without%20sacrificing%20factual%20reasoning.%20%5Ctextsc%7Breflect%7D%20is%20particularly%20effective%20at%20reducing%20the%20rate%20of%20rare%20but%20significant%20violations%20of%20principles%2C%20thereby%20improving%20safety%20and%20robustness%20in%20the%20tail%20end%20of%20the%20distribution%20of%20generations.%20Finally%2C%20we%20show%20that%20%5Ctextsc%7Breflect%7D%20naturally%20generates%20useful%20training%20data%20for%20traditional%20parameter%20fine-tuning%20techniques%2C%20allowing%20for%20efficient%20scaling%20and%20the%20reduction%20of%20inference-time%20computational%20overhead%20in%20long-term%20deployment%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2601.18730v1&entry.124074799=Read"},
{"title": "Masked Generative Policy for Robotic Control", "author": "Lipeng Zhuang and Shiyu Fan and Florent P. Audonnet and Yingdong Ru and Edmond S. L. Ho and Gerardo Aragon Camarasa and Paul Henderson", "abstract": "We present Masked Generative Policy (MGP), a novel framework for visuomotor imitation learning. We represent actions as discrete tokens, and train a conditional masked transformer that generates tokens in parallel and then rapidly refines only low-confidence tokens. We further propose two new sampling paradigms: MGP-Short, which performs parallel masked generation with score-based refinement for Markovian tasks, and MGP-Long, which predicts full trajectories in a single pass and dynamically refines low-confidence action tokens based on new observations. With globally coherent prediction and robust adaptive execution capabilities, MGP-Long enables reliable control on complex and non-Markovian tasks that prior methods struggle with. Extensive evaluations on 150 robotic manipulation tasks spanning the Meta-World and LIBERO benchmarks show that MGP achieves both rapid inference and superior success rates compared to state-of-the-art diffusion and autoregressive policies. Specifically, MGP increases the average success rate by 9% across 150 tasks while cutting per-sequence inference time by up to 35x. It further improves the average success rate by 60% in dynamic and missing-observation environments, and solves two non-Markovian scenarios where other state-of-the-art methods fail.", "link": "http://arxiv.org/abs/2512.09101v2", "date": "2026-01-26", "relevancy": 2.376, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6092}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5878}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Masked%20Generative%20Policy%20for%20Robotic%20Control&body=Title%3A%20Masked%20Generative%20Policy%20for%20Robotic%20Control%0AAuthor%3A%20Lipeng%20Zhuang%20and%20Shiyu%20Fan%20and%20Florent%20P.%20Audonnet%20and%20Yingdong%20Ru%20and%20Edmond%20S.%20L.%20Ho%20and%20Gerardo%20Aragon%20Camarasa%20and%20Paul%20Henderson%0AAbstract%3A%20We%20present%20Masked%20Generative%20Policy%20%28MGP%29%2C%20a%20novel%20framework%20for%20visuomotor%20imitation%20learning.%20We%20represent%20actions%20as%20discrete%20tokens%2C%20and%20train%20a%20conditional%20masked%20transformer%20that%20generates%20tokens%20in%20parallel%20and%20then%20rapidly%20refines%20only%20low-confidence%20tokens.%20We%20further%20propose%20two%20new%20sampling%20paradigms%3A%20MGP-Short%2C%20which%20performs%20parallel%20masked%20generation%20with%20score-based%20refinement%20for%20Markovian%20tasks%2C%20and%20MGP-Long%2C%20which%20predicts%20full%20trajectories%20in%20a%20single%20pass%20and%20dynamically%20refines%20low-confidence%20action%20tokens%20based%20on%20new%20observations.%20With%20globally%20coherent%20prediction%20and%20robust%20adaptive%20execution%20capabilities%2C%20MGP-Long%20enables%20reliable%20control%20on%20complex%20and%20non-Markovian%20tasks%20that%20prior%20methods%20struggle%20with.%20Extensive%20evaluations%20on%20150%20robotic%20manipulation%20tasks%20spanning%20the%20Meta-World%20and%20LIBERO%20benchmarks%20show%20that%20MGP%20achieves%20both%20rapid%20inference%20and%20superior%20success%20rates%20compared%20to%20state-of-the-art%20diffusion%20and%20autoregressive%20policies.%20Specifically%2C%20MGP%20increases%20the%20average%20success%20rate%20by%209%25%20across%20150%20tasks%20while%20cutting%20per-sequence%20inference%20time%20by%20up%20to%2035x.%20It%20further%20improves%20the%20average%20success%20rate%20by%2060%25%20in%20dynamic%20and%20missing-observation%20environments%2C%20and%20solves%20two%20non-Markovian%20scenarios%20where%20other%20state-of-the-art%20methods%20fail.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09101v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMasked%2520Generative%2520Policy%2520for%2520Robotic%2520Control%26entry.906535625%3DLipeng%2520Zhuang%2520and%2520Shiyu%2520Fan%2520and%2520Florent%2520P.%2520Audonnet%2520and%2520Yingdong%2520Ru%2520and%2520Edmond%2520S.%2520L.%2520Ho%2520and%2520Gerardo%2520Aragon%2520Camarasa%2520and%2520Paul%2520Henderson%26entry.1292438233%3DWe%2520present%2520Masked%2520Generative%2520Policy%2520%2528MGP%2529%252C%2520a%2520novel%2520framework%2520for%2520visuomotor%2520imitation%2520learning.%2520We%2520represent%2520actions%2520as%2520discrete%2520tokens%252C%2520and%2520train%2520a%2520conditional%2520masked%2520transformer%2520that%2520generates%2520tokens%2520in%2520parallel%2520and%2520then%2520rapidly%2520refines%2520only%2520low-confidence%2520tokens.%2520We%2520further%2520propose%2520two%2520new%2520sampling%2520paradigms%253A%2520MGP-Short%252C%2520which%2520performs%2520parallel%2520masked%2520generation%2520with%2520score-based%2520refinement%2520for%2520Markovian%2520tasks%252C%2520and%2520MGP-Long%252C%2520which%2520predicts%2520full%2520trajectories%2520in%2520a%2520single%2520pass%2520and%2520dynamically%2520refines%2520low-confidence%2520action%2520tokens%2520based%2520on%2520new%2520observations.%2520With%2520globally%2520coherent%2520prediction%2520and%2520robust%2520adaptive%2520execution%2520capabilities%252C%2520MGP-Long%2520enables%2520reliable%2520control%2520on%2520complex%2520and%2520non-Markovian%2520tasks%2520that%2520prior%2520methods%2520struggle%2520with.%2520Extensive%2520evaluations%2520on%2520150%2520robotic%2520manipulation%2520tasks%2520spanning%2520the%2520Meta-World%2520and%2520LIBERO%2520benchmarks%2520show%2520that%2520MGP%2520achieves%2520both%2520rapid%2520inference%2520and%2520superior%2520success%2520rates%2520compared%2520to%2520state-of-the-art%2520diffusion%2520and%2520autoregressive%2520policies.%2520Specifically%252C%2520MGP%2520increases%2520the%2520average%2520success%2520rate%2520by%25209%2525%2520across%2520150%2520tasks%2520while%2520cutting%2520per-sequence%2520inference%2520time%2520by%2520up%2520to%252035x.%2520It%2520further%2520improves%2520the%2520average%2520success%2520rate%2520by%252060%2525%2520in%2520dynamic%2520and%2520missing-observation%2520environments%252C%2520and%2520solves%2520two%2520non-Markovian%2520scenarios%2520where%2520other%2520state-of-the-art%2520methods%2520fail.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09101v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Masked%20Generative%20Policy%20for%20Robotic%20Control&entry.906535625=Lipeng%20Zhuang%20and%20Shiyu%20Fan%20and%20Florent%20P.%20Audonnet%20and%20Yingdong%20Ru%20and%20Edmond%20S.%20L.%20Ho%20and%20Gerardo%20Aragon%20Camarasa%20and%20Paul%20Henderson&entry.1292438233=We%20present%20Masked%20Generative%20Policy%20%28MGP%29%2C%20a%20novel%20framework%20for%20visuomotor%20imitation%20learning.%20We%20represent%20actions%20as%20discrete%20tokens%2C%20and%20train%20a%20conditional%20masked%20transformer%20that%20generates%20tokens%20in%20parallel%20and%20then%20rapidly%20refines%20only%20low-confidence%20tokens.%20We%20further%20propose%20two%20new%20sampling%20paradigms%3A%20MGP-Short%2C%20which%20performs%20parallel%20masked%20generation%20with%20score-based%20refinement%20for%20Markovian%20tasks%2C%20and%20MGP-Long%2C%20which%20predicts%20full%20trajectories%20in%20a%20single%20pass%20and%20dynamically%20refines%20low-confidence%20action%20tokens%20based%20on%20new%20observations.%20With%20globally%20coherent%20prediction%20and%20robust%20adaptive%20execution%20capabilities%2C%20MGP-Long%20enables%20reliable%20control%20on%20complex%20and%20non-Markovian%20tasks%20that%20prior%20methods%20struggle%20with.%20Extensive%20evaluations%20on%20150%20robotic%20manipulation%20tasks%20spanning%20the%20Meta-World%20and%20LIBERO%20benchmarks%20show%20that%20MGP%20achieves%20both%20rapid%20inference%20and%20superior%20success%20rates%20compared%20to%20state-of-the-art%20diffusion%20and%20autoregressive%20policies.%20Specifically%2C%20MGP%20increases%20the%20average%20success%20rate%20by%209%25%20across%20150%20tasks%20while%20cutting%20per-sequence%20inference%20time%20by%20up%20to%2035x.%20It%20further%20improves%20the%20average%20success%20rate%20by%2060%25%20in%20dynamic%20and%20missing-observation%20environments%2C%20and%20solves%20two%20non-Markovian%20scenarios%20where%20other%20state-of-the-art%20methods%20fail.&entry.1838667208=http%3A//arxiv.org/abs/2512.09101v2&entry.124074799=Read"},
{"title": "Unsupervised Text Segmentation via Kernel Change-Point Detection on Sentence Embeddings", "author": "Mumin Jia and Jairo Diaz-Rodriguez", "abstract": "Unsupervised text segmentation is crucial because boundary labels are expensive, subjective, and often fail to transfer across domains and granularity choices. We propose Embed-KCPD, a training-free method that represents sentences as embedding vectors and estimates boundaries by minimizing a penalized KCPD objective. Beyond the algorithmic instantiation, we develop, to our knowledge, the first dependence-aware theory for KCPD under $m$-dependent sequences, a finite-memory abstraction of short-range dependence common in language. We prove an oracle inequality for the population penalized risk and a localization guarantee showing that each true change point is recovered within a window that is small relative to segment length. To connect theory to practice, we introduce an LLM-based simulation framework that generates synthetic documents with controlled finite-memory dependence and known boundaries, validating the predicted scaling behavior. Across standard segmentation benchmarks, Embed-KCPD often outperforms strong unsupervised baselines. A case study on Taylor Swift's tweets illustrates that Embed-KCPD combines strong theoretical guarantees, simulated reliability, and practical effectiveness for text segmentation.", "link": "http://arxiv.org/abs/2601.18788v1", "date": "2026-01-26", "relevancy": 2.3721, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4798}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4749}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Text%20Segmentation%20via%20Kernel%20Change-Point%20Detection%20on%20Sentence%20Embeddings&body=Title%3A%20Unsupervised%20Text%20Segmentation%20via%20Kernel%20Change-Point%20Detection%20on%20Sentence%20Embeddings%0AAuthor%3A%20Mumin%20Jia%20and%20Jairo%20Diaz-Rodriguez%0AAbstract%3A%20Unsupervised%20text%20segmentation%20is%20crucial%20because%20boundary%20labels%20are%20expensive%2C%20subjective%2C%20and%20often%20fail%20to%20transfer%20across%20domains%20and%20granularity%20choices.%20We%20propose%20Embed-KCPD%2C%20a%20training-free%20method%20that%20represents%20sentences%20as%20embedding%20vectors%20and%20estimates%20boundaries%20by%20minimizing%20a%20penalized%20KCPD%20objective.%20Beyond%20the%20algorithmic%20instantiation%2C%20we%20develop%2C%20to%20our%20knowledge%2C%20the%20first%20dependence-aware%20theory%20for%20KCPD%20under%20%24m%24-dependent%20sequences%2C%20a%20finite-memory%20abstraction%20of%20short-range%20dependence%20common%20in%20language.%20We%20prove%20an%20oracle%20inequality%20for%20the%20population%20penalized%20risk%20and%20a%20localization%20guarantee%20showing%20that%20each%20true%20change%20point%20is%20recovered%20within%20a%20window%20that%20is%20small%20relative%20to%20segment%20length.%20To%20connect%20theory%20to%20practice%2C%20we%20introduce%20an%20LLM-based%20simulation%20framework%20that%20generates%20synthetic%20documents%20with%20controlled%20finite-memory%20dependence%20and%20known%20boundaries%2C%20validating%20the%20predicted%20scaling%20behavior.%20Across%20standard%20segmentation%20benchmarks%2C%20Embed-KCPD%20often%20outperforms%20strong%20unsupervised%20baselines.%20A%20case%20study%20on%20Taylor%20Swift%27s%20tweets%20illustrates%20that%20Embed-KCPD%20combines%20strong%20theoretical%20guarantees%2C%20simulated%20reliability%2C%20and%20practical%20effectiveness%20for%20text%20segmentation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18788v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Text%2520Segmentation%2520via%2520Kernel%2520Change-Point%2520Detection%2520on%2520Sentence%2520Embeddings%26entry.906535625%3DMumin%2520Jia%2520and%2520Jairo%2520Diaz-Rodriguez%26entry.1292438233%3DUnsupervised%2520text%2520segmentation%2520is%2520crucial%2520because%2520boundary%2520labels%2520are%2520expensive%252C%2520subjective%252C%2520and%2520often%2520fail%2520to%2520transfer%2520across%2520domains%2520and%2520granularity%2520choices.%2520We%2520propose%2520Embed-KCPD%252C%2520a%2520training-free%2520method%2520that%2520represents%2520sentences%2520as%2520embedding%2520vectors%2520and%2520estimates%2520boundaries%2520by%2520minimizing%2520a%2520penalized%2520KCPD%2520objective.%2520Beyond%2520the%2520algorithmic%2520instantiation%252C%2520we%2520develop%252C%2520to%2520our%2520knowledge%252C%2520the%2520first%2520dependence-aware%2520theory%2520for%2520KCPD%2520under%2520%2524m%2524-dependent%2520sequences%252C%2520a%2520finite-memory%2520abstraction%2520of%2520short-range%2520dependence%2520common%2520in%2520language.%2520We%2520prove%2520an%2520oracle%2520inequality%2520for%2520the%2520population%2520penalized%2520risk%2520and%2520a%2520localization%2520guarantee%2520showing%2520that%2520each%2520true%2520change%2520point%2520is%2520recovered%2520within%2520a%2520window%2520that%2520is%2520small%2520relative%2520to%2520segment%2520length.%2520To%2520connect%2520theory%2520to%2520practice%252C%2520we%2520introduce%2520an%2520LLM-based%2520simulation%2520framework%2520that%2520generates%2520synthetic%2520documents%2520with%2520controlled%2520finite-memory%2520dependence%2520and%2520known%2520boundaries%252C%2520validating%2520the%2520predicted%2520scaling%2520behavior.%2520Across%2520standard%2520segmentation%2520benchmarks%252C%2520Embed-KCPD%2520often%2520outperforms%2520strong%2520unsupervised%2520baselines.%2520A%2520case%2520study%2520on%2520Taylor%2520Swift%2527s%2520tweets%2520illustrates%2520that%2520Embed-KCPD%2520combines%2520strong%2520theoretical%2520guarantees%252C%2520simulated%2520reliability%252C%2520and%2520practical%2520effectiveness%2520for%2520text%2520segmentation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18788v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Text%20Segmentation%20via%20Kernel%20Change-Point%20Detection%20on%20Sentence%20Embeddings&entry.906535625=Mumin%20Jia%20and%20Jairo%20Diaz-Rodriguez&entry.1292438233=Unsupervised%20text%20segmentation%20is%20crucial%20because%20boundary%20labels%20are%20expensive%2C%20subjective%2C%20and%20often%20fail%20to%20transfer%20across%20domains%20and%20granularity%20choices.%20We%20propose%20Embed-KCPD%2C%20a%20training-free%20method%20that%20represents%20sentences%20as%20embedding%20vectors%20and%20estimates%20boundaries%20by%20minimizing%20a%20penalized%20KCPD%20objective.%20Beyond%20the%20algorithmic%20instantiation%2C%20we%20develop%2C%20to%20our%20knowledge%2C%20the%20first%20dependence-aware%20theory%20for%20KCPD%20under%20%24m%24-dependent%20sequences%2C%20a%20finite-memory%20abstraction%20of%20short-range%20dependence%20common%20in%20language.%20We%20prove%20an%20oracle%20inequality%20for%20the%20population%20penalized%20risk%20and%20a%20localization%20guarantee%20showing%20that%20each%20true%20change%20point%20is%20recovered%20within%20a%20window%20that%20is%20small%20relative%20to%20segment%20length.%20To%20connect%20theory%20to%20practice%2C%20we%20introduce%20an%20LLM-based%20simulation%20framework%20that%20generates%20synthetic%20documents%20with%20controlled%20finite-memory%20dependence%20and%20known%20boundaries%2C%20validating%20the%20predicted%20scaling%20behavior.%20Across%20standard%20segmentation%20benchmarks%2C%20Embed-KCPD%20often%20outperforms%20strong%20unsupervised%20baselines.%20A%20case%20study%20on%20Taylor%20Swift%27s%20tweets%20illustrates%20that%20Embed-KCPD%20combines%20strong%20theoretical%20guarantees%2C%20simulated%20reliability%2C%20and%20practical%20effectiveness%20for%20text%20segmentation.&entry.1838667208=http%3A//arxiv.org/abs/2601.18788v1&entry.124074799=Read"},
{"title": "Data-Driven Qubit Characterization and Optimal Control using Deep Learning", "author": "Paul Surrey and Julian D. Teske and Tobias Hangleiter and Hendrik Bluhm and Pascal Cerfontaine", "abstract": "Quantum computing requires the optimization of control pulses to achieve high-fidelity quantum gates. We propose a machine learning-based protocol to address the challenges of evaluating gradients and modeling complex system dynamics. By training a recurrent neural network (RNN) to predict qubit behavior, our approach enables efficient gradient-based pulse optimization without the need for a detailed system model. First, we sample qubit dynamics using random control pulses with weak prior assumptions. We then train the RNN on the system's observed responses, and use the trained model to optimize high-fidelity control pulses. We demonstrate the effectiveness of this approach through simulations on a single $ST_0$ qubit.", "link": "http://arxiv.org/abs/2601.18704v1", "date": "2026-01-26", "relevancy": 2.3551, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4738}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4717}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4676}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Driven%20Qubit%20Characterization%20and%20Optimal%20Control%20using%20Deep%20Learning&body=Title%3A%20Data-Driven%20Qubit%20Characterization%20and%20Optimal%20Control%20using%20Deep%20Learning%0AAuthor%3A%20Paul%20Surrey%20and%20Julian%20D.%20Teske%20and%20Tobias%20Hangleiter%20and%20Hendrik%20Bluhm%20and%20Pascal%20Cerfontaine%0AAbstract%3A%20Quantum%20computing%20requires%20the%20optimization%20of%20control%20pulses%20to%20achieve%20high-fidelity%20quantum%20gates.%20We%20propose%20a%20machine%20learning-based%20protocol%20to%20address%20the%20challenges%20of%20evaluating%20gradients%20and%20modeling%20complex%20system%20dynamics.%20By%20training%20a%20recurrent%20neural%20network%20%28RNN%29%20to%20predict%20qubit%20behavior%2C%20our%20approach%20enables%20efficient%20gradient-based%20pulse%20optimization%20without%20the%20need%20for%20a%20detailed%20system%20model.%20First%2C%20we%20sample%20qubit%20dynamics%20using%20random%20control%20pulses%20with%20weak%20prior%20assumptions.%20We%20then%20train%20the%20RNN%20on%20the%20system%27s%20observed%20responses%2C%20and%20use%20the%20trained%20model%20to%20optimize%20high-fidelity%20control%20pulses.%20We%20demonstrate%20the%20effectiveness%20of%20this%20approach%20through%20simulations%20on%20a%20single%20%24ST_0%24%20qubit.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18704v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Driven%2520Qubit%2520Characterization%2520and%2520Optimal%2520Control%2520using%2520Deep%2520Learning%26entry.906535625%3DPaul%2520Surrey%2520and%2520Julian%2520D.%2520Teske%2520and%2520Tobias%2520Hangleiter%2520and%2520Hendrik%2520Bluhm%2520and%2520Pascal%2520Cerfontaine%26entry.1292438233%3DQuantum%2520computing%2520requires%2520the%2520optimization%2520of%2520control%2520pulses%2520to%2520achieve%2520high-fidelity%2520quantum%2520gates.%2520We%2520propose%2520a%2520machine%2520learning-based%2520protocol%2520to%2520address%2520the%2520challenges%2520of%2520evaluating%2520gradients%2520and%2520modeling%2520complex%2520system%2520dynamics.%2520By%2520training%2520a%2520recurrent%2520neural%2520network%2520%2528RNN%2529%2520to%2520predict%2520qubit%2520behavior%252C%2520our%2520approach%2520enables%2520efficient%2520gradient-based%2520pulse%2520optimization%2520without%2520the%2520need%2520for%2520a%2520detailed%2520system%2520model.%2520First%252C%2520we%2520sample%2520qubit%2520dynamics%2520using%2520random%2520control%2520pulses%2520with%2520weak%2520prior%2520assumptions.%2520We%2520then%2520train%2520the%2520RNN%2520on%2520the%2520system%2527s%2520observed%2520responses%252C%2520and%2520use%2520the%2520trained%2520model%2520to%2520optimize%2520high-fidelity%2520control%2520pulses.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520this%2520approach%2520through%2520simulations%2520on%2520a%2520single%2520%2524ST_0%2524%2520qubit.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18704v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Driven%20Qubit%20Characterization%20and%20Optimal%20Control%20using%20Deep%20Learning&entry.906535625=Paul%20Surrey%20and%20Julian%20D.%20Teske%20and%20Tobias%20Hangleiter%20and%20Hendrik%20Bluhm%20and%20Pascal%20Cerfontaine&entry.1292438233=Quantum%20computing%20requires%20the%20optimization%20of%20control%20pulses%20to%20achieve%20high-fidelity%20quantum%20gates.%20We%20propose%20a%20machine%20learning-based%20protocol%20to%20address%20the%20challenges%20of%20evaluating%20gradients%20and%20modeling%20complex%20system%20dynamics.%20By%20training%20a%20recurrent%20neural%20network%20%28RNN%29%20to%20predict%20qubit%20behavior%2C%20our%20approach%20enables%20efficient%20gradient-based%20pulse%20optimization%20without%20the%20need%20for%20a%20detailed%20system%20model.%20First%2C%20we%20sample%20qubit%20dynamics%20using%20random%20control%20pulses%20with%20weak%20prior%20assumptions.%20We%20then%20train%20the%20RNN%20on%20the%20system%27s%20observed%20responses%2C%20and%20use%20the%20trained%20model%20to%20optimize%20high-fidelity%20control%20pulses.%20We%20demonstrate%20the%20effectiveness%20of%20this%20approach%20through%20simulations%20on%20a%20single%20%24ST_0%24%20qubit.&entry.1838667208=http%3A//arxiv.org/abs/2601.18704v1&entry.124074799=Read"},
{"title": "Code over Words: Overcoming Semantic Inertia via Code-Grounded Reasoning", "author": "Manjie Xu and Isabella Yin and Xinyi Tu and Chi Zhang and Yixin Zhu", "abstract": "LLMs struggle with Semantic Inertia: the inability to inhibit pre-trained priors (e.g., \"Lava is Dangerous\") when dynamic, in-context rules contradict them. We probe this phenomenon using Baba Is You, where physical laws are mutable text rules, enabling precise evaluation of models' ability to override learned priors when rules change. We quantatively observe that larger models can exhibit inverse scaling: they perform worse than smaller models when natural language reasoning requires suppressing pre-trained associations (e.g., accepting \"Lava is Safe\"). Our analysis attributes this to natural language encoding, which entangles descriptive semantics and logical rules, leading to persistent hallucinations of familiar physics despite explicit contradictory rules. Here we show that representing dynamics as executable code, rather than descriptive text, reverses this trend and enables effective prior inhibition. We introduce Code-Grounded Vistas (LCV), which fine-tunes models on counterfactual pairs and identifies states with contradictory rules, thereby forcing attention to logical constraints rather than visual semantics. This training-time approach outperforms expensive inference-time search methods in both efficiency and accuracy. Our results demonstrate that representation fundamentally determines whether scaling improves or impairs contextual reasoning. This challenges the assumption that larger models are universally better, with implications for domains that require dynamic overriding of learned priors.", "link": "http://arxiv.org/abs/2601.18352v1", "date": "2026-01-26", "relevancy": 2.3446, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5834}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Code%20over%20Words%3A%20Overcoming%20Semantic%20Inertia%20via%20Code-Grounded%20Reasoning&body=Title%3A%20Code%20over%20Words%3A%20Overcoming%20Semantic%20Inertia%20via%20Code-Grounded%20Reasoning%0AAuthor%3A%20Manjie%20Xu%20and%20Isabella%20Yin%20and%20Xinyi%20Tu%20and%20Chi%20Zhang%20and%20Yixin%20Zhu%0AAbstract%3A%20LLMs%20struggle%20with%20Semantic%20Inertia%3A%20the%20inability%20to%20inhibit%20pre-trained%20priors%20%28e.g.%2C%20%22Lava%20is%20Dangerous%22%29%20when%20dynamic%2C%20in-context%20rules%20contradict%20them.%20We%20probe%20this%20phenomenon%20using%20Baba%20Is%20You%2C%20where%20physical%20laws%20are%20mutable%20text%20rules%2C%20enabling%20precise%20evaluation%20of%20models%27%20ability%20to%20override%20learned%20priors%20when%20rules%20change.%20We%20quantatively%20observe%20that%20larger%20models%20can%20exhibit%20inverse%20scaling%3A%20they%20perform%20worse%20than%20smaller%20models%20when%20natural%20language%20reasoning%20requires%20suppressing%20pre-trained%20associations%20%28e.g.%2C%20accepting%20%22Lava%20is%20Safe%22%29.%20Our%20analysis%20attributes%20this%20to%20natural%20language%20encoding%2C%20which%20entangles%20descriptive%20semantics%20and%20logical%20rules%2C%20leading%20to%20persistent%20hallucinations%20of%20familiar%20physics%20despite%20explicit%20contradictory%20rules.%20Here%20we%20show%20that%20representing%20dynamics%20as%20executable%20code%2C%20rather%20than%20descriptive%20text%2C%20reverses%20this%20trend%20and%20enables%20effective%20prior%20inhibition.%20We%20introduce%20Code-Grounded%20Vistas%20%28LCV%29%2C%20which%20fine-tunes%20models%20on%20counterfactual%20pairs%20and%20identifies%20states%20with%20contradictory%20rules%2C%20thereby%20forcing%20attention%20to%20logical%20constraints%20rather%20than%20visual%20semantics.%20This%20training-time%20approach%20outperforms%20expensive%20inference-time%20search%20methods%20in%20both%20efficiency%20and%20accuracy.%20Our%20results%20demonstrate%20that%20representation%20fundamentally%20determines%20whether%20scaling%20improves%20or%20impairs%20contextual%20reasoning.%20This%20challenges%20the%20assumption%20that%20larger%20models%20are%20universally%20better%2C%20with%20implications%20for%20domains%20that%20require%20dynamic%20overriding%20of%20learned%20priors.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18352v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCode%2520over%2520Words%253A%2520Overcoming%2520Semantic%2520Inertia%2520via%2520Code-Grounded%2520Reasoning%26entry.906535625%3DManjie%2520Xu%2520and%2520Isabella%2520Yin%2520and%2520Xinyi%2520Tu%2520and%2520Chi%2520Zhang%2520and%2520Yixin%2520Zhu%26entry.1292438233%3DLLMs%2520struggle%2520with%2520Semantic%2520Inertia%253A%2520the%2520inability%2520to%2520inhibit%2520pre-trained%2520priors%2520%2528e.g.%252C%2520%2522Lava%2520is%2520Dangerous%2522%2529%2520when%2520dynamic%252C%2520in-context%2520rules%2520contradict%2520them.%2520We%2520probe%2520this%2520phenomenon%2520using%2520Baba%2520Is%2520You%252C%2520where%2520physical%2520laws%2520are%2520mutable%2520text%2520rules%252C%2520enabling%2520precise%2520evaluation%2520of%2520models%2527%2520ability%2520to%2520override%2520learned%2520priors%2520when%2520rules%2520change.%2520We%2520quantatively%2520observe%2520that%2520larger%2520models%2520can%2520exhibit%2520inverse%2520scaling%253A%2520they%2520perform%2520worse%2520than%2520smaller%2520models%2520when%2520natural%2520language%2520reasoning%2520requires%2520suppressing%2520pre-trained%2520associations%2520%2528e.g.%252C%2520accepting%2520%2522Lava%2520is%2520Safe%2522%2529.%2520Our%2520analysis%2520attributes%2520this%2520to%2520natural%2520language%2520encoding%252C%2520which%2520entangles%2520descriptive%2520semantics%2520and%2520logical%2520rules%252C%2520leading%2520to%2520persistent%2520hallucinations%2520of%2520familiar%2520physics%2520despite%2520explicit%2520contradictory%2520rules.%2520Here%2520we%2520show%2520that%2520representing%2520dynamics%2520as%2520executable%2520code%252C%2520rather%2520than%2520descriptive%2520text%252C%2520reverses%2520this%2520trend%2520and%2520enables%2520effective%2520prior%2520inhibition.%2520We%2520introduce%2520Code-Grounded%2520Vistas%2520%2528LCV%2529%252C%2520which%2520fine-tunes%2520models%2520on%2520counterfactual%2520pairs%2520and%2520identifies%2520states%2520with%2520contradictory%2520rules%252C%2520thereby%2520forcing%2520attention%2520to%2520logical%2520constraints%2520rather%2520than%2520visual%2520semantics.%2520This%2520training-time%2520approach%2520outperforms%2520expensive%2520inference-time%2520search%2520methods%2520in%2520both%2520efficiency%2520and%2520accuracy.%2520Our%2520results%2520demonstrate%2520that%2520representation%2520fundamentally%2520determines%2520whether%2520scaling%2520improves%2520or%2520impairs%2520contextual%2520reasoning.%2520This%2520challenges%2520the%2520assumption%2520that%2520larger%2520models%2520are%2520universally%2520better%252C%2520with%2520implications%2520for%2520domains%2520that%2520require%2520dynamic%2520overriding%2520of%2520learned%2520priors.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18352v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Code%20over%20Words%3A%20Overcoming%20Semantic%20Inertia%20via%20Code-Grounded%20Reasoning&entry.906535625=Manjie%20Xu%20and%20Isabella%20Yin%20and%20Xinyi%20Tu%20and%20Chi%20Zhang%20and%20Yixin%20Zhu&entry.1292438233=LLMs%20struggle%20with%20Semantic%20Inertia%3A%20the%20inability%20to%20inhibit%20pre-trained%20priors%20%28e.g.%2C%20%22Lava%20is%20Dangerous%22%29%20when%20dynamic%2C%20in-context%20rules%20contradict%20them.%20We%20probe%20this%20phenomenon%20using%20Baba%20Is%20You%2C%20where%20physical%20laws%20are%20mutable%20text%20rules%2C%20enabling%20precise%20evaluation%20of%20models%27%20ability%20to%20override%20learned%20priors%20when%20rules%20change.%20We%20quantatively%20observe%20that%20larger%20models%20can%20exhibit%20inverse%20scaling%3A%20they%20perform%20worse%20than%20smaller%20models%20when%20natural%20language%20reasoning%20requires%20suppressing%20pre-trained%20associations%20%28e.g.%2C%20accepting%20%22Lava%20is%20Safe%22%29.%20Our%20analysis%20attributes%20this%20to%20natural%20language%20encoding%2C%20which%20entangles%20descriptive%20semantics%20and%20logical%20rules%2C%20leading%20to%20persistent%20hallucinations%20of%20familiar%20physics%20despite%20explicit%20contradictory%20rules.%20Here%20we%20show%20that%20representing%20dynamics%20as%20executable%20code%2C%20rather%20than%20descriptive%20text%2C%20reverses%20this%20trend%20and%20enables%20effective%20prior%20inhibition.%20We%20introduce%20Code-Grounded%20Vistas%20%28LCV%29%2C%20which%20fine-tunes%20models%20on%20counterfactual%20pairs%20and%20identifies%20states%20with%20contradictory%20rules%2C%20thereby%20forcing%20attention%20to%20logical%20constraints%20rather%20than%20visual%20semantics.%20This%20training-time%20approach%20outperforms%20expensive%20inference-time%20search%20methods%20in%20both%20efficiency%20and%20accuracy.%20Our%20results%20demonstrate%20that%20representation%20fundamentally%20determines%20whether%20scaling%20improves%20or%20impairs%20contextual%20reasoning.%20This%20challenges%20the%20assumption%20that%20larger%20models%20are%20universally%20better%2C%20with%20implications%20for%20domains%20that%20require%20dynamic%20overriding%20of%20learned%20priors.&entry.1838667208=http%3A//arxiv.org/abs/2601.18352v1&entry.124074799=Read"},
{"title": "Foundation Models in Autonomous Driving: A Survey on Scenario Generation and Scenario Analysis", "author": "Yuan Gao and Mattia Piccinini and Yuchen Zhang and Dingrui Wang and Korbinian Moller and Roberto Brusnicki and Baha Zarrouki and Alessio Gambi and Jan Frederik Totz and Kai Storms and Steven Peters and Andrea Stocco and Bassam Alrifaee and Marco Pavone and Johannes Betz", "abstract": "For autonomous vehicles, safe navigation in complex environments depends on handling a broad range of diverse and rare driving scenarios. Simulation- and scenario-based testing have emerged as key approaches to development and validation of autonomous driving systems. Traditional scenario generation relies on rule-based systems, knowledge-driven models, and data-driven synthesis, often producing limited diversity and unrealistic safety-critical cases. With the emergence of foundation models, which represent a new generation of pre-trained, general-purpose AI models, developers can process heterogeneous inputs (e.g., natural language, sensor data, HD maps, and control actions), enabling the synthesis and interpretation of complex driving scenarios. In this paper, we conduct a survey about the application of foundation models for scenario generation and scenario analysis in autonomous driving (as of May 2025). Our survey presents a unified taxonomy that includes large language models, vision-language models, multimodal large language models, diffusion models, and world models for the generation and analysis of autonomous driving scenarios. In addition, we review the methodologies, open-source datasets, simulation platforms, and benchmark challenges, and we examine the evaluation metrics tailored explicitly to scenario generation and analysis. Finally, the survey concludes by highlighting the open challenges and research questions, and outlining promising future research directions. All reviewed papers are listed in a continuously maintained repository, which contains supplementary materials and is available at https://github.com/TUM-AVS/FM-for-Scenario-Generation-Analysis.", "link": "http://arxiv.org/abs/2506.11526v3", "date": "2026-01-26", "relevancy": 2.3394, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5931}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5931}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundation%20Models%20in%20Autonomous%20Driving%3A%20A%20Survey%20on%20Scenario%20Generation%20and%20Scenario%20Analysis&body=Title%3A%20Foundation%20Models%20in%20Autonomous%20Driving%3A%20A%20Survey%20on%20Scenario%20Generation%20and%20Scenario%20Analysis%0AAuthor%3A%20Yuan%20Gao%20and%20Mattia%20Piccinini%20and%20Yuchen%20Zhang%20and%20Dingrui%20Wang%20and%20Korbinian%20Moller%20and%20Roberto%20Brusnicki%20and%20Baha%20Zarrouki%20and%20Alessio%20Gambi%20and%20Jan%20Frederik%20Totz%20and%20Kai%20Storms%20and%20Steven%20Peters%20and%20Andrea%20Stocco%20and%20Bassam%20Alrifaee%20and%20Marco%20Pavone%20and%20Johannes%20Betz%0AAbstract%3A%20For%20autonomous%20vehicles%2C%20safe%20navigation%20in%20complex%20environments%20depends%20on%20handling%20a%20broad%20range%20of%20diverse%20and%20rare%20driving%20scenarios.%20Simulation-%20and%20scenario-based%20testing%20have%20emerged%20as%20key%20approaches%20to%20development%20and%20validation%20of%20autonomous%20driving%20systems.%20Traditional%20scenario%20generation%20relies%20on%20rule-based%20systems%2C%20knowledge-driven%20models%2C%20and%20data-driven%20synthesis%2C%20often%20producing%20limited%20diversity%20and%20unrealistic%20safety-critical%20cases.%20With%20the%20emergence%20of%20foundation%20models%2C%20which%20represent%20a%20new%20generation%20of%20pre-trained%2C%20general-purpose%20AI%20models%2C%20developers%20can%20process%20heterogeneous%20inputs%20%28e.g.%2C%20natural%20language%2C%20sensor%20data%2C%20HD%20maps%2C%20and%20control%20actions%29%2C%20enabling%20the%20synthesis%20and%20interpretation%20of%20complex%20driving%20scenarios.%20In%20this%20paper%2C%20we%20conduct%20a%20survey%20about%20the%20application%20of%20foundation%20models%20for%20scenario%20generation%20and%20scenario%20analysis%20in%20autonomous%20driving%20%28as%20of%20May%202025%29.%20Our%20survey%20presents%20a%20unified%20taxonomy%20that%20includes%20large%20language%20models%2C%20vision-language%20models%2C%20multimodal%20large%20language%20models%2C%20diffusion%20models%2C%20and%20world%20models%20for%20the%20generation%20and%20analysis%20of%20autonomous%20driving%20scenarios.%20In%20addition%2C%20we%20review%20the%20methodologies%2C%20open-source%20datasets%2C%20simulation%20platforms%2C%20and%20benchmark%20challenges%2C%20and%20we%20examine%20the%20evaluation%20metrics%20tailored%20explicitly%20to%20scenario%20generation%20and%20analysis.%20Finally%2C%20the%20survey%20concludes%20by%20highlighting%20the%20open%20challenges%20and%20research%20questions%2C%20and%20outlining%20promising%20future%20research%20directions.%20All%20reviewed%20papers%20are%20listed%20in%20a%20continuously%20maintained%20repository%2C%20which%20contains%20supplementary%20materials%20and%20is%20available%20at%20https%3A//github.com/TUM-AVS/FM-for-Scenario-Generation-Analysis.%0ALink%3A%20http%3A//arxiv.org/abs/2506.11526v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundation%2520Models%2520in%2520Autonomous%2520Driving%253A%2520A%2520Survey%2520on%2520Scenario%2520Generation%2520and%2520Scenario%2520Analysis%26entry.906535625%3DYuan%2520Gao%2520and%2520Mattia%2520Piccinini%2520and%2520Yuchen%2520Zhang%2520and%2520Dingrui%2520Wang%2520and%2520Korbinian%2520Moller%2520and%2520Roberto%2520Brusnicki%2520and%2520Baha%2520Zarrouki%2520and%2520Alessio%2520Gambi%2520and%2520Jan%2520Frederik%2520Totz%2520and%2520Kai%2520Storms%2520and%2520Steven%2520Peters%2520and%2520Andrea%2520Stocco%2520and%2520Bassam%2520Alrifaee%2520and%2520Marco%2520Pavone%2520and%2520Johannes%2520Betz%26entry.1292438233%3DFor%2520autonomous%2520vehicles%252C%2520safe%2520navigation%2520in%2520complex%2520environments%2520depends%2520on%2520handling%2520a%2520broad%2520range%2520of%2520diverse%2520and%2520rare%2520driving%2520scenarios.%2520Simulation-%2520and%2520scenario-based%2520testing%2520have%2520emerged%2520as%2520key%2520approaches%2520to%2520development%2520and%2520validation%2520of%2520autonomous%2520driving%2520systems.%2520Traditional%2520scenario%2520generation%2520relies%2520on%2520rule-based%2520systems%252C%2520knowledge-driven%2520models%252C%2520and%2520data-driven%2520synthesis%252C%2520often%2520producing%2520limited%2520diversity%2520and%2520unrealistic%2520safety-critical%2520cases.%2520With%2520the%2520emergence%2520of%2520foundation%2520models%252C%2520which%2520represent%2520a%2520new%2520generation%2520of%2520pre-trained%252C%2520general-purpose%2520AI%2520models%252C%2520developers%2520can%2520process%2520heterogeneous%2520inputs%2520%2528e.g.%252C%2520natural%2520language%252C%2520sensor%2520data%252C%2520HD%2520maps%252C%2520and%2520control%2520actions%2529%252C%2520enabling%2520the%2520synthesis%2520and%2520interpretation%2520of%2520complex%2520driving%2520scenarios.%2520In%2520this%2520paper%252C%2520we%2520conduct%2520a%2520survey%2520about%2520the%2520application%2520of%2520foundation%2520models%2520for%2520scenario%2520generation%2520and%2520scenario%2520analysis%2520in%2520autonomous%2520driving%2520%2528as%2520of%2520May%25202025%2529.%2520Our%2520survey%2520presents%2520a%2520unified%2520taxonomy%2520that%2520includes%2520large%2520language%2520models%252C%2520vision-language%2520models%252C%2520multimodal%2520large%2520language%2520models%252C%2520diffusion%2520models%252C%2520and%2520world%2520models%2520for%2520the%2520generation%2520and%2520analysis%2520of%2520autonomous%2520driving%2520scenarios.%2520In%2520addition%252C%2520we%2520review%2520the%2520methodologies%252C%2520open-source%2520datasets%252C%2520simulation%2520platforms%252C%2520and%2520benchmark%2520challenges%252C%2520and%2520we%2520examine%2520the%2520evaluation%2520metrics%2520tailored%2520explicitly%2520to%2520scenario%2520generation%2520and%2520analysis.%2520Finally%252C%2520the%2520survey%2520concludes%2520by%2520highlighting%2520the%2520open%2520challenges%2520and%2520research%2520questions%252C%2520and%2520outlining%2520promising%2520future%2520research%2520directions.%2520All%2520reviewed%2520papers%2520are%2520listed%2520in%2520a%2520continuously%2520maintained%2520repository%252C%2520which%2520contains%2520supplementary%2520materials%2520and%2520is%2520available%2520at%2520https%253A//github.com/TUM-AVS/FM-for-Scenario-Generation-Analysis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11526v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundation%20Models%20in%20Autonomous%20Driving%3A%20A%20Survey%20on%20Scenario%20Generation%20and%20Scenario%20Analysis&entry.906535625=Yuan%20Gao%20and%20Mattia%20Piccinini%20and%20Yuchen%20Zhang%20and%20Dingrui%20Wang%20and%20Korbinian%20Moller%20and%20Roberto%20Brusnicki%20and%20Baha%20Zarrouki%20and%20Alessio%20Gambi%20and%20Jan%20Frederik%20Totz%20and%20Kai%20Storms%20and%20Steven%20Peters%20and%20Andrea%20Stocco%20and%20Bassam%20Alrifaee%20and%20Marco%20Pavone%20and%20Johannes%20Betz&entry.1292438233=For%20autonomous%20vehicles%2C%20safe%20navigation%20in%20complex%20environments%20depends%20on%20handling%20a%20broad%20range%20of%20diverse%20and%20rare%20driving%20scenarios.%20Simulation-%20and%20scenario-based%20testing%20have%20emerged%20as%20key%20approaches%20to%20development%20and%20validation%20of%20autonomous%20driving%20systems.%20Traditional%20scenario%20generation%20relies%20on%20rule-based%20systems%2C%20knowledge-driven%20models%2C%20and%20data-driven%20synthesis%2C%20often%20producing%20limited%20diversity%20and%20unrealistic%20safety-critical%20cases.%20With%20the%20emergence%20of%20foundation%20models%2C%20which%20represent%20a%20new%20generation%20of%20pre-trained%2C%20general-purpose%20AI%20models%2C%20developers%20can%20process%20heterogeneous%20inputs%20%28e.g.%2C%20natural%20language%2C%20sensor%20data%2C%20HD%20maps%2C%20and%20control%20actions%29%2C%20enabling%20the%20synthesis%20and%20interpretation%20of%20complex%20driving%20scenarios.%20In%20this%20paper%2C%20we%20conduct%20a%20survey%20about%20the%20application%20of%20foundation%20models%20for%20scenario%20generation%20and%20scenario%20analysis%20in%20autonomous%20driving%20%28as%20of%20May%202025%29.%20Our%20survey%20presents%20a%20unified%20taxonomy%20that%20includes%20large%20language%20models%2C%20vision-language%20models%2C%20multimodal%20large%20language%20models%2C%20diffusion%20models%2C%20and%20world%20models%20for%20the%20generation%20and%20analysis%20of%20autonomous%20driving%20scenarios.%20In%20addition%2C%20we%20review%20the%20methodologies%2C%20open-source%20datasets%2C%20simulation%20platforms%2C%20and%20benchmark%20challenges%2C%20and%20we%20examine%20the%20evaluation%20metrics%20tailored%20explicitly%20to%20scenario%20generation%20and%20analysis.%20Finally%2C%20the%20survey%20concludes%20by%20highlighting%20the%20open%20challenges%20and%20research%20questions%2C%20and%20outlining%20promising%20future%20research%20directions.%20All%20reviewed%20papers%20are%20listed%20in%20a%20continuously%20maintained%20repository%2C%20which%20contains%20supplementary%20materials%20and%20is%20available%20at%20https%3A//github.com/TUM-AVS/FM-for-Scenario-Generation-Analysis.&entry.1838667208=http%3A//arxiv.org/abs/2506.11526v3&entry.124074799=Read"},
{"title": "Cognitive Fusion of ZC Sequences and Time-Frequency Images for Out-of-Distribution Detection of Drone Signals", "author": "Jie Li and Jing Li and Lu Lv and Zhanyu Ju and Fengkui Gong", "abstract": "We propose a drone signal out-of-distribution detection (OODD) algorithm based on the cognitive fusion of Zadoff-Chu (ZC) sequences and time-frequency images (TFI). ZC sequences are identified by analyzing the communication protocols of DJI drones, while TFI capture the time-frequency characteristics of drone signals with unknown or non-standard communication protocols. Both modalities are used jointly to enable OODD in the drone remote identification (RID) task. Specifically, ZC sequence features and TFI features are generated from the received radio frequency signals, which are then processed through dedicated feature extraction module to enhance and align them. The resultant multi-modal features undergo multi-modal feature interaction, single-modal feature fusion, and multi-modal feature fusion to produce features that integrate and complement information across modalities. Discrimination scores are computed from the fused features along both spatial and channel dimensions to capture time-frequency characteristic differences dictated by the communication protocols, and these scores will be transformed into adaptive attention weights. The weighted features are then passed through a Softmax function to produce the signal classification results. Simulation results demonstrate that the proposed algorithm outperforms existing algorithms and achieves 1.7% and 7.5% improvements in RID and OODD metrics, respectively. The proposed algorithm also performs strong robustness under varying flight conditions and across different drone types.", "link": "http://arxiv.org/abs/2601.18326v1", "date": "2026-01-26", "relevancy": 2.3123, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4656}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4613}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cognitive%20Fusion%20of%20ZC%20Sequences%20and%20Time-Frequency%20Images%20for%20Out-of-Distribution%20Detection%20of%20Drone%20Signals&body=Title%3A%20Cognitive%20Fusion%20of%20ZC%20Sequences%20and%20Time-Frequency%20Images%20for%20Out-of-Distribution%20Detection%20of%20Drone%20Signals%0AAuthor%3A%20Jie%20Li%20and%20Jing%20Li%20and%20Lu%20Lv%20and%20Zhanyu%20Ju%20and%20Fengkui%20Gong%0AAbstract%3A%20We%20propose%20a%20drone%20signal%20out-of-distribution%20detection%20%28OODD%29%20algorithm%20based%20on%20the%20cognitive%20fusion%20of%20Zadoff-Chu%20%28ZC%29%20sequences%20and%20time-frequency%20images%20%28TFI%29.%20ZC%20sequences%20are%20identified%20by%20analyzing%20the%20communication%20protocols%20of%20DJI%20drones%2C%20while%20TFI%20capture%20the%20time-frequency%20characteristics%20of%20drone%20signals%20with%20unknown%20or%20non-standard%20communication%20protocols.%20Both%20modalities%20are%20used%20jointly%20to%20enable%20OODD%20in%20the%20drone%20remote%20identification%20%28RID%29%20task.%20Specifically%2C%20ZC%20sequence%20features%20and%20TFI%20features%20are%20generated%20from%20the%20received%20radio%20frequency%20signals%2C%20which%20are%20then%20processed%20through%20dedicated%20feature%20extraction%20module%20to%20enhance%20and%20align%20them.%20The%20resultant%20multi-modal%20features%20undergo%20multi-modal%20feature%20interaction%2C%20single-modal%20feature%20fusion%2C%20and%20multi-modal%20feature%20fusion%20to%20produce%20features%20that%20integrate%20and%20complement%20information%20across%20modalities.%20Discrimination%20scores%20are%20computed%20from%20the%20fused%20features%20along%20both%20spatial%20and%20channel%20dimensions%20to%20capture%20time-frequency%20characteristic%20differences%20dictated%20by%20the%20communication%20protocols%2C%20and%20these%20scores%20will%20be%20transformed%20into%20adaptive%20attention%20weights.%20The%20weighted%20features%20are%20then%20passed%20through%20a%20Softmax%20function%20to%20produce%20the%20signal%20classification%20results.%20Simulation%20results%20demonstrate%20that%20the%20proposed%20algorithm%20outperforms%20existing%20algorithms%20and%20achieves%201.7%25%20and%207.5%25%20improvements%20in%20RID%20and%20OODD%20metrics%2C%20respectively.%20The%20proposed%20algorithm%20also%20performs%20strong%20robustness%20under%20varying%20flight%20conditions%20and%20across%20different%20drone%20types.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18326v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCognitive%2520Fusion%2520of%2520ZC%2520Sequences%2520and%2520Time-Frequency%2520Images%2520for%2520Out-of-Distribution%2520Detection%2520of%2520Drone%2520Signals%26entry.906535625%3DJie%2520Li%2520and%2520Jing%2520Li%2520and%2520Lu%2520Lv%2520and%2520Zhanyu%2520Ju%2520and%2520Fengkui%2520Gong%26entry.1292438233%3DWe%2520propose%2520a%2520drone%2520signal%2520out-of-distribution%2520detection%2520%2528OODD%2529%2520algorithm%2520based%2520on%2520the%2520cognitive%2520fusion%2520of%2520Zadoff-Chu%2520%2528ZC%2529%2520sequences%2520and%2520time-frequency%2520images%2520%2528TFI%2529.%2520ZC%2520sequences%2520are%2520identified%2520by%2520analyzing%2520the%2520communication%2520protocols%2520of%2520DJI%2520drones%252C%2520while%2520TFI%2520capture%2520the%2520time-frequency%2520characteristics%2520of%2520drone%2520signals%2520with%2520unknown%2520or%2520non-standard%2520communication%2520protocols.%2520Both%2520modalities%2520are%2520used%2520jointly%2520to%2520enable%2520OODD%2520in%2520the%2520drone%2520remote%2520identification%2520%2528RID%2529%2520task.%2520Specifically%252C%2520ZC%2520sequence%2520features%2520and%2520TFI%2520features%2520are%2520generated%2520from%2520the%2520received%2520radio%2520frequency%2520signals%252C%2520which%2520are%2520then%2520processed%2520through%2520dedicated%2520feature%2520extraction%2520module%2520to%2520enhance%2520and%2520align%2520them.%2520The%2520resultant%2520multi-modal%2520features%2520undergo%2520multi-modal%2520feature%2520interaction%252C%2520single-modal%2520feature%2520fusion%252C%2520and%2520multi-modal%2520feature%2520fusion%2520to%2520produce%2520features%2520that%2520integrate%2520and%2520complement%2520information%2520across%2520modalities.%2520Discrimination%2520scores%2520are%2520computed%2520from%2520the%2520fused%2520features%2520along%2520both%2520spatial%2520and%2520channel%2520dimensions%2520to%2520capture%2520time-frequency%2520characteristic%2520differences%2520dictated%2520by%2520the%2520communication%2520protocols%252C%2520and%2520these%2520scores%2520will%2520be%2520transformed%2520into%2520adaptive%2520attention%2520weights.%2520The%2520weighted%2520features%2520are%2520then%2520passed%2520through%2520a%2520Softmax%2520function%2520to%2520produce%2520the%2520signal%2520classification%2520results.%2520Simulation%2520results%2520demonstrate%2520that%2520the%2520proposed%2520algorithm%2520outperforms%2520existing%2520algorithms%2520and%2520achieves%25201.7%2525%2520and%25207.5%2525%2520improvements%2520in%2520RID%2520and%2520OODD%2520metrics%252C%2520respectively.%2520The%2520proposed%2520algorithm%2520also%2520performs%2520strong%2520robustness%2520under%2520varying%2520flight%2520conditions%2520and%2520across%2520different%2520drone%2520types.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18326v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cognitive%20Fusion%20of%20ZC%20Sequences%20and%20Time-Frequency%20Images%20for%20Out-of-Distribution%20Detection%20of%20Drone%20Signals&entry.906535625=Jie%20Li%20and%20Jing%20Li%20and%20Lu%20Lv%20and%20Zhanyu%20Ju%20and%20Fengkui%20Gong&entry.1292438233=We%20propose%20a%20drone%20signal%20out-of-distribution%20detection%20%28OODD%29%20algorithm%20based%20on%20the%20cognitive%20fusion%20of%20Zadoff-Chu%20%28ZC%29%20sequences%20and%20time-frequency%20images%20%28TFI%29.%20ZC%20sequences%20are%20identified%20by%20analyzing%20the%20communication%20protocols%20of%20DJI%20drones%2C%20while%20TFI%20capture%20the%20time-frequency%20characteristics%20of%20drone%20signals%20with%20unknown%20or%20non-standard%20communication%20protocols.%20Both%20modalities%20are%20used%20jointly%20to%20enable%20OODD%20in%20the%20drone%20remote%20identification%20%28RID%29%20task.%20Specifically%2C%20ZC%20sequence%20features%20and%20TFI%20features%20are%20generated%20from%20the%20received%20radio%20frequency%20signals%2C%20which%20are%20then%20processed%20through%20dedicated%20feature%20extraction%20module%20to%20enhance%20and%20align%20them.%20The%20resultant%20multi-modal%20features%20undergo%20multi-modal%20feature%20interaction%2C%20single-modal%20feature%20fusion%2C%20and%20multi-modal%20feature%20fusion%20to%20produce%20features%20that%20integrate%20and%20complement%20information%20across%20modalities.%20Discrimination%20scores%20are%20computed%20from%20the%20fused%20features%20along%20both%20spatial%20and%20channel%20dimensions%20to%20capture%20time-frequency%20characteristic%20differences%20dictated%20by%20the%20communication%20protocols%2C%20and%20these%20scores%20will%20be%20transformed%20into%20adaptive%20attention%20weights.%20The%20weighted%20features%20are%20then%20passed%20through%20a%20Softmax%20function%20to%20produce%20the%20signal%20classification%20results.%20Simulation%20results%20demonstrate%20that%20the%20proposed%20algorithm%20outperforms%20existing%20algorithms%20and%20achieves%201.7%25%20and%207.5%25%20improvements%20in%20RID%20and%20OODD%20metrics%2C%20respectively.%20The%20proposed%20algorithm%20also%20performs%20strong%20robustness%20under%20varying%20flight%20conditions%20and%20across%20different%20drone%20types.&entry.1838667208=http%3A//arxiv.org/abs/2601.18326v1&entry.124074799=Read"},
{"title": "LaCoGSEA: Unsupervised deep learning for pathway analysis via latent correlation", "author": "Zhiwei Zheng and Kevin Bryson", "abstract": "Motivation: Pathway enrichment analysis is widely used to interpret gene expression data. Standard approaches, such as GSEA, rely on predefined phenotypic labels and pairwise comparisons, which limits their applicability in unsupervised settings. Existing unsupervised extensions, including single-sample methods, provide pathway-level summaries but primarily capture linear relationships and do not explicitly model gene-pathway associations. More recently, deep learning models have been explored to capture non-linear transcriptomic structure. However, their interpretation has typically relied on generic explainable AI (XAI) techniques designed for feature-level attribution. As these methods are not designed for pathway-level interpretation in unsupervised transcriptomic analyses, their effectiveness in this setting remains limited.\n  Results: To bridge this gap, we introduce LaCoGSEA (Latent Correlation GSEA), an unsupervised framework that integrates deep representation learning with robust pathway statistics. LaCoGSEA employs an autoencoder to capture non-linear manifolds and proposes a global gene-latent correlation metric as a proxy for differential expression, generating dense gene rankings without prior labels. We demonstrate that LaCoGSEA offers three key advantages: (i) it achieves improved clustering performance in distinguishing cancer subtypes compared to existing unsupervised baselines; (ii) it recovers a broader range of biologically meaningful pathways at higher ranks compared with linear dimensionality reduction and gradient-based XAI methods; and (iii) it maintains high robustness and consistency across varying experimental protocols and dataset sizes. Overall, LaCoGSEA provides state-of-the-art performance in unsupervised pathway enrichment analysis.\n  Availability and implementation: https://github.com/willyzzz/LaCoGSEA", "link": "http://arxiv.org/abs/2601.18604v1", "date": "2026-01-26", "relevancy": 2.3064, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4654}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4627}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LaCoGSEA%3A%20Unsupervised%20deep%20learning%20for%20pathway%20analysis%20via%20latent%20correlation&body=Title%3A%20LaCoGSEA%3A%20Unsupervised%20deep%20learning%20for%20pathway%20analysis%20via%20latent%20correlation%0AAuthor%3A%20Zhiwei%20Zheng%20and%20Kevin%20Bryson%0AAbstract%3A%20Motivation%3A%20Pathway%20enrichment%20analysis%20is%20widely%20used%20to%20interpret%20gene%20expression%20data.%20Standard%20approaches%2C%20such%20as%20GSEA%2C%20rely%20on%20predefined%20phenotypic%20labels%20and%20pairwise%20comparisons%2C%20which%20limits%20their%20applicability%20in%20unsupervised%20settings.%20Existing%20unsupervised%20extensions%2C%20including%20single-sample%20methods%2C%20provide%20pathway-level%20summaries%20but%20primarily%20capture%20linear%20relationships%20and%20do%20not%20explicitly%20model%20gene-pathway%20associations.%20More%20recently%2C%20deep%20learning%20models%20have%20been%20explored%20to%20capture%20non-linear%20transcriptomic%20structure.%20However%2C%20their%20interpretation%20has%20typically%20relied%20on%20generic%20explainable%20AI%20%28XAI%29%20techniques%20designed%20for%20feature-level%20attribution.%20As%20these%20methods%20are%20not%20designed%20for%20pathway-level%20interpretation%20in%20unsupervised%20transcriptomic%20analyses%2C%20their%20effectiveness%20in%20this%20setting%20remains%20limited.%0A%20%20Results%3A%20To%20bridge%20this%20gap%2C%20we%20introduce%20LaCoGSEA%20%28Latent%20Correlation%20GSEA%29%2C%20an%20unsupervised%20framework%20that%20integrates%20deep%20representation%20learning%20with%20robust%20pathway%20statistics.%20LaCoGSEA%20employs%20an%20autoencoder%20to%20capture%20non-linear%20manifolds%20and%20proposes%20a%20global%20gene-latent%20correlation%20metric%20as%20a%20proxy%20for%20differential%20expression%2C%20generating%20dense%20gene%20rankings%20without%20prior%20labels.%20We%20demonstrate%20that%20LaCoGSEA%20offers%20three%20key%20advantages%3A%20%28i%29%20it%20achieves%20improved%20clustering%20performance%20in%20distinguishing%20cancer%20subtypes%20compared%20to%20existing%20unsupervised%20baselines%3B%20%28ii%29%20it%20recovers%20a%20broader%20range%20of%20biologically%20meaningful%20pathways%20at%20higher%20ranks%20compared%20with%20linear%20dimensionality%20reduction%20and%20gradient-based%20XAI%20methods%3B%20and%20%28iii%29%20it%20maintains%20high%20robustness%20and%20consistency%20across%20varying%20experimental%20protocols%20and%20dataset%20sizes.%20Overall%2C%20LaCoGSEA%20provides%20state-of-the-art%20performance%20in%20unsupervised%20pathway%20enrichment%20analysis.%0A%20%20Availability%20and%20implementation%3A%20https%3A//github.com/willyzzz/LaCoGSEA%0ALink%3A%20http%3A//arxiv.org/abs/2601.18604v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaCoGSEA%253A%2520Unsupervised%2520deep%2520learning%2520for%2520pathway%2520analysis%2520via%2520latent%2520correlation%26entry.906535625%3DZhiwei%2520Zheng%2520and%2520Kevin%2520Bryson%26entry.1292438233%3DMotivation%253A%2520Pathway%2520enrichment%2520analysis%2520is%2520widely%2520used%2520to%2520interpret%2520gene%2520expression%2520data.%2520Standard%2520approaches%252C%2520such%2520as%2520GSEA%252C%2520rely%2520on%2520predefined%2520phenotypic%2520labels%2520and%2520pairwise%2520comparisons%252C%2520which%2520limits%2520their%2520applicability%2520in%2520unsupervised%2520settings.%2520Existing%2520unsupervised%2520extensions%252C%2520including%2520single-sample%2520methods%252C%2520provide%2520pathway-level%2520summaries%2520but%2520primarily%2520capture%2520linear%2520relationships%2520and%2520do%2520not%2520explicitly%2520model%2520gene-pathway%2520associations.%2520More%2520recently%252C%2520deep%2520learning%2520models%2520have%2520been%2520explored%2520to%2520capture%2520non-linear%2520transcriptomic%2520structure.%2520However%252C%2520their%2520interpretation%2520has%2520typically%2520relied%2520on%2520generic%2520explainable%2520AI%2520%2528XAI%2529%2520techniques%2520designed%2520for%2520feature-level%2520attribution.%2520As%2520these%2520methods%2520are%2520not%2520designed%2520for%2520pathway-level%2520interpretation%2520in%2520unsupervised%2520transcriptomic%2520analyses%252C%2520their%2520effectiveness%2520in%2520this%2520setting%2520remains%2520limited.%250A%2520%2520Results%253A%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520LaCoGSEA%2520%2528Latent%2520Correlation%2520GSEA%2529%252C%2520an%2520unsupervised%2520framework%2520that%2520integrates%2520deep%2520representation%2520learning%2520with%2520robust%2520pathway%2520statistics.%2520LaCoGSEA%2520employs%2520an%2520autoencoder%2520to%2520capture%2520non-linear%2520manifolds%2520and%2520proposes%2520a%2520global%2520gene-latent%2520correlation%2520metric%2520as%2520a%2520proxy%2520for%2520differential%2520expression%252C%2520generating%2520dense%2520gene%2520rankings%2520without%2520prior%2520labels.%2520We%2520demonstrate%2520that%2520LaCoGSEA%2520offers%2520three%2520key%2520advantages%253A%2520%2528i%2529%2520it%2520achieves%2520improved%2520clustering%2520performance%2520in%2520distinguishing%2520cancer%2520subtypes%2520compared%2520to%2520existing%2520unsupervised%2520baselines%253B%2520%2528ii%2529%2520it%2520recovers%2520a%2520broader%2520range%2520of%2520biologically%2520meaningful%2520pathways%2520at%2520higher%2520ranks%2520compared%2520with%2520linear%2520dimensionality%2520reduction%2520and%2520gradient-based%2520XAI%2520methods%253B%2520and%2520%2528iii%2529%2520it%2520maintains%2520high%2520robustness%2520and%2520consistency%2520across%2520varying%2520experimental%2520protocols%2520and%2520dataset%2520sizes.%2520Overall%252C%2520LaCoGSEA%2520provides%2520state-of-the-art%2520performance%2520in%2520unsupervised%2520pathway%2520enrichment%2520analysis.%250A%2520%2520Availability%2520and%2520implementation%253A%2520https%253A//github.com/willyzzz/LaCoGSEA%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18604v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaCoGSEA%3A%20Unsupervised%20deep%20learning%20for%20pathway%20analysis%20via%20latent%20correlation&entry.906535625=Zhiwei%20Zheng%20and%20Kevin%20Bryson&entry.1292438233=Motivation%3A%20Pathway%20enrichment%20analysis%20is%20widely%20used%20to%20interpret%20gene%20expression%20data.%20Standard%20approaches%2C%20such%20as%20GSEA%2C%20rely%20on%20predefined%20phenotypic%20labels%20and%20pairwise%20comparisons%2C%20which%20limits%20their%20applicability%20in%20unsupervised%20settings.%20Existing%20unsupervised%20extensions%2C%20including%20single-sample%20methods%2C%20provide%20pathway-level%20summaries%20but%20primarily%20capture%20linear%20relationships%20and%20do%20not%20explicitly%20model%20gene-pathway%20associations.%20More%20recently%2C%20deep%20learning%20models%20have%20been%20explored%20to%20capture%20non-linear%20transcriptomic%20structure.%20However%2C%20their%20interpretation%20has%20typically%20relied%20on%20generic%20explainable%20AI%20%28XAI%29%20techniques%20designed%20for%20feature-level%20attribution.%20As%20these%20methods%20are%20not%20designed%20for%20pathway-level%20interpretation%20in%20unsupervised%20transcriptomic%20analyses%2C%20their%20effectiveness%20in%20this%20setting%20remains%20limited.%0A%20%20Results%3A%20To%20bridge%20this%20gap%2C%20we%20introduce%20LaCoGSEA%20%28Latent%20Correlation%20GSEA%29%2C%20an%20unsupervised%20framework%20that%20integrates%20deep%20representation%20learning%20with%20robust%20pathway%20statistics.%20LaCoGSEA%20employs%20an%20autoencoder%20to%20capture%20non-linear%20manifolds%20and%20proposes%20a%20global%20gene-latent%20correlation%20metric%20as%20a%20proxy%20for%20differential%20expression%2C%20generating%20dense%20gene%20rankings%20without%20prior%20labels.%20We%20demonstrate%20that%20LaCoGSEA%20offers%20three%20key%20advantages%3A%20%28i%29%20it%20achieves%20improved%20clustering%20performance%20in%20distinguishing%20cancer%20subtypes%20compared%20to%20existing%20unsupervised%20baselines%3B%20%28ii%29%20it%20recovers%20a%20broader%20range%20of%20biologically%20meaningful%20pathways%20at%20higher%20ranks%20compared%20with%20linear%20dimensionality%20reduction%20and%20gradient-based%20XAI%20methods%3B%20and%20%28iii%29%20it%20maintains%20high%20robustness%20and%20consistency%20across%20varying%20experimental%20protocols%20and%20dataset%20sizes.%20Overall%2C%20LaCoGSEA%20provides%20state-of-the-art%20performance%20in%20unsupervised%20pathway%20enrichment%20analysis.%0A%20%20Availability%20and%20implementation%3A%20https%3A//github.com/willyzzz/LaCoGSEA&entry.1838667208=http%3A//arxiv.org/abs/2601.18604v1&entry.124074799=Read"},
{"title": "Beyond Rigid: Benchmarking Non-Rigid Video Editing", "author": "Bingzheng Qu and Kehai Chen and Xuefeng Bai and Jun Yu and Min Zhang", "abstract": "Despite the remarkable progress in text-driven video editing, generating coherent non-rigid deformations remains a critical challenge, often plagued by physical distortion and temporal flicker. To bridge this gap, we propose NRVBench, the first dedicated and comprehensive benchmark designed to evaluate non-rigid video editing. First, we curate a high-quality dataset consisting of 180 non-rigid motion videos from six physics-based categories, equipped with 2,340 fine-grained task instructions and 360 multiple-choice questions. Second, we propose NRVE-Acc, a novel evaluation metric based on Vision-Language Models that can rigorously assess physical compliance, temporal consistency, and instruction alignment, overcoming the limitations of general metrics in capturing complex dynamics. Third, we introduce a training-free baseline, VM-Edit, which utilizes a dual-region denoising mechanism to achieve structure-aware control, balancing structural preservation and dynamic deformation. Extensive experiments demonstrate that while current methods have shortcomings in maintaining physical plausibility, our method achieves excellent performance across both standard and proposed metrics. We believe the benchmark could serve as a standard testing platform for advancing physics-aware video editing.", "link": "http://arxiv.org/abs/2601.18340v1", "date": "2026-01-26", "relevancy": 2.2967, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5987}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5579}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Rigid%3A%20Benchmarking%20Non-Rigid%20Video%20Editing&body=Title%3A%20Beyond%20Rigid%3A%20Benchmarking%20Non-Rigid%20Video%20Editing%0AAuthor%3A%20Bingzheng%20Qu%20and%20Kehai%20Chen%20and%20Xuefeng%20Bai%20and%20Jun%20Yu%20and%20Min%20Zhang%0AAbstract%3A%20Despite%20the%20remarkable%20progress%20in%20text-driven%20video%20editing%2C%20generating%20coherent%20non-rigid%20deformations%20remains%20a%20critical%20challenge%2C%20often%20plagued%20by%20physical%20distortion%20and%20temporal%20flicker.%20To%20bridge%20this%20gap%2C%20we%20propose%20NRVBench%2C%20the%20first%20dedicated%20and%20comprehensive%20benchmark%20designed%20to%20evaluate%20non-rigid%20video%20editing.%20First%2C%20we%20curate%20a%20high-quality%20dataset%20consisting%20of%20180%20non-rigid%20motion%20videos%20from%20six%20physics-based%20categories%2C%20equipped%20with%202%2C340%20fine-grained%20task%20instructions%20and%20360%20multiple-choice%20questions.%20Second%2C%20we%20propose%20NRVE-Acc%2C%20a%20novel%20evaluation%20metric%20based%20on%20Vision-Language%20Models%20that%20can%20rigorously%20assess%20physical%20compliance%2C%20temporal%20consistency%2C%20and%20instruction%20alignment%2C%20overcoming%20the%20limitations%20of%20general%20metrics%20in%20capturing%20complex%20dynamics.%20Third%2C%20we%20introduce%20a%20training-free%20baseline%2C%20VM-Edit%2C%20which%20utilizes%20a%20dual-region%20denoising%20mechanism%20to%20achieve%20structure-aware%20control%2C%20balancing%20structural%20preservation%20and%20dynamic%20deformation.%20Extensive%20experiments%20demonstrate%20that%20while%20current%20methods%20have%20shortcomings%20in%20maintaining%20physical%20plausibility%2C%20our%20method%20achieves%20excellent%20performance%20across%20both%20standard%20and%20proposed%20metrics.%20We%20believe%20the%20benchmark%20could%20serve%20as%20a%20standard%20testing%20platform%20for%20advancing%20physics-aware%20video%20editing.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18340v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Rigid%253A%2520Benchmarking%2520Non-Rigid%2520Video%2520Editing%26entry.906535625%3DBingzheng%2520Qu%2520and%2520Kehai%2520Chen%2520and%2520Xuefeng%2520Bai%2520and%2520Jun%2520Yu%2520and%2520Min%2520Zhang%26entry.1292438233%3DDespite%2520the%2520remarkable%2520progress%2520in%2520text-driven%2520video%2520editing%252C%2520generating%2520coherent%2520non-rigid%2520deformations%2520remains%2520a%2520critical%2520challenge%252C%2520often%2520plagued%2520by%2520physical%2520distortion%2520and%2520temporal%2520flicker.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520NRVBench%252C%2520the%2520first%2520dedicated%2520and%2520comprehensive%2520benchmark%2520designed%2520to%2520evaluate%2520non-rigid%2520video%2520editing.%2520First%252C%2520we%2520curate%2520a%2520high-quality%2520dataset%2520consisting%2520of%2520180%2520non-rigid%2520motion%2520videos%2520from%2520six%2520physics-based%2520categories%252C%2520equipped%2520with%25202%252C340%2520fine-grained%2520task%2520instructions%2520and%2520360%2520multiple-choice%2520questions.%2520Second%252C%2520we%2520propose%2520NRVE-Acc%252C%2520a%2520novel%2520evaluation%2520metric%2520based%2520on%2520Vision-Language%2520Models%2520that%2520can%2520rigorously%2520assess%2520physical%2520compliance%252C%2520temporal%2520consistency%252C%2520and%2520instruction%2520alignment%252C%2520overcoming%2520the%2520limitations%2520of%2520general%2520metrics%2520in%2520capturing%2520complex%2520dynamics.%2520Third%252C%2520we%2520introduce%2520a%2520training-free%2520baseline%252C%2520VM-Edit%252C%2520which%2520utilizes%2520a%2520dual-region%2520denoising%2520mechanism%2520to%2520achieve%2520structure-aware%2520control%252C%2520balancing%2520structural%2520preservation%2520and%2520dynamic%2520deformation.%2520Extensive%2520experiments%2520demonstrate%2520that%2520while%2520current%2520methods%2520have%2520shortcomings%2520in%2520maintaining%2520physical%2520plausibility%252C%2520our%2520method%2520achieves%2520excellent%2520performance%2520across%2520both%2520standard%2520and%2520proposed%2520metrics.%2520We%2520believe%2520the%2520benchmark%2520could%2520serve%2520as%2520a%2520standard%2520testing%2520platform%2520for%2520advancing%2520physics-aware%2520video%2520editing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18340v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Rigid%3A%20Benchmarking%20Non-Rigid%20Video%20Editing&entry.906535625=Bingzheng%20Qu%20and%20Kehai%20Chen%20and%20Xuefeng%20Bai%20and%20Jun%20Yu%20and%20Min%20Zhang&entry.1292438233=Despite%20the%20remarkable%20progress%20in%20text-driven%20video%20editing%2C%20generating%20coherent%20non-rigid%20deformations%20remains%20a%20critical%20challenge%2C%20often%20plagued%20by%20physical%20distortion%20and%20temporal%20flicker.%20To%20bridge%20this%20gap%2C%20we%20propose%20NRVBench%2C%20the%20first%20dedicated%20and%20comprehensive%20benchmark%20designed%20to%20evaluate%20non-rigid%20video%20editing.%20First%2C%20we%20curate%20a%20high-quality%20dataset%20consisting%20of%20180%20non-rigid%20motion%20videos%20from%20six%20physics-based%20categories%2C%20equipped%20with%202%2C340%20fine-grained%20task%20instructions%20and%20360%20multiple-choice%20questions.%20Second%2C%20we%20propose%20NRVE-Acc%2C%20a%20novel%20evaluation%20metric%20based%20on%20Vision-Language%20Models%20that%20can%20rigorously%20assess%20physical%20compliance%2C%20temporal%20consistency%2C%20and%20instruction%20alignment%2C%20overcoming%20the%20limitations%20of%20general%20metrics%20in%20capturing%20complex%20dynamics.%20Third%2C%20we%20introduce%20a%20training-free%20baseline%2C%20VM-Edit%2C%20which%20utilizes%20a%20dual-region%20denoising%20mechanism%20to%20achieve%20structure-aware%20control%2C%20balancing%20structural%20preservation%20and%20dynamic%20deformation.%20Extensive%20experiments%20demonstrate%20that%20while%20current%20methods%20have%20shortcomings%20in%20maintaining%20physical%20plausibility%2C%20our%20method%20achieves%20excellent%20performance%20across%20both%20standard%20and%20proposed%20metrics.%20We%20believe%20the%20benchmark%20could%20serve%20as%20a%20standard%20testing%20platform%20for%20advancing%20physics-aware%20video%20editing.&entry.1838667208=http%3A//arxiv.org/abs/2601.18340v1&entry.124074799=Read"},
{"title": "Can Good Writing Be Generative? Expert-Level AI Writing Emerges through Fine-Tuning on High-Quality Books", "author": "Tuhin Chakrabarty and Paramveer S. Dhillon", "abstract": "Creative writing has long been considered a uniquely human endeavor, requiring voice and style that machines could not replicate. This assumption is challenged by Generative AI that can emulate thousands of author styles in seconds with negligible marginal labor. To understand this better, we conducted a behavioral experiment where 28 MFA writers (experts) competed against three LLMs in emulating 50 critically acclaimed authors. Based on blind pairwise comparisons by 28 expert judges and 131 lay judges, we find that experts preferred human writing in 82.7% of cases under the in-context prompting condition but this reversed to 62% preference for AI after fine-tuning on authors' complete works. Lay judges, however, consistently preferred AI writing. Debrief interviews with expert writers revealed that their preference for AI writing triggered an identity crisis, eroding aesthetic confidence and questioning what constitutes \"good writing.\" These findings challenge discourse about AI's creative limitations and raise fundamental questions about the future of creative labor.", "link": "http://arxiv.org/abs/2601.18353v1", "date": "2026-01-26", "relevancy": 2.2947, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4617}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4584}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Good%20Writing%20Be%20Generative%3F%20Expert-Level%20AI%20Writing%20Emerges%20through%20Fine-Tuning%20on%20High-Quality%20Books&body=Title%3A%20Can%20Good%20Writing%20Be%20Generative%3F%20Expert-Level%20AI%20Writing%20Emerges%20through%20Fine-Tuning%20on%20High-Quality%20Books%0AAuthor%3A%20Tuhin%20Chakrabarty%20and%20Paramveer%20S.%20Dhillon%0AAbstract%3A%20Creative%20writing%20has%20long%20been%20considered%20a%20uniquely%20human%20endeavor%2C%20requiring%20voice%20and%20style%20that%20machines%20could%20not%20replicate.%20This%20assumption%20is%20challenged%20by%20Generative%20AI%20that%20can%20emulate%20thousands%20of%20author%20styles%20in%20seconds%20with%20negligible%20marginal%20labor.%20To%20understand%20this%20better%2C%20we%20conducted%20a%20behavioral%20experiment%20where%2028%20MFA%20writers%20%28experts%29%20competed%20against%20three%20LLMs%20in%20emulating%2050%20critically%20acclaimed%20authors.%20Based%20on%20blind%20pairwise%20comparisons%20by%2028%20expert%20judges%20and%20131%20lay%20judges%2C%20we%20find%20that%20experts%20preferred%20human%20writing%20in%2082.7%25%20of%20cases%20under%20the%20in-context%20prompting%20condition%20but%20this%20reversed%20to%2062%25%20preference%20for%20AI%20after%20fine-tuning%20on%20authors%27%20complete%20works.%20Lay%20judges%2C%20however%2C%20consistently%20preferred%20AI%20writing.%20Debrief%20interviews%20with%20expert%20writers%20revealed%20that%20their%20preference%20for%20AI%20writing%20triggered%20an%20identity%20crisis%2C%20eroding%20aesthetic%20confidence%20and%20questioning%20what%20constitutes%20%22good%20writing.%22%20These%20findings%20challenge%20discourse%20about%20AI%27s%20creative%20limitations%20and%20raise%20fundamental%20questions%20about%20the%20future%20of%20creative%20labor.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18353v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Good%2520Writing%2520Be%2520Generative%253F%2520Expert-Level%2520AI%2520Writing%2520Emerges%2520through%2520Fine-Tuning%2520on%2520High-Quality%2520Books%26entry.906535625%3DTuhin%2520Chakrabarty%2520and%2520Paramveer%2520S.%2520Dhillon%26entry.1292438233%3DCreative%2520writing%2520has%2520long%2520been%2520considered%2520a%2520uniquely%2520human%2520endeavor%252C%2520requiring%2520voice%2520and%2520style%2520that%2520machines%2520could%2520not%2520replicate.%2520This%2520assumption%2520is%2520challenged%2520by%2520Generative%2520AI%2520that%2520can%2520emulate%2520thousands%2520of%2520author%2520styles%2520in%2520seconds%2520with%2520negligible%2520marginal%2520labor.%2520To%2520understand%2520this%2520better%252C%2520we%2520conducted%2520a%2520behavioral%2520experiment%2520where%252028%2520MFA%2520writers%2520%2528experts%2529%2520competed%2520against%2520three%2520LLMs%2520in%2520emulating%252050%2520critically%2520acclaimed%2520authors.%2520Based%2520on%2520blind%2520pairwise%2520comparisons%2520by%252028%2520expert%2520judges%2520and%2520131%2520lay%2520judges%252C%2520we%2520find%2520that%2520experts%2520preferred%2520human%2520writing%2520in%252082.7%2525%2520of%2520cases%2520under%2520the%2520in-context%2520prompting%2520condition%2520but%2520this%2520reversed%2520to%252062%2525%2520preference%2520for%2520AI%2520after%2520fine-tuning%2520on%2520authors%2527%2520complete%2520works.%2520Lay%2520judges%252C%2520however%252C%2520consistently%2520preferred%2520AI%2520writing.%2520Debrief%2520interviews%2520with%2520expert%2520writers%2520revealed%2520that%2520their%2520preference%2520for%2520AI%2520writing%2520triggered%2520an%2520identity%2520crisis%252C%2520eroding%2520aesthetic%2520confidence%2520and%2520questioning%2520what%2520constitutes%2520%2522good%2520writing.%2522%2520These%2520findings%2520challenge%2520discourse%2520about%2520AI%2527s%2520creative%2520limitations%2520and%2520raise%2520fundamental%2520questions%2520about%2520the%2520future%2520of%2520creative%2520labor.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18353v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Good%20Writing%20Be%20Generative%3F%20Expert-Level%20AI%20Writing%20Emerges%20through%20Fine-Tuning%20on%20High-Quality%20Books&entry.906535625=Tuhin%20Chakrabarty%20and%20Paramveer%20S.%20Dhillon&entry.1292438233=Creative%20writing%20has%20long%20been%20considered%20a%20uniquely%20human%20endeavor%2C%20requiring%20voice%20and%20style%20that%20machines%20could%20not%20replicate.%20This%20assumption%20is%20challenged%20by%20Generative%20AI%20that%20can%20emulate%20thousands%20of%20author%20styles%20in%20seconds%20with%20negligible%20marginal%20labor.%20To%20understand%20this%20better%2C%20we%20conducted%20a%20behavioral%20experiment%20where%2028%20MFA%20writers%20%28experts%29%20competed%20against%20three%20LLMs%20in%20emulating%2050%20critically%20acclaimed%20authors.%20Based%20on%20blind%20pairwise%20comparisons%20by%2028%20expert%20judges%20and%20131%20lay%20judges%2C%20we%20find%20that%20experts%20preferred%20human%20writing%20in%2082.7%25%20of%20cases%20under%20the%20in-context%20prompting%20condition%20but%20this%20reversed%20to%2062%25%20preference%20for%20AI%20after%20fine-tuning%20on%20authors%27%20complete%20works.%20Lay%20judges%2C%20however%2C%20consistently%20preferred%20AI%20writing.%20Debrief%20interviews%20with%20expert%20writers%20revealed%20that%20their%20preference%20for%20AI%20writing%20triggered%20an%20identity%20crisis%2C%20eroding%20aesthetic%20confidence%20and%20questioning%20what%20constitutes%20%22good%20writing.%22%20These%20findings%20challenge%20discourse%20about%20AI%27s%20creative%20limitations%20and%20raise%20fundamental%20questions%20about%20the%20future%20of%20creative%20labor.&entry.1838667208=http%3A//arxiv.org/abs/2601.18353v1&entry.124074799=Read"},
{"title": "Discriminability-Driven Spatial-Channel Selection with Gradient Norm for Drone Signal OOD Detection", "author": "Chuhan Feng and Jing Li and Jie Li and Lu Lv and Fengkui Gong", "abstract": "We propose a drone signal out-of-distribution (OOD) detection algorithm based on discriminability-driven spatial-channel selection with a gradient norm. Time-frequency image features are adaptively weighted along both spatial and channel dimensions by quantifying inter-class similarity and variance based on protocol-specific time-frequency characteristics. Subsequently, a gradient-norm metric is introduced to measure perturbation sensitivity for capturing the inherent instability of OOD samples, which is then fused with energy-based scores for joint inference. Simulation results demonstrate that the proposed algorithm provides superior discriminative power and robust performance via SNR and various drone types.", "link": "http://arxiv.org/abs/2601.18329v1", "date": "2026-01-26", "relevancy": 2.2935, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4711}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.457}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discriminability-Driven%20Spatial-Channel%20Selection%20with%20Gradient%20Norm%20for%20Drone%20Signal%20OOD%20Detection&body=Title%3A%20Discriminability-Driven%20Spatial-Channel%20Selection%20with%20Gradient%20Norm%20for%20Drone%20Signal%20OOD%20Detection%0AAuthor%3A%20Chuhan%20Feng%20and%20Jing%20Li%20and%20Jie%20Li%20and%20Lu%20Lv%20and%20Fengkui%20Gong%0AAbstract%3A%20We%20propose%20a%20drone%20signal%20out-of-distribution%20%28OOD%29%20detection%20algorithm%20based%20on%20discriminability-driven%20spatial-channel%20selection%20with%20a%20gradient%20norm.%20Time-frequency%20image%20features%20are%20adaptively%20weighted%20along%20both%20spatial%20and%20channel%20dimensions%20by%20quantifying%20inter-class%20similarity%20and%20variance%20based%20on%20protocol-specific%20time-frequency%20characteristics.%20Subsequently%2C%20a%20gradient-norm%20metric%20is%20introduced%20to%20measure%20perturbation%20sensitivity%20for%20capturing%20the%20inherent%20instability%20of%20OOD%20samples%2C%20which%20is%20then%20fused%20with%20energy-based%20scores%20for%20joint%20inference.%20Simulation%20results%20demonstrate%20that%20the%20proposed%20algorithm%20provides%20superior%20discriminative%20power%20and%20robust%20performance%20via%20SNR%20and%20various%20drone%20types.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18329v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscriminability-Driven%2520Spatial-Channel%2520Selection%2520with%2520Gradient%2520Norm%2520for%2520Drone%2520Signal%2520OOD%2520Detection%26entry.906535625%3DChuhan%2520Feng%2520and%2520Jing%2520Li%2520and%2520Jie%2520Li%2520and%2520Lu%2520Lv%2520and%2520Fengkui%2520Gong%26entry.1292438233%3DWe%2520propose%2520a%2520drone%2520signal%2520out-of-distribution%2520%2528OOD%2529%2520detection%2520algorithm%2520based%2520on%2520discriminability-driven%2520spatial-channel%2520selection%2520with%2520a%2520gradient%2520norm.%2520Time-frequency%2520image%2520features%2520are%2520adaptively%2520weighted%2520along%2520both%2520spatial%2520and%2520channel%2520dimensions%2520by%2520quantifying%2520inter-class%2520similarity%2520and%2520variance%2520based%2520on%2520protocol-specific%2520time-frequency%2520characteristics.%2520Subsequently%252C%2520a%2520gradient-norm%2520metric%2520is%2520introduced%2520to%2520measure%2520perturbation%2520sensitivity%2520for%2520capturing%2520the%2520inherent%2520instability%2520of%2520OOD%2520samples%252C%2520which%2520is%2520then%2520fused%2520with%2520energy-based%2520scores%2520for%2520joint%2520inference.%2520Simulation%2520results%2520demonstrate%2520that%2520the%2520proposed%2520algorithm%2520provides%2520superior%2520discriminative%2520power%2520and%2520robust%2520performance%2520via%2520SNR%2520and%2520various%2520drone%2520types.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18329v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discriminability-Driven%20Spatial-Channel%20Selection%20with%20Gradient%20Norm%20for%20Drone%20Signal%20OOD%20Detection&entry.906535625=Chuhan%20Feng%20and%20Jing%20Li%20and%20Jie%20Li%20and%20Lu%20Lv%20and%20Fengkui%20Gong&entry.1292438233=We%20propose%20a%20drone%20signal%20out-of-distribution%20%28OOD%29%20detection%20algorithm%20based%20on%20discriminability-driven%20spatial-channel%20selection%20with%20a%20gradient%20norm.%20Time-frequency%20image%20features%20are%20adaptively%20weighted%20along%20both%20spatial%20and%20channel%20dimensions%20by%20quantifying%20inter-class%20similarity%20and%20variance%20based%20on%20protocol-specific%20time-frequency%20characteristics.%20Subsequently%2C%20a%20gradient-norm%20metric%20is%20introduced%20to%20measure%20perturbation%20sensitivity%20for%20capturing%20the%20inherent%20instability%20of%20OOD%20samples%2C%20which%20is%20then%20fused%20with%20energy-based%20scores%20for%20joint%20inference.%20Simulation%20results%20demonstrate%20that%20the%20proposed%20algorithm%20provides%20superior%20discriminative%20power%20and%20robust%20performance%20via%20SNR%20and%20various%20drone%20types.&entry.1838667208=http%3A//arxiv.org/abs/2601.18329v1&entry.124074799=Read"},
{"title": "Goal-oriented Semantic Communication for Robot Arm Reconstruction in Digital Twin: Feature and Temporal Selections", "author": "Shutong Chen and Emmanouil Spyrakos-Papastavridis and Yichao Jin and Yansha Deng", "abstract": "As one of the most promising technologies in industry, the Digital Twin (DT) facilitates real-time monitoring and predictive analysis for real-world systems by precisely reconstructing virtual replicas of physical entities. However, this reconstruction faces unprecedented challenges due to the everincreasing communication overhead, especially for digital robot arm reconstruction. To this end, we propose a novel goal-oriented semantic communication (GSC) framework to extract the GSC information for the robot arm reconstruction task in the DT, with the aim of minimising the communication load under the strict and relaxed reconstruction error constraints. Unlike the traditional reconstruction framework that periodically transmits a reconstruction message for real-time DT reconstruction, our framework implements a feature selection (FS) algorithm to extract the semantic information from the reconstruction message, and a deep reinforcement learning-based temporal selection algorithm to selectively transmit the semantic information over time. We validate our proposed GSC framework through both Pybullet simulations and lab experiments based on the Franka Research 3 robot arm. For a range of distinct robotic tasks, simulation results show that our framework can reduce the communication load by at least 59.5% under strict reconstruction error constraints and 80% under relaxed reconstruction error constraints, compared with traditional communication framework. Also, experimental results confirm the effectiveness of our framework, where the communication load is reduced by 53% in strict constraint case and 74% in relaxed constraint case. The demo is available at: https://youtu.be/2OdeHKxcgnk.", "link": "http://arxiv.org/abs/2411.08835v2", "date": "2026-01-26", "relevancy": 2.2656, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5749}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5608}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Goal-oriented%20Semantic%20Communication%20for%20Robot%20Arm%20Reconstruction%20in%20Digital%20Twin%3A%20Feature%20and%20Temporal%20Selections&body=Title%3A%20Goal-oriented%20Semantic%20Communication%20for%20Robot%20Arm%20Reconstruction%20in%20Digital%20Twin%3A%20Feature%20and%20Temporal%20Selections%0AAuthor%3A%20Shutong%20Chen%20and%20Emmanouil%20Spyrakos-Papastavridis%20and%20Yichao%20Jin%20and%20Yansha%20Deng%0AAbstract%3A%20As%20one%20of%20the%20most%20promising%20technologies%20in%20industry%2C%20the%20Digital%20Twin%20%28DT%29%20facilitates%20real-time%20monitoring%20and%20predictive%20analysis%20for%20real-world%20systems%20by%20precisely%20reconstructing%20virtual%20replicas%20of%20physical%20entities.%20However%2C%20this%20reconstruction%20faces%20unprecedented%20challenges%20due%20to%20the%20everincreasing%20communication%20overhead%2C%20especially%20for%20digital%20robot%20arm%20reconstruction.%20To%20this%20end%2C%20we%20propose%20a%20novel%20goal-oriented%20semantic%20communication%20%28GSC%29%20framework%20to%20extract%20the%20GSC%20information%20for%20the%20robot%20arm%20reconstruction%20task%20in%20the%20DT%2C%20with%20the%20aim%20of%20minimising%20the%20communication%20load%20under%20the%20strict%20and%20relaxed%20reconstruction%20error%20constraints.%20Unlike%20the%20traditional%20reconstruction%20framework%20that%20periodically%20transmits%20a%20reconstruction%20message%20for%20real-time%20DT%20reconstruction%2C%20our%20framework%20implements%20a%20feature%20selection%20%28FS%29%20algorithm%20to%20extract%20the%20semantic%20information%20from%20the%20reconstruction%20message%2C%20and%20a%20deep%20reinforcement%20learning-based%20temporal%20selection%20algorithm%20to%20selectively%20transmit%20the%20semantic%20information%20over%20time.%20We%20validate%20our%20proposed%20GSC%20framework%20through%20both%20Pybullet%20simulations%20and%20lab%20experiments%20based%20on%20the%20Franka%20Research%203%20robot%20arm.%20For%20a%20range%20of%20distinct%20robotic%20tasks%2C%20simulation%20results%20show%20that%20our%20framework%20can%20reduce%20the%20communication%20load%20by%20at%20least%2059.5%25%20under%20strict%20reconstruction%20error%20constraints%20and%2080%25%20under%20relaxed%20reconstruction%20error%20constraints%2C%20compared%20with%20traditional%20communication%20framework.%20Also%2C%20experimental%20results%20confirm%20the%20effectiveness%20of%20our%20framework%2C%20where%20the%20communication%20load%20is%20reduced%20by%2053%25%20in%20strict%20constraint%20case%20and%2074%25%20in%20relaxed%20constraint%20case.%20The%20demo%20is%20available%20at%3A%20https%3A//youtu.be/2OdeHKxcgnk.%0ALink%3A%20http%3A//arxiv.org/abs/2411.08835v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGoal-oriented%2520Semantic%2520Communication%2520for%2520Robot%2520Arm%2520Reconstruction%2520in%2520Digital%2520Twin%253A%2520Feature%2520and%2520Temporal%2520Selections%26entry.906535625%3DShutong%2520Chen%2520and%2520Emmanouil%2520Spyrakos-Papastavridis%2520and%2520Yichao%2520Jin%2520and%2520Yansha%2520Deng%26entry.1292438233%3DAs%2520one%2520of%2520the%2520most%2520promising%2520technologies%2520in%2520industry%252C%2520the%2520Digital%2520Twin%2520%2528DT%2529%2520facilitates%2520real-time%2520monitoring%2520and%2520predictive%2520analysis%2520for%2520real-world%2520systems%2520by%2520precisely%2520reconstructing%2520virtual%2520replicas%2520of%2520physical%2520entities.%2520However%252C%2520this%2520reconstruction%2520faces%2520unprecedented%2520challenges%2520due%2520to%2520the%2520everincreasing%2520communication%2520overhead%252C%2520especially%2520for%2520digital%2520robot%2520arm%2520reconstruction.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520novel%2520goal-oriented%2520semantic%2520communication%2520%2528GSC%2529%2520framework%2520to%2520extract%2520the%2520GSC%2520information%2520for%2520the%2520robot%2520arm%2520reconstruction%2520task%2520in%2520the%2520DT%252C%2520with%2520the%2520aim%2520of%2520minimising%2520the%2520communication%2520load%2520under%2520the%2520strict%2520and%2520relaxed%2520reconstruction%2520error%2520constraints.%2520Unlike%2520the%2520traditional%2520reconstruction%2520framework%2520that%2520periodically%2520transmits%2520a%2520reconstruction%2520message%2520for%2520real-time%2520DT%2520reconstruction%252C%2520our%2520framework%2520implements%2520a%2520feature%2520selection%2520%2528FS%2529%2520algorithm%2520to%2520extract%2520the%2520semantic%2520information%2520from%2520the%2520reconstruction%2520message%252C%2520and%2520a%2520deep%2520reinforcement%2520learning-based%2520temporal%2520selection%2520algorithm%2520to%2520selectively%2520transmit%2520the%2520semantic%2520information%2520over%2520time.%2520We%2520validate%2520our%2520proposed%2520GSC%2520framework%2520through%2520both%2520Pybullet%2520simulations%2520and%2520lab%2520experiments%2520based%2520on%2520the%2520Franka%2520Research%25203%2520robot%2520arm.%2520For%2520a%2520range%2520of%2520distinct%2520robotic%2520tasks%252C%2520simulation%2520results%2520show%2520that%2520our%2520framework%2520can%2520reduce%2520the%2520communication%2520load%2520by%2520at%2520least%252059.5%2525%2520under%2520strict%2520reconstruction%2520error%2520constraints%2520and%252080%2525%2520under%2520relaxed%2520reconstruction%2520error%2520constraints%252C%2520compared%2520with%2520traditional%2520communication%2520framework.%2520Also%252C%2520experimental%2520results%2520confirm%2520the%2520effectiveness%2520of%2520our%2520framework%252C%2520where%2520the%2520communication%2520load%2520is%2520reduced%2520by%252053%2525%2520in%2520strict%2520constraint%2520case%2520and%252074%2525%2520in%2520relaxed%2520constraint%2520case.%2520The%2520demo%2520is%2520available%2520at%253A%2520https%253A//youtu.be/2OdeHKxcgnk.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08835v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Goal-oriented%20Semantic%20Communication%20for%20Robot%20Arm%20Reconstruction%20in%20Digital%20Twin%3A%20Feature%20and%20Temporal%20Selections&entry.906535625=Shutong%20Chen%20and%20Emmanouil%20Spyrakos-Papastavridis%20and%20Yichao%20Jin%20and%20Yansha%20Deng&entry.1292438233=As%20one%20of%20the%20most%20promising%20technologies%20in%20industry%2C%20the%20Digital%20Twin%20%28DT%29%20facilitates%20real-time%20monitoring%20and%20predictive%20analysis%20for%20real-world%20systems%20by%20precisely%20reconstructing%20virtual%20replicas%20of%20physical%20entities.%20However%2C%20this%20reconstruction%20faces%20unprecedented%20challenges%20due%20to%20the%20everincreasing%20communication%20overhead%2C%20especially%20for%20digital%20robot%20arm%20reconstruction.%20To%20this%20end%2C%20we%20propose%20a%20novel%20goal-oriented%20semantic%20communication%20%28GSC%29%20framework%20to%20extract%20the%20GSC%20information%20for%20the%20robot%20arm%20reconstruction%20task%20in%20the%20DT%2C%20with%20the%20aim%20of%20minimising%20the%20communication%20load%20under%20the%20strict%20and%20relaxed%20reconstruction%20error%20constraints.%20Unlike%20the%20traditional%20reconstruction%20framework%20that%20periodically%20transmits%20a%20reconstruction%20message%20for%20real-time%20DT%20reconstruction%2C%20our%20framework%20implements%20a%20feature%20selection%20%28FS%29%20algorithm%20to%20extract%20the%20semantic%20information%20from%20the%20reconstruction%20message%2C%20and%20a%20deep%20reinforcement%20learning-based%20temporal%20selection%20algorithm%20to%20selectively%20transmit%20the%20semantic%20information%20over%20time.%20We%20validate%20our%20proposed%20GSC%20framework%20through%20both%20Pybullet%20simulations%20and%20lab%20experiments%20based%20on%20the%20Franka%20Research%203%20robot%20arm.%20For%20a%20range%20of%20distinct%20robotic%20tasks%2C%20simulation%20results%20show%20that%20our%20framework%20can%20reduce%20the%20communication%20load%20by%20at%20least%2059.5%25%20under%20strict%20reconstruction%20error%20constraints%20and%2080%25%20under%20relaxed%20reconstruction%20error%20constraints%2C%20compared%20with%20traditional%20communication%20framework.%20Also%2C%20experimental%20results%20confirm%20the%20effectiveness%20of%20our%20framework%2C%20where%20the%20communication%20load%20is%20reduced%20by%2053%25%20in%20strict%20constraint%20case%20and%2074%25%20in%20relaxed%20constraint%20case.%20The%20demo%20is%20available%20at%3A%20https%3A//youtu.be/2OdeHKxcgnk.&entry.1838667208=http%3A//arxiv.org/abs/2411.08835v2&entry.124074799=Read"},
{"title": "SMART: Scalable Mesh-free Aerodynamic Simulations from Raw Geometries using a Transformer-based Surrogate Model", "author": "Jan Hagnberger and Mathias Niepert", "abstract": "Machine learning-based surrogate models have emerged as more efficient alternatives to numerical solvers for physical simulations over complex geometries, such as car bodies. Many existing models incorporate the simulation mesh as an additional input, thereby reducing prediction errors. However, generating a simulation mesh for new geometries is computationally costly. In contrast, mesh-free methods, which do not rely on the simulation mesh, typically incur higher errors. Motivated by these considerations, we introduce SMART, a neural surrogate model that predicts physical quantities at arbitrary query locations using only a point-cloud representation of the geometry, without requiring access to the simulation mesh. The geometry and simulation parameters are encoded into a shared latent space that captures both structural and parametric characteristics of the physical field. A physics decoder then attends to the encoder's intermediate latent representations to map spatial queries to physical quantities. Through this cross-layer interaction, the model jointly updates latent geometric features and the evolving physical field. Extensive experiments show that SMART is competitive with and often outperforms existing methods that rely on the simulation mesh as input, demonstrating its capabilities for industry-level simulations.", "link": "http://arxiv.org/abs/2601.18707v1", "date": "2026-01-26", "relevancy": 2.255, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5726}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5577}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMART%3A%20Scalable%20Mesh-free%20Aerodynamic%20Simulations%20from%20Raw%20Geometries%20using%20a%20Transformer-based%20Surrogate%20Model&body=Title%3A%20SMART%3A%20Scalable%20Mesh-free%20Aerodynamic%20Simulations%20from%20Raw%20Geometries%20using%20a%20Transformer-based%20Surrogate%20Model%0AAuthor%3A%20Jan%20Hagnberger%20and%20Mathias%20Niepert%0AAbstract%3A%20Machine%20learning-based%20surrogate%20models%20have%20emerged%20as%20more%20efficient%20alternatives%20to%20numerical%20solvers%20for%20physical%20simulations%20over%20complex%20geometries%2C%20such%20as%20car%20bodies.%20Many%20existing%20models%20incorporate%20the%20simulation%20mesh%20as%20an%20additional%20input%2C%20thereby%20reducing%20prediction%20errors.%20However%2C%20generating%20a%20simulation%20mesh%20for%20new%20geometries%20is%20computationally%20costly.%20In%20contrast%2C%20mesh-free%20methods%2C%20which%20do%20not%20rely%20on%20the%20simulation%20mesh%2C%20typically%20incur%20higher%20errors.%20Motivated%20by%20these%20considerations%2C%20we%20introduce%20SMART%2C%20a%20neural%20surrogate%20model%20that%20predicts%20physical%20quantities%20at%20arbitrary%20query%20locations%20using%20only%20a%20point-cloud%20representation%20of%20the%20geometry%2C%20without%20requiring%20access%20to%20the%20simulation%20mesh.%20The%20geometry%20and%20simulation%20parameters%20are%20encoded%20into%20a%20shared%20latent%20space%20that%20captures%20both%20structural%20and%20parametric%20characteristics%20of%20the%20physical%20field.%20A%20physics%20decoder%20then%20attends%20to%20the%20encoder%27s%20intermediate%20latent%20representations%20to%20map%20spatial%20queries%20to%20physical%20quantities.%20Through%20this%20cross-layer%20interaction%2C%20the%20model%20jointly%20updates%20latent%20geometric%20features%20and%20the%20evolving%20physical%20field.%20Extensive%20experiments%20show%20that%20SMART%20is%20competitive%20with%20and%20often%20outperforms%20existing%20methods%20that%20rely%20on%20the%20simulation%20mesh%20as%20input%2C%20demonstrating%20its%20capabilities%20for%20industry-level%20simulations.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMART%253A%2520Scalable%2520Mesh-free%2520Aerodynamic%2520Simulations%2520from%2520Raw%2520Geometries%2520using%2520a%2520Transformer-based%2520Surrogate%2520Model%26entry.906535625%3DJan%2520Hagnberger%2520and%2520Mathias%2520Niepert%26entry.1292438233%3DMachine%2520learning-based%2520surrogate%2520models%2520have%2520emerged%2520as%2520more%2520efficient%2520alternatives%2520to%2520numerical%2520solvers%2520for%2520physical%2520simulations%2520over%2520complex%2520geometries%252C%2520such%2520as%2520car%2520bodies.%2520Many%2520existing%2520models%2520incorporate%2520the%2520simulation%2520mesh%2520as%2520an%2520additional%2520input%252C%2520thereby%2520reducing%2520prediction%2520errors.%2520However%252C%2520generating%2520a%2520simulation%2520mesh%2520for%2520new%2520geometries%2520is%2520computationally%2520costly.%2520In%2520contrast%252C%2520mesh-free%2520methods%252C%2520which%2520do%2520not%2520rely%2520on%2520the%2520simulation%2520mesh%252C%2520typically%2520incur%2520higher%2520errors.%2520Motivated%2520by%2520these%2520considerations%252C%2520we%2520introduce%2520SMART%252C%2520a%2520neural%2520surrogate%2520model%2520that%2520predicts%2520physical%2520quantities%2520at%2520arbitrary%2520query%2520locations%2520using%2520only%2520a%2520point-cloud%2520representation%2520of%2520the%2520geometry%252C%2520without%2520requiring%2520access%2520to%2520the%2520simulation%2520mesh.%2520The%2520geometry%2520and%2520simulation%2520parameters%2520are%2520encoded%2520into%2520a%2520shared%2520latent%2520space%2520that%2520captures%2520both%2520structural%2520and%2520parametric%2520characteristics%2520of%2520the%2520physical%2520field.%2520A%2520physics%2520decoder%2520then%2520attends%2520to%2520the%2520encoder%2527s%2520intermediate%2520latent%2520representations%2520to%2520map%2520spatial%2520queries%2520to%2520physical%2520quantities.%2520Through%2520this%2520cross-layer%2520interaction%252C%2520the%2520model%2520jointly%2520updates%2520latent%2520geometric%2520features%2520and%2520the%2520evolving%2520physical%2520field.%2520Extensive%2520experiments%2520show%2520that%2520SMART%2520is%2520competitive%2520with%2520and%2520often%2520outperforms%2520existing%2520methods%2520that%2520rely%2520on%2520the%2520simulation%2520mesh%2520as%2520input%252C%2520demonstrating%2520its%2520capabilities%2520for%2520industry-level%2520simulations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMART%3A%20Scalable%20Mesh-free%20Aerodynamic%20Simulations%20from%20Raw%20Geometries%20using%20a%20Transformer-based%20Surrogate%20Model&entry.906535625=Jan%20Hagnberger%20and%20Mathias%20Niepert&entry.1292438233=Machine%20learning-based%20surrogate%20models%20have%20emerged%20as%20more%20efficient%20alternatives%20to%20numerical%20solvers%20for%20physical%20simulations%20over%20complex%20geometries%2C%20such%20as%20car%20bodies.%20Many%20existing%20models%20incorporate%20the%20simulation%20mesh%20as%20an%20additional%20input%2C%20thereby%20reducing%20prediction%20errors.%20However%2C%20generating%20a%20simulation%20mesh%20for%20new%20geometries%20is%20computationally%20costly.%20In%20contrast%2C%20mesh-free%20methods%2C%20which%20do%20not%20rely%20on%20the%20simulation%20mesh%2C%20typically%20incur%20higher%20errors.%20Motivated%20by%20these%20considerations%2C%20we%20introduce%20SMART%2C%20a%20neural%20surrogate%20model%20that%20predicts%20physical%20quantities%20at%20arbitrary%20query%20locations%20using%20only%20a%20point-cloud%20representation%20of%20the%20geometry%2C%20without%20requiring%20access%20to%20the%20simulation%20mesh.%20The%20geometry%20and%20simulation%20parameters%20are%20encoded%20into%20a%20shared%20latent%20space%20that%20captures%20both%20structural%20and%20parametric%20characteristics%20of%20the%20physical%20field.%20A%20physics%20decoder%20then%20attends%20to%20the%20encoder%27s%20intermediate%20latent%20representations%20to%20map%20spatial%20queries%20to%20physical%20quantities.%20Through%20this%20cross-layer%20interaction%2C%20the%20model%20jointly%20updates%20latent%20geometric%20features%20and%20the%20evolving%20physical%20field.%20Extensive%20experiments%20show%20that%20SMART%20is%20competitive%20with%20and%20often%20outperforms%20existing%20methods%20that%20rely%20on%20the%20simulation%20mesh%20as%20input%2C%20demonstrating%20its%20capabilities%20for%20industry-level%20simulations.&entry.1838667208=http%3A//arxiv.org/abs/2601.18707v1&entry.124074799=Read"},
{"title": "Closing the Modality Gap Aligns Group-Wise Semantics", "author": "Eleonora Grassucci and Giordano Cicchetti and Emanuele Frasca and Aurelio Uncini and Danilo Comminiello", "abstract": "In multimodal learning, CLIP has been recognized as the \\textit{de facto} method for learning a shared latent space across multiple modalities, placing similar representations close to each other and moving them away from dissimilar ones. Although CLIP-based losses effectively align modalities at the semantic level, the resulting latent spaces often remain only partially shared, revealing a structural mismatch known as the modality gap. While the necessity of addressing this phenomenon remains debated, particularly given its limited impact on instance-wise tasks (e.g., retrieval), we prove that its influence is instead strongly pronounced in group-level tasks (e.g., clustering). To support this claim, we introduce a novel method designed to consistently reduce this discrepancy in two-modal settings, with a straightforward extension to the general $n$-modal case. Through our extensive evaluation, we demonstrate our novel insight: while reducing the gap provides only marginal or inconsistent improvements in traditional instance-wise tasks, it significantly enhances group-wise tasks. These findings may reshape our understanding of the modality gap, highlighting its key role in improving performance on tasks requiring semantic grouping.", "link": "http://arxiv.org/abs/2601.18525v1", "date": "2026-01-26", "relevancy": 2.25, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6186}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5291}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Closing%20the%20Modality%20Gap%20Aligns%20Group-Wise%20Semantics&body=Title%3A%20Closing%20the%20Modality%20Gap%20Aligns%20Group-Wise%20Semantics%0AAuthor%3A%20Eleonora%20Grassucci%20and%20Giordano%20Cicchetti%20and%20Emanuele%20Frasca%20and%20Aurelio%20Uncini%20and%20Danilo%20Comminiello%0AAbstract%3A%20In%20multimodal%20learning%2C%20CLIP%20has%20been%20recognized%20as%20the%20%5Ctextit%7Bde%20facto%7D%20method%20for%20learning%20a%20shared%20latent%20space%20across%20multiple%20modalities%2C%20placing%20similar%20representations%20close%20to%20each%20other%20and%20moving%20them%20away%20from%20dissimilar%20ones.%20Although%20CLIP-based%20losses%20effectively%20align%20modalities%20at%20the%20semantic%20level%2C%20the%20resulting%20latent%20spaces%20often%20remain%20only%20partially%20shared%2C%20revealing%20a%20structural%20mismatch%20known%20as%20the%20modality%20gap.%20While%20the%20necessity%20of%20addressing%20this%20phenomenon%20remains%20debated%2C%20particularly%20given%20its%20limited%20impact%20on%20instance-wise%20tasks%20%28e.g.%2C%20retrieval%29%2C%20we%20prove%20that%20its%20influence%20is%20instead%20strongly%20pronounced%20in%20group-level%20tasks%20%28e.g.%2C%20clustering%29.%20To%20support%20this%20claim%2C%20we%20introduce%20a%20novel%20method%20designed%20to%20consistently%20reduce%20this%20discrepancy%20in%20two-modal%20settings%2C%20with%20a%20straightforward%20extension%20to%20the%20general%20%24n%24-modal%20case.%20Through%20our%20extensive%20evaluation%2C%20we%20demonstrate%20our%20novel%20insight%3A%20while%20reducing%20the%20gap%20provides%20only%20marginal%20or%20inconsistent%20improvements%20in%20traditional%20instance-wise%20tasks%2C%20it%20significantly%20enhances%20group-wise%20tasks.%20These%20findings%20may%20reshape%20our%20understanding%20of%20the%20modality%20gap%2C%20highlighting%20its%20key%20role%20in%20improving%20performance%20on%20tasks%20requiring%20semantic%20grouping.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18525v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClosing%2520the%2520Modality%2520Gap%2520Aligns%2520Group-Wise%2520Semantics%26entry.906535625%3DEleonora%2520Grassucci%2520and%2520Giordano%2520Cicchetti%2520and%2520Emanuele%2520Frasca%2520and%2520Aurelio%2520Uncini%2520and%2520Danilo%2520Comminiello%26entry.1292438233%3DIn%2520multimodal%2520learning%252C%2520CLIP%2520has%2520been%2520recognized%2520as%2520the%2520%255Ctextit%257Bde%2520facto%257D%2520method%2520for%2520learning%2520a%2520shared%2520latent%2520space%2520across%2520multiple%2520modalities%252C%2520placing%2520similar%2520representations%2520close%2520to%2520each%2520other%2520and%2520moving%2520them%2520away%2520from%2520dissimilar%2520ones.%2520Although%2520CLIP-based%2520losses%2520effectively%2520align%2520modalities%2520at%2520the%2520semantic%2520level%252C%2520the%2520resulting%2520latent%2520spaces%2520often%2520remain%2520only%2520partially%2520shared%252C%2520revealing%2520a%2520structural%2520mismatch%2520known%2520as%2520the%2520modality%2520gap.%2520While%2520the%2520necessity%2520of%2520addressing%2520this%2520phenomenon%2520remains%2520debated%252C%2520particularly%2520given%2520its%2520limited%2520impact%2520on%2520instance-wise%2520tasks%2520%2528e.g.%252C%2520retrieval%2529%252C%2520we%2520prove%2520that%2520its%2520influence%2520is%2520instead%2520strongly%2520pronounced%2520in%2520group-level%2520tasks%2520%2528e.g.%252C%2520clustering%2529.%2520To%2520support%2520this%2520claim%252C%2520we%2520introduce%2520a%2520novel%2520method%2520designed%2520to%2520consistently%2520reduce%2520this%2520discrepancy%2520in%2520two-modal%2520settings%252C%2520with%2520a%2520straightforward%2520extension%2520to%2520the%2520general%2520%2524n%2524-modal%2520case.%2520Through%2520our%2520extensive%2520evaluation%252C%2520we%2520demonstrate%2520our%2520novel%2520insight%253A%2520while%2520reducing%2520the%2520gap%2520provides%2520only%2520marginal%2520or%2520inconsistent%2520improvements%2520in%2520traditional%2520instance-wise%2520tasks%252C%2520it%2520significantly%2520enhances%2520group-wise%2520tasks.%2520These%2520findings%2520may%2520reshape%2520our%2520understanding%2520of%2520the%2520modality%2520gap%252C%2520highlighting%2520its%2520key%2520role%2520in%2520improving%2520performance%2520on%2520tasks%2520requiring%2520semantic%2520grouping.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18525v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Closing%20the%20Modality%20Gap%20Aligns%20Group-Wise%20Semantics&entry.906535625=Eleonora%20Grassucci%20and%20Giordano%20Cicchetti%20and%20Emanuele%20Frasca%20and%20Aurelio%20Uncini%20and%20Danilo%20Comminiello&entry.1292438233=In%20multimodal%20learning%2C%20CLIP%20has%20been%20recognized%20as%20the%20%5Ctextit%7Bde%20facto%7D%20method%20for%20learning%20a%20shared%20latent%20space%20across%20multiple%20modalities%2C%20placing%20similar%20representations%20close%20to%20each%20other%20and%20moving%20them%20away%20from%20dissimilar%20ones.%20Although%20CLIP-based%20losses%20effectively%20align%20modalities%20at%20the%20semantic%20level%2C%20the%20resulting%20latent%20spaces%20often%20remain%20only%20partially%20shared%2C%20revealing%20a%20structural%20mismatch%20known%20as%20the%20modality%20gap.%20While%20the%20necessity%20of%20addressing%20this%20phenomenon%20remains%20debated%2C%20particularly%20given%20its%20limited%20impact%20on%20instance-wise%20tasks%20%28e.g.%2C%20retrieval%29%2C%20we%20prove%20that%20its%20influence%20is%20instead%20strongly%20pronounced%20in%20group-level%20tasks%20%28e.g.%2C%20clustering%29.%20To%20support%20this%20claim%2C%20we%20introduce%20a%20novel%20method%20designed%20to%20consistently%20reduce%20this%20discrepancy%20in%20two-modal%20settings%2C%20with%20a%20straightforward%20extension%20to%20the%20general%20%24n%24-modal%20case.%20Through%20our%20extensive%20evaluation%2C%20we%20demonstrate%20our%20novel%20insight%3A%20while%20reducing%20the%20gap%20provides%20only%20marginal%20or%20inconsistent%20improvements%20in%20traditional%20instance-wise%20tasks%2C%20it%20significantly%20enhances%20group-wise%20tasks.%20These%20findings%20may%20reshape%20our%20understanding%20of%20the%20modality%20gap%2C%20highlighting%20its%20key%20role%20in%20improving%20performance%20on%20tasks%20requiring%20semantic%20grouping.&entry.1838667208=http%3A//arxiv.org/abs/2601.18525v1&entry.124074799=Read"},
{"title": "GimmBO: Interactive Generative Image Model Merging via Bayesian Optimization", "author": "Chenxi Liu and Selena Ling and Alec Jacobson", "abstract": "Fine-tuning-based adaptation is widely used to customize diffusion-based image generation, leading to large collections of community-created adapters that capture diverse subjects and styles. Adapters derived from the same base model can be merged with weights, enabling the synthesis of new visual results within a vast and continuous design space. To explore this space, current workflows rely on manual slider-based tuning, an approach that scales poorly and makes weight selection difficult, even when the candidate set is limited to 20-30 adapters. We propose GimmBO to support interactive exploration of adapter merging for image generation through Preferential Bayesian Optimization (PBO). Motivated by observations from real-world usage, including sparsity and constrained weight ranges, we introduce a two-stage BO backend that improves sampling efficiency and convergence in high-dimensional spaces. We evaluate our approach with simulated users and a user study, demonstrating improved convergence, high success rates, and consistent gains over BO and line-search baselines, and further show the flexibility of the framework through several extensions.", "link": "http://arxiv.org/abs/2601.18585v1", "date": "2026-01-26", "relevancy": 2.2419, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6183}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5508}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GimmBO%3A%20Interactive%20Generative%20Image%20Model%20Merging%20via%20Bayesian%20Optimization&body=Title%3A%20GimmBO%3A%20Interactive%20Generative%20Image%20Model%20Merging%20via%20Bayesian%20Optimization%0AAuthor%3A%20Chenxi%20Liu%20and%20Selena%20Ling%20and%20Alec%20Jacobson%0AAbstract%3A%20Fine-tuning-based%20adaptation%20is%20widely%20used%20to%20customize%20diffusion-based%20image%20generation%2C%20leading%20to%20large%20collections%20of%20community-created%20adapters%20that%20capture%20diverse%20subjects%20and%20styles.%20Adapters%20derived%20from%20the%20same%20base%20model%20can%20be%20merged%20with%20weights%2C%20enabling%20the%20synthesis%20of%20new%20visual%20results%20within%20a%20vast%20and%20continuous%20design%20space.%20To%20explore%20this%20space%2C%20current%20workflows%20rely%20on%20manual%20slider-based%20tuning%2C%20an%20approach%20that%20scales%20poorly%20and%20makes%20weight%20selection%20difficult%2C%20even%20when%20the%20candidate%20set%20is%20limited%20to%2020-30%20adapters.%20We%20propose%20GimmBO%20to%20support%20interactive%20exploration%20of%20adapter%20merging%20for%20image%20generation%20through%20Preferential%20Bayesian%20Optimization%20%28PBO%29.%20Motivated%20by%20observations%20from%20real-world%20usage%2C%20including%20sparsity%20and%20constrained%20weight%20ranges%2C%20we%20introduce%20a%20two-stage%20BO%20backend%20that%20improves%20sampling%20efficiency%20and%20convergence%20in%20high-dimensional%20spaces.%20We%20evaluate%20our%20approach%20with%20simulated%20users%20and%20a%20user%20study%2C%20demonstrating%20improved%20convergence%2C%20high%20success%20rates%2C%20and%20consistent%20gains%20over%20BO%20and%20line-search%20baselines%2C%20and%20further%20show%20the%20flexibility%20of%20the%20framework%20through%20several%20extensions.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18585v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGimmBO%253A%2520Interactive%2520Generative%2520Image%2520Model%2520Merging%2520via%2520Bayesian%2520Optimization%26entry.906535625%3DChenxi%2520Liu%2520and%2520Selena%2520Ling%2520and%2520Alec%2520Jacobson%26entry.1292438233%3DFine-tuning-based%2520adaptation%2520is%2520widely%2520used%2520to%2520customize%2520diffusion-based%2520image%2520generation%252C%2520leading%2520to%2520large%2520collections%2520of%2520community-created%2520adapters%2520that%2520capture%2520diverse%2520subjects%2520and%2520styles.%2520Adapters%2520derived%2520from%2520the%2520same%2520base%2520model%2520can%2520be%2520merged%2520with%2520weights%252C%2520enabling%2520the%2520synthesis%2520of%2520new%2520visual%2520results%2520within%2520a%2520vast%2520and%2520continuous%2520design%2520space.%2520To%2520explore%2520this%2520space%252C%2520current%2520workflows%2520rely%2520on%2520manual%2520slider-based%2520tuning%252C%2520an%2520approach%2520that%2520scales%2520poorly%2520and%2520makes%2520weight%2520selection%2520difficult%252C%2520even%2520when%2520the%2520candidate%2520set%2520is%2520limited%2520to%252020-30%2520adapters.%2520We%2520propose%2520GimmBO%2520to%2520support%2520interactive%2520exploration%2520of%2520adapter%2520merging%2520for%2520image%2520generation%2520through%2520Preferential%2520Bayesian%2520Optimization%2520%2528PBO%2529.%2520Motivated%2520by%2520observations%2520from%2520real-world%2520usage%252C%2520including%2520sparsity%2520and%2520constrained%2520weight%2520ranges%252C%2520we%2520introduce%2520a%2520two-stage%2520BO%2520backend%2520that%2520improves%2520sampling%2520efficiency%2520and%2520convergence%2520in%2520high-dimensional%2520spaces.%2520We%2520evaluate%2520our%2520approach%2520with%2520simulated%2520users%2520and%2520a%2520user%2520study%252C%2520demonstrating%2520improved%2520convergence%252C%2520high%2520success%2520rates%252C%2520and%2520consistent%2520gains%2520over%2520BO%2520and%2520line-search%2520baselines%252C%2520and%2520further%2520show%2520the%2520flexibility%2520of%2520the%2520framework%2520through%2520several%2520extensions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18585v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GimmBO%3A%20Interactive%20Generative%20Image%20Model%20Merging%20via%20Bayesian%20Optimization&entry.906535625=Chenxi%20Liu%20and%20Selena%20Ling%20and%20Alec%20Jacobson&entry.1292438233=Fine-tuning-based%20adaptation%20is%20widely%20used%20to%20customize%20diffusion-based%20image%20generation%2C%20leading%20to%20large%20collections%20of%20community-created%20adapters%20that%20capture%20diverse%20subjects%20and%20styles.%20Adapters%20derived%20from%20the%20same%20base%20model%20can%20be%20merged%20with%20weights%2C%20enabling%20the%20synthesis%20of%20new%20visual%20results%20within%20a%20vast%20and%20continuous%20design%20space.%20To%20explore%20this%20space%2C%20current%20workflows%20rely%20on%20manual%20slider-based%20tuning%2C%20an%20approach%20that%20scales%20poorly%20and%20makes%20weight%20selection%20difficult%2C%20even%20when%20the%20candidate%20set%20is%20limited%20to%2020-30%20adapters.%20We%20propose%20GimmBO%20to%20support%20interactive%20exploration%20of%20adapter%20merging%20for%20image%20generation%20through%20Preferential%20Bayesian%20Optimization%20%28PBO%29.%20Motivated%20by%20observations%20from%20real-world%20usage%2C%20including%20sparsity%20and%20constrained%20weight%20ranges%2C%20we%20introduce%20a%20two-stage%20BO%20backend%20that%20improves%20sampling%20efficiency%20and%20convergence%20in%20high-dimensional%20spaces.%20We%20evaluate%20our%20approach%20with%20simulated%20users%20and%20a%20user%20study%2C%20demonstrating%20improved%20convergence%2C%20high%20success%20rates%2C%20and%20consistent%20gains%20over%20BO%20and%20line-search%20baselines%2C%20and%20further%20show%20the%20flexibility%20of%20the%20framework%20through%20several%20extensions.&entry.1838667208=http%3A//arxiv.org/abs/2601.18585v1&entry.124074799=Read"},
{"title": "DV-VLN: Dual Verification for Reliable LLM-Based Vision-and-Language Navigation", "author": "Zijun Li and Shijie Li and Zhenxi Zhang and Bin Li and Shoujun Zhou", "abstract": "Vision-and-Language Navigation (VLN) requires an embodied agent to navigate in a complex 3D environment according to natural language instructions. Recent progress in large language models (LLMs) has enabled language-driven navigation with improved interpretability. However, most LLM-based agents still rely on single-shot action decisions, where the model must choose one option from noisy, textualized multi-perspective observations. Due to local mismatches and imperfect intermediate reasoning, such decisions can easily deviate from the correct path, leading to error accumulation and reduced reliability in unseen environments. In this paper, we propose DV-VLN, a new VLN framework that follows a generate-then-verify paradigm. DV-VLN first performs parameter-efficient in-domain adaptation of an open-source LLaMA-2 backbone to produce a structured navigational chain-of-thought, and then verifies candidate actions with two complementary channels: True-False Verification (TFV) and Masked-Entity Verification (MEV). DV-VLN selects actions by aggregating verification successes across multiple samples, yielding interpretable scores for reranking. Experiments on R2R, RxR (English subset), and REVERIE show that DV-VLN consistently improves over direct prediction and sampling-only baselines, achieving competitive performance among language-only VLN agents and promising results compared with several cross-modal systems.Code is available at https://github.com/PlumJun/DV-VLN.", "link": "http://arxiv.org/abs/2601.18492v1", "date": "2026-01-26", "relevancy": 2.2348, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5651}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5651}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DV-VLN%3A%20Dual%20Verification%20for%20Reliable%20LLM-Based%20Vision-and-Language%20Navigation&body=Title%3A%20DV-VLN%3A%20Dual%20Verification%20for%20Reliable%20LLM-Based%20Vision-and-Language%20Navigation%0AAuthor%3A%20Zijun%20Li%20and%20Shijie%20Li%20and%20Zhenxi%20Zhang%20and%20Bin%20Li%20and%20Shoujun%20Zhou%0AAbstract%3A%20Vision-and-Language%20Navigation%20%28VLN%29%20requires%20an%20embodied%20agent%20to%20navigate%20in%20a%20complex%203D%20environment%20according%20to%20natural%20language%20instructions.%20Recent%20progress%20in%20large%20language%20models%20%28LLMs%29%20has%20enabled%20language-driven%20navigation%20with%20improved%20interpretability.%20However%2C%20most%20LLM-based%20agents%20still%20rely%20on%20single-shot%20action%20decisions%2C%20where%20the%20model%20must%20choose%20one%20option%20from%20noisy%2C%20textualized%20multi-perspective%20observations.%20Due%20to%20local%20mismatches%20and%20imperfect%20intermediate%20reasoning%2C%20such%20decisions%20can%20easily%20deviate%20from%20the%20correct%20path%2C%20leading%20to%20error%20accumulation%20and%20reduced%20reliability%20in%20unseen%20environments.%20In%20this%20paper%2C%20we%20propose%20DV-VLN%2C%20a%20new%20VLN%20framework%20that%20follows%20a%20generate-then-verify%20paradigm.%20DV-VLN%20first%20performs%20parameter-efficient%20in-domain%20adaptation%20of%20an%20open-source%20LLaMA-2%20backbone%20to%20produce%20a%20structured%20navigational%20chain-of-thought%2C%20and%20then%20verifies%20candidate%20actions%20with%20two%20complementary%20channels%3A%20True-False%20Verification%20%28TFV%29%20and%20Masked-Entity%20Verification%20%28MEV%29.%20DV-VLN%20selects%20actions%20by%20aggregating%20verification%20successes%20across%20multiple%20samples%2C%20yielding%20interpretable%20scores%20for%20reranking.%20Experiments%20on%20R2R%2C%20RxR%20%28English%20subset%29%2C%20and%20REVERIE%20show%20that%20DV-VLN%20consistently%20improves%20over%20direct%20prediction%20and%20sampling-only%20baselines%2C%20achieving%20competitive%20performance%20among%20language-only%20VLN%20agents%20and%20promising%20results%20compared%20with%20several%20cross-modal%20systems.Code%20is%20available%20at%20https%3A//github.com/PlumJun/DV-VLN.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18492v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDV-VLN%253A%2520Dual%2520Verification%2520for%2520Reliable%2520LLM-Based%2520Vision-and-Language%2520Navigation%26entry.906535625%3DZijun%2520Li%2520and%2520Shijie%2520Li%2520and%2520Zhenxi%2520Zhang%2520and%2520Bin%2520Li%2520and%2520Shoujun%2520Zhou%26entry.1292438233%3DVision-and-Language%2520Navigation%2520%2528VLN%2529%2520requires%2520an%2520embodied%2520agent%2520to%2520navigate%2520in%2520a%2520complex%25203D%2520environment%2520according%2520to%2520natural%2520language%2520instructions.%2520Recent%2520progress%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520enabled%2520language-driven%2520navigation%2520with%2520improved%2520interpretability.%2520However%252C%2520most%2520LLM-based%2520agents%2520still%2520rely%2520on%2520single-shot%2520action%2520decisions%252C%2520where%2520the%2520model%2520must%2520choose%2520one%2520option%2520from%2520noisy%252C%2520textualized%2520multi-perspective%2520observations.%2520Due%2520to%2520local%2520mismatches%2520and%2520imperfect%2520intermediate%2520reasoning%252C%2520such%2520decisions%2520can%2520easily%2520deviate%2520from%2520the%2520correct%2520path%252C%2520leading%2520to%2520error%2520accumulation%2520and%2520reduced%2520reliability%2520in%2520unseen%2520environments.%2520In%2520this%2520paper%252C%2520we%2520propose%2520DV-VLN%252C%2520a%2520new%2520VLN%2520framework%2520that%2520follows%2520a%2520generate-then-verify%2520paradigm.%2520DV-VLN%2520first%2520performs%2520parameter-efficient%2520in-domain%2520adaptation%2520of%2520an%2520open-source%2520LLaMA-2%2520backbone%2520to%2520produce%2520a%2520structured%2520navigational%2520chain-of-thought%252C%2520and%2520then%2520verifies%2520candidate%2520actions%2520with%2520two%2520complementary%2520channels%253A%2520True-False%2520Verification%2520%2528TFV%2529%2520and%2520Masked-Entity%2520Verification%2520%2528MEV%2529.%2520DV-VLN%2520selects%2520actions%2520by%2520aggregating%2520verification%2520successes%2520across%2520multiple%2520samples%252C%2520yielding%2520interpretable%2520scores%2520for%2520reranking.%2520Experiments%2520on%2520R2R%252C%2520RxR%2520%2528English%2520subset%2529%252C%2520and%2520REVERIE%2520show%2520that%2520DV-VLN%2520consistently%2520improves%2520over%2520direct%2520prediction%2520and%2520sampling-only%2520baselines%252C%2520achieving%2520competitive%2520performance%2520among%2520language-only%2520VLN%2520agents%2520and%2520promising%2520results%2520compared%2520with%2520several%2520cross-modal%2520systems.Code%2520is%2520available%2520at%2520https%253A//github.com/PlumJun/DV-VLN.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18492v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DV-VLN%3A%20Dual%20Verification%20for%20Reliable%20LLM-Based%20Vision-and-Language%20Navigation&entry.906535625=Zijun%20Li%20and%20Shijie%20Li%20and%20Zhenxi%20Zhang%20and%20Bin%20Li%20and%20Shoujun%20Zhou&entry.1292438233=Vision-and-Language%20Navigation%20%28VLN%29%20requires%20an%20embodied%20agent%20to%20navigate%20in%20a%20complex%203D%20environment%20according%20to%20natural%20language%20instructions.%20Recent%20progress%20in%20large%20language%20models%20%28LLMs%29%20has%20enabled%20language-driven%20navigation%20with%20improved%20interpretability.%20However%2C%20most%20LLM-based%20agents%20still%20rely%20on%20single-shot%20action%20decisions%2C%20where%20the%20model%20must%20choose%20one%20option%20from%20noisy%2C%20textualized%20multi-perspective%20observations.%20Due%20to%20local%20mismatches%20and%20imperfect%20intermediate%20reasoning%2C%20such%20decisions%20can%20easily%20deviate%20from%20the%20correct%20path%2C%20leading%20to%20error%20accumulation%20and%20reduced%20reliability%20in%20unseen%20environments.%20In%20this%20paper%2C%20we%20propose%20DV-VLN%2C%20a%20new%20VLN%20framework%20that%20follows%20a%20generate-then-verify%20paradigm.%20DV-VLN%20first%20performs%20parameter-efficient%20in-domain%20adaptation%20of%20an%20open-source%20LLaMA-2%20backbone%20to%20produce%20a%20structured%20navigational%20chain-of-thought%2C%20and%20then%20verifies%20candidate%20actions%20with%20two%20complementary%20channels%3A%20True-False%20Verification%20%28TFV%29%20and%20Masked-Entity%20Verification%20%28MEV%29.%20DV-VLN%20selects%20actions%20by%20aggregating%20verification%20successes%20across%20multiple%20samples%2C%20yielding%20interpretable%20scores%20for%20reranking.%20Experiments%20on%20R2R%2C%20RxR%20%28English%20subset%29%2C%20and%20REVERIE%20show%20that%20DV-VLN%20consistently%20improves%20over%20direct%20prediction%20and%20sampling-only%20baselines%2C%20achieving%20competitive%20performance%20among%20language-only%20VLN%20agents%20and%20promising%20results%20compared%20with%20several%20cross-modal%20systems.Code%20is%20available%20at%20https%3A//github.com/PlumJun/DV-VLN.&entry.1838667208=http%3A//arxiv.org/abs/2601.18492v1&entry.124074799=Read"},
{"title": "A Dataset for Automatic Vocal Mode Classification", "author": "Reemt Hinrichs and Sonja Stephan and Alexander Lange and J\u00f6rn Ostermann", "abstract": "The Complete Vocal Technique (CVT) is a school of singing developed in the past decades by Cathrin Sadolin et al.. CVT groups the use of the voice into so called vocal modes, namely Neutral, Curbing, Overdrive and Edge. Knowledge of the desired vocal mode can be helpful for singing students. Automatic classification of vocal modes can thus be important for technology-assisted singing teaching. Previously, automatic classification of vocal modes has been attempted without major success, potentially due to a lack of data. Therefore, we recorded a novel vocal mode dataset consisting of sustained vowels recorded from four singers, three of which professional singers with more than five years of CVT-experience. The dataset covers the entire vocal range of the subjects, totaling 3,752 unique samples. By using four microphones, thereby offering a natural data augmentation, the dataset consists of more than 13,000 samples combined. An annotation was created using three CVT-experienced annotators, each providing an individual annotation. The merged annotation as well as the three individual annotations come with the published dataset. Additionally, we provide some baseline classification results. The best balanced accuracy across a 5-fold cross validation of 81.3\\,\\% was achieved with a ResNet18. The dataset can be downloaded under https://zenodo.org/records/14276415.", "link": "http://arxiv.org/abs/2601.18339v1", "date": "2026-01-26", "relevancy": 2.223, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4702}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4419}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Dataset%20for%20Automatic%20Vocal%20Mode%20Classification&body=Title%3A%20A%20Dataset%20for%20Automatic%20Vocal%20Mode%20Classification%0AAuthor%3A%20Reemt%20Hinrichs%20and%20Sonja%20Stephan%20and%20Alexander%20Lange%20and%20J%C3%B6rn%20Ostermann%0AAbstract%3A%20The%20Complete%20Vocal%20Technique%20%28CVT%29%20is%20a%20school%20of%20singing%20developed%20in%20the%20past%20decades%20by%20Cathrin%20Sadolin%20et%20al..%20CVT%20groups%20the%20use%20of%20the%20voice%20into%20so%20called%20vocal%20modes%2C%20namely%20Neutral%2C%20Curbing%2C%20Overdrive%20and%20Edge.%20Knowledge%20of%20the%20desired%20vocal%20mode%20can%20be%20helpful%20for%20singing%20students.%20Automatic%20classification%20of%20vocal%20modes%20can%20thus%20be%20important%20for%20technology-assisted%20singing%20teaching.%20Previously%2C%20automatic%20classification%20of%20vocal%20modes%20has%20been%20attempted%20without%20major%20success%2C%20potentially%20due%20to%20a%20lack%20of%20data.%20Therefore%2C%20we%20recorded%20a%20novel%20vocal%20mode%20dataset%20consisting%20of%20sustained%20vowels%20recorded%20from%20four%20singers%2C%20three%20of%20which%20professional%20singers%20with%20more%20than%20five%20years%20of%20CVT-experience.%20The%20dataset%20covers%20the%20entire%20vocal%20range%20of%20the%20subjects%2C%20totaling%203%2C752%20unique%20samples.%20By%20using%20four%20microphones%2C%20thereby%20offering%20a%20natural%20data%20augmentation%2C%20the%20dataset%20consists%20of%20more%20than%2013%2C000%20samples%20combined.%20An%20annotation%20was%20created%20using%20three%20CVT-experienced%20annotators%2C%20each%20providing%20an%20individual%20annotation.%20The%20merged%20annotation%20as%20well%20as%20the%20three%20individual%20annotations%20come%20with%20the%20published%20dataset.%20Additionally%2C%20we%20provide%20some%20baseline%20classification%20results.%20The%20best%20balanced%20accuracy%20across%20a%205-fold%20cross%20validation%20of%2081.3%5C%2C%5C%25%20was%20achieved%20with%20a%20ResNet18.%20The%20dataset%20can%20be%20downloaded%20under%20https%3A//zenodo.org/records/14276415.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18339v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Dataset%2520for%2520Automatic%2520Vocal%2520Mode%2520Classification%26entry.906535625%3DReemt%2520Hinrichs%2520and%2520Sonja%2520Stephan%2520and%2520Alexander%2520Lange%2520and%2520J%25C3%25B6rn%2520Ostermann%26entry.1292438233%3DThe%2520Complete%2520Vocal%2520Technique%2520%2528CVT%2529%2520is%2520a%2520school%2520of%2520singing%2520developed%2520in%2520the%2520past%2520decades%2520by%2520Cathrin%2520Sadolin%2520et%2520al..%2520CVT%2520groups%2520the%2520use%2520of%2520the%2520voice%2520into%2520so%2520called%2520vocal%2520modes%252C%2520namely%2520Neutral%252C%2520Curbing%252C%2520Overdrive%2520and%2520Edge.%2520Knowledge%2520of%2520the%2520desired%2520vocal%2520mode%2520can%2520be%2520helpful%2520for%2520singing%2520students.%2520Automatic%2520classification%2520of%2520vocal%2520modes%2520can%2520thus%2520be%2520important%2520for%2520technology-assisted%2520singing%2520teaching.%2520Previously%252C%2520automatic%2520classification%2520of%2520vocal%2520modes%2520has%2520been%2520attempted%2520without%2520major%2520success%252C%2520potentially%2520due%2520to%2520a%2520lack%2520of%2520data.%2520Therefore%252C%2520we%2520recorded%2520a%2520novel%2520vocal%2520mode%2520dataset%2520consisting%2520of%2520sustained%2520vowels%2520recorded%2520from%2520four%2520singers%252C%2520three%2520of%2520which%2520professional%2520singers%2520with%2520more%2520than%2520five%2520years%2520of%2520CVT-experience.%2520The%2520dataset%2520covers%2520the%2520entire%2520vocal%2520range%2520of%2520the%2520subjects%252C%2520totaling%25203%252C752%2520unique%2520samples.%2520By%2520using%2520four%2520microphones%252C%2520thereby%2520offering%2520a%2520natural%2520data%2520augmentation%252C%2520the%2520dataset%2520consists%2520of%2520more%2520than%252013%252C000%2520samples%2520combined.%2520An%2520annotation%2520was%2520created%2520using%2520three%2520CVT-experienced%2520annotators%252C%2520each%2520providing%2520an%2520individual%2520annotation.%2520The%2520merged%2520annotation%2520as%2520well%2520as%2520the%2520three%2520individual%2520annotations%2520come%2520with%2520the%2520published%2520dataset.%2520Additionally%252C%2520we%2520provide%2520some%2520baseline%2520classification%2520results.%2520The%2520best%2520balanced%2520accuracy%2520across%2520a%25205-fold%2520cross%2520validation%2520of%252081.3%255C%252C%255C%2525%2520was%2520achieved%2520with%2520a%2520ResNet18.%2520The%2520dataset%2520can%2520be%2520downloaded%2520under%2520https%253A//zenodo.org/records/14276415.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18339v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Dataset%20for%20Automatic%20Vocal%20Mode%20Classification&entry.906535625=Reemt%20Hinrichs%20and%20Sonja%20Stephan%20and%20Alexander%20Lange%20and%20J%C3%B6rn%20Ostermann&entry.1292438233=The%20Complete%20Vocal%20Technique%20%28CVT%29%20is%20a%20school%20of%20singing%20developed%20in%20the%20past%20decades%20by%20Cathrin%20Sadolin%20et%20al..%20CVT%20groups%20the%20use%20of%20the%20voice%20into%20so%20called%20vocal%20modes%2C%20namely%20Neutral%2C%20Curbing%2C%20Overdrive%20and%20Edge.%20Knowledge%20of%20the%20desired%20vocal%20mode%20can%20be%20helpful%20for%20singing%20students.%20Automatic%20classification%20of%20vocal%20modes%20can%20thus%20be%20important%20for%20technology-assisted%20singing%20teaching.%20Previously%2C%20automatic%20classification%20of%20vocal%20modes%20has%20been%20attempted%20without%20major%20success%2C%20potentially%20due%20to%20a%20lack%20of%20data.%20Therefore%2C%20we%20recorded%20a%20novel%20vocal%20mode%20dataset%20consisting%20of%20sustained%20vowels%20recorded%20from%20four%20singers%2C%20three%20of%20which%20professional%20singers%20with%20more%20than%20five%20years%20of%20CVT-experience.%20The%20dataset%20covers%20the%20entire%20vocal%20range%20of%20the%20subjects%2C%20totaling%203%2C752%20unique%20samples.%20By%20using%20four%20microphones%2C%20thereby%20offering%20a%20natural%20data%20augmentation%2C%20the%20dataset%20consists%20of%20more%20than%2013%2C000%20samples%20combined.%20An%20annotation%20was%20created%20using%20three%20CVT-experienced%20annotators%2C%20each%20providing%20an%20individual%20annotation.%20The%20merged%20annotation%20as%20well%20as%20the%20three%20individual%20annotations%20come%20with%20the%20published%20dataset.%20Additionally%2C%20we%20provide%20some%20baseline%20classification%20results.%20The%20best%20balanced%20accuracy%20across%20a%205-fold%20cross%20validation%20of%2081.3%5C%2C%5C%25%20was%20achieved%20with%20a%20ResNet18.%20The%20dataset%20can%20be%20downloaded%20under%20https%3A//zenodo.org/records/14276415.&entry.1838667208=http%3A//arxiv.org/abs/2601.18339v1&entry.124074799=Read"},
{"title": "Real-Time Object Detection Meets DINOv3", "author": "Shihua Huang and Yongjie Hou and Longfei Liu and Xuanlong Yu and Xi Shen", "abstract": "Driven by the simple and effective Dense O2O, DEIM demonstrates faster convergence and enhanced performance. In this work, we extend it with DINOv3 features, resulting in DEIMv2. DEIMv2 spans eight model sizes from X to Atto, covering GPU, edge, and mobile deployment. For the X, L, M, and S variants, we adopt DINOv3-pretrained or distilled backbones and introduce a Spatial Tuning Adapter (STA), which efficiently converts DINOv3's single-scale output into multi-scale features and complements strong semantics with fine-grained details to enhance detection. For ultra-lightweight models (Nano, Pico, Femto, and Atto), we employ HGNetv2 with depth and width pruning to meet strict resource budgets. Together with a simplified decoder and an upgraded Dense O2O, this unified design enables DEIMv2 to achieve a superior performance-cost trade-off across diverse scenarios, establishing new state-of-the-art results. Notably, our largest model, DEIMv2-X, achieves 57.8 AP with only 50.3 million parameters, surpassing prior X-scale models that require over 60 million parameters for just 56.5 AP. On the compact side, DEIMv2-S is the first sub-10 million model (9.71 million) to exceed the 50 AP milestone on COCO, reaching 50.9 AP. Even the ultra-lightweight DEIMv2-Pico, with just 1.5 million parameters, delivers 38.5 AP, matching YOLOv10-Nano (2.3 million) with around 50 percent fewer parameters. Our code and pre-trained models are available at https://github.com/Intellindust-AI-Lab/DEIMv2", "link": "http://arxiv.org/abs/2509.20787v4", "date": "2026-01-26", "relevancy": 2.2192, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5744}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.556}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real-Time%20Object%20Detection%20Meets%20DINOv3&body=Title%3A%20Real-Time%20Object%20Detection%20Meets%20DINOv3%0AAuthor%3A%20Shihua%20Huang%20and%20Yongjie%20Hou%20and%20Longfei%20Liu%20and%20Xuanlong%20Yu%20and%20Xi%20Shen%0AAbstract%3A%20Driven%20by%20the%20simple%20and%20effective%20Dense%20O2O%2C%20DEIM%20demonstrates%20faster%20convergence%20and%20enhanced%20performance.%20In%20this%20work%2C%20we%20extend%20it%20with%20DINOv3%20features%2C%20resulting%20in%20DEIMv2.%20DEIMv2%20spans%20eight%20model%20sizes%20from%20X%20to%20Atto%2C%20covering%20GPU%2C%20edge%2C%20and%20mobile%20deployment.%20For%20the%20X%2C%20L%2C%20M%2C%20and%20S%20variants%2C%20we%20adopt%20DINOv3-pretrained%20or%20distilled%20backbones%20and%20introduce%20a%20Spatial%20Tuning%20Adapter%20%28STA%29%2C%20which%20efficiently%20converts%20DINOv3%27s%20single-scale%20output%20into%20multi-scale%20features%20and%20complements%20strong%20semantics%20with%20fine-grained%20details%20to%20enhance%20detection.%20For%20ultra-lightweight%20models%20%28Nano%2C%20Pico%2C%20Femto%2C%20and%20Atto%29%2C%20we%20employ%20HGNetv2%20with%20depth%20and%20width%20pruning%20to%20meet%20strict%20resource%20budgets.%20Together%20with%20a%20simplified%20decoder%20and%20an%20upgraded%20Dense%20O2O%2C%20this%20unified%20design%20enables%20DEIMv2%20to%20achieve%20a%20superior%20performance-cost%20trade-off%20across%20diverse%20scenarios%2C%20establishing%20new%20state-of-the-art%20results.%20Notably%2C%20our%20largest%20model%2C%20DEIMv2-X%2C%20achieves%2057.8%20AP%20with%20only%2050.3%20million%20parameters%2C%20surpassing%20prior%20X-scale%20models%20that%20require%20over%2060%20million%20parameters%20for%20just%2056.5%20AP.%20On%20the%20compact%20side%2C%20DEIMv2-S%20is%20the%20first%20sub-10%20million%20model%20%289.71%20million%29%20to%20exceed%20the%2050%20AP%20milestone%20on%20COCO%2C%20reaching%2050.9%20AP.%20Even%20the%20ultra-lightweight%20DEIMv2-Pico%2C%20with%20just%201.5%20million%20parameters%2C%20delivers%2038.5%20AP%2C%20matching%20YOLOv10-Nano%20%282.3%20million%29%20with%20around%2050%20percent%20fewer%20parameters.%20Our%20code%20and%20pre-trained%20models%20are%20available%20at%20https%3A//github.com/Intellindust-AI-Lab/DEIMv2%0ALink%3A%20http%3A//arxiv.org/abs/2509.20787v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal-Time%2520Object%2520Detection%2520Meets%2520DINOv3%26entry.906535625%3DShihua%2520Huang%2520and%2520Yongjie%2520Hou%2520and%2520Longfei%2520Liu%2520and%2520Xuanlong%2520Yu%2520and%2520Xi%2520Shen%26entry.1292438233%3DDriven%2520by%2520the%2520simple%2520and%2520effective%2520Dense%2520O2O%252C%2520DEIM%2520demonstrates%2520faster%2520convergence%2520and%2520enhanced%2520performance.%2520In%2520this%2520work%252C%2520we%2520extend%2520it%2520with%2520DINOv3%2520features%252C%2520resulting%2520in%2520DEIMv2.%2520DEIMv2%2520spans%2520eight%2520model%2520sizes%2520from%2520X%2520to%2520Atto%252C%2520covering%2520GPU%252C%2520edge%252C%2520and%2520mobile%2520deployment.%2520For%2520the%2520X%252C%2520L%252C%2520M%252C%2520and%2520S%2520variants%252C%2520we%2520adopt%2520DINOv3-pretrained%2520or%2520distilled%2520backbones%2520and%2520introduce%2520a%2520Spatial%2520Tuning%2520Adapter%2520%2528STA%2529%252C%2520which%2520efficiently%2520converts%2520DINOv3%2527s%2520single-scale%2520output%2520into%2520multi-scale%2520features%2520and%2520complements%2520strong%2520semantics%2520with%2520fine-grained%2520details%2520to%2520enhance%2520detection.%2520For%2520ultra-lightweight%2520models%2520%2528Nano%252C%2520Pico%252C%2520Femto%252C%2520and%2520Atto%2529%252C%2520we%2520employ%2520HGNetv2%2520with%2520depth%2520and%2520width%2520pruning%2520to%2520meet%2520strict%2520resource%2520budgets.%2520Together%2520with%2520a%2520simplified%2520decoder%2520and%2520an%2520upgraded%2520Dense%2520O2O%252C%2520this%2520unified%2520design%2520enables%2520DEIMv2%2520to%2520achieve%2520a%2520superior%2520performance-cost%2520trade-off%2520across%2520diverse%2520scenarios%252C%2520establishing%2520new%2520state-of-the-art%2520results.%2520Notably%252C%2520our%2520largest%2520model%252C%2520DEIMv2-X%252C%2520achieves%252057.8%2520AP%2520with%2520only%252050.3%2520million%2520parameters%252C%2520surpassing%2520prior%2520X-scale%2520models%2520that%2520require%2520over%252060%2520million%2520parameters%2520for%2520just%252056.5%2520AP.%2520On%2520the%2520compact%2520side%252C%2520DEIMv2-S%2520is%2520the%2520first%2520sub-10%2520million%2520model%2520%25289.71%2520million%2529%2520to%2520exceed%2520the%252050%2520AP%2520milestone%2520on%2520COCO%252C%2520reaching%252050.9%2520AP.%2520Even%2520the%2520ultra-lightweight%2520DEIMv2-Pico%252C%2520with%2520just%25201.5%2520million%2520parameters%252C%2520delivers%252038.5%2520AP%252C%2520matching%2520YOLOv10-Nano%2520%25282.3%2520million%2529%2520with%2520around%252050%2520percent%2520fewer%2520parameters.%2520Our%2520code%2520and%2520pre-trained%2520models%2520are%2520available%2520at%2520https%253A//github.com/Intellindust-AI-Lab/DEIMv2%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.20787v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-Time%20Object%20Detection%20Meets%20DINOv3&entry.906535625=Shihua%20Huang%20and%20Yongjie%20Hou%20and%20Longfei%20Liu%20and%20Xuanlong%20Yu%20and%20Xi%20Shen&entry.1292438233=Driven%20by%20the%20simple%20and%20effective%20Dense%20O2O%2C%20DEIM%20demonstrates%20faster%20convergence%20and%20enhanced%20performance.%20In%20this%20work%2C%20we%20extend%20it%20with%20DINOv3%20features%2C%20resulting%20in%20DEIMv2.%20DEIMv2%20spans%20eight%20model%20sizes%20from%20X%20to%20Atto%2C%20covering%20GPU%2C%20edge%2C%20and%20mobile%20deployment.%20For%20the%20X%2C%20L%2C%20M%2C%20and%20S%20variants%2C%20we%20adopt%20DINOv3-pretrained%20or%20distilled%20backbones%20and%20introduce%20a%20Spatial%20Tuning%20Adapter%20%28STA%29%2C%20which%20efficiently%20converts%20DINOv3%27s%20single-scale%20output%20into%20multi-scale%20features%20and%20complements%20strong%20semantics%20with%20fine-grained%20details%20to%20enhance%20detection.%20For%20ultra-lightweight%20models%20%28Nano%2C%20Pico%2C%20Femto%2C%20and%20Atto%29%2C%20we%20employ%20HGNetv2%20with%20depth%20and%20width%20pruning%20to%20meet%20strict%20resource%20budgets.%20Together%20with%20a%20simplified%20decoder%20and%20an%20upgraded%20Dense%20O2O%2C%20this%20unified%20design%20enables%20DEIMv2%20to%20achieve%20a%20superior%20performance-cost%20trade-off%20across%20diverse%20scenarios%2C%20establishing%20new%20state-of-the-art%20results.%20Notably%2C%20our%20largest%20model%2C%20DEIMv2-X%2C%20achieves%2057.8%20AP%20with%20only%2050.3%20million%20parameters%2C%20surpassing%20prior%20X-scale%20models%20that%20require%20over%2060%20million%20parameters%20for%20just%2056.5%20AP.%20On%20the%20compact%20side%2C%20DEIMv2-S%20is%20the%20first%20sub-10%20million%20model%20%289.71%20million%29%20to%20exceed%20the%2050%20AP%20milestone%20on%20COCO%2C%20reaching%2050.9%20AP.%20Even%20the%20ultra-lightweight%20DEIMv2-Pico%2C%20with%20just%201.5%20million%20parameters%2C%20delivers%2038.5%20AP%2C%20matching%20YOLOv10-Nano%20%282.3%20million%29%20with%20around%2050%20percent%20fewer%20parameters.%20Our%20code%20and%20pre-trained%20models%20are%20available%20at%20https%3A//github.com/Intellindust-AI-Lab/DEIMv2&entry.1838667208=http%3A//arxiv.org/abs/2509.20787v4&entry.124074799=Read"},
{"title": "SG-CADVLM: A Context-Aware Decoding Powered Vision Language Model for Safety-Critical Scenario Generation", "author": "Hongyi Zhao and Shuo Wang and Qijie He and Ziyuan Pu", "abstract": "Autonomous vehicle safety validation requires testing on safety-critical scenarios, but these events are rare in real-world driving and costly to test due to collision risks. Crash reports provide authentic specifications of safety-critical events, offering a vital alternative to scarce real-world collision trajectory data. This makes them valuable sources for generating realistic high-risk scenarios through simulation. Existing approaches face significant limitations because data-driven methods lack diversity due to their reliance on existing latent distributions, whereas adversarial methods often produce unrealistic scenarios lacking physical fidelity. Large Language Model (LLM) and Vision Language Model (VLM)-based methods show significant promise. However, they suffer from context suppression issues where internal parametric knowledge overrides crash specifications, producing scenarios that deviate from actual accident characteristics. This paper presents SG-CADVLM (A Context-Aware Decoding Powered Vision Language Model for Safety-Critical Scenario Generation), a framework that integrates Context-Aware Decoding with multi-modal input processing to generate safety-critical scenarios from crash reports and road network diagrams. The framework mitigates VLM hallucination issues while enabling the simultaneous generation of road geometry and vehicle trajectories. The experimental results demonstrate that SG-CADVLM generates critical risk scenarios at a rate of 84.4% compared to 12.5% for the baseline methods, representing an improvement of 469%, while producing executable simulations for autonomous vehicle testing.", "link": "http://arxiv.org/abs/2601.18442v1", "date": "2026-01-26", "relevancy": 2.212, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5574}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5574}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SG-CADVLM%3A%20A%20Context-Aware%20Decoding%20Powered%20Vision%20Language%20Model%20for%20Safety-Critical%20Scenario%20Generation&body=Title%3A%20SG-CADVLM%3A%20A%20Context-Aware%20Decoding%20Powered%20Vision%20Language%20Model%20for%20Safety-Critical%20Scenario%20Generation%0AAuthor%3A%20Hongyi%20Zhao%20and%20Shuo%20Wang%20and%20Qijie%20He%20and%20Ziyuan%20Pu%0AAbstract%3A%20Autonomous%20vehicle%20safety%20validation%20requires%20testing%20on%20safety-critical%20scenarios%2C%20but%20these%20events%20are%20rare%20in%20real-world%20driving%20and%20costly%20to%20test%20due%20to%20collision%20risks.%20Crash%20reports%20provide%20authentic%20specifications%20of%20safety-critical%20events%2C%20offering%20a%20vital%20alternative%20to%20scarce%20real-world%20collision%20trajectory%20data.%20This%20makes%20them%20valuable%20sources%20for%20generating%20realistic%20high-risk%20scenarios%20through%20simulation.%20Existing%20approaches%20face%20significant%20limitations%20because%20data-driven%20methods%20lack%20diversity%20due%20to%20their%20reliance%20on%20existing%20latent%20distributions%2C%20whereas%20adversarial%20methods%20often%20produce%20unrealistic%20scenarios%20lacking%20physical%20fidelity.%20Large%20Language%20Model%20%28LLM%29%20and%20Vision%20Language%20Model%20%28VLM%29-based%20methods%20show%20significant%20promise.%20However%2C%20they%20suffer%20from%20context%20suppression%20issues%20where%20internal%20parametric%20knowledge%20overrides%20crash%20specifications%2C%20producing%20scenarios%20that%20deviate%20from%20actual%20accident%20characteristics.%20This%20paper%20presents%20SG-CADVLM%20%28A%20Context-Aware%20Decoding%20Powered%20Vision%20Language%20Model%20for%20Safety-Critical%20Scenario%20Generation%29%2C%20a%20framework%20that%20integrates%20Context-Aware%20Decoding%20with%20multi-modal%20input%20processing%20to%20generate%20safety-critical%20scenarios%20from%20crash%20reports%20and%20road%20network%20diagrams.%20The%20framework%20mitigates%20VLM%20hallucination%20issues%20while%20enabling%20the%20simultaneous%20generation%20of%20road%20geometry%20and%20vehicle%20trajectories.%20The%20experimental%20results%20demonstrate%20that%20SG-CADVLM%20generates%20critical%20risk%20scenarios%20at%20a%20rate%20of%2084.4%25%20compared%20to%2012.5%25%20for%20the%20baseline%20methods%2C%20representing%20an%20improvement%20of%20469%25%2C%20while%20producing%20executable%20simulations%20for%20autonomous%20vehicle%20testing.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18442v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSG-CADVLM%253A%2520A%2520Context-Aware%2520Decoding%2520Powered%2520Vision%2520Language%2520Model%2520for%2520Safety-Critical%2520Scenario%2520Generation%26entry.906535625%3DHongyi%2520Zhao%2520and%2520Shuo%2520Wang%2520and%2520Qijie%2520He%2520and%2520Ziyuan%2520Pu%26entry.1292438233%3DAutonomous%2520vehicle%2520safety%2520validation%2520requires%2520testing%2520on%2520safety-critical%2520scenarios%252C%2520but%2520these%2520events%2520are%2520rare%2520in%2520real-world%2520driving%2520and%2520costly%2520to%2520test%2520due%2520to%2520collision%2520risks.%2520Crash%2520reports%2520provide%2520authentic%2520specifications%2520of%2520safety-critical%2520events%252C%2520offering%2520a%2520vital%2520alternative%2520to%2520scarce%2520real-world%2520collision%2520trajectory%2520data.%2520This%2520makes%2520them%2520valuable%2520sources%2520for%2520generating%2520realistic%2520high-risk%2520scenarios%2520through%2520simulation.%2520Existing%2520approaches%2520face%2520significant%2520limitations%2520because%2520data-driven%2520methods%2520lack%2520diversity%2520due%2520to%2520their%2520reliance%2520on%2520existing%2520latent%2520distributions%252C%2520whereas%2520adversarial%2520methods%2520often%2520produce%2520unrealistic%2520scenarios%2520lacking%2520physical%2520fidelity.%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520and%2520Vision%2520Language%2520Model%2520%2528VLM%2529-based%2520methods%2520show%2520significant%2520promise.%2520However%252C%2520they%2520suffer%2520from%2520context%2520suppression%2520issues%2520where%2520internal%2520parametric%2520knowledge%2520overrides%2520crash%2520specifications%252C%2520producing%2520scenarios%2520that%2520deviate%2520from%2520actual%2520accident%2520characteristics.%2520This%2520paper%2520presents%2520SG-CADVLM%2520%2528A%2520Context-Aware%2520Decoding%2520Powered%2520Vision%2520Language%2520Model%2520for%2520Safety-Critical%2520Scenario%2520Generation%2529%252C%2520a%2520framework%2520that%2520integrates%2520Context-Aware%2520Decoding%2520with%2520multi-modal%2520input%2520processing%2520to%2520generate%2520safety-critical%2520scenarios%2520from%2520crash%2520reports%2520and%2520road%2520network%2520diagrams.%2520The%2520framework%2520mitigates%2520VLM%2520hallucination%2520issues%2520while%2520enabling%2520the%2520simultaneous%2520generation%2520of%2520road%2520geometry%2520and%2520vehicle%2520trajectories.%2520The%2520experimental%2520results%2520demonstrate%2520that%2520SG-CADVLM%2520generates%2520critical%2520risk%2520scenarios%2520at%2520a%2520rate%2520of%252084.4%2525%2520compared%2520to%252012.5%2525%2520for%2520the%2520baseline%2520methods%252C%2520representing%2520an%2520improvement%2520of%2520469%2525%252C%2520while%2520producing%2520executable%2520simulations%2520for%2520autonomous%2520vehicle%2520testing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18442v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SG-CADVLM%3A%20A%20Context-Aware%20Decoding%20Powered%20Vision%20Language%20Model%20for%20Safety-Critical%20Scenario%20Generation&entry.906535625=Hongyi%20Zhao%20and%20Shuo%20Wang%20and%20Qijie%20He%20and%20Ziyuan%20Pu&entry.1292438233=Autonomous%20vehicle%20safety%20validation%20requires%20testing%20on%20safety-critical%20scenarios%2C%20but%20these%20events%20are%20rare%20in%20real-world%20driving%20and%20costly%20to%20test%20due%20to%20collision%20risks.%20Crash%20reports%20provide%20authentic%20specifications%20of%20safety-critical%20events%2C%20offering%20a%20vital%20alternative%20to%20scarce%20real-world%20collision%20trajectory%20data.%20This%20makes%20them%20valuable%20sources%20for%20generating%20realistic%20high-risk%20scenarios%20through%20simulation.%20Existing%20approaches%20face%20significant%20limitations%20because%20data-driven%20methods%20lack%20diversity%20due%20to%20their%20reliance%20on%20existing%20latent%20distributions%2C%20whereas%20adversarial%20methods%20often%20produce%20unrealistic%20scenarios%20lacking%20physical%20fidelity.%20Large%20Language%20Model%20%28LLM%29%20and%20Vision%20Language%20Model%20%28VLM%29-based%20methods%20show%20significant%20promise.%20However%2C%20they%20suffer%20from%20context%20suppression%20issues%20where%20internal%20parametric%20knowledge%20overrides%20crash%20specifications%2C%20producing%20scenarios%20that%20deviate%20from%20actual%20accident%20characteristics.%20This%20paper%20presents%20SG-CADVLM%20%28A%20Context-Aware%20Decoding%20Powered%20Vision%20Language%20Model%20for%20Safety-Critical%20Scenario%20Generation%29%2C%20a%20framework%20that%20integrates%20Context-Aware%20Decoding%20with%20multi-modal%20input%20processing%20to%20generate%20safety-critical%20scenarios%20from%20crash%20reports%20and%20road%20network%20diagrams.%20The%20framework%20mitigates%20VLM%20hallucination%20issues%20while%20enabling%20the%20simultaneous%20generation%20of%20road%20geometry%20and%20vehicle%20trajectories.%20The%20experimental%20results%20demonstrate%20that%20SG-CADVLM%20generates%20critical%20risk%20scenarios%20at%20a%20rate%20of%2084.4%25%20compared%20to%2012.5%25%20for%20the%20baseline%20methods%2C%20representing%20an%20improvement%20of%20469%25%2C%20while%20producing%20executable%20simulations%20for%20autonomous%20vehicle%20testing.&entry.1838667208=http%3A//arxiv.org/abs/2601.18442v1&entry.124074799=Read"},
{"title": "Why Keep Your Doubts to Yourself? Trading Visual Uncertainties in Multi-Agent Bandit Systems", "author": "Jusheng Zhang and Yijia Fan and Kaitong Cai and Jing Yang and Jiawei Yao and Jian Wang and Guanlong Qu and Ziliang Chen and Keze Wang", "abstract": "Vision-Language Models (VLMs) enable powerful multi-agent systems, but scaling them is economically unsustainable: coordinating heterogeneous agents under information asymmetry often spirals costs. Existing paradigms, such as Mixture-of-Agents and knowledge-based routers, rely on heuristic proxies that ignore costs and collapse uncertainty structure, leading to provably suboptimal coordination. We introduce Agora, a framework that reframes coordination as a decentralized market for uncertainty. Agora formalizes epistemic uncertainty into a structured, tradable asset (perceptual, semantic, inferential), and enforces profitability-driven trading among agents based on rational economic rules. A market-aware broker, extending Thompson Sampling, initiates collaboration and guides the system toward cost-efficient equilibria. Experiments on five multimodal benchmarks (MMMU, MMBench, MathVision, InfoVQA, CC-OCR) show that Agora outperforms strong VLMs and heuristic multi-agent strategies, e.g., achieving +8.5% accuracy over the best baseline on MMMU while reducing cost by over 3x. These results establish market-based coordination as a principled and scalable paradigm for building economically viable multi-agent visual intelligence systems.", "link": "http://arxiv.org/abs/2601.18735v1", "date": "2026-01-26", "relevancy": 2.2069, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5817}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5581}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Why%20Keep%20Your%20Doubts%20to%20Yourself%3F%20Trading%20Visual%20Uncertainties%20in%20Multi-Agent%20Bandit%20Systems&body=Title%3A%20Why%20Keep%20Your%20Doubts%20to%20Yourself%3F%20Trading%20Visual%20Uncertainties%20in%20Multi-Agent%20Bandit%20Systems%0AAuthor%3A%20Jusheng%20Zhang%20and%20Yijia%20Fan%20and%20Kaitong%20Cai%20and%20Jing%20Yang%20and%20Jiawei%20Yao%20and%20Jian%20Wang%20and%20Guanlong%20Qu%20and%20Ziliang%20Chen%20and%20Keze%20Wang%0AAbstract%3A%20Vision-Language%20Models%20%28VLMs%29%20enable%20powerful%20multi-agent%20systems%2C%20but%20scaling%20them%20is%20economically%20unsustainable%3A%20coordinating%20heterogeneous%20agents%20under%20information%20asymmetry%20often%20spirals%20costs.%20Existing%20paradigms%2C%20such%20as%20Mixture-of-Agents%20and%20knowledge-based%20routers%2C%20rely%20on%20heuristic%20proxies%20that%20ignore%20costs%20and%20collapse%20uncertainty%20structure%2C%20leading%20to%20provably%20suboptimal%20coordination.%20We%20introduce%20Agora%2C%20a%20framework%20that%20reframes%20coordination%20as%20a%20decentralized%20market%20for%20uncertainty.%20Agora%20formalizes%20epistemic%20uncertainty%20into%20a%20structured%2C%20tradable%20asset%20%28perceptual%2C%20semantic%2C%20inferential%29%2C%20and%20enforces%20profitability-driven%20trading%20among%20agents%20based%20on%20rational%20economic%20rules.%20A%20market-aware%20broker%2C%20extending%20Thompson%20Sampling%2C%20initiates%20collaboration%20and%20guides%20the%20system%20toward%20cost-efficient%20equilibria.%20Experiments%20on%20five%20multimodal%20benchmarks%20%28MMMU%2C%20MMBench%2C%20MathVision%2C%20InfoVQA%2C%20CC-OCR%29%20show%20that%20Agora%20outperforms%20strong%20VLMs%20and%20heuristic%20multi-agent%20strategies%2C%20e.g.%2C%20achieving%20%2B8.5%25%20accuracy%20over%20the%20best%20baseline%20on%20MMMU%20while%20reducing%20cost%20by%20over%203x.%20These%20results%20establish%20market-based%20coordination%20as%20a%20principled%20and%20scalable%20paradigm%20for%20building%20economically%20viable%20multi-agent%20visual%20intelligence%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18735v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhy%2520Keep%2520Your%2520Doubts%2520to%2520Yourself%253F%2520Trading%2520Visual%2520Uncertainties%2520in%2520Multi-Agent%2520Bandit%2520Systems%26entry.906535625%3DJusheng%2520Zhang%2520and%2520Yijia%2520Fan%2520and%2520Kaitong%2520Cai%2520and%2520Jing%2520Yang%2520and%2520Jiawei%2520Yao%2520and%2520Jian%2520Wang%2520and%2520Guanlong%2520Qu%2520and%2520Ziliang%2520Chen%2520and%2520Keze%2520Wang%26entry.1292438233%3DVision-Language%2520Models%2520%2528VLMs%2529%2520enable%2520powerful%2520multi-agent%2520systems%252C%2520but%2520scaling%2520them%2520is%2520economically%2520unsustainable%253A%2520coordinating%2520heterogeneous%2520agents%2520under%2520information%2520asymmetry%2520often%2520spirals%2520costs.%2520Existing%2520paradigms%252C%2520such%2520as%2520Mixture-of-Agents%2520and%2520knowledge-based%2520routers%252C%2520rely%2520on%2520heuristic%2520proxies%2520that%2520ignore%2520costs%2520and%2520collapse%2520uncertainty%2520structure%252C%2520leading%2520to%2520provably%2520suboptimal%2520coordination.%2520We%2520introduce%2520Agora%252C%2520a%2520framework%2520that%2520reframes%2520coordination%2520as%2520a%2520decentralized%2520market%2520for%2520uncertainty.%2520Agora%2520formalizes%2520epistemic%2520uncertainty%2520into%2520a%2520structured%252C%2520tradable%2520asset%2520%2528perceptual%252C%2520semantic%252C%2520inferential%2529%252C%2520and%2520enforces%2520profitability-driven%2520trading%2520among%2520agents%2520based%2520on%2520rational%2520economic%2520rules.%2520A%2520market-aware%2520broker%252C%2520extending%2520Thompson%2520Sampling%252C%2520initiates%2520collaboration%2520and%2520guides%2520the%2520system%2520toward%2520cost-efficient%2520equilibria.%2520Experiments%2520on%2520five%2520multimodal%2520benchmarks%2520%2528MMMU%252C%2520MMBench%252C%2520MathVision%252C%2520InfoVQA%252C%2520CC-OCR%2529%2520show%2520that%2520Agora%2520outperforms%2520strong%2520VLMs%2520and%2520heuristic%2520multi-agent%2520strategies%252C%2520e.g.%252C%2520achieving%2520%252B8.5%2525%2520accuracy%2520over%2520the%2520best%2520baseline%2520on%2520MMMU%2520while%2520reducing%2520cost%2520by%2520over%25203x.%2520These%2520results%2520establish%2520market-based%2520coordination%2520as%2520a%2520principled%2520and%2520scalable%2520paradigm%2520for%2520building%2520economically%2520viable%2520multi-agent%2520visual%2520intelligence%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18735v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Why%20Keep%20Your%20Doubts%20to%20Yourself%3F%20Trading%20Visual%20Uncertainties%20in%20Multi-Agent%20Bandit%20Systems&entry.906535625=Jusheng%20Zhang%20and%20Yijia%20Fan%20and%20Kaitong%20Cai%20and%20Jing%20Yang%20and%20Jiawei%20Yao%20and%20Jian%20Wang%20and%20Guanlong%20Qu%20and%20Ziliang%20Chen%20and%20Keze%20Wang&entry.1292438233=Vision-Language%20Models%20%28VLMs%29%20enable%20powerful%20multi-agent%20systems%2C%20but%20scaling%20them%20is%20economically%20unsustainable%3A%20coordinating%20heterogeneous%20agents%20under%20information%20asymmetry%20often%20spirals%20costs.%20Existing%20paradigms%2C%20such%20as%20Mixture-of-Agents%20and%20knowledge-based%20routers%2C%20rely%20on%20heuristic%20proxies%20that%20ignore%20costs%20and%20collapse%20uncertainty%20structure%2C%20leading%20to%20provably%20suboptimal%20coordination.%20We%20introduce%20Agora%2C%20a%20framework%20that%20reframes%20coordination%20as%20a%20decentralized%20market%20for%20uncertainty.%20Agora%20formalizes%20epistemic%20uncertainty%20into%20a%20structured%2C%20tradable%20asset%20%28perceptual%2C%20semantic%2C%20inferential%29%2C%20and%20enforces%20profitability-driven%20trading%20among%20agents%20based%20on%20rational%20economic%20rules.%20A%20market-aware%20broker%2C%20extending%20Thompson%20Sampling%2C%20initiates%20collaboration%20and%20guides%20the%20system%20toward%20cost-efficient%20equilibria.%20Experiments%20on%20five%20multimodal%20benchmarks%20%28MMMU%2C%20MMBench%2C%20MathVision%2C%20InfoVQA%2C%20CC-OCR%29%20show%20that%20Agora%20outperforms%20strong%20VLMs%20and%20heuristic%20multi-agent%20strategies%2C%20e.g.%2C%20achieving%20%2B8.5%25%20accuracy%20over%20the%20best%20baseline%20on%20MMMU%20while%20reducing%20cost%20by%20over%203x.%20These%20results%20establish%20market-based%20coordination%20as%20a%20principled%20and%20scalable%20paradigm%20for%20building%20economically%20viable%20multi-agent%20visual%20intelligence%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2601.18735v1&entry.124074799=Read"},
{"title": "Beyond Preferences: Learning Alignment Principles Grounded in Human Reasons and Values", "author": "Henry Bell and Lara Neubauer da Costa Schertel and Bochu Ding and Brandon Fain", "abstract": "A crucial consideration when developing and deploying Large Language Models (LLMs) is the human values to which these models are aligned. In the constitutional framework of alignment models are aligned to a set of principles (the constitution) specified in natural language. However, it is unclear how to fairly determine this constitution with widespread stakeholder input. In this work we propose Grounded Constitutional AI (GCAI), a unified framework for generating constitutions of principles that are representative of both users' general expectations toward AI (general principles) and their interaction-time preferences (contextual principles). We extend the Inverse Constitutional AI (ICAI) approach to generate contextual principles from human preference annotation data by leveraging human-provided \\textit{reasons} for their preferences. We supplement these contextual principles with general principles surfaced from user statements of \\textit{values} regarding AI. We show that a constitution generated by GCAI is preferred by humans over one generated through ICAI both personally, and for widespread use in governing AI behavior. Additionally participants consider the GCAI constitution to be more morally grounded, coherent, and pluralistic.", "link": "http://arxiv.org/abs/2601.18760v1", "date": "2026-01-26", "relevancy": 2.2062, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4503}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4503}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Preferences%3A%20Learning%20Alignment%20Principles%20Grounded%20in%20Human%20Reasons%20and%20Values&body=Title%3A%20Beyond%20Preferences%3A%20Learning%20Alignment%20Principles%20Grounded%20in%20Human%20Reasons%20and%20Values%0AAuthor%3A%20Henry%20Bell%20and%20Lara%20Neubauer%20da%20Costa%20Schertel%20and%20Bochu%20Ding%20and%20Brandon%20Fain%0AAbstract%3A%20A%20crucial%20consideration%20when%20developing%20and%20deploying%20Large%20Language%20Models%20%28LLMs%29%20is%20the%20human%20values%20to%20which%20these%20models%20are%20aligned.%20In%20the%20constitutional%20framework%20of%20alignment%20models%20are%20aligned%20to%20a%20set%20of%20principles%20%28the%20constitution%29%20specified%20in%20natural%20language.%20However%2C%20it%20is%20unclear%20how%20to%20fairly%20determine%20this%20constitution%20with%20widespread%20stakeholder%20input.%20In%20this%20work%20we%20propose%20Grounded%20Constitutional%20AI%20%28GCAI%29%2C%20a%20unified%20framework%20for%20generating%20constitutions%20of%20principles%20that%20are%20representative%20of%20both%20users%27%20general%20expectations%20toward%20AI%20%28general%20principles%29%20and%20their%20interaction-time%20preferences%20%28contextual%20principles%29.%20We%20extend%20the%20Inverse%20Constitutional%20AI%20%28ICAI%29%20approach%20to%20generate%20contextual%20principles%20from%20human%20preference%20annotation%20data%20by%20leveraging%20human-provided%20%5Ctextit%7Breasons%7D%20for%20their%20preferences.%20We%20supplement%20these%20contextual%20principles%20with%20general%20principles%20surfaced%20from%20user%20statements%20of%20%5Ctextit%7Bvalues%7D%20regarding%20AI.%20We%20show%20that%20a%20constitution%20generated%20by%20GCAI%20is%20preferred%20by%20humans%20over%20one%20generated%20through%20ICAI%20both%20personally%2C%20and%20for%20widespread%20use%20in%20governing%20AI%20behavior.%20Additionally%20participants%20consider%20the%20GCAI%20constitution%20to%20be%20more%20morally%20grounded%2C%20coherent%2C%20and%20pluralistic.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Preferences%253A%2520Learning%2520Alignment%2520Principles%2520Grounded%2520in%2520Human%2520Reasons%2520and%2520Values%26entry.906535625%3DHenry%2520Bell%2520and%2520Lara%2520Neubauer%2520da%2520Costa%2520Schertel%2520and%2520Bochu%2520Ding%2520and%2520Brandon%2520Fain%26entry.1292438233%3DA%2520crucial%2520consideration%2520when%2520developing%2520and%2520deploying%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520is%2520the%2520human%2520values%2520to%2520which%2520these%2520models%2520are%2520aligned.%2520In%2520the%2520constitutional%2520framework%2520of%2520alignment%2520models%2520are%2520aligned%2520to%2520a%2520set%2520of%2520principles%2520%2528the%2520constitution%2529%2520specified%2520in%2520natural%2520language.%2520However%252C%2520it%2520is%2520unclear%2520how%2520to%2520fairly%2520determine%2520this%2520constitution%2520with%2520widespread%2520stakeholder%2520input.%2520In%2520this%2520work%2520we%2520propose%2520Grounded%2520Constitutional%2520AI%2520%2528GCAI%2529%252C%2520a%2520unified%2520framework%2520for%2520generating%2520constitutions%2520of%2520principles%2520that%2520are%2520representative%2520of%2520both%2520users%2527%2520general%2520expectations%2520toward%2520AI%2520%2528general%2520principles%2529%2520and%2520their%2520interaction-time%2520preferences%2520%2528contextual%2520principles%2529.%2520We%2520extend%2520the%2520Inverse%2520Constitutional%2520AI%2520%2528ICAI%2529%2520approach%2520to%2520generate%2520contextual%2520principles%2520from%2520human%2520preference%2520annotation%2520data%2520by%2520leveraging%2520human-provided%2520%255Ctextit%257Breasons%257D%2520for%2520their%2520preferences.%2520We%2520supplement%2520these%2520contextual%2520principles%2520with%2520general%2520principles%2520surfaced%2520from%2520user%2520statements%2520of%2520%255Ctextit%257Bvalues%257D%2520regarding%2520AI.%2520We%2520show%2520that%2520a%2520constitution%2520generated%2520by%2520GCAI%2520is%2520preferred%2520by%2520humans%2520over%2520one%2520generated%2520through%2520ICAI%2520both%2520personally%252C%2520and%2520for%2520widespread%2520use%2520in%2520governing%2520AI%2520behavior.%2520Additionally%2520participants%2520consider%2520the%2520GCAI%2520constitution%2520to%2520be%2520more%2520morally%2520grounded%252C%2520coherent%252C%2520and%2520pluralistic.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Preferences%3A%20Learning%20Alignment%20Principles%20Grounded%20in%20Human%20Reasons%20and%20Values&entry.906535625=Henry%20Bell%20and%20Lara%20Neubauer%20da%20Costa%20Schertel%20and%20Bochu%20Ding%20and%20Brandon%20Fain&entry.1292438233=A%20crucial%20consideration%20when%20developing%20and%20deploying%20Large%20Language%20Models%20%28LLMs%29%20is%20the%20human%20values%20to%20which%20these%20models%20are%20aligned.%20In%20the%20constitutional%20framework%20of%20alignment%20models%20are%20aligned%20to%20a%20set%20of%20principles%20%28the%20constitution%29%20specified%20in%20natural%20language.%20However%2C%20it%20is%20unclear%20how%20to%20fairly%20determine%20this%20constitution%20with%20widespread%20stakeholder%20input.%20In%20this%20work%20we%20propose%20Grounded%20Constitutional%20AI%20%28GCAI%29%2C%20a%20unified%20framework%20for%20generating%20constitutions%20of%20principles%20that%20are%20representative%20of%20both%20users%27%20general%20expectations%20toward%20AI%20%28general%20principles%29%20and%20their%20interaction-time%20preferences%20%28contextual%20principles%29.%20We%20extend%20the%20Inverse%20Constitutional%20AI%20%28ICAI%29%20approach%20to%20generate%20contextual%20principles%20from%20human%20preference%20annotation%20data%20by%20leveraging%20human-provided%20%5Ctextit%7Breasons%7D%20for%20their%20preferences.%20We%20supplement%20these%20contextual%20principles%20with%20general%20principles%20surfaced%20from%20user%20statements%20of%20%5Ctextit%7Bvalues%7D%20regarding%20AI.%20We%20show%20that%20a%20constitution%20generated%20by%20GCAI%20is%20preferred%20by%20humans%20over%20one%20generated%20through%20ICAI%20both%20personally%2C%20and%20for%20widespread%20use%20in%20governing%20AI%20behavior.%20Additionally%20participants%20consider%20the%20GCAI%20constitution%20to%20be%20more%20morally%20grounded%2C%20coherent%2C%20and%20pluralistic.&entry.1838667208=http%3A//arxiv.org/abs/2601.18760v1&entry.124074799=Read"},
{"title": "Larger than memory image processing", "author": "Jon Sporring and David Stansby", "abstract": "This report addresses larger-than-memory image analysis for petascale datasets such as 1.4 PB electron-microscopy volumes and 150 TB human-organ atlases. We argue that performance is fundamentally I/O-bound. We show that structuring analysis as streaming passes over data is crucial. For 3D volumes, two representations are popular: stacks of 2D slices (e.g., directories or multi-page TIFF) and 3D chunked layouts (e.g., Zarr/HDF5). While for a few algorithms, chunked layout on disk is crucial to keep disk I/O at a minimum, we show how the slice-based streaming architecture can be built on top of either image representation in a manner that minimizes disk I/O. This is in particular advantageous for algorithms relying on neighbouring values, since the slicing streaming architecture is 1D, which implies that there are only 2 possible sweeping orders, both of which are aligned with the order in which images are read from the disk. This is in contrast to 3D chunks, in which any sweep cannot be done without accessing each chunk at least 9 times. We formalize this with sweep-based execution (natural 2D/3D orders), windowed operations, and overlap-aware tiling to minimize redundant access. Building on these principles, we introduce a domain-specific language (DSL) that encodes algorithms with intrinsic knowledge of their optimal streaming and memory use; the DSL performs compile-time and run-time pipeline analyses to automatically select window sizes, fuse stages, tee and zip streams, and schedule passes for limited-RAM machines, yielding near-linear I/O scans and predictable memory footprints. The approach integrates with existing tooling for segmentation and morphology but reframes pre/post-processing as pipelines that privilege sequential read/write patterns, delivering substantial throughput gains for extremely large images without requiring full-volume residency in memory.", "link": "http://arxiv.org/abs/2601.18407v1", "date": "2026-01-26", "relevancy": 2.1973, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5803}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5331}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5249}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Larger%20than%20memory%20image%20processing&body=Title%3A%20Larger%20than%20memory%20image%20processing%0AAuthor%3A%20Jon%20Sporring%20and%20David%20Stansby%0AAbstract%3A%20This%20report%20addresses%20larger-than-memory%20image%20analysis%20for%20petascale%20datasets%20such%20as%201.4%20PB%20electron-microscopy%20volumes%20and%20150%20TB%20human-organ%20atlases.%20We%20argue%20that%20performance%20is%20fundamentally%20I/O-bound.%20We%20show%20that%20structuring%20analysis%20as%20streaming%20passes%20over%20data%20is%20crucial.%20For%203D%20volumes%2C%20two%20representations%20are%20popular%3A%20stacks%20of%202D%20slices%20%28e.g.%2C%20directories%20or%20multi-page%20TIFF%29%20and%203D%20chunked%20layouts%20%28e.g.%2C%20Zarr/HDF5%29.%20While%20for%20a%20few%20algorithms%2C%20chunked%20layout%20on%20disk%20is%20crucial%20to%20keep%20disk%20I/O%20at%20a%20minimum%2C%20we%20show%20how%20the%20slice-based%20streaming%20architecture%20can%20be%20built%20on%20top%20of%20either%20image%20representation%20in%20a%20manner%20that%20minimizes%20disk%20I/O.%20This%20is%20in%20particular%20advantageous%20for%20algorithms%20relying%20on%20neighbouring%20values%2C%20since%20the%20slicing%20streaming%20architecture%20is%201D%2C%20which%20implies%20that%20there%20are%20only%202%20possible%20sweeping%20orders%2C%20both%20of%20which%20are%20aligned%20with%20the%20order%20in%20which%20images%20are%20read%20from%20the%20disk.%20This%20is%20in%20contrast%20to%203D%20chunks%2C%20in%20which%20any%20sweep%20cannot%20be%20done%20without%20accessing%20each%20chunk%20at%20least%209%20times.%20We%20formalize%20this%20with%20sweep-based%20execution%20%28natural%202D/3D%20orders%29%2C%20windowed%20operations%2C%20and%20overlap-aware%20tiling%20to%20minimize%20redundant%20access.%20Building%20on%20these%20principles%2C%20we%20introduce%20a%20domain-specific%20language%20%28DSL%29%20that%20encodes%20algorithms%20with%20intrinsic%20knowledge%20of%20their%20optimal%20streaming%20and%20memory%20use%3B%20the%20DSL%20performs%20compile-time%20and%20run-time%20pipeline%20analyses%20to%20automatically%20select%20window%20sizes%2C%20fuse%20stages%2C%20tee%20and%20zip%20streams%2C%20and%20schedule%20passes%20for%20limited-RAM%20machines%2C%20yielding%20near-linear%20I/O%20scans%20and%20predictable%20memory%20footprints.%20The%20approach%20integrates%20with%20existing%20tooling%20for%20segmentation%20and%20morphology%20but%20reframes%20pre/post-processing%20as%20pipelines%20that%20privilege%20sequential%20read/write%20patterns%2C%20delivering%20substantial%20throughput%20gains%20for%20extremely%20large%20images%20without%20requiring%20full-volume%20residency%20in%20memory.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18407v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarger%2520than%2520memory%2520image%2520processing%26entry.906535625%3DJon%2520Sporring%2520and%2520David%2520Stansby%26entry.1292438233%3DThis%2520report%2520addresses%2520larger-than-memory%2520image%2520analysis%2520for%2520petascale%2520datasets%2520such%2520as%25201.4%2520PB%2520electron-microscopy%2520volumes%2520and%2520150%2520TB%2520human-organ%2520atlases.%2520We%2520argue%2520that%2520performance%2520is%2520fundamentally%2520I/O-bound.%2520We%2520show%2520that%2520structuring%2520analysis%2520as%2520streaming%2520passes%2520over%2520data%2520is%2520crucial.%2520For%25203D%2520volumes%252C%2520two%2520representations%2520are%2520popular%253A%2520stacks%2520of%25202D%2520slices%2520%2528e.g.%252C%2520directories%2520or%2520multi-page%2520TIFF%2529%2520and%25203D%2520chunked%2520layouts%2520%2528e.g.%252C%2520Zarr/HDF5%2529.%2520While%2520for%2520a%2520few%2520algorithms%252C%2520chunked%2520layout%2520on%2520disk%2520is%2520crucial%2520to%2520keep%2520disk%2520I/O%2520at%2520a%2520minimum%252C%2520we%2520show%2520how%2520the%2520slice-based%2520streaming%2520architecture%2520can%2520be%2520built%2520on%2520top%2520of%2520either%2520image%2520representation%2520in%2520a%2520manner%2520that%2520minimizes%2520disk%2520I/O.%2520This%2520is%2520in%2520particular%2520advantageous%2520for%2520algorithms%2520relying%2520on%2520neighbouring%2520values%252C%2520since%2520the%2520slicing%2520streaming%2520architecture%2520is%25201D%252C%2520which%2520implies%2520that%2520there%2520are%2520only%25202%2520possible%2520sweeping%2520orders%252C%2520both%2520of%2520which%2520are%2520aligned%2520with%2520the%2520order%2520in%2520which%2520images%2520are%2520read%2520from%2520the%2520disk.%2520This%2520is%2520in%2520contrast%2520to%25203D%2520chunks%252C%2520in%2520which%2520any%2520sweep%2520cannot%2520be%2520done%2520without%2520accessing%2520each%2520chunk%2520at%2520least%25209%2520times.%2520We%2520formalize%2520this%2520with%2520sweep-based%2520execution%2520%2528natural%25202D/3D%2520orders%2529%252C%2520windowed%2520operations%252C%2520and%2520overlap-aware%2520tiling%2520to%2520minimize%2520redundant%2520access.%2520Building%2520on%2520these%2520principles%252C%2520we%2520introduce%2520a%2520domain-specific%2520language%2520%2528DSL%2529%2520that%2520encodes%2520algorithms%2520with%2520intrinsic%2520knowledge%2520of%2520their%2520optimal%2520streaming%2520and%2520memory%2520use%253B%2520the%2520DSL%2520performs%2520compile-time%2520and%2520run-time%2520pipeline%2520analyses%2520to%2520automatically%2520select%2520window%2520sizes%252C%2520fuse%2520stages%252C%2520tee%2520and%2520zip%2520streams%252C%2520and%2520schedule%2520passes%2520for%2520limited-RAM%2520machines%252C%2520yielding%2520near-linear%2520I/O%2520scans%2520and%2520predictable%2520memory%2520footprints.%2520The%2520approach%2520integrates%2520with%2520existing%2520tooling%2520for%2520segmentation%2520and%2520morphology%2520but%2520reframes%2520pre/post-processing%2520as%2520pipelines%2520that%2520privilege%2520sequential%2520read/write%2520patterns%252C%2520delivering%2520substantial%2520throughput%2520gains%2520for%2520extremely%2520large%2520images%2520without%2520requiring%2520full-volume%2520residency%2520in%2520memory.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18407v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Larger%20than%20memory%20image%20processing&entry.906535625=Jon%20Sporring%20and%20David%20Stansby&entry.1292438233=This%20report%20addresses%20larger-than-memory%20image%20analysis%20for%20petascale%20datasets%20such%20as%201.4%20PB%20electron-microscopy%20volumes%20and%20150%20TB%20human-organ%20atlases.%20We%20argue%20that%20performance%20is%20fundamentally%20I/O-bound.%20We%20show%20that%20structuring%20analysis%20as%20streaming%20passes%20over%20data%20is%20crucial.%20For%203D%20volumes%2C%20two%20representations%20are%20popular%3A%20stacks%20of%202D%20slices%20%28e.g.%2C%20directories%20or%20multi-page%20TIFF%29%20and%203D%20chunked%20layouts%20%28e.g.%2C%20Zarr/HDF5%29.%20While%20for%20a%20few%20algorithms%2C%20chunked%20layout%20on%20disk%20is%20crucial%20to%20keep%20disk%20I/O%20at%20a%20minimum%2C%20we%20show%20how%20the%20slice-based%20streaming%20architecture%20can%20be%20built%20on%20top%20of%20either%20image%20representation%20in%20a%20manner%20that%20minimizes%20disk%20I/O.%20This%20is%20in%20particular%20advantageous%20for%20algorithms%20relying%20on%20neighbouring%20values%2C%20since%20the%20slicing%20streaming%20architecture%20is%201D%2C%20which%20implies%20that%20there%20are%20only%202%20possible%20sweeping%20orders%2C%20both%20of%20which%20are%20aligned%20with%20the%20order%20in%20which%20images%20are%20read%20from%20the%20disk.%20This%20is%20in%20contrast%20to%203D%20chunks%2C%20in%20which%20any%20sweep%20cannot%20be%20done%20without%20accessing%20each%20chunk%20at%20least%209%20times.%20We%20formalize%20this%20with%20sweep-based%20execution%20%28natural%202D/3D%20orders%29%2C%20windowed%20operations%2C%20and%20overlap-aware%20tiling%20to%20minimize%20redundant%20access.%20Building%20on%20these%20principles%2C%20we%20introduce%20a%20domain-specific%20language%20%28DSL%29%20that%20encodes%20algorithms%20with%20intrinsic%20knowledge%20of%20their%20optimal%20streaming%20and%20memory%20use%3B%20the%20DSL%20performs%20compile-time%20and%20run-time%20pipeline%20analyses%20to%20automatically%20select%20window%20sizes%2C%20fuse%20stages%2C%20tee%20and%20zip%20streams%2C%20and%20schedule%20passes%20for%20limited-RAM%20machines%2C%20yielding%20near-linear%20I/O%20scans%20and%20predictable%20memory%20footprints.%20The%20approach%20integrates%20with%20existing%20tooling%20for%20segmentation%20and%20morphology%20but%20reframes%20pre/post-processing%20as%20pipelines%20that%20privilege%20sequential%20read/write%20patterns%2C%20delivering%20substantial%20throughput%20gains%20for%20extremely%20large%20images%20without%20requiring%20full-volume%20residency%20in%20memory.&entry.1838667208=http%3A//arxiv.org/abs/2601.18407v1&entry.124074799=Read"},
{"title": "LLMPopcorn: Exploring LLMs as Assistants for Popular Micro-video Generation", "author": "Junchen Fu and Xuri Ge and Kaiwen Zheng and Alexandros Karatzoglou and Ioannis Arapakis and Xin Xin and Yongxin Ni and Joemon M. Jose", "abstract": "In an era where micro-videos dominate platforms like TikTok and YouTube, AI-generated content is nearing cinematic quality. The next frontier is using large language models (LLMs) to autonomously create viral micro-videos, a largely untapped potential that could shape the future of AI-driven content creation. To address this gap, this paper presents the first exploration of LLM-assisted popular micro-video generation (LLMPopcorn). We selected popcorn as the icon for this paper because it symbolizes leisure and entertainment, aligning with this study on leveraging LLMs as assistants for generating popular micro-videos that are often consumed during leisure time. Specifically, we empirically study the following research questions: (i) How can LLMs be effectively utilized to assist popular micro-video generation? (ii) To what extent can prompt-based enhancements optimize the LLM-generated content for higher popularity? (iii) How well do various LLMs and video generators perform in the popular micro-video generation task? Exploring these questions, we show that advanced LLMs like DeepSeek-V3 can generate micro-videos with popularity rivaling human content. Prompt enhancement further boosts results, while benchmarking highlights DeepSeek-V3 and R1 for LLMs, and LTX-Video and HunyuanVideo for video generation. This work advances AI-assisted micro-video creation and opens new research directions. The code is publicly available at https://github.com/GAIR-Lab/LLMPopcorn.", "link": "http://arxiv.org/abs/2502.12945v3", "date": "2026-01-26", "relevancy": 2.1947, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5678}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5425}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMPopcorn%3A%20Exploring%20LLMs%20as%20Assistants%20for%20Popular%20Micro-video%20Generation&body=Title%3A%20LLMPopcorn%3A%20Exploring%20LLMs%20as%20Assistants%20for%20Popular%20Micro-video%20Generation%0AAuthor%3A%20Junchen%20Fu%20and%20Xuri%20Ge%20and%20Kaiwen%20Zheng%20and%20Alexandros%20Karatzoglou%20and%20Ioannis%20Arapakis%20and%20Xin%20Xin%20and%20Yongxin%20Ni%20and%20Joemon%20M.%20Jose%0AAbstract%3A%20In%20an%20era%20where%20micro-videos%20dominate%20platforms%20like%20TikTok%20and%20YouTube%2C%20AI-generated%20content%20is%20nearing%20cinematic%20quality.%20The%20next%20frontier%20is%20using%20large%20language%20models%20%28LLMs%29%20to%20autonomously%20create%20viral%20micro-videos%2C%20a%20largely%20untapped%20potential%20that%20could%20shape%20the%20future%20of%20AI-driven%20content%20creation.%20To%20address%20this%20gap%2C%20this%20paper%20presents%20the%20first%20exploration%20of%20LLM-assisted%20popular%20micro-video%20generation%20%28LLMPopcorn%29.%20We%20selected%20popcorn%20as%20the%20icon%20for%20this%20paper%20because%20it%20symbolizes%20leisure%20and%20entertainment%2C%20aligning%20with%20this%20study%20on%20leveraging%20LLMs%20as%20assistants%20for%20generating%20popular%20micro-videos%20that%20are%20often%20consumed%20during%20leisure%20time.%20Specifically%2C%20we%20empirically%20study%20the%20following%20research%20questions%3A%20%28i%29%20How%20can%20LLMs%20be%20effectively%20utilized%20to%20assist%20popular%20micro-video%20generation%3F%20%28ii%29%20To%20what%20extent%20can%20prompt-based%20enhancements%20optimize%20the%20LLM-generated%20content%20for%20higher%20popularity%3F%20%28iii%29%20How%20well%20do%20various%20LLMs%20and%20video%20generators%20perform%20in%20the%20popular%20micro-video%20generation%20task%3F%20Exploring%20these%20questions%2C%20we%20show%20that%20advanced%20LLMs%20like%20DeepSeek-V3%20can%20generate%20micro-videos%20with%20popularity%20rivaling%20human%20content.%20Prompt%20enhancement%20further%20boosts%20results%2C%20while%20benchmarking%20highlights%20DeepSeek-V3%20and%20R1%20for%20LLMs%2C%20and%20LTX-Video%20and%20HunyuanVideo%20for%20video%20generation.%20This%20work%20advances%20AI-assisted%20micro-video%20creation%20and%20opens%20new%20research%20directions.%20The%20code%20is%20publicly%20available%20at%20https%3A//github.com/GAIR-Lab/LLMPopcorn.%0ALink%3A%20http%3A//arxiv.org/abs/2502.12945v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMPopcorn%253A%2520Exploring%2520LLMs%2520as%2520Assistants%2520for%2520Popular%2520Micro-video%2520Generation%26entry.906535625%3DJunchen%2520Fu%2520and%2520Xuri%2520Ge%2520and%2520Kaiwen%2520Zheng%2520and%2520Alexandros%2520Karatzoglou%2520and%2520Ioannis%2520Arapakis%2520and%2520Xin%2520Xin%2520and%2520Yongxin%2520Ni%2520and%2520Joemon%2520M.%2520Jose%26entry.1292438233%3DIn%2520an%2520era%2520where%2520micro-videos%2520dominate%2520platforms%2520like%2520TikTok%2520and%2520YouTube%252C%2520AI-generated%2520content%2520is%2520nearing%2520cinematic%2520quality.%2520The%2520next%2520frontier%2520is%2520using%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520autonomously%2520create%2520viral%2520micro-videos%252C%2520a%2520largely%2520untapped%2520potential%2520that%2520could%2520shape%2520the%2520future%2520of%2520AI-driven%2520content%2520creation.%2520To%2520address%2520this%2520gap%252C%2520this%2520paper%2520presents%2520the%2520first%2520exploration%2520of%2520LLM-assisted%2520popular%2520micro-video%2520generation%2520%2528LLMPopcorn%2529.%2520We%2520selected%2520popcorn%2520as%2520the%2520icon%2520for%2520this%2520paper%2520because%2520it%2520symbolizes%2520leisure%2520and%2520entertainment%252C%2520aligning%2520with%2520this%2520study%2520on%2520leveraging%2520LLMs%2520as%2520assistants%2520for%2520generating%2520popular%2520micro-videos%2520that%2520are%2520often%2520consumed%2520during%2520leisure%2520time.%2520Specifically%252C%2520we%2520empirically%2520study%2520the%2520following%2520research%2520questions%253A%2520%2528i%2529%2520How%2520can%2520LLMs%2520be%2520effectively%2520utilized%2520to%2520assist%2520popular%2520micro-video%2520generation%253F%2520%2528ii%2529%2520To%2520what%2520extent%2520can%2520prompt-based%2520enhancements%2520optimize%2520the%2520LLM-generated%2520content%2520for%2520higher%2520popularity%253F%2520%2528iii%2529%2520How%2520well%2520do%2520various%2520LLMs%2520and%2520video%2520generators%2520perform%2520in%2520the%2520popular%2520micro-video%2520generation%2520task%253F%2520Exploring%2520these%2520questions%252C%2520we%2520show%2520that%2520advanced%2520LLMs%2520like%2520DeepSeek-V3%2520can%2520generate%2520micro-videos%2520with%2520popularity%2520rivaling%2520human%2520content.%2520Prompt%2520enhancement%2520further%2520boosts%2520results%252C%2520while%2520benchmarking%2520highlights%2520DeepSeek-V3%2520and%2520R1%2520for%2520LLMs%252C%2520and%2520LTX-Video%2520and%2520HunyuanVideo%2520for%2520video%2520generation.%2520This%2520work%2520advances%2520AI-assisted%2520micro-video%2520creation%2520and%2520opens%2520new%2520research%2520directions.%2520The%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/GAIR-Lab/LLMPopcorn.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12945v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMPopcorn%3A%20Exploring%20LLMs%20as%20Assistants%20for%20Popular%20Micro-video%20Generation&entry.906535625=Junchen%20Fu%20and%20Xuri%20Ge%20and%20Kaiwen%20Zheng%20and%20Alexandros%20Karatzoglou%20and%20Ioannis%20Arapakis%20and%20Xin%20Xin%20and%20Yongxin%20Ni%20and%20Joemon%20M.%20Jose&entry.1292438233=In%20an%20era%20where%20micro-videos%20dominate%20platforms%20like%20TikTok%20and%20YouTube%2C%20AI-generated%20content%20is%20nearing%20cinematic%20quality.%20The%20next%20frontier%20is%20using%20large%20language%20models%20%28LLMs%29%20to%20autonomously%20create%20viral%20micro-videos%2C%20a%20largely%20untapped%20potential%20that%20could%20shape%20the%20future%20of%20AI-driven%20content%20creation.%20To%20address%20this%20gap%2C%20this%20paper%20presents%20the%20first%20exploration%20of%20LLM-assisted%20popular%20micro-video%20generation%20%28LLMPopcorn%29.%20We%20selected%20popcorn%20as%20the%20icon%20for%20this%20paper%20because%20it%20symbolizes%20leisure%20and%20entertainment%2C%20aligning%20with%20this%20study%20on%20leveraging%20LLMs%20as%20assistants%20for%20generating%20popular%20micro-videos%20that%20are%20often%20consumed%20during%20leisure%20time.%20Specifically%2C%20we%20empirically%20study%20the%20following%20research%20questions%3A%20%28i%29%20How%20can%20LLMs%20be%20effectively%20utilized%20to%20assist%20popular%20micro-video%20generation%3F%20%28ii%29%20To%20what%20extent%20can%20prompt-based%20enhancements%20optimize%20the%20LLM-generated%20content%20for%20higher%20popularity%3F%20%28iii%29%20How%20well%20do%20various%20LLMs%20and%20video%20generators%20perform%20in%20the%20popular%20micro-video%20generation%20task%3F%20Exploring%20these%20questions%2C%20we%20show%20that%20advanced%20LLMs%20like%20DeepSeek-V3%20can%20generate%20micro-videos%20with%20popularity%20rivaling%20human%20content.%20Prompt%20enhancement%20further%20boosts%20results%2C%20while%20benchmarking%20highlights%20DeepSeek-V3%20and%20R1%20for%20LLMs%2C%20and%20LTX-Video%20and%20HunyuanVideo%20for%20video%20generation.%20This%20work%20advances%20AI-assisted%20micro-video%20creation%20and%20opens%20new%20research%20directions.%20The%20code%20is%20publicly%20available%20at%20https%3A//github.com/GAIR-Lab/LLMPopcorn.&entry.1838667208=http%3A//arxiv.org/abs/2502.12945v3&entry.124074799=Read"},
{"title": "A Pragmatic VLA Foundation Model", "author": "Wei Wu and Fan Lu and Yunnan Wang and Shuai Yang and Shi Liu and Fangjing Wang and Qian Zhu and He Sun and Yong Wang and Shuailei Ma and Yiyu Ren and Kejia Zhang and Hui Yu and Jingmei Zhao and Shuai Zhou and Zhenqi Qiu and Houlong Xiong and Ziyu Wang and Zechen Wang and Ran Cheng and Yong-Lu Li and Yongtao Huang and Xing Zhu and Yujun Shen and Kecheng Zheng", "abstract": "Offering great potential in robotic manipulation, a capable Vision-Language-Action (VLA) foundation model is expected to faithfully generalize across tasks and platforms while ensuring cost efficiency (e.g., data and GPU hours required for adaptation). To this end, we develop LingBot-VLA with around 20,000 hours of real-world data from 9 popular dual-arm robot configurations. Through a systematic assessment on 3 robotic platforms, each completing 100 tasks with 130 post-training episodes per task, our model achieves clear superiority over competitors, showcasing its strong performance and broad generalizability. We have also built an efficient codebase, which delivers a throughput of 261 samples per second per GPU with an 8-GPU training setup, representing a 1.5~2.8$\\times$ (depending on the relied VLM base model) speedup over existing VLA-oriented codebases. The above features ensure that our model is well-suited for real-world deployment. To advance the field of robot learning, we provide open access to the code, base model, and benchmark data, with a focus on enabling more challenging tasks and promoting sound evaluation standards.", "link": "http://arxiv.org/abs/2601.18692v1", "date": "2026-01-26", "relevancy": 2.1944, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5515}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5515}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Pragmatic%20VLA%20Foundation%20Model&body=Title%3A%20A%20Pragmatic%20VLA%20Foundation%20Model%0AAuthor%3A%20Wei%20Wu%20and%20Fan%20Lu%20and%20Yunnan%20Wang%20and%20Shuai%20Yang%20and%20Shi%20Liu%20and%20Fangjing%20Wang%20and%20Qian%20Zhu%20and%20He%20Sun%20and%20Yong%20Wang%20and%20Shuailei%20Ma%20and%20Yiyu%20Ren%20and%20Kejia%20Zhang%20and%20Hui%20Yu%20and%20Jingmei%20Zhao%20and%20Shuai%20Zhou%20and%20Zhenqi%20Qiu%20and%20Houlong%20Xiong%20and%20Ziyu%20Wang%20and%20Zechen%20Wang%20and%20Ran%20Cheng%20and%20Yong-Lu%20Li%20and%20Yongtao%20Huang%20and%20Xing%20Zhu%20and%20Yujun%20Shen%20and%20Kecheng%20Zheng%0AAbstract%3A%20Offering%20great%20potential%20in%20robotic%20manipulation%2C%20a%20capable%20Vision-Language-Action%20%28VLA%29%20foundation%20model%20is%20expected%20to%20faithfully%20generalize%20across%20tasks%20and%20platforms%20while%20ensuring%20cost%20efficiency%20%28e.g.%2C%20data%20and%20GPU%20hours%20required%20for%20adaptation%29.%20To%20this%20end%2C%20we%20develop%20LingBot-VLA%20with%20around%2020%2C000%20hours%20of%20real-world%20data%20from%209%20popular%20dual-arm%20robot%20configurations.%20Through%20a%20systematic%20assessment%20on%203%20robotic%20platforms%2C%20each%20completing%20100%20tasks%20with%20130%20post-training%20episodes%20per%20task%2C%20our%20model%20achieves%20clear%20superiority%20over%20competitors%2C%20showcasing%20its%20strong%20performance%20and%20broad%20generalizability.%20We%20have%20also%20built%20an%20efficient%20codebase%2C%20which%20delivers%20a%20throughput%20of%20261%20samples%20per%20second%20per%20GPU%20with%20an%208-GPU%20training%20setup%2C%20representing%20a%201.5~2.8%24%5Ctimes%24%20%28depending%20on%20the%20relied%20VLM%20base%20model%29%20speedup%20over%20existing%20VLA-oriented%20codebases.%20The%20above%20features%20ensure%20that%20our%20model%20is%20well-suited%20for%20real-world%20deployment.%20To%20advance%20the%20field%20of%20robot%20learning%2C%20we%20provide%20open%20access%20to%20the%20code%2C%20base%20model%2C%20and%20benchmark%20data%2C%20with%20a%20focus%20on%20enabling%20more%20challenging%20tasks%20and%20promoting%20sound%20evaluation%20standards.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18692v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Pragmatic%2520VLA%2520Foundation%2520Model%26entry.906535625%3DWei%2520Wu%2520and%2520Fan%2520Lu%2520and%2520Yunnan%2520Wang%2520and%2520Shuai%2520Yang%2520and%2520Shi%2520Liu%2520and%2520Fangjing%2520Wang%2520and%2520Qian%2520Zhu%2520and%2520He%2520Sun%2520and%2520Yong%2520Wang%2520and%2520Shuailei%2520Ma%2520and%2520Yiyu%2520Ren%2520and%2520Kejia%2520Zhang%2520and%2520Hui%2520Yu%2520and%2520Jingmei%2520Zhao%2520and%2520Shuai%2520Zhou%2520and%2520Zhenqi%2520Qiu%2520and%2520Houlong%2520Xiong%2520and%2520Ziyu%2520Wang%2520and%2520Zechen%2520Wang%2520and%2520Ran%2520Cheng%2520and%2520Yong-Lu%2520Li%2520and%2520Yongtao%2520Huang%2520and%2520Xing%2520Zhu%2520and%2520Yujun%2520Shen%2520and%2520Kecheng%2520Zheng%26entry.1292438233%3DOffering%2520great%2520potential%2520in%2520robotic%2520manipulation%252C%2520a%2520capable%2520Vision-Language-Action%2520%2528VLA%2529%2520foundation%2520model%2520is%2520expected%2520to%2520faithfully%2520generalize%2520across%2520tasks%2520and%2520platforms%2520while%2520ensuring%2520cost%2520efficiency%2520%2528e.g.%252C%2520data%2520and%2520GPU%2520hours%2520required%2520for%2520adaptation%2529.%2520To%2520this%2520end%252C%2520we%2520develop%2520LingBot-VLA%2520with%2520around%252020%252C000%2520hours%2520of%2520real-world%2520data%2520from%25209%2520popular%2520dual-arm%2520robot%2520configurations.%2520Through%2520a%2520systematic%2520assessment%2520on%25203%2520robotic%2520platforms%252C%2520each%2520completing%2520100%2520tasks%2520with%2520130%2520post-training%2520episodes%2520per%2520task%252C%2520our%2520model%2520achieves%2520clear%2520superiority%2520over%2520competitors%252C%2520showcasing%2520its%2520strong%2520performance%2520and%2520broad%2520generalizability.%2520We%2520have%2520also%2520built%2520an%2520efficient%2520codebase%252C%2520which%2520delivers%2520a%2520throughput%2520of%2520261%2520samples%2520per%2520second%2520per%2520GPU%2520with%2520an%25208-GPU%2520training%2520setup%252C%2520representing%2520a%25201.5~2.8%2524%255Ctimes%2524%2520%2528depending%2520on%2520the%2520relied%2520VLM%2520base%2520model%2529%2520speedup%2520over%2520existing%2520VLA-oriented%2520codebases.%2520The%2520above%2520features%2520ensure%2520that%2520our%2520model%2520is%2520well-suited%2520for%2520real-world%2520deployment.%2520To%2520advance%2520the%2520field%2520of%2520robot%2520learning%252C%2520we%2520provide%2520open%2520access%2520to%2520the%2520code%252C%2520base%2520model%252C%2520and%2520benchmark%2520data%252C%2520with%2520a%2520focus%2520on%2520enabling%2520more%2520challenging%2520tasks%2520and%2520promoting%2520sound%2520evaluation%2520standards.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18692v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Pragmatic%20VLA%20Foundation%20Model&entry.906535625=Wei%20Wu%20and%20Fan%20Lu%20and%20Yunnan%20Wang%20and%20Shuai%20Yang%20and%20Shi%20Liu%20and%20Fangjing%20Wang%20and%20Qian%20Zhu%20and%20He%20Sun%20and%20Yong%20Wang%20and%20Shuailei%20Ma%20and%20Yiyu%20Ren%20and%20Kejia%20Zhang%20and%20Hui%20Yu%20and%20Jingmei%20Zhao%20and%20Shuai%20Zhou%20and%20Zhenqi%20Qiu%20and%20Houlong%20Xiong%20and%20Ziyu%20Wang%20and%20Zechen%20Wang%20and%20Ran%20Cheng%20and%20Yong-Lu%20Li%20and%20Yongtao%20Huang%20and%20Xing%20Zhu%20and%20Yujun%20Shen%20and%20Kecheng%20Zheng&entry.1292438233=Offering%20great%20potential%20in%20robotic%20manipulation%2C%20a%20capable%20Vision-Language-Action%20%28VLA%29%20foundation%20model%20is%20expected%20to%20faithfully%20generalize%20across%20tasks%20and%20platforms%20while%20ensuring%20cost%20efficiency%20%28e.g.%2C%20data%20and%20GPU%20hours%20required%20for%20adaptation%29.%20To%20this%20end%2C%20we%20develop%20LingBot-VLA%20with%20around%2020%2C000%20hours%20of%20real-world%20data%20from%209%20popular%20dual-arm%20robot%20configurations.%20Through%20a%20systematic%20assessment%20on%203%20robotic%20platforms%2C%20each%20completing%20100%20tasks%20with%20130%20post-training%20episodes%20per%20task%2C%20our%20model%20achieves%20clear%20superiority%20over%20competitors%2C%20showcasing%20its%20strong%20performance%20and%20broad%20generalizability.%20We%20have%20also%20built%20an%20efficient%20codebase%2C%20which%20delivers%20a%20throughput%20of%20261%20samples%20per%20second%20per%20GPU%20with%20an%208-GPU%20training%20setup%2C%20representing%20a%201.5~2.8%24%5Ctimes%24%20%28depending%20on%20the%20relied%20VLM%20base%20model%29%20speedup%20over%20existing%20VLA-oriented%20codebases.%20The%20above%20features%20ensure%20that%20our%20model%20is%20well-suited%20for%20real-world%20deployment.%20To%20advance%20the%20field%20of%20robot%20learning%2C%20we%20provide%20open%20access%20to%20the%20code%2C%20base%20model%2C%20and%20benchmark%20data%2C%20with%20a%20focus%20on%20enabling%20more%20challenging%20tasks%20and%20promoting%20sound%20evaluation%20standards.&entry.1838667208=http%3A//arxiv.org/abs/2601.18692v1&entry.124074799=Read"},
{"title": "From Filters to VLMs: Benchmarking Defogging Methods through Object Detection and Segmentation Performance", "author": "Ardalan Aryashad and Parsa Razmara and Amin Mahjoub and Seyedarmin Azizi and Mahdi Salmani and Arad Firouzkouhi", "abstract": "Autonomous driving perception systems are particularly vulnerable in foggy conditions, where light scattering reduces contrast and obscures fine details critical for safe operation. While numerous defogging methods exist, from handcrafted filters to learned restoration models, improvements in image fidelity do not consistently translate into better downstream detection and segmentation. Moreover, prior evaluations often rely on synthetic data, raising concerns about real-world transferability.\n  We present a structured empirical study that benchmarks a comprehensive set of defogging pipelines, including classical dehazing filters, modern defogging networks, chained variants combining filters and models, and prompt-driven visual language image editing models applied directly to foggy images. To bridge the gap between simulated and physical environments, we evaluate these pipelines on both the synthetic Foggy Cityscapes dataset and the real-world Adverse Conditions Dataset with Correspondences (ACDC).\n  We examine generalization by evaluating performance on synthetic fog and real-world conditions, assessing both image quality and downstream perception in terms of object detection mean average precision and segmentation panoptic quality. Our analysis identifies when defogging is effective, the impact of combining models, and how visual language models compare to traditional approaches. We additionally report qualitative rubric-based evaluations from both human and visual language model judges and analyze their alignment with downstream task metrics.\n  Together, these results establish a transparent, task-oriented benchmark for defogging methods and identify the conditions under which pre-processing meaningfully improves autonomous perception in adverse weather.\n  Project page: https://aradfir.github.io/filters-to-vlms-defogging-page/", "link": "http://arxiv.org/abs/2510.03906v2", "date": "2026-01-26", "relevancy": 2.1869, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.548}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.548}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5403}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Filters%20to%20VLMs%3A%20Benchmarking%20Defogging%20Methods%20through%20Object%20Detection%20and%20Segmentation%20Performance&body=Title%3A%20From%20Filters%20to%20VLMs%3A%20Benchmarking%20Defogging%20Methods%20through%20Object%20Detection%20and%20Segmentation%20Performance%0AAuthor%3A%20Ardalan%20Aryashad%20and%20Parsa%20Razmara%20and%20Amin%20Mahjoub%20and%20Seyedarmin%20Azizi%20and%20Mahdi%20Salmani%20and%20Arad%20Firouzkouhi%0AAbstract%3A%20Autonomous%20driving%20perception%20systems%20are%20particularly%20vulnerable%20in%20foggy%20conditions%2C%20where%20light%20scattering%20reduces%20contrast%20and%20obscures%20fine%20details%20critical%20for%20safe%20operation.%20While%20numerous%20defogging%20methods%20exist%2C%20from%20handcrafted%20filters%20to%20learned%20restoration%20models%2C%20improvements%20in%20image%20fidelity%20do%20not%20consistently%20translate%20into%20better%20downstream%20detection%20and%20segmentation.%20Moreover%2C%20prior%20evaluations%20often%20rely%20on%20synthetic%20data%2C%20raising%20concerns%20about%20real-world%20transferability.%0A%20%20We%20present%20a%20structured%20empirical%20study%20that%20benchmarks%20a%20comprehensive%20set%20of%20defogging%20pipelines%2C%20including%20classical%20dehazing%20filters%2C%20modern%20defogging%20networks%2C%20chained%20variants%20combining%20filters%20and%20models%2C%20and%20prompt-driven%20visual%20language%20image%20editing%20models%20applied%20directly%20to%20foggy%20images.%20To%20bridge%20the%20gap%20between%20simulated%20and%20physical%20environments%2C%20we%20evaluate%20these%20pipelines%20on%20both%20the%20synthetic%20Foggy%20Cityscapes%20dataset%20and%20the%20real-world%20Adverse%20Conditions%20Dataset%20with%20Correspondences%20%28ACDC%29.%0A%20%20We%20examine%20generalization%20by%20evaluating%20performance%20on%20synthetic%20fog%20and%20real-world%20conditions%2C%20assessing%20both%20image%20quality%20and%20downstream%20perception%20in%20terms%20of%20object%20detection%20mean%20average%20precision%20and%20segmentation%20panoptic%20quality.%20Our%20analysis%20identifies%20when%20defogging%20is%20effective%2C%20the%20impact%20of%20combining%20models%2C%20and%20how%20visual%20language%20models%20compare%20to%20traditional%20approaches.%20We%20additionally%20report%20qualitative%20rubric-based%20evaluations%20from%20both%20human%20and%20visual%20language%20model%20judges%20and%20analyze%20their%20alignment%20with%20downstream%20task%20metrics.%0A%20%20Together%2C%20these%20results%20establish%20a%20transparent%2C%20task-oriented%20benchmark%20for%20defogging%20methods%20and%20identify%20the%20conditions%20under%20which%20pre-processing%20meaningfully%20improves%20autonomous%20perception%20in%20adverse%20weather.%0A%20%20Project%20page%3A%20https%3A//aradfir.github.io/filters-to-vlms-defogging-page/%0ALink%3A%20http%3A//arxiv.org/abs/2510.03906v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Filters%2520to%2520VLMs%253A%2520Benchmarking%2520Defogging%2520Methods%2520through%2520Object%2520Detection%2520and%2520Segmentation%2520Performance%26entry.906535625%3DArdalan%2520Aryashad%2520and%2520Parsa%2520Razmara%2520and%2520Amin%2520Mahjoub%2520and%2520Seyedarmin%2520Azizi%2520and%2520Mahdi%2520Salmani%2520and%2520Arad%2520Firouzkouhi%26entry.1292438233%3DAutonomous%2520driving%2520perception%2520systems%2520are%2520particularly%2520vulnerable%2520in%2520foggy%2520conditions%252C%2520where%2520light%2520scattering%2520reduces%2520contrast%2520and%2520obscures%2520fine%2520details%2520critical%2520for%2520safe%2520operation.%2520While%2520numerous%2520defogging%2520methods%2520exist%252C%2520from%2520handcrafted%2520filters%2520to%2520learned%2520restoration%2520models%252C%2520improvements%2520in%2520image%2520fidelity%2520do%2520not%2520consistently%2520translate%2520into%2520better%2520downstream%2520detection%2520and%2520segmentation.%2520Moreover%252C%2520prior%2520evaluations%2520often%2520rely%2520on%2520synthetic%2520data%252C%2520raising%2520concerns%2520about%2520real-world%2520transferability.%250A%2520%2520We%2520present%2520a%2520structured%2520empirical%2520study%2520that%2520benchmarks%2520a%2520comprehensive%2520set%2520of%2520defogging%2520pipelines%252C%2520including%2520classical%2520dehazing%2520filters%252C%2520modern%2520defogging%2520networks%252C%2520chained%2520variants%2520combining%2520filters%2520and%2520models%252C%2520and%2520prompt-driven%2520visual%2520language%2520image%2520editing%2520models%2520applied%2520directly%2520to%2520foggy%2520images.%2520To%2520bridge%2520the%2520gap%2520between%2520simulated%2520and%2520physical%2520environments%252C%2520we%2520evaluate%2520these%2520pipelines%2520on%2520both%2520the%2520synthetic%2520Foggy%2520Cityscapes%2520dataset%2520and%2520the%2520real-world%2520Adverse%2520Conditions%2520Dataset%2520with%2520Correspondences%2520%2528ACDC%2529.%250A%2520%2520We%2520examine%2520generalization%2520by%2520evaluating%2520performance%2520on%2520synthetic%2520fog%2520and%2520real-world%2520conditions%252C%2520assessing%2520both%2520image%2520quality%2520and%2520downstream%2520perception%2520in%2520terms%2520of%2520object%2520detection%2520mean%2520average%2520precision%2520and%2520segmentation%2520panoptic%2520quality.%2520Our%2520analysis%2520identifies%2520when%2520defogging%2520is%2520effective%252C%2520the%2520impact%2520of%2520combining%2520models%252C%2520and%2520how%2520visual%2520language%2520models%2520compare%2520to%2520traditional%2520approaches.%2520We%2520additionally%2520report%2520qualitative%2520rubric-based%2520evaluations%2520from%2520both%2520human%2520and%2520visual%2520language%2520model%2520judges%2520and%2520analyze%2520their%2520alignment%2520with%2520downstream%2520task%2520metrics.%250A%2520%2520Together%252C%2520these%2520results%2520establish%2520a%2520transparent%252C%2520task-oriented%2520benchmark%2520for%2520defogging%2520methods%2520and%2520identify%2520the%2520conditions%2520under%2520which%2520pre-processing%2520meaningfully%2520improves%2520autonomous%2520perception%2520in%2520adverse%2520weather.%250A%2520%2520Project%2520page%253A%2520https%253A//aradfir.github.io/filters-to-vlms-defogging-page/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03906v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Filters%20to%20VLMs%3A%20Benchmarking%20Defogging%20Methods%20through%20Object%20Detection%20and%20Segmentation%20Performance&entry.906535625=Ardalan%20Aryashad%20and%20Parsa%20Razmara%20and%20Amin%20Mahjoub%20and%20Seyedarmin%20Azizi%20and%20Mahdi%20Salmani%20and%20Arad%20Firouzkouhi&entry.1292438233=Autonomous%20driving%20perception%20systems%20are%20particularly%20vulnerable%20in%20foggy%20conditions%2C%20where%20light%20scattering%20reduces%20contrast%20and%20obscures%20fine%20details%20critical%20for%20safe%20operation.%20While%20numerous%20defogging%20methods%20exist%2C%20from%20handcrafted%20filters%20to%20learned%20restoration%20models%2C%20improvements%20in%20image%20fidelity%20do%20not%20consistently%20translate%20into%20better%20downstream%20detection%20and%20segmentation.%20Moreover%2C%20prior%20evaluations%20often%20rely%20on%20synthetic%20data%2C%20raising%20concerns%20about%20real-world%20transferability.%0A%20%20We%20present%20a%20structured%20empirical%20study%20that%20benchmarks%20a%20comprehensive%20set%20of%20defogging%20pipelines%2C%20including%20classical%20dehazing%20filters%2C%20modern%20defogging%20networks%2C%20chained%20variants%20combining%20filters%20and%20models%2C%20and%20prompt-driven%20visual%20language%20image%20editing%20models%20applied%20directly%20to%20foggy%20images.%20To%20bridge%20the%20gap%20between%20simulated%20and%20physical%20environments%2C%20we%20evaluate%20these%20pipelines%20on%20both%20the%20synthetic%20Foggy%20Cityscapes%20dataset%20and%20the%20real-world%20Adverse%20Conditions%20Dataset%20with%20Correspondences%20%28ACDC%29.%0A%20%20We%20examine%20generalization%20by%20evaluating%20performance%20on%20synthetic%20fog%20and%20real-world%20conditions%2C%20assessing%20both%20image%20quality%20and%20downstream%20perception%20in%20terms%20of%20object%20detection%20mean%20average%20precision%20and%20segmentation%20panoptic%20quality.%20Our%20analysis%20identifies%20when%20defogging%20is%20effective%2C%20the%20impact%20of%20combining%20models%2C%20and%20how%20visual%20language%20models%20compare%20to%20traditional%20approaches.%20We%20additionally%20report%20qualitative%20rubric-based%20evaluations%20from%20both%20human%20and%20visual%20language%20model%20judges%20and%20analyze%20their%20alignment%20with%20downstream%20task%20metrics.%0A%20%20Together%2C%20these%20results%20establish%20a%20transparent%2C%20task-oriented%20benchmark%20for%20defogging%20methods%20and%20identify%20the%20conditions%20under%20which%20pre-processing%20meaningfully%20improves%20autonomous%20perception%20in%20adverse%20weather.%0A%20%20Project%20page%3A%20https%3A//aradfir.github.io/filters-to-vlms-defogging-page/&entry.1838667208=http%3A//arxiv.org/abs/2510.03906v2&entry.124074799=Read"},
{"title": "Radiance Fields from Photons", "author": "Sacha Jungerman and Aryan Garg and Mohit Gupta", "abstract": "Neural radiance fields, or NeRFs, have become the de facto approach for high-quality view synthesis from a collection of images captured from multiple viewpoints. However, many issues remain when capturing images in-the-wild under challenging conditions, such as low light, high dynamic range, or rapid motion leading to smeared reconstructions with noticeable artifacts. In this work, we introduce quanta radiance fields, a novel class of neural radiance fields that are trained at the granularity of individual photons using single-photon cameras (SPCs). We develop theory and practical computational techniques for building radiance fields and estimating dense camera poses from unconventional, stochastic, and high-speed binary frame sequences captured by SPCs. We demonstrate, both via simulations and a SPC hardware prototype, high-fidelity reconstructions under high-speed motion, in low light, and for extreme dynamic range settings.", "link": "http://arxiv.org/abs/2407.09386v3", "date": "2026-01-26", "relevancy": 2.1849, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5705}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5347}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5265}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Radiance%20Fields%20from%20Photons&body=Title%3A%20Radiance%20Fields%20from%20Photons%0AAuthor%3A%20Sacha%20Jungerman%20and%20Aryan%20Garg%20and%20Mohit%20Gupta%0AAbstract%3A%20Neural%20radiance%20fields%2C%20or%20NeRFs%2C%20have%20become%20the%20de%20facto%20approach%20for%20high-quality%20view%20synthesis%20from%20a%20collection%20of%20images%20captured%20from%20multiple%20viewpoints.%20However%2C%20many%20issues%20remain%20when%20capturing%20images%20in-the-wild%20under%20challenging%20conditions%2C%20such%20as%20low%20light%2C%20high%20dynamic%20range%2C%20or%20rapid%20motion%20leading%20to%20smeared%20reconstructions%20with%20noticeable%20artifacts.%20In%20this%20work%2C%20we%20introduce%20quanta%20radiance%20fields%2C%20a%20novel%20class%20of%20neural%20radiance%20fields%20that%20are%20trained%20at%20the%20granularity%20of%20individual%20photons%20using%20single-photon%20cameras%20%28SPCs%29.%20We%20develop%20theory%20and%20practical%20computational%20techniques%20for%20building%20radiance%20fields%20and%20estimating%20dense%20camera%20poses%20from%20unconventional%2C%20stochastic%2C%20and%20high-speed%20binary%20frame%20sequences%20captured%20by%20SPCs.%20We%20demonstrate%2C%20both%20via%20simulations%20and%20a%20SPC%20hardware%20prototype%2C%20high-fidelity%20reconstructions%20under%20high-speed%20motion%2C%20in%20low%20light%2C%20and%20for%20extreme%20dynamic%20range%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2407.09386v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRadiance%2520Fields%2520from%2520Photons%26entry.906535625%3DSacha%2520Jungerman%2520and%2520Aryan%2520Garg%2520and%2520Mohit%2520Gupta%26entry.1292438233%3DNeural%2520radiance%2520fields%252C%2520or%2520NeRFs%252C%2520have%2520become%2520the%2520de%2520facto%2520approach%2520for%2520high-quality%2520view%2520synthesis%2520from%2520a%2520collection%2520of%2520images%2520captured%2520from%2520multiple%2520viewpoints.%2520However%252C%2520many%2520issues%2520remain%2520when%2520capturing%2520images%2520in-the-wild%2520under%2520challenging%2520conditions%252C%2520such%2520as%2520low%2520light%252C%2520high%2520dynamic%2520range%252C%2520or%2520rapid%2520motion%2520leading%2520to%2520smeared%2520reconstructions%2520with%2520noticeable%2520artifacts.%2520In%2520this%2520work%252C%2520we%2520introduce%2520quanta%2520radiance%2520fields%252C%2520a%2520novel%2520class%2520of%2520neural%2520radiance%2520fields%2520that%2520are%2520trained%2520at%2520the%2520granularity%2520of%2520individual%2520photons%2520using%2520single-photon%2520cameras%2520%2528SPCs%2529.%2520We%2520develop%2520theory%2520and%2520practical%2520computational%2520techniques%2520for%2520building%2520radiance%2520fields%2520and%2520estimating%2520dense%2520camera%2520poses%2520from%2520unconventional%252C%2520stochastic%252C%2520and%2520high-speed%2520binary%2520frame%2520sequences%2520captured%2520by%2520SPCs.%2520We%2520demonstrate%252C%2520both%2520via%2520simulations%2520and%2520a%2520SPC%2520hardware%2520prototype%252C%2520high-fidelity%2520reconstructions%2520under%2520high-speed%2520motion%252C%2520in%2520low%2520light%252C%2520and%2520for%2520extreme%2520dynamic%2520range%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09386v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Radiance%20Fields%20from%20Photons&entry.906535625=Sacha%20Jungerman%20and%20Aryan%20Garg%20and%20Mohit%20Gupta&entry.1292438233=Neural%20radiance%20fields%2C%20or%20NeRFs%2C%20have%20become%20the%20de%20facto%20approach%20for%20high-quality%20view%20synthesis%20from%20a%20collection%20of%20images%20captured%20from%20multiple%20viewpoints.%20However%2C%20many%20issues%20remain%20when%20capturing%20images%20in-the-wild%20under%20challenging%20conditions%2C%20such%20as%20low%20light%2C%20high%20dynamic%20range%2C%20or%20rapid%20motion%20leading%20to%20smeared%20reconstructions%20with%20noticeable%20artifacts.%20In%20this%20work%2C%20we%20introduce%20quanta%20radiance%20fields%2C%20a%20novel%20class%20of%20neural%20radiance%20fields%20that%20are%20trained%20at%20the%20granularity%20of%20individual%20photons%20using%20single-photon%20cameras%20%28SPCs%29.%20We%20develop%20theory%20and%20practical%20computational%20techniques%20for%20building%20radiance%20fields%20and%20estimating%20dense%20camera%20poses%20from%20unconventional%2C%20stochastic%2C%20and%20high-speed%20binary%20frame%20sequences%20captured%20by%20SPCs.%20We%20demonstrate%2C%20both%20via%20simulations%20and%20a%20SPC%20hardware%20prototype%2C%20high-fidelity%20reconstructions%20under%20high-speed%20motion%2C%20in%20low%20light%2C%20and%20for%20extreme%20dynamic%20range%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2407.09386v3&entry.124074799=Read"},
{"title": "Tandem Training for Language Models", "author": "Robert West and Ashton Anderson and Ece Kamar and Eric Horvitz", "abstract": "As language models continue to rapidly improve, we can expect their actions and reasoning to become difficult or impossible for weaker agents and humans to follow, undermining interpretability and oversight. With an eye on long-term futures, we pursue methods that encourage models to produce solutions that remain intelligible to weaker collaborators. We formalize intelligibility as handoff robustness: a strong model's solution is intelligible to a weaker model if randomly handing off control to the weaker model along the solution path does not cause failure. Building on this criterion, we introduce tandem training for language models, a reinforcement learning (RL) paradigm in which rollout tokens are intermittently and randomly sampled from a frozen weak model rather than the strong model being trained. Because rollouts succeed only when the strong model's actions and reasoning process can be continued by the weak model -- when the two can co-construct a successful solution -- optimizing standard RL objectives with tandem training implicitly incentivizes both correctness and intelligibility. In the GSM8K math reasoning task, tandem training reliably teaches models to abandon jargon and adapt their language to weaker partners while keeping task accuracy high. Our results demonstrate a promising route to building AI systems that remain auditable by weaker agents, with implications for human--AI collaboration and multi-agent communication.", "link": "http://arxiv.org/abs/2510.13551v2", "date": "2026-01-26", "relevancy": 2.1644, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5485}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5484}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tandem%20Training%20for%20Language%20Models&body=Title%3A%20Tandem%20Training%20for%20Language%20Models%0AAuthor%3A%20Robert%20West%20and%20Ashton%20Anderson%20and%20Ece%20Kamar%20and%20Eric%20Horvitz%0AAbstract%3A%20As%20language%20models%20continue%20to%20rapidly%20improve%2C%20we%20can%20expect%20their%20actions%20and%20reasoning%20to%20become%20difficult%20or%20impossible%20for%20weaker%20agents%20and%20humans%20to%20follow%2C%20undermining%20interpretability%20and%20oversight.%20With%20an%20eye%20on%20long-term%20futures%2C%20we%20pursue%20methods%20that%20encourage%20models%20to%20produce%20solutions%20that%20remain%20intelligible%20to%20weaker%20collaborators.%20We%20formalize%20intelligibility%20as%20handoff%20robustness%3A%20a%20strong%20model%27s%20solution%20is%20intelligible%20to%20a%20weaker%20model%20if%20randomly%20handing%20off%20control%20to%20the%20weaker%20model%20along%20the%20solution%20path%20does%20not%20cause%20failure.%20Building%20on%20this%20criterion%2C%20we%20introduce%20tandem%20training%20for%20language%20models%2C%20a%20reinforcement%20learning%20%28RL%29%20paradigm%20in%20which%20rollout%20tokens%20are%20intermittently%20and%20randomly%20sampled%20from%20a%20frozen%20weak%20model%20rather%20than%20the%20strong%20model%20being%20trained.%20Because%20rollouts%20succeed%20only%20when%20the%20strong%20model%27s%20actions%20and%20reasoning%20process%20can%20be%20continued%20by%20the%20weak%20model%20--%20when%20the%20two%20can%20co-construct%20a%20successful%20solution%20--%20optimizing%20standard%20RL%20objectives%20with%20tandem%20training%20implicitly%20incentivizes%20both%20correctness%20and%20intelligibility.%20In%20the%20GSM8K%20math%20reasoning%20task%2C%20tandem%20training%20reliably%20teaches%20models%20to%20abandon%20jargon%20and%20adapt%20their%20language%20to%20weaker%20partners%20while%20keeping%20task%20accuracy%20high.%20Our%20results%20demonstrate%20a%20promising%20route%20to%20building%20AI%20systems%20that%20remain%20auditable%20by%20weaker%20agents%2C%20with%20implications%20for%20human--AI%20collaboration%20and%20multi-agent%20communication.%0ALink%3A%20http%3A//arxiv.org/abs/2510.13551v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTandem%2520Training%2520for%2520Language%2520Models%26entry.906535625%3DRobert%2520West%2520and%2520Ashton%2520Anderson%2520and%2520Ece%2520Kamar%2520and%2520Eric%2520Horvitz%26entry.1292438233%3DAs%2520language%2520models%2520continue%2520to%2520rapidly%2520improve%252C%2520we%2520can%2520expect%2520their%2520actions%2520and%2520reasoning%2520to%2520become%2520difficult%2520or%2520impossible%2520for%2520weaker%2520agents%2520and%2520humans%2520to%2520follow%252C%2520undermining%2520interpretability%2520and%2520oversight.%2520With%2520an%2520eye%2520on%2520long-term%2520futures%252C%2520we%2520pursue%2520methods%2520that%2520encourage%2520models%2520to%2520produce%2520solutions%2520that%2520remain%2520intelligible%2520to%2520weaker%2520collaborators.%2520We%2520formalize%2520intelligibility%2520as%2520handoff%2520robustness%253A%2520a%2520strong%2520model%2527s%2520solution%2520is%2520intelligible%2520to%2520a%2520weaker%2520model%2520if%2520randomly%2520handing%2520off%2520control%2520to%2520the%2520weaker%2520model%2520along%2520the%2520solution%2520path%2520does%2520not%2520cause%2520failure.%2520Building%2520on%2520this%2520criterion%252C%2520we%2520introduce%2520tandem%2520training%2520for%2520language%2520models%252C%2520a%2520reinforcement%2520learning%2520%2528RL%2529%2520paradigm%2520in%2520which%2520rollout%2520tokens%2520are%2520intermittently%2520and%2520randomly%2520sampled%2520from%2520a%2520frozen%2520weak%2520model%2520rather%2520than%2520the%2520strong%2520model%2520being%2520trained.%2520Because%2520rollouts%2520succeed%2520only%2520when%2520the%2520strong%2520model%2527s%2520actions%2520and%2520reasoning%2520process%2520can%2520be%2520continued%2520by%2520the%2520weak%2520model%2520--%2520when%2520the%2520two%2520can%2520co-construct%2520a%2520successful%2520solution%2520--%2520optimizing%2520standard%2520RL%2520objectives%2520with%2520tandem%2520training%2520implicitly%2520incentivizes%2520both%2520correctness%2520and%2520intelligibility.%2520In%2520the%2520GSM8K%2520math%2520reasoning%2520task%252C%2520tandem%2520training%2520reliably%2520teaches%2520models%2520to%2520abandon%2520jargon%2520and%2520adapt%2520their%2520language%2520to%2520weaker%2520partners%2520while%2520keeping%2520task%2520accuracy%2520high.%2520Our%2520results%2520demonstrate%2520a%2520promising%2520route%2520to%2520building%2520AI%2520systems%2520that%2520remain%2520auditable%2520by%2520weaker%2520agents%252C%2520with%2520implications%2520for%2520human--AI%2520collaboration%2520and%2520multi-agent%2520communication.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13551v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tandem%20Training%20for%20Language%20Models&entry.906535625=Robert%20West%20and%20Ashton%20Anderson%20and%20Ece%20Kamar%20and%20Eric%20Horvitz&entry.1292438233=As%20language%20models%20continue%20to%20rapidly%20improve%2C%20we%20can%20expect%20their%20actions%20and%20reasoning%20to%20become%20difficult%20or%20impossible%20for%20weaker%20agents%20and%20humans%20to%20follow%2C%20undermining%20interpretability%20and%20oversight.%20With%20an%20eye%20on%20long-term%20futures%2C%20we%20pursue%20methods%20that%20encourage%20models%20to%20produce%20solutions%20that%20remain%20intelligible%20to%20weaker%20collaborators.%20We%20formalize%20intelligibility%20as%20handoff%20robustness%3A%20a%20strong%20model%27s%20solution%20is%20intelligible%20to%20a%20weaker%20model%20if%20randomly%20handing%20off%20control%20to%20the%20weaker%20model%20along%20the%20solution%20path%20does%20not%20cause%20failure.%20Building%20on%20this%20criterion%2C%20we%20introduce%20tandem%20training%20for%20language%20models%2C%20a%20reinforcement%20learning%20%28RL%29%20paradigm%20in%20which%20rollout%20tokens%20are%20intermittently%20and%20randomly%20sampled%20from%20a%20frozen%20weak%20model%20rather%20than%20the%20strong%20model%20being%20trained.%20Because%20rollouts%20succeed%20only%20when%20the%20strong%20model%27s%20actions%20and%20reasoning%20process%20can%20be%20continued%20by%20the%20weak%20model%20--%20when%20the%20two%20can%20co-construct%20a%20successful%20solution%20--%20optimizing%20standard%20RL%20objectives%20with%20tandem%20training%20implicitly%20incentivizes%20both%20correctness%20and%20intelligibility.%20In%20the%20GSM8K%20math%20reasoning%20task%2C%20tandem%20training%20reliably%20teaches%20models%20to%20abandon%20jargon%20and%20adapt%20their%20language%20to%20weaker%20partners%20while%20keeping%20task%20accuracy%20high.%20Our%20results%20demonstrate%20a%20promising%20route%20to%20building%20AI%20systems%20that%20remain%20auditable%20by%20weaker%20agents%2C%20with%20implications%20for%20human--AI%20collaboration%20and%20multi-agent%20communication.&entry.1838667208=http%3A//arxiv.org/abs/2510.13551v2&entry.124074799=Read"},
{"title": "Gaze Prediction in Virtual Reality Without Eye Tracking Using Visual and Head Motion Cues", "author": "Christos Petrou and Harris Partaourides and Athanasios Balomenos and Yannis Kopsinis and Sotirios Chatzis", "abstract": "Gaze prediction plays a critical role in Virtual Reality (VR) applications by reducing sensor-induced latency and enabling computationally demanding techniques such as foveated rendering, which rely on anticipating user attention. However, direct eye tracking is often unavailable due to hardware limitations or privacy concerns. To address this, we present a novel gaze prediction framework that combines Head-Mounted Display (HMD) motion signals with visual saliency cues derived from video frames. Our method employs UniSal, a lightweight saliency encoder, to extract visual features, which are then fused with HMD motion data and processed through a time-series prediction module. We evaluate two lightweight architectures, TSMixer and LSTM, for forecasting future gaze directions. Experiments on the EHTask dataset, along with deployment on commercial VR hardware, show that our approach consistently outperforms baselines such as Center-of-HMD and Mean Gaze. These results demonstrate the effectiveness of predictive gaze modeling in reducing perceptual lag and enhancing natural interaction in VR environments where direct eye tracking is constrained.", "link": "http://arxiv.org/abs/2601.18372v1", "date": "2026-01-26", "relevancy": 2.1636, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5494}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5448}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5308}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaze%20Prediction%20in%20Virtual%20Reality%20Without%20Eye%20Tracking%20Using%20Visual%20and%20Head%20Motion%20Cues&body=Title%3A%20Gaze%20Prediction%20in%20Virtual%20Reality%20Without%20Eye%20Tracking%20Using%20Visual%20and%20Head%20Motion%20Cues%0AAuthor%3A%20Christos%20Petrou%20and%20Harris%20Partaourides%20and%20Athanasios%20Balomenos%20and%20Yannis%20Kopsinis%20and%20Sotirios%20Chatzis%0AAbstract%3A%20Gaze%20prediction%20plays%20a%20critical%20role%20in%20Virtual%20Reality%20%28VR%29%20applications%20by%20reducing%20sensor-induced%20latency%20and%20enabling%20computationally%20demanding%20techniques%20such%20as%20foveated%20rendering%2C%20which%20rely%20on%20anticipating%20user%20attention.%20However%2C%20direct%20eye%20tracking%20is%20often%20unavailable%20due%20to%20hardware%20limitations%20or%20privacy%20concerns.%20To%20address%20this%2C%20we%20present%20a%20novel%20gaze%20prediction%20framework%20that%20combines%20Head-Mounted%20Display%20%28HMD%29%20motion%20signals%20with%20visual%20saliency%20cues%20derived%20from%20video%20frames.%20Our%20method%20employs%20UniSal%2C%20a%20lightweight%20saliency%20encoder%2C%20to%20extract%20visual%20features%2C%20which%20are%20then%20fused%20with%20HMD%20motion%20data%20and%20processed%20through%20a%20time-series%20prediction%20module.%20We%20evaluate%20two%20lightweight%20architectures%2C%20TSMixer%20and%20LSTM%2C%20for%20forecasting%20future%20gaze%20directions.%20Experiments%20on%20the%20EHTask%20dataset%2C%20along%20with%20deployment%20on%20commercial%20VR%20hardware%2C%20show%20that%20our%20approach%20consistently%20outperforms%20baselines%20such%20as%20Center-of-HMD%20and%20Mean%20Gaze.%20These%20results%20demonstrate%20the%20effectiveness%20of%20predictive%20gaze%20modeling%20in%20reducing%20perceptual%20lag%20and%20enhancing%20natural%20interaction%20in%20VR%20environments%20where%20direct%20eye%20tracking%20is%20constrained.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18372v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaze%2520Prediction%2520in%2520Virtual%2520Reality%2520Without%2520Eye%2520Tracking%2520Using%2520Visual%2520and%2520Head%2520Motion%2520Cues%26entry.906535625%3DChristos%2520Petrou%2520and%2520Harris%2520Partaourides%2520and%2520Athanasios%2520Balomenos%2520and%2520Yannis%2520Kopsinis%2520and%2520Sotirios%2520Chatzis%26entry.1292438233%3DGaze%2520prediction%2520plays%2520a%2520critical%2520role%2520in%2520Virtual%2520Reality%2520%2528VR%2529%2520applications%2520by%2520reducing%2520sensor-induced%2520latency%2520and%2520enabling%2520computationally%2520demanding%2520techniques%2520such%2520as%2520foveated%2520rendering%252C%2520which%2520rely%2520on%2520anticipating%2520user%2520attention.%2520However%252C%2520direct%2520eye%2520tracking%2520is%2520often%2520unavailable%2520due%2520to%2520hardware%2520limitations%2520or%2520privacy%2520concerns.%2520To%2520address%2520this%252C%2520we%2520present%2520a%2520novel%2520gaze%2520prediction%2520framework%2520that%2520combines%2520Head-Mounted%2520Display%2520%2528HMD%2529%2520motion%2520signals%2520with%2520visual%2520saliency%2520cues%2520derived%2520from%2520video%2520frames.%2520Our%2520method%2520employs%2520UniSal%252C%2520a%2520lightweight%2520saliency%2520encoder%252C%2520to%2520extract%2520visual%2520features%252C%2520which%2520are%2520then%2520fused%2520with%2520HMD%2520motion%2520data%2520and%2520processed%2520through%2520a%2520time-series%2520prediction%2520module.%2520We%2520evaluate%2520two%2520lightweight%2520architectures%252C%2520TSMixer%2520and%2520LSTM%252C%2520for%2520forecasting%2520future%2520gaze%2520directions.%2520Experiments%2520on%2520the%2520EHTask%2520dataset%252C%2520along%2520with%2520deployment%2520on%2520commercial%2520VR%2520hardware%252C%2520show%2520that%2520our%2520approach%2520consistently%2520outperforms%2520baselines%2520such%2520as%2520Center-of-HMD%2520and%2520Mean%2520Gaze.%2520These%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520predictive%2520gaze%2520modeling%2520in%2520reducing%2520perceptual%2520lag%2520and%2520enhancing%2520natural%2520interaction%2520in%2520VR%2520environments%2520where%2520direct%2520eye%2520tracking%2520is%2520constrained.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18372v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaze%20Prediction%20in%20Virtual%20Reality%20Without%20Eye%20Tracking%20Using%20Visual%20and%20Head%20Motion%20Cues&entry.906535625=Christos%20Petrou%20and%20Harris%20Partaourides%20and%20Athanasios%20Balomenos%20and%20Yannis%20Kopsinis%20and%20Sotirios%20Chatzis&entry.1292438233=Gaze%20prediction%20plays%20a%20critical%20role%20in%20Virtual%20Reality%20%28VR%29%20applications%20by%20reducing%20sensor-induced%20latency%20and%20enabling%20computationally%20demanding%20techniques%20such%20as%20foveated%20rendering%2C%20which%20rely%20on%20anticipating%20user%20attention.%20However%2C%20direct%20eye%20tracking%20is%20often%20unavailable%20due%20to%20hardware%20limitations%20or%20privacy%20concerns.%20To%20address%20this%2C%20we%20present%20a%20novel%20gaze%20prediction%20framework%20that%20combines%20Head-Mounted%20Display%20%28HMD%29%20motion%20signals%20with%20visual%20saliency%20cues%20derived%20from%20video%20frames.%20Our%20method%20employs%20UniSal%2C%20a%20lightweight%20saliency%20encoder%2C%20to%20extract%20visual%20features%2C%20which%20are%20then%20fused%20with%20HMD%20motion%20data%20and%20processed%20through%20a%20time-series%20prediction%20module.%20We%20evaluate%20two%20lightweight%20architectures%2C%20TSMixer%20and%20LSTM%2C%20for%20forecasting%20future%20gaze%20directions.%20Experiments%20on%20the%20EHTask%20dataset%2C%20along%20with%20deployment%20on%20commercial%20VR%20hardware%2C%20show%20that%20our%20approach%20consistently%20outperforms%20baselines%20such%20as%20Center-of-HMD%20and%20Mean%20Gaze.%20These%20results%20demonstrate%20the%20effectiveness%20of%20predictive%20gaze%20modeling%20in%20reducing%20perceptual%20lag%20and%20enhancing%20natural%20interaction%20in%20VR%20environments%20where%20direct%20eye%20tracking%20is%20constrained.&entry.1838667208=http%3A//arxiv.org/abs/2601.18372v1&entry.124074799=Read"},
{"title": "PPISP: Physically-Plausible Compensation and Control of Photometric Variations in Radiance Field Reconstruction", "author": "Isaac Deutsch and Nicolas Mo\u00ebnne-Loccoz and Gavriel State and Zan Gojcic", "abstract": "Multi-view 3D reconstruction methods remain highly sensitive to photometric inconsistencies arising from camera optical characteristics and variations in image signal processing (ISP). Existing mitigation strategies such as per-frame latent variables or affine color corrections lack physical grounding and generalize poorly to novel views. We propose the Physically-Plausible ISP (PPISP) correction module, which disentangles camera-intrinsic and capture-dependent effects through physically based and interpretable transformations. A dedicated PPISP controller, trained on the input views, predicts ISP parameters for novel viewpoints, analogous to auto exposure and auto white balance in real cameras. This design enables realistic and fair evaluation on novel views without access to ground-truth images. PPISP achieves SoTA performance on standard benchmarks, while providing intuitive control and supporting the integration of metadata when available. The source code is available at: https://github.com/nv-tlabs/ppisp", "link": "http://arxiv.org/abs/2601.18336v1", "date": "2026-01-26", "relevancy": 2.1629, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5624}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.526}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5249}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PPISP%3A%20Physically-Plausible%20Compensation%20and%20Control%20of%20Photometric%20Variations%20in%20Radiance%20Field%20Reconstruction&body=Title%3A%20PPISP%3A%20Physically-Plausible%20Compensation%20and%20Control%20of%20Photometric%20Variations%20in%20Radiance%20Field%20Reconstruction%0AAuthor%3A%20Isaac%20Deutsch%20and%20Nicolas%20Mo%C3%ABnne-Loccoz%20and%20Gavriel%20State%20and%20Zan%20Gojcic%0AAbstract%3A%20Multi-view%203D%20reconstruction%20methods%20remain%20highly%20sensitive%20to%20photometric%20inconsistencies%20arising%20from%20camera%20optical%20characteristics%20and%20variations%20in%20image%20signal%20processing%20%28ISP%29.%20Existing%20mitigation%20strategies%20such%20as%20per-frame%20latent%20variables%20or%20affine%20color%20corrections%20lack%20physical%20grounding%20and%20generalize%20poorly%20to%20novel%20views.%20We%20propose%20the%20Physically-Plausible%20ISP%20%28PPISP%29%20correction%20module%2C%20which%20disentangles%20camera-intrinsic%20and%20capture-dependent%20effects%20through%20physically%20based%20and%20interpretable%20transformations.%20A%20dedicated%20PPISP%20controller%2C%20trained%20on%20the%20input%20views%2C%20predicts%20ISP%20parameters%20for%20novel%20viewpoints%2C%20analogous%20to%20auto%20exposure%20and%20auto%20white%20balance%20in%20real%20cameras.%20This%20design%20enables%20realistic%20and%20fair%20evaluation%20on%20novel%20views%20without%20access%20to%20ground-truth%20images.%20PPISP%20achieves%20SoTA%20performance%20on%20standard%20benchmarks%2C%20while%20providing%20intuitive%20control%20and%20supporting%20the%20integration%20of%20metadata%20when%20available.%20The%20source%20code%20is%20available%20at%3A%20https%3A//github.com/nv-tlabs/ppisp%0ALink%3A%20http%3A//arxiv.org/abs/2601.18336v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPPISP%253A%2520Physically-Plausible%2520Compensation%2520and%2520Control%2520of%2520Photometric%2520Variations%2520in%2520Radiance%2520Field%2520Reconstruction%26entry.906535625%3DIsaac%2520Deutsch%2520and%2520Nicolas%2520Mo%25C3%25ABnne-Loccoz%2520and%2520Gavriel%2520State%2520and%2520Zan%2520Gojcic%26entry.1292438233%3DMulti-view%25203D%2520reconstruction%2520methods%2520remain%2520highly%2520sensitive%2520to%2520photometric%2520inconsistencies%2520arising%2520from%2520camera%2520optical%2520characteristics%2520and%2520variations%2520in%2520image%2520signal%2520processing%2520%2528ISP%2529.%2520Existing%2520mitigation%2520strategies%2520such%2520as%2520per-frame%2520latent%2520variables%2520or%2520affine%2520color%2520corrections%2520lack%2520physical%2520grounding%2520and%2520generalize%2520poorly%2520to%2520novel%2520views.%2520We%2520propose%2520the%2520Physically-Plausible%2520ISP%2520%2528PPISP%2529%2520correction%2520module%252C%2520which%2520disentangles%2520camera-intrinsic%2520and%2520capture-dependent%2520effects%2520through%2520physically%2520based%2520and%2520interpretable%2520transformations.%2520A%2520dedicated%2520PPISP%2520controller%252C%2520trained%2520on%2520the%2520input%2520views%252C%2520predicts%2520ISP%2520parameters%2520for%2520novel%2520viewpoints%252C%2520analogous%2520to%2520auto%2520exposure%2520and%2520auto%2520white%2520balance%2520in%2520real%2520cameras.%2520This%2520design%2520enables%2520realistic%2520and%2520fair%2520evaluation%2520on%2520novel%2520views%2520without%2520access%2520to%2520ground-truth%2520images.%2520PPISP%2520achieves%2520SoTA%2520performance%2520on%2520standard%2520benchmarks%252C%2520while%2520providing%2520intuitive%2520control%2520and%2520supporting%2520the%2520integration%2520of%2520metadata%2520when%2520available.%2520The%2520source%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/nv-tlabs/ppisp%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18336v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PPISP%3A%20Physically-Plausible%20Compensation%20and%20Control%20of%20Photometric%20Variations%20in%20Radiance%20Field%20Reconstruction&entry.906535625=Isaac%20Deutsch%20and%20Nicolas%20Mo%C3%ABnne-Loccoz%20and%20Gavriel%20State%20and%20Zan%20Gojcic&entry.1292438233=Multi-view%203D%20reconstruction%20methods%20remain%20highly%20sensitive%20to%20photometric%20inconsistencies%20arising%20from%20camera%20optical%20characteristics%20and%20variations%20in%20image%20signal%20processing%20%28ISP%29.%20Existing%20mitigation%20strategies%20such%20as%20per-frame%20latent%20variables%20or%20affine%20color%20corrections%20lack%20physical%20grounding%20and%20generalize%20poorly%20to%20novel%20views.%20We%20propose%20the%20Physically-Plausible%20ISP%20%28PPISP%29%20correction%20module%2C%20which%20disentangles%20camera-intrinsic%20and%20capture-dependent%20effects%20through%20physically%20based%20and%20interpretable%20transformations.%20A%20dedicated%20PPISP%20controller%2C%20trained%20on%20the%20input%20views%2C%20predicts%20ISP%20parameters%20for%20novel%20viewpoints%2C%20analogous%20to%20auto%20exposure%20and%20auto%20white%20balance%20in%20real%20cameras.%20This%20design%20enables%20realistic%20and%20fair%20evaluation%20on%20novel%20views%20without%20access%20to%20ground-truth%20images.%20PPISP%20achieves%20SoTA%20performance%20on%20standard%20benchmarks%2C%20while%20providing%20intuitive%20control%20and%20supporting%20the%20integration%20of%20metadata%20when%20available.%20The%20source%20code%20is%20available%20at%3A%20https%3A//github.com/nv-tlabs/ppisp&entry.1838667208=http%3A//arxiv.org/abs/2601.18336v1&entry.124074799=Read"},
{"title": "When Domain Pretraining Interferes with Instruction Alignment: An Empirical Study of Adapter Merging in Medical LLMs", "author": "Junyi Zou", "abstract": "Large language models (LLMs) show strong general capability but often struggle with medical terminology precision and safety-critical instruction following. We present a case study for adapter interference in safety-critical domains using a 14B-parameter base model through a two-stage LoRA pipeline: (1) domain-adaptive pre-training (PT) to inject broad medical knowledge via continued pre-training (DAPT), and (2) supervised fine-tuning (SFT) to align the model with medical question-answering behaviors through instruction-style data. To balance instruction-following ability and domain knowledge retention, we propose Weighted Adapter Merging, linearly combining SFT and PT adapters before exporting a merged base-model checkpoint. On a held-out medical validation set (F5/F6), the merged model achieves BLEU-4 = 16.38, ROUGE-1 = 20.42, ROUGE-2 = 4.60, and ROUGE-L = 11.54 under a practical decoding configuration. We further analyze decoding sensitivity and training stability with loss curves and controlled decoding comparisons.", "link": "http://arxiv.org/abs/2601.18350v1", "date": "2026-01-26", "relevancy": 2.1617, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5854}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5174}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Domain%20Pretraining%20Interferes%20with%20Instruction%20Alignment%3A%20An%20Empirical%20Study%20of%20Adapter%20Merging%20in%20Medical%20LLMs&body=Title%3A%20When%20Domain%20Pretraining%20Interferes%20with%20Instruction%20Alignment%3A%20An%20Empirical%20Study%20of%20Adapter%20Merging%20in%20Medical%20LLMs%0AAuthor%3A%20Junyi%20Zou%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20show%20strong%20general%20capability%20but%20often%20struggle%20with%20medical%20terminology%20precision%20and%20safety-critical%20instruction%20following.%20We%20present%20a%20case%20study%20for%20adapter%20interference%20in%20safety-critical%20domains%20using%20a%2014B-parameter%20base%20model%20through%20a%20two-stage%20LoRA%20pipeline%3A%20%281%29%20domain-adaptive%20pre-training%20%28PT%29%20to%20inject%20broad%20medical%20knowledge%20via%20continued%20pre-training%20%28DAPT%29%2C%20and%20%282%29%20supervised%20fine-tuning%20%28SFT%29%20to%20align%20the%20model%20with%20medical%20question-answering%20behaviors%20through%20instruction-style%20data.%20To%20balance%20instruction-following%20ability%20and%20domain%20knowledge%20retention%2C%20we%20propose%20Weighted%20Adapter%20Merging%2C%20linearly%20combining%20SFT%20and%20PT%20adapters%20before%20exporting%20a%20merged%20base-model%20checkpoint.%20On%20a%20held-out%20medical%20validation%20set%20%28F5/F6%29%2C%20the%20merged%20model%20achieves%20BLEU-4%20%3D%2016.38%2C%20ROUGE-1%20%3D%2020.42%2C%20ROUGE-2%20%3D%204.60%2C%20and%20ROUGE-L%20%3D%2011.54%20under%20a%20practical%20decoding%20configuration.%20We%20further%20analyze%20decoding%20sensitivity%20and%20training%20stability%20with%20loss%20curves%20and%20controlled%20decoding%20comparisons.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18350v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Domain%2520Pretraining%2520Interferes%2520with%2520Instruction%2520Alignment%253A%2520An%2520Empirical%2520Study%2520of%2520Adapter%2520Merging%2520in%2520Medical%2520LLMs%26entry.906535625%3DJunyi%2520Zou%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520show%2520strong%2520general%2520capability%2520but%2520often%2520struggle%2520with%2520medical%2520terminology%2520precision%2520and%2520safety-critical%2520instruction%2520following.%2520We%2520present%2520a%2520case%2520study%2520for%2520adapter%2520interference%2520in%2520safety-critical%2520domains%2520using%2520a%252014B-parameter%2520base%2520model%2520through%2520a%2520two-stage%2520LoRA%2520pipeline%253A%2520%25281%2529%2520domain-adaptive%2520pre-training%2520%2528PT%2529%2520to%2520inject%2520broad%2520medical%2520knowledge%2520via%2520continued%2520pre-training%2520%2528DAPT%2529%252C%2520and%2520%25282%2529%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520to%2520align%2520the%2520model%2520with%2520medical%2520question-answering%2520behaviors%2520through%2520instruction-style%2520data.%2520To%2520balance%2520instruction-following%2520ability%2520and%2520domain%2520knowledge%2520retention%252C%2520we%2520propose%2520Weighted%2520Adapter%2520Merging%252C%2520linearly%2520combining%2520SFT%2520and%2520PT%2520adapters%2520before%2520exporting%2520a%2520merged%2520base-model%2520checkpoint.%2520On%2520a%2520held-out%2520medical%2520validation%2520set%2520%2528F5/F6%2529%252C%2520the%2520merged%2520model%2520achieves%2520BLEU-4%2520%253D%252016.38%252C%2520ROUGE-1%2520%253D%252020.42%252C%2520ROUGE-2%2520%253D%25204.60%252C%2520and%2520ROUGE-L%2520%253D%252011.54%2520under%2520a%2520practical%2520decoding%2520configuration.%2520We%2520further%2520analyze%2520decoding%2520sensitivity%2520and%2520training%2520stability%2520with%2520loss%2520curves%2520and%2520controlled%2520decoding%2520comparisons.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18350v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Domain%20Pretraining%20Interferes%20with%20Instruction%20Alignment%3A%20An%20Empirical%20Study%20of%20Adapter%20Merging%20in%20Medical%20LLMs&entry.906535625=Junyi%20Zou&entry.1292438233=Large%20language%20models%20%28LLMs%29%20show%20strong%20general%20capability%20but%20often%20struggle%20with%20medical%20terminology%20precision%20and%20safety-critical%20instruction%20following.%20We%20present%20a%20case%20study%20for%20adapter%20interference%20in%20safety-critical%20domains%20using%20a%2014B-parameter%20base%20model%20through%20a%20two-stage%20LoRA%20pipeline%3A%20%281%29%20domain-adaptive%20pre-training%20%28PT%29%20to%20inject%20broad%20medical%20knowledge%20via%20continued%20pre-training%20%28DAPT%29%2C%20and%20%282%29%20supervised%20fine-tuning%20%28SFT%29%20to%20align%20the%20model%20with%20medical%20question-answering%20behaviors%20through%20instruction-style%20data.%20To%20balance%20instruction-following%20ability%20and%20domain%20knowledge%20retention%2C%20we%20propose%20Weighted%20Adapter%20Merging%2C%20linearly%20combining%20SFT%20and%20PT%20adapters%20before%20exporting%20a%20merged%20base-model%20checkpoint.%20On%20a%20held-out%20medical%20validation%20set%20%28F5/F6%29%2C%20the%20merged%20model%20achieves%20BLEU-4%20%3D%2016.38%2C%20ROUGE-1%20%3D%2020.42%2C%20ROUGE-2%20%3D%204.60%2C%20and%20ROUGE-L%20%3D%2011.54%20under%20a%20practical%20decoding%20configuration.%20We%20further%20analyze%20decoding%20sensitivity%20and%20training%20stability%20with%20loss%20curves%20and%20controlled%20decoding%20comparisons.&entry.1838667208=http%3A//arxiv.org/abs/2601.18350v1&entry.124074799=Read"},
{"title": "When Swin Transformer Meets KANs: An Improved Transformer Architecture for Medical Image Segmentation", "author": "Nishchal Sapkota and Haoyan Shi and Yejia Zhang and Xianshi Ma and Bofang Zheng and Fabian Vazquez and Pengfei Gu and Danny Z. Chen", "abstract": "Medical image segmentation is critical for accurate diagnostics and treatment planning, but remains challenging due to complex anatomical structures and limited annotated training data. CNN-based segmentation methods excel at local feature extraction, but struggle with modeling long-range dependencies. Transformers, on the other hand, capture global context more effectively, but are inherently data-hungry and computationally expensive. In this work, we introduce UKAST, a U-Net like architecture that integrates rational-function based Kolmogorov-Arnold Networks (KANs) into Swin Transformer encoders. By leveraging rational base functions and Group Rational KANs (GR-KANs) from the Kolmogorov-Arnold Transformer (KAT), our architecture addresses the inefficiencies of vanilla spline-based KANs, yielding a more expressive and data-efficient framework with reduced FLOPs and only a very small increase in parameter count compared to SwinUNETR. UKAST achieves state-of-the-art performance on four diverse 2D and 3D medical image segmentation benchmarks, consistently surpassing both CNN- and Transformer-based baselines. Notably, it attains superior accuracy in data-scarce settings, alleviating the data-hungry limitations of standard Vision Transformers. These results show the potential of KAN-enhanced Transformers to advance data-efficient medical image segmentation. Code is available at: https://github.com/nsapkota417/UKAST", "link": "http://arxiv.org/abs/2511.04084v2", "date": "2026-01-26", "relevancy": 2.1612, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5621}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5251}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Swin%20Transformer%20Meets%20KANs%3A%20An%20Improved%20Transformer%20Architecture%20for%20Medical%20Image%20Segmentation&body=Title%3A%20When%20Swin%20Transformer%20Meets%20KANs%3A%20An%20Improved%20Transformer%20Architecture%20for%20Medical%20Image%20Segmentation%0AAuthor%3A%20Nishchal%20Sapkota%20and%20Haoyan%20Shi%20and%20Yejia%20Zhang%20and%20Xianshi%20Ma%20and%20Bofang%20Zheng%20and%20Fabian%20Vazquez%20and%20Pengfei%20Gu%20and%20Danny%20Z.%20Chen%0AAbstract%3A%20Medical%20image%20segmentation%20is%20critical%20for%20accurate%20diagnostics%20and%20treatment%20planning%2C%20but%20remains%20challenging%20due%20to%20complex%20anatomical%20structures%20and%20limited%20annotated%20training%20data.%20CNN-based%20segmentation%20methods%20excel%20at%20local%20feature%20extraction%2C%20but%20struggle%20with%20modeling%20long-range%20dependencies.%20Transformers%2C%20on%20the%20other%20hand%2C%20capture%20global%20context%20more%20effectively%2C%20but%20are%20inherently%20data-hungry%20and%20computationally%20expensive.%20In%20this%20work%2C%20we%20introduce%20UKAST%2C%20a%20U-Net%20like%20architecture%20that%20integrates%20rational-function%20based%20Kolmogorov-Arnold%20Networks%20%28KANs%29%20into%20Swin%20Transformer%20encoders.%20By%20leveraging%20rational%20base%20functions%20and%20Group%20Rational%20KANs%20%28GR-KANs%29%20from%20the%20Kolmogorov-Arnold%20Transformer%20%28KAT%29%2C%20our%20architecture%20addresses%20the%20inefficiencies%20of%20vanilla%20spline-based%20KANs%2C%20yielding%20a%20more%20expressive%20and%20data-efficient%20framework%20with%20reduced%20FLOPs%20and%20only%20a%20very%20small%20increase%20in%20parameter%20count%20compared%20to%20SwinUNETR.%20UKAST%20achieves%20state-of-the-art%20performance%20on%20four%20diverse%202D%20and%203D%20medical%20image%20segmentation%20benchmarks%2C%20consistently%20surpassing%20both%20CNN-%20and%20Transformer-based%20baselines.%20Notably%2C%20it%20attains%20superior%20accuracy%20in%20data-scarce%20settings%2C%20alleviating%20the%20data-hungry%20limitations%20of%20standard%20Vision%20Transformers.%20These%20results%20show%20the%20potential%20of%20KAN-enhanced%20Transformers%20to%20advance%20data-efficient%20medical%20image%20segmentation.%20Code%20is%20available%20at%3A%20https%3A//github.com/nsapkota417/UKAST%0ALink%3A%20http%3A//arxiv.org/abs/2511.04084v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Swin%2520Transformer%2520Meets%2520KANs%253A%2520An%2520Improved%2520Transformer%2520Architecture%2520for%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DNishchal%2520Sapkota%2520and%2520Haoyan%2520Shi%2520and%2520Yejia%2520Zhang%2520and%2520Xianshi%2520Ma%2520and%2520Bofang%2520Zheng%2520and%2520Fabian%2520Vazquez%2520and%2520Pengfei%2520Gu%2520and%2520Danny%2520Z.%2520Chen%26entry.1292438233%3DMedical%2520image%2520segmentation%2520is%2520critical%2520for%2520accurate%2520diagnostics%2520and%2520treatment%2520planning%252C%2520but%2520remains%2520challenging%2520due%2520to%2520complex%2520anatomical%2520structures%2520and%2520limited%2520annotated%2520training%2520data.%2520CNN-based%2520segmentation%2520methods%2520excel%2520at%2520local%2520feature%2520extraction%252C%2520but%2520struggle%2520with%2520modeling%2520long-range%2520dependencies.%2520Transformers%252C%2520on%2520the%2520other%2520hand%252C%2520capture%2520global%2520context%2520more%2520effectively%252C%2520but%2520are%2520inherently%2520data-hungry%2520and%2520computationally%2520expensive.%2520In%2520this%2520work%252C%2520we%2520introduce%2520UKAST%252C%2520a%2520U-Net%2520like%2520architecture%2520that%2520integrates%2520rational-function%2520based%2520Kolmogorov-Arnold%2520Networks%2520%2528KANs%2529%2520into%2520Swin%2520Transformer%2520encoders.%2520By%2520leveraging%2520rational%2520base%2520functions%2520and%2520Group%2520Rational%2520KANs%2520%2528GR-KANs%2529%2520from%2520the%2520Kolmogorov-Arnold%2520Transformer%2520%2528KAT%2529%252C%2520our%2520architecture%2520addresses%2520the%2520inefficiencies%2520of%2520vanilla%2520spline-based%2520KANs%252C%2520yielding%2520a%2520more%2520expressive%2520and%2520data-efficient%2520framework%2520with%2520reduced%2520FLOPs%2520and%2520only%2520a%2520very%2520small%2520increase%2520in%2520parameter%2520count%2520compared%2520to%2520SwinUNETR.%2520UKAST%2520achieves%2520state-of-the-art%2520performance%2520on%2520four%2520diverse%25202D%2520and%25203D%2520medical%2520image%2520segmentation%2520benchmarks%252C%2520consistently%2520surpassing%2520both%2520CNN-%2520and%2520Transformer-based%2520baselines.%2520Notably%252C%2520it%2520attains%2520superior%2520accuracy%2520in%2520data-scarce%2520settings%252C%2520alleviating%2520the%2520data-hungry%2520limitations%2520of%2520standard%2520Vision%2520Transformers.%2520These%2520results%2520show%2520the%2520potential%2520of%2520KAN-enhanced%2520Transformers%2520to%2520advance%2520data-efficient%2520medical%2520image%2520segmentation.%2520Code%2520is%2520available%2520at%253A%2520https%253A//github.com/nsapkota417/UKAST%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.04084v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Swin%20Transformer%20Meets%20KANs%3A%20An%20Improved%20Transformer%20Architecture%20for%20Medical%20Image%20Segmentation&entry.906535625=Nishchal%20Sapkota%20and%20Haoyan%20Shi%20and%20Yejia%20Zhang%20and%20Xianshi%20Ma%20and%20Bofang%20Zheng%20and%20Fabian%20Vazquez%20and%20Pengfei%20Gu%20and%20Danny%20Z.%20Chen&entry.1292438233=Medical%20image%20segmentation%20is%20critical%20for%20accurate%20diagnostics%20and%20treatment%20planning%2C%20but%20remains%20challenging%20due%20to%20complex%20anatomical%20structures%20and%20limited%20annotated%20training%20data.%20CNN-based%20segmentation%20methods%20excel%20at%20local%20feature%20extraction%2C%20but%20struggle%20with%20modeling%20long-range%20dependencies.%20Transformers%2C%20on%20the%20other%20hand%2C%20capture%20global%20context%20more%20effectively%2C%20but%20are%20inherently%20data-hungry%20and%20computationally%20expensive.%20In%20this%20work%2C%20we%20introduce%20UKAST%2C%20a%20U-Net%20like%20architecture%20that%20integrates%20rational-function%20based%20Kolmogorov-Arnold%20Networks%20%28KANs%29%20into%20Swin%20Transformer%20encoders.%20By%20leveraging%20rational%20base%20functions%20and%20Group%20Rational%20KANs%20%28GR-KANs%29%20from%20the%20Kolmogorov-Arnold%20Transformer%20%28KAT%29%2C%20our%20architecture%20addresses%20the%20inefficiencies%20of%20vanilla%20spline-based%20KANs%2C%20yielding%20a%20more%20expressive%20and%20data-efficient%20framework%20with%20reduced%20FLOPs%20and%20only%20a%20very%20small%20increase%20in%20parameter%20count%20compared%20to%20SwinUNETR.%20UKAST%20achieves%20state-of-the-art%20performance%20on%20four%20diverse%202D%20and%203D%20medical%20image%20segmentation%20benchmarks%2C%20consistently%20surpassing%20both%20CNN-%20and%20Transformer-based%20baselines.%20Notably%2C%20it%20attains%20superior%20accuracy%20in%20data-scarce%20settings%2C%20alleviating%20the%20data-hungry%20limitations%20of%20standard%20Vision%20Transformers.%20These%20results%20show%20the%20potential%20of%20KAN-enhanced%20Transformers%20to%20advance%20data-efficient%20medical%20image%20segmentation.%20Code%20is%20available%20at%3A%20https%3A//github.com/nsapkota417/UKAST&entry.1838667208=http%3A//arxiv.org/abs/2511.04084v2&entry.124074799=Read"},
{"title": "Actor-Critic Cooperative Compensation to Model Predictive Control for Off-Road Autonomous Vehicles Under Unknown Dynamics", "author": "Prakhar Gupta and Jonathon M Smereka and Yunyi Jia", "abstract": "This study presents an Actor-Critic Cooperative Compensated Model Predictive Controller (AC3MPC) designed to address unknown system dynamics. To avoid the difficulty of modeling highly complex dynamics and ensuring realtime control feasibility and performance, this work uses deep reinforcement learning with a model predictive controller in a cooperative framework to handle unknown dynamics. The model-based controller takes on the primary role as both controllers are provided with predictive information about the other. This improves tracking performance and retention of inherent robustness of the model predictive controller. We evaluate this framework for off-road autonomous driving on unknown deformable terrains that represent sandy deformable soil, sandy and rocky soil, and cohesive clay-like deformable soil. Our findings demonstrate that our controller statistically outperforms standalone model-based and learning-based controllers by upto 29.2% and 10.2%. This framework generalized well over varied and previously unseen terrain characteristics to track longitudinal reference speeds with lower errors. Furthermore, this required significantly less training data compared to purely learning-based controller, while delivering better performance even when under-trained.", "link": "http://arxiv.org/abs/2503.00577v2", "date": "2026-01-26", "relevancy": 2.1608, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5642}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5375}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Actor-Critic%20Cooperative%20Compensation%20to%20Model%20Predictive%20Control%20for%20Off-Road%20Autonomous%20Vehicles%20Under%20Unknown%20Dynamics&body=Title%3A%20Actor-Critic%20Cooperative%20Compensation%20to%20Model%20Predictive%20Control%20for%20Off-Road%20Autonomous%20Vehicles%20Under%20Unknown%20Dynamics%0AAuthor%3A%20Prakhar%20Gupta%20and%20Jonathon%20M%20Smereka%20and%20Yunyi%20Jia%0AAbstract%3A%20This%20study%20presents%20an%20Actor-Critic%20Cooperative%20Compensated%20Model%20Predictive%20Controller%20%28AC3MPC%29%20designed%20to%20address%20unknown%20system%20dynamics.%20To%20avoid%20the%20difficulty%20of%20modeling%20highly%20complex%20dynamics%20and%20ensuring%20realtime%20control%20feasibility%20and%20performance%2C%20this%20work%20uses%20deep%20reinforcement%20learning%20with%20a%20model%20predictive%20controller%20in%20a%20cooperative%20framework%20to%20handle%20unknown%20dynamics.%20The%20model-based%20controller%20takes%20on%20the%20primary%20role%20as%20both%20controllers%20are%20provided%20with%20predictive%20information%20about%20the%20other.%20This%20improves%20tracking%20performance%20and%20retention%20of%20inherent%20robustness%20of%20the%20model%20predictive%20controller.%20We%20evaluate%20this%20framework%20for%20off-road%20autonomous%20driving%20on%20unknown%20deformable%20terrains%20that%20represent%20sandy%20deformable%20soil%2C%20sandy%20and%20rocky%20soil%2C%20and%20cohesive%20clay-like%20deformable%20soil.%20Our%20findings%20demonstrate%20that%20our%20controller%20statistically%20outperforms%20standalone%20model-based%20and%20learning-based%20controllers%20by%20upto%2029.2%25%20and%2010.2%25.%20This%20framework%20generalized%20well%20over%20varied%20and%20previously%20unseen%20terrain%20characteristics%20to%20track%20longitudinal%20reference%20speeds%20with%20lower%20errors.%20Furthermore%2C%20this%20required%20significantly%20less%20training%20data%20compared%20to%20purely%20learning-based%20controller%2C%20while%20delivering%20better%20performance%20even%20when%20under-trained.%0ALink%3A%20http%3A//arxiv.org/abs/2503.00577v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActor-Critic%2520Cooperative%2520Compensation%2520to%2520Model%2520Predictive%2520Control%2520for%2520Off-Road%2520Autonomous%2520Vehicles%2520Under%2520Unknown%2520Dynamics%26entry.906535625%3DPrakhar%2520Gupta%2520and%2520Jonathon%2520M%2520Smereka%2520and%2520Yunyi%2520Jia%26entry.1292438233%3DThis%2520study%2520presents%2520an%2520Actor-Critic%2520Cooperative%2520Compensated%2520Model%2520Predictive%2520Controller%2520%2528AC3MPC%2529%2520designed%2520to%2520address%2520unknown%2520system%2520dynamics.%2520To%2520avoid%2520the%2520difficulty%2520of%2520modeling%2520highly%2520complex%2520dynamics%2520and%2520ensuring%2520realtime%2520control%2520feasibility%2520and%2520performance%252C%2520this%2520work%2520uses%2520deep%2520reinforcement%2520learning%2520with%2520a%2520model%2520predictive%2520controller%2520in%2520a%2520cooperative%2520framework%2520to%2520handle%2520unknown%2520dynamics.%2520The%2520model-based%2520controller%2520takes%2520on%2520the%2520primary%2520role%2520as%2520both%2520controllers%2520are%2520provided%2520with%2520predictive%2520information%2520about%2520the%2520other.%2520This%2520improves%2520tracking%2520performance%2520and%2520retention%2520of%2520inherent%2520robustness%2520of%2520the%2520model%2520predictive%2520controller.%2520We%2520evaluate%2520this%2520framework%2520for%2520off-road%2520autonomous%2520driving%2520on%2520unknown%2520deformable%2520terrains%2520that%2520represent%2520sandy%2520deformable%2520soil%252C%2520sandy%2520and%2520rocky%2520soil%252C%2520and%2520cohesive%2520clay-like%2520deformable%2520soil.%2520Our%2520findings%2520demonstrate%2520that%2520our%2520controller%2520statistically%2520outperforms%2520standalone%2520model-based%2520and%2520learning-based%2520controllers%2520by%2520upto%252029.2%2525%2520and%252010.2%2525.%2520This%2520framework%2520generalized%2520well%2520over%2520varied%2520and%2520previously%2520unseen%2520terrain%2520characteristics%2520to%2520track%2520longitudinal%2520reference%2520speeds%2520with%2520lower%2520errors.%2520Furthermore%252C%2520this%2520required%2520significantly%2520less%2520training%2520data%2520compared%2520to%2520purely%2520learning-based%2520controller%252C%2520while%2520delivering%2520better%2520performance%2520even%2520when%2520under-trained.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.00577v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Actor-Critic%20Cooperative%20Compensation%20to%20Model%20Predictive%20Control%20for%20Off-Road%20Autonomous%20Vehicles%20Under%20Unknown%20Dynamics&entry.906535625=Prakhar%20Gupta%20and%20Jonathon%20M%20Smereka%20and%20Yunyi%20Jia&entry.1292438233=This%20study%20presents%20an%20Actor-Critic%20Cooperative%20Compensated%20Model%20Predictive%20Controller%20%28AC3MPC%29%20designed%20to%20address%20unknown%20system%20dynamics.%20To%20avoid%20the%20difficulty%20of%20modeling%20highly%20complex%20dynamics%20and%20ensuring%20realtime%20control%20feasibility%20and%20performance%2C%20this%20work%20uses%20deep%20reinforcement%20learning%20with%20a%20model%20predictive%20controller%20in%20a%20cooperative%20framework%20to%20handle%20unknown%20dynamics.%20The%20model-based%20controller%20takes%20on%20the%20primary%20role%20as%20both%20controllers%20are%20provided%20with%20predictive%20information%20about%20the%20other.%20This%20improves%20tracking%20performance%20and%20retention%20of%20inherent%20robustness%20of%20the%20model%20predictive%20controller.%20We%20evaluate%20this%20framework%20for%20off-road%20autonomous%20driving%20on%20unknown%20deformable%20terrains%20that%20represent%20sandy%20deformable%20soil%2C%20sandy%20and%20rocky%20soil%2C%20and%20cohesive%20clay-like%20deformable%20soil.%20Our%20findings%20demonstrate%20that%20our%20controller%20statistically%20outperforms%20standalone%20model-based%20and%20learning-based%20controllers%20by%20upto%2029.2%25%20and%2010.2%25.%20This%20framework%20generalized%20well%20over%20varied%20and%20previously%20unseen%20terrain%20characteristics%20to%20track%20longitudinal%20reference%20speeds%20with%20lower%20errors.%20Furthermore%2C%20this%20required%20significantly%20less%20training%20data%20compared%20to%20purely%20learning-based%20controller%2C%20while%20delivering%20better%20performance%20even%20when%20under-trained.&entry.1838667208=http%3A//arxiv.org/abs/2503.00577v2&entry.124074799=Read"},
{"title": "A Computationally Efficient Maximum A Posteriori Sequence Estimation via Stein Variational Inference", "author": "Min-Won Seo and Solmaz S. Kia", "abstract": "State estimation in robotic systems presents significant challenges, particularly due to the prevalence of multimodal posterior distributions in real-world scenarios. One effective strategy for handling such complexity is to compute maximum a posteriori (MAP) sequences over a discretized or sampled state space, which enables a concise representation of the most likely state trajectory. However, this approach often incurs substantial computational costs, especially in high-dimensional settings. In this article, we propose a novel MAP sequence estimation method, Stein-MAP-Seq, which effectively addresses multimodality while substantially reducing computational and memory overhead. Our key contribution is a sequential variational inference framework that captures temporal dependencies in dynamical system models and integrates Stein variational gradient descent (SVGD) into a Viterbi-style dynamic programming algorithm, enabling computationally efficient MAP sequence estimation. This integration allows the method to focus computational effort on MAP-consistent modes rather than exhaustively exploring the entire state space. Stein-MAP-Seq inherits the parallelism and mode-seeking behavior of SVGD, allowing particle updates to be efficiently executed on parallel hardware and significantly reducing the number of trajectory candidates required for MAP-sequence recursion compared to conventional methods that rely on hundreds to thousands of particles. We validate the proposed approach on a range of highly multimodal scenarios, including nonlinear dynamics with ambiguous observations, unknown data association with outliers, range-only localization under temporary unobservability, and high-dimensional robotic manipulators. Experimental results demonstrate substantial improvements in estimation accuracy and robustness to multimodality over existing estimation methods.", "link": "http://arxiv.org/abs/2312.08684v4", "date": "2026-01-26", "relevancy": 2.1583, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.636}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5314}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5091}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Computationally%20Efficient%20Maximum%20A%20Posteriori%20Sequence%20Estimation%20via%20Stein%20Variational%20Inference&body=Title%3A%20A%20Computationally%20Efficient%20Maximum%20A%20Posteriori%20Sequence%20Estimation%20via%20Stein%20Variational%20Inference%0AAuthor%3A%20Min-Won%20Seo%20and%20Solmaz%20S.%20Kia%0AAbstract%3A%20State%20estimation%20in%20robotic%20systems%20presents%20significant%20challenges%2C%20particularly%20due%20to%20the%20prevalence%20of%20multimodal%20posterior%20distributions%20in%20real-world%20scenarios.%20One%20effective%20strategy%20for%20handling%20such%20complexity%20is%20to%20compute%20maximum%20a%20posteriori%20%28MAP%29%20sequences%20over%20a%20discretized%20or%20sampled%20state%20space%2C%20which%20enables%20a%20concise%20representation%20of%20the%20most%20likely%20state%20trajectory.%20However%2C%20this%20approach%20often%20incurs%20substantial%20computational%20costs%2C%20especially%20in%20high-dimensional%20settings.%20In%20this%20article%2C%20we%20propose%20a%20novel%20MAP%20sequence%20estimation%20method%2C%20Stein-MAP-Seq%2C%20which%20effectively%20addresses%20multimodality%20while%20substantially%20reducing%20computational%20and%20memory%20overhead.%20Our%20key%20contribution%20is%20a%20sequential%20variational%20inference%20framework%20that%20captures%20temporal%20dependencies%20in%20dynamical%20system%20models%20and%20integrates%20Stein%20variational%20gradient%20descent%20%28SVGD%29%20into%20a%20Viterbi-style%20dynamic%20programming%20algorithm%2C%20enabling%20computationally%20efficient%20MAP%20sequence%20estimation.%20This%20integration%20allows%20the%20method%20to%20focus%20computational%20effort%20on%20MAP-consistent%20modes%20rather%20than%20exhaustively%20exploring%20the%20entire%20state%20space.%20Stein-MAP-Seq%20inherits%20the%20parallelism%20and%20mode-seeking%20behavior%20of%20SVGD%2C%20allowing%20particle%20updates%20to%20be%20efficiently%20executed%20on%20parallel%20hardware%20and%20significantly%20reducing%20the%20number%20of%20trajectory%20candidates%20required%20for%20MAP-sequence%20recursion%20compared%20to%20conventional%20methods%20that%20rely%20on%20hundreds%20to%20thousands%20of%20particles.%20We%20validate%20the%20proposed%20approach%20on%20a%20range%20of%20highly%20multimodal%20scenarios%2C%20including%20nonlinear%20dynamics%20with%20ambiguous%20observations%2C%20unknown%20data%20association%20with%20outliers%2C%20range-only%20localization%20under%20temporary%20unobservability%2C%20and%20high-dimensional%20robotic%20manipulators.%20Experimental%20results%20demonstrate%20substantial%20improvements%20in%20estimation%20accuracy%20and%20robustness%20to%20multimodality%20over%20existing%20estimation%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2312.08684v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Computationally%2520Efficient%2520Maximum%2520A%2520Posteriori%2520Sequence%2520Estimation%2520via%2520Stein%2520Variational%2520Inference%26entry.906535625%3DMin-Won%2520Seo%2520and%2520Solmaz%2520S.%2520Kia%26entry.1292438233%3DState%2520estimation%2520in%2520robotic%2520systems%2520presents%2520significant%2520challenges%252C%2520particularly%2520due%2520to%2520the%2520prevalence%2520of%2520multimodal%2520posterior%2520distributions%2520in%2520real-world%2520scenarios.%2520One%2520effective%2520strategy%2520for%2520handling%2520such%2520complexity%2520is%2520to%2520compute%2520maximum%2520a%2520posteriori%2520%2528MAP%2529%2520sequences%2520over%2520a%2520discretized%2520or%2520sampled%2520state%2520space%252C%2520which%2520enables%2520a%2520concise%2520representation%2520of%2520the%2520most%2520likely%2520state%2520trajectory.%2520However%252C%2520this%2520approach%2520often%2520incurs%2520substantial%2520computational%2520costs%252C%2520especially%2520in%2520high-dimensional%2520settings.%2520In%2520this%2520article%252C%2520we%2520propose%2520a%2520novel%2520MAP%2520sequence%2520estimation%2520method%252C%2520Stein-MAP-Seq%252C%2520which%2520effectively%2520addresses%2520multimodality%2520while%2520substantially%2520reducing%2520computational%2520and%2520memory%2520overhead.%2520Our%2520key%2520contribution%2520is%2520a%2520sequential%2520variational%2520inference%2520framework%2520that%2520captures%2520temporal%2520dependencies%2520in%2520dynamical%2520system%2520models%2520and%2520integrates%2520Stein%2520variational%2520gradient%2520descent%2520%2528SVGD%2529%2520into%2520a%2520Viterbi-style%2520dynamic%2520programming%2520algorithm%252C%2520enabling%2520computationally%2520efficient%2520MAP%2520sequence%2520estimation.%2520This%2520integration%2520allows%2520the%2520method%2520to%2520focus%2520computational%2520effort%2520on%2520MAP-consistent%2520modes%2520rather%2520than%2520exhaustively%2520exploring%2520the%2520entire%2520state%2520space.%2520Stein-MAP-Seq%2520inherits%2520the%2520parallelism%2520and%2520mode-seeking%2520behavior%2520of%2520SVGD%252C%2520allowing%2520particle%2520updates%2520to%2520be%2520efficiently%2520executed%2520on%2520parallel%2520hardware%2520and%2520significantly%2520reducing%2520the%2520number%2520of%2520trajectory%2520candidates%2520required%2520for%2520MAP-sequence%2520recursion%2520compared%2520to%2520conventional%2520methods%2520that%2520rely%2520on%2520hundreds%2520to%2520thousands%2520of%2520particles.%2520We%2520validate%2520the%2520proposed%2520approach%2520on%2520a%2520range%2520of%2520highly%2520multimodal%2520scenarios%252C%2520including%2520nonlinear%2520dynamics%2520with%2520ambiguous%2520observations%252C%2520unknown%2520data%2520association%2520with%2520outliers%252C%2520range-only%2520localization%2520under%2520temporary%2520unobservability%252C%2520and%2520high-dimensional%2520robotic%2520manipulators.%2520Experimental%2520results%2520demonstrate%2520substantial%2520improvements%2520in%2520estimation%2520accuracy%2520and%2520robustness%2520to%2520multimodality%2520over%2520existing%2520estimation%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.08684v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Computationally%20Efficient%20Maximum%20A%20Posteriori%20Sequence%20Estimation%20via%20Stein%20Variational%20Inference&entry.906535625=Min-Won%20Seo%20and%20Solmaz%20S.%20Kia&entry.1292438233=State%20estimation%20in%20robotic%20systems%20presents%20significant%20challenges%2C%20particularly%20due%20to%20the%20prevalence%20of%20multimodal%20posterior%20distributions%20in%20real-world%20scenarios.%20One%20effective%20strategy%20for%20handling%20such%20complexity%20is%20to%20compute%20maximum%20a%20posteriori%20%28MAP%29%20sequences%20over%20a%20discretized%20or%20sampled%20state%20space%2C%20which%20enables%20a%20concise%20representation%20of%20the%20most%20likely%20state%20trajectory.%20However%2C%20this%20approach%20often%20incurs%20substantial%20computational%20costs%2C%20especially%20in%20high-dimensional%20settings.%20In%20this%20article%2C%20we%20propose%20a%20novel%20MAP%20sequence%20estimation%20method%2C%20Stein-MAP-Seq%2C%20which%20effectively%20addresses%20multimodality%20while%20substantially%20reducing%20computational%20and%20memory%20overhead.%20Our%20key%20contribution%20is%20a%20sequential%20variational%20inference%20framework%20that%20captures%20temporal%20dependencies%20in%20dynamical%20system%20models%20and%20integrates%20Stein%20variational%20gradient%20descent%20%28SVGD%29%20into%20a%20Viterbi-style%20dynamic%20programming%20algorithm%2C%20enabling%20computationally%20efficient%20MAP%20sequence%20estimation.%20This%20integration%20allows%20the%20method%20to%20focus%20computational%20effort%20on%20MAP-consistent%20modes%20rather%20than%20exhaustively%20exploring%20the%20entire%20state%20space.%20Stein-MAP-Seq%20inherits%20the%20parallelism%20and%20mode-seeking%20behavior%20of%20SVGD%2C%20allowing%20particle%20updates%20to%20be%20efficiently%20executed%20on%20parallel%20hardware%20and%20significantly%20reducing%20the%20number%20of%20trajectory%20candidates%20required%20for%20MAP-sequence%20recursion%20compared%20to%20conventional%20methods%20that%20rely%20on%20hundreds%20to%20thousands%20of%20particles.%20We%20validate%20the%20proposed%20approach%20on%20a%20range%20of%20highly%20multimodal%20scenarios%2C%20including%20nonlinear%20dynamics%20with%20ambiguous%20observations%2C%20unknown%20data%20association%20with%20outliers%2C%20range-only%20localization%20under%20temporary%20unobservability%2C%20and%20high-dimensional%20robotic%20manipulators.%20Experimental%20results%20demonstrate%20substantial%20improvements%20in%20estimation%20accuracy%20and%20robustness%20to%20multimodality%20over%20existing%20estimation%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2312.08684v4&entry.124074799=Read"},
{"title": "DecompGAIL: Learning Realistic Traffic Behaviors with Decomposed Multi-Agent Generative Adversarial Imitation Learning", "author": "Ke Guo and Haochen Liu and Xiaojun Wu and Chen Lv", "abstract": "Realistic traffic simulation is critical for the development of autonomous driving systems and urban mobility planning, yet existing imitation learning approaches often fail to model realistic traffic behaviors. Behavior cloning suffers from covariate shift, while Generative Adversarial Imitation Learning (GAIL) is notoriously unstable in multi-agent settings. We identify a key source of this instability: irrelevant interaction misguidance, where a discriminator penalizes an ego vehicle's realistic behavior due to unrealistic interactions among its neighbors. To address this, we propose Decomposed Multi-agent GAIL (DecompGAIL), which explicitly decomposes realism into ego-map and ego-neighbor components, filtering out misleading neighbor: neighbor and neighbor: map interactions. We further introduce a social PPO objective that augments ego rewards with distance-weighted neighborhood rewards, encouraging overall realism across agents. Integrated into a lightweight SMART-based backbone, DecompGAIL achieves state-of-the-art performance on the WOMD Sim Agents 2025 benchmark.", "link": "http://arxiv.org/abs/2510.06913v2", "date": "2026-01-26", "relevancy": 2.1248, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5486}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.522}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DecompGAIL%3A%20Learning%20Realistic%20Traffic%20Behaviors%20with%20Decomposed%20Multi-Agent%20Generative%20Adversarial%20Imitation%20Learning&body=Title%3A%20DecompGAIL%3A%20Learning%20Realistic%20Traffic%20Behaviors%20with%20Decomposed%20Multi-Agent%20Generative%20Adversarial%20Imitation%20Learning%0AAuthor%3A%20Ke%20Guo%20and%20Haochen%20Liu%20and%20Xiaojun%20Wu%20and%20Chen%20Lv%0AAbstract%3A%20Realistic%20traffic%20simulation%20is%20critical%20for%20the%20development%20of%20autonomous%20driving%20systems%20and%20urban%20mobility%20planning%2C%20yet%20existing%20imitation%20learning%20approaches%20often%20fail%20to%20model%20realistic%20traffic%20behaviors.%20Behavior%20cloning%20suffers%20from%20covariate%20shift%2C%20while%20Generative%20Adversarial%20Imitation%20Learning%20%28GAIL%29%20is%20notoriously%20unstable%20in%20multi-agent%20settings.%20We%20identify%20a%20key%20source%20of%20this%20instability%3A%20irrelevant%20interaction%20misguidance%2C%20where%20a%20discriminator%20penalizes%20an%20ego%20vehicle%27s%20realistic%20behavior%20due%20to%20unrealistic%20interactions%20among%20its%20neighbors.%20To%20address%20this%2C%20we%20propose%20Decomposed%20Multi-agent%20GAIL%20%28DecompGAIL%29%2C%20which%20explicitly%20decomposes%20realism%20into%20ego-map%20and%20ego-neighbor%20components%2C%20filtering%20out%20misleading%20neighbor%3A%20neighbor%20and%20neighbor%3A%20map%20interactions.%20We%20further%20introduce%20a%20social%20PPO%20objective%20that%20augments%20ego%20rewards%20with%20distance-weighted%20neighborhood%20rewards%2C%20encouraging%20overall%20realism%20across%20agents.%20Integrated%20into%20a%20lightweight%20SMART-based%20backbone%2C%20DecompGAIL%20achieves%20state-of-the-art%20performance%20on%20the%20WOMD%20Sim%20Agents%202025%20benchmark.%0ALink%3A%20http%3A//arxiv.org/abs/2510.06913v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecompGAIL%253A%2520Learning%2520Realistic%2520Traffic%2520Behaviors%2520with%2520Decomposed%2520Multi-Agent%2520Generative%2520Adversarial%2520Imitation%2520Learning%26entry.906535625%3DKe%2520Guo%2520and%2520Haochen%2520Liu%2520and%2520Xiaojun%2520Wu%2520and%2520Chen%2520Lv%26entry.1292438233%3DRealistic%2520traffic%2520simulation%2520is%2520critical%2520for%2520the%2520development%2520of%2520autonomous%2520driving%2520systems%2520and%2520urban%2520mobility%2520planning%252C%2520yet%2520existing%2520imitation%2520learning%2520approaches%2520often%2520fail%2520to%2520model%2520realistic%2520traffic%2520behaviors.%2520Behavior%2520cloning%2520suffers%2520from%2520covariate%2520shift%252C%2520while%2520Generative%2520Adversarial%2520Imitation%2520Learning%2520%2528GAIL%2529%2520is%2520notoriously%2520unstable%2520in%2520multi-agent%2520settings.%2520We%2520identify%2520a%2520key%2520source%2520of%2520this%2520instability%253A%2520irrelevant%2520interaction%2520misguidance%252C%2520where%2520a%2520discriminator%2520penalizes%2520an%2520ego%2520vehicle%2527s%2520realistic%2520behavior%2520due%2520to%2520unrealistic%2520interactions%2520among%2520its%2520neighbors.%2520To%2520address%2520this%252C%2520we%2520propose%2520Decomposed%2520Multi-agent%2520GAIL%2520%2528DecompGAIL%2529%252C%2520which%2520explicitly%2520decomposes%2520realism%2520into%2520ego-map%2520and%2520ego-neighbor%2520components%252C%2520filtering%2520out%2520misleading%2520neighbor%253A%2520neighbor%2520and%2520neighbor%253A%2520map%2520interactions.%2520We%2520further%2520introduce%2520a%2520social%2520PPO%2520objective%2520that%2520augments%2520ego%2520rewards%2520with%2520distance-weighted%2520neighborhood%2520rewards%252C%2520encouraging%2520overall%2520realism%2520across%2520agents.%2520Integrated%2520into%2520a%2520lightweight%2520SMART-based%2520backbone%252C%2520DecompGAIL%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%2520WOMD%2520Sim%2520Agents%25202025%2520benchmark.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06913v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DecompGAIL%3A%20Learning%20Realistic%20Traffic%20Behaviors%20with%20Decomposed%20Multi-Agent%20Generative%20Adversarial%20Imitation%20Learning&entry.906535625=Ke%20Guo%20and%20Haochen%20Liu%20and%20Xiaojun%20Wu%20and%20Chen%20Lv&entry.1292438233=Realistic%20traffic%20simulation%20is%20critical%20for%20the%20development%20of%20autonomous%20driving%20systems%20and%20urban%20mobility%20planning%2C%20yet%20existing%20imitation%20learning%20approaches%20often%20fail%20to%20model%20realistic%20traffic%20behaviors.%20Behavior%20cloning%20suffers%20from%20covariate%20shift%2C%20while%20Generative%20Adversarial%20Imitation%20Learning%20%28GAIL%29%20is%20notoriously%20unstable%20in%20multi-agent%20settings.%20We%20identify%20a%20key%20source%20of%20this%20instability%3A%20irrelevant%20interaction%20misguidance%2C%20where%20a%20discriminator%20penalizes%20an%20ego%20vehicle%27s%20realistic%20behavior%20due%20to%20unrealistic%20interactions%20among%20its%20neighbors.%20To%20address%20this%2C%20we%20propose%20Decomposed%20Multi-agent%20GAIL%20%28DecompGAIL%29%2C%20which%20explicitly%20decomposes%20realism%20into%20ego-map%20and%20ego-neighbor%20components%2C%20filtering%20out%20misleading%20neighbor%3A%20neighbor%20and%20neighbor%3A%20map%20interactions.%20We%20further%20introduce%20a%20social%20PPO%20objective%20that%20augments%20ego%20rewards%20with%20distance-weighted%20neighborhood%20rewards%2C%20encouraging%20overall%20realism%20across%20agents.%20Integrated%20into%20a%20lightweight%20SMART-based%20backbone%2C%20DecompGAIL%20achieves%20state-of-the-art%20performance%20on%20the%20WOMD%20Sim%20Agents%202025%20benchmark.&entry.1838667208=http%3A//arxiv.org/abs/2510.06913v2&entry.124074799=Read"},
{"title": "Stability as a Liability:Systematic Breakdown of Linguistic Structure in LLMs", "author": "Xianzhe Meng and Qiangsheng Zeng and Ling Luo and Qinghan Yang and Jiarui Hao and Wenbo Wu and Qinyu Wang and Rui Yin and Lin Qi and Renzhi Lu", "abstract": "Training stability is typically regarded as a prerequisite for reliable optimization in large language models. In this work, we analyze how stabilizing training dynamics affects the induced generation distribution. We show that under standard maximum likelihood training, stable parameter trajectories lead stationary solutions to approximately minimize the forward KL divergence to the empirical distribution, while implicitly reducing generative entropy. As a consequence, the learned model can concentrate probability mass on a limited subset of empirical modes, exhibiting systematic degeneration despite smooth loss convergence. We empirically validate this effect using a controlled feedback-based training framework that stabilizes internal generation statistics, observing consistent low-entropy outputs and repetitive behavior across architectures and random seeds. It indicates that optimization stability and generative expressivity are not inherently aligned, and that stability alone is an insufficient indicator of generative quality.", "link": "http://arxiv.org/abs/2601.18588v1", "date": "2026-01-26", "relevancy": 2.1234, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4295}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4252}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4193}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stability%20as%20a%20Liability%3ASystematic%20Breakdown%20of%20Linguistic%20Structure%20in%20LLMs&body=Title%3A%20Stability%20as%20a%20Liability%3ASystematic%20Breakdown%20of%20Linguistic%20Structure%20in%20LLMs%0AAuthor%3A%20Xianzhe%20Meng%20and%20Qiangsheng%20Zeng%20and%20Ling%20Luo%20and%20Qinghan%20Yang%20and%20Jiarui%20Hao%20and%20Wenbo%20Wu%20and%20Qinyu%20Wang%20and%20Rui%20Yin%20and%20Lin%20Qi%20and%20Renzhi%20Lu%0AAbstract%3A%20Training%20stability%20is%20typically%20regarded%20as%20a%20prerequisite%20for%20reliable%20optimization%20in%20large%20language%20models.%20In%20this%20work%2C%20we%20analyze%20how%20stabilizing%20training%20dynamics%20affects%20the%20induced%20generation%20distribution.%20We%20show%20that%20under%20standard%20maximum%20likelihood%20training%2C%20stable%20parameter%20trajectories%20lead%20stationary%20solutions%20to%20approximately%20minimize%20the%20forward%20KL%20divergence%20to%20the%20empirical%20distribution%2C%20while%20implicitly%20reducing%20generative%20entropy.%20As%20a%20consequence%2C%20the%20learned%20model%20can%20concentrate%20probability%20mass%20on%20a%20limited%20subset%20of%20empirical%20modes%2C%20exhibiting%20systematic%20degeneration%20despite%20smooth%20loss%20convergence.%20We%20empirically%20validate%20this%20effect%20using%20a%20controlled%20feedback-based%20training%20framework%20that%20stabilizes%20internal%20generation%20statistics%2C%20observing%20consistent%20low-entropy%20outputs%20and%20repetitive%20behavior%20across%20architectures%20and%20random%20seeds.%20It%20indicates%20that%20optimization%20stability%20and%20generative%20expressivity%20are%20not%20inherently%20aligned%2C%20and%20that%20stability%20alone%20is%20an%20insufficient%20indicator%20of%20generative%20quality.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18588v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStability%2520as%2520a%2520Liability%253ASystematic%2520Breakdown%2520of%2520Linguistic%2520Structure%2520in%2520LLMs%26entry.906535625%3DXianzhe%2520Meng%2520and%2520Qiangsheng%2520Zeng%2520and%2520Ling%2520Luo%2520and%2520Qinghan%2520Yang%2520and%2520Jiarui%2520Hao%2520and%2520Wenbo%2520Wu%2520and%2520Qinyu%2520Wang%2520and%2520Rui%2520Yin%2520and%2520Lin%2520Qi%2520and%2520Renzhi%2520Lu%26entry.1292438233%3DTraining%2520stability%2520is%2520typically%2520regarded%2520as%2520a%2520prerequisite%2520for%2520reliable%2520optimization%2520in%2520large%2520language%2520models.%2520In%2520this%2520work%252C%2520we%2520analyze%2520how%2520stabilizing%2520training%2520dynamics%2520affects%2520the%2520induced%2520generation%2520distribution.%2520We%2520show%2520that%2520under%2520standard%2520maximum%2520likelihood%2520training%252C%2520stable%2520parameter%2520trajectories%2520lead%2520stationary%2520solutions%2520to%2520approximately%2520minimize%2520the%2520forward%2520KL%2520divergence%2520to%2520the%2520empirical%2520distribution%252C%2520while%2520implicitly%2520reducing%2520generative%2520entropy.%2520As%2520a%2520consequence%252C%2520the%2520learned%2520model%2520can%2520concentrate%2520probability%2520mass%2520on%2520a%2520limited%2520subset%2520of%2520empirical%2520modes%252C%2520exhibiting%2520systematic%2520degeneration%2520despite%2520smooth%2520loss%2520convergence.%2520We%2520empirically%2520validate%2520this%2520effect%2520using%2520a%2520controlled%2520feedback-based%2520training%2520framework%2520that%2520stabilizes%2520internal%2520generation%2520statistics%252C%2520observing%2520consistent%2520low-entropy%2520outputs%2520and%2520repetitive%2520behavior%2520across%2520architectures%2520and%2520random%2520seeds.%2520It%2520indicates%2520that%2520optimization%2520stability%2520and%2520generative%2520expressivity%2520are%2520not%2520inherently%2520aligned%252C%2520and%2520that%2520stability%2520alone%2520is%2520an%2520insufficient%2520indicator%2520of%2520generative%2520quality.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18588v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stability%20as%20a%20Liability%3ASystematic%20Breakdown%20of%20Linguistic%20Structure%20in%20LLMs&entry.906535625=Xianzhe%20Meng%20and%20Qiangsheng%20Zeng%20and%20Ling%20Luo%20and%20Qinghan%20Yang%20and%20Jiarui%20Hao%20and%20Wenbo%20Wu%20and%20Qinyu%20Wang%20and%20Rui%20Yin%20and%20Lin%20Qi%20and%20Renzhi%20Lu&entry.1292438233=Training%20stability%20is%20typically%20regarded%20as%20a%20prerequisite%20for%20reliable%20optimization%20in%20large%20language%20models.%20In%20this%20work%2C%20we%20analyze%20how%20stabilizing%20training%20dynamics%20affects%20the%20induced%20generation%20distribution.%20We%20show%20that%20under%20standard%20maximum%20likelihood%20training%2C%20stable%20parameter%20trajectories%20lead%20stationary%20solutions%20to%20approximately%20minimize%20the%20forward%20KL%20divergence%20to%20the%20empirical%20distribution%2C%20while%20implicitly%20reducing%20generative%20entropy.%20As%20a%20consequence%2C%20the%20learned%20model%20can%20concentrate%20probability%20mass%20on%20a%20limited%20subset%20of%20empirical%20modes%2C%20exhibiting%20systematic%20degeneration%20despite%20smooth%20loss%20convergence.%20We%20empirically%20validate%20this%20effect%20using%20a%20controlled%20feedback-based%20training%20framework%20that%20stabilizes%20internal%20generation%20statistics%2C%20observing%20consistent%20low-entropy%20outputs%20and%20repetitive%20behavior%20across%20architectures%20and%20random%20seeds.%20It%20indicates%20that%20optimization%20stability%20and%20generative%20expressivity%20are%20not%20inherently%20aligned%2C%20and%20that%20stability%20alone%20is%20an%20insufficient%20indicator%20of%20generative%20quality.&entry.1838667208=http%3A//arxiv.org/abs/2601.18588v1&entry.124074799=Read"},
{"title": "AGSP-DSA: An Adaptive Graph Signal Processing Framework for Robust Multimodal Fusion with Dynamic Semantic Alignment", "author": "KV Karthikeya and Ashok Kumar Das and Shantanu Pal and Vivekananda Bhat K and Arun Sekar Rajasekaran", "abstract": "In this paper, we introduce an Adaptive Graph Signal Processing with Dynamic Semantic Alignment (AGSP DSA) framework to perform robust multimodal data fusion over heterogeneous sources, including text, audio, and images. The requested approach uses a dual-graph construction to learn both intra-modal and inter-modal relations, spectral graph filtering to boost the informative signals, and effective node embedding with Multi-scale Graph Convolutional Networks (GCNs). Semantic aware attention mechanism: each modality may dynamically contribute to the context with respect to contextual relevance. The experimental outcomes on three benchmark datasets, including CMU-MOSEI, AVE, and MM-IMDB, show that AGSP-DSA performs as the state of the art. More precisely, it achieves 95.3% accuracy, 0.936 F1-score, and 0.924 mAP on CMU-MOSEI, improving MM-GNN by 2.6 percent in accuracy. It gets 93.4% accuracy and 0.911 F1-score on AVE and 91.8% accuracy and 0.886 F1-score on MM-IMDB, which demonstrate good generalization and robustness in the missing modality setting. These findings verify the efficiency of AGSP-DSA in promoting multimodal learning in sentiment analysis, event recognition and multimedia classification.", "link": "http://arxiv.org/abs/2601.18589v1", "date": "2026-01-26", "relevancy": 2.1195, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.534}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5333}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AGSP-DSA%3A%20An%20Adaptive%20Graph%20Signal%20Processing%20Framework%20for%20Robust%20Multimodal%20Fusion%20with%20Dynamic%20Semantic%20Alignment&body=Title%3A%20AGSP-DSA%3A%20An%20Adaptive%20Graph%20Signal%20Processing%20Framework%20for%20Robust%20Multimodal%20Fusion%20with%20Dynamic%20Semantic%20Alignment%0AAuthor%3A%20KV%20Karthikeya%20and%20Ashok%20Kumar%20Das%20and%20Shantanu%20Pal%20and%20Vivekananda%20Bhat%20K%20and%20Arun%20Sekar%20Rajasekaran%0AAbstract%3A%20In%20this%20paper%2C%20we%20introduce%20an%20Adaptive%20Graph%20Signal%20Processing%20with%20Dynamic%20Semantic%20Alignment%20%28AGSP%20DSA%29%20framework%20to%20perform%20robust%20multimodal%20data%20fusion%20over%20heterogeneous%20sources%2C%20including%20text%2C%20audio%2C%20and%20images.%20The%20requested%20approach%20uses%20a%20dual-graph%20construction%20to%20learn%20both%20intra-modal%20and%20inter-modal%20relations%2C%20spectral%20graph%20filtering%20to%20boost%20the%20informative%20signals%2C%20and%20effective%20node%20embedding%20with%20Multi-scale%20Graph%20Convolutional%20Networks%20%28GCNs%29.%20Semantic%20aware%20attention%20mechanism%3A%20each%20modality%20may%20dynamically%20contribute%20to%20the%20context%20with%20respect%20to%20contextual%20relevance.%20The%20experimental%20outcomes%20on%20three%20benchmark%20datasets%2C%20including%20CMU-MOSEI%2C%20AVE%2C%20and%20MM-IMDB%2C%20show%20that%20AGSP-DSA%20performs%20as%20the%20state%20of%20the%20art.%20More%20precisely%2C%20it%20achieves%2095.3%25%20accuracy%2C%200.936%20F1-score%2C%20and%200.924%20mAP%20on%20CMU-MOSEI%2C%20improving%20MM-GNN%20by%202.6%20percent%20in%20accuracy.%20It%20gets%2093.4%25%20accuracy%20and%200.911%20F1-score%20on%20AVE%20and%2091.8%25%20accuracy%20and%200.886%20F1-score%20on%20MM-IMDB%2C%20which%20demonstrate%20good%20generalization%20and%20robustness%20in%20the%20missing%20modality%20setting.%20These%20findings%20verify%20the%20efficiency%20of%20AGSP-DSA%20in%20promoting%20multimodal%20learning%20in%20sentiment%20analysis%2C%20event%20recognition%20and%20multimedia%20classification.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18589v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAGSP-DSA%253A%2520An%2520Adaptive%2520Graph%2520Signal%2520Processing%2520Framework%2520for%2520Robust%2520Multimodal%2520Fusion%2520with%2520Dynamic%2520Semantic%2520Alignment%26entry.906535625%3DKV%2520Karthikeya%2520and%2520Ashok%2520Kumar%2520Das%2520and%2520Shantanu%2520Pal%2520and%2520Vivekananda%2520Bhat%2520K%2520and%2520Arun%2520Sekar%2520Rajasekaran%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520introduce%2520an%2520Adaptive%2520Graph%2520Signal%2520Processing%2520with%2520Dynamic%2520Semantic%2520Alignment%2520%2528AGSP%2520DSA%2529%2520framework%2520to%2520perform%2520robust%2520multimodal%2520data%2520fusion%2520over%2520heterogeneous%2520sources%252C%2520including%2520text%252C%2520audio%252C%2520and%2520images.%2520The%2520requested%2520approach%2520uses%2520a%2520dual-graph%2520construction%2520to%2520learn%2520both%2520intra-modal%2520and%2520inter-modal%2520relations%252C%2520spectral%2520graph%2520filtering%2520to%2520boost%2520the%2520informative%2520signals%252C%2520and%2520effective%2520node%2520embedding%2520with%2520Multi-scale%2520Graph%2520Convolutional%2520Networks%2520%2528GCNs%2529.%2520Semantic%2520aware%2520attention%2520mechanism%253A%2520each%2520modality%2520may%2520dynamically%2520contribute%2520to%2520the%2520context%2520with%2520respect%2520to%2520contextual%2520relevance.%2520The%2520experimental%2520outcomes%2520on%2520three%2520benchmark%2520datasets%252C%2520including%2520CMU-MOSEI%252C%2520AVE%252C%2520and%2520MM-IMDB%252C%2520show%2520that%2520AGSP-DSA%2520performs%2520as%2520the%2520state%2520of%2520the%2520art.%2520More%2520precisely%252C%2520it%2520achieves%252095.3%2525%2520accuracy%252C%25200.936%2520F1-score%252C%2520and%25200.924%2520mAP%2520on%2520CMU-MOSEI%252C%2520improving%2520MM-GNN%2520by%25202.6%2520percent%2520in%2520accuracy.%2520It%2520gets%252093.4%2525%2520accuracy%2520and%25200.911%2520F1-score%2520on%2520AVE%2520and%252091.8%2525%2520accuracy%2520and%25200.886%2520F1-score%2520on%2520MM-IMDB%252C%2520which%2520demonstrate%2520good%2520generalization%2520and%2520robustness%2520in%2520the%2520missing%2520modality%2520setting.%2520These%2520findings%2520verify%2520the%2520efficiency%2520of%2520AGSP-DSA%2520in%2520promoting%2520multimodal%2520learning%2520in%2520sentiment%2520analysis%252C%2520event%2520recognition%2520and%2520multimedia%2520classification.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18589v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AGSP-DSA%3A%20An%20Adaptive%20Graph%20Signal%20Processing%20Framework%20for%20Robust%20Multimodal%20Fusion%20with%20Dynamic%20Semantic%20Alignment&entry.906535625=KV%20Karthikeya%20and%20Ashok%20Kumar%20Das%20and%20Shantanu%20Pal%20and%20Vivekananda%20Bhat%20K%20and%20Arun%20Sekar%20Rajasekaran&entry.1292438233=In%20this%20paper%2C%20we%20introduce%20an%20Adaptive%20Graph%20Signal%20Processing%20with%20Dynamic%20Semantic%20Alignment%20%28AGSP%20DSA%29%20framework%20to%20perform%20robust%20multimodal%20data%20fusion%20over%20heterogeneous%20sources%2C%20including%20text%2C%20audio%2C%20and%20images.%20The%20requested%20approach%20uses%20a%20dual-graph%20construction%20to%20learn%20both%20intra-modal%20and%20inter-modal%20relations%2C%20spectral%20graph%20filtering%20to%20boost%20the%20informative%20signals%2C%20and%20effective%20node%20embedding%20with%20Multi-scale%20Graph%20Convolutional%20Networks%20%28GCNs%29.%20Semantic%20aware%20attention%20mechanism%3A%20each%20modality%20may%20dynamically%20contribute%20to%20the%20context%20with%20respect%20to%20contextual%20relevance.%20The%20experimental%20outcomes%20on%20three%20benchmark%20datasets%2C%20including%20CMU-MOSEI%2C%20AVE%2C%20and%20MM-IMDB%2C%20show%20that%20AGSP-DSA%20performs%20as%20the%20state%20of%20the%20art.%20More%20precisely%2C%20it%20achieves%2095.3%25%20accuracy%2C%200.936%20F1-score%2C%20and%200.924%20mAP%20on%20CMU-MOSEI%2C%20improving%20MM-GNN%20by%202.6%20percent%20in%20accuracy.%20It%20gets%2093.4%25%20accuracy%20and%200.911%20F1-score%20on%20AVE%20and%2091.8%25%20accuracy%20and%200.886%20F1-score%20on%20MM-IMDB%2C%20which%20demonstrate%20good%20generalization%20and%20robustness%20in%20the%20missing%20modality%20setting.%20These%20findings%20verify%20the%20efficiency%20of%20AGSP-DSA%20in%20promoting%20multimodal%20learning%20in%20sentiment%20analysis%2C%20event%20recognition%20and%20multimedia%20classification.&entry.1838667208=http%3A//arxiv.org/abs/2601.18589v1&entry.124074799=Read"},
{"title": "Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs", "author": "Amir Taherin and Juyi Lin and Arash Akbari and Arman Akbari and Pu Zhao and Weiwei Chen and David Kaeli and Yanzhi Wang", "abstract": "Vision-Language-Action (VLA) models have emerged as powerful generalist policies for robotic control, yet their performance scaling across model architectures and hardware platforms, as well as their associated power budgets, remain poorly understood. This work presents an evaluation of five representative VLA models -- spanning state-of-the-art baselines and two newly proposed architectures -- targeting edge and datacenter GPU platforms. Using the LIBERO benchmark, we measure accuracy alongside system-level metrics, including latency, throughput, and peak memory usage, under varying edge power constraints and high-performance datacenter GPU configurations. Our results identify distinct scaling trends: (1) architectural choices, such as action tokenization and model backbone size, strongly influence throughput and memory footprint; (2) power-constrained edge devices exhibit non-linear performance degradation, with some configurations matching or exceeding older datacenter GPUs; and (3) high-throughput variants can be achieved without significant accuracy loss. These findings provide actionable insights when selecting and optimizing VLAs across a range of deployment constraints. Our work challenges current assumptions about the superiority of datacenter hardware for robotic inference.", "link": "http://arxiv.org/abs/2509.11480v2", "date": "2026-01-26", "relevancy": 2.1181, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.533}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.533}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Platform%20Scaling%20of%20Vision-Language-Action%20Models%20from%20Edge%20to%20Cloud%20GPUs&body=Title%3A%20Cross-Platform%20Scaling%20of%20Vision-Language-Action%20Models%20from%20Edge%20to%20Cloud%20GPUs%0AAuthor%3A%20Amir%20Taherin%20and%20Juyi%20Lin%20and%20Arash%20Akbari%20and%20Arman%20Akbari%20and%20Pu%20Zhao%20and%20Weiwei%20Chen%20and%20David%20Kaeli%20and%20Yanzhi%20Wang%0AAbstract%3A%20Vision-Language-Action%20%28VLA%29%20models%20have%20emerged%20as%20powerful%20generalist%20policies%20for%20robotic%20control%2C%20yet%20their%20performance%20scaling%20across%20model%20architectures%20and%20hardware%20platforms%2C%20as%20well%20as%20their%20associated%20power%20budgets%2C%20remain%20poorly%20understood.%20This%20work%20presents%20an%20evaluation%20of%20five%20representative%20VLA%20models%20--%20spanning%20state-of-the-art%20baselines%20and%20two%20newly%20proposed%20architectures%20--%20targeting%20edge%20and%20datacenter%20GPU%20platforms.%20Using%20the%20LIBERO%20benchmark%2C%20we%20measure%20accuracy%20alongside%20system-level%20metrics%2C%20including%20latency%2C%20throughput%2C%20and%20peak%20memory%20usage%2C%20under%20varying%20edge%20power%20constraints%20and%20high-performance%20datacenter%20GPU%20configurations.%20Our%20results%20identify%20distinct%20scaling%20trends%3A%20%281%29%20architectural%20choices%2C%20such%20as%20action%20tokenization%20and%20model%20backbone%20size%2C%20strongly%20influence%20throughput%20and%20memory%20footprint%3B%20%282%29%20power-constrained%20edge%20devices%20exhibit%20non-linear%20performance%20degradation%2C%20with%20some%20configurations%20matching%20or%20exceeding%20older%20datacenter%20GPUs%3B%20and%20%283%29%20high-throughput%20variants%20can%20be%20achieved%20without%20significant%20accuracy%20loss.%20These%20findings%20provide%20actionable%20insights%20when%20selecting%20and%20optimizing%20VLAs%20across%20a%20range%20of%20deployment%20constraints.%20Our%20work%20challenges%20current%20assumptions%20about%20the%20superiority%20of%20datacenter%20hardware%20for%20robotic%20inference.%0ALink%3A%20http%3A//arxiv.org/abs/2509.11480v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Platform%2520Scaling%2520of%2520Vision-Language-Action%2520Models%2520from%2520Edge%2520to%2520Cloud%2520GPUs%26entry.906535625%3DAmir%2520Taherin%2520and%2520Juyi%2520Lin%2520and%2520Arash%2520Akbari%2520and%2520Arman%2520Akbari%2520and%2520Pu%2520Zhao%2520and%2520Weiwei%2520Chen%2520and%2520David%2520Kaeli%2520and%2520Yanzhi%2520Wang%26entry.1292438233%3DVision-Language-Action%2520%2528VLA%2529%2520models%2520have%2520emerged%2520as%2520powerful%2520generalist%2520policies%2520for%2520robotic%2520control%252C%2520yet%2520their%2520performance%2520scaling%2520across%2520model%2520architectures%2520and%2520hardware%2520platforms%252C%2520as%2520well%2520as%2520their%2520associated%2520power%2520budgets%252C%2520remain%2520poorly%2520understood.%2520This%2520work%2520presents%2520an%2520evaluation%2520of%2520five%2520representative%2520VLA%2520models%2520--%2520spanning%2520state-of-the-art%2520baselines%2520and%2520two%2520newly%2520proposed%2520architectures%2520--%2520targeting%2520edge%2520and%2520datacenter%2520GPU%2520platforms.%2520Using%2520the%2520LIBERO%2520benchmark%252C%2520we%2520measure%2520accuracy%2520alongside%2520system-level%2520metrics%252C%2520including%2520latency%252C%2520throughput%252C%2520and%2520peak%2520memory%2520usage%252C%2520under%2520varying%2520edge%2520power%2520constraints%2520and%2520high-performance%2520datacenter%2520GPU%2520configurations.%2520Our%2520results%2520identify%2520distinct%2520scaling%2520trends%253A%2520%25281%2529%2520architectural%2520choices%252C%2520such%2520as%2520action%2520tokenization%2520and%2520model%2520backbone%2520size%252C%2520strongly%2520influence%2520throughput%2520and%2520memory%2520footprint%253B%2520%25282%2529%2520power-constrained%2520edge%2520devices%2520exhibit%2520non-linear%2520performance%2520degradation%252C%2520with%2520some%2520configurations%2520matching%2520or%2520exceeding%2520older%2520datacenter%2520GPUs%253B%2520and%2520%25283%2529%2520high-throughput%2520variants%2520can%2520be%2520achieved%2520without%2520significant%2520accuracy%2520loss.%2520These%2520findings%2520provide%2520actionable%2520insights%2520when%2520selecting%2520and%2520optimizing%2520VLAs%2520across%2520a%2520range%2520of%2520deployment%2520constraints.%2520Our%2520work%2520challenges%2520current%2520assumptions%2520about%2520the%2520superiority%2520of%2520datacenter%2520hardware%2520for%2520robotic%2520inference.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11480v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Platform%20Scaling%20of%20Vision-Language-Action%20Models%20from%20Edge%20to%20Cloud%20GPUs&entry.906535625=Amir%20Taherin%20and%20Juyi%20Lin%20and%20Arash%20Akbari%20and%20Arman%20Akbari%20and%20Pu%20Zhao%20and%20Weiwei%20Chen%20and%20David%20Kaeli%20and%20Yanzhi%20Wang&entry.1292438233=Vision-Language-Action%20%28VLA%29%20models%20have%20emerged%20as%20powerful%20generalist%20policies%20for%20robotic%20control%2C%20yet%20their%20performance%20scaling%20across%20model%20architectures%20and%20hardware%20platforms%2C%20as%20well%20as%20their%20associated%20power%20budgets%2C%20remain%20poorly%20understood.%20This%20work%20presents%20an%20evaluation%20of%20five%20representative%20VLA%20models%20--%20spanning%20state-of-the-art%20baselines%20and%20two%20newly%20proposed%20architectures%20--%20targeting%20edge%20and%20datacenter%20GPU%20platforms.%20Using%20the%20LIBERO%20benchmark%2C%20we%20measure%20accuracy%20alongside%20system-level%20metrics%2C%20including%20latency%2C%20throughput%2C%20and%20peak%20memory%20usage%2C%20under%20varying%20edge%20power%20constraints%20and%20high-performance%20datacenter%20GPU%20configurations.%20Our%20results%20identify%20distinct%20scaling%20trends%3A%20%281%29%20architectural%20choices%2C%20such%20as%20action%20tokenization%20and%20model%20backbone%20size%2C%20strongly%20influence%20throughput%20and%20memory%20footprint%3B%20%282%29%20power-constrained%20edge%20devices%20exhibit%20non-linear%20performance%20degradation%2C%20with%20some%20configurations%20matching%20or%20exceeding%20older%20datacenter%20GPUs%3B%20and%20%283%29%20high-throughput%20variants%20can%20be%20achieved%20without%20significant%20accuracy%20loss.%20These%20findings%20provide%20actionable%20insights%20when%20selecting%20and%20optimizing%20VLAs%20across%20a%20range%20of%20deployment%20constraints.%20Our%20work%20challenges%20current%20assumptions%20about%20the%20superiority%20of%20datacenter%20hardware%20for%20robotic%20inference.&entry.1838667208=http%3A//arxiv.org/abs/2509.11480v2&entry.124074799=Read"},
{"title": "Q-Bench-Portrait: Benchmarking Multimodal Large Language Models on Portrait Image Quality Perception", "author": "Sijing Wu and Yunhao Li and Zicheng Zhang and Qi Jia and Xinyue Li and Huiyu Duan and Xiongkuo Min and Guangtao Zhai", "abstract": "Recent advances in multimodal large language models (MLLMs) have demonstrated impressive performance on existing low-level vision benchmarks, which primarily focus on generic images. However, their capabilities to perceive and assess portrait images, a domain characterized by distinct structural and perceptual properties, remain largely underexplored. To this end, we introduce Q-Bench-Portrait, the first holistic benchmark specifically designed for portrait image quality perception, comprising 2,765 image-question-answer triplets and featuring (1) diverse portrait image sources, including natural, synthetic distortion, AI-generated, artistic, and computer graphics images; (2) comprehensive quality dimensions, covering technical distortions, AIGC-specific distortions, and aesthetics; and (3) a range of question formats, including single-choice, multiple-choice, true/false, and open-ended questions, at both global and local levels. Based on Q-Bench-Portrait, we evaluate 20 open-source and 5 closed-source MLLMs, revealing that although current models demonstrate some competence in portrait image perception, their performance remains limited and imprecise, with a clear gap relative to human judgments. We hope that the proposed benchmark will foster further research into enhancing the portrait image perception capabilities of both general-purpose and domain-specific MLLMs.", "link": "http://arxiv.org/abs/2601.18346v1", "date": "2026-01-26", "relevancy": 2.1131, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5305}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5305}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Q-Bench-Portrait%3A%20Benchmarking%20Multimodal%20Large%20Language%20Models%20on%20Portrait%20Image%20Quality%20Perception&body=Title%3A%20Q-Bench-Portrait%3A%20Benchmarking%20Multimodal%20Large%20Language%20Models%20on%20Portrait%20Image%20Quality%20Perception%0AAuthor%3A%20Sijing%20Wu%20and%20Yunhao%20Li%20and%20Zicheng%20Zhang%20and%20Qi%20Jia%20and%20Xinyue%20Li%20and%20Huiyu%20Duan%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai%0AAbstract%3A%20Recent%20advances%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20demonstrated%20impressive%20performance%20on%20existing%20low-level%20vision%20benchmarks%2C%20which%20primarily%20focus%20on%20generic%20images.%20However%2C%20their%20capabilities%20to%20perceive%20and%20assess%20portrait%20images%2C%20a%20domain%20characterized%20by%20distinct%20structural%20and%20perceptual%20properties%2C%20remain%20largely%20underexplored.%20To%20this%20end%2C%20we%20introduce%20Q-Bench-Portrait%2C%20the%20first%20holistic%20benchmark%20specifically%20designed%20for%20portrait%20image%20quality%20perception%2C%20comprising%202%2C765%20image-question-answer%20triplets%20and%20featuring%20%281%29%20diverse%20portrait%20image%20sources%2C%20including%20natural%2C%20synthetic%20distortion%2C%20AI-generated%2C%20artistic%2C%20and%20computer%20graphics%20images%3B%20%282%29%20comprehensive%20quality%20dimensions%2C%20covering%20technical%20distortions%2C%20AIGC-specific%20distortions%2C%20and%20aesthetics%3B%20and%20%283%29%20a%20range%20of%20question%20formats%2C%20including%20single-choice%2C%20multiple-choice%2C%20true/false%2C%20and%20open-ended%20questions%2C%20at%20both%20global%20and%20local%20levels.%20Based%20on%20Q-Bench-Portrait%2C%20we%20evaluate%2020%20open-source%20and%205%20closed-source%20MLLMs%2C%20revealing%20that%20although%20current%20models%20demonstrate%20some%20competence%20in%20portrait%20image%20perception%2C%20their%20performance%20remains%20limited%20and%20imprecise%2C%20with%20a%20clear%20gap%20relative%20to%20human%20judgments.%20We%20hope%20that%20the%20proposed%20benchmark%20will%20foster%20further%20research%20into%20enhancing%20the%20portrait%20image%20perception%20capabilities%20of%20both%20general-purpose%20and%20domain-specific%20MLLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18346v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQ-Bench-Portrait%253A%2520Benchmarking%2520Multimodal%2520Large%2520Language%2520Models%2520on%2520Portrait%2520Image%2520Quality%2520Perception%26entry.906535625%3DSijing%2520Wu%2520and%2520Yunhao%2520Li%2520and%2520Zicheng%2520Zhang%2520and%2520Qi%2520Jia%2520and%2520Xinyue%2520Li%2520and%2520Huiyu%2520Duan%2520and%2520Xiongkuo%2520Min%2520and%2520Guangtao%2520Zhai%26entry.1292438233%3DRecent%2520advances%2520in%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520impressive%2520performance%2520on%2520existing%2520low-level%2520vision%2520benchmarks%252C%2520which%2520primarily%2520focus%2520on%2520generic%2520images.%2520However%252C%2520their%2520capabilities%2520to%2520perceive%2520and%2520assess%2520portrait%2520images%252C%2520a%2520domain%2520characterized%2520by%2520distinct%2520structural%2520and%2520perceptual%2520properties%252C%2520remain%2520largely%2520underexplored.%2520To%2520this%2520end%252C%2520we%2520introduce%2520Q-Bench-Portrait%252C%2520the%2520first%2520holistic%2520benchmark%2520specifically%2520designed%2520for%2520portrait%2520image%2520quality%2520perception%252C%2520comprising%25202%252C765%2520image-question-answer%2520triplets%2520and%2520featuring%2520%25281%2529%2520diverse%2520portrait%2520image%2520sources%252C%2520including%2520natural%252C%2520synthetic%2520distortion%252C%2520AI-generated%252C%2520artistic%252C%2520and%2520computer%2520graphics%2520images%253B%2520%25282%2529%2520comprehensive%2520quality%2520dimensions%252C%2520covering%2520technical%2520distortions%252C%2520AIGC-specific%2520distortions%252C%2520and%2520aesthetics%253B%2520and%2520%25283%2529%2520a%2520range%2520of%2520question%2520formats%252C%2520including%2520single-choice%252C%2520multiple-choice%252C%2520true/false%252C%2520and%2520open-ended%2520questions%252C%2520at%2520both%2520global%2520and%2520local%2520levels.%2520Based%2520on%2520Q-Bench-Portrait%252C%2520we%2520evaluate%252020%2520open-source%2520and%25205%2520closed-source%2520MLLMs%252C%2520revealing%2520that%2520although%2520current%2520models%2520demonstrate%2520some%2520competence%2520in%2520portrait%2520image%2520perception%252C%2520their%2520performance%2520remains%2520limited%2520and%2520imprecise%252C%2520with%2520a%2520clear%2520gap%2520relative%2520to%2520human%2520judgments.%2520We%2520hope%2520that%2520the%2520proposed%2520benchmark%2520will%2520foster%2520further%2520research%2520into%2520enhancing%2520the%2520portrait%2520image%2520perception%2520capabilities%2520of%2520both%2520general-purpose%2520and%2520domain-specific%2520MLLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18346v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Q-Bench-Portrait%3A%20Benchmarking%20Multimodal%20Large%20Language%20Models%20on%20Portrait%20Image%20Quality%20Perception&entry.906535625=Sijing%20Wu%20and%20Yunhao%20Li%20and%20Zicheng%20Zhang%20and%20Qi%20Jia%20and%20Xinyue%20Li%20and%20Huiyu%20Duan%20and%20Xiongkuo%20Min%20and%20Guangtao%20Zhai&entry.1292438233=Recent%20advances%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20demonstrated%20impressive%20performance%20on%20existing%20low-level%20vision%20benchmarks%2C%20which%20primarily%20focus%20on%20generic%20images.%20However%2C%20their%20capabilities%20to%20perceive%20and%20assess%20portrait%20images%2C%20a%20domain%20characterized%20by%20distinct%20structural%20and%20perceptual%20properties%2C%20remain%20largely%20underexplored.%20To%20this%20end%2C%20we%20introduce%20Q-Bench-Portrait%2C%20the%20first%20holistic%20benchmark%20specifically%20designed%20for%20portrait%20image%20quality%20perception%2C%20comprising%202%2C765%20image-question-answer%20triplets%20and%20featuring%20%281%29%20diverse%20portrait%20image%20sources%2C%20including%20natural%2C%20synthetic%20distortion%2C%20AI-generated%2C%20artistic%2C%20and%20computer%20graphics%20images%3B%20%282%29%20comprehensive%20quality%20dimensions%2C%20covering%20technical%20distortions%2C%20AIGC-specific%20distortions%2C%20and%20aesthetics%3B%20and%20%283%29%20a%20range%20of%20question%20formats%2C%20including%20single-choice%2C%20multiple-choice%2C%20true/false%2C%20and%20open-ended%20questions%2C%20at%20both%20global%20and%20local%20levels.%20Based%20on%20Q-Bench-Portrait%2C%20we%20evaluate%2020%20open-source%20and%205%20closed-source%20MLLMs%2C%20revealing%20that%20although%20current%20models%20demonstrate%20some%20competence%20in%20portrait%20image%20perception%2C%20their%20performance%20remains%20limited%20and%20imprecise%2C%20with%20a%20clear%20gap%20relative%20to%20human%20judgments.%20We%20hope%20that%20the%20proposed%20benchmark%20will%20foster%20further%20research%20into%20enhancing%20the%20portrait%20image%20perception%20capabilities%20of%20both%20general-purpose%20and%20domain-specific%20MLLMs.&entry.1838667208=http%3A//arxiv.org/abs/2601.18346v1&entry.124074799=Read"},
{"title": "Making medical vision-language models think causally across modalities with retrieval-augmented cross-modal reasoning", "author": "Weiqin Yang and Haowen Xue and Qingyi Peng and Hexuan Hu and Qian Huang and Tingbo Zhang", "abstract": "Medical vision-language models (VLMs) achieve strong performance in diagnostic reporting and image-text alignment, yet their underlying reasoning mechanisms remain fundamentally correlational, exhibiting reliance on superficial statistical associations that fail to capture the causal pathophysiological mechanisms central to clinical decision-making. This limitation makes them fragile, prone to hallucinations, and sensitive to dataset biases. Retrieval-augmented generation (RAG) offers a partial remedy by grounding predictions in external knowledge. However, conventional RAG depends on semantic similarity, introducing new spurious correlations. We propose Multimodal Causal Retrieval-Augmented Generation, a framework that integrates causal inference principles with multimodal retrieval. It retrieves clinically relevant exemplars and causal graphs from external sources, conditioning model reasoning on counterfactual and interventional evidence rather than correlations alone. Applied to radiology report generation, diagnosis prediction, and visual question answering, it improves factual accuracy, robustness to distribution shifts, and interpretability. Our results highlight causal retrieval as a scalable path toward medical VLMs that think beyond pattern matching, enabling trustworthy multimodal reasoning in high-stakes clinical settings.", "link": "http://arxiv.org/abs/2601.18356v1", "date": "2026-01-26", "relevancy": 2.1129, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5296}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5296}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Making%20medical%20vision-language%20models%20think%20causally%20across%20modalities%20with%20retrieval-augmented%20cross-modal%20reasoning&body=Title%3A%20Making%20medical%20vision-language%20models%20think%20causally%20across%20modalities%20with%20retrieval-augmented%20cross-modal%20reasoning%0AAuthor%3A%20Weiqin%20Yang%20and%20Haowen%20Xue%20and%20Qingyi%20Peng%20and%20Hexuan%20Hu%20and%20Qian%20Huang%20and%20Tingbo%20Zhang%0AAbstract%3A%20Medical%20vision-language%20models%20%28VLMs%29%20achieve%20strong%20performance%20in%20diagnostic%20reporting%20and%20image-text%20alignment%2C%20yet%20their%20underlying%20reasoning%20mechanisms%20remain%20fundamentally%20correlational%2C%20exhibiting%20reliance%20on%20superficial%20statistical%20associations%20that%20fail%20to%20capture%20the%20causal%20pathophysiological%20mechanisms%20central%20to%20clinical%20decision-making.%20This%20limitation%20makes%20them%20fragile%2C%20prone%20to%20hallucinations%2C%20and%20sensitive%20to%20dataset%20biases.%20Retrieval-augmented%20generation%20%28RAG%29%20offers%20a%20partial%20remedy%20by%20grounding%20predictions%20in%20external%20knowledge.%20However%2C%20conventional%20RAG%20depends%20on%20semantic%20similarity%2C%20introducing%20new%20spurious%20correlations.%20We%20propose%20Multimodal%20Causal%20Retrieval-Augmented%20Generation%2C%20a%20framework%20that%20integrates%20causal%20inference%20principles%20with%20multimodal%20retrieval.%20It%20retrieves%20clinically%20relevant%20exemplars%20and%20causal%20graphs%20from%20external%20sources%2C%20conditioning%20model%20reasoning%20on%20counterfactual%20and%20interventional%20evidence%20rather%20than%20correlations%20alone.%20Applied%20to%20radiology%20report%20generation%2C%20diagnosis%20prediction%2C%20and%20visual%20question%20answering%2C%20it%20improves%20factual%20accuracy%2C%20robustness%20to%20distribution%20shifts%2C%20and%20interpretability.%20Our%20results%20highlight%20causal%20retrieval%20as%20a%20scalable%20path%20toward%20medical%20VLMs%20that%20think%20beyond%20pattern%20matching%2C%20enabling%20trustworthy%20multimodal%20reasoning%20in%20high-stakes%20clinical%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18356v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaking%2520medical%2520vision-language%2520models%2520think%2520causally%2520across%2520modalities%2520with%2520retrieval-augmented%2520cross-modal%2520reasoning%26entry.906535625%3DWeiqin%2520Yang%2520and%2520Haowen%2520Xue%2520and%2520Qingyi%2520Peng%2520and%2520Hexuan%2520Hu%2520and%2520Qian%2520Huang%2520and%2520Tingbo%2520Zhang%26entry.1292438233%3DMedical%2520vision-language%2520models%2520%2528VLMs%2529%2520achieve%2520strong%2520performance%2520in%2520diagnostic%2520reporting%2520and%2520image-text%2520alignment%252C%2520yet%2520their%2520underlying%2520reasoning%2520mechanisms%2520remain%2520fundamentally%2520correlational%252C%2520exhibiting%2520reliance%2520on%2520superficial%2520statistical%2520associations%2520that%2520fail%2520to%2520capture%2520the%2520causal%2520pathophysiological%2520mechanisms%2520central%2520to%2520clinical%2520decision-making.%2520This%2520limitation%2520makes%2520them%2520fragile%252C%2520prone%2520to%2520hallucinations%252C%2520and%2520sensitive%2520to%2520dataset%2520biases.%2520Retrieval-augmented%2520generation%2520%2528RAG%2529%2520offers%2520a%2520partial%2520remedy%2520by%2520grounding%2520predictions%2520in%2520external%2520knowledge.%2520However%252C%2520conventional%2520RAG%2520depends%2520on%2520semantic%2520similarity%252C%2520introducing%2520new%2520spurious%2520correlations.%2520We%2520propose%2520Multimodal%2520Causal%2520Retrieval-Augmented%2520Generation%252C%2520a%2520framework%2520that%2520integrates%2520causal%2520inference%2520principles%2520with%2520multimodal%2520retrieval.%2520It%2520retrieves%2520clinically%2520relevant%2520exemplars%2520and%2520causal%2520graphs%2520from%2520external%2520sources%252C%2520conditioning%2520model%2520reasoning%2520on%2520counterfactual%2520and%2520interventional%2520evidence%2520rather%2520than%2520correlations%2520alone.%2520Applied%2520to%2520radiology%2520report%2520generation%252C%2520diagnosis%2520prediction%252C%2520and%2520visual%2520question%2520answering%252C%2520it%2520improves%2520factual%2520accuracy%252C%2520robustness%2520to%2520distribution%2520shifts%252C%2520and%2520interpretability.%2520Our%2520results%2520highlight%2520causal%2520retrieval%2520as%2520a%2520scalable%2520path%2520toward%2520medical%2520VLMs%2520that%2520think%2520beyond%2520pattern%2520matching%252C%2520enabling%2520trustworthy%2520multimodal%2520reasoning%2520in%2520high-stakes%2520clinical%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18356v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Making%20medical%20vision-language%20models%20think%20causally%20across%20modalities%20with%20retrieval-augmented%20cross-modal%20reasoning&entry.906535625=Weiqin%20Yang%20and%20Haowen%20Xue%20and%20Qingyi%20Peng%20and%20Hexuan%20Hu%20and%20Qian%20Huang%20and%20Tingbo%20Zhang&entry.1292438233=Medical%20vision-language%20models%20%28VLMs%29%20achieve%20strong%20performance%20in%20diagnostic%20reporting%20and%20image-text%20alignment%2C%20yet%20their%20underlying%20reasoning%20mechanisms%20remain%20fundamentally%20correlational%2C%20exhibiting%20reliance%20on%20superficial%20statistical%20associations%20that%20fail%20to%20capture%20the%20causal%20pathophysiological%20mechanisms%20central%20to%20clinical%20decision-making.%20This%20limitation%20makes%20them%20fragile%2C%20prone%20to%20hallucinations%2C%20and%20sensitive%20to%20dataset%20biases.%20Retrieval-augmented%20generation%20%28RAG%29%20offers%20a%20partial%20remedy%20by%20grounding%20predictions%20in%20external%20knowledge.%20However%2C%20conventional%20RAG%20depends%20on%20semantic%20similarity%2C%20introducing%20new%20spurious%20correlations.%20We%20propose%20Multimodal%20Causal%20Retrieval-Augmented%20Generation%2C%20a%20framework%20that%20integrates%20causal%20inference%20principles%20with%20multimodal%20retrieval.%20It%20retrieves%20clinically%20relevant%20exemplars%20and%20causal%20graphs%20from%20external%20sources%2C%20conditioning%20model%20reasoning%20on%20counterfactual%20and%20interventional%20evidence%20rather%20than%20correlations%20alone.%20Applied%20to%20radiology%20report%20generation%2C%20diagnosis%20prediction%2C%20and%20visual%20question%20answering%2C%20it%20improves%20factual%20accuracy%2C%20robustness%20to%20distribution%20shifts%2C%20and%20interpretability.%20Our%20results%20highlight%20causal%20retrieval%20as%20a%20scalable%20path%20toward%20medical%20VLMs%20that%20think%20beyond%20pattern%20matching%2C%20enabling%20trustworthy%20multimodal%20reasoning%20in%20high-stakes%20clinical%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2601.18356v1&entry.124074799=Read"},
{"title": "Deconstructing Instruction-Following: A New Benchmark for Granular Evaluation of Large Language Model Instruction Compliance Abilities", "author": "Alberto Purpura and Li Wang and Sahil Badyal and Eugenio Beaufrand and Adam Faulkner", "abstract": "Reliably ensuring Large Language Models (LLMs) follow complex instructions is a critical challenge, as existing benchmarks often fail to reflect real-world use or isolate compliance from task success. We introduce MOSAIC (MOdular Synthetic Assessment of Instruction Compliance), a modular framework that uses a dynamically generated dataset with up to 20 application-oriented generation constraints to enable a granular and independent analysis of this capability. Our evaluation of five LLMs from different families based on this new benchmark demonstrates that compliance is not a monolithic capability but varies significantly with constraint type, quantity, and position. The analysis reveals model-specific weaknesses, uncovers synergistic and conflicting interactions between instructions, and identifies distinct positional biases such as primacy and recency effects. These granular insights are critical for diagnosing model failures and developing more reliable LLMs for systems that demand strict adherence to complex instructions.", "link": "http://arxiv.org/abs/2601.18554v1", "date": "2026-01-26", "relevancy": 2.0447, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5171}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5171}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deconstructing%20Instruction-Following%3A%20A%20New%20Benchmark%20for%20Granular%20Evaluation%20of%20Large%20Language%20Model%20Instruction%20Compliance%20Abilities&body=Title%3A%20Deconstructing%20Instruction-Following%3A%20A%20New%20Benchmark%20for%20Granular%20Evaluation%20of%20Large%20Language%20Model%20Instruction%20Compliance%20Abilities%0AAuthor%3A%20Alberto%20Purpura%20and%20Li%20Wang%20and%20Sahil%20Badyal%20and%20Eugenio%20Beaufrand%20and%20Adam%20Faulkner%0AAbstract%3A%20Reliably%20ensuring%20Large%20Language%20Models%20%28LLMs%29%20follow%20complex%20instructions%20is%20a%20critical%20challenge%2C%20as%20existing%20benchmarks%20often%20fail%20to%20reflect%20real-world%20use%20or%20isolate%20compliance%20from%20task%20success.%20We%20introduce%20MOSAIC%20%28MOdular%20Synthetic%20Assessment%20of%20Instruction%20Compliance%29%2C%20a%20modular%20framework%20that%20uses%20a%20dynamically%20generated%20dataset%20with%20up%20to%2020%20application-oriented%20generation%20constraints%20to%20enable%20a%20granular%20and%20independent%20analysis%20of%20this%20capability.%20Our%20evaluation%20of%20five%20LLMs%20from%20different%20families%20based%20on%20this%20new%20benchmark%20demonstrates%20that%20compliance%20is%20not%20a%20monolithic%20capability%20but%20varies%20significantly%20with%20constraint%20type%2C%20quantity%2C%20and%20position.%20The%20analysis%20reveals%20model-specific%20weaknesses%2C%20uncovers%20synergistic%20and%20conflicting%20interactions%20between%20instructions%2C%20and%20identifies%20distinct%20positional%20biases%20such%20as%20primacy%20and%20recency%20effects.%20These%20granular%20insights%20are%20critical%20for%20diagnosing%20model%20failures%20and%20developing%20more%20reliable%20LLMs%20for%20systems%20that%20demand%20strict%20adherence%20to%20complex%20instructions.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18554v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeconstructing%2520Instruction-Following%253A%2520A%2520New%2520Benchmark%2520for%2520Granular%2520Evaluation%2520of%2520Large%2520Language%2520Model%2520Instruction%2520Compliance%2520Abilities%26entry.906535625%3DAlberto%2520Purpura%2520and%2520Li%2520Wang%2520and%2520Sahil%2520Badyal%2520and%2520Eugenio%2520Beaufrand%2520and%2520Adam%2520Faulkner%26entry.1292438233%3DReliably%2520ensuring%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520follow%2520complex%2520instructions%2520is%2520a%2520critical%2520challenge%252C%2520as%2520existing%2520benchmarks%2520often%2520fail%2520to%2520reflect%2520real-world%2520use%2520or%2520isolate%2520compliance%2520from%2520task%2520success.%2520We%2520introduce%2520MOSAIC%2520%2528MOdular%2520Synthetic%2520Assessment%2520of%2520Instruction%2520Compliance%2529%252C%2520a%2520modular%2520framework%2520that%2520uses%2520a%2520dynamically%2520generated%2520dataset%2520with%2520up%2520to%252020%2520application-oriented%2520generation%2520constraints%2520to%2520enable%2520a%2520granular%2520and%2520independent%2520analysis%2520of%2520this%2520capability.%2520Our%2520evaluation%2520of%2520five%2520LLMs%2520from%2520different%2520families%2520based%2520on%2520this%2520new%2520benchmark%2520demonstrates%2520that%2520compliance%2520is%2520not%2520a%2520monolithic%2520capability%2520but%2520varies%2520significantly%2520with%2520constraint%2520type%252C%2520quantity%252C%2520and%2520position.%2520The%2520analysis%2520reveals%2520model-specific%2520weaknesses%252C%2520uncovers%2520synergistic%2520and%2520conflicting%2520interactions%2520between%2520instructions%252C%2520and%2520identifies%2520distinct%2520positional%2520biases%2520such%2520as%2520primacy%2520and%2520recency%2520effects.%2520These%2520granular%2520insights%2520are%2520critical%2520for%2520diagnosing%2520model%2520failures%2520and%2520developing%2520more%2520reliable%2520LLMs%2520for%2520systems%2520that%2520demand%2520strict%2520adherence%2520to%2520complex%2520instructions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18554v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deconstructing%20Instruction-Following%3A%20A%20New%20Benchmark%20for%20Granular%20Evaluation%20of%20Large%20Language%20Model%20Instruction%20Compliance%20Abilities&entry.906535625=Alberto%20Purpura%20and%20Li%20Wang%20and%20Sahil%20Badyal%20and%20Eugenio%20Beaufrand%20and%20Adam%20Faulkner&entry.1292438233=Reliably%20ensuring%20Large%20Language%20Models%20%28LLMs%29%20follow%20complex%20instructions%20is%20a%20critical%20challenge%2C%20as%20existing%20benchmarks%20often%20fail%20to%20reflect%20real-world%20use%20or%20isolate%20compliance%20from%20task%20success.%20We%20introduce%20MOSAIC%20%28MOdular%20Synthetic%20Assessment%20of%20Instruction%20Compliance%29%2C%20a%20modular%20framework%20that%20uses%20a%20dynamically%20generated%20dataset%20with%20up%20to%2020%20application-oriented%20generation%20constraints%20to%20enable%20a%20granular%20and%20independent%20analysis%20of%20this%20capability.%20Our%20evaluation%20of%20five%20LLMs%20from%20different%20families%20based%20on%20this%20new%20benchmark%20demonstrates%20that%20compliance%20is%20not%20a%20monolithic%20capability%20but%20varies%20significantly%20with%20constraint%20type%2C%20quantity%2C%20and%20position.%20The%20analysis%20reveals%20model-specific%20weaknesses%2C%20uncovers%20synergistic%20and%20conflicting%20interactions%20between%20instructions%2C%20and%20identifies%20distinct%20positional%20biases%20such%20as%20primacy%20and%20recency%20effects.%20These%20granular%20insights%20are%20critical%20for%20diagnosing%20model%20failures%20and%20developing%20more%20reliable%20LLMs%20for%20systems%20that%20demand%20strict%20adherence%20to%20complex%20instructions.&entry.1838667208=http%3A//arxiv.org/abs/2601.18554v1&entry.124074799=Read"},
{"title": "AI-enabled Satellite Edge Computing: A Single-Pixel Feature based Shallow Classification Model for Hyperspectral Imaging", "author": "Li Fang and Tianyu Li and Yanghong Lin and Shudong Zhou and Wei Yao", "abstract": "As the important component of the Earth observation system, hyperspectral imaging satellites provide high-fidelity and enriched information for the formulation of related policies due to the powerful spectral measurement capabilities. However, the transmission speed of the satellite downlink has become a major bottleneck in certain applications, such as disaster monitoring and emergency mapping, which demand a fast response ability. We propose an efficient AI-enabled Satellite Edge Computing paradigm for hyperspectral image classification, facilitating the satellites to attain autonomous decision-making. To accommodate the resource constraints of satellite platforms, the proposed method adopts a lightweight, non-deep learning framework integrated with a few-shot learning strategy. Moreover, onboard processing on satellites could be faced with sensor failure and scan pattern errors, which result in degraded image quality with bad/misaligned pixels and mixed noise. To address these challenges, we develop a novel two-stage pixel-wise label propagation scheme that utilizes only intrinsic spectral features at the single pixel level without the necessity to consider spatial structural information as requested by deep neural networks. In the first stage, initial pixel labels are obtained by propagating selected anchor labels through the constructed anchor-pixel affinity matrix. Subsequently, a top-k pruned sparse graph is generated by directly computing pixel-level similarities. In the second stage, a closed-form solution derived from the sparse graph is employed to replace iterative computations. Furthermore, we developed a rank constraint-based graph clustering algorithm to determine the anchor labels.", "link": "http://arxiv.org/abs/2601.18560v1", "date": "2026-01-26", "relevancy": 2.0156, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5089}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5042}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5016}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-enabled%20Satellite%20Edge%20Computing%3A%20A%20Single-Pixel%20Feature%20based%20Shallow%20Classification%20Model%20for%20Hyperspectral%20Imaging&body=Title%3A%20AI-enabled%20Satellite%20Edge%20Computing%3A%20A%20Single-Pixel%20Feature%20based%20Shallow%20Classification%20Model%20for%20Hyperspectral%20Imaging%0AAuthor%3A%20Li%20Fang%20and%20Tianyu%20Li%20and%20Yanghong%20Lin%20and%20Shudong%20Zhou%20and%20Wei%20Yao%0AAbstract%3A%20As%20the%20important%20component%20of%20the%20Earth%20observation%20system%2C%20hyperspectral%20imaging%20satellites%20provide%20high-fidelity%20and%20enriched%20information%20for%20the%20formulation%20of%20related%20policies%20due%20to%20the%20powerful%20spectral%20measurement%20capabilities.%20However%2C%20the%20transmission%20speed%20of%20the%20satellite%20downlink%20has%20become%20a%20major%20bottleneck%20in%20certain%20applications%2C%20such%20as%20disaster%20monitoring%20and%20emergency%20mapping%2C%20which%20demand%20a%20fast%20response%20ability.%20We%20propose%20an%20efficient%20AI-enabled%20Satellite%20Edge%20Computing%20paradigm%20for%20hyperspectral%20image%20classification%2C%20facilitating%20the%20satellites%20to%20attain%20autonomous%20decision-making.%20To%20accommodate%20the%20resource%20constraints%20of%20satellite%20platforms%2C%20the%20proposed%20method%20adopts%20a%20lightweight%2C%20non-deep%20learning%20framework%20integrated%20with%20a%20few-shot%20learning%20strategy.%20Moreover%2C%20onboard%20processing%20on%20satellites%20could%20be%20faced%20with%20sensor%20failure%20and%20scan%20pattern%20errors%2C%20which%20result%20in%20degraded%20image%20quality%20with%20bad/misaligned%20pixels%20and%20mixed%20noise.%20To%20address%20these%20challenges%2C%20we%20develop%20a%20novel%20two-stage%20pixel-wise%20label%20propagation%20scheme%20that%20utilizes%20only%20intrinsic%20spectral%20features%20at%20the%20single%20pixel%20level%20without%20the%20necessity%20to%20consider%20spatial%20structural%20information%20as%20requested%20by%20deep%20neural%20networks.%20In%20the%20first%20stage%2C%20initial%20pixel%20labels%20are%20obtained%20by%20propagating%20selected%20anchor%20labels%20through%20the%20constructed%20anchor-pixel%20affinity%20matrix.%20Subsequently%2C%20a%20top-k%20pruned%20sparse%20graph%20is%20generated%20by%20directly%20computing%20pixel-level%20similarities.%20In%20the%20second%20stage%2C%20a%20closed-form%20solution%20derived%20from%20the%20sparse%20graph%20is%20employed%20to%20replace%20iterative%20computations.%20Furthermore%2C%20we%20developed%20a%20rank%20constraint-based%20graph%20clustering%20algorithm%20to%20determine%20the%20anchor%20labels.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18560v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-enabled%2520Satellite%2520Edge%2520Computing%253A%2520A%2520Single-Pixel%2520Feature%2520based%2520Shallow%2520Classification%2520Model%2520for%2520Hyperspectral%2520Imaging%26entry.906535625%3DLi%2520Fang%2520and%2520Tianyu%2520Li%2520and%2520Yanghong%2520Lin%2520and%2520Shudong%2520Zhou%2520and%2520Wei%2520Yao%26entry.1292438233%3DAs%2520the%2520important%2520component%2520of%2520the%2520Earth%2520observation%2520system%252C%2520hyperspectral%2520imaging%2520satellites%2520provide%2520high-fidelity%2520and%2520enriched%2520information%2520for%2520the%2520formulation%2520of%2520related%2520policies%2520due%2520to%2520the%2520powerful%2520spectral%2520measurement%2520capabilities.%2520However%252C%2520the%2520transmission%2520speed%2520of%2520the%2520satellite%2520downlink%2520has%2520become%2520a%2520major%2520bottleneck%2520in%2520certain%2520applications%252C%2520such%2520as%2520disaster%2520monitoring%2520and%2520emergency%2520mapping%252C%2520which%2520demand%2520a%2520fast%2520response%2520ability.%2520We%2520propose%2520an%2520efficient%2520AI-enabled%2520Satellite%2520Edge%2520Computing%2520paradigm%2520for%2520hyperspectral%2520image%2520classification%252C%2520facilitating%2520the%2520satellites%2520to%2520attain%2520autonomous%2520decision-making.%2520To%2520accommodate%2520the%2520resource%2520constraints%2520of%2520satellite%2520platforms%252C%2520the%2520proposed%2520method%2520adopts%2520a%2520lightweight%252C%2520non-deep%2520learning%2520framework%2520integrated%2520with%2520a%2520few-shot%2520learning%2520strategy.%2520Moreover%252C%2520onboard%2520processing%2520on%2520satellites%2520could%2520be%2520faced%2520with%2520sensor%2520failure%2520and%2520scan%2520pattern%2520errors%252C%2520which%2520result%2520in%2520degraded%2520image%2520quality%2520with%2520bad/misaligned%2520pixels%2520and%2520mixed%2520noise.%2520To%2520address%2520these%2520challenges%252C%2520we%2520develop%2520a%2520novel%2520two-stage%2520pixel-wise%2520label%2520propagation%2520scheme%2520that%2520utilizes%2520only%2520intrinsic%2520spectral%2520features%2520at%2520the%2520single%2520pixel%2520level%2520without%2520the%2520necessity%2520to%2520consider%2520spatial%2520structural%2520information%2520as%2520requested%2520by%2520deep%2520neural%2520networks.%2520In%2520the%2520first%2520stage%252C%2520initial%2520pixel%2520labels%2520are%2520obtained%2520by%2520propagating%2520selected%2520anchor%2520labels%2520through%2520the%2520constructed%2520anchor-pixel%2520affinity%2520matrix.%2520Subsequently%252C%2520a%2520top-k%2520pruned%2520sparse%2520graph%2520is%2520generated%2520by%2520directly%2520computing%2520pixel-level%2520similarities.%2520In%2520the%2520second%2520stage%252C%2520a%2520closed-form%2520solution%2520derived%2520from%2520the%2520sparse%2520graph%2520is%2520employed%2520to%2520replace%2520iterative%2520computations.%2520Furthermore%252C%2520we%2520developed%2520a%2520rank%2520constraint-based%2520graph%2520clustering%2520algorithm%2520to%2520determine%2520the%2520anchor%2520labels.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18560v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-enabled%20Satellite%20Edge%20Computing%3A%20A%20Single-Pixel%20Feature%20based%20Shallow%20Classification%20Model%20for%20Hyperspectral%20Imaging&entry.906535625=Li%20Fang%20and%20Tianyu%20Li%20and%20Yanghong%20Lin%20and%20Shudong%20Zhou%20and%20Wei%20Yao&entry.1292438233=As%20the%20important%20component%20of%20the%20Earth%20observation%20system%2C%20hyperspectral%20imaging%20satellites%20provide%20high-fidelity%20and%20enriched%20information%20for%20the%20formulation%20of%20related%20policies%20due%20to%20the%20powerful%20spectral%20measurement%20capabilities.%20However%2C%20the%20transmission%20speed%20of%20the%20satellite%20downlink%20has%20become%20a%20major%20bottleneck%20in%20certain%20applications%2C%20such%20as%20disaster%20monitoring%20and%20emergency%20mapping%2C%20which%20demand%20a%20fast%20response%20ability.%20We%20propose%20an%20efficient%20AI-enabled%20Satellite%20Edge%20Computing%20paradigm%20for%20hyperspectral%20image%20classification%2C%20facilitating%20the%20satellites%20to%20attain%20autonomous%20decision-making.%20To%20accommodate%20the%20resource%20constraints%20of%20satellite%20platforms%2C%20the%20proposed%20method%20adopts%20a%20lightweight%2C%20non-deep%20learning%20framework%20integrated%20with%20a%20few-shot%20learning%20strategy.%20Moreover%2C%20onboard%20processing%20on%20satellites%20could%20be%20faced%20with%20sensor%20failure%20and%20scan%20pattern%20errors%2C%20which%20result%20in%20degraded%20image%20quality%20with%20bad/misaligned%20pixels%20and%20mixed%20noise.%20To%20address%20these%20challenges%2C%20we%20develop%20a%20novel%20two-stage%20pixel-wise%20label%20propagation%20scheme%20that%20utilizes%20only%20intrinsic%20spectral%20features%20at%20the%20single%20pixel%20level%20without%20the%20necessity%20to%20consider%20spatial%20structural%20information%20as%20requested%20by%20deep%20neural%20networks.%20In%20the%20first%20stage%2C%20initial%20pixel%20labels%20are%20obtained%20by%20propagating%20selected%20anchor%20labels%20through%20the%20constructed%20anchor-pixel%20affinity%20matrix.%20Subsequently%2C%20a%20top-k%20pruned%20sparse%20graph%20is%20generated%20by%20directly%20computing%20pixel-level%20similarities.%20In%20the%20second%20stage%2C%20a%20closed-form%20solution%20derived%20from%20the%20sparse%20graph%20is%20employed%20to%20replace%20iterative%20computations.%20Furthermore%2C%20we%20developed%20a%20rank%20constraint-based%20graph%20clustering%20algorithm%20to%20determine%20the%20anchor%20labels.&entry.1838667208=http%3A//arxiv.org/abs/2601.18560v1&entry.124074799=Read"},
{"title": "OffSeeker: Online Reinforcement Learning Is Not All You Need for Deep Research Agents", "author": "Yuhang Zhou and Kai Zheng and Qiguang Chen and Mengkang Hu and Qingfeng Sun and Can Xu and Jingjing Chen", "abstract": "Deep research agents have shown remarkable potential in handling long-horizon tasks. However, state-of-the-art performance typically relies on online reinforcement learning (RL), which is financially expensive due to extensive API calls. While offline training offers a more efficient alternative, its progress is hindered by the scarcity of high-quality research trajectories. In this paper, we demonstrate that expensive online reinforcement learning is not all you need to build powerful research agents. To bridge this gap, we introduce a fully open-source suite designed for effective offline training. Our core contributions include DeepForge, a ready-to-use task synthesis framework that generates large-scale research queries without heavy preprocessing; and a curated collection of 66k QA pairs, 33k SFT trajectories, and 21k DPO pairs. Leveraging these resources, we train OffSeeker (8B), a model developed entirely offline. Extensive evaluations across six benchmarks show that OffSeeker not only leads among similar-sized agents but also remains competitive with 30B-parameter systems trained via heavy online RL.", "link": "http://arxiv.org/abs/2601.18467v1", "date": "2026-01-26", "relevancy": 1.504, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5241}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4827}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OffSeeker%3A%20Online%20Reinforcement%20Learning%20Is%20Not%20All%20You%20Need%20for%20Deep%20Research%20Agents&body=Title%3A%20OffSeeker%3A%20Online%20Reinforcement%20Learning%20Is%20Not%20All%20You%20Need%20for%20Deep%20Research%20Agents%0AAuthor%3A%20Yuhang%20Zhou%20and%20Kai%20Zheng%20and%20Qiguang%20Chen%20and%20Mengkang%20Hu%20and%20Qingfeng%20Sun%20and%20Can%20Xu%20and%20Jingjing%20Chen%0AAbstract%3A%20Deep%20research%20agents%20have%20shown%20remarkable%20potential%20in%20handling%20long-horizon%20tasks.%20However%2C%20state-of-the-art%20performance%20typically%20relies%20on%20online%20reinforcement%20learning%20%28RL%29%2C%20which%20is%20financially%20expensive%20due%20to%20extensive%20API%20calls.%20While%20offline%20training%20offers%20a%20more%20efficient%20alternative%2C%20its%20progress%20is%20hindered%20by%20the%20scarcity%20of%20high-quality%20research%20trajectories.%20In%20this%20paper%2C%20we%20demonstrate%20that%20expensive%20online%20reinforcement%20learning%20is%20not%20all%20you%20need%20to%20build%20powerful%20research%20agents.%20To%20bridge%20this%20gap%2C%20we%20introduce%20a%20fully%20open-source%20suite%20designed%20for%20effective%20offline%20training.%20Our%20core%20contributions%20include%20DeepForge%2C%20a%20ready-to-use%20task%20synthesis%20framework%20that%20generates%20large-scale%20research%20queries%20without%20heavy%20preprocessing%3B%20and%20a%20curated%20collection%20of%2066k%20QA%20pairs%2C%2033k%20SFT%20trajectories%2C%20and%2021k%20DPO%20pairs.%20Leveraging%20these%20resources%2C%20we%20train%20OffSeeker%20%288B%29%2C%20a%20model%20developed%20entirely%20offline.%20Extensive%20evaluations%20across%20six%20benchmarks%20show%20that%20OffSeeker%20not%20only%20leads%20among%20similar-sized%20agents%20but%20also%20remains%20competitive%20with%2030B-parameter%20systems%20trained%20via%20heavy%20online%20RL.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18467v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOffSeeker%253A%2520Online%2520Reinforcement%2520Learning%2520Is%2520Not%2520All%2520You%2520Need%2520for%2520Deep%2520Research%2520Agents%26entry.906535625%3DYuhang%2520Zhou%2520and%2520Kai%2520Zheng%2520and%2520Qiguang%2520Chen%2520and%2520Mengkang%2520Hu%2520and%2520Qingfeng%2520Sun%2520and%2520Can%2520Xu%2520and%2520Jingjing%2520Chen%26entry.1292438233%3DDeep%2520research%2520agents%2520have%2520shown%2520remarkable%2520potential%2520in%2520handling%2520long-horizon%2520tasks.%2520However%252C%2520state-of-the-art%2520performance%2520typically%2520relies%2520on%2520online%2520reinforcement%2520learning%2520%2528RL%2529%252C%2520which%2520is%2520financially%2520expensive%2520due%2520to%2520extensive%2520API%2520calls.%2520While%2520offline%2520training%2520offers%2520a%2520more%2520efficient%2520alternative%252C%2520its%2520progress%2520is%2520hindered%2520by%2520the%2520scarcity%2520of%2520high-quality%2520research%2520trajectories.%2520In%2520this%2520paper%252C%2520we%2520demonstrate%2520that%2520expensive%2520online%2520reinforcement%2520learning%2520is%2520not%2520all%2520you%2520need%2520to%2520build%2520powerful%2520research%2520agents.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520a%2520fully%2520open-source%2520suite%2520designed%2520for%2520effective%2520offline%2520training.%2520Our%2520core%2520contributions%2520include%2520DeepForge%252C%2520a%2520ready-to-use%2520task%2520synthesis%2520framework%2520that%2520generates%2520large-scale%2520research%2520queries%2520without%2520heavy%2520preprocessing%253B%2520and%2520a%2520curated%2520collection%2520of%252066k%2520QA%2520pairs%252C%252033k%2520SFT%2520trajectories%252C%2520and%252021k%2520DPO%2520pairs.%2520Leveraging%2520these%2520resources%252C%2520we%2520train%2520OffSeeker%2520%25288B%2529%252C%2520a%2520model%2520developed%2520entirely%2520offline.%2520Extensive%2520evaluations%2520across%2520six%2520benchmarks%2520show%2520that%2520OffSeeker%2520not%2520only%2520leads%2520among%2520similar-sized%2520agents%2520but%2520also%2520remains%2520competitive%2520with%252030B-parameter%2520systems%2520trained%2520via%2520heavy%2520online%2520RL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18467v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OffSeeker%3A%20Online%20Reinforcement%20Learning%20Is%20Not%20All%20You%20Need%20for%20Deep%20Research%20Agents&entry.906535625=Yuhang%20Zhou%20and%20Kai%20Zheng%20and%20Qiguang%20Chen%20and%20Mengkang%20Hu%20and%20Qingfeng%20Sun%20and%20Can%20Xu%20and%20Jingjing%20Chen&entry.1292438233=Deep%20research%20agents%20have%20shown%20remarkable%20potential%20in%20handling%20long-horizon%20tasks.%20However%2C%20state-of-the-art%20performance%20typically%20relies%20on%20online%20reinforcement%20learning%20%28RL%29%2C%20which%20is%20financially%20expensive%20due%20to%20extensive%20API%20calls.%20While%20offline%20training%20offers%20a%20more%20efficient%20alternative%2C%20its%20progress%20is%20hindered%20by%20the%20scarcity%20of%20high-quality%20research%20trajectories.%20In%20this%20paper%2C%20we%20demonstrate%20that%20expensive%20online%20reinforcement%20learning%20is%20not%20all%20you%20need%20to%20build%20powerful%20research%20agents.%20To%20bridge%20this%20gap%2C%20we%20introduce%20a%20fully%20open-source%20suite%20designed%20for%20effective%20offline%20training.%20Our%20core%20contributions%20include%20DeepForge%2C%20a%20ready-to-use%20task%20synthesis%20framework%20that%20generates%20large-scale%20research%20queries%20without%20heavy%20preprocessing%3B%20and%20a%20curated%20collection%20of%2066k%20QA%20pairs%2C%2033k%20SFT%20trajectories%2C%20and%2021k%20DPO%20pairs.%20Leveraging%20these%20resources%2C%20we%20train%20OffSeeker%20%288B%29%2C%20a%20model%20developed%20entirely%20offline.%20Extensive%20evaluations%20across%20six%20benchmarks%20show%20that%20OffSeeker%20not%20only%20leads%20among%20similar-sized%20agents%20but%20also%20remains%20competitive%20with%2030B-parameter%20systems%20trained%20via%20heavy%20online%20RL.&entry.1838667208=http%3A//arxiv.org/abs/2601.18467v1&entry.124074799=Read"},
{"title": "Resisting Manipulative Bots in Meme Coin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning", "author": "Yichen Luo and Yebo Feng and Jiahua Xu and Yang Liu", "abstract": "Copy trading has become the dominant entry strategy in meme coin markets. However, due to the market's extreme illiquid and volatile nature, the strategy exposes an exploitable attack surface: adversaries deploy manipulative bots to front-run trades, conceal positions, and fabricate sentiment, systematically extracting value from na\u00efve copiers at scale. Despite its prevalence, bot-driven manipulation remains largely unexplored, and no robust defensive framework exists. We propose a manipulation-resistant copy-trading system based on a multi-agent architecture powered by a multi-modal, explainable large language model (LLM). Our system decomposes copy trading into three specialized agents for coin evaluation, wallet selection, and timing assessment. Evaluated on historical data from over 6,000 meme coins, our approach outperforms zero-shot and most statistic-driven baselines in prediction accuracy as well as all baselines in economic performance, achieving an average return of 14% for identified smart-money trades and an estimated copier return of 3% per trade under realistic market frictions. Overall, our results demonstrate the effectiveness of agent-based defenses and predictability of trader profitability in adversarial meme coin markets, providing a practical foundation for robust copy trading.", "link": "http://arxiv.org/abs/2601.08641v2", "date": "2026-01-26", "relevancy": 1.3411, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4579}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4353}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Resisting%20Manipulative%20Bots%20in%20Meme%20Coin%20Copy%20Trading%3A%20A%20Multi-Agent%20Approach%20with%20Chain-of-Thought%20Reasoning&body=Title%3A%20Resisting%20Manipulative%20Bots%20in%20Meme%20Coin%20Copy%20Trading%3A%20A%20Multi-Agent%20Approach%20with%20Chain-of-Thought%20Reasoning%0AAuthor%3A%20Yichen%20Luo%20and%20Yebo%20Feng%20and%20Jiahua%20Xu%20and%20Yang%20Liu%0AAbstract%3A%20Copy%20trading%20has%20become%20the%20dominant%20entry%20strategy%20in%20meme%20coin%20markets.%20However%2C%20due%20to%20the%20market%27s%20extreme%20illiquid%20and%20volatile%20nature%2C%20the%20strategy%20exposes%20an%20exploitable%20attack%20surface%3A%20adversaries%20deploy%20manipulative%20bots%20to%20front-run%20trades%2C%20conceal%20positions%2C%20and%20fabricate%20sentiment%2C%20systematically%20extracting%20value%20from%20na%C3%AFve%20copiers%20at%20scale.%20Despite%20its%20prevalence%2C%20bot-driven%20manipulation%20remains%20largely%20unexplored%2C%20and%20no%20robust%20defensive%20framework%20exists.%20We%20propose%20a%20manipulation-resistant%20copy-trading%20system%20based%20on%20a%20multi-agent%20architecture%20powered%20by%20a%20multi-modal%2C%20explainable%20large%20language%20model%20%28LLM%29.%20Our%20system%20decomposes%20copy%20trading%20into%20three%20specialized%20agents%20for%20coin%20evaluation%2C%20wallet%20selection%2C%20and%20timing%20assessment.%20Evaluated%20on%20historical%20data%20from%20over%206%2C000%20meme%20coins%2C%20our%20approach%20outperforms%20zero-shot%20and%20most%20statistic-driven%20baselines%20in%20prediction%20accuracy%20as%20well%20as%20all%20baselines%20in%20economic%20performance%2C%20achieving%20an%20average%20return%20of%2014%25%20for%20identified%20smart-money%20trades%20and%20an%20estimated%20copier%20return%20of%203%25%20per%20trade%20under%20realistic%20market%20frictions.%20Overall%2C%20our%20results%20demonstrate%20the%20effectiveness%20of%20agent-based%20defenses%20and%20predictability%20of%20trader%20profitability%20in%20adversarial%20meme%20coin%20markets%2C%20providing%20a%20practical%20foundation%20for%20robust%20copy%20trading.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08641v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResisting%2520Manipulative%2520Bots%2520in%2520Meme%2520Coin%2520Copy%2520Trading%253A%2520A%2520Multi-Agent%2520Approach%2520with%2520Chain-of-Thought%2520Reasoning%26entry.906535625%3DYichen%2520Luo%2520and%2520Yebo%2520Feng%2520and%2520Jiahua%2520Xu%2520and%2520Yang%2520Liu%26entry.1292438233%3DCopy%2520trading%2520has%2520become%2520the%2520dominant%2520entry%2520strategy%2520in%2520meme%2520coin%2520markets.%2520However%252C%2520due%2520to%2520the%2520market%2527s%2520extreme%2520illiquid%2520and%2520volatile%2520nature%252C%2520the%2520strategy%2520exposes%2520an%2520exploitable%2520attack%2520surface%253A%2520adversaries%2520deploy%2520manipulative%2520bots%2520to%2520front-run%2520trades%252C%2520conceal%2520positions%252C%2520and%2520fabricate%2520sentiment%252C%2520systematically%2520extracting%2520value%2520from%2520na%25C3%25AFve%2520copiers%2520at%2520scale.%2520Despite%2520its%2520prevalence%252C%2520bot-driven%2520manipulation%2520remains%2520largely%2520unexplored%252C%2520and%2520no%2520robust%2520defensive%2520framework%2520exists.%2520We%2520propose%2520a%2520manipulation-resistant%2520copy-trading%2520system%2520based%2520on%2520a%2520multi-agent%2520architecture%2520powered%2520by%2520a%2520multi-modal%252C%2520explainable%2520large%2520language%2520model%2520%2528LLM%2529.%2520Our%2520system%2520decomposes%2520copy%2520trading%2520into%2520three%2520specialized%2520agents%2520for%2520coin%2520evaluation%252C%2520wallet%2520selection%252C%2520and%2520timing%2520assessment.%2520Evaluated%2520on%2520historical%2520data%2520from%2520over%25206%252C000%2520meme%2520coins%252C%2520our%2520approach%2520outperforms%2520zero-shot%2520and%2520most%2520statistic-driven%2520baselines%2520in%2520prediction%2520accuracy%2520as%2520well%2520as%2520all%2520baselines%2520in%2520economic%2520performance%252C%2520achieving%2520an%2520average%2520return%2520of%252014%2525%2520for%2520identified%2520smart-money%2520trades%2520and%2520an%2520estimated%2520copier%2520return%2520of%25203%2525%2520per%2520trade%2520under%2520realistic%2520market%2520frictions.%2520Overall%252C%2520our%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520agent-based%2520defenses%2520and%2520predictability%2520of%2520trader%2520profitability%2520in%2520adversarial%2520meme%2520coin%2520markets%252C%2520providing%2520a%2520practical%2520foundation%2520for%2520robust%2520copy%2520trading.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08641v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Resisting%20Manipulative%20Bots%20in%20Meme%20Coin%20Copy%20Trading%3A%20A%20Multi-Agent%20Approach%20with%20Chain-of-Thought%20Reasoning&entry.906535625=Yichen%20Luo%20and%20Yebo%20Feng%20and%20Jiahua%20Xu%20and%20Yang%20Liu&entry.1292438233=Copy%20trading%20has%20become%20the%20dominant%20entry%20strategy%20in%20meme%20coin%20markets.%20However%2C%20due%20to%20the%20market%27s%20extreme%20illiquid%20and%20volatile%20nature%2C%20the%20strategy%20exposes%20an%20exploitable%20attack%20surface%3A%20adversaries%20deploy%20manipulative%20bots%20to%20front-run%20trades%2C%20conceal%20positions%2C%20and%20fabricate%20sentiment%2C%20systematically%20extracting%20value%20from%20na%C3%AFve%20copiers%20at%20scale.%20Despite%20its%20prevalence%2C%20bot-driven%20manipulation%20remains%20largely%20unexplored%2C%20and%20no%20robust%20defensive%20framework%20exists.%20We%20propose%20a%20manipulation-resistant%20copy-trading%20system%20based%20on%20a%20multi-agent%20architecture%20powered%20by%20a%20multi-modal%2C%20explainable%20large%20language%20model%20%28LLM%29.%20Our%20system%20decomposes%20copy%20trading%20into%20three%20specialized%20agents%20for%20coin%20evaluation%2C%20wallet%20selection%2C%20and%20timing%20assessment.%20Evaluated%20on%20historical%20data%20from%20over%206%2C000%20meme%20coins%2C%20our%20approach%20outperforms%20zero-shot%20and%20most%20statistic-driven%20baselines%20in%20prediction%20accuracy%20as%20well%20as%20all%20baselines%20in%20economic%20performance%2C%20achieving%20an%20average%20return%20of%2014%25%20for%20identified%20smart-money%20trades%20and%20an%20estimated%20copier%20return%20of%203%25%20per%20trade%20under%20realistic%20market%20frictions.%20Overall%2C%20our%20results%20demonstrate%20the%20effectiveness%20of%20agent-based%20defenses%20and%20predictability%20of%20trader%20profitability%20in%20adversarial%20meme%20coin%20markets%2C%20providing%20a%20practical%20foundation%20for%20robust%20copy%20trading.&entry.1838667208=http%3A//arxiv.org/abs/2601.08641v2&entry.124074799=Read"},
{"title": "Estimating Dense-Packed Zone Height in Liquid-Liquid Separation: A Physics-Informed Neural Network Approach", "author": "Mehmet Velioglu and Song Zhai and Alexander Mitsos and Adel Mhamdi and Andreas Jupke and Manuel Dahmen", "abstract": "Separating liquid-liquid dispersions in gravity settlers is critical in chemical, pharmaceutical, and recycling processes. The dense-packed zone height is an important performance and safety indicator but it is often expensive and impractical to measure due to optical limitations. We propose to estimate phase heights using only inexpensive volume flow measurements. To this end, a physics-informed neural network (PINN) is first pretrained on synthetic data and physics equations derived from a low-fidelity (approximate) mechanistic model to reduce the need for extensive experimental data. While the mechanistic model is used to generate synthetic training data, only volume balance equations are used in the PINN, since the integration of submodels describing droplet coalescence and sedimentation into the PINN would be computationally prohibitive. The pretrained PINN is then fine-tuned with scarce experimental data to capture the actual dynamics of the separator. We then employ the differentiable PINN as a predictive model in an Extended Kalman Filter inspired state estimation framework, enabling the phase heights to be tracked and updated from flow-rate measurements. We first test the two-stage trained PINN by forward simulation from a known initial state against the mechanistic model and a non-pretrained PINN. We then evaluate phase height estimation performance with the filter, comparing the two-stage trained PINN with a two-stage trained purely data-driven neural network. All model types are trained and evaluated using ensembles to account for model parameter uncertainty. In all evaluations, the two-stage trained PINN yields the most accurate phase-height estimates.", "link": "http://arxiv.org/abs/2601.18399v1", "date": "2026-01-26", "relevancy": 1.4402, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4962}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4832}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Estimating%20Dense-Packed%20Zone%20Height%20in%20Liquid-Liquid%20Separation%3A%20A%20Physics-Informed%20Neural%20Network%20Approach&body=Title%3A%20Estimating%20Dense-Packed%20Zone%20Height%20in%20Liquid-Liquid%20Separation%3A%20A%20Physics-Informed%20Neural%20Network%20Approach%0AAuthor%3A%20Mehmet%20Velioglu%20and%20Song%20Zhai%20and%20Alexander%20Mitsos%20and%20Adel%20Mhamdi%20and%20Andreas%20Jupke%20and%20Manuel%20Dahmen%0AAbstract%3A%20Separating%20liquid-liquid%20dispersions%20in%20gravity%20settlers%20is%20critical%20in%20chemical%2C%20pharmaceutical%2C%20and%20recycling%20processes.%20The%20dense-packed%20zone%20height%20is%20an%20important%20performance%20and%20safety%20indicator%20but%20it%20is%20often%20expensive%20and%20impractical%20to%20measure%20due%20to%20optical%20limitations.%20We%20propose%20to%20estimate%20phase%20heights%20using%20only%20inexpensive%20volume%20flow%20measurements.%20To%20this%20end%2C%20a%20physics-informed%20neural%20network%20%28PINN%29%20is%20first%20pretrained%20on%20synthetic%20data%20and%20physics%20equations%20derived%20from%20a%20low-fidelity%20%28approximate%29%20mechanistic%20model%20to%20reduce%20the%20need%20for%20extensive%20experimental%20data.%20While%20the%20mechanistic%20model%20is%20used%20to%20generate%20synthetic%20training%20data%2C%20only%20volume%20balance%20equations%20are%20used%20in%20the%20PINN%2C%20since%20the%20integration%20of%20submodels%20describing%20droplet%20coalescence%20and%20sedimentation%20into%20the%20PINN%20would%20be%20computationally%20prohibitive.%20The%20pretrained%20PINN%20is%20then%20fine-tuned%20with%20scarce%20experimental%20data%20to%20capture%20the%20actual%20dynamics%20of%20the%20separator.%20We%20then%20employ%20the%20differentiable%20PINN%20as%20a%20predictive%20model%20in%20an%20Extended%20Kalman%20Filter%20inspired%20state%20estimation%20framework%2C%20enabling%20the%20phase%20heights%20to%20be%20tracked%20and%20updated%20from%20flow-rate%20measurements.%20We%20first%20test%20the%20two-stage%20trained%20PINN%20by%20forward%20simulation%20from%20a%20known%20initial%20state%20against%20the%20mechanistic%20model%20and%20a%20non-pretrained%20PINN.%20We%20then%20evaluate%20phase%20height%20estimation%20performance%20with%20the%20filter%2C%20comparing%20the%20two-stage%20trained%20PINN%20with%20a%20two-stage%20trained%20purely%20data-driven%20neural%20network.%20All%20model%20types%20are%20trained%20and%20evaluated%20using%20ensembles%20to%20account%20for%20model%20parameter%20uncertainty.%20In%20all%20evaluations%2C%20the%20two-stage%20trained%20PINN%20yields%20the%20most%20accurate%20phase-height%20estimates.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18399v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstimating%2520Dense-Packed%2520Zone%2520Height%2520in%2520Liquid-Liquid%2520Separation%253A%2520A%2520Physics-Informed%2520Neural%2520Network%2520Approach%26entry.906535625%3DMehmet%2520Velioglu%2520and%2520Song%2520Zhai%2520and%2520Alexander%2520Mitsos%2520and%2520Adel%2520Mhamdi%2520and%2520Andreas%2520Jupke%2520and%2520Manuel%2520Dahmen%26entry.1292438233%3DSeparating%2520liquid-liquid%2520dispersions%2520in%2520gravity%2520settlers%2520is%2520critical%2520in%2520chemical%252C%2520pharmaceutical%252C%2520and%2520recycling%2520processes.%2520The%2520dense-packed%2520zone%2520height%2520is%2520an%2520important%2520performance%2520and%2520safety%2520indicator%2520but%2520it%2520is%2520often%2520expensive%2520and%2520impractical%2520to%2520measure%2520due%2520to%2520optical%2520limitations.%2520We%2520propose%2520to%2520estimate%2520phase%2520heights%2520using%2520only%2520inexpensive%2520volume%2520flow%2520measurements.%2520To%2520this%2520end%252C%2520a%2520physics-informed%2520neural%2520network%2520%2528PINN%2529%2520is%2520first%2520pretrained%2520on%2520synthetic%2520data%2520and%2520physics%2520equations%2520derived%2520from%2520a%2520low-fidelity%2520%2528approximate%2529%2520mechanistic%2520model%2520to%2520reduce%2520the%2520need%2520for%2520extensive%2520experimental%2520data.%2520While%2520the%2520mechanistic%2520model%2520is%2520used%2520to%2520generate%2520synthetic%2520training%2520data%252C%2520only%2520volume%2520balance%2520equations%2520are%2520used%2520in%2520the%2520PINN%252C%2520since%2520the%2520integration%2520of%2520submodels%2520describing%2520droplet%2520coalescence%2520and%2520sedimentation%2520into%2520the%2520PINN%2520would%2520be%2520computationally%2520prohibitive.%2520The%2520pretrained%2520PINN%2520is%2520then%2520fine-tuned%2520with%2520scarce%2520experimental%2520data%2520to%2520capture%2520the%2520actual%2520dynamics%2520of%2520the%2520separator.%2520We%2520then%2520employ%2520the%2520differentiable%2520PINN%2520as%2520a%2520predictive%2520model%2520in%2520an%2520Extended%2520Kalman%2520Filter%2520inspired%2520state%2520estimation%2520framework%252C%2520enabling%2520the%2520phase%2520heights%2520to%2520be%2520tracked%2520and%2520updated%2520from%2520flow-rate%2520measurements.%2520We%2520first%2520test%2520the%2520two-stage%2520trained%2520PINN%2520by%2520forward%2520simulation%2520from%2520a%2520known%2520initial%2520state%2520against%2520the%2520mechanistic%2520model%2520and%2520a%2520non-pretrained%2520PINN.%2520We%2520then%2520evaluate%2520phase%2520height%2520estimation%2520performance%2520with%2520the%2520filter%252C%2520comparing%2520the%2520two-stage%2520trained%2520PINN%2520with%2520a%2520two-stage%2520trained%2520purely%2520data-driven%2520neural%2520network.%2520All%2520model%2520types%2520are%2520trained%2520and%2520evaluated%2520using%2520ensembles%2520to%2520account%2520for%2520model%2520parameter%2520uncertainty.%2520In%2520all%2520evaluations%252C%2520the%2520two-stage%2520trained%2520PINN%2520yields%2520the%2520most%2520accurate%2520phase-height%2520estimates.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18399v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimating%20Dense-Packed%20Zone%20Height%20in%20Liquid-Liquid%20Separation%3A%20A%20Physics-Informed%20Neural%20Network%20Approach&entry.906535625=Mehmet%20Velioglu%20and%20Song%20Zhai%20and%20Alexander%20Mitsos%20and%20Adel%20Mhamdi%20and%20Andreas%20Jupke%20and%20Manuel%20Dahmen&entry.1292438233=Separating%20liquid-liquid%20dispersions%20in%20gravity%20settlers%20is%20critical%20in%20chemical%2C%20pharmaceutical%2C%20and%20recycling%20processes.%20The%20dense-packed%20zone%20height%20is%20an%20important%20performance%20and%20safety%20indicator%20but%20it%20is%20often%20expensive%20and%20impractical%20to%20measure%20due%20to%20optical%20limitations.%20We%20propose%20to%20estimate%20phase%20heights%20using%20only%20inexpensive%20volume%20flow%20measurements.%20To%20this%20end%2C%20a%20physics-informed%20neural%20network%20%28PINN%29%20is%20first%20pretrained%20on%20synthetic%20data%20and%20physics%20equations%20derived%20from%20a%20low-fidelity%20%28approximate%29%20mechanistic%20model%20to%20reduce%20the%20need%20for%20extensive%20experimental%20data.%20While%20the%20mechanistic%20model%20is%20used%20to%20generate%20synthetic%20training%20data%2C%20only%20volume%20balance%20equations%20are%20used%20in%20the%20PINN%2C%20since%20the%20integration%20of%20submodels%20describing%20droplet%20coalescence%20and%20sedimentation%20into%20the%20PINN%20would%20be%20computationally%20prohibitive.%20The%20pretrained%20PINN%20is%20then%20fine-tuned%20with%20scarce%20experimental%20data%20to%20capture%20the%20actual%20dynamics%20of%20the%20separator.%20We%20then%20employ%20the%20differentiable%20PINN%20as%20a%20predictive%20model%20in%20an%20Extended%20Kalman%20Filter%20inspired%20state%20estimation%20framework%2C%20enabling%20the%20phase%20heights%20to%20be%20tracked%20and%20updated%20from%20flow-rate%20measurements.%20We%20first%20test%20the%20two-stage%20trained%20PINN%20by%20forward%20simulation%20from%20a%20known%20initial%20state%20against%20the%20mechanistic%20model%20and%20a%20non-pretrained%20PINN.%20We%20then%20evaluate%20phase%20height%20estimation%20performance%20with%20the%20filter%2C%20comparing%20the%20two-stage%20trained%20PINN%20with%20a%20two-stage%20trained%20purely%20data-driven%20neural%20network.%20All%20model%20types%20are%20trained%20and%20evaluated%20using%20ensembles%20to%20account%20for%20model%20parameter%20uncertainty.%20In%20all%20evaluations%2C%20the%20two-stage%20trained%20PINN%20yields%20the%20most%20accurate%20phase-height%20estimates.&entry.1838667208=http%3A//arxiv.org/abs/2601.18399v1&entry.124074799=Read"},
{"title": "Beyond Expected Goals: A Probabilistic Framework for Shot Occurrences in Soccer", "author": "Jonathan Pipping-Gam\u00f3n and Tianshu Feng and R. Paul Sabin", "abstract": "Expected goals (xG) models estimate the probability that a shot results in a goal from its context (e.g., location, pressure), but they operate only on observed shots. We propose xG+, a possession-level framework that first estimates the probability that a shot occurs within the next second and its corresponding xG if it were to occur. We also introduce ways to aggregate this joint probability estimate over the course of a possession. By jointly modeling shot-taking behavior and shot quality, xG+ remedies the conditioning-on-shots limitation of standard xG. We show that this improves predictive accuracy at the team level and produces a more persistent player skill signal than standard xG models.", "link": "http://arxiv.org/abs/2512.00203v2", "date": "2026-01-26", "relevancy": 1.3999, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4694}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4674}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Expected%20Goals%3A%20A%20Probabilistic%20Framework%20for%20Shot%20Occurrences%20in%20Soccer&body=Title%3A%20Beyond%20Expected%20Goals%3A%20A%20Probabilistic%20Framework%20for%20Shot%20Occurrences%20in%20Soccer%0AAuthor%3A%20Jonathan%20Pipping-Gam%C3%B3n%20and%20Tianshu%20Feng%20and%20R.%20Paul%20Sabin%0AAbstract%3A%20Expected%20goals%20%28xG%29%20models%20estimate%20the%20probability%20that%20a%20shot%20results%20in%20a%20goal%20from%20its%20context%20%28e.g.%2C%20location%2C%20pressure%29%2C%20but%20they%20operate%20only%20on%20observed%20shots.%20We%20propose%20xG%2B%2C%20a%20possession-level%20framework%20that%20first%20estimates%20the%20probability%20that%20a%20shot%20occurs%20within%20the%20next%20second%20and%20its%20corresponding%20xG%20if%20it%20were%20to%20occur.%20We%20also%20introduce%20ways%20to%20aggregate%20this%20joint%20probability%20estimate%20over%20the%20course%20of%20a%20possession.%20By%20jointly%20modeling%20shot-taking%20behavior%20and%20shot%20quality%2C%20xG%2B%20remedies%20the%20conditioning-on-shots%20limitation%20of%20standard%20xG.%20We%20show%20that%20this%20improves%20predictive%20accuracy%20at%20the%20team%20level%20and%20produces%20a%20more%20persistent%20player%20skill%20signal%20than%20standard%20xG%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2512.00203v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Expected%2520Goals%253A%2520A%2520Probabilistic%2520Framework%2520for%2520Shot%2520Occurrences%2520in%2520Soccer%26entry.906535625%3DJonathan%2520Pipping-Gam%25C3%25B3n%2520and%2520Tianshu%2520Feng%2520and%2520R.%2520Paul%2520Sabin%26entry.1292438233%3DExpected%2520goals%2520%2528xG%2529%2520models%2520estimate%2520the%2520probability%2520that%2520a%2520shot%2520results%2520in%2520a%2520goal%2520from%2520its%2520context%2520%2528e.g.%252C%2520location%252C%2520pressure%2529%252C%2520but%2520they%2520operate%2520only%2520on%2520observed%2520shots.%2520We%2520propose%2520xG%252B%252C%2520a%2520possession-level%2520framework%2520that%2520first%2520estimates%2520the%2520probability%2520that%2520a%2520shot%2520occurs%2520within%2520the%2520next%2520second%2520and%2520its%2520corresponding%2520xG%2520if%2520it%2520were%2520to%2520occur.%2520We%2520also%2520introduce%2520ways%2520to%2520aggregate%2520this%2520joint%2520probability%2520estimate%2520over%2520the%2520course%2520of%2520a%2520possession.%2520By%2520jointly%2520modeling%2520shot-taking%2520behavior%2520and%2520shot%2520quality%252C%2520xG%252B%2520remedies%2520the%2520conditioning-on-shots%2520limitation%2520of%2520standard%2520xG.%2520We%2520show%2520that%2520this%2520improves%2520predictive%2520accuracy%2520at%2520the%2520team%2520level%2520and%2520produces%2520a%2520more%2520persistent%2520player%2520skill%2520signal%2520than%2520standard%2520xG%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.00203v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Expected%20Goals%3A%20A%20Probabilistic%20Framework%20for%20Shot%20Occurrences%20in%20Soccer&entry.906535625=Jonathan%20Pipping-Gam%C3%B3n%20and%20Tianshu%20Feng%20and%20R.%20Paul%20Sabin&entry.1292438233=Expected%20goals%20%28xG%29%20models%20estimate%20the%20probability%20that%20a%20shot%20results%20in%20a%20goal%20from%20its%20context%20%28e.g.%2C%20location%2C%20pressure%29%2C%20but%20they%20operate%20only%20on%20observed%20shots.%20We%20propose%20xG%2B%2C%20a%20possession-level%20framework%20that%20first%20estimates%20the%20probability%20that%20a%20shot%20occurs%20within%20the%20next%20second%20and%20its%20corresponding%20xG%20if%20it%20were%20to%20occur.%20We%20also%20introduce%20ways%20to%20aggregate%20this%20joint%20probability%20estimate%20over%20the%20course%20of%20a%20possession.%20By%20jointly%20modeling%20shot-taking%20behavior%20and%20shot%20quality%2C%20xG%2B%20remedies%20the%20conditioning-on-shots%20limitation%20of%20standard%20xG.%20We%20show%20that%20this%20improves%20predictive%20accuracy%20at%20the%20team%20level%20and%20produces%20a%20more%20persistent%20player%20skill%20signal%20than%20standard%20xG%20models.&entry.1838667208=http%3A//arxiv.org/abs/2512.00203v2&entry.124074799=Read"},
{"title": "Estimating the Joint Probability of Scenario Parameters with Gaussian Mixture Copula Models", "author": "Christian Reichenb\u00e4cher and Philipp Rank and Jochen Hipp and Oliver Bringmann", "abstract": "This paper presents the first application of Gaussian Mixture Copula Models to the statistical modeling of driving scenarios for the safety validation of automated driving systems. Knowledge of the joint probability distribution of scenario parameters is essential for scenario-based safety assessment, where risk quantification depends on the likelihood of concrete parameter combinations. Gaussian Mixture Copula Models bring together the multimodal expressivity of Gaussian Mixture Models and the flexibility of copulas, enabling separate modeling of marginal distributions and dependence. We benchmark Gaussian Mixture Copula Models against previously proposed approaches - Gaussian Mixture Models and Gaussian Copula Models - using real-world driving data drawn from two scenarios defined in United Nations Regulation No. 157. Our evaluation on approximately 18 million instances of these two scenarios demonstrates that Gaussian Mixture Copula Models consistently surpass Gaussian Copula Models and perform competitively with Gaussian Mixture Models, as measured by both log-likelihood and Sinkhorn distance, with relative performance depending on the scenario. The results are promising for the adoption of Gaussian Mixture Copula Models as a statistical foundation for future scenario-based validation frameworks.", "link": "http://arxiv.org/abs/2506.10098v4", "date": "2026-01-26", "relevancy": 1.7553, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5098}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.432}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Estimating%20the%20Joint%20Probability%20of%20Scenario%20Parameters%20with%20Gaussian%20Mixture%20Copula%20Models&body=Title%3A%20Estimating%20the%20Joint%20Probability%20of%20Scenario%20Parameters%20with%20Gaussian%20Mixture%20Copula%20Models%0AAuthor%3A%20Christian%20Reichenb%C3%A4cher%20and%20Philipp%20Rank%20and%20Jochen%20Hipp%20and%20Oliver%20Bringmann%0AAbstract%3A%20This%20paper%20presents%20the%20first%20application%20of%20Gaussian%20Mixture%20Copula%20Models%20to%20the%20statistical%20modeling%20of%20driving%20scenarios%20for%20the%20safety%20validation%20of%20automated%20driving%20systems.%20Knowledge%20of%20the%20joint%20probability%20distribution%20of%20scenario%20parameters%20is%20essential%20for%20scenario-based%20safety%20assessment%2C%20where%20risk%20quantification%20depends%20on%20the%20likelihood%20of%20concrete%20parameter%20combinations.%20Gaussian%20Mixture%20Copula%20Models%20bring%20together%20the%20multimodal%20expressivity%20of%20Gaussian%20Mixture%20Models%20and%20the%20flexibility%20of%20copulas%2C%20enabling%20separate%20modeling%20of%20marginal%20distributions%20and%20dependence.%20We%20benchmark%20Gaussian%20Mixture%20Copula%20Models%20against%20previously%20proposed%20approaches%20-%20Gaussian%20Mixture%20Models%20and%20Gaussian%20Copula%20Models%20-%20using%20real-world%20driving%20data%20drawn%20from%20two%20scenarios%20defined%20in%20United%20Nations%20Regulation%20No.%20157.%20Our%20evaluation%20on%20approximately%2018%20million%20instances%20of%20these%20two%20scenarios%20demonstrates%20that%20Gaussian%20Mixture%20Copula%20Models%20consistently%20surpass%20Gaussian%20Copula%20Models%20and%20perform%20competitively%20with%20Gaussian%20Mixture%20Models%2C%20as%20measured%20by%20both%20log-likelihood%20and%20Sinkhorn%20distance%2C%20with%20relative%20performance%20depending%20on%20the%20scenario.%20The%20results%20are%20promising%20for%20the%20adoption%20of%20Gaussian%20Mixture%20Copula%20Models%20as%20a%20statistical%20foundation%20for%20future%20scenario-based%20validation%20frameworks.%0ALink%3A%20http%3A//arxiv.org/abs/2506.10098v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstimating%2520the%2520Joint%2520Probability%2520of%2520Scenario%2520Parameters%2520with%2520Gaussian%2520Mixture%2520Copula%2520Models%26entry.906535625%3DChristian%2520Reichenb%25C3%25A4cher%2520and%2520Philipp%2520Rank%2520and%2520Jochen%2520Hipp%2520and%2520Oliver%2520Bringmann%26entry.1292438233%3DThis%2520paper%2520presents%2520the%2520first%2520application%2520of%2520Gaussian%2520Mixture%2520Copula%2520Models%2520to%2520the%2520statistical%2520modeling%2520of%2520driving%2520scenarios%2520for%2520the%2520safety%2520validation%2520of%2520automated%2520driving%2520systems.%2520Knowledge%2520of%2520the%2520joint%2520probability%2520distribution%2520of%2520scenario%2520parameters%2520is%2520essential%2520for%2520scenario-based%2520safety%2520assessment%252C%2520where%2520risk%2520quantification%2520depends%2520on%2520the%2520likelihood%2520of%2520concrete%2520parameter%2520combinations.%2520Gaussian%2520Mixture%2520Copula%2520Models%2520bring%2520together%2520the%2520multimodal%2520expressivity%2520of%2520Gaussian%2520Mixture%2520Models%2520and%2520the%2520flexibility%2520of%2520copulas%252C%2520enabling%2520separate%2520modeling%2520of%2520marginal%2520distributions%2520and%2520dependence.%2520We%2520benchmark%2520Gaussian%2520Mixture%2520Copula%2520Models%2520against%2520previously%2520proposed%2520approaches%2520-%2520Gaussian%2520Mixture%2520Models%2520and%2520Gaussian%2520Copula%2520Models%2520-%2520using%2520real-world%2520driving%2520data%2520drawn%2520from%2520two%2520scenarios%2520defined%2520in%2520United%2520Nations%2520Regulation%2520No.%2520157.%2520Our%2520evaluation%2520on%2520approximately%252018%2520million%2520instances%2520of%2520these%2520two%2520scenarios%2520demonstrates%2520that%2520Gaussian%2520Mixture%2520Copula%2520Models%2520consistently%2520surpass%2520Gaussian%2520Copula%2520Models%2520and%2520perform%2520competitively%2520with%2520Gaussian%2520Mixture%2520Models%252C%2520as%2520measured%2520by%2520both%2520log-likelihood%2520and%2520Sinkhorn%2520distance%252C%2520with%2520relative%2520performance%2520depending%2520on%2520the%2520scenario.%2520The%2520results%2520are%2520promising%2520for%2520the%2520adoption%2520of%2520Gaussian%2520Mixture%2520Copula%2520Models%2520as%2520a%2520statistical%2520foundation%2520for%2520future%2520scenario-based%2520validation%2520frameworks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10098v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimating%20the%20Joint%20Probability%20of%20Scenario%20Parameters%20with%20Gaussian%20Mixture%20Copula%20Models&entry.906535625=Christian%20Reichenb%C3%A4cher%20and%20Philipp%20Rank%20and%20Jochen%20Hipp%20and%20Oliver%20Bringmann&entry.1292438233=This%20paper%20presents%20the%20first%20application%20of%20Gaussian%20Mixture%20Copula%20Models%20to%20the%20statistical%20modeling%20of%20driving%20scenarios%20for%20the%20safety%20validation%20of%20automated%20driving%20systems.%20Knowledge%20of%20the%20joint%20probability%20distribution%20of%20scenario%20parameters%20is%20essential%20for%20scenario-based%20safety%20assessment%2C%20where%20risk%20quantification%20depends%20on%20the%20likelihood%20of%20concrete%20parameter%20combinations.%20Gaussian%20Mixture%20Copula%20Models%20bring%20together%20the%20multimodal%20expressivity%20of%20Gaussian%20Mixture%20Models%20and%20the%20flexibility%20of%20copulas%2C%20enabling%20separate%20modeling%20of%20marginal%20distributions%20and%20dependence.%20We%20benchmark%20Gaussian%20Mixture%20Copula%20Models%20against%20previously%20proposed%20approaches%20-%20Gaussian%20Mixture%20Models%20and%20Gaussian%20Copula%20Models%20-%20using%20real-world%20driving%20data%20drawn%20from%20two%20scenarios%20defined%20in%20United%20Nations%20Regulation%20No.%20157.%20Our%20evaluation%20on%20approximately%2018%20million%20instances%20of%20these%20two%20scenarios%20demonstrates%20that%20Gaussian%20Mixture%20Copula%20Models%20consistently%20surpass%20Gaussian%20Copula%20Models%20and%20perform%20competitively%20with%20Gaussian%20Mixture%20Models%2C%20as%20measured%20by%20both%20log-likelihood%20and%20Sinkhorn%20distance%2C%20with%20relative%20performance%20depending%20on%20the%20scenario.%20The%20results%20are%20promising%20for%20the%20adoption%20of%20Gaussian%20Mixture%20Copula%20Models%20as%20a%20statistical%20foundation%20for%20future%20scenario-based%20validation%20frameworks.&entry.1838667208=http%3A//arxiv.org/abs/2506.10098v4&entry.124074799=Read"},
{"title": "HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs", "author": "Xinyue Zeng and Junhong Lin and Yujun Yan and Feng Guo and Liang Shi and Jun Wu and Dawei Zhou", "abstract": "The reliability of Large Language Models (LLMs) in high-stakes domains such as healthcare, law, and scientific discovery is often compromised by hallucinations. These failures typically stem from two sources: data-driven hallucinations and reasoning-driven hallucinations. However, existing detection methods usually address only one source and rely on task-specific heuristics, limiting their generalization to complex scenarios. To overcome these limitations, we introduce the Hallucination Risk Bound, a unified theoretical framework that formally decomposes hallucination risk into data-driven and reasoning-driven components, linked respectively to training-time mismatches and inference-time instabilities. This provides a principled foundation for analyzing how hallucinations emerge and evolve. Building on this foundation, we introduce HalluGuard, an NTK-based score that leverages the induced geometry and captured representations of the NTK to jointly identify data-driven and reasoning-driven hallucinations. We evaluate HalluGuard on 10 diverse benchmarks, 11 competitive baselines, and 9 popular LLM backbones, consistently achieving state-of-the-art performance in detecting diverse forms of LLM hallucinations.", "link": "http://arxiv.org/abs/2601.18753v1", "date": "2026-01-26", "relevancy": 2.0086, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5513}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5007}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HalluGuard%3A%20Demystifying%20Data-Driven%20and%20Reasoning-Driven%20Hallucinations%20in%20LLMs&body=Title%3A%20HalluGuard%3A%20Demystifying%20Data-Driven%20and%20Reasoning-Driven%20Hallucinations%20in%20LLMs%0AAuthor%3A%20Xinyue%20Zeng%20and%20Junhong%20Lin%20and%20Yujun%20Yan%20and%20Feng%20Guo%20and%20Liang%20Shi%20and%20Jun%20Wu%20and%20Dawei%20Zhou%0AAbstract%3A%20The%20reliability%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20high-stakes%20domains%20such%20as%20healthcare%2C%20law%2C%20and%20scientific%20discovery%20is%20often%20compromised%20by%20hallucinations.%20These%20failures%20typically%20stem%20from%20two%20sources%3A%20data-driven%20hallucinations%20and%20reasoning-driven%20hallucinations.%20However%2C%20existing%20detection%20methods%20usually%20address%20only%20one%20source%20and%20rely%20on%20task-specific%20heuristics%2C%20limiting%20their%20generalization%20to%20complex%20scenarios.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20the%20Hallucination%20Risk%20Bound%2C%20a%20unified%20theoretical%20framework%20that%20formally%20decomposes%20hallucination%20risk%20into%20data-driven%20and%20reasoning-driven%20components%2C%20linked%20respectively%20to%20training-time%20mismatches%20and%20inference-time%20instabilities.%20This%20provides%20a%20principled%20foundation%20for%20analyzing%20how%20hallucinations%20emerge%20and%20evolve.%20Building%20on%20this%20foundation%2C%20we%20introduce%20HalluGuard%2C%20an%20NTK-based%20score%20that%20leverages%20the%20induced%20geometry%20and%20captured%20representations%20of%20the%20NTK%20to%20jointly%20identify%20data-driven%20and%20reasoning-driven%20hallucinations.%20We%20evaluate%20HalluGuard%20on%2010%20diverse%20benchmarks%2C%2011%20competitive%20baselines%2C%20and%209%20popular%20LLM%20backbones%2C%20consistently%20achieving%20state-of-the-art%20performance%20in%20detecting%20diverse%20forms%20of%20LLM%20hallucinations.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18753v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHalluGuard%253A%2520Demystifying%2520Data-Driven%2520and%2520Reasoning-Driven%2520Hallucinations%2520in%2520LLMs%26entry.906535625%3DXinyue%2520Zeng%2520and%2520Junhong%2520Lin%2520and%2520Yujun%2520Yan%2520and%2520Feng%2520Guo%2520and%2520Liang%2520Shi%2520and%2520Jun%2520Wu%2520and%2520Dawei%2520Zhou%26entry.1292438233%3DThe%2520reliability%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%2520high-stakes%2520domains%2520such%2520as%2520healthcare%252C%2520law%252C%2520and%2520scientific%2520discovery%2520is%2520often%2520compromised%2520by%2520hallucinations.%2520These%2520failures%2520typically%2520stem%2520from%2520two%2520sources%253A%2520data-driven%2520hallucinations%2520and%2520reasoning-driven%2520hallucinations.%2520However%252C%2520existing%2520detection%2520methods%2520usually%2520address%2520only%2520one%2520source%2520and%2520rely%2520on%2520task-specific%2520heuristics%252C%2520limiting%2520their%2520generalization%2520to%2520complex%2520scenarios.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520introduce%2520the%2520Hallucination%2520Risk%2520Bound%252C%2520a%2520unified%2520theoretical%2520framework%2520that%2520formally%2520decomposes%2520hallucination%2520risk%2520into%2520data-driven%2520and%2520reasoning-driven%2520components%252C%2520linked%2520respectively%2520to%2520training-time%2520mismatches%2520and%2520inference-time%2520instabilities.%2520This%2520provides%2520a%2520principled%2520foundation%2520for%2520analyzing%2520how%2520hallucinations%2520emerge%2520and%2520evolve.%2520Building%2520on%2520this%2520foundation%252C%2520we%2520introduce%2520HalluGuard%252C%2520an%2520NTK-based%2520score%2520that%2520leverages%2520the%2520induced%2520geometry%2520and%2520captured%2520representations%2520of%2520the%2520NTK%2520to%2520jointly%2520identify%2520data-driven%2520and%2520reasoning-driven%2520hallucinations.%2520We%2520evaluate%2520HalluGuard%2520on%252010%2520diverse%2520benchmarks%252C%252011%2520competitive%2520baselines%252C%2520and%25209%2520popular%2520LLM%2520backbones%252C%2520consistently%2520achieving%2520state-of-the-art%2520performance%2520in%2520detecting%2520diverse%2520forms%2520of%2520LLM%2520hallucinations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18753v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HalluGuard%3A%20Demystifying%20Data-Driven%20and%20Reasoning-Driven%20Hallucinations%20in%20LLMs&entry.906535625=Xinyue%20Zeng%20and%20Junhong%20Lin%20and%20Yujun%20Yan%20and%20Feng%20Guo%20and%20Liang%20Shi%20and%20Jun%20Wu%20and%20Dawei%20Zhou&entry.1292438233=The%20reliability%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20high-stakes%20domains%20such%20as%20healthcare%2C%20law%2C%20and%20scientific%20discovery%20is%20often%20compromised%20by%20hallucinations.%20These%20failures%20typically%20stem%20from%20two%20sources%3A%20data-driven%20hallucinations%20and%20reasoning-driven%20hallucinations.%20However%2C%20existing%20detection%20methods%20usually%20address%20only%20one%20source%20and%20rely%20on%20task-specific%20heuristics%2C%20limiting%20their%20generalization%20to%20complex%20scenarios.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20the%20Hallucination%20Risk%20Bound%2C%20a%20unified%20theoretical%20framework%20that%20formally%20decomposes%20hallucination%20risk%20into%20data-driven%20and%20reasoning-driven%20components%2C%20linked%20respectively%20to%20training-time%20mismatches%20and%20inference-time%20instabilities.%20This%20provides%20a%20principled%20foundation%20for%20analyzing%20how%20hallucinations%20emerge%20and%20evolve.%20Building%20on%20this%20foundation%2C%20we%20introduce%20HalluGuard%2C%20an%20NTK-based%20score%20that%20leverages%20the%20induced%20geometry%20and%20captured%20representations%20of%20the%20NTK%20to%20jointly%20identify%20data-driven%20and%20reasoning-driven%20hallucinations.%20We%20evaluate%20HalluGuard%20on%2010%20diverse%20benchmarks%2C%2011%20competitive%20baselines%2C%20and%209%20popular%20LLM%20backbones%2C%20consistently%20achieving%20state-of-the-art%20performance%20in%20detecting%20diverse%20forms%20of%20LLM%20hallucinations.&entry.1838667208=http%3A//arxiv.org/abs/2601.18753v1&entry.124074799=Read"},
{"title": "RocqStar: Leveraging Similarity-driven Retrieval and Agentic Systems for Rocq generation", "author": "Andrei Kozyrev and Nikita Khramov and Gleb Solovev and Anton Podkopaev", "abstract": "Interactive Theorem Proving was repeatedly shown to be fruitful when combined with Generative Artificial Intelligence. This paper assesses multiple approaches to Rocq generation and illuminates potential avenues for improvement. We identify retrieval-based premise selection as a central component of effective Rocq proof generation and propose a novel approach based on a self-attentive embedder model. The evaluation of the designed approach shows up to 28% relative increase of the generator's performance. We tackle the problem of writing Rocq proofs using a multi-stage agentic system, tailored for formal verification, and demonstrate its high effectiveness. We conduct an ablation study and demonstrate that incorporating multi-agent debate during the planning stage increases the proof success rate by 20% overall and nearly doubles it for complex theorems, while the reflection mechanism further enhances stability and consistency.", "link": "http://arxiv.org/abs/2505.22846v3", "date": "2026-01-26", "relevancy": 1.8684, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.513}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4598}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RocqStar%3A%20Leveraging%20Similarity-driven%20Retrieval%20and%20Agentic%20Systems%20for%20Rocq%20generation&body=Title%3A%20RocqStar%3A%20Leveraging%20Similarity-driven%20Retrieval%20and%20Agentic%20Systems%20for%20Rocq%20generation%0AAuthor%3A%20Andrei%20Kozyrev%20and%20Nikita%20Khramov%20and%20Gleb%20Solovev%20and%20Anton%20Podkopaev%0AAbstract%3A%20Interactive%20Theorem%20Proving%20was%20repeatedly%20shown%20to%20be%20fruitful%20when%20combined%20with%20Generative%20Artificial%20Intelligence.%20This%20paper%20assesses%20multiple%20approaches%20to%20Rocq%20generation%20and%20illuminates%20potential%20avenues%20for%20improvement.%20We%20identify%20retrieval-based%20premise%20selection%20as%20a%20central%20component%20of%20effective%20Rocq%20proof%20generation%20and%20propose%20a%20novel%20approach%20based%20on%20a%20self-attentive%20embedder%20model.%20The%20evaluation%20of%20the%20designed%20approach%20shows%20up%20to%2028%25%20relative%20increase%20of%20the%20generator%27s%20performance.%20We%20tackle%20the%20problem%20of%20writing%20Rocq%20proofs%20using%20a%20multi-stage%20agentic%20system%2C%20tailored%20for%20formal%20verification%2C%20and%20demonstrate%20its%20high%20effectiveness.%20We%20conduct%20an%20ablation%20study%20and%20demonstrate%20that%20incorporating%20multi-agent%20debate%20during%20the%20planning%20stage%20increases%20the%20proof%20success%20rate%20by%2020%25%20overall%20and%20nearly%20doubles%20it%20for%20complex%20theorems%2C%20while%20the%20reflection%20mechanism%20further%20enhances%20stability%20and%20consistency.%0ALink%3A%20http%3A//arxiv.org/abs/2505.22846v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRocqStar%253A%2520Leveraging%2520Similarity-driven%2520Retrieval%2520and%2520Agentic%2520Systems%2520for%2520Rocq%2520generation%26entry.906535625%3DAndrei%2520Kozyrev%2520and%2520Nikita%2520Khramov%2520and%2520Gleb%2520Solovev%2520and%2520Anton%2520Podkopaev%26entry.1292438233%3DInteractive%2520Theorem%2520Proving%2520was%2520repeatedly%2520shown%2520to%2520be%2520fruitful%2520when%2520combined%2520with%2520Generative%2520Artificial%2520Intelligence.%2520This%2520paper%2520assesses%2520multiple%2520approaches%2520to%2520Rocq%2520generation%2520and%2520illuminates%2520potential%2520avenues%2520for%2520improvement.%2520We%2520identify%2520retrieval-based%2520premise%2520selection%2520as%2520a%2520central%2520component%2520of%2520effective%2520Rocq%2520proof%2520generation%2520and%2520propose%2520a%2520novel%2520approach%2520based%2520on%2520a%2520self-attentive%2520embedder%2520model.%2520The%2520evaluation%2520of%2520the%2520designed%2520approach%2520shows%2520up%2520to%252028%2525%2520relative%2520increase%2520of%2520the%2520generator%2527s%2520performance.%2520We%2520tackle%2520the%2520problem%2520of%2520writing%2520Rocq%2520proofs%2520using%2520a%2520multi-stage%2520agentic%2520system%252C%2520tailored%2520for%2520formal%2520verification%252C%2520and%2520demonstrate%2520its%2520high%2520effectiveness.%2520We%2520conduct%2520an%2520ablation%2520study%2520and%2520demonstrate%2520that%2520incorporating%2520multi-agent%2520debate%2520during%2520the%2520planning%2520stage%2520increases%2520the%2520proof%2520success%2520rate%2520by%252020%2525%2520overall%2520and%2520nearly%2520doubles%2520it%2520for%2520complex%2520theorems%252C%2520while%2520the%2520reflection%2520mechanism%2520further%2520enhances%2520stability%2520and%2520consistency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22846v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RocqStar%3A%20Leveraging%20Similarity-driven%20Retrieval%20and%20Agentic%20Systems%20for%20Rocq%20generation&entry.906535625=Andrei%20Kozyrev%20and%20Nikita%20Khramov%20and%20Gleb%20Solovev%20and%20Anton%20Podkopaev&entry.1292438233=Interactive%20Theorem%20Proving%20was%20repeatedly%20shown%20to%20be%20fruitful%20when%20combined%20with%20Generative%20Artificial%20Intelligence.%20This%20paper%20assesses%20multiple%20approaches%20to%20Rocq%20generation%20and%20illuminates%20potential%20avenues%20for%20improvement.%20We%20identify%20retrieval-based%20premise%20selection%20as%20a%20central%20component%20of%20effective%20Rocq%20proof%20generation%20and%20propose%20a%20novel%20approach%20based%20on%20a%20self-attentive%20embedder%20model.%20The%20evaluation%20of%20the%20designed%20approach%20shows%20up%20to%2028%25%20relative%20increase%20of%20the%20generator%27s%20performance.%20We%20tackle%20the%20problem%20of%20writing%20Rocq%20proofs%20using%20a%20multi-stage%20agentic%20system%2C%20tailored%20for%20formal%20verification%2C%20and%20demonstrate%20its%20high%20effectiveness.%20We%20conduct%20an%20ablation%20study%20and%20demonstrate%20that%20incorporating%20multi-agent%20debate%20during%20the%20planning%20stage%20increases%20the%20proof%20success%20rate%20by%2020%25%20overall%20and%20nearly%20doubles%20it%20for%20complex%20theorems%2C%20while%20the%20reflection%20mechanism%20further%20enhances%20stability%20and%20consistency.&entry.1838667208=http%3A//arxiv.org/abs/2505.22846v3&entry.124074799=Read"},
{"title": "Comparative Evaluation of Machine Learning Algorithms for Affective State Recognition from Children's Drawings", "author": "Aura Loredana Dan", "abstract": "Autism spectrum disorder (ASD) represents a neurodevelopmental condition characterized by difficulties in expressing emotions and communication, particularly during early childhood. Understanding the affective state of children at an early age remains challenging, as conventional assessment methods are often intrusive, subjective, or difficult to apply consistently. This paper builds upon previous work on affective state recognition from children's drawings by presenting a comparative evaluation of machine learning models for emotion classification. Three deep learning architectures -- MobileNet, EfficientNet, and VGG16 -- are evaluated within a unified experimental framework to analyze classification performance, robustness, and computational efficiency. The models are trained using transfer learning on a dataset of children's drawings annotated with emotional labels provided by psychological experts. The results highlight important trade-offs between lightweight and deeper architectures when applied to drawing-based affective computing tasks, particularly in mobile and real-time application contexts.", "link": "http://arxiv.org/abs/2601.18414v1", "date": "2026-01-26", "relevancy": 1.4175, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4823}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4804}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparative%20Evaluation%20of%20Machine%20Learning%20Algorithms%20for%20Affective%20State%20Recognition%20from%20Children%27s%20Drawings&body=Title%3A%20Comparative%20Evaluation%20of%20Machine%20Learning%20Algorithms%20for%20Affective%20State%20Recognition%20from%20Children%27s%20Drawings%0AAuthor%3A%20Aura%20Loredana%20Dan%0AAbstract%3A%20Autism%20spectrum%20disorder%20%28ASD%29%20represents%20a%20neurodevelopmental%20condition%20characterized%20by%20difficulties%20in%20expressing%20emotions%20and%20communication%2C%20particularly%20during%20early%20childhood.%20Understanding%20the%20affective%20state%20of%20children%20at%20an%20early%20age%20remains%20challenging%2C%20as%20conventional%20assessment%20methods%20are%20often%20intrusive%2C%20subjective%2C%20or%20difficult%20to%20apply%20consistently.%20This%20paper%20builds%20upon%20previous%20work%20on%20affective%20state%20recognition%20from%20children%27s%20drawings%20by%20presenting%20a%20comparative%20evaluation%20of%20machine%20learning%20models%20for%20emotion%20classification.%20Three%20deep%20learning%20architectures%20--%20MobileNet%2C%20EfficientNet%2C%20and%20VGG16%20--%20are%20evaluated%20within%20a%20unified%20experimental%20framework%20to%20analyze%20classification%20performance%2C%20robustness%2C%20and%20computational%20efficiency.%20The%20models%20are%20trained%20using%20transfer%20learning%20on%20a%20dataset%20of%20children%27s%20drawings%20annotated%20with%20emotional%20labels%20provided%20by%20psychological%20experts.%20The%20results%20highlight%20important%20trade-offs%20between%20lightweight%20and%20deeper%20architectures%20when%20applied%20to%20drawing-based%20affective%20computing%20tasks%2C%20particularly%20in%20mobile%20and%20real-time%20application%20contexts.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18414v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparative%2520Evaluation%2520of%2520Machine%2520Learning%2520Algorithms%2520for%2520Affective%2520State%2520Recognition%2520from%2520Children%2527s%2520Drawings%26entry.906535625%3DAura%2520Loredana%2520Dan%26entry.1292438233%3DAutism%2520spectrum%2520disorder%2520%2528ASD%2529%2520represents%2520a%2520neurodevelopmental%2520condition%2520characterized%2520by%2520difficulties%2520in%2520expressing%2520emotions%2520and%2520communication%252C%2520particularly%2520during%2520early%2520childhood.%2520Understanding%2520the%2520affective%2520state%2520of%2520children%2520at%2520an%2520early%2520age%2520remains%2520challenging%252C%2520as%2520conventional%2520assessment%2520methods%2520are%2520often%2520intrusive%252C%2520subjective%252C%2520or%2520difficult%2520to%2520apply%2520consistently.%2520This%2520paper%2520builds%2520upon%2520previous%2520work%2520on%2520affective%2520state%2520recognition%2520from%2520children%2527s%2520drawings%2520by%2520presenting%2520a%2520comparative%2520evaluation%2520of%2520machine%2520learning%2520models%2520for%2520emotion%2520classification.%2520Three%2520deep%2520learning%2520architectures%2520--%2520MobileNet%252C%2520EfficientNet%252C%2520and%2520VGG16%2520--%2520are%2520evaluated%2520within%2520a%2520unified%2520experimental%2520framework%2520to%2520analyze%2520classification%2520performance%252C%2520robustness%252C%2520and%2520computational%2520efficiency.%2520The%2520models%2520are%2520trained%2520using%2520transfer%2520learning%2520on%2520a%2520dataset%2520of%2520children%2527s%2520drawings%2520annotated%2520with%2520emotional%2520labels%2520provided%2520by%2520psychological%2520experts.%2520The%2520results%2520highlight%2520important%2520trade-offs%2520between%2520lightweight%2520and%2520deeper%2520architectures%2520when%2520applied%2520to%2520drawing-based%2520affective%2520computing%2520tasks%252C%2520particularly%2520in%2520mobile%2520and%2520real-time%2520application%2520contexts.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18414v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparative%20Evaluation%20of%20Machine%20Learning%20Algorithms%20for%20Affective%20State%20Recognition%20from%20Children%27s%20Drawings&entry.906535625=Aura%20Loredana%20Dan&entry.1292438233=Autism%20spectrum%20disorder%20%28ASD%29%20represents%20a%20neurodevelopmental%20condition%20characterized%20by%20difficulties%20in%20expressing%20emotions%20and%20communication%2C%20particularly%20during%20early%20childhood.%20Understanding%20the%20affective%20state%20of%20children%20at%20an%20early%20age%20remains%20challenging%2C%20as%20conventional%20assessment%20methods%20are%20often%20intrusive%2C%20subjective%2C%20or%20difficult%20to%20apply%20consistently.%20This%20paper%20builds%20upon%20previous%20work%20on%20affective%20state%20recognition%20from%20children%27s%20drawings%20by%20presenting%20a%20comparative%20evaluation%20of%20machine%20learning%20models%20for%20emotion%20classification.%20Three%20deep%20learning%20architectures%20--%20MobileNet%2C%20EfficientNet%2C%20and%20VGG16%20--%20are%20evaluated%20within%20a%20unified%20experimental%20framework%20to%20analyze%20classification%20performance%2C%20robustness%2C%20and%20computational%20efficiency.%20The%20models%20are%20trained%20using%20transfer%20learning%20on%20a%20dataset%20of%20children%27s%20drawings%20annotated%20with%20emotional%20labels%20provided%20by%20psychological%20experts.%20The%20results%20highlight%20important%20trade-offs%20between%20lightweight%20and%20deeper%20architectures%20when%20applied%20to%20drawing-based%20affective%20computing%20tasks%2C%20particularly%20in%20mobile%20and%20real-time%20application%20contexts.&entry.1838667208=http%3A//arxiv.org/abs/2601.18414v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


