<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240826.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "MagicMan: Generative Novel View Synthesis of Humans with 3D-Aware\n  Diffusion and Iterative Refinement", "author": "Xu He and Xiaoyu Li and Di Kang and Jiangnan Ye and Chaopeng Zhang and Liyang Chen and Xiangjun Gao and Han Zhang and Zhiyong Wu and Haolin Zhuang", "abstract": "  Existing works in single-image human reconstruction suffer from weak\ngeneralizability due to insufficient training data or 3D inconsistencies for a\nlack of comprehensive multi-view knowledge. In this paper, we introduce\nMagicMan, a human-specific multi-view diffusion model designed to generate\nhigh-quality novel view images from a single reference image. As its core, we\nleverage a pre-trained 2D diffusion model as the generative prior for\ngeneralizability, with the parametric SMPL-X model as the 3D body prior to\npromote 3D awareness. To tackle the critical challenge of maintaining\nconsistency while achieving dense multi-view generation for improved 3D human\nreconstruction, we first introduce hybrid multi-view attention to facilitate\nboth efficient and thorough information interchange across different views.\nAdditionally, we present a geometry-aware dual branch to perform concurrent\ngeneration in both RGB and normal domains, further enhancing consistency via\ngeometry cues. Last but not least, to address ill-shaped issues arising from\ninaccurate SMPL-X estimation that conflicts with the reference image, we\npropose a novel iterative refinement strategy, which progressively optimizes\nSMPL-X accuracy while enhancing the quality and consistency of the generated\nmulti-views. Extensive experimental results demonstrate that our method\nsignificantly outperforms existing approaches in both novel view synthesis and\nsubsequent 3D human reconstruction tasks.\n", "link": "http://arxiv.org/abs/2408.14211v1", "date": "2024-08-26", "relevancy": 3.3106, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6746}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6746}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6373}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MagicMan%3A%20Generative%20Novel%20View%20Synthesis%20of%20Humans%20with%203D-Aware%0A%20%20Diffusion%20and%20Iterative%20Refinement&body=Title%3A%20MagicMan%3A%20Generative%20Novel%20View%20Synthesis%20of%20Humans%20with%203D-Aware%0A%20%20Diffusion%20and%20Iterative%20Refinement%0AAuthor%3A%20Xu%20He%20and%20Xiaoyu%20Li%20and%20Di%20Kang%20and%20Jiangnan%20Ye%20and%20Chaopeng%20Zhang%20and%20Liyang%20Chen%20and%20Xiangjun%20Gao%20and%20Han%20Zhang%20and%20Zhiyong%20Wu%20and%20Haolin%20Zhuang%0AAbstract%3A%20%20%20Existing%20works%20in%20single-image%20human%20reconstruction%20suffer%20from%20weak%0Ageneralizability%20due%20to%20insufficient%20training%20data%20or%203D%20inconsistencies%20for%20a%0Alack%20of%20comprehensive%20multi-view%20knowledge.%20In%20this%20paper%2C%20we%20introduce%0AMagicMan%2C%20a%20human-specific%20multi-view%20diffusion%20model%20designed%20to%20generate%0Ahigh-quality%20novel%20view%20images%20from%20a%20single%20reference%20image.%20As%20its%20core%2C%20we%0Aleverage%20a%20pre-trained%202D%20diffusion%20model%20as%20the%20generative%20prior%20for%0Ageneralizability%2C%20with%20the%20parametric%20SMPL-X%20model%20as%20the%203D%20body%20prior%20to%0Apromote%203D%20awareness.%20To%20tackle%20the%20critical%20challenge%20of%20maintaining%0Aconsistency%20while%20achieving%20dense%20multi-view%20generation%20for%20improved%203D%20human%0Areconstruction%2C%20we%20first%20introduce%20hybrid%20multi-view%20attention%20to%20facilitate%0Aboth%20efficient%20and%20thorough%20information%20interchange%20across%20different%20views.%0AAdditionally%2C%20we%20present%20a%20geometry-aware%20dual%20branch%20to%20perform%20concurrent%0Ageneration%20in%20both%20RGB%20and%20normal%20domains%2C%20further%20enhancing%20consistency%20via%0Ageometry%20cues.%20Last%20but%20not%20least%2C%20to%20address%20ill-shaped%20issues%20arising%20from%0Ainaccurate%20SMPL-X%20estimation%20that%20conflicts%20with%20the%20reference%20image%2C%20we%0Apropose%20a%20novel%20iterative%20refinement%20strategy%2C%20which%20progressively%20optimizes%0ASMPL-X%20accuracy%20while%20enhancing%20the%20quality%20and%20consistency%20of%20the%20generated%0Amulti-views.%20Extensive%20experimental%20results%20demonstrate%20that%20our%20method%0Asignificantly%20outperforms%20existing%20approaches%20in%20both%20novel%20view%20synthesis%20and%0Asubsequent%203D%20human%20reconstruction%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14211v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMagicMan%253A%2520Generative%2520Novel%2520View%2520Synthesis%2520of%2520Humans%2520with%25203D-Aware%250A%2520%2520Diffusion%2520and%2520Iterative%2520Refinement%26entry.906535625%3DXu%2520He%2520and%2520Xiaoyu%2520Li%2520and%2520Di%2520Kang%2520and%2520Jiangnan%2520Ye%2520and%2520Chaopeng%2520Zhang%2520and%2520Liyang%2520Chen%2520and%2520Xiangjun%2520Gao%2520and%2520Han%2520Zhang%2520and%2520Zhiyong%2520Wu%2520and%2520Haolin%2520Zhuang%26entry.1292438233%3D%2520%2520Existing%2520works%2520in%2520single-image%2520human%2520reconstruction%2520suffer%2520from%2520weak%250Ageneralizability%2520due%2520to%2520insufficient%2520training%2520data%2520or%25203D%2520inconsistencies%2520for%2520a%250Alack%2520of%2520comprehensive%2520multi-view%2520knowledge.%2520In%2520this%2520paper%252C%2520we%2520introduce%250AMagicMan%252C%2520a%2520human-specific%2520multi-view%2520diffusion%2520model%2520designed%2520to%2520generate%250Ahigh-quality%2520novel%2520view%2520images%2520from%2520a%2520single%2520reference%2520image.%2520As%2520its%2520core%252C%2520we%250Aleverage%2520a%2520pre-trained%25202D%2520diffusion%2520model%2520as%2520the%2520generative%2520prior%2520for%250Ageneralizability%252C%2520with%2520the%2520parametric%2520SMPL-X%2520model%2520as%2520the%25203D%2520body%2520prior%2520to%250Apromote%25203D%2520awareness.%2520To%2520tackle%2520the%2520critical%2520challenge%2520of%2520maintaining%250Aconsistency%2520while%2520achieving%2520dense%2520multi-view%2520generation%2520for%2520improved%25203D%2520human%250Areconstruction%252C%2520we%2520first%2520introduce%2520hybrid%2520multi-view%2520attention%2520to%2520facilitate%250Aboth%2520efficient%2520and%2520thorough%2520information%2520interchange%2520across%2520different%2520views.%250AAdditionally%252C%2520we%2520present%2520a%2520geometry-aware%2520dual%2520branch%2520to%2520perform%2520concurrent%250Ageneration%2520in%2520both%2520RGB%2520and%2520normal%2520domains%252C%2520further%2520enhancing%2520consistency%2520via%250Ageometry%2520cues.%2520Last%2520but%2520not%2520least%252C%2520to%2520address%2520ill-shaped%2520issues%2520arising%2520from%250Ainaccurate%2520SMPL-X%2520estimation%2520that%2520conflicts%2520with%2520the%2520reference%2520image%252C%2520we%250Apropose%2520a%2520novel%2520iterative%2520refinement%2520strategy%252C%2520which%2520progressively%2520optimizes%250ASMPL-X%2520accuracy%2520while%2520enhancing%2520the%2520quality%2520and%2520consistency%2520of%2520the%2520generated%250Amulti-views.%2520Extensive%2520experimental%2520results%2520demonstrate%2520that%2520our%2520method%250Asignificantly%2520outperforms%2520existing%2520approaches%2520in%2520both%2520novel%2520view%2520synthesis%2520and%250Asubsequent%25203D%2520human%2520reconstruction%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14211v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MagicMan%3A%20Generative%20Novel%20View%20Synthesis%20of%20Humans%20with%203D-Aware%0A%20%20Diffusion%20and%20Iterative%20Refinement&entry.906535625=Xu%20He%20and%20Xiaoyu%20Li%20and%20Di%20Kang%20and%20Jiangnan%20Ye%20and%20Chaopeng%20Zhang%20and%20Liyang%20Chen%20and%20Xiangjun%20Gao%20and%20Han%20Zhang%20and%20Zhiyong%20Wu%20and%20Haolin%20Zhuang&entry.1292438233=%20%20Existing%20works%20in%20single-image%20human%20reconstruction%20suffer%20from%20weak%0Ageneralizability%20due%20to%20insufficient%20training%20data%20or%203D%20inconsistencies%20for%20a%0Alack%20of%20comprehensive%20multi-view%20knowledge.%20In%20this%20paper%2C%20we%20introduce%0AMagicMan%2C%20a%20human-specific%20multi-view%20diffusion%20model%20designed%20to%20generate%0Ahigh-quality%20novel%20view%20images%20from%20a%20single%20reference%20image.%20As%20its%20core%2C%20we%0Aleverage%20a%20pre-trained%202D%20diffusion%20model%20as%20the%20generative%20prior%20for%0Ageneralizability%2C%20with%20the%20parametric%20SMPL-X%20model%20as%20the%203D%20body%20prior%20to%0Apromote%203D%20awareness.%20To%20tackle%20the%20critical%20challenge%20of%20maintaining%0Aconsistency%20while%20achieving%20dense%20multi-view%20generation%20for%20improved%203D%20human%0Areconstruction%2C%20we%20first%20introduce%20hybrid%20multi-view%20attention%20to%20facilitate%0Aboth%20efficient%20and%20thorough%20information%20interchange%20across%20different%20views.%0AAdditionally%2C%20we%20present%20a%20geometry-aware%20dual%20branch%20to%20perform%20concurrent%0Ageneration%20in%20both%20RGB%20and%20normal%20domains%2C%20further%20enhancing%20consistency%20via%0Ageometry%20cues.%20Last%20but%20not%20least%2C%20to%20address%20ill-shaped%20issues%20arising%20from%0Ainaccurate%20SMPL-X%20estimation%20that%20conflicts%20with%20the%20reference%20image%2C%20we%0Apropose%20a%20novel%20iterative%20refinement%20strategy%2C%20which%20progressively%20optimizes%0ASMPL-X%20accuracy%20while%20enhancing%20the%20quality%20and%20consistency%20of%20the%20generated%0Amulti-views.%20Extensive%20experimental%20results%20demonstrate%20that%20our%20method%0Asignificantly%20outperforms%20existing%20approaches%20in%20both%20novel%20view%20synthesis%20and%0Asubsequent%203D%20human%20reconstruction%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14211v1&entry.124074799=Read"},
{"title": "SpikeGS: Reconstruct 3D scene via fast-moving bio-inspired sensors", "author": "Yijia Guo and Liwen Hu and Lei Ma and Tiejun Huang", "abstract": "  3D Gaussian Splatting (3DGS) demonstrates unparalleled superior performance\nin 3D scene reconstruction. However, 3DGS heavily relies on the sharp images.\nFulfilling this requirement can be challenging in real-world scenarios\nespecially when the camera moves fast, which severely limits the application of\n3DGS. To address these challenges, we proposed Spike Gausian Splatting\n(SpikeGS), the first framework that integrates the spike streams into 3DGS\npipeline to reconstruct 3D scenes via a fast-moving bio-inspired camera. With\naccumulation rasterization, interval supervision, and a specially designed\npipeline, SpikeGS extracts detailed geometry and texture from high temporal\nresolution but texture lacking spike stream, reconstructs 3D scenes captured in\n1 second. Extensive experiments on multiple synthetic and real-world datasets\ndemonstrate the superiority of SpikeGS compared with existing spike-based and\ndeblur 3D scene reconstruction methods. Codes and data will be released soon.\n", "link": "http://arxiv.org/abs/2407.03771v2", "date": "2024-08-26", "relevancy": 3.2672, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7261}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6536}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5807}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpikeGS%3A%20Reconstruct%203D%20scene%20via%20fast-moving%20bio-inspired%20sensors&body=Title%3A%20SpikeGS%3A%20Reconstruct%203D%20scene%20via%20fast-moving%20bio-inspired%20sensors%0AAuthor%3A%20Yijia%20Guo%20and%20Liwen%20Hu%20and%20Lei%20Ma%20and%20Tiejun%20Huang%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20demonstrates%20unparalleled%20superior%20performance%0Ain%203D%20scene%20reconstruction.%20However%2C%203DGS%20heavily%20relies%20on%20the%20sharp%20images.%0AFulfilling%20this%20requirement%20can%20be%20challenging%20in%20real-world%20scenarios%0Aespecially%20when%20the%20camera%20moves%20fast%2C%20which%20severely%20limits%20the%20application%20of%0A3DGS.%20To%20address%20these%20challenges%2C%20we%20proposed%20Spike%20Gausian%20Splatting%0A%28SpikeGS%29%2C%20the%20first%20framework%20that%20integrates%20the%20spike%20streams%20into%203DGS%0Apipeline%20to%20reconstruct%203D%20scenes%20via%20a%20fast-moving%20bio-inspired%20camera.%20With%0Aaccumulation%20rasterization%2C%20interval%20supervision%2C%20and%20a%20specially%20designed%0Apipeline%2C%20SpikeGS%20extracts%20detailed%20geometry%20and%20texture%20from%20high%20temporal%0Aresolution%20but%20texture%20lacking%20spike%20stream%2C%20reconstructs%203D%20scenes%20captured%20in%0A1%20second.%20Extensive%20experiments%20on%20multiple%20synthetic%20and%20real-world%20datasets%0Ademonstrate%20the%20superiority%20of%20SpikeGS%20compared%20with%20existing%20spike-based%20and%0Adeblur%203D%20scene%20reconstruction%20methods.%20Codes%20and%20data%20will%20be%20released%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03771v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpikeGS%253A%2520Reconstruct%25203D%2520scene%2520via%2520fast-moving%2520bio-inspired%2520sensors%26entry.906535625%3DYijia%2520Guo%2520and%2520Liwen%2520Hu%2520and%2520Lei%2520Ma%2520and%2520Tiejun%2520Huang%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520demonstrates%2520unparalleled%2520superior%2520performance%250Ain%25203D%2520scene%2520reconstruction.%2520However%252C%25203DGS%2520heavily%2520relies%2520on%2520the%2520sharp%2520images.%250AFulfilling%2520this%2520requirement%2520can%2520be%2520challenging%2520in%2520real-world%2520scenarios%250Aespecially%2520when%2520the%2520camera%2520moves%2520fast%252C%2520which%2520severely%2520limits%2520the%2520application%2520of%250A3DGS.%2520To%2520address%2520these%2520challenges%252C%2520we%2520proposed%2520Spike%2520Gausian%2520Splatting%250A%2528SpikeGS%2529%252C%2520the%2520first%2520framework%2520that%2520integrates%2520the%2520spike%2520streams%2520into%25203DGS%250Apipeline%2520to%2520reconstruct%25203D%2520scenes%2520via%2520a%2520fast-moving%2520bio-inspired%2520camera.%2520With%250Aaccumulation%2520rasterization%252C%2520interval%2520supervision%252C%2520and%2520a%2520specially%2520designed%250Apipeline%252C%2520SpikeGS%2520extracts%2520detailed%2520geometry%2520and%2520texture%2520from%2520high%2520temporal%250Aresolution%2520but%2520texture%2520lacking%2520spike%2520stream%252C%2520reconstructs%25203D%2520scenes%2520captured%2520in%250A1%2520second.%2520Extensive%2520experiments%2520on%2520multiple%2520synthetic%2520and%2520real-world%2520datasets%250Ademonstrate%2520the%2520superiority%2520of%2520SpikeGS%2520compared%2520with%2520existing%2520spike-based%2520and%250Adeblur%25203D%2520scene%2520reconstruction%2520methods.%2520Codes%2520and%2520data%2520will%2520be%2520released%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03771v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpikeGS%3A%20Reconstruct%203D%20scene%20via%20fast-moving%20bio-inspired%20sensors&entry.906535625=Yijia%20Guo%20and%20Liwen%20Hu%20and%20Lei%20Ma%20and%20Tiejun%20Huang&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20demonstrates%20unparalleled%20superior%20performance%0Ain%203D%20scene%20reconstruction.%20However%2C%203DGS%20heavily%20relies%20on%20the%20sharp%20images.%0AFulfilling%20this%20requirement%20can%20be%20challenging%20in%20real-world%20scenarios%0Aespecially%20when%20the%20camera%20moves%20fast%2C%20which%20severely%20limits%20the%20application%20of%0A3DGS.%20To%20address%20these%20challenges%2C%20we%20proposed%20Spike%20Gausian%20Splatting%0A%28SpikeGS%29%2C%20the%20first%20framework%20that%20integrates%20the%20spike%20streams%20into%203DGS%0Apipeline%20to%20reconstruct%203D%20scenes%20via%20a%20fast-moving%20bio-inspired%20camera.%20With%0Aaccumulation%20rasterization%2C%20interval%20supervision%2C%20and%20a%20specially%20designed%0Apipeline%2C%20SpikeGS%20extracts%20detailed%20geometry%20and%20texture%20from%20high%20temporal%0Aresolution%20but%20texture%20lacking%20spike%20stream%2C%20reconstructs%203D%20scenes%20captured%20in%0A1%20second.%20Extensive%20experiments%20on%20multiple%20synthetic%20and%20real-world%20datasets%0Ademonstrate%20the%20superiority%20of%20SpikeGS%20compared%20with%20existing%20spike-based%20and%0Adeblur%203D%20scene%20reconstruction%20methods.%20Codes%20and%20data%20will%20be%20released%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03771v2&entry.124074799=Read"},
{"title": "Focus on Neighbors and Know the Whole: Towards Consistent Dense\n  Multiview Text-to-Image Generator for 3D Creation", "author": "Bonan Li and Zicheng Zhang and Xingyi Yang and Xinchao Wang", "abstract": "  Generating dense multiview images from text prompts is crucial for creating\nhigh-fidelity 3D assets. Nevertheless, existing methods struggle with\nspace-view correspondences, resulting in sparse and low-quality outputs. In\nthis paper, we introduce CoSER, a novel consistent dense Multiview\nText-to-Image Generator for Text-to-3D, achieving both efficiency and quality\nby meticulously learning neighbor-view coherence and further alleviating\nambiguity through the swift traversal of all views. For achieving neighbor-view\nconsistency, each viewpoint densely interacts with adjacent viewpoints to\nperceive the global spatial structure, and aggregates information along motion\npaths explicitly defined by physical principles to refine details. To further\nenhance cross-view consistency and alleviate content drift, CoSER rapidly scan\nall views in spiral bidirectional manner to aware holistic information and then\nscores each point based on semantic material. Subsequently, we conduct weighted\ndown-sampling along the spatial dimension based on scores, thereby facilitating\nprominent information fusion across all views with lightweight computation.\nTechnically, the core module is built by integrating the attention mechanism\nwith a selective state space model, exploiting the robust learning capabilities\nof the former and the low overhead of the latter. Extensive evaluation shows\nthat CoSER is capable of producing dense, high-fidelity, content-consistent\nmultiview images that can be flexibly integrated into various 3D generation\nmodels.\n", "link": "http://arxiv.org/abs/2408.13149v2", "date": "2024-08-26", "relevancy": 3.2102, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.662}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.662}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Focus%20on%20Neighbors%20and%20Know%20the%20Whole%3A%20Towards%20Consistent%20Dense%0A%20%20Multiview%20Text-to-Image%20Generator%20for%203D%20Creation&body=Title%3A%20Focus%20on%20Neighbors%20and%20Know%20the%20Whole%3A%20Towards%20Consistent%20Dense%0A%20%20Multiview%20Text-to-Image%20Generator%20for%203D%20Creation%0AAuthor%3A%20Bonan%20Li%20and%20Zicheng%20Zhang%20and%20Xingyi%20Yang%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20Generating%20dense%20multiview%20images%20from%20text%20prompts%20is%20crucial%20for%20creating%0Ahigh-fidelity%203D%20assets.%20Nevertheless%2C%20existing%20methods%20struggle%20with%0Aspace-view%20correspondences%2C%20resulting%20in%20sparse%20and%20low-quality%20outputs.%20In%0Athis%20paper%2C%20we%20introduce%20CoSER%2C%20a%20novel%20consistent%20dense%20Multiview%0AText-to-Image%20Generator%20for%20Text-to-3D%2C%20achieving%20both%20efficiency%20and%20quality%0Aby%20meticulously%20learning%20neighbor-view%20coherence%20and%20further%20alleviating%0Aambiguity%20through%20the%20swift%20traversal%20of%20all%20views.%20For%20achieving%20neighbor-view%0Aconsistency%2C%20each%20viewpoint%20densely%20interacts%20with%20adjacent%20viewpoints%20to%0Aperceive%20the%20global%20spatial%20structure%2C%20and%20aggregates%20information%20along%20motion%0Apaths%20explicitly%20defined%20by%20physical%20principles%20to%20refine%20details.%20To%20further%0Aenhance%20cross-view%20consistency%20and%20alleviate%20content%20drift%2C%20CoSER%20rapidly%20scan%0Aall%20views%20in%20spiral%20bidirectional%20manner%20to%20aware%20holistic%20information%20and%20then%0Ascores%20each%20point%20based%20on%20semantic%20material.%20Subsequently%2C%20we%20conduct%20weighted%0Adown-sampling%20along%20the%20spatial%20dimension%20based%20on%20scores%2C%20thereby%20facilitating%0Aprominent%20information%20fusion%20across%20all%20views%20with%20lightweight%20computation.%0ATechnically%2C%20the%20core%20module%20is%20built%20by%20integrating%20the%20attention%20mechanism%0Awith%20a%20selective%20state%20space%20model%2C%20exploiting%20the%20robust%20learning%20capabilities%0Aof%20the%20former%20and%20the%20low%20overhead%20of%20the%20latter.%20Extensive%20evaluation%20shows%0Athat%20CoSER%20is%20capable%20of%20producing%20dense%2C%20high-fidelity%2C%20content-consistent%0Amultiview%20images%20that%20can%20be%20flexibly%20integrated%20into%20various%203D%20generation%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.13149v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFocus%2520on%2520Neighbors%2520and%2520Know%2520the%2520Whole%253A%2520Towards%2520Consistent%2520Dense%250A%2520%2520Multiview%2520Text-to-Image%2520Generator%2520for%25203D%2520Creation%26entry.906535625%3DBonan%2520Li%2520and%2520Zicheng%2520Zhang%2520and%2520Xingyi%2520Yang%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520Generating%2520dense%2520multiview%2520images%2520from%2520text%2520prompts%2520is%2520crucial%2520for%2520creating%250Ahigh-fidelity%25203D%2520assets.%2520Nevertheless%252C%2520existing%2520methods%2520struggle%2520with%250Aspace-view%2520correspondences%252C%2520resulting%2520in%2520sparse%2520and%2520low-quality%2520outputs.%2520In%250Athis%2520paper%252C%2520we%2520introduce%2520CoSER%252C%2520a%2520novel%2520consistent%2520dense%2520Multiview%250AText-to-Image%2520Generator%2520for%2520Text-to-3D%252C%2520achieving%2520both%2520efficiency%2520and%2520quality%250Aby%2520meticulously%2520learning%2520neighbor-view%2520coherence%2520and%2520further%2520alleviating%250Aambiguity%2520through%2520the%2520swift%2520traversal%2520of%2520all%2520views.%2520For%2520achieving%2520neighbor-view%250Aconsistency%252C%2520each%2520viewpoint%2520densely%2520interacts%2520with%2520adjacent%2520viewpoints%2520to%250Aperceive%2520the%2520global%2520spatial%2520structure%252C%2520and%2520aggregates%2520information%2520along%2520motion%250Apaths%2520explicitly%2520defined%2520by%2520physical%2520principles%2520to%2520refine%2520details.%2520To%2520further%250Aenhance%2520cross-view%2520consistency%2520and%2520alleviate%2520content%2520drift%252C%2520CoSER%2520rapidly%2520scan%250Aall%2520views%2520in%2520spiral%2520bidirectional%2520manner%2520to%2520aware%2520holistic%2520information%2520and%2520then%250Ascores%2520each%2520point%2520based%2520on%2520semantic%2520material.%2520Subsequently%252C%2520we%2520conduct%2520weighted%250Adown-sampling%2520along%2520the%2520spatial%2520dimension%2520based%2520on%2520scores%252C%2520thereby%2520facilitating%250Aprominent%2520information%2520fusion%2520across%2520all%2520views%2520with%2520lightweight%2520computation.%250ATechnically%252C%2520the%2520core%2520module%2520is%2520built%2520by%2520integrating%2520the%2520attention%2520mechanism%250Awith%2520a%2520selective%2520state%2520space%2520model%252C%2520exploiting%2520the%2520robust%2520learning%2520capabilities%250Aof%2520the%2520former%2520and%2520the%2520low%2520overhead%2520of%2520the%2520latter.%2520Extensive%2520evaluation%2520shows%250Athat%2520CoSER%2520is%2520capable%2520of%2520producing%2520dense%252C%2520high-fidelity%252C%2520content-consistent%250Amultiview%2520images%2520that%2520can%2520be%2520flexibly%2520integrated%2520into%2520various%25203D%2520generation%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.13149v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Focus%20on%20Neighbors%20and%20Know%20the%20Whole%3A%20Towards%20Consistent%20Dense%0A%20%20Multiview%20Text-to-Image%20Generator%20for%203D%20Creation&entry.906535625=Bonan%20Li%20and%20Zicheng%20Zhang%20and%20Xingyi%20Yang%20and%20Xinchao%20Wang&entry.1292438233=%20%20Generating%20dense%20multiview%20images%20from%20text%20prompts%20is%20crucial%20for%20creating%0Ahigh-fidelity%203D%20assets.%20Nevertheless%2C%20existing%20methods%20struggle%20with%0Aspace-view%20correspondences%2C%20resulting%20in%20sparse%20and%20low-quality%20outputs.%20In%0Athis%20paper%2C%20we%20introduce%20CoSER%2C%20a%20novel%20consistent%20dense%20Multiview%0AText-to-Image%20Generator%20for%20Text-to-3D%2C%20achieving%20both%20efficiency%20and%20quality%0Aby%20meticulously%20learning%20neighbor-view%20coherence%20and%20further%20alleviating%0Aambiguity%20through%20the%20swift%20traversal%20of%20all%20views.%20For%20achieving%20neighbor-view%0Aconsistency%2C%20each%20viewpoint%20densely%20interacts%20with%20adjacent%20viewpoints%20to%0Aperceive%20the%20global%20spatial%20structure%2C%20and%20aggregates%20information%20along%20motion%0Apaths%20explicitly%20defined%20by%20physical%20principles%20to%20refine%20details.%20To%20further%0Aenhance%20cross-view%20consistency%20and%20alleviate%20content%20drift%2C%20CoSER%20rapidly%20scan%0Aall%20views%20in%20spiral%20bidirectional%20manner%20to%20aware%20holistic%20information%20and%20then%0Ascores%20each%20point%20based%20on%20semantic%20material.%20Subsequently%2C%20we%20conduct%20weighted%0Adown-sampling%20along%20the%20spatial%20dimension%20based%20on%20scores%2C%20thereby%20facilitating%0Aprominent%20information%20fusion%20across%20all%20views%20with%20lightweight%20computation.%0ATechnically%2C%20the%20core%20module%20is%20built%20by%20integrating%20the%20attention%20mechanism%0Awith%20a%20selective%20state%20space%20model%2C%20exploiting%20the%20robust%20learning%20capabilities%0Aof%20the%20former%20and%20the%20low%20overhead%20of%20the%20latter.%20Extensive%20evaluation%20shows%0Athat%20CoSER%20is%20capable%20of%20producing%20dense%2C%20high-fidelity%2C%20content-consistent%0Amultiview%20images%20that%20can%20be%20flexibly%20integrated%20into%20various%203D%20generation%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.13149v2&entry.124074799=Read"},
{"title": "VFMM3D: Releasing the Potential of Image by Vision Foundation Model for\n  Monocular 3D Object Detection", "author": "Bonan Ding and Jin Xie and Jing Nie and Jiale Cao and Xuelong Li and Yanwei Pang", "abstract": "  Due to its cost-effectiveness and widespread availability, monocular 3D\nobject detection, which relies solely on a single camera during inference,\nholds significant importance across various applications, including autonomous\ndriving and robotics. Nevertheless, directly predicting the coordinates of\nobjects in 3D space from monocular images poses challenges. Therefore, an\neffective solution involves transforming monocular images into LiDAR-like\nrepresentations and employing a LiDAR-based 3D object detector to predict the\n3D coordinates of objects. The key step in this method is accurately converting\nthe monocular image into a reliable point cloud form. In this paper, we present\nVFMM3D, an innovative framework that leverages the capabilities of Vision\nFoundation Models (VFMs) to accurately transform single-view images into LiDAR\npoint cloud representations. VFMM3D utilizes the Segment Anything Model (SAM)\nand Depth Anything Model (DAM) to generate high-quality pseudo-LiDAR data\nenriched with rich foreground information. Specifically, the Depth Anything\nModel (DAM) is employed to generate dense depth maps. Subsequently, the Segment\nAnything Model (SAM) is utilized to differentiate foreground and background\nregions by predicting instance masks. These predicted instance masks and depth\nmaps are then combined and projected into 3D space to generate pseudo-LiDAR\npoints. Finally, any object detectors based on point clouds can be utilized to\npredict the 3D coordinates of objects. Comprehensive experiments are conducted\non two challenging 3D object detection datasets, KITTI and Waymo. Our VFMM3D\nestablishes a new state-of-the-art performance on both datasets. Additionally,\nexperimental results demonstrate the generality of VFMM3D, showcasing its\nseamless integration into various LiDAR-based 3D object detectors.\n", "link": "http://arxiv.org/abs/2404.09431v2", "date": "2024-08-26", "relevancy": 3.1578, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6362}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6362}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VFMM3D%3A%20Releasing%20the%20Potential%20of%20Image%20by%20Vision%20Foundation%20Model%20for%0A%20%20Monocular%203D%20Object%20Detection&body=Title%3A%20VFMM3D%3A%20Releasing%20the%20Potential%20of%20Image%20by%20Vision%20Foundation%20Model%20for%0A%20%20Monocular%203D%20Object%20Detection%0AAuthor%3A%20Bonan%20Ding%20and%20Jin%20Xie%20and%20Jing%20Nie%20and%20Jiale%20Cao%20and%20Xuelong%20Li%20and%20Yanwei%20Pang%0AAbstract%3A%20%20%20Due%20to%20its%20cost-effectiveness%20and%20widespread%20availability%2C%20monocular%203D%0Aobject%20detection%2C%20which%20relies%20solely%20on%20a%20single%20camera%20during%20inference%2C%0Aholds%20significant%20importance%20across%20various%20applications%2C%20including%20autonomous%0Adriving%20and%20robotics.%20Nevertheless%2C%20directly%20predicting%20the%20coordinates%20of%0Aobjects%20in%203D%20space%20from%20monocular%20images%20poses%20challenges.%20Therefore%2C%20an%0Aeffective%20solution%20involves%20transforming%20monocular%20images%20into%20LiDAR-like%0Arepresentations%20and%20employing%20a%20LiDAR-based%203D%20object%20detector%20to%20predict%20the%0A3D%20coordinates%20of%20objects.%20The%20key%20step%20in%20this%20method%20is%20accurately%20converting%0Athe%20monocular%20image%20into%20a%20reliable%20point%20cloud%20form.%20In%20this%20paper%2C%20we%20present%0AVFMM3D%2C%20an%20innovative%20framework%20that%20leverages%20the%20capabilities%20of%20Vision%0AFoundation%20Models%20%28VFMs%29%20to%20accurately%20transform%20single-view%20images%20into%20LiDAR%0Apoint%20cloud%20representations.%20VFMM3D%20utilizes%20the%20Segment%20Anything%20Model%20%28SAM%29%0Aand%20Depth%20Anything%20Model%20%28DAM%29%20to%20generate%20high-quality%20pseudo-LiDAR%20data%0Aenriched%20with%20rich%20foreground%20information.%20Specifically%2C%20the%20Depth%20Anything%0AModel%20%28DAM%29%20is%20employed%20to%20generate%20dense%20depth%20maps.%20Subsequently%2C%20the%20Segment%0AAnything%20Model%20%28SAM%29%20is%20utilized%20to%20differentiate%20foreground%20and%20background%0Aregions%20by%20predicting%20instance%20masks.%20These%20predicted%20instance%20masks%20and%20depth%0Amaps%20are%20then%20combined%20and%20projected%20into%203D%20space%20to%20generate%20pseudo-LiDAR%0Apoints.%20Finally%2C%20any%20object%20detectors%20based%20on%20point%20clouds%20can%20be%20utilized%20to%0Apredict%20the%203D%20coordinates%20of%20objects.%20Comprehensive%20experiments%20are%20conducted%0Aon%20two%20challenging%203D%20object%20detection%20datasets%2C%20KITTI%20and%20Waymo.%20Our%20VFMM3D%0Aestablishes%20a%20new%20state-of-the-art%20performance%20on%20both%20datasets.%20Additionally%2C%0Aexperimental%20results%20demonstrate%20the%20generality%20of%20VFMM3D%2C%20showcasing%20its%0Aseamless%20integration%20into%20various%20LiDAR-based%203D%20object%20detectors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09431v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVFMM3D%253A%2520Releasing%2520the%2520Potential%2520of%2520Image%2520by%2520Vision%2520Foundation%2520Model%2520for%250A%2520%2520Monocular%25203D%2520Object%2520Detection%26entry.906535625%3DBonan%2520Ding%2520and%2520Jin%2520Xie%2520and%2520Jing%2520Nie%2520and%2520Jiale%2520Cao%2520and%2520Xuelong%2520Li%2520and%2520Yanwei%2520Pang%26entry.1292438233%3D%2520%2520Due%2520to%2520its%2520cost-effectiveness%2520and%2520widespread%2520availability%252C%2520monocular%25203D%250Aobject%2520detection%252C%2520which%2520relies%2520solely%2520on%2520a%2520single%2520camera%2520during%2520inference%252C%250Aholds%2520significant%2520importance%2520across%2520various%2520applications%252C%2520including%2520autonomous%250Adriving%2520and%2520robotics.%2520Nevertheless%252C%2520directly%2520predicting%2520the%2520coordinates%2520of%250Aobjects%2520in%25203D%2520space%2520from%2520monocular%2520images%2520poses%2520challenges.%2520Therefore%252C%2520an%250Aeffective%2520solution%2520involves%2520transforming%2520monocular%2520images%2520into%2520LiDAR-like%250Arepresentations%2520and%2520employing%2520a%2520LiDAR-based%25203D%2520object%2520detector%2520to%2520predict%2520the%250A3D%2520coordinates%2520of%2520objects.%2520The%2520key%2520step%2520in%2520this%2520method%2520is%2520accurately%2520converting%250Athe%2520monocular%2520image%2520into%2520a%2520reliable%2520point%2520cloud%2520form.%2520In%2520this%2520paper%252C%2520we%2520present%250AVFMM3D%252C%2520an%2520innovative%2520framework%2520that%2520leverages%2520the%2520capabilities%2520of%2520Vision%250AFoundation%2520Models%2520%2528VFMs%2529%2520to%2520accurately%2520transform%2520single-view%2520images%2520into%2520LiDAR%250Apoint%2520cloud%2520representations.%2520VFMM3D%2520utilizes%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%250Aand%2520Depth%2520Anything%2520Model%2520%2528DAM%2529%2520to%2520generate%2520high-quality%2520pseudo-LiDAR%2520data%250Aenriched%2520with%2520rich%2520foreground%2520information.%2520Specifically%252C%2520the%2520Depth%2520Anything%250AModel%2520%2528DAM%2529%2520is%2520employed%2520to%2520generate%2520dense%2520depth%2520maps.%2520Subsequently%252C%2520the%2520Segment%250AAnything%2520Model%2520%2528SAM%2529%2520is%2520utilized%2520to%2520differentiate%2520foreground%2520and%2520background%250Aregions%2520by%2520predicting%2520instance%2520masks.%2520These%2520predicted%2520instance%2520masks%2520and%2520depth%250Amaps%2520are%2520then%2520combined%2520and%2520projected%2520into%25203D%2520space%2520to%2520generate%2520pseudo-LiDAR%250Apoints.%2520Finally%252C%2520any%2520object%2520detectors%2520based%2520on%2520point%2520clouds%2520can%2520be%2520utilized%2520to%250Apredict%2520the%25203D%2520coordinates%2520of%2520objects.%2520Comprehensive%2520experiments%2520are%2520conducted%250Aon%2520two%2520challenging%25203D%2520object%2520detection%2520datasets%252C%2520KITTI%2520and%2520Waymo.%2520Our%2520VFMM3D%250Aestablishes%2520a%2520new%2520state-of-the-art%2520performance%2520on%2520both%2520datasets.%2520Additionally%252C%250Aexperimental%2520results%2520demonstrate%2520the%2520generality%2520of%2520VFMM3D%252C%2520showcasing%2520its%250Aseamless%2520integration%2520into%2520various%2520LiDAR-based%25203D%2520object%2520detectors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.09431v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VFMM3D%3A%20Releasing%20the%20Potential%20of%20Image%20by%20Vision%20Foundation%20Model%20for%0A%20%20Monocular%203D%20Object%20Detection&entry.906535625=Bonan%20Ding%20and%20Jin%20Xie%20and%20Jing%20Nie%20and%20Jiale%20Cao%20and%20Xuelong%20Li%20and%20Yanwei%20Pang&entry.1292438233=%20%20Due%20to%20its%20cost-effectiveness%20and%20widespread%20availability%2C%20monocular%203D%0Aobject%20detection%2C%20which%20relies%20solely%20on%20a%20single%20camera%20during%20inference%2C%0Aholds%20significant%20importance%20across%20various%20applications%2C%20including%20autonomous%0Adriving%20and%20robotics.%20Nevertheless%2C%20directly%20predicting%20the%20coordinates%20of%0Aobjects%20in%203D%20space%20from%20monocular%20images%20poses%20challenges.%20Therefore%2C%20an%0Aeffective%20solution%20involves%20transforming%20monocular%20images%20into%20LiDAR-like%0Arepresentations%20and%20employing%20a%20LiDAR-based%203D%20object%20detector%20to%20predict%20the%0A3D%20coordinates%20of%20objects.%20The%20key%20step%20in%20this%20method%20is%20accurately%20converting%0Athe%20monocular%20image%20into%20a%20reliable%20point%20cloud%20form.%20In%20this%20paper%2C%20we%20present%0AVFMM3D%2C%20an%20innovative%20framework%20that%20leverages%20the%20capabilities%20of%20Vision%0AFoundation%20Models%20%28VFMs%29%20to%20accurately%20transform%20single-view%20images%20into%20LiDAR%0Apoint%20cloud%20representations.%20VFMM3D%20utilizes%20the%20Segment%20Anything%20Model%20%28SAM%29%0Aand%20Depth%20Anything%20Model%20%28DAM%29%20to%20generate%20high-quality%20pseudo-LiDAR%20data%0Aenriched%20with%20rich%20foreground%20information.%20Specifically%2C%20the%20Depth%20Anything%0AModel%20%28DAM%29%20is%20employed%20to%20generate%20dense%20depth%20maps.%20Subsequently%2C%20the%20Segment%0AAnything%20Model%20%28SAM%29%20is%20utilized%20to%20differentiate%20foreground%20and%20background%0Aregions%20by%20predicting%20instance%20masks.%20These%20predicted%20instance%20masks%20and%20depth%0Amaps%20are%20then%20combined%20and%20projected%20into%203D%20space%20to%20generate%20pseudo-LiDAR%0Apoints.%20Finally%2C%20any%20object%20detectors%20based%20on%20point%20clouds%20can%20be%20utilized%20to%0Apredict%20the%203D%20coordinates%20of%20objects.%20Comprehensive%20experiments%20are%20conducted%0Aon%20two%20challenging%203D%20object%20detection%20datasets%2C%20KITTI%20and%20Waymo.%20Our%20VFMM3D%0Aestablishes%20a%20new%20state-of-the-art%20performance%20on%20both%20datasets.%20Additionally%2C%0Aexperimental%20results%20demonstrate%20the%20generality%20of%20VFMM3D%2C%20showcasing%20its%0Aseamless%20integration%20into%20various%20LiDAR-based%203D%20object%20detectors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09431v2&entry.124074799=Read"},
{"title": "On the Error Analysis of 3D Gaussian Splatting and an Optimal Projection\n  Strategy", "author": "Letian Huang and Jiayang Bai and Jie Guo and Yuanqi Li and Yanwen Guo", "abstract": "  3D Gaussian Splatting has garnered extensive attention and application in\nreal-time neural rendering. Concurrently, concerns have been raised about the\nlimitations of this technology in aspects such as point cloud storage,\nperformance, and robustness in sparse viewpoints, leading to various\nimprovements. However, there has been a notable lack of attention to the\nfundamental problem of projection errors introduced by the local affine\napproximation inherent in the splatting itself, and the consequential impact of\nthese errors on the quality of photo-realistic rendering. This paper addresses\nthe projection error function of 3D Gaussian Splatting, commencing with the\nresidual error from the first-order Taylor expansion of the projection\nfunction. The analysis establishes a correlation between the error and the\nGaussian mean position. Subsequently, leveraging function optimization theory,\nthis paper analyzes the function's minima to provide an optimal projection\nstrategy for Gaussian Splatting referred to Optimal Gaussian Splatting, which\ncan accommodate a variety of camera models. Experimental validation further\nconfirms that this projection methodology reduces artifacts, resulting in a\nmore convincingly realistic rendering.\n", "link": "http://arxiv.org/abs/2402.00752v4", "date": "2024-08-26", "relevancy": 2.985, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6724}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5897}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Error%20Analysis%20of%203D%20Gaussian%20Splatting%20and%20an%20Optimal%20Projection%0A%20%20Strategy&body=Title%3A%20On%20the%20Error%20Analysis%20of%203D%20Gaussian%20Splatting%20and%20an%20Optimal%20Projection%0A%20%20Strategy%0AAuthor%3A%20Letian%20Huang%20and%20Jiayang%20Bai%20and%20Jie%20Guo%20and%20Yuanqi%20Li%20and%20Yanwen%20Guo%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20has%20garnered%20extensive%20attention%20and%20application%20in%0Areal-time%20neural%20rendering.%20Concurrently%2C%20concerns%20have%20been%20raised%20about%20the%0Alimitations%20of%20this%20technology%20in%20aspects%20such%20as%20point%20cloud%20storage%2C%0Aperformance%2C%20and%20robustness%20in%20sparse%20viewpoints%2C%20leading%20to%20various%0Aimprovements.%20However%2C%20there%20has%20been%20a%20notable%20lack%20of%20attention%20to%20the%0Afundamental%20problem%20of%20projection%20errors%20introduced%20by%20the%20local%20affine%0Aapproximation%20inherent%20in%20the%20splatting%20itself%2C%20and%20the%20consequential%20impact%20of%0Athese%20errors%20on%20the%20quality%20of%20photo-realistic%20rendering.%20This%20paper%20addresses%0Athe%20projection%20error%20function%20of%203D%20Gaussian%20Splatting%2C%20commencing%20with%20the%0Aresidual%20error%20from%20the%20first-order%20Taylor%20expansion%20of%20the%20projection%0Afunction.%20The%20analysis%20establishes%20a%20correlation%20between%20the%20error%20and%20the%0AGaussian%20mean%20position.%20Subsequently%2C%20leveraging%20function%20optimization%20theory%2C%0Athis%20paper%20analyzes%20the%20function%27s%20minima%20to%20provide%20an%20optimal%20projection%0Astrategy%20for%20Gaussian%20Splatting%20referred%20to%20Optimal%20Gaussian%20Splatting%2C%20which%0Acan%20accommodate%20a%20variety%20of%20camera%20models.%20Experimental%20validation%20further%0Aconfirms%20that%20this%20projection%20methodology%20reduces%20artifacts%2C%20resulting%20in%20a%0Amore%20convincingly%20realistic%20rendering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.00752v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Error%2520Analysis%2520of%25203D%2520Gaussian%2520Splatting%2520and%2520an%2520Optimal%2520Projection%250A%2520%2520Strategy%26entry.906535625%3DLetian%2520Huang%2520and%2520Jiayang%2520Bai%2520and%2520Jie%2520Guo%2520and%2520Yuanqi%2520Li%2520and%2520Yanwen%2520Guo%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520has%2520garnered%2520extensive%2520attention%2520and%2520application%2520in%250Areal-time%2520neural%2520rendering.%2520Concurrently%252C%2520concerns%2520have%2520been%2520raised%2520about%2520the%250Alimitations%2520of%2520this%2520technology%2520in%2520aspects%2520such%2520as%2520point%2520cloud%2520storage%252C%250Aperformance%252C%2520and%2520robustness%2520in%2520sparse%2520viewpoints%252C%2520leading%2520to%2520various%250Aimprovements.%2520However%252C%2520there%2520has%2520been%2520a%2520notable%2520lack%2520of%2520attention%2520to%2520the%250Afundamental%2520problem%2520of%2520projection%2520errors%2520introduced%2520by%2520the%2520local%2520affine%250Aapproximation%2520inherent%2520in%2520the%2520splatting%2520itself%252C%2520and%2520the%2520consequential%2520impact%2520of%250Athese%2520errors%2520on%2520the%2520quality%2520of%2520photo-realistic%2520rendering.%2520This%2520paper%2520addresses%250Athe%2520projection%2520error%2520function%2520of%25203D%2520Gaussian%2520Splatting%252C%2520commencing%2520with%2520the%250Aresidual%2520error%2520from%2520the%2520first-order%2520Taylor%2520expansion%2520of%2520the%2520projection%250Afunction.%2520The%2520analysis%2520establishes%2520a%2520correlation%2520between%2520the%2520error%2520and%2520the%250AGaussian%2520mean%2520position.%2520Subsequently%252C%2520leveraging%2520function%2520optimization%2520theory%252C%250Athis%2520paper%2520analyzes%2520the%2520function%2527s%2520minima%2520to%2520provide%2520an%2520optimal%2520projection%250Astrategy%2520for%2520Gaussian%2520Splatting%2520referred%2520to%2520Optimal%2520Gaussian%2520Splatting%252C%2520which%250Acan%2520accommodate%2520a%2520variety%2520of%2520camera%2520models.%2520Experimental%2520validation%2520further%250Aconfirms%2520that%2520this%2520projection%2520methodology%2520reduces%2520artifacts%252C%2520resulting%2520in%2520a%250Amore%2520convincingly%2520realistic%2520rendering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.00752v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Error%20Analysis%20of%203D%20Gaussian%20Splatting%20and%20an%20Optimal%20Projection%0A%20%20Strategy&entry.906535625=Letian%20Huang%20and%20Jiayang%20Bai%20and%20Jie%20Guo%20and%20Yuanqi%20Li%20and%20Yanwen%20Guo&entry.1292438233=%20%203D%20Gaussian%20Splatting%20has%20garnered%20extensive%20attention%20and%20application%20in%0Areal-time%20neural%20rendering.%20Concurrently%2C%20concerns%20have%20been%20raised%20about%20the%0Alimitations%20of%20this%20technology%20in%20aspects%20such%20as%20point%20cloud%20storage%2C%0Aperformance%2C%20and%20robustness%20in%20sparse%20viewpoints%2C%20leading%20to%20various%0Aimprovements.%20However%2C%20there%20has%20been%20a%20notable%20lack%20of%20attention%20to%20the%0Afundamental%20problem%20of%20projection%20errors%20introduced%20by%20the%20local%20affine%0Aapproximation%20inherent%20in%20the%20splatting%20itself%2C%20and%20the%20consequential%20impact%20of%0Athese%20errors%20on%20the%20quality%20of%20photo-realistic%20rendering.%20This%20paper%20addresses%0Athe%20projection%20error%20function%20of%203D%20Gaussian%20Splatting%2C%20commencing%20with%20the%0Aresidual%20error%20from%20the%20first-order%20Taylor%20expansion%20of%20the%20projection%0Afunction.%20The%20analysis%20establishes%20a%20correlation%20between%20the%20error%20and%20the%0AGaussian%20mean%20position.%20Subsequently%2C%20leveraging%20function%20optimization%20theory%2C%0Athis%20paper%20analyzes%20the%20function%27s%20minima%20to%20provide%20an%20optimal%20projection%0Astrategy%20for%20Gaussian%20Splatting%20referred%20to%20Optimal%20Gaussian%20Splatting%2C%20which%0Acan%20accommodate%20a%20variety%20of%20camera%20models.%20Experimental%20validation%20further%0Aconfirms%20that%20this%20projection%20methodology%20reduces%20artifacts%2C%20resulting%20in%20a%0Amore%20convincingly%20realistic%20rendering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.00752v4&entry.124074799=Read"},
{"title": "InstantStyleGaussian: Efficient Art Style Transfer with 3D Gaussian\n  Splatting", "author": "Xin-Yi Yu and Jun-Xin Yu and Li-Bo Zhou and Yan Wei and Lin-Lin Ou", "abstract": "  We present InstantStyleGaussian, an innovative 3D style transfer method based\non the 3D Gaussian Splatting (3DGS) scene representation. By inputting a\ntarget-style image, it quickly generates new 3D GS scenes. Our method operates\non pre-reconstructed GS scenes, combining diffusion models with an improved\niterative dataset update strategy. It utilizes diffusion models to generate\ntarget style images, adds these new images to the training dataset, and uses\nthis dataset to iteratively update and optimize the GS scenes, significantly\naccelerating the style editing process while ensuring the quality of the\ngenerated scenes. Extensive experimental results demonstrate that our method\nensures high-quality stylized scenes while offering significant advantages in\nstyle transfer speed and consistency.\n", "link": "http://arxiv.org/abs/2408.04249v2", "date": "2024-08-26", "relevancy": 2.9143, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6215}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5636}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InstantStyleGaussian%3A%20Efficient%20Art%20Style%20Transfer%20with%203D%20Gaussian%0A%20%20Splatting&body=Title%3A%20InstantStyleGaussian%3A%20Efficient%20Art%20Style%20Transfer%20with%203D%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Xin-Yi%20Yu%20and%20Jun-Xin%20Yu%20and%20Li-Bo%20Zhou%20and%20Yan%20Wei%20and%20Lin-Lin%20Ou%0AAbstract%3A%20%20%20We%20present%20InstantStyleGaussian%2C%20an%20innovative%203D%20style%20transfer%20method%20based%0Aon%20the%203D%20Gaussian%20Splatting%20%283DGS%29%20scene%20representation.%20By%20inputting%20a%0Atarget-style%20image%2C%20it%20quickly%20generates%20new%203D%20GS%20scenes.%20Our%20method%20operates%0Aon%20pre-reconstructed%20GS%20scenes%2C%20combining%20diffusion%20models%20with%20an%20improved%0Aiterative%20dataset%20update%20strategy.%20It%20utilizes%20diffusion%20models%20to%20generate%0Atarget%20style%20images%2C%20adds%20these%20new%20images%20to%20the%20training%20dataset%2C%20and%20uses%0Athis%20dataset%20to%20iteratively%20update%20and%20optimize%20the%20GS%20scenes%2C%20significantly%0Aaccelerating%20the%20style%20editing%20process%20while%20ensuring%20the%20quality%20of%20the%0Agenerated%20scenes.%20Extensive%20experimental%20results%20demonstrate%20that%20our%20method%0Aensures%20high-quality%20stylized%20scenes%20while%20offering%20significant%20advantages%20in%0Astyle%20transfer%20speed%20and%20consistency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.04249v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstantStyleGaussian%253A%2520Efficient%2520Art%2520Style%2520Transfer%2520with%25203D%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DXin-Yi%2520Yu%2520and%2520Jun-Xin%2520Yu%2520and%2520Li-Bo%2520Zhou%2520and%2520Yan%2520Wei%2520and%2520Lin-Lin%2520Ou%26entry.1292438233%3D%2520%2520We%2520present%2520InstantStyleGaussian%252C%2520an%2520innovative%25203D%2520style%2520transfer%2520method%2520based%250Aon%2520the%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520scene%2520representation.%2520By%2520inputting%2520a%250Atarget-style%2520image%252C%2520it%2520quickly%2520generates%2520new%25203D%2520GS%2520scenes.%2520Our%2520method%2520operates%250Aon%2520pre-reconstructed%2520GS%2520scenes%252C%2520combining%2520diffusion%2520models%2520with%2520an%2520improved%250Aiterative%2520dataset%2520update%2520strategy.%2520It%2520utilizes%2520diffusion%2520models%2520to%2520generate%250Atarget%2520style%2520images%252C%2520adds%2520these%2520new%2520images%2520to%2520the%2520training%2520dataset%252C%2520and%2520uses%250Athis%2520dataset%2520to%2520iteratively%2520update%2520and%2520optimize%2520the%2520GS%2520scenes%252C%2520significantly%250Aaccelerating%2520the%2520style%2520editing%2520process%2520while%2520ensuring%2520the%2520quality%2520of%2520the%250Agenerated%2520scenes.%2520Extensive%2520experimental%2520results%2520demonstrate%2520that%2520our%2520method%250Aensures%2520high-quality%2520stylized%2520scenes%2520while%2520offering%2520significant%2520advantages%2520in%250Astyle%2520transfer%2520speed%2520and%2520consistency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04249v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstantStyleGaussian%3A%20Efficient%20Art%20Style%20Transfer%20with%203D%20Gaussian%0A%20%20Splatting&entry.906535625=Xin-Yi%20Yu%20and%20Jun-Xin%20Yu%20and%20Li-Bo%20Zhou%20and%20Yan%20Wei%20and%20Lin-Lin%20Ou&entry.1292438233=%20%20We%20present%20InstantStyleGaussian%2C%20an%20innovative%203D%20style%20transfer%20method%20based%0Aon%20the%203D%20Gaussian%20Splatting%20%283DGS%29%20scene%20representation.%20By%20inputting%20a%0Atarget-style%20image%2C%20it%20quickly%20generates%20new%203D%20GS%20scenes.%20Our%20method%20operates%0Aon%20pre-reconstructed%20GS%20scenes%2C%20combining%20diffusion%20models%20with%20an%20improved%0Aiterative%20dataset%20update%20strategy.%20It%20utilizes%20diffusion%20models%20to%20generate%0Atarget%20style%20images%2C%20adds%20these%20new%20images%20to%20the%20training%20dataset%2C%20and%20uses%0Athis%20dataset%20to%20iteratively%20update%20and%20optimize%20the%20GS%20scenes%2C%20significantly%0Aaccelerating%20the%20style%20editing%20process%20while%20ensuring%20the%20quality%20of%20the%0Agenerated%20scenes.%20Extensive%20experimental%20results%20demonstrate%20that%20our%20method%0Aensures%20high-quality%20stylized%20scenes%20while%20offering%20significant%20advantages%20in%0Astyle%20transfer%20speed%20and%20consistency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.04249v2&entry.124074799=Read"},
{"title": "Learning Local Pattern Modularization for Point Cloud Reconstruction\n  from Unseen Classes", "author": "Chao Chen and Zhizhong Han and Yu-Shen Liu", "abstract": "  It is challenging to reconstruct 3D point clouds in unseen classes from\nsingle 2D images. Instead of object-centered coordinate system, current methods\ngeneralized global priors learned in seen classes to reconstruct 3D shapes from\nunseen classes in viewer-centered coordinate system. However, the\nreconstruction accuracy and interpretability are still eager to get improved.\nTo resolve this issue, we introduce to learn local pattern modularization for\nreconstructing 3D shapes in unseen classes, which achieves both good\ngeneralization ability and high reconstruction accuracy. Our insight is to\nlearn a local prior which is class-agnostic and easy to generalize in\nobject-centered coordinate system. Specifically, the local prior is learned via\na process of learning and customizing local pattern modularization in seen\nclasses. During this process, we first learn a set of patterns in local\nregions, which is the basis in the object-centered coordinate system to\nrepresent an arbitrary region on shapes across different classes. Then, we\nmodularize each region on an initially reconstructed shape using the learned\nlocal patterns. Based on that, we customize the local pattern modularization\nusing the input image by refining the reconstruction with more details. Our\nmethod enables to reconstruct high fidelity point clouds from unseen classes in\nobject-centered coordinate system without requiring a large number of patterns\nor any additional information, such as segmentation supervision or camera\nposes. Our experimental results under widely used benchmarks show that our\nmethod achieves the state-of-the-art reconstruction accuracy for shapes from\nunseen classes. The code is available at https://github.com/chenchao15/Unseen.\n", "link": "http://arxiv.org/abs/2408.14279v1", "date": "2024-08-26", "relevancy": 2.8686, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.602}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5722}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Local%20Pattern%20Modularization%20for%20Point%20Cloud%20Reconstruction%0A%20%20from%20Unseen%20Classes&body=Title%3A%20Learning%20Local%20Pattern%20Modularization%20for%20Point%20Cloud%20Reconstruction%0A%20%20from%20Unseen%20Classes%0AAuthor%3A%20Chao%20Chen%20and%20Zhizhong%20Han%20and%20Yu-Shen%20Liu%0AAbstract%3A%20%20%20It%20is%20challenging%20to%20reconstruct%203D%20point%20clouds%20in%20unseen%20classes%20from%0Asingle%202D%20images.%20Instead%20of%20object-centered%20coordinate%20system%2C%20current%20methods%0Ageneralized%20global%20priors%20learned%20in%20seen%20classes%20to%20reconstruct%203D%20shapes%20from%0Aunseen%20classes%20in%20viewer-centered%20coordinate%20system.%20However%2C%20the%0Areconstruction%20accuracy%20and%20interpretability%20are%20still%20eager%20to%20get%20improved.%0ATo%20resolve%20this%20issue%2C%20we%20introduce%20to%20learn%20local%20pattern%20modularization%20for%0Areconstructing%203D%20shapes%20in%20unseen%20classes%2C%20which%20achieves%20both%20good%0Ageneralization%20ability%20and%20high%20reconstruction%20accuracy.%20Our%20insight%20is%20to%0Alearn%20a%20local%20prior%20which%20is%20class-agnostic%20and%20easy%20to%20generalize%20in%0Aobject-centered%20coordinate%20system.%20Specifically%2C%20the%20local%20prior%20is%20learned%20via%0Aa%20process%20of%20learning%20and%20customizing%20local%20pattern%20modularization%20in%20seen%0Aclasses.%20During%20this%20process%2C%20we%20first%20learn%20a%20set%20of%20patterns%20in%20local%0Aregions%2C%20which%20is%20the%20basis%20in%20the%20object-centered%20coordinate%20system%20to%0Arepresent%20an%20arbitrary%20region%20on%20shapes%20across%20different%20classes.%20Then%2C%20we%0Amodularize%20each%20region%20on%20an%20initially%20reconstructed%20shape%20using%20the%20learned%0Alocal%20patterns.%20Based%20on%20that%2C%20we%20customize%20the%20local%20pattern%20modularization%0Ausing%20the%20input%20image%20by%20refining%20the%20reconstruction%20with%20more%20details.%20Our%0Amethod%20enables%20to%20reconstruct%20high%20fidelity%20point%20clouds%20from%20unseen%20classes%20in%0Aobject-centered%20coordinate%20system%20without%20requiring%20a%20large%20number%20of%20patterns%0Aor%20any%20additional%20information%2C%20such%20as%20segmentation%20supervision%20or%20camera%0Aposes.%20Our%20experimental%20results%20under%20widely%20used%20benchmarks%20show%20that%20our%0Amethod%20achieves%20the%20state-of-the-art%20reconstruction%20accuracy%20for%20shapes%20from%0Aunseen%20classes.%20The%20code%20is%20available%20at%20https%3A//github.com/chenchao15/Unseen.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14279v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Local%2520Pattern%2520Modularization%2520for%2520Point%2520Cloud%2520Reconstruction%250A%2520%2520from%2520Unseen%2520Classes%26entry.906535625%3DChao%2520Chen%2520and%2520Zhizhong%2520Han%2520and%2520Yu-Shen%2520Liu%26entry.1292438233%3D%2520%2520It%2520is%2520challenging%2520to%2520reconstruct%25203D%2520point%2520clouds%2520in%2520unseen%2520classes%2520from%250Asingle%25202D%2520images.%2520Instead%2520of%2520object-centered%2520coordinate%2520system%252C%2520current%2520methods%250Ageneralized%2520global%2520priors%2520learned%2520in%2520seen%2520classes%2520to%2520reconstruct%25203D%2520shapes%2520from%250Aunseen%2520classes%2520in%2520viewer-centered%2520coordinate%2520system.%2520However%252C%2520the%250Areconstruction%2520accuracy%2520and%2520interpretability%2520are%2520still%2520eager%2520to%2520get%2520improved.%250ATo%2520resolve%2520this%2520issue%252C%2520we%2520introduce%2520to%2520learn%2520local%2520pattern%2520modularization%2520for%250Areconstructing%25203D%2520shapes%2520in%2520unseen%2520classes%252C%2520which%2520achieves%2520both%2520good%250Ageneralization%2520ability%2520and%2520high%2520reconstruction%2520accuracy.%2520Our%2520insight%2520is%2520to%250Alearn%2520a%2520local%2520prior%2520which%2520is%2520class-agnostic%2520and%2520easy%2520to%2520generalize%2520in%250Aobject-centered%2520coordinate%2520system.%2520Specifically%252C%2520the%2520local%2520prior%2520is%2520learned%2520via%250Aa%2520process%2520of%2520learning%2520and%2520customizing%2520local%2520pattern%2520modularization%2520in%2520seen%250Aclasses.%2520During%2520this%2520process%252C%2520we%2520first%2520learn%2520a%2520set%2520of%2520patterns%2520in%2520local%250Aregions%252C%2520which%2520is%2520the%2520basis%2520in%2520the%2520object-centered%2520coordinate%2520system%2520to%250Arepresent%2520an%2520arbitrary%2520region%2520on%2520shapes%2520across%2520different%2520classes.%2520Then%252C%2520we%250Amodularize%2520each%2520region%2520on%2520an%2520initially%2520reconstructed%2520shape%2520using%2520the%2520learned%250Alocal%2520patterns.%2520Based%2520on%2520that%252C%2520we%2520customize%2520the%2520local%2520pattern%2520modularization%250Ausing%2520the%2520input%2520image%2520by%2520refining%2520the%2520reconstruction%2520with%2520more%2520details.%2520Our%250Amethod%2520enables%2520to%2520reconstruct%2520high%2520fidelity%2520point%2520clouds%2520from%2520unseen%2520classes%2520in%250Aobject-centered%2520coordinate%2520system%2520without%2520requiring%2520a%2520large%2520number%2520of%2520patterns%250Aor%2520any%2520additional%2520information%252C%2520such%2520as%2520segmentation%2520supervision%2520or%2520camera%250Aposes.%2520Our%2520experimental%2520results%2520under%2520widely%2520used%2520benchmarks%2520show%2520that%2520our%250Amethod%2520achieves%2520the%2520state-of-the-art%2520reconstruction%2520accuracy%2520for%2520shapes%2520from%250Aunseen%2520classes.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/chenchao15/Unseen.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14279v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Local%20Pattern%20Modularization%20for%20Point%20Cloud%20Reconstruction%0A%20%20from%20Unseen%20Classes&entry.906535625=Chao%20Chen%20and%20Zhizhong%20Han%20and%20Yu-Shen%20Liu&entry.1292438233=%20%20It%20is%20challenging%20to%20reconstruct%203D%20point%20clouds%20in%20unseen%20classes%20from%0Asingle%202D%20images.%20Instead%20of%20object-centered%20coordinate%20system%2C%20current%20methods%0Ageneralized%20global%20priors%20learned%20in%20seen%20classes%20to%20reconstruct%203D%20shapes%20from%0Aunseen%20classes%20in%20viewer-centered%20coordinate%20system.%20However%2C%20the%0Areconstruction%20accuracy%20and%20interpretability%20are%20still%20eager%20to%20get%20improved.%0ATo%20resolve%20this%20issue%2C%20we%20introduce%20to%20learn%20local%20pattern%20modularization%20for%0Areconstructing%203D%20shapes%20in%20unseen%20classes%2C%20which%20achieves%20both%20good%0Ageneralization%20ability%20and%20high%20reconstruction%20accuracy.%20Our%20insight%20is%20to%0Alearn%20a%20local%20prior%20which%20is%20class-agnostic%20and%20easy%20to%20generalize%20in%0Aobject-centered%20coordinate%20system.%20Specifically%2C%20the%20local%20prior%20is%20learned%20via%0Aa%20process%20of%20learning%20and%20customizing%20local%20pattern%20modularization%20in%20seen%0Aclasses.%20During%20this%20process%2C%20we%20first%20learn%20a%20set%20of%20patterns%20in%20local%0Aregions%2C%20which%20is%20the%20basis%20in%20the%20object-centered%20coordinate%20system%20to%0Arepresent%20an%20arbitrary%20region%20on%20shapes%20across%20different%20classes.%20Then%2C%20we%0Amodularize%20each%20region%20on%20an%20initially%20reconstructed%20shape%20using%20the%20learned%0Alocal%20patterns.%20Based%20on%20that%2C%20we%20customize%20the%20local%20pattern%20modularization%0Ausing%20the%20input%20image%20by%20refining%20the%20reconstruction%20with%20more%20details.%20Our%0Amethod%20enables%20to%20reconstruct%20high%20fidelity%20point%20clouds%20from%20unseen%20classes%20in%0Aobject-centered%20coordinate%20system%20without%20requiring%20a%20large%20number%20of%20patterns%0Aor%20any%20additional%20information%2C%20such%20as%20segmentation%20supervision%20or%20camera%0Aposes.%20Our%20experimental%20results%20under%20widely%20used%20benchmarks%20show%20that%20our%0Amethod%20achieves%20the%20state-of-the-art%20reconstruction%20accuracy%20for%20shapes%20from%0Aunseen%20classes.%20The%20code%20is%20available%20at%20https%3A//github.com/chenchao15/Unseen.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14279v1&entry.124074799=Read"},
{"title": "Dense Center-Direction Regression for Object Counting and Localization\n  with Point Supervision", "author": "Domen Tabernik and Jon Muhovi\u010d and Danijel Sko\u010daj", "abstract": "  Object counting and localization problems are commonly addressed with point\nsupervised learning, which allows the use of less labor-intensive point\nannotations. However, learning based on point annotations poses challenges due\nto the high imbalance between the sets of annotated and unannotated pixels,\nwhich is often treated with Gaussian smoothing of point annotations and focal\nloss. However, these approaches still focus on the pixels in the immediate\nvicinity of the point annotations and exploit the rest of the data only\nindirectly. In this work, we propose a novel approach termed CeDiRNet for\npoint-supervised learning that uses a dense regression of directions pointing\ntowards the nearest object centers, i.e. center-directions. This provides\ngreater support for each center point arising from many surrounding pixels\npointing towards the object center. We propose a formulation of\ncenter-directions that allows the problem to be split into the domain-specific\ndense regression of center-directions and the final localization task based on\na small, lightweight, and domain-agnostic localization network that can be\ntrained with synthetic data completely independent of the target domain. We\ndemonstrate the performance of the proposed method on six different datasets\nfor object counting and localization, and show that it outperforms the existing\nstate-of-the-art methods. The code is accessible on GitHub at\nhttps://github.com/vicoslab/CeDiRNet.git.\n", "link": "http://arxiv.org/abs/2408.14457v1", "date": "2024-08-26", "relevancy": 2.8268, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6418}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5395}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dense%20Center-Direction%20Regression%20for%20Object%20Counting%20and%20Localization%0A%20%20with%20Point%20Supervision&body=Title%3A%20Dense%20Center-Direction%20Regression%20for%20Object%20Counting%20and%20Localization%0A%20%20with%20Point%20Supervision%0AAuthor%3A%20Domen%20Tabernik%20and%20Jon%20Muhovi%C4%8D%20and%20Danijel%20Sko%C4%8Daj%0AAbstract%3A%20%20%20Object%20counting%20and%20localization%20problems%20are%20commonly%20addressed%20with%20point%0Asupervised%20learning%2C%20which%20allows%20the%20use%20of%20less%20labor-intensive%20point%0Aannotations.%20However%2C%20learning%20based%20on%20point%20annotations%20poses%20challenges%20due%0Ato%20the%20high%20imbalance%20between%20the%20sets%20of%20annotated%20and%20unannotated%20pixels%2C%0Awhich%20is%20often%20treated%20with%20Gaussian%20smoothing%20of%20point%20annotations%20and%20focal%0Aloss.%20However%2C%20these%20approaches%20still%20focus%20on%20the%20pixels%20in%20the%20immediate%0Avicinity%20of%20the%20point%20annotations%20and%20exploit%20the%20rest%20of%20the%20data%20only%0Aindirectly.%20In%20this%20work%2C%20we%20propose%20a%20novel%20approach%20termed%20CeDiRNet%20for%0Apoint-supervised%20learning%20that%20uses%20a%20dense%20regression%20of%20directions%20pointing%0Atowards%20the%20nearest%20object%20centers%2C%20i.e.%20center-directions.%20This%20provides%0Agreater%20support%20for%20each%20center%20point%20arising%20from%20many%20surrounding%20pixels%0Apointing%20towards%20the%20object%20center.%20We%20propose%20a%20formulation%20of%0Acenter-directions%20that%20allows%20the%20problem%20to%20be%20split%20into%20the%20domain-specific%0Adense%20regression%20of%20center-directions%20and%20the%20final%20localization%20task%20based%20on%0Aa%20small%2C%20lightweight%2C%20and%20domain-agnostic%20localization%20network%20that%20can%20be%0Atrained%20with%20synthetic%20data%20completely%20independent%20of%20the%20target%20domain.%20We%0Ademonstrate%20the%20performance%20of%20the%20proposed%20method%20on%20six%20different%20datasets%0Afor%20object%20counting%20and%20localization%2C%20and%20show%20that%20it%20outperforms%20the%20existing%0Astate-of-the-art%20methods.%20The%20code%20is%20accessible%20on%20GitHub%20at%0Ahttps%3A//github.com/vicoslab/CeDiRNet.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14457v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDense%2520Center-Direction%2520Regression%2520for%2520Object%2520Counting%2520and%2520Localization%250A%2520%2520with%2520Point%2520Supervision%26entry.906535625%3DDomen%2520Tabernik%2520and%2520Jon%2520Muhovi%25C4%258D%2520and%2520Danijel%2520Sko%25C4%258Daj%26entry.1292438233%3D%2520%2520Object%2520counting%2520and%2520localization%2520problems%2520are%2520commonly%2520addressed%2520with%2520point%250Asupervised%2520learning%252C%2520which%2520allows%2520the%2520use%2520of%2520less%2520labor-intensive%2520point%250Aannotations.%2520However%252C%2520learning%2520based%2520on%2520point%2520annotations%2520poses%2520challenges%2520due%250Ato%2520the%2520high%2520imbalance%2520between%2520the%2520sets%2520of%2520annotated%2520and%2520unannotated%2520pixels%252C%250Awhich%2520is%2520often%2520treated%2520with%2520Gaussian%2520smoothing%2520of%2520point%2520annotations%2520and%2520focal%250Aloss.%2520However%252C%2520these%2520approaches%2520still%2520focus%2520on%2520the%2520pixels%2520in%2520the%2520immediate%250Avicinity%2520of%2520the%2520point%2520annotations%2520and%2520exploit%2520the%2520rest%2520of%2520the%2520data%2520only%250Aindirectly.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520approach%2520termed%2520CeDiRNet%2520for%250Apoint-supervised%2520learning%2520that%2520uses%2520a%2520dense%2520regression%2520of%2520directions%2520pointing%250Atowards%2520the%2520nearest%2520object%2520centers%252C%2520i.e.%2520center-directions.%2520This%2520provides%250Agreater%2520support%2520for%2520each%2520center%2520point%2520arising%2520from%2520many%2520surrounding%2520pixels%250Apointing%2520towards%2520the%2520object%2520center.%2520We%2520propose%2520a%2520formulation%2520of%250Acenter-directions%2520that%2520allows%2520the%2520problem%2520to%2520be%2520split%2520into%2520the%2520domain-specific%250Adense%2520regression%2520of%2520center-directions%2520and%2520the%2520final%2520localization%2520task%2520based%2520on%250Aa%2520small%252C%2520lightweight%252C%2520and%2520domain-agnostic%2520localization%2520network%2520that%2520can%2520be%250Atrained%2520with%2520synthetic%2520data%2520completely%2520independent%2520of%2520the%2520target%2520domain.%2520We%250Ademonstrate%2520the%2520performance%2520of%2520the%2520proposed%2520method%2520on%2520six%2520different%2520datasets%250Afor%2520object%2520counting%2520and%2520localization%252C%2520and%2520show%2520that%2520it%2520outperforms%2520the%2520existing%250Astate-of-the-art%2520methods.%2520The%2520code%2520is%2520accessible%2520on%2520GitHub%2520at%250Ahttps%253A//github.com/vicoslab/CeDiRNet.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14457v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dense%20Center-Direction%20Regression%20for%20Object%20Counting%20and%20Localization%0A%20%20with%20Point%20Supervision&entry.906535625=Domen%20Tabernik%20and%20Jon%20Muhovi%C4%8D%20and%20Danijel%20Sko%C4%8Daj&entry.1292438233=%20%20Object%20counting%20and%20localization%20problems%20are%20commonly%20addressed%20with%20point%0Asupervised%20learning%2C%20which%20allows%20the%20use%20of%20less%20labor-intensive%20point%0Aannotations.%20However%2C%20learning%20based%20on%20point%20annotations%20poses%20challenges%20due%0Ato%20the%20high%20imbalance%20between%20the%20sets%20of%20annotated%20and%20unannotated%20pixels%2C%0Awhich%20is%20often%20treated%20with%20Gaussian%20smoothing%20of%20point%20annotations%20and%20focal%0Aloss.%20However%2C%20these%20approaches%20still%20focus%20on%20the%20pixels%20in%20the%20immediate%0Avicinity%20of%20the%20point%20annotations%20and%20exploit%20the%20rest%20of%20the%20data%20only%0Aindirectly.%20In%20this%20work%2C%20we%20propose%20a%20novel%20approach%20termed%20CeDiRNet%20for%0Apoint-supervised%20learning%20that%20uses%20a%20dense%20regression%20of%20directions%20pointing%0Atowards%20the%20nearest%20object%20centers%2C%20i.e.%20center-directions.%20This%20provides%0Agreater%20support%20for%20each%20center%20point%20arising%20from%20many%20surrounding%20pixels%0Apointing%20towards%20the%20object%20center.%20We%20propose%20a%20formulation%20of%0Acenter-directions%20that%20allows%20the%20problem%20to%20be%20split%20into%20the%20domain-specific%0Adense%20regression%20of%20center-directions%20and%20the%20final%20localization%20task%20based%20on%0Aa%20small%2C%20lightweight%2C%20and%20domain-agnostic%20localization%20network%20that%20can%20be%0Atrained%20with%20synthetic%20data%20completely%20independent%20of%20the%20target%20domain.%20We%0Ademonstrate%20the%20performance%20of%20the%20proposed%20method%20on%20six%20different%20datasets%0Afor%20object%20counting%20and%20localization%2C%20and%20show%20that%20it%20outperforms%20the%20existing%0Astate-of-the-art%20methods.%20The%20code%20is%20accessible%20on%20GitHub%20at%0Ahttps%3A//github.com/vicoslab/CeDiRNet.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14457v1&entry.124074799=Read"},
{"title": "Few-Shot 3D Volumetric Segmentation with Multi-Surrogate Fusion", "author": "Meng Zheng and Benjamin Planche and Zhongpai Gao and Terrence Chen and Richard J. Radke and Ziyan Wu", "abstract": "  Conventional 3D medical image segmentation methods typically require learning\nheavy 3D networks (e.g., 3D-UNet), as well as large amounts of in-domain data\nwith accurate pixel/voxel-level labels to avoid overfitting. These solutions\nare thus extremely time- and labor-expensive, but also may easily fail to\ngeneralize to unseen objects during training. To alleviate this issue, we\npresent MSFSeg, a novel few-shot 3D segmentation framework with a lightweight\nmulti-surrogate fusion (MSF). MSFSeg is able to automatically segment unseen 3D\nobjects/organs (during training) provided with one or a few annotated 2D slices\nor 3D sequence segments, via learning dense query-support organ/lesion anatomy\ncorrelations across patient populations. Our proposed MSF module mines\ncomprehensive and diversified morphology correlations between unlabeled and the\nfew labeled slices/sequences through multiple designated surrogates, making it\nable to generate accurate cross-domain 3D segmentation masks given annotated\nslices or sequences. We demonstrate the effectiveness of our proposed framework\nby showing superior performance on conventional few-shot segmentation\nbenchmarks compared to prior art, and remarkable cross-domain cross-volume\nsegmentation performance on proprietary 3D segmentation datasets for\nchallenging entities, i.e., tubular structures, with only limited 2D or 3D\nlabels.\n", "link": "http://arxiv.org/abs/2408.14427v1", "date": "2024-08-26", "relevancy": 2.8232, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5685}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5685}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few-Shot%203D%20Volumetric%20Segmentation%20with%20Multi-Surrogate%20Fusion&body=Title%3A%20Few-Shot%203D%20Volumetric%20Segmentation%20with%20Multi-Surrogate%20Fusion%0AAuthor%3A%20Meng%20Zheng%20and%20Benjamin%20Planche%20and%20Zhongpai%20Gao%20and%20Terrence%20Chen%20and%20Richard%20J.%20Radke%20and%20Ziyan%20Wu%0AAbstract%3A%20%20%20Conventional%203D%20medical%20image%20segmentation%20methods%20typically%20require%20learning%0Aheavy%203D%20networks%20%28e.g.%2C%203D-UNet%29%2C%20as%20well%20as%20large%20amounts%20of%20in-domain%20data%0Awith%20accurate%20pixel/voxel-level%20labels%20to%20avoid%20overfitting.%20These%20solutions%0Aare%20thus%20extremely%20time-%20and%20labor-expensive%2C%20but%20also%20may%20easily%20fail%20to%0Ageneralize%20to%20unseen%20objects%20during%20training.%20To%20alleviate%20this%20issue%2C%20we%0Apresent%20MSFSeg%2C%20a%20novel%20few-shot%203D%20segmentation%20framework%20with%20a%20lightweight%0Amulti-surrogate%20fusion%20%28MSF%29.%20MSFSeg%20is%20able%20to%20automatically%20segment%20unseen%203D%0Aobjects/organs%20%28during%20training%29%20provided%20with%20one%20or%20a%20few%20annotated%202D%20slices%0Aor%203D%20sequence%20segments%2C%20via%20learning%20dense%20query-support%20organ/lesion%20anatomy%0Acorrelations%20across%20patient%20populations.%20Our%20proposed%20MSF%20module%20mines%0Acomprehensive%20and%20diversified%20morphology%20correlations%20between%20unlabeled%20and%20the%0Afew%20labeled%20slices/sequences%20through%20multiple%20designated%20surrogates%2C%20making%20it%0Aable%20to%20generate%20accurate%20cross-domain%203D%20segmentation%20masks%20given%20annotated%0Aslices%20or%20sequences.%20We%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20framework%0Aby%20showing%20superior%20performance%20on%20conventional%20few-shot%20segmentation%0Abenchmarks%20compared%20to%20prior%20art%2C%20and%20remarkable%20cross-domain%20cross-volume%0Asegmentation%20performance%20on%20proprietary%203D%20segmentation%20datasets%20for%0Achallenging%20entities%2C%20i.e.%2C%20tubular%20structures%2C%20with%20only%20limited%202D%20or%203D%0Alabels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14427v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew-Shot%25203D%2520Volumetric%2520Segmentation%2520with%2520Multi-Surrogate%2520Fusion%26entry.906535625%3DMeng%2520Zheng%2520and%2520Benjamin%2520Planche%2520and%2520Zhongpai%2520Gao%2520and%2520Terrence%2520Chen%2520and%2520Richard%2520J.%2520Radke%2520and%2520Ziyan%2520Wu%26entry.1292438233%3D%2520%2520Conventional%25203D%2520medical%2520image%2520segmentation%2520methods%2520typically%2520require%2520learning%250Aheavy%25203D%2520networks%2520%2528e.g.%252C%25203D-UNet%2529%252C%2520as%2520well%2520as%2520large%2520amounts%2520of%2520in-domain%2520data%250Awith%2520accurate%2520pixel/voxel-level%2520labels%2520to%2520avoid%2520overfitting.%2520These%2520solutions%250Aare%2520thus%2520extremely%2520time-%2520and%2520labor-expensive%252C%2520but%2520also%2520may%2520easily%2520fail%2520to%250Ageneralize%2520to%2520unseen%2520objects%2520during%2520training.%2520To%2520alleviate%2520this%2520issue%252C%2520we%250Apresent%2520MSFSeg%252C%2520a%2520novel%2520few-shot%25203D%2520segmentation%2520framework%2520with%2520a%2520lightweight%250Amulti-surrogate%2520fusion%2520%2528MSF%2529.%2520MSFSeg%2520is%2520able%2520to%2520automatically%2520segment%2520unseen%25203D%250Aobjects/organs%2520%2528during%2520training%2529%2520provided%2520with%2520one%2520or%2520a%2520few%2520annotated%25202D%2520slices%250Aor%25203D%2520sequence%2520segments%252C%2520via%2520learning%2520dense%2520query-support%2520organ/lesion%2520anatomy%250Acorrelations%2520across%2520patient%2520populations.%2520Our%2520proposed%2520MSF%2520module%2520mines%250Acomprehensive%2520and%2520diversified%2520morphology%2520correlations%2520between%2520unlabeled%2520and%2520the%250Afew%2520labeled%2520slices/sequences%2520through%2520multiple%2520designated%2520surrogates%252C%2520making%2520it%250Aable%2520to%2520generate%2520accurate%2520cross-domain%25203D%2520segmentation%2520masks%2520given%2520annotated%250Aslices%2520or%2520sequences.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520framework%250Aby%2520showing%2520superior%2520performance%2520on%2520conventional%2520few-shot%2520segmentation%250Abenchmarks%2520compared%2520to%2520prior%2520art%252C%2520and%2520remarkable%2520cross-domain%2520cross-volume%250Asegmentation%2520performance%2520on%2520proprietary%25203D%2520segmentation%2520datasets%2520for%250Achallenging%2520entities%252C%2520i.e.%252C%2520tubular%2520structures%252C%2520with%2520only%2520limited%25202D%2520or%25203D%250Alabels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14427v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-Shot%203D%20Volumetric%20Segmentation%20with%20Multi-Surrogate%20Fusion&entry.906535625=Meng%20Zheng%20and%20Benjamin%20Planche%20and%20Zhongpai%20Gao%20and%20Terrence%20Chen%20and%20Richard%20J.%20Radke%20and%20Ziyan%20Wu&entry.1292438233=%20%20Conventional%203D%20medical%20image%20segmentation%20methods%20typically%20require%20learning%0Aheavy%203D%20networks%20%28e.g.%2C%203D-UNet%29%2C%20as%20well%20as%20large%20amounts%20of%20in-domain%20data%0Awith%20accurate%20pixel/voxel-level%20labels%20to%20avoid%20overfitting.%20These%20solutions%0Aare%20thus%20extremely%20time-%20and%20labor-expensive%2C%20but%20also%20may%20easily%20fail%20to%0Ageneralize%20to%20unseen%20objects%20during%20training.%20To%20alleviate%20this%20issue%2C%20we%0Apresent%20MSFSeg%2C%20a%20novel%20few-shot%203D%20segmentation%20framework%20with%20a%20lightweight%0Amulti-surrogate%20fusion%20%28MSF%29.%20MSFSeg%20is%20able%20to%20automatically%20segment%20unseen%203D%0Aobjects/organs%20%28during%20training%29%20provided%20with%20one%20or%20a%20few%20annotated%202D%20slices%0Aor%203D%20sequence%20segments%2C%20via%20learning%20dense%20query-support%20organ/lesion%20anatomy%0Acorrelations%20across%20patient%20populations.%20Our%20proposed%20MSF%20module%20mines%0Acomprehensive%20and%20diversified%20morphology%20correlations%20between%20unlabeled%20and%20the%0Afew%20labeled%20slices/sequences%20through%20multiple%20designated%20surrogates%2C%20making%20it%0Aable%20to%20generate%20accurate%20cross-domain%203D%20segmentation%20masks%20given%20annotated%0Aslices%20or%20sequences.%20We%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20framework%0Aby%20showing%20superior%20performance%20on%20conventional%20few-shot%20segmentation%0Abenchmarks%20compared%20to%20prior%20art%2C%20and%20remarkable%20cross-domain%20cross-volume%0Asegmentation%20performance%20on%20proprietary%203D%20segmentation%20datasets%20for%0Achallenging%20entities%2C%20i.e.%2C%20tubular%20structures%2C%20with%20only%20limited%202D%20or%203D%0Alabels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14427v1&entry.124074799=Read"},
{"title": "Tackling GenAI Copyright Issues: Originality Estimation and\n  Genericization", "author": "Hiroaki Chiba-Okabe and Weijie J. Su", "abstract": "  The rapid progress of generative AI technology has sparked significant\ncopyright concerns, leading to numerous lawsuits filed against AI developers.\nWhile various techniques for mitigating copyright issues have been studied,\nsignificant risks remain. Here, we propose a genericization method that\nmodifies the outputs of a generative model to make them more generic and less\nlikely to infringe copyright. To achieve this, we introduce a metric for\nquantifying the level of originality of data in a manner that is consistent\nwith the legal framework. This metric can be practically estimated by drawing\nsamples from a generative model, which is then used for the genericization\nprocess. As a practical implementation, we introduce PREGen, which combines our\ngenericization method with an existing mitigation technique. Experiments\ndemonstrate that our genericization method successfully modifies the output of\na text-to-image generative model so that it produces more generic,\ncopyright-compliant images. Compared to the existing method, PREGen reduces the\nlikelihood of generating copyrighted characters by more than half when the\nnames of copyrighted characters are used as the prompt, dramatically improving\nthe performance. Additionally, while generative models can produce copyrighted\ncharacters even when their names are not directly mentioned in the prompt,\nPREGen almost entirely prevents the generation of such characters in these\ncases.\n", "link": "http://arxiv.org/abs/2406.03341v4", "date": "2024-08-26", "relevancy": 2.7005, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5481}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5442}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tackling%20GenAI%20Copyright%20Issues%3A%20Originality%20Estimation%20and%0A%20%20Genericization&body=Title%3A%20Tackling%20GenAI%20Copyright%20Issues%3A%20Originality%20Estimation%20and%0A%20%20Genericization%0AAuthor%3A%20Hiroaki%20Chiba-Okabe%20and%20Weijie%20J.%20Su%0AAbstract%3A%20%20%20The%20rapid%20progress%20of%20generative%20AI%20technology%20has%20sparked%20significant%0Acopyright%20concerns%2C%20leading%20to%20numerous%20lawsuits%20filed%20against%20AI%20developers.%0AWhile%20various%20techniques%20for%20mitigating%20copyright%20issues%20have%20been%20studied%2C%0Asignificant%20risks%20remain.%20Here%2C%20we%20propose%20a%20genericization%20method%20that%0Amodifies%20the%20outputs%20of%20a%20generative%20model%20to%20make%20them%20more%20generic%20and%20less%0Alikely%20to%20infringe%20copyright.%20To%20achieve%20this%2C%20we%20introduce%20a%20metric%20for%0Aquantifying%20the%20level%20of%20originality%20of%20data%20in%20a%20manner%20that%20is%20consistent%0Awith%20the%20legal%20framework.%20This%20metric%20can%20be%20practically%20estimated%20by%20drawing%0Asamples%20from%20a%20generative%20model%2C%20which%20is%20then%20used%20for%20the%20genericization%0Aprocess.%20As%20a%20practical%20implementation%2C%20we%20introduce%20PREGen%2C%20which%20combines%20our%0Agenericization%20method%20with%20an%20existing%20mitigation%20technique.%20Experiments%0Ademonstrate%20that%20our%20genericization%20method%20successfully%20modifies%20the%20output%20of%0Aa%20text-to-image%20generative%20model%20so%20that%20it%20produces%20more%20generic%2C%0Acopyright-compliant%20images.%20Compared%20to%20the%20existing%20method%2C%20PREGen%20reduces%20the%0Alikelihood%20of%20generating%20copyrighted%20characters%20by%20more%20than%20half%20when%20the%0Anames%20of%20copyrighted%20characters%20are%20used%20as%20the%20prompt%2C%20dramatically%20improving%0Athe%20performance.%20Additionally%2C%20while%20generative%20models%20can%20produce%20copyrighted%0Acharacters%20even%20when%20their%20names%20are%20not%20directly%20mentioned%20in%20the%20prompt%2C%0APREGen%20almost%20entirely%20prevents%20the%20generation%20of%20such%20characters%20in%20these%0Acases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03341v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTackling%2520GenAI%2520Copyright%2520Issues%253A%2520Originality%2520Estimation%2520and%250A%2520%2520Genericization%26entry.906535625%3DHiroaki%2520Chiba-Okabe%2520and%2520Weijie%2520J.%2520Su%26entry.1292438233%3D%2520%2520The%2520rapid%2520progress%2520of%2520generative%2520AI%2520technology%2520has%2520sparked%2520significant%250Acopyright%2520concerns%252C%2520leading%2520to%2520numerous%2520lawsuits%2520filed%2520against%2520AI%2520developers.%250AWhile%2520various%2520techniques%2520for%2520mitigating%2520copyright%2520issues%2520have%2520been%2520studied%252C%250Asignificant%2520risks%2520remain.%2520Here%252C%2520we%2520propose%2520a%2520genericization%2520method%2520that%250Amodifies%2520the%2520outputs%2520of%2520a%2520generative%2520model%2520to%2520make%2520them%2520more%2520generic%2520and%2520less%250Alikely%2520to%2520infringe%2520copyright.%2520To%2520achieve%2520this%252C%2520we%2520introduce%2520a%2520metric%2520for%250Aquantifying%2520the%2520level%2520of%2520originality%2520of%2520data%2520in%2520a%2520manner%2520that%2520is%2520consistent%250Awith%2520the%2520legal%2520framework.%2520This%2520metric%2520can%2520be%2520practically%2520estimated%2520by%2520drawing%250Asamples%2520from%2520a%2520generative%2520model%252C%2520which%2520is%2520then%2520used%2520for%2520the%2520genericization%250Aprocess.%2520As%2520a%2520practical%2520implementation%252C%2520we%2520introduce%2520PREGen%252C%2520which%2520combines%2520our%250Agenericization%2520method%2520with%2520an%2520existing%2520mitigation%2520technique.%2520Experiments%250Ademonstrate%2520that%2520our%2520genericization%2520method%2520successfully%2520modifies%2520the%2520output%2520of%250Aa%2520text-to-image%2520generative%2520model%2520so%2520that%2520it%2520produces%2520more%2520generic%252C%250Acopyright-compliant%2520images.%2520Compared%2520to%2520the%2520existing%2520method%252C%2520PREGen%2520reduces%2520the%250Alikelihood%2520of%2520generating%2520copyrighted%2520characters%2520by%2520more%2520than%2520half%2520when%2520the%250Anames%2520of%2520copyrighted%2520characters%2520are%2520used%2520as%2520the%2520prompt%252C%2520dramatically%2520improving%250Athe%2520performance.%2520Additionally%252C%2520while%2520generative%2520models%2520can%2520produce%2520copyrighted%250Acharacters%2520even%2520when%2520their%2520names%2520are%2520not%2520directly%2520mentioned%2520in%2520the%2520prompt%252C%250APREGen%2520almost%2520entirely%2520prevents%2520the%2520generation%2520of%2520such%2520characters%2520in%2520these%250Acases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03341v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tackling%20GenAI%20Copyright%20Issues%3A%20Originality%20Estimation%20and%0A%20%20Genericization&entry.906535625=Hiroaki%20Chiba-Okabe%20and%20Weijie%20J.%20Su&entry.1292438233=%20%20The%20rapid%20progress%20of%20generative%20AI%20technology%20has%20sparked%20significant%0Acopyright%20concerns%2C%20leading%20to%20numerous%20lawsuits%20filed%20against%20AI%20developers.%0AWhile%20various%20techniques%20for%20mitigating%20copyright%20issues%20have%20been%20studied%2C%0Asignificant%20risks%20remain.%20Here%2C%20we%20propose%20a%20genericization%20method%20that%0Amodifies%20the%20outputs%20of%20a%20generative%20model%20to%20make%20them%20more%20generic%20and%20less%0Alikely%20to%20infringe%20copyright.%20To%20achieve%20this%2C%20we%20introduce%20a%20metric%20for%0Aquantifying%20the%20level%20of%20originality%20of%20data%20in%20a%20manner%20that%20is%20consistent%0Awith%20the%20legal%20framework.%20This%20metric%20can%20be%20practically%20estimated%20by%20drawing%0Asamples%20from%20a%20generative%20model%2C%20which%20is%20then%20used%20for%20the%20genericization%0Aprocess.%20As%20a%20practical%20implementation%2C%20we%20introduce%20PREGen%2C%20which%20combines%20our%0Agenericization%20method%20with%20an%20existing%20mitigation%20technique.%20Experiments%0Ademonstrate%20that%20our%20genericization%20method%20successfully%20modifies%20the%20output%20of%0Aa%20text-to-image%20generative%20model%20so%20that%20it%20produces%20more%20generic%2C%0Acopyright-compliant%20images.%20Compared%20to%20the%20existing%20method%2C%20PREGen%20reduces%20the%0Alikelihood%20of%20generating%20copyrighted%20characters%20by%20more%20than%20half%20when%20the%0Anames%20of%20copyrighted%20characters%20are%20used%20as%20the%20prompt%2C%20dramatically%20improving%0Athe%20performance.%20Additionally%2C%20while%20generative%20models%20can%20produce%20copyrighted%0Acharacters%20even%20when%20their%20names%20are%20not%20directly%20mentioned%20in%20the%20prompt%2C%0APREGen%20almost%20entirely%20prevents%20the%20generation%20of%20such%20characters%20in%20these%0Acases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03341v4&entry.124074799=Read"},
{"title": "Center Direction Network for Grasping Point Localization on Cloths", "author": "Domen Tabernik and Jon Muhovi\u010d and Matej Urbas and Danijel Sko\u010daj", "abstract": "  Object grasping is a fundamental challenge in robotics and computer vision,\ncritical for advancing robotic manipulation capabilities. Deformable objects,\nlike fabrics and cloths, pose additional challenges due to their non-rigid\nnature. In this work, we introduce CeDiRNet-3DoF, a deep-learning model for\ngrasp point detection, with a particular focus on cloth objects. CeDiRNet-3DoF\nemploys center direction regression alongside a localization network, attaining\nfirst place in the perception task of ICRA 2023's Cloth Manipulation Challenge.\nRecognizing the lack of standardized benchmarks in the literature that hinder\neffective method comparison, we present the ViCoS Towel Dataset. This extensive\nbenchmark dataset comprises 8,000 real and 12,000 synthetic images, serving as\na robust resource for training and evaluating contemporary data-driven\ndeep-learning approaches. Extensive evaluation revealed CeDiRNet-3DoF's\nrobustness in real-world performance, outperforming state-of-the-art methods,\nincluding the latest transformer-based models. Our work bridges a crucial gap,\noffering a robust solution and benchmark for cloth grasping in computer vision\nand robotics. Code and dataset are available at:\nhttps://github.com/vicoslab/CeDiRNet-3DoF\n", "link": "http://arxiv.org/abs/2408.14456v1", "date": "2024-08-26", "relevancy": 2.6898, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5484}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5341}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Center%20Direction%20Network%20for%20Grasping%20Point%20Localization%20on%20Cloths&body=Title%3A%20Center%20Direction%20Network%20for%20Grasping%20Point%20Localization%20on%20Cloths%0AAuthor%3A%20Domen%20Tabernik%20and%20Jon%20Muhovi%C4%8D%20and%20Matej%20Urbas%20and%20Danijel%20Sko%C4%8Daj%0AAbstract%3A%20%20%20Object%20grasping%20is%20a%20fundamental%20challenge%20in%20robotics%20and%20computer%20vision%2C%0Acritical%20for%20advancing%20robotic%20manipulation%20capabilities.%20Deformable%20objects%2C%0Alike%20fabrics%20and%20cloths%2C%20pose%20additional%20challenges%20due%20to%20their%20non-rigid%0Anature.%20In%20this%20work%2C%20we%20introduce%20CeDiRNet-3DoF%2C%20a%20deep-learning%20model%20for%0Agrasp%20point%20detection%2C%20with%20a%20particular%20focus%20on%20cloth%20objects.%20CeDiRNet-3DoF%0Aemploys%20center%20direction%20regression%20alongside%20a%20localization%20network%2C%20attaining%0Afirst%20place%20in%20the%20perception%20task%20of%20ICRA%202023%27s%20Cloth%20Manipulation%20Challenge.%0ARecognizing%20the%20lack%20of%20standardized%20benchmarks%20in%20the%20literature%20that%20hinder%0Aeffective%20method%20comparison%2C%20we%20present%20the%20ViCoS%20Towel%20Dataset.%20This%20extensive%0Abenchmark%20dataset%20comprises%208%2C000%20real%20and%2012%2C000%20synthetic%20images%2C%20serving%20as%0Aa%20robust%20resource%20for%20training%20and%20evaluating%20contemporary%20data-driven%0Adeep-learning%20approaches.%20Extensive%20evaluation%20revealed%20CeDiRNet-3DoF%27s%0Arobustness%20in%20real-world%20performance%2C%20outperforming%20state-of-the-art%20methods%2C%0Aincluding%20the%20latest%20transformer-based%20models.%20Our%20work%20bridges%20a%20crucial%20gap%2C%0Aoffering%20a%20robust%20solution%20and%20benchmark%20for%20cloth%20grasping%20in%20computer%20vision%0Aand%20robotics.%20Code%20and%20dataset%20are%20available%20at%3A%0Ahttps%3A//github.com/vicoslab/CeDiRNet-3DoF%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14456v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCenter%2520Direction%2520Network%2520for%2520Grasping%2520Point%2520Localization%2520on%2520Cloths%26entry.906535625%3DDomen%2520Tabernik%2520and%2520Jon%2520Muhovi%25C4%258D%2520and%2520Matej%2520Urbas%2520and%2520Danijel%2520Sko%25C4%258Daj%26entry.1292438233%3D%2520%2520Object%2520grasping%2520is%2520a%2520fundamental%2520challenge%2520in%2520robotics%2520and%2520computer%2520vision%252C%250Acritical%2520for%2520advancing%2520robotic%2520manipulation%2520capabilities.%2520Deformable%2520objects%252C%250Alike%2520fabrics%2520and%2520cloths%252C%2520pose%2520additional%2520challenges%2520due%2520to%2520their%2520non-rigid%250Anature.%2520In%2520this%2520work%252C%2520we%2520introduce%2520CeDiRNet-3DoF%252C%2520a%2520deep-learning%2520model%2520for%250Agrasp%2520point%2520detection%252C%2520with%2520a%2520particular%2520focus%2520on%2520cloth%2520objects.%2520CeDiRNet-3DoF%250Aemploys%2520center%2520direction%2520regression%2520alongside%2520a%2520localization%2520network%252C%2520attaining%250Afirst%2520place%2520in%2520the%2520perception%2520task%2520of%2520ICRA%25202023%2527s%2520Cloth%2520Manipulation%2520Challenge.%250ARecognizing%2520the%2520lack%2520of%2520standardized%2520benchmarks%2520in%2520the%2520literature%2520that%2520hinder%250Aeffective%2520method%2520comparison%252C%2520we%2520present%2520the%2520ViCoS%2520Towel%2520Dataset.%2520This%2520extensive%250Abenchmark%2520dataset%2520comprises%25208%252C000%2520real%2520and%252012%252C000%2520synthetic%2520images%252C%2520serving%2520as%250Aa%2520robust%2520resource%2520for%2520training%2520and%2520evaluating%2520contemporary%2520data-driven%250Adeep-learning%2520approaches.%2520Extensive%2520evaluation%2520revealed%2520CeDiRNet-3DoF%2527s%250Arobustness%2520in%2520real-world%2520performance%252C%2520outperforming%2520state-of-the-art%2520methods%252C%250Aincluding%2520the%2520latest%2520transformer-based%2520models.%2520Our%2520work%2520bridges%2520a%2520crucial%2520gap%252C%250Aoffering%2520a%2520robust%2520solution%2520and%2520benchmark%2520for%2520cloth%2520grasping%2520in%2520computer%2520vision%250Aand%2520robotics.%2520Code%2520and%2520dataset%2520are%2520available%2520at%253A%250Ahttps%253A//github.com/vicoslab/CeDiRNet-3DoF%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14456v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Center%20Direction%20Network%20for%20Grasping%20Point%20Localization%20on%20Cloths&entry.906535625=Domen%20Tabernik%20and%20Jon%20Muhovi%C4%8D%20and%20Matej%20Urbas%20and%20Danijel%20Sko%C4%8Daj&entry.1292438233=%20%20Object%20grasping%20is%20a%20fundamental%20challenge%20in%20robotics%20and%20computer%20vision%2C%0Acritical%20for%20advancing%20robotic%20manipulation%20capabilities.%20Deformable%20objects%2C%0Alike%20fabrics%20and%20cloths%2C%20pose%20additional%20challenges%20due%20to%20their%20non-rigid%0Anature.%20In%20this%20work%2C%20we%20introduce%20CeDiRNet-3DoF%2C%20a%20deep-learning%20model%20for%0Agrasp%20point%20detection%2C%20with%20a%20particular%20focus%20on%20cloth%20objects.%20CeDiRNet-3DoF%0Aemploys%20center%20direction%20regression%20alongside%20a%20localization%20network%2C%20attaining%0Afirst%20place%20in%20the%20perception%20task%20of%20ICRA%202023%27s%20Cloth%20Manipulation%20Challenge.%0ARecognizing%20the%20lack%20of%20standardized%20benchmarks%20in%20the%20literature%20that%20hinder%0Aeffective%20method%20comparison%2C%20we%20present%20the%20ViCoS%20Towel%20Dataset.%20This%20extensive%0Abenchmark%20dataset%20comprises%208%2C000%20real%20and%2012%2C000%20synthetic%20images%2C%20serving%20as%0Aa%20robust%20resource%20for%20training%20and%20evaluating%20contemporary%20data-driven%0Adeep-learning%20approaches.%20Extensive%20evaluation%20revealed%20CeDiRNet-3DoF%27s%0Arobustness%20in%20real-world%20performance%2C%20outperforming%20state-of-the-art%20methods%2C%0Aincluding%20the%20latest%20transformer-based%20models.%20Our%20work%20bridges%20a%20crucial%20gap%2C%0Aoffering%20a%20robust%20solution%20and%20benchmark%20for%20cloth%20grasping%20in%20computer%20vision%0Aand%20robotics.%20Code%20and%20dataset%20are%20available%20at%3A%0Ahttps%3A//github.com/vicoslab/CeDiRNet-3DoF%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14456v1&entry.124074799=Read"},
{"title": "Feature Aligning Few shot Learning Method Using Local Descriptors\n  Weighted Rules", "author": "Bingchen Yan", "abstract": "  Few-shot classification involves identifying new categories using a limited\nnumber of labeled samples. Current few-shot classification methods based on\nlocal descriptors primarily leverage underlying consistent features across\nvisible and invisible classes, facing challenges including redundant\nneighboring information, noisy representations, and limited interpretability.\nThis paper proposes a Feature Aligning Few-shot Learning Method Using Local\nDescriptors Weighted Rules (FAFD-LDWR). It innovatively introduces a\ncross-normalization method into few-shot image classification to preserve the\ndiscriminative information of local descriptors as much as possible; and\nenhances classification performance by aligning key local descriptors of\nsupport and query sets to remove background noise. FAFD-LDWR performs\nexcellently on three benchmark datasets , outperforming state-of-the-art\nmethods in both 1-shot and 5-shot settings. The designed visualization\nexperiments also demonstrate FAFD-LDWR's improvement in prediction\ninterpretability.\n", "link": "http://arxiv.org/abs/2408.14192v1", "date": "2024-08-26", "relevancy": 2.6773, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.562}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5319}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5124}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Feature%20Aligning%20Few%20shot%20Learning%20Method%20Using%20Local%20Descriptors%0A%20%20Weighted%20Rules&body=Title%3A%20Feature%20Aligning%20Few%20shot%20Learning%20Method%20Using%20Local%20Descriptors%0A%20%20Weighted%20Rules%0AAuthor%3A%20Bingchen%20Yan%0AAbstract%3A%20%20%20Few-shot%20classification%20involves%20identifying%20new%20categories%20using%20a%20limited%0Anumber%20of%20labeled%20samples.%20Current%20few-shot%20classification%20methods%20based%20on%0Alocal%20descriptors%20primarily%20leverage%20underlying%20consistent%20features%20across%0Avisible%20and%20invisible%20classes%2C%20facing%20challenges%20including%20redundant%0Aneighboring%20information%2C%20noisy%20representations%2C%20and%20limited%20interpretability.%0AThis%20paper%20proposes%20a%20Feature%20Aligning%20Few-shot%20Learning%20Method%20Using%20Local%0ADescriptors%20Weighted%20Rules%20%28FAFD-LDWR%29.%20It%20innovatively%20introduces%20a%0Across-normalization%20method%20into%20few-shot%20image%20classification%20to%20preserve%20the%0Adiscriminative%20information%20of%20local%20descriptors%20as%20much%20as%20possible%3B%20and%0Aenhances%20classification%20performance%20by%20aligning%20key%20local%20descriptors%20of%0Asupport%20and%20query%20sets%20to%20remove%20background%20noise.%20FAFD-LDWR%20performs%0Aexcellently%20on%20three%20benchmark%20datasets%20%2C%20outperforming%20state-of-the-art%0Amethods%20in%20both%201-shot%20and%205-shot%20settings.%20The%20designed%20visualization%0Aexperiments%20also%20demonstrate%20FAFD-LDWR%27s%20improvement%20in%20prediction%0Ainterpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14192v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFeature%2520Aligning%2520Few%2520shot%2520Learning%2520Method%2520Using%2520Local%2520Descriptors%250A%2520%2520Weighted%2520Rules%26entry.906535625%3DBingchen%2520Yan%26entry.1292438233%3D%2520%2520Few-shot%2520classification%2520involves%2520identifying%2520new%2520categories%2520using%2520a%2520limited%250Anumber%2520of%2520labeled%2520samples.%2520Current%2520few-shot%2520classification%2520methods%2520based%2520on%250Alocal%2520descriptors%2520primarily%2520leverage%2520underlying%2520consistent%2520features%2520across%250Avisible%2520and%2520invisible%2520classes%252C%2520facing%2520challenges%2520including%2520redundant%250Aneighboring%2520information%252C%2520noisy%2520representations%252C%2520and%2520limited%2520interpretability.%250AThis%2520paper%2520proposes%2520a%2520Feature%2520Aligning%2520Few-shot%2520Learning%2520Method%2520Using%2520Local%250ADescriptors%2520Weighted%2520Rules%2520%2528FAFD-LDWR%2529.%2520It%2520innovatively%2520introduces%2520a%250Across-normalization%2520method%2520into%2520few-shot%2520image%2520classification%2520to%2520preserve%2520the%250Adiscriminative%2520information%2520of%2520local%2520descriptors%2520as%2520much%2520as%2520possible%253B%2520and%250Aenhances%2520classification%2520performance%2520by%2520aligning%2520key%2520local%2520descriptors%2520of%250Asupport%2520and%2520query%2520sets%2520to%2520remove%2520background%2520noise.%2520FAFD-LDWR%2520performs%250Aexcellently%2520on%2520three%2520benchmark%2520datasets%2520%252C%2520outperforming%2520state-of-the-art%250Amethods%2520in%2520both%25201-shot%2520and%25205-shot%2520settings.%2520The%2520designed%2520visualization%250Aexperiments%2520also%2520demonstrate%2520FAFD-LDWR%2527s%2520improvement%2520in%2520prediction%250Ainterpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14192v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature%20Aligning%20Few%20shot%20Learning%20Method%20Using%20Local%20Descriptors%0A%20%20Weighted%20Rules&entry.906535625=Bingchen%20Yan&entry.1292438233=%20%20Few-shot%20classification%20involves%20identifying%20new%20categories%20using%20a%20limited%0Anumber%20of%20labeled%20samples.%20Current%20few-shot%20classification%20methods%20based%20on%0Alocal%20descriptors%20primarily%20leverage%20underlying%20consistent%20features%20across%0Avisible%20and%20invisible%20classes%2C%20facing%20challenges%20including%20redundant%0Aneighboring%20information%2C%20noisy%20representations%2C%20and%20limited%20interpretability.%0AThis%20paper%20proposes%20a%20Feature%20Aligning%20Few-shot%20Learning%20Method%20Using%20Local%0ADescriptors%20Weighted%20Rules%20%28FAFD-LDWR%29.%20It%20innovatively%20introduces%20a%0Across-normalization%20method%20into%20few-shot%20image%20classification%20to%20preserve%20the%0Adiscriminative%20information%20of%20local%20descriptors%20as%20much%20as%20possible%3B%20and%0Aenhances%20classification%20performance%20by%20aligning%20key%20local%20descriptors%20of%0Asupport%20and%20query%20sets%20to%20remove%20background%20noise.%20FAFD-LDWR%20performs%0Aexcellently%20on%20three%20benchmark%20datasets%20%2C%20outperforming%20state-of-the-art%0Amethods%20in%20both%201-shot%20and%205-shot%20settings.%20The%20designed%20visualization%0Aexperiments%20also%20demonstrate%20FAFD-LDWR%27s%20improvement%20in%20prediction%0Ainterpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14192v1&entry.124074799=Read"},
{"title": "Affine steerers for structured keypoint description", "author": "Georg B\u00f6kman and Johan Edstedt and Michael Felsberg and Fredrik Kahl", "abstract": "  We propose a way to train deep learning based keypoint descriptors that makes\nthem approximately equivariant for locally affine transformations of the image\nplane. The main idea is to use the representation theory of GL(2) to generalize\nthe recently introduced concept of steerers from rotations to affine\ntransformations. Affine steerers give high control over how keypoint\ndescriptions transform under image transformations. We demonstrate the\npotential of using this control for image matching. Finally, we propose a way\nto finetune keypoint descriptors with a set of steerers on upright images and\nobtain state-of-the-art results on several standard benchmarks. Code will be\npublished at github.com/georg-bn/affine-steerers.\n", "link": "http://arxiv.org/abs/2408.14186v1", "date": "2024-08-26", "relevancy": 2.6663, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6126}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4983}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Affine%20steerers%20for%20structured%20keypoint%20description&body=Title%3A%20Affine%20steerers%20for%20structured%20keypoint%20description%0AAuthor%3A%20Georg%20B%C3%B6kman%20and%20Johan%20Edstedt%20and%20Michael%20Felsberg%20and%20Fredrik%20Kahl%0AAbstract%3A%20%20%20We%20propose%20a%20way%20to%20train%20deep%20learning%20based%20keypoint%20descriptors%20that%20makes%0Athem%20approximately%20equivariant%20for%20locally%20affine%20transformations%20of%20the%20image%0Aplane.%20The%20main%20idea%20is%20to%20use%20the%20representation%20theory%20of%20GL%282%29%20to%20generalize%0Athe%20recently%20introduced%20concept%20of%20steerers%20from%20rotations%20to%20affine%0Atransformations.%20Affine%20steerers%20give%20high%20control%20over%20how%20keypoint%0Adescriptions%20transform%20under%20image%20transformations.%20We%20demonstrate%20the%0Apotential%20of%20using%20this%20control%20for%20image%20matching.%20Finally%2C%20we%20propose%20a%20way%0Ato%20finetune%20keypoint%20descriptors%20with%20a%20set%20of%20steerers%20on%20upright%20images%20and%0Aobtain%20state-of-the-art%20results%20on%20several%20standard%20benchmarks.%20Code%20will%20be%0Apublished%20at%20github.com/georg-bn/affine-steerers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14186v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAffine%2520steerers%2520for%2520structured%2520keypoint%2520description%26entry.906535625%3DGeorg%2520B%25C3%25B6kman%2520and%2520Johan%2520Edstedt%2520and%2520Michael%2520Felsberg%2520and%2520Fredrik%2520Kahl%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520way%2520to%2520train%2520deep%2520learning%2520based%2520keypoint%2520descriptors%2520that%2520makes%250Athem%2520approximately%2520equivariant%2520for%2520locally%2520affine%2520transformations%2520of%2520the%2520image%250Aplane.%2520The%2520main%2520idea%2520is%2520to%2520use%2520the%2520representation%2520theory%2520of%2520GL%25282%2529%2520to%2520generalize%250Athe%2520recently%2520introduced%2520concept%2520of%2520steerers%2520from%2520rotations%2520to%2520affine%250Atransformations.%2520Affine%2520steerers%2520give%2520high%2520control%2520over%2520how%2520keypoint%250Adescriptions%2520transform%2520under%2520image%2520transformations.%2520We%2520demonstrate%2520the%250Apotential%2520of%2520using%2520this%2520control%2520for%2520image%2520matching.%2520Finally%252C%2520we%2520propose%2520a%2520way%250Ato%2520finetune%2520keypoint%2520descriptors%2520with%2520a%2520set%2520of%2520steerers%2520on%2520upright%2520images%2520and%250Aobtain%2520state-of-the-art%2520results%2520on%2520several%2520standard%2520benchmarks.%2520Code%2520will%2520be%250Apublished%2520at%2520github.com/georg-bn/affine-steerers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14186v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Affine%20steerers%20for%20structured%20keypoint%20description&entry.906535625=Georg%20B%C3%B6kman%20and%20Johan%20Edstedt%20and%20Michael%20Felsberg%20and%20Fredrik%20Kahl&entry.1292438233=%20%20We%20propose%20a%20way%20to%20train%20deep%20learning%20based%20keypoint%20descriptors%20that%20makes%0Athem%20approximately%20equivariant%20for%20locally%20affine%20transformations%20of%20the%20image%0Aplane.%20The%20main%20idea%20is%20to%20use%20the%20representation%20theory%20of%20GL%282%29%20to%20generalize%0Athe%20recently%20introduced%20concept%20of%20steerers%20from%20rotations%20to%20affine%0Atransformations.%20Affine%20steerers%20give%20high%20control%20over%20how%20keypoint%0Adescriptions%20transform%20under%20image%20transformations.%20We%20demonstrate%20the%0Apotential%20of%20using%20this%20control%20for%20image%20matching.%20Finally%2C%20we%20propose%20a%20way%0Ato%20finetune%20keypoint%20descriptors%20with%20a%20set%20of%20steerers%20on%20upright%20images%20and%0Aobtain%20state-of-the-art%20results%20on%20several%20standard%20benchmarks.%20Code%20will%20be%0Apublished%20at%20github.com/georg-bn/affine-steerers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14186v1&entry.124074799=Read"},
{"title": "OLGA: One-cLass Graph Autoencoder", "author": "M. P. S. G\u00f4lo and J. G. B. M. Junior and D. F. Silva and R. M. Marcacini", "abstract": "  One-class learning (OCL) comprises a set of techniques applied when\nreal-world problems have a single class of interest. The usual procedure for\nOCL is learning a hypersphere that comprises instances of this class and,\nideally, repels unseen instances from any other classes. Besides, several OCL\nalgorithms for graphs have been proposed since graph representation learning\nhas succeeded in various fields. These methods may use a two-step strategy,\ninitially representing the graph and, in a second step, classifying its nodes.\nOn the other hand, end-to-end methods learn the node representations while\nclassifying the nodes in one learning process. We highlight three main gaps in\nthe literature on OCL for graphs: (i) non-customized representations for OCL;\n(ii) the lack of constraints on hypersphere parameters learning; and (iii) the\nmethods' lack of interpretability and visualization. We propose One-cLass Graph\nAutoencoder (OLGA). OLGA is end-to-end and learns the representations for the\ngraph nodes while encapsulating the interest instances by combining two loss\nfunctions. We propose a new hypersphere loss function to encapsulate the\ninterest instances. OLGA combines this new hypersphere loss with the graph\nautoencoder reconstruction loss to improve model learning. OLGA achieved\nstate-of-the-art results and outperformed six other methods with a\nstatistically significant difference from five methods. Moreover, OLGA learns\nlow-dimensional representations maintaining the classification performance with\nan interpretable model representation learning and results.\n", "link": "http://arxiv.org/abs/2406.09131v2", "date": "2024-08-26", "relevancy": 2.6361, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5835}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5242}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OLGA%3A%20One-cLass%20Graph%20Autoencoder&body=Title%3A%20OLGA%3A%20One-cLass%20Graph%20Autoencoder%0AAuthor%3A%20M.%20P.%20S.%20G%C3%B4lo%20and%20J.%20G.%20B.%20M.%20Junior%20and%20D.%20F.%20Silva%20and%20R.%20M.%20Marcacini%0AAbstract%3A%20%20%20One-class%20learning%20%28OCL%29%20comprises%20a%20set%20of%20techniques%20applied%20when%0Areal-world%20problems%20have%20a%20single%20class%20of%20interest.%20The%20usual%20procedure%20for%0AOCL%20is%20learning%20a%20hypersphere%20that%20comprises%20instances%20of%20this%20class%20and%2C%0Aideally%2C%20repels%20unseen%20instances%20from%20any%20other%20classes.%20Besides%2C%20several%20OCL%0Aalgorithms%20for%20graphs%20have%20been%20proposed%20since%20graph%20representation%20learning%0Ahas%20succeeded%20in%20various%20fields.%20These%20methods%20may%20use%20a%20two-step%20strategy%2C%0Ainitially%20representing%20the%20graph%20and%2C%20in%20a%20second%20step%2C%20classifying%20its%20nodes.%0AOn%20the%20other%20hand%2C%20end-to-end%20methods%20learn%20the%20node%20representations%20while%0Aclassifying%20the%20nodes%20in%20one%20learning%20process.%20We%20highlight%20three%20main%20gaps%20in%0Athe%20literature%20on%20OCL%20for%20graphs%3A%20%28i%29%20non-customized%20representations%20for%20OCL%3B%0A%28ii%29%20the%20lack%20of%20constraints%20on%20hypersphere%20parameters%20learning%3B%20and%20%28iii%29%20the%0Amethods%27%20lack%20of%20interpretability%20and%20visualization.%20We%20propose%20One-cLass%20Graph%0AAutoencoder%20%28OLGA%29.%20OLGA%20is%20end-to-end%20and%20learns%20the%20representations%20for%20the%0Agraph%20nodes%20while%20encapsulating%20the%20interest%20instances%20by%20combining%20two%20loss%0Afunctions.%20We%20propose%20a%20new%20hypersphere%20loss%20function%20to%20encapsulate%20the%0Ainterest%20instances.%20OLGA%20combines%20this%20new%20hypersphere%20loss%20with%20the%20graph%0Aautoencoder%20reconstruction%20loss%20to%20improve%20model%20learning.%20OLGA%20achieved%0Astate-of-the-art%20results%20and%20outperformed%20six%20other%20methods%20with%20a%0Astatistically%20significant%20difference%20from%20five%20methods.%20Moreover%2C%20OLGA%20learns%0Alow-dimensional%20representations%20maintaining%20the%20classification%20performance%20with%0Aan%20interpretable%20model%20representation%20learning%20and%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09131v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOLGA%253A%2520One-cLass%2520Graph%2520Autoencoder%26entry.906535625%3DM.%2520P.%2520S.%2520G%25C3%25B4lo%2520and%2520J.%2520G.%2520B.%2520M.%2520Junior%2520and%2520D.%2520F.%2520Silva%2520and%2520R.%2520M.%2520Marcacini%26entry.1292438233%3D%2520%2520One-class%2520learning%2520%2528OCL%2529%2520comprises%2520a%2520set%2520of%2520techniques%2520applied%2520when%250Areal-world%2520problems%2520have%2520a%2520single%2520class%2520of%2520interest.%2520The%2520usual%2520procedure%2520for%250AOCL%2520is%2520learning%2520a%2520hypersphere%2520that%2520comprises%2520instances%2520of%2520this%2520class%2520and%252C%250Aideally%252C%2520repels%2520unseen%2520instances%2520from%2520any%2520other%2520classes.%2520Besides%252C%2520several%2520OCL%250Aalgorithms%2520for%2520graphs%2520have%2520been%2520proposed%2520since%2520graph%2520representation%2520learning%250Ahas%2520succeeded%2520in%2520various%2520fields.%2520These%2520methods%2520may%2520use%2520a%2520two-step%2520strategy%252C%250Ainitially%2520representing%2520the%2520graph%2520and%252C%2520in%2520a%2520second%2520step%252C%2520classifying%2520its%2520nodes.%250AOn%2520the%2520other%2520hand%252C%2520end-to-end%2520methods%2520learn%2520the%2520node%2520representations%2520while%250Aclassifying%2520the%2520nodes%2520in%2520one%2520learning%2520process.%2520We%2520highlight%2520three%2520main%2520gaps%2520in%250Athe%2520literature%2520on%2520OCL%2520for%2520graphs%253A%2520%2528i%2529%2520non-customized%2520representations%2520for%2520OCL%253B%250A%2528ii%2529%2520the%2520lack%2520of%2520constraints%2520on%2520hypersphere%2520parameters%2520learning%253B%2520and%2520%2528iii%2529%2520the%250Amethods%2527%2520lack%2520of%2520interpretability%2520and%2520visualization.%2520We%2520propose%2520One-cLass%2520Graph%250AAutoencoder%2520%2528OLGA%2529.%2520OLGA%2520is%2520end-to-end%2520and%2520learns%2520the%2520representations%2520for%2520the%250Agraph%2520nodes%2520while%2520encapsulating%2520the%2520interest%2520instances%2520by%2520combining%2520two%2520loss%250Afunctions.%2520We%2520propose%2520a%2520new%2520hypersphere%2520loss%2520function%2520to%2520encapsulate%2520the%250Ainterest%2520instances.%2520OLGA%2520combines%2520this%2520new%2520hypersphere%2520loss%2520with%2520the%2520graph%250Aautoencoder%2520reconstruction%2520loss%2520to%2520improve%2520model%2520learning.%2520OLGA%2520achieved%250Astate-of-the-art%2520results%2520and%2520outperformed%2520six%2520other%2520methods%2520with%2520a%250Astatistically%2520significant%2520difference%2520from%2520five%2520methods.%2520Moreover%252C%2520OLGA%2520learns%250Alow-dimensional%2520representations%2520maintaining%2520the%2520classification%2520performance%2520with%250Aan%2520interpretable%2520model%2520representation%2520learning%2520and%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09131v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OLGA%3A%20One-cLass%20Graph%20Autoencoder&entry.906535625=M.%20P.%20S.%20G%C3%B4lo%20and%20J.%20G.%20B.%20M.%20Junior%20and%20D.%20F.%20Silva%20and%20R.%20M.%20Marcacini&entry.1292438233=%20%20One-class%20learning%20%28OCL%29%20comprises%20a%20set%20of%20techniques%20applied%20when%0Areal-world%20problems%20have%20a%20single%20class%20of%20interest.%20The%20usual%20procedure%20for%0AOCL%20is%20learning%20a%20hypersphere%20that%20comprises%20instances%20of%20this%20class%20and%2C%0Aideally%2C%20repels%20unseen%20instances%20from%20any%20other%20classes.%20Besides%2C%20several%20OCL%0Aalgorithms%20for%20graphs%20have%20been%20proposed%20since%20graph%20representation%20learning%0Ahas%20succeeded%20in%20various%20fields.%20These%20methods%20may%20use%20a%20two-step%20strategy%2C%0Ainitially%20representing%20the%20graph%20and%2C%20in%20a%20second%20step%2C%20classifying%20its%20nodes.%0AOn%20the%20other%20hand%2C%20end-to-end%20methods%20learn%20the%20node%20representations%20while%0Aclassifying%20the%20nodes%20in%20one%20learning%20process.%20We%20highlight%20three%20main%20gaps%20in%0Athe%20literature%20on%20OCL%20for%20graphs%3A%20%28i%29%20non-customized%20representations%20for%20OCL%3B%0A%28ii%29%20the%20lack%20of%20constraints%20on%20hypersphere%20parameters%20learning%3B%20and%20%28iii%29%20the%0Amethods%27%20lack%20of%20interpretability%20and%20visualization.%20We%20propose%20One-cLass%20Graph%0AAutoencoder%20%28OLGA%29.%20OLGA%20is%20end-to-end%20and%20learns%20the%20representations%20for%20the%0Agraph%20nodes%20while%20encapsulating%20the%20interest%20instances%20by%20combining%20two%20loss%0Afunctions.%20We%20propose%20a%20new%20hypersphere%20loss%20function%20to%20encapsulate%20the%0Ainterest%20instances.%20OLGA%20combines%20this%20new%20hypersphere%20loss%20with%20the%20graph%0Aautoencoder%20reconstruction%20loss%20to%20improve%20model%20learning.%20OLGA%20achieved%0Astate-of-the-art%20results%20and%20outperformed%20six%20other%20methods%20with%20a%0Astatistically%20significant%20difference%20from%20five%20methods.%20Moreover%2C%20OLGA%20learns%0Alow-dimensional%20representations%20maintaining%20the%20classification%20performance%20with%0Aan%20interpretable%20model%20representation%20learning%20and%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09131v2&entry.124074799=Read"},
{"title": "Be Persistent: Towards a Unified Solution for Mitigating Shortcuts in\n  Deep Learning", "author": "Hadi M. Dolatabadi and Sarah M. Erfani and Christopher Leckie", "abstract": "  Deep neural networks (DNNs) are vulnerable to shortcut learning: rather than\nlearning the intended task, they tend to draw inconclusive relationships\nbetween their inputs and outputs. Shortcut learning is ubiquitous among many\nfailure cases of neural networks, and traces of this phenomenon can be seen in\ntheir generalizability issues, domain shift, adversarial vulnerability, and\neven bias towards majority groups. In this paper, we argue that this\ncommonality in the cause of various DNN issues creates a significant\nopportunity that should be leveraged to find a unified solution for shortcut\nlearning. To this end, we outline the recent advances in topological data\nanalysis (TDA), and persistent homology (PH) in particular, to sketch a unified\nroadmap for detecting shortcuts in deep learning. We demonstrate our arguments\nby investigating the topological features of computational graphs in DNNs using\ntwo cases of unlearnable examples and bias in decision-making as our test\nstudies. Our analysis of these two failure cases of DNNs reveals that finding a\nunified solution for shortcut learning in DNNs is not out of reach, and TDA can\nplay a significant role in forming such a framework.\n", "link": "http://arxiv.org/abs/2402.11237v2", "date": "2024-08-26", "relevancy": 2.5922, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5333}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5128}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Be%20Persistent%3A%20Towards%20a%20Unified%20Solution%20for%20Mitigating%20Shortcuts%20in%0A%20%20Deep%20Learning&body=Title%3A%20Be%20Persistent%3A%20Towards%20a%20Unified%20Solution%20for%20Mitigating%20Shortcuts%20in%0A%20%20Deep%20Learning%0AAuthor%3A%20Hadi%20M.%20Dolatabadi%20and%20Sarah%20M.%20Erfani%20and%20Christopher%20Leckie%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20are%20vulnerable%20to%20shortcut%20learning%3A%20rather%20than%0Alearning%20the%20intended%20task%2C%20they%20tend%20to%20draw%20inconclusive%20relationships%0Abetween%20their%20inputs%20and%20outputs.%20Shortcut%20learning%20is%20ubiquitous%20among%20many%0Afailure%20cases%20of%20neural%20networks%2C%20and%20traces%20of%20this%20phenomenon%20can%20be%20seen%20in%0Atheir%20generalizability%20issues%2C%20domain%20shift%2C%20adversarial%20vulnerability%2C%20and%0Aeven%20bias%20towards%20majority%20groups.%20In%20this%20paper%2C%20we%20argue%20that%20this%0Acommonality%20in%20the%20cause%20of%20various%20DNN%20issues%20creates%20a%20significant%0Aopportunity%20that%20should%20be%20leveraged%20to%20find%20a%20unified%20solution%20for%20shortcut%0Alearning.%20To%20this%20end%2C%20we%20outline%20the%20recent%20advances%20in%20topological%20data%0Aanalysis%20%28TDA%29%2C%20and%20persistent%20homology%20%28PH%29%20in%20particular%2C%20to%20sketch%20a%20unified%0Aroadmap%20for%20detecting%20shortcuts%20in%20deep%20learning.%20We%20demonstrate%20our%20arguments%0Aby%20investigating%20the%20topological%20features%20of%20computational%20graphs%20in%20DNNs%20using%0Atwo%20cases%20of%20unlearnable%20examples%20and%20bias%20in%20decision-making%20as%20our%20test%0Astudies.%20Our%20analysis%20of%20these%20two%20failure%20cases%20of%20DNNs%20reveals%20that%20finding%20a%0Aunified%20solution%20for%20shortcut%20learning%20in%20DNNs%20is%20not%20out%20of%20reach%2C%20and%20TDA%20can%0Aplay%20a%20significant%20role%20in%20forming%20such%20a%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11237v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBe%2520Persistent%253A%2520Towards%2520a%2520Unified%2520Solution%2520for%2520Mitigating%2520Shortcuts%2520in%250A%2520%2520Deep%2520Learning%26entry.906535625%3DHadi%2520M.%2520Dolatabadi%2520and%2520Sarah%2520M.%2520Erfani%2520and%2520Christopher%2520Leckie%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520are%2520vulnerable%2520to%2520shortcut%2520learning%253A%2520rather%2520than%250Alearning%2520the%2520intended%2520task%252C%2520they%2520tend%2520to%2520draw%2520inconclusive%2520relationships%250Abetween%2520their%2520inputs%2520and%2520outputs.%2520Shortcut%2520learning%2520is%2520ubiquitous%2520among%2520many%250Afailure%2520cases%2520of%2520neural%2520networks%252C%2520and%2520traces%2520of%2520this%2520phenomenon%2520can%2520be%2520seen%2520in%250Atheir%2520generalizability%2520issues%252C%2520domain%2520shift%252C%2520adversarial%2520vulnerability%252C%2520and%250Aeven%2520bias%2520towards%2520majority%2520groups.%2520In%2520this%2520paper%252C%2520we%2520argue%2520that%2520this%250Acommonality%2520in%2520the%2520cause%2520of%2520various%2520DNN%2520issues%2520creates%2520a%2520significant%250Aopportunity%2520that%2520should%2520be%2520leveraged%2520to%2520find%2520a%2520unified%2520solution%2520for%2520shortcut%250Alearning.%2520To%2520this%2520end%252C%2520we%2520outline%2520the%2520recent%2520advances%2520in%2520topological%2520data%250Aanalysis%2520%2528TDA%2529%252C%2520and%2520persistent%2520homology%2520%2528PH%2529%2520in%2520particular%252C%2520to%2520sketch%2520a%2520unified%250Aroadmap%2520for%2520detecting%2520shortcuts%2520in%2520deep%2520learning.%2520We%2520demonstrate%2520our%2520arguments%250Aby%2520investigating%2520the%2520topological%2520features%2520of%2520computational%2520graphs%2520in%2520DNNs%2520using%250Atwo%2520cases%2520of%2520unlearnable%2520examples%2520and%2520bias%2520in%2520decision-making%2520as%2520our%2520test%250Astudies.%2520Our%2520analysis%2520of%2520these%2520two%2520failure%2520cases%2520of%2520DNNs%2520reveals%2520that%2520finding%2520a%250Aunified%2520solution%2520for%2520shortcut%2520learning%2520in%2520DNNs%2520is%2520not%2520out%2520of%2520reach%252C%2520and%2520TDA%2520can%250Aplay%2520a%2520significant%2520role%2520in%2520forming%2520such%2520a%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.11237v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Be%20Persistent%3A%20Towards%20a%20Unified%20Solution%20for%20Mitigating%20Shortcuts%20in%0A%20%20Deep%20Learning&entry.906535625=Hadi%20M.%20Dolatabadi%20and%20Sarah%20M.%20Erfani%20and%20Christopher%20Leckie&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20are%20vulnerable%20to%20shortcut%20learning%3A%20rather%20than%0Alearning%20the%20intended%20task%2C%20they%20tend%20to%20draw%20inconclusive%20relationships%0Abetween%20their%20inputs%20and%20outputs.%20Shortcut%20learning%20is%20ubiquitous%20among%20many%0Afailure%20cases%20of%20neural%20networks%2C%20and%20traces%20of%20this%20phenomenon%20can%20be%20seen%20in%0Atheir%20generalizability%20issues%2C%20domain%20shift%2C%20adversarial%20vulnerability%2C%20and%0Aeven%20bias%20towards%20majority%20groups.%20In%20this%20paper%2C%20we%20argue%20that%20this%0Acommonality%20in%20the%20cause%20of%20various%20DNN%20issues%20creates%20a%20significant%0Aopportunity%20that%20should%20be%20leveraged%20to%20find%20a%20unified%20solution%20for%20shortcut%0Alearning.%20To%20this%20end%2C%20we%20outline%20the%20recent%20advances%20in%20topological%20data%0Aanalysis%20%28TDA%29%2C%20and%20persistent%20homology%20%28PH%29%20in%20particular%2C%20to%20sketch%20a%20unified%0Aroadmap%20for%20detecting%20shortcuts%20in%20deep%20learning.%20We%20demonstrate%20our%20arguments%0Aby%20investigating%20the%20topological%20features%20of%20computational%20graphs%20in%20DNNs%20using%0Atwo%20cases%20of%20unlearnable%20examples%20and%20bias%20in%20decision-making%20as%20our%20test%0Astudies.%20Our%20analysis%20of%20these%20two%20failure%20cases%20of%20DNNs%20reveals%20that%20finding%20a%0Aunified%20solution%20for%20shortcut%20learning%20in%20DNNs%20is%20not%20out%20of%20reach%2C%20and%20TDA%20can%0Aplay%20a%20significant%20role%20in%20forming%20such%20a%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11237v2&entry.124074799=Read"},
{"title": "Pruning Large Language Models with Semi-Structural Adaptive Sparse\n  Training", "author": "Weiyu Huang and Yuezhou Hu and Guohao Jian and Jun Zhu and Jianfei Chen", "abstract": "  The tremendous success of Large Language Models (LLMs) across various complex\ntasks relies heavily on their substantial scale, which raises challenges during\nmodel deployment due to their large memory consumption. Recently, numerous\nstudies have attempted to compress LLMs using one-shot pruning methods.\nHowever, these methods often experience considerable performance degradation on\ncomplex language understanding tasks, calling into question the feasibility of\npruning in LLMs. To address this issue, we propose a pruning pipeline for\nsemi-structured sparse models via retraining, termed Adaptive Sparse Trainer\n(AST). Unlike previous one-shot pruning methods, AST incrementally transforms\ndense models into sparse ones by applying decay to masked weights while\nallowing the model to adaptively select masks throughout the training process.\nFurthermore, we observe that using distillation with a dense model as the\nteacher can prevent the sparse model from falling into local optima and\naccelerate convergence. In addition, we incorporate extra well-initialized\nparameters to further enhance model performance with minimal increase in memory\nfootprint. AST can significantly enhance model performance, approaching the\nlevel of dense models. When applied to the LLaMA2-7B model, AST reduces the\nzero-shot accuracy gap between dense and semi-structured sparse models to 1.12%\nacross multiple zero-shot tasks, utilizing less than 0.4% of the pretraining\ntokens. Our work demonstrates the feasibility of deploying semi-structured\nsparse large language models and introduces a novel method for achieving highly\ncompressed models when combined with existing quantization techniques.\n", "link": "http://arxiv.org/abs/2407.20584v2", "date": "2024-08-26", "relevancy": 2.5802, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5244}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5185}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pruning%20Large%20Language%20Models%20with%20Semi-Structural%20Adaptive%20Sparse%0A%20%20Training&body=Title%3A%20Pruning%20Large%20Language%20Models%20with%20Semi-Structural%20Adaptive%20Sparse%0A%20%20Training%0AAuthor%3A%20Weiyu%20Huang%20and%20Yuezhou%20Hu%20and%20Guohao%20Jian%20and%20Jun%20Zhu%20and%20Jianfei%20Chen%0AAbstract%3A%20%20%20The%20tremendous%20success%20of%20Large%20Language%20Models%20%28LLMs%29%20across%20various%20complex%0Atasks%20relies%20heavily%20on%20their%20substantial%20scale%2C%20which%20raises%20challenges%20during%0Amodel%20deployment%20due%20to%20their%20large%20memory%20consumption.%20Recently%2C%20numerous%0Astudies%20have%20attempted%20to%20compress%20LLMs%20using%20one-shot%20pruning%20methods.%0AHowever%2C%20these%20methods%20often%20experience%20considerable%20performance%20degradation%20on%0Acomplex%20language%20understanding%20tasks%2C%20calling%20into%20question%20the%20feasibility%20of%0Apruning%20in%20LLMs.%20To%20address%20this%20issue%2C%20we%20propose%20a%20pruning%20pipeline%20for%0Asemi-structured%20sparse%20models%20via%20retraining%2C%20termed%20Adaptive%20Sparse%20Trainer%0A%28AST%29.%20Unlike%20previous%20one-shot%20pruning%20methods%2C%20AST%20incrementally%20transforms%0Adense%20models%20into%20sparse%20ones%20by%20applying%20decay%20to%20masked%20weights%20while%0Aallowing%20the%20model%20to%20adaptively%20select%20masks%20throughout%20the%20training%20process.%0AFurthermore%2C%20we%20observe%20that%20using%20distillation%20with%20a%20dense%20model%20as%20the%0Ateacher%20can%20prevent%20the%20sparse%20model%20from%20falling%20into%20local%20optima%20and%0Aaccelerate%20convergence.%20In%20addition%2C%20we%20incorporate%20extra%20well-initialized%0Aparameters%20to%20further%20enhance%20model%20performance%20with%20minimal%20increase%20in%20memory%0Afootprint.%20AST%20can%20significantly%20enhance%20model%20performance%2C%20approaching%20the%0Alevel%20of%20dense%20models.%20When%20applied%20to%20the%20LLaMA2-7B%20model%2C%20AST%20reduces%20the%0Azero-shot%20accuracy%20gap%20between%20dense%20and%20semi-structured%20sparse%20models%20to%201.12%25%0Aacross%20multiple%20zero-shot%20tasks%2C%20utilizing%20less%20than%200.4%25%20of%20the%20pretraining%0Atokens.%20Our%20work%20demonstrates%20the%20feasibility%20of%20deploying%20semi-structured%0Asparse%20large%20language%20models%20and%20introduces%20a%20novel%20method%20for%20achieving%20highly%0Acompressed%20models%20when%20combined%20with%20existing%20quantization%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.20584v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPruning%2520Large%2520Language%2520Models%2520with%2520Semi-Structural%2520Adaptive%2520Sparse%250A%2520%2520Training%26entry.906535625%3DWeiyu%2520Huang%2520and%2520Yuezhou%2520Hu%2520and%2520Guohao%2520Jian%2520and%2520Jun%2520Zhu%2520and%2520Jianfei%2520Chen%26entry.1292438233%3D%2520%2520The%2520tremendous%2520success%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520across%2520various%2520complex%250Atasks%2520relies%2520heavily%2520on%2520their%2520substantial%2520scale%252C%2520which%2520raises%2520challenges%2520during%250Amodel%2520deployment%2520due%2520to%2520their%2520large%2520memory%2520consumption.%2520Recently%252C%2520numerous%250Astudies%2520have%2520attempted%2520to%2520compress%2520LLMs%2520using%2520one-shot%2520pruning%2520methods.%250AHowever%252C%2520these%2520methods%2520often%2520experience%2520considerable%2520performance%2520degradation%2520on%250Acomplex%2520language%2520understanding%2520tasks%252C%2520calling%2520into%2520question%2520the%2520feasibility%2520of%250Apruning%2520in%2520LLMs.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520pruning%2520pipeline%2520for%250Asemi-structured%2520sparse%2520models%2520via%2520retraining%252C%2520termed%2520Adaptive%2520Sparse%2520Trainer%250A%2528AST%2529.%2520Unlike%2520previous%2520one-shot%2520pruning%2520methods%252C%2520AST%2520incrementally%2520transforms%250Adense%2520models%2520into%2520sparse%2520ones%2520by%2520applying%2520decay%2520to%2520masked%2520weights%2520while%250Aallowing%2520the%2520model%2520to%2520adaptively%2520select%2520masks%2520throughout%2520the%2520training%2520process.%250AFurthermore%252C%2520we%2520observe%2520that%2520using%2520distillation%2520with%2520a%2520dense%2520model%2520as%2520the%250Ateacher%2520can%2520prevent%2520the%2520sparse%2520model%2520from%2520falling%2520into%2520local%2520optima%2520and%250Aaccelerate%2520convergence.%2520In%2520addition%252C%2520we%2520incorporate%2520extra%2520well-initialized%250Aparameters%2520to%2520further%2520enhance%2520model%2520performance%2520with%2520minimal%2520increase%2520in%2520memory%250Afootprint.%2520AST%2520can%2520significantly%2520enhance%2520model%2520performance%252C%2520approaching%2520the%250Alevel%2520of%2520dense%2520models.%2520When%2520applied%2520to%2520the%2520LLaMA2-7B%2520model%252C%2520AST%2520reduces%2520the%250Azero-shot%2520accuracy%2520gap%2520between%2520dense%2520and%2520semi-structured%2520sparse%2520models%2520to%25201.12%2525%250Aacross%2520multiple%2520zero-shot%2520tasks%252C%2520utilizing%2520less%2520than%25200.4%2525%2520of%2520the%2520pretraining%250Atokens.%2520Our%2520work%2520demonstrates%2520the%2520feasibility%2520of%2520deploying%2520semi-structured%250Asparse%2520large%2520language%2520models%2520and%2520introduces%2520a%2520novel%2520method%2520for%2520achieving%2520highly%250Acompressed%2520models%2520when%2520combined%2520with%2520existing%2520quantization%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.20584v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pruning%20Large%20Language%20Models%20with%20Semi-Structural%20Adaptive%20Sparse%0A%20%20Training&entry.906535625=Weiyu%20Huang%20and%20Yuezhou%20Hu%20and%20Guohao%20Jian%20and%20Jun%20Zhu%20and%20Jianfei%20Chen&entry.1292438233=%20%20The%20tremendous%20success%20of%20Large%20Language%20Models%20%28LLMs%29%20across%20various%20complex%0Atasks%20relies%20heavily%20on%20their%20substantial%20scale%2C%20which%20raises%20challenges%20during%0Amodel%20deployment%20due%20to%20their%20large%20memory%20consumption.%20Recently%2C%20numerous%0Astudies%20have%20attempted%20to%20compress%20LLMs%20using%20one-shot%20pruning%20methods.%0AHowever%2C%20these%20methods%20often%20experience%20considerable%20performance%20degradation%20on%0Acomplex%20language%20understanding%20tasks%2C%20calling%20into%20question%20the%20feasibility%20of%0Apruning%20in%20LLMs.%20To%20address%20this%20issue%2C%20we%20propose%20a%20pruning%20pipeline%20for%0Asemi-structured%20sparse%20models%20via%20retraining%2C%20termed%20Adaptive%20Sparse%20Trainer%0A%28AST%29.%20Unlike%20previous%20one-shot%20pruning%20methods%2C%20AST%20incrementally%20transforms%0Adense%20models%20into%20sparse%20ones%20by%20applying%20decay%20to%20masked%20weights%20while%0Aallowing%20the%20model%20to%20adaptively%20select%20masks%20throughout%20the%20training%20process.%0AFurthermore%2C%20we%20observe%20that%20using%20distillation%20with%20a%20dense%20model%20as%20the%0Ateacher%20can%20prevent%20the%20sparse%20model%20from%20falling%20into%20local%20optima%20and%0Aaccelerate%20convergence.%20In%20addition%2C%20we%20incorporate%20extra%20well-initialized%0Aparameters%20to%20further%20enhance%20model%20performance%20with%20minimal%20increase%20in%20memory%0Afootprint.%20AST%20can%20significantly%20enhance%20model%20performance%2C%20approaching%20the%0Alevel%20of%20dense%20models.%20When%20applied%20to%20the%20LLaMA2-7B%20model%2C%20AST%20reduces%20the%0Azero-shot%20accuracy%20gap%20between%20dense%20and%20semi-structured%20sparse%20models%20to%201.12%25%0Aacross%20multiple%20zero-shot%20tasks%2C%20utilizing%20less%20than%200.4%25%20of%20the%20pretraining%0Atokens.%20Our%20work%20demonstrates%20the%20feasibility%20of%20deploying%20semi-structured%0Asparse%20large%20language%20models%20and%20introduces%20a%20novel%20method%20for%20achieving%20highly%0Acompressed%20models%20when%20combined%20with%20existing%20quantization%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.20584v2&entry.124074799=Read"},
{"title": "Learning Tree-Structured Composition of Data Augmentation", "author": "Dongyue Li and Kailai Chen and Predrag Radivojac and Hongyang R. Zhang", "abstract": "  Data augmentation is widely used for training a neural network given little\nlabeled data. A common practice of augmentation training is applying a\ncomposition of multiple transformations sequentially to the data. Existing\naugmentation methods such as RandAugment randomly sample from a list of\npre-selected transformations, while methods such as AutoAugment apply advanced\nsearch to optimize over an augmentation set of size $k^d$, which is the number\nof transformation sequences of length $d$, given a list of $k$ transformations.\n  In this paper, we design efficient algorithms whose running time complexity\nis much faster than the worst-case complexity of $O(k^d)$, provably. We propose\na new algorithm to search for a binary tree-structured composition of $k$\ntransformations, where each tree node corresponds to one transformation. The\nbinary tree generalizes sequential augmentations, such as the SimCLR\naugmentation scheme for contrastive learning. Using a top-down, recursive\nsearch procedure, our algorithm achieves a runtime complexity of $O(2^d k)$,\nwhich is much faster than $O(k^d)$ as $k$ increases above $2$. We apply our\nalgorithm to tackle data distributions with heterogeneous subpopulations by\nsearching for one tree in each subpopulation and then learning a weighted\ncombination, resulting in a forest of trees.\n  We validate our proposed algorithms on numerous graph and image datasets,\nincluding a multi-label graph classification dataset we collected. The dataset\nexhibits significant variations in the sizes of graphs and their average\ndegrees, making it ideal for studying data augmentation. We show that our\napproach can reduce the computation cost by 43% over existing search methods\nwhile improving performance by 4.3%. The tree structures can be used to\ninterpret the relative importance of each transformation, such as identifying\nthe important transformations on small vs. large graphs.\n", "link": "http://arxiv.org/abs/2408.14381v1", "date": "2024-08-26", "relevancy": 2.5797, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5207}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5193}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Tree-Structured%20Composition%20of%20Data%20Augmentation&body=Title%3A%20Learning%20Tree-Structured%20Composition%20of%20Data%20Augmentation%0AAuthor%3A%20Dongyue%20Li%20and%20Kailai%20Chen%20and%20Predrag%20Radivojac%20and%20Hongyang%20R.%20Zhang%0AAbstract%3A%20%20%20Data%20augmentation%20is%20widely%20used%20for%20training%20a%20neural%20network%20given%20little%0Alabeled%20data.%20A%20common%20practice%20of%20augmentation%20training%20is%20applying%20a%0Acomposition%20of%20multiple%20transformations%20sequentially%20to%20the%20data.%20Existing%0Aaugmentation%20methods%20such%20as%20RandAugment%20randomly%20sample%20from%20a%20list%20of%0Apre-selected%20transformations%2C%20while%20methods%20such%20as%20AutoAugment%20apply%20advanced%0Asearch%20to%20optimize%20over%20an%20augmentation%20set%20of%20size%20%24k%5Ed%24%2C%20which%20is%20the%20number%0Aof%20transformation%20sequences%20of%20length%20%24d%24%2C%20given%20a%20list%20of%20%24k%24%20transformations.%0A%20%20In%20this%20paper%2C%20we%20design%20efficient%20algorithms%20whose%20running%20time%20complexity%0Ais%20much%20faster%20than%20the%20worst-case%20complexity%20of%20%24O%28k%5Ed%29%24%2C%20provably.%20We%20propose%0Aa%20new%20algorithm%20to%20search%20for%20a%20binary%20tree-structured%20composition%20of%20%24k%24%0Atransformations%2C%20where%20each%20tree%20node%20corresponds%20to%20one%20transformation.%20The%0Abinary%20tree%20generalizes%20sequential%20augmentations%2C%20such%20as%20the%20SimCLR%0Aaugmentation%20scheme%20for%20contrastive%20learning.%20Using%20a%20top-down%2C%20recursive%0Asearch%20procedure%2C%20our%20algorithm%20achieves%20a%20runtime%20complexity%20of%20%24O%282%5Ed%20k%29%24%2C%0Awhich%20is%20much%20faster%20than%20%24O%28k%5Ed%29%24%20as%20%24k%24%20increases%20above%20%242%24.%20We%20apply%20our%0Aalgorithm%20to%20tackle%20data%20distributions%20with%20heterogeneous%20subpopulations%20by%0Asearching%20for%20one%20tree%20in%20each%20subpopulation%20and%20then%20learning%20a%20weighted%0Acombination%2C%20resulting%20in%20a%20forest%20of%20trees.%0A%20%20We%20validate%20our%20proposed%20algorithms%20on%20numerous%20graph%20and%20image%20datasets%2C%0Aincluding%20a%20multi-label%20graph%20classification%20dataset%20we%20collected.%20The%20dataset%0Aexhibits%20significant%20variations%20in%20the%20sizes%20of%20graphs%20and%20their%20average%0Adegrees%2C%20making%20it%20ideal%20for%20studying%20data%20augmentation.%20We%20show%20that%20our%0Aapproach%20can%20reduce%20the%20computation%20cost%20by%2043%25%20over%20existing%20search%20methods%0Awhile%20improving%20performance%20by%204.3%25.%20The%20tree%20structures%20can%20be%20used%20to%0Ainterpret%20the%20relative%20importance%20of%20each%20transformation%2C%20such%20as%20identifying%0Athe%20important%20transformations%20on%20small%20vs.%20large%20graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14381v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Tree-Structured%2520Composition%2520of%2520Data%2520Augmentation%26entry.906535625%3DDongyue%2520Li%2520and%2520Kailai%2520Chen%2520and%2520Predrag%2520Radivojac%2520and%2520Hongyang%2520R.%2520Zhang%26entry.1292438233%3D%2520%2520Data%2520augmentation%2520is%2520widely%2520used%2520for%2520training%2520a%2520neural%2520network%2520given%2520little%250Alabeled%2520data.%2520A%2520common%2520practice%2520of%2520augmentation%2520training%2520is%2520applying%2520a%250Acomposition%2520of%2520multiple%2520transformations%2520sequentially%2520to%2520the%2520data.%2520Existing%250Aaugmentation%2520methods%2520such%2520as%2520RandAugment%2520randomly%2520sample%2520from%2520a%2520list%2520of%250Apre-selected%2520transformations%252C%2520while%2520methods%2520such%2520as%2520AutoAugment%2520apply%2520advanced%250Asearch%2520to%2520optimize%2520over%2520an%2520augmentation%2520set%2520of%2520size%2520%2524k%255Ed%2524%252C%2520which%2520is%2520the%2520number%250Aof%2520transformation%2520sequences%2520of%2520length%2520%2524d%2524%252C%2520given%2520a%2520list%2520of%2520%2524k%2524%2520transformations.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520design%2520efficient%2520algorithms%2520whose%2520running%2520time%2520complexity%250Ais%2520much%2520faster%2520than%2520the%2520worst-case%2520complexity%2520of%2520%2524O%2528k%255Ed%2529%2524%252C%2520provably.%2520We%2520propose%250Aa%2520new%2520algorithm%2520to%2520search%2520for%2520a%2520binary%2520tree-structured%2520composition%2520of%2520%2524k%2524%250Atransformations%252C%2520where%2520each%2520tree%2520node%2520corresponds%2520to%2520one%2520transformation.%2520The%250Abinary%2520tree%2520generalizes%2520sequential%2520augmentations%252C%2520such%2520as%2520the%2520SimCLR%250Aaugmentation%2520scheme%2520for%2520contrastive%2520learning.%2520Using%2520a%2520top-down%252C%2520recursive%250Asearch%2520procedure%252C%2520our%2520algorithm%2520achieves%2520a%2520runtime%2520complexity%2520of%2520%2524O%25282%255Ed%2520k%2529%2524%252C%250Awhich%2520is%2520much%2520faster%2520than%2520%2524O%2528k%255Ed%2529%2524%2520as%2520%2524k%2524%2520increases%2520above%2520%25242%2524.%2520We%2520apply%2520our%250Aalgorithm%2520to%2520tackle%2520data%2520distributions%2520with%2520heterogeneous%2520subpopulations%2520by%250Asearching%2520for%2520one%2520tree%2520in%2520each%2520subpopulation%2520and%2520then%2520learning%2520a%2520weighted%250Acombination%252C%2520resulting%2520in%2520a%2520forest%2520of%2520trees.%250A%2520%2520We%2520validate%2520our%2520proposed%2520algorithms%2520on%2520numerous%2520graph%2520and%2520image%2520datasets%252C%250Aincluding%2520a%2520multi-label%2520graph%2520classification%2520dataset%2520we%2520collected.%2520The%2520dataset%250Aexhibits%2520significant%2520variations%2520in%2520the%2520sizes%2520of%2520graphs%2520and%2520their%2520average%250Adegrees%252C%2520making%2520it%2520ideal%2520for%2520studying%2520data%2520augmentation.%2520We%2520show%2520that%2520our%250Aapproach%2520can%2520reduce%2520the%2520computation%2520cost%2520by%252043%2525%2520over%2520existing%2520search%2520methods%250Awhile%2520improving%2520performance%2520by%25204.3%2525.%2520The%2520tree%2520structures%2520can%2520be%2520used%2520to%250Ainterpret%2520the%2520relative%2520importance%2520of%2520each%2520transformation%252C%2520such%2520as%2520identifying%250Athe%2520important%2520transformations%2520on%2520small%2520vs.%2520large%2520graphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14381v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Tree-Structured%20Composition%20of%20Data%20Augmentation&entry.906535625=Dongyue%20Li%20and%20Kailai%20Chen%20and%20Predrag%20Radivojac%20and%20Hongyang%20R.%20Zhang&entry.1292438233=%20%20Data%20augmentation%20is%20widely%20used%20for%20training%20a%20neural%20network%20given%20little%0Alabeled%20data.%20A%20common%20practice%20of%20augmentation%20training%20is%20applying%20a%0Acomposition%20of%20multiple%20transformations%20sequentially%20to%20the%20data.%20Existing%0Aaugmentation%20methods%20such%20as%20RandAugment%20randomly%20sample%20from%20a%20list%20of%0Apre-selected%20transformations%2C%20while%20methods%20such%20as%20AutoAugment%20apply%20advanced%0Asearch%20to%20optimize%20over%20an%20augmentation%20set%20of%20size%20%24k%5Ed%24%2C%20which%20is%20the%20number%0Aof%20transformation%20sequences%20of%20length%20%24d%24%2C%20given%20a%20list%20of%20%24k%24%20transformations.%0A%20%20In%20this%20paper%2C%20we%20design%20efficient%20algorithms%20whose%20running%20time%20complexity%0Ais%20much%20faster%20than%20the%20worst-case%20complexity%20of%20%24O%28k%5Ed%29%24%2C%20provably.%20We%20propose%0Aa%20new%20algorithm%20to%20search%20for%20a%20binary%20tree-structured%20composition%20of%20%24k%24%0Atransformations%2C%20where%20each%20tree%20node%20corresponds%20to%20one%20transformation.%20The%0Abinary%20tree%20generalizes%20sequential%20augmentations%2C%20such%20as%20the%20SimCLR%0Aaugmentation%20scheme%20for%20contrastive%20learning.%20Using%20a%20top-down%2C%20recursive%0Asearch%20procedure%2C%20our%20algorithm%20achieves%20a%20runtime%20complexity%20of%20%24O%282%5Ed%20k%29%24%2C%0Awhich%20is%20much%20faster%20than%20%24O%28k%5Ed%29%24%20as%20%24k%24%20increases%20above%20%242%24.%20We%20apply%20our%0Aalgorithm%20to%20tackle%20data%20distributions%20with%20heterogeneous%20subpopulations%20by%0Asearching%20for%20one%20tree%20in%20each%20subpopulation%20and%20then%20learning%20a%20weighted%0Acombination%2C%20resulting%20in%20a%20forest%20of%20trees.%0A%20%20We%20validate%20our%20proposed%20algorithms%20on%20numerous%20graph%20and%20image%20datasets%2C%0Aincluding%20a%20multi-label%20graph%20classification%20dataset%20we%20collected.%20The%20dataset%0Aexhibits%20significant%20variations%20in%20the%20sizes%20of%20graphs%20and%20their%20average%0Adegrees%2C%20making%20it%20ideal%20for%20studying%20data%20augmentation.%20We%20show%20that%20our%0Aapproach%20can%20reduce%20the%20computation%20cost%20by%2043%25%20over%20existing%20search%20methods%0Awhile%20improving%20performance%20by%204.3%25.%20The%20tree%20structures%20can%20be%20used%20to%0Ainterpret%20the%20relative%20importance%20of%20each%20transformation%2C%20such%20as%20identifying%0Athe%20important%20transformations%20on%20small%20vs.%20large%20graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14381v1&entry.124074799=Read"},
{"title": "Application of Disentanglement to Map Registration Problem", "author": "Hae Jin Song and Patrycja Krawczuk and Po-Hsuan Huang", "abstract": "  Geospatial data come from various sources, such as satellites, aircraft, and\nLiDAR. The variability of the source is not limited to the types of data\nacquisition techniques, as we have maps from different time periods. To\nincorporate these data for a coherent analysis, it is essential to first align\ndifferent \"styles\" of geospatial data to its matching images that point to the\nsame location on the surface of the Earth. In this paper, we approach the image\nregistration as a two-step process of (1) extracting geospatial contents\ninvariant to visual (and any other non-content-related) information, and (2)\nmatching the data based on such (purely) geospatial contents. We hypothesize\nthat a combination of $\\beta$-VAE-like architecture [2] and adversarial\ntraining will achieve both the disentanglement of the geographic information\nand artistic styles and generation of new map tiles by composing the encoded\ngeographic information with any artistic style.\n", "link": "http://arxiv.org/abs/2408.14152v1", "date": "2024-08-26", "relevancy": 2.5681, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5331}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5143}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4934}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Application%20of%20Disentanglement%20to%20Map%20Registration%20Problem&body=Title%3A%20Application%20of%20Disentanglement%20to%20Map%20Registration%20Problem%0AAuthor%3A%20Hae%20Jin%20Song%20and%20Patrycja%20Krawczuk%20and%20Po-Hsuan%20Huang%0AAbstract%3A%20%20%20Geospatial%20data%20come%20from%20various%20sources%2C%20such%20as%20satellites%2C%20aircraft%2C%20and%0ALiDAR.%20The%20variability%20of%20the%20source%20is%20not%20limited%20to%20the%20types%20of%20data%0Aacquisition%20techniques%2C%20as%20we%20have%20maps%20from%20different%20time%20periods.%20To%0Aincorporate%20these%20data%20for%20a%20coherent%20analysis%2C%20it%20is%20essential%20to%20first%20align%0Adifferent%20%22styles%22%20of%20geospatial%20data%20to%20its%20matching%20images%20that%20point%20to%20the%0Asame%20location%20on%20the%20surface%20of%20the%20Earth.%20In%20this%20paper%2C%20we%20approach%20the%20image%0Aregistration%20as%20a%20two-step%20process%20of%20%281%29%20extracting%20geospatial%20contents%0Ainvariant%20to%20visual%20%28and%20any%20other%20non-content-related%29%20information%2C%20and%20%282%29%0Amatching%20the%20data%20based%20on%20such%20%28purely%29%20geospatial%20contents.%20We%20hypothesize%0Athat%20a%20combination%20of%20%24%5Cbeta%24-VAE-like%20architecture%20%5B2%5D%20and%20adversarial%0Atraining%20will%20achieve%20both%20the%20disentanglement%20of%20the%20geographic%20information%0Aand%20artistic%20styles%20and%20generation%20of%20new%20map%20tiles%20by%20composing%20the%20encoded%0Ageographic%20information%20with%20any%20artistic%20style.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApplication%2520of%2520Disentanglement%2520to%2520Map%2520Registration%2520Problem%26entry.906535625%3DHae%2520Jin%2520Song%2520and%2520Patrycja%2520Krawczuk%2520and%2520Po-Hsuan%2520Huang%26entry.1292438233%3D%2520%2520Geospatial%2520data%2520come%2520from%2520various%2520sources%252C%2520such%2520as%2520satellites%252C%2520aircraft%252C%2520and%250ALiDAR.%2520The%2520variability%2520of%2520the%2520source%2520is%2520not%2520limited%2520to%2520the%2520types%2520of%2520data%250Aacquisition%2520techniques%252C%2520as%2520we%2520have%2520maps%2520from%2520different%2520time%2520periods.%2520To%250Aincorporate%2520these%2520data%2520for%2520a%2520coherent%2520analysis%252C%2520it%2520is%2520essential%2520to%2520first%2520align%250Adifferent%2520%2522styles%2522%2520of%2520geospatial%2520data%2520to%2520its%2520matching%2520images%2520that%2520point%2520to%2520the%250Asame%2520location%2520on%2520the%2520surface%2520of%2520the%2520Earth.%2520In%2520this%2520paper%252C%2520we%2520approach%2520the%2520image%250Aregistration%2520as%2520a%2520two-step%2520process%2520of%2520%25281%2529%2520extracting%2520geospatial%2520contents%250Ainvariant%2520to%2520visual%2520%2528and%2520any%2520other%2520non-content-related%2529%2520information%252C%2520and%2520%25282%2529%250Amatching%2520the%2520data%2520based%2520on%2520such%2520%2528purely%2529%2520geospatial%2520contents.%2520We%2520hypothesize%250Athat%2520a%2520combination%2520of%2520%2524%255Cbeta%2524-VAE-like%2520architecture%2520%255B2%255D%2520and%2520adversarial%250Atraining%2520will%2520achieve%2520both%2520the%2520disentanglement%2520of%2520the%2520geographic%2520information%250Aand%2520artistic%2520styles%2520and%2520generation%2520of%2520new%2520map%2520tiles%2520by%2520composing%2520the%2520encoded%250Ageographic%2520information%2520with%2520any%2520artistic%2520style.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Application%20of%20Disentanglement%20to%20Map%20Registration%20Problem&entry.906535625=Hae%20Jin%20Song%20and%20Patrycja%20Krawczuk%20and%20Po-Hsuan%20Huang&entry.1292438233=%20%20Geospatial%20data%20come%20from%20various%20sources%2C%20such%20as%20satellites%2C%20aircraft%2C%20and%0ALiDAR.%20The%20variability%20of%20the%20source%20is%20not%20limited%20to%20the%20types%20of%20data%0Aacquisition%20techniques%2C%20as%20we%20have%20maps%20from%20different%20time%20periods.%20To%0Aincorporate%20these%20data%20for%20a%20coherent%20analysis%2C%20it%20is%20essential%20to%20first%20align%0Adifferent%20%22styles%22%20of%20geospatial%20data%20to%20its%20matching%20images%20that%20point%20to%20the%0Asame%20location%20on%20the%20surface%20of%20the%20Earth.%20In%20this%20paper%2C%20we%20approach%20the%20image%0Aregistration%20as%20a%20two-step%20process%20of%20%281%29%20extracting%20geospatial%20contents%0Ainvariant%20to%20visual%20%28and%20any%20other%20non-content-related%29%20information%2C%20and%20%282%29%0Amatching%20the%20data%20based%20on%20such%20%28purely%29%20geospatial%20contents.%20We%20hypothesize%0Athat%20a%20combination%20of%20%24%5Cbeta%24-VAE-like%20architecture%20%5B2%5D%20and%20adversarial%0Atraining%20will%20achieve%20both%20the%20disentanglement%20of%20the%20geographic%20information%0Aand%20artistic%20styles%20and%20generation%20of%20new%20map%20tiles%20by%20composing%20the%20encoded%0Ageographic%20information%20with%20any%20artistic%20style.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14152v1&entry.124074799=Read"},
{"title": "Efficient Model-Stealing Attacks Against Inductive Graph Neural Networks", "author": "Marcin Podhajski and Jan Dubi\u0144ski and Franziska Boenisch and Adam Dziedzic and Agnieszka Pregowska And Tomasz Michalak", "abstract": "  Graph Neural Networks (GNNs) are recognized as potent tools for processing\nreal-world data organized in graph structures. Especially inductive GNNs, which\nallow for the processing of graph-structured data without relying on predefined\ngraph structures, are becoming increasingly important in a wide range of\napplications. As such these networks become attractive targets for\nmodel-stealing attacks where an adversary seeks to replicate the functionality\nof the targeted network. Significant efforts have been devoted to developing\nmodel-stealing attacks that extract models trained on images and texts.\nHowever, little attention has been given to stealing GNNs trained on graph\ndata. This paper identifies a new method of performing unsupervised\nmodel-stealing attacks against inductive GNNs, utilizing graph contrastive\nlearning and spectral graph augmentations to efficiently extract information\nfrom the targeted model. The new type of attack is thoroughly evaluated on six\ndatasets and the results show that our approach outperforms the current\nstate-of-the-art by Shen et al. (2021). In particular, our attack surpasses the\nbaseline across all benchmarks, attaining superior fidelity and downstream\naccuracy of the stolen model while necessitating fewer queries directed toward\nthe target model.\n", "link": "http://arxiv.org/abs/2405.12295v3", "date": "2024-08-26", "relevancy": 2.5279, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5231}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5061}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4876}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Model-Stealing%20Attacks%20Against%20Inductive%20Graph%20Neural%20Networks&body=Title%3A%20Efficient%20Model-Stealing%20Attacks%20Against%20Inductive%20Graph%20Neural%20Networks%0AAuthor%3A%20Marcin%20Podhajski%20and%20Jan%20Dubi%C5%84ski%20and%20Franziska%20Boenisch%20and%20Adam%20Dziedzic%20and%20Agnieszka%20Pregowska%20And%20Tomasz%20Michalak%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20recognized%20as%20potent%20tools%20for%20processing%0Areal-world%20data%20organized%20in%20graph%20structures.%20Especially%20inductive%20GNNs%2C%20which%0Aallow%20for%20the%20processing%20of%20graph-structured%20data%20without%20relying%20on%20predefined%0Agraph%20structures%2C%20are%20becoming%20increasingly%20important%20in%20a%20wide%20range%20of%0Aapplications.%20As%20such%20these%20networks%20become%20attractive%20targets%20for%0Amodel-stealing%20attacks%20where%20an%20adversary%20seeks%20to%20replicate%20the%20functionality%0Aof%20the%20targeted%20network.%20Significant%20efforts%20have%20been%20devoted%20to%20developing%0Amodel-stealing%20attacks%20that%20extract%20models%20trained%20on%20images%20and%20texts.%0AHowever%2C%20little%20attention%20has%20been%20given%20to%20stealing%20GNNs%20trained%20on%20graph%0Adata.%20This%20paper%20identifies%20a%20new%20method%20of%20performing%20unsupervised%0Amodel-stealing%20attacks%20against%20inductive%20GNNs%2C%20utilizing%20graph%20contrastive%0Alearning%20and%20spectral%20graph%20augmentations%20to%20efficiently%20extract%20information%0Afrom%20the%20targeted%20model.%20The%20new%20type%20of%20attack%20is%20thoroughly%20evaluated%20on%20six%0Adatasets%20and%20the%20results%20show%20that%20our%20approach%20outperforms%20the%20current%0Astate-of-the-art%20by%20Shen%20et%20al.%20%282021%29.%20In%20particular%2C%20our%20attack%20surpasses%20the%0Abaseline%20across%20all%20benchmarks%2C%20attaining%20superior%20fidelity%20and%20downstream%0Aaccuracy%20of%20the%20stolen%20model%20while%20necessitating%20fewer%20queries%20directed%20toward%0Athe%20target%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.12295v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Model-Stealing%2520Attacks%2520Against%2520Inductive%2520Graph%2520Neural%2520Networks%26entry.906535625%3DMarcin%2520Podhajski%2520and%2520Jan%2520Dubi%25C5%2584ski%2520and%2520Franziska%2520Boenisch%2520and%2520Adam%2520Dziedzic%2520and%2520Agnieszka%2520Pregowska%2520And%2520Tomasz%2520Michalak%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520are%2520recognized%2520as%2520potent%2520tools%2520for%2520processing%250Areal-world%2520data%2520organized%2520in%2520graph%2520structures.%2520Especially%2520inductive%2520GNNs%252C%2520which%250Aallow%2520for%2520the%2520processing%2520of%2520graph-structured%2520data%2520without%2520relying%2520on%2520predefined%250Agraph%2520structures%252C%2520are%2520becoming%2520increasingly%2520important%2520in%2520a%2520wide%2520range%2520of%250Aapplications.%2520As%2520such%2520these%2520networks%2520become%2520attractive%2520targets%2520for%250Amodel-stealing%2520attacks%2520where%2520an%2520adversary%2520seeks%2520to%2520replicate%2520the%2520functionality%250Aof%2520the%2520targeted%2520network.%2520Significant%2520efforts%2520have%2520been%2520devoted%2520to%2520developing%250Amodel-stealing%2520attacks%2520that%2520extract%2520models%2520trained%2520on%2520images%2520and%2520texts.%250AHowever%252C%2520little%2520attention%2520has%2520been%2520given%2520to%2520stealing%2520GNNs%2520trained%2520on%2520graph%250Adata.%2520This%2520paper%2520identifies%2520a%2520new%2520method%2520of%2520performing%2520unsupervised%250Amodel-stealing%2520attacks%2520against%2520inductive%2520GNNs%252C%2520utilizing%2520graph%2520contrastive%250Alearning%2520and%2520spectral%2520graph%2520augmentations%2520to%2520efficiently%2520extract%2520information%250Afrom%2520the%2520targeted%2520model.%2520The%2520new%2520type%2520of%2520attack%2520is%2520thoroughly%2520evaluated%2520on%2520six%250Adatasets%2520and%2520the%2520results%2520show%2520that%2520our%2520approach%2520outperforms%2520the%2520current%250Astate-of-the-art%2520by%2520Shen%2520et%2520al.%2520%25282021%2529.%2520In%2520particular%252C%2520our%2520attack%2520surpasses%2520the%250Abaseline%2520across%2520all%2520benchmarks%252C%2520attaining%2520superior%2520fidelity%2520and%2520downstream%250Aaccuracy%2520of%2520the%2520stolen%2520model%2520while%2520necessitating%2520fewer%2520queries%2520directed%2520toward%250Athe%2520target%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.12295v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Model-Stealing%20Attacks%20Against%20Inductive%20Graph%20Neural%20Networks&entry.906535625=Marcin%20Podhajski%20and%20Jan%20Dubi%C5%84ski%20and%20Franziska%20Boenisch%20and%20Adam%20Dziedzic%20and%20Agnieszka%20Pregowska%20And%20Tomasz%20Michalak&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20are%20recognized%20as%20potent%20tools%20for%20processing%0Areal-world%20data%20organized%20in%20graph%20structures.%20Especially%20inductive%20GNNs%2C%20which%0Aallow%20for%20the%20processing%20of%20graph-structured%20data%20without%20relying%20on%20predefined%0Agraph%20structures%2C%20are%20becoming%20increasingly%20important%20in%20a%20wide%20range%20of%0Aapplications.%20As%20such%20these%20networks%20become%20attractive%20targets%20for%0Amodel-stealing%20attacks%20where%20an%20adversary%20seeks%20to%20replicate%20the%20functionality%0Aof%20the%20targeted%20network.%20Significant%20efforts%20have%20been%20devoted%20to%20developing%0Amodel-stealing%20attacks%20that%20extract%20models%20trained%20on%20images%20and%20texts.%0AHowever%2C%20little%20attention%20has%20been%20given%20to%20stealing%20GNNs%20trained%20on%20graph%0Adata.%20This%20paper%20identifies%20a%20new%20method%20of%20performing%20unsupervised%0Amodel-stealing%20attacks%20against%20inductive%20GNNs%2C%20utilizing%20graph%20contrastive%0Alearning%20and%20spectral%20graph%20augmentations%20to%20efficiently%20extract%20information%0Afrom%20the%20targeted%20model.%20The%20new%20type%20of%20attack%20is%20thoroughly%20evaluated%20on%20six%0Adatasets%20and%20the%20results%20show%20that%20our%20approach%20outperforms%20the%20current%0Astate-of-the-art%20by%20Shen%20et%20al.%20%282021%29.%20In%20particular%2C%20our%20attack%20surpasses%20the%0Abaseline%20across%20all%20benchmarks%2C%20attaining%20superior%20fidelity%20and%20downstream%0Aaccuracy%20of%20the%20stolen%20model%20while%20necessitating%20fewer%20queries%20directed%20toward%0Athe%20target%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.12295v3&entry.124074799=Read"},
{"title": "Satellite Sunroof: High-res Digital Surface Models and Roof Segmentation\n  for Global Solar Mapping", "author": "Vishal Batchu and Alex Wilson and Betty Peng and Carl Elkin and Umangi Jain and Christopher Van Arsdale and Ross Goroshin and Varun Gulshan", "abstract": "  The transition to renewable energy, particularly solar, is key to mitigating\nclimate change. Google's Solar API aids this transition by estimating solar\npotential from aerial imagery, but its impact is constrained by geographical\ncoverage. This paper proposes expanding the API's reach using satellite\nimagery, enabling global solar potential assessment. We tackle challenges\ninvolved in building a Digital Surface Model (DSM) and roof instance\nsegmentation from lower resolution and single oblique views using deep learning\nmodels. Our models, trained on aligned satellite and aerial datasets, produce\n25cm DSMs and roof segments. With ~1m DSM MAE on buildings, ~5deg roof pitch\nerror and ~56% IOU on roof segmentation, they significantly enhance the Solar\nAPI's potential to promote solar adoption.\n", "link": "http://arxiv.org/abs/2408.14400v1", "date": "2024-08-26", "relevancy": 2.5226, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5158}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5044}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4933}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Satellite%20Sunroof%3A%20High-res%20Digital%20Surface%20Models%20and%20Roof%20Segmentation%0A%20%20for%20Global%20Solar%20Mapping&body=Title%3A%20Satellite%20Sunroof%3A%20High-res%20Digital%20Surface%20Models%20and%20Roof%20Segmentation%0A%20%20for%20Global%20Solar%20Mapping%0AAuthor%3A%20Vishal%20Batchu%20and%20Alex%20Wilson%20and%20Betty%20Peng%20and%20Carl%20Elkin%20and%20Umangi%20Jain%20and%20Christopher%20Van%20Arsdale%20and%20Ross%20Goroshin%20and%20Varun%20Gulshan%0AAbstract%3A%20%20%20The%20transition%20to%20renewable%20energy%2C%20particularly%20solar%2C%20is%20key%20to%20mitigating%0Aclimate%20change.%20Google%27s%20Solar%20API%20aids%20this%20transition%20by%20estimating%20solar%0Apotential%20from%20aerial%20imagery%2C%20but%20its%20impact%20is%20constrained%20by%20geographical%0Acoverage.%20This%20paper%20proposes%20expanding%20the%20API%27s%20reach%20using%20satellite%0Aimagery%2C%20enabling%20global%20solar%20potential%20assessment.%20We%20tackle%20challenges%0Ainvolved%20in%20building%20a%20Digital%20Surface%20Model%20%28DSM%29%20and%20roof%20instance%0Asegmentation%20from%20lower%20resolution%20and%20single%20oblique%20views%20using%20deep%20learning%0Amodels.%20Our%20models%2C%20trained%20on%20aligned%20satellite%20and%20aerial%20datasets%2C%20produce%0A25cm%20DSMs%20and%20roof%20segments.%20With%20~1m%20DSM%20MAE%20on%20buildings%2C%20~5deg%20roof%20pitch%0Aerror%20and%20~56%25%20IOU%20on%20roof%20segmentation%2C%20they%20significantly%20enhance%20the%20Solar%0AAPI%27s%20potential%20to%20promote%20solar%20adoption.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14400v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSatellite%2520Sunroof%253A%2520High-res%2520Digital%2520Surface%2520Models%2520and%2520Roof%2520Segmentation%250A%2520%2520for%2520Global%2520Solar%2520Mapping%26entry.906535625%3DVishal%2520Batchu%2520and%2520Alex%2520Wilson%2520and%2520Betty%2520Peng%2520and%2520Carl%2520Elkin%2520and%2520Umangi%2520Jain%2520and%2520Christopher%2520Van%2520Arsdale%2520and%2520Ross%2520Goroshin%2520and%2520Varun%2520Gulshan%26entry.1292438233%3D%2520%2520The%2520transition%2520to%2520renewable%2520energy%252C%2520particularly%2520solar%252C%2520is%2520key%2520to%2520mitigating%250Aclimate%2520change.%2520Google%2527s%2520Solar%2520API%2520aids%2520this%2520transition%2520by%2520estimating%2520solar%250Apotential%2520from%2520aerial%2520imagery%252C%2520but%2520its%2520impact%2520is%2520constrained%2520by%2520geographical%250Acoverage.%2520This%2520paper%2520proposes%2520expanding%2520the%2520API%2527s%2520reach%2520using%2520satellite%250Aimagery%252C%2520enabling%2520global%2520solar%2520potential%2520assessment.%2520We%2520tackle%2520challenges%250Ainvolved%2520in%2520building%2520a%2520Digital%2520Surface%2520Model%2520%2528DSM%2529%2520and%2520roof%2520instance%250Asegmentation%2520from%2520lower%2520resolution%2520and%2520single%2520oblique%2520views%2520using%2520deep%2520learning%250Amodels.%2520Our%2520models%252C%2520trained%2520on%2520aligned%2520satellite%2520and%2520aerial%2520datasets%252C%2520produce%250A25cm%2520DSMs%2520and%2520roof%2520segments.%2520With%2520~1m%2520DSM%2520MAE%2520on%2520buildings%252C%2520~5deg%2520roof%2520pitch%250Aerror%2520and%2520~56%2525%2520IOU%2520on%2520roof%2520segmentation%252C%2520they%2520significantly%2520enhance%2520the%2520Solar%250AAPI%2527s%2520potential%2520to%2520promote%2520solar%2520adoption.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14400v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Satellite%20Sunroof%3A%20High-res%20Digital%20Surface%20Models%20and%20Roof%20Segmentation%0A%20%20for%20Global%20Solar%20Mapping&entry.906535625=Vishal%20Batchu%20and%20Alex%20Wilson%20and%20Betty%20Peng%20and%20Carl%20Elkin%20and%20Umangi%20Jain%20and%20Christopher%20Van%20Arsdale%20and%20Ross%20Goroshin%20and%20Varun%20Gulshan&entry.1292438233=%20%20The%20transition%20to%20renewable%20energy%2C%20particularly%20solar%2C%20is%20key%20to%20mitigating%0Aclimate%20change.%20Google%27s%20Solar%20API%20aids%20this%20transition%20by%20estimating%20solar%0Apotential%20from%20aerial%20imagery%2C%20but%20its%20impact%20is%20constrained%20by%20geographical%0Acoverage.%20This%20paper%20proposes%20expanding%20the%20API%27s%20reach%20using%20satellite%0Aimagery%2C%20enabling%20global%20solar%20potential%20assessment.%20We%20tackle%20challenges%0Ainvolved%20in%20building%20a%20Digital%20Surface%20Model%20%28DSM%29%20and%20roof%20instance%0Asegmentation%20from%20lower%20resolution%20and%20single%20oblique%20views%20using%20deep%20learning%0Amodels.%20Our%20models%2C%20trained%20on%20aligned%20satellite%20and%20aerial%20datasets%2C%20produce%0A25cm%20DSMs%20and%20roof%20segments.%20With%20~1m%20DSM%20MAE%20on%20buildings%2C%20~5deg%20roof%20pitch%0Aerror%20and%20~56%25%20IOU%20on%20roof%20segmentation%2C%20they%20significantly%20enhance%20the%20Solar%0AAPI%27s%20potential%20to%20promote%20solar%20adoption.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14400v1&entry.124074799=Read"},
{"title": "Interpretable Representation Learning of Cardiac MRI via Attribute\n  Regularization", "author": "Maxime Di Folco and Cosmin I. Bercea and Emily Chan and Julia A. Schnabel", "abstract": "  Interpretability is essential in medical imaging to ensure that clinicians\ncan comprehend and trust artificial intelligence models. Several approaches\nhave been recently considered to encode attributes in the latent space to\nenhance its interpretability. Notably, attribute regularization aims to encode\na set of attributes along the dimensions of a latent representation. However,\nthis approach is based on Variational AutoEncoder and suffers from blurry\nreconstruction. In this paper, we propose an Attributed-regularized Soft\nIntrospective Variational Autoencoder that combines attribute regularization of\nthe latent space within the framework of an adversarially trained variational\nautoencoder. We demonstrate on short-axis cardiac Magnetic Resonance images of\nthe UK Biobank the ability of the proposed method to address blurry\nreconstruction issues of variational autoencoder methods while preserving the\nlatent space interpretability.\n", "link": "http://arxiv.org/abs/2406.08282v3", "date": "2024-08-26", "relevancy": 2.5132, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5097}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5015}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Representation%20Learning%20of%20Cardiac%20MRI%20via%20Attribute%0A%20%20Regularization&body=Title%3A%20Interpretable%20Representation%20Learning%20of%20Cardiac%20MRI%20via%20Attribute%0A%20%20Regularization%0AAuthor%3A%20Maxime%20Di%20Folco%20and%20Cosmin%20I.%20Bercea%20and%20Emily%20Chan%20and%20Julia%20A.%20Schnabel%0AAbstract%3A%20%20%20Interpretability%20is%20essential%20in%20medical%20imaging%20to%20ensure%20that%20clinicians%0Acan%20comprehend%20and%20trust%20artificial%20intelligence%20models.%20Several%20approaches%0Ahave%20been%20recently%20considered%20to%20encode%20attributes%20in%20the%20latent%20space%20to%0Aenhance%20its%20interpretability.%20Notably%2C%20attribute%20regularization%20aims%20to%20encode%0Aa%20set%20of%20attributes%20along%20the%20dimensions%20of%20a%20latent%20representation.%20However%2C%0Athis%20approach%20is%20based%20on%20Variational%20AutoEncoder%20and%20suffers%20from%20blurry%0Areconstruction.%20In%20this%20paper%2C%20we%20propose%20an%20Attributed-regularized%20Soft%0AIntrospective%20Variational%20Autoencoder%20that%20combines%20attribute%20regularization%20of%0Athe%20latent%20space%20within%20the%20framework%20of%20an%20adversarially%20trained%20variational%0Aautoencoder.%20We%20demonstrate%20on%20short-axis%20cardiac%20Magnetic%20Resonance%20images%20of%0Athe%20UK%20Biobank%20the%20ability%20of%20the%20proposed%20method%20to%20address%20blurry%0Areconstruction%20issues%20of%20variational%20autoencoder%20methods%20while%20preserving%20the%0Alatent%20space%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08282v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Representation%2520Learning%2520of%2520Cardiac%2520MRI%2520via%2520Attribute%250A%2520%2520Regularization%26entry.906535625%3DMaxime%2520Di%2520Folco%2520and%2520Cosmin%2520I.%2520Bercea%2520and%2520Emily%2520Chan%2520and%2520Julia%2520A.%2520Schnabel%26entry.1292438233%3D%2520%2520Interpretability%2520is%2520essential%2520in%2520medical%2520imaging%2520to%2520ensure%2520that%2520clinicians%250Acan%2520comprehend%2520and%2520trust%2520artificial%2520intelligence%2520models.%2520Several%2520approaches%250Ahave%2520been%2520recently%2520considered%2520to%2520encode%2520attributes%2520in%2520the%2520latent%2520space%2520to%250Aenhance%2520its%2520interpretability.%2520Notably%252C%2520attribute%2520regularization%2520aims%2520to%2520encode%250Aa%2520set%2520of%2520attributes%2520along%2520the%2520dimensions%2520of%2520a%2520latent%2520representation.%2520However%252C%250Athis%2520approach%2520is%2520based%2520on%2520Variational%2520AutoEncoder%2520and%2520suffers%2520from%2520blurry%250Areconstruction.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520Attributed-regularized%2520Soft%250AIntrospective%2520Variational%2520Autoencoder%2520that%2520combines%2520attribute%2520regularization%2520of%250Athe%2520latent%2520space%2520within%2520the%2520framework%2520of%2520an%2520adversarially%2520trained%2520variational%250Aautoencoder.%2520We%2520demonstrate%2520on%2520short-axis%2520cardiac%2520Magnetic%2520Resonance%2520images%2520of%250Athe%2520UK%2520Biobank%2520the%2520ability%2520of%2520the%2520proposed%2520method%2520to%2520address%2520blurry%250Areconstruction%2520issues%2520of%2520variational%2520autoencoder%2520methods%2520while%2520preserving%2520the%250Alatent%2520space%2520interpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08282v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Representation%20Learning%20of%20Cardiac%20MRI%20via%20Attribute%0A%20%20Regularization&entry.906535625=Maxime%20Di%20Folco%20and%20Cosmin%20I.%20Bercea%20and%20Emily%20Chan%20and%20Julia%20A.%20Schnabel&entry.1292438233=%20%20Interpretability%20is%20essential%20in%20medical%20imaging%20to%20ensure%20that%20clinicians%0Acan%20comprehend%20and%20trust%20artificial%20intelligence%20models.%20Several%20approaches%0Ahave%20been%20recently%20considered%20to%20encode%20attributes%20in%20the%20latent%20space%20to%0Aenhance%20its%20interpretability.%20Notably%2C%20attribute%20regularization%20aims%20to%20encode%0Aa%20set%20of%20attributes%20along%20the%20dimensions%20of%20a%20latent%20representation.%20However%2C%0Athis%20approach%20is%20based%20on%20Variational%20AutoEncoder%20and%20suffers%20from%20blurry%0Areconstruction.%20In%20this%20paper%2C%20we%20propose%20an%20Attributed-regularized%20Soft%0AIntrospective%20Variational%20Autoencoder%20that%20combines%20attribute%20regularization%20of%0Athe%20latent%20space%20within%20the%20framework%20of%20an%20adversarially%20trained%20variational%0Aautoencoder.%20We%20demonstrate%20on%20short-axis%20cardiac%20Magnetic%20Resonance%20images%20of%0Athe%20UK%20Biobank%20the%20ability%20of%20the%20proposed%20method%20to%20address%20blurry%0Areconstruction%20issues%20of%20variational%20autoencoder%20methods%20while%20preserving%20the%0Alatent%20space%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08282v3&entry.124074799=Read"},
{"title": "TC-PDM: Temporally Consistent Patch Diffusion Models for\n  Infrared-to-Visible Video Translation", "author": "Anh-Dzung Doan and Vu Minh Hieu Phan and Surabhi Gupta and Markus Wagner and Tat-Jun Chin and Ian Reid", "abstract": "  Infrared imaging offers resilience against changing lighting conditions by\ncapturing object temperatures. Yet, in few scenarios, its lack of visual\ndetails compared to daytime visible images, poses a significant challenge for\nhuman and machine interpretation. This paper proposes a novel diffusion method,\ndubbed Temporally Consistent Patch Diffusion Models (TC-DPM), for\ninfrared-to-visible video translation. Our method, extending the Patch\nDiffusion Model, consists of two key components. Firstly, we propose a\nsemantic-guided denoising, leveraging the strong representations of\nfoundational models. As such, our method faithfully preserves the semantic\nstructure of generated visible images. Secondly, we propose a novel temporal\nblending module to guide the denoising trajectory, ensuring the temporal\nconsistency between consecutive frames. Experiment shows that TC-PDM\noutperforms state-of-the-art methods by 35.3% in FVD for infrared-to-visible\nvideo translation and by 6.1% in AP50 for day-to-night object detection. Our\ncode is publicly available at https://github.com/dzungdoan6/tc-pdm\n", "link": "http://arxiv.org/abs/2408.14227v1", "date": "2024-08-26", "relevancy": 2.5013, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6445}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6267}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TC-PDM%3A%20Temporally%20Consistent%20Patch%20Diffusion%20Models%20for%0A%20%20Infrared-to-Visible%20Video%20Translation&body=Title%3A%20TC-PDM%3A%20Temporally%20Consistent%20Patch%20Diffusion%20Models%20for%0A%20%20Infrared-to-Visible%20Video%20Translation%0AAuthor%3A%20Anh-Dzung%20Doan%20and%20Vu%20Minh%20Hieu%20Phan%20and%20Surabhi%20Gupta%20and%20Markus%20Wagner%20and%20Tat-Jun%20Chin%20and%20Ian%20Reid%0AAbstract%3A%20%20%20Infrared%20imaging%20offers%20resilience%20against%20changing%20lighting%20conditions%20by%0Acapturing%20object%20temperatures.%20Yet%2C%20in%20few%20scenarios%2C%20its%20lack%20of%20visual%0Adetails%20compared%20to%20daytime%20visible%20images%2C%20poses%20a%20significant%20challenge%20for%0Ahuman%20and%20machine%20interpretation.%20This%20paper%20proposes%20a%20novel%20diffusion%20method%2C%0Adubbed%20Temporally%20Consistent%20Patch%20Diffusion%20Models%20%28TC-DPM%29%2C%20for%0Ainfrared-to-visible%20video%20translation.%20Our%20method%2C%20extending%20the%20Patch%0ADiffusion%20Model%2C%20consists%20of%20two%20key%20components.%20Firstly%2C%20we%20propose%20a%0Asemantic-guided%20denoising%2C%20leveraging%20the%20strong%20representations%20of%0Afoundational%20models.%20As%20such%2C%20our%20method%20faithfully%20preserves%20the%20semantic%0Astructure%20of%20generated%20visible%20images.%20Secondly%2C%20we%20propose%20a%20novel%20temporal%0Ablending%20module%20to%20guide%20the%20denoising%20trajectory%2C%20ensuring%20the%20temporal%0Aconsistency%20between%20consecutive%20frames.%20Experiment%20shows%20that%20TC-PDM%0Aoutperforms%20state-of-the-art%20methods%20by%2035.3%25%20in%20FVD%20for%20infrared-to-visible%0Avideo%20translation%20and%20by%206.1%25%20in%20AP50%20for%20day-to-night%20object%20detection.%20Our%0Acode%20is%20publicly%20available%20at%20https%3A//github.com/dzungdoan6/tc-pdm%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14227v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTC-PDM%253A%2520Temporally%2520Consistent%2520Patch%2520Diffusion%2520Models%2520for%250A%2520%2520Infrared-to-Visible%2520Video%2520Translation%26entry.906535625%3DAnh-Dzung%2520Doan%2520and%2520Vu%2520Minh%2520Hieu%2520Phan%2520and%2520Surabhi%2520Gupta%2520and%2520Markus%2520Wagner%2520and%2520Tat-Jun%2520Chin%2520and%2520Ian%2520Reid%26entry.1292438233%3D%2520%2520Infrared%2520imaging%2520offers%2520resilience%2520against%2520changing%2520lighting%2520conditions%2520by%250Acapturing%2520object%2520temperatures.%2520Yet%252C%2520in%2520few%2520scenarios%252C%2520its%2520lack%2520of%2520visual%250Adetails%2520compared%2520to%2520daytime%2520visible%2520images%252C%2520poses%2520a%2520significant%2520challenge%2520for%250Ahuman%2520and%2520machine%2520interpretation.%2520This%2520paper%2520proposes%2520a%2520novel%2520diffusion%2520method%252C%250Adubbed%2520Temporally%2520Consistent%2520Patch%2520Diffusion%2520Models%2520%2528TC-DPM%2529%252C%2520for%250Ainfrared-to-visible%2520video%2520translation.%2520Our%2520method%252C%2520extending%2520the%2520Patch%250ADiffusion%2520Model%252C%2520consists%2520of%2520two%2520key%2520components.%2520Firstly%252C%2520we%2520propose%2520a%250Asemantic-guided%2520denoising%252C%2520leveraging%2520the%2520strong%2520representations%2520of%250Afoundational%2520models.%2520As%2520such%252C%2520our%2520method%2520faithfully%2520preserves%2520the%2520semantic%250Astructure%2520of%2520generated%2520visible%2520images.%2520Secondly%252C%2520we%2520propose%2520a%2520novel%2520temporal%250Ablending%2520module%2520to%2520guide%2520the%2520denoising%2520trajectory%252C%2520ensuring%2520the%2520temporal%250Aconsistency%2520between%2520consecutive%2520frames.%2520Experiment%2520shows%2520that%2520TC-PDM%250Aoutperforms%2520state-of-the-art%2520methods%2520by%252035.3%2525%2520in%2520FVD%2520for%2520infrared-to-visible%250Avideo%2520translation%2520and%2520by%25206.1%2525%2520in%2520AP50%2520for%2520day-to-night%2520object%2520detection.%2520Our%250Acode%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/dzungdoan6/tc-pdm%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14227v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TC-PDM%3A%20Temporally%20Consistent%20Patch%20Diffusion%20Models%20for%0A%20%20Infrared-to-Visible%20Video%20Translation&entry.906535625=Anh-Dzung%20Doan%20and%20Vu%20Minh%20Hieu%20Phan%20and%20Surabhi%20Gupta%20and%20Markus%20Wagner%20and%20Tat-Jun%20Chin%20and%20Ian%20Reid&entry.1292438233=%20%20Infrared%20imaging%20offers%20resilience%20against%20changing%20lighting%20conditions%20by%0Acapturing%20object%20temperatures.%20Yet%2C%20in%20few%20scenarios%2C%20its%20lack%20of%20visual%0Adetails%20compared%20to%20daytime%20visible%20images%2C%20poses%20a%20significant%20challenge%20for%0Ahuman%20and%20machine%20interpretation.%20This%20paper%20proposes%20a%20novel%20diffusion%20method%2C%0Adubbed%20Temporally%20Consistent%20Patch%20Diffusion%20Models%20%28TC-DPM%29%2C%20for%0Ainfrared-to-visible%20video%20translation.%20Our%20method%2C%20extending%20the%20Patch%0ADiffusion%20Model%2C%20consists%20of%20two%20key%20components.%20Firstly%2C%20we%20propose%20a%0Asemantic-guided%20denoising%2C%20leveraging%20the%20strong%20representations%20of%0Afoundational%20models.%20As%20such%2C%20our%20method%20faithfully%20preserves%20the%20semantic%0Astructure%20of%20generated%20visible%20images.%20Secondly%2C%20we%20propose%20a%20novel%20temporal%0Ablending%20module%20to%20guide%20the%20denoising%20trajectory%2C%20ensuring%20the%20temporal%0Aconsistency%20between%20consecutive%20frames.%20Experiment%20shows%20that%20TC-PDM%0Aoutperforms%20state-of-the-art%20methods%20by%2035.3%25%20in%20FVD%20for%20infrared-to-visible%0Avideo%20translation%20and%20by%206.1%25%20in%20AP50%20for%20day-to-night%20object%20detection.%20Our%0Acode%20is%20publicly%20available%20at%20https%3A//github.com/dzungdoan6/tc-pdm%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14227v1&entry.124074799=Read"},
{"title": "HGPROMPT: Bridging Homogeneous and Heterogeneous Graphs for Few-shot\n  Prompt Learning", "author": "Xingtong Yu and Yuan Fang and Zemin Liu and Xinming Zhang", "abstract": "  Graph neural networks (GNNs) and heterogeneous graph neural networks (HGNNs)\nare prominent techniques for homogeneous and heterogeneous graph representation\nlearning, yet their performance in an end-to-end supervised framework greatly\ndepends on the availability of task-specific supervision. To reduce the\nlabeling cost, pre-training on self-supervised pretext tasks has become a\npopular paradigm,but there is often a gap between the pre-trained model and\ndownstream tasks, stemming from the divergence in their objectives. To bridge\nthe gap, prompt learning has risen as a promising direction especially in\nfew-shot settings, without the need to fully fine-tune the pre-trained model.\nWhile there has been some early exploration of prompt-based learning on graphs,\nthey primarily deal with homogeneous graphs, ignoring the heterogeneous graphs\nthat are prevalent in downstream applications. In this paper, we propose\nHGPROMPT, a novel pre-training and prompting framework to unify not only\npre-training and downstream tasks but also homogeneous and heterogeneous graphs\nvia a dual-template design. Moreover, we propose dual-prompt in HGPROMPT to\nassist a downstream task in locating the most relevant prior to bridge the gaps\ncaused by not only feature variations but also heterogeneity differences across\ntasks. Finally, we thoroughly evaluate and analyze HGPROMPT through extensive\nexperiments on three public datasets.\n", "link": "http://arxiv.org/abs/2312.01878v8", "date": "2024-08-26", "relevancy": 2.4915, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5285}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4844}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HGPROMPT%3A%20Bridging%20Homogeneous%20and%20Heterogeneous%20Graphs%20for%20Few-shot%0A%20%20Prompt%20Learning&body=Title%3A%20HGPROMPT%3A%20Bridging%20Homogeneous%20and%20Heterogeneous%20Graphs%20for%20Few-shot%0A%20%20Prompt%20Learning%0AAuthor%3A%20Xingtong%20Yu%20and%20Yuan%20Fang%20and%20Zemin%20Liu%20and%20Xinming%20Zhang%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20and%20heterogeneous%20graph%20neural%20networks%20%28HGNNs%29%0Aare%20prominent%20techniques%20for%20homogeneous%20and%20heterogeneous%20graph%20representation%0Alearning%2C%20yet%20their%20performance%20in%20an%20end-to-end%20supervised%20framework%20greatly%0Adepends%20on%20the%20availability%20of%20task-specific%20supervision.%20To%20reduce%20the%0Alabeling%20cost%2C%20pre-training%20on%20self-supervised%20pretext%20tasks%20has%20become%20a%0Apopular%20paradigm%2Cbut%20there%20is%20often%20a%20gap%20between%20the%20pre-trained%20model%20and%0Adownstream%20tasks%2C%20stemming%20from%20the%20divergence%20in%20their%20objectives.%20To%20bridge%0Athe%20gap%2C%20prompt%20learning%20has%20risen%20as%20a%20promising%20direction%20especially%20in%0Afew-shot%20settings%2C%20without%20the%20need%20to%20fully%20fine-tune%20the%20pre-trained%20model.%0AWhile%20there%20has%20been%20some%20early%20exploration%20of%20prompt-based%20learning%20on%20graphs%2C%0Athey%20primarily%20deal%20with%20homogeneous%20graphs%2C%20ignoring%20the%20heterogeneous%20graphs%0Athat%20are%20prevalent%20in%20downstream%20applications.%20In%20this%20paper%2C%20we%20propose%0AHGPROMPT%2C%20a%20novel%20pre-training%20and%20prompting%20framework%20to%20unify%20not%20only%0Apre-training%20and%20downstream%20tasks%20but%20also%20homogeneous%20and%20heterogeneous%20graphs%0Avia%20a%20dual-template%20design.%20Moreover%2C%20we%20propose%20dual-prompt%20in%20HGPROMPT%20to%0Aassist%20a%20downstream%20task%20in%20locating%20the%20most%20relevant%20prior%20to%20bridge%20the%20gaps%0Acaused%20by%20not%20only%20feature%20variations%20but%20also%20heterogeneity%20differences%20across%0Atasks.%20Finally%2C%20we%20thoroughly%20evaluate%20and%20analyze%20HGPROMPT%20through%20extensive%0Aexperiments%20on%20three%20public%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.01878v8%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHGPROMPT%253A%2520Bridging%2520Homogeneous%2520and%2520Heterogeneous%2520Graphs%2520for%2520Few-shot%250A%2520%2520Prompt%2520Learning%26entry.906535625%3DXingtong%2520Yu%2520and%2520Yuan%2520Fang%2520and%2520Zemin%2520Liu%2520and%2520Xinming%2520Zhang%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520and%2520heterogeneous%2520graph%2520neural%2520networks%2520%2528HGNNs%2529%250Aare%2520prominent%2520techniques%2520for%2520homogeneous%2520and%2520heterogeneous%2520graph%2520representation%250Alearning%252C%2520yet%2520their%2520performance%2520in%2520an%2520end-to-end%2520supervised%2520framework%2520greatly%250Adepends%2520on%2520the%2520availability%2520of%2520task-specific%2520supervision.%2520To%2520reduce%2520the%250Alabeling%2520cost%252C%2520pre-training%2520on%2520self-supervised%2520pretext%2520tasks%2520has%2520become%2520a%250Apopular%2520paradigm%252Cbut%2520there%2520is%2520often%2520a%2520gap%2520between%2520the%2520pre-trained%2520model%2520and%250Adownstream%2520tasks%252C%2520stemming%2520from%2520the%2520divergence%2520in%2520their%2520objectives.%2520To%2520bridge%250Athe%2520gap%252C%2520prompt%2520learning%2520has%2520risen%2520as%2520a%2520promising%2520direction%2520especially%2520in%250Afew-shot%2520settings%252C%2520without%2520the%2520need%2520to%2520fully%2520fine-tune%2520the%2520pre-trained%2520model.%250AWhile%2520there%2520has%2520been%2520some%2520early%2520exploration%2520of%2520prompt-based%2520learning%2520on%2520graphs%252C%250Athey%2520primarily%2520deal%2520with%2520homogeneous%2520graphs%252C%2520ignoring%2520the%2520heterogeneous%2520graphs%250Athat%2520are%2520prevalent%2520in%2520downstream%2520applications.%2520In%2520this%2520paper%252C%2520we%2520propose%250AHGPROMPT%252C%2520a%2520novel%2520pre-training%2520and%2520prompting%2520framework%2520to%2520unify%2520not%2520only%250Apre-training%2520and%2520downstream%2520tasks%2520but%2520also%2520homogeneous%2520and%2520heterogeneous%2520graphs%250Avia%2520a%2520dual-template%2520design.%2520Moreover%252C%2520we%2520propose%2520dual-prompt%2520in%2520HGPROMPT%2520to%250Aassist%2520a%2520downstream%2520task%2520in%2520locating%2520the%2520most%2520relevant%2520prior%2520to%2520bridge%2520the%2520gaps%250Acaused%2520by%2520not%2520only%2520feature%2520variations%2520but%2520also%2520heterogeneity%2520differences%2520across%250Atasks.%2520Finally%252C%2520we%2520thoroughly%2520evaluate%2520and%2520analyze%2520HGPROMPT%2520through%2520extensive%250Aexperiments%2520on%2520three%2520public%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.01878v8%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HGPROMPT%3A%20Bridging%20Homogeneous%20and%20Heterogeneous%20Graphs%20for%20Few-shot%0A%20%20Prompt%20Learning&entry.906535625=Xingtong%20Yu%20and%20Yuan%20Fang%20and%20Zemin%20Liu%20and%20Xinming%20Zhang&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20and%20heterogeneous%20graph%20neural%20networks%20%28HGNNs%29%0Aare%20prominent%20techniques%20for%20homogeneous%20and%20heterogeneous%20graph%20representation%0Alearning%2C%20yet%20their%20performance%20in%20an%20end-to-end%20supervised%20framework%20greatly%0Adepends%20on%20the%20availability%20of%20task-specific%20supervision.%20To%20reduce%20the%0Alabeling%20cost%2C%20pre-training%20on%20self-supervised%20pretext%20tasks%20has%20become%20a%0Apopular%20paradigm%2Cbut%20there%20is%20often%20a%20gap%20between%20the%20pre-trained%20model%20and%0Adownstream%20tasks%2C%20stemming%20from%20the%20divergence%20in%20their%20objectives.%20To%20bridge%0Athe%20gap%2C%20prompt%20learning%20has%20risen%20as%20a%20promising%20direction%20especially%20in%0Afew-shot%20settings%2C%20without%20the%20need%20to%20fully%20fine-tune%20the%20pre-trained%20model.%0AWhile%20there%20has%20been%20some%20early%20exploration%20of%20prompt-based%20learning%20on%20graphs%2C%0Athey%20primarily%20deal%20with%20homogeneous%20graphs%2C%20ignoring%20the%20heterogeneous%20graphs%0Athat%20are%20prevalent%20in%20downstream%20applications.%20In%20this%20paper%2C%20we%20propose%0AHGPROMPT%2C%20a%20novel%20pre-training%20and%20prompting%20framework%20to%20unify%20not%20only%0Apre-training%20and%20downstream%20tasks%20but%20also%20homogeneous%20and%20heterogeneous%20graphs%0Avia%20a%20dual-template%20design.%20Moreover%2C%20we%20propose%20dual-prompt%20in%20HGPROMPT%20to%0Aassist%20a%20downstream%20task%20in%20locating%20the%20most%20relevant%20prior%20to%20bridge%20the%20gaps%0Acaused%20by%20not%20only%20feature%20variations%20but%20also%20heterogeneity%20differences%20across%0Atasks.%20Finally%2C%20we%20thoroughly%20evaluate%20and%20analyze%20HGPROMPT%20through%20extensive%0Aexperiments%20on%20three%20public%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.01878v8&entry.124074799=Read"},
{"title": "Uncertainties of Latent Representations in Computer Vision", "author": "Michael Kirchhof", "abstract": "  Uncertainty quantification is a key pillar of trustworthy machine learning.\nIt enables safe reactions under unsafe inputs, like predicting only when the\nmachine learning model detects sufficient evidence, discarding anomalous data,\nor emitting warnings when an error is likely to be inbound. This is\nparticularly crucial in safety-critical areas like medical image classification\nor self-driving cars. Despite the plethora of proposed uncertainty\nquantification methods achieving increasingly higher scores on performance\nbenchmarks, uncertainty estimates are often shied away from in practice. Many\nmachine learning projects start from pretrained latent representations that\ncome without uncertainty estimates. Uncertainties would need to be trained by\npractitioners on their own, which is notoriously difficult and\nresource-intense.\n  This thesis makes uncertainty estimates easily accessible by adding them to\nthe latent representation vectors of pretrained computer vision models. Besides\nproposing approaches rooted in probability and decision theory, such as\nMonte-Carlo InfoNCE (MCInfoNCE) and loss prediction, we delve into both\ntheoretical and empirical questions. We show that these unobservable\nuncertainties about unobservable latent representations are indeed provably\ncorrect. We also provide an uncertainty-aware representation learning (URL)\nbenchmark to compare these unobservables against observable ground-truths.\nFinally, we compile our findings to pretrain lightweight representation\nuncertainties on large-scale computer vision models that transfer to unseen\ndatasets in a zero-shot manner.\n  Our findings do not only advance the current theoretical understanding of\nuncertainties over latent variables, but also facilitate the access to\nuncertainty quantification for future researchers inside and outside the field,\nenabling straightforward but trustworthy machine learning.\n", "link": "http://arxiv.org/abs/2408.14281v1", "date": "2024-08-26", "relevancy": 2.4725, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6363}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.63}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainties%20of%20Latent%20Representations%20in%20Computer%20Vision&body=Title%3A%20Uncertainties%20of%20Latent%20Representations%20in%20Computer%20Vision%0AAuthor%3A%20Michael%20Kirchhof%0AAbstract%3A%20%20%20Uncertainty%20quantification%20is%20a%20key%20pillar%20of%20trustworthy%20machine%20learning.%0AIt%20enables%20safe%20reactions%20under%20unsafe%20inputs%2C%20like%20predicting%20only%20when%20the%0Amachine%20learning%20model%20detects%20sufficient%20evidence%2C%20discarding%20anomalous%20data%2C%0Aor%20emitting%20warnings%20when%20an%20error%20is%20likely%20to%20be%20inbound.%20This%20is%0Aparticularly%20crucial%20in%20safety-critical%20areas%20like%20medical%20image%20classification%0Aor%20self-driving%20cars.%20Despite%20the%20plethora%20of%20proposed%20uncertainty%0Aquantification%20methods%20achieving%20increasingly%20higher%20scores%20on%20performance%0Abenchmarks%2C%20uncertainty%20estimates%20are%20often%20shied%20away%20from%20in%20practice.%20Many%0Amachine%20learning%20projects%20start%20from%20pretrained%20latent%20representations%20that%0Acome%20without%20uncertainty%20estimates.%20Uncertainties%20would%20need%20to%20be%20trained%20by%0Apractitioners%20on%20their%20own%2C%20which%20is%20notoriously%20difficult%20and%0Aresource-intense.%0A%20%20This%20thesis%20makes%20uncertainty%20estimates%20easily%20accessible%20by%20adding%20them%20to%0Athe%20latent%20representation%20vectors%20of%20pretrained%20computer%20vision%20models.%20Besides%0Aproposing%20approaches%20rooted%20in%20probability%20and%20decision%20theory%2C%20such%20as%0AMonte-Carlo%20InfoNCE%20%28MCInfoNCE%29%20and%20loss%20prediction%2C%20we%20delve%20into%20both%0Atheoretical%20and%20empirical%20questions.%20We%20show%20that%20these%20unobservable%0Auncertainties%20about%20unobservable%20latent%20representations%20are%20indeed%20provably%0Acorrect.%20We%20also%20provide%20an%20uncertainty-aware%20representation%20learning%20%28URL%29%0Abenchmark%20to%20compare%20these%20unobservables%20against%20observable%20ground-truths.%0AFinally%2C%20we%20compile%20our%20findings%20to%20pretrain%20lightweight%20representation%0Auncertainties%20on%20large-scale%20computer%20vision%20models%20that%20transfer%20to%20unseen%0Adatasets%20in%20a%20zero-shot%20manner.%0A%20%20Our%20findings%20do%20not%20only%20advance%20the%20current%20theoretical%20understanding%20of%0Auncertainties%20over%20latent%20variables%2C%20but%20also%20facilitate%20the%20access%20to%0Auncertainty%20quantification%20for%20future%20researchers%20inside%20and%20outside%20the%20field%2C%0Aenabling%20straightforward%20but%20trustworthy%20machine%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14281v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainties%2520of%2520Latent%2520Representations%2520in%2520Computer%2520Vision%26entry.906535625%3DMichael%2520Kirchhof%26entry.1292438233%3D%2520%2520Uncertainty%2520quantification%2520is%2520a%2520key%2520pillar%2520of%2520trustworthy%2520machine%2520learning.%250AIt%2520enables%2520safe%2520reactions%2520under%2520unsafe%2520inputs%252C%2520like%2520predicting%2520only%2520when%2520the%250Amachine%2520learning%2520model%2520detects%2520sufficient%2520evidence%252C%2520discarding%2520anomalous%2520data%252C%250Aor%2520emitting%2520warnings%2520when%2520an%2520error%2520is%2520likely%2520to%2520be%2520inbound.%2520This%2520is%250Aparticularly%2520crucial%2520in%2520safety-critical%2520areas%2520like%2520medical%2520image%2520classification%250Aor%2520self-driving%2520cars.%2520Despite%2520the%2520plethora%2520of%2520proposed%2520uncertainty%250Aquantification%2520methods%2520achieving%2520increasingly%2520higher%2520scores%2520on%2520performance%250Abenchmarks%252C%2520uncertainty%2520estimates%2520are%2520often%2520shied%2520away%2520from%2520in%2520practice.%2520Many%250Amachine%2520learning%2520projects%2520start%2520from%2520pretrained%2520latent%2520representations%2520that%250Acome%2520without%2520uncertainty%2520estimates.%2520Uncertainties%2520would%2520need%2520to%2520be%2520trained%2520by%250Apractitioners%2520on%2520their%2520own%252C%2520which%2520is%2520notoriously%2520difficult%2520and%250Aresource-intense.%250A%2520%2520This%2520thesis%2520makes%2520uncertainty%2520estimates%2520easily%2520accessible%2520by%2520adding%2520them%2520to%250Athe%2520latent%2520representation%2520vectors%2520of%2520pretrained%2520computer%2520vision%2520models.%2520Besides%250Aproposing%2520approaches%2520rooted%2520in%2520probability%2520and%2520decision%2520theory%252C%2520such%2520as%250AMonte-Carlo%2520InfoNCE%2520%2528MCInfoNCE%2529%2520and%2520loss%2520prediction%252C%2520we%2520delve%2520into%2520both%250Atheoretical%2520and%2520empirical%2520questions.%2520We%2520show%2520that%2520these%2520unobservable%250Auncertainties%2520about%2520unobservable%2520latent%2520representations%2520are%2520indeed%2520provably%250Acorrect.%2520We%2520also%2520provide%2520an%2520uncertainty-aware%2520representation%2520learning%2520%2528URL%2529%250Abenchmark%2520to%2520compare%2520these%2520unobservables%2520against%2520observable%2520ground-truths.%250AFinally%252C%2520we%2520compile%2520our%2520findings%2520to%2520pretrain%2520lightweight%2520representation%250Auncertainties%2520on%2520large-scale%2520computer%2520vision%2520models%2520that%2520transfer%2520to%2520unseen%250Adatasets%2520in%2520a%2520zero-shot%2520manner.%250A%2520%2520Our%2520findings%2520do%2520not%2520only%2520advance%2520the%2520current%2520theoretical%2520understanding%2520of%250Auncertainties%2520over%2520latent%2520variables%252C%2520but%2520also%2520facilitate%2520the%2520access%2520to%250Auncertainty%2520quantification%2520for%2520future%2520researchers%2520inside%2520and%2520outside%2520the%2520field%252C%250Aenabling%2520straightforward%2520but%2520trustworthy%2520machine%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14281v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainties%20of%20Latent%20Representations%20in%20Computer%20Vision&entry.906535625=Michael%20Kirchhof&entry.1292438233=%20%20Uncertainty%20quantification%20is%20a%20key%20pillar%20of%20trustworthy%20machine%20learning.%0AIt%20enables%20safe%20reactions%20under%20unsafe%20inputs%2C%20like%20predicting%20only%20when%20the%0Amachine%20learning%20model%20detects%20sufficient%20evidence%2C%20discarding%20anomalous%20data%2C%0Aor%20emitting%20warnings%20when%20an%20error%20is%20likely%20to%20be%20inbound.%20This%20is%0Aparticularly%20crucial%20in%20safety-critical%20areas%20like%20medical%20image%20classification%0Aor%20self-driving%20cars.%20Despite%20the%20plethora%20of%20proposed%20uncertainty%0Aquantification%20methods%20achieving%20increasingly%20higher%20scores%20on%20performance%0Abenchmarks%2C%20uncertainty%20estimates%20are%20often%20shied%20away%20from%20in%20practice.%20Many%0Amachine%20learning%20projects%20start%20from%20pretrained%20latent%20representations%20that%0Acome%20without%20uncertainty%20estimates.%20Uncertainties%20would%20need%20to%20be%20trained%20by%0Apractitioners%20on%20their%20own%2C%20which%20is%20notoriously%20difficult%20and%0Aresource-intense.%0A%20%20This%20thesis%20makes%20uncertainty%20estimates%20easily%20accessible%20by%20adding%20them%20to%0Athe%20latent%20representation%20vectors%20of%20pretrained%20computer%20vision%20models.%20Besides%0Aproposing%20approaches%20rooted%20in%20probability%20and%20decision%20theory%2C%20such%20as%0AMonte-Carlo%20InfoNCE%20%28MCInfoNCE%29%20and%20loss%20prediction%2C%20we%20delve%20into%20both%0Atheoretical%20and%20empirical%20questions.%20We%20show%20that%20these%20unobservable%0Auncertainties%20about%20unobservable%20latent%20representations%20are%20indeed%20provably%0Acorrect.%20We%20also%20provide%20an%20uncertainty-aware%20representation%20learning%20%28URL%29%0Abenchmark%20to%20compare%20these%20unobservables%20against%20observable%20ground-truths.%0AFinally%2C%20we%20compile%20our%20findings%20to%20pretrain%20lightweight%20representation%0Auncertainties%20on%20large-scale%20computer%20vision%20models%20that%20transfer%20to%20unseen%0Adatasets%20in%20a%20zero-shot%20manner.%0A%20%20Our%20findings%20do%20not%20only%20advance%20the%20current%20theoretical%20understanding%20of%0Auncertainties%20over%20latent%20variables%2C%20but%20also%20facilitate%20the%20access%20to%0Auncertainty%20quantification%20for%20future%20researchers%20inside%20and%20outside%20the%20field%2C%0Aenabling%20straightforward%20but%20trustworthy%20machine%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14281v1&entry.124074799=Read"},
{"title": "Gallery-Aware Uncertainty Estimation For Open-Set Face Recognition", "author": "Leonid Erlygin and Alexey Zaytsev", "abstract": "  Accurately estimating image quality and model robustness improvement are\ncritical challenges in unconstrained face recognition, which can be addressed\nthrough uncertainty estimation via probabilistic face embeddings. Previous\nresearch mainly focused on uncertainty estimation in face verification, leaving\nthe open-set face recognition task underexplored. In open-set face recognition,\none seeks to classify an image, which could also be unknown. Here, the low\nvariance of probabilistic embedding does not imply a low error probability: an\nimage embedding could be close to several classes in a gallery, thus yielding\nhigh uncertainty. We propose a method aware of two sources of ambiguity in the\nopen-set recognition system: (1) the gallery uncertainty caused by overlapping\nclasses and (2) the uncertainty of the face embeddings. To detect both types,\nwe use a Bayesian probabilistic model of embedding distribution, which provides\na principled uncertainty estimate. Challenging open-set face recognition\ndatasets, such as IJB-C, serve as a testbed for our method. We also propose a\nnew open-set recognition protocol for whale and dolphin identification. The\nproposed approach better identifies recognition errors than uncertainty\nestimation methods based solely on image quality.\n", "link": "http://arxiv.org/abs/2408.14229v1", "date": "2024-08-26", "relevancy": 2.3786, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.6039}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5918}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gallery-Aware%20Uncertainty%20Estimation%20For%20Open-Set%20Face%20Recognition&body=Title%3A%20Gallery-Aware%20Uncertainty%20Estimation%20For%20Open-Set%20Face%20Recognition%0AAuthor%3A%20Leonid%20Erlygin%20and%20Alexey%20Zaytsev%0AAbstract%3A%20%20%20Accurately%20estimating%20image%20quality%20and%20model%20robustness%20improvement%20are%0Acritical%20challenges%20in%20unconstrained%20face%20recognition%2C%20which%20can%20be%20addressed%0Athrough%20uncertainty%20estimation%20via%20probabilistic%20face%20embeddings.%20Previous%0Aresearch%20mainly%20focused%20on%20uncertainty%20estimation%20in%20face%20verification%2C%20leaving%0Athe%20open-set%20face%20recognition%20task%20underexplored.%20In%20open-set%20face%20recognition%2C%0Aone%20seeks%20to%20classify%20an%20image%2C%20which%20could%20also%20be%20unknown.%20Here%2C%20the%20low%0Avariance%20of%20probabilistic%20embedding%20does%20not%20imply%20a%20low%20error%20probability%3A%20an%0Aimage%20embedding%20could%20be%20close%20to%20several%20classes%20in%20a%20gallery%2C%20thus%20yielding%0Ahigh%20uncertainty.%20We%20propose%20a%20method%20aware%20of%20two%20sources%20of%20ambiguity%20in%20the%0Aopen-set%20recognition%20system%3A%20%281%29%20the%20gallery%20uncertainty%20caused%20by%20overlapping%0Aclasses%20and%20%282%29%20the%20uncertainty%20of%20the%20face%20embeddings.%20To%20detect%20both%20types%2C%0Awe%20use%20a%20Bayesian%20probabilistic%20model%20of%20embedding%20distribution%2C%20which%20provides%0Aa%20principled%20uncertainty%20estimate.%20Challenging%20open-set%20face%20recognition%0Adatasets%2C%20such%20as%20IJB-C%2C%20serve%20as%20a%20testbed%20for%20our%20method.%20We%20also%20propose%20a%0Anew%20open-set%20recognition%20protocol%20for%20whale%20and%20dolphin%20identification.%20The%0Aproposed%20approach%20better%20identifies%20recognition%20errors%20than%20uncertainty%0Aestimation%20methods%20based%20solely%20on%20image%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14229v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGallery-Aware%2520Uncertainty%2520Estimation%2520For%2520Open-Set%2520Face%2520Recognition%26entry.906535625%3DLeonid%2520Erlygin%2520and%2520Alexey%2520Zaytsev%26entry.1292438233%3D%2520%2520Accurately%2520estimating%2520image%2520quality%2520and%2520model%2520robustness%2520improvement%2520are%250Acritical%2520challenges%2520in%2520unconstrained%2520face%2520recognition%252C%2520which%2520can%2520be%2520addressed%250Athrough%2520uncertainty%2520estimation%2520via%2520probabilistic%2520face%2520embeddings.%2520Previous%250Aresearch%2520mainly%2520focused%2520on%2520uncertainty%2520estimation%2520in%2520face%2520verification%252C%2520leaving%250Athe%2520open-set%2520face%2520recognition%2520task%2520underexplored.%2520In%2520open-set%2520face%2520recognition%252C%250Aone%2520seeks%2520to%2520classify%2520an%2520image%252C%2520which%2520could%2520also%2520be%2520unknown.%2520Here%252C%2520the%2520low%250Avariance%2520of%2520probabilistic%2520embedding%2520does%2520not%2520imply%2520a%2520low%2520error%2520probability%253A%2520an%250Aimage%2520embedding%2520could%2520be%2520close%2520to%2520several%2520classes%2520in%2520a%2520gallery%252C%2520thus%2520yielding%250Ahigh%2520uncertainty.%2520We%2520propose%2520a%2520method%2520aware%2520of%2520two%2520sources%2520of%2520ambiguity%2520in%2520the%250Aopen-set%2520recognition%2520system%253A%2520%25281%2529%2520the%2520gallery%2520uncertainty%2520caused%2520by%2520overlapping%250Aclasses%2520and%2520%25282%2529%2520the%2520uncertainty%2520of%2520the%2520face%2520embeddings.%2520To%2520detect%2520both%2520types%252C%250Awe%2520use%2520a%2520Bayesian%2520probabilistic%2520model%2520of%2520embedding%2520distribution%252C%2520which%2520provides%250Aa%2520principled%2520uncertainty%2520estimate.%2520Challenging%2520open-set%2520face%2520recognition%250Adatasets%252C%2520such%2520as%2520IJB-C%252C%2520serve%2520as%2520a%2520testbed%2520for%2520our%2520method.%2520We%2520also%2520propose%2520a%250Anew%2520open-set%2520recognition%2520protocol%2520for%2520whale%2520and%2520dolphin%2520identification.%2520The%250Aproposed%2520approach%2520better%2520identifies%2520recognition%2520errors%2520than%2520uncertainty%250Aestimation%2520methods%2520based%2520solely%2520on%2520image%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14229v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gallery-Aware%20Uncertainty%20Estimation%20For%20Open-Set%20Face%20Recognition&entry.906535625=Leonid%20Erlygin%20and%20Alexey%20Zaytsev&entry.1292438233=%20%20Accurately%20estimating%20image%20quality%20and%20model%20robustness%20improvement%20are%0Acritical%20challenges%20in%20unconstrained%20face%20recognition%2C%20which%20can%20be%20addressed%0Athrough%20uncertainty%20estimation%20via%20probabilistic%20face%20embeddings.%20Previous%0Aresearch%20mainly%20focused%20on%20uncertainty%20estimation%20in%20face%20verification%2C%20leaving%0Athe%20open-set%20face%20recognition%20task%20underexplored.%20In%20open-set%20face%20recognition%2C%0Aone%20seeks%20to%20classify%20an%20image%2C%20which%20could%20also%20be%20unknown.%20Here%2C%20the%20low%0Avariance%20of%20probabilistic%20embedding%20does%20not%20imply%20a%20low%20error%20probability%3A%20an%0Aimage%20embedding%20could%20be%20close%20to%20several%20classes%20in%20a%20gallery%2C%20thus%20yielding%0Ahigh%20uncertainty.%20We%20propose%20a%20method%20aware%20of%20two%20sources%20of%20ambiguity%20in%20the%0Aopen-set%20recognition%20system%3A%20%281%29%20the%20gallery%20uncertainty%20caused%20by%20overlapping%0Aclasses%20and%20%282%29%20the%20uncertainty%20of%20the%20face%20embeddings.%20To%20detect%20both%20types%2C%0Awe%20use%20a%20Bayesian%20probabilistic%20model%20of%20embedding%20distribution%2C%20which%20provides%0Aa%20principled%20uncertainty%20estimate.%20Challenging%20open-set%20face%20recognition%0Adatasets%2C%20such%20as%20IJB-C%2C%20serve%20as%20a%20testbed%20for%20our%20method.%20We%20also%20propose%20a%0Anew%20open-set%20recognition%20protocol%20for%20whale%20and%20dolphin%20identification.%20The%0Aproposed%20approach%20better%20identifies%20recognition%20errors%20than%20uncertainty%0Aestimation%20methods%20based%20solely%20on%20image%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14229v1&entry.124074799=Read"},
{"title": "Text3DAug -- Prompted Instance Augmentation for LiDAR Perception", "author": "Laurenz Reichardt and Luca Uhr and Oliver Wasenm\u00fcller", "abstract": "  LiDAR data of urban scenarios poses unique challenges, such as heterogeneous\ncharacteristics and inherent class imbalance. Therefore, large-scale datasets\nare necessary to apply deep learning methods. Instance augmentation has emerged\nas an efficient method to increase dataset diversity. However, current methods\nrequire the time-consuming curation of 3D models or costly manual data\nannotation. To overcome these limitations, we propose Text3DAug, a novel\napproach leveraging generative models for instance augmentation. Text3DAug does\nnot depend on labeled data and is the first of its kind to generate instances\nand annotations from text. This allows for a fully automated pipeline,\neliminating the need for manual effort in practical applications. Additionally,\nText3DAug is sensor agnostic and can be applied regardless of the LiDAR sensor\nused. Comprehensive experimental analysis on LiDAR segmentation, detection and\nnovel class discovery demonstrates that Text3DAug is effective in supplementing\nexisting methods or as a standalone method, performing on par or better than\nestablished methods, however while overcoming their specific drawbacks. The\ncode is publicly available.\n", "link": "http://arxiv.org/abs/2408.14253v1", "date": "2024-08-26", "relevancy": 2.3074, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5775}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5775}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5736}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text3DAug%20--%20Prompted%20Instance%20Augmentation%20for%20LiDAR%20Perception&body=Title%3A%20Text3DAug%20--%20Prompted%20Instance%20Augmentation%20for%20LiDAR%20Perception%0AAuthor%3A%20Laurenz%20Reichardt%20and%20Luca%20Uhr%20and%20Oliver%20Wasenm%C3%BCller%0AAbstract%3A%20%20%20LiDAR%20data%20of%20urban%20scenarios%20poses%20unique%20challenges%2C%20such%20as%20heterogeneous%0Acharacteristics%20and%20inherent%20class%20imbalance.%20Therefore%2C%20large-scale%20datasets%0Aare%20necessary%20to%20apply%20deep%20learning%20methods.%20Instance%20augmentation%20has%20emerged%0Aas%20an%20efficient%20method%20to%20increase%20dataset%20diversity.%20However%2C%20current%20methods%0Arequire%20the%20time-consuming%20curation%20of%203D%20models%20or%20costly%20manual%20data%0Aannotation.%20To%20overcome%20these%20limitations%2C%20we%20propose%20Text3DAug%2C%20a%20novel%0Aapproach%20leveraging%20generative%20models%20for%20instance%20augmentation.%20Text3DAug%20does%0Anot%20depend%20on%20labeled%20data%20and%20is%20the%20first%20of%20its%20kind%20to%20generate%20instances%0Aand%20annotations%20from%20text.%20This%20allows%20for%20a%20fully%20automated%20pipeline%2C%0Aeliminating%20the%20need%20for%20manual%20effort%20in%20practical%20applications.%20Additionally%2C%0AText3DAug%20is%20sensor%20agnostic%20and%20can%20be%20applied%20regardless%20of%20the%20LiDAR%20sensor%0Aused.%20Comprehensive%20experimental%20analysis%20on%20LiDAR%20segmentation%2C%20detection%20and%0Anovel%20class%20discovery%20demonstrates%20that%20Text3DAug%20is%20effective%20in%20supplementing%0Aexisting%20methods%20or%20as%20a%20standalone%20method%2C%20performing%20on%20par%20or%20better%20than%0Aestablished%20methods%2C%20however%20while%20overcoming%20their%20specific%20drawbacks.%20The%0Acode%20is%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14253v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText3DAug%2520--%2520Prompted%2520Instance%2520Augmentation%2520for%2520LiDAR%2520Perception%26entry.906535625%3DLaurenz%2520Reichardt%2520and%2520Luca%2520Uhr%2520and%2520Oliver%2520Wasenm%25C3%25BCller%26entry.1292438233%3D%2520%2520LiDAR%2520data%2520of%2520urban%2520scenarios%2520poses%2520unique%2520challenges%252C%2520such%2520as%2520heterogeneous%250Acharacteristics%2520and%2520inherent%2520class%2520imbalance.%2520Therefore%252C%2520large-scale%2520datasets%250Aare%2520necessary%2520to%2520apply%2520deep%2520learning%2520methods.%2520Instance%2520augmentation%2520has%2520emerged%250Aas%2520an%2520efficient%2520method%2520to%2520increase%2520dataset%2520diversity.%2520However%252C%2520current%2520methods%250Arequire%2520the%2520time-consuming%2520curation%2520of%25203D%2520models%2520or%2520costly%2520manual%2520data%250Aannotation.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520Text3DAug%252C%2520a%2520novel%250Aapproach%2520leveraging%2520generative%2520models%2520for%2520instance%2520augmentation.%2520Text3DAug%2520does%250Anot%2520depend%2520on%2520labeled%2520data%2520and%2520is%2520the%2520first%2520of%2520its%2520kind%2520to%2520generate%2520instances%250Aand%2520annotations%2520from%2520text.%2520This%2520allows%2520for%2520a%2520fully%2520automated%2520pipeline%252C%250Aeliminating%2520the%2520need%2520for%2520manual%2520effort%2520in%2520practical%2520applications.%2520Additionally%252C%250AText3DAug%2520is%2520sensor%2520agnostic%2520and%2520can%2520be%2520applied%2520regardless%2520of%2520the%2520LiDAR%2520sensor%250Aused.%2520Comprehensive%2520experimental%2520analysis%2520on%2520LiDAR%2520segmentation%252C%2520detection%2520and%250Anovel%2520class%2520discovery%2520demonstrates%2520that%2520Text3DAug%2520is%2520effective%2520in%2520supplementing%250Aexisting%2520methods%2520or%2520as%2520a%2520standalone%2520method%252C%2520performing%2520on%2520par%2520or%2520better%2520than%250Aestablished%2520methods%252C%2520however%2520while%2520overcoming%2520their%2520specific%2520drawbacks.%2520The%250Acode%2520is%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14253v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text3DAug%20--%20Prompted%20Instance%20Augmentation%20for%20LiDAR%20Perception&entry.906535625=Laurenz%20Reichardt%20and%20Luca%20Uhr%20and%20Oliver%20Wasenm%C3%BCller&entry.1292438233=%20%20LiDAR%20data%20of%20urban%20scenarios%20poses%20unique%20challenges%2C%20such%20as%20heterogeneous%0Acharacteristics%20and%20inherent%20class%20imbalance.%20Therefore%2C%20large-scale%20datasets%0Aare%20necessary%20to%20apply%20deep%20learning%20methods.%20Instance%20augmentation%20has%20emerged%0Aas%20an%20efficient%20method%20to%20increase%20dataset%20diversity.%20However%2C%20current%20methods%0Arequire%20the%20time-consuming%20curation%20of%203D%20models%20or%20costly%20manual%20data%0Aannotation.%20To%20overcome%20these%20limitations%2C%20we%20propose%20Text3DAug%2C%20a%20novel%0Aapproach%20leveraging%20generative%20models%20for%20instance%20augmentation.%20Text3DAug%20does%0Anot%20depend%20on%20labeled%20data%20and%20is%20the%20first%20of%20its%20kind%20to%20generate%20instances%0Aand%20annotations%20from%20text.%20This%20allows%20for%20a%20fully%20automated%20pipeline%2C%0Aeliminating%20the%20need%20for%20manual%20effort%20in%20practical%20applications.%20Additionally%2C%0AText3DAug%20is%20sensor%20agnostic%20and%20can%20be%20applied%20regardless%20of%20the%20LiDAR%20sensor%0Aused.%20Comprehensive%20experimental%20analysis%20on%20LiDAR%20segmentation%2C%20detection%20and%0Anovel%20class%20discovery%20demonstrates%20that%20Text3DAug%20is%20effective%20in%20supplementing%0Aexisting%20methods%20or%20as%20a%20standalone%20method%2C%20performing%20on%20par%20or%20better%20than%0Aestablished%20methods%2C%20however%20while%20overcoming%20their%20specific%20drawbacks.%20The%0Acode%20is%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14253v1&entry.124074799=Read"},
{"title": "LF Tracy: A Unified Single-Pipeline Approach for Salient Object\n  Detection in Light Field Cameras", "author": "Fei Teng and Jiaming Zhang and Jiawei Liu and Kunyu Peng and Xina Cheng and Zhiyong Li and Kailun Yang", "abstract": "  Leveraging rich information is crucial for dense prediction tasks. Light\nfield (LF) cameras are instrumental in this regard, as they allow data to be\nsampled from various perspectives. This capability provides valuable spatial,\ndepth, and angular information, enhancing scene-parsing tasks. However, we have\nidentified two overlooked issues for the LF salient object detection (SOD)\ntask. (1): Previous approaches predominantly employ a customized two-stream\ndesign to discover the spatial and depth features within light field images.\nThe network struggles to learn the implicit angular information between\ndifferent images due to a lack of intra-network data connectivity. (2): Little\nresearch has been directed towards the data augmentation strategy for LF SOD.\nResearch on inter-network data connectivity is scant. In this study, we propose\nan efficient paradigm (LF Tracy) to address those issues. This comprises a\nsingle-pipeline encoder paired with a highly efficient information aggregation\n(IA) module (around 8M parameters) to establish an intra-network connection.\nThen, a simple yet effective data augmentation strategy called MixLD is\ndesigned to bridge the inter-network connections. Owing to this innovative\nparadigm, our model surpasses the existing state-of-the-art method through\nextensive experiments. Especially, LF Tracy demonstrates a 23% improvement over\nprevious results on the latest large-scale PKU dataset. The source code is\npublicly available at: https://github.com/FeiBryantkit/LF-Tracy.\n", "link": "http://arxiv.org/abs/2401.16712v2", "date": "2024-08-26", "relevancy": 2.2991, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5857}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5696}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LF%20Tracy%3A%20A%20Unified%20Single-Pipeline%20Approach%20for%20Salient%20Object%0A%20%20Detection%20in%20Light%20Field%20Cameras&body=Title%3A%20LF%20Tracy%3A%20A%20Unified%20Single-Pipeline%20Approach%20for%20Salient%20Object%0A%20%20Detection%20in%20Light%20Field%20Cameras%0AAuthor%3A%20Fei%20Teng%20and%20Jiaming%20Zhang%20and%20Jiawei%20Liu%20and%20Kunyu%20Peng%20and%20Xina%20Cheng%20and%20Zhiyong%20Li%20and%20Kailun%20Yang%0AAbstract%3A%20%20%20Leveraging%20rich%20information%20is%20crucial%20for%20dense%20prediction%20tasks.%20Light%0Afield%20%28LF%29%20cameras%20are%20instrumental%20in%20this%20regard%2C%20as%20they%20allow%20data%20to%20be%0Asampled%20from%20various%20perspectives.%20This%20capability%20provides%20valuable%20spatial%2C%0Adepth%2C%20and%20angular%20information%2C%20enhancing%20scene-parsing%20tasks.%20However%2C%20we%20have%0Aidentified%20two%20overlooked%20issues%20for%20the%20LF%20salient%20object%20detection%20%28SOD%29%0Atask.%20%281%29%3A%20Previous%20approaches%20predominantly%20employ%20a%20customized%20two-stream%0Adesign%20to%20discover%20the%20spatial%20and%20depth%20features%20within%20light%20field%20images.%0AThe%20network%20struggles%20to%20learn%20the%20implicit%20angular%20information%20between%0Adifferent%20images%20due%20to%20a%20lack%20of%20intra-network%20data%20connectivity.%20%282%29%3A%20Little%0Aresearch%20has%20been%20directed%20towards%20the%20data%20augmentation%20strategy%20for%20LF%20SOD.%0AResearch%20on%20inter-network%20data%20connectivity%20is%20scant.%20In%20this%20study%2C%20we%20propose%0Aan%20efficient%20paradigm%20%28LF%20Tracy%29%20to%20address%20those%20issues.%20This%20comprises%20a%0Asingle-pipeline%20encoder%20paired%20with%20a%20highly%20efficient%20information%20aggregation%0A%28IA%29%20module%20%28around%208M%20parameters%29%20to%20establish%20an%20intra-network%20connection.%0AThen%2C%20a%20simple%20yet%20effective%20data%20augmentation%20strategy%20called%20MixLD%20is%0Adesigned%20to%20bridge%20the%20inter-network%20connections.%20Owing%20to%20this%20innovative%0Aparadigm%2C%20our%20model%20surpasses%20the%20existing%20state-of-the-art%20method%20through%0Aextensive%20experiments.%20Especially%2C%20LF%20Tracy%20demonstrates%20a%2023%25%20improvement%20over%0Aprevious%20results%20on%20the%20latest%20large-scale%20PKU%20dataset.%20The%20source%20code%20is%0Apublicly%20available%20at%3A%20https%3A//github.com/FeiBryantkit/LF-Tracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.16712v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLF%2520Tracy%253A%2520A%2520Unified%2520Single-Pipeline%2520Approach%2520for%2520Salient%2520Object%250A%2520%2520Detection%2520in%2520Light%2520Field%2520Cameras%26entry.906535625%3DFei%2520Teng%2520and%2520Jiaming%2520Zhang%2520and%2520Jiawei%2520Liu%2520and%2520Kunyu%2520Peng%2520and%2520Xina%2520Cheng%2520and%2520Zhiyong%2520Li%2520and%2520Kailun%2520Yang%26entry.1292438233%3D%2520%2520Leveraging%2520rich%2520information%2520is%2520crucial%2520for%2520dense%2520prediction%2520tasks.%2520Light%250Afield%2520%2528LF%2529%2520cameras%2520are%2520instrumental%2520in%2520this%2520regard%252C%2520as%2520they%2520allow%2520data%2520to%2520be%250Asampled%2520from%2520various%2520perspectives.%2520This%2520capability%2520provides%2520valuable%2520spatial%252C%250Adepth%252C%2520and%2520angular%2520information%252C%2520enhancing%2520scene-parsing%2520tasks.%2520However%252C%2520we%2520have%250Aidentified%2520two%2520overlooked%2520issues%2520for%2520the%2520LF%2520salient%2520object%2520detection%2520%2528SOD%2529%250Atask.%2520%25281%2529%253A%2520Previous%2520approaches%2520predominantly%2520employ%2520a%2520customized%2520two-stream%250Adesign%2520to%2520discover%2520the%2520spatial%2520and%2520depth%2520features%2520within%2520light%2520field%2520images.%250AThe%2520network%2520struggles%2520to%2520learn%2520the%2520implicit%2520angular%2520information%2520between%250Adifferent%2520images%2520due%2520to%2520a%2520lack%2520of%2520intra-network%2520data%2520connectivity.%2520%25282%2529%253A%2520Little%250Aresearch%2520has%2520been%2520directed%2520towards%2520the%2520data%2520augmentation%2520strategy%2520for%2520LF%2520SOD.%250AResearch%2520on%2520inter-network%2520data%2520connectivity%2520is%2520scant.%2520In%2520this%2520study%252C%2520we%2520propose%250Aan%2520efficient%2520paradigm%2520%2528LF%2520Tracy%2529%2520to%2520address%2520those%2520issues.%2520This%2520comprises%2520a%250Asingle-pipeline%2520encoder%2520paired%2520with%2520a%2520highly%2520efficient%2520information%2520aggregation%250A%2528IA%2529%2520module%2520%2528around%25208M%2520parameters%2529%2520to%2520establish%2520an%2520intra-network%2520connection.%250AThen%252C%2520a%2520simple%2520yet%2520effective%2520data%2520augmentation%2520strategy%2520called%2520MixLD%2520is%250Adesigned%2520to%2520bridge%2520the%2520inter-network%2520connections.%2520Owing%2520to%2520this%2520innovative%250Aparadigm%252C%2520our%2520model%2520surpasses%2520the%2520existing%2520state-of-the-art%2520method%2520through%250Aextensive%2520experiments.%2520Especially%252C%2520LF%2520Tracy%2520demonstrates%2520a%252023%2525%2520improvement%2520over%250Aprevious%2520results%2520on%2520the%2520latest%2520large-scale%2520PKU%2520dataset.%2520The%2520source%2520code%2520is%250Apublicly%2520available%2520at%253A%2520https%253A//github.com/FeiBryantkit/LF-Tracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.16712v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LF%20Tracy%3A%20A%20Unified%20Single-Pipeline%20Approach%20for%20Salient%20Object%0A%20%20Detection%20in%20Light%20Field%20Cameras&entry.906535625=Fei%20Teng%20and%20Jiaming%20Zhang%20and%20Jiawei%20Liu%20and%20Kunyu%20Peng%20and%20Xina%20Cheng%20and%20Zhiyong%20Li%20and%20Kailun%20Yang&entry.1292438233=%20%20Leveraging%20rich%20information%20is%20crucial%20for%20dense%20prediction%20tasks.%20Light%0Afield%20%28LF%29%20cameras%20are%20instrumental%20in%20this%20regard%2C%20as%20they%20allow%20data%20to%20be%0Asampled%20from%20various%20perspectives.%20This%20capability%20provides%20valuable%20spatial%2C%0Adepth%2C%20and%20angular%20information%2C%20enhancing%20scene-parsing%20tasks.%20However%2C%20we%20have%0Aidentified%20two%20overlooked%20issues%20for%20the%20LF%20salient%20object%20detection%20%28SOD%29%0Atask.%20%281%29%3A%20Previous%20approaches%20predominantly%20employ%20a%20customized%20two-stream%0Adesign%20to%20discover%20the%20spatial%20and%20depth%20features%20within%20light%20field%20images.%0AThe%20network%20struggles%20to%20learn%20the%20implicit%20angular%20information%20between%0Adifferent%20images%20due%20to%20a%20lack%20of%20intra-network%20data%20connectivity.%20%282%29%3A%20Little%0Aresearch%20has%20been%20directed%20towards%20the%20data%20augmentation%20strategy%20for%20LF%20SOD.%0AResearch%20on%20inter-network%20data%20connectivity%20is%20scant.%20In%20this%20study%2C%20we%20propose%0Aan%20efficient%20paradigm%20%28LF%20Tracy%29%20to%20address%20those%20issues.%20This%20comprises%20a%0Asingle-pipeline%20encoder%20paired%20with%20a%20highly%20efficient%20information%20aggregation%0A%28IA%29%20module%20%28around%208M%20parameters%29%20to%20establish%20an%20intra-network%20connection.%0AThen%2C%20a%20simple%20yet%20effective%20data%20augmentation%20strategy%20called%20MixLD%20is%0Adesigned%20to%20bridge%20the%20inter-network%20connections.%20Owing%20to%20this%20innovative%0Aparadigm%2C%20our%20model%20surpasses%20the%20existing%20state-of-the-art%20method%20through%0Aextensive%20experiments.%20Especially%2C%20LF%20Tracy%20demonstrates%20a%2023%25%20improvement%20over%0Aprevious%20results%20on%20the%20latest%20large-scale%20PKU%20dataset.%20The%20source%20code%20is%0Apublicly%20available%20at%3A%20https%3A//github.com/FeiBryantkit/LF-Tracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.16712v2&entry.124074799=Read"},
{"title": "Fusion of Visual-Inertial Odometry with LiDAR Relative Localization for\n  Cooperative Guidance of a Micro-Scale Aerial Vehicle", "author": "V\u00e1clav Pritzl and Matou\u0161 Vrba and Petr \u0160t\u011bp\u00e1n and Martin Saska", "abstract": "  A novel relative localization approach for guidance of a micro-scale UAV by a\nwell-equipped aerial robot fusing VIO with LiDAR is proposed in this paper.\nLiDAR-based localization is accurate and robust to challenging environmental\nconditions, but 3D LiDARs are relatively heavy and require large UAV platforms,\nin contrast to lightweight cameras. However, visual-based self-localization\nmethods exhibit lower accuracy and can suffer from significant drift with\nrespect to the global reference frame. To benefit from both sensory modalities,\nwe focus on cooperative navigation in a heterogeneous team of a primary\nLiDAR-equipped UAV and a secondary micro-scale camera-equipped UAV. We propose\na novel cooperative approach combining LiDAR relative localization data with\nVIO output on board the primary UAV to obtain an accurate pose of the secondary\nUAV. The pose estimate is used to precisely and reliably guide the secondary\nUAV along trajectories defined in the primary UAV reference frame. The\nexperimental evaluation has shown the superior accuracy of our method to the\nraw VIO output and demonstrated its capability to guide the secondary UAV along\ndesired trajectories while mitigating VIO drift. Thus, such a heterogeneous\nsystem can explore large areas with LiDAR precision, as well as visit locations\ninaccessible to the large LiDAR-carrying UAV platforms, as was showcased in a\nreal-world cooperative mapping scenario.\n", "link": "http://arxiv.org/abs/2306.17544v2", "date": "2024-08-26", "relevancy": 2.2947, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6043}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5543}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fusion%20of%20Visual-Inertial%20Odometry%20with%20LiDAR%20Relative%20Localization%20for%0A%20%20Cooperative%20Guidance%20of%20a%20Micro-Scale%20Aerial%20Vehicle&body=Title%3A%20Fusion%20of%20Visual-Inertial%20Odometry%20with%20LiDAR%20Relative%20Localization%20for%0A%20%20Cooperative%20Guidance%20of%20a%20Micro-Scale%20Aerial%20Vehicle%0AAuthor%3A%20V%C3%A1clav%20Pritzl%20and%20Matou%C5%A1%20Vrba%20and%20Petr%20%C5%A0t%C4%9Bp%C3%A1n%20and%20Martin%20Saska%0AAbstract%3A%20%20%20A%20novel%20relative%20localization%20approach%20for%20guidance%20of%20a%20micro-scale%20UAV%20by%20a%0Awell-equipped%20aerial%20robot%20fusing%20VIO%20with%20LiDAR%20is%20proposed%20in%20this%20paper.%0ALiDAR-based%20localization%20is%20accurate%20and%20robust%20to%20challenging%20environmental%0Aconditions%2C%20but%203D%20LiDARs%20are%20relatively%20heavy%20and%20require%20large%20UAV%20platforms%2C%0Ain%20contrast%20to%20lightweight%20cameras.%20However%2C%20visual-based%20self-localization%0Amethods%20exhibit%20lower%20accuracy%20and%20can%20suffer%20from%20significant%20drift%20with%0Arespect%20to%20the%20global%20reference%20frame.%20To%20benefit%20from%20both%20sensory%20modalities%2C%0Awe%20focus%20on%20cooperative%20navigation%20in%20a%20heterogeneous%20team%20of%20a%20primary%0ALiDAR-equipped%20UAV%20and%20a%20secondary%20micro-scale%20camera-equipped%20UAV.%20We%20propose%0Aa%20novel%20cooperative%20approach%20combining%20LiDAR%20relative%20localization%20data%20with%0AVIO%20output%20on%20board%20the%20primary%20UAV%20to%20obtain%20an%20accurate%20pose%20of%20the%20secondary%0AUAV.%20The%20pose%20estimate%20is%20used%20to%20precisely%20and%20reliably%20guide%20the%20secondary%0AUAV%20along%20trajectories%20defined%20in%20the%20primary%20UAV%20reference%20frame.%20The%0Aexperimental%20evaluation%20has%20shown%20the%20superior%20accuracy%20of%20our%20method%20to%20the%0Araw%20VIO%20output%20and%20demonstrated%20its%20capability%20to%20guide%20the%20secondary%20UAV%20along%0Adesired%20trajectories%20while%20mitigating%20VIO%20drift.%20Thus%2C%20such%20a%20heterogeneous%0Asystem%20can%20explore%20large%20areas%20with%20LiDAR%20precision%2C%20as%20well%20as%20visit%20locations%0Ainaccessible%20to%20the%20large%20LiDAR-carrying%20UAV%20platforms%2C%20as%20was%20showcased%20in%20a%0Areal-world%20cooperative%20mapping%20scenario.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.17544v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFusion%2520of%2520Visual-Inertial%2520Odometry%2520with%2520LiDAR%2520Relative%2520Localization%2520for%250A%2520%2520Cooperative%2520Guidance%2520of%2520a%2520Micro-Scale%2520Aerial%2520Vehicle%26entry.906535625%3DV%25C3%25A1clav%2520Pritzl%2520and%2520Matou%25C5%25A1%2520Vrba%2520and%2520Petr%2520%25C5%25A0t%25C4%259Bp%25C3%25A1n%2520and%2520Martin%2520Saska%26entry.1292438233%3D%2520%2520A%2520novel%2520relative%2520localization%2520approach%2520for%2520guidance%2520of%2520a%2520micro-scale%2520UAV%2520by%2520a%250Awell-equipped%2520aerial%2520robot%2520fusing%2520VIO%2520with%2520LiDAR%2520is%2520proposed%2520in%2520this%2520paper.%250ALiDAR-based%2520localization%2520is%2520accurate%2520and%2520robust%2520to%2520challenging%2520environmental%250Aconditions%252C%2520but%25203D%2520LiDARs%2520are%2520relatively%2520heavy%2520and%2520require%2520large%2520UAV%2520platforms%252C%250Ain%2520contrast%2520to%2520lightweight%2520cameras.%2520However%252C%2520visual-based%2520self-localization%250Amethods%2520exhibit%2520lower%2520accuracy%2520and%2520can%2520suffer%2520from%2520significant%2520drift%2520with%250Arespect%2520to%2520the%2520global%2520reference%2520frame.%2520To%2520benefit%2520from%2520both%2520sensory%2520modalities%252C%250Awe%2520focus%2520on%2520cooperative%2520navigation%2520in%2520a%2520heterogeneous%2520team%2520of%2520a%2520primary%250ALiDAR-equipped%2520UAV%2520and%2520a%2520secondary%2520micro-scale%2520camera-equipped%2520UAV.%2520We%2520propose%250Aa%2520novel%2520cooperative%2520approach%2520combining%2520LiDAR%2520relative%2520localization%2520data%2520with%250AVIO%2520output%2520on%2520board%2520the%2520primary%2520UAV%2520to%2520obtain%2520an%2520accurate%2520pose%2520of%2520the%2520secondary%250AUAV.%2520The%2520pose%2520estimate%2520is%2520used%2520to%2520precisely%2520and%2520reliably%2520guide%2520the%2520secondary%250AUAV%2520along%2520trajectories%2520defined%2520in%2520the%2520primary%2520UAV%2520reference%2520frame.%2520The%250Aexperimental%2520evaluation%2520has%2520shown%2520the%2520superior%2520accuracy%2520of%2520our%2520method%2520to%2520the%250Araw%2520VIO%2520output%2520and%2520demonstrated%2520its%2520capability%2520to%2520guide%2520the%2520secondary%2520UAV%2520along%250Adesired%2520trajectories%2520while%2520mitigating%2520VIO%2520drift.%2520Thus%252C%2520such%2520a%2520heterogeneous%250Asystem%2520can%2520explore%2520large%2520areas%2520with%2520LiDAR%2520precision%252C%2520as%2520well%2520as%2520visit%2520locations%250Ainaccessible%2520to%2520the%2520large%2520LiDAR-carrying%2520UAV%2520platforms%252C%2520as%2520was%2520showcased%2520in%2520a%250Areal-world%2520cooperative%2520mapping%2520scenario.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.17544v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fusion%20of%20Visual-Inertial%20Odometry%20with%20LiDAR%20Relative%20Localization%20for%0A%20%20Cooperative%20Guidance%20of%20a%20Micro-Scale%20Aerial%20Vehicle&entry.906535625=V%C3%A1clav%20Pritzl%20and%20Matou%C5%A1%20Vrba%20and%20Petr%20%C5%A0t%C4%9Bp%C3%A1n%20and%20Martin%20Saska&entry.1292438233=%20%20A%20novel%20relative%20localization%20approach%20for%20guidance%20of%20a%20micro-scale%20UAV%20by%20a%0Awell-equipped%20aerial%20robot%20fusing%20VIO%20with%20LiDAR%20is%20proposed%20in%20this%20paper.%0ALiDAR-based%20localization%20is%20accurate%20and%20robust%20to%20challenging%20environmental%0Aconditions%2C%20but%203D%20LiDARs%20are%20relatively%20heavy%20and%20require%20large%20UAV%20platforms%2C%0Ain%20contrast%20to%20lightweight%20cameras.%20However%2C%20visual-based%20self-localization%0Amethods%20exhibit%20lower%20accuracy%20and%20can%20suffer%20from%20significant%20drift%20with%0Arespect%20to%20the%20global%20reference%20frame.%20To%20benefit%20from%20both%20sensory%20modalities%2C%0Awe%20focus%20on%20cooperative%20navigation%20in%20a%20heterogeneous%20team%20of%20a%20primary%0ALiDAR-equipped%20UAV%20and%20a%20secondary%20micro-scale%20camera-equipped%20UAV.%20We%20propose%0Aa%20novel%20cooperative%20approach%20combining%20LiDAR%20relative%20localization%20data%20with%0AVIO%20output%20on%20board%20the%20primary%20UAV%20to%20obtain%20an%20accurate%20pose%20of%20the%20secondary%0AUAV.%20The%20pose%20estimate%20is%20used%20to%20precisely%20and%20reliably%20guide%20the%20secondary%0AUAV%20along%20trajectories%20defined%20in%20the%20primary%20UAV%20reference%20frame.%20The%0Aexperimental%20evaluation%20has%20shown%20the%20superior%20accuracy%20of%20our%20method%20to%20the%0Araw%20VIO%20output%20and%20demonstrated%20its%20capability%20to%20guide%20the%20secondary%20UAV%20along%0Adesired%20trajectories%20while%20mitigating%20VIO%20drift.%20Thus%2C%20such%20a%20heterogeneous%0Asystem%20can%20explore%20large%20areas%20with%20LiDAR%20precision%2C%20as%20well%20as%20visit%20locations%0Ainaccessible%20to%20the%20large%20LiDAR-carrying%20UAV%20platforms%2C%20as%20was%20showcased%20in%20a%0Areal-world%20cooperative%20mapping%20scenario.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.17544v2&entry.124074799=Read"},
{"title": "Planner3D: LLM-enhanced graph prior meets 3D indoor scene explicit\n  regularization", "author": "Yao Wei and Martin Renqiang Min and George Vosselman and Li Erran Li and Michael Ying Yang", "abstract": "  Compositional 3D scene synthesis has diverse applications across a spectrum\nof industries such as robotics, films, and video games, as it closely mirrors\nthe complexity of real-world multi-object environments. Conventional works\ntypically employ shape retrieval based frameworks which naturally suffer from\nlimited shape diversity. Recent progresses have been made in object shape\ngeneration with generative models such as diffusion models, which increases the\nshape fidelity. However, these approaches separately treat 3D shape generation\nand layout generation. The synthesized scenes are usually hampered by layout\ncollision, which suggests that the scene-level fidelity is still\nunder-explored. In this paper, we aim at generating realistic and reasonable 3D\nindoor scenes from scene graph. To enrich the priors of the given scene graph\ninputs, large language model is utilized to aggregate the global-wise features\nwith local node-wise and edge-wise features. With a unified graph encoder,\ngraph features are extracted to guide joint layout-shape generation. Additional\nregularization is introduced to explicitly constrain the produced 3D layouts.\nBenchmarked on the SG-FRONT dataset, our method achieves better 3D scene\nsynthesis, especially in terms of scene-level fidelity. The source code will be\nreleased after publication.\n", "link": "http://arxiv.org/abs/2403.12848v2", "date": "2024-08-26", "relevancy": 2.2784, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.577}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5668}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Planner3D%3A%20LLM-enhanced%20graph%20prior%20meets%203D%20indoor%20scene%20explicit%0A%20%20regularization&body=Title%3A%20Planner3D%3A%20LLM-enhanced%20graph%20prior%20meets%203D%20indoor%20scene%20explicit%0A%20%20regularization%0AAuthor%3A%20Yao%20Wei%20and%20Martin%20Renqiang%20Min%20and%20George%20Vosselman%20and%20Li%20Erran%20Li%20and%20Michael%20Ying%20Yang%0AAbstract%3A%20%20%20Compositional%203D%20scene%20synthesis%20has%20diverse%20applications%20across%20a%20spectrum%0Aof%20industries%20such%20as%20robotics%2C%20films%2C%20and%20video%20games%2C%20as%20it%20closely%20mirrors%0Athe%20complexity%20of%20real-world%20multi-object%20environments.%20Conventional%20works%0Atypically%20employ%20shape%20retrieval%20based%20frameworks%20which%20naturally%20suffer%20from%0Alimited%20shape%20diversity.%20Recent%20progresses%20have%20been%20made%20in%20object%20shape%0Ageneration%20with%20generative%20models%20such%20as%20diffusion%20models%2C%20which%20increases%20the%0Ashape%20fidelity.%20However%2C%20these%20approaches%20separately%20treat%203D%20shape%20generation%0Aand%20layout%20generation.%20The%20synthesized%20scenes%20are%20usually%20hampered%20by%20layout%0Acollision%2C%20which%20suggests%20that%20the%20scene-level%20fidelity%20is%20still%0Aunder-explored.%20In%20this%20paper%2C%20we%20aim%20at%20generating%20realistic%20and%20reasonable%203D%0Aindoor%20scenes%20from%20scene%20graph.%20To%20enrich%20the%20priors%20of%20the%20given%20scene%20graph%0Ainputs%2C%20large%20language%20model%20is%20utilized%20to%20aggregate%20the%20global-wise%20features%0Awith%20local%20node-wise%20and%20edge-wise%20features.%20With%20a%20unified%20graph%20encoder%2C%0Agraph%20features%20are%20extracted%20to%20guide%20joint%20layout-shape%20generation.%20Additional%0Aregularization%20is%20introduced%20to%20explicitly%20constrain%20the%20produced%203D%20layouts.%0ABenchmarked%20on%20the%20SG-FRONT%20dataset%2C%20our%20method%20achieves%20better%203D%20scene%0Asynthesis%2C%20especially%20in%20terms%20of%20scene-level%20fidelity.%20The%20source%20code%20will%20be%0Areleased%20after%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12848v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPlanner3D%253A%2520LLM-enhanced%2520graph%2520prior%2520meets%25203D%2520indoor%2520scene%2520explicit%250A%2520%2520regularization%26entry.906535625%3DYao%2520Wei%2520and%2520Martin%2520Renqiang%2520Min%2520and%2520George%2520Vosselman%2520and%2520Li%2520Erran%2520Li%2520and%2520Michael%2520Ying%2520Yang%26entry.1292438233%3D%2520%2520Compositional%25203D%2520scene%2520synthesis%2520has%2520diverse%2520applications%2520across%2520a%2520spectrum%250Aof%2520industries%2520such%2520as%2520robotics%252C%2520films%252C%2520and%2520video%2520games%252C%2520as%2520it%2520closely%2520mirrors%250Athe%2520complexity%2520of%2520real-world%2520multi-object%2520environments.%2520Conventional%2520works%250Atypically%2520employ%2520shape%2520retrieval%2520based%2520frameworks%2520which%2520naturally%2520suffer%2520from%250Alimited%2520shape%2520diversity.%2520Recent%2520progresses%2520have%2520been%2520made%2520in%2520object%2520shape%250Ageneration%2520with%2520generative%2520models%2520such%2520as%2520diffusion%2520models%252C%2520which%2520increases%2520the%250Ashape%2520fidelity.%2520However%252C%2520these%2520approaches%2520separately%2520treat%25203D%2520shape%2520generation%250Aand%2520layout%2520generation.%2520The%2520synthesized%2520scenes%2520are%2520usually%2520hampered%2520by%2520layout%250Acollision%252C%2520which%2520suggests%2520that%2520the%2520scene-level%2520fidelity%2520is%2520still%250Aunder-explored.%2520In%2520this%2520paper%252C%2520we%2520aim%2520at%2520generating%2520realistic%2520and%2520reasonable%25203D%250Aindoor%2520scenes%2520from%2520scene%2520graph.%2520To%2520enrich%2520the%2520priors%2520of%2520the%2520given%2520scene%2520graph%250Ainputs%252C%2520large%2520language%2520model%2520is%2520utilized%2520to%2520aggregate%2520the%2520global-wise%2520features%250Awith%2520local%2520node-wise%2520and%2520edge-wise%2520features.%2520With%2520a%2520unified%2520graph%2520encoder%252C%250Agraph%2520features%2520are%2520extracted%2520to%2520guide%2520joint%2520layout-shape%2520generation.%2520Additional%250Aregularization%2520is%2520introduced%2520to%2520explicitly%2520constrain%2520the%2520produced%25203D%2520layouts.%250ABenchmarked%2520on%2520the%2520SG-FRONT%2520dataset%252C%2520our%2520method%2520achieves%2520better%25203D%2520scene%250Asynthesis%252C%2520especially%2520in%2520terms%2520of%2520scene-level%2520fidelity.%2520The%2520source%2520code%2520will%2520be%250Areleased%2520after%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12848v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Planner3D%3A%20LLM-enhanced%20graph%20prior%20meets%203D%20indoor%20scene%20explicit%0A%20%20regularization&entry.906535625=Yao%20Wei%20and%20Martin%20Renqiang%20Min%20and%20George%20Vosselman%20and%20Li%20Erran%20Li%20and%20Michael%20Ying%20Yang&entry.1292438233=%20%20Compositional%203D%20scene%20synthesis%20has%20diverse%20applications%20across%20a%20spectrum%0Aof%20industries%20such%20as%20robotics%2C%20films%2C%20and%20video%20games%2C%20as%20it%20closely%20mirrors%0Athe%20complexity%20of%20real-world%20multi-object%20environments.%20Conventional%20works%0Atypically%20employ%20shape%20retrieval%20based%20frameworks%20which%20naturally%20suffer%20from%0Alimited%20shape%20diversity.%20Recent%20progresses%20have%20been%20made%20in%20object%20shape%0Ageneration%20with%20generative%20models%20such%20as%20diffusion%20models%2C%20which%20increases%20the%0Ashape%20fidelity.%20However%2C%20these%20approaches%20separately%20treat%203D%20shape%20generation%0Aand%20layout%20generation.%20The%20synthesized%20scenes%20are%20usually%20hampered%20by%20layout%0Acollision%2C%20which%20suggests%20that%20the%20scene-level%20fidelity%20is%20still%0Aunder-explored.%20In%20this%20paper%2C%20we%20aim%20at%20generating%20realistic%20and%20reasonable%203D%0Aindoor%20scenes%20from%20scene%20graph.%20To%20enrich%20the%20priors%20of%20the%20given%20scene%20graph%0Ainputs%2C%20large%20language%20model%20is%20utilized%20to%20aggregate%20the%20global-wise%20features%0Awith%20local%20node-wise%20and%20edge-wise%20features.%20With%20a%20unified%20graph%20encoder%2C%0Agraph%20features%20are%20extracted%20to%20guide%20joint%20layout-shape%20generation.%20Additional%0Aregularization%20is%20introduced%20to%20explicitly%20constrain%20the%20produced%203D%20layouts.%0ABenchmarked%20on%20the%20SG-FRONT%20dataset%2C%20our%20method%20achieves%20better%203D%20scene%0Asynthesis%2C%20especially%20in%20terms%20of%20scene-level%20fidelity.%20The%20source%20code%20will%20be%0Areleased%20after%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12848v2&entry.124074799=Read"},
{"title": "SigFormer: Sparse Signal-Guided Transformer for Multi-Modal Human Action\n  Segmentation", "author": "Qi Liu and Xinchen Liu and Kun Liu and Xiaoyan Gu and Wu Liu", "abstract": "  Multi-modal human action segmentation is a critical and challenging task with\na wide range of applications. Nowadays, the majority of approaches concentrate\non the fusion of dense signals (i.e., RGB, optical flow, and depth maps).\nHowever, the potential contributions of sparse IoT sensor signals, which can be\ncrucial for achieving accurate recognition, have not been fully explored. To\nmake up for this, we introduce a Sparse signalguided Transformer (SigFormer) to\ncombine both dense and sparse signals. We employ mask attention to fuse\nlocalized features by constraining cross-attention within the regions where\nsparse signals are valid. However, since sparse signals are discrete, they lack\nsufficient information about the temporal action boundaries. Therefore, in\nSigFormer, we propose to emphasize the boundary information at two stages to\nalleviate this problem. In the first feature extraction stage, we introduce an\nintermediate bottleneck module to jointly learn both category and boundary\nfeatures of each dense modality through the inner loss functions. After the\nfusion of dense modalities and sparse signals, we then devise a two-branch\narchitecture that explicitly models the interrelationship between action\ncategory and temporal boundary. Experimental results demonstrate that SigFormer\noutperforms the state-of-the-art approaches on a multi-modal action\nsegmentation dataset from real industrial environments, reaching an outstanding\nF1 score of 0.958. The codes and pre-trained models have been available at\nhttps://github.com/LIUQI-creat/SigFormer.\n", "link": "http://arxiv.org/abs/2311.17428v2", "date": "2024-08-26", "relevancy": 2.273, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.584}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5634}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SigFormer%3A%20Sparse%20Signal-Guided%20Transformer%20for%20Multi-Modal%20Human%20Action%0A%20%20Segmentation&body=Title%3A%20SigFormer%3A%20Sparse%20Signal-Guided%20Transformer%20for%20Multi-Modal%20Human%20Action%0A%20%20Segmentation%0AAuthor%3A%20Qi%20Liu%20and%20Xinchen%20Liu%20and%20Kun%20Liu%20and%20Xiaoyan%20Gu%20and%20Wu%20Liu%0AAbstract%3A%20%20%20Multi-modal%20human%20action%20segmentation%20is%20a%20critical%20and%20challenging%20task%20with%0Aa%20wide%20range%20of%20applications.%20Nowadays%2C%20the%20majority%20of%20approaches%20concentrate%0Aon%20the%20fusion%20of%20dense%20signals%20%28i.e.%2C%20RGB%2C%20optical%20flow%2C%20and%20depth%20maps%29.%0AHowever%2C%20the%20potential%20contributions%20of%20sparse%20IoT%20sensor%20signals%2C%20which%20can%20be%0Acrucial%20for%20achieving%20accurate%20recognition%2C%20have%20not%20been%20fully%20explored.%20To%0Amake%20up%20for%20this%2C%20we%20introduce%20a%20Sparse%20signalguided%20Transformer%20%28SigFormer%29%20to%0Acombine%20both%20dense%20and%20sparse%20signals.%20We%20employ%20mask%20attention%20to%20fuse%0Alocalized%20features%20by%20constraining%20cross-attention%20within%20the%20regions%20where%0Asparse%20signals%20are%20valid.%20However%2C%20since%20sparse%20signals%20are%20discrete%2C%20they%20lack%0Asufficient%20information%20about%20the%20temporal%20action%20boundaries.%20Therefore%2C%20in%0ASigFormer%2C%20we%20propose%20to%20emphasize%20the%20boundary%20information%20at%20two%20stages%20to%0Aalleviate%20this%20problem.%20In%20the%20first%20feature%20extraction%20stage%2C%20we%20introduce%20an%0Aintermediate%20bottleneck%20module%20to%20jointly%20learn%20both%20category%20and%20boundary%0Afeatures%20of%20each%20dense%20modality%20through%20the%20inner%20loss%20functions.%20After%20the%0Afusion%20of%20dense%20modalities%20and%20sparse%20signals%2C%20we%20then%20devise%20a%20two-branch%0Aarchitecture%20that%20explicitly%20models%20the%20interrelationship%20between%20action%0Acategory%20and%20temporal%20boundary.%20Experimental%20results%20demonstrate%20that%20SigFormer%0Aoutperforms%20the%20state-of-the-art%20approaches%20on%20a%20multi-modal%20action%0Asegmentation%20dataset%20from%20real%20industrial%20environments%2C%20reaching%20an%20outstanding%0AF1%20score%20of%200.958.%20The%20codes%20and%20pre-trained%20models%20have%20been%20available%20at%0Ahttps%3A//github.com/LIUQI-creat/SigFormer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.17428v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSigFormer%253A%2520Sparse%2520Signal-Guided%2520Transformer%2520for%2520Multi-Modal%2520Human%2520Action%250A%2520%2520Segmentation%26entry.906535625%3DQi%2520Liu%2520and%2520Xinchen%2520Liu%2520and%2520Kun%2520Liu%2520and%2520Xiaoyan%2520Gu%2520and%2520Wu%2520Liu%26entry.1292438233%3D%2520%2520Multi-modal%2520human%2520action%2520segmentation%2520is%2520a%2520critical%2520and%2520challenging%2520task%2520with%250Aa%2520wide%2520range%2520of%2520applications.%2520Nowadays%252C%2520the%2520majority%2520of%2520approaches%2520concentrate%250Aon%2520the%2520fusion%2520of%2520dense%2520signals%2520%2528i.e.%252C%2520RGB%252C%2520optical%2520flow%252C%2520and%2520depth%2520maps%2529.%250AHowever%252C%2520the%2520potential%2520contributions%2520of%2520sparse%2520IoT%2520sensor%2520signals%252C%2520which%2520can%2520be%250Acrucial%2520for%2520achieving%2520accurate%2520recognition%252C%2520have%2520not%2520been%2520fully%2520explored.%2520To%250Amake%2520up%2520for%2520this%252C%2520we%2520introduce%2520a%2520Sparse%2520signalguided%2520Transformer%2520%2528SigFormer%2529%2520to%250Acombine%2520both%2520dense%2520and%2520sparse%2520signals.%2520We%2520employ%2520mask%2520attention%2520to%2520fuse%250Alocalized%2520features%2520by%2520constraining%2520cross-attention%2520within%2520the%2520regions%2520where%250Asparse%2520signals%2520are%2520valid.%2520However%252C%2520since%2520sparse%2520signals%2520are%2520discrete%252C%2520they%2520lack%250Asufficient%2520information%2520about%2520the%2520temporal%2520action%2520boundaries.%2520Therefore%252C%2520in%250ASigFormer%252C%2520we%2520propose%2520to%2520emphasize%2520the%2520boundary%2520information%2520at%2520two%2520stages%2520to%250Aalleviate%2520this%2520problem.%2520In%2520the%2520first%2520feature%2520extraction%2520stage%252C%2520we%2520introduce%2520an%250Aintermediate%2520bottleneck%2520module%2520to%2520jointly%2520learn%2520both%2520category%2520and%2520boundary%250Afeatures%2520of%2520each%2520dense%2520modality%2520through%2520the%2520inner%2520loss%2520functions.%2520After%2520the%250Afusion%2520of%2520dense%2520modalities%2520and%2520sparse%2520signals%252C%2520we%2520then%2520devise%2520a%2520two-branch%250Aarchitecture%2520that%2520explicitly%2520models%2520the%2520interrelationship%2520between%2520action%250Acategory%2520and%2520temporal%2520boundary.%2520Experimental%2520results%2520demonstrate%2520that%2520SigFormer%250Aoutperforms%2520the%2520state-of-the-art%2520approaches%2520on%2520a%2520multi-modal%2520action%250Asegmentation%2520dataset%2520from%2520real%2520industrial%2520environments%252C%2520reaching%2520an%2520outstanding%250AF1%2520score%2520of%25200.958.%2520The%2520codes%2520and%2520pre-trained%2520models%2520have%2520been%2520available%2520at%250Ahttps%253A//github.com/LIUQI-creat/SigFormer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.17428v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SigFormer%3A%20Sparse%20Signal-Guided%20Transformer%20for%20Multi-Modal%20Human%20Action%0A%20%20Segmentation&entry.906535625=Qi%20Liu%20and%20Xinchen%20Liu%20and%20Kun%20Liu%20and%20Xiaoyan%20Gu%20and%20Wu%20Liu&entry.1292438233=%20%20Multi-modal%20human%20action%20segmentation%20is%20a%20critical%20and%20challenging%20task%20with%0Aa%20wide%20range%20of%20applications.%20Nowadays%2C%20the%20majority%20of%20approaches%20concentrate%0Aon%20the%20fusion%20of%20dense%20signals%20%28i.e.%2C%20RGB%2C%20optical%20flow%2C%20and%20depth%20maps%29.%0AHowever%2C%20the%20potential%20contributions%20of%20sparse%20IoT%20sensor%20signals%2C%20which%20can%20be%0Acrucial%20for%20achieving%20accurate%20recognition%2C%20have%20not%20been%20fully%20explored.%20To%0Amake%20up%20for%20this%2C%20we%20introduce%20a%20Sparse%20signalguided%20Transformer%20%28SigFormer%29%20to%0Acombine%20both%20dense%20and%20sparse%20signals.%20We%20employ%20mask%20attention%20to%20fuse%0Alocalized%20features%20by%20constraining%20cross-attention%20within%20the%20regions%20where%0Asparse%20signals%20are%20valid.%20However%2C%20since%20sparse%20signals%20are%20discrete%2C%20they%20lack%0Asufficient%20information%20about%20the%20temporal%20action%20boundaries.%20Therefore%2C%20in%0ASigFormer%2C%20we%20propose%20to%20emphasize%20the%20boundary%20information%20at%20two%20stages%20to%0Aalleviate%20this%20problem.%20In%20the%20first%20feature%20extraction%20stage%2C%20we%20introduce%20an%0Aintermediate%20bottleneck%20module%20to%20jointly%20learn%20both%20category%20and%20boundary%0Afeatures%20of%20each%20dense%20modality%20through%20the%20inner%20loss%20functions.%20After%20the%0Afusion%20of%20dense%20modalities%20and%20sparse%20signals%2C%20we%20then%20devise%20a%20two-branch%0Aarchitecture%20that%20explicitly%20models%20the%20interrelationship%20between%20action%0Acategory%20and%20temporal%20boundary.%20Experimental%20results%20demonstrate%20that%20SigFormer%0Aoutperforms%20the%20state-of-the-art%20approaches%20on%20a%20multi-modal%20action%0Asegmentation%20dataset%20from%20real%20industrial%20environments%2C%20reaching%20an%20outstanding%0AF1%20score%20of%200.958.%20The%20codes%20and%20pre-trained%20models%20have%20been%20available%20at%0Ahttps%3A//github.com/LIUQI-creat/SigFormer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.17428v2&entry.124074799=Read"},
{"title": "NimbleD: Enhancing Self-supervised Monocular Depth Estimation with\n  Pseudo-labels and Large-scale Video Pre-training", "author": "Albert Luginov and Muhammad Shahzad", "abstract": "  We introduce NimbleD, an efficient self-supervised monocular depth estimation\nlearning framework that incorporates supervision from pseudo-labels generated\nby a large vision model. This framework does not require camera intrinsics,\nenabling large-scale pre-training on publicly available videos. Our\nstraightforward yet effective learning strategy significantly enhances the\nperformance of fast and lightweight models without introducing any overhead,\nallowing them to achieve performance comparable to state-of-the-art\nself-supervised monocular depth estimation models. This advancement is\nparticularly beneficial for virtual and augmented reality applications\nrequiring low latency inference. The source code, model weights, and\nacknowledgments are available at https://github.com/xapaxca/nimbled .\n", "link": "http://arxiv.org/abs/2408.14177v1", "date": "2024-08-26", "relevancy": 2.2613, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6033}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5625}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NimbleD%3A%20Enhancing%20Self-supervised%20Monocular%20Depth%20Estimation%20with%0A%20%20Pseudo-labels%20and%20Large-scale%20Video%20Pre-training&body=Title%3A%20NimbleD%3A%20Enhancing%20Self-supervised%20Monocular%20Depth%20Estimation%20with%0A%20%20Pseudo-labels%20and%20Large-scale%20Video%20Pre-training%0AAuthor%3A%20Albert%20Luginov%20and%20Muhammad%20Shahzad%0AAbstract%3A%20%20%20We%20introduce%20NimbleD%2C%20an%20efficient%20self-supervised%20monocular%20depth%20estimation%0Alearning%20framework%20that%20incorporates%20supervision%20from%20pseudo-labels%20generated%0Aby%20a%20large%20vision%20model.%20This%20framework%20does%20not%20require%20camera%20intrinsics%2C%0Aenabling%20large-scale%20pre-training%20on%20publicly%20available%20videos.%20Our%0Astraightforward%20yet%20effective%20learning%20strategy%20significantly%20enhances%20the%0Aperformance%20of%20fast%20and%20lightweight%20models%20without%20introducing%20any%20overhead%2C%0Aallowing%20them%20to%20achieve%20performance%20comparable%20to%20state-of-the-art%0Aself-supervised%20monocular%20depth%20estimation%20models.%20This%20advancement%20is%0Aparticularly%20beneficial%20for%20virtual%20and%20augmented%20reality%20applications%0Arequiring%20low%20latency%20inference.%20The%20source%20code%2C%20model%20weights%2C%20and%0Aacknowledgments%20are%20available%20at%20https%3A//github.com/xapaxca/nimbled%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNimbleD%253A%2520Enhancing%2520Self-supervised%2520Monocular%2520Depth%2520Estimation%2520with%250A%2520%2520Pseudo-labels%2520and%2520Large-scale%2520Video%2520Pre-training%26entry.906535625%3DAlbert%2520Luginov%2520and%2520Muhammad%2520Shahzad%26entry.1292438233%3D%2520%2520We%2520introduce%2520NimbleD%252C%2520an%2520efficient%2520self-supervised%2520monocular%2520depth%2520estimation%250Alearning%2520framework%2520that%2520incorporates%2520supervision%2520from%2520pseudo-labels%2520generated%250Aby%2520a%2520large%2520vision%2520model.%2520This%2520framework%2520does%2520not%2520require%2520camera%2520intrinsics%252C%250Aenabling%2520large-scale%2520pre-training%2520on%2520publicly%2520available%2520videos.%2520Our%250Astraightforward%2520yet%2520effective%2520learning%2520strategy%2520significantly%2520enhances%2520the%250Aperformance%2520of%2520fast%2520and%2520lightweight%2520models%2520without%2520introducing%2520any%2520overhead%252C%250Aallowing%2520them%2520to%2520achieve%2520performance%2520comparable%2520to%2520state-of-the-art%250Aself-supervised%2520monocular%2520depth%2520estimation%2520models.%2520This%2520advancement%2520is%250Aparticularly%2520beneficial%2520for%2520virtual%2520and%2520augmented%2520reality%2520applications%250Arequiring%2520low%2520latency%2520inference.%2520The%2520source%2520code%252C%2520model%2520weights%252C%2520and%250Aacknowledgments%2520are%2520available%2520at%2520https%253A//github.com/xapaxca/nimbled%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NimbleD%3A%20Enhancing%20Self-supervised%20Monocular%20Depth%20Estimation%20with%0A%20%20Pseudo-labels%20and%20Large-scale%20Video%20Pre-training&entry.906535625=Albert%20Luginov%20and%20Muhammad%20Shahzad&entry.1292438233=%20%20We%20introduce%20NimbleD%2C%20an%20efficient%20self-supervised%20monocular%20depth%20estimation%0Alearning%20framework%20that%20incorporates%20supervision%20from%20pseudo-labels%20generated%0Aby%20a%20large%20vision%20model.%20This%20framework%20does%20not%20require%20camera%20intrinsics%2C%0Aenabling%20large-scale%20pre-training%20on%20publicly%20available%20videos.%20Our%0Astraightforward%20yet%20effective%20learning%20strategy%20significantly%20enhances%20the%0Aperformance%20of%20fast%20and%20lightweight%20models%20without%20introducing%20any%20overhead%2C%0Aallowing%20them%20to%20achieve%20performance%20comparable%20to%20state-of-the-art%0Aself-supervised%20monocular%20depth%20estimation%20models.%20This%20advancement%20is%0Aparticularly%20beneficial%20for%20virtual%20and%20augmented%20reality%20applications%0Arequiring%20low%20latency%20inference.%20The%20source%20code%2C%20model%20weights%2C%20and%0Aacknowledgments%20are%20available%20at%20https%3A//github.com/xapaxca/nimbled%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14177v1&entry.124074799=Read"},
{"title": "Explaining Vision-Language Similarities in Dual Encoders with\n  Feature-Pair Attributions", "author": "Lucas M\u00f6ller and Pascal Tilli and Ngoc Thang Vu and Sebastian Pad\u00f3", "abstract": "  Dual encoder architectures like CLIP models map two types of inputs into a\nshared embedding space and learn similarities between them. However, it is not\nunderstood how such models compare two inputs. Here, we address this research\ngap with two contributions. First, we derive a method to attribute predictions\nof any differentiable dual encoder onto feature-pair interactions between its\ninputs. Second, we apply our method to CLIP-type models and show that they\nlearn fine-grained correspondences between parts of captions and regions in\nimages. They match objects across input modes and also account for mismatches.\nHowever, this visual-linguistic grounding ability heavily varies between object\nclasses, depends on the training data distribution, and largely improves after\nin-domain training. Using our method we can identify knowledge gaps about\nspecific object classes in individual models and can monitor their improvement\nupon fine-tuning.\n", "link": "http://arxiv.org/abs/2408.14153v1", "date": "2024-08-26", "relevancy": 2.249, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5906}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5623}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5339}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explaining%20Vision-Language%20Similarities%20in%20Dual%20Encoders%20with%0A%20%20Feature-Pair%20Attributions&body=Title%3A%20Explaining%20Vision-Language%20Similarities%20in%20Dual%20Encoders%20with%0A%20%20Feature-Pair%20Attributions%0AAuthor%3A%20Lucas%20M%C3%B6ller%20and%20Pascal%20Tilli%20and%20Ngoc%20Thang%20Vu%20and%20Sebastian%20Pad%C3%B3%0AAbstract%3A%20%20%20Dual%20encoder%20architectures%20like%20CLIP%20models%20map%20two%20types%20of%20inputs%20into%20a%0Ashared%20embedding%20space%20and%20learn%20similarities%20between%20them.%20However%2C%20it%20is%20not%0Aunderstood%20how%20such%20models%20compare%20two%20inputs.%20Here%2C%20we%20address%20this%20research%0Agap%20with%20two%20contributions.%20First%2C%20we%20derive%20a%20method%20to%20attribute%20predictions%0Aof%20any%20differentiable%20dual%20encoder%20onto%20feature-pair%20interactions%20between%20its%0Ainputs.%20Second%2C%20we%20apply%20our%20method%20to%20CLIP-type%20models%20and%20show%20that%20they%0Alearn%20fine-grained%20correspondences%20between%20parts%20of%20captions%20and%20regions%20in%0Aimages.%20They%20match%20objects%20across%20input%20modes%20and%20also%20account%20for%20mismatches.%0AHowever%2C%20this%20visual-linguistic%20grounding%20ability%20heavily%20varies%20between%20object%0Aclasses%2C%20depends%20on%20the%20training%20data%20distribution%2C%20and%20largely%20improves%20after%0Ain-domain%20training.%20Using%20our%20method%20we%20can%20identify%20knowledge%20gaps%20about%0Aspecific%20object%20classes%20in%20individual%20models%20and%20can%20monitor%20their%20improvement%0Aupon%20fine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14153v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplaining%2520Vision-Language%2520Similarities%2520in%2520Dual%2520Encoders%2520with%250A%2520%2520Feature-Pair%2520Attributions%26entry.906535625%3DLucas%2520M%25C3%25B6ller%2520and%2520Pascal%2520Tilli%2520and%2520Ngoc%2520Thang%2520Vu%2520and%2520Sebastian%2520Pad%25C3%25B3%26entry.1292438233%3D%2520%2520Dual%2520encoder%2520architectures%2520like%2520CLIP%2520models%2520map%2520two%2520types%2520of%2520inputs%2520into%2520a%250Ashared%2520embedding%2520space%2520and%2520learn%2520similarities%2520between%2520them.%2520However%252C%2520it%2520is%2520not%250Aunderstood%2520how%2520such%2520models%2520compare%2520two%2520inputs.%2520Here%252C%2520we%2520address%2520this%2520research%250Agap%2520with%2520two%2520contributions.%2520First%252C%2520we%2520derive%2520a%2520method%2520to%2520attribute%2520predictions%250Aof%2520any%2520differentiable%2520dual%2520encoder%2520onto%2520feature-pair%2520interactions%2520between%2520its%250Ainputs.%2520Second%252C%2520we%2520apply%2520our%2520method%2520to%2520CLIP-type%2520models%2520and%2520show%2520that%2520they%250Alearn%2520fine-grained%2520correspondences%2520between%2520parts%2520of%2520captions%2520and%2520regions%2520in%250Aimages.%2520They%2520match%2520objects%2520across%2520input%2520modes%2520and%2520also%2520account%2520for%2520mismatches.%250AHowever%252C%2520this%2520visual-linguistic%2520grounding%2520ability%2520heavily%2520varies%2520between%2520object%250Aclasses%252C%2520depends%2520on%2520the%2520training%2520data%2520distribution%252C%2520and%2520largely%2520improves%2520after%250Ain-domain%2520training.%2520Using%2520our%2520method%2520we%2520can%2520identify%2520knowledge%2520gaps%2520about%250Aspecific%2520object%2520classes%2520in%2520individual%2520models%2520and%2520can%2520monitor%2520their%2520improvement%250Aupon%2520fine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14153v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explaining%20Vision-Language%20Similarities%20in%20Dual%20Encoders%20with%0A%20%20Feature-Pair%20Attributions&entry.906535625=Lucas%20M%C3%B6ller%20and%20Pascal%20Tilli%20and%20Ngoc%20Thang%20Vu%20and%20Sebastian%20Pad%C3%B3&entry.1292438233=%20%20Dual%20encoder%20architectures%20like%20CLIP%20models%20map%20two%20types%20of%20inputs%20into%20a%0Ashared%20embedding%20space%20and%20learn%20similarities%20between%20them.%20However%2C%20it%20is%20not%0Aunderstood%20how%20such%20models%20compare%20two%20inputs.%20Here%2C%20we%20address%20this%20research%0Agap%20with%20two%20contributions.%20First%2C%20we%20derive%20a%20method%20to%20attribute%20predictions%0Aof%20any%20differentiable%20dual%20encoder%20onto%20feature-pair%20interactions%20between%20its%0Ainputs.%20Second%2C%20we%20apply%20our%20method%20to%20CLIP-type%20models%20and%20show%20that%20they%0Alearn%20fine-grained%20correspondences%20between%20parts%20of%20captions%20and%20regions%20in%0Aimages.%20They%20match%20objects%20across%20input%20modes%20and%20also%20account%20for%20mismatches.%0AHowever%2C%20this%20visual-linguistic%20grounding%20ability%20heavily%20varies%20between%20object%0Aclasses%2C%20depends%20on%20the%20training%20data%20distribution%2C%20and%20largely%20improves%20after%0Ain-domain%20training.%20Using%20our%20method%20we%20can%20identify%20knowledge%20gaps%20about%0Aspecific%20object%20classes%20in%20individual%20models%20and%20can%20monitor%20their%20improvement%0Aupon%20fine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14153v1&entry.124074799=Read"},
{"title": "Hierarchical Generative Modeling of Melodic Vocal Contours in Hindustani\n  Classical Music", "author": "Nithya Shikarpur and Krishna Maneesha Dendukuri and Yusong Wu and Antoine Caillon and Cheng-Zhi Anna Huang", "abstract": "  Hindustani music is a performance-driven oral tradition that exhibits the\nrendition of rich melodic patterns. In this paper, we focus on generative\nmodeling of singers' vocal melodies extracted from audio recordings, as the\nvoice is musically prominent within the tradition. Prior generative work in\nHindustani music models melodies as coarse discrete symbols which fails to\ncapture the rich expressive melodic intricacies of singing. Thus, we propose to\nuse a finely quantized pitch contour, as an intermediate representation for\nhierarchical audio modeling. We propose GaMaDHaNi, a modular two-level\nhierarchy, consisting of a generative model on pitch contours, and a pitch\ncontour to audio synthesis model. We compare our approach to non-hierarchical\naudio models and hierarchical models that use a self-supervised intermediate\nrepresentation, through a listening test and qualitative analysis. We also\nevaluate audio model's ability to faithfully represent the pitch contour input\nusing Pearson correlation coefficient. By using pitch contours as an\nintermediate representation, we show that our model may be better equipped to\nlisten and respond to musicians in a human-AI collaborative setting by\nhighlighting two potential interaction use cases (1) primed generation, and (2)\ncoarse pitch conditioning.\n", "link": "http://arxiv.org/abs/2408.12658v2", "date": "2024-08-26", "relevancy": 2.2405, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4488}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4488}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Generative%20Modeling%20of%20Melodic%20Vocal%20Contours%20in%20Hindustani%0A%20%20Classical%20Music&body=Title%3A%20Hierarchical%20Generative%20Modeling%20of%20Melodic%20Vocal%20Contours%20in%20Hindustani%0A%20%20Classical%20Music%0AAuthor%3A%20Nithya%20Shikarpur%20and%20Krishna%20Maneesha%20Dendukuri%20and%20Yusong%20Wu%20and%20Antoine%20Caillon%20and%20Cheng-Zhi%20Anna%20Huang%0AAbstract%3A%20%20%20Hindustani%20music%20is%20a%20performance-driven%20oral%20tradition%20that%20exhibits%20the%0Arendition%20of%20rich%20melodic%20patterns.%20In%20this%20paper%2C%20we%20focus%20on%20generative%0Amodeling%20of%20singers%27%20vocal%20melodies%20extracted%20from%20audio%20recordings%2C%20as%20the%0Avoice%20is%20musically%20prominent%20within%20the%20tradition.%20Prior%20generative%20work%20in%0AHindustani%20music%20models%20melodies%20as%20coarse%20discrete%20symbols%20which%20fails%20to%0Acapture%20the%20rich%20expressive%20melodic%20intricacies%20of%20singing.%20Thus%2C%20we%20propose%20to%0Ause%20a%20finely%20quantized%20pitch%20contour%2C%20as%20an%20intermediate%20representation%20for%0Ahierarchical%20audio%20modeling.%20We%20propose%20GaMaDHaNi%2C%20a%20modular%20two-level%0Ahierarchy%2C%20consisting%20of%20a%20generative%20model%20on%20pitch%20contours%2C%20and%20a%20pitch%0Acontour%20to%20audio%20synthesis%20model.%20We%20compare%20our%20approach%20to%20non-hierarchical%0Aaudio%20models%20and%20hierarchical%20models%20that%20use%20a%20self-supervised%20intermediate%0Arepresentation%2C%20through%20a%20listening%20test%20and%20qualitative%20analysis.%20We%20also%0Aevaluate%20audio%20model%27s%20ability%20to%20faithfully%20represent%20the%20pitch%20contour%20input%0Ausing%20Pearson%20correlation%20coefficient.%20By%20using%20pitch%20contours%20as%20an%0Aintermediate%20representation%2C%20we%20show%20that%20our%20model%20may%20be%20better%20equipped%20to%0Alisten%20and%20respond%20to%20musicians%20in%20a%20human-AI%20collaborative%20setting%20by%0Ahighlighting%20two%20potential%20interaction%20use%20cases%20%281%29%20primed%20generation%2C%20and%20%282%29%0Acoarse%20pitch%20conditioning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12658v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Generative%2520Modeling%2520of%2520Melodic%2520Vocal%2520Contours%2520in%2520Hindustani%250A%2520%2520Classical%2520Music%26entry.906535625%3DNithya%2520Shikarpur%2520and%2520Krishna%2520Maneesha%2520Dendukuri%2520and%2520Yusong%2520Wu%2520and%2520Antoine%2520Caillon%2520and%2520Cheng-Zhi%2520Anna%2520Huang%26entry.1292438233%3D%2520%2520Hindustani%2520music%2520is%2520a%2520performance-driven%2520oral%2520tradition%2520that%2520exhibits%2520the%250Arendition%2520of%2520rich%2520melodic%2520patterns.%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520generative%250Amodeling%2520of%2520singers%2527%2520vocal%2520melodies%2520extracted%2520from%2520audio%2520recordings%252C%2520as%2520the%250Avoice%2520is%2520musically%2520prominent%2520within%2520the%2520tradition.%2520Prior%2520generative%2520work%2520in%250AHindustani%2520music%2520models%2520melodies%2520as%2520coarse%2520discrete%2520symbols%2520which%2520fails%2520to%250Acapture%2520the%2520rich%2520expressive%2520melodic%2520intricacies%2520of%2520singing.%2520Thus%252C%2520we%2520propose%2520to%250Ause%2520a%2520finely%2520quantized%2520pitch%2520contour%252C%2520as%2520an%2520intermediate%2520representation%2520for%250Ahierarchical%2520audio%2520modeling.%2520We%2520propose%2520GaMaDHaNi%252C%2520a%2520modular%2520two-level%250Ahierarchy%252C%2520consisting%2520of%2520a%2520generative%2520model%2520on%2520pitch%2520contours%252C%2520and%2520a%2520pitch%250Acontour%2520to%2520audio%2520synthesis%2520model.%2520We%2520compare%2520our%2520approach%2520to%2520non-hierarchical%250Aaudio%2520models%2520and%2520hierarchical%2520models%2520that%2520use%2520a%2520self-supervised%2520intermediate%250Arepresentation%252C%2520through%2520a%2520listening%2520test%2520and%2520qualitative%2520analysis.%2520We%2520also%250Aevaluate%2520audio%2520model%2527s%2520ability%2520to%2520faithfully%2520represent%2520the%2520pitch%2520contour%2520input%250Ausing%2520Pearson%2520correlation%2520coefficient.%2520By%2520using%2520pitch%2520contours%2520as%2520an%250Aintermediate%2520representation%252C%2520we%2520show%2520that%2520our%2520model%2520may%2520be%2520better%2520equipped%2520to%250Alisten%2520and%2520respond%2520to%2520musicians%2520in%2520a%2520human-AI%2520collaborative%2520setting%2520by%250Ahighlighting%2520two%2520potential%2520interaction%2520use%2520cases%2520%25281%2529%2520primed%2520generation%252C%2520and%2520%25282%2529%250Acoarse%2520pitch%2520conditioning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12658v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Generative%20Modeling%20of%20Melodic%20Vocal%20Contours%20in%20Hindustani%0A%20%20Classical%20Music&entry.906535625=Nithya%20Shikarpur%20and%20Krishna%20Maneesha%20Dendukuri%20and%20Yusong%20Wu%20and%20Antoine%20Caillon%20and%20Cheng-Zhi%20Anna%20Huang&entry.1292438233=%20%20Hindustani%20music%20is%20a%20performance-driven%20oral%20tradition%20that%20exhibits%20the%0Arendition%20of%20rich%20melodic%20patterns.%20In%20this%20paper%2C%20we%20focus%20on%20generative%0Amodeling%20of%20singers%27%20vocal%20melodies%20extracted%20from%20audio%20recordings%2C%20as%20the%0Avoice%20is%20musically%20prominent%20within%20the%20tradition.%20Prior%20generative%20work%20in%0AHindustani%20music%20models%20melodies%20as%20coarse%20discrete%20symbols%20which%20fails%20to%0Acapture%20the%20rich%20expressive%20melodic%20intricacies%20of%20singing.%20Thus%2C%20we%20propose%20to%0Ause%20a%20finely%20quantized%20pitch%20contour%2C%20as%20an%20intermediate%20representation%20for%0Ahierarchical%20audio%20modeling.%20We%20propose%20GaMaDHaNi%2C%20a%20modular%20two-level%0Ahierarchy%2C%20consisting%20of%20a%20generative%20model%20on%20pitch%20contours%2C%20and%20a%20pitch%0Acontour%20to%20audio%20synthesis%20model.%20We%20compare%20our%20approach%20to%20non-hierarchical%0Aaudio%20models%20and%20hierarchical%20models%20that%20use%20a%20self-supervised%20intermediate%0Arepresentation%2C%20through%20a%20listening%20test%20and%20qualitative%20analysis.%20We%20also%0Aevaluate%20audio%20model%27s%20ability%20to%20faithfully%20represent%20the%20pitch%20contour%20input%0Ausing%20Pearson%20correlation%20coefficient.%20By%20using%20pitch%20contours%20as%20an%0Aintermediate%20representation%2C%20we%20show%20that%20our%20model%20may%20be%20better%20equipped%20to%0Alisten%20and%20respond%20to%20musicians%20in%20a%20human-AI%20collaborative%20setting%20by%0Ahighlighting%20two%20potential%20interaction%20use%20cases%20%281%29%20primed%20generation%2C%20and%20%282%29%0Acoarse%20pitch%20conditioning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12658v2&entry.124074799=Read"},
{"title": "LoG-VMamba: Local-Global Vision Mamba for Medical Image Segmentation", "author": "Trung Dinh Quoc Dang and Huy Hoang Nguyen and Aleksei Tiulpin", "abstract": "  Mamba, a State Space Model (SSM), has recently shown competitive performance\nto Convolutional Neural Networks (CNNs) and Transformers in Natural Language\nProcessing and general sequence modeling. Various attempts have been made to\nadapt Mamba to Computer Vision tasks, including medical image segmentation\n(MIS). Vision Mamba (VM)-based networks are particularly attractive due to\ntheir ability to achieve global receptive fields, similar to Vision\nTransformers, while also maintaining linear complexity in the number of tokens.\nHowever, the existing VM models still struggle to maintain both spatially local\nand global dependencies of tokens in high dimensional arrays due to their\nsequential nature. Employing multiple and/or complicated scanning strategies is\ncomputationally costly, which hinders applications of SSMs to high-dimensional\n2D and 3D images that are common in MIS problems. In this work, we propose\nLocal-Global Vision Mamba, LoG-VMamba, that explicitly enforces spatially\nadjacent tokens to remain nearby on the channel axis, and retains the global\ncontext in a compressed form. Our method allows the SSMs to access the local\nand global contexts even before reaching the last token while requiring only a\nsimple scanning strategy. Our segmentation models are computationally efficient\nand substantially outperform both CNN and Transformers-based baselines on a\ndiverse set of 2D and 3D MIS tasks. The implementation of LoG-VMamba is\navailable at \\url{https://github.com/Oulu-IMEDS/LoG-VMamba}.\n", "link": "http://arxiv.org/abs/2408.14415v1", "date": "2024-08-26", "relevancy": 2.1894, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5566}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5491}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5198}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoG-VMamba%3A%20Local-Global%20Vision%20Mamba%20for%20Medical%20Image%20Segmentation&body=Title%3A%20LoG-VMamba%3A%20Local-Global%20Vision%20Mamba%20for%20Medical%20Image%20Segmentation%0AAuthor%3A%20Trung%20Dinh%20Quoc%20Dang%20and%20Huy%20Hoang%20Nguyen%20and%20Aleksei%20Tiulpin%0AAbstract%3A%20%20%20Mamba%2C%20a%20State%20Space%20Model%20%28SSM%29%2C%20has%20recently%20shown%20competitive%20performance%0Ato%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%20Transformers%20in%20Natural%20Language%0AProcessing%20and%20general%20sequence%20modeling.%20Various%20attempts%20have%20been%20made%20to%0Aadapt%20Mamba%20to%20Computer%20Vision%20tasks%2C%20including%20medical%20image%20segmentation%0A%28MIS%29.%20Vision%20Mamba%20%28VM%29-based%20networks%20are%20particularly%20attractive%20due%20to%0Atheir%20ability%20to%20achieve%20global%20receptive%20fields%2C%20similar%20to%20Vision%0ATransformers%2C%20while%20also%20maintaining%20linear%20complexity%20in%20the%20number%20of%20tokens.%0AHowever%2C%20the%20existing%20VM%20models%20still%20struggle%20to%20maintain%20both%20spatially%20local%0Aand%20global%20dependencies%20of%20tokens%20in%20high%20dimensional%20arrays%20due%20to%20their%0Asequential%20nature.%20Employing%20multiple%20and/or%20complicated%20scanning%20strategies%20is%0Acomputationally%20costly%2C%20which%20hinders%20applications%20of%20SSMs%20to%20high-dimensional%0A2D%20and%203D%20images%20that%20are%20common%20in%20MIS%20problems.%20In%20this%20work%2C%20we%20propose%0ALocal-Global%20Vision%20Mamba%2C%20LoG-VMamba%2C%20that%20explicitly%20enforces%20spatially%0Aadjacent%20tokens%20to%20remain%20nearby%20on%20the%20channel%20axis%2C%20and%20retains%20the%20global%0Acontext%20in%20a%20compressed%20form.%20Our%20method%20allows%20the%20SSMs%20to%20access%20the%20local%0Aand%20global%20contexts%20even%20before%20reaching%20the%20last%20token%20while%20requiring%20only%20a%0Asimple%20scanning%20strategy.%20Our%20segmentation%20models%20are%20computationally%20efficient%0Aand%20substantially%20outperform%20both%20CNN%20and%20Transformers-based%20baselines%20on%20a%0Adiverse%20set%20of%202D%20and%203D%20MIS%20tasks.%20The%20implementation%20of%20LoG-VMamba%20is%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/Oulu-IMEDS/LoG-VMamba%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14415v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoG-VMamba%253A%2520Local-Global%2520Vision%2520Mamba%2520for%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DTrung%2520Dinh%2520Quoc%2520Dang%2520and%2520Huy%2520Hoang%2520Nguyen%2520and%2520Aleksei%2520Tiulpin%26entry.1292438233%3D%2520%2520Mamba%252C%2520a%2520State%2520Space%2520Model%2520%2528SSM%2529%252C%2520has%2520recently%2520shown%2520competitive%2520performance%250Ato%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520and%2520Transformers%2520in%2520Natural%2520Language%250AProcessing%2520and%2520general%2520sequence%2520modeling.%2520Various%2520attempts%2520have%2520been%2520made%2520to%250Aadapt%2520Mamba%2520to%2520Computer%2520Vision%2520tasks%252C%2520including%2520medical%2520image%2520segmentation%250A%2528MIS%2529.%2520Vision%2520Mamba%2520%2528VM%2529-based%2520networks%2520are%2520particularly%2520attractive%2520due%2520to%250Atheir%2520ability%2520to%2520achieve%2520global%2520receptive%2520fields%252C%2520similar%2520to%2520Vision%250ATransformers%252C%2520while%2520also%2520maintaining%2520linear%2520complexity%2520in%2520the%2520number%2520of%2520tokens.%250AHowever%252C%2520the%2520existing%2520VM%2520models%2520still%2520struggle%2520to%2520maintain%2520both%2520spatially%2520local%250Aand%2520global%2520dependencies%2520of%2520tokens%2520in%2520high%2520dimensional%2520arrays%2520due%2520to%2520their%250Asequential%2520nature.%2520Employing%2520multiple%2520and/or%2520complicated%2520scanning%2520strategies%2520is%250Acomputationally%2520costly%252C%2520which%2520hinders%2520applications%2520of%2520SSMs%2520to%2520high-dimensional%250A2D%2520and%25203D%2520images%2520that%2520are%2520common%2520in%2520MIS%2520problems.%2520In%2520this%2520work%252C%2520we%2520propose%250ALocal-Global%2520Vision%2520Mamba%252C%2520LoG-VMamba%252C%2520that%2520explicitly%2520enforces%2520spatially%250Aadjacent%2520tokens%2520to%2520remain%2520nearby%2520on%2520the%2520channel%2520axis%252C%2520and%2520retains%2520the%2520global%250Acontext%2520in%2520a%2520compressed%2520form.%2520Our%2520method%2520allows%2520the%2520SSMs%2520to%2520access%2520the%2520local%250Aand%2520global%2520contexts%2520even%2520before%2520reaching%2520the%2520last%2520token%2520while%2520requiring%2520only%2520a%250Asimple%2520scanning%2520strategy.%2520Our%2520segmentation%2520models%2520are%2520computationally%2520efficient%250Aand%2520substantially%2520outperform%2520both%2520CNN%2520and%2520Transformers-based%2520baselines%2520on%2520a%250Adiverse%2520set%2520of%25202D%2520and%25203D%2520MIS%2520tasks.%2520The%2520implementation%2520of%2520LoG-VMamba%2520is%250Aavailable%2520at%2520%255Curl%257Bhttps%253A//github.com/Oulu-IMEDS/LoG-VMamba%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14415v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoG-VMamba%3A%20Local-Global%20Vision%20Mamba%20for%20Medical%20Image%20Segmentation&entry.906535625=Trung%20Dinh%20Quoc%20Dang%20and%20Huy%20Hoang%20Nguyen%20and%20Aleksei%20Tiulpin&entry.1292438233=%20%20Mamba%2C%20a%20State%20Space%20Model%20%28SSM%29%2C%20has%20recently%20shown%20competitive%20performance%0Ato%20Convolutional%20Neural%20Networks%20%28CNNs%29%20and%20Transformers%20in%20Natural%20Language%0AProcessing%20and%20general%20sequence%20modeling.%20Various%20attempts%20have%20been%20made%20to%0Aadapt%20Mamba%20to%20Computer%20Vision%20tasks%2C%20including%20medical%20image%20segmentation%0A%28MIS%29.%20Vision%20Mamba%20%28VM%29-based%20networks%20are%20particularly%20attractive%20due%20to%0Atheir%20ability%20to%20achieve%20global%20receptive%20fields%2C%20similar%20to%20Vision%0ATransformers%2C%20while%20also%20maintaining%20linear%20complexity%20in%20the%20number%20of%20tokens.%0AHowever%2C%20the%20existing%20VM%20models%20still%20struggle%20to%20maintain%20both%20spatially%20local%0Aand%20global%20dependencies%20of%20tokens%20in%20high%20dimensional%20arrays%20due%20to%20their%0Asequential%20nature.%20Employing%20multiple%20and/or%20complicated%20scanning%20strategies%20is%0Acomputationally%20costly%2C%20which%20hinders%20applications%20of%20SSMs%20to%20high-dimensional%0A2D%20and%203D%20images%20that%20are%20common%20in%20MIS%20problems.%20In%20this%20work%2C%20we%20propose%0ALocal-Global%20Vision%20Mamba%2C%20LoG-VMamba%2C%20that%20explicitly%20enforces%20spatially%0Aadjacent%20tokens%20to%20remain%20nearby%20on%20the%20channel%20axis%2C%20and%20retains%20the%20global%0Acontext%20in%20a%20compressed%20form.%20Our%20method%20allows%20the%20SSMs%20to%20access%20the%20local%0Aand%20global%20contexts%20even%20before%20reaching%20the%20last%20token%20while%20requiring%20only%20a%0Asimple%20scanning%20strategy.%20Our%20segmentation%20models%20are%20computationally%20efficient%0Aand%20substantially%20outperform%20both%20CNN%20and%20Transformers-based%20baselines%20on%20a%0Adiverse%20set%20of%202D%20and%203D%20MIS%20tasks.%20The%20implementation%20of%20LoG-VMamba%20is%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/Oulu-IMEDS/LoG-VMamba%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14415v1&entry.124074799=Read"},
{"title": "EMDFNet: Efficient Multi-scale and Diverse Feature Network for Traffic\n  Sign Detection", "author": "Pengyu Li and Chenhe Liu and Tengfei Li and Xinyu Wang and Shihui Zhang and Dongyang Yu", "abstract": "  The detection of small objects, particularly traffic signs, is a critical\nsubtask within object detection and autonomous driving. Despite the notable\nachievements in previous research, two primary challenges persist. Firstly, the\nmain issue is the singleness of feature extraction. Secondly, the detection\nprocess fails to effectively integrate with objects of varying sizes or scales.\nThese issues are also prevalent in generic object detection. Motivated by these\nchallenges, in this paper, we propose a novel object detection network named\nEfficient Multi-scale and Diverse Feature Network (EMDFNet) for traffic sign\ndetection that integrates an Augmented Shortcut Module and an Efficient Hybrid\nEncoder to address the aforementioned issues simultaneously. Specifically, the\nAugmented Shortcut Module utilizes multiple branches to integrate various\nspatial semantic information and channel semantic information, thereby\nenhancing feature diversity. The Efficient Hybrid Encoder utilizes global\nfeature fusion and local feature interaction based on various features to\ngenerate distinctive classification features by integrating feature information\nin an adaptable manner. Extensive experiments on the Tsinghua-Tencent 100K\n(TT100K) benchmark and the German Traffic Sign Detection Benchmark (GTSDB)\ndemonstrate that our EMDFNet outperforms other state-of-the-art detectors in\nperformance while retaining the real-time processing capabilities of\nsingle-stage models. This substantiates the effectiveness of EMDFNet in\ndetecting small traffic signs.\n", "link": "http://arxiv.org/abs/2408.14189v1", "date": "2024-08-26", "relevancy": 2.1662, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5486}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5446}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EMDFNet%3A%20Efficient%20Multi-scale%20and%20Diverse%20Feature%20Network%20for%20Traffic%0A%20%20Sign%20Detection&body=Title%3A%20EMDFNet%3A%20Efficient%20Multi-scale%20and%20Diverse%20Feature%20Network%20for%20Traffic%0A%20%20Sign%20Detection%0AAuthor%3A%20Pengyu%20Li%20and%20Chenhe%20Liu%20and%20Tengfei%20Li%20and%20Xinyu%20Wang%20and%20Shihui%20Zhang%20and%20Dongyang%20Yu%0AAbstract%3A%20%20%20The%20detection%20of%20small%20objects%2C%20particularly%20traffic%20signs%2C%20is%20a%20critical%0Asubtask%20within%20object%20detection%20and%20autonomous%20driving.%20Despite%20the%20notable%0Aachievements%20in%20previous%20research%2C%20two%20primary%20challenges%20persist.%20Firstly%2C%20the%0Amain%20issue%20is%20the%20singleness%20of%20feature%20extraction.%20Secondly%2C%20the%20detection%0Aprocess%20fails%20to%20effectively%20integrate%20with%20objects%20of%20varying%20sizes%20or%20scales.%0AThese%20issues%20are%20also%20prevalent%20in%20generic%20object%20detection.%20Motivated%20by%20these%0Achallenges%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%20object%20detection%20network%20named%0AEfficient%20Multi-scale%20and%20Diverse%20Feature%20Network%20%28EMDFNet%29%20for%20traffic%20sign%0Adetection%20that%20integrates%20an%20Augmented%20Shortcut%20Module%20and%20an%20Efficient%20Hybrid%0AEncoder%20to%20address%20the%20aforementioned%20issues%20simultaneously.%20Specifically%2C%20the%0AAugmented%20Shortcut%20Module%20utilizes%20multiple%20branches%20to%20integrate%20various%0Aspatial%20semantic%20information%20and%20channel%20semantic%20information%2C%20thereby%0Aenhancing%20feature%20diversity.%20The%20Efficient%20Hybrid%20Encoder%20utilizes%20global%0Afeature%20fusion%20and%20local%20feature%20interaction%20based%20on%20various%20features%20to%0Agenerate%20distinctive%20classification%20features%20by%20integrating%20feature%20information%0Ain%20an%20adaptable%20manner.%20Extensive%20experiments%20on%20the%20Tsinghua-Tencent%20100K%0A%28TT100K%29%20benchmark%20and%20the%20German%20Traffic%20Sign%20Detection%20Benchmark%20%28GTSDB%29%0Ademonstrate%20that%20our%20EMDFNet%20outperforms%20other%20state-of-the-art%20detectors%20in%0Aperformance%20while%20retaining%20the%20real-time%20processing%20capabilities%20of%0Asingle-stage%20models.%20This%20substantiates%20the%20effectiveness%20of%20EMDFNet%20in%0Adetecting%20small%20traffic%20signs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14189v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEMDFNet%253A%2520Efficient%2520Multi-scale%2520and%2520Diverse%2520Feature%2520Network%2520for%2520Traffic%250A%2520%2520Sign%2520Detection%26entry.906535625%3DPengyu%2520Li%2520and%2520Chenhe%2520Liu%2520and%2520Tengfei%2520Li%2520and%2520Xinyu%2520Wang%2520and%2520Shihui%2520Zhang%2520and%2520Dongyang%2520Yu%26entry.1292438233%3D%2520%2520The%2520detection%2520of%2520small%2520objects%252C%2520particularly%2520traffic%2520signs%252C%2520is%2520a%2520critical%250Asubtask%2520within%2520object%2520detection%2520and%2520autonomous%2520driving.%2520Despite%2520the%2520notable%250Aachievements%2520in%2520previous%2520research%252C%2520two%2520primary%2520challenges%2520persist.%2520Firstly%252C%2520the%250Amain%2520issue%2520is%2520the%2520singleness%2520of%2520feature%2520extraction.%2520Secondly%252C%2520the%2520detection%250Aprocess%2520fails%2520to%2520effectively%2520integrate%2520with%2520objects%2520of%2520varying%2520sizes%2520or%2520scales.%250AThese%2520issues%2520are%2520also%2520prevalent%2520in%2520generic%2520object%2520detection.%2520Motivated%2520by%2520these%250Achallenges%252C%2520in%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520object%2520detection%2520network%2520named%250AEfficient%2520Multi-scale%2520and%2520Diverse%2520Feature%2520Network%2520%2528EMDFNet%2529%2520for%2520traffic%2520sign%250Adetection%2520that%2520integrates%2520an%2520Augmented%2520Shortcut%2520Module%2520and%2520an%2520Efficient%2520Hybrid%250AEncoder%2520to%2520address%2520the%2520aforementioned%2520issues%2520simultaneously.%2520Specifically%252C%2520the%250AAugmented%2520Shortcut%2520Module%2520utilizes%2520multiple%2520branches%2520to%2520integrate%2520various%250Aspatial%2520semantic%2520information%2520and%2520channel%2520semantic%2520information%252C%2520thereby%250Aenhancing%2520feature%2520diversity.%2520The%2520Efficient%2520Hybrid%2520Encoder%2520utilizes%2520global%250Afeature%2520fusion%2520and%2520local%2520feature%2520interaction%2520based%2520on%2520various%2520features%2520to%250Agenerate%2520distinctive%2520classification%2520features%2520by%2520integrating%2520feature%2520information%250Ain%2520an%2520adaptable%2520manner.%2520Extensive%2520experiments%2520on%2520the%2520Tsinghua-Tencent%2520100K%250A%2528TT100K%2529%2520benchmark%2520and%2520the%2520German%2520Traffic%2520Sign%2520Detection%2520Benchmark%2520%2528GTSDB%2529%250Ademonstrate%2520that%2520our%2520EMDFNet%2520outperforms%2520other%2520state-of-the-art%2520detectors%2520in%250Aperformance%2520while%2520retaining%2520the%2520real-time%2520processing%2520capabilities%2520of%250Asingle-stage%2520models.%2520This%2520substantiates%2520the%2520effectiveness%2520of%2520EMDFNet%2520in%250Adetecting%2520small%2520traffic%2520signs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14189v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EMDFNet%3A%20Efficient%20Multi-scale%20and%20Diverse%20Feature%20Network%20for%20Traffic%0A%20%20Sign%20Detection&entry.906535625=Pengyu%20Li%20and%20Chenhe%20Liu%20and%20Tengfei%20Li%20and%20Xinyu%20Wang%20and%20Shihui%20Zhang%20and%20Dongyang%20Yu&entry.1292438233=%20%20The%20detection%20of%20small%20objects%2C%20particularly%20traffic%20signs%2C%20is%20a%20critical%0Asubtask%20within%20object%20detection%20and%20autonomous%20driving.%20Despite%20the%20notable%0Aachievements%20in%20previous%20research%2C%20two%20primary%20challenges%20persist.%20Firstly%2C%20the%0Amain%20issue%20is%20the%20singleness%20of%20feature%20extraction.%20Secondly%2C%20the%20detection%0Aprocess%20fails%20to%20effectively%20integrate%20with%20objects%20of%20varying%20sizes%20or%20scales.%0AThese%20issues%20are%20also%20prevalent%20in%20generic%20object%20detection.%20Motivated%20by%20these%0Achallenges%2C%20in%20this%20paper%2C%20we%20propose%20a%20novel%20object%20detection%20network%20named%0AEfficient%20Multi-scale%20and%20Diverse%20Feature%20Network%20%28EMDFNet%29%20for%20traffic%20sign%0Adetection%20that%20integrates%20an%20Augmented%20Shortcut%20Module%20and%20an%20Efficient%20Hybrid%0AEncoder%20to%20address%20the%20aforementioned%20issues%20simultaneously.%20Specifically%2C%20the%0AAugmented%20Shortcut%20Module%20utilizes%20multiple%20branches%20to%20integrate%20various%0Aspatial%20semantic%20information%20and%20channel%20semantic%20information%2C%20thereby%0Aenhancing%20feature%20diversity.%20The%20Efficient%20Hybrid%20Encoder%20utilizes%20global%0Afeature%20fusion%20and%20local%20feature%20interaction%20based%20on%20various%20features%20to%0Agenerate%20distinctive%20classification%20features%20by%20integrating%20feature%20information%0Ain%20an%20adaptable%20manner.%20Extensive%20experiments%20on%20the%20Tsinghua-Tencent%20100K%0A%28TT100K%29%20benchmark%20and%20the%20German%20Traffic%20Sign%20Detection%20Benchmark%20%28GTSDB%29%0Ademonstrate%20that%20our%20EMDFNet%20outperforms%20other%20state-of-the-art%20detectors%20in%0Aperformance%20while%20retaining%20the%20real-time%20processing%20capabilities%20of%0Asingle-stage%20models.%20This%20substantiates%20the%20effectiveness%20of%20EMDFNet%20in%0Adetecting%20small%20traffic%20signs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14189v1&entry.124074799=Read"},
{"title": "ConceptMix: A Compositional Image Generation Benchmark with Controllable\n  Difficulty", "author": "Xindi Wu and Dingli Yu and Yangsibo Huang and Olga Russakovsky and Sanjeev Arora", "abstract": "  Compositionality is a critical capability in Text-to-Image (T2I) models, as\nit reflects their ability to understand and combine multiple concepts from text\ndescriptions. Existing evaluations of compositional capability rely heavily on\nhuman-designed text prompts or fixed templates, limiting their diversity and\ncomplexity, and yielding low discriminative power. We propose ConceptMix, a\nscalable, controllable, and customizable benchmark which automatically\nevaluates compositional generation ability of T2I models. This is done in two\nstages. First, ConceptMix generates the text prompts: concretely, using\ncategories of visual concepts (e.g., objects, colors, shapes, spatial\nrelationships), it randomly samples an object and k-tuples of visual concepts,\nthen uses GPT4-o to generate text prompts for image generation based on these\nsampled concepts. Second, ConceptMix evaluates the images generated in response\nto these prompts: concretely, it checks how many of the k concepts actually\nappeared in the image by generating one question per visual concept and using a\nstrong VLM to answer them. Through administering ConceptMix to a diverse set of\nT2I models (proprietary as well as open ones) using increasing values of k, we\nshow that our ConceptMix has higher discrimination power than earlier\nbenchmarks. Specifically, ConceptMix reveals that the performance of several\nmodels, especially open models, drops dramatically with increased k.\nImportantly, it also provides insight into the lack of prompt diversity in\nwidely-used training datasets. Additionally, we conduct extensive human studies\nto validate the design of ConceptMix and compare our automatic grading with\nhuman judgement. We hope it will guide future T2I model development.\n", "link": "http://arxiv.org/abs/2408.14339v1", "date": "2024-08-26", "relevancy": 2.1507, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5523}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.552}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConceptMix%3A%20A%20Compositional%20Image%20Generation%20Benchmark%20with%20Controllable%0A%20%20Difficulty&body=Title%3A%20ConceptMix%3A%20A%20Compositional%20Image%20Generation%20Benchmark%20with%20Controllable%0A%20%20Difficulty%0AAuthor%3A%20Xindi%20Wu%20and%20Dingli%20Yu%20and%20Yangsibo%20Huang%20and%20Olga%20Russakovsky%20and%20Sanjeev%20Arora%0AAbstract%3A%20%20%20Compositionality%20is%20a%20critical%20capability%20in%20Text-to-Image%20%28T2I%29%20models%2C%20as%0Ait%20reflects%20their%20ability%20to%20understand%20and%20combine%20multiple%20concepts%20from%20text%0Adescriptions.%20Existing%20evaluations%20of%20compositional%20capability%20rely%20heavily%20on%0Ahuman-designed%20text%20prompts%20or%20fixed%20templates%2C%20limiting%20their%20diversity%20and%0Acomplexity%2C%20and%20yielding%20low%20discriminative%20power.%20We%20propose%20ConceptMix%2C%20a%0Ascalable%2C%20controllable%2C%20and%20customizable%20benchmark%20which%20automatically%0Aevaluates%20compositional%20generation%20ability%20of%20T2I%20models.%20This%20is%20done%20in%20two%0Astages.%20First%2C%20ConceptMix%20generates%20the%20text%20prompts%3A%20concretely%2C%20using%0Acategories%20of%20visual%20concepts%20%28e.g.%2C%20objects%2C%20colors%2C%20shapes%2C%20spatial%0Arelationships%29%2C%20it%20randomly%20samples%20an%20object%20and%20k-tuples%20of%20visual%20concepts%2C%0Athen%20uses%20GPT4-o%20to%20generate%20text%20prompts%20for%20image%20generation%20based%20on%20these%0Asampled%20concepts.%20Second%2C%20ConceptMix%20evaluates%20the%20images%20generated%20in%20response%0Ato%20these%20prompts%3A%20concretely%2C%20it%20checks%20how%20many%20of%20the%20k%20concepts%20actually%0Aappeared%20in%20the%20image%20by%20generating%20one%20question%20per%20visual%20concept%20and%20using%20a%0Astrong%20VLM%20to%20answer%20them.%20Through%20administering%20ConceptMix%20to%20a%20diverse%20set%20of%0AT2I%20models%20%28proprietary%20as%20well%20as%20open%20ones%29%20using%20increasing%20values%20of%20k%2C%20we%0Ashow%20that%20our%20ConceptMix%20has%20higher%20discrimination%20power%20than%20earlier%0Abenchmarks.%20Specifically%2C%20ConceptMix%20reveals%20that%20the%20performance%20of%20several%0Amodels%2C%20especially%20open%20models%2C%20drops%20dramatically%20with%20increased%20k.%0AImportantly%2C%20it%20also%20provides%20insight%20into%20the%20lack%20of%20prompt%20diversity%20in%0Awidely-used%20training%20datasets.%20Additionally%2C%20we%20conduct%20extensive%20human%20studies%0Ato%20validate%20the%20design%20of%20ConceptMix%20and%20compare%20our%20automatic%20grading%20with%0Ahuman%20judgement.%20We%20hope%20it%20will%20guide%20future%20T2I%20model%20development.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14339v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConceptMix%253A%2520A%2520Compositional%2520Image%2520Generation%2520Benchmark%2520with%2520Controllable%250A%2520%2520Difficulty%26entry.906535625%3DXindi%2520Wu%2520and%2520Dingli%2520Yu%2520and%2520Yangsibo%2520Huang%2520and%2520Olga%2520Russakovsky%2520and%2520Sanjeev%2520Arora%26entry.1292438233%3D%2520%2520Compositionality%2520is%2520a%2520critical%2520capability%2520in%2520Text-to-Image%2520%2528T2I%2529%2520models%252C%2520as%250Ait%2520reflects%2520their%2520ability%2520to%2520understand%2520and%2520combine%2520multiple%2520concepts%2520from%2520text%250Adescriptions.%2520Existing%2520evaluations%2520of%2520compositional%2520capability%2520rely%2520heavily%2520on%250Ahuman-designed%2520text%2520prompts%2520or%2520fixed%2520templates%252C%2520limiting%2520their%2520diversity%2520and%250Acomplexity%252C%2520and%2520yielding%2520low%2520discriminative%2520power.%2520We%2520propose%2520ConceptMix%252C%2520a%250Ascalable%252C%2520controllable%252C%2520and%2520customizable%2520benchmark%2520which%2520automatically%250Aevaluates%2520compositional%2520generation%2520ability%2520of%2520T2I%2520models.%2520This%2520is%2520done%2520in%2520two%250Astages.%2520First%252C%2520ConceptMix%2520generates%2520the%2520text%2520prompts%253A%2520concretely%252C%2520using%250Acategories%2520of%2520visual%2520concepts%2520%2528e.g.%252C%2520objects%252C%2520colors%252C%2520shapes%252C%2520spatial%250Arelationships%2529%252C%2520it%2520randomly%2520samples%2520an%2520object%2520and%2520k-tuples%2520of%2520visual%2520concepts%252C%250Athen%2520uses%2520GPT4-o%2520to%2520generate%2520text%2520prompts%2520for%2520image%2520generation%2520based%2520on%2520these%250Asampled%2520concepts.%2520Second%252C%2520ConceptMix%2520evaluates%2520the%2520images%2520generated%2520in%2520response%250Ato%2520these%2520prompts%253A%2520concretely%252C%2520it%2520checks%2520how%2520many%2520of%2520the%2520k%2520concepts%2520actually%250Aappeared%2520in%2520the%2520image%2520by%2520generating%2520one%2520question%2520per%2520visual%2520concept%2520and%2520using%2520a%250Astrong%2520VLM%2520to%2520answer%2520them.%2520Through%2520administering%2520ConceptMix%2520to%2520a%2520diverse%2520set%2520of%250AT2I%2520models%2520%2528proprietary%2520as%2520well%2520as%2520open%2520ones%2529%2520using%2520increasing%2520values%2520of%2520k%252C%2520we%250Ashow%2520that%2520our%2520ConceptMix%2520has%2520higher%2520discrimination%2520power%2520than%2520earlier%250Abenchmarks.%2520Specifically%252C%2520ConceptMix%2520reveals%2520that%2520the%2520performance%2520of%2520several%250Amodels%252C%2520especially%2520open%2520models%252C%2520drops%2520dramatically%2520with%2520increased%2520k.%250AImportantly%252C%2520it%2520also%2520provides%2520insight%2520into%2520the%2520lack%2520of%2520prompt%2520diversity%2520in%250Awidely-used%2520training%2520datasets.%2520Additionally%252C%2520we%2520conduct%2520extensive%2520human%2520studies%250Ato%2520validate%2520the%2520design%2520of%2520ConceptMix%2520and%2520compare%2520our%2520automatic%2520grading%2520with%250Ahuman%2520judgement.%2520We%2520hope%2520it%2520will%2520guide%2520future%2520T2I%2520model%2520development.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14339v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConceptMix%3A%20A%20Compositional%20Image%20Generation%20Benchmark%20with%20Controllable%0A%20%20Difficulty&entry.906535625=Xindi%20Wu%20and%20Dingli%20Yu%20and%20Yangsibo%20Huang%20and%20Olga%20Russakovsky%20and%20Sanjeev%20Arora&entry.1292438233=%20%20Compositionality%20is%20a%20critical%20capability%20in%20Text-to-Image%20%28T2I%29%20models%2C%20as%0Ait%20reflects%20their%20ability%20to%20understand%20and%20combine%20multiple%20concepts%20from%20text%0Adescriptions.%20Existing%20evaluations%20of%20compositional%20capability%20rely%20heavily%20on%0Ahuman-designed%20text%20prompts%20or%20fixed%20templates%2C%20limiting%20their%20diversity%20and%0Acomplexity%2C%20and%20yielding%20low%20discriminative%20power.%20We%20propose%20ConceptMix%2C%20a%0Ascalable%2C%20controllable%2C%20and%20customizable%20benchmark%20which%20automatically%0Aevaluates%20compositional%20generation%20ability%20of%20T2I%20models.%20This%20is%20done%20in%20two%0Astages.%20First%2C%20ConceptMix%20generates%20the%20text%20prompts%3A%20concretely%2C%20using%0Acategories%20of%20visual%20concepts%20%28e.g.%2C%20objects%2C%20colors%2C%20shapes%2C%20spatial%0Arelationships%29%2C%20it%20randomly%20samples%20an%20object%20and%20k-tuples%20of%20visual%20concepts%2C%0Athen%20uses%20GPT4-o%20to%20generate%20text%20prompts%20for%20image%20generation%20based%20on%20these%0Asampled%20concepts.%20Second%2C%20ConceptMix%20evaluates%20the%20images%20generated%20in%20response%0Ato%20these%20prompts%3A%20concretely%2C%20it%20checks%20how%20many%20of%20the%20k%20concepts%20actually%0Aappeared%20in%20the%20image%20by%20generating%20one%20question%20per%20visual%20concept%20and%20using%20a%0Astrong%20VLM%20to%20answer%20them.%20Through%20administering%20ConceptMix%20to%20a%20diverse%20set%20of%0AT2I%20models%20%28proprietary%20as%20well%20as%20open%20ones%29%20using%20increasing%20values%20of%20k%2C%20we%0Ashow%20that%20our%20ConceptMix%20has%20higher%20discrimination%20power%20than%20earlier%0Abenchmarks.%20Specifically%2C%20ConceptMix%20reveals%20that%20the%20performance%20of%20several%0Amodels%2C%20especially%20open%20models%2C%20drops%20dramatically%20with%20increased%20k.%0AImportantly%2C%20it%20also%20provides%20insight%20into%20the%20lack%20of%20prompt%20diversity%20in%0Awidely-used%20training%20datasets.%20Additionally%2C%20we%20conduct%20extensive%20human%20studies%0Ato%20validate%20the%20design%20of%20ConceptMix%20and%20compare%20our%20automatic%20grading%20with%0Ahuman%20judgement.%20We%20hope%20it%20will%20guide%20future%20T2I%20model%20development.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14339v1&entry.124074799=Read"},
{"title": "Reliable Multi-modal Medical Image-to-image Translation Independent of\n  Pixel-wise Aligned Data", "author": "Langrui Zhou and Guang Li", "abstract": "  The current mainstream multi-modal medical image-to-image translation methods\nface a contradiction. Supervised methods with outstanding performance rely on\npixel-wise aligned training data to constrain the model optimization. However,\nobtaining pixel-wise aligned multi-modal medical image datasets is challenging.\nUnsupervised methods can be trained without paired data, but their reliability\ncannot be guaranteed. At present, there is no ideal multi-modal medical\nimage-to-image translation method that can generate reliable translation\nresults without the need for pixel-wise aligned data. This work aims to develop\na novel medical image-to-image translation model that is independent of\npixel-wise aligned data (MITIA), enabling reliable multi-modal medical\nimage-to-image translation under the condition of misaligned training data. The\nproposed MITIA model utilizes a prior extraction network composed of a\nmulti-modal medical image registration module and a multi-modal misalignment\nerror detection module to extract pixel-level prior information from training\ndata with misalignment errors to the largest extent. The extracted prior\ninformation is then used to construct a regularization term to constrain the\noptimization of the unsupervised cycle-consistent GAN model, restricting its\nsolution space and thereby improving the performance and reliability of the\ngenerator. We trained the MITIA model using six datasets containing different\nmisalignment errors and two well-aligned datasets. Subsequently, we compared\nthe proposed method with six other state-of-the-art image-to-image translation\nmethods. The results of both quantitative analysis and qualitative visual\ninspection indicate that MITIA achieves superior performance compared to the\ncompeting state-of-the-art methods, both on misaligned data and aligned data.\n", "link": "http://arxiv.org/abs/2408.14270v1", "date": "2024-08-26", "relevancy": 2.1409, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5555}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5318}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reliable%20Multi-modal%20Medical%20Image-to-image%20Translation%20Independent%20of%0A%20%20Pixel-wise%20Aligned%20Data&body=Title%3A%20Reliable%20Multi-modal%20Medical%20Image-to-image%20Translation%20Independent%20of%0A%20%20Pixel-wise%20Aligned%20Data%0AAuthor%3A%20Langrui%20Zhou%20and%20Guang%20Li%0AAbstract%3A%20%20%20The%20current%20mainstream%20multi-modal%20medical%20image-to-image%20translation%20methods%0Aface%20a%20contradiction.%20Supervised%20methods%20with%20outstanding%20performance%20rely%20on%0Apixel-wise%20aligned%20training%20data%20to%20constrain%20the%20model%20optimization.%20However%2C%0Aobtaining%20pixel-wise%20aligned%20multi-modal%20medical%20image%20datasets%20is%20challenging.%0AUnsupervised%20methods%20can%20be%20trained%20without%20paired%20data%2C%20but%20their%20reliability%0Acannot%20be%20guaranteed.%20At%20present%2C%20there%20is%20no%20ideal%20multi-modal%20medical%0Aimage-to-image%20translation%20method%20that%20can%20generate%20reliable%20translation%0Aresults%20without%20the%20need%20for%20pixel-wise%20aligned%20data.%20This%20work%20aims%20to%20develop%0Aa%20novel%20medical%20image-to-image%20translation%20model%20that%20is%20independent%20of%0Apixel-wise%20aligned%20data%20%28MITIA%29%2C%20enabling%20reliable%20multi-modal%20medical%0Aimage-to-image%20translation%20under%20the%20condition%20of%20misaligned%20training%20data.%20The%0Aproposed%20MITIA%20model%20utilizes%20a%20prior%20extraction%20network%20composed%20of%20a%0Amulti-modal%20medical%20image%20registration%20module%20and%20a%20multi-modal%20misalignment%0Aerror%20detection%20module%20to%20extract%20pixel-level%20prior%20information%20from%20training%0Adata%20with%20misalignment%20errors%20to%20the%20largest%20extent.%20The%20extracted%20prior%0Ainformation%20is%20then%20used%20to%20construct%20a%20regularization%20term%20to%20constrain%20the%0Aoptimization%20of%20the%20unsupervised%20cycle-consistent%20GAN%20model%2C%20restricting%20its%0Asolution%20space%20and%20thereby%20improving%20the%20performance%20and%20reliability%20of%20the%0Agenerator.%20We%20trained%20the%20MITIA%20model%20using%20six%20datasets%20containing%20different%0Amisalignment%20errors%20and%20two%20well-aligned%20datasets.%20Subsequently%2C%20we%20compared%0Athe%20proposed%20method%20with%20six%20other%20state-of-the-art%20image-to-image%20translation%0Amethods.%20The%20results%20of%20both%20quantitative%20analysis%20and%20qualitative%20visual%0Ainspection%20indicate%20that%20MITIA%20achieves%20superior%20performance%20compared%20to%20the%0Acompeting%20state-of-the-art%20methods%2C%20both%20on%20misaligned%20data%20and%20aligned%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14270v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReliable%2520Multi-modal%2520Medical%2520Image-to-image%2520Translation%2520Independent%2520of%250A%2520%2520Pixel-wise%2520Aligned%2520Data%26entry.906535625%3DLangrui%2520Zhou%2520and%2520Guang%2520Li%26entry.1292438233%3D%2520%2520The%2520current%2520mainstream%2520multi-modal%2520medical%2520image-to-image%2520translation%2520methods%250Aface%2520a%2520contradiction.%2520Supervised%2520methods%2520with%2520outstanding%2520performance%2520rely%2520on%250Apixel-wise%2520aligned%2520training%2520data%2520to%2520constrain%2520the%2520model%2520optimization.%2520However%252C%250Aobtaining%2520pixel-wise%2520aligned%2520multi-modal%2520medical%2520image%2520datasets%2520is%2520challenging.%250AUnsupervised%2520methods%2520can%2520be%2520trained%2520without%2520paired%2520data%252C%2520but%2520their%2520reliability%250Acannot%2520be%2520guaranteed.%2520At%2520present%252C%2520there%2520is%2520no%2520ideal%2520multi-modal%2520medical%250Aimage-to-image%2520translation%2520method%2520that%2520can%2520generate%2520reliable%2520translation%250Aresults%2520without%2520the%2520need%2520for%2520pixel-wise%2520aligned%2520data.%2520This%2520work%2520aims%2520to%2520develop%250Aa%2520novel%2520medical%2520image-to-image%2520translation%2520model%2520that%2520is%2520independent%2520of%250Apixel-wise%2520aligned%2520data%2520%2528MITIA%2529%252C%2520enabling%2520reliable%2520multi-modal%2520medical%250Aimage-to-image%2520translation%2520under%2520the%2520condition%2520of%2520misaligned%2520training%2520data.%2520The%250Aproposed%2520MITIA%2520model%2520utilizes%2520a%2520prior%2520extraction%2520network%2520composed%2520of%2520a%250Amulti-modal%2520medical%2520image%2520registration%2520module%2520and%2520a%2520multi-modal%2520misalignment%250Aerror%2520detection%2520module%2520to%2520extract%2520pixel-level%2520prior%2520information%2520from%2520training%250Adata%2520with%2520misalignment%2520errors%2520to%2520the%2520largest%2520extent.%2520The%2520extracted%2520prior%250Ainformation%2520is%2520then%2520used%2520to%2520construct%2520a%2520regularization%2520term%2520to%2520constrain%2520the%250Aoptimization%2520of%2520the%2520unsupervised%2520cycle-consistent%2520GAN%2520model%252C%2520restricting%2520its%250Asolution%2520space%2520and%2520thereby%2520improving%2520the%2520performance%2520and%2520reliability%2520of%2520the%250Agenerator.%2520We%2520trained%2520the%2520MITIA%2520model%2520using%2520six%2520datasets%2520containing%2520different%250Amisalignment%2520errors%2520and%2520two%2520well-aligned%2520datasets.%2520Subsequently%252C%2520we%2520compared%250Athe%2520proposed%2520method%2520with%2520six%2520other%2520state-of-the-art%2520image-to-image%2520translation%250Amethods.%2520The%2520results%2520of%2520both%2520quantitative%2520analysis%2520and%2520qualitative%2520visual%250Ainspection%2520indicate%2520that%2520MITIA%2520achieves%2520superior%2520performance%2520compared%2520to%2520the%250Acompeting%2520state-of-the-art%2520methods%252C%2520both%2520on%2520misaligned%2520data%2520and%2520aligned%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14270v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reliable%20Multi-modal%20Medical%20Image-to-image%20Translation%20Independent%20of%0A%20%20Pixel-wise%20Aligned%20Data&entry.906535625=Langrui%20Zhou%20and%20Guang%20Li&entry.1292438233=%20%20The%20current%20mainstream%20multi-modal%20medical%20image-to-image%20translation%20methods%0Aface%20a%20contradiction.%20Supervised%20methods%20with%20outstanding%20performance%20rely%20on%0Apixel-wise%20aligned%20training%20data%20to%20constrain%20the%20model%20optimization.%20However%2C%0Aobtaining%20pixel-wise%20aligned%20multi-modal%20medical%20image%20datasets%20is%20challenging.%0AUnsupervised%20methods%20can%20be%20trained%20without%20paired%20data%2C%20but%20their%20reliability%0Acannot%20be%20guaranteed.%20At%20present%2C%20there%20is%20no%20ideal%20multi-modal%20medical%0Aimage-to-image%20translation%20method%20that%20can%20generate%20reliable%20translation%0Aresults%20without%20the%20need%20for%20pixel-wise%20aligned%20data.%20This%20work%20aims%20to%20develop%0Aa%20novel%20medical%20image-to-image%20translation%20model%20that%20is%20independent%20of%0Apixel-wise%20aligned%20data%20%28MITIA%29%2C%20enabling%20reliable%20multi-modal%20medical%0Aimage-to-image%20translation%20under%20the%20condition%20of%20misaligned%20training%20data.%20The%0Aproposed%20MITIA%20model%20utilizes%20a%20prior%20extraction%20network%20composed%20of%20a%0Amulti-modal%20medical%20image%20registration%20module%20and%20a%20multi-modal%20misalignment%0Aerror%20detection%20module%20to%20extract%20pixel-level%20prior%20information%20from%20training%0Adata%20with%20misalignment%20errors%20to%20the%20largest%20extent.%20The%20extracted%20prior%0Ainformation%20is%20then%20used%20to%20construct%20a%20regularization%20term%20to%20constrain%20the%0Aoptimization%20of%20the%20unsupervised%20cycle-consistent%20GAN%20model%2C%20restricting%20its%0Asolution%20space%20and%20thereby%20improving%20the%20performance%20and%20reliability%20of%20the%0Agenerator.%20We%20trained%20the%20MITIA%20model%20using%20six%20datasets%20containing%20different%0Amisalignment%20errors%20and%20two%20well-aligned%20datasets.%20Subsequently%2C%20we%20compared%0Athe%20proposed%20method%20with%20six%20other%20state-of-the-art%20image-to-image%20translation%0Amethods.%20The%20results%20of%20both%20quantitative%20analysis%20and%20qualitative%20visual%0Ainspection%20indicate%20that%20MITIA%20achieves%20superior%20performance%20compared%20to%20the%0Acompeting%20state-of-the-art%20methods%2C%20both%20on%20misaligned%20data%20and%20aligned%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14270v1&entry.124074799=Read"},
{"title": "Evaluating saliency scores in point clouds of natural environments by\n  learning surface anomalies", "author": "Reuma Arav and Dennis Wittich and Franz Rottensteiner", "abstract": "  In recent years, three-dimensional point clouds are used increasingly to\ndocument natural environments. Each dataset contains a diverse set of objects,\nat varying shapes and sizes, distributed throughout the data and intricately\nintertwined with the topography. Therefore, regions of interest are difficult\nto find and consequent analyses become a challenge. Inspired from visual\nperception principles, we propose to differentiate objects of interest from the\ncluttered environment by evaluating how much they stand out from their\nsurroundings, i.e., their geometric salience. Previous saliency detection\napproaches suggested mostly handcrafted attributes for the task. However, such\nmethods fail when the data are too noisy or have high levels of texture. Here\nwe propose a learning-based mechanism that accommodates noise and textured\nsurfaces. We assume that within the natural environment any change from the\nprevalent surface would suggest a salient object. Thus, we first learn the\nunderlying surface and then search for anomalies within it. Initially, a deep\nneural network is trained to reconstruct the surface. Regions where the\nreconstructed part deviates significantly from the original point cloud yield a\nsubstantial reconstruction error, signifying an anomaly, i.e., saliency. We\ndemonstrate the effectiveness of the proposed approach by searching for salient\nfeatures in various natural scenarios, which were acquired by different\nacquisition platforms. We show the strong correlation between the\nreconstruction error and salient objects.\n", "link": "http://arxiv.org/abs/2408.14421v1", "date": "2024-08-26", "relevancy": 2.1408, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5503}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5256}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20saliency%20scores%20in%20point%20clouds%20of%20natural%20environments%20by%0A%20%20learning%20surface%20anomalies&body=Title%3A%20Evaluating%20saliency%20scores%20in%20point%20clouds%20of%20natural%20environments%20by%0A%20%20learning%20surface%20anomalies%0AAuthor%3A%20Reuma%20Arav%20and%20Dennis%20Wittich%20and%20Franz%20Rottensteiner%0AAbstract%3A%20%20%20In%20recent%20years%2C%20three-dimensional%20point%20clouds%20are%20used%20increasingly%20to%0Adocument%20natural%20environments.%20Each%20dataset%20contains%20a%20diverse%20set%20of%20objects%2C%0Aat%20varying%20shapes%20and%20sizes%2C%20distributed%20throughout%20the%20data%20and%20intricately%0Aintertwined%20with%20the%20topography.%20Therefore%2C%20regions%20of%20interest%20are%20difficult%0Ato%20find%20and%20consequent%20analyses%20become%20a%20challenge.%20Inspired%20from%20visual%0Aperception%20principles%2C%20we%20propose%20to%20differentiate%20objects%20of%20interest%20from%20the%0Acluttered%20environment%20by%20evaluating%20how%20much%20they%20stand%20out%20from%20their%0Asurroundings%2C%20i.e.%2C%20their%20geometric%20salience.%20Previous%20saliency%20detection%0Aapproaches%20suggested%20mostly%20handcrafted%20attributes%20for%20the%20task.%20However%2C%20such%0Amethods%20fail%20when%20the%20data%20are%20too%20noisy%20or%20have%20high%20levels%20of%20texture.%20Here%0Awe%20propose%20a%20learning-based%20mechanism%20that%20accommodates%20noise%20and%20textured%0Asurfaces.%20We%20assume%20that%20within%20the%20natural%20environment%20any%20change%20from%20the%0Aprevalent%20surface%20would%20suggest%20a%20salient%20object.%20Thus%2C%20we%20first%20learn%20the%0Aunderlying%20surface%20and%20then%20search%20for%20anomalies%20within%20it.%20Initially%2C%20a%20deep%0Aneural%20network%20is%20trained%20to%20reconstruct%20the%20surface.%20Regions%20where%20the%0Areconstructed%20part%20deviates%20significantly%20from%20the%20original%20point%20cloud%20yield%20a%0Asubstantial%20reconstruction%20error%2C%20signifying%20an%20anomaly%2C%20i.e.%2C%20saliency.%20We%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20approach%20by%20searching%20for%20salient%0Afeatures%20in%20various%20natural%20scenarios%2C%20which%20were%20acquired%20by%20different%0Aacquisition%20platforms.%20We%20show%20the%20strong%20correlation%20between%20the%0Areconstruction%20error%20and%20salient%20objects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14421v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520saliency%2520scores%2520in%2520point%2520clouds%2520of%2520natural%2520environments%2520by%250A%2520%2520learning%2520surface%2520anomalies%26entry.906535625%3DReuma%2520Arav%2520and%2520Dennis%2520Wittich%2520and%2520Franz%2520Rottensteiner%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520three-dimensional%2520point%2520clouds%2520are%2520used%2520increasingly%2520to%250Adocument%2520natural%2520environments.%2520Each%2520dataset%2520contains%2520a%2520diverse%2520set%2520of%2520objects%252C%250Aat%2520varying%2520shapes%2520and%2520sizes%252C%2520distributed%2520throughout%2520the%2520data%2520and%2520intricately%250Aintertwined%2520with%2520the%2520topography.%2520Therefore%252C%2520regions%2520of%2520interest%2520are%2520difficult%250Ato%2520find%2520and%2520consequent%2520analyses%2520become%2520a%2520challenge.%2520Inspired%2520from%2520visual%250Aperception%2520principles%252C%2520we%2520propose%2520to%2520differentiate%2520objects%2520of%2520interest%2520from%2520the%250Acluttered%2520environment%2520by%2520evaluating%2520how%2520much%2520they%2520stand%2520out%2520from%2520their%250Asurroundings%252C%2520i.e.%252C%2520their%2520geometric%2520salience.%2520Previous%2520saliency%2520detection%250Aapproaches%2520suggested%2520mostly%2520handcrafted%2520attributes%2520for%2520the%2520task.%2520However%252C%2520such%250Amethods%2520fail%2520when%2520the%2520data%2520are%2520too%2520noisy%2520or%2520have%2520high%2520levels%2520of%2520texture.%2520Here%250Awe%2520propose%2520a%2520learning-based%2520mechanism%2520that%2520accommodates%2520noise%2520and%2520textured%250Asurfaces.%2520We%2520assume%2520that%2520within%2520the%2520natural%2520environment%2520any%2520change%2520from%2520the%250Aprevalent%2520surface%2520would%2520suggest%2520a%2520salient%2520object.%2520Thus%252C%2520we%2520first%2520learn%2520the%250Aunderlying%2520surface%2520and%2520then%2520search%2520for%2520anomalies%2520within%2520it.%2520Initially%252C%2520a%2520deep%250Aneural%2520network%2520is%2520trained%2520to%2520reconstruct%2520the%2520surface.%2520Regions%2520where%2520the%250Areconstructed%2520part%2520deviates%2520significantly%2520from%2520the%2520original%2520point%2520cloud%2520yield%2520a%250Asubstantial%2520reconstruction%2520error%252C%2520signifying%2520an%2520anomaly%252C%2520i.e.%252C%2520saliency.%2520We%250Ademonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approach%2520by%2520searching%2520for%2520salient%250Afeatures%2520in%2520various%2520natural%2520scenarios%252C%2520which%2520were%2520acquired%2520by%2520different%250Aacquisition%2520platforms.%2520We%2520show%2520the%2520strong%2520correlation%2520between%2520the%250Areconstruction%2520error%2520and%2520salient%2520objects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14421v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20saliency%20scores%20in%20point%20clouds%20of%20natural%20environments%20by%0A%20%20learning%20surface%20anomalies&entry.906535625=Reuma%20Arav%20and%20Dennis%20Wittich%20and%20Franz%20Rottensteiner&entry.1292438233=%20%20In%20recent%20years%2C%20three-dimensional%20point%20clouds%20are%20used%20increasingly%20to%0Adocument%20natural%20environments.%20Each%20dataset%20contains%20a%20diverse%20set%20of%20objects%2C%0Aat%20varying%20shapes%20and%20sizes%2C%20distributed%20throughout%20the%20data%20and%20intricately%0Aintertwined%20with%20the%20topography.%20Therefore%2C%20regions%20of%20interest%20are%20difficult%0Ato%20find%20and%20consequent%20analyses%20become%20a%20challenge.%20Inspired%20from%20visual%0Aperception%20principles%2C%20we%20propose%20to%20differentiate%20objects%20of%20interest%20from%20the%0Acluttered%20environment%20by%20evaluating%20how%20much%20they%20stand%20out%20from%20their%0Asurroundings%2C%20i.e.%2C%20their%20geometric%20salience.%20Previous%20saliency%20detection%0Aapproaches%20suggested%20mostly%20handcrafted%20attributes%20for%20the%20task.%20However%2C%20such%0Amethods%20fail%20when%20the%20data%20are%20too%20noisy%20or%20have%20high%20levels%20of%20texture.%20Here%0Awe%20propose%20a%20learning-based%20mechanism%20that%20accommodates%20noise%20and%20textured%0Asurfaces.%20We%20assume%20that%20within%20the%20natural%20environment%20any%20change%20from%20the%0Aprevalent%20surface%20would%20suggest%20a%20salient%20object.%20Thus%2C%20we%20first%20learn%20the%0Aunderlying%20surface%20and%20then%20search%20for%20anomalies%20within%20it.%20Initially%2C%20a%20deep%0Aneural%20network%20is%20trained%20to%20reconstruct%20the%20surface.%20Regions%20where%20the%0Areconstructed%20part%20deviates%20significantly%20from%20the%20original%20point%20cloud%20yield%20a%0Asubstantial%20reconstruction%20error%2C%20signifying%20an%20anomaly%2C%20i.e.%2C%20saliency.%20We%0Ademonstrate%20the%20effectiveness%20of%20the%20proposed%20approach%20by%20searching%20for%20salient%0Afeatures%20in%20various%20natural%20scenarios%2C%20which%20were%20acquired%20by%20different%0Aacquisition%20platforms.%20We%20show%20the%20strong%20correlation%20between%20the%0Areconstruction%20error%20and%20salient%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14421v1&entry.124074799=Read"},
{"title": "Docling Technical Report", "author": "Christoph Auer and Maksym Lysak and Ahmed Nassar and Michele Dolfi and Nikolaos Livathinos and Panos Vagenas and Cesar Berrospi Ramis and Matteo Omenetti and Fabian Lindlbauer and Kasper Dinkla and Valery Weber and Lucas Morin and Ingmar Meijer and Viktor Kuropiatnyk and Peter W. J. Staar", "abstract": "  This technical report introduces Docling, an easy to use, self-contained,\nMIT-licensed open-source package for PDF document conversion. It is powered by\nstate-of-the-art specialized AI models for layout analysis (DocLayNet) and\ntable structure recognition (TableFormer), and runs efficiently on commodity\nhardware in a small resource budget. The code interface allows for easy\nextensibility and addition of new features and models.\n", "link": "http://arxiv.org/abs/2408.09869v2", "date": "2024-08-26", "relevancy": 2.1317, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4557}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4117}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Docling%20Technical%20Report&body=Title%3A%20Docling%20Technical%20Report%0AAuthor%3A%20Christoph%20Auer%20and%20Maksym%20Lysak%20and%20Ahmed%20Nassar%20and%20Michele%20Dolfi%20and%20Nikolaos%20Livathinos%20and%20Panos%20Vagenas%20and%20Cesar%20Berrospi%20Ramis%20and%20Matteo%20Omenetti%20and%20Fabian%20Lindlbauer%20and%20Kasper%20Dinkla%20and%20Valery%20Weber%20and%20Lucas%20Morin%20and%20Ingmar%20Meijer%20and%20Viktor%20Kuropiatnyk%20and%20Peter%20W.%20J.%20Staar%0AAbstract%3A%20%20%20This%20technical%20report%20introduces%20Docling%2C%20an%20easy%20to%20use%2C%20self-contained%2C%0AMIT-licensed%20open-source%20package%20for%20PDF%20document%20conversion.%20It%20is%20powered%20by%0Astate-of-the-art%20specialized%20AI%20models%20for%20layout%20analysis%20%28DocLayNet%29%20and%0Atable%20structure%20recognition%20%28TableFormer%29%2C%20and%20runs%20efficiently%20on%20commodity%0Ahardware%20in%20a%20small%20resource%20budget.%20The%20code%20interface%20allows%20for%20easy%0Aextensibility%20and%20addition%20of%20new%20features%20and%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09869v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDocling%2520Technical%2520Report%26entry.906535625%3DChristoph%2520Auer%2520and%2520Maksym%2520Lysak%2520and%2520Ahmed%2520Nassar%2520and%2520Michele%2520Dolfi%2520and%2520Nikolaos%2520Livathinos%2520and%2520Panos%2520Vagenas%2520and%2520Cesar%2520Berrospi%2520Ramis%2520and%2520Matteo%2520Omenetti%2520and%2520Fabian%2520Lindlbauer%2520and%2520Kasper%2520Dinkla%2520and%2520Valery%2520Weber%2520and%2520Lucas%2520Morin%2520and%2520Ingmar%2520Meijer%2520and%2520Viktor%2520Kuropiatnyk%2520and%2520Peter%2520W.%2520J.%2520Staar%26entry.1292438233%3D%2520%2520This%2520technical%2520report%2520introduces%2520Docling%252C%2520an%2520easy%2520to%2520use%252C%2520self-contained%252C%250AMIT-licensed%2520open-source%2520package%2520for%2520PDF%2520document%2520conversion.%2520It%2520is%2520powered%2520by%250Astate-of-the-art%2520specialized%2520AI%2520models%2520for%2520layout%2520analysis%2520%2528DocLayNet%2529%2520and%250Atable%2520structure%2520recognition%2520%2528TableFormer%2529%252C%2520and%2520runs%2520efficiently%2520on%2520commodity%250Ahardware%2520in%2520a%2520small%2520resource%2520budget.%2520The%2520code%2520interface%2520allows%2520for%2520easy%250Aextensibility%2520and%2520addition%2520of%2520new%2520features%2520and%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09869v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Docling%20Technical%20Report&entry.906535625=Christoph%20Auer%20and%20Maksym%20Lysak%20and%20Ahmed%20Nassar%20and%20Michele%20Dolfi%20and%20Nikolaos%20Livathinos%20and%20Panos%20Vagenas%20and%20Cesar%20Berrospi%20Ramis%20and%20Matteo%20Omenetti%20and%20Fabian%20Lindlbauer%20and%20Kasper%20Dinkla%20and%20Valery%20Weber%20and%20Lucas%20Morin%20and%20Ingmar%20Meijer%20and%20Viktor%20Kuropiatnyk%20and%20Peter%20W.%20J.%20Staar&entry.1292438233=%20%20This%20technical%20report%20introduces%20Docling%2C%20an%20easy%20to%20use%2C%20self-contained%2C%0AMIT-licensed%20open-source%20package%20for%20PDF%20document%20conversion.%20It%20is%20powered%20by%0Astate-of-the-art%20specialized%20AI%20models%20for%20layout%20analysis%20%28DocLayNet%29%20and%0Atable%20structure%20recognition%20%28TableFormer%29%2C%20and%20runs%20efficiently%20on%20commodity%0Ahardware%20in%20a%20small%20resource%20budget.%20The%20code%20interface%20allows%20for%20easy%0Aextensibility%20and%20addition%20of%20new%20features%20and%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09869v2&entry.124074799=Read"},
{"title": "Provable Imbalanced Point Clustering", "author": "David Denisov and Dan Feldman and Shlomi Dolev and Michael Segal", "abstract": "  We suggest efficient and provable methods to compute an approximation for\nimbalanced point clustering, that is, fitting $k$-centers to a set of points in\n$\\mathbb{R}^d$, for any $d,k\\geq 1$. To this end, we utilize \\emph{coresets},\nwhich, in the context of the paper, are essentially weighted sets of points in\n$\\mathbb{R}^d$ that approximate the fitting loss for every model in a given\nset, up to a multiplicative factor of $1\\pm\\varepsilon$. We provide [Section 3\nand Section E in the appendix] experiments that show the empirical contribution\nof our suggested methods for real images (novel and reference), synthetic data,\nand real-world data. We also propose choice clustering, which by combining\nclustering algorithms yields better performance than each one separately.\n", "link": "http://arxiv.org/abs/2408.14225v1", "date": "2024-08-26", "relevancy": 2.1192, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.445}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4207}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Provable%20Imbalanced%20Point%20Clustering&body=Title%3A%20Provable%20Imbalanced%20Point%20Clustering%0AAuthor%3A%20David%20Denisov%20and%20Dan%20Feldman%20and%20Shlomi%20Dolev%20and%20Michael%20Segal%0AAbstract%3A%20%20%20We%20suggest%20efficient%20and%20provable%20methods%20to%20compute%20an%20approximation%20for%0Aimbalanced%20point%20clustering%2C%20that%20is%2C%20fitting%20%24k%24-centers%20to%20a%20set%20of%20points%20in%0A%24%5Cmathbb%7BR%7D%5Ed%24%2C%20for%20any%20%24d%2Ck%5Cgeq%201%24.%20To%20this%20end%2C%20we%20utilize%20%5Cemph%7Bcoresets%7D%2C%0Awhich%2C%20in%20the%20context%20of%20the%20paper%2C%20are%20essentially%20weighted%20sets%20of%20points%20in%0A%24%5Cmathbb%7BR%7D%5Ed%24%20that%20approximate%20the%20fitting%20loss%20for%20every%20model%20in%20a%20given%0Aset%2C%20up%20to%20a%20multiplicative%20factor%20of%20%241%5Cpm%5Cvarepsilon%24.%20We%20provide%20%5BSection%203%0Aand%20Section%20E%20in%20the%20appendix%5D%20experiments%20that%20show%20the%20empirical%20contribution%0Aof%20our%20suggested%20methods%20for%20real%20images%20%28novel%20and%20reference%29%2C%20synthetic%20data%2C%0Aand%20real-world%20data.%20We%20also%20propose%20choice%20clustering%2C%20which%20by%20combining%0Aclustering%20algorithms%20yields%20better%20performance%20than%20each%20one%20separately.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14225v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProvable%2520Imbalanced%2520Point%2520Clustering%26entry.906535625%3DDavid%2520Denisov%2520and%2520Dan%2520Feldman%2520and%2520Shlomi%2520Dolev%2520and%2520Michael%2520Segal%26entry.1292438233%3D%2520%2520We%2520suggest%2520efficient%2520and%2520provable%2520methods%2520to%2520compute%2520an%2520approximation%2520for%250Aimbalanced%2520point%2520clustering%252C%2520that%2520is%252C%2520fitting%2520%2524k%2524-centers%2520to%2520a%2520set%2520of%2520points%2520in%250A%2524%255Cmathbb%257BR%257D%255Ed%2524%252C%2520for%2520any%2520%2524d%252Ck%255Cgeq%25201%2524.%2520To%2520this%2520end%252C%2520we%2520utilize%2520%255Cemph%257Bcoresets%257D%252C%250Awhich%252C%2520in%2520the%2520context%2520of%2520the%2520paper%252C%2520are%2520essentially%2520weighted%2520sets%2520of%2520points%2520in%250A%2524%255Cmathbb%257BR%257D%255Ed%2524%2520that%2520approximate%2520the%2520fitting%2520loss%2520for%2520every%2520model%2520in%2520a%2520given%250Aset%252C%2520up%2520to%2520a%2520multiplicative%2520factor%2520of%2520%25241%255Cpm%255Cvarepsilon%2524.%2520We%2520provide%2520%255BSection%25203%250Aand%2520Section%2520E%2520in%2520the%2520appendix%255D%2520experiments%2520that%2520show%2520the%2520empirical%2520contribution%250Aof%2520our%2520suggested%2520methods%2520for%2520real%2520images%2520%2528novel%2520and%2520reference%2529%252C%2520synthetic%2520data%252C%250Aand%2520real-world%2520data.%2520We%2520also%2520propose%2520choice%2520clustering%252C%2520which%2520by%2520combining%250Aclustering%2520algorithms%2520yields%2520better%2520performance%2520than%2520each%2520one%2520separately.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14225v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provable%20Imbalanced%20Point%20Clustering&entry.906535625=David%20Denisov%20and%20Dan%20Feldman%20and%20Shlomi%20Dolev%20and%20Michael%20Segal&entry.1292438233=%20%20We%20suggest%20efficient%20and%20provable%20methods%20to%20compute%20an%20approximation%20for%0Aimbalanced%20point%20clustering%2C%20that%20is%2C%20fitting%20%24k%24-centers%20to%20a%20set%20of%20points%20in%0A%24%5Cmathbb%7BR%7D%5Ed%24%2C%20for%20any%20%24d%2Ck%5Cgeq%201%24.%20To%20this%20end%2C%20we%20utilize%20%5Cemph%7Bcoresets%7D%2C%0Awhich%2C%20in%20the%20context%20of%20the%20paper%2C%20are%20essentially%20weighted%20sets%20of%20points%20in%0A%24%5Cmathbb%7BR%7D%5Ed%24%20that%20approximate%20the%20fitting%20loss%20for%20every%20model%20in%20a%20given%0Aset%2C%20up%20to%20a%20multiplicative%20factor%20of%20%241%5Cpm%5Cvarepsilon%24.%20We%20provide%20%5BSection%203%0Aand%20Section%20E%20in%20the%20appendix%5D%20experiments%20that%20show%20the%20empirical%20contribution%0Aof%20our%20suggested%20methods%20for%20real%20images%20%28novel%20and%20reference%29%2C%20synthetic%20data%2C%0Aand%20real-world%20data.%20We%20also%20propose%20choice%20clustering%2C%20which%20by%20combining%0Aclustering%20algorithms%20yields%20better%20performance%20than%20each%20one%20separately.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14225v1&entry.124074799=Read"},
{"title": "Integrated Brain Connectivity Analysis with fMRI, DTI, and sMRI Powered\n  by Interpretable Graph Neural Networks", "author": "Gang Qu and Ziyu Zhou and Vince D. Calhoun and Aiying Zhang and Yu-Ping Wang", "abstract": "  Multimodal neuroimaging modeling has becomes a widely used approach but\nconfronts considerable challenges due to heterogeneity, which encompasses\nvariability in data types, scales, and formats across modalities. This\nvariability necessitates the deployment of advanced computational methods to\nintegrate and interpret these diverse datasets within a cohesive analytical\nframework. In our research, we amalgamate functional magnetic resonance\nimaging, diffusion tensor imaging, and structural MRI into a cohesive\nframework. This integration capitalizes on the unique strengths of each\nmodality and their inherent interconnections, aiming for a comprehensive\nunderstanding of the brain's connectivity and anatomical characteristics.\nUtilizing the Glasser atlas for parcellation, we integrate imaging derived\nfeatures from various modalities: functional connectivity from fMRI, structural\nconnectivity from DTI, and anatomical features from sMRI within consistent\nregions. Our approach incorporates a masking strategy to differentially weight\nneural connections, thereby facilitating a holistic amalgamation of multimodal\nimaging data. This technique enhances interpretability at connectivity level,\ntranscending traditional analyses centered on singular regional attributes. The\nmodel is applied to the Human Connectome Project's Development study to\nelucidate the associations between multimodal imaging and cognitive functions\nthroughout youth. The analysis demonstrates improved predictive accuracy and\nuncovers crucial anatomical features and essential neural connections,\ndeepening our understanding of brain structure and function.\n", "link": "http://arxiv.org/abs/2408.14254v1", "date": "2024-08-26", "relevancy": 2.1134, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.536}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5233}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Integrated%20Brain%20Connectivity%20Analysis%20with%20fMRI%2C%20DTI%2C%20and%20sMRI%20Powered%0A%20%20by%20Interpretable%20Graph%20Neural%20Networks&body=Title%3A%20Integrated%20Brain%20Connectivity%20Analysis%20with%20fMRI%2C%20DTI%2C%20and%20sMRI%20Powered%0A%20%20by%20Interpretable%20Graph%20Neural%20Networks%0AAuthor%3A%20Gang%20Qu%20and%20Ziyu%20Zhou%20and%20Vince%20D.%20Calhoun%20and%20Aiying%20Zhang%20and%20Yu-Ping%20Wang%0AAbstract%3A%20%20%20Multimodal%20neuroimaging%20modeling%20has%20becomes%20a%20widely%20used%20approach%20but%0Aconfronts%20considerable%20challenges%20due%20to%20heterogeneity%2C%20which%20encompasses%0Avariability%20in%20data%20types%2C%20scales%2C%20and%20formats%20across%20modalities.%20This%0Avariability%20necessitates%20the%20deployment%20of%20advanced%20computational%20methods%20to%0Aintegrate%20and%20interpret%20these%20diverse%20datasets%20within%20a%20cohesive%20analytical%0Aframework.%20In%20our%20research%2C%20we%20amalgamate%20functional%20magnetic%20resonance%0Aimaging%2C%20diffusion%20tensor%20imaging%2C%20and%20structural%20MRI%20into%20a%20cohesive%0Aframework.%20This%20integration%20capitalizes%20on%20the%20unique%20strengths%20of%20each%0Amodality%20and%20their%20inherent%20interconnections%2C%20aiming%20for%20a%20comprehensive%0Aunderstanding%20of%20the%20brain%27s%20connectivity%20and%20anatomical%20characteristics.%0AUtilizing%20the%20Glasser%20atlas%20for%20parcellation%2C%20we%20integrate%20imaging%20derived%0Afeatures%20from%20various%20modalities%3A%20functional%20connectivity%20from%20fMRI%2C%20structural%0Aconnectivity%20from%20DTI%2C%20and%20anatomical%20features%20from%20sMRI%20within%20consistent%0Aregions.%20Our%20approach%20incorporates%20a%20masking%20strategy%20to%20differentially%20weight%0Aneural%20connections%2C%20thereby%20facilitating%20a%20holistic%20amalgamation%20of%20multimodal%0Aimaging%20data.%20This%20technique%20enhances%20interpretability%20at%20connectivity%20level%2C%0Atranscending%20traditional%20analyses%20centered%20on%20singular%20regional%20attributes.%20The%0Amodel%20is%20applied%20to%20the%20Human%20Connectome%20Project%27s%20Development%20study%20to%0Aelucidate%20the%20associations%20between%20multimodal%20imaging%20and%20cognitive%20functions%0Athroughout%20youth.%20The%20analysis%20demonstrates%20improved%20predictive%20accuracy%20and%0Auncovers%20crucial%20anatomical%20features%20and%20essential%20neural%20connections%2C%0Adeepening%20our%20understanding%20of%20brain%20structure%20and%20function.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14254v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntegrated%2520Brain%2520Connectivity%2520Analysis%2520with%2520fMRI%252C%2520DTI%252C%2520and%2520sMRI%2520Powered%250A%2520%2520by%2520Interpretable%2520Graph%2520Neural%2520Networks%26entry.906535625%3DGang%2520Qu%2520and%2520Ziyu%2520Zhou%2520and%2520Vince%2520D.%2520Calhoun%2520and%2520Aiying%2520Zhang%2520and%2520Yu-Ping%2520Wang%26entry.1292438233%3D%2520%2520Multimodal%2520neuroimaging%2520modeling%2520has%2520becomes%2520a%2520widely%2520used%2520approach%2520but%250Aconfronts%2520considerable%2520challenges%2520due%2520to%2520heterogeneity%252C%2520which%2520encompasses%250Avariability%2520in%2520data%2520types%252C%2520scales%252C%2520and%2520formats%2520across%2520modalities.%2520This%250Avariability%2520necessitates%2520the%2520deployment%2520of%2520advanced%2520computational%2520methods%2520to%250Aintegrate%2520and%2520interpret%2520these%2520diverse%2520datasets%2520within%2520a%2520cohesive%2520analytical%250Aframework.%2520In%2520our%2520research%252C%2520we%2520amalgamate%2520functional%2520magnetic%2520resonance%250Aimaging%252C%2520diffusion%2520tensor%2520imaging%252C%2520and%2520structural%2520MRI%2520into%2520a%2520cohesive%250Aframework.%2520This%2520integration%2520capitalizes%2520on%2520the%2520unique%2520strengths%2520of%2520each%250Amodality%2520and%2520their%2520inherent%2520interconnections%252C%2520aiming%2520for%2520a%2520comprehensive%250Aunderstanding%2520of%2520the%2520brain%2527s%2520connectivity%2520and%2520anatomical%2520characteristics.%250AUtilizing%2520the%2520Glasser%2520atlas%2520for%2520parcellation%252C%2520we%2520integrate%2520imaging%2520derived%250Afeatures%2520from%2520various%2520modalities%253A%2520functional%2520connectivity%2520from%2520fMRI%252C%2520structural%250Aconnectivity%2520from%2520DTI%252C%2520and%2520anatomical%2520features%2520from%2520sMRI%2520within%2520consistent%250Aregions.%2520Our%2520approach%2520incorporates%2520a%2520masking%2520strategy%2520to%2520differentially%2520weight%250Aneural%2520connections%252C%2520thereby%2520facilitating%2520a%2520holistic%2520amalgamation%2520of%2520multimodal%250Aimaging%2520data.%2520This%2520technique%2520enhances%2520interpretability%2520at%2520connectivity%2520level%252C%250Atranscending%2520traditional%2520analyses%2520centered%2520on%2520singular%2520regional%2520attributes.%2520The%250Amodel%2520is%2520applied%2520to%2520the%2520Human%2520Connectome%2520Project%2527s%2520Development%2520study%2520to%250Aelucidate%2520the%2520associations%2520between%2520multimodal%2520imaging%2520and%2520cognitive%2520functions%250Athroughout%2520youth.%2520The%2520analysis%2520demonstrates%2520improved%2520predictive%2520accuracy%2520and%250Auncovers%2520crucial%2520anatomical%2520features%2520and%2520essential%2520neural%2520connections%252C%250Adeepening%2520our%2520understanding%2520of%2520brain%2520structure%2520and%2520function.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14254v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Integrated%20Brain%20Connectivity%20Analysis%20with%20fMRI%2C%20DTI%2C%20and%20sMRI%20Powered%0A%20%20by%20Interpretable%20Graph%20Neural%20Networks&entry.906535625=Gang%20Qu%20and%20Ziyu%20Zhou%20and%20Vince%20D.%20Calhoun%20and%20Aiying%20Zhang%20and%20Yu-Ping%20Wang&entry.1292438233=%20%20Multimodal%20neuroimaging%20modeling%20has%20becomes%20a%20widely%20used%20approach%20but%0Aconfronts%20considerable%20challenges%20due%20to%20heterogeneity%2C%20which%20encompasses%0Avariability%20in%20data%20types%2C%20scales%2C%20and%20formats%20across%20modalities.%20This%0Avariability%20necessitates%20the%20deployment%20of%20advanced%20computational%20methods%20to%0Aintegrate%20and%20interpret%20these%20diverse%20datasets%20within%20a%20cohesive%20analytical%0Aframework.%20In%20our%20research%2C%20we%20amalgamate%20functional%20magnetic%20resonance%0Aimaging%2C%20diffusion%20tensor%20imaging%2C%20and%20structural%20MRI%20into%20a%20cohesive%0Aframework.%20This%20integration%20capitalizes%20on%20the%20unique%20strengths%20of%20each%0Amodality%20and%20their%20inherent%20interconnections%2C%20aiming%20for%20a%20comprehensive%0Aunderstanding%20of%20the%20brain%27s%20connectivity%20and%20anatomical%20characteristics.%0AUtilizing%20the%20Glasser%20atlas%20for%20parcellation%2C%20we%20integrate%20imaging%20derived%0Afeatures%20from%20various%20modalities%3A%20functional%20connectivity%20from%20fMRI%2C%20structural%0Aconnectivity%20from%20DTI%2C%20and%20anatomical%20features%20from%20sMRI%20within%20consistent%0Aregions.%20Our%20approach%20incorporates%20a%20masking%20strategy%20to%20differentially%20weight%0Aneural%20connections%2C%20thereby%20facilitating%20a%20holistic%20amalgamation%20of%20multimodal%0Aimaging%20data.%20This%20technique%20enhances%20interpretability%20at%20connectivity%20level%2C%0Atranscending%20traditional%20analyses%20centered%20on%20singular%20regional%20attributes.%20The%0Amodel%20is%20applied%20to%20the%20Human%20Connectome%20Project%27s%20Development%20study%20to%0Aelucidate%20the%20associations%20between%20multimodal%20imaging%20and%20cognitive%20functions%0Athroughout%20youth.%20The%20analysis%20demonstrates%20improved%20predictive%20accuracy%20and%0Auncovers%20crucial%20anatomical%20features%20and%20essential%20neural%20connections%2C%0Adeepening%20our%20understanding%20of%20brain%20structure%20and%20function.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14254v1&entry.124074799=Read"},
{"title": "DQ-DETR: DETR with Dynamic Query for Tiny Object Detection", "author": "Yi-Xin Huang and Hou-I Liu and Hong-Han Shuai and Wen-Huang Cheng", "abstract": "  Despite previous DETR-like methods having performed successfully in generic\nobject detection, tiny object detection is still a challenging task for them\nsince the positional information of object queries is not customized for\ndetecting tiny objects, whose scale is extraordinarily smaller than general\nobjects. Also, DETR-like methods using a fixed number of queries make them\nunsuitable for aerial datasets, which only contain tiny objects, and the\nnumbers of instances are imbalanced between different images. Thus, we present\na simple yet effective model, named DQ-DETR, which consists of three different\ncomponents: categorical counting module, counting-guided feature enhancement,\nand dynamic query selection to solve the above-mentioned problems. DQ-DETR uses\nthe prediction and density maps from the categorical counting module to\ndynamically adjust the number of object queries and improve the positional\ninformation of queries. Our model DQ-DETR outperforms previous CNN-based and\nDETR-like methods, achieving state-of-the-art mAP 30.2% on the AI-TOD-V2\ndataset, which mostly consists of tiny objects.\n", "link": "http://arxiv.org/abs/2404.03507v3", "date": "2024-08-26", "relevancy": 2.1065, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5371}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.523}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DQ-DETR%3A%20DETR%20with%20Dynamic%20Query%20for%20Tiny%20Object%20Detection&body=Title%3A%20DQ-DETR%3A%20DETR%20with%20Dynamic%20Query%20for%20Tiny%20Object%20Detection%0AAuthor%3A%20Yi-Xin%20Huang%20and%20Hou-I%20Liu%20and%20Hong-Han%20Shuai%20and%20Wen-Huang%20Cheng%0AAbstract%3A%20%20%20Despite%20previous%20DETR-like%20methods%20having%20performed%20successfully%20in%20generic%0Aobject%20detection%2C%20tiny%20object%20detection%20is%20still%20a%20challenging%20task%20for%20them%0Asince%20the%20positional%20information%20of%20object%20queries%20is%20not%20customized%20for%0Adetecting%20tiny%20objects%2C%20whose%20scale%20is%20extraordinarily%20smaller%20than%20general%0Aobjects.%20Also%2C%20DETR-like%20methods%20using%20a%20fixed%20number%20of%20queries%20make%20them%0Aunsuitable%20for%20aerial%20datasets%2C%20which%20only%20contain%20tiny%20objects%2C%20and%20the%0Anumbers%20of%20instances%20are%20imbalanced%20between%20different%20images.%20Thus%2C%20we%20present%0Aa%20simple%20yet%20effective%20model%2C%20named%20DQ-DETR%2C%20which%20consists%20of%20three%20different%0Acomponents%3A%20categorical%20counting%20module%2C%20counting-guided%20feature%20enhancement%2C%0Aand%20dynamic%20query%20selection%20to%20solve%20the%20above-mentioned%20problems.%20DQ-DETR%20uses%0Athe%20prediction%20and%20density%20maps%20from%20the%20categorical%20counting%20module%20to%0Adynamically%20adjust%20the%20number%20of%20object%20queries%20and%20improve%20the%20positional%0Ainformation%20of%20queries.%20Our%20model%20DQ-DETR%20outperforms%20previous%20CNN-based%20and%0ADETR-like%20methods%2C%20achieving%20state-of-the-art%20mAP%2030.2%25%20on%20the%20AI-TOD-V2%0Adataset%2C%20which%20mostly%20consists%20of%20tiny%20objects.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03507v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDQ-DETR%253A%2520DETR%2520with%2520Dynamic%2520Query%2520for%2520Tiny%2520Object%2520Detection%26entry.906535625%3DYi-Xin%2520Huang%2520and%2520Hou-I%2520Liu%2520and%2520Hong-Han%2520Shuai%2520and%2520Wen-Huang%2520Cheng%26entry.1292438233%3D%2520%2520Despite%2520previous%2520DETR-like%2520methods%2520having%2520performed%2520successfully%2520in%2520generic%250Aobject%2520detection%252C%2520tiny%2520object%2520detection%2520is%2520still%2520a%2520challenging%2520task%2520for%2520them%250Asince%2520the%2520positional%2520information%2520of%2520object%2520queries%2520is%2520not%2520customized%2520for%250Adetecting%2520tiny%2520objects%252C%2520whose%2520scale%2520is%2520extraordinarily%2520smaller%2520than%2520general%250Aobjects.%2520Also%252C%2520DETR-like%2520methods%2520using%2520a%2520fixed%2520number%2520of%2520queries%2520make%2520them%250Aunsuitable%2520for%2520aerial%2520datasets%252C%2520which%2520only%2520contain%2520tiny%2520objects%252C%2520and%2520the%250Anumbers%2520of%2520instances%2520are%2520imbalanced%2520between%2520different%2520images.%2520Thus%252C%2520we%2520present%250Aa%2520simple%2520yet%2520effective%2520model%252C%2520named%2520DQ-DETR%252C%2520which%2520consists%2520of%2520three%2520different%250Acomponents%253A%2520categorical%2520counting%2520module%252C%2520counting-guided%2520feature%2520enhancement%252C%250Aand%2520dynamic%2520query%2520selection%2520to%2520solve%2520the%2520above-mentioned%2520problems.%2520DQ-DETR%2520uses%250Athe%2520prediction%2520and%2520density%2520maps%2520from%2520the%2520categorical%2520counting%2520module%2520to%250Adynamically%2520adjust%2520the%2520number%2520of%2520object%2520queries%2520and%2520improve%2520the%2520positional%250Ainformation%2520of%2520queries.%2520Our%2520model%2520DQ-DETR%2520outperforms%2520previous%2520CNN-based%2520and%250ADETR-like%2520methods%252C%2520achieving%2520state-of-the-art%2520mAP%252030.2%2525%2520on%2520the%2520AI-TOD-V2%250Adataset%252C%2520which%2520mostly%2520consists%2520of%2520tiny%2520objects.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.03507v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DQ-DETR%3A%20DETR%20with%20Dynamic%20Query%20for%20Tiny%20Object%20Detection&entry.906535625=Yi-Xin%20Huang%20and%20Hou-I%20Liu%20and%20Hong-Han%20Shuai%20and%20Wen-Huang%20Cheng&entry.1292438233=%20%20Despite%20previous%20DETR-like%20methods%20having%20performed%20successfully%20in%20generic%0Aobject%20detection%2C%20tiny%20object%20detection%20is%20still%20a%20challenging%20task%20for%20them%0Asince%20the%20positional%20information%20of%20object%20queries%20is%20not%20customized%20for%0Adetecting%20tiny%20objects%2C%20whose%20scale%20is%20extraordinarily%20smaller%20than%20general%0Aobjects.%20Also%2C%20DETR-like%20methods%20using%20a%20fixed%20number%20of%20queries%20make%20them%0Aunsuitable%20for%20aerial%20datasets%2C%20which%20only%20contain%20tiny%20objects%2C%20and%20the%0Anumbers%20of%20instances%20are%20imbalanced%20between%20different%20images.%20Thus%2C%20we%20present%0Aa%20simple%20yet%20effective%20model%2C%20named%20DQ-DETR%2C%20which%20consists%20of%20three%20different%0Acomponents%3A%20categorical%20counting%20module%2C%20counting-guided%20feature%20enhancement%2C%0Aand%20dynamic%20query%20selection%20to%20solve%20the%20above-mentioned%20problems.%20DQ-DETR%20uses%0Athe%20prediction%20and%20density%20maps%20from%20the%20categorical%20counting%20module%20to%0Adynamically%20adjust%20the%20number%20of%20object%20queries%20and%20improve%20the%20positional%0Ainformation%20of%20queries.%20Our%20model%20DQ-DETR%20outperforms%20previous%20CNN-based%20and%0ADETR-like%20methods%2C%20achieving%20state-of-the-art%20mAP%2030.2%25%20on%20the%20AI-TOD-V2%0Adataset%2C%20which%20mostly%20consists%20of%20tiny%20objects.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03507v3&entry.124074799=Read"},
{"title": "Model Parallel Training and Transfer Learning for Convolutional Neural\n  Networks by Domain Decomposition", "author": "Axel Klawonn and Martin Lanser and Janine Weber", "abstract": "  Deep convolutional neural networks (CNNs) have been shown to be very\nsuccessful in a wide range of image processing applications. However, due to\ntheir increasing number of model parameters and an increasing availability of\nlarge amounts of training data, parallelization strategies to efficiently train\ncomplex CNNs are necessary. In previous work by the authors, a novel model\nparallel CNN architecture was proposed which is loosely inspired by domain\ndecomposition. In particular, the novel network architecture is based on a\ndecomposition of the input data into smaller subimages. For each of these\nsubimages, local CNNs with a proportionally smaller number of parameters are\ntrained in parallel and the resulting local classifications are then aggregated\nin a second step by a dense feedforward neural network (DNN). In the present\nwork, we compare the resulting CNN-DNN architecture to less costly alternatives\nto combine the local classifications into a final, global decision.\nAdditionally, we investigate the performance of the CNN-DNN trained as one\ncoherent model as well as using a transfer learning strategy, where the\nparameters of the pre-trained local CNNs are used as initial values for a\nsubsequently trained global coherent CNN-DNN model.\n", "link": "http://arxiv.org/abs/2408.14442v1", "date": "2024-08-26", "relevancy": 2.1, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5281}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5241}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5222}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20Parallel%20Training%20and%20Transfer%20Learning%20for%20Convolutional%20Neural%0A%20%20Networks%20by%20Domain%20Decomposition&body=Title%3A%20Model%20Parallel%20Training%20and%20Transfer%20Learning%20for%20Convolutional%20Neural%0A%20%20Networks%20by%20Domain%20Decomposition%0AAuthor%3A%20Axel%20Klawonn%20and%20Martin%20Lanser%20and%20Janine%20Weber%0AAbstract%3A%20%20%20Deep%20convolutional%20neural%20networks%20%28CNNs%29%20have%20been%20shown%20to%20be%20very%0Asuccessful%20in%20a%20wide%20range%20of%20image%20processing%20applications.%20However%2C%20due%20to%0Atheir%20increasing%20number%20of%20model%20parameters%20and%20an%20increasing%20availability%20of%0Alarge%20amounts%20of%20training%20data%2C%20parallelization%20strategies%20to%20efficiently%20train%0Acomplex%20CNNs%20are%20necessary.%20In%20previous%20work%20by%20the%20authors%2C%20a%20novel%20model%0Aparallel%20CNN%20architecture%20was%20proposed%20which%20is%20loosely%20inspired%20by%20domain%0Adecomposition.%20In%20particular%2C%20the%20novel%20network%20architecture%20is%20based%20on%20a%0Adecomposition%20of%20the%20input%20data%20into%20smaller%20subimages.%20For%20each%20of%20these%0Asubimages%2C%20local%20CNNs%20with%20a%20proportionally%20smaller%20number%20of%20parameters%20are%0Atrained%20in%20parallel%20and%20the%20resulting%20local%20classifications%20are%20then%20aggregated%0Ain%20a%20second%20step%20by%20a%20dense%20feedforward%20neural%20network%20%28DNN%29.%20In%20the%20present%0Awork%2C%20we%20compare%20the%20resulting%20CNN-DNN%20architecture%20to%20less%20costly%20alternatives%0Ato%20combine%20the%20local%20classifications%20into%20a%20final%2C%20global%20decision.%0AAdditionally%2C%20we%20investigate%20the%20performance%20of%20the%20CNN-DNN%20trained%20as%20one%0Acoherent%20model%20as%20well%20as%20using%20a%20transfer%20learning%20strategy%2C%20where%20the%0Aparameters%20of%20the%20pre-trained%20local%20CNNs%20are%20used%20as%20initial%20values%20for%20a%0Asubsequently%20trained%20global%20coherent%20CNN-DNN%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14442v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520Parallel%2520Training%2520and%2520Transfer%2520Learning%2520for%2520Convolutional%2520Neural%250A%2520%2520Networks%2520by%2520Domain%2520Decomposition%26entry.906535625%3DAxel%2520Klawonn%2520and%2520Martin%2520Lanser%2520and%2520Janine%2520Weber%26entry.1292438233%3D%2520%2520Deep%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520have%2520been%2520shown%2520to%2520be%2520very%250Asuccessful%2520in%2520a%2520wide%2520range%2520of%2520image%2520processing%2520applications.%2520However%252C%2520due%2520to%250Atheir%2520increasing%2520number%2520of%2520model%2520parameters%2520and%2520an%2520increasing%2520availability%2520of%250Alarge%2520amounts%2520of%2520training%2520data%252C%2520parallelization%2520strategies%2520to%2520efficiently%2520train%250Acomplex%2520CNNs%2520are%2520necessary.%2520In%2520previous%2520work%2520by%2520the%2520authors%252C%2520a%2520novel%2520model%250Aparallel%2520CNN%2520architecture%2520was%2520proposed%2520which%2520is%2520loosely%2520inspired%2520by%2520domain%250Adecomposition.%2520In%2520particular%252C%2520the%2520novel%2520network%2520architecture%2520is%2520based%2520on%2520a%250Adecomposition%2520of%2520the%2520input%2520data%2520into%2520smaller%2520subimages.%2520For%2520each%2520of%2520these%250Asubimages%252C%2520local%2520CNNs%2520with%2520a%2520proportionally%2520smaller%2520number%2520of%2520parameters%2520are%250Atrained%2520in%2520parallel%2520and%2520the%2520resulting%2520local%2520classifications%2520are%2520then%2520aggregated%250Ain%2520a%2520second%2520step%2520by%2520a%2520dense%2520feedforward%2520neural%2520network%2520%2528DNN%2529.%2520In%2520the%2520present%250Awork%252C%2520we%2520compare%2520the%2520resulting%2520CNN-DNN%2520architecture%2520to%2520less%2520costly%2520alternatives%250Ato%2520combine%2520the%2520local%2520classifications%2520into%2520a%2520final%252C%2520global%2520decision.%250AAdditionally%252C%2520we%2520investigate%2520the%2520performance%2520of%2520the%2520CNN-DNN%2520trained%2520as%2520one%250Acoherent%2520model%2520as%2520well%2520as%2520using%2520a%2520transfer%2520learning%2520strategy%252C%2520where%2520the%250Aparameters%2520of%2520the%2520pre-trained%2520local%2520CNNs%2520are%2520used%2520as%2520initial%2520values%2520for%2520a%250Asubsequently%2520trained%2520global%2520coherent%2520CNN-DNN%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14442v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Parallel%20Training%20and%20Transfer%20Learning%20for%20Convolutional%20Neural%0A%20%20Networks%20by%20Domain%20Decomposition&entry.906535625=Axel%20Klawonn%20and%20Martin%20Lanser%20and%20Janine%20Weber&entry.1292438233=%20%20Deep%20convolutional%20neural%20networks%20%28CNNs%29%20have%20been%20shown%20to%20be%20very%0Asuccessful%20in%20a%20wide%20range%20of%20image%20processing%20applications.%20However%2C%20due%20to%0Atheir%20increasing%20number%20of%20model%20parameters%20and%20an%20increasing%20availability%20of%0Alarge%20amounts%20of%20training%20data%2C%20parallelization%20strategies%20to%20efficiently%20train%0Acomplex%20CNNs%20are%20necessary.%20In%20previous%20work%20by%20the%20authors%2C%20a%20novel%20model%0Aparallel%20CNN%20architecture%20was%20proposed%20which%20is%20loosely%20inspired%20by%20domain%0Adecomposition.%20In%20particular%2C%20the%20novel%20network%20architecture%20is%20based%20on%20a%0Adecomposition%20of%20the%20input%20data%20into%20smaller%20subimages.%20For%20each%20of%20these%0Asubimages%2C%20local%20CNNs%20with%20a%20proportionally%20smaller%20number%20of%20parameters%20are%0Atrained%20in%20parallel%20and%20the%20resulting%20local%20classifications%20are%20then%20aggregated%0Ain%20a%20second%20step%20by%20a%20dense%20feedforward%20neural%20network%20%28DNN%29.%20In%20the%20present%0Awork%2C%20we%20compare%20the%20resulting%20CNN-DNN%20architecture%20to%20less%20costly%20alternatives%0Ato%20combine%20the%20local%20classifications%20into%20a%20final%2C%20global%20decision.%0AAdditionally%2C%20we%20investigate%20the%20performance%20of%20the%20CNN-DNN%20trained%20as%20one%0Acoherent%20model%20as%20well%20as%20using%20a%20transfer%20learning%20strategy%2C%20where%20the%0Aparameters%20of%20the%20pre-trained%20local%20CNNs%20are%20used%20as%20initial%20values%20for%20a%0Asubsequently%20trained%20global%20coherent%20CNN-DNN%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14442v1&entry.124074799=Read"},
{"title": "Cross-view Action Recognition Understanding From Exocentric to\n  Egocentric Perspective", "author": "Thanh-Dat Truong and Khoa Luu", "abstract": "  Understanding action recognition in egocentric videos has emerged as a vital\nresearch topic with numerous practical applications. With the limitation in the\nscale of egocentric data collection, learning robust deep learning-based action\nrecognition models remains difficult. Transferring knowledge learned from the\nlarge-scale exocentric data to the egocentric data is challenging due to the\ndifference in videos across views. Our work introduces a novel cross-view\nlearning approach to action recognition (CVAR) that effectively transfers\nknowledge from the exocentric to the selfish view. First, we present a novel\ngeometric-based constraint into the self-attention mechanism in Transformer\nbased on analyzing the camera positions between two views. Then, we propose a\nnew cross-view self-attention loss learned on unpaired cross-view data to\nenforce the self-attention mechanism learning to transfer knowledge across\nviews. Finally, to further improve the performance of our cross-view learning\napproach, we present the metrics to measure the correlations in videos and\nattention maps effectively. Experimental results on standard egocentric action\nrecognition benchmarks, i.e., Charades-Ego, EPIC-Kitchens-55, and\nEPIC-Kitchens-100, have shown our approach's effectiveness and state-of-the-art\nperformance.\n", "link": "http://arxiv.org/abs/2305.15699v3", "date": "2024-08-26", "relevancy": 2.0999, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5335}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.528}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-view%20Action%20Recognition%20Understanding%20From%20Exocentric%20to%0A%20%20Egocentric%20Perspective&body=Title%3A%20Cross-view%20Action%20Recognition%20Understanding%20From%20Exocentric%20to%0A%20%20Egocentric%20Perspective%0AAuthor%3A%20Thanh-Dat%20Truong%20and%20Khoa%20Luu%0AAbstract%3A%20%20%20Understanding%20action%20recognition%20in%20egocentric%20videos%20has%20emerged%20as%20a%20vital%0Aresearch%20topic%20with%20numerous%20practical%20applications.%20With%20the%20limitation%20in%20the%0Ascale%20of%20egocentric%20data%20collection%2C%20learning%20robust%20deep%20learning-based%20action%0Arecognition%20models%20remains%20difficult.%20Transferring%20knowledge%20learned%20from%20the%0Alarge-scale%20exocentric%20data%20to%20the%20egocentric%20data%20is%20challenging%20due%20to%20the%0Adifference%20in%20videos%20across%20views.%20Our%20work%20introduces%20a%20novel%20cross-view%0Alearning%20approach%20to%20action%20recognition%20%28CVAR%29%20that%20effectively%20transfers%0Aknowledge%20from%20the%20exocentric%20to%20the%20selfish%20view.%20First%2C%20we%20present%20a%20novel%0Ageometric-based%20constraint%20into%20the%20self-attention%20mechanism%20in%20Transformer%0Abased%20on%20analyzing%20the%20camera%20positions%20between%20two%20views.%20Then%2C%20we%20propose%20a%0Anew%20cross-view%20self-attention%20loss%20learned%20on%20unpaired%20cross-view%20data%20to%0Aenforce%20the%20self-attention%20mechanism%20learning%20to%20transfer%20knowledge%20across%0Aviews.%20Finally%2C%20to%20further%20improve%20the%20performance%20of%20our%20cross-view%20learning%0Aapproach%2C%20we%20present%20the%20metrics%20to%20measure%20the%20correlations%20in%20videos%20and%0Aattention%20maps%20effectively.%20Experimental%20results%20on%20standard%20egocentric%20action%0Arecognition%20benchmarks%2C%20i.e.%2C%20Charades-Ego%2C%20EPIC-Kitchens-55%2C%20and%0AEPIC-Kitchens-100%2C%20have%20shown%20our%20approach%27s%20effectiveness%20and%20state-of-the-art%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.15699v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-view%2520Action%2520Recognition%2520Understanding%2520From%2520Exocentric%2520to%250A%2520%2520Egocentric%2520Perspective%26entry.906535625%3DThanh-Dat%2520Truong%2520and%2520Khoa%2520Luu%26entry.1292438233%3D%2520%2520Understanding%2520action%2520recognition%2520in%2520egocentric%2520videos%2520has%2520emerged%2520as%2520a%2520vital%250Aresearch%2520topic%2520with%2520numerous%2520practical%2520applications.%2520With%2520the%2520limitation%2520in%2520the%250Ascale%2520of%2520egocentric%2520data%2520collection%252C%2520learning%2520robust%2520deep%2520learning-based%2520action%250Arecognition%2520models%2520remains%2520difficult.%2520Transferring%2520knowledge%2520learned%2520from%2520the%250Alarge-scale%2520exocentric%2520data%2520to%2520the%2520egocentric%2520data%2520is%2520challenging%2520due%2520to%2520the%250Adifference%2520in%2520videos%2520across%2520views.%2520Our%2520work%2520introduces%2520a%2520novel%2520cross-view%250Alearning%2520approach%2520to%2520action%2520recognition%2520%2528CVAR%2529%2520that%2520effectively%2520transfers%250Aknowledge%2520from%2520the%2520exocentric%2520to%2520the%2520selfish%2520view.%2520First%252C%2520we%2520present%2520a%2520novel%250Ageometric-based%2520constraint%2520into%2520the%2520self-attention%2520mechanism%2520in%2520Transformer%250Abased%2520on%2520analyzing%2520the%2520camera%2520positions%2520between%2520two%2520views.%2520Then%252C%2520we%2520propose%2520a%250Anew%2520cross-view%2520self-attention%2520loss%2520learned%2520on%2520unpaired%2520cross-view%2520data%2520to%250Aenforce%2520the%2520self-attention%2520mechanism%2520learning%2520to%2520transfer%2520knowledge%2520across%250Aviews.%2520Finally%252C%2520to%2520further%2520improve%2520the%2520performance%2520of%2520our%2520cross-view%2520learning%250Aapproach%252C%2520we%2520present%2520the%2520metrics%2520to%2520measure%2520the%2520correlations%2520in%2520videos%2520and%250Aattention%2520maps%2520effectively.%2520Experimental%2520results%2520on%2520standard%2520egocentric%2520action%250Arecognition%2520benchmarks%252C%2520i.e.%252C%2520Charades-Ego%252C%2520EPIC-Kitchens-55%252C%2520and%250AEPIC-Kitchens-100%252C%2520have%2520shown%2520our%2520approach%2527s%2520effectiveness%2520and%2520state-of-the-art%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.15699v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-view%20Action%20Recognition%20Understanding%20From%20Exocentric%20to%0A%20%20Egocentric%20Perspective&entry.906535625=Thanh-Dat%20Truong%20and%20Khoa%20Luu&entry.1292438233=%20%20Understanding%20action%20recognition%20in%20egocentric%20videos%20has%20emerged%20as%20a%20vital%0Aresearch%20topic%20with%20numerous%20practical%20applications.%20With%20the%20limitation%20in%20the%0Ascale%20of%20egocentric%20data%20collection%2C%20learning%20robust%20deep%20learning-based%20action%0Arecognition%20models%20remains%20difficult.%20Transferring%20knowledge%20learned%20from%20the%0Alarge-scale%20exocentric%20data%20to%20the%20egocentric%20data%20is%20challenging%20due%20to%20the%0Adifference%20in%20videos%20across%20views.%20Our%20work%20introduces%20a%20novel%20cross-view%0Alearning%20approach%20to%20action%20recognition%20%28CVAR%29%20that%20effectively%20transfers%0Aknowledge%20from%20the%20exocentric%20to%20the%20selfish%20view.%20First%2C%20we%20present%20a%20novel%0Ageometric-based%20constraint%20into%20the%20self-attention%20mechanism%20in%20Transformer%0Abased%20on%20analyzing%20the%20camera%20positions%20between%20two%20views.%20Then%2C%20we%20propose%20a%0Anew%20cross-view%20self-attention%20loss%20learned%20on%20unpaired%20cross-view%20data%20to%0Aenforce%20the%20self-attention%20mechanism%20learning%20to%20transfer%20knowledge%20across%0Aviews.%20Finally%2C%20to%20further%20improve%20the%20performance%20of%20our%20cross-view%20learning%0Aapproach%2C%20we%20present%20the%20metrics%20to%20measure%20the%20correlations%20in%20videos%20and%0Aattention%20maps%20effectively.%20Experimental%20results%20on%20standard%20egocentric%20action%0Arecognition%20benchmarks%2C%20i.e.%2C%20Charades-Ego%2C%20EPIC-Kitchens-55%2C%20and%0AEPIC-Kitchens-100%2C%20have%20shown%20our%20approach%27s%20effectiveness%20and%20state-of-the-art%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.15699v3&entry.124074799=Read"},
{"title": "Deep Spectral Improvement for Unsupervised Image Instance Segmentation", "author": "Farnoosh Arefi and Amir M. Mansourian and Shohreh Kasaei", "abstract": "  Deep spectral methods reframe the image decomposition process as a graph\npartitioning task by extracting features using self-supervised learning and\nutilizing the Laplacian of the affinity matrix to obtain eigensegments.\nHowever, instance segmentation has received less attention compared to other\ntasks within the context of deep spectral methods. This paper addresses the\nfact that not all channels of the feature map extracted from a self-supervised\nbackbone contain sufficient information for instance segmentation purposes. In\nfact, Some channels are noisy and hinder the accuracy of the task. To overcome\nthis issue, this paper proposes two channel reduction modules: Noise Channel\nReduction (NCR) and Deviation-based Channel Reduction (DCR). The NCR retains\nchannels with lower entropy, as they are less likely to be noisy, while DCR\nprunes channels with low standard deviation, as they lack sufficient\ninformation for effective instance segmentation. Furthermore, the paper\ndemonstrates that the dot product, commonly used in deep spectral methods, is\nnot suitable for instance segmentation due to its sensitivity to feature map\nvalues, potentially leading to incorrect instance segments. A new similarity\nmetric called Bray-Curtis over Chebyshev (BoC) is proposed to address this\nissue. It takes into account the distribution of features in addition to their\nvalues, providing a more robust similarity measure for instance segmentation.\nQuantitative and qualitative results on the Youtube-VIS2019 dataset highlight\nthe improvements achieved by the proposed channel reduction methods and the use\nof BoC instead of the conventional dot product for creating the affinity\nmatrix. These improvements are observed in terms of mean Intersection over\nUnion and extracted instance segments, demonstrating enhanced instance\nsegmentation performance. The code is available on:\nhttps://github.com/farnooshar/SpecUnIIS\n", "link": "http://arxiv.org/abs/2402.02474v3", "date": "2024-08-26", "relevancy": 2.0817, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.538}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5139}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4929}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Spectral%20Improvement%20for%20Unsupervised%20Image%20Instance%20Segmentation&body=Title%3A%20Deep%20Spectral%20Improvement%20for%20Unsupervised%20Image%20Instance%20Segmentation%0AAuthor%3A%20Farnoosh%20Arefi%20and%20Amir%20M.%20Mansourian%20and%20Shohreh%20Kasaei%0AAbstract%3A%20%20%20Deep%20spectral%20methods%20reframe%20the%20image%20decomposition%20process%20as%20a%20graph%0Apartitioning%20task%20by%20extracting%20features%20using%20self-supervised%20learning%20and%0Autilizing%20the%20Laplacian%20of%20the%20affinity%20matrix%20to%20obtain%20eigensegments.%0AHowever%2C%20instance%20segmentation%20has%20received%20less%20attention%20compared%20to%20other%0Atasks%20within%20the%20context%20of%20deep%20spectral%20methods.%20This%20paper%20addresses%20the%0Afact%20that%20not%20all%20channels%20of%20the%20feature%20map%20extracted%20from%20a%20self-supervised%0Abackbone%20contain%20sufficient%20information%20for%20instance%20segmentation%20purposes.%20In%0Afact%2C%20Some%20channels%20are%20noisy%20and%20hinder%20the%20accuracy%20of%20the%20task.%20To%20overcome%0Athis%20issue%2C%20this%20paper%20proposes%20two%20channel%20reduction%20modules%3A%20Noise%20Channel%0AReduction%20%28NCR%29%20and%20Deviation-based%20Channel%20Reduction%20%28DCR%29.%20The%20NCR%20retains%0Achannels%20with%20lower%20entropy%2C%20as%20they%20are%20less%20likely%20to%20be%20noisy%2C%20while%20DCR%0Aprunes%20channels%20with%20low%20standard%20deviation%2C%20as%20they%20lack%20sufficient%0Ainformation%20for%20effective%20instance%20segmentation.%20Furthermore%2C%20the%20paper%0Ademonstrates%20that%20the%20dot%20product%2C%20commonly%20used%20in%20deep%20spectral%20methods%2C%20is%0Anot%20suitable%20for%20instance%20segmentation%20due%20to%20its%20sensitivity%20to%20feature%20map%0Avalues%2C%20potentially%20leading%20to%20incorrect%20instance%20segments.%20A%20new%20similarity%0Ametric%20called%20Bray-Curtis%20over%20Chebyshev%20%28BoC%29%20is%20proposed%20to%20address%20this%0Aissue.%20It%20takes%20into%20account%20the%20distribution%20of%20features%20in%20addition%20to%20their%0Avalues%2C%20providing%20a%20more%20robust%20similarity%20measure%20for%20instance%20segmentation.%0AQuantitative%20and%20qualitative%20results%20on%20the%20Youtube-VIS2019%20dataset%20highlight%0Athe%20improvements%20achieved%20by%20the%20proposed%20channel%20reduction%20methods%20and%20the%20use%0Aof%20BoC%20instead%20of%20the%20conventional%20dot%20product%20for%20creating%20the%20affinity%0Amatrix.%20These%20improvements%20are%20observed%20in%20terms%20of%20mean%20Intersection%20over%0AUnion%20and%20extracted%20instance%20segments%2C%20demonstrating%20enhanced%20instance%0Asegmentation%20performance.%20The%20code%20is%20available%20on%3A%0Ahttps%3A//github.com/farnooshar/SpecUnIIS%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02474v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Spectral%2520Improvement%2520for%2520Unsupervised%2520Image%2520Instance%2520Segmentation%26entry.906535625%3DFarnoosh%2520Arefi%2520and%2520Amir%2520M.%2520Mansourian%2520and%2520Shohreh%2520Kasaei%26entry.1292438233%3D%2520%2520Deep%2520spectral%2520methods%2520reframe%2520the%2520image%2520decomposition%2520process%2520as%2520a%2520graph%250Apartitioning%2520task%2520by%2520extracting%2520features%2520using%2520self-supervised%2520learning%2520and%250Autilizing%2520the%2520Laplacian%2520of%2520the%2520affinity%2520matrix%2520to%2520obtain%2520eigensegments.%250AHowever%252C%2520instance%2520segmentation%2520has%2520received%2520less%2520attention%2520compared%2520to%2520other%250Atasks%2520within%2520the%2520context%2520of%2520deep%2520spectral%2520methods.%2520This%2520paper%2520addresses%2520the%250Afact%2520that%2520not%2520all%2520channels%2520of%2520the%2520feature%2520map%2520extracted%2520from%2520a%2520self-supervised%250Abackbone%2520contain%2520sufficient%2520information%2520for%2520instance%2520segmentation%2520purposes.%2520In%250Afact%252C%2520Some%2520channels%2520are%2520noisy%2520and%2520hinder%2520the%2520accuracy%2520of%2520the%2520task.%2520To%2520overcome%250Athis%2520issue%252C%2520this%2520paper%2520proposes%2520two%2520channel%2520reduction%2520modules%253A%2520Noise%2520Channel%250AReduction%2520%2528NCR%2529%2520and%2520Deviation-based%2520Channel%2520Reduction%2520%2528DCR%2529.%2520The%2520NCR%2520retains%250Achannels%2520with%2520lower%2520entropy%252C%2520as%2520they%2520are%2520less%2520likely%2520to%2520be%2520noisy%252C%2520while%2520DCR%250Aprunes%2520channels%2520with%2520low%2520standard%2520deviation%252C%2520as%2520they%2520lack%2520sufficient%250Ainformation%2520for%2520effective%2520instance%2520segmentation.%2520Furthermore%252C%2520the%2520paper%250Ademonstrates%2520that%2520the%2520dot%2520product%252C%2520commonly%2520used%2520in%2520deep%2520spectral%2520methods%252C%2520is%250Anot%2520suitable%2520for%2520instance%2520segmentation%2520due%2520to%2520its%2520sensitivity%2520to%2520feature%2520map%250Avalues%252C%2520potentially%2520leading%2520to%2520incorrect%2520instance%2520segments.%2520A%2520new%2520similarity%250Ametric%2520called%2520Bray-Curtis%2520over%2520Chebyshev%2520%2528BoC%2529%2520is%2520proposed%2520to%2520address%2520this%250Aissue.%2520It%2520takes%2520into%2520account%2520the%2520distribution%2520of%2520features%2520in%2520addition%2520to%2520their%250Avalues%252C%2520providing%2520a%2520more%2520robust%2520similarity%2520measure%2520for%2520instance%2520segmentation.%250AQuantitative%2520and%2520qualitative%2520results%2520on%2520the%2520Youtube-VIS2019%2520dataset%2520highlight%250Athe%2520improvements%2520achieved%2520by%2520the%2520proposed%2520channel%2520reduction%2520methods%2520and%2520the%2520use%250Aof%2520BoC%2520instead%2520of%2520the%2520conventional%2520dot%2520product%2520for%2520creating%2520the%2520affinity%250Amatrix.%2520These%2520improvements%2520are%2520observed%2520in%2520terms%2520of%2520mean%2520Intersection%2520over%250AUnion%2520and%2520extracted%2520instance%2520segments%252C%2520demonstrating%2520enhanced%2520instance%250Asegmentation%2520performance.%2520The%2520code%2520is%2520available%2520on%253A%250Ahttps%253A//github.com/farnooshar/SpecUnIIS%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02474v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Spectral%20Improvement%20for%20Unsupervised%20Image%20Instance%20Segmentation&entry.906535625=Farnoosh%20Arefi%20and%20Amir%20M.%20Mansourian%20and%20Shohreh%20Kasaei&entry.1292438233=%20%20Deep%20spectral%20methods%20reframe%20the%20image%20decomposition%20process%20as%20a%20graph%0Apartitioning%20task%20by%20extracting%20features%20using%20self-supervised%20learning%20and%0Autilizing%20the%20Laplacian%20of%20the%20affinity%20matrix%20to%20obtain%20eigensegments.%0AHowever%2C%20instance%20segmentation%20has%20received%20less%20attention%20compared%20to%20other%0Atasks%20within%20the%20context%20of%20deep%20spectral%20methods.%20This%20paper%20addresses%20the%0Afact%20that%20not%20all%20channels%20of%20the%20feature%20map%20extracted%20from%20a%20self-supervised%0Abackbone%20contain%20sufficient%20information%20for%20instance%20segmentation%20purposes.%20In%0Afact%2C%20Some%20channels%20are%20noisy%20and%20hinder%20the%20accuracy%20of%20the%20task.%20To%20overcome%0Athis%20issue%2C%20this%20paper%20proposes%20two%20channel%20reduction%20modules%3A%20Noise%20Channel%0AReduction%20%28NCR%29%20and%20Deviation-based%20Channel%20Reduction%20%28DCR%29.%20The%20NCR%20retains%0Achannels%20with%20lower%20entropy%2C%20as%20they%20are%20less%20likely%20to%20be%20noisy%2C%20while%20DCR%0Aprunes%20channels%20with%20low%20standard%20deviation%2C%20as%20they%20lack%20sufficient%0Ainformation%20for%20effective%20instance%20segmentation.%20Furthermore%2C%20the%20paper%0Ademonstrates%20that%20the%20dot%20product%2C%20commonly%20used%20in%20deep%20spectral%20methods%2C%20is%0Anot%20suitable%20for%20instance%20segmentation%20due%20to%20its%20sensitivity%20to%20feature%20map%0Avalues%2C%20potentially%20leading%20to%20incorrect%20instance%20segments.%20A%20new%20similarity%0Ametric%20called%20Bray-Curtis%20over%20Chebyshev%20%28BoC%29%20is%20proposed%20to%20address%20this%0Aissue.%20It%20takes%20into%20account%20the%20distribution%20of%20features%20in%20addition%20to%20their%0Avalues%2C%20providing%20a%20more%20robust%20similarity%20measure%20for%20instance%20segmentation.%0AQuantitative%20and%20qualitative%20results%20on%20the%20Youtube-VIS2019%20dataset%20highlight%0Athe%20improvements%20achieved%20by%20the%20proposed%20channel%20reduction%20methods%20and%20the%20use%0Aof%20BoC%20instead%20of%20the%20conventional%20dot%20product%20for%20creating%20the%20affinity%0Amatrix.%20These%20improvements%20are%20observed%20in%20terms%20of%20mean%20Intersection%20over%0AUnion%20and%20extracted%20instance%20segments%2C%20demonstrating%20enhanced%20instance%0Asegmentation%20performance.%20The%20code%20is%20available%20on%3A%0Ahttps%3A//github.com/farnooshar/SpecUnIIS%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02474v3&entry.124074799=Read"},
{"title": "Continuum Limits of Ollivier's Ricci Curvature on data clouds: pointwise\n  consistency and global lower bounds", "author": "Nicolas Garcia Trillos and Melanie Weber", "abstract": "  Let $M$ denote a low-dimensional manifold embedded in Euclidean space and let\n${X}= \\{ x_1, \\dots, x_n \\}$ be a collection of points uniformly sampled from\nit. We study the relationship between the curvature of a random geometric graph\nbuilt from ${X}$ and the curvature of the manifold $M$ via continuum limits of\nOllivier's discrete Ricci curvature. We prove pointwise, non-asymptotic\nconsistency results and also show that if $M$ has Ricci curvature bounded from\nbelow by a positive constant, then the random geometric graph will inherit this\nglobal structural property with high probability. We discuss applications of\nthe global discrete curvature bounds to contraction properties of heat kernels\non graphs, as well as implications for manifold learning from data clouds. In\nparticular, we show that our consistency results allow for estimating the\nintrinsic curvature of a manifold by first estimating concrete extrinsic\nquantities.\n", "link": "http://arxiv.org/abs/2307.02378v2", "date": "2024-08-26", "relevancy": 2.0695, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4462}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3988}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3967}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continuum%20Limits%20of%20Ollivier%27s%20Ricci%20Curvature%20on%20data%20clouds%3A%20pointwise%0A%20%20consistency%20and%20global%20lower%20bounds&body=Title%3A%20Continuum%20Limits%20of%20Ollivier%27s%20Ricci%20Curvature%20on%20data%20clouds%3A%20pointwise%0A%20%20consistency%20and%20global%20lower%20bounds%0AAuthor%3A%20Nicolas%20Garcia%20Trillos%20and%20Melanie%20Weber%0AAbstract%3A%20%20%20Let%20%24M%24%20denote%20a%20low-dimensional%20manifold%20embedded%20in%20Euclidean%20space%20and%20let%0A%24%7BX%7D%3D%20%5C%7B%20x_1%2C%20%5Cdots%2C%20x_n%20%5C%7D%24%20be%20a%20collection%20of%20points%20uniformly%20sampled%20from%0Ait.%20We%20study%20the%20relationship%20between%20the%20curvature%20of%20a%20random%20geometric%20graph%0Abuilt%20from%20%24%7BX%7D%24%20and%20the%20curvature%20of%20the%20manifold%20%24M%24%20via%20continuum%20limits%20of%0AOllivier%27s%20discrete%20Ricci%20curvature.%20We%20prove%20pointwise%2C%20non-asymptotic%0Aconsistency%20results%20and%20also%20show%20that%20if%20%24M%24%20has%20Ricci%20curvature%20bounded%20from%0Abelow%20by%20a%20positive%20constant%2C%20then%20the%20random%20geometric%20graph%20will%20inherit%20this%0Aglobal%20structural%20property%20with%20high%20probability.%20We%20discuss%20applications%20of%0Athe%20global%20discrete%20curvature%20bounds%20to%20contraction%20properties%20of%20heat%20kernels%0Aon%20graphs%2C%20as%20well%20as%20implications%20for%20manifold%20learning%20from%20data%20clouds.%20In%0Aparticular%2C%20we%20show%20that%20our%20consistency%20results%20allow%20for%20estimating%20the%0Aintrinsic%20curvature%20of%20a%20manifold%20by%20first%20estimating%20concrete%20extrinsic%0Aquantities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.02378v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinuum%2520Limits%2520of%2520Ollivier%2527s%2520Ricci%2520Curvature%2520on%2520data%2520clouds%253A%2520pointwise%250A%2520%2520consistency%2520and%2520global%2520lower%2520bounds%26entry.906535625%3DNicolas%2520Garcia%2520Trillos%2520and%2520Melanie%2520Weber%26entry.1292438233%3D%2520%2520Let%2520%2524M%2524%2520denote%2520a%2520low-dimensional%2520manifold%2520embedded%2520in%2520Euclidean%2520space%2520and%2520let%250A%2524%257BX%257D%253D%2520%255C%257B%2520x_1%252C%2520%255Cdots%252C%2520x_n%2520%255C%257D%2524%2520be%2520a%2520collection%2520of%2520points%2520uniformly%2520sampled%2520from%250Ait.%2520We%2520study%2520the%2520relationship%2520between%2520the%2520curvature%2520of%2520a%2520random%2520geometric%2520graph%250Abuilt%2520from%2520%2524%257BX%257D%2524%2520and%2520the%2520curvature%2520of%2520the%2520manifold%2520%2524M%2524%2520via%2520continuum%2520limits%2520of%250AOllivier%2527s%2520discrete%2520Ricci%2520curvature.%2520We%2520prove%2520pointwise%252C%2520non-asymptotic%250Aconsistency%2520results%2520and%2520also%2520show%2520that%2520if%2520%2524M%2524%2520has%2520Ricci%2520curvature%2520bounded%2520from%250Abelow%2520by%2520a%2520positive%2520constant%252C%2520then%2520the%2520random%2520geometric%2520graph%2520will%2520inherit%2520this%250Aglobal%2520structural%2520property%2520with%2520high%2520probability.%2520We%2520discuss%2520applications%2520of%250Athe%2520global%2520discrete%2520curvature%2520bounds%2520to%2520contraction%2520properties%2520of%2520heat%2520kernels%250Aon%2520graphs%252C%2520as%2520well%2520as%2520implications%2520for%2520manifold%2520learning%2520from%2520data%2520clouds.%2520In%250Aparticular%252C%2520we%2520show%2520that%2520our%2520consistency%2520results%2520allow%2520for%2520estimating%2520the%250Aintrinsic%2520curvature%2520of%2520a%2520manifold%2520by%2520first%2520estimating%2520concrete%2520extrinsic%250Aquantities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.02378v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continuum%20Limits%20of%20Ollivier%27s%20Ricci%20Curvature%20on%20data%20clouds%3A%20pointwise%0A%20%20consistency%20and%20global%20lower%20bounds&entry.906535625=Nicolas%20Garcia%20Trillos%20and%20Melanie%20Weber&entry.1292438233=%20%20Let%20%24M%24%20denote%20a%20low-dimensional%20manifold%20embedded%20in%20Euclidean%20space%20and%20let%0A%24%7BX%7D%3D%20%5C%7B%20x_1%2C%20%5Cdots%2C%20x_n%20%5C%7D%24%20be%20a%20collection%20of%20points%20uniformly%20sampled%20from%0Ait.%20We%20study%20the%20relationship%20between%20the%20curvature%20of%20a%20random%20geometric%20graph%0Abuilt%20from%20%24%7BX%7D%24%20and%20the%20curvature%20of%20the%20manifold%20%24M%24%20via%20continuum%20limits%20of%0AOllivier%27s%20discrete%20Ricci%20curvature.%20We%20prove%20pointwise%2C%20non-asymptotic%0Aconsistency%20results%20and%20also%20show%20that%20if%20%24M%24%20has%20Ricci%20curvature%20bounded%20from%0Abelow%20by%20a%20positive%20constant%2C%20then%20the%20random%20geometric%20graph%20will%20inherit%20this%0Aglobal%20structural%20property%20with%20high%20probability.%20We%20discuss%20applications%20of%0Athe%20global%20discrete%20curvature%20bounds%20to%20contraction%20properties%20of%20heat%20kernels%0Aon%20graphs%2C%20as%20well%20as%20implications%20for%20manifold%20learning%20from%20data%20clouds.%20In%0Aparticular%2C%20we%20show%20that%20our%20consistency%20results%20allow%20for%20estimating%20the%0Aintrinsic%20curvature%20of%20a%20manifold%20by%20first%20estimating%20concrete%20extrinsic%0Aquantities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.02378v2&entry.124074799=Read"},
{"title": "Social perception of faces in a vision-language model", "author": "Carina I. Hausladen and Manuel Knott and Colin F. Camerer and Pietro Perona", "abstract": "  We explore social perception of human faces in CLIP, a widely used\nopen-source vision-language model. To this end, we compare the similarity in\nCLIP embeddings between different textual prompts and a set of face images. Our\ntextual prompts are constructed from well-validated social psychology terms\ndenoting social perception. The face images are synthetic and are\nsystematically and independently varied along six dimensions: the legally\nprotected attributes of age, gender, and race, as well as facial expression,\nlighting, and pose. Independently and systematically manipulating face\nattributes allows us to study the effect of each on social perception and\navoids confounds that can occur in wild-collected data due to uncontrolled\nsystematic correlations between attributes. Thus, our findings are experimental\nrather than observational. Our main findings are three. First, while CLIP is\ntrained on the widest variety of images and texts, it is able to make\nfine-grained human-like social judgments on face images. Second, age, gender,\nand race do systematically impact CLIP's social perception of faces, suggesting\nan undesirable bias in CLIP vis-a-vis legally protected attributes. Most\nstrikingly, we find a strong pattern of bias concerning the faces of Black\nwomen, where CLIP produces extreme values of social perception across different\nages and facial expressions. Third, facial expression impacts social perception\nmore than age and lighting as much as age. The last finding predicts that\nstudies that do not control for unprotected visual attributes may reach the\nwrong conclusions on bias. Our novel method of investigation, which is founded\non the social psychology literature and on the experiments involving the\nmanipulation of individual attributes, yields sharper and more reliable\nobservations than previous observational methods and may be applied to study\nbiases in any vision-language model.\n", "link": "http://arxiv.org/abs/2408.14435v1", "date": "2024-08-26", "relevancy": 2.068, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5196}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5152}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Social%20perception%20of%20faces%20in%20a%20vision-language%20model&body=Title%3A%20Social%20perception%20of%20faces%20in%20a%20vision-language%20model%0AAuthor%3A%20Carina%20I.%20Hausladen%20and%20Manuel%20Knott%20and%20Colin%20F.%20Camerer%20and%20Pietro%20Perona%0AAbstract%3A%20%20%20We%20explore%20social%20perception%20of%20human%20faces%20in%20CLIP%2C%20a%20widely%20used%0Aopen-source%20vision-language%20model.%20To%20this%20end%2C%20we%20compare%20the%20similarity%20in%0ACLIP%20embeddings%20between%20different%20textual%20prompts%20and%20a%20set%20of%20face%20images.%20Our%0Atextual%20prompts%20are%20constructed%20from%20well-validated%20social%20psychology%20terms%0Adenoting%20social%20perception.%20The%20face%20images%20are%20synthetic%20and%20are%0Asystematically%20and%20independently%20varied%20along%20six%20dimensions%3A%20the%20legally%0Aprotected%20attributes%20of%20age%2C%20gender%2C%20and%20race%2C%20as%20well%20as%20facial%20expression%2C%0Alighting%2C%20and%20pose.%20Independently%20and%20systematically%20manipulating%20face%0Aattributes%20allows%20us%20to%20study%20the%20effect%20of%20each%20on%20social%20perception%20and%0Aavoids%20confounds%20that%20can%20occur%20in%20wild-collected%20data%20due%20to%20uncontrolled%0Asystematic%20correlations%20between%20attributes.%20Thus%2C%20our%20findings%20are%20experimental%0Arather%20than%20observational.%20Our%20main%20findings%20are%20three.%20First%2C%20while%20CLIP%20is%0Atrained%20on%20the%20widest%20variety%20of%20images%20and%20texts%2C%20it%20is%20able%20to%20make%0Afine-grained%20human-like%20social%20judgments%20on%20face%20images.%20Second%2C%20age%2C%20gender%2C%0Aand%20race%20do%20systematically%20impact%20CLIP%27s%20social%20perception%20of%20faces%2C%20suggesting%0Aan%20undesirable%20bias%20in%20CLIP%20vis-a-vis%20legally%20protected%20attributes.%20Most%0Astrikingly%2C%20we%20find%20a%20strong%20pattern%20of%20bias%20concerning%20the%20faces%20of%20Black%0Awomen%2C%20where%20CLIP%20produces%20extreme%20values%20of%20social%20perception%20across%20different%0Aages%20and%20facial%20expressions.%20Third%2C%20facial%20expression%20impacts%20social%20perception%0Amore%20than%20age%20and%20lighting%20as%20much%20as%20age.%20The%20last%20finding%20predicts%20that%0Astudies%20that%20do%20not%20control%20for%20unprotected%20visual%20attributes%20may%20reach%20the%0Awrong%20conclusions%20on%20bias.%20Our%20novel%20method%20of%20investigation%2C%20which%20is%20founded%0Aon%20the%20social%20psychology%20literature%20and%20on%20the%20experiments%20involving%20the%0Amanipulation%20of%20individual%20attributes%2C%20yields%20sharper%20and%20more%20reliable%0Aobservations%20than%20previous%20observational%20methods%20and%20may%20be%20applied%20to%20study%0Abiases%20in%20any%20vision-language%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14435v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSocial%2520perception%2520of%2520faces%2520in%2520a%2520vision-language%2520model%26entry.906535625%3DCarina%2520I.%2520Hausladen%2520and%2520Manuel%2520Knott%2520and%2520Colin%2520F.%2520Camerer%2520and%2520Pietro%2520Perona%26entry.1292438233%3D%2520%2520We%2520explore%2520social%2520perception%2520of%2520human%2520faces%2520in%2520CLIP%252C%2520a%2520widely%2520used%250Aopen-source%2520vision-language%2520model.%2520To%2520this%2520end%252C%2520we%2520compare%2520the%2520similarity%2520in%250ACLIP%2520embeddings%2520between%2520different%2520textual%2520prompts%2520and%2520a%2520set%2520of%2520face%2520images.%2520Our%250Atextual%2520prompts%2520are%2520constructed%2520from%2520well-validated%2520social%2520psychology%2520terms%250Adenoting%2520social%2520perception.%2520The%2520face%2520images%2520are%2520synthetic%2520and%2520are%250Asystematically%2520and%2520independently%2520varied%2520along%2520six%2520dimensions%253A%2520the%2520legally%250Aprotected%2520attributes%2520of%2520age%252C%2520gender%252C%2520and%2520race%252C%2520as%2520well%2520as%2520facial%2520expression%252C%250Alighting%252C%2520and%2520pose.%2520Independently%2520and%2520systematically%2520manipulating%2520face%250Aattributes%2520allows%2520us%2520to%2520study%2520the%2520effect%2520of%2520each%2520on%2520social%2520perception%2520and%250Aavoids%2520confounds%2520that%2520can%2520occur%2520in%2520wild-collected%2520data%2520due%2520to%2520uncontrolled%250Asystematic%2520correlations%2520between%2520attributes.%2520Thus%252C%2520our%2520findings%2520are%2520experimental%250Arather%2520than%2520observational.%2520Our%2520main%2520findings%2520are%2520three.%2520First%252C%2520while%2520CLIP%2520is%250Atrained%2520on%2520the%2520widest%2520variety%2520of%2520images%2520and%2520texts%252C%2520it%2520is%2520able%2520to%2520make%250Afine-grained%2520human-like%2520social%2520judgments%2520on%2520face%2520images.%2520Second%252C%2520age%252C%2520gender%252C%250Aand%2520race%2520do%2520systematically%2520impact%2520CLIP%2527s%2520social%2520perception%2520of%2520faces%252C%2520suggesting%250Aan%2520undesirable%2520bias%2520in%2520CLIP%2520vis-a-vis%2520legally%2520protected%2520attributes.%2520Most%250Astrikingly%252C%2520we%2520find%2520a%2520strong%2520pattern%2520of%2520bias%2520concerning%2520the%2520faces%2520of%2520Black%250Awomen%252C%2520where%2520CLIP%2520produces%2520extreme%2520values%2520of%2520social%2520perception%2520across%2520different%250Aages%2520and%2520facial%2520expressions.%2520Third%252C%2520facial%2520expression%2520impacts%2520social%2520perception%250Amore%2520than%2520age%2520and%2520lighting%2520as%2520much%2520as%2520age.%2520The%2520last%2520finding%2520predicts%2520that%250Astudies%2520that%2520do%2520not%2520control%2520for%2520unprotected%2520visual%2520attributes%2520may%2520reach%2520the%250Awrong%2520conclusions%2520on%2520bias.%2520Our%2520novel%2520method%2520of%2520investigation%252C%2520which%2520is%2520founded%250Aon%2520the%2520social%2520psychology%2520literature%2520and%2520on%2520the%2520experiments%2520involving%2520the%250Amanipulation%2520of%2520individual%2520attributes%252C%2520yields%2520sharper%2520and%2520more%2520reliable%250Aobservations%2520than%2520previous%2520observational%2520methods%2520and%2520may%2520be%2520applied%2520to%2520study%250Abiases%2520in%2520any%2520vision-language%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14435v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Social%20perception%20of%20faces%20in%20a%20vision-language%20model&entry.906535625=Carina%20I.%20Hausladen%20and%20Manuel%20Knott%20and%20Colin%20F.%20Camerer%20and%20Pietro%20Perona&entry.1292438233=%20%20We%20explore%20social%20perception%20of%20human%20faces%20in%20CLIP%2C%20a%20widely%20used%0Aopen-source%20vision-language%20model.%20To%20this%20end%2C%20we%20compare%20the%20similarity%20in%0ACLIP%20embeddings%20between%20different%20textual%20prompts%20and%20a%20set%20of%20face%20images.%20Our%0Atextual%20prompts%20are%20constructed%20from%20well-validated%20social%20psychology%20terms%0Adenoting%20social%20perception.%20The%20face%20images%20are%20synthetic%20and%20are%0Asystematically%20and%20independently%20varied%20along%20six%20dimensions%3A%20the%20legally%0Aprotected%20attributes%20of%20age%2C%20gender%2C%20and%20race%2C%20as%20well%20as%20facial%20expression%2C%0Alighting%2C%20and%20pose.%20Independently%20and%20systematically%20manipulating%20face%0Aattributes%20allows%20us%20to%20study%20the%20effect%20of%20each%20on%20social%20perception%20and%0Aavoids%20confounds%20that%20can%20occur%20in%20wild-collected%20data%20due%20to%20uncontrolled%0Asystematic%20correlations%20between%20attributes.%20Thus%2C%20our%20findings%20are%20experimental%0Arather%20than%20observational.%20Our%20main%20findings%20are%20three.%20First%2C%20while%20CLIP%20is%0Atrained%20on%20the%20widest%20variety%20of%20images%20and%20texts%2C%20it%20is%20able%20to%20make%0Afine-grained%20human-like%20social%20judgments%20on%20face%20images.%20Second%2C%20age%2C%20gender%2C%0Aand%20race%20do%20systematically%20impact%20CLIP%27s%20social%20perception%20of%20faces%2C%20suggesting%0Aan%20undesirable%20bias%20in%20CLIP%20vis-a-vis%20legally%20protected%20attributes.%20Most%0Astrikingly%2C%20we%20find%20a%20strong%20pattern%20of%20bias%20concerning%20the%20faces%20of%20Black%0Awomen%2C%20where%20CLIP%20produces%20extreme%20values%20of%20social%20perception%20across%20different%0Aages%20and%20facial%20expressions.%20Third%2C%20facial%20expression%20impacts%20social%20perception%0Amore%20than%20age%20and%20lighting%20as%20much%20as%20age.%20The%20last%20finding%20predicts%20that%0Astudies%20that%20do%20not%20control%20for%20unprotected%20visual%20attributes%20may%20reach%20the%0Awrong%20conclusions%20on%20bias.%20Our%20novel%20method%20of%20investigation%2C%20which%20is%20founded%0Aon%20the%20social%20psychology%20literature%20and%20on%20the%20experiments%20involving%20the%0Amanipulation%20of%20individual%20attributes%2C%20yields%20sharper%20and%20more%20reliable%0Aobservations%20than%20previous%20observational%20methods%20and%20may%20be%20applied%20to%20study%0Abiases%20in%20any%20vision-language%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14435v1&entry.124074799=Read"},
{"title": "Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification", "author": "Mahrukh Awan and Asmar Nadeem and Muhammad Junaid Awan and Armin Mustafa and Syed Sameed Husain", "abstract": "  Exploiting both audio and visual modalities for video classification is a\nchallenging task, as the existing methods require large model architectures,\nleading to high computational complexity and resource requirements. Smaller\narchitectures, on the other hand, struggle to achieve optimal performance. In\nthis paper, we propose Attend-Fusion, an audio-visual (AV) fusion approach that\nintroduces a compact model architecture specifically designed to capture\nintricate audio-visual relationships in video data. Through extensive\nexperiments on the challenging YouTube-8M dataset, we demonstrate that\nAttend-Fusion achieves an F1 score of 75.64\\% with only 72M parameters, which\nis comparable to the performance of larger baseline models such as\nFully-Connected Late Fusion (75.96\\% F1 score, 341M parameters). Attend-Fusion\nachieves similar performance to the larger baseline model while reducing the\nmodel size by nearly 80\\%, highlighting its efficiency in terms of model\ncomplexity. Our work demonstrates that the Attend-Fusion model effectively\ncombines audio and visual information for video classification, achieving\ncompetitive performance with significantly reduced model size. This approach\nopens new possibilities for deploying high-performance video understanding\nsystems in resource-constrained environments across various applications.\n", "link": "http://arxiv.org/abs/2408.14441v1", "date": "2024-08-26", "relevancy": 2.0587, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5498}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5087}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attend-Fusion%3A%20Efficient%20Audio-Visual%20Fusion%20for%20Video%20Classification&body=Title%3A%20Attend-Fusion%3A%20Efficient%20Audio-Visual%20Fusion%20for%20Video%20Classification%0AAuthor%3A%20Mahrukh%20Awan%20and%20Asmar%20Nadeem%20and%20Muhammad%20Junaid%20Awan%20and%20Armin%20Mustafa%20and%20Syed%20Sameed%20Husain%0AAbstract%3A%20%20%20Exploiting%20both%20audio%20and%20visual%20modalities%20for%20video%20classification%20is%20a%0Achallenging%20task%2C%20as%20the%20existing%20methods%20require%20large%20model%20architectures%2C%0Aleading%20to%20high%20computational%20complexity%20and%20resource%20requirements.%20Smaller%0Aarchitectures%2C%20on%20the%20other%20hand%2C%20struggle%20to%20achieve%20optimal%20performance.%20In%0Athis%20paper%2C%20we%20propose%20Attend-Fusion%2C%20an%20audio-visual%20%28AV%29%20fusion%20approach%20that%0Aintroduces%20a%20compact%20model%20architecture%20specifically%20designed%20to%20capture%0Aintricate%20audio-visual%20relationships%20in%20video%20data.%20Through%20extensive%0Aexperiments%20on%20the%20challenging%20YouTube-8M%20dataset%2C%20we%20demonstrate%20that%0AAttend-Fusion%20achieves%20an%20F1%20score%20of%2075.64%5C%25%20with%20only%2072M%20parameters%2C%20which%0Ais%20comparable%20to%20the%20performance%20of%20larger%20baseline%20models%20such%20as%0AFully-Connected%20Late%20Fusion%20%2875.96%5C%25%20F1%20score%2C%20341M%20parameters%29.%20Attend-Fusion%0Aachieves%20similar%20performance%20to%20the%20larger%20baseline%20model%20while%20reducing%20the%0Amodel%20size%20by%20nearly%2080%5C%25%2C%20highlighting%20its%20efficiency%20in%20terms%20of%20model%0Acomplexity.%20Our%20work%20demonstrates%20that%20the%20Attend-Fusion%20model%20effectively%0Acombines%20audio%20and%20visual%20information%20for%20video%20classification%2C%20achieving%0Acompetitive%20performance%20with%20significantly%20reduced%20model%20size.%20This%20approach%0Aopens%20new%20possibilities%20for%20deploying%20high-performance%20video%20understanding%0Asystems%20in%20resource-constrained%20environments%20across%20various%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14441v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttend-Fusion%253A%2520Efficient%2520Audio-Visual%2520Fusion%2520for%2520Video%2520Classification%26entry.906535625%3DMahrukh%2520Awan%2520and%2520Asmar%2520Nadeem%2520and%2520Muhammad%2520Junaid%2520Awan%2520and%2520Armin%2520Mustafa%2520and%2520Syed%2520Sameed%2520Husain%26entry.1292438233%3D%2520%2520Exploiting%2520both%2520audio%2520and%2520visual%2520modalities%2520for%2520video%2520classification%2520is%2520a%250Achallenging%2520task%252C%2520as%2520the%2520existing%2520methods%2520require%2520large%2520model%2520architectures%252C%250Aleading%2520to%2520high%2520computational%2520complexity%2520and%2520resource%2520requirements.%2520Smaller%250Aarchitectures%252C%2520on%2520the%2520other%2520hand%252C%2520struggle%2520to%2520achieve%2520optimal%2520performance.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520Attend-Fusion%252C%2520an%2520audio-visual%2520%2528AV%2529%2520fusion%2520approach%2520that%250Aintroduces%2520a%2520compact%2520model%2520architecture%2520specifically%2520designed%2520to%2520capture%250Aintricate%2520audio-visual%2520relationships%2520in%2520video%2520data.%2520Through%2520extensive%250Aexperiments%2520on%2520the%2520challenging%2520YouTube-8M%2520dataset%252C%2520we%2520demonstrate%2520that%250AAttend-Fusion%2520achieves%2520an%2520F1%2520score%2520of%252075.64%255C%2525%2520with%2520only%252072M%2520parameters%252C%2520which%250Ais%2520comparable%2520to%2520the%2520performance%2520of%2520larger%2520baseline%2520models%2520such%2520as%250AFully-Connected%2520Late%2520Fusion%2520%252875.96%255C%2525%2520F1%2520score%252C%2520341M%2520parameters%2529.%2520Attend-Fusion%250Aachieves%2520similar%2520performance%2520to%2520the%2520larger%2520baseline%2520model%2520while%2520reducing%2520the%250Amodel%2520size%2520by%2520nearly%252080%255C%2525%252C%2520highlighting%2520its%2520efficiency%2520in%2520terms%2520of%2520model%250Acomplexity.%2520Our%2520work%2520demonstrates%2520that%2520the%2520Attend-Fusion%2520model%2520effectively%250Acombines%2520audio%2520and%2520visual%2520information%2520for%2520video%2520classification%252C%2520achieving%250Acompetitive%2520performance%2520with%2520significantly%2520reduced%2520model%2520size.%2520This%2520approach%250Aopens%2520new%2520possibilities%2520for%2520deploying%2520high-performance%2520video%2520understanding%250Asystems%2520in%2520resource-constrained%2520environments%2520across%2520various%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14441v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attend-Fusion%3A%20Efficient%20Audio-Visual%20Fusion%20for%20Video%20Classification&entry.906535625=Mahrukh%20Awan%20and%20Asmar%20Nadeem%20and%20Muhammad%20Junaid%20Awan%20and%20Armin%20Mustafa%20and%20Syed%20Sameed%20Husain&entry.1292438233=%20%20Exploiting%20both%20audio%20and%20visual%20modalities%20for%20video%20classification%20is%20a%0Achallenging%20task%2C%20as%20the%20existing%20methods%20require%20large%20model%20architectures%2C%0Aleading%20to%20high%20computational%20complexity%20and%20resource%20requirements.%20Smaller%0Aarchitectures%2C%20on%20the%20other%20hand%2C%20struggle%20to%20achieve%20optimal%20performance.%20In%0Athis%20paper%2C%20we%20propose%20Attend-Fusion%2C%20an%20audio-visual%20%28AV%29%20fusion%20approach%20that%0Aintroduces%20a%20compact%20model%20architecture%20specifically%20designed%20to%20capture%0Aintricate%20audio-visual%20relationships%20in%20video%20data.%20Through%20extensive%0Aexperiments%20on%20the%20challenging%20YouTube-8M%20dataset%2C%20we%20demonstrate%20that%0AAttend-Fusion%20achieves%20an%20F1%20score%20of%2075.64%5C%25%20with%20only%2072M%20parameters%2C%20which%0Ais%20comparable%20to%20the%20performance%20of%20larger%20baseline%20models%20such%20as%0AFully-Connected%20Late%20Fusion%20%2875.96%5C%25%20F1%20score%2C%20341M%20parameters%29.%20Attend-Fusion%0Aachieves%20similar%20performance%20to%20the%20larger%20baseline%20model%20while%20reducing%20the%0Amodel%20size%20by%20nearly%2080%5C%25%2C%20highlighting%20its%20efficiency%20in%20terms%20of%20model%0Acomplexity.%20Our%20work%20demonstrates%20that%20the%20Attend-Fusion%20model%20effectively%0Acombines%20audio%20and%20visual%20information%20for%20video%20classification%2C%20achieving%0Acompetitive%20performance%20with%20significantly%20reduced%20model%20size.%20This%20approach%0Aopens%20new%20possibilities%20for%20deploying%20high-performance%20video%20understanding%0Asystems%20in%20resource-constrained%20environments%20across%20various%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14441v1&entry.124074799=Read"},
{"title": "DynamicRouteGPT: A Real-Time Multi-Vehicle Dynamic Navigation Framework\n  Based on Large Language Models", "author": "Ziai Zhou and Bin Zhou and Hao Liu", "abstract": "  Real-time dynamic path planning in complex traffic environments presents\nchallenges, such as varying traffic volumes and signal wait times. Traditional\nstatic routing algorithms like Dijkstra and A* compute shortest paths but often\nfail under dynamic conditions. Recent Reinforcement Learning (RL) approaches\noffer improvements but tend to focus on local optima, risking dead-ends or\nboundary issues. This paper proposes a novel approach based on causal inference\nfor real-time dynamic path planning, balancing global and local optimality. We\nfirst use the static Dijkstra algorithm to compute a globally optimal baseline\npath. A distributed control strategy then guides vehicles along this path. At\nintersections, DynamicRouteGPT performs real-time decision-making for local\npath selection, considering real-time traffic, driving preferences, and\nunexpected events. DynamicRouteGPT integrates Markov chains, Bayesian\ninference, and large-scale pretrained language models like Llama3 8B to provide\nan efficient path planning solution. It dynamically adjusts to traffic\nscenarios and driver preferences and requires no pre-training, offering broad\napplicability across road networks. A key innovation is the construction of\ncausal graphs for counterfactual reasoning, optimizing path decisions.\nExperimental results show that our method achieves state-of-the-art performance\nin real-time dynamic path planning for multiple vehicles while providing\nexplainable path selections, offering a novel and efficient solution for\ncomplex traffic environments.\n", "link": "http://arxiv.org/abs/2408.14185v1", "date": "2024-08-26", "relevancy": 2.0578, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5522}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5074}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynamicRouteGPT%3A%20A%20Real-Time%20Multi-Vehicle%20Dynamic%20Navigation%20Framework%0A%20%20Based%20on%20Large%20Language%20Models&body=Title%3A%20DynamicRouteGPT%3A%20A%20Real-Time%20Multi-Vehicle%20Dynamic%20Navigation%20Framework%0A%20%20Based%20on%20Large%20Language%20Models%0AAuthor%3A%20Ziai%20Zhou%20and%20Bin%20Zhou%20and%20Hao%20Liu%0AAbstract%3A%20%20%20Real-time%20dynamic%20path%20planning%20in%20complex%20traffic%20environments%20presents%0Achallenges%2C%20such%20as%20varying%20traffic%20volumes%20and%20signal%20wait%20times.%20Traditional%0Astatic%20routing%20algorithms%20like%20Dijkstra%20and%20A%2A%20compute%20shortest%20paths%20but%20often%0Afail%20under%20dynamic%20conditions.%20Recent%20Reinforcement%20Learning%20%28RL%29%20approaches%0Aoffer%20improvements%20but%20tend%20to%20focus%20on%20local%20optima%2C%20risking%20dead-ends%20or%0Aboundary%20issues.%20This%20paper%20proposes%20a%20novel%20approach%20based%20on%20causal%20inference%0Afor%20real-time%20dynamic%20path%20planning%2C%20balancing%20global%20and%20local%20optimality.%20We%0Afirst%20use%20the%20static%20Dijkstra%20algorithm%20to%20compute%20a%20globally%20optimal%20baseline%0Apath.%20A%20distributed%20control%20strategy%20then%20guides%20vehicles%20along%20this%20path.%20At%0Aintersections%2C%20DynamicRouteGPT%20performs%20real-time%20decision-making%20for%20local%0Apath%20selection%2C%20considering%20real-time%20traffic%2C%20driving%20preferences%2C%20and%0Aunexpected%20events.%20DynamicRouteGPT%20integrates%20Markov%20chains%2C%20Bayesian%0Ainference%2C%20and%20large-scale%20pretrained%20language%20models%20like%20Llama3%208B%20to%20provide%0Aan%20efficient%20path%20planning%20solution.%20It%20dynamically%20adjusts%20to%20traffic%0Ascenarios%20and%20driver%20preferences%20and%20requires%20no%20pre-training%2C%20offering%20broad%0Aapplicability%20across%20road%20networks.%20A%20key%20innovation%20is%20the%20construction%20of%0Acausal%20graphs%20for%20counterfactual%20reasoning%2C%20optimizing%20path%20decisions.%0AExperimental%20results%20show%20that%20our%20method%20achieves%20state-of-the-art%20performance%0Ain%20real-time%20dynamic%20path%20planning%20for%20multiple%20vehicles%20while%20providing%0Aexplainable%20path%20selections%2C%20offering%20a%20novel%20and%20efficient%20solution%20for%0Acomplex%20traffic%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14185v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamicRouteGPT%253A%2520A%2520Real-Time%2520Multi-Vehicle%2520Dynamic%2520Navigation%2520Framework%250A%2520%2520Based%2520on%2520Large%2520Language%2520Models%26entry.906535625%3DZiai%2520Zhou%2520and%2520Bin%2520Zhou%2520and%2520Hao%2520Liu%26entry.1292438233%3D%2520%2520Real-time%2520dynamic%2520path%2520planning%2520in%2520complex%2520traffic%2520environments%2520presents%250Achallenges%252C%2520such%2520as%2520varying%2520traffic%2520volumes%2520and%2520signal%2520wait%2520times.%2520Traditional%250Astatic%2520routing%2520algorithms%2520like%2520Dijkstra%2520and%2520A%252A%2520compute%2520shortest%2520paths%2520but%2520often%250Afail%2520under%2520dynamic%2520conditions.%2520Recent%2520Reinforcement%2520Learning%2520%2528RL%2529%2520approaches%250Aoffer%2520improvements%2520but%2520tend%2520to%2520focus%2520on%2520local%2520optima%252C%2520risking%2520dead-ends%2520or%250Aboundary%2520issues.%2520This%2520paper%2520proposes%2520a%2520novel%2520approach%2520based%2520on%2520causal%2520inference%250Afor%2520real-time%2520dynamic%2520path%2520planning%252C%2520balancing%2520global%2520and%2520local%2520optimality.%2520We%250Afirst%2520use%2520the%2520static%2520Dijkstra%2520algorithm%2520to%2520compute%2520a%2520globally%2520optimal%2520baseline%250Apath.%2520A%2520distributed%2520control%2520strategy%2520then%2520guides%2520vehicles%2520along%2520this%2520path.%2520At%250Aintersections%252C%2520DynamicRouteGPT%2520performs%2520real-time%2520decision-making%2520for%2520local%250Apath%2520selection%252C%2520considering%2520real-time%2520traffic%252C%2520driving%2520preferences%252C%2520and%250Aunexpected%2520events.%2520DynamicRouteGPT%2520integrates%2520Markov%2520chains%252C%2520Bayesian%250Ainference%252C%2520and%2520large-scale%2520pretrained%2520language%2520models%2520like%2520Llama3%25208B%2520to%2520provide%250Aan%2520efficient%2520path%2520planning%2520solution.%2520It%2520dynamically%2520adjusts%2520to%2520traffic%250Ascenarios%2520and%2520driver%2520preferences%2520and%2520requires%2520no%2520pre-training%252C%2520offering%2520broad%250Aapplicability%2520across%2520road%2520networks.%2520A%2520key%2520innovation%2520is%2520the%2520construction%2520of%250Acausal%2520graphs%2520for%2520counterfactual%2520reasoning%252C%2520optimizing%2520path%2520decisions.%250AExperimental%2520results%2520show%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520performance%250Ain%2520real-time%2520dynamic%2520path%2520planning%2520for%2520multiple%2520vehicles%2520while%2520providing%250Aexplainable%2520path%2520selections%252C%2520offering%2520a%2520novel%2520and%2520efficient%2520solution%2520for%250Acomplex%2520traffic%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14185v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynamicRouteGPT%3A%20A%20Real-Time%20Multi-Vehicle%20Dynamic%20Navigation%20Framework%0A%20%20Based%20on%20Large%20Language%20Models&entry.906535625=Ziai%20Zhou%20and%20Bin%20Zhou%20and%20Hao%20Liu&entry.1292438233=%20%20Real-time%20dynamic%20path%20planning%20in%20complex%20traffic%20environments%20presents%0Achallenges%2C%20such%20as%20varying%20traffic%20volumes%20and%20signal%20wait%20times.%20Traditional%0Astatic%20routing%20algorithms%20like%20Dijkstra%20and%20A%2A%20compute%20shortest%20paths%20but%20often%0Afail%20under%20dynamic%20conditions.%20Recent%20Reinforcement%20Learning%20%28RL%29%20approaches%0Aoffer%20improvements%20but%20tend%20to%20focus%20on%20local%20optima%2C%20risking%20dead-ends%20or%0Aboundary%20issues.%20This%20paper%20proposes%20a%20novel%20approach%20based%20on%20causal%20inference%0Afor%20real-time%20dynamic%20path%20planning%2C%20balancing%20global%20and%20local%20optimality.%20We%0Afirst%20use%20the%20static%20Dijkstra%20algorithm%20to%20compute%20a%20globally%20optimal%20baseline%0Apath.%20A%20distributed%20control%20strategy%20then%20guides%20vehicles%20along%20this%20path.%20At%0Aintersections%2C%20DynamicRouteGPT%20performs%20real-time%20decision-making%20for%20local%0Apath%20selection%2C%20considering%20real-time%20traffic%2C%20driving%20preferences%2C%20and%0Aunexpected%20events.%20DynamicRouteGPT%20integrates%20Markov%20chains%2C%20Bayesian%0Ainference%2C%20and%20large-scale%20pretrained%20language%20models%20like%20Llama3%208B%20to%20provide%0Aan%20efficient%20path%20planning%20solution.%20It%20dynamically%20adjusts%20to%20traffic%0Ascenarios%20and%20driver%20preferences%20and%20requires%20no%20pre-training%2C%20offering%20broad%0Aapplicability%20across%20road%20networks.%20A%20key%20innovation%20is%20the%20construction%20of%0Acausal%20graphs%20for%20counterfactual%20reasoning%2C%20optimizing%20path%20decisions.%0AExperimental%20results%20show%20that%20our%20method%20achieves%20state-of-the-art%20performance%0Ain%20real-time%20dynamic%20path%20planning%20for%20multiple%20vehicles%20while%20providing%0Aexplainable%20path%20selections%2C%20offering%20a%20novel%20and%20efficient%20solution%20for%0Acomplex%20traffic%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14185v1&entry.124074799=Read"},
{"title": "Exploiting Conjugate Label Information for Multi-Instance Partial-Label\n  Learning", "author": "Wei Tang and Weijia Zhang and Min-Ling Zhang", "abstract": "  Multi-instance partial-label learning (MIPL) addresses scenarios where each\ntraining sample is represented as a multi-instance bag associated with a\ncandidate label set containing one true label and several false positives.\nExisting MIPL algorithms have primarily focused on mapping multi-instance bags\nto candidate label sets for disambiguation, disregarding the intrinsic\nproperties of the label space and the supervised information provided by\nnon-candidate label sets. In this paper, we propose an algorithm named ELIMIPL,\ni.e., Exploiting conjugate Label Information for Multi-Instance Partial-Label\nlearning, which exploits the conjugate label information to improve the\ndisambiguation performance. To achieve this, we extract the label information\nembedded in both candidate and non-candidate label sets, incorporating the\nintrinsic properties of the label space. Experimental results obtained from\nbenchmark and real-world datasets demonstrate the superiority of the proposed\nELIMIPL over existing MIPL algorithms and other well-established partial-label\nlearning algorithms.\n", "link": "http://arxiv.org/abs/2408.14369v1", "date": "2024-08-26", "relevancy": 2.0513, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5385}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5298}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Conjugate%20Label%20Information%20for%20Multi-Instance%20Partial-Label%0A%20%20Learning&body=Title%3A%20Exploiting%20Conjugate%20Label%20Information%20for%20Multi-Instance%20Partial-Label%0A%20%20Learning%0AAuthor%3A%20Wei%20Tang%20and%20Weijia%20Zhang%20and%20Min-Ling%20Zhang%0AAbstract%3A%20%20%20Multi-instance%20partial-label%20learning%20%28MIPL%29%20addresses%20scenarios%20where%20each%0Atraining%20sample%20is%20represented%20as%20a%20multi-instance%20bag%20associated%20with%20a%0Acandidate%20label%20set%20containing%20one%20true%20label%20and%20several%20false%20positives.%0AExisting%20MIPL%20algorithms%20have%20primarily%20focused%20on%20mapping%20multi-instance%20bags%0Ato%20candidate%20label%20sets%20for%20disambiguation%2C%20disregarding%20the%20intrinsic%0Aproperties%20of%20the%20label%20space%20and%20the%20supervised%20information%20provided%20by%0Anon-candidate%20label%20sets.%20In%20this%20paper%2C%20we%20propose%20an%20algorithm%20named%20ELIMIPL%2C%0Ai.e.%2C%20Exploiting%20conjugate%20Label%20Information%20for%20Multi-Instance%20Partial-Label%0Alearning%2C%20which%20exploits%20the%20conjugate%20label%20information%20to%20improve%20the%0Adisambiguation%20performance.%20To%20achieve%20this%2C%20we%20extract%20the%20label%20information%0Aembedded%20in%20both%20candidate%20and%20non-candidate%20label%20sets%2C%20incorporating%20the%0Aintrinsic%20properties%20of%20the%20label%20space.%20Experimental%20results%20obtained%20from%0Abenchmark%20and%20real-world%20datasets%20demonstrate%20the%20superiority%20of%20the%20proposed%0AELIMIPL%20over%20existing%20MIPL%20algorithms%20and%20other%20well-established%20partial-label%0Alearning%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14369v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520Conjugate%2520Label%2520Information%2520for%2520Multi-Instance%2520Partial-Label%250A%2520%2520Learning%26entry.906535625%3DWei%2520Tang%2520and%2520Weijia%2520Zhang%2520and%2520Min-Ling%2520Zhang%26entry.1292438233%3D%2520%2520Multi-instance%2520partial-label%2520learning%2520%2528MIPL%2529%2520addresses%2520scenarios%2520where%2520each%250Atraining%2520sample%2520is%2520represented%2520as%2520a%2520multi-instance%2520bag%2520associated%2520with%2520a%250Acandidate%2520label%2520set%2520containing%2520one%2520true%2520label%2520and%2520several%2520false%2520positives.%250AExisting%2520MIPL%2520algorithms%2520have%2520primarily%2520focused%2520on%2520mapping%2520multi-instance%2520bags%250Ato%2520candidate%2520label%2520sets%2520for%2520disambiguation%252C%2520disregarding%2520the%2520intrinsic%250Aproperties%2520of%2520the%2520label%2520space%2520and%2520the%2520supervised%2520information%2520provided%2520by%250Anon-candidate%2520label%2520sets.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520algorithm%2520named%2520ELIMIPL%252C%250Ai.e.%252C%2520Exploiting%2520conjugate%2520Label%2520Information%2520for%2520Multi-Instance%2520Partial-Label%250Alearning%252C%2520which%2520exploits%2520the%2520conjugate%2520label%2520information%2520to%2520improve%2520the%250Adisambiguation%2520performance.%2520To%2520achieve%2520this%252C%2520we%2520extract%2520the%2520label%2520information%250Aembedded%2520in%2520both%2520candidate%2520and%2520non-candidate%2520label%2520sets%252C%2520incorporating%2520the%250Aintrinsic%2520properties%2520of%2520the%2520label%2520space.%2520Experimental%2520results%2520obtained%2520from%250Abenchmark%2520and%2520real-world%2520datasets%2520demonstrate%2520the%2520superiority%2520of%2520the%2520proposed%250AELIMIPL%2520over%2520existing%2520MIPL%2520algorithms%2520and%2520other%2520well-established%2520partial-label%250Alearning%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14369v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Conjugate%20Label%20Information%20for%20Multi-Instance%20Partial-Label%0A%20%20Learning&entry.906535625=Wei%20Tang%20and%20Weijia%20Zhang%20and%20Min-Ling%20Zhang&entry.1292438233=%20%20Multi-instance%20partial-label%20learning%20%28MIPL%29%20addresses%20scenarios%20where%20each%0Atraining%20sample%20is%20represented%20as%20a%20multi-instance%20bag%20associated%20with%20a%0Acandidate%20label%20set%20containing%20one%20true%20label%20and%20several%20false%20positives.%0AExisting%20MIPL%20algorithms%20have%20primarily%20focused%20on%20mapping%20multi-instance%20bags%0Ato%20candidate%20label%20sets%20for%20disambiguation%2C%20disregarding%20the%20intrinsic%0Aproperties%20of%20the%20label%20space%20and%20the%20supervised%20information%20provided%20by%0Anon-candidate%20label%20sets.%20In%20this%20paper%2C%20we%20propose%20an%20algorithm%20named%20ELIMIPL%2C%0Ai.e.%2C%20Exploiting%20conjugate%20Label%20Information%20for%20Multi-Instance%20Partial-Label%0Alearning%2C%20which%20exploits%20the%20conjugate%20label%20information%20to%20improve%20the%0Adisambiguation%20performance.%20To%20achieve%20this%2C%20we%20extract%20the%20label%20information%0Aembedded%20in%20both%20candidate%20and%20non-candidate%20label%20sets%2C%20incorporating%20the%0Aintrinsic%20properties%20of%20the%20label%20space.%20Experimental%20results%20obtained%20from%0Abenchmark%20and%20real-world%20datasets%20demonstrate%20the%20superiority%20of%20the%20proposed%0AELIMIPL%20over%20existing%20MIPL%20algorithms%20and%20other%20well-established%20partial-label%0Alearning%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14369v1&entry.124074799=Read"},
{"title": "Fact Probability Vector Based Goal Recognition", "author": "Nils Wilken and Lea Cohausz and Christian Bartelt and Heiner Stuckenschmidt", "abstract": "  We present a new approach to goal recognition that involves comparing\nobserved facts with their expected probabilities. These probabilities depend on\na specified goal g and initial state s0. Our method maps these probabilities\nand observed facts into a real vector space to compute heuristic values for\npotential goals. These values estimate the likelihood of a given goal being the\ntrue objective of the observed agent. As obtaining exact expected probabilities\nfor observed facts in an observation sequence is often practically infeasible,\nwe propose and empirically validate a method for approximating these\nprobabilities. Our empirical results show that the proposed approach offers\nimproved goal recognition precision compared to state-of-the-art techniques\nwhile reducing computational complexity.\n", "link": "http://arxiv.org/abs/2408.14224v1", "date": "2024-08-26", "relevancy": 2.0466, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5525}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5325}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4625}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fact%20Probability%20Vector%20Based%20Goal%20Recognition&body=Title%3A%20Fact%20Probability%20Vector%20Based%20Goal%20Recognition%0AAuthor%3A%20Nils%20Wilken%20and%20Lea%20Cohausz%20and%20Christian%20Bartelt%20and%20Heiner%20Stuckenschmidt%0AAbstract%3A%20%20%20We%20present%20a%20new%20approach%20to%20goal%20recognition%20that%20involves%20comparing%0Aobserved%20facts%20with%20their%20expected%20probabilities.%20These%20probabilities%20depend%20on%0Aa%20specified%20goal%20g%20and%20initial%20state%20s0.%20Our%20method%20maps%20these%20probabilities%0Aand%20observed%20facts%20into%20a%20real%20vector%20space%20to%20compute%20heuristic%20values%20for%0Apotential%20goals.%20These%20values%20estimate%20the%20likelihood%20of%20a%20given%20goal%20being%20the%0Atrue%20objective%20of%20the%20observed%20agent.%20As%20obtaining%20exact%20expected%20probabilities%0Afor%20observed%20facts%20in%20an%20observation%20sequence%20is%20often%20practically%20infeasible%2C%0Awe%20propose%20and%20empirically%20validate%20a%20method%20for%20approximating%20these%0Aprobabilities.%20Our%20empirical%20results%20show%20that%20the%20proposed%20approach%20offers%0Aimproved%20goal%20recognition%20precision%20compared%20to%20state-of-the-art%20techniques%0Awhile%20reducing%20computational%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14224v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFact%2520Probability%2520Vector%2520Based%2520Goal%2520Recognition%26entry.906535625%3DNils%2520Wilken%2520and%2520Lea%2520Cohausz%2520and%2520Christian%2520Bartelt%2520and%2520Heiner%2520Stuckenschmidt%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520new%2520approach%2520to%2520goal%2520recognition%2520that%2520involves%2520comparing%250Aobserved%2520facts%2520with%2520their%2520expected%2520probabilities.%2520These%2520probabilities%2520depend%2520on%250Aa%2520specified%2520goal%2520g%2520and%2520initial%2520state%2520s0.%2520Our%2520method%2520maps%2520these%2520probabilities%250Aand%2520observed%2520facts%2520into%2520a%2520real%2520vector%2520space%2520to%2520compute%2520heuristic%2520values%2520for%250Apotential%2520goals.%2520These%2520values%2520estimate%2520the%2520likelihood%2520of%2520a%2520given%2520goal%2520being%2520the%250Atrue%2520objective%2520of%2520the%2520observed%2520agent.%2520As%2520obtaining%2520exact%2520expected%2520probabilities%250Afor%2520observed%2520facts%2520in%2520an%2520observation%2520sequence%2520is%2520often%2520practically%2520infeasible%252C%250Awe%2520propose%2520and%2520empirically%2520validate%2520a%2520method%2520for%2520approximating%2520these%250Aprobabilities.%2520Our%2520empirical%2520results%2520show%2520that%2520the%2520proposed%2520approach%2520offers%250Aimproved%2520goal%2520recognition%2520precision%2520compared%2520to%2520state-of-the-art%2520techniques%250Awhile%2520reducing%2520computational%2520complexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14224v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fact%20Probability%20Vector%20Based%20Goal%20Recognition&entry.906535625=Nils%20Wilken%20and%20Lea%20Cohausz%20and%20Christian%20Bartelt%20and%20Heiner%20Stuckenschmidt&entry.1292438233=%20%20We%20present%20a%20new%20approach%20to%20goal%20recognition%20that%20involves%20comparing%0Aobserved%20facts%20with%20their%20expected%20probabilities.%20These%20probabilities%20depend%20on%0Aa%20specified%20goal%20g%20and%20initial%20state%20s0.%20Our%20method%20maps%20these%20probabilities%0Aand%20observed%20facts%20into%20a%20real%20vector%20space%20to%20compute%20heuristic%20values%20for%0Apotential%20goals.%20These%20values%20estimate%20the%20likelihood%20of%20a%20given%20goal%20being%20the%0Atrue%20objective%20of%20the%20observed%20agent.%20As%20obtaining%20exact%20expected%20probabilities%0Afor%20observed%20facts%20in%20an%20observation%20sequence%20is%20often%20practically%20infeasible%2C%0Awe%20propose%20and%20empirically%20validate%20a%20method%20for%20approximating%20these%0Aprobabilities.%20Our%20empirical%20results%20show%20that%20the%20proposed%20approach%20offers%0Aimproved%20goal%20recognition%20precision%20compared%20to%20state-of-the-art%20techniques%0Awhile%20reducing%20computational%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14224v1&entry.124074799=Read"},
{"title": "Multi-head Spatial-Spectral Mamba for Hyperspectral Image Classification", "author": "Muhammad Ahmad and Muhammad Hassaan Farooq Butt and Muhammad Usama and Hamad Ahmed Altuwaijri and Manuel Mazzara and Salvatore Distefano", "abstract": "  Spatial-Spectral Mamba (SSM) improves computational efficiency and captures\nlong-range dependencies, addressing Transformer limitations. However,\ntraditional Mamba models overlook rich spectral information in HSIs and\nstruggle with high dimensionality and sequential data. To address these issues,\nwe propose the SSM with multi-head self-attention and token enhancement\n(MHSSMamba). This model integrates spectral and spatial information by\nenhancing spectral tokens and using multi-head attention to capture complex\nrelationships between spectral bands and spatial locations. It also manages\nlong-range dependencies and the sequential nature of HSI data, preserving\ncontextual information across spectral bands. MHSSMamba achieved remarkable\nclassification accuracies of 97.62\\% on Pavia University, 96.92\\% on the\nUniversity of Houston, 96.85\\% on Salinas, and 99.49\\% on Wuhan-longKou\ndatasets. The source code is available at\n\\href{https://github.com/MHassaanButt/MHA\\_SS\\_Mamba}{GitHub}.\n", "link": "http://arxiv.org/abs/2408.01224v3", "date": "2024-08-26", "relevancy": 2.04, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5292}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4965}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-head%20Spatial-Spectral%20Mamba%20for%20Hyperspectral%20Image%20Classification&body=Title%3A%20Multi-head%20Spatial-Spectral%20Mamba%20for%20Hyperspectral%20Image%20Classification%0AAuthor%3A%20Muhammad%20Ahmad%20and%20Muhammad%20Hassaan%20Farooq%20Butt%20and%20Muhammad%20Usama%20and%20Hamad%20Ahmed%20Altuwaijri%20and%20Manuel%20Mazzara%20and%20Salvatore%20Distefano%0AAbstract%3A%20%20%20Spatial-Spectral%20Mamba%20%28SSM%29%20improves%20computational%20efficiency%20and%20captures%0Along-range%20dependencies%2C%20addressing%20Transformer%20limitations.%20However%2C%0Atraditional%20Mamba%20models%20overlook%20rich%20spectral%20information%20in%20HSIs%20and%0Astruggle%20with%20high%20dimensionality%20and%20sequential%20data.%20To%20address%20these%20issues%2C%0Awe%20propose%20the%20SSM%20with%20multi-head%20self-attention%20and%20token%20enhancement%0A%28MHSSMamba%29.%20This%20model%20integrates%20spectral%20and%20spatial%20information%20by%0Aenhancing%20spectral%20tokens%20and%20using%20multi-head%20attention%20to%20capture%20complex%0Arelationships%20between%20spectral%20bands%20and%20spatial%20locations.%20It%20also%20manages%0Along-range%20dependencies%20and%20the%20sequential%20nature%20of%20HSI%20data%2C%20preserving%0Acontextual%20information%20across%20spectral%20bands.%20MHSSMamba%20achieved%20remarkable%0Aclassification%20accuracies%20of%2097.62%5C%25%20on%20Pavia%20University%2C%2096.92%5C%25%20on%20the%0AUniversity%20of%20Houston%2C%2096.85%5C%25%20on%20Salinas%2C%20and%2099.49%5C%25%20on%20Wuhan-longKou%0Adatasets.%20The%20source%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/MHassaanButt/MHA%5C_SS%5C_Mamba%7D%7BGitHub%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01224v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-head%2520Spatial-Spectral%2520Mamba%2520for%2520Hyperspectral%2520Image%2520Classification%26entry.906535625%3DMuhammad%2520Ahmad%2520and%2520Muhammad%2520Hassaan%2520Farooq%2520Butt%2520and%2520Muhammad%2520Usama%2520and%2520Hamad%2520Ahmed%2520Altuwaijri%2520and%2520Manuel%2520Mazzara%2520and%2520Salvatore%2520Distefano%26entry.1292438233%3D%2520%2520Spatial-Spectral%2520Mamba%2520%2528SSM%2529%2520improves%2520computational%2520efficiency%2520and%2520captures%250Along-range%2520dependencies%252C%2520addressing%2520Transformer%2520limitations.%2520However%252C%250Atraditional%2520Mamba%2520models%2520overlook%2520rich%2520spectral%2520information%2520in%2520HSIs%2520and%250Astruggle%2520with%2520high%2520dimensionality%2520and%2520sequential%2520data.%2520To%2520address%2520these%2520issues%252C%250Awe%2520propose%2520the%2520SSM%2520with%2520multi-head%2520self-attention%2520and%2520token%2520enhancement%250A%2528MHSSMamba%2529.%2520This%2520model%2520integrates%2520spectral%2520and%2520spatial%2520information%2520by%250Aenhancing%2520spectral%2520tokens%2520and%2520using%2520multi-head%2520attention%2520to%2520capture%2520complex%250Arelationships%2520between%2520spectral%2520bands%2520and%2520spatial%2520locations.%2520It%2520also%2520manages%250Along-range%2520dependencies%2520and%2520the%2520sequential%2520nature%2520of%2520HSI%2520data%252C%2520preserving%250Acontextual%2520information%2520across%2520spectral%2520bands.%2520MHSSMamba%2520achieved%2520remarkable%250Aclassification%2520accuracies%2520of%252097.62%255C%2525%2520on%2520Pavia%2520University%252C%252096.92%255C%2525%2520on%2520the%250AUniversity%2520of%2520Houston%252C%252096.85%255C%2525%2520on%2520Salinas%252C%2520and%252099.49%255C%2525%2520on%2520Wuhan-longKou%250Adatasets.%2520The%2520source%2520code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/MHassaanButt/MHA%255C_SS%255C_Mamba%257D%257BGitHub%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01224v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-head%20Spatial-Spectral%20Mamba%20for%20Hyperspectral%20Image%20Classification&entry.906535625=Muhammad%20Ahmad%20and%20Muhammad%20Hassaan%20Farooq%20Butt%20and%20Muhammad%20Usama%20and%20Hamad%20Ahmed%20Altuwaijri%20and%20Manuel%20Mazzara%20and%20Salvatore%20Distefano&entry.1292438233=%20%20Spatial-Spectral%20Mamba%20%28SSM%29%20improves%20computational%20efficiency%20and%20captures%0Along-range%20dependencies%2C%20addressing%20Transformer%20limitations.%20However%2C%0Atraditional%20Mamba%20models%20overlook%20rich%20spectral%20information%20in%20HSIs%20and%0Astruggle%20with%20high%20dimensionality%20and%20sequential%20data.%20To%20address%20these%20issues%2C%0Awe%20propose%20the%20SSM%20with%20multi-head%20self-attention%20and%20token%20enhancement%0A%28MHSSMamba%29.%20This%20model%20integrates%20spectral%20and%20spatial%20information%20by%0Aenhancing%20spectral%20tokens%20and%20using%20multi-head%20attention%20to%20capture%20complex%0Arelationships%20between%20spectral%20bands%20and%20spatial%20locations.%20It%20also%20manages%0Along-range%20dependencies%20and%20the%20sequential%20nature%20of%20HSI%20data%2C%20preserving%0Acontextual%20information%20across%20spectral%20bands.%20MHSSMamba%20achieved%20remarkable%0Aclassification%20accuracies%20of%2097.62%5C%25%20on%20Pavia%20University%2C%2096.92%5C%25%20on%20the%0AUniversity%20of%20Houston%2C%2096.85%5C%25%20on%20Salinas%2C%20and%2099.49%5C%25%20on%20Wuhan-longKou%0Adatasets.%20The%20source%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/MHassaanButt/MHA%5C_SS%5C_Mamba%7D%7BGitHub%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01224v3&entry.124074799=Read"},
{"title": "May the Forgetting Be with You: Alternate Replay for Learning with Noisy\n  Labels", "author": "Monica Millunzi and Lorenzo Bonicelli and Angelo Porrello and Jacopo Credi and Petter N. Kolm and Simone Calderara", "abstract": "  Forgetting presents a significant challenge during incremental training,\nmaking it particularly demanding for contemporary AI systems to assimilate new\nknowledge in streaming data environments. To address this issue, most\napproaches in Continual Learning (CL) rely on the replay of a restricted buffer\nof past data. However, the presence of noise in real-world scenarios, where\nhuman annotation is constrained by time limitations or where data is\nautomatically gathered from the web, frequently renders these strategies\nvulnerable. In this study, we address the problem of CL under Noisy Labels\n(CLN) by introducing Alternate Experience Replay (AER), which takes advantage\nof forgetting to maintain a clear distinction between clean, complex, and noisy\nsamples in the memory buffer. The idea is that complex or mislabeled examples,\nwhich hardly fit the previously learned data distribution, are most likely to\nbe forgotten. To grasp the benefits of such a separation, we equip AER with\nAsymmetric Balanced Sampling (ABS): a new sample selection strategy that\nprioritizes purity on the current task while retaining relevant samples from\nthe past. Through extensive computational comparisons, we demonstrate the\neffectiveness of our approach in terms of both accuracy and purity of the\nobtained buffer, resulting in a remarkable average gain of 4.71% points in\naccuracy with respect to existing loss-based purification strategies. Code is\navailable at https://github.com/aimagelab/mammoth.\n", "link": "http://arxiv.org/abs/2408.14284v1", "date": "2024-08-26", "relevancy": 2.0376, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5331}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4974}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20May%20the%20Forgetting%20Be%20with%20You%3A%20Alternate%20Replay%20for%20Learning%20with%20Noisy%0A%20%20Labels&body=Title%3A%20May%20the%20Forgetting%20Be%20with%20You%3A%20Alternate%20Replay%20for%20Learning%20with%20Noisy%0A%20%20Labels%0AAuthor%3A%20Monica%20Millunzi%20and%20Lorenzo%20Bonicelli%20and%20Angelo%20Porrello%20and%20Jacopo%20Credi%20and%20Petter%20N.%20Kolm%20and%20Simone%20Calderara%0AAbstract%3A%20%20%20Forgetting%20presents%20a%20significant%20challenge%20during%20incremental%20training%2C%0Amaking%20it%20particularly%20demanding%20for%20contemporary%20AI%20systems%20to%20assimilate%20new%0Aknowledge%20in%20streaming%20data%20environments.%20To%20address%20this%20issue%2C%20most%0Aapproaches%20in%20Continual%20Learning%20%28CL%29%20rely%20on%20the%20replay%20of%20a%20restricted%20buffer%0Aof%20past%20data.%20However%2C%20the%20presence%20of%20noise%20in%20real-world%20scenarios%2C%20where%0Ahuman%20annotation%20is%20constrained%20by%20time%20limitations%20or%20where%20data%20is%0Aautomatically%20gathered%20from%20the%20web%2C%20frequently%20renders%20these%20strategies%0Avulnerable.%20In%20this%20study%2C%20we%20address%20the%20problem%20of%20CL%20under%20Noisy%20Labels%0A%28CLN%29%20by%20introducing%20Alternate%20Experience%20Replay%20%28AER%29%2C%20which%20takes%20advantage%0Aof%20forgetting%20to%20maintain%20a%20clear%20distinction%20between%20clean%2C%20complex%2C%20and%20noisy%0Asamples%20in%20the%20memory%20buffer.%20The%20idea%20is%20that%20complex%20or%20mislabeled%20examples%2C%0Awhich%20hardly%20fit%20the%20previously%20learned%20data%20distribution%2C%20are%20most%20likely%20to%0Abe%20forgotten.%20To%20grasp%20the%20benefits%20of%20such%20a%20separation%2C%20we%20equip%20AER%20with%0AAsymmetric%20Balanced%20Sampling%20%28ABS%29%3A%20a%20new%20sample%20selection%20strategy%20that%0Aprioritizes%20purity%20on%20the%20current%20task%20while%20retaining%20relevant%20samples%20from%0Athe%20past.%20Through%20extensive%20computational%20comparisons%2C%20we%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20in%20terms%20of%20both%20accuracy%20and%20purity%20of%20the%0Aobtained%20buffer%2C%20resulting%20in%20a%20remarkable%20average%20gain%20of%204.71%25%20points%20in%0Aaccuracy%20with%20respect%20to%20existing%20loss-based%20purification%20strategies.%20Code%20is%0Aavailable%20at%20https%3A//github.com/aimagelab/mammoth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14284v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMay%2520the%2520Forgetting%2520Be%2520with%2520You%253A%2520Alternate%2520Replay%2520for%2520Learning%2520with%2520Noisy%250A%2520%2520Labels%26entry.906535625%3DMonica%2520Millunzi%2520and%2520Lorenzo%2520Bonicelli%2520and%2520Angelo%2520Porrello%2520and%2520Jacopo%2520Credi%2520and%2520Petter%2520N.%2520Kolm%2520and%2520Simone%2520Calderara%26entry.1292438233%3D%2520%2520Forgetting%2520presents%2520a%2520significant%2520challenge%2520during%2520incremental%2520training%252C%250Amaking%2520it%2520particularly%2520demanding%2520for%2520contemporary%2520AI%2520systems%2520to%2520assimilate%2520new%250Aknowledge%2520in%2520streaming%2520data%2520environments.%2520To%2520address%2520this%2520issue%252C%2520most%250Aapproaches%2520in%2520Continual%2520Learning%2520%2528CL%2529%2520rely%2520on%2520the%2520replay%2520of%2520a%2520restricted%2520buffer%250Aof%2520past%2520data.%2520However%252C%2520the%2520presence%2520of%2520noise%2520in%2520real-world%2520scenarios%252C%2520where%250Ahuman%2520annotation%2520is%2520constrained%2520by%2520time%2520limitations%2520or%2520where%2520data%2520is%250Aautomatically%2520gathered%2520from%2520the%2520web%252C%2520frequently%2520renders%2520these%2520strategies%250Avulnerable.%2520In%2520this%2520study%252C%2520we%2520address%2520the%2520problem%2520of%2520CL%2520under%2520Noisy%2520Labels%250A%2528CLN%2529%2520by%2520introducing%2520Alternate%2520Experience%2520Replay%2520%2528AER%2529%252C%2520which%2520takes%2520advantage%250Aof%2520forgetting%2520to%2520maintain%2520a%2520clear%2520distinction%2520between%2520clean%252C%2520complex%252C%2520and%2520noisy%250Asamples%2520in%2520the%2520memory%2520buffer.%2520The%2520idea%2520is%2520that%2520complex%2520or%2520mislabeled%2520examples%252C%250Awhich%2520hardly%2520fit%2520the%2520previously%2520learned%2520data%2520distribution%252C%2520are%2520most%2520likely%2520to%250Abe%2520forgotten.%2520To%2520grasp%2520the%2520benefits%2520of%2520such%2520a%2520separation%252C%2520we%2520equip%2520AER%2520with%250AAsymmetric%2520Balanced%2520Sampling%2520%2528ABS%2529%253A%2520a%2520new%2520sample%2520selection%2520strategy%2520that%250Aprioritizes%2520purity%2520on%2520the%2520current%2520task%2520while%2520retaining%2520relevant%2520samples%2520from%250Athe%2520past.%2520Through%2520extensive%2520computational%2520comparisons%252C%2520we%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520approach%2520in%2520terms%2520of%2520both%2520accuracy%2520and%2520purity%2520of%2520the%250Aobtained%2520buffer%252C%2520resulting%2520in%2520a%2520remarkable%2520average%2520gain%2520of%25204.71%2525%2520points%2520in%250Aaccuracy%2520with%2520respect%2520to%2520existing%2520loss-based%2520purification%2520strategies.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/aimagelab/mammoth.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14284v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=May%20the%20Forgetting%20Be%20with%20You%3A%20Alternate%20Replay%20for%20Learning%20with%20Noisy%0A%20%20Labels&entry.906535625=Monica%20Millunzi%20and%20Lorenzo%20Bonicelli%20and%20Angelo%20Porrello%20and%20Jacopo%20Credi%20and%20Petter%20N.%20Kolm%20and%20Simone%20Calderara&entry.1292438233=%20%20Forgetting%20presents%20a%20significant%20challenge%20during%20incremental%20training%2C%0Amaking%20it%20particularly%20demanding%20for%20contemporary%20AI%20systems%20to%20assimilate%20new%0Aknowledge%20in%20streaming%20data%20environments.%20To%20address%20this%20issue%2C%20most%0Aapproaches%20in%20Continual%20Learning%20%28CL%29%20rely%20on%20the%20replay%20of%20a%20restricted%20buffer%0Aof%20past%20data.%20However%2C%20the%20presence%20of%20noise%20in%20real-world%20scenarios%2C%20where%0Ahuman%20annotation%20is%20constrained%20by%20time%20limitations%20or%20where%20data%20is%0Aautomatically%20gathered%20from%20the%20web%2C%20frequently%20renders%20these%20strategies%0Avulnerable.%20In%20this%20study%2C%20we%20address%20the%20problem%20of%20CL%20under%20Noisy%20Labels%0A%28CLN%29%20by%20introducing%20Alternate%20Experience%20Replay%20%28AER%29%2C%20which%20takes%20advantage%0Aof%20forgetting%20to%20maintain%20a%20clear%20distinction%20between%20clean%2C%20complex%2C%20and%20noisy%0Asamples%20in%20the%20memory%20buffer.%20The%20idea%20is%20that%20complex%20or%20mislabeled%20examples%2C%0Awhich%20hardly%20fit%20the%20previously%20learned%20data%20distribution%2C%20are%20most%20likely%20to%0Abe%20forgotten.%20To%20grasp%20the%20benefits%20of%20such%20a%20separation%2C%20we%20equip%20AER%20with%0AAsymmetric%20Balanced%20Sampling%20%28ABS%29%3A%20a%20new%20sample%20selection%20strategy%20that%0Aprioritizes%20purity%20on%20the%20current%20task%20while%20retaining%20relevant%20samples%20from%0Athe%20past.%20Through%20extensive%20computational%20comparisons%2C%20we%20demonstrate%20the%0Aeffectiveness%20of%20our%20approach%20in%20terms%20of%20both%20accuracy%20and%20purity%20of%20the%0Aobtained%20buffer%2C%20resulting%20in%20a%20remarkable%20average%20gain%20of%204.71%25%20points%20in%0Aaccuracy%20with%20respect%20to%20existing%20loss-based%20purification%20strategies.%20Code%20is%0Aavailable%20at%20https%3A//github.com/aimagelab/mammoth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14284v1&entry.124074799=Read"},
{"title": "K-Sort Arena: Efficient and Reliable Benchmarking for Generative Models\n  via K-wise Human Preferences", "author": "Zhikai Li and Xuewen Liu and Dongrong Fu and Jianquan Li and Qingyi Gu and Kurt Keutzer and Zhen Dong", "abstract": "  The rapid advancement of visual generative models necessitates efficient and\nreliable evaluation methods. Arena platform, which gathers user votes on model\ncomparisons, can rank models with human preferences. However, traditional Arena\nmethods, while established, require an excessive number of comparisons for\nranking to converge and are vulnerable to preference noise in voting,\nsuggesting the need for better approaches tailored to contemporary evaluation\nchallenges. In this paper, we introduce K-Sort Arena, an efficient and reliable\nplatform based on a key insight: images and videos possess higher perceptual\nintuitiveness than texts, enabling rapid evaluation of multiple samples\nsimultaneously. Consequently, K-Sort Arena employs K-wise comparisons, allowing\nK models to engage in free-for-all competitions, which yield much richer\ninformation than pairwise comparisons. To enhance the robustness of the system,\nwe leverage probabilistic modeling and Bayesian updating techniques. We propose\nan exploration-exploitation-based matchmaking strategy to facilitate more\ninformative comparisons. In our experiments, K-Sort Arena exhibits 16.3x faster\nconvergence compared to the widely used ELO algorithm. To further validate the\nsuperiority and obtain a comprehensive leaderboard, we collect human feedback\nvia crowdsourced evaluations of numerous cutting-edge text-to-image and\ntext-to-video models. Thanks to its high efficiency, K-Sort Arena can\ncontinuously incorporate emerging models and update the leaderboard with\nminimal votes. Our project has undergone several months of internal testing and\nis now available at https://huggingface.co/spaces/ksort/K-Sort-Arena\n", "link": "http://arxiv.org/abs/2408.14468v1", "date": "2024-08-26", "relevancy": 2.0295, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5106}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5074}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4993}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20K-Sort%20Arena%3A%20Efficient%20and%20Reliable%20Benchmarking%20for%20Generative%20Models%0A%20%20via%20K-wise%20Human%20Preferences&body=Title%3A%20K-Sort%20Arena%3A%20Efficient%20and%20Reliable%20Benchmarking%20for%20Generative%20Models%0A%20%20via%20K-wise%20Human%20Preferences%0AAuthor%3A%20Zhikai%20Li%20and%20Xuewen%20Liu%20and%20Dongrong%20Fu%20and%20Jianquan%20Li%20and%20Qingyi%20Gu%20and%20Kurt%20Keutzer%20and%20Zhen%20Dong%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20visual%20generative%20models%20necessitates%20efficient%20and%0Areliable%20evaluation%20methods.%20Arena%20platform%2C%20which%20gathers%20user%20votes%20on%20model%0Acomparisons%2C%20can%20rank%20models%20with%20human%20preferences.%20However%2C%20traditional%20Arena%0Amethods%2C%20while%20established%2C%20require%20an%20excessive%20number%20of%20comparisons%20for%0Aranking%20to%20converge%20and%20are%20vulnerable%20to%20preference%20noise%20in%20voting%2C%0Asuggesting%20the%20need%20for%20better%20approaches%20tailored%20to%20contemporary%20evaluation%0Achallenges.%20In%20this%20paper%2C%20we%20introduce%20K-Sort%20Arena%2C%20an%20efficient%20and%20reliable%0Aplatform%20based%20on%20a%20key%20insight%3A%20images%20and%20videos%20possess%20higher%20perceptual%0Aintuitiveness%20than%20texts%2C%20enabling%20rapid%20evaluation%20of%20multiple%20samples%0Asimultaneously.%20Consequently%2C%20K-Sort%20Arena%20employs%20K-wise%20comparisons%2C%20allowing%0AK%20models%20to%20engage%20in%20free-for-all%20competitions%2C%20which%20yield%20much%20richer%0Ainformation%20than%20pairwise%20comparisons.%20To%20enhance%20the%20robustness%20of%20the%20system%2C%0Awe%20leverage%20probabilistic%20modeling%20and%20Bayesian%20updating%20techniques.%20We%20propose%0Aan%20exploration-exploitation-based%20matchmaking%20strategy%20to%20facilitate%20more%0Ainformative%20comparisons.%20In%20our%20experiments%2C%20K-Sort%20Arena%20exhibits%2016.3x%20faster%0Aconvergence%20compared%20to%20the%20widely%20used%20ELO%20algorithm.%20To%20further%20validate%20the%0Asuperiority%20and%20obtain%20a%20comprehensive%20leaderboard%2C%20we%20collect%20human%20feedback%0Avia%20crowdsourced%20evaluations%20of%20numerous%20cutting-edge%20text-to-image%20and%0Atext-to-video%20models.%20Thanks%20to%20its%20high%20efficiency%2C%20K-Sort%20Arena%20can%0Acontinuously%20incorporate%20emerging%20models%20and%20update%20the%20leaderboard%20with%0Aminimal%20votes.%20Our%20project%20has%20undergone%20several%20months%20of%20internal%20testing%20and%0Ais%20now%20available%20at%20https%3A//huggingface.co/spaces/ksort/K-Sort-Arena%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14468v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DK-Sort%2520Arena%253A%2520Efficient%2520and%2520Reliable%2520Benchmarking%2520for%2520Generative%2520Models%250A%2520%2520via%2520K-wise%2520Human%2520Preferences%26entry.906535625%3DZhikai%2520Li%2520and%2520Xuewen%2520Liu%2520and%2520Dongrong%2520Fu%2520and%2520Jianquan%2520Li%2520and%2520Qingyi%2520Gu%2520and%2520Kurt%2520Keutzer%2520and%2520Zhen%2520Dong%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520visual%2520generative%2520models%2520necessitates%2520efficient%2520and%250Areliable%2520evaluation%2520methods.%2520Arena%2520platform%252C%2520which%2520gathers%2520user%2520votes%2520on%2520model%250Acomparisons%252C%2520can%2520rank%2520models%2520with%2520human%2520preferences.%2520However%252C%2520traditional%2520Arena%250Amethods%252C%2520while%2520established%252C%2520require%2520an%2520excessive%2520number%2520of%2520comparisons%2520for%250Aranking%2520to%2520converge%2520and%2520are%2520vulnerable%2520to%2520preference%2520noise%2520in%2520voting%252C%250Asuggesting%2520the%2520need%2520for%2520better%2520approaches%2520tailored%2520to%2520contemporary%2520evaluation%250Achallenges.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520K-Sort%2520Arena%252C%2520an%2520efficient%2520and%2520reliable%250Aplatform%2520based%2520on%2520a%2520key%2520insight%253A%2520images%2520and%2520videos%2520possess%2520higher%2520perceptual%250Aintuitiveness%2520than%2520texts%252C%2520enabling%2520rapid%2520evaluation%2520of%2520multiple%2520samples%250Asimultaneously.%2520Consequently%252C%2520K-Sort%2520Arena%2520employs%2520K-wise%2520comparisons%252C%2520allowing%250AK%2520models%2520to%2520engage%2520in%2520free-for-all%2520competitions%252C%2520which%2520yield%2520much%2520richer%250Ainformation%2520than%2520pairwise%2520comparisons.%2520To%2520enhance%2520the%2520robustness%2520of%2520the%2520system%252C%250Awe%2520leverage%2520probabilistic%2520modeling%2520and%2520Bayesian%2520updating%2520techniques.%2520We%2520propose%250Aan%2520exploration-exploitation-based%2520matchmaking%2520strategy%2520to%2520facilitate%2520more%250Ainformative%2520comparisons.%2520In%2520our%2520experiments%252C%2520K-Sort%2520Arena%2520exhibits%252016.3x%2520faster%250Aconvergence%2520compared%2520to%2520the%2520widely%2520used%2520ELO%2520algorithm.%2520To%2520further%2520validate%2520the%250Asuperiority%2520and%2520obtain%2520a%2520comprehensive%2520leaderboard%252C%2520we%2520collect%2520human%2520feedback%250Avia%2520crowdsourced%2520evaluations%2520of%2520numerous%2520cutting-edge%2520text-to-image%2520and%250Atext-to-video%2520models.%2520Thanks%2520to%2520its%2520high%2520efficiency%252C%2520K-Sort%2520Arena%2520can%250Acontinuously%2520incorporate%2520emerging%2520models%2520and%2520update%2520the%2520leaderboard%2520with%250Aminimal%2520votes.%2520Our%2520project%2520has%2520undergone%2520several%2520months%2520of%2520internal%2520testing%2520and%250Ais%2520now%2520available%2520at%2520https%253A//huggingface.co/spaces/ksort/K-Sort-Arena%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14468v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=K-Sort%20Arena%3A%20Efficient%20and%20Reliable%20Benchmarking%20for%20Generative%20Models%0A%20%20via%20K-wise%20Human%20Preferences&entry.906535625=Zhikai%20Li%20and%20Xuewen%20Liu%20and%20Dongrong%20Fu%20and%20Jianquan%20Li%20and%20Qingyi%20Gu%20and%20Kurt%20Keutzer%20and%20Zhen%20Dong&entry.1292438233=%20%20The%20rapid%20advancement%20of%20visual%20generative%20models%20necessitates%20efficient%20and%0Areliable%20evaluation%20methods.%20Arena%20platform%2C%20which%20gathers%20user%20votes%20on%20model%0Acomparisons%2C%20can%20rank%20models%20with%20human%20preferences.%20However%2C%20traditional%20Arena%0Amethods%2C%20while%20established%2C%20require%20an%20excessive%20number%20of%20comparisons%20for%0Aranking%20to%20converge%20and%20are%20vulnerable%20to%20preference%20noise%20in%20voting%2C%0Asuggesting%20the%20need%20for%20better%20approaches%20tailored%20to%20contemporary%20evaluation%0Achallenges.%20In%20this%20paper%2C%20we%20introduce%20K-Sort%20Arena%2C%20an%20efficient%20and%20reliable%0Aplatform%20based%20on%20a%20key%20insight%3A%20images%20and%20videos%20possess%20higher%20perceptual%0Aintuitiveness%20than%20texts%2C%20enabling%20rapid%20evaluation%20of%20multiple%20samples%0Asimultaneously.%20Consequently%2C%20K-Sort%20Arena%20employs%20K-wise%20comparisons%2C%20allowing%0AK%20models%20to%20engage%20in%20free-for-all%20competitions%2C%20which%20yield%20much%20richer%0Ainformation%20than%20pairwise%20comparisons.%20To%20enhance%20the%20robustness%20of%20the%20system%2C%0Awe%20leverage%20probabilistic%20modeling%20and%20Bayesian%20updating%20techniques.%20We%20propose%0Aan%20exploration-exploitation-based%20matchmaking%20strategy%20to%20facilitate%20more%0Ainformative%20comparisons.%20In%20our%20experiments%2C%20K-Sort%20Arena%20exhibits%2016.3x%20faster%0Aconvergence%20compared%20to%20the%20widely%20used%20ELO%20algorithm.%20To%20further%20validate%20the%0Asuperiority%20and%20obtain%20a%20comprehensive%20leaderboard%2C%20we%20collect%20human%20feedback%0Avia%20crowdsourced%20evaluations%20of%20numerous%20cutting-edge%20text-to-image%20and%0Atext-to-video%20models.%20Thanks%20to%20its%20high%20efficiency%2C%20K-Sort%20Arena%20can%0Acontinuously%20incorporate%20emerging%20models%20and%20update%20the%20leaderboard%20with%0Aminimal%20votes.%20Our%20project%20has%20undergone%20several%20months%20of%20internal%20testing%20and%0Ais%20now%20available%20at%20https%3A//huggingface.co/spaces/ksort/K-Sort-Arena%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14468v1&entry.124074799=Read"},
{"title": "Ensemble Predicate Decoding for Unbiased Scene Graph Generation", "author": "Jiasong Feng and Lichun Wang and Hongbo Xu and Kai Xu and Baocai Yin", "abstract": "  Scene Graph Generation (SGG) aims to generate a comprehensive graphical\nrepresentation that accurately captures the semantic information of a given\nscenario. However, the SGG model's performance in predicting more fine-grained\npredicates is hindered by a significant predicate bias. According to existing\nworks, the long-tail distribution of predicates in training data results in the\nbiased scene graph. However, the semantic overlap between predicate categories\nmakes predicate prediction difficult, and there is a significant difference in\nthe sample size of semantically similar predicates, making the predicate\nprediction more difficult. Therefore, higher requirements are placed on the\ndiscriminative ability of the model. In order to address this problem, this\npaper proposes Ensemble Predicate Decoding (EPD), which employs multiple\ndecoders to attain unbiased scene graph generation. Two auxiliary decoders\ntrained on lower-frequency predicates are used to improve the discriminative\nability of the model. Extensive experiments are conducted on the VG, and the\nexperiment results show that EPD enhances the model's representation capability\nfor predicates. In addition, we find that our approach ensures a relatively\nsuperior predictive capability for more frequent predicates compared to\nprevious unbiased SGG methods.\n", "link": "http://arxiv.org/abs/2408.14187v1", "date": "2024-08-26", "relevancy": 2.0243, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5224}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4968}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4886}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ensemble%20Predicate%20Decoding%20for%20Unbiased%20Scene%20Graph%20Generation&body=Title%3A%20Ensemble%20Predicate%20Decoding%20for%20Unbiased%20Scene%20Graph%20Generation%0AAuthor%3A%20Jiasong%20Feng%20and%20Lichun%20Wang%20and%20Hongbo%20Xu%20and%20Kai%20Xu%20and%20Baocai%20Yin%0AAbstract%3A%20%20%20Scene%20Graph%20Generation%20%28SGG%29%20aims%20to%20generate%20a%20comprehensive%20graphical%0Arepresentation%20that%20accurately%20captures%20the%20semantic%20information%20of%20a%20given%0Ascenario.%20However%2C%20the%20SGG%20model%27s%20performance%20in%20predicting%20more%20fine-grained%0Apredicates%20is%20hindered%20by%20a%20significant%20predicate%20bias.%20According%20to%20existing%0Aworks%2C%20the%20long-tail%20distribution%20of%20predicates%20in%20training%20data%20results%20in%20the%0Abiased%20scene%20graph.%20However%2C%20the%20semantic%20overlap%20between%20predicate%20categories%0Amakes%20predicate%20prediction%20difficult%2C%20and%20there%20is%20a%20significant%20difference%20in%0Athe%20sample%20size%20of%20semantically%20similar%20predicates%2C%20making%20the%20predicate%0Aprediction%20more%20difficult.%20Therefore%2C%20higher%20requirements%20are%20placed%20on%20the%0Adiscriminative%20ability%20of%20the%20model.%20In%20order%20to%20address%20this%20problem%2C%20this%0Apaper%20proposes%20Ensemble%20Predicate%20Decoding%20%28EPD%29%2C%20which%20employs%20multiple%0Adecoders%20to%20attain%20unbiased%20scene%20graph%20generation.%20Two%20auxiliary%20decoders%0Atrained%20on%20lower-frequency%20predicates%20are%20used%20to%20improve%20the%20discriminative%0Aability%20of%20the%20model.%20Extensive%20experiments%20are%20conducted%20on%20the%20VG%2C%20and%20the%0Aexperiment%20results%20show%20that%20EPD%20enhances%20the%20model%27s%20representation%20capability%0Afor%20predicates.%20In%20addition%2C%20we%20find%20that%20our%20approach%20ensures%20a%20relatively%0Asuperior%20predictive%20capability%20for%20more%20frequent%20predicates%20compared%20to%0Aprevious%20unbiased%20SGG%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14187v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnsemble%2520Predicate%2520Decoding%2520for%2520Unbiased%2520Scene%2520Graph%2520Generation%26entry.906535625%3DJiasong%2520Feng%2520and%2520Lichun%2520Wang%2520and%2520Hongbo%2520Xu%2520and%2520Kai%2520Xu%2520and%2520Baocai%2520Yin%26entry.1292438233%3D%2520%2520Scene%2520Graph%2520Generation%2520%2528SGG%2529%2520aims%2520to%2520generate%2520a%2520comprehensive%2520graphical%250Arepresentation%2520that%2520accurately%2520captures%2520the%2520semantic%2520information%2520of%2520a%2520given%250Ascenario.%2520However%252C%2520the%2520SGG%2520model%2527s%2520performance%2520in%2520predicting%2520more%2520fine-grained%250Apredicates%2520is%2520hindered%2520by%2520a%2520significant%2520predicate%2520bias.%2520According%2520to%2520existing%250Aworks%252C%2520the%2520long-tail%2520distribution%2520of%2520predicates%2520in%2520training%2520data%2520results%2520in%2520the%250Abiased%2520scene%2520graph.%2520However%252C%2520the%2520semantic%2520overlap%2520between%2520predicate%2520categories%250Amakes%2520predicate%2520prediction%2520difficult%252C%2520and%2520there%2520is%2520a%2520significant%2520difference%2520in%250Athe%2520sample%2520size%2520of%2520semantically%2520similar%2520predicates%252C%2520making%2520the%2520predicate%250Aprediction%2520more%2520difficult.%2520Therefore%252C%2520higher%2520requirements%2520are%2520placed%2520on%2520the%250Adiscriminative%2520ability%2520of%2520the%2520model.%2520In%2520order%2520to%2520address%2520this%2520problem%252C%2520this%250Apaper%2520proposes%2520Ensemble%2520Predicate%2520Decoding%2520%2528EPD%2529%252C%2520which%2520employs%2520multiple%250Adecoders%2520to%2520attain%2520unbiased%2520scene%2520graph%2520generation.%2520Two%2520auxiliary%2520decoders%250Atrained%2520on%2520lower-frequency%2520predicates%2520are%2520used%2520to%2520improve%2520the%2520discriminative%250Aability%2520of%2520the%2520model.%2520Extensive%2520experiments%2520are%2520conducted%2520on%2520the%2520VG%252C%2520and%2520the%250Aexperiment%2520results%2520show%2520that%2520EPD%2520enhances%2520the%2520model%2527s%2520representation%2520capability%250Afor%2520predicates.%2520In%2520addition%252C%2520we%2520find%2520that%2520our%2520approach%2520ensures%2520a%2520relatively%250Asuperior%2520predictive%2520capability%2520for%2520more%2520frequent%2520predicates%2520compared%2520to%250Aprevious%2520unbiased%2520SGG%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14187v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ensemble%20Predicate%20Decoding%20for%20Unbiased%20Scene%20Graph%20Generation&entry.906535625=Jiasong%20Feng%20and%20Lichun%20Wang%20and%20Hongbo%20Xu%20and%20Kai%20Xu%20and%20Baocai%20Yin&entry.1292438233=%20%20Scene%20Graph%20Generation%20%28SGG%29%20aims%20to%20generate%20a%20comprehensive%20graphical%0Arepresentation%20that%20accurately%20captures%20the%20semantic%20information%20of%20a%20given%0Ascenario.%20However%2C%20the%20SGG%20model%27s%20performance%20in%20predicting%20more%20fine-grained%0Apredicates%20is%20hindered%20by%20a%20significant%20predicate%20bias.%20According%20to%20existing%0Aworks%2C%20the%20long-tail%20distribution%20of%20predicates%20in%20training%20data%20results%20in%20the%0Abiased%20scene%20graph.%20However%2C%20the%20semantic%20overlap%20between%20predicate%20categories%0Amakes%20predicate%20prediction%20difficult%2C%20and%20there%20is%20a%20significant%20difference%20in%0Athe%20sample%20size%20of%20semantically%20similar%20predicates%2C%20making%20the%20predicate%0Aprediction%20more%20difficult.%20Therefore%2C%20higher%20requirements%20are%20placed%20on%20the%0Adiscriminative%20ability%20of%20the%20model.%20In%20order%20to%20address%20this%20problem%2C%20this%0Apaper%20proposes%20Ensemble%20Predicate%20Decoding%20%28EPD%29%2C%20which%20employs%20multiple%0Adecoders%20to%20attain%20unbiased%20scene%20graph%20generation.%20Two%20auxiliary%20decoders%0Atrained%20on%20lower-frequency%20predicates%20are%20used%20to%20improve%20the%20discriminative%0Aability%20of%20the%20model.%20Extensive%20experiments%20are%20conducted%20on%20the%20VG%2C%20and%20the%0Aexperiment%20results%20show%20that%20EPD%20enhances%20the%20model%27s%20representation%20capability%0Afor%20predicates.%20In%20addition%2C%20we%20find%20that%20our%20approach%20ensures%20a%20relatively%0Asuperior%20predictive%20capability%20for%20more%20frequent%20predicates%20compared%20to%0Aprevious%20unbiased%20SGG%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14187v1&entry.124074799=Read"},
{"title": "An Embedding is Worth a Thousand Noisy Labels", "author": "Francesco Di Salvo and Sebastian Doerrich and Ines Rieger and Christian Ledig", "abstract": "  The performance of deep neural networks scales with dataset size and label\nquality, rendering the efficient mitigation of low-quality data annotations\ncrucial for building robust and cost-effective systems. Existing strategies to\naddress label noise exhibit severe limitations due to computational complexity\nand application dependency. In this work, we propose WANN, a Weighted Adaptive\nNearest Neighbor approach that builds on self-supervised feature\nrepresentations obtained from foundation models. To guide the weighted voting\nscheme, we introduce a reliability score, which measures the likelihood of a\ndata label being correct. WANN outperforms reference methods, including a\nlinear layer trained with robust loss functions, on diverse datasets of varying\nsize and under various noise types and severities. WANN also exhibits superior\ngeneralization on imbalanced data compared to both Adaptive-NNs (ANN) and fixed\nk-NNs. Furthermore, the proposed weighting scheme enhances supervised\ndimensionality reduction under noisy labels. This yields a significant boost in\nclassification performance with 10x and 100x smaller image embeddings,\nminimizing latency and storage requirements. Our approach, emphasizing\nefficiency and explainability, emerges as a simple, robust solution to overcome\nthe inherent limitations of deep neural network training. The code is available\nat https://github.com/francescodisalvo05/wann-noisy-labels .\n", "link": "http://arxiv.org/abs/2408.14358v1", "date": "2024-08-26", "relevancy": 2.0233, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5169}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.515}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4911}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Embedding%20is%20Worth%20a%20Thousand%20Noisy%20Labels&body=Title%3A%20An%20Embedding%20is%20Worth%20a%20Thousand%20Noisy%20Labels%0AAuthor%3A%20Francesco%20Di%20Salvo%20and%20Sebastian%20Doerrich%20and%20Ines%20Rieger%20and%20Christian%20Ledig%0AAbstract%3A%20%20%20The%20performance%20of%20deep%20neural%20networks%20scales%20with%20dataset%20size%20and%20label%0Aquality%2C%20rendering%20the%20efficient%20mitigation%20of%20low-quality%20data%20annotations%0Acrucial%20for%20building%20robust%20and%20cost-effective%20systems.%20Existing%20strategies%20to%0Aaddress%20label%20noise%20exhibit%20severe%20limitations%20due%20to%20computational%20complexity%0Aand%20application%20dependency.%20In%20this%20work%2C%20we%20propose%20WANN%2C%20a%20Weighted%20Adaptive%0ANearest%20Neighbor%20approach%20that%20builds%20on%20self-supervised%20feature%0Arepresentations%20obtained%20from%20foundation%20models.%20To%20guide%20the%20weighted%20voting%0Ascheme%2C%20we%20introduce%20a%20reliability%20score%2C%20which%20measures%20the%20likelihood%20of%20a%0Adata%20label%20being%20correct.%20WANN%20outperforms%20reference%20methods%2C%20including%20a%0Alinear%20layer%20trained%20with%20robust%20loss%20functions%2C%20on%20diverse%20datasets%20of%20varying%0Asize%20and%20under%20various%20noise%20types%20and%20severities.%20WANN%20also%20exhibits%20superior%0Ageneralization%20on%20imbalanced%20data%20compared%20to%20both%20Adaptive-NNs%20%28ANN%29%20and%20fixed%0Ak-NNs.%20Furthermore%2C%20the%20proposed%20weighting%20scheme%20enhances%20supervised%0Adimensionality%20reduction%20under%20noisy%20labels.%20This%20yields%20a%20significant%20boost%20in%0Aclassification%20performance%20with%2010x%20and%20100x%20smaller%20image%20embeddings%2C%0Aminimizing%20latency%20and%20storage%20requirements.%20Our%20approach%2C%20emphasizing%0Aefficiency%20and%20explainability%2C%20emerges%20as%20a%20simple%2C%20robust%20solution%20to%20overcome%0Athe%20inherent%20limitations%20of%20deep%20neural%20network%20training.%20The%20code%20is%20available%0Aat%20https%3A//github.com/francescodisalvo05/wann-noisy-labels%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14358v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Embedding%2520is%2520Worth%2520a%2520Thousand%2520Noisy%2520Labels%26entry.906535625%3DFrancesco%2520Di%2520Salvo%2520and%2520Sebastian%2520Doerrich%2520and%2520Ines%2520Rieger%2520and%2520Christian%2520Ledig%26entry.1292438233%3D%2520%2520The%2520performance%2520of%2520deep%2520neural%2520networks%2520scales%2520with%2520dataset%2520size%2520and%2520label%250Aquality%252C%2520rendering%2520the%2520efficient%2520mitigation%2520of%2520low-quality%2520data%2520annotations%250Acrucial%2520for%2520building%2520robust%2520and%2520cost-effective%2520systems.%2520Existing%2520strategies%2520to%250Aaddress%2520label%2520noise%2520exhibit%2520severe%2520limitations%2520due%2520to%2520computational%2520complexity%250Aand%2520application%2520dependency.%2520In%2520this%2520work%252C%2520we%2520propose%2520WANN%252C%2520a%2520Weighted%2520Adaptive%250ANearest%2520Neighbor%2520approach%2520that%2520builds%2520on%2520self-supervised%2520feature%250Arepresentations%2520obtained%2520from%2520foundation%2520models.%2520To%2520guide%2520the%2520weighted%2520voting%250Ascheme%252C%2520we%2520introduce%2520a%2520reliability%2520score%252C%2520which%2520measures%2520the%2520likelihood%2520of%2520a%250Adata%2520label%2520being%2520correct.%2520WANN%2520outperforms%2520reference%2520methods%252C%2520including%2520a%250Alinear%2520layer%2520trained%2520with%2520robust%2520loss%2520functions%252C%2520on%2520diverse%2520datasets%2520of%2520varying%250Asize%2520and%2520under%2520various%2520noise%2520types%2520and%2520severities.%2520WANN%2520also%2520exhibits%2520superior%250Ageneralization%2520on%2520imbalanced%2520data%2520compared%2520to%2520both%2520Adaptive-NNs%2520%2528ANN%2529%2520and%2520fixed%250Ak-NNs.%2520Furthermore%252C%2520the%2520proposed%2520weighting%2520scheme%2520enhances%2520supervised%250Adimensionality%2520reduction%2520under%2520noisy%2520labels.%2520This%2520yields%2520a%2520significant%2520boost%2520in%250Aclassification%2520performance%2520with%252010x%2520and%2520100x%2520smaller%2520image%2520embeddings%252C%250Aminimizing%2520latency%2520and%2520storage%2520requirements.%2520Our%2520approach%252C%2520emphasizing%250Aefficiency%2520and%2520explainability%252C%2520emerges%2520as%2520a%2520simple%252C%2520robust%2520solution%2520to%2520overcome%250Athe%2520inherent%2520limitations%2520of%2520deep%2520neural%2520network%2520training.%2520The%2520code%2520is%2520available%250Aat%2520https%253A//github.com/francescodisalvo05/wann-noisy-labels%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14358v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Embedding%20is%20Worth%20a%20Thousand%20Noisy%20Labels&entry.906535625=Francesco%20Di%20Salvo%20and%20Sebastian%20Doerrich%20and%20Ines%20Rieger%20and%20Christian%20Ledig&entry.1292438233=%20%20The%20performance%20of%20deep%20neural%20networks%20scales%20with%20dataset%20size%20and%20label%0Aquality%2C%20rendering%20the%20efficient%20mitigation%20of%20low-quality%20data%20annotations%0Acrucial%20for%20building%20robust%20and%20cost-effective%20systems.%20Existing%20strategies%20to%0Aaddress%20label%20noise%20exhibit%20severe%20limitations%20due%20to%20computational%20complexity%0Aand%20application%20dependency.%20In%20this%20work%2C%20we%20propose%20WANN%2C%20a%20Weighted%20Adaptive%0ANearest%20Neighbor%20approach%20that%20builds%20on%20self-supervised%20feature%0Arepresentations%20obtained%20from%20foundation%20models.%20To%20guide%20the%20weighted%20voting%0Ascheme%2C%20we%20introduce%20a%20reliability%20score%2C%20which%20measures%20the%20likelihood%20of%20a%0Adata%20label%20being%20correct.%20WANN%20outperforms%20reference%20methods%2C%20including%20a%0Alinear%20layer%20trained%20with%20robust%20loss%20functions%2C%20on%20diverse%20datasets%20of%20varying%0Asize%20and%20under%20various%20noise%20types%20and%20severities.%20WANN%20also%20exhibits%20superior%0Ageneralization%20on%20imbalanced%20data%20compared%20to%20both%20Adaptive-NNs%20%28ANN%29%20and%20fixed%0Ak-NNs.%20Furthermore%2C%20the%20proposed%20weighting%20scheme%20enhances%20supervised%0Adimensionality%20reduction%20under%20noisy%20labels.%20This%20yields%20a%20significant%20boost%20in%0Aclassification%20performance%20with%2010x%20and%20100x%20smaller%20image%20embeddings%2C%0Aminimizing%20latency%20and%20storage%20requirements.%20Our%20approach%2C%20emphasizing%0Aefficiency%20and%20explainability%2C%20emerges%20as%20a%20simple%2C%20robust%20solution%20to%20overcome%0Athe%20inherent%20limitations%20of%20deep%20neural%20network%20training.%20The%20code%20is%20available%0Aat%20https%3A//github.com/francescodisalvo05/wann-noisy-labels%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14358v1&entry.124074799=Read"},
{"title": "LLM-3D Print: Large Language Models To Monitor and Control 3D Printing", "author": "Yayati Jadhav and Peter Pak and Amir Barati Farimani", "abstract": "  Industry 4.0 has revolutionized manufacturing by driving digitalization and\nshifting the paradigm toward additive manufacturing (AM). Fused Deposition\nModeling (FDM), a key AM technology, enables the creation of highly customized,\ncost-effective products with minimal material waste through layer-by-layer\nextrusion, posing a significant challenge to traditional subtractive methods.\nHowever, the susceptibility of material extrusion techniques to errors often\nrequires expert intervention to detect and mitigate defects that can severely\ncompromise product quality. While automated error detection and machine\nlearning models exist, their generalizability across diverse 3D printer setups,\nfirmware, and sensors is limited, and deep learning methods require extensive\nlabeled datasets, hindering scalability and adaptability. To address these\nchallenges, we present a process monitoring and control framework that\nleverages pre-trained Large Language Models (LLMs) alongside 3D printers to\ndetect and address printing defects. The LLM evaluates print quality by\nanalyzing images captured after each layer or print segment, identifying\nfailure modes and querying the printer for relevant parameters. It then\ngenerates and executes a corrective action plan. We validated the effectiveness\nof the proposed framework in identifying defects by comparing it against a\ncontrol group of engineers with diverse AM expertise. Our evaluation\ndemonstrated that LLM-based agents not only accurately identify common 3D\nprinting errors, such as inconsistent extrusion, stringing, warping, and layer\nadhesion, but also effectively determine the parameters causing these failures\nand autonomously correct them without any need for human intervention.\n", "link": "http://arxiv.org/abs/2408.14307v1", "date": "2024-08-26", "relevancy": 2.0187, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5416}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5028}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4918}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-3D%20Print%3A%20Large%20Language%20Models%20To%20Monitor%20and%20Control%203D%20Printing&body=Title%3A%20LLM-3D%20Print%3A%20Large%20Language%20Models%20To%20Monitor%20and%20Control%203D%20Printing%0AAuthor%3A%20Yayati%20Jadhav%20and%20Peter%20Pak%20and%20Amir%20Barati%20Farimani%0AAbstract%3A%20%20%20Industry%204.0%20has%20revolutionized%20manufacturing%20by%20driving%20digitalization%20and%0Ashifting%20the%20paradigm%20toward%20additive%20manufacturing%20%28AM%29.%20Fused%20Deposition%0AModeling%20%28FDM%29%2C%20a%20key%20AM%20technology%2C%20enables%20the%20creation%20of%20highly%20customized%2C%0Acost-effective%20products%20with%20minimal%20material%20waste%20through%20layer-by-layer%0Aextrusion%2C%20posing%20a%20significant%20challenge%20to%20traditional%20subtractive%20methods.%0AHowever%2C%20the%20susceptibility%20of%20material%20extrusion%20techniques%20to%20errors%20often%0Arequires%20expert%20intervention%20to%20detect%20and%20mitigate%20defects%20that%20can%20severely%0Acompromise%20product%20quality.%20While%20automated%20error%20detection%20and%20machine%0Alearning%20models%20exist%2C%20their%20generalizability%20across%20diverse%203D%20printer%20setups%2C%0Afirmware%2C%20and%20sensors%20is%20limited%2C%20and%20deep%20learning%20methods%20require%20extensive%0Alabeled%20datasets%2C%20hindering%20scalability%20and%20adaptability.%20To%20address%20these%0Achallenges%2C%20we%20present%20a%20process%20monitoring%20and%20control%20framework%20that%0Aleverages%20pre-trained%20Large%20Language%20Models%20%28LLMs%29%20alongside%203D%20printers%20to%0Adetect%20and%20address%20printing%20defects.%20The%20LLM%20evaluates%20print%20quality%20by%0Aanalyzing%20images%20captured%20after%20each%20layer%20or%20print%20segment%2C%20identifying%0Afailure%20modes%20and%20querying%20the%20printer%20for%20relevant%20parameters.%20It%20then%0Agenerates%20and%20executes%20a%20corrective%20action%20plan.%20We%20validated%20the%20effectiveness%0Aof%20the%20proposed%20framework%20in%20identifying%20defects%20by%20comparing%20it%20against%20a%0Acontrol%20group%20of%20engineers%20with%20diverse%20AM%20expertise.%20Our%20evaluation%0Ademonstrated%20that%20LLM-based%20agents%20not%20only%20accurately%20identify%20common%203D%0Aprinting%20errors%2C%20such%20as%20inconsistent%20extrusion%2C%20stringing%2C%20warping%2C%20and%20layer%0Aadhesion%2C%20but%20also%20effectively%20determine%20the%20parameters%20causing%20these%20failures%0Aand%20autonomously%20correct%20them%20without%20any%20need%20for%20human%20intervention.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14307v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-3D%2520Print%253A%2520Large%2520Language%2520Models%2520To%2520Monitor%2520and%2520Control%25203D%2520Printing%26entry.906535625%3DYayati%2520Jadhav%2520and%2520Peter%2520Pak%2520and%2520Amir%2520Barati%2520Farimani%26entry.1292438233%3D%2520%2520Industry%25204.0%2520has%2520revolutionized%2520manufacturing%2520by%2520driving%2520digitalization%2520and%250Ashifting%2520the%2520paradigm%2520toward%2520additive%2520manufacturing%2520%2528AM%2529.%2520Fused%2520Deposition%250AModeling%2520%2528FDM%2529%252C%2520a%2520key%2520AM%2520technology%252C%2520enables%2520the%2520creation%2520of%2520highly%2520customized%252C%250Acost-effective%2520products%2520with%2520minimal%2520material%2520waste%2520through%2520layer-by-layer%250Aextrusion%252C%2520posing%2520a%2520significant%2520challenge%2520to%2520traditional%2520subtractive%2520methods.%250AHowever%252C%2520the%2520susceptibility%2520of%2520material%2520extrusion%2520techniques%2520to%2520errors%2520often%250Arequires%2520expert%2520intervention%2520to%2520detect%2520and%2520mitigate%2520defects%2520that%2520can%2520severely%250Acompromise%2520product%2520quality.%2520While%2520automated%2520error%2520detection%2520and%2520machine%250Alearning%2520models%2520exist%252C%2520their%2520generalizability%2520across%2520diverse%25203D%2520printer%2520setups%252C%250Afirmware%252C%2520and%2520sensors%2520is%2520limited%252C%2520and%2520deep%2520learning%2520methods%2520require%2520extensive%250Alabeled%2520datasets%252C%2520hindering%2520scalability%2520and%2520adaptability.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520present%2520a%2520process%2520monitoring%2520and%2520control%2520framework%2520that%250Aleverages%2520pre-trained%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520alongside%25203D%2520printers%2520to%250Adetect%2520and%2520address%2520printing%2520defects.%2520The%2520LLM%2520evaluates%2520print%2520quality%2520by%250Aanalyzing%2520images%2520captured%2520after%2520each%2520layer%2520or%2520print%2520segment%252C%2520identifying%250Afailure%2520modes%2520and%2520querying%2520the%2520printer%2520for%2520relevant%2520parameters.%2520It%2520then%250Agenerates%2520and%2520executes%2520a%2520corrective%2520action%2520plan.%2520We%2520validated%2520the%2520effectiveness%250Aof%2520the%2520proposed%2520framework%2520in%2520identifying%2520defects%2520by%2520comparing%2520it%2520against%2520a%250Acontrol%2520group%2520of%2520engineers%2520with%2520diverse%2520AM%2520expertise.%2520Our%2520evaluation%250Ademonstrated%2520that%2520LLM-based%2520agents%2520not%2520only%2520accurately%2520identify%2520common%25203D%250Aprinting%2520errors%252C%2520such%2520as%2520inconsistent%2520extrusion%252C%2520stringing%252C%2520warping%252C%2520and%2520layer%250Aadhesion%252C%2520but%2520also%2520effectively%2520determine%2520the%2520parameters%2520causing%2520these%2520failures%250Aand%2520autonomously%2520correct%2520them%2520without%2520any%2520need%2520for%2520human%2520intervention.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14307v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-3D%20Print%3A%20Large%20Language%20Models%20To%20Monitor%20and%20Control%203D%20Printing&entry.906535625=Yayati%20Jadhav%20and%20Peter%20Pak%20and%20Amir%20Barati%20Farimani&entry.1292438233=%20%20Industry%204.0%20has%20revolutionized%20manufacturing%20by%20driving%20digitalization%20and%0Ashifting%20the%20paradigm%20toward%20additive%20manufacturing%20%28AM%29.%20Fused%20Deposition%0AModeling%20%28FDM%29%2C%20a%20key%20AM%20technology%2C%20enables%20the%20creation%20of%20highly%20customized%2C%0Acost-effective%20products%20with%20minimal%20material%20waste%20through%20layer-by-layer%0Aextrusion%2C%20posing%20a%20significant%20challenge%20to%20traditional%20subtractive%20methods.%0AHowever%2C%20the%20susceptibility%20of%20material%20extrusion%20techniques%20to%20errors%20often%0Arequires%20expert%20intervention%20to%20detect%20and%20mitigate%20defects%20that%20can%20severely%0Acompromise%20product%20quality.%20While%20automated%20error%20detection%20and%20machine%0Alearning%20models%20exist%2C%20their%20generalizability%20across%20diverse%203D%20printer%20setups%2C%0Afirmware%2C%20and%20sensors%20is%20limited%2C%20and%20deep%20learning%20methods%20require%20extensive%0Alabeled%20datasets%2C%20hindering%20scalability%20and%20adaptability.%20To%20address%20these%0Achallenges%2C%20we%20present%20a%20process%20monitoring%20and%20control%20framework%20that%0Aleverages%20pre-trained%20Large%20Language%20Models%20%28LLMs%29%20alongside%203D%20printers%20to%0Adetect%20and%20address%20printing%20defects.%20The%20LLM%20evaluates%20print%20quality%20by%0Aanalyzing%20images%20captured%20after%20each%20layer%20or%20print%20segment%2C%20identifying%0Afailure%20modes%20and%20querying%20the%20printer%20for%20relevant%20parameters.%20It%20then%0Agenerates%20and%20executes%20a%20corrective%20action%20plan.%20We%20validated%20the%20effectiveness%0Aof%20the%20proposed%20framework%20in%20identifying%20defects%20by%20comparing%20it%20against%20a%0Acontrol%20group%20of%20engineers%20with%20diverse%20AM%20expertise.%20Our%20evaluation%0Ademonstrated%20that%20LLM-based%20agents%20not%20only%20accurately%20identify%20common%203D%0Aprinting%20errors%2C%20such%20as%20inconsistent%20extrusion%2C%20stringing%2C%20warping%2C%20and%20layer%0Aadhesion%2C%20but%20also%20effectively%20determine%20the%20parameters%20causing%20these%20failures%0Aand%20autonomously%20correct%20them%20without%20any%20need%20for%20human%20intervention.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14307v1&entry.124074799=Read"},
{"title": "Equivariant Reinforcement Learning under Partial Observability", "author": "Hai Nguyen and Andrea Baisero and David Klee and Dian Wang and Robert Platt and Christopher Amato", "abstract": "  Incorporating inductive biases is a promising approach for tackling\nchallenging robot learning domains with sample-efficient solutions. This paper\nidentifies partially observable domains where symmetries can be a useful\ninductive bias for efficient learning. Specifically, by encoding the\nequivariance regarding specific group symmetries into the neural networks, our\nactor-critic reinforcement learning agents can reuse solutions in the past for\nrelated scenarios. Consequently, our equivariant agents outperform\nnon-equivariant approaches significantly in terms of sample efficiency and\nfinal performance, demonstrated through experiments on a range of robotic tasks\nin simulation and real hardware.\n", "link": "http://arxiv.org/abs/2408.14336v1", "date": "2024-08-26", "relevancy": 2.0065, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5238}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4975}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4969}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Equivariant%20Reinforcement%20Learning%20under%20Partial%20Observability&body=Title%3A%20Equivariant%20Reinforcement%20Learning%20under%20Partial%20Observability%0AAuthor%3A%20Hai%20Nguyen%20and%20Andrea%20Baisero%20and%20David%20Klee%20and%20Dian%20Wang%20and%20Robert%20Platt%20and%20Christopher%20Amato%0AAbstract%3A%20%20%20Incorporating%20inductive%20biases%20is%20a%20promising%20approach%20for%20tackling%0Achallenging%20robot%20learning%20domains%20with%20sample-efficient%20solutions.%20This%20paper%0Aidentifies%20partially%20observable%20domains%20where%20symmetries%20can%20be%20a%20useful%0Ainductive%20bias%20for%20efficient%20learning.%20Specifically%2C%20by%20encoding%20the%0Aequivariance%20regarding%20specific%20group%20symmetries%20into%20the%20neural%20networks%2C%20our%0Aactor-critic%20reinforcement%20learning%20agents%20can%20reuse%20solutions%20in%20the%20past%20for%0Arelated%20scenarios.%20Consequently%2C%20our%20equivariant%20agents%20outperform%0Anon-equivariant%20approaches%20significantly%20in%20terms%20of%20sample%20efficiency%20and%0Afinal%20performance%2C%20demonstrated%20through%20experiments%20on%20a%20range%20of%20robotic%20tasks%0Ain%20simulation%20and%20real%20hardware.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14336v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEquivariant%2520Reinforcement%2520Learning%2520under%2520Partial%2520Observability%26entry.906535625%3DHai%2520Nguyen%2520and%2520Andrea%2520Baisero%2520and%2520David%2520Klee%2520and%2520Dian%2520Wang%2520and%2520Robert%2520Platt%2520and%2520Christopher%2520Amato%26entry.1292438233%3D%2520%2520Incorporating%2520inductive%2520biases%2520is%2520a%2520promising%2520approach%2520for%2520tackling%250Achallenging%2520robot%2520learning%2520domains%2520with%2520sample-efficient%2520solutions.%2520This%2520paper%250Aidentifies%2520partially%2520observable%2520domains%2520where%2520symmetries%2520can%2520be%2520a%2520useful%250Ainductive%2520bias%2520for%2520efficient%2520learning.%2520Specifically%252C%2520by%2520encoding%2520the%250Aequivariance%2520regarding%2520specific%2520group%2520symmetries%2520into%2520the%2520neural%2520networks%252C%2520our%250Aactor-critic%2520reinforcement%2520learning%2520agents%2520can%2520reuse%2520solutions%2520in%2520the%2520past%2520for%250Arelated%2520scenarios.%2520Consequently%252C%2520our%2520equivariant%2520agents%2520outperform%250Anon-equivariant%2520approaches%2520significantly%2520in%2520terms%2520of%2520sample%2520efficiency%2520and%250Afinal%2520performance%252C%2520demonstrated%2520through%2520experiments%2520on%2520a%2520range%2520of%2520robotic%2520tasks%250Ain%2520simulation%2520and%2520real%2520hardware.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14336v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Equivariant%20Reinforcement%20Learning%20under%20Partial%20Observability&entry.906535625=Hai%20Nguyen%20and%20Andrea%20Baisero%20and%20David%20Klee%20and%20Dian%20Wang%20and%20Robert%20Platt%20and%20Christopher%20Amato&entry.1292438233=%20%20Incorporating%20inductive%20biases%20is%20a%20promising%20approach%20for%20tackling%0Achallenging%20robot%20learning%20domains%20with%20sample-efficient%20solutions.%20This%20paper%0Aidentifies%20partially%20observable%20domains%20where%20symmetries%20can%20be%20a%20useful%0Ainductive%20bias%20for%20efficient%20learning.%20Specifically%2C%20by%20encoding%20the%0Aequivariance%20regarding%20specific%20group%20symmetries%20into%20the%20neural%20networks%2C%20our%0Aactor-critic%20reinforcement%20learning%20agents%20can%20reuse%20solutions%20in%20the%20past%20for%0Arelated%20scenarios.%20Consequently%2C%20our%20equivariant%20agents%20outperform%0Anon-equivariant%20approaches%20significantly%20in%20terms%20of%20sample%20efficiency%20and%0Afinal%20performance%2C%20demonstrated%20through%20experiments%20on%20a%20range%20of%20robotic%20tasks%0Ain%20simulation%20and%20real%20hardware.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14336v1&entry.124074799=Read"},
{"title": "PHEVA: A Privacy-preserving Human-centric Video Anomaly Detection\n  Dataset", "author": "Ghazal Alinezhad Noghre and Shanle Yao and Armin Danesh Pazho and Babak Rahimi Ardabili and Vinit Katariya and Hamed Tabkhi", "abstract": "  PHEVA, a Privacy-preserving Human-centric Ethical Video Anomaly detection\ndataset. By removing pixel information and providing only de-identified human\nannotations, PHEVA safeguards personally identifiable information. The dataset\nincludes seven indoor/outdoor scenes, featuring one novel, context-specific\ncamera, and offers over 5x the pose-annotated frames compared to the largest\nprevious dataset. This study benchmarks state-of-the-art methods on PHEVA using\na comprehensive set of metrics, including the 10% Error Rate (10ER), a metric\nused for anomaly detection for the first time providing insights relevant to\nreal-world deployment. As the first of its kind, PHEVA bridges the gap between\nconventional training and real-world deployment by introducing continual\nlearning benchmarks, with models outperforming traditional methods in 82.14% of\ncases. The dataset is publicly available at\nhttps://github.com/TeCSAR-UNCC/PHEVA.git.\n", "link": "http://arxiv.org/abs/2408.14329v1", "date": "2024-08-26", "relevancy": 1.9871, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5153}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4996}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PHEVA%3A%20A%20Privacy-preserving%20Human-centric%20Video%20Anomaly%20Detection%0A%20%20Dataset&body=Title%3A%20PHEVA%3A%20A%20Privacy-preserving%20Human-centric%20Video%20Anomaly%20Detection%0A%20%20Dataset%0AAuthor%3A%20Ghazal%20Alinezhad%20Noghre%20and%20Shanle%20Yao%20and%20Armin%20Danesh%20Pazho%20and%20Babak%20Rahimi%20Ardabili%20and%20Vinit%20Katariya%20and%20Hamed%20Tabkhi%0AAbstract%3A%20%20%20PHEVA%2C%20a%20Privacy-preserving%20Human-centric%20Ethical%20Video%20Anomaly%20detection%0Adataset.%20By%20removing%20pixel%20information%20and%20providing%20only%20de-identified%20human%0Aannotations%2C%20PHEVA%20safeguards%20personally%20identifiable%20information.%20The%20dataset%0Aincludes%20seven%20indoor/outdoor%20scenes%2C%20featuring%20one%20novel%2C%20context-specific%0Acamera%2C%20and%20offers%20over%205x%20the%20pose-annotated%20frames%20compared%20to%20the%20largest%0Aprevious%20dataset.%20This%20study%20benchmarks%20state-of-the-art%20methods%20on%20PHEVA%20using%0Aa%20comprehensive%20set%20of%20metrics%2C%20including%20the%2010%25%20Error%20Rate%20%2810ER%29%2C%20a%20metric%0Aused%20for%20anomaly%20detection%20for%20the%20first%20time%20providing%20insights%20relevant%20to%0Areal-world%20deployment.%20As%20the%20first%20of%20its%20kind%2C%20PHEVA%20bridges%20the%20gap%20between%0Aconventional%20training%20and%20real-world%20deployment%20by%20introducing%20continual%0Alearning%20benchmarks%2C%20with%20models%20outperforming%20traditional%20methods%20in%2082.14%25%20of%0Acases.%20The%20dataset%20is%20publicly%20available%20at%0Ahttps%3A//github.com/TeCSAR-UNCC/PHEVA.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14329v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPHEVA%253A%2520A%2520Privacy-preserving%2520Human-centric%2520Video%2520Anomaly%2520Detection%250A%2520%2520Dataset%26entry.906535625%3DGhazal%2520Alinezhad%2520Noghre%2520and%2520Shanle%2520Yao%2520and%2520Armin%2520Danesh%2520Pazho%2520and%2520Babak%2520Rahimi%2520Ardabili%2520and%2520Vinit%2520Katariya%2520and%2520Hamed%2520Tabkhi%26entry.1292438233%3D%2520%2520PHEVA%252C%2520a%2520Privacy-preserving%2520Human-centric%2520Ethical%2520Video%2520Anomaly%2520detection%250Adataset.%2520By%2520removing%2520pixel%2520information%2520and%2520providing%2520only%2520de-identified%2520human%250Aannotations%252C%2520PHEVA%2520safeguards%2520personally%2520identifiable%2520information.%2520The%2520dataset%250Aincludes%2520seven%2520indoor/outdoor%2520scenes%252C%2520featuring%2520one%2520novel%252C%2520context-specific%250Acamera%252C%2520and%2520offers%2520over%25205x%2520the%2520pose-annotated%2520frames%2520compared%2520to%2520the%2520largest%250Aprevious%2520dataset.%2520This%2520study%2520benchmarks%2520state-of-the-art%2520methods%2520on%2520PHEVA%2520using%250Aa%2520comprehensive%2520set%2520of%2520metrics%252C%2520including%2520the%252010%2525%2520Error%2520Rate%2520%252810ER%2529%252C%2520a%2520metric%250Aused%2520for%2520anomaly%2520detection%2520for%2520the%2520first%2520time%2520providing%2520insights%2520relevant%2520to%250Areal-world%2520deployment.%2520As%2520the%2520first%2520of%2520its%2520kind%252C%2520PHEVA%2520bridges%2520the%2520gap%2520between%250Aconventional%2520training%2520and%2520real-world%2520deployment%2520by%2520introducing%2520continual%250Alearning%2520benchmarks%252C%2520with%2520models%2520outperforming%2520traditional%2520methods%2520in%252082.14%2525%2520of%250Acases.%2520The%2520dataset%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/TeCSAR-UNCC/PHEVA.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14329v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PHEVA%3A%20A%20Privacy-preserving%20Human-centric%20Video%20Anomaly%20Detection%0A%20%20Dataset&entry.906535625=Ghazal%20Alinezhad%20Noghre%20and%20Shanle%20Yao%20and%20Armin%20Danesh%20Pazho%20and%20Babak%20Rahimi%20Ardabili%20and%20Vinit%20Katariya%20and%20Hamed%20Tabkhi&entry.1292438233=%20%20PHEVA%2C%20a%20Privacy-preserving%20Human-centric%20Ethical%20Video%20Anomaly%20detection%0Adataset.%20By%20removing%20pixel%20information%20and%20providing%20only%20de-identified%20human%0Aannotations%2C%20PHEVA%20safeguards%20personally%20identifiable%20information.%20The%20dataset%0Aincludes%20seven%20indoor/outdoor%20scenes%2C%20featuring%20one%20novel%2C%20context-specific%0Acamera%2C%20and%20offers%20over%205x%20the%20pose-annotated%20frames%20compared%20to%20the%20largest%0Aprevious%20dataset.%20This%20study%20benchmarks%20state-of-the-art%20methods%20on%20PHEVA%20using%0Aa%20comprehensive%20set%20of%20metrics%2C%20including%20the%2010%25%20Error%20Rate%20%2810ER%29%2C%20a%20metric%0Aused%20for%20anomaly%20detection%20for%20the%20first%20time%20providing%20insights%20relevant%20to%0Areal-world%20deployment.%20As%20the%20first%20of%20its%20kind%2C%20PHEVA%20bridges%20the%20gap%20between%0Aconventional%20training%20and%20real-world%20deployment%20by%20introducing%20continual%0Alearning%20benchmarks%2C%20with%20models%20outperforming%20traditional%20methods%20in%2082.14%25%20of%0Acases.%20The%20dataset%20is%20publicly%20available%20at%0Ahttps%3A//github.com/TeCSAR-UNCC/PHEVA.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14329v1&entry.124074799=Read"},
{"title": "GloSoFarID: Global multispectral dataset for Solar Farm IDentification\n  in satellite imagery", "author": "Zhiyuan Yang and Ryan Rad", "abstract": "  Solar Photovoltaic (PV) technology is increasingly recognized as a pivotal\nsolution in the global pursuit of clean and renewable energy. This technology\naddresses the urgent need for sustainable energy alternatives by converting\nsolar power into electricity without greenhouse gas emissions. It not only\ncurtails global carbon emissions but also reduces reliance on finite,\nnon-renewable energy sources. In this context, monitoring solar panel farms\nbecomes essential for understanding and facilitating the worldwide shift toward\nclean energy. This study contributes to this effort by developing the first\ncomprehensive global dataset of multispectral satellite imagery of solar panel\nfarms. This dataset is intended to form the basis for training robust machine\nlearning models, which can accurately map and analyze the expansion and\ndistribution of solar panel farms globally. The insights gained from this\nendeavor will be instrumental in guiding informed decision-making for a\nsustainable energy future. https://github.com/yzyly1992/GloSoFarID\n", "link": "http://arxiv.org/abs/2404.05180v2", "date": "2024-08-26", "relevancy": 1.9802, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4042}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.3994}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.3845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GloSoFarID%3A%20Global%20multispectral%20dataset%20for%20Solar%20Farm%20IDentification%0A%20%20in%20satellite%20imagery&body=Title%3A%20GloSoFarID%3A%20Global%20multispectral%20dataset%20for%20Solar%20Farm%20IDentification%0A%20%20in%20satellite%20imagery%0AAuthor%3A%20Zhiyuan%20Yang%20and%20Ryan%20Rad%0AAbstract%3A%20%20%20Solar%20Photovoltaic%20%28PV%29%20technology%20is%20increasingly%20recognized%20as%20a%20pivotal%0Asolution%20in%20the%20global%20pursuit%20of%20clean%20and%20renewable%20energy.%20This%20technology%0Aaddresses%20the%20urgent%20need%20for%20sustainable%20energy%20alternatives%20by%20converting%0Asolar%20power%20into%20electricity%20without%20greenhouse%20gas%20emissions.%20It%20not%20only%0Acurtails%20global%20carbon%20emissions%20but%20also%20reduces%20reliance%20on%20finite%2C%0Anon-renewable%20energy%20sources.%20In%20this%20context%2C%20monitoring%20solar%20panel%20farms%0Abecomes%20essential%20for%20understanding%20and%20facilitating%20the%20worldwide%20shift%20toward%0Aclean%20energy.%20This%20study%20contributes%20to%20this%20effort%20by%20developing%20the%20first%0Acomprehensive%20global%20dataset%20of%20multispectral%20satellite%20imagery%20of%20solar%20panel%0Afarms.%20This%20dataset%20is%20intended%20to%20form%20the%20basis%20for%20training%20robust%20machine%0Alearning%20models%2C%20which%20can%20accurately%20map%20and%20analyze%20the%20expansion%20and%0Adistribution%20of%20solar%20panel%20farms%20globally.%20The%20insights%20gained%20from%20this%0Aendeavor%20will%20be%20instrumental%20in%20guiding%20informed%20decision-making%20for%20a%0Asustainable%20energy%20future.%20https%3A//github.com/yzyly1992/GloSoFarID%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05180v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGloSoFarID%253A%2520Global%2520multispectral%2520dataset%2520for%2520Solar%2520Farm%2520IDentification%250A%2520%2520in%2520satellite%2520imagery%26entry.906535625%3DZhiyuan%2520Yang%2520and%2520Ryan%2520Rad%26entry.1292438233%3D%2520%2520Solar%2520Photovoltaic%2520%2528PV%2529%2520technology%2520is%2520increasingly%2520recognized%2520as%2520a%2520pivotal%250Asolution%2520in%2520the%2520global%2520pursuit%2520of%2520clean%2520and%2520renewable%2520energy.%2520This%2520technology%250Aaddresses%2520the%2520urgent%2520need%2520for%2520sustainable%2520energy%2520alternatives%2520by%2520converting%250Asolar%2520power%2520into%2520electricity%2520without%2520greenhouse%2520gas%2520emissions.%2520It%2520not%2520only%250Acurtails%2520global%2520carbon%2520emissions%2520but%2520also%2520reduces%2520reliance%2520on%2520finite%252C%250Anon-renewable%2520energy%2520sources.%2520In%2520this%2520context%252C%2520monitoring%2520solar%2520panel%2520farms%250Abecomes%2520essential%2520for%2520understanding%2520and%2520facilitating%2520the%2520worldwide%2520shift%2520toward%250Aclean%2520energy.%2520This%2520study%2520contributes%2520to%2520this%2520effort%2520by%2520developing%2520the%2520first%250Acomprehensive%2520global%2520dataset%2520of%2520multispectral%2520satellite%2520imagery%2520of%2520solar%2520panel%250Afarms.%2520This%2520dataset%2520is%2520intended%2520to%2520form%2520the%2520basis%2520for%2520training%2520robust%2520machine%250Alearning%2520models%252C%2520which%2520can%2520accurately%2520map%2520and%2520analyze%2520the%2520expansion%2520and%250Adistribution%2520of%2520solar%2520panel%2520farms%2520globally.%2520The%2520insights%2520gained%2520from%2520this%250Aendeavor%2520will%2520be%2520instrumental%2520in%2520guiding%2520informed%2520decision-making%2520for%2520a%250Asustainable%2520energy%2520future.%2520https%253A//github.com/yzyly1992/GloSoFarID%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.05180v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GloSoFarID%3A%20Global%20multispectral%20dataset%20for%20Solar%20Farm%20IDentification%0A%20%20in%20satellite%20imagery&entry.906535625=Zhiyuan%20Yang%20and%20Ryan%20Rad&entry.1292438233=%20%20Solar%20Photovoltaic%20%28PV%29%20technology%20is%20increasingly%20recognized%20as%20a%20pivotal%0Asolution%20in%20the%20global%20pursuit%20of%20clean%20and%20renewable%20energy.%20This%20technology%0Aaddresses%20the%20urgent%20need%20for%20sustainable%20energy%20alternatives%20by%20converting%0Asolar%20power%20into%20electricity%20without%20greenhouse%20gas%20emissions.%20It%20not%20only%0Acurtails%20global%20carbon%20emissions%20but%20also%20reduces%20reliance%20on%20finite%2C%0Anon-renewable%20energy%20sources.%20In%20this%20context%2C%20monitoring%20solar%20panel%20farms%0Abecomes%20essential%20for%20understanding%20and%20facilitating%20the%20worldwide%20shift%20toward%0Aclean%20energy.%20This%20study%20contributes%20to%20this%20effort%20by%20developing%20the%20first%0Acomprehensive%20global%20dataset%20of%20multispectral%20satellite%20imagery%20of%20solar%20panel%0Afarms.%20This%20dataset%20is%20intended%20to%20form%20the%20basis%20for%20training%20robust%20machine%0Alearning%20models%2C%20which%20can%20accurately%20map%20and%20analyze%20the%20expansion%20and%0Adistribution%20of%20solar%20panel%20farms%20globally.%20The%20insights%20gained%20from%20this%0Aendeavor%20will%20be%20instrumental%20in%20guiding%20informed%20decision-making%20for%20a%0Asustainable%20energy%20future.%20https%3A//github.com/yzyly1992/GloSoFarID%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05180v2&entry.124074799=Read"},
{"title": "Reprogramming Foundational Large Language Models(LLMs) for Enterprise\n  Adoption for Spatio-Temporal Forecasting Applications: Unveiling a New Era in\n  Copilot-Guided Cross-Modal Time Series Representation Learning", "author": "Sakhinana Sagar Srinivas and Chidaksh Ravuru and Geethan Sannidhi and Venkataramana Runkana", "abstract": "  Spatio-temporal forecasting plays a crucial role in various sectors such as\ntransportation systems, logistics, and supply chain management. However,\nexisting methods are limited by their ability to handle large, complex\ndatasets. To overcome this limitation, we introduce a hybrid approach that\ncombines the strengths of open-source large and small-scale language models\n(LLMs and LMs) with traditional forecasting methods. We augment traditional\nmethods with dynamic prompting and a grouped-query, multi-head attention\nmechanism to more effectively capture both intra-series and inter-series\ndependencies in evolving nonlinear time series data. In addition, we facilitate\non-premises customization by fine-tuning smaller open-source LMs for time\nseries trend analysis utilizing descriptions generated by open-source large LMs\non consumer-grade hardware using Low-Rank Adaptation with Activation Memory\nReduction (LoRA-AMR) technique to reduce computational overhead and activation\nstorage memory demands while preserving inference latency. We combine language\nmodel processing for time series trend analysis with traditional time series\nrepresentation learning method for cross-modal integration, achieving robust\nand accurate forecasts. The framework effectiveness is demonstrated through\nextensive experiments on various real-world datasets, outperforming existing\nmethods by significant margins in terms of forecast accuracy.\n", "link": "http://arxiv.org/abs/2408.14387v1", "date": "2024-08-26", "relevancy": 1.9786, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5087}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4934}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reprogramming%20Foundational%20Large%20Language%20Models%28LLMs%29%20for%20Enterprise%0A%20%20Adoption%20for%20Spatio-Temporal%20Forecasting%20Applications%3A%20Unveiling%20a%20New%20Era%20in%0A%20%20Copilot-Guided%20Cross-Modal%20Time%20Series%20Representation%20Learning&body=Title%3A%20Reprogramming%20Foundational%20Large%20Language%20Models%28LLMs%29%20for%20Enterprise%0A%20%20Adoption%20for%20Spatio-Temporal%20Forecasting%20Applications%3A%20Unveiling%20a%20New%20Era%20in%0A%20%20Copilot-Guided%20Cross-Modal%20Time%20Series%20Representation%20Learning%0AAuthor%3A%20Sakhinana%20Sagar%20Srinivas%20and%20Chidaksh%20Ravuru%20and%20Geethan%20Sannidhi%20and%20Venkataramana%20Runkana%0AAbstract%3A%20%20%20Spatio-temporal%20forecasting%20plays%20a%20crucial%20role%20in%20various%20sectors%20such%20as%0Atransportation%20systems%2C%20logistics%2C%20and%20supply%20chain%20management.%20However%2C%0Aexisting%20methods%20are%20limited%20by%20their%20ability%20to%20handle%20large%2C%20complex%0Adatasets.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20a%20hybrid%20approach%20that%0Acombines%20the%20strengths%20of%20open-source%20large%20and%20small-scale%20language%20models%0A%28LLMs%20and%20LMs%29%20with%20traditional%20forecasting%20methods.%20We%20augment%20traditional%0Amethods%20with%20dynamic%20prompting%20and%20a%20grouped-query%2C%20multi-head%20attention%0Amechanism%20to%20more%20effectively%20capture%20both%20intra-series%20and%20inter-series%0Adependencies%20in%20evolving%20nonlinear%20time%20series%20data.%20In%20addition%2C%20we%20facilitate%0Aon-premises%20customization%20by%20fine-tuning%20smaller%20open-source%20LMs%20for%20time%0Aseries%20trend%20analysis%20utilizing%20descriptions%20generated%20by%20open-source%20large%20LMs%0Aon%20consumer-grade%20hardware%20using%20Low-Rank%20Adaptation%20with%20Activation%20Memory%0AReduction%20%28LoRA-AMR%29%20technique%20to%20reduce%20computational%20overhead%20and%20activation%0Astorage%20memory%20demands%20while%20preserving%20inference%20latency.%20We%20combine%20language%0Amodel%20processing%20for%20time%20series%20trend%20analysis%20with%20traditional%20time%20series%0Arepresentation%20learning%20method%20for%20cross-modal%20integration%2C%20achieving%20robust%0Aand%20accurate%20forecasts.%20The%20framework%20effectiveness%20is%20demonstrated%20through%0Aextensive%20experiments%20on%20various%20real-world%20datasets%2C%20outperforming%20existing%0Amethods%20by%20significant%20margins%20in%20terms%20of%20forecast%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14387v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReprogramming%2520Foundational%2520Large%2520Language%2520Models%2528LLMs%2529%2520for%2520Enterprise%250A%2520%2520Adoption%2520for%2520Spatio-Temporal%2520Forecasting%2520Applications%253A%2520Unveiling%2520a%2520New%2520Era%2520in%250A%2520%2520Copilot-Guided%2520Cross-Modal%2520Time%2520Series%2520Representation%2520Learning%26entry.906535625%3DSakhinana%2520Sagar%2520Srinivas%2520and%2520Chidaksh%2520Ravuru%2520and%2520Geethan%2520Sannidhi%2520and%2520Venkataramana%2520Runkana%26entry.1292438233%3D%2520%2520Spatio-temporal%2520forecasting%2520plays%2520a%2520crucial%2520role%2520in%2520various%2520sectors%2520such%2520as%250Atransportation%2520systems%252C%2520logistics%252C%2520and%2520supply%2520chain%2520management.%2520However%252C%250Aexisting%2520methods%2520are%2520limited%2520by%2520their%2520ability%2520to%2520handle%2520large%252C%2520complex%250Adatasets.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520introduce%2520a%2520hybrid%2520approach%2520that%250Acombines%2520the%2520strengths%2520of%2520open-source%2520large%2520and%2520small-scale%2520language%2520models%250A%2528LLMs%2520and%2520LMs%2529%2520with%2520traditional%2520forecasting%2520methods.%2520We%2520augment%2520traditional%250Amethods%2520with%2520dynamic%2520prompting%2520and%2520a%2520grouped-query%252C%2520multi-head%2520attention%250Amechanism%2520to%2520more%2520effectively%2520capture%2520both%2520intra-series%2520and%2520inter-series%250Adependencies%2520in%2520evolving%2520nonlinear%2520time%2520series%2520data.%2520In%2520addition%252C%2520we%2520facilitate%250Aon-premises%2520customization%2520by%2520fine-tuning%2520smaller%2520open-source%2520LMs%2520for%2520time%250Aseries%2520trend%2520analysis%2520utilizing%2520descriptions%2520generated%2520by%2520open-source%2520large%2520LMs%250Aon%2520consumer-grade%2520hardware%2520using%2520Low-Rank%2520Adaptation%2520with%2520Activation%2520Memory%250AReduction%2520%2528LoRA-AMR%2529%2520technique%2520to%2520reduce%2520computational%2520overhead%2520and%2520activation%250Astorage%2520memory%2520demands%2520while%2520preserving%2520inference%2520latency.%2520We%2520combine%2520language%250Amodel%2520processing%2520for%2520time%2520series%2520trend%2520analysis%2520with%2520traditional%2520time%2520series%250Arepresentation%2520learning%2520method%2520for%2520cross-modal%2520integration%252C%2520achieving%2520robust%250Aand%2520accurate%2520forecasts.%2520The%2520framework%2520effectiveness%2520is%2520demonstrated%2520through%250Aextensive%2520experiments%2520on%2520various%2520real-world%2520datasets%252C%2520outperforming%2520existing%250Amethods%2520by%2520significant%2520margins%2520in%2520terms%2520of%2520forecast%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14387v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reprogramming%20Foundational%20Large%20Language%20Models%28LLMs%29%20for%20Enterprise%0A%20%20Adoption%20for%20Spatio-Temporal%20Forecasting%20Applications%3A%20Unveiling%20a%20New%20Era%20in%0A%20%20Copilot-Guided%20Cross-Modal%20Time%20Series%20Representation%20Learning&entry.906535625=Sakhinana%20Sagar%20Srinivas%20and%20Chidaksh%20Ravuru%20and%20Geethan%20Sannidhi%20and%20Venkataramana%20Runkana&entry.1292438233=%20%20Spatio-temporal%20forecasting%20plays%20a%20crucial%20role%20in%20various%20sectors%20such%20as%0Atransportation%20systems%2C%20logistics%2C%20and%20supply%20chain%20management.%20However%2C%0Aexisting%20methods%20are%20limited%20by%20their%20ability%20to%20handle%20large%2C%20complex%0Adatasets.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20a%20hybrid%20approach%20that%0Acombines%20the%20strengths%20of%20open-source%20large%20and%20small-scale%20language%20models%0A%28LLMs%20and%20LMs%29%20with%20traditional%20forecasting%20methods.%20We%20augment%20traditional%0Amethods%20with%20dynamic%20prompting%20and%20a%20grouped-query%2C%20multi-head%20attention%0Amechanism%20to%20more%20effectively%20capture%20both%20intra-series%20and%20inter-series%0Adependencies%20in%20evolving%20nonlinear%20time%20series%20data.%20In%20addition%2C%20we%20facilitate%0Aon-premises%20customization%20by%20fine-tuning%20smaller%20open-source%20LMs%20for%20time%0Aseries%20trend%20analysis%20utilizing%20descriptions%20generated%20by%20open-source%20large%20LMs%0Aon%20consumer-grade%20hardware%20using%20Low-Rank%20Adaptation%20with%20Activation%20Memory%0AReduction%20%28LoRA-AMR%29%20technique%20to%20reduce%20computational%20overhead%20and%20activation%0Astorage%20memory%20demands%20while%20preserving%20inference%20latency.%20We%20combine%20language%0Amodel%20processing%20for%20time%20series%20trend%20analysis%20with%20traditional%20time%20series%0Arepresentation%20learning%20method%20for%20cross-modal%20integration%2C%20achieving%20robust%0Aand%20accurate%20forecasts.%20The%20framework%20effectiveness%20is%20demonstrated%20through%0Aextensive%20experiments%20on%20various%20real-world%20datasets%2C%20outperforming%20existing%0Amethods%20by%20significant%20margins%20in%20terms%20of%20forecast%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14387v1&entry.124074799=Read"},
{"title": "SelEx: Self-Expertise in Fine-Grained Generalized Category Discovery", "author": "Sarah Rastegar and Mohammadreza Salehi and Yuki M. Asano and Hazel Doughty and Cees G. M. Snoek", "abstract": "  In this paper, we address Generalized Category Discovery, aiming to\nsimultaneously uncover novel categories and accurately classify known ones.\nTraditional methods, which lean heavily on self-supervision and contrastive\nlearning, often fall short when distinguishing between fine-grained categories.\nTo address this, we introduce a novel concept called `self-expertise', which\nenhances the model's ability to recognize subtle differences and uncover\nunknown categories. Our approach combines unsupervised and supervised\nself-expertise strategies to refine the model's discernment and generalization.\nInitially, hierarchical pseudo-labeling is used to provide `soft supervision',\nimproving the effectiveness of self-expertise. Our supervised technique differs\nfrom traditional methods by utilizing more abstract positive and negative\nsamples, aiding in the formation of clusters that can generalize to novel\ncategories. Meanwhile, our unsupervised strategy encourages the model to\nsharpen its category distinctions by considering within-category examples as\n`hard' negatives. Supported by theoretical insights, our empirical results\nshowcase that our method outperforms existing state-of-the-art techniques in\nGeneralized Category Discovery across several fine-grained datasets. Our code\nis available at: https://github.com/SarahRastegar/SelEx.\n", "link": "http://arxiv.org/abs/2408.14371v1", "date": "2024-08-26", "relevancy": 1.9785, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5067}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4889}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SelEx%3A%20Self-Expertise%20in%20Fine-Grained%20Generalized%20Category%20Discovery&body=Title%3A%20SelEx%3A%20Self-Expertise%20in%20Fine-Grained%20Generalized%20Category%20Discovery%0AAuthor%3A%20Sarah%20Rastegar%20and%20Mohammadreza%20Salehi%20and%20Yuki%20M.%20Asano%20and%20Hazel%20Doughty%20and%20Cees%20G.%20M.%20Snoek%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20address%20Generalized%20Category%20Discovery%2C%20aiming%20to%0Asimultaneously%20uncover%20novel%20categories%20and%20accurately%20classify%20known%20ones.%0ATraditional%20methods%2C%20which%20lean%20heavily%20on%20self-supervision%20and%20contrastive%0Alearning%2C%20often%20fall%20short%20when%20distinguishing%20between%20fine-grained%20categories.%0ATo%20address%20this%2C%20we%20introduce%20a%20novel%20concept%20called%20%60self-expertise%27%2C%20which%0Aenhances%20the%20model%27s%20ability%20to%20recognize%20subtle%20differences%20and%20uncover%0Aunknown%20categories.%20Our%20approach%20combines%20unsupervised%20and%20supervised%0Aself-expertise%20strategies%20to%20refine%20the%20model%27s%20discernment%20and%20generalization.%0AInitially%2C%20hierarchical%20pseudo-labeling%20is%20used%20to%20provide%20%60soft%20supervision%27%2C%0Aimproving%20the%20effectiveness%20of%20self-expertise.%20Our%20supervised%20technique%20differs%0Afrom%20traditional%20methods%20by%20utilizing%20more%20abstract%20positive%20and%20negative%0Asamples%2C%20aiding%20in%20the%20formation%20of%20clusters%20that%20can%20generalize%20to%20novel%0Acategories.%20Meanwhile%2C%20our%20unsupervised%20strategy%20encourages%20the%20model%20to%0Asharpen%20its%20category%20distinctions%20by%20considering%20within-category%20examples%20as%0A%60hard%27%20negatives.%20Supported%20by%20theoretical%20insights%2C%20our%20empirical%20results%0Ashowcase%20that%20our%20method%20outperforms%20existing%20state-of-the-art%20techniques%20in%0AGeneralized%20Category%20Discovery%20across%20several%20fine-grained%20datasets.%20Our%20code%0Ais%20available%20at%3A%20https%3A//github.com/SarahRastegar/SelEx.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14371v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelEx%253A%2520Self-Expertise%2520in%2520Fine-Grained%2520Generalized%2520Category%2520Discovery%26entry.906535625%3DSarah%2520Rastegar%2520and%2520Mohammadreza%2520Salehi%2520and%2520Yuki%2520M.%2520Asano%2520and%2520Hazel%2520Doughty%2520and%2520Cees%2520G.%2520M.%2520Snoek%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520address%2520Generalized%2520Category%2520Discovery%252C%2520aiming%2520to%250Asimultaneously%2520uncover%2520novel%2520categories%2520and%2520accurately%2520classify%2520known%2520ones.%250ATraditional%2520methods%252C%2520which%2520lean%2520heavily%2520on%2520self-supervision%2520and%2520contrastive%250Alearning%252C%2520often%2520fall%2520short%2520when%2520distinguishing%2520between%2520fine-grained%2520categories.%250ATo%2520address%2520this%252C%2520we%2520introduce%2520a%2520novel%2520concept%2520called%2520%2560self-expertise%2527%252C%2520which%250Aenhances%2520the%2520model%2527s%2520ability%2520to%2520recognize%2520subtle%2520differences%2520and%2520uncover%250Aunknown%2520categories.%2520Our%2520approach%2520combines%2520unsupervised%2520and%2520supervised%250Aself-expertise%2520strategies%2520to%2520refine%2520the%2520model%2527s%2520discernment%2520and%2520generalization.%250AInitially%252C%2520hierarchical%2520pseudo-labeling%2520is%2520used%2520to%2520provide%2520%2560soft%2520supervision%2527%252C%250Aimproving%2520the%2520effectiveness%2520of%2520self-expertise.%2520Our%2520supervised%2520technique%2520differs%250Afrom%2520traditional%2520methods%2520by%2520utilizing%2520more%2520abstract%2520positive%2520and%2520negative%250Asamples%252C%2520aiding%2520in%2520the%2520formation%2520of%2520clusters%2520that%2520can%2520generalize%2520to%2520novel%250Acategories.%2520Meanwhile%252C%2520our%2520unsupervised%2520strategy%2520encourages%2520the%2520model%2520to%250Asharpen%2520its%2520category%2520distinctions%2520by%2520considering%2520within-category%2520examples%2520as%250A%2560hard%2527%2520negatives.%2520Supported%2520by%2520theoretical%2520insights%252C%2520our%2520empirical%2520results%250Ashowcase%2520that%2520our%2520method%2520outperforms%2520existing%2520state-of-the-art%2520techniques%2520in%250AGeneralized%2520Category%2520Discovery%2520across%2520several%2520fine-grained%2520datasets.%2520Our%2520code%250Ais%2520available%2520at%253A%2520https%253A//github.com/SarahRastegar/SelEx.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14371v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SelEx%3A%20Self-Expertise%20in%20Fine-Grained%20Generalized%20Category%20Discovery&entry.906535625=Sarah%20Rastegar%20and%20Mohammadreza%20Salehi%20and%20Yuki%20M.%20Asano%20and%20Hazel%20Doughty%20and%20Cees%20G.%20M.%20Snoek&entry.1292438233=%20%20In%20this%20paper%2C%20we%20address%20Generalized%20Category%20Discovery%2C%20aiming%20to%0Asimultaneously%20uncover%20novel%20categories%20and%20accurately%20classify%20known%20ones.%0ATraditional%20methods%2C%20which%20lean%20heavily%20on%20self-supervision%20and%20contrastive%0Alearning%2C%20often%20fall%20short%20when%20distinguishing%20between%20fine-grained%20categories.%0ATo%20address%20this%2C%20we%20introduce%20a%20novel%20concept%20called%20%60self-expertise%27%2C%20which%0Aenhances%20the%20model%27s%20ability%20to%20recognize%20subtle%20differences%20and%20uncover%0Aunknown%20categories.%20Our%20approach%20combines%20unsupervised%20and%20supervised%0Aself-expertise%20strategies%20to%20refine%20the%20model%27s%20discernment%20and%20generalization.%0AInitially%2C%20hierarchical%20pseudo-labeling%20is%20used%20to%20provide%20%60soft%20supervision%27%2C%0Aimproving%20the%20effectiveness%20of%20self-expertise.%20Our%20supervised%20technique%20differs%0Afrom%20traditional%20methods%20by%20utilizing%20more%20abstract%20positive%20and%20negative%0Asamples%2C%20aiding%20in%20the%20formation%20of%20clusters%20that%20can%20generalize%20to%20novel%0Acategories.%20Meanwhile%2C%20our%20unsupervised%20strategy%20encourages%20the%20model%20to%0Asharpen%20its%20category%20distinctions%20by%20considering%20within-category%20examples%20as%0A%60hard%27%20negatives.%20Supported%20by%20theoretical%20insights%2C%20our%20empirical%20results%0Ashowcase%20that%20our%20method%20outperforms%20existing%20state-of-the-art%20techniques%20in%0AGeneralized%20Category%20Discovery%20across%20several%20fine-grained%20datasets.%20Our%20code%0Ais%20available%20at%3A%20https%3A//github.com/SarahRastegar/SelEx.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14371v1&entry.124074799=Read"},
{"title": "I2EBench: A Comprehensive Benchmark for Instruction-based Image Editing", "author": "Yiwei Ma and Jiayi Ji and Ke Ye and Weihuang Lin and Zhibin Wang and Yonghan Zheng and Qiang Zhou and Xiaoshuai Sun and Rongrong Ji", "abstract": "  Significant progress has been made in the field of Instruction-based Image\nEditing (IIE). However, evaluating these models poses a significant challenge.\nA crucial requirement in this field is the establishment of a comprehensive\nevaluation benchmark for accurately assessing editing results and providing\nvaluable insights for its further development. In response to this need, we\npropose I2EBench, a comprehensive benchmark designed to automatically evaluate\nthe quality of edited images produced by IIE models from multiple dimensions.\nI2EBench consists of 2,000+ images for editing, along with 4,000+ corresponding\noriginal and diverse instructions. It offers three distinctive characteristics:\n1) Comprehensive Evaluation Dimensions: I2EBench comprises 16 evaluation\ndimensions that cover both high-level and low-level aspects, providing a\ncomprehensive assessment of each IIE model. 2) Human Perception Alignment: To\nensure the alignment of our benchmark with human perception, we conducted an\nextensive user study for each evaluation dimension. 3) Valuable Research\nInsights: By analyzing the advantages and disadvantages of existing IIE models\nacross the 16 dimensions, we offer valuable research insights to guide future\ndevelopment in the field. We will open-source I2EBench, including all\ninstructions, input images, human annotations, edited images from all evaluated\nmethods, and a simple script for evaluating the results from new IIE models.\nThe code, dataset and generated images from all IIE models are provided in\ngithub: https://github.com/cocoshe/I2EBench.\n", "link": "http://arxiv.org/abs/2408.14180v1", "date": "2024-08-26", "relevancy": 1.9748, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.505}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4954}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4875}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20I2EBench%3A%20A%20Comprehensive%20Benchmark%20for%20Instruction-based%20Image%20Editing&body=Title%3A%20I2EBench%3A%20A%20Comprehensive%20Benchmark%20for%20Instruction-based%20Image%20Editing%0AAuthor%3A%20Yiwei%20Ma%20and%20Jiayi%20Ji%20and%20Ke%20Ye%20and%20Weihuang%20Lin%20and%20Zhibin%20Wang%20and%20Yonghan%20Zheng%20and%20Qiang%20Zhou%20and%20Xiaoshuai%20Sun%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20Significant%20progress%20has%20been%20made%20in%20the%20field%20of%20Instruction-based%20Image%0AEditing%20%28IIE%29.%20However%2C%20evaluating%20these%20models%20poses%20a%20significant%20challenge.%0AA%20crucial%20requirement%20in%20this%20field%20is%20the%20establishment%20of%20a%20comprehensive%0Aevaluation%20benchmark%20for%20accurately%20assessing%20editing%20results%20and%20providing%0Avaluable%20insights%20for%20its%20further%20development.%20In%20response%20to%20this%20need%2C%20we%0Apropose%20I2EBench%2C%20a%20comprehensive%20benchmark%20designed%20to%20automatically%20evaluate%0Athe%20quality%20of%20edited%20images%20produced%20by%20IIE%20models%20from%20multiple%20dimensions.%0AI2EBench%20consists%20of%202%2C000%2B%20images%20for%20editing%2C%20along%20with%204%2C000%2B%20corresponding%0Aoriginal%20and%20diverse%20instructions.%20It%20offers%20three%20distinctive%20characteristics%3A%0A1%29%20Comprehensive%20Evaluation%20Dimensions%3A%20I2EBench%20comprises%2016%20evaluation%0Adimensions%20that%20cover%20both%20high-level%20and%20low-level%20aspects%2C%20providing%20a%0Acomprehensive%20assessment%20of%20each%20IIE%20model.%202%29%20Human%20Perception%20Alignment%3A%20To%0Aensure%20the%20alignment%20of%20our%20benchmark%20with%20human%20perception%2C%20we%20conducted%20an%0Aextensive%20user%20study%20for%20each%20evaluation%20dimension.%203%29%20Valuable%20Research%0AInsights%3A%20By%20analyzing%20the%20advantages%20and%20disadvantages%20of%20existing%20IIE%20models%0Aacross%20the%2016%20dimensions%2C%20we%20offer%20valuable%20research%20insights%20to%20guide%20future%0Adevelopment%20in%20the%20field.%20We%20will%20open-source%20I2EBench%2C%20including%20all%0Ainstructions%2C%20input%20images%2C%20human%20annotations%2C%20edited%20images%20from%20all%20evaluated%0Amethods%2C%20and%20a%20simple%20script%20for%20evaluating%20the%20results%20from%20new%20IIE%20models.%0AThe%20code%2C%20dataset%20and%20generated%20images%20from%20all%20IIE%20models%20are%20provided%20in%0Agithub%3A%20https%3A//github.com/cocoshe/I2EBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14180v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DI2EBench%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Instruction-based%2520Image%2520Editing%26entry.906535625%3DYiwei%2520Ma%2520and%2520Jiayi%2520Ji%2520and%2520Ke%2520Ye%2520and%2520Weihuang%2520Lin%2520and%2520Zhibin%2520Wang%2520and%2520Yonghan%2520Zheng%2520and%2520Qiang%2520Zhou%2520and%2520Xiaoshuai%2520Sun%2520and%2520Rongrong%2520Ji%26entry.1292438233%3D%2520%2520Significant%2520progress%2520has%2520been%2520made%2520in%2520the%2520field%2520of%2520Instruction-based%2520Image%250AEditing%2520%2528IIE%2529.%2520However%252C%2520evaluating%2520these%2520models%2520poses%2520a%2520significant%2520challenge.%250AA%2520crucial%2520requirement%2520in%2520this%2520field%2520is%2520the%2520establishment%2520of%2520a%2520comprehensive%250Aevaluation%2520benchmark%2520for%2520accurately%2520assessing%2520editing%2520results%2520and%2520providing%250Avaluable%2520insights%2520for%2520its%2520further%2520development.%2520In%2520response%2520to%2520this%2520need%252C%2520we%250Apropose%2520I2EBench%252C%2520a%2520comprehensive%2520benchmark%2520designed%2520to%2520automatically%2520evaluate%250Athe%2520quality%2520of%2520edited%2520images%2520produced%2520by%2520IIE%2520models%2520from%2520multiple%2520dimensions.%250AI2EBench%2520consists%2520of%25202%252C000%252B%2520images%2520for%2520editing%252C%2520along%2520with%25204%252C000%252B%2520corresponding%250Aoriginal%2520and%2520diverse%2520instructions.%2520It%2520offers%2520three%2520distinctive%2520characteristics%253A%250A1%2529%2520Comprehensive%2520Evaluation%2520Dimensions%253A%2520I2EBench%2520comprises%252016%2520evaluation%250Adimensions%2520that%2520cover%2520both%2520high-level%2520and%2520low-level%2520aspects%252C%2520providing%2520a%250Acomprehensive%2520assessment%2520of%2520each%2520IIE%2520model.%25202%2529%2520Human%2520Perception%2520Alignment%253A%2520To%250Aensure%2520the%2520alignment%2520of%2520our%2520benchmark%2520with%2520human%2520perception%252C%2520we%2520conducted%2520an%250Aextensive%2520user%2520study%2520for%2520each%2520evaluation%2520dimension.%25203%2529%2520Valuable%2520Research%250AInsights%253A%2520By%2520analyzing%2520the%2520advantages%2520and%2520disadvantages%2520of%2520existing%2520IIE%2520models%250Aacross%2520the%252016%2520dimensions%252C%2520we%2520offer%2520valuable%2520research%2520insights%2520to%2520guide%2520future%250Adevelopment%2520in%2520the%2520field.%2520We%2520will%2520open-source%2520I2EBench%252C%2520including%2520all%250Ainstructions%252C%2520input%2520images%252C%2520human%2520annotations%252C%2520edited%2520images%2520from%2520all%2520evaluated%250Amethods%252C%2520and%2520a%2520simple%2520script%2520for%2520evaluating%2520the%2520results%2520from%2520new%2520IIE%2520models.%250AThe%2520code%252C%2520dataset%2520and%2520generated%2520images%2520from%2520all%2520IIE%2520models%2520are%2520provided%2520in%250Agithub%253A%2520https%253A//github.com/cocoshe/I2EBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14180v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=I2EBench%3A%20A%20Comprehensive%20Benchmark%20for%20Instruction-based%20Image%20Editing&entry.906535625=Yiwei%20Ma%20and%20Jiayi%20Ji%20and%20Ke%20Ye%20and%20Weihuang%20Lin%20and%20Zhibin%20Wang%20and%20Yonghan%20Zheng%20and%20Qiang%20Zhou%20and%20Xiaoshuai%20Sun%20and%20Rongrong%20Ji&entry.1292438233=%20%20Significant%20progress%20has%20been%20made%20in%20the%20field%20of%20Instruction-based%20Image%0AEditing%20%28IIE%29.%20However%2C%20evaluating%20these%20models%20poses%20a%20significant%20challenge.%0AA%20crucial%20requirement%20in%20this%20field%20is%20the%20establishment%20of%20a%20comprehensive%0Aevaluation%20benchmark%20for%20accurately%20assessing%20editing%20results%20and%20providing%0Avaluable%20insights%20for%20its%20further%20development.%20In%20response%20to%20this%20need%2C%20we%0Apropose%20I2EBench%2C%20a%20comprehensive%20benchmark%20designed%20to%20automatically%20evaluate%0Athe%20quality%20of%20edited%20images%20produced%20by%20IIE%20models%20from%20multiple%20dimensions.%0AI2EBench%20consists%20of%202%2C000%2B%20images%20for%20editing%2C%20along%20with%204%2C000%2B%20corresponding%0Aoriginal%20and%20diverse%20instructions.%20It%20offers%20three%20distinctive%20characteristics%3A%0A1%29%20Comprehensive%20Evaluation%20Dimensions%3A%20I2EBench%20comprises%2016%20evaluation%0Adimensions%20that%20cover%20both%20high-level%20and%20low-level%20aspects%2C%20providing%20a%0Acomprehensive%20assessment%20of%20each%20IIE%20model.%202%29%20Human%20Perception%20Alignment%3A%20To%0Aensure%20the%20alignment%20of%20our%20benchmark%20with%20human%20perception%2C%20we%20conducted%20an%0Aextensive%20user%20study%20for%20each%20evaluation%20dimension.%203%29%20Valuable%20Research%0AInsights%3A%20By%20analyzing%20the%20advantages%20and%20disadvantages%20of%20existing%20IIE%20models%0Aacross%20the%2016%20dimensions%2C%20we%20offer%20valuable%20research%20insights%20to%20guide%20future%0Adevelopment%20in%20the%20field.%20We%20will%20open-source%20I2EBench%2C%20including%20all%0Ainstructions%2C%20input%20images%2C%20human%20annotations%2C%20edited%20images%20from%20all%20evaluated%0Amethods%2C%20and%20a%20simple%20script%20for%20evaluating%20the%20results%20from%20new%20IIE%20models.%0AThe%20code%2C%20dataset%20and%20generated%20images%20from%20all%20IIE%20models%20are%20provided%20in%0Agithub%3A%20https%3A//github.com/cocoshe/I2EBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14180v1&entry.124074799=Read"},
{"title": "SwiftBrush v2: Make Your One-step Diffusion Model Better Than Its\n  Teacher", "author": "Trung Dao and Thuan Hoang Nguyen and Thanh Le and Duc Vu and Khoi Nguyen and Cuong Pham and Anh Tran", "abstract": "  In this paper, we aim to enhance the performance of SwiftBrush, a prominent\none-step text-to-image diffusion model, to be competitive with its multi-step\nStable Diffusion counterpart. Initially, we explore the quality-diversity\ntrade-off between SwiftBrush and SD Turbo: the former excels in image\ndiversity, while the latter excels in image quality. This observation motivates\nour proposed modifications in the training methodology, including better weight\ninitialization and efficient LoRA training. Moreover, our introduction of a\nnovel clamped CLIP loss enhances image-text alignment and results in improved\nimage quality. Remarkably, by combining the weights of models trained with\nefficient LoRA and full training, we achieve a new state-of-the-art one-step\ndiffusion model, achieving an FID of 8.14 and surpassing all GAN-based and\nmulti-step Stable Diffusion models. The evaluation code is available at:\nhttps://github.com/vinairesearch/swiftbrushv2.\n", "link": "http://arxiv.org/abs/2408.14176v1", "date": "2024-08-26", "relevancy": 1.9615, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6801}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6528}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6437}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SwiftBrush%20v2%3A%20Make%20Your%20One-step%20Diffusion%20Model%20Better%20Than%20Its%0A%20%20Teacher&body=Title%3A%20SwiftBrush%20v2%3A%20Make%20Your%20One-step%20Diffusion%20Model%20Better%20Than%20Its%0A%20%20Teacher%0AAuthor%3A%20Trung%20Dao%20and%20Thuan%20Hoang%20Nguyen%20and%20Thanh%20Le%20and%20Duc%20Vu%20and%20Khoi%20Nguyen%20and%20Cuong%20Pham%20and%20Anh%20Tran%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20aim%20to%20enhance%20the%20performance%20of%20SwiftBrush%2C%20a%20prominent%0Aone-step%20text-to-image%20diffusion%20model%2C%20to%20be%20competitive%20with%20its%20multi-step%0AStable%20Diffusion%20counterpart.%20Initially%2C%20we%20explore%20the%20quality-diversity%0Atrade-off%20between%20SwiftBrush%20and%20SD%20Turbo%3A%20the%20former%20excels%20in%20image%0Adiversity%2C%20while%20the%20latter%20excels%20in%20image%20quality.%20This%20observation%20motivates%0Aour%20proposed%20modifications%20in%20the%20training%20methodology%2C%20including%20better%20weight%0Ainitialization%20and%20efficient%20LoRA%20training.%20Moreover%2C%20our%20introduction%20of%20a%0Anovel%20clamped%20CLIP%20loss%20enhances%20image-text%20alignment%20and%20results%20in%20improved%0Aimage%20quality.%20Remarkably%2C%20by%20combining%20the%20weights%20of%20models%20trained%20with%0Aefficient%20LoRA%20and%20full%20training%2C%20we%20achieve%20a%20new%20state-of-the-art%20one-step%0Adiffusion%20model%2C%20achieving%20an%20FID%20of%208.14%20and%20surpassing%20all%20GAN-based%20and%0Amulti-step%20Stable%20Diffusion%20models.%20The%20evaluation%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/vinairesearch/swiftbrushv2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14176v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSwiftBrush%2520v2%253A%2520Make%2520Your%2520One-step%2520Diffusion%2520Model%2520Better%2520Than%2520Its%250A%2520%2520Teacher%26entry.906535625%3DTrung%2520Dao%2520and%2520Thuan%2520Hoang%2520Nguyen%2520and%2520Thanh%2520Le%2520and%2520Duc%2520Vu%2520and%2520Khoi%2520Nguyen%2520and%2520Cuong%2520Pham%2520and%2520Anh%2520Tran%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520enhance%2520the%2520performance%2520of%2520SwiftBrush%252C%2520a%2520prominent%250Aone-step%2520text-to-image%2520diffusion%2520model%252C%2520to%2520be%2520competitive%2520with%2520its%2520multi-step%250AStable%2520Diffusion%2520counterpart.%2520Initially%252C%2520we%2520explore%2520the%2520quality-diversity%250Atrade-off%2520between%2520SwiftBrush%2520and%2520SD%2520Turbo%253A%2520the%2520former%2520excels%2520in%2520image%250Adiversity%252C%2520while%2520the%2520latter%2520excels%2520in%2520image%2520quality.%2520This%2520observation%2520motivates%250Aour%2520proposed%2520modifications%2520in%2520the%2520training%2520methodology%252C%2520including%2520better%2520weight%250Ainitialization%2520and%2520efficient%2520LoRA%2520training.%2520Moreover%252C%2520our%2520introduction%2520of%2520a%250Anovel%2520clamped%2520CLIP%2520loss%2520enhances%2520image-text%2520alignment%2520and%2520results%2520in%2520improved%250Aimage%2520quality.%2520Remarkably%252C%2520by%2520combining%2520the%2520weights%2520of%2520models%2520trained%2520with%250Aefficient%2520LoRA%2520and%2520full%2520training%252C%2520we%2520achieve%2520a%2520new%2520state-of-the-art%2520one-step%250Adiffusion%2520model%252C%2520achieving%2520an%2520FID%2520of%25208.14%2520and%2520surpassing%2520all%2520GAN-based%2520and%250Amulti-step%2520Stable%2520Diffusion%2520models.%2520The%2520evaluation%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/vinairesearch/swiftbrushv2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14176v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SwiftBrush%20v2%3A%20Make%20Your%20One-step%20Diffusion%20Model%20Better%20Than%20Its%0A%20%20Teacher&entry.906535625=Trung%20Dao%20and%20Thuan%20Hoang%20Nguyen%20and%20Thanh%20Le%20and%20Duc%20Vu%20and%20Khoi%20Nguyen%20and%20Cuong%20Pham%20and%20Anh%20Tran&entry.1292438233=%20%20In%20this%20paper%2C%20we%20aim%20to%20enhance%20the%20performance%20of%20SwiftBrush%2C%20a%20prominent%0Aone-step%20text-to-image%20diffusion%20model%2C%20to%20be%20competitive%20with%20its%20multi-step%0AStable%20Diffusion%20counterpart.%20Initially%2C%20we%20explore%20the%20quality-diversity%0Atrade-off%20between%20SwiftBrush%20and%20SD%20Turbo%3A%20the%20former%20excels%20in%20image%0Adiversity%2C%20while%20the%20latter%20excels%20in%20image%20quality.%20This%20observation%20motivates%0Aour%20proposed%20modifications%20in%20the%20training%20methodology%2C%20including%20better%20weight%0Ainitialization%20and%20efficient%20LoRA%20training.%20Moreover%2C%20our%20introduction%20of%20a%0Anovel%20clamped%20CLIP%20loss%20enhances%20image-text%20alignment%20and%20results%20in%20improved%0Aimage%20quality.%20Remarkably%2C%20by%20combining%20the%20weights%20of%20models%20trained%20with%0Aefficient%20LoRA%20and%20full%20training%2C%20we%20achieve%20a%20new%20state-of-the-art%20one-step%0Adiffusion%20model%2C%20achieving%20an%20FID%20of%208.14%20and%20surpassing%20all%20GAN-based%20and%0Amulti-step%20Stable%20Diffusion%20models.%20The%20evaluation%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/vinairesearch/swiftbrushv2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14176v1&entry.124074799=Read"},
{"title": "Urban Region Pre-training and Prompting: A Graph-based Approach", "author": "Jiahui Jin and Yifan Song and Dong Kan and Haojia Zhu and Xiangguo Sun and Zhicheng Li and Xigang Sun and Jinghui Zhang", "abstract": "  Urban region representation is crucial for various urban downstream tasks.\nHowever, despite the proliferation of methods and their success, acquiring\ngeneral urban region knowledge and adapting to different tasks remains\nchallenging. Previous work often neglects the spatial structures and functional\nlayouts between entities, limiting their ability to capture transferable\nknowledge across regions. Further, these methods struggle to adapt effectively\nto specific downstream tasks, as they do not adequately address the unique\nfeatures and relationships required for different downstream tasks. In this\npaper, we propose a $\\textbf{G}$raph-based $\\textbf{U}$rban $\\textbf{R}$egion\n$\\textbf{P}$re-training and $\\textbf{P}$rompting framework ($\\textbf{GURPP}$)\nfor region representation learning. Specifically, we first construct an urban\nregion graph that integrates detailed spatial entity data for more effective\nurban region representation. Then, we develop a subgraph-centric urban region\npre-training model to capture the heterogeneous and transferable patterns of\ninteractions among entities. To further enhance the adaptability of these\nembeddings to different tasks, we design two graph-based prompting methods to\nincorporate explicit/hidden task knowledge. Extensive experiments on various\nurban region prediction tasks and different cities demonstrate the superior\nperformance of our GURPP framework.\n", "link": "http://arxiv.org/abs/2408.05920v3", "date": "2024-08-26", "relevancy": 1.9257, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4858}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4821}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Urban%20Region%20Pre-training%20and%20Prompting%3A%20A%20Graph-based%20Approach&body=Title%3A%20Urban%20Region%20Pre-training%20and%20Prompting%3A%20A%20Graph-based%20Approach%0AAuthor%3A%20Jiahui%20Jin%20and%20Yifan%20Song%20and%20Dong%20Kan%20and%20Haojia%20Zhu%20and%20Xiangguo%20Sun%20and%20Zhicheng%20Li%20and%20Xigang%20Sun%20and%20Jinghui%20Zhang%0AAbstract%3A%20%20%20Urban%20region%20representation%20is%20crucial%20for%20various%20urban%20downstream%20tasks.%0AHowever%2C%20despite%20the%20proliferation%20of%20methods%20and%20their%20success%2C%20acquiring%0Ageneral%20urban%20region%20knowledge%20and%20adapting%20to%20different%20tasks%20remains%0Achallenging.%20Previous%20work%20often%20neglects%20the%20spatial%20structures%20and%20functional%0Alayouts%20between%20entities%2C%20limiting%20their%20ability%20to%20capture%20transferable%0Aknowledge%20across%20regions.%20Further%2C%20these%20methods%20struggle%20to%20adapt%20effectively%0Ato%20specific%20downstream%20tasks%2C%20as%20they%20do%20not%20adequately%20address%20the%20unique%0Afeatures%20and%20relationships%20required%20for%20different%20downstream%20tasks.%20In%20this%0Apaper%2C%20we%20propose%20a%20%24%5Ctextbf%7BG%7D%24raph-based%20%24%5Ctextbf%7BU%7D%24rban%20%24%5Ctextbf%7BR%7D%24egion%0A%24%5Ctextbf%7BP%7D%24re-training%20and%20%24%5Ctextbf%7BP%7D%24rompting%20framework%20%28%24%5Ctextbf%7BGURPP%7D%24%29%0Afor%20region%20representation%20learning.%20Specifically%2C%20we%20first%20construct%20an%20urban%0Aregion%20graph%20that%20integrates%20detailed%20spatial%20entity%20data%20for%20more%20effective%0Aurban%20region%20representation.%20Then%2C%20we%20develop%20a%20subgraph-centric%20urban%20region%0Apre-training%20model%20to%20capture%20the%20heterogeneous%20and%20transferable%20patterns%20of%0Ainteractions%20among%20entities.%20To%20further%20enhance%20the%20adaptability%20of%20these%0Aembeddings%20to%20different%20tasks%2C%20we%20design%20two%20graph-based%20prompting%20methods%20to%0Aincorporate%20explicit/hidden%20task%20knowledge.%20Extensive%20experiments%20on%20various%0Aurban%20region%20prediction%20tasks%20and%20different%20cities%20demonstrate%20the%20superior%0Aperformance%20of%20our%20GURPP%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05920v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUrban%2520Region%2520Pre-training%2520and%2520Prompting%253A%2520A%2520Graph-based%2520Approach%26entry.906535625%3DJiahui%2520Jin%2520and%2520Yifan%2520Song%2520and%2520Dong%2520Kan%2520and%2520Haojia%2520Zhu%2520and%2520Xiangguo%2520Sun%2520and%2520Zhicheng%2520Li%2520and%2520Xigang%2520Sun%2520and%2520Jinghui%2520Zhang%26entry.1292438233%3D%2520%2520Urban%2520region%2520representation%2520is%2520crucial%2520for%2520various%2520urban%2520downstream%2520tasks.%250AHowever%252C%2520despite%2520the%2520proliferation%2520of%2520methods%2520and%2520their%2520success%252C%2520acquiring%250Ageneral%2520urban%2520region%2520knowledge%2520and%2520adapting%2520to%2520different%2520tasks%2520remains%250Achallenging.%2520Previous%2520work%2520often%2520neglects%2520the%2520spatial%2520structures%2520and%2520functional%250Alayouts%2520between%2520entities%252C%2520limiting%2520their%2520ability%2520to%2520capture%2520transferable%250Aknowledge%2520across%2520regions.%2520Further%252C%2520these%2520methods%2520struggle%2520to%2520adapt%2520effectively%250Ato%2520specific%2520downstream%2520tasks%252C%2520as%2520they%2520do%2520not%2520adequately%2520address%2520the%2520unique%250Afeatures%2520and%2520relationships%2520required%2520for%2520different%2520downstream%2520tasks.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520%2524%255Ctextbf%257BG%257D%2524raph-based%2520%2524%255Ctextbf%257BU%257D%2524rban%2520%2524%255Ctextbf%257BR%257D%2524egion%250A%2524%255Ctextbf%257BP%257D%2524re-training%2520and%2520%2524%255Ctextbf%257BP%257D%2524rompting%2520framework%2520%2528%2524%255Ctextbf%257BGURPP%257D%2524%2529%250Afor%2520region%2520representation%2520learning.%2520Specifically%252C%2520we%2520first%2520construct%2520an%2520urban%250Aregion%2520graph%2520that%2520integrates%2520detailed%2520spatial%2520entity%2520data%2520for%2520more%2520effective%250Aurban%2520region%2520representation.%2520Then%252C%2520we%2520develop%2520a%2520subgraph-centric%2520urban%2520region%250Apre-training%2520model%2520to%2520capture%2520the%2520heterogeneous%2520and%2520transferable%2520patterns%2520of%250Ainteractions%2520among%2520entities.%2520To%2520further%2520enhance%2520the%2520adaptability%2520of%2520these%250Aembeddings%2520to%2520different%2520tasks%252C%2520we%2520design%2520two%2520graph-based%2520prompting%2520methods%2520to%250Aincorporate%2520explicit/hidden%2520task%2520knowledge.%2520Extensive%2520experiments%2520on%2520various%250Aurban%2520region%2520prediction%2520tasks%2520and%2520different%2520cities%2520demonstrate%2520the%2520superior%250Aperformance%2520of%2520our%2520GURPP%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05920v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Urban%20Region%20Pre-training%20and%20Prompting%3A%20A%20Graph-based%20Approach&entry.906535625=Jiahui%20Jin%20and%20Yifan%20Song%20and%20Dong%20Kan%20and%20Haojia%20Zhu%20and%20Xiangguo%20Sun%20and%20Zhicheng%20Li%20and%20Xigang%20Sun%20and%20Jinghui%20Zhang&entry.1292438233=%20%20Urban%20region%20representation%20is%20crucial%20for%20various%20urban%20downstream%20tasks.%0AHowever%2C%20despite%20the%20proliferation%20of%20methods%20and%20their%20success%2C%20acquiring%0Ageneral%20urban%20region%20knowledge%20and%20adapting%20to%20different%20tasks%20remains%0Achallenging.%20Previous%20work%20often%20neglects%20the%20spatial%20structures%20and%20functional%0Alayouts%20between%20entities%2C%20limiting%20their%20ability%20to%20capture%20transferable%0Aknowledge%20across%20regions.%20Further%2C%20these%20methods%20struggle%20to%20adapt%20effectively%0Ato%20specific%20downstream%20tasks%2C%20as%20they%20do%20not%20adequately%20address%20the%20unique%0Afeatures%20and%20relationships%20required%20for%20different%20downstream%20tasks.%20In%20this%0Apaper%2C%20we%20propose%20a%20%24%5Ctextbf%7BG%7D%24raph-based%20%24%5Ctextbf%7BU%7D%24rban%20%24%5Ctextbf%7BR%7D%24egion%0A%24%5Ctextbf%7BP%7D%24re-training%20and%20%24%5Ctextbf%7BP%7D%24rompting%20framework%20%28%24%5Ctextbf%7BGURPP%7D%24%29%0Afor%20region%20representation%20learning.%20Specifically%2C%20we%20first%20construct%20an%20urban%0Aregion%20graph%20that%20integrates%20detailed%20spatial%20entity%20data%20for%20more%20effective%0Aurban%20region%20representation.%20Then%2C%20we%20develop%20a%20subgraph-centric%20urban%20region%0Apre-training%20model%20to%20capture%20the%20heterogeneous%20and%20transferable%20patterns%20of%0Ainteractions%20among%20entities.%20To%20further%20enhance%20the%20adaptability%20of%20these%0Aembeddings%20to%20different%20tasks%2C%20we%20design%20two%20graph-based%20prompting%20methods%20to%0Aincorporate%20explicit/hidden%20task%20knowledge.%20Extensive%20experiments%20on%20various%0Aurban%20region%20prediction%20tasks%20and%20different%20cities%20demonstrate%20the%20superior%0Aperformance%20of%20our%20GURPP%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05920v3&entry.124074799=Read"},
{"title": "Celtibero: Robust Layered Aggregation for Federated Learning", "author": "Borja Molina-Coronado", "abstract": "  Federated Learning (FL) is an innovative approach to distributed machine\nlearning. While FL offers significant privacy advantages, it also faces\nsecurity challenges, particularly from poisoning attacks where adversaries\ndeliberately manipulate local model updates to degrade model performance or\nintroduce hidden backdoors. Existing defenses against these attacks have been\nshown to be effective when the data on the nodes is identically and\nindependently distributed (i.i.d.), but they often fail under less restrictive,\nnon-i.i.d data conditions. To overcome these limitations, we introduce\nCeltibero, a novel defense mechanism that integrates layered aggregation to\nenhance robustness against adversarial manipulation. Through extensive\nexperiments on the MNIST and IMDB datasets, we demonstrate that Celtibero\nconsistently achieves high main task accuracy (MTA) while maintaining minimal\nattack success rates (ASR) across a range of untargeted and targeted poisoning\nattacks. Our results highlight the superiority of Celtibero over existing\ndefenses such as FL-Defender, LFighter, and FLAME, establishing it as a highly\neffective solution for securing federated learning systems against\nsophisticated poisoning attacks.\n", "link": "http://arxiv.org/abs/2408.14240v1", "date": "2024-08-26", "relevancy": 1.9129, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4846}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.477}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Celtibero%3A%20Robust%20Layered%20Aggregation%20for%20Federated%20Learning&body=Title%3A%20Celtibero%3A%20Robust%20Layered%20Aggregation%20for%20Federated%20Learning%0AAuthor%3A%20Borja%20Molina-Coronado%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20is%20an%20innovative%20approach%20to%20distributed%20machine%0Alearning.%20While%20FL%20offers%20significant%20privacy%20advantages%2C%20it%20also%20faces%0Asecurity%20challenges%2C%20particularly%20from%20poisoning%20attacks%20where%20adversaries%0Adeliberately%20manipulate%20local%20model%20updates%20to%20degrade%20model%20performance%20or%0Aintroduce%20hidden%20backdoors.%20Existing%20defenses%20against%20these%20attacks%20have%20been%0Ashown%20to%20be%20effective%20when%20the%20data%20on%20the%20nodes%20is%20identically%20and%0Aindependently%20distributed%20%28i.i.d.%29%2C%20but%20they%20often%20fail%20under%20less%20restrictive%2C%0Anon-i.i.d%20data%20conditions.%20To%20overcome%20these%20limitations%2C%20we%20introduce%0ACeltibero%2C%20a%20novel%20defense%20mechanism%20that%20integrates%20layered%20aggregation%20to%0Aenhance%20robustness%20against%20adversarial%20manipulation.%20Through%20extensive%0Aexperiments%20on%20the%20MNIST%20and%20IMDB%20datasets%2C%20we%20demonstrate%20that%20Celtibero%0Aconsistently%20achieves%20high%20main%20task%20accuracy%20%28MTA%29%20while%20maintaining%20minimal%0Aattack%20success%20rates%20%28ASR%29%20across%20a%20range%20of%20untargeted%20and%20targeted%20poisoning%0Aattacks.%20Our%20results%20highlight%20the%20superiority%20of%20Celtibero%20over%20existing%0Adefenses%20such%20as%20FL-Defender%2C%20LFighter%2C%20and%20FLAME%2C%20establishing%20it%20as%20a%20highly%0Aeffective%20solution%20for%20securing%20federated%20learning%20systems%20against%0Asophisticated%20poisoning%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14240v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCeltibero%253A%2520Robust%2520Layered%2520Aggregation%2520for%2520Federated%2520Learning%26entry.906535625%3DBorja%2520Molina-Coronado%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520an%2520innovative%2520approach%2520to%2520distributed%2520machine%250Alearning.%2520While%2520FL%2520offers%2520significant%2520privacy%2520advantages%252C%2520it%2520also%2520faces%250Asecurity%2520challenges%252C%2520particularly%2520from%2520poisoning%2520attacks%2520where%2520adversaries%250Adeliberately%2520manipulate%2520local%2520model%2520updates%2520to%2520degrade%2520model%2520performance%2520or%250Aintroduce%2520hidden%2520backdoors.%2520Existing%2520defenses%2520against%2520these%2520attacks%2520have%2520been%250Ashown%2520to%2520be%2520effective%2520when%2520the%2520data%2520on%2520the%2520nodes%2520is%2520identically%2520and%250Aindependently%2520distributed%2520%2528i.i.d.%2529%252C%2520but%2520they%2520often%2520fail%2520under%2520less%2520restrictive%252C%250Anon-i.i.d%2520data%2520conditions.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520introduce%250ACeltibero%252C%2520a%2520novel%2520defense%2520mechanism%2520that%2520integrates%2520layered%2520aggregation%2520to%250Aenhance%2520robustness%2520against%2520adversarial%2520manipulation.%2520Through%2520extensive%250Aexperiments%2520on%2520the%2520MNIST%2520and%2520IMDB%2520datasets%252C%2520we%2520demonstrate%2520that%2520Celtibero%250Aconsistently%2520achieves%2520high%2520main%2520task%2520accuracy%2520%2528MTA%2529%2520while%2520maintaining%2520minimal%250Aattack%2520success%2520rates%2520%2528ASR%2529%2520across%2520a%2520range%2520of%2520untargeted%2520and%2520targeted%2520poisoning%250Aattacks.%2520Our%2520results%2520highlight%2520the%2520superiority%2520of%2520Celtibero%2520over%2520existing%250Adefenses%2520such%2520as%2520FL-Defender%252C%2520LFighter%252C%2520and%2520FLAME%252C%2520establishing%2520it%2520as%2520a%2520highly%250Aeffective%2520solution%2520for%2520securing%2520federated%2520learning%2520systems%2520against%250Asophisticated%2520poisoning%2520attacks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14240v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Celtibero%3A%20Robust%20Layered%20Aggregation%20for%20Federated%20Learning&entry.906535625=Borja%20Molina-Coronado&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20is%20an%20innovative%20approach%20to%20distributed%20machine%0Alearning.%20While%20FL%20offers%20significant%20privacy%20advantages%2C%20it%20also%20faces%0Asecurity%20challenges%2C%20particularly%20from%20poisoning%20attacks%20where%20adversaries%0Adeliberately%20manipulate%20local%20model%20updates%20to%20degrade%20model%20performance%20or%0Aintroduce%20hidden%20backdoors.%20Existing%20defenses%20against%20these%20attacks%20have%20been%0Ashown%20to%20be%20effective%20when%20the%20data%20on%20the%20nodes%20is%20identically%20and%0Aindependently%20distributed%20%28i.i.d.%29%2C%20but%20they%20often%20fail%20under%20less%20restrictive%2C%0Anon-i.i.d%20data%20conditions.%20To%20overcome%20these%20limitations%2C%20we%20introduce%0ACeltibero%2C%20a%20novel%20defense%20mechanism%20that%20integrates%20layered%20aggregation%20to%0Aenhance%20robustness%20against%20adversarial%20manipulation.%20Through%20extensive%0Aexperiments%20on%20the%20MNIST%20and%20IMDB%20datasets%2C%20we%20demonstrate%20that%20Celtibero%0Aconsistently%20achieves%20high%20main%20task%20accuracy%20%28MTA%29%20while%20maintaining%20minimal%0Aattack%20success%20rates%20%28ASR%29%20across%20a%20range%20of%20untargeted%20and%20targeted%20poisoning%0Aattacks.%20Our%20results%20highlight%20the%20superiority%20of%20Celtibero%20over%20existing%0Adefenses%20such%20as%20FL-Defender%2C%20LFighter%2C%20and%20FLAME%2C%20establishing%20it%20as%20a%20highly%0Aeffective%20solution%20for%20securing%20federated%20learning%20systems%20against%0Asophisticated%20poisoning%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14240v1&entry.124074799=Read"},
{"title": "Graph-SCP: Accelerating Set Cover Problems with Graph Neural Networks", "author": "Zohair Shafi and Benjamin A. Miller and Tina Eliassi-Rad and Rajmonda S. Caceres", "abstract": "  Machine learning (ML) approaches are increasingly being used to accelerate\ncombinatorial optimization (CO) problems. We investigate the Set Cover Problem\n(SCP) and propose Graph-SCP, a graph neural network method that augments\nexisting optimization solvers by learning to identify a much smaller\nsub-problem that contains the solution space. Graph-SCP uses both supervised\nlearning from prior solved instances and unsupervised learning aimed at\nminimizing the SCP objective. We evaluate the performance of Graph-SCP on\nsynthetically weighted and unweighted SCP instances with diverse problem\ncharacteristics and complexities, and on instances from the OR Library, a\ncanonical benchmark for SCP. We show that Graph-SCP reduces the problem size by\n60-80% and achieves runtime speedups of up to 10x on average when compared to\nGurobi (a state-of-the-art commercial solver), while maintaining solution\nquality. This is in contrast to fast greedy solutions that significantly\ncompromise solution quality to achieve guaranteed polynomial runtime. We\nshowcase Graph-SCP's ability to generalize to larger problem sizes, training on\nSCP instances with up to 3,000 subsets and testing on SCP instances with up to\n10,000 subsets.\n", "link": "http://arxiv.org/abs/2310.07979v2", "date": "2024-08-26", "relevancy": 1.9124, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5117}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.501}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph-SCP%3A%20Accelerating%20Set%20Cover%20Problems%20with%20Graph%20Neural%20Networks&body=Title%3A%20Graph-SCP%3A%20Accelerating%20Set%20Cover%20Problems%20with%20Graph%20Neural%20Networks%0AAuthor%3A%20Zohair%20Shafi%20and%20Benjamin%20A.%20Miller%20and%20Tina%20Eliassi-Rad%20and%20Rajmonda%20S.%20Caceres%0AAbstract%3A%20%20%20Machine%20learning%20%28ML%29%20approaches%20are%20increasingly%20being%20used%20to%20accelerate%0Acombinatorial%20optimization%20%28CO%29%20problems.%20We%20investigate%20the%20Set%20Cover%20Problem%0A%28SCP%29%20and%20propose%20Graph-SCP%2C%20a%20graph%20neural%20network%20method%20that%20augments%0Aexisting%20optimization%20solvers%20by%20learning%20to%20identify%20a%20much%20smaller%0Asub-problem%20that%20contains%20the%20solution%20space.%20Graph-SCP%20uses%20both%20supervised%0Alearning%20from%20prior%20solved%20instances%20and%20unsupervised%20learning%20aimed%20at%0Aminimizing%20the%20SCP%20objective.%20We%20evaluate%20the%20performance%20of%20Graph-SCP%20on%0Asynthetically%20weighted%20and%20unweighted%20SCP%20instances%20with%20diverse%20problem%0Acharacteristics%20and%20complexities%2C%20and%20on%20instances%20from%20the%20OR%20Library%2C%20a%0Acanonical%20benchmark%20for%20SCP.%20We%20show%20that%20Graph-SCP%20reduces%20the%20problem%20size%20by%0A60-80%25%20and%20achieves%20runtime%20speedups%20of%20up%20to%2010x%20on%20average%20when%20compared%20to%0AGurobi%20%28a%20state-of-the-art%20commercial%20solver%29%2C%20while%20maintaining%20solution%0Aquality.%20This%20is%20in%20contrast%20to%20fast%20greedy%20solutions%20that%20significantly%0Acompromise%20solution%20quality%20to%20achieve%20guaranteed%20polynomial%20runtime.%20We%0Ashowcase%20Graph-SCP%27s%20ability%20to%20generalize%20to%20larger%20problem%20sizes%2C%20training%20on%0ASCP%20instances%20with%20up%20to%203%2C000%20subsets%20and%20testing%20on%20SCP%20instances%20with%20up%20to%0A10%2C000%20subsets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.07979v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph-SCP%253A%2520Accelerating%2520Set%2520Cover%2520Problems%2520with%2520Graph%2520Neural%2520Networks%26entry.906535625%3DZohair%2520Shafi%2520and%2520Benjamin%2520A.%2520Miller%2520and%2520Tina%2520Eliassi-Rad%2520and%2520Rajmonda%2520S.%2520Caceres%26entry.1292438233%3D%2520%2520Machine%2520learning%2520%2528ML%2529%2520approaches%2520are%2520increasingly%2520being%2520used%2520to%2520accelerate%250Acombinatorial%2520optimization%2520%2528CO%2529%2520problems.%2520We%2520investigate%2520the%2520Set%2520Cover%2520Problem%250A%2528SCP%2529%2520and%2520propose%2520Graph-SCP%252C%2520a%2520graph%2520neural%2520network%2520method%2520that%2520augments%250Aexisting%2520optimization%2520solvers%2520by%2520learning%2520to%2520identify%2520a%2520much%2520smaller%250Asub-problem%2520that%2520contains%2520the%2520solution%2520space.%2520Graph-SCP%2520uses%2520both%2520supervised%250Alearning%2520from%2520prior%2520solved%2520instances%2520and%2520unsupervised%2520learning%2520aimed%2520at%250Aminimizing%2520the%2520SCP%2520objective.%2520We%2520evaluate%2520the%2520performance%2520of%2520Graph-SCP%2520on%250Asynthetically%2520weighted%2520and%2520unweighted%2520SCP%2520instances%2520with%2520diverse%2520problem%250Acharacteristics%2520and%2520complexities%252C%2520and%2520on%2520instances%2520from%2520the%2520OR%2520Library%252C%2520a%250Acanonical%2520benchmark%2520for%2520SCP.%2520We%2520show%2520that%2520Graph-SCP%2520reduces%2520the%2520problem%2520size%2520by%250A60-80%2525%2520and%2520achieves%2520runtime%2520speedups%2520of%2520up%2520to%252010x%2520on%2520average%2520when%2520compared%2520to%250AGurobi%2520%2528a%2520state-of-the-art%2520commercial%2520solver%2529%252C%2520while%2520maintaining%2520solution%250Aquality.%2520This%2520is%2520in%2520contrast%2520to%2520fast%2520greedy%2520solutions%2520that%2520significantly%250Acompromise%2520solution%2520quality%2520to%2520achieve%2520guaranteed%2520polynomial%2520runtime.%2520We%250Ashowcase%2520Graph-SCP%2527s%2520ability%2520to%2520generalize%2520to%2520larger%2520problem%2520sizes%252C%2520training%2520on%250ASCP%2520instances%2520with%2520up%2520to%25203%252C000%2520subsets%2520and%2520testing%2520on%2520SCP%2520instances%2520with%2520up%2520to%250A10%252C000%2520subsets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.07979v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph-SCP%3A%20Accelerating%20Set%20Cover%20Problems%20with%20Graph%20Neural%20Networks&entry.906535625=Zohair%20Shafi%20and%20Benjamin%20A.%20Miller%20and%20Tina%20Eliassi-Rad%20and%20Rajmonda%20S.%20Caceres&entry.1292438233=%20%20Machine%20learning%20%28ML%29%20approaches%20are%20increasingly%20being%20used%20to%20accelerate%0Acombinatorial%20optimization%20%28CO%29%20problems.%20We%20investigate%20the%20Set%20Cover%20Problem%0A%28SCP%29%20and%20propose%20Graph-SCP%2C%20a%20graph%20neural%20network%20method%20that%20augments%0Aexisting%20optimization%20solvers%20by%20learning%20to%20identify%20a%20much%20smaller%0Asub-problem%20that%20contains%20the%20solution%20space.%20Graph-SCP%20uses%20both%20supervised%0Alearning%20from%20prior%20solved%20instances%20and%20unsupervised%20learning%20aimed%20at%0Aminimizing%20the%20SCP%20objective.%20We%20evaluate%20the%20performance%20of%20Graph-SCP%20on%0Asynthetically%20weighted%20and%20unweighted%20SCP%20instances%20with%20diverse%20problem%0Acharacteristics%20and%20complexities%2C%20and%20on%20instances%20from%20the%20OR%20Library%2C%20a%0Acanonical%20benchmark%20for%20SCP.%20We%20show%20that%20Graph-SCP%20reduces%20the%20problem%20size%20by%0A60-80%25%20and%20achieves%20runtime%20speedups%20of%20up%20to%2010x%20on%20average%20when%20compared%20to%0AGurobi%20%28a%20state-of-the-art%20commercial%20solver%29%2C%20while%20maintaining%20solution%0Aquality.%20This%20is%20in%20contrast%20to%20fast%20greedy%20solutions%20that%20significantly%0Acompromise%20solution%20quality%20to%20achieve%20guaranteed%20polynomial%20runtime.%20We%0Ashowcase%20Graph-SCP%27s%20ability%20to%20generalize%20to%20larger%20problem%20sizes%2C%20training%20on%0ASCP%20instances%20with%20up%20to%203%2C000%20subsets%20and%20testing%20on%20SCP%20instances%20with%20up%20to%0A10%2C000%20subsets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.07979v2&entry.124074799=Read"},
{"title": "MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs", "author": "Xingtong Yu and Chang Zhou and Yuan Fang and Xinming Zhang", "abstract": "  Graphs can inherently model interconnected objects on the Web, thereby\nfacilitating a series of Web applications, such as web analyzing and content\nrecommendation. Recently, Graph Neural Networks (GNNs) have emerged as a\nmainstream technique for graph representation learning. However, their efficacy\nwithin an end-to-end supervised framework is significantly tied to the\navailabilityof task-specific labels. To mitigate labeling costs and enhance\nrobustness in few-shot settings, pre-training on self-supervised tasks has\nemerged as a promising method, while prompting has been proposed to further\nnarrow the objective gap between pretext and downstream tasks. Although there\nhas been some initial exploration of prompt-based learning on graphs, they\nprimarily leverage a single pretext task, resulting in a limited subset of\ngeneral knowledge that could be learned from the pre-training data. Hence, in\nthis paper, we propose MultiGPrompt, a novel multi-task pre-training and\nprompting framework to exploit multiple pretext tasks for more comprehensive\npre-trained knowledge. First, in pre-training, we design a set of pretext\ntokens to synergize multiple pretext tasks. Second, we propose a dual-prompt\nmechanism consisting of composed and open prompts to leverage task-specific and\nglobal pre-training knowledge, to guide downstream tasks in few-shot settings.\nFinally, we conduct extensive experiments on six public datasets to evaluate\nand analyze MultiGPrompt.\n", "link": "http://arxiv.org/abs/2312.03731v7", "date": "2024-08-26", "relevancy": 1.9017, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.478}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4736}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4734}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiGPrompt%20for%20Multi-Task%20Pre-Training%20and%20Prompting%20on%20Graphs&body=Title%3A%20MultiGPrompt%20for%20Multi-Task%20Pre-Training%20and%20Prompting%20on%20Graphs%0AAuthor%3A%20Xingtong%20Yu%20and%20Chang%20Zhou%20and%20Yuan%20Fang%20and%20Xinming%20Zhang%0AAbstract%3A%20%20%20Graphs%20can%20inherently%20model%20interconnected%20objects%20on%20the%20Web%2C%20thereby%0Afacilitating%20a%20series%20of%20Web%20applications%2C%20such%20as%20web%20analyzing%20and%20content%0Arecommendation.%20Recently%2C%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20emerged%20as%20a%0Amainstream%20technique%20for%20graph%20representation%20learning.%20However%2C%20their%20efficacy%0Awithin%20an%20end-to-end%20supervised%20framework%20is%20significantly%20tied%20to%20the%0Aavailabilityof%20task-specific%20labels.%20To%20mitigate%20labeling%20costs%20and%20enhance%0Arobustness%20in%20few-shot%20settings%2C%20pre-training%20on%20self-supervised%20tasks%20has%0Aemerged%20as%20a%20promising%20method%2C%20while%20prompting%20has%20been%20proposed%20to%20further%0Anarrow%20the%20objective%20gap%20between%20pretext%20and%20downstream%20tasks.%20Although%20there%0Ahas%20been%20some%20initial%20exploration%20of%20prompt-based%20learning%20on%20graphs%2C%20they%0Aprimarily%20leverage%20a%20single%20pretext%20task%2C%20resulting%20in%20a%20limited%20subset%20of%0Ageneral%20knowledge%20that%20could%20be%20learned%20from%20the%20pre-training%20data.%20Hence%2C%20in%0Athis%20paper%2C%20we%20propose%20MultiGPrompt%2C%20a%20novel%20multi-task%20pre-training%20and%0Aprompting%20framework%20to%20exploit%20multiple%20pretext%20tasks%20for%20more%20comprehensive%0Apre-trained%20knowledge.%20First%2C%20in%20pre-training%2C%20we%20design%20a%20set%20of%20pretext%0Atokens%20to%20synergize%20multiple%20pretext%20tasks.%20Second%2C%20we%20propose%20a%20dual-prompt%0Amechanism%20consisting%20of%20composed%20and%20open%20prompts%20to%20leverage%20task-specific%20and%0Aglobal%20pre-training%20knowledge%2C%20to%20guide%20downstream%20tasks%20in%20few-shot%20settings.%0AFinally%2C%20we%20conduct%20extensive%20experiments%20on%20six%20public%20datasets%20to%20evaluate%0Aand%20analyze%20MultiGPrompt.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.03731v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiGPrompt%2520for%2520Multi-Task%2520Pre-Training%2520and%2520Prompting%2520on%2520Graphs%26entry.906535625%3DXingtong%2520Yu%2520and%2520Chang%2520Zhou%2520and%2520Yuan%2520Fang%2520and%2520Xinming%2520Zhang%26entry.1292438233%3D%2520%2520Graphs%2520can%2520inherently%2520model%2520interconnected%2520objects%2520on%2520the%2520Web%252C%2520thereby%250Afacilitating%2520a%2520series%2520of%2520Web%2520applications%252C%2520such%2520as%2520web%2520analyzing%2520and%2520content%250Arecommendation.%2520Recently%252C%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520emerged%2520as%2520a%250Amainstream%2520technique%2520for%2520graph%2520representation%2520learning.%2520However%252C%2520their%2520efficacy%250Awithin%2520an%2520end-to-end%2520supervised%2520framework%2520is%2520significantly%2520tied%2520to%2520the%250Aavailabilityof%2520task-specific%2520labels.%2520To%2520mitigate%2520labeling%2520costs%2520and%2520enhance%250Arobustness%2520in%2520few-shot%2520settings%252C%2520pre-training%2520on%2520self-supervised%2520tasks%2520has%250Aemerged%2520as%2520a%2520promising%2520method%252C%2520while%2520prompting%2520has%2520been%2520proposed%2520to%2520further%250Anarrow%2520the%2520objective%2520gap%2520between%2520pretext%2520and%2520downstream%2520tasks.%2520Although%2520there%250Ahas%2520been%2520some%2520initial%2520exploration%2520of%2520prompt-based%2520learning%2520on%2520graphs%252C%2520they%250Aprimarily%2520leverage%2520a%2520single%2520pretext%2520task%252C%2520resulting%2520in%2520a%2520limited%2520subset%2520of%250Ageneral%2520knowledge%2520that%2520could%2520be%2520learned%2520from%2520the%2520pre-training%2520data.%2520Hence%252C%2520in%250Athis%2520paper%252C%2520we%2520propose%2520MultiGPrompt%252C%2520a%2520novel%2520multi-task%2520pre-training%2520and%250Aprompting%2520framework%2520to%2520exploit%2520multiple%2520pretext%2520tasks%2520for%2520more%2520comprehensive%250Apre-trained%2520knowledge.%2520First%252C%2520in%2520pre-training%252C%2520we%2520design%2520a%2520set%2520of%2520pretext%250Atokens%2520to%2520synergize%2520multiple%2520pretext%2520tasks.%2520Second%252C%2520we%2520propose%2520a%2520dual-prompt%250Amechanism%2520consisting%2520of%2520composed%2520and%2520open%2520prompts%2520to%2520leverage%2520task-specific%2520and%250Aglobal%2520pre-training%2520knowledge%252C%2520to%2520guide%2520downstream%2520tasks%2520in%2520few-shot%2520settings.%250AFinally%252C%2520we%2520conduct%2520extensive%2520experiments%2520on%2520six%2520public%2520datasets%2520to%2520evaluate%250Aand%2520analyze%2520MultiGPrompt.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.03731v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiGPrompt%20for%20Multi-Task%20Pre-Training%20and%20Prompting%20on%20Graphs&entry.906535625=Xingtong%20Yu%20and%20Chang%20Zhou%20and%20Yuan%20Fang%20and%20Xinming%20Zhang&entry.1292438233=%20%20Graphs%20can%20inherently%20model%20interconnected%20objects%20on%20the%20Web%2C%20thereby%0Afacilitating%20a%20series%20of%20Web%20applications%2C%20such%20as%20web%20analyzing%20and%20content%0Arecommendation.%20Recently%2C%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20emerged%20as%20a%0Amainstream%20technique%20for%20graph%20representation%20learning.%20However%2C%20their%20efficacy%0Awithin%20an%20end-to-end%20supervised%20framework%20is%20significantly%20tied%20to%20the%0Aavailabilityof%20task-specific%20labels.%20To%20mitigate%20labeling%20costs%20and%20enhance%0Arobustness%20in%20few-shot%20settings%2C%20pre-training%20on%20self-supervised%20tasks%20has%0Aemerged%20as%20a%20promising%20method%2C%20while%20prompting%20has%20been%20proposed%20to%20further%0Anarrow%20the%20objective%20gap%20between%20pretext%20and%20downstream%20tasks.%20Although%20there%0Ahas%20been%20some%20initial%20exploration%20of%20prompt-based%20learning%20on%20graphs%2C%20they%0Aprimarily%20leverage%20a%20single%20pretext%20task%2C%20resulting%20in%20a%20limited%20subset%20of%0Ageneral%20knowledge%20that%20could%20be%20learned%20from%20the%20pre-training%20data.%20Hence%2C%20in%0Athis%20paper%2C%20we%20propose%20MultiGPrompt%2C%20a%20novel%20multi-task%20pre-training%20and%0Aprompting%20framework%20to%20exploit%20multiple%20pretext%20tasks%20for%20more%20comprehensive%0Apre-trained%20knowledge.%20First%2C%20in%20pre-training%2C%20we%20design%20a%20set%20of%20pretext%0Atokens%20to%20synergize%20multiple%20pretext%20tasks.%20Second%2C%20we%20propose%20a%20dual-prompt%0Amechanism%20consisting%20of%20composed%20and%20open%20prompts%20to%20leverage%20task-specific%20and%0Aglobal%20pre-training%20knowledge%2C%20to%20guide%20downstream%20tasks%20in%20few-shot%20settings.%0AFinally%2C%20we%20conduct%20extensive%20experiments%20on%20six%20public%20datasets%20to%20evaluate%0Aand%20analyze%20MultiGPrompt.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.03731v7&entry.124074799=Read"},
{"title": "Swin transformers are robust to distribution and concept drift in\n  endoscopy-based longitudinal rectal cancer assessment", "author": "Jorge Tapias Gomez and Aneesh Rangnekar and Hannah Williams and Hannah Thompson and Julio Garcia-Aguilar and Joshua Jesse Smith and Harini Veeraraghavan", "abstract": "  Endoscopic images are used at various stages of rectal cancer treatment\nstarting from cancer screening, diagnosis, during treatment to assess response\nand toxicity from treatments such as colitis, and at follow up to detect new\ntumor or local regrowth (LR). However, subjective assessment is highly variable\nand can underestimate the degree of response in some patients, subjecting them\nto unnecessary surgery, or overestimate response that places patients at risk\nof disease spread. Advances in deep learning has shown the ability to produce\nconsistent and objective response assessment for endoscopic images. However,\nmethods for detecting cancers, regrowth, and monitoring response during the\nentire course of patient treatment and follow-up are lacking. This is because,\nautomated diagnosis and rectal cancer response assessment requires methods that\nare robust to inherent imaging illumination variations and confounding\nconditions (blood, scope, blurring) present in endoscopy images as well as\nchanges to the normal lumen and tumor during treatment. Hence, a hierarchical\nshifted window (Swin) transformer was trained to distinguish rectal cancer from\nnormal lumen using endoscopy images. Swin as well as two convolutional\n(ResNet-50, WideResNet-50), and vision transformer (ViT) models were trained\nand evaluated on follow-up longitudinal images to detect LR on private dataset\nas well as on out-of-distribution (OOD) public colonoscopy datasets to detect\npre/non-cancerous polyps. Color shifts were applied using optimal transport to\nsimulate distribution shifts. Swin and ResNet models were similarly accurate in\nthe in-distribution dataset. Swin was more accurate than other methods\n(follow-up: 0.84, OOD: 0.83) even when subject to color shifts (follow-up:\n0.83, OOD: 0.87), indicating capability to provide robust performance for\nlongitudinal cancer assessment.\n", "link": "http://arxiv.org/abs/2405.03762v2", "date": "2024-08-26", "relevancy": 1.8989, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5142}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4782}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Swin%20transformers%20are%20robust%20to%20distribution%20and%20concept%20drift%20in%0A%20%20endoscopy-based%20longitudinal%20rectal%20cancer%20assessment&body=Title%3A%20Swin%20transformers%20are%20robust%20to%20distribution%20and%20concept%20drift%20in%0A%20%20endoscopy-based%20longitudinal%20rectal%20cancer%20assessment%0AAuthor%3A%20Jorge%20Tapias%20Gomez%20and%20Aneesh%20Rangnekar%20and%20Hannah%20Williams%20and%20Hannah%20Thompson%20and%20Julio%20Garcia-Aguilar%20and%20Joshua%20Jesse%20Smith%20and%20Harini%20Veeraraghavan%0AAbstract%3A%20%20%20Endoscopic%20images%20are%20used%20at%20various%20stages%20of%20rectal%20cancer%20treatment%0Astarting%20from%20cancer%20screening%2C%20diagnosis%2C%20during%20treatment%20to%20assess%20response%0Aand%20toxicity%20from%20treatments%20such%20as%20colitis%2C%20and%20at%20follow%20up%20to%20detect%20new%0Atumor%20or%20local%20regrowth%20%28LR%29.%20However%2C%20subjective%20assessment%20is%20highly%20variable%0Aand%20can%20underestimate%20the%20degree%20of%20response%20in%20some%20patients%2C%20subjecting%20them%0Ato%20unnecessary%20surgery%2C%20or%20overestimate%20response%20that%20places%20patients%20at%20risk%0Aof%20disease%20spread.%20Advances%20in%20deep%20learning%20has%20shown%20the%20ability%20to%20produce%0Aconsistent%20and%20objective%20response%20assessment%20for%20endoscopic%20images.%20However%2C%0Amethods%20for%20detecting%20cancers%2C%20regrowth%2C%20and%20monitoring%20response%20during%20the%0Aentire%20course%20of%20patient%20treatment%20and%20follow-up%20are%20lacking.%20This%20is%20because%2C%0Aautomated%20diagnosis%20and%20rectal%20cancer%20response%20assessment%20requires%20methods%20that%0Aare%20robust%20to%20inherent%20imaging%20illumination%20variations%20and%20confounding%0Aconditions%20%28blood%2C%20scope%2C%20blurring%29%20present%20in%20endoscopy%20images%20as%20well%20as%0Achanges%20to%20the%20normal%20lumen%20and%20tumor%20during%20treatment.%20Hence%2C%20a%20hierarchical%0Ashifted%20window%20%28Swin%29%20transformer%20was%20trained%20to%20distinguish%20rectal%20cancer%20from%0Anormal%20lumen%20using%20endoscopy%20images.%20Swin%20as%20well%20as%20two%20convolutional%0A%28ResNet-50%2C%20WideResNet-50%29%2C%20and%20vision%20transformer%20%28ViT%29%20models%20were%20trained%0Aand%20evaluated%20on%20follow-up%20longitudinal%20images%20to%20detect%20LR%20on%20private%20dataset%0Aas%20well%20as%20on%20out-of-distribution%20%28OOD%29%20public%20colonoscopy%20datasets%20to%20detect%0Apre/non-cancerous%20polyps.%20Color%20shifts%20were%20applied%20using%20optimal%20transport%20to%0Asimulate%20distribution%20shifts.%20Swin%20and%20ResNet%20models%20were%20similarly%20accurate%20in%0Athe%20in-distribution%20dataset.%20Swin%20was%20more%20accurate%20than%20other%20methods%0A%28follow-up%3A%200.84%2C%20OOD%3A%200.83%29%20even%20when%20subject%20to%20color%20shifts%20%28follow-up%3A%0A0.83%2C%20OOD%3A%200.87%29%2C%20indicating%20capability%20to%20provide%20robust%20performance%20for%0Alongitudinal%20cancer%20assessment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03762v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSwin%2520transformers%2520are%2520robust%2520to%2520distribution%2520and%2520concept%2520drift%2520in%250A%2520%2520endoscopy-based%2520longitudinal%2520rectal%2520cancer%2520assessment%26entry.906535625%3DJorge%2520Tapias%2520Gomez%2520and%2520Aneesh%2520Rangnekar%2520and%2520Hannah%2520Williams%2520and%2520Hannah%2520Thompson%2520and%2520Julio%2520Garcia-Aguilar%2520and%2520Joshua%2520Jesse%2520Smith%2520and%2520Harini%2520Veeraraghavan%26entry.1292438233%3D%2520%2520Endoscopic%2520images%2520are%2520used%2520at%2520various%2520stages%2520of%2520rectal%2520cancer%2520treatment%250Astarting%2520from%2520cancer%2520screening%252C%2520diagnosis%252C%2520during%2520treatment%2520to%2520assess%2520response%250Aand%2520toxicity%2520from%2520treatments%2520such%2520as%2520colitis%252C%2520and%2520at%2520follow%2520up%2520to%2520detect%2520new%250Atumor%2520or%2520local%2520regrowth%2520%2528LR%2529.%2520However%252C%2520subjective%2520assessment%2520is%2520highly%2520variable%250Aand%2520can%2520underestimate%2520the%2520degree%2520of%2520response%2520in%2520some%2520patients%252C%2520subjecting%2520them%250Ato%2520unnecessary%2520surgery%252C%2520or%2520overestimate%2520response%2520that%2520places%2520patients%2520at%2520risk%250Aof%2520disease%2520spread.%2520Advances%2520in%2520deep%2520learning%2520has%2520shown%2520the%2520ability%2520to%2520produce%250Aconsistent%2520and%2520objective%2520response%2520assessment%2520for%2520endoscopic%2520images.%2520However%252C%250Amethods%2520for%2520detecting%2520cancers%252C%2520regrowth%252C%2520and%2520monitoring%2520response%2520during%2520the%250Aentire%2520course%2520of%2520patient%2520treatment%2520and%2520follow-up%2520are%2520lacking.%2520This%2520is%2520because%252C%250Aautomated%2520diagnosis%2520and%2520rectal%2520cancer%2520response%2520assessment%2520requires%2520methods%2520that%250Aare%2520robust%2520to%2520inherent%2520imaging%2520illumination%2520variations%2520and%2520confounding%250Aconditions%2520%2528blood%252C%2520scope%252C%2520blurring%2529%2520present%2520in%2520endoscopy%2520images%2520as%2520well%2520as%250Achanges%2520to%2520the%2520normal%2520lumen%2520and%2520tumor%2520during%2520treatment.%2520Hence%252C%2520a%2520hierarchical%250Ashifted%2520window%2520%2528Swin%2529%2520transformer%2520was%2520trained%2520to%2520distinguish%2520rectal%2520cancer%2520from%250Anormal%2520lumen%2520using%2520endoscopy%2520images.%2520Swin%2520as%2520well%2520as%2520two%2520convolutional%250A%2528ResNet-50%252C%2520WideResNet-50%2529%252C%2520and%2520vision%2520transformer%2520%2528ViT%2529%2520models%2520were%2520trained%250Aand%2520evaluated%2520on%2520follow-up%2520longitudinal%2520images%2520to%2520detect%2520LR%2520on%2520private%2520dataset%250Aas%2520well%2520as%2520on%2520out-of-distribution%2520%2528OOD%2529%2520public%2520colonoscopy%2520datasets%2520to%2520detect%250Apre/non-cancerous%2520polyps.%2520Color%2520shifts%2520were%2520applied%2520using%2520optimal%2520transport%2520to%250Asimulate%2520distribution%2520shifts.%2520Swin%2520and%2520ResNet%2520models%2520were%2520similarly%2520accurate%2520in%250Athe%2520in-distribution%2520dataset.%2520Swin%2520was%2520more%2520accurate%2520than%2520other%2520methods%250A%2528follow-up%253A%25200.84%252C%2520OOD%253A%25200.83%2529%2520even%2520when%2520subject%2520to%2520color%2520shifts%2520%2528follow-up%253A%250A0.83%252C%2520OOD%253A%25200.87%2529%252C%2520indicating%2520capability%2520to%2520provide%2520robust%2520performance%2520for%250Alongitudinal%2520cancer%2520assessment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03762v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Swin%20transformers%20are%20robust%20to%20distribution%20and%20concept%20drift%20in%0A%20%20endoscopy-based%20longitudinal%20rectal%20cancer%20assessment&entry.906535625=Jorge%20Tapias%20Gomez%20and%20Aneesh%20Rangnekar%20and%20Hannah%20Williams%20and%20Hannah%20Thompson%20and%20Julio%20Garcia-Aguilar%20and%20Joshua%20Jesse%20Smith%20and%20Harini%20Veeraraghavan&entry.1292438233=%20%20Endoscopic%20images%20are%20used%20at%20various%20stages%20of%20rectal%20cancer%20treatment%0Astarting%20from%20cancer%20screening%2C%20diagnosis%2C%20during%20treatment%20to%20assess%20response%0Aand%20toxicity%20from%20treatments%20such%20as%20colitis%2C%20and%20at%20follow%20up%20to%20detect%20new%0Atumor%20or%20local%20regrowth%20%28LR%29.%20However%2C%20subjective%20assessment%20is%20highly%20variable%0Aand%20can%20underestimate%20the%20degree%20of%20response%20in%20some%20patients%2C%20subjecting%20them%0Ato%20unnecessary%20surgery%2C%20or%20overestimate%20response%20that%20places%20patients%20at%20risk%0Aof%20disease%20spread.%20Advances%20in%20deep%20learning%20has%20shown%20the%20ability%20to%20produce%0Aconsistent%20and%20objective%20response%20assessment%20for%20endoscopic%20images.%20However%2C%0Amethods%20for%20detecting%20cancers%2C%20regrowth%2C%20and%20monitoring%20response%20during%20the%0Aentire%20course%20of%20patient%20treatment%20and%20follow-up%20are%20lacking.%20This%20is%20because%2C%0Aautomated%20diagnosis%20and%20rectal%20cancer%20response%20assessment%20requires%20methods%20that%0Aare%20robust%20to%20inherent%20imaging%20illumination%20variations%20and%20confounding%0Aconditions%20%28blood%2C%20scope%2C%20blurring%29%20present%20in%20endoscopy%20images%20as%20well%20as%0Achanges%20to%20the%20normal%20lumen%20and%20tumor%20during%20treatment.%20Hence%2C%20a%20hierarchical%0Ashifted%20window%20%28Swin%29%20transformer%20was%20trained%20to%20distinguish%20rectal%20cancer%20from%0Anormal%20lumen%20using%20endoscopy%20images.%20Swin%20as%20well%20as%20two%20convolutional%0A%28ResNet-50%2C%20WideResNet-50%29%2C%20and%20vision%20transformer%20%28ViT%29%20models%20were%20trained%0Aand%20evaluated%20on%20follow-up%20longitudinal%20images%20to%20detect%20LR%20on%20private%20dataset%0Aas%20well%20as%20on%20out-of-distribution%20%28OOD%29%20public%20colonoscopy%20datasets%20to%20detect%0Apre/non-cancerous%20polyps.%20Color%20shifts%20were%20applied%20using%20optimal%20transport%20to%0Asimulate%20distribution%20shifts.%20Swin%20and%20ResNet%20models%20were%20similarly%20accurate%20in%0Athe%20in-distribution%20dataset.%20Swin%20was%20more%20accurate%20than%20other%20methods%0A%28follow-up%3A%200.84%2C%20OOD%3A%200.83%29%20even%20when%20subject%20to%20color%20shifts%20%28follow-up%3A%0A0.83%2C%20OOD%3A%200.87%29%2C%20indicating%20capability%20to%20provide%20robust%20performance%20for%0Alongitudinal%20cancer%20assessment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03762v2&entry.124074799=Read"},
{"title": "Optimistic Online Non-stochastic Control via FTRL", "author": "Naram Mhaisen and George Iosifidis", "abstract": "  This paper brings the concept of ``optimism\" to the new and promising\nframework of online Non-stochastic Control (NSC). Namely, we study how NSC can\nbenefit from a prediction oracle of unknown quality responsible for forecasting\nfuture costs. The posed problem is first reduced to an optimistic learning with\ndelayed feedback problem, which is handled through the Optimistic Follow the\nRegularized Leader (OFTRL) algorithmic family. This reduction enables the\ndesign of \\texttt{OptFTRL-C}, the first Disturbance Action Controller (DAC)\nwith optimistic policy regret bounds. These new bounds are commensurate with\nthe oracle's accuracy, ranging from $\\mathcal{O}(1)$ for perfect predictions to\nthe order-optimal $\\mathcal{O}(\\sqrt{T})$ even when all predictions fail. By\naddressing the challenge of incorporating untrusted predictions into online\ncontrol, this work contributes to the advancement of the NSC framework and\npaves the way toward effective and robust learning-based controllers.\n", "link": "http://arxiv.org/abs/2404.03309v2", "date": "2024-08-26", "relevancy": 1.8962, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5142}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4604}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4393}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimistic%20Online%20Non-stochastic%20Control%20via%20FTRL&body=Title%3A%20Optimistic%20Online%20Non-stochastic%20Control%20via%20FTRL%0AAuthor%3A%20Naram%20Mhaisen%20and%20George%20Iosifidis%0AAbstract%3A%20%20%20This%20paper%20brings%20the%20concept%20of%20%60%60optimism%22%20to%20the%20new%20and%20promising%0Aframework%20of%20online%20Non-stochastic%20Control%20%28NSC%29.%20Namely%2C%20we%20study%20how%20NSC%20can%0Abenefit%20from%20a%20prediction%20oracle%20of%20unknown%20quality%20responsible%20for%20forecasting%0Afuture%20costs.%20The%20posed%20problem%20is%20first%20reduced%20to%20an%20optimistic%20learning%20with%0Adelayed%20feedback%20problem%2C%20which%20is%20handled%20through%20the%20Optimistic%20Follow%20the%0ARegularized%20Leader%20%28OFTRL%29%20algorithmic%20family.%20This%20reduction%20enables%20the%0Adesign%20of%20%5Ctexttt%7BOptFTRL-C%7D%2C%20the%20first%20Disturbance%20Action%20Controller%20%28DAC%29%0Awith%20optimistic%20policy%20regret%20bounds.%20These%20new%20bounds%20are%20commensurate%20with%0Athe%20oracle%27s%20accuracy%2C%20ranging%20from%20%24%5Cmathcal%7BO%7D%281%29%24%20for%20perfect%20predictions%20to%0Athe%20order-optimal%20%24%5Cmathcal%7BO%7D%28%5Csqrt%7BT%7D%29%24%20even%20when%20all%20predictions%20fail.%20By%0Aaddressing%20the%20challenge%20of%20incorporating%20untrusted%20predictions%20into%20online%0Acontrol%2C%20this%20work%20contributes%20to%20the%20advancement%20of%20the%20NSC%20framework%20and%0Apaves%20the%20way%20toward%20effective%20and%20robust%20learning-based%20controllers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03309v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimistic%2520Online%2520Non-stochastic%2520Control%2520via%2520FTRL%26entry.906535625%3DNaram%2520Mhaisen%2520and%2520George%2520Iosifidis%26entry.1292438233%3D%2520%2520This%2520paper%2520brings%2520the%2520concept%2520of%2520%2560%2560optimism%2522%2520to%2520the%2520new%2520and%2520promising%250Aframework%2520of%2520online%2520Non-stochastic%2520Control%2520%2528NSC%2529.%2520Namely%252C%2520we%2520study%2520how%2520NSC%2520can%250Abenefit%2520from%2520a%2520prediction%2520oracle%2520of%2520unknown%2520quality%2520responsible%2520for%2520forecasting%250Afuture%2520costs.%2520The%2520posed%2520problem%2520is%2520first%2520reduced%2520to%2520an%2520optimistic%2520learning%2520with%250Adelayed%2520feedback%2520problem%252C%2520which%2520is%2520handled%2520through%2520the%2520Optimistic%2520Follow%2520the%250ARegularized%2520Leader%2520%2528OFTRL%2529%2520algorithmic%2520family.%2520This%2520reduction%2520enables%2520the%250Adesign%2520of%2520%255Ctexttt%257BOptFTRL-C%257D%252C%2520the%2520first%2520Disturbance%2520Action%2520Controller%2520%2528DAC%2529%250Awith%2520optimistic%2520policy%2520regret%2520bounds.%2520These%2520new%2520bounds%2520are%2520commensurate%2520with%250Athe%2520oracle%2527s%2520accuracy%252C%2520ranging%2520from%2520%2524%255Cmathcal%257BO%257D%25281%2529%2524%2520for%2520perfect%2520predictions%2520to%250Athe%2520order-optimal%2520%2524%255Cmathcal%257BO%257D%2528%255Csqrt%257BT%257D%2529%2524%2520even%2520when%2520all%2520predictions%2520fail.%2520By%250Aaddressing%2520the%2520challenge%2520of%2520incorporating%2520untrusted%2520predictions%2520into%2520online%250Acontrol%252C%2520this%2520work%2520contributes%2520to%2520the%2520advancement%2520of%2520the%2520NSC%2520framework%2520and%250Apaves%2520the%2520way%2520toward%2520effective%2520and%2520robust%2520learning-based%2520controllers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.03309v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimistic%20Online%20Non-stochastic%20Control%20via%20FTRL&entry.906535625=Naram%20Mhaisen%20and%20George%20Iosifidis&entry.1292438233=%20%20This%20paper%20brings%20the%20concept%20of%20%60%60optimism%22%20to%20the%20new%20and%20promising%0Aframework%20of%20online%20Non-stochastic%20Control%20%28NSC%29.%20Namely%2C%20we%20study%20how%20NSC%20can%0Abenefit%20from%20a%20prediction%20oracle%20of%20unknown%20quality%20responsible%20for%20forecasting%0Afuture%20costs.%20The%20posed%20problem%20is%20first%20reduced%20to%20an%20optimistic%20learning%20with%0Adelayed%20feedback%20problem%2C%20which%20is%20handled%20through%20the%20Optimistic%20Follow%20the%0ARegularized%20Leader%20%28OFTRL%29%20algorithmic%20family.%20This%20reduction%20enables%20the%0Adesign%20of%20%5Ctexttt%7BOptFTRL-C%7D%2C%20the%20first%20Disturbance%20Action%20Controller%20%28DAC%29%0Awith%20optimistic%20policy%20regret%20bounds.%20These%20new%20bounds%20are%20commensurate%20with%0Athe%20oracle%27s%20accuracy%2C%20ranging%20from%20%24%5Cmathcal%7BO%7D%281%29%24%20for%20perfect%20predictions%20to%0Athe%20order-optimal%20%24%5Cmathcal%7BO%7D%28%5Csqrt%7BT%7D%29%24%20even%20when%20all%20predictions%20fail.%20By%0Aaddressing%20the%20challenge%20of%20incorporating%20untrusted%20predictions%20into%20online%0Acontrol%2C%20this%20work%20contributes%20to%20the%20advancement%20of%20the%20NSC%20framework%20and%0Apaves%20the%20way%20toward%20effective%20and%20robust%20learning-based%20controllers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03309v2&entry.124074799=Read"},
{"title": "Rethinking Knowledge Transfer in Learning Using Privileged Information", "author": "Danil Provodin and Bram van den Akker and Christina Katsimerou and Maurits Kaptein and Mykola Pechenizkiy", "abstract": "  In supervised machine learning, privileged information (PI) is information\nthat is unavailable at inference, but is accessible during training time.\nResearch on learning using privileged information (LUPI) aims to transfer the\nknowledge captured in PI onto a model that can perform inference without PI. It\nseems that this extra bit of information ought to make the resulting model\nbetter. However, finding conclusive theoretical or empirical evidence that\nsupports the ability to transfer knowledge using PI has been challenging. In\nthis paper, we critically examine the assumptions underlying existing\ntheoretical analyses and argue that there is little theoretical justification\nfor when LUPI should work. We analyze LUPI methods and reveal that apparent\nimprovements in empirical risk of existing research may not directly result\nfrom PI. Instead, these improvements often stem from dataset anomalies or\nmodifications in model design misguidedly attributed to PI. Our experiments for\na wide variety of application domains further demonstrate that state-of-the-art\nLUPI approaches fail to effectively transfer knowledge from PI. Thus, we\nadvocate for practitioners to exercise caution when working with PI to avoid\nunintended inductive biases.\n", "link": "http://arxiv.org/abs/2408.14319v1", "date": "2024-08-26", "relevancy": 1.8946, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4806}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4688}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Knowledge%20Transfer%20in%20Learning%20Using%20Privileged%20Information&body=Title%3A%20Rethinking%20Knowledge%20Transfer%20in%20Learning%20Using%20Privileged%20Information%0AAuthor%3A%20Danil%20Provodin%20and%20Bram%20van%20den%20Akker%20and%20Christina%20Katsimerou%20and%20Maurits%20Kaptein%20and%20Mykola%20Pechenizkiy%0AAbstract%3A%20%20%20In%20supervised%20machine%20learning%2C%20privileged%20information%20%28PI%29%20is%20information%0Athat%20is%20unavailable%20at%20inference%2C%20but%20is%20accessible%20during%20training%20time.%0AResearch%20on%20learning%20using%20privileged%20information%20%28LUPI%29%20aims%20to%20transfer%20the%0Aknowledge%20captured%20in%20PI%20onto%20a%20model%20that%20can%20perform%20inference%20without%20PI.%20It%0Aseems%20that%20this%20extra%20bit%20of%20information%20ought%20to%20make%20the%20resulting%20model%0Abetter.%20However%2C%20finding%20conclusive%20theoretical%20or%20empirical%20evidence%20that%0Asupports%20the%20ability%20to%20transfer%20knowledge%20using%20PI%20has%20been%20challenging.%20In%0Athis%20paper%2C%20we%20critically%20examine%20the%20assumptions%20underlying%20existing%0Atheoretical%20analyses%20and%20argue%20that%20there%20is%20little%20theoretical%20justification%0Afor%20when%20LUPI%20should%20work.%20We%20analyze%20LUPI%20methods%20and%20reveal%20that%20apparent%0Aimprovements%20in%20empirical%20risk%20of%20existing%20research%20may%20not%20directly%20result%0Afrom%20PI.%20Instead%2C%20these%20improvements%20often%20stem%20from%20dataset%20anomalies%20or%0Amodifications%20in%20model%20design%20misguidedly%20attributed%20to%20PI.%20Our%20experiments%20for%0Aa%20wide%20variety%20of%20application%20domains%20further%20demonstrate%20that%20state-of-the-art%0ALUPI%20approaches%20fail%20to%20effectively%20transfer%20knowledge%20from%20PI.%20Thus%2C%20we%0Aadvocate%20for%20practitioners%20to%20exercise%20caution%20when%20working%20with%20PI%20to%20avoid%0Aunintended%20inductive%20biases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14319v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Knowledge%2520Transfer%2520in%2520Learning%2520Using%2520Privileged%2520Information%26entry.906535625%3DDanil%2520Provodin%2520and%2520Bram%2520van%2520den%2520Akker%2520and%2520Christina%2520Katsimerou%2520and%2520Maurits%2520Kaptein%2520and%2520Mykola%2520Pechenizkiy%26entry.1292438233%3D%2520%2520In%2520supervised%2520machine%2520learning%252C%2520privileged%2520information%2520%2528PI%2529%2520is%2520information%250Athat%2520is%2520unavailable%2520at%2520inference%252C%2520but%2520is%2520accessible%2520during%2520training%2520time.%250AResearch%2520on%2520learning%2520using%2520privileged%2520information%2520%2528LUPI%2529%2520aims%2520to%2520transfer%2520the%250Aknowledge%2520captured%2520in%2520PI%2520onto%2520a%2520model%2520that%2520can%2520perform%2520inference%2520without%2520PI.%2520It%250Aseems%2520that%2520this%2520extra%2520bit%2520of%2520information%2520ought%2520to%2520make%2520the%2520resulting%2520model%250Abetter.%2520However%252C%2520finding%2520conclusive%2520theoretical%2520or%2520empirical%2520evidence%2520that%250Asupports%2520the%2520ability%2520to%2520transfer%2520knowledge%2520using%2520PI%2520has%2520been%2520challenging.%2520In%250Athis%2520paper%252C%2520we%2520critically%2520examine%2520the%2520assumptions%2520underlying%2520existing%250Atheoretical%2520analyses%2520and%2520argue%2520that%2520there%2520is%2520little%2520theoretical%2520justification%250Afor%2520when%2520LUPI%2520should%2520work.%2520We%2520analyze%2520LUPI%2520methods%2520and%2520reveal%2520that%2520apparent%250Aimprovements%2520in%2520empirical%2520risk%2520of%2520existing%2520research%2520may%2520not%2520directly%2520result%250Afrom%2520PI.%2520Instead%252C%2520these%2520improvements%2520often%2520stem%2520from%2520dataset%2520anomalies%2520or%250Amodifications%2520in%2520model%2520design%2520misguidedly%2520attributed%2520to%2520PI.%2520Our%2520experiments%2520for%250Aa%2520wide%2520variety%2520of%2520application%2520domains%2520further%2520demonstrate%2520that%2520state-of-the-art%250ALUPI%2520approaches%2520fail%2520to%2520effectively%2520transfer%2520knowledge%2520from%2520PI.%2520Thus%252C%2520we%250Aadvocate%2520for%2520practitioners%2520to%2520exercise%2520caution%2520when%2520working%2520with%2520PI%2520to%2520avoid%250Aunintended%2520inductive%2520biases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14319v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Knowledge%20Transfer%20in%20Learning%20Using%20Privileged%20Information&entry.906535625=Danil%20Provodin%20and%20Bram%20van%20den%20Akker%20and%20Christina%20Katsimerou%20and%20Maurits%20Kaptein%20and%20Mykola%20Pechenizkiy&entry.1292438233=%20%20In%20supervised%20machine%20learning%2C%20privileged%20information%20%28PI%29%20is%20information%0Athat%20is%20unavailable%20at%20inference%2C%20but%20is%20accessible%20during%20training%20time.%0AResearch%20on%20learning%20using%20privileged%20information%20%28LUPI%29%20aims%20to%20transfer%20the%0Aknowledge%20captured%20in%20PI%20onto%20a%20model%20that%20can%20perform%20inference%20without%20PI.%20It%0Aseems%20that%20this%20extra%20bit%20of%20information%20ought%20to%20make%20the%20resulting%20model%0Abetter.%20However%2C%20finding%20conclusive%20theoretical%20or%20empirical%20evidence%20that%0Asupports%20the%20ability%20to%20transfer%20knowledge%20using%20PI%20has%20been%20challenging.%20In%0Athis%20paper%2C%20we%20critically%20examine%20the%20assumptions%20underlying%20existing%0Atheoretical%20analyses%20and%20argue%20that%20there%20is%20little%20theoretical%20justification%0Afor%20when%20LUPI%20should%20work.%20We%20analyze%20LUPI%20methods%20and%20reveal%20that%20apparent%0Aimprovements%20in%20empirical%20risk%20of%20existing%20research%20may%20not%20directly%20result%0Afrom%20PI.%20Instead%2C%20these%20improvements%20often%20stem%20from%20dataset%20anomalies%20or%0Amodifications%20in%20model%20design%20misguidedly%20attributed%20to%20PI.%20Our%20experiments%20for%0Aa%20wide%20variety%20of%20application%20domains%20further%20demonstrate%20that%20state-of-the-art%0ALUPI%20approaches%20fail%20to%20effectively%20transfer%20knowledge%20from%20PI.%20Thus%2C%20we%0Aadvocate%20for%20practitioners%20to%20exercise%20caution%20when%20working%20with%20PI%20to%20avoid%0Aunintended%20inductive%20biases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14319v1&entry.124074799=Read"},
{"title": "Deep learning-based ecological analysis of camera trap images is\n  impacted by training data quality and size", "author": "Omiros Pantazis and Peggy Bevan and Holly Pringle and Guilherme Braga Ferreira and Daniel J. Ingram and Emily Madsen and Liam Thomas and Dol Raj Thanet and Thakur Silwal and Santosh Rayamajhi and Gabriel Brostow and Oisin Mac Aodha and Kate E. Jones", "abstract": "  Large wildlife image collections from camera traps are crucial for\nbiodiversity monitoring, offering insights into species richness, occupancy,\nand activity patterns. However, manual processing of these data is\ntime-consuming, hindering analytical processes. To address this, deep neural\nnetworks have been widely adopted to automate image analysis. Despite their\ngrowing use, the impact of model training decisions on downstream ecological\nmetrics remains unclear. Here, we analyse camera trap data from an African\nsavannah and an Asian sub-tropical dry forest to compare key ecological metrics\nderived from expert-generated species identifications with those generated from\ndeep neural networks. We assess the impact of model architecture, training data\nnoise, and dataset size on ecological metrics, including species richness,\noccupancy, and activity patterns. Our results show that while model\narchitecture has minimal impact, large amounts of noise and reduced dataset\nsize significantly affect these metrics. Nonetheless, estimated ecological\nmetrics are resilient to considerable noise, tolerating up to 10% error in\nspecies labels and a 50% reduction in training set size without changing\nsignificantly. We also highlight that conventional metrics like classification\nerror may not always be representative of a model's ability to accurately\nmeasure ecological metrics. We conclude that ecological metrics derived from\ndeep neural network predictions closely match those calculated from expert\nlabels and remain robust to variations in the factors explored. However,\ntraining decisions for deep neural networks can impact downstream ecological\nanalysis. Therefore, practitioners should prioritize creating large, clean\ntraining sets and evaluate deep neural network solutions based on their ability\nto measure the ecological metrics of interest.\n", "link": "http://arxiv.org/abs/2408.14348v1", "date": "2024-08-26", "relevancy": 1.8944, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.481}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4787}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20learning-based%20ecological%20analysis%20of%20camera%20trap%20images%20is%0A%20%20impacted%20by%20training%20data%20quality%20and%20size&body=Title%3A%20Deep%20learning-based%20ecological%20analysis%20of%20camera%20trap%20images%20is%0A%20%20impacted%20by%20training%20data%20quality%20and%20size%0AAuthor%3A%20Omiros%20Pantazis%20and%20Peggy%20Bevan%20and%20Holly%20Pringle%20and%20Guilherme%20Braga%20Ferreira%20and%20Daniel%20J.%20Ingram%20and%20Emily%20Madsen%20and%20Liam%20Thomas%20and%20Dol%20Raj%20Thanet%20and%20Thakur%20Silwal%20and%20Santosh%20Rayamajhi%20and%20Gabriel%20Brostow%20and%20Oisin%20Mac%20Aodha%20and%20Kate%20E.%20Jones%0AAbstract%3A%20%20%20Large%20wildlife%20image%20collections%20from%20camera%20traps%20are%20crucial%20for%0Abiodiversity%20monitoring%2C%20offering%20insights%20into%20species%20richness%2C%20occupancy%2C%0Aand%20activity%20patterns.%20However%2C%20manual%20processing%20of%20these%20data%20is%0Atime-consuming%2C%20hindering%20analytical%20processes.%20To%20address%20this%2C%20deep%20neural%0Anetworks%20have%20been%20widely%20adopted%20to%20automate%20image%20analysis.%20Despite%20their%0Agrowing%20use%2C%20the%20impact%20of%20model%20training%20decisions%20on%20downstream%20ecological%0Ametrics%20remains%20unclear.%20Here%2C%20we%20analyse%20camera%20trap%20data%20from%20an%20African%0Asavannah%20and%20an%20Asian%20sub-tropical%20dry%20forest%20to%20compare%20key%20ecological%20metrics%0Aderived%20from%20expert-generated%20species%20identifications%20with%20those%20generated%20from%0Adeep%20neural%20networks.%20We%20assess%20the%20impact%20of%20model%20architecture%2C%20training%20data%0Anoise%2C%20and%20dataset%20size%20on%20ecological%20metrics%2C%20including%20species%20richness%2C%0Aoccupancy%2C%20and%20activity%20patterns.%20Our%20results%20show%20that%20while%20model%0Aarchitecture%20has%20minimal%20impact%2C%20large%20amounts%20of%20noise%20and%20reduced%20dataset%0Asize%20significantly%20affect%20these%20metrics.%20Nonetheless%2C%20estimated%20ecological%0Ametrics%20are%20resilient%20to%20considerable%20noise%2C%20tolerating%20up%20to%2010%25%20error%20in%0Aspecies%20labels%20and%20a%2050%25%20reduction%20in%20training%20set%20size%20without%20changing%0Asignificantly.%20We%20also%20highlight%20that%20conventional%20metrics%20like%20classification%0Aerror%20may%20not%20always%20be%20representative%20of%20a%20model%27s%20ability%20to%20accurately%0Ameasure%20ecological%20metrics.%20We%20conclude%20that%20ecological%20metrics%20derived%20from%0Adeep%20neural%20network%20predictions%20closely%20match%20those%20calculated%20from%20expert%0Alabels%20and%20remain%20robust%20to%20variations%20in%20the%20factors%20explored.%20However%2C%0Atraining%20decisions%20for%20deep%20neural%20networks%20can%20impact%20downstream%20ecological%0Aanalysis.%20Therefore%2C%20practitioners%20should%20prioritize%20creating%20large%2C%20clean%0Atraining%20sets%20and%20evaluate%20deep%20neural%20network%20solutions%20based%20on%20their%20ability%0Ato%20measure%20the%20ecological%20metrics%20of%20interest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14348v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520learning-based%2520ecological%2520analysis%2520of%2520camera%2520trap%2520images%2520is%250A%2520%2520impacted%2520by%2520training%2520data%2520quality%2520and%2520size%26entry.906535625%3DOmiros%2520Pantazis%2520and%2520Peggy%2520Bevan%2520and%2520Holly%2520Pringle%2520and%2520Guilherme%2520Braga%2520Ferreira%2520and%2520Daniel%2520J.%2520Ingram%2520and%2520Emily%2520Madsen%2520and%2520Liam%2520Thomas%2520and%2520Dol%2520Raj%2520Thanet%2520and%2520Thakur%2520Silwal%2520and%2520Santosh%2520Rayamajhi%2520and%2520Gabriel%2520Brostow%2520and%2520Oisin%2520Mac%2520Aodha%2520and%2520Kate%2520E.%2520Jones%26entry.1292438233%3D%2520%2520Large%2520wildlife%2520image%2520collections%2520from%2520camera%2520traps%2520are%2520crucial%2520for%250Abiodiversity%2520monitoring%252C%2520offering%2520insights%2520into%2520species%2520richness%252C%2520occupancy%252C%250Aand%2520activity%2520patterns.%2520However%252C%2520manual%2520processing%2520of%2520these%2520data%2520is%250Atime-consuming%252C%2520hindering%2520analytical%2520processes.%2520To%2520address%2520this%252C%2520deep%2520neural%250Anetworks%2520have%2520been%2520widely%2520adopted%2520to%2520automate%2520image%2520analysis.%2520Despite%2520their%250Agrowing%2520use%252C%2520the%2520impact%2520of%2520model%2520training%2520decisions%2520on%2520downstream%2520ecological%250Ametrics%2520remains%2520unclear.%2520Here%252C%2520we%2520analyse%2520camera%2520trap%2520data%2520from%2520an%2520African%250Asavannah%2520and%2520an%2520Asian%2520sub-tropical%2520dry%2520forest%2520to%2520compare%2520key%2520ecological%2520metrics%250Aderived%2520from%2520expert-generated%2520species%2520identifications%2520with%2520those%2520generated%2520from%250Adeep%2520neural%2520networks.%2520We%2520assess%2520the%2520impact%2520of%2520model%2520architecture%252C%2520training%2520data%250Anoise%252C%2520and%2520dataset%2520size%2520on%2520ecological%2520metrics%252C%2520including%2520species%2520richness%252C%250Aoccupancy%252C%2520and%2520activity%2520patterns.%2520Our%2520results%2520show%2520that%2520while%2520model%250Aarchitecture%2520has%2520minimal%2520impact%252C%2520large%2520amounts%2520of%2520noise%2520and%2520reduced%2520dataset%250Asize%2520significantly%2520affect%2520these%2520metrics.%2520Nonetheless%252C%2520estimated%2520ecological%250Ametrics%2520are%2520resilient%2520to%2520considerable%2520noise%252C%2520tolerating%2520up%2520to%252010%2525%2520error%2520in%250Aspecies%2520labels%2520and%2520a%252050%2525%2520reduction%2520in%2520training%2520set%2520size%2520without%2520changing%250Asignificantly.%2520We%2520also%2520highlight%2520that%2520conventional%2520metrics%2520like%2520classification%250Aerror%2520may%2520not%2520always%2520be%2520representative%2520of%2520a%2520model%2527s%2520ability%2520to%2520accurately%250Ameasure%2520ecological%2520metrics.%2520We%2520conclude%2520that%2520ecological%2520metrics%2520derived%2520from%250Adeep%2520neural%2520network%2520predictions%2520closely%2520match%2520those%2520calculated%2520from%2520expert%250Alabels%2520and%2520remain%2520robust%2520to%2520variations%2520in%2520the%2520factors%2520explored.%2520However%252C%250Atraining%2520decisions%2520for%2520deep%2520neural%2520networks%2520can%2520impact%2520downstream%2520ecological%250Aanalysis.%2520Therefore%252C%2520practitioners%2520should%2520prioritize%2520creating%2520large%252C%2520clean%250Atraining%2520sets%2520and%2520evaluate%2520deep%2520neural%2520network%2520solutions%2520based%2520on%2520their%2520ability%250Ato%2520measure%2520the%2520ecological%2520metrics%2520of%2520interest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14348v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20learning-based%20ecological%20analysis%20of%20camera%20trap%20images%20is%0A%20%20impacted%20by%20training%20data%20quality%20and%20size&entry.906535625=Omiros%20Pantazis%20and%20Peggy%20Bevan%20and%20Holly%20Pringle%20and%20Guilherme%20Braga%20Ferreira%20and%20Daniel%20J.%20Ingram%20and%20Emily%20Madsen%20and%20Liam%20Thomas%20and%20Dol%20Raj%20Thanet%20and%20Thakur%20Silwal%20and%20Santosh%20Rayamajhi%20and%20Gabriel%20Brostow%20and%20Oisin%20Mac%20Aodha%20and%20Kate%20E.%20Jones&entry.1292438233=%20%20Large%20wildlife%20image%20collections%20from%20camera%20traps%20are%20crucial%20for%0Abiodiversity%20monitoring%2C%20offering%20insights%20into%20species%20richness%2C%20occupancy%2C%0Aand%20activity%20patterns.%20However%2C%20manual%20processing%20of%20these%20data%20is%0Atime-consuming%2C%20hindering%20analytical%20processes.%20To%20address%20this%2C%20deep%20neural%0Anetworks%20have%20been%20widely%20adopted%20to%20automate%20image%20analysis.%20Despite%20their%0Agrowing%20use%2C%20the%20impact%20of%20model%20training%20decisions%20on%20downstream%20ecological%0Ametrics%20remains%20unclear.%20Here%2C%20we%20analyse%20camera%20trap%20data%20from%20an%20African%0Asavannah%20and%20an%20Asian%20sub-tropical%20dry%20forest%20to%20compare%20key%20ecological%20metrics%0Aderived%20from%20expert-generated%20species%20identifications%20with%20those%20generated%20from%0Adeep%20neural%20networks.%20We%20assess%20the%20impact%20of%20model%20architecture%2C%20training%20data%0Anoise%2C%20and%20dataset%20size%20on%20ecological%20metrics%2C%20including%20species%20richness%2C%0Aoccupancy%2C%20and%20activity%20patterns.%20Our%20results%20show%20that%20while%20model%0Aarchitecture%20has%20minimal%20impact%2C%20large%20amounts%20of%20noise%20and%20reduced%20dataset%0Asize%20significantly%20affect%20these%20metrics.%20Nonetheless%2C%20estimated%20ecological%0Ametrics%20are%20resilient%20to%20considerable%20noise%2C%20tolerating%20up%20to%2010%25%20error%20in%0Aspecies%20labels%20and%20a%2050%25%20reduction%20in%20training%20set%20size%20without%20changing%0Asignificantly.%20We%20also%20highlight%20that%20conventional%20metrics%20like%20classification%0Aerror%20may%20not%20always%20be%20representative%20of%20a%20model%27s%20ability%20to%20accurately%0Ameasure%20ecological%20metrics.%20We%20conclude%20that%20ecological%20metrics%20derived%20from%0Adeep%20neural%20network%20predictions%20closely%20match%20those%20calculated%20from%20expert%0Alabels%20and%20remain%20robust%20to%20variations%20in%20the%20factors%20explored.%20However%2C%0Atraining%20decisions%20for%20deep%20neural%20networks%20can%20impact%20downstream%20ecological%0Aanalysis.%20Therefore%2C%20practitioners%20should%20prioritize%20creating%20large%2C%20clean%0Atraining%20sets%20and%20evaluate%20deep%20neural%20network%20solutions%20based%20on%20their%20ability%0Ato%20measure%20the%20ecological%20metrics%20of%20interest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14348v1&entry.124074799=Read"},
{"title": "Bayesian neural networks via MCMC: a Python-based tutorial", "author": "Rohitash Chandra and Joshua Simmons", "abstract": "  Bayesian inference provides a methodology for parameter estimation and\nuncertainty quantification in machine learning and deep learning methods.\nVariational inference and Markov Chain Monte-Carlo (MCMC) sampling methods are\nused to implement Bayesian inference. In the past three decades, MCMC sampling\nmethods have faced some challenges in being adapted to larger models (such as\nin deep learning) and big data problems. Advanced proposal distributions that\nincorporate gradients, such as a Langevin proposal distribution, provide a\nmeans to address some of the limitations of MCMC sampling for Bayesian neural\nnetworks. Furthermore, MCMC methods have typically been constrained to\nstatisticians and currently not well-known among deep learning researchers. We\npresent a tutorial for MCMC methods that covers simple Bayesian linear and\nlogistic models, and Bayesian neural networks. The aim of this tutorial is to\nbridge the gap between theory and implementation via coding, given a general\nsparsity of libraries and tutorials to this end. This tutorial provides code in\nPython with data and instructions that enable their use and extension. We\nprovide results for some benchmark problems showing the strengths and\nweaknesses of implementing the respective Bayesian models via MCMC. We\nhighlight the challenges in sampling multi-modal posterior distributions for\nthe case of Bayesian neural networks and the need for further improvement of\nconvergence diagnosis methods.\n", "link": "http://arxiv.org/abs/2304.02595v3", "date": "2024-08-26", "relevancy": 1.8943, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.546}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4831}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4351}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20neural%20networks%20via%20MCMC%3A%20a%20Python-based%20tutorial&body=Title%3A%20Bayesian%20neural%20networks%20via%20MCMC%3A%20a%20Python-based%20tutorial%0AAuthor%3A%20Rohitash%20Chandra%20and%20Joshua%20Simmons%0AAbstract%3A%20%20%20Bayesian%20inference%20provides%20a%20methodology%20for%20parameter%20estimation%20and%0Auncertainty%20quantification%20in%20machine%20learning%20and%20deep%20learning%20methods.%0AVariational%20inference%20and%20Markov%20Chain%20Monte-Carlo%20%28MCMC%29%20sampling%20methods%20are%0Aused%20to%20implement%20Bayesian%20inference.%20In%20the%20past%20three%20decades%2C%20MCMC%20sampling%0Amethods%20have%20faced%20some%20challenges%20in%20being%20adapted%20to%20larger%20models%20%28such%20as%0Ain%20deep%20learning%29%20and%20big%20data%20problems.%20Advanced%20proposal%20distributions%20that%0Aincorporate%20gradients%2C%20such%20as%20a%20Langevin%20proposal%20distribution%2C%20provide%20a%0Ameans%20to%20address%20some%20of%20the%20limitations%20of%20MCMC%20sampling%20for%20Bayesian%20neural%0Anetworks.%20Furthermore%2C%20MCMC%20methods%20have%20typically%20been%20constrained%20to%0Astatisticians%20and%20currently%20not%20well-known%20among%20deep%20learning%20researchers.%20We%0Apresent%20a%20tutorial%20for%20MCMC%20methods%20that%20covers%20simple%20Bayesian%20linear%20and%0Alogistic%20models%2C%20and%20Bayesian%20neural%20networks.%20The%20aim%20of%20this%20tutorial%20is%20to%0Abridge%20the%20gap%20between%20theory%20and%20implementation%20via%20coding%2C%20given%20a%20general%0Asparsity%20of%20libraries%20and%20tutorials%20to%20this%20end.%20This%20tutorial%20provides%20code%20in%0APython%20with%20data%20and%20instructions%20that%20enable%20their%20use%20and%20extension.%20We%0Aprovide%20results%20for%20some%20benchmark%20problems%20showing%20the%20strengths%20and%0Aweaknesses%20of%20implementing%20the%20respective%20Bayesian%20models%20via%20MCMC.%20We%0Ahighlight%20the%20challenges%20in%20sampling%20multi-modal%20posterior%20distributions%20for%0Athe%20case%20of%20Bayesian%20neural%20networks%20and%20the%20need%20for%20further%20improvement%20of%0Aconvergence%20diagnosis%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.02595v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520neural%2520networks%2520via%2520MCMC%253A%2520a%2520Python-based%2520tutorial%26entry.906535625%3DRohitash%2520Chandra%2520and%2520Joshua%2520Simmons%26entry.1292438233%3D%2520%2520Bayesian%2520inference%2520provides%2520a%2520methodology%2520for%2520parameter%2520estimation%2520and%250Auncertainty%2520quantification%2520in%2520machine%2520learning%2520and%2520deep%2520learning%2520methods.%250AVariational%2520inference%2520and%2520Markov%2520Chain%2520Monte-Carlo%2520%2528MCMC%2529%2520sampling%2520methods%2520are%250Aused%2520to%2520implement%2520Bayesian%2520inference.%2520In%2520the%2520past%2520three%2520decades%252C%2520MCMC%2520sampling%250Amethods%2520have%2520faced%2520some%2520challenges%2520in%2520being%2520adapted%2520to%2520larger%2520models%2520%2528such%2520as%250Ain%2520deep%2520learning%2529%2520and%2520big%2520data%2520problems.%2520Advanced%2520proposal%2520distributions%2520that%250Aincorporate%2520gradients%252C%2520such%2520as%2520a%2520Langevin%2520proposal%2520distribution%252C%2520provide%2520a%250Ameans%2520to%2520address%2520some%2520of%2520the%2520limitations%2520of%2520MCMC%2520sampling%2520for%2520Bayesian%2520neural%250Anetworks.%2520Furthermore%252C%2520MCMC%2520methods%2520have%2520typically%2520been%2520constrained%2520to%250Astatisticians%2520and%2520currently%2520not%2520well-known%2520among%2520deep%2520learning%2520researchers.%2520We%250Apresent%2520a%2520tutorial%2520for%2520MCMC%2520methods%2520that%2520covers%2520simple%2520Bayesian%2520linear%2520and%250Alogistic%2520models%252C%2520and%2520Bayesian%2520neural%2520networks.%2520The%2520aim%2520of%2520this%2520tutorial%2520is%2520to%250Abridge%2520the%2520gap%2520between%2520theory%2520and%2520implementation%2520via%2520coding%252C%2520given%2520a%2520general%250Asparsity%2520of%2520libraries%2520and%2520tutorials%2520to%2520this%2520end.%2520This%2520tutorial%2520provides%2520code%2520in%250APython%2520with%2520data%2520and%2520instructions%2520that%2520enable%2520their%2520use%2520and%2520extension.%2520We%250Aprovide%2520results%2520for%2520some%2520benchmark%2520problems%2520showing%2520the%2520strengths%2520and%250Aweaknesses%2520of%2520implementing%2520the%2520respective%2520Bayesian%2520models%2520via%2520MCMC.%2520We%250Ahighlight%2520the%2520challenges%2520in%2520sampling%2520multi-modal%2520posterior%2520distributions%2520for%250Athe%2520case%2520of%2520Bayesian%2520neural%2520networks%2520and%2520the%2520need%2520for%2520further%2520improvement%2520of%250Aconvergence%2520diagnosis%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.02595v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20neural%20networks%20via%20MCMC%3A%20a%20Python-based%20tutorial&entry.906535625=Rohitash%20Chandra%20and%20Joshua%20Simmons&entry.1292438233=%20%20Bayesian%20inference%20provides%20a%20methodology%20for%20parameter%20estimation%20and%0Auncertainty%20quantification%20in%20machine%20learning%20and%20deep%20learning%20methods.%0AVariational%20inference%20and%20Markov%20Chain%20Monte-Carlo%20%28MCMC%29%20sampling%20methods%20are%0Aused%20to%20implement%20Bayesian%20inference.%20In%20the%20past%20three%20decades%2C%20MCMC%20sampling%0Amethods%20have%20faced%20some%20challenges%20in%20being%20adapted%20to%20larger%20models%20%28such%20as%0Ain%20deep%20learning%29%20and%20big%20data%20problems.%20Advanced%20proposal%20distributions%20that%0Aincorporate%20gradients%2C%20such%20as%20a%20Langevin%20proposal%20distribution%2C%20provide%20a%0Ameans%20to%20address%20some%20of%20the%20limitations%20of%20MCMC%20sampling%20for%20Bayesian%20neural%0Anetworks.%20Furthermore%2C%20MCMC%20methods%20have%20typically%20been%20constrained%20to%0Astatisticians%20and%20currently%20not%20well-known%20among%20deep%20learning%20researchers.%20We%0Apresent%20a%20tutorial%20for%20MCMC%20methods%20that%20covers%20simple%20Bayesian%20linear%20and%0Alogistic%20models%2C%20and%20Bayesian%20neural%20networks.%20The%20aim%20of%20this%20tutorial%20is%20to%0Abridge%20the%20gap%20between%20theory%20and%20implementation%20via%20coding%2C%20given%20a%20general%0Asparsity%20of%20libraries%20and%20tutorials%20to%20this%20end.%20This%20tutorial%20provides%20code%20in%0APython%20with%20data%20and%20instructions%20that%20enable%20their%20use%20and%20extension.%20We%0Aprovide%20results%20for%20some%20benchmark%20problems%20showing%20the%20strengths%20and%0Aweaknesses%20of%20implementing%20the%20respective%20Bayesian%20models%20via%20MCMC.%20We%0Ahighlight%20the%20challenges%20in%20sampling%20multi-modal%20posterior%20distributions%20for%0Athe%20case%20of%20Bayesian%20neural%20networks%20and%20the%20need%20for%20further%20improvement%20of%0Aconvergence%20diagnosis%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.02595v3&entry.124074799=Read"},
{"title": "CURE4Rec: A Benchmark for Recommendation Unlearning with Deeper\n  Influence", "author": "Chaochao Chen and Jiaming Zhang and Yizhao Zhang and Li Zhang and Lingjuan Lyu and Yuyuan Li and Biao Gong and Chenggang Yan", "abstract": "  With increasing privacy concerns in artificial intelligence, regulations have\nmandated the right to be forgotten, granting individuals the right to withdraw\ntheir data from models. Machine unlearning has emerged as a potential solution\nto enable selective forgetting in models, particularly in recommender systems\nwhere historical data contains sensitive user information. Despite recent\nadvances in recommendation unlearning, evaluating unlearning methods\ncomprehensively remains challenging due to the absence of a unified evaluation\nframework and overlooked aspects of deeper influence, e.g., fairness. To\naddress these gaps, we propose CURE4Rec, the first comprehensive benchmark for\nrecommendation unlearning evaluation. CURE4Rec covers four aspects, i.e.,\nunlearning Completeness, recommendation Utility, unleaRning efficiency, and\nrecommendation fairnEss, under three data selection strategies, i.e., core\ndata, edge data, and random data. Specifically, we consider the deeper\ninfluence of unlearning on recommendation fairness and robustness towards data\nwith varying impact levels. We construct multiple datasets with CURE4Rec\nevaluation and conduct extensive experiments on existing recommendation\nunlearning methods. Our code is released at\nhttps://github.com/xiye7lai/CURE4Rec.\n", "link": "http://arxiv.org/abs/2408.14393v1", "date": "2024-08-26", "relevancy": 1.8881, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5266}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4706}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CURE4Rec%3A%20A%20Benchmark%20for%20Recommendation%20Unlearning%20with%20Deeper%0A%20%20Influence&body=Title%3A%20CURE4Rec%3A%20A%20Benchmark%20for%20Recommendation%20Unlearning%20with%20Deeper%0A%20%20Influence%0AAuthor%3A%20Chaochao%20Chen%20and%20Jiaming%20Zhang%20and%20Yizhao%20Zhang%20and%20Li%20Zhang%20and%20Lingjuan%20Lyu%20and%20Yuyuan%20Li%20and%20Biao%20Gong%20and%20Chenggang%20Yan%0AAbstract%3A%20%20%20With%20increasing%20privacy%20concerns%20in%20artificial%20intelligence%2C%20regulations%20have%0Amandated%20the%20right%20to%20be%20forgotten%2C%20granting%20individuals%20the%20right%20to%20withdraw%0Atheir%20data%20from%20models.%20Machine%20unlearning%20has%20emerged%20as%20a%20potential%20solution%0Ato%20enable%20selective%20forgetting%20in%20models%2C%20particularly%20in%20recommender%20systems%0Awhere%20historical%20data%20contains%20sensitive%20user%20information.%20Despite%20recent%0Aadvances%20in%20recommendation%20unlearning%2C%20evaluating%20unlearning%20methods%0Acomprehensively%20remains%20challenging%20due%20to%20the%20absence%20of%20a%20unified%20evaluation%0Aframework%20and%20overlooked%20aspects%20of%20deeper%20influence%2C%20e.g.%2C%20fairness.%20To%0Aaddress%20these%20gaps%2C%20we%20propose%20CURE4Rec%2C%20the%20first%20comprehensive%20benchmark%20for%0Arecommendation%20unlearning%20evaluation.%20CURE4Rec%20covers%20four%20aspects%2C%20i.e.%2C%0Aunlearning%20Completeness%2C%20recommendation%20Utility%2C%20unleaRning%20efficiency%2C%20and%0Arecommendation%20fairnEss%2C%20under%20three%20data%20selection%20strategies%2C%20i.e.%2C%20core%0Adata%2C%20edge%20data%2C%20and%20random%20data.%20Specifically%2C%20we%20consider%20the%20deeper%0Ainfluence%20of%20unlearning%20on%20recommendation%20fairness%20and%20robustness%20towards%20data%0Awith%20varying%20impact%20levels.%20We%20construct%20multiple%20datasets%20with%20CURE4Rec%0Aevaluation%20and%20conduct%20extensive%20experiments%20on%20existing%20recommendation%0Aunlearning%20methods.%20Our%20code%20is%20released%20at%0Ahttps%3A//github.com/xiye7lai/CURE4Rec.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14393v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCURE4Rec%253A%2520A%2520Benchmark%2520for%2520Recommendation%2520Unlearning%2520with%2520Deeper%250A%2520%2520Influence%26entry.906535625%3DChaochao%2520Chen%2520and%2520Jiaming%2520Zhang%2520and%2520Yizhao%2520Zhang%2520and%2520Li%2520Zhang%2520and%2520Lingjuan%2520Lyu%2520and%2520Yuyuan%2520Li%2520and%2520Biao%2520Gong%2520and%2520Chenggang%2520Yan%26entry.1292438233%3D%2520%2520With%2520increasing%2520privacy%2520concerns%2520in%2520artificial%2520intelligence%252C%2520regulations%2520have%250Amandated%2520the%2520right%2520to%2520be%2520forgotten%252C%2520granting%2520individuals%2520the%2520right%2520to%2520withdraw%250Atheir%2520data%2520from%2520models.%2520Machine%2520unlearning%2520has%2520emerged%2520as%2520a%2520potential%2520solution%250Ato%2520enable%2520selective%2520forgetting%2520in%2520models%252C%2520particularly%2520in%2520recommender%2520systems%250Awhere%2520historical%2520data%2520contains%2520sensitive%2520user%2520information.%2520Despite%2520recent%250Aadvances%2520in%2520recommendation%2520unlearning%252C%2520evaluating%2520unlearning%2520methods%250Acomprehensively%2520remains%2520challenging%2520due%2520to%2520the%2520absence%2520of%2520a%2520unified%2520evaluation%250Aframework%2520and%2520overlooked%2520aspects%2520of%2520deeper%2520influence%252C%2520e.g.%252C%2520fairness.%2520To%250Aaddress%2520these%2520gaps%252C%2520we%2520propose%2520CURE4Rec%252C%2520the%2520first%2520comprehensive%2520benchmark%2520for%250Arecommendation%2520unlearning%2520evaluation.%2520CURE4Rec%2520covers%2520four%2520aspects%252C%2520i.e.%252C%250Aunlearning%2520Completeness%252C%2520recommendation%2520Utility%252C%2520unleaRning%2520efficiency%252C%2520and%250Arecommendation%2520fairnEss%252C%2520under%2520three%2520data%2520selection%2520strategies%252C%2520i.e.%252C%2520core%250Adata%252C%2520edge%2520data%252C%2520and%2520random%2520data.%2520Specifically%252C%2520we%2520consider%2520the%2520deeper%250Ainfluence%2520of%2520unlearning%2520on%2520recommendation%2520fairness%2520and%2520robustness%2520towards%2520data%250Awith%2520varying%2520impact%2520levels.%2520We%2520construct%2520multiple%2520datasets%2520with%2520CURE4Rec%250Aevaluation%2520and%2520conduct%2520extensive%2520experiments%2520on%2520existing%2520recommendation%250Aunlearning%2520methods.%2520Our%2520code%2520is%2520released%2520at%250Ahttps%253A//github.com/xiye7lai/CURE4Rec.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14393v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CURE4Rec%3A%20A%20Benchmark%20for%20Recommendation%20Unlearning%20with%20Deeper%0A%20%20Influence&entry.906535625=Chaochao%20Chen%20and%20Jiaming%20Zhang%20and%20Yizhao%20Zhang%20and%20Li%20Zhang%20and%20Lingjuan%20Lyu%20and%20Yuyuan%20Li%20and%20Biao%20Gong%20and%20Chenggang%20Yan&entry.1292438233=%20%20With%20increasing%20privacy%20concerns%20in%20artificial%20intelligence%2C%20regulations%20have%0Amandated%20the%20right%20to%20be%20forgotten%2C%20granting%20individuals%20the%20right%20to%20withdraw%0Atheir%20data%20from%20models.%20Machine%20unlearning%20has%20emerged%20as%20a%20potential%20solution%0Ato%20enable%20selective%20forgetting%20in%20models%2C%20particularly%20in%20recommender%20systems%0Awhere%20historical%20data%20contains%20sensitive%20user%20information.%20Despite%20recent%0Aadvances%20in%20recommendation%20unlearning%2C%20evaluating%20unlearning%20methods%0Acomprehensively%20remains%20challenging%20due%20to%20the%20absence%20of%20a%20unified%20evaluation%0Aframework%20and%20overlooked%20aspects%20of%20deeper%20influence%2C%20e.g.%2C%20fairness.%20To%0Aaddress%20these%20gaps%2C%20we%20propose%20CURE4Rec%2C%20the%20first%20comprehensive%20benchmark%20for%0Arecommendation%20unlearning%20evaluation.%20CURE4Rec%20covers%20four%20aspects%2C%20i.e.%2C%0Aunlearning%20Completeness%2C%20recommendation%20Utility%2C%20unleaRning%20efficiency%2C%20and%0Arecommendation%20fairnEss%2C%20under%20three%20data%20selection%20strategies%2C%20i.e.%2C%20core%0Adata%2C%20edge%20data%2C%20and%20random%20data.%20Specifically%2C%20we%20consider%20the%20deeper%0Ainfluence%20of%20unlearning%20on%20recommendation%20fairness%20and%20robustness%20towards%20data%0Awith%20varying%20impact%20levels.%20We%20construct%20multiple%20datasets%20with%20CURE4Rec%0Aevaluation%20and%20conduct%20extensive%20experiments%20on%20existing%20recommendation%0Aunlearning%20methods.%20Our%20code%20is%20released%20at%0Ahttps%3A//github.com/xiye7lai/CURE4Rec.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14393v1&entry.124074799=Read"},
{"title": "A Dataset and Benchmark for Hospital Course Summarization with Adapted\n  Large Language Models", "author": "Asad Aali and Dave Van Veen and Yamin Ishraq Arefeen and Jason Hom and Christian Bluethgen and Eduardo Pontes Reis and Sergios Gatidis and Namuun Clifford and Joseph Daws and Arash S. Tehrani and Jangwon Kim and Akshay S. Chaudhari", "abstract": "  Brief hospital course (BHC) summaries are clinical documents that summarize a\npatient's hospital stay. While large language models (LLMs) depict remarkable\ncapabilities in automating real-world tasks, their capabilities for healthcare\napplications such as synthesizing BHCs from clinical notes have not been shown.\nWe introduce a novel pre-processed dataset, the MIMIC-IV-BHC, encapsulating\nclinical note and brief hospital course (BHC) pairs to adapt LLMs for BHC\nsynthesis. Furthermore, we introduce a benchmark of the summarization\nperformance of two general-purpose LLMs and three healthcare-adapted LLMs.\n  Using clinical notes as input, we apply prompting-based (using in-context\nlearning) and fine-tuning-based adaptation strategies to three open-source LLMs\n(Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5,\nGPT-4). We evaluate these LLMs across multiple context-length inputs using\nnatural language similarity metrics. We further conduct a clinical study with\nfive clinicians, comparing clinician-written and LLM-generated BHCs across 30\nsamples, focusing on their potential to enhance clinical decision-making\nthrough improved summary quality. We observe that the Llama2-13B fine-tuned LLM\noutperforms other domain-adapted models given quantitative evaluation metrics\nof BLEU and BERT-Score. GPT-4 with in-context learning shows more robustness to\nincreasing context lengths of clinical note inputs than fine-tuned Llama2-13B.\nDespite comparable quantitative metrics, the reader study depicts a significant\npreference for summaries generated by GPT-4 with in-context learning compared\nto both Llama2-13B fine-tuned summaries and the original summaries,\nhighlighting the need for qualitative clinical evaluation.\n", "link": "http://arxiv.org/abs/2403.05720v2", "date": "2024-08-26", "relevancy": 1.8867, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4928}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4645}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Dataset%20and%20Benchmark%20for%20Hospital%20Course%20Summarization%20with%20Adapted%0A%20%20Large%20Language%20Models&body=Title%3A%20A%20Dataset%20and%20Benchmark%20for%20Hospital%20Course%20Summarization%20with%20Adapted%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Asad%20Aali%20and%20Dave%20Van%20Veen%20and%20Yamin%20Ishraq%20Arefeen%20and%20Jason%20Hom%20and%20Christian%20Bluethgen%20and%20Eduardo%20Pontes%20Reis%20and%20Sergios%20Gatidis%20and%20Namuun%20Clifford%20and%20Joseph%20Daws%20and%20Arash%20S.%20Tehrani%20and%20Jangwon%20Kim%20and%20Akshay%20S.%20Chaudhari%0AAbstract%3A%20%20%20Brief%20hospital%20course%20%28BHC%29%20summaries%20are%20clinical%20documents%20that%20summarize%20a%0Apatient%27s%20hospital%20stay.%20While%20large%20language%20models%20%28LLMs%29%20depict%20remarkable%0Acapabilities%20in%20automating%20real-world%20tasks%2C%20their%20capabilities%20for%20healthcare%0Aapplications%20such%20as%20synthesizing%20BHCs%20from%20clinical%20notes%20have%20not%20been%20shown.%0AWe%20introduce%20a%20novel%20pre-processed%20dataset%2C%20the%20MIMIC-IV-BHC%2C%20encapsulating%0Aclinical%20note%20and%20brief%20hospital%20course%20%28BHC%29%20pairs%20to%20adapt%20LLMs%20for%20BHC%0Asynthesis.%20Furthermore%2C%20we%20introduce%20a%20benchmark%20of%20the%20summarization%0Aperformance%20of%20two%20general-purpose%20LLMs%20and%20three%20healthcare-adapted%20LLMs.%0A%20%20Using%20clinical%20notes%20as%20input%2C%20we%20apply%20prompting-based%20%28using%20in-context%0Alearning%29%20and%20fine-tuning-based%20adaptation%20strategies%20to%20three%20open-source%20LLMs%0A%28Clinical-T5-Large%2C%20Llama2-13B%2C%20FLAN-UL2%29%20and%20two%20proprietary%20LLMs%20%28GPT-3.5%2C%0AGPT-4%29.%20We%20evaluate%20these%20LLMs%20across%20multiple%20context-length%20inputs%20using%0Anatural%20language%20similarity%20metrics.%20We%20further%20conduct%20a%20clinical%20study%20with%0Afive%20clinicians%2C%20comparing%20clinician-written%20and%20LLM-generated%20BHCs%20across%2030%0Asamples%2C%20focusing%20on%20their%20potential%20to%20enhance%20clinical%20decision-making%0Athrough%20improved%20summary%20quality.%20We%20observe%20that%20the%20Llama2-13B%20fine-tuned%20LLM%0Aoutperforms%20other%20domain-adapted%20models%20given%20quantitative%20evaluation%20metrics%0Aof%20BLEU%20and%20BERT-Score.%20GPT-4%20with%20in-context%20learning%20shows%20more%20robustness%20to%0Aincreasing%20context%20lengths%20of%20clinical%20note%20inputs%20than%20fine-tuned%20Llama2-13B.%0ADespite%20comparable%20quantitative%20metrics%2C%20the%20reader%20study%20depicts%20a%20significant%0Apreference%20for%20summaries%20generated%20by%20GPT-4%20with%20in-context%20learning%20compared%0Ato%20both%20Llama2-13B%20fine-tuned%20summaries%20and%20the%20original%20summaries%2C%0Ahighlighting%20the%20need%20for%20qualitative%20clinical%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05720v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Dataset%2520and%2520Benchmark%2520for%2520Hospital%2520Course%2520Summarization%2520with%2520Adapted%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DAsad%2520Aali%2520and%2520Dave%2520Van%2520Veen%2520and%2520Yamin%2520Ishraq%2520Arefeen%2520and%2520Jason%2520Hom%2520and%2520Christian%2520Bluethgen%2520and%2520Eduardo%2520Pontes%2520Reis%2520and%2520Sergios%2520Gatidis%2520and%2520Namuun%2520Clifford%2520and%2520Joseph%2520Daws%2520and%2520Arash%2520S.%2520Tehrani%2520and%2520Jangwon%2520Kim%2520and%2520Akshay%2520S.%2520Chaudhari%26entry.1292438233%3D%2520%2520Brief%2520hospital%2520course%2520%2528BHC%2529%2520summaries%2520are%2520clinical%2520documents%2520that%2520summarize%2520a%250Apatient%2527s%2520hospital%2520stay.%2520While%2520large%2520language%2520models%2520%2528LLMs%2529%2520depict%2520remarkable%250Acapabilities%2520in%2520automating%2520real-world%2520tasks%252C%2520their%2520capabilities%2520for%2520healthcare%250Aapplications%2520such%2520as%2520synthesizing%2520BHCs%2520from%2520clinical%2520notes%2520have%2520not%2520been%2520shown.%250AWe%2520introduce%2520a%2520novel%2520pre-processed%2520dataset%252C%2520the%2520MIMIC-IV-BHC%252C%2520encapsulating%250Aclinical%2520note%2520and%2520brief%2520hospital%2520course%2520%2528BHC%2529%2520pairs%2520to%2520adapt%2520LLMs%2520for%2520BHC%250Asynthesis.%2520Furthermore%252C%2520we%2520introduce%2520a%2520benchmark%2520of%2520the%2520summarization%250Aperformance%2520of%2520two%2520general-purpose%2520LLMs%2520and%2520three%2520healthcare-adapted%2520LLMs.%250A%2520%2520Using%2520clinical%2520notes%2520as%2520input%252C%2520we%2520apply%2520prompting-based%2520%2528using%2520in-context%250Alearning%2529%2520and%2520fine-tuning-based%2520adaptation%2520strategies%2520to%2520three%2520open-source%2520LLMs%250A%2528Clinical-T5-Large%252C%2520Llama2-13B%252C%2520FLAN-UL2%2529%2520and%2520two%2520proprietary%2520LLMs%2520%2528GPT-3.5%252C%250AGPT-4%2529.%2520We%2520evaluate%2520these%2520LLMs%2520across%2520multiple%2520context-length%2520inputs%2520using%250Anatural%2520language%2520similarity%2520metrics.%2520We%2520further%2520conduct%2520a%2520clinical%2520study%2520with%250Afive%2520clinicians%252C%2520comparing%2520clinician-written%2520and%2520LLM-generated%2520BHCs%2520across%252030%250Asamples%252C%2520focusing%2520on%2520their%2520potential%2520to%2520enhance%2520clinical%2520decision-making%250Athrough%2520improved%2520summary%2520quality.%2520We%2520observe%2520that%2520the%2520Llama2-13B%2520fine-tuned%2520LLM%250Aoutperforms%2520other%2520domain-adapted%2520models%2520given%2520quantitative%2520evaluation%2520metrics%250Aof%2520BLEU%2520and%2520BERT-Score.%2520GPT-4%2520with%2520in-context%2520learning%2520shows%2520more%2520robustness%2520to%250Aincreasing%2520context%2520lengths%2520of%2520clinical%2520note%2520inputs%2520than%2520fine-tuned%2520Llama2-13B.%250ADespite%2520comparable%2520quantitative%2520metrics%252C%2520the%2520reader%2520study%2520depicts%2520a%2520significant%250Apreference%2520for%2520summaries%2520generated%2520by%2520GPT-4%2520with%2520in-context%2520learning%2520compared%250Ato%2520both%2520Llama2-13B%2520fine-tuned%2520summaries%2520and%2520the%2520original%2520summaries%252C%250Ahighlighting%2520the%2520need%2520for%2520qualitative%2520clinical%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.05720v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Dataset%20and%20Benchmark%20for%20Hospital%20Course%20Summarization%20with%20Adapted%0A%20%20Large%20Language%20Models&entry.906535625=Asad%20Aali%20and%20Dave%20Van%20Veen%20and%20Yamin%20Ishraq%20Arefeen%20and%20Jason%20Hom%20and%20Christian%20Bluethgen%20and%20Eduardo%20Pontes%20Reis%20and%20Sergios%20Gatidis%20and%20Namuun%20Clifford%20and%20Joseph%20Daws%20and%20Arash%20S.%20Tehrani%20and%20Jangwon%20Kim%20and%20Akshay%20S.%20Chaudhari&entry.1292438233=%20%20Brief%20hospital%20course%20%28BHC%29%20summaries%20are%20clinical%20documents%20that%20summarize%20a%0Apatient%27s%20hospital%20stay.%20While%20large%20language%20models%20%28LLMs%29%20depict%20remarkable%0Acapabilities%20in%20automating%20real-world%20tasks%2C%20their%20capabilities%20for%20healthcare%0Aapplications%20such%20as%20synthesizing%20BHCs%20from%20clinical%20notes%20have%20not%20been%20shown.%0AWe%20introduce%20a%20novel%20pre-processed%20dataset%2C%20the%20MIMIC-IV-BHC%2C%20encapsulating%0Aclinical%20note%20and%20brief%20hospital%20course%20%28BHC%29%20pairs%20to%20adapt%20LLMs%20for%20BHC%0Asynthesis.%20Furthermore%2C%20we%20introduce%20a%20benchmark%20of%20the%20summarization%0Aperformance%20of%20two%20general-purpose%20LLMs%20and%20three%20healthcare-adapted%20LLMs.%0A%20%20Using%20clinical%20notes%20as%20input%2C%20we%20apply%20prompting-based%20%28using%20in-context%0Alearning%29%20and%20fine-tuning-based%20adaptation%20strategies%20to%20three%20open-source%20LLMs%0A%28Clinical-T5-Large%2C%20Llama2-13B%2C%20FLAN-UL2%29%20and%20two%20proprietary%20LLMs%20%28GPT-3.5%2C%0AGPT-4%29.%20We%20evaluate%20these%20LLMs%20across%20multiple%20context-length%20inputs%20using%0Anatural%20language%20similarity%20metrics.%20We%20further%20conduct%20a%20clinical%20study%20with%0Afive%20clinicians%2C%20comparing%20clinician-written%20and%20LLM-generated%20BHCs%20across%2030%0Asamples%2C%20focusing%20on%20their%20potential%20to%20enhance%20clinical%20decision-making%0Athrough%20improved%20summary%20quality.%20We%20observe%20that%20the%20Llama2-13B%20fine-tuned%20LLM%0Aoutperforms%20other%20domain-adapted%20models%20given%20quantitative%20evaluation%20metrics%0Aof%20BLEU%20and%20BERT-Score.%20GPT-4%20with%20in-context%20learning%20shows%20more%20robustness%20to%0Aincreasing%20context%20lengths%20of%20clinical%20note%20inputs%20than%20fine-tuned%20Llama2-13B.%0ADespite%20comparable%20quantitative%20metrics%2C%20the%20reader%20study%20depicts%20a%20significant%0Apreference%20for%20summaries%20generated%20by%20GPT-4%20with%20in-context%20learning%20compared%0Ato%20both%20Llama2-13B%20fine-tuned%20summaries%20and%20the%20original%20summaries%2C%0Ahighlighting%20the%20need%20for%20qualitative%20clinical%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05720v2&entry.124074799=Read"},
{"title": "Function-Space MCMC for Bayesian Wide Neural Networks", "author": "Lucia Pezzetti and Stefano Favaro and Stefano Pelucchetti", "abstract": "  Bayesian Neural Networks represent a fascinating confluence of deep learning\nand probabilistic reasoning, offering a compelling framework for understanding\nuncertainty in complex predictive models. In this paper, we investigate the use\nof the preconditioned Crank-Nicolson algorithm and its Langevin version to\nsample from the reparametrised posterior distribution of the weights as the\nwidths of Bayesian Neural Networks grow larger. In addition to being robust in\nthe infinite-dimensional setting, we prove that the acceptance probabilities of\nthe proposed methods approach 1 as the width of the network increases,\nindependently of any stepsize tuning. Moreover, we examine and compare how the\nmixing speeds of the underdamped Langevin Monte Carlo, the preconditioned\nCrank-Nicolson and the preconditioned Crank-Nicolson Langevin samplers are\ninfluenced by changes in the network width in some real-world cases. Our\nfindings suggest that, in wide Bayesian Neural Networks configurations, the\npreconditioned Crank-Nicolson method allows for more efficient sampling of the\nreparametrised posterior distribution, as evidenced by a higher effective\nsample size and improved diagnostic results compared with the other analysed\nalgorithms.\n", "link": "http://arxiv.org/abs/2408.14325v1", "date": "2024-08-26", "relevancy": 1.8863, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5077}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4722}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Function-Space%20MCMC%20for%20Bayesian%20Wide%20Neural%20Networks&body=Title%3A%20Function-Space%20MCMC%20for%20Bayesian%20Wide%20Neural%20Networks%0AAuthor%3A%20Lucia%20Pezzetti%20and%20Stefano%20Favaro%20and%20Stefano%20Pelucchetti%0AAbstract%3A%20%20%20Bayesian%20Neural%20Networks%20represent%20a%20fascinating%20confluence%20of%20deep%20learning%0Aand%20probabilistic%20reasoning%2C%20offering%20a%20compelling%20framework%20for%20understanding%0Auncertainty%20in%20complex%20predictive%20models.%20In%20this%20paper%2C%20we%20investigate%20the%20use%0Aof%20the%20preconditioned%20Crank-Nicolson%20algorithm%20and%20its%20Langevin%20version%20to%0Asample%20from%20the%20reparametrised%20posterior%20distribution%20of%20the%20weights%20as%20the%0Awidths%20of%20Bayesian%20Neural%20Networks%20grow%20larger.%20In%20addition%20to%20being%20robust%20in%0Athe%20infinite-dimensional%20setting%2C%20we%20prove%20that%20the%20acceptance%20probabilities%20of%0Athe%20proposed%20methods%20approach%201%20as%20the%20width%20of%20the%20network%20increases%2C%0Aindependently%20of%20any%20stepsize%20tuning.%20Moreover%2C%20we%20examine%20and%20compare%20how%20the%0Amixing%20speeds%20of%20the%20underdamped%20Langevin%20Monte%20Carlo%2C%20the%20preconditioned%0ACrank-Nicolson%20and%20the%20preconditioned%20Crank-Nicolson%20Langevin%20samplers%20are%0Ainfluenced%20by%20changes%20in%20the%20network%20width%20in%20some%20real-world%20cases.%20Our%0Afindings%20suggest%20that%2C%20in%20wide%20Bayesian%20Neural%20Networks%20configurations%2C%20the%0Apreconditioned%20Crank-Nicolson%20method%20allows%20for%20more%20efficient%20sampling%20of%20the%0Areparametrised%20posterior%20distribution%2C%20as%20evidenced%20by%20a%20higher%20effective%0Asample%20size%20and%20improved%20diagnostic%20results%20compared%20with%20the%20other%20analysed%0Aalgorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14325v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFunction-Space%2520MCMC%2520for%2520Bayesian%2520Wide%2520Neural%2520Networks%26entry.906535625%3DLucia%2520Pezzetti%2520and%2520Stefano%2520Favaro%2520and%2520Stefano%2520Pelucchetti%26entry.1292438233%3D%2520%2520Bayesian%2520Neural%2520Networks%2520represent%2520a%2520fascinating%2520confluence%2520of%2520deep%2520learning%250Aand%2520probabilistic%2520reasoning%252C%2520offering%2520a%2520compelling%2520framework%2520for%2520understanding%250Auncertainty%2520in%2520complex%2520predictive%2520models.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520use%250Aof%2520the%2520preconditioned%2520Crank-Nicolson%2520algorithm%2520and%2520its%2520Langevin%2520version%2520to%250Asample%2520from%2520the%2520reparametrised%2520posterior%2520distribution%2520of%2520the%2520weights%2520as%2520the%250Awidths%2520of%2520Bayesian%2520Neural%2520Networks%2520grow%2520larger.%2520In%2520addition%2520to%2520being%2520robust%2520in%250Athe%2520infinite-dimensional%2520setting%252C%2520we%2520prove%2520that%2520the%2520acceptance%2520probabilities%2520of%250Athe%2520proposed%2520methods%2520approach%25201%2520as%2520the%2520width%2520of%2520the%2520network%2520increases%252C%250Aindependently%2520of%2520any%2520stepsize%2520tuning.%2520Moreover%252C%2520we%2520examine%2520and%2520compare%2520how%2520the%250Amixing%2520speeds%2520of%2520the%2520underdamped%2520Langevin%2520Monte%2520Carlo%252C%2520the%2520preconditioned%250ACrank-Nicolson%2520and%2520the%2520preconditioned%2520Crank-Nicolson%2520Langevin%2520samplers%2520are%250Ainfluenced%2520by%2520changes%2520in%2520the%2520network%2520width%2520in%2520some%2520real-world%2520cases.%2520Our%250Afindings%2520suggest%2520that%252C%2520in%2520wide%2520Bayesian%2520Neural%2520Networks%2520configurations%252C%2520the%250Apreconditioned%2520Crank-Nicolson%2520method%2520allows%2520for%2520more%2520efficient%2520sampling%2520of%2520the%250Areparametrised%2520posterior%2520distribution%252C%2520as%2520evidenced%2520by%2520a%2520higher%2520effective%250Asample%2520size%2520and%2520improved%2520diagnostic%2520results%2520compared%2520with%2520the%2520other%2520analysed%250Aalgorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14325v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Function-Space%20MCMC%20for%20Bayesian%20Wide%20Neural%20Networks&entry.906535625=Lucia%20Pezzetti%20and%20Stefano%20Favaro%20and%20Stefano%20Pelucchetti&entry.1292438233=%20%20Bayesian%20Neural%20Networks%20represent%20a%20fascinating%20confluence%20of%20deep%20learning%0Aand%20probabilistic%20reasoning%2C%20offering%20a%20compelling%20framework%20for%20understanding%0Auncertainty%20in%20complex%20predictive%20models.%20In%20this%20paper%2C%20we%20investigate%20the%20use%0Aof%20the%20preconditioned%20Crank-Nicolson%20algorithm%20and%20its%20Langevin%20version%20to%0Asample%20from%20the%20reparametrised%20posterior%20distribution%20of%20the%20weights%20as%20the%0Awidths%20of%20Bayesian%20Neural%20Networks%20grow%20larger.%20In%20addition%20to%20being%20robust%20in%0Athe%20infinite-dimensional%20setting%2C%20we%20prove%20that%20the%20acceptance%20probabilities%20of%0Athe%20proposed%20methods%20approach%201%20as%20the%20width%20of%20the%20network%20increases%2C%0Aindependently%20of%20any%20stepsize%20tuning.%20Moreover%2C%20we%20examine%20and%20compare%20how%20the%0Amixing%20speeds%20of%20the%20underdamped%20Langevin%20Monte%20Carlo%2C%20the%20preconditioned%0ACrank-Nicolson%20and%20the%20preconditioned%20Crank-Nicolson%20Langevin%20samplers%20are%0Ainfluenced%20by%20changes%20in%20the%20network%20width%20in%20some%20real-world%20cases.%20Our%0Afindings%20suggest%20that%2C%20in%20wide%20Bayesian%20Neural%20Networks%20configurations%2C%20the%0Apreconditioned%20Crank-Nicolson%20method%20allows%20for%20more%20efficient%20sampling%20of%20the%0Areparametrised%20posterior%20distribution%2C%20as%20evidenced%20by%20a%20higher%20effective%0Asample%20size%20and%20improved%20diagnostic%20results%20compared%20with%20the%20other%20analysed%0Aalgorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14325v1&entry.124074799=Read"},
{"title": "Generalized Graph Prompt: Toward a Unification of Pre-Training and\n  Downstream Tasks on Graphs", "author": "Xingtong Yu and Zhenghao Liu and Yuan Fang and Zemin Liu and Sihong Chen and Xinming Zhang", "abstract": "  Graph neural networks have emerged as a powerful tool for graph\nrepresentation learning, but their performance heavily relies on abundant\ntask-specific supervision. To reduce labeling requirement, the \"pre-train,\nprompt\" paradigms have become increasingly common. However, existing study of\nprompting on graphs is limited, lacking a universal treatment to appeal to\ndifferent downstream tasks. In this paper, we propose GraphPrompt, a novel\npre-training and prompting framework on graphs. GraphPrompt not only unifies\npre-training and downstream tasks into a common task template but also employs\na learnable prompt to assist a downstream task in locating the most relevant\nknowledge from the pre-trained model in a task-specific manner. To further\nenhance GraphPrompt in these two stages, we extend it into GraphPrompt+ with\ntwo major enhancements. First, we generalize several popular graph pre-training\ntasks beyond simple link prediction to broaden the compatibility with our task\ntemplate. Second, we propose a more generalized prompt design that incorporates\na series of prompt vectors within every layer of the pre-trained graph encoder,\nin order to capitalize on the hierarchical information across different layers\nbeyond just the readout layer. Finally, we conduct extensive experiments on\nfive public datasets to evaluate and analyze GraphPrompt and GraphPrompt+.\n", "link": "http://arxiv.org/abs/2311.15317v5", "date": "2024-08-26", "relevancy": 1.8837, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5038}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.448}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20Graph%20Prompt%3A%20Toward%20a%20Unification%20of%20Pre-Training%20and%0A%20%20Downstream%20Tasks%20on%20Graphs&body=Title%3A%20Generalized%20Graph%20Prompt%3A%20Toward%20a%20Unification%20of%20Pre-Training%20and%0A%20%20Downstream%20Tasks%20on%20Graphs%0AAuthor%3A%20Xingtong%20Yu%20and%20Zhenghao%20Liu%20and%20Yuan%20Fang%20and%20Zemin%20Liu%20and%20Sihong%20Chen%20and%20Xinming%20Zhang%0AAbstract%3A%20%20%20Graph%20neural%20networks%20have%20emerged%20as%20a%20powerful%20tool%20for%20graph%0Arepresentation%20learning%2C%20but%20their%20performance%20heavily%20relies%20on%20abundant%0Atask-specific%20supervision.%20To%20reduce%20labeling%20requirement%2C%20the%20%22pre-train%2C%0Aprompt%22%20paradigms%20have%20become%20increasingly%20common.%20However%2C%20existing%20study%20of%0Aprompting%20on%20graphs%20is%20limited%2C%20lacking%20a%20universal%20treatment%20to%20appeal%20to%0Adifferent%20downstream%20tasks.%20In%20this%20paper%2C%20we%20propose%20GraphPrompt%2C%20a%20novel%0Apre-training%20and%20prompting%20framework%20on%20graphs.%20GraphPrompt%20not%20only%20unifies%0Apre-training%20and%20downstream%20tasks%20into%20a%20common%20task%20template%20but%20also%20employs%0Aa%20learnable%20prompt%20to%20assist%20a%20downstream%20task%20in%20locating%20the%20most%20relevant%0Aknowledge%20from%20the%20pre-trained%20model%20in%20a%20task-specific%20manner.%20To%20further%0Aenhance%20GraphPrompt%20in%20these%20two%20stages%2C%20we%20extend%20it%20into%20GraphPrompt%2B%20with%0Atwo%20major%20enhancements.%20First%2C%20we%20generalize%20several%20popular%20graph%20pre-training%0Atasks%20beyond%20simple%20link%20prediction%20to%20broaden%20the%20compatibility%20with%20our%20task%0Atemplate.%20Second%2C%20we%20propose%20a%20more%20generalized%20prompt%20design%20that%20incorporates%0Aa%20series%20of%20prompt%20vectors%20within%20every%20layer%20of%20the%20pre-trained%20graph%20encoder%2C%0Ain%20order%20to%20capitalize%20on%20the%20hierarchical%20information%20across%20different%20layers%0Abeyond%20just%20the%20readout%20layer.%20Finally%2C%20we%20conduct%20extensive%20experiments%20on%0Afive%20public%20datasets%20to%20evaluate%20and%20analyze%20GraphPrompt%20and%20GraphPrompt%2B.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.15317v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520Graph%2520Prompt%253A%2520Toward%2520a%2520Unification%2520of%2520Pre-Training%2520and%250A%2520%2520Downstream%2520Tasks%2520on%2520Graphs%26entry.906535625%3DXingtong%2520Yu%2520and%2520Zhenghao%2520Liu%2520and%2520Yuan%2520Fang%2520and%2520Zemin%2520Liu%2520and%2520Sihong%2520Chen%2520and%2520Xinming%2520Zhang%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520have%2520emerged%2520as%2520a%2520powerful%2520tool%2520for%2520graph%250Arepresentation%2520learning%252C%2520but%2520their%2520performance%2520heavily%2520relies%2520on%2520abundant%250Atask-specific%2520supervision.%2520To%2520reduce%2520labeling%2520requirement%252C%2520the%2520%2522pre-train%252C%250Aprompt%2522%2520paradigms%2520have%2520become%2520increasingly%2520common.%2520However%252C%2520existing%2520study%2520of%250Aprompting%2520on%2520graphs%2520is%2520limited%252C%2520lacking%2520a%2520universal%2520treatment%2520to%2520appeal%2520to%250Adifferent%2520downstream%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520propose%2520GraphPrompt%252C%2520a%2520novel%250Apre-training%2520and%2520prompting%2520framework%2520on%2520graphs.%2520GraphPrompt%2520not%2520only%2520unifies%250Apre-training%2520and%2520downstream%2520tasks%2520into%2520a%2520common%2520task%2520template%2520but%2520also%2520employs%250Aa%2520learnable%2520prompt%2520to%2520assist%2520a%2520downstream%2520task%2520in%2520locating%2520the%2520most%2520relevant%250Aknowledge%2520from%2520the%2520pre-trained%2520model%2520in%2520a%2520task-specific%2520manner.%2520To%2520further%250Aenhance%2520GraphPrompt%2520in%2520these%2520two%2520stages%252C%2520we%2520extend%2520it%2520into%2520GraphPrompt%252B%2520with%250Atwo%2520major%2520enhancements.%2520First%252C%2520we%2520generalize%2520several%2520popular%2520graph%2520pre-training%250Atasks%2520beyond%2520simple%2520link%2520prediction%2520to%2520broaden%2520the%2520compatibility%2520with%2520our%2520task%250Atemplate.%2520Second%252C%2520we%2520propose%2520a%2520more%2520generalized%2520prompt%2520design%2520that%2520incorporates%250Aa%2520series%2520of%2520prompt%2520vectors%2520within%2520every%2520layer%2520of%2520the%2520pre-trained%2520graph%2520encoder%252C%250Ain%2520order%2520to%2520capitalize%2520on%2520the%2520hierarchical%2520information%2520across%2520different%2520layers%250Abeyond%2520just%2520the%2520readout%2520layer.%2520Finally%252C%2520we%2520conduct%2520extensive%2520experiments%2520on%250Afive%2520public%2520datasets%2520to%2520evaluate%2520and%2520analyze%2520GraphPrompt%2520and%2520GraphPrompt%252B.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.15317v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Graph%20Prompt%3A%20Toward%20a%20Unification%20of%20Pre-Training%20and%0A%20%20Downstream%20Tasks%20on%20Graphs&entry.906535625=Xingtong%20Yu%20and%20Zhenghao%20Liu%20and%20Yuan%20Fang%20and%20Zemin%20Liu%20and%20Sihong%20Chen%20and%20Xinming%20Zhang&entry.1292438233=%20%20Graph%20neural%20networks%20have%20emerged%20as%20a%20powerful%20tool%20for%20graph%0Arepresentation%20learning%2C%20but%20their%20performance%20heavily%20relies%20on%20abundant%0Atask-specific%20supervision.%20To%20reduce%20labeling%20requirement%2C%20the%20%22pre-train%2C%0Aprompt%22%20paradigms%20have%20become%20increasingly%20common.%20However%2C%20existing%20study%20of%0Aprompting%20on%20graphs%20is%20limited%2C%20lacking%20a%20universal%20treatment%20to%20appeal%20to%0Adifferent%20downstream%20tasks.%20In%20this%20paper%2C%20we%20propose%20GraphPrompt%2C%20a%20novel%0Apre-training%20and%20prompting%20framework%20on%20graphs.%20GraphPrompt%20not%20only%20unifies%0Apre-training%20and%20downstream%20tasks%20into%20a%20common%20task%20template%20but%20also%20employs%0Aa%20learnable%20prompt%20to%20assist%20a%20downstream%20task%20in%20locating%20the%20most%20relevant%0Aknowledge%20from%20the%20pre-trained%20model%20in%20a%20task-specific%20manner.%20To%20further%0Aenhance%20GraphPrompt%20in%20these%20two%20stages%2C%20we%20extend%20it%20into%20GraphPrompt%2B%20with%0Atwo%20major%20enhancements.%20First%2C%20we%20generalize%20several%20popular%20graph%20pre-training%0Atasks%20beyond%20simple%20link%20prediction%20to%20broaden%20the%20compatibility%20with%20our%20task%0Atemplate.%20Second%2C%20we%20propose%20a%20more%20generalized%20prompt%20design%20that%20incorporates%0Aa%20series%20of%20prompt%20vectors%20within%20every%20layer%20of%20the%20pre-trained%20graph%20encoder%2C%0Ain%20order%20to%20capitalize%20on%20the%20hierarchical%20information%20across%20different%20layers%0Abeyond%20just%20the%20readout%20layer.%20Finally%2C%20we%20conduct%20extensive%20experiments%20on%0Afive%20public%20datasets%20to%20evaluate%20and%20analyze%20GraphPrompt%20and%20GraphPrompt%2B.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.15317v5&entry.124074799=Read"},
{"title": "Field theory for optimal signal propagation in ResNets", "author": "Kirsten Fischer and David Dahmen and Moritz Helias", "abstract": "  Residual networks have significantly better trainability and thus performance\nthan feed-forward networks at large depth. Introducing skip connections\nfacilitates signal propagation to deeper layers. In addition, previous works\nfound that adding a scaling parameter for the residual branch further improves\ngeneralization performance. While they empirically identified a particularly\nbeneficial range of values for this scaling parameter, the associated\nperformance improvement and its universality across network hyperparameters yet\nneed to be understood. For feed-forward networks, finite-size theories have led\nto important insights with regard to signal propagation and hyperparameter\ntuning. We here derive a systematic finite-size field theory for residual\nnetworks to study signal propagation and its dependence on the scaling for the\nresidual branch. We derive analytical expressions for the response function, a\nmeasure for the network's sensitivity to inputs, and show that for deep\nnetworks the empirically found values for the scaling parameter lie within the\nrange of maximal sensitivity. Furthermore, we obtain an analytical expression\nfor the optimal scaling parameter that depends only weakly on other network\nhyperparameters, such as the weight variance, thereby explaining its\nuniversality across hyperparameters. Overall, this work provides a theoretical\nframework to study ResNets at finite size.\n", "link": "http://arxiv.org/abs/2305.07715v2", "date": "2024-08-26", "relevancy": 1.88, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5075}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4732}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Field%20theory%20for%20optimal%20signal%20propagation%20in%20ResNets&body=Title%3A%20Field%20theory%20for%20optimal%20signal%20propagation%20in%20ResNets%0AAuthor%3A%20Kirsten%20Fischer%20and%20David%20Dahmen%20and%20Moritz%20Helias%0AAbstract%3A%20%20%20Residual%20networks%20have%20significantly%20better%20trainability%20and%20thus%20performance%0Athan%20feed-forward%20networks%20at%20large%20depth.%20Introducing%20skip%20connections%0Afacilitates%20signal%20propagation%20to%20deeper%20layers.%20In%20addition%2C%20previous%20works%0Afound%20that%20adding%20a%20scaling%20parameter%20for%20the%20residual%20branch%20further%20improves%0Ageneralization%20performance.%20While%20they%20empirically%20identified%20a%20particularly%0Abeneficial%20range%20of%20values%20for%20this%20scaling%20parameter%2C%20the%20associated%0Aperformance%20improvement%20and%20its%20universality%20across%20network%20hyperparameters%20yet%0Aneed%20to%20be%20understood.%20For%20feed-forward%20networks%2C%20finite-size%20theories%20have%20led%0Ato%20important%20insights%20with%20regard%20to%20signal%20propagation%20and%20hyperparameter%0Atuning.%20We%20here%20derive%20a%20systematic%20finite-size%20field%20theory%20for%20residual%0Anetworks%20to%20study%20signal%20propagation%20and%20its%20dependence%20on%20the%20scaling%20for%20the%0Aresidual%20branch.%20We%20derive%20analytical%20expressions%20for%20the%20response%20function%2C%20a%0Ameasure%20for%20the%20network%27s%20sensitivity%20to%20inputs%2C%20and%20show%20that%20for%20deep%0Anetworks%20the%20empirically%20found%20values%20for%20the%20scaling%20parameter%20lie%20within%20the%0Arange%20of%20maximal%20sensitivity.%20Furthermore%2C%20we%20obtain%20an%20analytical%20expression%0Afor%20the%20optimal%20scaling%20parameter%20that%20depends%20only%20weakly%20on%20other%20network%0Ahyperparameters%2C%20such%20as%20the%20weight%20variance%2C%20thereby%20explaining%20its%0Auniversality%20across%20hyperparameters.%20Overall%2C%20this%20work%20provides%20a%20theoretical%0Aframework%20to%20study%20ResNets%20at%20finite%20size.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.07715v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DField%2520theory%2520for%2520optimal%2520signal%2520propagation%2520in%2520ResNets%26entry.906535625%3DKirsten%2520Fischer%2520and%2520David%2520Dahmen%2520and%2520Moritz%2520Helias%26entry.1292438233%3D%2520%2520Residual%2520networks%2520have%2520significantly%2520better%2520trainability%2520and%2520thus%2520performance%250Athan%2520feed-forward%2520networks%2520at%2520large%2520depth.%2520Introducing%2520skip%2520connections%250Afacilitates%2520signal%2520propagation%2520to%2520deeper%2520layers.%2520In%2520addition%252C%2520previous%2520works%250Afound%2520that%2520adding%2520a%2520scaling%2520parameter%2520for%2520the%2520residual%2520branch%2520further%2520improves%250Ageneralization%2520performance.%2520While%2520they%2520empirically%2520identified%2520a%2520particularly%250Abeneficial%2520range%2520of%2520values%2520for%2520this%2520scaling%2520parameter%252C%2520the%2520associated%250Aperformance%2520improvement%2520and%2520its%2520universality%2520across%2520network%2520hyperparameters%2520yet%250Aneed%2520to%2520be%2520understood.%2520For%2520feed-forward%2520networks%252C%2520finite-size%2520theories%2520have%2520led%250Ato%2520important%2520insights%2520with%2520regard%2520to%2520signal%2520propagation%2520and%2520hyperparameter%250Atuning.%2520We%2520here%2520derive%2520a%2520systematic%2520finite-size%2520field%2520theory%2520for%2520residual%250Anetworks%2520to%2520study%2520signal%2520propagation%2520and%2520its%2520dependence%2520on%2520the%2520scaling%2520for%2520the%250Aresidual%2520branch.%2520We%2520derive%2520analytical%2520expressions%2520for%2520the%2520response%2520function%252C%2520a%250Ameasure%2520for%2520the%2520network%2527s%2520sensitivity%2520to%2520inputs%252C%2520and%2520show%2520that%2520for%2520deep%250Anetworks%2520the%2520empirically%2520found%2520values%2520for%2520the%2520scaling%2520parameter%2520lie%2520within%2520the%250Arange%2520of%2520maximal%2520sensitivity.%2520Furthermore%252C%2520we%2520obtain%2520an%2520analytical%2520expression%250Afor%2520the%2520optimal%2520scaling%2520parameter%2520that%2520depends%2520only%2520weakly%2520on%2520other%2520network%250Ahyperparameters%252C%2520such%2520as%2520the%2520weight%2520variance%252C%2520thereby%2520explaining%2520its%250Auniversality%2520across%2520hyperparameters.%2520Overall%252C%2520this%2520work%2520provides%2520a%2520theoretical%250Aframework%2520to%2520study%2520ResNets%2520at%2520finite%2520size.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.07715v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Field%20theory%20for%20optimal%20signal%20propagation%20in%20ResNets&entry.906535625=Kirsten%20Fischer%20and%20David%20Dahmen%20and%20Moritz%20Helias&entry.1292438233=%20%20Residual%20networks%20have%20significantly%20better%20trainability%20and%20thus%20performance%0Athan%20feed-forward%20networks%20at%20large%20depth.%20Introducing%20skip%20connections%0Afacilitates%20signal%20propagation%20to%20deeper%20layers.%20In%20addition%2C%20previous%20works%0Afound%20that%20adding%20a%20scaling%20parameter%20for%20the%20residual%20branch%20further%20improves%0Ageneralization%20performance.%20While%20they%20empirically%20identified%20a%20particularly%0Abeneficial%20range%20of%20values%20for%20this%20scaling%20parameter%2C%20the%20associated%0Aperformance%20improvement%20and%20its%20universality%20across%20network%20hyperparameters%20yet%0Aneed%20to%20be%20understood.%20For%20feed-forward%20networks%2C%20finite-size%20theories%20have%20led%0Ato%20important%20insights%20with%20regard%20to%20signal%20propagation%20and%20hyperparameter%0Atuning.%20We%20here%20derive%20a%20systematic%20finite-size%20field%20theory%20for%20residual%0Anetworks%20to%20study%20signal%20propagation%20and%20its%20dependence%20on%20the%20scaling%20for%20the%0Aresidual%20branch.%20We%20derive%20analytical%20expressions%20for%20the%20response%20function%2C%20a%0Ameasure%20for%20the%20network%27s%20sensitivity%20to%20inputs%2C%20and%20show%20that%20for%20deep%0Anetworks%20the%20empirically%20found%20values%20for%20the%20scaling%20parameter%20lie%20within%20the%0Arange%20of%20maximal%20sensitivity.%20Furthermore%2C%20we%20obtain%20an%20analytical%20expression%0Afor%20the%20optimal%20scaling%20parameter%20that%20depends%20only%20weakly%20on%20other%20network%0Ahyperparameters%2C%20such%20as%20the%20weight%20variance%2C%20thereby%20explaining%20its%0Auniversality%20across%20hyperparameters.%20Overall%2C%20this%20work%20provides%20a%20theoretical%0Aframework%20to%20study%20ResNets%20at%20finite%20size.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.07715v2&entry.124074799=Read"},
{"title": "Beyond Silence: Bias Analysis through Loss and Asymmetric Approach in\n  Audio Anti-Spoofing", "author": "Hye-jin Shim and Md Sahidullah and Jee-weon Jung and Shinji Watanabe and Tomi Kinnunen", "abstract": "  Current trends in audio anti-spoofing detection research strive to improve\nmodels' ability to generalize across unseen attacks by learning to identify a\nvariety of spoofing artifacts. This emphasis has primarily focused on the spoof\nclass. Recently, several studies have noted that the distribution of silence\ndiffers between the two classes, which can serve as a shortcut. In this paper,\nwe extend class-wise interpretations beyond silence. We employ loss analysis\nand asymmetric methodologies to move away from traditional attack-focused and\nresult-oriented evaluations towards a deeper examination of model behaviors.\nOur investigations highlight the significant differences in training dynamics\nbetween the two classes, emphasizing the need for future research to focus on\nrobust modeling of the bonafide class.\n", "link": "http://arxiv.org/abs/2406.17246v2", "date": "2024-08-26", "relevancy": 1.8679, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4939}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4486}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Silence%3A%20Bias%20Analysis%20through%20Loss%20and%20Asymmetric%20Approach%20in%0A%20%20Audio%20Anti-Spoofing&body=Title%3A%20Beyond%20Silence%3A%20Bias%20Analysis%20through%20Loss%20and%20Asymmetric%20Approach%20in%0A%20%20Audio%20Anti-Spoofing%0AAuthor%3A%20Hye-jin%20Shim%20and%20Md%20Sahidullah%20and%20Jee-weon%20Jung%20and%20Shinji%20Watanabe%20and%20Tomi%20Kinnunen%0AAbstract%3A%20%20%20Current%20trends%20in%20audio%20anti-spoofing%20detection%20research%20strive%20to%20improve%0Amodels%27%20ability%20to%20generalize%20across%20unseen%20attacks%20by%20learning%20to%20identify%20a%0Avariety%20of%20spoofing%20artifacts.%20This%20emphasis%20has%20primarily%20focused%20on%20the%20spoof%0Aclass.%20Recently%2C%20several%20studies%20have%20noted%20that%20the%20distribution%20of%20silence%0Adiffers%20between%20the%20two%20classes%2C%20which%20can%20serve%20as%20a%20shortcut.%20In%20this%20paper%2C%0Awe%20extend%20class-wise%20interpretations%20beyond%20silence.%20We%20employ%20loss%20analysis%0Aand%20asymmetric%20methodologies%20to%20move%20away%20from%20traditional%20attack-focused%20and%0Aresult-oriented%20evaluations%20towards%20a%20deeper%20examination%20of%20model%20behaviors.%0AOur%20investigations%20highlight%20the%20significant%20differences%20in%20training%20dynamics%0Abetween%20the%20two%20classes%2C%20emphasizing%20the%20need%20for%20future%20research%20to%20focus%20on%0Arobust%20modeling%20of%20the%20bonafide%20class.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17246v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Silence%253A%2520Bias%2520Analysis%2520through%2520Loss%2520and%2520Asymmetric%2520Approach%2520in%250A%2520%2520Audio%2520Anti-Spoofing%26entry.906535625%3DHye-jin%2520Shim%2520and%2520Md%2520Sahidullah%2520and%2520Jee-weon%2520Jung%2520and%2520Shinji%2520Watanabe%2520and%2520Tomi%2520Kinnunen%26entry.1292438233%3D%2520%2520Current%2520trends%2520in%2520audio%2520anti-spoofing%2520detection%2520research%2520strive%2520to%2520improve%250Amodels%2527%2520ability%2520to%2520generalize%2520across%2520unseen%2520attacks%2520by%2520learning%2520to%2520identify%2520a%250Avariety%2520of%2520spoofing%2520artifacts.%2520This%2520emphasis%2520has%2520primarily%2520focused%2520on%2520the%2520spoof%250Aclass.%2520Recently%252C%2520several%2520studies%2520have%2520noted%2520that%2520the%2520distribution%2520of%2520silence%250Adiffers%2520between%2520the%2520two%2520classes%252C%2520which%2520can%2520serve%2520as%2520a%2520shortcut.%2520In%2520this%2520paper%252C%250Awe%2520extend%2520class-wise%2520interpretations%2520beyond%2520silence.%2520We%2520employ%2520loss%2520analysis%250Aand%2520asymmetric%2520methodologies%2520to%2520move%2520away%2520from%2520traditional%2520attack-focused%2520and%250Aresult-oriented%2520evaluations%2520towards%2520a%2520deeper%2520examination%2520of%2520model%2520behaviors.%250AOur%2520investigations%2520highlight%2520the%2520significant%2520differences%2520in%2520training%2520dynamics%250Abetween%2520the%2520two%2520classes%252C%2520emphasizing%2520the%2520need%2520for%2520future%2520research%2520to%2520focus%2520on%250Arobust%2520modeling%2520of%2520the%2520bonafide%2520class.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17246v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Silence%3A%20Bias%20Analysis%20through%20Loss%20and%20Asymmetric%20Approach%20in%0A%20%20Audio%20Anti-Spoofing&entry.906535625=Hye-jin%20Shim%20and%20Md%20Sahidullah%20and%20Jee-weon%20Jung%20and%20Shinji%20Watanabe%20and%20Tomi%20Kinnunen&entry.1292438233=%20%20Current%20trends%20in%20audio%20anti-spoofing%20detection%20research%20strive%20to%20improve%0Amodels%27%20ability%20to%20generalize%20across%20unseen%20attacks%20by%20learning%20to%20identify%20a%0Avariety%20of%20spoofing%20artifacts.%20This%20emphasis%20has%20primarily%20focused%20on%20the%20spoof%0Aclass.%20Recently%2C%20several%20studies%20have%20noted%20that%20the%20distribution%20of%20silence%0Adiffers%20between%20the%20two%20classes%2C%20which%20can%20serve%20as%20a%20shortcut.%20In%20this%20paper%2C%0Awe%20extend%20class-wise%20interpretations%20beyond%20silence.%20We%20employ%20loss%20analysis%0Aand%20asymmetric%20methodologies%20to%20move%20away%20from%20traditional%20attack-focused%20and%0Aresult-oriented%20evaluations%20towards%20a%20deeper%20examination%20of%20model%20behaviors.%0AOur%20investigations%20highlight%20the%20significant%20differences%20in%20training%20dynamics%0Abetween%20the%20two%20classes%2C%20emphasizing%20the%20need%20for%20future%20research%20to%20focus%20on%0Arobust%20modeling%20of%20the%20bonafide%20class.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17246v2&entry.124074799=Read"},
{"title": "Beyond Few-shot Object Detection: A Detailed Survey", "author": "Vishal Chudasama and Hiran Sarkar and Pankaj Wasnik and Vineeth N Balasubramanian and Jayateja Kalla", "abstract": "  Object detection is a critical field in computer vision focusing on\naccurately identifying and locating specific objects in images or videos.\nTraditional methods for object detection rely on large labeled training\ndatasets for each object category, which can be time-consuming and expensive to\ncollect and annotate. To address this issue, researchers have introduced\nfew-shot object detection (FSOD) approaches that merge few-shot learning and\nobject detection principles. These approaches allow models to quickly adapt to\nnew object categories with only a few annotated samples. While traditional FSOD\nmethods have been studied before, this survey paper comprehensively reviews\nFSOD research with a specific focus on covering different FSOD settings such as\nstandard FSOD, generalized FSOD, incremental FSOD, open-set FSOD, and domain\nadaptive FSOD. These approaches play a vital role in reducing the reliance on\nextensive labeled datasets, particularly as the need for efficient machine\nlearning models continues to rise. This survey paper aims to provide a\ncomprehensive understanding of the above-mentioned few-shot settings and\nexplore the methodologies for each FSOD task. It thoroughly compares\nstate-of-the-art methods across different FSOD settings, analyzing them in\ndetail based on their evaluation protocols. Additionally, it offers insights\ninto their applications, challenges, and potential future directions in the\nevolving field of object detection with limited data.\n", "link": "http://arxiv.org/abs/2408.14249v1", "date": "2024-08-26", "relevancy": 1.865, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.468}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4676}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Few-shot%20Object%20Detection%3A%20A%20Detailed%20Survey&body=Title%3A%20Beyond%20Few-shot%20Object%20Detection%3A%20A%20Detailed%20Survey%0AAuthor%3A%20Vishal%20Chudasama%20and%20Hiran%20Sarkar%20and%20Pankaj%20Wasnik%20and%20Vineeth%20N%20Balasubramanian%20and%20Jayateja%20Kalla%0AAbstract%3A%20%20%20Object%20detection%20is%20a%20critical%20field%20in%20computer%20vision%20focusing%20on%0Aaccurately%20identifying%20and%20locating%20specific%20objects%20in%20images%20or%20videos.%0ATraditional%20methods%20for%20object%20detection%20rely%20on%20large%20labeled%20training%0Adatasets%20for%20each%20object%20category%2C%20which%20can%20be%20time-consuming%20and%20expensive%20to%0Acollect%20and%20annotate.%20To%20address%20this%20issue%2C%20researchers%20have%20introduced%0Afew-shot%20object%20detection%20%28FSOD%29%20approaches%20that%20merge%20few-shot%20learning%20and%0Aobject%20detection%20principles.%20These%20approaches%20allow%20models%20to%20quickly%20adapt%20to%0Anew%20object%20categories%20with%20only%20a%20few%20annotated%20samples.%20While%20traditional%20FSOD%0Amethods%20have%20been%20studied%20before%2C%20this%20survey%20paper%20comprehensively%20reviews%0AFSOD%20research%20with%20a%20specific%20focus%20on%20covering%20different%20FSOD%20settings%20such%20as%0Astandard%20FSOD%2C%20generalized%20FSOD%2C%20incremental%20FSOD%2C%20open-set%20FSOD%2C%20and%20domain%0Aadaptive%20FSOD.%20These%20approaches%20play%20a%20vital%20role%20in%20reducing%20the%20reliance%20on%0Aextensive%20labeled%20datasets%2C%20particularly%20as%20the%20need%20for%20efficient%20machine%0Alearning%20models%20continues%20to%20rise.%20This%20survey%20paper%20aims%20to%20provide%20a%0Acomprehensive%20understanding%20of%20the%20above-mentioned%20few-shot%20settings%20and%0Aexplore%20the%20methodologies%20for%20each%20FSOD%20task.%20It%20thoroughly%20compares%0Astate-of-the-art%20methods%20across%20different%20FSOD%20settings%2C%20analyzing%20them%20in%0Adetail%20based%20on%20their%20evaluation%20protocols.%20Additionally%2C%20it%20offers%20insights%0Ainto%20their%20applications%2C%20challenges%2C%20and%20potential%20future%20directions%20in%20the%0Aevolving%20field%20of%20object%20detection%20with%20limited%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14249v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Few-shot%2520Object%2520Detection%253A%2520A%2520Detailed%2520Survey%26entry.906535625%3DVishal%2520Chudasama%2520and%2520Hiran%2520Sarkar%2520and%2520Pankaj%2520Wasnik%2520and%2520Vineeth%2520N%2520Balasubramanian%2520and%2520Jayateja%2520Kalla%26entry.1292438233%3D%2520%2520Object%2520detection%2520is%2520a%2520critical%2520field%2520in%2520computer%2520vision%2520focusing%2520on%250Aaccurately%2520identifying%2520and%2520locating%2520specific%2520objects%2520in%2520images%2520or%2520videos.%250ATraditional%2520methods%2520for%2520object%2520detection%2520rely%2520on%2520large%2520labeled%2520training%250Adatasets%2520for%2520each%2520object%2520category%252C%2520which%2520can%2520be%2520time-consuming%2520and%2520expensive%2520to%250Acollect%2520and%2520annotate.%2520To%2520address%2520this%2520issue%252C%2520researchers%2520have%2520introduced%250Afew-shot%2520object%2520detection%2520%2528FSOD%2529%2520approaches%2520that%2520merge%2520few-shot%2520learning%2520and%250Aobject%2520detection%2520principles.%2520These%2520approaches%2520allow%2520models%2520to%2520quickly%2520adapt%2520to%250Anew%2520object%2520categories%2520with%2520only%2520a%2520few%2520annotated%2520samples.%2520While%2520traditional%2520FSOD%250Amethods%2520have%2520been%2520studied%2520before%252C%2520this%2520survey%2520paper%2520comprehensively%2520reviews%250AFSOD%2520research%2520with%2520a%2520specific%2520focus%2520on%2520covering%2520different%2520FSOD%2520settings%2520such%2520as%250Astandard%2520FSOD%252C%2520generalized%2520FSOD%252C%2520incremental%2520FSOD%252C%2520open-set%2520FSOD%252C%2520and%2520domain%250Aadaptive%2520FSOD.%2520These%2520approaches%2520play%2520a%2520vital%2520role%2520in%2520reducing%2520the%2520reliance%2520on%250Aextensive%2520labeled%2520datasets%252C%2520particularly%2520as%2520the%2520need%2520for%2520efficient%2520machine%250Alearning%2520models%2520continues%2520to%2520rise.%2520This%2520survey%2520paper%2520aims%2520to%2520provide%2520a%250Acomprehensive%2520understanding%2520of%2520the%2520above-mentioned%2520few-shot%2520settings%2520and%250Aexplore%2520the%2520methodologies%2520for%2520each%2520FSOD%2520task.%2520It%2520thoroughly%2520compares%250Astate-of-the-art%2520methods%2520across%2520different%2520FSOD%2520settings%252C%2520analyzing%2520them%2520in%250Adetail%2520based%2520on%2520their%2520evaluation%2520protocols.%2520Additionally%252C%2520it%2520offers%2520insights%250Ainto%2520their%2520applications%252C%2520challenges%252C%2520and%2520potential%2520future%2520directions%2520in%2520the%250Aevolving%2520field%2520of%2520object%2520detection%2520with%2520limited%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14249v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Few-shot%20Object%20Detection%3A%20A%20Detailed%20Survey&entry.906535625=Vishal%20Chudasama%20and%20Hiran%20Sarkar%20and%20Pankaj%20Wasnik%20and%20Vineeth%20N%20Balasubramanian%20and%20Jayateja%20Kalla&entry.1292438233=%20%20Object%20detection%20is%20a%20critical%20field%20in%20computer%20vision%20focusing%20on%0Aaccurately%20identifying%20and%20locating%20specific%20objects%20in%20images%20or%20videos.%0ATraditional%20methods%20for%20object%20detection%20rely%20on%20large%20labeled%20training%0Adatasets%20for%20each%20object%20category%2C%20which%20can%20be%20time-consuming%20and%20expensive%20to%0Acollect%20and%20annotate.%20To%20address%20this%20issue%2C%20researchers%20have%20introduced%0Afew-shot%20object%20detection%20%28FSOD%29%20approaches%20that%20merge%20few-shot%20learning%20and%0Aobject%20detection%20principles.%20These%20approaches%20allow%20models%20to%20quickly%20adapt%20to%0Anew%20object%20categories%20with%20only%20a%20few%20annotated%20samples.%20While%20traditional%20FSOD%0Amethods%20have%20been%20studied%20before%2C%20this%20survey%20paper%20comprehensively%20reviews%0AFSOD%20research%20with%20a%20specific%20focus%20on%20covering%20different%20FSOD%20settings%20such%20as%0Astandard%20FSOD%2C%20generalized%20FSOD%2C%20incremental%20FSOD%2C%20open-set%20FSOD%2C%20and%20domain%0Aadaptive%20FSOD.%20These%20approaches%20play%20a%20vital%20role%20in%20reducing%20the%20reliance%20on%0Aextensive%20labeled%20datasets%2C%20particularly%20as%20the%20need%20for%20efficient%20machine%0Alearning%20models%20continues%20to%20rise.%20This%20survey%20paper%20aims%20to%20provide%20a%0Acomprehensive%20understanding%20of%20the%20above-mentioned%20few-shot%20settings%20and%0Aexplore%20the%20methodologies%20for%20each%20FSOD%20task.%20It%20thoroughly%20compares%0Astate-of-the-art%20methods%20across%20different%20FSOD%20settings%2C%20analyzing%20them%20in%0Adetail%20based%20on%20their%20evaluation%20protocols.%20Additionally%2C%20it%20offers%20insights%0Ainto%20their%20applications%2C%20challenges%2C%20and%20potential%20future%20directions%20in%20the%0Aevolving%20field%20of%20object%20detection%20with%20limited%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14249v1&entry.124074799=Read"},
{"title": "A Brief Analysis of the Iterative Next Boundary Detection Network for\n  Tree Rings Delineation in Images of Pinus taeda", "author": "Henry Marichal and Gregory Randall", "abstract": "  This work presents the INBD network proposed by Gillert et al. in CVPR-2023\nand studies its application for delineating tree rings in RGB images of Pinus\ntaeda cross sections captured by a smartphone (UruDendro dataset), which are\nimages with different characteristics from the ones used to train the method.\nThe INBD network operates in two stages: first, it segments the background,\npith, and ring boundaries. In the second stage, the image is transformed into\npolar coordinates, and ring boundaries are iteratively segmented from the pith\nto the bark. Both stages are based on the U-Net architecture. The method\nachieves an F-Score of 77.5, a mAR of 0.540, and an ARAND of 0.205 on the\nevaluation set. The code for the experiments is available at\nhttps://github.com/hmarichal93/mlbrief_inbd.\n", "link": "http://arxiv.org/abs/2408.14343v1", "date": "2024-08-26", "relevancy": 1.8625, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4878}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4517}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.445}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Brief%20Analysis%20of%20the%20Iterative%20Next%20Boundary%20Detection%20Network%20for%0A%20%20Tree%20Rings%20Delineation%20in%20Images%20of%20Pinus%20taeda&body=Title%3A%20A%20Brief%20Analysis%20of%20the%20Iterative%20Next%20Boundary%20Detection%20Network%20for%0A%20%20Tree%20Rings%20Delineation%20in%20Images%20of%20Pinus%20taeda%0AAuthor%3A%20Henry%20Marichal%20and%20Gregory%20Randall%0AAbstract%3A%20%20%20This%20work%20presents%20the%20INBD%20network%20proposed%20by%20Gillert%20et%20al.%20in%20CVPR-2023%0Aand%20studies%20its%20application%20for%20delineating%20tree%20rings%20in%20RGB%20images%20of%20Pinus%0Ataeda%20cross%20sections%20captured%20by%20a%20smartphone%20%28UruDendro%20dataset%29%2C%20which%20are%0Aimages%20with%20different%20characteristics%20from%20the%20ones%20used%20to%20train%20the%20method.%0AThe%20INBD%20network%20operates%20in%20two%20stages%3A%20first%2C%20it%20segments%20the%20background%2C%0Apith%2C%20and%20ring%20boundaries.%20In%20the%20second%20stage%2C%20the%20image%20is%20transformed%20into%0Apolar%20coordinates%2C%20and%20ring%20boundaries%20are%20iteratively%20segmented%20from%20the%20pith%0Ato%20the%20bark.%20Both%20stages%20are%20based%20on%20the%20U-Net%20architecture.%20The%20method%0Aachieves%20an%20F-Score%20of%2077.5%2C%20a%20mAR%20of%200.540%2C%20and%20an%20ARAND%20of%200.205%20on%20the%0Aevaluation%20set.%20The%20code%20for%20the%20experiments%20is%20available%20at%0Ahttps%3A//github.com/hmarichal93/mlbrief_inbd.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14343v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Brief%2520Analysis%2520of%2520the%2520Iterative%2520Next%2520Boundary%2520Detection%2520Network%2520for%250A%2520%2520Tree%2520Rings%2520Delineation%2520in%2520Images%2520of%2520Pinus%2520taeda%26entry.906535625%3DHenry%2520Marichal%2520and%2520Gregory%2520Randall%26entry.1292438233%3D%2520%2520This%2520work%2520presents%2520the%2520INBD%2520network%2520proposed%2520by%2520Gillert%2520et%2520al.%2520in%2520CVPR-2023%250Aand%2520studies%2520its%2520application%2520for%2520delineating%2520tree%2520rings%2520in%2520RGB%2520images%2520of%2520Pinus%250Ataeda%2520cross%2520sections%2520captured%2520by%2520a%2520smartphone%2520%2528UruDendro%2520dataset%2529%252C%2520which%2520are%250Aimages%2520with%2520different%2520characteristics%2520from%2520the%2520ones%2520used%2520to%2520train%2520the%2520method.%250AThe%2520INBD%2520network%2520operates%2520in%2520two%2520stages%253A%2520first%252C%2520it%2520segments%2520the%2520background%252C%250Apith%252C%2520and%2520ring%2520boundaries.%2520In%2520the%2520second%2520stage%252C%2520the%2520image%2520is%2520transformed%2520into%250Apolar%2520coordinates%252C%2520and%2520ring%2520boundaries%2520are%2520iteratively%2520segmented%2520from%2520the%2520pith%250Ato%2520the%2520bark.%2520Both%2520stages%2520are%2520based%2520on%2520the%2520U-Net%2520architecture.%2520The%2520method%250Aachieves%2520an%2520F-Score%2520of%252077.5%252C%2520a%2520mAR%2520of%25200.540%252C%2520and%2520an%2520ARAND%2520of%25200.205%2520on%2520the%250Aevaluation%2520set.%2520The%2520code%2520for%2520the%2520experiments%2520is%2520available%2520at%250Ahttps%253A//github.com/hmarichal93/mlbrief_inbd.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14343v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Brief%20Analysis%20of%20the%20Iterative%20Next%20Boundary%20Detection%20Network%20for%0A%20%20Tree%20Rings%20Delineation%20in%20Images%20of%20Pinus%20taeda&entry.906535625=Henry%20Marichal%20and%20Gregory%20Randall&entry.1292438233=%20%20This%20work%20presents%20the%20INBD%20network%20proposed%20by%20Gillert%20et%20al.%20in%20CVPR-2023%0Aand%20studies%20its%20application%20for%20delineating%20tree%20rings%20in%20RGB%20images%20of%20Pinus%0Ataeda%20cross%20sections%20captured%20by%20a%20smartphone%20%28UruDendro%20dataset%29%2C%20which%20are%0Aimages%20with%20different%20characteristics%20from%20the%20ones%20used%20to%20train%20the%20method.%0AThe%20INBD%20network%20operates%20in%20two%20stages%3A%20first%2C%20it%20segments%20the%20background%2C%0Apith%2C%20and%20ring%20boundaries.%20In%20the%20second%20stage%2C%20the%20image%20is%20transformed%20into%0Apolar%20coordinates%2C%20and%20ring%20boundaries%20are%20iteratively%20segmented%20from%20the%20pith%0Ato%20the%20bark.%20Both%20stages%20are%20based%20on%20the%20U-Net%20architecture.%20The%20method%0Aachieves%20an%20F-Score%20of%2077.5%2C%20a%20mAR%20of%200.540%2C%20and%20an%20ARAND%20of%200.205%20on%20the%0Aevaluation%20set.%20The%20code%20for%20the%20experiments%20is%20available%20at%0Ahttps%3A//github.com/hmarichal93/mlbrief_inbd.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14343v1&entry.124074799=Read"},
{"title": "Efficient Generation of Hidden Outliers for Improved Outlier Detection", "author": "Jose Cribeiro-Ramallo and Vadim Arzamasov and Klemens B\u00f6hm", "abstract": "  Outlier generation is a popular technique used for solving important outlier\ndetection tasks. Generating outliers with realistic behavior is challenging.\nPopular existing methods tend to disregard the 'multiple views' property of\noutliers in high-dimensional spaces. The only existing method accounting for\nthis property falls short in efficiency and effectiveness. We propose BISECT, a\nnew outlier generation method that creates realistic outliers mimicking said\nproperty. To do so, BISECT employs a novel proposition introduced in this\narticle stating how to efficiently generate said realistic outliers. Our method\nhas better guarantees and complexity than the current methodology for\nrecreating 'multiple views'. We use the synthetic outliers generated by BISECT\nto effectively enhance outlier detection in diverse datasets, for multiple use\ncases. For instance, oversampling with BISECT reduced the error by up to 3\ntimes when compared with the baselines.\n", "link": "http://arxiv.org/abs/2402.03846v2", "date": "2024-08-26", "relevancy": 1.862, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4742}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.464}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Generation%20of%20Hidden%20Outliers%20for%20Improved%20Outlier%20Detection&body=Title%3A%20Efficient%20Generation%20of%20Hidden%20Outliers%20for%20Improved%20Outlier%20Detection%0AAuthor%3A%20Jose%20Cribeiro-Ramallo%20and%20Vadim%20Arzamasov%20and%20Klemens%20B%C3%B6hm%0AAbstract%3A%20%20%20Outlier%20generation%20is%20a%20popular%20technique%20used%20for%20solving%20important%20outlier%0Adetection%20tasks.%20Generating%20outliers%20with%20realistic%20behavior%20is%20challenging.%0APopular%20existing%20methods%20tend%20to%20disregard%20the%20%27multiple%20views%27%20property%20of%0Aoutliers%20in%20high-dimensional%20spaces.%20The%20only%20existing%20method%20accounting%20for%0Athis%20property%20falls%20short%20in%20efficiency%20and%20effectiveness.%20We%20propose%20BISECT%2C%20a%0Anew%20outlier%20generation%20method%20that%20creates%20realistic%20outliers%20mimicking%20said%0Aproperty.%20To%20do%20so%2C%20BISECT%20employs%20a%20novel%20proposition%20introduced%20in%20this%0Aarticle%20stating%20how%20to%20efficiently%20generate%20said%20realistic%20outliers.%20Our%20method%0Ahas%20better%20guarantees%20and%20complexity%20than%20the%20current%20methodology%20for%0Arecreating%20%27multiple%20views%27.%20We%20use%20the%20synthetic%20outliers%20generated%20by%20BISECT%0Ato%20effectively%20enhance%20outlier%20detection%20in%20diverse%20datasets%2C%20for%20multiple%20use%0Acases.%20For%20instance%2C%20oversampling%20with%20BISECT%20reduced%20the%20error%20by%20up%20to%203%0Atimes%20when%20compared%20with%20the%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03846v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Generation%2520of%2520Hidden%2520Outliers%2520for%2520Improved%2520Outlier%2520Detection%26entry.906535625%3DJose%2520Cribeiro-Ramallo%2520and%2520Vadim%2520Arzamasov%2520and%2520Klemens%2520B%25C3%25B6hm%26entry.1292438233%3D%2520%2520Outlier%2520generation%2520is%2520a%2520popular%2520technique%2520used%2520for%2520solving%2520important%2520outlier%250Adetection%2520tasks.%2520Generating%2520outliers%2520with%2520realistic%2520behavior%2520is%2520challenging.%250APopular%2520existing%2520methods%2520tend%2520to%2520disregard%2520the%2520%2527multiple%2520views%2527%2520property%2520of%250Aoutliers%2520in%2520high-dimensional%2520spaces.%2520The%2520only%2520existing%2520method%2520accounting%2520for%250Athis%2520property%2520falls%2520short%2520in%2520efficiency%2520and%2520effectiveness.%2520We%2520propose%2520BISECT%252C%2520a%250Anew%2520outlier%2520generation%2520method%2520that%2520creates%2520realistic%2520outliers%2520mimicking%2520said%250Aproperty.%2520To%2520do%2520so%252C%2520BISECT%2520employs%2520a%2520novel%2520proposition%2520introduced%2520in%2520this%250Aarticle%2520stating%2520how%2520to%2520efficiently%2520generate%2520said%2520realistic%2520outliers.%2520Our%2520method%250Ahas%2520better%2520guarantees%2520and%2520complexity%2520than%2520the%2520current%2520methodology%2520for%250Arecreating%2520%2527multiple%2520views%2527.%2520We%2520use%2520the%2520synthetic%2520outliers%2520generated%2520by%2520BISECT%250Ato%2520effectively%2520enhance%2520outlier%2520detection%2520in%2520diverse%2520datasets%252C%2520for%2520multiple%2520use%250Acases.%2520For%2520instance%252C%2520oversampling%2520with%2520BISECT%2520reduced%2520the%2520error%2520by%2520up%2520to%25203%250Atimes%2520when%2520compared%2520with%2520the%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.03846v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Generation%20of%20Hidden%20Outliers%20for%20Improved%20Outlier%20Detection&entry.906535625=Jose%20Cribeiro-Ramallo%20and%20Vadim%20Arzamasov%20and%20Klemens%20B%C3%B6hm&entry.1292438233=%20%20Outlier%20generation%20is%20a%20popular%20technique%20used%20for%20solving%20important%20outlier%0Adetection%20tasks.%20Generating%20outliers%20with%20realistic%20behavior%20is%20challenging.%0APopular%20existing%20methods%20tend%20to%20disregard%20the%20%27multiple%20views%27%20property%20of%0Aoutliers%20in%20high-dimensional%20spaces.%20The%20only%20existing%20method%20accounting%20for%0Athis%20property%20falls%20short%20in%20efficiency%20and%20effectiveness.%20We%20propose%20BISECT%2C%20a%0Anew%20outlier%20generation%20method%20that%20creates%20realistic%20outliers%20mimicking%20said%0Aproperty.%20To%20do%20so%2C%20BISECT%20employs%20a%20novel%20proposition%20introduced%20in%20this%0Aarticle%20stating%20how%20to%20efficiently%20generate%20said%20realistic%20outliers.%20Our%20method%0Ahas%20better%20guarantees%20and%20complexity%20than%20the%20current%20methodology%20for%0Arecreating%20%27multiple%20views%27.%20We%20use%20the%20synthetic%20outliers%20generated%20by%20BISECT%0Ato%20effectively%20enhance%20outlier%20detection%20in%20diverse%20datasets%2C%20for%20multiple%20use%0Acases.%20For%20instance%2C%20oversampling%20with%20BISECT%20reduced%20the%20error%20by%20up%20to%203%0Atimes%20when%20compared%20with%20the%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03846v2&entry.124074799=Read"},
{"title": "An Evaluation of Explanation Methods for Black-Box Detectors of\n  Machine-Generated Text", "author": "Loris Schoenegger and Yuxi Xia and Benjamin Roth", "abstract": "  The increasing difficulty to distinguish language-model-generated from\nhuman-written text has led to the development of detectors of machine-generated\ntext (MGT). However, in many contexts, a black-box prediction is not\nsufficient, it is equally important to know on what grounds a detector made\nthat prediction. Explanation methods that estimate feature importance promise\nto provide indications of which parts of an input are used by classifiers for\nprediction. However, the quality of different explanation methods has not\npreviously been assessed for detectors of MGT. This study conducts the first\nsystematic evaluation of explanation quality for this task. The dimensions of\nfaithfulness and stability are assessed with five automated experiments, and\nusefulness is evaluated in a user study. We use a dataset of ChatGPT-generated\nand human-written documents, and pair predictions of three existing\nlanguage-model-based detectors with the corresponding SHAP, LIME, and Anchor\nexplanations. We find that SHAP performs best in terms of faithfulness,\nstability, and in helping users to predict the detector's behavior. In\ncontrast, LIME, perceived as most useful by users, scores the worst in terms of\nuser performance at predicting the detectors' behavior.\n", "link": "http://arxiv.org/abs/2408.14252v1", "date": "2024-08-26", "relevancy": 1.8555, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.507}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4609}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Evaluation%20of%20Explanation%20Methods%20for%20Black-Box%20Detectors%20of%0A%20%20Machine-Generated%20Text&body=Title%3A%20An%20Evaluation%20of%20Explanation%20Methods%20for%20Black-Box%20Detectors%20of%0A%20%20Machine-Generated%20Text%0AAuthor%3A%20Loris%20Schoenegger%20and%20Yuxi%20Xia%20and%20Benjamin%20Roth%0AAbstract%3A%20%20%20The%20increasing%20difficulty%20to%20distinguish%20language-model-generated%20from%0Ahuman-written%20text%20has%20led%20to%20the%20development%20of%20detectors%20of%20machine-generated%0Atext%20%28MGT%29.%20However%2C%20in%20many%20contexts%2C%20a%20black-box%20prediction%20is%20not%0Asufficient%2C%20it%20is%20equally%20important%20to%20know%20on%20what%20grounds%20a%20detector%20made%0Athat%20prediction.%20Explanation%20methods%20that%20estimate%20feature%20importance%20promise%0Ato%20provide%20indications%20of%20which%20parts%20of%20an%20input%20are%20used%20by%20classifiers%20for%0Aprediction.%20However%2C%20the%20quality%20of%20different%20explanation%20methods%20has%20not%0Apreviously%20been%20assessed%20for%20detectors%20of%20MGT.%20This%20study%20conducts%20the%20first%0Asystematic%20evaluation%20of%20explanation%20quality%20for%20this%20task.%20The%20dimensions%20of%0Afaithfulness%20and%20stability%20are%20assessed%20with%20five%20automated%20experiments%2C%20and%0Ausefulness%20is%20evaluated%20in%20a%20user%20study.%20We%20use%20a%20dataset%20of%20ChatGPT-generated%0Aand%20human-written%20documents%2C%20and%20pair%20predictions%20of%20three%20existing%0Alanguage-model-based%20detectors%20with%20the%20corresponding%20SHAP%2C%20LIME%2C%20and%20Anchor%0Aexplanations.%20We%20find%20that%20SHAP%20performs%20best%20in%20terms%20of%20faithfulness%2C%0Astability%2C%20and%20in%20helping%20users%20to%20predict%20the%20detector%27s%20behavior.%20In%0Acontrast%2C%20LIME%2C%20perceived%20as%20most%20useful%20by%20users%2C%20scores%20the%20worst%20in%20terms%20of%0Auser%20performance%20at%20predicting%20the%20detectors%27%20behavior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14252v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Evaluation%2520of%2520Explanation%2520Methods%2520for%2520Black-Box%2520Detectors%2520of%250A%2520%2520Machine-Generated%2520Text%26entry.906535625%3DLoris%2520Schoenegger%2520and%2520Yuxi%2520Xia%2520and%2520Benjamin%2520Roth%26entry.1292438233%3D%2520%2520The%2520increasing%2520difficulty%2520to%2520distinguish%2520language-model-generated%2520from%250Ahuman-written%2520text%2520has%2520led%2520to%2520the%2520development%2520of%2520detectors%2520of%2520machine-generated%250Atext%2520%2528MGT%2529.%2520However%252C%2520in%2520many%2520contexts%252C%2520a%2520black-box%2520prediction%2520is%2520not%250Asufficient%252C%2520it%2520is%2520equally%2520important%2520to%2520know%2520on%2520what%2520grounds%2520a%2520detector%2520made%250Athat%2520prediction.%2520Explanation%2520methods%2520that%2520estimate%2520feature%2520importance%2520promise%250Ato%2520provide%2520indications%2520of%2520which%2520parts%2520of%2520an%2520input%2520are%2520used%2520by%2520classifiers%2520for%250Aprediction.%2520However%252C%2520the%2520quality%2520of%2520different%2520explanation%2520methods%2520has%2520not%250Apreviously%2520been%2520assessed%2520for%2520detectors%2520of%2520MGT.%2520This%2520study%2520conducts%2520the%2520first%250Asystematic%2520evaluation%2520of%2520explanation%2520quality%2520for%2520this%2520task.%2520The%2520dimensions%2520of%250Afaithfulness%2520and%2520stability%2520are%2520assessed%2520with%2520five%2520automated%2520experiments%252C%2520and%250Ausefulness%2520is%2520evaluated%2520in%2520a%2520user%2520study.%2520We%2520use%2520a%2520dataset%2520of%2520ChatGPT-generated%250Aand%2520human-written%2520documents%252C%2520and%2520pair%2520predictions%2520of%2520three%2520existing%250Alanguage-model-based%2520detectors%2520with%2520the%2520corresponding%2520SHAP%252C%2520LIME%252C%2520and%2520Anchor%250Aexplanations.%2520We%2520find%2520that%2520SHAP%2520performs%2520best%2520in%2520terms%2520of%2520faithfulness%252C%250Astability%252C%2520and%2520in%2520helping%2520users%2520to%2520predict%2520the%2520detector%2527s%2520behavior.%2520In%250Acontrast%252C%2520LIME%252C%2520perceived%2520as%2520most%2520useful%2520by%2520users%252C%2520scores%2520the%2520worst%2520in%2520terms%2520of%250Auser%2520performance%2520at%2520predicting%2520the%2520detectors%2527%2520behavior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14252v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Evaluation%20of%20Explanation%20Methods%20for%20Black-Box%20Detectors%20of%0A%20%20Machine-Generated%20Text&entry.906535625=Loris%20Schoenegger%20and%20Yuxi%20Xia%20and%20Benjamin%20Roth&entry.1292438233=%20%20The%20increasing%20difficulty%20to%20distinguish%20language-model-generated%20from%0Ahuman-written%20text%20has%20led%20to%20the%20development%20of%20detectors%20of%20machine-generated%0Atext%20%28MGT%29.%20However%2C%20in%20many%20contexts%2C%20a%20black-box%20prediction%20is%20not%0Asufficient%2C%20it%20is%20equally%20important%20to%20know%20on%20what%20grounds%20a%20detector%20made%0Athat%20prediction.%20Explanation%20methods%20that%20estimate%20feature%20importance%20promise%0Ato%20provide%20indications%20of%20which%20parts%20of%20an%20input%20are%20used%20by%20classifiers%20for%0Aprediction.%20However%2C%20the%20quality%20of%20different%20explanation%20methods%20has%20not%0Apreviously%20been%20assessed%20for%20detectors%20of%20MGT.%20This%20study%20conducts%20the%20first%0Asystematic%20evaluation%20of%20explanation%20quality%20for%20this%20task.%20The%20dimensions%20of%0Afaithfulness%20and%20stability%20are%20assessed%20with%20five%20automated%20experiments%2C%20and%0Ausefulness%20is%20evaluated%20in%20a%20user%20study.%20We%20use%20a%20dataset%20of%20ChatGPT-generated%0Aand%20human-written%20documents%2C%20and%20pair%20predictions%20of%20three%20existing%0Alanguage-model-based%20detectors%20with%20the%20corresponding%20SHAP%2C%20LIME%2C%20and%20Anchor%0Aexplanations.%20We%20find%20that%20SHAP%20performs%20best%20in%20terms%20of%20faithfulness%2C%0Astability%2C%20and%20in%20helping%20users%20to%20predict%20the%20detector%27s%20behavior.%20In%0Acontrast%2C%20LIME%2C%20perceived%20as%20most%20useful%20by%20users%2C%20scores%20the%20worst%20in%20terms%20of%0Auser%20performance%20at%20predicting%20the%20detectors%27%20behavior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14252v1&entry.124074799=Read"},
{"title": "GR-MG: Leveraging Partially Annotated Data via Multi-Modal Goal\n  Conditioned Policy", "author": "Peiyan Li and Hongtao Wu and Yan Huang and Chilam Cheang and Liang Wang and Tao Kong", "abstract": "  The robotics community has consistently aimed to achieve generalizable robot\nmanipulation with flexible natural language instructions. One of the primary\nchallenges is that obtaining robot data fully annotated with both actions and\ntexts is time-consuming and labor-intensive. However, partially annotated data,\nsuch as human activity videos without action labels and robot play data without\nlanguage labels, is much easier to collect. Can we leverage these data to\nenhance the generalization capability of robots? In this paper, we propose\nGR-MG, a novel method which supports conditioning on both a language\ninstruction and a goal image. During training, GR-MG samples goal images from\ntrajectories and conditions on both the text and the goal image or solely on\nthe image when text is unavailable. During inference, where only the text is\nprovided, GR-MG generates the goal image via a diffusion-based image-editing\nmodel and condition on both the text and the generated image. This approach\nenables GR-MG to leverage large amounts of partially annotated data while still\nusing language to flexibly specify tasks. To generate accurate goal images, we\npropose a novel progress-guided goal image generation model which injects task\nprogress information into the generation process, significantly improving the\nfidelity and the performance. In simulation experiments, GR-MG improves the\naverage number of tasks completed in a row of 5 from 3.35 to 4.04. In\nreal-robot experiments, GR-MG is able to perform 47 different tasks and\nimproves the success rate from 62.5% to 75.0% and 42.4% to 57.6% in simple and\ngeneralization settings, respectively. Code and checkpoints will be available\nat the project page: https://gr-mg.github.io/.\n", "link": "http://arxiv.org/abs/2408.14368v1", "date": "2024-08-26", "relevancy": 1.8514, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6247}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6197}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6131}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GR-MG%3A%20Leveraging%20Partially%20Annotated%20Data%20via%20Multi-Modal%20Goal%0A%20%20Conditioned%20Policy&body=Title%3A%20GR-MG%3A%20Leveraging%20Partially%20Annotated%20Data%20via%20Multi-Modal%20Goal%0A%20%20Conditioned%20Policy%0AAuthor%3A%20Peiyan%20Li%20and%20Hongtao%20Wu%20and%20Yan%20Huang%20and%20Chilam%20Cheang%20and%20Liang%20Wang%20and%20Tao%20Kong%0AAbstract%3A%20%20%20The%20robotics%20community%20has%20consistently%20aimed%20to%20achieve%20generalizable%20robot%0Amanipulation%20with%20flexible%20natural%20language%20instructions.%20One%20of%20the%20primary%0Achallenges%20is%20that%20obtaining%20robot%20data%20fully%20annotated%20with%20both%20actions%20and%0Atexts%20is%20time-consuming%20and%20labor-intensive.%20However%2C%20partially%20annotated%20data%2C%0Asuch%20as%20human%20activity%20videos%20without%20action%20labels%20and%20robot%20play%20data%20without%0Alanguage%20labels%2C%20is%20much%20easier%20to%20collect.%20Can%20we%20leverage%20these%20data%20to%0Aenhance%20the%20generalization%20capability%20of%20robots%3F%20In%20this%20paper%2C%20we%20propose%0AGR-MG%2C%20a%20novel%20method%20which%20supports%20conditioning%20on%20both%20a%20language%0Ainstruction%20and%20a%20goal%20image.%20During%20training%2C%20GR-MG%20samples%20goal%20images%20from%0Atrajectories%20and%20conditions%20on%20both%20the%20text%20and%20the%20goal%20image%20or%20solely%20on%0Athe%20image%20when%20text%20is%20unavailable.%20During%20inference%2C%20where%20only%20the%20text%20is%0Aprovided%2C%20GR-MG%20generates%20the%20goal%20image%20via%20a%20diffusion-based%20image-editing%0Amodel%20and%20condition%20on%20both%20the%20text%20and%20the%20generated%20image.%20This%20approach%0Aenables%20GR-MG%20to%20leverage%20large%20amounts%20of%20partially%20annotated%20data%20while%20still%0Ausing%20language%20to%20flexibly%20specify%20tasks.%20To%20generate%20accurate%20goal%20images%2C%20we%0Apropose%20a%20novel%20progress-guided%20goal%20image%20generation%20model%20which%20injects%20task%0Aprogress%20information%20into%20the%20generation%20process%2C%20significantly%20improving%20the%0Afidelity%20and%20the%20performance.%20In%20simulation%20experiments%2C%20GR-MG%20improves%20the%0Aaverage%20number%20of%20tasks%20completed%20in%20a%20row%20of%205%20from%203.35%20to%204.04.%20In%0Areal-robot%20experiments%2C%20GR-MG%20is%20able%20to%20perform%2047%20different%20tasks%20and%0Aimproves%20the%20success%20rate%20from%2062.5%25%20to%2075.0%25%20and%2042.4%25%20to%2057.6%25%20in%20simple%20and%0Ageneralization%20settings%2C%20respectively.%20Code%20and%20checkpoints%20will%20be%20available%0Aat%20the%20project%20page%3A%20https%3A//gr-mg.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14368v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGR-MG%253A%2520Leveraging%2520Partially%2520Annotated%2520Data%2520via%2520Multi-Modal%2520Goal%250A%2520%2520Conditioned%2520Policy%26entry.906535625%3DPeiyan%2520Li%2520and%2520Hongtao%2520Wu%2520and%2520Yan%2520Huang%2520and%2520Chilam%2520Cheang%2520and%2520Liang%2520Wang%2520and%2520Tao%2520Kong%26entry.1292438233%3D%2520%2520The%2520robotics%2520community%2520has%2520consistently%2520aimed%2520to%2520achieve%2520generalizable%2520robot%250Amanipulation%2520with%2520flexible%2520natural%2520language%2520instructions.%2520One%2520of%2520the%2520primary%250Achallenges%2520is%2520that%2520obtaining%2520robot%2520data%2520fully%2520annotated%2520with%2520both%2520actions%2520and%250Atexts%2520is%2520time-consuming%2520and%2520labor-intensive.%2520However%252C%2520partially%2520annotated%2520data%252C%250Asuch%2520as%2520human%2520activity%2520videos%2520without%2520action%2520labels%2520and%2520robot%2520play%2520data%2520without%250Alanguage%2520labels%252C%2520is%2520much%2520easier%2520to%2520collect.%2520Can%2520we%2520leverage%2520these%2520data%2520to%250Aenhance%2520the%2520generalization%2520capability%2520of%2520robots%253F%2520In%2520this%2520paper%252C%2520we%2520propose%250AGR-MG%252C%2520a%2520novel%2520method%2520which%2520supports%2520conditioning%2520on%2520both%2520a%2520language%250Ainstruction%2520and%2520a%2520goal%2520image.%2520During%2520training%252C%2520GR-MG%2520samples%2520goal%2520images%2520from%250Atrajectories%2520and%2520conditions%2520on%2520both%2520the%2520text%2520and%2520the%2520goal%2520image%2520or%2520solely%2520on%250Athe%2520image%2520when%2520text%2520is%2520unavailable.%2520During%2520inference%252C%2520where%2520only%2520the%2520text%2520is%250Aprovided%252C%2520GR-MG%2520generates%2520the%2520goal%2520image%2520via%2520a%2520diffusion-based%2520image-editing%250Amodel%2520and%2520condition%2520on%2520both%2520the%2520text%2520and%2520the%2520generated%2520image.%2520This%2520approach%250Aenables%2520GR-MG%2520to%2520leverage%2520large%2520amounts%2520of%2520partially%2520annotated%2520data%2520while%2520still%250Ausing%2520language%2520to%2520flexibly%2520specify%2520tasks.%2520To%2520generate%2520accurate%2520goal%2520images%252C%2520we%250Apropose%2520a%2520novel%2520progress-guided%2520goal%2520image%2520generation%2520model%2520which%2520injects%2520task%250Aprogress%2520information%2520into%2520the%2520generation%2520process%252C%2520significantly%2520improving%2520the%250Afidelity%2520and%2520the%2520performance.%2520In%2520simulation%2520experiments%252C%2520GR-MG%2520improves%2520the%250Aaverage%2520number%2520of%2520tasks%2520completed%2520in%2520a%2520row%2520of%25205%2520from%25203.35%2520to%25204.04.%2520In%250Areal-robot%2520experiments%252C%2520GR-MG%2520is%2520able%2520to%2520perform%252047%2520different%2520tasks%2520and%250Aimproves%2520the%2520success%2520rate%2520from%252062.5%2525%2520to%252075.0%2525%2520and%252042.4%2525%2520to%252057.6%2525%2520in%2520simple%2520and%250Ageneralization%2520settings%252C%2520respectively.%2520Code%2520and%2520checkpoints%2520will%2520be%2520available%250Aat%2520the%2520project%2520page%253A%2520https%253A//gr-mg.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14368v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GR-MG%3A%20Leveraging%20Partially%20Annotated%20Data%20via%20Multi-Modal%20Goal%0A%20%20Conditioned%20Policy&entry.906535625=Peiyan%20Li%20and%20Hongtao%20Wu%20and%20Yan%20Huang%20and%20Chilam%20Cheang%20and%20Liang%20Wang%20and%20Tao%20Kong&entry.1292438233=%20%20The%20robotics%20community%20has%20consistently%20aimed%20to%20achieve%20generalizable%20robot%0Amanipulation%20with%20flexible%20natural%20language%20instructions.%20One%20of%20the%20primary%0Achallenges%20is%20that%20obtaining%20robot%20data%20fully%20annotated%20with%20both%20actions%20and%0Atexts%20is%20time-consuming%20and%20labor-intensive.%20However%2C%20partially%20annotated%20data%2C%0Asuch%20as%20human%20activity%20videos%20without%20action%20labels%20and%20robot%20play%20data%20without%0Alanguage%20labels%2C%20is%20much%20easier%20to%20collect.%20Can%20we%20leverage%20these%20data%20to%0Aenhance%20the%20generalization%20capability%20of%20robots%3F%20In%20this%20paper%2C%20we%20propose%0AGR-MG%2C%20a%20novel%20method%20which%20supports%20conditioning%20on%20both%20a%20language%0Ainstruction%20and%20a%20goal%20image.%20During%20training%2C%20GR-MG%20samples%20goal%20images%20from%0Atrajectories%20and%20conditions%20on%20both%20the%20text%20and%20the%20goal%20image%20or%20solely%20on%0Athe%20image%20when%20text%20is%20unavailable.%20During%20inference%2C%20where%20only%20the%20text%20is%0Aprovided%2C%20GR-MG%20generates%20the%20goal%20image%20via%20a%20diffusion-based%20image-editing%0Amodel%20and%20condition%20on%20both%20the%20text%20and%20the%20generated%20image.%20This%20approach%0Aenables%20GR-MG%20to%20leverage%20large%20amounts%20of%20partially%20annotated%20data%20while%20still%0Ausing%20language%20to%20flexibly%20specify%20tasks.%20To%20generate%20accurate%20goal%20images%2C%20we%0Apropose%20a%20novel%20progress-guided%20goal%20image%20generation%20model%20which%20injects%20task%0Aprogress%20information%20into%20the%20generation%20process%2C%20significantly%20improving%20the%0Afidelity%20and%20the%20performance.%20In%20simulation%20experiments%2C%20GR-MG%20improves%20the%0Aaverage%20number%20of%20tasks%20completed%20in%20a%20row%20of%205%20from%203.35%20to%204.04.%20In%0Areal-robot%20experiments%2C%20GR-MG%20is%20able%20to%20perform%2047%20different%20tasks%20and%0Aimproves%20the%20success%20rate%20from%2062.5%25%20to%2075.0%25%20and%2042.4%25%20to%2057.6%25%20in%20simple%20and%0Ageneralization%20settings%2C%20respectively.%20Code%20and%20checkpoints%20will%20be%20available%0Aat%20the%20project%20page%3A%20https%3A//gr-mg.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14368v1&entry.124074799=Read"},
{"title": "Demystifying the Recency Heuristic in Temporal-Difference Learning", "author": "Brett Daley and Marlos C. Machado and Martha White", "abstract": "  The recency heuristic in reinforcement learning is the assumption that\nstimuli that occurred closer in time to an acquired reward should be more\nheavily reinforced. The recency heuristic is one of the key assumptions made by\nTD($\\lambda$), which reinforces recent experiences according to an\nexponentially decaying weighting. In fact, all other widely used return\nestimators for TD learning, such as $n$-step returns, satisfy a weaker (i.e.,\nnon-monotonic) recency heuristic. Why is the recency heuristic effective for\ntemporal credit assignment? What happens when credit is assigned in a way that\nviolates this heuristic? In this paper, we analyze the specific mathematical\nimplications of adopting the recency heuristic in TD learning. We prove that\nany return estimator satisfying this heuristic: 1) is guaranteed to converge to\nthe correct value function, 2) has a relatively fast contraction rate, and 3)\nhas a long window of effective credit assignment, yet bounded worst-case\nvariance. We also give a counterexample where on-policy, tabular TD methods\nviolating the recency heuristic diverge. Our results offer some of the first\ntheoretical evidence that credit assignment based on the recency heuristic\nfacilitates learning.\n", "link": "http://arxiv.org/abs/2406.12284v2", "date": "2024-08-26", "relevancy": 1.8352, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4843}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4444}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Demystifying%20the%20Recency%20Heuristic%20in%20Temporal-Difference%20Learning&body=Title%3A%20Demystifying%20the%20Recency%20Heuristic%20in%20Temporal-Difference%20Learning%0AAuthor%3A%20Brett%20Daley%20and%20Marlos%20C.%20Machado%20and%20Martha%20White%0AAbstract%3A%20%20%20The%20recency%20heuristic%20in%20reinforcement%20learning%20is%20the%20assumption%20that%0Astimuli%20that%20occurred%20closer%20in%20time%20to%20an%20acquired%20reward%20should%20be%20more%0Aheavily%20reinforced.%20The%20recency%20heuristic%20is%20one%20of%20the%20key%20assumptions%20made%20by%0ATD%28%24%5Clambda%24%29%2C%20which%20reinforces%20recent%20experiences%20according%20to%20an%0Aexponentially%20decaying%20weighting.%20In%20fact%2C%20all%20other%20widely%20used%20return%0Aestimators%20for%20TD%20learning%2C%20such%20as%20%24n%24-step%20returns%2C%20satisfy%20a%20weaker%20%28i.e.%2C%0Anon-monotonic%29%20recency%20heuristic.%20Why%20is%20the%20recency%20heuristic%20effective%20for%0Atemporal%20credit%20assignment%3F%20What%20happens%20when%20credit%20is%20assigned%20in%20a%20way%20that%0Aviolates%20this%20heuristic%3F%20In%20this%20paper%2C%20we%20analyze%20the%20specific%20mathematical%0Aimplications%20of%20adopting%20the%20recency%20heuristic%20in%20TD%20learning.%20We%20prove%20that%0Aany%20return%20estimator%20satisfying%20this%20heuristic%3A%201%29%20is%20guaranteed%20to%20converge%20to%0Athe%20correct%20value%20function%2C%202%29%20has%20a%20relatively%20fast%20contraction%20rate%2C%20and%203%29%0Ahas%20a%20long%20window%20of%20effective%20credit%20assignment%2C%20yet%20bounded%20worst-case%0Avariance.%20We%20also%20give%20a%20counterexample%20where%20on-policy%2C%20tabular%20TD%20methods%0Aviolating%20the%20recency%20heuristic%20diverge.%20Our%20results%20offer%20some%20of%20the%20first%0Atheoretical%20evidence%20that%20credit%20assignment%20based%20on%20the%20recency%20heuristic%0Afacilitates%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12284v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDemystifying%2520the%2520Recency%2520Heuristic%2520in%2520Temporal-Difference%2520Learning%26entry.906535625%3DBrett%2520Daley%2520and%2520Marlos%2520C.%2520Machado%2520and%2520Martha%2520White%26entry.1292438233%3D%2520%2520The%2520recency%2520heuristic%2520in%2520reinforcement%2520learning%2520is%2520the%2520assumption%2520that%250Astimuli%2520that%2520occurred%2520closer%2520in%2520time%2520to%2520an%2520acquired%2520reward%2520should%2520be%2520more%250Aheavily%2520reinforced.%2520The%2520recency%2520heuristic%2520is%2520one%2520of%2520the%2520key%2520assumptions%2520made%2520by%250ATD%2528%2524%255Clambda%2524%2529%252C%2520which%2520reinforces%2520recent%2520experiences%2520according%2520to%2520an%250Aexponentially%2520decaying%2520weighting.%2520In%2520fact%252C%2520all%2520other%2520widely%2520used%2520return%250Aestimators%2520for%2520TD%2520learning%252C%2520such%2520as%2520%2524n%2524-step%2520returns%252C%2520satisfy%2520a%2520weaker%2520%2528i.e.%252C%250Anon-monotonic%2529%2520recency%2520heuristic.%2520Why%2520is%2520the%2520recency%2520heuristic%2520effective%2520for%250Atemporal%2520credit%2520assignment%253F%2520What%2520happens%2520when%2520credit%2520is%2520assigned%2520in%2520a%2520way%2520that%250Aviolates%2520this%2520heuristic%253F%2520In%2520this%2520paper%252C%2520we%2520analyze%2520the%2520specific%2520mathematical%250Aimplications%2520of%2520adopting%2520the%2520recency%2520heuristic%2520in%2520TD%2520learning.%2520We%2520prove%2520that%250Aany%2520return%2520estimator%2520satisfying%2520this%2520heuristic%253A%25201%2529%2520is%2520guaranteed%2520to%2520converge%2520to%250Athe%2520correct%2520value%2520function%252C%25202%2529%2520has%2520a%2520relatively%2520fast%2520contraction%2520rate%252C%2520and%25203%2529%250Ahas%2520a%2520long%2520window%2520of%2520effective%2520credit%2520assignment%252C%2520yet%2520bounded%2520worst-case%250Avariance.%2520We%2520also%2520give%2520a%2520counterexample%2520where%2520on-policy%252C%2520tabular%2520TD%2520methods%250Aviolating%2520the%2520recency%2520heuristic%2520diverge.%2520Our%2520results%2520offer%2520some%2520of%2520the%2520first%250Atheoretical%2520evidence%2520that%2520credit%2520assignment%2520based%2520on%2520the%2520recency%2520heuristic%250Afacilitates%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12284v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Demystifying%20the%20Recency%20Heuristic%20in%20Temporal-Difference%20Learning&entry.906535625=Brett%20Daley%20and%20Marlos%20C.%20Machado%20and%20Martha%20White&entry.1292438233=%20%20The%20recency%20heuristic%20in%20reinforcement%20learning%20is%20the%20assumption%20that%0Astimuli%20that%20occurred%20closer%20in%20time%20to%20an%20acquired%20reward%20should%20be%20more%0Aheavily%20reinforced.%20The%20recency%20heuristic%20is%20one%20of%20the%20key%20assumptions%20made%20by%0ATD%28%24%5Clambda%24%29%2C%20which%20reinforces%20recent%20experiences%20according%20to%20an%0Aexponentially%20decaying%20weighting.%20In%20fact%2C%20all%20other%20widely%20used%20return%0Aestimators%20for%20TD%20learning%2C%20such%20as%20%24n%24-step%20returns%2C%20satisfy%20a%20weaker%20%28i.e.%2C%0Anon-monotonic%29%20recency%20heuristic.%20Why%20is%20the%20recency%20heuristic%20effective%20for%0Atemporal%20credit%20assignment%3F%20What%20happens%20when%20credit%20is%20assigned%20in%20a%20way%20that%0Aviolates%20this%20heuristic%3F%20In%20this%20paper%2C%20we%20analyze%20the%20specific%20mathematical%0Aimplications%20of%20adopting%20the%20recency%20heuristic%20in%20TD%20learning.%20We%20prove%20that%0Aany%20return%20estimator%20satisfying%20this%20heuristic%3A%201%29%20is%20guaranteed%20to%20converge%20to%0Athe%20correct%20value%20function%2C%202%29%20has%20a%20relatively%20fast%20contraction%20rate%2C%20and%203%29%0Ahas%20a%20long%20window%20of%20effective%20credit%20assignment%2C%20yet%20bounded%20worst-case%0Avariance.%20We%20also%20give%20a%20counterexample%20where%20on-policy%2C%20tabular%20TD%20methods%0Aviolating%20the%20recency%20heuristic%20diverge.%20Our%20results%20offer%20some%20of%20the%20first%0Atheoretical%20evidence%20that%20credit%20assignment%20based%20on%20the%20recency%20heuristic%0Afacilitates%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12284v2&entry.124074799=Read"},
{"title": "Brain Inspired Probabilistic Occupancy Grid Mapping with\n  Hyperdimensional Computing", "author": "Shay Snyder and Andrew Capodieci and David Gorsich and Maryam Parsa", "abstract": "  Real-time robotic systems require advanced perception, computation, and\naction capability. However, the main bottleneck in current autonomous systems\nis the trade-off between computational capability, energy efficiency and model\ndeterminism. World modeling, a key objective of many robotic systems, commonly\nuses occupancy grid mapping (OGM) as the first step towards building an\nend-to-end robotic system with perception, planning, autonomous maneuvering,\nand decision making capabilities. OGM divides the environment into discrete\ncells and assigns probability values to attributes such as occupancy and\ntraversability. Existing methods fall into two categories: traditional methods\nand neural methods. Traditional methods rely on dense statistical calculations,\nwhile neural methods employ deep learning for probabilistic information\nprocessing. Recent works formulate a deterministic theory of neural computation\nat the intersection of cognitive science and vector symbolic architectures. In\nthis study, we propose a Fourier-based hyperdimensional OGM system, VSA-OGM,\ncombined with a novel application of Shannon entropy that retains the\ninterpretability and stability of traditional methods along with the improved\ncomputational efficiency of neural methods. Our approach, validated across\nmultiple datasets, achieves similar accuracy to covariant traditional methods\nwhile approximately reducing latency by 200x and memory by 1000x. Compared to\ninvariant traditional methods, we see similar accuracy values while reducing\nlatency by 3.7x. Moreover, we achieve 1.5x latency reductions compared to\nneural methods while eliminating the need for domain-specific model training.\n", "link": "http://arxiv.org/abs/2408.09066v2", "date": "2024-08-26", "relevancy": 1.8312, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6616}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6002}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5846}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brain%20Inspired%20Probabilistic%20Occupancy%20Grid%20Mapping%20with%0A%20%20Hyperdimensional%20Computing&body=Title%3A%20Brain%20Inspired%20Probabilistic%20Occupancy%20Grid%20Mapping%20with%0A%20%20Hyperdimensional%20Computing%0AAuthor%3A%20Shay%20Snyder%20and%20Andrew%20Capodieci%20and%20David%20Gorsich%20and%20Maryam%20Parsa%0AAbstract%3A%20%20%20Real-time%20robotic%20systems%20require%20advanced%20perception%2C%20computation%2C%20and%0Aaction%20capability.%20However%2C%20the%20main%20bottleneck%20in%20current%20autonomous%20systems%0Ais%20the%20trade-off%20between%20computational%20capability%2C%20energy%20efficiency%20and%20model%0Adeterminism.%20World%20modeling%2C%20a%20key%20objective%20of%20many%20robotic%20systems%2C%20commonly%0Auses%20occupancy%20grid%20mapping%20%28OGM%29%20as%20the%20first%20step%20towards%20building%20an%0Aend-to-end%20robotic%20system%20with%20perception%2C%20planning%2C%20autonomous%20maneuvering%2C%0Aand%20decision%20making%20capabilities.%20OGM%20divides%20the%20environment%20into%20discrete%0Acells%20and%20assigns%20probability%20values%20to%20attributes%20such%20as%20occupancy%20and%0Atraversability.%20Existing%20methods%20fall%20into%20two%20categories%3A%20traditional%20methods%0Aand%20neural%20methods.%20Traditional%20methods%20rely%20on%20dense%20statistical%20calculations%2C%0Awhile%20neural%20methods%20employ%20deep%20learning%20for%20probabilistic%20information%0Aprocessing.%20Recent%20works%20formulate%20a%20deterministic%20theory%20of%20neural%20computation%0Aat%20the%20intersection%20of%20cognitive%20science%20and%20vector%20symbolic%20architectures.%20In%0Athis%20study%2C%20we%20propose%20a%20Fourier-based%20hyperdimensional%20OGM%20system%2C%20VSA-OGM%2C%0Acombined%20with%20a%20novel%20application%20of%20Shannon%20entropy%20that%20retains%20the%0Ainterpretability%20and%20stability%20of%20traditional%20methods%20along%20with%20the%20improved%0Acomputational%20efficiency%20of%20neural%20methods.%20Our%20approach%2C%20validated%20across%0Amultiple%20datasets%2C%20achieves%20similar%20accuracy%20to%20covariant%20traditional%20methods%0Awhile%20approximately%20reducing%20latency%20by%20200x%20and%20memory%20by%201000x.%20Compared%20to%0Ainvariant%20traditional%20methods%2C%20we%20see%20similar%20accuracy%20values%20while%20reducing%0Alatency%20by%203.7x.%20Moreover%2C%20we%20achieve%201.5x%20latency%20reductions%20compared%20to%0Aneural%20methods%20while%20eliminating%20the%20need%20for%20domain-specific%20model%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.09066v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrain%2520Inspired%2520Probabilistic%2520Occupancy%2520Grid%2520Mapping%2520with%250A%2520%2520Hyperdimensional%2520Computing%26entry.906535625%3DShay%2520Snyder%2520and%2520Andrew%2520Capodieci%2520and%2520David%2520Gorsich%2520and%2520Maryam%2520Parsa%26entry.1292438233%3D%2520%2520Real-time%2520robotic%2520systems%2520require%2520advanced%2520perception%252C%2520computation%252C%2520and%250Aaction%2520capability.%2520However%252C%2520the%2520main%2520bottleneck%2520in%2520current%2520autonomous%2520systems%250Ais%2520the%2520trade-off%2520between%2520computational%2520capability%252C%2520energy%2520efficiency%2520and%2520model%250Adeterminism.%2520World%2520modeling%252C%2520a%2520key%2520objective%2520of%2520many%2520robotic%2520systems%252C%2520commonly%250Auses%2520occupancy%2520grid%2520mapping%2520%2528OGM%2529%2520as%2520the%2520first%2520step%2520towards%2520building%2520an%250Aend-to-end%2520robotic%2520system%2520with%2520perception%252C%2520planning%252C%2520autonomous%2520maneuvering%252C%250Aand%2520decision%2520making%2520capabilities.%2520OGM%2520divides%2520the%2520environment%2520into%2520discrete%250Acells%2520and%2520assigns%2520probability%2520values%2520to%2520attributes%2520such%2520as%2520occupancy%2520and%250Atraversability.%2520Existing%2520methods%2520fall%2520into%2520two%2520categories%253A%2520traditional%2520methods%250Aand%2520neural%2520methods.%2520Traditional%2520methods%2520rely%2520on%2520dense%2520statistical%2520calculations%252C%250Awhile%2520neural%2520methods%2520employ%2520deep%2520learning%2520for%2520probabilistic%2520information%250Aprocessing.%2520Recent%2520works%2520formulate%2520a%2520deterministic%2520theory%2520of%2520neural%2520computation%250Aat%2520the%2520intersection%2520of%2520cognitive%2520science%2520and%2520vector%2520symbolic%2520architectures.%2520In%250Athis%2520study%252C%2520we%2520propose%2520a%2520Fourier-based%2520hyperdimensional%2520OGM%2520system%252C%2520VSA-OGM%252C%250Acombined%2520with%2520a%2520novel%2520application%2520of%2520Shannon%2520entropy%2520that%2520retains%2520the%250Ainterpretability%2520and%2520stability%2520of%2520traditional%2520methods%2520along%2520with%2520the%2520improved%250Acomputational%2520efficiency%2520of%2520neural%2520methods.%2520Our%2520approach%252C%2520validated%2520across%250Amultiple%2520datasets%252C%2520achieves%2520similar%2520accuracy%2520to%2520covariant%2520traditional%2520methods%250Awhile%2520approximately%2520reducing%2520latency%2520by%2520200x%2520and%2520memory%2520by%25201000x.%2520Compared%2520to%250Ainvariant%2520traditional%2520methods%252C%2520we%2520see%2520similar%2520accuracy%2520values%2520while%2520reducing%250Alatency%2520by%25203.7x.%2520Moreover%252C%2520we%2520achieve%25201.5x%2520latency%2520reductions%2520compared%2520to%250Aneural%2520methods%2520while%2520eliminating%2520the%2520need%2520for%2520domain-specific%2520model%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.09066v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brain%20Inspired%20Probabilistic%20Occupancy%20Grid%20Mapping%20with%0A%20%20Hyperdimensional%20Computing&entry.906535625=Shay%20Snyder%20and%20Andrew%20Capodieci%20and%20David%20Gorsich%20and%20Maryam%20Parsa&entry.1292438233=%20%20Real-time%20robotic%20systems%20require%20advanced%20perception%2C%20computation%2C%20and%0Aaction%20capability.%20However%2C%20the%20main%20bottleneck%20in%20current%20autonomous%20systems%0Ais%20the%20trade-off%20between%20computational%20capability%2C%20energy%20efficiency%20and%20model%0Adeterminism.%20World%20modeling%2C%20a%20key%20objective%20of%20many%20robotic%20systems%2C%20commonly%0Auses%20occupancy%20grid%20mapping%20%28OGM%29%20as%20the%20first%20step%20towards%20building%20an%0Aend-to-end%20robotic%20system%20with%20perception%2C%20planning%2C%20autonomous%20maneuvering%2C%0Aand%20decision%20making%20capabilities.%20OGM%20divides%20the%20environment%20into%20discrete%0Acells%20and%20assigns%20probability%20values%20to%20attributes%20such%20as%20occupancy%20and%0Atraversability.%20Existing%20methods%20fall%20into%20two%20categories%3A%20traditional%20methods%0Aand%20neural%20methods.%20Traditional%20methods%20rely%20on%20dense%20statistical%20calculations%2C%0Awhile%20neural%20methods%20employ%20deep%20learning%20for%20probabilistic%20information%0Aprocessing.%20Recent%20works%20formulate%20a%20deterministic%20theory%20of%20neural%20computation%0Aat%20the%20intersection%20of%20cognitive%20science%20and%20vector%20symbolic%20architectures.%20In%0Athis%20study%2C%20we%20propose%20a%20Fourier-based%20hyperdimensional%20OGM%20system%2C%20VSA-OGM%2C%0Acombined%20with%20a%20novel%20application%20of%20Shannon%20entropy%20that%20retains%20the%0Ainterpretability%20and%20stability%20of%20traditional%20methods%20along%20with%20the%20improved%0Acomputational%20efficiency%20of%20neural%20methods.%20Our%20approach%2C%20validated%20across%0Amultiple%20datasets%2C%20achieves%20similar%20accuracy%20to%20covariant%20traditional%20methods%0Awhile%20approximately%20reducing%20latency%20by%20200x%20and%20memory%20by%201000x.%20Compared%20to%0Ainvariant%20traditional%20methods%2C%20we%20see%20similar%20accuracy%20values%20while%20reducing%0Alatency%20by%203.7x.%20Moreover%2C%20we%20achieve%201.5x%20latency%20reductions%20compared%20to%0Aneural%20methods%20while%20eliminating%20the%20need%20for%20domain-specific%20model%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.09066v2&entry.124074799=Read"},
{"title": "Visuo-Tactile Exploration of Unknown Rigid 3D Curvatures by\n  Vision-Augmented Unified Force-Impedance Control", "author": "K\u00fcbra Karacan and Anran Zhang and Hamid Sadeghian and Fan Wu and Sami Haddadin", "abstract": "  Despite recent advancements in torque-controlled tactile robots, integrating\nthem into manufacturing settings remains challenging, particularly in complex\nenvironments. Simplifying robotic skill programming for non-experts is crucial\nfor increasing robot deployment in manufacturing. This work proposes an\ninnovative approach, Vision-Augmented Unified Force-Impedance Control\n(VA-UFIC), aimed at intuitive visuo-tactile exploration of unknown 3D\ncurvatures. VA-UFIC stands out by seamlessly integrating vision and tactile\ndata, enabling the exploration of diverse contact shapes in three dimensions,\nincluding point contacts, flat contacts with concave and convex curvatures, and\nscenarios involving contact loss. A pivotal component of our method is a robust\nonline contact alignment monitoring system that considers tactile error, local\nsurface curvature, and orientation, facilitating adaptive adjustments of robot\nstiffness and force regulation during exploration. We introduce virtual energy\ntanks within the control framework to ensure safety and stability, effectively\naddressing inherent safety concerns in visuo-tactile exploration. Evaluation\nusing a Franka Emika research robot demonstrates the efficacy of VA-UFIC in\nexploring unknown 3D curvatures while adhering to arbitrarily defined\nforce-motion policies. By seamlessly integrating vision and tactile sensing,\nVA-UFIC offers a promising avenue for intuitive exploration of complex\nenvironments, with potential applications spanning manufacturing, inspection,\nand beyond.\n", "link": "http://arxiv.org/abs/2408.14219v1", "date": "2024-08-26", "relevancy": 1.8036, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6174}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5885}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visuo-Tactile%20Exploration%20of%20Unknown%20Rigid%203D%20Curvatures%20by%0A%20%20Vision-Augmented%20Unified%20Force-Impedance%20Control&body=Title%3A%20Visuo-Tactile%20Exploration%20of%20Unknown%20Rigid%203D%20Curvatures%20by%0A%20%20Vision-Augmented%20Unified%20Force-Impedance%20Control%0AAuthor%3A%20K%C3%BCbra%20Karacan%20and%20Anran%20Zhang%20and%20Hamid%20Sadeghian%20and%20Fan%20Wu%20and%20Sami%20Haddadin%0AAbstract%3A%20%20%20Despite%20recent%20advancements%20in%20torque-controlled%20tactile%20robots%2C%20integrating%0Athem%20into%20manufacturing%20settings%20remains%20challenging%2C%20particularly%20in%20complex%0Aenvironments.%20Simplifying%20robotic%20skill%20programming%20for%20non-experts%20is%20crucial%0Afor%20increasing%20robot%20deployment%20in%20manufacturing.%20This%20work%20proposes%20an%0Ainnovative%20approach%2C%20Vision-Augmented%20Unified%20Force-Impedance%20Control%0A%28VA-UFIC%29%2C%20aimed%20at%20intuitive%20visuo-tactile%20exploration%20of%20unknown%203D%0Acurvatures.%20VA-UFIC%20stands%20out%20by%20seamlessly%20integrating%20vision%20and%20tactile%0Adata%2C%20enabling%20the%20exploration%20of%20diverse%20contact%20shapes%20in%20three%20dimensions%2C%0Aincluding%20point%20contacts%2C%20flat%20contacts%20with%20concave%20and%20convex%20curvatures%2C%20and%0Ascenarios%20involving%20contact%20loss.%20A%20pivotal%20component%20of%20our%20method%20is%20a%20robust%0Aonline%20contact%20alignment%20monitoring%20system%20that%20considers%20tactile%20error%2C%20local%0Asurface%20curvature%2C%20and%20orientation%2C%20facilitating%20adaptive%20adjustments%20of%20robot%0Astiffness%20and%20force%20regulation%20during%20exploration.%20We%20introduce%20virtual%20energy%0Atanks%20within%20the%20control%20framework%20to%20ensure%20safety%20and%20stability%2C%20effectively%0Aaddressing%20inherent%20safety%20concerns%20in%20visuo-tactile%20exploration.%20Evaluation%0Ausing%20a%20Franka%20Emika%20research%20robot%20demonstrates%20the%20efficacy%20of%20VA-UFIC%20in%0Aexploring%20unknown%203D%20curvatures%20while%20adhering%20to%20arbitrarily%20defined%0Aforce-motion%20policies.%20By%20seamlessly%20integrating%20vision%20and%20tactile%20sensing%2C%0AVA-UFIC%20offers%20a%20promising%20avenue%20for%20intuitive%20exploration%20of%20complex%0Aenvironments%2C%20with%20potential%20applications%20spanning%20manufacturing%2C%20inspection%2C%0Aand%20beyond.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14219v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisuo-Tactile%2520Exploration%2520of%2520Unknown%2520Rigid%25203D%2520Curvatures%2520by%250A%2520%2520Vision-Augmented%2520Unified%2520Force-Impedance%2520Control%26entry.906535625%3DK%25C3%25BCbra%2520Karacan%2520and%2520Anran%2520Zhang%2520and%2520Hamid%2520Sadeghian%2520and%2520Fan%2520Wu%2520and%2520Sami%2520Haddadin%26entry.1292438233%3D%2520%2520Despite%2520recent%2520advancements%2520in%2520torque-controlled%2520tactile%2520robots%252C%2520integrating%250Athem%2520into%2520manufacturing%2520settings%2520remains%2520challenging%252C%2520particularly%2520in%2520complex%250Aenvironments.%2520Simplifying%2520robotic%2520skill%2520programming%2520for%2520non-experts%2520is%2520crucial%250Afor%2520increasing%2520robot%2520deployment%2520in%2520manufacturing.%2520This%2520work%2520proposes%2520an%250Ainnovative%2520approach%252C%2520Vision-Augmented%2520Unified%2520Force-Impedance%2520Control%250A%2528VA-UFIC%2529%252C%2520aimed%2520at%2520intuitive%2520visuo-tactile%2520exploration%2520of%2520unknown%25203D%250Acurvatures.%2520VA-UFIC%2520stands%2520out%2520by%2520seamlessly%2520integrating%2520vision%2520and%2520tactile%250Adata%252C%2520enabling%2520the%2520exploration%2520of%2520diverse%2520contact%2520shapes%2520in%2520three%2520dimensions%252C%250Aincluding%2520point%2520contacts%252C%2520flat%2520contacts%2520with%2520concave%2520and%2520convex%2520curvatures%252C%2520and%250Ascenarios%2520involving%2520contact%2520loss.%2520A%2520pivotal%2520component%2520of%2520our%2520method%2520is%2520a%2520robust%250Aonline%2520contact%2520alignment%2520monitoring%2520system%2520that%2520considers%2520tactile%2520error%252C%2520local%250Asurface%2520curvature%252C%2520and%2520orientation%252C%2520facilitating%2520adaptive%2520adjustments%2520of%2520robot%250Astiffness%2520and%2520force%2520regulation%2520during%2520exploration.%2520We%2520introduce%2520virtual%2520energy%250Atanks%2520within%2520the%2520control%2520framework%2520to%2520ensure%2520safety%2520and%2520stability%252C%2520effectively%250Aaddressing%2520inherent%2520safety%2520concerns%2520in%2520visuo-tactile%2520exploration.%2520Evaluation%250Ausing%2520a%2520Franka%2520Emika%2520research%2520robot%2520demonstrates%2520the%2520efficacy%2520of%2520VA-UFIC%2520in%250Aexploring%2520unknown%25203D%2520curvatures%2520while%2520adhering%2520to%2520arbitrarily%2520defined%250Aforce-motion%2520policies.%2520By%2520seamlessly%2520integrating%2520vision%2520and%2520tactile%2520sensing%252C%250AVA-UFIC%2520offers%2520a%2520promising%2520avenue%2520for%2520intuitive%2520exploration%2520of%2520complex%250Aenvironments%252C%2520with%2520potential%2520applications%2520spanning%2520manufacturing%252C%2520inspection%252C%250Aand%2520beyond.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14219v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visuo-Tactile%20Exploration%20of%20Unknown%20Rigid%203D%20Curvatures%20by%0A%20%20Vision-Augmented%20Unified%20Force-Impedance%20Control&entry.906535625=K%C3%BCbra%20Karacan%20and%20Anran%20Zhang%20and%20Hamid%20Sadeghian%20and%20Fan%20Wu%20and%20Sami%20Haddadin&entry.1292438233=%20%20Despite%20recent%20advancements%20in%20torque-controlled%20tactile%20robots%2C%20integrating%0Athem%20into%20manufacturing%20settings%20remains%20challenging%2C%20particularly%20in%20complex%0Aenvironments.%20Simplifying%20robotic%20skill%20programming%20for%20non-experts%20is%20crucial%0Afor%20increasing%20robot%20deployment%20in%20manufacturing.%20This%20work%20proposes%20an%0Ainnovative%20approach%2C%20Vision-Augmented%20Unified%20Force-Impedance%20Control%0A%28VA-UFIC%29%2C%20aimed%20at%20intuitive%20visuo-tactile%20exploration%20of%20unknown%203D%0Acurvatures.%20VA-UFIC%20stands%20out%20by%20seamlessly%20integrating%20vision%20and%20tactile%0Adata%2C%20enabling%20the%20exploration%20of%20diverse%20contact%20shapes%20in%20three%20dimensions%2C%0Aincluding%20point%20contacts%2C%20flat%20contacts%20with%20concave%20and%20convex%20curvatures%2C%20and%0Ascenarios%20involving%20contact%20loss.%20A%20pivotal%20component%20of%20our%20method%20is%20a%20robust%0Aonline%20contact%20alignment%20monitoring%20system%20that%20considers%20tactile%20error%2C%20local%0Asurface%20curvature%2C%20and%20orientation%2C%20facilitating%20adaptive%20adjustments%20of%20robot%0Astiffness%20and%20force%20regulation%20during%20exploration.%20We%20introduce%20virtual%20energy%0Atanks%20within%20the%20control%20framework%20to%20ensure%20safety%20and%20stability%2C%20effectively%0Aaddressing%20inherent%20safety%20concerns%20in%20visuo-tactile%20exploration.%20Evaluation%0Ausing%20a%20Franka%20Emika%20research%20robot%20demonstrates%20the%20efficacy%20of%20VA-UFIC%20in%0Aexploring%20unknown%203D%20curvatures%20while%20adhering%20to%20arbitrarily%20defined%0Aforce-motion%20policies.%20By%20seamlessly%20integrating%20vision%20and%20tactile%20sensing%2C%0AVA-UFIC%20offers%20a%20promising%20avenue%20for%20intuitive%20exploration%20of%20complex%0Aenvironments%2C%20with%20potential%20applications%20spanning%20manufacturing%2C%20inspection%2C%0Aand%20beyond.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14219v1&entry.124074799=Read"},
{"title": "Solar Active Regions Detection Via 2D Circular Kernel Time Series\n  Transformation, Entropy and Machine Learning Approach", "author": "Irewola Aaron Oludehinwa and Andrei Velichko and Maksim Belyaev and Olasunkanmi I. Olusola", "abstract": "  This study proposes an enhancement to the existing method for detecting Solar\nActive Regions (ARs). Our technique tracks ARs using images from the\nAtmospheric Imaging Assembly (AIA) of NASA's Solar Dynamics Observatory (SDO).\nIt involves a 2D circular kernel time series transformation, combined with\nStatistical and Entropy measures, and a Machine Learning (ML) approach. The\ntechnique transforms the circular area around pixels in the SDO AIA images into\none-dimensional time series (1-DTS). Statistical measures (Median Value, Xmed;\n95th Percentile, X95) and Entropy measures (Distribution Entropy, DisEn; Fuzzy\nEntropy, FuzzyEn) are used as feature selection methods (FSM 1), alongside a\nmethod applying 1-DTS elements directly as features (FSM 2). The ML algorithm\nclassifies these series into three categories: no Active Region (nARs type 1,\nclass 1), non-flaring Regions outside active regions with brightness (nARs type\n2, class 2), and flaring Active Regions (ARs, class 3). The ML model achieves a\nclassification accuracy of 0.900 and 0.914 for Entropy and Statistical\nmeasures, respectively. Notably, Fuzzy Entropy shows the highest classification\naccuracy (AKF=0.895), surpassing DisEn (AKF=0.738), X95 (AKF=0.873), and Xmed\n(AKF=0.840). This indicates the high effectiveness of Entropy and Statistical\nmeasures for AR detection in SDO AIA images. FSM 2 captures a similar\ndistribution of flaring AR activities as FSM 1. Additionally, we introduce a\ngeneralizing characteristic of AR activities (GSA), finding a direct agreement\nbetween increased AR activities and higher GSA values. The Python code\nimplementation of the proposed method is available in supplementary material.\n", "link": "http://arxiv.org/abs/2306.08270v2", "date": "2024-08-26", "relevancy": 1.7958, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4559}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4446}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solar%20Active%20Regions%20Detection%20Via%202D%20Circular%20Kernel%20Time%20Series%0A%20%20Transformation%2C%20Entropy%20and%20Machine%20Learning%20Approach&body=Title%3A%20Solar%20Active%20Regions%20Detection%20Via%202D%20Circular%20Kernel%20Time%20Series%0A%20%20Transformation%2C%20Entropy%20and%20Machine%20Learning%20Approach%0AAuthor%3A%20Irewola%20Aaron%20Oludehinwa%20and%20Andrei%20Velichko%20and%20Maksim%20Belyaev%20and%20Olasunkanmi%20I.%20Olusola%0AAbstract%3A%20%20%20This%20study%20proposes%20an%20enhancement%20to%20the%20existing%20method%20for%20detecting%20Solar%0AActive%20Regions%20%28ARs%29.%20Our%20technique%20tracks%20ARs%20using%20images%20from%20the%0AAtmospheric%20Imaging%20Assembly%20%28AIA%29%20of%20NASA%27s%20Solar%20Dynamics%20Observatory%20%28SDO%29.%0AIt%20involves%20a%202D%20circular%20kernel%20time%20series%20transformation%2C%20combined%20with%0AStatistical%20and%20Entropy%20measures%2C%20and%20a%20Machine%20Learning%20%28ML%29%20approach.%20The%0Atechnique%20transforms%20the%20circular%20area%20around%20pixels%20in%20the%20SDO%20AIA%20images%20into%0Aone-dimensional%20time%20series%20%281-DTS%29.%20Statistical%20measures%20%28Median%20Value%2C%20Xmed%3B%0A95th%20Percentile%2C%20X95%29%20and%20Entropy%20measures%20%28Distribution%20Entropy%2C%20DisEn%3B%20Fuzzy%0AEntropy%2C%20FuzzyEn%29%20are%20used%20as%20feature%20selection%20methods%20%28FSM%201%29%2C%20alongside%20a%0Amethod%20applying%201-DTS%20elements%20directly%20as%20features%20%28FSM%202%29.%20The%20ML%20algorithm%0Aclassifies%20these%20series%20into%20three%20categories%3A%20no%20Active%20Region%20%28nARs%20type%201%2C%0Aclass%201%29%2C%20non-flaring%20Regions%20outside%20active%20regions%20with%20brightness%20%28nARs%20type%0A2%2C%20class%202%29%2C%20and%20flaring%20Active%20Regions%20%28ARs%2C%20class%203%29.%20The%20ML%20model%20achieves%20a%0Aclassification%20accuracy%20of%200.900%20and%200.914%20for%20Entropy%20and%20Statistical%0Ameasures%2C%20respectively.%20Notably%2C%20Fuzzy%20Entropy%20shows%20the%20highest%20classification%0Aaccuracy%20%28AKF%3D0.895%29%2C%20surpassing%20DisEn%20%28AKF%3D0.738%29%2C%20X95%20%28AKF%3D0.873%29%2C%20and%20Xmed%0A%28AKF%3D0.840%29.%20This%20indicates%20the%20high%20effectiveness%20of%20Entropy%20and%20Statistical%0Ameasures%20for%20AR%20detection%20in%20SDO%20AIA%20images.%20FSM%202%20captures%20a%20similar%0Adistribution%20of%20flaring%20AR%20activities%20as%20FSM%201.%20Additionally%2C%20we%20introduce%20a%0Ageneralizing%20characteristic%20of%20AR%20activities%20%28GSA%29%2C%20finding%20a%20direct%20agreement%0Abetween%20increased%20AR%20activities%20and%20higher%20GSA%20values.%20The%20Python%20code%0Aimplementation%20of%20the%20proposed%20method%20is%20available%20in%20supplementary%20material.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.08270v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolar%2520Active%2520Regions%2520Detection%2520Via%25202D%2520Circular%2520Kernel%2520Time%2520Series%250A%2520%2520Transformation%252C%2520Entropy%2520and%2520Machine%2520Learning%2520Approach%26entry.906535625%3DIrewola%2520Aaron%2520Oludehinwa%2520and%2520Andrei%2520Velichko%2520and%2520Maksim%2520Belyaev%2520and%2520Olasunkanmi%2520I.%2520Olusola%26entry.1292438233%3D%2520%2520This%2520study%2520proposes%2520an%2520enhancement%2520to%2520the%2520existing%2520method%2520for%2520detecting%2520Solar%250AActive%2520Regions%2520%2528ARs%2529.%2520Our%2520technique%2520tracks%2520ARs%2520using%2520images%2520from%2520the%250AAtmospheric%2520Imaging%2520Assembly%2520%2528AIA%2529%2520of%2520NASA%2527s%2520Solar%2520Dynamics%2520Observatory%2520%2528SDO%2529.%250AIt%2520involves%2520a%25202D%2520circular%2520kernel%2520time%2520series%2520transformation%252C%2520combined%2520with%250AStatistical%2520and%2520Entropy%2520measures%252C%2520and%2520a%2520Machine%2520Learning%2520%2528ML%2529%2520approach.%2520The%250Atechnique%2520transforms%2520the%2520circular%2520area%2520around%2520pixels%2520in%2520the%2520SDO%2520AIA%2520images%2520into%250Aone-dimensional%2520time%2520series%2520%25281-DTS%2529.%2520Statistical%2520measures%2520%2528Median%2520Value%252C%2520Xmed%253B%250A95th%2520Percentile%252C%2520X95%2529%2520and%2520Entropy%2520measures%2520%2528Distribution%2520Entropy%252C%2520DisEn%253B%2520Fuzzy%250AEntropy%252C%2520FuzzyEn%2529%2520are%2520used%2520as%2520feature%2520selection%2520methods%2520%2528FSM%25201%2529%252C%2520alongside%2520a%250Amethod%2520applying%25201-DTS%2520elements%2520directly%2520as%2520features%2520%2528FSM%25202%2529.%2520The%2520ML%2520algorithm%250Aclassifies%2520these%2520series%2520into%2520three%2520categories%253A%2520no%2520Active%2520Region%2520%2528nARs%2520type%25201%252C%250Aclass%25201%2529%252C%2520non-flaring%2520Regions%2520outside%2520active%2520regions%2520with%2520brightness%2520%2528nARs%2520type%250A2%252C%2520class%25202%2529%252C%2520and%2520flaring%2520Active%2520Regions%2520%2528ARs%252C%2520class%25203%2529.%2520The%2520ML%2520model%2520achieves%2520a%250Aclassification%2520accuracy%2520of%25200.900%2520and%25200.914%2520for%2520Entropy%2520and%2520Statistical%250Ameasures%252C%2520respectively.%2520Notably%252C%2520Fuzzy%2520Entropy%2520shows%2520the%2520highest%2520classification%250Aaccuracy%2520%2528AKF%253D0.895%2529%252C%2520surpassing%2520DisEn%2520%2528AKF%253D0.738%2529%252C%2520X95%2520%2528AKF%253D0.873%2529%252C%2520and%2520Xmed%250A%2528AKF%253D0.840%2529.%2520This%2520indicates%2520the%2520high%2520effectiveness%2520of%2520Entropy%2520and%2520Statistical%250Ameasures%2520for%2520AR%2520detection%2520in%2520SDO%2520AIA%2520images.%2520FSM%25202%2520captures%2520a%2520similar%250Adistribution%2520of%2520flaring%2520AR%2520activities%2520as%2520FSM%25201.%2520Additionally%252C%2520we%2520introduce%2520a%250Ageneralizing%2520characteristic%2520of%2520AR%2520activities%2520%2528GSA%2529%252C%2520finding%2520a%2520direct%2520agreement%250Abetween%2520increased%2520AR%2520activities%2520and%2520higher%2520GSA%2520values.%2520The%2520Python%2520code%250Aimplementation%2520of%2520the%2520proposed%2520method%2520is%2520available%2520in%2520supplementary%2520material.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.08270v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solar%20Active%20Regions%20Detection%20Via%202D%20Circular%20Kernel%20Time%20Series%0A%20%20Transformation%2C%20Entropy%20and%20Machine%20Learning%20Approach&entry.906535625=Irewola%20Aaron%20Oludehinwa%20and%20Andrei%20Velichko%20and%20Maksim%20Belyaev%20and%20Olasunkanmi%20I.%20Olusola&entry.1292438233=%20%20This%20study%20proposes%20an%20enhancement%20to%20the%20existing%20method%20for%20detecting%20Solar%0AActive%20Regions%20%28ARs%29.%20Our%20technique%20tracks%20ARs%20using%20images%20from%20the%0AAtmospheric%20Imaging%20Assembly%20%28AIA%29%20of%20NASA%27s%20Solar%20Dynamics%20Observatory%20%28SDO%29.%0AIt%20involves%20a%202D%20circular%20kernel%20time%20series%20transformation%2C%20combined%20with%0AStatistical%20and%20Entropy%20measures%2C%20and%20a%20Machine%20Learning%20%28ML%29%20approach.%20The%0Atechnique%20transforms%20the%20circular%20area%20around%20pixels%20in%20the%20SDO%20AIA%20images%20into%0Aone-dimensional%20time%20series%20%281-DTS%29.%20Statistical%20measures%20%28Median%20Value%2C%20Xmed%3B%0A95th%20Percentile%2C%20X95%29%20and%20Entropy%20measures%20%28Distribution%20Entropy%2C%20DisEn%3B%20Fuzzy%0AEntropy%2C%20FuzzyEn%29%20are%20used%20as%20feature%20selection%20methods%20%28FSM%201%29%2C%20alongside%20a%0Amethod%20applying%201-DTS%20elements%20directly%20as%20features%20%28FSM%202%29.%20The%20ML%20algorithm%0Aclassifies%20these%20series%20into%20three%20categories%3A%20no%20Active%20Region%20%28nARs%20type%201%2C%0Aclass%201%29%2C%20non-flaring%20Regions%20outside%20active%20regions%20with%20brightness%20%28nARs%20type%0A2%2C%20class%202%29%2C%20and%20flaring%20Active%20Regions%20%28ARs%2C%20class%203%29.%20The%20ML%20model%20achieves%20a%0Aclassification%20accuracy%20of%200.900%20and%200.914%20for%20Entropy%20and%20Statistical%0Ameasures%2C%20respectively.%20Notably%2C%20Fuzzy%20Entropy%20shows%20the%20highest%20classification%0Aaccuracy%20%28AKF%3D0.895%29%2C%20surpassing%20DisEn%20%28AKF%3D0.738%29%2C%20X95%20%28AKF%3D0.873%29%2C%20and%20Xmed%0A%28AKF%3D0.840%29.%20This%20indicates%20the%20high%20effectiveness%20of%20Entropy%20and%20Statistical%0Ameasures%20for%20AR%20detection%20in%20SDO%20AIA%20images.%20FSM%202%20captures%20a%20similar%0Adistribution%20of%20flaring%20AR%20activities%20as%20FSM%201.%20Additionally%2C%20we%20introduce%20a%0Ageneralizing%20characteristic%20of%20AR%20activities%20%28GSA%29%2C%20finding%20a%20direct%20agreement%0Abetween%20increased%20AR%20activities%20and%20higher%20GSA%20values.%20The%20Python%20code%0Aimplementation%20of%20the%20proposed%20method%20is%20available%20in%20supplementary%20material.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.08270v2&entry.124074799=Read"},
{"title": "Sparsity-Aware Hardware-Software Co-Design of Spiking Neural Networks:\n  An Overview", "author": "Ilkin Aliyev and Kama Svoboda and Tosiron Adegbija and Jean-Marc Fellous", "abstract": "  Spiking Neural Networks (SNNs) are inspired by the sparse and event-driven\nnature of biological neural processing, and offer the potential for\nultra-low-power artificial intelligence. However, realizing their efficiency\nbenefits requires specialized hardware and a co-design approach that\neffectively leverages sparsity. We explore the hardware-software co-design of\nsparse SNNs, examining how sparsity representation, hardware architectures, and\ntraining techniques influence hardware efficiency. We analyze the impact of\nstatic and dynamic sparsity, discuss the implications of different neuron\nmodels and encoding schemes, and investigate the need for adaptability in\nhardware designs. Our work aims to illuminate the path towards embedded\nneuromorphic systems that fully exploit the computational advantages of sparse\nSNNs.\n", "link": "http://arxiv.org/abs/2408.14437v1", "date": "2024-08-26", "relevancy": 1.7745, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4594}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4407}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparsity-Aware%20Hardware-Software%20Co-Design%20of%20Spiking%20Neural%20Networks%3A%0A%20%20An%20Overview&body=Title%3A%20Sparsity-Aware%20Hardware-Software%20Co-Design%20of%20Spiking%20Neural%20Networks%3A%0A%20%20An%20Overview%0AAuthor%3A%20Ilkin%20Aliyev%20and%20Kama%20Svoboda%20and%20Tosiron%20Adegbija%20and%20Jean-Marc%20Fellous%0AAbstract%3A%20%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20are%20inspired%20by%20the%20sparse%20and%20event-driven%0Anature%20of%20biological%20neural%20processing%2C%20and%20offer%20the%20potential%20for%0Aultra-low-power%20artificial%20intelligence.%20However%2C%20realizing%20their%20efficiency%0Abenefits%20requires%20specialized%20hardware%20and%20a%20co-design%20approach%20that%0Aeffectively%20leverages%20sparsity.%20We%20explore%20the%20hardware-software%20co-design%20of%0Asparse%20SNNs%2C%20examining%20how%20sparsity%20representation%2C%20hardware%20architectures%2C%20and%0Atraining%20techniques%20influence%20hardware%20efficiency.%20We%20analyze%20the%20impact%20of%0Astatic%20and%20dynamic%20sparsity%2C%20discuss%20the%20implications%20of%20different%20neuron%0Amodels%20and%20encoding%20schemes%2C%20and%20investigate%20the%20need%20for%20adaptability%20in%0Ahardware%20designs.%20Our%20work%20aims%20to%20illuminate%20the%20path%20towards%20embedded%0Aneuromorphic%20systems%20that%20fully%20exploit%20the%20computational%20advantages%20of%20sparse%0ASNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14437v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparsity-Aware%2520Hardware-Software%2520Co-Design%2520of%2520Spiking%2520Neural%2520Networks%253A%250A%2520%2520An%2520Overview%26entry.906535625%3DIlkin%2520Aliyev%2520and%2520Kama%2520Svoboda%2520and%2520Tosiron%2520Adegbija%2520and%2520Jean-Marc%2520Fellous%26entry.1292438233%3D%2520%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520are%2520inspired%2520by%2520the%2520sparse%2520and%2520event-driven%250Anature%2520of%2520biological%2520neural%2520processing%252C%2520and%2520offer%2520the%2520potential%2520for%250Aultra-low-power%2520artificial%2520intelligence.%2520However%252C%2520realizing%2520their%2520efficiency%250Abenefits%2520requires%2520specialized%2520hardware%2520and%2520a%2520co-design%2520approach%2520that%250Aeffectively%2520leverages%2520sparsity.%2520We%2520explore%2520the%2520hardware-software%2520co-design%2520of%250Asparse%2520SNNs%252C%2520examining%2520how%2520sparsity%2520representation%252C%2520hardware%2520architectures%252C%2520and%250Atraining%2520techniques%2520influence%2520hardware%2520efficiency.%2520We%2520analyze%2520the%2520impact%2520of%250Astatic%2520and%2520dynamic%2520sparsity%252C%2520discuss%2520the%2520implications%2520of%2520different%2520neuron%250Amodels%2520and%2520encoding%2520schemes%252C%2520and%2520investigate%2520the%2520need%2520for%2520adaptability%2520in%250Ahardware%2520designs.%2520Our%2520work%2520aims%2520to%2520illuminate%2520the%2520path%2520towards%2520embedded%250Aneuromorphic%2520systems%2520that%2520fully%2520exploit%2520the%2520computational%2520advantages%2520of%2520sparse%250ASNNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14437v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparsity-Aware%20Hardware-Software%20Co-Design%20of%20Spiking%20Neural%20Networks%3A%0A%20%20An%20Overview&entry.906535625=Ilkin%20Aliyev%20and%20Kama%20Svoboda%20and%20Tosiron%20Adegbija%20and%20Jean-Marc%20Fellous&entry.1292438233=%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20are%20inspired%20by%20the%20sparse%20and%20event-driven%0Anature%20of%20biological%20neural%20processing%2C%20and%20offer%20the%20potential%20for%0Aultra-low-power%20artificial%20intelligence.%20However%2C%20realizing%20their%20efficiency%0Abenefits%20requires%20specialized%20hardware%20and%20a%20co-design%20approach%20that%0Aeffectively%20leverages%20sparsity.%20We%20explore%20the%20hardware-software%20co-design%20of%0Asparse%20SNNs%2C%20examining%20how%20sparsity%20representation%2C%20hardware%20architectures%2C%20and%0Atraining%20techniques%20influence%20hardware%20efficiency.%20We%20analyze%20the%20impact%20of%0Astatic%20and%20dynamic%20sparsity%2C%20discuss%20the%20implications%20of%20different%20neuron%0Amodels%20and%20encoding%20schemes%2C%20and%20investigate%20the%20need%20for%20adaptability%20in%0Ahardware%20designs.%20Our%20work%20aims%20to%20illuminate%20the%20path%20towards%20embedded%0Aneuromorphic%20systems%20that%20fully%20exploit%20the%20computational%20advantages%20of%20sparse%0ASNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14437v1&entry.124074799=Read"},
{"title": "Contextual Bandit with Herding Effects: Algorithms and Recommendation\n  Applications", "author": "Luyue Xu and Liming Wang and Hong Xie and Mingqiang Zhou", "abstract": "  Contextual bandits serve as a fundamental algorithmic framework for\noptimizing recommendation decisions online. Though extensive attention has been\npaid to tailoring contextual bandits for recommendation applications, the\n\"herding effects\" in user feedback have been ignored. These herding effects\nbias user feedback toward historical ratings, breaking down the assumption of\nunbiased feedback inherent in contextual bandits. This paper develops a novel\nvariant of the contextual bandit that is tailored to address the feedback bias\ncaused by the herding effects. A user feedback model is formulated to capture\nthis feedback bias. We design the TS-Conf (Thompson Sampling under Conformity)\nalgorithm, which employs posterior sampling to balance the exploration and\nexploitation tradeoff. We prove an upper bound for the regret of the algorithm,\nrevealing the impact of herding effects on learning speed. Extensive\nexperiments on datasets demonstrate that TS-Conf outperforms four benchmark\nalgorithms. Analysis reveals that TS-Conf effectively mitigates the negative\nimpact of herding effects, resulting in faster learning and improved\nrecommendation accuracy.\n", "link": "http://arxiv.org/abs/2408.14432v1", "date": "2024-08-26", "relevancy": 1.7727, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4646}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4454}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4209}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contextual%20Bandit%20with%20Herding%20Effects%3A%20Algorithms%20and%20Recommendation%0A%20%20Applications&body=Title%3A%20Contextual%20Bandit%20with%20Herding%20Effects%3A%20Algorithms%20and%20Recommendation%0A%20%20Applications%0AAuthor%3A%20Luyue%20Xu%20and%20Liming%20Wang%20and%20Hong%20Xie%20and%20Mingqiang%20Zhou%0AAbstract%3A%20%20%20Contextual%20bandits%20serve%20as%20a%20fundamental%20algorithmic%20framework%20for%0Aoptimizing%20recommendation%20decisions%20online.%20Though%20extensive%20attention%20has%20been%0Apaid%20to%20tailoring%20contextual%20bandits%20for%20recommendation%20applications%2C%20the%0A%22herding%20effects%22%20in%20user%20feedback%20have%20been%20ignored.%20These%20herding%20effects%0Abias%20user%20feedback%20toward%20historical%20ratings%2C%20breaking%20down%20the%20assumption%20of%0Aunbiased%20feedback%20inherent%20in%20contextual%20bandits.%20This%20paper%20develops%20a%20novel%0Avariant%20of%20the%20contextual%20bandit%20that%20is%20tailored%20to%20address%20the%20feedback%20bias%0Acaused%20by%20the%20herding%20effects.%20A%20user%20feedback%20model%20is%20formulated%20to%20capture%0Athis%20feedback%20bias.%20We%20design%20the%20TS-Conf%20%28Thompson%20Sampling%20under%20Conformity%29%0Aalgorithm%2C%20which%20employs%20posterior%20sampling%20to%20balance%20the%20exploration%20and%0Aexploitation%20tradeoff.%20We%20prove%20an%20upper%20bound%20for%20the%20regret%20of%20the%20algorithm%2C%0Arevealing%20the%20impact%20of%20herding%20effects%20on%20learning%20speed.%20Extensive%0Aexperiments%20on%20datasets%20demonstrate%20that%20TS-Conf%20outperforms%20four%20benchmark%0Aalgorithms.%20Analysis%20reveals%20that%20TS-Conf%20effectively%20mitigates%20the%20negative%0Aimpact%20of%20herding%20effects%2C%20resulting%20in%20faster%20learning%20and%20improved%0Arecommendation%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14432v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextual%2520Bandit%2520with%2520Herding%2520Effects%253A%2520Algorithms%2520and%2520Recommendation%250A%2520%2520Applications%26entry.906535625%3DLuyue%2520Xu%2520and%2520Liming%2520Wang%2520and%2520Hong%2520Xie%2520and%2520Mingqiang%2520Zhou%26entry.1292438233%3D%2520%2520Contextual%2520bandits%2520serve%2520as%2520a%2520fundamental%2520algorithmic%2520framework%2520for%250Aoptimizing%2520recommendation%2520decisions%2520online.%2520Though%2520extensive%2520attention%2520has%2520been%250Apaid%2520to%2520tailoring%2520contextual%2520bandits%2520for%2520recommendation%2520applications%252C%2520the%250A%2522herding%2520effects%2522%2520in%2520user%2520feedback%2520have%2520been%2520ignored.%2520These%2520herding%2520effects%250Abias%2520user%2520feedback%2520toward%2520historical%2520ratings%252C%2520breaking%2520down%2520the%2520assumption%2520of%250Aunbiased%2520feedback%2520inherent%2520in%2520contextual%2520bandits.%2520This%2520paper%2520develops%2520a%2520novel%250Avariant%2520of%2520the%2520contextual%2520bandit%2520that%2520is%2520tailored%2520to%2520address%2520the%2520feedback%2520bias%250Acaused%2520by%2520the%2520herding%2520effects.%2520A%2520user%2520feedback%2520model%2520is%2520formulated%2520to%2520capture%250Athis%2520feedback%2520bias.%2520We%2520design%2520the%2520TS-Conf%2520%2528Thompson%2520Sampling%2520under%2520Conformity%2529%250Aalgorithm%252C%2520which%2520employs%2520posterior%2520sampling%2520to%2520balance%2520the%2520exploration%2520and%250Aexploitation%2520tradeoff.%2520We%2520prove%2520an%2520upper%2520bound%2520for%2520the%2520regret%2520of%2520the%2520algorithm%252C%250Arevealing%2520the%2520impact%2520of%2520herding%2520effects%2520on%2520learning%2520speed.%2520Extensive%250Aexperiments%2520on%2520datasets%2520demonstrate%2520that%2520TS-Conf%2520outperforms%2520four%2520benchmark%250Aalgorithms.%2520Analysis%2520reveals%2520that%2520TS-Conf%2520effectively%2520mitigates%2520the%2520negative%250Aimpact%2520of%2520herding%2520effects%252C%2520resulting%2520in%2520faster%2520learning%2520and%2520improved%250Arecommendation%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14432v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contextual%20Bandit%20with%20Herding%20Effects%3A%20Algorithms%20and%20Recommendation%0A%20%20Applications&entry.906535625=Luyue%20Xu%20and%20Liming%20Wang%20and%20Hong%20Xie%20and%20Mingqiang%20Zhou&entry.1292438233=%20%20Contextual%20bandits%20serve%20as%20a%20fundamental%20algorithmic%20framework%20for%0Aoptimizing%20recommendation%20decisions%20online.%20Though%20extensive%20attention%20has%20been%0Apaid%20to%20tailoring%20contextual%20bandits%20for%20recommendation%20applications%2C%20the%0A%22herding%20effects%22%20in%20user%20feedback%20have%20been%20ignored.%20These%20herding%20effects%0Abias%20user%20feedback%20toward%20historical%20ratings%2C%20breaking%20down%20the%20assumption%20of%0Aunbiased%20feedback%20inherent%20in%20contextual%20bandits.%20This%20paper%20develops%20a%20novel%0Avariant%20of%20the%20contextual%20bandit%20that%20is%20tailored%20to%20address%20the%20feedback%20bias%0Acaused%20by%20the%20herding%20effects.%20A%20user%20feedback%20model%20is%20formulated%20to%20capture%0Athis%20feedback%20bias.%20We%20design%20the%20TS-Conf%20%28Thompson%20Sampling%20under%20Conformity%29%0Aalgorithm%2C%20which%20employs%20posterior%20sampling%20to%20balance%20the%20exploration%20and%0Aexploitation%20tradeoff.%20We%20prove%20an%20upper%20bound%20for%20the%20regret%20of%20the%20algorithm%2C%0Arevealing%20the%20impact%20of%20herding%20effects%20on%20learning%20speed.%20Extensive%0Aexperiments%20on%20datasets%20demonstrate%20that%20TS-Conf%20outperforms%20four%20benchmark%0Aalgorithms.%20Analysis%20reveals%20that%20TS-Conf%20effectively%20mitigates%20the%20negative%0Aimpact%20of%20herding%20effects%2C%20resulting%20in%20faster%20learning%20and%20improved%0Arecommendation%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14432v1&entry.124074799=Read"},
{"title": "Implicit Concept Removal of Diffusion Models", "author": "Zhili Liu and Kai Chen and Yifan Zhang and Jianhua Han and Lanqing Hong and Hang Xu and Zhenguo Li and Dit-Yan Yeung and James Kwok", "abstract": "  Text-to-image (T2I) diffusion models often inadvertently generate unwanted\nconcepts such as watermarks and unsafe images. These concepts, termed as the\n\"implicit concepts\", could be unintentionally learned during training and then\nbe generated uncontrollably during inference. Existing removal methods still\nstruggle to eliminate implicit concepts primarily due to their dependency on\nthe model's ability to recognize concepts it actually can not discern. To\naddress this, we utilize the intrinsic geometric characteristics of implicit\nconcepts and present the Geom-Erasing, a novel concept removal method based on\nthe geometric-driven control. Specifically, once an unwanted implicit concept\nis identified, we integrate the existence and geometric information of the\nconcept into the text prompts with the help of an accessible classifier or\ndetector model. Subsequently, the model is optimized to identify and\ndisentangle this information, which is then adopted as negative prompts during\ngeneration. Moreover, we introduce the Implicit Concept Dataset (ICD), a novel\nimage-text dataset imbued with three typical implicit concepts (i.e., QR codes,\nwatermarks, and text), reflecting real-life situations where implicit concepts\nare easily injected. Geom-Erasing effectively mitigates the generation of\nimplicit concepts, achieving the state-of-the-art results on the Inappropriate\nImage Prompts (I2P) and our challenging Implicit Concept Dataset (ICD)\nbenchmarks.\n", "link": "http://arxiv.org/abs/2310.05873v6", "date": "2024-08-26", "relevancy": 1.6415, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5507}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5475}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implicit%20Concept%20Removal%20of%20Diffusion%20Models&body=Title%3A%20Implicit%20Concept%20Removal%20of%20Diffusion%20Models%0AAuthor%3A%20Zhili%20Liu%20and%20Kai%20Chen%20and%20Yifan%20Zhang%20and%20Jianhua%20Han%20and%20Lanqing%20Hong%20and%20Hang%20Xu%20and%20Zhenguo%20Li%20and%20Dit-Yan%20Yeung%20and%20James%20Kwok%0AAbstract%3A%20%20%20Text-to-image%20%28T2I%29%20diffusion%20models%20often%20inadvertently%20generate%20unwanted%0Aconcepts%20such%20as%20watermarks%20and%20unsafe%20images.%20These%20concepts%2C%20termed%20as%20the%0A%22implicit%20concepts%22%2C%20could%20be%20unintentionally%20learned%20during%20training%20and%20then%0Abe%20generated%20uncontrollably%20during%20inference.%20Existing%20removal%20methods%20still%0Astruggle%20to%20eliminate%20implicit%20concepts%20primarily%20due%20to%20their%20dependency%20on%0Athe%20model%27s%20ability%20to%20recognize%20concepts%20it%20actually%20can%20not%20discern.%20To%0Aaddress%20this%2C%20we%20utilize%20the%20intrinsic%20geometric%20characteristics%20of%20implicit%0Aconcepts%20and%20present%20the%20Geom-Erasing%2C%20a%20novel%20concept%20removal%20method%20based%20on%0Athe%20geometric-driven%20control.%20Specifically%2C%20once%20an%20unwanted%20implicit%20concept%0Ais%20identified%2C%20we%20integrate%20the%20existence%20and%20geometric%20information%20of%20the%0Aconcept%20into%20the%20text%20prompts%20with%20the%20help%20of%20an%20accessible%20classifier%20or%0Adetector%20model.%20Subsequently%2C%20the%20model%20is%20optimized%20to%20identify%20and%0Adisentangle%20this%20information%2C%20which%20is%20then%20adopted%20as%20negative%20prompts%20during%0Ageneration.%20Moreover%2C%20we%20introduce%20the%20Implicit%20Concept%20Dataset%20%28ICD%29%2C%20a%20novel%0Aimage-text%20dataset%20imbued%20with%20three%20typical%20implicit%20concepts%20%28i.e.%2C%20QR%20codes%2C%0Awatermarks%2C%20and%20text%29%2C%20reflecting%20real-life%20situations%20where%20implicit%20concepts%0Aare%20easily%20injected.%20Geom-Erasing%20effectively%20mitigates%20the%20generation%20of%0Aimplicit%20concepts%2C%20achieving%20the%20state-of-the-art%20results%20on%20the%20Inappropriate%0AImage%20Prompts%20%28I2P%29%20and%20our%20challenging%20Implicit%20Concept%20Dataset%20%28ICD%29%0Abenchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.05873v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplicit%2520Concept%2520Removal%2520of%2520Diffusion%2520Models%26entry.906535625%3DZhili%2520Liu%2520and%2520Kai%2520Chen%2520and%2520Yifan%2520Zhang%2520and%2520Jianhua%2520Han%2520and%2520Lanqing%2520Hong%2520and%2520Hang%2520Xu%2520and%2520Zhenguo%2520Li%2520and%2520Dit-Yan%2520Yeung%2520and%2520James%2520Kwok%26entry.1292438233%3D%2520%2520Text-to-image%2520%2528T2I%2529%2520diffusion%2520models%2520often%2520inadvertently%2520generate%2520unwanted%250Aconcepts%2520such%2520as%2520watermarks%2520and%2520unsafe%2520images.%2520These%2520concepts%252C%2520termed%2520as%2520the%250A%2522implicit%2520concepts%2522%252C%2520could%2520be%2520unintentionally%2520learned%2520during%2520training%2520and%2520then%250Abe%2520generated%2520uncontrollably%2520during%2520inference.%2520Existing%2520removal%2520methods%2520still%250Astruggle%2520to%2520eliminate%2520implicit%2520concepts%2520primarily%2520due%2520to%2520their%2520dependency%2520on%250Athe%2520model%2527s%2520ability%2520to%2520recognize%2520concepts%2520it%2520actually%2520can%2520not%2520discern.%2520To%250Aaddress%2520this%252C%2520we%2520utilize%2520the%2520intrinsic%2520geometric%2520characteristics%2520of%2520implicit%250Aconcepts%2520and%2520present%2520the%2520Geom-Erasing%252C%2520a%2520novel%2520concept%2520removal%2520method%2520based%2520on%250Athe%2520geometric-driven%2520control.%2520Specifically%252C%2520once%2520an%2520unwanted%2520implicit%2520concept%250Ais%2520identified%252C%2520we%2520integrate%2520the%2520existence%2520and%2520geometric%2520information%2520of%2520the%250Aconcept%2520into%2520the%2520text%2520prompts%2520with%2520the%2520help%2520of%2520an%2520accessible%2520classifier%2520or%250Adetector%2520model.%2520Subsequently%252C%2520the%2520model%2520is%2520optimized%2520to%2520identify%2520and%250Adisentangle%2520this%2520information%252C%2520which%2520is%2520then%2520adopted%2520as%2520negative%2520prompts%2520during%250Ageneration.%2520Moreover%252C%2520we%2520introduce%2520the%2520Implicit%2520Concept%2520Dataset%2520%2528ICD%2529%252C%2520a%2520novel%250Aimage-text%2520dataset%2520imbued%2520with%2520three%2520typical%2520implicit%2520concepts%2520%2528i.e.%252C%2520QR%2520codes%252C%250Awatermarks%252C%2520and%2520text%2529%252C%2520reflecting%2520real-life%2520situations%2520where%2520implicit%2520concepts%250Aare%2520easily%2520injected.%2520Geom-Erasing%2520effectively%2520mitigates%2520the%2520generation%2520of%250Aimplicit%2520concepts%252C%2520achieving%2520the%2520state-of-the-art%2520results%2520on%2520the%2520Inappropriate%250AImage%2520Prompts%2520%2528I2P%2529%2520and%2520our%2520challenging%2520Implicit%2520Concept%2520Dataset%2520%2528ICD%2529%250Abenchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.05873v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20Concept%20Removal%20of%20Diffusion%20Models&entry.906535625=Zhili%20Liu%20and%20Kai%20Chen%20and%20Yifan%20Zhang%20and%20Jianhua%20Han%20and%20Lanqing%20Hong%20and%20Hang%20Xu%20and%20Zhenguo%20Li%20and%20Dit-Yan%20Yeung%20and%20James%20Kwok&entry.1292438233=%20%20Text-to-image%20%28T2I%29%20diffusion%20models%20often%20inadvertently%20generate%20unwanted%0Aconcepts%20such%20as%20watermarks%20and%20unsafe%20images.%20These%20concepts%2C%20termed%20as%20the%0A%22implicit%20concepts%22%2C%20could%20be%20unintentionally%20learned%20during%20training%20and%20then%0Abe%20generated%20uncontrollably%20during%20inference.%20Existing%20removal%20methods%20still%0Astruggle%20to%20eliminate%20implicit%20concepts%20primarily%20due%20to%20their%20dependency%20on%0Athe%20model%27s%20ability%20to%20recognize%20concepts%20it%20actually%20can%20not%20discern.%20To%0Aaddress%20this%2C%20we%20utilize%20the%20intrinsic%20geometric%20characteristics%20of%20implicit%0Aconcepts%20and%20present%20the%20Geom-Erasing%2C%20a%20novel%20concept%20removal%20method%20based%20on%0Athe%20geometric-driven%20control.%20Specifically%2C%20once%20an%20unwanted%20implicit%20concept%0Ais%20identified%2C%20we%20integrate%20the%20existence%20and%20geometric%20information%20of%20the%0Aconcept%20into%20the%20text%20prompts%20with%20the%20help%20of%20an%20accessible%20classifier%20or%0Adetector%20model.%20Subsequently%2C%20the%20model%20is%20optimized%20to%20identify%20and%0Adisentangle%20this%20information%2C%20which%20is%20then%20adopted%20as%20negative%20prompts%20during%0Ageneration.%20Moreover%2C%20we%20introduce%20the%20Implicit%20Concept%20Dataset%20%28ICD%29%2C%20a%20novel%0Aimage-text%20dataset%20imbued%20with%20three%20typical%20implicit%20concepts%20%28i.e.%2C%20QR%20codes%2C%0Awatermarks%2C%20and%20text%29%2C%20reflecting%20real-life%20situations%20where%20implicit%20concepts%0Aare%20easily%20injected.%20Geom-Erasing%20effectively%20mitigates%20the%20generation%20of%0Aimplicit%20concepts%2C%20achieving%20the%20state-of-the-art%20results%20on%20the%20Inappropriate%0AImage%20Prompts%20%28I2P%29%20and%20our%20challenging%20Implicit%20Concept%20Dataset%20%28ICD%29%0Abenchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.05873v6&entry.124074799=Read"},
{"title": "The Causal Chambers: Real Physical Systems as a Testbed for AI\n  Methodology", "author": "Juan L. Gamella and Jonas Peters and Peter B\u00fchlmann", "abstract": "  In some fields of AI, machine learning and statistics, the validation of new\nmethods and algorithms is often hindered by the scarcity of suitable real-world\ndatasets. Researchers must often turn to simulated data, which yields limited\ninformation about the applicability of the proposed methods to real problems.\nAs a step forward, we have constructed two devices that allow us to quickly and\ninexpensively produce large datasets from non-trivial but well-understood\nphysical systems. The devices, which we call causal chambers, are\ncomputer-controlled laboratories that allow us to manipulate and measure an\narray of variables from these physical systems, providing a rich testbed for\nalgorithms from a variety of fields. We illustrate potential applications\nthrough a series of case studies in fields such as causal discovery,\nout-of-distribution generalization, change point detection, independent\ncomponent analysis, and symbolic regression. For applications to causal\ninference, the chambers allow us to carefully perform interventions. We also\nprovide and empirically validate a causal model of each chamber, which can be\nused as ground truth for different tasks. All hardware and software is made\nopen source, and the datasets are publicly available at causalchamber.org or\nthrough the Python package causalchamber.\n", "link": "http://arxiv.org/abs/2404.11341v2", "date": "2024-08-26", "relevancy": 1.4302, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5344}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4709}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Causal%20Chambers%3A%20Real%20Physical%20Systems%20as%20a%20Testbed%20for%20AI%0A%20%20Methodology&body=Title%3A%20The%20Causal%20Chambers%3A%20Real%20Physical%20Systems%20as%20a%20Testbed%20for%20AI%0A%20%20Methodology%0AAuthor%3A%20Juan%20L.%20Gamella%20and%20Jonas%20Peters%20and%20Peter%20B%C3%BChlmann%0AAbstract%3A%20%20%20In%20some%20fields%20of%20AI%2C%20machine%20learning%20and%20statistics%2C%20the%20validation%20of%20new%0Amethods%20and%20algorithms%20is%20often%20hindered%20by%20the%20scarcity%20of%20suitable%20real-world%0Adatasets.%20Researchers%20must%20often%20turn%20to%20simulated%20data%2C%20which%20yields%20limited%0Ainformation%20about%20the%20applicability%20of%20the%20proposed%20methods%20to%20real%20problems.%0AAs%20a%20step%20forward%2C%20we%20have%20constructed%20two%20devices%20that%20allow%20us%20to%20quickly%20and%0Ainexpensively%20produce%20large%20datasets%20from%20non-trivial%20but%20well-understood%0Aphysical%20systems.%20The%20devices%2C%20which%20we%20call%20causal%20chambers%2C%20are%0Acomputer-controlled%20laboratories%20that%20allow%20us%20to%20manipulate%20and%20measure%20an%0Aarray%20of%20variables%20from%20these%20physical%20systems%2C%20providing%20a%20rich%20testbed%20for%0Aalgorithms%20from%20a%20variety%20of%20fields.%20We%20illustrate%20potential%20applications%0Athrough%20a%20series%20of%20case%20studies%20in%20fields%20such%20as%20causal%20discovery%2C%0Aout-of-distribution%20generalization%2C%20change%20point%20detection%2C%20independent%0Acomponent%20analysis%2C%20and%20symbolic%20regression.%20For%20applications%20to%20causal%0Ainference%2C%20the%20chambers%20allow%20us%20to%20carefully%20perform%20interventions.%20We%20also%0Aprovide%20and%20empirically%20validate%20a%20causal%20model%20of%20each%20chamber%2C%20which%20can%20be%0Aused%20as%20ground%20truth%20for%20different%20tasks.%20All%20hardware%20and%20software%20is%20made%0Aopen%20source%2C%20and%20the%20datasets%20are%20publicly%20available%20at%20causalchamber.org%20or%0Athrough%20the%20Python%20package%20causalchamber.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.11341v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Causal%2520Chambers%253A%2520Real%2520Physical%2520Systems%2520as%2520a%2520Testbed%2520for%2520AI%250A%2520%2520Methodology%26entry.906535625%3DJuan%2520L.%2520Gamella%2520and%2520Jonas%2520Peters%2520and%2520Peter%2520B%25C3%25BChlmann%26entry.1292438233%3D%2520%2520In%2520some%2520fields%2520of%2520AI%252C%2520machine%2520learning%2520and%2520statistics%252C%2520the%2520validation%2520of%2520new%250Amethods%2520and%2520algorithms%2520is%2520often%2520hindered%2520by%2520the%2520scarcity%2520of%2520suitable%2520real-world%250Adatasets.%2520Researchers%2520must%2520often%2520turn%2520to%2520simulated%2520data%252C%2520which%2520yields%2520limited%250Ainformation%2520about%2520the%2520applicability%2520of%2520the%2520proposed%2520methods%2520to%2520real%2520problems.%250AAs%2520a%2520step%2520forward%252C%2520we%2520have%2520constructed%2520two%2520devices%2520that%2520allow%2520us%2520to%2520quickly%2520and%250Ainexpensively%2520produce%2520large%2520datasets%2520from%2520non-trivial%2520but%2520well-understood%250Aphysical%2520systems.%2520The%2520devices%252C%2520which%2520we%2520call%2520causal%2520chambers%252C%2520are%250Acomputer-controlled%2520laboratories%2520that%2520allow%2520us%2520to%2520manipulate%2520and%2520measure%2520an%250Aarray%2520of%2520variables%2520from%2520these%2520physical%2520systems%252C%2520providing%2520a%2520rich%2520testbed%2520for%250Aalgorithms%2520from%2520a%2520variety%2520of%2520fields.%2520We%2520illustrate%2520potential%2520applications%250Athrough%2520a%2520series%2520of%2520case%2520studies%2520in%2520fields%2520such%2520as%2520causal%2520discovery%252C%250Aout-of-distribution%2520generalization%252C%2520change%2520point%2520detection%252C%2520independent%250Acomponent%2520analysis%252C%2520and%2520symbolic%2520regression.%2520For%2520applications%2520to%2520causal%250Ainference%252C%2520the%2520chambers%2520allow%2520us%2520to%2520carefully%2520perform%2520interventions.%2520We%2520also%250Aprovide%2520and%2520empirically%2520validate%2520a%2520causal%2520model%2520of%2520each%2520chamber%252C%2520which%2520can%2520be%250Aused%2520as%2520ground%2520truth%2520for%2520different%2520tasks.%2520All%2520hardware%2520and%2520software%2520is%2520made%250Aopen%2520source%252C%2520and%2520the%2520datasets%2520are%2520publicly%2520available%2520at%2520causalchamber.org%2520or%250Athrough%2520the%2520Python%2520package%2520causalchamber.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.11341v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Causal%20Chambers%3A%20Real%20Physical%20Systems%20as%20a%20Testbed%20for%20AI%0A%20%20Methodology&entry.906535625=Juan%20L.%20Gamella%20and%20Jonas%20Peters%20and%20Peter%20B%C3%BChlmann&entry.1292438233=%20%20In%20some%20fields%20of%20AI%2C%20machine%20learning%20and%20statistics%2C%20the%20validation%20of%20new%0Amethods%20and%20algorithms%20is%20often%20hindered%20by%20the%20scarcity%20of%20suitable%20real-world%0Adatasets.%20Researchers%20must%20often%20turn%20to%20simulated%20data%2C%20which%20yields%20limited%0Ainformation%20about%20the%20applicability%20of%20the%20proposed%20methods%20to%20real%20problems.%0AAs%20a%20step%20forward%2C%20we%20have%20constructed%20two%20devices%20that%20allow%20us%20to%20quickly%20and%0Ainexpensively%20produce%20large%20datasets%20from%20non-trivial%20but%20well-understood%0Aphysical%20systems.%20The%20devices%2C%20which%20we%20call%20causal%20chambers%2C%20are%0Acomputer-controlled%20laboratories%20that%20allow%20us%20to%20manipulate%20and%20measure%20an%0Aarray%20of%20variables%20from%20these%20physical%20systems%2C%20providing%20a%20rich%20testbed%20for%0Aalgorithms%20from%20a%20variety%20of%20fields.%20We%20illustrate%20potential%20applications%0Athrough%20a%20series%20of%20case%20studies%20in%20fields%20such%20as%20causal%20discovery%2C%0Aout-of-distribution%20generalization%2C%20change%20point%20detection%2C%20independent%0Acomponent%20analysis%2C%20and%20symbolic%20regression.%20For%20applications%20to%20causal%0Ainference%2C%20the%20chambers%20allow%20us%20to%20carefully%20perform%20interventions.%20We%20also%0Aprovide%20and%20empirically%20validate%20a%20causal%20model%20of%20each%20chamber%2C%20which%20can%20be%0Aused%20as%20ground%20truth%20for%20different%20tasks.%20All%20hardware%20and%20software%20is%20made%0Aopen%20source%2C%20and%20the%20datasets%20are%20publicly%20available%20at%20causalchamber.org%20or%0Athrough%20the%20Python%20package%20causalchamber.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.11341v2&entry.124074799=Read"},
{"title": "Prediction Instability in Machine Learning Ensembles", "author": "Jeremy Kedziora", "abstract": "  In machine learning ensembles predictions from multiple models are\naggregated. Despite widespread use and strong performance of ensembles in\napplied problems little is known about the mathematical properties of\naggregating models and associated consequences for safe, explainable use of\nsuch models. In this paper we prove a theorem that shows that any ensemble will\nexhibit at least one of the following forms of prediction instability. It will\neither ignore agreement among all underlying models, change its mind when none\nof the underlying models have done so, or be manipulable through inclusion or\nexclusion of options it would never actually predict. As a consequence,\nensemble aggregation procedures will always need to balance the benefits of\ninformation use against the risk of these prediction instabilities. This\nanalysis also sheds light on what specific forms of prediction instability to\nexpect from particular ensemble algorithms; for example popular tree ensembles\nlike random forest, or xgboost will violate basic, intuitive fairness\nproperties. Finally, we show that this can be ameliorated by using consistent\nmodels in asymptotic conditions.\n", "link": "http://arxiv.org/abs/2407.03194v5", "date": "2024-08-26", "relevancy": 1.3815, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4692}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4536}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prediction%20Instability%20in%20Machine%20Learning%20Ensembles&body=Title%3A%20Prediction%20Instability%20in%20Machine%20Learning%20Ensembles%0AAuthor%3A%20Jeremy%20Kedziora%0AAbstract%3A%20%20%20In%20machine%20learning%20ensembles%20predictions%20from%20multiple%20models%20are%0Aaggregated.%20Despite%20widespread%20use%20and%20strong%20performance%20of%20ensembles%20in%0Aapplied%20problems%20little%20is%20known%20about%20the%20mathematical%20properties%20of%0Aaggregating%20models%20and%20associated%20consequences%20for%20safe%2C%20explainable%20use%20of%0Asuch%20models.%20In%20this%20paper%20we%20prove%20a%20theorem%20that%20shows%20that%20any%20ensemble%20will%0Aexhibit%20at%20least%20one%20of%20the%20following%20forms%20of%20prediction%20instability.%20It%20will%0Aeither%20ignore%20agreement%20among%20all%20underlying%20models%2C%20change%20its%20mind%20when%20none%0Aof%20the%20underlying%20models%20have%20done%20so%2C%20or%20be%20manipulable%20through%20inclusion%20or%0Aexclusion%20of%20options%20it%20would%20never%20actually%20predict.%20As%20a%20consequence%2C%0Aensemble%20aggregation%20procedures%20will%20always%20need%20to%20balance%20the%20benefits%20of%0Ainformation%20use%20against%20the%20risk%20of%20these%20prediction%20instabilities.%20This%0Aanalysis%20also%20sheds%20light%20on%20what%20specific%20forms%20of%20prediction%20instability%20to%0Aexpect%20from%20particular%20ensemble%20algorithms%3B%20for%20example%20popular%20tree%20ensembles%0Alike%20random%20forest%2C%20or%20xgboost%20will%20violate%20basic%2C%20intuitive%20fairness%0Aproperties.%20Finally%2C%20we%20show%20that%20this%20can%20be%20ameliorated%20by%20using%20consistent%0Amodels%20in%20asymptotic%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03194v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrediction%2520Instability%2520in%2520Machine%2520Learning%2520Ensembles%26entry.906535625%3DJeremy%2520Kedziora%26entry.1292438233%3D%2520%2520In%2520machine%2520learning%2520ensembles%2520predictions%2520from%2520multiple%2520models%2520are%250Aaggregated.%2520Despite%2520widespread%2520use%2520and%2520strong%2520performance%2520of%2520ensembles%2520in%250Aapplied%2520problems%2520little%2520is%2520known%2520about%2520the%2520mathematical%2520properties%2520of%250Aaggregating%2520models%2520and%2520associated%2520consequences%2520for%2520safe%252C%2520explainable%2520use%2520of%250Asuch%2520models.%2520In%2520this%2520paper%2520we%2520prove%2520a%2520theorem%2520that%2520shows%2520that%2520any%2520ensemble%2520will%250Aexhibit%2520at%2520least%2520one%2520of%2520the%2520following%2520forms%2520of%2520prediction%2520instability.%2520It%2520will%250Aeither%2520ignore%2520agreement%2520among%2520all%2520underlying%2520models%252C%2520change%2520its%2520mind%2520when%2520none%250Aof%2520the%2520underlying%2520models%2520have%2520done%2520so%252C%2520or%2520be%2520manipulable%2520through%2520inclusion%2520or%250Aexclusion%2520of%2520options%2520it%2520would%2520never%2520actually%2520predict.%2520As%2520a%2520consequence%252C%250Aensemble%2520aggregation%2520procedures%2520will%2520always%2520need%2520to%2520balance%2520the%2520benefits%2520of%250Ainformation%2520use%2520against%2520the%2520risk%2520of%2520these%2520prediction%2520instabilities.%2520This%250Aanalysis%2520also%2520sheds%2520light%2520on%2520what%2520specific%2520forms%2520of%2520prediction%2520instability%2520to%250Aexpect%2520from%2520particular%2520ensemble%2520algorithms%253B%2520for%2520example%2520popular%2520tree%2520ensembles%250Alike%2520random%2520forest%252C%2520or%2520xgboost%2520will%2520violate%2520basic%252C%2520intuitive%2520fairness%250Aproperties.%2520Finally%252C%2520we%2520show%2520that%2520this%2520can%2520be%2520ameliorated%2520by%2520using%2520consistent%250Amodels%2520in%2520asymptotic%2520conditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03194v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prediction%20Instability%20in%20Machine%20Learning%20Ensembles&entry.906535625=Jeremy%20Kedziora&entry.1292438233=%20%20In%20machine%20learning%20ensembles%20predictions%20from%20multiple%20models%20are%0Aaggregated.%20Despite%20widespread%20use%20and%20strong%20performance%20of%20ensembles%20in%0Aapplied%20problems%20little%20is%20known%20about%20the%20mathematical%20properties%20of%0Aaggregating%20models%20and%20associated%20consequences%20for%20safe%2C%20explainable%20use%20of%0Asuch%20models.%20In%20this%20paper%20we%20prove%20a%20theorem%20that%20shows%20that%20any%20ensemble%20will%0Aexhibit%20at%20least%20one%20of%20the%20following%20forms%20of%20prediction%20instability.%20It%20will%0Aeither%20ignore%20agreement%20among%20all%20underlying%20models%2C%20change%20its%20mind%20when%20none%0Aof%20the%20underlying%20models%20have%20done%20so%2C%20or%20be%20manipulable%20through%20inclusion%20or%0Aexclusion%20of%20options%20it%20would%20never%20actually%20predict.%20As%20a%20consequence%2C%0Aensemble%20aggregation%20procedures%20will%20always%20need%20to%20balance%20the%20benefits%20of%0Ainformation%20use%20against%20the%20risk%20of%20these%20prediction%20instabilities.%20This%0Aanalysis%20also%20sheds%20light%20on%20what%20specific%20forms%20of%20prediction%20instability%20to%0Aexpect%20from%20particular%20ensemble%20algorithms%3B%20for%20example%20popular%20tree%20ensembles%0Alike%20random%20forest%2C%20or%20xgboost%20will%20violate%20basic%2C%20intuitive%20fairness%0Aproperties.%20Finally%2C%20we%20show%20that%20this%20can%20be%20ameliorated%20by%20using%20consistent%0Amodels%20in%20asymptotic%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03194v5&entry.124074799=Read"},
{"title": "Beyond Scale: The Diversity Coefficient as a Data Quality Metric for\n  Variability in Natural Language Data", "author": "Brando Miranda and Alycia Lee and Sudharsan Sundar and Allison Casasola and Sanmi Koyejo", "abstract": "  Current trends in pre-training Large Language Models (LLMs) primarily focus\non the scaling of model and dataset size. While the quality of pre-training\ndata is considered an important factor for training powerful LLMs, it remains a\nnebulous concept that has not been rigorously characterized. To this end, we\npropose a formalization of one key aspect of data quality -- measuring the\nvariability of natural language data -- specifically via a measure we call the\ndiversity coefficient. Our empirical analysis shows that the proposed diversity\ncoefficient aligns with the intuitive properties of diversity and variability,\ne.g., it increases as the number of latent concepts increases. Then, we measure\nthe diversity coefficient of publicly available pre-training datasets and\ndemonstrate that their formal diversity is high compared to theoretical lower\nand upper bounds. Finally, we conduct a comprehensive set of controlled\ninterventional experiments with GPT-2 and LLaMAv2 that demonstrate the\ndiversity coefficient of pre-training data characterizes useful aspects of\ndownstream model evaluation performance -- totaling 44 models of various sizes\n(51M to 7B parameters). We conclude that our formal notion of diversity is an\nimportant aspect of data quality that captures variability and causally leads\nto improved evaluation performance.\n", "link": "http://arxiv.org/abs/2306.13840v3", "date": "2024-08-26", "relevancy": 1.4169, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.487}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4853}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Scale%3A%20The%20Diversity%20Coefficient%20as%20a%20Data%20Quality%20Metric%20for%0A%20%20Variability%20in%20Natural%20Language%20Data&body=Title%3A%20Beyond%20Scale%3A%20The%20Diversity%20Coefficient%20as%20a%20Data%20Quality%20Metric%20for%0A%20%20Variability%20in%20Natural%20Language%20Data%0AAuthor%3A%20Brando%20Miranda%20and%20Alycia%20Lee%20and%20Sudharsan%20Sundar%20and%20Allison%20Casasola%20and%20Sanmi%20Koyejo%0AAbstract%3A%20%20%20Current%20trends%20in%20pre-training%20Large%20Language%20Models%20%28LLMs%29%20primarily%20focus%0Aon%20the%20scaling%20of%20model%20and%20dataset%20size.%20While%20the%20quality%20of%20pre-training%0Adata%20is%20considered%20an%20important%20factor%20for%20training%20powerful%20LLMs%2C%20it%20remains%20a%0Anebulous%20concept%20that%20has%20not%20been%20rigorously%20characterized.%20To%20this%20end%2C%20we%0Apropose%20a%20formalization%20of%20one%20key%20aspect%20of%20data%20quality%20--%20measuring%20the%0Avariability%20of%20natural%20language%20data%20--%20specifically%20via%20a%20measure%20we%20call%20the%0Adiversity%20coefficient.%20Our%20empirical%20analysis%20shows%20that%20the%20proposed%20diversity%0Acoefficient%20aligns%20with%20the%20intuitive%20properties%20of%20diversity%20and%20variability%2C%0Ae.g.%2C%20it%20increases%20as%20the%20number%20of%20latent%20concepts%20increases.%20Then%2C%20we%20measure%0Athe%20diversity%20coefficient%20of%20publicly%20available%20pre-training%20datasets%20and%0Ademonstrate%20that%20their%20formal%20diversity%20is%20high%20compared%20to%20theoretical%20lower%0Aand%20upper%20bounds.%20Finally%2C%20we%20conduct%20a%20comprehensive%20set%20of%20controlled%0Ainterventional%20experiments%20with%20GPT-2%20and%20LLaMAv2%20that%20demonstrate%20the%0Adiversity%20coefficient%20of%20pre-training%20data%20characterizes%20useful%20aspects%20of%0Adownstream%20model%20evaluation%20performance%20--%20totaling%2044%20models%20of%20various%20sizes%0A%2851M%20to%207B%20parameters%29.%20We%20conclude%20that%20our%20formal%20notion%20of%20diversity%20is%20an%0Aimportant%20aspect%20of%20data%20quality%20that%20captures%20variability%20and%20causally%20leads%0Ato%20improved%20evaluation%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.13840v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Scale%253A%2520The%2520Diversity%2520Coefficient%2520as%2520a%2520Data%2520Quality%2520Metric%2520for%250A%2520%2520Variability%2520in%2520Natural%2520Language%2520Data%26entry.906535625%3DBrando%2520Miranda%2520and%2520Alycia%2520Lee%2520and%2520Sudharsan%2520Sundar%2520and%2520Allison%2520Casasola%2520and%2520Sanmi%2520Koyejo%26entry.1292438233%3D%2520%2520Current%2520trends%2520in%2520pre-training%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520primarily%2520focus%250Aon%2520the%2520scaling%2520of%2520model%2520and%2520dataset%2520size.%2520While%2520the%2520quality%2520of%2520pre-training%250Adata%2520is%2520considered%2520an%2520important%2520factor%2520for%2520training%2520powerful%2520LLMs%252C%2520it%2520remains%2520a%250Anebulous%2520concept%2520that%2520has%2520not%2520been%2520rigorously%2520characterized.%2520To%2520this%2520end%252C%2520we%250Apropose%2520a%2520formalization%2520of%2520one%2520key%2520aspect%2520of%2520data%2520quality%2520--%2520measuring%2520the%250Avariability%2520of%2520natural%2520language%2520data%2520--%2520specifically%2520via%2520a%2520measure%2520we%2520call%2520the%250Adiversity%2520coefficient.%2520Our%2520empirical%2520analysis%2520shows%2520that%2520the%2520proposed%2520diversity%250Acoefficient%2520aligns%2520with%2520the%2520intuitive%2520properties%2520of%2520diversity%2520and%2520variability%252C%250Ae.g.%252C%2520it%2520increases%2520as%2520the%2520number%2520of%2520latent%2520concepts%2520increases.%2520Then%252C%2520we%2520measure%250Athe%2520diversity%2520coefficient%2520of%2520publicly%2520available%2520pre-training%2520datasets%2520and%250Ademonstrate%2520that%2520their%2520formal%2520diversity%2520is%2520high%2520compared%2520to%2520theoretical%2520lower%250Aand%2520upper%2520bounds.%2520Finally%252C%2520we%2520conduct%2520a%2520comprehensive%2520set%2520of%2520controlled%250Ainterventional%2520experiments%2520with%2520GPT-2%2520and%2520LLaMAv2%2520that%2520demonstrate%2520the%250Adiversity%2520coefficient%2520of%2520pre-training%2520data%2520characterizes%2520useful%2520aspects%2520of%250Adownstream%2520model%2520evaluation%2520performance%2520--%2520totaling%252044%2520models%2520of%2520various%2520sizes%250A%252851M%2520to%25207B%2520parameters%2529.%2520We%2520conclude%2520that%2520our%2520formal%2520notion%2520of%2520diversity%2520is%2520an%250Aimportant%2520aspect%2520of%2520data%2520quality%2520that%2520captures%2520variability%2520and%2520causally%2520leads%250Ato%2520improved%2520evaluation%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.13840v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Scale%3A%20The%20Diversity%20Coefficient%20as%20a%20Data%20Quality%20Metric%20for%0A%20%20Variability%20in%20Natural%20Language%20Data&entry.906535625=Brando%20Miranda%20and%20Alycia%20Lee%20and%20Sudharsan%20Sundar%20and%20Allison%20Casasola%20and%20Sanmi%20Koyejo&entry.1292438233=%20%20Current%20trends%20in%20pre-training%20Large%20Language%20Models%20%28LLMs%29%20primarily%20focus%0Aon%20the%20scaling%20of%20model%20and%20dataset%20size.%20While%20the%20quality%20of%20pre-training%0Adata%20is%20considered%20an%20important%20factor%20for%20training%20powerful%20LLMs%2C%20it%20remains%20a%0Anebulous%20concept%20that%20has%20not%20been%20rigorously%20characterized.%20To%20this%20end%2C%20we%0Apropose%20a%20formalization%20of%20one%20key%20aspect%20of%20data%20quality%20--%20measuring%20the%0Avariability%20of%20natural%20language%20data%20--%20specifically%20via%20a%20measure%20we%20call%20the%0Adiversity%20coefficient.%20Our%20empirical%20analysis%20shows%20that%20the%20proposed%20diversity%0Acoefficient%20aligns%20with%20the%20intuitive%20properties%20of%20diversity%20and%20variability%2C%0Ae.g.%2C%20it%20increases%20as%20the%20number%20of%20latent%20concepts%20increases.%20Then%2C%20we%20measure%0Athe%20diversity%20coefficient%20of%20publicly%20available%20pre-training%20datasets%20and%0Ademonstrate%20that%20their%20formal%20diversity%20is%20high%20compared%20to%20theoretical%20lower%0Aand%20upper%20bounds.%20Finally%2C%20we%20conduct%20a%20comprehensive%20set%20of%20controlled%0Ainterventional%20experiments%20with%20GPT-2%20and%20LLaMAv2%20that%20demonstrate%20the%0Adiversity%20coefficient%20of%20pre-training%20data%20characterizes%20useful%20aspects%20of%0Adownstream%20model%20evaluation%20performance%20--%20totaling%2044%20models%20of%20various%20sizes%0A%2851M%20to%207B%20parameters%29.%20We%20conclude%20that%20our%20formal%20notion%20of%20diversity%20is%20an%0Aimportant%20aspect%20of%20data%20quality%20that%20captures%20variability%20and%20causally%20leads%0Ato%20improved%20evaluation%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.13840v3&entry.124074799=Read"},
{"title": "Binocular Model: A deep learning solution for online melt pool\n  temperature analysis using dual-wavelength Imaging Pyrometry", "author": "Javid Akhavan and Chaitanya Krishna Vallabh and Xiayun Zhao and Souran Manoochehri", "abstract": "  In metal Additive Manufacturing (AM), monitoring the temperature of the Melt\nPool (MP) is crucial for ensuring part quality, process stability, defect\nprevention, and overall process optimization. Traditional methods, are slow to\nconverge and require extensive manual effort to translate data into actionable\ninsights, rendering them impractical for real-time monitoring and control. To\naddress this challenge, we propose an Artificial Intelligence (AI)-based\nsolution aimed at reducing manual data processing reliance and improving the\nefficiency of transitioning from data to insight. In our study, we utilize a\ndataset comprising dual-wavelength real-time process monitoring data and\ncorresponding temperature maps. We introduce a deep learning model called the\n\"Binocular model,\" which exploits dual input observations to perform a precise\nanalysis of MP temperature in Laser Powder Bed Fusion (L-PBF). Through advanced\ndeep learning techniques, we seamlessly convert raw data into temperature maps,\nsignificantly streamlining the process and enabling batch processing at a rate\nof up to 750 frames per second, approximately 1000 times faster than\nconventional methods. Our Binocular model achieves high accuracy in temperature\nestimation, evidenced by a 0.95 R-squared score, while simultaneously enhancing\nprocessing efficiency by a factor of $\\sim1000x$ times. This model directly\naddresses the challenge of real-time MP temperature monitoring and offers\ninsights into the encountered constraints and the benefits of our Deep\nLearning-based approach. By combining efficiency and precision, our work\ncontributes to the advancement of temperature monitoring in L-PBF, thus driving\nprogress in the field of metal AM.\n", "link": "http://arxiv.org/abs/2408.11126v2", "date": "2024-08-26", "relevancy": 1.5313, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5215}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5191}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5025}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Binocular%20Model%3A%20A%20deep%20learning%20solution%20for%20online%20melt%20pool%0A%20%20temperature%20analysis%20using%20dual-wavelength%20Imaging%20Pyrometry&body=Title%3A%20Binocular%20Model%3A%20A%20deep%20learning%20solution%20for%20online%20melt%20pool%0A%20%20temperature%20analysis%20using%20dual-wavelength%20Imaging%20Pyrometry%0AAuthor%3A%20Javid%20Akhavan%20and%20Chaitanya%20Krishna%20Vallabh%20and%20Xiayun%20Zhao%20and%20Souran%20Manoochehri%0AAbstract%3A%20%20%20In%20metal%20Additive%20Manufacturing%20%28AM%29%2C%20monitoring%20the%20temperature%20of%20the%20Melt%0APool%20%28MP%29%20is%20crucial%20for%20ensuring%20part%20quality%2C%20process%20stability%2C%20defect%0Aprevention%2C%20and%20overall%20process%20optimization.%20Traditional%20methods%2C%20are%20slow%20to%0Aconverge%20and%20require%20extensive%20manual%20effort%20to%20translate%20data%20into%20actionable%0Ainsights%2C%20rendering%20them%20impractical%20for%20real-time%20monitoring%20and%20control.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20an%20Artificial%20Intelligence%20%28AI%29-based%0Asolution%20aimed%20at%20reducing%20manual%20data%20processing%20reliance%20and%20improving%20the%0Aefficiency%20of%20transitioning%20from%20data%20to%20insight.%20In%20our%20study%2C%20we%20utilize%20a%0Adataset%20comprising%20dual-wavelength%20real-time%20process%20monitoring%20data%20and%0Acorresponding%20temperature%20maps.%20We%20introduce%20a%20deep%20learning%20model%20called%20the%0A%22Binocular%20model%2C%22%20which%20exploits%20dual%20input%20observations%20to%20perform%20a%20precise%0Aanalysis%20of%20MP%20temperature%20in%20Laser%20Powder%20Bed%20Fusion%20%28L-PBF%29.%20Through%20advanced%0Adeep%20learning%20techniques%2C%20we%20seamlessly%20convert%20raw%20data%20into%20temperature%20maps%2C%0Asignificantly%20streamlining%20the%20process%20and%20enabling%20batch%20processing%20at%20a%20rate%0Aof%20up%20to%20750%20frames%20per%20second%2C%20approximately%201000%20times%20faster%20than%0Aconventional%20methods.%20Our%20Binocular%20model%20achieves%20high%20accuracy%20in%20temperature%0Aestimation%2C%20evidenced%20by%20a%200.95%20R-squared%20score%2C%20while%20simultaneously%20enhancing%0Aprocessing%20efficiency%20by%20a%20factor%20of%20%24%5Csim1000x%24%20times.%20This%20model%20directly%0Aaddresses%20the%20challenge%20of%20real-time%20MP%20temperature%20monitoring%20and%20offers%0Ainsights%20into%20the%20encountered%20constraints%20and%20the%20benefits%20of%20our%20Deep%0ALearning-based%20approach.%20By%20combining%20efficiency%20and%20precision%2C%20our%20work%0Acontributes%20to%20the%20advancement%20of%20temperature%20monitoring%20in%20L-PBF%2C%20thus%20driving%0Aprogress%20in%20the%20field%20of%20metal%20AM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11126v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBinocular%2520Model%253A%2520A%2520deep%2520learning%2520solution%2520for%2520online%2520melt%2520pool%250A%2520%2520temperature%2520analysis%2520using%2520dual-wavelength%2520Imaging%2520Pyrometry%26entry.906535625%3DJavid%2520Akhavan%2520and%2520Chaitanya%2520Krishna%2520Vallabh%2520and%2520Xiayun%2520Zhao%2520and%2520Souran%2520Manoochehri%26entry.1292438233%3D%2520%2520In%2520metal%2520Additive%2520Manufacturing%2520%2528AM%2529%252C%2520monitoring%2520the%2520temperature%2520of%2520the%2520Melt%250APool%2520%2528MP%2529%2520is%2520crucial%2520for%2520ensuring%2520part%2520quality%252C%2520process%2520stability%252C%2520defect%250Aprevention%252C%2520and%2520overall%2520process%2520optimization.%2520Traditional%2520methods%252C%2520are%2520slow%2520to%250Aconverge%2520and%2520require%2520extensive%2520manual%2520effort%2520to%2520translate%2520data%2520into%2520actionable%250Ainsights%252C%2520rendering%2520them%2520impractical%2520for%2520real-time%2520monitoring%2520and%2520control.%2520To%250Aaddress%2520this%2520challenge%252C%2520we%2520propose%2520an%2520Artificial%2520Intelligence%2520%2528AI%2529-based%250Asolution%2520aimed%2520at%2520reducing%2520manual%2520data%2520processing%2520reliance%2520and%2520improving%2520the%250Aefficiency%2520of%2520transitioning%2520from%2520data%2520to%2520insight.%2520In%2520our%2520study%252C%2520we%2520utilize%2520a%250Adataset%2520comprising%2520dual-wavelength%2520real-time%2520process%2520monitoring%2520data%2520and%250Acorresponding%2520temperature%2520maps.%2520We%2520introduce%2520a%2520deep%2520learning%2520model%2520called%2520the%250A%2522Binocular%2520model%252C%2522%2520which%2520exploits%2520dual%2520input%2520observations%2520to%2520perform%2520a%2520precise%250Aanalysis%2520of%2520MP%2520temperature%2520in%2520Laser%2520Powder%2520Bed%2520Fusion%2520%2528L-PBF%2529.%2520Through%2520advanced%250Adeep%2520learning%2520techniques%252C%2520we%2520seamlessly%2520convert%2520raw%2520data%2520into%2520temperature%2520maps%252C%250Asignificantly%2520streamlining%2520the%2520process%2520and%2520enabling%2520batch%2520processing%2520at%2520a%2520rate%250Aof%2520up%2520to%2520750%2520frames%2520per%2520second%252C%2520approximately%25201000%2520times%2520faster%2520than%250Aconventional%2520methods.%2520Our%2520Binocular%2520model%2520achieves%2520high%2520accuracy%2520in%2520temperature%250Aestimation%252C%2520evidenced%2520by%2520a%25200.95%2520R-squared%2520score%252C%2520while%2520simultaneously%2520enhancing%250Aprocessing%2520efficiency%2520by%2520a%2520factor%2520of%2520%2524%255Csim1000x%2524%2520times.%2520This%2520model%2520directly%250Aaddresses%2520the%2520challenge%2520of%2520real-time%2520MP%2520temperature%2520monitoring%2520and%2520offers%250Ainsights%2520into%2520the%2520encountered%2520constraints%2520and%2520the%2520benefits%2520of%2520our%2520Deep%250ALearning-based%2520approach.%2520By%2520combining%2520efficiency%2520and%2520precision%252C%2520our%2520work%250Acontributes%2520to%2520the%2520advancement%2520of%2520temperature%2520monitoring%2520in%2520L-PBF%252C%2520thus%2520driving%250Aprogress%2520in%2520the%2520field%2520of%2520metal%2520AM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11126v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Binocular%20Model%3A%20A%20deep%20learning%20solution%20for%20online%20melt%20pool%0A%20%20temperature%20analysis%20using%20dual-wavelength%20Imaging%20Pyrometry&entry.906535625=Javid%20Akhavan%20and%20Chaitanya%20Krishna%20Vallabh%20and%20Xiayun%20Zhao%20and%20Souran%20Manoochehri&entry.1292438233=%20%20In%20metal%20Additive%20Manufacturing%20%28AM%29%2C%20monitoring%20the%20temperature%20of%20the%20Melt%0APool%20%28MP%29%20is%20crucial%20for%20ensuring%20part%20quality%2C%20process%20stability%2C%20defect%0Aprevention%2C%20and%20overall%20process%20optimization.%20Traditional%20methods%2C%20are%20slow%20to%0Aconverge%20and%20require%20extensive%20manual%20effort%20to%20translate%20data%20into%20actionable%0Ainsights%2C%20rendering%20them%20impractical%20for%20real-time%20monitoring%20and%20control.%20To%0Aaddress%20this%20challenge%2C%20we%20propose%20an%20Artificial%20Intelligence%20%28AI%29-based%0Asolution%20aimed%20at%20reducing%20manual%20data%20processing%20reliance%20and%20improving%20the%0Aefficiency%20of%20transitioning%20from%20data%20to%20insight.%20In%20our%20study%2C%20we%20utilize%20a%0Adataset%20comprising%20dual-wavelength%20real-time%20process%20monitoring%20data%20and%0Acorresponding%20temperature%20maps.%20We%20introduce%20a%20deep%20learning%20model%20called%20the%0A%22Binocular%20model%2C%22%20which%20exploits%20dual%20input%20observations%20to%20perform%20a%20precise%0Aanalysis%20of%20MP%20temperature%20in%20Laser%20Powder%20Bed%20Fusion%20%28L-PBF%29.%20Through%20advanced%0Adeep%20learning%20techniques%2C%20we%20seamlessly%20convert%20raw%20data%20into%20temperature%20maps%2C%0Asignificantly%20streamlining%20the%20process%20and%20enabling%20batch%20processing%20at%20a%20rate%0Aof%20up%20to%20750%20frames%20per%20second%2C%20approximately%201000%20times%20faster%20than%0Aconventional%20methods.%20Our%20Binocular%20model%20achieves%20high%20accuracy%20in%20temperature%0Aestimation%2C%20evidenced%20by%20a%200.95%20R-squared%20score%2C%20while%20simultaneously%20enhancing%0Aprocessing%20efficiency%20by%20a%20factor%20of%20%24%5Csim1000x%24%20times.%20This%20model%20directly%0Aaddresses%20the%20challenge%20of%20real-time%20MP%20temperature%20monitoring%20and%20offers%0Ainsights%20into%20the%20encountered%20constraints%20and%20the%20benefits%20of%20our%20Deep%0ALearning-based%20approach.%20By%20combining%20efficiency%20and%20precision%2C%20our%20work%0Acontributes%20to%20the%20advancement%20of%20temperature%20monitoring%20in%20L-PBF%2C%20thus%20driving%0Aprogress%20in%20the%20field%20of%20metal%20AM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11126v2&entry.124074799=Read"},
{"title": "SWE-bench-java: A GitHub Issue Resolving Benchmark for Java", "author": "Daoguang Zan and Zhirong Huang and Ailun Yu and Shaoxin Lin and Yifan Shi and Wei Liu and Dong Chen and Zongshuai Qi and Hao Yu and Lei Yu and Dezhi Ran and Muhan Zeng and Bo Shen and Pan Bian and Guangtai Liang and Bei Guan and Pengjie Huang and Tao Xie and Yongji Wang and Qianxiang Wang", "abstract": "  GitHub issue resolving is a critical task in software engineering, recently\ngaining significant attention in both industry and academia. Within this task,\nSWE-bench has been released to evaluate issue resolving capabilities of large\nlanguage models (LLMs), but has so far only focused on Python version. However,\nsupporting more programming languages is also important, as there is a strong\ndemand in industry. As a first step toward multilingual support, we have\ndeveloped a Java version of SWE-bench, called SWE-bench-java. We have publicly\nreleased the dataset, along with the corresponding Docker-based evaluation\nenvironment and leaderboard, which will be continuously maintained and updated\nin the coming months. To verify the reliability of SWE-bench-java, we implement\na classic method SWE-agent and test several powerful LLMs on it. As is well\nknown, developing a high-quality multi-lingual benchmark is time-consuming and\nlabor-intensive, so we welcome contributions through pull requests or\ncollaboration to accelerate its iteration and refinement, paving the way for\nfully automated programming.\n", "link": "http://arxiv.org/abs/2408.14354v1", "date": "2024-08-26", "relevancy": 1.0806, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.3716}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.3577}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SWE-bench-java%3A%20A%20GitHub%20Issue%20Resolving%20Benchmark%20for%20Java&body=Title%3A%20SWE-bench-java%3A%20A%20GitHub%20Issue%20Resolving%20Benchmark%20for%20Java%0AAuthor%3A%20Daoguang%20Zan%20and%20Zhirong%20Huang%20and%20Ailun%20Yu%20and%20Shaoxin%20Lin%20and%20Yifan%20Shi%20and%20Wei%20Liu%20and%20Dong%20Chen%20and%20Zongshuai%20Qi%20and%20Hao%20Yu%20and%20Lei%20Yu%20and%20Dezhi%20Ran%20and%20Muhan%20Zeng%20and%20Bo%20Shen%20and%20Pan%20Bian%20and%20Guangtai%20Liang%20and%20Bei%20Guan%20and%20Pengjie%20Huang%20and%20Tao%20Xie%20and%20Yongji%20Wang%20and%20Qianxiang%20Wang%0AAbstract%3A%20%20%20GitHub%20issue%20resolving%20is%20a%20critical%20task%20in%20software%20engineering%2C%20recently%0Againing%20significant%20attention%20in%20both%20industry%20and%20academia.%20Within%20this%20task%2C%0ASWE-bench%20has%20been%20released%20to%20evaluate%20issue%20resolving%20capabilities%20of%20large%0Alanguage%20models%20%28LLMs%29%2C%20but%20has%20so%20far%20only%20focused%20on%20Python%20version.%20However%2C%0Asupporting%20more%20programming%20languages%20is%20also%20important%2C%20as%20there%20is%20a%20strong%0Ademand%20in%20industry.%20As%20a%20first%20step%20toward%20multilingual%20support%2C%20we%20have%0Adeveloped%20a%20Java%20version%20of%20SWE-bench%2C%20called%20SWE-bench-java.%20We%20have%20publicly%0Areleased%20the%20dataset%2C%20along%20with%20the%20corresponding%20Docker-based%20evaluation%0Aenvironment%20and%20leaderboard%2C%20which%20will%20be%20continuously%20maintained%20and%20updated%0Ain%20the%20coming%20months.%20To%20verify%20the%20reliability%20of%20SWE-bench-java%2C%20we%20implement%0Aa%20classic%20method%20SWE-agent%20and%20test%20several%20powerful%20LLMs%20on%20it.%20As%20is%20well%0Aknown%2C%20developing%20a%20high-quality%20multi-lingual%20benchmark%20is%20time-consuming%20and%0Alabor-intensive%2C%20so%20we%20welcome%20contributions%20through%20pull%20requests%20or%0Acollaboration%20to%20accelerate%20its%20iteration%20and%20refinement%2C%20paving%20the%20way%20for%0Afully%20automated%20programming.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14354v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSWE-bench-java%253A%2520A%2520GitHub%2520Issue%2520Resolving%2520Benchmark%2520for%2520Java%26entry.906535625%3DDaoguang%2520Zan%2520and%2520Zhirong%2520Huang%2520and%2520Ailun%2520Yu%2520and%2520Shaoxin%2520Lin%2520and%2520Yifan%2520Shi%2520and%2520Wei%2520Liu%2520and%2520Dong%2520Chen%2520and%2520Zongshuai%2520Qi%2520and%2520Hao%2520Yu%2520and%2520Lei%2520Yu%2520and%2520Dezhi%2520Ran%2520and%2520Muhan%2520Zeng%2520and%2520Bo%2520Shen%2520and%2520Pan%2520Bian%2520and%2520Guangtai%2520Liang%2520and%2520Bei%2520Guan%2520and%2520Pengjie%2520Huang%2520and%2520Tao%2520Xie%2520and%2520Yongji%2520Wang%2520and%2520Qianxiang%2520Wang%26entry.1292438233%3D%2520%2520GitHub%2520issue%2520resolving%2520is%2520a%2520critical%2520task%2520in%2520software%2520engineering%252C%2520recently%250Againing%2520significant%2520attention%2520in%2520both%2520industry%2520and%2520academia.%2520Within%2520this%2520task%252C%250ASWE-bench%2520has%2520been%2520released%2520to%2520evaluate%2520issue%2520resolving%2520capabilities%2520of%2520large%250Alanguage%2520models%2520%2528LLMs%2529%252C%2520but%2520has%2520so%2520far%2520only%2520focused%2520on%2520Python%2520version.%2520However%252C%250Asupporting%2520more%2520programming%2520languages%2520is%2520also%2520important%252C%2520as%2520there%2520is%2520a%2520strong%250Ademand%2520in%2520industry.%2520As%2520a%2520first%2520step%2520toward%2520multilingual%2520support%252C%2520we%2520have%250Adeveloped%2520a%2520Java%2520version%2520of%2520SWE-bench%252C%2520called%2520SWE-bench-java.%2520We%2520have%2520publicly%250Areleased%2520the%2520dataset%252C%2520along%2520with%2520the%2520corresponding%2520Docker-based%2520evaluation%250Aenvironment%2520and%2520leaderboard%252C%2520which%2520will%2520be%2520continuously%2520maintained%2520and%2520updated%250Ain%2520the%2520coming%2520months.%2520To%2520verify%2520the%2520reliability%2520of%2520SWE-bench-java%252C%2520we%2520implement%250Aa%2520classic%2520method%2520SWE-agent%2520and%2520test%2520several%2520powerful%2520LLMs%2520on%2520it.%2520As%2520is%2520well%250Aknown%252C%2520developing%2520a%2520high-quality%2520multi-lingual%2520benchmark%2520is%2520time-consuming%2520and%250Alabor-intensive%252C%2520so%2520we%2520welcome%2520contributions%2520through%2520pull%2520requests%2520or%250Acollaboration%2520to%2520accelerate%2520its%2520iteration%2520and%2520refinement%252C%2520paving%2520the%2520way%2520for%250Afully%2520automated%2520programming.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14354v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SWE-bench-java%3A%20A%20GitHub%20Issue%20Resolving%20Benchmark%20for%20Java&entry.906535625=Daoguang%20Zan%20and%20Zhirong%20Huang%20and%20Ailun%20Yu%20and%20Shaoxin%20Lin%20and%20Yifan%20Shi%20and%20Wei%20Liu%20and%20Dong%20Chen%20and%20Zongshuai%20Qi%20and%20Hao%20Yu%20and%20Lei%20Yu%20and%20Dezhi%20Ran%20and%20Muhan%20Zeng%20and%20Bo%20Shen%20and%20Pan%20Bian%20and%20Guangtai%20Liang%20and%20Bei%20Guan%20and%20Pengjie%20Huang%20and%20Tao%20Xie%20and%20Yongji%20Wang%20and%20Qianxiang%20Wang&entry.1292438233=%20%20GitHub%20issue%20resolving%20is%20a%20critical%20task%20in%20software%20engineering%2C%20recently%0Againing%20significant%20attention%20in%20both%20industry%20and%20academia.%20Within%20this%20task%2C%0ASWE-bench%20has%20been%20released%20to%20evaluate%20issue%20resolving%20capabilities%20of%20large%0Alanguage%20models%20%28LLMs%29%2C%20but%20has%20so%20far%20only%20focused%20on%20Python%20version.%20However%2C%0Asupporting%20more%20programming%20languages%20is%20also%20important%2C%20as%20there%20is%20a%20strong%0Ademand%20in%20industry.%20As%20a%20first%20step%20toward%20multilingual%20support%2C%20we%20have%0Adeveloped%20a%20Java%20version%20of%20SWE-bench%2C%20called%20SWE-bench-java.%20We%20have%20publicly%0Areleased%20the%20dataset%2C%20along%20with%20the%20corresponding%20Docker-based%20evaluation%0Aenvironment%20and%20leaderboard%2C%20which%20will%20be%20continuously%20maintained%20and%20updated%0Ain%20the%20coming%20months.%20To%20verify%20the%20reliability%20of%20SWE-bench-java%2C%20we%20implement%0Aa%20classic%20method%20SWE-agent%20and%20test%20several%20powerful%20LLMs%20on%20it.%20As%20is%20well%0Aknown%2C%20developing%20a%20high-quality%20multi-lingual%20benchmark%20is%20time-consuming%20and%0Alabor-intensive%2C%20so%20we%20welcome%20contributions%20through%20pull%20requests%20or%0Acollaboration%20to%20accelerate%20its%20iteration%20and%20refinement%2C%20paving%20the%20way%20for%0Afully%20automated%20programming.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14354v1&entry.124074799=Read"},
{"title": "Collision-Free Trajectory Optimization in Cluttered Environments Using\n  Sums-of-Squares Programming", "author": "Yulin Li and Chunxin Zheng and Kai Chen and Yusen Xie and Xindong Tang and Michael Yu Wang and Jun Ma", "abstract": "  In this work, we propose a trajectory optimization approach for robot\nnavigation in cluttered 3D environments. We represent the robot's geometry as a\nsemialgebraic set defined by polynomial inequalities such that robots with\ngeneral shapes can be suitably characterized. To address the robot navigation\ntask in obstacle-dense environments, we exploit the free space directly to\nconstruct a sequence of free regions, and allocate each waypoint on the\ntrajectory to a specific region. Then, we incorporate a uniform scaling factor\nfor each free region, and formulate a Sums-of-Squares (SOS) optimization\nproblem that renders the containment relationship between the robot and the\nfree space computationally tractable. The SOS optimization problem is further\nreformulated to a semidefinite program (SDP), and the collision-free\nconstraints are shown to be equivalent to limiting the scaling factor along the\nentire trajectory. In this context, the robot at a specific configuration is\ntailored to stay within the free region. Next, to solve the trajectory\noptimization problem with the proposed safety constraints (which are implicitly\ndependent on the robot configurations), we derive the analytical solution to\nthe gradient of the minimum scaling factor with respect to the robot\nconfiguration. As a result, this seamlessly facilitates the use of\ngradient-based methods in efficient solving of the trajectory optimization\nproblem. Through a series of simulations and real-world experiments, the\nproposed trajectory optimization approach is validated in various challenging\nscenarios, and the results demonstrate its effectiveness in generating\ncollision-free trajectories in dense and intricate environments populated with\nobstacles. Our code is available at:\nhttps://github.com/lyl00/minimum_scaling_free_region\n", "link": "http://arxiv.org/abs/2404.05242v2", "date": "2024-08-26", "relevancy": 1.5754, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5492}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5341}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collision-Free%20Trajectory%20Optimization%20in%20Cluttered%20Environments%20Using%0A%20%20Sums-of-Squares%20Programming&body=Title%3A%20Collision-Free%20Trajectory%20Optimization%20in%20Cluttered%20Environments%20Using%0A%20%20Sums-of-Squares%20Programming%0AAuthor%3A%20Yulin%20Li%20and%20Chunxin%20Zheng%20and%20Kai%20Chen%20and%20Yusen%20Xie%20and%20Xindong%20Tang%20and%20Michael%20Yu%20Wang%20and%20Jun%20Ma%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20a%20trajectory%20optimization%20approach%20for%20robot%0Anavigation%20in%20cluttered%203D%20environments.%20We%20represent%20the%20robot%27s%20geometry%20as%20a%0Asemialgebraic%20set%20defined%20by%20polynomial%20inequalities%20such%20that%20robots%20with%0Ageneral%20shapes%20can%20be%20suitably%20characterized.%20To%20address%20the%20robot%20navigation%0Atask%20in%20obstacle-dense%20environments%2C%20we%20exploit%20the%20free%20space%20directly%20to%0Aconstruct%20a%20sequence%20of%20free%20regions%2C%20and%20allocate%20each%20waypoint%20on%20the%0Atrajectory%20to%20a%20specific%20region.%20Then%2C%20we%20incorporate%20a%20uniform%20scaling%20factor%0Afor%20each%20free%20region%2C%20and%20formulate%20a%20Sums-of-Squares%20%28SOS%29%20optimization%0Aproblem%20that%20renders%20the%20containment%20relationship%20between%20the%20robot%20and%20the%0Afree%20space%20computationally%20tractable.%20The%20SOS%20optimization%20problem%20is%20further%0Areformulated%20to%20a%20semidefinite%20program%20%28SDP%29%2C%20and%20the%20collision-free%0Aconstraints%20are%20shown%20to%20be%20equivalent%20to%20limiting%20the%20scaling%20factor%20along%20the%0Aentire%20trajectory.%20In%20this%20context%2C%20the%20robot%20at%20a%20specific%20configuration%20is%0Atailored%20to%20stay%20within%20the%20free%20region.%20Next%2C%20to%20solve%20the%20trajectory%0Aoptimization%20problem%20with%20the%20proposed%20safety%20constraints%20%28which%20are%20implicitly%0Adependent%20on%20the%20robot%20configurations%29%2C%20we%20derive%20the%20analytical%20solution%20to%0Athe%20gradient%20of%20the%20minimum%20scaling%20factor%20with%20respect%20to%20the%20robot%0Aconfiguration.%20As%20a%20result%2C%20this%20seamlessly%20facilitates%20the%20use%20of%0Agradient-based%20methods%20in%20efficient%20solving%20of%20the%20trajectory%20optimization%0Aproblem.%20Through%20a%20series%20of%20simulations%20and%20real-world%20experiments%2C%20the%0Aproposed%20trajectory%20optimization%20approach%20is%20validated%20in%20various%20challenging%0Ascenarios%2C%20and%20the%20results%20demonstrate%20its%20effectiveness%20in%20generating%0Acollision-free%20trajectories%20in%20dense%20and%20intricate%20environments%20populated%20with%0Aobstacles.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/lyl00/minimum_scaling_free_region%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05242v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollision-Free%2520Trajectory%2520Optimization%2520in%2520Cluttered%2520Environments%2520Using%250A%2520%2520Sums-of-Squares%2520Programming%26entry.906535625%3DYulin%2520Li%2520and%2520Chunxin%2520Zheng%2520and%2520Kai%2520Chen%2520and%2520Yusen%2520Xie%2520and%2520Xindong%2520Tang%2520and%2520Michael%2520Yu%2520Wang%2520and%2520Jun%2520Ma%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520trajectory%2520optimization%2520approach%2520for%2520robot%250Anavigation%2520in%2520cluttered%25203D%2520environments.%2520We%2520represent%2520the%2520robot%2527s%2520geometry%2520as%2520a%250Asemialgebraic%2520set%2520defined%2520by%2520polynomial%2520inequalities%2520such%2520that%2520robots%2520with%250Ageneral%2520shapes%2520can%2520be%2520suitably%2520characterized.%2520To%2520address%2520the%2520robot%2520navigation%250Atask%2520in%2520obstacle-dense%2520environments%252C%2520we%2520exploit%2520the%2520free%2520space%2520directly%2520to%250Aconstruct%2520a%2520sequence%2520of%2520free%2520regions%252C%2520and%2520allocate%2520each%2520waypoint%2520on%2520the%250Atrajectory%2520to%2520a%2520specific%2520region.%2520Then%252C%2520we%2520incorporate%2520a%2520uniform%2520scaling%2520factor%250Afor%2520each%2520free%2520region%252C%2520and%2520formulate%2520a%2520Sums-of-Squares%2520%2528SOS%2529%2520optimization%250Aproblem%2520that%2520renders%2520the%2520containment%2520relationship%2520between%2520the%2520robot%2520and%2520the%250Afree%2520space%2520computationally%2520tractable.%2520The%2520SOS%2520optimization%2520problem%2520is%2520further%250Areformulated%2520to%2520a%2520semidefinite%2520program%2520%2528SDP%2529%252C%2520and%2520the%2520collision-free%250Aconstraints%2520are%2520shown%2520to%2520be%2520equivalent%2520to%2520limiting%2520the%2520scaling%2520factor%2520along%2520the%250Aentire%2520trajectory.%2520In%2520this%2520context%252C%2520the%2520robot%2520at%2520a%2520specific%2520configuration%2520is%250Atailored%2520to%2520stay%2520within%2520the%2520free%2520region.%2520Next%252C%2520to%2520solve%2520the%2520trajectory%250Aoptimization%2520problem%2520with%2520the%2520proposed%2520safety%2520constraints%2520%2528which%2520are%2520implicitly%250Adependent%2520on%2520the%2520robot%2520configurations%2529%252C%2520we%2520derive%2520the%2520analytical%2520solution%2520to%250Athe%2520gradient%2520of%2520the%2520minimum%2520scaling%2520factor%2520with%2520respect%2520to%2520the%2520robot%250Aconfiguration.%2520As%2520a%2520result%252C%2520this%2520seamlessly%2520facilitates%2520the%2520use%2520of%250Agradient-based%2520methods%2520in%2520efficient%2520solving%2520of%2520the%2520trajectory%2520optimization%250Aproblem.%2520Through%2520a%2520series%2520of%2520simulations%2520and%2520real-world%2520experiments%252C%2520the%250Aproposed%2520trajectory%2520optimization%2520approach%2520is%2520validated%2520in%2520various%2520challenging%250Ascenarios%252C%2520and%2520the%2520results%2520demonstrate%2520its%2520effectiveness%2520in%2520generating%250Acollision-free%2520trajectories%2520in%2520dense%2520and%2520intricate%2520environments%2520populated%2520with%250Aobstacles.%2520Our%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/lyl00/minimum_scaling_free_region%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.05242v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collision-Free%20Trajectory%20Optimization%20in%20Cluttered%20Environments%20Using%0A%20%20Sums-of-Squares%20Programming&entry.906535625=Yulin%20Li%20and%20Chunxin%20Zheng%20and%20Kai%20Chen%20and%20Yusen%20Xie%20and%20Xindong%20Tang%20and%20Michael%20Yu%20Wang%20and%20Jun%20Ma&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20a%20trajectory%20optimization%20approach%20for%20robot%0Anavigation%20in%20cluttered%203D%20environments.%20We%20represent%20the%20robot%27s%20geometry%20as%20a%0Asemialgebraic%20set%20defined%20by%20polynomial%20inequalities%20such%20that%20robots%20with%0Ageneral%20shapes%20can%20be%20suitably%20characterized.%20To%20address%20the%20robot%20navigation%0Atask%20in%20obstacle-dense%20environments%2C%20we%20exploit%20the%20free%20space%20directly%20to%0Aconstruct%20a%20sequence%20of%20free%20regions%2C%20and%20allocate%20each%20waypoint%20on%20the%0Atrajectory%20to%20a%20specific%20region.%20Then%2C%20we%20incorporate%20a%20uniform%20scaling%20factor%0Afor%20each%20free%20region%2C%20and%20formulate%20a%20Sums-of-Squares%20%28SOS%29%20optimization%0Aproblem%20that%20renders%20the%20containment%20relationship%20between%20the%20robot%20and%20the%0Afree%20space%20computationally%20tractable.%20The%20SOS%20optimization%20problem%20is%20further%0Areformulated%20to%20a%20semidefinite%20program%20%28SDP%29%2C%20and%20the%20collision-free%0Aconstraints%20are%20shown%20to%20be%20equivalent%20to%20limiting%20the%20scaling%20factor%20along%20the%0Aentire%20trajectory.%20In%20this%20context%2C%20the%20robot%20at%20a%20specific%20configuration%20is%0Atailored%20to%20stay%20within%20the%20free%20region.%20Next%2C%20to%20solve%20the%20trajectory%0Aoptimization%20problem%20with%20the%20proposed%20safety%20constraints%20%28which%20are%20implicitly%0Adependent%20on%20the%20robot%20configurations%29%2C%20we%20derive%20the%20analytical%20solution%20to%0Athe%20gradient%20of%20the%20minimum%20scaling%20factor%20with%20respect%20to%20the%20robot%0Aconfiguration.%20As%20a%20result%2C%20this%20seamlessly%20facilitates%20the%20use%20of%0Agradient-based%20methods%20in%20efficient%20solving%20of%20the%20trajectory%20optimization%0Aproblem.%20Through%20a%20series%20of%20simulations%20and%20real-world%20experiments%2C%20the%0Aproposed%20trajectory%20optimization%20approach%20is%20validated%20in%20various%20challenging%0Ascenarios%2C%20and%20the%20results%20demonstrate%20its%20effectiveness%20in%20generating%0Acollision-free%20trajectories%20in%20dense%20and%20intricate%20environments%20populated%20with%0Aobstacles.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/lyl00/minimum_scaling_free_region%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05242v2&entry.124074799=Read"},
{"title": "Whole-body Humanoid Robot Locomotion with Human Reference", "author": "Qiang Zhang and Peter Cui and David Yan and Jingkai Sun and Yiqun Duan and Gang Han and Wen Zhao and Weining Zhang and Yijie Guo and Arthur Zhang and Renjing Xu", "abstract": "  Recently, humanoid robots have made significant advances in their ability to\nperform challenging tasks due to the deployment of Reinforcement Learning (RL),\nhowever, the inherent complexity of humanoid robots, including the difficulty\nof designing complicated reward functions and training entire sophisticated\nsystems, still poses a notable challenge. To conquer these challenges, after\nmany iterations and in-depth investigations, we have meticulously developed a\nfull-size humanoid robot, \"Adam\", whose innovative structural design greatly\nimproves the efficiency and effectiveness of the imitation learning process. In\naddition, we have developed a novel imitation learning framework based on an\nadversarial motion prior, which applies not only to Adam but also to humanoid\nrobots in general. Using the framework, Adam can exhibit unprecedented\nhuman-like characteristics in locomotion tasks. Our experimental results\ndemonstrate that the proposed framework enables Adam to achieve\nhuman-comparable performance in complex locomotion tasks, marking the first\ntime that human locomotion data has been used for imitation learning in a\nfull-size humanoid robot.\n", "link": "http://arxiv.org/abs/2402.18294v4", "date": "2024-08-26", "relevancy": 1.6061, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5615}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5375}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Whole-body%20Humanoid%20Robot%20Locomotion%20with%20Human%20Reference&body=Title%3A%20Whole-body%20Humanoid%20Robot%20Locomotion%20with%20Human%20Reference%0AAuthor%3A%20Qiang%20Zhang%20and%20Peter%20Cui%20and%20David%20Yan%20and%20Jingkai%20Sun%20and%20Yiqun%20Duan%20and%20Gang%20Han%20and%20Wen%20Zhao%20and%20Weining%20Zhang%20and%20Yijie%20Guo%20and%20Arthur%20Zhang%20and%20Renjing%20Xu%0AAbstract%3A%20%20%20Recently%2C%20humanoid%20robots%20have%20made%20significant%20advances%20in%20their%20ability%20to%0Aperform%20challenging%20tasks%20due%20to%20the%20deployment%20of%20Reinforcement%20Learning%20%28RL%29%2C%0Ahowever%2C%20the%20inherent%20complexity%20of%20humanoid%20robots%2C%20including%20the%20difficulty%0Aof%20designing%20complicated%20reward%20functions%20and%20training%20entire%20sophisticated%0Asystems%2C%20still%20poses%20a%20notable%20challenge.%20To%20conquer%20these%20challenges%2C%20after%0Amany%20iterations%20and%20in-depth%20investigations%2C%20we%20have%20meticulously%20developed%20a%0Afull-size%20humanoid%20robot%2C%20%22Adam%22%2C%20whose%20innovative%20structural%20design%20greatly%0Aimproves%20the%20efficiency%20and%20effectiveness%20of%20the%20imitation%20learning%20process.%20In%0Aaddition%2C%20we%20have%20developed%20a%20novel%20imitation%20learning%20framework%20based%20on%20an%0Aadversarial%20motion%20prior%2C%20which%20applies%20not%20only%20to%20Adam%20but%20also%20to%20humanoid%0Arobots%20in%20general.%20Using%20the%20framework%2C%20Adam%20can%20exhibit%20unprecedented%0Ahuman-like%20characteristics%20in%20locomotion%20tasks.%20Our%20experimental%20results%0Ademonstrate%20that%20the%20proposed%20framework%20enables%20Adam%20to%20achieve%0Ahuman-comparable%20performance%20in%20complex%20locomotion%20tasks%2C%20marking%20the%20first%0Atime%20that%20human%20locomotion%20data%20has%20been%20used%20for%20imitation%20learning%20in%20a%0Afull-size%20humanoid%20robot.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.18294v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhole-body%2520Humanoid%2520Robot%2520Locomotion%2520with%2520Human%2520Reference%26entry.906535625%3DQiang%2520Zhang%2520and%2520Peter%2520Cui%2520and%2520David%2520Yan%2520and%2520Jingkai%2520Sun%2520and%2520Yiqun%2520Duan%2520and%2520Gang%2520Han%2520and%2520Wen%2520Zhao%2520and%2520Weining%2520Zhang%2520and%2520Yijie%2520Guo%2520and%2520Arthur%2520Zhang%2520and%2520Renjing%2520Xu%26entry.1292438233%3D%2520%2520Recently%252C%2520humanoid%2520robots%2520have%2520made%2520significant%2520advances%2520in%2520their%2520ability%2520to%250Aperform%2520challenging%2520tasks%2520due%2520to%2520the%2520deployment%2520of%2520Reinforcement%2520Learning%2520%2528RL%2529%252C%250Ahowever%252C%2520the%2520inherent%2520complexity%2520of%2520humanoid%2520robots%252C%2520including%2520the%2520difficulty%250Aof%2520designing%2520complicated%2520reward%2520functions%2520and%2520training%2520entire%2520sophisticated%250Asystems%252C%2520still%2520poses%2520a%2520notable%2520challenge.%2520To%2520conquer%2520these%2520challenges%252C%2520after%250Amany%2520iterations%2520and%2520in-depth%2520investigations%252C%2520we%2520have%2520meticulously%2520developed%2520a%250Afull-size%2520humanoid%2520robot%252C%2520%2522Adam%2522%252C%2520whose%2520innovative%2520structural%2520design%2520greatly%250Aimproves%2520the%2520efficiency%2520and%2520effectiveness%2520of%2520the%2520imitation%2520learning%2520process.%2520In%250Aaddition%252C%2520we%2520have%2520developed%2520a%2520novel%2520imitation%2520learning%2520framework%2520based%2520on%2520an%250Aadversarial%2520motion%2520prior%252C%2520which%2520applies%2520not%2520only%2520to%2520Adam%2520but%2520also%2520to%2520humanoid%250Arobots%2520in%2520general.%2520Using%2520the%2520framework%252C%2520Adam%2520can%2520exhibit%2520unprecedented%250Ahuman-like%2520characteristics%2520in%2520locomotion%2520tasks.%2520Our%2520experimental%2520results%250Ademonstrate%2520that%2520the%2520proposed%2520framework%2520enables%2520Adam%2520to%2520achieve%250Ahuman-comparable%2520performance%2520in%2520complex%2520locomotion%2520tasks%252C%2520marking%2520the%2520first%250Atime%2520that%2520human%2520locomotion%2520data%2520has%2520been%2520used%2520for%2520imitation%2520learning%2520in%2520a%250Afull-size%2520humanoid%2520robot.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.18294v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Whole-body%20Humanoid%20Robot%20Locomotion%20with%20Human%20Reference&entry.906535625=Qiang%20Zhang%20and%20Peter%20Cui%20and%20David%20Yan%20and%20Jingkai%20Sun%20and%20Yiqun%20Duan%20and%20Gang%20Han%20and%20Wen%20Zhao%20and%20Weining%20Zhang%20and%20Yijie%20Guo%20and%20Arthur%20Zhang%20and%20Renjing%20Xu&entry.1292438233=%20%20Recently%2C%20humanoid%20robots%20have%20made%20significant%20advances%20in%20their%20ability%20to%0Aperform%20challenging%20tasks%20due%20to%20the%20deployment%20of%20Reinforcement%20Learning%20%28RL%29%2C%0Ahowever%2C%20the%20inherent%20complexity%20of%20humanoid%20robots%2C%20including%20the%20difficulty%0Aof%20designing%20complicated%20reward%20functions%20and%20training%20entire%20sophisticated%0Asystems%2C%20still%20poses%20a%20notable%20challenge.%20To%20conquer%20these%20challenges%2C%20after%0Amany%20iterations%20and%20in-depth%20investigations%2C%20we%20have%20meticulously%20developed%20a%0Afull-size%20humanoid%20robot%2C%20%22Adam%22%2C%20whose%20innovative%20structural%20design%20greatly%0Aimproves%20the%20efficiency%20and%20effectiveness%20of%20the%20imitation%20learning%20process.%20In%0Aaddition%2C%20we%20have%20developed%20a%20novel%20imitation%20learning%20framework%20based%20on%20an%0Aadversarial%20motion%20prior%2C%20which%20applies%20not%20only%20to%20Adam%20but%20also%20to%20humanoid%0Arobots%20in%20general.%20Using%20the%20framework%2C%20Adam%20can%20exhibit%20unprecedented%0Ahuman-like%20characteristics%20in%20locomotion%20tasks.%20Our%20experimental%20results%0Ademonstrate%20that%20the%20proposed%20framework%20enables%20Adam%20to%20achieve%0Ahuman-comparable%20performance%20in%20complex%20locomotion%20tasks%2C%20marking%20the%20first%0Atime%20that%20human%20locomotion%20data%20has%20been%20used%20for%20imitation%20learning%20in%20a%0Afull-size%20humanoid%20robot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.18294v4&entry.124074799=Read"},
{"title": "A Survey on Small-Scale Testbeds for Connected and Automated Vehicles\n  and Robot Swarms", "author": "Armin Mokhtarian and Jianye Xu and Patrick Scheffe and Maximilian Kloock and Simon Sch\u00e4fer and Heeseung Bang and Viet-Anh Le and Sangeet Ulhas and Johannes Betz and Sean Wilson and Spring Berman and Liam Paull and Amanda Prorok and Bassam Alrifaee", "abstract": "  Connected and automated vehicles and robot swarms hold transformative\npotential for enhancing safety, efficiency, and sustainability in the\ntransportation and manufacturing sectors. Extensive testing and validation of\nthese technologies is crucial for their deployment in the real world. While\nsimulations are essential for initial testing, they often have limitations in\ncapturing the complex dynamics of real-world interactions. This limitation\nunderscores the importance of small-scale testbeds. These testbeds provide a\nrealistic, cost-effective, and controlled environment for testing and\nvalidating algorithms, acting as an essential intermediary between simulation\nand full-scale experiments. This work serves to facilitate researchers' efforts\nin identifying existing small-scale testbeds suitable for their experiments and\nprovide insights for those who want to build their own. In addition, it\ndelivers a comprehensive survey of the current landscape of these testbeds. We\nderive 62 characteristics of testbeds based on the well-known sense-plan-act\nparadigm and offer an online table comparing 22 small-scale testbeds based on\nthese characteristics. The online table is hosted on our designated public\nwebpage www.cpm-remote.de/testbeds, and we invite testbed creators and\ndevelopers to contribute to it. We closely examine nine testbeds in this paper,\ndemonstrating how the derived characteristics can be used to present testbeds.\nFurthermore, we discuss three ongoing challenges concerning small-scale\ntestbeds that we identified, i.e., small-scale to full-scale transition,\nsustainability, and power and resource management.\n", "link": "http://arxiv.org/abs/2408.14199v1", "date": "2024-08-26", "relevancy": 1.4159, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4779}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4713}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Small-Scale%20Testbeds%20for%20Connected%20and%20Automated%20Vehicles%0A%20%20and%20Robot%20Swarms&body=Title%3A%20A%20Survey%20on%20Small-Scale%20Testbeds%20for%20Connected%20and%20Automated%20Vehicles%0A%20%20and%20Robot%20Swarms%0AAuthor%3A%20Armin%20Mokhtarian%20and%20Jianye%20Xu%20and%20Patrick%20Scheffe%20and%20Maximilian%20Kloock%20and%20Simon%20Sch%C3%A4fer%20and%20Heeseung%20Bang%20and%20Viet-Anh%20Le%20and%20Sangeet%20Ulhas%20and%20Johannes%20Betz%20and%20Sean%20Wilson%20and%20Spring%20Berman%20and%20Liam%20Paull%20and%20Amanda%20Prorok%20and%20Bassam%20Alrifaee%0AAbstract%3A%20%20%20Connected%20and%20automated%20vehicles%20and%20robot%20swarms%20hold%20transformative%0Apotential%20for%20enhancing%20safety%2C%20efficiency%2C%20and%20sustainability%20in%20the%0Atransportation%20and%20manufacturing%20sectors.%20Extensive%20testing%20and%20validation%20of%0Athese%20technologies%20is%20crucial%20for%20their%20deployment%20in%20the%20real%20world.%20While%0Asimulations%20are%20essential%20for%20initial%20testing%2C%20they%20often%20have%20limitations%20in%0Acapturing%20the%20complex%20dynamics%20of%20real-world%20interactions.%20This%20limitation%0Aunderscores%20the%20importance%20of%20small-scale%20testbeds.%20These%20testbeds%20provide%20a%0Arealistic%2C%20cost-effective%2C%20and%20controlled%20environment%20for%20testing%20and%0Avalidating%20algorithms%2C%20acting%20as%20an%20essential%20intermediary%20between%20simulation%0Aand%20full-scale%20experiments.%20This%20work%20serves%20to%20facilitate%20researchers%27%20efforts%0Ain%20identifying%20existing%20small-scale%20testbeds%20suitable%20for%20their%20experiments%20and%0Aprovide%20insights%20for%20those%20who%20want%20to%20build%20their%20own.%20In%20addition%2C%20it%0Adelivers%20a%20comprehensive%20survey%20of%20the%20current%20landscape%20of%20these%20testbeds.%20We%0Aderive%2062%20characteristics%20of%20testbeds%20based%20on%20the%20well-known%20sense-plan-act%0Aparadigm%20and%20offer%20an%20online%20table%20comparing%2022%20small-scale%20testbeds%20based%20on%0Athese%20characteristics.%20The%20online%20table%20is%20hosted%20on%20our%20designated%20public%0Awebpage%20www.cpm-remote.de/testbeds%2C%20and%20we%20invite%20testbed%20creators%20and%0Adevelopers%20to%20contribute%20to%20it.%20We%20closely%20examine%20nine%20testbeds%20in%20this%20paper%2C%0Ademonstrating%20how%20the%20derived%20characteristics%20can%20be%20used%20to%20present%20testbeds.%0AFurthermore%2C%20we%20discuss%20three%20ongoing%20challenges%20concerning%20small-scale%0Atestbeds%20that%20we%20identified%2C%20i.e.%2C%20small-scale%20to%20full-scale%20transition%2C%0Asustainability%2C%20and%20power%20and%20resource%20management.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14199v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Small-Scale%2520Testbeds%2520for%2520Connected%2520and%2520Automated%2520Vehicles%250A%2520%2520and%2520Robot%2520Swarms%26entry.906535625%3DArmin%2520Mokhtarian%2520and%2520Jianye%2520Xu%2520and%2520Patrick%2520Scheffe%2520and%2520Maximilian%2520Kloock%2520and%2520Simon%2520Sch%25C3%25A4fer%2520and%2520Heeseung%2520Bang%2520and%2520Viet-Anh%2520Le%2520and%2520Sangeet%2520Ulhas%2520and%2520Johannes%2520Betz%2520and%2520Sean%2520Wilson%2520and%2520Spring%2520Berman%2520and%2520Liam%2520Paull%2520and%2520Amanda%2520Prorok%2520and%2520Bassam%2520Alrifaee%26entry.1292438233%3D%2520%2520Connected%2520and%2520automated%2520vehicles%2520and%2520robot%2520swarms%2520hold%2520transformative%250Apotential%2520for%2520enhancing%2520safety%252C%2520efficiency%252C%2520and%2520sustainability%2520in%2520the%250Atransportation%2520and%2520manufacturing%2520sectors.%2520Extensive%2520testing%2520and%2520validation%2520of%250Athese%2520technologies%2520is%2520crucial%2520for%2520their%2520deployment%2520in%2520the%2520real%2520world.%2520While%250Asimulations%2520are%2520essential%2520for%2520initial%2520testing%252C%2520they%2520often%2520have%2520limitations%2520in%250Acapturing%2520the%2520complex%2520dynamics%2520of%2520real-world%2520interactions.%2520This%2520limitation%250Aunderscores%2520the%2520importance%2520of%2520small-scale%2520testbeds.%2520These%2520testbeds%2520provide%2520a%250Arealistic%252C%2520cost-effective%252C%2520and%2520controlled%2520environment%2520for%2520testing%2520and%250Avalidating%2520algorithms%252C%2520acting%2520as%2520an%2520essential%2520intermediary%2520between%2520simulation%250Aand%2520full-scale%2520experiments.%2520This%2520work%2520serves%2520to%2520facilitate%2520researchers%2527%2520efforts%250Ain%2520identifying%2520existing%2520small-scale%2520testbeds%2520suitable%2520for%2520their%2520experiments%2520and%250Aprovide%2520insights%2520for%2520those%2520who%2520want%2520to%2520build%2520their%2520own.%2520In%2520addition%252C%2520it%250Adelivers%2520a%2520comprehensive%2520survey%2520of%2520the%2520current%2520landscape%2520of%2520these%2520testbeds.%2520We%250Aderive%252062%2520characteristics%2520of%2520testbeds%2520based%2520on%2520the%2520well-known%2520sense-plan-act%250Aparadigm%2520and%2520offer%2520an%2520online%2520table%2520comparing%252022%2520small-scale%2520testbeds%2520based%2520on%250Athese%2520characteristics.%2520The%2520online%2520table%2520is%2520hosted%2520on%2520our%2520designated%2520public%250Awebpage%2520www.cpm-remote.de/testbeds%252C%2520and%2520we%2520invite%2520testbed%2520creators%2520and%250Adevelopers%2520to%2520contribute%2520to%2520it.%2520We%2520closely%2520examine%2520nine%2520testbeds%2520in%2520this%2520paper%252C%250Ademonstrating%2520how%2520the%2520derived%2520characteristics%2520can%2520be%2520used%2520to%2520present%2520testbeds.%250AFurthermore%252C%2520we%2520discuss%2520three%2520ongoing%2520challenges%2520concerning%2520small-scale%250Atestbeds%2520that%2520we%2520identified%252C%2520i.e.%252C%2520small-scale%2520to%2520full-scale%2520transition%252C%250Asustainability%252C%2520and%2520power%2520and%2520resource%2520management.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14199v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Small-Scale%20Testbeds%20for%20Connected%20and%20Automated%20Vehicles%0A%20%20and%20Robot%20Swarms&entry.906535625=Armin%20Mokhtarian%20and%20Jianye%20Xu%20and%20Patrick%20Scheffe%20and%20Maximilian%20Kloock%20and%20Simon%20Sch%C3%A4fer%20and%20Heeseung%20Bang%20and%20Viet-Anh%20Le%20and%20Sangeet%20Ulhas%20and%20Johannes%20Betz%20and%20Sean%20Wilson%20and%20Spring%20Berman%20and%20Liam%20Paull%20and%20Amanda%20Prorok%20and%20Bassam%20Alrifaee&entry.1292438233=%20%20Connected%20and%20automated%20vehicles%20and%20robot%20swarms%20hold%20transformative%0Apotential%20for%20enhancing%20safety%2C%20efficiency%2C%20and%20sustainability%20in%20the%0Atransportation%20and%20manufacturing%20sectors.%20Extensive%20testing%20and%20validation%20of%0Athese%20technologies%20is%20crucial%20for%20their%20deployment%20in%20the%20real%20world.%20While%0Asimulations%20are%20essential%20for%20initial%20testing%2C%20they%20often%20have%20limitations%20in%0Acapturing%20the%20complex%20dynamics%20of%20real-world%20interactions.%20This%20limitation%0Aunderscores%20the%20importance%20of%20small-scale%20testbeds.%20These%20testbeds%20provide%20a%0Arealistic%2C%20cost-effective%2C%20and%20controlled%20environment%20for%20testing%20and%0Avalidating%20algorithms%2C%20acting%20as%20an%20essential%20intermediary%20between%20simulation%0Aand%20full-scale%20experiments.%20This%20work%20serves%20to%20facilitate%20researchers%27%20efforts%0Ain%20identifying%20existing%20small-scale%20testbeds%20suitable%20for%20their%20experiments%20and%0Aprovide%20insights%20for%20those%20who%20want%20to%20build%20their%20own.%20In%20addition%2C%20it%0Adelivers%20a%20comprehensive%20survey%20of%20the%20current%20landscape%20of%20these%20testbeds.%20We%0Aderive%2062%20characteristics%20of%20testbeds%20based%20on%20the%20well-known%20sense-plan-act%0Aparadigm%20and%20offer%20an%20online%20table%20comparing%2022%20small-scale%20testbeds%20based%20on%0Athese%20characteristics.%20The%20online%20table%20is%20hosted%20on%20our%20designated%20public%0Awebpage%20www.cpm-remote.de/testbeds%2C%20and%20we%20invite%20testbed%20creators%20and%0Adevelopers%20to%20contribute%20to%20it.%20We%20closely%20examine%20nine%20testbeds%20in%20this%20paper%2C%0Ademonstrating%20how%20the%20derived%20characteristics%20can%20be%20used%20to%20present%20testbeds.%0AFurthermore%2C%20we%20discuss%20three%20ongoing%20challenges%20concerning%20small-scale%0Atestbeds%20that%20we%20identified%2C%20i.e.%2C%20small-scale%20to%20full-scale%20transition%2C%0Asustainability%2C%20and%20power%20and%20resource%20management.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14199v1&entry.124074799=Read"},
{"title": "Grounded Multi-Hop VideoQA in Long-Form Egocentric Videos", "author": "Qirui Chen and Shangzhe Di and Weidi Xie", "abstract": "  This paper considers the problem of Multi-Hop Video Question Answering\n(MH-VidQA) in long-form egocentric videos. This task not only requires to\nanswer visual questions, but also to localize multiple relevant time intervals\nwithin the video as visual evidences. We develop an automated pipeline to\ncreate multi-hop question-answering pairs with associated temporal evidence,\nenabling to construct a large-scale dataset for instruction-tuning. To monitor\nthe progress of this new task, we further curate a high-quality benchmark,\nMultiHop-EgoQA, with careful manual verification and refinement. Experimental\nresults reveal that existing multi-modal systems exhibit inadequate multi-hop\ngrounding and reasoning abilities, resulting in unsatisfactory performance. We\nthen propose a novel architecture, termed as Grounding Scattered Evidence with\nLarge Language Model (GeLM), that enhances multi-modal large language models\n(MLLMs) by incorporating a grounding module to retrieve temporal evidence from\nvideos using flexible grounding tokens. Trained on our visual instruction data,\nGeLM demonstrates improved multi-hop grounding and reasoning capabilities,\nsetting a new baseline for this challenging task. Furthermore, when trained on\nthird-person view videos, the same architecture also achieves state-of-the-art\nperformance on the single-hop VidQA benchmark, ActivityNet-RTL, demonstrating\nits effectiveness.\n", "link": "http://arxiv.org/abs/2408.14469v1", "date": "2024-08-26", "relevancy": 1.5664, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5377}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5278}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Grounded%20Multi-Hop%20VideoQA%20in%20Long-Form%20Egocentric%20Videos&body=Title%3A%20Grounded%20Multi-Hop%20VideoQA%20in%20Long-Form%20Egocentric%20Videos%0AAuthor%3A%20Qirui%20Chen%20and%20Shangzhe%20Di%20and%20Weidi%20Xie%0AAbstract%3A%20%20%20This%20paper%20considers%20the%20problem%20of%20Multi-Hop%20Video%20Question%20Answering%0A%28MH-VidQA%29%20in%20long-form%20egocentric%20videos.%20This%20task%20not%20only%20requires%20to%0Aanswer%20visual%20questions%2C%20but%20also%20to%20localize%20multiple%20relevant%20time%20intervals%0Awithin%20the%20video%20as%20visual%20evidences.%20We%20develop%20an%20automated%20pipeline%20to%0Acreate%20multi-hop%20question-answering%20pairs%20with%20associated%20temporal%20evidence%2C%0Aenabling%20to%20construct%20a%20large-scale%20dataset%20for%20instruction-tuning.%20To%20monitor%0Athe%20progress%20of%20this%20new%20task%2C%20we%20further%20curate%20a%20high-quality%20benchmark%2C%0AMultiHop-EgoQA%2C%20with%20careful%20manual%20verification%20and%20refinement.%20Experimental%0Aresults%20reveal%20that%20existing%20multi-modal%20systems%20exhibit%20inadequate%20multi-hop%0Agrounding%20and%20reasoning%20abilities%2C%20resulting%20in%20unsatisfactory%20performance.%20We%0Athen%20propose%20a%20novel%20architecture%2C%20termed%20as%20Grounding%20Scattered%20Evidence%20with%0ALarge%20Language%20Model%20%28GeLM%29%2C%20that%20enhances%20multi-modal%20large%20language%20models%0A%28MLLMs%29%20by%20incorporating%20a%20grounding%20module%20to%20retrieve%20temporal%20evidence%20from%0Avideos%20using%20flexible%20grounding%20tokens.%20Trained%20on%20our%20visual%20instruction%20data%2C%0AGeLM%20demonstrates%20improved%20multi-hop%20grounding%20and%20reasoning%20capabilities%2C%0Asetting%20a%20new%20baseline%20for%20this%20challenging%20task.%20Furthermore%2C%20when%20trained%20on%0Athird-person%20view%20videos%2C%20the%20same%20architecture%20also%20achieves%20state-of-the-art%0Aperformance%20on%20the%20single-hop%20VidQA%20benchmark%2C%20ActivityNet-RTL%2C%20demonstrating%0Aits%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.14469v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGrounded%2520Multi-Hop%2520VideoQA%2520in%2520Long-Form%2520Egocentric%2520Videos%26entry.906535625%3DQirui%2520Chen%2520and%2520Shangzhe%2520Di%2520and%2520Weidi%2520Xie%26entry.1292438233%3D%2520%2520This%2520paper%2520considers%2520the%2520problem%2520of%2520Multi-Hop%2520Video%2520Question%2520Answering%250A%2528MH-VidQA%2529%2520in%2520long-form%2520egocentric%2520videos.%2520This%2520task%2520not%2520only%2520requires%2520to%250Aanswer%2520visual%2520questions%252C%2520but%2520also%2520to%2520localize%2520multiple%2520relevant%2520time%2520intervals%250Awithin%2520the%2520video%2520as%2520visual%2520evidences.%2520We%2520develop%2520an%2520automated%2520pipeline%2520to%250Acreate%2520multi-hop%2520question-answering%2520pairs%2520with%2520associated%2520temporal%2520evidence%252C%250Aenabling%2520to%2520construct%2520a%2520large-scale%2520dataset%2520for%2520instruction-tuning.%2520To%2520monitor%250Athe%2520progress%2520of%2520this%2520new%2520task%252C%2520we%2520further%2520curate%2520a%2520high-quality%2520benchmark%252C%250AMultiHop-EgoQA%252C%2520with%2520careful%2520manual%2520verification%2520and%2520refinement.%2520Experimental%250Aresults%2520reveal%2520that%2520existing%2520multi-modal%2520systems%2520exhibit%2520inadequate%2520multi-hop%250Agrounding%2520and%2520reasoning%2520abilities%252C%2520resulting%2520in%2520unsatisfactory%2520performance.%2520We%250Athen%2520propose%2520a%2520novel%2520architecture%252C%2520termed%2520as%2520Grounding%2520Scattered%2520Evidence%2520with%250ALarge%2520Language%2520Model%2520%2528GeLM%2529%252C%2520that%2520enhances%2520multi-modal%2520large%2520language%2520models%250A%2528MLLMs%2529%2520by%2520incorporating%2520a%2520grounding%2520module%2520to%2520retrieve%2520temporal%2520evidence%2520from%250Avideos%2520using%2520flexible%2520grounding%2520tokens.%2520Trained%2520on%2520our%2520visual%2520instruction%2520data%252C%250AGeLM%2520demonstrates%2520improved%2520multi-hop%2520grounding%2520and%2520reasoning%2520capabilities%252C%250Asetting%2520a%2520new%2520baseline%2520for%2520this%2520challenging%2520task.%2520Furthermore%252C%2520when%2520trained%2520on%250Athird-person%2520view%2520videos%252C%2520the%2520same%2520architecture%2520also%2520achieves%2520state-of-the-art%250Aperformance%2520on%2520the%2520single-hop%2520VidQA%2520benchmark%252C%2520ActivityNet-RTL%252C%2520demonstrating%250Aits%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.14469v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Grounded%20Multi-Hop%20VideoQA%20in%20Long-Form%20Egocentric%20Videos&entry.906535625=Qirui%20Chen%20and%20Shangzhe%20Di%20and%20Weidi%20Xie&entry.1292438233=%20%20This%20paper%20considers%20the%20problem%20of%20Multi-Hop%20Video%20Question%20Answering%0A%28MH-VidQA%29%20in%20long-form%20egocentric%20videos.%20This%20task%20not%20only%20requires%20to%0Aanswer%20visual%20questions%2C%20but%20also%20to%20localize%20multiple%20relevant%20time%20intervals%0Awithin%20the%20video%20as%20visual%20evidences.%20We%20develop%20an%20automated%20pipeline%20to%0Acreate%20multi-hop%20question-answering%20pairs%20with%20associated%20temporal%20evidence%2C%0Aenabling%20to%20construct%20a%20large-scale%20dataset%20for%20instruction-tuning.%20To%20monitor%0Athe%20progress%20of%20this%20new%20task%2C%20we%20further%20curate%20a%20high-quality%20benchmark%2C%0AMultiHop-EgoQA%2C%20with%20careful%20manual%20verification%20and%20refinement.%20Experimental%0Aresults%20reveal%20that%20existing%20multi-modal%20systems%20exhibit%20inadequate%20multi-hop%0Agrounding%20and%20reasoning%20abilities%2C%20resulting%20in%20unsatisfactory%20performance.%20We%0Athen%20propose%20a%20novel%20architecture%2C%20termed%20as%20Grounding%20Scattered%20Evidence%20with%0ALarge%20Language%20Model%20%28GeLM%29%2C%20that%20enhances%20multi-modal%20large%20language%20models%0A%28MLLMs%29%20by%20incorporating%20a%20grounding%20module%20to%20retrieve%20temporal%20evidence%20from%0Avideos%20using%20flexible%20grounding%20tokens.%20Trained%20on%20our%20visual%20instruction%20data%2C%0AGeLM%20demonstrates%20improved%20multi-hop%20grounding%20and%20reasoning%20capabilities%2C%0Asetting%20a%20new%20baseline%20for%20this%20challenging%20task.%20Furthermore%2C%20when%20trained%20on%0Athird-person%20view%20videos%2C%20the%20same%20architecture%20also%20achieves%20state-of-the-art%0Aperformance%20on%20the%20single-hop%20VidQA%20benchmark%2C%20ActivityNet-RTL%2C%20demonstrating%0Aits%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.14469v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


