<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20260128.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction", "author": "Hung Nguyen and Runfa Li and An Le and Truong Nguyen", "abstract": "3D Gaussian Splatting (3DGS) has become a powerful representation for image-based object reconstruction, yet its performance drops sharply in sparse-view settings. Prior works address this limitation by employing diffusion models to repair corrupted renders, subsequently using them as pseudo ground truths for later optimization. While effective, such approaches incur heavy computation from the diffusion fine-tuning and repair steps. We present WaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object reconstruction. Our key idea is to shift diffusion into the wavelet domain: diffusion is applied only to the low-resolution LL subband, while high-frequency subbands are refined with a lightweight network. We further propose an efficient online random masking strategy to curate training pairs for diffusion fine-tuning, replacing the commonly used, but inefficient, leave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360 and OmniObject3D, show WaveletGaussian achieves competitive rendering quality while substantially reducing training time.", "link": "http://arxiv.org/abs/2509.19073v3", "date": "2026-01-28", "relevancy": 3.328, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6773}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6708}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WaveletGaussian%3A%20Wavelet-domain%20Diffusion%20for%20Sparse-view%203D%20Gaussian%20Object%20Reconstruction&body=Title%3A%20WaveletGaussian%3A%20Wavelet-domain%20Diffusion%20for%20Sparse-view%203D%20Gaussian%20Object%20Reconstruction%0AAuthor%3A%20Hung%20Nguyen%20and%20Runfa%20Li%20and%20An%20Le%20and%20Truong%20Nguyen%0AAbstract%3A%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20become%20a%20powerful%20representation%20for%20image-based%20object%20reconstruction%2C%20yet%20its%20performance%20drops%20sharply%20in%20sparse-view%20settings.%20Prior%20works%20address%20this%20limitation%20by%20employing%20diffusion%20models%20to%20repair%20corrupted%20renders%2C%20subsequently%20using%20them%20as%20pseudo%20ground%20truths%20for%20later%20optimization.%20While%20effective%2C%20such%20approaches%20incur%20heavy%20computation%20from%20the%20diffusion%20fine-tuning%20and%20repair%20steps.%20We%20present%20WaveletGaussian%2C%20a%20framework%20for%20more%20efficient%20sparse-view%203D%20Gaussian%20object%20reconstruction.%20Our%20key%20idea%20is%20to%20shift%20diffusion%20into%20the%20wavelet%20domain%3A%20diffusion%20is%20applied%20only%20to%20the%20low-resolution%20LL%20subband%2C%20while%20high-frequency%20subbands%20are%20refined%20with%20a%20lightweight%20network.%20We%20further%20propose%20an%20efficient%20online%20random%20masking%20strategy%20to%20curate%20training%20pairs%20for%20diffusion%20fine-tuning%2C%20replacing%20the%20commonly%20used%2C%20but%20inefficient%2C%20leave-one-out%20strategy.%20Experiments%20across%20two%20benchmark%20datasets%2C%20Mip-NeRF%20360%20and%20OmniObject3D%2C%20show%20WaveletGaussian%20achieves%20competitive%20rendering%20quality%20while%20substantially%20reducing%20training%20time.%0ALink%3A%20http%3A//arxiv.org/abs/2509.19073v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWaveletGaussian%253A%2520Wavelet-domain%2520Diffusion%2520for%2520Sparse-view%25203D%2520Gaussian%2520Object%2520Reconstruction%26entry.906535625%3DHung%2520Nguyen%2520and%2520Runfa%2520Li%2520and%2520An%2520Le%2520and%2520Truong%2520Nguyen%26entry.1292438233%3D3D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520become%2520a%2520powerful%2520representation%2520for%2520image-based%2520object%2520reconstruction%252C%2520yet%2520its%2520performance%2520drops%2520sharply%2520in%2520sparse-view%2520settings.%2520Prior%2520works%2520address%2520this%2520limitation%2520by%2520employing%2520diffusion%2520models%2520to%2520repair%2520corrupted%2520renders%252C%2520subsequently%2520using%2520them%2520as%2520pseudo%2520ground%2520truths%2520for%2520later%2520optimization.%2520While%2520effective%252C%2520such%2520approaches%2520incur%2520heavy%2520computation%2520from%2520the%2520diffusion%2520fine-tuning%2520and%2520repair%2520steps.%2520We%2520present%2520WaveletGaussian%252C%2520a%2520framework%2520for%2520more%2520efficient%2520sparse-view%25203D%2520Gaussian%2520object%2520reconstruction.%2520Our%2520key%2520idea%2520is%2520to%2520shift%2520diffusion%2520into%2520the%2520wavelet%2520domain%253A%2520diffusion%2520is%2520applied%2520only%2520to%2520the%2520low-resolution%2520LL%2520subband%252C%2520while%2520high-frequency%2520subbands%2520are%2520refined%2520with%2520a%2520lightweight%2520network.%2520We%2520further%2520propose%2520an%2520efficient%2520online%2520random%2520masking%2520strategy%2520to%2520curate%2520training%2520pairs%2520for%2520diffusion%2520fine-tuning%252C%2520replacing%2520the%2520commonly%2520used%252C%2520but%2520inefficient%252C%2520leave-one-out%2520strategy.%2520Experiments%2520across%2520two%2520benchmark%2520datasets%252C%2520Mip-NeRF%2520360%2520and%2520OmniObject3D%252C%2520show%2520WaveletGaussian%2520achieves%2520competitive%2520rendering%2520quality%2520while%2520substantially%2520reducing%2520training%2520time.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.19073v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WaveletGaussian%3A%20Wavelet-domain%20Diffusion%20for%20Sparse-view%203D%20Gaussian%20Object%20Reconstruction&entry.906535625=Hung%20Nguyen%20and%20Runfa%20Li%20and%20An%20Le%20and%20Truong%20Nguyen&entry.1292438233=3D%20Gaussian%20Splatting%20%283DGS%29%20has%20become%20a%20powerful%20representation%20for%20image-based%20object%20reconstruction%2C%20yet%20its%20performance%20drops%20sharply%20in%20sparse-view%20settings.%20Prior%20works%20address%20this%20limitation%20by%20employing%20diffusion%20models%20to%20repair%20corrupted%20renders%2C%20subsequently%20using%20them%20as%20pseudo%20ground%20truths%20for%20later%20optimization.%20While%20effective%2C%20such%20approaches%20incur%20heavy%20computation%20from%20the%20diffusion%20fine-tuning%20and%20repair%20steps.%20We%20present%20WaveletGaussian%2C%20a%20framework%20for%20more%20efficient%20sparse-view%203D%20Gaussian%20object%20reconstruction.%20Our%20key%20idea%20is%20to%20shift%20diffusion%20into%20the%20wavelet%20domain%3A%20diffusion%20is%20applied%20only%20to%20the%20low-resolution%20LL%20subband%2C%20while%20high-frequency%20subbands%20are%20refined%20with%20a%20lightweight%20network.%20We%20further%20propose%20an%20efficient%20online%20random%20masking%20strategy%20to%20curate%20training%20pairs%20for%20diffusion%20fine-tuning%2C%20replacing%20the%20commonly%20used%2C%20but%20inefficient%2C%20leave-one-out%20strategy.%20Experiments%20across%20two%20benchmark%20datasets%2C%20Mip-NeRF%20360%20and%20OmniObject3D%2C%20show%20WaveletGaussian%20achieves%20competitive%20rendering%20quality%20while%20substantially%20reducing%20training%20time.&entry.1838667208=http%3A//arxiv.org/abs/2509.19073v3&entry.124074799=Read"},
{"title": "DeepSeek-OCR 2: Visual Causal Flow", "author": "Haoran Wei and Yaofeng Sun and Yukun Li", "abstract": "We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.", "link": "http://arxiv.org/abs/2601.20552v1", "date": "2026-01-28", "relevancy": 3.0396, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6421}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6421}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5395}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepSeek-OCR%202%3A%20Visual%20Causal%20Flow&body=Title%3A%20DeepSeek-OCR%202%3A%20Visual%20Causal%20Flow%0AAuthor%3A%20Haoran%20Wei%20and%20Yaofeng%20Sun%20and%20Yukun%20Li%0AAbstract%3A%20We%20present%20DeepSeek-OCR%202%20to%20investigate%20the%20feasibility%20of%20a%20novel%20encoder-DeepEncoder%20V2-capable%20of%20dynamically%20reordering%20visual%20tokens%20upon%20image%20semantics.%20Conventional%20vision-language%20models%20%28VLMs%29%20invariably%20process%20visual%20tokens%20in%20a%20rigid%20raster-scan%20order%20%28top-left%20to%20bottom-right%29%20with%20fixed%20positional%20encoding%20when%20fed%20into%20LLMs.%20However%2C%20this%20contradicts%20human%20visual%20perception%2C%20which%20follows%20flexible%20yet%20semantically%20coherent%20scanning%20patterns%20driven%20by%20inherent%20logical%20structures.%20Particularly%20for%20images%20with%20complex%20layouts%2C%20human%20vision%20exhibits%20causally-informed%20sequential%20processing.%20Inspired%20by%20this%20cognitive%20mechanism%2C%20DeepEncoder%20V2%20is%20designed%20to%20endow%20the%20encoder%20with%20causal%20reasoning%20capabilities%2C%20enabling%20it%20to%20intelligently%20reorder%20visual%20tokens%20prior%20to%20LLM-based%20content%20interpretation.%20This%20work%20explores%20a%20novel%20paradigm%3A%20whether%202D%20image%20understanding%20can%20be%20effectively%20achieved%20through%20two-cascaded%201D%20causal%20reasoning%20structures%2C%20thereby%20offering%20a%20new%20architectural%20approach%20with%20the%20potential%20to%20achieve%20genuine%202D%20reasoning.%20Codes%20and%20model%20weights%20are%20publicly%20accessible%20at%20http%3A//github.com/deepseek-ai/DeepSeek-OCR-2.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20552v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepSeek-OCR%25202%253A%2520Visual%2520Causal%2520Flow%26entry.906535625%3DHaoran%2520Wei%2520and%2520Yaofeng%2520Sun%2520and%2520Yukun%2520Li%26entry.1292438233%3DWe%2520present%2520DeepSeek-OCR%25202%2520to%2520investigate%2520the%2520feasibility%2520of%2520a%2520novel%2520encoder-DeepEncoder%2520V2-capable%2520of%2520dynamically%2520reordering%2520visual%2520tokens%2520upon%2520image%2520semantics.%2520Conventional%2520vision-language%2520models%2520%2528VLMs%2529%2520invariably%2520process%2520visual%2520tokens%2520in%2520a%2520rigid%2520raster-scan%2520order%2520%2528top-left%2520to%2520bottom-right%2529%2520with%2520fixed%2520positional%2520encoding%2520when%2520fed%2520into%2520LLMs.%2520However%252C%2520this%2520contradicts%2520human%2520visual%2520perception%252C%2520which%2520follows%2520flexible%2520yet%2520semantically%2520coherent%2520scanning%2520patterns%2520driven%2520by%2520inherent%2520logical%2520structures.%2520Particularly%2520for%2520images%2520with%2520complex%2520layouts%252C%2520human%2520vision%2520exhibits%2520causally-informed%2520sequential%2520processing.%2520Inspired%2520by%2520this%2520cognitive%2520mechanism%252C%2520DeepEncoder%2520V2%2520is%2520designed%2520to%2520endow%2520the%2520encoder%2520with%2520causal%2520reasoning%2520capabilities%252C%2520enabling%2520it%2520to%2520intelligently%2520reorder%2520visual%2520tokens%2520prior%2520to%2520LLM-based%2520content%2520interpretation.%2520This%2520work%2520explores%2520a%2520novel%2520paradigm%253A%2520whether%25202D%2520image%2520understanding%2520can%2520be%2520effectively%2520achieved%2520through%2520two-cascaded%25201D%2520causal%2520reasoning%2520structures%252C%2520thereby%2520offering%2520a%2520new%2520architectural%2520approach%2520with%2520the%2520potential%2520to%2520achieve%2520genuine%25202D%2520reasoning.%2520Codes%2520and%2520model%2520weights%2520are%2520publicly%2520accessible%2520at%2520http%253A//github.com/deepseek-ai/DeepSeek-OCR-2.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20552v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepSeek-OCR%202%3A%20Visual%20Causal%20Flow&entry.906535625=Haoran%20Wei%20and%20Yaofeng%20Sun%20and%20Yukun%20Li&entry.1292438233=We%20present%20DeepSeek-OCR%202%20to%20investigate%20the%20feasibility%20of%20a%20novel%20encoder-DeepEncoder%20V2-capable%20of%20dynamically%20reordering%20visual%20tokens%20upon%20image%20semantics.%20Conventional%20vision-language%20models%20%28VLMs%29%20invariably%20process%20visual%20tokens%20in%20a%20rigid%20raster-scan%20order%20%28top-left%20to%20bottom-right%29%20with%20fixed%20positional%20encoding%20when%20fed%20into%20LLMs.%20However%2C%20this%20contradicts%20human%20visual%20perception%2C%20which%20follows%20flexible%20yet%20semantically%20coherent%20scanning%20patterns%20driven%20by%20inherent%20logical%20structures.%20Particularly%20for%20images%20with%20complex%20layouts%2C%20human%20vision%20exhibits%20causally-informed%20sequential%20processing.%20Inspired%20by%20this%20cognitive%20mechanism%2C%20DeepEncoder%20V2%20is%20designed%20to%20endow%20the%20encoder%20with%20causal%20reasoning%20capabilities%2C%20enabling%20it%20to%20intelligently%20reorder%20visual%20tokens%20prior%20to%20LLM-based%20content%20interpretation.%20This%20work%20explores%20a%20novel%20paradigm%3A%20whether%202D%20image%20understanding%20can%20be%20effectively%20achieved%20through%20two-cascaded%201D%20causal%20reasoning%20structures%2C%20thereby%20offering%20a%20new%20architectural%20approach%20with%20the%20potential%20to%20achieve%20genuine%202D%20reasoning.%20Codes%20and%20model%20weights%20are%20publicly%20accessible%20at%20http%3A//github.com/deepseek-ai/DeepSeek-OCR-2.&entry.1838667208=http%3A//arxiv.org/abs/2601.20552v1&entry.124074799=Read"},
{"title": "Compression Tells Intelligence: Visual Coding, Visual Token Technology, and the Unification", "author": "Xin Jin and Jinming Liu and Yuntao Wei and Junyan Lin and Zhicheng Wang and Jianguo Huang and Xudong Yang and Yanxiao Liu and Wenjun Zeng", "abstract": "\"Compression Tells Intelligence\", is supported by research in artificial intelligence, particularly concerning (multimodal) large language models (LLMs/MLLMs), where compression efficiency often correlates with improved model performance and capabilities. For compression, classical visual coding based on traditional information theory has developed over decades, achieving great success with numerous international industrial standards widely applied in multimedia (e.g., image/video) systems. Except that, the recent emergingvisual token technology of generative multi-modal large models also shares a similar fundamental objective like visual coding: maximizing semantic information fidelity during the representation learning while minimizing computational cost. Therefore, this paper provides a comprehensive overview of two dominant technique families first -- Visual Coding and Vision Token Technology -- then we further unify them from the aspect of optimization, discussing the essence of compression efficiency and model performance trade-off behind. Next, based on the proposed unified formulation bridging visual coding andvisual token technology, we synthesize bidirectional insights of themselves and forecast the next-gen visual codec and token techniques. Last but not least, we experimentally show a large potential of the task-oriented token developments in the more practical tasks like multimodal LLMs (MLLMs), AI-generated content (AIGC), and embodied AI, as well as shedding light on the future possibility of standardizing a general token technology like the traditional codecs (e.g., H.264/265) with high efficiency for a wide range of intelligent tasks in a unified and effective manner.", "link": "http://arxiv.org/abs/2601.20742v1", "date": "2026-01-28", "relevancy": 2.9712, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6132}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6132}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compression%20Tells%20Intelligence%3A%20Visual%20Coding%2C%20Visual%20Token%20Technology%2C%20and%20the%20Unification&body=Title%3A%20Compression%20Tells%20Intelligence%3A%20Visual%20Coding%2C%20Visual%20Token%20Technology%2C%20and%20the%20Unification%0AAuthor%3A%20Xin%20Jin%20and%20Jinming%20Liu%20and%20Yuntao%20Wei%20and%20Junyan%20Lin%20and%20Zhicheng%20Wang%20and%20Jianguo%20Huang%20and%20Xudong%20Yang%20and%20Yanxiao%20Liu%20and%20Wenjun%20Zeng%0AAbstract%3A%20%22Compression%20Tells%20Intelligence%22%2C%20is%20supported%20by%20research%20in%20artificial%20intelligence%2C%20particularly%20concerning%20%28multimodal%29%20large%20language%20models%20%28LLMs/MLLMs%29%2C%20where%20compression%20efficiency%20often%20correlates%20with%20improved%20model%20performance%20and%20capabilities.%20For%20compression%2C%20classical%20visual%20coding%20based%20on%20traditional%20information%20theory%20has%20developed%20over%20decades%2C%20achieving%20great%20success%20with%20numerous%20international%20industrial%20standards%20widely%20applied%20in%20multimedia%20%28e.g.%2C%20image/video%29%20systems.%20Except%20that%2C%20the%20recent%20emergingvisual%20token%20technology%20of%20generative%20multi-modal%20large%20models%20also%20shares%20a%20similar%20fundamental%20objective%20like%20visual%20coding%3A%20maximizing%20semantic%20information%20fidelity%20during%20the%20representation%20learning%20while%20minimizing%20computational%20cost.%20Therefore%2C%20this%20paper%20provides%20a%20comprehensive%20overview%20of%20two%20dominant%20technique%20families%20first%20--%20Visual%20Coding%20and%20Vision%20Token%20Technology%20--%20then%20we%20further%20unify%20them%20from%20the%20aspect%20of%20optimization%2C%20discussing%20the%20essence%20of%20compression%20efficiency%20and%20model%20performance%20trade-off%20behind.%20Next%2C%20based%20on%20the%20proposed%20unified%20formulation%20bridging%20visual%20coding%20andvisual%20token%20technology%2C%20we%20synthesize%20bidirectional%20insights%20of%20themselves%20and%20forecast%20the%20next-gen%20visual%20codec%20and%20token%20techniques.%20Last%20but%20not%20least%2C%20we%20experimentally%20show%20a%20large%20potential%20of%20the%20task-oriented%20token%20developments%20in%20the%20more%20practical%20tasks%20like%20multimodal%20LLMs%20%28MLLMs%29%2C%20AI-generated%20content%20%28AIGC%29%2C%20and%20embodied%20AI%2C%20as%20well%20as%20shedding%20light%20on%20the%20future%20possibility%20of%20standardizing%20a%20general%20token%20technology%20like%20the%20traditional%20codecs%20%28e.g.%2C%20H.264/265%29%20with%20high%20efficiency%20for%20a%20wide%20range%20of%20intelligent%20tasks%20in%20a%20unified%20and%20effective%20manner.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20742v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompression%2520Tells%2520Intelligence%253A%2520Visual%2520Coding%252C%2520Visual%2520Token%2520Technology%252C%2520and%2520the%2520Unification%26entry.906535625%3DXin%2520Jin%2520and%2520Jinming%2520Liu%2520and%2520Yuntao%2520Wei%2520and%2520Junyan%2520Lin%2520and%2520Zhicheng%2520Wang%2520and%2520Jianguo%2520Huang%2520and%2520Xudong%2520Yang%2520and%2520Yanxiao%2520Liu%2520and%2520Wenjun%2520Zeng%26entry.1292438233%3D%2522Compression%2520Tells%2520Intelligence%2522%252C%2520is%2520supported%2520by%2520research%2520in%2520artificial%2520intelligence%252C%2520particularly%2520concerning%2520%2528multimodal%2529%2520large%2520language%2520models%2520%2528LLMs/MLLMs%2529%252C%2520where%2520compression%2520efficiency%2520often%2520correlates%2520with%2520improved%2520model%2520performance%2520and%2520capabilities.%2520For%2520compression%252C%2520classical%2520visual%2520coding%2520based%2520on%2520traditional%2520information%2520theory%2520has%2520developed%2520over%2520decades%252C%2520achieving%2520great%2520success%2520with%2520numerous%2520international%2520industrial%2520standards%2520widely%2520applied%2520in%2520multimedia%2520%2528e.g.%252C%2520image/video%2529%2520systems.%2520Except%2520that%252C%2520the%2520recent%2520emergingvisual%2520token%2520technology%2520of%2520generative%2520multi-modal%2520large%2520models%2520also%2520shares%2520a%2520similar%2520fundamental%2520objective%2520like%2520visual%2520coding%253A%2520maximizing%2520semantic%2520information%2520fidelity%2520during%2520the%2520representation%2520learning%2520while%2520minimizing%2520computational%2520cost.%2520Therefore%252C%2520this%2520paper%2520provides%2520a%2520comprehensive%2520overview%2520of%2520two%2520dominant%2520technique%2520families%2520first%2520--%2520Visual%2520Coding%2520and%2520Vision%2520Token%2520Technology%2520--%2520then%2520we%2520further%2520unify%2520them%2520from%2520the%2520aspect%2520of%2520optimization%252C%2520discussing%2520the%2520essence%2520of%2520compression%2520efficiency%2520and%2520model%2520performance%2520trade-off%2520behind.%2520Next%252C%2520based%2520on%2520the%2520proposed%2520unified%2520formulation%2520bridging%2520visual%2520coding%2520andvisual%2520token%2520technology%252C%2520we%2520synthesize%2520bidirectional%2520insights%2520of%2520themselves%2520and%2520forecast%2520the%2520next-gen%2520visual%2520codec%2520and%2520token%2520techniques.%2520Last%2520but%2520not%2520least%252C%2520we%2520experimentally%2520show%2520a%2520large%2520potential%2520of%2520the%2520task-oriented%2520token%2520developments%2520in%2520the%2520more%2520practical%2520tasks%2520like%2520multimodal%2520LLMs%2520%2528MLLMs%2529%252C%2520AI-generated%2520content%2520%2528AIGC%2529%252C%2520and%2520embodied%2520AI%252C%2520as%2520well%2520as%2520shedding%2520light%2520on%2520the%2520future%2520possibility%2520of%2520standardizing%2520a%2520general%2520token%2520technology%2520like%2520the%2520traditional%2520codecs%2520%2528e.g.%252C%2520H.264/265%2529%2520with%2520high%2520efficiency%2520for%2520a%2520wide%2520range%2520of%2520intelligent%2520tasks%2520in%2520a%2520unified%2520and%2520effective%2520manner.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20742v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compression%20Tells%20Intelligence%3A%20Visual%20Coding%2C%20Visual%20Token%20Technology%2C%20and%20the%20Unification&entry.906535625=Xin%20Jin%20and%20Jinming%20Liu%20and%20Yuntao%20Wei%20and%20Junyan%20Lin%20and%20Zhicheng%20Wang%20and%20Jianguo%20Huang%20and%20Xudong%20Yang%20and%20Yanxiao%20Liu%20and%20Wenjun%20Zeng&entry.1292438233=%22Compression%20Tells%20Intelligence%22%2C%20is%20supported%20by%20research%20in%20artificial%20intelligence%2C%20particularly%20concerning%20%28multimodal%29%20large%20language%20models%20%28LLMs/MLLMs%29%2C%20where%20compression%20efficiency%20often%20correlates%20with%20improved%20model%20performance%20and%20capabilities.%20For%20compression%2C%20classical%20visual%20coding%20based%20on%20traditional%20information%20theory%20has%20developed%20over%20decades%2C%20achieving%20great%20success%20with%20numerous%20international%20industrial%20standards%20widely%20applied%20in%20multimedia%20%28e.g.%2C%20image/video%29%20systems.%20Except%20that%2C%20the%20recent%20emergingvisual%20token%20technology%20of%20generative%20multi-modal%20large%20models%20also%20shares%20a%20similar%20fundamental%20objective%20like%20visual%20coding%3A%20maximizing%20semantic%20information%20fidelity%20during%20the%20representation%20learning%20while%20minimizing%20computational%20cost.%20Therefore%2C%20this%20paper%20provides%20a%20comprehensive%20overview%20of%20two%20dominant%20technique%20families%20first%20--%20Visual%20Coding%20and%20Vision%20Token%20Technology%20--%20then%20we%20further%20unify%20them%20from%20the%20aspect%20of%20optimization%2C%20discussing%20the%20essence%20of%20compression%20efficiency%20and%20model%20performance%20trade-off%20behind.%20Next%2C%20based%20on%20the%20proposed%20unified%20formulation%20bridging%20visual%20coding%20andvisual%20token%20technology%2C%20we%20synthesize%20bidirectional%20insights%20of%20themselves%20and%20forecast%20the%20next-gen%20visual%20codec%20and%20token%20techniques.%20Last%20but%20not%20least%2C%20we%20experimentally%20show%20a%20large%20potential%20of%20the%20task-oriented%20token%20developments%20in%20the%20more%20practical%20tasks%20like%20multimodal%20LLMs%20%28MLLMs%29%2C%20AI-generated%20content%20%28AIGC%29%2C%20and%20embodied%20AI%2C%20as%20well%20as%20shedding%20light%20on%20the%20future%20possibility%20of%20standardizing%20a%20general%20token%20technology%20like%20the%20traditional%20codecs%20%28e.g.%2C%20H.264/265%29%20with%20high%20efficiency%20for%20a%20wide%20range%20of%20intelligent%20tasks%20in%20a%20unified%20and%20effective%20manner.&entry.1838667208=http%3A//arxiv.org/abs/2601.20742v1&entry.124074799=Read"},
{"title": "Dense-SfM: Structure from Motion with Dense Consistent Matching", "author": "JongMin Lee and Sungjoo Yoo", "abstract": "We present Dense-SfM, a novel Structure from Motion (SfM) framework designed for dense and accurate 3D reconstruction from multi-view images. Sparse keypoint matching, which traditional SfM methods often rely on, limits both accuracy and point density, especially in texture-less areas. Dense-SfM addresses this limitation by integrating dense matching with a Gaussian Splatting (GS) based track extension which gives more consistent, longer feature tracks. To further improve reconstruction accuracy, Dense-SfM is equipped with a multi-view kernelized matching module leveraging transformer and Gaussian Process architectures, for robust track refinement across multi-views. Evaluations on the ETH3D and Texture-Poor SfM datasets show that Dense-SfM offers significant improvements in accuracy and density over state-of-the-art methods. Project page: https://icetea-cv.github.io/densesfm/.", "link": "http://arxiv.org/abs/2501.14277v3", "date": "2026-01-28", "relevancy": 2.9619, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.602}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5935}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dense-SfM%3A%20Structure%20from%20Motion%20with%20Dense%20Consistent%20Matching&body=Title%3A%20Dense-SfM%3A%20Structure%20from%20Motion%20with%20Dense%20Consistent%20Matching%0AAuthor%3A%20JongMin%20Lee%20and%20Sungjoo%20Yoo%0AAbstract%3A%20We%20present%20Dense-SfM%2C%20a%20novel%20Structure%20from%20Motion%20%28SfM%29%20framework%20designed%20for%20dense%20and%20accurate%203D%20reconstruction%20from%20multi-view%20images.%20Sparse%20keypoint%20matching%2C%20which%20traditional%20SfM%20methods%20often%20rely%20on%2C%20limits%20both%20accuracy%20and%20point%20density%2C%20especially%20in%20texture-less%20areas.%20Dense-SfM%20addresses%20this%20limitation%20by%20integrating%20dense%20matching%20with%20a%20Gaussian%20Splatting%20%28GS%29%20based%20track%20extension%20which%20gives%20more%20consistent%2C%20longer%20feature%20tracks.%20To%20further%20improve%20reconstruction%20accuracy%2C%20Dense-SfM%20is%20equipped%20with%20a%20multi-view%20kernelized%20matching%20module%20leveraging%20transformer%20and%20Gaussian%20Process%20architectures%2C%20for%20robust%20track%20refinement%20across%20multi-views.%20Evaluations%20on%20the%20ETH3D%20and%20Texture-Poor%20SfM%20datasets%20show%20that%20Dense-SfM%20offers%20significant%20improvements%20in%20accuracy%20and%20density%20over%20state-of-the-art%20methods.%20Project%20page%3A%20https%3A//icetea-cv.github.io/densesfm/.%0ALink%3A%20http%3A//arxiv.org/abs/2501.14277v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDense-SfM%253A%2520Structure%2520from%2520Motion%2520with%2520Dense%2520Consistent%2520Matching%26entry.906535625%3DJongMin%2520Lee%2520and%2520Sungjoo%2520Yoo%26entry.1292438233%3DWe%2520present%2520Dense-SfM%252C%2520a%2520novel%2520Structure%2520from%2520Motion%2520%2528SfM%2529%2520framework%2520designed%2520for%2520dense%2520and%2520accurate%25203D%2520reconstruction%2520from%2520multi-view%2520images.%2520Sparse%2520keypoint%2520matching%252C%2520which%2520traditional%2520SfM%2520methods%2520often%2520rely%2520on%252C%2520limits%2520both%2520accuracy%2520and%2520point%2520density%252C%2520especially%2520in%2520texture-less%2520areas.%2520Dense-SfM%2520addresses%2520this%2520limitation%2520by%2520integrating%2520dense%2520matching%2520with%2520a%2520Gaussian%2520Splatting%2520%2528GS%2529%2520based%2520track%2520extension%2520which%2520gives%2520more%2520consistent%252C%2520longer%2520feature%2520tracks.%2520To%2520further%2520improve%2520reconstruction%2520accuracy%252C%2520Dense-SfM%2520is%2520equipped%2520with%2520a%2520multi-view%2520kernelized%2520matching%2520module%2520leveraging%2520transformer%2520and%2520Gaussian%2520Process%2520architectures%252C%2520for%2520robust%2520track%2520refinement%2520across%2520multi-views.%2520Evaluations%2520on%2520the%2520ETH3D%2520and%2520Texture-Poor%2520SfM%2520datasets%2520show%2520that%2520Dense-SfM%2520offers%2520significant%2520improvements%2520in%2520accuracy%2520and%2520density%2520over%2520state-of-the-art%2520methods.%2520Project%2520page%253A%2520https%253A//icetea-cv.github.io/densesfm/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.14277v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dense-SfM%3A%20Structure%20from%20Motion%20with%20Dense%20Consistent%20Matching&entry.906535625=JongMin%20Lee%20and%20Sungjoo%20Yoo&entry.1292438233=We%20present%20Dense-SfM%2C%20a%20novel%20Structure%20from%20Motion%20%28SfM%29%20framework%20designed%20for%20dense%20and%20accurate%203D%20reconstruction%20from%20multi-view%20images.%20Sparse%20keypoint%20matching%2C%20which%20traditional%20SfM%20methods%20often%20rely%20on%2C%20limits%20both%20accuracy%20and%20point%20density%2C%20especially%20in%20texture-less%20areas.%20Dense-SfM%20addresses%20this%20limitation%20by%20integrating%20dense%20matching%20with%20a%20Gaussian%20Splatting%20%28GS%29%20based%20track%20extension%20which%20gives%20more%20consistent%2C%20longer%20feature%20tracks.%20To%20further%20improve%20reconstruction%20accuracy%2C%20Dense-SfM%20is%20equipped%20with%20a%20multi-view%20kernelized%20matching%20module%20leveraging%20transformer%20and%20Gaussian%20Process%20architectures%2C%20for%20robust%20track%20refinement%20across%20multi-views.%20Evaluations%20on%20the%20ETH3D%20and%20Texture-Poor%20SfM%20datasets%20show%20that%20Dense-SfM%20offers%20significant%20improvements%20in%20accuracy%20and%20density%20over%20state-of-the-art%20methods.%20Project%20page%3A%20https%3A//icetea-cv.github.io/densesfm/.&entry.1838667208=http%3A//arxiv.org/abs/2501.14277v3&entry.124074799=Read"},
{"title": "X-SAM: From Segment Anything to Any Segmentation", "author": "Hao Wang and Limeng Qiao and Zequn Jie and Zhijian Huang and Chengjian Feng and Qingfang Zheng and Lin Ma and Xiangyuan Lan and Xiaodan Liang", "abstract": "Large Language Models (LLMs) demonstrate strong capabilities in broad knowledge representation, yet they are inherently deficient in pixel-level perceptual understanding. Although the Segment Anything Model (SAM) represents a significant advancement in visual-prompt-driven image segmentation, it exhibits notable limitations in multi-mask prediction and category-specific segmentation tasks, and it cannot integrate all segmentation tasks within a unified model architecture. To address these limitations, we present X-SAM, a streamlined Multimodal Large Language Model (MLLM) framework that extends the segmentation paradigm from \\textit{segment anything} to \\textit{any segmentation}. Specifically, we introduce a novel unified framework that enables more advanced pixel-level perceptual comprehension for MLLMs. Furthermore, we propose a new segmentation task, termed Visual GrounDed (VGD) segmentation, which segments all instance objects with interactive visual prompts and empowers MLLMs with visual grounded, pixel-wise interpretative capabilities. To enable effective training on diverse data sources, we present a unified training strategy that supports co-training across multiple datasets. Experimental results demonstrate that X-SAM achieves state-of-the-art performance on a wide range of image segmentation benchmarks, highlighting its efficiency for multimodal, pixel-level visual understanding. Code is available at https://github.com/wanghao9610/X-SAM.", "link": "http://arxiv.org/abs/2508.04655v2", "date": "2026-01-28", "relevancy": 2.9495, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6155}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5771}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20X-SAM%3A%20From%20Segment%20Anything%20to%20Any%20Segmentation&body=Title%3A%20X-SAM%3A%20From%20Segment%20Anything%20to%20Any%20Segmentation%0AAuthor%3A%20Hao%20Wang%20and%20Limeng%20Qiao%20and%20Zequn%20Jie%20and%20Zhijian%20Huang%20and%20Chengjian%20Feng%20and%20Qingfang%20Zheng%20and%20Lin%20Ma%20and%20Xiangyuan%20Lan%20and%20Xiaodan%20Liang%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20strong%20capabilities%20in%20broad%20knowledge%20representation%2C%20yet%20they%20are%20inherently%20deficient%20in%20pixel-level%20perceptual%20understanding.%20Although%20the%20Segment%20Anything%20Model%20%28SAM%29%20represents%20a%20significant%20advancement%20in%20visual-prompt-driven%20image%20segmentation%2C%20it%20exhibits%20notable%20limitations%20in%20multi-mask%20prediction%20and%20category-specific%20segmentation%20tasks%2C%20and%20it%20cannot%20integrate%20all%20segmentation%20tasks%20within%20a%20unified%20model%20architecture.%20To%20address%20these%20limitations%2C%20we%20present%20X-SAM%2C%20a%20streamlined%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20framework%20that%20extends%20the%20segmentation%20paradigm%20from%20%5Ctextit%7Bsegment%20anything%7D%20to%20%5Ctextit%7Bany%20segmentation%7D.%20Specifically%2C%20we%20introduce%20a%20novel%20unified%20framework%20that%20enables%20more%20advanced%20pixel-level%20perceptual%20comprehension%20for%20MLLMs.%20Furthermore%2C%20we%20propose%20a%20new%20segmentation%20task%2C%20termed%20Visual%20GrounDed%20%28VGD%29%20segmentation%2C%20which%20segments%20all%20instance%20objects%20with%20interactive%20visual%20prompts%20and%20empowers%20MLLMs%20with%20visual%20grounded%2C%20pixel-wise%20interpretative%20capabilities.%20To%20enable%20effective%20training%20on%20diverse%20data%20sources%2C%20we%20present%20a%20unified%20training%20strategy%20that%20supports%20co-training%20across%20multiple%20datasets.%20Experimental%20results%20demonstrate%20that%20X-SAM%20achieves%20state-of-the-art%20performance%20on%20a%20wide%20range%20of%20image%20segmentation%20benchmarks%2C%20highlighting%20its%20efficiency%20for%20multimodal%2C%20pixel-level%20visual%20understanding.%20Code%20is%20available%20at%20https%3A//github.com/wanghao9610/X-SAM.%0ALink%3A%20http%3A//arxiv.org/abs/2508.04655v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DX-SAM%253A%2520From%2520Segment%2520Anything%2520to%2520Any%2520Segmentation%26entry.906535625%3DHao%2520Wang%2520and%2520Limeng%2520Qiao%2520and%2520Zequn%2520Jie%2520and%2520Zhijian%2520Huang%2520and%2520Chengjian%2520Feng%2520and%2520Qingfang%2520Zheng%2520and%2520Lin%2520Ma%2520and%2520Xiangyuan%2520Lan%2520and%2520Xiaodan%2520Liang%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520demonstrate%2520strong%2520capabilities%2520in%2520broad%2520knowledge%2520representation%252C%2520yet%2520they%2520are%2520inherently%2520deficient%2520in%2520pixel-level%2520perceptual%2520understanding.%2520Although%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520represents%2520a%2520significant%2520advancement%2520in%2520visual-prompt-driven%2520image%2520segmentation%252C%2520it%2520exhibits%2520notable%2520limitations%2520in%2520multi-mask%2520prediction%2520and%2520category-specific%2520segmentation%2520tasks%252C%2520and%2520it%2520cannot%2520integrate%2520all%2520segmentation%2520tasks%2520within%2520a%2520unified%2520model%2520architecture.%2520To%2520address%2520these%2520limitations%252C%2520we%2520present%2520X-SAM%252C%2520a%2520streamlined%2520Multimodal%2520Large%2520Language%2520Model%2520%2528MLLM%2529%2520framework%2520that%2520extends%2520the%2520segmentation%2520paradigm%2520from%2520%255Ctextit%257Bsegment%2520anything%257D%2520to%2520%255Ctextit%257Bany%2520segmentation%257D.%2520Specifically%252C%2520we%2520introduce%2520a%2520novel%2520unified%2520framework%2520that%2520enables%2520more%2520advanced%2520pixel-level%2520perceptual%2520comprehension%2520for%2520MLLMs.%2520Furthermore%252C%2520we%2520propose%2520a%2520new%2520segmentation%2520task%252C%2520termed%2520Visual%2520GrounDed%2520%2528VGD%2529%2520segmentation%252C%2520which%2520segments%2520all%2520instance%2520objects%2520with%2520interactive%2520visual%2520prompts%2520and%2520empowers%2520MLLMs%2520with%2520visual%2520grounded%252C%2520pixel-wise%2520interpretative%2520capabilities.%2520To%2520enable%2520effective%2520training%2520on%2520diverse%2520data%2520sources%252C%2520we%2520present%2520a%2520unified%2520training%2520strategy%2520that%2520supports%2520co-training%2520across%2520multiple%2520datasets.%2520Experimental%2520results%2520demonstrate%2520that%2520X-SAM%2520achieves%2520state-of-the-art%2520performance%2520on%2520a%2520wide%2520range%2520of%2520image%2520segmentation%2520benchmarks%252C%2520highlighting%2520its%2520efficiency%2520for%2520multimodal%252C%2520pixel-level%2520visual%2520understanding.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/wanghao9610/X-SAM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04655v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=X-SAM%3A%20From%20Segment%20Anything%20to%20Any%20Segmentation&entry.906535625=Hao%20Wang%20and%20Limeng%20Qiao%20and%20Zequn%20Jie%20and%20Zhijian%20Huang%20and%20Chengjian%20Feng%20and%20Qingfang%20Zheng%20and%20Lin%20Ma%20and%20Xiangyuan%20Lan%20and%20Xiaodan%20Liang&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20demonstrate%20strong%20capabilities%20in%20broad%20knowledge%20representation%2C%20yet%20they%20are%20inherently%20deficient%20in%20pixel-level%20perceptual%20understanding.%20Although%20the%20Segment%20Anything%20Model%20%28SAM%29%20represents%20a%20significant%20advancement%20in%20visual-prompt-driven%20image%20segmentation%2C%20it%20exhibits%20notable%20limitations%20in%20multi-mask%20prediction%20and%20category-specific%20segmentation%20tasks%2C%20and%20it%20cannot%20integrate%20all%20segmentation%20tasks%20within%20a%20unified%20model%20architecture.%20To%20address%20these%20limitations%2C%20we%20present%20X-SAM%2C%20a%20streamlined%20Multimodal%20Large%20Language%20Model%20%28MLLM%29%20framework%20that%20extends%20the%20segmentation%20paradigm%20from%20%5Ctextit%7Bsegment%20anything%7D%20to%20%5Ctextit%7Bany%20segmentation%7D.%20Specifically%2C%20we%20introduce%20a%20novel%20unified%20framework%20that%20enables%20more%20advanced%20pixel-level%20perceptual%20comprehension%20for%20MLLMs.%20Furthermore%2C%20we%20propose%20a%20new%20segmentation%20task%2C%20termed%20Visual%20GrounDed%20%28VGD%29%20segmentation%2C%20which%20segments%20all%20instance%20objects%20with%20interactive%20visual%20prompts%20and%20empowers%20MLLMs%20with%20visual%20grounded%2C%20pixel-wise%20interpretative%20capabilities.%20To%20enable%20effective%20training%20on%20diverse%20data%20sources%2C%20we%20present%20a%20unified%20training%20strategy%20that%20supports%20co-training%20across%20multiple%20datasets.%20Experimental%20results%20demonstrate%20that%20X-SAM%20achieves%20state-of-the-art%20performance%20on%20a%20wide%20range%20of%20image%20segmentation%20benchmarks%2C%20highlighting%20its%20efficiency%20for%20multimodal%2C%20pixel-level%20visual%20understanding.%20Code%20is%20available%20at%20https%3A//github.com/wanghao9610/X-SAM.&entry.1838667208=http%3A//arxiv.org/abs/2508.04655v2&entry.124074799=Read"},
{"title": "Splat Feature Solver", "author": "Butian Xiong and Rong Liu and Kenneth Xu and Meida Chen and Andrew Feng", "abstract": "Feature lifting has emerged as a crucial component in 3D scene understanding, enabling the attachment of rich image feature descriptors (e.g., DINO, CLIP) onto splat-based 3D representations. The core challenge lies in optimally assigning rich general attributes to 3D primitives while addressing the inconsistency issues from multi-view images. We present a unified, kernel- and feature-agnostic formulation of the feature lifting problem as a sparse linear inverse problem, which can be solved efficiently in closed form. Our approach admits a provable upper bound on the global optimal error under convex losses for delivering high quality lifted features. To address inconsistencies and noise in multi-view observations, we introduce two complementary regularization strategies to stabilize the solution and enhance semantic fidelity. Tikhonov Guidance enforces numerical stability through soft diagonal dominance, while Post-Lifting Aggregation filters noisy inputs via feature clustering. Extensive experiments demonstrate that our approach achieves state-of-the-art performance on open-vocabulary 3D segmentation benchmarks, outperforming training-based, grouping-based, and heuristic-forward baselines while producing lifted features in minutes. Our \\textbf{code} is available in the \\href{https://github.com/saliteta/splat-distiller/tree/main}{\\textcolor{blue}{GitHub}}. We provide additional \\href{https://splat-distiller.pages.dev/}{\\textcolor{blue}{website}} for more visualization, as well as the \\href{https://www.youtube.com/watch?v=CH-G5hbvArM}{\\textcolor{blue}{video}}.", "link": "http://arxiv.org/abs/2508.12216v2", "date": "2026-01-28", "relevancy": 2.9184, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6204}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5986}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5321}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Splat%20Feature%20Solver&body=Title%3A%20Splat%20Feature%20Solver%0AAuthor%3A%20Butian%20Xiong%20and%20Rong%20Liu%20and%20Kenneth%20Xu%20and%20Meida%20Chen%20and%20Andrew%20Feng%0AAbstract%3A%20Feature%20lifting%20has%20emerged%20as%20a%20crucial%20component%20in%203D%20scene%20understanding%2C%20enabling%20the%20attachment%20of%20rich%20image%20feature%20descriptors%20%28e.g.%2C%20DINO%2C%20CLIP%29%20onto%20splat-based%203D%20representations.%20The%20core%20challenge%20lies%20in%20optimally%20assigning%20rich%20general%20attributes%20to%203D%20primitives%20while%20addressing%20the%20inconsistency%20issues%20from%20multi-view%20images.%20We%20present%20a%20unified%2C%20kernel-%20and%20feature-agnostic%20formulation%20of%20the%20feature%20lifting%20problem%20as%20a%20sparse%20linear%20inverse%20problem%2C%20which%20can%20be%20solved%20efficiently%20in%20closed%20form.%20Our%20approach%20admits%20a%20provable%20upper%20bound%20on%20the%20global%20optimal%20error%20under%20convex%20losses%20for%20delivering%20high%20quality%20lifted%20features.%20To%20address%20inconsistencies%20and%20noise%20in%20multi-view%20observations%2C%20we%20introduce%20two%20complementary%20regularization%20strategies%20to%20stabilize%20the%20solution%20and%20enhance%20semantic%20fidelity.%20Tikhonov%20Guidance%20enforces%20numerical%20stability%20through%20soft%20diagonal%20dominance%2C%20while%20Post-Lifting%20Aggregation%20filters%20noisy%20inputs%20via%20feature%20clustering.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20achieves%20state-of-the-art%20performance%20on%20open-vocabulary%203D%20segmentation%20benchmarks%2C%20outperforming%20training-based%2C%20grouping-based%2C%20and%20heuristic-forward%20baselines%20while%20producing%20lifted%20features%20in%20minutes.%20Our%20%5Ctextbf%7Bcode%7D%20is%20available%20in%20the%20%5Chref%7Bhttps%3A//github.com/saliteta/splat-distiller/tree/main%7D%7B%5Ctextcolor%7Bblue%7D%7BGitHub%7D%7D.%20We%20provide%20additional%20%5Chref%7Bhttps%3A//splat-distiller.pages.dev/%7D%7B%5Ctextcolor%7Bblue%7D%7Bwebsite%7D%7D%20for%20more%20visualization%2C%20as%20well%20as%20the%20%5Chref%7Bhttps%3A//www.youtube.com/watch%3Fv%3DCH-G5hbvArM%7D%7B%5Ctextcolor%7Bblue%7D%7Bvideo%7D%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2508.12216v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSplat%2520Feature%2520Solver%26entry.906535625%3DButian%2520Xiong%2520and%2520Rong%2520Liu%2520and%2520Kenneth%2520Xu%2520and%2520Meida%2520Chen%2520and%2520Andrew%2520Feng%26entry.1292438233%3DFeature%2520lifting%2520has%2520emerged%2520as%2520a%2520crucial%2520component%2520in%25203D%2520scene%2520understanding%252C%2520enabling%2520the%2520attachment%2520of%2520rich%2520image%2520feature%2520descriptors%2520%2528e.g.%252C%2520DINO%252C%2520CLIP%2529%2520onto%2520splat-based%25203D%2520representations.%2520The%2520core%2520challenge%2520lies%2520in%2520optimally%2520assigning%2520rich%2520general%2520attributes%2520to%25203D%2520primitives%2520while%2520addressing%2520the%2520inconsistency%2520issues%2520from%2520multi-view%2520images.%2520We%2520present%2520a%2520unified%252C%2520kernel-%2520and%2520feature-agnostic%2520formulation%2520of%2520the%2520feature%2520lifting%2520problem%2520as%2520a%2520sparse%2520linear%2520inverse%2520problem%252C%2520which%2520can%2520be%2520solved%2520efficiently%2520in%2520closed%2520form.%2520Our%2520approach%2520admits%2520a%2520provable%2520upper%2520bound%2520on%2520the%2520global%2520optimal%2520error%2520under%2520convex%2520losses%2520for%2520delivering%2520high%2520quality%2520lifted%2520features.%2520To%2520address%2520inconsistencies%2520and%2520noise%2520in%2520multi-view%2520observations%252C%2520we%2520introduce%2520two%2520complementary%2520regularization%2520strategies%2520to%2520stabilize%2520the%2520solution%2520and%2520enhance%2520semantic%2520fidelity.%2520Tikhonov%2520Guidance%2520enforces%2520numerical%2520stability%2520through%2520soft%2520diagonal%2520dominance%252C%2520while%2520Post-Lifting%2520Aggregation%2520filters%2520noisy%2520inputs%2520via%2520feature%2520clustering.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520state-of-the-art%2520performance%2520on%2520open-vocabulary%25203D%2520segmentation%2520benchmarks%252C%2520outperforming%2520training-based%252C%2520grouping-based%252C%2520and%2520heuristic-forward%2520baselines%2520while%2520producing%2520lifted%2520features%2520in%2520minutes.%2520Our%2520%255Ctextbf%257Bcode%257D%2520is%2520available%2520in%2520the%2520%255Chref%257Bhttps%253A//github.com/saliteta/splat-distiller/tree/main%257D%257B%255Ctextcolor%257Bblue%257D%257BGitHub%257D%257D.%2520We%2520provide%2520additional%2520%255Chref%257Bhttps%253A//splat-distiller.pages.dev/%257D%257B%255Ctextcolor%257Bblue%257D%257Bwebsite%257D%257D%2520for%2520more%2520visualization%252C%2520as%2520well%2520as%2520the%2520%255Chref%257Bhttps%253A//www.youtube.com/watch%253Fv%253DCH-G5hbvArM%257D%257B%255Ctextcolor%257Bblue%257D%257Bvideo%257D%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.12216v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Splat%20Feature%20Solver&entry.906535625=Butian%20Xiong%20and%20Rong%20Liu%20and%20Kenneth%20Xu%20and%20Meida%20Chen%20and%20Andrew%20Feng&entry.1292438233=Feature%20lifting%20has%20emerged%20as%20a%20crucial%20component%20in%203D%20scene%20understanding%2C%20enabling%20the%20attachment%20of%20rich%20image%20feature%20descriptors%20%28e.g.%2C%20DINO%2C%20CLIP%29%20onto%20splat-based%203D%20representations.%20The%20core%20challenge%20lies%20in%20optimally%20assigning%20rich%20general%20attributes%20to%203D%20primitives%20while%20addressing%20the%20inconsistency%20issues%20from%20multi-view%20images.%20We%20present%20a%20unified%2C%20kernel-%20and%20feature-agnostic%20formulation%20of%20the%20feature%20lifting%20problem%20as%20a%20sparse%20linear%20inverse%20problem%2C%20which%20can%20be%20solved%20efficiently%20in%20closed%20form.%20Our%20approach%20admits%20a%20provable%20upper%20bound%20on%20the%20global%20optimal%20error%20under%20convex%20losses%20for%20delivering%20high%20quality%20lifted%20features.%20To%20address%20inconsistencies%20and%20noise%20in%20multi-view%20observations%2C%20we%20introduce%20two%20complementary%20regularization%20strategies%20to%20stabilize%20the%20solution%20and%20enhance%20semantic%20fidelity.%20Tikhonov%20Guidance%20enforces%20numerical%20stability%20through%20soft%20diagonal%20dominance%2C%20while%20Post-Lifting%20Aggregation%20filters%20noisy%20inputs%20via%20feature%20clustering.%20Extensive%20experiments%20demonstrate%20that%20our%20approach%20achieves%20state-of-the-art%20performance%20on%20open-vocabulary%203D%20segmentation%20benchmarks%2C%20outperforming%20training-based%2C%20grouping-based%2C%20and%20heuristic-forward%20baselines%20while%20producing%20lifted%20features%20in%20minutes.%20Our%20%5Ctextbf%7Bcode%7D%20is%20available%20in%20the%20%5Chref%7Bhttps%3A//github.com/saliteta/splat-distiller/tree/main%7D%7B%5Ctextcolor%7Bblue%7D%7BGitHub%7D%7D.%20We%20provide%20additional%20%5Chref%7Bhttps%3A//splat-distiller.pages.dev/%7D%7B%5Ctextcolor%7Bblue%7D%7Bwebsite%7D%7D%20for%20more%20visualization%2C%20as%20well%20as%20the%20%5Chref%7Bhttps%3A//www.youtube.com/watch%3Fv%3DCH-G5hbvArM%7D%7B%5Ctextcolor%7Bblue%7D%7Bvideo%7D%7D.&entry.1838667208=http%3A//arxiv.org/abs/2508.12216v2&entry.124074799=Read"},
{"title": "Fusion of Visual-Inertial Odometry with LiDAR Relative Localization for Cooperative Guidance of a Micro-Scale Aerial Vehicle", "author": "V\u00e1clav Pritzl and Matou\u0161 Vrba and Petr \u0160t\u011bp\u00e1n and Martin Saska", "abstract": "A novel relative localization approach for guidance of a micro-scale Unmanned Aerial Vehicle (UAV) by a well-equipped aerial robot fusing Visual-Inertial Odometry (VIO) with Light Detection and Ranging (LiDAR) is proposed in this paper. LiDAR-based localization is accurate and robust to challenging environmental conditions, but 3D LiDARs are relatively heavy and require large UAV platforms, in contrast to lightweight cameras. However, visual-based self-localization methods exhibit lower accuracy and can suffer from significant drift with respect to the global reference frame. To benefit from both sensory modalities, we focus on cooperative navigation in a heterogeneous team of a primary LiDAR-equipped UAV and a secondary micro-scale camera-equipped UAV. We propose a novel cooperative approach combining LiDAR relative localization data with VIO output on board the primary UAV to obtain an accurate pose of the secondary UAV. The pose estimate is used to precisely and reliably guide the secondary UAV along trajectories defined in the primary UAV reference frame. The experimental evaluation has shown the superior accuracy of our method to the raw VIO output, reaching the average 3D Absolute Trajectory Error (ATE) of 0.28 m, and demonstrated its capability to guide the secondary UAV along desired trajectories while mitigating VIO drift. Thus, such a heterogeneous system can explore large areas with LiDAR precision, as well as visit locations inaccessible to the large LiDAR-carrying UAV platforms, as was showcased in a real-world cooperative mapping scenario.", "link": "http://arxiv.org/abs/2306.17544v3", "date": "2026-01-28", "relevancy": 2.8624, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5912}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5644}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fusion%20of%20Visual-Inertial%20Odometry%20with%20LiDAR%20Relative%20Localization%20for%20Cooperative%20Guidance%20of%20a%20Micro-Scale%20Aerial%20Vehicle&body=Title%3A%20Fusion%20of%20Visual-Inertial%20Odometry%20with%20LiDAR%20Relative%20Localization%20for%20Cooperative%20Guidance%20of%20a%20Micro-Scale%20Aerial%20Vehicle%0AAuthor%3A%20V%C3%A1clav%20Pritzl%20and%20Matou%C5%A1%20Vrba%20and%20Petr%20%C5%A0t%C4%9Bp%C3%A1n%20and%20Martin%20Saska%0AAbstract%3A%20A%20novel%20relative%20localization%20approach%20for%20guidance%20of%20a%20micro-scale%20Unmanned%20Aerial%20Vehicle%20%28UAV%29%20by%20a%20well-equipped%20aerial%20robot%20fusing%20Visual-Inertial%20Odometry%20%28VIO%29%20with%20Light%20Detection%20and%20Ranging%20%28LiDAR%29%20is%20proposed%20in%20this%20paper.%20LiDAR-based%20localization%20is%20accurate%20and%20robust%20to%20challenging%20environmental%20conditions%2C%20but%203D%20LiDARs%20are%20relatively%20heavy%20and%20require%20large%20UAV%20platforms%2C%20in%20contrast%20to%20lightweight%20cameras.%20However%2C%20visual-based%20self-localization%20methods%20exhibit%20lower%20accuracy%20and%20can%20suffer%20from%20significant%20drift%20with%20respect%20to%20the%20global%20reference%20frame.%20To%20benefit%20from%20both%20sensory%20modalities%2C%20we%20focus%20on%20cooperative%20navigation%20in%20a%20heterogeneous%20team%20of%20a%20primary%20LiDAR-equipped%20UAV%20and%20a%20secondary%20micro-scale%20camera-equipped%20UAV.%20We%20propose%20a%20novel%20cooperative%20approach%20combining%20LiDAR%20relative%20localization%20data%20with%20VIO%20output%20on%20board%20the%20primary%20UAV%20to%20obtain%20an%20accurate%20pose%20of%20the%20secondary%20UAV.%20The%20pose%20estimate%20is%20used%20to%20precisely%20and%20reliably%20guide%20the%20secondary%20UAV%20along%20trajectories%20defined%20in%20the%20primary%20UAV%20reference%20frame.%20The%20experimental%20evaluation%20has%20shown%20the%20superior%20accuracy%20of%20our%20method%20to%20the%20raw%20VIO%20output%2C%20reaching%20the%20average%203D%20Absolute%20Trajectory%20Error%20%28ATE%29%20of%200.28%20m%2C%20and%20demonstrated%20its%20capability%20to%20guide%20the%20secondary%20UAV%20along%20desired%20trajectories%20while%20mitigating%20VIO%20drift.%20Thus%2C%20such%20a%20heterogeneous%20system%20can%20explore%20large%20areas%20with%20LiDAR%20precision%2C%20as%20well%20as%20visit%20locations%20inaccessible%20to%20the%20large%20LiDAR-carrying%20UAV%20platforms%2C%20as%20was%20showcased%20in%20a%20real-world%20cooperative%20mapping%20scenario.%0ALink%3A%20http%3A//arxiv.org/abs/2306.17544v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFusion%2520of%2520Visual-Inertial%2520Odometry%2520with%2520LiDAR%2520Relative%2520Localization%2520for%2520Cooperative%2520Guidance%2520of%2520a%2520Micro-Scale%2520Aerial%2520Vehicle%26entry.906535625%3DV%25C3%25A1clav%2520Pritzl%2520and%2520Matou%25C5%25A1%2520Vrba%2520and%2520Petr%2520%25C5%25A0t%25C4%259Bp%25C3%25A1n%2520and%2520Martin%2520Saska%26entry.1292438233%3DA%2520novel%2520relative%2520localization%2520approach%2520for%2520guidance%2520of%2520a%2520micro-scale%2520Unmanned%2520Aerial%2520Vehicle%2520%2528UAV%2529%2520by%2520a%2520well-equipped%2520aerial%2520robot%2520fusing%2520Visual-Inertial%2520Odometry%2520%2528VIO%2529%2520with%2520Light%2520Detection%2520and%2520Ranging%2520%2528LiDAR%2529%2520is%2520proposed%2520in%2520this%2520paper.%2520LiDAR-based%2520localization%2520is%2520accurate%2520and%2520robust%2520to%2520challenging%2520environmental%2520conditions%252C%2520but%25203D%2520LiDARs%2520are%2520relatively%2520heavy%2520and%2520require%2520large%2520UAV%2520platforms%252C%2520in%2520contrast%2520to%2520lightweight%2520cameras.%2520However%252C%2520visual-based%2520self-localization%2520methods%2520exhibit%2520lower%2520accuracy%2520and%2520can%2520suffer%2520from%2520significant%2520drift%2520with%2520respect%2520to%2520the%2520global%2520reference%2520frame.%2520To%2520benefit%2520from%2520both%2520sensory%2520modalities%252C%2520we%2520focus%2520on%2520cooperative%2520navigation%2520in%2520a%2520heterogeneous%2520team%2520of%2520a%2520primary%2520LiDAR-equipped%2520UAV%2520and%2520a%2520secondary%2520micro-scale%2520camera-equipped%2520UAV.%2520We%2520propose%2520a%2520novel%2520cooperative%2520approach%2520combining%2520LiDAR%2520relative%2520localization%2520data%2520with%2520VIO%2520output%2520on%2520board%2520the%2520primary%2520UAV%2520to%2520obtain%2520an%2520accurate%2520pose%2520of%2520the%2520secondary%2520UAV.%2520The%2520pose%2520estimate%2520is%2520used%2520to%2520precisely%2520and%2520reliably%2520guide%2520the%2520secondary%2520UAV%2520along%2520trajectories%2520defined%2520in%2520the%2520primary%2520UAV%2520reference%2520frame.%2520The%2520experimental%2520evaluation%2520has%2520shown%2520the%2520superior%2520accuracy%2520of%2520our%2520method%2520to%2520the%2520raw%2520VIO%2520output%252C%2520reaching%2520the%2520average%25203D%2520Absolute%2520Trajectory%2520Error%2520%2528ATE%2529%2520of%25200.28%2520m%252C%2520and%2520demonstrated%2520its%2520capability%2520to%2520guide%2520the%2520secondary%2520UAV%2520along%2520desired%2520trajectories%2520while%2520mitigating%2520VIO%2520drift.%2520Thus%252C%2520such%2520a%2520heterogeneous%2520system%2520can%2520explore%2520large%2520areas%2520with%2520LiDAR%2520precision%252C%2520as%2520well%2520as%2520visit%2520locations%2520inaccessible%2520to%2520the%2520large%2520LiDAR-carrying%2520UAV%2520platforms%252C%2520as%2520was%2520showcased%2520in%2520a%2520real-world%2520cooperative%2520mapping%2520scenario.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.17544v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fusion%20of%20Visual-Inertial%20Odometry%20with%20LiDAR%20Relative%20Localization%20for%20Cooperative%20Guidance%20of%20a%20Micro-Scale%20Aerial%20Vehicle&entry.906535625=V%C3%A1clav%20Pritzl%20and%20Matou%C5%A1%20Vrba%20and%20Petr%20%C5%A0t%C4%9Bp%C3%A1n%20and%20Martin%20Saska&entry.1292438233=A%20novel%20relative%20localization%20approach%20for%20guidance%20of%20a%20micro-scale%20Unmanned%20Aerial%20Vehicle%20%28UAV%29%20by%20a%20well-equipped%20aerial%20robot%20fusing%20Visual-Inertial%20Odometry%20%28VIO%29%20with%20Light%20Detection%20and%20Ranging%20%28LiDAR%29%20is%20proposed%20in%20this%20paper.%20LiDAR-based%20localization%20is%20accurate%20and%20robust%20to%20challenging%20environmental%20conditions%2C%20but%203D%20LiDARs%20are%20relatively%20heavy%20and%20require%20large%20UAV%20platforms%2C%20in%20contrast%20to%20lightweight%20cameras.%20However%2C%20visual-based%20self-localization%20methods%20exhibit%20lower%20accuracy%20and%20can%20suffer%20from%20significant%20drift%20with%20respect%20to%20the%20global%20reference%20frame.%20To%20benefit%20from%20both%20sensory%20modalities%2C%20we%20focus%20on%20cooperative%20navigation%20in%20a%20heterogeneous%20team%20of%20a%20primary%20LiDAR-equipped%20UAV%20and%20a%20secondary%20micro-scale%20camera-equipped%20UAV.%20We%20propose%20a%20novel%20cooperative%20approach%20combining%20LiDAR%20relative%20localization%20data%20with%20VIO%20output%20on%20board%20the%20primary%20UAV%20to%20obtain%20an%20accurate%20pose%20of%20the%20secondary%20UAV.%20The%20pose%20estimate%20is%20used%20to%20precisely%20and%20reliably%20guide%20the%20secondary%20UAV%20along%20trajectories%20defined%20in%20the%20primary%20UAV%20reference%20frame.%20The%20experimental%20evaluation%20has%20shown%20the%20superior%20accuracy%20of%20our%20method%20to%20the%20raw%20VIO%20output%2C%20reaching%20the%20average%203D%20Absolute%20Trajectory%20Error%20%28ATE%29%20of%200.28%20m%2C%20and%20demonstrated%20its%20capability%20to%20guide%20the%20secondary%20UAV%20along%20desired%20trajectories%20while%20mitigating%20VIO%20drift.%20Thus%2C%20such%20a%20heterogeneous%20system%20can%20explore%20large%20areas%20with%20LiDAR%20precision%2C%20as%20well%20as%20visit%20locations%20inaccessible%20to%20the%20large%20LiDAR-carrying%20UAV%20platforms%2C%20as%20was%20showcased%20in%20a%20real-world%20cooperative%20mapping%20scenario.&entry.1838667208=http%3A//arxiv.org/abs/2306.17544v3&entry.124074799=Read"},
{"title": "DaMO: A Data-Efficient Multimodal Orchestrator for Temporal Reasoning with Video LLMs", "author": "Bo-Cheng Chiu and Jen-Jee Chen and Yu-Chee Tseng and Feng-Chi Chen and An-Zi Yen", "abstract": "Large Language Models (LLMs) have recently been extended to the video domain, enabling sophisticated video-language understanding. However, existing Video LLMs often exhibit limitations in fine-grained temporal reasoning, restricting their ability to precisely attribute responses to specific video moments, especially under constrained supervision. We introduce DaMO, a data-efficient Video LLM explicitly designed for accurate temporal reasoning and multimodal understanding. At its core, the proposed Temporal-aware Fuseformer employs a hierarchical dual-stream architecture that progressively captures temporal dynamics within each modality and effectively fuses complementary visual and audio information. To further enhance computational efficiency, DaMO integrates a global residual that reduces spatial redundancy while preserving essential semantic details. We train DaMO via a structured four-stage progressive training paradigm, incrementally equipping the model with multimodal alignment, semantic grounding, and temporal reasoning capabilities. This work also contributes multiple datasets augmented from existing ones with LLM-generated temporally grounded QA pairs for tasks requiring temporal supervision. Comprehensive experiments on temporal grounding and video QA benchmarks demonstrate that DaMO consistently surpasses prior methods, particularly in tasks demanding precise temporal alignment and reasoning. Our work establishes a promising direction for data-efficient video-language modeling.", "link": "http://arxiv.org/abs/2506.11558v4", "date": "2026-01-28", "relevancy": 2.8537, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5861}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5631}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DaMO%3A%20A%20Data-Efficient%20Multimodal%20Orchestrator%20for%20Temporal%20Reasoning%20with%20Video%20LLMs&body=Title%3A%20DaMO%3A%20A%20Data-Efficient%20Multimodal%20Orchestrator%20for%20Temporal%20Reasoning%20with%20Video%20LLMs%0AAuthor%3A%20Bo-Cheng%20Chiu%20and%20Jen-Jee%20Chen%20and%20Yu-Chee%20Tseng%20and%20Feng-Chi%20Chen%20and%20An-Zi%20Yen%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20have%20recently%20been%20extended%20to%20the%20video%20domain%2C%20enabling%20sophisticated%20video-language%20understanding.%20However%2C%20existing%20Video%20LLMs%20often%20exhibit%20limitations%20in%20fine-grained%20temporal%20reasoning%2C%20restricting%20their%20ability%20to%20precisely%20attribute%20responses%20to%20specific%20video%20moments%2C%20especially%20under%20constrained%20supervision.%20We%20introduce%20DaMO%2C%20a%20data-efficient%20Video%20LLM%20explicitly%20designed%20for%20accurate%20temporal%20reasoning%20and%20multimodal%20understanding.%20At%20its%20core%2C%20the%20proposed%20Temporal-aware%20Fuseformer%20employs%20a%20hierarchical%20dual-stream%20architecture%20that%20progressively%20captures%20temporal%20dynamics%20within%20each%20modality%20and%20effectively%20fuses%20complementary%20visual%20and%20audio%20information.%20To%20further%20enhance%20computational%20efficiency%2C%20DaMO%20integrates%20a%20global%20residual%20that%20reduces%20spatial%20redundancy%20while%20preserving%20essential%20semantic%20details.%20We%20train%20DaMO%20via%20a%20structured%20four-stage%20progressive%20training%20paradigm%2C%20incrementally%20equipping%20the%20model%20with%20multimodal%20alignment%2C%20semantic%20grounding%2C%20and%20temporal%20reasoning%20capabilities.%20This%20work%20also%20contributes%20multiple%20datasets%20augmented%20from%20existing%20ones%20with%20LLM-generated%20temporally%20grounded%20QA%20pairs%20for%20tasks%20requiring%20temporal%20supervision.%20Comprehensive%20experiments%20on%20temporal%20grounding%20and%20video%20QA%20benchmarks%20demonstrate%20that%20DaMO%20consistently%20surpasses%20prior%20methods%2C%20particularly%20in%20tasks%20demanding%20precise%20temporal%20alignment%20and%20reasoning.%20Our%20work%20establishes%20a%20promising%20direction%20for%20data-efficient%20video-language%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2506.11558v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDaMO%253A%2520A%2520Data-Efficient%2520Multimodal%2520Orchestrator%2520for%2520Temporal%2520Reasoning%2520with%2520Video%2520LLMs%26entry.906535625%3DBo-Cheng%2520Chiu%2520and%2520Jen-Jee%2520Chen%2520and%2520Yu-Chee%2520Tseng%2520and%2520Feng-Chi%2520Chen%2520and%2520An-Zi%2520Yen%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520recently%2520been%2520extended%2520to%2520the%2520video%2520domain%252C%2520enabling%2520sophisticated%2520video-language%2520understanding.%2520However%252C%2520existing%2520Video%2520LLMs%2520often%2520exhibit%2520limitations%2520in%2520fine-grained%2520temporal%2520reasoning%252C%2520restricting%2520their%2520ability%2520to%2520precisely%2520attribute%2520responses%2520to%2520specific%2520video%2520moments%252C%2520especially%2520under%2520constrained%2520supervision.%2520We%2520introduce%2520DaMO%252C%2520a%2520data-efficient%2520Video%2520LLM%2520explicitly%2520designed%2520for%2520accurate%2520temporal%2520reasoning%2520and%2520multimodal%2520understanding.%2520At%2520its%2520core%252C%2520the%2520proposed%2520Temporal-aware%2520Fuseformer%2520employs%2520a%2520hierarchical%2520dual-stream%2520architecture%2520that%2520progressively%2520captures%2520temporal%2520dynamics%2520within%2520each%2520modality%2520and%2520effectively%2520fuses%2520complementary%2520visual%2520and%2520audio%2520information.%2520To%2520further%2520enhance%2520computational%2520efficiency%252C%2520DaMO%2520integrates%2520a%2520global%2520residual%2520that%2520reduces%2520spatial%2520redundancy%2520while%2520preserving%2520essential%2520semantic%2520details.%2520We%2520train%2520DaMO%2520via%2520a%2520structured%2520four-stage%2520progressive%2520training%2520paradigm%252C%2520incrementally%2520equipping%2520the%2520model%2520with%2520multimodal%2520alignment%252C%2520semantic%2520grounding%252C%2520and%2520temporal%2520reasoning%2520capabilities.%2520This%2520work%2520also%2520contributes%2520multiple%2520datasets%2520augmented%2520from%2520existing%2520ones%2520with%2520LLM-generated%2520temporally%2520grounded%2520QA%2520pairs%2520for%2520tasks%2520requiring%2520temporal%2520supervision.%2520Comprehensive%2520experiments%2520on%2520temporal%2520grounding%2520and%2520video%2520QA%2520benchmarks%2520demonstrate%2520that%2520DaMO%2520consistently%2520surpasses%2520prior%2520methods%252C%2520particularly%2520in%2520tasks%2520demanding%2520precise%2520temporal%2520alignment%2520and%2520reasoning.%2520Our%2520work%2520establishes%2520a%2520promising%2520direction%2520for%2520data-efficient%2520video-language%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11558v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DaMO%3A%20A%20Data-Efficient%20Multimodal%20Orchestrator%20for%20Temporal%20Reasoning%20with%20Video%20LLMs&entry.906535625=Bo-Cheng%20Chiu%20and%20Jen-Jee%20Chen%20and%20Yu-Chee%20Tseng%20and%20Feng-Chi%20Chen%20and%20An-Zi%20Yen&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20have%20recently%20been%20extended%20to%20the%20video%20domain%2C%20enabling%20sophisticated%20video-language%20understanding.%20However%2C%20existing%20Video%20LLMs%20often%20exhibit%20limitations%20in%20fine-grained%20temporal%20reasoning%2C%20restricting%20their%20ability%20to%20precisely%20attribute%20responses%20to%20specific%20video%20moments%2C%20especially%20under%20constrained%20supervision.%20We%20introduce%20DaMO%2C%20a%20data-efficient%20Video%20LLM%20explicitly%20designed%20for%20accurate%20temporal%20reasoning%20and%20multimodal%20understanding.%20At%20its%20core%2C%20the%20proposed%20Temporal-aware%20Fuseformer%20employs%20a%20hierarchical%20dual-stream%20architecture%20that%20progressively%20captures%20temporal%20dynamics%20within%20each%20modality%20and%20effectively%20fuses%20complementary%20visual%20and%20audio%20information.%20To%20further%20enhance%20computational%20efficiency%2C%20DaMO%20integrates%20a%20global%20residual%20that%20reduces%20spatial%20redundancy%20while%20preserving%20essential%20semantic%20details.%20We%20train%20DaMO%20via%20a%20structured%20four-stage%20progressive%20training%20paradigm%2C%20incrementally%20equipping%20the%20model%20with%20multimodal%20alignment%2C%20semantic%20grounding%2C%20and%20temporal%20reasoning%20capabilities.%20This%20work%20also%20contributes%20multiple%20datasets%20augmented%20from%20existing%20ones%20with%20LLM-generated%20temporally%20grounded%20QA%20pairs%20for%20tasks%20requiring%20temporal%20supervision.%20Comprehensive%20experiments%20on%20temporal%20grounding%20and%20video%20QA%20benchmarks%20demonstrate%20that%20DaMO%20consistently%20surpasses%20prior%20methods%2C%20particularly%20in%20tasks%20demanding%20precise%20temporal%20alignment%20and%20reasoning.%20Our%20work%20establishes%20a%20promising%20direction%20for%20data-efficient%20video-language%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2506.11558v4&entry.124074799=Read"},
{"title": "Person Re-ID in 2025: Supervised, Self-Supervised, and Language-Aligned. What Works?", "author": "Lakshman Balasubramanian", "abstract": "Person Re-Identification (ReID) remains a challenging problem in computer vision. This work reviews various training paradigm and evaluates the robustness of state-of-the-art ReID models in cross-domain applications and examines the role of foundation models in improving generalization through richer, more transferable visual representations. We compare three training paradigms, supervised, self-supervised, and language-aligned models. Through the study the aim is to answer the following questions: Can supervised models generalize in cross-domain scenarios? How does foundation models like SigLIP2 perform for the ReID tasks? What are the weaknesses of current supervised and foundational models for ReID? We have conducted the analysis across 11 models and 9 datasets. Our results show a clear split: supervised models dominate their training domain but crumble on cross-domain data. Language-aligned models, however, show surprising robustness cross-domain for ReID tasks, even though they are not explicitly trained to do so. Code and data available at: https://github.com/moiiai-tech/object-reid-benchmark.", "link": "http://arxiv.org/abs/2601.20598v1", "date": "2026-01-28", "relevancy": 2.8437, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5925}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5925}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Person%20Re-ID%20in%202025%3A%20Supervised%2C%20Self-Supervised%2C%20and%20Language-Aligned.%20What%20Works%3F&body=Title%3A%20Person%20Re-ID%20in%202025%3A%20Supervised%2C%20Self-Supervised%2C%20and%20Language-Aligned.%20What%20Works%3F%0AAuthor%3A%20Lakshman%20Balasubramanian%0AAbstract%3A%20Person%20Re-Identification%20%28ReID%29%20remains%20a%20challenging%20problem%20in%20computer%20vision.%20This%20work%20reviews%20various%20training%20paradigm%20and%20evaluates%20the%20robustness%20of%20state-of-the-art%20ReID%20models%20in%20cross-domain%20applications%20and%20examines%20the%20role%20of%20foundation%20models%20in%20improving%20generalization%20through%20richer%2C%20more%20transferable%20visual%20representations.%20We%20compare%20three%20training%20paradigms%2C%20supervised%2C%20self-supervised%2C%20and%20language-aligned%20models.%20Through%20the%20study%20the%20aim%20is%20to%20answer%20the%20following%20questions%3A%20Can%20supervised%20models%20generalize%20in%20cross-domain%20scenarios%3F%20How%20does%20foundation%20models%20like%20SigLIP2%20perform%20for%20the%20ReID%20tasks%3F%20What%20are%20the%20weaknesses%20of%20current%20supervised%20and%20foundational%20models%20for%20ReID%3F%20We%20have%20conducted%20the%20analysis%20across%2011%20models%20and%209%20datasets.%20Our%20results%20show%20a%20clear%20split%3A%20supervised%20models%20dominate%20their%20training%20domain%20but%20crumble%20on%20cross-domain%20data.%20Language-aligned%20models%2C%20however%2C%20show%20surprising%20robustness%20cross-domain%20for%20ReID%20tasks%2C%20even%20though%20they%20are%20not%20explicitly%20trained%20to%20do%20so.%20Code%20and%20data%20available%20at%3A%20https%3A//github.com/moiiai-tech/object-reid-benchmark.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20598v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerson%2520Re-ID%2520in%25202025%253A%2520Supervised%252C%2520Self-Supervised%252C%2520and%2520Language-Aligned.%2520What%2520Works%253F%26entry.906535625%3DLakshman%2520Balasubramanian%26entry.1292438233%3DPerson%2520Re-Identification%2520%2528ReID%2529%2520remains%2520a%2520challenging%2520problem%2520in%2520computer%2520vision.%2520This%2520work%2520reviews%2520various%2520training%2520paradigm%2520and%2520evaluates%2520the%2520robustness%2520of%2520state-of-the-art%2520ReID%2520models%2520in%2520cross-domain%2520applications%2520and%2520examines%2520the%2520role%2520of%2520foundation%2520models%2520in%2520improving%2520generalization%2520through%2520richer%252C%2520more%2520transferable%2520visual%2520representations.%2520We%2520compare%2520three%2520training%2520paradigms%252C%2520supervised%252C%2520self-supervised%252C%2520and%2520language-aligned%2520models.%2520Through%2520the%2520study%2520the%2520aim%2520is%2520to%2520answer%2520the%2520following%2520questions%253A%2520Can%2520supervised%2520models%2520generalize%2520in%2520cross-domain%2520scenarios%253F%2520How%2520does%2520foundation%2520models%2520like%2520SigLIP2%2520perform%2520for%2520the%2520ReID%2520tasks%253F%2520What%2520are%2520the%2520weaknesses%2520of%2520current%2520supervised%2520and%2520foundational%2520models%2520for%2520ReID%253F%2520We%2520have%2520conducted%2520the%2520analysis%2520across%252011%2520models%2520and%25209%2520datasets.%2520Our%2520results%2520show%2520a%2520clear%2520split%253A%2520supervised%2520models%2520dominate%2520their%2520training%2520domain%2520but%2520crumble%2520on%2520cross-domain%2520data.%2520Language-aligned%2520models%252C%2520however%252C%2520show%2520surprising%2520robustness%2520cross-domain%2520for%2520ReID%2520tasks%252C%2520even%2520though%2520they%2520are%2520not%2520explicitly%2520trained%2520to%2520do%2520so.%2520Code%2520and%2520data%2520available%2520at%253A%2520https%253A//github.com/moiiai-tech/object-reid-benchmark.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20598v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Person%20Re-ID%20in%202025%3A%20Supervised%2C%20Self-Supervised%2C%20and%20Language-Aligned.%20What%20Works%3F&entry.906535625=Lakshman%20Balasubramanian&entry.1292438233=Person%20Re-Identification%20%28ReID%29%20remains%20a%20challenging%20problem%20in%20computer%20vision.%20This%20work%20reviews%20various%20training%20paradigm%20and%20evaluates%20the%20robustness%20of%20state-of-the-art%20ReID%20models%20in%20cross-domain%20applications%20and%20examines%20the%20role%20of%20foundation%20models%20in%20improving%20generalization%20through%20richer%2C%20more%20transferable%20visual%20representations.%20We%20compare%20three%20training%20paradigms%2C%20supervised%2C%20self-supervised%2C%20and%20language-aligned%20models.%20Through%20the%20study%20the%20aim%20is%20to%20answer%20the%20following%20questions%3A%20Can%20supervised%20models%20generalize%20in%20cross-domain%20scenarios%3F%20How%20does%20foundation%20models%20like%20SigLIP2%20perform%20for%20the%20ReID%20tasks%3F%20What%20are%20the%20weaknesses%20of%20current%20supervised%20and%20foundational%20models%20for%20ReID%3F%20We%20have%20conducted%20the%20analysis%20across%2011%20models%20and%209%20datasets.%20Our%20results%20show%20a%20clear%20split%3A%20supervised%20models%20dominate%20their%20training%20domain%20but%20crumble%20on%20cross-domain%20data.%20Language-aligned%20models%2C%20however%2C%20show%20surprising%20robustness%20cross-domain%20for%20ReID%20tasks%2C%20even%20though%20they%20are%20not%20explicitly%20trained%20to%20do%20so.%20Code%20and%20data%20available%20at%3A%20https%3A//github.com/moiiai-tech/object-reid-benchmark.&entry.1838667208=http%3A//arxiv.org/abs/2601.20598v1&entry.124074799=Read"},
{"title": "Tendon-based modelling, estimation and control for a simulated high-DoF anthropomorphic hand model", "author": "P\u00e9ter Polcz and Katalin Sch\u00e4ffer and Mikl\u00f3s Koller", "abstract": "Tendon-driven anthropomorphic robotic hands often lack direct joint angle sensing, as the integration of joint encoders can compromise mechanical compactness and dexterity. This paper presents a computational method for estimating joint positions from measured tendon displacements and tensions. An efficient kinematic modeling framework for anthropomorphic hands is first introduced based on the Denavit-Hartenberg convention. Using a simplified tendon model, a system of nonlinear equations relating tendon states to joint positions is derived and solved via a nonlinear optimization approach. The estimated joint angles are then employed for closed-loop control through a Jacobian-based proportional-integral (PI) controller augmented with a feedforward term, enabling gesture tracking without direct joint sensing. The effectiveness and limitations of the proposed estimation and control framework are demonstrated in the MuJoCo simulation environment using the Anatomically Correct Biomechatronic Hand, featuring five degrees of freedom for each long finger and six degrees of freedom for the thumb.", "link": "http://arxiv.org/abs/2601.20682v1", "date": "2026-01-28", "relevancy": 2.7786, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5765}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5509}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tendon-based%20modelling%2C%20estimation%20and%20control%20for%20a%20simulated%20high-DoF%20anthropomorphic%20hand%20model&body=Title%3A%20Tendon-based%20modelling%2C%20estimation%20and%20control%20for%20a%20simulated%20high-DoF%20anthropomorphic%20hand%20model%0AAuthor%3A%20P%C3%A9ter%20Polcz%20and%20Katalin%20Sch%C3%A4ffer%20and%20Mikl%C3%B3s%20Koller%0AAbstract%3A%20Tendon-driven%20anthropomorphic%20robotic%20hands%20often%20lack%20direct%20joint%20angle%20sensing%2C%20as%20the%20integration%20of%20joint%20encoders%20can%20compromise%20mechanical%20compactness%20and%20dexterity.%20This%20paper%20presents%20a%20computational%20method%20for%20estimating%20joint%20positions%20from%20measured%20tendon%20displacements%20and%20tensions.%20An%20efficient%20kinematic%20modeling%20framework%20for%20anthropomorphic%20hands%20is%20first%20introduced%20based%20on%20the%20Denavit-Hartenberg%20convention.%20Using%20a%20simplified%20tendon%20model%2C%20a%20system%20of%20nonlinear%20equations%20relating%20tendon%20states%20to%20joint%20positions%20is%20derived%20and%20solved%20via%20a%20nonlinear%20optimization%20approach.%20The%20estimated%20joint%20angles%20are%20then%20employed%20for%20closed-loop%20control%20through%20a%20Jacobian-based%20proportional-integral%20%28PI%29%20controller%20augmented%20with%20a%20feedforward%20term%2C%20enabling%20gesture%20tracking%20without%20direct%20joint%20sensing.%20The%20effectiveness%20and%20limitations%20of%20the%20proposed%20estimation%20and%20control%20framework%20are%20demonstrated%20in%20the%20MuJoCo%20simulation%20environment%20using%20the%20Anatomically%20Correct%20Biomechatronic%20Hand%2C%20featuring%20five%20degrees%20of%20freedom%20for%20each%20long%20finger%20and%20six%20degrees%20of%20freedom%20for%20the%20thumb.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20682v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTendon-based%2520modelling%252C%2520estimation%2520and%2520control%2520for%2520a%2520simulated%2520high-DoF%2520anthropomorphic%2520hand%2520model%26entry.906535625%3DP%25C3%25A9ter%2520Polcz%2520and%2520Katalin%2520Sch%25C3%25A4ffer%2520and%2520Mikl%25C3%25B3s%2520Koller%26entry.1292438233%3DTendon-driven%2520anthropomorphic%2520robotic%2520hands%2520often%2520lack%2520direct%2520joint%2520angle%2520sensing%252C%2520as%2520the%2520integration%2520of%2520joint%2520encoders%2520can%2520compromise%2520mechanical%2520compactness%2520and%2520dexterity.%2520This%2520paper%2520presents%2520a%2520computational%2520method%2520for%2520estimating%2520joint%2520positions%2520from%2520measured%2520tendon%2520displacements%2520and%2520tensions.%2520An%2520efficient%2520kinematic%2520modeling%2520framework%2520for%2520anthropomorphic%2520hands%2520is%2520first%2520introduced%2520based%2520on%2520the%2520Denavit-Hartenberg%2520convention.%2520Using%2520a%2520simplified%2520tendon%2520model%252C%2520a%2520system%2520of%2520nonlinear%2520equations%2520relating%2520tendon%2520states%2520to%2520joint%2520positions%2520is%2520derived%2520and%2520solved%2520via%2520a%2520nonlinear%2520optimization%2520approach.%2520The%2520estimated%2520joint%2520angles%2520are%2520then%2520employed%2520for%2520closed-loop%2520control%2520through%2520a%2520Jacobian-based%2520proportional-integral%2520%2528PI%2529%2520controller%2520augmented%2520with%2520a%2520feedforward%2520term%252C%2520enabling%2520gesture%2520tracking%2520without%2520direct%2520joint%2520sensing.%2520The%2520effectiveness%2520and%2520limitations%2520of%2520the%2520proposed%2520estimation%2520and%2520control%2520framework%2520are%2520demonstrated%2520in%2520the%2520MuJoCo%2520simulation%2520environment%2520using%2520the%2520Anatomically%2520Correct%2520Biomechatronic%2520Hand%252C%2520featuring%2520five%2520degrees%2520of%2520freedom%2520for%2520each%2520long%2520finger%2520and%2520six%2520degrees%2520of%2520freedom%2520for%2520the%2520thumb.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20682v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tendon-based%20modelling%2C%20estimation%20and%20control%20for%20a%20simulated%20high-DoF%20anthropomorphic%20hand%20model&entry.906535625=P%C3%A9ter%20Polcz%20and%20Katalin%20Sch%C3%A4ffer%20and%20Mikl%C3%B3s%20Koller&entry.1292438233=Tendon-driven%20anthropomorphic%20robotic%20hands%20often%20lack%20direct%20joint%20angle%20sensing%2C%20as%20the%20integration%20of%20joint%20encoders%20can%20compromise%20mechanical%20compactness%20and%20dexterity.%20This%20paper%20presents%20a%20computational%20method%20for%20estimating%20joint%20positions%20from%20measured%20tendon%20displacements%20and%20tensions.%20An%20efficient%20kinematic%20modeling%20framework%20for%20anthropomorphic%20hands%20is%20first%20introduced%20based%20on%20the%20Denavit-Hartenberg%20convention.%20Using%20a%20simplified%20tendon%20model%2C%20a%20system%20of%20nonlinear%20equations%20relating%20tendon%20states%20to%20joint%20positions%20is%20derived%20and%20solved%20via%20a%20nonlinear%20optimization%20approach.%20The%20estimated%20joint%20angles%20are%20then%20employed%20for%20closed-loop%20control%20through%20a%20Jacobian-based%20proportional-integral%20%28PI%29%20controller%20augmented%20with%20a%20feedforward%20term%2C%20enabling%20gesture%20tracking%20without%20direct%20joint%20sensing.%20The%20effectiveness%20and%20limitations%20of%20the%20proposed%20estimation%20and%20control%20framework%20are%20demonstrated%20in%20the%20MuJoCo%20simulation%20environment%20using%20the%20Anatomically%20Correct%20Biomechatronic%20Hand%2C%20featuring%20five%20degrees%20of%20freedom%20for%20each%20long%20finger%20and%20six%20degrees%20of%20freedom%20for%20the%20thumb.&entry.1838667208=http%3A//arxiv.org/abs/2601.20682v1&entry.124074799=Read"},
{"title": "AnomalyVFM -- Transforming Vision Foundation Models into Zero-Shot Anomaly Detectors", "author": "Matic Fu\u010dka and Vitjan Zavrtanik and Danijel Sko\u010daj", "abstract": "Zero-shot anomaly detection aims to detect and localise abnormal regions in the image without access to any in-domain training images. While recent approaches leverage vision-language models (VLMs), such as CLIP, to transfer high-level concept knowledge, methods based on purely vision foundation models (VFMs), like DINOv2, have lagged behind in performance. We argue that this gap stems from two practical issues: (i) limited diversity in existing auxiliary anomaly detection datasets and (ii) overly shallow VFM adaptation strategies. To address both challenges, we propose AnomalyVFM, a general and effective framework that turns any pretrained VFM into a strong zero-shot anomaly detector. Our approach combines a robust three-stage synthetic dataset generation scheme with a parameter-efficient adaptation mechanism, utilising low-rank feature adapters and a confidence-weighted pixel loss. Together, these components enable modern VFMs to substantially outperform current state-of-the-art methods. More specifically, with RADIO as a backbone, AnomalyVFM achieves an average image-level AUROC of 94.1% across 9 diverse datasets, surpassing previous methods by significant 3.3 percentage points. Project Page: https://maticfuc.github.io/anomaly_vfm/", "link": "http://arxiv.org/abs/2601.20524v1", "date": "2026-01-28", "relevancy": 2.7679, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5548}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5548}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnomalyVFM%20--%20Transforming%20Vision%20Foundation%20Models%20into%20Zero-Shot%20Anomaly%20Detectors&body=Title%3A%20AnomalyVFM%20--%20Transforming%20Vision%20Foundation%20Models%20into%20Zero-Shot%20Anomaly%20Detectors%0AAuthor%3A%20Matic%20Fu%C4%8Dka%20and%20Vitjan%20Zavrtanik%20and%20Danijel%20Sko%C4%8Daj%0AAbstract%3A%20Zero-shot%20anomaly%20detection%20aims%20to%20detect%20and%20localise%20abnormal%20regions%20in%20the%20image%20without%20access%20to%20any%20in-domain%20training%20images.%20While%20recent%20approaches%20leverage%20vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20to%20transfer%20high-level%20concept%20knowledge%2C%20methods%20based%20on%20purely%20vision%20foundation%20models%20%28VFMs%29%2C%20like%20DINOv2%2C%20have%20lagged%20behind%20in%20performance.%20We%20argue%20that%20this%20gap%20stems%20from%20two%20practical%20issues%3A%20%28i%29%20limited%20diversity%20in%20existing%20auxiliary%20anomaly%20detection%20datasets%20and%20%28ii%29%20overly%20shallow%20VFM%20adaptation%20strategies.%20To%20address%20both%20challenges%2C%20we%20propose%20AnomalyVFM%2C%20a%20general%20and%20effective%20framework%20that%20turns%20any%20pretrained%20VFM%20into%20a%20strong%20zero-shot%20anomaly%20detector.%20Our%20approach%20combines%20a%20robust%20three-stage%20synthetic%20dataset%20generation%20scheme%20with%20a%20parameter-efficient%20adaptation%20mechanism%2C%20utilising%20low-rank%20feature%20adapters%20and%20a%20confidence-weighted%20pixel%20loss.%20Together%2C%20these%20components%20enable%20modern%20VFMs%20to%20substantially%20outperform%20current%20state-of-the-art%20methods.%20More%20specifically%2C%20with%20RADIO%20as%20a%20backbone%2C%20AnomalyVFM%20achieves%20an%20average%20image-level%20AUROC%20of%2094.1%25%20across%209%20diverse%20datasets%2C%20surpassing%20previous%20methods%20by%20significant%203.3%20percentage%20points.%20Project%20Page%3A%20https%3A//maticfuc.github.io/anomaly_vfm/%0ALink%3A%20http%3A//arxiv.org/abs/2601.20524v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnomalyVFM%2520--%2520Transforming%2520Vision%2520Foundation%2520Models%2520into%2520Zero-Shot%2520Anomaly%2520Detectors%26entry.906535625%3DMatic%2520Fu%25C4%258Dka%2520and%2520Vitjan%2520Zavrtanik%2520and%2520Danijel%2520Sko%25C4%258Daj%26entry.1292438233%3DZero-shot%2520anomaly%2520detection%2520aims%2520to%2520detect%2520and%2520localise%2520abnormal%2520regions%2520in%2520the%2520image%2520without%2520access%2520to%2520any%2520in-domain%2520training%2520images.%2520While%2520recent%2520approaches%2520leverage%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520such%2520as%2520CLIP%252C%2520to%2520transfer%2520high-level%2520concept%2520knowledge%252C%2520methods%2520based%2520on%2520purely%2520vision%2520foundation%2520models%2520%2528VFMs%2529%252C%2520like%2520DINOv2%252C%2520have%2520lagged%2520behind%2520in%2520performance.%2520We%2520argue%2520that%2520this%2520gap%2520stems%2520from%2520two%2520practical%2520issues%253A%2520%2528i%2529%2520limited%2520diversity%2520in%2520existing%2520auxiliary%2520anomaly%2520detection%2520datasets%2520and%2520%2528ii%2529%2520overly%2520shallow%2520VFM%2520adaptation%2520strategies.%2520To%2520address%2520both%2520challenges%252C%2520we%2520propose%2520AnomalyVFM%252C%2520a%2520general%2520and%2520effective%2520framework%2520that%2520turns%2520any%2520pretrained%2520VFM%2520into%2520a%2520strong%2520zero-shot%2520anomaly%2520detector.%2520Our%2520approach%2520combines%2520a%2520robust%2520three-stage%2520synthetic%2520dataset%2520generation%2520scheme%2520with%2520a%2520parameter-efficient%2520adaptation%2520mechanism%252C%2520utilising%2520low-rank%2520feature%2520adapters%2520and%2520a%2520confidence-weighted%2520pixel%2520loss.%2520Together%252C%2520these%2520components%2520enable%2520modern%2520VFMs%2520to%2520substantially%2520outperform%2520current%2520state-of-the-art%2520methods.%2520More%2520specifically%252C%2520with%2520RADIO%2520as%2520a%2520backbone%252C%2520AnomalyVFM%2520achieves%2520an%2520average%2520image-level%2520AUROC%2520of%252094.1%2525%2520across%25209%2520diverse%2520datasets%252C%2520surpassing%2520previous%2520methods%2520by%2520significant%25203.3%2520percentage%2520points.%2520Project%2520Page%253A%2520https%253A//maticfuc.github.io/anomaly_vfm/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20524v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnomalyVFM%20--%20Transforming%20Vision%20Foundation%20Models%20into%20Zero-Shot%20Anomaly%20Detectors&entry.906535625=Matic%20Fu%C4%8Dka%20and%20Vitjan%20Zavrtanik%20and%20Danijel%20Sko%C4%8Daj&entry.1292438233=Zero-shot%20anomaly%20detection%20aims%20to%20detect%20and%20localise%20abnormal%20regions%20in%20the%20image%20without%20access%20to%20any%20in-domain%20training%20images.%20While%20recent%20approaches%20leverage%20vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20to%20transfer%20high-level%20concept%20knowledge%2C%20methods%20based%20on%20purely%20vision%20foundation%20models%20%28VFMs%29%2C%20like%20DINOv2%2C%20have%20lagged%20behind%20in%20performance.%20We%20argue%20that%20this%20gap%20stems%20from%20two%20practical%20issues%3A%20%28i%29%20limited%20diversity%20in%20existing%20auxiliary%20anomaly%20detection%20datasets%20and%20%28ii%29%20overly%20shallow%20VFM%20adaptation%20strategies.%20To%20address%20both%20challenges%2C%20we%20propose%20AnomalyVFM%2C%20a%20general%20and%20effective%20framework%20that%20turns%20any%20pretrained%20VFM%20into%20a%20strong%20zero-shot%20anomaly%20detector.%20Our%20approach%20combines%20a%20robust%20three-stage%20synthetic%20dataset%20generation%20scheme%20with%20a%20parameter-efficient%20adaptation%20mechanism%2C%20utilising%20low-rank%20feature%20adapters%20and%20a%20confidence-weighted%20pixel%20loss.%20Together%2C%20these%20components%20enable%20modern%20VFMs%20to%20substantially%20outperform%20current%20state-of-the-art%20methods.%20More%20specifically%2C%20with%20RADIO%20as%20a%20backbone%2C%20AnomalyVFM%20achieves%20an%20average%20image-level%20AUROC%20of%2094.1%25%20across%209%20diverse%20datasets%2C%20surpassing%20previous%20methods%20by%20significant%203.3%20percentage%20points.%20Project%20Page%3A%20https%3A//maticfuc.github.io/anomaly_vfm/&entry.1838667208=http%3A//arxiv.org/abs/2601.20524v1&entry.124074799=Read"},
{"title": "S$^3$-Attention:Attention-Aligned Endogenous Retrieval for Memory-Bounded Long-Context Inference", "author": "Qingsen Ma and Dianyun Wang and Yaoye Wang and Lechen Ning and Sujie Zhu and Xiaohang Zhang and Jiaming Lyu and Linhao Ren and Zhenbo Xu and Zhaofeng He", "abstract": "Large language models are increasingly applied to multi-document and long-form inputs, yet long-context inference remains memory- and noise-inefficient. Key-value (KV) caching scales linearly with context length, while external retrieval methods often return lexically similar but causally irrelevant passages.\n  We present S3-Attention, a memory-first inference-time framework that treats long-context processing as attention-aligned endogenous retrieval. S3-Attention decodes transient key and query projections into top-k sparse feature identifiers using lightweight sparse autoencoders, and constructs a CPU-based inverted index mapping features to token positions or spans during a single streaming scan. This design allows the KV cache to be discarded entirely and bounds GPU memory usage by the scan chunk size.\n  At generation time, feature co-activation is used to retrieve compact evidence spans, optionally fused with BM25 for exact lexical matching. Under a unified LongBench evaluation protocol with fixed prompting, decoding, and matched token budgets, S3-Hybrid closely matches full-context inference across multiple model families and improves robustness in several information-dense settings. We also report an engineering limitation of the current prototype, which incurs higher wall-clock latency than optimized full-KV baselines, motivating future kernel-level optimization.", "link": "http://arxiv.org/abs/2601.17702v2", "date": "2026-01-28", "relevancy": 2.7467, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5709}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5709}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5062}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S%24%5E3%24-Attention%3AAttention-Aligned%20Endogenous%20Retrieval%20for%20Memory-Bounded%20Long-Context%20Inference&body=Title%3A%20S%24%5E3%24-Attention%3AAttention-Aligned%20Endogenous%20Retrieval%20for%20Memory-Bounded%20Long-Context%20Inference%0AAuthor%3A%20Qingsen%20Ma%20and%20Dianyun%20Wang%20and%20Yaoye%20Wang%20and%20Lechen%20Ning%20and%20Sujie%20Zhu%20and%20Xiaohang%20Zhang%20and%20Jiaming%20Lyu%20and%20Linhao%20Ren%20and%20Zhenbo%20Xu%20and%20Zhaofeng%20He%0AAbstract%3A%20Large%20language%20models%20are%20increasingly%20applied%20to%20multi-document%20and%20long-form%20inputs%2C%20yet%20long-context%20inference%20remains%20memory-%20and%20noise-inefficient.%20Key-value%20%28KV%29%20caching%20scales%20linearly%20with%20context%20length%2C%20while%20external%20retrieval%20methods%20often%20return%20lexically%20similar%20but%20causally%20irrelevant%20passages.%0A%20%20We%20present%20S3-Attention%2C%20a%20memory-first%20inference-time%20framework%20that%20treats%20long-context%20processing%20as%20attention-aligned%20endogenous%20retrieval.%20S3-Attention%20decodes%20transient%20key%20and%20query%20projections%20into%20top-k%20sparse%20feature%20identifiers%20using%20lightweight%20sparse%20autoencoders%2C%20and%20constructs%20a%20CPU-based%20inverted%20index%20mapping%20features%20to%20token%20positions%20or%20spans%20during%20a%20single%20streaming%20scan.%20This%20design%20allows%20the%20KV%20cache%20to%20be%20discarded%20entirely%20and%20bounds%20GPU%20memory%20usage%20by%20the%20scan%20chunk%20size.%0A%20%20At%20generation%20time%2C%20feature%20co-activation%20is%20used%20to%20retrieve%20compact%20evidence%20spans%2C%20optionally%20fused%20with%20BM25%20for%20exact%20lexical%20matching.%20Under%20a%20unified%20LongBench%20evaluation%20protocol%20with%20fixed%20prompting%2C%20decoding%2C%20and%20matched%20token%20budgets%2C%20S3-Hybrid%20closely%20matches%20full-context%20inference%20across%20multiple%20model%20families%20and%20improves%20robustness%20in%20several%20information-dense%20settings.%20We%20also%20report%20an%20engineering%20limitation%20of%20the%20current%20prototype%2C%20which%20incurs%20higher%20wall-clock%20latency%20than%20optimized%20full-KV%20baselines%2C%20motivating%20future%20kernel-level%20optimization.%0ALink%3A%20http%3A//arxiv.org/abs/2601.17702v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS%2524%255E3%2524-Attention%253AAttention-Aligned%2520Endogenous%2520Retrieval%2520for%2520Memory-Bounded%2520Long-Context%2520Inference%26entry.906535625%3DQingsen%2520Ma%2520and%2520Dianyun%2520Wang%2520and%2520Yaoye%2520Wang%2520and%2520Lechen%2520Ning%2520and%2520Sujie%2520Zhu%2520and%2520Xiaohang%2520Zhang%2520and%2520Jiaming%2520Lyu%2520and%2520Linhao%2520Ren%2520and%2520Zhenbo%2520Xu%2520and%2520Zhaofeng%2520He%26entry.1292438233%3DLarge%2520language%2520models%2520are%2520increasingly%2520applied%2520to%2520multi-document%2520and%2520long-form%2520inputs%252C%2520yet%2520long-context%2520inference%2520remains%2520memory-%2520and%2520noise-inefficient.%2520Key-value%2520%2528KV%2529%2520caching%2520scales%2520linearly%2520with%2520context%2520length%252C%2520while%2520external%2520retrieval%2520methods%2520often%2520return%2520lexically%2520similar%2520but%2520causally%2520irrelevant%2520passages.%250A%2520%2520We%2520present%2520S3-Attention%252C%2520a%2520memory-first%2520inference-time%2520framework%2520that%2520treats%2520long-context%2520processing%2520as%2520attention-aligned%2520endogenous%2520retrieval.%2520S3-Attention%2520decodes%2520transient%2520key%2520and%2520query%2520projections%2520into%2520top-k%2520sparse%2520feature%2520identifiers%2520using%2520lightweight%2520sparse%2520autoencoders%252C%2520and%2520constructs%2520a%2520CPU-based%2520inverted%2520index%2520mapping%2520features%2520to%2520token%2520positions%2520or%2520spans%2520during%2520a%2520single%2520streaming%2520scan.%2520This%2520design%2520allows%2520the%2520KV%2520cache%2520to%2520be%2520discarded%2520entirely%2520and%2520bounds%2520GPU%2520memory%2520usage%2520by%2520the%2520scan%2520chunk%2520size.%250A%2520%2520At%2520generation%2520time%252C%2520feature%2520co-activation%2520is%2520used%2520to%2520retrieve%2520compact%2520evidence%2520spans%252C%2520optionally%2520fused%2520with%2520BM25%2520for%2520exact%2520lexical%2520matching.%2520Under%2520a%2520unified%2520LongBench%2520evaluation%2520protocol%2520with%2520fixed%2520prompting%252C%2520decoding%252C%2520and%2520matched%2520token%2520budgets%252C%2520S3-Hybrid%2520closely%2520matches%2520full-context%2520inference%2520across%2520multiple%2520model%2520families%2520and%2520improves%2520robustness%2520in%2520several%2520information-dense%2520settings.%2520We%2520also%2520report%2520an%2520engineering%2520limitation%2520of%2520the%2520current%2520prototype%252C%2520which%2520incurs%2520higher%2520wall-clock%2520latency%2520than%2520optimized%2520full-KV%2520baselines%252C%2520motivating%2520future%2520kernel-level%2520optimization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.17702v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S%24%5E3%24-Attention%3AAttention-Aligned%20Endogenous%20Retrieval%20for%20Memory-Bounded%20Long-Context%20Inference&entry.906535625=Qingsen%20Ma%20and%20Dianyun%20Wang%20and%20Yaoye%20Wang%20and%20Lechen%20Ning%20and%20Sujie%20Zhu%20and%20Xiaohang%20Zhang%20and%20Jiaming%20Lyu%20and%20Linhao%20Ren%20and%20Zhenbo%20Xu%20and%20Zhaofeng%20He&entry.1292438233=Large%20language%20models%20are%20increasingly%20applied%20to%20multi-document%20and%20long-form%20inputs%2C%20yet%20long-context%20inference%20remains%20memory-%20and%20noise-inefficient.%20Key-value%20%28KV%29%20caching%20scales%20linearly%20with%20context%20length%2C%20while%20external%20retrieval%20methods%20often%20return%20lexically%20similar%20but%20causally%20irrelevant%20passages.%0A%20%20We%20present%20S3-Attention%2C%20a%20memory-first%20inference-time%20framework%20that%20treats%20long-context%20processing%20as%20attention-aligned%20endogenous%20retrieval.%20S3-Attention%20decodes%20transient%20key%20and%20query%20projections%20into%20top-k%20sparse%20feature%20identifiers%20using%20lightweight%20sparse%20autoencoders%2C%20and%20constructs%20a%20CPU-based%20inverted%20index%20mapping%20features%20to%20token%20positions%20or%20spans%20during%20a%20single%20streaming%20scan.%20This%20design%20allows%20the%20KV%20cache%20to%20be%20discarded%20entirely%20and%20bounds%20GPU%20memory%20usage%20by%20the%20scan%20chunk%20size.%0A%20%20At%20generation%20time%2C%20feature%20co-activation%20is%20used%20to%20retrieve%20compact%20evidence%20spans%2C%20optionally%20fused%20with%20BM25%20for%20exact%20lexical%20matching.%20Under%20a%20unified%20LongBench%20evaluation%20protocol%20with%20fixed%20prompting%2C%20decoding%2C%20and%20matched%20token%20budgets%2C%20S3-Hybrid%20closely%20matches%20full-context%20inference%20across%20multiple%20model%20families%20and%20improves%20robustness%20in%20several%20information-dense%20settings.%20We%20also%20report%20an%20engineering%20limitation%20of%20the%20current%20prototype%2C%20which%20incurs%20higher%20wall-clock%20latency%20than%20optimized%20full-KV%20baselines%2C%20motivating%20future%20kernel-level%20optimization.&entry.1838667208=http%3A//arxiv.org/abs/2601.17702v2&entry.124074799=Read"},
{"title": "Exploiting the Final Component of Generator Architectures for AI-Generated Image Detection", "author": "Yanzhu Liu and Xiao Liu and Yuexuan Wang and Mondal Soumik", "abstract": "With the rapid proliferation of powerful image generators, accurate detection of AI-generated images has become essential for maintaining a trustworthy online environment. However, existing deepfake detectors often generalize poorly to images produced by unseen generators. Notably, despite being trained under vastly different paradigms, such as diffusion or autoregressive modeling, many modern image generators share common final architectural components that serve as the last stage for converting intermediate representations into images. Motivated by this insight, we propose to \"contaminate\" real images using the generator's final component and train a detector to distinguish them from the original real images. We further introduce a taxonomy based on generators' final components and categorize 21 widely used generators accordingly, enabling a comprehensive investigation of our method's generalization capability. Using only 100 samples from each of three representative categories, our detector-fine-tuned on the DINOv3 backbone-achieves an average accuracy of 98.83% across 22 testing sets from unseen generators.", "link": "http://arxiv.org/abs/2601.20461v1", "date": "2026-01-28", "relevancy": 2.7207, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.572}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5315}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5289}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20the%20Final%20Component%20of%20Generator%20Architectures%20for%20AI-Generated%20Image%20Detection&body=Title%3A%20Exploiting%20the%20Final%20Component%20of%20Generator%20Architectures%20for%20AI-Generated%20Image%20Detection%0AAuthor%3A%20Yanzhu%20Liu%20and%20Xiao%20Liu%20and%20Yuexuan%20Wang%20and%20Mondal%20Soumik%0AAbstract%3A%20With%20the%20rapid%20proliferation%20of%20powerful%20image%20generators%2C%20accurate%20detection%20of%20AI-generated%20images%20has%20become%20essential%20for%20maintaining%20a%20trustworthy%20online%20environment.%20However%2C%20existing%20deepfake%20detectors%20often%20generalize%20poorly%20to%20images%20produced%20by%20unseen%20generators.%20Notably%2C%20despite%20being%20trained%20under%20vastly%20different%20paradigms%2C%20such%20as%20diffusion%20or%20autoregressive%20modeling%2C%20many%20modern%20image%20generators%20share%20common%20final%20architectural%20components%20that%20serve%20as%20the%20last%20stage%20for%20converting%20intermediate%20representations%20into%20images.%20Motivated%20by%20this%20insight%2C%20we%20propose%20to%20%22contaminate%22%20real%20images%20using%20the%20generator%27s%20final%20component%20and%20train%20a%20detector%20to%20distinguish%20them%20from%20the%20original%20real%20images.%20We%20further%20introduce%20a%20taxonomy%20based%20on%20generators%27%20final%20components%20and%20categorize%2021%20widely%20used%20generators%20accordingly%2C%20enabling%20a%20comprehensive%20investigation%20of%20our%20method%27s%20generalization%20capability.%20Using%20only%20100%20samples%20from%20each%20of%20three%20representative%20categories%2C%20our%20detector-fine-tuned%20on%20the%20DINOv3%20backbone-achieves%20an%20average%20accuracy%20of%2098.83%25%20across%2022%20testing%20sets%20from%20unseen%20generators.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20461v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520the%2520Final%2520Component%2520of%2520Generator%2520Architectures%2520for%2520AI-Generated%2520Image%2520Detection%26entry.906535625%3DYanzhu%2520Liu%2520and%2520Xiao%2520Liu%2520and%2520Yuexuan%2520Wang%2520and%2520Mondal%2520Soumik%26entry.1292438233%3DWith%2520the%2520rapid%2520proliferation%2520of%2520powerful%2520image%2520generators%252C%2520accurate%2520detection%2520of%2520AI-generated%2520images%2520has%2520become%2520essential%2520for%2520maintaining%2520a%2520trustworthy%2520online%2520environment.%2520However%252C%2520existing%2520deepfake%2520detectors%2520often%2520generalize%2520poorly%2520to%2520images%2520produced%2520by%2520unseen%2520generators.%2520Notably%252C%2520despite%2520being%2520trained%2520under%2520vastly%2520different%2520paradigms%252C%2520such%2520as%2520diffusion%2520or%2520autoregressive%2520modeling%252C%2520many%2520modern%2520image%2520generators%2520share%2520common%2520final%2520architectural%2520components%2520that%2520serve%2520as%2520the%2520last%2520stage%2520for%2520converting%2520intermediate%2520representations%2520into%2520images.%2520Motivated%2520by%2520this%2520insight%252C%2520we%2520propose%2520to%2520%2522contaminate%2522%2520real%2520images%2520using%2520the%2520generator%2527s%2520final%2520component%2520and%2520train%2520a%2520detector%2520to%2520distinguish%2520them%2520from%2520the%2520original%2520real%2520images.%2520We%2520further%2520introduce%2520a%2520taxonomy%2520based%2520on%2520generators%2527%2520final%2520components%2520and%2520categorize%252021%2520widely%2520used%2520generators%2520accordingly%252C%2520enabling%2520a%2520comprehensive%2520investigation%2520of%2520our%2520method%2527s%2520generalization%2520capability.%2520Using%2520only%2520100%2520samples%2520from%2520each%2520of%2520three%2520representative%2520categories%252C%2520our%2520detector-fine-tuned%2520on%2520the%2520DINOv3%2520backbone-achieves%2520an%2520average%2520accuracy%2520of%252098.83%2525%2520across%252022%2520testing%2520sets%2520from%2520unseen%2520generators.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20461v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20the%20Final%20Component%20of%20Generator%20Architectures%20for%20AI-Generated%20Image%20Detection&entry.906535625=Yanzhu%20Liu%20and%20Xiao%20Liu%20and%20Yuexuan%20Wang%20and%20Mondal%20Soumik&entry.1292438233=With%20the%20rapid%20proliferation%20of%20powerful%20image%20generators%2C%20accurate%20detection%20of%20AI-generated%20images%20has%20become%20essential%20for%20maintaining%20a%20trustworthy%20online%20environment.%20However%2C%20existing%20deepfake%20detectors%20often%20generalize%20poorly%20to%20images%20produced%20by%20unseen%20generators.%20Notably%2C%20despite%20being%20trained%20under%20vastly%20different%20paradigms%2C%20such%20as%20diffusion%20or%20autoregressive%20modeling%2C%20many%20modern%20image%20generators%20share%20common%20final%20architectural%20components%20that%20serve%20as%20the%20last%20stage%20for%20converting%20intermediate%20representations%20into%20images.%20Motivated%20by%20this%20insight%2C%20we%20propose%20to%20%22contaminate%22%20real%20images%20using%20the%20generator%27s%20final%20component%20and%20train%20a%20detector%20to%20distinguish%20them%20from%20the%20original%20real%20images.%20We%20further%20introduce%20a%20taxonomy%20based%20on%20generators%27%20final%20components%20and%20categorize%2021%20widely%20used%20generators%20accordingly%2C%20enabling%20a%20comprehensive%20investigation%20of%20our%20method%27s%20generalization%20capability.%20Using%20only%20100%20samples%20from%20each%20of%20three%20representative%20categories%2C%20our%20detector-fine-tuned%20on%20the%20DINOv3%20backbone-achieves%20an%20average%20accuracy%20of%2098.83%25%20across%2022%20testing%20sets%20from%20unseen%20generators.&entry.1838667208=http%3A//arxiv.org/abs/2601.20461v1&entry.124074799=Read"},
{"title": "Robust MAE-Driven NAS: From Mask Reconstruction to Architecture Innovation", "author": "Yiming Hu and Xiangxiang Chu and Yong Wang", "abstract": "Neural Architecture Search (NAS) relies heavily on labeled data, which is labor-intensive and time-consuming to obtain. In this paper, we propose a novel NAS method based on an unsupervised paradigm, specifically Masked Autoencoders (MAE), thereby eliminating the need for labeled data. By replacing the supervised learning objective with an image reconstruction task, our approach enables the efficient discovery of network architectures without compromising performance and generalization ability. Additionally, we address the problem of performance collapse encountered in the widely-used Differentiable Architecture Search (DARTS) in the unsupervised setting by designing a hierarchical decoder. Extensive experiments across various datasets demonstrate the effectiveness and robustness of our method, offering empirical evidence of its superiority over the counterparts.", "link": "http://arxiv.org/abs/2311.12086v3", "date": "2026-01-28", "relevancy": 2.6761, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5502}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5379}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20MAE-Driven%20NAS%3A%20From%20Mask%20Reconstruction%20to%20Architecture%20Innovation&body=Title%3A%20Robust%20MAE-Driven%20NAS%3A%20From%20Mask%20Reconstruction%20to%20Architecture%20Innovation%0AAuthor%3A%20Yiming%20Hu%20and%20Xiangxiang%20Chu%20and%20Yong%20Wang%0AAbstract%3A%20Neural%20Architecture%20Search%20%28NAS%29%20relies%20heavily%20on%20labeled%20data%2C%20which%20is%20labor-intensive%20and%20time-consuming%20to%20obtain.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20NAS%20method%20based%20on%20an%20unsupervised%20paradigm%2C%20specifically%20Masked%20Autoencoders%20%28MAE%29%2C%20thereby%20eliminating%20the%20need%20for%20labeled%20data.%20By%20replacing%20the%20supervised%20learning%20objective%20with%20an%20image%20reconstruction%20task%2C%20our%20approach%20enables%20the%20efficient%20discovery%20of%20network%20architectures%20without%20compromising%20performance%20and%20generalization%20ability.%20Additionally%2C%20we%20address%20the%20problem%20of%20performance%20collapse%20encountered%20in%20the%20widely-used%20Differentiable%20Architecture%20Search%20%28DARTS%29%20in%20the%20unsupervised%20setting%20by%20designing%20a%20hierarchical%20decoder.%20Extensive%20experiments%20across%20various%20datasets%20demonstrate%20the%20effectiveness%20and%20robustness%20of%20our%20method%2C%20offering%20empirical%20evidence%20of%20its%20superiority%20over%20the%20counterparts.%0ALink%3A%20http%3A//arxiv.org/abs/2311.12086v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520MAE-Driven%2520NAS%253A%2520From%2520Mask%2520Reconstruction%2520to%2520Architecture%2520Innovation%26entry.906535625%3DYiming%2520Hu%2520and%2520Xiangxiang%2520Chu%2520and%2520Yong%2520Wang%26entry.1292438233%3DNeural%2520Architecture%2520Search%2520%2528NAS%2529%2520relies%2520heavily%2520on%2520labeled%2520data%252C%2520which%2520is%2520labor-intensive%2520and%2520time-consuming%2520to%2520obtain.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520NAS%2520method%2520based%2520on%2520an%2520unsupervised%2520paradigm%252C%2520specifically%2520Masked%2520Autoencoders%2520%2528MAE%2529%252C%2520thereby%2520eliminating%2520the%2520need%2520for%2520labeled%2520data.%2520By%2520replacing%2520the%2520supervised%2520learning%2520objective%2520with%2520an%2520image%2520reconstruction%2520task%252C%2520our%2520approach%2520enables%2520the%2520efficient%2520discovery%2520of%2520network%2520architectures%2520without%2520compromising%2520performance%2520and%2520generalization%2520ability.%2520Additionally%252C%2520we%2520address%2520the%2520problem%2520of%2520performance%2520collapse%2520encountered%2520in%2520the%2520widely-used%2520Differentiable%2520Architecture%2520Search%2520%2528DARTS%2529%2520in%2520the%2520unsupervised%2520setting%2520by%2520designing%2520a%2520hierarchical%2520decoder.%2520Extensive%2520experiments%2520across%2520various%2520datasets%2520demonstrate%2520the%2520effectiveness%2520and%2520robustness%2520of%2520our%2520method%252C%2520offering%2520empirical%2520evidence%2520of%2520its%2520superiority%2520over%2520the%2520counterparts.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.12086v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20MAE-Driven%20NAS%3A%20From%20Mask%20Reconstruction%20to%20Architecture%20Innovation&entry.906535625=Yiming%20Hu%20and%20Xiangxiang%20Chu%20and%20Yong%20Wang&entry.1292438233=Neural%20Architecture%20Search%20%28NAS%29%20relies%20heavily%20on%20labeled%20data%2C%20which%20is%20labor-intensive%20and%20time-consuming%20to%20obtain.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20NAS%20method%20based%20on%20an%20unsupervised%20paradigm%2C%20specifically%20Masked%20Autoencoders%20%28MAE%29%2C%20thereby%20eliminating%20the%20need%20for%20labeled%20data.%20By%20replacing%20the%20supervised%20learning%20objective%20with%20an%20image%20reconstruction%20task%2C%20our%20approach%20enables%20the%20efficient%20discovery%20of%20network%20architectures%20without%20compromising%20performance%20and%20generalization%20ability.%20Additionally%2C%20we%20address%20the%20problem%20of%20performance%20collapse%20encountered%20in%20the%20widely-used%20Differentiable%20Architecture%20Search%20%28DARTS%29%20in%20the%20unsupervised%20setting%20by%20designing%20a%20hierarchical%20decoder.%20Extensive%20experiments%20across%20various%20datasets%20demonstrate%20the%20effectiveness%20and%20robustness%20of%20our%20method%2C%20offering%20empirical%20evidence%20of%20its%20superiority%20over%20the%20counterparts.&entry.1838667208=http%3A//arxiv.org/abs/2311.12086v3&entry.124074799=Read"},
{"title": "TRIM: Token-wise Attention-Derived Saliency for Data-Efficient Instruction Tuning", "author": "Manish Nagaraj and Sakshi Choudhary and Utkarsh Saxena and Deepak Ravikumar and Kaushik Roy", "abstract": "Instruction tuning is essential for aligning large language models (LLMs) to downstream tasks and commonly relies on large, diverse corpora. However, small, high-quality subsets, known as coresets, can deliver comparable or superior results, though curating them remains challenging. Existing methods often rely on coarse, sample-level signals like gradients, an approach that is computationally expensive and overlooks fine-grained features. To address this, we introduce TRIM (Token Relevance via Interpretable Multi-layer Attention), a forward-only, token-centric framework. Instead of using gradients, TRIM operates by matching underlying representational patterns identified via attention-based \"fingerprints\" from a handful of target samples. Such an approach makes TRIM highly efficient and uniquely sensitive to the structural features that define a task. Coresets selected by our method consistently outperform state-of-the-art baselines by up to 9% on downstream tasks and even surpass the performance of full-data fine-tuning in some settings. By avoiding expensive backward passes, TRIM achieves this at a fraction of the computational cost. These findings establish TRIM as a scalable and efficient alternative for building high-quality instruction-tuning datasets.", "link": "http://arxiv.org/abs/2510.07118v2", "date": "2026-01-28", "relevancy": 2.6608, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5543}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5367}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRIM%3A%20Token-wise%20Attention-Derived%20Saliency%20for%20Data-Efficient%20Instruction%20Tuning&body=Title%3A%20TRIM%3A%20Token-wise%20Attention-Derived%20Saliency%20for%20Data-Efficient%20Instruction%20Tuning%0AAuthor%3A%20Manish%20Nagaraj%20and%20Sakshi%20Choudhary%20and%20Utkarsh%20Saxena%20and%20Deepak%20Ravikumar%20and%20Kaushik%20Roy%0AAbstract%3A%20Instruction%20tuning%20is%20essential%20for%20aligning%20large%20language%20models%20%28LLMs%29%20to%20downstream%20tasks%20and%20commonly%20relies%20on%20large%2C%20diverse%20corpora.%20However%2C%20small%2C%20high-quality%20subsets%2C%20known%20as%20coresets%2C%20can%20deliver%20comparable%20or%20superior%20results%2C%20though%20curating%20them%20remains%20challenging.%20Existing%20methods%20often%20rely%20on%20coarse%2C%20sample-level%20signals%20like%20gradients%2C%20an%20approach%20that%20is%20computationally%20expensive%20and%20overlooks%20fine-grained%20features.%20To%20address%20this%2C%20we%20introduce%20TRIM%20%28Token%20Relevance%20via%20Interpretable%20Multi-layer%20Attention%29%2C%20a%20forward-only%2C%20token-centric%20framework.%20Instead%20of%20using%20gradients%2C%20TRIM%20operates%20by%20matching%20underlying%20representational%20patterns%20identified%20via%20attention-based%20%22fingerprints%22%20from%20a%20handful%20of%20target%20samples.%20Such%20an%20approach%20makes%20TRIM%20highly%20efficient%20and%20uniquely%20sensitive%20to%20the%20structural%20features%20that%20define%20a%20task.%20Coresets%20selected%20by%20our%20method%20consistently%20outperform%20state-of-the-art%20baselines%20by%20up%20to%209%25%20on%20downstream%20tasks%20and%20even%20surpass%20the%20performance%20of%20full-data%20fine-tuning%20in%20some%20settings.%20By%20avoiding%20expensive%20backward%20passes%2C%20TRIM%20achieves%20this%20at%20a%20fraction%20of%20the%20computational%20cost.%20These%20findings%20establish%20TRIM%20as%20a%20scalable%20and%20efficient%20alternative%20for%20building%20high-quality%20instruction-tuning%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2510.07118v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRIM%253A%2520Token-wise%2520Attention-Derived%2520Saliency%2520for%2520Data-Efficient%2520Instruction%2520Tuning%26entry.906535625%3DManish%2520Nagaraj%2520and%2520Sakshi%2520Choudhary%2520and%2520Utkarsh%2520Saxena%2520and%2520Deepak%2520Ravikumar%2520and%2520Kaushik%2520Roy%26entry.1292438233%3DInstruction%2520tuning%2520is%2520essential%2520for%2520aligning%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520downstream%2520tasks%2520and%2520commonly%2520relies%2520on%2520large%252C%2520diverse%2520corpora.%2520However%252C%2520small%252C%2520high-quality%2520subsets%252C%2520known%2520as%2520coresets%252C%2520can%2520deliver%2520comparable%2520or%2520superior%2520results%252C%2520though%2520curating%2520them%2520remains%2520challenging.%2520Existing%2520methods%2520often%2520rely%2520on%2520coarse%252C%2520sample-level%2520signals%2520like%2520gradients%252C%2520an%2520approach%2520that%2520is%2520computationally%2520expensive%2520and%2520overlooks%2520fine-grained%2520features.%2520To%2520address%2520this%252C%2520we%2520introduce%2520TRIM%2520%2528Token%2520Relevance%2520via%2520Interpretable%2520Multi-layer%2520Attention%2529%252C%2520a%2520forward-only%252C%2520token-centric%2520framework.%2520Instead%2520of%2520using%2520gradients%252C%2520TRIM%2520operates%2520by%2520matching%2520underlying%2520representational%2520patterns%2520identified%2520via%2520attention-based%2520%2522fingerprints%2522%2520from%2520a%2520handful%2520of%2520target%2520samples.%2520Such%2520an%2520approach%2520makes%2520TRIM%2520highly%2520efficient%2520and%2520uniquely%2520sensitive%2520to%2520the%2520structural%2520features%2520that%2520define%2520a%2520task.%2520Coresets%2520selected%2520by%2520our%2520method%2520consistently%2520outperform%2520state-of-the-art%2520baselines%2520by%2520up%2520to%25209%2525%2520on%2520downstream%2520tasks%2520and%2520even%2520surpass%2520the%2520performance%2520of%2520full-data%2520fine-tuning%2520in%2520some%2520settings.%2520By%2520avoiding%2520expensive%2520backward%2520passes%252C%2520TRIM%2520achieves%2520this%2520at%2520a%2520fraction%2520of%2520the%2520computational%2520cost.%2520These%2520findings%2520establish%2520TRIM%2520as%2520a%2520scalable%2520and%2520efficient%2520alternative%2520for%2520building%2520high-quality%2520instruction-tuning%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07118v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRIM%3A%20Token-wise%20Attention-Derived%20Saliency%20for%20Data-Efficient%20Instruction%20Tuning&entry.906535625=Manish%20Nagaraj%20and%20Sakshi%20Choudhary%20and%20Utkarsh%20Saxena%20and%20Deepak%20Ravikumar%20and%20Kaushik%20Roy&entry.1292438233=Instruction%20tuning%20is%20essential%20for%20aligning%20large%20language%20models%20%28LLMs%29%20to%20downstream%20tasks%20and%20commonly%20relies%20on%20large%2C%20diverse%20corpora.%20However%2C%20small%2C%20high-quality%20subsets%2C%20known%20as%20coresets%2C%20can%20deliver%20comparable%20or%20superior%20results%2C%20though%20curating%20them%20remains%20challenging.%20Existing%20methods%20often%20rely%20on%20coarse%2C%20sample-level%20signals%20like%20gradients%2C%20an%20approach%20that%20is%20computationally%20expensive%20and%20overlooks%20fine-grained%20features.%20To%20address%20this%2C%20we%20introduce%20TRIM%20%28Token%20Relevance%20via%20Interpretable%20Multi-layer%20Attention%29%2C%20a%20forward-only%2C%20token-centric%20framework.%20Instead%20of%20using%20gradients%2C%20TRIM%20operates%20by%20matching%20underlying%20representational%20patterns%20identified%20via%20attention-based%20%22fingerprints%22%20from%20a%20handful%20of%20target%20samples.%20Such%20an%20approach%20makes%20TRIM%20highly%20efficient%20and%20uniquely%20sensitive%20to%20the%20structural%20features%20that%20define%20a%20task.%20Coresets%20selected%20by%20our%20method%20consistently%20outperform%20state-of-the-art%20baselines%20by%20up%20to%209%25%20on%20downstream%20tasks%20and%20even%20surpass%20the%20performance%20of%20full-data%20fine-tuning%20in%20some%20settings.%20By%20avoiding%20expensive%20backward%20passes%2C%20TRIM%20achieves%20this%20at%20a%20fraction%20of%20the%20computational%20cost.%20These%20findings%20establish%20TRIM%20as%20a%20scalable%20and%20efficient%20alternative%20for%20building%20high-quality%20instruction-tuning%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2510.07118v2&entry.124074799=Read"},
{"title": "Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving", "author": "Ziang Guo and Feng Yang and Xuefeng Zhang and Jiaqi Guo and Kun Zhao and Yixiao Zhou and Peng Lu and Zufeng Zhang and Sifa Zheng", "abstract": "Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\\%$ and the collision rate by $74.4\\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user's speech.", "link": "http://arxiv.org/abs/2601.12142v2", "date": "2026-01-28", "relevancy": 2.6587, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5347}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5303}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Listen%2C%20Look%2C%20Drive%3A%20Coupling%20Audio%20Instructions%20for%20User-aware%20VLA-based%20Autonomous%20Driving&body=Title%3A%20Listen%2C%20Look%2C%20Drive%3A%20Coupling%20Audio%20Instructions%20for%20User-aware%20VLA-based%20Autonomous%20Driving%0AAuthor%3A%20Ziang%20Guo%20and%20Feng%20Yang%20and%20Xuefeng%20Zhang%20and%20Jiaqi%20Guo%20and%20Kun%20Zhao%20and%20Yixiao%20Zhou%20and%20Peng%20Lu%20and%20Zufeng%20Zhang%20and%20Sifa%20Zheng%0AAbstract%3A%20Vision%20Language%20Action%20%28VLA%29%20models%20promise%20an%20open-vocabulary%20interface%20that%20can%20translate%20perceptual%20ambiguity%20into%20semantically%20grounded%20driving%20decisions%2C%20yet%20they%20still%20treat%20language%20as%20a%20static%20prior%20fixed%20at%20inference%20time.%20As%20a%20result%2C%20the%20model%20must%20infer%20continuously%20shifting%20objectives%20from%20pixels%20alone%2C%20yielding%20delayed%20or%20overly%20conservative%20maneuvers.%20We%20argue%20that%20effective%20VLAs%20for%20autonomous%20driving%20need%20an%20online%20channel%20in%20which%20users%20can%20influence%20driving%20with%20specific%20intentions.%20To%20this%20end%2C%20we%20present%20EchoVLA%2C%20a%20user-aware%20VLA%20that%20couples%20camera%20streams%20with%20in%20situ%20audio%20instructions.%20We%20augment%20the%20nuScenes%20dataset%20with%20temporally%20aligned%2C%20intent-specific%20speech%20commands%20generated%20by%20converting%20ego-motion%20descriptions%20into%20synthetic%20audios.%20Further%2C%20we%20compose%20emotional%20speech-trajectory%20pairs%20into%20a%20multimodal%20Chain-of-Thought%20%28CoT%29%20for%20fine-tuning%20a%20Multimodal%20Large%20Model%20%28MLM%29%20based%20on%20Qwen2.5-Omni.%20Specifically%2C%20we%20synthesize%20the%20audio-augmented%20dataset%20with%20different%20emotion%20types%20paired%20with%20corresponding%20driving%20behaviors%2C%20leveraging%20the%20emotional%20cues%20embedded%20in%20tone%2C%20pitch%2C%20and%20speech%20tempo%20to%20reflect%20varying%20user%20states%2C%20such%20as%20urgent%20or%20hesitant%20intentions%2C%20thus%20enabling%20our%20EchoVLA%20to%20interpret%20not%20only%20the%20semantic%20content%20but%20also%20the%20emotional%20context%20of%20audio%20commands%20for%20more%20nuanced%20and%20emotionally%20adaptive%20driving%20behavior.%20In%20open-loop%20benchmarks%2C%20our%20approach%20reduces%20the%20average%20L2%20error%20by%20%2459.4%5C%25%24%20and%20the%20collision%20rate%20by%20%2474.4%5C%25%24%20compared%20to%20the%20baseline%20of%20vision-only%20perception.%20More%20experiments%20on%20nuScenes%20dataset%20validate%20that%20EchoVLA%20not%20only%20steers%20the%20trajectory%20through%20audio%20instructions%2C%20but%20also%20modulates%20driving%20behavior%20in%20response%20to%20the%20emotions%20detected%20in%20the%20user%27s%20speech.%0ALink%3A%20http%3A//arxiv.org/abs/2601.12142v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DListen%252C%2520Look%252C%2520Drive%253A%2520Coupling%2520Audio%2520Instructions%2520for%2520User-aware%2520VLA-based%2520Autonomous%2520Driving%26entry.906535625%3DZiang%2520Guo%2520and%2520Feng%2520Yang%2520and%2520Xuefeng%2520Zhang%2520and%2520Jiaqi%2520Guo%2520and%2520Kun%2520Zhao%2520and%2520Yixiao%2520Zhou%2520and%2520Peng%2520Lu%2520and%2520Zufeng%2520Zhang%2520and%2520Sifa%2520Zheng%26entry.1292438233%3DVision%2520Language%2520Action%2520%2528VLA%2529%2520models%2520promise%2520an%2520open-vocabulary%2520interface%2520that%2520can%2520translate%2520perceptual%2520ambiguity%2520into%2520semantically%2520grounded%2520driving%2520decisions%252C%2520yet%2520they%2520still%2520treat%2520language%2520as%2520a%2520static%2520prior%2520fixed%2520at%2520inference%2520time.%2520As%2520a%2520result%252C%2520the%2520model%2520must%2520infer%2520continuously%2520shifting%2520objectives%2520from%2520pixels%2520alone%252C%2520yielding%2520delayed%2520or%2520overly%2520conservative%2520maneuvers.%2520We%2520argue%2520that%2520effective%2520VLAs%2520for%2520autonomous%2520driving%2520need%2520an%2520online%2520channel%2520in%2520which%2520users%2520can%2520influence%2520driving%2520with%2520specific%2520intentions.%2520To%2520this%2520end%252C%2520we%2520present%2520EchoVLA%252C%2520a%2520user-aware%2520VLA%2520that%2520couples%2520camera%2520streams%2520with%2520in%2520situ%2520audio%2520instructions.%2520We%2520augment%2520the%2520nuScenes%2520dataset%2520with%2520temporally%2520aligned%252C%2520intent-specific%2520speech%2520commands%2520generated%2520by%2520converting%2520ego-motion%2520descriptions%2520into%2520synthetic%2520audios.%2520Further%252C%2520we%2520compose%2520emotional%2520speech-trajectory%2520pairs%2520into%2520a%2520multimodal%2520Chain-of-Thought%2520%2528CoT%2529%2520for%2520fine-tuning%2520a%2520Multimodal%2520Large%2520Model%2520%2528MLM%2529%2520based%2520on%2520Qwen2.5-Omni.%2520Specifically%252C%2520we%2520synthesize%2520the%2520audio-augmented%2520dataset%2520with%2520different%2520emotion%2520types%2520paired%2520with%2520corresponding%2520driving%2520behaviors%252C%2520leveraging%2520the%2520emotional%2520cues%2520embedded%2520in%2520tone%252C%2520pitch%252C%2520and%2520speech%2520tempo%2520to%2520reflect%2520varying%2520user%2520states%252C%2520such%2520as%2520urgent%2520or%2520hesitant%2520intentions%252C%2520thus%2520enabling%2520our%2520EchoVLA%2520to%2520interpret%2520not%2520only%2520the%2520semantic%2520content%2520but%2520also%2520the%2520emotional%2520context%2520of%2520audio%2520commands%2520for%2520more%2520nuanced%2520and%2520emotionally%2520adaptive%2520driving%2520behavior.%2520In%2520open-loop%2520benchmarks%252C%2520our%2520approach%2520reduces%2520the%2520average%2520L2%2520error%2520by%2520%252459.4%255C%2525%2524%2520and%2520the%2520collision%2520rate%2520by%2520%252474.4%255C%2525%2524%2520compared%2520to%2520the%2520baseline%2520of%2520vision-only%2520perception.%2520More%2520experiments%2520on%2520nuScenes%2520dataset%2520validate%2520that%2520EchoVLA%2520not%2520only%2520steers%2520the%2520trajectory%2520through%2520audio%2520instructions%252C%2520but%2520also%2520modulates%2520driving%2520behavior%2520in%2520response%2520to%2520the%2520emotions%2520detected%2520in%2520the%2520user%2527s%2520speech.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.12142v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Listen%2C%20Look%2C%20Drive%3A%20Coupling%20Audio%20Instructions%20for%20User-aware%20VLA-based%20Autonomous%20Driving&entry.906535625=Ziang%20Guo%20and%20Feng%20Yang%20and%20Xuefeng%20Zhang%20and%20Jiaqi%20Guo%20and%20Kun%20Zhao%20and%20Yixiao%20Zhou%20and%20Peng%20Lu%20and%20Zufeng%20Zhang%20and%20Sifa%20Zheng&entry.1292438233=Vision%20Language%20Action%20%28VLA%29%20models%20promise%20an%20open-vocabulary%20interface%20that%20can%20translate%20perceptual%20ambiguity%20into%20semantically%20grounded%20driving%20decisions%2C%20yet%20they%20still%20treat%20language%20as%20a%20static%20prior%20fixed%20at%20inference%20time.%20As%20a%20result%2C%20the%20model%20must%20infer%20continuously%20shifting%20objectives%20from%20pixels%20alone%2C%20yielding%20delayed%20or%20overly%20conservative%20maneuvers.%20We%20argue%20that%20effective%20VLAs%20for%20autonomous%20driving%20need%20an%20online%20channel%20in%20which%20users%20can%20influence%20driving%20with%20specific%20intentions.%20To%20this%20end%2C%20we%20present%20EchoVLA%2C%20a%20user-aware%20VLA%20that%20couples%20camera%20streams%20with%20in%20situ%20audio%20instructions.%20We%20augment%20the%20nuScenes%20dataset%20with%20temporally%20aligned%2C%20intent-specific%20speech%20commands%20generated%20by%20converting%20ego-motion%20descriptions%20into%20synthetic%20audios.%20Further%2C%20we%20compose%20emotional%20speech-trajectory%20pairs%20into%20a%20multimodal%20Chain-of-Thought%20%28CoT%29%20for%20fine-tuning%20a%20Multimodal%20Large%20Model%20%28MLM%29%20based%20on%20Qwen2.5-Omni.%20Specifically%2C%20we%20synthesize%20the%20audio-augmented%20dataset%20with%20different%20emotion%20types%20paired%20with%20corresponding%20driving%20behaviors%2C%20leveraging%20the%20emotional%20cues%20embedded%20in%20tone%2C%20pitch%2C%20and%20speech%20tempo%20to%20reflect%20varying%20user%20states%2C%20such%20as%20urgent%20or%20hesitant%20intentions%2C%20thus%20enabling%20our%20EchoVLA%20to%20interpret%20not%20only%20the%20semantic%20content%20but%20also%20the%20emotional%20context%20of%20audio%20commands%20for%20more%20nuanced%20and%20emotionally%20adaptive%20driving%20behavior.%20In%20open-loop%20benchmarks%2C%20our%20approach%20reduces%20the%20average%20L2%20error%20by%20%2459.4%5C%25%24%20and%20the%20collision%20rate%20by%20%2474.4%5C%25%24%20compared%20to%20the%20baseline%20of%20vision-only%20perception.%20More%20experiments%20on%20nuScenes%20dataset%20validate%20that%20EchoVLA%20not%20only%20steers%20the%20trajectory%20through%20audio%20instructions%2C%20but%20also%20modulates%20driving%20behavior%20in%20response%20to%20the%20emotions%20detected%20in%20the%20user%27s%20speech.&entry.1838667208=http%3A//arxiv.org/abs/2601.12142v2&entry.124074799=Read"},
{"title": "GeoDiff3D: Self-Supervised 3D Scene Generation with Geometry-Constrained 2D Diffusion Guidance", "author": "Haozhi Zhu and Miaomiao Zhao and Dingyao Liu and Runze Tian and Yan Zhang and Jie Guo and Fenggen Yu", "abstract": "3D scene generation is a core technology for gaming, film/VFX, and VR/AR. Growing demand for rapid iteration, high-fidelity detail, and accessible content creation has further increased interest in this area. Existing methods broadly follow two paradigms - indirect 2D-to-3D reconstruction and direct 3D generation - but both are limited by weak structural modeling and heavy reliance on large-scale ground-truth supervision, often producing structural artifacts, geometric inconsistencies, and degraded high-frequency details in complex scenes. We propose GeoDiff3D, an efficient self-supervised framework that uses coarse geometry as a structural anchor and a geometry-constrained 2D diffusion model to provide texture-rich reference images. Importantly, GeoDiff3D does not require strict multi-view consistency of the diffusion-generated references and remains robust to the resulting noisy, inconsistent guidance. We further introduce voxel-aligned 3D feature aggregation and dual self-supervision to maintain scene coherence and fine details while substantially reducing dependence on labeled data. GeoDiff3D also trains with low computational cost and enables fast, high-quality 3D scene generation. Extensive experiments on challenging scenes show improved generalization and generation quality over existing baselines, offering a practical solution for accessible and efficient 3D scene construction.", "link": "http://arxiv.org/abs/2601.19785v2", "date": "2026-01-28", "relevancy": 2.6494, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6684}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6611}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoDiff3D%3A%20Self-Supervised%203D%20Scene%20Generation%20with%20Geometry-Constrained%202D%20Diffusion%20Guidance&body=Title%3A%20GeoDiff3D%3A%20Self-Supervised%203D%20Scene%20Generation%20with%20Geometry-Constrained%202D%20Diffusion%20Guidance%0AAuthor%3A%20Haozhi%20Zhu%20and%20Miaomiao%20Zhao%20and%20Dingyao%20Liu%20and%20Runze%20Tian%20and%20Yan%20Zhang%20and%20Jie%20Guo%20and%20Fenggen%20Yu%0AAbstract%3A%203D%20scene%20generation%20is%20a%20core%20technology%20for%20gaming%2C%20film/VFX%2C%20and%20VR/AR.%20Growing%20demand%20for%20rapid%20iteration%2C%20high-fidelity%20detail%2C%20and%20accessible%20content%20creation%20has%20further%20increased%20interest%20in%20this%20area.%20Existing%20methods%20broadly%20follow%20two%20paradigms%20-%20indirect%202D-to-3D%20reconstruction%20and%20direct%203D%20generation%20-%20but%20both%20are%20limited%20by%20weak%20structural%20modeling%20and%20heavy%20reliance%20on%20large-scale%20ground-truth%20supervision%2C%20often%20producing%20structural%20artifacts%2C%20geometric%20inconsistencies%2C%20and%20degraded%20high-frequency%20details%20in%20complex%20scenes.%20We%20propose%20GeoDiff3D%2C%20an%20efficient%20self-supervised%20framework%20that%20uses%20coarse%20geometry%20as%20a%20structural%20anchor%20and%20a%20geometry-constrained%202D%20diffusion%20model%20to%20provide%20texture-rich%20reference%20images.%20Importantly%2C%20GeoDiff3D%20does%20not%20require%20strict%20multi-view%20consistency%20of%20the%20diffusion-generated%20references%20and%20remains%20robust%20to%20the%20resulting%20noisy%2C%20inconsistent%20guidance.%20We%20further%20introduce%20voxel-aligned%203D%20feature%20aggregation%20and%20dual%20self-supervision%20to%20maintain%20scene%20coherence%20and%20fine%20details%20while%20substantially%20reducing%20dependence%20on%20labeled%20data.%20GeoDiff3D%20also%20trains%20with%20low%20computational%20cost%20and%20enables%20fast%2C%20high-quality%203D%20scene%20generation.%20Extensive%20experiments%20on%20challenging%20scenes%20show%20improved%20generalization%20and%20generation%20quality%20over%20existing%20baselines%2C%20offering%20a%20practical%20solution%20for%20accessible%20and%20efficient%203D%20scene%20construction.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19785v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoDiff3D%253A%2520Self-Supervised%25203D%2520Scene%2520Generation%2520with%2520Geometry-Constrained%25202D%2520Diffusion%2520Guidance%26entry.906535625%3DHaozhi%2520Zhu%2520and%2520Miaomiao%2520Zhao%2520and%2520Dingyao%2520Liu%2520and%2520Runze%2520Tian%2520and%2520Yan%2520Zhang%2520and%2520Jie%2520Guo%2520and%2520Fenggen%2520Yu%26entry.1292438233%3D3D%2520scene%2520generation%2520is%2520a%2520core%2520technology%2520for%2520gaming%252C%2520film/VFX%252C%2520and%2520VR/AR.%2520Growing%2520demand%2520for%2520rapid%2520iteration%252C%2520high-fidelity%2520detail%252C%2520and%2520accessible%2520content%2520creation%2520has%2520further%2520increased%2520interest%2520in%2520this%2520area.%2520Existing%2520methods%2520broadly%2520follow%2520two%2520paradigms%2520-%2520indirect%25202D-to-3D%2520reconstruction%2520and%2520direct%25203D%2520generation%2520-%2520but%2520both%2520are%2520limited%2520by%2520weak%2520structural%2520modeling%2520and%2520heavy%2520reliance%2520on%2520large-scale%2520ground-truth%2520supervision%252C%2520often%2520producing%2520structural%2520artifacts%252C%2520geometric%2520inconsistencies%252C%2520and%2520degraded%2520high-frequency%2520details%2520in%2520complex%2520scenes.%2520We%2520propose%2520GeoDiff3D%252C%2520an%2520efficient%2520self-supervised%2520framework%2520that%2520uses%2520coarse%2520geometry%2520as%2520a%2520structural%2520anchor%2520and%2520a%2520geometry-constrained%25202D%2520diffusion%2520model%2520to%2520provide%2520texture-rich%2520reference%2520images.%2520Importantly%252C%2520GeoDiff3D%2520does%2520not%2520require%2520strict%2520multi-view%2520consistency%2520of%2520the%2520diffusion-generated%2520references%2520and%2520remains%2520robust%2520to%2520the%2520resulting%2520noisy%252C%2520inconsistent%2520guidance.%2520We%2520further%2520introduce%2520voxel-aligned%25203D%2520feature%2520aggregation%2520and%2520dual%2520self-supervision%2520to%2520maintain%2520scene%2520coherence%2520and%2520fine%2520details%2520while%2520substantially%2520reducing%2520dependence%2520on%2520labeled%2520data.%2520GeoDiff3D%2520also%2520trains%2520with%2520low%2520computational%2520cost%2520and%2520enables%2520fast%252C%2520high-quality%25203D%2520scene%2520generation.%2520Extensive%2520experiments%2520on%2520challenging%2520scenes%2520show%2520improved%2520generalization%2520and%2520generation%2520quality%2520over%2520existing%2520baselines%252C%2520offering%2520a%2520practical%2520solution%2520for%2520accessible%2520and%2520efficient%25203D%2520scene%2520construction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19785v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoDiff3D%3A%20Self-Supervised%203D%20Scene%20Generation%20with%20Geometry-Constrained%202D%20Diffusion%20Guidance&entry.906535625=Haozhi%20Zhu%20and%20Miaomiao%20Zhao%20and%20Dingyao%20Liu%20and%20Runze%20Tian%20and%20Yan%20Zhang%20and%20Jie%20Guo%20and%20Fenggen%20Yu&entry.1292438233=3D%20scene%20generation%20is%20a%20core%20technology%20for%20gaming%2C%20film/VFX%2C%20and%20VR/AR.%20Growing%20demand%20for%20rapid%20iteration%2C%20high-fidelity%20detail%2C%20and%20accessible%20content%20creation%20has%20further%20increased%20interest%20in%20this%20area.%20Existing%20methods%20broadly%20follow%20two%20paradigms%20-%20indirect%202D-to-3D%20reconstruction%20and%20direct%203D%20generation%20-%20but%20both%20are%20limited%20by%20weak%20structural%20modeling%20and%20heavy%20reliance%20on%20large-scale%20ground-truth%20supervision%2C%20often%20producing%20structural%20artifacts%2C%20geometric%20inconsistencies%2C%20and%20degraded%20high-frequency%20details%20in%20complex%20scenes.%20We%20propose%20GeoDiff3D%2C%20an%20efficient%20self-supervised%20framework%20that%20uses%20coarse%20geometry%20as%20a%20structural%20anchor%20and%20a%20geometry-constrained%202D%20diffusion%20model%20to%20provide%20texture-rich%20reference%20images.%20Importantly%2C%20GeoDiff3D%20does%20not%20require%20strict%20multi-view%20consistency%20of%20the%20diffusion-generated%20references%20and%20remains%20robust%20to%20the%20resulting%20noisy%2C%20inconsistent%20guidance.%20We%20further%20introduce%20voxel-aligned%203D%20feature%20aggregation%20and%20dual%20self-supervision%20to%20maintain%20scene%20coherence%20and%20fine%20details%20while%20substantially%20reducing%20dependence%20on%20labeled%20data.%20GeoDiff3D%20also%20trains%20with%20low%20computational%20cost%20and%20enables%20fast%2C%20high-quality%203D%20scene%20generation.%20Extensive%20experiments%20on%20challenging%20scenes%20show%20improved%20generalization%20and%20generation%20quality%20over%20existing%20baselines%2C%20offering%20a%20practical%20solution%20for%20accessible%20and%20efficient%203D%20scene%20construction.&entry.1838667208=http%3A//arxiv.org/abs/2601.19785v2&entry.124074799=Read"},
{"title": "LVLMs and Humans Ground Differently in Referential Communication", "author": "Peter Zeng and Weiling Li and Amie Paige and Zhengxiang Wang and Panagiotis Kaliosis and Dimitris Samaras and Gregory Zelinsky and Susan Brennan and Owen Rambow", "abstract": "For generative AI agents to partner effectively with human users, the ability to accurately predict human intent is critical. But this ability to collaborate remains limited by a critical deficit: an inability to model common ground. Here, we present a referential communication experiment with a factorial design involving director-matcher pairs (human-human, human-AI, AI-human, and AI-AI) that interact with multiple turns in repeated rounds to match pictures of objects not associated with any obvious lexicalized labels. We release the online pipeline for data collection, the tools and analyses for accuracy, efficiency, and lexical overlap, and a corpus of 356 dialogues (89 pairs over 4 rounds each) that unmasks LVLMs' limitations in interactively resolving referring expressions, a crucial skill that underlies human language use.", "link": "http://arxiv.org/abs/2601.19792v2", "date": "2026-01-28", "relevancy": 2.6001, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5342}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5342}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LVLMs%20and%20Humans%20Ground%20Differently%20in%20Referential%20Communication&body=Title%3A%20LVLMs%20and%20Humans%20Ground%20Differently%20in%20Referential%20Communication%0AAuthor%3A%20Peter%20Zeng%20and%20Weiling%20Li%20and%20Amie%20Paige%20and%20Zhengxiang%20Wang%20and%20Panagiotis%20Kaliosis%20and%20Dimitris%20Samaras%20and%20Gregory%20Zelinsky%20and%20Susan%20Brennan%20and%20Owen%20Rambow%0AAbstract%3A%20For%20generative%20AI%20agents%20to%20partner%20effectively%20with%20human%20users%2C%20the%20ability%20to%20accurately%20predict%20human%20intent%20is%20critical.%20But%20this%20ability%20to%20collaborate%20remains%20limited%20by%20a%20critical%20deficit%3A%20an%20inability%20to%20model%20common%20ground.%20Here%2C%20we%20present%20a%20referential%20communication%20experiment%20with%20a%20factorial%20design%20involving%20director-matcher%20pairs%20%28human-human%2C%20human-AI%2C%20AI-human%2C%20and%20AI-AI%29%20that%20interact%20with%20multiple%20turns%20in%20repeated%20rounds%20to%20match%20pictures%20of%20objects%20not%20associated%20with%20any%20obvious%20lexicalized%20labels.%20We%20release%20the%20online%20pipeline%20for%20data%20collection%2C%20the%20tools%20and%20analyses%20for%20accuracy%2C%20efficiency%2C%20and%20lexical%20overlap%2C%20and%20a%20corpus%20of%20356%20dialogues%20%2889%20pairs%20over%204%20rounds%20each%29%20that%20unmasks%20LVLMs%27%20limitations%20in%20interactively%20resolving%20referring%20expressions%2C%20a%20crucial%20skill%20that%20underlies%20human%20language%20use.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19792v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLVLMs%2520and%2520Humans%2520Ground%2520Differently%2520in%2520Referential%2520Communication%26entry.906535625%3DPeter%2520Zeng%2520and%2520Weiling%2520Li%2520and%2520Amie%2520Paige%2520and%2520Zhengxiang%2520Wang%2520and%2520Panagiotis%2520Kaliosis%2520and%2520Dimitris%2520Samaras%2520and%2520Gregory%2520Zelinsky%2520and%2520Susan%2520Brennan%2520and%2520Owen%2520Rambow%26entry.1292438233%3DFor%2520generative%2520AI%2520agents%2520to%2520partner%2520effectively%2520with%2520human%2520users%252C%2520the%2520ability%2520to%2520accurately%2520predict%2520human%2520intent%2520is%2520critical.%2520But%2520this%2520ability%2520to%2520collaborate%2520remains%2520limited%2520by%2520a%2520critical%2520deficit%253A%2520an%2520inability%2520to%2520model%2520common%2520ground.%2520Here%252C%2520we%2520present%2520a%2520referential%2520communication%2520experiment%2520with%2520a%2520factorial%2520design%2520involving%2520director-matcher%2520pairs%2520%2528human-human%252C%2520human-AI%252C%2520AI-human%252C%2520and%2520AI-AI%2529%2520that%2520interact%2520with%2520multiple%2520turns%2520in%2520repeated%2520rounds%2520to%2520match%2520pictures%2520of%2520objects%2520not%2520associated%2520with%2520any%2520obvious%2520lexicalized%2520labels.%2520We%2520release%2520the%2520online%2520pipeline%2520for%2520data%2520collection%252C%2520the%2520tools%2520and%2520analyses%2520for%2520accuracy%252C%2520efficiency%252C%2520and%2520lexical%2520overlap%252C%2520and%2520a%2520corpus%2520of%2520356%2520dialogues%2520%252889%2520pairs%2520over%25204%2520rounds%2520each%2529%2520that%2520unmasks%2520LVLMs%2527%2520limitations%2520in%2520interactively%2520resolving%2520referring%2520expressions%252C%2520a%2520crucial%2520skill%2520that%2520underlies%2520human%2520language%2520use.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19792v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LVLMs%20and%20Humans%20Ground%20Differently%20in%20Referential%20Communication&entry.906535625=Peter%20Zeng%20and%20Weiling%20Li%20and%20Amie%20Paige%20and%20Zhengxiang%20Wang%20and%20Panagiotis%20Kaliosis%20and%20Dimitris%20Samaras%20and%20Gregory%20Zelinsky%20and%20Susan%20Brennan%20and%20Owen%20Rambow&entry.1292438233=For%20generative%20AI%20agents%20to%20partner%20effectively%20with%20human%20users%2C%20the%20ability%20to%20accurately%20predict%20human%20intent%20is%20critical.%20But%20this%20ability%20to%20collaborate%20remains%20limited%20by%20a%20critical%20deficit%3A%20an%20inability%20to%20model%20common%20ground.%20Here%2C%20we%20present%20a%20referential%20communication%20experiment%20with%20a%20factorial%20design%20involving%20director-matcher%20pairs%20%28human-human%2C%20human-AI%2C%20AI-human%2C%20and%20AI-AI%29%20that%20interact%20with%20multiple%20turns%20in%20repeated%20rounds%20to%20match%20pictures%20of%20objects%20not%20associated%20with%20any%20obvious%20lexicalized%20labels.%20We%20release%20the%20online%20pipeline%20for%20data%20collection%2C%20the%20tools%20and%20analyses%20for%20accuracy%2C%20efficiency%2C%20and%20lexical%20overlap%2C%20and%20a%20corpus%20of%20356%20dialogues%20%2889%20pairs%20over%204%20rounds%20each%29%20that%20unmasks%20LVLMs%27%20limitations%20in%20interactively%20resolving%20referring%20expressions%2C%20a%20crucial%20skill%20that%20underlies%20human%20language%20use.&entry.1838667208=http%3A//arxiv.org/abs/2601.19792v2&entry.124074799=Read"},
{"title": "MuRAL-CPD: Active Learning for Multiresolution Change Point Detection", "author": "Stefano Bertolasi and Diego Carrera and Diego Stucchi and Pasqualina Fragneto and Luigi Amedeo Bianchi", "abstract": "Change Point Detection (CPD) is a critical task in time series analysis, aiming to identify moments when the underlying data-generating process shifts. Traditional CPD methods often rely on unsupervised techniques, which lack adaptability to task-specific definitions of change and cannot benefit from user knowledge. To address these limitations, we propose MuRAL-CPD, a novel semi-supervised method that integrates active learning into a multiresolution CPD algorithm. MuRAL-CPD leverages a wavelet-based multiresolution decomposition to detect changes across multiple temporal scales and incorporates user feedback to iteratively optimize key hyperparameters. This interaction enables the model to align its notion of change with that of the user, improving both accuracy and interpretability. Our experimental results on several real-world datasets show the effectiveness of MuRAL-CPD against state-of-the-art methods, particularly in scenarios where minimal supervision is available.", "link": "http://arxiv.org/abs/2601.20686v1", "date": "2026-01-28", "relevancy": 2.5722, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5194}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5135}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MuRAL-CPD%3A%20Active%20Learning%20for%20Multiresolution%20Change%20Point%20Detection&body=Title%3A%20MuRAL-CPD%3A%20Active%20Learning%20for%20Multiresolution%20Change%20Point%20Detection%0AAuthor%3A%20Stefano%20Bertolasi%20and%20Diego%20Carrera%20and%20Diego%20Stucchi%20and%20Pasqualina%20Fragneto%20and%20Luigi%20Amedeo%20Bianchi%0AAbstract%3A%20Change%20Point%20Detection%20%28CPD%29%20is%20a%20critical%20task%20in%20time%20series%20analysis%2C%20aiming%20to%20identify%20moments%20when%20the%20underlying%20data-generating%20process%20shifts.%20Traditional%20CPD%20methods%20often%20rely%20on%20unsupervised%20techniques%2C%20which%20lack%20adaptability%20to%20task-specific%20definitions%20of%20change%20and%20cannot%20benefit%20from%20user%20knowledge.%20To%20address%20these%20limitations%2C%20we%20propose%20MuRAL-CPD%2C%20a%20novel%20semi-supervised%20method%20that%20integrates%20active%20learning%20into%20a%20multiresolution%20CPD%20algorithm.%20MuRAL-CPD%20leverages%20a%20wavelet-based%20multiresolution%20decomposition%20to%20detect%20changes%20across%20multiple%20temporal%20scales%20and%20incorporates%20user%20feedback%20to%20iteratively%20optimize%20key%20hyperparameters.%20This%20interaction%20enables%20the%20model%20to%20align%20its%20notion%20of%20change%20with%20that%20of%20the%20user%2C%20improving%20both%20accuracy%20and%20interpretability.%20Our%20experimental%20results%20on%20several%20real-world%20datasets%20show%20the%20effectiveness%20of%20MuRAL-CPD%20against%20state-of-the-art%20methods%2C%20particularly%20in%20scenarios%20where%20minimal%20supervision%20is%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20686v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMuRAL-CPD%253A%2520Active%2520Learning%2520for%2520Multiresolution%2520Change%2520Point%2520Detection%26entry.906535625%3DStefano%2520Bertolasi%2520and%2520Diego%2520Carrera%2520and%2520Diego%2520Stucchi%2520and%2520Pasqualina%2520Fragneto%2520and%2520Luigi%2520Amedeo%2520Bianchi%26entry.1292438233%3DChange%2520Point%2520Detection%2520%2528CPD%2529%2520is%2520a%2520critical%2520task%2520in%2520time%2520series%2520analysis%252C%2520aiming%2520to%2520identify%2520moments%2520when%2520the%2520underlying%2520data-generating%2520process%2520shifts.%2520Traditional%2520CPD%2520methods%2520often%2520rely%2520on%2520unsupervised%2520techniques%252C%2520which%2520lack%2520adaptability%2520to%2520task-specific%2520definitions%2520of%2520change%2520and%2520cannot%2520benefit%2520from%2520user%2520knowledge.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520MuRAL-CPD%252C%2520a%2520novel%2520semi-supervised%2520method%2520that%2520integrates%2520active%2520learning%2520into%2520a%2520multiresolution%2520CPD%2520algorithm.%2520MuRAL-CPD%2520leverages%2520a%2520wavelet-based%2520multiresolution%2520decomposition%2520to%2520detect%2520changes%2520across%2520multiple%2520temporal%2520scales%2520and%2520incorporates%2520user%2520feedback%2520to%2520iteratively%2520optimize%2520key%2520hyperparameters.%2520This%2520interaction%2520enables%2520the%2520model%2520to%2520align%2520its%2520notion%2520of%2520change%2520with%2520that%2520of%2520the%2520user%252C%2520improving%2520both%2520accuracy%2520and%2520interpretability.%2520Our%2520experimental%2520results%2520on%2520several%2520real-world%2520datasets%2520show%2520the%2520effectiveness%2520of%2520MuRAL-CPD%2520against%2520state-of-the-art%2520methods%252C%2520particularly%2520in%2520scenarios%2520where%2520minimal%2520supervision%2520is%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20686v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MuRAL-CPD%3A%20Active%20Learning%20for%20Multiresolution%20Change%20Point%20Detection&entry.906535625=Stefano%20Bertolasi%20and%20Diego%20Carrera%20and%20Diego%20Stucchi%20and%20Pasqualina%20Fragneto%20and%20Luigi%20Amedeo%20Bianchi&entry.1292438233=Change%20Point%20Detection%20%28CPD%29%20is%20a%20critical%20task%20in%20time%20series%20analysis%2C%20aiming%20to%20identify%20moments%20when%20the%20underlying%20data-generating%20process%20shifts.%20Traditional%20CPD%20methods%20often%20rely%20on%20unsupervised%20techniques%2C%20which%20lack%20adaptability%20to%20task-specific%20definitions%20of%20change%20and%20cannot%20benefit%20from%20user%20knowledge.%20To%20address%20these%20limitations%2C%20we%20propose%20MuRAL-CPD%2C%20a%20novel%20semi-supervised%20method%20that%20integrates%20active%20learning%20into%20a%20multiresolution%20CPD%20algorithm.%20MuRAL-CPD%20leverages%20a%20wavelet-based%20multiresolution%20decomposition%20to%20detect%20changes%20across%20multiple%20temporal%20scales%20and%20incorporates%20user%20feedback%20to%20iteratively%20optimize%20key%20hyperparameters.%20This%20interaction%20enables%20the%20model%20to%20align%20its%20notion%20of%20change%20with%20that%20of%20the%20user%2C%20improving%20both%20accuracy%20and%20interpretability.%20Our%20experimental%20results%20on%20several%20real-world%20datasets%20show%20the%20effectiveness%20of%20MuRAL-CPD%20against%20state-of-the-art%20methods%2C%20particularly%20in%20scenarios%20where%20minimal%20supervision%20is%20available.&entry.1838667208=http%3A//arxiv.org/abs/2601.20686v1&entry.124074799=Read"},
{"title": "Understanding Post-Training Structural Changes in Large Language Models", "author": "Xinyu He and Xianghui Cao", "abstract": "Post-training fundamentally alters the behavior of large language models (LLMs), yet its impact on the internal parameter space remains poorly understood. In this work, we conduct a systematic singular value decomposition (SVD) analysis of principal linear layers in pretrained LLMs, focusing on two widely adopted post-training methods: instruction tuning and long-chain-of-thought (Long-CoT) distillation. Our analysis reveals two unexpected and robust structural changes: (1) a near-uniform geometric scaling of singular values across layers; and (2) highly consistent orthogonal transformations are applied to the left and right singular vectors of each matrix. Based on these findings, We propose a simple yet effective framework to describe the coordinated dynamics of parameters in LLMs, which elucidates why post-training inherently relies on the foundational capabilities developed during pre-training. Further experiments demonstrate that singular value scaling underpins the temperature-controlled regulatory mechanisms of post-training, while the coordinated rotation of singular vectors encodes the essential semantic alignment. These results challenge the prevailing view of the parameter space in large models as a black box, uncovering the first clear regularities in how parameters evolve during training, and providing a new perspective for deeper investigation into model parameter changes.", "link": "http://arxiv.org/abs/2509.17866v3", "date": "2026-01-28", "relevancy": 2.5453, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5145}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5145}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4982}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Post-Training%20Structural%20Changes%20in%20Large%20Language%20Models&body=Title%3A%20Understanding%20Post-Training%20Structural%20Changes%20in%20Large%20Language%20Models%0AAuthor%3A%20Xinyu%20He%20and%20Xianghui%20Cao%0AAbstract%3A%20Post-training%20fundamentally%20alters%20the%20behavior%20of%20large%20language%20models%20%28LLMs%29%2C%20yet%20its%20impact%20on%20the%20internal%20parameter%20space%20remains%20poorly%20understood.%20In%20this%20work%2C%20we%20conduct%20a%20systematic%20singular%20value%20decomposition%20%28SVD%29%20analysis%20of%20principal%20linear%20layers%20in%20pretrained%20LLMs%2C%20focusing%20on%20two%20widely%20adopted%20post-training%20methods%3A%20instruction%20tuning%20and%20long-chain-of-thought%20%28Long-CoT%29%20distillation.%20Our%20analysis%20reveals%20two%20unexpected%20and%20robust%20structural%20changes%3A%20%281%29%20a%20near-uniform%20geometric%20scaling%20of%20singular%20values%20across%20layers%3B%20and%20%282%29%20highly%20consistent%20orthogonal%20transformations%20are%20applied%20to%20the%20left%20and%20right%20singular%20vectors%20of%20each%20matrix.%20Based%20on%20these%20findings%2C%20We%20propose%20a%20simple%20yet%20effective%20framework%20to%20describe%20the%20coordinated%20dynamics%20of%20parameters%20in%20LLMs%2C%20which%20elucidates%20why%20post-training%20inherently%20relies%20on%20the%20foundational%20capabilities%20developed%20during%20pre-training.%20Further%20experiments%20demonstrate%20that%20singular%20value%20scaling%20underpins%20the%20temperature-controlled%20regulatory%20mechanisms%20of%20post-training%2C%20while%20the%20coordinated%20rotation%20of%20singular%20vectors%20encodes%20the%20essential%20semantic%20alignment.%20These%20results%20challenge%20the%20prevailing%20view%20of%20the%20parameter%20space%20in%20large%20models%20as%20a%20black%20box%2C%20uncovering%20the%20first%20clear%20regularities%20in%20how%20parameters%20evolve%20during%20training%2C%20and%20providing%20a%20new%20perspective%20for%20deeper%20investigation%20into%20model%20parameter%20changes.%0ALink%3A%20http%3A//arxiv.org/abs/2509.17866v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Post-Training%2520Structural%2520Changes%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DXinyu%2520He%2520and%2520Xianghui%2520Cao%26entry.1292438233%3DPost-training%2520fundamentally%2520alters%2520the%2520behavior%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520yet%2520its%2520impact%2520on%2520the%2520internal%2520parameter%2520space%2520remains%2520poorly%2520understood.%2520In%2520this%2520work%252C%2520we%2520conduct%2520a%2520systematic%2520singular%2520value%2520decomposition%2520%2528SVD%2529%2520analysis%2520of%2520principal%2520linear%2520layers%2520in%2520pretrained%2520LLMs%252C%2520focusing%2520on%2520two%2520widely%2520adopted%2520post-training%2520methods%253A%2520instruction%2520tuning%2520and%2520long-chain-of-thought%2520%2528Long-CoT%2529%2520distillation.%2520Our%2520analysis%2520reveals%2520two%2520unexpected%2520and%2520robust%2520structural%2520changes%253A%2520%25281%2529%2520a%2520near-uniform%2520geometric%2520scaling%2520of%2520singular%2520values%2520across%2520layers%253B%2520and%2520%25282%2529%2520highly%2520consistent%2520orthogonal%2520transformations%2520are%2520applied%2520to%2520the%2520left%2520and%2520right%2520singular%2520vectors%2520of%2520each%2520matrix.%2520Based%2520on%2520these%2520findings%252C%2520We%2520propose%2520a%2520simple%2520yet%2520effective%2520framework%2520to%2520describe%2520the%2520coordinated%2520dynamics%2520of%2520parameters%2520in%2520LLMs%252C%2520which%2520elucidates%2520why%2520post-training%2520inherently%2520relies%2520on%2520the%2520foundational%2520capabilities%2520developed%2520during%2520pre-training.%2520Further%2520experiments%2520demonstrate%2520that%2520singular%2520value%2520scaling%2520underpins%2520the%2520temperature-controlled%2520regulatory%2520mechanisms%2520of%2520post-training%252C%2520while%2520the%2520coordinated%2520rotation%2520of%2520singular%2520vectors%2520encodes%2520the%2520essential%2520semantic%2520alignment.%2520These%2520results%2520challenge%2520the%2520prevailing%2520view%2520of%2520the%2520parameter%2520space%2520in%2520large%2520models%2520as%2520a%2520black%2520box%252C%2520uncovering%2520the%2520first%2520clear%2520regularities%2520in%2520how%2520parameters%2520evolve%2520during%2520training%252C%2520and%2520providing%2520a%2520new%2520perspective%2520for%2520deeper%2520investigation%2520into%2520model%2520parameter%2520changes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.17866v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Post-Training%20Structural%20Changes%20in%20Large%20Language%20Models&entry.906535625=Xinyu%20He%20and%20Xianghui%20Cao&entry.1292438233=Post-training%20fundamentally%20alters%20the%20behavior%20of%20large%20language%20models%20%28LLMs%29%2C%20yet%20its%20impact%20on%20the%20internal%20parameter%20space%20remains%20poorly%20understood.%20In%20this%20work%2C%20we%20conduct%20a%20systematic%20singular%20value%20decomposition%20%28SVD%29%20analysis%20of%20principal%20linear%20layers%20in%20pretrained%20LLMs%2C%20focusing%20on%20two%20widely%20adopted%20post-training%20methods%3A%20instruction%20tuning%20and%20long-chain-of-thought%20%28Long-CoT%29%20distillation.%20Our%20analysis%20reveals%20two%20unexpected%20and%20robust%20structural%20changes%3A%20%281%29%20a%20near-uniform%20geometric%20scaling%20of%20singular%20values%20across%20layers%3B%20and%20%282%29%20highly%20consistent%20orthogonal%20transformations%20are%20applied%20to%20the%20left%20and%20right%20singular%20vectors%20of%20each%20matrix.%20Based%20on%20these%20findings%2C%20We%20propose%20a%20simple%20yet%20effective%20framework%20to%20describe%20the%20coordinated%20dynamics%20of%20parameters%20in%20LLMs%2C%20which%20elucidates%20why%20post-training%20inherently%20relies%20on%20the%20foundational%20capabilities%20developed%20during%20pre-training.%20Further%20experiments%20demonstrate%20that%20singular%20value%20scaling%20underpins%20the%20temperature-controlled%20regulatory%20mechanisms%20of%20post-training%2C%20while%20the%20coordinated%20rotation%20of%20singular%20vectors%20encodes%20the%20essential%20semantic%20alignment.%20These%20results%20challenge%20the%20prevailing%20view%20of%20the%20parameter%20space%20in%20large%20models%20as%20a%20black%20box%2C%20uncovering%20the%20first%20clear%20regularities%20in%20how%20parameters%20evolve%20during%20training%2C%20and%20providing%20a%20new%20perspective%20for%20deeper%20investigation%20into%20model%20parameter%20changes.&entry.1838667208=http%3A//arxiv.org/abs/2509.17866v3&entry.124074799=Read"},
{"title": "Weakly supervised framework for wildlife detection and counting in challenging Arctic environments: a case study on caribou (Rangifer tarandus)", "author": "Ghazaleh Serati and Samuel Foucher and Jerome Theau", "abstract": "Caribou across the Arctic has declined in recent decades, motivating scalable and accurate monitoring approaches to guide evidence-based conservation actions and policy decisions. Manual interpretation from this imagery is labor-intensive and error-prone, underscoring the need for automatic and reliable detection across varying scenes. Yet, such automatic detection is challenging due to severe background heterogeneity, dominant empty terrain (class imbalance), small or occluded targets, and wide variation in density and scale. To make the detection model (HerdNet) more robust to these challenges, a weakly supervised patch-level pretraining based on a detection network's architecture is proposed. The detection dataset includes five caribou herds distributed across Alaska. By learning from empty vs. non-empty labels in this dataset, the approach produces early weakly supervised knowledge for enhanced detection compared to HerdNet, which is initialized from generic weights. Accordingly, the patch-based pretrain network attained high accuracy on multi-herd imagery (2017) and on an independent year's (2019) test sets (F1: 93.7%/92.6%, respectively), enabling reliable mapping of regions containing animals to facilitate manual counting on large aerial imagery. Transferred to detection, initialization from weakly supervised pretraining yielded consistent gains over ImageNet weights on both positive patches (F1: 92.6%/93.5% vs. 89.3%/88.6%), and full-image counting (F1: 95.5%/93.3% vs. 91.5%/90.4%). Remaining limitations are false positives from animal-like background clutter and false negatives related to low animal density occlusions. Overall, pretraining on coarse labels prior to detection makes it possible to rely on weakly-supervised pretrained weights even when labeled data are limited, achieving results comparable to generic-weight initialization.", "link": "http://arxiv.org/abs/2601.18891v2", "date": "2026-01-28", "relevancy": 2.5397, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5587}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4834}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4817}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weakly%20supervised%20framework%20for%20wildlife%20detection%20and%20counting%20in%20challenging%20Arctic%20environments%3A%20a%20case%20study%20on%20caribou%20%28Rangifer%20tarandus%29&body=Title%3A%20Weakly%20supervised%20framework%20for%20wildlife%20detection%20and%20counting%20in%20challenging%20Arctic%20environments%3A%20a%20case%20study%20on%20caribou%20%28Rangifer%20tarandus%29%0AAuthor%3A%20Ghazaleh%20Serati%20and%20Samuel%20Foucher%20and%20Jerome%20Theau%0AAbstract%3A%20Caribou%20across%20the%20Arctic%20has%20declined%20in%20recent%20decades%2C%20motivating%20scalable%20and%20accurate%20monitoring%20approaches%20to%20guide%20evidence-based%20conservation%20actions%20and%20policy%20decisions.%20Manual%20interpretation%20from%20this%20imagery%20is%20labor-intensive%20and%20error-prone%2C%20underscoring%20the%20need%20for%20automatic%20and%20reliable%20detection%20across%20varying%20scenes.%20Yet%2C%20such%20automatic%20detection%20is%20challenging%20due%20to%20severe%20background%20heterogeneity%2C%20dominant%20empty%20terrain%20%28class%20imbalance%29%2C%20small%20or%20occluded%20targets%2C%20and%20wide%20variation%20in%20density%20and%20scale.%20To%20make%20the%20detection%20model%20%28HerdNet%29%20more%20robust%20to%20these%20challenges%2C%20a%20weakly%20supervised%20patch-level%20pretraining%20based%20on%20a%20detection%20network%27s%20architecture%20is%20proposed.%20The%20detection%20dataset%20includes%20five%20caribou%20herds%20distributed%20across%20Alaska.%20By%20learning%20from%20empty%20vs.%20non-empty%20labels%20in%20this%20dataset%2C%20the%20approach%20produces%20early%20weakly%20supervised%20knowledge%20for%20enhanced%20detection%20compared%20to%20HerdNet%2C%20which%20is%20initialized%20from%20generic%20weights.%20Accordingly%2C%20the%20patch-based%20pretrain%20network%20attained%20high%20accuracy%20on%20multi-herd%20imagery%20%282017%29%20and%20on%20an%20independent%20year%27s%20%282019%29%20test%20sets%20%28F1%3A%2093.7%25/92.6%25%2C%20respectively%29%2C%20enabling%20reliable%20mapping%20of%20regions%20containing%20animals%20to%20facilitate%20manual%20counting%20on%20large%20aerial%20imagery.%20Transferred%20to%20detection%2C%20initialization%20from%20weakly%20supervised%20pretraining%20yielded%20consistent%20gains%20over%20ImageNet%20weights%20on%20both%20positive%20patches%20%28F1%3A%2092.6%25/93.5%25%20vs.%2089.3%25/88.6%25%29%2C%20and%20full-image%20counting%20%28F1%3A%2095.5%25/93.3%25%20vs.%2091.5%25/90.4%25%29.%20Remaining%20limitations%20are%20false%20positives%20from%20animal-like%20background%20clutter%20and%20false%20negatives%20related%20to%20low%20animal%20density%20occlusions.%20Overall%2C%20pretraining%20on%20coarse%20labels%20prior%20to%20detection%20makes%20it%20possible%20to%20rely%20on%20weakly-supervised%20pretrained%20weights%20even%20when%20labeled%20data%20are%20limited%2C%20achieving%20results%20comparable%20to%20generic-weight%20initialization.%0ALink%3A%20http%3A//arxiv.org/abs/2601.18891v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeakly%2520supervised%2520framework%2520for%2520wildlife%2520detection%2520and%2520counting%2520in%2520challenging%2520Arctic%2520environments%253A%2520a%2520case%2520study%2520on%2520caribou%2520%2528Rangifer%2520tarandus%2529%26entry.906535625%3DGhazaleh%2520Serati%2520and%2520Samuel%2520Foucher%2520and%2520Jerome%2520Theau%26entry.1292438233%3DCaribou%2520across%2520the%2520Arctic%2520has%2520declined%2520in%2520recent%2520decades%252C%2520motivating%2520scalable%2520and%2520accurate%2520monitoring%2520approaches%2520to%2520guide%2520evidence-based%2520conservation%2520actions%2520and%2520policy%2520decisions.%2520Manual%2520interpretation%2520from%2520this%2520imagery%2520is%2520labor-intensive%2520and%2520error-prone%252C%2520underscoring%2520the%2520need%2520for%2520automatic%2520and%2520reliable%2520detection%2520across%2520varying%2520scenes.%2520Yet%252C%2520such%2520automatic%2520detection%2520is%2520challenging%2520due%2520to%2520severe%2520background%2520heterogeneity%252C%2520dominant%2520empty%2520terrain%2520%2528class%2520imbalance%2529%252C%2520small%2520or%2520occluded%2520targets%252C%2520and%2520wide%2520variation%2520in%2520density%2520and%2520scale.%2520To%2520make%2520the%2520detection%2520model%2520%2528HerdNet%2529%2520more%2520robust%2520to%2520these%2520challenges%252C%2520a%2520weakly%2520supervised%2520patch-level%2520pretraining%2520based%2520on%2520a%2520detection%2520network%2527s%2520architecture%2520is%2520proposed.%2520The%2520detection%2520dataset%2520includes%2520five%2520caribou%2520herds%2520distributed%2520across%2520Alaska.%2520By%2520learning%2520from%2520empty%2520vs.%2520non-empty%2520labels%2520in%2520this%2520dataset%252C%2520the%2520approach%2520produces%2520early%2520weakly%2520supervised%2520knowledge%2520for%2520enhanced%2520detection%2520compared%2520to%2520HerdNet%252C%2520which%2520is%2520initialized%2520from%2520generic%2520weights.%2520Accordingly%252C%2520the%2520patch-based%2520pretrain%2520network%2520attained%2520high%2520accuracy%2520on%2520multi-herd%2520imagery%2520%25282017%2529%2520and%2520on%2520an%2520independent%2520year%2527s%2520%25282019%2529%2520test%2520sets%2520%2528F1%253A%252093.7%2525/92.6%2525%252C%2520respectively%2529%252C%2520enabling%2520reliable%2520mapping%2520of%2520regions%2520containing%2520animals%2520to%2520facilitate%2520manual%2520counting%2520on%2520large%2520aerial%2520imagery.%2520Transferred%2520to%2520detection%252C%2520initialization%2520from%2520weakly%2520supervised%2520pretraining%2520yielded%2520consistent%2520gains%2520over%2520ImageNet%2520weights%2520on%2520both%2520positive%2520patches%2520%2528F1%253A%252092.6%2525/93.5%2525%2520vs.%252089.3%2525/88.6%2525%2529%252C%2520and%2520full-image%2520counting%2520%2528F1%253A%252095.5%2525/93.3%2525%2520vs.%252091.5%2525/90.4%2525%2529.%2520Remaining%2520limitations%2520are%2520false%2520positives%2520from%2520animal-like%2520background%2520clutter%2520and%2520false%2520negatives%2520related%2520to%2520low%2520animal%2520density%2520occlusions.%2520Overall%252C%2520pretraining%2520on%2520coarse%2520labels%2520prior%2520to%2520detection%2520makes%2520it%2520possible%2520to%2520rely%2520on%2520weakly-supervised%2520pretrained%2520weights%2520even%2520when%2520labeled%2520data%2520are%2520limited%252C%2520achieving%2520results%2520comparable%2520to%2520generic-weight%2520initialization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.18891v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weakly%20supervised%20framework%20for%20wildlife%20detection%20and%20counting%20in%20challenging%20Arctic%20environments%3A%20a%20case%20study%20on%20caribou%20%28Rangifer%20tarandus%29&entry.906535625=Ghazaleh%20Serati%20and%20Samuel%20Foucher%20and%20Jerome%20Theau&entry.1292438233=Caribou%20across%20the%20Arctic%20has%20declined%20in%20recent%20decades%2C%20motivating%20scalable%20and%20accurate%20monitoring%20approaches%20to%20guide%20evidence-based%20conservation%20actions%20and%20policy%20decisions.%20Manual%20interpretation%20from%20this%20imagery%20is%20labor-intensive%20and%20error-prone%2C%20underscoring%20the%20need%20for%20automatic%20and%20reliable%20detection%20across%20varying%20scenes.%20Yet%2C%20such%20automatic%20detection%20is%20challenging%20due%20to%20severe%20background%20heterogeneity%2C%20dominant%20empty%20terrain%20%28class%20imbalance%29%2C%20small%20or%20occluded%20targets%2C%20and%20wide%20variation%20in%20density%20and%20scale.%20To%20make%20the%20detection%20model%20%28HerdNet%29%20more%20robust%20to%20these%20challenges%2C%20a%20weakly%20supervised%20patch-level%20pretraining%20based%20on%20a%20detection%20network%27s%20architecture%20is%20proposed.%20The%20detection%20dataset%20includes%20five%20caribou%20herds%20distributed%20across%20Alaska.%20By%20learning%20from%20empty%20vs.%20non-empty%20labels%20in%20this%20dataset%2C%20the%20approach%20produces%20early%20weakly%20supervised%20knowledge%20for%20enhanced%20detection%20compared%20to%20HerdNet%2C%20which%20is%20initialized%20from%20generic%20weights.%20Accordingly%2C%20the%20patch-based%20pretrain%20network%20attained%20high%20accuracy%20on%20multi-herd%20imagery%20%282017%29%20and%20on%20an%20independent%20year%27s%20%282019%29%20test%20sets%20%28F1%3A%2093.7%25/92.6%25%2C%20respectively%29%2C%20enabling%20reliable%20mapping%20of%20regions%20containing%20animals%20to%20facilitate%20manual%20counting%20on%20large%20aerial%20imagery.%20Transferred%20to%20detection%2C%20initialization%20from%20weakly%20supervised%20pretraining%20yielded%20consistent%20gains%20over%20ImageNet%20weights%20on%20both%20positive%20patches%20%28F1%3A%2092.6%25/93.5%25%20vs.%2089.3%25/88.6%25%29%2C%20and%20full-image%20counting%20%28F1%3A%2095.5%25/93.3%25%20vs.%2091.5%25/90.4%25%29.%20Remaining%20limitations%20are%20false%20positives%20from%20animal-like%20background%20clutter%20and%20false%20negatives%20related%20to%20low%20animal%20density%20occlusions.%20Overall%2C%20pretraining%20on%20coarse%20labels%20prior%20to%20detection%20makes%20it%20possible%20to%20rely%20on%20weakly-supervised%20pretrained%20weights%20even%20when%20labeled%20data%20are%20limited%2C%20achieving%20results%20comparable%20to%20generic-weight%20initialization.&entry.1838667208=http%3A//arxiv.org/abs/2601.18891v2&entry.124074799=Read"},
{"title": "GCL-OT: Graph Contrastive Learning with Optimal Transport for Heterophilic Text-Attributed Graphs", "author": "Yating Ren and Yikun Ban and Huobin Tan", "abstract": "Recently, structure-text contrastive learning has shown promising performance on text-attributed graphs by leveraging the complementary strengths of graph neural networks and language models. However, existing methods typically rely on homophily assumptions in similarity estimation and hard optimization objectives, which limit their applicability to heterophilic graphs. Although existing methods can mitigate heterophily through structural adjustments or neighbor aggregation, they usually treat textual embeddings as static targets, leading to suboptimal alignment. In this work, we identify multi-granular heterophily in text-attributed graphs, including complete heterophily, partial heterophily, and latent homophily, which makes structure-text alignment particularly challenging due to mixed, noisy, and missing semantic correlations. To achieve flexible and bidirectional alignment, we propose GCL-OT, a novel graph contrastive learning framework with optimal transport, equipped with tailored mechanisms for each type of heterophily. Specifically, for partial heterophily, we design a RealSoftMax-based similarity estimator to emphasize key neighbor-word interactions while easing background noise. For complete heterophily, we introduce a prompt-based filter that adaptively excludes irrelevant noise during optimal transport alignment. Furthermore, we incorporate OT-guided soft supervision to uncover potential neighbors with similar semantics, enhancing the learning of latent homophily. Theoretical analysis shows that GCL-OT can improve the mutual information bound and Bayes error guarantees. Extensive experiments on nine benchmarks show that GCL-OT outperforms state-of-the-art methods, demonstrating its effectiveness and robustness.", "link": "http://arxiv.org/abs/2511.16778v2", "date": "2026-01-28", "relevancy": 2.5367, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.518}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5128}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4913}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GCL-OT%3A%20Graph%20Contrastive%20Learning%20with%20Optimal%20Transport%20for%20Heterophilic%20Text-Attributed%20Graphs&body=Title%3A%20GCL-OT%3A%20Graph%20Contrastive%20Learning%20with%20Optimal%20Transport%20for%20Heterophilic%20Text-Attributed%20Graphs%0AAuthor%3A%20Yating%20Ren%20and%20Yikun%20Ban%20and%20Huobin%20Tan%0AAbstract%3A%20Recently%2C%20structure-text%20contrastive%20learning%20has%20shown%20promising%20performance%20on%20text-attributed%20graphs%20by%20leveraging%20the%20complementary%20strengths%20of%20graph%20neural%20networks%20and%20language%20models.%20However%2C%20existing%20methods%20typically%20rely%20on%20homophily%20assumptions%20in%20similarity%20estimation%20and%20hard%20optimization%20objectives%2C%20which%20limit%20their%20applicability%20to%20heterophilic%20graphs.%20Although%20existing%20methods%20can%20mitigate%20heterophily%20through%20structural%20adjustments%20or%20neighbor%20aggregation%2C%20they%20usually%20treat%20textual%20embeddings%20as%20static%20targets%2C%20leading%20to%20suboptimal%20alignment.%20In%20this%20work%2C%20we%20identify%20multi-granular%20heterophily%20in%20text-attributed%20graphs%2C%20including%20complete%20heterophily%2C%20partial%20heterophily%2C%20and%20latent%20homophily%2C%20which%20makes%20structure-text%20alignment%20particularly%20challenging%20due%20to%20mixed%2C%20noisy%2C%20and%20missing%20semantic%20correlations.%20To%20achieve%20flexible%20and%20bidirectional%20alignment%2C%20we%20propose%20GCL-OT%2C%20a%20novel%20graph%20contrastive%20learning%20framework%20with%20optimal%20transport%2C%20equipped%20with%20tailored%20mechanisms%20for%20each%20type%20of%20heterophily.%20Specifically%2C%20for%20partial%20heterophily%2C%20we%20design%20a%20RealSoftMax-based%20similarity%20estimator%20to%20emphasize%20key%20neighbor-word%20interactions%20while%20easing%20background%20noise.%20For%20complete%20heterophily%2C%20we%20introduce%20a%20prompt-based%20filter%20that%20adaptively%20excludes%20irrelevant%20noise%20during%20optimal%20transport%20alignment.%20Furthermore%2C%20we%20incorporate%20OT-guided%20soft%20supervision%20to%20uncover%20potential%20neighbors%20with%20similar%20semantics%2C%20enhancing%20the%20learning%20of%20latent%20homophily.%20Theoretical%20analysis%20shows%20that%20GCL-OT%20can%20improve%20the%20mutual%20information%20bound%20and%20Bayes%20error%20guarantees.%20Extensive%20experiments%20on%20nine%20benchmarks%20show%20that%20GCL-OT%20outperforms%20state-of-the-art%20methods%2C%20demonstrating%20its%20effectiveness%20and%20robustness.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16778v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGCL-OT%253A%2520Graph%2520Contrastive%2520Learning%2520with%2520Optimal%2520Transport%2520for%2520Heterophilic%2520Text-Attributed%2520Graphs%26entry.906535625%3DYating%2520Ren%2520and%2520Yikun%2520Ban%2520and%2520Huobin%2520Tan%26entry.1292438233%3DRecently%252C%2520structure-text%2520contrastive%2520learning%2520has%2520shown%2520promising%2520performance%2520on%2520text-attributed%2520graphs%2520by%2520leveraging%2520the%2520complementary%2520strengths%2520of%2520graph%2520neural%2520networks%2520and%2520language%2520models.%2520However%252C%2520existing%2520methods%2520typically%2520rely%2520on%2520homophily%2520assumptions%2520in%2520similarity%2520estimation%2520and%2520hard%2520optimization%2520objectives%252C%2520which%2520limit%2520their%2520applicability%2520to%2520heterophilic%2520graphs.%2520Although%2520existing%2520methods%2520can%2520mitigate%2520heterophily%2520through%2520structural%2520adjustments%2520or%2520neighbor%2520aggregation%252C%2520they%2520usually%2520treat%2520textual%2520embeddings%2520as%2520static%2520targets%252C%2520leading%2520to%2520suboptimal%2520alignment.%2520In%2520this%2520work%252C%2520we%2520identify%2520multi-granular%2520heterophily%2520in%2520text-attributed%2520graphs%252C%2520including%2520complete%2520heterophily%252C%2520partial%2520heterophily%252C%2520and%2520latent%2520homophily%252C%2520which%2520makes%2520structure-text%2520alignment%2520particularly%2520challenging%2520due%2520to%2520mixed%252C%2520noisy%252C%2520and%2520missing%2520semantic%2520correlations.%2520To%2520achieve%2520flexible%2520and%2520bidirectional%2520alignment%252C%2520we%2520propose%2520GCL-OT%252C%2520a%2520novel%2520graph%2520contrastive%2520learning%2520framework%2520with%2520optimal%2520transport%252C%2520equipped%2520with%2520tailored%2520mechanisms%2520for%2520each%2520type%2520of%2520heterophily.%2520Specifically%252C%2520for%2520partial%2520heterophily%252C%2520we%2520design%2520a%2520RealSoftMax-based%2520similarity%2520estimator%2520to%2520emphasize%2520key%2520neighbor-word%2520interactions%2520while%2520easing%2520background%2520noise.%2520For%2520complete%2520heterophily%252C%2520we%2520introduce%2520a%2520prompt-based%2520filter%2520that%2520adaptively%2520excludes%2520irrelevant%2520noise%2520during%2520optimal%2520transport%2520alignment.%2520Furthermore%252C%2520we%2520incorporate%2520OT-guided%2520soft%2520supervision%2520to%2520uncover%2520potential%2520neighbors%2520with%2520similar%2520semantics%252C%2520enhancing%2520the%2520learning%2520of%2520latent%2520homophily.%2520Theoretical%2520analysis%2520shows%2520that%2520GCL-OT%2520can%2520improve%2520the%2520mutual%2520information%2520bound%2520and%2520Bayes%2520error%2520guarantees.%2520Extensive%2520experiments%2520on%2520nine%2520benchmarks%2520show%2520that%2520GCL-OT%2520outperforms%2520state-of-the-art%2520methods%252C%2520demonstrating%2520its%2520effectiveness%2520and%2520robustness.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16778v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GCL-OT%3A%20Graph%20Contrastive%20Learning%20with%20Optimal%20Transport%20for%20Heterophilic%20Text-Attributed%20Graphs&entry.906535625=Yating%20Ren%20and%20Yikun%20Ban%20and%20Huobin%20Tan&entry.1292438233=Recently%2C%20structure-text%20contrastive%20learning%20has%20shown%20promising%20performance%20on%20text-attributed%20graphs%20by%20leveraging%20the%20complementary%20strengths%20of%20graph%20neural%20networks%20and%20language%20models.%20However%2C%20existing%20methods%20typically%20rely%20on%20homophily%20assumptions%20in%20similarity%20estimation%20and%20hard%20optimization%20objectives%2C%20which%20limit%20their%20applicability%20to%20heterophilic%20graphs.%20Although%20existing%20methods%20can%20mitigate%20heterophily%20through%20structural%20adjustments%20or%20neighbor%20aggregation%2C%20they%20usually%20treat%20textual%20embeddings%20as%20static%20targets%2C%20leading%20to%20suboptimal%20alignment.%20In%20this%20work%2C%20we%20identify%20multi-granular%20heterophily%20in%20text-attributed%20graphs%2C%20including%20complete%20heterophily%2C%20partial%20heterophily%2C%20and%20latent%20homophily%2C%20which%20makes%20structure-text%20alignment%20particularly%20challenging%20due%20to%20mixed%2C%20noisy%2C%20and%20missing%20semantic%20correlations.%20To%20achieve%20flexible%20and%20bidirectional%20alignment%2C%20we%20propose%20GCL-OT%2C%20a%20novel%20graph%20contrastive%20learning%20framework%20with%20optimal%20transport%2C%20equipped%20with%20tailored%20mechanisms%20for%20each%20type%20of%20heterophily.%20Specifically%2C%20for%20partial%20heterophily%2C%20we%20design%20a%20RealSoftMax-based%20similarity%20estimator%20to%20emphasize%20key%20neighbor-word%20interactions%20while%20easing%20background%20noise.%20For%20complete%20heterophily%2C%20we%20introduce%20a%20prompt-based%20filter%20that%20adaptively%20excludes%20irrelevant%20noise%20during%20optimal%20transport%20alignment.%20Furthermore%2C%20we%20incorporate%20OT-guided%20soft%20supervision%20to%20uncover%20potential%20neighbors%20with%20similar%20semantics%2C%20enhancing%20the%20learning%20of%20latent%20homophily.%20Theoretical%20analysis%20shows%20that%20GCL-OT%20can%20improve%20the%20mutual%20information%20bound%20and%20Bayes%20error%20guarantees.%20Extensive%20experiments%20on%20nine%20benchmarks%20show%20that%20GCL-OT%20outperforms%20state-of-the-art%20methods%2C%20demonstrating%20its%20effectiveness%20and%20robustness.&entry.1838667208=http%3A//arxiv.org/abs/2511.16778v2&entry.124074799=Read"},
{"title": "A New Convergence Analysis of Plug-and-Play Proximal Gradient Descent Under Prior Mismatch", "author": "Guixian Xu and Jinglai Li and Junqi Tang", "abstract": "In this work, we provide a new convergence theory for plug-and-play proximal gradient descent (PnP-PGD) under prior mismatch where the denoiser is trained on a different data distribution to the inference task at hand. To the best of our knowledge, this is the first convergence proof of PnP-PGD under prior mismatch. Compared with the existing theoretical results for PnP algorithms, our new results removed the need for several restrictive and unverifiable assumptions. Moreover, we derive the convergence theory for equivariant PnP (EPnP) under the prior mismatch setting, proving that EPnP reduces error variance and explicitly tightens the convergence bound.", "link": "http://arxiv.org/abs/2601.09831v2", "date": "2026-01-28", "relevancy": 2.5142, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5099}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5012}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20New%20Convergence%20Analysis%20of%20Plug-and-Play%20Proximal%20Gradient%20Descent%20Under%20Prior%20Mismatch&body=Title%3A%20A%20New%20Convergence%20Analysis%20of%20Plug-and-Play%20Proximal%20Gradient%20Descent%20Under%20Prior%20Mismatch%0AAuthor%3A%20Guixian%20Xu%20and%20Jinglai%20Li%20and%20Junqi%20Tang%0AAbstract%3A%20In%20this%20work%2C%20we%20provide%20a%20new%20convergence%20theory%20for%20plug-and-play%20proximal%20gradient%20descent%20%28PnP-PGD%29%20under%20prior%20mismatch%20where%20the%20denoiser%20is%20trained%20on%20a%20different%20data%20distribution%20to%20the%20inference%20task%20at%20hand.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20convergence%20proof%20of%20PnP-PGD%20under%20prior%20mismatch.%20Compared%20with%20the%20existing%20theoretical%20results%20for%20PnP%20algorithms%2C%20our%20new%20results%20removed%20the%20need%20for%20several%20restrictive%20and%20unverifiable%20assumptions.%20Moreover%2C%20we%20derive%20the%20convergence%20theory%20for%20equivariant%20PnP%20%28EPnP%29%20under%20the%20prior%20mismatch%20setting%2C%20proving%20that%20EPnP%20reduces%20error%20variance%20and%20explicitly%20tightens%20the%20convergence%20bound.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09831v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520New%2520Convergence%2520Analysis%2520of%2520Plug-and-Play%2520Proximal%2520Gradient%2520Descent%2520Under%2520Prior%2520Mismatch%26entry.906535625%3DGuixian%2520Xu%2520and%2520Jinglai%2520Li%2520and%2520Junqi%2520Tang%26entry.1292438233%3DIn%2520this%2520work%252C%2520we%2520provide%2520a%2520new%2520convergence%2520theory%2520for%2520plug-and-play%2520proximal%2520gradient%2520descent%2520%2528PnP-PGD%2529%2520under%2520prior%2520mismatch%2520where%2520the%2520denoiser%2520is%2520trained%2520on%2520a%2520different%2520data%2520distribution%2520to%2520the%2520inference%2520task%2520at%2520hand.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520convergence%2520proof%2520of%2520PnP-PGD%2520under%2520prior%2520mismatch.%2520Compared%2520with%2520the%2520existing%2520theoretical%2520results%2520for%2520PnP%2520algorithms%252C%2520our%2520new%2520results%2520removed%2520the%2520need%2520for%2520several%2520restrictive%2520and%2520unverifiable%2520assumptions.%2520Moreover%252C%2520we%2520derive%2520the%2520convergence%2520theory%2520for%2520equivariant%2520PnP%2520%2528EPnP%2529%2520under%2520the%2520prior%2520mismatch%2520setting%252C%2520proving%2520that%2520EPnP%2520reduces%2520error%2520variance%2520and%2520explicitly%2520tightens%2520the%2520convergence%2520bound.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09831v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20New%20Convergence%20Analysis%20of%20Plug-and-Play%20Proximal%20Gradient%20Descent%20Under%20Prior%20Mismatch&entry.906535625=Guixian%20Xu%20and%20Jinglai%20Li%20and%20Junqi%20Tang&entry.1292438233=In%20this%20work%2C%20we%20provide%20a%20new%20convergence%20theory%20for%20plug-and-play%20proximal%20gradient%20descent%20%28PnP-PGD%29%20under%20prior%20mismatch%20where%20the%20denoiser%20is%20trained%20on%20a%20different%20data%20distribution%20to%20the%20inference%20task%20at%20hand.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%20convergence%20proof%20of%20PnP-PGD%20under%20prior%20mismatch.%20Compared%20with%20the%20existing%20theoretical%20results%20for%20PnP%20algorithms%2C%20our%20new%20results%20removed%20the%20need%20for%20several%20restrictive%20and%20unverifiable%20assumptions.%20Moreover%2C%20we%20derive%20the%20convergence%20theory%20for%20equivariant%20PnP%20%28EPnP%29%20under%20the%20prior%20mismatch%20setting%2C%20proving%20that%20EPnP%20reduces%20error%20variance%20and%20explicitly%20tightens%20the%20convergence%20bound.&entry.1838667208=http%3A//arxiv.org/abs/2601.09831v2&entry.124074799=Read"},
{"title": "FAIRT2V: Training-Free Debiasing for Text-to-Video Diffusion Models", "author": "Haonan Zhong and Wei Song and Tingxu Han and Maurice Pagnucco and Jingling Xue and Yang Song", "abstract": "Text-to-video (T2V) diffusion models have achieved rapid progress, yet their demographic biases, particularly gender bias, remain largely unexplored. We present FairT2V, a training-free debiasing framework for text-to-video generation that mitigates encoder-induced bias without finetuning. We first analyze demographic bias in T2V models and show that it primarily originates from pretrained text encoders, which encode implicit gender associations even for neutral prompts. We quantify this effect with a gender-leaning score that correlates with bias in generated videos.\n  Based on this insight, FairT2V mitigates demographic bias by neutralizing prompt embeddings via anchor-based spherical geodesic transformations while preserving semantics. To maintain temporal coherence, we apply debiasing only during early identity-forming steps through a dynamic denoising schedule. We further propose a video-level fairness evaluation protocol combining VideoLLM-based reasoning with human verification. Experiments on the modern T2V model Open-Sora show that FairT2V substantially reduces demographic bias across occupations with minimal impact on video quality.", "link": "http://arxiv.org/abs/2601.20791v1", "date": "2026-01-28", "relevancy": 2.5033, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6381}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6282}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6185}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FAIRT2V%3A%20Training-Free%20Debiasing%20for%20Text-to-Video%20Diffusion%20Models&body=Title%3A%20FAIRT2V%3A%20Training-Free%20Debiasing%20for%20Text-to-Video%20Diffusion%20Models%0AAuthor%3A%20Haonan%20Zhong%20and%20Wei%20Song%20and%20Tingxu%20Han%20and%20Maurice%20Pagnucco%20and%20Jingling%20Xue%20and%20Yang%20Song%0AAbstract%3A%20Text-to-video%20%28T2V%29%20diffusion%20models%20have%20achieved%20rapid%20progress%2C%20yet%20their%20demographic%20biases%2C%20particularly%20gender%20bias%2C%20remain%20largely%20unexplored.%20We%20present%20FairT2V%2C%20a%20training-free%20debiasing%20framework%20for%20text-to-video%20generation%20that%20mitigates%20encoder-induced%20bias%20without%20finetuning.%20We%20first%20analyze%20demographic%20bias%20in%20T2V%20models%20and%20show%20that%20it%20primarily%20originates%20from%20pretrained%20text%20encoders%2C%20which%20encode%20implicit%20gender%20associations%20even%20for%20neutral%20prompts.%20We%20quantify%20this%20effect%20with%20a%20gender-leaning%20score%20that%20correlates%20with%20bias%20in%20generated%20videos.%0A%20%20Based%20on%20this%20insight%2C%20FairT2V%20mitigates%20demographic%20bias%20by%20neutralizing%20prompt%20embeddings%20via%20anchor-based%20spherical%20geodesic%20transformations%20while%20preserving%20semantics.%20To%20maintain%20temporal%20coherence%2C%20we%20apply%20debiasing%20only%20during%20early%20identity-forming%20steps%20through%20a%20dynamic%20denoising%20schedule.%20We%20further%20propose%20a%20video-level%20fairness%20evaluation%20protocol%20combining%20VideoLLM-based%20reasoning%20with%20human%20verification.%20Experiments%20on%20the%20modern%20T2V%20model%20Open-Sora%20show%20that%20FairT2V%20substantially%20reduces%20demographic%20bias%20across%20occupations%20with%20minimal%20impact%20on%20video%20quality.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20791v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFAIRT2V%253A%2520Training-Free%2520Debiasing%2520for%2520Text-to-Video%2520Diffusion%2520Models%26entry.906535625%3DHaonan%2520Zhong%2520and%2520Wei%2520Song%2520and%2520Tingxu%2520Han%2520and%2520Maurice%2520Pagnucco%2520and%2520Jingling%2520Xue%2520and%2520Yang%2520Song%26entry.1292438233%3DText-to-video%2520%2528T2V%2529%2520diffusion%2520models%2520have%2520achieved%2520rapid%2520progress%252C%2520yet%2520their%2520demographic%2520biases%252C%2520particularly%2520gender%2520bias%252C%2520remain%2520largely%2520unexplored.%2520We%2520present%2520FairT2V%252C%2520a%2520training-free%2520debiasing%2520framework%2520for%2520text-to-video%2520generation%2520that%2520mitigates%2520encoder-induced%2520bias%2520without%2520finetuning.%2520We%2520first%2520analyze%2520demographic%2520bias%2520in%2520T2V%2520models%2520and%2520show%2520that%2520it%2520primarily%2520originates%2520from%2520pretrained%2520text%2520encoders%252C%2520which%2520encode%2520implicit%2520gender%2520associations%2520even%2520for%2520neutral%2520prompts.%2520We%2520quantify%2520this%2520effect%2520with%2520a%2520gender-leaning%2520score%2520that%2520correlates%2520with%2520bias%2520in%2520generated%2520videos.%250A%2520%2520Based%2520on%2520this%2520insight%252C%2520FairT2V%2520mitigates%2520demographic%2520bias%2520by%2520neutralizing%2520prompt%2520embeddings%2520via%2520anchor-based%2520spherical%2520geodesic%2520transformations%2520while%2520preserving%2520semantics.%2520To%2520maintain%2520temporal%2520coherence%252C%2520we%2520apply%2520debiasing%2520only%2520during%2520early%2520identity-forming%2520steps%2520through%2520a%2520dynamic%2520denoising%2520schedule.%2520We%2520further%2520propose%2520a%2520video-level%2520fairness%2520evaluation%2520protocol%2520combining%2520VideoLLM-based%2520reasoning%2520with%2520human%2520verification.%2520Experiments%2520on%2520the%2520modern%2520T2V%2520model%2520Open-Sora%2520show%2520that%2520FairT2V%2520substantially%2520reduces%2520demographic%2520bias%2520across%2520occupations%2520with%2520minimal%2520impact%2520on%2520video%2520quality.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20791v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FAIRT2V%3A%20Training-Free%20Debiasing%20for%20Text-to-Video%20Diffusion%20Models&entry.906535625=Haonan%20Zhong%20and%20Wei%20Song%20and%20Tingxu%20Han%20and%20Maurice%20Pagnucco%20and%20Jingling%20Xue%20and%20Yang%20Song&entry.1292438233=Text-to-video%20%28T2V%29%20diffusion%20models%20have%20achieved%20rapid%20progress%2C%20yet%20their%20demographic%20biases%2C%20particularly%20gender%20bias%2C%20remain%20largely%20unexplored.%20We%20present%20FairT2V%2C%20a%20training-free%20debiasing%20framework%20for%20text-to-video%20generation%20that%20mitigates%20encoder-induced%20bias%20without%20finetuning.%20We%20first%20analyze%20demographic%20bias%20in%20T2V%20models%20and%20show%20that%20it%20primarily%20originates%20from%20pretrained%20text%20encoders%2C%20which%20encode%20implicit%20gender%20associations%20even%20for%20neutral%20prompts.%20We%20quantify%20this%20effect%20with%20a%20gender-leaning%20score%20that%20correlates%20with%20bias%20in%20generated%20videos.%0A%20%20Based%20on%20this%20insight%2C%20FairT2V%20mitigates%20demographic%20bias%20by%20neutralizing%20prompt%20embeddings%20via%20anchor-based%20spherical%20geodesic%20transformations%20while%20preserving%20semantics.%20To%20maintain%20temporal%20coherence%2C%20we%20apply%20debiasing%20only%20during%20early%20identity-forming%20steps%20through%20a%20dynamic%20denoising%20schedule.%20We%20further%20propose%20a%20video-level%20fairness%20evaluation%20protocol%20combining%20VideoLLM-based%20reasoning%20with%20human%20verification.%20Experiments%20on%20the%20modern%20T2V%20model%20Open-Sora%20show%20that%20FairT2V%20substantially%20reduces%20demographic%20bias%20across%20occupations%20with%20minimal%20impact%20on%20video%20quality.&entry.1838667208=http%3A//arxiv.org/abs/2601.20791v1&entry.124074799=Read"},
{"title": "Implicit Hypothesis Testing and Divergence Preservation in Neural Network Representations", "author": "Kadircan Aksoy and Peter Jung and Protim Bhattacharjee", "abstract": "We study the supervised training dynamics of neural classifiers through the lens of binary hypothesis testing. We model classification as a set of binary tests between class-conditional distributions of representations and empirically show that, along training trajectories, well-generalizing networks increasingly align with Neyman-Pearson optimal decision rules via monotonic improvements in KL divergence that relate to error rate exponents. We finally discuss how this yields an explanation and possible training or regularization strategies for different classes of neural networks.", "link": "http://arxiv.org/abs/2601.20477v1", "date": "2026-01-28", "relevancy": 2.4865, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5425}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4787}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implicit%20Hypothesis%20Testing%20and%20Divergence%20Preservation%20in%20Neural%20Network%20Representations&body=Title%3A%20Implicit%20Hypothesis%20Testing%20and%20Divergence%20Preservation%20in%20Neural%20Network%20Representations%0AAuthor%3A%20Kadircan%20Aksoy%20and%20Peter%20Jung%20and%20Protim%20Bhattacharjee%0AAbstract%3A%20We%20study%20the%20supervised%20training%20dynamics%20of%20neural%20classifiers%20through%20the%20lens%20of%20binary%20hypothesis%20testing.%20We%20model%20classification%20as%20a%20set%20of%20binary%20tests%20between%20class-conditional%20distributions%20of%20representations%20and%20empirically%20show%20that%2C%20along%20training%20trajectories%2C%20well-generalizing%20networks%20increasingly%20align%20with%20Neyman-Pearson%20optimal%20decision%20rules%20via%20monotonic%20improvements%20in%20KL%20divergence%20that%20relate%20to%20error%20rate%20exponents.%20We%20finally%20discuss%20how%20this%20yields%20an%20explanation%20and%20possible%20training%20or%20regularization%20strategies%20for%20different%20classes%20of%20neural%20networks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20477v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplicit%2520Hypothesis%2520Testing%2520and%2520Divergence%2520Preservation%2520in%2520Neural%2520Network%2520Representations%26entry.906535625%3DKadircan%2520Aksoy%2520and%2520Peter%2520Jung%2520and%2520Protim%2520Bhattacharjee%26entry.1292438233%3DWe%2520study%2520the%2520supervised%2520training%2520dynamics%2520of%2520neural%2520classifiers%2520through%2520the%2520lens%2520of%2520binary%2520hypothesis%2520testing.%2520We%2520model%2520classification%2520as%2520a%2520set%2520of%2520binary%2520tests%2520between%2520class-conditional%2520distributions%2520of%2520representations%2520and%2520empirically%2520show%2520that%252C%2520along%2520training%2520trajectories%252C%2520well-generalizing%2520networks%2520increasingly%2520align%2520with%2520Neyman-Pearson%2520optimal%2520decision%2520rules%2520via%2520monotonic%2520improvements%2520in%2520KL%2520divergence%2520that%2520relate%2520to%2520error%2520rate%2520exponents.%2520We%2520finally%2520discuss%2520how%2520this%2520yields%2520an%2520explanation%2520and%2520possible%2520training%2520or%2520regularization%2520strategies%2520for%2520different%2520classes%2520of%2520neural%2520networks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20477v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20Hypothesis%20Testing%20and%20Divergence%20Preservation%20in%20Neural%20Network%20Representations&entry.906535625=Kadircan%20Aksoy%20and%20Peter%20Jung%20and%20Protim%20Bhattacharjee&entry.1292438233=We%20study%20the%20supervised%20training%20dynamics%20of%20neural%20classifiers%20through%20the%20lens%20of%20binary%20hypothesis%20testing.%20We%20model%20classification%20as%20a%20set%20of%20binary%20tests%20between%20class-conditional%20distributions%20of%20representations%20and%20empirically%20show%20that%2C%20along%20training%20trajectories%2C%20well-generalizing%20networks%20increasingly%20align%20with%20Neyman-Pearson%20optimal%20decision%20rules%20via%20monotonic%20improvements%20in%20KL%20divergence%20that%20relate%20to%20error%20rate%20exponents.%20We%20finally%20discuss%20how%20this%20yields%20an%20explanation%20and%20possible%20training%20or%20regularization%20strategies%20for%20different%20classes%20of%20neural%20networks.&entry.1838667208=http%3A//arxiv.org/abs/2601.20477v1&entry.124074799=Read"},
{"title": "Context Tokens are Anchors: Understanding the Repetition Curse in dMLLMs from an Information Flow Perspective", "author": "Qiyan Zhao and Xiaofeng Zhang and Shuochen Chang and Qianyu Chen and Xiaosong Yuan and Xuhang Chen and Luoqi Liu and Jiajun Zhang and Xu-Yao Zhang and Da-Han Wang", "abstract": "Recent diffusion-based Multimodal Large Language Models (dMLLMs) suffer from high inference latency and therefore rely on caching techniques to accelerate decoding. However, the application of cache mechanisms often introduces undesirable repetitive text generation, a phenomenon we term the \\textbf{Repeat Curse}. To better investigate underlying mechanism behind this issue, we analyze repetition generation through the lens of information flow. Our work reveals three key findings: (1) context tokens aggregate semantic information as anchors and guide the final predictions; (2) as information propagates across layers, the entropy of context tokens converges in deeper layers, reflecting the model's growing prediction certainty; (3) Repetition is typically linked to disruptions in the information flow of context tokens and to the inability of their entropy to converge in deeper layers. Based on these insights, we present \\textbf{CoTA}, a plug-and-play method for mitigating repetition. CoTA enhances the attention of context tokens to preserve intrinsic information flow patterns, while introducing a penalty term to the confidence score during decoding to avoid outputs driven by uncertain context tokens. With extensive experiments, CoTA demonstrates significant effectiveness in alleviating repetition and achieves consistent performance improvements on general tasks. Code is available at https://github.com/ErikZ719/CoTA", "link": "http://arxiv.org/abs/2601.20520v1", "date": "2026-01-28", "relevancy": 2.4777, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5032}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4946}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context%20Tokens%20are%20Anchors%3A%20Understanding%20the%20Repetition%20Curse%20in%20dMLLMs%20from%20an%20Information%20Flow%20Perspective&body=Title%3A%20Context%20Tokens%20are%20Anchors%3A%20Understanding%20the%20Repetition%20Curse%20in%20dMLLMs%20from%20an%20Information%20Flow%20Perspective%0AAuthor%3A%20Qiyan%20Zhao%20and%20Xiaofeng%20Zhang%20and%20Shuochen%20Chang%20and%20Qianyu%20Chen%20and%20Xiaosong%20Yuan%20and%20Xuhang%20Chen%20and%20Luoqi%20Liu%20and%20Jiajun%20Zhang%20and%20Xu-Yao%20Zhang%20and%20Da-Han%20Wang%0AAbstract%3A%20Recent%20diffusion-based%20Multimodal%20Large%20Language%20Models%20%28dMLLMs%29%20suffer%20from%20high%20inference%20latency%20and%20therefore%20rely%20on%20caching%20techniques%20to%20accelerate%20decoding.%20However%2C%20the%20application%20of%20cache%20mechanisms%20often%20introduces%20undesirable%20repetitive%20text%20generation%2C%20a%20phenomenon%20we%20term%20the%20%5Ctextbf%7BRepeat%20Curse%7D.%20To%20better%20investigate%20underlying%20mechanism%20behind%20this%20issue%2C%20we%20analyze%20repetition%20generation%20through%20the%20lens%20of%20information%20flow.%20Our%20work%20reveals%20three%20key%20findings%3A%20%281%29%20context%20tokens%20aggregate%20semantic%20information%20as%20anchors%20and%20guide%20the%20final%20predictions%3B%20%282%29%20as%20information%20propagates%20across%20layers%2C%20the%20entropy%20of%20context%20tokens%20converges%20in%20deeper%20layers%2C%20reflecting%20the%20model%27s%20growing%20prediction%20certainty%3B%20%283%29%20Repetition%20is%20typically%20linked%20to%20disruptions%20in%20the%20information%20flow%20of%20context%20tokens%20and%20to%20the%20inability%20of%20their%20entropy%20to%20converge%20in%20deeper%20layers.%20Based%20on%20these%20insights%2C%20we%20present%20%5Ctextbf%7BCoTA%7D%2C%20a%20plug-and-play%20method%20for%20mitigating%20repetition.%20CoTA%20enhances%20the%20attention%20of%20context%20tokens%20to%20preserve%20intrinsic%20information%20flow%20patterns%2C%20while%20introducing%20a%20penalty%20term%20to%20the%20confidence%20score%20during%20decoding%20to%20avoid%20outputs%20driven%20by%20uncertain%20context%20tokens.%20With%20extensive%20experiments%2C%20CoTA%20demonstrates%20significant%20effectiveness%20in%20alleviating%20repetition%20and%20achieves%20consistent%20performance%20improvements%20on%20general%20tasks.%20Code%20is%20available%20at%20https%3A//github.com/ErikZ719/CoTA%0ALink%3A%20http%3A//arxiv.org/abs/2601.20520v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext%2520Tokens%2520are%2520Anchors%253A%2520Understanding%2520the%2520Repetition%2520Curse%2520in%2520dMLLMs%2520from%2520an%2520Information%2520Flow%2520Perspective%26entry.906535625%3DQiyan%2520Zhao%2520and%2520Xiaofeng%2520Zhang%2520and%2520Shuochen%2520Chang%2520and%2520Qianyu%2520Chen%2520and%2520Xiaosong%2520Yuan%2520and%2520Xuhang%2520Chen%2520and%2520Luoqi%2520Liu%2520and%2520Jiajun%2520Zhang%2520and%2520Xu-Yao%2520Zhang%2520and%2520Da-Han%2520Wang%26entry.1292438233%3DRecent%2520diffusion-based%2520Multimodal%2520Large%2520Language%2520Models%2520%2528dMLLMs%2529%2520suffer%2520from%2520high%2520inference%2520latency%2520and%2520therefore%2520rely%2520on%2520caching%2520techniques%2520to%2520accelerate%2520decoding.%2520However%252C%2520the%2520application%2520of%2520cache%2520mechanisms%2520often%2520introduces%2520undesirable%2520repetitive%2520text%2520generation%252C%2520a%2520phenomenon%2520we%2520term%2520the%2520%255Ctextbf%257BRepeat%2520Curse%257D.%2520To%2520better%2520investigate%2520underlying%2520mechanism%2520behind%2520this%2520issue%252C%2520we%2520analyze%2520repetition%2520generation%2520through%2520the%2520lens%2520of%2520information%2520flow.%2520Our%2520work%2520reveals%2520three%2520key%2520findings%253A%2520%25281%2529%2520context%2520tokens%2520aggregate%2520semantic%2520information%2520as%2520anchors%2520and%2520guide%2520the%2520final%2520predictions%253B%2520%25282%2529%2520as%2520information%2520propagates%2520across%2520layers%252C%2520the%2520entropy%2520of%2520context%2520tokens%2520converges%2520in%2520deeper%2520layers%252C%2520reflecting%2520the%2520model%2527s%2520growing%2520prediction%2520certainty%253B%2520%25283%2529%2520Repetition%2520is%2520typically%2520linked%2520to%2520disruptions%2520in%2520the%2520information%2520flow%2520of%2520context%2520tokens%2520and%2520to%2520the%2520inability%2520of%2520their%2520entropy%2520to%2520converge%2520in%2520deeper%2520layers.%2520Based%2520on%2520these%2520insights%252C%2520we%2520present%2520%255Ctextbf%257BCoTA%257D%252C%2520a%2520plug-and-play%2520method%2520for%2520mitigating%2520repetition.%2520CoTA%2520enhances%2520the%2520attention%2520of%2520context%2520tokens%2520to%2520preserve%2520intrinsic%2520information%2520flow%2520patterns%252C%2520while%2520introducing%2520a%2520penalty%2520term%2520to%2520the%2520confidence%2520score%2520during%2520decoding%2520to%2520avoid%2520outputs%2520driven%2520by%2520uncertain%2520context%2520tokens.%2520With%2520extensive%2520experiments%252C%2520CoTA%2520demonstrates%2520significant%2520effectiveness%2520in%2520alleviating%2520repetition%2520and%2520achieves%2520consistent%2520performance%2520improvements%2520on%2520general%2520tasks.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/ErikZ719/CoTA%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20520v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context%20Tokens%20are%20Anchors%3A%20Understanding%20the%20Repetition%20Curse%20in%20dMLLMs%20from%20an%20Information%20Flow%20Perspective&entry.906535625=Qiyan%20Zhao%20and%20Xiaofeng%20Zhang%20and%20Shuochen%20Chang%20and%20Qianyu%20Chen%20and%20Xiaosong%20Yuan%20and%20Xuhang%20Chen%20and%20Luoqi%20Liu%20and%20Jiajun%20Zhang%20and%20Xu-Yao%20Zhang%20and%20Da-Han%20Wang&entry.1292438233=Recent%20diffusion-based%20Multimodal%20Large%20Language%20Models%20%28dMLLMs%29%20suffer%20from%20high%20inference%20latency%20and%20therefore%20rely%20on%20caching%20techniques%20to%20accelerate%20decoding.%20However%2C%20the%20application%20of%20cache%20mechanisms%20often%20introduces%20undesirable%20repetitive%20text%20generation%2C%20a%20phenomenon%20we%20term%20the%20%5Ctextbf%7BRepeat%20Curse%7D.%20To%20better%20investigate%20underlying%20mechanism%20behind%20this%20issue%2C%20we%20analyze%20repetition%20generation%20through%20the%20lens%20of%20information%20flow.%20Our%20work%20reveals%20three%20key%20findings%3A%20%281%29%20context%20tokens%20aggregate%20semantic%20information%20as%20anchors%20and%20guide%20the%20final%20predictions%3B%20%282%29%20as%20information%20propagates%20across%20layers%2C%20the%20entropy%20of%20context%20tokens%20converges%20in%20deeper%20layers%2C%20reflecting%20the%20model%27s%20growing%20prediction%20certainty%3B%20%283%29%20Repetition%20is%20typically%20linked%20to%20disruptions%20in%20the%20information%20flow%20of%20context%20tokens%20and%20to%20the%20inability%20of%20their%20entropy%20to%20converge%20in%20deeper%20layers.%20Based%20on%20these%20insights%2C%20we%20present%20%5Ctextbf%7BCoTA%7D%2C%20a%20plug-and-play%20method%20for%20mitigating%20repetition.%20CoTA%20enhances%20the%20attention%20of%20context%20tokens%20to%20preserve%20intrinsic%20information%20flow%20patterns%2C%20while%20introducing%20a%20penalty%20term%20to%20the%20confidence%20score%20during%20decoding%20to%20avoid%20outputs%20driven%20by%20uncertain%20context%20tokens.%20With%20extensive%20experiments%2C%20CoTA%20demonstrates%20significant%20effectiveness%20in%20alleviating%20repetition%20and%20achieves%20consistent%20performance%20improvements%20on%20general%20tasks.%20Code%20is%20available%20at%20https%3A//github.com/ErikZ719/CoTA&entry.1838667208=http%3A//arxiv.org/abs/2601.20520v1&entry.124074799=Read"},
{"title": "Beyond Random Sampling: Efficient Language Model Pretraining via Curriculum Learning", "author": "Yang Zhang and Amr Mohamed and Hadi Abdine and Guokan Shang and Michalis Vazirgiannis", "abstract": "Curriculum learning-organizing training data from easy to hard-has improved efficiency across machine learning domains, yet remains underexplored for language model pretraining. We present the first systematic investigation of curriculum learning in LLM pretraining, with over 200 models trained on up to 100B tokens across three strategies: vanilla curriculum learning, pacing-based sampling, and interleaved curricula, guided by six difficulty metrics spanning linguistic and information-theoretic properties. We evaluate performance on eight benchmarks under three realistic scenarios: limited data, unlimited data, and continual training. Our experiments show that curriculum learning consistently accelerates convergence in early and mid-training phases,reducing training steps by $18-45\\%$ to reach baseline performance. When applied as a warmup strategy before standard random sampling, curriculum learning yields sustained improvements up to $3.5\\%$. We identify compression ratio, lexical diversity (MTLD), and readability (Flesch Reading Ease) as the most effective difficulty signals. Our findings demonstrate that data ordering-orthogonal to existing data selection methods-provides a practical mechanism for more efficient LLM pretraining.", "link": "http://arxiv.org/abs/2506.11300v2", "date": "2026-01-28", "relevancy": 2.4743, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5008}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4919}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4919}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Random%20Sampling%3A%20Efficient%20Language%20Model%20Pretraining%20via%20Curriculum%20Learning&body=Title%3A%20Beyond%20Random%20Sampling%3A%20Efficient%20Language%20Model%20Pretraining%20via%20Curriculum%20Learning%0AAuthor%3A%20Yang%20Zhang%20and%20Amr%20Mohamed%20and%20Hadi%20Abdine%20and%20Guokan%20Shang%20and%20Michalis%20Vazirgiannis%0AAbstract%3A%20Curriculum%20learning-organizing%20training%20data%20from%20easy%20to%20hard-has%20improved%20efficiency%20across%20machine%20learning%20domains%2C%20yet%20remains%20underexplored%20for%20language%20model%20pretraining.%20We%20present%20the%20first%20systematic%20investigation%20of%20curriculum%20learning%20in%20LLM%20pretraining%2C%20with%20over%20200%20models%20trained%20on%20up%20to%20100B%20tokens%20across%20three%20strategies%3A%20vanilla%20curriculum%20learning%2C%20pacing-based%20sampling%2C%20and%20interleaved%20curricula%2C%20guided%20by%20six%20difficulty%20metrics%20spanning%20linguistic%20and%20information-theoretic%20properties.%20We%20evaluate%20performance%20on%20eight%20benchmarks%20under%20three%20realistic%20scenarios%3A%20limited%20data%2C%20unlimited%20data%2C%20and%20continual%20training.%20Our%20experiments%20show%20that%20curriculum%20learning%20consistently%20accelerates%20convergence%20in%20early%20and%20mid-training%20phases%2Creducing%20training%20steps%20by%20%2418-45%5C%25%24%20to%20reach%20baseline%20performance.%20When%20applied%20as%20a%20warmup%20strategy%20before%20standard%20random%20sampling%2C%20curriculum%20learning%20yields%20sustained%20improvements%20up%20to%20%243.5%5C%25%24.%20We%20identify%20compression%20ratio%2C%20lexical%20diversity%20%28MTLD%29%2C%20and%20readability%20%28Flesch%20Reading%20Ease%29%20as%20the%20most%20effective%20difficulty%20signals.%20Our%20findings%20demonstrate%20that%20data%20ordering-orthogonal%20to%20existing%20data%20selection%20methods-provides%20a%20practical%20mechanism%20for%20more%20efficient%20LLM%20pretraining.%0ALink%3A%20http%3A//arxiv.org/abs/2506.11300v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Random%2520Sampling%253A%2520Efficient%2520Language%2520Model%2520Pretraining%2520via%2520Curriculum%2520Learning%26entry.906535625%3DYang%2520Zhang%2520and%2520Amr%2520Mohamed%2520and%2520Hadi%2520Abdine%2520and%2520Guokan%2520Shang%2520and%2520Michalis%2520Vazirgiannis%26entry.1292438233%3DCurriculum%2520learning-organizing%2520training%2520data%2520from%2520easy%2520to%2520hard-has%2520improved%2520efficiency%2520across%2520machine%2520learning%2520domains%252C%2520yet%2520remains%2520underexplored%2520for%2520language%2520model%2520pretraining.%2520We%2520present%2520the%2520first%2520systematic%2520investigation%2520of%2520curriculum%2520learning%2520in%2520LLM%2520pretraining%252C%2520with%2520over%2520200%2520models%2520trained%2520on%2520up%2520to%2520100B%2520tokens%2520across%2520three%2520strategies%253A%2520vanilla%2520curriculum%2520learning%252C%2520pacing-based%2520sampling%252C%2520and%2520interleaved%2520curricula%252C%2520guided%2520by%2520six%2520difficulty%2520metrics%2520spanning%2520linguistic%2520and%2520information-theoretic%2520properties.%2520We%2520evaluate%2520performance%2520on%2520eight%2520benchmarks%2520under%2520three%2520realistic%2520scenarios%253A%2520limited%2520data%252C%2520unlimited%2520data%252C%2520and%2520continual%2520training.%2520Our%2520experiments%2520show%2520that%2520curriculum%2520learning%2520consistently%2520accelerates%2520convergence%2520in%2520early%2520and%2520mid-training%2520phases%252Creducing%2520training%2520steps%2520by%2520%252418-45%255C%2525%2524%2520to%2520reach%2520baseline%2520performance.%2520When%2520applied%2520as%2520a%2520warmup%2520strategy%2520before%2520standard%2520random%2520sampling%252C%2520curriculum%2520learning%2520yields%2520sustained%2520improvements%2520up%2520to%2520%25243.5%255C%2525%2524.%2520We%2520identify%2520compression%2520ratio%252C%2520lexical%2520diversity%2520%2528MTLD%2529%252C%2520and%2520readability%2520%2528Flesch%2520Reading%2520Ease%2529%2520as%2520the%2520most%2520effective%2520difficulty%2520signals.%2520Our%2520findings%2520demonstrate%2520that%2520data%2520ordering-orthogonal%2520to%2520existing%2520data%2520selection%2520methods-provides%2520a%2520practical%2520mechanism%2520for%2520more%2520efficient%2520LLM%2520pretraining.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11300v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Random%20Sampling%3A%20Efficient%20Language%20Model%20Pretraining%20via%20Curriculum%20Learning&entry.906535625=Yang%20Zhang%20and%20Amr%20Mohamed%20and%20Hadi%20Abdine%20and%20Guokan%20Shang%20and%20Michalis%20Vazirgiannis&entry.1292438233=Curriculum%20learning-organizing%20training%20data%20from%20easy%20to%20hard-has%20improved%20efficiency%20across%20machine%20learning%20domains%2C%20yet%20remains%20underexplored%20for%20language%20model%20pretraining.%20We%20present%20the%20first%20systematic%20investigation%20of%20curriculum%20learning%20in%20LLM%20pretraining%2C%20with%20over%20200%20models%20trained%20on%20up%20to%20100B%20tokens%20across%20three%20strategies%3A%20vanilla%20curriculum%20learning%2C%20pacing-based%20sampling%2C%20and%20interleaved%20curricula%2C%20guided%20by%20six%20difficulty%20metrics%20spanning%20linguistic%20and%20information-theoretic%20properties.%20We%20evaluate%20performance%20on%20eight%20benchmarks%20under%20three%20realistic%20scenarios%3A%20limited%20data%2C%20unlimited%20data%2C%20and%20continual%20training.%20Our%20experiments%20show%20that%20curriculum%20learning%20consistently%20accelerates%20convergence%20in%20early%20and%20mid-training%20phases%2Creducing%20training%20steps%20by%20%2418-45%5C%25%24%20to%20reach%20baseline%20performance.%20When%20applied%20as%20a%20warmup%20strategy%20before%20standard%20random%20sampling%2C%20curriculum%20learning%20yields%20sustained%20improvements%20up%20to%20%243.5%5C%25%24.%20We%20identify%20compression%20ratio%2C%20lexical%20diversity%20%28MTLD%29%2C%20and%20readability%20%28Flesch%20Reading%20Ease%29%20as%20the%20most%20effective%20difficulty%20signals.%20Our%20findings%20demonstrate%20that%20data%20ordering-orthogonal%20to%20existing%20data%20selection%20methods-provides%20a%20practical%20mechanism%20for%20more%20efficient%20LLM%20pretraining.&entry.1838667208=http%3A//arxiv.org/abs/2506.11300v2&entry.124074799=Read"},
{"title": "QVGen: Pushing the Limit of Quantized Video Generative Models", "author": "Yushi Huang and Ruihao Gong and Jing Liu and Yifu Ding and Chengtao Lv and Haotong Qin and Jun Zhang", "abstract": "Video diffusion models (DMs) have enabled high-quality video synthesis. Yet, their substantial computational and memory demands pose serious challenges to real-world deployment, even on high-end GPUs. As a commonly adopted solution, quantization has proven notable success in reducing cost for image DMs, while its direct application to video DMs remains ineffective. In this paper, we present QVGen, a novel quantization-aware training (QAT) framework tailored for high-performance and inference-efficient video DMs under extremely low-bit quantization (e.g., 4-bit or below). We begin with a theoretical analysis demonstrating that reducing the gradient norm is essential to facilitate convergence for QAT. To this end, we introduce auxiliary modules ($\u03a6$) to mitigate large quantization errors, leading to significantly enhanced convergence. To eliminate the inference overhead of $\u03a6$, we propose a rank-decay strategy that progressively eliminates $\u03a6$. Specifically, we repeatedly employ singular value decomposition (SVD) and a proposed rank-based regularization $\\mathbf\u03b3$ to identify and decay low-contributing components. This strategy retains performance while zeroing out additional inference overhead. Extensive experiments across $4$ state-of-the-art (SOTA) video DMs, with parameter sizes ranging from $1.3\\text{B}\\sim14\\text{B}$, show that QVGen is the first to reach full-precision comparable quality under 4-bit settings. Moreover, it significantly outperforms existing methods. For instance, our 3-bit CogVideoX-2B achieves improvements of $+25.28$ in Dynamic Degree and $+8.43$ in Scene Consistency on VBench. Code and models are available at https://github.com/ModelTC/QVGen.", "link": "http://arxiv.org/abs/2505.11497v5", "date": "2026-01-28", "relevancy": 2.473, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6391}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6052}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5989}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QVGen%3A%20Pushing%20the%20Limit%20of%20Quantized%20Video%20Generative%20Models&body=Title%3A%20QVGen%3A%20Pushing%20the%20Limit%20of%20Quantized%20Video%20Generative%20Models%0AAuthor%3A%20Yushi%20Huang%20and%20Ruihao%20Gong%20and%20Jing%20Liu%20and%20Yifu%20Ding%20and%20Chengtao%20Lv%20and%20Haotong%20Qin%20and%20Jun%20Zhang%0AAbstract%3A%20Video%20diffusion%20models%20%28DMs%29%20have%20enabled%20high-quality%20video%20synthesis.%20Yet%2C%20their%20substantial%20computational%20and%20memory%20demands%20pose%20serious%20challenges%20to%20real-world%20deployment%2C%20even%20on%20high-end%20GPUs.%20As%20a%20commonly%20adopted%20solution%2C%20quantization%20has%20proven%20notable%20success%20in%20reducing%20cost%20for%20image%20DMs%2C%20while%20its%20direct%20application%20to%20video%20DMs%20remains%20ineffective.%20In%20this%20paper%2C%20we%20present%20QVGen%2C%20a%20novel%20quantization-aware%20training%20%28QAT%29%20framework%20tailored%20for%20high-performance%20and%20inference-efficient%20video%20DMs%20under%20extremely%20low-bit%20quantization%20%28e.g.%2C%204-bit%20or%20below%29.%20We%20begin%20with%20a%20theoretical%20analysis%20demonstrating%20that%20reducing%20the%20gradient%20norm%20is%20essential%20to%20facilitate%20convergence%20for%20QAT.%20To%20this%20end%2C%20we%20introduce%20auxiliary%20modules%20%28%24%CE%A6%24%29%20to%20mitigate%20large%20quantization%20errors%2C%20leading%20to%20significantly%20enhanced%20convergence.%20To%20eliminate%20the%20inference%20overhead%20of%20%24%CE%A6%24%2C%20we%20propose%20a%20rank-decay%20strategy%20that%20progressively%20eliminates%20%24%CE%A6%24.%20Specifically%2C%20we%20repeatedly%20employ%20singular%20value%20decomposition%20%28SVD%29%20and%20a%20proposed%20rank-based%20regularization%20%24%5Cmathbf%CE%B3%24%20to%20identify%20and%20decay%20low-contributing%20components.%20This%20strategy%20retains%20performance%20while%20zeroing%20out%20additional%20inference%20overhead.%20Extensive%20experiments%20across%20%244%24%20state-of-the-art%20%28SOTA%29%20video%20DMs%2C%20with%20parameter%20sizes%20ranging%20from%20%241.3%5Ctext%7BB%7D%5Csim14%5Ctext%7BB%7D%24%2C%20show%20that%20QVGen%20is%20the%20first%20to%20reach%20full-precision%20comparable%20quality%20under%204-bit%20settings.%20Moreover%2C%20it%20significantly%20outperforms%20existing%20methods.%20For%20instance%2C%20our%203-bit%20CogVideoX-2B%20achieves%20improvements%20of%20%24%2B25.28%24%20in%20Dynamic%20Degree%20and%20%24%2B8.43%24%20in%20Scene%20Consistency%20on%20VBench.%20Code%20and%20models%20are%20available%20at%20https%3A//github.com/ModelTC/QVGen.%0ALink%3A%20http%3A//arxiv.org/abs/2505.11497v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQVGen%253A%2520Pushing%2520the%2520Limit%2520of%2520Quantized%2520Video%2520Generative%2520Models%26entry.906535625%3DYushi%2520Huang%2520and%2520Ruihao%2520Gong%2520and%2520Jing%2520Liu%2520and%2520Yifu%2520Ding%2520and%2520Chengtao%2520Lv%2520and%2520Haotong%2520Qin%2520and%2520Jun%2520Zhang%26entry.1292438233%3DVideo%2520diffusion%2520models%2520%2528DMs%2529%2520have%2520enabled%2520high-quality%2520video%2520synthesis.%2520Yet%252C%2520their%2520substantial%2520computational%2520and%2520memory%2520demands%2520pose%2520serious%2520challenges%2520to%2520real-world%2520deployment%252C%2520even%2520on%2520high-end%2520GPUs.%2520As%2520a%2520commonly%2520adopted%2520solution%252C%2520quantization%2520has%2520proven%2520notable%2520success%2520in%2520reducing%2520cost%2520for%2520image%2520DMs%252C%2520while%2520its%2520direct%2520application%2520to%2520video%2520DMs%2520remains%2520ineffective.%2520In%2520this%2520paper%252C%2520we%2520present%2520QVGen%252C%2520a%2520novel%2520quantization-aware%2520training%2520%2528QAT%2529%2520framework%2520tailored%2520for%2520high-performance%2520and%2520inference-efficient%2520video%2520DMs%2520under%2520extremely%2520low-bit%2520quantization%2520%2528e.g.%252C%25204-bit%2520or%2520below%2529.%2520We%2520begin%2520with%2520a%2520theoretical%2520analysis%2520demonstrating%2520that%2520reducing%2520the%2520gradient%2520norm%2520is%2520essential%2520to%2520facilitate%2520convergence%2520for%2520QAT.%2520To%2520this%2520end%252C%2520we%2520introduce%2520auxiliary%2520modules%2520%2528%2524%25CE%25A6%2524%2529%2520to%2520mitigate%2520large%2520quantization%2520errors%252C%2520leading%2520to%2520significantly%2520enhanced%2520convergence.%2520To%2520eliminate%2520the%2520inference%2520overhead%2520of%2520%2524%25CE%25A6%2524%252C%2520we%2520propose%2520a%2520rank-decay%2520strategy%2520that%2520progressively%2520eliminates%2520%2524%25CE%25A6%2524.%2520Specifically%252C%2520we%2520repeatedly%2520employ%2520singular%2520value%2520decomposition%2520%2528SVD%2529%2520and%2520a%2520proposed%2520rank-based%2520regularization%2520%2524%255Cmathbf%25CE%25B3%2524%2520to%2520identify%2520and%2520decay%2520low-contributing%2520components.%2520This%2520strategy%2520retains%2520performance%2520while%2520zeroing%2520out%2520additional%2520inference%2520overhead.%2520Extensive%2520experiments%2520across%2520%25244%2524%2520state-of-the-art%2520%2528SOTA%2529%2520video%2520DMs%252C%2520with%2520parameter%2520sizes%2520ranging%2520from%2520%25241.3%255Ctext%257BB%257D%255Csim14%255Ctext%257BB%257D%2524%252C%2520show%2520that%2520QVGen%2520is%2520the%2520first%2520to%2520reach%2520full-precision%2520comparable%2520quality%2520under%25204-bit%2520settings.%2520Moreover%252C%2520it%2520significantly%2520outperforms%2520existing%2520methods.%2520For%2520instance%252C%2520our%25203-bit%2520CogVideoX-2B%2520achieves%2520improvements%2520of%2520%2524%252B25.28%2524%2520in%2520Dynamic%2520Degree%2520and%2520%2524%252B8.43%2524%2520in%2520Scene%2520Consistency%2520on%2520VBench.%2520Code%2520and%2520models%2520are%2520available%2520at%2520https%253A//github.com/ModelTC/QVGen.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11497v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QVGen%3A%20Pushing%20the%20Limit%20of%20Quantized%20Video%20Generative%20Models&entry.906535625=Yushi%20Huang%20and%20Ruihao%20Gong%20and%20Jing%20Liu%20and%20Yifu%20Ding%20and%20Chengtao%20Lv%20and%20Haotong%20Qin%20and%20Jun%20Zhang&entry.1292438233=Video%20diffusion%20models%20%28DMs%29%20have%20enabled%20high-quality%20video%20synthesis.%20Yet%2C%20their%20substantial%20computational%20and%20memory%20demands%20pose%20serious%20challenges%20to%20real-world%20deployment%2C%20even%20on%20high-end%20GPUs.%20As%20a%20commonly%20adopted%20solution%2C%20quantization%20has%20proven%20notable%20success%20in%20reducing%20cost%20for%20image%20DMs%2C%20while%20its%20direct%20application%20to%20video%20DMs%20remains%20ineffective.%20In%20this%20paper%2C%20we%20present%20QVGen%2C%20a%20novel%20quantization-aware%20training%20%28QAT%29%20framework%20tailored%20for%20high-performance%20and%20inference-efficient%20video%20DMs%20under%20extremely%20low-bit%20quantization%20%28e.g.%2C%204-bit%20or%20below%29.%20We%20begin%20with%20a%20theoretical%20analysis%20demonstrating%20that%20reducing%20the%20gradient%20norm%20is%20essential%20to%20facilitate%20convergence%20for%20QAT.%20To%20this%20end%2C%20we%20introduce%20auxiliary%20modules%20%28%24%CE%A6%24%29%20to%20mitigate%20large%20quantization%20errors%2C%20leading%20to%20significantly%20enhanced%20convergence.%20To%20eliminate%20the%20inference%20overhead%20of%20%24%CE%A6%24%2C%20we%20propose%20a%20rank-decay%20strategy%20that%20progressively%20eliminates%20%24%CE%A6%24.%20Specifically%2C%20we%20repeatedly%20employ%20singular%20value%20decomposition%20%28SVD%29%20and%20a%20proposed%20rank-based%20regularization%20%24%5Cmathbf%CE%B3%24%20to%20identify%20and%20decay%20low-contributing%20components.%20This%20strategy%20retains%20performance%20while%20zeroing%20out%20additional%20inference%20overhead.%20Extensive%20experiments%20across%20%244%24%20state-of-the-art%20%28SOTA%29%20video%20DMs%2C%20with%20parameter%20sizes%20ranging%20from%20%241.3%5Ctext%7BB%7D%5Csim14%5Ctext%7BB%7D%24%2C%20show%20that%20QVGen%20is%20the%20first%20to%20reach%20full-precision%20comparable%20quality%20under%204-bit%20settings.%20Moreover%2C%20it%20significantly%20outperforms%20existing%20methods.%20For%20instance%2C%20our%203-bit%20CogVideoX-2B%20achieves%20improvements%20of%20%24%2B25.28%24%20in%20Dynamic%20Degree%20and%20%24%2B8.43%24%20in%20Scene%20Consistency%20on%20VBench.%20Code%20and%20models%20are%20available%20at%20https%3A//github.com/ModelTC/QVGen.&entry.1838667208=http%3A//arxiv.org/abs/2505.11497v5&entry.124074799=Read"},
{"title": "When More Data Doesn't Help: Limits of Adaptation in Multitask Learning", "author": "Steve Hanneke and Mingyue Xu", "abstract": "Multitask learning and related frameworks have achieved tremendous success in modern applications. In multitask learning problem, we are given a set of heterogeneous datasets collected from related source tasks and hope to enhance the performance above what we could hope to achieve by solving each of them individually. The recent work of arXiv:2006.15785 has showed that, without access to distributional information, no algorithm based on aggregating samples alone can guarantee optimal risk as long as the sample size per task is bounded.\n  In this paper, we focus on understanding the statistical limits of multitask learning. We go beyond the no-free-lunch theorem in arXiv:2006.15785 by establishing a stronger impossibility result of adaptation that holds for arbitrarily large sample size per task. This improvement conveys an important message that the hardness of multitask learning cannot be overcame by having abundant data per task. We also discuss the notion of optimal adaptivity that may be of future interests.", "link": "http://arxiv.org/abs/2601.20774v1", "date": "2026-01-28", "relevancy": 2.4656, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5134}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4911}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20More%20Data%20Doesn%27t%20Help%3A%20Limits%20of%20Adaptation%20in%20Multitask%20Learning&body=Title%3A%20When%20More%20Data%20Doesn%27t%20Help%3A%20Limits%20of%20Adaptation%20in%20Multitask%20Learning%0AAuthor%3A%20Steve%20Hanneke%20and%20Mingyue%20Xu%0AAbstract%3A%20Multitask%20learning%20and%20related%20frameworks%20have%20achieved%20tremendous%20success%20in%20modern%20applications.%20In%20multitask%20learning%20problem%2C%20we%20are%20given%20a%20set%20of%20heterogeneous%20datasets%20collected%20from%20related%20source%20tasks%20and%20hope%20to%20enhance%20the%20performance%20above%20what%20we%20could%20hope%20to%20achieve%20by%20solving%20each%20of%20them%20individually.%20The%20recent%20work%20of%20arXiv%3A2006.15785%20has%20showed%20that%2C%20without%20access%20to%20distributional%20information%2C%20no%20algorithm%20based%20on%20aggregating%20samples%20alone%20can%20guarantee%20optimal%20risk%20as%20long%20as%20the%20sample%20size%20per%20task%20is%20bounded.%0A%20%20In%20this%20paper%2C%20we%20focus%20on%20understanding%20the%20statistical%20limits%20of%20multitask%20learning.%20We%20go%20beyond%20the%20no-free-lunch%20theorem%20in%20arXiv%3A2006.15785%20by%20establishing%20a%20stronger%20impossibility%20result%20of%20adaptation%20that%20holds%20for%20arbitrarily%20large%20sample%20size%20per%20task.%20This%20improvement%20conveys%20an%20important%20message%20that%20the%20hardness%20of%20multitask%20learning%20cannot%20be%20overcame%20by%20having%20abundant%20data%20per%20task.%20We%20also%20discuss%20the%20notion%20of%20optimal%20adaptivity%20that%20may%20be%20of%20future%20interests.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20774v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520More%2520Data%2520Doesn%2527t%2520Help%253A%2520Limits%2520of%2520Adaptation%2520in%2520Multitask%2520Learning%26entry.906535625%3DSteve%2520Hanneke%2520and%2520Mingyue%2520Xu%26entry.1292438233%3DMultitask%2520learning%2520and%2520related%2520frameworks%2520have%2520achieved%2520tremendous%2520success%2520in%2520modern%2520applications.%2520In%2520multitask%2520learning%2520problem%252C%2520we%2520are%2520given%2520a%2520set%2520of%2520heterogeneous%2520datasets%2520collected%2520from%2520related%2520source%2520tasks%2520and%2520hope%2520to%2520enhance%2520the%2520performance%2520above%2520what%2520we%2520could%2520hope%2520to%2520achieve%2520by%2520solving%2520each%2520of%2520them%2520individually.%2520The%2520recent%2520work%2520of%2520arXiv%253A2006.15785%2520has%2520showed%2520that%252C%2520without%2520access%2520to%2520distributional%2520information%252C%2520no%2520algorithm%2520based%2520on%2520aggregating%2520samples%2520alone%2520can%2520guarantee%2520optimal%2520risk%2520as%2520long%2520as%2520the%2520sample%2520size%2520per%2520task%2520is%2520bounded.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520understanding%2520the%2520statistical%2520limits%2520of%2520multitask%2520learning.%2520We%2520go%2520beyond%2520the%2520no-free-lunch%2520theorem%2520in%2520arXiv%253A2006.15785%2520by%2520establishing%2520a%2520stronger%2520impossibility%2520result%2520of%2520adaptation%2520that%2520holds%2520for%2520arbitrarily%2520large%2520sample%2520size%2520per%2520task.%2520This%2520improvement%2520conveys%2520an%2520important%2520message%2520that%2520the%2520hardness%2520of%2520multitask%2520learning%2520cannot%2520be%2520overcame%2520by%2520having%2520abundant%2520data%2520per%2520task.%2520We%2520also%2520discuss%2520the%2520notion%2520of%2520optimal%2520adaptivity%2520that%2520may%2520be%2520of%2520future%2520interests.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20774v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20More%20Data%20Doesn%27t%20Help%3A%20Limits%20of%20Adaptation%20in%20Multitask%20Learning&entry.906535625=Steve%20Hanneke%20and%20Mingyue%20Xu&entry.1292438233=Multitask%20learning%20and%20related%20frameworks%20have%20achieved%20tremendous%20success%20in%20modern%20applications.%20In%20multitask%20learning%20problem%2C%20we%20are%20given%20a%20set%20of%20heterogeneous%20datasets%20collected%20from%20related%20source%20tasks%20and%20hope%20to%20enhance%20the%20performance%20above%20what%20we%20could%20hope%20to%20achieve%20by%20solving%20each%20of%20them%20individually.%20The%20recent%20work%20of%20arXiv%3A2006.15785%20has%20showed%20that%2C%20without%20access%20to%20distributional%20information%2C%20no%20algorithm%20based%20on%20aggregating%20samples%20alone%20can%20guarantee%20optimal%20risk%20as%20long%20as%20the%20sample%20size%20per%20task%20is%20bounded.%0A%20%20In%20this%20paper%2C%20we%20focus%20on%20understanding%20the%20statistical%20limits%20of%20multitask%20learning.%20We%20go%20beyond%20the%20no-free-lunch%20theorem%20in%20arXiv%3A2006.15785%20by%20establishing%20a%20stronger%20impossibility%20result%20of%20adaptation%20that%20holds%20for%20arbitrarily%20large%20sample%20size%20per%20task.%20This%20improvement%20conveys%20an%20important%20message%20that%20the%20hardness%20of%20multitask%20learning%20cannot%20be%20overcame%20by%20having%20abundant%20data%20per%20task.%20We%20also%20discuss%20the%20notion%20of%20optimal%20adaptivity%20that%20may%20be%20of%20future%20interests.&entry.1838667208=http%3A//arxiv.org/abs/2601.20774v1&entry.124074799=Read"},
{"title": "VSCOUT: A Hybrid Variational Autoencoder Approach to Outlier Detection in High-Dimensional Retrospective Monitoring", "author": "Waldyn G. Martinez", "abstract": "Modern industrial and service processes generate high-dimensional, non-Gaussian, and contamination-prone data that challenge the foundational assumptions of classical Statistical Process Control (SPC). Heavy tails, multimodality, nonlinear dependencies, and sparse special-cause observations can distort baseline estimation, mask true anomalies, and prevent reliable identification of an in-control (IC) reference set. To address these challenges, we introduce VSCOUT, a distribution-free framework designed specifically for retrospective (Phase I) monitoring in high-dimensional settings. VSCOUT combines an Automatic Relevance Determination Variational Autoencoder (ARD-VAE) architecture with ensemble-based latent outlier filtering and changepoint detection. The ARD prior isolates the most informative latent dimensions, while the ensemble and changepoint filters identify pointwise and structural contamination within the determined latent space. A second-stage retraining step removes flagged observations and re-estimates the latent structure using only the retained inliers, mitigating masking and stabilizing the IC latent manifold. This two-stage refinement produces a clean and reliable IC baseline suitable for subsequent Phase II deployment. Extensive experiments across benchmark datasets demonstrate that VSCOUT achieves superior sensitivity to special-cause structure while maintaining controlled false alarms, outperforming classical SPC procedures, robust estimators, and modern machine-learning baselines. Its scalability, distributional flexibility, and resilience to complex contamination patterns position VSCOUT as a practical and effective method for retrospective modeling and anomaly detection in AI-enabled environments.", "link": "http://arxiv.org/abs/2601.20830v1", "date": "2026-01-28", "relevancy": 2.4453, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5186}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4786}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.47}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VSCOUT%3A%20A%20Hybrid%20Variational%20Autoencoder%20Approach%20to%20Outlier%20Detection%20in%20High-Dimensional%20Retrospective%20Monitoring&body=Title%3A%20VSCOUT%3A%20A%20Hybrid%20Variational%20Autoencoder%20Approach%20to%20Outlier%20Detection%20in%20High-Dimensional%20Retrospective%20Monitoring%0AAuthor%3A%20Waldyn%20G.%20Martinez%0AAbstract%3A%20Modern%20industrial%20and%20service%20processes%20generate%20high-dimensional%2C%20non-Gaussian%2C%20and%20contamination-prone%20data%20that%20challenge%20the%20foundational%20assumptions%20of%20classical%20Statistical%20Process%20Control%20%28SPC%29.%20Heavy%20tails%2C%20multimodality%2C%20nonlinear%20dependencies%2C%20and%20sparse%20special-cause%20observations%20can%20distort%20baseline%20estimation%2C%20mask%20true%20anomalies%2C%20and%20prevent%20reliable%20identification%20of%20an%20in-control%20%28IC%29%20reference%20set.%20To%20address%20these%20challenges%2C%20we%20introduce%20VSCOUT%2C%20a%20distribution-free%20framework%20designed%20specifically%20for%20retrospective%20%28Phase%20I%29%20monitoring%20in%20high-dimensional%20settings.%20VSCOUT%20combines%20an%20Automatic%20Relevance%20Determination%20Variational%20Autoencoder%20%28ARD-VAE%29%20architecture%20with%20ensemble-based%20latent%20outlier%20filtering%20and%20changepoint%20detection.%20The%20ARD%20prior%20isolates%20the%20most%20informative%20latent%20dimensions%2C%20while%20the%20ensemble%20and%20changepoint%20filters%20identify%20pointwise%20and%20structural%20contamination%20within%20the%20determined%20latent%20space.%20A%20second-stage%20retraining%20step%20removes%20flagged%20observations%20and%20re-estimates%20the%20latent%20structure%20using%20only%20the%20retained%20inliers%2C%20mitigating%20masking%20and%20stabilizing%20the%20IC%20latent%20manifold.%20This%20two-stage%20refinement%20produces%20a%20clean%20and%20reliable%20IC%20baseline%20suitable%20for%20subsequent%20Phase%20II%20deployment.%20Extensive%20experiments%20across%20benchmark%20datasets%20demonstrate%20that%20VSCOUT%20achieves%20superior%20sensitivity%20to%20special-cause%20structure%20while%20maintaining%20controlled%20false%20alarms%2C%20outperforming%20classical%20SPC%20procedures%2C%20robust%20estimators%2C%20and%20modern%20machine-learning%20baselines.%20Its%20scalability%2C%20distributional%20flexibility%2C%20and%20resilience%20to%20complex%20contamination%20patterns%20position%20VSCOUT%20as%20a%20practical%20and%20effective%20method%20for%20retrospective%20modeling%20and%20anomaly%20detection%20in%20AI-enabled%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20830v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVSCOUT%253A%2520A%2520Hybrid%2520Variational%2520Autoencoder%2520Approach%2520to%2520Outlier%2520Detection%2520in%2520High-Dimensional%2520Retrospective%2520Monitoring%26entry.906535625%3DWaldyn%2520G.%2520Martinez%26entry.1292438233%3DModern%2520industrial%2520and%2520service%2520processes%2520generate%2520high-dimensional%252C%2520non-Gaussian%252C%2520and%2520contamination-prone%2520data%2520that%2520challenge%2520the%2520foundational%2520assumptions%2520of%2520classical%2520Statistical%2520Process%2520Control%2520%2528SPC%2529.%2520Heavy%2520tails%252C%2520multimodality%252C%2520nonlinear%2520dependencies%252C%2520and%2520sparse%2520special-cause%2520observations%2520can%2520distort%2520baseline%2520estimation%252C%2520mask%2520true%2520anomalies%252C%2520and%2520prevent%2520reliable%2520identification%2520of%2520an%2520in-control%2520%2528IC%2529%2520reference%2520set.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520VSCOUT%252C%2520a%2520distribution-free%2520framework%2520designed%2520specifically%2520for%2520retrospective%2520%2528Phase%2520I%2529%2520monitoring%2520in%2520high-dimensional%2520settings.%2520VSCOUT%2520combines%2520an%2520Automatic%2520Relevance%2520Determination%2520Variational%2520Autoencoder%2520%2528ARD-VAE%2529%2520architecture%2520with%2520ensemble-based%2520latent%2520outlier%2520filtering%2520and%2520changepoint%2520detection.%2520The%2520ARD%2520prior%2520isolates%2520the%2520most%2520informative%2520latent%2520dimensions%252C%2520while%2520the%2520ensemble%2520and%2520changepoint%2520filters%2520identify%2520pointwise%2520and%2520structural%2520contamination%2520within%2520the%2520determined%2520latent%2520space.%2520A%2520second-stage%2520retraining%2520step%2520removes%2520flagged%2520observations%2520and%2520re-estimates%2520the%2520latent%2520structure%2520using%2520only%2520the%2520retained%2520inliers%252C%2520mitigating%2520masking%2520and%2520stabilizing%2520the%2520IC%2520latent%2520manifold.%2520This%2520two-stage%2520refinement%2520produces%2520a%2520clean%2520and%2520reliable%2520IC%2520baseline%2520suitable%2520for%2520subsequent%2520Phase%2520II%2520deployment.%2520Extensive%2520experiments%2520across%2520benchmark%2520datasets%2520demonstrate%2520that%2520VSCOUT%2520achieves%2520superior%2520sensitivity%2520to%2520special-cause%2520structure%2520while%2520maintaining%2520controlled%2520false%2520alarms%252C%2520outperforming%2520classical%2520SPC%2520procedures%252C%2520robust%2520estimators%252C%2520and%2520modern%2520machine-learning%2520baselines.%2520Its%2520scalability%252C%2520distributional%2520flexibility%252C%2520and%2520resilience%2520to%2520complex%2520contamination%2520patterns%2520position%2520VSCOUT%2520as%2520a%2520practical%2520and%2520effective%2520method%2520for%2520retrospective%2520modeling%2520and%2520anomaly%2520detection%2520in%2520AI-enabled%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20830v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VSCOUT%3A%20A%20Hybrid%20Variational%20Autoencoder%20Approach%20to%20Outlier%20Detection%20in%20High-Dimensional%20Retrospective%20Monitoring&entry.906535625=Waldyn%20G.%20Martinez&entry.1292438233=Modern%20industrial%20and%20service%20processes%20generate%20high-dimensional%2C%20non-Gaussian%2C%20and%20contamination-prone%20data%20that%20challenge%20the%20foundational%20assumptions%20of%20classical%20Statistical%20Process%20Control%20%28SPC%29.%20Heavy%20tails%2C%20multimodality%2C%20nonlinear%20dependencies%2C%20and%20sparse%20special-cause%20observations%20can%20distort%20baseline%20estimation%2C%20mask%20true%20anomalies%2C%20and%20prevent%20reliable%20identification%20of%20an%20in-control%20%28IC%29%20reference%20set.%20To%20address%20these%20challenges%2C%20we%20introduce%20VSCOUT%2C%20a%20distribution-free%20framework%20designed%20specifically%20for%20retrospective%20%28Phase%20I%29%20monitoring%20in%20high-dimensional%20settings.%20VSCOUT%20combines%20an%20Automatic%20Relevance%20Determination%20Variational%20Autoencoder%20%28ARD-VAE%29%20architecture%20with%20ensemble-based%20latent%20outlier%20filtering%20and%20changepoint%20detection.%20The%20ARD%20prior%20isolates%20the%20most%20informative%20latent%20dimensions%2C%20while%20the%20ensemble%20and%20changepoint%20filters%20identify%20pointwise%20and%20structural%20contamination%20within%20the%20determined%20latent%20space.%20A%20second-stage%20retraining%20step%20removes%20flagged%20observations%20and%20re-estimates%20the%20latent%20structure%20using%20only%20the%20retained%20inliers%2C%20mitigating%20masking%20and%20stabilizing%20the%20IC%20latent%20manifold.%20This%20two-stage%20refinement%20produces%20a%20clean%20and%20reliable%20IC%20baseline%20suitable%20for%20subsequent%20Phase%20II%20deployment.%20Extensive%20experiments%20across%20benchmark%20datasets%20demonstrate%20that%20VSCOUT%20achieves%20superior%20sensitivity%20to%20special-cause%20structure%20while%20maintaining%20controlled%20false%20alarms%2C%20outperforming%20classical%20SPC%20procedures%2C%20robust%20estimators%2C%20and%20modern%20machine-learning%20baselines.%20Its%20scalability%2C%20distributional%20flexibility%2C%20and%20resilience%20to%20complex%20contamination%20patterns%20position%20VSCOUT%20as%20a%20practical%20and%20effective%20method%20for%20retrospective%20modeling%20and%20anomaly%20detection%20in%20AI-enabled%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2601.20830v1&entry.124074799=Read"},
{"title": "Latent Temporal Discrepancy as Motion Prior: A Loss-Weighting Strategy for Dynamic Fidelity in T2V", "author": "Meiqi Wu and Bingze Song and Ruimin Lin and Chen Zhu and Xiaokun Feng and Jiahong Wu and Xiangxiang Chu and Kaiqi Huang", "abstract": "Video generation models have achieved notable progress in static scenarios, yet their performance in motion video generation remains limited, with quality degrading under drastic dynamic changes. This is due to noise disrupting temporal coherence and increasing the difficulty of learning dynamic regions. {Unfortunately, existing diffusion models rely on static loss for all scenarios, constraining their ability to capture complex dynamics.} To address this issue, we introduce Latent Temporal Discrepancy (LTD) as a motion prior to guide loss weighting. LTD measures frame-to-frame variation in the latent space, assigning larger penalties to regions with higher discrepancy while maintaining regular optimization for stable regions. This motion-aware strategy stabilizes training and enables the model to better reconstruct high-frequency dynamics. Extensive experiments on the general benchmark VBench and the motion-focused VMBench show consistent gains, with our method outperforming strong baselines by 3.31% on VBench and 3.58% on VMBench, achieving significant improvements in motion quality.", "link": "http://arxiv.org/abs/2601.20504v1", "date": "2026-01-28", "relevancy": 2.4404, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6492}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6184}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5861}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Temporal%20Discrepancy%20as%20Motion%20Prior%3A%20A%20Loss-Weighting%20Strategy%20for%20Dynamic%20Fidelity%20in%20T2V&body=Title%3A%20Latent%20Temporal%20Discrepancy%20as%20Motion%20Prior%3A%20A%20Loss-Weighting%20Strategy%20for%20Dynamic%20Fidelity%20in%20T2V%0AAuthor%3A%20Meiqi%20Wu%20and%20Bingze%20Song%20and%20Ruimin%20Lin%20and%20Chen%20Zhu%20and%20Xiaokun%20Feng%20and%20Jiahong%20Wu%20and%20Xiangxiang%20Chu%20and%20Kaiqi%20Huang%0AAbstract%3A%20Video%20generation%20models%20have%20achieved%20notable%20progress%20in%20static%20scenarios%2C%20yet%20their%20performance%20in%20motion%20video%20generation%20remains%20limited%2C%20with%20quality%20degrading%20under%20drastic%20dynamic%20changes.%20This%20is%20due%20to%20noise%20disrupting%20temporal%20coherence%20and%20increasing%20the%20difficulty%20of%20learning%20dynamic%20regions.%20%7BUnfortunately%2C%20existing%20diffusion%20models%20rely%20on%20static%20loss%20for%20all%20scenarios%2C%20constraining%20their%20ability%20to%20capture%20complex%20dynamics.%7D%20To%20address%20this%20issue%2C%20we%20introduce%20Latent%20Temporal%20Discrepancy%20%28LTD%29%20as%20a%20motion%20prior%20to%20guide%20loss%20weighting.%20LTD%20measures%20frame-to-frame%20variation%20in%20the%20latent%20space%2C%20assigning%20larger%20penalties%20to%20regions%20with%20higher%20discrepancy%20while%20maintaining%20regular%20optimization%20for%20stable%20regions.%20This%20motion-aware%20strategy%20stabilizes%20training%20and%20enables%20the%20model%20to%20better%20reconstruct%20high-frequency%20dynamics.%20Extensive%20experiments%20on%20the%20general%20benchmark%20VBench%20and%20the%20motion-focused%20VMBench%20show%20consistent%20gains%2C%20with%20our%20method%20outperforming%20strong%20baselines%20by%203.31%25%20on%20VBench%20and%203.58%25%20on%20VMBench%2C%20achieving%20significant%20improvements%20in%20motion%20quality.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20504v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Temporal%2520Discrepancy%2520as%2520Motion%2520Prior%253A%2520A%2520Loss-Weighting%2520Strategy%2520for%2520Dynamic%2520Fidelity%2520in%2520T2V%26entry.906535625%3DMeiqi%2520Wu%2520and%2520Bingze%2520Song%2520and%2520Ruimin%2520Lin%2520and%2520Chen%2520Zhu%2520and%2520Xiaokun%2520Feng%2520and%2520Jiahong%2520Wu%2520and%2520Xiangxiang%2520Chu%2520and%2520Kaiqi%2520Huang%26entry.1292438233%3DVideo%2520generation%2520models%2520have%2520achieved%2520notable%2520progress%2520in%2520static%2520scenarios%252C%2520yet%2520their%2520performance%2520in%2520motion%2520video%2520generation%2520remains%2520limited%252C%2520with%2520quality%2520degrading%2520under%2520drastic%2520dynamic%2520changes.%2520This%2520is%2520due%2520to%2520noise%2520disrupting%2520temporal%2520coherence%2520and%2520increasing%2520the%2520difficulty%2520of%2520learning%2520dynamic%2520regions.%2520%257BUnfortunately%252C%2520existing%2520diffusion%2520models%2520rely%2520on%2520static%2520loss%2520for%2520all%2520scenarios%252C%2520constraining%2520their%2520ability%2520to%2520capture%2520complex%2520dynamics.%257D%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520Latent%2520Temporal%2520Discrepancy%2520%2528LTD%2529%2520as%2520a%2520motion%2520prior%2520to%2520guide%2520loss%2520weighting.%2520LTD%2520measures%2520frame-to-frame%2520variation%2520in%2520the%2520latent%2520space%252C%2520assigning%2520larger%2520penalties%2520to%2520regions%2520with%2520higher%2520discrepancy%2520while%2520maintaining%2520regular%2520optimization%2520for%2520stable%2520regions.%2520This%2520motion-aware%2520strategy%2520stabilizes%2520training%2520and%2520enables%2520the%2520model%2520to%2520better%2520reconstruct%2520high-frequency%2520dynamics.%2520Extensive%2520experiments%2520on%2520the%2520general%2520benchmark%2520VBench%2520and%2520the%2520motion-focused%2520VMBench%2520show%2520consistent%2520gains%252C%2520with%2520our%2520method%2520outperforming%2520strong%2520baselines%2520by%25203.31%2525%2520on%2520VBench%2520and%25203.58%2525%2520on%2520VMBench%252C%2520achieving%2520significant%2520improvements%2520in%2520motion%2520quality.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20504v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Temporal%20Discrepancy%20as%20Motion%20Prior%3A%20A%20Loss-Weighting%20Strategy%20for%20Dynamic%20Fidelity%20in%20T2V&entry.906535625=Meiqi%20Wu%20and%20Bingze%20Song%20and%20Ruimin%20Lin%20and%20Chen%20Zhu%20and%20Xiaokun%20Feng%20and%20Jiahong%20Wu%20and%20Xiangxiang%20Chu%20and%20Kaiqi%20Huang&entry.1292438233=Video%20generation%20models%20have%20achieved%20notable%20progress%20in%20static%20scenarios%2C%20yet%20their%20performance%20in%20motion%20video%20generation%20remains%20limited%2C%20with%20quality%20degrading%20under%20drastic%20dynamic%20changes.%20This%20is%20due%20to%20noise%20disrupting%20temporal%20coherence%20and%20increasing%20the%20difficulty%20of%20learning%20dynamic%20regions.%20%7BUnfortunately%2C%20existing%20diffusion%20models%20rely%20on%20static%20loss%20for%20all%20scenarios%2C%20constraining%20their%20ability%20to%20capture%20complex%20dynamics.%7D%20To%20address%20this%20issue%2C%20we%20introduce%20Latent%20Temporal%20Discrepancy%20%28LTD%29%20as%20a%20motion%20prior%20to%20guide%20loss%20weighting.%20LTD%20measures%20frame-to-frame%20variation%20in%20the%20latent%20space%2C%20assigning%20larger%20penalties%20to%20regions%20with%20higher%20discrepancy%20while%20maintaining%20regular%20optimization%20for%20stable%20regions.%20This%20motion-aware%20strategy%20stabilizes%20training%20and%20enables%20the%20model%20to%20better%20reconstruct%20high-frequency%20dynamics.%20Extensive%20experiments%20on%20the%20general%20benchmark%20VBench%20and%20the%20motion-focused%20VMBench%20show%20consistent%20gains%2C%20with%20our%20method%20outperforming%20strong%20baselines%20by%203.31%25%20on%20VBench%20and%203.58%25%20on%20VMBench%2C%20achieving%20significant%20improvements%20in%20motion%20quality.&entry.1838667208=http%3A//arxiv.org/abs/2601.20504v1&entry.124074799=Read"},
{"title": "Mechanism of Task-oriented Information Removal in In-context Learning", "author": "Hakaze Cho and Haolin Yang and Gouki Minegishi and Naoya Inoue", "abstract": "In-context Learning (ICL) is an emerging few-shot learning paradigm based on modern Language Models (LMs), yet its inner mechanism remains unclear. In this paper, we investigate the mechanism through a novel perspective of information removal. Specifically, we demonstrate that in the zero-shot scenario, LMs encode queries into non-selective representations in hidden states containing information for all possible tasks, leading to arbitrary outputs without focusing on the intended task, resulting in near-zero accuracy. Meanwhile, we find that selectively removing specific information from hidden states by a low-rank filter effectively steers LMs toward the intended task. Building on these findings, by measuring the hidden states on carefully designed metrics, we observe that few-shot ICL effectively simulates such task-oriented information removal processes, selectively removing the redundant information from entangled non-selective representations, and improving the output based on the demonstrations, which constitutes a key mechanism underlying ICL. Moreover, we identify essential attention heads inducing the removal operation, termed Denoising Heads, which enables the ablation experiments blocking the information removal operation from the inference, where the ICL accuracy significantly degrades, especially when the correct label is absent from the few-shot demonstrations, confirming both the critical role of the information removal mechanism and denoising heads.", "link": "http://arxiv.org/abs/2509.21012v3", "date": "2026-01-28", "relevancy": 2.4323, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4921}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4921}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4752}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mechanism%20of%20Task-oriented%20Information%20Removal%20in%20In-context%20Learning&body=Title%3A%20Mechanism%20of%20Task-oriented%20Information%20Removal%20in%20In-context%20Learning%0AAuthor%3A%20Hakaze%20Cho%20and%20Haolin%20Yang%20and%20Gouki%20Minegishi%20and%20Naoya%20Inoue%0AAbstract%3A%20In-context%20Learning%20%28ICL%29%20is%20an%20emerging%20few-shot%20learning%20paradigm%20based%20on%20modern%20Language%20Models%20%28LMs%29%2C%20yet%20its%20inner%20mechanism%20remains%20unclear.%20In%20this%20paper%2C%20we%20investigate%20the%20mechanism%20through%20a%20novel%20perspective%20of%20information%20removal.%20Specifically%2C%20we%20demonstrate%20that%20in%20the%20zero-shot%20scenario%2C%20LMs%20encode%20queries%20into%20non-selective%20representations%20in%20hidden%20states%20containing%20information%20for%20all%20possible%20tasks%2C%20leading%20to%20arbitrary%20outputs%20without%20focusing%20on%20the%20intended%20task%2C%20resulting%20in%20near-zero%20accuracy.%20Meanwhile%2C%20we%20find%20that%20selectively%20removing%20specific%20information%20from%20hidden%20states%20by%20a%20low-rank%20filter%20effectively%20steers%20LMs%20toward%20the%20intended%20task.%20Building%20on%20these%20findings%2C%20by%20measuring%20the%20hidden%20states%20on%20carefully%20designed%20metrics%2C%20we%20observe%20that%20few-shot%20ICL%20effectively%20simulates%20such%20task-oriented%20information%20removal%20processes%2C%20selectively%20removing%20the%20redundant%20information%20from%20entangled%20non-selective%20representations%2C%20and%20improving%20the%20output%20based%20on%20the%20demonstrations%2C%20which%20constitutes%20a%20key%20mechanism%20underlying%20ICL.%20Moreover%2C%20we%20identify%20essential%20attention%20heads%20inducing%20the%20removal%20operation%2C%20termed%20Denoising%20Heads%2C%20which%20enables%20the%20ablation%20experiments%20blocking%20the%20information%20removal%20operation%20from%20the%20inference%2C%20where%20the%20ICL%20accuracy%20significantly%20degrades%2C%20especially%20when%20the%20correct%20label%20is%20absent%20from%20the%20few-shot%20demonstrations%2C%20confirming%20both%20the%20critical%20role%20of%20the%20information%20removal%20mechanism%20and%20denoising%20heads.%0ALink%3A%20http%3A//arxiv.org/abs/2509.21012v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMechanism%2520of%2520Task-oriented%2520Information%2520Removal%2520in%2520In-context%2520Learning%26entry.906535625%3DHakaze%2520Cho%2520and%2520Haolin%2520Yang%2520and%2520Gouki%2520Minegishi%2520and%2520Naoya%2520Inoue%26entry.1292438233%3DIn-context%2520Learning%2520%2528ICL%2529%2520is%2520an%2520emerging%2520few-shot%2520learning%2520paradigm%2520based%2520on%2520modern%2520Language%2520Models%2520%2528LMs%2529%252C%2520yet%2520its%2520inner%2520mechanism%2520remains%2520unclear.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520mechanism%2520through%2520a%2520novel%2520perspective%2520of%2520information%2520removal.%2520Specifically%252C%2520we%2520demonstrate%2520that%2520in%2520the%2520zero-shot%2520scenario%252C%2520LMs%2520encode%2520queries%2520into%2520non-selective%2520representations%2520in%2520hidden%2520states%2520containing%2520information%2520for%2520all%2520possible%2520tasks%252C%2520leading%2520to%2520arbitrary%2520outputs%2520without%2520focusing%2520on%2520the%2520intended%2520task%252C%2520resulting%2520in%2520near-zero%2520accuracy.%2520Meanwhile%252C%2520we%2520find%2520that%2520selectively%2520removing%2520specific%2520information%2520from%2520hidden%2520states%2520by%2520a%2520low-rank%2520filter%2520effectively%2520steers%2520LMs%2520toward%2520the%2520intended%2520task.%2520Building%2520on%2520these%2520findings%252C%2520by%2520measuring%2520the%2520hidden%2520states%2520on%2520carefully%2520designed%2520metrics%252C%2520we%2520observe%2520that%2520few-shot%2520ICL%2520effectively%2520simulates%2520such%2520task-oriented%2520information%2520removal%2520processes%252C%2520selectively%2520removing%2520the%2520redundant%2520information%2520from%2520entangled%2520non-selective%2520representations%252C%2520and%2520improving%2520the%2520output%2520based%2520on%2520the%2520demonstrations%252C%2520which%2520constitutes%2520a%2520key%2520mechanism%2520underlying%2520ICL.%2520Moreover%252C%2520we%2520identify%2520essential%2520attention%2520heads%2520inducing%2520the%2520removal%2520operation%252C%2520termed%2520Denoising%2520Heads%252C%2520which%2520enables%2520the%2520ablation%2520experiments%2520blocking%2520the%2520information%2520removal%2520operation%2520from%2520the%2520inference%252C%2520where%2520the%2520ICL%2520accuracy%2520significantly%2520degrades%252C%2520especially%2520when%2520the%2520correct%2520label%2520is%2520absent%2520from%2520the%2520few-shot%2520demonstrations%252C%2520confirming%2520both%2520the%2520critical%2520role%2520of%2520the%2520information%2520removal%2520mechanism%2520and%2520denoising%2520heads.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21012v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mechanism%20of%20Task-oriented%20Information%20Removal%20in%20In-context%20Learning&entry.906535625=Hakaze%20Cho%20and%20Haolin%20Yang%20and%20Gouki%20Minegishi%20and%20Naoya%20Inoue&entry.1292438233=In-context%20Learning%20%28ICL%29%20is%20an%20emerging%20few-shot%20learning%20paradigm%20based%20on%20modern%20Language%20Models%20%28LMs%29%2C%20yet%20its%20inner%20mechanism%20remains%20unclear.%20In%20this%20paper%2C%20we%20investigate%20the%20mechanism%20through%20a%20novel%20perspective%20of%20information%20removal.%20Specifically%2C%20we%20demonstrate%20that%20in%20the%20zero-shot%20scenario%2C%20LMs%20encode%20queries%20into%20non-selective%20representations%20in%20hidden%20states%20containing%20information%20for%20all%20possible%20tasks%2C%20leading%20to%20arbitrary%20outputs%20without%20focusing%20on%20the%20intended%20task%2C%20resulting%20in%20near-zero%20accuracy.%20Meanwhile%2C%20we%20find%20that%20selectively%20removing%20specific%20information%20from%20hidden%20states%20by%20a%20low-rank%20filter%20effectively%20steers%20LMs%20toward%20the%20intended%20task.%20Building%20on%20these%20findings%2C%20by%20measuring%20the%20hidden%20states%20on%20carefully%20designed%20metrics%2C%20we%20observe%20that%20few-shot%20ICL%20effectively%20simulates%20such%20task-oriented%20information%20removal%20processes%2C%20selectively%20removing%20the%20redundant%20information%20from%20entangled%20non-selective%20representations%2C%20and%20improving%20the%20output%20based%20on%20the%20demonstrations%2C%20which%20constitutes%20a%20key%20mechanism%20underlying%20ICL.%20Moreover%2C%20we%20identify%20essential%20attention%20heads%20inducing%20the%20removal%20operation%2C%20termed%20Denoising%20Heads%2C%20which%20enables%20the%20ablation%20experiments%20blocking%20the%20information%20removal%20operation%20from%20the%20inference%2C%20where%20the%20ICL%20accuracy%20significantly%20degrades%2C%20especially%20when%20the%20correct%20label%20is%20absent%20from%20the%20few-shot%20demonstrations%2C%20confirming%20both%20the%20critical%20role%20of%20the%20information%20removal%20mechanism%20and%20denoising%20heads.&entry.1838667208=http%3A//arxiv.org/abs/2509.21012v3&entry.124074799=Read"},
{"title": "Structurally Human, Semantically Biased: Detecting LLM-Generated References with Embeddings and GNNs", "author": "Melika Mobini and Vincent Holst and Floriano Tori and Andres Algaba and Vincent Ginis", "abstract": "Large language models are increasingly used to curate bibliographies, raising the question: are their reference lists distinguishable from human ones? We build paired citation graphs, ground truth and GPT-4o-generated (from parametric knowledge), for 10,000 focal papers ($\\approx$ 275k references) from SciSciNet, and added a field-matched random baseline that preserves out-degree and field distributions while breaking latent structure. We compare (i) structure-only node features (degree/closeness/eigenvector centrality, clustering, edge count) with (ii) 3072-D title/abstract embeddings, using an RF on graph-level aggregates and Graph Neural Networks with node features. Structure alone barely separates GPT from ground truth (RF accuracy $\\approx$ 0.60) despite cleanly rejecting the random baseline ($\\approx$ 0.89--0.92). By contrast, embeddings sharply increase separability: RF on aggregated embeddings reaches $\\approx$ 0.83, and GNNs with embedding node features achieve 93\\% test accuracy on GPT vs.\\ ground truth. We show the robustness of our findings by replicating the pipeline with Claude Sonnet 4.5 and with multiple embedding models (OpenAI and SPECTER), with RF separability for ground truth vs.\\ Claude $\\approx 0.77$ and clean rejection of the random baseline. Thus, LLM bibliographies, generated purely from parametric knowledge, closely mimic human citation topology, but leave detectable semantic fingerprints; detection and debiasing should target content signals rather than global graph structure.", "link": "http://arxiv.org/abs/2601.20704v1", "date": "2026-01-28", "relevancy": 2.4311, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4884}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4884}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4819}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structurally%20Human%2C%20Semantically%20Biased%3A%20Detecting%20LLM-Generated%20References%20with%20Embeddings%20and%20GNNs&body=Title%3A%20Structurally%20Human%2C%20Semantically%20Biased%3A%20Detecting%20LLM-Generated%20References%20with%20Embeddings%20and%20GNNs%0AAuthor%3A%20Melika%20Mobini%20and%20Vincent%20Holst%20and%20Floriano%20Tori%20and%20Andres%20Algaba%20and%20Vincent%20Ginis%0AAbstract%3A%20Large%20language%20models%20are%20increasingly%20used%20to%20curate%20bibliographies%2C%20raising%20the%20question%3A%20are%20their%20reference%20lists%20distinguishable%20from%20human%20ones%3F%20We%20build%20paired%20citation%20graphs%2C%20ground%20truth%20and%20GPT-4o-generated%20%28from%20parametric%20knowledge%29%2C%20for%2010%2C000%20focal%20papers%20%28%24%5Capprox%24%20275k%20references%29%20from%20SciSciNet%2C%20and%20added%20a%20field-matched%20random%20baseline%20that%20preserves%20out-degree%20and%20field%20distributions%20while%20breaking%20latent%20structure.%20We%20compare%20%28i%29%20structure-only%20node%20features%20%28degree/closeness/eigenvector%20centrality%2C%20clustering%2C%20edge%20count%29%20with%20%28ii%29%203072-D%20title/abstract%20embeddings%2C%20using%20an%20RF%20on%20graph-level%20aggregates%20and%20Graph%20Neural%20Networks%20with%20node%20features.%20Structure%20alone%20barely%20separates%20GPT%20from%20ground%20truth%20%28RF%20accuracy%20%24%5Capprox%24%200.60%29%20despite%20cleanly%20rejecting%20the%20random%20baseline%20%28%24%5Capprox%24%200.89--0.92%29.%20By%20contrast%2C%20embeddings%20sharply%20increase%20separability%3A%20RF%20on%20aggregated%20embeddings%20reaches%20%24%5Capprox%24%200.83%2C%20and%20GNNs%20with%20embedding%20node%20features%20achieve%2093%5C%25%20test%20accuracy%20on%20GPT%20vs.%5C%20ground%20truth.%20We%20show%20the%20robustness%20of%20our%20findings%20by%20replicating%20the%20pipeline%20with%20Claude%20Sonnet%204.5%20and%20with%20multiple%20embedding%20models%20%28OpenAI%20and%20SPECTER%29%2C%20with%20RF%20separability%20for%20ground%20truth%20vs.%5C%20Claude%20%24%5Capprox%200.77%24%20and%20clean%20rejection%20of%20the%20random%20baseline.%20Thus%2C%20LLM%20bibliographies%2C%20generated%20purely%20from%20parametric%20knowledge%2C%20closely%20mimic%20human%20citation%20topology%2C%20but%20leave%20detectable%20semantic%20fingerprints%3B%20detection%20and%20debiasing%20should%20target%20content%20signals%20rather%20than%20global%20graph%20structure.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20704v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructurally%2520Human%252C%2520Semantically%2520Biased%253A%2520Detecting%2520LLM-Generated%2520References%2520with%2520Embeddings%2520and%2520GNNs%26entry.906535625%3DMelika%2520Mobini%2520and%2520Vincent%2520Holst%2520and%2520Floriano%2520Tori%2520and%2520Andres%2520Algaba%2520and%2520Vincent%2520Ginis%26entry.1292438233%3DLarge%2520language%2520models%2520are%2520increasingly%2520used%2520to%2520curate%2520bibliographies%252C%2520raising%2520the%2520question%253A%2520are%2520their%2520reference%2520lists%2520distinguishable%2520from%2520human%2520ones%253F%2520We%2520build%2520paired%2520citation%2520graphs%252C%2520ground%2520truth%2520and%2520GPT-4o-generated%2520%2528from%2520parametric%2520knowledge%2529%252C%2520for%252010%252C000%2520focal%2520papers%2520%2528%2524%255Capprox%2524%2520275k%2520references%2529%2520from%2520SciSciNet%252C%2520and%2520added%2520a%2520field-matched%2520random%2520baseline%2520that%2520preserves%2520out-degree%2520and%2520field%2520distributions%2520while%2520breaking%2520latent%2520structure.%2520We%2520compare%2520%2528i%2529%2520structure-only%2520node%2520features%2520%2528degree/closeness/eigenvector%2520centrality%252C%2520clustering%252C%2520edge%2520count%2529%2520with%2520%2528ii%2529%25203072-D%2520title/abstract%2520embeddings%252C%2520using%2520an%2520RF%2520on%2520graph-level%2520aggregates%2520and%2520Graph%2520Neural%2520Networks%2520with%2520node%2520features.%2520Structure%2520alone%2520barely%2520separates%2520GPT%2520from%2520ground%2520truth%2520%2528RF%2520accuracy%2520%2524%255Capprox%2524%25200.60%2529%2520despite%2520cleanly%2520rejecting%2520the%2520random%2520baseline%2520%2528%2524%255Capprox%2524%25200.89--0.92%2529.%2520By%2520contrast%252C%2520embeddings%2520sharply%2520increase%2520separability%253A%2520RF%2520on%2520aggregated%2520embeddings%2520reaches%2520%2524%255Capprox%2524%25200.83%252C%2520and%2520GNNs%2520with%2520embedding%2520node%2520features%2520achieve%252093%255C%2525%2520test%2520accuracy%2520on%2520GPT%2520vs.%255C%2520ground%2520truth.%2520We%2520show%2520the%2520robustness%2520of%2520our%2520findings%2520by%2520replicating%2520the%2520pipeline%2520with%2520Claude%2520Sonnet%25204.5%2520and%2520with%2520multiple%2520embedding%2520models%2520%2528OpenAI%2520and%2520SPECTER%2529%252C%2520with%2520RF%2520separability%2520for%2520ground%2520truth%2520vs.%255C%2520Claude%2520%2524%255Capprox%25200.77%2524%2520and%2520clean%2520rejection%2520of%2520the%2520random%2520baseline.%2520Thus%252C%2520LLM%2520bibliographies%252C%2520generated%2520purely%2520from%2520parametric%2520knowledge%252C%2520closely%2520mimic%2520human%2520citation%2520topology%252C%2520but%2520leave%2520detectable%2520semantic%2520fingerprints%253B%2520detection%2520and%2520debiasing%2520should%2520target%2520content%2520signals%2520rather%2520than%2520global%2520graph%2520structure.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20704v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structurally%20Human%2C%20Semantically%20Biased%3A%20Detecting%20LLM-Generated%20References%20with%20Embeddings%20and%20GNNs&entry.906535625=Melika%20Mobini%20and%20Vincent%20Holst%20and%20Floriano%20Tori%20and%20Andres%20Algaba%20and%20Vincent%20Ginis&entry.1292438233=Large%20language%20models%20are%20increasingly%20used%20to%20curate%20bibliographies%2C%20raising%20the%20question%3A%20are%20their%20reference%20lists%20distinguishable%20from%20human%20ones%3F%20We%20build%20paired%20citation%20graphs%2C%20ground%20truth%20and%20GPT-4o-generated%20%28from%20parametric%20knowledge%29%2C%20for%2010%2C000%20focal%20papers%20%28%24%5Capprox%24%20275k%20references%29%20from%20SciSciNet%2C%20and%20added%20a%20field-matched%20random%20baseline%20that%20preserves%20out-degree%20and%20field%20distributions%20while%20breaking%20latent%20structure.%20We%20compare%20%28i%29%20structure-only%20node%20features%20%28degree/closeness/eigenvector%20centrality%2C%20clustering%2C%20edge%20count%29%20with%20%28ii%29%203072-D%20title/abstract%20embeddings%2C%20using%20an%20RF%20on%20graph-level%20aggregates%20and%20Graph%20Neural%20Networks%20with%20node%20features.%20Structure%20alone%20barely%20separates%20GPT%20from%20ground%20truth%20%28RF%20accuracy%20%24%5Capprox%24%200.60%29%20despite%20cleanly%20rejecting%20the%20random%20baseline%20%28%24%5Capprox%24%200.89--0.92%29.%20By%20contrast%2C%20embeddings%20sharply%20increase%20separability%3A%20RF%20on%20aggregated%20embeddings%20reaches%20%24%5Capprox%24%200.83%2C%20and%20GNNs%20with%20embedding%20node%20features%20achieve%2093%5C%25%20test%20accuracy%20on%20GPT%20vs.%5C%20ground%20truth.%20We%20show%20the%20robustness%20of%20our%20findings%20by%20replicating%20the%20pipeline%20with%20Claude%20Sonnet%204.5%20and%20with%20multiple%20embedding%20models%20%28OpenAI%20and%20SPECTER%29%2C%20with%20RF%20separability%20for%20ground%20truth%20vs.%5C%20Claude%20%24%5Capprox%200.77%24%20and%20clean%20rejection%20of%20the%20random%20baseline.%20Thus%2C%20LLM%20bibliographies%2C%20generated%20purely%20from%20parametric%20knowledge%2C%20closely%20mimic%20human%20citation%20topology%2C%20but%20leave%20detectable%20semantic%20fingerprints%3B%20detection%20and%20debiasing%20should%20target%20content%20signals%20rather%20than%20global%20graph%20structure.&entry.1838667208=http%3A//arxiv.org/abs/2601.20704v1&entry.124074799=Read"},
{"title": "Li-ViP3D++: Query-Gated Deformable Camera-LiDAR Fusion for End-to-End Perception and Trajectory Prediction", "author": "Matej Halinkovic and Nina Masarykova and Alexey Vinel and Marek Galinski", "abstract": "End-to-end perception and trajectory prediction from raw sensor data is one of the key capabilities for autonomous driving. Modular pipelines restrict information flow and can amplify upstream errors. Recent query-based, fully differentiable perception-and-prediction (PnP) models mitigate these issues, yet the complementarity of cameras and LiDAR in the query-space has not been sufficiently explored. Models often rely on fusion schemes that introduce heuristic alignment and discrete selection steps which prevent full utilization of available information and can introduce unwanted bias. We propose Li-ViP3D++, a query-based multimodal PnP framework that introduces Query-Gated Deformable Fusion (QGDF) to integrate multi-view RGB and LiDAR in query space. QGDF (i) aggregates image evidence via masked attention across cameras and feature levels, (ii) extracts LiDAR context through fully differentiable BEV sampling with learned per-query offsets, and (iii) applies query-conditioned gating to adaptively weight visual and geometric cues per agent. The resulting architecture jointly optimizes detection, tracking, and multi-hypothesis trajectory forecasting in a single end-to-end model. On nuScenes, Li-ViP3D++ improves end-to-end behavior and detection quality, achieving higher EPA (0.335) and mAP (0.502) while substantially reducing false positives (FP ratio 0.147), and it is faster than the prior Li-ViP3D variant (139.82 ms vs. 145.91 ms). These results indicate that query-space, fully differentiable camera-LiDAR fusion can increase robustness of end-to-end PnP without sacrificing deployability.", "link": "http://arxiv.org/abs/2601.20720v1", "date": "2026-01-28", "relevancy": 2.4295, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6205}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6051}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Li-ViP3D%2B%2B%3A%20Query-Gated%20Deformable%20Camera-LiDAR%20Fusion%20for%20End-to-End%20Perception%20and%20Trajectory%20Prediction&body=Title%3A%20Li-ViP3D%2B%2B%3A%20Query-Gated%20Deformable%20Camera-LiDAR%20Fusion%20for%20End-to-End%20Perception%20and%20Trajectory%20Prediction%0AAuthor%3A%20Matej%20Halinkovic%20and%20Nina%20Masarykova%20and%20Alexey%20Vinel%20and%20Marek%20Galinski%0AAbstract%3A%20End-to-end%20perception%20and%20trajectory%20prediction%20from%20raw%20sensor%20data%20is%20one%20of%20the%20key%20capabilities%20for%20autonomous%20driving.%20Modular%20pipelines%20restrict%20information%20flow%20and%20can%20amplify%20upstream%20errors.%20Recent%20query-based%2C%20fully%20differentiable%20perception-and-prediction%20%28PnP%29%20models%20mitigate%20these%20issues%2C%20yet%20the%20complementarity%20of%20cameras%20and%20LiDAR%20in%20the%20query-space%20has%20not%20been%20sufficiently%20explored.%20Models%20often%20rely%20on%20fusion%20schemes%20that%20introduce%20heuristic%20alignment%20and%20discrete%20selection%20steps%20which%20prevent%20full%20utilization%20of%20available%20information%20and%20can%20introduce%20unwanted%20bias.%20We%20propose%20Li-ViP3D%2B%2B%2C%20a%20query-based%20multimodal%20PnP%20framework%20that%20introduces%20Query-Gated%20Deformable%20Fusion%20%28QGDF%29%20to%20integrate%20multi-view%20RGB%20and%20LiDAR%20in%20query%20space.%20QGDF%20%28i%29%20aggregates%20image%20evidence%20via%20masked%20attention%20across%20cameras%20and%20feature%20levels%2C%20%28ii%29%20extracts%20LiDAR%20context%20through%20fully%20differentiable%20BEV%20sampling%20with%20learned%20per-query%20offsets%2C%20and%20%28iii%29%20applies%20query-conditioned%20gating%20to%20adaptively%20weight%20visual%20and%20geometric%20cues%20per%20agent.%20The%20resulting%20architecture%20jointly%20optimizes%20detection%2C%20tracking%2C%20and%20multi-hypothesis%20trajectory%20forecasting%20in%20a%20single%20end-to-end%20model.%20On%20nuScenes%2C%20Li-ViP3D%2B%2B%20improves%20end-to-end%20behavior%20and%20detection%20quality%2C%20achieving%20higher%20EPA%20%280.335%29%20and%20mAP%20%280.502%29%20while%20substantially%20reducing%20false%20positives%20%28FP%20ratio%200.147%29%2C%20and%20it%20is%20faster%20than%20the%20prior%20Li-ViP3D%20variant%20%28139.82%20ms%20vs.%20145.91%20ms%29.%20These%20results%20indicate%20that%20query-space%2C%20fully%20differentiable%20camera-LiDAR%20fusion%20can%20increase%20robustness%20of%20end-to-end%20PnP%20without%20sacrificing%20deployability.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20720v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLi-ViP3D%252B%252B%253A%2520Query-Gated%2520Deformable%2520Camera-LiDAR%2520Fusion%2520for%2520End-to-End%2520Perception%2520and%2520Trajectory%2520Prediction%26entry.906535625%3DMatej%2520Halinkovic%2520and%2520Nina%2520Masarykova%2520and%2520Alexey%2520Vinel%2520and%2520Marek%2520Galinski%26entry.1292438233%3DEnd-to-end%2520perception%2520and%2520trajectory%2520prediction%2520from%2520raw%2520sensor%2520data%2520is%2520one%2520of%2520the%2520key%2520capabilities%2520for%2520autonomous%2520driving.%2520Modular%2520pipelines%2520restrict%2520information%2520flow%2520and%2520can%2520amplify%2520upstream%2520errors.%2520Recent%2520query-based%252C%2520fully%2520differentiable%2520perception-and-prediction%2520%2528PnP%2529%2520models%2520mitigate%2520these%2520issues%252C%2520yet%2520the%2520complementarity%2520of%2520cameras%2520and%2520LiDAR%2520in%2520the%2520query-space%2520has%2520not%2520been%2520sufficiently%2520explored.%2520Models%2520often%2520rely%2520on%2520fusion%2520schemes%2520that%2520introduce%2520heuristic%2520alignment%2520and%2520discrete%2520selection%2520steps%2520which%2520prevent%2520full%2520utilization%2520of%2520available%2520information%2520and%2520can%2520introduce%2520unwanted%2520bias.%2520We%2520propose%2520Li-ViP3D%252B%252B%252C%2520a%2520query-based%2520multimodal%2520PnP%2520framework%2520that%2520introduces%2520Query-Gated%2520Deformable%2520Fusion%2520%2528QGDF%2529%2520to%2520integrate%2520multi-view%2520RGB%2520and%2520LiDAR%2520in%2520query%2520space.%2520QGDF%2520%2528i%2529%2520aggregates%2520image%2520evidence%2520via%2520masked%2520attention%2520across%2520cameras%2520and%2520feature%2520levels%252C%2520%2528ii%2529%2520extracts%2520LiDAR%2520context%2520through%2520fully%2520differentiable%2520BEV%2520sampling%2520with%2520learned%2520per-query%2520offsets%252C%2520and%2520%2528iii%2529%2520applies%2520query-conditioned%2520gating%2520to%2520adaptively%2520weight%2520visual%2520and%2520geometric%2520cues%2520per%2520agent.%2520The%2520resulting%2520architecture%2520jointly%2520optimizes%2520detection%252C%2520tracking%252C%2520and%2520multi-hypothesis%2520trajectory%2520forecasting%2520in%2520a%2520single%2520end-to-end%2520model.%2520On%2520nuScenes%252C%2520Li-ViP3D%252B%252B%2520improves%2520end-to-end%2520behavior%2520and%2520detection%2520quality%252C%2520achieving%2520higher%2520EPA%2520%25280.335%2529%2520and%2520mAP%2520%25280.502%2529%2520while%2520substantially%2520reducing%2520false%2520positives%2520%2528FP%2520ratio%25200.147%2529%252C%2520and%2520it%2520is%2520faster%2520than%2520the%2520prior%2520Li-ViP3D%2520variant%2520%2528139.82%2520ms%2520vs.%2520145.91%2520ms%2529.%2520These%2520results%2520indicate%2520that%2520query-space%252C%2520fully%2520differentiable%2520camera-LiDAR%2520fusion%2520can%2520increase%2520robustness%2520of%2520end-to-end%2520PnP%2520without%2520sacrificing%2520deployability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20720v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Li-ViP3D%2B%2B%3A%20Query-Gated%20Deformable%20Camera-LiDAR%20Fusion%20for%20End-to-End%20Perception%20and%20Trajectory%20Prediction&entry.906535625=Matej%20Halinkovic%20and%20Nina%20Masarykova%20and%20Alexey%20Vinel%20and%20Marek%20Galinski&entry.1292438233=End-to-end%20perception%20and%20trajectory%20prediction%20from%20raw%20sensor%20data%20is%20one%20of%20the%20key%20capabilities%20for%20autonomous%20driving.%20Modular%20pipelines%20restrict%20information%20flow%20and%20can%20amplify%20upstream%20errors.%20Recent%20query-based%2C%20fully%20differentiable%20perception-and-prediction%20%28PnP%29%20models%20mitigate%20these%20issues%2C%20yet%20the%20complementarity%20of%20cameras%20and%20LiDAR%20in%20the%20query-space%20has%20not%20been%20sufficiently%20explored.%20Models%20often%20rely%20on%20fusion%20schemes%20that%20introduce%20heuristic%20alignment%20and%20discrete%20selection%20steps%20which%20prevent%20full%20utilization%20of%20available%20information%20and%20can%20introduce%20unwanted%20bias.%20We%20propose%20Li-ViP3D%2B%2B%2C%20a%20query-based%20multimodal%20PnP%20framework%20that%20introduces%20Query-Gated%20Deformable%20Fusion%20%28QGDF%29%20to%20integrate%20multi-view%20RGB%20and%20LiDAR%20in%20query%20space.%20QGDF%20%28i%29%20aggregates%20image%20evidence%20via%20masked%20attention%20across%20cameras%20and%20feature%20levels%2C%20%28ii%29%20extracts%20LiDAR%20context%20through%20fully%20differentiable%20BEV%20sampling%20with%20learned%20per-query%20offsets%2C%20and%20%28iii%29%20applies%20query-conditioned%20gating%20to%20adaptively%20weight%20visual%20and%20geometric%20cues%20per%20agent.%20The%20resulting%20architecture%20jointly%20optimizes%20detection%2C%20tracking%2C%20and%20multi-hypothesis%20trajectory%20forecasting%20in%20a%20single%20end-to-end%20model.%20On%20nuScenes%2C%20Li-ViP3D%2B%2B%20improves%20end-to-end%20behavior%20and%20detection%20quality%2C%20achieving%20higher%20EPA%20%280.335%29%20and%20mAP%20%280.502%29%20while%20substantially%20reducing%20false%20positives%20%28FP%20ratio%200.147%29%2C%20and%20it%20is%20faster%20than%20the%20prior%20Li-ViP3D%20variant%20%28139.82%20ms%20vs.%20145.91%20ms%29.%20These%20results%20indicate%20that%20query-space%2C%20fully%20differentiable%20camera-LiDAR%20fusion%20can%20increase%20robustness%20of%20end-to-end%20PnP%20without%20sacrificing%20deployability.&entry.1838667208=http%3A//arxiv.org/abs/2601.20720v1&entry.124074799=Read"},
{"title": "Hashing-Baseline: Rethinking Hashing in the Age of Pretrained Models", "author": "Ilyass Moummad and Kawtar Zaher and Lukas Rauch and Alexis Joly", "abstract": "Information retrieval with compact binary embeddings, also referred to as hashing, is crucial for scalable fast search applications, yet state-of-the-art hashing methods require expensive, scenario-specific training. In this work, we introduce Hashing-Baseline, a strong training-free hashing method leveraging powerful pretrained encoders that produce rich pretrained embeddings. We revisit classical, training-free hashing techniques: principal component analysis, random orthogonal projection, and threshold binarization, to produce a strong baseline for hashing. Our approach combines these techniques with frozen embeddings from state-of-the-art vision and audio encoders to yield competitive retrieval performance without any additional learning or fine-tuning. To demonstrate the generality and effectiveness of this approach, we evaluate it on standard image retrieval benchmarks as well as a newly introduced benchmark for audio hashing.", "link": "http://arxiv.org/abs/2509.14427v2", "date": "2026-01-28", "relevancy": 2.4245, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4889}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4889}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hashing-Baseline%3A%20Rethinking%20Hashing%20in%20the%20Age%20of%20Pretrained%20Models&body=Title%3A%20Hashing-Baseline%3A%20Rethinking%20Hashing%20in%20the%20Age%20of%20Pretrained%20Models%0AAuthor%3A%20Ilyass%20Moummad%20and%20Kawtar%20Zaher%20and%20Lukas%20Rauch%20and%20Alexis%20Joly%0AAbstract%3A%20Information%20retrieval%20with%20compact%20binary%20embeddings%2C%20also%20referred%20to%20as%20hashing%2C%20is%20crucial%20for%20scalable%20fast%20search%20applications%2C%20yet%20state-of-the-art%20hashing%20methods%20require%20expensive%2C%20scenario-specific%20training.%20In%20this%20work%2C%20we%20introduce%20Hashing-Baseline%2C%20a%20strong%20training-free%20hashing%20method%20leveraging%20powerful%20pretrained%20encoders%20that%20produce%20rich%20pretrained%20embeddings.%20We%20revisit%20classical%2C%20training-free%20hashing%20techniques%3A%20principal%20component%20analysis%2C%20random%20orthogonal%20projection%2C%20and%20threshold%20binarization%2C%20to%20produce%20a%20strong%20baseline%20for%20hashing.%20Our%20approach%20combines%20these%20techniques%20with%20frozen%20embeddings%20from%20state-of-the-art%20vision%20and%20audio%20encoders%20to%20yield%20competitive%20retrieval%20performance%20without%20any%20additional%20learning%20or%20fine-tuning.%20To%20demonstrate%20the%20generality%20and%20effectiveness%20of%20this%20approach%2C%20we%20evaluate%20it%20on%20standard%20image%20retrieval%20benchmarks%20as%20well%20as%20a%20newly%20introduced%20benchmark%20for%20audio%20hashing.%0ALink%3A%20http%3A//arxiv.org/abs/2509.14427v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHashing-Baseline%253A%2520Rethinking%2520Hashing%2520in%2520the%2520Age%2520of%2520Pretrained%2520Models%26entry.906535625%3DIlyass%2520Moummad%2520and%2520Kawtar%2520Zaher%2520and%2520Lukas%2520Rauch%2520and%2520Alexis%2520Joly%26entry.1292438233%3DInformation%2520retrieval%2520with%2520compact%2520binary%2520embeddings%252C%2520also%2520referred%2520to%2520as%2520hashing%252C%2520is%2520crucial%2520for%2520scalable%2520fast%2520search%2520applications%252C%2520yet%2520state-of-the-art%2520hashing%2520methods%2520require%2520expensive%252C%2520scenario-specific%2520training.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Hashing-Baseline%252C%2520a%2520strong%2520training-free%2520hashing%2520method%2520leveraging%2520powerful%2520pretrained%2520encoders%2520that%2520produce%2520rich%2520pretrained%2520embeddings.%2520We%2520revisit%2520classical%252C%2520training-free%2520hashing%2520techniques%253A%2520principal%2520component%2520analysis%252C%2520random%2520orthogonal%2520projection%252C%2520and%2520threshold%2520binarization%252C%2520to%2520produce%2520a%2520strong%2520baseline%2520for%2520hashing.%2520Our%2520approach%2520combines%2520these%2520techniques%2520with%2520frozen%2520embeddings%2520from%2520state-of-the-art%2520vision%2520and%2520audio%2520encoders%2520to%2520yield%2520competitive%2520retrieval%2520performance%2520without%2520any%2520additional%2520learning%2520or%2520fine-tuning.%2520To%2520demonstrate%2520the%2520generality%2520and%2520effectiveness%2520of%2520this%2520approach%252C%2520we%2520evaluate%2520it%2520on%2520standard%2520image%2520retrieval%2520benchmarks%2520as%2520well%2520as%2520a%2520newly%2520introduced%2520benchmark%2520for%2520audio%2520hashing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14427v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hashing-Baseline%3A%20Rethinking%20Hashing%20in%20the%20Age%20of%20Pretrained%20Models&entry.906535625=Ilyass%20Moummad%20and%20Kawtar%20Zaher%20and%20Lukas%20Rauch%20and%20Alexis%20Joly&entry.1292438233=Information%20retrieval%20with%20compact%20binary%20embeddings%2C%20also%20referred%20to%20as%20hashing%2C%20is%20crucial%20for%20scalable%20fast%20search%20applications%2C%20yet%20state-of-the-art%20hashing%20methods%20require%20expensive%2C%20scenario-specific%20training.%20In%20this%20work%2C%20we%20introduce%20Hashing-Baseline%2C%20a%20strong%20training-free%20hashing%20method%20leveraging%20powerful%20pretrained%20encoders%20that%20produce%20rich%20pretrained%20embeddings.%20We%20revisit%20classical%2C%20training-free%20hashing%20techniques%3A%20principal%20component%20analysis%2C%20random%20orthogonal%20projection%2C%20and%20threshold%20binarization%2C%20to%20produce%20a%20strong%20baseline%20for%20hashing.%20Our%20approach%20combines%20these%20techniques%20with%20frozen%20embeddings%20from%20state-of-the-art%20vision%20and%20audio%20encoders%20to%20yield%20competitive%20retrieval%20performance%20without%20any%20additional%20learning%20or%20fine-tuning.%20To%20demonstrate%20the%20generality%20and%20effectiveness%20of%20this%20approach%2C%20we%20evaluate%20it%20on%20standard%20image%20retrieval%20benchmarks%20as%20well%20as%20a%20newly%20introduced%20benchmark%20for%20audio%20hashing.&entry.1838667208=http%3A//arxiv.org/abs/2509.14427v2&entry.124074799=Read"},
{"title": "SERA: Soft-Verified Efficient Repository Agents", "author": "Ethan Shen and Danny Tormoen and Saurabh Shah and Ali Farhadi and Tim Dettmers", "abstract": "Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2's Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.", "link": "http://arxiv.org/abs/2601.20789v1", "date": "2026-01-28", "relevancy": 2.4235, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5083}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4838}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SERA%3A%20Soft-Verified%20Efficient%20Repository%20Agents&body=Title%3A%20SERA%3A%20Soft-Verified%20Efficient%20Repository%20Agents%0AAuthor%3A%20Ethan%20Shen%20and%20Danny%20Tormoen%20and%20Saurabh%20Shah%20and%20Ali%20Farhadi%20and%20Tim%20Dettmers%0AAbstract%3A%20Open-weight%20coding%20agents%20should%20hold%20a%20fundamental%20advantage%20over%20closed-source%20systems%3A%20they%20can%20be%20specialized%20to%20private%20codebases%2C%20encoding%20repository-specific%20information%20directly%20in%20their%20weights.%20Yet%20the%20cost%20and%20complexity%20of%20training%20has%20kept%20this%20advantage%20theoretical.%20We%20show%20it%20is%20now%20practical.%20We%20present%20Soft-Verified%20Efficient%20Repository%20Agents%20%28SERA%29%2C%20an%20efficient%20method%20for%20training%20coding%20agents%20that%20enables%20the%20rapid%20and%20cheap%20creation%20of%20agents%20specialized%20to%20private%20codebases.%20Using%20only%20supervised%20finetuning%20%28SFT%29%2C%20SERA%20achieves%20state-of-the-art%20results%20among%20fully%20open-source%20%28open%20data%2C%20method%2C%20code%29%20models%20while%20matching%20the%20performance%20of%20frontier%20open-weight%20models%20like%20Devstral-Small-2.%20Creating%20SERA%20models%20is%2026x%20cheaper%20than%20reinforcement%20learning%20and%2057x%20cheaper%20than%20previous%20synthetic%20data%20methods%20to%20reach%20equivalent%20performance.%20Our%20method%2C%20Soft%20Verified%20Generation%20%28SVG%29%2C%20generates%20thousands%20of%20trajectories%20from%20a%20single%20code%20repository.%20Combined%20with%20cost-efficiency%2C%20this%20enables%20specialization%20to%20private%20codebases.%20Beyond%20repository%20specialization%2C%20we%20apply%20SVG%20to%20a%20larger%20corpus%20of%20codebases%2C%20generating%20over%20200%2C000%20synthetic%20trajectories.%20We%20use%20this%20dataset%20to%20provide%20detailed%20analysis%20of%20scaling%20laws%2C%20ablations%2C%20and%20confounding%20factors%20for%20training%20coding%20agents.%20Overall%2C%20we%20believe%20our%20work%20will%20greatly%20accelerate%20research%20on%20open%20coding%20agents%20and%20showcase%20the%20advantage%20of%20open-source%20models%20that%20can%20specialize%20to%20private%20codebases.%20We%20release%20SERA%20as%20the%20first%20model%20in%20Ai2%27s%20Open%20Coding%20Agents%20series%2C%20along%20with%20all%20our%20code%2C%20data%2C%20and%20Claude%20Code%20integration%20to%20support%20the%20research%20community.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20789v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSERA%253A%2520Soft-Verified%2520Efficient%2520Repository%2520Agents%26entry.906535625%3DEthan%2520Shen%2520and%2520Danny%2520Tormoen%2520and%2520Saurabh%2520Shah%2520and%2520Ali%2520Farhadi%2520and%2520Tim%2520Dettmers%26entry.1292438233%3DOpen-weight%2520coding%2520agents%2520should%2520hold%2520a%2520fundamental%2520advantage%2520over%2520closed-source%2520systems%253A%2520they%2520can%2520be%2520specialized%2520to%2520private%2520codebases%252C%2520encoding%2520repository-specific%2520information%2520directly%2520in%2520their%2520weights.%2520Yet%2520the%2520cost%2520and%2520complexity%2520of%2520training%2520has%2520kept%2520this%2520advantage%2520theoretical.%2520We%2520show%2520it%2520is%2520now%2520practical.%2520We%2520present%2520Soft-Verified%2520Efficient%2520Repository%2520Agents%2520%2528SERA%2529%252C%2520an%2520efficient%2520method%2520for%2520training%2520coding%2520agents%2520that%2520enables%2520the%2520rapid%2520and%2520cheap%2520creation%2520of%2520agents%2520specialized%2520to%2520private%2520codebases.%2520Using%2520only%2520supervised%2520finetuning%2520%2528SFT%2529%252C%2520SERA%2520achieves%2520state-of-the-art%2520results%2520among%2520fully%2520open-source%2520%2528open%2520data%252C%2520method%252C%2520code%2529%2520models%2520while%2520matching%2520the%2520performance%2520of%2520frontier%2520open-weight%2520models%2520like%2520Devstral-Small-2.%2520Creating%2520SERA%2520models%2520is%252026x%2520cheaper%2520than%2520reinforcement%2520learning%2520and%252057x%2520cheaper%2520than%2520previous%2520synthetic%2520data%2520methods%2520to%2520reach%2520equivalent%2520performance.%2520Our%2520method%252C%2520Soft%2520Verified%2520Generation%2520%2528SVG%2529%252C%2520generates%2520thousands%2520of%2520trajectories%2520from%2520a%2520single%2520code%2520repository.%2520Combined%2520with%2520cost-efficiency%252C%2520this%2520enables%2520specialization%2520to%2520private%2520codebases.%2520Beyond%2520repository%2520specialization%252C%2520we%2520apply%2520SVG%2520to%2520a%2520larger%2520corpus%2520of%2520codebases%252C%2520generating%2520over%2520200%252C000%2520synthetic%2520trajectories.%2520We%2520use%2520this%2520dataset%2520to%2520provide%2520detailed%2520analysis%2520of%2520scaling%2520laws%252C%2520ablations%252C%2520and%2520confounding%2520factors%2520for%2520training%2520coding%2520agents.%2520Overall%252C%2520we%2520believe%2520our%2520work%2520will%2520greatly%2520accelerate%2520research%2520on%2520open%2520coding%2520agents%2520and%2520showcase%2520the%2520advantage%2520of%2520open-source%2520models%2520that%2520can%2520specialize%2520to%2520private%2520codebases.%2520We%2520release%2520SERA%2520as%2520the%2520first%2520model%2520in%2520Ai2%2527s%2520Open%2520Coding%2520Agents%2520series%252C%2520along%2520with%2520all%2520our%2520code%252C%2520data%252C%2520and%2520Claude%2520Code%2520integration%2520to%2520support%2520the%2520research%2520community.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20789v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SERA%3A%20Soft-Verified%20Efficient%20Repository%20Agents&entry.906535625=Ethan%20Shen%20and%20Danny%20Tormoen%20and%20Saurabh%20Shah%20and%20Ali%20Farhadi%20and%20Tim%20Dettmers&entry.1292438233=Open-weight%20coding%20agents%20should%20hold%20a%20fundamental%20advantage%20over%20closed-source%20systems%3A%20they%20can%20be%20specialized%20to%20private%20codebases%2C%20encoding%20repository-specific%20information%20directly%20in%20their%20weights.%20Yet%20the%20cost%20and%20complexity%20of%20training%20has%20kept%20this%20advantage%20theoretical.%20We%20show%20it%20is%20now%20practical.%20We%20present%20Soft-Verified%20Efficient%20Repository%20Agents%20%28SERA%29%2C%20an%20efficient%20method%20for%20training%20coding%20agents%20that%20enables%20the%20rapid%20and%20cheap%20creation%20of%20agents%20specialized%20to%20private%20codebases.%20Using%20only%20supervised%20finetuning%20%28SFT%29%2C%20SERA%20achieves%20state-of-the-art%20results%20among%20fully%20open-source%20%28open%20data%2C%20method%2C%20code%29%20models%20while%20matching%20the%20performance%20of%20frontier%20open-weight%20models%20like%20Devstral-Small-2.%20Creating%20SERA%20models%20is%2026x%20cheaper%20than%20reinforcement%20learning%20and%2057x%20cheaper%20than%20previous%20synthetic%20data%20methods%20to%20reach%20equivalent%20performance.%20Our%20method%2C%20Soft%20Verified%20Generation%20%28SVG%29%2C%20generates%20thousands%20of%20trajectories%20from%20a%20single%20code%20repository.%20Combined%20with%20cost-efficiency%2C%20this%20enables%20specialization%20to%20private%20codebases.%20Beyond%20repository%20specialization%2C%20we%20apply%20SVG%20to%20a%20larger%20corpus%20of%20codebases%2C%20generating%20over%20200%2C000%20synthetic%20trajectories.%20We%20use%20this%20dataset%20to%20provide%20detailed%20analysis%20of%20scaling%20laws%2C%20ablations%2C%20and%20confounding%20factors%20for%20training%20coding%20agents.%20Overall%2C%20we%20believe%20our%20work%20will%20greatly%20accelerate%20research%20on%20open%20coding%20agents%20and%20showcase%20the%20advantage%20of%20open-source%20models%20that%20can%20specialize%20to%20private%20codebases.%20We%20release%20SERA%20as%20the%20first%20model%20in%20Ai2%27s%20Open%20Coding%20Agents%20series%2C%20along%20with%20all%20our%20code%2C%20data%2C%20and%20Claude%20Code%20integration%20to%20support%20the%20research%20community.&entry.1838667208=http%3A//arxiv.org/abs/2601.20789v1&entry.124074799=Read"},
{"title": "LogogramNLP: Comparing Visual and Textual Representations of Ancient Logographic Writing Systems for NLP", "author": "Danlu Chen and Freda Shi and Aditi Agarwal and Jacobo Myerston and Taylor Berg-Kirkpatrick", "abstract": "Standard natural language processing (NLP) pipelines operate on symbolic representations of language, which typically consist of sequences of discrete tokens. However, creating an analogous representation for ancient logographic writing systems is an extremely labor intensive process that requires expert knowledge. At present, a large portion of logographic data persists in a purely visual form due to the absence of transcription -- this issue poses a bottleneck for researchers seeking to apply NLP toolkits to study ancient logographic languages: most of the relevant data are images of writing.\n  This paper investigates whether direct processing of visual representations of language offers a potential solution. We introduce LogogramNLP, the first benchmark enabling NLP analysis of ancient logographic languages, featuring both transcribed and visual datasets for four writing systems along with annotations for tasks like classification, translation, and parsing. Our experiments compare systems that employ recent visual and text encoding strategies as backbones. The results demonstrate that visual representations outperform textual representations for some investigated tasks, suggesting that visual processing pipelines may unlock a large amount of cultural heritage data of logographic languages for NLP-based analyses.", "link": "http://arxiv.org/abs/2408.04628v2", "date": "2026-01-28", "relevancy": 2.3976, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4943}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4943}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LogogramNLP%3A%20Comparing%20Visual%20and%20Textual%20Representations%20of%20Ancient%20Logographic%20Writing%20Systems%20for%20NLP&body=Title%3A%20LogogramNLP%3A%20Comparing%20Visual%20and%20Textual%20Representations%20of%20Ancient%20Logographic%20Writing%20Systems%20for%20NLP%0AAuthor%3A%20Danlu%20Chen%20and%20Freda%20Shi%20and%20Aditi%20Agarwal%20and%20Jacobo%20Myerston%20and%20Taylor%20Berg-Kirkpatrick%0AAbstract%3A%20Standard%20natural%20language%20processing%20%28NLP%29%20pipelines%20operate%20on%20symbolic%20representations%20of%20language%2C%20which%20typically%20consist%20of%20sequences%20of%20discrete%20tokens.%20However%2C%20creating%20an%20analogous%20representation%20for%20ancient%20logographic%20writing%20systems%20is%20an%20extremely%20labor%20intensive%20process%20that%20requires%20expert%20knowledge.%20At%20present%2C%20a%20large%20portion%20of%20logographic%20data%20persists%20in%20a%20purely%20visual%20form%20due%20to%20the%20absence%20of%20transcription%20--%20this%20issue%20poses%20a%20bottleneck%20for%20researchers%20seeking%20to%20apply%20NLP%20toolkits%20to%20study%20ancient%20logographic%20languages%3A%20most%20of%20the%20relevant%20data%20are%20images%20of%20writing.%0A%20%20This%20paper%20investigates%20whether%20direct%20processing%20of%20visual%20representations%20of%20language%20offers%20a%20potential%20solution.%20We%20introduce%20LogogramNLP%2C%20the%20first%20benchmark%20enabling%20NLP%20analysis%20of%20ancient%20logographic%20languages%2C%20featuring%20both%20transcribed%20and%20visual%20datasets%20for%20four%20writing%20systems%20along%20with%20annotations%20for%20tasks%20like%20classification%2C%20translation%2C%20and%20parsing.%20Our%20experiments%20compare%20systems%20that%20employ%20recent%20visual%20and%20text%20encoding%20strategies%20as%20backbones.%20The%20results%20demonstrate%20that%20visual%20representations%20outperform%20textual%20representations%20for%20some%20investigated%20tasks%2C%20suggesting%20that%20visual%20processing%20pipelines%20may%20unlock%20a%20large%20amount%20of%20cultural%20heritage%20data%20of%20logographic%20languages%20for%20NLP-based%20analyses.%0ALink%3A%20http%3A//arxiv.org/abs/2408.04628v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLogogramNLP%253A%2520Comparing%2520Visual%2520and%2520Textual%2520Representations%2520of%2520Ancient%2520Logographic%2520Writing%2520Systems%2520for%2520NLP%26entry.906535625%3DDanlu%2520Chen%2520and%2520Freda%2520Shi%2520and%2520Aditi%2520Agarwal%2520and%2520Jacobo%2520Myerston%2520and%2520Taylor%2520Berg-Kirkpatrick%26entry.1292438233%3DStandard%2520natural%2520language%2520processing%2520%2528NLP%2529%2520pipelines%2520operate%2520on%2520symbolic%2520representations%2520of%2520language%252C%2520which%2520typically%2520consist%2520of%2520sequences%2520of%2520discrete%2520tokens.%2520However%252C%2520creating%2520an%2520analogous%2520representation%2520for%2520ancient%2520logographic%2520writing%2520systems%2520is%2520an%2520extremely%2520labor%2520intensive%2520process%2520that%2520requires%2520expert%2520knowledge.%2520At%2520present%252C%2520a%2520large%2520portion%2520of%2520logographic%2520data%2520persists%2520in%2520a%2520purely%2520visual%2520form%2520due%2520to%2520the%2520absence%2520of%2520transcription%2520--%2520this%2520issue%2520poses%2520a%2520bottleneck%2520for%2520researchers%2520seeking%2520to%2520apply%2520NLP%2520toolkits%2520to%2520study%2520ancient%2520logographic%2520languages%253A%2520most%2520of%2520the%2520relevant%2520data%2520are%2520images%2520of%2520writing.%250A%2520%2520This%2520paper%2520investigates%2520whether%2520direct%2520processing%2520of%2520visual%2520representations%2520of%2520language%2520offers%2520a%2520potential%2520solution.%2520We%2520introduce%2520LogogramNLP%252C%2520the%2520first%2520benchmark%2520enabling%2520NLP%2520analysis%2520of%2520ancient%2520logographic%2520languages%252C%2520featuring%2520both%2520transcribed%2520and%2520visual%2520datasets%2520for%2520four%2520writing%2520systems%2520along%2520with%2520annotations%2520for%2520tasks%2520like%2520classification%252C%2520translation%252C%2520and%2520parsing.%2520Our%2520experiments%2520compare%2520systems%2520that%2520employ%2520recent%2520visual%2520and%2520text%2520encoding%2520strategies%2520as%2520backbones.%2520The%2520results%2520demonstrate%2520that%2520visual%2520representations%2520outperform%2520textual%2520representations%2520for%2520some%2520investigated%2520tasks%252C%2520suggesting%2520that%2520visual%2520processing%2520pipelines%2520may%2520unlock%2520a%2520large%2520amount%2520of%2520cultural%2520heritage%2520data%2520of%2520logographic%2520languages%2520for%2520NLP-based%2520analyses.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.04628v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LogogramNLP%3A%20Comparing%20Visual%20and%20Textual%20Representations%20of%20Ancient%20Logographic%20Writing%20Systems%20for%20NLP&entry.906535625=Danlu%20Chen%20and%20Freda%20Shi%20and%20Aditi%20Agarwal%20and%20Jacobo%20Myerston%20and%20Taylor%20Berg-Kirkpatrick&entry.1292438233=Standard%20natural%20language%20processing%20%28NLP%29%20pipelines%20operate%20on%20symbolic%20representations%20of%20language%2C%20which%20typically%20consist%20of%20sequences%20of%20discrete%20tokens.%20However%2C%20creating%20an%20analogous%20representation%20for%20ancient%20logographic%20writing%20systems%20is%20an%20extremely%20labor%20intensive%20process%20that%20requires%20expert%20knowledge.%20At%20present%2C%20a%20large%20portion%20of%20logographic%20data%20persists%20in%20a%20purely%20visual%20form%20due%20to%20the%20absence%20of%20transcription%20--%20this%20issue%20poses%20a%20bottleneck%20for%20researchers%20seeking%20to%20apply%20NLP%20toolkits%20to%20study%20ancient%20logographic%20languages%3A%20most%20of%20the%20relevant%20data%20are%20images%20of%20writing.%0A%20%20This%20paper%20investigates%20whether%20direct%20processing%20of%20visual%20representations%20of%20language%20offers%20a%20potential%20solution.%20We%20introduce%20LogogramNLP%2C%20the%20first%20benchmark%20enabling%20NLP%20analysis%20of%20ancient%20logographic%20languages%2C%20featuring%20both%20transcribed%20and%20visual%20datasets%20for%20four%20writing%20systems%20along%20with%20annotations%20for%20tasks%20like%20classification%2C%20translation%2C%20and%20parsing.%20Our%20experiments%20compare%20systems%20that%20employ%20recent%20visual%20and%20text%20encoding%20strategies%20as%20backbones.%20The%20results%20demonstrate%20that%20visual%20representations%20outperform%20textual%20representations%20for%20some%20investigated%20tasks%2C%20suggesting%20that%20visual%20processing%20pipelines%20may%20unlock%20a%20large%20amount%20of%20cultural%20heritage%20data%20of%20logographic%20languages%20for%20NLP-based%20analyses.&entry.1838667208=http%3A//arxiv.org/abs/2408.04628v2&entry.124074799=Read"},
{"title": "FD-MAD: Frequency-Domain Residual Analysis for Face Morphing Attack Detection", "author": "Diogo J. Paulo and Hugo Proen\u00e7a and Jo\u00e3o C. Neves", "abstract": "Face morphing attacks present a significant threat to face recognition systems used in electronic identity enrolment and border control, particularly in single-image morphing attack detection (S-MAD) scenarios where no trusted reference is available. In spite of the vast amount of research on this problem, morph detection systems struggle in cross-dataset scenarios. To address this problem, we introduce a region-aware frequency-based morph detection strategy that drastically improves over strong baseline methods in challenging cross-dataset and cross-morph settings using a lightweight approach. Having observed the separability of bona fide and morph samples in the frequency domain of different facial parts, our approach 1) introduces the concept of residual frequency domain, where the frequency of the signal is decoupled from the natural spectral decay to easily discriminate between morph and bona fide data; 2) additionally, we reason in a global and local manner by combining the evidence from different facial regions in a Markov Random Field, which infers a globally consistent decision. The proposed method, trained exclusively on the synthetic morphing attack detection development dataset (SMDD), is evaluated in challenging cross-dataset and cross-morph settings on FRLL-Morph and MAD22 sets. Our approach achieves an average equal error rate (EER) of 1.85\\% on FRLL-Morph and ranks second on MAD22 with an average EER of 6.12\\%, while also obtaining a good bona fide presentation classification error rate (BPCER) at a low attack presentation classification error rate (APCER) using only spectral features. These findings indicate that Fourier-domain residual modeling with structured regional fusion offers a competitive alternative to deep S-MAD architectures.", "link": "http://arxiv.org/abs/2601.20656v1", "date": "2026-01-28", "relevancy": 2.3959, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4804}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4788}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FD-MAD%3A%20Frequency-Domain%20Residual%20Analysis%20for%20Face%20Morphing%20Attack%20Detection&body=Title%3A%20FD-MAD%3A%20Frequency-Domain%20Residual%20Analysis%20for%20Face%20Morphing%20Attack%20Detection%0AAuthor%3A%20Diogo%20J.%20Paulo%20and%20Hugo%20Proen%C3%A7a%20and%20Jo%C3%A3o%20C.%20Neves%0AAbstract%3A%20Face%20morphing%20attacks%20present%20a%20significant%20threat%20to%20face%20recognition%20systems%20used%20in%20electronic%20identity%20enrolment%20and%20border%20control%2C%20particularly%20in%20single-image%20morphing%20attack%20detection%20%28S-MAD%29%20scenarios%20where%20no%20trusted%20reference%20is%20available.%20In%20spite%20of%20the%20vast%20amount%20of%20research%20on%20this%20problem%2C%20morph%20detection%20systems%20struggle%20in%20cross-dataset%20scenarios.%20To%20address%20this%20problem%2C%20we%20introduce%20a%20region-aware%20frequency-based%20morph%20detection%20strategy%20that%20drastically%20improves%20over%20strong%20baseline%20methods%20in%20challenging%20cross-dataset%20and%20cross-morph%20settings%20using%20a%20lightweight%20approach.%20Having%20observed%20the%20separability%20of%20bona%20fide%20and%20morph%20samples%20in%20the%20frequency%20domain%20of%20different%20facial%20parts%2C%20our%20approach%201%29%20introduces%20the%20concept%20of%20residual%20frequency%20domain%2C%20where%20the%20frequency%20of%20the%20signal%20is%20decoupled%20from%20the%20natural%20spectral%20decay%20to%20easily%20discriminate%20between%20morph%20and%20bona%20fide%20data%3B%202%29%20additionally%2C%20we%20reason%20in%20a%20global%20and%20local%20manner%20by%20combining%20the%20evidence%20from%20different%20facial%20regions%20in%20a%20Markov%20Random%20Field%2C%20which%20infers%20a%20globally%20consistent%20decision.%20The%20proposed%20method%2C%20trained%20exclusively%20on%20the%20synthetic%20morphing%20attack%20detection%20development%20dataset%20%28SMDD%29%2C%20is%20evaluated%20in%20challenging%20cross-dataset%20and%20cross-morph%20settings%20on%20FRLL-Morph%20and%20MAD22%20sets.%20Our%20approach%20achieves%20an%20average%20equal%20error%20rate%20%28EER%29%20of%201.85%5C%25%20on%20FRLL-Morph%20and%20ranks%20second%20on%20MAD22%20with%20an%20average%20EER%20of%206.12%5C%25%2C%20while%20also%20obtaining%20a%20good%20bona%20fide%20presentation%20classification%20error%20rate%20%28BPCER%29%20at%20a%20low%20attack%20presentation%20classification%20error%20rate%20%28APCER%29%20using%20only%20spectral%20features.%20These%20findings%20indicate%20that%20Fourier-domain%20residual%20modeling%20with%20structured%20regional%20fusion%20offers%20a%20competitive%20alternative%20to%20deep%20S-MAD%20architectures.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20656v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFD-MAD%253A%2520Frequency-Domain%2520Residual%2520Analysis%2520for%2520Face%2520Morphing%2520Attack%2520Detection%26entry.906535625%3DDiogo%2520J.%2520Paulo%2520and%2520Hugo%2520Proen%25C3%25A7a%2520and%2520Jo%25C3%25A3o%2520C.%2520Neves%26entry.1292438233%3DFace%2520morphing%2520attacks%2520present%2520a%2520significant%2520threat%2520to%2520face%2520recognition%2520systems%2520used%2520in%2520electronic%2520identity%2520enrolment%2520and%2520border%2520control%252C%2520particularly%2520in%2520single-image%2520morphing%2520attack%2520detection%2520%2528S-MAD%2529%2520scenarios%2520where%2520no%2520trusted%2520reference%2520is%2520available.%2520In%2520spite%2520of%2520the%2520vast%2520amount%2520of%2520research%2520on%2520this%2520problem%252C%2520morph%2520detection%2520systems%2520struggle%2520in%2520cross-dataset%2520scenarios.%2520To%2520address%2520this%2520problem%252C%2520we%2520introduce%2520a%2520region-aware%2520frequency-based%2520morph%2520detection%2520strategy%2520that%2520drastically%2520improves%2520over%2520strong%2520baseline%2520methods%2520in%2520challenging%2520cross-dataset%2520and%2520cross-morph%2520settings%2520using%2520a%2520lightweight%2520approach.%2520Having%2520observed%2520the%2520separability%2520of%2520bona%2520fide%2520and%2520morph%2520samples%2520in%2520the%2520frequency%2520domain%2520of%2520different%2520facial%2520parts%252C%2520our%2520approach%25201%2529%2520introduces%2520the%2520concept%2520of%2520residual%2520frequency%2520domain%252C%2520where%2520the%2520frequency%2520of%2520the%2520signal%2520is%2520decoupled%2520from%2520the%2520natural%2520spectral%2520decay%2520to%2520easily%2520discriminate%2520between%2520morph%2520and%2520bona%2520fide%2520data%253B%25202%2529%2520additionally%252C%2520we%2520reason%2520in%2520a%2520global%2520and%2520local%2520manner%2520by%2520combining%2520the%2520evidence%2520from%2520different%2520facial%2520regions%2520in%2520a%2520Markov%2520Random%2520Field%252C%2520which%2520infers%2520a%2520globally%2520consistent%2520decision.%2520The%2520proposed%2520method%252C%2520trained%2520exclusively%2520on%2520the%2520synthetic%2520morphing%2520attack%2520detection%2520development%2520dataset%2520%2528SMDD%2529%252C%2520is%2520evaluated%2520in%2520challenging%2520cross-dataset%2520and%2520cross-morph%2520settings%2520on%2520FRLL-Morph%2520and%2520MAD22%2520sets.%2520Our%2520approach%2520achieves%2520an%2520average%2520equal%2520error%2520rate%2520%2528EER%2529%2520of%25201.85%255C%2525%2520on%2520FRLL-Morph%2520and%2520ranks%2520second%2520on%2520MAD22%2520with%2520an%2520average%2520EER%2520of%25206.12%255C%2525%252C%2520while%2520also%2520obtaining%2520a%2520good%2520bona%2520fide%2520presentation%2520classification%2520error%2520rate%2520%2528BPCER%2529%2520at%2520a%2520low%2520attack%2520presentation%2520classification%2520error%2520rate%2520%2528APCER%2529%2520using%2520only%2520spectral%2520features.%2520These%2520findings%2520indicate%2520that%2520Fourier-domain%2520residual%2520modeling%2520with%2520structured%2520regional%2520fusion%2520offers%2520a%2520competitive%2520alternative%2520to%2520deep%2520S-MAD%2520architectures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20656v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FD-MAD%3A%20Frequency-Domain%20Residual%20Analysis%20for%20Face%20Morphing%20Attack%20Detection&entry.906535625=Diogo%20J.%20Paulo%20and%20Hugo%20Proen%C3%A7a%20and%20Jo%C3%A3o%20C.%20Neves&entry.1292438233=Face%20morphing%20attacks%20present%20a%20significant%20threat%20to%20face%20recognition%20systems%20used%20in%20electronic%20identity%20enrolment%20and%20border%20control%2C%20particularly%20in%20single-image%20morphing%20attack%20detection%20%28S-MAD%29%20scenarios%20where%20no%20trusted%20reference%20is%20available.%20In%20spite%20of%20the%20vast%20amount%20of%20research%20on%20this%20problem%2C%20morph%20detection%20systems%20struggle%20in%20cross-dataset%20scenarios.%20To%20address%20this%20problem%2C%20we%20introduce%20a%20region-aware%20frequency-based%20morph%20detection%20strategy%20that%20drastically%20improves%20over%20strong%20baseline%20methods%20in%20challenging%20cross-dataset%20and%20cross-morph%20settings%20using%20a%20lightweight%20approach.%20Having%20observed%20the%20separability%20of%20bona%20fide%20and%20morph%20samples%20in%20the%20frequency%20domain%20of%20different%20facial%20parts%2C%20our%20approach%201%29%20introduces%20the%20concept%20of%20residual%20frequency%20domain%2C%20where%20the%20frequency%20of%20the%20signal%20is%20decoupled%20from%20the%20natural%20spectral%20decay%20to%20easily%20discriminate%20between%20morph%20and%20bona%20fide%20data%3B%202%29%20additionally%2C%20we%20reason%20in%20a%20global%20and%20local%20manner%20by%20combining%20the%20evidence%20from%20different%20facial%20regions%20in%20a%20Markov%20Random%20Field%2C%20which%20infers%20a%20globally%20consistent%20decision.%20The%20proposed%20method%2C%20trained%20exclusively%20on%20the%20synthetic%20morphing%20attack%20detection%20development%20dataset%20%28SMDD%29%2C%20is%20evaluated%20in%20challenging%20cross-dataset%20and%20cross-morph%20settings%20on%20FRLL-Morph%20and%20MAD22%20sets.%20Our%20approach%20achieves%20an%20average%20equal%20error%20rate%20%28EER%29%20of%201.85%5C%25%20on%20FRLL-Morph%20and%20ranks%20second%20on%20MAD22%20with%20an%20average%20EER%20of%206.12%5C%25%2C%20while%20also%20obtaining%20a%20good%20bona%20fide%20presentation%20classification%20error%20rate%20%28BPCER%29%20at%20a%20low%20attack%20presentation%20classification%20error%20rate%20%28APCER%29%20using%20only%20spectral%20features.%20These%20findings%20indicate%20that%20Fourier-domain%20residual%20modeling%20with%20structured%20regional%20fusion%20offers%20a%20competitive%20alternative%20to%20deep%20S-MAD%20architectures.&entry.1838667208=http%3A//arxiv.org/abs/2601.20656v1&entry.124074799=Read"},
{"title": "DiffVC-RT: Towards Practical Real-Time Diffusion-based Perceptual Neural Video Compression", "author": "Wenzhuo Ma and Zhenzhong Chen", "abstract": "The practical deployment of diffusion-based Neural Video Compression (NVC) faces critical challenges, including severe information loss, prohibitive inference latency, and poor temporal consistency. To bridge this gap, we propose DiffVC-RT, the first framework designed to achieve real-time diffusion-based perceptual NVC. First, we introduce an Efficient and Informative Model Architecture. Through strategic module replacements and pruning, this architecture significantly reduces computational complexity while mitigating structural information loss. Second, to address generative flickering artifacts, we propose Explicit and Implicit Consistency Modeling. We enhance temporal consistency by explicitly incorporating a zero-cost Online Temporal Shift Module within the U-Net, complemented by hybrid implicit consistency constraints. Finally, we present an Asynchronous and Parallel Decoding Pipeline incorporating Mixed Half Precision, which enables asynchronous latent decoding and parallel frame reconstruction via a Batch-dimension Temporal Shift design. Experiments show that DiffVC-RT achieves 80.1% bitrate savings in terms of LPIPS over VTM-17.0 on HEVC dataset with real-time encoding and decoding speeds of 206 / 30 fps for 720p videos on an NVIDIA H800 GPU, marking a significant milestone in diffusion-based video compression.", "link": "http://arxiv.org/abs/2601.20564v1", "date": "2026-01-28", "relevancy": 2.3841, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6277}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6022}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiffVC-RT%3A%20Towards%20Practical%20Real-Time%20Diffusion-based%20Perceptual%20Neural%20Video%20Compression&body=Title%3A%20DiffVC-RT%3A%20Towards%20Practical%20Real-Time%20Diffusion-based%20Perceptual%20Neural%20Video%20Compression%0AAuthor%3A%20Wenzhuo%20Ma%20and%20Zhenzhong%20Chen%0AAbstract%3A%20The%20practical%20deployment%20of%20diffusion-based%20Neural%20Video%20Compression%20%28NVC%29%20faces%20critical%20challenges%2C%20including%20severe%20information%20loss%2C%20prohibitive%20inference%20latency%2C%20and%20poor%20temporal%20consistency.%20To%20bridge%20this%20gap%2C%20we%20propose%20DiffVC-RT%2C%20the%20first%20framework%20designed%20to%20achieve%20real-time%20diffusion-based%20perceptual%20NVC.%20First%2C%20we%20introduce%20an%20Efficient%20and%20Informative%20Model%20Architecture.%20Through%20strategic%20module%20replacements%20and%20pruning%2C%20this%20architecture%20significantly%20reduces%20computational%20complexity%20while%20mitigating%20structural%20information%20loss.%20Second%2C%20to%20address%20generative%20flickering%20artifacts%2C%20we%20propose%20Explicit%20and%20Implicit%20Consistency%20Modeling.%20We%20enhance%20temporal%20consistency%20by%20explicitly%20incorporating%20a%20zero-cost%20Online%20Temporal%20Shift%20Module%20within%20the%20U-Net%2C%20complemented%20by%20hybrid%20implicit%20consistency%20constraints.%20Finally%2C%20we%20present%20an%20Asynchronous%20and%20Parallel%20Decoding%20Pipeline%20incorporating%20Mixed%20Half%20Precision%2C%20which%20enables%20asynchronous%20latent%20decoding%20and%20parallel%20frame%20reconstruction%20via%20a%20Batch-dimension%20Temporal%20Shift%20design.%20Experiments%20show%20that%20DiffVC-RT%20achieves%2080.1%25%20bitrate%20savings%20in%20terms%20of%20LPIPS%20over%20VTM-17.0%20on%20HEVC%20dataset%20with%20real-time%20encoding%20and%20decoding%20speeds%20of%20206%20/%2030%20fps%20for%20720p%20videos%20on%20an%20NVIDIA%20H800%20GPU%2C%20marking%20a%20significant%20milestone%20in%20diffusion-based%20video%20compression.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiffVC-RT%253A%2520Towards%2520Practical%2520Real-Time%2520Diffusion-based%2520Perceptual%2520Neural%2520Video%2520Compression%26entry.906535625%3DWenzhuo%2520Ma%2520and%2520Zhenzhong%2520Chen%26entry.1292438233%3DThe%2520practical%2520deployment%2520of%2520diffusion-based%2520Neural%2520Video%2520Compression%2520%2528NVC%2529%2520faces%2520critical%2520challenges%252C%2520including%2520severe%2520information%2520loss%252C%2520prohibitive%2520inference%2520latency%252C%2520and%2520poor%2520temporal%2520consistency.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520DiffVC-RT%252C%2520the%2520first%2520framework%2520designed%2520to%2520achieve%2520real-time%2520diffusion-based%2520perceptual%2520NVC.%2520First%252C%2520we%2520introduce%2520an%2520Efficient%2520and%2520Informative%2520Model%2520Architecture.%2520Through%2520strategic%2520module%2520replacements%2520and%2520pruning%252C%2520this%2520architecture%2520significantly%2520reduces%2520computational%2520complexity%2520while%2520mitigating%2520structural%2520information%2520loss.%2520Second%252C%2520to%2520address%2520generative%2520flickering%2520artifacts%252C%2520we%2520propose%2520Explicit%2520and%2520Implicit%2520Consistency%2520Modeling.%2520We%2520enhance%2520temporal%2520consistency%2520by%2520explicitly%2520incorporating%2520a%2520zero-cost%2520Online%2520Temporal%2520Shift%2520Module%2520within%2520the%2520U-Net%252C%2520complemented%2520by%2520hybrid%2520implicit%2520consistency%2520constraints.%2520Finally%252C%2520we%2520present%2520an%2520Asynchronous%2520and%2520Parallel%2520Decoding%2520Pipeline%2520incorporating%2520Mixed%2520Half%2520Precision%252C%2520which%2520enables%2520asynchronous%2520latent%2520decoding%2520and%2520parallel%2520frame%2520reconstruction%2520via%2520a%2520Batch-dimension%2520Temporal%2520Shift%2520design.%2520Experiments%2520show%2520that%2520DiffVC-RT%2520achieves%252080.1%2525%2520bitrate%2520savings%2520in%2520terms%2520of%2520LPIPS%2520over%2520VTM-17.0%2520on%2520HEVC%2520dataset%2520with%2520real-time%2520encoding%2520and%2520decoding%2520speeds%2520of%2520206%2520/%252030%2520fps%2520for%2520720p%2520videos%2520on%2520an%2520NVIDIA%2520H800%2520GPU%252C%2520marking%2520a%2520significant%2520milestone%2520in%2520diffusion-based%2520video%2520compression.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiffVC-RT%3A%20Towards%20Practical%20Real-Time%20Diffusion-based%20Perceptual%20Neural%20Video%20Compression&entry.906535625=Wenzhuo%20Ma%20and%20Zhenzhong%20Chen&entry.1292438233=The%20practical%20deployment%20of%20diffusion-based%20Neural%20Video%20Compression%20%28NVC%29%20faces%20critical%20challenges%2C%20including%20severe%20information%20loss%2C%20prohibitive%20inference%20latency%2C%20and%20poor%20temporal%20consistency.%20To%20bridge%20this%20gap%2C%20we%20propose%20DiffVC-RT%2C%20the%20first%20framework%20designed%20to%20achieve%20real-time%20diffusion-based%20perceptual%20NVC.%20First%2C%20we%20introduce%20an%20Efficient%20and%20Informative%20Model%20Architecture.%20Through%20strategic%20module%20replacements%20and%20pruning%2C%20this%20architecture%20significantly%20reduces%20computational%20complexity%20while%20mitigating%20structural%20information%20loss.%20Second%2C%20to%20address%20generative%20flickering%20artifacts%2C%20we%20propose%20Explicit%20and%20Implicit%20Consistency%20Modeling.%20We%20enhance%20temporal%20consistency%20by%20explicitly%20incorporating%20a%20zero-cost%20Online%20Temporal%20Shift%20Module%20within%20the%20U-Net%2C%20complemented%20by%20hybrid%20implicit%20consistency%20constraints.%20Finally%2C%20we%20present%20an%20Asynchronous%20and%20Parallel%20Decoding%20Pipeline%20incorporating%20Mixed%20Half%20Precision%2C%20which%20enables%20asynchronous%20latent%20decoding%20and%20parallel%20frame%20reconstruction%20via%20a%20Batch-dimension%20Temporal%20Shift%20design.%20Experiments%20show%20that%20DiffVC-RT%20achieves%2080.1%25%20bitrate%20savings%20in%20terms%20of%20LPIPS%20over%20VTM-17.0%20on%20HEVC%20dataset%20with%20real-time%20encoding%20and%20decoding%20speeds%20of%20206%20/%2030%20fps%20for%20720p%20videos%20on%20an%20NVIDIA%20H800%20GPU%2C%20marking%20a%20significant%20milestone%20in%20diffusion-based%20video%20compression.&entry.1838667208=http%3A//arxiv.org/abs/2601.20564v1&entry.124074799=Read"},
{"title": "JAFAR: Jack up Any Feature at Any Resolution", "author": "Paul Couairon and Loick Chambon and Louis Serrano and Jean-Emmanuel Haugeard and Matthieu Cord and Nicolas Thome", "abstract": "Foundation Vision Encoders have become essential for a wide range of dense vision tasks. However, their low-resolution spatial feature outputs necessitate feature upsampling to produce the high-resolution modalities required for downstream tasks. In this work, we introduce JAFAR, a lightweight and flexible feature upsampler that enhances the spatial resolution of visual features from any Foundation Vision Encoder to an arbitrary target resolution. JAFAR employs an attention-based module designed to promote semantic alignment between high-resolution queries, derived from low-level image features, and semantically enriched low-resolution keys, using Spatial Feature Transform (SFT) modulation. Notably, despite the absence of high-resolution supervision, we demonstrate that learning at low upsampling ratios and resolutions generalizes remarkably well to significantly higher output scales. Extensive experiments show that JAFAR effectively recovers fine-grained spatial details and consistently outperforms existing feature upsampling methods across a diverse set of downstream tasks. Project page at https://jafar-upsampler.github.io", "link": "http://arxiv.org/abs/2506.11136v3", "date": "2026-01-28", "relevancy": 2.3835, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5034}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.465}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4618}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JAFAR%3A%20Jack%20up%20Any%20Feature%20at%20Any%20Resolution&body=Title%3A%20JAFAR%3A%20Jack%20up%20Any%20Feature%20at%20Any%20Resolution%0AAuthor%3A%20Paul%20Couairon%20and%20Loick%20Chambon%20and%20Louis%20Serrano%20and%20Jean-Emmanuel%20Haugeard%20and%20Matthieu%20Cord%20and%20Nicolas%20Thome%0AAbstract%3A%20Foundation%20Vision%20Encoders%20have%20become%20essential%20for%20a%20wide%20range%20of%20dense%20vision%20tasks.%20However%2C%20their%20low-resolution%20spatial%20feature%20outputs%20necessitate%20feature%20upsampling%20to%20produce%20the%20high-resolution%20modalities%20required%20for%20downstream%20tasks.%20In%20this%20work%2C%20we%20introduce%20JAFAR%2C%20a%20lightweight%20and%20flexible%20feature%20upsampler%20that%20enhances%20the%20spatial%20resolution%20of%20visual%20features%20from%20any%20Foundation%20Vision%20Encoder%20to%20an%20arbitrary%20target%20resolution.%20JAFAR%20employs%20an%20attention-based%20module%20designed%20to%20promote%20semantic%20alignment%20between%20high-resolution%20queries%2C%20derived%20from%20low-level%20image%20features%2C%20and%20semantically%20enriched%20low-resolution%20keys%2C%20using%20Spatial%20Feature%20Transform%20%28SFT%29%20modulation.%20Notably%2C%20despite%20the%20absence%20of%20high-resolution%20supervision%2C%20we%20demonstrate%20that%20learning%20at%20low%20upsampling%20ratios%20and%20resolutions%20generalizes%20remarkably%20well%20to%20significantly%20higher%20output%20scales.%20Extensive%20experiments%20show%20that%20JAFAR%20effectively%20recovers%20fine-grained%20spatial%20details%20and%20consistently%20outperforms%20existing%20feature%20upsampling%20methods%20across%20a%20diverse%20set%20of%20downstream%20tasks.%20Project%20page%20at%20https%3A//jafar-upsampler.github.io%0ALink%3A%20http%3A//arxiv.org/abs/2506.11136v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJAFAR%253A%2520Jack%2520up%2520Any%2520Feature%2520at%2520Any%2520Resolution%26entry.906535625%3DPaul%2520Couairon%2520and%2520Loick%2520Chambon%2520and%2520Louis%2520Serrano%2520and%2520Jean-Emmanuel%2520Haugeard%2520and%2520Matthieu%2520Cord%2520and%2520Nicolas%2520Thome%26entry.1292438233%3DFoundation%2520Vision%2520Encoders%2520have%2520become%2520essential%2520for%2520a%2520wide%2520range%2520of%2520dense%2520vision%2520tasks.%2520However%252C%2520their%2520low-resolution%2520spatial%2520feature%2520outputs%2520necessitate%2520feature%2520upsampling%2520to%2520produce%2520the%2520high-resolution%2520modalities%2520required%2520for%2520downstream%2520tasks.%2520In%2520this%2520work%252C%2520we%2520introduce%2520JAFAR%252C%2520a%2520lightweight%2520and%2520flexible%2520feature%2520upsampler%2520that%2520enhances%2520the%2520spatial%2520resolution%2520of%2520visual%2520features%2520from%2520any%2520Foundation%2520Vision%2520Encoder%2520to%2520an%2520arbitrary%2520target%2520resolution.%2520JAFAR%2520employs%2520an%2520attention-based%2520module%2520designed%2520to%2520promote%2520semantic%2520alignment%2520between%2520high-resolution%2520queries%252C%2520derived%2520from%2520low-level%2520image%2520features%252C%2520and%2520semantically%2520enriched%2520low-resolution%2520keys%252C%2520using%2520Spatial%2520Feature%2520Transform%2520%2528SFT%2529%2520modulation.%2520Notably%252C%2520despite%2520the%2520absence%2520of%2520high-resolution%2520supervision%252C%2520we%2520demonstrate%2520that%2520learning%2520at%2520low%2520upsampling%2520ratios%2520and%2520resolutions%2520generalizes%2520remarkably%2520well%2520to%2520significantly%2520higher%2520output%2520scales.%2520Extensive%2520experiments%2520show%2520that%2520JAFAR%2520effectively%2520recovers%2520fine-grained%2520spatial%2520details%2520and%2520consistently%2520outperforms%2520existing%2520feature%2520upsampling%2520methods%2520across%2520a%2520diverse%2520set%2520of%2520downstream%2520tasks.%2520Project%2520page%2520at%2520https%253A//jafar-upsampler.github.io%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11136v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JAFAR%3A%20Jack%20up%20Any%20Feature%20at%20Any%20Resolution&entry.906535625=Paul%20Couairon%20and%20Loick%20Chambon%20and%20Louis%20Serrano%20and%20Jean-Emmanuel%20Haugeard%20and%20Matthieu%20Cord%20and%20Nicolas%20Thome&entry.1292438233=Foundation%20Vision%20Encoders%20have%20become%20essential%20for%20a%20wide%20range%20of%20dense%20vision%20tasks.%20However%2C%20their%20low-resolution%20spatial%20feature%20outputs%20necessitate%20feature%20upsampling%20to%20produce%20the%20high-resolution%20modalities%20required%20for%20downstream%20tasks.%20In%20this%20work%2C%20we%20introduce%20JAFAR%2C%20a%20lightweight%20and%20flexible%20feature%20upsampler%20that%20enhances%20the%20spatial%20resolution%20of%20visual%20features%20from%20any%20Foundation%20Vision%20Encoder%20to%20an%20arbitrary%20target%20resolution.%20JAFAR%20employs%20an%20attention-based%20module%20designed%20to%20promote%20semantic%20alignment%20between%20high-resolution%20queries%2C%20derived%20from%20low-level%20image%20features%2C%20and%20semantically%20enriched%20low-resolution%20keys%2C%20using%20Spatial%20Feature%20Transform%20%28SFT%29%20modulation.%20Notably%2C%20despite%20the%20absence%20of%20high-resolution%20supervision%2C%20we%20demonstrate%20that%20learning%20at%20low%20upsampling%20ratios%20and%20resolutions%20generalizes%20remarkably%20well%20to%20significantly%20higher%20output%20scales.%20Extensive%20experiments%20show%20that%20JAFAR%20effectively%20recovers%20fine-grained%20spatial%20details%20and%20consistently%20outperforms%20existing%20feature%20upsampling%20methods%20across%20a%20diverse%20set%20of%20downstream%20tasks.%20Project%20page%20at%20https%3A//jafar-upsampler.github.io&entry.1838667208=http%3A//arxiv.org/abs/2506.11136v3&entry.124074799=Read"},
{"title": "Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning", "author": "Minwu Kim and Safal Shrestha and Keith Ross", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has substantially improved the reasoning abilities of large language models (LLMs), yet training often stalls as problems become saturated. We identify the core challenge as the poor accessibility of informative failures: learning signals exist but are rarely encountered during standard rollouts. To address this, we propose failure-prefix conditioning, a simple and effective method for learning from saturated problems. Rather than starting from the original question, our approach reallocates exploration by conditioning training on prefixes derived from rare incorrect reasoning trajectories, thereby exposing the model to failure-prone states. We observe that failure-prefix conditioning yields performance gains matching those of training on medium-difficulty problems, while preserving token efficiency. Furthermore, we analyze the model's robustness, finding that our method reduces performance degradation under misleading failure prefixes, albeit with a mild trade-off in adherence to correct early reasoning. Finally, we demonstrate that an iterative approach, which refreshes failure prefixes during training, unlocks additional gains after performance plateaus. Overall, our results suggest that failure-prefix conditioning offers an effective pathway to extend RLVR training on saturated problems.", "link": "http://arxiv.org/abs/2601.20829v1", "date": "2026-01-28", "relevancy": 2.3775, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4809}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4728}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20Reasoning%20Models%20on%20Saturated%20Problems%20via%20Failure-Prefix%20Conditioning&body=Title%3A%20Training%20Reasoning%20Models%20on%20Saturated%20Problems%20via%20Failure-Prefix%20Conditioning%0AAuthor%3A%20Minwu%20Kim%20and%20Safal%20Shrestha%20and%20Keith%20Ross%0AAbstract%3A%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20has%20substantially%20improved%20the%20reasoning%20abilities%20of%20large%20language%20models%20%28LLMs%29%2C%20yet%20training%20often%20stalls%20as%20problems%20become%20saturated.%20We%20identify%20the%20core%20challenge%20as%20the%20poor%20accessibility%20of%20informative%20failures%3A%20learning%20signals%20exist%20but%20are%20rarely%20encountered%20during%20standard%20rollouts.%20To%20address%20this%2C%20we%20propose%20failure-prefix%20conditioning%2C%20a%20simple%20and%20effective%20method%20for%20learning%20from%20saturated%20problems.%20Rather%20than%20starting%20from%20the%20original%20question%2C%20our%20approach%20reallocates%20exploration%20by%20conditioning%20training%20on%20prefixes%20derived%20from%20rare%20incorrect%20reasoning%20trajectories%2C%20thereby%20exposing%20the%20model%20to%20failure-prone%20states.%20We%20observe%20that%20failure-prefix%20conditioning%20yields%20performance%20gains%20matching%20those%20of%20training%20on%20medium-difficulty%20problems%2C%20while%20preserving%20token%20efficiency.%20Furthermore%2C%20we%20analyze%20the%20model%27s%20robustness%2C%20finding%20that%20our%20method%20reduces%20performance%20degradation%20under%20misleading%20failure%20prefixes%2C%20albeit%20with%20a%20mild%20trade-off%20in%20adherence%20to%20correct%20early%20reasoning.%20Finally%2C%20we%20demonstrate%20that%20an%20iterative%20approach%2C%20which%20refreshes%20failure%20prefixes%20during%20training%2C%20unlocks%20additional%20gains%20after%20performance%20plateaus.%20Overall%2C%20our%20results%20suggest%20that%20failure-prefix%20conditioning%20offers%20an%20effective%20pathway%20to%20extend%20RLVR%20training%20on%20saturated%20problems.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20829v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520Reasoning%2520Models%2520on%2520Saturated%2520Problems%2520via%2520Failure-Prefix%2520Conditioning%26entry.906535625%3DMinwu%2520Kim%2520and%2520Safal%2520Shrestha%2520and%2520Keith%2520Ross%26entry.1292438233%3DReinforcement%2520Learning%2520with%2520Verifiable%2520Rewards%2520%2528RLVR%2529%2520has%2520substantially%2520improved%2520the%2520reasoning%2520abilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520yet%2520training%2520often%2520stalls%2520as%2520problems%2520become%2520saturated.%2520We%2520identify%2520the%2520core%2520challenge%2520as%2520the%2520poor%2520accessibility%2520of%2520informative%2520failures%253A%2520learning%2520signals%2520exist%2520but%2520are%2520rarely%2520encountered%2520during%2520standard%2520rollouts.%2520To%2520address%2520this%252C%2520we%2520propose%2520failure-prefix%2520conditioning%252C%2520a%2520simple%2520and%2520effective%2520method%2520for%2520learning%2520from%2520saturated%2520problems.%2520Rather%2520than%2520starting%2520from%2520the%2520original%2520question%252C%2520our%2520approach%2520reallocates%2520exploration%2520by%2520conditioning%2520training%2520on%2520prefixes%2520derived%2520from%2520rare%2520incorrect%2520reasoning%2520trajectories%252C%2520thereby%2520exposing%2520the%2520model%2520to%2520failure-prone%2520states.%2520We%2520observe%2520that%2520failure-prefix%2520conditioning%2520yields%2520performance%2520gains%2520matching%2520those%2520of%2520training%2520on%2520medium-difficulty%2520problems%252C%2520while%2520preserving%2520token%2520efficiency.%2520Furthermore%252C%2520we%2520analyze%2520the%2520model%2527s%2520robustness%252C%2520finding%2520that%2520our%2520method%2520reduces%2520performance%2520degradation%2520under%2520misleading%2520failure%2520prefixes%252C%2520albeit%2520with%2520a%2520mild%2520trade-off%2520in%2520adherence%2520to%2520correct%2520early%2520reasoning.%2520Finally%252C%2520we%2520demonstrate%2520that%2520an%2520iterative%2520approach%252C%2520which%2520refreshes%2520failure%2520prefixes%2520during%2520training%252C%2520unlocks%2520additional%2520gains%2520after%2520performance%2520plateaus.%2520Overall%252C%2520our%2520results%2520suggest%2520that%2520failure-prefix%2520conditioning%2520offers%2520an%2520effective%2520pathway%2520to%2520extend%2520RLVR%2520training%2520on%2520saturated%2520problems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20829v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20Reasoning%20Models%20on%20Saturated%20Problems%20via%20Failure-Prefix%20Conditioning&entry.906535625=Minwu%20Kim%20and%20Safal%20Shrestha%20and%20Keith%20Ross&entry.1292438233=Reinforcement%20Learning%20with%20Verifiable%20Rewards%20%28RLVR%29%20has%20substantially%20improved%20the%20reasoning%20abilities%20of%20large%20language%20models%20%28LLMs%29%2C%20yet%20training%20often%20stalls%20as%20problems%20become%20saturated.%20We%20identify%20the%20core%20challenge%20as%20the%20poor%20accessibility%20of%20informative%20failures%3A%20learning%20signals%20exist%20but%20are%20rarely%20encountered%20during%20standard%20rollouts.%20To%20address%20this%2C%20we%20propose%20failure-prefix%20conditioning%2C%20a%20simple%20and%20effective%20method%20for%20learning%20from%20saturated%20problems.%20Rather%20than%20starting%20from%20the%20original%20question%2C%20our%20approach%20reallocates%20exploration%20by%20conditioning%20training%20on%20prefixes%20derived%20from%20rare%20incorrect%20reasoning%20trajectories%2C%20thereby%20exposing%20the%20model%20to%20failure-prone%20states.%20We%20observe%20that%20failure-prefix%20conditioning%20yields%20performance%20gains%20matching%20those%20of%20training%20on%20medium-difficulty%20problems%2C%20while%20preserving%20token%20efficiency.%20Furthermore%2C%20we%20analyze%20the%20model%27s%20robustness%2C%20finding%20that%20our%20method%20reduces%20performance%20degradation%20under%20misleading%20failure%20prefixes%2C%20albeit%20with%20a%20mild%20trade-off%20in%20adherence%20to%20correct%20early%20reasoning.%20Finally%2C%20we%20demonstrate%20that%20an%20iterative%20approach%2C%20which%20refreshes%20failure%20prefixes%20during%20training%2C%20unlocks%20additional%20gains%20after%20performance%20plateaus.%20Overall%2C%20our%20results%20suggest%20that%20failure-prefix%20conditioning%20offers%20an%20effective%20pathway%20to%20extend%20RLVR%20training%20on%20saturated%20problems.&entry.1838667208=http%3A//arxiv.org/abs/2601.20829v1&entry.124074799=Read"},
{"title": "Sparsity-Aware Low-Rank Representation for Efficient Fine-Tuning of Large Language Models", "author": "Longteng Zhang and Sen Wu and Shuai Hou and Zhengyu Qing and Zhuo Zheng and Danning Ke and Qihong Lin and Qiang Wang and Shaohuai Shi and Xiaowen Chu", "abstract": "Adapting large pre-trained language models to downstream tasks often entails fine-tuning millions of parameters or deploying costly dense weight updates, which hinders their use in resource-constrained environments. Low-rank Adaptation (LoRA) reduces trainable parameters by factorizing weight updates, yet the underlying dense weights still impose high storage and computation costs. Magnitude-based pruning can yield sparse models but typically degrades LoRA's performance when applied naively. In this paper, we introduce SALR (Sparsity-Aware Low-Rank Representation), a novel fine-tuning paradigm that unifies low-rank adaptation with sparse pruning under a rigorous mean-squared-error framework. We prove that statically pruning only the frozen base weights minimizes the pruning error bound, and we recover the discarded residual information via a truncated-SVD low-rank adapter, which provably reduces per-entry MSE by a factor of $(1 - r/\\min(d,k))$. To maximize hardware efficiency, we fuse multiple low-rank adapters into a single concatenated GEMM, and we adopt a bitmap-based encoding with a two-stage pipelined decoding + GEMM design to achieve true model compression and speedup. Empirically, SALR attains 50\\% sparsity on various LLMs while matching the performance of LoRA on GSM8K and MMLU, reduces model size by $2\\times$, and delivers up to a $1.7\\times$ inference speedup.", "link": "http://arxiv.org/abs/2601.16991v2", "date": "2026-01-28", "relevancy": 2.3665, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4828}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4708}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparsity-Aware%20Low-Rank%20Representation%20for%20Efficient%20Fine-Tuning%20of%20Large%20Language%20Models&body=Title%3A%20Sparsity-Aware%20Low-Rank%20Representation%20for%20Efficient%20Fine-Tuning%20of%20Large%20Language%20Models%0AAuthor%3A%20Longteng%20Zhang%20and%20Sen%20Wu%20and%20Shuai%20Hou%20and%20Zhengyu%20Qing%20and%20Zhuo%20Zheng%20and%20Danning%20Ke%20and%20Qihong%20Lin%20and%20Qiang%20Wang%20and%20Shaohuai%20Shi%20and%20Xiaowen%20Chu%0AAbstract%3A%20Adapting%20large%20pre-trained%20language%20models%20to%20downstream%20tasks%20often%20entails%20fine-tuning%20millions%20of%20parameters%20or%20deploying%20costly%20dense%20weight%20updates%2C%20which%20hinders%20their%20use%20in%20resource-constrained%20environments.%20Low-rank%20Adaptation%20%28LoRA%29%20reduces%20trainable%20parameters%20by%20factorizing%20weight%20updates%2C%20yet%20the%20underlying%20dense%20weights%20still%20impose%20high%20storage%20and%20computation%20costs.%20Magnitude-based%20pruning%20can%20yield%20sparse%20models%20but%20typically%20degrades%20LoRA%27s%20performance%20when%20applied%20naively.%20In%20this%20paper%2C%20we%20introduce%20SALR%20%28Sparsity-Aware%20Low-Rank%20Representation%29%2C%20a%20novel%20fine-tuning%20paradigm%20that%20unifies%20low-rank%20adaptation%20with%20sparse%20pruning%20under%20a%20rigorous%20mean-squared-error%20framework.%20We%20prove%20that%20statically%20pruning%20only%20the%20frozen%20base%20weights%20minimizes%20the%20pruning%20error%20bound%2C%20and%20we%20recover%20the%20discarded%20residual%20information%20via%20a%20truncated-SVD%20low-rank%20adapter%2C%20which%20provably%20reduces%20per-entry%20MSE%20by%20a%20factor%20of%20%24%281%20-%20r/%5Cmin%28d%2Ck%29%29%24.%20To%20maximize%20hardware%20efficiency%2C%20we%20fuse%20multiple%20low-rank%20adapters%20into%20a%20single%20concatenated%20GEMM%2C%20and%20we%20adopt%20a%20bitmap-based%20encoding%20with%20a%20two-stage%20pipelined%20decoding%20%2B%20GEMM%20design%20to%20achieve%20true%20model%20compression%20and%20speedup.%20Empirically%2C%20SALR%20attains%2050%5C%25%20sparsity%20on%20various%20LLMs%20while%20matching%20the%20performance%20of%20LoRA%20on%20GSM8K%20and%20MMLU%2C%20reduces%20model%20size%20by%20%242%5Ctimes%24%2C%20and%20delivers%20up%20to%20a%20%241.7%5Ctimes%24%20inference%20speedup.%0ALink%3A%20http%3A//arxiv.org/abs/2601.16991v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparsity-Aware%2520Low-Rank%2520Representation%2520for%2520Efficient%2520Fine-Tuning%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DLongteng%2520Zhang%2520and%2520Sen%2520Wu%2520and%2520Shuai%2520Hou%2520and%2520Zhengyu%2520Qing%2520and%2520Zhuo%2520Zheng%2520and%2520Danning%2520Ke%2520and%2520Qihong%2520Lin%2520and%2520Qiang%2520Wang%2520and%2520Shaohuai%2520Shi%2520and%2520Xiaowen%2520Chu%26entry.1292438233%3DAdapting%2520large%2520pre-trained%2520language%2520models%2520to%2520downstream%2520tasks%2520often%2520entails%2520fine-tuning%2520millions%2520of%2520parameters%2520or%2520deploying%2520costly%2520dense%2520weight%2520updates%252C%2520which%2520hinders%2520their%2520use%2520in%2520resource-constrained%2520environments.%2520Low-rank%2520Adaptation%2520%2528LoRA%2529%2520reduces%2520trainable%2520parameters%2520by%2520factorizing%2520weight%2520updates%252C%2520yet%2520the%2520underlying%2520dense%2520weights%2520still%2520impose%2520high%2520storage%2520and%2520computation%2520costs.%2520Magnitude-based%2520pruning%2520can%2520yield%2520sparse%2520models%2520but%2520typically%2520degrades%2520LoRA%2527s%2520performance%2520when%2520applied%2520naively.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520SALR%2520%2528Sparsity-Aware%2520Low-Rank%2520Representation%2529%252C%2520a%2520novel%2520fine-tuning%2520paradigm%2520that%2520unifies%2520low-rank%2520adaptation%2520with%2520sparse%2520pruning%2520under%2520a%2520rigorous%2520mean-squared-error%2520framework.%2520We%2520prove%2520that%2520statically%2520pruning%2520only%2520the%2520frozen%2520base%2520weights%2520minimizes%2520the%2520pruning%2520error%2520bound%252C%2520and%2520we%2520recover%2520the%2520discarded%2520residual%2520information%2520via%2520a%2520truncated-SVD%2520low-rank%2520adapter%252C%2520which%2520provably%2520reduces%2520per-entry%2520MSE%2520by%2520a%2520factor%2520of%2520%2524%25281%2520-%2520r/%255Cmin%2528d%252Ck%2529%2529%2524.%2520To%2520maximize%2520hardware%2520efficiency%252C%2520we%2520fuse%2520multiple%2520low-rank%2520adapters%2520into%2520a%2520single%2520concatenated%2520GEMM%252C%2520and%2520we%2520adopt%2520a%2520bitmap-based%2520encoding%2520with%2520a%2520two-stage%2520pipelined%2520decoding%2520%252B%2520GEMM%2520design%2520to%2520achieve%2520true%2520model%2520compression%2520and%2520speedup.%2520Empirically%252C%2520SALR%2520attains%252050%255C%2525%2520sparsity%2520on%2520various%2520LLMs%2520while%2520matching%2520the%2520performance%2520of%2520LoRA%2520on%2520GSM8K%2520and%2520MMLU%252C%2520reduces%2520model%2520size%2520by%2520%25242%255Ctimes%2524%252C%2520and%2520delivers%2520up%2520to%2520a%2520%25241.7%255Ctimes%2524%2520inference%2520speedup.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.16991v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparsity-Aware%20Low-Rank%20Representation%20for%20Efficient%20Fine-Tuning%20of%20Large%20Language%20Models&entry.906535625=Longteng%20Zhang%20and%20Sen%20Wu%20and%20Shuai%20Hou%20and%20Zhengyu%20Qing%20and%20Zhuo%20Zheng%20and%20Danning%20Ke%20and%20Qihong%20Lin%20and%20Qiang%20Wang%20and%20Shaohuai%20Shi%20and%20Xiaowen%20Chu&entry.1292438233=Adapting%20large%20pre-trained%20language%20models%20to%20downstream%20tasks%20often%20entails%20fine-tuning%20millions%20of%20parameters%20or%20deploying%20costly%20dense%20weight%20updates%2C%20which%20hinders%20their%20use%20in%20resource-constrained%20environments.%20Low-rank%20Adaptation%20%28LoRA%29%20reduces%20trainable%20parameters%20by%20factorizing%20weight%20updates%2C%20yet%20the%20underlying%20dense%20weights%20still%20impose%20high%20storage%20and%20computation%20costs.%20Magnitude-based%20pruning%20can%20yield%20sparse%20models%20but%20typically%20degrades%20LoRA%27s%20performance%20when%20applied%20naively.%20In%20this%20paper%2C%20we%20introduce%20SALR%20%28Sparsity-Aware%20Low-Rank%20Representation%29%2C%20a%20novel%20fine-tuning%20paradigm%20that%20unifies%20low-rank%20adaptation%20with%20sparse%20pruning%20under%20a%20rigorous%20mean-squared-error%20framework.%20We%20prove%20that%20statically%20pruning%20only%20the%20frozen%20base%20weights%20minimizes%20the%20pruning%20error%20bound%2C%20and%20we%20recover%20the%20discarded%20residual%20information%20via%20a%20truncated-SVD%20low-rank%20adapter%2C%20which%20provably%20reduces%20per-entry%20MSE%20by%20a%20factor%20of%20%24%281%20-%20r/%5Cmin%28d%2Ck%29%29%24.%20To%20maximize%20hardware%20efficiency%2C%20we%20fuse%20multiple%20low-rank%20adapters%20into%20a%20single%20concatenated%20GEMM%2C%20and%20we%20adopt%20a%20bitmap-based%20encoding%20with%20a%20two-stage%20pipelined%20decoding%20%2B%20GEMM%20design%20to%20achieve%20true%20model%20compression%20and%20speedup.%20Empirically%2C%20SALR%20attains%2050%5C%25%20sparsity%20on%20various%20LLMs%20while%20matching%20the%20performance%20of%20LoRA%20on%20GSM8K%20and%20MMLU%2C%20reduces%20model%20size%20by%20%242%5Ctimes%24%2C%20and%20delivers%20up%20to%20a%20%241.7%5Ctimes%24%20inference%20speedup.&entry.1838667208=http%3A//arxiv.org/abs/2601.16991v2&entry.124074799=Read"},
{"title": "Regularized Gradient Temporal-Difference Learning", "author": "Hyunjun Na and Donghwan Lee", "abstract": "Gradient temporal-difference (GTD) learning algorithms are widely used for off-policy policy evaluation with function approximation. However, existing convergence analyses rely on the restrictive assumption that the so-called feature interaction matrix (FIM) is nonsingular. In practice, the FIM can become singular and leads to instability or degraded performance. In this paper, we propose a regularized optimization objective by reformulating the mean-square projected Bellman error (MSPBE) minimization. This formulation naturally yields a regularized GTD algorithms, referred to as R-GTD, which guarantees convergence to a unique solution even when the FIM is singular. We establish theoretical convergence guarantees and explicit error bounds for the proposed method, and validate its effectiveness through empirical experiments.", "link": "http://arxiv.org/abs/2601.20599v1", "date": "2026-01-28", "relevancy": 2.3645, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4813}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4756}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Regularized%20Gradient%20Temporal-Difference%20Learning&body=Title%3A%20Regularized%20Gradient%20Temporal-Difference%20Learning%0AAuthor%3A%20Hyunjun%20Na%20and%20Donghwan%20Lee%0AAbstract%3A%20Gradient%20temporal-difference%20%28GTD%29%20learning%20algorithms%20are%20widely%20used%20for%20off-policy%20policy%20evaluation%20with%20function%20approximation.%20However%2C%20existing%20convergence%20analyses%20rely%20on%20the%20restrictive%20assumption%20that%20the%20so-called%20feature%20interaction%20matrix%20%28FIM%29%20is%20nonsingular.%20In%20practice%2C%20the%20FIM%20can%20become%20singular%20and%20leads%20to%20instability%20or%20degraded%20performance.%20In%20this%20paper%2C%20we%20propose%20a%20regularized%20optimization%20objective%20by%20reformulating%20the%20mean-square%20projected%20Bellman%20error%20%28MSPBE%29%20minimization.%20This%20formulation%20naturally%20yields%20a%20regularized%20GTD%20algorithms%2C%20referred%20to%20as%20R-GTD%2C%20which%20guarantees%20convergence%20to%20a%20unique%20solution%20even%20when%20the%20FIM%20is%20singular.%20We%20establish%20theoretical%20convergence%20guarantees%20and%20explicit%20error%20bounds%20for%20the%20proposed%20method%2C%20and%20validate%20its%20effectiveness%20through%20empirical%20experiments.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRegularized%2520Gradient%2520Temporal-Difference%2520Learning%26entry.906535625%3DHyunjun%2520Na%2520and%2520Donghwan%2520Lee%26entry.1292438233%3DGradient%2520temporal-difference%2520%2528GTD%2529%2520learning%2520algorithms%2520are%2520widely%2520used%2520for%2520off-policy%2520policy%2520evaluation%2520with%2520function%2520approximation.%2520However%252C%2520existing%2520convergence%2520analyses%2520rely%2520on%2520the%2520restrictive%2520assumption%2520that%2520the%2520so-called%2520feature%2520interaction%2520matrix%2520%2528FIM%2529%2520is%2520nonsingular.%2520In%2520practice%252C%2520the%2520FIM%2520can%2520become%2520singular%2520and%2520leads%2520to%2520instability%2520or%2520degraded%2520performance.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520regularized%2520optimization%2520objective%2520by%2520reformulating%2520the%2520mean-square%2520projected%2520Bellman%2520error%2520%2528MSPBE%2529%2520minimization.%2520This%2520formulation%2520naturally%2520yields%2520a%2520regularized%2520GTD%2520algorithms%252C%2520referred%2520to%2520as%2520R-GTD%252C%2520which%2520guarantees%2520convergence%2520to%2520a%2520unique%2520solution%2520even%2520when%2520the%2520FIM%2520is%2520singular.%2520We%2520establish%2520theoretical%2520convergence%2520guarantees%2520and%2520explicit%2520error%2520bounds%2520for%2520the%2520proposed%2520method%252C%2520and%2520validate%2520its%2520effectiveness%2520through%2520empirical%2520experiments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Regularized%20Gradient%20Temporal-Difference%20Learning&entry.906535625=Hyunjun%20Na%20and%20Donghwan%20Lee&entry.1292438233=Gradient%20temporal-difference%20%28GTD%29%20learning%20algorithms%20are%20widely%20used%20for%20off-policy%20policy%20evaluation%20with%20function%20approximation.%20However%2C%20existing%20convergence%20analyses%20rely%20on%20the%20restrictive%20assumption%20that%20the%20so-called%20feature%20interaction%20matrix%20%28FIM%29%20is%20nonsingular.%20In%20practice%2C%20the%20FIM%20can%20become%20singular%20and%20leads%20to%20instability%20or%20degraded%20performance.%20In%20this%20paper%2C%20we%20propose%20a%20regularized%20optimization%20objective%20by%20reformulating%20the%20mean-square%20projected%20Bellman%20error%20%28MSPBE%29%20minimization.%20This%20formulation%20naturally%20yields%20a%20regularized%20GTD%20algorithms%2C%20referred%20to%20as%20R-GTD%2C%20which%20guarantees%20convergence%20to%20a%20unique%20solution%20even%20when%20the%20FIM%20is%20singular.%20We%20establish%20theoretical%20convergence%20guarantees%20and%20explicit%20error%20bounds%20for%20the%20proposed%20method%2C%20and%20validate%20its%20effectiveness%20through%20empirical%20experiments.&entry.1838667208=http%3A//arxiv.org/abs/2601.20599v1&entry.124074799=Read"},
{"title": "IOTA: Corrective Knowledge-Guided Prompt Learning via Black-White Box Framework", "author": "Shaokun Wang and Yifan Yu and Yuhang He and Weili Guan and Yihong Gong", "abstract": "Recently, adapting pre-trained models to downstream tasks has attracted increasing interest. Previous Parameter-Efficient-Tuning (PET) methods regard the pre-trained model as an opaque Black Box model, relying purely on data-driven optimization and underutilizing their inherent prior knowledge. This oversight limits the models' potential for effective downstream task adaptation. To address these issues, we propose a novel black-whIte bOx prompT leArning framework (IOTA), which integrates a data-driven Black Box module with a knowledge-driven White Box module for downstream task adaptation. Specifically, the White Box module derives corrective knowledge by contrasting the wrong predictions with the right cognition. This knowledge is verbalized into interpretable human prompts and leveraged through a corrective knowledge-guided prompt selection strategy to guide the Black Box module toward more accurate predictions. By jointly leveraging knowledge- and data-driven learning signals, IOTA achieves effective downstream task adaptation. Experimental results on 12 image classification benchmarks under few-shot and easy-to-hard adaptation settings demonstrate the effectiveness of corrective knowledge and the superiority of our method over state-of-the-art methods.", "link": "http://arxiv.org/abs/2601.20526v1", "date": "2026-01-28", "relevancy": 2.3597, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4959}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4605}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4594}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IOTA%3A%20Corrective%20Knowledge-Guided%20Prompt%20Learning%20via%20Black-White%20Box%20Framework&body=Title%3A%20IOTA%3A%20Corrective%20Knowledge-Guided%20Prompt%20Learning%20via%20Black-White%20Box%20Framework%0AAuthor%3A%20Shaokun%20Wang%20and%20Yifan%20Yu%20and%20Yuhang%20He%20and%20Weili%20Guan%20and%20Yihong%20Gong%0AAbstract%3A%20Recently%2C%20adapting%20pre-trained%20models%20to%20downstream%20tasks%20has%20attracted%20increasing%20interest.%20Previous%20Parameter-Efficient-Tuning%20%28PET%29%20methods%20regard%20the%20pre-trained%20model%20as%20an%20opaque%20Black%20Box%20model%2C%20relying%20purely%20on%20data-driven%20optimization%20and%20underutilizing%20their%20inherent%20prior%20knowledge.%20This%20oversight%20limits%20the%20models%27%20potential%20for%20effective%20downstream%20task%20adaptation.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20black-whIte%20bOx%20prompT%20leArning%20framework%20%28IOTA%29%2C%20which%20integrates%20a%20data-driven%20Black%20Box%20module%20with%20a%20knowledge-driven%20White%20Box%20module%20for%20downstream%20task%20adaptation.%20Specifically%2C%20the%20White%20Box%20module%20derives%20corrective%20knowledge%20by%20contrasting%20the%20wrong%20predictions%20with%20the%20right%20cognition.%20This%20knowledge%20is%20verbalized%20into%20interpretable%20human%20prompts%20and%20leveraged%20through%20a%20corrective%20knowledge-guided%20prompt%20selection%20strategy%20to%20guide%20the%20Black%20Box%20module%20toward%20more%20accurate%20predictions.%20By%20jointly%20leveraging%20knowledge-%20and%20data-driven%20learning%20signals%2C%20IOTA%20achieves%20effective%20downstream%20task%20adaptation.%20Experimental%20results%20on%2012%20image%20classification%20benchmarks%20under%20few-shot%20and%20easy-to-hard%20adaptation%20settings%20demonstrate%20the%20effectiveness%20of%20corrective%20knowledge%20and%20the%20superiority%20of%20our%20method%20over%20state-of-the-art%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20526v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIOTA%253A%2520Corrective%2520Knowledge-Guided%2520Prompt%2520Learning%2520via%2520Black-White%2520Box%2520Framework%26entry.906535625%3DShaokun%2520Wang%2520and%2520Yifan%2520Yu%2520and%2520Yuhang%2520He%2520and%2520Weili%2520Guan%2520and%2520Yihong%2520Gong%26entry.1292438233%3DRecently%252C%2520adapting%2520pre-trained%2520models%2520to%2520downstream%2520tasks%2520has%2520attracted%2520increasing%2520interest.%2520Previous%2520Parameter-Efficient-Tuning%2520%2528PET%2529%2520methods%2520regard%2520the%2520pre-trained%2520model%2520as%2520an%2520opaque%2520Black%2520Box%2520model%252C%2520relying%2520purely%2520on%2520data-driven%2520optimization%2520and%2520underutilizing%2520their%2520inherent%2520prior%2520knowledge.%2520This%2520oversight%2520limits%2520the%2520models%2527%2520potential%2520for%2520effective%2520downstream%2520task%2520adaptation.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%2520black-whIte%2520bOx%2520prompT%2520leArning%2520framework%2520%2528IOTA%2529%252C%2520which%2520integrates%2520a%2520data-driven%2520Black%2520Box%2520module%2520with%2520a%2520knowledge-driven%2520White%2520Box%2520module%2520for%2520downstream%2520task%2520adaptation.%2520Specifically%252C%2520the%2520White%2520Box%2520module%2520derives%2520corrective%2520knowledge%2520by%2520contrasting%2520the%2520wrong%2520predictions%2520with%2520the%2520right%2520cognition.%2520This%2520knowledge%2520is%2520verbalized%2520into%2520interpretable%2520human%2520prompts%2520and%2520leveraged%2520through%2520a%2520corrective%2520knowledge-guided%2520prompt%2520selection%2520strategy%2520to%2520guide%2520the%2520Black%2520Box%2520module%2520toward%2520more%2520accurate%2520predictions.%2520By%2520jointly%2520leveraging%2520knowledge-%2520and%2520data-driven%2520learning%2520signals%252C%2520IOTA%2520achieves%2520effective%2520downstream%2520task%2520adaptation.%2520Experimental%2520results%2520on%252012%2520image%2520classification%2520benchmarks%2520under%2520few-shot%2520and%2520easy-to-hard%2520adaptation%2520settings%2520demonstrate%2520the%2520effectiveness%2520of%2520corrective%2520knowledge%2520and%2520the%2520superiority%2520of%2520our%2520method%2520over%2520state-of-the-art%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20526v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IOTA%3A%20Corrective%20Knowledge-Guided%20Prompt%20Learning%20via%20Black-White%20Box%20Framework&entry.906535625=Shaokun%20Wang%20and%20Yifan%20Yu%20and%20Yuhang%20He%20and%20Weili%20Guan%20and%20Yihong%20Gong&entry.1292438233=Recently%2C%20adapting%20pre-trained%20models%20to%20downstream%20tasks%20has%20attracted%20increasing%20interest.%20Previous%20Parameter-Efficient-Tuning%20%28PET%29%20methods%20regard%20the%20pre-trained%20model%20as%20an%20opaque%20Black%20Box%20model%2C%20relying%20purely%20on%20data-driven%20optimization%20and%20underutilizing%20their%20inherent%20prior%20knowledge.%20This%20oversight%20limits%20the%20models%27%20potential%20for%20effective%20downstream%20task%20adaptation.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%20black-whIte%20bOx%20prompT%20leArning%20framework%20%28IOTA%29%2C%20which%20integrates%20a%20data-driven%20Black%20Box%20module%20with%20a%20knowledge-driven%20White%20Box%20module%20for%20downstream%20task%20adaptation.%20Specifically%2C%20the%20White%20Box%20module%20derives%20corrective%20knowledge%20by%20contrasting%20the%20wrong%20predictions%20with%20the%20right%20cognition.%20This%20knowledge%20is%20verbalized%20into%20interpretable%20human%20prompts%20and%20leveraged%20through%20a%20corrective%20knowledge-guided%20prompt%20selection%20strategy%20to%20guide%20the%20Black%20Box%20module%20toward%20more%20accurate%20predictions.%20By%20jointly%20leveraging%20knowledge-%20and%20data-driven%20learning%20signals%2C%20IOTA%20achieves%20effective%20downstream%20task%20adaptation.%20Experimental%20results%20on%2012%20image%20classification%20benchmarks%20under%20few-shot%20and%20easy-to-hard%20adaptation%20settings%20demonstrate%20the%20effectiveness%20of%20corrective%20knowledge%20and%20the%20superiority%20of%20our%20method%20over%20state-of-the-art%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2601.20526v1&entry.124074799=Read"},
{"title": "An explainable framework for the relationship between dementia and glucose metabolism patterns", "author": "C. V\u00e1zquez-Garc\u00eda and F. J. Mart\u00ednez-Murcia and F. Segovia Rom\u00e1n and A. Forte and J. Ram\u00edrez and I. Ill\u00e1n and A. Hern\u00e1ndez-Segura and C. Jim\u00e9nez-Mesa and Juan M. G\u00f3rriz", "abstract": "High-dimensional neuroimaging data presents challenges for assessing neurodegenerative diseases due to complex non-linear relationships. Variational Autoencoders (VAEs) can encode scans into lower-dimensional latent spaces capturing disease-relevant features. We propose a semi-supervised VAE framework with a flexible similarity regularization term that aligns selected latent variables with clinical or biomarker measures of dementia progression. This allows adapting the similarity metric and supervised variables to specific goals or available data. We demonstrate the approach using PET scans from the Alzheimer's Disease Neuroimaging Initiative (ADNI), guiding the first latent dimension to align with a cognitive score. Using this supervised latent variable, we generate average reconstructions across levels of cognitive impairment. Voxel-wise GLM analysis reveals reduced metabolism in key regions, mainly the hippocampus, and within major Resting State Networks, particularly the Default Mode and Central Executive Networks. The remaining latent variables encode affine transformations and intensity variations, capturing confounds such as inter-subject variability and site effects. Our framework effectively extracts disease-related patterns aligned with established Alzheimer's biomarkers, offering an interpretable and adaptable tool for studying neurodegenerative progression.", "link": "http://arxiv.org/abs/2601.20480v1", "date": "2026-01-28", "relevancy": 2.3562, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4744}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4744}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20explainable%20framework%20for%20the%20relationship%20between%20dementia%20and%20glucose%20metabolism%20patterns&body=Title%3A%20An%20explainable%20framework%20for%20the%20relationship%20between%20dementia%20and%20glucose%20metabolism%20patterns%0AAuthor%3A%20C.%20V%C3%A1zquez-Garc%C3%ADa%20and%20F.%20J.%20Mart%C3%ADnez-Murcia%20and%20F.%20Segovia%20Rom%C3%A1n%20and%20A.%20Forte%20and%20J.%20Ram%C3%ADrez%20and%20I.%20Ill%C3%A1n%20and%20A.%20Hern%C3%A1ndez-Segura%20and%20C.%20Jim%C3%A9nez-Mesa%20and%20Juan%20M.%20G%C3%B3rriz%0AAbstract%3A%20High-dimensional%20neuroimaging%20data%20presents%20challenges%20for%20assessing%20neurodegenerative%20diseases%20due%20to%20complex%20non-linear%20relationships.%20Variational%20Autoencoders%20%28VAEs%29%20can%20encode%20scans%20into%20lower-dimensional%20latent%20spaces%20capturing%20disease-relevant%20features.%20We%20propose%20a%20semi-supervised%20VAE%20framework%20with%20a%20flexible%20similarity%20regularization%20term%20that%20aligns%20selected%20latent%20variables%20with%20clinical%20or%20biomarker%20measures%20of%20dementia%20progression.%20This%20allows%20adapting%20the%20similarity%20metric%20and%20supervised%20variables%20to%20specific%20goals%20or%20available%20data.%20We%20demonstrate%20the%20approach%20using%20PET%20scans%20from%20the%20Alzheimer%27s%20Disease%20Neuroimaging%20Initiative%20%28ADNI%29%2C%20guiding%20the%20first%20latent%20dimension%20to%20align%20with%20a%20cognitive%20score.%20Using%20this%20supervised%20latent%20variable%2C%20we%20generate%20average%20reconstructions%20across%20levels%20of%20cognitive%20impairment.%20Voxel-wise%20GLM%20analysis%20reveals%20reduced%20metabolism%20in%20key%20regions%2C%20mainly%20the%20hippocampus%2C%20and%20within%20major%20Resting%20State%20Networks%2C%20particularly%20the%20Default%20Mode%20and%20Central%20Executive%20Networks.%20The%20remaining%20latent%20variables%20encode%20affine%20transformations%20and%20intensity%20variations%2C%20capturing%20confounds%20such%20as%20inter-subject%20variability%20and%20site%20effects.%20Our%20framework%20effectively%20extracts%20disease-related%20patterns%20aligned%20with%20established%20Alzheimer%27s%20biomarkers%2C%20offering%20an%20interpretable%20and%20adaptable%20tool%20for%20studying%20neurodegenerative%20progression.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20480v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520explainable%2520framework%2520for%2520the%2520relationship%2520between%2520dementia%2520and%2520glucose%2520metabolism%2520patterns%26entry.906535625%3DC.%2520V%25C3%25A1zquez-Garc%25C3%25ADa%2520and%2520F.%2520J.%2520Mart%25C3%25ADnez-Murcia%2520and%2520F.%2520Segovia%2520Rom%25C3%25A1n%2520and%2520A.%2520Forte%2520and%2520J.%2520Ram%25C3%25ADrez%2520and%2520I.%2520Ill%25C3%25A1n%2520and%2520A.%2520Hern%25C3%25A1ndez-Segura%2520and%2520C.%2520Jim%25C3%25A9nez-Mesa%2520and%2520Juan%2520M.%2520G%25C3%25B3rriz%26entry.1292438233%3DHigh-dimensional%2520neuroimaging%2520data%2520presents%2520challenges%2520for%2520assessing%2520neurodegenerative%2520diseases%2520due%2520to%2520complex%2520non-linear%2520relationships.%2520Variational%2520Autoencoders%2520%2528VAEs%2529%2520can%2520encode%2520scans%2520into%2520lower-dimensional%2520latent%2520spaces%2520capturing%2520disease-relevant%2520features.%2520We%2520propose%2520a%2520semi-supervised%2520VAE%2520framework%2520with%2520a%2520flexible%2520similarity%2520regularization%2520term%2520that%2520aligns%2520selected%2520latent%2520variables%2520with%2520clinical%2520or%2520biomarker%2520measures%2520of%2520dementia%2520progression.%2520This%2520allows%2520adapting%2520the%2520similarity%2520metric%2520and%2520supervised%2520variables%2520to%2520specific%2520goals%2520or%2520available%2520data.%2520We%2520demonstrate%2520the%2520approach%2520using%2520PET%2520scans%2520from%2520the%2520Alzheimer%2527s%2520Disease%2520Neuroimaging%2520Initiative%2520%2528ADNI%2529%252C%2520guiding%2520the%2520first%2520latent%2520dimension%2520to%2520align%2520with%2520a%2520cognitive%2520score.%2520Using%2520this%2520supervised%2520latent%2520variable%252C%2520we%2520generate%2520average%2520reconstructions%2520across%2520levels%2520of%2520cognitive%2520impairment.%2520Voxel-wise%2520GLM%2520analysis%2520reveals%2520reduced%2520metabolism%2520in%2520key%2520regions%252C%2520mainly%2520the%2520hippocampus%252C%2520and%2520within%2520major%2520Resting%2520State%2520Networks%252C%2520particularly%2520the%2520Default%2520Mode%2520and%2520Central%2520Executive%2520Networks.%2520The%2520remaining%2520latent%2520variables%2520encode%2520affine%2520transformations%2520and%2520intensity%2520variations%252C%2520capturing%2520confounds%2520such%2520as%2520inter-subject%2520variability%2520and%2520site%2520effects.%2520Our%2520framework%2520effectively%2520extracts%2520disease-related%2520patterns%2520aligned%2520with%2520established%2520Alzheimer%2527s%2520biomarkers%252C%2520offering%2520an%2520interpretable%2520and%2520adaptable%2520tool%2520for%2520studying%2520neurodegenerative%2520progression.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20480v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20explainable%20framework%20for%20the%20relationship%20between%20dementia%20and%20glucose%20metabolism%20patterns&entry.906535625=C.%20V%C3%A1zquez-Garc%C3%ADa%20and%20F.%20J.%20Mart%C3%ADnez-Murcia%20and%20F.%20Segovia%20Rom%C3%A1n%20and%20A.%20Forte%20and%20J.%20Ram%C3%ADrez%20and%20I.%20Ill%C3%A1n%20and%20A.%20Hern%C3%A1ndez-Segura%20and%20C.%20Jim%C3%A9nez-Mesa%20and%20Juan%20M.%20G%C3%B3rriz&entry.1292438233=High-dimensional%20neuroimaging%20data%20presents%20challenges%20for%20assessing%20neurodegenerative%20diseases%20due%20to%20complex%20non-linear%20relationships.%20Variational%20Autoencoders%20%28VAEs%29%20can%20encode%20scans%20into%20lower-dimensional%20latent%20spaces%20capturing%20disease-relevant%20features.%20We%20propose%20a%20semi-supervised%20VAE%20framework%20with%20a%20flexible%20similarity%20regularization%20term%20that%20aligns%20selected%20latent%20variables%20with%20clinical%20or%20biomarker%20measures%20of%20dementia%20progression.%20This%20allows%20adapting%20the%20similarity%20metric%20and%20supervised%20variables%20to%20specific%20goals%20or%20available%20data.%20We%20demonstrate%20the%20approach%20using%20PET%20scans%20from%20the%20Alzheimer%27s%20Disease%20Neuroimaging%20Initiative%20%28ADNI%29%2C%20guiding%20the%20first%20latent%20dimension%20to%20align%20with%20a%20cognitive%20score.%20Using%20this%20supervised%20latent%20variable%2C%20we%20generate%20average%20reconstructions%20across%20levels%20of%20cognitive%20impairment.%20Voxel-wise%20GLM%20analysis%20reveals%20reduced%20metabolism%20in%20key%20regions%2C%20mainly%20the%20hippocampus%2C%20and%20within%20major%20Resting%20State%20Networks%2C%20particularly%20the%20Default%20Mode%20and%20Central%20Executive%20Networks.%20The%20remaining%20latent%20variables%20encode%20affine%20transformations%20and%20intensity%20variations%2C%20capturing%20confounds%20such%20as%20inter-subject%20variability%20and%20site%20effects.%20Our%20framework%20effectively%20extracts%20disease-related%20patterns%20aligned%20with%20established%20Alzheimer%27s%20biomarkers%2C%20offering%20an%20interpretable%20and%20adaptable%20tool%20for%20studying%20neurodegenerative%20progression.&entry.1838667208=http%3A//arxiv.org/abs/2601.20480v1&entry.124074799=Read"},
{"title": "Robust Distributed Learning under Resource Constraints: Decentralized Quantile Estimation via (Asynchronous) ADMM", "author": "Anna van Elst and Igor Colin and Stephan Cl\u00e9men\u00e7on", "abstract": "Specifications for decentralized learning on resource-constrained edge devices require algorithms that are communication-efficient, robust to data corruption, and lightweight in memory usage. While state-of-the-art gossip-based methods satisfy the first requirement, achieving robustness remains challenging. Asynchronous decentralized ADMM-based methods have been explored for estimating the median, a statistical centrality measure that is notoriously more robust than the mean. However, existing approaches require memory that scales with node degree, making them impractical when memory is limited. In this paper, we propose AsylADMM, a novel gossip algorithm for decentralized median and quantile estimation, primarily designed for asynchronous updates and requiring only two variables per node. We analyze a synchronous variant of AsylADMM to establish theoretical guarantees and empirically demonstrate fast convergence for the asynchronous algorithm. We then show that our algorithm enables quantile-based trimming, geometric median estimation, and depth-based trimming, with quantile-based trimming empirically outperforming existing rank-based methods. Finally, we provide a novel theoretical analysis of rank-based trimming via Markov chain theory.", "link": "http://arxiv.org/abs/2601.20571v1", "date": "2026-01-28", "relevancy": 2.3311, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4673}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4663}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4651}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Distributed%20Learning%20under%20Resource%20Constraints%3A%20Decentralized%20Quantile%20Estimation%20via%20%28Asynchronous%29%20ADMM&body=Title%3A%20Robust%20Distributed%20Learning%20under%20Resource%20Constraints%3A%20Decentralized%20Quantile%20Estimation%20via%20%28Asynchronous%29%20ADMM%0AAuthor%3A%20Anna%20van%20Elst%20and%20Igor%20Colin%20and%20Stephan%20Cl%C3%A9men%C3%A7on%0AAbstract%3A%20Specifications%20for%20decentralized%20learning%20on%20resource-constrained%20edge%20devices%20require%20algorithms%20that%20are%20communication-efficient%2C%20robust%20to%20data%20corruption%2C%20and%20lightweight%20in%20memory%20usage.%20While%20state-of-the-art%20gossip-based%20methods%20satisfy%20the%20first%20requirement%2C%20achieving%20robustness%20remains%20challenging.%20Asynchronous%20decentralized%20ADMM-based%20methods%20have%20been%20explored%20for%20estimating%20the%20median%2C%20a%20statistical%20centrality%20measure%20that%20is%20notoriously%20more%20robust%20than%20the%20mean.%20However%2C%20existing%20approaches%20require%20memory%20that%20scales%20with%20node%20degree%2C%20making%20them%20impractical%20when%20memory%20is%20limited.%20In%20this%20paper%2C%20we%20propose%20AsylADMM%2C%20a%20novel%20gossip%20algorithm%20for%20decentralized%20median%20and%20quantile%20estimation%2C%20primarily%20designed%20for%20asynchronous%20updates%20and%20requiring%20only%20two%20variables%20per%20node.%20We%20analyze%20a%20synchronous%20variant%20of%20AsylADMM%20to%20establish%20theoretical%20guarantees%20and%20empirically%20demonstrate%20fast%20convergence%20for%20the%20asynchronous%20algorithm.%20We%20then%20show%20that%20our%20algorithm%20enables%20quantile-based%20trimming%2C%20geometric%20median%20estimation%2C%20and%20depth-based%20trimming%2C%20with%20quantile-based%20trimming%20empirically%20outperforming%20existing%20rank-based%20methods.%20Finally%2C%20we%20provide%20a%20novel%20theoretical%20analysis%20of%20rank-based%20trimming%20via%20Markov%20chain%20theory.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20571v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Distributed%2520Learning%2520under%2520Resource%2520Constraints%253A%2520Decentralized%2520Quantile%2520Estimation%2520via%2520%2528Asynchronous%2529%2520ADMM%26entry.906535625%3DAnna%2520van%2520Elst%2520and%2520Igor%2520Colin%2520and%2520Stephan%2520Cl%25C3%25A9men%25C3%25A7on%26entry.1292438233%3DSpecifications%2520for%2520decentralized%2520learning%2520on%2520resource-constrained%2520edge%2520devices%2520require%2520algorithms%2520that%2520are%2520communication-efficient%252C%2520robust%2520to%2520data%2520corruption%252C%2520and%2520lightweight%2520in%2520memory%2520usage.%2520While%2520state-of-the-art%2520gossip-based%2520methods%2520satisfy%2520the%2520first%2520requirement%252C%2520achieving%2520robustness%2520remains%2520challenging.%2520Asynchronous%2520decentralized%2520ADMM-based%2520methods%2520have%2520been%2520explored%2520for%2520estimating%2520the%2520median%252C%2520a%2520statistical%2520centrality%2520measure%2520that%2520is%2520notoriously%2520more%2520robust%2520than%2520the%2520mean.%2520However%252C%2520existing%2520approaches%2520require%2520memory%2520that%2520scales%2520with%2520node%2520degree%252C%2520making%2520them%2520impractical%2520when%2520memory%2520is%2520limited.%2520In%2520this%2520paper%252C%2520we%2520propose%2520AsylADMM%252C%2520a%2520novel%2520gossip%2520algorithm%2520for%2520decentralized%2520median%2520and%2520quantile%2520estimation%252C%2520primarily%2520designed%2520for%2520asynchronous%2520updates%2520and%2520requiring%2520only%2520two%2520variables%2520per%2520node.%2520We%2520analyze%2520a%2520synchronous%2520variant%2520of%2520AsylADMM%2520to%2520establish%2520theoretical%2520guarantees%2520and%2520empirically%2520demonstrate%2520fast%2520convergence%2520for%2520the%2520asynchronous%2520algorithm.%2520We%2520then%2520show%2520that%2520our%2520algorithm%2520enables%2520quantile-based%2520trimming%252C%2520geometric%2520median%2520estimation%252C%2520and%2520depth-based%2520trimming%252C%2520with%2520quantile-based%2520trimming%2520empirically%2520outperforming%2520existing%2520rank-based%2520methods.%2520Finally%252C%2520we%2520provide%2520a%2520novel%2520theoretical%2520analysis%2520of%2520rank-based%2520trimming%2520via%2520Markov%2520chain%2520theory.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20571v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Distributed%20Learning%20under%20Resource%20Constraints%3A%20Decentralized%20Quantile%20Estimation%20via%20%28Asynchronous%29%20ADMM&entry.906535625=Anna%20van%20Elst%20and%20Igor%20Colin%20and%20Stephan%20Cl%C3%A9men%C3%A7on&entry.1292438233=Specifications%20for%20decentralized%20learning%20on%20resource-constrained%20edge%20devices%20require%20algorithms%20that%20are%20communication-efficient%2C%20robust%20to%20data%20corruption%2C%20and%20lightweight%20in%20memory%20usage.%20While%20state-of-the-art%20gossip-based%20methods%20satisfy%20the%20first%20requirement%2C%20achieving%20robustness%20remains%20challenging.%20Asynchronous%20decentralized%20ADMM-based%20methods%20have%20been%20explored%20for%20estimating%20the%20median%2C%20a%20statistical%20centrality%20measure%20that%20is%20notoriously%20more%20robust%20than%20the%20mean.%20However%2C%20existing%20approaches%20require%20memory%20that%20scales%20with%20node%20degree%2C%20making%20them%20impractical%20when%20memory%20is%20limited.%20In%20this%20paper%2C%20we%20propose%20AsylADMM%2C%20a%20novel%20gossip%20algorithm%20for%20decentralized%20median%20and%20quantile%20estimation%2C%20primarily%20designed%20for%20asynchronous%20updates%20and%20requiring%20only%20two%20variables%20per%20node.%20We%20analyze%20a%20synchronous%20variant%20of%20AsylADMM%20to%20establish%20theoretical%20guarantees%20and%20empirically%20demonstrate%20fast%20convergence%20for%20the%20asynchronous%20algorithm.%20We%20then%20show%20that%20our%20algorithm%20enables%20quantile-based%20trimming%2C%20geometric%20median%20estimation%2C%20and%20depth-based%20trimming%2C%20with%20quantile-based%20trimming%20empirically%20outperforming%20existing%20rank-based%20methods.%20Finally%2C%20we%20provide%20a%20novel%20theoretical%20analysis%20of%20rank-based%20trimming%20via%20Markov%20chain%20theory.&entry.1838667208=http%3A//arxiv.org/abs/2601.20571v1&entry.124074799=Read"},
{"title": "C3Box: A CLIP-based Class-Incremental Learning Toolbox", "author": "Hao Sun and Da-Wei Zhou", "abstract": "Traditional machine learning systems are typically designed for static data distributions, which suffer from catastrophic forgetting when learning from evolving data streams. Class-Incremental Learning (CIL) addresses this challenge by enabling learning systems to continuously learn new classes while preserving prior knowledge. With the rise of pre-trained models (PTMs) such as CLIP, leveraging their strong generalization and semantic alignment capabilities has become a promising direction in CIL. However, existing CLIP-based CIL methods are often scattered across disparate codebases, rely on inconsistent configurations, hindering fair comparisons, reproducibility, and practical adoption. Therefore, we propose C3Box (CLIP-based Class-inCremental learning toolBOX), a modular and comprehensive Python toolbox. C3Box integrates representative traditional CIL methods, ViT-based CIL methods, and state-of-the-art CLIP-based CIL methods into a unified CLIP-based framework. By inheriting the streamlined design of PyCIL, C3Box provides a JSON-based configuration and standardized execution pipeline. This design enables reproducible experimentation with low engineering overhead and makes C3Box a reliable benchmark platform for continual learning research. Designed to be user-friendly, C3Box relies only on widely used open-source libraries and supports major operating systems. The code is available at https://github.com/LAMDA-CL/C3Box.", "link": "http://arxiv.org/abs/2601.20852v1", "date": "2026-01-28", "relevancy": 2.3234, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5186}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4414}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4341}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20C3Box%3A%20A%20CLIP-based%20Class-Incremental%20Learning%20Toolbox&body=Title%3A%20C3Box%3A%20A%20CLIP-based%20Class-Incremental%20Learning%20Toolbox%0AAuthor%3A%20Hao%20Sun%20and%20Da-Wei%20Zhou%0AAbstract%3A%20Traditional%20machine%20learning%20systems%20are%20typically%20designed%20for%20static%20data%20distributions%2C%20which%20suffer%20from%20catastrophic%20forgetting%20when%20learning%20from%20evolving%20data%20streams.%20Class-Incremental%20Learning%20%28CIL%29%20addresses%20this%20challenge%20by%20enabling%20learning%20systems%20to%20continuously%20learn%20new%20classes%20while%20preserving%20prior%20knowledge.%20With%20the%20rise%20of%20pre-trained%20models%20%28PTMs%29%20such%20as%20CLIP%2C%20leveraging%20their%20strong%20generalization%20and%20semantic%20alignment%20capabilities%20has%20become%20a%20promising%20direction%20in%20CIL.%20However%2C%20existing%20CLIP-based%20CIL%20methods%20are%20often%20scattered%20across%20disparate%20codebases%2C%20rely%20on%20inconsistent%20configurations%2C%20hindering%20fair%20comparisons%2C%20reproducibility%2C%20and%20practical%20adoption.%20Therefore%2C%20we%20propose%20C3Box%20%28CLIP-based%20Class-inCremental%20learning%20toolBOX%29%2C%20a%20modular%20and%20comprehensive%20Python%20toolbox.%20C3Box%20integrates%20representative%20traditional%20CIL%20methods%2C%20ViT-based%20CIL%20methods%2C%20and%20state-of-the-art%20CLIP-based%20CIL%20methods%20into%20a%20unified%20CLIP-based%20framework.%20By%20inheriting%20the%20streamlined%20design%20of%20PyCIL%2C%20C3Box%20provides%20a%20JSON-based%20configuration%20and%20standardized%20execution%20pipeline.%20This%20design%20enables%20reproducible%20experimentation%20with%20low%20engineering%20overhead%20and%20makes%20C3Box%20a%20reliable%20benchmark%20platform%20for%20continual%20learning%20research.%20Designed%20to%20be%20user-friendly%2C%20C3Box%20relies%20only%20on%20widely%20used%20open-source%20libraries%20and%20supports%20major%20operating%20systems.%20The%20code%20is%20available%20at%20https%3A//github.com/LAMDA-CL/C3Box.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20852v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DC3Box%253A%2520A%2520CLIP-based%2520Class-Incremental%2520Learning%2520Toolbox%26entry.906535625%3DHao%2520Sun%2520and%2520Da-Wei%2520Zhou%26entry.1292438233%3DTraditional%2520machine%2520learning%2520systems%2520are%2520typically%2520designed%2520for%2520static%2520data%2520distributions%252C%2520which%2520suffer%2520from%2520catastrophic%2520forgetting%2520when%2520learning%2520from%2520evolving%2520data%2520streams.%2520Class-Incremental%2520Learning%2520%2528CIL%2529%2520addresses%2520this%2520challenge%2520by%2520enabling%2520learning%2520systems%2520to%2520continuously%2520learn%2520new%2520classes%2520while%2520preserving%2520prior%2520knowledge.%2520With%2520the%2520rise%2520of%2520pre-trained%2520models%2520%2528PTMs%2529%2520such%2520as%2520CLIP%252C%2520leveraging%2520their%2520strong%2520generalization%2520and%2520semantic%2520alignment%2520capabilities%2520has%2520become%2520a%2520promising%2520direction%2520in%2520CIL.%2520However%252C%2520existing%2520CLIP-based%2520CIL%2520methods%2520are%2520often%2520scattered%2520across%2520disparate%2520codebases%252C%2520rely%2520on%2520inconsistent%2520configurations%252C%2520hindering%2520fair%2520comparisons%252C%2520reproducibility%252C%2520and%2520practical%2520adoption.%2520Therefore%252C%2520we%2520propose%2520C3Box%2520%2528CLIP-based%2520Class-inCremental%2520learning%2520toolBOX%2529%252C%2520a%2520modular%2520and%2520comprehensive%2520Python%2520toolbox.%2520C3Box%2520integrates%2520representative%2520traditional%2520CIL%2520methods%252C%2520ViT-based%2520CIL%2520methods%252C%2520and%2520state-of-the-art%2520CLIP-based%2520CIL%2520methods%2520into%2520a%2520unified%2520CLIP-based%2520framework.%2520By%2520inheriting%2520the%2520streamlined%2520design%2520of%2520PyCIL%252C%2520C3Box%2520provides%2520a%2520JSON-based%2520configuration%2520and%2520standardized%2520execution%2520pipeline.%2520This%2520design%2520enables%2520reproducible%2520experimentation%2520with%2520low%2520engineering%2520overhead%2520and%2520makes%2520C3Box%2520a%2520reliable%2520benchmark%2520platform%2520for%2520continual%2520learning%2520research.%2520Designed%2520to%2520be%2520user-friendly%252C%2520C3Box%2520relies%2520only%2520on%2520widely%2520used%2520open-source%2520libraries%2520and%2520supports%2520major%2520operating%2520systems.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/LAMDA-CL/C3Box.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20852v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=C3Box%3A%20A%20CLIP-based%20Class-Incremental%20Learning%20Toolbox&entry.906535625=Hao%20Sun%20and%20Da-Wei%20Zhou&entry.1292438233=Traditional%20machine%20learning%20systems%20are%20typically%20designed%20for%20static%20data%20distributions%2C%20which%20suffer%20from%20catastrophic%20forgetting%20when%20learning%20from%20evolving%20data%20streams.%20Class-Incremental%20Learning%20%28CIL%29%20addresses%20this%20challenge%20by%20enabling%20learning%20systems%20to%20continuously%20learn%20new%20classes%20while%20preserving%20prior%20knowledge.%20With%20the%20rise%20of%20pre-trained%20models%20%28PTMs%29%20such%20as%20CLIP%2C%20leveraging%20their%20strong%20generalization%20and%20semantic%20alignment%20capabilities%20has%20become%20a%20promising%20direction%20in%20CIL.%20However%2C%20existing%20CLIP-based%20CIL%20methods%20are%20often%20scattered%20across%20disparate%20codebases%2C%20rely%20on%20inconsistent%20configurations%2C%20hindering%20fair%20comparisons%2C%20reproducibility%2C%20and%20practical%20adoption.%20Therefore%2C%20we%20propose%20C3Box%20%28CLIP-based%20Class-inCremental%20learning%20toolBOX%29%2C%20a%20modular%20and%20comprehensive%20Python%20toolbox.%20C3Box%20integrates%20representative%20traditional%20CIL%20methods%2C%20ViT-based%20CIL%20methods%2C%20and%20state-of-the-art%20CLIP-based%20CIL%20methods%20into%20a%20unified%20CLIP-based%20framework.%20By%20inheriting%20the%20streamlined%20design%20of%20PyCIL%2C%20C3Box%20provides%20a%20JSON-based%20configuration%20and%20standardized%20execution%20pipeline.%20This%20design%20enables%20reproducible%20experimentation%20with%20low%20engineering%20overhead%20and%20makes%20C3Box%20a%20reliable%20benchmark%20platform%20for%20continual%20learning%20research.%20Designed%20to%20be%20user-friendly%2C%20C3Box%20relies%20only%20on%20widely%20used%20open-source%20libraries%20and%20supports%20major%20operating%20systems.%20The%20code%20is%20available%20at%20https%3A//github.com/LAMDA-CL/C3Box.&entry.1838667208=http%3A//arxiv.org/abs/2601.20852v1&entry.124074799=Read"},
{"title": "Learning From a Steady Hand: A Weakly Supervised Agent for Robot Assistance under Microscopy", "author": "Huanyu Tian and Martin Huber and Lingyun Zeng and Zhe Han and Wayne Bennett and Giuseppe Silvestri and Gerardo Mendizabal-Ruiz and Tom Vercauteren and Alejandro Chavez-Badiola and Christos Bergeles", "abstract": "This paper rethinks steady-hand robotic manipulation by using a weakly supervised framework that fuses calibration-aware perception with admittance control. Unlike conventional automation that relies on labor-intensive 2D labeling, our framework leverages reusable warm-up trajectories to extract implicit spatial information, thereby achieving calibration-aware, depth-resolved perception without the need for external fiducials or manual depth annotation. By explicitly characterizing residuals from observation and calibration models, the system establishes a task-space error budget from recorded warm-ups. The uncertainty budget yields a lateral closed-loop accuracy of approx. 49 micrometers at 95% confidence (worst-case testing subset) and a depth accuracy of <= 291 micrometers at 95% confidence bound during large in-plane moves. In a within-subject user study (N=8), the learned agent reduces overall NASA-TLX workload by 77.1% relative to the simple steady-hand assistance baseline. These results demonstrate that the weakly supervised agent improves the reliability of microscope-guided biomedical micromanipulation without introducing complex setup requirements, offering a practical framework for microscope-guided intervention.", "link": "http://arxiv.org/abs/2601.20776v1", "date": "2026-01-28", "relevancy": 2.3076, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6007}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5608}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20From%20a%20Steady%20Hand%3A%20A%20Weakly%20Supervised%20Agent%20for%20Robot%20Assistance%20under%20Microscopy&body=Title%3A%20Learning%20From%20a%20Steady%20Hand%3A%20A%20Weakly%20Supervised%20Agent%20for%20Robot%20Assistance%20under%20Microscopy%0AAuthor%3A%20Huanyu%20Tian%20and%20Martin%20Huber%20and%20Lingyun%20Zeng%20and%20Zhe%20Han%20and%20Wayne%20Bennett%20and%20Giuseppe%20Silvestri%20and%20Gerardo%20Mendizabal-Ruiz%20and%20Tom%20Vercauteren%20and%20Alejandro%20Chavez-Badiola%20and%20Christos%20Bergeles%0AAbstract%3A%20This%20paper%20rethinks%20steady-hand%20robotic%20manipulation%20by%20using%20a%20weakly%20supervised%20framework%20that%20fuses%20calibration-aware%20perception%20with%20admittance%20control.%20Unlike%20conventional%20automation%20that%20relies%20on%20labor-intensive%202D%20labeling%2C%20our%20framework%20leverages%20reusable%20warm-up%20trajectories%20to%20extract%20implicit%20spatial%20information%2C%20thereby%20achieving%20calibration-aware%2C%20depth-resolved%20perception%20without%20the%20need%20for%20external%20fiducials%20or%20manual%20depth%20annotation.%20By%20explicitly%20characterizing%20residuals%20from%20observation%20and%20calibration%20models%2C%20the%20system%20establishes%20a%20task-space%20error%20budget%20from%20recorded%20warm-ups.%20The%20uncertainty%20budget%20yields%20a%20lateral%20closed-loop%20accuracy%20of%20approx.%2049%20micrometers%20at%2095%25%20confidence%20%28worst-case%20testing%20subset%29%20and%20a%20depth%20accuracy%20of%20%3C%3D%20291%20micrometers%20at%2095%25%20confidence%20bound%20during%20large%20in-plane%20moves.%20In%20a%20within-subject%20user%20study%20%28N%3D8%29%2C%20the%20learned%20agent%20reduces%20overall%20NASA-TLX%20workload%20by%2077.1%25%20relative%20to%20the%20simple%20steady-hand%20assistance%20baseline.%20These%20results%20demonstrate%20that%20the%20weakly%20supervised%20agent%20improves%20the%20reliability%20of%20microscope-guided%20biomedical%20micromanipulation%20without%20introducing%20complex%20setup%20requirements%2C%20offering%20a%20practical%20framework%20for%20microscope-guided%20intervention.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20776v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520From%2520a%2520Steady%2520Hand%253A%2520A%2520Weakly%2520Supervised%2520Agent%2520for%2520Robot%2520Assistance%2520under%2520Microscopy%26entry.906535625%3DHuanyu%2520Tian%2520and%2520Martin%2520Huber%2520and%2520Lingyun%2520Zeng%2520and%2520Zhe%2520Han%2520and%2520Wayne%2520Bennett%2520and%2520Giuseppe%2520Silvestri%2520and%2520Gerardo%2520Mendizabal-Ruiz%2520and%2520Tom%2520Vercauteren%2520and%2520Alejandro%2520Chavez-Badiola%2520and%2520Christos%2520Bergeles%26entry.1292438233%3DThis%2520paper%2520rethinks%2520steady-hand%2520robotic%2520manipulation%2520by%2520using%2520a%2520weakly%2520supervised%2520framework%2520that%2520fuses%2520calibration-aware%2520perception%2520with%2520admittance%2520control.%2520Unlike%2520conventional%2520automation%2520that%2520relies%2520on%2520labor-intensive%25202D%2520labeling%252C%2520our%2520framework%2520leverages%2520reusable%2520warm-up%2520trajectories%2520to%2520extract%2520implicit%2520spatial%2520information%252C%2520thereby%2520achieving%2520calibration-aware%252C%2520depth-resolved%2520perception%2520without%2520the%2520need%2520for%2520external%2520fiducials%2520or%2520manual%2520depth%2520annotation.%2520By%2520explicitly%2520characterizing%2520residuals%2520from%2520observation%2520and%2520calibration%2520models%252C%2520the%2520system%2520establishes%2520a%2520task-space%2520error%2520budget%2520from%2520recorded%2520warm-ups.%2520The%2520uncertainty%2520budget%2520yields%2520a%2520lateral%2520closed-loop%2520accuracy%2520of%2520approx.%252049%2520micrometers%2520at%252095%2525%2520confidence%2520%2528worst-case%2520testing%2520subset%2529%2520and%2520a%2520depth%2520accuracy%2520of%2520%253C%253D%2520291%2520micrometers%2520at%252095%2525%2520confidence%2520bound%2520during%2520large%2520in-plane%2520moves.%2520In%2520a%2520within-subject%2520user%2520study%2520%2528N%253D8%2529%252C%2520the%2520learned%2520agent%2520reduces%2520overall%2520NASA-TLX%2520workload%2520by%252077.1%2525%2520relative%2520to%2520the%2520simple%2520steady-hand%2520assistance%2520baseline.%2520These%2520results%2520demonstrate%2520that%2520the%2520weakly%2520supervised%2520agent%2520improves%2520the%2520reliability%2520of%2520microscope-guided%2520biomedical%2520micromanipulation%2520without%2520introducing%2520complex%2520setup%2520requirements%252C%2520offering%2520a%2520practical%2520framework%2520for%2520microscope-guided%2520intervention.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20776v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20From%20a%20Steady%20Hand%3A%20A%20Weakly%20Supervised%20Agent%20for%20Robot%20Assistance%20under%20Microscopy&entry.906535625=Huanyu%20Tian%20and%20Martin%20Huber%20and%20Lingyun%20Zeng%20and%20Zhe%20Han%20and%20Wayne%20Bennett%20and%20Giuseppe%20Silvestri%20and%20Gerardo%20Mendizabal-Ruiz%20and%20Tom%20Vercauteren%20and%20Alejandro%20Chavez-Badiola%20and%20Christos%20Bergeles&entry.1292438233=This%20paper%20rethinks%20steady-hand%20robotic%20manipulation%20by%20using%20a%20weakly%20supervised%20framework%20that%20fuses%20calibration-aware%20perception%20with%20admittance%20control.%20Unlike%20conventional%20automation%20that%20relies%20on%20labor-intensive%202D%20labeling%2C%20our%20framework%20leverages%20reusable%20warm-up%20trajectories%20to%20extract%20implicit%20spatial%20information%2C%20thereby%20achieving%20calibration-aware%2C%20depth-resolved%20perception%20without%20the%20need%20for%20external%20fiducials%20or%20manual%20depth%20annotation.%20By%20explicitly%20characterizing%20residuals%20from%20observation%20and%20calibration%20models%2C%20the%20system%20establishes%20a%20task-space%20error%20budget%20from%20recorded%20warm-ups.%20The%20uncertainty%20budget%20yields%20a%20lateral%20closed-loop%20accuracy%20of%20approx.%2049%20micrometers%20at%2095%25%20confidence%20%28worst-case%20testing%20subset%29%20and%20a%20depth%20accuracy%20of%20%3C%3D%20291%20micrometers%20at%2095%25%20confidence%20bound%20during%20large%20in-plane%20moves.%20In%20a%20within-subject%20user%20study%20%28N%3D8%29%2C%20the%20learned%20agent%20reduces%20overall%20NASA-TLX%20workload%20by%2077.1%25%20relative%20to%20the%20simple%20steady-hand%20assistance%20baseline.%20These%20results%20demonstrate%20that%20the%20weakly%20supervised%20agent%20improves%20the%20reliability%20of%20microscope-guided%20biomedical%20micromanipulation%20without%20introducing%20complex%20setup%20requirements%2C%20offering%20a%20practical%20framework%20for%20microscope-guided%20intervention.&entry.1838667208=http%3A//arxiv.org/abs/2601.20776v1&entry.124074799=Read"},
{"title": "ReactionMamba: Generating Short & Long Human Reaction Sequences", "author": "Hajra Anwar Beg and Baptiste Chopin and Hao Tang and Mohamed Daoudi", "abstract": "We present ReactionMamba, a novel framework for generating long 3D human reaction motions. Reaction-Mamba integrates a motion VAE for efficient motion encoding with Mamba-based state-space models to decode temporally consistent reactions. This design enables ReactionMamba to generate both short sequences of simple motions and long sequences of complex motions, such as dance and martial arts. We evaluate ReactionMamba on three datasets--NTU120-AS, Lindy Hop, and InterX--and demonstrate competitive performance in terms of realism, diversity, and long-sequence generation compared to previous methods, including InterFormer, ReMoS, and Ready-to-React, while achieving substantial improvements in inference speed.", "link": "http://arxiv.org/abs/2512.00208v2", "date": "2026-01-28", "relevancy": 2.3051, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6371}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5353}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReactionMamba%3A%20Generating%20Short%20%26%20Long%20Human%20Reaction%20Sequences&body=Title%3A%20ReactionMamba%3A%20Generating%20Short%20%26%20Long%20Human%20Reaction%20Sequences%0AAuthor%3A%20Hajra%20Anwar%20Beg%20and%20Baptiste%20Chopin%20and%20Hao%20Tang%20and%20Mohamed%20Daoudi%0AAbstract%3A%20We%20present%20ReactionMamba%2C%20a%20novel%20framework%20for%20generating%20long%203D%20human%20reaction%20motions.%20Reaction-Mamba%20integrates%20a%20motion%20VAE%20for%20efficient%20motion%20encoding%20with%20Mamba-based%20state-space%20models%20to%20decode%20temporally%20consistent%20reactions.%20This%20design%20enables%20ReactionMamba%20to%20generate%20both%20short%20sequences%20of%20simple%20motions%20and%20long%20sequences%20of%20complex%20motions%2C%20such%20as%20dance%20and%20martial%20arts.%20We%20evaluate%20ReactionMamba%20on%20three%20datasets--NTU120-AS%2C%20Lindy%20Hop%2C%20and%20InterX--and%20demonstrate%20competitive%20performance%20in%20terms%20of%20realism%2C%20diversity%2C%20and%20long-sequence%20generation%20compared%20to%20previous%20methods%2C%20including%20InterFormer%2C%20ReMoS%2C%20and%20Ready-to-React%2C%20while%20achieving%20substantial%20improvements%20in%20inference%20speed.%0ALink%3A%20http%3A//arxiv.org/abs/2512.00208v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReactionMamba%253A%2520Generating%2520Short%2520%2526%2520Long%2520Human%2520Reaction%2520Sequences%26entry.906535625%3DHajra%2520Anwar%2520Beg%2520and%2520Baptiste%2520Chopin%2520and%2520Hao%2520Tang%2520and%2520Mohamed%2520Daoudi%26entry.1292438233%3DWe%2520present%2520ReactionMamba%252C%2520a%2520novel%2520framework%2520for%2520generating%2520long%25203D%2520human%2520reaction%2520motions.%2520Reaction-Mamba%2520integrates%2520a%2520motion%2520VAE%2520for%2520efficient%2520motion%2520encoding%2520with%2520Mamba-based%2520state-space%2520models%2520to%2520decode%2520temporally%2520consistent%2520reactions.%2520This%2520design%2520enables%2520ReactionMamba%2520to%2520generate%2520both%2520short%2520sequences%2520of%2520simple%2520motions%2520and%2520long%2520sequences%2520of%2520complex%2520motions%252C%2520such%2520as%2520dance%2520and%2520martial%2520arts.%2520We%2520evaluate%2520ReactionMamba%2520on%2520three%2520datasets--NTU120-AS%252C%2520Lindy%2520Hop%252C%2520and%2520InterX--and%2520demonstrate%2520competitive%2520performance%2520in%2520terms%2520of%2520realism%252C%2520diversity%252C%2520and%2520long-sequence%2520generation%2520compared%2520to%2520previous%2520methods%252C%2520including%2520InterFormer%252C%2520ReMoS%252C%2520and%2520Ready-to-React%252C%2520while%2520achieving%2520substantial%2520improvements%2520in%2520inference%2520speed.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.00208v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReactionMamba%3A%20Generating%20Short%20%26%20Long%20Human%20Reaction%20Sequences&entry.906535625=Hajra%20Anwar%20Beg%20and%20Baptiste%20Chopin%20and%20Hao%20Tang%20and%20Mohamed%20Daoudi&entry.1292438233=We%20present%20ReactionMamba%2C%20a%20novel%20framework%20for%20generating%20long%203D%20human%20reaction%20motions.%20Reaction-Mamba%20integrates%20a%20motion%20VAE%20for%20efficient%20motion%20encoding%20with%20Mamba-based%20state-space%20models%20to%20decode%20temporally%20consistent%20reactions.%20This%20design%20enables%20ReactionMamba%20to%20generate%20both%20short%20sequences%20of%20simple%20motions%20and%20long%20sequences%20of%20complex%20motions%2C%20such%20as%20dance%20and%20martial%20arts.%20We%20evaluate%20ReactionMamba%20on%20three%20datasets--NTU120-AS%2C%20Lindy%20Hop%2C%20and%20InterX--and%20demonstrate%20competitive%20performance%20in%20terms%20of%20realism%2C%20diversity%2C%20and%20long-sequence%20generation%20compared%20to%20previous%20methods%2C%20including%20InterFormer%2C%20ReMoS%2C%20and%20Ready-to-React%2C%20while%20achieving%20substantial%20improvements%20in%20inference%20speed.&entry.1838667208=http%3A//arxiv.org/abs/2512.00208v2&entry.124074799=Read"},
{"title": "SegRap2025: A Benchmark of Gross Tumor Volume and Lymph Node Clinical Target Volume Segmentation for Radiotherapy Planning of Nasopharyngeal Carcinoma", "author": "Jia Fu and Litingyu Wang and He Li and Zihao Luo and Huamin Wang and Chenyuan Bian and Zijun Gao and Chunbin Gu and Xin Weng and Jianghao Wu and Yicheng Wu and Jin Ye and Linhao Li and Yiwen Ye and Yong Xia and Elias Tappeiner and Fei He and Abdul qayyum and Moona Mazher and Steven A Niederer and Junqiang Chen and Chuanyi Huang and Lisheng Wang and Zhaohu Xing and Hongqiu Wang and Lei Zhu and Shichuan Zhang and Shaoting Zhang and Wenjun Liao and Guotai Wang", "abstract": "Accurate delineation of Gross Tumor Volume (GTV), Lymph Node Clinical Target Volume (LN CTV), and Organ-at-Risk (OAR) from Computed Tomography (CT) scans is essential for precise radiotherapy planning in Nasopharyngeal Carcinoma (NPC). Building upon SegRap2023, which focused on OAR and GTV segmentation using single-center paired non-contrast CT (ncCT) and contrast-enhanced CT (ceCT) scans, the SegRap2025 challenge aims to enhance the generalizability and robustness of segmentation models across imaging centers and modalities. SegRap2025 comprises two tasks: Task01 addresses GTV segmentation using paired CT from the SegRap2023 dataset, with an additional external testing set to evaluate cross-center generalization, and Task02 focuses on LN CTV segmentation using multi-center training data and an unseen external testing set, where each case contains paired CT scans or a single modality, emphasizing both cross-center and cross-modality robustness. This paper presents the challenge setup and provides a comprehensive analysis of the solutions submitted by ten participating teams. For GTV segmentation task, the top-performing models achieved average Dice Similarity Coefficient (DSC) of 74.61% and 56.79% on the internal and external testing cohorts, respectively. For LN CTV segmentation task, the highest average DSC values reached 60.24%, 60.50%, and 57.23% on paired CT, ceCT-only, and ncCT-only subsets, respectively. SegRap2025 establishes a large-scale multi-center, multi-modality benchmark for evaluating the generalization and robustness in radiotherapy target segmentation, providing valuable insights toward clinically applicable automated radiotherapy planning systems. The benchmark is available at: https://hilab-git.github.io/SegRap2025_Challenge.", "link": "http://arxiv.org/abs/2601.20575v1", "date": "2026-01-28", "relevancy": 2.2901, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4739}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4525}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SegRap2025%3A%20A%20Benchmark%20of%20Gross%20Tumor%20Volume%20and%20Lymph%20Node%20Clinical%20Target%20Volume%20Segmentation%20for%20Radiotherapy%20Planning%20of%20Nasopharyngeal%20Carcinoma&body=Title%3A%20SegRap2025%3A%20A%20Benchmark%20of%20Gross%20Tumor%20Volume%20and%20Lymph%20Node%20Clinical%20Target%20Volume%20Segmentation%20for%20Radiotherapy%20Planning%20of%20Nasopharyngeal%20Carcinoma%0AAuthor%3A%20Jia%20Fu%20and%20Litingyu%20Wang%20and%20He%20Li%20and%20Zihao%20Luo%20and%20Huamin%20Wang%20and%20Chenyuan%20Bian%20and%20Zijun%20Gao%20and%20Chunbin%20Gu%20and%20Xin%20Weng%20and%20Jianghao%20Wu%20and%20Yicheng%20Wu%20and%20Jin%20Ye%20and%20Linhao%20Li%20and%20Yiwen%20Ye%20and%20Yong%20Xia%20and%20Elias%20Tappeiner%20and%20Fei%20He%20and%20Abdul%20qayyum%20and%20Moona%20Mazher%20and%20Steven%20A%20Niederer%20and%20Junqiang%20Chen%20and%20Chuanyi%20Huang%20and%20Lisheng%20Wang%20and%20Zhaohu%20Xing%20and%20Hongqiu%20Wang%20and%20Lei%20Zhu%20and%20Shichuan%20Zhang%20and%20Shaoting%20Zhang%20and%20Wenjun%20Liao%20and%20Guotai%20Wang%0AAbstract%3A%20Accurate%20delineation%20of%20Gross%20Tumor%20Volume%20%28GTV%29%2C%20Lymph%20Node%20Clinical%20Target%20Volume%20%28LN%20CTV%29%2C%20and%20Organ-at-Risk%20%28OAR%29%20from%20Computed%20Tomography%20%28CT%29%20scans%20is%20essential%20for%20precise%20radiotherapy%20planning%20in%20Nasopharyngeal%20Carcinoma%20%28NPC%29.%20Building%20upon%20SegRap2023%2C%20which%20focused%20on%20OAR%20and%20GTV%20segmentation%20using%20single-center%20paired%20non-contrast%20CT%20%28ncCT%29%20and%20contrast-enhanced%20CT%20%28ceCT%29%20scans%2C%20the%20SegRap2025%20challenge%20aims%20to%20enhance%20the%20generalizability%20and%20robustness%20of%20segmentation%20models%20across%20imaging%20centers%20and%20modalities.%20SegRap2025%20comprises%20two%20tasks%3A%20Task01%20addresses%20GTV%20segmentation%20using%20paired%20CT%20from%20the%20SegRap2023%20dataset%2C%20with%20an%20additional%20external%20testing%20set%20to%20evaluate%20cross-center%20generalization%2C%20and%20Task02%20focuses%20on%20LN%20CTV%20segmentation%20using%20multi-center%20training%20data%20and%20an%20unseen%20external%20testing%20set%2C%20where%20each%20case%20contains%20paired%20CT%20scans%20or%20a%20single%20modality%2C%20emphasizing%20both%20cross-center%20and%20cross-modality%20robustness.%20This%20paper%20presents%20the%20challenge%20setup%20and%20provides%20a%20comprehensive%20analysis%20of%20the%20solutions%20submitted%20by%20ten%20participating%20teams.%20For%20GTV%20segmentation%20task%2C%20the%20top-performing%20models%20achieved%20average%20Dice%20Similarity%20Coefficient%20%28DSC%29%20of%2074.61%25%20and%2056.79%25%20on%20the%20internal%20and%20external%20testing%20cohorts%2C%20respectively.%20For%20LN%20CTV%20segmentation%20task%2C%20the%20highest%20average%20DSC%20values%20reached%2060.24%25%2C%2060.50%25%2C%20and%2057.23%25%20on%20paired%20CT%2C%20ceCT-only%2C%20and%20ncCT-only%20subsets%2C%20respectively.%20SegRap2025%20establishes%20a%20large-scale%20multi-center%2C%20multi-modality%20benchmark%20for%20evaluating%20the%20generalization%20and%20robustness%20in%20radiotherapy%20target%20segmentation%2C%20providing%20valuable%20insights%20toward%20clinically%20applicable%20automated%20radiotherapy%20planning%20systems.%20The%20benchmark%20is%20available%20at%3A%20https%3A//hilab-git.github.io/SegRap2025_Challenge.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20575v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegRap2025%253A%2520A%2520Benchmark%2520of%2520Gross%2520Tumor%2520Volume%2520and%2520Lymph%2520Node%2520Clinical%2520Target%2520Volume%2520Segmentation%2520for%2520Radiotherapy%2520Planning%2520of%2520Nasopharyngeal%2520Carcinoma%26entry.906535625%3DJia%2520Fu%2520and%2520Litingyu%2520Wang%2520and%2520He%2520Li%2520and%2520Zihao%2520Luo%2520and%2520Huamin%2520Wang%2520and%2520Chenyuan%2520Bian%2520and%2520Zijun%2520Gao%2520and%2520Chunbin%2520Gu%2520and%2520Xin%2520Weng%2520and%2520Jianghao%2520Wu%2520and%2520Yicheng%2520Wu%2520and%2520Jin%2520Ye%2520and%2520Linhao%2520Li%2520and%2520Yiwen%2520Ye%2520and%2520Yong%2520Xia%2520and%2520Elias%2520Tappeiner%2520and%2520Fei%2520He%2520and%2520Abdul%2520qayyum%2520and%2520Moona%2520Mazher%2520and%2520Steven%2520A%2520Niederer%2520and%2520Junqiang%2520Chen%2520and%2520Chuanyi%2520Huang%2520and%2520Lisheng%2520Wang%2520and%2520Zhaohu%2520Xing%2520and%2520Hongqiu%2520Wang%2520and%2520Lei%2520Zhu%2520and%2520Shichuan%2520Zhang%2520and%2520Shaoting%2520Zhang%2520and%2520Wenjun%2520Liao%2520and%2520Guotai%2520Wang%26entry.1292438233%3DAccurate%2520delineation%2520of%2520Gross%2520Tumor%2520Volume%2520%2528GTV%2529%252C%2520Lymph%2520Node%2520Clinical%2520Target%2520Volume%2520%2528LN%2520CTV%2529%252C%2520and%2520Organ-at-Risk%2520%2528OAR%2529%2520from%2520Computed%2520Tomography%2520%2528CT%2529%2520scans%2520is%2520essential%2520for%2520precise%2520radiotherapy%2520planning%2520in%2520Nasopharyngeal%2520Carcinoma%2520%2528NPC%2529.%2520Building%2520upon%2520SegRap2023%252C%2520which%2520focused%2520on%2520OAR%2520and%2520GTV%2520segmentation%2520using%2520single-center%2520paired%2520non-contrast%2520CT%2520%2528ncCT%2529%2520and%2520contrast-enhanced%2520CT%2520%2528ceCT%2529%2520scans%252C%2520the%2520SegRap2025%2520challenge%2520aims%2520to%2520enhance%2520the%2520generalizability%2520and%2520robustness%2520of%2520segmentation%2520models%2520across%2520imaging%2520centers%2520and%2520modalities.%2520SegRap2025%2520comprises%2520two%2520tasks%253A%2520Task01%2520addresses%2520GTV%2520segmentation%2520using%2520paired%2520CT%2520from%2520the%2520SegRap2023%2520dataset%252C%2520with%2520an%2520additional%2520external%2520testing%2520set%2520to%2520evaluate%2520cross-center%2520generalization%252C%2520and%2520Task02%2520focuses%2520on%2520LN%2520CTV%2520segmentation%2520using%2520multi-center%2520training%2520data%2520and%2520an%2520unseen%2520external%2520testing%2520set%252C%2520where%2520each%2520case%2520contains%2520paired%2520CT%2520scans%2520or%2520a%2520single%2520modality%252C%2520emphasizing%2520both%2520cross-center%2520and%2520cross-modality%2520robustness.%2520This%2520paper%2520presents%2520the%2520challenge%2520setup%2520and%2520provides%2520a%2520comprehensive%2520analysis%2520of%2520the%2520solutions%2520submitted%2520by%2520ten%2520participating%2520teams.%2520For%2520GTV%2520segmentation%2520task%252C%2520the%2520top-performing%2520models%2520achieved%2520average%2520Dice%2520Similarity%2520Coefficient%2520%2528DSC%2529%2520of%252074.61%2525%2520and%252056.79%2525%2520on%2520the%2520internal%2520and%2520external%2520testing%2520cohorts%252C%2520respectively.%2520For%2520LN%2520CTV%2520segmentation%2520task%252C%2520the%2520highest%2520average%2520DSC%2520values%2520reached%252060.24%2525%252C%252060.50%2525%252C%2520and%252057.23%2525%2520on%2520paired%2520CT%252C%2520ceCT-only%252C%2520and%2520ncCT-only%2520subsets%252C%2520respectively.%2520SegRap2025%2520establishes%2520a%2520large-scale%2520multi-center%252C%2520multi-modality%2520benchmark%2520for%2520evaluating%2520the%2520generalization%2520and%2520robustness%2520in%2520radiotherapy%2520target%2520segmentation%252C%2520providing%2520valuable%2520insights%2520toward%2520clinically%2520applicable%2520automated%2520radiotherapy%2520planning%2520systems.%2520The%2520benchmark%2520is%2520available%2520at%253A%2520https%253A//hilab-git.github.io/SegRap2025_Challenge.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20575v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SegRap2025%3A%20A%20Benchmark%20of%20Gross%20Tumor%20Volume%20and%20Lymph%20Node%20Clinical%20Target%20Volume%20Segmentation%20for%20Radiotherapy%20Planning%20of%20Nasopharyngeal%20Carcinoma&entry.906535625=Jia%20Fu%20and%20Litingyu%20Wang%20and%20He%20Li%20and%20Zihao%20Luo%20and%20Huamin%20Wang%20and%20Chenyuan%20Bian%20and%20Zijun%20Gao%20and%20Chunbin%20Gu%20and%20Xin%20Weng%20and%20Jianghao%20Wu%20and%20Yicheng%20Wu%20and%20Jin%20Ye%20and%20Linhao%20Li%20and%20Yiwen%20Ye%20and%20Yong%20Xia%20and%20Elias%20Tappeiner%20and%20Fei%20He%20and%20Abdul%20qayyum%20and%20Moona%20Mazher%20and%20Steven%20A%20Niederer%20and%20Junqiang%20Chen%20and%20Chuanyi%20Huang%20and%20Lisheng%20Wang%20and%20Zhaohu%20Xing%20and%20Hongqiu%20Wang%20and%20Lei%20Zhu%20and%20Shichuan%20Zhang%20and%20Shaoting%20Zhang%20and%20Wenjun%20Liao%20and%20Guotai%20Wang&entry.1292438233=Accurate%20delineation%20of%20Gross%20Tumor%20Volume%20%28GTV%29%2C%20Lymph%20Node%20Clinical%20Target%20Volume%20%28LN%20CTV%29%2C%20and%20Organ-at-Risk%20%28OAR%29%20from%20Computed%20Tomography%20%28CT%29%20scans%20is%20essential%20for%20precise%20radiotherapy%20planning%20in%20Nasopharyngeal%20Carcinoma%20%28NPC%29.%20Building%20upon%20SegRap2023%2C%20which%20focused%20on%20OAR%20and%20GTV%20segmentation%20using%20single-center%20paired%20non-contrast%20CT%20%28ncCT%29%20and%20contrast-enhanced%20CT%20%28ceCT%29%20scans%2C%20the%20SegRap2025%20challenge%20aims%20to%20enhance%20the%20generalizability%20and%20robustness%20of%20segmentation%20models%20across%20imaging%20centers%20and%20modalities.%20SegRap2025%20comprises%20two%20tasks%3A%20Task01%20addresses%20GTV%20segmentation%20using%20paired%20CT%20from%20the%20SegRap2023%20dataset%2C%20with%20an%20additional%20external%20testing%20set%20to%20evaluate%20cross-center%20generalization%2C%20and%20Task02%20focuses%20on%20LN%20CTV%20segmentation%20using%20multi-center%20training%20data%20and%20an%20unseen%20external%20testing%20set%2C%20where%20each%20case%20contains%20paired%20CT%20scans%20or%20a%20single%20modality%2C%20emphasizing%20both%20cross-center%20and%20cross-modality%20robustness.%20This%20paper%20presents%20the%20challenge%20setup%20and%20provides%20a%20comprehensive%20analysis%20of%20the%20solutions%20submitted%20by%20ten%20participating%20teams.%20For%20GTV%20segmentation%20task%2C%20the%20top-performing%20models%20achieved%20average%20Dice%20Similarity%20Coefficient%20%28DSC%29%20of%2074.61%25%20and%2056.79%25%20on%20the%20internal%20and%20external%20testing%20cohorts%2C%20respectively.%20For%20LN%20CTV%20segmentation%20task%2C%20the%20highest%20average%20DSC%20values%20reached%2060.24%25%2C%2060.50%25%2C%20and%2057.23%25%20on%20paired%20CT%2C%20ceCT-only%2C%20and%20ncCT-only%20subsets%2C%20respectively.%20SegRap2025%20establishes%20a%20large-scale%20multi-center%2C%20multi-modality%20benchmark%20for%20evaluating%20the%20generalization%20and%20robustness%20in%20radiotherapy%20target%20segmentation%2C%20providing%20valuable%20insights%20toward%20clinically%20applicable%20automated%20radiotherapy%20planning%20systems.%20The%20benchmark%20is%20available%20at%3A%20https%3A//hilab-git.github.io/SegRap2025_Challenge.&entry.1838667208=http%3A//arxiv.org/abs/2601.20575v1&entry.124074799=Read"},
{"title": "A Scalable Inter-edge Correlation Modeling in CopulaGNN for Link Sign Prediction", "author": "Jinkyu Sung and Myunggeum Jee and Joonseok Lee", "abstract": "Link sign prediction on a signed graph is a task to determine whether the relationship represented by an edge is positive or negative. Since the presence of negative edges violates the graph homophily assumption that adjacent nodes are similar, regular graph methods have not been applicable without auxiliary structures to handle them. We aim to directly model the latent statistical dependency among edges with the Gaussian copula and its corresponding correlation matrix, extending CopulaGNN (Ma et al., 2021). However, a naive modeling of edge-edge relations is computationally intractable even for a graph with moderate scale. To address this, we propose to 1) represent the correlation matrix as a Gramian of edge embeddings, significantly reducing the number of parameters, and 2) reformulate the conditional probability distribution to dramatically reduce the inference cost. We theoretically verify scalability of our method by proving its linear convergence. Also, our extensive experiments demonstrate that it achieves significantly faster convergence than baselines, maintaining competitive prediction performance to the state-of-the-art models.", "link": "http://arxiv.org/abs/2601.19175v2", "date": "2026-01-28", "relevancy": 2.2861, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4703}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4564}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Scalable%20Inter-edge%20Correlation%20Modeling%20in%20CopulaGNN%20for%20Link%20Sign%20Prediction&body=Title%3A%20A%20Scalable%20Inter-edge%20Correlation%20Modeling%20in%20CopulaGNN%20for%20Link%20Sign%20Prediction%0AAuthor%3A%20Jinkyu%20Sung%20and%20Myunggeum%20Jee%20and%20Joonseok%20Lee%0AAbstract%3A%20Link%20sign%20prediction%20on%20a%20signed%20graph%20is%20a%20task%20to%20determine%20whether%20the%20relationship%20represented%20by%20an%20edge%20is%20positive%20or%20negative.%20Since%20the%20presence%20of%20negative%20edges%20violates%20the%20graph%20homophily%20assumption%20that%20adjacent%20nodes%20are%20similar%2C%20regular%20graph%20methods%20have%20not%20been%20applicable%20without%20auxiliary%20structures%20to%20handle%20them.%20We%20aim%20to%20directly%20model%20the%20latent%20statistical%20dependency%20among%20edges%20with%20the%20Gaussian%20copula%20and%20its%20corresponding%20correlation%20matrix%2C%20extending%20CopulaGNN%20%28Ma%20et%20al.%2C%202021%29.%20However%2C%20a%20naive%20modeling%20of%20edge-edge%20relations%20is%20computationally%20intractable%20even%20for%20a%20graph%20with%20moderate%20scale.%20To%20address%20this%2C%20we%20propose%20to%201%29%20represent%20the%20correlation%20matrix%20as%20a%20Gramian%20of%20edge%20embeddings%2C%20significantly%20reducing%20the%20number%20of%20parameters%2C%20and%202%29%20reformulate%20the%20conditional%20probability%20distribution%20to%20dramatically%20reduce%20the%20inference%20cost.%20We%20theoretically%20verify%20scalability%20of%20our%20method%20by%20proving%20its%20linear%20convergence.%20Also%2C%20our%20extensive%20experiments%20demonstrate%20that%20it%20achieves%20significantly%20faster%20convergence%20than%20baselines%2C%20maintaining%20competitive%20prediction%20performance%20to%20the%20state-of-the-art%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19175v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Scalable%2520Inter-edge%2520Correlation%2520Modeling%2520in%2520CopulaGNN%2520for%2520Link%2520Sign%2520Prediction%26entry.906535625%3DJinkyu%2520Sung%2520and%2520Myunggeum%2520Jee%2520and%2520Joonseok%2520Lee%26entry.1292438233%3DLink%2520sign%2520prediction%2520on%2520a%2520signed%2520graph%2520is%2520a%2520task%2520to%2520determine%2520whether%2520the%2520relationship%2520represented%2520by%2520an%2520edge%2520is%2520positive%2520or%2520negative.%2520Since%2520the%2520presence%2520of%2520negative%2520edges%2520violates%2520the%2520graph%2520homophily%2520assumption%2520that%2520adjacent%2520nodes%2520are%2520similar%252C%2520regular%2520graph%2520methods%2520have%2520not%2520been%2520applicable%2520without%2520auxiliary%2520structures%2520to%2520handle%2520them.%2520We%2520aim%2520to%2520directly%2520model%2520the%2520latent%2520statistical%2520dependency%2520among%2520edges%2520with%2520the%2520Gaussian%2520copula%2520and%2520its%2520corresponding%2520correlation%2520matrix%252C%2520extending%2520CopulaGNN%2520%2528Ma%2520et%2520al.%252C%25202021%2529.%2520However%252C%2520a%2520naive%2520modeling%2520of%2520edge-edge%2520relations%2520is%2520computationally%2520intractable%2520even%2520for%2520a%2520graph%2520with%2520moderate%2520scale.%2520To%2520address%2520this%252C%2520we%2520propose%2520to%25201%2529%2520represent%2520the%2520correlation%2520matrix%2520as%2520a%2520Gramian%2520of%2520edge%2520embeddings%252C%2520significantly%2520reducing%2520the%2520number%2520of%2520parameters%252C%2520and%25202%2529%2520reformulate%2520the%2520conditional%2520probability%2520distribution%2520to%2520dramatically%2520reduce%2520the%2520inference%2520cost.%2520We%2520theoretically%2520verify%2520scalability%2520of%2520our%2520method%2520by%2520proving%2520its%2520linear%2520convergence.%2520Also%252C%2520our%2520extensive%2520experiments%2520demonstrate%2520that%2520it%2520achieves%2520significantly%2520faster%2520convergence%2520than%2520baselines%252C%2520maintaining%2520competitive%2520prediction%2520performance%2520to%2520the%2520state-of-the-art%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19175v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Scalable%20Inter-edge%20Correlation%20Modeling%20in%20CopulaGNN%20for%20Link%20Sign%20Prediction&entry.906535625=Jinkyu%20Sung%20and%20Myunggeum%20Jee%20and%20Joonseok%20Lee&entry.1292438233=Link%20sign%20prediction%20on%20a%20signed%20graph%20is%20a%20task%20to%20determine%20whether%20the%20relationship%20represented%20by%20an%20edge%20is%20positive%20or%20negative.%20Since%20the%20presence%20of%20negative%20edges%20violates%20the%20graph%20homophily%20assumption%20that%20adjacent%20nodes%20are%20similar%2C%20regular%20graph%20methods%20have%20not%20been%20applicable%20without%20auxiliary%20structures%20to%20handle%20them.%20We%20aim%20to%20directly%20model%20the%20latent%20statistical%20dependency%20among%20edges%20with%20the%20Gaussian%20copula%20and%20its%20corresponding%20correlation%20matrix%2C%20extending%20CopulaGNN%20%28Ma%20et%20al.%2C%202021%29.%20However%2C%20a%20naive%20modeling%20of%20edge-edge%20relations%20is%20computationally%20intractable%20even%20for%20a%20graph%20with%20moderate%20scale.%20To%20address%20this%2C%20we%20propose%20to%201%29%20represent%20the%20correlation%20matrix%20as%20a%20Gramian%20of%20edge%20embeddings%2C%20significantly%20reducing%20the%20number%20of%20parameters%2C%20and%202%29%20reformulate%20the%20conditional%20probability%20distribution%20to%20dramatically%20reduce%20the%20inference%20cost.%20We%20theoretically%20verify%20scalability%20of%20our%20method%20by%20proving%20its%20linear%20convergence.%20Also%2C%20our%20extensive%20experiments%20demonstrate%20that%20it%20achieves%20significantly%20faster%20convergence%20than%20baselines%2C%20maintaining%20competitive%20prediction%20performance%20to%20the%20state-of-the-art%20models.&entry.1838667208=http%3A//arxiv.org/abs/2601.19175v2&entry.124074799=Read"},
{"title": "Reward Models Inherit Value Biases from Pretraining", "author": "Brian Christian and Jessica A. F. Thompson and Elle Michelle Yang and Vincent Adam and Hannah Rose Kirk and Christopher Summerfield and Tsvetomira Dumbalska", "abstract": "Reward models (RMs) are central to aligning large language models (LLMs) with human values but have received less attention than pre-trained and post-trained LLMs themselves. Because RMs are initialized from LLMs, they inherit representations that shape their behavior, but the nature and extent of this influence remain understudied. In a comprehensive study of 10 leading open-weight RMs using validated psycholinguistic corpora, we show that RMs exhibit significant differences along multiple dimensions of human value as a function of their base model. Using the \"Big Two\" psychological axes, we show a robust preference of Llama RMs for \"agency\" and a corresponding robust preference of Gemma RMs for \"communion.\" This phenomenon holds even when the preference data and finetuning process are identical, and we trace it back to the logits of the respective instruction-tuned and pre-trained models. These log-probability differences themselves can be formulated as an implicit RM; we derive usable implicit reward scores and show that they exhibit the very same agency/communion difference. We run experiments training RMs with ablations for preference data source and quantity, which demonstrate that this effect is not only repeatable but surprisingly durable. Despite RMs being designed to represent human preferences, our evidence shows that their outputs are influenced by the pretrained LLMs on which they are based. This work underscores the importance of safety and alignment efforts at the pretraining stage, and makes clear that open-source developers' choice of base model is as much a consideration of values as of performance.", "link": "http://arxiv.org/abs/2601.20838v1", "date": "2026-01-28", "relevancy": 2.283, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4593}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4593}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reward%20Models%20Inherit%20Value%20Biases%20from%20Pretraining&body=Title%3A%20Reward%20Models%20Inherit%20Value%20Biases%20from%20Pretraining%0AAuthor%3A%20Brian%20Christian%20and%20Jessica%20A.%20F.%20Thompson%20and%20Elle%20Michelle%20Yang%20and%20Vincent%20Adam%20and%20Hannah%20Rose%20Kirk%20and%20Christopher%20Summerfield%20and%20Tsvetomira%20Dumbalska%0AAbstract%3A%20Reward%20models%20%28RMs%29%20are%20central%20to%20aligning%20large%20language%20models%20%28LLMs%29%20with%20human%20values%20but%20have%20received%20less%20attention%20than%20pre-trained%20and%20post-trained%20LLMs%20themselves.%20Because%20RMs%20are%20initialized%20from%20LLMs%2C%20they%20inherit%20representations%20that%20shape%20their%20behavior%2C%20but%20the%20nature%20and%20extent%20of%20this%20influence%20remain%20understudied.%20In%20a%20comprehensive%20study%20of%2010%20leading%20open-weight%20RMs%20using%20validated%20psycholinguistic%20corpora%2C%20we%20show%20that%20RMs%20exhibit%20significant%20differences%20along%20multiple%20dimensions%20of%20human%20value%20as%20a%20function%20of%20their%20base%20model.%20Using%20the%20%22Big%20Two%22%20psychological%20axes%2C%20we%20show%20a%20robust%20preference%20of%20Llama%20RMs%20for%20%22agency%22%20and%20a%20corresponding%20robust%20preference%20of%20Gemma%20RMs%20for%20%22communion.%22%20This%20phenomenon%20holds%20even%20when%20the%20preference%20data%20and%20finetuning%20process%20are%20identical%2C%20and%20we%20trace%20it%20back%20to%20the%20logits%20of%20the%20respective%20instruction-tuned%20and%20pre-trained%20models.%20These%20log-probability%20differences%20themselves%20can%20be%20formulated%20as%20an%20implicit%20RM%3B%20we%20derive%20usable%20implicit%20reward%20scores%20and%20show%20that%20they%20exhibit%20the%20very%20same%20agency/communion%20difference.%20We%20run%20experiments%20training%20RMs%20with%20ablations%20for%20preference%20data%20source%20and%20quantity%2C%20which%20demonstrate%20that%20this%20effect%20is%20not%20only%20repeatable%20but%20surprisingly%20durable.%20Despite%20RMs%20being%20designed%20to%20represent%20human%20preferences%2C%20our%20evidence%20shows%20that%20their%20outputs%20are%20influenced%20by%20the%20pretrained%20LLMs%20on%20which%20they%20are%20based.%20This%20work%20underscores%20the%20importance%20of%20safety%20and%20alignment%20efforts%20at%20the%20pretraining%20stage%2C%20and%20makes%20clear%20that%20open-source%20developers%27%20choice%20of%20base%20model%20is%20as%20much%20a%20consideration%20of%20values%20as%20of%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20838v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReward%2520Models%2520Inherit%2520Value%2520Biases%2520from%2520Pretraining%26entry.906535625%3DBrian%2520Christian%2520and%2520Jessica%2520A.%2520F.%2520Thompson%2520and%2520Elle%2520Michelle%2520Yang%2520and%2520Vincent%2520Adam%2520and%2520Hannah%2520Rose%2520Kirk%2520and%2520Christopher%2520Summerfield%2520and%2520Tsvetomira%2520Dumbalska%26entry.1292438233%3DReward%2520models%2520%2528RMs%2529%2520are%2520central%2520to%2520aligning%2520large%2520language%2520models%2520%2528LLMs%2529%2520with%2520human%2520values%2520but%2520have%2520received%2520less%2520attention%2520than%2520pre-trained%2520and%2520post-trained%2520LLMs%2520themselves.%2520Because%2520RMs%2520are%2520initialized%2520from%2520LLMs%252C%2520they%2520inherit%2520representations%2520that%2520shape%2520their%2520behavior%252C%2520but%2520the%2520nature%2520and%2520extent%2520of%2520this%2520influence%2520remain%2520understudied.%2520In%2520a%2520comprehensive%2520study%2520of%252010%2520leading%2520open-weight%2520RMs%2520using%2520validated%2520psycholinguistic%2520corpora%252C%2520we%2520show%2520that%2520RMs%2520exhibit%2520significant%2520differences%2520along%2520multiple%2520dimensions%2520of%2520human%2520value%2520as%2520a%2520function%2520of%2520their%2520base%2520model.%2520Using%2520the%2520%2522Big%2520Two%2522%2520psychological%2520axes%252C%2520we%2520show%2520a%2520robust%2520preference%2520of%2520Llama%2520RMs%2520for%2520%2522agency%2522%2520and%2520a%2520corresponding%2520robust%2520preference%2520of%2520Gemma%2520RMs%2520for%2520%2522communion.%2522%2520This%2520phenomenon%2520holds%2520even%2520when%2520the%2520preference%2520data%2520and%2520finetuning%2520process%2520are%2520identical%252C%2520and%2520we%2520trace%2520it%2520back%2520to%2520the%2520logits%2520of%2520the%2520respective%2520instruction-tuned%2520and%2520pre-trained%2520models.%2520These%2520log-probability%2520differences%2520themselves%2520can%2520be%2520formulated%2520as%2520an%2520implicit%2520RM%253B%2520we%2520derive%2520usable%2520implicit%2520reward%2520scores%2520and%2520show%2520that%2520they%2520exhibit%2520the%2520very%2520same%2520agency/communion%2520difference.%2520We%2520run%2520experiments%2520training%2520RMs%2520with%2520ablations%2520for%2520preference%2520data%2520source%2520and%2520quantity%252C%2520which%2520demonstrate%2520that%2520this%2520effect%2520is%2520not%2520only%2520repeatable%2520but%2520surprisingly%2520durable.%2520Despite%2520RMs%2520being%2520designed%2520to%2520represent%2520human%2520preferences%252C%2520our%2520evidence%2520shows%2520that%2520their%2520outputs%2520are%2520influenced%2520by%2520the%2520pretrained%2520LLMs%2520on%2520which%2520they%2520are%2520based.%2520This%2520work%2520underscores%2520the%2520importance%2520of%2520safety%2520and%2520alignment%2520efforts%2520at%2520the%2520pretraining%2520stage%252C%2520and%2520makes%2520clear%2520that%2520open-source%2520developers%2527%2520choice%2520of%2520base%2520model%2520is%2520as%2520much%2520a%2520consideration%2520of%2520values%2520as%2520of%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20838v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reward%20Models%20Inherit%20Value%20Biases%20from%20Pretraining&entry.906535625=Brian%20Christian%20and%20Jessica%20A.%20F.%20Thompson%20and%20Elle%20Michelle%20Yang%20and%20Vincent%20Adam%20and%20Hannah%20Rose%20Kirk%20and%20Christopher%20Summerfield%20and%20Tsvetomira%20Dumbalska&entry.1292438233=Reward%20models%20%28RMs%29%20are%20central%20to%20aligning%20large%20language%20models%20%28LLMs%29%20with%20human%20values%20but%20have%20received%20less%20attention%20than%20pre-trained%20and%20post-trained%20LLMs%20themselves.%20Because%20RMs%20are%20initialized%20from%20LLMs%2C%20they%20inherit%20representations%20that%20shape%20their%20behavior%2C%20but%20the%20nature%20and%20extent%20of%20this%20influence%20remain%20understudied.%20In%20a%20comprehensive%20study%20of%2010%20leading%20open-weight%20RMs%20using%20validated%20psycholinguistic%20corpora%2C%20we%20show%20that%20RMs%20exhibit%20significant%20differences%20along%20multiple%20dimensions%20of%20human%20value%20as%20a%20function%20of%20their%20base%20model.%20Using%20the%20%22Big%20Two%22%20psychological%20axes%2C%20we%20show%20a%20robust%20preference%20of%20Llama%20RMs%20for%20%22agency%22%20and%20a%20corresponding%20robust%20preference%20of%20Gemma%20RMs%20for%20%22communion.%22%20This%20phenomenon%20holds%20even%20when%20the%20preference%20data%20and%20finetuning%20process%20are%20identical%2C%20and%20we%20trace%20it%20back%20to%20the%20logits%20of%20the%20respective%20instruction-tuned%20and%20pre-trained%20models.%20These%20log-probability%20differences%20themselves%20can%20be%20formulated%20as%20an%20implicit%20RM%3B%20we%20derive%20usable%20implicit%20reward%20scores%20and%20show%20that%20they%20exhibit%20the%20very%20same%20agency/communion%20difference.%20We%20run%20experiments%20training%20RMs%20with%20ablations%20for%20preference%20data%20source%20and%20quantity%2C%20which%20demonstrate%20that%20this%20effect%20is%20not%20only%20repeatable%20but%20surprisingly%20durable.%20Despite%20RMs%20being%20designed%20to%20represent%20human%20preferences%2C%20our%20evidence%20shows%20that%20their%20outputs%20are%20influenced%20by%20the%20pretrained%20LLMs%20on%20which%20they%20are%20based.%20This%20work%20underscores%20the%20importance%20of%20safety%20and%20alignment%20efforts%20at%20the%20pretraining%20stage%2C%20and%20makes%20clear%20that%20open-source%20developers%27%20choice%20of%20base%20model%20is%20as%20much%20a%20consideration%20of%20values%20as%20of%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2601.20838v1&entry.124074799=Read"},
{"title": "BlindSight: Harnessing Sparsity for Efficient Vision-Language Models", "author": "Tharun Adithya Srikrishnan and Deval Shah and Timothy Hein and Ahmed Hasssan and Stephen Youn and Steven K. Reinhardt", "abstract": "Large vision-language models (VLMs) enable joint processing of text and images. However, incorporating vision data significantly increases the prompt length, resulting in a longer time to first token (TTFT). This bottleneck can be alleviated by leveraging the inherent sparsity in the attention computation. Analyzing these attention patterns in VLMs when processing a series of images, we observe the absence of inter-image attention in a substantial portion of layers. Based on this, we propose BlindSight: an approach to optimize multi-image VLM inference using an input-template-aware attention sparsity mask with no runtime overhead. We utilize a dataset to derive a prompt-agnostic categorization for attention heads: Dense, Sink, Intra-Image, and Intra-Image+Sink. We develop a Triton-based GPU kernel to leverage this sparsity. BlindSight achieves a 1.8-3.2x speedup in the attention computation (prompt length 36K-300K). BlindSight generalizes across VLMs (Qwen2-VL, Qwen2.5-VL, Gemma 3), with only a 0.78% absolute accuracy degradation on average on multi-image comprehension benchmarks. Finally, we advocate for the design of efficient VLMs that combine BlindSight-inspired sparse and dense layers.", "link": "http://arxiv.org/abs/2507.09071v2", "date": "2026-01-28", "relevancy": 2.2814, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5779}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5746}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BlindSight%3A%20Harnessing%20Sparsity%20for%20Efficient%20Vision-Language%20Models&body=Title%3A%20BlindSight%3A%20Harnessing%20Sparsity%20for%20Efficient%20Vision-Language%20Models%0AAuthor%3A%20Tharun%20Adithya%20Srikrishnan%20and%20Deval%20Shah%20and%20Timothy%20Hein%20and%20Ahmed%20Hasssan%20and%20Stephen%20Youn%20and%20Steven%20K.%20Reinhardt%0AAbstract%3A%20Large%20vision-language%20models%20%28VLMs%29%20enable%20joint%20processing%20of%20text%20and%20images.%20However%2C%20incorporating%20vision%20data%20significantly%20increases%20the%20prompt%20length%2C%20resulting%20in%20a%20longer%20time%20to%20first%20token%20%28TTFT%29.%20This%20bottleneck%20can%20be%20alleviated%20by%20leveraging%20the%20inherent%20sparsity%20in%20the%20attention%20computation.%20Analyzing%20these%20attention%20patterns%20in%20VLMs%20when%20processing%20a%20series%20of%20images%2C%20we%20observe%20the%20absence%20of%20inter-image%20attention%20in%20a%20substantial%20portion%20of%20layers.%20Based%20on%20this%2C%20we%20propose%20BlindSight%3A%20an%20approach%20to%20optimize%20multi-image%20VLM%20inference%20using%20an%20input-template-aware%20attention%20sparsity%20mask%20with%20no%20runtime%20overhead.%20We%20utilize%20a%20dataset%20to%20derive%20a%20prompt-agnostic%20categorization%20for%20attention%20heads%3A%20Dense%2C%20Sink%2C%20Intra-Image%2C%20and%20Intra-Image%2BSink.%20We%20develop%20a%20Triton-based%20GPU%20kernel%20to%20leverage%20this%20sparsity.%20BlindSight%20achieves%20a%201.8-3.2x%20speedup%20in%20the%20attention%20computation%20%28prompt%20length%2036K-300K%29.%20BlindSight%20generalizes%20across%20VLMs%20%28Qwen2-VL%2C%20Qwen2.5-VL%2C%20Gemma%203%29%2C%20with%20only%20a%200.78%25%20absolute%20accuracy%20degradation%20on%20average%20on%20multi-image%20comprehension%20benchmarks.%20Finally%2C%20we%20advocate%20for%20the%20design%20of%20efficient%20VLMs%20that%20combine%20BlindSight-inspired%20sparse%20and%20dense%20layers.%0ALink%3A%20http%3A//arxiv.org/abs/2507.09071v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlindSight%253A%2520Harnessing%2520Sparsity%2520for%2520Efficient%2520Vision-Language%2520Models%26entry.906535625%3DTharun%2520Adithya%2520Srikrishnan%2520and%2520Deval%2520Shah%2520and%2520Timothy%2520Hein%2520and%2520Ahmed%2520Hasssan%2520and%2520Stephen%2520Youn%2520and%2520Steven%2520K.%2520Reinhardt%26entry.1292438233%3DLarge%2520vision-language%2520models%2520%2528VLMs%2529%2520enable%2520joint%2520processing%2520of%2520text%2520and%2520images.%2520However%252C%2520incorporating%2520vision%2520data%2520significantly%2520increases%2520the%2520prompt%2520length%252C%2520resulting%2520in%2520a%2520longer%2520time%2520to%2520first%2520token%2520%2528TTFT%2529.%2520This%2520bottleneck%2520can%2520be%2520alleviated%2520by%2520leveraging%2520the%2520inherent%2520sparsity%2520in%2520the%2520attention%2520computation.%2520Analyzing%2520these%2520attention%2520patterns%2520in%2520VLMs%2520when%2520processing%2520a%2520series%2520of%2520images%252C%2520we%2520observe%2520the%2520absence%2520of%2520inter-image%2520attention%2520in%2520a%2520substantial%2520portion%2520of%2520layers.%2520Based%2520on%2520this%252C%2520we%2520propose%2520BlindSight%253A%2520an%2520approach%2520to%2520optimize%2520multi-image%2520VLM%2520inference%2520using%2520an%2520input-template-aware%2520attention%2520sparsity%2520mask%2520with%2520no%2520runtime%2520overhead.%2520We%2520utilize%2520a%2520dataset%2520to%2520derive%2520a%2520prompt-agnostic%2520categorization%2520for%2520attention%2520heads%253A%2520Dense%252C%2520Sink%252C%2520Intra-Image%252C%2520and%2520Intra-Image%252BSink.%2520We%2520develop%2520a%2520Triton-based%2520GPU%2520kernel%2520to%2520leverage%2520this%2520sparsity.%2520BlindSight%2520achieves%2520a%25201.8-3.2x%2520speedup%2520in%2520the%2520attention%2520computation%2520%2528prompt%2520length%252036K-300K%2529.%2520BlindSight%2520generalizes%2520across%2520VLMs%2520%2528Qwen2-VL%252C%2520Qwen2.5-VL%252C%2520Gemma%25203%2529%252C%2520with%2520only%2520a%25200.78%2525%2520absolute%2520accuracy%2520degradation%2520on%2520average%2520on%2520multi-image%2520comprehension%2520benchmarks.%2520Finally%252C%2520we%2520advocate%2520for%2520the%2520design%2520of%2520efficient%2520VLMs%2520that%2520combine%2520BlindSight-inspired%2520sparse%2520and%2520dense%2520layers.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.09071v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BlindSight%3A%20Harnessing%20Sparsity%20for%20Efficient%20Vision-Language%20Models&entry.906535625=Tharun%20Adithya%20Srikrishnan%20and%20Deval%20Shah%20and%20Timothy%20Hein%20and%20Ahmed%20Hasssan%20and%20Stephen%20Youn%20and%20Steven%20K.%20Reinhardt&entry.1292438233=Large%20vision-language%20models%20%28VLMs%29%20enable%20joint%20processing%20of%20text%20and%20images.%20However%2C%20incorporating%20vision%20data%20significantly%20increases%20the%20prompt%20length%2C%20resulting%20in%20a%20longer%20time%20to%20first%20token%20%28TTFT%29.%20This%20bottleneck%20can%20be%20alleviated%20by%20leveraging%20the%20inherent%20sparsity%20in%20the%20attention%20computation.%20Analyzing%20these%20attention%20patterns%20in%20VLMs%20when%20processing%20a%20series%20of%20images%2C%20we%20observe%20the%20absence%20of%20inter-image%20attention%20in%20a%20substantial%20portion%20of%20layers.%20Based%20on%20this%2C%20we%20propose%20BlindSight%3A%20an%20approach%20to%20optimize%20multi-image%20VLM%20inference%20using%20an%20input-template-aware%20attention%20sparsity%20mask%20with%20no%20runtime%20overhead.%20We%20utilize%20a%20dataset%20to%20derive%20a%20prompt-agnostic%20categorization%20for%20attention%20heads%3A%20Dense%2C%20Sink%2C%20Intra-Image%2C%20and%20Intra-Image%2BSink.%20We%20develop%20a%20Triton-based%20GPU%20kernel%20to%20leverage%20this%20sparsity.%20BlindSight%20achieves%20a%201.8-3.2x%20speedup%20in%20the%20attention%20computation%20%28prompt%20length%2036K-300K%29.%20BlindSight%20generalizes%20across%20VLMs%20%28Qwen2-VL%2C%20Qwen2.5-VL%2C%20Gemma%203%29%2C%20with%20only%20a%200.78%25%20absolute%20accuracy%20degradation%20on%20average%20on%20multi-image%20comprehension%20benchmarks.%20Finally%2C%20we%20advocate%20for%20the%20design%20of%20efficient%20VLMs%20that%20combine%20BlindSight-inspired%20sparse%20and%20dense%20layers.&entry.1838667208=http%3A//arxiv.org/abs/2507.09071v2&entry.124074799=Read"},
{"title": "A New Dataset and Framework for Robust Road Surface Classification via Camera-IMU Fusion", "author": "Willams de Lima Costa and Thifany Ketuli Silva de Souza and Jonas Ferreira Silva and Carlos Gabriel Bezerra Pereira and Bruno Reis Vila Nova and Leonardo Silvino Brito and Rafael Raider Leoni and Juliano Silva and Valter Ferreira and Sibele Miguel Soares Neto and Samantha Uehara and Daniel Giacomo and Jo\u00e3o Marcelo Teixeira and Veronica Teichrieb and Cristiano Coelho de Ara\u00fajo", "abstract": "Road surface classification (RSC) is a key enabler for environment-aware predictive maintenance systems. However, existing RSC techniques often fail to generalize beyond narrow operational conditions due to limited sensing modalities and datasets that lack environmental diversity. This work addresses these limitations by introducing a multimodal framework that fuses images and inertial measurements using a lightweight bidirectional cross-attention module followed by an adaptive gating layer that adjusts modality contributions under domain shifts. Given the limitations of current benchmarks, especially regarding lack of variability, we introduce ROAD, a new dataset composed of three complementary subsets: (i) real-world multimodal recordings with RGB-IMU streams synchronized using a gold-standard industry datalogger, captured across diverse lighting, weather, and surface conditions; (ii) a large vision-only subset designed to assess robustness under adverse illumination and heterogeneous capture setups; and (iii) a synthetic subset generated to study out-of-distribution generalization in scenarios difficult to obtain in practice. Experiments show that our method achieves a +1.4 pp improvement over the previous state-of-the-art on the PVS benchmark and an +11.6 pp improvement on our multimodal ROAD subset, with consistently higher F1-scores on minority classes. The framework also demonstrates stable performance across challenging visual conditions, including nighttime, heavy rain, and mixed-surface transitions. These findings indicate that combining affordable camera and IMU sensors with multimodal attention mechanisms provides a scalable, robust foundation for road surface understanding, particularly relevant for regions where environmental variability and cost constraints limit the adoption of high-end sensing suites.", "link": "http://arxiv.org/abs/2601.20847v1", "date": "2026-01-28", "relevancy": 2.2729, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6044}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5761}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20New%20Dataset%20and%20Framework%20for%20Robust%20Road%20Surface%20Classification%20via%20Camera-IMU%20Fusion&body=Title%3A%20A%20New%20Dataset%20and%20Framework%20for%20Robust%20Road%20Surface%20Classification%20via%20Camera-IMU%20Fusion%0AAuthor%3A%20Willams%20de%20Lima%20Costa%20and%20Thifany%20Ketuli%20Silva%20de%20Souza%20and%20Jonas%20Ferreira%20Silva%20and%20Carlos%20Gabriel%20Bezerra%20Pereira%20and%20Bruno%20Reis%20Vila%20Nova%20and%20Leonardo%20Silvino%20Brito%20and%20Rafael%20Raider%20Leoni%20and%20Juliano%20Silva%20and%20Valter%20Ferreira%20and%20Sibele%20Miguel%20Soares%20Neto%20and%20Samantha%20Uehara%20and%20Daniel%20Giacomo%20and%20Jo%C3%A3o%20Marcelo%20Teixeira%20and%20Veronica%20Teichrieb%20and%20Cristiano%20Coelho%20de%20Ara%C3%BAjo%0AAbstract%3A%20Road%20surface%20classification%20%28RSC%29%20is%20a%20key%20enabler%20for%20environment-aware%20predictive%20maintenance%20systems.%20However%2C%20existing%20RSC%20techniques%20often%20fail%20to%20generalize%20beyond%20narrow%20operational%20conditions%20due%20to%20limited%20sensing%20modalities%20and%20datasets%20that%20lack%20environmental%20diversity.%20This%20work%20addresses%20these%20limitations%20by%20introducing%20a%20multimodal%20framework%20that%20fuses%20images%20and%20inertial%20measurements%20using%20a%20lightweight%20bidirectional%20cross-attention%20module%20followed%20by%20an%20adaptive%20gating%20layer%20that%20adjusts%20modality%20contributions%20under%20domain%20shifts.%20Given%20the%20limitations%20of%20current%20benchmarks%2C%20especially%20regarding%20lack%20of%20variability%2C%20we%20introduce%20ROAD%2C%20a%20new%20dataset%20composed%20of%20three%20complementary%20subsets%3A%20%28i%29%20real-world%20multimodal%20recordings%20with%20RGB-IMU%20streams%20synchronized%20using%20a%20gold-standard%20industry%20datalogger%2C%20captured%20across%20diverse%20lighting%2C%20weather%2C%20and%20surface%20conditions%3B%20%28ii%29%20a%20large%20vision-only%20subset%20designed%20to%20assess%20robustness%20under%20adverse%20illumination%20and%20heterogeneous%20capture%20setups%3B%20and%20%28iii%29%20a%20synthetic%20subset%20generated%20to%20study%20out-of-distribution%20generalization%20in%20scenarios%20difficult%20to%20obtain%20in%20practice.%20Experiments%20show%20that%20our%20method%20achieves%20a%20%2B1.4%20pp%20improvement%20over%20the%20previous%20state-of-the-art%20on%20the%20PVS%20benchmark%20and%20an%20%2B11.6%20pp%20improvement%20on%20our%20multimodal%20ROAD%20subset%2C%20with%20consistently%20higher%20F1-scores%20on%20minority%20classes.%20The%20framework%20also%20demonstrates%20stable%20performance%20across%20challenging%20visual%20conditions%2C%20including%20nighttime%2C%20heavy%20rain%2C%20and%20mixed-surface%20transitions.%20These%20findings%20indicate%20that%20combining%20affordable%20camera%20and%20IMU%20sensors%20with%20multimodal%20attention%20mechanisms%20provides%20a%20scalable%2C%20robust%20foundation%20for%20road%20surface%20understanding%2C%20particularly%20relevant%20for%20regions%20where%20environmental%20variability%20and%20cost%20constraints%20limit%20the%20adoption%20of%20high-end%20sensing%20suites.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20847v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520New%2520Dataset%2520and%2520Framework%2520for%2520Robust%2520Road%2520Surface%2520Classification%2520via%2520Camera-IMU%2520Fusion%26entry.906535625%3DWillams%2520de%2520Lima%2520Costa%2520and%2520Thifany%2520Ketuli%2520Silva%2520de%2520Souza%2520and%2520Jonas%2520Ferreira%2520Silva%2520and%2520Carlos%2520Gabriel%2520Bezerra%2520Pereira%2520and%2520Bruno%2520Reis%2520Vila%2520Nova%2520and%2520Leonardo%2520Silvino%2520Brito%2520and%2520Rafael%2520Raider%2520Leoni%2520and%2520Juliano%2520Silva%2520and%2520Valter%2520Ferreira%2520and%2520Sibele%2520Miguel%2520Soares%2520Neto%2520and%2520Samantha%2520Uehara%2520and%2520Daniel%2520Giacomo%2520and%2520Jo%25C3%25A3o%2520Marcelo%2520Teixeira%2520and%2520Veronica%2520Teichrieb%2520and%2520Cristiano%2520Coelho%2520de%2520Ara%25C3%25BAjo%26entry.1292438233%3DRoad%2520surface%2520classification%2520%2528RSC%2529%2520is%2520a%2520key%2520enabler%2520for%2520environment-aware%2520predictive%2520maintenance%2520systems.%2520However%252C%2520existing%2520RSC%2520techniques%2520often%2520fail%2520to%2520generalize%2520beyond%2520narrow%2520operational%2520conditions%2520due%2520to%2520limited%2520sensing%2520modalities%2520and%2520datasets%2520that%2520lack%2520environmental%2520diversity.%2520This%2520work%2520addresses%2520these%2520limitations%2520by%2520introducing%2520a%2520multimodal%2520framework%2520that%2520fuses%2520images%2520and%2520inertial%2520measurements%2520using%2520a%2520lightweight%2520bidirectional%2520cross-attention%2520module%2520followed%2520by%2520an%2520adaptive%2520gating%2520layer%2520that%2520adjusts%2520modality%2520contributions%2520under%2520domain%2520shifts.%2520Given%2520the%2520limitations%2520of%2520current%2520benchmarks%252C%2520especially%2520regarding%2520lack%2520of%2520variability%252C%2520we%2520introduce%2520ROAD%252C%2520a%2520new%2520dataset%2520composed%2520of%2520three%2520complementary%2520subsets%253A%2520%2528i%2529%2520real-world%2520multimodal%2520recordings%2520with%2520RGB-IMU%2520streams%2520synchronized%2520using%2520a%2520gold-standard%2520industry%2520datalogger%252C%2520captured%2520across%2520diverse%2520lighting%252C%2520weather%252C%2520and%2520surface%2520conditions%253B%2520%2528ii%2529%2520a%2520large%2520vision-only%2520subset%2520designed%2520to%2520assess%2520robustness%2520under%2520adverse%2520illumination%2520and%2520heterogeneous%2520capture%2520setups%253B%2520and%2520%2528iii%2529%2520a%2520synthetic%2520subset%2520generated%2520to%2520study%2520out-of-distribution%2520generalization%2520in%2520scenarios%2520difficult%2520to%2520obtain%2520in%2520practice.%2520Experiments%2520show%2520that%2520our%2520method%2520achieves%2520a%2520%252B1.4%2520pp%2520improvement%2520over%2520the%2520previous%2520state-of-the-art%2520on%2520the%2520PVS%2520benchmark%2520and%2520an%2520%252B11.6%2520pp%2520improvement%2520on%2520our%2520multimodal%2520ROAD%2520subset%252C%2520with%2520consistently%2520higher%2520F1-scores%2520on%2520minority%2520classes.%2520The%2520framework%2520also%2520demonstrates%2520stable%2520performance%2520across%2520challenging%2520visual%2520conditions%252C%2520including%2520nighttime%252C%2520heavy%2520rain%252C%2520and%2520mixed-surface%2520transitions.%2520These%2520findings%2520indicate%2520that%2520combining%2520affordable%2520camera%2520and%2520IMU%2520sensors%2520with%2520multimodal%2520attention%2520mechanisms%2520provides%2520a%2520scalable%252C%2520robust%2520foundation%2520for%2520road%2520surface%2520understanding%252C%2520particularly%2520relevant%2520for%2520regions%2520where%2520environmental%2520variability%2520and%2520cost%2520constraints%2520limit%2520the%2520adoption%2520of%2520high-end%2520sensing%2520suites.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20847v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20New%20Dataset%20and%20Framework%20for%20Robust%20Road%20Surface%20Classification%20via%20Camera-IMU%20Fusion&entry.906535625=Willams%20de%20Lima%20Costa%20and%20Thifany%20Ketuli%20Silva%20de%20Souza%20and%20Jonas%20Ferreira%20Silva%20and%20Carlos%20Gabriel%20Bezerra%20Pereira%20and%20Bruno%20Reis%20Vila%20Nova%20and%20Leonardo%20Silvino%20Brito%20and%20Rafael%20Raider%20Leoni%20and%20Juliano%20Silva%20and%20Valter%20Ferreira%20and%20Sibele%20Miguel%20Soares%20Neto%20and%20Samantha%20Uehara%20and%20Daniel%20Giacomo%20and%20Jo%C3%A3o%20Marcelo%20Teixeira%20and%20Veronica%20Teichrieb%20and%20Cristiano%20Coelho%20de%20Ara%C3%BAjo&entry.1292438233=Road%20surface%20classification%20%28RSC%29%20is%20a%20key%20enabler%20for%20environment-aware%20predictive%20maintenance%20systems.%20However%2C%20existing%20RSC%20techniques%20often%20fail%20to%20generalize%20beyond%20narrow%20operational%20conditions%20due%20to%20limited%20sensing%20modalities%20and%20datasets%20that%20lack%20environmental%20diversity.%20This%20work%20addresses%20these%20limitations%20by%20introducing%20a%20multimodal%20framework%20that%20fuses%20images%20and%20inertial%20measurements%20using%20a%20lightweight%20bidirectional%20cross-attention%20module%20followed%20by%20an%20adaptive%20gating%20layer%20that%20adjusts%20modality%20contributions%20under%20domain%20shifts.%20Given%20the%20limitations%20of%20current%20benchmarks%2C%20especially%20regarding%20lack%20of%20variability%2C%20we%20introduce%20ROAD%2C%20a%20new%20dataset%20composed%20of%20three%20complementary%20subsets%3A%20%28i%29%20real-world%20multimodal%20recordings%20with%20RGB-IMU%20streams%20synchronized%20using%20a%20gold-standard%20industry%20datalogger%2C%20captured%20across%20diverse%20lighting%2C%20weather%2C%20and%20surface%20conditions%3B%20%28ii%29%20a%20large%20vision-only%20subset%20designed%20to%20assess%20robustness%20under%20adverse%20illumination%20and%20heterogeneous%20capture%20setups%3B%20and%20%28iii%29%20a%20synthetic%20subset%20generated%20to%20study%20out-of-distribution%20generalization%20in%20scenarios%20difficult%20to%20obtain%20in%20practice.%20Experiments%20show%20that%20our%20method%20achieves%20a%20%2B1.4%20pp%20improvement%20over%20the%20previous%20state-of-the-art%20on%20the%20PVS%20benchmark%20and%20an%20%2B11.6%20pp%20improvement%20on%20our%20multimodal%20ROAD%20subset%2C%20with%20consistently%20higher%20F1-scores%20on%20minority%20classes.%20The%20framework%20also%20demonstrates%20stable%20performance%20across%20challenging%20visual%20conditions%2C%20including%20nighttime%2C%20heavy%20rain%2C%20and%20mixed-surface%20transitions.%20These%20findings%20indicate%20that%20combining%20affordable%20camera%20and%20IMU%20sensors%20with%20multimodal%20attention%20mechanisms%20provides%20a%20scalable%2C%20robust%20foundation%20for%20road%20surface%20understanding%2C%20particularly%20relevant%20for%20regions%20where%20environmental%20variability%20and%20cost%20constraints%20limit%20the%20adoption%20of%20high-end%20sensing%20suites.&entry.1838667208=http%3A//arxiv.org/abs/2601.20847v1&entry.124074799=Read"},
{"title": "AEDR: Training-Free AI-Generated Image Attribution via Autoencoder Double-Reconstruction", "author": "Chao Wang and Zijin Yang and Yaofei Wang and Weiming Zhang and Kejiang Chen", "abstract": "The rapid advancement of image-generation technologies has made it possible for anyone to create photorealistic images using generative models, raising significant security concerns. To mitigate malicious use, tracing the origin of such images is essential. Reconstruction-based attribution methods offer a promising solution, but they often suffer from reduced accuracy and high computational costs when applied to state-of-the-art (SOTA) models. To address these challenges, we propose AEDR (AutoEncoder Double-Reconstruction), a novel training-free attribution method designed for generative models with continuous autoencoders. Unlike existing reconstruction-based approaches that rely on the value of a single reconstruction loss, AEDR performs two consecutive reconstructions using the model's autoencoder, and adopts the ratio of these two reconstruction losses as the attribution signal. This signal is further calibrated using the image homogeneity metric to improve accuracy, which inherently cancels out absolute biases caused by image complexity, with autoencoder-based reconstruction ensuring superior computational efficiency. Experiments on eight top latent diffusion models show that AEDR achieves 25.5% higher attribution accuracy than existing reconstruction-based methods, while requiring only 1% of the computational time.", "link": "http://arxiv.org/abs/2507.18988v2", "date": "2026-01-28", "relevancy": 2.2493, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5791}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5532}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AEDR%3A%20Training-Free%20AI-Generated%20Image%20Attribution%20via%20Autoencoder%20Double-Reconstruction&body=Title%3A%20AEDR%3A%20Training-Free%20AI-Generated%20Image%20Attribution%20via%20Autoencoder%20Double-Reconstruction%0AAuthor%3A%20Chao%20Wang%20and%20Zijin%20Yang%20and%20Yaofei%20Wang%20and%20Weiming%20Zhang%20and%20Kejiang%20Chen%0AAbstract%3A%20The%20rapid%20advancement%20of%20image-generation%20technologies%20has%20made%20it%20possible%20for%20anyone%20to%20create%20photorealistic%20images%20using%20generative%20models%2C%20raising%20significant%20security%20concerns.%20To%20mitigate%20malicious%20use%2C%20tracing%20the%20origin%20of%20such%20images%20is%20essential.%20Reconstruction-based%20attribution%20methods%20offer%20a%20promising%20solution%2C%20but%20they%20often%20suffer%20from%20reduced%20accuracy%20and%20high%20computational%20costs%20when%20applied%20to%20state-of-the-art%20%28SOTA%29%20models.%20To%20address%20these%20challenges%2C%20we%20propose%20AEDR%20%28AutoEncoder%20Double-Reconstruction%29%2C%20a%20novel%20training-free%20attribution%20method%20designed%20for%20generative%20models%20with%20continuous%20autoencoders.%20Unlike%20existing%20reconstruction-based%20approaches%20that%20rely%20on%20the%20value%20of%20a%20single%20reconstruction%20loss%2C%20AEDR%20performs%20two%20consecutive%20reconstructions%20using%20the%20model%27s%20autoencoder%2C%20and%20adopts%20the%20ratio%20of%20these%20two%20reconstruction%20losses%20as%20the%20attribution%20signal.%20This%20signal%20is%20further%20calibrated%20using%20the%20image%20homogeneity%20metric%20to%20improve%20accuracy%2C%20which%20inherently%20cancels%20out%20absolute%20biases%20caused%20by%20image%20complexity%2C%20with%20autoencoder-based%20reconstruction%20ensuring%20superior%20computational%20efficiency.%20Experiments%20on%20eight%20top%20latent%20diffusion%20models%20show%20that%20AEDR%20achieves%2025.5%25%20higher%20attribution%20accuracy%20than%20existing%20reconstruction-based%20methods%2C%20while%20requiring%20only%201%25%20of%20the%20computational%20time.%0ALink%3A%20http%3A//arxiv.org/abs/2507.18988v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAEDR%253A%2520Training-Free%2520AI-Generated%2520Image%2520Attribution%2520via%2520Autoencoder%2520Double-Reconstruction%26entry.906535625%3DChao%2520Wang%2520and%2520Zijin%2520Yang%2520and%2520Yaofei%2520Wang%2520and%2520Weiming%2520Zhang%2520and%2520Kejiang%2520Chen%26entry.1292438233%3DThe%2520rapid%2520advancement%2520of%2520image-generation%2520technologies%2520has%2520made%2520it%2520possible%2520for%2520anyone%2520to%2520create%2520photorealistic%2520images%2520using%2520generative%2520models%252C%2520raising%2520significant%2520security%2520concerns.%2520To%2520mitigate%2520malicious%2520use%252C%2520tracing%2520the%2520origin%2520of%2520such%2520images%2520is%2520essential.%2520Reconstruction-based%2520attribution%2520methods%2520offer%2520a%2520promising%2520solution%252C%2520but%2520they%2520often%2520suffer%2520from%2520reduced%2520accuracy%2520and%2520high%2520computational%2520costs%2520when%2520applied%2520to%2520state-of-the-art%2520%2528SOTA%2529%2520models.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520AEDR%2520%2528AutoEncoder%2520Double-Reconstruction%2529%252C%2520a%2520novel%2520training-free%2520attribution%2520method%2520designed%2520for%2520generative%2520models%2520with%2520continuous%2520autoencoders.%2520Unlike%2520existing%2520reconstruction-based%2520approaches%2520that%2520rely%2520on%2520the%2520value%2520of%2520a%2520single%2520reconstruction%2520loss%252C%2520AEDR%2520performs%2520two%2520consecutive%2520reconstructions%2520using%2520the%2520model%2527s%2520autoencoder%252C%2520and%2520adopts%2520the%2520ratio%2520of%2520these%2520two%2520reconstruction%2520losses%2520as%2520the%2520attribution%2520signal.%2520This%2520signal%2520is%2520further%2520calibrated%2520using%2520the%2520image%2520homogeneity%2520metric%2520to%2520improve%2520accuracy%252C%2520which%2520inherently%2520cancels%2520out%2520absolute%2520biases%2520caused%2520by%2520image%2520complexity%252C%2520with%2520autoencoder-based%2520reconstruction%2520ensuring%2520superior%2520computational%2520efficiency.%2520Experiments%2520on%2520eight%2520top%2520latent%2520diffusion%2520models%2520show%2520that%2520AEDR%2520achieves%252025.5%2525%2520higher%2520attribution%2520accuracy%2520than%2520existing%2520reconstruction-based%2520methods%252C%2520while%2520requiring%2520only%25201%2525%2520of%2520the%2520computational%2520time.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.18988v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AEDR%3A%20Training-Free%20AI-Generated%20Image%20Attribution%20via%20Autoencoder%20Double-Reconstruction&entry.906535625=Chao%20Wang%20and%20Zijin%20Yang%20and%20Yaofei%20Wang%20and%20Weiming%20Zhang%20and%20Kejiang%20Chen&entry.1292438233=The%20rapid%20advancement%20of%20image-generation%20technologies%20has%20made%20it%20possible%20for%20anyone%20to%20create%20photorealistic%20images%20using%20generative%20models%2C%20raising%20significant%20security%20concerns.%20To%20mitigate%20malicious%20use%2C%20tracing%20the%20origin%20of%20such%20images%20is%20essential.%20Reconstruction-based%20attribution%20methods%20offer%20a%20promising%20solution%2C%20but%20they%20often%20suffer%20from%20reduced%20accuracy%20and%20high%20computational%20costs%20when%20applied%20to%20state-of-the-art%20%28SOTA%29%20models.%20To%20address%20these%20challenges%2C%20we%20propose%20AEDR%20%28AutoEncoder%20Double-Reconstruction%29%2C%20a%20novel%20training-free%20attribution%20method%20designed%20for%20generative%20models%20with%20continuous%20autoencoders.%20Unlike%20existing%20reconstruction-based%20approaches%20that%20rely%20on%20the%20value%20of%20a%20single%20reconstruction%20loss%2C%20AEDR%20performs%20two%20consecutive%20reconstructions%20using%20the%20model%27s%20autoencoder%2C%20and%20adopts%20the%20ratio%20of%20these%20two%20reconstruction%20losses%20as%20the%20attribution%20signal.%20This%20signal%20is%20further%20calibrated%20using%20the%20image%20homogeneity%20metric%20to%20improve%20accuracy%2C%20which%20inherently%20cancels%20out%20absolute%20biases%20caused%20by%20image%20complexity%2C%20with%20autoencoder-based%20reconstruction%20ensuring%20superior%20computational%20efficiency.%20Experiments%20on%20eight%20top%20latent%20diffusion%20models%20show%20that%20AEDR%20achieves%2025.5%25%20higher%20attribution%20accuracy%20than%20existing%20reconstruction-based%20methods%2C%20while%20requiring%20only%201%25%20of%20the%20computational%20time.&entry.1838667208=http%3A//arxiv.org/abs/2507.18988v2&entry.124074799=Read"},
{"title": "Evolutionary Strategies lead to Catastrophic Forgetting in LLMs", "author": "Immanuel Abdi and Akshat Gupta and Micah Mok and Alexander Lu and Nicholas Lee and Gopala Anumanchipalli", "abstract": "One of the biggest missing capabilities in current AI systems is the ability to learn continuously after deployment. Implementing such continually learning systems have several challenges, one of which is the large memory requirement of gradient-based algorithms that are used to train state-of-the-art LLMs. Evolutionary Strategies (ES) have recently re-emerged as a gradient-free alternative to traditional learning algorithms and have shown encouraging performance on specific tasks in LLMs. In this paper, we perform a comprehensive analysis of ES and specifically evaluate its forgetting curves when training for an increasing number of update steps. We first find that ES is able to reach performance numbers close to GRPO for math and reasoning tasks with a comparable compute budget. However, and most importantly for continual learning, the performance gains in ES is accompanied by significant forgetting of prior abilities, limiting its applicability for training models online. We also explore the reason behind this behavior and show that the updates made using ES are much less sparse and have orders of magnitude larger $\\ell_2$ norm compared to corresponding GRPO updates, explaining the contrasting forgetting curves between the two algorithms. With this study, we aim to highlight the issue of forgetting in gradient-free algorithms like ES and hope to inspire future work to mitigate these issues.", "link": "http://arxiv.org/abs/2601.20861v1", "date": "2026-01-28", "relevancy": 2.2349, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4476}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4475}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evolutionary%20Strategies%20lead%20to%20Catastrophic%20Forgetting%20in%20LLMs&body=Title%3A%20Evolutionary%20Strategies%20lead%20to%20Catastrophic%20Forgetting%20in%20LLMs%0AAuthor%3A%20Immanuel%20Abdi%20and%20Akshat%20Gupta%20and%20Micah%20Mok%20and%20Alexander%20Lu%20and%20Nicholas%20Lee%20and%20Gopala%20Anumanchipalli%0AAbstract%3A%20One%20of%20the%20biggest%20missing%20capabilities%20in%20current%20AI%20systems%20is%20the%20ability%20to%20learn%20continuously%20after%20deployment.%20Implementing%20such%20continually%20learning%20systems%20have%20several%20challenges%2C%20one%20of%20which%20is%20the%20large%20memory%20requirement%20of%20gradient-based%20algorithms%20that%20are%20used%20to%20train%20state-of-the-art%20LLMs.%20Evolutionary%20Strategies%20%28ES%29%20have%20recently%20re-emerged%20as%20a%20gradient-free%20alternative%20to%20traditional%20learning%20algorithms%20and%20have%20shown%20encouraging%20performance%20on%20specific%20tasks%20in%20LLMs.%20In%20this%20paper%2C%20we%20perform%20a%20comprehensive%20analysis%20of%20ES%20and%20specifically%20evaluate%20its%20forgetting%20curves%20when%20training%20for%20an%20increasing%20number%20of%20update%20steps.%20We%20first%20find%20that%20ES%20is%20able%20to%20reach%20performance%20numbers%20close%20to%20GRPO%20for%20math%20and%20reasoning%20tasks%20with%20a%20comparable%20compute%20budget.%20However%2C%20and%20most%20importantly%20for%20continual%20learning%2C%20the%20performance%20gains%20in%20ES%20is%20accompanied%20by%20significant%20forgetting%20of%20prior%20abilities%2C%20limiting%20its%20applicability%20for%20training%20models%20online.%20We%20also%20explore%20the%20reason%20behind%20this%20behavior%20and%20show%20that%20the%20updates%20made%20using%20ES%20are%20much%20less%20sparse%20and%20have%20orders%20of%20magnitude%20larger%20%24%5Cell_2%24%20norm%20compared%20to%20corresponding%20GRPO%20updates%2C%20explaining%20the%20contrasting%20forgetting%20curves%20between%20the%20two%20algorithms.%20With%20this%20study%2C%20we%20aim%20to%20highlight%20the%20issue%20of%20forgetting%20in%20gradient-free%20algorithms%20like%20ES%20and%20hope%20to%20inspire%20future%20work%20to%20mitigate%20these%20issues.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20861v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvolutionary%2520Strategies%2520lead%2520to%2520Catastrophic%2520Forgetting%2520in%2520LLMs%26entry.906535625%3DImmanuel%2520Abdi%2520and%2520Akshat%2520Gupta%2520and%2520Micah%2520Mok%2520and%2520Alexander%2520Lu%2520and%2520Nicholas%2520Lee%2520and%2520Gopala%2520Anumanchipalli%26entry.1292438233%3DOne%2520of%2520the%2520biggest%2520missing%2520capabilities%2520in%2520current%2520AI%2520systems%2520is%2520the%2520ability%2520to%2520learn%2520continuously%2520after%2520deployment.%2520Implementing%2520such%2520continually%2520learning%2520systems%2520have%2520several%2520challenges%252C%2520one%2520of%2520which%2520is%2520the%2520large%2520memory%2520requirement%2520of%2520gradient-based%2520algorithms%2520that%2520are%2520used%2520to%2520train%2520state-of-the-art%2520LLMs.%2520Evolutionary%2520Strategies%2520%2528ES%2529%2520have%2520recently%2520re-emerged%2520as%2520a%2520gradient-free%2520alternative%2520to%2520traditional%2520learning%2520algorithms%2520and%2520have%2520shown%2520encouraging%2520performance%2520on%2520specific%2520tasks%2520in%2520LLMs.%2520In%2520this%2520paper%252C%2520we%2520perform%2520a%2520comprehensive%2520analysis%2520of%2520ES%2520and%2520specifically%2520evaluate%2520its%2520forgetting%2520curves%2520when%2520training%2520for%2520an%2520increasing%2520number%2520of%2520update%2520steps.%2520We%2520first%2520find%2520that%2520ES%2520is%2520able%2520to%2520reach%2520performance%2520numbers%2520close%2520to%2520GRPO%2520for%2520math%2520and%2520reasoning%2520tasks%2520with%2520a%2520comparable%2520compute%2520budget.%2520However%252C%2520and%2520most%2520importantly%2520for%2520continual%2520learning%252C%2520the%2520performance%2520gains%2520in%2520ES%2520is%2520accompanied%2520by%2520significant%2520forgetting%2520of%2520prior%2520abilities%252C%2520limiting%2520its%2520applicability%2520for%2520training%2520models%2520online.%2520We%2520also%2520explore%2520the%2520reason%2520behind%2520this%2520behavior%2520and%2520show%2520that%2520the%2520updates%2520made%2520using%2520ES%2520are%2520much%2520less%2520sparse%2520and%2520have%2520orders%2520of%2520magnitude%2520larger%2520%2524%255Cell_2%2524%2520norm%2520compared%2520to%2520corresponding%2520GRPO%2520updates%252C%2520explaining%2520the%2520contrasting%2520forgetting%2520curves%2520between%2520the%2520two%2520algorithms.%2520With%2520this%2520study%252C%2520we%2520aim%2520to%2520highlight%2520the%2520issue%2520of%2520forgetting%2520in%2520gradient-free%2520algorithms%2520like%2520ES%2520and%2520hope%2520to%2520inspire%2520future%2520work%2520to%2520mitigate%2520these%2520issues.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20861v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evolutionary%20Strategies%20lead%20to%20Catastrophic%20Forgetting%20in%20LLMs&entry.906535625=Immanuel%20Abdi%20and%20Akshat%20Gupta%20and%20Micah%20Mok%20and%20Alexander%20Lu%20and%20Nicholas%20Lee%20and%20Gopala%20Anumanchipalli&entry.1292438233=One%20of%20the%20biggest%20missing%20capabilities%20in%20current%20AI%20systems%20is%20the%20ability%20to%20learn%20continuously%20after%20deployment.%20Implementing%20such%20continually%20learning%20systems%20have%20several%20challenges%2C%20one%20of%20which%20is%20the%20large%20memory%20requirement%20of%20gradient-based%20algorithms%20that%20are%20used%20to%20train%20state-of-the-art%20LLMs.%20Evolutionary%20Strategies%20%28ES%29%20have%20recently%20re-emerged%20as%20a%20gradient-free%20alternative%20to%20traditional%20learning%20algorithms%20and%20have%20shown%20encouraging%20performance%20on%20specific%20tasks%20in%20LLMs.%20In%20this%20paper%2C%20we%20perform%20a%20comprehensive%20analysis%20of%20ES%20and%20specifically%20evaluate%20its%20forgetting%20curves%20when%20training%20for%20an%20increasing%20number%20of%20update%20steps.%20We%20first%20find%20that%20ES%20is%20able%20to%20reach%20performance%20numbers%20close%20to%20GRPO%20for%20math%20and%20reasoning%20tasks%20with%20a%20comparable%20compute%20budget.%20However%2C%20and%20most%20importantly%20for%20continual%20learning%2C%20the%20performance%20gains%20in%20ES%20is%20accompanied%20by%20significant%20forgetting%20of%20prior%20abilities%2C%20limiting%20its%20applicability%20for%20training%20models%20online.%20We%20also%20explore%20the%20reason%20behind%20this%20behavior%20and%20show%20that%20the%20updates%20made%20using%20ES%20are%20much%20less%20sparse%20and%20have%20orders%20of%20magnitude%20larger%20%24%5Cell_2%24%20norm%20compared%20to%20corresponding%20GRPO%20updates%2C%20explaining%20the%20contrasting%20forgetting%20curves%20between%20the%20two%20algorithms.%20With%20this%20study%2C%20we%20aim%20to%20highlight%20the%20issue%20of%20forgetting%20in%20gradient-free%20algorithms%20like%20ES%20and%20hope%20to%20inspire%20future%20work%20to%20mitigate%20these%20issues.&entry.1838667208=http%3A//arxiv.org/abs/2601.20861v1&entry.124074799=Read"},
{"title": "Dissecting Multimodal In-Context Learning: Modality Asymmetries and Circuit Dynamics in modern Transformers", "author": "Yiran Huang and Karsten Roth and Quentin Bouniot and Wenjia Xu and Zeynep Akata", "abstract": "Transformer-based multimodal large language models often exhibit in-context learning (ICL) abilities. Motivated by this phenomenon, we ask: how do transformers learn to associate information across modalities from in-context examples? We investigate this question through controlled experiments on small transformers trained on synthetic classification tasks, enabling precise manipulation of data statistics and model architecture. We begin by revisiting core principles of unimodal ICL in modern transformers. While several prior findings replicate, we find that Rotary Position Embeddings (RoPE) increases the data complexity threshold for ICL. Extending to the multimodal setting reveals a fundamental learning asymmetry: when pretrained on high-diversity data from a primary modality, surprisingly low data complexity in the secondary modality suffices for multimodal ICL to emerge. Mechanistic analysis shows that both settings rely on an induction-style mechanism that copies labels from matching in-context exemplars; multimodal training refines and extends these circuits across modalities. Our findings provide a mechanistic foundation for understanding multimodal ICL in modern transformers and introduce a controlled testbed for future investigation.", "link": "http://arxiv.org/abs/2601.20796v1", "date": "2026-01-28", "relevancy": 2.2272, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5931}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5397}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dissecting%20Multimodal%20In-Context%20Learning%3A%20Modality%20Asymmetries%20and%20Circuit%20Dynamics%20in%20modern%20Transformers&body=Title%3A%20Dissecting%20Multimodal%20In-Context%20Learning%3A%20Modality%20Asymmetries%20and%20Circuit%20Dynamics%20in%20modern%20Transformers%0AAuthor%3A%20Yiran%20Huang%20and%20Karsten%20Roth%20and%20Quentin%20Bouniot%20and%20Wenjia%20Xu%20and%20Zeynep%20Akata%0AAbstract%3A%20Transformer-based%20multimodal%20large%20language%20models%20often%20exhibit%20in-context%20learning%20%28ICL%29%20abilities.%20Motivated%20by%20this%20phenomenon%2C%20we%20ask%3A%20how%20do%20transformers%20learn%20to%20associate%20information%20across%20modalities%20from%20in-context%20examples%3F%20We%20investigate%20this%20question%20through%20controlled%20experiments%20on%20small%20transformers%20trained%20on%20synthetic%20classification%20tasks%2C%20enabling%20precise%20manipulation%20of%20data%20statistics%20and%20model%20architecture.%20We%20begin%20by%20revisiting%20core%20principles%20of%20unimodal%20ICL%20in%20modern%20transformers.%20While%20several%20prior%20findings%20replicate%2C%20we%20find%20that%20Rotary%20Position%20Embeddings%20%28RoPE%29%20increases%20the%20data%20complexity%20threshold%20for%20ICL.%20Extending%20to%20the%20multimodal%20setting%20reveals%20a%20fundamental%20learning%20asymmetry%3A%20when%20pretrained%20on%20high-diversity%20data%20from%20a%20primary%20modality%2C%20surprisingly%20low%20data%20complexity%20in%20the%20secondary%20modality%20suffices%20for%20multimodal%20ICL%20to%20emerge.%20Mechanistic%20analysis%20shows%20that%20both%20settings%20rely%20on%20an%20induction-style%20mechanism%20that%20copies%20labels%20from%20matching%20in-context%20exemplars%3B%20multimodal%20training%20refines%20and%20extends%20these%20circuits%20across%20modalities.%20Our%20findings%20provide%20a%20mechanistic%20foundation%20for%20understanding%20multimodal%20ICL%20in%20modern%20transformers%20and%20introduce%20a%20controlled%20testbed%20for%20future%20investigation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20796v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDissecting%2520Multimodal%2520In-Context%2520Learning%253A%2520Modality%2520Asymmetries%2520and%2520Circuit%2520Dynamics%2520in%2520modern%2520Transformers%26entry.906535625%3DYiran%2520Huang%2520and%2520Karsten%2520Roth%2520and%2520Quentin%2520Bouniot%2520and%2520Wenjia%2520Xu%2520and%2520Zeynep%2520Akata%26entry.1292438233%3DTransformer-based%2520multimodal%2520large%2520language%2520models%2520often%2520exhibit%2520in-context%2520learning%2520%2528ICL%2529%2520abilities.%2520Motivated%2520by%2520this%2520phenomenon%252C%2520we%2520ask%253A%2520how%2520do%2520transformers%2520learn%2520to%2520associate%2520information%2520across%2520modalities%2520from%2520in-context%2520examples%253F%2520We%2520investigate%2520this%2520question%2520through%2520controlled%2520experiments%2520on%2520small%2520transformers%2520trained%2520on%2520synthetic%2520classification%2520tasks%252C%2520enabling%2520precise%2520manipulation%2520of%2520data%2520statistics%2520and%2520model%2520architecture.%2520We%2520begin%2520by%2520revisiting%2520core%2520principles%2520of%2520unimodal%2520ICL%2520in%2520modern%2520transformers.%2520While%2520several%2520prior%2520findings%2520replicate%252C%2520we%2520find%2520that%2520Rotary%2520Position%2520Embeddings%2520%2528RoPE%2529%2520increases%2520the%2520data%2520complexity%2520threshold%2520for%2520ICL.%2520Extending%2520to%2520the%2520multimodal%2520setting%2520reveals%2520a%2520fundamental%2520learning%2520asymmetry%253A%2520when%2520pretrained%2520on%2520high-diversity%2520data%2520from%2520a%2520primary%2520modality%252C%2520surprisingly%2520low%2520data%2520complexity%2520in%2520the%2520secondary%2520modality%2520suffices%2520for%2520multimodal%2520ICL%2520to%2520emerge.%2520Mechanistic%2520analysis%2520shows%2520that%2520both%2520settings%2520rely%2520on%2520an%2520induction-style%2520mechanism%2520that%2520copies%2520labels%2520from%2520matching%2520in-context%2520exemplars%253B%2520multimodal%2520training%2520refines%2520and%2520extends%2520these%2520circuits%2520across%2520modalities.%2520Our%2520findings%2520provide%2520a%2520mechanistic%2520foundation%2520for%2520understanding%2520multimodal%2520ICL%2520in%2520modern%2520transformers%2520and%2520introduce%2520a%2520controlled%2520testbed%2520for%2520future%2520investigation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20796v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dissecting%20Multimodal%20In-Context%20Learning%3A%20Modality%20Asymmetries%20and%20Circuit%20Dynamics%20in%20modern%20Transformers&entry.906535625=Yiran%20Huang%20and%20Karsten%20Roth%20and%20Quentin%20Bouniot%20and%20Wenjia%20Xu%20and%20Zeynep%20Akata&entry.1292438233=Transformer-based%20multimodal%20large%20language%20models%20often%20exhibit%20in-context%20learning%20%28ICL%29%20abilities.%20Motivated%20by%20this%20phenomenon%2C%20we%20ask%3A%20how%20do%20transformers%20learn%20to%20associate%20information%20across%20modalities%20from%20in-context%20examples%3F%20We%20investigate%20this%20question%20through%20controlled%20experiments%20on%20small%20transformers%20trained%20on%20synthetic%20classification%20tasks%2C%20enabling%20precise%20manipulation%20of%20data%20statistics%20and%20model%20architecture.%20We%20begin%20by%20revisiting%20core%20principles%20of%20unimodal%20ICL%20in%20modern%20transformers.%20While%20several%20prior%20findings%20replicate%2C%20we%20find%20that%20Rotary%20Position%20Embeddings%20%28RoPE%29%20increases%20the%20data%20complexity%20threshold%20for%20ICL.%20Extending%20to%20the%20multimodal%20setting%20reveals%20a%20fundamental%20learning%20asymmetry%3A%20when%20pretrained%20on%20high-diversity%20data%20from%20a%20primary%20modality%2C%20surprisingly%20low%20data%20complexity%20in%20the%20secondary%20modality%20suffices%20for%20multimodal%20ICL%20to%20emerge.%20Mechanistic%20analysis%20shows%20that%20both%20settings%20rely%20on%20an%20induction-style%20mechanism%20that%20copies%20labels%20from%20matching%20in-context%20exemplars%3B%20multimodal%20training%20refines%20and%20extends%20these%20circuits%20across%20modalities.%20Our%20findings%20provide%20a%20mechanistic%20foundation%20for%20understanding%20multimodal%20ICL%20in%20modern%20transformers%20and%20introduce%20a%20controlled%20testbed%20for%20future%20investigation.&entry.1838667208=http%3A//arxiv.org/abs/2601.20796v1&entry.124074799=Read"},
{"title": "QueerGen: How LLMs Reflect Societal Norms on Gender and Sexuality in Sentence Completion Tasks", "author": "Mae Sosto and Delfina Sol Martinez Pandiani and Laura Hollink", "abstract": "This paper examines how Large Language Models (LLMs) reproduce societal norms, particularly heterocisnormativity, and how these norms translate into measurable biases in their text generations. We investigate whether explicit information about a subject's gender or sexuality influences LLM responses across three subject categories: queer-marked, non-queer-marked, and the normalized \"unmarked\" category. Representational imbalances are operationalized as measurable differences in English sentence completions across four dimensions: sentiment, regard, toxicity, and prediction diversity. Our findings show that Masked Language Models (MLMs) produce the least favorable sentiment, higher toxicity, and more negative regard for queer-marked subjects. Autoregressive Language Models (ARLMs) partially mitigate these patterns, while closed-access ARLMs tend to produce more harmful outputs for unmarked subjects. Results suggest that LLMs reproduce normative social assumptions, though the form and degree of bias depend strongly on specific model characteristics, which may redistribute, but not eliminate, representational harms.", "link": "http://arxiv.org/abs/2601.20731v1", "date": "2026-01-28", "relevancy": 2.2218, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4539}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4539}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4253}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QueerGen%3A%20How%20LLMs%20Reflect%20Societal%20Norms%20on%20Gender%20and%20Sexuality%20in%20Sentence%20Completion%20Tasks&body=Title%3A%20QueerGen%3A%20How%20LLMs%20Reflect%20Societal%20Norms%20on%20Gender%20and%20Sexuality%20in%20Sentence%20Completion%20Tasks%0AAuthor%3A%20Mae%20Sosto%20and%20Delfina%20Sol%20Martinez%20Pandiani%20and%20Laura%20Hollink%0AAbstract%3A%20This%20paper%20examines%20how%20Large%20Language%20Models%20%28LLMs%29%20reproduce%20societal%20norms%2C%20particularly%20heterocisnormativity%2C%20and%20how%20these%20norms%20translate%20into%20measurable%20biases%20in%20their%20text%20generations.%20We%20investigate%20whether%20explicit%20information%20about%20a%20subject%27s%20gender%20or%20sexuality%20influences%20LLM%20responses%20across%20three%20subject%20categories%3A%20queer-marked%2C%20non-queer-marked%2C%20and%20the%20normalized%20%22unmarked%22%20category.%20Representational%20imbalances%20are%20operationalized%20as%20measurable%20differences%20in%20English%20sentence%20completions%20across%20four%20dimensions%3A%20sentiment%2C%20regard%2C%20toxicity%2C%20and%20prediction%20diversity.%20Our%20findings%20show%20that%20Masked%20Language%20Models%20%28MLMs%29%20produce%20the%20least%20favorable%20sentiment%2C%20higher%20toxicity%2C%20and%20more%20negative%20regard%20for%20queer-marked%20subjects.%20Autoregressive%20Language%20Models%20%28ARLMs%29%20partially%20mitigate%20these%20patterns%2C%20while%20closed-access%20ARLMs%20tend%20to%20produce%20more%20harmful%20outputs%20for%20unmarked%20subjects.%20Results%20suggest%20that%20LLMs%20reproduce%20normative%20social%20assumptions%2C%20though%20the%20form%20and%20degree%20of%20bias%20depend%20strongly%20on%20specific%20model%20characteristics%2C%20which%20may%20redistribute%2C%20but%20not%20eliminate%2C%20representational%20harms.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20731v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQueerGen%253A%2520How%2520LLMs%2520Reflect%2520Societal%2520Norms%2520on%2520Gender%2520and%2520Sexuality%2520in%2520Sentence%2520Completion%2520Tasks%26entry.906535625%3DMae%2520Sosto%2520and%2520Delfina%2520Sol%2520Martinez%2520Pandiani%2520and%2520Laura%2520Hollink%26entry.1292438233%3DThis%2520paper%2520examines%2520how%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520reproduce%2520societal%2520norms%252C%2520particularly%2520heterocisnormativity%252C%2520and%2520how%2520these%2520norms%2520translate%2520into%2520measurable%2520biases%2520in%2520their%2520text%2520generations.%2520We%2520investigate%2520whether%2520explicit%2520information%2520about%2520a%2520subject%2527s%2520gender%2520or%2520sexuality%2520influences%2520LLM%2520responses%2520across%2520three%2520subject%2520categories%253A%2520queer-marked%252C%2520non-queer-marked%252C%2520and%2520the%2520normalized%2520%2522unmarked%2522%2520category.%2520Representational%2520imbalances%2520are%2520operationalized%2520as%2520measurable%2520differences%2520in%2520English%2520sentence%2520completions%2520across%2520four%2520dimensions%253A%2520sentiment%252C%2520regard%252C%2520toxicity%252C%2520and%2520prediction%2520diversity.%2520Our%2520findings%2520show%2520that%2520Masked%2520Language%2520Models%2520%2528MLMs%2529%2520produce%2520the%2520least%2520favorable%2520sentiment%252C%2520higher%2520toxicity%252C%2520and%2520more%2520negative%2520regard%2520for%2520queer-marked%2520subjects.%2520Autoregressive%2520Language%2520Models%2520%2528ARLMs%2529%2520partially%2520mitigate%2520these%2520patterns%252C%2520while%2520closed-access%2520ARLMs%2520tend%2520to%2520produce%2520more%2520harmful%2520outputs%2520for%2520unmarked%2520subjects.%2520Results%2520suggest%2520that%2520LLMs%2520reproduce%2520normative%2520social%2520assumptions%252C%2520though%2520the%2520form%2520and%2520degree%2520of%2520bias%2520depend%2520strongly%2520on%2520specific%2520model%2520characteristics%252C%2520which%2520may%2520redistribute%252C%2520but%2520not%2520eliminate%252C%2520representational%2520harms.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20731v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QueerGen%3A%20How%20LLMs%20Reflect%20Societal%20Norms%20on%20Gender%20and%20Sexuality%20in%20Sentence%20Completion%20Tasks&entry.906535625=Mae%20Sosto%20and%20Delfina%20Sol%20Martinez%20Pandiani%20and%20Laura%20Hollink&entry.1292438233=This%20paper%20examines%20how%20Large%20Language%20Models%20%28LLMs%29%20reproduce%20societal%20norms%2C%20particularly%20heterocisnormativity%2C%20and%20how%20these%20norms%20translate%20into%20measurable%20biases%20in%20their%20text%20generations.%20We%20investigate%20whether%20explicit%20information%20about%20a%20subject%27s%20gender%20or%20sexuality%20influences%20LLM%20responses%20across%20three%20subject%20categories%3A%20queer-marked%2C%20non-queer-marked%2C%20and%20the%20normalized%20%22unmarked%22%20category.%20Representational%20imbalances%20are%20operationalized%20as%20measurable%20differences%20in%20English%20sentence%20completions%20across%20four%20dimensions%3A%20sentiment%2C%20regard%2C%20toxicity%2C%20and%20prediction%20diversity.%20Our%20findings%20show%20that%20Masked%20Language%20Models%20%28MLMs%29%20produce%20the%20least%20favorable%20sentiment%2C%20higher%20toxicity%2C%20and%20more%20negative%20regard%20for%20queer-marked%20subjects.%20Autoregressive%20Language%20Models%20%28ARLMs%29%20partially%20mitigate%20these%20patterns%2C%20while%20closed-access%20ARLMs%20tend%20to%20produce%20more%20harmful%20outputs%20for%20unmarked%20subjects.%20Results%20suggest%20that%20LLMs%20reproduce%20normative%20social%20assumptions%2C%20though%20the%20form%20and%20degree%20of%20bias%20depend%20strongly%20on%20specific%20model%20characteristics%2C%20which%20may%20redistribute%2C%20but%20not%20eliminate%2C%20representational%20harms.&entry.1838667208=http%3A//arxiv.org/abs/2601.20731v1&entry.124074799=Read"},
{"title": "LLMBind: A Unified Modality-Task Integration Framework", "author": "Bin Zhu and Munan Ning and Peng Jin and Bin Lin and Jinfa Huang and Qi Song and Junwu Zhang and Zhenyu Tang and Mingjun Pan and Li Yuan", "abstract": "Despite recent progress in Multi-Modal Large Language Models (MLLMs), it remains challenging to integrate diverse tasks ranging from pixel-level perception to high-fidelity generation. Existing approaches often suffer from either restricted task extensibility or severe performance degradation due to modality interference. n this paper, we present LLMBind, an extensible framework that unifies multimodal tasks through a dual-pathway mechanism: In-Situ semantic embeddings for localization-sensitive tasks like semantic segmentation and Ex-Situ task-prompts for generation across image, video, and audio modalities. Additionally, we employ a Mixture-of-Experts (MoE) architecture to route task-specific tokens, thereby achieving modality disentanglement and mitigating negative transfer. We also curate a 400k multi-turn interactive dataset focused on iterative visual refinement to enable human-like interaction. Extensive experiments demonstrate that LLMBind achieves excellent performance across multiple perception and generation benchmarks while maintaining superior expandability.", "link": "http://arxiv.org/abs/2402.14891v6", "date": "2026-01-28", "relevancy": 2.2207, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5753}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5598}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5332}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMBind%3A%20A%20Unified%20Modality-Task%20Integration%20Framework&body=Title%3A%20LLMBind%3A%20A%20Unified%20Modality-Task%20Integration%20Framework%0AAuthor%3A%20Bin%20Zhu%20and%20Munan%20Ning%20and%20Peng%20Jin%20and%20Bin%20Lin%20and%20Jinfa%20Huang%20and%20Qi%20Song%20and%20Junwu%20Zhang%20and%20Zhenyu%20Tang%20and%20Mingjun%20Pan%20and%20Li%20Yuan%0AAbstract%3A%20Despite%20recent%20progress%20in%20Multi-Modal%20Large%20Language%20Models%20%28MLLMs%29%2C%20it%20remains%20challenging%20to%20integrate%20diverse%20tasks%20ranging%20from%20pixel-level%20perception%20to%20high-fidelity%20generation.%20Existing%20approaches%20often%20suffer%20from%20either%20restricted%20task%20extensibility%20or%20severe%20performance%20degradation%20due%20to%20modality%20interference.%20n%20this%20paper%2C%20we%20present%20LLMBind%2C%20an%20extensible%20framework%20that%20unifies%20multimodal%20tasks%20through%20a%20dual-pathway%20mechanism%3A%20In-Situ%20semantic%20embeddings%20for%20localization-sensitive%20tasks%20like%20semantic%20segmentation%20and%20Ex-Situ%20task-prompts%20for%20generation%20across%20image%2C%20video%2C%20and%20audio%20modalities.%20Additionally%2C%20we%20employ%20a%20Mixture-of-Experts%20%28MoE%29%20architecture%20to%20route%20task-specific%20tokens%2C%20thereby%20achieving%20modality%20disentanglement%20and%20mitigating%20negative%20transfer.%20We%20also%20curate%20a%20400k%20multi-turn%20interactive%20dataset%20focused%20on%20iterative%20visual%20refinement%20to%20enable%20human-like%20interaction.%20Extensive%20experiments%20demonstrate%20that%20LLMBind%20achieves%20excellent%20performance%20across%20multiple%20perception%20and%20generation%20benchmarks%20while%20maintaining%20superior%20expandability.%0ALink%3A%20http%3A//arxiv.org/abs/2402.14891v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMBind%253A%2520A%2520Unified%2520Modality-Task%2520Integration%2520Framework%26entry.906535625%3DBin%2520Zhu%2520and%2520Munan%2520Ning%2520and%2520Peng%2520Jin%2520and%2520Bin%2520Lin%2520and%2520Jinfa%2520Huang%2520and%2520Qi%2520Song%2520and%2520Junwu%2520Zhang%2520and%2520Zhenyu%2520Tang%2520and%2520Mingjun%2520Pan%2520and%2520Li%2520Yuan%26entry.1292438233%3DDespite%2520recent%2520progress%2520in%2520Multi-Modal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520it%2520remains%2520challenging%2520to%2520integrate%2520diverse%2520tasks%2520ranging%2520from%2520pixel-level%2520perception%2520to%2520high-fidelity%2520generation.%2520Existing%2520approaches%2520often%2520suffer%2520from%2520either%2520restricted%2520task%2520extensibility%2520or%2520severe%2520performance%2520degradation%2520due%2520to%2520modality%2520interference.%2520n%2520this%2520paper%252C%2520we%2520present%2520LLMBind%252C%2520an%2520extensible%2520framework%2520that%2520unifies%2520multimodal%2520tasks%2520through%2520a%2520dual-pathway%2520mechanism%253A%2520In-Situ%2520semantic%2520embeddings%2520for%2520localization-sensitive%2520tasks%2520like%2520semantic%2520segmentation%2520and%2520Ex-Situ%2520task-prompts%2520for%2520generation%2520across%2520image%252C%2520video%252C%2520and%2520audio%2520modalities.%2520Additionally%252C%2520we%2520employ%2520a%2520Mixture-of-Experts%2520%2528MoE%2529%2520architecture%2520to%2520route%2520task-specific%2520tokens%252C%2520thereby%2520achieving%2520modality%2520disentanglement%2520and%2520mitigating%2520negative%2520transfer.%2520We%2520also%2520curate%2520a%2520400k%2520multi-turn%2520interactive%2520dataset%2520focused%2520on%2520iterative%2520visual%2520refinement%2520to%2520enable%2520human-like%2520interaction.%2520Extensive%2520experiments%2520demonstrate%2520that%2520LLMBind%2520achieves%2520excellent%2520performance%2520across%2520multiple%2520perception%2520and%2520generation%2520benchmarks%2520while%2520maintaining%2520superior%2520expandability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.14891v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMBind%3A%20A%20Unified%20Modality-Task%20Integration%20Framework&entry.906535625=Bin%20Zhu%20and%20Munan%20Ning%20and%20Peng%20Jin%20and%20Bin%20Lin%20and%20Jinfa%20Huang%20and%20Qi%20Song%20and%20Junwu%20Zhang%20and%20Zhenyu%20Tang%20and%20Mingjun%20Pan%20and%20Li%20Yuan&entry.1292438233=Despite%20recent%20progress%20in%20Multi-Modal%20Large%20Language%20Models%20%28MLLMs%29%2C%20it%20remains%20challenging%20to%20integrate%20diverse%20tasks%20ranging%20from%20pixel-level%20perception%20to%20high-fidelity%20generation.%20Existing%20approaches%20often%20suffer%20from%20either%20restricted%20task%20extensibility%20or%20severe%20performance%20degradation%20due%20to%20modality%20interference.%20n%20this%20paper%2C%20we%20present%20LLMBind%2C%20an%20extensible%20framework%20that%20unifies%20multimodal%20tasks%20through%20a%20dual-pathway%20mechanism%3A%20In-Situ%20semantic%20embeddings%20for%20localization-sensitive%20tasks%20like%20semantic%20segmentation%20and%20Ex-Situ%20task-prompts%20for%20generation%20across%20image%2C%20video%2C%20and%20audio%20modalities.%20Additionally%2C%20we%20employ%20a%20Mixture-of-Experts%20%28MoE%29%20architecture%20to%20route%20task-specific%20tokens%2C%20thereby%20achieving%20modality%20disentanglement%20and%20mitigating%20negative%20transfer.%20We%20also%20curate%20a%20400k%20multi-turn%20interactive%20dataset%20focused%20on%20iterative%20visual%20refinement%20to%20enable%20human-like%20interaction.%20Extensive%20experiments%20demonstrate%20that%20LLMBind%20achieves%20excellent%20performance%20across%20multiple%20perception%20and%20generation%20benchmarks%20while%20maintaining%20superior%20expandability.&entry.1838667208=http%3A//arxiv.org/abs/2402.14891v6&entry.124074799=Read"},
{"title": "StructAlign: Structured Cross-Modal Alignment for Continual Text-to-Video Retrieval", "author": "Shaokun Wang and Weili Guan and Jizhou Han and Jianlong Wu and Yupeng Hu and Liqiang Nie", "abstract": "Continual Text-to-Video Retrieval (CTVR) is a challenging multimodal continual learning setting, where models must incrementally learn new semantic categories while maintaining accurate text-video alignment for previously learned ones, thus making it particularly prone to catastrophic forgetting. A key challenge in CTVR is feature drift, which manifests in two forms: intra-modal feature drift caused by continual learning within each modality, and non-cooperative feature drift across modalities that leads to modality misalignment. To mitigate these issues, we propose StructAlign, a structured cross-modal alignment method for CTVR. First, StructAlign introduces a simplex Equiangular Tight Frame (ETF) geometry as a unified geometric prior to mitigate modality misalignment. Building upon this geometric prior, we design a cross-modal ETF alignment loss that aligns text and video features with category-level ETF prototypes, encouraging the learned representations to form an approximate simplex ETF geometry. In addition, to suppress intra-modal feature drift, we design a Cross-modal Relation Preserving loss, which leverages complementary modalities to preserve cross-modal similarity relations, providing stable relational supervision for feature updates. By jointly addressing non-cooperative feature drift across modalities and intra-modal feature drift, StructAlign effectively alleviates catastrophic forgetting in CTVR. Extensive experiments on benchmark datasets demonstrate that our method consistently outperforms state-of-the-art continual retrieval approaches.", "link": "http://arxiv.org/abs/2601.20597v1", "date": "2026-01-28", "relevancy": 2.2138, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5959}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5467}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StructAlign%3A%20Structured%20Cross-Modal%20Alignment%20for%20Continual%20Text-to-Video%20Retrieval&body=Title%3A%20StructAlign%3A%20Structured%20Cross-Modal%20Alignment%20for%20Continual%20Text-to-Video%20Retrieval%0AAuthor%3A%20Shaokun%20Wang%20and%20Weili%20Guan%20and%20Jizhou%20Han%20and%20Jianlong%20Wu%20and%20Yupeng%20Hu%20and%20Liqiang%20Nie%0AAbstract%3A%20Continual%20Text-to-Video%20Retrieval%20%28CTVR%29%20is%20a%20challenging%20multimodal%20continual%20learning%20setting%2C%20where%20models%20must%20incrementally%20learn%20new%20semantic%20categories%20while%20maintaining%20accurate%20text-video%20alignment%20for%20previously%20learned%20ones%2C%20thus%20making%20it%20particularly%20prone%20to%20catastrophic%20forgetting.%20A%20key%20challenge%20in%20CTVR%20is%20feature%20drift%2C%20which%20manifests%20in%20two%20forms%3A%20intra-modal%20feature%20drift%20caused%20by%20continual%20learning%20within%20each%20modality%2C%20and%20non-cooperative%20feature%20drift%20across%20modalities%20that%20leads%20to%20modality%20misalignment.%20To%20mitigate%20these%20issues%2C%20we%20propose%20StructAlign%2C%20a%20structured%20cross-modal%20alignment%20method%20for%20CTVR.%20First%2C%20StructAlign%20introduces%20a%20simplex%20Equiangular%20Tight%20Frame%20%28ETF%29%20geometry%20as%20a%20unified%20geometric%20prior%20to%20mitigate%20modality%20misalignment.%20Building%20upon%20this%20geometric%20prior%2C%20we%20design%20a%20cross-modal%20ETF%20alignment%20loss%20that%20aligns%20text%20and%20video%20features%20with%20category-level%20ETF%20prototypes%2C%20encouraging%20the%20learned%20representations%20to%20form%20an%20approximate%20simplex%20ETF%20geometry.%20In%20addition%2C%20to%20suppress%20intra-modal%20feature%20drift%2C%20we%20design%20a%20Cross-modal%20Relation%20Preserving%20loss%2C%20which%20leverages%20complementary%20modalities%20to%20preserve%20cross-modal%20similarity%20relations%2C%20providing%20stable%20relational%20supervision%20for%20feature%20updates.%20By%20jointly%20addressing%20non-cooperative%20feature%20drift%20across%20modalities%20and%20intra-modal%20feature%20drift%2C%20StructAlign%20effectively%20alleviates%20catastrophic%20forgetting%20in%20CTVR.%20Extensive%20experiments%20on%20benchmark%20datasets%20demonstrate%20that%20our%20method%20consistently%20outperforms%20state-of-the-art%20continual%20retrieval%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20597v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructAlign%253A%2520Structured%2520Cross-Modal%2520Alignment%2520for%2520Continual%2520Text-to-Video%2520Retrieval%26entry.906535625%3DShaokun%2520Wang%2520and%2520Weili%2520Guan%2520and%2520Jizhou%2520Han%2520and%2520Jianlong%2520Wu%2520and%2520Yupeng%2520Hu%2520and%2520Liqiang%2520Nie%26entry.1292438233%3DContinual%2520Text-to-Video%2520Retrieval%2520%2528CTVR%2529%2520is%2520a%2520challenging%2520multimodal%2520continual%2520learning%2520setting%252C%2520where%2520models%2520must%2520incrementally%2520learn%2520new%2520semantic%2520categories%2520while%2520maintaining%2520accurate%2520text-video%2520alignment%2520for%2520previously%2520learned%2520ones%252C%2520thus%2520making%2520it%2520particularly%2520prone%2520to%2520catastrophic%2520forgetting.%2520A%2520key%2520challenge%2520in%2520CTVR%2520is%2520feature%2520drift%252C%2520which%2520manifests%2520in%2520two%2520forms%253A%2520intra-modal%2520feature%2520drift%2520caused%2520by%2520continual%2520learning%2520within%2520each%2520modality%252C%2520and%2520non-cooperative%2520feature%2520drift%2520across%2520modalities%2520that%2520leads%2520to%2520modality%2520misalignment.%2520To%2520mitigate%2520these%2520issues%252C%2520we%2520propose%2520StructAlign%252C%2520a%2520structured%2520cross-modal%2520alignment%2520method%2520for%2520CTVR.%2520First%252C%2520StructAlign%2520introduces%2520a%2520simplex%2520Equiangular%2520Tight%2520Frame%2520%2528ETF%2529%2520geometry%2520as%2520a%2520unified%2520geometric%2520prior%2520to%2520mitigate%2520modality%2520misalignment.%2520Building%2520upon%2520this%2520geometric%2520prior%252C%2520we%2520design%2520a%2520cross-modal%2520ETF%2520alignment%2520loss%2520that%2520aligns%2520text%2520and%2520video%2520features%2520with%2520category-level%2520ETF%2520prototypes%252C%2520encouraging%2520the%2520learned%2520representations%2520to%2520form%2520an%2520approximate%2520simplex%2520ETF%2520geometry.%2520In%2520addition%252C%2520to%2520suppress%2520intra-modal%2520feature%2520drift%252C%2520we%2520design%2520a%2520Cross-modal%2520Relation%2520Preserving%2520loss%252C%2520which%2520leverages%2520complementary%2520modalities%2520to%2520preserve%2520cross-modal%2520similarity%2520relations%252C%2520providing%2520stable%2520relational%2520supervision%2520for%2520feature%2520updates.%2520By%2520jointly%2520addressing%2520non-cooperative%2520feature%2520drift%2520across%2520modalities%2520and%2520intra-modal%2520feature%2520drift%252C%2520StructAlign%2520effectively%2520alleviates%2520catastrophic%2520forgetting%2520in%2520CTVR.%2520Extensive%2520experiments%2520on%2520benchmark%2520datasets%2520demonstrate%2520that%2520our%2520method%2520consistently%2520outperforms%2520state-of-the-art%2520continual%2520retrieval%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20597v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StructAlign%3A%20Structured%20Cross-Modal%20Alignment%20for%20Continual%20Text-to-Video%20Retrieval&entry.906535625=Shaokun%20Wang%20and%20Weili%20Guan%20and%20Jizhou%20Han%20and%20Jianlong%20Wu%20and%20Yupeng%20Hu%20and%20Liqiang%20Nie&entry.1292438233=Continual%20Text-to-Video%20Retrieval%20%28CTVR%29%20is%20a%20challenging%20multimodal%20continual%20learning%20setting%2C%20where%20models%20must%20incrementally%20learn%20new%20semantic%20categories%20while%20maintaining%20accurate%20text-video%20alignment%20for%20previously%20learned%20ones%2C%20thus%20making%20it%20particularly%20prone%20to%20catastrophic%20forgetting.%20A%20key%20challenge%20in%20CTVR%20is%20feature%20drift%2C%20which%20manifests%20in%20two%20forms%3A%20intra-modal%20feature%20drift%20caused%20by%20continual%20learning%20within%20each%20modality%2C%20and%20non-cooperative%20feature%20drift%20across%20modalities%20that%20leads%20to%20modality%20misalignment.%20To%20mitigate%20these%20issues%2C%20we%20propose%20StructAlign%2C%20a%20structured%20cross-modal%20alignment%20method%20for%20CTVR.%20First%2C%20StructAlign%20introduces%20a%20simplex%20Equiangular%20Tight%20Frame%20%28ETF%29%20geometry%20as%20a%20unified%20geometric%20prior%20to%20mitigate%20modality%20misalignment.%20Building%20upon%20this%20geometric%20prior%2C%20we%20design%20a%20cross-modal%20ETF%20alignment%20loss%20that%20aligns%20text%20and%20video%20features%20with%20category-level%20ETF%20prototypes%2C%20encouraging%20the%20learned%20representations%20to%20form%20an%20approximate%20simplex%20ETF%20geometry.%20In%20addition%2C%20to%20suppress%20intra-modal%20feature%20drift%2C%20we%20design%20a%20Cross-modal%20Relation%20Preserving%20loss%2C%20which%20leverages%20complementary%20modalities%20to%20preserve%20cross-modal%20similarity%20relations%2C%20providing%20stable%20relational%20supervision%20for%20feature%20updates.%20By%20jointly%20addressing%20non-cooperative%20feature%20drift%20across%20modalities%20and%20intra-modal%20feature%20drift%2C%20StructAlign%20effectively%20alleviates%20catastrophic%20forgetting%20in%20CTVR.%20Extensive%20experiments%20on%20benchmark%20datasets%20demonstrate%20that%20our%20method%20consistently%20outperforms%20state-of-the-art%20continual%20retrieval%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2601.20597v1&entry.124074799=Read"},
{"title": "Investigating the Development of Task-Oriented Communication in Vision-Language Models", "author": "Boaz Carmeli and Orr Paradise and Shafi Goldwasser and Yonatan Belinkov and Ron Meir", "abstract": "We investigate whether \\emph{LLM-based agents} can develop task-oriented communication protocols that differ from standard natural language in collaborative reasoning tasks. Our focus is on two core properties such task-oriented protocols may exhibit: Efficiency -- conveying task-relevant information more concisely than natural language, and Covertness -- becoming difficult for external observers to interpret, raising concerns about transparency and control. To investigate these aspects, we use a referential-game framework in which vision-language model (VLM) agents communicate, providing a controlled, measurable setting for evaluating language variants. Experiments show that VLMs can develop effective, task-adapted communication patterns. At the same time, they can develop covert protocols that are difficult for humans and external agents to interpret. We also observe spontaneous coordination between similar models without explicitly shared protocols. These findings highlight both the potential and the risks of task-oriented communication, and position referential games as a valuable testbed for future work in this area.", "link": "http://arxiv.org/abs/2601.20641v1", "date": "2026-01-28", "relevancy": 2.2116, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5637}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5637}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20the%20Development%20of%20Task-Oriented%20Communication%20in%20Vision-Language%20Models&body=Title%3A%20Investigating%20the%20Development%20of%20Task-Oriented%20Communication%20in%20Vision-Language%20Models%0AAuthor%3A%20Boaz%20Carmeli%20and%20Orr%20Paradise%20and%20Shafi%20Goldwasser%20and%20Yonatan%20Belinkov%20and%20Ron%20Meir%0AAbstract%3A%20We%20investigate%20whether%20%5Cemph%7BLLM-based%20agents%7D%20can%20develop%20task-oriented%20communication%20protocols%20that%20differ%20from%20standard%20natural%20language%20in%20collaborative%20reasoning%20tasks.%20Our%20focus%20is%20on%20two%20core%20properties%20such%20task-oriented%20protocols%20may%20exhibit%3A%20Efficiency%20--%20conveying%20task-relevant%20information%20more%20concisely%20than%20natural%20language%2C%20and%20Covertness%20--%20becoming%20difficult%20for%20external%20observers%20to%20interpret%2C%20raising%20concerns%20about%20transparency%20and%20control.%20To%20investigate%20these%20aspects%2C%20we%20use%20a%20referential-game%20framework%20in%20which%20vision-language%20model%20%28VLM%29%20agents%20communicate%2C%20providing%20a%20controlled%2C%20measurable%20setting%20for%20evaluating%20language%20variants.%20Experiments%20show%20that%20VLMs%20can%20develop%20effective%2C%20task-adapted%20communication%20patterns.%20At%20the%20same%20time%2C%20they%20can%20develop%20covert%20protocols%20that%20are%20difficult%20for%20humans%20and%20external%20agents%20to%20interpret.%20We%20also%20observe%20spontaneous%20coordination%20between%20similar%20models%20without%20explicitly%20shared%20protocols.%20These%20findings%20highlight%20both%20the%20potential%20and%20the%20risks%20of%20task-oriented%20communication%2C%20and%20position%20referential%20games%20as%20a%20valuable%20testbed%20for%20future%20work%20in%20this%20area.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20641v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520the%2520Development%2520of%2520Task-Oriented%2520Communication%2520in%2520Vision-Language%2520Models%26entry.906535625%3DBoaz%2520Carmeli%2520and%2520Orr%2520Paradise%2520and%2520Shafi%2520Goldwasser%2520and%2520Yonatan%2520Belinkov%2520and%2520Ron%2520Meir%26entry.1292438233%3DWe%2520investigate%2520whether%2520%255Cemph%257BLLM-based%2520agents%257D%2520can%2520develop%2520task-oriented%2520communication%2520protocols%2520that%2520differ%2520from%2520standard%2520natural%2520language%2520in%2520collaborative%2520reasoning%2520tasks.%2520Our%2520focus%2520is%2520on%2520two%2520core%2520properties%2520such%2520task-oriented%2520protocols%2520may%2520exhibit%253A%2520Efficiency%2520--%2520conveying%2520task-relevant%2520information%2520more%2520concisely%2520than%2520natural%2520language%252C%2520and%2520Covertness%2520--%2520becoming%2520difficult%2520for%2520external%2520observers%2520to%2520interpret%252C%2520raising%2520concerns%2520about%2520transparency%2520and%2520control.%2520To%2520investigate%2520these%2520aspects%252C%2520we%2520use%2520a%2520referential-game%2520framework%2520in%2520which%2520vision-language%2520model%2520%2528VLM%2529%2520agents%2520communicate%252C%2520providing%2520a%2520controlled%252C%2520measurable%2520setting%2520for%2520evaluating%2520language%2520variants.%2520Experiments%2520show%2520that%2520VLMs%2520can%2520develop%2520effective%252C%2520task-adapted%2520communication%2520patterns.%2520At%2520the%2520same%2520time%252C%2520they%2520can%2520develop%2520covert%2520protocols%2520that%2520are%2520difficult%2520for%2520humans%2520and%2520external%2520agents%2520to%2520interpret.%2520We%2520also%2520observe%2520spontaneous%2520coordination%2520between%2520similar%2520models%2520without%2520explicitly%2520shared%2520protocols.%2520These%2520findings%2520highlight%2520both%2520the%2520potential%2520and%2520the%2520risks%2520of%2520task-oriented%2520communication%252C%2520and%2520position%2520referential%2520games%2520as%2520a%2520valuable%2520testbed%2520for%2520future%2520work%2520in%2520this%2520area.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20641v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20the%20Development%20of%20Task-Oriented%20Communication%20in%20Vision-Language%20Models&entry.906535625=Boaz%20Carmeli%20and%20Orr%20Paradise%20and%20Shafi%20Goldwasser%20and%20Yonatan%20Belinkov%20and%20Ron%20Meir&entry.1292438233=We%20investigate%20whether%20%5Cemph%7BLLM-based%20agents%7D%20can%20develop%20task-oriented%20communication%20protocols%20that%20differ%20from%20standard%20natural%20language%20in%20collaborative%20reasoning%20tasks.%20Our%20focus%20is%20on%20two%20core%20properties%20such%20task-oriented%20protocols%20may%20exhibit%3A%20Efficiency%20--%20conveying%20task-relevant%20information%20more%20concisely%20than%20natural%20language%2C%20and%20Covertness%20--%20becoming%20difficult%20for%20external%20observers%20to%20interpret%2C%20raising%20concerns%20about%20transparency%20and%20control.%20To%20investigate%20these%20aspects%2C%20we%20use%20a%20referential-game%20framework%20in%20which%20vision-language%20model%20%28VLM%29%20agents%20communicate%2C%20providing%20a%20controlled%2C%20measurable%20setting%20for%20evaluating%20language%20variants.%20Experiments%20show%20that%20VLMs%20can%20develop%20effective%2C%20task-adapted%20communication%20patterns.%20At%20the%20same%20time%2C%20they%20can%20develop%20covert%20protocols%20that%20are%20difficult%20for%20humans%20and%20external%20agents%20to%20interpret.%20We%20also%20observe%20spontaneous%20coordination%20between%20similar%20models%20without%20explicitly%20shared%20protocols.%20These%20findings%20highlight%20both%20the%20potential%20and%20the%20risks%20of%20task-oriented%20communication%2C%20and%20position%20referential%20games%20as%20a%20valuable%20testbed%20for%20future%20work%20in%20this%20area.&entry.1838667208=http%3A//arxiv.org/abs/2601.20641v1&entry.124074799=Read"},
{"title": "Detecting and Mitigating Memorization in Diffusion Models through Anisotropy of the Log-Probability", "author": "Rohan Asthana and Vasileios Belagiannis", "abstract": "Diffusion-based image generative models produce high-fidelity images through iterative denoising but remain vulnerable to memorization, where they unintentionally reproduce exact copies or parts of training images. Recent memorization detection methods are primarily based on the norm of score difference as indicators of memorization. We prove that such norm-based metrics are mainly effective under the assumption of isotropic log-probability distributions, which generally holds at high or medium noise levels. In contrast, analyzing the anisotropic regime reveals that memorized samples exhibit strong angular alignment between the guidance vector and unconditional scores in the low-noise setting. Through these insights, we develop a memorization detection metric by integrating isotropic norm and anisotropic alignment. Our detection metric can be computed directly on pure noise inputs via two conditional and unconditional forward passes, eliminating the need for costly denoising steps. Detection experiments on Stable Diffusion v1.4 and v2 show that our metric outperforms existing denoising-free detection methods while being at least approximately 5x faster than the previous best approach. Finally, we demonstrate the effectiveness of our approach by utilizing a mitigation strategy that adapts memorized prompts based on our developed metric.", "link": "http://arxiv.org/abs/2601.20642v1", "date": "2026-01-28", "relevancy": 2.2097, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5715}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5499}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20and%20Mitigating%20Memorization%20in%20Diffusion%20Models%20through%20Anisotropy%20of%20the%20Log-Probability&body=Title%3A%20Detecting%20and%20Mitigating%20Memorization%20in%20Diffusion%20Models%20through%20Anisotropy%20of%20the%20Log-Probability%0AAuthor%3A%20Rohan%20Asthana%20and%20Vasileios%20Belagiannis%0AAbstract%3A%20Diffusion-based%20image%20generative%20models%20produce%20high-fidelity%20images%20through%20iterative%20denoising%20but%20remain%20vulnerable%20to%20memorization%2C%20where%20they%20unintentionally%20reproduce%20exact%20copies%20or%20parts%20of%20training%20images.%20Recent%20memorization%20detection%20methods%20are%20primarily%20based%20on%20the%20norm%20of%20score%20difference%20as%20indicators%20of%20memorization.%20We%20prove%20that%20such%20norm-based%20metrics%20are%20mainly%20effective%20under%20the%20assumption%20of%20isotropic%20log-probability%20distributions%2C%20which%20generally%20holds%20at%20high%20or%20medium%20noise%20levels.%20In%20contrast%2C%20analyzing%20the%20anisotropic%20regime%20reveals%20that%20memorized%20samples%20exhibit%20strong%20angular%20alignment%20between%20the%20guidance%20vector%20and%20unconditional%20scores%20in%20the%20low-noise%20setting.%20Through%20these%20insights%2C%20we%20develop%20a%20memorization%20detection%20metric%20by%20integrating%20isotropic%20norm%20and%20anisotropic%20alignment.%20Our%20detection%20metric%20can%20be%20computed%20directly%20on%20pure%20noise%20inputs%20via%20two%20conditional%20and%20unconditional%20forward%20passes%2C%20eliminating%20the%20need%20for%20costly%20denoising%20steps.%20Detection%20experiments%20on%20Stable%20Diffusion%20v1.4%20and%20v2%20show%20that%20our%20metric%20outperforms%20existing%20denoising-free%20detection%20methods%20while%20being%20at%20least%20approximately%205x%20faster%20than%20the%20previous%20best%20approach.%20Finally%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20approach%20by%20utilizing%20a%20mitigation%20strategy%20that%20adapts%20memorized%20prompts%20based%20on%20our%20developed%20metric.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20642v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520and%2520Mitigating%2520Memorization%2520in%2520Diffusion%2520Models%2520through%2520Anisotropy%2520of%2520the%2520Log-Probability%26entry.906535625%3DRohan%2520Asthana%2520and%2520Vasileios%2520Belagiannis%26entry.1292438233%3DDiffusion-based%2520image%2520generative%2520models%2520produce%2520high-fidelity%2520images%2520through%2520iterative%2520denoising%2520but%2520remain%2520vulnerable%2520to%2520memorization%252C%2520where%2520they%2520unintentionally%2520reproduce%2520exact%2520copies%2520or%2520parts%2520of%2520training%2520images.%2520Recent%2520memorization%2520detection%2520methods%2520are%2520primarily%2520based%2520on%2520the%2520norm%2520of%2520score%2520difference%2520as%2520indicators%2520of%2520memorization.%2520We%2520prove%2520that%2520such%2520norm-based%2520metrics%2520are%2520mainly%2520effective%2520under%2520the%2520assumption%2520of%2520isotropic%2520log-probability%2520distributions%252C%2520which%2520generally%2520holds%2520at%2520high%2520or%2520medium%2520noise%2520levels.%2520In%2520contrast%252C%2520analyzing%2520the%2520anisotropic%2520regime%2520reveals%2520that%2520memorized%2520samples%2520exhibit%2520strong%2520angular%2520alignment%2520between%2520the%2520guidance%2520vector%2520and%2520unconditional%2520scores%2520in%2520the%2520low-noise%2520setting.%2520Through%2520these%2520insights%252C%2520we%2520develop%2520a%2520memorization%2520detection%2520metric%2520by%2520integrating%2520isotropic%2520norm%2520and%2520anisotropic%2520alignment.%2520Our%2520detection%2520metric%2520can%2520be%2520computed%2520directly%2520on%2520pure%2520noise%2520inputs%2520via%2520two%2520conditional%2520and%2520unconditional%2520forward%2520passes%252C%2520eliminating%2520the%2520need%2520for%2520costly%2520denoising%2520steps.%2520Detection%2520experiments%2520on%2520Stable%2520Diffusion%2520v1.4%2520and%2520v2%2520show%2520that%2520our%2520metric%2520outperforms%2520existing%2520denoising-free%2520detection%2520methods%2520while%2520being%2520at%2520least%2520approximately%25205x%2520faster%2520than%2520the%2520previous%2520best%2520approach.%2520Finally%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520by%2520utilizing%2520a%2520mitigation%2520strategy%2520that%2520adapts%2520memorized%2520prompts%2520based%2520on%2520our%2520developed%2520metric.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20642v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20and%20Mitigating%20Memorization%20in%20Diffusion%20Models%20through%20Anisotropy%20of%20the%20Log-Probability&entry.906535625=Rohan%20Asthana%20and%20Vasileios%20Belagiannis&entry.1292438233=Diffusion-based%20image%20generative%20models%20produce%20high-fidelity%20images%20through%20iterative%20denoising%20but%20remain%20vulnerable%20to%20memorization%2C%20where%20they%20unintentionally%20reproduce%20exact%20copies%20or%20parts%20of%20training%20images.%20Recent%20memorization%20detection%20methods%20are%20primarily%20based%20on%20the%20norm%20of%20score%20difference%20as%20indicators%20of%20memorization.%20We%20prove%20that%20such%20norm-based%20metrics%20are%20mainly%20effective%20under%20the%20assumption%20of%20isotropic%20log-probability%20distributions%2C%20which%20generally%20holds%20at%20high%20or%20medium%20noise%20levels.%20In%20contrast%2C%20analyzing%20the%20anisotropic%20regime%20reveals%20that%20memorized%20samples%20exhibit%20strong%20angular%20alignment%20between%20the%20guidance%20vector%20and%20unconditional%20scores%20in%20the%20low-noise%20setting.%20Through%20these%20insights%2C%20we%20develop%20a%20memorization%20detection%20metric%20by%20integrating%20isotropic%20norm%20and%20anisotropic%20alignment.%20Our%20detection%20metric%20can%20be%20computed%20directly%20on%20pure%20noise%20inputs%20via%20two%20conditional%20and%20unconditional%20forward%20passes%2C%20eliminating%20the%20need%20for%20costly%20denoising%20steps.%20Detection%20experiments%20on%20Stable%20Diffusion%20v1.4%20and%20v2%20show%20that%20our%20metric%20outperforms%20existing%20denoising-free%20detection%20methods%20while%20being%20at%20least%20approximately%205x%20faster%20than%20the%20previous%20best%20approach.%20Finally%2C%20we%20demonstrate%20the%20effectiveness%20of%20our%20approach%20by%20utilizing%20a%20mitigation%20strategy%20that%20adapts%20memorized%20prompts%20based%20on%20our%20developed%20metric.&entry.1838667208=http%3A//arxiv.org/abs/2601.20642v1&entry.124074799=Read"},
{"title": "Diagonal Linear Networks and the Lasso Regularization Path", "author": "Rapha\u00ebl Berthier", "abstract": "Diagonal linear networks are neural networks with linear activation and diagonal weight matrices. Their theoretical interest is that their implicit regularization can be rigorously analyzed: from a small initialization, the training of diagonal linear networks converges to the linear predictor with minimal 1-norm among minimizers of the training loss. In this paper, we deepen this analysis showing that the full training trajectory of diagonal linear networks is closely related to the lasso regularization path. In this connection, the training time plays the role of an inverse regularization parameter. Both rigorous results and simulations are provided to illustrate this conclusion. Under a monotonicity assumption on the lasso regularization path, the connection is exact while in the general case, we show an approximate connection.", "link": "http://arxiv.org/abs/2509.18766v2", "date": "2026-01-28", "relevancy": 2.2087, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4636}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4417}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diagonal%20Linear%20Networks%20and%20the%20Lasso%20Regularization%20Path&body=Title%3A%20Diagonal%20Linear%20Networks%20and%20the%20Lasso%20Regularization%20Path%0AAuthor%3A%20Rapha%C3%ABl%20Berthier%0AAbstract%3A%20Diagonal%20linear%20networks%20are%20neural%20networks%20with%20linear%20activation%20and%20diagonal%20weight%20matrices.%20Their%20theoretical%20interest%20is%20that%20their%20implicit%20regularization%20can%20be%20rigorously%20analyzed%3A%20from%20a%20small%20initialization%2C%20the%20training%20of%20diagonal%20linear%20networks%20converges%20to%20the%20linear%20predictor%20with%20minimal%201-norm%20among%20minimizers%20of%20the%20training%20loss.%20In%20this%20paper%2C%20we%20deepen%20this%20analysis%20showing%20that%20the%20full%20training%20trajectory%20of%20diagonal%20linear%20networks%20is%20closely%20related%20to%20the%20lasso%20regularization%20path.%20In%20this%20connection%2C%20the%20training%20time%20plays%20the%20role%20of%20an%20inverse%20regularization%20parameter.%20Both%20rigorous%20results%20and%20simulations%20are%20provided%20to%20illustrate%20this%20conclusion.%20Under%20a%20monotonicity%20assumption%20on%20the%20lasso%20regularization%20path%2C%20the%20connection%20is%20exact%20while%20in%20the%20general%20case%2C%20we%20show%20an%20approximate%20connection.%0ALink%3A%20http%3A//arxiv.org/abs/2509.18766v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiagonal%2520Linear%2520Networks%2520and%2520the%2520Lasso%2520Regularization%2520Path%26entry.906535625%3DRapha%25C3%25ABl%2520Berthier%26entry.1292438233%3DDiagonal%2520linear%2520networks%2520are%2520neural%2520networks%2520with%2520linear%2520activation%2520and%2520diagonal%2520weight%2520matrices.%2520Their%2520theoretical%2520interest%2520is%2520that%2520their%2520implicit%2520regularization%2520can%2520be%2520rigorously%2520analyzed%253A%2520from%2520a%2520small%2520initialization%252C%2520the%2520training%2520of%2520diagonal%2520linear%2520networks%2520converges%2520to%2520the%2520linear%2520predictor%2520with%2520minimal%25201-norm%2520among%2520minimizers%2520of%2520the%2520training%2520loss.%2520In%2520this%2520paper%252C%2520we%2520deepen%2520this%2520analysis%2520showing%2520that%2520the%2520full%2520training%2520trajectory%2520of%2520diagonal%2520linear%2520networks%2520is%2520closely%2520related%2520to%2520the%2520lasso%2520regularization%2520path.%2520In%2520this%2520connection%252C%2520the%2520training%2520time%2520plays%2520the%2520role%2520of%2520an%2520inverse%2520regularization%2520parameter.%2520Both%2520rigorous%2520results%2520and%2520simulations%2520are%2520provided%2520to%2520illustrate%2520this%2520conclusion.%2520Under%2520a%2520monotonicity%2520assumption%2520on%2520the%2520lasso%2520regularization%2520path%252C%2520the%2520connection%2520is%2520exact%2520while%2520in%2520the%2520general%2520case%252C%2520we%2520show%2520an%2520approximate%2520connection.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.18766v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diagonal%20Linear%20Networks%20and%20the%20Lasso%20Regularization%20Path&entry.906535625=Rapha%C3%ABl%20Berthier&entry.1292438233=Diagonal%20linear%20networks%20are%20neural%20networks%20with%20linear%20activation%20and%20diagonal%20weight%20matrices.%20Their%20theoretical%20interest%20is%20that%20their%20implicit%20regularization%20can%20be%20rigorously%20analyzed%3A%20from%20a%20small%20initialization%2C%20the%20training%20of%20diagonal%20linear%20networks%20converges%20to%20the%20linear%20predictor%20with%20minimal%201-norm%20among%20minimizers%20of%20the%20training%20loss.%20In%20this%20paper%2C%20we%20deepen%20this%20analysis%20showing%20that%20the%20full%20training%20trajectory%20of%20diagonal%20linear%20networks%20is%20closely%20related%20to%20the%20lasso%20regularization%20path.%20In%20this%20connection%2C%20the%20training%20time%20plays%20the%20role%20of%20an%20inverse%20regularization%20parameter.%20Both%20rigorous%20results%20and%20simulations%20are%20provided%20to%20illustrate%20this%20conclusion.%20Under%20a%20monotonicity%20assumption%20on%20the%20lasso%20regularization%20path%2C%20the%20connection%20is%20exact%20while%20in%20the%20general%20case%2C%20we%20show%20an%20approximate%20connection.&entry.1838667208=http%3A//arxiv.org/abs/2509.18766v2&entry.124074799=Read"},
{"title": "Summaries as Centroids for Interpretable and Scalable Text Clustering", "author": "Jairo Diaz-Rodriguez", "abstract": "We introduce k-NLPmeans and k-LLMmeans, text-clustering variants of k-means that periodically replace numeric centroids with textual summaries. The key idea, summary-as-centroid, retains k-means assignments in embedding space while producing human-readable, auditable cluster prototypes. The method is LLM-optional: k-NLPmeans uses lightweight, deterministic summarizers, enabling offline, low-cost, and stable operation; k-LLMmeans is a drop-in upgrade that uses an LLM for summaries under a fixed per-iteration budget whose cost does not grow with dataset size. We also present a mini-batch extension for real-time clustering of streaming text. Across diverse datasets, embedding models, and summarization strategies, our approach consistently outperforms classical baselines and approaches the accuracy of recent LLM-based clustering-without extensive LLM calls. Finally, we provide a case study on sequential text streams and release a StackExchange-derived benchmark for evaluating streaming text clustering.", "link": "http://arxiv.org/abs/2502.09667v4", "date": "2026-01-28", "relevancy": 2.2057, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4456}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4403}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Summaries%20as%20Centroids%20for%20Interpretable%20and%20Scalable%20Text%20Clustering&body=Title%3A%20Summaries%20as%20Centroids%20for%20Interpretable%20and%20Scalable%20Text%20Clustering%0AAuthor%3A%20Jairo%20Diaz-Rodriguez%0AAbstract%3A%20We%20introduce%20k-NLPmeans%20and%20k-LLMmeans%2C%20text-clustering%20variants%20of%20k-means%20that%20periodically%20replace%20numeric%20centroids%20with%20textual%20summaries.%20The%20key%20idea%2C%20summary-as-centroid%2C%20retains%20k-means%20assignments%20in%20embedding%20space%20while%20producing%20human-readable%2C%20auditable%20cluster%20prototypes.%20The%20method%20is%20LLM-optional%3A%20k-NLPmeans%20uses%20lightweight%2C%20deterministic%20summarizers%2C%20enabling%20offline%2C%20low-cost%2C%20and%20stable%20operation%3B%20k-LLMmeans%20is%20a%20drop-in%20upgrade%20that%20uses%20an%20LLM%20for%20summaries%20under%20a%20fixed%20per-iteration%20budget%20whose%20cost%20does%20not%20grow%20with%20dataset%20size.%20We%20also%20present%20a%20mini-batch%20extension%20for%20real-time%20clustering%20of%20streaming%20text.%20Across%20diverse%20datasets%2C%20embedding%20models%2C%20and%20summarization%20strategies%2C%20our%20approach%20consistently%20outperforms%20classical%20baselines%20and%20approaches%20the%20accuracy%20of%20recent%20LLM-based%20clustering-without%20extensive%20LLM%20calls.%20Finally%2C%20we%20provide%20a%20case%20study%20on%20sequential%20text%20streams%20and%20release%20a%20StackExchange-derived%20benchmark%20for%20evaluating%20streaming%20text%20clustering.%0ALink%3A%20http%3A//arxiv.org/abs/2502.09667v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSummaries%2520as%2520Centroids%2520for%2520Interpretable%2520and%2520Scalable%2520Text%2520Clustering%26entry.906535625%3DJairo%2520Diaz-Rodriguez%26entry.1292438233%3DWe%2520introduce%2520k-NLPmeans%2520and%2520k-LLMmeans%252C%2520text-clustering%2520variants%2520of%2520k-means%2520that%2520periodically%2520replace%2520numeric%2520centroids%2520with%2520textual%2520summaries.%2520The%2520key%2520idea%252C%2520summary-as-centroid%252C%2520retains%2520k-means%2520assignments%2520in%2520embedding%2520space%2520while%2520producing%2520human-readable%252C%2520auditable%2520cluster%2520prototypes.%2520The%2520method%2520is%2520LLM-optional%253A%2520k-NLPmeans%2520uses%2520lightweight%252C%2520deterministic%2520summarizers%252C%2520enabling%2520offline%252C%2520low-cost%252C%2520and%2520stable%2520operation%253B%2520k-LLMmeans%2520is%2520a%2520drop-in%2520upgrade%2520that%2520uses%2520an%2520LLM%2520for%2520summaries%2520under%2520a%2520fixed%2520per-iteration%2520budget%2520whose%2520cost%2520does%2520not%2520grow%2520with%2520dataset%2520size.%2520We%2520also%2520present%2520a%2520mini-batch%2520extension%2520for%2520real-time%2520clustering%2520of%2520streaming%2520text.%2520Across%2520diverse%2520datasets%252C%2520embedding%2520models%252C%2520and%2520summarization%2520strategies%252C%2520our%2520approach%2520consistently%2520outperforms%2520classical%2520baselines%2520and%2520approaches%2520the%2520accuracy%2520of%2520recent%2520LLM-based%2520clustering-without%2520extensive%2520LLM%2520calls.%2520Finally%252C%2520we%2520provide%2520a%2520case%2520study%2520on%2520sequential%2520text%2520streams%2520and%2520release%2520a%2520StackExchange-derived%2520benchmark%2520for%2520evaluating%2520streaming%2520text%2520clustering.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.09667v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Summaries%20as%20Centroids%20for%20Interpretable%20and%20Scalable%20Text%20Clustering&entry.906535625=Jairo%20Diaz-Rodriguez&entry.1292438233=We%20introduce%20k-NLPmeans%20and%20k-LLMmeans%2C%20text-clustering%20variants%20of%20k-means%20that%20periodically%20replace%20numeric%20centroids%20with%20textual%20summaries.%20The%20key%20idea%2C%20summary-as-centroid%2C%20retains%20k-means%20assignments%20in%20embedding%20space%20while%20producing%20human-readable%2C%20auditable%20cluster%20prototypes.%20The%20method%20is%20LLM-optional%3A%20k-NLPmeans%20uses%20lightweight%2C%20deterministic%20summarizers%2C%20enabling%20offline%2C%20low-cost%2C%20and%20stable%20operation%3B%20k-LLMmeans%20is%20a%20drop-in%20upgrade%20that%20uses%20an%20LLM%20for%20summaries%20under%20a%20fixed%20per-iteration%20budget%20whose%20cost%20does%20not%20grow%20with%20dataset%20size.%20We%20also%20present%20a%20mini-batch%20extension%20for%20real-time%20clustering%20of%20streaming%20text.%20Across%20diverse%20datasets%2C%20embedding%20models%2C%20and%20summarization%20strategies%2C%20our%20approach%20consistently%20outperforms%20classical%20baselines%20and%20approaches%20the%20accuracy%20of%20recent%20LLM-based%20clustering-without%20extensive%20LLM%20calls.%20Finally%2C%20we%20provide%20a%20case%20study%20on%20sequential%20text%20streams%20and%20release%20a%20StackExchange-derived%20benchmark%20for%20evaluating%20streaming%20text%20clustering.&entry.1838667208=http%3A//arxiv.org/abs/2502.09667v4&entry.124074799=Read"},
{"title": "Semantic Depth Matters: Explaining Errors of Deep Vision Networks through Perceived Class Similarities", "author": "Katarzyna Filus and Micha\u0142 Romaszewski and Mateusz \u017barski", "abstract": "Understanding deep neural network (DNN) behavior requires more than evaluating classification accuracy alone; analyzing errors and their predictability is equally crucial. Current evaluation methodologies lack transparency, particularly in explaining the underlying causes of network misclassifications. To address this, we introduce a novel framework that investigates the relationship between the semantic hierarchy depth perceived by a network and its real-data misclassification patterns. Central to our framework is the Similarity Depth (SD) metric, which quantifies the semantic hierarchy depth perceived by a network along with a method of evaluation of how closely the network's errors align with its internally perceived similarity structure. We also propose a graph-based visualization of model semantic relationships and misperceptions. A key advantage of our approach is that leveraging class templates -- representations derived from classifier layer weights -- is applicable to already trained networks without requiring additional data or experiments. Our approach reveals that deep vision networks encode specific semantic hierarchies and that high semantic depth improves the compliance between perceived class similarities and actual errors.", "link": "http://arxiv.org/abs/2504.09956v2", "date": "2026-01-28", "relevancy": 2.2054, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5643}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5488}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20Depth%20Matters%3A%20Explaining%20Errors%20of%20Deep%20Vision%20Networks%20through%20Perceived%20Class%20Similarities&body=Title%3A%20Semantic%20Depth%20Matters%3A%20Explaining%20Errors%20of%20Deep%20Vision%20Networks%20through%20Perceived%20Class%20Similarities%0AAuthor%3A%20Katarzyna%20Filus%20and%20Micha%C5%82%20Romaszewski%20and%20Mateusz%20%C5%BBarski%0AAbstract%3A%20Understanding%20deep%20neural%20network%20%28DNN%29%20behavior%20requires%20more%20than%20evaluating%20classification%20accuracy%20alone%3B%20analyzing%20errors%20and%20their%20predictability%20is%20equally%20crucial.%20Current%20evaluation%20methodologies%20lack%20transparency%2C%20particularly%20in%20explaining%20the%20underlying%20causes%20of%20network%20misclassifications.%20To%20address%20this%2C%20we%20introduce%20a%20novel%20framework%20that%20investigates%20the%20relationship%20between%20the%20semantic%20hierarchy%20depth%20perceived%20by%20a%20network%20and%20its%20real-data%20misclassification%20patterns.%20Central%20to%20our%20framework%20is%20the%20Similarity%20Depth%20%28SD%29%20metric%2C%20which%20quantifies%20the%20semantic%20hierarchy%20depth%20perceived%20by%20a%20network%20along%20with%20a%20method%20of%20evaluation%20of%20how%20closely%20the%20network%27s%20errors%20align%20with%20its%20internally%20perceived%20similarity%20structure.%20We%20also%20propose%20a%20graph-based%20visualization%20of%20model%20semantic%20relationships%20and%20misperceptions.%20A%20key%20advantage%20of%20our%20approach%20is%20that%20leveraging%20class%20templates%20--%20representations%20derived%20from%20classifier%20layer%20weights%20--%20is%20applicable%20to%20already%20trained%20networks%20without%20requiring%20additional%20data%20or%20experiments.%20Our%20approach%20reveals%20that%20deep%20vision%20networks%20encode%20specific%20semantic%20hierarchies%20and%20that%20high%20semantic%20depth%20improves%20the%20compliance%20between%20perceived%20class%20similarities%20and%20actual%20errors.%0ALink%3A%20http%3A//arxiv.org/abs/2504.09956v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520Depth%2520Matters%253A%2520Explaining%2520Errors%2520of%2520Deep%2520Vision%2520Networks%2520through%2520Perceived%2520Class%2520Similarities%26entry.906535625%3DKatarzyna%2520Filus%2520and%2520Micha%25C5%2582%2520Romaszewski%2520and%2520Mateusz%2520%25C5%25BBarski%26entry.1292438233%3DUnderstanding%2520deep%2520neural%2520network%2520%2528DNN%2529%2520behavior%2520requires%2520more%2520than%2520evaluating%2520classification%2520accuracy%2520alone%253B%2520analyzing%2520errors%2520and%2520their%2520predictability%2520is%2520equally%2520crucial.%2520Current%2520evaluation%2520methodologies%2520lack%2520transparency%252C%2520particularly%2520in%2520explaining%2520the%2520underlying%2520causes%2520of%2520network%2520misclassifications.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520novel%2520framework%2520that%2520investigates%2520the%2520relationship%2520between%2520the%2520semantic%2520hierarchy%2520depth%2520perceived%2520by%2520a%2520network%2520and%2520its%2520real-data%2520misclassification%2520patterns.%2520Central%2520to%2520our%2520framework%2520is%2520the%2520Similarity%2520Depth%2520%2528SD%2529%2520metric%252C%2520which%2520quantifies%2520the%2520semantic%2520hierarchy%2520depth%2520perceived%2520by%2520a%2520network%2520along%2520with%2520a%2520method%2520of%2520evaluation%2520of%2520how%2520closely%2520the%2520network%2527s%2520errors%2520align%2520with%2520its%2520internally%2520perceived%2520similarity%2520structure.%2520We%2520also%2520propose%2520a%2520graph-based%2520visualization%2520of%2520model%2520semantic%2520relationships%2520and%2520misperceptions.%2520A%2520key%2520advantage%2520of%2520our%2520approach%2520is%2520that%2520leveraging%2520class%2520templates%2520--%2520representations%2520derived%2520from%2520classifier%2520layer%2520weights%2520--%2520is%2520applicable%2520to%2520already%2520trained%2520networks%2520without%2520requiring%2520additional%2520data%2520or%2520experiments.%2520Our%2520approach%2520reveals%2520that%2520deep%2520vision%2520networks%2520encode%2520specific%2520semantic%2520hierarchies%2520and%2520that%2520high%2520semantic%2520depth%2520improves%2520the%2520compliance%2520between%2520perceived%2520class%2520similarities%2520and%2520actual%2520errors.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.09956v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Depth%20Matters%3A%20Explaining%20Errors%20of%20Deep%20Vision%20Networks%20through%20Perceived%20Class%20Similarities&entry.906535625=Katarzyna%20Filus%20and%20Micha%C5%82%20Romaszewski%20and%20Mateusz%20%C5%BBarski&entry.1292438233=Understanding%20deep%20neural%20network%20%28DNN%29%20behavior%20requires%20more%20than%20evaluating%20classification%20accuracy%20alone%3B%20analyzing%20errors%20and%20their%20predictability%20is%20equally%20crucial.%20Current%20evaluation%20methodologies%20lack%20transparency%2C%20particularly%20in%20explaining%20the%20underlying%20causes%20of%20network%20misclassifications.%20To%20address%20this%2C%20we%20introduce%20a%20novel%20framework%20that%20investigates%20the%20relationship%20between%20the%20semantic%20hierarchy%20depth%20perceived%20by%20a%20network%20and%20its%20real-data%20misclassification%20patterns.%20Central%20to%20our%20framework%20is%20the%20Similarity%20Depth%20%28SD%29%20metric%2C%20which%20quantifies%20the%20semantic%20hierarchy%20depth%20perceived%20by%20a%20network%20along%20with%20a%20method%20of%20evaluation%20of%20how%20closely%20the%20network%27s%20errors%20align%20with%20its%20internally%20perceived%20similarity%20structure.%20We%20also%20propose%20a%20graph-based%20visualization%20of%20model%20semantic%20relationships%20and%20misperceptions.%20A%20key%20advantage%20of%20our%20approach%20is%20that%20leveraging%20class%20templates%20--%20representations%20derived%20from%20classifier%20layer%20weights%20--%20is%20applicable%20to%20already%20trained%20networks%20without%20requiring%20additional%20data%20or%20experiments.%20Our%20approach%20reveals%20that%20deep%20vision%20networks%20encode%20specific%20semantic%20hierarchies%20and%20that%20high%20semantic%20depth%20improves%20the%20compliance%20between%20perceived%20class%20similarities%20and%20actual%20errors.&entry.1838667208=http%3A//arxiv.org/abs/2504.09956v2&entry.124074799=Read"},
{"title": "Vibro-Sense: Robust Vibration-based Impulse Response Localization and Trajectory Tracking for Robotic Hands", "author": "Wadhah Zai El Amri and Nicol\u00e1s Navarro-Guerrero", "abstract": "Rich contact perception is crucial for robotic manipulation, yet traditional tactile skins remain expensive and complex to integrate. This paper presents a scalable alternative: high-accuracy whole-body touch localization via vibro-acoustic sensing. By equipping a robotic hand with seven low-cost piezoelectric microphones and leveraging an Audio Spectrogram Transformer, we decode the vibrational signatures generated during physical interaction. Extensive evaluation across stationary and dynamic tasks reveals a localization error of under 5 mm in static conditions. Furthermore, our analysis highlights the distinct influence of material properties: stiff materials (e.g., metal) excel in impulse response localization due to sharp, high-bandwidth responses, whereas textured materials (e.g., wood) provide superior friction-based features for trajectory tracking. The system demonstrates robustness to the robot's own motion, maintaining effective tracking even during active operation. Our primary contribution is demonstrating that complex physical contact dynamics can be effectively decoded from simple vibrational signals, offering a viable pathway to widespread, affordable contact perception in robotics. To accelerate research, we provide our full datasets, models, and experimental setups as open-source resources.", "link": "http://arxiv.org/abs/2601.20555v1", "date": "2026-01-28", "relevancy": 2.2042, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5934}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5817}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5035}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vibro-Sense%3A%20Robust%20Vibration-based%20Impulse%20Response%20Localization%20and%20Trajectory%20Tracking%20for%20Robotic%20Hands&body=Title%3A%20Vibro-Sense%3A%20Robust%20Vibration-based%20Impulse%20Response%20Localization%20and%20Trajectory%20Tracking%20for%20Robotic%20Hands%0AAuthor%3A%20Wadhah%20Zai%20El%20Amri%20and%20Nicol%C3%A1s%20Navarro-Guerrero%0AAbstract%3A%20Rich%20contact%20perception%20is%20crucial%20for%20robotic%20manipulation%2C%20yet%20traditional%20tactile%20skins%20remain%20expensive%20and%20complex%20to%20integrate.%20This%20paper%20presents%20a%20scalable%20alternative%3A%20high-accuracy%20whole-body%20touch%20localization%20via%20vibro-acoustic%20sensing.%20By%20equipping%20a%20robotic%20hand%20with%20seven%20low-cost%20piezoelectric%20microphones%20and%20leveraging%20an%20Audio%20Spectrogram%20Transformer%2C%20we%20decode%20the%20vibrational%20signatures%20generated%20during%20physical%20interaction.%20Extensive%20evaluation%20across%20stationary%20and%20dynamic%20tasks%20reveals%20a%20localization%20error%20of%20under%205%20mm%20in%20static%20conditions.%20Furthermore%2C%20our%20analysis%20highlights%20the%20distinct%20influence%20of%20material%20properties%3A%20stiff%20materials%20%28e.g.%2C%20metal%29%20excel%20in%20impulse%20response%20localization%20due%20to%20sharp%2C%20high-bandwidth%20responses%2C%20whereas%20textured%20materials%20%28e.g.%2C%20wood%29%20provide%20superior%20friction-based%20features%20for%20trajectory%20tracking.%20The%20system%20demonstrates%20robustness%20to%20the%20robot%27s%20own%20motion%2C%20maintaining%20effective%20tracking%20even%20during%20active%20operation.%20Our%20primary%20contribution%20is%20demonstrating%20that%20complex%20physical%20contact%20dynamics%20can%20be%20effectively%20decoded%20from%20simple%20vibrational%20signals%2C%20offering%20a%20viable%20pathway%20to%20widespread%2C%20affordable%20contact%20perception%20in%20robotics.%20To%20accelerate%20research%2C%20we%20provide%20our%20full%20datasets%2C%20models%2C%20and%20experimental%20setups%20as%20open-source%20resources.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20555v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVibro-Sense%253A%2520Robust%2520Vibration-based%2520Impulse%2520Response%2520Localization%2520and%2520Trajectory%2520Tracking%2520for%2520Robotic%2520Hands%26entry.906535625%3DWadhah%2520Zai%2520El%2520Amri%2520and%2520Nicol%25C3%25A1s%2520Navarro-Guerrero%26entry.1292438233%3DRich%2520contact%2520perception%2520is%2520crucial%2520for%2520robotic%2520manipulation%252C%2520yet%2520traditional%2520tactile%2520skins%2520remain%2520expensive%2520and%2520complex%2520to%2520integrate.%2520This%2520paper%2520presents%2520a%2520scalable%2520alternative%253A%2520high-accuracy%2520whole-body%2520touch%2520localization%2520via%2520vibro-acoustic%2520sensing.%2520By%2520equipping%2520a%2520robotic%2520hand%2520with%2520seven%2520low-cost%2520piezoelectric%2520microphones%2520and%2520leveraging%2520an%2520Audio%2520Spectrogram%2520Transformer%252C%2520we%2520decode%2520the%2520vibrational%2520signatures%2520generated%2520during%2520physical%2520interaction.%2520Extensive%2520evaluation%2520across%2520stationary%2520and%2520dynamic%2520tasks%2520reveals%2520a%2520localization%2520error%2520of%2520under%25205%2520mm%2520in%2520static%2520conditions.%2520Furthermore%252C%2520our%2520analysis%2520highlights%2520the%2520distinct%2520influence%2520of%2520material%2520properties%253A%2520stiff%2520materials%2520%2528e.g.%252C%2520metal%2529%2520excel%2520in%2520impulse%2520response%2520localization%2520due%2520to%2520sharp%252C%2520high-bandwidth%2520responses%252C%2520whereas%2520textured%2520materials%2520%2528e.g.%252C%2520wood%2529%2520provide%2520superior%2520friction-based%2520features%2520for%2520trajectory%2520tracking.%2520The%2520system%2520demonstrates%2520robustness%2520to%2520the%2520robot%2527s%2520own%2520motion%252C%2520maintaining%2520effective%2520tracking%2520even%2520during%2520active%2520operation.%2520Our%2520primary%2520contribution%2520is%2520demonstrating%2520that%2520complex%2520physical%2520contact%2520dynamics%2520can%2520be%2520effectively%2520decoded%2520from%2520simple%2520vibrational%2520signals%252C%2520offering%2520a%2520viable%2520pathway%2520to%2520widespread%252C%2520affordable%2520contact%2520perception%2520in%2520robotics.%2520To%2520accelerate%2520research%252C%2520we%2520provide%2520our%2520full%2520datasets%252C%2520models%252C%2520and%2520experimental%2520setups%2520as%2520open-source%2520resources.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20555v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vibro-Sense%3A%20Robust%20Vibration-based%20Impulse%20Response%20Localization%20and%20Trajectory%20Tracking%20for%20Robotic%20Hands&entry.906535625=Wadhah%20Zai%20El%20Amri%20and%20Nicol%C3%A1s%20Navarro-Guerrero&entry.1292438233=Rich%20contact%20perception%20is%20crucial%20for%20robotic%20manipulation%2C%20yet%20traditional%20tactile%20skins%20remain%20expensive%20and%20complex%20to%20integrate.%20This%20paper%20presents%20a%20scalable%20alternative%3A%20high-accuracy%20whole-body%20touch%20localization%20via%20vibro-acoustic%20sensing.%20By%20equipping%20a%20robotic%20hand%20with%20seven%20low-cost%20piezoelectric%20microphones%20and%20leveraging%20an%20Audio%20Spectrogram%20Transformer%2C%20we%20decode%20the%20vibrational%20signatures%20generated%20during%20physical%20interaction.%20Extensive%20evaluation%20across%20stationary%20and%20dynamic%20tasks%20reveals%20a%20localization%20error%20of%20under%205%20mm%20in%20static%20conditions.%20Furthermore%2C%20our%20analysis%20highlights%20the%20distinct%20influence%20of%20material%20properties%3A%20stiff%20materials%20%28e.g.%2C%20metal%29%20excel%20in%20impulse%20response%20localization%20due%20to%20sharp%2C%20high-bandwidth%20responses%2C%20whereas%20textured%20materials%20%28e.g.%2C%20wood%29%20provide%20superior%20friction-based%20features%20for%20trajectory%20tracking.%20The%20system%20demonstrates%20robustness%20to%20the%20robot%27s%20own%20motion%2C%20maintaining%20effective%20tracking%20even%20during%20active%20operation.%20Our%20primary%20contribution%20is%20demonstrating%20that%20complex%20physical%20contact%20dynamics%20can%20be%20effectively%20decoded%20from%20simple%20vibrational%20signals%2C%20offering%20a%20viable%20pathway%20to%20widespread%2C%20affordable%20contact%20perception%20in%20robotics.%20To%20accelerate%20research%2C%20we%20provide%20our%20full%20datasets%2C%20models%2C%20and%20experimental%20setups%20as%20open-source%20resources.&entry.1838667208=http%3A//arxiv.org/abs/2601.20555v1&entry.124074799=Read"},
{"title": "bi-modal textual prompt learning for vision-language models in remote sensing", "author": "Pankhi Kashyap and Mainak Singha and Biplab Banerjee", "abstract": "Prompt learning (PL) has emerged as an effective strategy to adapt vision-language models (VLMs), such as CLIP, for downstream tasks under limited supervision. While PL has demonstrated strong generalization on natural image datasets, its transferability to remote sensing (RS) imagery remains underexplored. RS data present unique challenges, including multi-label scenes, high intra-class variability, and diverse spatial resolutions, that hinder the direct applicability of existing PL methods. In particular, current prompt-based approaches often struggle to identify dominant semantic cues and fail to generalize to novel classes in RS scenarios. To address these challenges, we propose BiMoRS, a lightweight bi-modal prompt learning framework tailored for RS tasks. BiMoRS employs a frozen image captioning model (e.g., BLIP-2) to extract textual semantic summaries from RS images. These captions are tokenized using a BERT tokenizer and fused with high-level visual features from the CLIP encoder. A lightweight cross-attention module then conditions a learnable query prompt on the fused textual-visual representation, yielding contextualized prompts without altering the CLIP backbone. We evaluate BiMoRS on four RS datasets across three domain generalization (DG) tasks and observe consistent performance gains, outperforming strong baselines by up to 2% on average. Codes are available at https://github.com/ipankhi/BiMoRS.", "link": "http://arxiv.org/abs/2601.20675v1", "date": "2026-01-28", "relevancy": 2.1982, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5628}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.557}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20bi-modal%20textual%20prompt%20learning%20for%20vision-language%20models%20in%20remote%20sensing&body=Title%3A%20bi-modal%20textual%20prompt%20learning%20for%20vision-language%20models%20in%20remote%20sensing%0AAuthor%3A%20Pankhi%20Kashyap%20and%20Mainak%20Singha%20and%20Biplab%20Banerjee%0AAbstract%3A%20Prompt%20learning%20%28PL%29%20has%20emerged%20as%20an%20effective%20strategy%20to%20adapt%20vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20for%20downstream%20tasks%20under%20limited%20supervision.%20While%20PL%20has%20demonstrated%20strong%20generalization%20on%20natural%20image%20datasets%2C%20its%20transferability%20to%20remote%20sensing%20%28RS%29%20imagery%20remains%20underexplored.%20RS%20data%20present%20unique%20challenges%2C%20including%20multi-label%20scenes%2C%20high%20intra-class%20variability%2C%20and%20diverse%20spatial%20resolutions%2C%20that%20hinder%20the%20direct%20applicability%20of%20existing%20PL%20methods.%20In%20particular%2C%20current%20prompt-based%20approaches%20often%20struggle%20to%20identify%20dominant%20semantic%20cues%20and%20fail%20to%20generalize%20to%20novel%20classes%20in%20RS%20scenarios.%20To%20address%20these%20challenges%2C%20we%20propose%20BiMoRS%2C%20a%20lightweight%20bi-modal%20prompt%20learning%20framework%20tailored%20for%20RS%20tasks.%20BiMoRS%20employs%20a%20frozen%20image%20captioning%20model%20%28e.g.%2C%20BLIP-2%29%20to%20extract%20textual%20semantic%20summaries%20from%20RS%20images.%20These%20captions%20are%20tokenized%20using%20a%20BERT%20tokenizer%20and%20fused%20with%20high-level%20visual%20features%20from%20the%20CLIP%20encoder.%20A%20lightweight%20cross-attention%20module%20then%20conditions%20a%20learnable%20query%20prompt%20on%20the%20fused%20textual-visual%20representation%2C%20yielding%20contextualized%20prompts%20without%20altering%20the%20CLIP%20backbone.%20We%20evaluate%20BiMoRS%20on%20four%20RS%20datasets%20across%20three%20domain%20generalization%20%28DG%29%20tasks%20and%20observe%20consistent%20performance%20gains%2C%20outperforming%20strong%20baselines%20by%20up%20to%202%25%20on%20average.%20Codes%20are%20available%20at%20https%3A//github.com/ipankhi/BiMoRS.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20675v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dbi-modal%2520textual%2520prompt%2520learning%2520for%2520vision-language%2520models%2520in%2520remote%2520sensing%26entry.906535625%3DPankhi%2520Kashyap%2520and%2520Mainak%2520Singha%2520and%2520Biplab%2520Banerjee%26entry.1292438233%3DPrompt%2520learning%2520%2528PL%2529%2520has%2520emerged%2520as%2520an%2520effective%2520strategy%2520to%2520adapt%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520such%2520as%2520CLIP%252C%2520for%2520downstream%2520tasks%2520under%2520limited%2520supervision.%2520While%2520PL%2520has%2520demonstrated%2520strong%2520generalization%2520on%2520natural%2520image%2520datasets%252C%2520its%2520transferability%2520to%2520remote%2520sensing%2520%2528RS%2529%2520imagery%2520remains%2520underexplored.%2520RS%2520data%2520present%2520unique%2520challenges%252C%2520including%2520multi-label%2520scenes%252C%2520high%2520intra-class%2520variability%252C%2520and%2520diverse%2520spatial%2520resolutions%252C%2520that%2520hinder%2520the%2520direct%2520applicability%2520of%2520existing%2520PL%2520methods.%2520In%2520particular%252C%2520current%2520prompt-based%2520approaches%2520often%2520struggle%2520to%2520identify%2520dominant%2520semantic%2520cues%2520and%2520fail%2520to%2520generalize%2520to%2520novel%2520classes%2520in%2520RS%2520scenarios.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520BiMoRS%252C%2520a%2520lightweight%2520bi-modal%2520prompt%2520learning%2520framework%2520tailored%2520for%2520RS%2520tasks.%2520BiMoRS%2520employs%2520a%2520frozen%2520image%2520captioning%2520model%2520%2528e.g.%252C%2520BLIP-2%2529%2520to%2520extract%2520textual%2520semantic%2520summaries%2520from%2520RS%2520images.%2520These%2520captions%2520are%2520tokenized%2520using%2520a%2520BERT%2520tokenizer%2520and%2520fused%2520with%2520high-level%2520visual%2520features%2520from%2520the%2520CLIP%2520encoder.%2520A%2520lightweight%2520cross-attention%2520module%2520then%2520conditions%2520a%2520learnable%2520query%2520prompt%2520on%2520the%2520fused%2520textual-visual%2520representation%252C%2520yielding%2520contextualized%2520prompts%2520without%2520altering%2520the%2520CLIP%2520backbone.%2520We%2520evaluate%2520BiMoRS%2520on%2520four%2520RS%2520datasets%2520across%2520three%2520domain%2520generalization%2520%2528DG%2529%2520tasks%2520and%2520observe%2520consistent%2520performance%2520gains%252C%2520outperforming%2520strong%2520baselines%2520by%2520up%2520to%25202%2525%2520on%2520average.%2520Codes%2520are%2520available%2520at%2520https%253A//github.com/ipankhi/BiMoRS.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20675v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=bi-modal%20textual%20prompt%20learning%20for%20vision-language%20models%20in%20remote%20sensing&entry.906535625=Pankhi%20Kashyap%20and%20Mainak%20Singha%20and%20Biplab%20Banerjee&entry.1292438233=Prompt%20learning%20%28PL%29%20has%20emerged%20as%20an%20effective%20strategy%20to%20adapt%20vision-language%20models%20%28VLMs%29%2C%20such%20as%20CLIP%2C%20for%20downstream%20tasks%20under%20limited%20supervision.%20While%20PL%20has%20demonstrated%20strong%20generalization%20on%20natural%20image%20datasets%2C%20its%20transferability%20to%20remote%20sensing%20%28RS%29%20imagery%20remains%20underexplored.%20RS%20data%20present%20unique%20challenges%2C%20including%20multi-label%20scenes%2C%20high%20intra-class%20variability%2C%20and%20diverse%20spatial%20resolutions%2C%20that%20hinder%20the%20direct%20applicability%20of%20existing%20PL%20methods.%20In%20particular%2C%20current%20prompt-based%20approaches%20often%20struggle%20to%20identify%20dominant%20semantic%20cues%20and%20fail%20to%20generalize%20to%20novel%20classes%20in%20RS%20scenarios.%20To%20address%20these%20challenges%2C%20we%20propose%20BiMoRS%2C%20a%20lightweight%20bi-modal%20prompt%20learning%20framework%20tailored%20for%20RS%20tasks.%20BiMoRS%20employs%20a%20frozen%20image%20captioning%20model%20%28e.g.%2C%20BLIP-2%29%20to%20extract%20textual%20semantic%20summaries%20from%20RS%20images.%20These%20captions%20are%20tokenized%20using%20a%20BERT%20tokenizer%20and%20fused%20with%20high-level%20visual%20features%20from%20the%20CLIP%20encoder.%20A%20lightweight%20cross-attention%20module%20then%20conditions%20a%20learnable%20query%20prompt%20on%20the%20fused%20textual-visual%20representation%2C%20yielding%20contextualized%20prompts%20without%20altering%20the%20CLIP%20backbone.%20We%20evaluate%20BiMoRS%20on%20four%20RS%20datasets%20across%20three%20domain%20generalization%20%28DG%29%20tasks%20and%20observe%20consistent%20performance%20gains%2C%20outperforming%20strong%20baselines%20by%20up%20to%202%25%20on%20average.%20Codes%20are%20available%20at%20https%3A//github.com/ipankhi/BiMoRS.&entry.1838667208=http%3A//arxiv.org/abs/2601.20675v1&entry.124074799=Read"},
{"title": "Membership Privacy Risks of Sharpness Aware Minimization", "author": "Young In Kim and Andrea Agiollo and Pratiksha Agrawal and Johannes O. Royset and Rajiv Khanna", "abstract": "Optimization algorithms that seek flatter minima, such as Sharpness-Aware Minimization (SAM), are credited with improved generalization and robustness to noise. We ask whether such gains impact membership privacy. Surprisingly, we find that SAM is more prone to Membership Inference Attacks (MIA) than classical SGD across multiple datasets and attack methods, despite achieving lower test error. This suggests that the geometric mechanism of SAM that improves generalization simultaneously exacerbates membership leakage. We investigate this phenomenon through extensive analysis of memorization and influence scores. Our results reveal that SAM is more capable of capturing atypical subpatterns, leading to higher memorization scores of samples. Conversely, SGD depends more heavily on majority features, exhibiting worse generalization on atypical subgroups and lower memorization. Crucially, this characteristic of SAM can be linked to lower variance in the prediction confidence of unseen samples, thereby amplifying membership signals. Finally, we model SAM under a perfectly interpolating linear regime and theoretically show that sharpness regularization inherently reduces variance, guaranteeing a higher MIA advantage for confidence and likelihood ratio attacks.", "link": "http://arxiv.org/abs/2310.00488v4", "date": "2026-01-28", "relevancy": 2.1779, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4606}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4313}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Membership%20Privacy%20Risks%20of%20Sharpness%20Aware%20Minimization&body=Title%3A%20Membership%20Privacy%20Risks%20of%20Sharpness%20Aware%20Minimization%0AAuthor%3A%20Young%20In%20Kim%20and%20Andrea%20Agiollo%20and%20Pratiksha%20Agrawal%20and%20Johannes%20O.%20Royset%20and%20Rajiv%20Khanna%0AAbstract%3A%20Optimization%20algorithms%20that%20seek%20flatter%20minima%2C%20such%20as%20Sharpness-Aware%20Minimization%20%28SAM%29%2C%20are%20credited%20with%20improved%20generalization%20and%20robustness%20to%20noise.%20We%20ask%20whether%20such%20gains%20impact%20membership%20privacy.%20Surprisingly%2C%20we%20find%20that%20SAM%20is%20more%20prone%20to%20Membership%20Inference%20Attacks%20%28MIA%29%20than%20classical%20SGD%20across%20multiple%20datasets%20and%20attack%20methods%2C%20despite%20achieving%20lower%20test%20error.%20This%20suggests%20that%20the%20geometric%20mechanism%20of%20SAM%20that%20improves%20generalization%20simultaneously%20exacerbates%20membership%20leakage.%20We%20investigate%20this%20phenomenon%20through%20extensive%20analysis%20of%20memorization%20and%20influence%20scores.%20Our%20results%20reveal%20that%20SAM%20is%20more%20capable%20of%20capturing%20atypical%20subpatterns%2C%20leading%20to%20higher%20memorization%20scores%20of%20samples.%20Conversely%2C%20SGD%20depends%20more%20heavily%20on%20majority%20features%2C%20exhibiting%20worse%20generalization%20on%20atypical%20subgroups%20and%20lower%20memorization.%20Crucially%2C%20this%20characteristic%20of%20SAM%20can%20be%20linked%20to%20lower%20variance%20in%20the%20prediction%20confidence%20of%20unseen%20samples%2C%20thereby%20amplifying%20membership%20signals.%20Finally%2C%20we%20model%20SAM%20under%20a%20perfectly%20interpolating%20linear%20regime%20and%20theoretically%20show%20that%20sharpness%20regularization%20inherently%20reduces%20variance%2C%20guaranteeing%20a%20higher%20MIA%20advantage%20for%20confidence%20and%20likelihood%20ratio%20attacks.%0ALink%3A%20http%3A//arxiv.org/abs/2310.00488v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMembership%2520Privacy%2520Risks%2520of%2520Sharpness%2520Aware%2520Minimization%26entry.906535625%3DYoung%2520In%2520Kim%2520and%2520Andrea%2520Agiollo%2520and%2520Pratiksha%2520Agrawal%2520and%2520Johannes%2520O.%2520Royset%2520and%2520Rajiv%2520Khanna%26entry.1292438233%3DOptimization%2520algorithms%2520that%2520seek%2520flatter%2520minima%252C%2520such%2520as%2520Sharpness-Aware%2520Minimization%2520%2528SAM%2529%252C%2520are%2520credited%2520with%2520improved%2520generalization%2520and%2520robustness%2520to%2520noise.%2520We%2520ask%2520whether%2520such%2520gains%2520impact%2520membership%2520privacy.%2520Surprisingly%252C%2520we%2520find%2520that%2520SAM%2520is%2520more%2520prone%2520to%2520Membership%2520Inference%2520Attacks%2520%2528MIA%2529%2520than%2520classical%2520SGD%2520across%2520multiple%2520datasets%2520and%2520attack%2520methods%252C%2520despite%2520achieving%2520lower%2520test%2520error.%2520This%2520suggests%2520that%2520the%2520geometric%2520mechanism%2520of%2520SAM%2520that%2520improves%2520generalization%2520simultaneously%2520exacerbates%2520membership%2520leakage.%2520We%2520investigate%2520this%2520phenomenon%2520through%2520extensive%2520analysis%2520of%2520memorization%2520and%2520influence%2520scores.%2520Our%2520results%2520reveal%2520that%2520SAM%2520is%2520more%2520capable%2520of%2520capturing%2520atypical%2520subpatterns%252C%2520leading%2520to%2520higher%2520memorization%2520scores%2520of%2520samples.%2520Conversely%252C%2520SGD%2520depends%2520more%2520heavily%2520on%2520majority%2520features%252C%2520exhibiting%2520worse%2520generalization%2520on%2520atypical%2520subgroups%2520and%2520lower%2520memorization.%2520Crucially%252C%2520this%2520characteristic%2520of%2520SAM%2520can%2520be%2520linked%2520to%2520lower%2520variance%2520in%2520the%2520prediction%2520confidence%2520of%2520unseen%2520samples%252C%2520thereby%2520amplifying%2520membership%2520signals.%2520Finally%252C%2520we%2520model%2520SAM%2520under%2520a%2520perfectly%2520interpolating%2520linear%2520regime%2520and%2520theoretically%2520show%2520that%2520sharpness%2520regularization%2520inherently%2520reduces%2520variance%252C%2520guaranteeing%2520a%2520higher%2520MIA%2520advantage%2520for%2520confidence%2520and%2520likelihood%2520ratio%2520attacks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.00488v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Membership%20Privacy%20Risks%20of%20Sharpness%20Aware%20Minimization&entry.906535625=Young%20In%20Kim%20and%20Andrea%20Agiollo%20and%20Pratiksha%20Agrawal%20and%20Johannes%20O.%20Royset%20and%20Rajiv%20Khanna&entry.1292438233=Optimization%20algorithms%20that%20seek%20flatter%20minima%2C%20such%20as%20Sharpness-Aware%20Minimization%20%28SAM%29%2C%20are%20credited%20with%20improved%20generalization%20and%20robustness%20to%20noise.%20We%20ask%20whether%20such%20gains%20impact%20membership%20privacy.%20Surprisingly%2C%20we%20find%20that%20SAM%20is%20more%20prone%20to%20Membership%20Inference%20Attacks%20%28MIA%29%20than%20classical%20SGD%20across%20multiple%20datasets%20and%20attack%20methods%2C%20despite%20achieving%20lower%20test%20error.%20This%20suggests%20that%20the%20geometric%20mechanism%20of%20SAM%20that%20improves%20generalization%20simultaneously%20exacerbates%20membership%20leakage.%20We%20investigate%20this%20phenomenon%20through%20extensive%20analysis%20of%20memorization%20and%20influence%20scores.%20Our%20results%20reveal%20that%20SAM%20is%20more%20capable%20of%20capturing%20atypical%20subpatterns%2C%20leading%20to%20higher%20memorization%20scores%20of%20samples.%20Conversely%2C%20SGD%20depends%20more%20heavily%20on%20majority%20features%2C%20exhibiting%20worse%20generalization%20on%20atypical%20subgroups%20and%20lower%20memorization.%20Crucially%2C%20this%20characteristic%20of%20SAM%20can%20be%20linked%20to%20lower%20variance%20in%20the%20prediction%20confidence%20of%20unseen%20samples%2C%20thereby%20amplifying%20membership%20signals.%20Finally%2C%20we%20model%20SAM%20under%20a%20perfectly%20interpolating%20linear%20regime%20and%20theoretically%20show%20that%20sharpness%20regularization%20inherently%20reduces%20variance%2C%20guaranteeing%20a%20higher%20MIA%20advantage%20for%20confidence%20and%20likelihood%20ratio%20attacks.&entry.1838667208=http%3A//arxiv.org/abs/2310.00488v4&entry.124074799=Read"},
{"title": "FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering", "author": "Chaodong Tong and Qi Zhang and Chen Li and Lei Jiang and Yanbing Liu", "abstract": "Faithfulness hallucinations in VQA occur when vision-language models produce fluent yet visually ungrounded answers, severely undermining their reliability in safety-critical applications. Existing detection methods mainly fall into two categories: external verification approaches relying on auxiliary models or knowledge bases, and uncertainty-driven approaches using repeated sampling or uncertainty estimates. The former suffer from high computational overhead and are limited by external resource quality, while the latter capture only limited facets of model uncertainty and fail to sufficiently explore the rich internal signals associated with the diverse failure modes. Both paradigms thus have inherent limitations in efficiency, robustness, and detection performance. To address these challenges, we propose FaithSCAN: a lightweight network that detects hallucinations by exploiting rich internal signals of VLMs, including token-level decoding uncertainty, intermediate visual representations, and cross-modal alignment features. These signals are fused via branch-wise evidence encoding and uncertainty-aware attention. We also extend the LLM-as-a-Judge paradigm to VQA hallucination and propose a low-cost strategy to automatically generate model-dependent supervision signals, enabling supervised training without costly human labels while maintaining high detection accuracy. Experiments on multiple VQA benchmarks show that FaithSCAN significantly outperforms existing methods in both effectiveness and efficiency. In-depth analysis shows hallucinations arise from systematic internal state variations in visual perception, cross-modal reasoning, and language decoding. Different internal signals provide complementary diagnostic cues, and hallucination patterns vary across VLM architectures, offering new insights into the underlying causes of multimodal hallucinations.", "link": "http://arxiv.org/abs/2601.00269v2", "date": "2026-01-28", "relevancy": 2.1763, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5598}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5354}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FaithSCAN%3A%20Model-Driven%20Single-Pass%20Hallucination%20Detection%20for%20Faithful%20Visual%20Question%20Answering&body=Title%3A%20FaithSCAN%3A%20Model-Driven%20Single-Pass%20Hallucination%20Detection%20for%20Faithful%20Visual%20Question%20Answering%0AAuthor%3A%20Chaodong%20Tong%20and%20Qi%20Zhang%20and%20Chen%20Li%20and%20Lei%20Jiang%20and%20Yanbing%20Liu%0AAbstract%3A%20Faithfulness%20hallucinations%20in%20VQA%20occur%20when%20vision-language%20models%20produce%20fluent%20yet%20visually%20ungrounded%20answers%2C%20severely%20undermining%20their%20reliability%20in%20safety-critical%20applications.%20Existing%20detection%20methods%20mainly%20fall%20into%20two%20categories%3A%20external%20verification%20approaches%20relying%20on%20auxiliary%20models%20or%20knowledge%20bases%2C%20and%20uncertainty-driven%20approaches%20using%20repeated%20sampling%20or%20uncertainty%20estimates.%20The%20former%20suffer%20from%20high%20computational%20overhead%20and%20are%20limited%20by%20external%20resource%20quality%2C%20while%20the%20latter%20capture%20only%20limited%20facets%20of%20model%20uncertainty%20and%20fail%20to%20sufficiently%20explore%20the%20rich%20internal%20signals%20associated%20with%20the%20diverse%20failure%20modes.%20Both%20paradigms%20thus%20have%20inherent%20limitations%20in%20efficiency%2C%20robustness%2C%20and%20detection%20performance.%20To%20address%20these%20challenges%2C%20we%20propose%20FaithSCAN%3A%20a%20lightweight%20network%20that%20detects%20hallucinations%20by%20exploiting%20rich%20internal%20signals%20of%20VLMs%2C%20including%20token-level%20decoding%20uncertainty%2C%20intermediate%20visual%20representations%2C%20and%20cross-modal%20alignment%20features.%20These%20signals%20are%20fused%20via%20branch-wise%20evidence%20encoding%20and%20uncertainty-aware%20attention.%20We%20also%20extend%20the%20LLM-as-a-Judge%20paradigm%20to%20VQA%20hallucination%20and%20propose%20a%20low-cost%20strategy%20to%20automatically%20generate%20model-dependent%20supervision%20signals%2C%20enabling%20supervised%20training%20without%20costly%20human%20labels%20while%20maintaining%20high%20detection%20accuracy.%20Experiments%20on%20multiple%20VQA%20benchmarks%20show%20that%20FaithSCAN%20significantly%20outperforms%20existing%20methods%20in%20both%20effectiveness%20and%20efficiency.%20In-depth%20analysis%20shows%20hallucinations%20arise%20from%20systematic%20internal%20state%20variations%20in%20visual%20perception%2C%20cross-modal%20reasoning%2C%20and%20language%20decoding.%20Different%20internal%20signals%20provide%20complementary%20diagnostic%20cues%2C%20and%20hallucination%20patterns%20vary%20across%20VLM%20architectures%2C%20offering%20new%20insights%20into%20the%20underlying%20causes%20of%20multimodal%20hallucinations.%0ALink%3A%20http%3A//arxiv.org/abs/2601.00269v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaithSCAN%253A%2520Model-Driven%2520Single-Pass%2520Hallucination%2520Detection%2520for%2520Faithful%2520Visual%2520Question%2520Answering%26entry.906535625%3DChaodong%2520Tong%2520and%2520Qi%2520Zhang%2520and%2520Chen%2520Li%2520and%2520Lei%2520Jiang%2520and%2520Yanbing%2520Liu%26entry.1292438233%3DFaithfulness%2520hallucinations%2520in%2520VQA%2520occur%2520when%2520vision-language%2520models%2520produce%2520fluent%2520yet%2520visually%2520ungrounded%2520answers%252C%2520severely%2520undermining%2520their%2520reliability%2520in%2520safety-critical%2520applications.%2520Existing%2520detection%2520methods%2520mainly%2520fall%2520into%2520two%2520categories%253A%2520external%2520verification%2520approaches%2520relying%2520on%2520auxiliary%2520models%2520or%2520knowledge%2520bases%252C%2520and%2520uncertainty-driven%2520approaches%2520using%2520repeated%2520sampling%2520or%2520uncertainty%2520estimates.%2520The%2520former%2520suffer%2520from%2520high%2520computational%2520overhead%2520and%2520are%2520limited%2520by%2520external%2520resource%2520quality%252C%2520while%2520the%2520latter%2520capture%2520only%2520limited%2520facets%2520of%2520model%2520uncertainty%2520and%2520fail%2520to%2520sufficiently%2520explore%2520the%2520rich%2520internal%2520signals%2520associated%2520with%2520the%2520diverse%2520failure%2520modes.%2520Both%2520paradigms%2520thus%2520have%2520inherent%2520limitations%2520in%2520efficiency%252C%2520robustness%252C%2520and%2520detection%2520performance.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520FaithSCAN%253A%2520a%2520lightweight%2520network%2520that%2520detects%2520hallucinations%2520by%2520exploiting%2520rich%2520internal%2520signals%2520of%2520VLMs%252C%2520including%2520token-level%2520decoding%2520uncertainty%252C%2520intermediate%2520visual%2520representations%252C%2520and%2520cross-modal%2520alignment%2520features.%2520These%2520signals%2520are%2520fused%2520via%2520branch-wise%2520evidence%2520encoding%2520and%2520uncertainty-aware%2520attention.%2520We%2520also%2520extend%2520the%2520LLM-as-a-Judge%2520paradigm%2520to%2520VQA%2520hallucination%2520and%2520propose%2520a%2520low-cost%2520strategy%2520to%2520automatically%2520generate%2520model-dependent%2520supervision%2520signals%252C%2520enabling%2520supervised%2520training%2520without%2520costly%2520human%2520labels%2520while%2520maintaining%2520high%2520detection%2520accuracy.%2520Experiments%2520on%2520multiple%2520VQA%2520benchmarks%2520show%2520that%2520FaithSCAN%2520significantly%2520outperforms%2520existing%2520methods%2520in%2520both%2520effectiveness%2520and%2520efficiency.%2520In-depth%2520analysis%2520shows%2520hallucinations%2520arise%2520from%2520systematic%2520internal%2520state%2520variations%2520in%2520visual%2520perception%252C%2520cross-modal%2520reasoning%252C%2520and%2520language%2520decoding.%2520Different%2520internal%2520signals%2520provide%2520complementary%2520diagnostic%2520cues%252C%2520and%2520hallucination%2520patterns%2520vary%2520across%2520VLM%2520architectures%252C%2520offering%2520new%2520insights%2520into%2520the%2520underlying%2520causes%2520of%2520multimodal%2520hallucinations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00269v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FaithSCAN%3A%20Model-Driven%20Single-Pass%20Hallucination%20Detection%20for%20Faithful%20Visual%20Question%20Answering&entry.906535625=Chaodong%20Tong%20and%20Qi%20Zhang%20and%20Chen%20Li%20and%20Lei%20Jiang%20and%20Yanbing%20Liu&entry.1292438233=Faithfulness%20hallucinations%20in%20VQA%20occur%20when%20vision-language%20models%20produce%20fluent%20yet%20visually%20ungrounded%20answers%2C%20severely%20undermining%20their%20reliability%20in%20safety-critical%20applications.%20Existing%20detection%20methods%20mainly%20fall%20into%20two%20categories%3A%20external%20verification%20approaches%20relying%20on%20auxiliary%20models%20or%20knowledge%20bases%2C%20and%20uncertainty-driven%20approaches%20using%20repeated%20sampling%20or%20uncertainty%20estimates.%20The%20former%20suffer%20from%20high%20computational%20overhead%20and%20are%20limited%20by%20external%20resource%20quality%2C%20while%20the%20latter%20capture%20only%20limited%20facets%20of%20model%20uncertainty%20and%20fail%20to%20sufficiently%20explore%20the%20rich%20internal%20signals%20associated%20with%20the%20diverse%20failure%20modes.%20Both%20paradigms%20thus%20have%20inherent%20limitations%20in%20efficiency%2C%20robustness%2C%20and%20detection%20performance.%20To%20address%20these%20challenges%2C%20we%20propose%20FaithSCAN%3A%20a%20lightweight%20network%20that%20detects%20hallucinations%20by%20exploiting%20rich%20internal%20signals%20of%20VLMs%2C%20including%20token-level%20decoding%20uncertainty%2C%20intermediate%20visual%20representations%2C%20and%20cross-modal%20alignment%20features.%20These%20signals%20are%20fused%20via%20branch-wise%20evidence%20encoding%20and%20uncertainty-aware%20attention.%20We%20also%20extend%20the%20LLM-as-a-Judge%20paradigm%20to%20VQA%20hallucination%20and%20propose%20a%20low-cost%20strategy%20to%20automatically%20generate%20model-dependent%20supervision%20signals%2C%20enabling%20supervised%20training%20without%20costly%20human%20labels%20while%20maintaining%20high%20detection%20accuracy.%20Experiments%20on%20multiple%20VQA%20benchmarks%20show%20that%20FaithSCAN%20significantly%20outperforms%20existing%20methods%20in%20both%20effectiveness%20and%20efficiency.%20In-depth%20analysis%20shows%20hallucinations%20arise%20from%20systematic%20internal%20state%20variations%20in%20visual%20perception%2C%20cross-modal%20reasoning%2C%20and%20language%20decoding.%20Different%20internal%20signals%20provide%20complementary%20diagnostic%20cues%2C%20and%20hallucination%20patterns%20vary%20across%20VLM%20architectures%2C%20offering%20new%20insights%20into%20the%20underlying%20causes%20of%20multimodal%20hallucinations.&entry.1838667208=http%3A//arxiv.org/abs/2601.00269v2&entry.124074799=Read"},
{"title": "CLEAR-Mamba:Towards Accurate, Adaptive and Trustworthy Multi-Sequence Ophthalmic Angiography Classification", "author": "Zhuonan Wang and Wenjie Yan and Wenqiao Zhang and Xiaohui Song and Jian Ma and Ke Yao and Yibo Yu and Beng Chin Ooi", "abstract": "Medical image classification is a core task in computer-aided diagnosis (CAD), playing a pivotal role in early disease detection, treatment planning, and patient prognosis assessment. In ophthalmic practice, fluorescein fundus angiography (FFA) and indocyanine green angiography (ICGA) provide hemodynamic and lesion-structural information that conventional fundus photography cannot capture. However, due to the single-modality nature, subtle lesion patterns, and significant inter-device variability, existing methods still face limitations in generalization and high-confidence prediction. To address these challenges, we propose CLEAR-Mamba, an enhanced framework built upon MedMamba with optimizations in both architecture and training strategy. Architecturally, we introduce HaC, a hypernetwork-based adaptive conditioning layer that dynamically generates parameters according to input feature distributions, thereby improving cross-domain adaptability. From a training perspective, we develop RaP, a reliability-aware prediction scheme built upon evidential uncertainty learning, which encourages the model to emphasize low-confidence samples and improves overall stability and reliability. We further construct a large-scale ophthalmic angiography dataset covering both FFA and ICGA modalities, comprising multiple retinal disease categories for model training and evaluation. Experimental results demonstrate that CLEAR-Mamba consistently outperforms multiple baseline models, including the original MedMamba, across various metrics-showing particular advantages in multi-disease classification and reliability-aware prediction. This study provides an effective solution that balances generalizability and reliability for modality-specific medical image classification tasks.", "link": "http://arxiv.org/abs/2601.20601v1", "date": "2026-01-28", "relevancy": 2.1762, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5536}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5521}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLEAR-Mamba%3ATowards%20Accurate%2C%20Adaptive%20and%20Trustworthy%20Multi-Sequence%20Ophthalmic%20Angiography%20Classification&body=Title%3A%20CLEAR-Mamba%3ATowards%20Accurate%2C%20Adaptive%20and%20Trustworthy%20Multi-Sequence%20Ophthalmic%20Angiography%20Classification%0AAuthor%3A%20Zhuonan%20Wang%20and%20Wenjie%20Yan%20and%20Wenqiao%20Zhang%20and%20Xiaohui%20Song%20and%20Jian%20Ma%20and%20Ke%20Yao%20and%20Yibo%20Yu%20and%20Beng%20Chin%20Ooi%0AAbstract%3A%20Medical%20image%20classification%20is%20a%20core%20task%20in%20computer-aided%20diagnosis%20%28CAD%29%2C%20playing%20a%20pivotal%20role%20in%20early%20disease%20detection%2C%20treatment%20planning%2C%20and%20patient%20prognosis%20assessment.%20In%20ophthalmic%20practice%2C%20fluorescein%20fundus%20angiography%20%28FFA%29%20and%20indocyanine%20green%20angiography%20%28ICGA%29%20provide%20hemodynamic%20and%20lesion-structural%20information%20that%20conventional%20fundus%20photography%20cannot%20capture.%20However%2C%20due%20to%20the%20single-modality%20nature%2C%20subtle%20lesion%20patterns%2C%20and%20significant%20inter-device%20variability%2C%20existing%20methods%20still%20face%20limitations%20in%20generalization%20and%20high-confidence%20prediction.%20To%20address%20these%20challenges%2C%20we%20propose%20CLEAR-Mamba%2C%20an%20enhanced%20framework%20built%20upon%20MedMamba%20with%20optimizations%20in%20both%20architecture%20and%20training%20strategy.%20Architecturally%2C%20we%20introduce%20HaC%2C%20a%20hypernetwork-based%20adaptive%20conditioning%20layer%20that%20dynamically%20generates%20parameters%20according%20to%20input%20feature%20distributions%2C%20thereby%20improving%20cross-domain%20adaptability.%20From%20a%20training%20perspective%2C%20we%20develop%20RaP%2C%20a%20reliability-aware%20prediction%20scheme%20built%20upon%20evidential%20uncertainty%20learning%2C%20which%20encourages%20the%20model%20to%20emphasize%20low-confidence%20samples%20and%20improves%20overall%20stability%20and%20reliability.%20We%20further%20construct%20a%20large-scale%20ophthalmic%20angiography%20dataset%20covering%20both%20FFA%20and%20ICGA%20modalities%2C%20comprising%20multiple%20retinal%20disease%20categories%20for%20model%20training%20and%20evaluation.%20Experimental%20results%20demonstrate%20that%20CLEAR-Mamba%20consistently%20outperforms%20multiple%20baseline%20models%2C%20including%20the%20original%20MedMamba%2C%20across%20various%20metrics-showing%20particular%20advantages%20in%20multi-disease%20classification%20and%20reliability-aware%20prediction.%20This%20study%20provides%20an%20effective%20solution%20that%20balances%20generalizability%20and%20reliability%20for%20modality-specific%20medical%20image%20classification%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20601v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLEAR-Mamba%253ATowards%2520Accurate%252C%2520Adaptive%2520and%2520Trustworthy%2520Multi-Sequence%2520Ophthalmic%2520Angiography%2520Classification%26entry.906535625%3DZhuonan%2520Wang%2520and%2520Wenjie%2520Yan%2520and%2520Wenqiao%2520Zhang%2520and%2520Xiaohui%2520Song%2520and%2520Jian%2520Ma%2520and%2520Ke%2520Yao%2520and%2520Yibo%2520Yu%2520and%2520Beng%2520Chin%2520Ooi%26entry.1292438233%3DMedical%2520image%2520classification%2520is%2520a%2520core%2520task%2520in%2520computer-aided%2520diagnosis%2520%2528CAD%2529%252C%2520playing%2520a%2520pivotal%2520role%2520in%2520early%2520disease%2520detection%252C%2520treatment%2520planning%252C%2520and%2520patient%2520prognosis%2520assessment.%2520In%2520ophthalmic%2520practice%252C%2520fluorescein%2520fundus%2520angiography%2520%2528FFA%2529%2520and%2520indocyanine%2520green%2520angiography%2520%2528ICGA%2529%2520provide%2520hemodynamic%2520and%2520lesion-structural%2520information%2520that%2520conventional%2520fundus%2520photography%2520cannot%2520capture.%2520However%252C%2520due%2520to%2520the%2520single-modality%2520nature%252C%2520subtle%2520lesion%2520patterns%252C%2520and%2520significant%2520inter-device%2520variability%252C%2520existing%2520methods%2520still%2520face%2520limitations%2520in%2520generalization%2520and%2520high-confidence%2520prediction.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520CLEAR-Mamba%252C%2520an%2520enhanced%2520framework%2520built%2520upon%2520MedMamba%2520with%2520optimizations%2520in%2520both%2520architecture%2520and%2520training%2520strategy.%2520Architecturally%252C%2520we%2520introduce%2520HaC%252C%2520a%2520hypernetwork-based%2520adaptive%2520conditioning%2520layer%2520that%2520dynamically%2520generates%2520parameters%2520according%2520to%2520input%2520feature%2520distributions%252C%2520thereby%2520improving%2520cross-domain%2520adaptability.%2520From%2520a%2520training%2520perspective%252C%2520we%2520develop%2520RaP%252C%2520a%2520reliability-aware%2520prediction%2520scheme%2520built%2520upon%2520evidential%2520uncertainty%2520learning%252C%2520which%2520encourages%2520the%2520model%2520to%2520emphasize%2520low-confidence%2520samples%2520and%2520improves%2520overall%2520stability%2520and%2520reliability.%2520We%2520further%2520construct%2520a%2520large-scale%2520ophthalmic%2520angiography%2520dataset%2520covering%2520both%2520FFA%2520and%2520ICGA%2520modalities%252C%2520comprising%2520multiple%2520retinal%2520disease%2520categories%2520for%2520model%2520training%2520and%2520evaluation.%2520Experimental%2520results%2520demonstrate%2520that%2520CLEAR-Mamba%2520consistently%2520outperforms%2520multiple%2520baseline%2520models%252C%2520including%2520the%2520original%2520MedMamba%252C%2520across%2520various%2520metrics-showing%2520particular%2520advantages%2520in%2520multi-disease%2520classification%2520and%2520reliability-aware%2520prediction.%2520This%2520study%2520provides%2520an%2520effective%2520solution%2520that%2520balances%2520generalizability%2520and%2520reliability%2520for%2520modality-specific%2520medical%2520image%2520classification%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20601v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLEAR-Mamba%3ATowards%20Accurate%2C%20Adaptive%20and%20Trustworthy%20Multi-Sequence%20Ophthalmic%20Angiography%20Classification&entry.906535625=Zhuonan%20Wang%20and%20Wenjie%20Yan%20and%20Wenqiao%20Zhang%20and%20Xiaohui%20Song%20and%20Jian%20Ma%20and%20Ke%20Yao%20and%20Yibo%20Yu%20and%20Beng%20Chin%20Ooi&entry.1292438233=Medical%20image%20classification%20is%20a%20core%20task%20in%20computer-aided%20diagnosis%20%28CAD%29%2C%20playing%20a%20pivotal%20role%20in%20early%20disease%20detection%2C%20treatment%20planning%2C%20and%20patient%20prognosis%20assessment.%20In%20ophthalmic%20practice%2C%20fluorescein%20fundus%20angiography%20%28FFA%29%20and%20indocyanine%20green%20angiography%20%28ICGA%29%20provide%20hemodynamic%20and%20lesion-structural%20information%20that%20conventional%20fundus%20photography%20cannot%20capture.%20However%2C%20due%20to%20the%20single-modality%20nature%2C%20subtle%20lesion%20patterns%2C%20and%20significant%20inter-device%20variability%2C%20existing%20methods%20still%20face%20limitations%20in%20generalization%20and%20high-confidence%20prediction.%20To%20address%20these%20challenges%2C%20we%20propose%20CLEAR-Mamba%2C%20an%20enhanced%20framework%20built%20upon%20MedMamba%20with%20optimizations%20in%20both%20architecture%20and%20training%20strategy.%20Architecturally%2C%20we%20introduce%20HaC%2C%20a%20hypernetwork-based%20adaptive%20conditioning%20layer%20that%20dynamically%20generates%20parameters%20according%20to%20input%20feature%20distributions%2C%20thereby%20improving%20cross-domain%20adaptability.%20From%20a%20training%20perspective%2C%20we%20develop%20RaP%2C%20a%20reliability-aware%20prediction%20scheme%20built%20upon%20evidential%20uncertainty%20learning%2C%20which%20encourages%20the%20model%20to%20emphasize%20low-confidence%20samples%20and%20improves%20overall%20stability%20and%20reliability.%20We%20further%20construct%20a%20large-scale%20ophthalmic%20angiography%20dataset%20covering%20both%20FFA%20and%20ICGA%20modalities%2C%20comprising%20multiple%20retinal%20disease%20categories%20for%20model%20training%20and%20evaluation.%20Experimental%20results%20demonstrate%20that%20CLEAR-Mamba%20consistently%20outperforms%20multiple%20baseline%20models%2C%20including%20the%20original%20MedMamba%2C%20across%20various%20metrics-showing%20particular%20advantages%20in%20multi-disease%20classification%20and%20reliability-aware%20prediction.%20This%20study%20provides%20an%20effective%20solution%20that%20balances%20generalizability%20and%20reliability%20for%20modality-specific%20medical%20image%20classification%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2601.20601v1&entry.124074799=Read"},
{"title": "Advancing Open-source World Models", "author": " Robbyant Team and Zelin Gao and Qiuyu Wang and Yanhong Zeng and Jiapeng Zhu and Ka Leong Cheng and Yixuan Li and Hanlin Wang and Yinghao Xu and Shuailei Ma and Yihang Chen and Jie Liu and Yansong Cheng and Yao Yao and Jiayi Zhu and Yihao Meng and Kecheng Zheng and Qingyan Bai and Jingye Chen and Zehong Shen and Yue Yu and Xing Zhu and Yujun Shen and Hao Ouyang", "abstract": "We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as \"long-term memory\". (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.", "link": "http://arxiv.org/abs/2601.20540v1", "date": "2026-01-28", "relevancy": 2.1725, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5769}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5647}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Open-source%20World%20Models&body=Title%3A%20Advancing%20Open-source%20World%20Models%0AAuthor%3A%20%20Robbyant%20Team%20and%20Zelin%20Gao%20and%20Qiuyu%20Wang%20and%20Yanhong%20Zeng%20and%20Jiapeng%20Zhu%20and%20Ka%20Leong%20Cheng%20and%20Yixuan%20Li%20and%20Hanlin%20Wang%20and%20Yinghao%20Xu%20and%20Shuailei%20Ma%20and%20Yihang%20Chen%20and%20Jie%20Liu%20and%20Yansong%20Cheng%20and%20Yao%20Yao%20and%20Jiayi%20Zhu%20and%20Yihao%20Meng%20and%20Kecheng%20Zheng%20and%20Qingyan%20Bai%20and%20Jingye%20Chen%20and%20Zehong%20Shen%20and%20Yue%20Yu%20and%20Xing%20Zhu%20and%20Yujun%20Shen%20and%20Hao%20Ouyang%0AAbstract%3A%20We%20present%20LingBot-World%2C%20an%20open-sourced%20world%20simulator%20stemming%20from%20video%20generation.%20Positioned%20as%20a%20top-tier%20world%20model%2C%20LingBot-World%20offers%20the%20following%20features.%20%281%29%20It%20maintains%20high%20fidelity%20and%20robust%20dynamics%20in%20a%20broad%20spectrum%20of%20environments%2C%20including%20realism%2C%20scientific%20contexts%2C%20cartoon%20styles%2C%20and%20beyond.%20%282%29%20It%20enables%20a%20minute-level%20horizon%20while%20preserving%20contextual%20consistency%20over%20time%2C%20which%20is%20also%20known%20as%20%22long-term%20memory%22.%20%283%29%20It%20supports%20real-time%20interactivity%2C%20achieving%20a%20latency%20of%20under%201%20second%20when%20producing%2016%20frames%20per%20second.%20We%20provide%20public%20access%20to%20the%20code%20and%20model%20in%20an%20effort%20to%20narrow%20the%20divide%20between%20open-source%20and%20closed-source%20technologies.%20We%20believe%20our%20release%20will%20empower%20the%20community%20with%20practical%20applications%20across%20areas%20like%20content%20creation%2C%20gaming%2C%20and%20robot%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20540v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Open-source%2520World%2520Models%26entry.906535625%3D%2520Robbyant%2520Team%2520and%2520Zelin%2520Gao%2520and%2520Qiuyu%2520Wang%2520and%2520Yanhong%2520Zeng%2520and%2520Jiapeng%2520Zhu%2520and%2520Ka%2520Leong%2520Cheng%2520and%2520Yixuan%2520Li%2520and%2520Hanlin%2520Wang%2520and%2520Yinghao%2520Xu%2520and%2520Shuailei%2520Ma%2520and%2520Yihang%2520Chen%2520and%2520Jie%2520Liu%2520and%2520Yansong%2520Cheng%2520and%2520Yao%2520Yao%2520and%2520Jiayi%2520Zhu%2520and%2520Yihao%2520Meng%2520and%2520Kecheng%2520Zheng%2520and%2520Qingyan%2520Bai%2520and%2520Jingye%2520Chen%2520and%2520Zehong%2520Shen%2520and%2520Yue%2520Yu%2520and%2520Xing%2520Zhu%2520and%2520Yujun%2520Shen%2520and%2520Hao%2520Ouyang%26entry.1292438233%3DWe%2520present%2520LingBot-World%252C%2520an%2520open-sourced%2520world%2520simulator%2520stemming%2520from%2520video%2520generation.%2520Positioned%2520as%2520a%2520top-tier%2520world%2520model%252C%2520LingBot-World%2520offers%2520the%2520following%2520features.%2520%25281%2529%2520It%2520maintains%2520high%2520fidelity%2520and%2520robust%2520dynamics%2520in%2520a%2520broad%2520spectrum%2520of%2520environments%252C%2520including%2520realism%252C%2520scientific%2520contexts%252C%2520cartoon%2520styles%252C%2520and%2520beyond.%2520%25282%2529%2520It%2520enables%2520a%2520minute-level%2520horizon%2520while%2520preserving%2520contextual%2520consistency%2520over%2520time%252C%2520which%2520is%2520also%2520known%2520as%2520%2522long-term%2520memory%2522.%2520%25283%2529%2520It%2520supports%2520real-time%2520interactivity%252C%2520achieving%2520a%2520latency%2520of%2520under%25201%2520second%2520when%2520producing%252016%2520frames%2520per%2520second.%2520We%2520provide%2520public%2520access%2520to%2520the%2520code%2520and%2520model%2520in%2520an%2520effort%2520to%2520narrow%2520the%2520divide%2520between%2520open-source%2520and%2520closed-source%2520technologies.%2520We%2520believe%2520our%2520release%2520will%2520empower%2520the%2520community%2520with%2520practical%2520applications%2520across%2520areas%2520like%2520content%2520creation%252C%2520gaming%252C%2520and%2520robot%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20540v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Open-source%20World%20Models&entry.906535625=%20Robbyant%20Team%20and%20Zelin%20Gao%20and%20Qiuyu%20Wang%20and%20Yanhong%20Zeng%20and%20Jiapeng%20Zhu%20and%20Ka%20Leong%20Cheng%20and%20Yixuan%20Li%20and%20Hanlin%20Wang%20and%20Yinghao%20Xu%20and%20Shuailei%20Ma%20and%20Yihang%20Chen%20and%20Jie%20Liu%20and%20Yansong%20Cheng%20and%20Yao%20Yao%20and%20Jiayi%20Zhu%20and%20Yihao%20Meng%20and%20Kecheng%20Zheng%20and%20Qingyan%20Bai%20and%20Jingye%20Chen%20and%20Zehong%20Shen%20and%20Yue%20Yu%20and%20Xing%20Zhu%20and%20Yujun%20Shen%20and%20Hao%20Ouyang&entry.1292438233=We%20present%20LingBot-World%2C%20an%20open-sourced%20world%20simulator%20stemming%20from%20video%20generation.%20Positioned%20as%20a%20top-tier%20world%20model%2C%20LingBot-World%20offers%20the%20following%20features.%20%281%29%20It%20maintains%20high%20fidelity%20and%20robust%20dynamics%20in%20a%20broad%20spectrum%20of%20environments%2C%20including%20realism%2C%20scientific%20contexts%2C%20cartoon%20styles%2C%20and%20beyond.%20%282%29%20It%20enables%20a%20minute-level%20horizon%20while%20preserving%20contextual%20consistency%20over%20time%2C%20which%20is%20also%20known%20as%20%22long-term%20memory%22.%20%283%29%20It%20supports%20real-time%20interactivity%2C%20achieving%20a%20latency%20of%20under%201%20second%20when%20producing%2016%20frames%20per%20second.%20We%20provide%20public%20access%20to%20the%20code%20and%20model%20in%20an%20effort%20to%20narrow%20the%20divide%20between%20open-source%20and%20closed-source%20technologies.%20We%20believe%20our%20release%20will%20empower%20the%20community%20with%20practical%20applications%20across%20areas%20like%20content%20creation%2C%20gaming%2C%20and%20robot%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2601.20540v1&entry.124074799=Read"},
{"title": "Assembling the Mind's Mosaic: Towards EEG Semantic Intent Decoding", "author": "Jiahe Li and Junru Chen and Fanqi Shen and Jialan Yang and Jada Li and Zhizhang Yuan and Baowen Cheng and Meng Li and Yang Yang", "abstract": "Enabling natural communication through brain-computer interfaces (BCIs) remains one of the most profound challenges in neuroscience and neurotechnology. While existing frameworks offer partial solutions, they are constrained by oversimplified semantic representations and a lack of interpretability. To overcome these limitations, we introduce Semantic Intent Decoding (SID), a novel framework that translates neural activity into natural language by modeling meaning as a flexible set of compositional semantic units. SID is built on three core principles: semantic compositionality, continuity and expandability of semantic space, and fidelity in reconstruction. We present BrainMosaic, a deep learning architecture implementing SID. BrainMosaic decodes multiple semantic units from EEG/SEEG signals using set matching and then reconstructs coherent sentences through semantic-guided reconstruction. This approach moves beyond traditional pipelines that rely on fixed-class classification or unconstrained generation, enabling a more interpretable and expressive communication paradigm. Extensive experiments on multilingual EEG and clinical SEEG datasets demonstrate that SID and BrainMosaic offer substantial advantages over existing frameworks, paving the way for natural and effective BCI-mediated communication.", "link": "http://arxiv.org/abs/2601.20447v1", "date": "2026-01-28", "relevancy": 2.1651, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5475}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5475}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assembling%20the%20Mind%27s%20Mosaic%3A%20Towards%20EEG%20Semantic%20Intent%20Decoding&body=Title%3A%20Assembling%20the%20Mind%27s%20Mosaic%3A%20Towards%20EEG%20Semantic%20Intent%20Decoding%0AAuthor%3A%20Jiahe%20Li%20and%20Junru%20Chen%20and%20Fanqi%20Shen%20and%20Jialan%20Yang%20and%20Jada%20Li%20and%20Zhizhang%20Yuan%20and%20Baowen%20Cheng%20and%20Meng%20Li%20and%20Yang%20Yang%0AAbstract%3A%20Enabling%20natural%20communication%20through%20brain-computer%20interfaces%20%28BCIs%29%20remains%20one%20of%20the%20most%20profound%20challenges%20in%20neuroscience%20and%20neurotechnology.%20While%20existing%20frameworks%20offer%20partial%20solutions%2C%20they%20are%20constrained%20by%20oversimplified%20semantic%20representations%20and%20a%20lack%20of%20interpretability.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20Semantic%20Intent%20Decoding%20%28SID%29%2C%20a%20novel%20framework%20that%20translates%20neural%20activity%20into%20natural%20language%20by%20modeling%20meaning%20as%20a%20flexible%20set%20of%20compositional%20semantic%20units.%20SID%20is%20built%20on%20three%20core%20principles%3A%20semantic%20compositionality%2C%20continuity%20and%20expandability%20of%20semantic%20space%2C%20and%20fidelity%20in%20reconstruction.%20We%20present%20BrainMosaic%2C%20a%20deep%20learning%20architecture%20implementing%20SID.%20BrainMosaic%20decodes%20multiple%20semantic%20units%20from%20EEG/SEEG%20signals%20using%20set%20matching%20and%20then%20reconstructs%20coherent%20sentences%20through%20semantic-guided%20reconstruction.%20This%20approach%20moves%20beyond%20traditional%20pipelines%20that%20rely%20on%20fixed-class%20classification%20or%20unconstrained%20generation%2C%20enabling%20a%20more%20interpretable%20and%20expressive%20communication%20paradigm.%20Extensive%20experiments%20on%20multilingual%20EEG%20and%20clinical%20SEEG%20datasets%20demonstrate%20that%20SID%20and%20BrainMosaic%20offer%20substantial%20advantages%20over%20existing%20frameworks%2C%20paving%20the%20way%20for%20natural%20and%20effective%20BCI-mediated%20communication.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20447v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssembling%2520the%2520Mind%2527s%2520Mosaic%253A%2520Towards%2520EEG%2520Semantic%2520Intent%2520Decoding%26entry.906535625%3DJiahe%2520Li%2520and%2520Junru%2520Chen%2520and%2520Fanqi%2520Shen%2520and%2520Jialan%2520Yang%2520and%2520Jada%2520Li%2520and%2520Zhizhang%2520Yuan%2520and%2520Baowen%2520Cheng%2520and%2520Meng%2520Li%2520and%2520Yang%2520Yang%26entry.1292438233%3DEnabling%2520natural%2520communication%2520through%2520brain-computer%2520interfaces%2520%2528BCIs%2529%2520remains%2520one%2520of%2520the%2520most%2520profound%2520challenges%2520in%2520neuroscience%2520and%2520neurotechnology.%2520While%2520existing%2520frameworks%2520offer%2520partial%2520solutions%252C%2520they%2520are%2520constrained%2520by%2520oversimplified%2520semantic%2520representations%2520and%2520a%2520lack%2520of%2520interpretability.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520introduce%2520Semantic%2520Intent%2520Decoding%2520%2528SID%2529%252C%2520a%2520novel%2520framework%2520that%2520translates%2520neural%2520activity%2520into%2520natural%2520language%2520by%2520modeling%2520meaning%2520as%2520a%2520flexible%2520set%2520of%2520compositional%2520semantic%2520units.%2520SID%2520is%2520built%2520on%2520three%2520core%2520principles%253A%2520semantic%2520compositionality%252C%2520continuity%2520and%2520expandability%2520of%2520semantic%2520space%252C%2520and%2520fidelity%2520in%2520reconstruction.%2520We%2520present%2520BrainMosaic%252C%2520a%2520deep%2520learning%2520architecture%2520implementing%2520SID.%2520BrainMosaic%2520decodes%2520multiple%2520semantic%2520units%2520from%2520EEG/SEEG%2520signals%2520using%2520set%2520matching%2520and%2520then%2520reconstructs%2520coherent%2520sentences%2520through%2520semantic-guided%2520reconstruction.%2520This%2520approach%2520moves%2520beyond%2520traditional%2520pipelines%2520that%2520rely%2520on%2520fixed-class%2520classification%2520or%2520unconstrained%2520generation%252C%2520enabling%2520a%2520more%2520interpretable%2520and%2520expressive%2520communication%2520paradigm.%2520Extensive%2520experiments%2520on%2520multilingual%2520EEG%2520and%2520clinical%2520SEEG%2520datasets%2520demonstrate%2520that%2520SID%2520and%2520BrainMosaic%2520offer%2520substantial%2520advantages%2520over%2520existing%2520frameworks%252C%2520paving%2520the%2520way%2520for%2520natural%2520and%2520effective%2520BCI-mediated%2520communication.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20447v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assembling%20the%20Mind%27s%20Mosaic%3A%20Towards%20EEG%20Semantic%20Intent%20Decoding&entry.906535625=Jiahe%20Li%20and%20Junru%20Chen%20and%20Fanqi%20Shen%20and%20Jialan%20Yang%20and%20Jada%20Li%20and%20Zhizhang%20Yuan%20and%20Baowen%20Cheng%20and%20Meng%20Li%20and%20Yang%20Yang&entry.1292438233=Enabling%20natural%20communication%20through%20brain-computer%20interfaces%20%28BCIs%29%20remains%20one%20of%20the%20most%20profound%20challenges%20in%20neuroscience%20and%20neurotechnology.%20While%20existing%20frameworks%20offer%20partial%20solutions%2C%20they%20are%20constrained%20by%20oversimplified%20semantic%20representations%20and%20a%20lack%20of%20interpretability.%20To%20overcome%20these%20limitations%2C%20we%20introduce%20Semantic%20Intent%20Decoding%20%28SID%29%2C%20a%20novel%20framework%20that%20translates%20neural%20activity%20into%20natural%20language%20by%20modeling%20meaning%20as%20a%20flexible%20set%20of%20compositional%20semantic%20units.%20SID%20is%20built%20on%20three%20core%20principles%3A%20semantic%20compositionality%2C%20continuity%20and%20expandability%20of%20semantic%20space%2C%20and%20fidelity%20in%20reconstruction.%20We%20present%20BrainMosaic%2C%20a%20deep%20learning%20architecture%20implementing%20SID.%20BrainMosaic%20decodes%20multiple%20semantic%20units%20from%20EEG/SEEG%20signals%20using%20set%20matching%20and%20then%20reconstructs%20coherent%20sentences%20through%20semantic-guided%20reconstruction.%20This%20approach%20moves%20beyond%20traditional%20pipelines%20that%20rely%20on%20fixed-class%20classification%20or%20unconstrained%20generation%2C%20enabling%20a%20more%20interpretable%20and%20expressive%20communication%20paradigm.%20Extensive%20experiments%20on%20multilingual%20EEG%20and%20clinical%20SEEG%20datasets%20demonstrate%20that%20SID%20and%20BrainMosaic%20offer%20substantial%20advantages%20over%20existing%20frameworks%2C%20paving%20the%20way%20for%20natural%20and%20effective%20BCI-mediated%20communication.&entry.1838667208=http%3A//arxiv.org/abs/2601.20447v1&entry.124074799=Read"},
{"title": "Continual GUI Agents", "author": "Ziwei Liu and Borui Kang and Hangjie Yuan and Zixiang Zhao and Wei Li and Yifan Zhu and Tao Feng", "abstract": "As digital environments (data distribution) are in flux, with new GUI data arriving over time-introducing new domains or resolutions-agents trained on static environments deteriorate in performance. In this work, we introduce Continual GUI Agents, a new task that requires GUI agents to perform continual learning under shifted domains and resolutions. We find existing methods fail to maintain stable grounding as GUI distributions shift over time, due to the diversity of UI interaction points and regions in fluxing scenarios. To address this, we introduce GUI-Anchoring in Flux (GUI-AiF), a new reinforcement fine-tuning framework that stabilizes continual learning through two novel rewards: Anchoring Point Reward in Flux (APR-iF) and Anchoring Region Reward in Flux (ARR-iF). These rewards guide the agents to align with shifting interaction points and regions, mitigating the tendency of existing reward strategies to over-adapt to static grounding cues (e.g., fixed coordinates or element scales). Extensive experiments show GUI-AiF surpasses state-of-the-art baselines. Our work establishes the first continual learning framework for GUI agents, revealing the untapped potential of reinforcement fine-tuning for continual GUI Agents.", "link": "http://arxiv.org/abs/2601.20732v1", "date": "2026-01-28", "relevancy": 2.1649, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5539}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5346}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Continual%20GUI%20Agents&body=Title%3A%20Continual%20GUI%20Agents%0AAuthor%3A%20Ziwei%20Liu%20and%20Borui%20Kang%20and%20Hangjie%20Yuan%20and%20Zixiang%20Zhao%20and%20Wei%20Li%20and%20Yifan%20Zhu%20and%20Tao%20Feng%0AAbstract%3A%20As%20digital%20environments%20%28data%20distribution%29%20are%20in%20flux%2C%20with%20new%20GUI%20data%20arriving%20over%20time-introducing%20new%20domains%20or%20resolutions-agents%20trained%20on%20static%20environments%20deteriorate%20in%20performance.%20In%20this%20work%2C%20we%20introduce%20Continual%20GUI%20Agents%2C%20a%20new%20task%20that%20requires%20GUI%20agents%20to%20perform%20continual%20learning%20under%20shifted%20domains%20and%20resolutions.%20We%20find%20existing%20methods%20fail%20to%20maintain%20stable%20grounding%20as%20GUI%20distributions%20shift%20over%20time%2C%20due%20to%20the%20diversity%20of%20UI%20interaction%20points%20and%20regions%20in%20fluxing%20scenarios.%20To%20address%20this%2C%20we%20introduce%20GUI-Anchoring%20in%20Flux%20%28GUI-AiF%29%2C%20a%20new%20reinforcement%20fine-tuning%20framework%20that%20stabilizes%20continual%20learning%20through%20two%20novel%20rewards%3A%20Anchoring%20Point%20Reward%20in%20Flux%20%28APR-iF%29%20and%20Anchoring%20Region%20Reward%20in%20Flux%20%28ARR-iF%29.%20These%20rewards%20guide%20the%20agents%20to%20align%20with%20shifting%20interaction%20points%20and%20regions%2C%20mitigating%20the%20tendency%20of%20existing%20reward%20strategies%20to%20over-adapt%20to%20static%20grounding%20cues%20%28e.g.%2C%20fixed%20coordinates%20or%20element%20scales%29.%20Extensive%20experiments%20show%20GUI-AiF%20surpasses%20state-of-the-art%20baselines.%20Our%20work%20establishes%20the%20first%20continual%20learning%20framework%20for%20GUI%20agents%2C%20revealing%20the%20untapped%20potential%20of%20reinforcement%20fine-tuning%20for%20continual%20GUI%20Agents.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20732v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContinual%2520GUI%2520Agents%26entry.906535625%3DZiwei%2520Liu%2520and%2520Borui%2520Kang%2520and%2520Hangjie%2520Yuan%2520and%2520Zixiang%2520Zhao%2520and%2520Wei%2520Li%2520and%2520Yifan%2520Zhu%2520and%2520Tao%2520Feng%26entry.1292438233%3DAs%2520digital%2520environments%2520%2528data%2520distribution%2529%2520are%2520in%2520flux%252C%2520with%2520new%2520GUI%2520data%2520arriving%2520over%2520time-introducing%2520new%2520domains%2520or%2520resolutions-agents%2520trained%2520on%2520static%2520environments%2520deteriorate%2520in%2520performance.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Continual%2520GUI%2520Agents%252C%2520a%2520new%2520task%2520that%2520requires%2520GUI%2520agents%2520to%2520perform%2520continual%2520learning%2520under%2520shifted%2520domains%2520and%2520resolutions.%2520We%2520find%2520existing%2520methods%2520fail%2520to%2520maintain%2520stable%2520grounding%2520as%2520GUI%2520distributions%2520shift%2520over%2520time%252C%2520due%2520to%2520the%2520diversity%2520of%2520UI%2520interaction%2520points%2520and%2520regions%2520in%2520fluxing%2520scenarios.%2520To%2520address%2520this%252C%2520we%2520introduce%2520GUI-Anchoring%2520in%2520Flux%2520%2528GUI-AiF%2529%252C%2520a%2520new%2520reinforcement%2520fine-tuning%2520framework%2520that%2520stabilizes%2520continual%2520learning%2520through%2520two%2520novel%2520rewards%253A%2520Anchoring%2520Point%2520Reward%2520in%2520Flux%2520%2528APR-iF%2529%2520and%2520Anchoring%2520Region%2520Reward%2520in%2520Flux%2520%2528ARR-iF%2529.%2520These%2520rewards%2520guide%2520the%2520agents%2520to%2520align%2520with%2520shifting%2520interaction%2520points%2520and%2520regions%252C%2520mitigating%2520the%2520tendency%2520of%2520existing%2520reward%2520strategies%2520to%2520over-adapt%2520to%2520static%2520grounding%2520cues%2520%2528e.g.%252C%2520fixed%2520coordinates%2520or%2520element%2520scales%2529.%2520Extensive%2520experiments%2520show%2520GUI-AiF%2520surpasses%2520state-of-the-art%2520baselines.%2520Our%2520work%2520establishes%2520the%2520first%2520continual%2520learning%2520framework%2520for%2520GUI%2520agents%252C%2520revealing%2520the%2520untapped%2520potential%2520of%2520reinforcement%2520fine-tuning%2520for%2520continual%2520GUI%2520Agents.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20732v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20GUI%20Agents&entry.906535625=Ziwei%20Liu%20and%20Borui%20Kang%20and%20Hangjie%20Yuan%20and%20Zixiang%20Zhao%20and%20Wei%20Li%20and%20Yifan%20Zhu%20and%20Tao%20Feng&entry.1292438233=As%20digital%20environments%20%28data%20distribution%29%20are%20in%20flux%2C%20with%20new%20GUI%20data%20arriving%20over%20time-introducing%20new%20domains%20or%20resolutions-agents%20trained%20on%20static%20environments%20deteriorate%20in%20performance.%20In%20this%20work%2C%20we%20introduce%20Continual%20GUI%20Agents%2C%20a%20new%20task%20that%20requires%20GUI%20agents%20to%20perform%20continual%20learning%20under%20shifted%20domains%20and%20resolutions.%20We%20find%20existing%20methods%20fail%20to%20maintain%20stable%20grounding%20as%20GUI%20distributions%20shift%20over%20time%2C%20due%20to%20the%20diversity%20of%20UI%20interaction%20points%20and%20regions%20in%20fluxing%20scenarios.%20To%20address%20this%2C%20we%20introduce%20GUI-Anchoring%20in%20Flux%20%28GUI-AiF%29%2C%20a%20new%20reinforcement%20fine-tuning%20framework%20that%20stabilizes%20continual%20learning%20through%20two%20novel%20rewards%3A%20Anchoring%20Point%20Reward%20in%20Flux%20%28APR-iF%29%20and%20Anchoring%20Region%20Reward%20in%20Flux%20%28ARR-iF%29.%20These%20rewards%20guide%20the%20agents%20to%20align%20with%20shifting%20interaction%20points%20and%20regions%2C%20mitigating%20the%20tendency%20of%20existing%20reward%20strategies%20to%20over-adapt%20to%20static%20grounding%20cues%20%28e.g.%2C%20fixed%20coordinates%20or%20element%20scales%29.%20Extensive%20experiments%20show%20GUI-AiF%20surpasses%20state-of-the-art%20baselines.%20Our%20work%20establishes%20the%20first%20continual%20learning%20framework%20for%20GUI%20agents%2C%20revealing%20the%20untapped%20potential%20of%20reinforcement%20fine-tuning%20for%20continual%20GUI%20Agents.&entry.1838667208=http%3A//arxiv.org/abs/2601.20732v1&entry.124074799=Read"},
{"title": "From Specialist to Generalist: Unlocking SAM's Learning Potential on Unlabeled Medical Images", "author": "Vi Vu and Thanh-Huy Nguyen and Tien-Thinh Nguyen and Ba-Thinh Lam and Hoang-Thien Nguyen and Tianyang Wang and Xingjian Li and Min Xu", "abstract": "Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM's adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at https://github.com/vnlvi2k3/SC-SAM.", "link": "http://arxiv.org/abs/2601.17934v2", "date": "2026-01-28", "relevancy": 2.1564, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5637}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5288}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Specialist%20to%20Generalist%3A%20Unlocking%20SAM%27s%20Learning%20Potential%20on%20Unlabeled%20Medical%20Images&body=Title%3A%20From%20Specialist%20to%20Generalist%3A%20Unlocking%20SAM%27s%20Learning%20Potential%20on%20Unlabeled%20Medical%20Images%0AAuthor%3A%20Vi%20Vu%20and%20Thanh-Huy%20Nguyen%20and%20Tien-Thinh%20Nguyen%20and%20Ba-Thinh%20Lam%20and%20Hoang-Thien%20Nguyen%20and%20Tianyang%20Wang%20and%20Xingjian%20Li%20and%20Min%20Xu%0AAbstract%3A%20Foundation%20models%20like%20the%20Segment%20Anything%20Model%20%28SAM%29%20show%20strong%20generalization%2C%20yet%20adapting%20them%20to%20medical%20images%20remains%20difficult%20due%20to%20domain%20shift%2C%20scarce%20labels%2C%20and%20the%20inability%20of%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20to%20exploit%20unlabeled%20data.%20While%20conventional%20models%20like%20U-Net%20excel%20in%20semi-supervised%20medical%20learning%2C%20their%20potential%20to%20assist%20a%20PEFT%20SAM%20has%20been%20largely%20overlooked.%20We%20introduce%20SC-SAM%2C%20a%20specialist-generalist%20framework%20where%20U-Net%20provides%20point-based%20prompts%20and%20pseudo-labels%20to%20guide%20SAM%27s%20adaptation%2C%20while%20SAM%20serves%20as%20a%20powerful%20generalist%20supervisor%20to%20regularize%20U-Net.%20This%20reciprocal%20guidance%20forms%20a%20bidirectional%20co-training%20loop%20that%20allows%20both%20models%20to%20effectively%20exploit%20the%20unlabeled%20data.%20Across%20prostate%20MRI%20and%20polyp%20segmentation%20benchmarks%2C%20our%20method%20achieves%20state-of-the-art%20results%2C%20outperforming%20other%20existing%20semi-supervised%20SAM%20variants%20and%20even%20medical%20foundation%20models%20like%20MedSAM%2C%20highlighting%20the%20value%20of%20specialist-generalist%20cooperation%20for%20label-efficient%20medical%20image%20segmentation.%20Our%20code%20is%20available%20at%20https%3A//github.com/vnlvi2k3/SC-SAM.%0ALink%3A%20http%3A//arxiv.org/abs/2601.17934v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Specialist%2520to%2520Generalist%253A%2520Unlocking%2520SAM%2527s%2520Learning%2520Potential%2520on%2520Unlabeled%2520Medical%2520Images%26entry.906535625%3DVi%2520Vu%2520and%2520Thanh-Huy%2520Nguyen%2520and%2520Tien-Thinh%2520Nguyen%2520and%2520Ba-Thinh%2520Lam%2520and%2520Hoang-Thien%2520Nguyen%2520and%2520Tianyang%2520Wang%2520and%2520Xingjian%2520Li%2520and%2520Min%2520Xu%26entry.1292438233%3DFoundation%2520models%2520like%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520show%2520strong%2520generalization%252C%2520yet%2520adapting%2520them%2520to%2520medical%2520images%2520remains%2520difficult%2520due%2520to%2520domain%2520shift%252C%2520scarce%2520labels%252C%2520and%2520the%2520inability%2520of%2520Parameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520to%2520exploit%2520unlabeled%2520data.%2520While%2520conventional%2520models%2520like%2520U-Net%2520excel%2520in%2520semi-supervised%2520medical%2520learning%252C%2520their%2520potential%2520to%2520assist%2520a%2520PEFT%2520SAM%2520has%2520been%2520largely%2520overlooked.%2520We%2520introduce%2520SC-SAM%252C%2520a%2520specialist-generalist%2520framework%2520where%2520U-Net%2520provides%2520point-based%2520prompts%2520and%2520pseudo-labels%2520to%2520guide%2520SAM%2527s%2520adaptation%252C%2520while%2520SAM%2520serves%2520as%2520a%2520powerful%2520generalist%2520supervisor%2520to%2520regularize%2520U-Net.%2520This%2520reciprocal%2520guidance%2520forms%2520a%2520bidirectional%2520co-training%2520loop%2520that%2520allows%2520both%2520models%2520to%2520effectively%2520exploit%2520the%2520unlabeled%2520data.%2520Across%2520prostate%2520MRI%2520and%2520polyp%2520segmentation%2520benchmarks%252C%2520our%2520method%2520achieves%2520state-of-the-art%2520results%252C%2520outperforming%2520other%2520existing%2520semi-supervised%2520SAM%2520variants%2520and%2520even%2520medical%2520foundation%2520models%2520like%2520MedSAM%252C%2520highlighting%2520the%2520value%2520of%2520specialist-generalist%2520cooperation%2520for%2520label-efficient%2520medical%2520image%2520segmentation.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/vnlvi2k3/SC-SAM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.17934v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Specialist%20to%20Generalist%3A%20Unlocking%20SAM%27s%20Learning%20Potential%20on%20Unlabeled%20Medical%20Images&entry.906535625=Vi%20Vu%20and%20Thanh-Huy%20Nguyen%20and%20Tien-Thinh%20Nguyen%20and%20Ba-Thinh%20Lam%20and%20Hoang-Thien%20Nguyen%20and%20Tianyang%20Wang%20and%20Xingjian%20Li%20and%20Min%20Xu&entry.1292438233=Foundation%20models%20like%20the%20Segment%20Anything%20Model%20%28SAM%29%20show%20strong%20generalization%2C%20yet%20adapting%20them%20to%20medical%20images%20remains%20difficult%20due%20to%20domain%20shift%2C%20scarce%20labels%2C%20and%20the%20inability%20of%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20to%20exploit%20unlabeled%20data.%20While%20conventional%20models%20like%20U-Net%20excel%20in%20semi-supervised%20medical%20learning%2C%20their%20potential%20to%20assist%20a%20PEFT%20SAM%20has%20been%20largely%20overlooked.%20We%20introduce%20SC-SAM%2C%20a%20specialist-generalist%20framework%20where%20U-Net%20provides%20point-based%20prompts%20and%20pseudo-labels%20to%20guide%20SAM%27s%20adaptation%2C%20while%20SAM%20serves%20as%20a%20powerful%20generalist%20supervisor%20to%20regularize%20U-Net.%20This%20reciprocal%20guidance%20forms%20a%20bidirectional%20co-training%20loop%20that%20allows%20both%20models%20to%20effectively%20exploit%20the%20unlabeled%20data.%20Across%20prostate%20MRI%20and%20polyp%20segmentation%20benchmarks%2C%20our%20method%20achieves%20state-of-the-art%20results%2C%20outperforming%20other%20existing%20semi-supervised%20SAM%20variants%20and%20even%20medical%20foundation%20models%20like%20MedSAM%2C%20highlighting%20the%20value%20of%20specialist-generalist%20cooperation%20for%20label-efficient%20medical%20image%20segmentation.%20Our%20code%20is%20available%20at%20https%3A//github.com/vnlvi2k3/SC-SAM.&entry.1838667208=http%3A//arxiv.org/abs/2601.17934v2&entry.124074799=Read"},
{"title": "DCP-Bench-Open: Evaluating LLMs for Constraint Modelling of Discrete Combinatorial Problems", "author": "Kostis Michailidis and Dimos Tsouros and Tias Guns", "abstract": "Discrete Combinatorial Problems (DCPs) are prevalent in industrial decision-making and optimisation. However, while constraint solving technologies for DCPs have advanced significantly, the core process of formalising them, namely constraint modelling, requires significant expertise and remains a bottleneck for wider adoption. Aiming to alleviate this bottleneck, recent studies have explored using Large Language Models (LLMs) to transform combinatorial problem descriptions into executable constraint models. However, the existing evaluation datasets for discrete constraint modelling are often limited to small, homogeneous, or domain-specific problems, which do not capture the diversity of real-world scenarios. This work addresses this gap by introducing DCP-Bench-Open, a novel benchmark that includes a diverse set of well-known discrete combinatorial problems sourced from the Constraint Programming (CP) and Operations Research (OR) communities, structured explicitly for evaluating LLM-driven constraint modelling. With this dataset, and given the variety of modelling frameworks, we compare and evaluate the modelling capabilities of LLMs for three distinct constraint modelling systems, which vary in abstraction level and underlying syntax. Notably, the results show higher performance when modelling with a high-level Python-based framework. Additionally, we systematically evaluate the use of prompt-based and inference-time compute methods across different LLMs, which further increase accuracy, reaching up to 91% on this highly challenging benchmark. DCP-Bench-Open is publicly available.", "link": "http://arxiv.org/abs/2506.06052v3", "date": "2026-01-28", "relevancy": 2.141, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5427}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5427}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4978}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DCP-Bench-Open%3A%20Evaluating%20LLMs%20for%20Constraint%20Modelling%20of%20Discrete%20Combinatorial%20Problems&body=Title%3A%20DCP-Bench-Open%3A%20Evaluating%20LLMs%20for%20Constraint%20Modelling%20of%20Discrete%20Combinatorial%20Problems%0AAuthor%3A%20Kostis%20Michailidis%20and%20Dimos%20Tsouros%20and%20Tias%20Guns%0AAbstract%3A%20Discrete%20Combinatorial%20Problems%20%28DCPs%29%20are%20prevalent%20in%20industrial%20decision-making%20and%20optimisation.%20However%2C%20while%20constraint%20solving%20technologies%20for%20DCPs%20have%20advanced%20significantly%2C%20the%20core%20process%20of%20formalising%20them%2C%20namely%20constraint%20modelling%2C%20requires%20significant%20expertise%20and%20remains%20a%20bottleneck%20for%20wider%20adoption.%20Aiming%20to%20alleviate%20this%20bottleneck%2C%20recent%20studies%20have%20explored%20using%20Large%20Language%20Models%20%28LLMs%29%20to%20transform%20combinatorial%20problem%20descriptions%20into%20executable%20constraint%20models.%20However%2C%20the%20existing%20evaluation%20datasets%20for%20discrete%20constraint%20modelling%20are%20often%20limited%20to%20small%2C%20homogeneous%2C%20or%20domain-specific%20problems%2C%20which%20do%20not%20capture%20the%20diversity%20of%20real-world%20scenarios.%20This%20work%20addresses%20this%20gap%20by%20introducing%20DCP-Bench-Open%2C%20a%20novel%20benchmark%20that%20includes%20a%20diverse%20set%20of%20well-known%20discrete%20combinatorial%20problems%20sourced%20from%20the%20Constraint%20Programming%20%28CP%29%20and%20Operations%20Research%20%28OR%29%20communities%2C%20structured%20explicitly%20for%20evaluating%20LLM-driven%20constraint%20modelling.%20With%20this%20dataset%2C%20and%20given%20the%20variety%20of%20modelling%20frameworks%2C%20we%20compare%20and%20evaluate%20the%20modelling%20capabilities%20of%20LLMs%20for%20three%20distinct%20constraint%20modelling%20systems%2C%20which%20vary%20in%20abstraction%20level%20and%20underlying%20syntax.%20Notably%2C%20the%20results%20show%20higher%20performance%20when%20modelling%20with%20a%20high-level%20Python-based%20framework.%20Additionally%2C%20we%20systematically%20evaluate%20the%20use%20of%20prompt-based%20and%20inference-time%20compute%20methods%20across%20different%20LLMs%2C%20which%20further%20increase%20accuracy%2C%20reaching%20up%20to%2091%25%20on%20this%20highly%20challenging%20benchmark.%20DCP-Bench-Open%20is%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2506.06052v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDCP-Bench-Open%253A%2520Evaluating%2520LLMs%2520for%2520Constraint%2520Modelling%2520of%2520Discrete%2520Combinatorial%2520Problems%26entry.906535625%3DKostis%2520Michailidis%2520and%2520Dimos%2520Tsouros%2520and%2520Tias%2520Guns%26entry.1292438233%3DDiscrete%2520Combinatorial%2520Problems%2520%2528DCPs%2529%2520are%2520prevalent%2520in%2520industrial%2520decision-making%2520and%2520optimisation.%2520However%252C%2520while%2520constraint%2520solving%2520technologies%2520for%2520DCPs%2520have%2520advanced%2520significantly%252C%2520the%2520core%2520process%2520of%2520formalising%2520them%252C%2520namely%2520constraint%2520modelling%252C%2520requires%2520significant%2520expertise%2520and%2520remains%2520a%2520bottleneck%2520for%2520wider%2520adoption.%2520Aiming%2520to%2520alleviate%2520this%2520bottleneck%252C%2520recent%2520studies%2520have%2520explored%2520using%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520transform%2520combinatorial%2520problem%2520descriptions%2520into%2520executable%2520constraint%2520models.%2520However%252C%2520the%2520existing%2520evaluation%2520datasets%2520for%2520discrete%2520constraint%2520modelling%2520are%2520often%2520limited%2520to%2520small%252C%2520homogeneous%252C%2520or%2520domain-specific%2520problems%252C%2520which%2520do%2520not%2520capture%2520the%2520diversity%2520of%2520real-world%2520scenarios.%2520This%2520work%2520addresses%2520this%2520gap%2520by%2520introducing%2520DCP-Bench-Open%252C%2520a%2520novel%2520benchmark%2520that%2520includes%2520a%2520diverse%2520set%2520of%2520well-known%2520discrete%2520combinatorial%2520problems%2520sourced%2520from%2520the%2520Constraint%2520Programming%2520%2528CP%2529%2520and%2520Operations%2520Research%2520%2528OR%2529%2520communities%252C%2520structured%2520explicitly%2520for%2520evaluating%2520LLM-driven%2520constraint%2520modelling.%2520With%2520this%2520dataset%252C%2520and%2520given%2520the%2520variety%2520of%2520modelling%2520frameworks%252C%2520we%2520compare%2520and%2520evaluate%2520the%2520modelling%2520capabilities%2520of%2520LLMs%2520for%2520three%2520distinct%2520constraint%2520modelling%2520systems%252C%2520which%2520vary%2520in%2520abstraction%2520level%2520and%2520underlying%2520syntax.%2520Notably%252C%2520the%2520results%2520show%2520higher%2520performance%2520when%2520modelling%2520with%2520a%2520high-level%2520Python-based%2520framework.%2520Additionally%252C%2520we%2520systematically%2520evaluate%2520the%2520use%2520of%2520prompt-based%2520and%2520inference-time%2520compute%2520methods%2520across%2520different%2520LLMs%252C%2520which%2520further%2520increase%2520accuracy%252C%2520reaching%2520up%2520to%252091%2525%2520on%2520this%2520highly%2520challenging%2520benchmark.%2520DCP-Bench-Open%2520is%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.06052v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DCP-Bench-Open%3A%20Evaluating%20LLMs%20for%20Constraint%20Modelling%20of%20Discrete%20Combinatorial%20Problems&entry.906535625=Kostis%20Michailidis%20and%20Dimos%20Tsouros%20and%20Tias%20Guns&entry.1292438233=Discrete%20Combinatorial%20Problems%20%28DCPs%29%20are%20prevalent%20in%20industrial%20decision-making%20and%20optimisation.%20However%2C%20while%20constraint%20solving%20technologies%20for%20DCPs%20have%20advanced%20significantly%2C%20the%20core%20process%20of%20formalising%20them%2C%20namely%20constraint%20modelling%2C%20requires%20significant%20expertise%20and%20remains%20a%20bottleneck%20for%20wider%20adoption.%20Aiming%20to%20alleviate%20this%20bottleneck%2C%20recent%20studies%20have%20explored%20using%20Large%20Language%20Models%20%28LLMs%29%20to%20transform%20combinatorial%20problem%20descriptions%20into%20executable%20constraint%20models.%20However%2C%20the%20existing%20evaluation%20datasets%20for%20discrete%20constraint%20modelling%20are%20often%20limited%20to%20small%2C%20homogeneous%2C%20or%20domain-specific%20problems%2C%20which%20do%20not%20capture%20the%20diversity%20of%20real-world%20scenarios.%20This%20work%20addresses%20this%20gap%20by%20introducing%20DCP-Bench-Open%2C%20a%20novel%20benchmark%20that%20includes%20a%20diverse%20set%20of%20well-known%20discrete%20combinatorial%20problems%20sourced%20from%20the%20Constraint%20Programming%20%28CP%29%20and%20Operations%20Research%20%28OR%29%20communities%2C%20structured%20explicitly%20for%20evaluating%20LLM-driven%20constraint%20modelling.%20With%20this%20dataset%2C%20and%20given%20the%20variety%20of%20modelling%20frameworks%2C%20we%20compare%20and%20evaluate%20the%20modelling%20capabilities%20of%20LLMs%20for%20three%20distinct%20constraint%20modelling%20systems%2C%20which%20vary%20in%20abstraction%20level%20and%20underlying%20syntax.%20Notably%2C%20the%20results%20show%20higher%20performance%20when%20modelling%20with%20a%20high-level%20Python-based%20framework.%20Additionally%2C%20we%20systematically%20evaluate%20the%20use%20of%20prompt-based%20and%20inference-time%20compute%20methods%20across%20different%20LLMs%2C%20which%20further%20increase%20accuracy%2C%20reaching%20up%20to%2091%25%20on%20this%20highly%20challenging%20benchmark.%20DCP-Bench-Open%20is%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2506.06052v3&entry.124074799=Read"},
{"title": "FLOL: Fast Baselines for Real-World Low-Light Enhancement", "author": "Juan C. Benito and Daniel Feijoo and Alvaro Garcia and Marcos V. Conde", "abstract": "Low-Light Image Enhancement (LLIE) is a key task in computational photography and imaging. The problem of enhancing images captured during night or in dark environments has been well-studied in the computer vision literature. However, current deep learning-based solutions struggle with efficiency and robustness for real-world scenarios (e.g., scenes with noise, saturated pixels). We propose a lightweight neural network that combines image processing in the frequency and spatial domains. Our baseline method, FLOL, is one of the fastest models for this task, achieving results comparable to the state-of-the-art on popular real-world benchmarks such as LOLv2, LSRW, MIT-5K and UHD-LL. Moreover, we are able to process 1080p images in real-time under 12ms. Code and models at https://github.com/cidautai/FLOL", "link": "http://arxiv.org/abs/2501.09718v2", "date": "2026-01-28", "relevancy": 2.1344, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5738}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5297}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FLOL%3A%20Fast%20Baselines%20for%20Real-World%20Low-Light%20Enhancement&body=Title%3A%20FLOL%3A%20Fast%20Baselines%20for%20Real-World%20Low-Light%20Enhancement%0AAuthor%3A%20Juan%20C.%20Benito%20and%20Daniel%20Feijoo%20and%20Alvaro%20Garcia%20and%20Marcos%20V.%20Conde%0AAbstract%3A%20Low-Light%20Image%20Enhancement%20%28LLIE%29%20is%20a%20key%20task%20in%20computational%20photography%20and%20imaging.%20The%20problem%20of%20enhancing%20images%20captured%20during%20night%20or%20in%20dark%20environments%20has%20been%20well-studied%20in%20the%20computer%20vision%20literature.%20However%2C%20current%20deep%20learning-based%20solutions%20struggle%20with%20efficiency%20and%20robustness%20for%20real-world%20scenarios%20%28e.g.%2C%20scenes%20with%20noise%2C%20saturated%20pixels%29.%20We%20propose%20a%20lightweight%20neural%20network%20that%20combines%20image%20processing%20in%20the%20frequency%20and%20spatial%20domains.%20Our%20baseline%20method%2C%20FLOL%2C%20is%20one%20of%20the%20fastest%20models%20for%20this%20task%2C%20achieving%20results%20comparable%20to%20the%20state-of-the-art%20on%20popular%20real-world%20benchmarks%20such%20as%20LOLv2%2C%20LSRW%2C%20MIT-5K%20and%20UHD-LL.%20Moreover%2C%20we%20are%20able%20to%20process%201080p%20images%20in%20real-time%20under%2012ms.%20Code%20and%20models%20at%20https%3A//github.com/cidautai/FLOL%0ALink%3A%20http%3A//arxiv.org/abs/2501.09718v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFLOL%253A%2520Fast%2520Baselines%2520for%2520Real-World%2520Low-Light%2520Enhancement%26entry.906535625%3DJuan%2520C.%2520Benito%2520and%2520Daniel%2520Feijoo%2520and%2520Alvaro%2520Garcia%2520and%2520Marcos%2520V.%2520Conde%26entry.1292438233%3DLow-Light%2520Image%2520Enhancement%2520%2528LLIE%2529%2520is%2520a%2520key%2520task%2520in%2520computational%2520photography%2520and%2520imaging.%2520The%2520problem%2520of%2520enhancing%2520images%2520captured%2520during%2520night%2520or%2520in%2520dark%2520environments%2520has%2520been%2520well-studied%2520in%2520the%2520computer%2520vision%2520literature.%2520However%252C%2520current%2520deep%2520learning-based%2520solutions%2520struggle%2520with%2520efficiency%2520and%2520robustness%2520for%2520real-world%2520scenarios%2520%2528e.g.%252C%2520scenes%2520with%2520noise%252C%2520saturated%2520pixels%2529.%2520We%2520propose%2520a%2520lightweight%2520neural%2520network%2520that%2520combines%2520image%2520processing%2520in%2520the%2520frequency%2520and%2520spatial%2520domains.%2520Our%2520baseline%2520method%252C%2520FLOL%252C%2520is%2520one%2520of%2520the%2520fastest%2520models%2520for%2520this%2520task%252C%2520achieving%2520results%2520comparable%2520to%2520the%2520state-of-the-art%2520on%2520popular%2520real-world%2520benchmarks%2520such%2520as%2520LOLv2%252C%2520LSRW%252C%2520MIT-5K%2520and%2520UHD-LL.%2520Moreover%252C%2520we%2520are%2520able%2520to%2520process%25201080p%2520images%2520in%2520real-time%2520under%252012ms.%2520Code%2520and%2520models%2520at%2520https%253A//github.com/cidautai/FLOL%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09718v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FLOL%3A%20Fast%20Baselines%20for%20Real-World%20Low-Light%20Enhancement&entry.906535625=Juan%20C.%20Benito%20and%20Daniel%20Feijoo%20and%20Alvaro%20Garcia%20and%20Marcos%20V.%20Conde&entry.1292438233=Low-Light%20Image%20Enhancement%20%28LLIE%29%20is%20a%20key%20task%20in%20computational%20photography%20and%20imaging.%20The%20problem%20of%20enhancing%20images%20captured%20during%20night%20or%20in%20dark%20environments%20has%20been%20well-studied%20in%20the%20computer%20vision%20literature.%20However%2C%20current%20deep%20learning-based%20solutions%20struggle%20with%20efficiency%20and%20robustness%20for%20real-world%20scenarios%20%28e.g.%2C%20scenes%20with%20noise%2C%20saturated%20pixels%29.%20We%20propose%20a%20lightweight%20neural%20network%20that%20combines%20image%20processing%20in%20the%20frequency%20and%20spatial%20domains.%20Our%20baseline%20method%2C%20FLOL%2C%20is%20one%20of%20the%20fastest%20models%20for%20this%20task%2C%20achieving%20results%20comparable%20to%20the%20state-of-the-art%20on%20popular%20real-world%20benchmarks%20such%20as%20LOLv2%2C%20LSRW%2C%20MIT-5K%20and%20UHD-LL.%20Moreover%2C%20we%20are%20able%20to%20process%201080p%20images%20in%20real-time%20under%2012ms.%20Code%20and%20models%20at%20https%3A//github.com/cidautai/FLOL&entry.1838667208=http%3A//arxiv.org/abs/2501.09718v2&entry.124074799=Read"},
{"title": "Mixing Importance with Diversity: Joint Optimization for KV Cache Compression in Large Vision-Language Models", "author": "Xuyang Liu and Xiyan Gui and Yuchao Zhang and Linfeng Zhang", "abstract": "Recent large vision-language models (LVLMs) demonstrate remarkable capabilities in processing extended multi-modal sequences, yet the resulting key-value (KV) cache expansion creates a critical memory bottleneck that fundamentally limits deployment scalability. While existing KV cache compression methods focus on retaining high-importance KV pairs to minimize storage, they often overlook the modality-specific semantic redundancy patterns that emerge distinctively in multi-modal KV caches. In this work, we first analyze how, beyond simple importance, the KV cache in LVLMs exhibits varying levels of redundancy across attention heads. We show that relying solely on importance can only cover a subset of the full KV cache information distribution, leading to potential loss of semantic coverage. To address this, we propose MixKV, a novel method that mixes importance with diversity for optimized KV cache compression in LVLMs. MixKV adapts to head-wise semantic redundancy, selectively balancing diversity and importance when compressing KV pairs. Extensive experiments demonstrate that MixKV consistently enhances existing methods across multiple LVLMs. Under extreme compression (budget=64), MixKV improves baseline methods by an average of 5.1% across five multi-modal understanding benchmarks and achieves remarkable gains of 8.0% and 9.0% for SnapKV and AdaKV on GUI grounding tasks, all while maintaining comparable inference efficiency. Furthermore, MixKV extends seamlessly to LLMs with comparable performance gains. Our code is available at https://github.com/xuyang-liu16/MixKV.", "link": "http://arxiv.org/abs/2510.20707v2", "date": "2026-01-28", "relevancy": 2.1315, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5383}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5383}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5056}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixing%20Importance%20with%20Diversity%3A%20Joint%20Optimization%20for%20KV%20Cache%20Compression%20in%20Large%20Vision-Language%20Models&body=Title%3A%20Mixing%20Importance%20with%20Diversity%3A%20Joint%20Optimization%20for%20KV%20Cache%20Compression%20in%20Large%20Vision-Language%20Models%0AAuthor%3A%20Xuyang%20Liu%20and%20Xiyan%20Gui%20and%20Yuchao%20Zhang%20and%20Linfeng%20Zhang%0AAbstract%3A%20Recent%20large%20vision-language%20models%20%28LVLMs%29%20demonstrate%20remarkable%20capabilities%20in%20processing%20extended%20multi-modal%20sequences%2C%20yet%20the%20resulting%20key-value%20%28KV%29%20cache%20expansion%20creates%20a%20critical%20memory%20bottleneck%20that%20fundamentally%20limits%20deployment%20scalability.%20While%20existing%20KV%20cache%20compression%20methods%20focus%20on%20retaining%20high-importance%20KV%20pairs%20to%20minimize%20storage%2C%20they%20often%20overlook%20the%20modality-specific%20semantic%20redundancy%20patterns%20that%20emerge%20distinctively%20in%20multi-modal%20KV%20caches.%20In%20this%20work%2C%20we%20first%20analyze%20how%2C%20beyond%20simple%20importance%2C%20the%20KV%20cache%20in%20LVLMs%20exhibits%20varying%20levels%20of%20redundancy%20across%20attention%20heads.%20We%20show%20that%20relying%20solely%20on%20importance%20can%20only%20cover%20a%20subset%20of%20the%20full%20KV%20cache%20information%20distribution%2C%20leading%20to%20potential%20loss%20of%20semantic%20coverage.%20To%20address%20this%2C%20we%20propose%20MixKV%2C%20a%20novel%20method%20that%20mixes%20importance%20with%20diversity%20for%20optimized%20KV%20cache%20compression%20in%20LVLMs.%20MixKV%20adapts%20to%20head-wise%20semantic%20redundancy%2C%20selectively%20balancing%20diversity%20and%20importance%20when%20compressing%20KV%20pairs.%20Extensive%20experiments%20demonstrate%20that%20MixKV%20consistently%20enhances%20existing%20methods%20across%20multiple%20LVLMs.%20Under%20extreme%20compression%20%28budget%3D64%29%2C%20MixKV%20improves%20baseline%20methods%20by%20an%20average%20of%205.1%25%20across%20five%20multi-modal%20understanding%20benchmarks%20and%20achieves%20remarkable%20gains%20of%208.0%25%20and%209.0%25%20for%20SnapKV%20and%20AdaKV%20on%20GUI%20grounding%20tasks%2C%20all%20while%20maintaining%20comparable%20inference%20efficiency.%20Furthermore%2C%20MixKV%20extends%20seamlessly%20to%20LLMs%20with%20comparable%20performance%20gains.%20Our%20code%20is%20available%20at%20https%3A//github.com/xuyang-liu16/MixKV.%0ALink%3A%20http%3A//arxiv.org/abs/2510.20707v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixing%2520Importance%2520with%2520Diversity%253A%2520Joint%2520Optimization%2520for%2520KV%2520Cache%2520Compression%2520in%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DXuyang%2520Liu%2520and%2520Xiyan%2520Gui%2520and%2520Yuchao%2520Zhang%2520and%2520Linfeng%2520Zhang%26entry.1292438233%3DRecent%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520demonstrate%2520remarkable%2520capabilities%2520in%2520processing%2520extended%2520multi-modal%2520sequences%252C%2520yet%2520the%2520resulting%2520key-value%2520%2528KV%2529%2520cache%2520expansion%2520creates%2520a%2520critical%2520memory%2520bottleneck%2520that%2520fundamentally%2520limits%2520deployment%2520scalability.%2520While%2520existing%2520KV%2520cache%2520compression%2520methods%2520focus%2520on%2520retaining%2520high-importance%2520KV%2520pairs%2520to%2520minimize%2520storage%252C%2520they%2520often%2520overlook%2520the%2520modality-specific%2520semantic%2520redundancy%2520patterns%2520that%2520emerge%2520distinctively%2520in%2520multi-modal%2520KV%2520caches.%2520In%2520this%2520work%252C%2520we%2520first%2520analyze%2520how%252C%2520beyond%2520simple%2520importance%252C%2520the%2520KV%2520cache%2520in%2520LVLMs%2520exhibits%2520varying%2520levels%2520of%2520redundancy%2520across%2520attention%2520heads.%2520We%2520show%2520that%2520relying%2520solely%2520on%2520importance%2520can%2520only%2520cover%2520a%2520subset%2520of%2520the%2520full%2520KV%2520cache%2520information%2520distribution%252C%2520leading%2520to%2520potential%2520loss%2520of%2520semantic%2520coverage.%2520To%2520address%2520this%252C%2520we%2520propose%2520MixKV%252C%2520a%2520novel%2520method%2520that%2520mixes%2520importance%2520with%2520diversity%2520for%2520optimized%2520KV%2520cache%2520compression%2520in%2520LVLMs.%2520MixKV%2520adapts%2520to%2520head-wise%2520semantic%2520redundancy%252C%2520selectively%2520balancing%2520diversity%2520and%2520importance%2520when%2520compressing%2520KV%2520pairs.%2520Extensive%2520experiments%2520demonstrate%2520that%2520MixKV%2520consistently%2520enhances%2520existing%2520methods%2520across%2520multiple%2520LVLMs.%2520Under%2520extreme%2520compression%2520%2528budget%253D64%2529%252C%2520MixKV%2520improves%2520baseline%2520methods%2520by%2520an%2520average%2520of%25205.1%2525%2520across%2520five%2520multi-modal%2520understanding%2520benchmarks%2520and%2520achieves%2520remarkable%2520gains%2520of%25208.0%2525%2520and%25209.0%2525%2520for%2520SnapKV%2520and%2520AdaKV%2520on%2520GUI%2520grounding%2520tasks%252C%2520all%2520while%2520maintaining%2520comparable%2520inference%2520efficiency.%2520Furthermore%252C%2520MixKV%2520extends%2520seamlessly%2520to%2520LLMs%2520with%2520comparable%2520performance%2520gains.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/xuyang-liu16/MixKV.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.20707v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixing%20Importance%20with%20Diversity%3A%20Joint%20Optimization%20for%20KV%20Cache%20Compression%20in%20Large%20Vision-Language%20Models&entry.906535625=Xuyang%20Liu%20and%20Xiyan%20Gui%20and%20Yuchao%20Zhang%20and%20Linfeng%20Zhang&entry.1292438233=Recent%20large%20vision-language%20models%20%28LVLMs%29%20demonstrate%20remarkable%20capabilities%20in%20processing%20extended%20multi-modal%20sequences%2C%20yet%20the%20resulting%20key-value%20%28KV%29%20cache%20expansion%20creates%20a%20critical%20memory%20bottleneck%20that%20fundamentally%20limits%20deployment%20scalability.%20While%20existing%20KV%20cache%20compression%20methods%20focus%20on%20retaining%20high-importance%20KV%20pairs%20to%20minimize%20storage%2C%20they%20often%20overlook%20the%20modality-specific%20semantic%20redundancy%20patterns%20that%20emerge%20distinctively%20in%20multi-modal%20KV%20caches.%20In%20this%20work%2C%20we%20first%20analyze%20how%2C%20beyond%20simple%20importance%2C%20the%20KV%20cache%20in%20LVLMs%20exhibits%20varying%20levels%20of%20redundancy%20across%20attention%20heads.%20We%20show%20that%20relying%20solely%20on%20importance%20can%20only%20cover%20a%20subset%20of%20the%20full%20KV%20cache%20information%20distribution%2C%20leading%20to%20potential%20loss%20of%20semantic%20coverage.%20To%20address%20this%2C%20we%20propose%20MixKV%2C%20a%20novel%20method%20that%20mixes%20importance%20with%20diversity%20for%20optimized%20KV%20cache%20compression%20in%20LVLMs.%20MixKV%20adapts%20to%20head-wise%20semantic%20redundancy%2C%20selectively%20balancing%20diversity%20and%20importance%20when%20compressing%20KV%20pairs.%20Extensive%20experiments%20demonstrate%20that%20MixKV%20consistently%20enhances%20existing%20methods%20across%20multiple%20LVLMs.%20Under%20extreme%20compression%20%28budget%3D64%29%2C%20MixKV%20improves%20baseline%20methods%20by%20an%20average%20of%205.1%25%20across%20five%20multi-modal%20understanding%20benchmarks%20and%20achieves%20remarkable%20gains%20of%208.0%25%20and%209.0%25%20for%20SnapKV%20and%20AdaKV%20on%20GUI%20grounding%20tasks%2C%20all%20while%20maintaining%20comparable%20inference%20efficiency.%20Furthermore%2C%20MixKV%20extends%20seamlessly%20to%20LLMs%20with%20comparable%20performance%20gains.%20Our%20code%20is%20available%20at%20https%3A//github.com/xuyang-liu16/MixKV.&entry.1838667208=http%3A//arxiv.org/abs/2510.20707v2&entry.124074799=Read"},
{"title": "GNN Explanations that do not Explain and How to find Them", "author": "Steve Azzolin and Stefano Teso and Bruno Lepri and Andrea Passerini and Sagar Malhotra", "abstract": "Explanations provided by Self-explainable Graph Neural Networks (SE-GNNs) are fundamental for understanding the model's inner workings and for identifying potential misuse of sensitive attributes. Although recent works have highlighted that these explanations can be suboptimal and potentially misleading, a characterization of their failure cases is unavailable. In this work, we identify a critical failure of SE-GNN explanations: explanations can be unambiguously unrelated to how the SE-GNNs infer labels. We show that, on the one hand, many SE-GNNs can achieve optimal true risk while producing these degenerate explanations, and on the other, most faithfulness metrics can fail to identify these failure modes. Our empirical analysis reveals that degenerate explanations can be maliciously planted (allowing an attacker to hide the use of sensitive attributes) and can also emerge naturally, highlighting the need for reliable auditing. To address this, we introduce a novel faithfulness metric that reliably marks degenerate explanations as unfaithful, in both malicious and natural settings. Our code is available in the supplemental.", "link": "http://arxiv.org/abs/2601.20815v1", "date": "2026-01-28", "relevancy": 2.1269, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4421}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4227}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GNN%20Explanations%20that%20do%20not%20Explain%20and%20How%20to%20find%20Them&body=Title%3A%20GNN%20Explanations%20that%20do%20not%20Explain%20and%20How%20to%20find%20Them%0AAuthor%3A%20Steve%20Azzolin%20and%20Stefano%20Teso%20and%20Bruno%20Lepri%20and%20Andrea%20Passerini%20and%20Sagar%20Malhotra%0AAbstract%3A%20Explanations%20provided%20by%20Self-explainable%20Graph%20Neural%20Networks%20%28SE-GNNs%29%20are%20fundamental%20for%20understanding%20the%20model%27s%20inner%20workings%20and%20for%20identifying%20potential%20misuse%20of%20sensitive%20attributes.%20Although%20recent%20works%20have%20highlighted%20that%20these%20explanations%20can%20be%20suboptimal%20and%20potentially%20misleading%2C%20a%20characterization%20of%20their%20failure%20cases%20is%20unavailable.%20In%20this%20work%2C%20we%20identify%20a%20critical%20failure%20of%20SE-GNN%20explanations%3A%20explanations%20can%20be%20unambiguously%20unrelated%20to%20how%20the%20SE-GNNs%20infer%20labels.%20We%20show%20that%2C%20on%20the%20one%20hand%2C%20many%20SE-GNNs%20can%20achieve%20optimal%20true%20risk%20while%20producing%20these%20degenerate%20explanations%2C%20and%20on%20the%20other%2C%20most%20faithfulness%20metrics%20can%20fail%20to%20identify%20these%20failure%20modes.%20Our%20empirical%20analysis%20reveals%20that%20degenerate%20explanations%20can%20be%20maliciously%20planted%20%28allowing%20an%20attacker%20to%20hide%20the%20use%20of%20sensitive%20attributes%29%20and%20can%20also%20emerge%20naturally%2C%20highlighting%20the%20need%20for%20reliable%20auditing.%20To%20address%20this%2C%20we%20introduce%20a%20novel%20faithfulness%20metric%20that%20reliably%20marks%20degenerate%20explanations%20as%20unfaithful%2C%20in%20both%20malicious%20and%20natural%20settings.%20Our%20code%20is%20available%20in%20the%20supplemental.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20815v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGNN%2520Explanations%2520that%2520do%2520not%2520Explain%2520and%2520How%2520to%2520find%2520Them%26entry.906535625%3DSteve%2520Azzolin%2520and%2520Stefano%2520Teso%2520and%2520Bruno%2520Lepri%2520and%2520Andrea%2520Passerini%2520and%2520Sagar%2520Malhotra%26entry.1292438233%3DExplanations%2520provided%2520by%2520Self-explainable%2520Graph%2520Neural%2520Networks%2520%2528SE-GNNs%2529%2520are%2520fundamental%2520for%2520understanding%2520the%2520model%2527s%2520inner%2520workings%2520and%2520for%2520identifying%2520potential%2520misuse%2520of%2520sensitive%2520attributes.%2520Although%2520recent%2520works%2520have%2520highlighted%2520that%2520these%2520explanations%2520can%2520be%2520suboptimal%2520and%2520potentially%2520misleading%252C%2520a%2520characterization%2520of%2520their%2520failure%2520cases%2520is%2520unavailable.%2520In%2520this%2520work%252C%2520we%2520identify%2520a%2520critical%2520failure%2520of%2520SE-GNN%2520explanations%253A%2520explanations%2520can%2520be%2520unambiguously%2520unrelated%2520to%2520how%2520the%2520SE-GNNs%2520infer%2520labels.%2520We%2520show%2520that%252C%2520on%2520the%2520one%2520hand%252C%2520many%2520SE-GNNs%2520can%2520achieve%2520optimal%2520true%2520risk%2520while%2520producing%2520these%2520degenerate%2520explanations%252C%2520and%2520on%2520the%2520other%252C%2520most%2520faithfulness%2520metrics%2520can%2520fail%2520to%2520identify%2520these%2520failure%2520modes.%2520Our%2520empirical%2520analysis%2520reveals%2520that%2520degenerate%2520explanations%2520can%2520be%2520maliciously%2520planted%2520%2528allowing%2520an%2520attacker%2520to%2520hide%2520the%2520use%2520of%2520sensitive%2520attributes%2529%2520and%2520can%2520also%2520emerge%2520naturally%252C%2520highlighting%2520the%2520need%2520for%2520reliable%2520auditing.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520novel%2520faithfulness%2520metric%2520that%2520reliably%2520marks%2520degenerate%2520explanations%2520as%2520unfaithful%252C%2520in%2520both%2520malicious%2520and%2520natural%2520settings.%2520Our%2520code%2520is%2520available%2520in%2520the%2520supplemental.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20815v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GNN%20Explanations%20that%20do%20not%20Explain%20and%20How%20to%20find%20Them&entry.906535625=Steve%20Azzolin%20and%20Stefano%20Teso%20and%20Bruno%20Lepri%20and%20Andrea%20Passerini%20and%20Sagar%20Malhotra&entry.1292438233=Explanations%20provided%20by%20Self-explainable%20Graph%20Neural%20Networks%20%28SE-GNNs%29%20are%20fundamental%20for%20understanding%20the%20model%27s%20inner%20workings%20and%20for%20identifying%20potential%20misuse%20of%20sensitive%20attributes.%20Although%20recent%20works%20have%20highlighted%20that%20these%20explanations%20can%20be%20suboptimal%20and%20potentially%20misleading%2C%20a%20characterization%20of%20their%20failure%20cases%20is%20unavailable.%20In%20this%20work%2C%20we%20identify%20a%20critical%20failure%20of%20SE-GNN%20explanations%3A%20explanations%20can%20be%20unambiguously%20unrelated%20to%20how%20the%20SE-GNNs%20infer%20labels.%20We%20show%20that%2C%20on%20the%20one%20hand%2C%20many%20SE-GNNs%20can%20achieve%20optimal%20true%20risk%20while%20producing%20these%20degenerate%20explanations%2C%20and%20on%20the%20other%2C%20most%20faithfulness%20metrics%20can%20fail%20to%20identify%20these%20failure%20modes.%20Our%20empirical%20analysis%20reveals%20that%20degenerate%20explanations%20can%20be%20maliciously%20planted%20%28allowing%20an%20attacker%20to%20hide%20the%20use%20of%20sensitive%20attributes%29%20and%20can%20also%20emerge%20naturally%2C%20highlighting%20the%20need%20for%20reliable%20auditing.%20To%20address%20this%2C%20we%20introduce%20a%20novel%20faithfulness%20metric%20that%20reliably%20marks%20degenerate%20explanations%20as%20unfaithful%2C%20in%20both%20malicious%20and%20natural%20settings.%20Our%20code%20is%20available%20in%20the%20supplemental.&entry.1838667208=http%3A//arxiv.org/abs/2601.20815v1&entry.124074799=Read"},
{"title": "Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers", "author": "Zecheng Tang and Quantong Qiu and Yi Yang and Zhiyi Hong and Haiya Xiang and Kebin Liu and Qingqing Dang and Juntao Li and Min Zhang", "abstract": "The quadratic complexity of standard attention mechanisms poses a significant scalability bottleneck for large language models (LLMs) in long-context scenarios. While hybrid attention strategies that combine sparse and full attention within a single model offer a viable solution, they typically employ static computation ratios (i.e., fixed proportions of sparse versus full attention) and fail to adapt to the varying sparsity sensitivities of downstream tasks during inference. To address this issue, we propose Elastic Attention, which allows the model to dynamically adjust its overall sparsity based on the input. This is achieved by integrating a lightweight Attention Router into the existing pretrained model, which dynamically assigns each attention head to different computation modes. Within only 12 hours of training on 8xA800 GPUs, our method enables models to achieve both strong performance and efficient inference. Experiments across three long-context benchmarks on widely-used LLMs demonstrate the superiority of our method.", "link": "http://arxiv.org/abs/2601.17367v2", "date": "2026-01-28", "relevancy": 2.1257, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5625}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5331}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Elastic%20Attention%3A%20Test-time%20Adaptive%20Sparsity%20Ratios%20for%20Efficient%20Transformers&body=Title%3A%20Elastic%20Attention%3A%20Test-time%20Adaptive%20Sparsity%20Ratios%20for%20Efficient%20Transformers%0AAuthor%3A%20Zecheng%20Tang%20and%20Quantong%20Qiu%20and%20Yi%20Yang%20and%20Zhiyi%20Hong%20and%20Haiya%20Xiang%20and%20Kebin%20Liu%20and%20Qingqing%20Dang%20and%20Juntao%20Li%20and%20Min%20Zhang%0AAbstract%3A%20The%20quadratic%20complexity%20of%20standard%20attention%20mechanisms%20poses%20a%20significant%20scalability%20bottleneck%20for%20large%20language%20models%20%28LLMs%29%20in%20long-context%20scenarios.%20While%20hybrid%20attention%20strategies%20that%20combine%20sparse%20and%20full%20attention%20within%20a%20single%20model%20offer%20a%20viable%20solution%2C%20they%20typically%20employ%20static%20computation%20ratios%20%28i.e.%2C%20fixed%20proportions%20of%20sparse%20versus%20full%20attention%29%20and%20fail%20to%20adapt%20to%20the%20varying%20sparsity%20sensitivities%20of%20downstream%20tasks%20during%20inference.%20To%20address%20this%20issue%2C%20we%20propose%20Elastic%20Attention%2C%20which%20allows%20the%20model%20to%20dynamically%20adjust%20its%20overall%20sparsity%20based%20on%20the%20input.%20This%20is%20achieved%20by%20integrating%20a%20lightweight%20Attention%20Router%20into%20the%20existing%20pretrained%20model%2C%20which%20dynamically%20assigns%20each%20attention%20head%20to%20different%20computation%20modes.%20Within%20only%2012%20hours%20of%20training%20on%208xA800%20GPUs%2C%20our%20method%20enables%20models%20to%20achieve%20both%20strong%20performance%20and%20efficient%20inference.%20Experiments%20across%20three%20long-context%20benchmarks%20on%20widely-used%20LLMs%20demonstrate%20the%20superiority%20of%20our%20method.%0ALink%3A%20http%3A//arxiv.org/abs/2601.17367v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DElastic%2520Attention%253A%2520Test-time%2520Adaptive%2520Sparsity%2520Ratios%2520for%2520Efficient%2520Transformers%26entry.906535625%3DZecheng%2520Tang%2520and%2520Quantong%2520Qiu%2520and%2520Yi%2520Yang%2520and%2520Zhiyi%2520Hong%2520and%2520Haiya%2520Xiang%2520and%2520Kebin%2520Liu%2520and%2520Qingqing%2520Dang%2520and%2520Juntao%2520Li%2520and%2520Min%2520Zhang%26entry.1292438233%3DThe%2520quadratic%2520complexity%2520of%2520standard%2520attention%2520mechanisms%2520poses%2520a%2520significant%2520scalability%2520bottleneck%2520for%2520large%2520language%2520models%2520%2528LLMs%2529%2520in%2520long-context%2520scenarios.%2520While%2520hybrid%2520attention%2520strategies%2520that%2520combine%2520sparse%2520and%2520full%2520attention%2520within%2520a%2520single%2520model%2520offer%2520a%2520viable%2520solution%252C%2520they%2520typically%2520employ%2520static%2520computation%2520ratios%2520%2528i.e.%252C%2520fixed%2520proportions%2520of%2520sparse%2520versus%2520full%2520attention%2529%2520and%2520fail%2520to%2520adapt%2520to%2520the%2520varying%2520sparsity%2520sensitivities%2520of%2520downstream%2520tasks%2520during%2520inference.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520Elastic%2520Attention%252C%2520which%2520allows%2520the%2520model%2520to%2520dynamically%2520adjust%2520its%2520overall%2520sparsity%2520based%2520on%2520the%2520input.%2520This%2520is%2520achieved%2520by%2520integrating%2520a%2520lightweight%2520Attention%2520Router%2520into%2520the%2520existing%2520pretrained%2520model%252C%2520which%2520dynamically%2520assigns%2520each%2520attention%2520head%2520to%2520different%2520computation%2520modes.%2520Within%2520only%252012%2520hours%2520of%2520training%2520on%25208xA800%2520GPUs%252C%2520our%2520method%2520enables%2520models%2520to%2520achieve%2520both%2520strong%2520performance%2520and%2520efficient%2520inference.%2520Experiments%2520across%2520three%2520long-context%2520benchmarks%2520on%2520widely-used%2520LLMs%2520demonstrate%2520the%2520superiority%2520of%2520our%2520method.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.17367v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Elastic%20Attention%3A%20Test-time%20Adaptive%20Sparsity%20Ratios%20for%20Efficient%20Transformers&entry.906535625=Zecheng%20Tang%20and%20Quantong%20Qiu%20and%20Yi%20Yang%20and%20Zhiyi%20Hong%20and%20Haiya%20Xiang%20and%20Kebin%20Liu%20and%20Qingqing%20Dang%20and%20Juntao%20Li%20and%20Min%20Zhang&entry.1292438233=The%20quadratic%20complexity%20of%20standard%20attention%20mechanisms%20poses%20a%20significant%20scalability%20bottleneck%20for%20large%20language%20models%20%28LLMs%29%20in%20long-context%20scenarios.%20While%20hybrid%20attention%20strategies%20that%20combine%20sparse%20and%20full%20attention%20within%20a%20single%20model%20offer%20a%20viable%20solution%2C%20they%20typically%20employ%20static%20computation%20ratios%20%28i.e.%2C%20fixed%20proportions%20of%20sparse%20versus%20full%20attention%29%20and%20fail%20to%20adapt%20to%20the%20varying%20sparsity%20sensitivities%20of%20downstream%20tasks%20during%20inference.%20To%20address%20this%20issue%2C%20we%20propose%20Elastic%20Attention%2C%20which%20allows%20the%20model%20to%20dynamically%20adjust%20its%20overall%20sparsity%20based%20on%20the%20input.%20This%20is%20achieved%20by%20integrating%20a%20lightweight%20Attention%20Router%20into%20the%20existing%20pretrained%20model%2C%20which%20dynamically%20assigns%20each%20attention%20head%20to%20different%20computation%20modes.%20Within%20only%2012%20hours%20of%20training%20on%208xA800%20GPUs%2C%20our%20method%20enables%20models%20to%20achieve%20both%20strong%20performance%20and%20efficient%20inference.%20Experiments%20across%20three%20long-context%20benchmarks%20on%20widely-used%20LLMs%20demonstrate%20the%20superiority%20of%20our%20method.&entry.1838667208=http%3A//arxiv.org/abs/2601.17367v2&entry.124074799=Read"},
{"title": "LLMStinger: Jailbreaking LLMs using RL fine-tuned LLMs", "author": "Piyush Jha and Arnav Arora and Vijay Ganesh", "abstract": "We introduce LLMStinger, a novel approach that leverages Large Language Models (LLMs) to automatically generate adversarial suffixes for jailbreak attacks. Unlike traditional methods, which require complex prompt engineering or white-box access, LLMStinger uses a reinforcement learning (RL) loop to fine-tune an attacker LLM, generating new suffixes based on existing attacks for harmful questions from the HarmBench benchmark. Our method significantly outperforms existing red-teaming approaches (we compared against 15 of the latest methods), achieving a +57.2% improvement in Attack Success Rate (ASR) on LLaMA2-7B-chat and a +50.3% ASR increase on Claude 2, both models known for their extensive safety measures. Additionally, we achieved a 94.97% ASR on GPT-3.5 and 99.4% on Gemma-2B-it, demonstrating the robustness and adaptability of LLMStinger across open and closed-source models.", "link": "http://arxiv.org/abs/2411.08862v2", "date": "2026-01-28", "relevancy": 2.1178, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4683}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4097}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.3927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMStinger%3A%20Jailbreaking%20LLMs%20using%20RL%20fine-tuned%20LLMs&body=Title%3A%20LLMStinger%3A%20Jailbreaking%20LLMs%20using%20RL%20fine-tuned%20LLMs%0AAuthor%3A%20Piyush%20Jha%20and%20Arnav%20Arora%20and%20Vijay%20Ganesh%0AAbstract%3A%20We%20introduce%20LLMStinger%2C%20a%20novel%20approach%20that%20leverages%20Large%20Language%20Models%20%28LLMs%29%20to%20automatically%20generate%20adversarial%20suffixes%20for%20jailbreak%20attacks.%20Unlike%20traditional%20methods%2C%20which%20require%20complex%20prompt%20engineering%20or%20white-box%20access%2C%20LLMStinger%20uses%20a%20reinforcement%20learning%20%28RL%29%20loop%20to%20fine-tune%20an%20attacker%20LLM%2C%20generating%20new%20suffixes%20based%20on%20existing%20attacks%20for%20harmful%20questions%20from%20the%20HarmBench%20benchmark.%20Our%20method%20significantly%20outperforms%20existing%20red-teaming%20approaches%20%28we%20compared%20against%2015%20of%20the%20latest%20methods%29%2C%20achieving%20a%20%2B57.2%25%20improvement%20in%20Attack%20Success%20Rate%20%28ASR%29%20on%20LLaMA2-7B-chat%20and%20a%20%2B50.3%25%20ASR%20increase%20on%20Claude%202%2C%20both%20models%20known%20for%20their%20extensive%20safety%20measures.%20Additionally%2C%20we%20achieved%20a%2094.97%25%20ASR%20on%20GPT-3.5%20and%2099.4%25%20on%20Gemma-2B-it%2C%20demonstrating%20the%20robustness%20and%20adaptability%20of%20LLMStinger%20across%20open%20and%20closed-source%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2411.08862v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMStinger%253A%2520Jailbreaking%2520LLMs%2520using%2520RL%2520fine-tuned%2520LLMs%26entry.906535625%3DPiyush%2520Jha%2520and%2520Arnav%2520Arora%2520and%2520Vijay%2520Ganesh%26entry.1292438233%3DWe%2520introduce%2520LLMStinger%252C%2520a%2520novel%2520approach%2520that%2520leverages%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%2520automatically%2520generate%2520adversarial%2520suffixes%2520for%2520jailbreak%2520attacks.%2520Unlike%2520traditional%2520methods%252C%2520which%2520require%2520complex%2520prompt%2520engineering%2520or%2520white-box%2520access%252C%2520LLMStinger%2520uses%2520a%2520reinforcement%2520learning%2520%2528RL%2529%2520loop%2520to%2520fine-tune%2520an%2520attacker%2520LLM%252C%2520generating%2520new%2520suffixes%2520based%2520on%2520existing%2520attacks%2520for%2520harmful%2520questions%2520from%2520the%2520HarmBench%2520benchmark.%2520Our%2520method%2520significantly%2520outperforms%2520existing%2520red-teaming%2520approaches%2520%2528we%2520compared%2520against%252015%2520of%2520the%2520latest%2520methods%2529%252C%2520achieving%2520a%2520%252B57.2%2525%2520improvement%2520in%2520Attack%2520Success%2520Rate%2520%2528ASR%2529%2520on%2520LLaMA2-7B-chat%2520and%2520a%2520%252B50.3%2525%2520ASR%2520increase%2520on%2520Claude%25202%252C%2520both%2520models%2520known%2520for%2520their%2520extensive%2520safety%2520measures.%2520Additionally%252C%2520we%2520achieved%2520a%252094.97%2525%2520ASR%2520on%2520GPT-3.5%2520and%252099.4%2525%2520on%2520Gemma-2B-it%252C%2520demonstrating%2520the%2520robustness%2520and%2520adaptability%2520of%2520LLMStinger%2520across%2520open%2520and%2520closed-source%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08862v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMStinger%3A%20Jailbreaking%20LLMs%20using%20RL%20fine-tuned%20LLMs&entry.906535625=Piyush%20Jha%20and%20Arnav%20Arora%20and%20Vijay%20Ganesh&entry.1292438233=We%20introduce%20LLMStinger%2C%20a%20novel%20approach%20that%20leverages%20Large%20Language%20Models%20%28LLMs%29%20to%20automatically%20generate%20adversarial%20suffixes%20for%20jailbreak%20attacks.%20Unlike%20traditional%20methods%2C%20which%20require%20complex%20prompt%20engineering%20or%20white-box%20access%2C%20LLMStinger%20uses%20a%20reinforcement%20learning%20%28RL%29%20loop%20to%20fine-tune%20an%20attacker%20LLM%2C%20generating%20new%20suffixes%20based%20on%20existing%20attacks%20for%20harmful%20questions%20from%20the%20HarmBench%20benchmark.%20Our%20method%20significantly%20outperforms%20existing%20red-teaming%20approaches%20%28we%20compared%20against%2015%20of%20the%20latest%20methods%29%2C%20achieving%20a%20%2B57.2%25%20improvement%20in%20Attack%20Success%20Rate%20%28ASR%29%20on%20LLaMA2-7B-chat%20and%20a%20%2B50.3%25%20ASR%20increase%20on%20Claude%202%2C%20both%20models%20known%20for%20their%20extensive%20safety%20measures.%20Additionally%2C%20we%20achieved%20a%2094.97%25%20ASR%20on%20GPT-3.5%20and%2099.4%25%20on%20Gemma-2B-it%2C%20demonstrating%20the%20robustness%20and%20adaptability%20of%20LLMStinger%20across%20open%20and%20closed-source%20models.&entry.1838667208=http%3A//arxiv.org/abs/2411.08862v2&entry.124074799=Read"},
{"title": "A Foundation Model for Virtual Sensors", "author": "Leon G\u00f6tz and Lars Frederik Peiss and Erik Sauer and Andreas Udo Sass and Thorsten Bagdonat and Stephan G\u00fcnnemann and Leo Schwinn", "abstract": "Virtual sensors use machine learning to predict target signals from available measurements, replacing expensive physical sensors in critical applications. Existing virtual sensor approaches require application-specific models with hand-selected inputs for each sensor, cannot leverage task synergies, and lack consistent benchmarks. At the same time, emerging time series foundation models are computationally expensive and limited to predicting their input signals, making them incompatible with virtual sensors. We introduce the first foundation model for virtual sensors addressing both limitations. Our unified model can simultaneously predict diverse virtual sensors exploiting synergies while maintaining computational efficiency. It learns relevant input signals for each virtual sensor, eliminating expert knowledge requirements while adding explainability. In our large-scale evaluation on a standard benchmark and an application-specific dataset with over 18 billion samples, our architecture achieves 415x reduction in computation time and 951x reduction in memory requirements, while maintaining or even improving predictive quality compared to baselines. Our model scales gracefully to hundreds of virtual sensors with nearly constant parameter count, enabling practical deployment in large-scale sensor networks.", "link": "http://arxiv.org/abs/2601.20634v1", "date": "2026-01-28", "relevancy": 2.1154, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5418}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5263}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Foundation%20Model%20for%20Virtual%20Sensors&body=Title%3A%20A%20Foundation%20Model%20for%20Virtual%20Sensors%0AAuthor%3A%20Leon%20G%C3%B6tz%20and%20Lars%20Frederik%20Peiss%20and%20Erik%20Sauer%20and%20Andreas%20Udo%20Sass%20and%20Thorsten%20Bagdonat%20and%20Stephan%20G%C3%BCnnemann%20and%20Leo%20Schwinn%0AAbstract%3A%20Virtual%20sensors%20use%20machine%20learning%20to%20predict%20target%20signals%20from%20available%20measurements%2C%20replacing%20expensive%20physical%20sensors%20in%20critical%20applications.%20Existing%20virtual%20sensor%20approaches%20require%20application-specific%20models%20with%20hand-selected%20inputs%20for%20each%20sensor%2C%20cannot%20leverage%20task%20synergies%2C%20and%20lack%20consistent%20benchmarks.%20At%20the%20same%20time%2C%20emerging%20time%20series%20foundation%20models%20are%20computationally%20expensive%20and%20limited%20to%20predicting%20their%20input%20signals%2C%20making%20them%20incompatible%20with%20virtual%20sensors.%20We%20introduce%20the%20first%20foundation%20model%20for%20virtual%20sensors%20addressing%20both%20limitations.%20Our%20unified%20model%20can%20simultaneously%20predict%20diverse%20virtual%20sensors%20exploiting%20synergies%20while%20maintaining%20computational%20efficiency.%20It%20learns%20relevant%20input%20signals%20for%20each%20virtual%20sensor%2C%20eliminating%20expert%20knowledge%20requirements%20while%20adding%20explainability.%20In%20our%20large-scale%20evaluation%20on%20a%20standard%20benchmark%20and%20an%20application-specific%20dataset%20with%20over%2018%20billion%20samples%2C%20our%20architecture%20achieves%20415x%20reduction%20in%20computation%20time%20and%20951x%20reduction%20in%20memory%20requirements%2C%20while%20maintaining%20or%20even%20improving%20predictive%20quality%20compared%20to%20baselines.%20Our%20model%20scales%20gracefully%20to%20hundreds%20of%20virtual%20sensors%20with%20nearly%20constant%20parameter%20count%2C%20enabling%20practical%20deployment%20in%20large-scale%20sensor%20networks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20634v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Foundation%2520Model%2520for%2520Virtual%2520Sensors%26entry.906535625%3DLeon%2520G%25C3%25B6tz%2520and%2520Lars%2520Frederik%2520Peiss%2520and%2520Erik%2520Sauer%2520and%2520Andreas%2520Udo%2520Sass%2520and%2520Thorsten%2520Bagdonat%2520and%2520Stephan%2520G%25C3%25BCnnemann%2520and%2520Leo%2520Schwinn%26entry.1292438233%3DVirtual%2520sensors%2520use%2520machine%2520learning%2520to%2520predict%2520target%2520signals%2520from%2520available%2520measurements%252C%2520replacing%2520expensive%2520physical%2520sensors%2520in%2520critical%2520applications.%2520Existing%2520virtual%2520sensor%2520approaches%2520require%2520application-specific%2520models%2520with%2520hand-selected%2520inputs%2520for%2520each%2520sensor%252C%2520cannot%2520leverage%2520task%2520synergies%252C%2520and%2520lack%2520consistent%2520benchmarks.%2520At%2520the%2520same%2520time%252C%2520emerging%2520time%2520series%2520foundation%2520models%2520are%2520computationally%2520expensive%2520and%2520limited%2520to%2520predicting%2520their%2520input%2520signals%252C%2520making%2520them%2520incompatible%2520with%2520virtual%2520sensors.%2520We%2520introduce%2520the%2520first%2520foundation%2520model%2520for%2520virtual%2520sensors%2520addressing%2520both%2520limitations.%2520Our%2520unified%2520model%2520can%2520simultaneously%2520predict%2520diverse%2520virtual%2520sensors%2520exploiting%2520synergies%2520while%2520maintaining%2520computational%2520efficiency.%2520It%2520learns%2520relevant%2520input%2520signals%2520for%2520each%2520virtual%2520sensor%252C%2520eliminating%2520expert%2520knowledge%2520requirements%2520while%2520adding%2520explainability.%2520In%2520our%2520large-scale%2520evaluation%2520on%2520a%2520standard%2520benchmark%2520and%2520an%2520application-specific%2520dataset%2520with%2520over%252018%2520billion%2520samples%252C%2520our%2520architecture%2520achieves%2520415x%2520reduction%2520in%2520computation%2520time%2520and%2520951x%2520reduction%2520in%2520memory%2520requirements%252C%2520while%2520maintaining%2520or%2520even%2520improving%2520predictive%2520quality%2520compared%2520to%2520baselines.%2520Our%2520model%2520scales%2520gracefully%2520to%2520hundreds%2520of%2520virtual%2520sensors%2520with%2520nearly%2520constant%2520parameter%2520count%252C%2520enabling%2520practical%2520deployment%2520in%2520large-scale%2520sensor%2520networks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20634v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Foundation%20Model%20for%20Virtual%20Sensors&entry.906535625=Leon%20G%C3%B6tz%20and%20Lars%20Frederik%20Peiss%20and%20Erik%20Sauer%20and%20Andreas%20Udo%20Sass%20and%20Thorsten%20Bagdonat%20and%20Stephan%20G%C3%BCnnemann%20and%20Leo%20Schwinn&entry.1292438233=Virtual%20sensors%20use%20machine%20learning%20to%20predict%20target%20signals%20from%20available%20measurements%2C%20replacing%20expensive%20physical%20sensors%20in%20critical%20applications.%20Existing%20virtual%20sensor%20approaches%20require%20application-specific%20models%20with%20hand-selected%20inputs%20for%20each%20sensor%2C%20cannot%20leverage%20task%20synergies%2C%20and%20lack%20consistent%20benchmarks.%20At%20the%20same%20time%2C%20emerging%20time%20series%20foundation%20models%20are%20computationally%20expensive%20and%20limited%20to%20predicting%20their%20input%20signals%2C%20making%20them%20incompatible%20with%20virtual%20sensors.%20We%20introduce%20the%20first%20foundation%20model%20for%20virtual%20sensors%20addressing%20both%20limitations.%20Our%20unified%20model%20can%20simultaneously%20predict%20diverse%20virtual%20sensors%20exploiting%20synergies%20while%20maintaining%20computational%20efficiency.%20It%20learns%20relevant%20input%20signals%20for%20each%20virtual%20sensor%2C%20eliminating%20expert%20knowledge%20requirements%20while%20adding%20explainability.%20In%20our%20large-scale%20evaluation%20on%20a%20standard%20benchmark%20and%20an%20application-specific%20dataset%20with%20over%2018%20billion%20samples%2C%20our%20architecture%20achieves%20415x%20reduction%20in%20computation%20time%20and%20951x%20reduction%20in%20memory%20requirements%2C%20while%20maintaining%20or%20even%20improving%20predictive%20quality%20compared%20to%20baselines.%20Our%20model%20scales%20gracefully%20to%20hundreds%20of%20virtual%20sensors%20with%20nearly%20constant%20parameter%20count%2C%20enabling%20practical%20deployment%20in%20large-scale%20sensor%20networks.&entry.1838667208=http%3A//arxiv.org/abs/2601.20634v1&entry.124074799=Read"},
{"title": "Federated k-Means over Networks", "author": "Xu Yang and Salvatore Rastelli and Alexander Jung", "abstract": "We study federated clustering, where interconnected devices collaboratively cluster the data points of private local datasets. Focusing on hard clustering via the k-means principle, we formulate federated k-means as an instance of generalized total variation minimization (GTVMin). This leads to a federated k-means algorithm in which each device updates its local cluster centroids by solving a regularized k-means problem with a regularizer that enforces consistency between neighbouring devices. The resulting algorithm is privacy-friendly, as only aggregated information is exchanged.", "link": "http://arxiv.org/abs/2510.09718v2", "date": "2026-01-28", "relevancy": 2.1151, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4488}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4169}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20k-Means%20over%20Networks&body=Title%3A%20Federated%20k-Means%20over%20Networks%0AAuthor%3A%20Xu%20Yang%20and%20Salvatore%20Rastelli%20and%20Alexander%20Jung%0AAbstract%3A%20We%20study%20federated%20clustering%2C%20where%20interconnected%20devices%20collaboratively%20cluster%20the%20data%20points%20of%20private%20local%20datasets.%20Focusing%20on%20hard%20clustering%20via%20the%20k-means%20principle%2C%20we%20formulate%20federated%20k-means%20as%20an%20instance%20of%20generalized%20total%20variation%20minimization%20%28GTVMin%29.%20This%20leads%20to%20a%20federated%20k-means%20algorithm%20in%20which%20each%20device%20updates%20its%20local%20cluster%20centroids%20by%20solving%20a%20regularized%20k-means%20problem%20with%20a%20regularizer%20that%20enforces%20consistency%20between%20neighbouring%20devices.%20The%20resulting%20algorithm%20is%20privacy-friendly%2C%20as%20only%20aggregated%20information%20is%20exchanged.%0ALink%3A%20http%3A//arxiv.org/abs/2510.09718v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520k-Means%2520over%2520Networks%26entry.906535625%3DXu%2520Yang%2520and%2520Salvatore%2520Rastelli%2520and%2520Alexander%2520Jung%26entry.1292438233%3DWe%2520study%2520federated%2520clustering%252C%2520where%2520interconnected%2520devices%2520collaboratively%2520cluster%2520the%2520data%2520points%2520of%2520private%2520local%2520datasets.%2520Focusing%2520on%2520hard%2520clustering%2520via%2520the%2520k-means%2520principle%252C%2520we%2520formulate%2520federated%2520k-means%2520as%2520an%2520instance%2520of%2520generalized%2520total%2520variation%2520minimization%2520%2528GTVMin%2529.%2520This%2520leads%2520to%2520a%2520federated%2520k-means%2520algorithm%2520in%2520which%2520each%2520device%2520updates%2520its%2520local%2520cluster%2520centroids%2520by%2520solving%2520a%2520regularized%2520k-means%2520problem%2520with%2520a%2520regularizer%2520that%2520enforces%2520consistency%2520between%2520neighbouring%2520devices.%2520The%2520resulting%2520algorithm%2520is%2520privacy-friendly%252C%2520as%2520only%2520aggregated%2520information%2520is%2520exchanged.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.09718v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20k-Means%20over%20Networks&entry.906535625=Xu%20Yang%20and%20Salvatore%20Rastelli%20and%20Alexander%20Jung&entry.1292438233=We%20study%20federated%20clustering%2C%20where%20interconnected%20devices%20collaboratively%20cluster%20the%20data%20points%20of%20private%20local%20datasets.%20Focusing%20on%20hard%20clustering%20via%20the%20k-means%20principle%2C%20we%20formulate%20federated%20k-means%20as%20an%20instance%20of%20generalized%20total%20variation%20minimization%20%28GTVMin%29.%20This%20leads%20to%20a%20federated%20k-means%20algorithm%20in%20which%20each%20device%20updates%20its%20local%20cluster%20centroids%20by%20solving%20a%20regularized%20k-means%20problem%20with%20a%20regularizer%20that%20enforces%20consistency%20between%20neighbouring%20devices.%20The%20resulting%20algorithm%20is%20privacy-friendly%2C%20as%20only%20aggregated%20information%20is%20exchanged.&entry.1838667208=http%3A//arxiv.org/abs/2510.09718v2&entry.124074799=Read"},
{"title": "Learning Contextual Runtime Monitors for Safe AI-Based Autonomy", "author": "Alejandro Luque-Cerpa and Mengyuan Wang and Emil Carlsson and Sanjit A. Seshia and Devdatt Dubhashi and Hazem Torfah", "abstract": "We introduce a novel framework for learning context-aware runtime monitors for AI-based control ensembles. Machine-learning (ML) controllers are increasingly deployed in (autonomous) cyber-physical systems because of their ability to solve complex decision-making tasks. However, their accuracy can degrade sharply in unfamiliar environments, creating significant safety concerns. Traditional ensemble methods aim to improve robustness by averaging or voting across multiple controllers, yet this often dilutes the specialized strengths that individual controllers exhibit in different operating contexts. We argue that, rather than blending controller outputs, a monitoring framework should identify and exploit these contextual strengths. In this paper, we reformulate the design of safe AI-based control ensembles as a contextual monitoring problem. A monitor continuously observes the system's context and selects the controller best suited to the current conditions. To achieve this, we cast monitor learning as a contextual learning task and draw on techniques from contextual multi-armed bandits. Our approach comes with two key benefits: (1) theoretical safety guarantees during controller selection, and (2) improved utilization of controller diversity. We validate our framework in two simulated autonomous driving scenarios, demonstrating significant improvements in both safety and performance compared to non-contextual baselines.", "link": "http://arxiv.org/abs/2601.20666v1", "date": "2026-01-28", "relevancy": 2.1136, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5554}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5245}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Contextual%20Runtime%20Monitors%20for%20Safe%20AI-Based%20Autonomy&body=Title%3A%20Learning%20Contextual%20Runtime%20Monitors%20for%20Safe%20AI-Based%20Autonomy%0AAuthor%3A%20Alejandro%20Luque-Cerpa%20and%20Mengyuan%20Wang%20and%20Emil%20Carlsson%20and%20Sanjit%20A.%20Seshia%20and%20Devdatt%20Dubhashi%20and%20Hazem%20Torfah%0AAbstract%3A%20We%20introduce%20a%20novel%20framework%20for%20learning%20context-aware%20runtime%20monitors%20for%20AI-based%20control%20ensembles.%20Machine-learning%20%28ML%29%20controllers%20are%20increasingly%20deployed%20in%20%28autonomous%29%20cyber-physical%20systems%20because%20of%20their%20ability%20to%20solve%20complex%20decision-making%20tasks.%20However%2C%20their%20accuracy%20can%20degrade%20sharply%20in%20unfamiliar%20environments%2C%20creating%20significant%20safety%20concerns.%20Traditional%20ensemble%20methods%20aim%20to%20improve%20robustness%20by%20averaging%20or%20voting%20across%20multiple%20controllers%2C%20yet%20this%20often%20dilutes%20the%20specialized%20strengths%20that%20individual%20controllers%20exhibit%20in%20different%20operating%20contexts.%20We%20argue%20that%2C%20rather%20than%20blending%20controller%20outputs%2C%20a%20monitoring%20framework%20should%20identify%20and%20exploit%20these%20contextual%20strengths.%20In%20this%20paper%2C%20we%20reformulate%20the%20design%20of%20safe%20AI-based%20control%20ensembles%20as%20a%20contextual%20monitoring%20problem.%20A%20monitor%20continuously%20observes%20the%20system%27s%20context%20and%20selects%20the%20controller%20best%20suited%20to%20the%20current%20conditions.%20To%20achieve%20this%2C%20we%20cast%20monitor%20learning%20as%20a%20contextual%20learning%20task%20and%20draw%20on%20techniques%20from%20contextual%20multi-armed%20bandits.%20Our%20approach%20comes%20with%20two%20key%20benefits%3A%20%281%29%20theoretical%20safety%20guarantees%20during%20controller%20selection%2C%20and%20%282%29%20improved%20utilization%20of%20controller%20diversity.%20We%20validate%20our%20framework%20in%20two%20simulated%20autonomous%20driving%20scenarios%2C%20demonstrating%20significant%20improvements%20in%20both%20safety%20and%20performance%20compared%20to%20non-contextual%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Contextual%2520Runtime%2520Monitors%2520for%2520Safe%2520AI-Based%2520Autonomy%26entry.906535625%3DAlejandro%2520Luque-Cerpa%2520and%2520Mengyuan%2520Wang%2520and%2520Emil%2520Carlsson%2520and%2520Sanjit%2520A.%2520Seshia%2520and%2520Devdatt%2520Dubhashi%2520and%2520Hazem%2520Torfah%26entry.1292438233%3DWe%2520introduce%2520a%2520novel%2520framework%2520for%2520learning%2520context-aware%2520runtime%2520monitors%2520for%2520AI-based%2520control%2520ensembles.%2520Machine-learning%2520%2528ML%2529%2520controllers%2520are%2520increasingly%2520deployed%2520in%2520%2528autonomous%2529%2520cyber-physical%2520systems%2520because%2520of%2520their%2520ability%2520to%2520solve%2520complex%2520decision-making%2520tasks.%2520However%252C%2520their%2520accuracy%2520can%2520degrade%2520sharply%2520in%2520unfamiliar%2520environments%252C%2520creating%2520significant%2520safety%2520concerns.%2520Traditional%2520ensemble%2520methods%2520aim%2520to%2520improve%2520robustness%2520by%2520averaging%2520or%2520voting%2520across%2520multiple%2520controllers%252C%2520yet%2520this%2520often%2520dilutes%2520the%2520specialized%2520strengths%2520that%2520individual%2520controllers%2520exhibit%2520in%2520different%2520operating%2520contexts.%2520We%2520argue%2520that%252C%2520rather%2520than%2520blending%2520controller%2520outputs%252C%2520a%2520monitoring%2520framework%2520should%2520identify%2520and%2520exploit%2520these%2520contextual%2520strengths.%2520In%2520this%2520paper%252C%2520we%2520reformulate%2520the%2520design%2520of%2520safe%2520AI-based%2520control%2520ensembles%2520as%2520a%2520contextual%2520monitoring%2520problem.%2520A%2520monitor%2520continuously%2520observes%2520the%2520system%2527s%2520context%2520and%2520selects%2520the%2520controller%2520best%2520suited%2520to%2520the%2520current%2520conditions.%2520To%2520achieve%2520this%252C%2520we%2520cast%2520monitor%2520learning%2520as%2520a%2520contextual%2520learning%2520task%2520and%2520draw%2520on%2520techniques%2520from%2520contextual%2520multi-armed%2520bandits.%2520Our%2520approach%2520comes%2520with%2520two%2520key%2520benefits%253A%2520%25281%2529%2520theoretical%2520safety%2520guarantees%2520during%2520controller%2520selection%252C%2520and%2520%25282%2529%2520improved%2520utilization%2520of%2520controller%2520diversity.%2520We%2520validate%2520our%2520framework%2520in%2520two%2520simulated%2520autonomous%2520driving%2520scenarios%252C%2520demonstrating%2520significant%2520improvements%2520in%2520both%2520safety%2520and%2520performance%2520compared%2520to%2520non-contextual%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Contextual%20Runtime%20Monitors%20for%20Safe%20AI-Based%20Autonomy&entry.906535625=Alejandro%20Luque-Cerpa%20and%20Mengyuan%20Wang%20and%20Emil%20Carlsson%20and%20Sanjit%20A.%20Seshia%20and%20Devdatt%20Dubhashi%20and%20Hazem%20Torfah&entry.1292438233=We%20introduce%20a%20novel%20framework%20for%20learning%20context-aware%20runtime%20monitors%20for%20AI-based%20control%20ensembles.%20Machine-learning%20%28ML%29%20controllers%20are%20increasingly%20deployed%20in%20%28autonomous%29%20cyber-physical%20systems%20because%20of%20their%20ability%20to%20solve%20complex%20decision-making%20tasks.%20However%2C%20their%20accuracy%20can%20degrade%20sharply%20in%20unfamiliar%20environments%2C%20creating%20significant%20safety%20concerns.%20Traditional%20ensemble%20methods%20aim%20to%20improve%20robustness%20by%20averaging%20or%20voting%20across%20multiple%20controllers%2C%20yet%20this%20often%20dilutes%20the%20specialized%20strengths%20that%20individual%20controllers%20exhibit%20in%20different%20operating%20contexts.%20We%20argue%20that%2C%20rather%20than%20blending%20controller%20outputs%2C%20a%20monitoring%20framework%20should%20identify%20and%20exploit%20these%20contextual%20strengths.%20In%20this%20paper%2C%20we%20reformulate%20the%20design%20of%20safe%20AI-based%20control%20ensembles%20as%20a%20contextual%20monitoring%20problem.%20A%20monitor%20continuously%20observes%20the%20system%27s%20context%20and%20selects%20the%20controller%20best%20suited%20to%20the%20current%20conditions.%20To%20achieve%20this%2C%20we%20cast%20monitor%20learning%20as%20a%20contextual%20learning%20task%20and%20draw%20on%20techniques%20from%20contextual%20multi-armed%20bandits.%20Our%20approach%20comes%20with%20two%20key%20benefits%3A%20%281%29%20theoretical%20safety%20guarantees%20during%20controller%20selection%2C%20and%20%282%29%20improved%20utilization%20of%20controller%20diversity.%20We%20validate%20our%20framework%20in%20two%20simulated%20autonomous%20driving%20scenarios%2C%20demonstrating%20significant%20improvements%20in%20both%20safety%20and%20performance%20compared%20to%20non-contextual%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2601.20666v1&entry.124074799=Read"},
{"title": "Decoupling Perception and Calibration: Label-Efficient Image Quality Assessment Framework", "author": "Xinyue Li and Zhichao Zhang and Zhiming Xu and Shubo Xu and Xiongkuo Min and Yitong Chen and Guangtao Zhai", "abstract": "Recent multimodal large language models (MLLMs) have demonstrated strong capabilities in image quality assessment (IQA) tasks. However, adapting such large-scale models is computationally expensive and still relies on substantial Mean Opinion Score (MOS) annotations. We argue that for MLLM-based IQA, the core bottleneck lies not in the quality perception capacity of MLLMs, but in MOS scale calibration. Therefore, we propose LEAF, a Label-Efficient Image Quality Assessment Framework that distills perceptual quality priors from an MLLM teacher into a lightweight student regressor, enabling MOS calibration with minimal human supervision. Specifically, the teacher conducts dense supervision through point-wise judgments and pair-wise preferences, with an estimate of decision reliability. Guided by these signals, the student learns the teacher's quality perception patterns through joint distillation and is calibrated on a small MOS subset to align with human annotations. Experiments on both user-generated and AI-generated IQA benchmarks demonstrate that our method significantly reduces the need for human annotations while maintaining strong MOS-aligned correlations, making lightweight IQA practical under limited annotation budgets.", "link": "http://arxiv.org/abs/2601.20689v1", "date": "2026-01-28", "relevancy": 2.0972, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.591}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5141}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoupling%20Perception%20and%20Calibration%3A%20Label-Efficient%20Image%20Quality%20Assessment%20Framework&body=Title%3A%20Decoupling%20Perception%20and%20Calibration%3A%20Label-Efficient%20Image%20Quality%20Assessment%20Framework%0AAuthor%3A%20Xinyue%20Li%20and%20Zhichao%20Zhang%20and%20Zhiming%20Xu%20and%20Shubo%20Xu%20and%20Xiongkuo%20Min%20and%20Yitong%20Chen%20and%20Guangtao%20Zhai%0AAbstract%3A%20Recent%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20demonstrated%20strong%20capabilities%20in%20image%20quality%20assessment%20%28IQA%29%20tasks.%20However%2C%20adapting%20such%20large-scale%20models%20is%20computationally%20expensive%20and%20still%20relies%20on%20substantial%20Mean%20Opinion%20Score%20%28MOS%29%20annotations.%20We%20argue%20that%20for%20MLLM-based%20IQA%2C%20the%20core%20bottleneck%20lies%20not%20in%20the%20quality%20perception%20capacity%20of%20MLLMs%2C%20but%20in%20MOS%20scale%20calibration.%20Therefore%2C%20we%20propose%20LEAF%2C%20a%20Label-Efficient%20Image%20Quality%20Assessment%20Framework%20that%20distills%20perceptual%20quality%20priors%20from%20an%20MLLM%20teacher%20into%20a%20lightweight%20student%20regressor%2C%20enabling%20MOS%20calibration%20with%20minimal%20human%20supervision.%20Specifically%2C%20the%20teacher%20conducts%20dense%20supervision%20through%20point-wise%20judgments%20and%20pair-wise%20preferences%2C%20with%20an%20estimate%20of%20decision%20reliability.%20Guided%20by%20these%20signals%2C%20the%20student%20learns%20the%20teacher%27s%20quality%20perception%20patterns%20through%20joint%20distillation%20and%20is%20calibrated%20on%20a%20small%20MOS%20subset%20to%20align%20with%20human%20annotations.%20Experiments%20on%20both%20user-generated%20and%20AI-generated%20IQA%20benchmarks%20demonstrate%20that%20our%20method%20significantly%20reduces%20the%20need%20for%20human%20annotations%20while%20maintaining%20strong%20MOS-aligned%20correlations%2C%20making%20lightweight%20IQA%20practical%20under%20limited%20annotation%20budgets.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20689v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoupling%2520Perception%2520and%2520Calibration%253A%2520Label-Efficient%2520Image%2520Quality%2520Assessment%2520Framework%26entry.906535625%3DXinyue%2520Li%2520and%2520Zhichao%2520Zhang%2520and%2520Zhiming%2520Xu%2520and%2520Shubo%2520Xu%2520and%2520Xiongkuo%2520Min%2520and%2520Yitong%2520Chen%2520and%2520Guangtao%2520Zhai%26entry.1292438233%3DRecent%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520strong%2520capabilities%2520in%2520image%2520quality%2520assessment%2520%2528IQA%2529%2520tasks.%2520However%252C%2520adapting%2520such%2520large-scale%2520models%2520is%2520computationally%2520expensive%2520and%2520still%2520relies%2520on%2520substantial%2520Mean%2520Opinion%2520Score%2520%2528MOS%2529%2520annotations.%2520We%2520argue%2520that%2520for%2520MLLM-based%2520IQA%252C%2520the%2520core%2520bottleneck%2520lies%2520not%2520in%2520the%2520quality%2520perception%2520capacity%2520of%2520MLLMs%252C%2520but%2520in%2520MOS%2520scale%2520calibration.%2520Therefore%252C%2520we%2520propose%2520LEAF%252C%2520a%2520Label-Efficient%2520Image%2520Quality%2520Assessment%2520Framework%2520that%2520distills%2520perceptual%2520quality%2520priors%2520from%2520an%2520MLLM%2520teacher%2520into%2520a%2520lightweight%2520student%2520regressor%252C%2520enabling%2520MOS%2520calibration%2520with%2520minimal%2520human%2520supervision.%2520Specifically%252C%2520the%2520teacher%2520conducts%2520dense%2520supervision%2520through%2520point-wise%2520judgments%2520and%2520pair-wise%2520preferences%252C%2520with%2520an%2520estimate%2520of%2520decision%2520reliability.%2520Guided%2520by%2520these%2520signals%252C%2520the%2520student%2520learns%2520the%2520teacher%2527s%2520quality%2520perception%2520patterns%2520through%2520joint%2520distillation%2520and%2520is%2520calibrated%2520on%2520a%2520small%2520MOS%2520subset%2520to%2520align%2520with%2520human%2520annotations.%2520Experiments%2520on%2520both%2520user-generated%2520and%2520AI-generated%2520IQA%2520benchmarks%2520demonstrate%2520that%2520our%2520method%2520significantly%2520reduces%2520the%2520need%2520for%2520human%2520annotations%2520while%2520maintaining%2520strong%2520MOS-aligned%2520correlations%252C%2520making%2520lightweight%2520IQA%2520practical%2520under%2520limited%2520annotation%2520budgets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20689v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoupling%20Perception%20and%20Calibration%3A%20Label-Efficient%20Image%20Quality%20Assessment%20Framework&entry.906535625=Xinyue%20Li%20and%20Zhichao%20Zhang%20and%20Zhiming%20Xu%20and%20Shubo%20Xu%20and%20Xiongkuo%20Min%20and%20Yitong%20Chen%20and%20Guangtao%20Zhai&entry.1292438233=Recent%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20demonstrated%20strong%20capabilities%20in%20image%20quality%20assessment%20%28IQA%29%20tasks.%20However%2C%20adapting%20such%20large-scale%20models%20is%20computationally%20expensive%20and%20still%20relies%20on%20substantial%20Mean%20Opinion%20Score%20%28MOS%29%20annotations.%20We%20argue%20that%20for%20MLLM-based%20IQA%2C%20the%20core%20bottleneck%20lies%20not%20in%20the%20quality%20perception%20capacity%20of%20MLLMs%2C%20but%20in%20MOS%20scale%20calibration.%20Therefore%2C%20we%20propose%20LEAF%2C%20a%20Label-Efficient%20Image%20Quality%20Assessment%20Framework%20that%20distills%20perceptual%20quality%20priors%20from%20an%20MLLM%20teacher%20into%20a%20lightweight%20student%20regressor%2C%20enabling%20MOS%20calibration%20with%20minimal%20human%20supervision.%20Specifically%2C%20the%20teacher%20conducts%20dense%20supervision%20through%20point-wise%20judgments%20and%20pair-wise%20preferences%2C%20with%20an%20estimate%20of%20decision%20reliability.%20Guided%20by%20these%20signals%2C%20the%20student%20learns%20the%20teacher%27s%20quality%20perception%20patterns%20through%20joint%20distillation%20and%20is%20calibrated%20on%20a%20small%20MOS%20subset%20to%20align%20with%20human%20annotations.%20Experiments%20on%20both%20user-generated%20and%20AI-generated%20IQA%20benchmarks%20demonstrate%20that%20our%20method%20significantly%20reduces%20the%20need%20for%20human%20annotations%20while%20maintaining%20strong%20MOS-aligned%20correlations%2C%20making%20lightweight%20IQA%20practical%20under%20limited%20annotation%20budgets.&entry.1838667208=http%3A//arxiv.org/abs/2601.20689v1&entry.124074799=Read"},
{"title": "Discrete Variational Autoencoding via Policy Search", "author": "Michael Drolet and Firas Al-Hafez and Aditya Bhatt and Jan Peters and Oleg Arenz", "abstract": "Discrete latent bottlenecks in variational autoencoders (VAEs) offer high bit efficiency and can be modeled with autoregressive discrete distributions, enabling parameter-efficient multimodal search with transformers. However, discrete random variables do not allow for exact differentiable parameterization; therefore, discrete VAEs typically rely on approximations, such as Gumbel-Softmax reparameterization or straight-through gradient estimates, or employ high-variance gradient-free methods such as REINFORCE that have had limited success on high-dimensional tasks such as image reconstruction. Inspired by popular techniques in policy search, we propose a training framework for discrete VAEs that leverages the natural gradient of a non-parametric encoder to update the parametric encoder without requiring reparameterization. Our method, combined with automatic step size adaptation and a transformer-based encoder, scales to challenging datasets such as ImageNet and outperforms both approximate reparameterization methods and quantization-based discrete autoencoders in reconstructing high-dimensional data from compact latent spaces.", "link": "http://arxiv.org/abs/2509.24716v2", "date": "2026-01-28", "relevancy": 2.0921, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.559}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.515}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discrete%20Variational%20Autoencoding%20via%20Policy%20Search&body=Title%3A%20Discrete%20Variational%20Autoencoding%20via%20Policy%20Search%0AAuthor%3A%20Michael%20Drolet%20and%20Firas%20Al-Hafez%20and%20Aditya%20Bhatt%20and%20Jan%20Peters%20and%20Oleg%20Arenz%0AAbstract%3A%20Discrete%20latent%20bottlenecks%20in%20variational%20autoencoders%20%28VAEs%29%20offer%20high%20bit%20efficiency%20and%20can%20be%20modeled%20with%20autoregressive%20discrete%20distributions%2C%20enabling%20parameter-efficient%20multimodal%20search%20with%20transformers.%20However%2C%20discrete%20random%20variables%20do%20not%20allow%20for%20exact%20differentiable%20parameterization%3B%20therefore%2C%20discrete%20VAEs%20typically%20rely%20on%20approximations%2C%20such%20as%20Gumbel-Softmax%20reparameterization%20or%20straight-through%20gradient%20estimates%2C%20or%20employ%20high-variance%20gradient-free%20methods%20such%20as%20REINFORCE%20that%20have%20had%20limited%20success%20on%20high-dimensional%20tasks%20such%20as%20image%20reconstruction.%20Inspired%20by%20popular%20techniques%20in%20policy%20search%2C%20we%20propose%20a%20training%20framework%20for%20discrete%20VAEs%20that%20leverages%20the%20natural%20gradient%20of%20a%20non-parametric%20encoder%20to%20update%20the%20parametric%20encoder%20without%20requiring%20reparameterization.%20Our%20method%2C%20combined%20with%20automatic%20step%20size%20adaptation%20and%20a%20transformer-based%20encoder%2C%20scales%20to%20challenging%20datasets%20such%20as%20ImageNet%20and%20outperforms%20both%20approximate%20reparameterization%20methods%20and%20quantization-based%20discrete%20autoencoders%20in%20reconstructing%20high-dimensional%20data%20from%20compact%20latent%20spaces.%0ALink%3A%20http%3A//arxiv.org/abs/2509.24716v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscrete%2520Variational%2520Autoencoding%2520via%2520Policy%2520Search%26entry.906535625%3DMichael%2520Drolet%2520and%2520Firas%2520Al-Hafez%2520and%2520Aditya%2520Bhatt%2520and%2520Jan%2520Peters%2520and%2520Oleg%2520Arenz%26entry.1292438233%3DDiscrete%2520latent%2520bottlenecks%2520in%2520variational%2520autoencoders%2520%2528VAEs%2529%2520offer%2520high%2520bit%2520efficiency%2520and%2520can%2520be%2520modeled%2520with%2520autoregressive%2520discrete%2520distributions%252C%2520enabling%2520parameter-efficient%2520multimodal%2520search%2520with%2520transformers.%2520However%252C%2520discrete%2520random%2520variables%2520do%2520not%2520allow%2520for%2520exact%2520differentiable%2520parameterization%253B%2520therefore%252C%2520discrete%2520VAEs%2520typically%2520rely%2520on%2520approximations%252C%2520such%2520as%2520Gumbel-Softmax%2520reparameterization%2520or%2520straight-through%2520gradient%2520estimates%252C%2520or%2520employ%2520high-variance%2520gradient-free%2520methods%2520such%2520as%2520REINFORCE%2520that%2520have%2520had%2520limited%2520success%2520on%2520high-dimensional%2520tasks%2520such%2520as%2520image%2520reconstruction.%2520Inspired%2520by%2520popular%2520techniques%2520in%2520policy%2520search%252C%2520we%2520propose%2520a%2520training%2520framework%2520for%2520discrete%2520VAEs%2520that%2520leverages%2520the%2520natural%2520gradient%2520of%2520a%2520non-parametric%2520encoder%2520to%2520update%2520the%2520parametric%2520encoder%2520without%2520requiring%2520reparameterization.%2520Our%2520method%252C%2520combined%2520with%2520automatic%2520step%2520size%2520adaptation%2520and%2520a%2520transformer-based%2520encoder%252C%2520scales%2520to%2520challenging%2520datasets%2520such%2520as%2520ImageNet%2520and%2520outperforms%2520both%2520approximate%2520reparameterization%2520methods%2520and%2520quantization-based%2520discrete%2520autoencoders%2520in%2520reconstructing%2520high-dimensional%2520data%2520from%2520compact%2520latent%2520spaces.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.24716v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discrete%20Variational%20Autoencoding%20via%20Policy%20Search&entry.906535625=Michael%20Drolet%20and%20Firas%20Al-Hafez%20and%20Aditya%20Bhatt%20and%20Jan%20Peters%20and%20Oleg%20Arenz&entry.1292438233=Discrete%20latent%20bottlenecks%20in%20variational%20autoencoders%20%28VAEs%29%20offer%20high%20bit%20efficiency%20and%20can%20be%20modeled%20with%20autoregressive%20discrete%20distributions%2C%20enabling%20parameter-efficient%20multimodal%20search%20with%20transformers.%20However%2C%20discrete%20random%20variables%20do%20not%20allow%20for%20exact%20differentiable%20parameterization%3B%20therefore%2C%20discrete%20VAEs%20typically%20rely%20on%20approximations%2C%20such%20as%20Gumbel-Softmax%20reparameterization%20or%20straight-through%20gradient%20estimates%2C%20or%20employ%20high-variance%20gradient-free%20methods%20such%20as%20REINFORCE%20that%20have%20had%20limited%20success%20on%20high-dimensional%20tasks%20such%20as%20image%20reconstruction.%20Inspired%20by%20popular%20techniques%20in%20policy%20search%2C%20we%20propose%20a%20training%20framework%20for%20discrete%20VAEs%20that%20leverages%20the%20natural%20gradient%20of%20a%20non-parametric%20encoder%20to%20update%20the%20parametric%20encoder%20without%20requiring%20reparameterization.%20Our%20method%2C%20combined%20with%20automatic%20step%20size%20adaptation%20and%20a%20transformer-based%20encoder%2C%20scales%20to%20challenging%20datasets%20such%20as%20ImageNet%20and%20outperforms%20both%20approximate%20reparameterization%20methods%20and%20quantization-based%20discrete%20autoencoders%20in%20reconstructing%20high-dimensional%20data%20from%20compact%20latent%20spaces.&entry.1838667208=http%3A//arxiv.org/abs/2509.24716v2&entry.124074799=Read"},
{"title": "DIVERSE: Disagreement-Inducing Vector Evolution for Rashomon Set Exploration", "author": "Gilles Eerlings and Brent Zoomers and Jori Liesenborgs and Gustavo Rovelo Ruiz and Kris Luyten", "abstract": "We propose DIVERSE, a framework for systematically exploring the Rashomon set of deep neural networks, the collection of models that match a reference model's accuracy while differing in their predictive behavior. DIVERSE augments a pretrained model with Feature-wise Linear Modulation (FiLM) layers and uses Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to search a latent modulation space, generating diverse model variants without retraining or gradient access. Across MNIST, PneumoniaMNIST, and CIFAR-10, DIVERSE uncovers multiple high-performing yet functionally distinct models. Our experiments show that DIVERSE offers a competitive and efficient exploration of the Rashomon set, making it feasible to construct diverse sets that maintain robustness and performance while supporting well-balanced model multiplicity. While retraining remains the baseline to generate Rashomon sets, DIVERSE achieves comparable diversity at reduced computational cost.", "link": "http://arxiv.org/abs/2601.20627v1", "date": "2026-01-28", "relevancy": 2.0868, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5221}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5216}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DIVERSE%3A%20Disagreement-Inducing%20Vector%20Evolution%20for%20Rashomon%20Set%20Exploration&body=Title%3A%20DIVERSE%3A%20Disagreement-Inducing%20Vector%20Evolution%20for%20Rashomon%20Set%20Exploration%0AAuthor%3A%20Gilles%20Eerlings%20and%20Brent%20Zoomers%20and%20Jori%20Liesenborgs%20and%20Gustavo%20Rovelo%20Ruiz%20and%20Kris%20Luyten%0AAbstract%3A%20We%20propose%20DIVERSE%2C%20a%20framework%20for%20systematically%20exploring%20the%20Rashomon%20set%20of%20deep%20neural%20networks%2C%20the%20collection%20of%20models%20that%20match%20a%20reference%20model%27s%20accuracy%20while%20differing%20in%20their%20predictive%20behavior.%20DIVERSE%20augments%20a%20pretrained%20model%20with%20Feature-wise%20Linear%20Modulation%20%28FiLM%29%20layers%20and%20uses%20Covariance%20Matrix%20Adaptation%20Evolution%20Strategy%20%28CMA-ES%29%20to%20search%20a%20latent%20modulation%20space%2C%20generating%20diverse%20model%20variants%20without%20retraining%20or%20gradient%20access.%20Across%20MNIST%2C%20PneumoniaMNIST%2C%20and%20CIFAR-10%2C%20DIVERSE%20uncovers%20multiple%20high-performing%20yet%20functionally%20distinct%20models.%20Our%20experiments%20show%20that%20DIVERSE%20offers%20a%20competitive%20and%20efficient%20exploration%20of%20the%20Rashomon%20set%2C%20making%20it%20feasible%20to%20construct%20diverse%20sets%20that%20maintain%20robustness%20and%20performance%20while%20supporting%20well-balanced%20model%20multiplicity.%20While%20retraining%20remains%20the%20baseline%20to%20generate%20Rashomon%20sets%2C%20DIVERSE%20achieves%20comparable%20diversity%20at%20reduced%20computational%20cost.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20627v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDIVERSE%253A%2520Disagreement-Inducing%2520Vector%2520Evolution%2520for%2520Rashomon%2520Set%2520Exploration%26entry.906535625%3DGilles%2520Eerlings%2520and%2520Brent%2520Zoomers%2520and%2520Jori%2520Liesenborgs%2520and%2520Gustavo%2520Rovelo%2520Ruiz%2520and%2520Kris%2520Luyten%26entry.1292438233%3DWe%2520propose%2520DIVERSE%252C%2520a%2520framework%2520for%2520systematically%2520exploring%2520the%2520Rashomon%2520set%2520of%2520deep%2520neural%2520networks%252C%2520the%2520collection%2520of%2520models%2520that%2520match%2520a%2520reference%2520model%2527s%2520accuracy%2520while%2520differing%2520in%2520their%2520predictive%2520behavior.%2520DIVERSE%2520augments%2520a%2520pretrained%2520model%2520with%2520Feature-wise%2520Linear%2520Modulation%2520%2528FiLM%2529%2520layers%2520and%2520uses%2520Covariance%2520Matrix%2520Adaptation%2520Evolution%2520Strategy%2520%2528CMA-ES%2529%2520to%2520search%2520a%2520latent%2520modulation%2520space%252C%2520generating%2520diverse%2520model%2520variants%2520without%2520retraining%2520or%2520gradient%2520access.%2520Across%2520MNIST%252C%2520PneumoniaMNIST%252C%2520and%2520CIFAR-10%252C%2520DIVERSE%2520uncovers%2520multiple%2520high-performing%2520yet%2520functionally%2520distinct%2520models.%2520Our%2520experiments%2520show%2520that%2520DIVERSE%2520offers%2520a%2520competitive%2520and%2520efficient%2520exploration%2520of%2520the%2520Rashomon%2520set%252C%2520making%2520it%2520feasible%2520to%2520construct%2520diverse%2520sets%2520that%2520maintain%2520robustness%2520and%2520performance%2520while%2520supporting%2520well-balanced%2520model%2520multiplicity.%2520While%2520retraining%2520remains%2520the%2520baseline%2520to%2520generate%2520Rashomon%2520sets%252C%2520DIVERSE%2520achieves%2520comparable%2520diversity%2520at%2520reduced%2520computational%2520cost.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20627v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DIVERSE%3A%20Disagreement-Inducing%20Vector%20Evolution%20for%20Rashomon%20Set%20Exploration&entry.906535625=Gilles%20Eerlings%20and%20Brent%20Zoomers%20and%20Jori%20Liesenborgs%20and%20Gustavo%20Rovelo%20Ruiz%20and%20Kris%20Luyten&entry.1292438233=We%20propose%20DIVERSE%2C%20a%20framework%20for%20systematically%20exploring%20the%20Rashomon%20set%20of%20deep%20neural%20networks%2C%20the%20collection%20of%20models%20that%20match%20a%20reference%20model%27s%20accuracy%20while%20differing%20in%20their%20predictive%20behavior.%20DIVERSE%20augments%20a%20pretrained%20model%20with%20Feature-wise%20Linear%20Modulation%20%28FiLM%29%20layers%20and%20uses%20Covariance%20Matrix%20Adaptation%20Evolution%20Strategy%20%28CMA-ES%29%20to%20search%20a%20latent%20modulation%20space%2C%20generating%20diverse%20model%20variants%20without%20retraining%20or%20gradient%20access.%20Across%20MNIST%2C%20PneumoniaMNIST%2C%20and%20CIFAR-10%2C%20DIVERSE%20uncovers%20multiple%20high-performing%20yet%20functionally%20distinct%20models.%20Our%20experiments%20show%20that%20DIVERSE%20offers%20a%20competitive%20and%20efficient%20exploration%20of%20the%20Rashomon%20set%2C%20making%20it%20feasible%20to%20construct%20diverse%20sets%20that%20maintain%20robustness%20and%20performance%20while%20supporting%20well-balanced%20model%20multiplicity.%20While%20retraining%20remains%20the%20baseline%20to%20generate%20Rashomon%20sets%2C%20DIVERSE%20achieves%20comparable%20diversity%20at%20reduced%20computational%20cost.&entry.1838667208=http%3A//arxiv.org/abs/2601.20627v1&entry.124074799=Read"},
{"title": "Benchmarking Multimodal Large Language Models for Missing Modality Completion in Product Catalogues", "author": "Junchen Fu and Wenhao Deng and Kaiwen Zheng and Ioannis Arapakis and Yu Ye and Yongxin Ni and Joemon M. Jose and Xuri Ge", "abstract": "Missing-modality information on e-commerce platforms, such as absent product images or textual descriptions, often arises from annotation errors or incomplete metadata, impairing both product presentation and downstream applications such as recommendation systems. Motivated by the multimodal generative capabilities of recent Multimodal Large Language Models (MLLMs), this work investigates a fundamental yet underexplored question: can MLLMs generate missing modalities for products in e-commerce scenarios? We propose the Missing Modality Product Completion Benchmark (MMPCBench), which consists of two sub-benchmarks: a Content Quality Completion Benchmark and a Recommendation Benchmark.\n  We further evaluate six state-of-the-art MLLMs from the Qwen2.5-VL and Gemma-3 model families across nine real-world e-commerce categories, focusing on image-to-text and text-to-image completion tasks. Experimental results show that while MLLMs can capture high-level semantics, they struggle with fine-grained word-level and pixel- or patch-level alignment. In addition, performance varies substantially across product categories and model scales, and we observe no trivial correlation between model size and performance, in contrast to trends commonly reported in mainstream benchmarks. We also explore Group Relative Policy Optimization (GRPO) to better align MLLMs with this task. GRPO improves image-to-text completion but does not yield gains for text-to-image completion. Overall, these findings expose the limitations of current MLLMs in real-world cross-modal generation and represent an early step toward more effective missing-modality product completion.", "link": "http://arxiv.org/abs/2601.19750v2", "date": "2026-01-28", "relevancy": 2.0861, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5505}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5172}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Multimodal%20Large%20Language%20Models%20for%20Missing%20Modality%20Completion%20in%20Product%20Catalogues&body=Title%3A%20Benchmarking%20Multimodal%20Large%20Language%20Models%20for%20Missing%20Modality%20Completion%20in%20Product%20Catalogues%0AAuthor%3A%20Junchen%20Fu%20and%20Wenhao%20Deng%20and%20Kaiwen%20Zheng%20and%20Ioannis%20Arapakis%20and%20Yu%20Ye%20and%20Yongxin%20Ni%20and%20Joemon%20M.%20Jose%20and%20Xuri%20Ge%0AAbstract%3A%20Missing-modality%20information%20on%20e-commerce%20platforms%2C%20such%20as%20absent%20product%20images%20or%20textual%20descriptions%2C%20often%20arises%20from%20annotation%20errors%20or%20incomplete%20metadata%2C%20impairing%20both%20product%20presentation%20and%20downstream%20applications%20such%20as%20recommendation%20systems.%20Motivated%20by%20the%20multimodal%20generative%20capabilities%20of%20recent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20this%20work%20investigates%20a%20fundamental%20yet%20underexplored%20question%3A%20can%20MLLMs%20generate%20missing%20modalities%20for%20products%20in%20e-commerce%20scenarios%3F%20We%20propose%20the%20Missing%20Modality%20Product%20Completion%20Benchmark%20%28MMPCBench%29%2C%20which%20consists%20of%20two%20sub-benchmarks%3A%20a%20Content%20Quality%20Completion%20Benchmark%20and%20a%20Recommendation%20Benchmark.%0A%20%20We%20further%20evaluate%20six%20state-of-the-art%20MLLMs%20from%20the%20Qwen2.5-VL%20and%20Gemma-3%20model%20families%20across%20nine%20real-world%20e-commerce%20categories%2C%20focusing%20on%20image-to-text%20and%20text-to-image%20completion%20tasks.%20Experimental%20results%20show%20that%20while%20MLLMs%20can%20capture%20high-level%20semantics%2C%20they%20struggle%20with%20fine-grained%20word-level%20and%20pixel-%20or%20patch-level%20alignment.%20In%20addition%2C%20performance%20varies%20substantially%20across%20product%20categories%20and%20model%20scales%2C%20and%20we%20observe%20no%20trivial%20correlation%20between%20model%20size%20and%20performance%2C%20in%20contrast%20to%20trends%20commonly%20reported%20in%20mainstream%20benchmarks.%20We%20also%20explore%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20to%20better%20align%20MLLMs%20with%20this%20task.%20GRPO%20improves%20image-to-text%20completion%20but%20does%20not%20yield%20gains%20for%20text-to-image%20completion.%20Overall%2C%20these%20findings%20expose%20the%20limitations%20of%20current%20MLLMs%20in%20real-world%20cross-modal%20generation%20and%20represent%20an%20early%20step%20toward%20more%20effective%20missing-modality%20product%20completion.%0ALink%3A%20http%3A//arxiv.org/abs/2601.19750v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Multimodal%2520Large%2520Language%2520Models%2520for%2520Missing%2520Modality%2520Completion%2520in%2520Product%2520Catalogues%26entry.906535625%3DJunchen%2520Fu%2520and%2520Wenhao%2520Deng%2520and%2520Kaiwen%2520Zheng%2520and%2520Ioannis%2520Arapakis%2520and%2520Yu%2520Ye%2520and%2520Yongxin%2520Ni%2520and%2520Joemon%2520M.%2520Jose%2520and%2520Xuri%2520Ge%26entry.1292438233%3DMissing-modality%2520information%2520on%2520e-commerce%2520platforms%252C%2520such%2520as%2520absent%2520product%2520images%2520or%2520textual%2520descriptions%252C%2520often%2520arises%2520from%2520annotation%2520errors%2520or%2520incomplete%2520metadata%252C%2520impairing%2520both%2520product%2520presentation%2520and%2520downstream%2520applications%2520such%2520as%2520recommendation%2520systems.%2520Motivated%2520by%2520the%2520multimodal%2520generative%2520capabilities%2520of%2520recent%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520this%2520work%2520investigates%2520a%2520fundamental%2520yet%2520underexplored%2520question%253A%2520can%2520MLLMs%2520generate%2520missing%2520modalities%2520for%2520products%2520in%2520e-commerce%2520scenarios%253F%2520We%2520propose%2520the%2520Missing%2520Modality%2520Product%2520Completion%2520Benchmark%2520%2528MMPCBench%2529%252C%2520which%2520consists%2520of%2520two%2520sub-benchmarks%253A%2520a%2520Content%2520Quality%2520Completion%2520Benchmark%2520and%2520a%2520Recommendation%2520Benchmark.%250A%2520%2520We%2520further%2520evaluate%2520six%2520state-of-the-art%2520MLLMs%2520from%2520the%2520Qwen2.5-VL%2520and%2520Gemma-3%2520model%2520families%2520across%2520nine%2520real-world%2520e-commerce%2520categories%252C%2520focusing%2520on%2520image-to-text%2520and%2520text-to-image%2520completion%2520tasks.%2520Experimental%2520results%2520show%2520that%2520while%2520MLLMs%2520can%2520capture%2520high-level%2520semantics%252C%2520they%2520struggle%2520with%2520fine-grained%2520word-level%2520and%2520pixel-%2520or%2520patch-level%2520alignment.%2520In%2520addition%252C%2520performance%2520varies%2520substantially%2520across%2520product%2520categories%2520and%2520model%2520scales%252C%2520and%2520we%2520observe%2520no%2520trivial%2520correlation%2520between%2520model%2520size%2520and%2520performance%252C%2520in%2520contrast%2520to%2520trends%2520commonly%2520reported%2520in%2520mainstream%2520benchmarks.%2520We%2520also%2520explore%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520to%2520better%2520align%2520MLLMs%2520with%2520this%2520task.%2520GRPO%2520improves%2520image-to-text%2520completion%2520but%2520does%2520not%2520yield%2520gains%2520for%2520text-to-image%2520completion.%2520Overall%252C%2520these%2520findings%2520expose%2520the%2520limitations%2520of%2520current%2520MLLMs%2520in%2520real-world%2520cross-modal%2520generation%2520and%2520represent%2520an%2520early%2520step%2520toward%2520more%2520effective%2520missing-modality%2520product%2520completion.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.19750v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Multimodal%20Large%20Language%20Models%20for%20Missing%20Modality%20Completion%20in%20Product%20Catalogues&entry.906535625=Junchen%20Fu%20and%20Wenhao%20Deng%20and%20Kaiwen%20Zheng%20and%20Ioannis%20Arapakis%20and%20Yu%20Ye%20and%20Yongxin%20Ni%20and%20Joemon%20M.%20Jose%20and%20Xuri%20Ge&entry.1292438233=Missing-modality%20information%20on%20e-commerce%20platforms%2C%20such%20as%20absent%20product%20images%20or%20textual%20descriptions%2C%20often%20arises%20from%20annotation%20errors%20or%20incomplete%20metadata%2C%20impairing%20both%20product%20presentation%20and%20downstream%20applications%20such%20as%20recommendation%20systems.%20Motivated%20by%20the%20multimodal%20generative%20capabilities%20of%20recent%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20this%20work%20investigates%20a%20fundamental%20yet%20underexplored%20question%3A%20can%20MLLMs%20generate%20missing%20modalities%20for%20products%20in%20e-commerce%20scenarios%3F%20We%20propose%20the%20Missing%20Modality%20Product%20Completion%20Benchmark%20%28MMPCBench%29%2C%20which%20consists%20of%20two%20sub-benchmarks%3A%20a%20Content%20Quality%20Completion%20Benchmark%20and%20a%20Recommendation%20Benchmark.%0A%20%20We%20further%20evaluate%20six%20state-of-the-art%20MLLMs%20from%20the%20Qwen2.5-VL%20and%20Gemma-3%20model%20families%20across%20nine%20real-world%20e-commerce%20categories%2C%20focusing%20on%20image-to-text%20and%20text-to-image%20completion%20tasks.%20Experimental%20results%20show%20that%20while%20MLLMs%20can%20capture%20high-level%20semantics%2C%20they%20struggle%20with%20fine-grained%20word-level%20and%20pixel-%20or%20patch-level%20alignment.%20In%20addition%2C%20performance%20varies%20substantially%20across%20product%20categories%20and%20model%20scales%2C%20and%20we%20observe%20no%20trivial%20correlation%20between%20model%20size%20and%20performance%2C%20in%20contrast%20to%20trends%20commonly%20reported%20in%20mainstream%20benchmarks.%20We%20also%20explore%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20to%20better%20align%20MLLMs%20with%20this%20task.%20GRPO%20improves%20image-to-text%20completion%20but%20does%20not%20yield%20gains%20for%20text-to-image%20completion.%20Overall%2C%20these%20findings%20expose%20the%20limitations%20of%20current%20MLLMs%20in%20real-world%20cross-modal%20generation%20and%20represent%20an%20early%20step%20toward%20more%20effective%20missing-modality%20product%20completion.&entry.1838667208=http%3A//arxiv.org/abs/2601.19750v2&entry.124074799=Read"},
{"title": "Unsupervised Ensemble Learning Through Deep Energy-based Models", "author": "Ariel Maymon and Yanir Buznah and Uri Shaham", "abstract": "Unsupervised ensemble learning emerged to address the challenge of combining multiple learners' predictions without access to ground truth labels or additional data. This paradigm is crucial in scenarios where evaluating individual classifier performance or understanding their strengths is challenging due to limited information. We propose a novel deep energy-based method for constructing an accurate meta-learner using only the predictions of individual learners, potentially capable of capturing complex dependence structures between them. Our approach requires no labeled data, learner features, or problem-specific information, and has theoretical guarantees for when learners are conditionally independent. We demonstrate superior performance across diverse ensemble scenarios, including challenging mixture of experts settings. Our experiments span standard ensemble datasets and curated datasets designed to test how the model fuses expertise from multiple sources. These results highlight the potential of unsupervised ensemble learning to harness collective intelligence, especially in data-scarce or privacy-sensitive environments.", "link": "http://arxiv.org/abs/2601.20556v1", "date": "2026-01-28", "relevancy": 2.0836, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5535}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5252}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5035}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Ensemble%20Learning%20Through%20Deep%20Energy-based%20Models&body=Title%3A%20Unsupervised%20Ensemble%20Learning%20Through%20Deep%20Energy-based%20Models%0AAuthor%3A%20Ariel%20Maymon%20and%20Yanir%20Buznah%20and%20Uri%20Shaham%0AAbstract%3A%20Unsupervised%20ensemble%20learning%20emerged%20to%20address%20the%20challenge%20of%20combining%20multiple%20learners%27%20predictions%20without%20access%20to%20ground%20truth%20labels%20or%20additional%20data.%20This%20paradigm%20is%20crucial%20in%20scenarios%20where%20evaluating%20individual%20classifier%20performance%20or%20understanding%20their%20strengths%20is%20challenging%20due%20to%20limited%20information.%20We%20propose%20a%20novel%20deep%20energy-based%20method%20for%20constructing%20an%20accurate%20meta-learner%20using%20only%20the%20predictions%20of%20individual%20learners%2C%20potentially%20capable%20of%20capturing%20complex%20dependence%20structures%20between%20them.%20Our%20approach%20requires%20no%20labeled%20data%2C%20learner%20features%2C%20or%20problem-specific%20information%2C%20and%20has%20theoretical%20guarantees%20for%20when%20learners%20are%20conditionally%20independent.%20We%20demonstrate%20superior%20performance%20across%20diverse%20ensemble%20scenarios%2C%20including%20challenging%20mixture%20of%20experts%20settings.%20Our%20experiments%20span%20standard%20ensemble%20datasets%20and%20curated%20datasets%20designed%20to%20test%20how%20the%20model%20fuses%20expertise%20from%20multiple%20sources.%20These%20results%20highlight%20the%20potential%20of%20unsupervised%20ensemble%20learning%20to%20harness%20collective%20intelligence%2C%20especially%20in%20data-scarce%20or%20privacy-sensitive%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20556v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Ensemble%2520Learning%2520Through%2520Deep%2520Energy-based%2520Models%26entry.906535625%3DAriel%2520Maymon%2520and%2520Yanir%2520Buznah%2520and%2520Uri%2520Shaham%26entry.1292438233%3DUnsupervised%2520ensemble%2520learning%2520emerged%2520to%2520address%2520the%2520challenge%2520of%2520combining%2520multiple%2520learners%2527%2520predictions%2520without%2520access%2520to%2520ground%2520truth%2520labels%2520or%2520additional%2520data.%2520This%2520paradigm%2520is%2520crucial%2520in%2520scenarios%2520where%2520evaluating%2520individual%2520classifier%2520performance%2520or%2520understanding%2520their%2520strengths%2520is%2520challenging%2520due%2520to%2520limited%2520information.%2520We%2520propose%2520a%2520novel%2520deep%2520energy-based%2520method%2520for%2520constructing%2520an%2520accurate%2520meta-learner%2520using%2520only%2520the%2520predictions%2520of%2520individual%2520learners%252C%2520potentially%2520capable%2520of%2520capturing%2520complex%2520dependence%2520structures%2520between%2520them.%2520Our%2520approach%2520requires%2520no%2520labeled%2520data%252C%2520learner%2520features%252C%2520or%2520problem-specific%2520information%252C%2520and%2520has%2520theoretical%2520guarantees%2520for%2520when%2520learners%2520are%2520conditionally%2520independent.%2520We%2520demonstrate%2520superior%2520performance%2520across%2520diverse%2520ensemble%2520scenarios%252C%2520including%2520challenging%2520mixture%2520of%2520experts%2520settings.%2520Our%2520experiments%2520span%2520standard%2520ensemble%2520datasets%2520and%2520curated%2520datasets%2520designed%2520to%2520test%2520how%2520the%2520model%2520fuses%2520expertise%2520from%2520multiple%2520sources.%2520These%2520results%2520highlight%2520the%2520potential%2520of%2520unsupervised%2520ensemble%2520learning%2520to%2520harness%2520collective%2520intelligence%252C%2520especially%2520in%2520data-scarce%2520or%2520privacy-sensitive%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20556v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Ensemble%20Learning%20Through%20Deep%20Energy-based%20Models&entry.906535625=Ariel%20Maymon%20and%20Yanir%20Buznah%20and%20Uri%20Shaham&entry.1292438233=Unsupervised%20ensemble%20learning%20emerged%20to%20address%20the%20challenge%20of%20combining%20multiple%20learners%27%20predictions%20without%20access%20to%20ground%20truth%20labels%20or%20additional%20data.%20This%20paradigm%20is%20crucial%20in%20scenarios%20where%20evaluating%20individual%20classifier%20performance%20or%20understanding%20their%20strengths%20is%20challenging%20due%20to%20limited%20information.%20We%20propose%20a%20novel%20deep%20energy-based%20method%20for%20constructing%20an%20accurate%20meta-learner%20using%20only%20the%20predictions%20of%20individual%20learners%2C%20potentially%20capable%20of%20capturing%20complex%20dependence%20structures%20between%20them.%20Our%20approach%20requires%20no%20labeled%20data%2C%20learner%20features%2C%20or%20problem-specific%20information%2C%20and%20has%20theoretical%20guarantees%20for%20when%20learners%20are%20conditionally%20independent.%20We%20demonstrate%20superior%20performance%20across%20diverse%20ensemble%20scenarios%2C%20including%20challenging%20mixture%20of%20experts%20settings.%20Our%20experiments%20span%20standard%20ensemble%20datasets%20and%20curated%20datasets%20designed%20to%20test%20how%20the%20model%20fuses%20expertise%20from%20multiple%20sources.%20These%20results%20highlight%20the%20potential%20of%20unsupervised%20ensemble%20learning%20to%20harness%20collective%20intelligence%2C%20especially%20in%20data-scarce%20or%20privacy-sensitive%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2601.20556v1&entry.124074799=Read"},
{"title": "SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models", "author": "Sebastiano Monti and Carlo Nicolini and Gianni Pellegrini and Jacopo Staiano and Bruno Lepri", "abstract": "Although the capabilities of large language models have been increasingly tested on complex reasoning tasks, their long-horizon planning abilities have not yet been extensively investigated. In this work, we provide a systematic assessment of the planning and long-horizon reasoning capabilities of state-of-the-art Large Reasoning Models (LRMs). We propose a novel benchmark based on Sokoban puzzles, intentionally simplified to isolate long-horizon planning from state persistence. Our findings reveal a consistent degradation in planning performance when more than 25 moves are required to reach the solution, suggesting a fundamental constraint on forward planning capacity. We show that equipping LRMs with Planning Domain Definition Language (PDDL) parsing, validation, and solving tools allows for modest improvements, suggesting inherent architectural limitations which might not be overcome by test-time scaling approaches alone.", "link": "http://arxiv.org/abs/2601.20856v1", "date": "2026-01-28", "relevancy": 2.0781, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5242}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5242}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SokoBench%3A%20Evaluating%20Long-Horizon%20Planning%20and%20Reasoning%20in%20Large%20Language%20Models&body=Title%3A%20SokoBench%3A%20Evaluating%20Long-Horizon%20Planning%20and%20Reasoning%20in%20Large%20Language%20Models%0AAuthor%3A%20Sebastiano%20Monti%20and%20Carlo%20Nicolini%20and%20Gianni%20Pellegrini%20and%20Jacopo%20Staiano%20and%20Bruno%20Lepri%0AAbstract%3A%20Although%20the%20capabilities%20of%20large%20language%20models%20have%20been%20increasingly%20tested%20on%20complex%20reasoning%20tasks%2C%20their%20long-horizon%20planning%20abilities%20have%20not%20yet%20been%20extensively%20investigated.%20In%20this%20work%2C%20we%20provide%20a%20systematic%20assessment%20of%20the%20planning%20and%20long-horizon%20reasoning%20capabilities%20of%20state-of-the-art%20Large%20Reasoning%20Models%20%28LRMs%29.%20We%20propose%20a%20novel%20benchmark%20based%20on%20Sokoban%20puzzles%2C%20intentionally%20simplified%20to%20isolate%20long-horizon%20planning%20from%20state%20persistence.%20Our%20findings%20reveal%20a%20consistent%20degradation%20in%20planning%20performance%20when%20more%20than%2025%20moves%20are%20required%20to%20reach%20the%20solution%2C%20suggesting%20a%20fundamental%20constraint%20on%20forward%20planning%20capacity.%20We%20show%20that%20equipping%20LRMs%20with%20Planning%20Domain%20Definition%20Language%20%28PDDL%29%20parsing%2C%20validation%2C%20and%20solving%20tools%20allows%20for%20modest%20improvements%2C%20suggesting%20inherent%20architectural%20limitations%20which%20might%20not%20be%20overcome%20by%20test-time%20scaling%20approaches%20alone.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20856v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSokoBench%253A%2520Evaluating%2520Long-Horizon%2520Planning%2520and%2520Reasoning%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DSebastiano%2520Monti%2520and%2520Carlo%2520Nicolini%2520and%2520Gianni%2520Pellegrini%2520and%2520Jacopo%2520Staiano%2520and%2520Bruno%2520Lepri%26entry.1292438233%3DAlthough%2520the%2520capabilities%2520of%2520large%2520language%2520models%2520have%2520been%2520increasingly%2520tested%2520on%2520complex%2520reasoning%2520tasks%252C%2520their%2520long-horizon%2520planning%2520abilities%2520have%2520not%2520yet%2520been%2520extensively%2520investigated.%2520In%2520this%2520work%252C%2520we%2520provide%2520a%2520systematic%2520assessment%2520of%2520the%2520planning%2520and%2520long-horizon%2520reasoning%2520capabilities%2520of%2520state-of-the-art%2520Large%2520Reasoning%2520Models%2520%2528LRMs%2529.%2520We%2520propose%2520a%2520novel%2520benchmark%2520based%2520on%2520Sokoban%2520puzzles%252C%2520intentionally%2520simplified%2520to%2520isolate%2520long-horizon%2520planning%2520from%2520state%2520persistence.%2520Our%2520findings%2520reveal%2520a%2520consistent%2520degradation%2520in%2520planning%2520performance%2520when%2520more%2520than%252025%2520moves%2520are%2520required%2520to%2520reach%2520the%2520solution%252C%2520suggesting%2520a%2520fundamental%2520constraint%2520on%2520forward%2520planning%2520capacity.%2520We%2520show%2520that%2520equipping%2520LRMs%2520with%2520Planning%2520Domain%2520Definition%2520Language%2520%2528PDDL%2529%2520parsing%252C%2520validation%252C%2520and%2520solving%2520tools%2520allows%2520for%2520modest%2520improvements%252C%2520suggesting%2520inherent%2520architectural%2520limitations%2520which%2520might%2520not%2520be%2520overcome%2520by%2520test-time%2520scaling%2520approaches%2520alone.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20856v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SokoBench%3A%20Evaluating%20Long-Horizon%20Planning%20and%20Reasoning%20in%20Large%20Language%20Models&entry.906535625=Sebastiano%20Monti%20and%20Carlo%20Nicolini%20and%20Gianni%20Pellegrini%20and%20Jacopo%20Staiano%20and%20Bruno%20Lepri&entry.1292438233=Although%20the%20capabilities%20of%20large%20language%20models%20have%20been%20increasingly%20tested%20on%20complex%20reasoning%20tasks%2C%20their%20long-horizon%20planning%20abilities%20have%20not%20yet%20been%20extensively%20investigated.%20In%20this%20work%2C%20we%20provide%20a%20systematic%20assessment%20of%20the%20planning%20and%20long-horizon%20reasoning%20capabilities%20of%20state-of-the-art%20Large%20Reasoning%20Models%20%28LRMs%29.%20We%20propose%20a%20novel%20benchmark%20based%20on%20Sokoban%20puzzles%2C%20intentionally%20simplified%20to%20isolate%20long-horizon%20planning%20from%20state%20persistence.%20Our%20findings%20reveal%20a%20consistent%20degradation%20in%20planning%20performance%20when%20more%20than%2025%20moves%20are%20required%20to%20reach%20the%20solution%2C%20suggesting%20a%20fundamental%20constraint%20on%20forward%20planning%20capacity.%20We%20show%20that%20equipping%20LRMs%20with%20Planning%20Domain%20Definition%20Language%20%28PDDL%29%20parsing%2C%20validation%2C%20and%20solving%20tools%20allows%20for%20modest%20improvements%2C%20suggesting%20inherent%20architectural%20limitations%20which%20might%20not%20be%20overcome%20by%20test-time%20scaling%20approaches%20alone.&entry.1838667208=http%3A//arxiv.org/abs/2601.20856v1&entry.124074799=Read"},
{"title": "GPO: Growing Policy Optimization for Legged Robot Locomotion and Whole-Body Control", "author": "Shuhao Liao and Peizhuo Li and Xinrong Yang and Linnan Chang and Zhaoxin Fan and Qing Wang and Lei Shi and Yuhong Cao and Wenjun Wu and Guillaume Sartoretti", "abstract": "Training reinforcement learning (RL) policies for legged robots remains challenging due to high-dimensional continuous actions, hardware constraints, and limited exploration. Existing methods for locomotion and whole-body control work well for position-based control with environment-specific heuristics (e.g., reward shaping, curriculum design, and manual initialization), but are less effective for torque-based control, where sufficiently exploring the action space and obtaining informative gradient signals for training is significantly more difficult. We introduce Growing Policy Optimization (GPO), a training framework that applies a time-varying action transformation to restrict the effective action space in the early stage, thereby encouraging more effective data collection and policy learning, and then progressively expands it to enhance exploration and achieve higher expected return. We prove that this transformation preserves the PPO update rule and introduces only bounded, vanishing gradient distortion, thereby ensuring stable training. We evaluate GPO on both quadruped and hexapod robots, including zero-shot deployment of simulation-trained policies on hardware. Policies trained with GPO consistently achieve better performance. These results suggest that GPO provides a general, environment-agnostic optimization framework for learning legged locomotion.", "link": "http://arxiv.org/abs/2601.20668v1", "date": "2026-01-28", "relevancy": 2.0772, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5301}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5222}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GPO%3A%20Growing%20Policy%20Optimization%20for%20Legged%20Robot%20Locomotion%20and%20Whole-Body%20Control&body=Title%3A%20GPO%3A%20Growing%20Policy%20Optimization%20for%20Legged%20Robot%20Locomotion%20and%20Whole-Body%20Control%0AAuthor%3A%20Shuhao%20Liao%20and%20Peizhuo%20Li%20and%20Xinrong%20Yang%20and%20Linnan%20Chang%20and%20Zhaoxin%20Fan%20and%20Qing%20Wang%20and%20Lei%20Shi%20and%20Yuhong%20Cao%20and%20Wenjun%20Wu%20and%20Guillaume%20Sartoretti%0AAbstract%3A%20Training%20reinforcement%20learning%20%28RL%29%20policies%20for%20legged%20robots%20remains%20challenging%20due%20to%20high-dimensional%20continuous%20actions%2C%20hardware%20constraints%2C%20and%20limited%20exploration.%20Existing%20methods%20for%20locomotion%20and%20whole-body%20control%20work%20well%20for%20position-based%20control%20with%20environment-specific%20heuristics%20%28e.g.%2C%20reward%20shaping%2C%20curriculum%20design%2C%20and%20manual%20initialization%29%2C%20but%20are%20less%20effective%20for%20torque-based%20control%2C%20where%20sufficiently%20exploring%20the%20action%20space%20and%20obtaining%20informative%20gradient%20signals%20for%20training%20is%20significantly%20more%20difficult.%20We%20introduce%20Growing%20Policy%20Optimization%20%28GPO%29%2C%20a%20training%20framework%20that%20applies%20a%20time-varying%20action%20transformation%20to%20restrict%20the%20effective%20action%20space%20in%20the%20early%20stage%2C%20thereby%20encouraging%20more%20effective%20data%20collection%20and%20policy%20learning%2C%20and%20then%20progressively%20expands%20it%20to%20enhance%20exploration%20and%20achieve%20higher%20expected%20return.%20We%20prove%20that%20this%20transformation%20preserves%20the%20PPO%20update%20rule%20and%20introduces%20only%20bounded%2C%20vanishing%20gradient%20distortion%2C%20thereby%20ensuring%20stable%20training.%20We%20evaluate%20GPO%20on%20both%20quadruped%20and%20hexapod%20robots%2C%20including%20zero-shot%20deployment%20of%20simulation-trained%20policies%20on%20hardware.%20Policies%20trained%20with%20GPO%20consistently%20achieve%20better%20performance.%20These%20results%20suggest%20that%20GPO%20provides%20a%20general%2C%20environment-agnostic%20optimization%20framework%20for%20learning%20legged%20locomotion.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20668v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGPO%253A%2520Growing%2520Policy%2520Optimization%2520for%2520Legged%2520Robot%2520Locomotion%2520and%2520Whole-Body%2520Control%26entry.906535625%3DShuhao%2520Liao%2520and%2520Peizhuo%2520Li%2520and%2520Xinrong%2520Yang%2520and%2520Linnan%2520Chang%2520and%2520Zhaoxin%2520Fan%2520and%2520Qing%2520Wang%2520and%2520Lei%2520Shi%2520and%2520Yuhong%2520Cao%2520and%2520Wenjun%2520Wu%2520and%2520Guillaume%2520Sartoretti%26entry.1292438233%3DTraining%2520reinforcement%2520learning%2520%2528RL%2529%2520policies%2520for%2520legged%2520robots%2520remains%2520challenging%2520due%2520to%2520high-dimensional%2520continuous%2520actions%252C%2520hardware%2520constraints%252C%2520and%2520limited%2520exploration.%2520Existing%2520methods%2520for%2520locomotion%2520and%2520whole-body%2520control%2520work%2520well%2520for%2520position-based%2520control%2520with%2520environment-specific%2520heuristics%2520%2528e.g.%252C%2520reward%2520shaping%252C%2520curriculum%2520design%252C%2520and%2520manual%2520initialization%2529%252C%2520but%2520are%2520less%2520effective%2520for%2520torque-based%2520control%252C%2520where%2520sufficiently%2520exploring%2520the%2520action%2520space%2520and%2520obtaining%2520informative%2520gradient%2520signals%2520for%2520training%2520is%2520significantly%2520more%2520difficult.%2520We%2520introduce%2520Growing%2520Policy%2520Optimization%2520%2528GPO%2529%252C%2520a%2520training%2520framework%2520that%2520applies%2520a%2520time-varying%2520action%2520transformation%2520to%2520restrict%2520the%2520effective%2520action%2520space%2520in%2520the%2520early%2520stage%252C%2520thereby%2520encouraging%2520more%2520effective%2520data%2520collection%2520and%2520policy%2520learning%252C%2520and%2520then%2520progressively%2520expands%2520it%2520to%2520enhance%2520exploration%2520and%2520achieve%2520higher%2520expected%2520return.%2520We%2520prove%2520that%2520this%2520transformation%2520preserves%2520the%2520PPO%2520update%2520rule%2520and%2520introduces%2520only%2520bounded%252C%2520vanishing%2520gradient%2520distortion%252C%2520thereby%2520ensuring%2520stable%2520training.%2520We%2520evaluate%2520GPO%2520on%2520both%2520quadruped%2520and%2520hexapod%2520robots%252C%2520including%2520zero-shot%2520deployment%2520of%2520simulation-trained%2520policies%2520on%2520hardware.%2520Policies%2520trained%2520with%2520GPO%2520consistently%2520achieve%2520better%2520performance.%2520These%2520results%2520suggest%2520that%2520GPO%2520provides%2520a%2520general%252C%2520environment-agnostic%2520optimization%2520framework%2520for%2520learning%2520legged%2520locomotion.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20668v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GPO%3A%20Growing%20Policy%20Optimization%20for%20Legged%20Robot%20Locomotion%20and%20Whole-Body%20Control&entry.906535625=Shuhao%20Liao%20and%20Peizhuo%20Li%20and%20Xinrong%20Yang%20and%20Linnan%20Chang%20and%20Zhaoxin%20Fan%20and%20Qing%20Wang%20and%20Lei%20Shi%20and%20Yuhong%20Cao%20and%20Wenjun%20Wu%20and%20Guillaume%20Sartoretti&entry.1292438233=Training%20reinforcement%20learning%20%28RL%29%20policies%20for%20legged%20robots%20remains%20challenging%20due%20to%20high-dimensional%20continuous%20actions%2C%20hardware%20constraints%2C%20and%20limited%20exploration.%20Existing%20methods%20for%20locomotion%20and%20whole-body%20control%20work%20well%20for%20position-based%20control%20with%20environment-specific%20heuristics%20%28e.g.%2C%20reward%20shaping%2C%20curriculum%20design%2C%20and%20manual%20initialization%29%2C%20but%20are%20less%20effective%20for%20torque-based%20control%2C%20where%20sufficiently%20exploring%20the%20action%20space%20and%20obtaining%20informative%20gradient%20signals%20for%20training%20is%20significantly%20more%20difficult.%20We%20introduce%20Growing%20Policy%20Optimization%20%28GPO%29%2C%20a%20training%20framework%20that%20applies%20a%20time-varying%20action%20transformation%20to%20restrict%20the%20effective%20action%20space%20in%20the%20early%20stage%2C%20thereby%20encouraging%20more%20effective%20data%20collection%20and%20policy%20learning%2C%20and%20then%20progressively%20expands%20it%20to%20enhance%20exploration%20and%20achieve%20higher%20expected%20return.%20We%20prove%20that%20this%20transformation%20preserves%20the%20PPO%20update%20rule%20and%20introduces%20only%20bounded%2C%20vanishing%20gradient%20distortion%2C%20thereby%20ensuring%20stable%20training.%20We%20evaluate%20GPO%20on%20both%20quadruped%20and%20hexapod%20robots%2C%20including%20zero-shot%20deployment%20of%20simulation-trained%20policies%20on%20hardware.%20Policies%20trained%20with%20GPO%20consistently%20achieve%20better%20performance.%20These%20results%20suggest%20that%20GPO%20provides%20a%20general%2C%20environment-agnostic%20optimization%20framework%20for%20learning%20legged%20locomotion.&entry.1838667208=http%3A//arxiv.org/abs/2601.20668v1&entry.124074799=Read"},
{"title": "In-Context Bias Propagation in LLM-Based Tabular Data Generation", "author": "Pol G. Recasens and Alberto Gutierrez and Jordi Torres and Josep. Ll Berral and Javier Carnerero-Cano and Anisa Halimi and Kieran Fraser", "abstract": "Large Language Models (LLMs) are increasingly used for synthetic tabular data generation through in-context learning (ICL), offering a practical solution for data augmentation in data scarce scenarios. While prior work has shown the potential of LLMs to improve downstream task performance through augmenting underrepresented groups, these benefits often assume access to a subset of unbiased in-context examples, representative of the real dataset. In real-world settings, however, data is frequently noisy and demographically skewed. In this paper, we systematically study how statistical biases within in-context examples propagate to the distribution of synthetic tabular data, showing that even mild in-context biases lead to global statistical distortions. We further introduce an adversarial scenario where a malicious contributor can inject bias into the synthetic dataset via a subset of in-context examples, ultimately compromising the fairness of downstream classifiers for a targeted and protected subgroup. Finally, we evaluate mitigation strategies based on preprocessing in-context examples, demonstrating that while such interventions can attenuate disparity, the inherent sensitivity of LLMs to adversarial prompts remains a persistent challenge. Our findings highlight a critical new vulnerability in LLM-based data generation pipelines within sensitive domains.", "link": "http://arxiv.org/abs/2506.09630v2", "date": "2026-01-28", "relevancy": 1.8527, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.465}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.465}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In-Context%20Bias%20Propagation%20in%20LLM-Based%20Tabular%20Data%20Generation&body=Title%3A%20In-Context%20Bias%20Propagation%20in%20LLM-Based%20Tabular%20Data%20Generation%0AAuthor%3A%20Pol%20G.%20Recasens%20and%20Alberto%20Gutierrez%20and%20Jordi%20Torres%20and%20Josep.%20Ll%20Berral%20and%20Javier%20Carnerero-Cano%20and%20Anisa%20Halimi%20and%20Kieran%20Fraser%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20used%20for%20synthetic%20tabular%20data%20generation%20through%20in-context%20learning%20%28ICL%29%2C%20offering%20a%20practical%20solution%20for%20data%20augmentation%20in%20data%20scarce%20scenarios.%20While%20prior%20work%20has%20shown%20the%20potential%20of%20LLMs%20to%20improve%20downstream%20task%20performance%20through%20augmenting%20underrepresented%20groups%2C%20these%20benefits%20often%20assume%20access%20to%20a%20subset%20of%20unbiased%20in-context%20examples%2C%20representative%20of%20the%20real%20dataset.%20In%20real-world%20settings%2C%20however%2C%20data%20is%20frequently%20noisy%20and%20demographically%20skewed.%20In%20this%20paper%2C%20we%20systematically%20study%20how%20statistical%20biases%20within%20in-context%20examples%20propagate%20to%20the%20distribution%20of%20synthetic%20tabular%20data%2C%20showing%20that%20even%20mild%20in-context%20biases%20lead%20to%20global%20statistical%20distortions.%20We%20further%20introduce%20an%20adversarial%20scenario%20where%20a%20malicious%20contributor%20can%20inject%20bias%20into%20the%20synthetic%20dataset%20via%20a%20subset%20of%20in-context%20examples%2C%20ultimately%20compromising%20the%20fairness%20of%20downstream%20classifiers%20for%20a%20targeted%20and%20protected%20subgroup.%20Finally%2C%20we%20evaluate%20mitigation%20strategies%20based%20on%20preprocessing%20in-context%20examples%2C%20demonstrating%20that%20while%20such%20interventions%20can%20attenuate%20disparity%2C%20the%20inherent%20sensitivity%20of%20LLMs%20to%20adversarial%20prompts%20remains%20a%20persistent%20challenge.%20Our%20findings%20highlight%20a%20critical%20new%20vulnerability%20in%20LLM-based%20data%20generation%20pipelines%20within%20sensitive%20domains.%0ALink%3A%20http%3A//arxiv.org/abs/2506.09630v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn-Context%2520Bias%2520Propagation%2520in%2520LLM-Based%2520Tabular%2520Data%2520Generation%26entry.906535625%3DPol%2520G.%2520Recasens%2520and%2520Alberto%2520Gutierrez%2520and%2520Jordi%2520Torres%2520and%2520Josep.%2520Ll%2520Berral%2520and%2520Javier%2520Carnerero-Cano%2520and%2520Anisa%2520Halimi%2520and%2520Kieran%2520Fraser%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520increasingly%2520used%2520for%2520synthetic%2520tabular%2520data%2520generation%2520through%2520in-context%2520learning%2520%2528ICL%2529%252C%2520offering%2520a%2520practical%2520solution%2520for%2520data%2520augmentation%2520in%2520data%2520scarce%2520scenarios.%2520While%2520prior%2520work%2520has%2520shown%2520the%2520potential%2520of%2520LLMs%2520to%2520improve%2520downstream%2520task%2520performance%2520through%2520augmenting%2520underrepresented%2520groups%252C%2520these%2520benefits%2520often%2520assume%2520access%2520to%2520a%2520subset%2520of%2520unbiased%2520in-context%2520examples%252C%2520representative%2520of%2520the%2520real%2520dataset.%2520In%2520real-world%2520settings%252C%2520however%252C%2520data%2520is%2520frequently%2520noisy%2520and%2520demographically%2520skewed.%2520In%2520this%2520paper%252C%2520we%2520systematically%2520study%2520how%2520statistical%2520biases%2520within%2520in-context%2520examples%2520propagate%2520to%2520the%2520distribution%2520of%2520synthetic%2520tabular%2520data%252C%2520showing%2520that%2520even%2520mild%2520in-context%2520biases%2520lead%2520to%2520global%2520statistical%2520distortions.%2520We%2520further%2520introduce%2520an%2520adversarial%2520scenario%2520where%2520a%2520malicious%2520contributor%2520can%2520inject%2520bias%2520into%2520the%2520synthetic%2520dataset%2520via%2520a%2520subset%2520of%2520in-context%2520examples%252C%2520ultimately%2520compromising%2520the%2520fairness%2520of%2520downstream%2520classifiers%2520for%2520a%2520targeted%2520and%2520protected%2520subgroup.%2520Finally%252C%2520we%2520evaluate%2520mitigation%2520strategies%2520based%2520on%2520preprocessing%2520in-context%2520examples%252C%2520demonstrating%2520that%2520while%2520such%2520interventions%2520can%2520attenuate%2520disparity%252C%2520the%2520inherent%2520sensitivity%2520of%2520LLMs%2520to%2520adversarial%2520prompts%2520remains%2520a%2520persistent%2520challenge.%2520Our%2520findings%2520highlight%2520a%2520critical%2520new%2520vulnerability%2520in%2520LLM-based%2520data%2520generation%2520pipelines%2520within%2520sensitive%2520domains.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.09630v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-Context%20Bias%20Propagation%20in%20LLM-Based%20Tabular%20Data%20Generation&entry.906535625=Pol%20G.%20Recasens%20and%20Alberto%20Gutierrez%20and%20Jordi%20Torres%20and%20Josep.%20Ll%20Berral%20and%20Javier%20Carnerero-Cano%20and%20Anisa%20Halimi%20and%20Kieran%20Fraser&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20used%20for%20synthetic%20tabular%20data%20generation%20through%20in-context%20learning%20%28ICL%29%2C%20offering%20a%20practical%20solution%20for%20data%20augmentation%20in%20data%20scarce%20scenarios.%20While%20prior%20work%20has%20shown%20the%20potential%20of%20LLMs%20to%20improve%20downstream%20task%20performance%20through%20augmenting%20underrepresented%20groups%2C%20these%20benefits%20often%20assume%20access%20to%20a%20subset%20of%20unbiased%20in-context%20examples%2C%20representative%20of%20the%20real%20dataset.%20In%20real-world%20settings%2C%20however%2C%20data%20is%20frequently%20noisy%20and%20demographically%20skewed.%20In%20this%20paper%2C%20we%20systematically%20study%20how%20statistical%20biases%20within%20in-context%20examples%20propagate%20to%20the%20distribution%20of%20synthetic%20tabular%20data%2C%20showing%20that%20even%20mild%20in-context%20biases%20lead%20to%20global%20statistical%20distortions.%20We%20further%20introduce%20an%20adversarial%20scenario%20where%20a%20malicious%20contributor%20can%20inject%20bias%20into%20the%20synthetic%20dataset%20via%20a%20subset%20of%20in-context%20examples%2C%20ultimately%20compromising%20the%20fairness%20of%20downstream%20classifiers%20for%20a%20targeted%20and%20protected%20subgroup.%20Finally%2C%20we%20evaluate%20mitigation%20strategies%20based%20on%20preprocessing%20in-context%20examples%2C%20demonstrating%20that%20while%20such%20interventions%20can%20attenuate%20disparity%2C%20the%20inherent%20sensitivity%20of%20LLMs%20to%20adversarial%20prompts%20remains%20a%20persistent%20challenge.%20Our%20findings%20highlight%20a%20critical%20new%20vulnerability%20in%20LLM-based%20data%20generation%20pipelines%20within%20sensitive%20domains.&entry.1838667208=http%3A//arxiv.org/abs/2506.09630v2&entry.124074799=Read"},
{"title": "Online Risk-Averse Planning in POMDPs Using Iterated CVaR Value Function", "author": "Yaacov Pariente and Vadim Indelman", "abstract": "We study risk-sensitive planning under partial observability using the dynamic risk measure Iterated Conditional Value-at-Risk (ICVaR). A policy evaluation algorithm for ICVaR is developed with finite-time performance guarantees that do not depend on the cardinality of the action space. Building on this foundation, three widely used online planning algorithms--Sparse Sampling, Particle Filter Trees with Double Progressive Widening (PFT-DPW), and Partially Observable Monte Carlo Planning with Observation Widening (POMCPOW)--are extended to optimize the ICVaR value function rather than the expectation of the return. Our formulations introduce a risk parameter $\u03b1$, where $\u03b1= 1$ recovers standard expectation-based planning and $\u03b1< 1$ induces increasing risk aversion. For ICVaR Sparse Sampling, we establish finite-time performance guarantees under the risk-sensitive objective, which further enable a novel exploration strategy tailored to ICVaR. Experiments on benchmark POMDP domains demonstrate that the proposed ICVaR planners achieve lower tail risk compared to their risk-neutral counterparts.", "link": "http://arxiv.org/abs/2601.20554v1", "date": "2026-01-28", "relevancy": 1.8074, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4843}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4764}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Risk-Averse%20Planning%20in%20POMDPs%20Using%20Iterated%20CVaR%20Value%20Function&body=Title%3A%20Online%20Risk-Averse%20Planning%20in%20POMDPs%20Using%20Iterated%20CVaR%20Value%20Function%0AAuthor%3A%20Yaacov%20Pariente%20and%20Vadim%20Indelman%0AAbstract%3A%20We%20study%20risk-sensitive%20planning%20under%20partial%20observability%20using%20the%20dynamic%20risk%20measure%20Iterated%20Conditional%20Value-at-Risk%20%28ICVaR%29.%20A%20policy%20evaluation%20algorithm%20for%20ICVaR%20is%20developed%20with%20finite-time%20performance%20guarantees%20that%20do%20not%20depend%20on%20the%20cardinality%20of%20the%20action%20space.%20Building%20on%20this%20foundation%2C%20three%20widely%20used%20online%20planning%20algorithms--Sparse%20Sampling%2C%20Particle%20Filter%20Trees%20with%20Double%20Progressive%20Widening%20%28PFT-DPW%29%2C%20and%20Partially%20Observable%20Monte%20Carlo%20Planning%20with%20Observation%20Widening%20%28POMCPOW%29--are%20extended%20to%20optimize%20the%20ICVaR%20value%20function%20rather%20than%20the%20expectation%20of%20the%20return.%20Our%20formulations%20introduce%20a%20risk%20parameter%20%24%CE%B1%24%2C%20where%20%24%CE%B1%3D%201%24%20recovers%20standard%20expectation-based%20planning%20and%20%24%CE%B1%3C%201%24%20induces%20increasing%20risk%20aversion.%20For%20ICVaR%20Sparse%20Sampling%2C%20we%20establish%20finite-time%20performance%20guarantees%20under%20the%20risk-sensitive%20objective%2C%20which%20further%20enable%20a%20novel%20exploration%20strategy%20tailored%20to%20ICVaR.%20Experiments%20on%20benchmark%20POMDP%20domains%20demonstrate%20that%20the%20proposed%20ICVaR%20planners%20achieve%20lower%20tail%20risk%20compared%20to%20their%20risk-neutral%20counterparts.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20554v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Risk-Averse%2520Planning%2520in%2520POMDPs%2520Using%2520Iterated%2520CVaR%2520Value%2520Function%26entry.906535625%3DYaacov%2520Pariente%2520and%2520Vadim%2520Indelman%26entry.1292438233%3DWe%2520study%2520risk-sensitive%2520planning%2520under%2520partial%2520observability%2520using%2520the%2520dynamic%2520risk%2520measure%2520Iterated%2520Conditional%2520Value-at-Risk%2520%2528ICVaR%2529.%2520A%2520policy%2520evaluation%2520algorithm%2520for%2520ICVaR%2520is%2520developed%2520with%2520finite-time%2520performance%2520guarantees%2520that%2520do%2520not%2520depend%2520on%2520the%2520cardinality%2520of%2520the%2520action%2520space.%2520Building%2520on%2520this%2520foundation%252C%2520three%2520widely%2520used%2520online%2520planning%2520algorithms--Sparse%2520Sampling%252C%2520Particle%2520Filter%2520Trees%2520with%2520Double%2520Progressive%2520Widening%2520%2528PFT-DPW%2529%252C%2520and%2520Partially%2520Observable%2520Monte%2520Carlo%2520Planning%2520with%2520Observation%2520Widening%2520%2528POMCPOW%2529--are%2520extended%2520to%2520optimize%2520the%2520ICVaR%2520value%2520function%2520rather%2520than%2520the%2520expectation%2520of%2520the%2520return.%2520Our%2520formulations%2520introduce%2520a%2520risk%2520parameter%2520%2524%25CE%25B1%2524%252C%2520where%2520%2524%25CE%25B1%253D%25201%2524%2520recovers%2520standard%2520expectation-based%2520planning%2520and%2520%2524%25CE%25B1%253C%25201%2524%2520induces%2520increasing%2520risk%2520aversion.%2520For%2520ICVaR%2520Sparse%2520Sampling%252C%2520we%2520establish%2520finite-time%2520performance%2520guarantees%2520under%2520the%2520risk-sensitive%2520objective%252C%2520which%2520further%2520enable%2520a%2520novel%2520exploration%2520strategy%2520tailored%2520to%2520ICVaR.%2520Experiments%2520on%2520benchmark%2520POMDP%2520domains%2520demonstrate%2520that%2520the%2520proposed%2520ICVaR%2520planners%2520achieve%2520lower%2520tail%2520risk%2520compared%2520to%2520their%2520risk-neutral%2520counterparts.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20554v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Risk-Averse%20Planning%20in%20POMDPs%20Using%20Iterated%20CVaR%20Value%20Function&entry.906535625=Yaacov%20Pariente%20and%20Vadim%20Indelman&entry.1292438233=We%20study%20risk-sensitive%20planning%20under%20partial%20observability%20using%20the%20dynamic%20risk%20measure%20Iterated%20Conditional%20Value-at-Risk%20%28ICVaR%29.%20A%20policy%20evaluation%20algorithm%20for%20ICVaR%20is%20developed%20with%20finite-time%20performance%20guarantees%20that%20do%20not%20depend%20on%20the%20cardinality%20of%20the%20action%20space.%20Building%20on%20this%20foundation%2C%20three%20widely%20used%20online%20planning%20algorithms--Sparse%20Sampling%2C%20Particle%20Filter%20Trees%20with%20Double%20Progressive%20Widening%20%28PFT-DPW%29%2C%20and%20Partially%20Observable%20Monte%20Carlo%20Planning%20with%20Observation%20Widening%20%28POMCPOW%29--are%20extended%20to%20optimize%20the%20ICVaR%20value%20function%20rather%20than%20the%20expectation%20of%20the%20return.%20Our%20formulations%20introduce%20a%20risk%20parameter%20%24%CE%B1%24%2C%20where%20%24%CE%B1%3D%201%24%20recovers%20standard%20expectation-based%20planning%20and%20%24%CE%B1%3C%201%24%20induces%20increasing%20risk%20aversion.%20For%20ICVaR%20Sparse%20Sampling%2C%20we%20establish%20finite-time%20performance%20guarantees%20under%20the%20risk-sensitive%20objective%2C%20which%20further%20enable%20a%20novel%20exploration%20strategy%20tailored%20to%20ICVaR.%20Experiments%20on%20benchmark%20POMDP%20domains%20demonstrate%20that%20the%20proposed%20ICVaR%20planners%20achieve%20lower%20tail%20risk%20compared%20to%20their%20risk-neutral%20counterparts.&entry.1838667208=http%3A//arxiv.org/abs/2601.20554v1&entry.124074799=Read"},
{"title": "GraphTARIF: Linear Graph Transformer with Augmented Rank and Improved Focus", "author": "Zhaolin Hu and Kun Li and Hehe Fan and Yi Yang", "abstract": "Linear attention mechanisms have emerged as efficient alternatives to full self-attention in Graph Transformers, offering linear time complexity. However, existing linear attention models often suffer from a significant drop in expressiveness due to low-rank projection structures and overly uniform attention distributions. We theoretically prove that these properties reduce the class separability of node representations, limiting the model's classification ability. To address this, we propose a novel hybrid framework that enhances both the rank and focus of attention. Specifically, we enhance linear attention by attaching a gated local graph network branch to the value matrix, thereby increasing the rank of the resulting attention map. Furthermore, to alleviate the excessive smoothing effect inherent in linear attention, we introduce a learnable log-power function into the attention scores to reduce entropy and sharpen focus. We theoretically show that this function decreases entropy in the attention distribution, enhancing the separability of learned embeddings. Extensive experiments on both homophilic and heterophilic graph benchmarks demonstrate that our method achieves competitive performance while preserving the scalability of linear attention.", "link": "http://arxiv.org/abs/2510.10631v2", "date": "2026-01-28", "relevancy": 1.9833, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5399}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5018}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4722}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphTARIF%3A%20Linear%20Graph%20Transformer%20with%20Augmented%20Rank%20and%20Improved%20Focus&body=Title%3A%20GraphTARIF%3A%20Linear%20Graph%20Transformer%20with%20Augmented%20Rank%20and%20Improved%20Focus%0AAuthor%3A%20Zhaolin%20Hu%20and%20Kun%20Li%20and%20Hehe%20Fan%20and%20Yi%20Yang%0AAbstract%3A%20Linear%20attention%20mechanisms%20have%20emerged%20as%20efficient%20alternatives%20to%20full%20self-attention%20in%20Graph%20Transformers%2C%20offering%20linear%20time%20complexity.%20However%2C%20existing%20linear%20attention%20models%20often%20suffer%20from%20a%20significant%20drop%20in%20expressiveness%20due%20to%20low-rank%20projection%20structures%20and%20overly%20uniform%20attention%20distributions.%20We%20theoretically%20prove%20that%20these%20properties%20reduce%20the%20class%20separability%20of%20node%20representations%2C%20limiting%20the%20model%27s%20classification%20ability.%20To%20address%20this%2C%20we%20propose%20a%20novel%20hybrid%20framework%20that%20enhances%20both%20the%20rank%20and%20focus%20of%20attention.%20Specifically%2C%20we%20enhance%20linear%20attention%20by%20attaching%20a%20gated%20local%20graph%20network%20branch%20to%20the%20value%20matrix%2C%20thereby%20increasing%20the%20rank%20of%20the%20resulting%20attention%20map.%20Furthermore%2C%20to%20alleviate%20the%20excessive%20smoothing%20effect%20inherent%20in%20linear%20attention%2C%20we%20introduce%20a%20learnable%20log-power%20function%20into%20the%20attention%20scores%20to%20reduce%20entropy%20and%20sharpen%20focus.%20We%20theoretically%20show%20that%20this%20function%20decreases%20entropy%20in%20the%20attention%20distribution%2C%20enhancing%20the%20separability%20of%20learned%20embeddings.%20Extensive%20experiments%20on%20both%20homophilic%20and%20heterophilic%20graph%20benchmarks%20demonstrate%20that%20our%20method%20achieves%20competitive%20performance%20while%20preserving%20the%20scalability%20of%20linear%20attention.%0ALink%3A%20http%3A//arxiv.org/abs/2510.10631v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphTARIF%253A%2520Linear%2520Graph%2520Transformer%2520with%2520Augmented%2520Rank%2520and%2520Improved%2520Focus%26entry.906535625%3DZhaolin%2520Hu%2520and%2520Kun%2520Li%2520and%2520Hehe%2520Fan%2520and%2520Yi%2520Yang%26entry.1292438233%3DLinear%2520attention%2520mechanisms%2520have%2520emerged%2520as%2520efficient%2520alternatives%2520to%2520full%2520self-attention%2520in%2520Graph%2520Transformers%252C%2520offering%2520linear%2520time%2520complexity.%2520However%252C%2520existing%2520linear%2520attention%2520models%2520often%2520suffer%2520from%2520a%2520significant%2520drop%2520in%2520expressiveness%2520due%2520to%2520low-rank%2520projection%2520structures%2520and%2520overly%2520uniform%2520attention%2520distributions.%2520We%2520theoretically%2520prove%2520that%2520these%2520properties%2520reduce%2520the%2520class%2520separability%2520of%2520node%2520representations%252C%2520limiting%2520the%2520model%2527s%2520classification%2520ability.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520novel%2520hybrid%2520framework%2520that%2520enhances%2520both%2520the%2520rank%2520and%2520focus%2520of%2520attention.%2520Specifically%252C%2520we%2520enhance%2520linear%2520attention%2520by%2520attaching%2520a%2520gated%2520local%2520graph%2520network%2520branch%2520to%2520the%2520value%2520matrix%252C%2520thereby%2520increasing%2520the%2520rank%2520of%2520the%2520resulting%2520attention%2520map.%2520Furthermore%252C%2520to%2520alleviate%2520the%2520excessive%2520smoothing%2520effect%2520inherent%2520in%2520linear%2520attention%252C%2520we%2520introduce%2520a%2520learnable%2520log-power%2520function%2520into%2520the%2520attention%2520scores%2520to%2520reduce%2520entropy%2520and%2520sharpen%2520focus.%2520We%2520theoretically%2520show%2520that%2520this%2520function%2520decreases%2520entropy%2520in%2520the%2520attention%2520distribution%252C%2520enhancing%2520the%2520separability%2520of%2520learned%2520embeddings.%2520Extensive%2520experiments%2520on%2520both%2520homophilic%2520and%2520heterophilic%2520graph%2520benchmarks%2520demonstrate%2520that%2520our%2520method%2520achieves%2520competitive%2520performance%2520while%2520preserving%2520the%2520scalability%2520of%2520linear%2520attention.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.10631v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphTARIF%3A%20Linear%20Graph%20Transformer%20with%20Augmented%20Rank%20and%20Improved%20Focus&entry.906535625=Zhaolin%20Hu%20and%20Kun%20Li%20and%20Hehe%20Fan%20and%20Yi%20Yang&entry.1292438233=Linear%20attention%20mechanisms%20have%20emerged%20as%20efficient%20alternatives%20to%20full%20self-attention%20in%20Graph%20Transformers%2C%20offering%20linear%20time%20complexity.%20However%2C%20existing%20linear%20attention%20models%20often%20suffer%20from%20a%20significant%20drop%20in%20expressiveness%20due%20to%20low-rank%20projection%20structures%20and%20overly%20uniform%20attention%20distributions.%20We%20theoretically%20prove%20that%20these%20properties%20reduce%20the%20class%20separability%20of%20node%20representations%2C%20limiting%20the%20model%27s%20classification%20ability.%20To%20address%20this%2C%20we%20propose%20a%20novel%20hybrid%20framework%20that%20enhances%20both%20the%20rank%20and%20focus%20of%20attention.%20Specifically%2C%20we%20enhance%20linear%20attention%20by%20attaching%20a%20gated%20local%20graph%20network%20branch%20to%20the%20value%20matrix%2C%20thereby%20increasing%20the%20rank%20of%20the%20resulting%20attention%20map.%20Furthermore%2C%20to%20alleviate%20the%20excessive%20smoothing%20effect%20inherent%20in%20linear%20attention%2C%20we%20introduce%20a%20learnable%20log-power%20function%20into%20the%20attention%20scores%20to%20reduce%20entropy%20and%20sharpen%20focus.%20We%20theoretically%20show%20that%20this%20function%20decreases%20entropy%20in%20the%20attention%20distribution%2C%20enhancing%20the%20separability%20of%20learned%20embeddings.%20Extensive%20experiments%20on%20both%20homophilic%20and%20heterophilic%20graph%20benchmarks%20demonstrate%20that%20our%20method%20achieves%20competitive%20performance%20while%20preserving%20the%20scalability%20of%20linear%20attention.&entry.1838667208=http%3A//arxiv.org/abs/2510.10631v2&entry.124074799=Read"},
{"title": "A Practical Framework of Key Performance Indicators for Multi-Robot Lunar and Planetary Field Tests", "author": "Julia Richter and David Oberacker and Gabriela Ligeza and Valentin T. Bickel and Philip Arm and William Talbot and Marvin Grosse Besselmann and Florian Kehl and Tristan Schnell and Hendrik Kolvenbach and R\u00fcdiger Dillmann and Arne Roennau and Marco Hutter", "abstract": "Robotic prospecting for critical resources on the Moon, such as ilmenite, rare earth elements, and water ice, requires robust exploration methods given the diverse terrain and harsh environmental conditions. Although numerous analog field trials address these goals, comparing their results remains challenging because of differences in robot platforms and experimental setups. These missions typically assess performance using selected, scenario-specific engineering metrics that fail to establish a clear link between field performance and science-driven objectives. In this paper, we address this gap by deriving a structured framework of KPI from three realistic multi-robot lunar scenarios reflecting scientific objectives and operational constraints. Our framework emphasizes scenario-dependent priorities in efficiency, robustness, and precision, and is explicitly designed for practical applicability in field deployments. We validated the framework in a multi-robot field test and found it practical and easy to apply for efficiency- and robustness-related KPI, whereas precision-oriented KPI require reliable ground-truth data that is not always feasible to obtain in outdoor analog environments. Overall, we propose this framework as a common evaluation standard enabling consistent, goal-oriented comparison of multi-robot field trials and supporting systematic development of robotic systems for future planetary exploration.", "link": "http://arxiv.org/abs/2601.20529v1", "date": "2026-01-28", "relevancy": 1.3691, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4846}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4841}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.434}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Practical%20Framework%20of%20Key%20Performance%20Indicators%20for%20Multi-Robot%20Lunar%20and%20Planetary%20Field%20Tests&body=Title%3A%20A%20Practical%20Framework%20of%20Key%20Performance%20Indicators%20for%20Multi-Robot%20Lunar%20and%20Planetary%20Field%20Tests%0AAuthor%3A%20Julia%20Richter%20and%20David%20Oberacker%20and%20Gabriela%20Ligeza%20and%20Valentin%20T.%20Bickel%20and%20Philip%20Arm%20and%20William%20Talbot%20and%20Marvin%20Grosse%20Besselmann%20and%20Florian%20Kehl%20and%20Tristan%20Schnell%20and%20Hendrik%20Kolvenbach%20and%20R%C3%BCdiger%20Dillmann%20and%20Arne%20Roennau%20and%20Marco%20Hutter%0AAbstract%3A%20Robotic%20prospecting%20for%20critical%20resources%20on%20the%20Moon%2C%20such%20as%20ilmenite%2C%20rare%20earth%20elements%2C%20and%20water%20ice%2C%20requires%20robust%20exploration%20methods%20given%20the%20diverse%20terrain%20and%20harsh%20environmental%20conditions.%20Although%20numerous%20analog%20field%20trials%20address%20these%20goals%2C%20comparing%20their%20results%20remains%20challenging%20because%20of%20differences%20in%20robot%20platforms%20and%20experimental%20setups.%20These%20missions%20typically%20assess%20performance%20using%20selected%2C%20scenario-specific%20engineering%20metrics%20that%20fail%20to%20establish%20a%20clear%20link%20between%20field%20performance%20and%20science-driven%20objectives.%20In%20this%20paper%2C%20we%20address%20this%20gap%20by%20deriving%20a%20structured%20framework%20of%20KPI%20from%20three%20realistic%20multi-robot%20lunar%20scenarios%20reflecting%20scientific%20objectives%20and%20operational%20constraints.%20Our%20framework%20emphasizes%20scenario-dependent%20priorities%20in%20efficiency%2C%20robustness%2C%20and%20precision%2C%20and%20is%20explicitly%20designed%20for%20practical%20applicability%20in%20field%20deployments.%20We%20validated%20the%20framework%20in%20a%20multi-robot%20field%20test%20and%20found%20it%20practical%20and%20easy%20to%20apply%20for%20efficiency-%20and%20robustness-related%20KPI%2C%20whereas%20precision-oriented%20KPI%20require%20reliable%20ground-truth%20data%20that%20is%20not%20always%20feasible%20to%20obtain%20in%20outdoor%20analog%20environments.%20Overall%2C%20we%20propose%20this%20framework%20as%20a%20common%20evaluation%20standard%20enabling%20consistent%2C%20goal-oriented%20comparison%20of%20multi-robot%20field%20trials%20and%20supporting%20systematic%20development%20of%20robotic%20systems%20for%20future%20planetary%20exploration.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20529v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Practical%2520Framework%2520of%2520Key%2520Performance%2520Indicators%2520for%2520Multi-Robot%2520Lunar%2520and%2520Planetary%2520Field%2520Tests%26entry.906535625%3DJulia%2520Richter%2520and%2520David%2520Oberacker%2520and%2520Gabriela%2520Ligeza%2520and%2520Valentin%2520T.%2520Bickel%2520and%2520Philip%2520Arm%2520and%2520William%2520Talbot%2520and%2520Marvin%2520Grosse%2520Besselmann%2520and%2520Florian%2520Kehl%2520and%2520Tristan%2520Schnell%2520and%2520Hendrik%2520Kolvenbach%2520and%2520R%25C3%25BCdiger%2520Dillmann%2520and%2520Arne%2520Roennau%2520and%2520Marco%2520Hutter%26entry.1292438233%3DRobotic%2520prospecting%2520for%2520critical%2520resources%2520on%2520the%2520Moon%252C%2520such%2520as%2520ilmenite%252C%2520rare%2520earth%2520elements%252C%2520and%2520water%2520ice%252C%2520requires%2520robust%2520exploration%2520methods%2520given%2520the%2520diverse%2520terrain%2520and%2520harsh%2520environmental%2520conditions.%2520Although%2520numerous%2520analog%2520field%2520trials%2520address%2520these%2520goals%252C%2520comparing%2520their%2520results%2520remains%2520challenging%2520because%2520of%2520differences%2520in%2520robot%2520platforms%2520and%2520experimental%2520setups.%2520These%2520missions%2520typically%2520assess%2520performance%2520using%2520selected%252C%2520scenario-specific%2520engineering%2520metrics%2520that%2520fail%2520to%2520establish%2520a%2520clear%2520link%2520between%2520field%2520performance%2520and%2520science-driven%2520objectives.%2520In%2520this%2520paper%252C%2520we%2520address%2520this%2520gap%2520by%2520deriving%2520a%2520structured%2520framework%2520of%2520KPI%2520from%2520three%2520realistic%2520multi-robot%2520lunar%2520scenarios%2520reflecting%2520scientific%2520objectives%2520and%2520operational%2520constraints.%2520Our%2520framework%2520emphasizes%2520scenario-dependent%2520priorities%2520in%2520efficiency%252C%2520robustness%252C%2520and%2520precision%252C%2520and%2520is%2520explicitly%2520designed%2520for%2520practical%2520applicability%2520in%2520field%2520deployments.%2520We%2520validated%2520the%2520framework%2520in%2520a%2520multi-robot%2520field%2520test%2520and%2520found%2520it%2520practical%2520and%2520easy%2520to%2520apply%2520for%2520efficiency-%2520and%2520robustness-related%2520KPI%252C%2520whereas%2520precision-oriented%2520KPI%2520require%2520reliable%2520ground-truth%2520data%2520that%2520is%2520not%2520always%2520feasible%2520to%2520obtain%2520in%2520outdoor%2520analog%2520environments.%2520Overall%252C%2520we%2520propose%2520this%2520framework%2520as%2520a%2520common%2520evaluation%2520standard%2520enabling%2520consistent%252C%2520goal-oriented%2520comparison%2520of%2520multi-robot%2520field%2520trials%2520and%2520supporting%2520systematic%2520development%2520of%2520robotic%2520systems%2520for%2520future%2520planetary%2520exploration.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20529v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Practical%20Framework%20of%20Key%20Performance%20Indicators%20for%20Multi-Robot%20Lunar%20and%20Planetary%20Field%20Tests&entry.906535625=Julia%20Richter%20and%20David%20Oberacker%20and%20Gabriela%20Ligeza%20and%20Valentin%20T.%20Bickel%20and%20Philip%20Arm%20and%20William%20Talbot%20and%20Marvin%20Grosse%20Besselmann%20and%20Florian%20Kehl%20and%20Tristan%20Schnell%20and%20Hendrik%20Kolvenbach%20and%20R%C3%BCdiger%20Dillmann%20and%20Arne%20Roennau%20and%20Marco%20Hutter&entry.1292438233=Robotic%20prospecting%20for%20critical%20resources%20on%20the%20Moon%2C%20such%20as%20ilmenite%2C%20rare%20earth%20elements%2C%20and%20water%20ice%2C%20requires%20robust%20exploration%20methods%20given%20the%20diverse%20terrain%20and%20harsh%20environmental%20conditions.%20Although%20numerous%20analog%20field%20trials%20address%20these%20goals%2C%20comparing%20their%20results%20remains%20challenging%20because%20of%20differences%20in%20robot%20platforms%20and%20experimental%20setups.%20These%20missions%20typically%20assess%20performance%20using%20selected%2C%20scenario-specific%20engineering%20metrics%20that%20fail%20to%20establish%20a%20clear%20link%20between%20field%20performance%20and%20science-driven%20objectives.%20In%20this%20paper%2C%20we%20address%20this%20gap%20by%20deriving%20a%20structured%20framework%20of%20KPI%20from%20three%20realistic%20multi-robot%20lunar%20scenarios%20reflecting%20scientific%20objectives%20and%20operational%20constraints.%20Our%20framework%20emphasizes%20scenario-dependent%20priorities%20in%20efficiency%2C%20robustness%2C%20and%20precision%2C%20and%20is%20explicitly%20designed%20for%20practical%20applicability%20in%20field%20deployments.%20We%20validated%20the%20framework%20in%20a%20multi-robot%20field%20test%20and%20found%20it%20practical%20and%20easy%20to%20apply%20for%20efficiency-%20and%20robustness-related%20KPI%2C%20whereas%20precision-oriented%20KPI%20require%20reliable%20ground-truth%20data%20that%20is%20not%20always%20feasible%20to%20obtain%20in%20outdoor%20analog%20environments.%20Overall%2C%20we%20propose%20this%20framework%20as%20a%20common%20evaluation%20standard%20enabling%20consistent%2C%20goal-oriented%20comparison%20of%20multi-robot%20field%20trials%20and%20supporting%20systematic%20development%20of%20robotic%20systems%20for%20future%20planetary%20exploration.&entry.1838667208=http%3A//arxiv.org/abs/2601.20529v1&entry.124074799=Read"},
{"title": "Recurrent Neural Networks with Linear Structures for Electricity Price Forecasting", "author": "Souhir Ben Amor and Florian Ziel", "abstract": "We present a novel recurrent neural network architecture specifically designed for day-ahead electricity price forecasting, aimed at improving short-term decision-making and operational management in energy systems. Our combined forecasting model embeds linear structures, such as expert models and Kalman filters, into recurrent networks, enabling efficient computation and enhanced interpretability. The design leverages the strengths of both linear and non-linear model structures, allowing it to capture all relevant stylized price characteristics in power markets, including calendar and autoregressive effects, as well as influences from load, renewable energy, and related fuel and carbon markets. For empirical testing, we use hourly data from the largest European electricity market spanning 2018 to 2025 in a comprehensive forecasting study, comparing our model against state-of-the-art approaches, particularly high-dimensional linear and neural network models. In terms of RMSE, the proposed model achieves approximately 11% higher accuracy than the best-performing benchmark. We evaluate the contributions of the interpretable model components and conclude on the impact of combining linear and non-linear structures. We further evaluate the temporal robustness of the model by examining the stability of hyperparameters and the economic significance of key features. Additionally, we introduce a probabilistic extension to quantify forecast uncertainty.", "link": "http://arxiv.org/abs/2512.04690v2", "date": "2026-01-28", "relevancy": 1.3656, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4838}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4324}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Recurrent%20Neural%20Networks%20with%20Linear%20Structures%20for%20Electricity%20Price%20Forecasting&body=Title%3A%20Recurrent%20Neural%20Networks%20with%20Linear%20Structures%20for%20Electricity%20Price%20Forecasting%0AAuthor%3A%20Souhir%20Ben%20Amor%20and%20Florian%20Ziel%0AAbstract%3A%20We%20present%20a%20novel%20recurrent%20neural%20network%20architecture%20specifically%20designed%20for%20day-ahead%20electricity%20price%20forecasting%2C%20aimed%20at%20improving%20short-term%20decision-making%20and%20operational%20management%20in%20energy%20systems.%20Our%20combined%20forecasting%20model%20embeds%20linear%20structures%2C%20such%20as%20expert%20models%20and%20Kalman%20filters%2C%20into%20recurrent%20networks%2C%20enabling%20efficient%20computation%20and%20enhanced%20interpretability.%20The%20design%20leverages%20the%20strengths%20of%20both%20linear%20and%20non-linear%20model%20structures%2C%20allowing%20it%20to%20capture%20all%20relevant%20stylized%20price%20characteristics%20in%20power%20markets%2C%20including%20calendar%20and%20autoregressive%20effects%2C%20as%20well%20as%20influences%20from%20load%2C%20renewable%20energy%2C%20and%20related%20fuel%20and%20carbon%20markets.%20For%20empirical%20testing%2C%20we%20use%20hourly%20data%20from%20the%20largest%20European%20electricity%20market%20spanning%202018%20to%202025%20in%20a%20comprehensive%20forecasting%20study%2C%20comparing%20our%20model%20against%20state-of-the-art%20approaches%2C%20particularly%20high-dimensional%20linear%20and%20neural%20network%20models.%20In%20terms%20of%20RMSE%2C%20the%20proposed%20model%20achieves%20approximately%2011%25%20higher%20accuracy%20than%20the%20best-performing%20benchmark.%20We%20evaluate%20the%20contributions%20of%20the%20interpretable%20model%20components%20and%20conclude%20on%20the%20impact%20of%20combining%20linear%20and%20non-linear%20structures.%20We%20further%20evaluate%20the%20temporal%20robustness%20of%20the%20model%20by%20examining%20the%20stability%20of%20hyperparameters%20and%20the%20economic%20significance%20of%20key%20features.%20Additionally%2C%20we%20introduce%20a%20probabilistic%20extension%20to%20quantify%20forecast%20uncertainty.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04690v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRecurrent%2520Neural%2520Networks%2520with%2520Linear%2520Structures%2520for%2520Electricity%2520Price%2520Forecasting%26entry.906535625%3DSouhir%2520Ben%2520Amor%2520and%2520Florian%2520Ziel%26entry.1292438233%3DWe%2520present%2520a%2520novel%2520recurrent%2520neural%2520network%2520architecture%2520specifically%2520designed%2520for%2520day-ahead%2520electricity%2520price%2520forecasting%252C%2520aimed%2520at%2520improving%2520short-term%2520decision-making%2520and%2520operational%2520management%2520in%2520energy%2520systems.%2520Our%2520combined%2520forecasting%2520model%2520embeds%2520linear%2520structures%252C%2520such%2520as%2520expert%2520models%2520and%2520Kalman%2520filters%252C%2520into%2520recurrent%2520networks%252C%2520enabling%2520efficient%2520computation%2520and%2520enhanced%2520interpretability.%2520The%2520design%2520leverages%2520the%2520strengths%2520of%2520both%2520linear%2520and%2520non-linear%2520model%2520structures%252C%2520allowing%2520it%2520to%2520capture%2520all%2520relevant%2520stylized%2520price%2520characteristics%2520in%2520power%2520markets%252C%2520including%2520calendar%2520and%2520autoregressive%2520effects%252C%2520as%2520well%2520as%2520influences%2520from%2520load%252C%2520renewable%2520energy%252C%2520and%2520related%2520fuel%2520and%2520carbon%2520markets.%2520For%2520empirical%2520testing%252C%2520we%2520use%2520hourly%2520data%2520from%2520the%2520largest%2520European%2520electricity%2520market%2520spanning%25202018%2520to%25202025%2520in%2520a%2520comprehensive%2520forecasting%2520study%252C%2520comparing%2520our%2520model%2520against%2520state-of-the-art%2520approaches%252C%2520particularly%2520high-dimensional%2520linear%2520and%2520neural%2520network%2520models.%2520In%2520terms%2520of%2520RMSE%252C%2520the%2520proposed%2520model%2520achieves%2520approximately%252011%2525%2520higher%2520accuracy%2520than%2520the%2520best-performing%2520benchmark.%2520We%2520evaluate%2520the%2520contributions%2520of%2520the%2520interpretable%2520model%2520components%2520and%2520conclude%2520on%2520the%2520impact%2520of%2520combining%2520linear%2520and%2520non-linear%2520structures.%2520We%2520further%2520evaluate%2520the%2520temporal%2520robustness%2520of%2520the%2520model%2520by%2520examining%2520the%2520stability%2520of%2520hyperparameters%2520and%2520the%2520economic%2520significance%2520of%2520key%2520features.%2520Additionally%252C%2520we%2520introduce%2520a%2520probabilistic%2520extension%2520to%2520quantify%2520forecast%2520uncertainty.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04690v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Recurrent%20Neural%20Networks%20with%20Linear%20Structures%20for%20Electricity%20Price%20Forecasting&entry.906535625=Souhir%20Ben%20Amor%20and%20Florian%20Ziel&entry.1292438233=We%20present%20a%20novel%20recurrent%20neural%20network%20architecture%20specifically%20designed%20for%20day-ahead%20electricity%20price%20forecasting%2C%20aimed%20at%20improving%20short-term%20decision-making%20and%20operational%20management%20in%20energy%20systems.%20Our%20combined%20forecasting%20model%20embeds%20linear%20structures%2C%20such%20as%20expert%20models%20and%20Kalman%20filters%2C%20into%20recurrent%20networks%2C%20enabling%20efficient%20computation%20and%20enhanced%20interpretability.%20The%20design%20leverages%20the%20strengths%20of%20both%20linear%20and%20non-linear%20model%20structures%2C%20allowing%20it%20to%20capture%20all%20relevant%20stylized%20price%20characteristics%20in%20power%20markets%2C%20including%20calendar%20and%20autoregressive%20effects%2C%20as%20well%20as%20influences%20from%20load%2C%20renewable%20energy%2C%20and%20related%20fuel%20and%20carbon%20markets.%20For%20empirical%20testing%2C%20we%20use%20hourly%20data%20from%20the%20largest%20European%20electricity%20market%20spanning%202018%20to%202025%20in%20a%20comprehensive%20forecasting%20study%2C%20comparing%20our%20model%20against%20state-of-the-art%20approaches%2C%20particularly%20high-dimensional%20linear%20and%20neural%20network%20models.%20In%20terms%20of%20RMSE%2C%20the%20proposed%20model%20achieves%20approximately%2011%25%20higher%20accuracy%20than%20the%20best-performing%20benchmark.%20We%20evaluate%20the%20contributions%20of%20the%20interpretable%20model%20components%20and%20conclude%20on%20the%20impact%20of%20combining%20linear%20and%20non-linear%20structures.%20We%20further%20evaluate%20the%20temporal%20robustness%20of%20the%20model%20by%20examining%20the%20stability%20of%20hyperparameters%20and%20the%20economic%20significance%20of%20key%20features.%20Additionally%2C%20we%20introduce%20a%20probabilistic%20extension%20to%20quantify%20forecast%20uncertainty.&entry.1838667208=http%3A//arxiv.org/abs/2512.04690v2&entry.124074799=Read"},
{"title": "Agent Benchmarks Fail Public Sector Requirements", "author": "Jonathan Rystr\u00f8m and Chris Schmitz and Karolina Korgul and Jan Batzner and Chris Russell", "abstract": "Deploying Large Language Model-based agents (LLM agents) in the public sector requires assuring that they meet the stringent legal, procedural, and structural requirements of public-sector institutions. Practitioners and researchers often turn to benchmarks for such assessments. However, it remains unclear what criteria benchmarks must meet to ensure they adequately reflect public-sector requirements, or how many existing benchmarks do so. In this paper, we first define such criteria based on a first-principles survey of public administration literature: benchmarks must be \\emph{process-based}, \\emph{realistic}, \\emph{public-sector-specific} and report \\emph{metrics} that reflect the unique requirements of the public sector. We analyse more than 1,300 benchmark papers for these criteria using an expert-validated LLM-assisted pipeline. Our results show that no single benchmark meets all of the criteria. Our findings provide a call to action for both researchers to develop public sector-relevant benchmarks and for public-sector officials to apply these criteria when evaluating their own agentic use cases.", "link": "http://arxiv.org/abs/2601.20617v1", "date": "2026-01-28", "relevancy": 1.5706, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4058}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.39}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.39}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agent%20Benchmarks%20Fail%20Public%20Sector%20Requirements&body=Title%3A%20Agent%20Benchmarks%20Fail%20Public%20Sector%20Requirements%0AAuthor%3A%20Jonathan%20Rystr%C3%B8m%20and%20Chris%20Schmitz%20and%20Karolina%20Korgul%20and%20Jan%20Batzner%20and%20Chris%20Russell%0AAbstract%3A%20Deploying%20Large%20Language%20Model-based%20agents%20%28LLM%20agents%29%20in%20the%20public%20sector%20requires%20assuring%20that%20they%20meet%20the%20stringent%20legal%2C%20procedural%2C%20and%20structural%20requirements%20of%20public-sector%20institutions.%20Practitioners%20and%20researchers%20often%20turn%20to%20benchmarks%20for%20such%20assessments.%20However%2C%20it%20remains%20unclear%20what%20criteria%20benchmarks%20must%20meet%20to%20ensure%20they%20adequately%20reflect%20public-sector%20requirements%2C%20or%20how%20many%20existing%20benchmarks%20do%20so.%20In%20this%20paper%2C%20we%20first%20define%20such%20criteria%20based%20on%20a%20first-principles%20survey%20of%20public%20administration%20literature%3A%20benchmarks%20must%20be%20%5Cemph%7Bprocess-based%7D%2C%20%5Cemph%7Brealistic%7D%2C%20%5Cemph%7Bpublic-sector-specific%7D%20and%20report%20%5Cemph%7Bmetrics%7D%20that%20reflect%20the%20unique%20requirements%20of%20the%20public%20sector.%20We%20analyse%20more%20than%201%2C300%20benchmark%20papers%20for%20these%20criteria%20using%20an%20expert-validated%20LLM-assisted%20pipeline.%20Our%20results%20show%20that%20no%20single%20benchmark%20meets%20all%20of%20the%20criteria.%20Our%20findings%20provide%20a%20call%20to%20action%20for%20both%20researchers%20to%20develop%20public%20sector-relevant%20benchmarks%20and%20for%20public-sector%20officials%20to%20apply%20these%20criteria%20when%20evaluating%20their%20own%20agentic%20use%20cases.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20617v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgent%2520Benchmarks%2520Fail%2520Public%2520Sector%2520Requirements%26entry.906535625%3DJonathan%2520Rystr%25C3%25B8m%2520and%2520Chris%2520Schmitz%2520and%2520Karolina%2520Korgul%2520and%2520Jan%2520Batzner%2520and%2520Chris%2520Russell%26entry.1292438233%3DDeploying%2520Large%2520Language%2520Model-based%2520agents%2520%2528LLM%2520agents%2529%2520in%2520the%2520public%2520sector%2520requires%2520assuring%2520that%2520they%2520meet%2520the%2520stringent%2520legal%252C%2520procedural%252C%2520and%2520structural%2520requirements%2520of%2520public-sector%2520institutions.%2520Practitioners%2520and%2520researchers%2520often%2520turn%2520to%2520benchmarks%2520for%2520such%2520assessments.%2520However%252C%2520it%2520remains%2520unclear%2520what%2520criteria%2520benchmarks%2520must%2520meet%2520to%2520ensure%2520they%2520adequately%2520reflect%2520public-sector%2520requirements%252C%2520or%2520how%2520many%2520existing%2520benchmarks%2520do%2520so.%2520In%2520this%2520paper%252C%2520we%2520first%2520define%2520such%2520criteria%2520based%2520on%2520a%2520first-principles%2520survey%2520of%2520public%2520administration%2520literature%253A%2520benchmarks%2520must%2520be%2520%255Cemph%257Bprocess-based%257D%252C%2520%255Cemph%257Brealistic%257D%252C%2520%255Cemph%257Bpublic-sector-specific%257D%2520and%2520report%2520%255Cemph%257Bmetrics%257D%2520that%2520reflect%2520the%2520unique%2520requirements%2520of%2520the%2520public%2520sector.%2520We%2520analyse%2520more%2520than%25201%252C300%2520benchmark%2520papers%2520for%2520these%2520criteria%2520using%2520an%2520expert-validated%2520LLM-assisted%2520pipeline.%2520Our%2520results%2520show%2520that%2520no%2520single%2520benchmark%2520meets%2520all%2520of%2520the%2520criteria.%2520Our%2520findings%2520provide%2520a%2520call%2520to%2520action%2520for%2520both%2520researchers%2520to%2520develop%2520public%2520sector-relevant%2520benchmarks%2520and%2520for%2520public-sector%2520officials%2520to%2520apply%2520these%2520criteria%2520when%2520evaluating%2520their%2520own%2520agentic%2520use%2520cases.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20617v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agent%20Benchmarks%20Fail%20Public%20Sector%20Requirements&entry.906535625=Jonathan%20Rystr%C3%B8m%20and%20Chris%20Schmitz%20and%20Karolina%20Korgul%20and%20Jan%20Batzner%20and%20Chris%20Russell&entry.1292438233=Deploying%20Large%20Language%20Model-based%20agents%20%28LLM%20agents%29%20in%20the%20public%20sector%20requires%20assuring%20that%20they%20meet%20the%20stringent%20legal%2C%20procedural%2C%20and%20structural%20requirements%20of%20public-sector%20institutions.%20Practitioners%20and%20researchers%20often%20turn%20to%20benchmarks%20for%20such%20assessments.%20However%2C%20it%20remains%20unclear%20what%20criteria%20benchmarks%20must%20meet%20to%20ensure%20they%20adequately%20reflect%20public-sector%20requirements%2C%20or%20how%20many%20existing%20benchmarks%20do%20so.%20In%20this%20paper%2C%20we%20first%20define%20such%20criteria%20based%20on%20a%20first-principles%20survey%20of%20public%20administration%20literature%3A%20benchmarks%20must%20be%20%5Cemph%7Bprocess-based%7D%2C%20%5Cemph%7Brealistic%7D%2C%20%5Cemph%7Bpublic-sector-specific%7D%20and%20report%20%5Cemph%7Bmetrics%7D%20that%20reflect%20the%20unique%20requirements%20of%20the%20public%20sector.%20We%20analyse%20more%20than%201%2C300%20benchmark%20papers%20for%20these%20criteria%20using%20an%20expert-validated%20LLM-assisted%20pipeline.%20Our%20results%20show%20that%20no%20single%20benchmark%20meets%20all%20of%20the%20criteria.%20Our%20findings%20provide%20a%20call%20to%20action%20for%20both%20researchers%20to%20develop%20public%20sector-relevant%20benchmarks%20and%20for%20public-sector%20officials%20to%20apply%20these%20criteria%20when%20evaluating%20their%20own%20agentic%20use%20cases.&entry.1838667208=http%3A//arxiv.org/abs/2601.20617v1&entry.124074799=Read"},
{"title": "UDEEP: Edge-based Computer Vision for In-Situ Underwater Crayfish and Plastic Detection", "author": "Dennis Monari and Farhad Fassihi Tash and Jordan J. Bird and Ahmad Lotfi and Isibor Kennedy Ihianle and Salisu Wada Yahaya and Isibor Kennedy Ihianle and Md Mahmudul Hasan and Pedro Sousa and Pedro Machado", "abstract": "Invasive signal crayfish have a detrimental impact on ecosystems. They spread the fungal-type crayfish plague disease (Aphanomyces astaci) that is lethal to the native white clawed crayfish, the only native crayfish species in Britain. Invasive signal crayfish extensively burrow, causing habitat destruction, erosion of river banks and adverse changes in water quality, while also competing with native species for resources leading to declines in native populations. Moreover, pollution exacerbates the vulnerability of White-clawed crayfish, with their populations declining by over 90%. To safeguard aquatic ecosystems, it is imperative to address the challenges posed by invasive species and pollution in aquatic ecosystem's. This article introduces the Cognitive Edge Device (CED) computing platform for the detection of crayfish and plastic. It also presents two publicly available underwater datasets, annotated with sequences of crayfish and aquatic plastic debris. Four You Only Look Once (YOLO) variants were trained and evaluated for crayfish and plastic object detection. YOLOv5s achieved the highest detection accuracy, with an mAP@0.5 of 0.90, and achieved the best precision", "link": "http://arxiv.org/abs/2401.06157v2", "date": "2026-01-28", "relevancy": 1.9974, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5101}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4972}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UDEEP%3A%20Edge-based%20Computer%20Vision%20for%20In-Situ%20Underwater%20Crayfish%20and%20Plastic%20Detection&body=Title%3A%20UDEEP%3A%20Edge-based%20Computer%20Vision%20for%20In-Situ%20Underwater%20Crayfish%20and%20Plastic%20Detection%0AAuthor%3A%20Dennis%20Monari%20and%20Farhad%20Fassihi%20Tash%20and%20Jordan%20J.%20Bird%20and%20Ahmad%20Lotfi%20and%20Isibor%20Kennedy%20Ihianle%20and%20Salisu%20Wada%20Yahaya%20and%20Isibor%20Kennedy%20Ihianle%20and%20Md%20Mahmudul%20Hasan%20and%20Pedro%20Sousa%20and%20Pedro%20Machado%0AAbstract%3A%20Invasive%20signal%20crayfish%20have%20a%20detrimental%20impact%20on%20ecosystems.%20They%20spread%20the%20fungal-type%20crayfish%20plague%20disease%20%28Aphanomyces%20astaci%29%20that%20is%20lethal%20to%20the%20native%20white%20clawed%20crayfish%2C%20the%20only%20native%20crayfish%20species%20in%20Britain.%20Invasive%20signal%20crayfish%20extensively%20burrow%2C%20causing%20habitat%20destruction%2C%20erosion%20of%20river%20banks%20and%20adverse%20changes%20in%20water%20quality%2C%20while%20also%20competing%20with%20native%20species%20for%20resources%20leading%20to%20declines%20in%20native%20populations.%20Moreover%2C%20pollution%20exacerbates%20the%20vulnerability%20of%20White-clawed%20crayfish%2C%20with%20their%20populations%20declining%20by%20over%2090%25.%20To%20safeguard%20aquatic%20ecosystems%2C%20it%20is%20imperative%20to%20address%20the%20challenges%20posed%20by%20invasive%20species%20and%20pollution%20in%20aquatic%20ecosystem%27s.%20This%20article%20introduces%20the%20Cognitive%20Edge%20Device%20%28CED%29%20computing%20platform%20for%20the%20detection%20of%20crayfish%20and%20plastic.%20It%20also%20presents%20two%20publicly%20available%20underwater%20datasets%2C%20annotated%20with%20sequences%20of%20crayfish%20and%20aquatic%20plastic%20debris.%20Four%20You%20Only%20Look%20Once%20%28YOLO%29%20variants%20were%20trained%20and%20evaluated%20for%20crayfish%20and%20plastic%20object%20detection.%20YOLOv5s%20achieved%20the%20highest%20detection%20accuracy%2C%20with%20an%20mAP%400.5%20of%200.90%2C%20and%20achieved%20the%20best%20precision%0ALink%3A%20http%3A//arxiv.org/abs/2401.06157v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUDEEP%253A%2520Edge-based%2520Computer%2520Vision%2520for%2520In-Situ%2520Underwater%2520Crayfish%2520and%2520Plastic%2520Detection%26entry.906535625%3DDennis%2520Monari%2520and%2520Farhad%2520Fassihi%2520Tash%2520and%2520Jordan%2520J.%2520Bird%2520and%2520Ahmad%2520Lotfi%2520and%2520Isibor%2520Kennedy%2520Ihianle%2520and%2520Salisu%2520Wada%2520Yahaya%2520and%2520Isibor%2520Kennedy%2520Ihianle%2520and%2520Md%2520Mahmudul%2520Hasan%2520and%2520Pedro%2520Sousa%2520and%2520Pedro%2520Machado%26entry.1292438233%3DInvasive%2520signal%2520crayfish%2520have%2520a%2520detrimental%2520impact%2520on%2520ecosystems.%2520They%2520spread%2520the%2520fungal-type%2520crayfish%2520plague%2520disease%2520%2528Aphanomyces%2520astaci%2529%2520that%2520is%2520lethal%2520to%2520the%2520native%2520white%2520clawed%2520crayfish%252C%2520the%2520only%2520native%2520crayfish%2520species%2520in%2520Britain.%2520Invasive%2520signal%2520crayfish%2520extensively%2520burrow%252C%2520causing%2520habitat%2520destruction%252C%2520erosion%2520of%2520river%2520banks%2520and%2520adverse%2520changes%2520in%2520water%2520quality%252C%2520while%2520also%2520competing%2520with%2520native%2520species%2520for%2520resources%2520leading%2520to%2520declines%2520in%2520native%2520populations.%2520Moreover%252C%2520pollution%2520exacerbates%2520the%2520vulnerability%2520of%2520White-clawed%2520crayfish%252C%2520with%2520their%2520populations%2520declining%2520by%2520over%252090%2525.%2520To%2520safeguard%2520aquatic%2520ecosystems%252C%2520it%2520is%2520imperative%2520to%2520address%2520the%2520challenges%2520posed%2520by%2520invasive%2520species%2520and%2520pollution%2520in%2520aquatic%2520ecosystem%2527s.%2520This%2520article%2520introduces%2520the%2520Cognitive%2520Edge%2520Device%2520%2528CED%2529%2520computing%2520platform%2520for%2520the%2520detection%2520of%2520crayfish%2520and%2520plastic.%2520It%2520also%2520presents%2520two%2520publicly%2520available%2520underwater%2520datasets%252C%2520annotated%2520with%2520sequences%2520of%2520crayfish%2520and%2520aquatic%2520plastic%2520debris.%2520Four%2520You%2520Only%2520Look%2520Once%2520%2528YOLO%2529%2520variants%2520were%2520trained%2520and%2520evaluated%2520for%2520crayfish%2520and%2520plastic%2520object%2520detection.%2520YOLOv5s%2520achieved%2520the%2520highest%2520detection%2520accuracy%252C%2520with%2520an%2520mAP%25400.5%2520of%25200.90%252C%2520and%2520achieved%2520the%2520best%2520precision%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.06157v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UDEEP%3A%20Edge-based%20Computer%20Vision%20for%20In-Situ%20Underwater%20Crayfish%20and%20Plastic%20Detection&entry.906535625=Dennis%20Monari%20and%20Farhad%20Fassihi%20Tash%20and%20Jordan%20J.%20Bird%20and%20Ahmad%20Lotfi%20and%20Isibor%20Kennedy%20Ihianle%20and%20Salisu%20Wada%20Yahaya%20and%20Isibor%20Kennedy%20Ihianle%20and%20Md%20Mahmudul%20Hasan%20and%20Pedro%20Sousa%20and%20Pedro%20Machado&entry.1292438233=Invasive%20signal%20crayfish%20have%20a%20detrimental%20impact%20on%20ecosystems.%20They%20spread%20the%20fungal-type%20crayfish%20plague%20disease%20%28Aphanomyces%20astaci%29%20that%20is%20lethal%20to%20the%20native%20white%20clawed%20crayfish%2C%20the%20only%20native%20crayfish%20species%20in%20Britain.%20Invasive%20signal%20crayfish%20extensively%20burrow%2C%20causing%20habitat%20destruction%2C%20erosion%20of%20river%20banks%20and%20adverse%20changes%20in%20water%20quality%2C%20while%20also%20competing%20with%20native%20species%20for%20resources%20leading%20to%20declines%20in%20native%20populations.%20Moreover%2C%20pollution%20exacerbates%20the%20vulnerability%20of%20White-clawed%20crayfish%2C%20with%20their%20populations%20declining%20by%20over%2090%25.%20To%20safeguard%20aquatic%20ecosystems%2C%20it%20is%20imperative%20to%20address%20the%20challenges%20posed%20by%20invasive%20species%20and%20pollution%20in%20aquatic%20ecosystem%27s.%20This%20article%20introduces%20the%20Cognitive%20Edge%20Device%20%28CED%29%20computing%20platform%20for%20the%20detection%20of%20crayfish%20and%20plastic.%20It%20also%20presents%20two%20publicly%20available%20underwater%20datasets%2C%20annotated%20with%20sequences%20of%20crayfish%20and%20aquatic%20plastic%20debris.%20Four%20You%20Only%20Look%20Once%20%28YOLO%29%20variants%20were%20trained%20and%20evaluated%20for%20crayfish%20and%20plastic%20object%20detection.%20YOLOv5s%20achieved%20the%20highest%20detection%20accuracy%2C%20with%20an%20mAP%400.5%20of%200.90%2C%20and%20achieved%20the%20best%20precision&entry.1838667208=http%3A//arxiv.org/abs/2401.06157v2&entry.124074799=Read"},
{"title": "AdaSCALE: Adaptive Scaling for OOD Detection", "author": "Sudarshan Regmi", "abstract": "The ability of the deep learning model to recognize when a sample falls outside its learned distribution is critical for safe and reliable deployment. Recent state-of-the-art out-of-distribution (OOD) detection methods leverage activation shaping to improve the separation between in-distribution (ID) and OOD inputs. These approaches resort to sample-specific scaling but apply a static percentile threshold across all samples regardless of their nature, resulting in suboptimal ID-OOD separability. In this work, we propose \\textbf{AdaSCALE}, an adaptive scaling procedure that dynamically adjusts the percentile threshold based on a sample's estimated OOD likelihood. This estimation leverages our key observation: OOD samples exhibit significantly more pronounced activation shifts at high-magnitude activations under minor perturbation compared to ID samples. AdaSCALE enables stronger scaling for likely ID samples and weaker scaling for likely OOD samples, yielding highly separable energy scores. Our approach achieves state-of-the-art OOD detection performance, outperforming the latest rival OptFS by 14.94% in near-OOD and 21.67% in far-OOD datasets in average FPR@95 metric on the ImageNet-1k benchmark across eight diverse architectures. The code is available at: https://github.com/sudarshanregmi/AdaSCALE/", "link": "http://arxiv.org/abs/2503.08023v3", "date": "2026-01-28", "relevancy": 2.0099, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5142}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5017}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaSCALE%3A%20Adaptive%20Scaling%20for%20OOD%20Detection&body=Title%3A%20AdaSCALE%3A%20Adaptive%20Scaling%20for%20OOD%20Detection%0AAuthor%3A%20Sudarshan%20Regmi%0AAbstract%3A%20The%20ability%20of%20the%20deep%20learning%20model%20to%20recognize%20when%20a%20sample%20falls%20outside%20its%20learned%20distribution%20is%20critical%20for%20safe%20and%20reliable%20deployment.%20Recent%20state-of-the-art%20out-of-distribution%20%28OOD%29%20detection%20methods%20leverage%20activation%20shaping%20to%20improve%20the%20separation%20between%20in-distribution%20%28ID%29%20and%20OOD%20inputs.%20These%20approaches%20resort%20to%20sample-specific%20scaling%20but%20apply%20a%20static%20percentile%20threshold%20across%20all%20samples%20regardless%20of%20their%20nature%2C%20resulting%20in%20suboptimal%20ID-OOD%20separability.%20In%20this%20work%2C%20we%20propose%20%5Ctextbf%7BAdaSCALE%7D%2C%20an%20adaptive%20scaling%20procedure%20that%20dynamically%20adjusts%20the%20percentile%20threshold%20based%20on%20a%20sample%27s%20estimated%20OOD%20likelihood.%20This%20estimation%20leverages%20our%20key%20observation%3A%20OOD%20samples%20exhibit%20significantly%20more%20pronounced%20activation%20shifts%20at%20high-magnitude%20activations%20under%20minor%20perturbation%20compared%20to%20ID%20samples.%20AdaSCALE%20enables%20stronger%20scaling%20for%20likely%20ID%20samples%20and%20weaker%20scaling%20for%20likely%20OOD%20samples%2C%20yielding%20highly%20separable%20energy%20scores.%20Our%20approach%20achieves%20state-of-the-art%20OOD%20detection%20performance%2C%20outperforming%20the%20latest%20rival%20OptFS%20by%2014.94%25%20in%20near-OOD%20and%2021.67%25%20in%20far-OOD%20datasets%20in%20average%20FPR%4095%20metric%20on%20the%20ImageNet-1k%20benchmark%20across%20eight%20diverse%20architectures.%20The%20code%20is%20available%20at%3A%20https%3A//github.com/sudarshanregmi/AdaSCALE/%0ALink%3A%20http%3A//arxiv.org/abs/2503.08023v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaSCALE%253A%2520Adaptive%2520Scaling%2520for%2520OOD%2520Detection%26entry.906535625%3DSudarshan%2520Regmi%26entry.1292438233%3DThe%2520ability%2520of%2520the%2520deep%2520learning%2520model%2520to%2520recognize%2520when%2520a%2520sample%2520falls%2520outside%2520its%2520learned%2520distribution%2520is%2520critical%2520for%2520safe%2520and%2520reliable%2520deployment.%2520Recent%2520state-of-the-art%2520out-of-distribution%2520%2528OOD%2529%2520detection%2520methods%2520leverage%2520activation%2520shaping%2520to%2520improve%2520the%2520separation%2520between%2520in-distribution%2520%2528ID%2529%2520and%2520OOD%2520inputs.%2520These%2520approaches%2520resort%2520to%2520sample-specific%2520scaling%2520but%2520apply%2520a%2520static%2520percentile%2520threshold%2520across%2520all%2520samples%2520regardless%2520of%2520their%2520nature%252C%2520resulting%2520in%2520suboptimal%2520ID-OOD%2520separability.%2520In%2520this%2520work%252C%2520we%2520propose%2520%255Ctextbf%257BAdaSCALE%257D%252C%2520an%2520adaptive%2520scaling%2520procedure%2520that%2520dynamically%2520adjusts%2520the%2520percentile%2520threshold%2520based%2520on%2520a%2520sample%2527s%2520estimated%2520OOD%2520likelihood.%2520This%2520estimation%2520leverages%2520our%2520key%2520observation%253A%2520OOD%2520samples%2520exhibit%2520significantly%2520more%2520pronounced%2520activation%2520shifts%2520at%2520high-magnitude%2520activations%2520under%2520minor%2520perturbation%2520compared%2520to%2520ID%2520samples.%2520AdaSCALE%2520enables%2520stronger%2520scaling%2520for%2520likely%2520ID%2520samples%2520and%2520weaker%2520scaling%2520for%2520likely%2520OOD%2520samples%252C%2520yielding%2520highly%2520separable%2520energy%2520scores.%2520Our%2520approach%2520achieves%2520state-of-the-art%2520OOD%2520detection%2520performance%252C%2520outperforming%2520the%2520latest%2520rival%2520OptFS%2520by%252014.94%2525%2520in%2520near-OOD%2520and%252021.67%2525%2520in%2520far-OOD%2520datasets%2520in%2520average%2520FPR%254095%2520metric%2520on%2520the%2520ImageNet-1k%2520benchmark%2520across%2520eight%2520diverse%2520architectures.%2520The%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/sudarshanregmi/AdaSCALE/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.08023v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaSCALE%3A%20Adaptive%20Scaling%20for%20OOD%20Detection&entry.906535625=Sudarshan%20Regmi&entry.1292438233=The%20ability%20of%20the%20deep%20learning%20model%20to%20recognize%20when%20a%20sample%20falls%20outside%20its%20learned%20distribution%20is%20critical%20for%20safe%20and%20reliable%20deployment.%20Recent%20state-of-the-art%20out-of-distribution%20%28OOD%29%20detection%20methods%20leverage%20activation%20shaping%20to%20improve%20the%20separation%20between%20in-distribution%20%28ID%29%20and%20OOD%20inputs.%20These%20approaches%20resort%20to%20sample-specific%20scaling%20but%20apply%20a%20static%20percentile%20threshold%20across%20all%20samples%20regardless%20of%20their%20nature%2C%20resulting%20in%20suboptimal%20ID-OOD%20separability.%20In%20this%20work%2C%20we%20propose%20%5Ctextbf%7BAdaSCALE%7D%2C%20an%20adaptive%20scaling%20procedure%20that%20dynamically%20adjusts%20the%20percentile%20threshold%20based%20on%20a%20sample%27s%20estimated%20OOD%20likelihood.%20This%20estimation%20leverages%20our%20key%20observation%3A%20OOD%20samples%20exhibit%20significantly%20more%20pronounced%20activation%20shifts%20at%20high-magnitude%20activations%20under%20minor%20perturbation%20compared%20to%20ID%20samples.%20AdaSCALE%20enables%20stronger%20scaling%20for%20likely%20ID%20samples%20and%20weaker%20scaling%20for%20likely%20OOD%20samples%2C%20yielding%20highly%20separable%20energy%20scores.%20Our%20approach%20achieves%20state-of-the-art%20OOD%20detection%20performance%2C%20outperforming%20the%20latest%20rival%20OptFS%20by%2014.94%25%20in%20near-OOD%20and%2021.67%25%20in%20far-OOD%20datasets%20in%20average%20FPR%4095%20metric%20on%20the%20ImageNet-1k%20benchmark%20across%20eight%20diverse%20architectures.%20The%20code%20is%20available%20at%3A%20https%3A//github.com/sudarshanregmi/AdaSCALE/&entry.1838667208=http%3A//arxiv.org/abs/2503.08023v3&entry.124074799=Read"},
{"title": "SimpleMem: Efficient Lifelong Memory for LLM Agents", "author": "Jiaqi Liu and Yaofeng Su and Peng Xia and Siwei Han and Zeyu Zheng and Cihang Xie and Mingyu Ding and Huaxiu Yao", "abstract": "To support long-term interaction in complex environments, LLM agents require memory systems that manage historical experiences. Existing approaches either retain full interaction histories via passive context extension, leading to substantial redundancy, or rely on iterative reasoning to filter noise, incurring high token costs. To address this challenge, we introduce SimpleMem, an efficient memory framework based on semantic lossless compression. We propose a three-stage pipeline designed to maximize information density and token utilization: (1) Semantic Structured Compression, which distills unstructured interactions into compact, multi-view indexed memory units; (2) Online Semantic Synthesis, an intra-session process that instantly integrates related context into unified abstract representations to eliminate redundancy; and (3) Intent-Aware Retrieval Planning, which infers search intent to dynamically determine retrieval scope and construct precise context efficiently. Experiments on benchmark datasets show that our method consistently outperforms baseline approaches in accuracy, retrieval efficiency, and inference cost, achieving an average F1 improvement of 26.4% while reducing inference-time token consumption by up to 30-fold, demonstrating a superior balance between performance and efficiency. Code is available at https://github.com/aiming-lab/SimpleMem.", "link": "http://arxiv.org/abs/2601.02553v2", "date": "2026-01-28", "relevancy": 1.9592, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5077}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4841}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SimpleMem%3A%20Efficient%20Lifelong%20Memory%20for%20LLM%20Agents&body=Title%3A%20SimpleMem%3A%20Efficient%20Lifelong%20Memory%20for%20LLM%20Agents%0AAuthor%3A%20Jiaqi%20Liu%20and%20Yaofeng%20Su%20and%20Peng%20Xia%20and%20Siwei%20Han%20and%20Zeyu%20Zheng%20and%20Cihang%20Xie%20and%20Mingyu%20Ding%20and%20Huaxiu%20Yao%0AAbstract%3A%20To%20support%20long-term%20interaction%20in%20complex%20environments%2C%20LLM%20agents%20require%20memory%20systems%20that%20manage%20historical%20experiences.%20Existing%20approaches%20either%20retain%20full%20interaction%20histories%20via%20passive%20context%20extension%2C%20leading%20to%20substantial%20redundancy%2C%20or%20rely%20on%20iterative%20reasoning%20to%20filter%20noise%2C%20incurring%20high%20token%20costs.%20To%20address%20this%20challenge%2C%20we%20introduce%20SimpleMem%2C%20an%20efficient%20memory%20framework%20based%20on%20semantic%20lossless%20compression.%20We%20propose%20a%20three-stage%20pipeline%20designed%20to%20maximize%20information%20density%20and%20token%20utilization%3A%20%281%29%20Semantic%20Structured%20Compression%2C%20which%20distills%20unstructured%20interactions%20into%20compact%2C%20multi-view%20indexed%20memory%20units%3B%20%282%29%20Online%20Semantic%20Synthesis%2C%20an%20intra-session%20process%20that%20instantly%20integrates%20related%20context%20into%20unified%20abstract%20representations%20to%20eliminate%20redundancy%3B%20and%20%283%29%20Intent-Aware%20Retrieval%20Planning%2C%20which%20infers%20search%20intent%20to%20dynamically%20determine%20retrieval%20scope%20and%20construct%20precise%20context%20efficiently.%20Experiments%20on%20benchmark%20datasets%20show%20that%20our%20method%20consistently%20outperforms%20baseline%20approaches%20in%20accuracy%2C%20retrieval%20efficiency%2C%20and%20inference%20cost%2C%20achieving%20an%20average%20F1%20improvement%20of%2026.4%25%20while%20reducing%20inference-time%20token%20consumption%20by%20up%20to%2030-fold%2C%20demonstrating%20a%20superior%20balance%20between%20performance%20and%20efficiency.%20Code%20is%20available%20at%20https%3A//github.com/aiming-lab/SimpleMem.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02553v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimpleMem%253A%2520Efficient%2520Lifelong%2520Memory%2520for%2520LLM%2520Agents%26entry.906535625%3DJiaqi%2520Liu%2520and%2520Yaofeng%2520Su%2520and%2520Peng%2520Xia%2520and%2520Siwei%2520Han%2520and%2520Zeyu%2520Zheng%2520and%2520Cihang%2520Xie%2520and%2520Mingyu%2520Ding%2520and%2520Huaxiu%2520Yao%26entry.1292438233%3DTo%2520support%2520long-term%2520interaction%2520in%2520complex%2520environments%252C%2520LLM%2520agents%2520require%2520memory%2520systems%2520that%2520manage%2520historical%2520experiences.%2520Existing%2520approaches%2520either%2520retain%2520full%2520interaction%2520histories%2520via%2520passive%2520context%2520extension%252C%2520leading%2520to%2520substantial%2520redundancy%252C%2520or%2520rely%2520on%2520iterative%2520reasoning%2520to%2520filter%2520noise%252C%2520incurring%2520high%2520token%2520costs.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520SimpleMem%252C%2520an%2520efficient%2520memory%2520framework%2520based%2520on%2520semantic%2520lossless%2520compression.%2520We%2520propose%2520a%2520three-stage%2520pipeline%2520designed%2520to%2520maximize%2520information%2520density%2520and%2520token%2520utilization%253A%2520%25281%2529%2520Semantic%2520Structured%2520Compression%252C%2520which%2520distills%2520unstructured%2520interactions%2520into%2520compact%252C%2520multi-view%2520indexed%2520memory%2520units%253B%2520%25282%2529%2520Online%2520Semantic%2520Synthesis%252C%2520an%2520intra-session%2520process%2520that%2520instantly%2520integrates%2520related%2520context%2520into%2520unified%2520abstract%2520representations%2520to%2520eliminate%2520redundancy%253B%2520and%2520%25283%2529%2520Intent-Aware%2520Retrieval%2520Planning%252C%2520which%2520infers%2520search%2520intent%2520to%2520dynamically%2520determine%2520retrieval%2520scope%2520and%2520construct%2520precise%2520context%2520efficiently.%2520Experiments%2520on%2520benchmark%2520datasets%2520show%2520that%2520our%2520method%2520consistently%2520outperforms%2520baseline%2520approaches%2520in%2520accuracy%252C%2520retrieval%2520efficiency%252C%2520and%2520inference%2520cost%252C%2520achieving%2520an%2520average%2520F1%2520improvement%2520of%252026.4%2525%2520while%2520reducing%2520inference-time%2520token%2520consumption%2520by%2520up%2520to%252030-fold%252C%2520demonstrating%2520a%2520superior%2520balance%2520between%2520performance%2520and%2520efficiency.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/aiming-lab/SimpleMem.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02553v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SimpleMem%3A%20Efficient%20Lifelong%20Memory%20for%20LLM%20Agents&entry.906535625=Jiaqi%20Liu%20and%20Yaofeng%20Su%20and%20Peng%20Xia%20and%20Siwei%20Han%20and%20Zeyu%20Zheng%20and%20Cihang%20Xie%20and%20Mingyu%20Ding%20and%20Huaxiu%20Yao&entry.1292438233=To%20support%20long-term%20interaction%20in%20complex%20environments%2C%20LLM%20agents%20require%20memory%20systems%20that%20manage%20historical%20experiences.%20Existing%20approaches%20either%20retain%20full%20interaction%20histories%20via%20passive%20context%20extension%2C%20leading%20to%20substantial%20redundancy%2C%20or%20rely%20on%20iterative%20reasoning%20to%20filter%20noise%2C%20incurring%20high%20token%20costs.%20To%20address%20this%20challenge%2C%20we%20introduce%20SimpleMem%2C%20an%20efficient%20memory%20framework%20based%20on%20semantic%20lossless%20compression.%20We%20propose%20a%20three-stage%20pipeline%20designed%20to%20maximize%20information%20density%20and%20token%20utilization%3A%20%281%29%20Semantic%20Structured%20Compression%2C%20which%20distills%20unstructured%20interactions%20into%20compact%2C%20multi-view%20indexed%20memory%20units%3B%20%282%29%20Online%20Semantic%20Synthesis%2C%20an%20intra-session%20process%20that%20instantly%20integrates%20related%20context%20into%20unified%20abstract%20representations%20to%20eliminate%20redundancy%3B%20and%20%283%29%20Intent-Aware%20Retrieval%20Planning%2C%20which%20infers%20search%20intent%20to%20dynamically%20determine%20retrieval%20scope%20and%20construct%20precise%20context%20efficiently.%20Experiments%20on%20benchmark%20datasets%20show%20that%20our%20method%20consistently%20outperforms%20baseline%20approaches%20in%20accuracy%2C%20retrieval%20efficiency%2C%20and%20inference%20cost%2C%20achieving%20an%20average%20F1%20improvement%20of%2026.4%25%20while%20reducing%20inference-time%20token%20consumption%20by%20up%20to%2030-fold%2C%20demonstrating%20a%20superior%20balance%20between%20performance%20and%20efficiency.%20Code%20is%20available%20at%20https%3A//github.com/aiming-lab/SimpleMem.&entry.1838667208=http%3A//arxiv.org/abs/2601.02553v2&entry.124074799=Read"},
{"title": "Normative Equivalence in human-AI Cooperation: Behaviour, Not Identity, Drives Cooperation in Mixed-Agent Groups", "author": "Nico Mutzner and Taha Yasseri and Heiko Rauhut", "abstract": "The introduction of artificial intelligence (AI) agents into human group settings raises essential questions about how these novel participants influence cooperative social norms. While previous studies on human-AI cooperation have primarily focused on dyadic interactions, little is known about how integrating AI agents affects the emergence and maintenance of cooperative norms in small groups. This study addresses this gap through an online experiment using a repeated four-player Public Goods Game (PGG). Each group consisted of three human participants and one bot, which was framed either as human or AI and followed one of three predefined decision strategies: unconditional cooperation, conditional cooperation, or free-riding. In our sample of 236 participants, we found that reciprocal group dynamics and behavioural inertia primarily drove cooperation. These normative mechanisms operated identically across conditions, resulting in cooperation levels that did not differ significantly between human and AI labels. Furthermore, we found no evidence of differences in norm persistence in a follow-up Prisoner's Dilemma, or in participants' normative perceptions. Participants' behaviour followed the same normative logic across human and AI conditions, indicating that cooperation depended on group behaviour rather than partner identity. This supports a pattern of normative equivalence, in which the mechanisms that sustain cooperation function similarly in mixed human-AI and all human groups. These findings suggest that cooperative norms are flexible enough to extend to artificial agents, blurring the boundary between humans and AI in collective decision-making.", "link": "http://arxiv.org/abs/2601.20487v1", "date": "2026-01-28", "relevancy": 1.2316, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4192}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4062}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.3931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Normative%20Equivalence%20in%20human-AI%20Cooperation%3A%20Behaviour%2C%20Not%20Identity%2C%20Drives%20Cooperation%20in%20Mixed-Agent%20Groups&body=Title%3A%20Normative%20Equivalence%20in%20human-AI%20Cooperation%3A%20Behaviour%2C%20Not%20Identity%2C%20Drives%20Cooperation%20in%20Mixed-Agent%20Groups%0AAuthor%3A%20Nico%20Mutzner%20and%20Taha%20Yasseri%20and%20Heiko%20Rauhut%0AAbstract%3A%20The%20introduction%20of%20artificial%20intelligence%20%28AI%29%20agents%20into%20human%20group%20settings%20raises%20essential%20questions%20about%20how%20these%20novel%20participants%20influence%20cooperative%20social%20norms.%20While%20previous%20studies%20on%20human-AI%20cooperation%20have%20primarily%20focused%20on%20dyadic%20interactions%2C%20little%20is%20known%20about%20how%20integrating%20AI%20agents%20affects%20the%20emergence%20and%20maintenance%20of%20cooperative%20norms%20in%20small%20groups.%20This%20study%20addresses%20this%20gap%20through%20an%20online%20experiment%20using%20a%20repeated%20four-player%20Public%20Goods%20Game%20%28PGG%29.%20Each%20group%20consisted%20of%20three%20human%20participants%20and%20one%20bot%2C%20which%20was%20framed%20either%20as%20human%20or%20AI%20and%20followed%20one%20of%20three%20predefined%20decision%20strategies%3A%20unconditional%20cooperation%2C%20conditional%20cooperation%2C%20or%20free-riding.%20In%20our%20sample%20of%20236%20participants%2C%20we%20found%20that%20reciprocal%20group%20dynamics%20and%20behavioural%20inertia%20primarily%20drove%20cooperation.%20These%20normative%20mechanisms%20operated%20identically%20across%20conditions%2C%20resulting%20in%20cooperation%20levels%20that%20did%20not%20differ%20significantly%20between%20human%20and%20AI%20labels.%20Furthermore%2C%20we%20found%20no%20evidence%20of%20differences%20in%20norm%20persistence%20in%20a%20follow-up%20Prisoner%27s%20Dilemma%2C%20or%20in%20participants%27%20normative%20perceptions.%20Participants%27%20behaviour%20followed%20the%20same%20normative%20logic%20across%20human%20and%20AI%20conditions%2C%20indicating%20that%20cooperation%20depended%20on%20group%20behaviour%20rather%20than%20partner%20identity.%20This%20supports%20a%20pattern%20of%20normative%20equivalence%2C%20in%20which%20the%20mechanisms%20that%20sustain%20cooperation%20function%20similarly%20in%20mixed%20human-AI%20and%20all%20human%20groups.%20These%20findings%20suggest%20that%20cooperative%20norms%20are%20flexible%20enough%20to%20extend%20to%20artificial%20agents%2C%20blurring%20the%20boundary%20between%20humans%20and%20AI%20in%20collective%20decision-making.%0ALink%3A%20http%3A//arxiv.org/abs/2601.20487v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNormative%2520Equivalence%2520in%2520human-AI%2520Cooperation%253A%2520Behaviour%252C%2520Not%2520Identity%252C%2520Drives%2520Cooperation%2520in%2520Mixed-Agent%2520Groups%26entry.906535625%3DNico%2520Mutzner%2520and%2520Taha%2520Yasseri%2520and%2520Heiko%2520Rauhut%26entry.1292438233%3DThe%2520introduction%2520of%2520artificial%2520intelligence%2520%2528AI%2529%2520agents%2520into%2520human%2520group%2520settings%2520raises%2520essential%2520questions%2520about%2520how%2520these%2520novel%2520participants%2520influence%2520cooperative%2520social%2520norms.%2520While%2520previous%2520studies%2520on%2520human-AI%2520cooperation%2520have%2520primarily%2520focused%2520on%2520dyadic%2520interactions%252C%2520little%2520is%2520known%2520about%2520how%2520integrating%2520AI%2520agents%2520affects%2520the%2520emergence%2520and%2520maintenance%2520of%2520cooperative%2520norms%2520in%2520small%2520groups.%2520This%2520study%2520addresses%2520this%2520gap%2520through%2520an%2520online%2520experiment%2520using%2520a%2520repeated%2520four-player%2520Public%2520Goods%2520Game%2520%2528PGG%2529.%2520Each%2520group%2520consisted%2520of%2520three%2520human%2520participants%2520and%2520one%2520bot%252C%2520which%2520was%2520framed%2520either%2520as%2520human%2520or%2520AI%2520and%2520followed%2520one%2520of%2520three%2520predefined%2520decision%2520strategies%253A%2520unconditional%2520cooperation%252C%2520conditional%2520cooperation%252C%2520or%2520free-riding.%2520In%2520our%2520sample%2520of%2520236%2520participants%252C%2520we%2520found%2520that%2520reciprocal%2520group%2520dynamics%2520and%2520behavioural%2520inertia%2520primarily%2520drove%2520cooperation.%2520These%2520normative%2520mechanisms%2520operated%2520identically%2520across%2520conditions%252C%2520resulting%2520in%2520cooperation%2520levels%2520that%2520did%2520not%2520differ%2520significantly%2520between%2520human%2520and%2520AI%2520labels.%2520Furthermore%252C%2520we%2520found%2520no%2520evidence%2520of%2520differences%2520in%2520norm%2520persistence%2520in%2520a%2520follow-up%2520Prisoner%2527s%2520Dilemma%252C%2520or%2520in%2520participants%2527%2520normative%2520perceptions.%2520Participants%2527%2520behaviour%2520followed%2520the%2520same%2520normative%2520logic%2520across%2520human%2520and%2520AI%2520conditions%252C%2520indicating%2520that%2520cooperation%2520depended%2520on%2520group%2520behaviour%2520rather%2520than%2520partner%2520identity.%2520This%2520supports%2520a%2520pattern%2520of%2520normative%2520equivalence%252C%2520in%2520which%2520the%2520mechanisms%2520that%2520sustain%2520cooperation%2520function%2520similarly%2520in%2520mixed%2520human-AI%2520and%2520all%2520human%2520groups.%2520These%2520findings%2520suggest%2520that%2520cooperative%2520norms%2520are%2520flexible%2520enough%2520to%2520extend%2520to%2520artificial%2520agents%252C%2520blurring%2520the%2520boundary%2520between%2520humans%2520and%2520AI%2520in%2520collective%2520decision-making.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.20487v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Normative%20Equivalence%20in%20human-AI%20Cooperation%3A%20Behaviour%2C%20Not%20Identity%2C%20Drives%20Cooperation%20in%20Mixed-Agent%20Groups&entry.906535625=Nico%20Mutzner%20and%20Taha%20Yasseri%20and%20Heiko%20Rauhut&entry.1292438233=The%20introduction%20of%20artificial%20intelligence%20%28AI%29%20agents%20into%20human%20group%20settings%20raises%20essential%20questions%20about%20how%20these%20novel%20participants%20influence%20cooperative%20social%20norms.%20While%20previous%20studies%20on%20human-AI%20cooperation%20have%20primarily%20focused%20on%20dyadic%20interactions%2C%20little%20is%20known%20about%20how%20integrating%20AI%20agents%20affects%20the%20emergence%20and%20maintenance%20of%20cooperative%20norms%20in%20small%20groups.%20This%20study%20addresses%20this%20gap%20through%20an%20online%20experiment%20using%20a%20repeated%20four-player%20Public%20Goods%20Game%20%28PGG%29.%20Each%20group%20consisted%20of%20three%20human%20participants%20and%20one%20bot%2C%20which%20was%20framed%20either%20as%20human%20or%20AI%20and%20followed%20one%20of%20three%20predefined%20decision%20strategies%3A%20unconditional%20cooperation%2C%20conditional%20cooperation%2C%20or%20free-riding.%20In%20our%20sample%20of%20236%20participants%2C%20we%20found%20that%20reciprocal%20group%20dynamics%20and%20behavioural%20inertia%20primarily%20drove%20cooperation.%20These%20normative%20mechanisms%20operated%20identically%20across%20conditions%2C%20resulting%20in%20cooperation%20levels%20that%20did%20not%20differ%20significantly%20between%20human%20and%20AI%20labels.%20Furthermore%2C%20we%20found%20no%20evidence%20of%20differences%20in%20norm%20persistence%20in%20a%20follow-up%20Prisoner%27s%20Dilemma%2C%20or%20in%20participants%27%20normative%20perceptions.%20Participants%27%20behaviour%20followed%20the%20same%20normative%20logic%20across%20human%20and%20AI%20conditions%2C%20indicating%20that%20cooperation%20depended%20on%20group%20behaviour%20rather%20than%20partner%20identity.%20This%20supports%20a%20pattern%20of%20normative%20equivalence%2C%20in%20which%20the%20mechanisms%20that%20sustain%20cooperation%20function%20similarly%20in%20mixed%20human-AI%20and%20all%20human%20groups.%20These%20findings%20suggest%20that%20cooperative%20norms%20are%20flexible%20enough%20to%20extend%20to%20artificial%20agents%2C%20blurring%20the%20boundary%20between%20humans%20and%20AI%20in%20collective%20decision-making.&entry.1838667208=http%3A//arxiv.org/abs/2601.20487v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


