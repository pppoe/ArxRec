<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241230.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Prometheus: 3D-Aware Latent Diffusion Models for Feed-Forward Text-to-3D\n  Scene Generation", "author": "Yuanbo Yang and Jiahao Shao and Xinyang Li and Yujun Shen and Andreas Geiger and Yiyi Liao", "abstract": "  In this work, we introduce Prometheus, a 3D-aware latent diffusion model for\ntext-to-3D generation at both object and scene levels in seconds. We formulate\n3D scene generation as multi-view, feed-forward, pixel-aligned 3D Gaussian\ngeneration within the latent diffusion paradigm. To ensure generalizability, we\nbuild our model upon pre-trained text-to-image generation model with only\nminimal adjustments, and further train it using a large number of images from\nboth single-view and multi-view datasets. Furthermore, we introduce an RGB-D\nlatent space into 3D Gaussian generation to disentangle appearance and geometry\ninformation, enabling efficient feed-forward generation of 3D Gaussians with\nbetter fidelity and geometry. Extensive experimental results demonstrate the\neffectiveness of our method in both feed-forward 3D Gaussian reconstruction and\ntext-to-3D generation. Project page:\nhttps://freemty.github.io/project-prometheus/\n", "link": "http://arxiv.org/abs/2412.21117v1", "date": "2024-12-30", "relevancy": 3.2942, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6674}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6674}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prometheus%3A%203D-Aware%20Latent%20Diffusion%20Models%20for%20Feed-Forward%20Text-to-3D%0A%20%20Scene%20Generation&body=Title%3A%20Prometheus%3A%203D-Aware%20Latent%20Diffusion%20Models%20for%20Feed-Forward%20Text-to-3D%0A%20%20Scene%20Generation%0AAuthor%3A%20Yuanbo%20Yang%20and%20Jiahao%20Shao%20and%20Xinyang%20Li%20and%20Yujun%20Shen%20and%20Andreas%20Geiger%20and%20Yiyi%20Liao%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20introduce%20Prometheus%2C%20a%203D-aware%20latent%20diffusion%20model%20for%0Atext-to-3D%20generation%20at%20both%20object%20and%20scene%20levels%20in%20seconds.%20We%20formulate%0A3D%20scene%20generation%20as%20multi-view%2C%20feed-forward%2C%20pixel-aligned%203D%20Gaussian%0Ageneration%20within%20the%20latent%20diffusion%20paradigm.%20To%20ensure%20generalizability%2C%20we%0Abuild%20our%20model%20upon%20pre-trained%20text-to-image%20generation%20model%20with%20only%0Aminimal%20adjustments%2C%20and%20further%20train%20it%20using%20a%20large%20number%20of%20images%20from%0Aboth%20single-view%20and%20multi-view%20datasets.%20Furthermore%2C%20we%20introduce%20an%20RGB-D%0Alatent%20space%20into%203D%20Gaussian%20generation%20to%20disentangle%20appearance%20and%20geometry%0Ainformation%2C%20enabling%20efficient%20feed-forward%20generation%20of%203D%20Gaussians%20with%0Abetter%20fidelity%20and%20geometry.%20Extensive%20experimental%20results%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%20in%20both%20feed-forward%203D%20Gaussian%20reconstruction%20and%0Atext-to-3D%20generation.%20Project%20page%3A%0Ahttps%3A//freemty.github.io/project-prometheus/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21117v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrometheus%253A%25203D-Aware%2520Latent%2520Diffusion%2520Models%2520for%2520Feed-Forward%2520Text-to-3D%250A%2520%2520Scene%2520Generation%26entry.906535625%3DYuanbo%2520Yang%2520and%2520Jiahao%2520Shao%2520and%2520Xinyang%2520Li%2520and%2520Yujun%2520Shen%2520and%2520Andreas%2520Geiger%2520and%2520Yiyi%2520Liao%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520introduce%2520Prometheus%252C%2520a%25203D-aware%2520latent%2520diffusion%2520model%2520for%250Atext-to-3D%2520generation%2520at%2520both%2520object%2520and%2520scene%2520levels%2520in%2520seconds.%2520We%2520formulate%250A3D%2520scene%2520generation%2520as%2520multi-view%252C%2520feed-forward%252C%2520pixel-aligned%25203D%2520Gaussian%250Ageneration%2520within%2520the%2520latent%2520diffusion%2520paradigm.%2520To%2520ensure%2520generalizability%252C%2520we%250Abuild%2520our%2520model%2520upon%2520pre-trained%2520text-to-image%2520generation%2520model%2520with%2520only%250Aminimal%2520adjustments%252C%2520and%2520further%2520train%2520it%2520using%2520a%2520large%2520number%2520of%2520images%2520from%250Aboth%2520single-view%2520and%2520multi-view%2520datasets.%2520Furthermore%252C%2520we%2520introduce%2520an%2520RGB-D%250Alatent%2520space%2520into%25203D%2520Gaussian%2520generation%2520to%2520disentangle%2520appearance%2520and%2520geometry%250Ainformation%252C%2520enabling%2520efficient%2520feed-forward%2520generation%2520of%25203D%2520Gaussians%2520with%250Abetter%2520fidelity%2520and%2520geometry.%2520Extensive%2520experimental%2520results%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520method%2520in%2520both%2520feed-forward%25203D%2520Gaussian%2520reconstruction%2520and%250Atext-to-3D%2520generation.%2520Project%2520page%253A%250Ahttps%253A//freemty.github.io/project-prometheus/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21117v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prometheus%3A%203D-Aware%20Latent%20Diffusion%20Models%20for%20Feed-Forward%20Text-to-3D%0A%20%20Scene%20Generation&entry.906535625=Yuanbo%20Yang%20and%20Jiahao%20Shao%20and%20Xinyang%20Li%20and%20Yujun%20Shen%20and%20Andreas%20Geiger%20and%20Yiyi%20Liao&entry.1292438233=%20%20In%20this%20work%2C%20we%20introduce%20Prometheus%2C%20a%203D-aware%20latent%20diffusion%20model%20for%0Atext-to-3D%20generation%20at%20both%20object%20and%20scene%20levels%20in%20seconds.%20We%20formulate%0A3D%20scene%20generation%20as%20multi-view%2C%20feed-forward%2C%20pixel-aligned%203D%20Gaussian%0Ageneration%20within%20the%20latent%20diffusion%20paradigm.%20To%20ensure%20generalizability%2C%20we%0Abuild%20our%20model%20upon%20pre-trained%20text-to-image%20generation%20model%20with%20only%0Aminimal%20adjustments%2C%20and%20further%20train%20it%20using%20a%20large%20number%20of%20images%20from%0Aboth%20single-view%20and%20multi-view%20datasets.%20Furthermore%2C%20we%20introduce%20an%20RGB-D%0Alatent%20space%20into%203D%20Gaussian%20generation%20to%20disentangle%20appearance%20and%20geometry%0Ainformation%2C%20enabling%20efficient%20feed-forward%20generation%20of%203D%20Gaussians%20with%0Abetter%20fidelity%20and%20geometry.%20Extensive%20experimental%20results%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%20in%20both%20feed-forward%203D%20Gaussian%20reconstruction%20and%0Atext-to-3D%20generation.%20Project%20page%3A%0Ahttps%3A//freemty.github.io/project-prometheus/%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21117v1&entry.124074799=Read"},
{"title": "PERSE: Personalized 3D Generative Avatars from A Single Portrait", "author": "Hyunsoo Cha and Inhee Lee and Hanbyul Joo", "abstract": "  We present PERSE, a method for building an animatable personalized generative\navatar from a reference portrait. Our avatar model enables facial attribute\nediting in a continuous and disentangled latent space to control each facial\nattribute, while preserving the individual's identity. To achieve this, our\nmethod begins by synthesizing large-scale synthetic 2D video datasets, where\neach video contains consistent changes in the facial expression and viewpoint,\ncombined with a variation in a specific facial attribute from the original\ninput. We propose a novel pipeline to produce high-quality, photorealistic 2D\nvideos with facial attribute editing. Leveraging this synthetic attribute\ndataset, we present a personalized avatar creation method based on the 3D\nGaussian Splatting, learning a continuous and disentangled latent space for\nintuitive facial attribute manipulation. To enforce smooth transitions in this\nlatent space, we introduce a latent space regularization technique by using\ninterpolated 2D faces as supervision. Compared to previous approaches, we\ndemonstrate that PERSE generates high-quality avatars with interpolated\nattributes while preserving identity of reference person.\n", "link": "http://arxiv.org/abs/2412.21206v1", "date": "2024-12-30", "relevancy": 3.1786, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.645}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.645}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PERSE%3A%20Personalized%203D%20Generative%20Avatars%20from%20A%20Single%20Portrait&body=Title%3A%20PERSE%3A%20Personalized%203D%20Generative%20Avatars%20from%20A%20Single%20Portrait%0AAuthor%3A%20Hyunsoo%20Cha%20and%20Inhee%20Lee%20and%20Hanbyul%20Joo%0AAbstract%3A%20%20%20We%20present%20PERSE%2C%20a%20method%20for%20building%20an%20animatable%20personalized%20generative%0Aavatar%20from%20a%20reference%20portrait.%20Our%20avatar%20model%20enables%20facial%20attribute%0Aediting%20in%20a%20continuous%20and%20disentangled%20latent%20space%20to%20control%20each%20facial%0Aattribute%2C%20while%20preserving%20the%20individual%27s%20identity.%20To%20achieve%20this%2C%20our%0Amethod%20begins%20by%20synthesizing%20large-scale%20synthetic%202D%20video%20datasets%2C%20where%0Aeach%20video%20contains%20consistent%20changes%20in%20the%20facial%20expression%20and%20viewpoint%2C%0Acombined%20with%20a%20variation%20in%20a%20specific%20facial%20attribute%20from%20the%20original%0Ainput.%20We%20propose%20a%20novel%20pipeline%20to%20produce%20high-quality%2C%20photorealistic%202D%0Avideos%20with%20facial%20attribute%20editing.%20Leveraging%20this%20synthetic%20attribute%0Adataset%2C%20we%20present%20a%20personalized%20avatar%20creation%20method%20based%20on%20the%203D%0AGaussian%20Splatting%2C%20learning%20a%20continuous%20and%20disentangled%20latent%20space%20for%0Aintuitive%20facial%20attribute%20manipulation.%20To%20enforce%20smooth%20transitions%20in%20this%0Alatent%20space%2C%20we%20introduce%20a%20latent%20space%20regularization%20technique%20by%20using%0Ainterpolated%202D%20faces%20as%20supervision.%20Compared%20to%20previous%20approaches%2C%20we%0Ademonstrate%20that%20PERSE%20generates%20high-quality%20avatars%20with%20interpolated%0Aattributes%20while%20preserving%20identity%20of%20reference%20person.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21206v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPERSE%253A%2520Personalized%25203D%2520Generative%2520Avatars%2520from%2520A%2520Single%2520Portrait%26entry.906535625%3DHyunsoo%2520Cha%2520and%2520Inhee%2520Lee%2520and%2520Hanbyul%2520Joo%26entry.1292438233%3D%2520%2520We%2520present%2520PERSE%252C%2520a%2520method%2520for%2520building%2520an%2520animatable%2520personalized%2520generative%250Aavatar%2520from%2520a%2520reference%2520portrait.%2520Our%2520avatar%2520model%2520enables%2520facial%2520attribute%250Aediting%2520in%2520a%2520continuous%2520and%2520disentangled%2520latent%2520space%2520to%2520control%2520each%2520facial%250Aattribute%252C%2520while%2520preserving%2520the%2520individual%2527s%2520identity.%2520To%2520achieve%2520this%252C%2520our%250Amethod%2520begins%2520by%2520synthesizing%2520large-scale%2520synthetic%25202D%2520video%2520datasets%252C%2520where%250Aeach%2520video%2520contains%2520consistent%2520changes%2520in%2520the%2520facial%2520expression%2520and%2520viewpoint%252C%250Acombined%2520with%2520a%2520variation%2520in%2520a%2520specific%2520facial%2520attribute%2520from%2520the%2520original%250Ainput.%2520We%2520propose%2520a%2520novel%2520pipeline%2520to%2520produce%2520high-quality%252C%2520photorealistic%25202D%250Avideos%2520with%2520facial%2520attribute%2520editing.%2520Leveraging%2520this%2520synthetic%2520attribute%250Adataset%252C%2520we%2520present%2520a%2520personalized%2520avatar%2520creation%2520method%2520based%2520on%2520the%25203D%250AGaussian%2520Splatting%252C%2520learning%2520a%2520continuous%2520and%2520disentangled%2520latent%2520space%2520for%250Aintuitive%2520facial%2520attribute%2520manipulation.%2520To%2520enforce%2520smooth%2520transitions%2520in%2520this%250Alatent%2520space%252C%2520we%2520introduce%2520a%2520latent%2520space%2520regularization%2520technique%2520by%2520using%250Ainterpolated%25202D%2520faces%2520as%2520supervision.%2520Compared%2520to%2520previous%2520approaches%252C%2520we%250Ademonstrate%2520that%2520PERSE%2520generates%2520high-quality%2520avatars%2520with%2520interpolated%250Aattributes%2520while%2520preserving%2520identity%2520of%2520reference%2520person.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21206v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PERSE%3A%20Personalized%203D%20Generative%20Avatars%20from%20A%20Single%20Portrait&entry.906535625=Hyunsoo%20Cha%20and%20Inhee%20Lee%20and%20Hanbyul%20Joo&entry.1292438233=%20%20We%20present%20PERSE%2C%20a%20method%20for%20building%20an%20animatable%20personalized%20generative%0Aavatar%20from%20a%20reference%20portrait.%20Our%20avatar%20model%20enables%20facial%20attribute%0Aediting%20in%20a%20continuous%20and%20disentangled%20latent%20space%20to%20control%20each%20facial%0Aattribute%2C%20while%20preserving%20the%20individual%27s%20identity.%20To%20achieve%20this%2C%20our%0Amethod%20begins%20by%20synthesizing%20large-scale%20synthetic%202D%20video%20datasets%2C%20where%0Aeach%20video%20contains%20consistent%20changes%20in%20the%20facial%20expression%20and%20viewpoint%2C%0Acombined%20with%20a%20variation%20in%20a%20specific%20facial%20attribute%20from%20the%20original%0Ainput.%20We%20propose%20a%20novel%20pipeline%20to%20produce%20high-quality%2C%20photorealistic%202D%0Avideos%20with%20facial%20attribute%20editing.%20Leveraging%20this%20synthetic%20attribute%0Adataset%2C%20we%20present%20a%20personalized%20avatar%20creation%20method%20based%20on%20the%203D%0AGaussian%20Splatting%2C%20learning%20a%20continuous%20and%20disentangled%20latent%20space%20for%0Aintuitive%20facial%20attribute%20manipulation.%20To%20enforce%20smooth%20transitions%20in%20this%0Alatent%20space%2C%20we%20introduce%20a%20latent%20space%20regularization%20technique%20by%20using%0Ainterpolated%202D%20faces%20as%20supervision.%20Compared%20to%20previous%20approaches%2C%20we%0Ademonstrate%20that%20PERSE%20generates%20high-quality%20avatars%20with%20interpolated%0Aattributes%20while%20preserving%20identity%20of%20reference%20person.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21206v1&entry.124074799=Read"},
{"title": "Enhanced Multimodal RAG-LLM for Accurate Visual Question Answering", "author": "Junxiao Xue and Quan Deng and Fei Yu and Yanhao Wang and Jun Wang and Yuehua Li", "abstract": "  Multimodal large language models (MLLMs), such as GPT-4o, Gemini, LLaVA, and\nFlamingo, have made significant progress in integrating visual and textual\nmodalities, excelling in tasks like visual question answering (VQA), image\ncaptioning, and content retrieval. They can generate coherent and contextually\nrelevant descriptions of images. However, they still face challenges in\naccurately identifying and counting objects and determining their spatial\nlocations, particularly in complex scenes with overlapping or small objects. To\naddress these limitations, we propose a novel framework based on multimodal\nretrieval-augmented generation (RAG), which introduces structured scene graphs\nto enhance object recognition, relationship identification, and spatial\nunderstanding within images. Our framework improves the MLLM's capacity to\nhandle tasks requiring precise visual descriptions, especially in scenarios\nwith challenging perspectives, such as aerial views or scenes with dense object\narrangements. Finally, we conduct extensive experiments on the VG-150 dataset\nthat focuses on first-person visual understanding and the AUG dataset that\ninvolves aerial imagery. The results show that our approach consistently\noutperforms existing MLLMs in VQA tasks, which stands out in recognizing,\nlocalizing, and quantifying objects in different spatial contexts and provides\nmore accurate visual descriptions.\n", "link": "http://arxiv.org/abs/2412.20927v1", "date": "2024-12-30", "relevancy": 2.9517, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5977}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5866}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhanced%20Multimodal%20RAG-LLM%20for%20Accurate%20Visual%20Question%20Answering&body=Title%3A%20Enhanced%20Multimodal%20RAG-LLM%20for%20Accurate%20Visual%20Question%20Answering%0AAuthor%3A%20Junxiao%20Xue%20and%20Quan%20Deng%20and%20Fei%20Yu%20and%20Yanhao%20Wang%20and%20Jun%20Wang%20and%20Yuehua%20Li%0AAbstract%3A%20%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%2C%20such%20as%20GPT-4o%2C%20Gemini%2C%20LLaVA%2C%20and%0AFlamingo%2C%20have%20made%20significant%20progress%20in%20integrating%20visual%20and%20textual%0Amodalities%2C%20excelling%20in%20tasks%20like%20visual%20question%20answering%20%28VQA%29%2C%20image%0Acaptioning%2C%20and%20content%20retrieval.%20They%20can%20generate%20coherent%20and%20contextually%0Arelevant%20descriptions%20of%20images.%20However%2C%20they%20still%20face%20challenges%20in%0Aaccurately%20identifying%20and%20counting%20objects%20and%20determining%20their%20spatial%0Alocations%2C%20particularly%20in%20complex%20scenes%20with%20overlapping%20or%20small%20objects.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20a%20novel%20framework%20based%20on%20multimodal%0Aretrieval-augmented%20generation%20%28RAG%29%2C%20which%20introduces%20structured%20scene%20graphs%0Ato%20enhance%20object%20recognition%2C%20relationship%20identification%2C%20and%20spatial%0Aunderstanding%20within%20images.%20Our%20framework%20improves%20the%20MLLM%27s%20capacity%20to%0Ahandle%20tasks%20requiring%20precise%20visual%20descriptions%2C%20especially%20in%20scenarios%0Awith%20challenging%20perspectives%2C%20such%20as%20aerial%20views%20or%20scenes%20with%20dense%20object%0Aarrangements.%20Finally%2C%20we%20conduct%20extensive%20experiments%20on%20the%20VG-150%20dataset%0Athat%20focuses%20on%20first-person%20visual%20understanding%20and%20the%20AUG%20dataset%20that%0Ainvolves%20aerial%20imagery.%20The%20results%20show%20that%20our%20approach%20consistently%0Aoutperforms%20existing%20MLLMs%20in%20VQA%20tasks%2C%20which%20stands%20out%20in%20recognizing%2C%0Alocalizing%2C%20and%20quantifying%20objects%20in%20different%20spatial%20contexts%20and%20provides%0Amore%20accurate%20visual%20descriptions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20927v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhanced%2520Multimodal%2520RAG-LLM%2520for%2520Accurate%2520Visual%2520Question%2520Answering%26entry.906535625%3DJunxiao%2520Xue%2520and%2520Quan%2520Deng%2520and%2520Fei%2520Yu%2520and%2520Yanhao%2520Wang%2520and%2520Jun%2520Wang%2520and%2520Yuehua%2520Li%26entry.1292438233%3D%2520%2520Multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%252C%2520such%2520as%2520GPT-4o%252C%2520Gemini%252C%2520LLaVA%252C%2520and%250AFlamingo%252C%2520have%2520made%2520significant%2520progress%2520in%2520integrating%2520visual%2520and%2520textual%250Amodalities%252C%2520excelling%2520in%2520tasks%2520like%2520visual%2520question%2520answering%2520%2528VQA%2529%252C%2520image%250Acaptioning%252C%2520and%2520content%2520retrieval.%2520They%2520can%2520generate%2520coherent%2520and%2520contextually%250Arelevant%2520descriptions%2520of%2520images.%2520However%252C%2520they%2520still%2520face%2520challenges%2520in%250Aaccurately%2520identifying%2520and%2520counting%2520objects%2520and%2520determining%2520their%2520spatial%250Alocations%252C%2520particularly%2520in%2520complex%2520scenes%2520with%2520overlapping%2520or%2520small%2520objects.%2520To%250Aaddress%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%2520framework%2520based%2520on%2520multimodal%250Aretrieval-augmented%2520generation%2520%2528RAG%2529%252C%2520which%2520introduces%2520structured%2520scene%2520graphs%250Ato%2520enhance%2520object%2520recognition%252C%2520relationship%2520identification%252C%2520and%2520spatial%250Aunderstanding%2520within%2520images.%2520Our%2520framework%2520improves%2520the%2520MLLM%2527s%2520capacity%2520to%250Ahandle%2520tasks%2520requiring%2520precise%2520visual%2520descriptions%252C%2520especially%2520in%2520scenarios%250Awith%2520challenging%2520perspectives%252C%2520such%2520as%2520aerial%2520views%2520or%2520scenes%2520with%2520dense%2520object%250Aarrangements.%2520Finally%252C%2520we%2520conduct%2520extensive%2520experiments%2520on%2520the%2520VG-150%2520dataset%250Athat%2520focuses%2520on%2520first-person%2520visual%2520understanding%2520and%2520the%2520AUG%2520dataset%2520that%250Ainvolves%2520aerial%2520imagery.%2520The%2520results%2520show%2520that%2520our%2520approach%2520consistently%250Aoutperforms%2520existing%2520MLLMs%2520in%2520VQA%2520tasks%252C%2520which%2520stands%2520out%2520in%2520recognizing%252C%250Alocalizing%252C%2520and%2520quantifying%2520objects%2520in%2520different%2520spatial%2520contexts%2520and%2520provides%250Amore%2520accurate%2520visual%2520descriptions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20927v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhanced%20Multimodal%20RAG-LLM%20for%20Accurate%20Visual%20Question%20Answering&entry.906535625=Junxiao%20Xue%20and%20Quan%20Deng%20and%20Fei%20Yu%20and%20Yanhao%20Wang%20and%20Jun%20Wang%20and%20Yuehua%20Li&entry.1292438233=%20%20Multimodal%20large%20language%20models%20%28MLLMs%29%2C%20such%20as%20GPT-4o%2C%20Gemini%2C%20LLaVA%2C%20and%0AFlamingo%2C%20have%20made%20significant%20progress%20in%20integrating%20visual%20and%20textual%0Amodalities%2C%20excelling%20in%20tasks%20like%20visual%20question%20answering%20%28VQA%29%2C%20image%0Acaptioning%2C%20and%20content%20retrieval.%20They%20can%20generate%20coherent%20and%20contextually%0Arelevant%20descriptions%20of%20images.%20However%2C%20they%20still%20face%20challenges%20in%0Aaccurately%20identifying%20and%20counting%20objects%20and%20determining%20their%20spatial%0Alocations%2C%20particularly%20in%20complex%20scenes%20with%20overlapping%20or%20small%20objects.%20To%0Aaddress%20these%20limitations%2C%20we%20propose%20a%20novel%20framework%20based%20on%20multimodal%0Aretrieval-augmented%20generation%20%28RAG%29%2C%20which%20introduces%20structured%20scene%20graphs%0Ato%20enhance%20object%20recognition%2C%20relationship%20identification%2C%20and%20spatial%0Aunderstanding%20within%20images.%20Our%20framework%20improves%20the%20MLLM%27s%20capacity%20to%0Ahandle%20tasks%20requiring%20precise%20visual%20descriptions%2C%20especially%20in%20scenarios%0Awith%20challenging%20perspectives%2C%20such%20as%20aerial%20views%20or%20scenes%20with%20dense%20object%0Aarrangements.%20Finally%2C%20we%20conduct%20extensive%20experiments%20on%20the%20VG-150%20dataset%0Athat%20focuses%20on%20first-person%20visual%20understanding%20and%20the%20AUG%20dataset%20that%0Ainvolves%20aerial%20imagery.%20The%20results%20show%20that%20our%20approach%20consistently%0Aoutperforms%20existing%20MLLMs%20in%20VQA%20tasks%2C%20which%20stands%20out%20in%20recognizing%2C%0Alocalizing%2C%20and%20quantifying%20objects%20in%20different%20spatial%20contexts%20and%20provides%0Amore%20accurate%20visual%20descriptions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20927v1&entry.124074799=Read"},
{"title": "HV-BEV: Decoupling Horizontal and Vertical Feature Sampling for\n  Multi-View 3D Object Detection", "author": "Di Wu and Feng Yang and Benlian Xu and Pan Liao and Wenhui Zhao and Dingwen Zhang", "abstract": "  The application of vision-based multi-view environmental perception system\nhas been increasingly recognized in autonomous driving technology, especially\nthe BEV-based models. Current state-of-the-art solutions primarily encode image\nfeatures from each camera view into the BEV space through explicit or implicit\ndepth prediction. However, these methods often focus on improving the accuracy\nof projecting 2D features into corresponding depth regions, while overlooking\nthe highly structured information of real-world objects and the varying height\ndistributions of objects across different scenes. In this work, we propose\nHV-BEV, a novel approach that decouples feature sampling in the BEV grid\nqueries paradigm into horizontal feature aggregation and vertical adaptive\nheight-aware reference point sampling, aiming to improve both the aggregation\nof objects' complete information and generalization to diverse road\nenvironments. Specifically, we construct a learnable graph structure in the\nhorizontal plane aligned with the ground for 3D reference points, reinforcing\nthe association of the same instance across different BEV grids, especially\nwhen the instance spans multiple image views around the vehicle. Additionally,\ninstead of relying on uniform sampling within a fixed height range, we\nintroduce a height-aware module that incorporates historical information,\nenabling the reference points to adaptively focus on the varying heights at\nwhich objects appear in different scenes. Extensive experiments validate the\neffectiveness of our proposed method, demonstrating its superior performance\nover the baseline across the nuScenes dataset. Moreover, our best-performing\nmodel achieves a remarkable 50.5% mAP and 59.8% NDS on the nuScenes testing\nset.\n", "link": "http://arxiv.org/abs/2412.18884v2", "date": "2024-12-30", "relevancy": 2.9383, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5997}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5997}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5635}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HV-BEV%3A%20Decoupling%20Horizontal%20and%20Vertical%20Feature%20Sampling%20for%0A%20%20Multi-View%203D%20Object%20Detection&body=Title%3A%20HV-BEV%3A%20Decoupling%20Horizontal%20and%20Vertical%20Feature%20Sampling%20for%0A%20%20Multi-View%203D%20Object%20Detection%0AAuthor%3A%20Di%20Wu%20and%20Feng%20Yang%20and%20Benlian%20Xu%20and%20Pan%20Liao%20and%20Wenhui%20Zhao%20and%20Dingwen%20Zhang%0AAbstract%3A%20%20%20The%20application%20of%20vision-based%20multi-view%20environmental%20perception%20system%0Ahas%20been%20increasingly%20recognized%20in%20autonomous%20driving%20technology%2C%20especially%0Athe%20BEV-based%20models.%20Current%20state-of-the-art%20solutions%20primarily%20encode%20image%0Afeatures%20from%20each%20camera%20view%20into%20the%20BEV%20space%20through%20explicit%20or%20implicit%0Adepth%20prediction.%20However%2C%20these%20methods%20often%20focus%20on%20improving%20the%20accuracy%0Aof%20projecting%202D%20features%20into%20corresponding%20depth%20regions%2C%20while%20overlooking%0Athe%20highly%20structured%20information%20of%20real-world%20objects%20and%20the%20varying%20height%0Adistributions%20of%20objects%20across%20different%20scenes.%20In%20this%20work%2C%20we%20propose%0AHV-BEV%2C%20a%20novel%20approach%20that%20decouples%20feature%20sampling%20in%20the%20BEV%20grid%0Aqueries%20paradigm%20into%20horizontal%20feature%20aggregation%20and%20vertical%20adaptive%0Aheight-aware%20reference%20point%20sampling%2C%20aiming%20to%20improve%20both%20the%20aggregation%0Aof%20objects%27%20complete%20information%20and%20generalization%20to%20diverse%20road%0Aenvironments.%20Specifically%2C%20we%20construct%20a%20learnable%20graph%20structure%20in%20the%0Ahorizontal%20plane%20aligned%20with%20the%20ground%20for%203D%20reference%20points%2C%20reinforcing%0Athe%20association%20of%20the%20same%20instance%20across%20different%20BEV%20grids%2C%20especially%0Awhen%20the%20instance%20spans%20multiple%20image%20views%20around%20the%20vehicle.%20Additionally%2C%0Ainstead%20of%20relying%20on%20uniform%20sampling%20within%20a%20fixed%20height%20range%2C%20we%0Aintroduce%20a%20height-aware%20module%20that%20incorporates%20historical%20information%2C%0Aenabling%20the%20reference%20points%20to%20adaptively%20focus%20on%20the%20varying%20heights%20at%0Awhich%20objects%20appear%20in%20different%20scenes.%20Extensive%20experiments%20validate%20the%0Aeffectiveness%20of%20our%20proposed%20method%2C%20demonstrating%20its%20superior%20performance%0Aover%20the%20baseline%20across%20the%20nuScenes%20dataset.%20Moreover%2C%20our%20best-performing%0Amodel%20achieves%20a%20remarkable%2050.5%25%20mAP%20and%2059.8%25%20NDS%20on%20the%20nuScenes%20testing%0Aset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18884v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHV-BEV%253A%2520Decoupling%2520Horizontal%2520and%2520Vertical%2520Feature%2520Sampling%2520for%250A%2520%2520Multi-View%25203D%2520Object%2520Detection%26entry.906535625%3DDi%2520Wu%2520and%2520Feng%2520Yang%2520and%2520Benlian%2520Xu%2520and%2520Pan%2520Liao%2520and%2520Wenhui%2520Zhao%2520and%2520Dingwen%2520Zhang%26entry.1292438233%3D%2520%2520The%2520application%2520of%2520vision-based%2520multi-view%2520environmental%2520perception%2520system%250Ahas%2520been%2520increasingly%2520recognized%2520in%2520autonomous%2520driving%2520technology%252C%2520especially%250Athe%2520BEV-based%2520models.%2520Current%2520state-of-the-art%2520solutions%2520primarily%2520encode%2520image%250Afeatures%2520from%2520each%2520camera%2520view%2520into%2520the%2520BEV%2520space%2520through%2520explicit%2520or%2520implicit%250Adepth%2520prediction.%2520However%252C%2520these%2520methods%2520often%2520focus%2520on%2520improving%2520the%2520accuracy%250Aof%2520projecting%25202D%2520features%2520into%2520corresponding%2520depth%2520regions%252C%2520while%2520overlooking%250Athe%2520highly%2520structured%2520information%2520of%2520real-world%2520objects%2520and%2520the%2520varying%2520height%250Adistributions%2520of%2520objects%2520across%2520different%2520scenes.%2520In%2520this%2520work%252C%2520we%2520propose%250AHV-BEV%252C%2520a%2520novel%2520approach%2520that%2520decouples%2520feature%2520sampling%2520in%2520the%2520BEV%2520grid%250Aqueries%2520paradigm%2520into%2520horizontal%2520feature%2520aggregation%2520and%2520vertical%2520adaptive%250Aheight-aware%2520reference%2520point%2520sampling%252C%2520aiming%2520to%2520improve%2520both%2520the%2520aggregation%250Aof%2520objects%2527%2520complete%2520information%2520and%2520generalization%2520to%2520diverse%2520road%250Aenvironments.%2520Specifically%252C%2520we%2520construct%2520a%2520learnable%2520graph%2520structure%2520in%2520the%250Ahorizontal%2520plane%2520aligned%2520with%2520the%2520ground%2520for%25203D%2520reference%2520points%252C%2520reinforcing%250Athe%2520association%2520of%2520the%2520same%2520instance%2520across%2520different%2520BEV%2520grids%252C%2520especially%250Awhen%2520the%2520instance%2520spans%2520multiple%2520image%2520views%2520around%2520the%2520vehicle.%2520Additionally%252C%250Ainstead%2520of%2520relying%2520on%2520uniform%2520sampling%2520within%2520a%2520fixed%2520height%2520range%252C%2520we%250Aintroduce%2520a%2520height-aware%2520module%2520that%2520incorporates%2520historical%2520information%252C%250Aenabling%2520the%2520reference%2520points%2520to%2520adaptively%2520focus%2520on%2520the%2520varying%2520heights%2520at%250Awhich%2520objects%2520appear%2520in%2520different%2520scenes.%2520Extensive%2520experiments%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520proposed%2520method%252C%2520demonstrating%2520its%2520superior%2520performance%250Aover%2520the%2520baseline%2520across%2520the%2520nuScenes%2520dataset.%2520Moreover%252C%2520our%2520best-performing%250Amodel%2520achieves%2520a%2520remarkable%252050.5%2525%2520mAP%2520and%252059.8%2525%2520NDS%2520on%2520the%2520nuScenes%2520testing%250Aset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18884v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HV-BEV%3A%20Decoupling%20Horizontal%20and%20Vertical%20Feature%20Sampling%20for%0A%20%20Multi-View%203D%20Object%20Detection&entry.906535625=Di%20Wu%20and%20Feng%20Yang%20and%20Benlian%20Xu%20and%20Pan%20Liao%20and%20Wenhui%20Zhao%20and%20Dingwen%20Zhang&entry.1292438233=%20%20The%20application%20of%20vision-based%20multi-view%20environmental%20perception%20system%0Ahas%20been%20increasingly%20recognized%20in%20autonomous%20driving%20technology%2C%20especially%0Athe%20BEV-based%20models.%20Current%20state-of-the-art%20solutions%20primarily%20encode%20image%0Afeatures%20from%20each%20camera%20view%20into%20the%20BEV%20space%20through%20explicit%20or%20implicit%0Adepth%20prediction.%20However%2C%20these%20methods%20often%20focus%20on%20improving%20the%20accuracy%0Aof%20projecting%202D%20features%20into%20corresponding%20depth%20regions%2C%20while%20overlooking%0Athe%20highly%20structured%20information%20of%20real-world%20objects%20and%20the%20varying%20height%0Adistributions%20of%20objects%20across%20different%20scenes.%20In%20this%20work%2C%20we%20propose%0AHV-BEV%2C%20a%20novel%20approach%20that%20decouples%20feature%20sampling%20in%20the%20BEV%20grid%0Aqueries%20paradigm%20into%20horizontal%20feature%20aggregation%20and%20vertical%20adaptive%0Aheight-aware%20reference%20point%20sampling%2C%20aiming%20to%20improve%20both%20the%20aggregation%0Aof%20objects%27%20complete%20information%20and%20generalization%20to%20diverse%20road%0Aenvironments.%20Specifically%2C%20we%20construct%20a%20learnable%20graph%20structure%20in%20the%0Ahorizontal%20plane%20aligned%20with%20the%20ground%20for%203D%20reference%20points%2C%20reinforcing%0Athe%20association%20of%20the%20same%20instance%20across%20different%20BEV%20grids%2C%20especially%0Awhen%20the%20instance%20spans%20multiple%20image%20views%20around%20the%20vehicle.%20Additionally%2C%0Ainstead%20of%20relying%20on%20uniform%20sampling%20within%20a%20fixed%20height%20range%2C%20we%0Aintroduce%20a%20height-aware%20module%20that%20incorporates%20historical%20information%2C%0Aenabling%20the%20reference%20points%20to%20adaptively%20focus%20on%20the%20varying%20heights%20at%0Awhich%20objects%20appear%20in%20different%20scenes.%20Extensive%20experiments%20validate%20the%0Aeffectiveness%20of%20our%20proposed%20method%2C%20demonstrating%20its%20superior%20performance%0Aover%20the%20baseline%20across%20the%20nuScenes%20dataset.%20Moreover%2C%20our%20best-performing%0Amodel%20achieves%20a%20remarkable%2050.5%25%20mAP%20and%2059.8%25%20NDS%20on%20the%20nuScenes%20testing%0Aset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18884v2&entry.124074799=Read"},
{"title": "NeRF-DetS: Enhanced Adaptive Spatial-wise Sampling and View-wise Fusion\n  Strategies for NeRF-based Indoor Multi-view 3D Object Detection", "author": "Chi Huang and Xinyang Li and Yansong Qu and Changli Wu and Xiaofan Li and Shengchuan Zhang and Liujuan Cao", "abstract": "  In indoor scenes, the diverse distribution of object locations and scales\nmakes the visual 3D perception task a big challenge.\n  Previous works (e.g, NeRF-Det) have demonstrated that implicit representation\nhas the capacity to benefit the visual 3D perception task in indoor scenes with\nhigh amount of overlap between input images.\n  However, previous works cannot fully utilize the advancement of implicit\nrepresentation because of fixed sampling and simple multi-view feature fusion.\n  In this paper, inspired by sparse fashion method (e.g, DETR3D), we propose a\nsimple yet effective method, NeRF-DetS, to address above issues. NeRF-DetS\nincludes two modules: Progressive Adaptive Sampling Strategy (PASS) and\nDepth-Guided Simplified Multi-Head Attention Fusion (DS-MHA).\n  Specifically,\n  (1)PASS can automatically sample features of each layer within a dense 3D\ndetector, using offsets predicted by the previous layer.\n  (2)DS-MHA can not only efficiently fuse multi-view features with strong\nocclusion awareness but also reduce computational cost.\n  Extensive experiments on ScanNetV2 dataset demonstrate our NeRF-DetS\noutperforms NeRF-Det, by achieving +5.02% and +5.92% improvement in mAP under\nIoU25 and IoU50, respectively. Also, NeRF-DetS shows consistent improvements on\nARKITScenes.\n", "link": "http://arxiv.org/abs/2404.13921v2", "date": "2024-12-30", "relevancy": 2.9193, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5864}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5864}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeRF-DetS%3A%20Enhanced%20Adaptive%20Spatial-wise%20Sampling%20and%20View-wise%20Fusion%0A%20%20Strategies%20for%20NeRF-based%20Indoor%20Multi-view%203D%20Object%20Detection&body=Title%3A%20NeRF-DetS%3A%20Enhanced%20Adaptive%20Spatial-wise%20Sampling%20and%20View-wise%20Fusion%0A%20%20Strategies%20for%20NeRF-based%20Indoor%20Multi-view%203D%20Object%20Detection%0AAuthor%3A%20Chi%20Huang%20and%20Xinyang%20Li%20and%20Yansong%20Qu%20and%20Changli%20Wu%20and%20Xiaofan%20Li%20and%20Shengchuan%20Zhang%20and%20Liujuan%20Cao%0AAbstract%3A%20%20%20In%20indoor%20scenes%2C%20the%20diverse%20distribution%20of%20object%20locations%20and%20scales%0Amakes%20the%20visual%203D%20perception%20task%20a%20big%20challenge.%0A%20%20Previous%20works%20%28e.g%2C%20NeRF-Det%29%20have%20demonstrated%20that%20implicit%20representation%0Ahas%20the%20capacity%20to%20benefit%20the%20visual%203D%20perception%20task%20in%20indoor%20scenes%20with%0Ahigh%20amount%20of%20overlap%20between%20input%20images.%0A%20%20However%2C%20previous%20works%20cannot%20fully%20utilize%20the%20advancement%20of%20implicit%0Arepresentation%20because%20of%20fixed%20sampling%20and%20simple%20multi-view%20feature%20fusion.%0A%20%20In%20this%20paper%2C%20inspired%20by%20sparse%20fashion%20method%20%28e.g%2C%20DETR3D%29%2C%20we%20propose%20a%0Asimple%20yet%20effective%20method%2C%20NeRF-DetS%2C%20to%20address%20above%20issues.%20NeRF-DetS%0Aincludes%20two%20modules%3A%20Progressive%20Adaptive%20Sampling%20Strategy%20%28PASS%29%20and%0ADepth-Guided%20Simplified%20Multi-Head%20Attention%20Fusion%20%28DS-MHA%29.%0A%20%20Specifically%2C%0A%20%20%281%29PASS%20can%20automatically%20sample%20features%20of%20each%20layer%20within%20a%20dense%203D%0Adetector%2C%20using%20offsets%20predicted%20by%20the%20previous%20layer.%0A%20%20%282%29DS-MHA%20can%20not%20only%20efficiently%20fuse%20multi-view%20features%20with%20strong%0Aocclusion%20awareness%20but%20also%20reduce%20computational%20cost.%0A%20%20Extensive%20experiments%20on%20ScanNetV2%20dataset%20demonstrate%20our%20NeRF-DetS%0Aoutperforms%20NeRF-Det%2C%20by%20achieving%20%2B5.02%25%20and%20%2B5.92%25%20improvement%20in%20mAP%20under%0AIoU25%20and%20IoU50%2C%20respectively.%20Also%2C%20NeRF-DetS%20shows%20consistent%20improvements%20on%0AARKITScenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13921v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeRF-DetS%253A%2520Enhanced%2520Adaptive%2520Spatial-wise%2520Sampling%2520and%2520View-wise%2520Fusion%250A%2520%2520Strategies%2520for%2520NeRF-based%2520Indoor%2520Multi-view%25203D%2520Object%2520Detection%26entry.906535625%3DChi%2520Huang%2520and%2520Xinyang%2520Li%2520and%2520Yansong%2520Qu%2520and%2520Changli%2520Wu%2520and%2520Xiaofan%2520Li%2520and%2520Shengchuan%2520Zhang%2520and%2520Liujuan%2520Cao%26entry.1292438233%3D%2520%2520In%2520indoor%2520scenes%252C%2520the%2520diverse%2520distribution%2520of%2520object%2520locations%2520and%2520scales%250Amakes%2520the%2520visual%25203D%2520perception%2520task%2520a%2520big%2520challenge.%250A%2520%2520Previous%2520works%2520%2528e.g%252C%2520NeRF-Det%2529%2520have%2520demonstrated%2520that%2520implicit%2520representation%250Ahas%2520the%2520capacity%2520to%2520benefit%2520the%2520visual%25203D%2520perception%2520task%2520in%2520indoor%2520scenes%2520with%250Ahigh%2520amount%2520of%2520overlap%2520between%2520input%2520images.%250A%2520%2520However%252C%2520previous%2520works%2520cannot%2520fully%2520utilize%2520the%2520advancement%2520of%2520implicit%250Arepresentation%2520because%2520of%2520fixed%2520sampling%2520and%2520simple%2520multi-view%2520feature%2520fusion.%250A%2520%2520In%2520this%2520paper%252C%2520inspired%2520by%2520sparse%2520fashion%2520method%2520%2528e.g%252C%2520DETR3D%2529%252C%2520we%2520propose%2520a%250Asimple%2520yet%2520effective%2520method%252C%2520NeRF-DetS%252C%2520to%2520address%2520above%2520issues.%2520NeRF-DetS%250Aincludes%2520two%2520modules%253A%2520Progressive%2520Adaptive%2520Sampling%2520Strategy%2520%2528PASS%2529%2520and%250ADepth-Guided%2520Simplified%2520Multi-Head%2520Attention%2520Fusion%2520%2528DS-MHA%2529.%250A%2520%2520Specifically%252C%250A%2520%2520%25281%2529PASS%2520can%2520automatically%2520sample%2520features%2520of%2520each%2520layer%2520within%2520a%2520dense%25203D%250Adetector%252C%2520using%2520offsets%2520predicted%2520by%2520the%2520previous%2520layer.%250A%2520%2520%25282%2529DS-MHA%2520can%2520not%2520only%2520efficiently%2520fuse%2520multi-view%2520features%2520with%2520strong%250Aocclusion%2520awareness%2520but%2520also%2520reduce%2520computational%2520cost.%250A%2520%2520Extensive%2520experiments%2520on%2520ScanNetV2%2520dataset%2520demonstrate%2520our%2520NeRF-DetS%250Aoutperforms%2520NeRF-Det%252C%2520by%2520achieving%2520%252B5.02%2525%2520and%2520%252B5.92%2525%2520improvement%2520in%2520mAP%2520under%250AIoU25%2520and%2520IoU50%252C%2520respectively.%2520Also%252C%2520NeRF-DetS%2520shows%2520consistent%2520improvements%2520on%250AARKITScenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.13921v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeRF-DetS%3A%20Enhanced%20Adaptive%20Spatial-wise%20Sampling%20and%20View-wise%20Fusion%0A%20%20Strategies%20for%20NeRF-based%20Indoor%20Multi-view%203D%20Object%20Detection&entry.906535625=Chi%20Huang%20and%20Xinyang%20Li%20and%20Yansong%20Qu%20and%20Changli%20Wu%20and%20Xiaofan%20Li%20and%20Shengchuan%20Zhang%20and%20Liujuan%20Cao&entry.1292438233=%20%20In%20indoor%20scenes%2C%20the%20diverse%20distribution%20of%20object%20locations%20and%20scales%0Amakes%20the%20visual%203D%20perception%20task%20a%20big%20challenge.%0A%20%20Previous%20works%20%28e.g%2C%20NeRF-Det%29%20have%20demonstrated%20that%20implicit%20representation%0Ahas%20the%20capacity%20to%20benefit%20the%20visual%203D%20perception%20task%20in%20indoor%20scenes%20with%0Ahigh%20amount%20of%20overlap%20between%20input%20images.%0A%20%20However%2C%20previous%20works%20cannot%20fully%20utilize%20the%20advancement%20of%20implicit%0Arepresentation%20because%20of%20fixed%20sampling%20and%20simple%20multi-view%20feature%20fusion.%0A%20%20In%20this%20paper%2C%20inspired%20by%20sparse%20fashion%20method%20%28e.g%2C%20DETR3D%29%2C%20we%20propose%20a%0Asimple%20yet%20effective%20method%2C%20NeRF-DetS%2C%20to%20address%20above%20issues.%20NeRF-DetS%0Aincludes%20two%20modules%3A%20Progressive%20Adaptive%20Sampling%20Strategy%20%28PASS%29%20and%0ADepth-Guided%20Simplified%20Multi-Head%20Attention%20Fusion%20%28DS-MHA%29.%0A%20%20Specifically%2C%0A%20%20%281%29PASS%20can%20automatically%20sample%20features%20of%20each%20layer%20within%20a%20dense%203D%0Adetector%2C%20using%20offsets%20predicted%20by%20the%20previous%20layer.%0A%20%20%282%29DS-MHA%20can%20not%20only%20efficiently%20fuse%20multi-view%20features%20with%20strong%0Aocclusion%20awareness%20but%20also%20reduce%20computational%20cost.%0A%20%20Extensive%20experiments%20on%20ScanNetV2%20dataset%20demonstrate%20our%20NeRF-DetS%0Aoutperforms%20NeRF-Det%2C%20by%20achieving%20%2B5.02%25%20and%20%2B5.92%25%20improvement%20in%20mAP%20under%0AIoU25%20and%20IoU50%2C%20respectively.%20Also%2C%20NeRF-DetS%20shows%20consistent%20improvements%20on%0AARKITScenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13921v2&entry.124074799=Read"},
{"title": "LiDAR-Camera Fusion for Video Panoptic Segmentation without Video\n  Training", "author": "Fardin Ayar and Ehsan Javanmardi and Manabu Tsukada and Mahdi Javanmardi and Mohammad Rahmati", "abstract": "  Panoptic segmentation, which combines instance and semantic segmentation, has\ngained a lot of attention in autonomous vehicles, due to its comprehensive\nrepresentation of the scene. This task can be applied for cameras and LiDAR\nsensors, but there has been a limited focus on combining both sensors to\nenhance image panoptic segmentation (PS). Although previous research has\nacknowledged the benefit of 3D data on camera-based scene perception, no\nspecific study has explored the influence of 3D data on image and video\npanoptic segmentation (VPS).This work seeks to introduce a feature fusion\nmodule that enhances PS and VPS by fusing LiDAR and image data for autonomous\nvehicles. We also illustrate that, in addition to this fusion, our proposed\nmodel, which utilizes two simple modifications, can further deliver even more\nhigh-quality VPS without being trained on video data. The results demonstrate a\nsubstantial improvement in both the image and video panoptic segmentation\nevaluation metrics by up to 5 points.\n", "link": "http://arxiv.org/abs/2412.20881v1", "date": "2024-12-30", "relevancy": 2.8923, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6094}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.563}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.563}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiDAR-Camera%20Fusion%20for%20Video%20Panoptic%20Segmentation%20without%20Video%0A%20%20Training&body=Title%3A%20LiDAR-Camera%20Fusion%20for%20Video%20Panoptic%20Segmentation%20without%20Video%0A%20%20Training%0AAuthor%3A%20Fardin%20Ayar%20and%20Ehsan%20Javanmardi%20and%20Manabu%20Tsukada%20and%20Mahdi%20Javanmardi%20and%20Mohammad%20Rahmati%0AAbstract%3A%20%20%20Panoptic%20segmentation%2C%20which%20combines%20instance%20and%20semantic%20segmentation%2C%20has%0Agained%20a%20lot%20of%20attention%20in%20autonomous%20vehicles%2C%20due%20to%20its%20comprehensive%0Arepresentation%20of%20the%20scene.%20This%20task%20can%20be%20applied%20for%20cameras%20and%20LiDAR%0Asensors%2C%20but%20there%20has%20been%20a%20limited%20focus%20on%20combining%20both%20sensors%20to%0Aenhance%20image%20panoptic%20segmentation%20%28PS%29.%20Although%20previous%20research%20has%0Aacknowledged%20the%20benefit%20of%203D%20data%20on%20camera-based%20scene%20perception%2C%20no%0Aspecific%20study%20has%20explored%20the%20influence%20of%203D%20data%20on%20image%20and%20video%0Apanoptic%20segmentation%20%28VPS%29.This%20work%20seeks%20to%20introduce%20a%20feature%20fusion%0Amodule%20that%20enhances%20PS%20and%20VPS%20by%20fusing%20LiDAR%20and%20image%20data%20for%20autonomous%0Avehicles.%20We%20also%20illustrate%20that%2C%20in%20addition%20to%20this%20fusion%2C%20our%20proposed%0Amodel%2C%20which%20utilizes%20two%20simple%20modifications%2C%20can%20further%20deliver%20even%20more%0Ahigh-quality%20VPS%20without%20being%20trained%20on%20video%20data.%20The%20results%20demonstrate%20a%0Asubstantial%20improvement%20in%20both%20the%20image%20and%20video%20panoptic%20segmentation%0Aevaluation%20metrics%20by%20up%20to%205%20points.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20881v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiDAR-Camera%2520Fusion%2520for%2520Video%2520Panoptic%2520Segmentation%2520without%2520Video%250A%2520%2520Training%26entry.906535625%3DFardin%2520Ayar%2520and%2520Ehsan%2520Javanmardi%2520and%2520Manabu%2520Tsukada%2520and%2520Mahdi%2520Javanmardi%2520and%2520Mohammad%2520Rahmati%26entry.1292438233%3D%2520%2520Panoptic%2520segmentation%252C%2520which%2520combines%2520instance%2520and%2520semantic%2520segmentation%252C%2520has%250Agained%2520a%2520lot%2520of%2520attention%2520in%2520autonomous%2520vehicles%252C%2520due%2520to%2520its%2520comprehensive%250Arepresentation%2520of%2520the%2520scene.%2520This%2520task%2520can%2520be%2520applied%2520for%2520cameras%2520and%2520LiDAR%250Asensors%252C%2520but%2520there%2520has%2520been%2520a%2520limited%2520focus%2520on%2520combining%2520both%2520sensors%2520to%250Aenhance%2520image%2520panoptic%2520segmentation%2520%2528PS%2529.%2520Although%2520previous%2520research%2520has%250Aacknowledged%2520the%2520benefit%2520of%25203D%2520data%2520on%2520camera-based%2520scene%2520perception%252C%2520no%250Aspecific%2520study%2520has%2520explored%2520the%2520influence%2520of%25203D%2520data%2520on%2520image%2520and%2520video%250Apanoptic%2520segmentation%2520%2528VPS%2529.This%2520work%2520seeks%2520to%2520introduce%2520a%2520feature%2520fusion%250Amodule%2520that%2520enhances%2520PS%2520and%2520VPS%2520by%2520fusing%2520LiDAR%2520and%2520image%2520data%2520for%2520autonomous%250Avehicles.%2520We%2520also%2520illustrate%2520that%252C%2520in%2520addition%2520to%2520this%2520fusion%252C%2520our%2520proposed%250Amodel%252C%2520which%2520utilizes%2520two%2520simple%2520modifications%252C%2520can%2520further%2520deliver%2520even%2520more%250Ahigh-quality%2520VPS%2520without%2520being%2520trained%2520on%2520video%2520data.%2520The%2520results%2520demonstrate%2520a%250Asubstantial%2520improvement%2520in%2520both%2520the%2520image%2520and%2520video%2520panoptic%2520segmentation%250Aevaluation%2520metrics%2520by%2520up%2520to%25205%2520points.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20881v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiDAR-Camera%20Fusion%20for%20Video%20Panoptic%20Segmentation%20without%20Video%0A%20%20Training&entry.906535625=Fardin%20Ayar%20and%20Ehsan%20Javanmardi%20and%20Manabu%20Tsukada%20and%20Mahdi%20Javanmardi%20and%20Mohammad%20Rahmati&entry.1292438233=%20%20Panoptic%20segmentation%2C%20which%20combines%20instance%20and%20semantic%20segmentation%2C%20has%0Agained%20a%20lot%20of%20attention%20in%20autonomous%20vehicles%2C%20due%20to%20its%20comprehensive%0Arepresentation%20of%20the%20scene.%20This%20task%20can%20be%20applied%20for%20cameras%20and%20LiDAR%0Asensors%2C%20but%20there%20has%20been%20a%20limited%20focus%20on%20combining%20both%20sensors%20to%0Aenhance%20image%20panoptic%20segmentation%20%28PS%29.%20Although%20previous%20research%20has%0Aacknowledged%20the%20benefit%20of%203D%20data%20on%20camera-based%20scene%20perception%2C%20no%0Aspecific%20study%20has%20explored%20the%20influence%20of%203D%20data%20on%20image%20and%20video%0Apanoptic%20segmentation%20%28VPS%29.This%20work%20seeks%20to%20introduce%20a%20feature%20fusion%0Amodule%20that%20enhances%20PS%20and%20VPS%20by%20fusing%20LiDAR%20and%20image%20data%20for%20autonomous%0Avehicles.%20We%20also%20illustrate%20that%2C%20in%20addition%20to%20this%20fusion%2C%20our%20proposed%0Amodel%2C%20which%20utilizes%20two%20simple%20modifications%2C%20can%20further%20deliver%20even%20more%0Ahigh-quality%20VPS%20without%20being%20trained%20on%20video%20data.%20The%20results%20demonstrate%20a%0Asubstantial%20improvement%20in%20both%20the%20image%20and%20video%20panoptic%20segmentation%0Aevaluation%20metrics%20by%20up%20to%205%20points.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20881v1&entry.124074799=Read"},
{"title": "LINK: Adaptive Modality Interaction for Audio-Visual Video Parsing", "author": "Langyu Wang and Bingke Zhu and Yingying Chen and Jinqiao Wang", "abstract": "  Audio-visual video parsing focuses on classifying videos through weak labels\nwhile identifying events as either visible, audible, or both, alongside their\nrespective temporal boundaries. Many methods ignore that different modalities\noften lack alignment, thereby introducing extra noise during modal interaction.\nIn this work, we introduce a Learning Interaction method for Non-aligned\nKnowledge (LINK), designed to equilibrate the contributions of distinct\nmodalities by dynamically adjusting their input during event prediction.\nAdditionally, we leverage the semantic information of pseudo-labels as a priori\nknowledge to mitigate noise from other modalities. Our experimental findings\ndemonstrate that our model outperforms existing methods on the LLP dataset.\n", "link": "http://arxiv.org/abs/2412.20872v1", "date": "2024-12-30", "relevancy": 2.834, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5708}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5648}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LINK%3A%20Adaptive%20Modality%20Interaction%20for%20Audio-Visual%20Video%20Parsing&body=Title%3A%20LINK%3A%20Adaptive%20Modality%20Interaction%20for%20Audio-Visual%20Video%20Parsing%0AAuthor%3A%20Langyu%20Wang%20and%20Bingke%20Zhu%20and%20Yingying%20Chen%20and%20Jinqiao%20Wang%0AAbstract%3A%20%20%20Audio-visual%20video%20parsing%20focuses%20on%20classifying%20videos%20through%20weak%20labels%0Awhile%20identifying%20events%20as%20either%20visible%2C%20audible%2C%20or%20both%2C%20alongside%20their%0Arespective%20temporal%20boundaries.%20Many%20methods%20ignore%20that%20different%20modalities%0Aoften%20lack%20alignment%2C%20thereby%20introducing%20extra%20noise%20during%20modal%20interaction.%0AIn%20this%20work%2C%20we%20introduce%20a%20Learning%20Interaction%20method%20for%20Non-aligned%0AKnowledge%20%28LINK%29%2C%20designed%20to%20equilibrate%20the%20contributions%20of%20distinct%0Amodalities%20by%20dynamically%20adjusting%20their%20input%20during%20event%20prediction.%0AAdditionally%2C%20we%20leverage%20the%20semantic%20information%20of%20pseudo-labels%20as%20a%20priori%0Aknowledge%20to%20mitigate%20noise%20from%20other%20modalities.%20Our%20experimental%20findings%0Ademonstrate%20that%20our%20model%20outperforms%20existing%20methods%20on%20the%20LLP%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20872v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLINK%253A%2520Adaptive%2520Modality%2520Interaction%2520for%2520Audio-Visual%2520Video%2520Parsing%26entry.906535625%3DLangyu%2520Wang%2520and%2520Bingke%2520Zhu%2520and%2520Yingying%2520Chen%2520and%2520Jinqiao%2520Wang%26entry.1292438233%3D%2520%2520Audio-visual%2520video%2520parsing%2520focuses%2520on%2520classifying%2520videos%2520through%2520weak%2520labels%250Awhile%2520identifying%2520events%2520as%2520either%2520visible%252C%2520audible%252C%2520or%2520both%252C%2520alongside%2520their%250Arespective%2520temporal%2520boundaries.%2520Many%2520methods%2520ignore%2520that%2520different%2520modalities%250Aoften%2520lack%2520alignment%252C%2520thereby%2520introducing%2520extra%2520noise%2520during%2520modal%2520interaction.%250AIn%2520this%2520work%252C%2520we%2520introduce%2520a%2520Learning%2520Interaction%2520method%2520for%2520Non-aligned%250AKnowledge%2520%2528LINK%2529%252C%2520designed%2520to%2520equilibrate%2520the%2520contributions%2520of%2520distinct%250Amodalities%2520by%2520dynamically%2520adjusting%2520their%2520input%2520during%2520event%2520prediction.%250AAdditionally%252C%2520we%2520leverage%2520the%2520semantic%2520information%2520of%2520pseudo-labels%2520as%2520a%2520priori%250Aknowledge%2520to%2520mitigate%2520noise%2520from%2520other%2520modalities.%2520Our%2520experimental%2520findings%250Ademonstrate%2520that%2520our%2520model%2520outperforms%2520existing%2520methods%2520on%2520the%2520LLP%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20872v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LINK%3A%20Adaptive%20Modality%20Interaction%20for%20Audio-Visual%20Video%20Parsing&entry.906535625=Langyu%20Wang%20and%20Bingke%20Zhu%20and%20Yingying%20Chen%20and%20Jinqiao%20Wang&entry.1292438233=%20%20Audio-visual%20video%20parsing%20focuses%20on%20classifying%20videos%20through%20weak%20labels%0Awhile%20identifying%20events%20as%20either%20visible%2C%20audible%2C%20or%20both%2C%20alongside%20their%0Arespective%20temporal%20boundaries.%20Many%20methods%20ignore%20that%20different%20modalities%0Aoften%20lack%20alignment%2C%20thereby%20introducing%20extra%20noise%20during%20modal%20interaction.%0AIn%20this%20work%2C%20we%20introduce%20a%20Learning%20Interaction%20method%20for%20Non-aligned%0AKnowledge%20%28LINK%29%2C%20designed%20to%20equilibrate%20the%20contributions%20of%20distinct%0Amodalities%20by%20dynamically%20adjusting%20their%20input%20during%20event%20prediction.%0AAdditionally%2C%20we%20leverage%20the%20semantic%20information%20of%20pseudo-labels%20as%20a%20priori%0Aknowledge%20to%20mitigate%20noise%20from%20other%20modalities.%20Our%20experimental%20findings%0Ademonstrate%20that%20our%20model%20outperforms%20existing%20methods%20on%20the%20LLP%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20872v1&entry.124074799=Read"},
{"title": "Towards Identity-Aware Cross-Modal Retrieval: a Dataset and a Baseline", "author": "Nicola Messina and Lucia Vadicamo and Leo Maltese and Claudio Gennaro", "abstract": "  Recent advancements in deep learning have significantly enhanced\ncontent-based retrieval methods, notably through models like CLIP that map\nimages and texts into a shared embedding space. However, these methods often\nstruggle with domain-specific entities and long-tail concepts absent from their\ntraining data, particularly in identifying specific individuals. In this paper,\nwe explore the task of identity-aware cross-modal retrieval, which aims to\nretrieve images of persons in specific contexts based on natural language\nqueries. This task is critical in various scenarios, such as for searching and\nbrowsing personalized video collections or large audio-visual archives\nmaintained by national broadcasters. We introduce a novel dataset, COCO Person\nFaceSwap (COCO-PFS), derived from the widely used COCO dataset and enriched\nwith deepfake-generated faces from VGGFace2. This dataset addresses the lack of\nlarge-scale datasets needed for training and evaluating models for this task.\nOur experiments assess the performance of different CLIP variations repurposed\nfor this task, including our architecture, Identity-aware CLIP (Id-CLIP), which\nachieves competitive retrieval performance through targeted fine-tuning. Our\ncontributions lay the groundwork for more robust cross-modal retrieval systems\ncapable of recognizing long-tail identities and contextual nuances. Data and\ncode are available at https://github.com/mesnico/IdCLIP.\n", "link": "http://arxiv.org/abs/2412.21009v1", "date": "2024-12-30", "relevancy": 2.8108, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6005}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5653}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Identity-Aware%20Cross-Modal%20Retrieval%3A%20a%20Dataset%20and%20a%20Baseline&body=Title%3A%20Towards%20Identity-Aware%20Cross-Modal%20Retrieval%3A%20a%20Dataset%20and%20a%20Baseline%0AAuthor%3A%20Nicola%20Messina%20and%20Lucia%20Vadicamo%20and%20Leo%20Maltese%20and%20Claudio%20Gennaro%0AAbstract%3A%20%20%20Recent%20advancements%20in%20deep%20learning%20have%20significantly%20enhanced%0Acontent-based%20retrieval%20methods%2C%20notably%20through%20models%20like%20CLIP%20that%20map%0Aimages%20and%20texts%20into%20a%20shared%20embedding%20space.%20However%2C%20these%20methods%20often%0Astruggle%20with%20domain-specific%20entities%20and%20long-tail%20concepts%20absent%20from%20their%0Atraining%20data%2C%20particularly%20in%20identifying%20specific%20individuals.%20In%20this%20paper%2C%0Awe%20explore%20the%20task%20of%20identity-aware%20cross-modal%20retrieval%2C%20which%20aims%20to%0Aretrieve%20images%20of%20persons%20in%20specific%20contexts%20based%20on%20natural%20language%0Aqueries.%20This%20task%20is%20critical%20in%20various%20scenarios%2C%20such%20as%20for%20searching%20and%0Abrowsing%20personalized%20video%20collections%20or%20large%20audio-visual%20archives%0Amaintained%20by%20national%20broadcasters.%20We%20introduce%20a%20novel%20dataset%2C%20COCO%20Person%0AFaceSwap%20%28COCO-PFS%29%2C%20derived%20from%20the%20widely%20used%20COCO%20dataset%20and%20enriched%0Awith%20deepfake-generated%20faces%20from%20VGGFace2.%20This%20dataset%20addresses%20the%20lack%20of%0Alarge-scale%20datasets%20needed%20for%20training%20and%20evaluating%20models%20for%20this%20task.%0AOur%20experiments%20assess%20the%20performance%20of%20different%20CLIP%20variations%20repurposed%0Afor%20this%20task%2C%20including%20our%20architecture%2C%20Identity-aware%20CLIP%20%28Id-CLIP%29%2C%20which%0Aachieves%20competitive%20retrieval%20performance%20through%20targeted%20fine-tuning.%20Our%0Acontributions%20lay%20the%20groundwork%20for%20more%20robust%20cross-modal%20retrieval%20systems%0Acapable%20of%20recognizing%20long-tail%20identities%20and%20contextual%20nuances.%20Data%20and%0Acode%20are%20available%20at%20https%3A//github.com/mesnico/IdCLIP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21009v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Identity-Aware%2520Cross-Modal%2520Retrieval%253A%2520a%2520Dataset%2520and%2520a%2520Baseline%26entry.906535625%3DNicola%2520Messina%2520and%2520Lucia%2520Vadicamo%2520and%2520Leo%2520Maltese%2520and%2520Claudio%2520Gennaro%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520deep%2520learning%2520have%2520significantly%2520enhanced%250Acontent-based%2520retrieval%2520methods%252C%2520notably%2520through%2520models%2520like%2520CLIP%2520that%2520map%250Aimages%2520and%2520texts%2520into%2520a%2520shared%2520embedding%2520space.%2520However%252C%2520these%2520methods%2520often%250Astruggle%2520with%2520domain-specific%2520entities%2520and%2520long-tail%2520concepts%2520absent%2520from%2520their%250Atraining%2520data%252C%2520particularly%2520in%2520identifying%2520specific%2520individuals.%2520In%2520this%2520paper%252C%250Awe%2520explore%2520the%2520task%2520of%2520identity-aware%2520cross-modal%2520retrieval%252C%2520which%2520aims%2520to%250Aretrieve%2520images%2520of%2520persons%2520in%2520specific%2520contexts%2520based%2520on%2520natural%2520language%250Aqueries.%2520This%2520task%2520is%2520critical%2520in%2520various%2520scenarios%252C%2520such%2520as%2520for%2520searching%2520and%250Abrowsing%2520personalized%2520video%2520collections%2520or%2520large%2520audio-visual%2520archives%250Amaintained%2520by%2520national%2520broadcasters.%2520We%2520introduce%2520a%2520novel%2520dataset%252C%2520COCO%2520Person%250AFaceSwap%2520%2528COCO-PFS%2529%252C%2520derived%2520from%2520the%2520widely%2520used%2520COCO%2520dataset%2520and%2520enriched%250Awith%2520deepfake-generated%2520faces%2520from%2520VGGFace2.%2520This%2520dataset%2520addresses%2520the%2520lack%2520of%250Alarge-scale%2520datasets%2520needed%2520for%2520training%2520and%2520evaluating%2520models%2520for%2520this%2520task.%250AOur%2520experiments%2520assess%2520the%2520performance%2520of%2520different%2520CLIP%2520variations%2520repurposed%250Afor%2520this%2520task%252C%2520including%2520our%2520architecture%252C%2520Identity-aware%2520CLIP%2520%2528Id-CLIP%2529%252C%2520which%250Aachieves%2520competitive%2520retrieval%2520performance%2520through%2520targeted%2520fine-tuning.%2520Our%250Acontributions%2520lay%2520the%2520groundwork%2520for%2520more%2520robust%2520cross-modal%2520retrieval%2520systems%250Acapable%2520of%2520recognizing%2520long-tail%2520identities%2520and%2520contextual%2520nuances.%2520Data%2520and%250Acode%2520are%2520available%2520at%2520https%253A//github.com/mesnico/IdCLIP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21009v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Identity-Aware%20Cross-Modal%20Retrieval%3A%20a%20Dataset%20and%20a%20Baseline&entry.906535625=Nicola%20Messina%20and%20Lucia%20Vadicamo%20and%20Leo%20Maltese%20and%20Claudio%20Gennaro&entry.1292438233=%20%20Recent%20advancements%20in%20deep%20learning%20have%20significantly%20enhanced%0Acontent-based%20retrieval%20methods%2C%20notably%20through%20models%20like%20CLIP%20that%20map%0Aimages%20and%20texts%20into%20a%20shared%20embedding%20space.%20However%2C%20these%20methods%20often%0Astruggle%20with%20domain-specific%20entities%20and%20long-tail%20concepts%20absent%20from%20their%0Atraining%20data%2C%20particularly%20in%20identifying%20specific%20individuals.%20In%20this%20paper%2C%0Awe%20explore%20the%20task%20of%20identity-aware%20cross-modal%20retrieval%2C%20which%20aims%20to%0Aretrieve%20images%20of%20persons%20in%20specific%20contexts%20based%20on%20natural%20language%0Aqueries.%20This%20task%20is%20critical%20in%20various%20scenarios%2C%20such%20as%20for%20searching%20and%0Abrowsing%20personalized%20video%20collections%20or%20large%20audio-visual%20archives%0Amaintained%20by%20national%20broadcasters.%20We%20introduce%20a%20novel%20dataset%2C%20COCO%20Person%0AFaceSwap%20%28COCO-PFS%29%2C%20derived%20from%20the%20widely%20used%20COCO%20dataset%20and%20enriched%0Awith%20deepfake-generated%20faces%20from%20VGGFace2.%20This%20dataset%20addresses%20the%20lack%20of%0Alarge-scale%20datasets%20needed%20for%20training%20and%20evaluating%20models%20for%20this%20task.%0AOur%20experiments%20assess%20the%20performance%20of%20different%20CLIP%20variations%20repurposed%0Afor%20this%20task%2C%20including%20our%20architecture%2C%20Identity-aware%20CLIP%20%28Id-CLIP%29%2C%20which%0Aachieves%20competitive%20retrieval%20performance%20through%20targeted%20fine-tuning.%20Our%0Acontributions%20lay%20the%20groundwork%20for%20more%20robust%20cross-modal%20retrieval%20systems%0Acapable%20of%20recognizing%20long-tail%20identities%20and%20contextual%20nuances.%20Data%20and%0Acode%20are%20available%20at%20https%3A//github.com/mesnico/IdCLIP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21009v1&entry.124074799=Read"},
{"title": "Vinci: A Real-time Embodied Smart Assistant based on Egocentric\n  Vision-Language Model", "author": "Yifei Huang and Jilan Xu and Baoqi Pei and Yuping He and Guo Chen and Lijin Yang and Xinyuan Chen and Yaohui Wang and Zheng Nie and Jinyao Liu and Guoshun Fan and Dechen Lin and Fang Fang and Kunpeng Li and Chang Yuan and Yali Wang and Yu Qiao and Limin Wang", "abstract": "  We introduce Vinci, a real-time embodied smart assistant built upon an\negocentric vision-language model. Designed for deployment on portable devices\nsuch as smartphones and wearable cameras, Vinci operates in an \"always on\"\nmode, continuously observing the environment to deliver seamless interaction\nand assistance. Users can wake up the system and engage in natural\nconversations to ask questions or seek assistance, with responses delivered\nthrough audio for hands-free convenience. With its ability to process long\nvideo streams in real-time, Vinci can answer user queries about current\nobservations and historical context while also providing task planning based on\npast interactions. To further enhance usability, Vinci integrates a video\ngeneration module that creates step-by-step visual demonstrations for tasks\nthat require detailed guidance. We hope that Vinci can establish a robust\nframework for portable, real-time egocentric AI systems, empowering users with\ncontextual and actionable insights. We release the complete implementation for\nthe development of the device in conjunction with a demo web platform to test\nuploaded videos at https://github.com/OpenGVLab/vinci.\n", "link": "http://arxiv.org/abs/2412.21080v1", "date": "2024-12-30", "relevancy": 2.7423, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5711}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.543}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vinci%3A%20A%20Real-time%20Embodied%20Smart%20Assistant%20based%20on%20Egocentric%0A%20%20Vision-Language%20Model&body=Title%3A%20Vinci%3A%20A%20Real-time%20Embodied%20Smart%20Assistant%20based%20on%20Egocentric%0A%20%20Vision-Language%20Model%0AAuthor%3A%20Yifei%20Huang%20and%20Jilan%20Xu%20and%20Baoqi%20Pei%20and%20Yuping%20He%20and%20Guo%20Chen%20and%20Lijin%20Yang%20and%20Xinyuan%20Chen%20and%20Yaohui%20Wang%20and%20Zheng%20Nie%20and%20Jinyao%20Liu%20and%20Guoshun%20Fan%20and%20Dechen%20Lin%20and%20Fang%20Fang%20and%20Kunpeng%20Li%20and%20Chang%20Yuan%20and%20Yali%20Wang%20and%20Yu%20Qiao%20and%20Limin%20Wang%0AAbstract%3A%20%20%20We%20introduce%20Vinci%2C%20a%20real-time%20embodied%20smart%20assistant%20built%20upon%20an%0Aegocentric%20vision-language%20model.%20Designed%20for%20deployment%20on%20portable%20devices%0Asuch%20as%20smartphones%20and%20wearable%20cameras%2C%20Vinci%20operates%20in%20an%20%22always%20on%22%0Amode%2C%20continuously%20observing%20the%20environment%20to%20deliver%20seamless%20interaction%0Aand%20assistance.%20Users%20can%20wake%20up%20the%20system%20and%20engage%20in%20natural%0Aconversations%20to%20ask%20questions%20or%20seek%20assistance%2C%20with%20responses%20delivered%0Athrough%20audio%20for%20hands-free%20convenience.%20With%20its%20ability%20to%20process%20long%0Avideo%20streams%20in%20real-time%2C%20Vinci%20can%20answer%20user%20queries%20about%20current%0Aobservations%20and%20historical%20context%20while%20also%20providing%20task%20planning%20based%20on%0Apast%20interactions.%20To%20further%20enhance%20usability%2C%20Vinci%20integrates%20a%20video%0Ageneration%20module%20that%20creates%20step-by-step%20visual%20demonstrations%20for%20tasks%0Athat%20require%20detailed%20guidance.%20We%20hope%20that%20Vinci%20can%20establish%20a%20robust%0Aframework%20for%20portable%2C%20real-time%20egocentric%20AI%20systems%2C%20empowering%20users%20with%0Acontextual%20and%20actionable%20insights.%20We%20release%20the%20complete%20implementation%20for%0Athe%20development%20of%20the%20device%20in%20conjunction%20with%20a%20demo%20web%20platform%20to%20test%0Auploaded%20videos%20at%20https%3A//github.com/OpenGVLab/vinci.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21080v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVinci%253A%2520A%2520Real-time%2520Embodied%2520Smart%2520Assistant%2520based%2520on%2520Egocentric%250A%2520%2520Vision-Language%2520Model%26entry.906535625%3DYifei%2520Huang%2520and%2520Jilan%2520Xu%2520and%2520Baoqi%2520Pei%2520and%2520Yuping%2520He%2520and%2520Guo%2520Chen%2520and%2520Lijin%2520Yang%2520and%2520Xinyuan%2520Chen%2520and%2520Yaohui%2520Wang%2520and%2520Zheng%2520Nie%2520and%2520Jinyao%2520Liu%2520and%2520Guoshun%2520Fan%2520and%2520Dechen%2520Lin%2520and%2520Fang%2520Fang%2520and%2520Kunpeng%2520Li%2520and%2520Chang%2520Yuan%2520and%2520Yali%2520Wang%2520and%2520Yu%2520Qiao%2520and%2520Limin%2520Wang%26entry.1292438233%3D%2520%2520We%2520introduce%2520Vinci%252C%2520a%2520real-time%2520embodied%2520smart%2520assistant%2520built%2520upon%2520an%250Aegocentric%2520vision-language%2520model.%2520Designed%2520for%2520deployment%2520on%2520portable%2520devices%250Asuch%2520as%2520smartphones%2520and%2520wearable%2520cameras%252C%2520Vinci%2520operates%2520in%2520an%2520%2522always%2520on%2522%250Amode%252C%2520continuously%2520observing%2520the%2520environment%2520to%2520deliver%2520seamless%2520interaction%250Aand%2520assistance.%2520Users%2520can%2520wake%2520up%2520the%2520system%2520and%2520engage%2520in%2520natural%250Aconversations%2520to%2520ask%2520questions%2520or%2520seek%2520assistance%252C%2520with%2520responses%2520delivered%250Athrough%2520audio%2520for%2520hands-free%2520convenience.%2520With%2520its%2520ability%2520to%2520process%2520long%250Avideo%2520streams%2520in%2520real-time%252C%2520Vinci%2520can%2520answer%2520user%2520queries%2520about%2520current%250Aobservations%2520and%2520historical%2520context%2520while%2520also%2520providing%2520task%2520planning%2520based%2520on%250Apast%2520interactions.%2520To%2520further%2520enhance%2520usability%252C%2520Vinci%2520integrates%2520a%2520video%250Ageneration%2520module%2520that%2520creates%2520step-by-step%2520visual%2520demonstrations%2520for%2520tasks%250Athat%2520require%2520detailed%2520guidance.%2520We%2520hope%2520that%2520Vinci%2520can%2520establish%2520a%2520robust%250Aframework%2520for%2520portable%252C%2520real-time%2520egocentric%2520AI%2520systems%252C%2520empowering%2520users%2520with%250Acontextual%2520and%2520actionable%2520insights.%2520We%2520release%2520the%2520complete%2520implementation%2520for%250Athe%2520development%2520of%2520the%2520device%2520in%2520conjunction%2520with%2520a%2520demo%2520web%2520platform%2520to%2520test%250Auploaded%2520videos%2520at%2520https%253A//github.com/OpenGVLab/vinci.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21080v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vinci%3A%20A%20Real-time%20Embodied%20Smart%20Assistant%20based%20on%20Egocentric%0A%20%20Vision-Language%20Model&entry.906535625=Yifei%20Huang%20and%20Jilan%20Xu%20and%20Baoqi%20Pei%20and%20Yuping%20He%20and%20Guo%20Chen%20and%20Lijin%20Yang%20and%20Xinyuan%20Chen%20and%20Yaohui%20Wang%20and%20Zheng%20Nie%20and%20Jinyao%20Liu%20and%20Guoshun%20Fan%20and%20Dechen%20Lin%20and%20Fang%20Fang%20and%20Kunpeng%20Li%20and%20Chang%20Yuan%20and%20Yali%20Wang%20and%20Yu%20Qiao%20and%20Limin%20Wang&entry.1292438233=%20%20We%20introduce%20Vinci%2C%20a%20real-time%20embodied%20smart%20assistant%20built%20upon%20an%0Aegocentric%20vision-language%20model.%20Designed%20for%20deployment%20on%20portable%20devices%0Asuch%20as%20smartphones%20and%20wearable%20cameras%2C%20Vinci%20operates%20in%20an%20%22always%20on%22%0Amode%2C%20continuously%20observing%20the%20environment%20to%20deliver%20seamless%20interaction%0Aand%20assistance.%20Users%20can%20wake%20up%20the%20system%20and%20engage%20in%20natural%0Aconversations%20to%20ask%20questions%20or%20seek%20assistance%2C%20with%20responses%20delivered%0Athrough%20audio%20for%20hands-free%20convenience.%20With%20its%20ability%20to%20process%20long%0Avideo%20streams%20in%20real-time%2C%20Vinci%20can%20answer%20user%20queries%20about%20current%0Aobservations%20and%20historical%20context%20while%20also%20providing%20task%20planning%20based%20on%0Apast%20interactions.%20To%20further%20enhance%20usability%2C%20Vinci%20integrates%20a%20video%0Ageneration%20module%20that%20creates%20step-by-step%20visual%20demonstrations%20for%20tasks%0Athat%20require%20detailed%20guidance.%20We%20hope%20that%20Vinci%20can%20establish%20a%20robust%0Aframework%20for%20portable%2C%20real-time%20egocentric%20AI%20systems%2C%20empowering%20users%20with%0Acontextual%20and%20actionable%20insights.%20We%20release%20the%20complete%20implementation%20for%0Athe%20development%20of%20the%20device%20in%20conjunction%20with%20a%20demo%20web%20platform%20to%20test%0Auploaded%20videos%20at%20https%3A//github.com/OpenGVLab/vinci.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21080v1&entry.124074799=Read"},
{"title": "Gaussian Mixture Models Based Augmentation Enhances GNN Generalization", "author": "Yassine Abbahaddou and Fragkiskos D. Malliaros and Johannes F. Lutzeyer and Amine Mohamed Aboussalah and Michalis Vazirgiannis", "abstract": "  Graph Neural Networks (GNNs) have shown great promise in tasks like node and\ngraph classification, but they often struggle to generalize, particularly to\nunseen or out-of-distribution (OOD) data. These challenges are exacerbated when\ntraining data is limited in size or diversity. To address these issues, we\nintroduce a theoretical framework using Rademacher complexity to compute a\nregret bound on the generalization error and then characterize the effect of\ndata augmentation. This framework informs the design of GMM-GDA, an efficient\ngraph data augmentation (GDA) algorithm leveraging the capability of Gaussian\nMixture Models (GMMs) to approximate any distribution. Our approach not only\noutperforms existing augmentation techniques in terms of generalization but\nalso offers improved time complexity, making it highly suitable for real-world\napplications.\n", "link": "http://arxiv.org/abs/2411.08638v2", "date": "2024-12-30", "relevancy": 2.7267, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5618}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5375}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Mixture%20Models%20Based%20Augmentation%20Enhances%20GNN%20Generalization&body=Title%3A%20Gaussian%20Mixture%20Models%20Based%20Augmentation%20Enhances%20GNN%20Generalization%0AAuthor%3A%20Yassine%20Abbahaddou%20and%20Fragkiskos%20D.%20Malliaros%20and%20Johannes%20F.%20Lutzeyer%20and%20Amine%20Mohamed%20Aboussalah%20and%20Michalis%20Vazirgiannis%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20shown%20great%20promise%20in%20tasks%20like%20node%20and%0Agraph%20classification%2C%20but%20they%20often%20struggle%20to%20generalize%2C%20particularly%20to%0Aunseen%20or%20out-of-distribution%20%28OOD%29%20data.%20These%20challenges%20are%20exacerbated%20when%0Atraining%20data%20is%20limited%20in%20size%20or%20diversity.%20To%20address%20these%20issues%2C%20we%0Aintroduce%20a%20theoretical%20framework%20using%20Rademacher%20complexity%20to%20compute%20a%0Aregret%20bound%20on%20the%20generalization%20error%20and%20then%20characterize%20the%20effect%20of%0Adata%20augmentation.%20This%20framework%20informs%20the%20design%20of%20GMM-GDA%2C%20an%20efficient%0Agraph%20data%20augmentation%20%28GDA%29%20algorithm%20leveraging%20the%20capability%20of%20Gaussian%0AMixture%20Models%20%28GMMs%29%20to%20approximate%20any%20distribution.%20Our%20approach%20not%20only%0Aoutperforms%20existing%20augmentation%20techniques%20in%20terms%20of%20generalization%20but%0Aalso%20offers%20improved%20time%20complexity%2C%20making%20it%20highly%20suitable%20for%20real-world%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08638v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Mixture%2520Models%2520Based%2520Augmentation%2520Enhances%2520GNN%2520Generalization%26entry.906535625%3DYassine%2520Abbahaddou%2520and%2520Fragkiskos%2520D.%2520Malliaros%2520and%2520Johannes%2520F.%2520Lutzeyer%2520and%2520Amine%2520Mohamed%2520Aboussalah%2520and%2520Michalis%2520Vazirgiannis%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520shown%2520great%2520promise%2520in%2520tasks%2520like%2520node%2520and%250Agraph%2520classification%252C%2520but%2520they%2520often%2520struggle%2520to%2520generalize%252C%2520particularly%2520to%250Aunseen%2520or%2520out-of-distribution%2520%2528OOD%2529%2520data.%2520These%2520challenges%2520are%2520exacerbated%2520when%250Atraining%2520data%2520is%2520limited%2520in%2520size%2520or%2520diversity.%2520To%2520address%2520these%2520issues%252C%2520we%250Aintroduce%2520a%2520theoretical%2520framework%2520using%2520Rademacher%2520complexity%2520to%2520compute%2520a%250Aregret%2520bound%2520on%2520the%2520generalization%2520error%2520and%2520then%2520characterize%2520the%2520effect%2520of%250Adata%2520augmentation.%2520This%2520framework%2520informs%2520the%2520design%2520of%2520GMM-GDA%252C%2520an%2520efficient%250Agraph%2520data%2520augmentation%2520%2528GDA%2529%2520algorithm%2520leveraging%2520the%2520capability%2520of%2520Gaussian%250AMixture%2520Models%2520%2528GMMs%2529%2520to%2520approximate%2520any%2520distribution.%2520Our%2520approach%2520not%2520only%250Aoutperforms%2520existing%2520augmentation%2520techniques%2520in%2520terms%2520of%2520generalization%2520but%250Aalso%2520offers%2520improved%2520time%2520complexity%252C%2520making%2520it%2520highly%2520suitable%2520for%2520real-world%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08638v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Mixture%20Models%20Based%20Augmentation%20Enhances%20GNN%20Generalization&entry.906535625=Yassine%20Abbahaddou%20and%20Fragkiskos%20D.%20Malliaros%20and%20Johannes%20F.%20Lutzeyer%20and%20Amine%20Mohamed%20Aboussalah%20and%20Michalis%20Vazirgiannis&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20shown%20great%20promise%20in%20tasks%20like%20node%20and%0Agraph%20classification%2C%20but%20they%20often%20struggle%20to%20generalize%2C%20particularly%20to%0Aunseen%20or%20out-of-distribution%20%28OOD%29%20data.%20These%20challenges%20are%20exacerbated%20when%0Atraining%20data%20is%20limited%20in%20size%20or%20diversity.%20To%20address%20these%20issues%2C%20we%0Aintroduce%20a%20theoretical%20framework%20using%20Rademacher%20complexity%20to%20compute%20a%0Aregret%20bound%20on%20the%20generalization%20error%20and%20then%20characterize%20the%20effect%20of%0Adata%20augmentation.%20This%20framework%20informs%20the%20design%20of%20GMM-GDA%2C%20an%20efficient%0Agraph%20data%20augmentation%20%28GDA%29%20algorithm%20leveraging%20the%20capability%20of%20Gaussian%0AMixture%20Models%20%28GMMs%29%20to%20approximate%20any%20distribution.%20Our%20approach%20not%20only%0Aoutperforms%20existing%20augmentation%20techniques%20in%20terms%20of%20generalization%20but%0Aalso%20offers%20improved%20time%20complexity%2C%20making%20it%20highly%20suitable%20for%20real-world%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08638v2&entry.124074799=Read"},
{"title": "Conservation-informed Graph Learning for Spatiotemporal Dynamics\n  Prediction", "author": "Yuan Mi and Pu Ren and Hongteng Xu and Hongsheng Liu and Zidong Wang and Yike Guo and Ji-Rong Wen and Hao Sun and Yang Liu", "abstract": "  Data-centric methods have shown great potential in understanding and\npredicting spatiotemporal dynamics, enabling better design and control of the\nobject system. However, pure deep learning models often lack interpretability,\nfail to obey intrinsic physics, and struggle to cope with the various domains.\nWhile geometry-based methods, e.g., graph neural networks (GNNs), have been\nproposed to further tackle these challenges, they still need to find the\nimplicit physical laws from large datasets and rely excessively on rich labeled\ndata. In this paper, we herein introduce the conservation-informed GNN (CiGNN),\nan end-to-end explainable learning framework, to learn spatiotemporal dynamics\nbased on limited training data. The network is designed to conform to the\ngeneral conservation law via symmetry, where conservative and non-conservative\ninformation passes over a multiscale space enhanced by a latent temporal\nmarching strategy. The efficacy of our model has been verified in various\nspatiotemporal systems based on synthetic and real-world datasets, showing\nsuperiority over baseline models. Results demonstrate that CiGNN exhibits\nremarkable accuracy and generalization ability, and is readily applicable to\nlearning for prediction of various spatiotemporal dynamics in a spatial domain\nwith complex geometry.\n", "link": "http://arxiv.org/abs/2412.20962v1", "date": "2024-12-30", "relevancy": 2.6485, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5577}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5344}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conservation-informed%20Graph%20Learning%20for%20Spatiotemporal%20Dynamics%0A%20%20Prediction&body=Title%3A%20Conservation-informed%20Graph%20Learning%20for%20Spatiotemporal%20Dynamics%0A%20%20Prediction%0AAuthor%3A%20Yuan%20Mi%20and%20Pu%20Ren%20and%20Hongteng%20Xu%20and%20Hongsheng%20Liu%20and%20Zidong%20Wang%20and%20Yike%20Guo%20and%20Ji-Rong%20Wen%20and%20Hao%20Sun%20and%20Yang%20Liu%0AAbstract%3A%20%20%20Data-centric%20methods%20have%20shown%20great%20potential%20in%20understanding%20and%0Apredicting%20spatiotemporal%20dynamics%2C%20enabling%20better%20design%20and%20control%20of%20the%0Aobject%20system.%20However%2C%20pure%20deep%20learning%20models%20often%20lack%20interpretability%2C%0Afail%20to%20obey%20intrinsic%20physics%2C%20and%20struggle%20to%20cope%20with%20the%20various%20domains.%0AWhile%20geometry-based%20methods%2C%20e.g.%2C%20graph%20neural%20networks%20%28GNNs%29%2C%20have%20been%0Aproposed%20to%20further%20tackle%20these%20challenges%2C%20they%20still%20need%20to%20find%20the%0Aimplicit%20physical%20laws%20from%20large%20datasets%20and%20rely%20excessively%20on%20rich%20labeled%0Adata.%20In%20this%20paper%2C%20we%20herein%20introduce%20the%20conservation-informed%20GNN%20%28CiGNN%29%2C%0Aan%20end-to-end%20explainable%20learning%20framework%2C%20to%20learn%20spatiotemporal%20dynamics%0Abased%20on%20limited%20training%20data.%20The%20network%20is%20designed%20to%20conform%20to%20the%0Ageneral%20conservation%20law%20via%20symmetry%2C%20where%20conservative%20and%20non-conservative%0Ainformation%20passes%20over%20a%20multiscale%20space%20enhanced%20by%20a%20latent%20temporal%0Amarching%20strategy.%20The%20efficacy%20of%20our%20model%20has%20been%20verified%20in%20various%0Aspatiotemporal%20systems%20based%20on%20synthetic%20and%20real-world%20datasets%2C%20showing%0Asuperiority%20over%20baseline%20models.%20Results%20demonstrate%20that%20CiGNN%20exhibits%0Aremarkable%20accuracy%20and%20generalization%20ability%2C%20and%20is%20readily%20applicable%20to%0Alearning%20for%20prediction%20of%20various%20spatiotemporal%20dynamics%20in%20a%20spatial%20domain%0Awith%20complex%20geometry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20962v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConservation-informed%2520Graph%2520Learning%2520for%2520Spatiotemporal%2520Dynamics%250A%2520%2520Prediction%26entry.906535625%3DYuan%2520Mi%2520and%2520Pu%2520Ren%2520and%2520Hongteng%2520Xu%2520and%2520Hongsheng%2520Liu%2520and%2520Zidong%2520Wang%2520and%2520Yike%2520Guo%2520and%2520Ji-Rong%2520Wen%2520and%2520Hao%2520Sun%2520and%2520Yang%2520Liu%26entry.1292438233%3D%2520%2520Data-centric%2520methods%2520have%2520shown%2520great%2520potential%2520in%2520understanding%2520and%250Apredicting%2520spatiotemporal%2520dynamics%252C%2520enabling%2520better%2520design%2520and%2520control%2520of%2520the%250Aobject%2520system.%2520However%252C%2520pure%2520deep%2520learning%2520models%2520often%2520lack%2520interpretability%252C%250Afail%2520to%2520obey%2520intrinsic%2520physics%252C%2520and%2520struggle%2520to%2520cope%2520with%2520the%2520various%2520domains.%250AWhile%2520geometry-based%2520methods%252C%2520e.g.%252C%2520graph%2520neural%2520networks%2520%2528GNNs%2529%252C%2520have%2520been%250Aproposed%2520to%2520further%2520tackle%2520these%2520challenges%252C%2520they%2520still%2520need%2520to%2520find%2520the%250Aimplicit%2520physical%2520laws%2520from%2520large%2520datasets%2520and%2520rely%2520excessively%2520on%2520rich%2520labeled%250Adata.%2520In%2520this%2520paper%252C%2520we%2520herein%2520introduce%2520the%2520conservation-informed%2520GNN%2520%2528CiGNN%2529%252C%250Aan%2520end-to-end%2520explainable%2520learning%2520framework%252C%2520to%2520learn%2520spatiotemporal%2520dynamics%250Abased%2520on%2520limited%2520training%2520data.%2520The%2520network%2520is%2520designed%2520to%2520conform%2520to%2520the%250Ageneral%2520conservation%2520law%2520via%2520symmetry%252C%2520where%2520conservative%2520and%2520non-conservative%250Ainformation%2520passes%2520over%2520a%2520multiscale%2520space%2520enhanced%2520by%2520a%2520latent%2520temporal%250Amarching%2520strategy.%2520The%2520efficacy%2520of%2520our%2520model%2520has%2520been%2520verified%2520in%2520various%250Aspatiotemporal%2520systems%2520based%2520on%2520synthetic%2520and%2520real-world%2520datasets%252C%2520showing%250Asuperiority%2520over%2520baseline%2520models.%2520Results%2520demonstrate%2520that%2520CiGNN%2520exhibits%250Aremarkable%2520accuracy%2520and%2520generalization%2520ability%252C%2520and%2520is%2520readily%2520applicable%2520to%250Alearning%2520for%2520prediction%2520of%2520various%2520spatiotemporal%2520dynamics%2520in%2520a%2520spatial%2520domain%250Awith%2520complex%2520geometry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20962v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conservation-informed%20Graph%20Learning%20for%20Spatiotemporal%20Dynamics%0A%20%20Prediction&entry.906535625=Yuan%20Mi%20and%20Pu%20Ren%20and%20Hongteng%20Xu%20and%20Hongsheng%20Liu%20and%20Zidong%20Wang%20and%20Yike%20Guo%20and%20Ji-Rong%20Wen%20and%20Hao%20Sun%20and%20Yang%20Liu&entry.1292438233=%20%20Data-centric%20methods%20have%20shown%20great%20potential%20in%20understanding%20and%0Apredicting%20spatiotemporal%20dynamics%2C%20enabling%20better%20design%20and%20control%20of%20the%0Aobject%20system.%20However%2C%20pure%20deep%20learning%20models%20often%20lack%20interpretability%2C%0Afail%20to%20obey%20intrinsic%20physics%2C%20and%20struggle%20to%20cope%20with%20the%20various%20domains.%0AWhile%20geometry-based%20methods%2C%20e.g.%2C%20graph%20neural%20networks%20%28GNNs%29%2C%20have%20been%0Aproposed%20to%20further%20tackle%20these%20challenges%2C%20they%20still%20need%20to%20find%20the%0Aimplicit%20physical%20laws%20from%20large%20datasets%20and%20rely%20excessively%20on%20rich%20labeled%0Adata.%20In%20this%20paper%2C%20we%20herein%20introduce%20the%20conservation-informed%20GNN%20%28CiGNN%29%2C%0Aan%20end-to-end%20explainable%20learning%20framework%2C%20to%20learn%20spatiotemporal%20dynamics%0Abased%20on%20limited%20training%20data.%20The%20network%20is%20designed%20to%20conform%20to%20the%0Ageneral%20conservation%20law%20via%20symmetry%2C%20where%20conservative%20and%20non-conservative%0Ainformation%20passes%20over%20a%20multiscale%20space%20enhanced%20by%20a%20latent%20temporal%0Amarching%20strategy.%20The%20efficacy%20of%20our%20model%20has%20been%20verified%20in%20various%0Aspatiotemporal%20systems%20based%20on%20synthetic%20and%20real-world%20datasets%2C%20showing%0Asuperiority%20over%20baseline%20models.%20Results%20demonstrate%20that%20CiGNN%20exhibits%0Aremarkable%20accuracy%20and%20generalization%20ability%2C%20and%20is%20readily%20applicable%20to%0Alearning%20for%20prediction%20of%20various%20spatiotemporal%20dynamics%20in%20a%20spatial%20domain%0Awith%20complex%20geometry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20962v1&entry.124074799=Read"},
{"title": "Federated Learning with MMD-based Early Stopping for Adaptive GNSS\n  Interference Classification", "author": "Nishant S. Gaikwad and Lucas Heublein and Nisha L. Raichur and Tobias Feigl and Christopher Mutschler and Felix Ott", "abstract": "  Federated learning (FL) enables multiple devices to collaboratively train a\nglobal model while maintaining data on local servers. Each device trains the\nmodel on its local server and shares only the model updates (i.e., gradient\nweights) during the aggregation step. A significant challenge in FL is managing\nthe feature distribution of novel and unbalanced data across devices. In this\npaper, we propose an FL approach using few-shot learning and aggregation of the\nmodel weights on a global server. We introduce a dynamic early stopping method\nto balance out-of-distribution classes based on representation learning,\nspecifically utilizing the maximum mean discrepancy of feature embeddings\nbetween local and global models. An exemplary application of FL is to\norchestrate machine learning models along highways for interference\nclassification based on snapshots from global navigation satellite system\n(GNSS) receivers. Extensive experiments on four GNSS datasets from two\nreal-world highways and controlled environments demonstrate that our FL method\nsurpasses state-of-the-art techniques in adapting to both novel interference\nclasses and multipath scenarios.\n", "link": "http://arxiv.org/abs/2410.15681v2", "date": "2024-12-30", "relevancy": 2.6117, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5419}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5141}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.511}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Learning%20with%20MMD-based%20Early%20Stopping%20for%20Adaptive%20GNSS%0A%20%20Interference%20Classification&body=Title%3A%20Federated%20Learning%20with%20MMD-based%20Early%20Stopping%20for%20Adaptive%20GNSS%0A%20%20Interference%20Classification%0AAuthor%3A%20Nishant%20S.%20Gaikwad%20and%20Lucas%20Heublein%20and%20Nisha%20L.%20Raichur%20and%20Tobias%20Feigl%20and%20Christopher%20Mutschler%20and%20Felix%20Ott%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20enables%20multiple%20devices%20to%20collaboratively%20train%20a%0Aglobal%20model%20while%20maintaining%20data%20on%20local%20servers.%20Each%20device%20trains%20the%0Amodel%20on%20its%20local%20server%20and%20shares%20only%20the%20model%20updates%20%28i.e.%2C%20gradient%0Aweights%29%20during%20the%20aggregation%20step.%20A%20significant%20challenge%20in%20FL%20is%20managing%0Athe%20feature%20distribution%20of%20novel%20and%20unbalanced%20data%20across%20devices.%20In%20this%0Apaper%2C%20we%20propose%20an%20FL%20approach%20using%20few-shot%20learning%20and%20aggregation%20of%20the%0Amodel%20weights%20on%20a%20global%20server.%20We%20introduce%20a%20dynamic%20early%20stopping%20method%0Ato%20balance%20out-of-distribution%20classes%20based%20on%20representation%20learning%2C%0Aspecifically%20utilizing%20the%20maximum%20mean%20discrepancy%20of%20feature%20embeddings%0Abetween%20local%20and%20global%20models.%20An%20exemplary%20application%20of%20FL%20is%20to%0Aorchestrate%20machine%20learning%20models%20along%20highways%20for%20interference%0Aclassification%20based%20on%20snapshots%20from%20global%20navigation%20satellite%20system%0A%28GNSS%29%20receivers.%20Extensive%20experiments%20on%20four%20GNSS%20datasets%20from%20two%0Areal-world%20highways%20and%20controlled%20environments%20demonstrate%20that%20our%20FL%20method%0Asurpasses%20state-of-the-art%20techniques%20in%20adapting%20to%20both%20novel%20interference%0Aclasses%20and%20multipath%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.15681v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Learning%2520with%2520MMD-based%2520Early%2520Stopping%2520for%2520Adaptive%2520GNSS%250A%2520%2520Interference%2520Classification%26entry.906535625%3DNishant%2520S.%2520Gaikwad%2520and%2520Lucas%2520Heublein%2520and%2520Nisha%2520L.%2520Raichur%2520and%2520Tobias%2520Feigl%2520and%2520Christopher%2520Mutschler%2520and%2520Felix%2520Ott%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520enables%2520multiple%2520devices%2520to%2520collaboratively%2520train%2520a%250Aglobal%2520model%2520while%2520maintaining%2520data%2520on%2520local%2520servers.%2520Each%2520device%2520trains%2520the%250Amodel%2520on%2520its%2520local%2520server%2520and%2520shares%2520only%2520the%2520model%2520updates%2520%2528i.e.%252C%2520gradient%250Aweights%2529%2520during%2520the%2520aggregation%2520step.%2520A%2520significant%2520challenge%2520in%2520FL%2520is%2520managing%250Athe%2520feature%2520distribution%2520of%2520novel%2520and%2520unbalanced%2520data%2520across%2520devices.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520an%2520FL%2520approach%2520using%2520few-shot%2520learning%2520and%2520aggregation%2520of%2520the%250Amodel%2520weights%2520on%2520a%2520global%2520server.%2520We%2520introduce%2520a%2520dynamic%2520early%2520stopping%2520method%250Ato%2520balance%2520out-of-distribution%2520classes%2520based%2520on%2520representation%2520learning%252C%250Aspecifically%2520utilizing%2520the%2520maximum%2520mean%2520discrepancy%2520of%2520feature%2520embeddings%250Abetween%2520local%2520and%2520global%2520models.%2520An%2520exemplary%2520application%2520of%2520FL%2520is%2520to%250Aorchestrate%2520machine%2520learning%2520models%2520along%2520highways%2520for%2520interference%250Aclassification%2520based%2520on%2520snapshots%2520from%2520global%2520navigation%2520satellite%2520system%250A%2528GNSS%2529%2520receivers.%2520Extensive%2520experiments%2520on%2520four%2520GNSS%2520datasets%2520from%2520two%250Areal-world%2520highways%2520and%2520controlled%2520environments%2520demonstrate%2520that%2520our%2520FL%2520method%250Asurpasses%2520state-of-the-art%2520techniques%2520in%2520adapting%2520to%2520both%2520novel%2520interference%250Aclasses%2520and%2520multipath%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.15681v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Learning%20with%20MMD-based%20Early%20Stopping%20for%20Adaptive%20GNSS%0A%20%20Interference%20Classification&entry.906535625=Nishant%20S.%20Gaikwad%20and%20Lucas%20Heublein%20and%20Nisha%20L.%20Raichur%20and%20Tobias%20Feigl%20and%20Christopher%20Mutschler%20and%20Felix%20Ott&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20enables%20multiple%20devices%20to%20collaboratively%20train%20a%0Aglobal%20model%20while%20maintaining%20data%20on%20local%20servers.%20Each%20device%20trains%20the%0Amodel%20on%20its%20local%20server%20and%20shares%20only%20the%20model%20updates%20%28i.e.%2C%20gradient%0Aweights%29%20during%20the%20aggregation%20step.%20A%20significant%20challenge%20in%20FL%20is%20managing%0Athe%20feature%20distribution%20of%20novel%20and%20unbalanced%20data%20across%20devices.%20In%20this%0Apaper%2C%20we%20propose%20an%20FL%20approach%20using%20few-shot%20learning%20and%20aggregation%20of%20the%0Amodel%20weights%20on%20a%20global%20server.%20We%20introduce%20a%20dynamic%20early%20stopping%20method%0Ato%20balance%20out-of-distribution%20classes%20based%20on%20representation%20learning%2C%0Aspecifically%20utilizing%20the%20maximum%20mean%20discrepancy%20of%20feature%20embeddings%0Abetween%20local%20and%20global%20models.%20An%20exemplary%20application%20of%20FL%20is%20to%0Aorchestrate%20machine%20learning%20models%20along%20highways%20for%20interference%0Aclassification%20based%20on%20snapshots%20from%20global%20navigation%20satellite%20system%0A%28GNSS%29%20receivers.%20Extensive%20experiments%20on%20four%20GNSS%20datasets%20from%20two%0Areal-world%20highways%20and%20controlled%20environments%20demonstrate%20that%20our%20FL%20method%0Asurpasses%20state-of-the-art%20techniques%20in%20adapting%20to%20both%20novel%20interference%0Aclasses%20and%20multipath%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.15681v2&entry.124074799=Read"},
{"title": "Facilitating large language model Russian adaptation with Learned\n  Embedding Propagation", "author": "Mikhail Tikhomirov and Daniil Chernyshev", "abstract": "  Rapid advancements of large language model (LLM) technologies led to the\nintroduction of powerful open-source instruction-tuned LLMs that have the same\ntext generation quality as the state-of-the-art counterparts such as GPT-4.\nWhile the emergence of such models accelerates the adoption of LLM technologies\nin sensitive-information environments the authors of such models don not\ndisclose the training data necessary for replication of the results thus making\nthe achievements model-exclusive. Since those open-source models are also\nmultilingual this in turn reduces the benefits of training a language specific\nLLMs as improved inference computation efficiency becomes the only guaranteed\nadvantage of such costly procedure. More cost-efficient options such as\nvocabulary extension and subsequent continued pre-training are also inhibited\nby the lack of access to high-quality instruction-tuning data since it is the\nmajor factor behind the resulting LLM task-solving capabilities. To address the\nlimitations and cut the costs of the language adaptation pipeline we propose\nLearned Embedding Propagation (LEP). Unlike existing approaches our method has\nlower training data size requirements due to minimal impact on existing LLM\nknowledge which we reinforce using novel ad-hoc embedding propagation procedure\nthat allows to skip the instruction-tuning step and instead implant the new\nlanguage knowledge directly into any existing instruct-tuned variant. We\nevaluated four Russian vocabulary adaptations for LLaMa-3-8B and Mistral-7B,\nshowing that LEP is competitive with traditional instruction-tuning methods,\nachieving performance comparable to OpenChat 3.5 and LLaMa-3-8B-Instruct, with\nfurther improvements via self-calibration and continued tuning enhancing\ntask-solving capabilities.\n", "link": "http://arxiv.org/abs/2412.21140v1", "date": "2024-12-30", "relevancy": 2.5813, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5312}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5088}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Facilitating%20large%20language%20model%20Russian%20adaptation%20with%20Learned%0A%20%20Embedding%20Propagation&body=Title%3A%20Facilitating%20large%20language%20model%20Russian%20adaptation%20with%20Learned%0A%20%20Embedding%20Propagation%0AAuthor%3A%20Mikhail%20Tikhomirov%20and%20Daniil%20Chernyshev%0AAbstract%3A%20%20%20Rapid%20advancements%20of%20large%20language%20model%20%28LLM%29%20technologies%20led%20to%20the%0Aintroduction%20of%20powerful%20open-source%20instruction-tuned%20LLMs%20that%20have%20the%20same%0Atext%20generation%20quality%20as%20the%20state-of-the-art%20counterparts%20such%20as%20GPT-4.%0AWhile%20the%20emergence%20of%20such%20models%20accelerates%20the%20adoption%20of%20LLM%20technologies%0Ain%20sensitive-information%20environments%20the%20authors%20of%20such%20models%20don%20not%0Adisclose%20the%20training%20data%20necessary%20for%20replication%20of%20the%20results%20thus%20making%0Athe%20achievements%20model-exclusive.%20Since%20those%20open-source%20models%20are%20also%0Amultilingual%20this%20in%20turn%20reduces%20the%20benefits%20of%20training%20a%20language%20specific%0ALLMs%20as%20improved%20inference%20computation%20efficiency%20becomes%20the%20only%20guaranteed%0Aadvantage%20of%20such%20costly%20procedure.%20More%20cost-efficient%20options%20such%20as%0Avocabulary%20extension%20and%20subsequent%20continued%20pre-training%20are%20also%20inhibited%0Aby%20the%20lack%20of%20access%20to%20high-quality%20instruction-tuning%20data%20since%20it%20is%20the%0Amajor%20factor%20behind%20the%20resulting%20LLM%20task-solving%20capabilities.%20To%20address%20the%0Alimitations%20and%20cut%20the%20costs%20of%20the%20language%20adaptation%20pipeline%20we%20propose%0ALearned%20Embedding%20Propagation%20%28LEP%29.%20Unlike%20existing%20approaches%20our%20method%20has%0Alower%20training%20data%20size%20requirements%20due%20to%20minimal%20impact%20on%20existing%20LLM%0Aknowledge%20which%20we%20reinforce%20using%20novel%20ad-hoc%20embedding%20propagation%20procedure%0Athat%20allows%20to%20skip%20the%20instruction-tuning%20step%20and%20instead%20implant%20the%20new%0Alanguage%20knowledge%20directly%20into%20any%20existing%20instruct-tuned%20variant.%20We%0Aevaluated%20four%20Russian%20vocabulary%20adaptations%20for%20LLaMa-3-8B%20and%20Mistral-7B%2C%0Ashowing%20that%20LEP%20is%20competitive%20with%20traditional%20instruction-tuning%20methods%2C%0Aachieving%20performance%20comparable%20to%20OpenChat%203.5%20and%20LLaMa-3-8B-Instruct%2C%20with%0Afurther%20improvements%20via%20self-calibration%20and%20continued%20tuning%20enhancing%0Atask-solving%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21140v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFacilitating%2520large%2520language%2520model%2520Russian%2520adaptation%2520with%2520Learned%250A%2520%2520Embedding%2520Propagation%26entry.906535625%3DMikhail%2520Tikhomirov%2520and%2520Daniil%2520Chernyshev%26entry.1292438233%3D%2520%2520Rapid%2520advancements%2520of%2520large%2520language%2520model%2520%2528LLM%2529%2520technologies%2520led%2520to%2520the%250Aintroduction%2520of%2520powerful%2520open-source%2520instruction-tuned%2520LLMs%2520that%2520have%2520the%2520same%250Atext%2520generation%2520quality%2520as%2520the%2520state-of-the-art%2520counterparts%2520such%2520as%2520GPT-4.%250AWhile%2520the%2520emergence%2520of%2520such%2520models%2520accelerates%2520the%2520adoption%2520of%2520LLM%2520technologies%250Ain%2520sensitive-information%2520environments%2520the%2520authors%2520of%2520such%2520models%2520don%2520not%250Adisclose%2520the%2520training%2520data%2520necessary%2520for%2520replication%2520of%2520the%2520results%2520thus%2520making%250Athe%2520achievements%2520model-exclusive.%2520Since%2520those%2520open-source%2520models%2520are%2520also%250Amultilingual%2520this%2520in%2520turn%2520reduces%2520the%2520benefits%2520of%2520training%2520a%2520language%2520specific%250ALLMs%2520as%2520improved%2520inference%2520computation%2520efficiency%2520becomes%2520the%2520only%2520guaranteed%250Aadvantage%2520of%2520such%2520costly%2520procedure.%2520More%2520cost-efficient%2520options%2520such%2520as%250Avocabulary%2520extension%2520and%2520subsequent%2520continued%2520pre-training%2520are%2520also%2520inhibited%250Aby%2520the%2520lack%2520of%2520access%2520to%2520high-quality%2520instruction-tuning%2520data%2520since%2520it%2520is%2520the%250Amajor%2520factor%2520behind%2520the%2520resulting%2520LLM%2520task-solving%2520capabilities.%2520To%2520address%2520the%250Alimitations%2520and%2520cut%2520the%2520costs%2520of%2520the%2520language%2520adaptation%2520pipeline%2520we%2520propose%250ALearned%2520Embedding%2520Propagation%2520%2528LEP%2529.%2520Unlike%2520existing%2520approaches%2520our%2520method%2520has%250Alower%2520training%2520data%2520size%2520requirements%2520due%2520to%2520minimal%2520impact%2520on%2520existing%2520LLM%250Aknowledge%2520which%2520we%2520reinforce%2520using%2520novel%2520ad-hoc%2520embedding%2520propagation%2520procedure%250Athat%2520allows%2520to%2520skip%2520the%2520instruction-tuning%2520step%2520and%2520instead%2520implant%2520the%2520new%250Alanguage%2520knowledge%2520directly%2520into%2520any%2520existing%2520instruct-tuned%2520variant.%2520We%250Aevaluated%2520four%2520Russian%2520vocabulary%2520adaptations%2520for%2520LLaMa-3-8B%2520and%2520Mistral-7B%252C%250Ashowing%2520that%2520LEP%2520is%2520competitive%2520with%2520traditional%2520instruction-tuning%2520methods%252C%250Aachieving%2520performance%2520comparable%2520to%2520OpenChat%25203.5%2520and%2520LLaMa-3-8B-Instruct%252C%2520with%250Afurther%2520improvements%2520via%2520self-calibration%2520and%2520continued%2520tuning%2520enhancing%250Atask-solving%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21140v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Facilitating%20large%20language%20model%20Russian%20adaptation%20with%20Learned%0A%20%20Embedding%20Propagation&entry.906535625=Mikhail%20Tikhomirov%20and%20Daniil%20Chernyshev&entry.1292438233=%20%20Rapid%20advancements%20of%20large%20language%20model%20%28LLM%29%20technologies%20led%20to%20the%0Aintroduction%20of%20powerful%20open-source%20instruction-tuned%20LLMs%20that%20have%20the%20same%0Atext%20generation%20quality%20as%20the%20state-of-the-art%20counterparts%20such%20as%20GPT-4.%0AWhile%20the%20emergence%20of%20such%20models%20accelerates%20the%20adoption%20of%20LLM%20technologies%0Ain%20sensitive-information%20environments%20the%20authors%20of%20such%20models%20don%20not%0Adisclose%20the%20training%20data%20necessary%20for%20replication%20of%20the%20results%20thus%20making%0Athe%20achievements%20model-exclusive.%20Since%20those%20open-source%20models%20are%20also%0Amultilingual%20this%20in%20turn%20reduces%20the%20benefits%20of%20training%20a%20language%20specific%0ALLMs%20as%20improved%20inference%20computation%20efficiency%20becomes%20the%20only%20guaranteed%0Aadvantage%20of%20such%20costly%20procedure.%20More%20cost-efficient%20options%20such%20as%0Avocabulary%20extension%20and%20subsequent%20continued%20pre-training%20are%20also%20inhibited%0Aby%20the%20lack%20of%20access%20to%20high-quality%20instruction-tuning%20data%20since%20it%20is%20the%0Amajor%20factor%20behind%20the%20resulting%20LLM%20task-solving%20capabilities.%20To%20address%20the%0Alimitations%20and%20cut%20the%20costs%20of%20the%20language%20adaptation%20pipeline%20we%20propose%0ALearned%20Embedding%20Propagation%20%28LEP%29.%20Unlike%20existing%20approaches%20our%20method%20has%0Alower%20training%20data%20size%20requirements%20due%20to%20minimal%20impact%20on%20existing%20LLM%0Aknowledge%20which%20we%20reinforce%20using%20novel%20ad-hoc%20embedding%20propagation%20procedure%0Athat%20allows%20to%20skip%20the%20instruction-tuning%20step%20and%20instead%20implant%20the%20new%0Alanguage%20knowledge%20directly%20into%20any%20existing%20instruct-tuned%20variant.%20We%0Aevaluated%20four%20Russian%20vocabulary%20adaptations%20for%20LLaMa-3-8B%20and%20Mistral-7B%2C%0Ashowing%20that%20LEP%20is%20competitive%20with%20traditional%20instruction-tuning%20methods%2C%0Aachieving%20performance%20comparable%20to%20OpenChat%203.5%20and%20LLaMa-3-8B-Instruct%2C%20with%0Afurther%20improvements%20via%20self-calibration%20and%20continued%20tuning%20enhancing%0Atask-solving%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21140v1&entry.124074799=Read"},
{"title": "Efficient Link Prediction via GNN Layers Induced by Negative Sampling", "author": "Yuxin Wang and Xiannian Hu and Quan Gan and Xuanjing Huang and Xipeng Qiu and David Wipf", "abstract": "  Graph neural networks (GNNs) for link prediction can loosely be divided into\ntwo broad categories. First, \\emph{node-wise} architectures pre-compute\nindividual embeddings for each node that are later combined by a simple decoder\nto make predictions. While extremely efficient at inference time, model\nexpressiveness is limited such that isomorphic nodes contributing to candidate\nedges may not be distinguishable, compromising accuracy. In contrast,\n\\emph{edge-wise} methods rely on the formation of edge-specific subgraph\nembeddings to enrich the representation of pair-wise relationships,\ndisambiguating isomorphic nodes to improve accuracy, but with increased model\ncomplexity. To better navigate this trade-off, we propose a novel GNN\narchitecture whereby the \\emph{forward pass} explicitly depends on \\emph{both}\npositive (as is typical) and negative (unique to our approach) edges to inform\nmore flexible, yet still cheap node-wise embeddings. This is achieved by\nrecasting the embeddings themselves as minimizers of a forward-pass-specific\nenergy function that favors separation of positive and negative samples.\nNotably, this energy is distinct from the actual training loss shared by most\nexisting link prediction models, where contrastive pairs only influence the\n\\textit{backward pass}. As demonstrated by extensive empirical evaluations, the\nresulting architecture retains the inference speed of node-wise models, while\nproducing competitive accuracy with edge-wise alternatives. We released our\ncode at https://github.com/yxzwang/SubmissionverOfYinYanGNN.\n", "link": "http://arxiv.org/abs/2310.09516v2", "date": "2024-12-30", "relevancy": 2.5469, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5628}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4835}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4819}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Link%20Prediction%20via%20GNN%20Layers%20Induced%20by%20Negative%20Sampling&body=Title%3A%20Efficient%20Link%20Prediction%20via%20GNN%20Layers%20Induced%20by%20Negative%20Sampling%0AAuthor%3A%20Yuxin%20Wang%20and%20Xiannian%20Hu%20and%20Quan%20Gan%20and%20Xuanjing%20Huang%20and%20Xipeng%20Qiu%20and%20David%20Wipf%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20for%20link%20prediction%20can%20loosely%20be%20divided%20into%0Atwo%20broad%20categories.%20First%2C%20%5Cemph%7Bnode-wise%7D%20architectures%20pre-compute%0Aindividual%20embeddings%20for%20each%20node%20that%20are%20later%20combined%20by%20a%20simple%20decoder%0Ato%20make%20predictions.%20While%20extremely%20efficient%20at%20inference%20time%2C%20model%0Aexpressiveness%20is%20limited%20such%20that%20isomorphic%20nodes%20contributing%20to%20candidate%0Aedges%20may%20not%20be%20distinguishable%2C%20compromising%20accuracy.%20In%20contrast%2C%0A%5Cemph%7Bedge-wise%7D%20methods%20rely%20on%20the%20formation%20of%20edge-specific%20subgraph%0Aembeddings%20to%20enrich%20the%20representation%20of%20pair-wise%20relationships%2C%0Adisambiguating%20isomorphic%20nodes%20to%20improve%20accuracy%2C%20but%20with%20increased%20model%0Acomplexity.%20To%20better%20navigate%20this%20trade-off%2C%20we%20propose%20a%20novel%20GNN%0Aarchitecture%20whereby%20the%20%5Cemph%7Bforward%20pass%7D%20explicitly%20depends%20on%20%5Cemph%7Bboth%7D%0Apositive%20%28as%20is%20typical%29%20and%20negative%20%28unique%20to%20our%20approach%29%20edges%20to%20inform%0Amore%20flexible%2C%20yet%20still%20cheap%20node-wise%20embeddings.%20This%20is%20achieved%20by%0Arecasting%20the%20embeddings%20themselves%20as%20minimizers%20of%20a%20forward-pass-specific%0Aenergy%20function%20that%20favors%20separation%20of%20positive%20and%20negative%20samples.%0ANotably%2C%20this%20energy%20is%20distinct%20from%20the%20actual%20training%20loss%20shared%20by%20most%0Aexisting%20link%20prediction%20models%2C%20where%20contrastive%20pairs%20only%20influence%20the%0A%5Ctextit%7Bbackward%20pass%7D.%20As%20demonstrated%20by%20extensive%20empirical%20evaluations%2C%20the%0Aresulting%20architecture%20retains%20the%20inference%20speed%20of%20node-wise%20models%2C%20while%0Aproducing%20competitive%20accuracy%20with%20edge-wise%20alternatives.%20We%20released%20our%0Acode%20at%20https%3A//github.com/yxzwang/SubmissionverOfYinYanGNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.09516v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Link%2520Prediction%2520via%2520GNN%2520Layers%2520Induced%2520by%2520Negative%2520Sampling%26entry.906535625%3DYuxin%2520Wang%2520and%2520Xiannian%2520Hu%2520and%2520Quan%2520Gan%2520and%2520Xuanjing%2520Huang%2520and%2520Xipeng%2520Qiu%2520and%2520David%2520Wipf%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520for%2520link%2520prediction%2520can%2520loosely%2520be%2520divided%2520into%250Atwo%2520broad%2520categories.%2520First%252C%2520%255Cemph%257Bnode-wise%257D%2520architectures%2520pre-compute%250Aindividual%2520embeddings%2520for%2520each%2520node%2520that%2520are%2520later%2520combined%2520by%2520a%2520simple%2520decoder%250Ato%2520make%2520predictions.%2520While%2520extremely%2520efficient%2520at%2520inference%2520time%252C%2520model%250Aexpressiveness%2520is%2520limited%2520such%2520that%2520isomorphic%2520nodes%2520contributing%2520to%2520candidate%250Aedges%2520may%2520not%2520be%2520distinguishable%252C%2520compromising%2520accuracy.%2520In%2520contrast%252C%250A%255Cemph%257Bedge-wise%257D%2520methods%2520rely%2520on%2520the%2520formation%2520of%2520edge-specific%2520subgraph%250Aembeddings%2520to%2520enrich%2520the%2520representation%2520of%2520pair-wise%2520relationships%252C%250Adisambiguating%2520isomorphic%2520nodes%2520to%2520improve%2520accuracy%252C%2520but%2520with%2520increased%2520model%250Acomplexity.%2520To%2520better%2520navigate%2520this%2520trade-off%252C%2520we%2520propose%2520a%2520novel%2520GNN%250Aarchitecture%2520whereby%2520the%2520%255Cemph%257Bforward%2520pass%257D%2520explicitly%2520depends%2520on%2520%255Cemph%257Bboth%257D%250Apositive%2520%2528as%2520is%2520typical%2529%2520and%2520negative%2520%2528unique%2520to%2520our%2520approach%2529%2520edges%2520to%2520inform%250Amore%2520flexible%252C%2520yet%2520still%2520cheap%2520node-wise%2520embeddings.%2520This%2520is%2520achieved%2520by%250Arecasting%2520the%2520embeddings%2520themselves%2520as%2520minimizers%2520of%2520a%2520forward-pass-specific%250Aenergy%2520function%2520that%2520favors%2520separation%2520of%2520positive%2520and%2520negative%2520samples.%250ANotably%252C%2520this%2520energy%2520is%2520distinct%2520from%2520the%2520actual%2520training%2520loss%2520shared%2520by%2520most%250Aexisting%2520link%2520prediction%2520models%252C%2520where%2520contrastive%2520pairs%2520only%2520influence%2520the%250A%255Ctextit%257Bbackward%2520pass%257D.%2520As%2520demonstrated%2520by%2520extensive%2520empirical%2520evaluations%252C%2520the%250Aresulting%2520architecture%2520retains%2520the%2520inference%2520speed%2520of%2520node-wise%2520models%252C%2520while%250Aproducing%2520competitive%2520accuracy%2520with%2520edge-wise%2520alternatives.%2520We%2520released%2520our%250Acode%2520at%2520https%253A//github.com/yxzwang/SubmissionverOfYinYanGNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.09516v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Link%20Prediction%20via%20GNN%20Layers%20Induced%20by%20Negative%20Sampling&entry.906535625=Yuxin%20Wang%20and%20Xiannian%20Hu%20and%20Quan%20Gan%20and%20Xuanjing%20Huang%20and%20Xipeng%20Qiu%20and%20David%20Wipf&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20for%20link%20prediction%20can%20loosely%20be%20divided%20into%0Atwo%20broad%20categories.%20First%2C%20%5Cemph%7Bnode-wise%7D%20architectures%20pre-compute%0Aindividual%20embeddings%20for%20each%20node%20that%20are%20later%20combined%20by%20a%20simple%20decoder%0Ato%20make%20predictions.%20While%20extremely%20efficient%20at%20inference%20time%2C%20model%0Aexpressiveness%20is%20limited%20such%20that%20isomorphic%20nodes%20contributing%20to%20candidate%0Aedges%20may%20not%20be%20distinguishable%2C%20compromising%20accuracy.%20In%20contrast%2C%0A%5Cemph%7Bedge-wise%7D%20methods%20rely%20on%20the%20formation%20of%20edge-specific%20subgraph%0Aembeddings%20to%20enrich%20the%20representation%20of%20pair-wise%20relationships%2C%0Adisambiguating%20isomorphic%20nodes%20to%20improve%20accuracy%2C%20but%20with%20increased%20model%0Acomplexity.%20To%20better%20navigate%20this%20trade-off%2C%20we%20propose%20a%20novel%20GNN%0Aarchitecture%20whereby%20the%20%5Cemph%7Bforward%20pass%7D%20explicitly%20depends%20on%20%5Cemph%7Bboth%7D%0Apositive%20%28as%20is%20typical%29%20and%20negative%20%28unique%20to%20our%20approach%29%20edges%20to%20inform%0Amore%20flexible%2C%20yet%20still%20cheap%20node-wise%20embeddings.%20This%20is%20achieved%20by%0Arecasting%20the%20embeddings%20themselves%20as%20minimizers%20of%20a%20forward-pass-specific%0Aenergy%20function%20that%20favors%20separation%20of%20positive%20and%20negative%20samples.%0ANotably%2C%20this%20energy%20is%20distinct%20from%20the%20actual%20training%20loss%20shared%20by%20most%0Aexisting%20link%20prediction%20models%2C%20where%20contrastive%20pairs%20only%20influence%20the%0A%5Ctextit%7Bbackward%20pass%7D.%20As%20demonstrated%20by%20extensive%20empirical%20evaluations%2C%20the%0Aresulting%20architecture%20retains%20the%20inference%20speed%20of%20node-wise%20models%2C%20while%0Aproducing%20competitive%20accuracy%20with%20edge-wise%20alternatives.%20We%20released%20our%0Acode%20at%20https%3A//github.com/yxzwang/SubmissionverOfYinYanGNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.09516v2&entry.124074799=Read"},
{"title": "CAVE-Net: Classifying Abnormalities in Video Capsule Endoscopy", "author": "Ishita Harish and Saurav Mishra and Neha Bhadoria and Rithik Kumar and Madhav Arora and Syed Rameem Zahra and Ankur Gupta", "abstract": "  Accurate classification of medical images is critical for detecting\nabnormalities in the gastrointestinal tract, a domain where misclassification\ncan significantly impact patient outcomes. We propose an ensemble-based\napproach to improve diagnostic accuracy in analyzing complex image datasets.\nUsing a Convolutional Block Attention Module along with a Deep Neural Network,\nwe leverage the unique feature extraction capabilities of each model to enhance\nthe overall accuracy. The classification models, such as Random Forest,\nXGBoost, Support Vector Machine and K-Nearest Neighbors are introduced to\nfurther diversify the predictive power of proposed ensemble. By using these\nmethods, the proposed framework, CAVE-Net, provides robust feature\ndiscrimination and improved classification results. Experimental evaluations\ndemonstrate that the CAVE-Net achieves high accuracy and robustness across\nchallenging and imbalanced classes, showing significant promise for broader\napplications in computer vision tasks.\n", "link": "http://arxiv.org/abs/2410.20231v3", "date": "2024-12-30", "relevancy": 2.5045, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5025}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5012}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAVE-Net%3A%20Classifying%20Abnormalities%20in%20Video%20Capsule%20Endoscopy&body=Title%3A%20CAVE-Net%3A%20Classifying%20Abnormalities%20in%20Video%20Capsule%20Endoscopy%0AAuthor%3A%20Ishita%20Harish%20and%20Saurav%20Mishra%20and%20Neha%20Bhadoria%20and%20Rithik%20Kumar%20and%20Madhav%20Arora%20and%20Syed%20Rameem%20Zahra%20and%20Ankur%20Gupta%0AAbstract%3A%20%20%20Accurate%20classification%20of%20medical%20images%20is%20critical%20for%20detecting%0Aabnormalities%20in%20the%20gastrointestinal%20tract%2C%20a%20domain%20where%20misclassification%0Acan%20significantly%20impact%20patient%20outcomes.%20We%20propose%20an%20ensemble-based%0Aapproach%20to%20improve%20diagnostic%20accuracy%20in%20analyzing%20complex%20image%20datasets.%0AUsing%20a%20Convolutional%20Block%20Attention%20Module%20along%20with%20a%20Deep%20Neural%20Network%2C%0Awe%20leverage%20the%20unique%20feature%20extraction%20capabilities%20of%20each%20model%20to%20enhance%0Athe%20overall%20accuracy.%20The%20classification%20models%2C%20such%20as%20Random%20Forest%2C%0AXGBoost%2C%20Support%20Vector%20Machine%20and%20K-Nearest%20Neighbors%20are%20introduced%20to%0Afurther%20diversify%20the%20predictive%20power%20of%20proposed%20ensemble.%20By%20using%20these%0Amethods%2C%20the%20proposed%20framework%2C%20CAVE-Net%2C%20provides%20robust%20feature%0Adiscrimination%20and%20improved%20classification%20results.%20Experimental%20evaluations%0Ademonstrate%20that%20the%20CAVE-Net%20achieves%20high%20accuracy%20and%20robustness%20across%0Achallenging%20and%20imbalanced%20classes%2C%20showing%20significant%20promise%20for%20broader%0Aapplications%20in%20computer%20vision%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.20231v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAVE-Net%253A%2520Classifying%2520Abnormalities%2520in%2520Video%2520Capsule%2520Endoscopy%26entry.906535625%3DIshita%2520Harish%2520and%2520Saurav%2520Mishra%2520and%2520Neha%2520Bhadoria%2520and%2520Rithik%2520Kumar%2520and%2520Madhav%2520Arora%2520and%2520Syed%2520Rameem%2520Zahra%2520and%2520Ankur%2520Gupta%26entry.1292438233%3D%2520%2520Accurate%2520classification%2520of%2520medical%2520images%2520is%2520critical%2520for%2520detecting%250Aabnormalities%2520in%2520the%2520gastrointestinal%2520tract%252C%2520a%2520domain%2520where%2520misclassification%250Acan%2520significantly%2520impact%2520patient%2520outcomes.%2520We%2520propose%2520an%2520ensemble-based%250Aapproach%2520to%2520improve%2520diagnostic%2520accuracy%2520in%2520analyzing%2520complex%2520image%2520datasets.%250AUsing%2520a%2520Convolutional%2520Block%2520Attention%2520Module%2520along%2520with%2520a%2520Deep%2520Neural%2520Network%252C%250Awe%2520leverage%2520the%2520unique%2520feature%2520extraction%2520capabilities%2520of%2520each%2520model%2520to%2520enhance%250Athe%2520overall%2520accuracy.%2520The%2520classification%2520models%252C%2520such%2520as%2520Random%2520Forest%252C%250AXGBoost%252C%2520Support%2520Vector%2520Machine%2520and%2520K-Nearest%2520Neighbors%2520are%2520introduced%2520to%250Afurther%2520diversify%2520the%2520predictive%2520power%2520of%2520proposed%2520ensemble.%2520By%2520using%2520these%250Amethods%252C%2520the%2520proposed%2520framework%252C%2520CAVE-Net%252C%2520provides%2520robust%2520feature%250Adiscrimination%2520and%2520improved%2520classification%2520results.%2520Experimental%2520evaluations%250Ademonstrate%2520that%2520the%2520CAVE-Net%2520achieves%2520high%2520accuracy%2520and%2520robustness%2520across%250Achallenging%2520and%2520imbalanced%2520classes%252C%2520showing%2520significant%2520promise%2520for%2520broader%250Aapplications%2520in%2520computer%2520vision%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.20231v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAVE-Net%3A%20Classifying%20Abnormalities%20in%20Video%20Capsule%20Endoscopy&entry.906535625=Ishita%20Harish%20and%20Saurav%20Mishra%20and%20Neha%20Bhadoria%20and%20Rithik%20Kumar%20and%20Madhav%20Arora%20and%20Syed%20Rameem%20Zahra%20and%20Ankur%20Gupta&entry.1292438233=%20%20Accurate%20classification%20of%20medical%20images%20is%20critical%20for%20detecting%0Aabnormalities%20in%20the%20gastrointestinal%20tract%2C%20a%20domain%20where%20misclassification%0Acan%20significantly%20impact%20patient%20outcomes.%20We%20propose%20an%20ensemble-based%0Aapproach%20to%20improve%20diagnostic%20accuracy%20in%20analyzing%20complex%20image%20datasets.%0AUsing%20a%20Convolutional%20Block%20Attention%20Module%20along%20with%20a%20Deep%20Neural%20Network%2C%0Awe%20leverage%20the%20unique%20feature%20extraction%20capabilities%20of%20each%20model%20to%20enhance%0Athe%20overall%20accuracy.%20The%20classification%20models%2C%20such%20as%20Random%20Forest%2C%0AXGBoost%2C%20Support%20Vector%20Machine%20and%20K-Nearest%20Neighbors%20are%20introduced%20to%0Afurther%20diversify%20the%20predictive%20power%20of%20proposed%20ensemble.%20By%20using%20these%0Amethods%2C%20the%20proposed%20framework%2C%20CAVE-Net%2C%20provides%20robust%20feature%0Adiscrimination%20and%20improved%20classification%20results.%20Experimental%20evaluations%0Ademonstrate%20that%20the%20CAVE-Net%20achieves%20high%20accuracy%20and%20robustness%20across%0Achallenging%20and%20imbalanced%20classes%2C%20showing%20significant%20promise%20for%20broader%0Aapplications%20in%20computer%20vision%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.20231v3&entry.124074799=Read"},
{"title": "What Makes for a Good Stereoscopic Image?", "author": "Netanel Y. Tamir and Shir Amir and Ranel Itzhaky and Noam Atia and Shobhita Sundaram and Stephanie Fu and Ron Sokolovsky and Phillip Isola and Tali Dekel and Richard Zhang and Miriam Farber", "abstract": "  With rapid advancements in virtual reality (VR) headsets, effectively\nmeasuring stereoscopic quality of experience (SQoE) has become essential for\ndelivering immersive and comfortable 3D experiences. However, most existing\nstereo metrics focus on isolated aspects of the viewing experience such as\nvisual discomfort or image quality, and have traditionally faced data\nlimitations. To address these gaps, we present SCOPE (Stereoscopic COntent\nPreference Evaluation), a new dataset comprised of real and synthetic\nstereoscopic images featuring a wide range of common perceptual distortions and\nartifacts. The dataset is labeled with preference annotations collected on a VR\nheadset, with our findings indicating a notable degree of consistency in user\npreferences across different headsets. Additionally, we present iSQoE, a new\nmodel for stereo quality of experience assessment trained on our dataset. We\nshow that iSQoE aligns better with human preferences than existing methods when\ncomparing mono-to-stereo conversion methods.\n", "link": "http://arxiv.org/abs/2412.21127v1", "date": "2024-12-30", "relevancy": 2.4948, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5175}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5175}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20Makes%20for%20a%20Good%20Stereoscopic%20Image%3F&body=Title%3A%20What%20Makes%20for%20a%20Good%20Stereoscopic%20Image%3F%0AAuthor%3A%20Netanel%20Y.%20Tamir%20and%20Shir%20Amir%20and%20Ranel%20Itzhaky%20and%20Noam%20Atia%20and%20Shobhita%20Sundaram%20and%20Stephanie%20Fu%20and%20Ron%20Sokolovsky%20and%20Phillip%20Isola%20and%20Tali%20Dekel%20and%20Richard%20Zhang%20and%20Miriam%20Farber%0AAbstract%3A%20%20%20With%20rapid%20advancements%20in%20virtual%20reality%20%28VR%29%20headsets%2C%20effectively%0Ameasuring%20stereoscopic%20quality%20of%20experience%20%28SQoE%29%20has%20become%20essential%20for%0Adelivering%20immersive%20and%20comfortable%203D%20experiences.%20However%2C%20most%20existing%0Astereo%20metrics%20focus%20on%20isolated%20aspects%20of%20the%20viewing%20experience%20such%20as%0Avisual%20discomfort%20or%20image%20quality%2C%20and%20have%20traditionally%20faced%20data%0Alimitations.%20To%20address%20these%20gaps%2C%20we%20present%20SCOPE%20%28Stereoscopic%20COntent%0APreference%20Evaluation%29%2C%20a%20new%20dataset%20comprised%20of%20real%20and%20synthetic%0Astereoscopic%20images%20featuring%20a%20wide%20range%20of%20common%20perceptual%20distortions%20and%0Aartifacts.%20The%20dataset%20is%20labeled%20with%20preference%20annotations%20collected%20on%20a%20VR%0Aheadset%2C%20with%20our%20findings%20indicating%20a%20notable%20degree%20of%20consistency%20in%20user%0Apreferences%20across%20different%20headsets.%20Additionally%2C%20we%20present%20iSQoE%2C%20a%20new%0Amodel%20for%20stereo%20quality%20of%20experience%20assessment%20trained%20on%20our%20dataset.%20We%0Ashow%20that%20iSQoE%20aligns%20better%20with%20human%20preferences%20than%20existing%20methods%20when%0Acomparing%20mono-to-stereo%20conversion%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21127v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520Makes%2520for%2520a%2520Good%2520Stereoscopic%2520Image%253F%26entry.906535625%3DNetanel%2520Y.%2520Tamir%2520and%2520Shir%2520Amir%2520and%2520Ranel%2520Itzhaky%2520and%2520Noam%2520Atia%2520and%2520Shobhita%2520Sundaram%2520and%2520Stephanie%2520Fu%2520and%2520Ron%2520Sokolovsky%2520and%2520Phillip%2520Isola%2520and%2520Tali%2520Dekel%2520and%2520Richard%2520Zhang%2520and%2520Miriam%2520Farber%26entry.1292438233%3D%2520%2520With%2520rapid%2520advancements%2520in%2520virtual%2520reality%2520%2528VR%2529%2520headsets%252C%2520effectively%250Ameasuring%2520stereoscopic%2520quality%2520of%2520experience%2520%2528SQoE%2529%2520has%2520become%2520essential%2520for%250Adelivering%2520immersive%2520and%2520comfortable%25203D%2520experiences.%2520However%252C%2520most%2520existing%250Astereo%2520metrics%2520focus%2520on%2520isolated%2520aspects%2520of%2520the%2520viewing%2520experience%2520such%2520as%250Avisual%2520discomfort%2520or%2520image%2520quality%252C%2520and%2520have%2520traditionally%2520faced%2520data%250Alimitations.%2520To%2520address%2520these%2520gaps%252C%2520we%2520present%2520SCOPE%2520%2528Stereoscopic%2520COntent%250APreference%2520Evaluation%2529%252C%2520a%2520new%2520dataset%2520comprised%2520of%2520real%2520and%2520synthetic%250Astereoscopic%2520images%2520featuring%2520a%2520wide%2520range%2520of%2520common%2520perceptual%2520distortions%2520and%250Aartifacts.%2520The%2520dataset%2520is%2520labeled%2520with%2520preference%2520annotations%2520collected%2520on%2520a%2520VR%250Aheadset%252C%2520with%2520our%2520findings%2520indicating%2520a%2520notable%2520degree%2520of%2520consistency%2520in%2520user%250Apreferences%2520across%2520different%2520headsets.%2520Additionally%252C%2520we%2520present%2520iSQoE%252C%2520a%2520new%250Amodel%2520for%2520stereo%2520quality%2520of%2520experience%2520assessment%2520trained%2520on%2520our%2520dataset.%2520We%250Ashow%2520that%2520iSQoE%2520aligns%2520better%2520with%2520human%2520preferences%2520than%2520existing%2520methods%2520when%250Acomparing%2520mono-to-stereo%2520conversion%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21127v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20Makes%20for%20a%20Good%20Stereoscopic%20Image%3F&entry.906535625=Netanel%20Y.%20Tamir%20and%20Shir%20Amir%20and%20Ranel%20Itzhaky%20and%20Noam%20Atia%20and%20Shobhita%20Sundaram%20and%20Stephanie%20Fu%20and%20Ron%20Sokolovsky%20and%20Phillip%20Isola%20and%20Tali%20Dekel%20and%20Richard%20Zhang%20and%20Miriam%20Farber&entry.1292438233=%20%20With%20rapid%20advancements%20in%20virtual%20reality%20%28VR%29%20headsets%2C%20effectively%0Ameasuring%20stereoscopic%20quality%20of%20experience%20%28SQoE%29%20has%20become%20essential%20for%0Adelivering%20immersive%20and%20comfortable%203D%20experiences.%20However%2C%20most%20existing%0Astereo%20metrics%20focus%20on%20isolated%20aspects%20of%20the%20viewing%20experience%20such%20as%0Avisual%20discomfort%20or%20image%20quality%2C%20and%20have%20traditionally%20faced%20data%0Alimitations.%20To%20address%20these%20gaps%2C%20we%20present%20SCOPE%20%28Stereoscopic%20COntent%0APreference%20Evaluation%29%2C%20a%20new%20dataset%20comprised%20of%20real%20and%20synthetic%0Astereoscopic%20images%20featuring%20a%20wide%20range%20of%20common%20perceptual%20distortions%20and%0Aartifacts.%20The%20dataset%20is%20labeled%20with%20preference%20annotations%20collected%20on%20a%20VR%0Aheadset%2C%20with%20our%20findings%20indicating%20a%20notable%20degree%20of%20consistency%20in%20user%0Apreferences%20across%20different%20headsets.%20Additionally%2C%20we%20present%20iSQoE%2C%20a%20new%0Amodel%20for%20stereo%20quality%20of%20experience%20assessment%20trained%20on%20our%20dataset.%20We%0Ashow%20that%20iSQoE%20aligns%20better%20with%20human%20preferences%20than%20existing%20methods%20when%0Acomparing%20mono-to-stereo%20conversion%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21127v1&entry.124074799=Read"},
{"title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator", "author": "Guoxuan Chen and Han Shi and Jiawei Li and Yihang Gao and Xiaozhe Ren and Yimeng Chen and Xin Jiang and Zhenguo Li and Weiyang Liu and Chao Huang", "abstract": "  Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.\n", "link": "http://arxiv.org/abs/2412.12094v3", "date": "2024-12-30", "relevancy": 2.4882, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.503}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.495}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SepLLM%3A%20Accelerate%20Large%20Language%20Models%20by%20Compressing%20One%20Segment%20into%0A%20%20One%20Separator&body=Title%3A%20SepLLM%3A%20Accelerate%20Large%20Language%20Models%20by%20Compressing%20One%20Segment%20into%0A%20%20One%20Separator%0AAuthor%3A%20Guoxuan%20Chen%20and%20Han%20Shi%20and%20Jiawei%20Li%20and%20Yihang%20Gao%20and%20Xiaozhe%20Ren%20and%20Yimeng%20Chen%20and%20Xin%20Jiang%20and%20Zhenguo%20Li%20and%20Weiyang%20Liu%20and%20Chao%20Huang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20exhibited%20exceptional%20performance%20across%20a%0Aspectrum%20of%20natural%20language%20processing%20tasks.%20However%2C%20their%20substantial%20sizes%0Apose%20considerable%20challenges%2C%20particularly%20in%20computational%20demands%20and%0Ainference%20speed%2C%20due%20to%20their%20quadratic%20complexity.%20In%20this%20work%2C%20we%20have%0Aidentified%20a%20key%20pattern%3A%20certain%20seemingly%20meaningless%20special%20tokens%20%28i.e.%2C%0Aseparators%29%20contribute%20disproportionately%20to%20attention%20scores%20compared%20to%0Asemantically%20meaningful%20tokens.%20This%20observation%20suggests%20that%20information%20of%0Athe%20segments%20between%20these%20separator%20tokens%20can%20be%20effectively%20condensed%20into%0Athe%20separator%20tokens%20themselves%20without%20significant%20information%20loss.%20Guided%20by%0Athis%20insight%2C%20we%20introduce%20SepLLM%2C%20a%20plug-and-play%20framework%20that%20accelerates%0Ainference%20by%20compressing%20these%20segments%20and%20eliminating%20redundant%20tokens.%0AAdditionally%2C%20we%20implement%20efficient%20kernels%20for%20training%20acceleration.%0AExperimental%20results%20across%20training-free%2C%20training-from-scratch%2C%20and%0Apost-training%20settings%20demonstrate%20SepLLM%27s%20effectiveness.%20Notably%2C%20using%20the%0ALlama-3-8B%20backbone%2C%20SepLLM%20achieves%20over%2050%25%20reduction%20in%20KV%20cache%20on%20the%0AGSM8K-CoT%20benchmark%20while%20maintaining%20comparable%20performance.%20Furthermore%2C%20in%0Astreaming%20settings%2C%20SepLLM%20effectively%20processes%20sequences%20of%20up%20to%204%20million%0Atokens%20or%20more%20while%20maintaining%20consistent%20language%20modeling%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.12094v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSepLLM%253A%2520Accelerate%2520Large%2520Language%2520Models%2520by%2520Compressing%2520One%2520Segment%2520into%250A%2520%2520One%2520Separator%26entry.906535625%3DGuoxuan%2520Chen%2520and%2520Han%2520Shi%2520and%2520Jiawei%2520Li%2520and%2520Yihang%2520Gao%2520and%2520Xiaozhe%2520Ren%2520and%2520Yimeng%2520Chen%2520and%2520Xin%2520Jiang%2520and%2520Zhenguo%2520Li%2520and%2520Weiyang%2520Liu%2520and%2520Chao%2520Huang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520exhibited%2520exceptional%2520performance%2520across%2520a%250Aspectrum%2520of%2520natural%2520language%2520processing%2520tasks.%2520However%252C%2520their%2520substantial%2520sizes%250Apose%2520considerable%2520challenges%252C%2520particularly%2520in%2520computational%2520demands%2520and%250Ainference%2520speed%252C%2520due%2520to%2520their%2520quadratic%2520complexity.%2520In%2520this%2520work%252C%2520we%2520have%250Aidentified%2520a%2520key%2520pattern%253A%2520certain%2520seemingly%2520meaningless%2520special%2520tokens%2520%2528i.e.%252C%250Aseparators%2529%2520contribute%2520disproportionately%2520to%2520attention%2520scores%2520compared%2520to%250Asemantically%2520meaningful%2520tokens.%2520This%2520observation%2520suggests%2520that%2520information%2520of%250Athe%2520segments%2520between%2520these%2520separator%2520tokens%2520can%2520be%2520effectively%2520condensed%2520into%250Athe%2520separator%2520tokens%2520themselves%2520without%2520significant%2520information%2520loss.%2520Guided%2520by%250Athis%2520insight%252C%2520we%2520introduce%2520SepLLM%252C%2520a%2520plug-and-play%2520framework%2520that%2520accelerates%250Ainference%2520by%2520compressing%2520these%2520segments%2520and%2520eliminating%2520redundant%2520tokens.%250AAdditionally%252C%2520we%2520implement%2520efficient%2520kernels%2520for%2520training%2520acceleration.%250AExperimental%2520results%2520across%2520training-free%252C%2520training-from-scratch%252C%2520and%250Apost-training%2520settings%2520demonstrate%2520SepLLM%2527s%2520effectiveness.%2520Notably%252C%2520using%2520the%250ALlama-3-8B%2520backbone%252C%2520SepLLM%2520achieves%2520over%252050%2525%2520reduction%2520in%2520KV%2520cache%2520on%2520the%250AGSM8K-CoT%2520benchmark%2520while%2520maintaining%2520comparable%2520performance.%2520Furthermore%252C%2520in%250Astreaming%2520settings%252C%2520SepLLM%2520effectively%2520processes%2520sequences%2520of%2520up%2520to%25204%2520million%250Atokens%2520or%2520more%2520while%2520maintaining%2520consistent%2520language%2520modeling%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.12094v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SepLLM%3A%20Accelerate%20Large%20Language%20Models%20by%20Compressing%20One%20Segment%20into%0A%20%20One%20Separator&entry.906535625=Guoxuan%20Chen%20and%20Han%20Shi%20and%20Jiawei%20Li%20and%20Yihang%20Gao%20and%20Xiaozhe%20Ren%20and%20Yimeng%20Chen%20and%20Xin%20Jiang%20and%20Zhenguo%20Li%20and%20Weiyang%20Liu%20and%20Chao%20Huang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20exhibited%20exceptional%20performance%20across%20a%0Aspectrum%20of%20natural%20language%20processing%20tasks.%20However%2C%20their%20substantial%20sizes%0Apose%20considerable%20challenges%2C%20particularly%20in%20computational%20demands%20and%0Ainference%20speed%2C%20due%20to%20their%20quadratic%20complexity.%20In%20this%20work%2C%20we%20have%0Aidentified%20a%20key%20pattern%3A%20certain%20seemingly%20meaningless%20special%20tokens%20%28i.e.%2C%0Aseparators%29%20contribute%20disproportionately%20to%20attention%20scores%20compared%20to%0Asemantically%20meaningful%20tokens.%20This%20observation%20suggests%20that%20information%20of%0Athe%20segments%20between%20these%20separator%20tokens%20can%20be%20effectively%20condensed%20into%0Athe%20separator%20tokens%20themselves%20without%20significant%20information%20loss.%20Guided%20by%0Athis%20insight%2C%20we%20introduce%20SepLLM%2C%20a%20plug-and-play%20framework%20that%20accelerates%0Ainference%20by%20compressing%20these%20segments%20and%20eliminating%20redundant%20tokens.%0AAdditionally%2C%20we%20implement%20efficient%20kernels%20for%20training%20acceleration.%0AExperimental%20results%20across%20training-free%2C%20training-from-scratch%2C%20and%0Apost-training%20settings%20demonstrate%20SepLLM%27s%20effectiveness.%20Notably%2C%20using%20the%0ALlama-3-8B%20backbone%2C%20SepLLM%20achieves%20over%2050%25%20reduction%20in%20KV%20cache%20on%20the%0AGSM8K-CoT%20benchmark%20while%20maintaining%20comparable%20performance.%20Furthermore%2C%20in%0Astreaming%20settings%2C%20SepLLM%20effectively%20processes%20sequences%20of%20up%20to%204%20million%0Atokens%20or%20more%20while%20maintaining%20consistent%20language%20modeling%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.12094v3&entry.124074799=Read"},
{"title": "Hierarchical Pose Estimation and Mapping with Multi-Scale Neural Feature\n  Fields", "author": "Evgenii Kruzhkov and Alena Savinykh and Sven Behnke", "abstract": "  Robotic applications require a comprehensive understanding of the scene. In\nrecent years, neural fields-based approaches that parameterize the entire\nenvironment have become popular. These approaches are promising due to their\ncontinuous nature and their ability to learn scene priors. However, the use of\nneural fields in robotics becomes challenging when dealing with unknown sensor\nposes and sequential measurements. This paper focuses on the problem of sensor\npose estimation for large-scale neural implicit SLAM. We investigate implicit\nmapping from a probabilistic perspective and propose hierarchical pose\nestimation with a corresponding neural network architecture. Our method is\nwell-suited for large-scale implicit map representations. The proposed approach\noperates on consecutive outdoor LiDAR scans and achieves accurate pose\nestimation, while maintaining stable mapping quality for both short and long\ntrajectories. We built our method on a structured and sparse implicit\nrepresentation suitable for large-scale reconstruction and evaluated it using\nthe KITTI and MaiCity datasets. Our approach outperforms the baseline in terms\nof mapping with unknown poses and achieves state-of-the-art localization\naccuracy.\n", "link": "http://arxiv.org/abs/2412.20976v1", "date": "2024-12-30", "relevancy": 2.4873, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6782}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6328}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Pose%20Estimation%20and%20Mapping%20with%20Multi-Scale%20Neural%20Feature%0A%20%20Fields&body=Title%3A%20Hierarchical%20Pose%20Estimation%20and%20Mapping%20with%20Multi-Scale%20Neural%20Feature%0A%20%20Fields%0AAuthor%3A%20Evgenii%20Kruzhkov%20and%20Alena%20Savinykh%20and%20Sven%20Behnke%0AAbstract%3A%20%20%20Robotic%20applications%20require%20a%20comprehensive%20understanding%20of%20the%20scene.%20In%0Arecent%20years%2C%20neural%20fields-based%20approaches%20that%20parameterize%20the%20entire%0Aenvironment%20have%20become%20popular.%20These%20approaches%20are%20promising%20due%20to%20their%0Acontinuous%20nature%20and%20their%20ability%20to%20learn%20scene%20priors.%20However%2C%20the%20use%20of%0Aneural%20fields%20in%20robotics%20becomes%20challenging%20when%20dealing%20with%20unknown%20sensor%0Aposes%20and%20sequential%20measurements.%20This%20paper%20focuses%20on%20the%20problem%20of%20sensor%0Apose%20estimation%20for%20large-scale%20neural%20implicit%20SLAM.%20We%20investigate%20implicit%0Amapping%20from%20a%20probabilistic%20perspective%20and%20propose%20hierarchical%20pose%0Aestimation%20with%20a%20corresponding%20neural%20network%20architecture.%20Our%20method%20is%0Awell-suited%20for%20large-scale%20implicit%20map%20representations.%20The%20proposed%20approach%0Aoperates%20on%20consecutive%20outdoor%20LiDAR%20scans%20and%20achieves%20accurate%20pose%0Aestimation%2C%20while%20maintaining%20stable%20mapping%20quality%20for%20both%20short%20and%20long%0Atrajectories.%20We%20built%20our%20method%20on%20a%20structured%20and%20sparse%20implicit%0Arepresentation%20suitable%20for%20large-scale%20reconstruction%20and%20evaluated%20it%20using%0Athe%20KITTI%20and%20MaiCity%20datasets.%20Our%20approach%20outperforms%20the%20baseline%20in%20terms%0Aof%20mapping%20with%20unknown%20poses%20and%20achieves%20state-of-the-art%20localization%0Aaccuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20976v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Pose%2520Estimation%2520and%2520Mapping%2520with%2520Multi-Scale%2520Neural%2520Feature%250A%2520%2520Fields%26entry.906535625%3DEvgenii%2520Kruzhkov%2520and%2520Alena%2520Savinykh%2520and%2520Sven%2520Behnke%26entry.1292438233%3D%2520%2520Robotic%2520applications%2520require%2520a%2520comprehensive%2520understanding%2520of%2520the%2520scene.%2520In%250Arecent%2520years%252C%2520neural%2520fields-based%2520approaches%2520that%2520parameterize%2520the%2520entire%250Aenvironment%2520have%2520become%2520popular.%2520These%2520approaches%2520are%2520promising%2520due%2520to%2520their%250Acontinuous%2520nature%2520and%2520their%2520ability%2520to%2520learn%2520scene%2520priors.%2520However%252C%2520the%2520use%2520of%250Aneural%2520fields%2520in%2520robotics%2520becomes%2520challenging%2520when%2520dealing%2520with%2520unknown%2520sensor%250Aposes%2520and%2520sequential%2520measurements.%2520This%2520paper%2520focuses%2520on%2520the%2520problem%2520of%2520sensor%250Apose%2520estimation%2520for%2520large-scale%2520neural%2520implicit%2520SLAM.%2520We%2520investigate%2520implicit%250Amapping%2520from%2520a%2520probabilistic%2520perspective%2520and%2520propose%2520hierarchical%2520pose%250Aestimation%2520with%2520a%2520corresponding%2520neural%2520network%2520architecture.%2520Our%2520method%2520is%250Awell-suited%2520for%2520large-scale%2520implicit%2520map%2520representations.%2520The%2520proposed%2520approach%250Aoperates%2520on%2520consecutive%2520outdoor%2520LiDAR%2520scans%2520and%2520achieves%2520accurate%2520pose%250Aestimation%252C%2520while%2520maintaining%2520stable%2520mapping%2520quality%2520for%2520both%2520short%2520and%2520long%250Atrajectories.%2520We%2520built%2520our%2520method%2520on%2520a%2520structured%2520and%2520sparse%2520implicit%250Arepresentation%2520suitable%2520for%2520large-scale%2520reconstruction%2520and%2520evaluated%2520it%2520using%250Athe%2520KITTI%2520and%2520MaiCity%2520datasets.%2520Our%2520approach%2520outperforms%2520the%2520baseline%2520in%2520terms%250Aof%2520mapping%2520with%2520unknown%2520poses%2520and%2520achieves%2520state-of-the-art%2520localization%250Aaccuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20976v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Pose%20Estimation%20and%20Mapping%20with%20Multi-Scale%20Neural%20Feature%0A%20%20Fields&entry.906535625=Evgenii%20Kruzhkov%20and%20Alena%20Savinykh%20and%20Sven%20Behnke&entry.1292438233=%20%20Robotic%20applications%20require%20a%20comprehensive%20understanding%20of%20the%20scene.%20In%0Arecent%20years%2C%20neural%20fields-based%20approaches%20that%20parameterize%20the%20entire%0Aenvironment%20have%20become%20popular.%20These%20approaches%20are%20promising%20due%20to%20their%0Acontinuous%20nature%20and%20their%20ability%20to%20learn%20scene%20priors.%20However%2C%20the%20use%20of%0Aneural%20fields%20in%20robotics%20becomes%20challenging%20when%20dealing%20with%20unknown%20sensor%0Aposes%20and%20sequential%20measurements.%20This%20paper%20focuses%20on%20the%20problem%20of%20sensor%0Apose%20estimation%20for%20large-scale%20neural%20implicit%20SLAM.%20We%20investigate%20implicit%0Amapping%20from%20a%20probabilistic%20perspective%20and%20propose%20hierarchical%20pose%0Aestimation%20with%20a%20corresponding%20neural%20network%20architecture.%20Our%20method%20is%0Awell-suited%20for%20large-scale%20implicit%20map%20representations.%20The%20proposed%20approach%0Aoperates%20on%20consecutive%20outdoor%20LiDAR%20scans%20and%20achieves%20accurate%20pose%0Aestimation%2C%20while%20maintaining%20stable%20mapping%20quality%20for%20both%20short%20and%20long%0Atrajectories.%20We%20built%20our%20method%20on%20a%20structured%20and%20sparse%20implicit%0Arepresentation%20suitable%20for%20large-scale%20reconstruction%20and%20evaluated%20it%20using%0Athe%20KITTI%20and%20MaiCity%20datasets.%20Our%20approach%20outperforms%20the%20baseline%20in%20terms%0Aof%20mapping%20with%20unknown%20poses%20and%20achieves%20state-of-the-art%20localization%0Aaccuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20976v1&entry.124074799=Read"},
{"title": "HisynSeg: Weakly-Supervised Histopathological Image Segmentation via\n  Image-Mixing Synthesis and Consistency Regularization", "author": "Zijie Fang and Yifeng Wang and Peizhang Xie and Zhi Wang and Yongbing Zhang", "abstract": "  Tissue semantic segmentation is one of the key tasks in computational\npathology. To avoid the expensive and laborious acquisition of pixel-level\nannotations, a wide range of studies attempt to adopt the class activation map\n(CAM), a weakly-supervised learning scheme, to achieve pixel-level tissue\nsegmentation. However, CAM-based methods are prone to suffer from\nunder-activation and over-activation issues, leading to poor segmentation\nperformance. To address this problem, we propose a novel weakly-supervised\nsemantic segmentation framework for histopathological images based on\nimage-mixing synthesis and consistency regularization, dubbed HisynSeg.\nSpecifically, synthesized histopathological images with pixel-level masks are\ngenerated for fully-supervised model training, where two synthesis strategies\nare proposed based on Mosaic transformation and B\\'ezier mask generation.\nBesides, an image filtering module is developed to guarantee the authenticity\nof the synthesized images. In order to further avoid the model overfitting to\nthe occasional synthesis artifacts, we additionally propose a novel\nself-supervised consistency regularization, which enables the real images\nwithout segmentation masks to supervise the training of the segmentation model.\nBy integrating the proposed techniques, the HisynSeg framework successfully\ntransforms the weakly-supervised semantic segmentation problem into a\nfully-supervised one, greatly improving the segmentation accuracy. Experimental\nresults on three datasets prove that the proposed method achieves a\nstate-of-the-art performance. Code is available at\nhttps://github.com/Vison307/HisynSeg.\n", "link": "http://arxiv.org/abs/2412.20924v1", "date": "2024-12-30", "relevancy": 2.4647, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4983}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.491}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HisynSeg%3A%20Weakly-Supervised%20Histopathological%20Image%20Segmentation%20via%0A%20%20Image-Mixing%20Synthesis%20and%20Consistency%20Regularization&body=Title%3A%20HisynSeg%3A%20Weakly-Supervised%20Histopathological%20Image%20Segmentation%20via%0A%20%20Image-Mixing%20Synthesis%20and%20Consistency%20Regularization%0AAuthor%3A%20Zijie%20Fang%20and%20Yifeng%20Wang%20and%20Peizhang%20Xie%20and%20Zhi%20Wang%20and%20Yongbing%20Zhang%0AAbstract%3A%20%20%20Tissue%20semantic%20segmentation%20is%20one%20of%20the%20key%20tasks%20in%20computational%0Apathology.%20To%20avoid%20the%20expensive%20and%20laborious%20acquisition%20of%20pixel-level%0Aannotations%2C%20a%20wide%20range%20of%20studies%20attempt%20to%20adopt%20the%20class%20activation%20map%0A%28CAM%29%2C%20a%20weakly-supervised%20learning%20scheme%2C%20to%20achieve%20pixel-level%20tissue%0Asegmentation.%20However%2C%20CAM-based%20methods%20are%20prone%20to%20suffer%20from%0Aunder-activation%20and%20over-activation%20issues%2C%20leading%20to%20poor%20segmentation%0Aperformance.%20To%20address%20this%20problem%2C%20we%20propose%20a%20novel%20weakly-supervised%0Asemantic%20segmentation%20framework%20for%20histopathological%20images%20based%20on%0Aimage-mixing%20synthesis%20and%20consistency%20regularization%2C%20dubbed%20HisynSeg.%0ASpecifically%2C%20synthesized%20histopathological%20images%20with%20pixel-level%20masks%20are%0Agenerated%20for%20fully-supervised%20model%20training%2C%20where%20two%20synthesis%20strategies%0Aare%20proposed%20based%20on%20Mosaic%20transformation%20and%20B%5C%27ezier%20mask%20generation.%0ABesides%2C%20an%20image%20filtering%20module%20is%20developed%20to%20guarantee%20the%20authenticity%0Aof%20the%20synthesized%20images.%20In%20order%20to%20further%20avoid%20the%20model%20overfitting%20to%0Athe%20occasional%20synthesis%20artifacts%2C%20we%20additionally%20propose%20a%20novel%0Aself-supervised%20consistency%20regularization%2C%20which%20enables%20the%20real%20images%0Awithout%20segmentation%20masks%20to%20supervise%20the%20training%20of%20the%20segmentation%20model.%0ABy%20integrating%20the%20proposed%20techniques%2C%20the%20HisynSeg%20framework%20successfully%0Atransforms%20the%20weakly-supervised%20semantic%20segmentation%20problem%20into%20a%0Afully-supervised%20one%2C%20greatly%20improving%20the%20segmentation%20accuracy.%20Experimental%0Aresults%20on%20three%20datasets%20prove%20that%20the%20proposed%20method%20achieves%20a%0Astate-of-the-art%20performance.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Vison307/HisynSeg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20924v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHisynSeg%253A%2520Weakly-Supervised%2520Histopathological%2520Image%2520Segmentation%2520via%250A%2520%2520Image-Mixing%2520Synthesis%2520and%2520Consistency%2520Regularization%26entry.906535625%3DZijie%2520Fang%2520and%2520Yifeng%2520Wang%2520and%2520Peizhang%2520Xie%2520and%2520Zhi%2520Wang%2520and%2520Yongbing%2520Zhang%26entry.1292438233%3D%2520%2520Tissue%2520semantic%2520segmentation%2520is%2520one%2520of%2520the%2520key%2520tasks%2520in%2520computational%250Apathology.%2520To%2520avoid%2520the%2520expensive%2520and%2520laborious%2520acquisition%2520of%2520pixel-level%250Aannotations%252C%2520a%2520wide%2520range%2520of%2520studies%2520attempt%2520to%2520adopt%2520the%2520class%2520activation%2520map%250A%2528CAM%2529%252C%2520a%2520weakly-supervised%2520learning%2520scheme%252C%2520to%2520achieve%2520pixel-level%2520tissue%250Asegmentation.%2520However%252C%2520CAM-based%2520methods%2520are%2520prone%2520to%2520suffer%2520from%250Aunder-activation%2520and%2520over-activation%2520issues%252C%2520leading%2520to%2520poor%2520segmentation%250Aperformance.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520a%2520novel%2520weakly-supervised%250Asemantic%2520segmentation%2520framework%2520for%2520histopathological%2520images%2520based%2520on%250Aimage-mixing%2520synthesis%2520and%2520consistency%2520regularization%252C%2520dubbed%2520HisynSeg.%250ASpecifically%252C%2520synthesized%2520histopathological%2520images%2520with%2520pixel-level%2520masks%2520are%250Agenerated%2520for%2520fully-supervised%2520model%2520training%252C%2520where%2520two%2520synthesis%2520strategies%250Aare%2520proposed%2520based%2520on%2520Mosaic%2520transformation%2520and%2520B%255C%2527ezier%2520mask%2520generation.%250ABesides%252C%2520an%2520image%2520filtering%2520module%2520is%2520developed%2520to%2520guarantee%2520the%2520authenticity%250Aof%2520the%2520synthesized%2520images.%2520In%2520order%2520to%2520further%2520avoid%2520the%2520model%2520overfitting%2520to%250Athe%2520occasional%2520synthesis%2520artifacts%252C%2520we%2520additionally%2520propose%2520a%2520novel%250Aself-supervised%2520consistency%2520regularization%252C%2520which%2520enables%2520the%2520real%2520images%250Awithout%2520segmentation%2520masks%2520to%2520supervise%2520the%2520training%2520of%2520the%2520segmentation%2520model.%250ABy%2520integrating%2520the%2520proposed%2520techniques%252C%2520the%2520HisynSeg%2520framework%2520successfully%250Atransforms%2520the%2520weakly-supervised%2520semantic%2520segmentation%2520problem%2520into%2520a%250Afully-supervised%2520one%252C%2520greatly%2520improving%2520the%2520segmentation%2520accuracy.%2520Experimental%250Aresults%2520on%2520three%2520datasets%2520prove%2520that%2520the%2520proposed%2520method%2520achieves%2520a%250Astate-of-the-art%2520performance.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/Vison307/HisynSeg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20924v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HisynSeg%3A%20Weakly-Supervised%20Histopathological%20Image%20Segmentation%20via%0A%20%20Image-Mixing%20Synthesis%20and%20Consistency%20Regularization&entry.906535625=Zijie%20Fang%20and%20Yifeng%20Wang%20and%20Peizhang%20Xie%20and%20Zhi%20Wang%20and%20Yongbing%20Zhang&entry.1292438233=%20%20Tissue%20semantic%20segmentation%20is%20one%20of%20the%20key%20tasks%20in%20computational%0Apathology.%20To%20avoid%20the%20expensive%20and%20laborious%20acquisition%20of%20pixel-level%0Aannotations%2C%20a%20wide%20range%20of%20studies%20attempt%20to%20adopt%20the%20class%20activation%20map%0A%28CAM%29%2C%20a%20weakly-supervised%20learning%20scheme%2C%20to%20achieve%20pixel-level%20tissue%0Asegmentation.%20However%2C%20CAM-based%20methods%20are%20prone%20to%20suffer%20from%0Aunder-activation%20and%20over-activation%20issues%2C%20leading%20to%20poor%20segmentation%0Aperformance.%20To%20address%20this%20problem%2C%20we%20propose%20a%20novel%20weakly-supervised%0Asemantic%20segmentation%20framework%20for%20histopathological%20images%20based%20on%0Aimage-mixing%20synthesis%20and%20consistency%20regularization%2C%20dubbed%20HisynSeg.%0ASpecifically%2C%20synthesized%20histopathological%20images%20with%20pixel-level%20masks%20are%0Agenerated%20for%20fully-supervised%20model%20training%2C%20where%20two%20synthesis%20strategies%0Aare%20proposed%20based%20on%20Mosaic%20transformation%20and%20B%5C%27ezier%20mask%20generation.%0ABesides%2C%20an%20image%20filtering%20module%20is%20developed%20to%20guarantee%20the%20authenticity%0Aof%20the%20synthesized%20images.%20In%20order%20to%20further%20avoid%20the%20model%20overfitting%20to%0Athe%20occasional%20synthesis%20artifacts%2C%20we%20additionally%20propose%20a%20novel%0Aself-supervised%20consistency%20regularization%2C%20which%20enables%20the%20real%20images%0Awithout%20segmentation%20masks%20to%20supervise%20the%20training%20of%20the%20segmentation%20model.%0ABy%20integrating%20the%20proposed%20techniques%2C%20the%20HisynSeg%20framework%20successfully%0Atransforms%20the%20weakly-supervised%20semantic%20segmentation%20problem%20into%20a%0Afully-supervised%20one%2C%20greatly%20improving%20the%20segmentation%20accuracy.%20Experimental%0Aresults%20on%20three%20datasets%20prove%20that%20the%20proposed%20method%20achieves%20a%0Astate-of-the-art%20performance.%20Code%20is%20available%20at%0Ahttps%3A//github.com/Vison307/HisynSeg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20924v1&entry.124074799=Read"},
{"title": "Rise of Generative Artificial Intelligence in Science", "author": "Liangping Ding and Cornelia Lawson and Philip Shapira", "abstract": "  Generative Artificial Intelligence (GenAI, generative AI) has rapidly become\navailable as a tool in scientific research. To explore the use of generative AI\nin science, we conduct an empirical analysis using OpenAlex. Analyzing GenAI\npublications and other AI publications from 2017 to 2023, we profile growth\npatterns, the diffusion of GenAI publications across fields of study, and the\ngeographical spread of scientific research on generative AI. We also\ninvestigate team size and international collaborations to explore whether\nGenAI, as an emerging scientific research area, shows different collaboration\npatterns compared to other AI technologies. The results indicate that\ngenerative AI has experienced rapid growth and increasing presence in\nscientific publications. The use of GenAI now extends beyond computer science\nto other scientific research domains. Over the study period, U.S. researchers\ncontributed nearly two-fifths of global GenAI publications. The U.S. is\nfollowed by China, with several small and medium-sized advanced economies\ndemonstrating relatively high levels of GenAI deployment in their research\npublications. Although scientific research overall is becoming increasingly\nspecialized and collaborative, our results suggest that GenAI research groups\ntend to have slightly smaller team sizes than found in other AI fields.\nFurthermore, notwithstanding recent geopolitical tensions, GenAI research\ncontinues to exhibit levels of international collaboration comparable to other\nAI technologies.\n", "link": "http://arxiv.org/abs/2412.20960v1", "date": "2024-12-30", "relevancy": 2.4439, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5462}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4671}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rise%20of%20Generative%20Artificial%20Intelligence%20in%20Science&body=Title%3A%20Rise%20of%20Generative%20Artificial%20Intelligence%20in%20Science%0AAuthor%3A%20Liangping%20Ding%20and%20Cornelia%20Lawson%20and%20Philip%20Shapira%0AAbstract%3A%20%20%20Generative%20Artificial%20Intelligence%20%28GenAI%2C%20generative%20AI%29%20has%20rapidly%20become%0Aavailable%20as%20a%20tool%20in%20scientific%20research.%20To%20explore%20the%20use%20of%20generative%20AI%0Ain%20science%2C%20we%20conduct%20an%20empirical%20analysis%20using%20OpenAlex.%20Analyzing%20GenAI%0Apublications%20and%20other%20AI%20publications%20from%202017%20to%202023%2C%20we%20profile%20growth%0Apatterns%2C%20the%20diffusion%20of%20GenAI%20publications%20across%20fields%20of%20study%2C%20and%20the%0Ageographical%20spread%20of%20scientific%20research%20on%20generative%20AI.%20We%20also%0Ainvestigate%20team%20size%20and%20international%20collaborations%20to%20explore%20whether%0AGenAI%2C%20as%20an%20emerging%20scientific%20research%20area%2C%20shows%20different%20collaboration%0Apatterns%20compared%20to%20other%20AI%20technologies.%20The%20results%20indicate%20that%0Agenerative%20AI%20has%20experienced%20rapid%20growth%20and%20increasing%20presence%20in%0Ascientific%20publications.%20The%20use%20of%20GenAI%20now%20extends%20beyond%20computer%20science%0Ato%20other%20scientific%20research%20domains.%20Over%20the%20study%20period%2C%20U.S.%20researchers%0Acontributed%20nearly%20two-fifths%20of%20global%20GenAI%20publications.%20The%20U.S.%20is%0Afollowed%20by%20China%2C%20with%20several%20small%20and%20medium-sized%20advanced%20economies%0Ademonstrating%20relatively%20high%20levels%20of%20GenAI%20deployment%20in%20their%20research%0Apublications.%20Although%20scientific%20research%20overall%20is%20becoming%20increasingly%0Aspecialized%20and%20collaborative%2C%20our%20results%20suggest%20that%20GenAI%20research%20groups%0Atend%20to%20have%20slightly%20smaller%20team%20sizes%20than%20found%20in%20other%20AI%20fields.%0AFurthermore%2C%20notwithstanding%20recent%20geopolitical%20tensions%2C%20GenAI%20research%0Acontinues%20to%20exhibit%20levels%20of%20international%20collaboration%20comparable%20to%20other%0AAI%20technologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20960v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRise%2520of%2520Generative%2520Artificial%2520Intelligence%2520in%2520Science%26entry.906535625%3DLiangping%2520Ding%2520and%2520Cornelia%2520Lawson%2520and%2520Philip%2520Shapira%26entry.1292438233%3D%2520%2520Generative%2520Artificial%2520Intelligence%2520%2528GenAI%252C%2520generative%2520AI%2529%2520has%2520rapidly%2520become%250Aavailable%2520as%2520a%2520tool%2520in%2520scientific%2520research.%2520To%2520explore%2520the%2520use%2520of%2520generative%2520AI%250Ain%2520science%252C%2520we%2520conduct%2520an%2520empirical%2520analysis%2520using%2520OpenAlex.%2520Analyzing%2520GenAI%250Apublications%2520and%2520other%2520AI%2520publications%2520from%25202017%2520to%25202023%252C%2520we%2520profile%2520growth%250Apatterns%252C%2520the%2520diffusion%2520of%2520GenAI%2520publications%2520across%2520fields%2520of%2520study%252C%2520and%2520the%250Ageographical%2520spread%2520of%2520scientific%2520research%2520on%2520generative%2520AI.%2520We%2520also%250Ainvestigate%2520team%2520size%2520and%2520international%2520collaborations%2520to%2520explore%2520whether%250AGenAI%252C%2520as%2520an%2520emerging%2520scientific%2520research%2520area%252C%2520shows%2520different%2520collaboration%250Apatterns%2520compared%2520to%2520other%2520AI%2520technologies.%2520The%2520results%2520indicate%2520that%250Agenerative%2520AI%2520has%2520experienced%2520rapid%2520growth%2520and%2520increasing%2520presence%2520in%250Ascientific%2520publications.%2520The%2520use%2520of%2520GenAI%2520now%2520extends%2520beyond%2520computer%2520science%250Ato%2520other%2520scientific%2520research%2520domains.%2520Over%2520the%2520study%2520period%252C%2520U.S.%2520researchers%250Acontributed%2520nearly%2520two-fifths%2520of%2520global%2520GenAI%2520publications.%2520The%2520U.S.%2520is%250Afollowed%2520by%2520China%252C%2520with%2520several%2520small%2520and%2520medium-sized%2520advanced%2520economies%250Ademonstrating%2520relatively%2520high%2520levels%2520of%2520GenAI%2520deployment%2520in%2520their%2520research%250Apublications.%2520Although%2520scientific%2520research%2520overall%2520is%2520becoming%2520increasingly%250Aspecialized%2520and%2520collaborative%252C%2520our%2520results%2520suggest%2520that%2520GenAI%2520research%2520groups%250Atend%2520to%2520have%2520slightly%2520smaller%2520team%2520sizes%2520than%2520found%2520in%2520other%2520AI%2520fields.%250AFurthermore%252C%2520notwithstanding%2520recent%2520geopolitical%2520tensions%252C%2520GenAI%2520research%250Acontinues%2520to%2520exhibit%2520levels%2520of%2520international%2520collaboration%2520comparable%2520to%2520other%250AAI%2520technologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20960v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rise%20of%20Generative%20Artificial%20Intelligence%20in%20Science&entry.906535625=Liangping%20Ding%20and%20Cornelia%20Lawson%20and%20Philip%20Shapira&entry.1292438233=%20%20Generative%20Artificial%20Intelligence%20%28GenAI%2C%20generative%20AI%29%20has%20rapidly%20become%0Aavailable%20as%20a%20tool%20in%20scientific%20research.%20To%20explore%20the%20use%20of%20generative%20AI%0Ain%20science%2C%20we%20conduct%20an%20empirical%20analysis%20using%20OpenAlex.%20Analyzing%20GenAI%0Apublications%20and%20other%20AI%20publications%20from%202017%20to%202023%2C%20we%20profile%20growth%0Apatterns%2C%20the%20diffusion%20of%20GenAI%20publications%20across%20fields%20of%20study%2C%20and%20the%0Ageographical%20spread%20of%20scientific%20research%20on%20generative%20AI.%20We%20also%0Ainvestigate%20team%20size%20and%20international%20collaborations%20to%20explore%20whether%0AGenAI%2C%20as%20an%20emerging%20scientific%20research%20area%2C%20shows%20different%20collaboration%0Apatterns%20compared%20to%20other%20AI%20technologies.%20The%20results%20indicate%20that%0Agenerative%20AI%20has%20experienced%20rapid%20growth%20and%20increasing%20presence%20in%0Ascientific%20publications.%20The%20use%20of%20GenAI%20now%20extends%20beyond%20computer%20science%0Ato%20other%20scientific%20research%20domains.%20Over%20the%20study%20period%2C%20U.S.%20researchers%0Acontributed%20nearly%20two-fifths%20of%20global%20GenAI%20publications.%20The%20U.S.%20is%0Afollowed%20by%20China%2C%20with%20several%20small%20and%20medium-sized%20advanced%20economies%0Ademonstrating%20relatively%20high%20levels%20of%20GenAI%20deployment%20in%20their%20research%0Apublications.%20Although%20scientific%20research%20overall%20is%20becoming%20increasingly%0Aspecialized%20and%20collaborative%2C%20our%20results%20suggest%20that%20GenAI%20research%20groups%0Atend%20to%20have%20slightly%20smaller%20team%20sizes%20than%20found%20in%20other%20AI%20fields.%0AFurthermore%2C%20notwithstanding%20recent%20geopolitical%20tensions%2C%20GenAI%20research%0Acontinues%20to%20exhibit%20levels%20of%20international%20collaboration%20comparable%20to%20other%0AAI%20technologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20960v1&entry.124074799=Read"},
{"title": "Order Matters in Hallucination: Reasoning Order as Benchmark and\n  Reflexive Prompting for Large-Language-Models", "author": "Zikai Xie", "abstract": "  Large language models (LLMs) have generated significant attention since their\ninception, finding applications across various academic and industrial domains.\nHowever, these models often suffer from the \"hallucination problem\", where\noutputs, though grammatically and logically coherent, lack factual accuracy or\nare entirely fabricated. A particularly troubling issue discovered and widely\ndiscussed recently is the numerical comparison error where multiple LLMs\nincorrectly infer that \"9.11$>$9.9\". We discovered that the order in which LLMs\ngenerate answers and reasoning impacts their consistency. Specifically, results\nvary significantly when an LLM generates an answer first and then provides the\nreasoning versus generating the reasoning process first and then the\nconclusion. Inspired by this, we propose a new benchmark method for assessing\nLLM consistency: comparing responses generated through these two different\napproaches. This benchmark effectively identifies instances where LLMs\nfabricate answers and subsequently generate justifications. Furthermore, we\nintroduce a novel and straightforward prompt strategy designed to mitigate this\nissue. Experimental results demonstrate that this strategy improves performance\nacross various LLMs compared to direct questioning. This work not only sheds\nlight on a critical flaw in LLMs but also offers a practical solution to\nenhance their reliability.\n", "link": "http://arxiv.org/abs/2408.05093v3", "date": "2024-12-30", "relevancy": 2.4387, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4957}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4957}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Order%20Matters%20in%20Hallucination%3A%20Reasoning%20Order%20as%20Benchmark%20and%0A%20%20Reflexive%20Prompting%20for%20Large-Language-Models&body=Title%3A%20Order%20Matters%20in%20Hallucination%3A%20Reasoning%20Order%20as%20Benchmark%20and%0A%20%20Reflexive%20Prompting%20for%20Large-Language-Models%0AAuthor%3A%20Zikai%20Xie%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20generated%20significant%20attention%20since%20their%0Ainception%2C%20finding%20applications%20across%20various%20academic%20and%20industrial%20domains.%0AHowever%2C%20these%20models%20often%20suffer%20from%20the%20%22hallucination%20problem%22%2C%20where%0Aoutputs%2C%20though%20grammatically%20and%20logically%20coherent%2C%20lack%20factual%20accuracy%20or%0Aare%20entirely%20fabricated.%20A%20particularly%20troubling%20issue%20discovered%20and%20widely%0Adiscussed%20recently%20is%20the%20numerical%20comparison%20error%20where%20multiple%20LLMs%0Aincorrectly%20infer%20that%20%229.11%24%3E%249.9%22.%20We%20discovered%20that%20the%20order%20in%20which%20LLMs%0Agenerate%20answers%20and%20reasoning%20impacts%20their%20consistency.%20Specifically%2C%20results%0Avary%20significantly%20when%20an%20LLM%20generates%20an%20answer%20first%20and%20then%20provides%20the%0Areasoning%20versus%20generating%20the%20reasoning%20process%20first%20and%20then%20the%0Aconclusion.%20Inspired%20by%20this%2C%20we%20propose%20a%20new%20benchmark%20method%20for%20assessing%0ALLM%20consistency%3A%20comparing%20responses%20generated%20through%20these%20two%20different%0Aapproaches.%20This%20benchmark%20effectively%20identifies%20instances%20where%20LLMs%0Afabricate%20answers%20and%20subsequently%20generate%20justifications.%20Furthermore%2C%20we%0Aintroduce%20a%20novel%20and%20straightforward%20prompt%20strategy%20designed%20to%20mitigate%20this%0Aissue.%20Experimental%20results%20demonstrate%20that%20this%20strategy%20improves%20performance%0Aacross%20various%20LLMs%20compared%20to%20direct%20questioning.%20This%20work%20not%20only%20sheds%0Alight%20on%20a%20critical%20flaw%20in%20LLMs%20but%20also%20offers%20a%20practical%20solution%20to%0Aenhance%20their%20reliability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05093v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOrder%2520Matters%2520in%2520Hallucination%253A%2520Reasoning%2520Order%2520as%2520Benchmark%2520and%250A%2520%2520Reflexive%2520Prompting%2520for%2520Large-Language-Models%26entry.906535625%3DZikai%2520Xie%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520generated%2520significant%2520attention%2520since%2520their%250Ainception%252C%2520finding%2520applications%2520across%2520various%2520academic%2520and%2520industrial%2520domains.%250AHowever%252C%2520these%2520models%2520often%2520suffer%2520from%2520the%2520%2522hallucination%2520problem%2522%252C%2520where%250Aoutputs%252C%2520though%2520grammatically%2520and%2520logically%2520coherent%252C%2520lack%2520factual%2520accuracy%2520or%250Aare%2520entirely%2520fabricated.%2520A%2520particularly%2520troubling%2520issue%2520discovered%2520and%2520widely%250Adiscussed%2520recently%2520is%2520the%2520numerical%2520comparison%2520error%2520where%2520multiple%2520LLMs%250Aincorrectly%2520infer%2520that%2520%25229.11%2524%253E%25249.9%2522.%2520We%2520discovered%2520that%2520the%2520order%2520in%2520which%2520LLMs%250Agenerate%2520answers%2520and%2520reasoning%2520impacts%2520their%2520consistency.%2520Specifically%252C%2520results%250Avary%2520significantly%2520when%2520an%2520LLM%2520generates%2520an%2520answer%2520first%2520and%2520then%2520provides%2520the%250Areasoning%2520versus%2520generating%2520the%2520reasoning%2520process%2520first%2520and%2520then%2520the%250Aconclusion.%2520Inspired%2520by%2520this%252C%2520we%2520propose%2520a%2520new%2520benchmark%2520method%2520for%2520assessing%250ALLM%2520consistency%253A%2520comparing%2520responses%2520generated%2520through%2520these%2520two%2520different%250Aapproaches.%2520This%2520benchmark%2520effectively%2520identifies%2520instances%2520where%2520LLMs%250Afabricate%2520answers%2520and%2520subsequently%2520generate%2520justifications.%2520Furthermore%252C%2520we%250Aintroduce%2520a%2520novel%2520and%2520straightforward%2520prompt%2520strategy%2520designed%2520to%2520mitigate%2520this%250Aissue.%2520Experimental%2520results%2520demonstrate%2520that%2520this%2520strategy%2520improves%2520performance%250Aacross%2520various%2520LLMs%2520compared%2520to%2520direct%2520questioning.%2520This%2520work%2520not%2520only%2520sheds%250Alight%2520on%2520a%2520critical%2520flaw%2520in%2520LLMs%2520but%2520also%2520offers%2520a%2520practical%2520solution%2520to%250Aenhance%2520their%2520reliability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05093v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Order%20Matters%20in%20Hallucination%3A%20Reasoning%20Order%20as%20Benchmark%20and%0A%20%20Reflexive%20Prompting%20for%20Large-Language-Models&entry.906535625=Zikai%20Xie&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20generated%20significant%20attention%20since%20their%0Ainception%2C%20finding%20applications%20across%20various%20academic%20and%20industrial%20domains.%0AHowever%2C%20these%20models%20often%20suffer%20from%20the%20%22hallucination%20problem%22%2C%20where%0Aoutputs%2C%20though%20grammatically%20and%20logically%20coherent%2C%20lack%20factual%20accuracy%20or%0Aare%20entirely%20fabricated.%20A%20particularly%20troubling%20issue%20discovered%20and%20widely%0Adiscussed%20recently%20is%20the%20numerical%20comparison%20error%20where%20multiple%20LLMs%0Aincorrectly%20infer%20that%20%229.11%24%3E%249.9%22.%20We%20discovered%20that%20the%20order%20in%20which%20LLMs%0Agenerate%20answers%20and%20reasoning%20impacts%20their%20consistency.%20Specifically%2C%20results%0Avary%20significantly%20when%20an%20LLM%20generates%20an%20answer%20first%20and%20then%20provides%20the%0Areasoning%20versus%20generating%20the%20reasoning%20process%20first%20and%20then%20the%0Aconclusion.%20Inspired%20by%20this%2C%20we%20propose%20a%20new%20benchmark%20method%20for%20assessing%0ALLM%20consistency%3A%20comparing%20responses%20generated%20through%20these%20two%20different%0Aapproaches.%20This%20benchmark%20effectively%20identifies%20instances%20where%20LLMs%0Afabricate%20answers%20and%20subsequently%20generate%20justifications.%20Furthermore%2C%20we%0Aintroduce%20a%20novel%20and%20straightforward%20prompt%20strategy%20designed%20to%20mitigate%20this%0Aissue.%20Experimental%20results%20demonstrate%20that%20this%20strategy%20improves%20performance%0Aacross%20various%20LLMs%20compared%20to%20direct%20questioning.%20This%20work%20not%20only%20sheds%0Alight%20on%20a%20critical%20flaw%20in%20LLMs%20but%20also%20offers%20a%20practical%20solution%20to%0Aenhance%20their%20reliability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05093v3&entry.124074799=Read"},
{"title": "A Simple Recipe for Contrastively Pre-training Video-First Encoders\n  Beyond 16 Frames", "author": "Pinelopi Papalampidi and Skanda Koppula and Shreya Pathak and Justin Chiu and Joe Heyward and Viorica Patraucean and Jiajun Shen and Antoine Miech and Andrew Zisserman and Aida Nematzadeh", "abstract": "  Understanding long, real-world videos requires modeling of long-range visual\ndependencies. To this end, we explore video-first architectures, building on\nthe common paradigm of transferring large-scale, image--text models to video\nvia shallow temporal fusion. However, we expose two limitations to the\napproach: (1) decreased spatial capabilities, likely due to poor\nvideo--language alignment in standard video datasets, and (2) higher memory\nconsumption, bottlenecking the number of frames that can be processed. To\nmitigate the memory bottleneck, we systematically analyze the memory/accuracy\ntrade-off of various efficient methods: factorized attention,\nparameter-efficient image-to-video adaptation, input masking, and\nmulti-resolution patchification. Surprisingly, simply masking large portions of\nthe video (up to 75%) during contrastive pre-training proves to be one of the\nmost robust ways to scale encoders to videos up to 4.3 minutes at 1 FPS. Our\nsimple approach for training long video-to-text models, which scales to 1B\nparameters, does not add new architectural complexity and is able to outperform\nthe popular paradigm of using much larger LLMs as an information aggregator\nover segment-based information on benchmarks with long-range temporal\ndependencies (YouCook2, EgoSchema).\n", "link": "http://arxiv.org/abs/2312.07395v2", "date": "2024-12-30", "relevancy": 2.434, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6095}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6095}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6035}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Simple%20Recipe%20for%20Contrastively%20Pre-training%20Video-First%20Encoders%0A%20%20Beyond%2016%20Frames&body=Title%3A%20A%20Simple%20Recipe%20for%20Contrastively%20Pre-training%20Video-First%20Encoders%0A%20%20Beyond%2016%20Frames%0AAuthor%3A%20Pinelopi%20Papalampidi%20and%20Skanda%20Koppula%20and%20Shreya%20Pathak%20and%20Justin%20Chiu%20and%20Joe%20Heyward%20and%20Viorica%20Patraucean%20and%20Jiajun%20Shen%20and%20Antoine%20Miech%20and%20Andrew%20Zisserman%20and%20Aida%20Nematzadeh%0AAbstract%3A%20%20%20Understanding%20long%2C%20real-world%20videos%20requires%20modeling%20of%20long-range%20visual%0Adependencies.%20To%20this%20end%2C%20we%20explore%20video-first%20architectures%2C%20building%20on%0Athe%20common%20paradigm%20of%20transferring%20large-scale%2C%20image--text%20models%20to%20video%0Avia%20shallow%20temporal%20fusion.%20However%2C%20we%20expose%20two%20limitations%20to%20the%0Aapproach%3A%20%281%29%20decreased%20spatial%20capabilities%2C%20likely%20due%20to%20poor%0Avideo--language%20alignment%20in%20standard%20video%20datasets%2C%20and%20%282%29%20higher%20memory%0Aconsumption%2C%20bottlenecking%20the%20number%20of%20frames%20that%20can%20be%20processed.%20To%0Amitigate%20the%20memory%20bottleneck%2C%20we%20systematically%20analyze%20the%20memory/accuracy%0Atrade-off%20of%20various%20efficient%20methods%3A%20factorized%20attention%2C%0Aparameter-efficient%20image-to-video%20adaptation%2C%20input%20masking%2C%20and%0Amulti-resolution%20patchification.%20Surprisingly%2C%20simply%20masking%20large%20portions%20of%0Athe%20video%20%28up%20to%2075%25%29%20during%20contrastive%20pre-training%20proves%20to%20be%20one%20of%20the%0Amost%20robust%20ways%20to%20scale%20encoders%20to%20videos%20up%20to%204.3%20minutes%20at%201%20FPS.%20Our%0Asimple%20approach%20for%20training%20long%20video-to-text%20models%2C%20which%20scales%20to%201B%0Aparameters%2C%20does%20not%20add%20new%20architectural%20complexity%20and%20is%20able%20to%20outperform%0Athe%20popular%20paradigm%20of%20using%20much%20larger%20LLMs%20as%20an%20information%20aggregator%0Aover%20segment-based%20information%20on%20benchmarks%20with%20long-range%20temporal%0Adependencies%20%28YouCook2%2C%20EgoSchema%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.07395v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Simple%2520Recipe%2520for%2520Contrastively%2520Pre-training%2520Video-First%2520Encoders%250A%2520%2520Beyond%252016%2520Frames%26entry.906535625%3DPinelopi%2520Papalampidi%2520and%2520Skanda%2520Koppula%2520and%2520Shreya%2520Pathak%2520and%2520Justin%2520Chiu%2520and%2520Joe%2520Heyward%2520and%2520Viorica%2520Patraucean%2520and%2520Jiajun%2520Shen%2520and%2520Antoine%2520Miech%2520and%2520Andrew%2520Zisserman%2520and%2520Aida%2520Nematzadeh%26entry.1292438233%3D%2520%2520Understanding%2520long%252C%2520real-world%2520videos%2520requires%2520modeling%2520of%2520long-range%2520visual%250Adependencies.%2520To%2520this%2520end%252C%2520we%2520explore%2520video-first%2520architectures%252C%2520building%2520on%250Athe%2520common%2520paradigm%2520of%2520transferring%2520large-scale%252C%2520image--text%2520models%2520to%2520video%250Avia%2520shallow%2520temporal%2520fusion.%2520However%252C%2520we%2520expose%2520two%2520limitations%2520to%2520the%250Aapproach%253A%2520%25281%2529%2520decreased%2520spatial%2520capabilities%252C%2520likely%2520due%2520to%2520poor%250Avideo--language%2520alignment%2520in%2520standard%2520video%2520datasets%252C%2520and%2520%25282%2529%2520higher%2520memory%250Aconsumption%252C%2520bottlenecking%2520the%2520number%2520of%2520frames%2520that%2520can%2520be%2520processed.%2520To%250Amitigate%2520the%2520memory%2520bottleneck%252C%2520we%2520systematically%2520analyze%2520the%2520memory/accuracy%250Atrade-off%2520of%2520various%2520efficient%2520methods%253A%2520factorized%2520attention%252C%250Aparameter-efficient%2520image-to-video%2520adaptation%252C%2520input%2520masking%252C%2520and%250Amulti-resolution%2520patchification.%2520Surprisingly%252C%2520simply%2520masking%2520large%2520portions%2520of%250Athe%2520video%2520%2528up%2520to%252075%2525%2529%2520during%2520contrastive%2520pre-training%2520proves%2520to%2520be%2520one%2520of%2520the%250Amost%2520robust%2520ways%2520to%2520scale%2520encoders%2520to%2520videos%2520up%2520to%25204.3%2520minutes%2520at%25201%2520FPS.%2520Our%250Asimple%2520approach%2520for%2520training%2520long%2520video-to-text%2520models%252C%2520which%2520scales%2520to%25201B%250Aparameters%252C%2520does%2520not%2520add%2520new%2520architectural%2520complexity%2520and%2520is%2520able%2520to%2520outperform%250Athe%2520popular%2520paradigm%2520of%2520using%2520much%2520larger%2520LLMs%2520as%2520an%2520information%2520aggregator%250Aover%2520segment-based%2520information%2520on%2520benchmarks%2520with%2520long-range%2520temporal%250Adependencies%2520%2528YouCook2%252C%2520EgoSchema%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.07395v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Simple%20Recipe%20for%20Contrastively%20Pre-training%20Video-First%20Encoders%0A%20%20Beyond%2016%20Frames&entry.906535625=Pinelopi%20Papalampidi%20and%20Skanda%20Koppula%20and%20Shreya%20Pathak%20and%20Justin%20Chiu%20and%20Joe%20Heyward%20and%20Viorica%20Patraucean%20and%20Jiajun%20Shen%20and%20Antoine%20Miech%20and%20Andrew%20Zisserman%20and%20Aida%20Nematzadeh&entry.1292438233=%20%20Understanding%20long%2C%20real-world%20videos%20requires%20modeling%20of%20long-range%20visual%0Adependencies.%20To%20this%20end%2C%20we%20explore%20video-first%20architectures%2C%20building%20on%0Athe%20common%20paradigm%20of%20transferring%20large-scale%2C%20image--text%20models%20to%20video%0Avia%20shallow%20temporal%20fusion.%20However%2C%20we%20expose%20two%20limitations%20to%20the%0Aapproach%3A%20%281%29%20decreased%20spatial%20capabilities%2C%20likely%20due%20to%20poor%0Avideo--language%20alignment%20in%20standard%20video%20datasets%2C%20and%20%282%29%20higher%20memory%0Aconsumption%2C%20bottlenecking%20the%20number%20of%20frames%20that%20can%20be%20processed.%20To%0Amitigate%20the%20memory%20bottleneck%2C%20we%20systematically%20analyze%20the%20memory/accuracy%0Atrade-off%20of%20various%20efficient%20methods%3A%20factorized%20attention%2C%0Aparameter-efficient%20image-to-video%20adaptation%2C%20input%20masking%2C%20and%0Amulti-resolution%20patchification.%20Surprisingly%2C%20simply%20masking%20large%20portions%20of%0Athe%20video%20%28up%20to%2075%25%29%20during%20contrastive%20pre-training%20proves%20to%20be%20one%20of%20the%0Amost%20robust%20ways%20to%20scale%20encoders%20to%20videos%20up%20to%204.3%20minutes%20at%201%20FPS.%20Our%0Asimple%20approach%20for%20training%20long%20video-to-text%20models%2C%20which%20scales%20to%201B%0Aparameters%2C%20does%20not%20add%20new%20architectural%20complexity%20and%20is%20able%20to%20outperform%0Athe%20popular%20paradigm%20of%20using%20much%20larger%20LLMs%20as%20an%20information%20aggregator%0Aover%20segment-based%20information%20on%20benchmarks%20with%20long-range%20temporal%0Adependencies%20%28YouCook2%2C%20EgoSchema%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.07395v2&entry.124074799=Read"},
{"title": "CNNtention: Can CNNs do better with Attention?", "author": "Nikhil Kapila and Julian Glattki and Tejas Rathi", "abstract": "  Convolutional Neural Networks (CNNs) have been the standard for image\nclassification tasks for a long time, but more recently attention-based\nmechanisms have gained traction. This project aims to compare traditional CNNs\nwith attention-augmented CNNs across an image classification task. By\nevaluating and comparing their performance, accuracy and computational\nefficiency, the project will highlight benefits and trade-off of the localized\nfeature extraction of traditional CNNs and the global context capture in\nattention-augmented CNNs. By doing this, we can reveal further insights into\ntheir respective strengths and weaknesses, guide the selection of models based\non specific application needs and ultimately, enhance understanding of these\narchitectures in the deep learning community.\n  This was our final project for CS7643 Deep Learning course at Georgia Tech.\n", "link": "http://arxiv.org/abs/2412.11657v3", "date": "2024-12-30", "relevancy": 2.4316, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5249}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4727}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CNNtention%3A%20Can%20CNNs%20do%20better%20with%20Attention%3F&body=Title%3A%20CNNtention%3A%20Can%20CNNs%20do%20better%20with%20Attention%3F%0AAuthor%3A%20Nikhil%20Kapila%20and%20Julian%20Glattki%20and%20Tejas%20Rathi%0AAbstract%3A%20%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20been%20the%20standard%20for%20image%0Aclassification%20tasks%20for%20a%20long%20time%2C%20but%20more%20recently%20attention-based%0Amechanisms%20have%20gained%20traction.%20This%20project%20aims%20to%20compare%20traditional%20CNNs%0Awith%20attention-augmented%20CNNs%20across%20an%20image%20classification%20task.%20By%0Aevaluating%20and%20comparing%20their%20performance%2C%20accuracy%20and%20computational%0Aefficiency%2C%20the%20project%20will%20highlight%20benefits%20and%20trade-off%20of%20the%20localized%0Afeature%20extraction%20of%20traditional%20CNNs%20and%20the%20global%20context%20capture%20in%0Aattention-augmented%20CNNs.%20By%20doing%20this%2C%20we%20can%20reveal%20further%20insights%20into%0Atheir%20respective%20strengths%20and%20weaknesses%2C%20guide%20the%20selection%20of%20models%20based%0Aon%20specific%20application%20needs%20and%20ultimately%2C%20enhance%20understanding%20of%20these%0Aarchitectures%20in%20the%20deep%20learning%20community.%0A%20%20This%20was%20our%20final%20project%20for%20CS7643%20Deep%20Learning%20course%20at%20Georgia%20Tech.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.11657v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCNNtention%253A%2520Can%2520CNNs%2520do%2520better%2520with%2520Attention%253F%26entry.906535625%3DNikhil%2520Kapila%2520and%2520Julian%2520Glattki%2520and%2520Tejas%2520Rathi%26entry.1292438233%3D%2520%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520have%2520been%2520the%2520standard%2520for%2520image%250Aclassification%2520tasks%2520for%2520a%2520long%2520time%252C%2520but%2520more%2520recently%2520attention-based%250Amechanisms%2520have%2520gained%2520traction.%2520This%2520project%2520aims%2520to%2520compare%2520traditional%2520CNNs%250Awith%2520attention-augmented%2520CNNs%2520across%2520an%2520image%2520classification%2520task.%2520By%250Aevaluating%2520and%2520comparing%2520their%2520performance%252C%2520accuracy%2520and%2520computational%250Aefficiency%252C%2520the%2520project%2520will%2520highlight%2520benefits%2520and%2520trade-off%2520of%2520the%2520localized%250Afeature%2520extraction%2520of%2520traditional%2520CNNs%2520and%2520the%2520global%2520context%2520capture%2520in%250Aattention-augmented%2520CNNs.%2520By%2520doing%2520this%252C%2520we%2520can%2520reveal%2520further%2520insights%2520into%250Atheir%2520respective%2520strengths%2520and%2520weaknesses%252C%2520guide%2520the%2520selection%2520of%2520models%2520based%250Aon%2520specific%2520application%2520needs%2520and%2520ultimately%252C%2520enhance%2520understanding%2520of%2520these%250Aarchitectures%2520in%2520the%2520deep%2520learning%2520community.%250A%2520%2520This%2520was%2520our%2520final%2520project%2520for%2520CS7643%2520Deep%2520Learning%2520course%2520at%2520Georgia%2520Tech.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11657v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CNNtention%3A%20Can%20CNNs%20do%20better%20with%20Attention%3F&entry.906535625=Nikhil%20Kapila%20and%20Julian%20Glattki%20and%20Tejas%20Rathi&entry.1292438233=%20%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20been%20the%20standard%20for%20image%0Aclassification%20tasks%20for%20a%20long%20time%2C%20but%20more%20recently%20attention-based%0Amechanisms%20have%20gained%20traction.%20This%20project%20aims%20to%20compare%20traditional%20CNNs%0Awith%20attention-augmented%20CNNs%20across%20an%20image%20classification%20task.%20By%0Aevaluating%20and%20comparing%20their%20performance%2C%20accuracy%20and%20computational%0Aefficiency%2C%20the%20project%20will%20highlight%20benefits%20and%20trade-off%20of%20the%20localized%0Afeature%20extraction%20of%20traditional%20CNNs%20and%20the%20global%20context%20capture%20in%0Aattention-augmented%20CNNs.%20By%20doing%20this%2C%20we%20can%20reveal%20further%20insights%20into%0Atheir%20respective%20strengths%20and%20weaknesses%2C%20guide%20the%20selection%20of%20models%20based%0Aon%20specific%20application%20needs%20and%20ultimately%2C%20enhance%20understanding%20of%20these%0Aarchitectures%20in%20the%20deep%20learning%20community.%0A%20%20This%20was%20our%20final%20project%20for%20CS7643%20Deep%20Learning%20course%20at%20Georgia%20Tech.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.11657v3&entry.124074799=Read"},
{"title": "TiGDistill-BEV: Multi-view BEV 3D Object Detection via Target\n  Inner-Geometry Learning Distillation", "author": "Shaoqing Xu and Fang Li and Peixiang Huang and Ziying Song and Zhi-Xin Yang", "abstract": "  Accurate multi-view 3D object detection is essential for applications such as\nautonomous driving. Researchers have consistently aimed to leverage LiDAR's\nprecise spatial information to enhance camera-based detectors through methods\nlike depth supervision and bird-eye-view (BEV) feature distillation. However,\nexisting approaches often face challenges due to the inherent differences\nbetween LiDAR and camera data representations. In this paper, we introduce the\nTiGDistill-BEV, a novel approach that effectively bridges this gap by\nleveraging the strengths of both sensors. Our method distills knowledge from\ndiverse modalities(e.g., LiDAR) as the teacher model to a camera-based student\ndetector, utilizing the Target Inner-Geometry learning scheme to enhance\ncamera-based BEV detectors through both depth and BEV features by leveraging\ndiverse modalities. Specially, we propose two key modules: an inner-depth\nsupervision module to learn the low-level relative depth relations within\nobjects which equips detectors with a deeper understanding of object-level\nspatial structures, and an inner-feature BEV distillation module to transfer\nhigh-level semantics of different key points within foreground targets. To\nfurther alleviate the domain gap, we incorporate both inter-channel and\ninter-keypoint distillation to model feature similarity. Extensive experiments\non the nuScenes benchmark demonstrate that TiGDistill-BEV significantly boosts\ncamera-based only detectors achieving a state-of-the-art with 62.8% NDS and\nsurpassing previous methods by a significant margin. The codes is available at:\nhttps://github.com/Public-BOTs/TiGDistill-BEV.git.\n", "link": "http://arxiv.org/abs/2412.20911v1", "date": "2024-12-30", "relevancy": 2.4124, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.613}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6054}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TiGDistill-BEV%3A%20Multi-view%20BEV%203D%20Object%20Detection%20via%20Target%0A%20%20Inner-Geometry%20Learning%20Distillation&body=Title%3A%20TiGDistill-BEV%3A%20Multi-view%20BEV%203D%20Object%20Detection%20via%20Target%0A%20%20Inner-Geometry%20Learning%20Distillation%0AAuthor%3A%20Shaoqing%20Xu%20and%20Fang%20Li%20and%20Peixiang%20Huang%20and%20Ziying%20Song%20and%20Zhi-Xin%20Yang%0AAbstract%3A%20%20%20Accurate%20multi-view%203D%20object%20detection%20is%20essential%20for%20applications%20such%20as%0Aautonomous%20driving.%20Researchers%20have%20consistently%20aimed%20to%20leverage%20LiDAR%27s%0Aprecise%20spatial%20information%20to%20enhance%20camera-based%20detectors%20through%20methods%0Alike%20depth%20supervision%20and%20bird-eye-view%20%28BEV%29%20feature%20distillation.%20However%2C%0Aexisting%20approaches%20often%20face%20challenges%20due%20to%20the%20inherent%20differences%0Abetween%20LiDAR%20and%20camera%20data%20representations.%20In%20this%20paper%2C%20we%20introduce%20the%0ATiGDistill-BEV%2C%20a%20novel%20approach%20that%20effectively%20bridges%20this%20gap%20by%0Aleveraging%20the%20strengths%20of%20both%20sensors.%20Our%20method%20distills%20knowledge%20from%0Adiverse%20modalities%28e.g.%2C%20LiDAR%29%20as%20the%20teacher%20model%20to%20a%20camera-based%20student%0Adetector%2C%20utilizing%20the%20Target%20Inner-Geometry%20learning%20scheme%20to%20enhance%0Acamera-based%20BEV%20detectors%20through%20both%20depth%20and%20BEV%20features%20by%20leveraging%0Adiverse%20modalities.%20Specially%2C%20we%20propose%20two%20key%20modules%3A%20an%20inner-depth%0Asupervision%20module%20to%20learn%20the%20low-level%20relative%20depth%20relations%20within%0Aobjects%20which%20equips%20detectors%20with%20a%20deeper%20understanding%20of%20object-level%0Aspatial%20structures%2C%20and%20an%20inner-feature%20BEV%20distillation%20module%20to%20transfer%0Ahigh-level%20semantics%20of%20different%20key%20points%20within%20foreground%20targets.%20To%0Afurther%20alleviate%20the%20domain%20gap%2C%20we%20incorporate%20both%20inter-channel%20and%0Ainter-keypoint%20distillation%20to%20model%20feature%20similarity.%20Extensive%20experiments%0Aon%20the%20nuScenes%20benchmark%20demonstrate%20that%20TiGDistill-BEV%20significantly%20boosts%0Acamera-based%20only%20detectors%20achieving%20a%20state-of-the-art%20with%2062.8%25%20NDS%20and%0Asurpassing%20previous%20methods%20by%20a%20significant%20margin.%20The%20codes%20is%20available%20at%3A%0Ahttps%3A//github.com/Public-BOTs/TiGDistill-BEV.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20911v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTiGDistill-BEV%253A%2520Multi-view%2520BEV%25203D%2520Object%2520Detection%2520via%2520Target%250A%2520%2520Inner-Geometry%2520Learning%2520Distillation%26entry.906535625%3DShaoqing%2520Xu%2520and%2520Fang%2520Li%2520and%2520Peixiang%2520Huang%2520and%2520Ziying%2520Song%2520and%2520Zhi-Xin%2520Yang%26entry.1292438233%3D%2520%2520Accurate%2520multi-view%25203D%2520object%2520detection%2520is%2520essential%2520for%2520applications%2520such%2520as%250Aautonomous%2520driving.%2520Researchers%2520have%2520consistently%2520aimed%2520to%2520leverage%2520LiDAR%2527s%250Aprecise%2520spatial%2520information%2520to%2520enhance%2520camera-based%2520detectors%2520through%2520methods%250Alike%2520depth%2520supervision%2520and%2520bird-eye-view%2520%2528BEV%2529%2520feature%2520distillation.%2520However%252C%250Aexisting%2520approaches%2520often%2520face%2520challenges%2520due%2520to%2520the%2520inherent%2520differences%250Abetween%2520LiDAR%2520and%2520camera%2520data%2520representations.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%250ATiGDistill-BEV%252C%2520a%2520novel%2520approach%2520that%2520effectively%2520bridges%2520this%2520gap%2520by%250Aleveraging%2520the%2520strengths%2520of%2520both%2520sensors.%2520Our%2520method%2520distills%2520knowledge%2520from%250Adiverse%2520modalities%2528e.g.%252C%2520LiDAR%2529%2520as%2520the%2520teacher%2520model%2520to%2520a%2520camera-based%2520student%250Adetector%252C%2520utilizing%2520the%2520Target%2520Inner-Geometry%2520learning%2520scheme%2520to%2520enhance%250Acamera-based%2520BEV%2520detectors%2520through%2520both%2520depth%2520and%2520BEV%2520features%2520by%2520leveraging%250Adiverse%2520modalities.%2520Specially%252C%2520we%2520propose%2520two%2520key%2520modules%253A%2520an%2520inner-depth%250Asupervision%2520module%2520to%2520learn%2520the%2520low-level%2520relative%2520depth%2520relations%2520within%250Aobjects%2520which%2520equips%2520detectors%2520with%2520a%2520deeper%2520understanding%2520of%2520object-level%250Aspatial%2520structures%252C%2520and%2520an%2520inner-feature%2520BEV%2520distillation%2520module%2520to%2520transfer%250Ahigh-level%2520semantics%2520of%2520different%2520key%2520points%2520within%2520foreground%2520targets.%2520To%250Afurther%2520alleviate%2520the%2520domain%2520gap%252C%2520we%2520incorporate%2520both%2520inter-channel%2520and%250Ainter-keypoint%2520distillation%2520to%2520model%2520feature%2520similarity.%2520Extensive%2520experiments%250Aon%2520the%2520nuScenes%2520benchmark%2520demonstrate%2520that%2520TiGDistill-BEV%2520significantly%2520boosts%250Acamera-based%2520only%2520detectors%2520achieving%2520a%2520state-of-the-art%2520with%252062.8%2525%2520NDS%2520and%250Asurpassing%2520previous%2520methods%2520by%2520a%2520significant%2520margin.%2520The%2520codes%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/Public-BOTs/TiGDistill-BEV.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20911v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TiGDistill-BEV%3A%20Multi-view%20BEV%203D%20Object%20Detection%20via%20Target%0A%20%20Inner-Geometry%20Learning%20Distillation&entry.906535625=Shaoqing%20Xu%20and%20Fang%20Li%20and%20Peixiang%20Huang%20and%20Ziying%20Song%20and%20Zhi-Xin%20Yang&entry.1292438233=%20%20Accurate%20multi-view%203D%20object%20detection%20is%20essential%20for%20applications%20such%20as%0Aautonomous%20driving.%20Researchers%20have%20consistently%20aimed%20to%20leverage%20LiDAR%27s%0Aprecise%20spatial%20information%20to%20enhance%20camera-based%20detectors%20through%20methods%0Alike%20depth%20supervision%20and%20bird-eye-view%20%28BEV%29%20feature%20distillation.%20However%2C%0Aexisting%20approaches%20often%20face%20challenges%20due%20to%20the%20inherent%20differences%0Abetween%20LiDAR%20and%20camera%20data%20representations.%20In%20this%20paper%2C%20we%20introduce%20the%0ATiGDistill-BEV%2C%20a%20novel%20approach%20that%20effectively%20bridges%20this%20gap%20by%0Aleveraging%20the%20strengths%20of%20both%20sensors.%20Our%20method%20distills%20knowledge%20from%0Adiverse%20modalities%28e.g.%2C%20LiDAR%29%20as%20the%20teacher%20model%20to%20a%20camera-based%20student%0Adetector%2C%20utilizing%20the%20Target%20Inner-Geometry%20learning%20scheme%20to%20enhance%0Acamera-based%20BEV%20detectors%20through%20both%20depth%20and%20BEV%20features%20by%20leveraging%0Adiverse%20modalities.%20Specially%2C%20we%20propose%20two%20key%20modules%3A%20an%20inner-depth%0Asupervision%20module%20to%20learn%20the%20low-level%20relative%20depth%20relations%20within%0Aobjects%20which%20equips%20detectors%20with%20a%20deeper%20understanding%20of%20object-level%0Aspatial%20structures%2C%20and%20an%20inner-feature%20BEV%20distillation%20module%20to%20transfer%0Ahigh-level%20semantics%20of%20different%20key%20points%20within%20foreground%20targets.%20To%0Afurther%20alleviate%20the%20domain%20gap%2C%20we%20incorporate%20both%20inter-channel%20and%0Ainter-keypoint%20distillation%20to%20model%20feature%20similarity.%20Extensive%20experiments%0Aon%20the%20nuScenes%20benchmark%20demonstrate%20that%20TiGDistill-BEV%20significantly%20boosts%0Acamera-based%20only%20detectors%20achieving%20a%20state-of-the-art%20with%2062.8%25%20NDS%20and%0Asurpassing%20previous%20methods%20by%20a%20significant%20margin.%20The%20codes%20is%20available%20at%3A%0Ahttps%3A//github.com/Public-BOTs/TiGDistill-BEV.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20911v1&entry.124074799=Read"},
{"title": "Verbosity-Aware Rationale Reduction: Effective Reduction of Redundant\n  Rationale via Principled Criteria", "author": "Joonwon Jang and Jaehee Kim and Wonbin Kweon and Hwanjo Yu", "abstract": "  Large Language Models (LLMs) rely on generating extensive intermediate\nreasoning units (e.g., tokens, sentences) to enhance final answer quality\nacross a wide range of complex tasks. While generating multiple reasoning paths\nor iteratively refining rationales proves effective for improving performance,\nthese approaches inevitably result in significantly higher inference costs. In\nthis work, we propose a novel sentence-level rationale reduction training\nframework that leverages likelihood-based criteria, verbosity, to identify and\nremove redundant reasoning sentences. Unlike previous approaches that utilize\ntoken-level reduction, our sentence-level reduction framework maintains model\nperformance while reducing generation length. This preserves the original\nreasoning abilities of LLMs and achieves an average 17.15% reduction in\ngeneration costs across various models and tasks.\n", "link": "http://arxiv.org/abs/2412.21006v1", "date": "2024-12-30", "relevancy": 2.3791, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4865}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4865}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Verbosity-Aware%20Rationale%20Reduction%3A%20Effective%20Reduction%20of%20Redundant%0A%20%20Rationale%20via%20Principled%20Criteria&body=Title%3A%20Verbosity-Aware%20Rationale%20Reduction%3A%20Effective%20Reduction%20of%20Redundant%0A%20%20Rationale%20via%20Principled%20Criteria%0AAuthor%3A%20Joonwon%20Jang%20and%20Jaehee%20Kim%20and%20Wonbin%20Kweon%20and%20Hwanjo%20Yu%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20rely%20on%20generating%20extensive%20intermediate%0Areasoning%20units%20%28e.g.%2C%20tokens%2C%20sentences%29%20to%20enhance%20final%20answer%20quality%0Aacross%20a%20wide%20range%20of%20complex%20tasks.%20While%20generating%20multiple%20reasoning%20paths%0Aor%20iteratively%20refining%20rationales%20proves%20effective%20for%20improving%20performance%2C%0Athese%20approaches%20inevitably%20result%20in%20significantly%20higher%20inference%20costs.%20In%0Athis%20work%2C%20we%20propose%20a%20novel%20sentence-level%20rationale%20reduction%20training%0Aframework%20that%20leverages%20likelihood-based%20criteria%2C%20verbosity%2C%20to%20identify%20and%0Aremove%20redundant%20reasoning%20sentences.%20Unlike%20previous%20approaches%20that%20utilize%0Atoken-level%20reduction%2C%20our%20sentence-level%20reduction%20framework%20maintains%20model%0Aperformance%20while%20reducing%20generation%20length.%20This%20preserves%20the%20original%0Areasoning%20abilities%20of%20LLMs%20and%20achieves%20an%20average%2017.15%25%20reduction%20in%0Ageneration%20costs%20across%20various%20models%20and%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21006v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVerbosity-Aware%2520Rationale%2520Reduction%253A%2520Effective%2520Reduction%2520of%2520Redundant%250A%2520%2520Rationale%2520via%2520Principled%2520Criteria%26entry.906535625%3DJoonwon%2520Jang%2520and%2520Jaehee%2520Kim%2520and%2520Wonbin%2520Kweon%2520and%2520Hwanjo%2520Yu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520rely%2520on%2520generating%2520extensive%2520intermediate%250Areasoning%2520units%2520%2528e.g.%252C%2520tokens%252C%2520sentences%2529%2520to%2520enhance%2520final%2520answer%2520quality%250Aacross%2520a%2520wide%2520range%2520of%2520complex%2520tasks.%2520While%2520generating%2520multiple%2520reasoning%2520paths%250Aor%2520iteratively%2520refining%2520rationales%2520proves%2520effective%2520for%2520improving%2520performance%252C%250Athese%2520approaches%2520inevitably%2520result%2520in%2520significantly%2520higher%2520inference%2520costs.%2520In%250Athis%2520work%252C%2520we%2520propose%2520a%2520novel%2520sentence-level%2520rationale%2520reduction%2520training%250Aframework%2520that%2520leverages%2520likelihood-based%2520criteria%252C%2520verbosity%252C%2520to%2520identify%2520and%250Aremove%2520redundant%2520reasoning%2520sentences.%2520Unlike%2520previous%2520approaches%2520that%2520utilize%250Atoken-level%2520reduction%252C%2520our%2520sentence-level%2520reduction%2520framework%2520maintains%2520model%250Aperformance%2520while%2520reducing%2520generation%2520length.%2520This%2520preserves%2520the%2520original%250Areasoning%2520abilities%2520of%2520LLMs%2520and%2520achieves%2520an%2520average%252017.15%2525%2520reduction%2520in%250Ageneration%2520costs%2520across%2520various%2520models%2520and%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21006v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Verbosity-Aware%20Rationale%20Reduction%3A%20Effective%20Reduction%20of%20Redundant%0A%20%20Rationale%20via%20Principled%20Criteria&entry.906535625=Joonwon%20Jang%20and%20Jaehee%20Kim%20and%20Wonbin%20Kweon%20and%20Hwanjo%20Yu&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20rely%20on%20generating%20extensive%20intermediate%0Areasoning%20units%20%28e.g.%2C%20tokens%2C%20sentences%29%20to%20enhance%20final%20answer%20quality%0Aacross%20a%20wide%20range%20of%20complex%20tasks.%20While%20generating%20multiple%20reasoning%20paths%0Aor%20iteratively%20refining%20rationales%20proves%20effective%20for%20improving%20performance%2C%0Athese%20approaches%20inevitably%20result%20in%20significantly%20higher%20inference%20costs.%20In%0Athis%20work%2C%20we%20propose%20a%20novel%20sentence-level%20rationale%20reduction%20training%0Aframework%20that%20leverages%20likelihood-based%20criteria%2C%20verbosity%2C%20to%20identify%20and%0Aremove%20redundant%20reasoning%20sentences.%20Unlike%20previous%20approaches%20that%20utilize%0Atoken-level%20reduction%2C%20our%20sentence-level%20reduction%20framework%20maintains%20model%0Aperformance%20while%20reducing%20generation%20length.%20This%20preserves%20the%20original%0Areasoning%20abilities%20of%20LLMs%20and%20achieves%20an%20average%2017.15%25%20reduction%20in%0Ageneration%20costs%20across%20various%20models%20and%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21006v1&entry.124074799=Read"},
{"title": "Sparse Generation: Making Pseudo Labels Sparse for Point Weakly\n  Supervised Object Detection on Low Data Volume", "author": "Chuyang Shang and Tian Ma and Wanzhu Ren and Yuancheng Li and Jiayi Yang", "abstract": "  Existing pseudo label generation methods for point weakly supervised object\ndetection are inadequate in low data volume and dense object detection tasks.\nWe consider the generation of weakly supervised pseudo labels as the model's\nsparse output, and propose Sparse Generation as a solution to make pseudo\nlabels sparse. The method employs three processing stages (Mapping, Mask,\nRegression), constructs dense tensors through the relationship between data and\ndetector model, optimizes three of its parameters, and obtains a sparse tensor,\nthereby indirectly obtaining higher quality pseudo labels, and addresses the\nmodel's density problem on low data volume. Additionally, we propose\nperspective-based matching, which provides more rational pseudo boxes for\nprediction missed on instances. In comparison to the SOTA method, on four\ndatasets (MS COCO-val, RSOD, SIMD, Bullet-Hole), the experimental results\ndemonstrated a significant advantage.\n", "link": "http://arxiv.org/abs/2403.19306v3", "date": "2024-12-30", "relevancy": 2.3608, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5985}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5924}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse%20Generation%3A%20Making%20Pseudo%20Labels%20Sparse%20for%20Point%20Weakly%0A%20%20Supervised%20Object%20Detection%20on%20Low%20Data%20Volume&body=Title%3A%20Sparse%20Generation%3A%20Making%20Pseudo%20Labels%20Sparse%20for%20Point%20Weakly%0A%20%20Supervised%20Object%20Detection%20on%20Low%20Data%20Volume%0AAuthor%3A%20Chuyang%20Shang%20and%20Tian%20Ma%20and%20Wanzhu%20Ren%20and%20Yuancheng%20Li%20and%20Jiayi%20Yang%0AAbstract%3A%20%20%20Existing%20pseudo%20label%20generation%20methods%20for%20point%20weakly%20supervised%20object%0Adetection%20are%20inadequate%20in%20low%20data%20volume%20and%20dense%20object%20detection%20tasks.%0AWe%20consider%20the%20generation%20of%20weakly%20supervised%20pseudo%20labels%20as%20the%20model%27s%0Asparse%20output%2C%20and%20propose%20Sparse%20Generation%20as%20a%20solution%20to%20make%20pseudo%0Alabels%20sparse.%20The%20method%20employs%20three%20processing%20stages%20%28Mapping%2C%20Mask%2C%0ARegression%29%2C%20constructs%20dense%20tensors%20through%20the%20relationship%20between%20data%20and%0Adetector%20model%2C%20optimizes%20three%20of%20its%20parameters%2C%20and%20obtains%20a%20sparse%20tensor%2C%0Athereby%20indirectly%20obtaining%20higher%20quality%20pseudo%20labels%2C%20and%20addresses%20the%0Amodel%27s%20density%20problem%20on%20low%20data%20volume.%20Additionally%2C%20we%20propose%0Aperspective-based%20matching%2C%20which%20provides%20more%20rational%20pseudo%20boxes%20for%0Aprediction%20missed%20on%20instances.%20In%20comparison%20to%20the%20SOTA%20method%2C%20on%20four%0Adatasets%20%28MS%20COCO-val%2C%20RSOD%2C%20SIMD%2C%20Bullet-Hole%29%2C%20the%20experimental%20results%0Ademonstrated%20a%20significant%20advantage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19306v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse%2520Generation%253A%2520Making%2520Pseudo%2520Labels%2520Sparse%2520for%2520Point%2520Weakly%250A%2520%2520Supervised%2520Object%2520Detection%2520on%2520Low%2520Data%2520Volume%26entry.906535625%3DChuyang%2520Shang%2520and%2520Tian%2520Ma%2520and%2520Wanzhu%2520Ren%2520and%2520Yuancheng%2520Li%2520and%2520Jiayi%2520Yang%26entry.1292438233%3D%2520%2520Existing%2520pseudo%2520label%2520generation%2520methods%2520for%2520point%2520weakly%2520supervised%2520object%250Adetection%2520are%2520inadequate%2520in%2520low%2520data%2520volume%2520and%2520dense%2520object%2520detection%2520tasks.%250AWe%2520consider%2520the%2520generation%2520of%2520weakly%2520supervised%2520pseudo%2520labels%2520as%2520the%2520model%2527s%250Asparse%2520output%252C%2520and%2520propose%2520Sparse%2520Generation%2520as%2520a%2520solution%2520to%2520make%2520pseudo%250Alabels%2520sparse.%2520The%2520method%2520employs%2520three%2520processing%2520stages%2520%2528Mapping%252C%2520Mask%252C%250ARegression%2529%252C%2520constructs%2520dense%2520tensors%2520through%2520the%2520relationship%2520between%2520data%2520and%250Adetector%2520model%252C%2520optimizes%2520three%2520of%2520its%2520parameters%252C%2520and%2520obtains%2520a%2520sparse%2520tensor%252C%250Athereby%2520indirectly%2520obtaining%2520higher%2520quality%2520pseudo%2520labels%252C%2520and%2520addresses%2520the%250Amodel%2527s%2520density%2520problem%2520on%2520low%2520data%2520volume.%2520Additionally%252C%2520we%2520propose%250Aperspective-based%2520matching%252C%2520which%2520provides%2520more%2520rational%2520pseudo%2520boxes%2520for%250Aprediction%2520missed%2520on%2520instances.%2520In%2520comparison%2520to%2520the%2520SOTA%2520method%252C%2520on%2520four%250Adatasets%2520%2528MS%2520COCO-val%252C%2520RSOD%252C%2520SIMD%252C%2520Bullet-Hole%2529%252C%2520the%2520experimental%2520results%250Ademonstrated%2520a%2520significant%2520advantage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.19306v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20Generation%3A%20Making%20Pseudo%20Labels%20Sparse%20for%20Point%20Weakly%0A%20%20Supervised%20Object%20Detection%20on%20Low%20Data%20Volume&entry.906535625=Chuyang%20Shang%20and%20Tian%20Ma%20and%20Wanzhu%20Ren%20and%20Yuancheng%20Li%20and%20Jiayi%20Yang&entry.1292438233=%20%20Existing%20pseudo%20label%20generation%20methods%20for%20point%20weakly%20supervised%20object%0Adetection%20are%20inadequate%20in%20low%20data%20volume%20and%20dense%20object%20detection%20tasks.%0AWe%20consider%20the%20generation%20of%20weakly%20supervised%20pseudo%20labels%20as%20the%20model%27s%0Asparse%20output%2C%20and%20propose%20Sparse%20Generation%20as%20a%20solution%20to%20make%20pseudo%0Alabels%20sparse.%20The%20method%20employs%20three%20processing%20stages%20%28Mapping%2C%20Mask%2C%0ARegression%29%2C%20constructs%20dense%20tensors%20through%20the%20relationship%20between%20data%20and%0Adetector%20model%2C%20optimizes%20three%20of%20its%20parameters%2C%20and%20obtains%20a%20sparse%20tensor%2C%0Athereby%20indirectly%20obtaining%20higher%20quality%20pseudo%20labels%2C%20and%20addresses%20the%0Amodel%27s%20density%20problem%20on%20low%20data%20volume.%20Additionally%2C%20we%20propose%0Aperspective-based%20matching%2C%20which%20provides%20more%20rational%20pseudo%20boxes%20for%0Aprediction%20missed%20on%20instances.%20In%20comparison%20to%20the%20SOTA%20method%2C%20on%20four%0Adatasets%20%28MS%20COCO-val%2C%20RSOD%2C%20SIMD%2C%20Bullet-Hole%29%2C%20the%20experimental%20results%0Ademonstrated%20a%20significant%20advantage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19306v3&entry.124074799=Read"},
{"title": "Disentangling Preference Representation and Text Generation for\n  Efficient Individual Preference Alignment", "author": "Jianfei Zhang and Jun Bai and Bei Li and Yanmeng Wang and Rumei Li and Chenghua Lin and Wenge Rong", "abstract": "  Aligning Large Language Models (LLMs) with general human preferences has been\nproved crucial in improving the interaction quality between LLMs and human.\nHowever, human values are inherently diverse among different individuals,\nmaking it insufficient to align LLMs solely with general preferences. To\naddress this, personalizing LLMs according to individual feedback emerges as a\npromising solution. Nonetheless, this approach presents challenges in terms of\nthe efficiency of alignment algorithms. In this work, we introduce a flexible\nparadigm for individual preference alignment. Our method fundamentally improves\nefficiency by disentangling preference representation from text generation in\nLLMs. We validate our approach across multiple text generation tasks and\ndemonstrate that it can produce aligned quality as well as or better than\nPEFT-based methods, while reducing additional training time for each new\nindividual preference by $80\\%$ to $90\\%$ in comparison with them.\n", "link": "http://arxiv.org/abs/2412.20834v1", "date": "2024-12-30", "relevancy": 2.3187, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4752}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4588}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangling%20Preference%20Representation%20and%20Text%20Generation%20for%0A%20%20Efficient%20Individual%20Preference%20Alignment&body=Title%3A%20Disentangling%20Preference%20Representation%20and%20Text%20Generation%20for%0A%20%20Efficient%20Individual%20Preference%20Alignment%0AAuthor%3A%20Jianfei%20Zhang%20and%20Jun%20Bai%20and%20Bei%20Li%20and%20Yanmeng%20Wang%20and%20Rumei%20Li%20and%20Chenghua%20Lin%20and%20Wenge%20Rong%0AAbstract%3A%20%20%20Aligning%20Large%20Language%20Models%20%28LLMs%29%20with%20general%20human%20preferences%20has%20been%0Aproved%20crucial%20in%20improving%20the%20interaction%20quality%20between%20LLMs%20and%20human.%0AHowever%2C%20human%20values%20are%20inherently%20diverse%20among%20different%20individuals%2C%0Amaking%20it%20insufficient%20to%20align%20LLMs%20solely%20with%20general%20preferences.%20To%0Aaddress%20this%2C%20personalizing%20LLMs%20according%20to%20individual%20feedback%20emerges%20as%20a%0Apromising%20solution.%20Nonetheless%2C%20this%20approach%20presents%20challenges%20in%20terms%20of%0Athe%20efficiency%20of%20alignment%20algorithms.%20In%20this%20work%2C%20we%20introduce%20a%20flexible%0Aparadigm%20for%20individual%20preference%20alignment.%20Our%20method%20fundamentally%20improves%0Aefficiency%20by%20disentangling%20preference%20representation%20from%20text%20generation%20in%0ALLMs.%20We%20validate%20our%20approach%20across%20multiple%20text%20generation%20tasks%20and%0Ademonstrate%20that%20it%20can%20produce%20aligned%20quality%20as%20well%20as%20or%20better%20than%0APEFT-based%20methods%2C%20while%20reducing%20additional%20training%20time%20for%20each%20new%0Aindividual%20preference%20by%20%2480%5C%25%24%20to%20%2490%5C%25%24%20in%20comparison%20with%20them.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20834v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangling%2520Preference%2520Representation%2520and%2520Text%2520Generation%2520for%250A%2520%2520Efficient%2520Individual%2520Preference%2520Alignment%26entry.906535625%3DJianfei%2520Zhang%2520and%2520Jun%2520Bai%2520and%2520Bei%2520Li%2520and%2520Yanmeng%2520Wang%2520and%2520Rumei%2520Li%2520and%2520Chenghua%2520Lin%2520and%2520Wenge%2520Rong%26entry.1292438233%3D%2520%2520Aligning%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520general%2520human%2520preferences%2520has%2520been%250Aproved%2520crucial%2520in%2520improving%2520the%2520interaction%2520quality%2520between%2520LLMs%2520and%2520human.%250AHowever%252C%2520human%2520values%2520are%2520inherently%2520diverse%2520among%2520different%2520individuals%252C%250Amaking%2520it%2520insufficient%2520to%2520align%2520LLMs%2520solely%2520with%2520general%2520preferences.%2520To%250Aaddress%2520this%252C%2520personalizing%2520LLMs%2520according%2520to%2520individual%2520feedback%2520emerges%2520as%2520a%250Apromising%2520solution.%2520Nonetheless%252C%2520this%2520approach%2520presents%2520challenges%2520in%2520terms%2520of%250Athe%2520efficiency%2520of%2520alignment%2520algorithms.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520flexible%250Aparadigm%2520for%2520individual%2520preference%2520alignment.%2520Our%2520method%2520fundamentally%2520improves%250Aefficiency%2520by%2520disentangling%2520preference%2520representation%2520from%2520text%2520generation%2520in%250ALLMs.%2520We%2520validate%2520our%2520approach%2520across%2520multiple%2520text%2520generation%2520tasks%2520and%250Ademonstrate%2520that%2520it%2520can%2520produce%2520aligned%2520quality%2520as%2520well%2520as%2520or%2520better%2520than%250APEFT-based%2520methods%252C%2520while%2520reducing%2520additional%2520training%2520time%2520for%2520each%2520new%250Aindividual%2520preference%2520by%2520%252480%255C%2525%2524%2520to%2520%252490%255C%2525%2524%2520in%2520comparison%2520with%2520them.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20834v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangling%20Preference%20Representation%20and%20Text%20Generation%20for%0A%20%20Efficient%20Individual%20Preference%20Alignment&entry.906535625=Jianfei%20Zhang%20and%20Jun%20Bai%20and%20Bei%20Li%20and%20Yanmeng%20Wang%20and%20Rumei%20Li%20and%20Chenghua%20Lin%20and%20Wenge%20Rong&entry.1292438233=%20%20Aligning%20Large%20Language%20Models%20%28LLMs%29%20with%20general%20human%20preferences%20has%20been%0Aproved%20crucial%20in%20improving%20the%20interaction%20quality%20between%20LLMs%20and%20human.%0AHowever%2C%20human%20values%20are%20inherently%20diverse%20among%20different%20individuals%2C%0Amaking%20it%20insufficient%20to%20align%20LLMs%20solely%20with%20general%20preferences.%20To%0Aaddress%20this%2C%20personalizing%20LLMs%20according%20to%20individual%20feedback%20emerges%20as%20a%0Apromising%20solution.%20Nonetheless%2C%20this%20approach%20presents%20challenges%20in%20terms%20of%0Athe%20efficiency%20of%20alignment%20algorithms.%20In%20this%20work%2C%20we%20introduce%20a%20flexible%0Aparadigm%20for%20individual%20preference%20alignment.%20Our%20method%20fundamentally%20improves%0Aefficiency%20by%20disentangling%20preference%20representation%20from%20text%20generation%20in%0ALLMs.%20We%20validate%20our%20approach%20across%20multiple%20text%20generation%20tasks%20and%0Ademonstrate%20that%20it%20can%20produce%20aligned%20quality%20as%20well%20as%20or%20better%20than%0APEFT-based%20methods%2C%20while%20reducing%20additional%20training%20time%20for%20each%20new%0Aindividual%20preference%20by%20%2480%5C%25%24%20to%20%2490%5C%25%24%20in%20comparison%20with%20them.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20834v1&entry.124074799=Read"},
{"title": "Toward Intelligent and Secure Cloud: Large Language Model Empowered\n  Proactive Defense", "author": "Yuyang Zhou and Guang Cheng and Kang Du and Zihan Chen", "abstract": "  The rapid evolution of cloud computing technologies and the increasing number\nof cloud applications have provided a large number of benefits in daily lives.\nHowever, the diversity and complexity of different components pose a\nsignificant challenge to cloud security, especially when dealing with\nsophisticated and advanced cyberattacks. Recent advancements in generative\nfoundation models (GFMs), particularly in the large language models (LLMs),\noffer promising solutions for security intelligence. By exploiting the powerful\nabilities in language understanding, data analysis, task inference, action\nplanning, and code generation, we present LLM-PD, a novel proactive defense\narchitecture that defeats various threats in a proactive manner. LLM-PD can\nefficiently make a decision through comprehensive data analysis and sequential\nreasoning, as well as dynamically creating and deploying actionable defense\nmechanisms on the target cloud. Furthermore, it can flexibly self-evolve based\non experience learned from previous interactions and adapt to new attack\nscenarios without additional training. The experimental results demonstrate its\nremarkable ability in terms of defense effectiveness and efficiency,\nparticularly highlighting an outstanding success rate when compared with other\nexisting methods.\n", "link": "http://arxiv.org/abs/2412.21051v1", "date": "2024-12-30", "relevancy": 2.3111, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4656}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4656}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4554}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Intelligent%20and%20Secure%20Cloud%3A%20Large%20Language%20Model%20Empowered%0A%20%20Proactive%20Defense&body=Title%3A%20Toward%20Intelligent%20and%20Secure%20Cloud%3A%20Large%20Language%20Model%20Empowered%0A%20%20Proactive%20Defense%0AAuthor%3A%20Yuyang%20Zhou%20and%20Guang%20Cheng%20and%20Kang%20Du%20and%20Zihan%20Chen%0AAbstract%3A%20%20%20The%20rapid%20evolution%20of%20cloud%20computing%20technologies%20and%20the%20increasing%20number%0Aof%20cloud%20applications%20have%20provided%20a%20large%20number%20of%20benefits%20in%20daily%20lives.%0AHowever%2C%20the%20diversity%20and%20complexity%20of%20different%20components%20pose%20a%0Asignificant%20challenge%20to%20cloud%20security%2C%20especially%20when%20dealing%20with%0Asophisticated%20and%20advanced%20cyberattacks.%20Recent%20advancements%20in%20generative%0Afoundation%20models%20%28GFMs%29%2C%20particularly%20in%20the%20large%20language%20models%20%28LLMs%29%2C%0Aoffer%20promising%20solutions%20for%20security%20intelligence.%20By%20exploiting%20the%20powerful%0Aabilities%20in%20language%20understanding%2C%20data%20analysis%2C%20task%20inference%2C%20action%0Aplanning%2C%20and%20code%20generation%2C%20we%20present%20LLM-PD%2C%20a%20novel%20proactive%20defense%0Aarchitecture%20that%20defeats%20various%20threats%20in%20a%20proactive%20manner.%20LLM-PD%20can%0Aefficiently%20make%20a%20decision%20through%20comprehensive%20data%20analysis%20and%20sequential%0Areasoning%2C%20as%20well%20as%20dynamically%20creating%20and%20deploying%20actionable%20defense%0Amechanisms%20on%20the%20target%20cloud.%20Furthermore%2C%20it%20can%20flexibly%20self-evolve%20based%0Aon%20experience%20learned%20from%20previous%20interactions%20and%20adapt%20to%20new%20attack%0Ascenarios%20without%20additional%20training.%20The%20experimental%20results%20demonstrate%20its%0Aremarkable%20ability%20in%20terms%20of%20defense%20effectiveness%20and%20efficiency%2C%0Aparticularly%20highlighting%20an%20outstanding%20success%20rate%20when%20compared%20with%20other%0Aexisting%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21051v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Intelligent%2520and%2520Secure%2520Cloud%253A%2520Large%2520Language%2520Model%2520Empowered%250A%2520%2520Proactive%2520Defense%26entry.906535625%3DYuyang%2520Zhou%2520and%2520Guang%2520Cheng%2520and%2520Kang%2520Du%2520and%2520Zihan%2520Chen%26entry.1292438233%3D%2520%2520The%2520rapid%2520evolution%2520of%2520cloud%2520computing%2520technologies%2520and%2520the%2520increasing%2520number%250Aof%2520cloud%2520applications%2520have%2520provided%2520a%2520large%2520number%2520of%2520benefits%2520in%2520daily%2520lives.%250AHowever%252C%2520the%2520diversity%2520and%2520complexity%2520of%2520different%2520components%2520pose%2520a%250Asignificant%2520challenge%2520to%2520cloud%2520security%252C%2520especially%2520when%2520dealing%2520with%250Asophisticated%2520and%2520advanced%2520cyberattacks.%2520Recent%2520advancements%2520in%2520generative%250Afoundation%2520models%2520%2528GFMs%2529%252C%2520particularly%2520in%2520the%2520large%2520language%2520models%2520%2528LLMs%2529%252C%250Aoffer%2520promising%2520solutions%2520for%2520security%2520intelligence.%2520By%2520exploiting%2520the%2520powerful%250Aabilities%2520in%2520language%2520understanding%252C%2520data%2520analysis%252C%2520task%2520inference%252C%2520action%250Aplanning%252C%2520and%2520code%2520generation%252C%2520we%2520present%2520LLM-PD%252C%2520a%2520novel%2520proactive%2520defense%250Aarchitecture%2520that%2520defeats%2520various%2520threats%2520in%2520a%2520proactive%2520manner.%2520LLM-PD%2520can%250Aefficiently%2520make%2520a%2520decision%2520through%2520comprehensive%2520data%2520analysis%2520and%2520sequential%250Areasoning%252C%2520as%2520well%2520as%2520dynamically%2520creating%2520and%2520deploying%2520actionable%2520defense%250Amechanisms%2520on%2520the%2520target%2520cloud.%2520Furthermore%252C%2520it%2520can%2520flexibly%2520self-evolve%2520based%250Aon%2520experience%2520learned%2520from%2520previous%2520interactions%2520and%2520adapt%2520to%2520new%2520attack%250Ascenarios%2520without%2520additional%2520training.%2520The%2520experimental%2520results%2520demonstrate%2520its%250Aremarkable%2520ability%2520in%2520terms%2520of%2520defense%2520effectiveness%2520and%2520efficiency%252C%250Aparticularly%2520highlighting%2520an%2520outstanding%2520success%2520rate%2520when%2520compared%2520with%2520other%250Aexisting%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21051v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Intelligent%20and%20Secure%20Cloud%3A%20Large%20Language%20Model%20Empowered%0A%20%20Proactive%20Defense&entry.906535625=Yuyang%20Zhou%20and%20Guang%20Cheng%20and%20Kang%20Du%20and%20Zihan%20Chen&entry.1292438233=%20%20The%20rapid%20evolution%20of%20cloud%20computing%20technologies%20and%20the%20increasing%20number%0Aof%20cloud%20applications%20have%20provided%20a%20large%20number%20of%20benefits%20in%20daily%20lives.%0AHowever%2C%20the%20diversity%20and%20complexity%20of%20different%20components%20pose%20a%0Asignificant%20challenge%20to%20cloud%20security%2C%20especially%20when%20dealing%20with%0Asophisticated%20and%20advanced%20cyberattacks.%20Recent%20advancements%20in%20generative%0Afoundation%20models%20%28GFMs%29%2C%20particularly%20in%20the%20large%20language%20models%20%28LLMs%29%2C%0Aoffer%20promising%20solutions%20for%20security%20intelligence.%20By%20exploiting%20the%20powerful%0Aabilities%20in%20language%20understanding%2C%20data%20analysis%2C%20task%20inference%2C%20action%0Aplanning%2C%20and%20code%20generation%2C%20we%20present%20LLM-PD%2C%20a%20novel%20proactive%20defense%0Aarchitecture%20that%20defeats%20various%20threats%20in%20a%20proactive%20manner.%20LLM-PD%20can%0Aefficiently%20make%20a%20decision%20through%20comprehensive%20data%20analysis%20and%20sequential%0Areasoning%2C%20as%20well%20as%20dynamically%20creating%20and%20deploying%20actionable%20defense%0Amechanisms%20on%20the%20target%20cloud.%20Furthermore%2C%20it%20can%20flexibly%20self-evolve%20based%0Aon%20experience%20learned%20from%20previous%20interactions%20and%20adapt%20to%20new%20attack%0Ascenarios%20without%20additional%20training.%20The%20experimental%20results%20demonstrate%20its%0Aremarkable%20ability%20in%20terms%20of%20defense%20effectiveness%20and%20efficiency%2C%0Aparticularly%20highlighting%20an%20outstanding%20success%20rate%20when%20compared%20with%20other%0Aexisting%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21051v1&entry.124074799=Read"},
{"title": "Towards Compatible Fine-tuning for Vision-Language Model Updates", "author": "Zhengbo Wang and Jian Liang and Lijun Sheng and Ran He and Zilei Wang and Tieniu Tan", "abstract": "  So far, efficient fine-tuning has become a popular strategy for enhancing the\ncapabilities of foundation models on downstream tasks by learning plug-and-play\nmodules. However, existing methods overlook a crucial issue: if the underlying\nfoundation model is updated, are these plug-and-play modules still effective?\nIn this paper, we first conduct a detailed analysis of various fine-tuning\nmethods on the CLIP in terms of their compatibility with model updates. The\nstudy reveals that many high-performing fine-tuning methods fail to be\ncompatible with the upgraded models. To address this, we propose a novel\napproach, Class-conditioned Context Optimization (ContCoOp), which integrates\nlearnable prompts with class embeddings using an attention layer before\ninputting them into the text encoder. Consequently, the prompts can dynamically\nadapt to the changes in embedding space (due to model updates), ensuring\ncontinued effectiveness. Extensive experiments over 15 datasets show that our\nContCoOp achieves the highest compatibility over the baseline methods, and\nexhibits robust out-of-distribution generalization.\n", "link": "http://arxiv.org/abs/2412.20895v1", "date": "2024-12-30", "relevancy": 2.2968, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5816}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5816}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Compatible%20Fine-tuning%20for%20Vision-Language%20Model%20Updates&body=Title%3A%20Towards%20Compatible%20Fine-tuning%20for%20Vision-Language%20Model%20Updates%0AAuthor%3A%20Zhengbo%20Wang%20and%20Jian%20Liang%20and%20Lijun%20Sheng%20and%20Ran%20He%20and%20Zilei%20Wang%20and%20Tieniu%20Tan%0AAbstract%3A%20%20%20So%20far%2C%20efficient%20fine-tuning%20has%20become%20a%20popular%20strategy%20for%20enhancing%20the%0Acapabilities%20of%20foundation%20models%20on%20downstream%20tasks%20by%20learning%20plug-and-play%0Amodules.%20However%2C%20existing%20methods%20overlook%20a%20crucial%20issue%3A%20if%20the%20underlying%0Afoundation%20model%20is%20updated%2C%20are%20these%20plug-and-play%20modules%20still%20effective%3F%0AIn%20this%20paper%2C%20we%20first%20conduct%20a%20detailed%20analysis%20of%20various%20fine-tuning%0Amethods%20on%20the%20CLIP%20in%20terms%20of%20their%20compatibility%20with%20model%20updates.%20The%0Astudy%20reveals%20that%20many%20high-performing%20fine-tuning%20methods%20fail%20to%20be%0Acompatible%20with%20the%20upgraded%20models.%20To%20address%20this%2C%20we%20propose%20a%20novel%0Aapproach%2C%20Class-conditioned%20Context%20Optimization%20%28ContCoOp%29%2C%20which%20integrates%0Alearnable%20prompts%20with%20class%20embeddings%20using%20an%20attention%20layer%20before%0Ainputting%20them%20into%20the%20text%20encoder.%20Consequently%2C%20the%20prompts%20can%20dynamically%0Aadapt%20to%20the%20changes%20in%20embedding%20space%20%28due%20to%20model%20updates%29%2C%20ensuring%0Acontinued%20effectiveness.%20Extensive%20experiments%20over%2015%20datasets%20show%20that%20our%0AContCoOp%20achieves%20the%20highest%20compatibility%20over%20the%20baseline%20methods%2C%20and%0Aexhibits%20robust%20out-of-distribution%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20895v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Compatible%2520Fine-tuning%2520for%2520Vision-Language%2520Model%2520Updates%26entry.906535625%3DZhengbo%2520Wang%2520and%2520Jian%2520Liang%2520and%2520Lijun%2520Sheng%2520and%2520Ran%2520He%2520and%2520Zilei%2520Wang%2520and%2520Tieniu%2520Tan%26entry.1292438233%3D%2520%2520So%2520far%252C%2520efficient%2520fine-tuning%2520has%2520become%2520a%2520popular%2520strategy%2520for%2520enhancing%2520the%250Acapabilities%2520of%2520foundation%2520models%2520on%2520downstream%2520tasks%2520by%2520learning%2520plug-and-play%250Amodules.%2520However%252C%2520existing%2520methods%2520overlook%2520a%2520crucial%2520issue%253A%2520if%2520the%2520underlying%250Afoundation%2520model%2520is%2520updated%252C%2520are%2520these%2520plug-and-play%2520modules%2520still%2520effective%253F%250AIn%2520this%2520paper%252C%2520we%2520first%2520conduct%2520a%2520detailed%2520analysis%2520of%2520various%2520fine-tuning%250Amethods%2520on%2520the%2520CLIP%2520in%2520terms%2520of%2520their%2520compatibility%2520with%2520model%2520updates.%2520The%250Astudy%2520reveals%2520that%2520many%2520high-performing%2520fine-tuning%2520methods%2520fail%2520to%2520be%250Acompatible%2520with%2520the%2520upgraded%2520models.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520novel%250Aapproach%252C%2520Class-conditioned%2520Context%2520Optimization%2520%2528ContCoOp%2529%252C%2520which%2520integrates%250Alearnable%2520prompts%2520with%2520class%2520embeddings%2520using%2520an%2520attention%2520layer%2520before%250Ainputting%2520them%2520into%2520the%2520text%2520encoder.%2520Consequently%252C%2520the%2520prompts%2520can%2520dynamically%250Aadapt%2520to%2520the%2520changes%2520in%2520embedding%2520space%2520%2528due%2520to%2520model%2520updates%2529%252C%2520ensuring%250Acontinued%2520effectiveness.%2520Extensive%2520experiments%2520over%252015%2520datasets%2520show%2520that%2520our%250AContCoOp%2520achieves%2520the%2520highest%2520compatibility%2520over%2520the%2520baseline%2520methods%252C%2520and%250Aexhibits%2520robust%2520out-of-distribution%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20895v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Compatible%20Fine-tuning%20for%20Vision-Language%20Model%20Updates&entry.906535625=Zhengbo%20Wang%20and%20Jian%20Liang%20and%20Lijun%20Sheng%20and%20Ran%20He%20and%20Zilei%20Wang%20and%20Tieniu%20Tan&entry.1292438233=%20%20So%20far%2C%20efficient%20fine-tuning%20has%20become%20a%20popular%20strategy%20for%20enhancing%20the%0Acapabilities%20of%20foundation%20models%20on%20downstream%20tasks%20by%20learning%20plug-and-play%0Amodules.%20However%2C%20existing%20methods%20overlook%20a%20crucial%20issue%3A%20if%20the%20underlying%0Afoundation%20model%20is%20updated%2C%20are%20these%20plug-and-play%20modules%20still%20effective%3F%0AIn%20this%20paper%2C%20we%20first%20conduct%20a%20detailed%20analysis%20of%20various%20fine-tuning%0Amethods%20on%20the%20CLIP%20in%20terms%20of%20their%20compatibility%20with%20model%20updates.%20The%0Astudy%20reveals%20that%20many%20high-performing%20fine-tuning%20methods%20fail%20to%20be%0Acompatible%20with%20the%20upgraded%20models.%20To%20address%20this%2C%20we%20propose%20a%20novel%0Aapproach%2C%20Class-conditioned%20Context%20Optimization%20%28ContCoOp%29%2C%20which%20integrates%0Alearnable%20prompts%20with%20class%20embeddings%20using%20an%20attention%20layer%20before%0Ainputting%20them%20into%20the%20text%20encoder.%20Consequently%2C%20the%20prompts%20can%20dynamically%0Aadapt%20to%20the%20changes%20in%20embedding%20space%20%28due%20to%20model%20updates%29%2C%20ensuring%0Acontinued%20effectiveness.%20Extensive%20experiments%20over%2015%20datasets%20show%20that%20our%0AContCoOp%20achieves%20the%20highest%20compatibility%20over%20the%20baseline%20methods%2C%20and%0Aexhibits%20robust%20out-of-distribution%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20895v1&entry.124074799=Read"},
{"title": "PyG-SSL: A Graph Self-Supervised Learning Toolkit", "author": "Lecheng Zheng and Baoyu Jing and Zihao Li and Zhichen Zeng and Tianxin Wei and Mengting Ai and Xinrui He and Lihui Liu and Dongqi Fu and Jiaxuan You and Hanghang Tong and Jingrui He", "abstract": "  Graph Self-Supervised Learning (SSL) has emerged as a pivotal area of\nresearch in recent years. By engaging in pretext tasks to learn the intricate\ntopological structures and properties of graphs using unlabeled data, these\ngraph SSL models achieve enhanced performance, improved generalization, and\nheightened robustness. Despite the remarkable achievements of these graph SSL\nmethods, their current implementation poses significant challenges for\nbeginners and practitioners due to the complex nature of graph structures,\ninconsistent evaluation metrics, and concerns regarding reproducibility hinder\nfurther progress in this field. Recognizing the growing interest within the\nresearch community, there is an urgent need for a comprehensive,\nbeginner-friendly, and accessible toolkit consisting of the most representative\ngraph SSL algorithms. To address these challenges, we present a Graph SSL\ntoolkit named PyG-SSL, which is built upon PyTorch and is compatible with\nvarious deep learning and scientific computing backends. Within the toolkit, we\noffer a unified framework encompassing dataset loading, hyper-parameter\nconfiguration, model training, and comprehensive performance evaluation for\ndiverse downstream tasks. Moreover, we provide beginner-friendly tutorials and\nthe best hyper-parameters of each graph SSL algorithm on different graph\ndatasets, facilitating the reproduction of results. The GitHub repository of\nthe library is https://github.com/iDEA-iSAIL-Lab-UIUC/pyg-ssl.\n", "link": "http://arxiv.org/abs/2412.21151v1", "date": "2024-12-30", "relevancy": 2.2874, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4974}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4408}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PyG-SSL%3A%20A%20Graph%20Self-Supervised%20Learning%20Toolkit&body=Title%3A%20PyG-SSL%3A%20A%20Graph%20Self-Supervised%20Learning%20Toolkit%0AAuthor%3A%20Lecheng%20Zheng%20and%20Baoyu%20Jing%20and%20Zihao%20Li%20and%20Zhichen%20Zeng%20and%20Tianxin%20Wei%20and%20Mengting%20Ai%20and%20Xinrui%20He%20and%20Lihui%20Liu%20and%20Dongqi%20Fu%20and%20Jiaxuan%20You%20and%20Hanghang%20Tong%20and%20Jingrui%20He%0AAbstract%3A%20%20%20Graph%20Self-Supervised%20Learning%20%28SSL%29%20has%20emerged%20as%20a%20pivotal%20area%20of%0Aresearch%20in%20recent%20years.%20By%20engaging%20in%20pretext%20tasks%20to%20learn%20the%20intricate%0Atopological%20structures%20and%20properties%20of%20graphs%20using%20unlabeled%20data%2C%20these%0Agraph%20SSL%20models%20achieve%20enhanced%20performance%2C%20improved%20generalization%2C%20and%0Aheightened%20robustness.%20Despite%20the%20remarkable%20achievements%20of%20these%20graph%20SSL%0Amethods%2C%20their%20current%20implementation%20poses%20significant%20challenges%20for%0Abeginners%20and%20practitioners%20due%20to%20the%20complex%20nature%20of%20graph%20structures%2C%0Ainconsistent%20evaluation%20metrics%2C%20and%20concerns%20regarding%20reproducibility%20hinder%0Afurther%20progress%20in%20this%20field.%20Recognizing%20the%20growing%20interest%20within%20the%0Aresearch%20community%2C%20there%20is%20an%20urgent%20need%20for%20a%20comprehensive%2C%0Abeginner-friendly%2C%20and%20accessible%20toolkit%20consisting%20of%20the%20most%20representative%0Agraph%20SSL%20algorithms.%20To%20address%20these%20challenges%2C%20we%20present%20a%20Graph%20SSL%0Atoolkit%20named%20PyG-SSL%2C%20which%20is%20built%20upon%20PyTorch%20and%20is%20compatible%20with%0Avarious%20deep%20learning%20and%20scientific%20computing%20backends.%20Within%20the%20toolkit%2C%20we%0Aoffer%20a%20unified%20framework%20encompassing%20dataset%20loading%2C%20hyper-parameter%0Aconfiguration%2C%20model%20training%2C%20and%20comprehensive%20performance%20evaluation%20for%0Adiverse%20downstream%20tasks.%20Moreover%2C%20we%20provide%20beginner-friendly%20tutorials%20and%0Athe%20best%20hyper-parameters%20of%20each%20graph%20SSL%20algorithm%20on%20different%20graph%0Adatasets%2C%20facilitating%20the%20reproduction%20of%20results.%20The%20GitHub%20repository%20of%0Athe%20library%20is%20https%3A//github.com/iDEA-iSAIL-Lab-UIUC/pyg-ssl.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21151v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPyG-SSL%253A%2520A%2520Graph%2520Self-Supervised%2520Learning%2520Toolkit%26entry.906535625%3DLecheng%2520Zheng%2520and%2520Baoyu%2520Jing%2520and%2520Zihao%2520Li%2520and%2520Zhichen%2520Zeng%2520and%2520Tianxin%2520Wei%2520and%2520Mengting%2520Ai%2520and%2520Xinrui%2520He%2520and%2520Lihui%2520Liu%2520and%2520Dongqi%2520Fu%2520and%2520Jiaxuan%2520You%2520and%2520Hanghang%2520Tong%2520and%2520Jingrui%2520He%26entry.1292438233%3D%2520%2520Graph%2520Self-Supervised%2520Learning%2520%2528SSL%2529%2520has%2520emerged%2520as%2520a%2520pivotal%2520area%2520of%250Aresearch%2520in%2520recent%2520years.%2520By%2520engaging%2520in%2520pretext%2520tasks%2520to%2520learn%2520the%2520intricate%250Atopological%2520structures%2520and%2520properties%2520of%2520graphs%2520using%2520unlabeled%2520data%252C%2520these%250Agraph%2520SSL%2520models%2520achieve%2520enhanced%2520performance%252C%2520improved%2520generalization%252C%2520and%250Aheightened%2520robustness.%2520Despite%2520the%2520remarkable%2520achievements%2520of%2520these%2520graph%2520SSL%250Amethods%252C%2520their%2520current%2520implementation%2520poses%2520significant%2520challenges%2520for%250Abeginners%2520and%2520practitioners%2520due%2520to%2520the%2520complex%2520nature%2520of%2520graph%2520structures%252C%250Ainconsistent%2520evaluation%2520metrics%252C%2520and%2520concerns%2520regarding%2520reproducibility%2520hinder%250Afurther%2520progress%2520in%2520this%2520field.%2520Recognizing%2520the%2520growing%2520interest%2520within%2520the%250Aresearch%2520community%252C%2520there%2520is%2520an%2520urgent%2520need%2520for%2520a%2520comprehensive%252C%250Abeginner-friendly%252C%2520and%2520accessible%2520toolkit%2520consisting%2520of%2520the%2520most%2520representative%250Agraph%2520SSL%2520algorithms.%2520To%2520address%2520these%2520challenges%252C%2520we%2520present%2520a%2520Graph%2520SSL%250Atoolkit%2520named%2520PyG-SSL%252C%2520which%2520is%2520built%2520upon%2520PyTorch%2520and%2520is%2520compatible%2520with%250Avarious%2520deep%2520learning%2520and%2520scientific%2520computing%2520backends.%2520Within%2520the%2520toolkit%252C%2520we%250Aoffer%2520a%2520unified%2520framework%2520encompassing%2520dataset%2520loading%252C%2520hyper-parameter%250Aconfiguration%252C%2520model%2520training%252C%2520and%2520comprehensive%2520performance%2520evaluation%2520for%250Adiverse%2520downstream%2520tasks.%2520Moreover%252C%2520we%2520provide%2520beginner-friendly%2520tutorials%2520and%250Athe%2520best%2520hyper-parameters%2520of%2520each%2520graph%2520SSL%2520algorithm%2520on%2520different%2520graph%250Adatasets%252C%2520facilitating%2520the%2520reproduction%2520of%2520results.%2520The%2520GitHub%2520repository%2520of%250Athe%2520library%2520is%2520https%253A//github.com/iDEA-iSAIL-Lab-UIUC/pyg-ssl.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21151v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PyG-SSL%3A%20A%20Graph%20Self-Supervised%20Learning%20Toolkit&entry.906535625=Lecheng%20Zheng%20and%20Baoyu%20Jing%20and%20Zihao%20Li%20and%20Zhichen%20Zeng%20and%20Tianxin%20Wei%20and%20Mengting%20Ai%20and%20Xinrui%20He%20and%20Lihui%20Liu%20and%20Dongqi%20Fu%20and%20Jiaxuan%20You%20and%20Hanghang%20Tong%20and%20Jingrui%20He&entry.1292438233=%20%20Graph%20Self-Supervised%20Learning%20%28SSL%29%20has%20emerged%20as%20a%20pivotal%20area%20of%0Aresearch%20in%20recent%20years.%20By%20engaging%20in%20pretext%20tasks%20to%20learn%20the%20intricate%0Atopological%20structures%20and%20properties%20of%20graphs%20using%20unlabeled%20data%2C%20these%0Agraph%20SSL%20models%20achieve%20enhanced%20performance%2C%20improved%20generalization%2C%20and%0Aheightened%20robustness.%20Despite%20the%20remarkable%20achievements%20of%20these%20graph%20SSL%0Amethods%2C%20their%20current%20implementation%20poses%20significant%20challenges%20for%0Abeginners%20and%20practitioners%20due%20to%20the%20complex%20nature%20of%20graph%20structures%2C%0Ainconsistent%20evaluation%20metrics%2C%20and%20concerns%20regarding%20reproducibility%20hinder%0Afurther%20progress%20in%20this%20field.%20Recognizing%20the%20growing%20interest%20within%20the%0Aresearch%20community%2C%20there%20is%20an%20urgent%20need%20for%20a%20comprehensive%2C%0Abeginner-friendly%2C%20and%20accessible%20toolkit%20consisting%20of%20the%20most%20representative%0Agraph%20SSL%20algorithms.%20To%20address%20these%20challenges%2C%20we%20present%20a%20Graph%20SSL%0Atoolkit%20named%20PyG-SSL%2C%20which%20is%20built%20upon%20PyTorch%20and%20is%20compatible%20with%0Avarious%20deep%20learning%20and%20scientific%20computing%20backends.%20Within%20the%20toolkit%2C%20we%0Aoffer%20a%20unified%20framework%20encompassing%20dataset%20loading%2C%20hyper-parameter%0Aconfiguration%2C%20model%20training%2C%20and%20comprehensive%20performance%20evaluation%20for%0Adiverse%20downstream%20tasks.%20Moreover%2C%20we%20provide%20beginner-friendly%20tutorials%20and%0Athe%20best%20hyper-parameters%20of%20each%20graph%20SSL%20algorithm%20on%20different%20graph%0Adatasets%2C%20facilitating%20the%20reproduction%20of%20results.%20The%20GitHub%20repository%20of%0Athe%20library%20is%20https%3A//github.com/iDEA-iSAIL-Lab-UIUC/pyg-ssl.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21151v1&entry.124074799=Read"},
{"title": "EdgeRAG: Online-Indexed RAG for Edge Devices", "author": "Korakit Seemakhupt and Sihang Liu and Samira Khan", "abstract": "  Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory.\n", "link": "http://arxiv.org/abs/2412.21023v1", "date": "2024-12-30", "relevancy": 2.2775, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4764}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4564}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EdgeRAG%3A%20Online-Indexed%20RAG%20for%20Edge%20Devices&body=Title%3A%20EdgeRAG%3A%20Online-Indexed%20RAG%20for%20Edge%20Devices%0AAuthor%3A%20Korakit%20Seemakhupt%20and%20Sihang%20Liu%20and%20Samira%20Khan%0AAbstract%3A%20%20%20Deploying%20Retrieval%20Augmented%20Generation%20%28RAG%29%20on%20resource-constrained%20edge%0Adevices%20is%20challenging%20due%20to%20limited%20memory%20and%20processing%20power.%20In%20this%0Awork%2C%20we%20propose%20EdgeRAG%20which%20addresses%20the%20memory%20constraint%20by%20pruning%0Aembeddings%20within%20clusters%20and%20generating%20embeddings%20on-demand%20during%0Aretrieval.%20To%20avoid%20the%20latency%20of%20generating%20embeddings%20for%20large%20tail%0Aclusters%2C%20EdgeRAG%20pre-computes%20and%20stores%20embeddings%20for%20these%20clusters%2C%20while%0Aadaptively%20caching%20remaining%20embeddings%20to%20minimize%20redundant%20computations%20and%0Afurther%20optimize%20latency.%20The%20result%20from%20BEIR%20suite%20shows%20that%20EdgeRAG%20offers%0Asignificant%20latency%20reduction%20over%20the%20baseline%20IVF%20index%2C%20but%20with%20similar%0Ageneration%20quality%20while%20allowing%20all%20of%20our%20evaluated%20datasets%20to%20fit%20into%20the%0Amemory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21023v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdgeRAG%253A%2520Online-Indexed%2520RAG%2520for%2520Edge%2520Devices%26entry.906535625%3DKorakit%2520Seemakhupt%2520and%2520Sihang%2520Liu%2520and%2520Samira%2520Khan%26entry.1292438233%3D%2520%2520Deploying%2520Retrieval%2520Augmented%2520Generation%2520%2528RAG%2529%2520on%2520resource-constrained%2520edge%250Adevices%2520is%2520challenging%2520due%2520to%2520limited%2520memory%2520and%2520processing%2520power.%2520In%2520this%250Awork%252C%2520we%2520propose%2520EdgeRAG%2520which%2520addresses%2520the%2520memory%2520constraint%2520by%2520pruning%250Aembeddings%2520within%2520clusters%2520and%2520generating%2520embeddings%2520on-demand%2520during%250Aretrieval.%2520To%2520avoid%2520the%2520latency%2520of%2520generating%2520embeddings%2520for%2520large%2520tail%250Aclusters%252C%2520EdgeRAG%2520pre-computes%2520and%2520stores%2520embeddings%2520for%2520these%2520clusters%252C%2520while%250Aadaptively%2520caching%2520remaining%2520embeddings%2520to%2520minimize%2520redundant%2520computations%2520and%250Afurther%2520optimize%2520latency.%2520The%2520result%2520from%2520BEIR%2520suite%2520shows%2520that%2520EdgeRAG%2520offers%250Asignificant%2520latency%2520reduction%2520over%2520the%2520baseline%2520IVF%2520index%252C%2520but%2520with%2520similar%250Ageneration%2520quality%2520while%2520allowing%2520all%2520of%2520our%2520evaluated%2520datasets%2520to%2520fit%2520into%2520the%250Amemory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21023v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EdgeRAG%3A%20Online-Indexed%20RAG%20for%20Edge%20Devices&entry.906535625=Korakit%20Seemakhupt%20and%20Sihang%20Liu%20and%20Samira%20Khan&entry.1292438233=%20%20Deploying%20Retrieval%20Augmented%20Generation%20%28RAG%29%20on%20resource-constrained%20edge%0Adevices%20is%20challenging%20due%20to%20limited%20memory%20and%20processing%20power.%20In%20this%0Awork%2C%20we%20propose%20EdgeRAG%20which%20addresses%20the%20memory%20constraint%20by%20pruning%0Aembeddings%20within%20clusters%20and%20generating%20embeddings%20on-demand%20during%0Aretrieval.%20To%20avoid%20the%20latency%20of%20generating%20embeddings%20for%20large%20tail%0Aclusters%2C%20EdgeRAG%20pre-computes%20and%20stores%20embeddings%20for%20these%20clusters%2C%20while%0Aadaptively%20caching%20remaining%20embeddings%20to%20minimize%20redundant%20computations%20and%0Afurther%20optimize%20latency.%20The%20result%20from%20BEIR%20suite%20shows%20that%20EdgeRAG%20offers%0Asignificant%20latency%20reduction%20over%20the%20baseline%20IVF%20index%2C%20but%20with%20similar%0Ageneration%20quality%20while%20allowing%20all%20of%20our%20evaluated%20datasets%20to%20fit%20into%20the%0Amemory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21023v1&entry.124074799=Read"},
{"title": "VisionReward: Fine-Grained Multi-Dimensional Human Preference Learning\n  for Image and Video Generation", "author": "Jiazheng Xu and Yu Huang and Jiale Cheng and Yuanming Yang and Jiajun Xu and Yuan Wang and Wenbo Duan and Shen Yang and Qunlin Jin and Shurun Li and Jiayan Teng and Zhuoyi Yang and Wendi Zheng and Xiao Liu and Ming Ding and Xiaohan Zhang and Xiaotao Gu and Shiyu Huang and Minlie Huang and Jie Tang and Yuxiao Dong", "abstract": "  We present a general strategy to aligning visual generation models -- both\nimage and video generation -- with human preference. To start with, we build\nVisionReward -- a fine-grained and multi-dimensional reward model. We decompose\nhuman preferences in images and videos into multiple dimensions, each\nrepresented by a series of judgment questions, linearly weighted and summed to\nan interpretable and accurate score. To address the challenges of video quality\nassessment, we systematically analyze various dynamic features of videos, which\nhelps VisionReward surpass VideoScore by 17.2% and achieve top performance for\nvideo preference prediction. Based on VisionReward, we develop a\nmulti-objective preference learning algorithm that effectively addresses the\nissue of confounding factors within preference data. Our approach significantly\noutperforms existing image and video scoring methods on both machine metrics\nand human evaluation. All code and datasets are provided at\nhttps://github.com/THUDM/VisionReward.\n", "link": "http://arxiv.org/abs/2412.21059v1", "date": "2024-12-30", "relevancy": 2.2733, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5828}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5661}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisionReward%3A%20Fine-Grained%20Multi-Dimensional%20Human%20Preference%20Learning%0A%20%20for%20Image%20and%20Video%20Generation&body=Title%3A%20VisionReward%3A%20Fine-Grained%20Multi-Dimensional%20Human%20Preference%20Learning%0A%20%20for%20Image%20and%20Video%20Generation%0AAuthor%3A%20Jiazheng%20Xu%20and%20Yu%20Huang%20and%20Jiale%20Cheng%20and%20Yuanming%20Yang%20and%20Jiajun%20Xu%20and%20Yuan%20Wang%20and%20Wenbo%20Duan%20and%20Shen%20Yang%20and%20Qunlin%20Jin%20and%20Shurun%20Li%20and%20Jiayan%20Teng%20and%20Zhuoyi%20Yang%20and%20Wendi%20Zheng%20and%20Xiao%20Liu%20and%20Ming%20Ding%20and%20Xiaohan%20Zhang%20and%20Xiaotao%20Gu%20and%20Shiyu%20Huang%20and%20Minlie%20Huang%20and%20Jie%20Tang%20and%20Yuxiao%20Dong%0AAbstract%3A%20%20%20We%20present%20a%20general%20strategy%20to%20aligning%20visual%20generation%20models%20--%20both%0Aimage%20and%20video%20generation%20--%20with%20human%20preference.%20To%20start%20with%2C%20we%20build%0AVisionReward%20--%20a%20fine-grained%20and%20multi-dimensional%20reward%20model.%20We%20decompose%0Ahuman%20preferences%20in%20images%20and%20videos%20into%20multiple%20dimensions%2C%20each%0Arepresented%20by%20a%20series%20of%20judgment%20questions%2C%20linearly%20weighted%20and%20summed%20to%0Aan%20interpretable%20and%20accurate%20score.%20To%20address%20the%20challenges%20of%20video%20quality%0Aassessment%2C%20we%20systematically%20analyze%20various%20dynamic%20features%20of%20videos%2C%20which%0Ahelps%20VisionReward%20surpass%20VideoScore%20by%2017.2%25%20and%20achieve%20top%20performance%20for%0Avideo%20preference%20prediction.%20Based%20on%20VisionReward%2C%20we%20develop%20a%0Amulti-objective%20preference%20learning%20algorithm%20that%20effectively%20addresses%20the%0Aissue%20of%20confounding%20factors%20within%20preference%20data.%20Our%20approach%20significantly%0Aoutperforms%20existing%20image%20and%20video%20scoring%20methods%20on%20both%20machine%20metrics%0Aand%20human%20evaluation.%20All%20code%20and%20datasets%20are%20provided%20at%0Ahttps%3A//github.com/THUDM/VisionReward.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21059v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisionReward%253A%2520Fine-Grained%2520Multi-Dimensional%2520Human%2520Preference%2520Learning%250A%2520%2520for%2520Image%2520and%2520Video%2520Generation%26entry.906535625%3DJiazheng%2520Xu%2520and%2520Yu%2520Huang%2520and%2520Jiale%2520Cheng%2520and%2520Yuanming%2520Yang%2520and%2520Jiajun%2520Xu%2520and%2520Yuan%2520Wang%2520and%2520Wenbo%2520Duan%2520and%2520Shen%2520Yang%2520and%2520Qunlin%2520Jin%2520and%2520Shurun%2520Li%2520and%2520Jiayan%2520Teng%2520and%2520Zhuoyi%2520Yang%2520and%2520Wendi%2520Zheng%2520and%2520Xiao%2520Liu%2520and%2520Ming%2520Ding%2520and%2520Xiaohan%2520Zhang%2520and%2520Xiaotao%2520Gu%2520and%2520Shiyu%2520Huang%2520and%2520Minlie%2520Huang%2520and%2520Jie%2520Tang%2520and%2520Yuxiao%2520Dong%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520general%2520strategy%2520to%2520aligning%2520visual%2520generation%2520models%2520--%2520both%250Aimage%2520and%2520video%2520generation%2520--%2520with%2520human%2520preference.%2520To%2520start%2520with%252C%2520we%2520build%250AVisionReward%2520--%2520a%2520fine-grained%2520and%2520multi-dimensional%2520reward%2520model.%2520We%2520decompose%250Ahuman%2520preferences%2520in%2520images%2520and%2520videos%2520into%2520multiple%2520dimensions%252C%2520each%250Arepresented%2520by%2520a%2520series%2520of%2520judgment%2520questions%252C%2520linearly%2520weighted%2520and%2520summed%2520to%250Aan%2520interpretable%2520and%2520accurate%2520score.%2520To%2520address%2520the%2520challenges%2520of%2520video%2520quality%250Aassessment%252C%2520we%2520systematically%2520analyze%2520various%2520dynamic%2520features%2520of%2520videos%252C%2520which%250Ahelps%2520VisionReward%2520surpass%2520VideoScore%2520by%252017.2%2525%2520and%2520achieve%2520top%2520performance%2520for%250Avideo%2520preference%2520prediction.%2520Based%2520on%2520VisionReward%252C%2520we%2520develop%2520a%250Amulti-objective%2520preference%2520learning%2520algorithm%2520that%2520effectively%2520addresses%2520the%250Aissue%2520of%2520confounding%2520factors%2520within%2520preference%2520data.%2520Our%2520approach%2520significantly%250Aoutperforms%2520existing%2520image%2520and%2520video%2520scoring%2520methods%2520on%2520both%2520machine%2520metrics%250Aand%2520human%2520evaluation.%2520All%2520code%2520and%2520datasets%2520are%2520provided%2520at%250Ahttps%253A//github.com/THUDM/VisionReward.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21059v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisionReward%3A%20Fine-Grained%20Multi-Dimensional%20Human%20Preference%20Learning%0A%20%20for%20Image%20and%20Video%20Generation&entry.906535625=Jiazheng%20Xu%20and%20Yu%20Huang%20and%20Jiale%20Cheng%20and%20Yuanming%20Yang%20and%20Jiajun%20Xu%20and%20Yuan%20Wang%20and%20Wenbo%20Duan%20and%20Shen%20Yang%20and%20Qunlin%20Jin%20and%20Shurun%20Li%20and%20Jiayan%20Teng%20and%20Zhuoyi%20Yang%20and%20Wendi%20Zheng%20and%20Xiao%20Liu%20and%20Ming%20Ding%20and%20Xiaohan%20Zhang%20and%20Xiaotao%20Gu%20and%20Shiyu%20Huang%20and%20Minlie%20Huang%20and%20Jie%20Tang%20and%20Yuxiao%20Dong&entry.1292438233=%20%20We%20present%20a%20general%20strategy%20to%20aligning%20visual%20generation%20models%20--%20both%0Aimage%20and%20video%20generation%20--%20with%20human%20preference.%20To%20start%20with%2C%20we%20build%0AVisionReward%20--%20a%20fine-grained%20and%20multi-dimensional%20reward%20model.%20We%20decompose%0Ahuman%20preferences%20in%20images%20and%20videos%20into%20multiple%20dimensions%2C%20each%0Arepresented%20by%20a%20series%20of%20judgment%20questions%2C%20linearly%20weighted%20and%20summed%20to%0Aan%20interpretable%20and%20accurate%20score.%20To%20address%20the%20challenges%20of%20video%20quality%0Aassessment%2C%20we%20systematically%20analyze%20various%20dynamic%20features%20of%20videos%2C%20which%0Ahelps%20VisionReward%20surpass%20VideoScore%20by%2017.2%25%20and%20achieve%20top%20performance%20for%0Avideo%20preference%20prediction.%20Based%20on%20VisionReward%2C%20we%20develop%20a%0Amulti-objective%20preference%20learning%20algorithm%20that%20effectively%20addresses%20the%0Aissue%20of%20confounding%20factors%20within%20preference%20data.%20Our%20approach%20significantly%0Aoutperforms%20existing%20image%20and%20video%20scoring%20methods%20on%20both%20machine%20metrics%0Aand%20human%20evaluation.%20All%20code%20and%20datasets%20are%20provided%20at%0Ahttps%3A//github.com/THUDM/VisionReward.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21059v1&entry.124074799=Read"},
{"title": "Hierarchical Banzhaf Interaction for General Video-Language\n  Representation Learning", "author": "Peng Jin and Hao Li and Li Yuan and Shuicheng Yan and Jie Chen", "abstract": "  Multimodal representation learning, with contrastive learning, plays an\nimportant role in the artificial intelligence domain. As an important subfield,\nvideo-language representation learning focuses on learning representations\nusing global semantic interactions between pre-defined video-text pairs.\nHowever, to enhance and refine such coarse-grained global interactions, more\ndetailed interactions are necessary for fine-grained multimodal learning. In\nthis study, we introduce a new approach that models video-text as game players\nusing multivariate cooperative game theory to handle uncertainty during\nfine-grained semantic interactions with diverse granularity, flexible\ncombination, and vague intensity. Specifically, we design the Hierarchical\nBanzhaf Interaction to simulate the fine-grained correspondence between video\nclips and textual words from hierarchical perspectives. Furthermore, to\nmitigate the bias in calculations within Banzhaf Interaction, we propose\nreconstructing the representation through a fusion of single-modal and\ncross-modal components. This reconstructed representation ensures fine\ngranularity comparable to that of the single-modal representation, while also\npreserving the adaptive encoding characteristics of cross-modal representation.\nAdditionally, we extend our original structure into a flexible encoder-decoder\nframework, enabling the model to adapt to various downstream tasks. Extensive\nexperiments on commonly used text-video retrieval, video-question answering,\nand video captioning benchmarks, with superior performance, validate the\neffectiveness and generalization of our method.\n", "link": "http://arxiv.org/abs/2412.20964v1", "date": "2024-12-30", "relevancy": 2.2643, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5684}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5684}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Banzhaf%20Interaction%20for%20General%20Video-Language%0A%20%20Representation%20Learning&body=Title%3A%20Hierarchical%20Banzhaf%20Interaction%20for%20General%20Video-Language%0A%20%20Representation%20Learning%0AAuthor%3A%20Peng%20Jin%20and%20Hao%20Li%20and%20Li%20Yuan%20and%20Shuicheng%20Yan%20and%20Jie%20Chen%0AAbstract%3A%20%20%20Multimodal%20representation%20learning%2C%20with%20contrastive%20learning%2C%20plays%20an%0Aimportant%20role%20in%20the%20artificial%20intelligence%20domain.%20As%20an%20important%20subfield%2C%0Avideo-language%20representation%20learning%20focuses%20on%20learning%20representations%0Ausing%20global%20semantic%20interactions%20between%20pre-defined%20video-text%20pairs.%0AHowever%2C%20to%20enhance%20and%20refine%20such%20coarse-grained%20global%20interactions%2C%20more%0Adetailed%20interactions%20are%20necessary%20for%20fine-grained%20multimodal%20learning.%20In%0Athis%20study%2C%20we%20introduce%20a%20new%20approach%20that%20models%20video-text%20as%20game%20players%0Ausing%20multivariate%20cooperative%20game%20theory%20to%20handle%20uncertainty%20during%0Afine-grained%20semantic%20interactions%20with%20diverse%20granularity%2C%20flexible%0Acombination%2C%20and%20vague%20intensity.%20Specifically%2C%20we%20design%20the%20Hierarchical%0ABanzhaf%20Interaction%20to%20simulate%20the%20fine-grained%20correspondence%20between%20video%0Aclips%20and%20textual%20words%20from%20hierarchical%20perspectives.%20Furthermore%2C%20to%0Amitigate%20the%20bias%20in%20calculations%20within%20Banzhaf%20Interaction%2C%20we%20propose%0Areconstructing%20the%20representation%20through%20a%20fusion%20of%20single-modal%20and%0Across-modal%20components.%20This%20reconstructed%20representation%20ensures%20fine%0Agranularity%20comparable%20to%20that%20of%20the%20single-modal%20representation%2C%20while%20also%0Apreserving%20the%20adaptive%20encoding%20characteristics%20of%20cross-modal%20representation.%0AAdditionally%2C%20we%20extend%20our%20original%20structure%20into%20a%20flexible%20encoder-decoder%0Aframework%2C%20enabling%20the%20model%20to%20adapt%20to%20various%20downstream%20tasks.%20Extensive%0Aexperiments%20on%20commonly%20used%20text-video%20retrieval%2C%20video-question%20answering%2C%0Aand%20video%20captioning%20benchmarks%2C%20with%20superior%20performance%2C%20validate%20the%0Aeffectiveness%20and%20generalization%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20964v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Banzhaf%2520Interaction%2520for%2520General%2520Video-Language%250A%2520%2520Representation%2520Learning%26entry.906535625%3DPeng%2520Jin%2520and%2520Hao%2520Li%2520and%2520Li%2520Yuan%2520and%2520Shuicheng%2520Yan%2520and%2520Jie%2520Chen%26entry.1292438233%3D%2520%2520Multimodal%2520representation%2520learning%252C%2520with%2520contrastive%2520learning%252C%2520plays%2520an%250Aimportant%2520role%2520in%2520the%2520artificial%2520intelligence%2520domain.%2520As%2520an%2520important%2520subfield%252C%250Avideo-language%2520representation%2520learning%2520focuses%2520on%2520learning%2520representations%250Ausing%2520global%2520semantic%2520interactions%2520between%2520pre-defined%2520video-text%2520pairs.%250AHowever%252C%2520to%2520enhance%2520and%2520refine%2520such%2520coarse-grained%2520global%2520interactions%252C%2520more%250Adetailed%2520interactions%2520are%2520necessary%2520for%2520fine-grained%2520multimodal%2520learning.%2520In%250Athis%2520study%252C%2520we%2520introduce%2520a%2520new%2520approach%2520that%2520models%2520video-text%2520as%2520game%2520players%250Ausing%2520multivariate%2520cooperative%2520game%2520theory%2520to%2520handle%2520uncertainty%2520during%250Afine-grained%2520semantic%2520interactions%2520with%2520diverse%2520granularity%252C%2520flexible%250Acombination%252C%2520and%2520vague%2520intensity.%2520Specifically%252C%2520we%2520design%2520the%2520Hierarchical%250ABanzhaf%2520Interaction%2520to%2520simulate%2520the%2520fine-grained%2520correspondence%2520between%2520video%250Aclips%2520and%2520textual%2520words%2520from%2520hierarchical%2520perspectives.%2520Furthermore%252C%2520to%250Amitigate%2520the%2520bias%2520in%2520calculations%2520within%2520Banzhaf%2520Interaction%252C%2520we%2520propose%250Areconstructing%2520the%2520representation%2520through%2520a%2520fusion%2520of%2520single-modal%2520and%250Across-modal%2520components.%2520This%2520reconstructed%2520representation%2520ensures%2520fine%250Agranularity%2520comparable%2520to%2520that%2520of%2520the%2520single-modal%2520representation%252C%2520while%2520also%250Apreserving%2520the%2520adaptive%2520encoding%2520characteristics%2520of%2520cross-modal%2520representation.%250AAdditionally%252C%2520we%2520extend%2520our%2520original%2520structure%2520into%2520a%2520flexible%2520encoder-decoder%250Aframework%252C%2520enabling%2520the%2520model%2520to%2520adapt%2520to%2520various%2520downstream%2520tasks.%2520Extensive%250Aexperiments%2520on%2520commonly%2520used%2520text-video%2520retrieval%252C%2520video-question%2520answering%252C%250Aand%2520video%2520captioning%2520benchmarks%252C%2520with%2520superior%2520performance%252C%2520validate%2520the%250Aeffectiveness%2520and%2520generalization%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20964v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Banzhaf%20Interaction%20for%20General%20Video-Language%0A%20%20Representation%20Learning&entry.906535625=Peng%20Jin%20and%20Hao%20Li%20and%20Li%20Yuan%20and%20Shuicheng%20Yan%20and%20Jie%20Chen&entry.1292438233=%20%20Multimodal%20representation%20learning%2C%20with%20contrastive%20learning%2C%20plays%20an%0Aimportant%20role%20in%20the%20artificial%20intelligence%20domain.%20As%20an%20important%20subfield%2C%0Avideo-language%20representation%20learning%20focuses%20on%20learning%20representations%0Ausing%20global%20semantic%20interactions%20between%20pre-defined%20video-text%20pairs.%0AHowever%2C%20to%20enhance%20and%20refine%20such%20coarse-grained%20global%20interactions%2C%20more%0Adetailed%20interactions%20are%20necessary%20for%20fine-grained%20multimodal%20learning.%20In%0Athis%20study%2C%20we%20introduce%20a%20new%20approach%20that%20models%20video-text%20as%20game%20players%0Ausing%20multivariate%20cooperative%20game%20theory%20to%20handle%20uncertainty%20during%0Afine-grained%20semantic%20interactions%20with%20diverse%20granularity%2C%20flexible%0Acombination%2C%20and%20vague%20intensity.%20Specifically%2C%20we%20design%20the%20Hierarchical%0ABanzhaf%20Interaction%20to%20simulate%20the%20fine-grained%20correspondence%20between%20video%0Aclips%20and%20textual%20words%20from%20hierarchical%20perspectives.%20Furthermore%2C%20to%0Amitigate%20the%20bias%20in%20calculations%20within%20Banzhaf%20Interaction%2C%20we%20propose%0Areconstructing%20the%20representation%20through%20a%20fusion%20of%20single-modal%20and%0Across-modal%20components.%20This%20reconstructed%20representation%20ensures%20fine%0Agranularity%20comparable%20to%20that%20of%20the%20single-modal%20representation%2C%20while%20also%0Apreserving%20the%20adaptive%20encoding%20characteristics%20of%20cross-modal%20representation.%0AAdditionally%2C%20we%20extend%20our%20original%20structure%20into%20a%20flexible%20encoder-decoder%0Aframework%2C%20enabling%20the%20model%20to%20adapt%20to%20various%20downstream%20tasks.%20Extensive%0Aexperiments%20on%20commonly%20used%20text-video%20retrieval%2C%20video-question%20answering%2C%0Aand%20video%20captioning%20benchmarks%2C%20with%20superior%20performance%2C%20validate%20the%0Aeffectiveness%20and%20generalization%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20964v1&entry.124074799=Read"},
{"title": "Aviary: training language agents on challenging scientific tasks", "author": "Siddharth Narayanan and James D. Braza and Ryan-Rhys Griffiths and Manu Ponnapati and Albert Bou and Jon Laurent and Ori Kabeli and Geemi Wellawatte and Sam Cox and Samuel G. Rodriques and Andrew D. White", "abstract": "  Solving complex real-world tasks requires cycles of actions and observations.\nThis is particularly true in science, where tasks require many cycles of\nanalysis, tool use, and experimentation. Language agents are promising for\nautomating intellectual tasks in science because they can interact with tools\nvia natural language or code. Yet their flexibility creates conceptual and\npractical challenges for software implementations, since agents may comprise\nnon-standard components such as internal reasoning, planning, tool usage, as\nwell as the inherent stochasticity of temperature-sampled language models.\nHere, we introduce Aviary, an extensible gymnasium for language agents. We\nformalize agents as policies solving language-grounded partially observable\nMarkov decision processes, which we term language decision processes. We then\nimplement five environments, including three challenging scientific\nenvironments: (1) manipulating DNA constructs for molecular cloning, (2)\nanswering research questions by accessing scientific literature, and (3)\nengineering protein stability. These environments were selected for their focus\non multi-step reasoning and their relevance to contemporary biology research.\nFinally, with online training and scaling inference-time compute, we show that\nlanguage agents backed by open-source, non-frontier LLMs can match and exceed\nboth frontier LLM agents and human experts on multiple tasks at up to 100x\nlower inference cost.\n", "link": "http://arxiv.org/abs/2412.21154v1", "date": "2024-12-30", "relevancy": 2.2625, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5684}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5676}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aviary%3A%20training%20language%20agents%20on%20challenging%20scientific%20tasks&body=Title%3A%20Aviary%3A%20training%20language%20agents%20on%20challenging%20scientific%20tasks%0AAuthor%3A%20Siddharth%20Narayanan%20and%20James%20D.%20Braza%20and%20Ryan-Rhys%20Griffiths%20and%20Manu%20Ponnapati%20and%20Albert%20Bou%20and%20Jon%20Laurent%20and%20Ori%20Kabeli%20and%20Geemi%20Wellawatte%20and%20Sam%20Cox%20and%20Samuel%20G.%20Rodriques%20and%20Andrew%20D.%20White%0AAbstract%3A%20%20%20Solving%20complex%20real-world%20tasks%20requires%20cycles%20of%20actions%20and%20observations.%0AThis%20is%20particularly%20true%20in%20science%2C%20where%20tasks%20require%20many%20cycles%20of%0Aanalysis%2C%20tool%20use%2C%20and%20experimentation.%20Language%20agents%20are%20promising%20for%0Aautomating%20intellectual%20tasks%20in%20science%20because%20they%20can%20interact%20with%20tools%0Avia%20natural%20language%20or%20code.%20Yet%20their%20flexibility%20creates%20conceptual%20and%0Apractical%20challenges%20for%20software%20implementations%2C%20since%20agents%20may%20comprise%0Anon-standard%20components%20such%20as%20internal%20reasoning%2C%20planning%2C%20tool%20usage%2C%20as%0Awell%20as%20the%20inherent%20stochasticity%20of%20temperature-sampled%20language%20models.%0AHere%2C%20we%20introduce%20Aviary%2C%20an%20extensible%20gymnasium%20for%20language%20agents.%20We%0Aformalize%20agents%20as%20policies%20solving%20language-grounded%20partially%20observable%0AMarkov%20decision%20processes%2C%20which%20we%20term%20language%20decision%20processes.%20We%20then%0Aimplement%20five%20environments%2C%20including%20three%20challenging%20scientific%0Aenvironments%3A%20%281%29%20manipulating%20DNA%20constructs%20for%20molecular%20cloning%2C%20%282%29%0Aanswering%20research%20questions%20by%20accessing%20scientific%20literature%2C%20and%20%283%29%0Aengineering%20protein%20stability.%20These%20environments%20were%20selected%20for%20their%20focus%0Aon%20multi-step%20reasoning%20and%20their%20relevance%20to%20contemporary%20biology%20research.%0AFinally%2C%20with%20online%20training%20and%20scaling%20inference-time%20compute%2C%20we%20show%20that%0Alanguage%20agents%20backed%20by%20open-source%2C%20non-frontier%20LLMs%20can%20match%20and%20exceed%0Aboth%20frontier%20LLM%20agents%20and%20human%20experts%20on%20multiple%20tasks%20at%20up%20to%20100x%0Alower%20inference%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21154v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAviary%253A%2520training%2520language%2520agents%2520on%2520challenging%2520scientific%2520tasks%26entry.906535625%3DSiddharth%2520Narayanan%2520and%2520James%2520D.%2520Braza%2520and%2520Ryan-Rhys%2520Griffiths%2520and%2520Manu%2520Ponnapati%2520and%2520Albert%2520Bou%2520and%2520Jon%2520Laurent%2520and%2520Ori%2520Kabeli%2520and%2520Geemi%2520Wellawatte%2520and%2520Sam%2520Cox%2520and%2520Samuel%2520G.%2520Rodriques%2520and%2520Andrew%2520D.%2520White%26entry.1292438233%3D%2520%2520Solving%2520complex%2520real-world%2520tasks%2520requires%2520cycles%2520of%2520actions%2520and%2520observations.%250AThis%2520is%2520particularly%2520true%2520in%2520science%252C%2520where%2520tasks%2520require%2520many%2520cycles%2520of%250Aanalysis%252C%2520tool%2520use%252C%2520and%2520experimentation.%2520Language%2520agents%2520are%2520promising%2520for%250Aautomating%2520intellectual%2520tasks%2520in%2520science%2520because%2520they%2520can%2520interact%2520with%2520tools%250Avia%2520natural%2520language%2520or%2520code.%2520Yet%2520their%2520flexibility%2520creates%2520conceptual%2520and%250Apractical%2520challenges%2520for%2520software%2520implementations%252C%2520since%2520agents%2520may%2520comprise%250Anon-standard%2520components%2520such%2520as%2520internal%2520reasoning%252C%2520planning%252C%2520tool%2520usage%252C%2520as%250Awell%2520as%2520the%2520inherent%2520stochasticity%2520of%2520temperature-sampled%2520language%2520models.%250AHere%252C%2520we%2520introduce%2520Aviary%252C%2520an%2520extensible%2520gymnasium%2520for%2520language%2520agents.%2520We%250Aformalize%2520agents%2520as%2520policies%2520solving%2520language-grounded%2520partially%2520observable%250AMarkov%2520decision%2520processes%252C%2520which%2520we%2520term%2520language%2520decision%2520processes.%2520We%2520then%250Aimplement%2520five%2520environments%252C%2520including%2520three%2520challenging%2520scientific%250Aenvironments%253A%2520%25281%2529%2520manipulating%2520DNA%2520constructs%2520for%2520molecular%2520cloning%252C%2520%25282%2529%250Aanswering%2520research%2520questions%2520by%2520accessing%2520scientific%2520literature%252C%2520and%2520%25283%2529%250Aengineering%2520protein%2520stability.%2520These%2520environments%2520were%2520selected%2520for%2520their%2520focus%250Aon%2520multi-step%2520reasoning%2520and%2520their%2520relevance%2520to%2520contemporary%2520biology%2520research.%250AFinally%252C%2520with%2520online%2520training%2520and%2520scaling%2520inference-time%2520compute%252C%2520we%2520show%2520that%250Alanguage%2520agents%2520backed%2520by%2520open-source%252C%2520non-frontier%2520LLMs%2520can%2520match%2520and%2520exceed%250Aboth%2520frontier%2520LLM%2520agents%2520and%2520human%2520experts%2520on%2520multiple%2520tasks%2520at%2520up%2520to%2520100x%250Alower%2520inference%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21154v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aviary%3A%20training%20language%20agents%20on%20challenging%20scientific%20tasks&entry.906535625=Siddharth%20Narayanan%20and%20James%20D.%20Braza%20and%20Ryan-Rhys%20Griffiths%20and%20Manu%20Ponnapati%20and%20Albert%20Bou%20and%20Jon%20Laurent%20and%20Ori%20Kabeli%20and%20Geemi%20Wellawatte%20and%20Sam%20Cox%20and%20Samuel%20G.%20Rodriques%20and%20Andrew%20D.%20White&entry.1292438233=%20%20Solving%20complex%20real-world%20tasks%20requires%20cycles%20of%20actions%20and%20observations.%0AThis%20is%20particularly%20true%20in%20science%2C%20where%20tasks%20require%20many%20cycles%20of%0Aanalysis%2C%20tool%20use%2C%20and%20experimentation.%20Language%20agents%20are%20promising%20for%0Aautomating%20intellectual%20tasks%20in%20science%20because%20they%20can%20interact%20with%20tools%0Avia%20natural%20language%20or%20code.%20Yet%20their%20flexibility%20creates%20conceptual%20and%0Apractical%20challenges%20for%20software%20implementations%2C%20since%20agents%20may%20comprise%0Anon-standard%20components%20such%20as%20internal%20reasoning%2C%20planning%2C%20tool%20usage%2C%20as%0Awell%20as%20the%20inherent%20stochasticity%20of%20temperature-sampled%20language%20models.%0AHere%2C%20we%20introduce%20Aviary%2C%20an%20extensible%20gymnasium%20for%20language%20agents.%20We%0Aformalize%20agents%20as%20policies%20solving%20language-grounded%20partially%20observable%0AMarkov%20decision%20processes%2C%20which%20we%20term%20language%20decision%20processes.%20We%20then%0Aimplement%20five%20environments%2C%20including%20three%20challenging%20scientific%0Aenvironments%3A%20%281%29%20manipulating%20DNA%20constructs%20for%20molecular%20cloning%2C%20%282%29%0Aanswering%20research%20questions%20by%20accessing%20scientific%20literature%2C%20and%20%283%29%0Aengineering%20protein%20stability.%20These%20environments%20were%20selected%20for%20their%20focus%0Aon%20multi-step%20reasoning%20and%20their%20relevance%20to%20contemporary%20biology%20research.%0AFinally%2C%20with%20online%20training%20and%20scaling%20inference-time%20compute%2C%20we%20show%20that%0Alanguage%20agents%20backed%20by%20open-source%2C%20non-frontier%20LLMs%20can%20match%20and%20exceed%0Aboth%20frontier%20LLM%20agents%20and%20human%20experts%20on%20multiple%20tasks%20at%20up%20to%20100x%0Alower%20inference%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21154v1&entry.124074799=Read"},
{"title": "Scaling Capability in Token Space: An Analysis of Large Vision Language\n  Model", "author": "Tenghui Li and Guoxu Zhou and Xuyang Zhao and Qibin Zhao", "abstract": "  The scaling capability has been widely validated in neural language models\nwith respect to the number of parameters and the size of training data.\n  One important question is that does the scaling capability also exists\nsimilarly with respect to the number of vision tokens in large vision language\nModel?\n  This study fills the gap by investigating the relationship between the number\nof vision tokens and the performance on vision-language models.\n  Our theoretical analysis and empirical evaluations demonstrate that the model\nexhibits scalable performance \\(S(N_l)\\) with respect to the number of vision\ntokens \\(N_l\\), characterized by the relationship \\(S(N_l) \\approx\n(c/N_l)^{\\alpha}\\).\n  Furthermore, we also investigate the impact of a fusion mechanism that\nintegrates the user's question with vision tokens.\n  The results reveal two key findings.\n  First, the scaling capability remains intact with the incorporation of the\nfusion mechanism.\n  Second, the fusion mechanism enhances model performance, particularly when\nthe user's question is task-specific and relevant.\n  The analysis, conducted on fifteen diverse benchmarks spanning a broad range\nof tasks and domains, validates the effectiveness of the proposed approach.\n", "link": "http://arxiv.org/abs/2412.18387v2", "date": "2024-12-30", "relevancy": 2.2323, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5622}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5622}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5373}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20Capability%20in%20Token%20Space%3A%20An%20Analysis%20of%20Large%20Vision%20Language%0A%20%20Model&body=Title%3A%20Scaling%20Capability%20in%20Token%20Space%3A%20An%20Analysis%20of%20Large%20Vision%20Language%0A%20%20Model%0AAuthor%3A%20Tenghui%20Li%20and%20Guoxu%20Zhou%20and%20Xuyang%20Zhao%20and%20Qibin%20Zhao%0AAbstract%3A%20%20%20The%20scaling%20capability%20has%20been%20widely%20validated%20in%20neural%20language%20models%0Awith%20respect%20to%20the%20number%20of%20parameters%20and%20the%20size%20of%20training%20data.%0A%20%20One%20important%20question%20is%20that%20does%20the%20scaling%20capability%20also%20exists%0Asimilarly%20with%20respect%20to%20the%20number%20of%20vision%20tokens%20in%20large%20vision%20language%0AModel%3F%0A%20%20This%20study%20fills%20the%20gap%20by%20investigating%20the%20relationship%20between%20the%20number%0Aof%20vision%20tokens%20and%20the%20performance%20on%20vision-language%20models.%0A%20%20Our%20theoretical%20analysis%20and%20empirical%20evaluations%20demonstrate%20that%20the%20model%0Aexhibits%20scalable%20performance%20%5C%28S%28N_l%29%5C%29%20with%20respect%20to%20the%20number%20of%20vision%0Atokens%20%5C%28N_l%5C%29%2C%20characterized%20by%20the%20relationship%20%5C%28S%28N_l%29%20%5Capprox%0A%28c/N_l%29%5E%7B%5Calpha%7D%5C%29.%0A%20%20Furthermore%2C%20we%20also%20investigate%20the%20impact%20of%20a%20fusion%20mechanism%20that%0Aintegrates%20the%20user%27s%20question%20with%20vision%20tokens.%0A%20%20The%20results%20reveal%20two%20key%20findings.%0A%20%20First%2C%20the%20scaling%20capability%20remains%20intact%20with%20the%20incorporation%20of%20the%0Afusion%20mechanism.%0A%20%20Second%2C%20the%20fusion%20mechanism%20enhances%20model%20performance%2C%20particularly%20when%0Athe%20user%27s%20question%20is%20task-specific%20and%20relevant.%0A%20%20The%20analysis%2C%20conducted%20on%20fifteen%20diverse%20benchmarks%20spanning%20a%20broad%20range%0Aof%20tasks%20and%20domains%2C%20validates%20the%20effectiveness%20of%20the%20proposed%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18387v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520Capability%2520in%2520Token%2520Space%253A%2520An%2520Analysis%2520of%2520Large%2520Vision%2520Language%250A%2520%2520Model%26entry.906535625%3DTenghui%2520Li%2520and%2520Guoxu%2520Zhou%2520and%2520Xuyang%2520Zhao%2520and%2520Qibin%2520Zhao%26entry.1292438233%3D%2520%2520The%2520scaling%2520capability%2520has%2520been%2520widely%2520validated%2520in%2520neural%2520language%2520models%250Awith%2520respect%2520to%2520the%2520number%2520of%2520parameters%2520and%2520the%2520size%2520of%2520training%2520data.%250A%2520%2520One%2520important%2520question%2520is%2520that%2520does%2520the%2520scaling%2520capability%2520also%2520exists%250Asimilarly%2520with%2520respect%2520to%2520the%2520number%2520of%2520vision%2520tokens%2520in%2520large%2520vision%2520language%250AModel%253F%250A%2520%2520This%2520study%2520fills%2520the%2520gap%2520by%2520investigating%2520the%2520relationship%2520between%2520the%2520number%250Aof%2520vision%2520tokens%2520and%2520the%2520performance%2520on%2520vision-language%2520models.%250A%2520%2520Our%2520theoretical%2520analysis%2520and%2520empirical%2520evaluations%2520demonstrate%2520that%2520the%2520model%250Aexhibits%2520scalable%2520performance%2520%255C%2528S%2528N_l%2529%255C%2529%2520with%2520respect%2520to%2520the%2520number%2520of%2520vision%250Atokens%2520%255C%2528N_l%255C%2529%252C%2520characterized%2520by%2520the%2520relationship%2520%255C%2528S%2528N_l%2529%2520%255Capprox%250A%2528c/N_l%2529%255E%257B%255Calpha%257D%255C%2529.%250A%2520%2520Furthermore%252C%2520we%2520also%2520investigate%2520the%2520impact%2520of%2520a%2520fusion%2520mechanism%2520that%250Aintegrates%2520the%2520user%2527s%2520question%2520with%2520vision%2520tokens.%250A%2520%2520The%2520results%2520reveal%2520two%2520key%2520findings.%250A%2520%2520First%252C%2520the%2520scaling%2520capability%2520remains%2520intact%2520with%2520the%2520incorporation%2520of%2520the%250Afusion%2520mechanism.%250A%2520%2520Second%252C%2520the%2520fusion%2520mechanism%2520enhances%2520model%2520performance%252C%2520particularly%2520when%250Athe%2520user%2527s%2520question%2520is%2520task-specific%2520and%2520relevant.%250A%2520%2520The%2520analysis%252C%2520conducted%2520on%2520fifteen%2520diverse%2520benchmarks%2520spanning%2520a%2520broad%2520range%250Aof%2520tasks%2520and%2520domains%252C%2520validates%2520the%2520effectiveness%2520of%2520the%2520proposed%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18387v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20Capability%20in%20Token%20Space%3A%20An%20Analysis%20of%20Large%20Vision%20Language%0A%20%20Model&entry.906535625=Tenghui%20Li%20and%20Guoxu%20Zhou%20and%20Xuyang%20Zhao%20and%20Qibin%20Zhao&entry.1292438233=%20%20The%20scaling%20capability%20has%20been%20widely%20validated%20in%20neural%20language%20models%0Awith%20respect%20to%20the%20number%20of%20parameters%20and%20the%20size%20of%20training%20data.%0A%20%20One%20important%20question%20is%20that%20does%20the%20scaling%20capability%20also%20exists%0Asimilarly%20with%20respect%20to%20the%20number%20of%20vision%20tokens%20in%20large%20vision%20language%0AModel%3F%0A%20%20This%20study%20fills%20the%20gap%20by%20investigating%20the%20relationship%20between%20the%20number%0Aof%20vision%20tokens%20and%20the%20performance%20on%20vision-language%20models.%0A%20%20Our%20theoretical%20analysis%20and%20empirical%20evaluations%20demonstrate%20that%20the%20model%0Aexhibits%20scalable%20performance%20%5C%28S%28N_l%29%5C%29%20with%20respect%20to%20the%20number%20of%20vision%0Atokens%20%5C%28N_l%5C%29%2C%20characterized%20by%20the%20relationship%20%5C%28S%28N_l%29%20%5Capprox%0A%28c/N_l%29%5E%7B%5Calpha%7D%5C%29.%0A%20%20Furthermore%2C%20we%20also%20investigate%20the%20impact%20of%20a%20fusion%20mechanism%20that%0Aintegrates%20the%20user%27s%20question%20with%20vision%20tokens.%0A%20%20The%20results%20reveal%20two%20key%20findings.%0A%20%20First%2C%20the%20scaling%20capability%20remains%20intact%20with%20the%20incorporation%20of%20the%0Afusion%20mechanism.%0A%20%20Second%2C%20the%20fusion%20mechanism%20enhances%20model%20performance%2C%20particularly%20when%0Athe%20user%27s%20question%20is%20task-specific%20and%20relevant.%0A%20%20The%20analysis%2C%20conducted%20on%20fifteen%20diverse%20benchmarks%20spanning%20a%20broad%20range%0Aof%20tasks%20and%20domains%2C%20validates%20the%20effectiveness%20of%20the%20proposed%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18387v2&entry.124074799=Read"},
{"title": "UnrealZoo: Enriching Photo-realistic Virtual Worlds for Embodied AI", "author": "Fangwei Zhong and Kui Wu and Churan Wang and Hao Chen and Hai Ci and Zhoujun Li and Yizhou Wang", "abstract": "  We introduce UnrealZoo, a rich collection of photo-realistic 3D virtual\nworlds built on Unreal Engine, designed to reflect the complexity and\nvariability of the open worlds. Additionally, we offer a variety of playable\nentities for embodied AI agents. Based on UnrealCV, we provide a suite of\neasy-to-use Python APIs and tools for various potential applications, such as\ndata collection, environment augmentation, distributed training, and\nbenchmarking. We optimize the rendering and communication efficiency of\nUnrealCV to support advanced applications, such as multi-agent interaction. Our\nexperiments benchmark agents in various complex scenes, focusing on visual\nnavigation and tracking, which are fundamental capabilities for embodied visual\nintelligence. The results yield valuable insights into the advantages of\ndiverse training environments for reinforcement learning (RL) agents and the\nchallenges faced by current embodied vision agents, including those based on RL\nand large vision-language models (VLMs), in open worlds. These challenges\ninvolve latency in closed-loop control in dynamic scenes and reasoning about 3D\nspatial structures in unstructured terrain.\n", "link": "http://arxiv.org/abs/2412.20977v1", "date": "2024-12-30", "relevancy": 2.2176, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5557}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5557}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UnrealZoo%3A%20Enriching%20Photo-realistic%20Virtual%20Worlds%20for%20Embodied%20AI&body=Title%3A%20UnrealZoo%3A%20Enriching%20Photo-realistic%20Virtual%20Worlds%20for%20Embodied%20AI%0AAuthor%3A%20Fangwei%20Zhong%20and%20Kui%20Wu%20and%20Churan%20Wang%20and%20Hao%20Chen%20and%20Hai%20Ci%20and%20Zhoujun%20Li%20and%20Yizhou%20Wang%0AAbstract%3A%20%20%20We%20introduce%20UnrealZoo%2C%20a%20rich%20collection%20of%20photo-realistic%203D%20virtual%0Aworlds%20built%20on%20Unreal%20Engine%2C%20designed%20to%20reflect%20the%20complexity%20and%0Avariability%20of%20the%20open%20worlds.%20Additionally%2C%20we%20offer%20a%20variety%20of%20playable%0Aentities%20for%20embodied%20AI%20agents.%20Based%20on%20UnrealCV%2C%20we%20provide%20a%20suite%20of%0Aeasy-to-use%20Python%20APIs%20and%20tools%20for%20various%20potential%20applications%2C%20such%20as%0Adata%20collection%2C%20environment%20augmentation%2C%20distributed%20training%2C%20and%0Abenchmarking.%20We%20optimize%20the%20rendering%20and%20communication%20efficiency%20of%0AUnrealCV%20to%20support%20advanced%20applications%2C%20such%20as%20multi-agent%20interaction.%20Our%0Aexperiments%20benchmark%20agents%20in%20various%20complex%20scenes%2C%20focusing%20on%20visual%0Anavigation%20and%20tracking%2C%20which%20are%20fundamental%20capabilities%20for%20embodied%20visual%0Aintelligence.%20The%20results%20yield%20valuable%20insights%20into%20the%20advantages%20of%0Adiverse%20training%20environments%20for%20reinforcement%20learning%20%28RL%29%20agents%20and%20the%0Achallenges%20faced%20by%20current%20embodied%20vision%20agents%2C%20including%20those%20based%20on%20RL%0Aand%20large%20vision-language%20models%20%28VLMs%29%2C%20in%20open%20worlds.%20These%20challenges%0Ainvolve%20latency%20in%20closed-loop%20control%20in%20dynamic%20scenes%20and%20reasoning%20about%203D%0Aspatial%20structures%20in%20unstructured%20terrain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20977v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnrealZoo%253A%2520Enriching%2520Photo-realistic%2520Virtual%2520Worlds%2520for%2520Embodied%2520AI%26entry.906535625%3DFangwei%2520Zhong%2520and%2520Kui%2520Wu%2520and%2520Churan%2520Wang%2520and%2520Hao%2520Chen%2520and%2520Hai%2520Ci%2520and%2520Zhoujun%2520Li%2520and%2520Yizhou%2520Wang%26entry.1292438233%3D%2520%2520We%2520introduce%2520UnrealZoo%252C%2520a%2520rich%2520collection%2520of%2520photo-realistic%25203D%2520virtual%250Aworlds%2520built%2520on%2520Unreal%2520Engine%252C%2520designed%2520to%2520reflect%2520the%2520complexity%2520and%250Avariability%2520of%2520the%2520open%2520worlds.%2520Additionally%252C%2520we%2520offer%2520a%2520variety%2520of%2520playable%250Aentities%2520for%2520embodied%2520AI%2520agents.%2520Based%2520on%2520UnrealCV%252C%2520we%2520provide%2520a%2520suite%2520of%250Aeasy-to-use%2520Python%2520APIs%2520and%2520tools%2520for%2520various%2520potential%2520applications%252C%2520such%2520as%250Adata%2520collection%252C%2520environment%2520augmentation%252C%2520distributed%2520training%252C%2520and%250Abenchmarking.%2520We%2520optimize%2520the%2520rendering%2520and%2520communication%2520efficiency%2520of%250AUnrealCV%2520to%2520support%2520advanced%2520applications%252C%2520such%2520as%2520multi-agent%2520interaction.%2520Our%250Aexperiments%2520benchmark%2520agents%2520in%2520various%2520complex%2520scenes%252C%2520focusing%2520on%2520visual%250Anavigation%2520and%2520tracking%252C%2520which%2520are%2520fundamental%2520capabilities%2520for%2520embodied%2520visual%250Aintelligence.%2520The%2520results%2520yield%2520valuable%2520insights%2520into%2520the%2520advantages%2520of%250Adiverse%2520training%2520environments%2520for%2520reinforcement%2520learning%2520%2528RL%2529%2520agents%2520and%2520the%250Achallenges%2520faced%2520by%2520current%2520embodied%2520vision%2520agents%252C%2520including%2520those%2520based%2520on%2520RL%250Aand%2520large%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520in%2520open%2520worlds.%2520These%2520challenges%250Ainvolve%2520latency%2520in%2520closed-loop%2520control%2520in%2520dynamic%2520scenes%2520and%2520reasoning%2520about%25203D%250Aspatial%2520structures%2520in%2520unstructured%2520terrain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20977v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UnrealZoo%3A%20Enriching%20Photo-realistic%20Virtual%20Worlds%20for%20Embodied%20AI&entry.906535625=Fangwei%20Zhong%20and%20Kui%20Wu%20and%20Churan%20Wang%20and%20Hao%20Chen%20and%20Hai%20Ci%20and%20Zhoujun%20Li%20and%20Yizhou%20Wang&entry.1292438233=%20%20We%20introduce%20UnrealZoo%2C%20a%20rich%20collection%20of%20photo-realistic%203D%20virtual%0Aworlds%20built%20on%20Unreal%20Engine%2C%20designed%20to%20reflect%20the%20complexity%20and%0Avariability%20of%20the%20open%20worlds.%20Additionally%2C%20we%20offer%20a%20variety%20of%20playable%0Aentities%20for%20embodied%20AI%20agents.%20Based%20on%20UnrealCV%2C%20we%20provide%20a%20suite%20of%0Aeasy-to-use%20Python%20APIs%20and%20tools%20for%20various%20potential%20applications%2C%20such%20as%0Adata%20collection%2C%20environment%20augmentation%2C%20distributed%20training%2C%20and%0Abenchmarking.%20We%20optimize%20the%20rendering%20and%20communication%20efficiency%20of%0AUnrealCV%20to%20support%20advanced%20applications%2C%20such%20as%20multi-agent%20interaction.%20Our%0Aexperiments%20benchmark%20agents%20in%20various%20complex%20scenes%2C%20focusing%20on%20visual%0Anavigation%20and%20tracking%2C%20which%20are%20fundamental%20capabilities%20for%20embodied%20visual%0Aintelligence.%20The%20results%20yield%20valuable%20insights%20into%20the%20advantages%20of%0Adiverse%20training%20environments%20for%20reinforcement%20learning%20%28RL%29%20agents%20and%20the%0Achallenges%20faced%20by%20current%20embodied%20vision%20agents%2C%20including%20those%20based%20on%20RL%0Aand%20large%20vision-language%20models%20%28VLMs%29%2C%20in%20open%20worlds.%20These%20challenges%0Ainvolve%20latency%20in%20closed-loop%20control%20in%20dynamic%20scenes%20and%20reasoning%20about%203D%0Aspatial%20structures%20in%20unstructured%20terrain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20977v1&entry.124074799=Read"},
{"title": "A Graph Neural Network deep-dive into successful counterattacks", "author": "Joris Bekkers and Amod Sahasrabudhe", "abstract": "  A counterattack in soccer is a high speed, high intensity direct attack that\ncan occur when a team transitions from a defensive state to an attacking state\nafter regaining possession of the ball. The aim is to create a goal-scoring\nopportunity by convering a lot of ground with minimal passes before the\nopposing team can recover their defensive shape. The purpose of this research\nis to build gender-specific Graph Neural Networks to model the likelihood of a\ncounterattack being successful and uncover what factors make them successful in\nprofessional soccer. These models are trained on a total of 20863 frames of\nsynchronized on-ball event and spatiotemporal (broadcast) tracking data. This\ndataset is derived from 632 games of MLS (2022), NWSL (2022) and international\nsoccer (2020-2022). With this data we demonstrate that gender-specific Graph\nNeural Networks outperform architecturally identical gender-ambiguous models in\npredicting the successful outcome of counterattacks. We show, using Permutation\nFeature Importance, that byline to byline speed, angle to the goal, angle to\nthe ball and sideline to sideline speed are the node features with the highest\nimpact on model performance. Additionally, we offer some illustrative examples\non how to navigate the infinite solution search space to aid in identifying\nimprovements for player decision making.\n  This research is accompanied by an open-source repository containing all data\nand code, and it is also accompanied by an open-source Python package which\nsimplifies converting spatiotemporal data into graphs. This package also\nfacilitates testing, validation, training and prediction with this data. This\nshould allow the reader to replicate and improve upon our research more easily.\n", "link": "http://arxiv.org/abs/2411.17450v2", "date": "2024-12-30", "relevancy": 2.1814, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.442}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4372}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4296}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Graph%20Neural%20Network%20deep-dive%20into%20successful%20counterattacks&body=Title%3A%20A%20Graph%20Neural%20Network%20deep-dive%20into%20successful%20counterattacks%0AAuthor%3A%20Joris%20Bekkers%20and%20Amod%20Sahasrabudhe%0AAbstract%3A%20%20%20A%20counterattack%20in%20soccer%20is%20a%20high%20speed%2C%20high%20intensity%20direct%20attack%20that%0Acan%20occur%20when%20a%20team%20transitions%20from%20a%20defensive%20state%20to%20an%20attacking%20state%0Aafter%20regaining%20possession%20of%20the%20ball.%20The%20aim%20is%20to%20create%20a%20goal-scoring%0Aopportunity%20by%20convering%20a%20lot%20of%20ground%20with%20minimal%20passes%20before%20the%0Aopposing%20team%20can%20recover%20their%20defensive%20shape.%20The%20purpose%20of%20this%20research%0Ais%20to%20build%20gender-specific%20Graph%20Neural%20Networks%20to%20model%20the%20likelihood%20of%20a%0Acounterattack%20being%20successful%20and%20uncover%20what%20factors%20make%20them%20successful%20in%0Aprofessional%20soccer.%20These%20models%20are%20trained%20on%20a%20total%20of%2020863%20frames%20of%0Asynchronized%20on-ball%20event%20and%20spatiotemporal%20%28broadcast%29%20tracking%20data.%20This%0Adataset%20is%20derived%20from%20632%20games%20of%20MLS%20%282022%29%2C%20NWSL%20%282022%29%20and%20international%0Asoccer%20%282020-2022%29.%20With%20this%20data%20we%20demonstrate%20that%20gender-specific%20Graph%0ANeural%20Networks%20outperform%20architecturally%20identical%20gender-ambiguous%20models%20in%0Apredicting%20the%20successful%20outcome%20of%20counterattacks.%20We%20show%2C%20using%20Permutation%0AFeature%20Importance%2C%20that%20byline%20to%20byline%20speed%2C%20angle%20to%20the%20goal%2C%20angle%20to%0Athe%20ball%20and%20sideline%20to%20sideline%20speed%20are%20the%20node%20features%20with%20the%20highest%0Aimpact%20on%20model%20performance.%20Additionally%2C%20we%20offer%20some%20illustrative%20examples%0Aon%20how%20to%20navigate%20the%20infinite%20solution%20search%20space%20to%20aid%20in%20identifying%0Aimprovements%20for%20player%20decision%20making.%0A%20%20This%20research%20is%20accompanied%20by%20an%20open-source%20repository%20containing%20all%20data%0Aand%20code%2C%20and%20it%20is%20also%20accompanied%20by%20an%20open-source%20Python%20package%20which%0Asimplifies%20converting%20spatiotemporal%20data%20into%20graphs.%20This%20package%20also%0Afacilitates%20testing%2C%20validation%2C%20training%20and%20prediction%20with%20this%20data.%20This%0Ashould%20allow%20the%20reader%20to%20replicate%20and%20improve%20upon%20our%20research%20more%20easily.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.17450v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Graph%2520Neural%2520Network%2520deep-dive%2520into%2520successful%2520counterattacks%26entry.906535625%3DJoris%2520Bekkers%2520and%2520Amod%2520Sahasrabudhe%26entry.1292438233%3D%2520%2520A%2520counterattack%2520in%2520soccer%2520is%2520a%2520high%2520speed%252C%2520high%2520intensity%2520direct%2520attack%2520that%250Acan%2520occur%2520when%2520a%2520team%2520transitions%2520from%2520a%2520defensive%2520state%2520to%2520an%2520attacking%2520state%250Aafter%2520regaining%2520possession%2520of%2520the%2520ball.%2520The%2520aim%2520is%2520to%2520create%2520a%2520goal-scoring%250Aopportunity%2520by%2520convering%2520a%2520lot%2520of%2520ground%2520with%2520minimal%2520passes%2520before%2520the%250Aopposing%2520team%2520can%2520recover%2520their%2520defensive%2520shape.%2520The%2520purpose%2520of%2520this%2520research%250Ais%2520to%2520build%2520gender-specific%2520Graph%2520Neural%2520Networks%2520to%2520model%2520the%2520likelihood%2520of%2520a%250Acounterattack%2520being%2520successful%2520and%2520uncover%2520what%2520factors%2520make%2520them%2520successful%2520in%250Aprofessional%2520soccer.%2520These%2520models%2520are%2520trained%2520on%2520a%2520total%2520of%252020863%2520frames%2520of%250Asynchronized%2520on-ball%2520event%2520and%2520spatiotemporal%2520%2528broadcast%2529%2520tracking%2520data.%2520This%250Adataset%2520is%2520derived%2520from%2520632%2520games%2520of%2520MLS%2520%25282022%2529%252C%2520NWSL%2520%25282022%2529%2520and%2520international%250Asoccer%2520%25282020-2022%2529.%2520With%2520this%2520data%2520we%2520demonstrate%2520that%2520gender-specific%2520Graph%250ANeural%2520Networks%2520outperform%2520architecturally%2520identical%2520gender-ambiguous%2520models%2520in%250Apredicting%2520the%2520successful%2520outcome%2520of%2520counterattacks.%2520We%2520show%252C%2520using%2520Permutation%250AFeature%2520Importance%252C%2520that%2520byline%2520to%2520byline%2520speed%252C%2520angle%2520to%2520the%2520goal%252C%2520angle%2520to%250Athe%2520ball%2520and%2520sideline%2520to%2520sideline%2520speed%2520are%2520the%2520node%2520features%2520with%2520the%2520highest%250Aimpact%2520on%2520model%2520performance.%2520Additionally%252C%2520we%2520offer%2520some%2520illustrative%2520examples%250Aon%2520how%2520to%2520navigate%2520the%2520infinite%2520solution%2520search%2520space%2520to%2520aid%2520in%2520identifying%250Aimprovements%2520for%2520player%2520decision%2520making.%250A%2520%2520This%2520research%2520is%2520accompanied%2520by%2520an%2520open-source%2520repository%2520containing%2520all%2520data%250Aand%2520code%252C%2520and%2520it%2520is%2520also%2520accompanied%2520by%2520an%2520open-source%2520Python%2520package%2520which%250Asimplifies%2520converting%2520spatiotemporal%2520data%2520into%2520graphs.%2520This%2520package%2520also%250Afacilitates%2520testing%252C%2520validation%252C%2520training%2520and%2520prediction%2520with%2520this%2520data.%2520This%250Ashould%2520allow%2520the%2520reader%2520to%2520replicate%2520and%2520improve%2520upon%2520our%2520research%2520more%2520easily.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.17450v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Graph%20Neural%20Network%20deep-dive%20into%20successful%20counterattacks&entry.906535625=Joris%20Bekkers%20and%20Amod%20Sahasrabudhe&entry.1292438233=%20%20A%20counterattack%20in%20soccer%20is%20a%20high%20speed%2C%20high%20intensity%20direct%20attack%20that%0Acan%20occur%20when%20a%20team%20transitions%20from%20a%20defensive%20state%20to%20an%20attacking%20state%0Aafter%20regaining%20possession%20of%20the%20ball.%20The%20aim%20is%20to%20create%20a%20goal-scoring%0Aopportunity%20by%20convering%20a%20lot%20of%20ground%20with%20minimal%20passes%20before%20the%0Aopposing%20team%20can%20recover%20their%20defensive%20shape.%20The%20purpose%20of%20this%20research%0Ais%20to%20build%20gender-specific%20Graph%20Neural%20Networks%20to%20model%20the%20likelihood%20of%20a%0Acounterattack%20being%20successful%20and%20uncover%20what%20factors%20make%20them%20successful%20in%0Aprofessional%20soccer.%20These%20models%20are%20trained%20on%20a%20total%20of%2020863%20frames%20of%0Asynchronized%20on-ball%20event%20and%20spatiotemporal%20%28broadcast%29%20tracking%20data.%20This%0Adataset%20is%20derived%20from%20632%20games%20of%20MLS%20%282022%29%2C%20NWSL%20%282022%29%20and%20international%0Asoccer%20%282020-2022%29.%20With%20this%20data%20we%20demonstrate%20that%20gender-specific%20Graph%0ANeural%20Networks%20outperform%20architecturally%20identical%20gender-ambiguous%20models%20in%0Apredicting%20the%20successful%20outcome%20of%20counterattacks.%20We%20show%2C%20using%20Permutation%0AFeature%20Importance%2C%20that%20byline%20to%20byline%20speed%2C%20angle%20to%20the%20goal%2C%20angle%20to%0Athe%20ball%20and%20sideline%20to%20sideline%20speed%20are%20the%20node%20features%20with%20the%20highest%0Aimpact%20on%20model%20performance.%20Additionally%2C%20we%20offer%20some%20illustrative%20examples%0Aon%20how%20to%20navigate%20the%20infinite%20solution%20search%20space%20to%20aid%20in%20identifying%0Aimprovements%20for%20player%20decision%20making.%0A%20%20This%20research%20is%20accompanied%20by%20an%20open-source%20repository%20containing%20all%20data%0Aand%20code%2C%20and%20it%20is%20also%20accompanied%20by%20an%20open-source%20Python%20package%20which%0Asimplifies%20converting%20spatiotemporal%20data%20into%20graphs.%20This%20package%20also%0Afacilitates%20testing%2C%20validation%2C%20training%20and%20prediction%20with%20this%20data.%20This%0Ashould%20allow%20the%20reader%20to%20replicate%20and%20improve%20upon%20our%20research%20more%20easily.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.17450v2&entry.124074799=Read"},
{"title": "Embodied Image Quality Assessment for Robotic Intelligence", "author": "Jianbo Zhang and Chunyi Li and Liang Yuan and Guoquan Zheng and Jie Hao and Guangtao Zhai", "abstract": "  Image quality assessment (IQA) of user-generated content (UGC) is a critical\ntechnique for human quality of experience (QoE). However, for robot-generated\ncontent (RGC), will its image quality be consistent with the Moravec paradox\nand counter to human common sense? Human subjective scoring is more based on\nthe attractiveness of the image. Embodied agent are required to interact and\nperceive in the environment, and finally perform specific tasks. Visual images\nas inputs directly influence downstream tasks. In this paper, we first propose\nan embodied image quality assessment (EIQA) frameworks. We establish assessment\nmetrics for input images based on the downstream tasks of robot. In addition,\nwe construct an Embodied Preference Database (EPD) containing 5,000 reference\nand distorted image annotations. The performance of mainstream IQA algorithms\non EPD dataset is finally verified. The experiments demonstrate that quality\nassessment of embodied images is different from that of humans. We sincerely\nhope that the EPD can contribute to the development of embodied AI by focusing\non image quality assessment. The benchmark is available at\nhttps://github.com/Jianbo-maker/EPD_benchmark.\n", "link": "http://arxiv.org/abs/2412.18774v2", "date": "2024-12-30", "relevancy": 2.1696, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5454}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5438}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embodied%20Image%20Quality%20Assessment%20for%20Robotic%20Intelligence&body=Title%3A%20Embodied%20Image%20Quality%20Assessment%20for%20Robotic%20Intelligence%0AAuthor%3A%20Jianbo%20Zhang%20and%20Chunyi%20Li%20and%20Liang%20Yuan%20and%20Guoquan%20Zheng%20and%20Jie%20Hao%20and%20Guangtao%20Zhai%0AAbstract%3A%20%20%20Image%20quality%20assessment%20%28IQA%29%20of%20user-generated%20content%20%28UGC%29%20is%20a%20critical%0Atechnique%20for%20human%20quality%20of%20experience%20%28QoE%29.%20However%2C%20for%20robot-generated%0Acontent%20%28RGC%29%2C%20will%20its%20image%20quality%20be%20consistent%20with%20the%20Moravec%20paradox%0Aand%20counter%20to%20human%20common%20sense%3F%20Human%20subjective%20scoring%20is%20more%20based%20on%0Athe%20attractiveness%20of%20the%20image.%20Embodied%20agent%20are%20required%20to%20interact%20and%0Aperceive%20in%20the%20environment%2C%20and%20finally%20perform%20specific%20tasks.%20Visual%20images%0Aas%20inputs%20directly%20influence%20downstream%20tasks.%20In%20this%20paper%2C%20we%20first%20propose%0Aan%20embodied%20image%20quality%20assessment%20%28EIQA%29%20frameworks.%20We%20establish%20assessment%0Ametrics%20for%20input%20images%20based%20on%20the%20downstream%20tasks%20of%20robot.%20In%20addition%2C%0Awe%20construct%20an%20Embodied%20Preference%20Database%20%28EPD%29%20containing%205%2C000%20reference%0Aand%20distorted%20image%20annotations.%20The%20performance%20of%20mainstream%20IQA%20algorithms%0Aon%20EPD%20dataset%20is%20finally%20verified.%20The%20experiments%20demonstrate%20that%20quality%0Aassessment%20of%20embodied%20images%20is%20different%20from%20that%20of%20humans.%20We%20sincerely%0Ahope%20that%20the%20EPD%20can%20contribute%20to%20the%20development%20of%20embodied%20AI%20by%20focusing%0Aon%20image%20quality%20assessment.%20The%20benchmark%20is%20available%20at%0Ahttps%3A//github.com/Jianbo-maker/EPD_benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18774v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbodied%2520Image%2520Quality%2520Assessment%2520for%2520Robotic%2520Intelligence%26entry.906535625%3DJianbo%2520Zhang%2520and%2520Chunyi%2520Li%2520and%2520Liang%2520Yuan%2520and%2520Guoquan%2520Zheng%2520and%2520Jie%2520Hao%2520and%2520Guangtao%2520Zhai%26entry.1292438233%3D%2520%2520Image%2520quality%2520assessment%2520%2528IQA%2529%2520of%2520user-generated%2520content%2520%2528UGC%2529%2520is%2520a%2520critical%250Atechnique%2520for%2520human%2520quality%2520of%2520experience%2520%2528QoE%2529.%2520However%252C%2520for%2520robot-generated%250Acontent%2520%2528RGC%2529%252C%2520will%2520its%2520image%2520quality%2520be%2520consistent%2520with%2520the%2520Moravec%2520paradox%250Aand%2520counter%2520to%2520human%2520common%2520sense%253F%2520Human%2520subjective%2520scoring%2520is%2520more%2520based%2520on%250Athe%2520attractiveness%2520of%2520the%2520image.%2520Embodied%2520agent%2520are%2520required%2520to%2520interact%2520and%250Aperceive%2520in%2520the%2520environment%252C%2520and%2520finally%2520perform%2520specific%2520tasks.%2520Visual%2520images%250Aas%2520inputs%2520directly%2520influence%2520downstream%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520first%2520propose%250Aan%2520embodied%2520image%2520quality%2520assessment%2520%2528EIQA%2529%2520frameworks.%2520We%2520establish%2520assessment%250Ametrics%2520for%2520input%2520images%2520based%2520on%2520the%2520downstream%2520tasks%2520of%2520robot.%2520In%2520addition%252C%250Awe%2520construct%2520an%2520Embodied%2520Preference%2520Database%2520%2528EPD%2529%2520containing%25205%252C000%2520reference%250Aand%2520distorted%2520image%2520annotations.%2520The%2520performance%2520of%2520mainstream%2520IQA%2520algorithms%250Aon%2520EPD%2520dataset%2520is%2520finally%2520verified.%2520The%2520experiments%2520demonstrate%2520that%2520quality%250Aassessment%2520of%2520embodied%2520images%2520is%2520different%2520from%2520that%2520of%2520humans.%2520We%2520sincerely%250Ahope%2520that%2520the%2520EPD%2520can%2520contribute%2520to%2520the%2520development%2520of%2520embodied%2520AI%2520by%2520focusing%250Aon%2520image%2520quality%2520assessment.%2520The%2520benchmark%2520is%2520available%2520at%250Ahttps%253A//github.com/Jianbo-maker/EPD_benchmark.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18774v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embodied%20Image%20Quality%20Assessment%20for%20Robotic%20Intelligence&entry.906535625=Jianbo%20Zhang%20and%20Chunyi%20Li%20and%20Liang%20Yuan%20and%20Guoquan%20Zheng%20and%20Jie%20Hao%20and%20Guangtao%20Zhai&entry.1292438233=%20%20Image%20quality%20assessment%20%28IQA%29%20of%20user-generated%20content%20%28UGC%29%20is%20a%20critical%0Atechnique%20for%20human%20quality%20of%20experience%20%28QoE%29.%20However%2C%20for%20robot-generated%0Acontent%20%28RGC%29%2C%20will%20its%20image%20quality%20be%20consistent%20with%20the%20Moravec%20paradox%0Aand%20counter%20to%20human%20common%20sense%3F%20Human%20subjective%20scoring%20is%20more%20based%20on%0Athe%20attractiveness%20of%20the%20image.%20Embodied%20agent%20are%20required%20to%20interact%20and%0Aperceive%20in%20the%20environment%2C%20and%20finally%20perform%20specific%20tasks.%20Visual%20images%0Aas%20inputs%20directly%20influence%20downstream%20tasks.%20In%20this%20paper%2C%20we%20first%20propose%0Aan%20embodied%20image%20quality%20assessment%20%28EIQA%29%20frameworks.%20We%20establish%20assessment%0Ametrics%20for%20input%20images%20based%20on%20the%20downstream%20tasks%20of%20robot.%20In%20addition%2C%0Awe%20construct%20an%20Embodied%20Preference%20Database%20%28EPD%29%20containing%205%2C000%20reference%0Aand%20distorted%20image%20annotations.%20The%20performance%20of%20mainstream%20IQA%20algorithms%0Aon%20EPD%20dataset%20is%20finally%20verified.%20The%20experiments%20demonstrate%20that%20quality%0Aassessment%20of%20embodied%20images%20is%20different%20from%20that%20of%20humans.%20We%20sincerely%0Ahope%20that%20the%20EPD%20can%20contribute%20to%20the%20development%20of%20embodied%20AI%20by%20focusing%0Aon%20image%20quality%20assessment.%20The%20benchmark%20is%20available%20at%0Ahttps%3A//github.com/Jianbo-maker/EPD_benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18774v2&entry.124074799=Read"},
{"title": "A Large-Scale Study on Video Action Dataset Condensation", "author": "Yang Chen and Sheng Guo and Limin Wang", "abstract": "  Dataset condensation has made significant progress in the image domain.\nUnlike images, videos possess an additional temporal dimension, which harbors\nconsiderable redundant information, making condensation even more crucial.\nHowever, video dataset condensation still remains an underexplored area. We aim\nto bridge this gap by providing a large-scale empirical study with systematic\ndesign and fair comparison. Specifically, our work delves into three key\naspects to provide valuable empirical insights: (1) temporal processing of\nvideo data, (2) establishing a comprehensive evaluation protocol for video\ndataset condensation, and (3) adaptation of condensation methods to the\nspace-time domain and fair comparisons among them. From this study, we derive\nseveral intriguing observations: (i) sample diversity appears to be more\ncrucial than temporal diversity for video dataset condensation, (ii) simple\nslide-window sampling proves to be effective, and (iii) sample selection\ncurrently outperforms dataset distillation in most cases. Furthermore, we\nconduct experiments on three prominent action recognition datasets (HMDB51,\nUCF101 and Kinetics-400) and achieve state-of-the-art results on all of them.\nOur code is available at https://github.com/MCG-NJU/Video-DC.\n", "link": "http://arxiv.org/abs/2412.21197v1", "date": "2024-12-30", "relevancy": 2.1687, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5707}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5377}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5353}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Large-Scale%20Study%20on%20Video%20Action%20Dataset%20Condensation&body=Title%3A%20A%20Large-Scale%20Study%20on%20Video%20Action%20Dataset%20Condensation%0AAuthor%3A%20Yang%20Chen%20and%20Sheng%20Guo%20and%20Limin%20Wang%0AAbstract%3A%20%20%20Dataset%20condensation%20has%20made%20significant%20progress%20in%20the%20image%20domain.%0AUnlike%20images%2C%20videos%20possess%20an%20additional%20temporal%20dimension%2C%20which%20harbors%0Aconsiderable%20redundant%20information%2C%20making%20condensation%20even%20more%20crucial.%0AHowever%2C%20video%20dataset%20condensation%20still%20remains%20an%20underexplored%20area.%20We%20aim%0Ato%20bridge%20this%20gap%20by%20providing%20a%20large-scale%20empirical%20study%20with%20systematic%0Adesign%20and%20fair%20comparison.%20Specifically%2C%20our%20work%20delves%20into%20three%20key%0Aaspects%20to%20provide%20valuable%20empirical%20insights%3A%20%281%29%20temporal%20processing%20of%0Avideo%20data%2C%20%282%29%20establishing%20a%20comprehensive%20evaluation%20protocol%20for%20video%0Adataset%20condensation%2C%20and%20%283%29%20adaptation%20of%20condensation%20methods%20to%20the%0Aspace-time%20domain%20and%20fair%20comparisons%20among%20them.%20From%20this%20study%2C%20we%20derive%0Aseveral%20intriguing%20observations%3A%20%28i%29%20sample%20diversity%20appears%20to%20be%20more%0Acrucial%20than%20temporal%20diversity%20for%20video%20dataset%20condensation%2C%20%28ii%29%20simple%0Aslide-window%20sampling%20proves%20to%20be%20effective%2C%20and%20%28iii%29%20sample%20selection%0Acurrently%20outperforms%20dataset%20distillation%20in%20most%20cases.%20Furthermore%2C%20we%0Aconduct%20experiments%20on%20three%20prominent%20action%20recognition%20datasets%20%28HMDB51%2C%0AUCF101%20and%20Kinetics-400%29%20and%20achieve%20state-of-the-art%20results%20on%20all%20of%20them.%0AOur%20code%20is%20available%20at%20https%3A//github.com/MCG-NJU/Video-DC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21197v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Large-Scale%2520Study%2520on%2520Video%2520Action%2520Dataset%2520Condensation%26entry.906535625%3DYang%2520Chen%2520and%2520Sheng%2520Guo%2520and%2520Limin%2520Wang%26entry.1292438233%3D%2520%2520Dataset%2520condensation%2520has%2520made%2520significant%2520progress%2520in%2520the%2520image%2520domain.%250AUnlike%2520images%252C%2520videos%2520possess%2520an%2520additional%2520temporal%2520dimension%252C%2520which%2520harbors%250Aconsiderable%2520redundant%2520information%252C%2520making%2520condensation%2520even%2520more%2520crucial.%250AHowever%252C%2520video%2520dataset%2520condensation%2520still%2520remains%2520an%2520underexplored%2520area.%2520We%2520aim%250Ato%2520bridge%2520this%2520gap%2520by%2520providing%2520a%2520large-scale%2520empirical%2520study%2520with%2520systematic%250Adesign%2520and%2520fair%2520comparison.%2520Specifically%252C%2520our%2520work%2520delves%2520into%2520three%2520key%250Aaspects%2520to%2520provide%2520valuable%2520empirical%2520insights%253A%2520%25281%2529%2520temporal%2520processing%2520of%250Avideo%2520data%252C%2520%25282%2529%2520establishing%2520a%2520comprehensive%2520evaluation%2520protocol%2520for%2520video%250Adataset%2520condensation%252C%2520and%2520%25283%2529%2520adaptation%2520of%2520condensation%2520methods%2520to%2520the%250Aspace-time%2520domain%2520and%2520fair%2520comparisons%2520among%2520them.%2520From%2520this%2520study%252C%2520we%2520derive%250Aseveral%2520intriguing%2520observations%253A%2520%2528i%2529%2520sample%2520diversity%2520appears%2520to%2520be%2520more%250Acrucial%2520than%2520temporal%2520diversity%2520for%2520video%2520dataset%2520condensation%252C%2520%2528ii%2529%2520simple%250Aslide-window%2520sampling%2520proves%2520to%2520be%2520effective%252C%2520and%2520%2528iii%2529%2520sample%2520selection%250Acurrently%2520outperforms%2520dataset%2520distillation%2520in%2520most%2520cases.%2520Furthermore%252C%2520we%250Aconduct%2520experiments%2520on%2520three%2520prominent%2520action%2520recognition%2520datasets%2520%2528HMDB51%252C%250AUCF101%2520and%2520Kinetics-400%2529%2520and%2520achieve%2520state-of-the-art%2520results%2520on%2520all%2520of%2520them.%250AOur%2520code%2520is%2520available%2520at%2520https%253A//github.com/MCG-NJU/Video-DC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21197v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Large-Scale%20Study%20on%20Video%20Action%20Dataset%20Condensation&entry.906535625=Yang%20Chen%20and%20Sheng%20Guo%20and%20Limin%20Wang&entry.1292438233=%20%20Dataset%20condensation%20has%20made%20significant%20progress%20in%20the%20image%20domain.%0AUnlike%20images%2C%20videos%20possess%20an%20additional%20temporal%20dimension%2C%20which%20harbors%0Aconsiderable%20redundant%20information%2C%20making%20condensation%20even%20more%20crucial.%0AHowever%2C%20video%20dataset%20condensation%20still%20remains%20an%20underexplored%20area.%20We%20aim%0Ato%20bridge%20this%20gap%20by%20providing%20a%20large-scale%20empirical%20study%20with%20systematic%0Adesign%20and%20fair%20comparison.%20Specifically%2C%20our%20work%20delves%20into%20three%20key%0Aaspects%20to%20provide%20valuable%20empirical%20insights%3A%20%281%29%20temporal%20processing%20of%0Avideo%20data%2C%20%282%29%20establishing%20a%20comprehensive%20evaluation%20protocol%20for%20video%0Adataset%20condensation%2C%20and%20%283%29%20adaptation%20of%20condensation%20methods%20to%20the%0Aspace-time%20domain%20and%20fair%20comparisons%20among%20them.%20From%20this%20study%2C%20we%20derive%0Aseveral%20intriguing%20observations%3A%20%28i%29%20sample%20diversity%20appears%20to%20be%20more%0Acrucial%20than%20temporal%20diversity%20for%20video%20dataset%20condensation%2C%20%28ii%29%20simple%0Aslide-window%20sampling%20proves%20to%20be%20effective%2C%20and%20%28iii%29%20sample%20selection%0Acurrently%20outperforms%20dataset%20distillation%20in%20most%20cases.%20Furthermore%2C%20we%0Aconduct%20experiments%20on%20three%20prominent%20action%20recognition%20datasets%20%28HMDB51%2C%0AUCF101%20and%20Kinetics-400%29%20and%20achieve%20state-of-the-art%20results%20on%20all%20of%20them.%0AOur%20code%20is%20available%20at%20https%3A//github.com/MCG-NJU/Video-DC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21197v1&entry.124074799=Read"},
{"title": "SoS Certificates for Sparse Singular Values and Their Applications:\n  Robust Statistics, Subspace Distortion, and More", "author": "Ilias Diakonikolas and Samuel B. Hopkins and Ankit Pensia and Stefan Tiegel", "abstract": "  We study $\\textit{sparse singular value certificates}$ for random rectangular\nmatrices. If $M$ is an $n \\times d$ matrix with independent Gaussian entries,\nwe give a new family of polynomial-time algorithms which can certify upper\nbounds on the maximum of $\\|M u\\|$, where $u$ is a unit vector with at most\n$\\eta n$ nonzero entries for a given $\\eta \\in (0,1)$. This basic algorithmic\nprimitive lies at the heart of a wide range of problems across algorithmic\nstatistics and theoretical computer science.\n  Our algorithms certify a bound which is asymptotically smaller than the naive\none, given by the maximum singular value of $M$, for nearly the widest-possible\nrange of $n,d,$ and $\\eta$. Efficiently certifying such a bound for a range of\n$n,d$ and $\\eta$ which is larger by any polynomial factor than what is achieved\nby our algorithm would violate lower bounds in the SQ and low-degree\npolynomials models. Our certification algorithm makes essential use of the\nSum-of-Squares hierarchy. To prove the correctness of our algorithm, we develop\na new combinatorial connection between the graph matrix approach to analyze\nrandom matrices with dependent entries, and the Efron-Stein decomposition of\nfunctions of independent random variables.\n  As applications of our certification algorithm, we obtain new efficient\nalgorithms for a wide range of well-studied algorithmic tasks. In algorithmic\nrobust statistics, we obtain new algorithms for robust mean and covariance\nestimation with tradeoffs between breakdown point and sample complexity, which\nare nearly matched by SQ and low-degree polynomial lower bounds (that we\nestablish). We also obtain new polynomial-time guarantees for certification of\n$\\ell_1/\\ell_2$ distortion of random subspaces of $\\mathbb{R}^n$ (also with\nnearly matching lower bounds), sparse principal component analysis, and\ncertification of the $2\\rightarrow p$ norm of a random matrix.\n", "link": "http://arxiv.org/abs/2412.21203v1", "date": "2024-12-30", "relevancy": 2.1475, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4403}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4262}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SoS%20Certificates%20for%20Sparse%20Singular%20Values%20and%20Their%20Applications%3A%0A%20%20Robust%20Statistics%2C%20Subspace%20Distortion%2C%20and%20More&body=Title%3A%20SoS%20Certificates%20for%20Sparse%20Singular%20Values%20and%20Their%20Applications%3A%0A%20%20Robust%20Statistics%2C%20Subspace%20Distortion%2C%20and%20More%0AAuthor%3A%20Ilias%20Diakonikolas%20and%20Samuel%20B.%20Hopkins%20and%20Ankit%20Pensia%20and%20Stefan%20Tiegel%0AAbstract%3A%20%20%20We%20study%20%24%5Ctextit%7Bsparse%20singular%20value%20certificates%7D%24%20for%20random%20rectangular%0Amatrices.%20If%20%24M%24%20is%20an%20%24n%20%5Ctimes%20d%24%20matrix%20with%20independent%20Gaussian%20entries%2C%0Awe%20give%20a%20new%20family%20of%20polynomial-time%20algorithms%20which%20can%20certify%20upper%0Abounds%20on%20the%20maximum%20of%20%24%5C%7CM%20u%5C%7C%24%2C%20where%20%24u%24%20is%20a%20unit%20vector%20with%20at%20most%0A%24%5Ceta%20n%24%20nonzero%20entries%20for%20a%20given%20%24%5Ceta%20%5Cin%20%280%2C1%29%24.%20This%20basic%20algorithmic%0Aprimitive%20lies%20at%20the%20heart%20of%20a%20wide%20range%20of%20problems%20across%20algorithmic%0Astatistics%20and%20theoretical%20computer%20science.%0A%20%20Our%20algorithms%20certify%20a%20bound%20which%20is%20asymptotically%20smaller%20than%20the%20naive%0Aone%2C%20given%20by%20the%20maximum%20singular%20value%20of%20%24M%24%2C%20for%20nearly%20the%20widest-possible%0Arange%20of%20%24n%2Cd%2C%24%20and%20%24%5Ceta%24.%20Efficiently%20certifying%20such%20a%20bound%20for%20a%20range%20of%0A%24n%2Cd%24%20and%20%24%5Ceta%24%20which%20is%20larger%20by%20any%20polynomial%20factor%20than%20what%20is%20achieved%0Aby%20our%20algorithm%20would%20violate%20lower%20bounds%20in%20the%20SQ%20and%20low-degree%0Apolynomials%20models.%20Our%20certification%20algorithm%20makes%20essential%20use%20of%20the%0ASum-of-Squares%20hierarchy.%20To%20prove%20the%20correctness%20of%20our%20algorithm%2C%20we%20develop%0Aa%20new%20combinatorial%20connection%20between%20the%20graph%20matrix%20approach%20to%20analyze%0Arandom%20matrices%20with%20dependent%20entries%2C%20and%20the%20Efron-Stein%20decomposition%20of%0Afunctions%20of%20independent%20random%20variables.%0A%20%20As%20applications%20of%20our%20certification%20algorithm%2C%20we%20obtain%20new%20efficient%0Aalgorithms%20for%20a%20wide%20range%20of%20well-studied%20algorithmic%20tasks.%20In%20algorithmic%0Arobust%20statistics%2C%20we%20obtain%20new%20algorithms%20for%20robust%20mean%20and%20covariance%0Aestimation%20with%20tradeoffs%20between%20breakdown%20point%20and%20sample%20complexity%2C%20which%0Aare%20nearly%20matched%20by%20SQ%20and%20low-degree%20polynomial%20lower%20bounds%20%28that%20we%0Aestablish%29.%20We%20also%20obtain%20new%20polynomial-time%20guarantees%20for%20certification%20of%0A%24%5Cell_1/%5Cell_2%24%20distortion%20of%20random%20subspaces%20of%20%24%5Cmathbb%7BR%7D%5En%24%20%28also%20with%0Anearly%20matching%20lower%20bounds%29%2C%20sparse%20principal%20component%20analysis%2C%20and%0Acertification%20of%20the%20%242%5Crightarrow%20p%24%20norm%20of%20a%20random%20matrix.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21203v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoS%2520Certificates%2520for%2520Sparse%2520Singular%2520Values%2520and%2520Their%2520Applications%253A%250A%2520%2520Robust%2520Statistics%252C%2520Subspace%2520Distortion%252C%2520and%2520More%26entry.906535625%3DIlias%2520Diakonikolas%2520and%2520Samuel%2520B.%2520Hopkins%2520and%2520Ankit%2520Pensia%2520and%2520Stefan%2520Tiegel%26entry.1292438233%3D%2520%2520We%2520study%2520%2524%255Ctextit%257Bsparse%2520singular%2520value%2520certificates%257D%2524%2520for%2520random%2520rectangular%250Amatrices.%2520If%2520%2524M%2524%2520is%2520an%2520%2524n%2520%255Ctimes%2520d%2524%2520matrix%2520with%2520independent%2520Gaussian%2520entries%252C%250Awe%2520give%2520a%2520new%2520family%2520of%2520polynomial-time%2520algorithms%2520which%2520can%2520certify%2520upper%250Abounds%2520on%2520the%2520maximum%2520of%2520%2524%255C%257CM%2520u%255C%257C%2524%252C%2520where%2520%2524u%2524%2520is%2520a%2520unit%2520vector%2520with%2520at%2520most%250A%2524%255Ceta%2520n%2524%2520nonzero%2520entries%2520for%2520a%2520given%2520%2524%255Ceta%2520%255Cin%2520%25280%252C1%2529%2524.%2520This%2520basic%2520algorithmic%250Aprimitive%2520lies%2520at%2520the%2520heart%2520of%2520a%2520wide%2520range%2520of%2520problems%2520across%2520algorithmic%250Astatistics%2520and%2520theoretical%2520computer%2520science.%250A%2520%2520Our%2520algorithms%2520certify%2520a%2520bound%2520which%2520is%2520asymptotically%2520smaller%2520than%2520the%2520naive%250Aone%252C%2520given%2520by%2520the%2520maximum%2520singular%2520value%2520of%2520%2524M%2524%252C%2520for%2520nearly%2520the%2520widest-possible%250Arange%2520of%2520%2524n%252Cd%252C%2524%2520and%2520%2524%255Ceta%2524.%2520Efficiently%2520certifying%2520such%2520a%2520bound%2520for%2520a%2520range%2520of%250A%2524n%252Cd%2524%2520and%2520%2524%255Ceta%2524%2520which%2520is%2520larger%2520by%2520any%2520polynomial%2520factor%2520than%2520what%2520is%2520achieved%250Aby%2520our%2520algorithm%2520would%2520violate%2520lower%2520bounds%2520in%2520the%2520SQ%2520and%2520low-degree%250Apolynomials%2520models.%2520Our%2520certification%2520algorithm%2520makes%2520essential%2520use%2520of%2520the%250ASum-of-Squares%2520hierarchy.%2520To%2520prove%2520the%2520correctness%2520of%2520our%2520algorithm%252C%2520we%2520develop%250Aa%2520new%2520combinatorial%2520connection%2520between%2520the%2520graph%2520matrix%2520approach%2520to%2520analyze%250Arandom%2520matrices%2520with%2520dependent%2520entries%252C%2520and%2520the%2520Efron-Stein%2520decomposition%2520of%250Afunctions%2520of%2520independent%2520random%2520variables.%250A%2520%2520As%2520applications%2520of%2520our%2520certification%2520algorithm%252C%2520we%2520obtain%2520new%2520efficient%250Aalgorithms%2520for%2520a%2520wide%2520range%2520of%2520well-studied%2520algorithmic%2520tasks.%2520In%2520algorithmic%250Arobust%2520statistics%252C%2520we%2520obtain%2520new%2520algorithms%2520for%2520robust%2520mean%2520and%2520covariance%250Aestimation%2520with%2520tradeoffs%2520between%2520breakdown%2520point%2520and%2520sample%2520complexity%252C%2520which%250Aare%2520nearly%2520matched%2520by%2520SQ%2520and%2520low-degree%2520polynomial%2520lower%2520bounds%2520%2528that%2520we%250Aestablish%2529.%2520We%2520also%2520obtain%2520new%2520polynomial-time%2520guarantees%2520for%2520certification%2520of%250A%2524%255Cell_1/%255Cell_2%2524%2520distortion%2520of%2520random%2520subspaces%2520of%2520%2524%255Cmathbb%257BR%257D%255En%2524%2520%2528also%2520with%250Anearly%2520matching%2520lower%2520bounds%2529%252C%2520sparse%2520principal%2520component%2520analysis%252C%2520and%250Acertification%2520of%2520the%2520%25242%255Crightarrow%2520p%2524%2520norm%2520of%2520a%2520random%2520matrix.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21203v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SoS%20Certificates%20for%20Sparse%20Singular%20Values%20and%20Their%20Applications%3A%0A%20%20Robust%20Statistics%2C%20Subspace%20Distortion%2C%20and%20More&entry.906535625=Ilias%20Diakonikolas%20and%20Samuel%20B.%20Hopkins%20and%20Ankit%20Pensia%20and%20Stefan%20Tiegel&entry.1292438233=%20%20We%20study%20%24%5Ctextit%7Bsparse%20singular%20value%20certificates%7D%24%20for%20random%20rectangular%0Amatrices.%20If%20%24M%24%20is%20an%20%24n%20%5Ctimes%20d%24%20matrix%20with%20independent%20Gaussian%20entries%2C%0Awe%20give%20a%20new%20family%20of%20polynomial-time%20algorithms%20which%20can%20certify%20upper%0Abounds%20on%20the%20maximum%20of%20%24%5C%7CM%20u%5C%7C%24%2C%20where%20%24u%24%20is%20a%20unit%20vector%20with%20at%20most%0A%24%5Ceta%20n%24%20nonzero%20entries%20for%20a%20given%20%24%5Ceta%20%5Cin%20%280%2C1%29%24.%20This%20basic%20algorithmic%0Aprimitive%20lies%20at%20the%20heart%20of%20a%20wide%20range%20of%20problems%20across%20algorithmic%0Astatistics%20and%20theoretical%20computer%20science.%0A%20%20Our%20algorithms%20certify%20a%20bound%20which%20is%20asymptotically%20smaller%20than%20the%20naive%0Aone%2C%20given%20by%20the%20maximum%20singular%20value%20of%20%24M%24%2C%20for%20nearly%20the%20widest-possible%0Arange%20of%20%24n%2Cd%2C%24%20and%20%24%5Ceta%24.%20Efficiently%20certifying%20such%20a%20bound%20for%20a%20range%20of%0A%24n%2Cd%24%20and%20%24%5Ceta%24%20which%20is%20larger%20by%20any%20polynomial%20factor%20than%20what%20is%20achieved%0Aby%20our%20algorithm%20would%20violate%20lower%20bounds%20in%20the%20SQ%20and%20low-degree%0Apolynomials%20models.%20Our%20certification%20algorithm%20makes%20essential%20use%20of%20the%0ASum-of-Squares%20hierarchy.%20To%20prove%20the%20correctness%20of%20our%20algorithm%2C%20we%20develop%0Aa%20new%20combinatorial%20connection%20between%20the%20graph%20matrix%20approach%20to%20analyze%0Arandom%20matrices%20with%20dependent%20entries%2C%20and%20the%20Efron-Stein%20decomposition%20of%0Afunctions%20of%20independent%20random%20variables.%0A%20%20As%20applications%20of%20our%20certification%20algorithm%2C%20we%20obtain%20new%20efficient%0Aalgorithms%20for%20a%20wide%20range%20of%20well-studied%20algorithmic%20tasks.%20In%20algorithmic%0Arobust%20statistics%2C%20we%20obtain%20new%20algorithms%20for%20robust%20mean%20and%20covariance%0Aestimation%20with%20tradeoffs%20between%20breakdown%20point%20and%20sample%20complexity%2C%20which%0Aare%20nearly%20matched%20by%20SQ%20and%20low-degree%20polynomial%20lower%20bounds%20%28that%20we%0Aestablish%29.%20We%20also%20obtain%20new%20polynomial-time%20guarantees%20for%20certification%20of%0A%24%5Cell_1/%5Cell_2%24%20distortion%20of%20random%20subspaces%20of%20%24%5Cmathbb%7BR%7D%5En%24%20%28also%20with%0Anearly%20matching%20lower%20bounds%29%2C%20sparse%20principal%20component%20analysis%2C%20and%0Acertification%20of%20the%20%242%5Crightarrow%20p%24%20norm%20of%20a%20random%20matrix.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21203v1&entry.124074799=Read"},
{"title": "T-DOM: A Taxonomy for Robotic Manipulation of Deformable Objects", "author": "David Blanco-Mulero and Yifei Dong and Julia Borras and Florian T. Pokorny and Carme Torras", "abstract": "  Robotic grasp and manipulation taxonomies, inspired by observing human\nmanipulation strategies, can provide key guidance for tasks ranging from\nrobotic gripper design to the development of manipulation algorithms. The\nexisting grasp and manipulation taxonomies, however, often assume object\nrigidity, which limits their ability to reason about the complex interactions\nin the robotic manipulation of deformable objects. Hence, to assist in tasks\ninvolving deformable objects, taxonomies need to capture more comprehensively\nthe interactions inherent in deformable object manipulation. To this end, we\nintroduce T-DOM, a taxonomy that analyses key aspects involved in the\nmanipulation of deformable objects, such as robot motion, forces, prehensile\nand non-prehensile interactions and, for the first time, a detailed\nclassification of object deformations. To evaluate T-DOM, we curate a dataset\nof ten tasks involving a variety of deformable objects, such as garments,\nropes, and surgical gloves, as well as diverse types of deformations. We\nanalyse the proposed tasks comparing the T-DOM taxonomy with previous well\nestablished manipulation taxonomies. Our analysis demonstrates that T-DOM can\neffectively distinguish between manipulation skills that were not identified in\nother taxonomies, across different deformable objects and manipulation actions,\noffering new categories to characterize a skill. The proposed taxonomy\nsignificantly extends past work, providing a more fine-grained classification\nthat can be used to describe the robotic manipulation of deformable objects.\nThis work establishes a foundation for advancing deformable object\nmanipulation, bridging theoretical understanding and practical implementation\nin robotic systems.\n", "link": "http://arxiv.org/abs/2412.20998v1", "date": "2024-12-30", "relevancy": 2.1296, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5549}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5432}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T-DOM%3A%20A%20Taxonomy%20for%20Robotic%20Manipulation%20of%20Deformable%20Objects&body=Title%3A%20T-DOM%3A%20A%20Taxonomy%20for%20Robotic%20Manipulation%20of%20Deformable%20Objects%0AAuthor%3A%20David%20Blanco-Mulero%20and%20Yifei%20Dong%20and%20Julia%20Borras%20and%20Florian%20T.%20Pokorny%20and%20Carme%20Torras%0AAbstract%3A%20%20%20Robotic%20grasp%20and%20manipulation%20taxonomies%2C%20inspired%20by%20observing%20human%0Amanipulation%20strategies%2C%20can%20provide%20key%20guidance%20for%20tasks%20ranging%20from%0Arobotic%20gripper%20design%20to%20the%20development%20of%20manipulation%20algorithms.%20The%0Aexisting%20grasp%20and%20manipulation%20taxonomies%2C%20however%2C%20often%20assume%20object%0Arigidity%2C%20which%20limits%20their%20ability%20to%20reason%20about%20the%20complex%20interactions%0Ain%20the%20robotic%20manipulation%20of%20deformable%20objects.%20Hence%2C%20to%20assist%20in%20tasks%0Ainvolving%20deformable%20objects%2C%20taxonomies%20need%20to%20capture%20more%20comprehensively%0Athe%20interactions%20inherent%20in%20deformable%20object%20manipulation.%20To%20this%20end%2C%20we%0Aintroduce%20T-DOM%2C%20a%20taxonomy%20that%20analyses%20key%20aspects%20involved%20in%20the%0Amanipulation%20of%20deformable%20objects%2C%20such%20as%20robot%20motion%2C%20forces%2C%20prehensile%0Aand%20non-prehensile%20interactions%20and%2C%20for%20the%20first%20time%2C%20a%20detailed%0Aclassification%20of%20object%20deformations.%20To%20evaluate%20T-DOM%2C%20we%20curate%20a%20dataset%0Aof%20ten%20tasks%20involving%20a%20variety%20of%20deformable%20objects%2C%20such%20as%20garments%2C%0Aropes%2C%20and%20surgical%20gloves%2C%20as%20well%20as%20diverse%20types%20of%20deformations.%20We%0Aanalyse%20the%20proposed%20tasks%20comparing%20the%20T-DOM%20taxonomy%20with%20previous%20well%0Aestablished%20manipulation%20taxonomies.%20Our%20analysis%20demonstrates%20that%20T-DOM%20can%0Aeffectively%20distinguish%20between%20manipulation%20skills%20that%20were%20not%20identified%20in%0Aother%20taxonomies%2C%20across%20different%20deformable%20objects%20and%20manipulation%20actions%2C%0Aoffering%20new%20categories%20to%20characterize%20a%20skill.%20The%20proposed%20taxonomy%0Asignificantly%20extends%20past%20work%2C%20providing%20a%20more%20fine-grained%20classification%0Athat%20can%20be%20used%20to%20describe%20the%20robotic%20manipulation%20of%20deformable%20objects.%0AThis%20work%20establishes%20a%20foundation%20for%20advancing%20deformable%20object%0Amanipulation%2C%20bridging%20theoretical%20understanding%20and%20practical%20implementation%0Ain%20robotic%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20998v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT-DOM%253A%2520A%2520Taxonomy%2520for%2520Robotic%2520Manipulation%2520of%2520Deformable%2520Objects%26entry.906535625%3DDavid%2520Blanco-Mulero%2520and%2520Yifei%2520Dong%2520and%2520Julia%2520Borras%2520and%2520Florian%2520T.%2520Pokorny%2520and%2520Carme%2520Torras%26entry.1292438233%3D%2520%2520Robotic%2520grasp%2520and%2520manipulation%2520taxonomies%252C%2520inspired%2520by%2520observing%2520human%250Amanipulation%2520strategies%252C%2520can%2520provide%2520key%2520guidance%2520for%2520tasks%2520ranging%2520from%250Arobotic%2520gripper%2520design%2520to%2520the%2520development%2520of%2520manipulation%2520algorithms.%2520The%250Aexisting%2520grasp%2520and%2520manipulation%2520taxonomies%252C%2520however%252C%2520often%2520assume%2520object%250Arigidity%252C%2520which%2520limits%2520their%2520ability%2520to%2520reason%2520about%2520the%2520complex%2520interactions%250Ain%2520the%2520robotic%2520manipulation%2520of%2520deformable%2520objects.%2520Hence%252C%2520to%2520assist%2520in%2520tasks%250Ainvolving%2520deformable%2520objects%252C%2520taxonomies%2520need%2520to%2520capture%2520more%2520comprehensively%250Athe%2520interactions%2520inherent%2520in%2520deformable%2520object%2520manipulation.%2520To%2520this%2520end%252C%2520we%250Aintroduce%2520T-DOM%252C%2520a%2520taxonomy%2520that%2520analyses%2520key%2520aspects%2520involved%2520in%2520the%250Amanipulation%2520of%2520deformable%2520objects%252C%2520such%2520as%2520robot%2520motion%252C%2520forces%252C%2520prehensile%250Aand%2520non-prehensile%2520interactions%2520and%252C%2520for%2520the%2520first%2520time%252C%2520a%2520detailed%250Aclassification%2520of%2520object%2520deformations.%2520To%2520evaluate%2520T-DOM%252C%2520we%2520curate%2520a%2520dataset%250Aof%2520ten%2520tasks%2520involving%2520a%2520variety%2520of%2520deformable%2520objects%252C%2520such%2520as%2520garments%252C%250Aropes%252C%2520and%2520surgical%2520gloves%252C%2520as%2520well%2520as%2520diverse%2520types%2520of%2520deformations.%2520We%250Aanalyse%2520the%2520proposed%2520tasks%2520comparing%2520the%2520T-DOM%2520taxonomy%2520with%2520previous%2520well%250Aestablished%2520manipulation%2520taxonomies.%2520Our%2520analysis%2520demonstrates%2520that%2520T-DOM%2520can%250Aeffectively%2520distinguish%2520between%2520manipulation%2520skills%2520that%2520were%2520not%2520identified%2520in%250Aother%2520taxonomies%252C%2520across%2520different%2520deformable%2520objects%2520and%2520manipulation%2520actions%252C%250Aoffering%2520new%2520categories%2520to%2520characterize%2520a%2520skill.%2520The%2520proposed%2520taxonomy%250Asignificantly%2520extends%2520past%2520work%252C%2520providing%2520a%2520more%2520fine-grained%2520classification%250Athat%2520can%2520be%2520used%2520to%2520describe%2520the%2520robotic%2520manipulation%2520of%2520deformable%2520objects.%250AThis%2520work%2520establishes%2520a%2520foundation%2520for%2520advancing%2520deformable%2520object%250Amanipulation%252C%2520bridging%2520theoretical%2520understanding%2520and%2520practical%2520implementation%250Ain%2520robotic%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20998v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T-DOM%3A%20A%20Taxonomy%20for%20Robotic%20Manipulation%20of%20Deformable%20Objects&entry.906535625=David%20Blanco-Mulero%20and%20Yifei%20Dong%20and%20Julia%20Borras%20and%20Florian%20T.%20Pokorny%20and%20Carme%20Torras&entry.1292438233=%20%20Robotic%20grasp%20and%20manipulation%20taxonomies%2C%20inspired%20by%20observing%20human%0Amanipulation%20strategies%2C%20can%20provide%20key%20guidance%20for%20tasks%20ranging%20from%0Arobotic%20gripper%20design%20to%20the%20development%20of%20manipulation%20algorithms.%20The%0Aexisting%20grasp%20and%20manipulation%20taxonomies%2C%20however%2C%20often%20assume%20object%0Arigidity%2C%20which%20limits%20their%20ability%20to%20reason%20about%20the%20complex%20interactions%0Ain%20the%20robotic%20manipulation%20of%20deformable%20objects.%20Hence%2C%20to%20assist%20in%20tasks%0Ainvolving%20deformable%20objects%2C%20taxonomies%20need%20to%20capture%20more%20comprehensively%0Athe%20interactions%20inherent%20in%20deformable%20object%20manipulation.%20To%20this%20end%2C%20we%0Aintroduce%20T-DOM%2C%20a%20taxonomy%20that%20analyses%20key%20aspects%20involved%20in%20the%0Amanipulation%20of%20deformable%20objects%2C%20such%20as%20robot%20motion%2C%20forces%2C%20prehensile%0Aand%20non-prehensile%20interactions%20and%2C%20for%20the%20first%20time%2C%20a%20detailed%0Aclassification%20of%20object%20deformations.%20To%20evaluate%20T-DOM%2C%20we%20curate%20a%20dataset%0Aof%20ten%20tasks%20involving%20a%20variety%20of%20deformable%20objects%2C%20such%20as%20garments%2C%0Aropes%2C%20and%20surgical%20gloves%2C%20as%20well%20as%20diverse%20types%20of%20deformations.%20We%0Aanalyse%20the%20proposed%20tasks%20comparing%20the%20T-DOM%20taxonomy%20with%20previous%20well%0Aestablished%20manipulation%20taxonomies.%20Our%20analysis%20demonstrates%20that%20T-DOM%20can%0Aeffectively%20distinguish%20between%20manipulation%20skills%20that%20were%20not%20identified%20in%0Aother%20taxonomies%2C%20across%20different%20deformable%20objects%20and%20manipulation%20actions%2C%0Aoffering%20new%20categories%20to%20characterize%20a%20skill.%20The%20proposed%20taxonomy%0Asignificantly%20extends%20past%20work%2C%20providing%20a%20more%20fine-grained%20classification%0Athat%20can%20be%20used%20to%20describe%20the%20robotic%20manipulation%20of%20deformable%20objects.%0AThis%20work%20establishes%20a%20foundation%20for%20advancing%20deformable%20object%0Amanipulation%2C%20bridging%20theoretical%20understanding%20and%20practical%20implementation%0Ain%20robotic%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20998v1&entry.124074799=Read"},
{"title": "WalkVLM:Aid Visually Impaired People Walking by Vision Language Model", "author": "Zhiqiang Yuan and Ting Zhang and Jiapei Zhang and Jie Zhou and Jinchao Zhang", "abstract": "  Approximately 200 million individuals around the world suffer from varying\ndegrees of visual impairment, making it crucial to leverage AI technology to\noffer walking assistance for these people. With the recent progress of\nvision-language models (VLMs), employing VLMs to improve this field has emerged\nas a popular research topic. However, most existing methods are studied on\nself-built question-answering datasets, lacking a unified training and testing\nbenchmark for walk guidance. Moreover, in blind walking task, it is necessary\nto perform real-time streaming video parsing and generate concise yet\ninformative reminders, which poses a great challenge for VLMs that suffer from\nredundant responses and low inference efficiency. In this paper, we firstly\nrelease a diverse, extensive, and unbiased walking awareness dataset,\ncontaining 12k video-manual annotation pairs from Europe and Asia to provide a\nfair training and testing benchmark for blind walking task. Furthermore, a\nWalkVLM model is proposed, which employs chain of thought for hierarchical\nplanning to generate concise but informative reminders and utilizes\ntemporal-aware adaptive prediction to reduce the temporal redundancy of\nreminders. Finally, we have established a solid benchmark for blind walking\ntask and verified the advantages of WalkVLM in stream video processing for this\ntask compared to other VLMs. Our dataset and code will be released at anonymous\nlink https://walkvlm2024.github.io.\n", "link": "http://arxiv.org/abs/2412.20903v1", "date": "2024-12-30", "relevancy": 2.0885, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.534}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5174}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WalkVLM%3AAid%20Visually%20Impaired%20People%20Walking%20by%20Vision%20Language%20Model&body=Title%3A%20WalkVLM%3AAid%20Visually%20Impaired%20People%20Walking%20by%20Vision%20Language%20Model%0AAuthor%3A%20Zhiqiang%20Yuan%20and%20Ting%20Zhang%20and%20Jiapei%20Zhang%20and%20Jie%20Zhou%20and%20Jinchao%20Zhang%0AAbstract%3A%20%20%20Approximately%20200%20million%20individuals%20around%20the%20world%20suffer%20from%20varying%0Adegrees%20of%20visual%20impairment%2C%20making%20it%20crucial%20to%20leverage%20AI%20technology%20to%0Aoffer%20walking%20assistance%20for%20these%20people.%20With%20the%20recent%20progress%20of%0Avision-language%20models%20%28VLMs%29%2C%20employing%20VLMs%20to%20improve%20this%20field%20has%20emerged%0Aas%20a%20popular%20research%20topic.%20However%2C%20most%20existing%20methods%20are%20studied%20on%0Aself-built%20question-answering%20datasets%2C%20lacking%20a%20unified%20training%20and%20testing%0Abenchmark%20for%20walk%20guidance.%20Moreover%2C%20in%20blind%20walking%20task%2C%20it%20is%20necessary%0Ato%20perform%20real-time%20streaming%20video%20parsing%20and%20generate%20concise%20yet%0Ainformative%20reminders%2C%20which%20poses%20a%20great%20challenge%20for%20VLMs%20that%20suffer%20from%0Aredundant%20responses%20and%20low%20inference%20efficiency.%20In%20this%20paper%2C%20we%20firstly%0Arelease%20a%20diverse%2C%20extensive%2C%20and%20unbiased%20walking%20awareness%20dataset%2C%0Acontaining%2012k%20video-manual%20annotation%20pairs%20from%20Europe%20and%20Asia%20to%20provide%20a%0Afair%20training%20and%20testing%20benchmark%20for%20blind%20walking%20task.%20Furthermore%2C%20a%0AWalkVLM%20model%20is%20proposed%2C%20which%20employs%20chain%20of%20thought%20for%20hierarchical%0Aplanning%20to%20generate%20concise%20but%20informative%20reminders%20and%20utilizes%0Atemporal-aware%20adaptive%20prediction%20to%20reduce%20the%20temporal%20redundancy%20of%0Areminders.%20Finally%2C%20we%20have%20established%20a%20solid%20benchmark%20for%20blind%20walking%0Atask%20and%20verified%20the%20advantages%20of%20WalkVLM%20in%20stream%20video%20processing%20for%20this%0Atask%20compared%20to%20other%20VLMs.%20Our%20dataset%20and%20code%20will%20be%20released%20at%20anonymous%0Alink%20https%3A//walkvlm2024.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20903v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWalkVLM%253AAid%2520Visually%2520Impaired%2520People%2520Walking%2520by%2520Vision%2520Language%2520Model%26entry.906535625%3DZhiqiang%2520Yuan%2520and%2520Ting%2520Zhang%2520and%2520Jiapei%2520Zhang%2520and%2520Jie%2520Zhou%2520and%2520Jinchao%2520Zhang%26entry.1292438233%3D%2520%2520Approximately%2520200%2520million%2520individuals%2520around%2520the%2520world%2520suffer%2520from%2520varying%250Adegrees%2520of%2520visual%2520impairment%252C%2520making%2520it%2520crucial%2520to%2520leverage%2520AI%2520technology%2520to%250Aoffer%2520walking%2520assistance%2520for%2520these%2520people.%2520With%2520the%2520recent%2520progress%2520of%250Avision-language%2520models%2520%2528VLMs%2529%252C%2520employing%2520VLMs%2520to%2520improve%2520this%2520field%2520has%2520emerged%250Aas%2520a%2520popular%2520research%2520topic.%2520However%252C%2520most%2520existing%2520methods%2520are%2520studied%2520on%250Aself-built%2520question-answering%2520datasets%252C%2520lacking%2520a%2520unified%2520training%2520and%2520testing%250Abenchmark%2520for%2520walk%2520guidance.%2520Moreover%252C%2520in%2520blind%2520walking%2520task%252C%2520it%2520is%2520necessary%250Ato%2520perform%2520real-time%2520streaming%2520video%2520parsing%2520and%2520generate%2520concise%2520yet%250Ainformative%2520reminders%252C%2520which%2520poses%2520a%2520great%2520challenge%2520for%2520VLMs%2520that%2520suffer%2520from%250Aredundant%2520responses%2520and%2520low%2520inference%2520efficiency.%2520In%2520this%2520paper%252C%2520we%2520firstly%250Arelease%2520a%2520diverse%252C%2520extensive%252C%2520and%2520unbiased%2520walking%2520awareness%2520dataset%252C%250Acontaining%252012k%2520video-manual%2520annotation%2520pairs%2520from%2520Europe%2520and%2520Asia%2520to%2520provide%2520a%250Afair%2520training%2520and%2520testing%2520benchmark%2520for%2520blind%2520walking%2520task.%2520Furthermore%252C%2520a%250AWalkVLM%2520model%2520is%2520proposed%252C%2520which%2520employs%2520chain%2520of%2520thought%2520for%2520hierarchical%250Aplanning%2520to%2520generate%2520concise%2520but%2520informative%2520reminders%2520and%2520utilizes%250Atemporal-aware%2520adaptive%2520prediction%2520to%2520reduce%2520the%2520temporal%2520redundancy%2520of%250Areminders.%2520Finally%252C%2520we%2520have%2520established%2520a%2520solid%2520benchmark%2520for%2520blind%2520walking%250Atask%2520and%2520verified%2520the%2520advantages%2520of%2520WalkVLM%2520in%2520stream%2520video%2520processing%2520for%2520this%250Atask%2520compared%2520to%2520other%2520VLMs.%2520Our%2520dataset%2520and%2520code%2520will%2520be%2520released%2520at%2520anonymous%250Alink%2520https%253A//walkvlm2024.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20903v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WalkVLM%3AAid%20Visually%20Impaired%20People%20Walking%20by%20Vision%20Language%20Model&entry.906535625=Zhiqiang%20Yuan%20and%20Ting%20Zhang%20and%20Jiapei%20Zhang%20and%20Jie%20Zhou%20and%20Jinchao%20Zhang&entry.1292438233=%20%20Approximately%20200%20million%20individuals%20around%20the%20world%20suffer%20from%20varying%0Adegrees%20of%20visual%20impairment%2C%20making%20it%20crucial%20to%20leverage%20AI%20technology%20to%0Aoffer%20walking%20assistance%20for%20these%20people.%20With%20the%20recent%20progress%20of%0Avision-language%20models%20%28VLMs%29%2C%20employing%20VLMs%20to%20improve%20this%20field%20has%20emerged%0Aas%20a%20popular%20research%20topic.%20However%2C%20most%20existing%20methods%20are%20studied%20on%0Aself-built%20question-answering%20datasets%2C%20lacking%20a%20unified%20training%20and%20testing%0Abenchmark%20for%20walk%20guidance.%20Moreover%2C%20in%20blind%20walking%20task%2C%20it%20is%20necessary%0Ato%20perform%20real-time%20streaming%20video%20parsing%20and%20generate%20concise%20yet%0Ainformative%20reminders%2C%20which%20poses%20a%20great%20challenge%20for%20VLMs%20that%20suffer%20from%0Aredundant%20responses%20and%20low%20inference%20efficiency.%20In%20this%20paper%2C%20we%20firstly%0Arelease%20a%20diverse%2C%20extensive%2C%20and%20unbiased%20walking%20awareness%20dataset%2C%0Acontaining%2012k%20video-manual%20annotation%20pairs%20from%20Europe%20and%20Asia%20to%20provide%20a%0Afair%20training%20and%20testing%20benchmark%20for%20blind%20walking%20task.%20Furthermore%2C%20a%0AWalkVLM%20model%20is%20proposed%2C%20which%20employs%20chain%20of%20thought%20for%20hierarchical%0Aplanning%20to%20generate%20concise%20but%20informative%20reminders%20and%20utilizes%0Atemporal-aware%20adaptive%20prediction%20to%20reduce%20the%20temporal%20redundancy%20of%0Areminders.%20Finally%2C%20we%20have%20established%20a%20solid%20benchmark%20for%20blind%20walking%0Atask%20and%20verified%20the%20advantages%20of%20WalkVLM%20in%20stream%20video%20processing%20for%20this%0Atask%20compared%20to%20other%20VLMs.%20Our%20dataset%20and%20code%20will%20be%20released%20at%20anonymous%0Alink%20https%3A//walkvlm2024.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20903v1&entry.124074799=Read"},
{"title": "Occam Gradient Descent", "author": "B. N. Kausik", "abstract": "  Deep learning neural network models must be large enough to adapt to their\nproblem domain, while small enough to avoid overfitting training data during\ngradient descent. To balance these competing demands, overprovisioned deep\nlearning models such as transformers are trained for a single epoch on large\ndata sets, and hence inefficient with both computing resources and training\ndata. In response to these inefficiencies, we exploit learning theory to derive\nOccam Gradient Descent, an algorithm that interleaves adaptive reduction of\nmodel size to minimize generalization error, with gradient descent on model\nweights to minimize fitting error. In contrast, traditional gradient descent\ngreedily minimizes fitting error without regard to generalization error. Our\nalgorithm simultaneously descends the space of weights and topological size of\nany neural network without modification. With respect to loss, compute and\nmodel size, our experiments show (a) on image classification benchmarks, linear\nand convolutional neural networks trained with Occam Gradient Descent\noutperform traditional gradient descent with or without post-train pruning; (b)\non a range of tabular data classification tasks, neural networks trained with\nOccam Gradient Descent outperform traditional gradient descent, as well as\nRandom Forests; (c) on natural language transformers, Occam Gradient Descent\noutperforms traditional gradient descent.\n", "link": "http://arxiv.org/abs/2405.20194v7", "date": "2024-12-30", "relevancy": 2.0874, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5339}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5311}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Occam%20Gradient%20Descent&body=Title%3A%20Occam%20Gradient%20Descent%0AAuthor%3A%20B.%20N.%20Kausik%0AAbstract%3A%20%20%20Deep%20learning%20neural%20network%20models%20must%20be%20large%20enough%20to%20adapt%20to%20their%0Aproblem%20domain%2C%20while%20small%20enough%20to%20avoid%20overfitting%20training%20data%20during%0Agradient%20descent.%20To%20balance%20these%20competing%20demands%2C%20overprovisioned%20deep%0Alearning%20models%20such%20as%20transformers%20are%20trained%20for%20a%20single%20epoch%20on%20large%0Adata%20sets%2C%20and%20hence%20inefficient%20with%20both%20computing%20resources%20and%20training%0Adata.%20In%20response%20to%20these%20inefficiencies%2C%20we%20exploit%20learning%20theory%20to%20derive%0AOccam%20Gradient%20Descent%2C%20an%20algorithm%20that%20interleaves%20adaptive%20reduction%20of%0Amodel%20size%20to%20minimize%20generalization%20error%2C%20with%20gradient%20descent%20on%20model%0Aweights%20to%20minimize%20fitting%20error.%20In%20contrast%2C%20traditional%20gradient%20descent%0Agreedily%20minimizes%20fitting%20error%20without%20regard%20to%20generalization%20error.%20Our%0Aalgorithm%20simultaneously%20descends%20the%20space%20of%20weights%20and%20topological%20size%20of%0Aany%20neural%20network%20without%20modification.%20With%20respect%20to%20loss%2C%20compute%20and%0Amodel%20size%2C%20our%20experiments%20show%20%28a%29%20on%20image%20classification%20benchmarks%2C%20linear%0Aand%20convolutional%20neural%20networks%20trained%20with%20Occam%20Gradient%20Descent%0Aoutperform%20traditional%20gradient%20descent%20with%20or%20without%20post-train%20pruning%3B%20%28b%29%0Aon%20a%20range%20of%20tabular%20data%20classification%20tasks%2C%20neural%20networks%20trained%20with%0AOccam%20Gradient%20Descent%20outperform%20traditional%20gradient%20descent%2C%20as%20well%20as%0ARandom%20Forests%3B%20%28c%29%20on%20natural%20language%20transformers%2C%20Occam%20Gradient%20Descent%0Aoutperforms%20traditional%20gradient%20descent.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20194v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOccam%2520Gradient%2520Descent%26entry.906535625%3DB.%2520N.%2520Kausik%26entry.1292438233%3D%2520%2520Deep%2520learning%2520neural%2520network%2520models%2520must%2520be%2520large%2520enough%2520to%2520adapt%2520to%2520their%250Aproblem%2520domain%252C%2520while%2520small%2520enough%2520to%2520avoid%2520overfitting%2520training%2520data%2520during%250Agradient%2520descent.%2520To%2520balance%2520these%2520competing%2520demands%252C%2520overprovisioned%2520deep%250Alearning%2520models%2520such%2520as%2520transformers%2520are%2520trained%2520for%2520a%2520single%2520epoch%2520on%2520large%250Adata%2520sets%252C%2520and%2520hence%2520inefficient%2520with%2520both%2520computing%2520resources%2520and%2520training%250Adata.%2520In%2520response%2520to%2520these%2520inefficiencies%252C%2520we%2520exploit%2520learning%2520theory%2520to%2520derive%250AOccam%2520Gradient%2520Descent%252C%2520an%2520algorithm%2520that%2520interleaves%2520adaptive%2520reduction%2520of%250Amodel%2520size%2520to%2520minimize%2520generalization%2520error%252C%2520with%2520gradient%2520descent%2520on%2520model%250Aweights%2520to%2520minimize%2520fitting%2520error.%2520In%2520contrast%252C%2520traditional%2520gradient%2520descent%250Agreedily%2520minimizes%2520fitting%2520error%2520without%2520regard%2520to%2520generalization%2520error.%2520Our%250Aalgorithm%2520simultaneously%2520descends%2520the%2520space%2520of%2520weights%2520and%2520topological%2520size%2520of%250Aany%2520neural%2520network%2520without%2520modification.%2520With%2520respect%2520to%2520loss%252C%2520compute%2520and%250Amodel%2520size%252C%2520our%2520experiments%2520show%2520%2528a%2529%2520on%2520image%2520classification%2520benchmarks%252C%2520linear%250Aand%2520convolutional%2520neural%2520networks%2520trained%2520with%2520Occam%2520Gradient%2520Descent%250Aoutperform%2520traditional%2520gradient%2520descent%2520with%2520or%2520without%2520post-train%2520pruning%253B%2520%2528b%2529%250Aon%2520a%2520range%2520of%2520tabular%2520data%2520classification%2520tasks%252C%2520neural%2520networks%2520trained%2520with%250AOccam%2520Gradient%2520Descent%2520outperform%2520traditional%2520gradient%2520descent%252C%2520as%2520well%2520as%250ARandom%2520Forests%253B%2520%2528c%2529%2520on%2520natural%2520language%2520transformers%252C%2520Occam%2520Gradient%2520Descent%250Aoutperforms%2520traditional%2520gradient%2520descent.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20194v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Occam%20Gradient%20Descent&entry.906535625=B.%20N.%20Kausik&entry.1292438233=%20%20Deep%20learning%20neural%20network%20models%20must%20be%20large%20enough%20to%20adapt%20to%20their%0Aproblem%20domain%2C%20while%20small%20enough%20to%20avoid%20overfitting%20training%20data%20during%0Agradient%20descent.%20To%20balance%20these%20competing%20demands%2C%20overprovisioned%20deep%0Alearning%20models%20such%20as%20transformers%20are%20trained%20for%20a%20single%20epoch%20on%20large%0Adata%20sets%2C%20and%20hence%20inefficient%20with%20both%20computing%20resources%20and%20training%0Adata.%20In%20response%20to%20these%20inefficiencies%2C%20we%20exploit%20learning%20theory%20to%20derive%0AOccam%20Gradient%20Descent%2C%20an%20algorithm%20that%20interleaves%20adaptive%20reduction%20of%0Amodel%20size%20to%20minimize%20generalization%20error%2C%20with%20gradient%20descent%20on%20model%0Aweights%20to%20minimize%20fitting%20error.%20In%20contrast%2C%20traditional%20gradient%20descent%0Agreedily%20minimizes%20fitting%20error%20without%20regard%20to%20generalization%20error.%20Our%0Aalgorithm%20simultaneously%20descends%20the%20space%20of%20weights%20and%20topological%20size%20of%0Aany%20neural%20network%20without%20modification.%20With%20respect%20to%20loss%2C%20compute%20and%0Amodel%20size%2C%20our%20experiments%20show%20%28a%29%20on%20image%20classification%20benchmarks%2C%20linear%0Aand%20convolutional%20neural%20networks%20trained%20with%20Occam%20Gradient%20Descent%0Aoutperform%20traditional%20gradient%20descent%20with%20or%20without%20post-train%20pruning%3B%20%28b%29%0Aon%20a%20range%20of%20tabular%20data%20classification%20tasks%2C%20neural%20networks%20trained%20with%0AOccam%20Gradient%20Descent%20outperform%20traditional%20gradient%20descent%2C%20as%20well%20as%0ARandom%20Forests%3B%20%28c%29%20on%20natural%20language%20transformers%2C%20Occam%20Gradient%20Descent%0Aoutperforms%20traditional%20gradient%20descent.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20194v7&entry.124074799=Read"},
{"title": "Improving Generalization for AI-Synthesized Voice Detection", "author": "Hainan Ren and Li Lin and Chun-Hao Liu and Xin Wang and Shu Hu", "abstract": "  AI-synthesized voice technology has the potential to create realistic human\nvoices for beneficial applications, but it can also be misused for malicious\npurposes. While existing AI-synthesized voice detection models excel in\nintra-domain evaluation, they face challenges in generalizing across different\ndomains, potentially becoming obsolete as new voice generators emerge. Current\nsolutions use diverse data and advanced machine learning techniques (e.g.,\ndomain-invariant representation, self-supervised learning), but are limited by\npredefined vocoders and sensitivity to factors like background noise and\nspeaker identity. In this work, we introduce an innovative disentanglement\nframework aimed at extracting domain-agnostic artifact features related to\nvocoders. Utilizing these features, we enhance model learning in a flat loss\nlandscape, enabling escape from suboptimal solutions and improving\ngeneralization. Extensive experiments on benchmarks show our approach\noutperforms state-of-the-art methods, achieving up to 5.12% improvement in the\nequal error rate metric in intra-domain and 7.59% in cross-domain evaluations.\n", "link": "http://arxiv.org/abs/2412.19279v2", "date": "2024-12-30", "relevancy": 2.0867, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5338}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5272}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Generalization%20for%20AI-Synthesized%20Voice%20Detection&body=Title%3A%20Improving%20Generalization%20for%20AI-Synthesized%20Voice%20Detection%0AAuthor%3A%20Hainan%20Ren%20and%20Li%20Lin%20and%20Chun-Hao%20Liu%20and%20Xin%20Wang%20and%20Shu%20Hu%0AAbstract%3A%20%20%20AI-synthesized%20voice%20technology%20has%20the%20potential%20to%20create%20realistic%20human%0Avoices%20for%20beneficial%20applications%2C%20but%20it%20can%20also%20be%20misused%20for%20malicious%0Apurposes.%20While%20existing%20AI-synthesized%20voice%20detection%20models%20excel%20in%0Aintra-domain%20evaluation%2C%20they%20face%20challenges%20in%20generalizing%20across%20different%0Adomains%2C%20potentially%20becoming%20obsolete%20as%20new%20voice%20generators%20emerge.%20Current%0Asolutions%20use%20diverse%20data%20and%20advanced%20machine%20learning%20techniques%20%28e.g.%2C%0Adomain-invariant%20representation%2C%20self-supervised%20learning%29%2C%20but%20are%20limited%20by%0Apredefined%20vocoders%20and%20sensitivity%20to%20factors%20like%20background%20noise%20and%0Aspeaker%20identity.%20In%20this%20work%2C%20we%20introduce%20an%20innovative%20disentanglement%0Aframework%20aimed%20at%20extracting%20domain-agnostic%20artifact%20features%20related%20to%0Avocoders.%20Utilizing%20these%20features%2C%20we%20enhance%20model%20learning%20in%20a%20flat%20loss%0Alandscape%2C%20enabling%20escape%20from%20suboptimal%20solutions%20and%20improving%0Ageneralization.%20Extensive%20experiments%20on%20benchmarks%20show%20our%20approach%0Aoutperforms%20state-of-the-art%20methods%2C%20achieving%20up%20to%205.12%25%20improvement%20in%20the%0Aequal%20error%20rate%20metric%20in%20intra-domain%20and%207.59%25%20in%20cross-domain%20evaluations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19279v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Generalization%2520for%2520AI-Synthesized%2520Voice%2520Detection%26entry.906535625%3DHainan%2520Ren%2520and%2520Li%2520Lin%2520and%2520Chun-Hao%2520Liu%2520and%2520Xin%2520Wang%2520and%2520Shu%2520Hu%26entry.1292438233%3D%2520%2520AI-synthesized%2520voice%2520technology%2520has%2520the%2520potential%2520to%2520create%2520realistic%2520human%250Avoices%2520for%2520beneficial%2520applications%252C%2520but%2520it%2520can%2520also%2520be%2520misused%2520for%2520malicious%250Apurposes.%2520While%2520existing%2520AI-synthesized%2520voice%2520detection%2520models%2520excel%2520in%250Aintra-domain%2520evaluation%252C%2520they%2520face%2520challenges%2520in%2520generalizing%2520across%2520different%250Adomains%252C%2520potentially%2520becoming%2520obsolete%2520as%2520new%2520voice%2520generators%2520emerge.%2520Current%250Asolutions%2520use%2520diverse%2520data%2520and%2520advanced%2520machine%2520learning%2520techniques%2520%2528e.g.%252C%250Adomain-invariant%2520representation%252C%2520self-supervised%2520learning%2529%252C%2520but%2520are%2520limited%2520by%250Apredefined%2520vocoders%2520and%2520sensitivity%2520to%2520factors%2520like%2520background%2520noise%2520and%250Aspeaker%2520identity.%2520In%2520this%2520work%252C%2520we%2520introduce%2520an%2520innovative%2520disentanglement%250Aframework%2520aimed%2520at%2520extracting%2520domain-agnostic%2520artifact%2520features%2520related%2520to%250Avocoders.%2520Utilizing%2520these%2520features%252C%2520we%2520enhance%2520model%2520learning%2520in%2520a%2520flat%2520loss%250Alandscape%252C%2520enabling%2520escape%2520from%2520suboptimal%2520solutions%2520and%2520improving%250Ageneralization.%2520Extensive%2520experiments%2520on%2520benchmarks%2520show%2520our%2520approach%250Aoutperforms%2520state-of-the-art%2520methods%252C%2520achieving%2520up%2520to%25205.12%2525%2520improvement%2520in%2520the%250Aequal%2520error%2520rate%2520metric%2520in%2520intra-domain%2520and%25207.59%2525%2520in%2520cross-domain%2520evaluations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19279v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Generalization%20for%20AI-Synthesized%20Voice%20Detection&entry.906535625=Hainan%20Ren%20and%20Li%20Lin%20and%20Chun-Hao%20Liu%20and%20Xin%20Wang%20and%20Shu%20Hu&entry.1292438233=%20%20AI-synthesized%20voice%20technology%20has%20the%20potential%20to%20create%20realistic%20human%0Avoices%20for%20beneficial%20applications%2C%20but%20it%20can%20also%20be%20misused%20for%20malicious%0Apurposes.%20While%20existing%20AI-synthesized%20voice%20detection%20models%20excel%20in%0Aintra-domain%20evaluation%2C%20they%20face%20challenges%20in%20generalizing%20across%20different%0Adomains%2C%20potentially%20becoming%20obsolete%20as%20new%20voice%20generators%20emerge.%20Current%0Asolutions%20use%20diverse%20data%20and%20advanced%20machine%20learning%20techniques%20%28e.g.%2C%0Adomain-invariant%20representation%2C%20self-supervised%20learning%29%2C%20but%20are%20limited%20by%0Apredefined%20vocoders%20and%20sensitivity%20to%20factors%20like%20background%20noise%20and%0Aspeaker%20identity.%20In%20this%20work%2C%20we%20introduce%20an%20innovative%20disentanglement%0Aframework%20aimed%20at%20extracting%20domain-agnostic%20artifact%20features%20related%20to%0Avocoders.%20Utilizing%20these%20features%2C%20we%20enhance%20model%20learning%20in%20a%20flat%20loss%0Alandscape%2C%20enabling%20escape%20from%20suboptimal%20solutions%20and%20improving%0Ageneralization.%20Extensive%20experiments%20on%20benchmarks%20show%20our%20approach%0Aoutperforms%20state-of-the-art%20methods%2C%20achieving%20up%20to%205.12%25%20improvement%20in%20the%0Aequal%20error%20rate%20metric%20in%20intra-domain%20and%207.59%25%20in%20cross-domain%20evaluations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19279v2&entry.124074799=Read"},
{"title": "Towards Instance-Wise Calibration: Local Amortized Diagnostics and\n  Reshaping of Conditional Densities (LADaR)", "author": "Biprateep Dey and David Zhao and Brett H. Andrews and Jeffrey A. Newman and Rafael Izbicki and Ann B. Lee", "abstract": "  There is a growing interest in conditional density estimation and generative\nmodeling of a target $y$ given complex inputs $\\mathbf{x}$. However,\noff-the-shelf methods often lack instance-wise calibration -- that is, for\nindividual inputs $\\mathbf{x}$, the individual estimated probabilities can be\nvery different from the true probabilities, even when the estimates are\nreasonable when averaged over the entire population. This paper introduces the\nLADaR (Local Amortized Diagnostics and Reshaping of Conditional Densities)\nframework and proposes an algorithm called $\\texttt{Cal-PIT}$ that produces\ninterpretable local calibration diagnostics and includes a mechanism to\nrecalibrate the initial model. Our $\\texttt{Cal-PIT}$ algorithm learns a single\nlocal probability-probability map from calibration data to assess and quantify\nwhere corrections are needed across the feature space. When necessary, it\nreshapes the initial distribution into an estimate with approximate\ninstance-wise calibration. We illustrate the LADaR framework by applying\n$\\texttt{Cal-PIT}$ to synthetic examples, including probabilistic forecasting\nwith sequences of images as inputs, akin to predicting the wind speed of\ntropical cyclones from satellite imagery. Our main science application is\nconditional density estimation of galaxy distances given imaging data\n(so-called photometric redshift estimation). On a benchmark photometric\nredshift data challenge, $\\texttt{Cal-PIT}$ achieves better conditional density\nestimation (as measured by the conditional density estimation loss) than all 11\nother literature methods tested. This demonstrates its potential for meeting\nthe stringent photometric redshift requirements for next generation weak\ngravitational lensing analyses.\n", "link": "http://arxiv.org/abs/2205.14568v6", "date": "2024-12-30", "relevancy": 2.0815, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5349}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.523}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Instance-Wise%20Calibration%3A%20Local%20Amortized%20Diagnostics%20and%0A%20%20Reshaping%20of%20Conditional%20Densities%20%28LADaR%29&body=Title%3A%20Towards%20Instance-Wise%20Calibration%3A%20Local%20Amortized%20Diagnostics%20and%0A%20%20Reshaping%20of%20Conditional%20Densities%20%28LADaR%29%0AAuthor%3A%20Biprateep%20Dey%20and%20David%20Zhao%20and%20Brett%20H.%20Andrews%20and%20Jeffrey%20A.%20Newman%20and%20Rafael%20Izbicki%20and%20Ann%20B.%20Lee%0AAbstract%3A%20%20%20There%20is%20a%20growing%20interest%20in%20conditional%20density%20estimation%20and%20generative%0Amodeling%20of%20a%20target%20%24y%24%20given%20complex%20inputs%20%24%5Cmathbf%7Bx%7D%24.%20However%2C%0Aoff-the-shelf%20methods%20often%20lack%20instance-wise%20calibration%20--%20that%20is%2C%20for%0Aindividual%20inputs%20%24%5Cmathbf%7Bx%7D%24%2C%20the%20individual%20estimated%20probabilities%20can%20be%0Avery%20different%20from%20the%20true%20probabilities%2C%20even%20when%20the%20estimates%20are%0Areasonable%20when%20averaged%20over%20the%20entire%20population.%20This%20paper%20introduces%20the%0ALADaR%20%28Local%20Amortized%20Diagnostics%20and%20Reshaping%20of%20Conditional%20Densities%29%0Aframework%20and%20proposes%20an%20algorithm%20called%20%24%5Ctexttt%7BCal-PIT%7D%24%20that%20produces%0Ainterpretable%20local%20calibration%20diagnostics%20and%20includes%20a%20mechanism%20to%0Arecalibrate%20the%20initial%20model.%20Our%20%24%5Ctexttt%7BCal-PIT%7D%24%20algorithm%20learns%20a%20single%0Alocal%20probability-probability%20map%20from%20calibration%20data%20to%20assess%20and%20quantify%0Awhere%20corrections%20are%20needed%20across%20the%20feature%20space.%20When%20necessary%2C%20it%0Areshapes%20the%20initial%20distribution%20into%20an%20estimate%20with%20approximate%0Ainstance-wise%20calibration.%20We%20illustrate%20the%20LADaR%20framework%20by%20applying%0A%24%5Ctexttt%7BCal-PIT%7D%24%20to%20synthetic%20examples%2C%20including%20probabilistic%20forecasting%0Awith%20sequences%20of%20images%20as%20inputs%2C%20akin%20to%20predicting%20the%20wind%20speed%20of%0Atropical%20cyclones%20from%20satellite%20imagery.%20Our%20main%20science%20application%20is%0Aconditional%20density%20estimation%20of%20galaxy%20distances%20given%20imaging%20data%0A%28so-called%20photometric%20redshift%20estimation%29.%20On%20a%20benchmark%20photometric%0Aredshift%20data%20challenge%2C%20%24%5Ctexttt%7BCal-PIT%7D%24%20achieves%20better%20conditional%20density%0Aestimation%20%28as%20measured%20by%20the%20conditional%20density%20estimation%20loss%29%20than%20all%2011%0Aother%20literature%20methods%20tested.%20This%20demonstrates%20its%20potential%20for%20meeting%0Athe%20stringent%20photometric%20redshift%20requirements%20for%20next%20generation%20weak%0Agravitational%20lensing%20analyses.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2205.14568v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Instance-Wise%2520Calibration%253A%2520Local%2520Amortized%2520Diagnostics%2520and%250A%2520%2520Reshaping%2520of%2520Conditional%2520Densities%2520%2528LADaR%2529%26entry.906535625%3DBiprateep%2520Dey%2520and%2520David%2520Zhao%2520and%2520Brett%2520H.%2520Andrews%2520and%2520Jeffrey%2520A.%2520Newman%2520and%2520Rafael%2520Izbicki%2520and%2520Ann%2520B.%2520Lee%26entry.1292438233%3D%2520%2520There%2520is%2520a%2520growing%2520interest%2520in%2520conditional%2520density%2520estimation%2520and%2520generative%250Amodeling%2520of%2520a%2520target%2520%2524y%2524%2520given%2520complex%2520inputs%2520%2524%255Cmathbf%257Bx%257D%2524.%2520However%252C%250Aoff-the-shelf%2520methods%2520often%2520lack%2520instance-wise%2520calibration%2520--%2520that%2520is%252C%2520for%250Aindividual%2520inputs%2520%2524%255Cmathbf%257Bx%257D%2524%252C%2520the%2520individual%2520estimated%2520probabilities%2520can%2520be%250Avery%2520different%2520from%2520the%2520true%2520probabilities%252C%2520even%2520when%2520the%2520estimates%2520are%250Areasonable%2520when%2520averaged%2520over%2520the%2520entire%2520population.%2520This%2520paper%2520introduces%2520the%250ALADaR%2520%2528Local%2520Amortized%2520Diagnostics%2520and%2520Reshaping%2520of%2520Conditional%2520Densities%2529%250Aframework%2520and%2520proposes%2520an%2520algorithm%2520called%2520%2524%255Ctexttt%257BCal-PIT%257D%2524%2520that%2520produces%250Ainterpretable%2520local%2520calibration%2520diagnostics%2520and%2520includes%2520a%2520mechanism%2520to%250Arecalibrate%2520the%2520initial%2520model.%2520Our%2520%2524%255Ctexttt%257BCal-PIT%257D%2524%2520algorithm%2520learns%2520a%2520single%250Alocal%2520probability-probability%2520map%2520from%2520calibration%2520data%2520to%2520assess%2520and%2520quantify%250Awhere%2520corrections%2520are%2520needed%2520across%2520the%2520feature%2520space.%2520When%2520necessary%252C%2520it%250Areshapes%2520the%2520initial%2520distribution%2520into%2520an%2520estimate%2520with%2520approximate%250Ainstance-wise%2520calibration.%2520We%2520illustrate%2520the%2520LADaR%2520framework%2520by%2520applying%250A%2524%255Ctexttt%257BCal-PIT%257D%2524%2520to%2520synthetic%2520examples%252C%2520including%2520probabilistic%2520forecasting%250Awith%2520sequences%2520of%2520images%2520as%2520inputs%252C%2520akin%2520to%2520predicting%2520the%2520wind%2520speed%2520of%250Atropical%2520cyclones%2520from%2520satellite%2520imagery.%2520Our%2520main%2520science%2520application%2520is%250Aconditional%2520density%2520estimation%2520of%2520galaxy%2520distances%2520given%2520imaging%2520data%250A%2528so-called%2520photometric%2520redshift%2520estimation%2529.%2520On%2520a%2520benchmark%2520photometric%250Aredshift%2520data%2520challenge%252C%2520%2524%255Ctexttt%257BCal-PIT%257D%2524%2520achieves%2520better%2520conditional%2520density%250Aestimation%2520%2528as%2520measured%2520by%2520the%2520conditional%2520density%2520estimation%2520loss%2529%2520than%2520all%252011%250Aother%2520literature%2520methods%2520tested.%2520This%2520demonstrates%2520its%2520potential%2520for%2520meeting%250Athe%2520stringent%2520photometric%2520redshift%2520requirements%2520for%2520next%2520generation%2520weak%250Agravitational%2520lensing%2520analyses.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2205.14568v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Instance-Wise%20Calibration%3A%20Local%20Amortized%20Diagnostics%20and%0A%20%20Reshaping%20of%20Conditional%20Densities%20%28LADaR%29&entry.906535625=Biprateep%20Dey%20and%20David%20Zhao%20and%20Brett%20H.%20Andrews%20and%20Jeffrey%20A.%20Newman%20and%20Rafael%20Izbicki%20and%20Ann%20B.%20Lee&entry.1292438233=%20%20There%20is%20a%20growing%20interest%20in%20conditional%20density%20estimation%20and%20generative%0Amodeling%20of%20a%20target%20%24y%24%20given%20complex%20inputs%20%24%5Cmathbf%7Bx%7D%24.%20However%2C%0Aoff-the-shelf%20methods%20often%20lack%20instance-wise%20calibration%20--%20that%20is%2C%20for%0Aindividual%20inputs%20%24%5Cmathbf%7Bx%7D%24%2C%20the%20individual%20estimated%20probabilities%20can%20be%0Avery%20different%20from%20the%20true%20probabilities%2C%20even%20when%20the%20estimates%20are%0Areasonable%20when%20averaged%20over%20the%20entire%20population.%20This%20paper%20introduces%20the%0ALADaR%20%28Local%20Amortized%20Diagnostics%20and%20Reshaping%20of%20Conditional%20Densities%29%0Aframework%20and%20proposes%20an%20algorithm%20called%20%24%5Ctexttt%7BCal-PIT%7D%24%20that%20produces%0Ainterpretable%20local%20calibration%20diagnostics%20and%20includes%20a%20mechanism%20to%0Arecalibrate%20the%20initial%20model.%20Our%20%24%5Ctexttt%7BCal-PIT%7D%24%20algorithm%20learns%20a%20single%0Alocal%20probability-probability%20map%20from%20calibration%20data%20to%20assess%20and%20quantify%0Awhere%20corrections%20are%20needed%20across%20the%20feature%20space.%20When%20necessary%2C%20it%0Areshapes%20the%20initial%20distribution%20into%20an%20estimate%20with%20approximate%0Ainstance-wise%20calibration.%20We%20illustrate%20the%20LADaR%20framework%20by%20applying%0A%24%5Ctexttt%7BCal-PIT%7D%24%20to%20synthetic%20examples%2C%20including%20probabilistic%20forecasting%0Awith%20sequences%20of%20images%20as%20inputs%2C%20akin%20to%20predicting%20the%20wind%20speed%20of%0Atropical%20cyclones%20from%20satellite%20imagery.%20Our%20main%20science%20application%20is%0Aconditional%20density%20estimation%20of%20galaxy%20distances%20given%20imaging%20data%0A%28so-called%20photometric%20redshift%20estimation%29.%20On%20a%20benchmark%20photometric%0Aredshift%20data%20challenge%2C%20%24%5Ctexttt%7BCal-PIT%7D%24%20achieves%20better%20conditional%20density%0Aestimation%20%28as%20measured%20by%20the%20conditional%20density%20estimation%20loss%29%20than%20all%2011%0Aother%20literature%20methods%20tested.%20This%20demonstrates%20its%20potential%20for%20meeting%0Athe%20stringent%20photometric%20redshift%20requirements%20for%20next%20generation%20weak%0Agravitational%20lensing%20analyses.%0A&entry.1838667208=http%3A//arxiv.org/abs/2205.14568v6&entry.124074799=Read"},
{"title": "Uncertainty-Aware Out-of-Distribution Detection with Gaussian Processes", "author": "Yang Chen and Chih-Li Sung and Arpan Kusari and Xiaoyang Song and Wenbo Sun", "abstract": "  Deep neural networks (DNNs) are often constructed under the closed-world\nassumption, which may fail to generalize to the out-of-distribution (OOD) data.\nThis leads to DNNs producing overconfident wrong predictions and can result in\ndisastrous consequences in safety-critical applications. Existing OOD detection\nmethods mainly rely on curating a set of OOD data for model training or\nhyper-parameter tuning to distinguish OOD data from training data (also known\nas in-distribution data or InD data). However, OOD samples are not always\navailable during the training phase in real-world applications, hindering the\nOOD detection accuracy. To overcome this limitation, we propose a\nGaussian-process-based OOD detection method to establish a decision boundary\nbased on InD data only. The basic idea is to perform uncertainty quantification\nof the unconstrained softmax scores of a DNN via a multi-class Gaussian process\n(GP), and then define a score function to separate InD and potential OOD data\nbased on their fundamental differences in the posterior predictive distribution\nfrom the GP. Two case studies on conventional image classification datasets and\nreal-world image datasets are conducted to demonstrate that the proposed method\noutperforms the state-of-the-art OOD detection methods when OOD samples are not\nobserved in the training phase.\n", "link": "http://arxiv.org/abs/2412.20918v1", "date": "2024-12-30", "relevancy": 2.0806, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5367}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5193}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty-Aware%20Out-of-Distribution%20Detection%20with%20Gaussian%20Processes&body=Title%3A%20Uncertainty-Aware%20Out-of-Distribution%20Detection%20with%20Gaussian%20Processes%0AAuthor%3A%20Yang%20Chen%20and%20Chih-Li%20Sung%20and%20Arpan%20Kusari%20and%20Xiaoyang%20Song%20and%20Wenbo%20Sun%0AAbstract%3A%20%20%20Deep%20neural%20networks%20%28DNNs%29%20are%20often%20constructed%20under%20the%20closed-world%0Aassumption%2C%20which%20may%20fail%20to%20generalize%20to%20the%20out-of-distribution%20%28OOD%29%20data.%0AThis%20leads%20to%20DNNs%20producing%20overconfident%20wrong%20predictions%20and%20can%20result%20in%0Adisastrous%20consequences%20in%20safety-critical%20applications.%20Existing%20OOD%20detection%0Amethods%20mainly%20rely%20on%20curating%20a%20set%20of%20OOD%20data%20for%20model%20training%20or%0Ahyper-parameter%20tuning%20to%20distinguish%20OOD%20data%20from%20training%20data%20%28also%20known%0Aas%20in-distribution%20data%20or%20InD%20data%29.%20However%2C%20OOD%20samples%20are%20not%20always%0Aavailable%20during%20the%20training%20phase%20in%20real-world%20applications%2C%20hindering%20the%0AOOD%20detection%20accuracy.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%0AGaussian-process-based%20OOD%20detection%20method%20to%20establish%20a%20decision%20boundary%0Abased%20on%20InD%20data%20only.%20The%20basic%20idea%20is%20to%20perform%20uncertainty%20quantification%0Aof%20the%20unconstrained%20softmax%20scores%20of%20a%20DNN%20via%20a%20multi-class%20Gaussian%20process%0A%28GP%29%2C%20and%20then%20define%20a%20score%20function%20to%20separate%20InD%20and%20potential%20OOD%20data%0Abased%20on%20their%20fundamental%20differences%20in%20the%20posterior%20predictive%20distribution%0Afrom%20the%20GP.%20Two%20case%20studies%20on%20conventional%20image%20classification%20datasets%20and%0Areal-world%20image%20datasets%20are%20conducted%20to%20demonstrate%20that%20the%20proposed%20method%0Aoutperforms%20the%20state-of-the-art%20OOD%20detection%20methods%20when%20OOD%20samples%20are%20not%0Aobserved%20in%20the%20training%20phase.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20918v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty-Aware%2520Out-of-Distribution%2520Detection%2520with%2520Gaussian%2520Processes%26entry.906535625%3DYang%2520Chen%2520and%2520Chih-Li%2520Sung%2520and%2520Arpan%2520Kusari%2520and%2520Xiaoyang%2520Song%2520and%2520Wenbo%2520Sun%26entry.1292438233%3D%2520%2520Deep%2520neural%2520networks%2520%2528DNNs%2529%2520are%2520often%2520constructed%2520under%2520the%2520closed-world%250Aassumption%252C%2520which%2520may%2520fail%2520to%2520generalize%2520to%2520the%2520out-of-distribution%2520%2528OOD%2529%2520data.%250AThis%2520leads%2520to%2520DNNs%2520producing%2520overconfident%2520wrong%2520predictions%2520and%2520can%2520result%2520in%250Adisastrous%2520consequences%2520in%2520safety-critical%2520applications.%2520Existing%2520OOD%2520detection%250Amethods%2520mainly%2520rely%2520on%2520curating%2520a%2520set%2520of%2520OOD%2520data%2520for%2520model%2520training%2520or%250Ahyper-parameter%2520tuning%2520to%2520distinguish%2520OOD%2520data%2520from%2520training%2520data%2520%2528also%2520known%250Aas%2520in-distribution%2520data%2520or%2520InD%2520data%2529.%2520However%252C%2520OOD%2520samples%2520are%2520not%2520always%250Aavailable%2520during%2520the%2520training%2520phase%2520in%2520real-world%2520applications%252C%2520hindering%2520the%250AOOD%2520detection%2520accuracy.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520a%250AGaussian-process-based%2520OOD%2520detection%2520method%2520to%2520establish%2520a%2520decision%2520boundary%250Abased%2520on%2520InD%2520data%2520only.%2520The%2520basic%2520idea%2520is%2520to%2520perform%2520uncertainty%2520quantification%250Aof%2520the%2520unconstrained%2520softmax%2520scores%2520of%2520a%2520DNN%2520via%2520a%2520multi-class%2520Gaussian%2520process%250A%2528GP%2529%252C%2520and%2520then%2520define%2520a%2520score%2520function%2520to%2520separate%2520InD%2520and%2520potential%2520OOD%2520data%250Abased%2520on%2520their%2520fundamental%2520differences%2520in%2520the%2520posterior%2520predictive%2520distribution%250Afrom%2520the%2520GP.%2520Two%2520case%2520studies%2520on%2520conventional%2520image%2520classification%2520datasets%2520and%250Areal-world%2520image%2520datasets%2520are%2520conducted%2520to%2520demonstrate%2520that%2520the%2520proposed%2520method%250Aoutperforms%2520the%2520state-of-the-art%2520OOD%2520detection%2520methods%2520when%2520OOD%2520samples%2520are%2520not%250Aobserved%2520in%2520the%2520training%2520phase.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20918v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty-Aware%20Out-of-Distribution%20Detection%20with%20Gaussian%20Processes&entry.906535625=Yang%20Chen%20and%20Chih-Li%20Sung%20and%20Arpan%20Kusari%20and%20Xiaoyang%20Song%20and%20Wenbo%20Sun&entry.1292438233=%20%20Deep%20neural%20networks%20%28DNNs%29%20are%20often%20constructed%20under%20the%20closed-world%0Aassumption%2C%20which%20may%20fail%20to%20generalize%20to%20the%20out-of-distribution%20%28OOD%29%20data.%0AThis%20leads%20to%20DNNs%20producing%20overconfident%20wrong%20predictions%20and%20can%20result%20in%0Adisastrous%20consequences%20in%20safety-critical%20applications.%20Existing%20OOD%20detection%0Amethods%20mainly%20rely%20on%20curating%20a%20set%20of%20OOD%20data%20for%20model%20training%20or%0Ahyper-parameter%20tuning%20to%20distinguish%20OOD%20data%20from%20training%20data%20%28also%20known%0Aas%20in-distribution%20data%20or%20InD%20data%29.%20However%2C%20OOD%20samples%20are%20not%20always%0Aavailable%20during%20the%20training%20phase%20in%20real-world%20applications%2C%20hindering%20the%0AOOD%20detection%20accuracy.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%0AGaussian-process-based%20OOD%20detection%20method%20to%20establish%20a%20decision%20boundary%0Abased%20on%20InD%20data%20only.%20The%20basic%20idea%20is%20to%20perform%20uncertainty%20quantification%0Aof%20the%20unconstrained%20softmax%20scores%20of%20a%20DNN%20via%20a%20multi-class%20Gaussian%20process%0A%28GP%29%2C%20and%20then%20define%20a%20score%20function%20to%20separate%20InD%20and%20potential%20OOD%20data%0Abased%20on%20their%20fundamental%20differences%20in%20the%20posterior%20predictive%20distribution%0Afrom%20the%20GP.%20Two%20case%20studies%20on%20conventional%20image%20classification%20datasets%20and%0Areal-world%20image%20datasets%20are%20conducted%20to%20demonstrate%20that%20the%20proposed%20method%0Aoutperforms%20the%20state-of-the-art%20OOD%20detection%20methods%20when%20OOD%20samples%20are%20not%0Aobserved%20in%20the%20training%20phase.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20918v1&entry.124074799=Read"},
{"title": "Hedging Is Not All You Need: A Simple Baseline for Online Learning Under\n  Haphazard Inputs", "author": "Himanshu Buckchash and Momojit Biswas and Rohit Agarwal and Dilip K. Prasad", "abstract": "  Handling haphazard streaming data, such as data from edge devices, presents a\nchallenging problem. Over time, the incoming data becomes inconsistent, with\nmissing, faulty, or new inputs reappearing. Therefore, it requires models that\nare reliable. Recent methods to solve this problem depend on a hedging-based\nsolution and require specialized elements like auxiliary dropouts, forked\narchitectures, and intricate network design. We observed that hedging can be\nreduced to a special case of weighted residual connection; this motivated us to\napproximate it with plain self-attention. In this work, we propose HapNet, a\nsimple baseline that is scalable, does not require online backpropagation, and\nis adaptable to varying input types. All present methods are restricted to\nscaling with a fixed window; however, we introduce a more complex problem of\nscaling with a variable window where the data becomes positionally\nuncorrelated, and cannot be addressed by present methods. We demonstrate that a\nvariant of the proposed approach can work even for this complex scenario. We\nextensively evaluated the proposed approach on five benchmarks and found\ncompetitive performance.\n", "link": "http://arxiv.org/abs/2409.10242v2", "date": "2024-12-30", "relevancy": 2.0784, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5491}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5022}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hedging%20Is%20Not%20All%20You%20Need%3A%20A%20Simple%20Baseline%20for%20Online%20Learning%20Under%0A%20%20Haphazard%20Inputs&body=Title%3A%20Hedging%20Is%20Not%20All%20You%20Need%3A%20A%20Simple%20Baseline%20for%20Online%20Learning%20Under%0A%20%20Haphazard%20Inputs%0AAuthor%3A%20Himanshu%20Buckchash%20and%20Momojit%20Biswas%20and%20Rohit%20Agarwal%20and%20Dilip%20K.%20Prasad%0AAbstract%3A%20%20%20Handling%20haphazard%20streaming%20data%2C%20such%20as%20data%20from%20edge%20devices%2C%20presents%20a%0Achallenging%20problem.%20Over%20time%2C%20the%20incoming%20data%20becomes%20inconsistent%2C%20with%0Amissing%2C%20faulty%2C%20or%20new%20inputs%20reappearing.%20Therefore%2C%20it%20requires%20models%20that%0Aare%20reliable.%20Recent%20methods%20to%20solve%20this%20problem%20depend%20on%20a%20hedging-based%0Asolution%20and%20require%20specialized%20elements%20like%20auxiliary%20dropouts%2C%20forked%0Aarchitectures%2C%20and%20intricate%20network%20design.%20We%20observed%20that%20hedging%20can%20be%0Areduced%20to%20a%20special%20case%20of%20weighted%20residual%20connection%3B%20this%20motivated%20us%20to%0Aapproximate%20it%20with%20plain%20self-attention.%20In%20this%20work%2C%20we%20propose%20HapNet%2C%20a%0Asimple%20baseline%20that%20is%20scalable%2C%20does%20not%20require%20online%20backpropagation%2C%20and%0Ais%20adaptable%20to%20varying%20input%20types.%20All%20present%20methods%20are%20restricted%20to%0Ascaling%20with%20a%20fixed%20window%3B%20however%2C%20we%20introduce%20a%20more%20complex%20problem%20of%0Ascaling%20with%20a%20variable%20window%20where%20the%20data%20becomes%20positionally%0Auncorrelated%2C%20and%20cannot%20be%20addressed%20by%20present%20methods.%20We%20demonstrate%20that%20a%0Avariant%20of%20the%20proposed%20approach%20can%20work%20even%20for%20this%20complex%20scenario.%20We%0Aextensively%20evaluated%20the%20proposed%20approach%20on%20five%20benchmarks%20and%20found%0Acompetitive%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.10242v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHedging%2520Is%2520Not%2520All%2520You%2520Need%253A%2520A%2520Simple%2520Baseline%2520for%2520Online%2520Learning%2520Under%250A%2520%2520Haphazard%2520Inputs%26entry.906535625%3DHimanshu%2520Buckchash%2520and%2520Momojit%2520Biswas%2520and%2520Rohit%2520Agarwal%2520and%2520Dilip%2520K.%2520Prasad%26entry.1292438233%3D%2520%2520Handling%2520haphazard%2520streaming%2520data%252C%2520such%2520as%2520data%2520from%2520edge%2520devices%252C%2520presents%2520a%250Achallenging%2520problem.%2520Over%2520time%252C%2520the%2520incoming%2520data%2520becomes%2520inconsistent%252C%2520with%250Amissing%252C%2520faulty%252C%2520or%2520new%2520inputs%2520reappearing.%2520Therefore%252C%2520it%2520requires%2520models%2520that%250Aare%2520reliable.%2520Recent%2520methods%2520to%2520solve%2520this%2520problem%2520depend%2520on%2520a%2520hedging-based%250Asolution%2520and%2520require%2520specialized%2520elements%2520like%2520auxiliary%2520dropouts%252C%2520forked%250Aarchitectures%252C%2520and%2520intricate%2520network%2520design.%2520We%2520observed%2520that%2520hedging%2520can%2520be%250Areduced%2520to%2520a%2520special%2520case%2520of%2520weighted%2520residual%2520connection%253B%2520this%2520motivated%2520us%2520to%250Aapproximate%2520it%2520with%2520plain%2520self-attention.%2520In%2520this%2520work%252C%2520we%2520propose%2520HapNet%252C%2520a%250Asimple%2520baseline%2520that%2520is%2520scalable%252C%2520does%2520not%2520require%2520online%2520backpropagation%252C%2520and%250Ais%2520adaptable%2520to%2520varying%2520input%2520types.%2520All%2520present%2520methods%2520are%2520restricted%2520to%250Ascaling%2520with%2520a%2520fixed%2520window%253B%2520however%252C%2520we%2520introduce%2520a%2520more%2520complex%2520problem%2520of%250Ascaling%2520with%2520a%2520variable%2520window%2520where%2520the%2520data%2520becomes%2520positionally%250Auncorrelated%252C%2520and%2520cannot%2520be%2520addressed%2520by%2520present%2520methods.%2520We%2520demonstrate%2520that%2520a%250Avariant%2520of%2520the%2520proposed%2520approach%2520can%2520work%2520even%2520for%2520this%2520complex%2520scenario.%2520We%250Aextensively%2520evaluated%2520the%2520proposed%2520approach%2520on%2520five%2520benchmarks%2520and%2520found%250Acompetitive%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.10242v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hedging%20Is%20Not%20All%20You%20Need%3A%20A%20Simple%20Baseline%20for%20Online%20Learning%20Under%0A%20%20Haphazard%20Inputs&entry.906535625=Himanshu%20Buckchash%20and%20Momojit%20Biswas%20and%20Rohit%20Agarwal%20and%20Dilip%20K.%20Prasad&entry.1292438233=%20%20Handling%20haphazard%20streaming%20data%2C%20such%20as%20data%20from%20edge%20devices%2C%20presents%20a%0Achallenging%20problem.%20Over%20time%2C%20the%20incoming%20data%20becomes%20inconsistent%2C%20with%0Amissing%2C%20faulty%2C%20or%20new%20inputs%20reappearing.%20Therefore%2C%20it%20requires%20models%20that%0Aare%20reliable.%20Recent%20methods%20to%20solve%20this%20problem%20depend%20on%20a%20hedging-based%0Asolution%20and%20require%20specialized%20elements%20like%20auxiliary%20dropouts%2C%20forked%0Aarchitectures%2C%20and%20intricate%20network%20design.%20We%20observed%20that%20hedging%20can%20be%0Areduced%20to%20a%20special%20case%20of%20weighted%20residual%20connection%3B%20this%20motivated%20us%20to%0Aapproximate%20it%20with%20plain%20self-attention.%20In%20this%20work%2C%20we%20propose%20HapNet%2C%20a%0Asimple%20baseline%20that%20is%20scalable%2C%20does%20not%20require%20online%20backpropagation%2C%20and%0Ais%20adaptable%20to%20varying%20input%20types.%20All%20present%20methods%20are%20restricted%20to%0Ascaling%20with%20a%20fixed%20window%3B%20however%2C%20we%20introduce%20a%20more%20complex%20problem%20of%0Ascaling%20with%20a%20variable%20window%20where%20the%20data%20becomes%20positionally%0Auncorrelated%2C%20and%20cannot%20be%20addressed%20by%20present%20methods.%20We%20demonstrate%20that%20a%0Avariant%20of%20the%20proposed%20approach%20can%20work%20even%20for%20this%20complex%20scenario.%20We%0Aextensively%20evaluated%20the%20proposed%20approach%20on%20five%20benchmarks%20and%20found%0Acompetitive%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.10242v2&entry.124074799=Read"},
{"title": "DoTA: Weight-Decomposed Tensor Adaptation for Large Language Models", "author": "Xiaolin Hu and Xiang Cheng and Peiyu Liu and Wei Liu and Jian Luan and Bin Wang and Yong Liu", "abstract": "  Low-rank adaptation (LoRA) reduces the computational and memory demands of\nfine-tuning large language models (LLMs) by approximating updates with low-rank\nmatrices. However, low-rank approximation in two-dimensional space fails to\ncapture high-dimensional structures within the target matrix. Recently, tensor\ndecomposition methods have been explored for fine-tuning LLMs, leveraging their\nability to extract structured information. Yet, these approaches primarily rely\non random initialization, and the impact of initialization on tensor adaptation\nremains underexplored. In this paper, we reveal that random initialization\nsignificantly diverges from the validation loss achieved by full fine-tuning.\nTo address this, we propose Weight-Decomposed Tensor Adaptation (DoTA), which\nleverages the Matrix Product Operator (MPO) decomposition of pre-trained\nweights for effective initialization in fine-tuning LLMs. Additionally, we\nintroduce QDoTA, a quantized version of DoTA designed for 4-bit quantization.\nExperiments on commonsense and arithmetic reasoning tasks show that DoTA\noutperforms random initialization methods with fewer parameters. QDoTA further\nreduces memory consumption and achieves comparable performance to DoTA on\ncommonsense reasoning tasks. We will release our code to support future\nresearch.\n", "link": "http://arxiv.org/abs/2412.20891v1", "date": "2024-12-30", "relevancy": 2.0783, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5432}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5067}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DoTA%3A%20Weight-Decomposed%20Tensor%20Adaptation%20for%20Large%20Language%20Models&body=Title%3A%20DoTA%3A%20Weight-Decomposed%20Tensor%20Adaptation%20for%20Large%20Language%20Models%0AAuthor%3A%20Xiaolin%20Hu%20and%20Xiang%20Cheng%20and%20Peiyu%20Liu%20and%20Wei%20Liu%20and%20Jian%20Luan%20and%20Bin%20Wang%20and%20Yong%20Liu%0AAbstract%3A%20%20%20Low-rank%20adaptation%20%28LoRA%29%20reduces%20the%20computational%20and%20memory%20demands%20of%0Afine-tuning%20large%20language%20models%20%28LLMs%29%20by%20approximating%20updates%20with%20low-rank%0Amatrices.%20However%2C%20low-rank%20approximation%20in%20two-dimensional%20space%20fails%20to%0Acapture%20high-dimensional%20structures%20within%20the%20target%20matrix.%20Recently%2C%20tensor%0Adecomposition%20methods%20have%20been%20explored%20for%20fine-tuning%20LLMs%2C%20leveraging%20their%0Aability%20to%20extract%20structured%20information.%20Yet%2C%20these%20approaches%20primarily%20rely%0Aon%20random%20initialization%2C%20and%20the%20impact%20of%20initialization%20on%20tensor%20adaptation%0Aremains%20underexplored.%20In%20this%20paper%2C%20we%20reveal%20that%20random%20initialization%0Asignificantly%20diverges%20from%20the%20validation%20loss%20achieved%20by%20full%20fine-tuning.%0ATo%20address%20this%2C%20we%20propose%20Weight-Decomposed%20Tensor%20Adaptation%20%28DoTA%29%2C%20which%0Aleverages%20the%20Matrix%20Product%20Operator%20%28MPO%29%20decomposition%20of%20pre-trained%0Aweights%20for%20effective%20initialization%20in%20fine-tuning%20LLMs.%20Additionally%2C%20we%0Aintroduce%20QDoTA%2C%20a%20quantized%20version%20of%20DoTA%20designed%20for%204-bit%20quantization.%0AExperiments%20on%20commonsense%20and%20arithmetic%20reasoning%20tasks%20show%20that%20DoTA%0Aoutperforms%20random%20initialization%20methods%20with%20fewer%20parameters.%20QDoTA%20further%0Areduces%20memory%20consumption%20and%20achieves%20comparable%20performance%20to%20DoTA%20on%0Acommonsense%20reasoning%20tasks.%20We%20will%20release%20our%20code%20to%20support%20future%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20891v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoTA%253A%2520Weight-Decomposed%2520Tensor%2520Adaptation%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DXiaolin%2520Hu%2520and%2520Xiang%2520Cheng%2520and%2520Peiyu%2520Liu%2520and%2520Wei%2520Liu%2520and%2520Jian%2520Luan%2520and%2520Bin%2520Wang%2520and%2520Yong%2520Liu%26entry.1292438233%3D%2520%2520Low-rank%2520adaptation%2520%2528LoRA%2529%2520reduces%2520the%2520computational%2520and%2520memory%2520demands%2520of%250Afine-tuning%2520large%2520language%2520models%2520%2528LLMs%2529%2520by%2520approximating%2520updates%2520with%2520low-rank%250Amatrices.%2520However%252C%2520low-rank%2520approximation%2520in%2520two-dimensional%2520space%2520fails%2520to%250Acapture%2520high-dimensional%2520structures%2520within%2520the%2520target%2520matrix.%2520Recently%252C%2520tensor%250Adecomposition%2520methods%2520have%2520been%2520explored%2520for%2520fine-tuning%2520LLMs%252C%2520leveraging%2520their%250Aability%2520to%2520extract%2520structured%2520information.%2520Yet%252C%2520these%2520approaches%2520primarily%2520rely%250Aon%2520random%2520initialization%252C%2520and%2520the%2520impact%2520of%2520initialization%2520on%2520tensor%2520adaptation%250Aremains%2520underexplored.%2520In%2520this%2520paper%252C%2520we%2520reveal%2520that%2520random%2520initialization%250Asignificantly%2520diverges%2520from%2520the%2520validation%2520loss%2520achieved%2520by%2520full%2520fine-tuning.%250ATo%2520address%2520this%252C%2520we%2520propose%2520Weight-Decomposed%2520Tensor%2520Adaptation%2520%2528DoTA%2529%252C%2520which%250Aleverages%2520the%2520Matrix%2520Product%2520Operator%2520%2528MPO%2529%2520decomposition%2520of%2520pre-trained%250Aweights%2520for%2520effective%2520initialization%2520in%2520fine-tuning%2520LLMs.%2520Additionally%252C%2520we%250Aintroduce%2520QDoTA%252C%2520a%2520quantized%2520version%2520of%2520DoTA%2520designed%2520for%25204-bit%2520quantization.%250AExperiments%2520on%2520commonsense%2520and%2520arithmetic%2520reasoning%2520tasks%2520show%2520that%2520DoTA%250Aoutperforms%2520random%2520initialization%2520methods%2520with%2520fewer%2520parameters.%2520QDoTA%2520further%250Areduces%2520memory%2520consumption%2520and%2520achieves%2520comparable%2520performance%2520to%2520DoTA%2520on%250Acommonsense%2520reasoning%2520tasks.%2520We%2520will%2520release%2520our%2520code%2520to%2520support%2520future%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20891v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DoTA%3A%20Weight-Decomposed%20Tensor%20Adaptation%20for%20Large%20Language%20Models&entry.906535625=Xiaolin%20Hu%20and%20Xiang%20Cheng%20and%20Peiyu%20Liu%20and%20Wei%20Liu%20and%20Jian%20Luan%20and%20Bin%20Wang%20and%20Yong%20Liu&entry.1292438233=%20%20Low-rank%20adaptation%20%28LoRA%29%20reduces%20the%20computational%20and%20memory%20demands%20of%0Afine-tuning%20large%20language%20models%20%28LLMs%29%20by%20approximating%20updates%20with%20low-rank%0Amatrices.%20However%2C%20low-rank%20approximation%20in%20two-dimensional%20space%20fails%20to%0Acapture%20high-dimensional%20structures%20within%20the%20target%20matrix.%20Recently%2C%20tensor%0Adecomposition%20methods%20have%20been%20explored%20for%20fine-tuning%20LLMs%2C%20leveraging%20their%0Aability%20to%20extract%20structured%20information.%20Yet%2C%20these%20approaches%20primarily%20rely%0Aon%20random%20initialization%2C%20and%20the%20impact%20of%20initialization%20on%20tensor%20adaptation%0Aremains%20underexplored.%20In%20this%20paper%2C%20we%20reveal%20that%20random%20initialization%0Asignificantly%20diverges%20from%20the%20validation%20loss%20achieved%20by%20full%20fine-tuning.%0ATo%20address%20this%2C%20we%20propose%20Weight-Decomposed%20Tensor%20Adaptation%20%28DoTA%29%2C%20which%0Aleverages%20the%20Matrix%20Product%20Operator%20%28MPO%29%20decomposition%20of%20pre-trained%0Aweights%20for%20effective%20initialization%20in%20fine-tuning%20LLMs.%20Additionally%2C%20we%0Aintroduce%20QDoTA%2C%20a%20quantized%20version%20of%20DoTA%20designed%20for%204-bit%20quantization.%0AExperiments%20on%20commonsense%20and%20arithmetic%20reasoning%20tasks%20show%20that%20DoTA%0Aoutperforms%20random%20initialization%20methods%20with%20fewer%20parameters.%20QDoTA%20further%0Areduces%20memory%20consumption%20and%20achieves%20comparable%20performance%20to%20DoTA%20on%0Acommonsense%20reasoning%20tasks.%20We%20will%20release%20our%20code%20to%20support%20future%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20891v1&entry.124074799=Read"},
{"title": "Dual-Space Augmented Intrinsic-LoRA for Wind Turbine Segmentation", "author": "Shubh Singhal and Ra\u00fcl P\u00e9rez-Gonzalo and Andreas Espersen and Antonio Agudo", "abstract": "  Accurate segmentation of wind turbine blade (WTB) images is critical for\neffective assessments, as it directly influences the performance of automated\ndamage detection systems. Despite advancements in large universal vision\nmodels, these models often underperform in domain-specific tasks like WTB\nsegmentation. To address this, we extend Intrinsic LoRA for image segmentation,\nand propose a novel dual-space augmentation strategy that integrates both\nimage-level and latent-space augmentations. The image-space augmentation is\nachieved through linear interpolation between image pairs, while the\nlatent-space augmentation is accomplished by introducing a noise-based latent\nprobabilistic model. Our approach significantly boosts segmentation accuracy,\nsurpassing current state-of-the-art methods in WTB image segmentation.\n", "link": "http://arxiv.org/abs/2412.20838v1", "date": "2024-12-30", "relevancy": 2.0767, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5237}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5185}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual-Space%20Augmented%20Intrinsic-LoRA%20for%20Wind%20Turbine%20Segmentation&body=Title%3A%20Dual-Space%20Augmented%20Intrinsic-LoRA%20for%20Wind%20Turbine%20Segmentation%0AAuthor%3A%20Shubh%20Singhal%20and%20Ra%C3%BCl%20P%C3%A9rez-Gonzalo%20and%20Andreas%20Espersen%20and%20Antonio%20Agudo%0AAbstract%3A%20%20%20Accurate%20segmentation%20of%20wind%20turbine%20blade%20%28WTB%29%20images%20is%20critical%20for%0Aeffective%20assessments%2C%20as%20it%20directly%20influences%20the%20performance%20of%20automated%0Adamage%20detection%20systems.%20Despite%20advancements%20in%20large%20universal%20vision%0Amodels%2C%20these%20models%20often%20underperform%20in%20domain-specific%20tasks%20like%20WTB%0Asegmentation.%20To%20address%20this%2C%20we%20extend%20Intrinsic%20LoRA%20for%20image%20segmentation%2C%0Aand%20propose%20a%20novel%20dual-space%20augmentation%20strategy%20that%20integrates%20both%0Aimage-level%20and%20latent-space%20augmentations.%20The%20image-space%20augmentation%20is%0Aachieved%20through%20linear%20interpolation%20between%20image%20pairs%2C%20while%20the%0Alatent-space%20augmentation%20is%20accomplished%20by%20introducing%20a%20noise-based%20latent%0Aprobabilistic%20model.%20Our%20approach%20significantly%20boosts%20segmentation%20accuracy%2C%0Asurpassing%20current%20state-of-the-art%20methods%20in%20WTB%20image%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20838v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual-Space%2520Augmented%2520Intrinsic-LoRA%2520for%2520Wind%2520Turbine%2520Segmentation%26entry.906535625%3DShubh%2520Singhal%2520and%2520Ra%25C3%25BCl%2520P%25C3%25A9rez-Gonzalo%2520and%2520Andreas%2520Espersen%2520and%2520Antonio%2520Agudo%26entry.1292438233%3D%2520%2520Accurate%2520segmentation%2520of%2520wind%2520turbine%2520blade%2520%2528WTB%2529%2520images%2520is%2520critical%2520for%250Aeffective%2520assessments%252C%2520as%2520it%2520directly%2520influences%2520the%2520performance%2520of%2520automated%250Adamage%2520detection%2520systems.%2520Despite%2520advancements%2520in%2520large%2520universal%2520vision%250Amodels%252C%2520these%2520models%2520often%2520underperform%2520in%2520domain-specific%2520tasks%2520like%2520WTB%250Asegmentation.%2520To%2520address%2520this%252C%2520we%2520extend%2520Intrinsic%2520LoRA%2520for%2520image%2520segmentation%252C%250Aand%2520propose%2520a%2520novel%2520dual-space%2520augmentation%2520strategy%2520that%2520integrates%2520both%250Aimage-level%2520and%2520latent-space%2520augmentations.%2520The%2520image-space%2520augmentation%2520is%250Aachieved%2520through%2520linear%2520interpolation%2520between%2520image%2520pairs%252C%2520while%2520the%250Alatent-space%2520augmentation%2520is%2520accomplished%2520by%2520introducing%2520a%2520noise-based%2520latent%250Aprobabilistic%2520model.%2520Our%2520approach%2520significantly%2520boosts%2520segmentation%2520accuracy%252C%250Asurpassing%2520current%2520state-of-the-art%2520methods%2520in%2520WTB%2520image%2520segmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20838v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-Space%20Augmented%20Intrinsic-LoRA%20for%20Wind%20Turbine%20Segmentation&entry.906535625=Shubh%20Singhal%20and%20Ra%C3%BCl%20P%C3%A9rez-Gonzalo%20and%20Andreas%20Espersen%20and%20Antonio%20Agudo&entry.1292438233=%20%20Accurate%20segmentation%20of%20wind%20turbine%20blade%20%28WTB%29%20images%20is%20critical%20for%0Aeffective%20assessments%2C%20as%20it%20directly%20influences%20the%20performance%20of%20automated%0Adamage%20detection%20systems.%20Despite%20advancements%20in%20large%20universal%20vision%0Amodels%2C%20these%20models%20often%20underperform%20in%20domain-specific%20tasks%20like%20WTB%0Asegmentation.%20To%20address%20this%2C%20we%20extend%20Intrinsic%20LoRA%20for%20image%20segmentation%2C%0Aand%20propose%20a%20novel%20dual-space%20augmentation%20strategy%20that%20integrates%20both%0Aimage-level%20and%20latent-space%20augmentations.%20The%20image-space%20augmentation%20is%0Aachieved%20through%20linear%20interpolation%20between%20image%20pairs%2C%20while%20the%0Alatent-space%20augmentation%20is%20accomplished%20by%20introducing%20a%20noise-based%20latent%0Aprobabilistic%20model.%20Our%20approach%20significantly%20boosts%20segmentation%20accuracy%2C%0Asurpassing%20current%20state-of-the-art%20methods%20in%20WTB%20image%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20838v1&entry.124074799=Read"},
{"title": "Attention Is All You Need For Mixture-of-Depths Routing", "author": "Advait Gadhikar and Souptik Kumar Majumdar and Niclas Popp and Piyapat Saranrittichai and Martin Rapp and Lukas Schott", "abstract": "  Advancements in deep learning are driven by training models with increasingly\nlarger numbers of parameters, which in turn heightens the computational\ndemands. To address this issue, Mixture-of-Depths (MoD) models have been\nproposed to dynamically assign computations only to the most relevant parts of\nthe inputs, thereby enabling the deployment of large-parameter models with high\nefficiency during inference and training. These MoD models utilize a routing\nmechanism to determine which tokens should be processed by a layer, or skipped.\nHowever, conventional MoD models employ additional network layers specifically\nfor the routing which are difficult to train, and add complexity and deployment\noverhead to the model. In this paper, we introduce a novel attention-based\nrouting mechanism A-MoD that leverages the existing attention map of the\npreceding layer for routing decisions within the current layer. Compared to\nstandard routing, A-MoD allows for more efficient training as it introduces no\nadditional trainable parameters and can be easily adapted from pretrained\ntransformer models. Furthermore, it can increase the performance of the MoD\nmodel. For instance, we observe up to 2% higher accuracy on ImageNet compared\nto standard routing and isoFLOP ViT baselines. Furthermore, A-MoD improves the\nMoD training convergence, leading to up to 2x faster transfer learning.\n", "link": "http://arxiv.org/abs/2412.20875v1", "date": "2024-12-30", "relevancy": 2.0615, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5357}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5018}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention%20Is%20All%20You%20Need%20For%20Mixture-of-Depths%20Routing&body=Title%3A%20Attention%20Is%20All%20You%20Need%20For%20Mixture-of-Depths%20Routing%0AAuthor%3A%20Advait%20Gadhikar%20and%20Souptik%20Kumar%20Majumdar%20and%20Niclas%20Popp%20and%20Piyapat%20Saranrittichai%20and%20Martin%20Rapp%20and%20Lukas%20Schott%0AAbstract%3A%20%20%20Advancements%20in%20deep%20learning%20are%20driven%20by%20training%20models%20with%20increasingly%0Alarger%20numbers%20of%20parameters%2C%20which%20in%20turn%20heightens%20the%20computational%0Ademands.%20To%20address%20this%20issue%2C%20Mixture-of-Depths%20%28MoD%29%20models%20have%20been%0Aproposed%20to%20dynamically%20assign%20computations%20only%20to%20the%20most%20relevant%20parts%20of%0Athe%20inputs%2C%20thereby%20enabling%20the%20deployment%20of%20large-parameter%20models%20with%20high%0Aefficiency%20during%20inference%20and%20training.%20These%20MoD%20models%20utilize%20a%20routing%0Amechanism%20to%20determine%20which%20tokens%20should%20be%20processed%20by%20a%20layer%2C%20or%20skipped.%0AHowever%2C%20conventional%20MoD%20models%20employ%20additional%20network%20layers%20specifically%0Afor%20the%20routing%20which%20are%20difficult%20to%20train%2C%20and%20add%20complexity%20and%20deployment%0Aoverhead%20to%20the%20model.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20attention-based%0Arouting%20mechanism%20A-MoD%20that%20leverages%20the%20existing%20attention%20map%20of%20the%0Apreceding%20layer%20for%20routing%20decisions%20within%20the%20current%20layer.%20Compared%20to%0Astandard%20routing%2C%20A-MoD%20allows%20for%20more%20efficient%20training%20as%20it%20introduces%20no%0Aadditional%20trainable%20parameters%20and%20can%20be%20easily%20adapted%20from%20pretrained%0Atransformer%20models.%20Furthermore%2C%20it%20can%20increase%20the%20performance%20of%20the%20MoD%0Amodel.%20For%20instance%2C%20we%20observe%20up%20to%202%25%20higher%20accuracy%20on%20ImageNet%20compared%0Ato%20standard%20routing%20and%20isoFLOP%20ViT%20baselines.%20Furthermore%2C%20A-MoD%20improves%20the%0AMoD%20training%20convergence%2C%20leading%20to%20up%20to%202x%20faster%20transfer%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20875v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention%2520Is%2520All%2520You%2520Need%2520For%2520Mixture-of-Depths%2520Routing%26entry.906535625%3DAdvait%2520Gadhikar%2520and%2520Souptik%2520Kumar%2520Majumdar%2520and%2520Niclas%2520Popp%2520and%2520Piyapat%2520Saranrittichai%2520and%2520Martin%2520Rapp%2520and%2520Lukas%2520Schott%26entry.1292438233%3D%2520%2520Advancements%2520in%2520deep%2520learning%2520are%2520driven%2520by%2520training%2520models%2520with%2520increasingly%250Alarger%2520numbers%2520of%2520parameters%252C%2520which%2520in%2520turn%2520heightens%2520the%2520computational%250Ademands.%2520To%2520address%2520this%2520issue%252C%2520Mixture-of-Depths%2520%2528MoD%2529%2520models%2520have%2520been%250Aproposed%2520to%2520dynamically%2520assign%2520computations%2520only%2520to%2520the%2520most%2520relevant%2520parts%2520of%250Athe%2520inputs%252C%2520thereby%2520enabling%2520the%2520deployment%2520of%2520large-parameter%2520models%2520with%2520high%250Aefficiency%2520during%2520inference%2520and%2520training.%2520These%2520MoD%2520models%2520utilize%2520a%2520routing%250Amechanism%2520to%2520determine%2520which%2520tokens%2520should%2520be%2520processed%2520by%2520a%2520layer%252C%2520or%2520skipped.%250AHowever%252C%2520conventional%2520MoD%2520models%2520employ%2520additional%2520network%2520layers%2520specifically%250Afor%2520the%2520routing%2520which%2520are%2520difficult%2520to%2520train%252C%2520and%2520add%2520complexity%2520and%2520deployment%250Aoverhead%2520to%2520the%2520model.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520novel%2520attention-based%250Arouting%2520mechanism%2520A-MoD%2520that%2520leverages%2520the%2520existing%2520attention%2520map%2520of%2520the%250Apreceding%2520layer%2520for%2520routing%2520decisions%2520within%2520the%2520current%2520layer.%2520Compared%2520to%250Astandard%2520routing%252C%2520A-MoD%2520allows%2520for%2520more%2520efficient%2520training%2520as%2520it%2520introduces%2520no%250Aadditional%2520trainable%2520parameters%2520and%2520can%2520be%2520easily%2520adapted%2520from%2520pretrained%250Atransformer%2520models.%2520Furthermore%252C%2520it%2520can%2520increase%2520the%2520performance%2520of%2520the%2520MoD%250Amodel.%2520For%2520instance%252C%2520we%2520observe%2520up%2520to%25202%2525%2520higher%2520accuracy%2520on%2520ImageNet%2520compared%250Ato%2520standard%2520routing%2520and%2520isoFLOP%2520ViT%2520baselines.%2520Furthermore%252C%2520A-MoD%2520improves%2520the%250AMoD%2520training%2520convergence%252C%2520leading%2520to%2520up%2520to%25202x%2520faster%2520transfer%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20875v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20Is%20All%20You%20Need%20For%20Mixture-of-Depths%20Routing&entry.906535625=Advait%20Gadhikar%20and%20Souptik%20Kumar%20Majumdar%20and%20Niclas%20Popp%20and%20Piyapat%20Saranrittichai%20and%20Martin%20Rapp%20and%20Lukas%20Schott&entry.1292438233=%20%20Advancements%20in%20deep%20learning%20are%20driven%20by%20training%20models%20with%20increasingly%0Alarger%20numbers%20of%20parameters%2C%20which%20in%20turn%20heightens%20the%20computational%0Ademands.%20To%20address%20this%20issue%2C%20Mixture-of-Depths%20%28MoD%29%20models%20have%20been%0Aproposed%20to%20dynamically%20assign%20computations%20only%20to%20the%20most%20relevant%20parts%20of%0Athe%20inputs%2C%20thereby%20enabling%20the%20deployment%20of%20large-parameter%20models%20with%20high%0Aefficiency%20during%20inference%20and%20training.%20These%20MoD%20models%20utilize%20a%20routing%0Amechanism%20to%20determine%20which%20tokens%20should%20be%20processed%20by%20a%20layer%2C%20or%20skipped.%0AHowever%2C%20conventional%20MoD%20models%20employ%20additional%20network%20layers%20specifically%0Afor%20the%20routing%20which%20are%20difficult%20to%20train%2C%20and%20add%20complexity%20and%20deployment%0Aoverhead%20to%20the%20model.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%20attention-based%0Arouting%20mechanism%20A-MoD%20that%20leverages%20the%20existing%20attention%20map%20of%20the%0Apreceding%20layer%20for%20routing%20decisions%20within%20the%20current%20layer.%20Compared%20to%0Astandard%20routing%2C%20A-MoD%20allows%20for%20more%20efficient%20training%20as%20it%20introduces%20no%0Aadditional%20trainable%20parameters%20and%20can%20be%20easily%20adapted%20from%20pretrained%0Atransformer%20models.%20Furthermore%2C%20it%20can%20increase%20the%20performance%20of%20the%20MoD%0Amodel.%20For%20instance%2C%20we%20observe%20up%20to%202%25%20higher%20accuracy%20on%20ImageNet%20compared%0Ato%20standard%20routing%20and%20isoFLOP%20ViT%20baselines.%20Furthermore%2C%20A-MoD%20improves%20the%0AMoD%20training%20convergence%2C%20leading%20to%20up%20to%202x%20faster%20transfer%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20875v1&entry.124074799=Read"},
{"title": "Mind the truncation gap: challenges of learning on dynamic graphs with\n  recurrent architectures", "author": "Jo\u00e3o Bravo and Jacopo Bono and Pedro Saleiro and Hugo Ferreira and Pedro Bizarro", "abstract": "  Systems characterized by evolving interactions, prevalent in social,\nfinancial, and biological domains, are effectively modeled as continuous-time\ndynamic graphs (CTDGs). To manage the scale and complexity of these graph\ndatasets, machine learning (ML) approaches have become essential. However,\nCTDGs pose challenges for ML because traditional static graph methods do not\nnaturally account for event timings. Newer approaches, such as graph recurrent\nneural networks (GRNNs), are inherently time-aware and offer advantages over\nstatic methods for CTDGs. However, GRNNs face another issue: the short\ntruncation of backpropagation-through-time (BPTT), whose impact has not been\nproperly examined until now. In this work, we demonstrate that this truncation\ncan limit the learning of dependencies beyond a single hop, resulting in\nreduced performance. Through experiments on a novel synthetic task and\nreal-world datasets, we reveal a performance gap between full\nbackpropagation-through-time (F-BPTT) and the truncated\nbackpropagation-through-time (T-BPTT) commonly used to train GRNN models. We\nterm this gap the \"truncation gap\" and argue that understanding and addressing\nit is essential as the importance of CTDGs grows, discussing potential future\ndirections for research in this area.\n", "link": "http://arxiv.org/abs/2412.21046v1", "date": "2024-12-30", "relevancy": 2.0604, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5266}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.508}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5039}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mind%20the%20truncation%20gap%3A%20challenges%20of%20learning%20on%20dynamic%20graphs%20with%0A%20%20recurrent%20architectures&body=Title%3A%20Mind%20the%20truncation%20gap%3A%20challenges%20of%20learning%20on%20dynamic%20graphs%20with%0A%20%20recurrent%20architectures%0AAuthor%3A%20Jo%C3%A3o%20Bravo%20and%20Jacopo%20Bono%20and%20Pedro%20Saleiro%20and%20Hugo%20Ferreira%20and%20Pedro%20Bizarro%0AAbstract%3A%20%20%20Systems%20characterized%20by%20evolving%20interactions%2C%20prevalent%20in%20social%2C%0Afinancial%2C%20and%20biological%20domains%2C%20are%20effectively%20modeled%20as%20continuous-time%0Adynamic%20graphs%20%28CTDGs%29.%20To%20manage%20the%20scale%20and%20complexity%20of%20these%20graph%0Adatasets%2C%20machine%20learning%20%28ML%29%20approaches%20have%20become%20essential.%20However%2C%0ACTDGs%20pose%20challenges%20for%20ML%20because%20traditional%20static%20graph%20methods%20do%20not%0Anaturally%20account%20for%20event%20timings.%20Newer%20approaches%2C%20such%20as%20graph%20recurrent%0Aneural%20networks%20%28GRNNs%29%2C%20are%20inherently%20time-aware%20and%20offer%20advantages%20over%0Astatic%20methods%20for%20CTDGs.%20However%2C%20GRNNs%20face%20another%20issue%3A%20the%20short%0Atruncation%20of%20backpropagation-through-time%20%28BPTT%29%2C%20whose%20impact%20has%20not%20been%0Aproperly%20examined%20until%20now.%20In%20this%20work%2C%20we%20demonstrate%20that%20this%20truncation%0Acan%20limit%20the%20learning%20of%20dependencies%20beyond%20a%20single%20hop%2C%20resulting%20in%0Areduced%20performance.%20Through%20experiments%20on%20a%20novel%20synthetic%20task%20and%0Areal-world%20datasets%2C%20we%20reveal%20a%20performance%20gap%20between%20full%0Abackpropagation-through-time%20%28F-BPTT%29%20and%20the%20truncated%0Abackpropagation-through-time%20%28T-BPTT%29%20commonly%20used%20to%20train%20GRNN%20models.%20We%0Aterm%20this%20gap%20the%20%22truncation%20gap%22%20and%20argue%20that%20understanding%20and%20addressing%0Ait%20is%20essential%20as%20the%20importance%20of%20CTDGs%20grows%2C%20discussing%20potential%20future%0Adirections%20for%20research%20in%20this%20area.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21046v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMind%2520the%2520truncation%2520gap%253A%2520challenges%2520of%2520learning%2520on%2520dynamic%2520graphs%2520with%250A%2520%2520recurrent%2520architectures%26entry.906535625%3DJo%25C3%25A3o%2520Bravo%2520and%2520Jacopo%2520Bono%2520and%2520Pedro%2520Saleiro%2520and%2520Hugo%2520Ferreira%2520and%2520Pedro%2520Bizarro%26entry.1292438233%3D%2520%2520Systems%2520characterized%2520by%2520evolving%2520interactions%252C%2520prevalent%2520in%2520social%252C%250Afinancial%252C%2520and%2520biological%2520domains%252C%2520are%2520effectively%2520modeled%2520as%2520continuous-time%250Adynamic%2520graphs%2520%2528CTDGs%2529.%2520To%2520manage%2520the%2520scale%2520and%2520complexity%2520of%2520these%2520graph%250Adatasets%252C%2520machine%2520learning%2520%2528ML%2529%2520approaches%2520have%2520become%2520essential.%2520However%252C%250ACTDGs%2520pose%2520challenges%2520for%2520ML%2520because%2520traditional%2520static%2520graph%2520methods%2520do%2520not%250Anaturally%2520account%2520for%2520event%2520timings.%2520Newer%2520approaches%252C%2520such%2520as%2520graph%2520recurrent%250Aneural%2520networks%2520%2528GRNNs%2529%252C%2520are%2520inherently%2520time-aware%2520and%2520offer%2520advantages%2520over%250Astatic%2520methods%2520for%2520CTDGs.%2520However%252C%2520GRNNs%2520face%2520another%2520issue%253A%2520the%2520short%250Atruncation%2520of%2520backpropagation-through-time%2520%2528BPTT%2529%252C%2520whose%2520impact%2520has%2520not%2520been%250Aproperly%2520examined%2520until%2520now.%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520that%2520this%2520truncation%250Acan%2520limit%2520the%2520learning%2520of%2520dependencies%2520beyond%2520a%2520single%2520hop%252C%2520resulting%2520in%250Areduced%2520performance.%2520Through%2520experiments%2520on%2520a%2520novel%2520synthetic%2520task%2520and%250Areal-world%2520datasets%252C%2520we%2520reveal%2520a%2520performance%2520gap%2520between%2520full%250Abackpropagation-through-time%2520%2528F-BPTT%2529%2520and%2520the%2520truncated%250Abackpropagation-through-time%2520%2528T-BPTT%2529%2520commonly%2520used%2520to%2520train%2520GRNN%2520models.%2520We%250Aterm%2520this%2520gap%2520the%2520%2522truncation%2520gap%2522%2520and%2520argue%2520that%2520understanding%2520and%2520addressing%250Ait%2520is%2520essential%2520as%2520the%2520importance%2520of%2520CTDGs%2520grows%252C%2520discussing%2520potential%2520future%250Adirections%2520for%2520research%2520in%2520this%2520area.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21046v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mind%20the%20truncation%20gap%3A%20challenges%20of%20learning%20on%20dynamic%20graphs%20with%0A%20%20recurrent%20architectures&entry.906535625=Jo%C3%A3o%20Bravo%20and%20Jacopo%20Bono%20and%20Pedro%20Saleiro%20and%20Hugo%20Ferreira%20and%20Pedro%20Bizarro&entry.1292438233=%20%20Systems%20characterized%20by%20evolving%20interactions%2C%20prevalent%20in%20social%2C%0Afinancial%2C%20and%20biological%20domains%2C%20are%20effectively%20modeled%20as%20continuous-time%0Adynamic%20graphs%20%28CTDGs%29.%20To%20manage%20the%20scale%20and%20complexity%20of%20these%20graph%0Adatasets%2C%20machine%20learning%20%28ML%29%20approaches%20have%20become%20essential.%20However%2C%0ACTDGs%20pose%20challenges%20for%20ML%20because%20traditional%20static%20graph%20methods%20do%20not%0Anaturally%20account%20for%20event%20timings.%20Newer%20approaches%2C%20such%20as%20graph%20recurrent%0Aneural%20networks%20%28GRNNs%29%2C%20are%20inherently%20time-aware%20and%20offer%20advantages%20over%0Astatic%20methods%20for%20CTDGs.%20However%2C%20GRNNs%20face%20another%20issue%3A%20the%20short%0Atruncation%20of%20backpropagation-through-time%20%28BPTT%29%2C%20whose%20impact%20has%20not%20been%0Aproperly%20examined%20until%20now.%20In%20this%20work%2C%20we%20demonstrate%20that%20this%20truncation%0Acan%20limit%20the%20learning%20of%20dependencies%20beyond%20a%20single%20hop%2C%20resulting%20in%0Areduced%20performance.%20Through%20experiments%20on%20a%20novel%20synthetic%20task%20and%0Areal-world%20datasets%2C%20we%20reveal%20a%20performance%20gap%20between%20full%0Abackpropagation-through-time%20%28F-BPTT%29%20and%20the%20truncated%0Abackpropagation-through-time%20%28T-BPTT%29%20commonly%20used%20to%20train%20GRNN%20models.%20We%0Aterm%20this%20gap%20the%20%22truncation%20gap%22%20and%20argue%20that%20understanding%20and%20addressing%0Ait%20is%20essential%20as%20the%20importance%20of%20CTDGs%20grows%2C%20discussing%20potential%20future%0Adirections%20for%20research%20in%20this%20area.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21046v1&entry.124074799=Read"},
{"title": "SoftPatch+: Fully Unsupervised Anomaly Classification and Segmentation", "author": "Chengjie Wang and Xi Jiang and Bin-Bin Gao and Zhenye Gan and Yong Liu and Feng Zheng and Lizhuang Ma", "abstract": "  Although mainstream unsupervised anomaly detection (AD) (including\nimage-level classification and pixel-level segmentation)algorithms perform well\nin academic datasets, their performance is limited in practical application due\nto the ideal experimental setting of clean training data. Training with noisy\ndata is an inevitable problem in real-world anomaly detection but is seldom\ndiscussed. This paper is the first to consider fully unsupervised industrial\nanomaly detection (i.e., unsupervised AD with noisy data). To solve this\nproblem, we proposed memory-based unsupervised AD methods, SoftPatch and\nSoftPatch+, which efficiently denoise the data at the patch level. Noise\ndiscriminators are utilized to generate outlier scores for patch-level noise\nelimination before coreset construction. The scores are then stored in the\nmemory bank to soften the anomaly detection boundary. Compared with existing\nmethods, SoftPatch maintains a strong modeling ability of normal data and\nalleviates the overconfidence problem in coreset, and SoftPatch+ has more\nrobust performance which is articularly useful in real-world industrial\ninspection scenarios with high levels of noise (from 10% to 40%). Comprehensive\nexperiments conducted in diverse noise scenarios demonstrate that both\nSoftPatch and SoftPatch+ outperform the state-of-the-art AD methods on the\nMVTecAD, ViSA, and BTAD benchmarks. Furthermore, the performance of SoftPatch\nand SoftPatch+ is comparable to that of the noise-free methods in conventional\nunsupervised AD setting. The code of the proposed methods can be found at\nhttps://github.com/TencentYoutuResearch/AnomalyDetection-SoftPatch.\n", "link": "http://arxiv.org/abs/2412.20870v1", "date": "2024-12-30", "relevancy": 2.0602, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5292}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5101}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SoftPatch%2B%3A%20Fully%20Unsupervised%20Anomaly%20Classification%20and%20Segmentation&body=Title%3A%20SoftPatch%2B%3A%20Fully%20Unsupervised%20Anomaly%20Classification%20and%20Segmentation%0AAuthor%3A%20Chengjie%20Wang%20and%20Xi%20Jiang%20and%20Bin-Bin%20Gao%20and%20Zhenye%20Gan%20and%20Yong%20Liu%20and%20Feng%20Zheng%20and%20Lizhuang%20Ma%0AAbstract%3A%20%20%20Although%20mainstream%20unsupervised%20anomaly%20detection%20%28AD%29%20%28including%0Aimage-level%20classification%20and%20pixel-level%20segmentation%29algorithms%20perform%20well%0Ain%20academic%20datasets%2C%20their%20performance%20is%20limited%20in%20practical%20application%20due%0Ato%20the%20ideal%20experimental%20setting%20of%20clean%20training%20data.%20Training%20with%20noisy%0Adata%20is%20an%20inevitable%20problem%20in%20real-world%20anomaly%20detection%20but%20is%20seldom%0Adiscussed.%20This%20paper%20is%20the%20first%20to%20consider%20fully%20unsupervised%20industrial%0Aanomaly%20detection%20%28i.e.%2C%20unsupervised%20AD%20with%20noisy%20data%29.%20To%20solve%20this%0Aproblem%2C%20we%20proposed%20memory-based%20unsupervised%20AD%20methods%2C%20SoftPatch%20and%0ASoftPatch%2B%2C%20which%20efficiently%20denoise%20the%20data%20at%20the%20patch%20level.%20Noise%0Adiscriminators%20are%20utilized%20to%20generate%20outlier%20scores%20for%20patch-level%20noise%0Aelimination%20before%20coreset%20construction.%20The%20scores%20are%20then%20stored%20in%20the%0Amemory%20bank%20to%20soften%20the%20anomaly%20detection%20boundary.%20Compared%20with%20existing%0Amethods%2C%20SoftPatch%20maintains%20a%20strong%20modeling%20ability%20of%20normal%20data%20and%0Aalleviates%20the%20overconfidence%20problem%20in%20coreset%2C%20and%20SoftPatch%2B%20has%20more%0Arobust%20performance%20which%20is%20articularly%20useful%20in%20real-world%20industrial%0Ainspection%20scenarios%20with%20high%20levels%20of%20noise%20%28from%2010%25%20to%2040%25%29.%20Comprehensive%0Aexperiments%20conducted%20in%20diverse%20noise%20scenarios%20demonstrate%20that%20both%0ASoftPatch%20and%20SoftPatch%2B%20outperform%20the%20state-of-the-art%20AD%20methods%20on%20the%0AMVTecAD%2C%20ViSA%2C%20and%20BTAD%20benchmarks.%20Furthermore%2C%20the%20performance%20of%20SoftPatch%0Aand%20SoftPatch%2B%20is%20comparable%20to%20that%20of%20the%20noise-free%20methods%20in%20conventional%0Aunsupervised%20AD%20setting.%20The%20code%20of%20the%20proposed%20methods%20can%20be%20found%20at%0Ahttps%3A//github.com/TencentYoutuResearch/AnomalyDetection-SoftPatch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20870v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoftPatch%252B%253A%2520Fully%2520Unsupervised%2520Anomaly%2520Classification%2520and%2520Segmentation%26entry.906535625%3DChengjie%2520Wang%2520and%2520Xi%2520Jiang%2520and%2520Bin-Bin%2520Gao%2520and%2520Zhenye%2520Gan%2520and%2520Yong%2520Liu%2520and%2520Feng%2520Zheng%2520and%2520Lizhuang%2520Ma%26entry.1292438233%3D%2520%2520Although%2520mainstream%2520unsupervised%2520anomaly%2520detection%2520%2528AD%2529%2520%2528including%250Aimage-level%2520classification%2520and%2520pixel-level%2520segmentation%2529algorithms%2520perform%2520well%250Ain%2520academic%2520datasets%252C%2520their%2520performance%2520is%2520limited%2520in%2520practical%2520application%2520due%250Ato%2520the%2520ideal%2520experimental%2520setting%2520of%2520clean%2520training%2520data.%2520Training%2520with%2520noisy%250Adata%2520is%2520an%2520inevitable%2520problem%2520in%2520real-world%2520anomaly%2520detection%2520but%2520is%2520seldom%250Adiscussed.%2520This%2520paper%2520is%2520the%2520first%2520to%2520consider%2520fully%2520unsupervised%2520industrial%250Aanomaly%2520detection%2520%2528i.e.%252C%2520unsupervised%2520AD%2520with%2520noisy%2520data%2529.%2520To%2520solve%2520this%250Aproblem%252C%2520we%2520proposed%2520memory-based%2520unsupervised%2520AD%2520methods%252C%2520SoftPatch%2520and%250ASoftPatch%252B%252C%2520which%2520efficiently%2520denoise%2520the%2520data%2520at%2520the%2520patch%2520level.%2520Noise%250Adiscriminators%2520are%2520utilized%2520to%2520generate%2520outlier%2520scores%2520for%2520patch-level%2520noise%250Aelimination%2520before%2520coreset%2520construction.%2520The%2520scores%2520are%2520then%2520stored%2520in%2520the%250Amemory%2520bank%2520to%2520soften%2520the%2520anomaly%2520detection%2520boundary.%2520Compared%2520with%2520existing%250Amethods%252C%2520SoftPatch%2520maintains%2520a%2520strong%2520modeling%2520ability%2520of%2520normal%2520data%2520and%250Aalleviates%2520the%2520overconfidence%2520problem%2520in%2520coreset%252C%2520and%2520SoftPatch%252B%2520has%2520more%250Arobust%2520performance%2520which%2520is%2520articularly%2520useful%2520in%2520real-world%2520industrial%250Ainspection%2520scenarios%2520with%2520high%2520levels%2520of%2520noise%2520%2528from%252010%2525%2520to%252040%2525%2529.%2520Comprehensive%250Aexperiments%2520conducted%2520in%2520diverse%2520noise%2520scenarios%2520demonstrate%2520that%2520both%250ASoftPatch%2520and%2520SoftPatch%252B%2520outperform%2520the%2520state-of-the-art%2520AD%2520methods%2520on%2520the%250AMVTecAD%252C%2520ViSA%252C%2520and%2520BTAD%2520benchmarks.%2520Furthermore%252C%2520the%2520performance%2520of%2520SoftPatch%250Aand%2520SoftPatch%252B%2520is%2520comparable%2520to%2520that%2520of%2520the%2520noise-free%2520methods%2520in%2520conventional%250Aunsupervised%2520AD%2520setting.%2520The%2520code%2520of%2520the%2520proposed%2520methods%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/TencentYoutuResearch/AnomalyDetection-SoftPatch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20870v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SoftPatch%2B%3A%20Fully%20Unsupervised%20Anomaly%20Classification%20and%20Segmentation&entry.906535625=Chengjie%20Wang%20and%20Xi%20Jiang%20and%20Bin-Bin%20Gao%20and%20Zhenye%20Gan%20and%20Yong%20Liu%20and%20Feng%20Zheng%20and%20Lizhuang%20Ma&entry.1292438233=%20%20Although%20mainstream%20unsupervised%20anomaly%20detection%20%28AD%29%20%28including%0Aimage-level%20classification%20and%20pixel-level%20segmentation%29algorithms%20perform%20well%0Ain%20academic%20datasets%2C%20their%20performance%20is%20limited%20in%20practical%20application%20due%0Ato%20the%20ideal%20experimental%20setting%20of%20clean%20training%20data.%20Training%20with%20noisy%0Adata%20is%20an%20inevitable%20problem%20in%20real-world%20anomaly%20detection%20but%20is%20seldom%0Adiscussed.%20This%20paper%20is%20the%20first%20to%20consider%20fully%20unsupervised%20industrial%0Aanomaly%20detection%20%28i.e.%2C%20unsupervised%20AD%20with%20noisy%20data%29.%20To%20solve%20this%0Aproblem%2C%20we%20proposed%20memory-based%20unsupervised%20AD%20methods%2C%20SoftPatch%20and%0ASoftPatch%2B%2C%20which%20efficiently%20denoise%20the%20data%20at%20the%20patch%20level.%20Noise%0Adiscriminators%20are%20utilized%20to%20generate%20outlier%20scores%20for%20patch-level%20noise%0Aelimination%20before%20coreset%20construction.%20The%20scores%20are%20then%20stored%20in%20the%0Amemory%20bank%20to%20soften%20the%20anomaly%20detection%20boundary.%20Compared%20with%20existing%0Amethods%2C%20SoftPatch%20maintains%20a%20strong%20modeling%20ability%20of%20normal%20data%20and%0Aalleviates%20the%20overconfidence%20problem%20in%20coreset%2C%20and%20SoftPatch%2B%20has%20more%0Arobust%20performance%20which%20is%20articularly%20useful%20in%20real-world%20industrial%0Ainspection%20scenarios%20with%20high%20levels%20of%20noise%20%28from%2010%25%20to%2040%25%29.%20Comprehensive%0Aexperiments%20conducted%20in%20diverse%20noise%20scenarios%20demonstrate%20that%20both%0ASoftPatch%20and%20SoftPatch%2B%20outperform%20the%20state-of-the-art%20AD%20methods%20on%20the%0AMVTecAD%2C%20ViSA%2C%20and%20BTAD%20benchmarks.%20Furthermore%2C%20the%20performance%20of%20SoftPatch%0Aand%20SoftPatch%2B%20is%20comparable%20to%20that%20of%20the%20noise-free%20methods%20in%20conventional%0Aunsupervised%20AD%20setting.%20The%20code%20of%20the%20proposed%20methods%20can%20be%20found%20at%0Ahttps%3A//github.com/TencentYoutuResearch/AnomalyDetection-SoftPatch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20870v1&entry.124074799=Read"},
{"title": "Rethinking Aleatoric and Epistemic Uncertainty", "author": "Freddie Bickford Smith and Jannik Kossen and Eleanor Trollope and Mark van der Wilk and Adam Foster and Tom Rainforth", "abstract": "  The ideas of aleatoric and epistemic uncertainty are widely used to reason\nabout the probabilistic predictions of machine-learning models. We identify\nincoherence in existing discussions of these ideas and suggest this stems from\nthe aleatoric-epistemic view being insufficiently expressive to capture all of\nthe distinct quantities that researchers are interested in. To explain and\naddress this we derive a simple delineation of different model-based\nuncertainties and the data-generating processes associated with training and\nevaluation. Using this in place of the aleatoric-epistemic view could produce\nclearer discourse as the field moves forward.\n", "link": "http://arxiv.org/abs/2412.20892v1", "date": "2024-12-30", "relevancy": 2.0479, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5298}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5272}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Aleatoric%20and%20Epistemic%20Uncertainty&body=Title%3A%20Rethinking%20Aleatoric%20and%20Epistemic%20Uncertainty%0AAuthor%3A%20Freddie%20Bickford%20Smith%20and%20Jannik%20Kossen%20and%20Eleanor%20Trollope%20and%20Mark%20van%20der%20Wilk%20and%20Adam%20Foster%20and%20Tom%20Rainforth%0AAbstract%3A%20%20%20The%20ideas%20of%20aleatoric%20and%20epistemic%20uncertainty%20are%20widely%20used%20to%20reason%0Aabout%20the%20probabilistic%20predictions%20of%20machine-learning%20models.%20We%20identify%0Aincoherence%20in%20existing%20discussions%20of%20these%20ideas%20and%20suggest%20this%20stems%20from%0Athe%20aleatoric-epistemic%20view%20being%20insufficiently%20expressive%20to%20capture%20all%20of%0Athe%20distinct%20quantities%20that%20researchers%20are%20interested%20in.%20To%20explain%20and%0Aaddress%20this%20we%20derive%20a%20simple%20delineation%20of%20different%20model-based%0Auncertainties%20and%20the%20data-generating%20processes%20associated%20with%20training%20and%0Aevaluation.%20Using%20this%20in%20place%20of%20the%20aleatoric-epistemic%20view%20could%20produce%0Aclearer%20discourse%20as%20the%20field%20moves%20forward.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20892v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Aleatoric%2520and%2520Epistemic%2520Uncertainty%26entry.906535625%3DFreddie%2520Bickford%2520Smith%2520and%2520Jannik%2520Kossen%2520and%2520Eleanor%2520Trollope%2520and%2520Mark%2520van%2520der%2520Wilk%2520and%2520Adam%2520Foster%2520and%2520Tom%2520Rainforth%26entry.1292438233%3D%2520%2520The%2520ideas%2520of%2520aleatoric%2520and%2520epistemic%2520uncertainty%2520are%2520widely%2520used%2520to%2520reason%250Aabout%2520the%2520probabilistic%2520predictions%2520of%2520machine-learning%2520models.%2520We%2520identify%250Aincoherence%2520in%2520existing%2520discussions%2520of%2520these%2520ideas%2520and%2520suggest%2520this%2520stems%2520from%250Athe%2520aleatoric-epistemic%2520view%2520being%2520insufficiently%2520expressive%2520to%2520capture%2520all%2520of%250Athe%2520distinct%2520quantities%2520that%2520researchers%2520are%2520interested%2520in.%2520To%2520explain%2520and%250Aaddress%2520this%2520we%2520derive%2520a%2520simple%2520delineation%2520of%2520different%2520model-based%250Auncertainties%2520and%2520the%2520data-generating%2520processes%2520associated%2520with%2520training%2520and%250Aevaluation.%2520Using%2520this%2520in%2520place%2520of%2520the%2520aleatoric-epistemic%2520view%2520could%2520produce%250Aclearer%2520discourse%2520as%2520the%2520field%2520moves%2520forward.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20892v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Aleatoric%20and%20Epistemic%20Uncertainty&entry.906535625=Freddie%20Bickford%20Smith%20and%20Jannik%20Kossen%20and%20Eleanor%20Trollope%20and%20Mark%20van%20der%20Wilk%20and%20Adam%20Foster%20and%20Tom%20Rainforth&entry.1292438233=%20%20The%20ideas%20of%20aleatoric%20and%20epistemic%20uncertainty%20are%20widely%20used%20to%20reason%0Aabout%20the%20probabilistic%20predictions%20of%20machine-learning%20models.%20We%20identify%0Aincoherence%20in%20existing%20discussions%20of%20these%20ideas%20and%20suggest%20this%20stems%20from%0Athe%20aleatoric-epistemic%20view%20being%20insufficiently%20expressive%20to%20capture%20all%20of%0Athe%20distinct%20quantities%20that%20researchers%20are%20interested%20in.%20To%20explain%20and%0Aaddress%20this%20we%20derive%20a%20simple%20delineation%20of%20different%20model-based%0Auncertainties%20and%20the%20data-generating%20processes%20associated%20with%20training%20and%0Aevaluation.%20Using%20this%20in%20place%20of%20the%20aleatoric-epistemic%20view%20could%20produce%0Aclearer%20discourse%20as%20the%20field%20moves%20forward.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20892v1&entry.124074799=Read"},
{"title": "EigenSR: Eigenimage-Bridged Pre-Trained RGB Learners for Single\n  Hyperspectral Image Super-Resolution", "author": "Xi Su and Xiangfei Shen and Mingyang Wan and Jing Nie and Lihui Chen and Haijun Liu and Xichuan Zhou", "abstract": "  Single hyperspectral image super-resolution (single-HSI-SR) aims to improve\nthe resolution of a single input low-resolution HSI. Due to the bottleneck of\ndata scarcity, the development of single-HSI-SR lags far behind that of RGB\nnatural images. In recent years, research on RGB SR has shown that models\npre-trained on large-scale benchmark datasets can greatly improve performance\non unseen data, which may stand as a remedy for HSI. But how can we transfer\nthe pre-trained RGB model to HSI, to overcome the data-scarcity bottleneck?\nBecause of the significant difference in the channels between the pre-trained\nRGB model and the HSI, the model cannot focus on the correlation along the\nspectral dimension, thus limiting its ability to utilize on HSI. Inspired by\nthe HSI spatial-spectral decoupling, we propose a new framework that first\nfine-tunes the pre-trained model with the spatial components (known as\neigenimages), and then infers on unseen HSI using an iterative spectral\nregularization (ISR) to maintain the spectral correlation. The advantages of\nour method lie in: 1) we effectively inject the spatial texture processing\ncapabilities of the pre-trained RGB model into HSI while keeping spectral\nfidelity, 2) learning in the spectral-decorrelated domain can improve the\ngeneralizability to spectral-agnostic data, and 3) our inference in the\neigenimage domain naturally exploits the spectral low-rank property of HSI,\nthereby reducing the complexity. This work bridges the gap between pre-trained\nRGB models and HSI via eigenimages, addressing the issue of limited HSI\ntraining data, hence the name EigenSR. Extensive experiments show that EigenSR\noutperforms the state-of-the-art (SOTA) methods in both spatial and spectral\nmetrics.\n", "link": "http://arxiv.org/abs/2409.04050v2", "date": "2024-12-30", "relevancy": 2.0427, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5264}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5198}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EigenSR%3A%20Eigenimage-Bridged%20Pre-Trained%20RGB%20Learners%20for%20Single%0A%20%20Hyperspectral%20Image%20Super-Resolution&body=Title%3A%20EigenSR%3A%20Eigenimage-Bridged%20Pre-Trained%20RGB%20Learners%20for%20Single%0A%20%20Hyperspectral%20Image%20Super-Resolution%0AAuthor%3A%20Xi%20Su%20and%20Xiangfei%20Shen%20and%20Mingyang%20Wan%20and%20Jing%20Nie%20and%20Lihui%20Chen%20and%20Haijun%20Liu%20and%20Xichuan%20Zhou%0AAbstract%3A%20%20%20Single%20hyperspectral%20image%20super-resolution%20%28single-HSI-SR%29%20aims%20to%20improve%0Athe%20resolution%20of%20a%20single%20input%20low-resolution%20HSI.%20Due%20to%20the%20bottleneck%20of%0Adata%20scarcity%2C%20the%20development%20of%20single-HSI-SR%20lags%20far%20behind%20that%20of%20RGB%0Anatural%20images.%20In%20recent%20years%2C%20research%20on%20RGB%20SR%20has%20shown%20that%20models%0Apre-trained%20on%20large-scale%20benchmark%20datasets%20can%20greatly%20improve%20performance%0Aon%20unseen%20data%2C%20which%20may%20stand%20as%20a%20remedy%20for%20HSI.%20But%20how%20can%20we%20transfer%0Athe%20pre-trained%20RGB%20model%20to%20HSI%2C%20to%20overcome%20the%20data-scarcity%20bottleneck%3F%0ABecause%20of%20the%20significant%20difference%20in%20the%20channels%20between%20the%20pre-trained%0ARGB%20model%20and%20the%20HSI%2C%20the%20model%20cannot%20focus%20on%20the%20correlation%20along%20the%0Aspectral%20dimension%2C%20thus%20limiting%20its%20ability%20to%20utilize%20on%20HSI.%20Inspired%20by%0Athe%20HSI%20spatial-spectral%20decoupling%2C%20we%20propose%20a%20new%20framework%20that%20first%0Afine-tunes%20the%20pre-trained%20model%20with%20the%20spatial%20components%20%28known%20as%0Aeigenimages%29%2C%20and%20then%20infers%20on%20unseen%20HSI%20using%20an%20iterative%20spectral%0Aregularization%20%28ISR%29%20to%20maintain%20the%20spectral%20correlation.%20The%20advantages%20of%0Aour%20method%20lie%20in%3A%201%29%20we%20effectively%20inject%20the%20spatial%20texture%20processing%0Acapabilities%20of%20the%20pre-trained%20RGB%20model%20into%20HSI%20while%20keeping%20spectral%0Afidelity%2C%202%29%20learning%20in%20the%20spectral-decorrelated%20domain%20can%20improve%20the%0Ageneralizability%20to%20spectral-agnostic%20data%2C%20and%203%29%20our%20inference%20in%20the%0Aeigenimage%20domain%20naturally%20exploits%20the%20spectral%20low-rank%20property%20of%20HSI%2C%0Athereby%20reducing%20the%20complexity.%20This%20work%20bridges%20the%20gap%20between%20pre-trained%0ARGB%20models%20and%20HSI%20via%20eigenimages%2C%20addressing%20the%20issue%20of%20limited%20HSI%0Atraining%20data%2C%20hence%20the%20name%20EigenSR.%20Extensive%20experiments%20show%20that%20EigenSR%0Aoutperforms%20the%20state-of-the-art%20%28SOTA%29%20methods%20in%20both%20spatial%20and%20spectral%0Ametrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04050v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEigenSR%253A%2520Eigenimage-Bridged%2520Pre-Trained%2520RGB%2520Learners%2520for%2520Single%250A%2520%2520Hyperspectral%2520Image%2520Super-Resolution%26entry.906535625%3DXi%2520Su%2520and%2520Xiangfei%2520Shen%2520and%2520Mingyang%2520Wan%2520and%2520Jing%2520Nie%2520and%2520Lihui%2520Chen%2520and%2520Haijun%2520Liu%2520and%2520Xichuan%2520Zhou%26entry.1292438233%3D%2520%2520Single%2520hyperspectral%2520image%2520super-resolution%2520%2528single-HSI-SR%2529%2520aims%2520to%2520improve%250Athe%2520resolution%2520of%2520a%2520single%2520input%2520low-resolution%2520HSI.%2520Due%2520to%2520the%2520bottleneck%2520of%250Adata%2520scarcity%252C%2520the%2520development%2520of%2520single-HSI-SR%2520lags%2520far%2520behind%2520that%2520of%2520RGB%250Anatural%2520images.%2520In%2520recent%2520years%252C%2520research%2520on%2520RGB%2520SR%2520has%2520shown%2520that%2520models%250Apre-trained%2520on%2520large-scale%2520benchmark%2520datasets%2520can%2520greatly%2520improve%2520performance%250Aon%2520unseen%2520data%252C%2520which%2520may%2520stand%2520as%2520a%2520remedy%2520for%2520HSI.%2520But%2520how%2520can%2520we%2520transfer%250Athe%2520pre-trained%2520RGB%2520model%2520to%2520HSI%252C%2520to%2520overcome%2520the%2520data-scarcity%2520bottleneck%253F%250ABecause%2520of%2520the%2520significant%2520difference%2520in%2520the%2520channels%2520between%2520the%2520pre-trained%250ARGB%2520model%2520and%2520the%2520HSI%252C%2520the%2520model%2520cannot%2520focus%2520on%2520the%2520correlation%2520along%2520the%250Aspectral%2520dimension%252C%2520thus%2520limiting%2520its%2520ability%2520to%2520utilize%2520on%2520HSI.%2520Inspired%2520by%250Athe%2520HSI%2520spatial-spectral%2520decoupling%252C%2520we%2520propose%2520a%2520new%2520framework%2520that%2520first%250Afine-tunes%2520the%2520pre-trained%2520model%2520with%2520the%2520spatial%2520components%2520%2528known%2520as%250Aeigenimages%2529%252C%2520and%2520then%2520infers%2520on%2520unseen%2520HSI%2520using%2520an%2520iterative%2520spectral%250Aregularization%2520%2528ISR%2529%2520to%2520maintain%2520the%2520spectral%2520correlation.%2520The%2520advantages%2520of%250Aour%2520method%2520lie%2520in%253A%25201%2529%2520we%2520effectively%2520inject%2520the%2520spatial%2520texture%2520processing%250Acapabilities%2520of%2520the%2520pre-trained%2520RGB%2520model%2520into%2520HSI%2520while%2520keeping%2520spectral%250Afidelity%252C%25202%2529%2520learning%2520in%2520the%2520spectral-decorrelated%2520domain%2520can%2520improve%2520the%250Ageneralizability%2520to%2520spectral-agnostic%2520data%252C%2520and%25203%2529%2520our%2520inference%2520in%2520the%250Aeigenimage%2520domain%2520naturally%2520exploits%2520the%2520spectral%2520low-rank%2520property%2520of%2520HSI%252C%250Athereby%2520reducing%2520the%2520complexity.%2520This%2520work%2520bridges%2520the%2520gap%2520between%2520pre-trained%250ARGB%2520models%2520and%2520HSI%2520via%2520eigenimages%252C%2520addressing%2520the%2520issue%2520of%2520limited%2520HSI%250Atraining%2520data%252C%2520hence%2520the%2520name%2520EigenSR.%2520Extensive%2520experiments%2520show%2520that%2520EigenSR%250Aoutperforms%2520the%2520state-of-the-art%2520%2528SOTA%2529%2520methods%2520in%2520both%2520spatial%2520and%2520spectral%250Ametrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04050v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EigenSR%3A%20Eigenimage-Bridged%20Pre-Trained%20RGB%20Learners%20for%20Single%0A%20%20Hyperspectral%20Image%20Super-Resolution&entry.906535625=Xi%20Su%20and%20Xiangfei%20Shen%20and%20Mingyang%20Wan%20and%20Jing%20Nie%20and%20Lihui%20Chen%20and%20Haijun%20Liu%20and%20Xichuan%20Zhou&entry.1292438233=%20%20Single%20hyperspectral%20image%20super-resolution%20%28single-HSI-SR%29%20aims%20to%20improve%0Athe%20resolution%20of%20a%20single%20input%20low-resolution%20HSI.%20Due%20to%20the%20bottleneck%20of%0Adata%20scarcity%2C%20the%20development%20of%20single-HSI-SR%20lags%20far%20behind%20that%20of%20RGB%0Anatural%20images.%20In%20recent%20years%2C%20research%20on%20RGB%20SR%20has%20shown%20that%20models%0Apre-trained%20on%20large-scale%20benchmark%20datasets%20can%20greatly%20improve%20performance%0Aon%20unseen%20data%2C%20which%20may%20stand%20as%20a%20remedy%20for%20HSI.%20But%20how%20can%20we%20transfer%0Athe%20pre-trained%20RGB%20model%20to%20HSI%2C%20to%20overcome%20the%20data-scarcity%20bottleneck%3F%0ABecause%20of%20the%20significant%20difference%20in%20the%20channels%20between%20the%20pre-trained%0ARGB%20model%20and%20the%20HSI%2C%20the%20model%20cannot%20focus%20on%20the%20correlation%20along%20the%0Aspectral%20dimension%2C%20thus%20limiting%20its%20ability%20to%20utilize%20on%20HSI.%20Inspired%20by%0Athe%20HSI%20spatial-spectral%20decoupling%2C%20we%20propose%20a%20new%20framework%20that%20first%0Afine-tunes%20the%20pre-trained%20model%20with%20the%20spatial%20components%20%28known%20as%0Aeigenimages%29%2C%20and%20then%20infers%20on%20unseen%20HSI%20using%20an%20iterative%20spectral%0Aregularization%20%28ISR%29%20to%20maintain%20the%20spectral%20correlation.%20The%20advantages%20of%0Aour%20method%20lie%20in%3A%201%29%20we%20effectively%20inject%20the%20spatial%20texture%20processing%0Acapabilities%20of%20the%20pre-trained%20RGB%20model%20into%20HSI%20while%20keeping%20spectral%0Afidelity%2C%202%29%20learning%20in%20the%20spectral-decorrelated%20domain%20can%20improve%20the%0Ageneralizability%20to%20spectral-agnostic%20data%2C%20and%203%29%20our%20inference%20in%20the%0Aeigenimage%20domain%20naturally%20exploits%20the%20spectral%20low-rank%20property%20of%20HSI%2C%0Athereby%20reducing%20the%20complexity.%20This%20work%20bridges%20the%20gap%20between%20pre-trained%0ARGB%20models%20and%20HSI%20via%20eigenimages%2C%20addressing%20the%20issue%20of%20limited%20HSI%0Atraining%20data%2C%20hence%20the%20name%20EigenSR.%20Extensive%20experiments%20show%20that%20EigenSR%0Aoutperforms%20the%20state-of-the-art%20%28SOTA%29%20methods%20in%20both%20spatial%20and%20spectral%0Ametrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04050v2&entry.124074799=Read"},
{"title": "Inclusion 2024 Global Multimedia Deepfake Detection: Towards\n  Multi-dimensional Facial Forgery Detection", "author": "Yi Zhang and Weize Gao and Changtao Miao and Man Luo and Jianshu Li and Wenzhong Deng and Zhe Li and Bingyu Hu and Weibin Yao and Wenbo Zhou and Tao Gong and Qi Chu", "abstract": "  In this paper, we present the Global Multimedia Deepfake Detection held\nconcurrently with the Inclusion 2024. Our Multimedia Deepfake Detection aims to\ndetect automatic image and audio-video manipulations including but not limited\nto editing, synthesis, generation, Photoshop,etc. Our challenge has attracted\n1500 teams from all over the world, with about 5000 valid result submission\ncounts. We invite the top 20 teams to present their solutions to the challenge,\nfrom which the top 3 teams are awarded prizes in the grand finale. In this\npaper, we present the solutions from the top 3 teams of the two tracks, to\nboost the research work in the field of image and audio-video forgery\ndetection. The methodologies developed through the challenge will contribute to\nthe development of next-generation deepfake detection systems and we encourage\nparticipants to open source their methods.\n", "link": "http://arxiv.org/abs/2412.20833v1", "date": "2024-12-30", "relevancy": 2.0391, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5201}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5102}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4992}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inclusion%202024%20Global%20Multimedia%20Deepfake%20Detection%3A%20Towards%0A%20%20Multi-dimensional%20Facial%20Forgery%20Detection&body=Title%3A%20Inclusion%202024%20Global%20Multimedia%20Deepfake%20Detection%3A%20Towards%0A%20%20Multi-dimensional%20Facial%20Forgery%20Detection%0AAuthor%3A%20Yi%20Zhang%20and%20Weize%20Gao%20and%20Changtao%20Miao%20and%20Man%20Luo%20and%20Jianshu%20Li%20and%20Wenzhong%20Deng%20and%20Zhe%20Li%20and%20Bingyu%20Hu%20and%20Weibin%20Yao%20and%20Wenbo%20Zhou%20and%20Tao%20Gong%20and%20Qi%20Chu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20the%20Global%20Multimedia%20Deepfake%20Detection%20held%0Aconcurrently%20with%20the%20Inclusion%202024.%20Our%20Multimedia%20Deepfake%20Detection%20aims%20to%0Adetect%20automatic%20image%20and%20audio-video%20manipulations%20including%20but%20not%20limited%0Ato%20editing%2C%20synthesis%2C%20generation%2C%20Photoshop%2Cetc.%20Our%20challenge%20has%20attracted%0A1500%20teams%20from%20all%20over%20the%20world%2C%20with%20about%205000%20valid%20result%20submission%0Acounts.%20We%20invite%20the%20top%2020%20teams%20to%20present%20their%20solutions%20to%20the%20challenge%2C%0Afrom%20which%20the%20top%203%20teams%20are%20awarded%20prizes%20in%20the%20grand%20finale.%20In%20this%0Apaper%2C%20we%20present%20the%20solutions%20from%20the%20top%203%20teams%20of%20the%20two%20tracks%2C%20to%0Aboost%20the%20research%20work%20in%20the%20field%20of%20image%20and%20audio-video%20forgery%0Adetection.%20The%20methodologies%20developed%20through%20the%20challenge%20will%20contribute%20to%0Athe%20development%20of%20next-generation%20deepfake%20detection%20systems%20and%20we%20encourage%0Aparticipants%20to%20open%20source%20their%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20833v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInclusion%25202024%2520Global%2520Multimedia%2520Deepfake%2520Detection%253A%2520Towards%250A%2520%2520Multi-dimensional%2520Facial%2520Forgery%2520Detection%26entry.906535625%3DYi%2520Zhang%2520and%2520Weize%2520Gao%2520and%2520Changtao%2520Miao%2520and%2520Man%2520Luo%2520and%2520Jianshu%2520Li%2520and%2520Wenzhong%2520Deng%2520and%2520Zhe%2520Li%2520and%2520Bingyu%2520Hu%2520and%2520Weibin%2520Yao%2520and%2520Wenbo%2520Zhou%2520and%2520Tao%2520Gong%2520and%2520Qi%2520Chu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520Global%2520Multimedia%2520Deepfake%2520Detection%2520held%250Aconcurrently%2520with%2520the%2520Inclusion%25202024.%2520Our%2520Multimedia%2520Deepfake%2520Detection%2520aims%2520to%250Adetect%2520automatic%2520image%2520and%2520audio-video%2520manipulations%2520including%2520but%2520not%2520limited%250Ato%2520editing%252C%2520synthesis%252C%2520generation%252C%2520Photoshop%252Cetc.%2520Our%2520challenge%2520has%2520attracted%250A1500%2520teams%2520from%2520all%2520over%2520the%2520world%252C%2520with%2520about%25205000%2520valid%2520result%2520submission%250Acounts.%2520We%2520invite%2520the%2520top%252020%2520teams%2520to%2520present%2520their%2520solutions%2520to%2520the%2520challenge%252C%250Afrom%2520which%2520the%2520top%25203%2520teams%2520are%2520awarded%2520prizes%2520in%2520the%2520grand%2520finale.%2520In%2520this%250Apaper%252C%2520we%2520present%2520the%2520solutions%2520from%2520the%2520top%25203%2520teams%2520of%2520the%2520two%2520tracks%252C%2520to%250Aboost%2520the%2520research%2520work%2520in%2520the%2520field%2520of%2520image%2520and%2520audio-video%2520forgery%250Adetection.%2520The%2520methodologies%2520developed%2520through%2520the%2520challenge%2520will%2520contribute%2520to%250Athe%2520development%2520of%2520next-generation%2520deepfake%2520detection%2520systems%2520and%2520we%2520encourage%250Aparticipants%2520to%2520open%2520source%2520their%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20833v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inclusion%202024%20Global%20Multimedia%20Deepfake%20Detection%3A%20Towards%0A%20%20Multi-dimensional%20Facial%20Forgery%20Detection&entry.906535625=Yi%20Zhang%20and%20Weize%20Gao%20and%20Changtao%20Miao%20and%20Man%20Luo%20and%20Jianshu%20Li%20and%20Wenzhong%20Deng%20and%20Zhe%20Li%20and%20Bingyu%20Hu%20and%20Weibin%20Yao%20and%20Wenbo%20Zhou%20and%20Tao%20Gong%20and%20Qi%20Chu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20the%20Global%20Multimedia%20Deepfake%20Detection%20held%0Aconcurrently%20with%20the%20Inclusion%202024.%20Our%20Multimedia%20Deepfake%20Detection%20aims%20to%0Adetect%20automatic%20image%20and%20audio-video%20manipulations%20including%20but%20not%20limited%0Ato%20editing%2C%20synthesis%2C%20generation%2C%20Photoshop%2Cetc.%20Our%20challenge%20has%20attracted%0A1500%20teams%20from%20all%20over%20the%20world%2C%20with%20about%205000%20valid%20result%20submission%0Acounts.%20We%20invite%20the%20top%2020%20teams%20to%20present%20their%20solutions%20to%20the%20challenge%2C%0Afrom%20which%20the%20top%203%20teams%20are%20awarded%20prizes%20in%20the%20grand%20finale.%20In%20this%0Apaper%2C%20we%20present%20the%20solutions%20from%20the%20top%203%20teams%20of%20the%20two%20tracks%2C%20to%0Aboost%20the%20research%20work%20in%20the%20field%20of%20image%20and%20audio-video%20forgery%0Adetection.%20The%20methodologies%20developed%20through%20the%20challenge%20will%20contribute%20to%0Athe%20development%20of%20next-generation%20deepfake%20detection%20systems%20and%20we%20encourage%0Aparticipants%20to%20open%20source%20their%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20833v1&entry.124074799=Read"},
{"title": "STITCHER: Real-Time Trajectory Planning with Motion Primitive Search", "author": "Helene J. Levy and Brett T. Lopez", "abstract": "  Autonomous high-speed navigation through large, complex environments requires\nreal-time generation of agile trajectories that are dynamically feasible,\ncollision-free, and satisfy state or actuator constraints. Most modern\ntrajectory planning techniques rely on numerical optimization because\nhigh-quality, expressive trajectories that satisfy various constraints can be\nsystematically computed. However, meeting computation time constraints and the\npotential for numerical instabilities can limit the use of optimization-based\nplanners in safety-critical scenarios. This work presents an optimization-free\nplanning framework that stitches short trajectory segments together with graph\nsearch to compute long range, expressive, and near-optimal trajectories in\nreal-time. Our STITCHER algorithm is shown to outperform modern\noptimization-based planners through our innovative planning architecture and\nseveral algorithmic developments that make real-time planning possible.\nExtensive simulation testing is conducted to analyze the algorithmic components\nthat make up STITCHER, and a thorough comparison with two state-of-the-art\noptimization planners is performed. It is shown STITCHER can generate\ntrajectories through complex environments over long distances (tens of meters)\nwith low computation times (milliseconds).\n", "link": "http://arxiv.org/abs/2412.21180v1", "date": "2024-12-30", "relevancy": 2.018, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.52}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5079}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STITCHER%3A%20Real-Time%20Trajectory%20Planning%20with%20Motion%20Primitive%20Search&body=Title%3A%20STITCHER%3A%20Real-Time%20Trajectory%20Planning%20with%20Motion%20Primitive%20Search%0AAuthor%3A%20Helene%20J.%20Levy%20and%20Brett%20T.%20Lopez%0AAbstract%3A%20%20%20Autonomous%20high-speed%20navigation%20through%20large%2C%20complex%20environments%20requires%0Areal-time%20generation%20of%20agile%20trajectories%20that%20are%20dynamically%20feasible%2C%0Acollision-free%2C%20and%20satisfy%20state%20or%20actuator%20constraints.%20Most%20modern%0Atrajectory%20planning%20techniques%20rely%20on%20numerical%20optimization%20because%0Ahigh-quality%2C%20expressive%20trajectories%20that%20satisfy%20various%20constraints%20can%20be%0Asystematically%20computed.%20However%2C%20meeting%20computation%20time%20constraints%20and%20the%0Apotential%20for%20numerical%20instabilities%20can%20limit%20the%20use%20of%20optimization-based%0Aplanners%20in%20safety-critical%20scenarios.%20This%20work%20presents%20an%20optimization-free%0Aplanning%20framework%20that%20stitches%20short%20trajectory%20segments%20together%20with%20graph%0Asearch%20to%20compute%20long%20range%2C%20expressive%2C%20and%20near-optimal%20trajectories%20in%0Areal-time.%20Our%20STITCHER%20algorithm%20is%20shown%20to%20outperform%20modern%0Aoptimization-based%20planners%20through%20our%20innovative%20planning%20architecture%20and%0Aseveral%20algorithmic%20developments%20that%20make%20real-time%20planning%20possible.%0AExtensive%20simulation%20testing%20is%20conducted%20to%20analyze%20the%20algorithmic%20components%0Athat%20make%20up%20STITCHER%2C%20and%20a%20thorough%20comparison%20with%20two%20state-of-the-art%0Aoptimization%20planners%20is%20performed.%20It%20is%20shown%20STITCHER%20can%20generate%0Atrajectories%20through%20complex%20environments%20over%20long%20distances%20%28tens%20of%20meters%29%0Awith%20low%20computation%20times%20%28milliseconds%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21180v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTITCHER%253A%2520Real-Time%2520Trajectory%2520Planning%2520with%2520Motion%2520Primitive%2520Search%26entry.906535625%3DHelene%2520J.%2520Levy%2520and%2520Brett%2520T.%2520Lopez%26entry.1292438233%3D%2520%2520Autonomous%2520high-speed%2520navigation%2520through%2520large%252C%2520complex%2520environments%2520requires%250Areal-time%2520generation%2520of%2520agile%2520trajectories%2520that%2520are%2520dynamically%2520feasible%252C%250Acollision-free%252C%2520and%2520satisfy%2520state%2520or%2520actuator%2520constraints.%2520Most%2520modern%250Atrajectory%2520planning%2520techniques%2520rely%2520on%2520numerical%2520optimization%2520because%250Ahigh-quality%252C%2520expressive%2520trajectories%2520that%2520satisfy%2520various%2520constraints%2520can%2520be%250Asystematically%2520computed.%2520However%252C%2520meeting%2520computation%2520time%2520constraints%2520and%2520the%250Apotential%2520for%2520numerical%2520instabilities%2520can%2520limit%2520the%2520use%2520of%2520optimization-based%250Aplanners%2520in%2520safety-critical%2520scenarios.%2520This%2520work%2520presents%2520an%2520optimization-free%250Aplanning%2520framework%2520that%2520stitches%2520short%2520trajectory%2520segments%2520together%2520with%2520graph%250Asearch%2520to%2520compute%2520long%2520range%252C%2520expressive%252C%2520and%2520near-optimal%2520trajectories%2520in%250Areal-time.%2520Our%2520STITCHER%2520algorithm%2520is%2520shown%2520to%2520outperform%2520modern%250Aoptimization-based%2520planners%2520through%2520our%2520innovative%2520planning%2520architecture%2520and%250Aseveral%2520algorithmic%2520developments%2520that%2520make%2520real-time%2520planning%2520possible.%250AExtensive%2520simulation%2520testing%2520is%2520conducted%2520to%2520analyze%2520the%2520algorithmic%2520components%250Athat%2520make%2520up%2520STITCHER%252C%2520and%2520a%2520thorough%2520comparison%2520with%2520two%2520state-of-the-art%250Aoptimization%2520planners%2520is%2520performed.%2520It%2520is%2520shown%2520STITCHER%2520can%2520generate%250Atrajectories%2520through%2520complex%2520environments%2520over%2520long%2520distances%2520%2528tens%2520of%2520meters%2529%250Awith%2520low%2520computation%2520times%2520%2528milliseconds%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21180v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STITCHER%3A%20Real-Time%20Trajectory%20Planning%20with%20Motion%20Primitive%20Search&entry.906535625=Helene%20J.%20Levy%20and%20Brett%20T.%20Lopez&entry.1292438233=%20%20Autonomous%20high-speed%20navigation%20through%20large%2C%20complex%20environments%20requires%0Areal-time%20generation%20of%20agile%20trajectories%20that%20are%20dynamically%20feasible%2C%0Acollision-free%2C%20and%20satisfy%20state%20or%20actuator%20constraints.%20Most%20modern%0Atrajectory%20planning%20techniques%20rely%20on%20numerical%20optimization%20because%0Ahigh-quality%2C%20expressive%20trajectories%20that%20satisfy%20various%20constraints%20can%20be%0Asystematically%20computed.%20However%2C%20meeting%20computation%20time%20constraints%20and%20the%0Apotential%20for%20numerical%20instabilities%20can%20limit%20the%20use%20of%20optimization-based%0Aplanners%20in%20safety-critical%20scenarios.%20This%20work%20presents%20an%20optimization-free%0Aplanning%20framework%20that%20stitches%20short%20trajectory%20segments%20together%20with%20graph%0Asearch%20to%20compute%20long%20range%2C%20expressive%2C%20and%20near-optimal%20trajectories%20in%0Areal-time.%20Our%20STITCHER%20algorithm%20is%20shown%20to%20outperform%20modern%0Aoptimization-based%20planners%20through%20our%20innovative%20planning%20architecture%20and%0Aseveral%20algorithmic%20developments%20that%20make%20real-time%20planning%20possible.%0AExtensive%20simulation%20testing%20is%20conducted%20to%20analyze%20the%20algorithmic%20components%0Athat%20make%20up%20STITCHER%2C%20and%20a%20thorough%20comparison%20with%20two%20state-of-the-art%0Aoptimization%20planners%20is%20performed.%20It%20is%20shown%20STITCHER%20can%20generate%0Atrajectories%20through%20complex%20environments%20over%20long%20distances%20%28tens%20of%20meters%29%0Awith%20low%20computation%20times%20%28milliseconds%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21180v1&entry.124074799=Read"},
{"title": "Causal Flow-based Variational Auto-Encoder for Disentangled Causal\n  Representation Learning", "author": "Di Fan and Yannian Kou and Chuanhou Gao", "abstract": "  Disentangled representation learning aims to learn low-dimensional\nrepresentations where each dimension corresponds to an underlying generative\nfactor. While the Variational Auto-Encoder (VAE) is widely used for this\npurpose, most existing methods assume independence among factors, a\nsimplification that does not hold in many real-world scenarios where factors\nare often interdependent and exhibit causal relationships. To overcome this\nlimitation, we propose the Disentangled Causal Variational Auto-Encoder\n(DCVAE), a novel supervised VAE framework that integrates causal flows into the\nrepresentation learning process, enabling the learning of more meaningful and\ninterpretable disentangled representations. We evaluate DCVAE on both synthetic\nand real-world datasets, demonstrating its superior ability in causal\ndisentanglement and intervention experiments. Furthermore, DCVAE outperforms\nstate-of-the-art methods in various downstream tasks, highlighting its\npotential for learning true causal structures among factors.\n", "link": "http://arxiv.org/abs/2304.09010v5", "date": "2024-12-30", "relevancy": 2.0105, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.507}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5059}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal%20Flow-based%20Variational%20Auto-Encoder%20for%20Disentangled%20Causal%0A%20%20Representation%20Learning&body=Title%3A%20Causal%20Flow-based%20Variational%20Auto-Encoder%20for%20Disentangled%20Causal%0A%20%20Representation%20Learning%0AAuthor%3A%20Di%20Fan%20and%20Yannian%20Kou%20and%20Chuanhou%20Gao%0AAbstract%3A%20%20%20Disentangled%20representation%20learning%20aims%20to%20learn%20low-dimensional%0Arepresentations%20where%20each%20dimension%20corresponds%20to%20an%20underlying%20generative%0Afactor.%20While%20the%20Variational%20Auto-Encoder%20%28VAE%29%20is%20widely%20used%20for%20this%0Apurpose%2C%20most%20existing%20methods%20assume%20independence%20among%20factors%2C%20a%0Asimplification%20that%20does%20not%20hold%20in%20many%20real-world%20scenarios%20where%20factors%0Aare%20often%20interdependent%20and%20exhibit%20causal%20relationships.%20To%20overcome%20this%0Alimitation%2C%20we%20propose%20the%20Disentangled%20Causal%20Variational%20Auto-Encoder%0A%28DCVAE%29%2C%20a%20novel%20supervised%20VAE%20framework%20that%20integrates%20causal%20flows%20into%20the%0Arepresentation%20learning%20process%2C%20enabling%20the%20learning%20of%20more%20meaningful%20and%0Ainterpretable%20disentangled%20representations.%20We%20evaluate%20DCVAE%20on%20both%20synthetic%0Aand%20real-world%20datasets%2C%20demonstrating%20its%20superior%20ability%20in%20causal%0Adisentanglement%20and%20intervention%20experiments.%20Furthermore%2C%20DCVAE%20outperforms%0Astate-of-the-art%20methods%20in%20various%20downstream%20tasks%2C%20highlighting%20its%0Apotential%20for%20learning%20true%20causal%20structures%20among%20factors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.09010v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal%2520Flow-based%2520Variational%2520Auto-Encoder%2520for%2520Disentangled%2520Causal%250A%2520%2520Representation%2520Learning%26entry.906535625%3DDi%2520Fan%2520and%2520Yannian%2520Kou%2520and%2520Chuanhou%2520Gao%26entry.1292438233%3D%2520%2520Disentangled%2520representation%2520learning%2520aims%2520to%2520learn%2520low-dimensional%250Arepresentations%2520where%2520each%2520dimension%2520corresponds%2520to%2520an%2520underlying%2520generative%250Afactor.%2520While%2520the%2520Variational%2520Auto-Encoder%2520%2528VAE%2529%2520is%2520widely%2520used%2520for%2520this%250Apurpose%252C%2520most%2520existing%2520methods%2520assume%2520independence%2520among%2520factors%252C%2520a%250Asimplification%2520that%2520does%2520not%2520hold%2520in%2520many%2520real-world%2520scenarios%2520where%2520factors%250Aare%2520often%2520interdependent%2520and%2520exhibit%2520causal%2520relationships.%2520To%2520overcome%2520this%250Alimitation%252C%2520we%2520propose%2520the%2520Disentangled%2520Causal%2520Variational%2520Auto-Encoder%250A%2528DCVAE%2529%252C%2520a%2520novel%2520supervised%2520VAE%2520framework%2520that%2520integrates%2520causal%2520flows%2520into%2520the%250Arepresentation%2520learning%2520process%252C%2520enabling%2520the%2520learning%2520of%2520more%2520meaningful%2520and%250Ainterpretable%2520disentangled%2520representations.%2520We%2520evaluate%2520DCVAE%2520on%2520both%2520synthetic%250Aand%2520real-world%2520datasets%252C%2520demonstrating%2520its%2520superior%2520ability%2520in%2520causal%250Adisentanglement%2520and%2520intervention%2520experiments.%2520Furthermore%252C%2520DCVAE%2520outperforms%250Astate-of-the-art%2520methods%2520in%2520various%2520downstream%2520tasks%252C%2520highlighting%2520its%250Apotential%2520for%2520learning%2520true%2520causal%2520structures%2520among%2520factors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.09010v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20Flow-based%20Variational%20Auto-Encoder%20for%20Disentangled%20Causal%0A%20%20Representation%20Learning&entry.906535625=Di%20Fan%20and%20Yannian%20Kou%20and%20Chuanhou%20Gao&entry.1292438233=%20%20Disentangled%20representation%20learning%20aims%20to%20learn%20low-dimensional%0Arepresentations%20where%20each%20dimension%20corresponds%20to%20an%20underlying%20generative%0Afactor.%20While%20the%20Variational%20Auto-Encoder%20%28VAE%29%20is%20widely%20used%20for%20this%0Apurpose%2C%20most%20existing%20methods%20assume%20independence%20among%20factors%2C%20a%0Asimplification%20that%20does%20not%20hold%20in%20many%20real-world%20scenarios%20where%20factors%0Aare%20often%20interdependent%20and%20exhibit%20causal%20relationships.%20To%20overcome%20this%0Alimitation%2C%20we%20propose%20the%20Disentangled%20Causal%20Variational%20Auto-Encoder%0A%28DCVAE%29%2C%20a%20novel%20supervised%20VAE%20framework%20that%20integrates%20causal%20flows%20into%20the%0Arepresentation%20learning%20process%2C%20enabling%20the%20learning%20of%20more%20meaningful%20and%0Ainterpretable%20disentangled%20representations.%20We%20evaluate%20DCVAE%20on%20both%20synthetic%0Aand%20real-world%20datasets%2C%20demonstrating%20its%20superior%20ability%20in%20causal%0Adisentanglement%20and%20intervention%20experiments.%20Furthermore%2C%20DCVAE%20outperforms%0Astate-of-the-art%20methods%20in%20various%20downstream%20tasks%2C%20highlighting%20its%0Apotential%20for%20learning%20true%20causal%20structures%20among%20factors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.09010v5&entry.124074799=Read"},
{"title": "DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought", "author": "Jiaan Wang and Fandong Meng and Yunlong Liang and Jie Zhou", "abstract": "  Recently, O1-like models have emerged as representative examples,\nillustrating the effectiveness of long chain-of-thought (CoT) in reasoning\ntasks such as math and coding tasks. In this paper, we introduce DRT-o1, an\nattempt to bring the success of long CoT to neural machine translation (MT).\nSpecifically, in view of the literature books that might involve similes and\nmetaphors, translating these texts to a target language is very difficult in\npractice due to cultural differences. In such cases, literal translation often\nfails to convey the intended meaning effectively. Even for professional human\ntranslators, considerable thought must be given to preserving semantics\nthroughout the translation process. To simulate LLMs' long thought ability in\nMT, we first mine sentences containing similes or metaphors from existing\nliterature books, and then develop a multi-agent framework to translate these\nsentences via long thought. In the multi-agent framework, a translator is used\nto iteratively translate the source sentence under the suggestions provided by\nan advisor. To ensure the effectiveness of the long thoughts, an evaluator is\nalso employed to quantify the translation in each round. In this way, we\ncollect tens of thousands of long-thought MT data, which is used to train our\nDRT-o1. Using Qwen2.5 and LLama-3.1 as the backbones, DRT-o1 models can learn\nthe thought process during machine translation, and outperform vanilla LLMs as\nwell as existing O1-like LLMs, showing their effectiveness The project is\navailable at https://github.com/krystalan/DRT-o1\n", "link": "http://arxiv.org/abs/2412.17498v2", "date": "2024-12-30", "relevancy": 2.0018, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5028}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5028}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DRT-o1%3A%20Optimized%20Deep%20Reasoning%20Translation%20via%20Long%20Chain-of-Thought&body=Title%3A%20DRT-o1%3A%20Optimized%20Deep%20Reasoning%20Translation%20via%20Long%20Chain-of-Thought%0AAuthor%3A%20Jiaan%20Wang%20and%20Fandong%20Meng%20and%20Yunlong%20Liang%20and%20Jie%20Zhou%0AAbstract%3A%20%20%20Recently%2C%20O1-like%20models%20have%20emerged%20as%20representative%20examples%2C%0Aillustrating%20the%20effectiveness%20of%20long%20chain-of-thought%20%28CoT%29%20in%20reasoning%0Atasks%20such%20as%20math%20and%20coding%20tasks.%20In%20this%20paper%2C%20we%20introduce%20DRT-o1%2C%20an%0Aattempt%20to%20bring%20the%20success%20of%20long%20CoT%20to%20neural%20machine%20translation%20%28MT%29.%0ASpecifically%2C%20in%20view%20of%20the%20literature%20books%20that%20might%20involve%20similes%20and%0Ametaphors%2C%20translating%20these%20texts%20to%20a%20target%20language%20is%20very%20difficult%20in%0Apractice%20due%20to%20cultural%20differences.%20In%20such%20cases%2C%20literal%20translation%20often%0Afails%20to%20convey%20the%20intended%20meaning%20effectively.%20Even%20for%20professional%20human%0Atranslators%2C%20considerable%20thought%20must%20be%20given%20to%20preserving%20semantics%0Athroughout%20the%20translation%20process.%20To%20simulate%20LLMs%27%20long%20thought%20ability%20in%0AMT%2C%20we%20first%20mine%20sentences%20containing%20similes%20or%20metaphors%20from%20existing%0Aliterature%20books%2C%20and%20then%20develop%20a%20multi-agent%20framework%20to%20translate%20these%0Asentences%20via%20long%20thought.%20In%20the%20multi-agent%20framework%2C%20a%20translator%20is%20used%0Ato%20iteratively%20translate%20the%20source%20sentence%20under%20the%20suggestions%20provided%20by%0Aan%20advisor.%20To%20ensure%20the%20effectiveness%20of%20the%20long%20thoughts%2C%20an%20evaluator%20is%0Aalso%20employed%20to%20quantify%20the%20translation%20in%20each%20round.%20In%20this%20way%2C%20we%0Acollect%20tens%20of%20thousands%20of%20long-thought%20MT%20data%2C%20which%20is%20used%20to%20train%20our%0ADRT-o1.%20Using%20Qwen2.5%20and%20LLama-3.1%20as%20the%20backbones%2C%20DRT-o1%20models%20can%20learn%0Athe%20thought%20process%20during%20machine%20translation%2C%20and%20outperform%20vanilla%20LLMs%20as%0Awell%20as%20existing%20O1-like%20LLMs%2C%20showing%20their%20effectiveness%20The%20project%20is%0Aavailable%20at%20https%3A//github.com/krystalan/DRT-o1%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17498v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDRT-o1%253A%2520Optimized%2520Deep%2520Reasoning%2520Translation%2520via%2520Long%2520Chain-of-Thought%26entry.906535625%3DJiaan%2520Wang%2520and%2520Fandong%2520Meng%2520and%2520Yunlong%2520Liang%2520and%2520Jie%2520Zhou%26entry.1292438233%3D%2520%2520Recently%252C%2520O1-like%2520models%2520have%2520emerged%2520as%2520representative%2520examples%252C%250Aillustrating%2520the%2520effectiveness%2520of%2520long%2520chain-of-thought%2520%2528CoT%2529%2520in%2520reasoning%250Atasks%2520such%2520as%2520math%2520and%2520coding%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520DRT-o1%252C%2520an%250Aattempt%2520to%2520bring%2520the%2520success%2520of%2520long%2520CoT%2520to%2520neural%2520machine%2520translation%2520%2528MT%2529.%250ASpecifically%252C%2520in%2520view%2520of%2520the%2520literature%2520books%2520that%2520might%2520involve%2520similes%2520and%250Ametaphors%252C%2520translating%2520these%2520texts%2520to%2520a%2520target%2520language%2520is%2520very%2520difficult%2520in%250Apractice%2520due%2520to%2520cultural%2520differences.%2520In%2520such%2520cases%252C%2520literal%2520translation%2520often%250Afails%2520to%2520convey%2520the%2520intended%2520meaning%2520effectively.%2520Even%2520for%2520professional%2520human%250Atranslators%252C%2520considerable%2520thought%2520must%2520be%2520given%2520to%2520preserving%2520semantics%250Athroughout%2520the%2520translation%2520process.%2520To%2520simulate%2520LLMs%2527%2520long%2520thought%2520ability%2520in%250AMT%252C%2520we%2520first%2520mine%2520sentences%2520containing%2520similes%2520or%2520metaphors%2520from%2520existing%250Aliterature%2520books%252C%2520and%2520then%2520develop%2520a%2520multi-agent%2520framework%2520to%2520translate%2520these%250Asentences%2520via%2520long%2520thought.%2520In%2520the%2520multi-agent%2520framework%252C%2520a%2520translator%2520is%2520used%250Ato%2520iteratively%2520translate%2520the%2520source%2520sentence%2520under%2520the%2520suggestions%2520provided%2520by%250Aan%2520advisor.%2520To%2520ensure%2520the%2520effectiveness%2520of%2520the%2520long%2520thoughts%252C%2520an%2520evaluator%2520is%250Aalso%2520employed%2520to%2520quantify%2520the%2520translation%2520in%2520each%2520round.%2520In%2520this%2520way%252C%2520we%250Acollect%2520tens%2520of%2520thousands%2520of%2520long-thought%2520MT%2520data%252C%2520which%2520is%2520used%2520to%2520train%2520our%250ADRT-o1.%2520Using%2520Qwen2.5%2520and%2520LLama-3.1%2520as%2520the%2520backbones%252C%2520DRT-o1%2520models%2520can%2520learn%250Athe%2520thought%2520process%2520during%2520machine%2520translation%252C%2520and%2520outperform%2520vanilla%2520LLMs%2520as%250Awell%2520as%2520existing%2520O1-like%2520LLMs%252C%2520showing%2520their%2520effectiveness%2520The%2520project%2520is%250Aavailable%2520at%2520https%253A//github.com/krystalan/DRT-o1%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17498v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DRT-o1%3A%20Optimized%20Deep%20Reasoning%20Translation%20via%20Long%20Chain-of-Thought&entry.906535625=Jiaan%20Wang%20and%20Fandong%20Meng%20and%20Yunlong%20Liang%20and%20Jie%20Zhou&entry.1292438233=%20%20Recently%2C%20O1-like%20models%20have%20emerged%20as%20representative%20examples%2C%0Aillustrating%20the%20effectiveness%20of%20long%20chain-of-thought%20%28CoT%29%20in%20reasoning%0Atasks%20such%20as%20math%20and%20coding%20tasks.%20In%20this%20paper%2C%20we%20introduce%20DRT-o1%2C%20an%0Aattempt%20to%20bring%20the%20success%20of%20long%20CoT%20to%20neural%20machine%20translation%20%28MT%29.%0ASpecifically%2C%20in%20view%20of%20the%20literature%20books%20that%20might%20involve%20similes%20and%0Ametaphors%2C%20translating%20these%20texts%20to%20a%20target%20language%20is%20very%20difficult%20in%0Apractice%20due%20to%20cultural%20differences.%20In%20such%20cases%2C%20literal%20translation%20often%0Afails%20to%20convey%20the%20intended%20meaning%20effectively.%20Even%20for%20professional%20human%0Atranslators%2C%20considerable%20thought%20must%20be%20given%20to%20preserving%20semantics%0Athroughout%20the%20translation%20process.%20To%20simulate%20LLMs%27%20long%20thought%20ability%20in%0AMT%2C%20we%20first%20mine%20sentences%20containing%20similes%20or%20metaphors%20from%20existing%0Aliterature%20books%2C%20and%20then%20develop%20a%20multi-agent%20framework%20to%20translate%20these%0Asentences%20via%20long%20thought.%20In%20the%20multi-agent%20framework%2C%20a%20translator%20is%20used%0Ato%20iteratively%20translate%20the%20source%20sentence%20under%20the%20suggestions%20provided%20by%0Aan%20advisor.%20To%20ensure%20the%20effectiveness%20of%20the%20long%20thoughts%2C%20an%20evaluator%20is%0Aalso%20employed%20to%20quantify%20the%20translation%20in%20each%20round.%20In%20this%20way%2C%20we%0Acollect%20tens%20of%20thousands%20of%20long-thought%20MT%20data%2C%20which%20is%20used%20to%20train%20our%0ADRT-o1.%20Using%20Qwen2.5%20and%20LLama-3.1%20as%20the%20backbones%2C%20DRT-o1%20models%20can%20learn%0Athe%20thought%20process%20during%20machine%20translation%2C%20and%20outperform%20vanilla%20LLMs%20as%0Awell%20as%20existing%20O1-like%20LLMs%2C%20showing%20their%20effectiveness%20The%20project%20is%0Aavailable%20at%20https%3A//github.com/krystalan/DRT-o1%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17498v2&entry.124074799=Read"},
{"title": "High-Rank Irreducible Cartesian Tensor Decomposition and Bases of\n  Equivariant Spaces", "author": "Shihao Shao and Yikang Li and Zhouchen Lin and Qinghua Cui", "abstract": "  Irreducible Cartesian tensors (ICTs) play a crucial role in the design of\nequivariant graph neural networks, as well as in theoretical chemistry and\nchemical physics. Meanwhile, the design space of available linear operations on\ntensors that preserve symmetry presents a significant challenge. The ICT\ndecomposition and a basis of this equivariant space are difficult to obtain for\nhigh-order tensors. After decades of research, we recently achieve an explicit\nICT decomposition for $n=5$ \\citep{bonvicini2024irreducible} with factorial\ntime/space complexity. This work, for the first time, obtains decomposition\nmatrices for ICTs up to rank $n=9$ with reduced and affordable complexity, by\nconstructing what we call path matrices. The path matrices are obtained via\nperforming chain-like contraction with Clebsch-Gordan matrices following the\nparentage scheme. We prove and leverage that the concatenation of path matrices\nis an orthonormal change-of-basis matrix between the Cartesian tensor product\nspace and the spherical direct sum spaces. Furthermore, we identify a complete\northogonal basis for the equivariant space, rather than a spanning set\n\\citep{pearce2023brauer}, through this path matrices technique. We further\nextend our result to the arbitrary tensor product and direct sum spaces,\nenabling free design between different spaces while keeping symmetry. The\nPython code is available in\nhttps://github.com/ShihaoShao-GH/ICT-decomposition-and-equivariant-bases where\nthe $n=6,\\dots,9$ ICT decomposition matrices are obtained in 1s, 3s, 11s, and\n4m32s, respectively.\n", "link": "http://arxiv.org/abs/2412.18263v2", "date": "2024-12-30", "relevancy": 2.0016, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4016}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.3997}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.3997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Rank%20Irreducible%20Cartesian%20Tensor%20Decomposition%20and%20Bases%20of%0A%20%20Equivariant%20Spaces&body=Title%3A%20High-Rank%20Irreducible%20Cartesian%20Tensor%20Decomposition%20and%20Bases%20of%0A%20%20Equivariant%20Spaces%0AAuthor%3A%20Shihao%20Shao%20and%20Yikang%20Li%20and%20Zhouchen%20Lin%20and%20Qinghua%20Cui%0AAbstract%3A%20%20%20Irreducible%20Cartesian%20tensors%20%28ICTs%29%20play%20a%20crucial%20role%20in%20the%20design%20of%0Aequivariant%20graph%20neural%20networks%2C%20as%20well%20as%20in%20theoretical%20chemistry%20and%0Achemical%20physics.%20Meanwhile%2C%20the%20design%20space%20of%20available%20linear%20operations%20on%0Atensors%20that%20preserve%20symmetry%20presents%20a%20significant%20challenge.%20The%20ICT%0Adecomposition%20and%20a%20basis%20of%20this%20equivariant%20space%20are%20difficult%20to%20obtain%20for%0Ahigh-order%20tensors.%20After%20decades%20of%20research%2C%20we%20recently%20achieve%20an%20explicit%0AICT%20decomposition%20for%20%24n%3D5%24%20%5Ccitep%7Bbonvicini2024irreducible%7D%20with%20factorial%0Atime/space%20complexity.%20This%20work%2C%20for%20the%20first%20time%2C%20obtains%20decomposition%0Amatrices%20for%20ICTs%20up%20to%20rank%20%24n%3D9%24%20with%20reduced%20and%20affordable%20complexity%2C%20by%0Aconstructing%20what%20we%20call%20path%20matrices.%20The%20path%20matrices%20are%20obtained%20via%0Aperforming%20chain-like%20contraction%20with%20Clebsch-Gordan%20matrices%20following%20the%0Aparentage%20scheme.%20We%20prove%20and%20leverage%20that%20the%20concatenation%20of%20path%20matrices%0Ais%20an%20orthonormal%20change-of-basis%20matrix%20between%20the%20Cartesian%20tensor%20product%0Aspace%20and%20the%20spherical%20direct%20sum%20spaces.%20Furthermore%2C%20we%20identify%20a%20complete%0Aorthogonal%20basis%20for%20the%20equivariant%20space%2C%20rather%20than%20a%20spanning%20set%0A%5Ccitep%7Bpearce2023brauer%7D%2C%20through%20this%20path%20matrices%20technique.%20We%20further%0Aextend%20our%20result%20to%20the%20arbitrary%20tensor%20product%20and%20direct%20sum%20spaces%2C%0Aenabling%20free%20design%20between%20different%20spaces%20while%20keeping%20symmetry.%20The%0APython%20code%20is%20available%20in%0Ahttps%3A//github.com/ShihaoShao-GH/ICT-decomposition-and-equivariant-bases%20where%0Athe%20%24n%3D6%2C%5Cdots%2C9%24%20ICT%20decomposition%20matrices%20are%20obtained%20in%201s%2C%203s%2C%2011s%2C%20and%0A4m32s%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18263v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Rank%2520Irreducible%2520Cartesian%2520Tensor%2520Decomposition%2520and%2520Bases%2520of%250A%2520%2520Equivariant%2520Spaces%26entry.906535625%3DShihao%2520Shao%2520and%2520Yikang%2520Li%2520and%2520Zhouchen%2520Lin%2520and%2520Qinghua%2520Cui%26entry.1292438233%3D%2520%2520Irreducible%2520Cartesian%2520tensors%2520%2528ICTs%2529%2520play%2520a%2520crucial%2520role%2520in%2520the%2520design%2520of%250Aequivariant%2520graph%2520neural%2520networks%252C%2520as%2520well%2520as%2520in%2520theoretical%2520chemistry%2520and%250Achemical%2520physics.%2520Meanwhile%252C%2520the%2520design%2520space%2520of%2520available%2520linear%2520operations%2520on%250Atensors%2520that%2520preserve%2520symmetry%2520presents%2520a%2520significant%2520challenge.%2520The%2520ICT%250Adecomposition%2520and%2520a%2520basis%2520of%2520this%2520equivariant%2520space%2520are%2520difficult%2520to%2520obtain%2520for%250Ahigh-order%2520tensors.%2520After%2520decades%2520of%2520research%252C%2520we%2520recently%2520achieve%2520an%2520explicit%250AICT%2520decomposition%2520for%2520%2524n%253D5%2524%2520%255Ccitep%257Bbonvicini2024irreducible%257D%2520with%2520factorial%250Atime/space%2520complexity.%2520This%2520work%252C%2520for%2520the%2520first%2520time%252C%2520obtains%2520decomposition%250Amatrices%2520for%2520ICTs%2520up%2520to%2520rank%2520%2524n%253D9%2524%2520with%2520reduced%2520and%2520affordable%2520complexity%252C%2520by%250Aconstructing%2520what%2520we%2520call%2520path%2520matrices.%2520The%2520path%2520matrices%2520are%2520obtained%2520via%250Aperforming%2520chain-like%2520contraction%2520with%2520Clebsch-Gordan%2520matrices%2520following%2520the%250Aparentage%2520scheme.%2520We%2520prove%2520and%2520leverage%2520that%2520the%2520concatenation%2520of%2520path%2520matrices%250Ais%2520an%2520orthonormal%2520change-of-basis%2520matrix%2520between%2520the%2520Cartesian%2520tensor%2520product%250Aspace%2520and%2520the%2520spherical%2520direct%2520sum%2520spaces.%2520Furthermore%252C%2520we%2520identify%2520a%2520complete%250Aorthogonal%2520basis%2520for%2520the%2520equivariant%2520space%252C%2520rather%2520than%2520a%2520spanning%2520set%250A%255Ccitep%257Bpearce2023brauer%257D%252C%2520through%2520this%2520path%2520matrices%2520technique.%2520We%2520further%250Aextend%2520our%2520result%2520to%2520the%2520arbitrary%2520tensor%2520product%2520and%2520direct%2520sum%2520spaces%252C%250Aenabling%2520free%2520design%2520between%2520different%2520spaces%2520while%2520keeping%2520symmetry.%2520The%250APython%2520code%2520is%2520available%2520in%250Ahttps%253A//github.com/ShihaoShao-GH/ICT-decomposition-and-equivariant-bases%2520where%250Athe%2520%2524n%253D6%252C%255Cdots%252C9%2524%2520ICT%2520decomposition%2520matrices%2520are%2520obtained%2520in%25201s%252C%25203s%252C%252011s%252C%2520and%250A4m32s%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18263v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Rank%20Irreducible%20Cartesian%20Tensor%20Decomposition%20and%20Bases%20of%0A%20%20Equivariant%20Spaces&entry.906535625=Shihao%20Shao%20and%20Yikang%20Li%20and%20Zhouchen%20Lin%20and%20Qinghua%20Cui&entry.1292438233=%20%20Irreducible%20Cartesian%20tensors%20%28ICTs%29%20play%20a%20crucial%20role%20in%20the%20design%20of%0Aequivariant%20graph%20neural%20networks%2C%20as%20well%20as%20in%20theoretical%20chemistry%20and%0Achemical%20physics.%20Meanwhile%2C%20the%20design%20space%20of%20available%20linear%20operations%20on%0Atensors%20that%20preserve%20symmetry%20presents%20a%20significant%20challenge.%20The%20ICT%0Adecomposition%20and%20a%20basis%20of%20this%20equivariant%20space%20are%20difficult%20to%20obtain%20for%0Ahigh-order%20tensors.%20After%20decades%20of%20research%2C%20we%20recently%20achieve%20an%20explicit%0AICT%20decomposition%20for%20%24n%3D5%24%20%5Ccitep%7Bbonvicini2024irreducible%7D%20with%20factorial%0Atime/space%20complexity.%20This%20work%2C%20for%20the%20first%20time%2C%20obtains%20decomposition%0Amatrices%20for%20ICTs%20up%20to%20rank%20%24n%3D9%24%20with%20reduced%20and%20affordable%20complexity%2C%20by%0Aconstructing%20what%20we%20call%20path%20matrices.%20The%20path%20matrices%20are%20obtained%20via%0Aperforming%20chain-like%20contraction%20with%20Clebsch-Gordan%20matrices%20following%20the%0Aparentage%20scheme.%20We%20prove%20and%20leverage%20that%20the%20concatenation%20of%20path%20matrices%0Ais%20an%20orthonormal%20change-of-basis%20matrix%20between%20the%20Cartesian%20tensor%20product%0Aspace%20and%20the%20spherical%20direct%20sum%20spaces.%20Furthermore%2C%20we%20identify%20a%20complete%0Aorthogonal%20basis%20for%20the%20equivariant%20space%2C%20rather%20than%20a%20spanning%20set%0A%5Ccitep%7Bpearce2023brauer%7D%2C%20through%20this%20path%20matrices%20technique.%20We%20further%0Aextend%20our%20result%20to%20the%20arbitrary%20tensor%20product%20and%20direct%20sum%20spaces%2C%0Aenabling%20free%20design%20between%20different%20spaces%20while%20keeping%20symmetry.%20The%0APython%20code%20is%20available%20in%0Ahttps%3A//github.com/ShihaoShao-GH/ICT-decomposition-and-equivariant-bases%20where%0Athe%20%24n%3D6%2C%5Cdots%2C9%24%20ICT%20decomposition%20matrices%20are%20obtained%20in%201s%2C%203s%2C%2011s%2C%20and%0A4m32s%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18263v2&entry.124074799=Read"},
{"title": "DeepF-fNet: a physics-informed neural network for vibration isolation\n  optimization", "author": "A. Tollardo and F. Cadini and M. Giglio and L. Lomazzi", "abstract": "  Structural optimization is essential for designing safe, efficient, and\ndurable components with minimal material usage. Traditional methods for\nvibration control often rely on active systems to mitigate unpredictable\nvibrations, which may lead to resonance and potential structural failure.\nHowever, these methods face significant challenges when addressing the\nnonlinear inverse eigenvalue problems required for optimizing structures\nsubjected to a wide range of frequencies. As a result, no existing approach has\neffectively addressed the need for real-time vibration suppression within this\ncontext, particularly in high-performance environments such as automotive\nnoise, vibration and harshness, where computational efficiency is crucial.\n  This study introduces DeepF-fNet, a novel neural network framework designed\nto replace traditional active systems in vibration-based structural\noptimization. Leveraging DeepONets within the context of physics-informed\nneural networks, DeepF-fNet integrates both data and the governing physical\nlaws. This enables rapid identification of optimal parameters to suppress\ncritical vibrations at specific frequencies, offering a more efficient and\nreal-time alternative to conventional methods.\n  The proposed framework is validated through a case study involving a locally\nresonant metamaterial used to isolate structures from user-defined frequency\nranges. The results demonstrate that DeepF-fNet outperforms traditional genetic\nalgorithms in terms of computational speed while achieving comparable results,\nmaking it a promising tool for vibration-sensitive applications. By replacing\nactive systems with machine learning techniques, DeepF-fNet paves the way for\nmore efficient and cost-effective structural optimization in real-world\nscenarios.\n", "link": "http://arxiv.org/abs/2412.21132v1", "date": "2024-12-30", "relevancy": 1.998, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5362}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4769}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepF-fNet%3A%20a%20physics-informed%20neural%20network%20for%20vibration%20isolation%0A%20%20optimization&body=Title%3A%20DeepF-fNet%3A%20a%20physics-informed%20neural%20network%20for%20vibration%20isolation%0A%20%20optimization%0AAuthor%3A%20A.%20Tollardo%20and%20F.%20Cadini%20and%20M.%20Giglio%20and%20L.%20Lomazzi%0AAbstract%3A%20%20%20Structural%20optimization%20is%20essential%20for%20designing%20safe%2C%20efficient%2C%20and%0Adurable%20components%20with%20minimal%20material%20usage.%20Traditional%20methods%20for%0Avibration%20control%20often%20rely%20on%20active%20systems%20to%20mitigate%20unpredictable%0Avibrations%2C%20which%20may%20lead%20to%20resonance%20and%20potential%20structural%20failure.%0AHowever%2C%20these%20methods%20face%20significant%20challenges%20when%20addressing%20the%0Anonlinear%20inverse%20eigenvalue%20problems%20required%20for%20optimizing%20structures%0Asubjected%20to%20a%20wide%20range%20of%20frequencies.%20As%20a%20result%2C%20no%20existing%20approach%20has%0Aeffectively%20addressed%20the%20need%20for%20real-time%20vibration%20suppression%20within%20this%0Acontext%2C%20particularly%20in%20high-performance%20environments%20such%20as%20automotive%0Anoise%2C%20vibration%20and%20harshness%2C%20where%20computational%20efficiency%20is%20crucial.%0A%20%20This%20study%20introduces%20DeepF-fNet%2C%20a%20novel%20neural%20network%20framework%20designed%0Ato%20replace%20traditional%20active%20systems%20in%20vibration-based%20structural%0Aoptimization.%20Leveraging%20DeepONets%20within%20the%20context%20of%20physics-informed%0Aneural%20networks%2C%20DeepF-fNet%20integrates%20both%20data%20and%20the%20governing%20physical%0Alaws.%20This%20enables%20rapid%20identification%20of%20optimal%20parameters%20to%20suppress%0Acritical%20vibrations%20at%20specific%20frequencies%2C%20offering%20a%20more%20efficient%20and%0Areal-time%20alternative%20to%20conventional%20methods.%0A%20%20The%20proposed%20framework%20is%20validated%20through%20a%20case%20study%20involving%20a%20locally%0Aresonant%20metamaterial%20used%20to%20isolate%20structures%20from%20user-defined%20frequency%0Aranges.%20The%20results%20demonstrate%20that%20DeepF-fNet%20outperforms%20traditional%20genetic%0Aalgorithms%20in%20terms%20of%20computational%20speed%20while%20achieving%20comparable%20results%2C%0Amaking%20it%20a%20promising%20tool%20for%20vibration-sensitive%20applications.%20By%20replacing%0Aactive%20systems%20with%20machine%20learning%20techniques%2C%20DeepF-fNet%20paves%20the%20way%20for%0Amore%20efficient%20and%20cost-effective%20structural%20optimization%20in%20real-world%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21132v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepF-fNet%253A%2520a%2520physics-informed%2520neural%2520network%2520for%2520vibration%2520isolation%250A%2520%2520optimization%26entry.906535625%3DA.%2520Tollardo%2520and%2520F.%2520Cadini%2520and%2520M.%2520Giglio%2520and%2520L.%2520Lomazzi%26entry.1292438233%3D%2520%2520Structural%2520optimization%2520is%2520essential%2520for%2520designing%2520safe%252C%2520efficient%252C%2520and%250Adurable%2520components%2520with%2520minimal%2520material%2520usage.%2520Traditional%2520methods%2520for%250Avibration%2520control%2520often%2520rely%2520on%2520active%2520systems%2520to%2520mitigate%2520unpredictable%250Avibrations%252C%2520which%2520may%2520lead%2520to%2520resonance%2520and%2520potential%2520structural%2520failure.%250AHowever%252C%2520these%2520methods%2520face%2520significant%2520challenges%2520when%2520addressing%2520the%250Anonlinear%2520inverse%2520eigenvalue%2520problems%2520required%2520for%2520optimizing%2520structures%250Asubjected%2520to%2520a%2520wide%2520range%2520of%2520frequencies.%2520As%2520a%2520result%252C%2520no%2520existing%2520approach%2520has%250Aeffectively%2520addressed%2520the%2520need%2520for%2520real-time%2520vibration%2520suppression%2520within%2520this%250Acontext%252C%2520particularly%2520in%2520high-performance%2520environments%2520such%2520as%2520automotive%250Anoise%252C%2520vibration%2520and%2520harshness%252C%2520where%2520computational%2520efficiency%2520is%2520crucial.%250A%2520%2520This%2520study%2520introduces%2520DeepF-fNet%252C%2520a%2520novel%2520neural%2520network%2520framework%2520designed%250Ato%2520replace%2520traditional%2520active%2520systems%2520in%2520vibration-based%2520structural%250Aoptimization.%2520Leveraging%2520DeepONets%2520within%2520the%2520context%2520of%2520physics-informed%250Aneural%2520networks%252C%2520DeepF-fNet%2520integrates%2520both%2520data%2520and%2520the%2520governing%2520physical%250Alaws.%2520This%2520enables%2520rapid%2520identification%2520of%2520optimal%2520parameters%2520to%2520suppress%250Acritical%2520vibrations%2520at%2520specific%2520frequencies%252C%2520offering%2520a%2520more%2520efficient%2520and%250Areal-time%2520alternative%2520to%2520conventional%2520methods.%250A%2520%2520The%2520proposed%2520framework%2520is%2520validated%2520through%2520a%2520case%2520study%2520involving%2520a%2520locally%250Aresonant%2520metamaterial%2520used%2520to%2520isolate%2520structures%2520from%2520user-defined%2520frequency%250Aranges.%2520The%2520results%2520demonstrate%2520that%2520DeepF-fNet%2520outperforms%2520traditional%2520genetic%250Aalgorithms%2520in%2520terms%2520of%2520computational%2520speed%2520while%2520achieving%2520comparable%2520results%252C%250Amaking%2520it%2520a%2520promising%2520tool%2520for%2520vibration-sensitive%2520applications.%2520By%2520replacing%250Aactive%2520systems%2520with%2520machine%2520learning%2520techniques%252C%2520DeepF-fNet%2520paves%2520the%2520way%2520for%250Amore%2520efficient%2520and%2520cost-effective%2520structural%2520optimization%2520in%2520real-world%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21132v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepF-fNet%3A%20a%20physics-informed%20neural%20network%20for%20vibration%20isolation%0A%20%20optimization&entry.906535625=A.%20Tollardo%20and%20F.%20Cadini%20and%20M.%20Giglio%20and%20L.%20Lomazzi&entry.1292438233=%20%20Structural%20optimization%20is%20essential%20for%20designing%20safe%2C%20efficient%2C%20and%0Adurable%20components%20with%20minimal%20material%20usage.%20Traditional%20methods%20for%0Avibration%20control%20often%20rely%20on%20active%20systems%20to%20mitigate%20unpredictable%0Avibrations%2C%20which%20may%20lead%20to%20resonance%20and%20potential%20structural%20failure.%0AHowever%2C%20these%20methods%20face%20significant%20challenges%20when%20addressing%20the%0Anonlinear%20inverse%20eigenvalue%20problems%20required%20for%20optimizing%20structures%0Asubjected%20to%20a%20wide%20range%20of%20frequencies.%20As%20a%20result%2C%20no%20existing%20approach%20has%0Aeffectively%20addressed%20the%20need%20for%20real-time%20vibration%20suppression%20within%20this%0Acontext%2C%20particularly%20in%20high-performance%20environments%20such%20as%20automotive%0Anoise%2C%20vibration%20and%20harshness%2C%20where%20computational%20efficiency%20is%20crucial.%0A%20%20This%20study%20introduces%20DeepF-fNet%2C%20a%20novel%20neural%20network%20framework%20designed%0Ato%20replace%20traditional%20active%20systems%20in%20vibration-based%20structural%0Aoptimization.%20Leveraging%20DeepONets%20within%20the%20context%20of%20physics-informed%0Aneural%20networks%2C%20DeepF-fNet%20integrates%20both%20data%20and%20the%20governing%20physical%0Alaws.%20This%20enables%20rapid%20identification%20of%20optimal%20parameters%20to%20suppress%0Acritical%20vibrations%20at%20specific%20frequencies%2C%20offering%20a%20more%20efficient%20and%0Areal-time%20alternative%20to%20conventional%20methods.%0A%20%20The%20proposed%20framework%20is%20validated%20through%20a%20case%20study%20involving%20a%20locally%0Aresonant%20metamaterial%20used%20to%20isolate%20structures%20from%20user-defined%20frequency%0Aranges.%20The%20results%20demonstrate%20that%20DeepF-fNet%20outperforms%20traditional%20genetic%0Aalgorithms%20in%20terms%20of%20computational%20speed%20while%20achieving%20comparable%20results%2C%0Amaking%20it%20a%20promising%20tool%20for%20vibration-sensitive%20applications.%20By%20replacing%0Aactive%20systems%20with%20machine%20learning%20techniques%2C%20DeepF-fNet%20paves%20the%20way%20for%0Amore%20efficient%20and%20cost-effective%20structural%20optimization%20in%20real-world%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21132v1&entry.124074799=Read"},
{"title": "Towards Effective Discrimination Testing for Generative AI", "author": "Thomas P. Zollo and Nikita Rajaneesh and Richard Zemel and Talia B. Gillis and Emily Black", "abstract": "  Generative AI (GenAI) models present new challenges in regulating against\ndiscriminatory behavior. In this paper, we argue that GenAI fairness research\nstill has not met these challenges; instead, a significant gap remains between\nexisting bias assessment methods and regulatory goals. This leads to\nineffective regulation that can allow deployment of reportedly fair, yet\nactually discriminatory, GenAI systems. Towards remedying this problem, we\nconnect the legal and technical literature around GenAI bias evaluation and\nidentify areas of misalignment. Through four case studies, we demonstrate how\nthis misalignment between fairness testing techniques and regulatory goals can\nresult in discriminatory outcomes in real-world deployments, especially in\nadaptive or complex environments. We offer practical recommendations for\nimproving discrimination testing to better align with regulatory goals and\nenhance the reliability of fairness assessments in future deployments.\n", "link": "http://arxiv.org/abs/2412.21052v1", "date": "2024-12-30", "relevancy": 1.971, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4986}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4892}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Effective%20Discrimination%20Testing%20for%20Generative%20AI&body=Title%3A%20Towards%20Effective%20Discrimination%20Testing%20for%20Generative%20AI%0AAuthor%3A%20Thomas%20P.%20Zollo%20and%20Nikita%20Rajaneesh%20and%20Richard%20Zemel%20and%20Talia%20B.%20Gillis%20and%20Emily%20Black%0AAbstract%3A%20%20%20Generative%20AI%20%28GenAI%29%20models%20present%20new%20challenges%20in%20regulating%20against%0Adiscriminatory%20behavior.%20In%20this%20paper%2C%20we%20argue%20that%20GenAI%20fairness%20research%0Astill%20has%20not%20met%20these%20challenges%3B%20instead%2C%20a%20significant%20gap%20remains%20between%0Aexisting%20bias%20assessment%20methods%20and%20regulatory%20goals.%20This%20leads%20to%0Aineffective%20regulation%20that%20can%20allow%20deployment%20of%20reportedly%20fair%2C%20yet%0Aactually%20discriminatory%2C%20GenAI%20systems.%20Towards%20remedying%20this%20problem%2C%20we%0Aconnect%20the%20legal%20and%20technical%20literature%20around%20GenAI%20bias%20evaluation%20and%0Aidentify%20areas%20of%20misalignment.%20Through%20four%20case%20studies%2C%20we%20demonstrate%20how%0Athis%20misalignment%20between%20fairness%20testing%20techniques%20and%20regulatory%20goals%20can%0Aresult%20in%20discriminatory%20outcomes%20in%20real-world%20deployments%2C%20especially%20in%0Aadaptive%20or%20complex%20environments.%20We%20offer%20practical%20recommendations%20for%0Aimproving%20discrimination%20testing%20to%20better%20align%20with%20regulatory%20goals%20and%0Aenhance%20the%20reliability%20of%20fairness%20assessments%20in%20future%20deployments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21052v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Effective%2520Discrimination%2520Testing%2520for%2520Generative%2520AI%26entry.906535625%3DThomas%2520P.%2520Zollo%2520and%2520Nikita%2520Rajaneesh%2520and%2520Richard%2520Zemel%2520and%2520Talia%2520B.%2520Gillis%2520and%2520Emily%2520Black%26entry.1292438233%3D%2520%2520Generative%2520AI%2520%2528GenAI%2529%2520models%2520present%2520new%2520challenges%2520in%2520regulating%2520against%250Adiscriminatory%2520behavior.%2520In%2520this%2520paper%252C%2520we%2520argue%2520that%2520GenAI%2520fairness%2520research%250Astill%2520has%2520not%2520met%2520these%2520challenges%253B%2520instead%252C%2520a%2520significant%2520gap%2520remains%2520between%250Aexisting%2520bias%2520assessment%2520methods%2520and%2520regulatory%2520goals.%2520This%2520leads%2520to%250Aineffective%2520regulation%2520that%2520can%2520allow%2520deployment%2520of%2520reportedly%2520fair%252C%2520yet%250Aactually%2520discriminatory%252C%2520GenAI%2520systems.%2520Towards%2520remedying%2520this%2520problem%252C%2520we%250Aconnect%2520the%2520legal%2520and%2520technical%2520literature%2520around%2520GenAI%2520bias%2520evaluation%2520and%250Aidentify%2520areas%2520of%2520misalignment.%2520Through%2520four%2520case%2520studies%252C%2520we%2520demonstrate%2520how%250Athis%2520misalignment%2520between%2520fairness%2520testing%2520techniques%2520and%2520regulatory%2520goals%2520can%250Aresult%2520in%2520discriminatory%2520outcomes%2520in%2520real-world%2520deployments%252C%2520especially%2520in%250Aadaptive%2520or%2520complex%2520environments.%2520We%2520offer%2520practical%2520recommendations%2520for%250Aimproving%2520discrimination%2520testing%2520to%2520better%2520align%2520with%2520regulatory%2520goals%2520and%250Aenhance%2520the%2520reliability%2520of%2520fairness%2520assessments%2520in%2520future%2520deployments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21052v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Effective%20Discrimination%20Testing%20for%20Generative%20AI&entry.906535625=Thomas%20P.%20Zollo%20and%20Nikita%20Rajaneesh%20and%20Richard%20Zemel%20and%20Talia%20B.%20Gillis%20and%20Emily%20Black&entry.1292438233=%20%20Generative%20AI%20%28GenAI%29%20models%20present%20new%20challenges%20in%20regulating%20against%0Adiscriminatory%20behavior.%20In%20this%20paper%2C%20we%20argue%20that%20GenAI%20fairness%20research%0Astill%20has%20not%20met%20these%20challenges%3B%20instead%2C%20a%20significant%20gap%20remains%20between%0Aexisting%20bias%20assessment%20methods%20and%20regulatory%20goals.%20This%20leads%20to%0Aineffective%20regulation%20that%20can%20allow%20deployment%20of%20reportedly%20fair%2C%20yet%0Aactually%20discriminatory%2C%20GenAI%20systems.%20Towards%20remedying%20this%20problem%2C%20we%0Aconnect%20the%20legal%20and%20technical%20literature%20around%20GenAI%20bias%20evaluation%20and%0Aidentify%20areas%20of%20misalignment.%20Through%20four%20case%20studies%2C%20we%20demonstrate%20how%0Athis%20misalignment%20between%20fairness%20testing%20techniques%20and%20regulatory%20goals%20can%0Aresult%20in%20discriminatory%20outcomes%20in%20real-world%20deployments%2C%20especially%20in%0Aadaptive%20or%20complex%20environments.%20We%20offer%20practical%20recommendations%20for%0Aimproving%20discrimination%20testing%20to%20better%20align%20with%20regulatory%20goals%20and%0Aenhance%20the%20reliability%20of%20fairness%20assessments%20in%20future%20deployments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21052v1&entry.124074799=Read"},
{"title": "FedSat: A Statistical Aggregation Approach for Class Imbalanced Clients\n  in Federated Learning", "author": "Sujit Chowdhury and Raju Halder", "abstract": "  Federated learning (FL) has emerged as a promising paradigm for\nprivacy-preserving distributed machine learning, but faces challenges with\nheterogeneous data distributions across clients. This paper presents FedSat, a\nnovel FL approach specifically designed to simultaneously handle three forms of\ndata heterogeneity, namely label skewness, missing classes, and quantity\nskewness, by proposing a prediction-sensitive loss function and a\nprioritized-class based weighted aggregation scheme. While the\nprediction-sensitive loss function enhances model performance on minority\nclasses, the prioritized-class based weighted aggregation scheme ensures client\ncontributions are weighted based on both statistical significance and\nperformance on critical classes. Extensive experiments across diverse\ndata-heterogeneity settings demonstrate that FedSat significantly outperforms\nstate-of-the-art baselines, with an average improvement of 1.8% over the\nsecond-best method and 19.87% over the weakest-performing baseline. The\napproach also demonstrates faster convergence compared to existing methods.\nThese results highlight FedSat's effectiveness in addressing the challenges of\nheterogeneous federated learning and its potential for real-world applications.\n", "link": "http://arxiv.org/abs/2407.03862v2", "date": "2024-12-30", "relevancy": 1.9602, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5317}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.461}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedSat%3A%20A%20Statistical%20Aggregation%20Approach%20for%20Class%20Imbalanced%20Clients%0A%20%20in%20Federated%20Learning&body=Title%3A%20FedSat%3A%20A%20Statistical%20Aggregation%20Approach%20for%20Class%20Imbalanced%20Clients%0A%20%20in%20Federated%20Learning%0AAuthor%3A%20Sujit%20Chowdhury%20and%20Raju%20Halder%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20has%20emerged%20as%20a%20promising%20paradigm%20for%0Aprivacy-preserving%20distributed%20machine%20learning%2C%20but%20faces%20challenges%20with%0Aheterogeneous%20data%20distributions%20across%20clients.%20This%20paper%20presents%20FedSat%2C%20a%0Anovel%20FL%20approach%20specifically%20designed%20to%20simultaneously%20handle%20three%20forms%20of%0Adata%20heterogeneity%2C%20namely%20label%20skewness%2C%20missing%20classes%2C%20and%20quantity%0Askewness%2C%20by%20proposing%20a%20prediction-sensitive%20loss%20function%20and%20a%0Aprioritized-class%20based%20weighted%20aggregation%20scheme.%20While%20the%0Aprediction-sensitive%20loss%20function%20enhances%20model%20performance%20on%20minority%0Aclasses%2C%20the%20prioritized-class%20based%20weighted%20aggregation%20scheme%20ensures%20client%0Acontributions%20are%20weighted%20based%20on%20both%20statistical%20significance%20and%0Aperformance%20on%20critical%20classes.%20Extensive%20experiments%20across%20diverse%0Adata-heterogeneity%20settings%20demonstrate%20that%20FedSat%20significantly%20outperforms%0Astate-of-the-art%20baselines%2C%20with%20an%20average%20improvement%20of%201.8%25%20over%20the%0Asecond-best%20method%20and%2019.87%25%20over%20the%20weakest-performing%20baseline.%20The%0Aapproach%20also%20demonstrates%20faster%20convergence%20compared%20to%20existing%20methods.%0AThese%20results%20highlight%20FedSat%27s%20effectiveness%20in%20addressing%20the%20challenges%20of%0Aheterogeneous%20federated%20learning%20and%20its%20potential%20for%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03862v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedSat%253A%2520A%2520Statistical%2520Aggregation%2520Approach%2520for%2520Class%2520Imbalanced%2520Clients%250A%2520%2520in%2520Federated%2520Learning%26entry.906535625%3DSujit%2520Chowdhury%2520and%2520Raju%2520Halder%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520has%2520emerged%2520as%2520a%2520promising%2520paradigm%2520for%250Aprivacy-preserving%2520distributed%2520machine%2520learning%252C%2520but%2520faces%2520challenges%2520with%250Aheterogeneous%2520data%2520distributions%2520across%2520clients.%2520This%2520paper%2520presents%2520FedSat%252C%2520a%250Anovel%2520FL%2520approach%2520specifically%2520designed%2520to%2520simultaneously%2520handle%2520three%2520forms%2520of%250Adata%2520heterogeneity%252C%2520namely%2520label%2520skewness%252C%2520missing%2520classes%252C%2520and%2520quantity%250Askewness%252C%2520by%2520proposing%2520a%2520prediction-sensitive%2520loss%2520function%2520and%2520a%250Aprioritized-class%2520based%2520weighted%2520aggregation%2520scheme.%2520While%2520the%250Aprediction-sensitive%2520loss%2520function%2520enhances%2520model%2520performance%2520on%2520minority%250Aclasses%252C%2520the%2520prioritized-class%2520based%2520weighted%2520aggregation%2520scheme%2520ensures%2520client%250Acontributions%2520are%2520weighted%2520based%2520on%2520both%2520statistical%2520significance%2520and%250Aperformance%2520on%2520critical%2520classes.%2520Extensive%2520experiments%2520across%2520diverse%250Adata-heterogeneity%2520settings%2520demonstrate%2520that%2520FedSat%2520significantly%2520outperforms%250Astate-of-the-art%2520baselines%252C%2520with%2520an%2520average%2520improvement%2520of%25201.8%2525%2520over%2520the%250Asecond-best%2520method%2520and%252019.87%2525%2520over%2520the%2520weakest-performing%2520baseline.%2520The%250Aapproach%2520also%2520demonstrates%2520faster%2520convergence%2520compared%2520to%2520existing%2520methods.%250AThese%2520results%2520highlight%2520FedSat%2527s%2520effectiveness%2520in%2520addressing%2520the%2520challenges%2520of%250Aheterogeneous%2520federated%2520learning%2520and%2520its%2520potential%2520for%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03862v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedSat%3A%20A%20Statistical%20Aggregation%20Approach%20for%20Class%20Imbalanced%20Clients%0A%20%20in%20Federated%20Learning&entry.906535625=Sujit%20Chowdhury%20and%20Raju%20Halder&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20has%20emerged%20as%20a%20promising%20paradigm%20for%0Aprivacy-preserving%20distributed%20machine%20learning%2C%20but%20faces%20challenges%20with%0Aheterogeneous%20data%20distributions%20across%20clients.%20This%20paper%20presents%20FedSat%2C%20a%0Anovel%20FL%20approach%20specifically%20designed%20to%20simultaneously%20handle%20three%20forms%20of%0Adata%20heterogeneity%2C%20namely%20label%20skewness%2C%20missing%20classes%2C%20and%20quantity%0Askewness%2C%20by%20proposing%20a%20prediction-sensitive%20loss%20function%20and%20a%0Aprioritized-class%20based%20weighted%20aggregation%20scheme.%20While%20the%0Aprediction-sensitive%20loss%20function%20enhances%20model%20performance%20on%20minority%0Aclasses%2C%20the%20prioritized-class%20based%20weighted%20aggregation%20scheme%20ensures%20client%0Acontributions%20are%20weighted%20based%20on%20both%20statistical%20significance%20and%0Aperformance%20on%20critical%20classes.%20Extensive%20experiments%20across%20diverse%0Adata-heterogeneity%20settings%20demonstrate%20that%20FedSat%20significantly%20outperforms%0Astate-of-the-art%20baselines%2C%20with%20an%20average%20improvement%20of%201.8%25%20over%20the%0Asecond-best%20method%20and%2019.87%25%20over%20the%20weakest-performing%20baseline.%20The%0Aapproach%20also%20demonstrates%20faster%20convergence%20compared%20to%20existing%20methods.%0AThese%20results%20highlight%20FedSat%27s%20effectiveness%20in%20addressing%20the%20challenges%20of%0Aheterogeneous%20federated%20learning%20and%20its%20potential%20for%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03862v2&entry.124074799=Read"},
{"title": "Adaptive Batch Size Schedules for Distributed Training of Language\n  Models with Data and Model Parallelism", "author": "Tim Tsz-Kit Lau and Weijian Li and Chenwei Xu and Han Liu and Mladen Kolar", "abstract": "  An appropriate choice of batch sizes in large-scale model training is\ncrucial, yet it involves an intrinsic yet inevitable dilemma: large-batch\ntraining improves training efficiency in terms of memory utilization, while\ngeneralization performance often deteriorates due to small amounts of gradient\nnoise. Despite this dilemma, the common practice of choosing batch sizes in\nlanguage model training often prioritizes training efficiency -- employing\neither constant large sizes with data parallelism or implementing batch size\nwarmup schedules. However, such batch size schedule designs remain heuristic\nand often fail to adapt to training dynamics, presenting the challenge of\ndesigning adaptive batch size schedules. Given the abundance of available\ndatasets and the data-hungry nature of language models, data parallelism has\nbecome an indispensable distributed training paradigm, enabling the use of\nlarger batch sizes for gradient computation. However, vanilla data parallelism\nrequires replicas of model parameters, gradients, and optimizer states at each\nworker, which prohibits training larger models with billions of parameters. To\noptimize memory usage, more advanced parallelism strategies must be employed.\nIn this work, we propose general-purpose and theoretically principled adaptive\nbatch size schedules compatible with data parallelism and model parallelism. We\ndevelop a practical implementation with PyTorch Fully Sharded Data Parallel,\nfacilitating the pretraining of language models of different sizes. We\nempirically demonstrate that our proposed approaches outperform constant batch\nsizes and heuristic batch size warmup schedules in the pretraining of models in\nthe Llama family, with particular focus on smaller models with up to 3 billion\nparameters. We also establish theoretical convergence guarantees for such\nadaptive batch size schedules with Adam for general smooth nonconvex\nobjectives.\n", "link": "http://arxiv.org/abs/2412.21124v1", "date": "2024-12-30", "relevancy": 1.9527, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4976}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4927}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Batch%20Size%20Schedules%20for%20Distributed%20Training%20of%20Language%0A%20%20Models%20with%20Data%20and%20Model%20Parallelism&body=Title%3A%20Adaptive%20Batch%20Size%20Schedules%20for%20Distributed%20Training%20of%20Language%0A%20%20Models%20with%20Data%20and%20Model%20Parallelism%0AAuthor%3A%20Tim%20Tsz-Kit%20Lau%20and%20Weijian%20Li%20and%20Chenwei%20Xu%20and%20Han%20Liu%20and%20Mladen%20Kolar%0AAbstract%3A%20%20%20An%20appropriate%20choice%20of%20batch%20sizes%20in%20large-scale%20model%20training%20is%0Acrucial%2C%20yet%20it%20involves%20an%20intrinsic%20yet%20inevitable%20dilemma%3A%20large-batch%0Atraining%20improves%20training%20efficiency%20in%20terms%20of%20memory%20utilization%2C%20while%0Ageneralization%20performance%20often%20deteriorates%20due%20to%20small%20amounts%20of%20gradient%0Anoise.%20Despite%20this%20dilemma%2C%20the%20common%20practice%20of%20choosing%20batch%20sizes%20in%0Alanguage%20model%20training%20often%20prioritizes%20training%20efficiency%20--%20employing%0Aeither%20constant%20large%20sizes%20with%20data%20parallelism%20or%20implementing%20batch%20size%0Awarmup%20schedules.%20However%2C%20such%20batch%20size%20schedule%20designs%20remain%20heuristic%0Aand%20often%20fail%20to%20adapt%20to%20training%20dynamics%2C%20presenting%20the%20challenge%20of%0Adesigning%20adaptive%20batch%20size%20schedules.%20Given%20the%20abundance%20of%20available%0Adatasets%20and%20the%20data-hungry%20nature%20of%20language%20models%2C%20data%20parallelism%20has%0Abecome%20an%20indispensable%20distributed%20training%20paradigm%2C%20enabling%20the%20use%20of%0Alarger%20batch%20sizes%20for%20gradient%20computation.%20However%2C%20vanilla%20data%20parallelism%0Arequires%20replicas%20of%20model%20parameters%2C%20gradients%2C%20and%20optimizer%20states%20at%20each%0Aworker%2C%20which%20prohibits%20training%20larger%20models%20with%20billions%20of%20parameters.%20To%0Aoptimize%20memory%20usage%2C%20more%20advanced%20parallelism%20strategies%20must%20be%20employed.%0AIn%20this%20work%2C%20we%20propose%20general-purpose%20and%20theoretically%20principled%20adaptive%0Abatch%20size%20schedules%20compatible%20with%20data%20parallelism%20and%20model%20parallelism.%20We%0Adevelop%20a%20practical%20implementation%20with%20PyTorch%20Fully%20Sharded%20Data%20Parallel%2C%0Afacilitating%20the%20pretraining%20of%20language%20models%20of%20different%20sizes.%20We%0Aempirically%20demonstrate%20that%20our%20proposed%20approaches%20outperform%20constant%20batch%0Asizes%20and%20heuristic%20batch%20size%20warmup%20schedules%20in%20the%20pretraining%20of%20models%20in%0Athe%20Llama%20family%2C%20with%20particular%20focus%20on%20smaller%20models%20with%20up%20to%203%20billion%0Aparameters.%20We%20also%20establish%20theoretical%20convergence%20guarantees%20for%20such%0Aadaptive%20batch%20size%20schedules%20with%20Adam%20for%20general%20smooth%20nonconvex%0Aobjectives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21124v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Batch%2520Size%2520Schedules%2520for%2520Distributed%2520Training%2520of%2520Language%250A%2520%2520Models%2520with%2520Data%2520and%2520Model%2520Parallelism%26entry.906535625%3DTim%2520Tsz-Kit%2520Lau%2520and%2520Weijian%2520Li%2520and%2520Chenwei%2520Xu%2520and%2520Han%2520Liu%2520and%2520Mladen%2520Kolar%26entry.1292438233%3D%2520%2520An%2520appropriate%2520choice%2520of%2520batch%2520sizes%2520in%2520large-scale%2520model%2520training%2520is%250Acrucial%252C%2520yet%2520it%2520involves%2520an%2520intrinsic%2520yet%2520inevitable%2520dilemma%253A%2520large-batch%250Atraining%2520improves%2520training%2520efficiency%2520in%2520terms%2520of%2520memory%2520utilization%252C%2520while%250Ageneralization%2520performance%2520often%2520deteriorates%2520due%2520to%2520small%2520amounts%2520of%2520gradient%250Anoise.%2520Despite%2520this%2520dilemma%252C%2520the%2520common%2520practice%2520of%2520choosing%2520batch%2520sizes%2520in%250Alanguage%2520model%2520training%2520often%2520prioritizes%2520training%2520efficiency%2520--%2520employing%250Aeither%2520constant%2520large%2520sizes%2520with%2520data%2520parallelism%2520or%2520implementing%2520batch%2520size%250Awarmup%2520schedules.%2520However%252C%2520such%2520batch%2520size%2520schedule%2520designs%2520remain%2520heuristic%250Aand%2520often%2520fail%2520to%2520adapt%2520to%2520training%2520dynamics%252C%2520presenting%2520the%2520challenge%2520of%250Adesigning%2520adaptive%2520batch%2520size%2520schedules.%2520Given%2520the%2520abundance%2520of%2520available%250Adatasets%2520and%2520the%2520data-hungry%2520nature%2520of%2520language%2520models%252C%2520data%2520parallelism%2520has%250Abecome%2520an%2520indispensable%2520distributed%2520training%2520paradigm%252C%2520enabling%2520the%2520use%2520of%250Alarger%2520batch%2520sizes%2520for%2520gradient%2520computation.%2520However%252C%2520vanilla%2520data%2520parallelism%250Arequires%2520replicas%2520of%2520model%2520parameters%252C%2520gradients%252C%2520and%2520optimizer%2520states%2520at%2520each%250Aworker%252C%2520which%2520prohibits%2520training%2520larger%2520models%2520with%2520billions%2520of%2520parameters.%2520To%250Aoptimize%2520memory%2520usage%252C%2520more%2520advanced%2520parallelism%2520strategies%2520must%2520be%2520employed.%250AIn%2520this%2520work%252C%2520we%2520propose%2520general-purpose%2520and%2520theoretically%2520principled%2520adaptive%250Abatch%2520size%2520schedules%2520compatible%2520with%2520data%2520parallelism%2520and%2520model%2520parallelism.%2520We%250Adevelop%2520a%2520practical%2520implementation%2520with%2520PyTorch%2520Fully%2520Sharded%2520Data%2520Parallel%252C%250Afacilitating%2520the%2520pretraining%2520of%2520language%2520models%2520of%2520different%2520sizes.%2520We%250Aempirically%2520demonstrate%2520that%2520our%2520proposed%2520approaches%2520outperform%2520constant%2520batch%250Asizes%2520and%2520heuristic%2520batch%2520size%2520warmup%2520schedules%2520in%2520the%2520pretraining%2520of%2520models%2520in%250Athe%2520Llama%2520family%252C%2520with%2520particular%2520focus%2520on%2520smaller%2520models%2520with%2520up%2520to%25203%2520billion%250Aparameters.%2520We%2520also%2520establish%2520theoretical%2520convergence%2520guarantees%2520for%2520such%250Aadaptive%2520batch%2520size%2520schedules%2520with%2520Adam%2520for%2520general%2520smooth%2520nonconvex%250Aobjectives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21124v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Batch%20Size%20Schedules%20for%20Distributed%20Training%20of%20Language%0A%20%20Models%20with%20Data%20and%20Model%20Parallelism&entry.906535625=Tim%20Tsz-Kit%20Lau%20and%20Weijian%20Li%20and%20Chenwei%20Xu%20and%20Han%20Liu%20and%20Mladen%20Kolar&entry.1292438233=%20%20An%20appropriate%20choice%20of%20batch%20sizes%20in%20large-scale%20model%20training%20is%0Acrucial%2C%20yet%20it%20involves%20an%20intrinsic%20yet%20inevitable%20dilemma%3A%20large-batch%0Atraining%20improves%20training%20efficiency%20in%20terms%20of%20memory%20utilization%2C%20while%0Ageneralization%20performance%20often%20deteriorates%20due%20to%20small%20amounts%20of%20gradient%0Anoise.%20Despite%20this%20dilemma%2C%20the%20common%20practice%20of%20choosing%20batch%20sizes%20in%0Alanguage%20model%20training%20often%20prioritizes%20training%20efficiency%20--%20employing%0Aeither%20constant%20large%20sizes%20with%20data%20parallelism%20or%20implementing%20batch%20size%0Awarmup%20schedules.%20However%2C%20such%20batch%20size%20schedule%20designs%20remain%20heuristic%0Aand%20often%20fail%20to%20adapt%20to%20training%20dynamics%2C%20presenting%20the%20challenge%20of%0Adesigning%20adaptive%20batch%20size%20schedules.%20Given%20the%20abundance%20of%20available%0Adatasets%20and%20the%20data-hungry%20nature%20of%20language%20models%2C%20data%20parallelism%20has%0Abecome%20an%20indispensable%20distributed%20training%20paradigm%2C%20enabling%20the%20use%20of%0Alarger%20batch%20sizes%20for%20gradient%20computation.%20However%2C%20vanilla%20data%20parallelism%0Arequires%20replicas%20of%20model%20parameters%2C%20gradients%2C%20and%20optimizer%20states%20at%20each%0Aworker%2C%20which%20prohibits%20training%20larger%20models%20with%20billions%20of%20parameters.%20To%0Aoptimize%20memory%20usage%2C%20more%20advanced%20parallelism%20strategies%20must%20be%20employed.%0AIn%20this%20work%2C%20we%20propose%20general-purpose%20and%20theoretically%20principled%20adaptive%0Abatch%20size%20schedules%20compatible%20with%20data%20parallelism%20and%20model%20parallelism.%20We%0Adevelop%20a%20practical%20implementation%20with%20PyTorch%20Fully%20Sharded%20Data%20Parallel%2C%0Afacilitating%20the%20pretraining%20of%20language%20models%20of%20different%20sizes.%20We%0Aempirically%20demonstrate%20that%20our%20proposed%20approaches%20outperform%20constant%20batch%0Asizes%20and%20heuristic%20batch%20size%20warmup%20schedules%20in%20the%20pretraining%20of%20models%20in%0Athe%20Llama%20family%2C%20with%20particular%20focus%20on%20smaller%20models%20with%20up%20to%203%20billion%0Aparameters.%20We%20also%20establish%20theoretical%20convergence%20guarantees%20for%20such%0Aadaptive%20batch%20size%20schedules%20with%20Adam%20for%20general%20smooth%20nonconvex%0Aobjectives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21124v1&entry.124074799=Read"},
{"title": "Text Classification: Neural Networks VS Machine Learning Models VS\n  Pre-trained Models", "author": "Christos Petridis", "abstract": "  Text classification is a very common task nowadays and there are many\nefficient methods and algorithms that we can employ to accomplish it.\nTransformers have revolutionized the field of deep learning, particularly in\nNatural Language Processing (NLP) and have rapidly expanded to other domains\nsuch as computer vision, time-series analysis and more. The transformer model\nwas firstly introduced in the context of machine translation and its\narchitecture relies on self-attention mechanisms to capture complex\nrelationships within data sequences. It is able to handle long-range\ndependencies more effectively than traditional neural networks (such as\nRecurrent Neural Networks and Multilayer Perceptrons). In this work, we present\na comparison between different techniques to perform text classification. We\ntake into consideration seven pre-trained models, three standard neural\nnetworks and three machine learning models. For standard neural networks and\nmachine learning models we also compare two embedding techniques: TF-IDF and\nGloVe, with the latter consistently outperforming the former. Finally, we\ndemonstrate the results from our experiments where pre-trained models such as\nBERT and DistilBERT always perform better than standard models/algorithms.\n", "link": "http://arxiv.org/abs/2412.21022v1", "date": "2024-12-30", "relevancy": 1.9318, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5283}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4739}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text%20Classification%3A%20Neural%20Networks%20VS%20Machine%20Learning%20Models%20VS%0A%20%20Pre-trained%20Models&body=Title%3A%20Text%20Classification%3A%20Neural%20Networks%20VS%20Machine%20Learning%20Models%20VS%0A%20%20Pre-trained%20Models%0AAuthor%3A%20Christos%20Petridis%0AAbstract%3A%20%20%20Text%20classification%20is%20a%20very%20common%20task%20nowadays%20and%20there%20are%20many%0Aefficient%20methods%20and%20algorithms%20that%20we%20can%20employ%20to%20accomplish%20it.%0ATransformers%20have%20revolutionized%20the%20field%20of%20deep%20learning%2C%20particularly%20in%0ANatural%20Language%20Processing%20%28NLP%29%20and%20have%20rapidly%20expanded%20to%20other%20domains%0Asuch%20as%20computer%20vision%2C%20time-series%20analysis%20and%20more.%20The%20transformer%20model%0Awas%20firstly%20introduced%20in%20the%20context%20of%20machine%20translation%20and%20its%0Aarchitecture%20relies%20on%20self-attention%20mechanisms%20to%20capture%20complex%0Arelationships%20within%20data%20sequences.%20It%20is%20able%20to%20handle%20long-range%0Adependencies%20more%20effectively%20than%20traditional%20neural%20networks%20%28such%20as%0ARecurrent%20Neural%20Networks%20and%20Multilayer%20Perceptrons%29.%20In%20this%20work%2C%20we%20present%0Aa%20comparison%20between%20different%20techniques%20to%20perform%20text%20classification.%20We%0Atake%20into%20consideration%20seven%20pre-trained%20models%2C%20three%20standard%20neural%0Anetworks%20and%20three%20machine%20learning%20models.%20For%20standard%20neural%20networks%20and%0Amachine%20learning%20models%20we%20also%20compare%20two%20embedding%20techniques%3A%20TF-IDF%20and%0AGloVe%2C%20with%20the%20latter%20consistently%20outperforming%20the%20former.%20Finally%2C%20we%0Ademonstrate%20the%20results%20from%20our%20experiments%20where%20pre-trained%20models%20such%20as%0ABERT%20and%20DistilBERT%20always%20perform%20better%20than%20standard%20models/algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21022v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText%2520Classification%253A%2520Neural%2520Networks%2520VS%2520Machine%2520Learning%2520Models%2520VS%250A%2520%2520Pre-trained%2520Models%26entry.906535625%3DChristos%2520Petridis%26entry.1292438233%3D%2520%2520Text%2520classification%2520is%2520a%2520very%2520common%2520task%2520nowadays%2520and%2520there%2520are%2520many%250Aefficient%2520methods%2520and%2520algorithms%2520that%2520we%2520can%2520employ%2520to%2520accomplish%2520it.%250ATransformers%2520have%2520revolutionized%2520the%2520field%2520of%2520deep%2520learning%252C%2520particularly%2520in%250ANatural%2520Language%2520Processing%2520%2528NLP%2529%2520and%2520have%2520rapidly%2520expanded%2520to%2520other%2520domains%250Asuch%2520as%2520computer%2520vision%252C%2520time-series%2520analysis%2520and%2520more.%2520The%2520transformer%2520model%250Awas%2520firstly%2520introduced%2520in%2520the%2520context%2520of%2520machine%2520translation%2520and%2520its%250Aarchitecture%2520relies%2520on%2520self-attention%2520mechanisms%2520to%2520capture%2520complex%250Arelationships%2520within%2520data%2520sequences.%2520It%2520is%2520able%2520to%2520handle%2520long-range%250Adependencies%2520more%2520effectively%2520than%2520traditional%2520neural%2520networks%2520%2528such%2520as%250ARecurrent%2520Neural%2520Networks%2520and%2520Multilayer%2520Perceptrons%2529.%2520In%2520this%2520work%252C%2520we%2520present%250Aa%2520comparison%2520between%2520different%2520techniques%2520to%2520perform%2520text%2520classification.%2520We%250Atake%2520into%2520consideration%2520seven%2520pre-trained%2520models%252C%2520three%2520standard%2520neural%250Anetworks%2520and%2520three%2520machine%2520learning%2520models.%2520For%2520standard%2520neural%2520networks%2520and%250Amachine%2520learning%2520models%2520we%2520also%2520compare%2520two%2520embedding%2520techniques%253A%2520TF-IDF%2520and%250AGloVe%252C%2520with%2520the%2520latter%2520consistently%2520outperforming%2520the%2520former.%2520Finally%252C%2520we%250Ademonstrate%2520the%2520results%2520from%2520our%2520experiments%2520where%2520pre-trained%2520models%2520such%2520as%250ABERT%2520and%2520DistilBERT%2520always%2520perform%2520better%2520than%2520standard%2520models/algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21022v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text%20Classification%3A%20Neural%20Networks%20VS%20Machine%20Learning%20Models%20VS%0A%20%20Pre-trained%20Models&entry.906535625=Christos%20Petridis&entry.1292438233=%20%20Text%20classification%20is%20a%20very%20common%20task%20nowadays%20and%20there%20are%20many%0Aefficient%20methods%20and%20algorithms%20that%20we%20can%20employ%20to%20accomplish%20it.%0ATransformers%20have%20revolutionized%20the%20field%20of%20deep%20learning%2C%20particularly%20in%0ANatural%20Language%20Processing%20%28NLP%29%20and%20have%20rapidly%20expanded%20to%20other%20domains%0Asuch%20as%20computer%20vision%2C%20time-series%20analysis%20and%20more.%20The%20transformer%20model%0Awas%20firstly%20introduced%20in%20the%20context%20of%20machine%20translation%20and%20its%0Aarchitecture%20relies%20on%20self-attention%20mechanisms%20to%20capture%20complex%0Arelationships%20within%20data%20sequences.%20It%20is%20able%20to%20handle%20long-range%0Adependencies%20more%20effectively%20than%20traditional%20neural%20networks%20%28such%20as%0ARecurrent%20Neural%20Networks%20and%20Multilayer%20Perceptrons%29.%20In%20this%20work%2C%20we%20present%0Aa%20comparison%20between%20different%20techniques%20to%20perform%20text%20classification.%20We%0Atake%20into%20consideration%20seven%20pre-trained%20models%2C%20three%20standard%20neural%0Anetworks%20and%20three%20machine%20learning%20models.%20For%20standard%20neural%20networks%20and%0Amachine%20learning%20models%20we%20also%20compare%20two%20embedding%20techniques%3A%20TF-IDF%20and%0AGloVe%2C%20with%20the%20latter%20consistently%20outperforming%20the%20former.%20Finally%2C%20we%0Ademonstrate%20the%20results%20from%20our%20experiments%20where%20pre-trained%20models%20such%20as%0ABERT%20and%20DistilBERT%20always%20perform%20better%20than%20standard%20models/algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21022v1&entry.124074799=Read"},
{"title": "Causal-aware Graph Neural Architecture Search under Distribution Shifts", "author": "Peiwen Li and Xin Wang and Zeyang Zhang and Yijian Qin and Ziwei Zhang and Jialong Wang and Yang Li and Wenwu Zhu", "abstract": "  Graph NAS has emerged as a promising approach for autonomously designing GNN\narchitectures by leveraging the correlations between graphs and architectures.\nExisting methods fail to generalize under distribution shifts that are\nubiquitous in real-world graph scenarios, mainly because the graph-architecture\ncorrelations they exploit might be spurious and varying across distributions.\nWe propose to handle the distribution shifts in the graph architecture search\nprocess by discovering and exploiting the causal relationship between graphs\nand architectures to search for the optimal architectures that can generalize\nunder distribution shifts. The problem remains unexplored with following\nchallenges: how to discover the causal graph-architecture relationship that has\nstable predictive abilities across distributions, and how to handle\ndistribution shifts with the discovered causal graph-architecture relationship\nto search the generalized graph architectures. To address these challenges, we\npropose Causal-aware Graph Neural Architecture Search (CARNAS), which is able\nto capture the causal graph-architecture relationship during the architecture\nsearch process and discover the generalized graph architecture under\ndistribution shifts. Specifically, we propose Disentangled Causal Subgraph\nIdentification to capture the causal subgraphs that have stable prediction\nabilities across distributions. Then, we propose Graph Embedding Intervention\nto intervene on causal subgraphs within the latent space, ensuring that these\nsubgraphs encapsulate essential features for prediction while excluding\nnon-causal elements. Additionally, we propose Invariant Architecture\nCustomization to reinforce the causal invariant nature of the causal subgraphs,\nwhich are utilized to tailor generalized graph architectures. Extensive\nexperiments demonstrate that CARNAS achieves advanced out-of-distribution\ngeneralization ability.\n", "link": "http://arxiv.org/abs/2405.16489v2", "date": "2024-12-30", "relevancy": 1.9298, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.496}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4831}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4468}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal-aware%20Graph%20Neural%20Architecture%20Search%20under%20Distribution%20Shifts&body=Title%3A%20Causal-aware%20Graph%20Neural%20Architecture%20Search%20under%20Distribution%20Shifts%0AAuthor%3A%20Peiwen%20Li%20and%20Xin%20Wang%20and%20Zeyang%20Zhang%20and%20Yijian%20Qin%20and%20Ziwei%20Zhang%20and%20Jialong%20Wang%20and%20Yang%20Li%20and%20Wenwu%20Zhu%0AAbstract%3A%20%20%20Graph%20NAS%20has%20emerged%20as%20a%20promising%20approach%20for%20autonomously%20designing%20GNN%0Aarchitectures%20by%20leveraging%20the%20correlations%20between%20graphs%20and%20architectures.%0AExisting%20methods%20fail%20to%20generalize%20under%20distribution%20shifts%20that%20are%0Aubiquitous%20in%20real-world%20graph%20scenarios%2C%20mainly%20because%20the%20graph-architecture%0Acorrelations%20they%20exploit%20might%20be%20spurious%20and%20varying%20across%20distributions.%0AWe%20propose%20to%20handle%20the%20distribution%20shifts%20in%20the%20graph%20architecture%20search%0Aprocess%20by%20discovering%20and%20exploiting%20the%20causal%20relationship%20between%20graphs%0Aand%20architectures%20to%20search%20for%20the%20optimal%20architectures%20that%20can%20generalize%0Aunder%20distribution%20shifts.%20The%20problem%20remains%20unexplored%20with%20following%0Achallenges%3A%20how%20to%20discover%20the%20causal%20graph-architecture%20relationship%20that%20has%0Astable%20predictive%20abilities%20across%20distributions%2C%20and%20how%20to%20handle%0Adistribution%20shifts%20with%20the%20discovered%20causal%20graph-architecture%20relationship%0Ato%20search%20the%20generalized%20graph%20architectures.%20To%20address%20these%20challenges%2C%20we%0Apropose%20Causal-aware%20Graph%20Neural%20Architecture%20Search%20%28CARNAS%29%2C%20which%20is%20able%0Ato%20capture%20the%20causal%20graph-architecture%20relationship%20during%20the%20architecture%0Asearch%20process%20and%20discover%20the%20generalized%20graph%20architecture%20under%0Adistribution%20shifts.%20Specifically%2C%20we%20propose%20Disentangled%20Causal%20Subgraph%0AIdentification%20to%20capture%20the%20causal%20subgraphs%20that%20have%20stable%20prediction%0Aabilities%20across%20distributions.%20Then%2C%20we%20propose%20Graph%20Embedding%20Intervention%0Ato%20intervene%20on%20causal%20subgraphs%20within%20the%20latent%20space%2C%20ensuring%20that%20these%0Asubgraphs%20encapsulate%20essential%20features%20for%20prediction%20while%20excluding%0Anon-causal%20elements.%20Additionally%2C%20we%20propose%20Invariant%20Architecture%0ACustomization%20to%20reinforce%20the%20causal%20invariant%20nature%20of%20the%20causal%20subgraphs%2C%0Awhich%20are%20utilized%20to%20tailor%20generalized%20graph%20architectures.%20Extensive%0Aexperiments%20demonstrate%20that%20CARNAS%20achieves%20advanced%20out-of-distribution%0Ageneralization%20ability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.16489v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal-aware%2520Graph%2520Neural%2520Architecture%2520Search%2520under%2520Distribution%2520Shifts%26entry.906535625%3DPeiwen%2520Li%2520and%2520Xin%2520Wang%2520and%2520Zeyang%2520Zhang%2520and%2520Yijian%2520Qin%2520and%2520Ziwei%2520Zhang%2520and%2520Jialong%2520Wang%2520and%2520Yang%2520Li%2520and%2520Wenwu%2520Zhu%26entry.1292438233%3D%2520%2520Graph%2520NAS%2520has%2520emerged%2520as%2520a%2520promising%2520approach%2520for%2520autonomously%2520designing%2520GNN%250Aarchitectures%2520by%2520leveraging%2520the%2520correlations%2520between%2520graphs%2520and%2520architectures.%250AExisting%2520methods%2520fail%2520to%2520generalize%2520under%2520distribution%2520shifts%2520that%2520are%250Aubiquitous%2520in%2520real-world%2520graph%2520scenarios%252C%2520mainly%2520because%2520the%2520graph-architecture%250Acorrelations%2520they%2520exploit%2520might%2520be%2520spurious%2520and%2520varying%2520across%2520distributions.%250AWe%2520propose%2520to%2520handle%2520the%2520distribution%2520shifts%2520in%2520the%2520graph%2520architecture%2520search%250Aprocess%2520by%2520discovering%2520and%2520exploiting%2520the%2520causal%2520relationship%2520between%2520graphs%250Aand%2520architectures%2520to%2520search%2520for%2520the%2520optimal%2520architectures%2520that%2520can%2520generalize%250Aunder%2520distribution%2520shifts.%2520The%2520problem%2520remains%2520unexplored%2520with%2520following%250Achallenges%253A%2520how%2520to%2520discover%2520the%2520causal%2520graph-architecture%2520relationship%2520that%2520has%250Astable%2520predictive%2520abilities%2520across%2520distributions%252C%2520and%2520how%2520to%2520handle%250Adistribution%2520shifts%2520with%2520the%2520discovered%2520causal%2520graph-architecture%2520relationship%250Ato%2520search%2520the%2520generalized%2520graph%2520architectures.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520Causal-aware%2520Graph%2520Neural%2520Architecture%2520Search%2520%2528CARNAS%2529%252C%2520which%2520is%2520able%250Ato%2520capture%2520the%2520causal%2520graph-architecture%2520relationship%2520during%2520the%2520architecture%250Asearch%2520process%2520and%2520discover%2520the%2520generalized%2520graph%2520architecture%2520under%250Adistribution%2520shifts.%2520Specifically%252C%2520we%2520propose%2520Disentangled%2520Causal%2520Subgraph%250AIdentification%2520to%2520capture%2520the%2520causal%2520subgraphs%2520that%2520have%2520stable%2520prediction%250Aabilities%2520across%2520distributions.%2520Then%252C%2520we%2520propose%2520Graph%2520Embedding%2520Intervention%250Ato%2520intervene%2520on%2520causal%2520subgraphs%2520within%2520the%2520latent%2520space%252C%2520ensuring%2520that%2520these%250Asubgraphs%2520encapsulate%2520essential%2520features%2520for%2520prediction%2520while%2520excluding%250Anon-causal%2520elements.%2520Additionally%252C%2520we%2520propose%2520Invariant%2520Architecture%250ACustomization%2520to%2520reinforce%2520the%2520causal%2520invariant%2520nature%2520of%2520the%2520causal%2520subgraphs%252C%250Awhich%2520are%2520utilized%2520to%2520tailor%2520generalized%2520graph%2520architectures.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520CARNAS%2520achieves%2520advanced%2520out-of-distribution%250Ageneralization%2520ability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.16489v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal-aware%20Graph%20Neural%20Architecture%20Search%20under%20Distribution%20Shifts&entry.906535625=Peiwen%20Li%20and%20Xin%20Wang%20and%20Zeyang%20Zhang%20and%20Yijian%20Qin%20and%20Ziwei%20Zhang%20and%20Jialong%20Wang%20and%20Yang%20Li%20and%20Wenwu%20Zhu&entry.1292438233=%20%20Graph%20NAS%20has%20emerged%20as%20a%20promising%20approach%20for%20autonomously%20designing%20GNN%0Aarchitectures%20by%20leveraging%20the%20correlations%20between%20graphs%20and%20architectures.%0AExisting%20methods%20fail%20to%20generalize%20under%20distribution%20shifts%20that%20are%0Aubiquitous%20in%20real-world%20graph%20scenarios%2C%20mainly%20because%20the%20graph-architecture%0Acorrelations%20they%20exploit%20might%20be%20spurious%20and%20varying%20across%20distributions.%0AWe%20propose%20to%20handle%20the%20distribution%20shifts%20in%20the%20graph%20architecture%20search%0Aprocess%20by%20discovering%20and%20exploiting%20the%20causal%20relationship%20between%20graphs%0Aand%20architectures%20to%20search%20for%20the%20optimal%20architectures%20that%20can%20generalize%0Aunder%20distribution%20shifts.%20The%20problem%20remains%20unexplored%20with%20following%0Achallenges%3A%20how%20to%20discover%20the%20causal%20graph-architecture%20relationship%20that%20has%0Astable%20predictive%20abilities%20across%20distributions%2C%20and%20how%20to%20handle%0Adistribution%20shifts%20with%20the%20discovered%20causal%20graph-architecture%20relationship%0Ato%20search%20the%20generalized%20graph%20architectures.%20To%20address%20these%20challenges%2C%20we%0Apropose%20Causal-aware%20Graph%20Neural%20Architecture%20Search%20%28CARNAS%29%2C%20which%20is%20able%0Ato%20capture%20the%20causal%20graph-architecture%20relationship%20during%20the%20architecture%0Asearch%20process%20and%20discover%20the%20generalized%20graph%20architecture%20under%0Adistribution%20shifts.%20Specifically%2C%20we%20propose%20Disentangled%20Causal%20Subgraph%0AIdentification%20to%20capture%20the%20causal%20subgraphs%20that%20have%20stable%20prediction%0Aabilities%20across%20distributions.%20Then%2C%20we%20propose%20Graph%20Embedding%20Intervention%0Ato%20intervene%20on%20causal%20subgraphs%20within%20the%20latent%20space%2C%20ensuring%20that%20these%0Asubgraphs%20encapsulate%20essential%20features%20for%20prediction%20while%20excluding%0Anon-causal%20elements.%20Additionally%2C%20we%20propose%20Invariant%20Architecture%0ACustomization%20to%20reinforce%20the%20causal%20invariant%20nature%20of%20the%20causal%20subgraphs%2C%0Awhich%20are%20utilized%20to%20tailor%20generalized%20graph%20architectures.%20Extensive%0Aexperiments%20demonstrate%20that%20CARNAS%20achieves%20advanced%20out-of-distribution%0Ageneralization%20ability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.16489v2&entry.124074799=Read"},
{"title": "Machine Learning Optimal Ordering in Global Routing Problems in\n  Semiconductors", "author": "Heejin Choi and Minji Lee and Chang Hyeong Lee and Jaeho Yang and Rak-Kyeong Seong", "abstract": "  In this work, we propose a new method for ordering nets during the process of\nlayer assignment in global routing problems. The global routing problems that\nwe focus on in this work are based on routing problems that occur in the design\nof substrates in multilayered semiconductor packages. The proposed new method\nis based on machine learning techniques and we show that the proposed method\nsupersedes conventional net ordering techniques based on heuristic score\nfunctions. We perform global routing experiments in multilayered semiconductor\npackage environments in order to illustrate that the routing order based on our\nnew proposed technique outperforms previous methods based on heuristics. Our\napproach of using machine learning for global routing targets specifically the\nnet ordering step which we show in this work can be significantly improved by\ndeep learning.\n", "link": "http://arxiv.org/abs/2412.21035v1", "date": "2024-12-30", "relevancy": 1.9282, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5255}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4732}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4421}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Machine%20Learning%20Optimal%20Ordering%20in%20Global%20Routing%20Problems%20in%0A%20%20Semiconductors&body=Title%3A%20Machine%20Learning%20Optimal%20Ordering%20in%20Global%20Routing%20Problems%20in%0A%20%20Semiconductors%0AAuthor%3A%20Heejin%20Choi%20and%20Minji%20Lee%20and%20Chang%20Hyeong%20Lee%20and%20Jaeho%20Yang%20and%20Rak-Kyeong%20Seong%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20a%20new%20method%20for%20ordering%20nets%20during%20the%20process%20of%0Alayer%20assignment%20in%20global%20routing%20problems.%20The%20global%20routing%20problems%20that%0Awe%20focus%20on%20in%20this%20work%20are%20based%20on%20routing%20problems%20that%20occur%20in%20the%20design%0Aof%20substrates%20in%20multilayered%20semiconductor%20packages.%20The%20proposed%20new%20method%0Ais%20based%20on%20machine%20learning%20techniques%20and%20we%20show%20that%20the%20proposed%20method%0Asupersedes%20conventional%20net%20ordering%20techniques%20based%20on%20heuristic%20score%0Afunctions.%20We%20perform%20global%20routing%20experiments%20in%20multilayered%20semiconductor%0Apackage%20environments%20in%20order%20to%20illustrate%20that%20the%20routing%20order%20based%20on%20our%0Anew%20proposed%20technique%20outperforms%20previous%20methods%20based%20on%20heuristics.%20Our%0Aapproach%20of%20using%20machine%20learning%20for%20global%20routing%20targets%20specifically%20the%0Anet%20ordering%20step%20which%20we%20show%20in%20this%20work%20can%20be%20significantly%20improved%20by%0Adeep%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21035v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachine%2520Learning%2520Optimal%2520Ordering%2520in%2520Global%2520Routing%2520Problems%2520in%250A%2520%2520Semiconductors%26entry.906535625%3DHeejin%2520Choi%2520and%2520Minji%2520Lee%2520and%2520Chang%2520Hyeong%2520Lee%2520and%2520Jaeho%2520Yang%2520and%2520Rak-Kyeong%2520Seong%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520new%2520method%2520for%2520ordering%2520nets%2520during%2520the%2520process%2520of%250Alayer%2520assignment%2520in%2520global%2520routing%2520problems.%2520The%2520global%2520routing%2520problems%2520that%250Awe%2520focus%2520on%2520in%2520this%2520work%2520are%2520based%2520on%2520routing%2520problems%2520that%2520occur%2520in%2520the%2520design%250Aof%2520substrates%2520in%2520multilayered%2520semiconductor%2520packages.%2520The%2520proposed%2520new%2520method%250Ais%2520based%2520on%2520machine%2520learning%2520techniques%2520and%2520we%2520show%2520that%2520the%2520proposed%2520method%250Asupersedes%2520conventional%2520net%2520ordering%2520techniques%2520based%2520on%2520heuristic%2520score%250Afunctions.%2520We%2520perform%2520global%2520routing%2520experiments%2520in%2520multilayered%2520semiconductor%250Apackage%2520environments%2520in%2520order%2520to%2520illustrate%2520that%2520the%2520routing%2520order%2520based%2520on%2520our%250Anew%2520proposed%2520technique%2520outperforms%2520previous%2520methods%2520based%2520on%2520heuristics.%2520Our%250Aapproach%2520of%2520using%2520machine%2520learning%2520for%2520global%2520routing%2520targets%2520specifically%2520the%250Anet%2520ordering%2520step%2520which%2520we%2520show%2520in%2520this%2520work%2520can%2520be%2520significantly%2520improved%2520by%250Adeep%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21035v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20Learning%20Optimal%20Ordering%20in%20Global%20Routing%20Problems%20in%0A%20%20Semiconductors&entry.906535625=Heejin%20Choi%20and%20Minji%20Lee%20and%20Chang%20Hyeong%20Lee%20and%20Jaeho%20Yang%20and%20Rak-Kyeong%20Seong&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20a%20new%20method%20for%20ordering%20nets%20during%20the%20process%20of%0Alayer%20assignment%20in%20global%20routing%20problems.%20The%20global%20routing%20problems%20that%0Awe%20focus%20on%20in%20this%20work%20are%20based%20on%20routing%20problems%20that%20occur%20in%20the%20design%0Aof%20substrates%20in%20multilayered%20semiconductor%20packages.%20The%20proposed%20new%20method%0Ais%20based%20on%20machine%20learning%20techniques%20and%20we%20show%20that%20the%20proposed%20method%0Asupersedes%20conventional%20net%20ordering%20techniques%20based%20on%20heuristic%20score%0Afunctions.%20We%20perform%20global%20routing%20experiments%20in%20multilayered%20semiconductor%0Apackage%20environments%20in%20order%20to%20illustrate%20that%20the%20routing%20order%20based%20on%20our%0Anew%20proposed%20technique%20outperforms%20previous%20methods%20based%20on%20heuristics.%20Our%0Aapproach%20of%20using%20machine%20learning%20for%20global%20routing%20targets%20specifically%20the%0Anet%20ordering%20step%20which%20we%20show%20in%20this%20work%20can%20be%20significantly%20improved%20by%0Adeep%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21035v1&entry.124074799=Read"},
{"title": "PepTune: De Novo Generation of Therapeutic Peptides with\n  Multi-Objective-Guided Discrete Diffusion", "author": "Sophia Tang and Yinuo Zhang and Pranam Chatterjee", "abstract": "  Peptide therapeutics, a major class of medicines, have achieved remarkable\nsuccess across diseases such as diabetes and cancer, with landmark examples\nsuch as GLP-1 receptor agonists revolutionizing the treatment of type-2\ndiabetes and obesity. Despite their success, designing peptides that satisfy\nmultiple conflicting objectives, such as target binding affinity, solubility,\nand membrane permeability, remains a major challenge. Classical drug\ndevelopment and structure-based design are ineffective for such tasks, as they\nfail to optimize global functional properties critical for therapeutic\nefficacy. Existing generative frameworks are largely limited to continuous\nspaces, unconditioned outputs, or single-objective guidance, making them\nunsuitable for discrete sequence optimization across multiple properties. To\naddress this, we present PepTune, a multi-objective discrete diffusion model\nfor the simultaneous generation and optimization of therapeutic peptide SMILES.\nBuilt on the Masked Discrete Language Model (MDLM) framework, PepTune ensures\nvalid peptide structures with state-dependent masking schedules and\npenalty-based objectives. To guide the diffusion process, we propose a Monte\nCarlo Tree Search (MCTS)-based strategy that balances exploration and\nexploitation to iteratively refine Pareto-optimal sequences. MCTS integrates\nclassifier-based rewards with search-tree expansion, overcoming gradient\nestimation challenges and data sparsity inherent to discrete spaces. Using\nPepTune, we generate diverse, chemically-modified peptides optimized for\nmultiple therapeutic properties, including target binding affinity, membrane\npermeability, solubility, hemolysis, and non-fouling characteristics on various\ndisease-relevant targets. In total, our results demonstrate that MCTS-guided\ndiscrete diffusion is a powerful and modular approach for multi-objective\nsequence design in discrete state spaces.\n", "link": "http://arxiv.org/abs/2412.17780v2", "date": "2024-12-30", "relevancy": 1.9269, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.496}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4875}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4652}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PepTune%3A%20De%20Novo%20Generation%20of%20Therapeutic%20Peptides%20with%0A%20%20Multi-Objective-Guided%20Discrete%20Diffusion&body=Title%3A%20PepTune%3A%20De%20Novo%20Generation%20of%20Therapeutic%20Peptides%20with%0A%20%20Multi-Objective-Guided%20Discrete%20Diffusion%0AAuthor%3A%20Sophia%20Tang%20and%20Yinuo%20Zhang%20and%20Pranam%20Chatterjee%0AAbstract%3A%20%20%20Peptide%20therapeutics%2C%20a%20major%20class%20of%20medicines%2C%20have%20achieved%20remarkable%0Asuccess%20across%20diseases%20such%20as%20diabetes%20and%20cancer%2C%20with%20landmark%20examples%0Asuch%20as%20GLP-1%20receptor%20agonists%20revolutionizing%20the%20treatment%20of%20type-2%0Adiabetes%20and%20obesity.%20Despite%20their%20success%2C%20designing%20peptides%20that%20satisfy%0Amultiple%20conflicting%20objectives%2C%20such%20as%20target%20binding%20affinity%2C%20solubility%2C%0Aand%20membrane%20permeability%2C%20remains%20a%20major%20challenge.%20Classical%20drug%0Adevelopment%20and%20structure-based%20design%20are%20ineffective%20for%20such%20tasks%2C%20as%20they%0Afail%20to%20optimize%20global%20functional%20properties%20critical%20for%20therapeutic%0Aefficacy.%20Existing%20generative%20frameworks%20are%20largely%20limited%20to%20continuous%0Aspaces%2C%20unconditioned%20outputs%2C%20or%20single-objective%20guidance%2C%20making%20them%0Aunsuitable%20for%20discrete%20sequence%20optimization%20across%20multiple%20properties.%20To%0Aaddress%20this%2C%20we%20present%20PepTune%2C%20a%20multi-objective%20discrete%20diffusion%20model%0Afor%20the%20simultaneous%20generation%20and%20optimization%20of%20therapeutic%20peptide%20SMILES.%0ABuilt%20on%20the%20Masked%20Discrete%20Language%20Model%20%28MDLM%29%20framework%2C%20PepTune%20ensures%0Avalid%20peptide%20structures%20with%20state-dependent%20masking%20schedules%20and%0Apenalty-based%20objectives.%20To%20guide%20the%20diffusion%20process%2C%20we%20propose%20a%20Monte%0ACarlo%20Tree%20Search%20%28MCTS%29-based%20strategy%20that%20balances%20exploration%20and%0Aexploitation%20to%20iteratively%20refine%20Pareto-optimal%20sequences.%20MCTS%20integrates%0Aclassifier-based%20rewards%20with%20search-tree%20expansion%2C%20overcoming%20gradient%0Aestimation%20challenges%20and%20data%20sparsity%20inherent%20to%20discrete%20spaces.%20Using%0APepTune%2C%20we%20generate%20diverse%2C%20chemically-modified%20peptides%20optimized%20for%0Amultiple%20therapeutic%20properties%2C%20including%20target%20binding%20affinity%2C%20membrane%0Apermeability%2C%20solubility%2C%20hemolysis%2C%20and%20non-fouling%20characteristics%20on%20various%0Adisease-relevant%20targets.%20In%20total%2C%20our%20results%20demonstrate%20that%20MCTS-guided%0Adiscrete%20diffusion%20is%20a%20powerful%20and%20modular%20approach%20for%20multi-objective%0Asequence%20design%20in%20discrete%20state%20spaces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17780v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPepTune%253A%2520De%2520Novo%2520Generation%2520of%2520Therapeutic%2520Peptides%2520with%250A%2520%2520Multi-Objective-Guided%2520Discrete%2520Diffusion%26entry.906535625%3DSophia%2520Tang%2520and%2520Yinuo%2520Zhang%2520and%2520Pranam%2520Chatterjee%26entry.1292438233%3D%2520%2520Peptide%2520therapeutics%252C%2520a%2520major%2520class%2520of%2520medicines%252C%2520have%2520achieved%2520remarkable%250Asuccess%2520across%2520diseases%2520such%2520as%2520diabetes%2520and%2520cancer%252C%2520with%2520landmark%2520examples%250Asuch%2520as%2520GLP-1%2520receptor%2520agonists%2520revolutionizing%2520the%2520treatment%2520of%2520type-2%250Adiabetes%2520and%2520obesity.%2520Despite%2520their%2520success%252C%2520designing%2520peptides%2520that%2520satisfy%250Amultiple%2520conflicting%2520objectives%252C%2520such%2520as%2520target%2520binding%2520affinity%252C%2520solubility%252C%250Aand%2520membrane%2520permeability%252C%2520remains%2520a%2520major%2520challenge.%2520Classical%2520drug%250Adevelopment%2520and%2520structure-based%2520design%2520are%2520ineffective%2520for%2520such%2520tasks%252C%2520as%2520they%250Afail%2520to%2520optimize%2520global%2520functional%2520properties%2520critical%2520for%2520therapeutic%250Aefficacy.%2520Existing%2520generative%2520frameworks%2520are%2520largely%2520limited%2520to%2520continuous%250Aspaces%252C%2520unconditioned%2520outputs%252C%2520or%2520single-objective%2520guidance%252C%2520making%2520them%250Aunsuitable%2520for%2520discrete%2520sequence%2520optimization%2520across%2520multiple%2520properties.%2520To%250Aaddress%2520this%252C%2520we%2520present%2520PepTune%252C%2520a%2520multi-objective%2520discrete%2520diffusion%2520model%250Afor%2520the%2520simultaneous%2520generation%2520and%2520optimization%2520of%2520therapeutic%2520peptide%2520SMILES.%250ABuilt%2520on%2520the%2520Masked%2520Discrete%2520Language%2520Model%2520%2528MDLM%2529%2520framework%252C%2520PepTune%2520ensures%250Avalid%2520peptide%2520structures%2520with%2520state-dependent%2520masking%2520schedules%2520and%250Apenalty-based%2520objectives.%2520To%2520guide%2520the%2520diffusion%2520process%252C%2520we%2520propose%2520a%2520Monte%250ACarlo%2520Tree%2520Search%2520%2528MCTS%2529-based%2520strategy%2520that%2520balances%2520exploration%2520and%250Aexploitation%2520to%2520iteratively%2520refine%2520Pareto-optimal%2520sequences.%2520MCTS%2520integrates%250Aclassifier-based%2520rewards%2520with%2520search-tree%2520expansion%252C%2520overcoming%2520gradient%250Aestimation%2520challenges%2520and%2520data%2520sparsity%2520inherent%2520to%2520discrete%2520spaces.%2520Using%250APepTune%252C%2520we%2520generate%2520diverse%252C%2520chemically-modified%2520peptides%2520optimized%2520for%250Amultiple%2520therapeutic%2520properties%252C%2520including%2520target%2520binding%2520affinity%252C%2520membrane%250Apermeability%252C%2520solubility%252C%2520hemolysis%252C%2520and%2520non-fouling%2520characteristics%2520on%2520various%250Adisease-relevant%2520targets.%2520In%2520total%252C%2520our%2520results%2520demonstrate%2520that%2520MCTS-guided%250Adiscrete%2520diffusion%2520is%2520a%2520powerful%2520and%2520modular%2520approach%2520for%2520multi-objective%250Asequence%2520design%2520in%2520discrete%2520state%2520spaces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17780v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PepTune%3A%20De%20Novo%20Generation%20of%20Therapeutic%20Peptides%20with%0A%20%20Multi-Objective-Guided%20Discrete%20Diffusion&entry.906535625=Sophia%20Tang%20and%20Yinuo%20Zhang%20and%20Pranam%20Chatterjee&entry.1292438233=%20%20Peptide%20therapeutics%2C%20a%20major%20class%20of%20medicines%2C%20have%20achieved%20remarkable%0Asuccess%20across%20diseases%20such%20as%20diabetes%20and%20cancer%2C%20with%20landmark%20examples%0Asuch%20as%20GLP-1%20receptor%20agonists%20revolutionizing%20the%20treatment%20of%20type-2%0Adiabetes%20and%20obesity.%20Despite%20their%20success%2C%20designing%20peptides%20that%20satisfy%0Amultiple%20conflicting%20objectives%2C%20such%20as%20target%20binding%20affinity%2C%20solubility%2C%0Aand%20membrane%20permeability%2C%20remains%20a%20major%20challenge.%20Classical%20drug%0Adevelopment%20and%20structure-based%20design%20are%20ineffective%20for%20such%20tasks%2C%20as%20they%0Afail%20to%20optimize%20global%20functional%20properties%20critical%20for%20therapeutic%0Aefficacy.%20Existing%20generative%20frameworks%20are%20largely%20limited%20to%20continuous%0Aspaces%2C%20unconditioned%20outputs%2C%20or%20single-objective%20guidance%2C%20making%20them%0Aunsuitable%20for%20discrete%20sequence%20optimization%20across%20multiple%20properties.%20To%0Aaddress%20this%2C%20we%20present%20PepTune%2C%20a%20multi-objective%20discrete%20diffusion%20model%0Afor%20the%20simultaneous%20generation%20and%20optimization%20of%20therapeutic%20peptide%20SMILES.%0ABuilt%20on%20the%20Masked%20Discrete%20Language%20Model%20%28MDLM%29%20framework%2C%20PepTune%20ensures%0Avalid%20peptide%20structures%20with%20state-dependent%20masking%20schedules%20and%0Apenalty-based%20objectives.%20To%20guide%20the%20diffusion%20process%2C%20we%20propose%20a%20Monte%0ACarlo%20Tree%20Search%20%28MCTS%29-based%20strategy%20that%20balances%20exploration%20and%0Aexploitation%20to%20iteratively%20refine%20Pareto-optimal%20sequences.%20MCTS%20integrates%0Aclassifier-based%20rewards%20with%20search-tree%20expansion%2C%20overcoming%20gradient%0Aestimation%20challenges%20and%20data%20sparsity%20inherent%20to%20discrete%20spaces.%20Using%0APepTune%2C%20we%20generate%20diverse%2C%20chemically-modified%20peptides%20optimized%20for%0Amultiple%20therapeutic%20properties%2C%20including%20target%20binding%20affinity%2C%20membrane%0Apermeability%2C%20solubility%2C%20hemolysis%2C%20and%20non-fouling%20characteristics%20on%20various%0Adisease-relevant%20targets.%20In%20total%2C%20our%20results%20demonstrate%20that%20MCTS-guided%0Adiscrete%20diffusion%20is%20a%20powerful%20and%20modular%20approach%20for%20multi-objective%0Asequence%20design%20in%20discrete%20state%20spaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17780v2&entry.124074799=Read"},
{"title": "Efficiently Serving LLM Reasoning Programs with Certaindex", "author": "Yichao Fu and Junda Chen and Siqi Zhu and Zheyu Fu and Zhongdongming Dai and Aurick Qiao and Hao Zhang", "abstract": "  The rapid evolution of large language models (LLMs) has unlocked their\ncapabilities in advanced reasoning tasks like mathematical problem-solving,\ncode generation, and legal analysis. Central to this progress are\ninference-time reasoning algorithms, which refine outputs by exploring multiple\nsolution paths, at the cost of increasing compute demands and response\nlatencies. Existing serving systems fail to adapt to the scaling behaviors of\nthese algorithms or the varying difficulty of queries, leading to inefficient\nresource use and unmet latency targets.\n  We present Dynasor, a system that optimizes inference-time compute for LLM\nreasoning queries. Unlike traditional engines, Dynasor tracks and schedules\nrequests within reasoning queries and uses Certaindex, a proxy that measures\nstatistical reasoning progress based on model certainty, to guide compute\nallocation dynamically. Dynasor co-adapts scheduling with reasoning progress:\nit allocates more compute to hard queries, reduces compute for simpler ones,\nand terminates unpromising queries early, balancing accuracy, latency, and\ncost. On diverse datasets and algorithms, Dynasor reduces compute by up to 50%\nin batch processing and sustaining 3.3x higher query rates or 4.7x tighter\nlatency SLOs in online serving.\n", "link": "http://arxiv.org/abs/2412.20993v1", "date": "2024-12-30", "relevancy": 1.9175, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4818}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4818}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficiently%20Serving%20LLM%20Reasoning%20Programs%20with%20Certaindex&body=Title%3A%20Efficiently%20Serving%20LLM%20Reasoning%20Programs%20with%20Certaindex%0AAuthor%3A%20Yichao%20Fu%20and%20Junda%20Chen%20and%20Siqi%20Zhu%20and%20Zheyu%20Fu%20and%20Zhongdongming%20Dai%20and%20Aurick%20Qiao%20and%20Hao%20Zhang%0AAbstract%3A%20%20%20The%20rapid%20evolution%20of%20large%20language%20models%20%28LLMs%29%20has%20unlocked%20their%0Acapabilities%20in%20advanced%20reasoning%20tasks%20like%20mathematical%20problem-solving%2C%0Acode%20generation%2C%20and%20legal%20analysis.%20Central%20to%20this%20progress%20are%0Ainference-time%20reasoning%20algorithms%2C%20which%20refine%20outputs%20by%20exploring%20multiple%0Asolution%20paths%2C%20at%20the%20cost%20of%20increasing%20compute%20demands%20and%20response%0Alatencies.%20Existing%20serving%20systems%20fail%20to%20adapt%20to%20the%20scaling%20behaviors%20of%0Athese%20algorithms%20or%20the%20varying%20difficulty%20of%20queries%2C%20leading%20to%20inefficient%0Aresource%20use%20and%20unmet%20latency%20targets.%0A%20%20We%20present%20Dynasor%2C%20a%20system%20that%20optimizes%20inference-time%20compute%20for%20LLM%0Areasoning%20queries.%20Unlike%20traditional%20engines%2C%20Dynasor%20tracks%20and%20schedules%0Arequests%20within%20reasoning%20queries%20and%20uses%20Certaindex%2C%20a%20proxy%20that%20measures%0Astatistical%20reasoning%20progress%20based%20on%20model%20certainty%2C%20to%20guide%20compute%0Aallocation%20dynamically.%20Dynasor%20co-adapts%20scheduling%20with%20reasoning%20progress%3A%0Ait%20allocates%20more%20compute%20to%20hard%20queries%2C%20reduces%20compute%20for%20simpler%20ones%2C%0Aand%20terminates%20unpromising%20queries%20early%2C%20balancing%20accuracy%2C%20latency%2C%20and%0Acost.%20On%20diverse%20datasets%20and%20algorithms%2C%20Dynasor%20reduces%20compute%20by%20up%20to%2050%25%0Ain%20batch%20processing%20and%20sustaining%203.3x%20higher%20query%20rates%20or%204.7x%20tighter%0Alatency%20SLOs%20in%20online%20serving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20993v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficiently%2520Serving%2520LLM%2520Reasoning%2520Programs%2520with%2520Certaindex%26entry.906535625%3DYichao%2520Fu%2520and%2520Junda%2520Chen%2520and%2520Siqi%2520Zhu%2520and%2520Zheyu%2520Fu%2520and%2520Zhongdongming%2520Dai%2520and%2520Aurick%2520Qiao%2520and%2520Hao%2520Zhang%26entry.1292438233%3D%2520%2520The%2520rapid%2520evolution%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520has%2520unlocked%2520their%250Acapabilities%2520in%2520advanced%2520reasoning%2520tasks%2520like%2520mathematical%2520problem-solving%252C%250Acode%2520generation%252C%2520and%2520legal%2520analysis.%2520Central%2520to%2520this%2520progress%2520are%250Ainference-time%2520reasoning%2520algorithms%252C%2520which%2520refine%2520outputs%2520by%2520exploring%2520multiple%250Asolution%2520paths%252C%2520at%2520the%2520cost%2520of%2520increasing%2520compute%2520demands%2520and%2520response%250Alatencies.%2520Existing%2520serving%2520systems%2520fail%2520to%2520adapt%2520to%2520the%2520scaling%2520behaviors%2520of%250Athese%2520algorithms%2520or%2520the%2520varying%2520difficulty%2520of%2520queries%252C%2520leading%2520to%2520inefficient%250Aresource%2520use%2520and%2520unmet%2520latency%2520targets.%250A%2520%2520We%2520present%2520Dynasor%252C%2520a%2520system%2520that%2520optimizes%2520inference-time%2520compute%2520for%2520LLM%250Areasoning%2520queries.%2520Unlike%2520traditional%2520engines%252C%2520Dynasor%2520tracks%2520and%2520schedules%250Arequests%2520within%2520reasoning%2520queries%2520and%2520uses%2520Certaindex%252C%2520a%2520proxy%2520that%2520measures%250Astatistical%2520reasoning%2520progress%2520based%2520on%2520model%2520certainty%252C%2520to%2520guide%2520compute%250Aallocation%2520dynamically.%2520Dynasor%2520co-adapts%2520scheduling%2520with%2520reasoning%2520progress%253A%250Ait%2520allocates%2520more%2520compute%2520to%2520hard%2520queries%252C%2520reduces%2520compute%2520for%2520simpler%2520ones%252C%250Aand%2520terminates%2520unpromising%2520queries%2520early%252C%2520balancing%2520accuracy%252C%2520latency%252C%2520and%250Acost.%2520On%2520diverse%2520datasets%2520and%2520algorithms%252C%2520Dynasor%2520reduces%2520compute%2520by%2520up%2520to%252050%2525%250Ain%2520batch%2520processing%2520and%2520sustaining%25203.3x%2520higher%2520query%2520rates%2520or%25204.7x%2520tighter%250Alatency%2520SLOs%2520in%2520online%2520serving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20993v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficiently%20Serving%20LLM%20Reasoning%20Programs%20with%20Certaindex&entry.906535625=Yichao%20Fu%20and%20Junda%20Chen%20and%20Siqi%20Zhu%20and%20Zheyu%20Fu%20and%20Zhongdongming%20Dai%20and%20Aurick%20Qiao%20and%20Hao%20Zhang&entry.1292438233=%20%20The%20rapid%20evolution%20of%20large%20language%20models%20%28LLMs%29%20has%20unlocked%20their%0Acapabilities%20in%20advanced%20reasoning%20tasks%20like%20mathematical%20problem-solving%2C%0Acode%20generation%2C%20and%20legal%20analysis.%20Central%20to%20this%20progress%20are%0Ainference-time%20reasoning%20algorithms%2C%20which%20refine%20outputs%20by%20exploring%20multiple%0Asolution%20paths%2C%20at%20the%20cost%20of%20increasing%20compute%20demands%20and%20response%0Alatencies.%20Existing%20serving%20systems%20fail%20to%20adapt%20to%20the%20scaling%20behaviors%20of%0Athese%20algorithms%20or%20the%20varying%20difficulty%20of%20queries%2C%20leading%20to%20inefficient%0Aresource%20use%20and%20unmet%20latency%20targets.%0A%20%20We%20present%20Dynasor%2C%20a%20system%20that%20optimizes%20inference-time%20compute%20for%20LLM%0Areasoning%20queries.%20Unlike%20traditional%20engines%2C%20Dynasor%20tracks%20and%20schedules%0Arequests%20within%20reasoning%20queries%20and%20uses%20Certaindex%2C%20a%20proxy%20that%20measures%0Astatistical%20reasoning%20progress%20based%20on%20model%20certainty%2C%20to%20guide%20compute%0Aallocation%20dynamically.%20Dynasor%20co-adapts%20scheduling%20with%20reasoning%20progress%3A%0Ait%20allocates%20more%20compute%20to%20hard%20queries%2C%20reduces%20compute%20for%20simpler%20ones%2C%0Aand%20terminates%20unpromising%20queries%20early%2C%20balancing%20accuracy%2C%20latency%2C%20and%0Acost.%20On%20diverse%20datasets%20and%20algorithms%2C%20Dynasor%20reduces%20compute%20by%20up%20to%2050%25%0Ain%20batch%20processing%20and%20sustaining%203.3x%20higher%20query%20rates%20or%204.7x%20tighter%0Alatency%20SLOs%20in%20online%20serving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20993v1&entry.124074799=Read"},
{"title": "E2EDiff: Direct Mapping from Noise to Data for Enhanced Diffusion Models", "author": "Zhiyu Tan and WenXu Qian and Hesen Chen and Mengping Yang and Lei Chen and Hao Li", "abstract": "  Diffusion models have emerged as a powerful framework for generative\nmodeling, achieving state-of-the-art performance across various tasks. However,\nthey face several inherent limitations, including a training-sampling gap,\ninformation leakage in the progressive noising process, and the inability to\nincorporate advanced loss functions like perceptual and adversarial losses\nduring training. To address these challenges, we propose an innovative\nend-to-end training framework that aligns the training and sampling processes\nby directly optimizing the final reconstruction output. Our method eliminates\nthe training-sampling gap, mitigates information leakage by treating the\ntraining process as a direct mapping from pure noise to the target data\ndistribution, and enables the integration of perceptual and adversarial losses\ninto the objective. Extensive experiments on benchmarks such as COCO30K and\nHW30K demonstrate that our approach consistently outperforms traditional\ndiffusion models, achieving superior results in terms of FID and CLIP score,\neven with reduced sampling steps. These findings highlight the potential of\nend-to-end training to advance diffusion-based generative models toward more\nrobust and efficient solutions.\n", "link": "http://arxiv.org/abs/2412.21044v1", "date": "2024-12-30", "relevancy": 1.8978, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6697}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6434}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6134}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20E2EDiff%3A%20Direct%20Mapping%20from%20Noise%20to%20Data%20for%20Enhanced%20Diffusion%20Models&body=Title%3A%20E2EDiff%3A%20Direct%20Mapping%20from%20Noise%20to%20Data%20for%20Enhanced%20Diffusion%20Models%0AAuthor%3A%20Zhiyu%20Tan%20and%20WenXu%20Qian%20and%20Hesen%20Chen%20and%20Mengping%20Yang%20and%20Lei%20Chen%20and%20Hao%20Li%0AAbstract%3A%20%20%20Diffusion%20models%20have%20emerged%20as%20a%20powerful%20framework%20for%20generative%0Amodeling%2C%20achieving%20state-of-the-art%20performance%20across%20various%20tasks.%20However%2C%0Athey%20face%20several%20inherent%20limitations%2C%20including%20a%20training-sampling%20gap%2C%0Ainformation%20leakage%20in%20the%20progressive%20noising%20process%2C%20and%20the%20inability%20to%0Aincorporate%20advanced%20loss%20functions%20like%20perceptual%20and%20adversarial%20losses%0Aduring%20training.%20To%20address%20these%20challenges%2C%20we%20propose%20an%20innovative%0Aend-to-end%20training%20framework%20that%20aligns%20the%20training%20and%20sampling%20processes%0Aby%20directly%20optimizing%20the%20final%20reconstruction%20output.%20Our%20method%20eliminates%0Athe%20training-sampling%20gap%2C%20mitigates%20information%20leakage%20by%20treating%20the%0Atraining%20process%20as%20a%20direct%20mapping%20from%20pure%20noise%20to%20the%20target%20data%0Adistribution%2C%20and%20enables%20the%20integration%20of%20perceptual%20and%20adversarial%20losses%0Ainto%20the%20objective.%20Extensive%20experiments%20on%20benchmarks%20such%20as%20COCO30K%20and%0AHW30K%20demonstrate%20that%20our%20approach%20consistently%20outperforms%20traditional%0Adiffusion%20models%2C%20achieving%20superior%20results%20in%20terms%20of%20FID%20and%20CLIP%20score%2C%0Aeven%20with%20reduced%20sampling%20steps.%20These%20findings%20highlight%20the%20potential%20of%0Aend-to-end%20training%20to%20advance%20diffusion-based%20generative%20models%20toward%20more%0Arobust%20and%20efficient%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21044v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DE2EDiff%253A%2520Direct%2520Mapping%2520from%2520Noise%2520to%2520Data%2520for%2520Enhanced%2520Diffusion%2520Models%26entry.906535625%3DZhiyu%2520Tan%2520and%2520WenXu%2520Qian%2520and%2520Hesen%2520Chen%2520and%2520Mengping%2520Yang%2520and%2520Lei%2520Chen%2520and%2520Hao%2520Li%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520emerged%2520as%2520a%2520powerful%2520framework%2520for%2520generative%250Amodeling%252C%2520achieving%2520state-of-the-art%2520performance%2520across%2520various%2520tasks.%2520However%252C%250Athey%2520face%2520several%2520inherent%2520limitations%252C%2520including%2520a%2520training-sampling%2520gap%252C%250Ainformation%2520leakage%2520in%2520the%2520progressive%2520noising%2520process%252C%2520and%2520the%2520inability%2520to%250Aincorporate%2520advanced%2520loss%2520functions%2520like%2520perceptual%2520and%2520adversarial%2520losses%250Aduring%2520training.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520an%2520innovative%250Aend-to-end%2520training%2520framework%2520that%2520aligns%2520the%2520training%2520and%2520sampling%2520processes%250Aby%2520directly%2520optimizing%2520the%2520final%2520reconstruction%2520output.%2520Our%2520method%2520eliminates%250Athe%2520training-sampling%2520gap%252C%2520mitigates%2520information%2520leakage%2520by%2520treating%2520the%250Atraining%2520process%2520as%2520a%2520direct%2520mapping%2520from%2520pure%2520noise%2520to%2520the%2520target%2520data%250Adistribution%252C%2520and%2520enables%2520the%2520integration%2520of%2520perceptual%2520and%2520adversarial%2520losses%250Ainto%2520the%2520objective.%2520Extensive%2520experiments%2520on%2520benchmarks%2520such%2520as%2520COCO30K%2520and%250AHW30K%2520demonstrate%2520that%2520our%2520approach%2520consistently%2520outperforms%2520traditional%250Adiffusion%2520models%252C%2520achieving%2520superior%2520results%2520in%2520terms%2520of%2520FID%2520and%2520CLIP%2520score%252C%250Aeven%2520with%2520reduced%2520sampling%2520steps.%2520These%2520findings%2520highlight%2520the%2520potential%2520of%250Aend-to-end%2520training%2520to%2520advance%2520diffusion-based%2520generative%2520models%2520toward%2520more%250Arobust%2520and%2520efficient%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21044v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=E2EDiff%3A%20Direct%20Mapping%20from%20Noise%20to%20Data%20for%20Enhanced%20Diffusion%20Models&entry.906535625=Zhiyu%20Tan%20and%20WenXu%20Qian%20and%20Hesen%20Chen%20and%20Mengping%20Yang%20and%20Lei%20Chen%20and%20Hao%20Li&entry.1292438233=%20%20Diffusion%20models%20have%20emerged%20as%20a%20powerful%20framework%20for%20generative%0Amodeling%2C%20achieving%20state-of-the-art%20performance%20across%20various%20tasks.%20However%2C%0Athey%20face%20several%20inherent%20limitations%2C%20including%20a%20training-sampling%20gap%2C%0Ainformation%20leakage%20in%20the%20progressive%20noising%20process%2C%20and%20the%20inability%20to%0Aincorporate%20advanced%20loss%20functions%20like%20perceptual%20and%20adversarial%20losses%0Aduring%20training.%20To%20address%20these%20challenges%2C%20we%20propose%20an%20innovative%0Aend-to-end%20training%20framework%20that%20aligns%20the%20training%20and%20sampling%20processes%0Aby%20directly%20optimizing%20the%20final%20reconstruction%20output.%20Our%20method%20eliminates%0Athe%20training-sampling%20gap%2C%20mitigates%20information%20leakage%20by%20treating%20the%0Atraining%20process%20as%20a%20direct%20mapping%20from%20pure%20noise%20to%20the%20target%20data%0Adistribution%2C%20and%20enables%20the%20integration%20of%20perceptual%20and%20adversarial%20losses%0Ainto%20the%20objective.%20Extensive%20experiments%20on%20benchmarks%20such%20as%20COCO30K%20and%0AHW30K%20demonstrate%20that%20our%20approach%20consistently%20outperforms%20traditional%0Adiffusion%20models%2C%20achieving%20superior%20results%20in%20terms%20of%20FID%20and%20CLIP%20score%2C%0Aeven%20with%20reduced%20sampling%20steps.%20These%20findings%20highlight%20the%20potential%20of%0Aend-to-end%20training%20to%20advance%20diffusion-based%20generative%20models%20toward%20more%0Arobust%20and%20efficient%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21044v1&entry.124074799=Read"},
{"title": "AlignAb: Pareto-Optimal Energy Alignment for Designing Nature-Like\n  Antibodies", "author": "Yibo Wen and Chenwei Xu and Jerry Yao-Chieh Hu and Han Liu", "abstract": "  We present a three-stage framework for training deep learning models\nspecializing in antibody sequence-structure co-design. We first pre-train a\nlanguage model using millions of antibody sequence data. Then, we employ the\nlearned representations to guide the training of a diffusion model for joint\noptimization over both sequence and structure of antibodies. During the final\nalignment stage, we optimize the model to favor antibodies with low repulsion\nand high attraction to the antigen binding site, enhancing the rationality and\nfunctionality of the designs. To mitigate conflicting energy preferences, we\nextend AbDPO (Antibody Direct Preference Optimization) to guide the model\ntowards Pareto optimality under multiple energy-based alignment objectives.\nFurthermore, we adopt an iterative learning paradigm with temperature scaling,\nenabling the model to benefit from diverse online datasets without requiring\nadditional data. In practice, our proposed methods achieve high stability and\nefficiency in producing a better Pareto front of antibody designs compared to\ntop samples generated by baselines and previous alignment techniques. Through\nextensive experiments, we showcase the superior performance of our methods in\ngenerating nature-like antibodies with high binding affinity consistently.\n", "link": "http://arxiv.org/abs/2412.20984v1", "date": "2024-12-30", "relevancy": 1.8969, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4836}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4711}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AlignAb%3A%20Pareto-Optimal%20Energy%20Alignment%20for%20Designing%20Nature-Like%0A%20%20Antibodies&body=Title%3A%20AlignAb%3A%20Pareto-Optimal%20Energy%20Alignment%20for%20Designing%20Nature-Like%0A%20%20Antibodies%0AAuthor%3A%20Yibo%20Wen%20and%20Chenwei%20Xu%20and%20Jerry%20Yao-Chieh%20Hu%20and%20Han%20Liu%0AAbstract%3A%20%20%20We%20present%20a%20three-stage%20framework%20for%20training%20deep%20learning%20models%0Aspecializing%20in%20antibody%20sequence-structure%20co-design.%20We%20first%20pre-train%20a%0Alanguage%20model%20using%20millions%20of%20antibody%20sequence%20data.%20Then%2C%20we%20employ%20the%0Alearned%20representations%20to%20guide%20the%20training%20of%20a%20diffusion%20model%20for%20joint%0Aoptimization%20over%20both%20sequence%20and%20structure%20of%20antibodies.%20During%20the%20final%0Aalignment%20stage%2C%20we%20optimize%20the%20model%20to%20favor%20antibodies%20with%20low%20repulsion%0Aand%20high%20attraction%20to%20the%20antigen%20binding%20site%2C%20enhancing%20the%20rationality%20and%0Afunctionality%20of%20the%20designs.%20To%20mitigate%20conflicting%20energy%20preferences%2C%20we%0Aextend%20AbDPO%20%28Antibody%20Direct%20Preference%20Optimization%29%20to%20guide%20the%20model%0Atowards%20Pareto%20optimality%20under%20multiple%20energy-based%20alignment%20objectives.%0AFurthermore%2C%20we%20adopt%20an%20iterative%20learning%20paradigm%20with%20temperature%20scaling%2C%0Aenabling%20the%20model%20to%20benefit%20from%20diverse%20online%20datasets%20without%20requiring%0Aadditional%20data.%20In%20practice%2C%20our%20proposed%20methods%20achieve%20high%20stability%20and%0Aefficiency%20in%20producing%20a%20better%20Pareto%20front%20of%20antibody%20designs%20compared%20to%0Atop%20samples%20generated%20by%20baselines%20and%20previous%20alignment%20techniques.%20Through%0Aextensive%20experiments%2C%20we%20showcase%20the%20superior%20performance%20of%20our%20methods%20in%0Agenerating%20nature-like%20antibodies%20with%20high%20binding%20affinity%20consistently.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20984v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlignAb%253A%2520Pareto-Optimal%2520Energy%2520Alignment%2520for%2520Designing%2520Nature-Like%250A%2520%2520Antibodies%26entry.906535625%3DYibo%2520Wen%2520and%2520Chenwei%2520Xu%2520and%2520Jerry%2520Yao-Chieh%2520Hu%2520and%2520Han%2520Liu%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520three-stage%2520framework%2520for%2520training%2520deep%2520learning%2520models%250Aspecializing%2520in%2520antibody%2520sequence-structure%2520co-design.%2520We%2520first%2520pre-train%2520a%250Alanguage%2520model%2520using%2520millions%2520of%2520antibody%2520sequence%2520data.%2520Then%252C%2520we%2520employ%2520the%250Alearned%2520representations%2520to%2520guide%2520the%2520training%2520of%2520a%2520diffusion%2520model%2520for%2520joint%250Aoptimization%2520over%2520both%2520sequence%2520and%2520structure%2520of%2520antibodies.%2520During%2520the%2520final%250Aalignment%2520stage%252C%2520we%2520optimize%2520the%2520model%2520to%2520favor%2520antibodies%2520with%2520low%2520repulsion%250Aand%2520high%2520attraction%2520to%2520the%2520antigen%2520binding%2520site%252C%2520enhancing%2520the%2520rationality%2520and%250Afunctionality%2520of%2520the%2520designs.%2520To%2520mitigate%2520conflicting%2520energy%2520preferences%252C%2520we%250Aextend%2520AbDPO%2520%2528Antibody%2520Direct%2520Preference%2520Optimization%2529%2520to%2520guide%2520the%2520model%250Atowards%2520Pareto%2520optimality%2520under%2520multiple%2520energy-based%2520alignment%2520objectives.%250AFurthermore%252C%2520we%2520adopt%2520an%2520iterative%2520learning%2520paradigm%2520with%2520temperature%2520scaling%252C%250Aenabling%2520the%2520model%2520to%2520benefit%2520from%2520diverse%2520online%2520datasets%2520without%2520requiring%250Aadditional%2520data.%2520In%2520practice%252C%2520our%2520proposed%2520methods%2520achieve%2520high%2520stability%2520and%250Aefficiency%2520in%2520producing%2520a%2520better%2520Pareto%2520front%2520of%2520antibody%2520designs%2520compared%2520to%250Atop%2520samples%2520generated%2520by%2520baselines%2520and%2520previous%2520alignment%2520techniques.%2520Through%250Aextensive%2520experiments%252C%2520we%2520showcase%2520the%2520superior%2520performance%2520of%2520our%2520methods%2520in%250Agenerating%2520nature-like%2520antibodies%2520with%2520high%2520binding%2520affinity%2520consistently.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20984v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlignAb%3A%20Pareto-Optimal%20Energy%20Alignment%20for%20Designing%20Nature-Like%0A%20%20Antibodies&entry.906535625=Yibo%20Wen%20and%20Chenwei%20Xu%20and%20Jerry%20Yao-Chieh%20Hu%20and%20Han%20Liu&entry.1292438233=%20%20We%20present%20a%20three-stage%20framework%20for%20training%20deep%20learning%20models%0Aspecializing%20in%20antibody%20sequence-structure%20co-design.%20We%20first%20pre-train%20a%0Alanguage%20model%20using%20millions%20of%20antibody%20sequence%20data.%20Then%2C%20we%20employ%20the%0Alearned%20representations%20to%20guide%20the%20training%20of%20a%20diffusion%20model%20for%20joint%0Aoptimization%20over%20both%20sequence%20and%20structure%20of%20antibodies.%20During%20the%20final%0Aalignment%20stage%2C%20we%20optimize%20the%20model%20to%20favor%20antibodies%20with%20low%20repulsion%0Aand%20high%20attraction%20to%20the%20antigen%20binding%20site%2C%20enhancing%20the%20rationality%20and%0Afunctionality%20of%20the%20designs.%20To%20mitigate%20conflicting%20energy%20preferences%2C%20we%0Aextend%20AbDPO%20%28Antibody%20Direct%20Preference%20Optimization%29%20to%20guide%20the%20model%0Atowards%20Pareto%20optimality%20under%20multiple%20energy-based%20alignment%20objectives.%0AFurthermore%2C%20we%20adopt%20an%20iterative%20learning%20paradigm%20with%20temperature%20scaling%2C%0Aenabling%20the%20model%20to%20benefit%20from%20diverse%20online%20datasets%20without%20requiring%0Aadditional%20data.%20In%20practice%2C%20our%20proposed%20methods%20achieve%20high%20stability%20and%0Aefficiency%20in%20producing%20a%20better%20Pareto%20front%20of%20antibody%20designs%20compared%20to%0Atop%20samples%20generated%20by%20baselines%20and%20previous%20alignment%20techniques.%20Through%0Aextensive%20experiments%2C%20we%20showcase%20the%20superior%20performance%20of%20our%20methods%20in%0Agenerating%20nature-like%20antibodies%20with%20high%20binding%20affinity%20consistently.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20984v1&entry.124074799=Read"},
{"title": "GISExplainer: On Explainability of Graph Neural Networks via\n  Game-theoretic Interaction Subgraphs", "author": "Xingping Xian and Jianlu Liu and Chao Wang and Tao Wu and Shaojie Qiao and Xiaochuan Tang and Qun Liu", "abstract": "  Explainability is crucial for the application of black-box Graph Neural\nNetworks (GNNs) in critical fields such as healthcare, finance, cybersecurity,\nand more. Various feature attribution methods, especially the\nperturbation-based methods, have been proposed to indicate how much each\nnode/edge contributes to the model predictions. However, these methods fail to\ngenerate connected explanatory subgraphs that consider the causal interaction\nbetween edges within different coalition scales, which will result in\nunfaithful explanations. In our study, we propose GISExplainer, a novel\ngame-theoretic interaction based explanation method that uncovers what the\nunderlying GNNs have learned for node classification by discovering\nhuman-interpretable causal explanatory subgraphs. First, GISExplainer defines a\ncausal attribution mechanism that considers the game-theoretic interaction of\nmulti-granularity coalitions in candidate explanatory subgraph to quantify the\ncausal effect of an edge on the prediction. Second, GISExplainer assumes that\nthe coalitions with negative effects on the predictions are also significant\nfor model interpretation, and the contribution of the computation graph stems\nfrom the combined influence of both positive and negative interactions within\nthe coalitions. Then, GISExplainer regards the explanation task as a sequential\ndecision process, in which a salient edges is successively selected and\nconnected to the previously selected subgraph based on its causal effect to\nform an explanatory subgraph, ultimately striving for better explanations.\nAdditionally, an efficiency optimization scheme is proposed for the causal\nattribution mechanism through coalition sampling. Extensive experiments\ndemonstrate that GISExplainer achieves better performance than state-of-the-art\napproaches w.r.t. two quantitative metrics: Fidelity and Sparsity.\n", "link": "http://arxiv.org/abs/2409.15698v2", "date": "2024-12-30", "relevancy": 1.8919, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.496}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4786}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GISExplainer%3A%20On%20Explainability%20of%20Graph%20Neural%20Networks%20via%0A%20%20Game-theoretic%20Interaction%20Subgraphs&body=Title%3A%20GISExplainer%3A%20On%20Explainability%20of%20Graph%20Neural%20Networks%20via%0A%20%20Game-theoretic%20Interaction%20Subgraphs%0AAuthor%3A%20Xingping%20Xian%20and%20Jianlu%20Liu%20and%20Chao%20Wang%20and%20Tao%20Wu%20and%20Shaojie%20Qiao%20and%20Xiaochuan%20Tang%20and%20Qun%20Liu%0AAbstract%3A%20%20%20Explainability%20is%20crucial%20for%20the%20application%20of%20black-box%20Graph%20Neural%0ANetworks%20%28GNNs%29%20in%20critical%20fields%20such%20as%20healthcare%2C%20finance%2C%20cybersecurity%2C%0Aand%20more.%20Various%20feature%20attribution%20methods%2C%20especially%20the%0Aperturbation-based%20methods%2C%20have%20been%20proposed%20to%20indicate%20how%20much%20each%0Anode/edge%20contributes%20to%20the%20model%20predictions.%20However%2C%20these%20methods%20fail%20to%0Agenerate%20connected%20explanatory%20subgraphs%20that%20consider%20the%20causal%20interaction%0Abetween%20edges%20within%20different%20coalition%20scales%2C%20which%20will%20result%20in%0Aunfaithful%20explanations.%20In%20our%20study%2C%20we%20propose%20GISExplainer%2C%20a%20novel%0Agame-theoretic%20interaction%20based%20explanation%20method%20that%20uncovers%20what%20the%0Aunderlying%20GNNs%20have%20learned%20for%20node%20classification%20by%20discovering%0Ahuman-interpretable%20causal%20explanatory%20subgraphs.%20First%2C%20GISExplainer%20defines%20a%0Acausal%20attribution%20mechanism%20that%20considers%20the%20game-theoretic%20interaction%20of%0Amulti-granularity%20coalitions%20in%20candidate%20explanatory%20subgraph%20to%20quantify%20the%0Acausal%20effect%20of%20an%20edge%20on%20the%20prediction.%20Second%2C%20GISExplainer%20assumes%20that%0Athe%20coalitions%20with%20negative%20effects%20on%20the%20predictions%20are%20also%20significant%0Afor%20model%20interpretation%2C%20and%20the%20contribution%20of%20the%20computation%20graph%20stems%0Afrom%20the%20combined%20influence%20of%20both%20positive%20and%20negative%20interactions%20within%0Athe%20coalitions.%20Then%2C%20GISExplainer%20regards%20the%20explanation%20task%20as%20a%20sequential%0Adecision%20process%2C%20in%20which%20a%20salient%20edges%20is%20successively%20selected%20and%0Aconnected%20to%20the%20previously%20selected%20subgraph%20based%20on%20its%20causal%20effect%20to%0Aform%20an%20explanatory%20subgraph%2C%20ultimately%20striving%20for%20better%20explanations.%0AAdditionally%2C%20an%20efficiency%20optimization%20scheme%20is%20proposed%20for%20the%20causal%0Aattribution%20mechanism%20through%20coalition%20sampling.%20Extensive%20experiments%0Ademonstrate%20that%20GISExplainer%20achieves%20better%20performance%20than%20state-of-the-art%0Aapproaches%20w.r.t.%20two%20quantitative%20metrics%3A%20Fidelity%20and%20Sparsity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.15698v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGISExplainer%253A%2520On%2520Explainability%2520of%2520Graph%2520Neural%2520Networks%2520via%250A%2520%2520Game-theoretic%2520Interaction%2520Subgraphs%26entry.906535625%3DXingping%2520Xian%2520and%2520Jianlu%2520Liu%2520and%2520Chao%2520Wang%2520and%2520Tao%2520Wu%2520and%2520Shaojie%2520Qiao%2520and%2520Xiaochuan%2520Tang%2520and%2520Qun%2520Liu%26entry.1292438233%3D%2520%2520Explainability%2520is%2520crucial%2520for%2520the%2520application%2520of%2520black-box%2520Graph%2520Neural%250ANetworks%2520%2528GNNs%2529%2520in%2520critical%2520fields%2520such%2520as%2520healthcare%252C%2520finance%252C%2520cybersecurity%252C%250Aand%2520more.%2520Various%2520feature%2520attribution%2520methods%252C%2520especially%2520the%250Aperturbation-based%2520methods%252C%2520have%2520been%2520proposed%2520to%2520indicate%2520how%2520much%2520each%250Anode/edge%2520contributes%2520to%2520the%2520model%2520predictions.%2520However%252C%2520these%2520methods%2520fail%2520to%250Agenerate%2520connected%2520explanatory%2520subgraphs%2520that%2520consider%2520the%2520causal%2520interaction%250Abetween%2520edges%2520within%2520different%2520coalition%2520scales%252C%2520which%2520will%2520result%2520in%250Aunfaithful%2520explanations.%2520In%2520our%2520study%252C%2520we%2520propose%2520GISExplainer%252C%2520a%2520novel%250Agame-theoretic%2520interaction%2520based%2520explanation%2520method%2520that%2520uncovers%2520what%2520the%250Aunderlying%2520GNNs%2520have%2520learned%2520for%2520node%2520classification%2520by%2520discovering%250Ahuman-interpretable%2520causal%2520explanatory%2520subgraphs.%2520First%252C%2520GISExplainer%2520defines%2520a%250Acausal%2520attribution%2520mechanism%2520that%2520considers%2520the%2520game-theoretic%2520interaction%2520of%250Amulti-granularity%2520coalitions%2520in%2520candidate%2520explanatory%2520subgraph%2520to%2520quantify%2520the%250Acausal%2520effect%2520of%2520an%2520edge%2520on%2520the%2520prediction.%2520Second%252C%2520GISExplainer%2520assumes%2520that%250Athe%2520coalitions%2520with%2520negative%2520effects%2520on%2520the%2520predictions%2520are%2520also%2520significant%250Afor%2520model%2520interpretation%252C%2520and%2520the%2520contribution%2520of%2520the%2520computation%2520graph%2520stems%250Afrom%2520the%2520combined%2520influence%2520of%2520both%2520positive%2520and%2520negative%2520interactions%2520within%250Athe%2520coalitions.%2520Then%252C%2520GISExplainer%2520regards%2520the%2520explanation%2520task%2520as%2520a%2520sequential%250Adecision%2520process%252C%2520in%2520which%2520a%2520salient%2520edges%2520is%2520successively%2520selected%2520and%250Aconnected%2520to%2520the%2520previously%2520selected%2520subgraph%2520based%2520on%2520its%2520causal%2520effect%2520to%250Aform%2520an%2520explanatory%2520subgraph%252C%2520ultimately%2520striving%2520for%2520better%2520explanations.%250AAdditionally%252C%2520an%2520efficiency%2520optimization%2520scheme%2520is%2520proposed%2520for%2520the%2520causal%250Aattribution%2520mechanism%2520through%2520coalition%2520sampling.%2520Extensive%2520experiments%250Ademonstrate%2520that%2520GISExplainer%2520achieves%2520better%2520performance%2520than%2520state-of-the-art%250Aapproaches%2520w.r.t.%2520two%2520quantitative%2520metrics%253A%2520Fidelity%2520and%2520Sparsity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.15698v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GISExplainer%3A%20On%20Explainability%20of%20Graph%20Neural%20Networks%20via%0A%20%20Game-theoretic%20Interaction%20Subgraphs&entry.906535625=Xingping%20Xian%20and%20Jianlu%20Liu%20and%20Chao%20Wang%20and%20Tao%20Wu%20and%20Shaojie%20Qiao%20and%20Xiaochuan%20Tang%20and%20Qun%20Liu&entry.1292438233=%20%20Explainability%20is%20crucial%20for%20the%20application%20of%20black-box%20Graph%20Neural%0ANetworks%20%28GNNs%29%20in%20critical%20fields%20such%20as%20healthcare%2C%20finance%2C%20cybersecurity%2C%0Aand%20more.%20Various%20feature%20attribution%20methods%2C%20especially%20the%0Aperturbation-based%20methods%2C%20have%20been%20proposed%20to%20indicate%20how%20much%20each%0Anode/edge%20contributes%20to%20the%20model%20predictions.%20However%2C%20these%20methods%20fail%20to%0Agenerate%20connected%20explanatory%20subgraphs%20that%20consider%20the%20causal%20interaction%0Abetween%20edges%20within%20different%20coalition%20scales%2C%20which%20will%20result%20in%0Aunfaithful%20explanations.%20In%20our%20study%2C%20we%20propose%20GISExplainer%2C%20a%20novel%0Agame-theoretic%20interaction%20based%20explanation%20method%20that%20uncovers%20what%20the%0Aunderlying%20GNNs%20have%20learned%20for%20node%20classification%20by%20discovering%0Ahuman-interpretable%20causal%20explanatory%20subgraphs.%20First%2C%20GISExplainer%20defines%20a%0Acausal%20attribution%20mechanism%20that%20considers%20the%20game-theoretic%20interaction%20of%0Amulti-granularity%20coalitions%20in%20candidate%20explanatory%20subgraph%20to%20quantify%20the%0Acausal%20effect%20of%20an%20edge%20on%20the%20prediction.%20Second%2C%20GISExplainer%20assumes%20that%0Athe%20coalitions%20with%20negative%20effects%20on%20the%20predictions%20are%20also%20significant%0Afor%20model%20interpretation%2C%20and%20the%20contribution%20of%20the%20computation%20graph%20stems%0Afrom%20the%20combined%20influence%20of%20both%20positive%20and%20negative%20interactions%20within%0Athe%20coalitions.%20Then%2C%20GISExplainer%20regards%20the%20explanation%20task%20as%20a%20sequential%0Adecision%20process%2C%20in%20which%20a%20salient%20edges%20is%20successively%20selected%20and%0Aconnected%20to%20the%20previously%20selected%20subgraph%20based%20on%20its%20causal%20effect%20to%0Aform%20an%20explanatory%20subgraph%2C%20ultimately%20striving%20for%20better%20explanations.%0AAdditionally%2C%20an%20efficiency%20optimization%20scheme%20is%20proposed%20for%20the%20causal%0Aattribution%20mechanism%20through%20coalition%20sampling.%20Extensive%20experiments%0Ademonstrate%20that%20GISExplainer%20achieves%20better%20performance%20than%20state-of-the-art%0Aapproaches%20w.r.t.%20two%20quantitative%20metrics%3A%20Fidelity%20and%20Sparsity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.15698v2&entry.124074799=Read"},
{"title": "ReXTrust: A Model for Fine-Grained Hallucination Detection in\n  AI-Generated Radiology Reports", "author": "Romain Hardy and Sung Eun Kim and Pranav Rajpurkar", "abstract": "  The increasing adoption of AI-generated radiology reports necessitates robust\nmethods for detecting hallucinations--false or unfounded statements that could\nimpact patient care. We present ReXTrust, a novel framework for fine-grained\nhallucination detection in AI-generated radiology reports. Our approach\nleverages sequences of hidden states from large vision-language models to\nproduce finding-level hallucination risk scores. We evaluate ReXTrust on a\nsubset of the MIMIC-CXR dataset and demonstrate superior performance compared\nto existing approaches, achieving an AUROC of 0.8751 across all findings and\n0.8963 on clinically significant findings. Our results show that white-box\napproaches leveraging model hidden states can provide reliable hallucination\ndetection for medical AI systems, potentially improving the safety and\nreliability of automated radiology reporting.\n", "link": "http://arxiv.org/abs/2412.15264v2", "date": "2024-12-30", "relevancy": 1.8904, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5085}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4654}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4654}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReXTrust%3A%20A%20Model%20for%20Fine-Grained%20Hallucination%20Detection%20in%0A%20%20AI-Generated%20Radiology%20Reports&body=Title%3A%20ReXTrust%3A%20A%20Model%20for%20Fine-Grained%20Hallucination%20Detection%20in%0A%20%20AI-Generated%20Radiology%20Reports%0AAuthor%3A%20Romain%20Hardy%20and%20Sung%20Eun%20Kim%20and%20Pranav%20Rajpurkar%0AAbstract%3A%20%20%20The%20increasing%20adoption%20of%20AI-generated%20radiology%20reports%20necessitates%20robust%0Amethods%20for%20detecting%20hallucinations--false%20or%20unfounded%20statements%20that%20could%0Aimpact%20patient%20care.%20We%20present%20ReXTrust%2C%20a%20novel%20framework%20for%20fine-grained%0Ahallucination%20detection%20in%20AI-generated%20radiology%20reports.%20Our%20approach%0Aleverages%20sequences%20of%20hidden%20states%20from%20large%20vision-language%20models%20to%0Aproduce%20finding-level%20hallucination%20risk%20scores.%20We%20evaluate%20ReXTrust%20on%20a%0Asubset%20of%20the%20MIMIC-CXR%20dataset%20and%20demonstrate%20superior%20performance%20compared%0Ato%20existing%20approaches%2C%20achieving%20an%20AUROC%20of%200.8751%20across%20all%20findings%20and%0A0.8963%20on%20clinically%20significant%20findings.%20Our%20results%20show%20that%20white-box%0Aapproaches%20leveraging%20model%20hidden%20states%20can%20provide%20reliable%20hallucination%0Adetection%20for%20medical%20AI%20systems%2C%20potentially%20improving%20the%20safety%20and%0Areliability%20of%20automated%20radiology%20reporting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.15264v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReXTrust%253A%2520A%2520Model%2520for%2520Fine-Grained%2520Hallucination%2520Detection%2520in%250A%2520%2520AI-Generated%2520Radiology%2520Reports%26entry.906535625%3DRomain%2520Hardy%2520and%2520Sung%2520Eun%2520Kim%2520and%2520Pranav%2520Rajpurkar%26entry.1292438233%3D%2520%2520The%2520increasing%2520adoption%2520of%2520AI-generated%2520radiology%2520reports%2520necessitates%2520robust%250Amethods%2520for%2520detecting%2520hallucinations--false%2520or%2520unfounded%2520statements%2520that%2520could%250Aimpact%2520patient%2520care.%2520We%2520present%2520ReXTrust%252C%2520a%2520novel%2520framework%2520for%2520fine-grained%250Ahallucination%2520detection%2520in%2520AI-generated%2520radiology%2520reports.%2520Our%2520approach%250Aleverages%2520sequences%2520of%2520hidden%2520states%2520from%2520large%2520vision-language%2520models%2520to%250Aproduce%2520finding-level%2520hallucination%2520risk%2520scores.%2520We%2520evaluate%2520ReXTrust%2520on%2520a%250Asubset%2520of%2520the%2520MIMIC-CXR%2520dataset%2520and%2520demonstrate%2520superior%2520performance%2520compared%250Ato%2520existing%2520approaches%252C%2520achieving%2520an%2520AUROC%2520of%25200.8751%2520across%2520all%2520findings%2520and%250A0.8963%2520on%2520clinically%2520significant%2520findings.%2520Our%2520results%2520show%2520that%2520white-box%250Aapproaches%2520leveraging%2520model%2520hidden%2520states%2520can%2520provide%2520reliable%2520hallucination%250Adetection%2520for%2520medical%2520AI%2520systems%252C%2520potentially%2520improving%2520the%2520safety%2520and%250Areliability%2520of%2520automated%2520radiology%2520reporting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.15264v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReXTrust%3A%20A%20Model%20for%20Fine-Grained%20Hallucination%20Detection%20in%0A%20%20AI-Generated%20Radiology%20Reports&entry.906535625=Romain%20Hardy%20and%20Sung%20Eun%20Kim%20and%20Pranav%20Rajpurkar&entry.1292438233=%20%20The%20increasing%20adoption%20of%20AI-generated%20radiology%20reports%20necessitates%20robust%0Amethods%20for%20detecting%20hallucinations--false%20or%20unfounded%20statements%20that%20could%0Aimpact%20patient%20care.%20We%20present%20ReXTrust%2C%20a%20novel%20framework%20for%20fine-grained%0Ahallucination%20detection%20in%20AI-generated%20radiology%20reports.%20Our%20approach%0Aleverages%20sequences%20of%20hidden%20states%20from%20large%20vision-language%20models%20to%0Aproduce%20finding-level%20hallucination%20risk%20scores.%20We%20evaluate%20ReXTrust%20on%20a%0Asubset%20of%20the%20MIMIC-CXR%20dataset%20and%20demonstrate%20superior%20performance%20compared%0Ato%20existing%20approaches%2C%20achieving%20an%20AUROC%20of%200.8751%20across%20all%20findings%20and%0A0.8963%20on%20clinically%20significant%20findings.%20Our%20results%20show%20that%20white-box%0Aapproaches%20leveraging%20model%20hidden%20states%20can%20provide%20reliable%20hallucination%0Adetection%20for%20medical%20AI%20systems%2C%20potentially%20improving%20the%20safety%20and%0Areliability%20of%20automated%20radiology%20reporting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.15264v2&entry.124074799=Read"},
{"title": "Enhancing Annotated Bibliography Generation with LLM Ensembles", "author": "Sergio Bermejo", "abstract": "  This work proposes a novel approach to enhancing annotated bibliography\ngeneration through Large Language Model (LLM) ensembles. In particular,\nmultiple LLMs in different roles -- controllable text generation, evaluation,\nand summarization -- are introduced and validated using a systematic\nmethodology to enhance model performance in scholarly tasks. Output diversity\namong the ensemble that generates text is obtained using different LLM\nparameters, followed by an LLM acting as a judge to assess relevance, accuracy,\nand coherence. Responses selected by several combining strategies are then\nmerged and refined through summarization and redundancy removal techniques. The\npreliminary experimental validation demonstrates that the combined outputs from\nthe LLM ensemble improve coherence and relevance compared to individual\nresponses, leading to a 38% improvement in annotation quality and a 51%\nreduction in content redundancy, thus highlighting the potential for automating\ncomplex scholarly tasks while maintaining high-quality standards.\n", "link": "http://arxiv.org/abs/2412.20864v1", "date": "2024-12-30", "relevancy": 1.885, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5249}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4605}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Annotated%20Bibliography%20Generation%20with%20LLM%20Ensembles&body=Title%3A%20Enhancing%20Annotated%20Bibliography%20Generation%20with%20LLM%20Ensembles%0AAuthor%3A%20Sergio%20Bermejo%0AAbstract%3A%20%20%20This%20work%20proposes%20a%20novel%20approach%20to%20enhancing%20annotated%20bibliography%0Ageneration%20through%20Large%20Language%20Model%20%28LLM%29%20ensembles.%20In%20particular%2C%0Amultiple%20LLMs%20in%20different%20roles%20--%20controllable%20text%20generation%2C%20evaluation%2C%0Aand%20summarization%20--%20are%20introduced%20and%20validated%20using%20a%20systematic%0Amethodology%20to%20enhance%20model%20performance%20in%20scholarly%20tasks.%20Output%20diversity%0Aamong%20the%20ensemble%20that%20generates%20text%20is%20obtained%20using%20different%20LLM%0Aparameters%2C%20followed%20by%20an%20LLM%20acting%20as%20a%20judge%20to%20assess%20relevance%2C%20accuracy%2C%0Aand%20coherence.%20Responses%20selected%20by%20several%20combining%20strategies%20are%20then%0Amerged%20and%20refined%20through%20summarization%20and%20redundancy%20removal%20techniques.%20The%0Apreliminary%20experimental%20validation%20demonstrates%20that%20the%20combined%20outputs%20from%0Athe%20LLM%20ensemble%20improve%20coherence%20and%20relevance%20compared%20to%20individual%0Aresponses%2C%20leading%20to%20a%2038%25%20improvement%20in%20annotation%20quality%20and%20a%2051%25%0Areduction%20in%20content%20redundancy%2C%20thus%20highlighting%20the%20potential%20for%20automating%0Acomplex%20scholarly%20tasks%20while%20maintaining%20high-quality%20standards.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20864v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Annotated%2520Bibliography%2520Generation%2520with%2520LLM%2520Ensembles%26entry.906535625%3DSergio%2520Bermejo%26entry.1292438233%3D%2520%2520This%2520work%2520proposes%2520a%2520novel%2520approach%2520to%2520enhancing%2520annotated%2520bibliography%250Ageneration%2520through%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520ensembles.%2520In%2520particular%252C%250Amultiple%2520LLMs%2520in%2520different%2520roles%2520--%2520controllable%2520text%2520generation%252C%2520evaluation%252C%250Aand%2520summarization%2520--%2520are%2520introduced%2520and%2520validated%2520using%2520a%2520systematic%250Amethodology%2520to%2520enhance%2520model%2520performance%2520in%2520scholarly%2520tasks.%2520Output%2520diversity%250Aamong%2520the%2520ensemble%2520that%2520generates%2520text%2520is%2520obtained%2520using%2520different%2520LLM%250Aparameters%252C%2520followed%2520by%2520an%2520LLM%2520acting%2520as%2520a%2520judge%2520to%2520assess%2520relevance%252C%2520accuracy%252C%250Aand%2520coherence.%2520Responses%2520selected%2520by%2520several%2520combining%2520strategies%2520are%2520then%250Amerged%2520and%2520refined%2520through%2520summarization%2520and%2520redundancy%2520removal%2520techniques.%2520The%250Apreliminary%2520experimental%2520validation%2520demonstrates%2520that%2520the%2520combined%2520outputs%2520from%250Athe%2520LLM%2520ensemble%2520improve%2520coherence%2520and%2520relevance%2520compared%2520to%2520individual%250Aresponses%252C%2520leading%2520to%2520a%252038%2525%2520improvement%2520in%2520annotation%2520quality%2520and%2520a%252051%2525%250Areduction%2520in%2520content%2520redundancy%252C%2520thus%2520highlighting%2520the%2520potential%2520for%2520automating%250Acomplex%2520scholarly%2520tasks%2520while%2520maintaining%2520high-quality%2520standards.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20864v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Annotated%20Bibliography%20Generation%20with%20LLM%20Ensembles&entry.906535625=Sergio%20Bermejo&entry.1292438233=%20%20This%20work%20proposes%20a%20novel%20approach%20to%20enhancing%20annotated%20bibliography%0Ageneration%20through%20Large%20Language%20Model%20%28LLM%29%20ensembles.%20In%20particular%2C%0Amultiple%20LLMs%20in%20different%20roles%20--%20controllable%20text%20generation%2C%20evaluation%2C%0Aand%20summarization%20--%20are%20introduced%20and%20validated%20using%20a%20systematic%0Amethodology%20to%20enhance%20model%20performance%20in%20scholarly%20tasks.%20Output%20diversity%0Aamong%20the%20ensemble%20that%20generates%20text%20is%20obtained%20using%20different%20LLM%0Aparameters%2C%20followed%20by%20an%20LLM%20acting%20as%20a%20judge%20to%20assess%20relevance%2C%20accuracy%2C%0Aand%20coherence.%20Responses%20selected%20by%20several%20combining%20strategies%20are%20then%0Amerged%20and%20refined%20through%20summarization%20and%20redundancy%20removal%20techniques.%20The%0Apreliminary%20experimental%20validation%20demonstrates%20that%20the%20combined%20outputs%20from%0Athe%20LLM%20ensemble%20improve%20coherence%20and%20relevance%20compared%20to%20individual%0Aresponses%2C%20leading%20to%20a%2038%25%20improvement%20in%20annotation%20quality%20and%20a%2051%25%0Areduction%20in%20content%20redundancy%2C%20thus%20highlighting%20the%20potential%20for%20automating%0Acomplex%20scholarly%20tasks%20while%20maintaining%20high-quality%20standards.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20864v1&entry.124074799=Read"},
{"title": "LEASE: Offline Preference-based Reinforcement Learning with High Sample\n  Efficiency", "author": "Xiao-Yin Liu and Guotao Li and Xiao-Hu Zhou and Zeng-Guang Hou", "abstract": "  Offline preference-based reinforcement learning (PbRL) provides an effective\nway to overcome the challenges of designing reward and the high costs of online\ninteraction. However, since labeling preference needs real-time human feedback,\nacquiring sufficient preference labels is challenging. To solve this, this\npaper proposes a offLine prEference-bAsed RL with high Sample Efficiency\n(LEASE) algorithm, where a learned transition model is leveraged to generate\nunlabeled preference data. Considering the pretrained reward model may generate\nincorrect labels for unlabeled data, we design an uncertainty-aware mechanism\nto ensure the performance of reward model, where only high confidence and low\nvariance data are selected. Moreover, we provide the generalization bound of\nreward model to analyze the factors influencing reward accuracy, and\ndemonstrate that the policy learned by LEASE has theoretical improvement\nguarantee. The developed theory is based on state-action pair, which can be\neasily combined with other offline algorithms. The experimental results show\nthat LEASE can achieve comparable performance to baseline under fewer\npreference data without online interaction.\n", "link": "http://arxiv.org/abs/2412.21001v1", "date": "2024-12-30", "relevancy": 1.8806, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5047}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4658}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LEASE%3A%20Offline%20Preference-based%20Reinforcement%20Learning%20with%20High%20Sample%0A%20%20Efficiency&body=Title%3A%20LEASE%3A%20Offline%20Preference-based%20Reinforcement%20Learning%20with%20High%20Sample%0A%20%20Efficiency%0AAuthor%3A%20Xiao-Yin%20Liu%20and%20Guotao%20Li%20and%20Xiao-Hu%20Zhou%20and%20Zeng-Guang%20Hou%0AAbstract%3A%20%20%20Offline%20preference-based%20reinforcement%20learning%20%28PbRL%29%20provides%20an%20effective%0Away%20to%20overcome%20the%20challenges%20of%20designing%20reward%20and%20the%20high%20costs%20of%20online%0Ainteraction.%20However%2C%20since%20labeling%20preference%20needs%20real-time%20human%20feedback%2C%0Aacquiring%20sufficient%20preference%20labels%20is%20challenging.%20To%20solve%20this%2C%20this%0Apaper%20proposes%20a%20offLine%20prEference-bAsed%20RL%20with%20high%20Sample%20Efficiency%0A%28LEASE%29%20algorithm%2C%20where%20a%20learned%20transition%20model%20is%20leveraged%20to%20generate%0Aunlabeled%20preference%20data.%20Considering%20the%20pretrained%20reward%20model%20may%20generate%0Aincorrect%20labels%20for%20unlabeled%20data%2C%20we%20design%20an%20uncertainty-aware%20mechanism%0Ato%20ensure%20the%20performance%20of%20reward%20model%2C%20where%20only%20high%20confidence%20and%20low%0Avariance%20data%20are%20selected.%20Moreover%2C%20we%20provide%20the%20generalization%20bound%20of%0Areward%20model%20to%20analyze%20the%20factors%20influencing%20reward%20accuracy%2C%20and%0Ademonstrate%20that%20the%20policy%20learned%20by%20LEASE%20has%20theoretical%20improvement%0Aguarantee.%20The%20developed%20theory%20is%20based%20on%20state-action%20pair%2C%20which%20can%20be%0Aeasily%20combined%20with%20other%20offline%20algorithms.%20The%20experimental%20results%20show%0Athat%20LEASE%20can%20achieve%20comparable%20performance%20to%20baseline%20under%20fewer%0Apreference%20data%20without%20online%20interaction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21001v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLEASE%253A%2520Offline%2520Preference-based%2520Reinforcement%2520Learning%2520with%2520High%2520Sample%250A%2520%2520Efficiency%26entry.906535625%3DXiao-Yin%2520Liu%2520and%2520Guotao%2520Li%2520and%2520Xiao-Hu%2520Zhou%2520and%2520Zeng-Guang%2520Hou%26entry.1292438233%3D%2520%2520Offline%2520preference-based%2520reinforcement%2520learning%2520%2528PbRL%2529%2520provides%2520an%2520effective%250Away%2520to%2520overcome%2520the%2520challenges%2520of%2520designing%2520reward%2520and%2520the%2520high%2520costs%2520of%2520online%250Ainteraction.%2520However%252C%2520since%2520labeling%2520preference%2520needs%2520real-time%2520human%2520feedback%252C%250Aacquiring%2520sufficient%2520preference%2520labels%2520is%2520challenging.%2520To%2520solve%2520this%252C%2520this%250Apaper%2520proposes%2520a%2520offLine%2520prEference-bAsed%2520RL%2520with%2520high%2520Sample%2520Efficiency%250A%2528LEASE%2529%2520algorithm%252C%2520where%2520a%2520learned%2520transition%2520model%2520is%2520leveraged%2520to%2520generate%250Aunlabeled%2520preference%2520data.%2520Considering%2520the%2520pretrained%2520reward%2520model%2520may%2520generate%250Aincorrect%2520labels%2520for%2520unlabeled%2520data%252C%2520we%2520design%2520an%2520uncertainty-aware%2520mechanism%250Ato%2520ensure%2520the%2520performance%2520of%2520reward%2520model%252C%2520where%2520only%2520high%2520confidence%2520and%2520low%250Avariance%2520data%2520are%2520selected.%2520Moreover%252C%2520we%2520provide%2520the%2520generalization%2520bound%2520of%250Areward%2520model%2520to%2520analyze%2520the%2520factors%2520influencing%2520reward%2520accuracy%252C%2520and%250Ademonstrate%2520that%2520the%2520policy%2520learned%2520by%2520LEASE%2520has%2520theoretical%2520improvement%250Aguarantee.%2520The%2520developed%2520theory%2520is%2520based%2520on%2520state-action%2520pair%252C%2520which%2520can%2520be%250Aeasily%2520combined%2520with%2520other%2520offline%2520algorithms.%2520The%2520experimental%2520results%2520show%250Athat%2520LEASE%2520can%2520achieve%2520comparable%2520performance%2520to%2520baseline%2520under%2520fewer%250Apreference%2520data%2520without%2520online%2520interaction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21001v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LEASE%3A%20Offline%20Preference-based%20Reinforcement%20Learning%20with%20High%20Sample%0A%20%20Efficiency&entry.906535625=Xiao-Yin%20Liu%20and%20Guotao%20Li%20and%20Xiao-Hu%20Zhou%20and%20Zeng-Guang%20Hou&entry.1292438233=%20%20Offline%20preference-based%20reinforcement%20learning%20%28PbRL%29%20provides%20an%20effective%0Away%20to%20overcome%20the%20challenges%20of%20designing%20reward%20and%20the%20high%20costs%20of%20online%0Ainteraction.%20However%2C%20since%20labeling%20preference%20needs%20real-time%20human%20feedback%2C%0Aacquiring%20sufficient%20preference%20labels%20is%20challenging.%20To%20solve%20this%2C%20this%0Apaper%20proposes%20a%20offLine%20prEference-bAsed%20RL%20with%20high%20Sample%20Efficiency%0A%28LEASE%29%20algorithm%2C%20where%20a%20learned%20transition%20model%20is%20leveraged%20to%20generate%0Aunlabeled%20preference%20data.%20Considering%20the%20pretrained%20reward%20model%20may%20generate%0Aincorrect%20labels%20for%20unlabeled%20data%2C%20we%20design%20an%20uncertainty-aware%20mechanism%0Ato%20ensure%20the%20performance%20of%20reward%20model%2C%20where%20only%20high%20confidence%20and%20low%0Avariance%20data%20are%20selected.%20Moreover%2C%20we%20provide%20the%20generalization%20bound%20of%0Areward%20model%20to%20analyze%20the%20factors%20influencing%20reward%20accuracy%2C%20and%0Ademonstrate%20that%20the%20policy%20learned%20by%20LEASE%20has%20theoretical%20improvement%0Aguarantee.%20The%20developed%20theory%20is%20based%20on%20state-action%20pair%2C%20which%20can%20be%0Aeasily%20combined%20with%20other%20offline%20algorithms.%20The%20experimental%20results%20show%0Athat%20LEASE%20can%20achieve%20comparable%20performance%20to%20baseline%20under%20fewer%0Apreference%20data%20without%20online%20interaction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21001v1&entry.124074799=Read"},
{"title": "Nash CoT: Multi-Path Inference with Preference Equilibrium", "author": "Ziqi Zhang and Cunxiang Wang and Xiong Xiao and Yue Zhang and Donglin Wang", "abstract": "  Chain of thought (CoT) is a reasoning framework that can enhance the\nperformance of Large Language Models (LLMs) on complex inference tasks. In\nparticular, among various studies related to CoT, multi-path inference stands\nout as a simple yet effective improvement. However, there is no optimal setting\nfor the number of inference paths. Therefore, we have to increase the number of\ninference paths to obtain better results, which in turn increases the inference\ncost. To address this limitation, we can utilize question-related role\ntemplates to guide LLMs into relevant roles, thereby increasing the possibility\nof correct inferences for each path and further reducing dependence on the\nnumber of inference paths while improving reasoning accuracy. However, placing\nLLMs into specific roles may reduce their reasoning diversity and performance\non a few tasks where role dependence is low. To alleviate the excessive\nimmersion of the LLM into a specific role, we propose Nash CoT by constructing\na game system on each path that balances the generation from role-specific\nLLMs' and the general LLMs' generation, thereby ensuring both effective role\nadoption and diversity in LLM generation further maintaining the performance of\nmulti-path inference while reducing the requirement of the number of inference\npaths. We evaluate Nash CoT across various inference tasks, including Arabic\nReasoning, Commonsense Question Answering, and Symbolic Inference, achieving\nresults that are comparable to or better than those of multi-path CoT with the\nequal number of inference paths.\n", "link": "http://arxiv.org/abs/2407.07099v3", "date": "2024-12-30", "relevancy": 1.8784, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5057}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4666}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4582}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nash%20CoT%3A%20Multi-Path%20Inference%20with%20Preference%20Equilibrium&body=Title%3A%20Nash%20CoT%3A%20Multi-Path%20Inference%20with%20Preference%20Equilibrium%0AAuthor%3A%20Ziqi%20Zhang%20and%20Cunxiang%20Wang%20and%20Xiong%20Xiao%20and%20Yue%20Zhang%20and%20Donglin%20Wang%0AAbstract%3A%20%20%20Chain%20of%20thought%20%28CoT%29%20is%20a%20reasoning%20framework%20that%20can%20enhance%20the%0Aperformance%20of%20Large%20Language%20Models%20%28LLMs%29%20on%20complex%20inference%20tasks.%20In%0Aparticular%2C%20among%20various%20studies%20related%20to%20CoT%2C%20multi-path%20inference%20stands%0Aout%20as%20a%20simple%20yet%20effective%20improvement.%20However%2C%20there%20is%20no%20optimal%20setting%0Afor%20the%20number%20of%20inference%20paths.%20Therefore%2C%20we%20have%20to%20increase%20the%20number%20of%0Ainference%20paths%20to%20obtain%20better%20results%2C%20which%20in%20turn%20increases%20the%20inference%0Acost.%20To%20address%20this%20limitation%2C%20we%20can%20utilize%20question-related%20role%0Atemplates%20to%20guide%20LLMs%20into%20relevant%20roles%2C%20thereby%20increasing%20the%20possibility%0Aof%20correct%20inferences%20for%20each%20path%20and%20further%20reducing%20dependence%20on%20the%0Anumber%20of%20inference%20paths%20while%20improving%20reasoning%20accuracy.%20However%2C%20placing%0ALLMs%20into%20specific%20roles%20may%20reduce%20their%20reasoning%20diversity%20and%20performance%0Aon%20a%20few%20tasks%20where%20role%20dependence%20is%20low.%20To%20alleviate%20the%20excessive%0Aimmersion%20of%20the%20LLM%20into%20a%20specific%20role%2C%20we%20propose%20Nash%20CoT%20by%20constructing%0Aa%20game%20system%20on%20each%20path%20that%20balances%20the%20generation%20from%20role-specific%0ALLMs%27%20and%20the%20general%20LLMs%27%20generation%2C%20thereby%20ensuring%20both%20effective%20role%0Aadoption%20and%20diversity%20in%20LLM%20generation%20further%20maintaining%20the%20performance%20of%0Amulti-path%20inference%20while%20reducing%20the%20requirement%20of%20the%20number%20of%20inference%0Apaths.%20We%20evaluate%20Nash%20CoT%20across%20various%20inference%20tasks%2C%20including%20Arabic%0AReasoning%2C%20Commonsense%20Question%20Answering%2C%20and%20Symbolic%20Inference%2C%20achieving%0Aresults%20that%20are%20comparable%20to%20or%20better%20than%20those%20of%20multi-path%20CoT%20with%20the%0Aequal%20number%20of%20inference%20paths.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07099v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNash%2520CoT%253A%2520Multi-Path%2520Inference%2520with%2520Preference%2520Equilibrium%26entry.906535625%3DZiqi%2520Zhang%2520and%2520Cunxiang%2520Wang%2520and%2520Xiong%2520Xiao%2520and%2520Yue%2520Zhang%2520and%2520Donglin%2520Wang%26entry.1292438233%3D%2520%2520Chain%2520of%2520thought%2520%2528CoT%2529%2520is%2520a%2520reasoning%2520framework%2520that%2520can%2520enhance%2520the%250Aperformance%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520on%2520complex%2520inference%2520tasks.%2520In%250Aparticular%252C%2520among%2520various%2520studies%2520related%2520to%2520CoT%252C%2520multi-path%2520inference%2520stands%250Aout%2520as%2520a%2520simple%2520yet%2520effective%2520improvement.%2520However%252C%2520there%2520is%2520no%2520optimal%2520setting%250Afor%2520the%2520number%2520of%2520inference%2520paths.%2520Therefore%252C%2520we%2520have%2520to%2520increase%2520the%2520number%2520of%250Ainference%2520paths%2520to%2520obtain%2520better%2520results%252C%2520which%2520in%2520turn%2520increases%2520the%2520inference%250Acost.%2520To%2520address%2520this%2520limitation%252C%2520we%2520can%2520utilize%2520question-related%2520role%250Atemplates%2520to%2520guide%2520LLMs%2520into%2520relevant%2520roles%252C%2520thereby%2520increasing%2520the%2520possibility%250Aof%2520correct%2520inferences%2520for%2520each%2520path%2520and%2520further%2520reducing%2520dependence%2520on%2520the%250Anumber%2520of%2520inference%2520paths%2520while%2520improving%2520reasoning%2520accuracy.%2520However%252C%2520placing%250ALLMs%2520into%2520specific%2520roles%2520may%2520reduce%2520their%2520reasoning%2520diversity%2520and%2520performance%250Aon%2520a%2520few%2520tasks%2520where%2520role%2520dependence%2520is%2520low.%2520To%2520alleviate%2520the%2520excessive%250Aimmersion%2520of%2520the%2520LLM%2520into%2520a%2520specific%2520role%252C%2520we%2520propose%2520Nash%2520CoT%2520by%2520constructing%250Aa%2520game%2520system%2520on%2520each%2520path%2520that%2520balances%2520the%2520generation%2520from%2520role-specific%250ALLMs%2527%2520and%2520the%2520general%2520LLMs%2527%2520generation%252C%2520thereby%2520ensuring%2520both%2520effective%2520role%250Aadoption%2520and%2520diversity%2520in%2520LLM%2520generation%2520further%2520maintaining%2520the%2520performance%2520of%250Amulti-path%2520inference%2520while%2520reducing%2520the%2520requirement%2520of%2520the%2520number%2520of%2520inference%250Apaths.%2520We%2520evaluate%2520Nash%2520CoT%2520across%2520various%2520inference%2520tasks%252C%2520including%2520Arabic%250AReasoning%252C%2520Commonsense%2520Question%2520Answering%252C%2520and%2520Symbolic%2520Inference%252C%2520achieving%250Aresults%2520that%2520are%2520comparable%2520to%2520or%2520better%2520than%2520those%2520of%2520multi-path%2520CoT%2520with%2520the%250Aequal%2520number%2520of%2520inference%2520paths.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07099v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nash%20CoT%3A%20Multi-Path%20Inference%20with%20Preference%20Equilibrium&entry.906535625=Ziqi%20Zhang%20and%20Cunxiang%20Wang%20and%20Xiong%20Xiao%20and%20Yue%20Zhang%20and%20Donglin%20Wang&entry.1292438233=%20%20Chain%20of%20thought%20%28CoT%29%20is%20a%20reasoning%20framework%20that%20can%20enhance%20the%0Aperformance%20of%20Large%20Language%20Models%20%28LLMs%29%20on%20complex%20inference%20tasks.%20In%0Aparticular%2C%20among%20various%20studies%20related%20to%20CoT%2C%20multi-path%20inference%20stands%0Aout%20as%20a%20simple%20yet%20effective%20improvement.%20However%2C%20there%20is%20no%20optimal%20setting%0Afor%20the%20number%20of%20inference%20paths.%20Therefore%2C%20we%20have%20to%20increase%20the%20number%20of%0Ainference%20paths%20to%20obtain%20better%20results%2C%20which%20in%20turn%20increases%20the%20inference%0Acost.%20To%20address%20this%20limitation%2C%20we%20can%20utilize%20question-related%20role%0Atemplates%20to%20guide%20LLMs%20into%20relevant%20roles%2C%20thereby%20increasing%20the%20possibility%0Aof%20correct%20inferences%20for%20each%20path%20and%20further%20reducing%20dependence%20on%20the%0Anumber%20of%20inference%20paths%20while%20improving%20reasoning%20accuracy.%20However%2C%20placing%0ALLMs%20into%20specific%20roles%20may%20reduce%20their%20reasoning%20diversity%20and%20performance%0Aon%20a%20few%20tasks%20where%20role%20dependence%20is%20low.%20To%20alleviate%20the%20excessive%0Aimmersion%20of%20the%20LLM%20into%20a%20specific%20role%2C%20we%20propose%20Nash%20CoT%20by%20constructing%0Aa%20game%20system%20on%20each%20path%20that%20balances%20the%20generation%20from%20role-specific%0ALLMs%27%20and%20the%20general%20LLMs%27%20generation%2C%20thereby%20ensuring%20both%20effective%20role%0Aadoption%20and%20diversity%20in%20LLM%20generation%20further%20maintaining%20the%20performance%20of%0Amulti-path%20inference%20while%20reducing%20the%20requirement%20of%20the%20number%20of%20inference%0Apaths.%20We%20evaluate%20Nash%20CoT%20across%20various%20inference%20tasks%2C%20including%20Arabic%0AReasoning%2C%20Commonsense%20Question%20Answering%2C%20and%20Symbolic%20Inference%2C%20achieving%0Aresults%20that%20are%20comparable%20to%20or%20better%20than%20those%20of%20multi-path%20CoT%20with%20the%0Aequal%20number%20of%20inference%20paths.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07099v3&entry.124074799=Read"},
{"title": "Active Learning with Variational Quantum Circuits for Quantum Process\n  Tomography", "author": "Jiaqi Yang and Xiaohua Xu and Wei Xie", "abstract": "  Quantum process tomography (QPT), used for reconstruction of an unknown\nquantum process from measurement data, is a fundamental tool for the diagnostic\nand full characterization of quantum systems. It relies on querying a set of\nquantum states as input to the quantum process. Previous works commonly use a\nstraightforward strategy to select a set of quantum states randomly,\noverlooking differences in informativeness among quantum states. Since querying\nthe quantum system requires multiple experiments that can be prohibitively\ncostly, it is always the case that there are not enough quantum states for\nhigh-quality reconstruction. In this paper, we propose a general framework for\nactive learning (AL) to adaptively select a set of informative quantum states\nthat improves the reconstruction most efficiently. In particular, we introduce\na learning framework that leverages the widely-used variational quantum\ncircuits (VQCs) to perform the QPT task and integrate our AL algorithms into\nthe query step. We design and evaluate three various types of AL algorithms:\ncommittee-based, uncertainty-based, and diversity-based, each exhibiting\ndistinct advantages in terms of performance and computational cost.\nAdditionally, we provide a guideline for selecting algorithms suitable for\ndifferent scenarios. Numerical results demonstrate that our algorithms achieve\nsignificantly improved reconstruction compared to the baseline method that\nselects a set of quantum states randomly. Moreover, these results suggest that\nactive learning based approaches are applicable to other complicated learning\ntasks in large-scale quantum information processing.\n", "link": "http://arxiv.org/abs/2412.20925v1", "date": "2024-12-30", "relevancy": 1.8763, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4697}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4697}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Learning%20with%20Variational%20Quantum%20Circuits%20for%20Quantum%20Process%0A%20%20Tomography&body=Title%3A%20Active%20Learning%20with%20Variational%20Quantum%20Circuits%20for%20Quantum%20Process%0A%20%20Tomography%0AAuthor%3A%20Jiaqi%20Yang%20and%20Xiaohua%20Xu%20and%20Wei%20Xie%0AAbstract%3A%20%20%20Quantum%20process%20tomography%20%28QPT%29%2C%20used%20for%20reconstruction%20of%20an%20unknown%0Aquantum%20process%20from%20measurement%20data%2C%20is%20a%20fundamental%20tool%20for%20the%20diagnostic%0Aand%20full%20characterization%20of%20quantum%20systems.%20It%20relies%20on%20querying%20a%20set%20of%0Aquantum%20states%20as%20input%20to%20the%20quantum%20process.%20Previous%20works%20commonly%20use%20a%0Astraightforward%20strategy%20to%20select%20a%20set%20of%20quantum%20states%20randomly%2C%0Aoverlooking%20differences%20in%20informativeness%20among%20quantum%20states.%20Since%20querying%0Athe%20quantum%20system%20requires%20multiple%20experiments%20that%20can%20be%20prohibitively%0Acostly%2C%20it%20is%20always%20the%20case%20that%20there%20are%20not%20enough%20quantum%20states%20for%0Ahigh-quality%20reconstruction.%20In%20this%20paper%2C%20we%20propose%20a%20general%20framework%20for%0Aactive%20learning%20%28AL%29%20to%20adaptively%20select%20a%20set%20of%20informative%20quantum%20states%0Athat%20improves%20the%20reconstruction%20most%20efficiently.%20In%20particular%2C%20we%20introduce%0Aa%20learning%20framework%20that%20leverages%20the%20widely-used%20variational%20quantum%0Acircuits%20%28VQCs%29%20to%20perform%20the%20QPT%20task%20and%20integrate%20our%20AL%20algorithms%20into%0Athe%20query%20step.%20We%20design%20and%20evaluate%20three%20various%20types%20of%20AL%20algorithms%3A%0Acommittee-based%2C%20uncertainty-based%2C%20and%20diversity-based%2C%20each%20exhibiting%0Adistinct%20advantages%20in%20terms%20of%20performance%20and%20computational%20cost.%0AAdditionally%2C%20we%20provide%20a%20guideline%20for%20selecting%20algorithms%20suitable%20for%0Adifferent%20scenarios.%20Numerical%20results%20demonstrate%20that%20our%20algorithms%20achieve%0Asignificantly%20improved%20reconstruction%20compared%20to%20the%20baseline%20method%20that%0Aselects%20a%20set%20of%20quantum%20states%20randomly.%20Moreover%2C%20these%20results%20suggest%20that%0Aactive%20learning%20based%20approaches%20are%20applicable%20to%20other%20complicated%20learning%0Atasks%20in%20large-scale%20quantum%20information%20processing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20925v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Learning%2520with%2520Variational%2520Quantum%2520Circuits%2520for%2520Quantum%2520Process%250A%2520%2520Tomography%26entry.906535625%3DJiaqi%2520Yang%2520and%2520Xiaohua%2520Xu%2520and%2520Wei%2520Xie%26entry.1292438233%3D%2520%2520Quantum%2520process%2520tomography%2520%2528QPT%2529%252C%2520used%2520for%2520reconstruction%2520of%2520an%2520unknown%250Aquantum%2520process%2520from%2520measurement%2520data%252C%2520is%2520a%2520fundamental%2520tool%2520for%2520the%2520diagnostic%250Aand%2520full%2520characterization%2520of%2520quantum%2520systems.%2520It%2520relies%2520on%2520querying%2520a%2520set%2520of%250Aquantum%2520states%2520as%2520input%2520to%2520the%2520quantum%2520process.%2520Previous%2520works%2520commonly%2520use%2520a%250Astraightforward%2520strategy%2520to%2520select%2520a%2520set%2520of%2520quantum%2520states%2520randomly%252C%250Aoverlooking%2520differences%2520in%2520informativeness%2520among%2520quantum%2520states.%2520Since%2520querying%250Athe%2520quantum%2520system%2520requires%2520multiple%2520experiments%2520that%2520can%2520be%2520prohibitively%250Acostly%252C%2520it%2520is%2520always%2520the%2520case%2520that%2520there%2520are%2520not%2520enough%2520quantum%2520states%2520for%250Ahigh-quality%2520reconstruction.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520general%2520framework%2520for%250Aactive%2520learning%2520%2528AL%2529%2520to%2520adaptively%2520select%2520a%2520set%2520of%2520informative%2520quantum%2520states%250Athat%2520improves%2520the%2520reconstruction%2520most%2520efficiently.%2520In%2520particular%252C%2520we%2520introduce%250Aa%2520learning%2520framework%2520that%2520leverages%2520the%2520widely-used%2520variational%2520quantum%250Acircuits%2520%2528VQCs%2529%2520to%2520perform%2520the%2520QPT%2520task%2520and%2520integrate%2520our%2520AL%2520algorithms%2520into%250Athe%2520query%2520step.%2520We%2520design%2520and%2520evaluate%2520three%2520various%2520types%2520of%2520AL%2520algorithms%253A%250Acommittee-based%252C%2520uncertainty-based%252C%2520and%2520diversity-based%252C%2520each%2520exhibiting%250Adistinct%2520advantages%2520in%2520terms%2520of%2520performance%2520and%2520computational%2520cost.%250AAdditionally%252C%2520we%2520provide%2520a%2520guideline%2520for%2520selecting%2520algorithms%2520suitable%2520for%250Adifferent%2520scenarios.%2520Numerical%2520results%2520demonstrate%2520that%2520our%2520algorithms%2520achieve%250Asignificantly%2520improved%2520reconstruction%2520compared%2520to%2520the%2520baseline%2520method%2520that%250Aselects%2520a%2520set%2520of%2520quantum%2520states%2520randomly.%2520Moreover%252C%2520these%2520results%2520suggest%2520that%250Aactive%2520learning%2520based%2520approaches%2520are%2520applicable%2520to%2520other%2520complicated%2520learning%250Atasks%2520in%2520large-scale%2520quantum%2520information%2520processing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20925v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Learning%20with%20Variational%20Quantum%20Circuits%20for%20Quantum%20Process%0A%20%20Tomography&entry.906535625=Jiaqi%20Yang%20and%20Xiaohua%20Xu%20and%20Wei%20Xie&entry.1292438233=%20%20Quantum%20process%20tomography%20%28QPT%29%2C%20used%20for%20reconstruction%20of%20an%20unknown%0Aquantum%20process%20from%20measurement%20data%2C%20is%20a%20fundamental%20tool%20for%20the%20diagnostic%0Aand%20full%20characterization%20of%20quantum%20systems.%20It%20relies%20on%20querying%20a%20set%20of%0Aquantum%20states%20as%20input%20to%20the%20quantum%20process.%20Previous%20works%20commonly%20use%20a%0Astraightforward%20strategy%20to%20select%20a%20set%20of%20quantum%20states%20randomly%2C%0Aoverlooking%20differences%20in%20informativeness%20among%20quantum%20states.%20Since%20querying%0Athe%20quantum%20system%20requires%20multiple%20experiments%20that%20can%20be%20prohibitively%0Acostly%2C%20it%20is%20always%20the%20case%20that%20there%20are%20not%20enough%20quantum%20states%20for%0Ahigh-quality%20reconstruction.%20In%20this%20paper%2C%20we%20propose%20a%20general%20framework%20for%0Aactive%20learning%20%28AL%29%20to%20adaptively%20select%20a%20set%20of%20informative%20quantum%20states%0Athat%20improves%20the%20reconstruction%20most%20efficiently.%20In%20particular%2C%20we%20introduce%0Aa%20learning%20framework%20that%20leverages%20the%20widely-used%20variational%20quantum%0Acircuits%20%28VQCs%29%20to%20perform%20the%20QPT%20task%20and%20integrate%20our%20AL%20algorithms%20into%0Athe%20query%20step.%20We%20design%20and%20evaluate%20three%20various%20types%20of%20AL%20algorithms%3A%0Acommittee-based%2C%20uncertainty-based%2C%20and%20diversity-based%2C%20each%20exhibiting%0Adistinct%20advantages%20in%20terms%20of%20performance%20and%20computational%20cost.%0AAdditionally%2C%20we%20provide%20a%20guideline%20for%20selecting%20algorithms%20suitable%20for%0Adifferent%20scenarios.%20Numerical%20results%20demonstrate%20that%20our%20algorithms%20achieve%0Asignificantly%20improved%20reconstruction%20compared%20to%20the%20baseline%20method%20that%0Aselects%20a%20set%20of%20quantum%20states%20randomly.%20Moreover%2C%20these%20results%20suggest%20that%0Aactive%20learning%20based%20approaches%20are%20applicable%20to%20other%20complicated%20learning%0Atasks%20in%20large-scale%20quantum%20information%20processing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20925v1&entry.124074799=Read"},
{"title": "From Interests to Insights: An LLM Approach to Course Recommendations\n  Using Natural Language Queries", "author": "Hugh Van Deventer and Mark Mills and August Evrard", "abstract": "  Most universities in the United States encourage their students to explore\nacademic areas before declaring a major and to acquire academic breadth by\nsatisfying a variety of requirements. Each term, students must choose among\nmany thousands of offerings, spanning dozens of subject areas, a handful of\ncourses to take. The curricular environment is also dynamic, and poor\ncommunication and search functions on campus can limit a student's ability to\ndiscover new courses of interest. To support both students and their advisers\nin such a setting, we explore a novel Large Language Model (LLM) course\nrecommendation system that applies a Retrieval Augmented Generation (RAG)\nmethod to the corpus of course descriptions. The system first generates an\n'ideal' course description based on the user's query. This description is\nconverted into a search vector using embeddings, which is then used to find\nactual courses with similar content by comparing embedding similarities. We\ndescribe the method and assess the quality and fairness of some example\nprompts. Steps to deploy a pilot system on campus are discussed.\n", "link": "http://arxiv.org/abs/2412.19312v2", "date": "2024-12-30", "relevancy": 1.869, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4729}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4661}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Interests%20to%20Insights%3A%20An%20LLM%20Approach%20to%20Course%20Recommendations%0A%20%20Using%20Natural%20Language%20Queries&body=Title%3A%20From%20Interests%20to%20Insights%3A%20An%20LLM%20Approach%20to%20Course%20Recommendations%0A%20%20Using%20Natural%20Language%20Queries%0AAuthor%3A%20Hugh%20Van%20Deventer%20and%20Mark%20Mills%20and%20August%20Evrard%0AAbstract%3A%20%20%20Most%20universities%20in%20the%20United%20States%20encourage%20their%20students%20to%20explore%0Aacademic%20areas%20before%20declaring%20a%20major%20and%20to%20acquire%20academic%20breadth%20by%0Asatisfying%20a%20variety%20of%20requirements.%20Each%20term%2C%20students%20must%20choose%20among%0Amany%20thousands%20of%20offerings%2C%20spanning%20dozens%20of%20subject%20areas%2C%20a%20handful%20of%0Acourses%20to%20take.%20The%20curricular%20environment%20is%20also%20dynamic%2C%20and%20poor%0Acommunication%20and%20search%20functions%20on%20campus%20can%20limit%20a%20student%27s%20ability%20to%0Adiscover%20new%20courses%20of%20interest.%20To%20support%20both%20students%20and%20their%20advisers%0Ain%20such%20a%20setting%2C%20we%20explore%20a%20novel%20Large%20Language%20Model%20%28LLM%29%20course%0Arecommendation%20system%20that%20applies%20a%20Retrieval%20Augmented%20Generation%20%28RAG%29%0Amethod%20to%20the%20corpus%20of%20course%20descriptions.%20The%20system%20first%20generates%20an%0A%27ideal%27%20course%20description%20based%20on%20the%20user%27s%20query.%20This%20description%20is%0Aconverted%20into%20a%20search%20vector%20using%20embeddings%2C%20which%20is%20then%20used%20to%20find%0Aactual%20courses%20with%20similar%20content%20by%20comparing%20embedding%20similarities.%20We%0Adescribe%20the%20method%20and%20assess%20the%20quality%20and%20fairness%20of%20some%20example%0Aprompts.%20Steps%20to%20deploy%20a%20pilot%20system%20on%20campus%20are%20discussed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19312v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Interests%2520to%2520Insights%253A%2520An%2520LLM%2520Approach%2520to%2520Course%2520Recommendations%250A%2520%2520Using%2520Natural%2520Language%2520Queries%26entry.906535625%3DHugh%2520Van%2520Deventer%2520and%2520Mark%2520Mills%2520and%2520August%2520Evrard%26entry.1292438233%3D%2520%2520Most%2520universities%2520in%2520the%2520United%2520States%2520encourage%2520their%2520students%2520to%2520explore%250Aacademic%2520areas%2520before%2520declaring%2520a%2520major%2520and%2520to%2520acquire%2520academic%2520breadth%2520by%250Asatisfying%2520a%2520variety%2520of%2520requirements.%2520Each%2520term%252C%2520students%2520must%2520choose%2520among%250Amany%2520thousands%2520of%2520offerings%252C%2520spanning%2520dozens%2520of%2520subject%2520areas%252C%2520a%2520handful%2520of%250Acourses%2520to%2520take.%2520The%2520curricular%2520environment%2520is%2520also%2520dynamic%252C%2520and%2520poor%250Acommunication%2520and%2520search%2520functions%2520on%2520campus%2520can%2520limit%2520a%2520student%2527s%2520ability%2520to%250Adiscover%2520new%2520courses%2520of%2520interest.%2520To%2520support%2520both%2520students%2520and%2520their%2520advisers%250Ain%2520such%2520a%2520setting%252C%2520we%2520explore%2520a%2520novel%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520course%250Arecommendation%2520system%2520that%2520applies%2520a%2520Retrieval%2520Augmented%2520Generation%2520%2528RAG%2529%250Amethod%2520to%2520the%2520corpus%2520of%2520course%2520descriptions.%2520The%2520system%2520first%2520generates%2520an%250A%2527ideal%2527%2520course%2520description%2520based%2520on%2520the%2520user%2527s%2520query.%2520This%2520description%2520is%250Aconverted%2520into%2520a%2520search%2520vector%2520using%2520embeddings%252C%2520which%2520is%2520then%2520used%2520to%2520find%250Aactual%2520courses%2520with%2520similar%2520content%2520by%2520comparing%2520embedding%2520similarities.%2520We%250Adescribe%2520the%2520method%2520and%2520assess%2520the%2520quality%2520and%2520fairness%2520of%2520some%2520example%250Aprompts.%2520Steps%2520to%2520deploy%2520a%2520pilot%2520system%2520on%2520campus%2520are%2520discussed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19312v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Interests%20to%20Insights%3A%20An%20LLM%20Approach%20to%20Course%20Recommendations%0A%20%20Using%20Natural%20Language%20Queries&entry.906535625=Hugh%20Van%20Deventer%20and%20Mark%20Mills%20and%20August%20Evrard&entry.1292438233=%20%20Most%20universities%20in%20the%20United%20States%20encourage%20their%20students%20to%20explore%0Aacademic%20areas%20before%20declaring%20a%20major%20and%20to%20acquire%20academic%20breadth%20by%0Asatisfying%20a%20variety%20of%20requirements.%20Each%20term%2C%20students%20must%20choose%20among%0Amany%20thousands%20of%20offerings%2C%20spanning%20dozens%20of%20subject%20areas%2C%20a%20handful%20of%0Acourses%20to%20take.%20The%20curricular%20environment%20is%20also%20dynamic%2C%20and%20poor%0Acommunication%20and%20search%20functions%20on%20campus%20can%20limit%20a%20student%27s%20ability%20to%0Adiscover%20new%20courses%20of%20interest.%20To%20support%20both%20students%20and%20their%20advisers%0Ain%20such%20a%20setting%2C%20we%20explore%20a%20novel%20Large%20Language%20Model%20%28LLM%29%20course%0Arecommendation%20system%20that%20applies%20a%20Retrieval%20Augmented%20Generation%20%28RAG%29%0Amethod%20to%20the%20corpus%20of%20course%20descriptions.%20The%20system%20first%20generates%20an%0A%27ideal%27%20course%20description%20based%20on%20the%20user%27s%20query.%20This%20description%20is%0Aconverted%20into%20a%20search%20vector%20using%20embeddings%2C%20which%20is%20then%20used%20to%20find%0Aactual%20courses%20with%20similar%20content%20by%20comparing%20embedding%20similarities.%20We%0Adescribe%20the%20method%20and%20assess%20the%20quality%20and%20fairness%20of%20some%20example%0Aprompts.%20Steps%20to%20deploy%20a%20pilot%20system%20on%20campus%20are%20discussed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19312v2&entry.124074799=Read"},
{"title": "Learning Epidemiological Dynamics via the Finite Expression Method", "author": "Jianda Du and Senwei Liang and Chunmei Wang", "abstract": "  Modeling and forecasting the spread of infectious diseases is essential for\neffective public health decision-making. Traditional epidemiological models\nrely on expert-defined frameworks to describe complex dynamics, while neural\nnetworks, despite their predictive power, often lack interpretability due to\ntheir ``black-box\" nature. This paper introduces the Finite Expression Method,\na symbolic learning framework that leverages reinforcement learning to derive\nexplicit mathematical expressions for epidemiological dynamics. Through\nnumerical experiments on both synthetic and real-world datasets, FEX\ndemonstrates high accuracy in modeling and predicting disease spread, while\nuncovering explicit relationships among epidemiological variables. These\nresults highlight FEX as a powerful tool for infectious disease modeling,\ncombining interpretability with strong predictive performance to support\npractical applications in public health.\n", "link": "http://arxiv.org/abs/2412.21049v1", "date": "2024-12-30", "relevancy": 1.8685, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4715}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4705}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Epidemiological%20Dynamics%20via%20the%20Finite%20Expression%20Method&body=Title%3A%20Learning%20Epidemiological%20Dynamics%20via%20the%20Finite%20Expression%20Method%0AAuthor%3A%20Jianda%20Du%20and%20Senwei%20Liang%20and%20Chunmei%20Wang%0AAbstract%3A%20%20%20Modeling%20and%20forecasting%20the%20spread%20of%20infectious%20diseases%20is%20essential%20for%0Aeffective%20public%20health%20decision-making.%20Traditional%20epidemiological%20models%0Arely%20on%20expert-defined%20frameworks%20to%20describe%20complex%20dynamics%2C%20while%20neural%0Anetworks%2C%20despite%20their%20predictive%20power%2C%20often%20lack%20interpretability%20due%20to%0Atheir%20%60%60black-box%22%20nature.%20This%20paper%20introduces%20the%20Finite%20Expression%20Method%2C%0Aa%20symbolic%20learning%20framework%20that%20leverages%20reinforcement%20learning%20to%20derive%0Aexplicit%20mathematical%20expressions%20for%20epidemiological%20dynamics.%20Through%0Anumerical%20experiments%20on%20both%20synthetic%20and%20real-world%20datasets%2C%20FEX%0Ademonstrates%20high%20accuracy%20in%20modeling%20and%20predicting%20disease%20spread%2C%20while%0Auncovering%20explicit%20relationships%20among%20epidemiological%20variables.%20These%0Aresults%20highlight%20FEX%20as%20a%20powerful%20tool%20for%20infectious%20disease%20modeling%2C%0Acombining%20interpretability%20with%20strong%20predictive%20performance%20to%20support%0Apractical%20applications%20in%20public%20health.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21049v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Epidemiological%2520Dynamics%2520via%2520the%2520Finite%2520Expression%2520Method%26entry.906535625%3DJianda%2520Du%2520and%2520Senwei%2520Liang%2520and%2520Chunmei%2520Wang%26entry.1292438233%3D%2520%2520Modeling%2520and%2520forecasting%2520the%2520spread%2520of%2520infectious%2520diseases%2520is%2520essential%2520for%250Aeffective%2520public%2520health%2520decision-making.%2520Traditional%2520epidemiological%2520models%250Arely%2520on%2520expert-defined%2520frameworks%2520to%2520describe%2520complex%2520dynamics%252C%2520while%2520neural%250Anetworks%252C%2520despite%2520their%2520predictive%2520power%252C%2520often%2520lack%2520interpretability%2520due%2520to%250Atheir%2520%2560%2560black-box%2522%2520nature.%2520This%2520paper%2520introduces%2520the%2520Finite%2520Expression%2520Method%252C%250Aa%2520symbolic%2520learning%2520framework%2520that%2520leverages%2520reinforcement%2520learning%2520to%2520derive%250Aexplicit%2520mathematical%2520expressions%2520for%2520epidemiological%2520dynamics.%2520Through%250Anumerical%2520experiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%252C%2520FEX%250Ademonstrates%2520high%2520accuracy%2520in%2520modeling%2520and%2520predicting%2520disease%2520spread%252C%2520while%250Auncovering%2520explicit%2520relationships%2520among%2520epidemiological%2520variables.%2520These%250Aresults%2520highlight%2520FEX%2520as%2520a%2520powerful%2520tool%2520for%2520infectious%2520disease%2520modeling%252C%250Acombining%2520interpretability%2520with%2520strong%2520predictive%2520performance%2520to%2520support%250Apractical%2520applications%2520in%2520public%2520health.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21049v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Epidemiological%20Dynamics%20via%20the%20Finite%20Expression%20Method&entry.906535625=Jianda%20Du%20and%20Senwei%20Liang%20and%20Chunmei%20Wang&entry.1292438233=%20%20Modeling%20and%20forecasting%20the%20spread%20of%20infectious%20diseases%20is%20essential%20for%0Aeffective%20public%20health%20decision-making.%20Traditional%20epidemiological%20models%0Arely%20on%20expert-defined%20frameworks%20to%20describe%20complex%20dynamics%2C%20while%20neural%0Anetworks%2C%20despite%20their%20predictive%20power%2C%20often%20lack%20interpretability%20due%20to%0Atheir%20%60%60black-box%22%20nature.%20This%20paper%20introduces%20the%20Finite%20Expression%20Method%2C%0Aa%20symbolic%20learning%20framework%20that%20leverages%20reinforcement%20learning%20to%20derive%0Aexplicit%20mathematical%20expressions%20for%20epidemiological%20dynamics.%20Through%0Anumerical%20experiments%20on%20both%20synthetic%20and%20real-world%20datasets%2C%20FEX%0Ademonstrates%20high%20accuracy%20in%20modeling%20and%20predicting%20disease%20spread%2C%20while%0Auncovering%20explicit%20relationships%20among%20epidemiological%20variables.%20These%0Aresults%20highlight%20FEX%20as%20a%20powerful%20tool%20for%20infectious%20disease%20modeling%2C%0Acombining%20interpretability%20with%20strong%20predictive%20performance%20to%20support%0Apractical%20applications%20in%20public%20health.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21049v1&entry.124074799=Read"},
{"title": "FPGA-based Acceleration of Neural Network for Image Classification using\n  Vitis AI", "author": "Zhengdong Li and Frederick Ziyang Hong and C. Patrick Yue", "abstract": "  In recent years, Convolutional Neural Networks (CNNs) have been widely\nadopted in computer vision. Complex CNN architecture running on CPU or GPU has\neither insufficient throughput or prohibitive power consumption. Hence, there\nis a need to have dedicated hardware to accelerate the computation workload to\nsolve these limitations. In this paper, we accelerate a CNN for image\nclassification with the CIFAR-10 dataset using Vitis-AI on Xilinx Zynq\nUltraScale+ MPSoC ZCU104 FPGA evaluation board. The work achieves 3.33-5.82x\nhigher throughput and 3.39-6.30x higher energy efficiency than CPU and GPU\nbaselines. It shows the potential to extract 2D features for downstream tasks,\nsuch as depth estimation and 3D reconstruction.\n", "link": "http://arxiv.org/abs/2412.20974v1", "date": "2024-12-30", "relevancy": 1.8633, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5186}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4581}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FPGA-based%20Acceleration%20of%20Neural%20Network%20for%20Image%20Classification%20using%0A%20%20Vitis%20AI&body=Title%3A%20FPGA-based%20Acceleration%20of%20Neural%20Network%20for%20Image%20Classification%20using%0A%20%20Vitis%20AI%0AAuthor%3A%20Zhengdong%20Li%20and%20Frederick%20Ziyang%20Hong%20and%20C.%20Patrick%20Yue%0AAbstract%3A%20%20%20In%20recent%20years%2C%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20been%20widely%0Aadopted%20in%20computer%20vision.%20Complex%20CNN%20architecture%20running%20on%20CPU%20or%20GPU%20has%0Aeither%20insufficient%20throughput%20or%20prohibitive%20power%20consumption.%20Hence%2C%20there%0Ais%20a%20need%20to%20have%20dedicated%20hardware%20to%20accelerate%20the%20computation%20workload%20to%0Asolve%20these%20limitations.%20In%20this%20paper%2C%20we%20accelerate%20a%20CNN%20for%20image%0Aclassification%20with%20the%20CIFAR-10%20dataset%20using%20Vitis-AI%20on%20Xilinx%20Zynq%0AUltraScale%2B%20MPSoC%20ZCU104%20FPGA%20evaluation%20board.%20The%20work%20achieves%203.33-5.82x%0Ahigher%20throughput%20and%203.39-6.30x%20higher%20energy%20efficiency%20than%20CPU%20and%20GPU%0Abaselines.%20It%20shows%20the%20potential%20to%20extract%202D%20features%20for%20downstream%20tasks%2C%0Asuch%20as%20depth%20estimation%20and%203D%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20974v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFPGA-based%2520Acceleration%2520of%2520Neural%2520Network%2520for%2520Image%2520Classification%2520using%250A%2520%2520Vitis%2520AI%26entry.906535625%3DZhengdong%2520Li%2520and%2520Frederick%2520Ziyang%2520Hong%2520and%2520C.%2520Patrick%2520Yue%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520Convolutional%2520Neural%2520Networks%2520%2528CNNs%2529%2520have%2520been%2520widely%250Aadopted%2520in%2520computer%2520vision.%2520Complex%2520CNN%2520architecture%2520running%2520on%2520CPU%2520or%2520GPU%2520has%250Aeither%2520insufficient%2520throughput%2520or%2520prohibitive%2520power%2520consumption.%2520Hence%252C%2520there%250Ais%2520a%2520need%2520to%2520have%2520dedicated%2520hardware%2520to%2520accelerate%2520the%2520computation%2520workload%2520to%250Asolve%2520these%2520limitations.%2520In%2520this%2520paper%252C%2520we%2520accelerate%2520a%2520CNN%2520for%2520image%250Aclassification%2520with%2520the%2520CIFAR-10%2520dataset%2520using%2520Vitis-AI%2520on%2520Xilinx%2520Zynq%250AUltraScale%252B%2520MPSoC%2520ZCU104%2520FPGA%2520evaluation%2520board.%2520The%2520work%2520achieves%25203.33-5.82x%250Ahigher%2520throughput%2520and%25203.39-6.30x%2520higher%2520energy%2520efficiency%2520than%2520CPU%2520and%2520GPU%250Abaselines.%2520It%2520shows%2520the%2520potential%2520to%2520extract%25202D%2520features%2520for%2520downstream%2520tasks%252C%250Asuch%2520as%2520depth%2520estimation%2520and%25203D%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20974v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FPGA-based%20Acceleration%20of%20Neural%20Network%20for%20Image%20Classification%20using%0A%20%20Vitis%20AI&entry.906535625=Zhengdong%20Li%20and%20Frederick%20Ziyang%20Hong%20and%20C.%20Patrick%20Yue&entry.1292438233=%20%20In%20recent%20years%2C%20Convolutional%20Neural%20Networks%20%28CNNs%29%20have%20been%20widely%0Aadopted%20in%20computer%20vision.%20Complex%20CNN%20architecture%20running%20on%20CPU%20or%20GPU%20has%0Aeither%20insufficient%20throughput%20or%20prohibitive%20power%20consumption.%20Hence%2C%20there%0Ais%20a%20need%20to%20have%20dedicated%20hardware%20to%20accelerate%20the%20computation%20workload%20to%0Asolve%20these%20limitations.%20In%20this%20paper%2C%20we%20accelerate%20a%20CNN%20for%20image%0Aclassification%20with%20the%20CIFAR-10%20dataset%20using%20Vitis-AI%20on%20Xilinx%20Zynq%0AUltraScale%2B%20MPSoC%20ZCU104%20FPGA%20evaluation%20board.%20The%20work%20achieves%203.33-5.82x%0Ahigher%20throughput%20and%203.39-6.30x%20higher%20energy%20efficiency%20than%20CPU%20and%20GPU%0Abaselines.%20It%20shows%20the%20potential%20to%20extract%202D%20features%20for%20downstream%20tasks%2C%0Asuch%20as%20depth%20estimation%20and%203D%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20974v1&entry.124074799=Read"},
{"title": "Ontology-grounded Automatic Knowledge Graph Construction by LLM under\n  Wikidata schema", "author": "Xiaohan Feng and Xixin Wu and Helen Meng", "abstract": "  We propose an ontology-grounded approach to Knowledge Graph (KG) construction\nusing Large Language Models (LLMs) on a knowledge base. An ontology is authored\nby generating Competency Questions (CQ) on knowledge base to discover knowledge\nscope, extracting relations from CQs, and attempt to replace equivalent\nrelations by their counterpart in Wikidata. To ensure consistency and\ninterpretability in the resulting KG, we ground generation of KG with the\nauthored ontology based on extracted relations. Evaluation on benchmark\ndatasets demonstrates competitive performance in knowledge graph construction\ntask. Our work presents a promising direction for scalable KG construction\npipeline with minimal human intervention, that yields high quality and\nhuman-interpretable KGs, which are interoperable with Wikidata semantics for\npotential knowledge base expansion.\n", "link": "http://arxiv.org/abs/2412.20942v1", "date": "2024-12-30", "relevancy": 1.849, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4737}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.463}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ontology-grounded%20Automatic%20Knowledge%20Graph%20Construction%20by%20LLM%20under%0A%20%20Wikidata%20schema&body=Title%3A%20Ontology-grounded%20Automatic%20Knowledge%20Graph%20Construction%20by%20LLM%20under%0A%20%20Wikidata%20schema%0AAuthor%3A%20Xiaohan%20Feng%20and%20Xixin%20Wu%20and%20Helen%20Meng%0AAbstract%3A%20%20%20We%20propose%20an%20ontology-grounded%20approach%20to%20Knowledge%20Graph%20%28KG%29%20construction%0Ausing%20Large%20Language%20Models%20%28LLMs%29%20on%20a%20knowledge%20base.%20An%20ontology%20is%20authored%0Aby%20generating%20Competency%20Questions%20%28CQ%29%20on%20knowledge%20base%20to%20discover%20knowledge%0Ascope%2C%20extracting%20relations%20from%20CQs%2C%20and%20attempt%20to%20replace%20equivalent%0Arelations%20by%20their%20counterpart%20in%20Wikidata.%20To%20ensure%20consistency%20and%0Ainterpretability%20in%20the%20resulting%20KG%2C%20we%20ground%20generation%20of%20KG%20with%20the%0Aauthored%20ontology%20based%20on%20extracted%20relations.%20Evaluation%20on%20benchmark%0Adatasets%20demonstrates%20competitive%20performance%20in%20knowledge%20graph%20construction%0Atask.%20Our%20work%20presents%20a%20promising%20direction%20for%20scalable%20KG%20construction%0Apipeline%20with%20minimal%20human%20intervention%2C%20that%20yields%20high%20quality%20and%0Ahuman-interpretable%20KGs%2C%20which%20are%20interoperable%20with%20Wikidata%20semantics%20for%0Apotential%20knowledge%20base%20expansion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20942v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOntology-grounded%2520Automatic%2520Knowledge%2520Graph%2520Construction%2520by%2520LLM%2520under%250A%2520%2520Wikidata%2520schema%26entry.906535625%3DXiaohan%2520Feng%2520and%2520Xixin%2520Wu%2520and%2520Helen%2520Meng%26entry.1292438233%3D%2520%2520We%2520propose%2520an%2520ontology-grounded%2520approach%2520to%2520Knowledge%2520Graph%2520%2528KG%2529%2520construction%250Ausing%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520on%2520a%2520knowledge%2520base.%2520An%2520ontology%2520is%2520authored%250Aby%2520generating%2520Competency%2520Questions%2520%2528CQ%2529%2520on%2520knowledge%2520base%2520to%2520discover%2520knowledge%250Ascope%252C%2520extracting%2520relations%2520from%2520CQs%252C%2520and%2520attempt%2520to%2520replace%2520equivalent%250Arelations%2520by%2520their%2520counterpart%2520in%2520Wikidata.%2520To%2520ensure%2520consistency%2520and%250Ainterpretability%2520in%2520the%2520resulting%2520KG%252C%2520we%2520ground%2520generation%2520of%2520KG%2520with%2520the%250Aauthored%2520ontology%2520based%2520on%2520extracted%2520relations.%2520Evaluation%2520on%2520benchmark%250Adatasets%2520demonstrates%2520competitive%2520performance%2520in%2520knowledge%2520graph%2520construction%250Atask.%2520Our%2520work%2520presents%2520a%2520promising%2520direction%2520for%2520scalable%2520KG%2520construction%250Apipeline%2520with%2520minimal%2520human%2520intervention%252C%2520that%2520yields%2520high%2520quality%2520and%250Ahuman-interpretable%2520KGs%252C%2520which%2520are%2520interoperable%2520with%2520Wikidata%2520semantics%2520for%250Apotential%2520knowledge%2520base%2520expansion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20942v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ontology-grounded%20Automatic%20Knowledge%20Graph%20Construction%20by%20LLM%20under%0A%20%20Wikidata%20schema&entry.906535625=Xiaohan%20Feng%20and%20Xixin%20Wu%20and%20Helen%20Meng&entry.1292438233=%20%20We%20propose%20an%20ontology-grounded%20approach%20to%20Knowledge%20Graph%20%28KG%29%20construction%0Ausing%20Large%20Language%20Models%20%28LLMs%29%20on%20a%20knowledge%20base.%20An%20ontology%20is%20authored%0Aby%20generating%20Competency%20Questions%20%28CQ%29%20on%20knowledge%20base%20to%20discover%20knowledge%0Ascope%2C%20extracting%20relations%20from%20CQs%2C%20and%20attempt%20to%20replace%20equivalent%0Arelations%20by%20their%20counterpart%20in%20Wikidata.%20To%20ensure%20consistency%20and%0Ainterpretability%20in%20the%20resulting%20KG%2C%20we%20ground%20generation%20of%20KG%20with%20the%0Aauthored%20ontology%20based%20on%20extracted%20relations.%20Evaluation%20on%20benchmark%0Adatasets%20demonstrates%20competitive%20performance%20in%20knowledge%20graph%20construction%0Atask.%20Our%20work%20presents%20a%20promising%20direction%20for%20scalable%20KG%20construction%0Apipeline%20with%20minimal%20human%20intervention%2C%20that%20yields%20high%20quality%20and%0Ahuman-interpretable%20KGs%2C%20which%20are%20interoperable%20with%20Wikidata%20semantics%20for%0Apotential%20knowledge%20base%20expansion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20942v1&entry.124074799=Read"},
{"title": "About rectified sigmoid function for enhancing the accuracy of\n  Physics-Informed Neural Networks", "author": "Vasiliy A. Es'kin and Alexey O. Malkhanov and Mikhail E. Smorkalov", "abstract": "  The article is devoted to the study of neural networks with one hidden layer\nand a modified activation function for solving physical problems. A rectified\nsigmoid activation function has been proposed to solve physical problems\ndescribed by the ODE with neural networks. Algorithms for physics-informed\ndata-driven initialization of a neural network and a neuron-by-neuron\ngradient-free fitting method have been presented for the neural network with\nthis activation function. Numerical experiments demonstrate the superiority of\nneural networks with a rectified sigmoid function over neural networks with a\nsigmoid function in the accuracy of solving physical problems (harmonic\noscillator, relativistic slingshot, and Lorentz system).\n", "link": "http://arxiv.org/abs/2412.20851v1", "date": "2024-12-30", "relevancy": 1.8314, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5008}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4547}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20About%20rectified%20sigmoid%20function%20for%20enhancing%20the%20accuracy%20of%0A%20%20Physics-Informed%20Neural%20Networks&body=Title%3A%20About%20rectified%20sigmoid%20function%20for%20enhancing%20the%20accuracy%20of%0A%20%20Physics-Informed%20Neural%20Networks%0AAuthor%3A%20Vasiliy%20A.%20Es%27kin%20and%20Alexey%20O.%20Malkhanov%20and%20Mikhail%20E.%20Smorkalov%0AAbstract%3A%20%20%20The%20article%20is%20devoted%20to%20the%20study%20of%20neural%20networks%20with%20one%20hidden%20layer%0Aand%20a%20modified%20activation%20function%20for%20solving%20physical%20problems.%20A%20rectified%0Asigmoid%20activation%20function%20has%20been%20proposed%20to%20solve%20physical%20problems%0Adescribed%20by%20the%20ODE%20with%20neural%20networks.%20Algorithms%20for%20physics-informed%0Adata-driven%20initialization%20of%20a%20neural%20network%20and%20a%20neuron-by-neuron%0Agradient-free%20fitting%20method%20have%20been%20presented%20for%20the%20neural%20network%20with%0Athis%20activation%20function.%20Numerical%20experiments%20demonstrate%20the%20superiority%20of%0Aneural%20networks%20with%20a%20rectified%20sigmoid%20function%20over%20neural%20networks%20with%20a%0Asigmoid%20function%20in%20the%20accuracy%20of%20solving%20physical%20problems%20%28harmonic%0Aoscillator%2C%20relativistic%20slingshot%2C%20and%20Lorentz%20system%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20851v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAbout%2520rectified%2520sigmoid%2520function%2520for%2520enhancing%2520the%2520accuracy%2520of%250A%2520%2520Physics-Informed%2520Neural%2520Networks%26entry.906535625%3DVasiliy%2520A.%2520Es%2527kin%2520and%2520Alexey%2520O.%2520Malkhanov%2520and%2520Mikhail%2520E.%2520Smorkalov%26entry.1292438233%3D%2520%2520The%2520article%2520is%2520devoted%2520to%2520the%2520study%2520of%2520neural%2520networks%2520with%2520one%2520hidden%2520layer%250Aand%2520a%2520modified%2520activation%2520function%2520for%2520solving%2520physical%2520problems.%2520A%2520rectified%250Asigmoid%2520activation%2520function%2520has%2520been%2520proposed%2520to%2520solve%2520physical%2520problems%250Adescribed%2520by%2520the%2520ODE%2520with%2520neural%2520networks.%2520Algorithms%2520for%2520physics-informed%250Adata-driven%2520initialization%2520of%2520a%2520neural%2520network%2520and%2520a%2520neuron-by-neuron%250Agradient-free%2520fitting%2520method%2520have%2520been%2520presented%2520for%2520the%2520neural%2520network%2520with%250Athis%2520activation%2520function.%2520Numerical%2520experiments%2520demonstrate%2520the%2520superiority%2520of%250Aneural%2520networks%2520with%2520a%2520rectified%2520sigmoid%2520function%2520over%2520neural%2520networks%2520with%2520a%250Asigmoid%2520function%2520in%2520the%2520accuracy%2520of%2520solving%2520physical%2520problems%2520%2528harmonic%250Aoscillator%252C%2520relativistic%2520slingshot%252C%2520and%2520Lorentz%2520system%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20851v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=About%20rectified%20sigmoid%20function%20for%20enhancing%20the%20accuracy%20of%0A%20%20Physics-Informed%20Neural%20Networks&entry.906535625=Vasiliy%20A.%20Es%27kin%20and%20Alexey%20O.%20Malkhanov%20and%20Mikhail%20E.%20Smorkalov&entry.1292438233=%20%20The%20article%20is%20devoted%20to%20the%20study%20of%20neural%20networks%20with%20one%20hidden%20layer%0Aand%20a%20modified%20activation%20function%20for%20solving%20physical%20problems.%20A%20rectified%0Asigmoid%20activation%20function%20has%20been%20proposed%20to%20solve%20physical%20problems%0Adescribed%20by%20the%20ODE%20with%20neural%20networks.%20Algorithms%20for%20physics-informed%0Adata-driven%20initialization%20of%20a%20neural%20network%20and%20a%20neuron-by-neuron%0Agradient-free%20fitting%20method%20have%20been%20presented%20for%20the%20neural%20network%20with%0Athis%20activation%20function.%20Numerical%20experiments%20demonstrate%20the%20superiority%20of%0Aneural%20networks%20with%20a%20rectified%20sigmoid%20function%20over%20neural%20networks%20with%20a%0Asigmoid%20function%20in%20the%20accuracy%20of%20solving%20physical%20problems%20%28harmonic%0Aoscillator%2C%20relativistic%20slingshot%2C%20and%20Lorentz%20system%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20851v1&entry.124074799=Read"},
{"title": "Graph Mixture of Experts and Memory-augmented Routers for Multivariate\n  Time Series Anomaly Detection", "author": "Xiaoyu Huang and Weidong Chen and Bo Hu and Zhendong Mao", "abstract": "  Multivariate time series (MTS) anomaly detection is a critical task that\ninvolves identifying abnormal patterns or events in data that consist of\nmultiple interrelated time series. In order to better model the complex\ninterdependence between entities and the various inherent characteristics of\neach entity, the GNN based methods are widely adopted by existing methods. In\neach layer of GNN, node features aggregate information from their neighboring\nnodes to update their information. In doing so, from shallow layer to deep\nlayer in GNN, original individual node features continue to be weakened and\nmore structural information,i.e., from short-distance neighborhood to\nlong-distance neighborhood, continues to be enhanced. However, research to date\nhas largely ignored the understanding of how hierarchical graph information is\nrepresented and their characteristics that can benefit anomaly detection.\nExisting methods simply leverage the output from the last layer of GNN for\nanomaly estimation while neglecting the essential information contained in the\nintermediate GNN layers. To address such limitations, in this paper, we propose\na Graph Mixture of Experts (Graph-MoE) network for multivariate time series\nanomaly detection, which incorporates the mixture of experts (MoE) module to\nadaptively represent and integrate hierarchical multi-layer graph information\ninto entity representations. It is worth noting that our Graph-MoE can be\nintegrated into any GNN-based MTS anomaly detection method in a plug-and-play\nmanner. In addition, the memory-augmented routers are proposed in this paper to\ncapture the correlation temporal information in terms of the global historical\nfeatures of MTS to adaptively weigh the obtained entity representations to\nachieve successful anomaly estimation. Extensive experiments on five\nchallenging datasets prove the superiority of our approach and each proposed\nmodule.\n", "link": "http://arxiv.org/abs/2412.19108v2", "date": "2024-12-30", "relevancy": 1.8257, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4603}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4591}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4515}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Mixture%20of%20Experts%20and%20Memory-augmented%20Routers%20for%20Multivariate%0A%20%20Time%20Series%20Anomaly%20Detection&body=Title%3A%20Graph%20Mixture%20of%20Experts%20and%20Memory-augmented%20Routers%20for%20Multivariate%0A%20%20Time%20Series%20Anomaly%20Detection%0AAuthor%3A%20Xiaoyu%20Huang%20and%20Weidong%20Chen%20and%20Bo%20Hu%20and%20Zhendong%20Mao%0AAbstract%3A%20%20%20Multivariate%20time%20series%20%28MTS%29%20anomaly%20detection%20is%20a%20critical%20task%20that%0Ainvolves%20identifying%20abnormal%20patterns%20or%20events%20in%20data%20that%20consist%20of%0Amultiple%20interrelated%20time%20series.%20In%20order%20to%20better%20model%20the%20complex%0Ainterdependence%20between%20entities%20and%20the%20various%20inherent%20characteristics%20of%0Aeach%20entity%2C%20the%20GNN%20based%20methods%20are%20widely%20adopted%20by%20existing%20methods.%20In%0Aeach%20layer%20of%20GNN%2C%20node%20features%20aggregate%20information%20from%20their%20neighboring%0Anodes%20to%20update%20their%20information.%20In%20doing%20so%2C%20from%20shallow%20layer%20to%20deep%0Alayer%20in%20GNN%2C%20original%20individual%20node%20features%20continue%20to%20be%20weakened%20and%0Amore%20structural%20information%2Ci.e.%2C%20from%20short-distance%20neighborhood%20to%0Along-distance%20neighborhood%2C%20continues%20to%20be%20enhanced.%20However%2C%20research%20to%20date%0Ahas%20largely%20ignored%20the%20understanding%20of%20how%20hierarchical%20graph%20information%20is%0Arepresented%20and%20their%20characteristics%20that%20can%20benefit%20anomaly%20detection.%0AExisting%20methods%20simply%20leverage%20the%20output%20from%20the%20last%20layer%20of%20GNN%20for%0Aanomaly%20estimation%20while%20neglecting%20the%20essential%20information%20contained%20in%20the%0Aintermediate%20GNN%20layers.%20To%20address%20such%20limitations%2C%20in%20this%20paper%2C%20we%20propose%0Aa%20Graph%20Mixture%20of%20Experts%20%28Graph-MoE%29%20network%20for%20multivariate%20time%20series%0Aanomaly%20detection%2C%20which%20incorporates%20the%20mixture%20of%20experts%20%28MoE%29%20module%20to%0Aadaptively%20represent%20and%20integrate%20hierarchical%20multi-layer%20graph%20information%0Ainto%20entity%20representations.%20It%20is%20worth%20noting%20that%20our%20Graph-MoE%20can%20be%0Aintegrated%20into%20any%20GNN-based%20MTS%20anomaly%20detection%20method%20in%20a%20plug-and-play%0Amanner.%20In%20addition%2C%20the%20memory-augmented%20routers%20are%20proposed%20in%20this%20paper%20to%0Acapture%20the%20correlation%20temporal%20information%20in%20terms%20of%20the%20global%20historical%0Afeatures%20of%20MTS%20to%20adaptively%20weigh%20the%20obtained%20entity%20representations%20to%0Aachieve%20successful%20anomaly%20estimation.%20Extensive%20experiments%20on%20five%0Achallenging%20datasets%20prove%20the%20superiority%20of%20our%20approach%20and%20each%20proposed%0Amodule.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.19108v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Mixture%2520of%2520Experts%2520and%2520Memory-augmented%2520Routers%2520for%2520Multivariate%250A%2520%2520Time%2520Series%2520Anomaly%2520Detection%26entry.906535625%3DXiaoyu%2520Huang%2520and%2520Weidong%2520Chen%2520and%2520Bo%2520Hu%2520and%2520Zhendong%2520Mao%26entry.1292438233%3D%2520%2520Multivariate%2520time%2520series%2520%2528MTS%2529%2520anomaly%2520detection%2520is%2520a%2520critical%2520task%2520that%250Ainvolves%2520identifying%2520abnormal%2520patterns%2520or%2520events%2520in%2520data%2520that%2520consist%2520of%250Amultiple%2520interrelated%2520time%2520series.%2520In%2520order%2520to%2520better%2520model%2520the%2520complex%250Ainterdependence%2520between%2520entities%2520and%2520the%2520various%2520inherent%2520characteristics%2520of%250Aeach%2520entity%252C%2520the%2520GNN%2520based%2520methods%2520are%2520widely%2520adopted%2520by%2520existing%2520methods.%2520In%250Aeach%2520layer%2520of%2520GNN%252C%2520node%2520features%2520aggregate%2520information%2520from%2520their%2520neighboring%250Anodes%2520to%2520update%2520their%2520information.%2520In%2520doing%2520so%252C%2520from%2520shallow%2520layer%2520to%2520deep%250Alayer%2520in%2520GNN%252C%2520original%2520individual%2520node%2520features%2520continue%2520to%2520be%2520weakened%2520and%250Amore%2520structural%2520information%252Ci.e.%252C%2520from%2520short-distance%2520neighborhood%2520to%250Along-distance%2520neighborhood%252C%2520continues%2520to%2520be%2520enhanced.%2520However%252C%2520research%2520to%2520date%250Ahas%2520largely%2520ignored%2520the%2520understanding%2520of%2520how%2520hierarchical%2520graph%2520information%2520is%250Arepresented%2520and%2520their%2520characteristics%2520that%2520can%2520benefit%2520anomaly%2520detection.%250AExisting%2520methods%2520simply%2520leverage%2520the%2520output%2520from%2520the%2520last%2520layer%2520of%2520GNN%2520for%250Aanomaly%2520estimation%2520while%2520neglecting%2520the%2520essential%2520information%2520contained%2520in%2520the%250Aintermediate%2520GNN%2520layers.%2520To%2520address%2520such%2520limitations%252C%2520in%2520this%2520paper%252C%2520we%2520propose%250Aa%2520Graph%2520Mixture%2520of%2520Experts%2520%2528Graph-MoE%2529%2520network%2520for%2520multivariate%2520time%2520series%250Aanomaly%2520detection%252C%2520which%2520incorporates%2520the%2520mixture%2520of%2520experts%2520%2528MoE%2529%2520module%2520to%250Aadaptively%2520represent%2520and%2520integrate%2520hierarchical%2520multi-layer%2520graph%2520information%250Ainto%2520entity%2520representations.%2520It%2520is%2520worth%2520noting%2520that%2520our%2520Graph-MoE%2520can%2520be%250Aintegrated%2520into%2520any%2520GNN-based%2520MTS%2520anomaly%2520detection%2520method%2520in%2520a%2520plug-and-play%250Amanner.%2520In%2520addition%252C%2520the%2520memory-augmented%2520routers%2520are%2520proposed%2520in%2520this%2520paper%2520to%250Acapture%2520the%2520correlation%2520temporal%2520information%2520in%2520terms%2520of%2520the%2520global%2520historical%250Afeatures%2520of%2520MTS%2520to%2520adaptively%2520weigh%2520the%2520obtained%2520entity%2520representations%2520to%250Aachieve%2520successful%2520anomaly%2520estimation.%2520Extensive%2520experiments%2520on%2520five%250Achallenging%2520datasets%2520prove%2520the%2520superiority%2520of%2520our%2520approach%2520and%2520each%2520proposed%250Amodule.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.19108v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Mixture%20of%20Experts%20and%20Memory-augmented%20Routers%20for%20Multivariate%0A%20%20Time%20Series%20Anomaly%20Detection&entry.906535625=Xiaoyu%20Huang%20and%20Weidong%20Chen%20and%20Bo%20Hu%20and%20Zhendong%20Mao&entry.1292438233=%20%20Multivariate%20time%20series%20%28MTS%29%20anomaly%20detection%20is%20a%20critical%20task%20that%0Ainvolves%20identifying%20abnormal%20patterns%20or%20events%20in%20data%20that%20consist%20of%0Amultiple%20interrelated%20time%20series.%20In%20order%20to%20better%20model%20the%20complex%0Ainterdependence%20between%20entities%20and%20the%20various%20inherent%20characteristics%20of%0Aeach%20entity%2C%20the%20GNN%20based%20methods%20are%20widely%20adopted%20by%20existing%20methods.%20In%0Aeach%20layer%20of%20GNN%2C%20node%20features%20aggregate%20information%20from%20their%20neighboring%0Anodes%20to%20update%20their%20information.%20In%20doing%20so%2C%20from%20shallow%20layer%20to%20deep%0Alayer%20in%20GNN%2C%20original%20individual%20node%20features%20continue%20to%20be%20weakened%20and%0Amore%20structural%20information%2Ci.e.%2C%20from%20short-distance%20neighborhood%20to%0Along-distance%20neighborhood%2C%20continues%20to%20be%20enhanced.%20However%2C%20research%20to%20date%0Ahas%20largely%20ignored%20the%20understanding%20of%20how%20hierarchical%20graph%20information%20is%0Arepresented%20and%20their%20characteristics%20that%20can%20benefit%20anomaly%20detection.%0AExisting%20methods%20simply%20leverage%20the%20output%20from%20the%20last%20layer%20of%20GNN%20for%0Aanomaly%20estimation%20while%20neglecting%20the%20essential%20information%20contained%20in%20the%0Aintermediate%20GNN%20layers.%20To%20address%20such%20limitations%2C%20in%20this%20paper%2C%20we%20propose%0Aa%20Graph%20Mixture%20of%20Experts%20%28Graph-MoE%29%20network%20for%20multivariate%20time%20series%0Aanomaly%20detection%2C%20which%20incorporates%20the%20mixture%20of%20experts%20%28MoE%29%20module%20to%0Aadaptively%20represent%20and%20integrate%20hierarchical%20multi-layer%20graph%20information%0Ainto%20entity%20representations.%20It%20is%20worth%20noting%20that%20our%20Graph-MoE%20can%20be%0Aintegrated%20into%20any%20GNN-based%20MTS%20anomaly%20detection%20method%20in%20a%20plug-and-play%0Amanner.%20In%20addition%2C%20the%20memory-augmented%20routers%20are%20proposed%20in%20this%20paper%20to%0Acapture%20the%20correlation%20temporal%20information%20in%20terms%20of%20the%20global%20historical%0Afeatures%20of%20MTS%20to%20adaptively%20weigh%20the%20obtained%20entity%20representations%20to%0Aachieve%20successful%20anomaly%20estimation.%20Extensive%20experiments%20on%20five%0Achallenging%20datasets%20prove%20the%20superiority%20of%20our%20approach%20and%20each%20proposed%0Amodule.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.19108v2&entry.124074799=Read"},
{"title": "Improving Location-based Thermal Emission Side-Channel Analysis Using\n  Iterative Transfer Learning", "author": "Tun-Chieh Lou and Chung-Che Wang and Jyh-Shing Roger Jang and Henian Li and Lang Lin and Norman Chang", "abstract": "  This paper proposes the use of iterative transfer learning applied to deep\nlearning models for side-channel attacks. Currently, most of the side-channel\nattack methods train a model for each individual byte, without considering the\ncorrelation between bytes. However, since the models' parameters for attacking\ndifferent bytes may be similar, we can leverage transfer learning, meaning that\nwe first train the model for one of the key bytes, then use the trained model\nas a pretrained model for the remaining bytes. This technique can be applied\niteratively, a process known as iterative transfer learning. Experimental\nresults show that when using thermal or power consumption map images as input,\nand multilayer perceptron or convolutional neural network as the model, our\nmethod improves average performance, especially when the amount of data is\ninsufficient.\n", "link": "http://arxiv.org/abs/2412.21030v1", "date": "2024-12-30", "relevancy": 1.8252, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4812}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4559}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Location-based%20Thermal%20Emission%20Side-Channel%20Analysis%20Using%0A%20%20Iterative%20Transfer%20Learning&body=Title%3A%20Improving%20Location-based%20Thermal%20Emission%20Side-Channel%20Analysis%20Using%0A%20%20Iterative%20Transfer%20Learning%0AAuthor%3A%20Tun-Chieh%20Lou%20and%20Chung-Che%20Wang%20and%20Jyh-Shing%20Roger%20Jang%20and%20Henian%20Li%20and%20Lang%20Lin%20and%20Norman%20Chang%0AAbstract%3A%20%20%20This%20paper%20proposes%20the%20use%20of%20iterative%20transfer%20learning%20applied%20to%20deep%0Alearning%20models%20for%20side-channel%20attacks.%20Currently%2C%20most%20of%20the%20side-channel%0Aattack%20methods%20train%20a%20model%20for%20each%20individual%20byte%2C%20without%20considering%20the%0Acorrelation%20between%20bytes.%20However%2C%20since%20the%20models%27%20parameters%20for%20attacking%0Adifferent%20bytes%20may%20be%20similar%2C%20we%20can%20leverage%20transfer%20learning%2C%20meaning%20that%0Awe%20first%20train%20the%20model%20for%20one%20of%20the%20key%20bytes%2C%20then%20use%20the%20trained%20model%0Aas%20a%20pretrained%20model%20for%20the%20remaining%20bytes.%20This%20technique%20can%20be%20applied%0Aiteratively%2C%20a%20process%20known%20as%20iterative%20transfer%20learning.%20Experimental%0Aresults%20show%20that%20when%20using%20thermal%20or%20power%20consumption%20map%20images%20as%20input%2C%0Aand%20multilayer%20perceptron%20or%20convolutional%20neural%20network%20as%20the%20model%2C%20our%0Amethod%20improves%20average%20performance%2C%20especially%20when%20the%20amount%20of%20data%20is%0Ainsufficient.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21030v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Location-based%2520Thermal%2520Emission%2520Side-Channel%2520Analysis%2520Using%250A%2520%2520Iterative%2520Transfer%2520Learning%26entry.906535625%3DTun-Chieh%2520Lou%2520and%2520Chung-Che%2520Wang%2520and%2520Jyh-Shing%2520Roger%2520Jang%2520and%2520Henian%2520Li%2520and%2520Lang%2520Lin%2520and%2520Norman%2520Chang%26entry.1292438233%3D%2520%2520This%2520paper%2520proposes%2520the%2520use%2520of%2520iterative%2520transfer%2520learning%2520applied%2520to%2520deep%250Alearning%2520models%2520for%2520side-channel%2520attacks.%2520Currently%252C%2520most%2520of%2520the%2520side-channel%250Aattack%2520methods%2520train%2520a%2520model%2520for%2520each%2520individual%2520byte%252C%2520without%2520considering%2520the%250Acorrelation%2520between%2520bytes.%2520However%252C%2520since%2520the%2520models%2527%2520parameters%2520for%2520attacking%250Adifferent%2520bytes%2520may%2520be%2520similar%252C%2520we%2520can%2520leverage%2520transfer%2520learning%252C%2520meaning%2520that%250Awe%2520first%2520train%2520the%2520model%2520for%2520one%2520of%2520the%2520key%2520bytes%252C%2520then%2520use%2520the%2520trained%2520model%250Aas%2520a%2520pretrained%2520model%2520for%2520the%2520remaining%2520bytes.%2520This%2520technique%2520can%2520be%2520applied%250Aiteratively%252C%2520a%2520process%2520known%2520as%2520iterative%2520transfer%2520learning.%2520Experimental%250Aresults%2520show%2520that%2520when%2520using%2520thermal%2520or%2520power%2520consumption%2520map%2520images%2520as%2520input%252C%250Aand%2520multilayer%2520perceptron%2520or%2520convolutional%2520neural%2520network%2520as%2520the%2520model%252C%2520our%250Amethod%2520improves%2520average%2520performance%252C%2520especially%2520when%2520the%2520amount%2520of%2520data%2520is%250Ainsufficient.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21030v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Location-based%20Thermal%20Emission%20Side-Channel%20Analysis%20Using%0A%20%20Iterative%20Transfer%20Learning&entry.906535625=Tun-Chieh%20Lou%20and%20Chung-Che%20Wang%20and%20Jyh-Shing%20Roger%20Jang%20and%20Henian%20Li%20and%20Lang%20Lin%20and%20Norman%20Chang&entry.1292438233=%20%20This%20paper%20proposes%20the%20use%20of%20iterative%20transfer%20learning%20applied%20to%20deep%0Alearning%20models%20for%20side-channel%20attacks.%20Currently%2C%20most%20of%20the%20side-channel%0Aattack%20methods%20train%20a%20model%20for%20each%20individual%20byte%2C%20without%20considering%20the%0Acorrelation%20between%20bytes.%20However%2C%20since%20the%20models%27%20parameters%20for%20attacking%0Adifferent%20bytes%20may%20be%20similar%2C%20we%20can%20leverage%20transfer%20learning%2C%20meaning%20that%0Awe%20first%20train%20the%20model%20for%20one%20of%20the%20key%20bytes%2C%20then%20use%20the%20trained%20model%0Aas%20a%20pretrained%20model%20for%20the%20remaining%20bytes.%20This%20technique%20can%20be%20applied%0Aiteratively%2C%20a%20process%20known%20as%20iterative%20transfer%20learning.%20Experimental%0Aresults%20show%20that%20when%20using%20thermal%20or%20power%20consumption%20map%20images%20as%20input%2C%0Aand%20multilayer%20perceptron%20or%20convolutional%20neural%20network%20as%20the%20model%2C%20our%0Amethod%20improves%20average%20performance%2C%20especially%20when%20the%20amount%20of%20data%20is%0Ainsufficient.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21030v1&entry.124074799=Read"},
{"title": "Adversarial Attack and Defense for LoRa Device Identification and\n  Authentication via Deep Learning", "author": "Yalin E. Sagduyu and Tugba Erpek", "abstract": "  LoRa provides long-range, energy-efficient communications in Internet of\nThings (IoT) applications that rely on Low-Power Wide-Area Network (LPWAN)\ncapabilities. Despite these merits, concerns persist regarding the security of\nLoRa networks, especially in situations where device identification and\nauthentication are imperative to secure the reliable access to the LoRa\nnetworks. This paper explores a deep learning (DL) approach to tackle these\nconcerns, focusing on two critical tasks, namely (i) identifying LoRa devices\nand (ii) classifying them to legitimate and rogue devices. Deep neural networks\n(DNNs), encompassing both convolutional and feedforward neural networks, are\ntrained for these tasks using actual LoRa signal data. In this setting, the\nadversaries may spoof rogue LoRa signals through the kernel density estimation\n(KDE) method based on legitimate device signals that are received by the\nadversaries. Two cases are considered, (i) training two separate classifiers,\none for each of the two tasks, and (ii) training a multi-task classifier for\nboth tasks. The vulnerabilities of the resulting DNNs to manipulations in input\nsamples are studied in form of untargeted and targeted adversarial attacks\nusing the Fast Gradient Sign Method (FGSM). Individual and common perturbations\nare considered against single-task and multi-task classifiers for the LoRa\nsignal analysis. To provide resilience against such attacks, a defense approach\nis presented by increasing the robustness of classifiers with adversarial\ntraining. Results quantify how vulnerable LoRa signal classification tasks are\nto adversarial attacks and emphasize the need to fortify IoT applications\nagainst these subtle yet effective threats.\n", "link": "http://arxiv.org/abs/2412.21164v1", "date": "2024-12-30", "relevancy": 1.8214, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4741}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.442}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Attack%20and%20Defense%20for%20LoRa%20Device%20Identification%20and%0A%20%20Authentication%20via%20Deep%20Learning&body=Title%3A%20Adversarial%20Attack%20and%20Defense%20for%20LoRa%20Device%20Identification%20and%0A%20%20Authentication%20via%20Deep%20Learning%0AAuthor%3A%20Yalin%20E.%20Sagduyu%20and%20Tugba%20Erpek%0AAbstract%3A%20%20%20LoRa%20provides%20long-range%2C%20energy-efficient%20communications%20in%20Internet%20of%0AThings%20%28IoT%29%20applications%20that%20rely%20on%20Low-Power%20Wide-Area%20Network%20%28LPWAN%29%0Acapabilities.%20Despite%20these%20merits%2C%20concerns%20persist%20regarding%20the%20security%20of%0ALoRa%20networks%2C%20especially%20in%20situations%20where%20device%20identification%20and%0Aauthentication%20are%20imperative%20to%20secure%20the%20reliable%20access%20to%20the%20LoRa%0Anetworks.%20This%20paper%20explores%20a%20deep%20learning%20%28DL%29%20approach%20to%20tackle%20these%0Aconcerns%2C%20focusing%20on%20two%20critical%20tasks%2C%20namely%20%28i%29%20identifying%20LoRa%20devices%0Aand%20%28ii%29%20classifying%20them%20to%20legitimate%20and%20rogue%20devices.%20Deep%20neural%20networks%0A%28DNNs%29%2C%20encompassing%20both%20convolutional%20and%20feedforward%20neural%20networks%2C%20are%0Atrained%20for%20these%20tasks%20using%20actual%20LoRa%20signal%20data.%20In%20this%20setting%2C%20the%0Aadversaries%20may%20spoof%20rogue%20LoRa%20signals%20through%20the%20kernel%20density%20estimation%0A%28KDE%29%20method%20based%20on%20legitimate%20device%20signals%20that%20are%20received%20by%20the%0Aadversaries.%20Two%20cases%20are%20considered%2C%20%28i%29%20training%20two%20separate%20classifiers%2C%0Aone%20for%20each%20of%20the%20two%20tasks%2C%20and%20%28ii%29%20training%20a%20multi-task%20classifier%20for%0Aboth%20tasks.%20The%20vulnerabilities%20of%20the%20resulting%20DNNs%20to%20manipulations%20in%20input%0Asamples%20are%20studied%20in%20form%20of%20untargeted%20and%20targeted%20adversarial%20attacks%0Ausing%20the%20Fast%20Gradient%20Sign%20Method%20%28FGSM%29.%20Individual%20and%20common%20perturbations%0Aare%20considered%20against%20single-task%20and%20multi-task%20classifiers%20for%20the%20LoRa%0Asignal%20analysis.%20To%20provide%20resilience%20against%20such%20attacks%2C%20a%20defense%20approach%0Ais%20presented%20by%20increasing%20the%20robustness%20of%20classifiers%20with%20adversarial%0Atraining.%20Results%20quantify%20how%20vulnerable%20LoRa%20signal%20classification%20tasks%20are%0Ato%20adversarial%20attacks%20and%20emphasize%20the%20need%20to%20fortify%20IoT%20applications%0Aagainst%20these%20subtle%20yet%20effective%20threats.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21164v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Attack%2520and%2520Defense%2520for%2520LoRa%2520Device%2520Identification%2520and%250A%2520%2520Authentication%2520via%2520Deep%2520Learning%26entry.906535625%3DYalin%2520E.%2520Sagduyu%2520and%2520Tugba%2520Erpek%26entry.1292438233%3D%2520%2520LoRa%2520provides%2520long-range%252C%2520energy-efficient%2520communications%2520in%2520Internet%2520of%250AThings%2520%2528IoT%2529%2520applications%2520that%2520rely%2520on%2520Low-Power%2520Wide-Area%2520Network%2520%2528LPWAN%2529%250Acapabilities.%2520Despite%2520these%2520merits%252C%2520concerns%2520persist%2520regarding%2520the%2520security%2520of%250ALoRa%2520networks%252C%2520especially%2520in%2520situations%2520where%2520device%2520identification%2520and%250Aauthentication%2520are%2520imperative%2520to%2520secure%2520the%2520reliable%2520access%2520to%2520the%2520LoRa%250Anetworks.%2520This%2520paper%2520explores%2520a%2520deep%2520learning%2520%2528DL%2529%2520approach%2520to%2520tackle%2520these%250Aconcerns%252C%2520focusing%2520on%2520two%2520critical%2520tasks%252C%2520namely%2520%2528i%2529%2520identifying%2520LoRa%2520devices%250Aand%2520%2528ii%2529%2520classifying%2520them%2520to%2520legitimate%2520and%2520rogue%2520devices.%2520Deep%2520neural%2520networks%250A%2528DNNs%2529%252C%2520encompassing%2520both%2520convolutional%2520and%2520feedforward%2520neural%2520networks%252C%2520are%250Atrained%2520for%2520these%2520tasks%2520using%2520actual%2520LoRa%2520signal%2520data.%2520In%2520this%2520setting%252C%2520the%250Aadversaries%2520may%2520spoof%2520rogue%2520LoRa%2520signals%2520through%2520the%2520kernel%2520density%2520estimation%250A%2528KDE%2529%2520method%2520based%2520on%2520legitimate%2520device%2520signals%2520that%2520are%2520received%2520by%2520the%250Aadversaries.%2520Two%2520cases%2520are%2520considered%252C%2520%2528i%2529%2520training%2520two%2520separate%2520classifiers%252C%250Aone%2520for%2520each%2520of%2520the%2520two%2520tasks%252C%2520and%2520%2528ii%2529%2520training%2520a%2520multi-task%2520classifier%2520for%250Aboth%2520tasks.%2520The%2520vulnerabilities%2520of%2520the%2520resulting%2520DNNs%2520to%2520manipulations%2520in%2520input%250Asamples%2520are%2520studied%2520in%2520form%2520of%2520untargeted%2520and%2520targeted%2520adversarial%2520attacks%250Ausing%2520the%2520Fast%2520Gradient%2520Sign%2520Method%2520%2528FGSM%2529.%2520Individual%2520and%2520common%2520perturbations%250Aare%2520considered%2520against%2520single-task%2520and%2520multi-task%2520classifiers%2520for%2520the%2520LoRa%250Asignal%2520analysis.%2520To%2520provide%2520resilience%2520against%2520such%2520attacks%252C%2520a%2520defense%2520approach%250Ais%2520presented%2520by%2520increasing%2520the%2520robustness%2520of%2520classifiers%2520with%2520adversarial%250Atraining.%2520Results%2520quantify%2520how%2520vulnerable%2520LoRa%2520signal%2520classification%2520tasks%2520are%250Ato%2520adversarial%2520attacks%2520and%2520emphasize%2520the%2520need%2520to%2520fortify%2520IoT%2520applications%250Aagainst%2520these%2520subtle%2520yet%2520effective%2520threats.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21164v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Attack%20and%20Defense%20for%20LoRa%20Device%20Identification%20and%0A%20%20Authentication%20via%20Deep%20Learning&entry.906535625=Yalin%20E.%20Sagduyu%20and%20Tugba%20Erpek&entry.1292438233=%20%20LoRa%20provides%20long-range%2C%20energy-efficient%20communications%20in%20Internet%20of%0AThings%20%28IoT%29%20applications%20that%20rely%20on%20Low-Power%20Wide-Area%20Network%20%28LPWAN%29%0Acapabilities.%20Despite%20these%20merits%2C%20concerns%20persist%20regarding%20the%20security%20of%0ALoRa%20networks%2C%20especially%20in%20situations%20where%20device%20identification%20and%0Aauthentication%20are%20imperative%20to%20secure%20the%20reliable%20access%20to%20the%20LoRa%0Anetworks.%20This%20paper%20explores%20a%20deep%20learning%20%28DL%29%20approach%20to%20tackle%20these%0Aconcerns%2C%20focusing%20on%20two%20critical%20tasks%2C%20namely%20%28i%29%20identifying%20LoRa%20devices%0Aand%20%28ii%29%20classifying%20them%20to%20legitimate%20and%20rogue%20devices.%20Deep%20neural%20networks%0A%28DNNs%29%2C%20encompassing%20both%20convolutional%20and%20feedforward%20neural%20networks%2C%20are%0Atrained%20for%20these%20tasks%20using%20actual%20LoRa%20signal%20data.%20In%20this%20setting%2C%20the%0Aadversaries%20may%20spoof%20rogue%20LoRa%20signals%20through%20the%20kernel%20density%20estimation%0A%28KDE%29%20method%20based%20on%20legitimate%20device%20signals%20that%20are%20received%20by%20the%0Aadversaries.%20Two%20cases%20are%20considered%2C%20%28i%29%20training%20two%20separate%20classifiers%2C%0Aone%20for%20each%20of%20the%20two%20tasks%2C%20and%20%28ii%29%20training%20a%20multi-task%20classifier%20for%0Aboth%20tasks.%20The%20vulnerabilities%20of%20the%20resulting%20DNNs%20to%20manipulations%20in%20input%0Asamples%20are%20studied%20in%20form%20of%20untargeted%20and%20targeted%20adversarial%20attacks%0Ausing%20the%20Fast%20Gradient%20Sign%20Method%20%28FGSM%29.%20Individual%20and%20common%20perturbations%0Aare%20considered%20against%20single-task%20and%20multi-task%20classifiers%20for%20the%20LoRa%0Asignal%20analysis.%20To%20provide%20resilience%20against%20such%20attacks%2C%20a%20defense%20approach%0Ais%20presented%20by%20increasing%20the%20robustness%20of%20classifiers%20with%20adversarial%0Atraining.%20Results%20quantify%20how%20vulnerable%20LoRa%20signal%20classification%20tasks%20are%0Ato%20adversarial%20attacks%20and%20emphasize%20the%20need%20to%20fortify%20IoT%20applications%0Aagainst%20these%20subtle%20yet%20effective%20threats.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21164v1&entry.124074799=Read"},
{"title": "RobustBlack: Challenging Black-Box Adversarial Attacks on\n  State-of-the-Art Defenses", "author": "Mohamed Djilani and Salah Ghamizi and Maxime Cordy", "abstract": "  Although adversarial robustness has been extensively studied in white-box\nsettings, recent advances in black-box attacks (including transfer- and\nquery-based approaches) are primarily benchmarked against weak defenses,\nleaving a significant gap in the evaluation of their effectiveness against more\nrecent and moderate robust models (e.g., those featured in the Robustbench\nleaderboard). In this paper, we question this lack of attention from black-box\nattacks to robust models. We establish a framework to evaluate the\neffectiveness of recent black-box attacks against both top-performing and\nstandard defense mechanisms, on the ImageNet dataset. Our empirical evaluation\nreveals the following key findings: (1) the most advanced black-box attacks\nstruggle to succeed even against simple adversarially trained models; (2)\nrobust models that are optimized to withstand strong white-box attacks, such as\nAutoAttack, also exhibits enhanced resilience against black-box attacks; and\n(3) robustness alignment between the surrogate models and the target model\nplays a key factor in the success rate of transfer-based attacks\n", "link": "http://arxiv.org/abs/2412.20987v1", "date": "2024-12-30", "relevancy": 1.8156, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4613}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4525}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RobustBlack%3A%20Challenging%20Black-Box%20Adversarial%20Attacks%20on%0A%20%20State-of-the-Art%20Defenses&body=Title%3A%20RobustBlack%3A%20Challenging%20Black-Box%20Adversarial%20Attacks%20on%0A%20%20State-of-the-Art%20Defenses%0AAuthor%3A%20Mohamed%20Djilani%20and%20Salah%20Ghamizi%20and%20Maxime%20Cordy%0AAbstract%3A%20%20%20Although%20adversarial%20robustness%20has%20been%20extensively%20studied%20in%20white-box%0Asettings%2C%20recent%20advances%20in%20black-box%20attacks%20%28including%20transfer-%20and%0Aquery-based%20approaches%29%20are%20primarily%20benchmarked%20against%20weak%20defenses%2C%0Aleaving%20a%20significant%20gap%20in%20the%20evaluation%20of%20their%20effectiveness%20against%20more%0Arecent%20and%20moderate%20robust%20models%20%28e.g.%2C%20those%20featured%20in%20the%20Robustbench%0Aleaderboard%29.%20In%20this%20paper%2C%20we%20question%20this%20lack%20of%20attention%20from%20black-box%0Aattacks%20to%20robust%20models.%20We%20establish%20a%20framework%20to%20evaluate%20the%0Aeffectiveness%20of%20recent%20black-box%20attacks%20against%20both%20top-performing%20and%0Astandard%20defense%20mechanisms%2C%20on%20the%20ImageNet%20dataset.%20Our%20empirical%20evaluation%0Areveals%20the%20following%20key%20findings%3A%20%281%29%20the%20most%20advanced%20black-box%20attacks%0Astruggle%20to%20succeed%20even%20against%20simple%20adversarially%20trained%20models%3B%20%282%29%0Arobust%20models%20that%20are%20optimized%20to%20withstand%20strong%20white-box%20attacks%2C%20such%20as%0AAutoAttack%2C%20also%20exhibits%20enhanced%20resilience%20against%20black-box%20attacks%3B%20and%0A%283%29%20robustness%20alignment%20between%20the%20surrogate%20models%20and%20the%20target%20model%0Aplays%20a%20key%20factor%20in%20the%20success%20rate%20of%20transfer-based%20attacks%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20987v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobustBlack%253A%2520Challenging%2520Black-Box%2520Adversarial%2520Attacks%2520on%250A%2520%2520State-of-the-Art%2520Defenses%26entry.906535625%3DMohamed%2520Djilani%2520and%2520Salah%2520Ghamizi%2520and%2520Maxime%2520Cordy%26entry.1292438233%3D%2520%2520Although%2520adversarial%2520robustness%2520has%2520been%2520extensively%2520studied%2520in%2520white-box%250Asettings%252C%2520recent%2520advances%2520in%2520black-box%2520attacks%2520%2528including%2520transfer-%2520and%250Aquery-based%2520approaches%2529%2520are%2520primarily%2520benchmarked%2520against%2520weak%2520defenses%252C%250Aleaving%2520a%2520significant%2520gap%2520in%2520the%2520evaluation%2520of%2520their%2520effectiveness%2520against%2520more%250Arecent%2520and%2520moderate%2520robust%2520models%2520%2528e.g.%252C%2520those%2520featured%2520in%2520the%2520Robustbench%250Aleaderboard%2529.%2520In%2520this%2520paper%252C%2520we%2520question%2520this%2520lack%2520of%2520attention%2520from%2520black-box%250Aattacks%2520to%2520robust%2520models.%2520We%2520establish%2520a%2520framework%2520to%2520evaluate%2520the%250Aeffectiveness%2520of%2520recent%2520black-box%2520attacks%2520against%2520both%2520top-performing%2520and%250Astandard%2520defense%2520mechanisms%252C%2520on%2520the%2520ImageNet%2520dataset.%2520Our%2520empirical%2520evaluation%250Areveals%2520the%2520following%2520key%2520findings%253A%2520%25281%2529%2520the%2520most%2520advanced%2520black-box%2520attacks%250Astruggle%2520to%2520succeed%2520even%2520against%2520simple%2520adversarially%2520trained%2520models%253B%2520%25282%2529%250Arobust%2520models%2520that%2520are%2520optimized%2520to%2520withstand%2520strong%2520white-box%2520attacks%252C%2520such%2520as%250AAutoAttack%252C%2520also%2520exhibits%2520enhanced%2520resilience%2520against%2520black-box%2520attacks%253B%2520and%250A%25283%2529%2520robustness%2520alignment%2520between%2520the%2520surrogate%2520models%2520and%2520the%2520target%2520model%250Aplays%2520a%2520key%2520factor%2520in%2520the%2520success%2520rate%2520of%2520transfer-based%2520attacks%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20987v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RobustBlack%3A%20Challenging%20Black-Box%20Adversarial%20Attacks%20on%0A%20%20State-of-the-Art%20Defenses&entry.906535625=Mohamed%20Djilani%20and%20Salah%20Ghamizi%20and%20Maxime%20Cordy&entry.1292438233=%20%20Although%20adversarial%20robustness%20has%20been%20extensively%20studied%20in%20white-box%0Asettings%2C%20recent%20advances%20in%20black-box%20attacks%20%28including%20transfer-%20and%0Aquery-based%20approaches%29%20are%20primarily%20benchmarked%20against%20weak%20defenses%2C%0Aleaving%20a%20significant%20gap%20in%20the%20evaluation%20of%20their%20effectiveness%20against%20more%0Arecent%20and%20moderate%20robust%20models%20%28e.g.%2C%20those%20featured%20in%20the%20Robustbench%0Aleaderboard%29.%20In%20this%20paper%2C%20we%20question%20this%20lack%20of%20attention%20from%20black-box%0Aattacks%20to%20robust%20models.%20We%20establish%20a%20framework%20to%20evaluate%20the%0Aeffectiveness%20of%20recent%20black-box%20attacks%20against%20both%20top-performing%20and%0Astandard%20defense%20mechanisms%2C%20on%20the%20ImageNet%20dataset.%20Our%20empirical%20evaluation%0Areveals%20the%20following%20key%20findings%3A%20%281%29%20the%20most%20advanced%20black-box%20attacks%0Astruggle%20to%20succeed%20even%20against%20simple%20adversarially%20trained%20models%3B%20%282%29%0Arobust%20models%20that%20are%20optimized%20to%20withstand%20strong%20white-box%20attacks%2C%20such%20as%0AAutoAttack%2C%20also%20exhibits%20enhanced%20resilience%20against%20black-box%20attacks%3B%20and%0A%283%29%20robustness%20alignment%20between%20the%20surrogate%20models%20and%20the%20target%20model%0Aplays%20a%20key%20factor%20in%20the%20success%20rate%20of%20transfer-based%20attacks%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20987v1&entry.124074799=Read"},
{"title": "Segment Discovery: Enhancing E-commerce Targeting", "author": "Qiqi Li and Roopali Singh and Charin Polpanumas and Tanner Fiez and Namita Kumar and Shreya Chakrabarti", "abstract": "  Modern e-commerce services frequently target customers with incentives or\ninterventions to engage them in their products such as games, shopping, video\nstreaming, etc. This customer engagement increases acquisition of more\ncustomers and retention of existing ones, leading to more business for the\ncompany while improving customer experience. Often, customers are either\nrandomly targeted or targeted based on the propensity of desirable behavior.\nHowever, such policies can be suboptimal as they do not target the set of\ncustomers who would benefit the most from the intervention and they may also\nnot take account of any constraints. In this paper, we propose a policy\nframework based on uplift modeling and constrained optimization that identifies\ncustomers to target for a use-case specific intervention so as to maximize the\nvalue to the business, while taking account of any given constraints. We\ndemonstrate improvement over state-of-the-art targeting approaches using two\nlarge-scale experimental studies and a production implementation.\n", "link": "http://arxiv.org/abs/2409.13847v2", "date": "2024-12-30", "relevancy": 1.8128, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4915}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4283}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segment%20Discovery%3A%20Enhancing%20E-commerce%20Targeting&body=Title%3A%20Segment%20Discovery%3A%20Enhancing%20E-commerce%20Targeting%0AAuthor%3A%20Qiqi%20Li%20and%20Roopali%20Singh%20and%20Charin%20Polpanumas%20and%20Tanner%20Fiez%20and%20Namita%20Kumar%20and%20Shreya%20Chakrabarti%0AAbstract%3A%20%20%20Modern%20e-commerce%20services%20frequently%20target%20customers%20with%20incentives%20or%0Ainterventions%20to%20engage%20them%20in%20their%20products%20such%20as%20games%2C%20shopping%2C%20video%0Astreaming%2C%20etc.%20This%20customer%20engagement%20increases%20acquisition%20of%20more%0Acustomers%20and%20retention%20of%20existing%20ones%2C%20leading%20to%20more%20business%20for%20the%0Acompany%20while%20improving%20customer%20experience.%20Often%2C%20customers%20are%20either%0Arandomly%20targeted%20or%20targeted%20based%20on%20the%20propensity%20of%20desirable%20behavior.%0AHowever%2C%20such%20policies%20can%20be%20suboptimal%20as%20they%20do%20not%20target%20the%20set%20of%0Acustomers%20who%20would%20benefit%20the%20most%20from%20the%20intervention%20and%20they%20may%20also%0Anot%20take%20account%20of%20any%20constraints.%20In%20this%20paper%2C%20we%20propose%20a%20policy%0Aframework%20based%20on%20uplift%20modeling%20and%20constrained%20optimization%20that%20identifies%0Acustomers%20to%20target%20for%20a%20use-case%20specific%20intervention%20so%20as%20to%20maximize%20the%0Avalue%20to%20the%20business%2C%20while%20taking%20account%20of%20any%20given%20constraints.%20We%0Ademonstrate%20improvement%20over%20state-of-the-art%20targeting%20approaches%20using%20two%0Alarge-scale%20experimental%20studies%20and%20a%20production%20implementation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.13847v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegment%2520Discovery%253A%2520Enhancing%2520E-commerce%2520Targeting%26entry.906535625%3DQiqi%2520Li%2520and%2520Roopali%2520Singh%2520and%2520Charin%2520Polpanumas%2520and%2520Tanner%2520Fiez%2520and%2520Namita%2520Kumar%2520and%2520Shreya%2520Chakrabarti%26entry.1292438233%3D%2520%2520Modern%2520e-commerce%2520services%2520frequently%2520target%2520customers%2520with%2520incentives%2520or%250Ainterventions%2520to%2520engage%2520them%2520in%2520their%2520products%2520such%2520as%2520games%252C%2520shopping%252C%2520video%250Astreaming%252C%2520etc.%2520This%2520customer%2520engagement%2520increases%2520acquisition%2520of%2520more%250Acustomers%2520and%2520retention%2520of%2520existing%2520ones%252C%2520leading%2520to%2520more%2520business%2520for%2520the%250Acompany%2520while%2520improving%2520customer%2520experience.%2520Often%252C%2520customers%2520are%2520either%250Arandomly%2520targeted%2520or%2520targeted%2520based%2520on%2520the%2520propensity%2520of%2520desirable%2520behavior.%250AHowever%252C%2520such%2520policies%2520can%2520be%2520suboptimal%2520as%2520they%2520do%2520not%2520target%2520the%2520set%2520of%250Acustomers%2520who%2520would%2520benefit%2520the%2520most%2520from%2520the%2520intervention%2520and%2520they%2520may%2520also%250Anot%2520take%2520account%2520of%2520any%2520constraints.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520policy%250Aframework%2520based%2520on%2520uplift%2520modeling%2520and%2520constrained%2520optimization%2520that%2520identifies%250Acustomers%2520to%2520target%2520for%2520a%2520use-case%2520specific%2520intervention%2520so%2520as%2520to%2520maximize%2520the%250Avalue%2520to%2520the%2520business%252C%2520while%2520taking%2520account%2520of%2520any%2520given%2520constraints.%2520We%250Ademonstrate%2520improvement%2520over%2520state-of-the-art%2520targeting%2520approaches%2520using%2520two%250Alarge-scale%2520experimental%2520studies%2520and%2520a%2520production%2520implementation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.13847v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segment%20Discovery%3A%20Enhancing%20E-commerce%20Targeting&entry.906535625=Qiqi%20Li%20and%20Roopali%20Singh%20and%20Charin%20Polpanumas%20and%20Tanner%20Fiez%20and%20Namita%20Kumar%20and%20Shreya%20Chakrabarti&entry.1292438233=%20%20Modern%20e-commerce%20services%20frequently%20target%20customers%20with%20incentives%20or%0Ainterventions%20to%20engage%20them%20in%20their%20products%20such%20as%20games%2C%20shopping%2C%20video%0Astreaming%2C%20etc.%20This%20customer%20engagement%20increases%20acquisition%20of%20more%0Acustomers%20and%20retention%20of%20existing%20ones%2C%20leading%20to%20more%20business%20for%20the%0Acompany%20while%20improving%20customer%20experience.%20Often%2C%20customers%20are%20either%0Arandomly%20targeted%20or%20targeted%20based%20on%20the%20propensity%20of%20desirable%20behavior.%0AHowever%2C%20such%20policies%20can%20be%20suboptimal%20as%20they%20do%20not%20target%20the%20set%20of%0Acustomers%20who%20would%20benefit%20the%20most%20from%20the%20intervention%20and%20they%20may%20also%0Anot%20take%20account%20of%20any%20constraints.%20In%20this%20paper%2C%20we%20propose%20a%20policy%0Aframework%20based%20on%20uplift%20modeling%20and%20constrained%20optimization%20that%20identifies%0Acustomers%20to%20target%20for%20a%20use-case%20specific%20intervention%20so%20as%20to%20maximize%20the%0Avalue%20to%20the%20business%2C%20while%20taking%20account%20of%20any%20given%20constraints.%20We%0Ademonstrate%20improvement%20over%20state-of-the-art%20targeting%20approaches%20using%20two%0Alarge-scale%20experimental%20studies%20and%20a%20production%20implementation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.13847v2&entry.124074799=Read"},
{"title": "ILDiff: Generate Transparent Animated Stickers by Implicit Layout\n  Distillation", "author": "Ting Zhang and Zhiqiang Yuan and Yeshuang Zhu and Jinchao Zhang", "abstract": "  High-quality animated stickers usually contain transparent channels, which\nare often ignored by current video generation models. To generate fine-grained\nanimated transparency channels, existing methods can be roughly divided into\nvideo matting algorithms and diffusion-based algorithms. The methods based on\nvideo matting have poor performance in dealing with semi-open areas in\nstickers, while diffusion-based methods are often used to model a single image,\nwhich will lead to local flicker when modeling animated stickers. In this\npaper, we firstly propose an ILDiff method to generate animated transparent\nchannels through implicit layout distillation, which solves the problems of\nsemi-open area collapse and no consideration of temporal information in\nexisting methods. Secondly, we create the Transparent Animated Sticker Dataset\n(TASD), which contains 0.32M high-quality samples with transparent channel, to\nprovide data support for related fields. Extensive experiments demonstrate that\nILDiff can produce finer and smoother transparent channels compared to other\nmethods such as Matting Anything and Layer Diffusion. Our code and dataset will\nbe released at link https://xiaoyuan1996.github.io.\n", "link": "http://arxiv.org/abs/2412.20901v1", "date": "2024-12-30", "relevancy": 1.7945, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6075}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.604}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ILDiff%3A%20Generate%20Transparent%20Animated%20Stickers%20by%20Implicit%20Layout%0A%20%20Distillation&body=Title%3A%20ILDiff%3A%20Generate%20Transparent%20Animated%20Stickers%20by%20Implicit%20Layout%0A%20%20Distillation%0AAuthor%3A%20Ting%20Zhang%20and%20Zhiqiang%20Yuan%20and%20Yeshuang%20Zhu%20and%20Jinchao%20Zhang%0AAbstract%3A%20%20%20High-quality%20animated%20stickers%20usually%20contain%20transparent%20channels%2C%20which%0Aare%20often%20ignored%20by%20current%20video%20generation%20models.%20To%20generate%20fine-grained%0Aanimated%20transparency%20channels%2C%20existing%20methods%20can%20be%20roughly%20divided%20into%0Avideo%20matting%20algorithms%20and%20diffusion-based%20algorithms.%20The%20methods%20based%20on%0Avideo%20matting%20have%20poor%20performance%20in%20dealing%20with%20semi-open%20areas%20in%0Astickers%2C%20while%20diffusion-based%20methods%20are%20often%20used%20to%20model%20a%20single%20image%2C%0Awhich%20will%20lead%20to%20local%20flicker%20when%20modeling%20animated%20stickers.%20In%20this%0Apaper%2C%20we%20firstly%20propose%20an%20ILDiff%20method%20to%20generate%20animated%20transparent%0Achannels%20through%20implicit%20layout%20distillation%2C%20which%20solves%20the%20problems%20of%0Asemi-open%20area%20collapse%20and%20no%20consideration%20of%20temporal%20information%20in%0Aexisting%20methods.%20Secondly%2C%20we%20create%20the%20Transparent%20Animated%20Sticker%20Dataset%0A%28TASD%29%2C%20which%20contains%200.32M%20high-quality%20samples%20with%20transparent%20channel%2C%20to%0Aprovide%20data%20support%20for%20related%20fields.%20Extensive%20experiments%20demonstrate%20that%0AILDiff%20can%20produce%20finer%20and%20smoother%20transparent%20channels%20compared%20to%20other%0Amethods%20such%20as%20Matting%20Anything%20and%20Layer%20Diffusion.%20Our%20code%20and%20dataset%20will%0Abe%20released%20at%20link%20https%3A//xiaoyuan1996.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20901v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DILDiff%253A%2520Generate%2520Transparent%2520Animated%2520Stickers%2520by%2520Implicit%2520Layout%250A%2520%2520Distillation%26entry.906535625%3DTing%2520Zhang%2520and%2520Zhiqiang%2520Yuan%2520and%2520Yeshuang%2520Zhu%2520and%2520Jinchao%2520Zhang%26entry.1292438233%3D%2520%2520High-quality%2520animated%2520stickers%2520usually%2520contain%2520transparent%2520channels%252C%2520which%250Aare%2520often%2520ignored%2520by%2520current%2520video%2520generation%2520models.%2520To%2520generate%2520fine-grained%250Aanimated%2520transparency%2520channels%252C%2520existing%2520methods%2520can%2520be%2520roughly%2520divided%2520into%250Avideo%2520matting%2520algorithms%2520and%2520diffusion-based%2520algorithms.%2520The%2520methods%2520based%2520on%250Avideo%2520matting%2520have%2520poor%2520performance%2520in%2520dealing%2520with%2520semi-open%2520areas%2520in%250Astickers%252C%2520while%2520diffusion-based%2520methods%2520are%2520often%2520used%2520to%2520model%2520a%2520single%2520image%252C%250Awhich%2520will%2520lead%2520to%2520local%2520flicker%2520when%2520modeling%2520animated%2520stickers.%2520In%2520this%250Apaper%252C%2520we%2520firstly%2520propose%2520an%2520ILDiff%2520method%2520to%2520generate%2520animated%2520transparent%250Achannels%2520through%2520implicit%2520layout%2520distillation%252C%2520which%2520solves%2520the%2520problems%2520of%250Asemi-open%2520area%2520collapse%2520and%2520no%2520consideration%2520of%2520temporal%2520information%2520in%250Aexisting%2520methods.%2520Secondly%252C%2520we%2520create%2520the%2520Transparent%2520Animated%2520Sticker%2520Dataset%250A%2528TASD%2529%252C%2520which%2520contains%25200.32M%2520high-quality%2520samples%2520with%2520transparent%2520channel%252C%2520to%250Aprovide%2520data%2520support%2520for%2520related%2520fields.%2520Extensive%2520experiments%2520demonstrate%2520that%250AILDiff%2520can%2520produce%2520finer%2520and%2520smoother%2520transparent%2520channels%2520compared%2520to%2520other%250Amethods%2520such%2520as%2520Matting%2520Anything%2520and%2520Layer%2520Diffusion.%2520Our%2520code%2520and%2520dataset%2520will%250Abe%2520released%2520at%2520link%2520https%253A//xiaoyuan1996.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20901v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ILDiff%3A%20Generate%20Transparent%20Animated%20Stickers%20by%20Implicit%20Layout%0A%20%20Distillation&entry.906535625=Ting%20Zhang%20and%20Zhiqiang%20Yuan%20and%20Yeshuang%20Zhu%20and%20Jinchao%20Zhang&entry.1292438233=%20%20High-quality%20animated%20stickers%20usually%20contain%20transparent%20channels%2C%20which%0Aare%20often%20ignored%20by%20current%20video%20generation%20models.%20To%20generate%20fine-grained%0Aanimated%20transparency%20channels%2C%20existing%20methods%20can%20be%20roughly%20divided%20into%0Avideo%20matting%20algorithms%20and%20diffusion-based%20algorithms.%20The%20methods%20based%20on%0Avideo%20matting%20have%20poor%20performance%20in%20dealing%20with%20semi-open%20areas%20in%0Astickers%2C%20while%20diffusion-based%20methods%20are%20often%20used%20to%20model%20a%20single%20image%2C%0Awhich%20will%20lead%20to%20local%20flicker%20when%20modeling%20animated%20stickers.%20In%20this%0Apaper%2C%20we%20firstly%20propose%20an%20ILDiff%20method%20to%20generate%20animated%20transparent%0Achannels%20through%20implicit%20layout%20distillation%2C%20which%20solves%20the%20problems%20of%0Asemi-open%20area%20collapse%20and%20no%20consideration%20of%20temporal%20information%20in%0Aexisting%20methods.%20Secondly%2C%20we%20create%20the%20Transparent%20Animated%20Sticker%20Dataset%0A%28TASD%29%2C%20which%20contains%200.32M%20high-quality%20samples%20with%20transparent%20channel%2C%20to%0Aprovide%20data%20support%20for%20related%20fields.%20Extensive%20experiments%20demonstrate%20that%0AILDiff%20can%20produce%20finer%20and%20smoother%20transparent%20channels%20compared%20to%20other%0Amethods%20such%20as%20Matting%20Anything%20and%20Layer%20Diffusion.%20Our%20code%20and%20dataset%20will%0Abe%20released%20at%20link%20https%3A//xiaoyuan1996.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20901v1&entry.124074799=Read"},
{"title": "Visual Style Prompt Learning Using Diffusion Models for Blind Face\n  Restoration", "author": "Wanglong Lu and Jikai Wang and Tao Wang and Kaihao Zhang and Xianta Jiang and Hanli Zhao", "abstract": "  Blind face restoration aims to recover high-quality facial images from\nvarious unidentified sources of degradation, posing significant challenges due\nto the minimal information retrievable from the degraded images. Prior\nknowledge-based methods, leveraging geometric priors and facial features, have\nled to advancements in face restoration but often fall short of capturing fine\ndetails. To address this, we introduce a visual style prompt learning framework\nthat utilizes diffusion probabilistic models to explicitly generate visual\nprompts within the latent space of pre-trained generative models. These prompts\nare designed to guide the restoration process. To fully utilize the visual\nprompts and enhance the extraction of informative and rich patterns, we\nintroduce a style-modulated aggregation transformation layer. Extensive\nexperiments and applications demonstrate the superiority of our method in\nachieving high-quality blind face restoration. The source code is available at\n\\href{https://github.com/LonglongaaaGo/VSPBFR}{https://github.com/LonglongaaaGo/VSPBFR}.\n", "link": "http://arxiv.org/abs/2412.21042v1", "date": "2024-12-30", "relevancy": 1.7918, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6618}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5964}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5718}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Style%20Prompt%20Learning%20Using%20Diffusion%20Models%20for%20Blind%20Face%0A%20%20Restoration&body=Title%3A%20Visual%20Style%20Prompt%20Learning%20Using%20Diffusion%20Models%20for%20Blind%20Face%0A%20%20Restoration%0AAuthor%3A%20Wanglong%20Lu%20and%20Jikai%20Wang%20and%20Tao%20Wang%20and%20Kaihao%20Zhang%20and%20Xianta%20Jiang%20and%20Hanli%20Zhao%0AAbstract%3A%20%20%20Blind%20face%20restoration%20aims%20to%20recover%20high-quality%20facial%20images%20from%0Avarious%20unidentified%20sources%20of%20degradation%2C%20posing%20significant%20challenges%20due%0Ato%20the%20minimal%20information%20retrievable%20from%20the%20degraded%20images.%20Prior%0Aknowledge-based%20methods%2C%20leveraging%20geometric%20priors%20and%20facial%20features%2C%20have%0Aled%20to%20advancements%20in%20face%20restoration%20but%20often%20fall%20short%20of%20capturing%20fine%0Adetails.%20To%20address%20this%2C%20we%20introduce%20a%20visual%20style%20prompt%20learning%20framework%0Athat%20utilizes%20diffusion%20probabilistic%20models%20to%20explicitly%20generate%20visual%0Aprompts%20within%20the%20latent%20space%20of%20pre-trained%20generative%20models.%20These%20prompts%0Aare%20designed%20to%20guide%20the%20restoration%20process.%20To%20fully%20utilize%20the%20visual%0Aprompts%20and%20enhance%20the%20extraction%20of%20informative%20and%20rich%20patterns%2C%20we%0Aintroduce%20a%20style-modulated%20aggregation%20transformation%20layer.%20Extensive%0Aexperiments%20and%20applications%20demonstrate%20the%20superiority%20of%20our%20method%20in%0Aachieving%20high-quality%20blind%20face%20restoration.%20The%20source%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/LonglongaaaGo/VSPBFR%7D%7Bhttps%3A//github.com/LonglongaaaGo/VSPBFR%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21042v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Style%2520Prompt%2520Learning%2520Using%2520Diffusion%2520Models%2520for%2520Blind%2520Face%250A%2520%2520Restoration%26entry.906535625%3DWanglong%2520Lu%2520and%2520Jikai%2520Wang%2520and%2520Tao%2520Wang%2520and%2520Kaihao%2520Zhang%2520and%2520Xianta%2520Jiang%2520and%2520Hanli%2520Zhao%26entry.1292438233%3D%2520%2520Blind%2520face%2520restoration%2520aims%2520to%2520recover%2520high-quality%2520facial%2520images%2520from%250Avarious%2520unidentified%2520sources%2520of%2520degradation%252C%2520posing%2520significant%2520challenges%2520due%250Ato%2520the%2520minimal%2520information%2520retrievable%2520from%2520the%2520degraded%2520images.%2520Prior%250Aknowledge-based%2520methods%252C%2520leveraging%2520geometric%2520priors%2520and%2520facial%2520features%252C%2520have%250Aled%2520to%2520advancements%2520in%2520face%2520restoration%2520but%2520often%2520fall%2520short%2520of%2520capturing%2520fine%250Adetails.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520visual%2520style%2520prompt%2520learning%2520framework%250Athat%2520utilizes%2520diffusion%2520probabilistic%2520models%2520to%2520explicitly%2520generate%2520visual%250Aprompts%2520within%2520the%2520latent%2520space%2520of%2520pre-trained%2520generative%2520models.%2520These%2520prompts%250Aare%2520designed%2520to%2520guide%2520the%2520restoration%2520process.%2520To%2520fully%2520utilize%2520the%2520visual%250Aprompts%2520and%2520enhance%2520the%2520extraction%2520of%2520informative%2520and%2520rich%2520patterns%252C%2520we%250Aintroduce%2520a%2520style-modulated%2520aggregation%2520transformation%2520layer.%2520Extensive%250Aexperiments%2520and%2520applications%2520demonstrate%2520the%2520superiority%2520of%2520our%2520method%2520in%250Aachieving%2520high-quality%2520blind%2520face%2520restoration.%2520The%2520source%2520code%2520is%2520available%2520at%250A%255Chref%257Bhttps%253A//github.com/LonglongaaaGo/VSPBFR%257D%257Bhttps%253A//github.com/LonglongaaaGo/VSPBFR%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21042v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Style%20Prompt%20Learning%20Using%20Diffusion%20Models%20for%20Blind%20Face%0A%20%20Restoration&entry.906535625=Wanglong%20Lu%20and%20Jikai%20Wang%20and%20Tao%20Wang%20and%20Kaihao%20Zhang%20and%20Xianta%20Jiang%20and%20Hanli%20Zhao&entry.1292438233=%20%20Blind%20face%20restoration%20aims%20to%20recover%20high-quality%20facial%20images%20from%0Avarious%20unidentified%20sources%20of%20degradation%2C%20posing%20significant%20challenges%20due%0Ato%20the%20minimal%20information%20retrievable%20from%20the%20degraded%20images.%20Prior%0Aknowledge-based%20methods%2C%20leveraging%20geometric%20priors%20and%20facial%20features%2C%20have%0Aled%20to%20advancements%20in%20face%20restoration%20but%20often%20fall%20short%20of%20capturing%20fine%0Adetails.%20To%20address%20this%2C%20we%20introduce%20a%20visual%20style%20prompt%20learning%20framework%0Athat%20utilizes%20diffusion%20probabilistic%20models%20to%20explicitly%20generate%20visual%0Aprompts%20within%20the%20latent%20space%20of%20pre-trained%20generative%20models.%20These%20prompts%0Aare%20designed%20to%20guide%20the%20restoration%20process.%20To%20fully%20utilize%20the%20visual%0Aprompts%20and%20enhance%20the%20extraction%20of%20informative%20and%20rich%20patterns%2C%20we%0Aintroduce%20a%20style-modulated%20aggregation%20transformation%20layer.%20Extensive%0Aexperiments%20and%20applications%20demonstrate%20the%20superiority%20of%20our%20method%20in%0Aachieving%20high-quality%20blind%20face%20restoration.%20The%20source%20code%20is%20available%20at%0A%5Chref%7Bhttps%3A//github.com/LonglongaaaGo/VSPBFR%7D%7Bhttps%3A//github.com/LonglongaaaGo/VSPBFR%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21042v1&entry.124074799=Read"},
{"title": "Action-Agnostic Point-Level Supervision for Temporal Action Detection", "author": "Shuhei M. Yoshida and Takashi Shibata and Makoto Terao and Takayuki Okatani and Masashi Sugiyama", "abstract": "  We propose action-agnostic point-level (AAPL) supervision for temporal action\ndetection to achieve accurate action instance detection with a lightly\nannotated dataset. In the proposed scheme, a small portion of video frames is\nsampled in an unsupervised manner and presented to human annotators, who then\nlabel the frames with action categories. Unlike point-level supervision, which\nrequires annotators to search for every action instance in an untrimmed video,\nframes to annotate are selected without human intervention in AAPL supervision.\nWe also propose a detection model and learning method to effectively utilize\nthe AAPL labels. Extensive experiments on the variety of datasets (THUMOS '14,\nFineAction, GTEA, BEOID, and ActivityNet 1.3) demonstrate that the proposed\napproach is competitive with or outperforms prior methods for video-level and\npoint-level supervision in terms of the trade-off between the annotation cost\nand detection performance.\n", "link": "http://arxiv.org/abs/2412.21205v1", "date": "2024-12-30", "relevancy": 1.7668, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6638}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4992}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4913}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Action-Agnostic%20Point-Level%20Supervision%20for%20Temporal%20Action%20Detection&body=Title%3A%20Action-Agnostic%20Point-Level%20Supervision%20for%20Temporal%20Action%20Detection%0AAuthor%3A%20Shuhei%20M.%20Yoshida%20and%20Takashi%20Shibata%20and%20Makoto%20Terao%20and%20Takayuki%20Okatani%20and%20Masashi%20Sugiyama%0AAbstract%3A%20%20%20We%20propose%20action-agnostic%20point-level%20%28AAPL%29%20supervision%20for%20temporal%20action%0Adetection%20to%20achieve%20accurate%20action%20instance%20detection%20with%20a%20lightly%0Aannotated%20dataset.%20In%20the%20proposed%20scheme%2C%20a%20small%20portion%20of%20video%20frames%20is%0Asampled%20in%20an%20unsupervised%20manner%20and%20presented%20to%20human%20annotators%2C%20who%20then%0Alabel%20the%20frames%20with%20action%20categories.%20Unlike%20point-level%20supervision%2C%20which%0Arequires%20annotators%20to%20search%20for%20every%20action%20instance%20in%20an%20untrimmed%20video%2C%0Aframes%20to%20annotate%20are%20selected%20without%20human%20intervention%20in%20AAPL%20supervision.%0AWe%20also%20propose%20a%20detection%20model%20and%20learning%20method%20to%20effectively%20utilize%0Athe%20AAPL%20labels.%20Extensive%20experiments%20on%20the%20variety%20of%20datasets%20%28THUMOS%20%2714%2C%0AFineAction%2C%20GTEA%2C%20BEOID%2C%20and%20ActivityNet%201.3%29%20demonstrate%20that%20the%20proposed%0Aapproach%20is%20competitive%20with%20or%20outperforms%20prior%20methods%20for%20video-level%20and%0Apoint-level%20supervision%20in%20terms%20of%20the%20trade-off%20between%20the%20annotation%20cost%0Aand%20detection%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21205v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAction-Agnostic%2520Point-Level%2520Supervision%2520for%2520Temporal%2520Action%2520Detection%26entry.906535625%3DShuhei%2520M.%2520Yoshida%2520and%2520Takashi%2520Shibata%2520and%2520Makoto%2520Terao%2520and%2520Takayuki%2520Okatani%2520and%2520Masashi%2520Sugiyama%26entry.1292438233%3D%2520%2520We%2520propose%2520action-agnostic%2520point-level%2520%2528AAPL%2529%2520supervision%2520for%2520temporal%2520action%250Adetection%2520to%2520achieve%2520accurate%2520action%2520instance%2520detection%2520with%2520a%2520lightly%250Aannotated%2520dataset.%2520In%2520the%2520proposed%2520scheme%252C%2520a%2520small%2520portion%2520of%2520video%2520frames%2520is%250Asampled%2520in%2520an%2520unsupervised%2520manner%2520and%2520presented%2520to%2520human%2520annotators%252C%2520who%2520then%250Alabel%2520the%2520frames%2520with%2520action%2520categories.%2520Unlike%2520point-level%2520supervision%252C%2520which%250Arequires%2520annotators%2520to%2520search%2520for%2520every%2520action%2520instance%2520in%2520an%2520untrimmed%2520video%252C%250Aframes%2520to%2520annotate%2520are%2520selected%2520without%2520human%2520intervention%2520in%2520AAPL%2520supervision.%250AWe%2520also%2520propose%2520a%2520detection%2520model%2520and%2520learning%2520method%2520to%2520effectively%2520utilize%250Athe%2520AAPL%2520labels.%2520Extensive%2520experiments%2520on%2520the%2520variety%2520of%2520datasets%2520%2528THUMOS%2520%252714%252C%250AFineAction%252C%2520GTEA%252C%2520BEOID%252C%2520and%2520ActivityNet%25201.3%2529%2520demonstrate%2520that%2520the%2520proposed%250Aapproach%2520is%2520competitive%2520with%2520or%2520outperforms%2520prior%2520methods%2520for%2520video-level%2520and%250Apoint-level%2520supervision%2520in%2520terms%2520of%2520the%2520trade-off%2520between%2520the%2520annotation%2520cost%250Aand%2520detection%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21205v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Action-Agnostic%20Point-Level%20Supervision%20for%20Temporal%20Action%20Detection&entry.906535625=Shuhei%20M.%20Yoshida%20and%20Takashi%20Shibata%20and%20Makoto%20Terao%20and%20Takayuki%20Okatani%20and%20Masashi%20Sugiyama&entry.1292438233=%20%20We%20propose%20action-agnostic%20point-level%20%28AAPL%29%20supervision%20for%20temporal%20action%0Adetection%20to%20achieve%20accurate%20action%20instance%20detection%20with%20a%20lightly%0Aannotated%20dataset.%20In%20the%20proposed%20scheme%2C%20a%20small%20portion%20of%20video%20frames%20is%0Asampled%20in%20an%20unsupervised%20manner%20and%20presented%20to%20human%20annotators%2C%20who%20then%0Alabel%20the%20frames%20with%20action%20categories.%20Unlike%20point-level%20supervision%2C%20which%0Arequires%20annotators%20to%20search%20for%20every%20action%20instance%20in%20an%20untrimmed%20video%2C%0Aframes%20to%20annotate%20are%20selected%20without%20human%20intervention%20in%20AAPL%20supervision.%0AWe%20also%20propose%20a%20detection%20model%20and%20learning%20method%20to%20effectively%20utilize%0Athe%20AAPL%20labels.%20Extensive%20experiments%20on%20the%20variety%20of%20datasets%20%28THUMOS%20%2714%2C%0AFineAction%2C%20GTEA%2C%20BEOID%2C%20and%20ActivityNet%201.3%29%20demonstrate%20that%20the%20proposed%0Aapproach%20is%20competitive%20with%20or%20outperforms%20prior%20methods%20for%20video-level%20and%0Apoint-level%20supervision%20in%20terms%20of%20the%20trade-off%20between%20the%20annotation%20cost%0Aand%20detection%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21205v1&entry.124074799=Read"},
{"title": "ReFlow6D: Refraction-Guided Transparent Object 6D Pose Estimation via\n  Intermediate Representation Learning", "author": "Hrishikesh Gupta and Stefan Thalhammer and Jean-Baptiste Weibel and Alexander Haberl and Markus Vincze", "abstract": "  Transparent objects are ubiquitous in daily life, making their perception and\nrobotics manipulation important. However, they present a major challenge due to\ntheir distinct refractive and reflective properties when it comes to accurately\nestimating the 6D pose. To solve this, we present ReFlow6D, a novel method for\ntransparent object 6D pose estimation that harnesses the\nrefractive-intermediate representation. Unlike conventional approaches, our\nmethod leverages a feature space impervious to changes in RGB image space and\nindependent of depth information. Drawing inspiration from image matting, we\nmodel the deformation of the light path through transparent objects, yielding a\nunique object-specific intermediate representation guided by light refraction\nthat is independent of the environment in which objects are observed. By\nintegrating these intermediate features into the pose estimation network, we\nshow that ReFlow6D achieves precise 6D pose estimation of transparent objects,\nusing only RGB images as input. Our method further introduces a novel\ntransparent object compositing loss, fostering the generation of superior\nrefractive-intermediate features. Empirical evaluations show that our approach\nsignificantly outperforms state-of-the-art methods on TOD and Trans32K-6D\ndatasets. Robot grasping experiments further demonstrate that ReFlow6D's pose\nestimation accuracy effectively translates to real-world robotics task. The\nsource code is available at: https://github.com/StoicGilgamesh/ReFlow6D and\nhttps://github.com/StoicGilgamesh/matting_rendering.\n", "link": "http://arxiv.org/abs/2412.20830v1", "date": "2024-12-30", "relevancy": 1.7461, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5934}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5792}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReFlow6D%3A%20Refraction-Guided%20Transparent%20Object%206D%20Pose%20Estimation%20via%0A%20%20Intermediate%20Representation%20Learning&body=Title%3A%20ReFlow6D%3A%20Refraction-Guided%20Transparent%20Object%206D%20Pose%20Estimation%20via%0A%20%20Intermediate%20Representation%20Learning%0AAuthor%3A%20Hrishikesh%20Gupta%20and%20Stefan%20Thalhammer%20and%20Jean-Baptiste%20Weibel%20and%20Alexander%20Haberl%20and%20Markus%20Vincze%0AAbstract%3A%20%20%20Transparent%20objects%20are%20ubiquitous%20in%20daily%20life%2C%20making%20their%20perception%20and%0Arobotics%20manipulation%20important.%20However%2C%20they%20present%20a%20major%20challenge%20due%20to%0Atheir%20distinct%20refractive%20and%20reflective%20properties%20when%20it%20comes%20to%20accurately%0Aestimating%20the%206D%20pose.%20To%20solve%20this%2C%20we%20present%20ReFlow6D%2C%20a%20novel%20method%20for%0Atransparent%20object%206D%20pose%20estimation%20that%20harnesses%20the%0Arefractive-intermediate%20representation.%20Unlike%20conventional%20approaches%2C%20our%0Amethod%20leverages%20a%20feature%20space%20impervious%20to%20changes%20in%20RGB%20image%20space%20and%0Aindependent%20of%20depth%20information.%20Drawing%20inspiration%20from%20image%20matting%2C%20we%0Amodel%20the%20deformation%20of%20the%20light%20path%20through%20transparent%20objects%2C%20yielding%20a%0Aunique%20object-specific%20intermediate%20representation%20guided%20by%20light%20refraction%0Athat%20is%20independent%20of%20the%20environment%20in%20which%20objects%20are%20observed.%20By%0Aintegrating%20these%20intermediate%20features%20into%20the%20pose%20estimation%20network%2C%20we%0Ashow%20that%20ReFlow6D%20achieves%20precise%206D%20pose%20estimation%20of%20transparent%20objects%2C%0Ausing%20only%20RGB%20images%20as%20input.%20Our%20method%20further%20introduces%20a%20novel%0Atransparent%20object%20compositing%20loss%2C%20fostering%20the%20generation%20of%20superior%0Arefractive-intermediate%20features.%20Empirical%20evaluations%20show%20that%20our%20approach%0Asignificantly%20outperforms%20state-of-the-art%20methods%20on%20TOD%20and%20Trans32K-6D%0Adatasets.%20Robot%20grasping%20experiments%20further%20demonstrate%20that%20ReFlow6D%27s%20pose%0Aestimation%20accuracy%20effectively%20translates%20to%20real-world%20robotics%20task.%20The%0Asource%20code%20is%20available%20at%3A%20https%3A//github.com/StoicGilgamesh/ReFlow6D%20and%0Ahttps%3A//github.com/StoicGilgamesh/matting_rendering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20830v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReFlow6D%253A%2520Refraction-Guided%2520Transparent%2520Object%25206D%2520Pose%2520Estimation%2520via%250A%2520%2520Intermediate%2520Representation%2520Learning%26entry.906535625%3DHrishikesh%2520Gupta%2520and%2520Stefan%2520Thalhammer%2520and%2520Jean-Baptiste%2520Weibel%2520and%2520Alexander%2520Haberl%2520and%2520Markus%2520Vincze%26entry.1292438233%3D%2520%2520Transparent%2520objects%2520are%2520ubiquitous%2520in%2520daily%2520life%252C%2520making%2520their%2520perception%2520and%250Arobotics%2520manipulation%2520important.%2520However%252C%2520they%2520present%2520a%2520major%2520challenge%2520due%2520to%250Atheir%2520distinct%2520refractive%2520and%2520reflective%2520properties%2520when%2520it%2520comes%2520to%2520accurately%250Aestimating%2520the%25206D%2520pose.%2520To%2520solve%2520this%252C%2520we%2520present%2520ReFlow6D%252C%2520a%2520novel%2520method%2520for%250Atransparent%2520object%25206D%2520pose%2520estimation%2520that%2520harnesses%2520the%250Arefractive-intermediate%2520representation.%2520Unlike%2520conventional%2520approaches%252C%2520our%250Amethod%2520leverages%2520a%2520feature%2520space%2520impervious%2520to%2520changes%2520in%2520RGB%2520image%2520space%2520and%250Aindependent%2520of%2520depth%2520information.%2520Drawing%2520inspiration%2520from%2520image%2520matting%252C%2520we%250Amodel%2520the%2520deformation%2520of%2520the%2520light%2520path%2520through%2520transparent%2520objects%252C%2520yielding%2520a%250Aunique%2520object-specific%2520intermediate%2520representation%2520guided%2520by%2520light%2520refraction%250Athat%2520is%2520independent%2520of%2520the%2520environment%2520in%2520which%2520objects%2520are%2520observed.%2520By%250Aintegrating%2520these%2520intermediate%2520features%2520into%2520the%2520pose%2520estimation%2520network%252C%2520we%250Ashow%2520that%2520ReFlow6D%2520achieves%2520precise%25206D%2520pose%2520estimation%2520of%2520transparent%2520objects%252C%250Ausing%2520only%2520RGB%2520images%2520as%2520input.%2520Our%2520method%2520further%2520introduces%2520a%2520novel%250Atransparent%2520object%2520compositing%2520loss%252C%2520fostering%2520the%2520generation%2520of%2520superior%250Arefractive-intermediate%2520features.%2520Empirical%2520evaluations%2520show%2520that%2520our%2520approach%250Asignificantly%2520outperforms%2520state-of-the-art%2520methods%2520on%2520TOD%2520and%2520Trans32K-6D%250Adatasets.%2520Robot%2520grasping%2520experiments%2520further%2520demonstrate%2520that%2520ReFlow6D%2527s%2520pose%250Aestimation%2520accuracy%2520effectively%2520translates%2520to%2520real-world%2520robotics%2520task.%2520The%250Asource%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/StoicGilgamesh/ReFlow6D%2520and%250Ahttps%253A//github.com/StoicGilgamesh/matting_rendering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20830v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReFlow6D%3A%20Refraction-Guided%20Transparent%20Object%206D%20Pose%20Estimation%20via%0A%20%20Intermediate%20Representation%20Learning&entry.906535625=Hrishikesh%20Gupta%20and%20Stefan%20Thalhammer%20and%20Jean-Baptiste%20Weibel%20and%20Alexander%20Haberl%20and%20Markus%20Vincze&entry.1292438233=%20%20Transparent%20objects%20are%20ubiquitous%20in%20daily%20life%2C%20making%20their%20perception%20and%0Arobotics%20manipulation%20important.%20However%2C%20they%20present%20a%20major%20challenge%20due%20to%0Atheir%20distinct%20refractive%20and%20reflective%20properties%20when%20it%20comes%20to%20accurately%0Aestimating%20the%206D%20pose.%20To%20solve%20this%2C%20we%20present%20ReFlow6D%2C%20a%20novel%20method%20for%0Atransparent%20object%206D%20pose%20estimation%20that%20harnesses%20the%0Arefractive-intermediate%20representation.%20Unlike%20conventional%20approaches%2C%20our%0Amethod%20leverages%20a%20feature%20space%20impervious%20to%20changes%20in%20RGB%20image%20space%20and%0Aindependent%20of%20depth%20information.%20Drawing%20inspiration%20from%20image%20matting%2C%20we%0Amodel%20the%20deformation%20of%20the%20light%20path%20through%20transparent%20objects%2C%20yielding%20a%0Aunique%20object-specific%20intermediate%20representation%20guided%20by%20light%20refraction%0Athat%20is%20independent%20of%20the%20environment%20in%20which%20objects%20are%20observed.%20By%0Aintegrating%20these%20intermediate%20features%20into%20the%20pose%20estimation%20network%2C%20we%0Ashow%20that%20ReFlow6D%20achieves%20precise%206D%20pose%20estimation%20of%20transparent%20objects%2C%0Ausing%20only%20RGB%20images%20as%20input.%20Our%20method%20further%20introduces%20a%20novel%0Atransparent%20object%20compositing%20loss%2C%20fostering%20the%20generation%20of%20superior%0Arefractive-intermediate%20features.%20Empirical%20evaluations%20show%20that%20our%20approach%0Asignificantly%20outperforms%20state-of-the-art%20methods%20on%20TOD%20and%20Trans32K-6D%0Adatasets.%20Robot%20grasping%20experiments%20further%20demonstrate%20that%20ReFlow6D%27s%20pose%0Aestimation%20accuracy%20effectively%20translates%20to%20real-world%20robotics%20task.%20The%0Asource%20code%20is%20available%20at%3A%20https%3A//github.com/StoicGilgamesh/ReFlow6D%20and%0Ahttps%3A//github.com/StoicGilgamesh/matting_rendering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20830v1&entry.124074799=Read"},
{"title": "BridgePure: Revealing the Fragility of Black-box Data Protection", "author": "Yihan Wang and Yiwei Lu and Xiao-Shan Gao and Gautam Kamath and Yaoliang Yu", "abstract": "  Availability attacks, or unlearnable examples, are defensive techniques that\nallow data owners to modify their datasets in ways that prevent unauthorized\nmachine learning models from learning effectively while maintaining the data's\nintended functionality. It has led to the release of popular black-box tools\nfor users to upload personal data and receive protected counterparts. In this\nwork, we show such black-box protections can be substantially bypassed if a\nsmall set of unprotected in-distribution data is available. Specifically, an\nadversary can (1) easily acquire (unprotected, protected) pairs by querying the\nblack-box protections with the unprotected dataset; and (2) train a diffusion\nbridge model to build a mapping. This mapping, termed BridgePure, can\neffectively remove the protection from any previously unseen data within the\nsame distribution. Under this threat model, our method demonstrates superior\npurification performance on classification and style mimicry tasks, exposing\ncritical vulnerabilities in black-box data protection.\n", "link": "http://arxiv.org/abs/2412.21061v1", "date": "2024-12-30", "relevancy": 1.363, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4625}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4565}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BridgePure%3A%20Revealing%20the%20Fragility%20of%20Black-box%20Data%20Protection&body=Title%3A%20BridgePure%3A%20Revealing%20the%20Fragility%20of%20Black-box%20Data%20Protection%0AAuthor%3A%20Yihan%20Wang%20and%20Yiwei%20Lu%20and%20Xiao-Shan%20Gao%20and%20Gautam%20Kamath%20and%20Yaoliang%20Yu%0AAbstract%3A%20%20%20Availability%20attacks%2C%20or%20unlearnable%20examples%2C%20are%20defensive%20techniques%20that%0Aallow%20data%20owners%20to%20modify%20their%20datasets%20in%20ways%20that%20prevent%20unauthorized%0Amachine%20learning%20models%20from%20learning%20effectively%20while%20maintaining%20the%20data%27s%0Aintended%20functionality.%20It%20has%20led%20to%20the%20release%20of%20popular%20black-box%20tools%0Afor%20users%20to%20upload%20personal%20data%20and%20receive%20protected%20counterparts.%20In%20this%0Awork%2C%20we%20show%20such%20black-box%20protections%20can%20be%20substantially%20bypassed%20if%20a%0Asmall%20set%20of%20unprotected%20in-distribution%20data%20is%20available.%20Specifically%2C%20an%0Aadversary%20can%20%281%29%20easily%20acquire%20%28unprotected%2C%20protected%29%20pairs%20by%20querying%20the%0Ablack-box%20protections%20with%20the%20unprotected%20dataset%3B%20and%20%282%29%20train%20a%20diffusion%0Abridge%20model%20to%20build%20a%20mapping.%20This%20mapping%2C%20termed%20BridgePure%2C%20can%0Aeffectively%20remove%20the%20protection%20from%20any%20previously%20unseen%20data%20within%20the%0Asame%20distribution.%20Under%20this%20threat%20model%2C%20our%20method%20demonstrates%20superior%0Apurification%20performance%20on%20classification%20and%20style%20mimicry%20tasks%2C%20exposing%0Acritical%20vulnerabilities%20in%20black-box%20data%20protection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21061v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridgePure%253A%2520Revealing%2520the%2520Fragility%2520of%2520Black-box%2520Data%2520Protection%26entry.906535625%3DYihan%2520Wang%2520and%2520Yiwei%2520Lu%2520and%2520Xiao-Shan%2520Gao%2520and%2520Gautam%2520Kamath%2520and%2520Yaoliang%2520Yu%26entry.1292438233%3D%2520%2520Availability%2520attacks%252C%2520or%2520unlearnable%2520examples%252C%2520are%2520defensive%2520techniques%2520that%250Aallow%2520data%2520owners%2520to%2520modify%2520their%2520datasets%2520in%2520ways%2520that%2520prevent%2520unauthorized%250Amachine%2520learning%2520models%2520from%2520learning%2520effectively%2520while%2520maintaining%2520the%2520data%2527s%250Aintended%2520functionality.%2520It%2520has%2520led%2520to%2520the%2520release%2520of%2520popular%2520black-box%2520tools%250Afor%2520users%2520to%2520upload%2520personal%2520data%2520and%2520receive%2520protected%2520counterparts.%2520In%2520this%250Awork%252C%2520we%2520show%2520such%2520black-box%2520protections%2520can%2520be%2520substantially%2520bypassed%2520if%2520a%250Asmall%2520set%2520of%2520unprotected%2520in-distribution%2520data%2520is%2520available.%2520Specifically%252C%2520an%250Aadversary%2520can%2520%25281%2529%2520easily%2520acquire%2520%2528unprotected%252C%2520protected%2529%2520pairs%2520by%2520querying%2520the%250Ablack-box%2520protections%2520with%2520the%2520unprotected%2520dataset%253B%2520and%2520%25282%2529%2520train%2520a%2520diffusion%250Abridge%2520model%2520to%2520build%2520a%2520mapping.%2520This%2520mapping%252C%2520termed%2520BridgePure%252C%2520can%250Aeffectively%2520remove%2520the%2520protection%2520from%2520any%2520previously%2520unseen%2520data%2520within%2520the%250Asame%2520distribution.%2520Under%2520this%2520threat%2520model%252C%2520our%2520method%2520demonstrates%2520superior%250Apurification%2520performance%2520on%2520classification%2520and%2520style%2520mimicry%2520tasks%252C%2520exposing%250Acritical%2520vulnerabilities%2520in%2520black-box%2520data%2520protection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21061v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BridgePure%3A%20Revealing%20the%20Fragility%20of%20Black-box%20Data%20Protection&entry.906535625=Yihan%20Wang%20and%20Yiwei%20Lu%20and%20Xiao-Shan%20Gao%20and%20Gautam%20Kamath%20and%20Yaoliang%20Yu&entry.1292438233=%20%20Availability%20attacks%2C%20or%20unlearnable%20examples%2C%20are%20defensive%20techniques%20that%0Aallow%20data%20owners%20to%20modify%20their%20datasets%20in%20ways%20that%20prevent%20unauthorized%0Amachine%20learning%20models%20from%20learning%20effectively%20while%20maintaining%20the%20data%27s%0Aintended%20functionality.%20It%20has%20led%20to%20the%20release%20of%20popular%20black-box%20tools%0Afor%20users%20to%20upload%20personal%20data%20and%20receive%20protected%20counterparts.%20In%20this%0Awork%2C%20we%20show%20such%20black-box%20protections%20can%20be%20substantially%20bypassed%20if%20a%0Asmall%20set%20of%20unprotected%20in-distribution%20data%20is%20available.%20Specifically%2C%20an%0Aadversary%20can%20%281%29%20easily%20acquire%20%28unprotected%2C%20protected%29%20pairs%20by%20querying%20the%0Ablack-box%20protections%20with%20the%20unprotected%20dataset%3B%20and%20%282%29%20train%20a%20diffusion%0Abridge%20model%20to%20build%20a%20mapping.%20This%20mapping%2C%20termed%20BridgePure%2C%20can%0Aeffectively%20remove%20the%20protection%20from%20any%20previously%20unseen%20data%20within%20the%0Asame%20distribution.%20Under%20this%20threat%20model%2C%20our%20method%20demonstrates%20superior%0Apurification%20performance%20on%20classification%20and%20style%20mimicry%20tasks%2C%20exposing%0Acritical%20vulnerabilities%20in%20black-box%20data%20protection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21061v1&entry.124074799=Read"},
{"title": "Finding the Right Moment: Human-Assisted Trailer Creation via Task\n  Composition", "author": "Pinelopi Papalampidi and Frank Keller and Mirella Lapata", "abstract": "  Movie trailers perform multiple functions: they introduce viewers to the\nstory, convey the mood and artistic style of the film, and encourage audiences\nto see the movie. These diverse functions make trailer creation a challenging\nendeavor. In this work, we focus on finding trailer moments in a movie, i.e.,\nshots that could be potentially included in a trailer. We decompose this task\ninto two subtasks: narrative structure identification and sentiment prediction.\nWe model movies as graphs, where nodes are shots and edges denote semantic\nrelations between them. We learn these relations using joint contrastive\ntraining which distills rich textual information (e.g., characters, actions,\nsituations) from screenplays. An unsupervised algorithm then traverses the\ngraph and selects trailer moments from the movie that human judges prefer to\nones selected by competitive supervised approaches. A main advantage of our\nalgorithm is that it uses interpretable criteria, which allows us to deploy it\nin an interactive tool for trailer creation with a human in the loop. Our tool\nallows users to select trailer shots in under 30 minutes that are superior to\nfully automatic methods and comparable to (exclusive) manual selection by\nexperts.\n", "link": "http://arxiv.org/abs/2111.08774v2", "date": "2024-12-30", "relevancy": 1.4947, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.518}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4946}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Finding%20the%20Right%20Moment%3A%20Human-Assisted%20Trailer%20Creation%20via%20Task%0A%20%20Composition&body=Title%3A%20Finding%20the%20Right%20Moment%3A%20Human-Assisted%20Trailer%20Creation%20via%20Task%0A%20%20Composition%0AAuthor%3A%20Pinelopi%20Papalampidi%20and%20Frank%20Keller%20and%20Mirella%20Lapata%0AAbstract%3A%20%20%20Movie%20trailers%20perform%20multiple%20functions%3A%20they%20introduce%20viewers%20to%20the%0Astory%2C%20convey%20the%20mood%20and%20artistic%20style%20of%20the%20film%2C%20and%20encourage%20audiences%0Ato%20see%20the%20movie.%20These%20diverse%20functions%20make%20trailer%20creation%20a%20challenging%0Aendeavor.%20In%20this%20work%2C%20we%20focus%20on%20finding%20trailer%20moments%20in%20a%20movie%2C%20i.e.%2C%0Ashots%20that%20could%20be%20potentially%20included%20in%20a%20trailer.%20We%20decompose%20this%20task%0Ainto%20two%20subtasks%3A%20narrative%20structure%20identification%20and%20sentiment%20prediction.%0AWe%20model%20movies%20as%20graphs%2C%20where%20nodes%20are%20shots%20and%20edges%20denote%20semantic%0Arelations%20between%20them.%20We%20learn%20these%20relations%20using%20joint%20contrastive%0Atraining%20which%20distills%20rich%20textual%20information%20%28e.g.%2C%20characters%2C%20actions%2C%0Asituations%29%20from%20screenplays.%20An%20unsupervised%20algorithm%20then%20traverses%20the%0Agraph%20and%20selects%20trailer%20moments%20from%20the%20movie%20that%20human%20judges%20prefer%20to%0Aones%20selected%20by%20competitive%20supervised%20approaches.%20A%20main%20advantage%20of%20our%0Aalgorithm%20is%20that%20it%20uses%20interpretable%20criteria%2C%20which%20allows%20us%20to%20deploy%20it%0Ain%20an%20interactive%20tool%20for%20trailer%20creation%20with%20a%20human%20in%20the%20loop.%20Our%20tool%0Aallows%20users%20to%20select%20trailer%20shots%20in%20under%2030%20minutes%20that%20are%20superior%20to%0Afully%20automatic%20methods%20and%20comparable%20to%20%28exclusive%29%20manual%20selection%20by%0Aexperts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2111.08774v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFinding%2520the%2520Right%2520Moment%253A%2520Human-Assisted%2520Trailer%2520Creation%2520via%2520Task%250A%2520%2520Composition%26entry.906535625%3DPinelopi%2520Papalampidi%2520and%2520Frank%2520Keller%2520and%2520Mirella%2520Lapata%26entry.1292438233%3D%2520%2520Movie%2520trailers%2520perform%2520multiple%2520functions%253A%2520they%2520introduce%2520viewers%2520to%2520the%250Astory%252C%2520convey%2520the%2520mood%2520and%2520artistic%2520style%2520of%2520the%2520film%252C%2520and%2520encourage%2520audiences%250Ato%2520see%2520the%2520movie.%2520These%2520diverse%2520functions%2520make%2520trailer%2520creation%2520a%2520challenging%250Aendeavor.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520finding%2520trailer%2520moments%2520in%2520a%2520movie%252C%2520i.e.%252C%250Ashots%2520that%2520could%2520be%2520potentially%2520included%2520in%2520a%2520trailer.%2520We%2520decompose%2520this%2520task%250Ainto%2520two%2520subtasks%253A%2520narrative%2520structure%2520identification%2520and%2520sentiment%2520prediction.%250AWe%2520model%2520movies%2520as%2520graphs%252C%2520where%2520nodes%2520are%2520shots%2520and%2520edges%2520denote%2520semantic%250Arelations%2520between%2520them.%2520We%2520learn%2520these%2520relations%2520using%2520joint%2520contrastive%250Atraining%2520which%2520distills%2520rich%2520textual%2520information%2520%2528e.g.%252C%2520characters%252C%2520actions%252C%250Asituations%2529%2520from%2520screenplays.%2520An%2520unsupervised%2520algorithm%2520then%2520traverses%2520the%250Agraph%2520and%2520selects%2520trailer%2520moments%2520from%2520the%2520movie%2520that%2520human%2520judges%2520prefer%2520to%250Aones%2520selected%2520by%2520competitive%2520supervised%2520approaches.%2520A%2520main%2520advantage%2520of%2520our%250Aalgorithm%2520is%2520that%2520it%2520uses%2520interpretable%2520criteria%252C%2520which%2520allows%2520us%2520to%2520deploy%2520it%250Ain%2520an%2520interactive%2520tool%2520for%2520trailer%2520creation%2520with%2520a%2520human%2520in%2520the%2520loop.%2520Our%2520tool%250Aallows%2520users%2520to%2520select%2520trailer%2520shots%2520in%2520under%252030%2520minutes%2520that%2520are%2520superior%2520to%250Afully%2520automatic%2520methods%2520and%2520comparable%2520to%2520%2528exclusive%2529%2520manual%2520selection%2520by%250Aexperts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2111.08774v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Finding%20the%20Right%20Moment%3A%20Human-Assisted%20Trailer%20Creation%20via%20Task%0A%20%20Composition&entry.906535625=Pinelopi%20Papalampidi%20and%20Frank%20Keller%20and%20Mirella%20Lapata&entry.1292438233=%20%20Movie%20trailers%20perform%20multiple%20functions%3A%20they%20introduce%20viewers%20to%20the%0Astory%2C%20convey%20the%20mood%20and%20artistic%20style%20of%20the%20film%2C%20and%20encourage%20audiences%0Ato%20see%20the%20movie.%20These%20diverse%20functions%20make%20trailer%20creation%20a%20challenging%0Aendeavor.%20In%20this%20work%2C%20we%20focus%20on%20finding%20trailer%20moments%20in%20a%20movie%2C%20i.e.%2C%0Ashots%20that%20could%20be%20potentially%20included%20in%20a%20trailer.%20We%20decompose%20this%20task%0Ainto%20two%20subtasks%3A%20narrative%20structure%20identification%20and%20sentiment%20prediction.%0AWe%20model%20movies%20as%20graphs%2C%20where%20nodes%20are%20shots%20and%20edges%20denote%20semantic%0Arelations%20between%20them.%20We%20learn%20these%20relations%20using%20joint%20contrastive%0Atraining%20which%20distills%20rich%20textual%20information%20%28e.g.%2C%20characters%2C%20actions%2C%0Asituations%29%20from%20screenplays.%20An%20unsupervised%20algorithm%20then%20traverses%20the%0Agraph%20and%20selects%20trailer%20moments%20from%20the%20movie%20that%20human%20judges%20prefer%20to%0Aones%20selected%20by%20competitive%20supervised%20approaches.%20A%20main%20advantage%20of%20our%0Aalgorithm%20is%20that%20it%20uses%20interpretable%20criteria%2C%20which%20allows%20us%20to%20deploy%20it%0Ain%20an%20interactive%20tool%20for%20trailer%20creation%20with%20a%20human%20in%20the%20loop.%20Our%20tool%0Aallows%20users%20to%20select%20trailer%20shots%20in%20under%2030%20minutes%20that%20are%20superior%20to%0Afully%20automatic%20methods%20and%20comparable%20to%20%28exclusive%29%20manual%20selection%20by%0Aexperts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2111.08774v2&entry.124074799=Read"},
{"title": "Holistic Construction Automation with Modular Robots: From High-Level\n  Task Specification to Execution", "author": "Jonathan K\u00fclz and Michael Terzer and Marco Magri and Andrea Giusti and Matthias Althoff", "abstract": "  In situ robotic automation in construction is challenging due to constantly\nchanging environments, a shortage of robotic experts, and a lack of\nstandardized frameworks bridging robotics and construction practices. This work\nproposes a holistic framework for construction task specification, optimization\nof robot morphology, and mission execution using a mobile modular\nreconfigurable robot. Users can specify and monitor the desired robot behavior\nthrough a graphical interface. Our framework identifies an optimized robot\nmorphology and enables automatic real-world execution by integrating Building\nInformation Modelling (BIM). By leveraging modular robot components, we ensure\nseamless and fast adaption to the specific demands of the construction task.\nExperimental validation demonstrates that our approach robustly enables the\nautonomous execution of robotic drilling.\n", "link": "http://arxiv.org/abs/2412.20867v1", "date": "2024-12-30", "relevancy": 1.6069, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6056}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5441}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5043}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Holistic%20Construction%20Automation%20with%20Modular%20Robots%3A%20From%20High-Level%0A%20%20Task%20Specification%20to%20Execution&body=Title%3A%20Holistic%20Construction%20Automation%20with%20Modular%20Robots%3A%20From%20High-Level%0A%20%20Task%20Specification%20to%20Execution%0AAuthor%3A%20Jonathan%20K%C3%BClz%20and%20Michael%20Terzer%20and%20Marco%20Magri%20and%20Andrea%20Giusti%20and%20Matthias%20Althoff%0AAbstract%3A%20%20%20In%20situ%20robotic%20automation%20in%20construction%20is%20challenging%20due%20to%20constantly%0Achanging%20environments%2C%20a%20shortage%20of%20robotic%20experts%2C%20and%20a%20lack%20of%0Astandardized%20frameworks%20bridging%20robotics%20and%20construction%20practices.%20This%20work%0Aproposes%20a%20holistic%20framework%20for%20construction%20task%20specification%2C%20optimization%0Aof%20robot%20morphology%2C%20and%20mission%20execution%20using%20a%20mobile%20modular%0Areconfigurable%20robot.%20Users%20can%20specify%20and%20monitor%20the%20desired%20robot%20behavior%0Athrough%20a%20graphical%20interface.%20Our%20framework%20identifies%20an%20optimized%20robot%0Amorphology%20and%20enables%20automatic%20real-world%20execution%20by%20integrating%20Building%0AInformation%20Modelling%20%28BIM%29.%20By%20leveraging%20modular%20robot%20components%2C%20we%20ensure%0Aseamless%20and%20fast%20adaption%20to%20the%20specific%20demands%20of%20the%20construction%20task.%0AExperimental%20validation%20demonstrates%20that%20our%20approach%20robustly%20enables%20the%0Aautonomous%20execution%20of%20robotic%20drilling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20867v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHolistic%2520Construction%2520Automation%2520with%2520Modular%2520Robots%253A%2520From%2520High-Level%250A%2520%2520Task%2520Specification%2520to%2520Execution%26entry.906535625%3DJonathan%2520K%25C3%25BClz%2520and%2520Michael%2520Terzer%2520and%2520Marco%2520Magri%2520and%2520Andrea%2520Giusti%2520and%2520Matthias%2520Althoff%26entry.1292438233%3D%2520%2520In%2520situ%2520robotic%2520automation%2520in%2520construction%2520is%2520challenging%2520due%2520to%2520constantly%250Achanging%2520environments%252C%2520a%2520shortage%2520of%2520robotic%2520experts%252C%2520and%2520a%2520lack%2520of%250Astandardized%2520frameworks%2520bridging%2520robotics%2520and%2520construction%2520practices.%2520This%2520work%250Aproposes%2520a%2520holistic%2520framework%2520for%2520construction%2520task%2520specification%252C%2520optimization%250Aof%2520robot%2520morphology%252C%2520and%2520mission%2520execution%2520using%2520a%2520mobile%2520modular%250Areconfigurable%2520robot.%2520Users%2520can%2520specify%2520and%2520monitor%2520the%2520desired%2520robot%2520behavior%250Athrough%2520a%2520graphical%2520interface.%2520Our%2520framework%2520identifies%2520an%2520optimized%2520robot%250Amorphology%2520and%2520enables%2520automatic%2520real-world%2520execution%2520by%2520integrating%2520Building%250AInformation%2520Modelling%2520%2528BIM%2529.%2520By%2520leveraging%2520modular%2520robot%2520components%252C%2520we%2520ensure%250Aseamless%2520and%2520fast%2520adaption%2520to%2520the%2520specific%2520demands%2520of%2520the%2520construction%2520task.%250AExperimental%2520validation%2520demonstrates%2520that%2520our%2520approach%2520robustly%2520enables%2520the%250Aautonomous%2520execution%2520of%2520robotic%2520drilling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20867v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Holistic%20Construction%20Automation%20with%20Modular%20Robots%3A%20From%20High-Level%0A%20%20Task%20Specification%20to%20Execution&entry.906535625=Jonathan%20K%C3%BClz%20and%20Michael%20Terzer%20and%20Marco%20Magri%20and%20Andrea%20Giusti%20and%20Matthias%20Althoff&entry.1292438233=%20%20In%20situ%20robotic%20automation%20in%20construction%20is%20challenging%20due%20to%20constantly%0Achanging%20environments%2C%20a%20shortage%20of%20robotic%20experts%2C%20and%20a%20lack%20of%0Astandardized%20frameworks%20bridging%20robotics%20and%20construction%20practices.%20This%20work%0Aproposes%20a%20holistic%20framework%20for%20construction%20task%20specification%2C%20optimization%0Aof%20robot%20morphology%2C%20and%20mission%20execution%20using%20a%20mobile%20modular%0Areconfigurable%20robot.%20Users%20can%20specify%20and%20monitor%20the%20desired%20robot%20behavior%0Athrough%20a%20graphical%20interface.%20Our%20framework%20identifies%20an%20optimized%20robot%0Amorphology%20and%20enables%20automatic%20real-world%20execution%20by%20integrating%20Building%0AInformation%20Modelling%20%28BIM%29.%20By%20leveraging%20modular%20robot%20components%2C%20we%20ensure%0Aseamless%20and%20fast%20adaption%20to%20the%20specific%20demands%20of%20the%20construction%20task.%0AExperimental%20validation%20demonstrates%20that%20our%20approach%20robustly%20enables%20the%0Aautonomous%20execution%20of%20robotic%20drilling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20867v1&entry.124074799=Read"},
{"title": "CF-CGN: Channel Fingerprints Extrapolation for Multi-band Massive MIMO\n  Transmission based on Cycle-Consistent Generative Networks", "author": "Chenjie Xie and Li You and Zhenzhou Jin and Jinke Tang and Xiqi Gao and Xiang-Gen Xia", "abstract": "  Multi-band massive multiple-input multiple-output (MIMO) communication can\npromote the cooperation of licensed and unlicensed spectra, effectively\nenhancing spectrum efficiency for Wi-Fi and other wireless systems. As an\nenabler for multi-band transmission, channel fingerprints (CF), also known as\nthe channel knowledge map or radio environment map, are used to assist channel\nstate information (CSI) acquisition and reduce computational complexity. In\nthis paper, we propose CF-CGN (Channel Fingerprints with Cycle-consistent\nGenerative Networks) to extrapolate CF for multi-band massive MIMO transmission\nwhere licensed and unlicensed spectra cooperate to provide ubiquitous\nconnectivity. Specifically, we first model CF as a multichannel image and\ntransform the extrapolation problem into an image translation task, which\nconverts CF from one frequency to another by exploring the shared\ncharacteristics of statistical CSI in the beam domain. Then, paired generative\nnetworks are designed and coupled by variable-weight cycle consistency losses\nto fit the reciprocal relationship at different bands. Matched with the coupled\nnetworks, a joint training strategy is developed accordingly, supporting\nsynchronous optimization of all trainable parameters. During the inference\nprocess, we also introduce a refining scheme to improve the extrapolation\naccuracy based on the resolution of CF. Numerical results illustrate that our\nproposed CF-CGN can achieve bidirectional extrapolation with an error of 5-17\ndB lower than the benchmarks in different communication scenarios,\ndemonstrating its excellent generalization ability. We further show that the\nsum rate performance assisted by CF-CGN-based CF is close to that with perfect\nCSI for multi-band massive MIMO transmission.\n", "link": "http://arxiv.org/abs/2412.20885v1", "date": "2024-12-30", "relevancy": 1.3568, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4531}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4521}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CF-CGN%3A%20Channel%20Fingerprints%20Extrapolation%20for%20Multi-band%20Massive%20MIMO%0A%20%20Transmission%20based%20on%20Cycle-Consistent%20Generative%20Networks&body=Title%3A%20CF-CGN%3A%20Channel%20Fingerprints%20Extrapolation%20for%20Multi-band%20Massive%20MIMO%0A%20%20Transmission%20based%20on%20Cycle-Consistent%20Generative%20Networks%0AAuthor%3A%20Chenjie%20Xie%20and%20Li%20You%20and%20Zhenzhou%20Jin%20and%20Jinke%20Tang%20and%20Xiqi%20Gao%20and%20Xiang-Gen%20Xia%0AAbstract%3A%20%20%20Multi-band%20massive%20multiple-input%20multiple-output%20%28MIMO%29%20communication%20can%0Apromote%20the%20cooperation%20of%20licensed%20and%20unlicensed%20spectra%2C%20effectively%0Aenhancing%20spectrum%20efficiency%20for%20Wi-Fi%20and%20other%20wireless%20systems.%20As%20an%0Aenabler%20for%20multi-band%20transmission%2C%20channel%20fingerprints%20%28CF%29%2C%20also%20known%20as%0Athe%20channel%20knowledge%20map%20or%20radio%20environment%20map%2C%20are%20used%20to%20assist%20channel%0Astate%20information%20%28CSI%29%20acquisition%20and%20reduce%20computational%20complexity.%20In%0Athis%20paper%2C%20we%20propose%20CF-CGN%20%28Channel%20Fingerprints%20with%20Cycle-consistent%0AGenerative%20Networks%29%20to%20extrapolate%20CF%20for%20multi-band%20massive%20MIMO%20transmission%0Awhere%20licensed%20and%20unlicensed%20spectra%20cooperate%20to%20provide%20ubiquitous%0Aconnectivity.%20Specifically%2C%20we%20first%20model%20CF%20as%20a%20multichannel%20image%20and%0Atransform%20the%20extrapolation%20problem%20into%20an%20image%20translation%20task%2C%20which%0Aconverts%20CF%20from%20one%20frequency%20to%20another%20by%20exploring%20the%20shared%0Acharacteristics%20of%20statistical%20CSI%20in%20the%20beam%20domain.%20Then%2C%20paired%20generative%0Anetworks%20are%20designed%20and%20coupled%20by%20variable-weight%20cycle%20consistency%20losses%0Ato%20fit%20the%20reciprocal%20relationship%20at%20different%20bands.%20Matched%20with%20the%20coupled%0Anetworks%2C%20a%20joint%20training%20strategy%20is%20developed%20accordingly%2C%20supporting%0Asynchronous%20optimization%20of%20all%20trainable%20parameters.%20During%20the%20inference%0Aprocess%2C%20we%20also%20introduce%20a%20refining%20scheme%20to%20improve%20the%20extrapolation%0Aaccuracy%20based%20on%20the%20resolution%20of%20CF.%20Numerical%20results%20illustrate%20that%20our%0Aproposed%20CF-CGN%20can%20achieve%20bidirectional%20extrapolation%20with%20an%20error%20of%205-17%0AdB%20lower%20than%20the%20benchmarks%20in%20different%20communication%20scenarios%2C%0Ademonstrating%20its%20excellent%20generalization%20ability.%20We%20further%20show%20that%20the%0Asum%20rate%20performance%20assisted%20by%20CF-CGN-based%20CF%20is%20close%20to%20that%20with%20perfect%0ACSI%20for%20multi-band%20massive%20MIMO%20transmission.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.20885v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCF-CGN%253A%2520Channel%2520Fingerprints%2520Extrapolation%2520for%2520Multi-band%2520Massive%2520MIMO%250A%2520%2520Transmission%2520based%2520on%2520Cycle-Consistent%2520Generative%2520Networks%26entry.906535625%3DChenjie%2520Xie%2520and%2520Li%2520You%2520and%2520Zhenzhou%2520Jin%2520and%2520Jinke%2520Tang%2520and%2520Xiqi%2520Gao%2520and%2520Xiang-Gen%2520Xia%26entry.1292438233%3D%2520%2520Multi-band%2520massive%2520multiple-input%2520multiple-output%2520%2528MIMO%2529%2520communication%2520can%250Apromote%2520the%2520cooperation%2520of%2520licensed%2520and%2520unlicensed%2520spectra%252C%2520effectively%250Aenhancing%2520spectrum%2520efficiency%2520for%2520Wi-Fi%2520and%2520other%2520wireless%2520systems.%2520As%2520an%250Aenabler%2520for%2520multi-band%2520transmission%252C%2520channel%2520fingerprints%2520%2528CF%2529%252C%2520also%2520known%2520as%250Athe%2520channel%2520knowledge%2520map%2520or%2520radio%2520environment%2520map%252C%2520are%2520used%2520to%2520assist%2520channel%250Astate%2520information%2520%2528CSI%2529%2520acquisition%2520and%2520reduce%2520computational%2520complexity.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520CF-CGN%2520%2528Channel%2520Fingerprints%2520with%2520Cycle-consistent%250AGenerative%2520Networks%2529%2520to%2520extrapolate%2520CF%2520for%2520multi-band%2520massive%2520MIMO%2520transmission%250Awhere%2520licensed%2520and%2520unlicensed%2520spectra%2520cooperate%2520to%2520provide%2520ubiquitous%250Aconnectivity.%2520Specifically%252C%2520we%2520first%2520model%2520CF%2520as%2520a%2520multichannel%2520image%2520and%250Atransform%2520the%2520extrapolation%2520problem%2520into%2520an%2520image%2520translation%2520task%252C%2520which%250Aconverts%2520CF%2520from%2520one%2520frequency%2520to%2520another%2520by%2520exploring%2520the%2520shared%250Acharacteristics%2520of%2520statistical%2520CSI%2520in%2520the%2520beam%2520domain.%2520Then%252C%2520paired%2520generative%250Anetworks%2520are%2520designed%2520and%2520coupled%2520by%2520variable-weight%2520cycle%2520consistency%2520losses%250Ato%2520fit%2520the%2520reciprocal%2520relationship%2520at%2520different%2520bands.%2520Matched%2520with%2520the%2520coupled%250Anetworks%252C%2520a%2520joint%2520training%2520strategy%2520is%2520developed%2520accordingly%252C%2520supporting%250Asynchronous%2520optimization%2520of%2520all%2520trainable%2520parameters.%2520During%2520the%2520inference%250Aprocess%252C%2520we%2520also%2520introduce%2520a%2520refining%2520scheme%2520to%2520improve%2520the%2520extrapolation%250Aaccuracy%2520based%2520on%2520the%2520resolution%2520of%2520CF.%2520Numerical%2520results%2520illustrate%2520that%2520our%250Aproposed%2520CF-CGN%2520can%2520achieve%2520bidirectional%2520extrapolation%2520with%2520an%2520error%2520of%25205-17%250AdB%2520lower%2520than%2520the%2520benchmarks%2520in%2520different%2520communication%2520scenarios%252C%250Ademonstrating%2520its%2520excellent%2520generalization%2520ability.%2520We%2520further%2520show%2520that%2520the%250Asum%2520rate%2520performance%2520assisted%2520by%2520CF-CGN-based%2520CF%2520is%2520close%2520to%2520that%2520with%2520perfect%250ACSI%2520for%2520multi-band%2520massive%2520MIMO%2520transmission.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.20885v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CF-CGN%3A%20Channel%20Fingerprints%20Extrapolation%20for%20Multi-band%20Massive%20MIMO%0A%20%20Transmission%20based%20on%20Cycle-Consistent%20Generative%20Networks&entry.906535625=Chenjie%20Xie%20and%20Li%20You%20and%20Zhenzhou%20Jin%20and%20Jinke%20Tang%20and%20Xiqi%20Gao%20and%20Xiang-Gen%20Xia&entry.1292438233=%20%20Multi-band%20massive%20multiple-input%20multiple-output%20%28MIMO%29%20communication%20can%0Apromote%20the%20cooperation%20of%20licensed%20and%20unlicensed%20spectra%2C%20effectively%0Aenhancing%20spectrum%20efficiency%20for%20Wi-Fi%20and%20other%20wireless%20systems.%20As%20an%0Aenabler%20for%20multi-band%20transmission%2C%20channel%20fingerprints%20%28CF%29%2C%20also%20known%20as%0Athe%20channel%20knowledge%20map%20or%20radio%20environment%20map%2C%20are%20used%20to%20assist%20channel%0Astate%20information%20%28CSI%29%20acquisition%20and%20reduce%20computational%20complexity.%20In%0Athis%20paper%2C%20we%20propose%20CF-CGN%20%28Channel%20Fingerprints%20with%20Cycle-consistent%0AGenerative%20Networks%29%20to%20extrapolate%20CF%20for%20multi-band%20massive%20MIMO%20transmission%0Awhere%20licensed%20and%20unlicensed%20spectra%20cooperate%20to%20provide%20ubiquitous%0Aconnectivity.%20Specifically%2C%20we%20first%20model%20CF%20as%20a%20multichannel%20image%20and%0Atransform%20the%20extrapolation%20problem%20into%20an%20image%20translation%20task%2C%20which%0Aconverts%20CF%20from%20one%20frequency%20to%20another%20by%20exploring%20the%20shared%0Acharacteristics%20of%20statistical%20CSI%20in%20the%20beam%20domain.%20Then%2C%20paired%20generative%0Anetworks%20are%20designed%20and%20coupled%20by%20variable-weight%20cycle%20consistency%20losses%0Ato%20fit%20the%20reciprocal%20relationship%20at%20different%20bands.%20Matched%20with%20the%20coupled%0Anetworks%2C%20a%20joint%20training%20strategy%20is%20developed%20accordingly%2C%20supporting%0Asynchronous%20optimization%20of%20all%20trainable%20parameters.%20During%20the%20inference%0Aprocess%2C%20we%20also%20introduce%20a%20refining%20scheme%20to%20improve%20the%20extrapolation%0Aaccuracy%20based%20on%20the%20resolution%20of%20CF.%20Numerical%20results%20illustrate%20that%20our%0Aproposed%20CF-CGN%20can%20achieve%20bidirectional%20extrapolation%20with%20an%20error%20of%205-17%0AdB%20lower%20than%20the%20benchmarks%20in%20different%20communication%20scenarios%2C%0Ademonstrating%20its%20excellent%20generalization%20ability.%20We%20further%20show%20that%20the%0Asum%20rate%20performance%20assisted%20by%20CF-CGN-based%20CF%20is%20close%20to%20that%20with%20perfect%0ACSI%20for%20multi-band%20massive%20MIMO%20transmission.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.20885v1&entry.124074799=Read"},
{"title": "On Reward Transferability in Adversarial Inverse Reinforcement Learning:\n  Insights from Random Matrix Theory", "author": "Yangchun Zhang and Wang Zhou and Yirui Zhou", "abstract": "  In the context of inverse reinforcement learning (IRL) with a single expert,\nadversarial inverse reinforcement learning (AIRL) serves as a foundational\napproach to providing comprehensive and transferable task descriptions.\nHowever, AIRL faces practical performance challenges, primarily stemming from\nthe framework's overly idealized decomposability condition, the unclear proof\nregarding the potential equilibrium in reward recovery, or questionable\nrobustness in high-dimensional environments. This paper revisits AIRL in\n\\textbf{high-dimensional scenarios where the state space tends to infinity}.\nSpecifically, we first establish a necessary and sufficient condition for\nreward transferability by examining the rank of the matrix derived from\nsubtracting the identity matrix from the transition matrix. Furthermore,\nleveraging random matrix theory, we analyze the spectral distribution of this\nmatrix, demonstrating that our rank criterion holds with high probability even\nwhen the transition matrices are unobservable. This suggests that the\nlimitations on transfer are not inherent to the AIRL framework itself, but are\ninstead related to the training variance of the reinforcement learning\nalgorithms employed within it. Based on this insight, we propose a hybrid\nframework that integrates on-policy proximal policy optimization in the source\nenvironment with off-policy soft actor-critic in the target environment,\nleading to significant improvements in reward transfer effectiveness.\n", "link": "http://arxiv.org/abs/2410.07643v2", "date": "2024-12-30", "relevancy": 1.34, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4628}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4477}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Reward%20Transferability%20in%20Adversarial%20Inverse%20Reinforcement%20Learning%3A%0A%20%20Insights%20from%20Random%20Matrix%20Theory&body=Title%3A%20On%20Reward%20Transferability%20in%20Adversarial%20Inverse%20Reinforcement%20Learning%3A%0A%20%20Insights%20from%20Random%20Matrix%20Theory%0AAuthor%3A%20Yangchun%20Zhang%20and%20Wang%20Zhou%20and%20Yirui%20Zhou%0AAbstract%3A%20%20%20In%20the%20context%20of%20inverse%20reinforcement%20learning%20%28IRL%29%20with%20a%20single%20expert%2C%0Aadversarial%20inverse%20reinforcement%20learning%20%28AIRL%29%20serves%20as%20a%20foundational%0Aapproach%20to%20providing%20comprehensive%20and%20transferable%20task%20descriptions.%0AHowever%2C%20AIRL%20faces%20practical%20performance%20challenges%2C%20primarily%20stemming%20from%0Athe%20framework%27s%20overly%20idealized%20decomposability%20condition%2C%20the%20unclear%20proof%0Aregarding%20the%20potential%20equilibrium%20in%20reward%20recovery%2C%20or%20questionable%0Arobustness%20in%20high-dimensional%20environments.%20This%20paper%20revisits%20AIRL%20in%0A%5Ctextbf%7Bhigh-dimensional%20scenarios%20where%20the%20state%20space%20tends%20to%20infinity%7D.%0ASpecifically%2C%20we%20first%20establish%20a%20necessary%20and%20sufficient%20condition%20for%0Areward%20transferability%20by%20examining%20the%20rank%20of%20the%20matrix%20derived%20from%0Asubtracting%20the%20identity%20matrix%20from%20the%20transition%20matrix.%20Furthermore%2C%0Aleveraging%20random%20matrix%20theory%2C%20we%20analyze%20the%20spectral%20distribution%20of%20this%0Amatrix%2C%20demonstrating%20that%20our%20rank%20criterion%20holds%20with%20high%20probability%20even%0Awhen%20the%20transition%20matrices%20are%20unobservable.%20This%20suggests%20that%20the%0Alimitations%20on%20transfer%20are%20not%20inherent%20to%20the%20AIRL%20framework%20itself%2C%20but%20are%0Ainstead%20related%20to%20the%20training%20variance%20of%20the%20reinforcement%20learning%0Aalgorithms%20employed%20within%20it.%20Based%20on%20this%20insight%2C%20we%20propose%20a%20hybrid%0Aframework%20that%20integrates%20on-policy%20proximal%20policy%20optimization%20in%20the%20source%0Aenvironment%20with%20off-policy%20soft%20actor-critic%20in%20the%20target%20environment%2C%0Aleading%20to%20significant%20improvements%20in%20reward%20transfer%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.07643v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Reward%2520Transferability%2520in%2520Adversarial%2520Inverse%2520Reinforcement%2520Learning%253A%250A%2520%2520Insights%2520from%2520Random%2520Matrix%2520Theory%26entry.906535625%3DYangchun%2520Zhang%2520and%2520Wang%2520Zhou%2520and%2520Yirui%2520Zhou%26entry.1292438233%3D%2520%2520In%2520the%2520context%2520of%2520inverse%2520reinforcement%2520learning%2520%2528IRL%2529%2520with%2520a%2520single%2520expert%252C%250Aadversarial%2520inverse%2520reinforcement%2520learning%2520%2528AIRL%2529%2520serves%2520as%2520a%2520foundational%250Aapproach%2520to%2520providing%2520comprehensive%2520and%2520transferable%2520task%2520descriptions.%250AHowever%252C%2520AIRL%2520faces%2520practical%2520performance%2520challenges%252C%2520primarily%2520stemming%2520from%250Athe%2520framework%2527s%2520overly%2520idealized%2520decomposability%2520condition%252C%2520the%2520unclear%2520proof%250Aregarding%2520the%2520potential%2520equilibrium%2520in%2520reward%2520recovery%252C%2520or%2520questionable%250Arobustness%2520in%2520high-dimensional%2520environments.%2520This%2520paper%2520revisits%2520AIRL%2520in%250A%255Ctextbf%257Bhigh-dimensional%2520scenarios%2520where%2520the%2520state%2520space%2520tends%2520to%2520infinity%257D.%250ASpecifically%252C%2520we%2520first%2520establish%2520a%2520necessary%2520and%2520sufficient%2520condition%2520for%250Areward%2520transferability%2520by%2520examining%2520the%2520rank%2520of%2520the%2520matrix%2520derived%2520from%250Asubtracting%2520the%2520identity%2520matrix%2520from%2520the%2520transition%2520matrix.%2520Furthermore%252C%250Aleveraging%2520random%2520matrix%2520theory%252C%2520we%2520analyze%2520the%2520spectral%2520distribution%2520of%2520this%250Amatrix%252C%2520demonstrating%2520that%2520our%2520rank%2520criterion%2520holds%2520with%2520high%2520probability%2520even%250Awhen%2520the%2520transition%2520matrices%2520are%2520unobservable.%2520This%2520suggests%2520that%2520the%250Alimitations%2520on%2520transfer%2520are%2520not%2520inherent%2520to%2520the%2520AIRL%2520framework%2520itself%252C%2520but%2520are%250Ainstead%2520related%2520to%2520the%2520training%2520variance%2520of%2520the%2520reinforcement%2520learning%250Aalgorithms%2520employed%2520within%2520it.%2520Based%2520on%2520this%2520insight%252C%2520we%2520propose%2520a%2520hybrid%250Aframework%2520that%2520integrates%2520on-policy%2520proximal%2520policy%2520optimization%2520in%2520the%2520source%250Aenvironment%2520with%2520off-policy%2520soft%2520actor-critic%2520in%2520the%2520target%2520environment%252C%250Aleading%2520to%2520significant%2520improvements%2520in%2520reward%2520transfer%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.07643v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Reward%20Transferability%20in%20Adversarial%20Inverse%20Reinforcement%20Learning%3A%0A%20%20Insights%20from%20Random%20Matrix%20Theory&entry.906535625=Yangchun%20Zhang%20and%20Wang%20Zhou%20and%20Yirui%20Zhou&entry.1292438233=%20%20In%20the%20context%20of%20inverse%20reinforcement%20learning%20%28IRL%29%20with%20a%20single%20expert%2C%0Aadversarial%20inverse%20reinforcement%20learning%20%28AIRL%29%20serves%20as%20a%20foundational%0Aapproach%20to%20providing%20comprehensive%20and%20transferable%20task%20descriptions.%0AHowever%2C%20AIRL%20faces%20practical%20performance%20challenges%2C%20primarily%20stemming%20from%0Athe%20framework%27s%20overly%20idealized%20decomposability%20condition%2C%20the%20unclear%20proof%0Aregarding%20the%20potential%20equilibrium%20in%20reward%20recovery%2C%20or%20questionable%0Arobustness%20in%20high-dimensional%20environments.%20This%20paper%20revisits%20AIRL%20in%0A%5Ctextbf%7Bhigh-dimensional%20scenarios%20where%20the%20state%20space%20tends%20to%20infinity%7D.%0ASpecifically%2C%20we%20first%20establish%20a%20necessary%20and%20sufficient%20condition%20for%0Areward%20transferability%20by%20examining%20the%20rank%20of%20the%20matrix%20derived%20from%0Asubtracting%20the%20identity%20matrix%20from%20the%20transition%20matrix.%20Furthermore%2C%0Aleveraging%20random%20matrix%20theory%2C%20we%20analyze%20the%20spectral%20distribution%20of%20this%0Amatrix%2C%20demonstrating%20that%20our%20rank%20criterion%20holds%20with%20high%20probability%20even%0Awhen%20the%20transition%20matrices%20are%20unobservable.%20This%20suggests%20that%20the%0Alimitations%20on%20transfer%20are%20not%20inherent%20to%20the%20AIRL%20framework%20itself%2C%20but%20are%0Ainstead%20related%20to%20the%20training%20variance%20of%20the%20reinforcement%20learning%0Aalgorithms%20employed%20within%20it.%20Based%20on%20this%20insight%2C%20we%20propose%20a%20hybrid%0Aframework%20that%20integrates%20on-policy%20proximal%20policy%20optimization%20in%20the%20source%0Aenvironment%20with%20off-policy%20soft%20actor-critic%20in%20the%20target%20environment%2C%0Aleading%20to%20significant%20improvements%20in%20reward%20transfer%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.07643v2&entry.124074799=Read"},
{"title": "Investigating layer-selective transfer learning of QAOA parameters for\n  Max-Cut problem", "author": "Francesco Aldo Venturelli and Sreetama Das and Filippo Caruso", "abstract": "  Quantum approximate optimization algorithm (QAOA) is a variational quantum\nalgorithm (VQA) ideal for noisy intermediate-scale quantum (NISQ) processors,\nand is highly successful for solving combinatorial optimization problems\n(COPs). It has been observed that the optimal variational parameters obtained\nfrom one instance of a COP can be transferred to another instance, producing\nsufficiently satisfactory solutions for the latter. In this context, a suitable\nmethod for further improving the solution is to fine-tune a subset of the\ntransferred parameters. We numerically explore the role of optimizing\nindividual QAOA layers in improving the approximate solution of the Max-Cut\nproblem after parameter transfer. We also investigate the trade-off between a\ngood approximation and the required optimization time when optimizing\ntransferred QAOA parameters. These studies show that optimizing a subset of\nlayers can be more effective at a lower time-cost compared to optimizing all\nlayers.\n", "link": "http://arxiv.org/abs/2412.21071v1", "date": "2024-12-30", "relevancy": 1.7098, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4356}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4232}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4175}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20layer-selective%20transfer%20learning%20of%20QAOA%20parameters%20for%0A%20%20Max-Cut%20problem&body=Title%3A%20Investigating%20layer-selective%20transfer%20learning%20of%20QAOA%20parameters%20for%0A%20%20Max-Cut%20problem%0AAuthor%3A%20Francesco%20Aldo%20Venturelli%20and%20Sreetama%20Das%20and%20Filippo%20Caruso%0AAbstract%3A%20%20%20Quantum%20approximate%20optimization%20algorithm%20%28QAOA%29%20is%20a%20variational%20quantum%0Aalgorithm%20%28VQA%29%20ideal%20for%20noisy%20intermediate-scale%20quantum%20%28NISQ%29%20processors%2C%0Aand%20is%20highly%20successful%20for%20solving%20combinatorial%20optimization%20problems%0A%28COPs%29.%20It%20has%20been%20observed%20that%20the%20optimal%20variational%20parameters%20obtained%0Afrom%20one%20instance%20of%20a%20COP%20can%20be%20transferred%20to%20another%20instance%2C%20producing%0Asufficiently%20satisfactory%20solutions%20for%20the%20latter.%20In%20this%20context%2C%20a%20suitable%0Amethod%20for%20further%20improving%20the%20solution%20is%20to%20fine-tune%20a%20subset%20of%20the%0Atransferred%20parameters.%20We%20numerically%20explore%20the%20role%20of%20optimizing%0Aindividual%20QAOA%20layers%20in%20improving%20the%20approximate%20solution%20of%20the%20Max-Cut%0Aproblem%20after%20parameter%20transfer.%20We%20also%20investigate%20the%20trade-off%20between%20a%0Agood%20approximation%20and%20the%20required%20optimization%20time%20when%20optimizing%0Atransferred%20QAOA%20parameters.%20These%20studies%20show%20that%20optimizing%20a%20subset%20of%0Alayers%20can%20be%20more%20effective%20at%20a%20lower%20time-cost%20compared%20to%20optimizing%20all%0Alayers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21071v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520layer-selective%2520transfer%2520learning%2520of%2520QAOA%2520parameters%2520for%250A%2520%2520Max-Cut%2520problem%26entry.906535625%3DFrancesco%2520Aldo%2520Venturelli%2520and%2520Sreetama%2520Das%2520and%2520Filippo%2520Caruso%26entry.1292438233%3D%2520%2520Quantum%2520approximate%2520optimization%2520algorithm%2520%2528QAOA%2529%2520is%2520a%2520variational%2520quantum%250Aalgorithm%2520%2528VQA%2529%2520ideal%2520for%2520noisy%2520intermediate-scale%2520quantum%2520%2528NISQ%2529%2520processors%252C%250Aand%2520is%2520highly%2520successful%2520for%2520solving%2520combinatorial%2520optimization%2520problems%250A%2528COPs%2529.%2520It%2520has%2520been%2520observed%2520that%2520the%2520optimal%2520variational%2520parameters%2520obtained%250Afrom%2520one%2520instance%2520of%2520a%2520COP%2520can%2520be%2520transferred%2520to%2520another%2520instance%252C%2520producing%250Asufficiently%2520satisfactory%2520solutions%2520for%2520the%2520latter.%2520In%2520this%2520context%252C%2520a%2520suitable%250Amethod%2520for%2520further%2520improving%2520the%2520solution%2520is%2520to%2520fine-tune%2520a%2520subset%2520of%2520the%250Atransferred%2520parameters.%2520We%2520numerically%2520explore%2520the%2520role%2520of%2520optimizing%250Aindividual%2520QAOA%2520layers%2520in%2520improving%2520the%2520approximate%2520solution%2520of%2520the%2520Max-Cut%250Aproblem%2520after%2520parameter%2520transfer.%2520We%2520also%2520investigate%2520the%2520trade-off%2520between%2520a%250Agood%2520approximation%2520and%2520the%2520required%2520optimization%2520time%2520when%2520optimizing%250Atransferred%2520QAOA%2520parameters.%2520These%2520studies%2520show%2520that%2520optimizing%2520a%2520subset%2520of%250Alayers%2520can%2520be%2520more%2520effective%2520at%2520a%2520lower%2520time-cost%2520compared%2520to%2520optimizing%2520all%250Alayers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21071v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20layer-selective%20transfer%20learning%20of%20QAOA%20parameters%20for%0A%20%20Max-Cut%20problem&entry.906535625=Francesco%20Aldo%20Venturelli%20and%20Sreetama%20Das%20and%20Filippo%20Caruso&entry.1292438233=%20%20Quantum%20approximate%20optimization%20algorithm%20%28QAOA%29%20is%20a%20variational%20quantum%0Aalgorithm%20%28VQA%29%20ideal%20for%20noisy%20intermediate-scale%20quantum%20%28NISQ%29%20processors%2C%0Aand%20is%20highly%20successful%20for%20solving%20combinatorial%20optimization%20problems%0A%28COPs%29.%20It%20has%20been%20observed%20that%20the%20optimal%20variational%20parameters%20obtained%0Afrom%20one%20instance%20of%20a%20COP%20can%20be%20transferred%20to%20another%20instance%2C%20producing%0Asufficiently%20satisfactory%20solutions%20for%20the%20latter.%20In%20this%20context%2C%20a%20suitable%0Amethod%20for%20further%20improving%20the%20solution%20is%20to%20fine-tune%20a%20subset%20of%20the%0Atransferred%20parameters.%20We%20numerically%20explore%20the%20role%20of%20optimizing%0Aindividual%20QAOA%20layers%20in%20improving%20the%20approximate%20solution%20of%20the%20Max-Cut%0Aproblem%20after%20parameter%20transfer.%20We%20also%20investigate%20the%20trade-off%20between%20a%0Agood%20approximation%20and%20the%20required%20optimization%20time%20when%20optimizing%0Atransferred%20QAOA%20parameters.%20These%20studies%20show%20that%20optimizing%20a%20subset%20of%0Alayers%20can%20be%20more%20effective%20at%20a%20lower%20time-cost%20compared%20to%20optimizing%20all%0Alayers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21071v1&entry.124074799=Read"},
{"title": "Timeseria: an object-oriented time series processing library", "author": "Stefano Alberto Russo and Giuliano Taffoni and Luca Bortolussi", "abstract": "  Timeseria is an object-oriented time series processing library implemented in\nPython, which aims at making it easier to manipulate time series data and to\nbuild statistical and machine learning models on top of it. Unlike common data\nanalysis frameworks, it builds up from well defined and reusable logical units\n(objects), which can be easily combined together in order to ensure a high\nlevel of consistency. Thanks to this approach, Timeseria can address by design\nseveral non-trivial issues which are often underestimated, such as handling\ndata losses, non-uniform sampling rates, differences between aggregated data\nand punctual observations, time zones, daylight saving times, and more.\nTimeseria comes with a comprehensive set of base data structures, data\ntransformations for resampling and aggregation, common data manipulation\noperations, and extensible models for data reconstruction, forecasting and\nanomaly detection. It also integrates a fully featured, interactive plotting\nengine capable of handling even millions of data points.\n", "link": "http://arxiv.org/abs/2410.09567v3", "date": "2024-12-30", "relevancy": 1.7145, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.3591}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.3591}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3104}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Timeseria%3A%20an%20object-oriented%20time%20series%20processing%20library&body=Title%3A%20Timeseria%3A%20an%20object-oriented%20time%20series%20processing%20library%0AAuthor%3A%20Stefano%20Alberto%20Russo%20and%20Giuliano%20Taffoni%20and%20Luca%20Bortolussi%0AAbstract%3A%20%20%20Timeseria%20is%20an%20object-oriented%20time%20series%20processing%20library%20implemented%20in%0APython%2C%20which%20aims%20at%20making%20it%20easier%20to%20manipulate%20time%20series%20data%20and%20to%0Abuild%20statistical%20and%20machine%20learning%20models%20on%20top%20of%20it.%20Unlike%20common%20data%0Aanalysis%20frameworks%2C%20it%20builds%20up%20from%20well%20defined%20and%20reusable%20logical%20units%0A%28objects%29%2C%20which%20can%20be%20easily%20combined%20together%20in%20order%20to%20ensure%20a%20high%0Alevel%20of%20consistency.%20Thanks%20to%20this%20approach%2C%20Timeseria%20can%20address%20by%20design%0Aseveral%20non-trivial%20issues%20which%20are%20often%20underestimated%2C%20such%20as%20handling%0Adata%20losses%2C%20non-uniform%20sampling%20rates%2C%20differences%20between%20aggregated%20data%0Aand%20punctual%20observations%2C%20time%20zones%2C%20daylight%20saving%20times%2C%20and%20more.%0ATimeseria%20comes%20with%20a%20comprehensive%20set%20of%20base%20data%20structures%2C%20data%0Atransformations%20for%20resampling%20and%20aggregation%2C%20common%20data%20manipulation%0Aoperations%2C%20and%20extensible%20models%20for%20data%20reconstruction%2C%20forecasting%20and%0Aanomaly%20detection.%20It%20also%20integrates%20a%20fully%20featured%2C%20interactive%20plotting%0Aengine%20capable%20of%20handling%20even%20millions%20of%20data%20points.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.09567v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTimeseria%253A%2520an%2520object-oriented%2520time%2520series%2520processing%2520library%26entry.906535625%3DStefano%2520Alberto%2520Russo%2520and%2520Giuliano%2520Taffoni%2520and%2520Luca%2520Bortolussi%26entry.1292438233%3D%2520%2520Timeseria%2520is%2520an%2520object-oriented%2520time%2520series%2520processing%2520library%2520implemented%2520in%250APython%252C%2520which%2520aims%2520at%2520making%2520it%2520easier%2520to%2520manipulate%2520time%2520series%2520data%2520and%2520to%250Abuild%2520statistical%2520and%2520machine%2520learning%2520models%2520on%2520top%2520of%2520it.%2520Unlike%2520common%2520data%250Aanalysis%2520frameworks%252C%2520it%2520builds%2520up%2520from%2520well%2520defined%2520and%2520reusable%2520logical%2520units%250A%2528objects%2529%252C%2520which%2520can%2520be%2520easily%2520combined%2520together%2520in%2520order%2520to%2520ensure%2520a%2520high%250Alevel%2520of%2520consistency.%2520Thanks%2520to%2520this%2520approach%252C%2520Timeseria%2520can%2520address%2520by%2520design%250Aseveral%2520non-trivial%2520issues%2520which%2520are%2520often%2520underestimated%252C%2520such%2520as%2520handling%250Adata%2520losses%252C%2520non-uniform%2520sampling%2520rates%252C%2520differences%2520between%2520aggregated%2520data%250Aand%2520punctual%2520observations%252C%2520time%2520zones%252C%2520daylight%2520saving%2520times%252C%2520and%2520more.%250ATimeseria%2520comes%2520with%2520a%2520comprehensive%2520set%2520of%2520base%2520data%2520structures%252C%2520data%250Atransformations%2520for%2520resampling%2520and%2520aggregation%252C%2520common%2520data%2520manipulation%250Aoperations%252C%2520and%2520extensible%2520models%2520for%2520data%2520reconstruction%252C%2520forecasting%2520and%250Aanomaly%2520detection.%2520It%2520also%2520integrates%2520a%2520fully%2520featured%252C%2520interactive%2520plotting%250Aengine%2520capable%2520of%2520handling%2520even%2520millions%2520of%2520data%2520points.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.09567v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Timeseria%3A%20an%20object-oriented%20time%20series%20processing%20library&entry.906535625=Stefano%20Alberto%20Russo%20and%20Giuliano%20Taffoni%20and%20Luca%20Bortolussi&entry.1292438233=%20%20Timeseria%20is%20an%20object-oriented%20time%20series%20processing%20library%20implemented%20in%0APython%2C%20which%20aims%20at%20making%20it%20easier%20to%20manipulate%20time%20series%20data%20and%20to%0Abuild%20statistical%20and%20machine%20learning%20models%20on%20top%20of%20it.%20Unlike%20common%20data%0Aanalysis%20frameworks%2C%20it%20builds%20up%20from%20well%20defined%20and%20reusable%20logical%20units%0A%28objects%29%2C%20which%20can%20be%20easily%20combined%20together%20in%20order%20to%20ensure%20a%20high%0Alevel%20of%20consistency.%20Thanks%20to%20this%20approach%2C%20Timeseria%20can%20address%20by%20design%0Aseveral%20non-trivial%20issues%20which%20are%20often%20underestimated%2C%20such%20as%20handling%0Adata%20losses%2C%20non-uniform%20sampling%20rates%2C%20differences%20between%20aggregated%20data%0Aand%20punctual%20observations%2C%20time%20zones%2C%20daylight%20saving%20times%2C%20and%20more.%0ATimeseria%20comes%20with%20a%20comprehensive%20set%20of%20base%20data%20structures%2C%20data%0Atransformations%20for%20resampling%20and%20aggregation%2C%20common%20data%20manipulation%0Aoperations%2C%20and%20extensible%20models%20for%20data%20reconstruction%2C%20forecasting%20and%0Aanomaly%20detection.%20It%20also%20integrates%20a%20fully%20featured%2C%20interactive%20plotting%0Aengine%20capable%20of%20handling%20even%20millions%20of%20data%20points.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.09567v3&entry.124074799=Read"},
{"title": "Physically Guided Deep Unsupervised Inversion for 1D Magnetotelluric\n  Models", "author": "Paul Goyes-Pe\u00f1afiel and Umair bin Waheed and Henry Arguello", "abstract": "  The global demand for unconventional energy sources such as geothermal energy\nand white hydrogen requires new exploration techniques for precise subsurface\nstructure characterization and potential reservoir identification. The\nMagnetotelluric (MT) method is crucial for these tasks, providing critical\ninformation on the distribution of subsurface electrical resistivity at depths\nranging from hundreds to thousands of meters. However, traditional iterative\nalgorithm-based inversion methods require the adjustment of multiple\nparameters, demanding time-consuming and exhaustive tuning processes to achieve\nproper cost function minimization. Although recent advances have incorporated\ndeep learning algorithms for MT inversion, primarily based on supervised\nlearning, \\paul{and} needs large labeled datasets for training. This work\nutilizes TensorFlow operations to create a differentiable forward MT operator,\nleveraging its automatic differentiation capability. Moreover, instead of\nsolving for the subsurface model directly, as classical algorithms perform,\nthis paper presents a new deep unsupervised inversion algorithm guided by\nphysics to estimate 1D MT models. Instead of using datasets with the observed\ndata and their respective model as labels during training, our method employs a\ndifferentiable modeling operator that physically guides the cost function\nminimization, making the proposed method solely dependent on observed data.\nTherefore, the optimization \\paul{algorithm} updates the network weights to\nminimize the data misfit. We test the proposed method with field and synthetic\ndata at different acquisition frequencies, demonstrating that the resistivity\nmodels obtained are more accurate than those calculated using other techniques.\n", "link": "http://arxiv.org/abs/2410.15274v2", "date": "2024-12-30", "relevancy": 1.4983, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.522}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4931}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physically%20Guided%20Deep%20Unsupervised%20Inversion%20for%201D%20Magnetotelluric%0A%20%20Models&body=Title%3A%20Physically%20Guided%20Deep%20Unsupervised%20Inversion%20for%201D%20Magnetotelluric%0A%20%20Models%0AAuthor%3A%20Paul%20Goyes-Pe%C3%B1afiel%20and%20Umair%20bin%20Waheed%20and%20Henry%20Arguello%0AAbstract%3A%20%20%20The%20global%20demand%20for%20unconventional%20energy%20sources%20such%20as%20geothermal%20energy%0Aand%20white%20hydrogen%20requires%20new%20exploration%20techniques%20for%20precise%20subsurface%0Astructure%20characterization%20and%20potential%20reservoir%20identification.%20The%0AMagnetotelluric%20%28MT%29%20method%20is%20crucial%20for%20these%20tasks%2C%20providing%20critical%0Ainformation%20on%20the%20distribution%20of%20subsurface%20electrical%20resistivity%20at%20depths%0Aranging%20from%20hundreds%20to%20thousands%20of%20meters.%20However%2C%20traditional%20iterative%0Aalgorithm-based%20inversion%20methods%20require%20the%20adjustment%20of%20multiple%0Aparameters%2C%20demanding%20time-consuming%20and%20exhaustive%20tuning%20processes%20to%20achieve%0Aproper%20cost%20function%20minimization.%20Although%20recent%20advances%20have%20incorporated%0Adeep%20learning%20algorithms%20for%20MT%20inversion%2C%20primarily%20based%20on%20supervised%0Alearning%2C%20%5Cpaul%7Band%7D%20needs%20large%20labeled%20datasets%20for%20training.%20This%20work%0Autilizes%20TensorFlow%20operations%20to%20create%20a%20differentiable%20forward%20MT%20operator%2C%0Aleveraging%20its%20automatic%20differentiation%20capability.%20Moreover%2C%20instead%20of%0Asolving%20for%20the%20subsurface%20model%20directly%2C%20as%20classical%20algorithms%20perform%2C%0Athis%20paper%20presents%20a%20new%20deep%20unsupervised%20inversion%20algorithm%20guided%20by%0Aphysics%20to%20estimate%201D%20MT%20models.%20Instead%20of%20using%20datasets%20with%20the%20observed%0Adata%20and%20their%20respective%20model%20as%20labels%20during%20training%2C%20our%20method%20employs%20a%0Adifferentiable%20modeling%20operator%20that%20physically%20guides%20the%20cost%20function%0Aminimization%2C%20making%20the%20proposed%20method%20solely%20dependent%20on%20observed%20data.%0ATherefore%2C%20the%20optimization%20%5Cpaul%7Balgorithm%7D%20updates%20the%20network%20weights%20to%0Aminimize%20the%20data%20misfit.%20We%20test%20the%20proposed%20method%20with%20field%20and%20synthetic%0Adata%20at%20different%20acquisition%20frequencies%2C%20demonstrating%20that%20the%20resistivity%0Amodels%20obtained%20are%20more%20accurate%20than%20those%20calculated%20using%20other%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.15274v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysically%2520Guided%2520Deep%2520Unsupervised%2520Inversion%2520for%25201D%2520Magnetotelluric%250A%2520%2520Models%26entry.906535625%3DPaul%2520Goyes-Pe%25C3%25B1afiel%2520and%2520Umair%2520bin%2520Waheed%2520and%2520Henry%2520Arguello%26entry.1292438233%3D%2520%2520The%2520global%2520demand%2520for%2520unconventional%2520energy%2520sources%2520such%2520as%2520geothermal%2520energy%250Aand%2520white%2520hydrogen%2520requires%2520new%2520exploration%2520techniques%2520for%2520precise%2520subsurface%250Astructure%2520characterization%2520and%2520potential%2520reservoir%2520identification.%2520The%250AMagnetotelluric%2520%2528MT%2529%2520method%2520is%2520crucial%2520for%2520these%2520tasks%252C%2520providing%2520critical%250Ainformation%2520on%2520the%2520distribution%2520of%2520subsurface%2520electrical%2520resistivity%2520at%2520depths%250Aranging%2520from%2520hundreds%2520to%2520thousands%2520of%2520meters.%2520However%252C%2520traditional%2520iterative%250Aalgorithm-based%2520inversion%2520methods%2520require%2520the%2520adjustment%2520of%2520multiple%250Aparameters%252C%2520demanding%2520time-consuming%2520and%2520exhaustive%2520tuning%2520processes%2520to%2520achieve%250Aproper%2520cost%2520function%2520minimization.%2520Although%2520recent%2520advances%2520have%2520incorporated%250Adeep%2520learning%2520algorithms%2520for%2520MT%2520inversion%252C%2520primarily%2520based%2520on%2520supervised%250Alearning%252C%2520%255Cpaul%257Band%257D%2520needs%2520large%2520labeled%2520datasets%2520for%2520training.%2520This%2520work%250Autilizes%2520TensorFlow%2520operations%2520to%2520create%2520a%2520differentiable%2520forward%2520MT%2520operator%252C%250Aleveraging%2520its%2520automatic%2520differentiation%2520capability.%2520Moreover%252C%2520instead%2520of%250Asolving%2520for%2520the%2520subsurface%2520model%2520directly%252C%2520as%2520classical%2520algorithms%2520perform%252C%250Athis%2520paper%2520presents%2520a%2520new%2520deep%2520unsupervised%2520inversion%2520algorithm%2520guided%2520by%250Aphysics%2520to%2520estimate%25201D%2520MT%2520models.%2520Instead%2520of%2520using%2520datasets%2520with%2520the%2520observed%250Adata%2520and%2520their%2520respective%2520model%2520as%2520labels%2520during%2520training%252C%2520our%2520method%2520employs%2520a%250Adifferentiable%2520modeling%2520operator%2520that%2520physically%2520guides%2520the%2520cost%2520function%250Aminimization%252C%2520making%2520the%2520proposed%2520method%2520solely%2520dependent%2520on%2520observed%2520data.%250ATherefore%252C%2520the%2520optimization%2520%255Cpaul%257Balgorithm%257D%2520updates%2520the%2520network%2520weights%2520to%250Aminimize%2520the%2520data%2520misfit.%2520We%2520test%2520the%2520proposed%2520method%2520with%2520field%2520and%2520synthetic%250Adata%2520at%2520different%2520acquisition%2520frequencies%252C%2520demonstrating%2520that%2520the%2520resistivity%250Amodels%2520obtained%2520are%2520more%2520accurate%2520than%2520those%2520calculated%2520using%2520other%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.15274v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physically%20Guided%20Deep%20Unsupervised%20Inversion%20for%201D%20Magnetotelluric%0A%20%20Models&entry.906535625=Paul%20Goyes-Pe%C3%B1afiel%20and%20Umair%20bin%20Waheed%20and%20Henry%20Arguello&entry.1292438233=%20%20The%20global%20demand%20for%20unconventional%20energy%20sources%20such%20as%20geothermal%20energy%0Aand%20white%20hydrogen%20requires%20new%20exploration%20techniques%20for%20precise%20subsurface%0Astructure%20characterization%20and%20potential%20reservoir%20identification.%20The%0AMagnetotelluric%20%28MT%29%20method%20is%20crucial%20for%20these%20tasks%2C%20providing%20critical%0Ainformation%20on%20the%20distribution%20of%20subsurface%20electrical%20resistivity%20at%20depths%0Aranging%20from%20hundreds%20to%20thousands%20of%20meters.%20However%2C%20traditional%20iterative%0Aalgorithm-based%20inversion%20methods%20require%20the%20adjustment%20of%20multiple%0Aparameters%2C%20demanding%20time-consuming%20and%20exhaustive%20tuning%20processes%20to%20achieve%0Aproper%20cost%20function%20minimization.%20Although%20recent%20advances%20have%20incorporated%0Adeep%20learning%20algorithms%20for%20MT%20inversion%2C%20primarily%20based%20on%20supervised%0Alearning%2C%20%5Cpaul%7Band%7D%20needs%20large%20labeled%20datasets%20for%20training.%20This%20work%0Autilizes%20TensorFlow%20operations%20to%20create%20a%20differentiable%20forward%20MT%20operator%2C%0Aleveraging%20its%20automatic%20differentiation%20capability.%20Moreover%2C%20instead%20of%0Asolving%20for%20the%20subsurface%20model%20directly%2C%20as%20classical%20algorithms%20perform%2C%0Athis%20paper%20presents%20a%20new%20deep%20unsupervised%20inversion%20algorithm%20guided%20by%0Aphysics%20to%20estimate%201D%20MT%20models.%20Instead%20of%20using%20datasets%20with%20the%20observed%0Adata%20and%20their%20respective%20model%20as%20labels%20during%20training%2C%20our%20method%20employs%20a%0Adifferentiable%20modeling%20operator%20that%20physically%20guides%20the%20cost%20function%0Aminimization%2C%20making%20the%20proposed%20method%20solely%20dependent%20on%20observed%20data.%0ATherefore%2C%20the%20optimization%20%5Cpaul%7Balgorithm%7D%20updates%20the%20network%20weights%20to%0Aminimize%20the%20data%20misfit.%20We%20test%20the%20proposed%20method%20with%20field%20and%20synthetic%0Adata%20at%20different%20acquisition%20frequencies%2C%20demonstrating%20that%20the%20resistivity%0Amodels%20obtained%20are%20more%20accurate%20than%20those%20calculated%20using%20other%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.15274v2&entry.124074799=Read"},
{"title": "Online Joint Assortment-Inventory Optimization under MNL Choices", "author": "Yong Liang and Xiaojie Mao and Shiyuan Wang", "abstract": "  We study an online joint assortment-inventory optimization problem, in which\nwe assume that the choice behavior of each customer follows the Multinomial\nLogit (MNL) choice model, and the attraction parameters are unknown a priori.\nThe retailer makes periodic assortment and inventory decisions to dynamically\nlearn from the customer choice observations about the attraction parameters\nwhile maximizing the expected total profit over time. In this paper, we propose\na novel algorithm that can effectively balance exploration and exploitation in\nthe online decision-making of assortment and inventory. Our algorithm builds on\na new estimator for the MNL attraction parameters, an innovative approach to\nincentivize exploration by adaptively tuning certain known and unknown\nparameters, and an optimization oracle to static single-cycle\nassortment-inventory planning problems with given parameters. We establish a\nregret upper bound for our algorithm and a lower bound for the online joint\nassortment-inventory optimization problem, suggesting that our algorithm\nachieves nearly optimal regret rate, provided that the static optimization\noracle is exact. Then we incorporate more practical approximate static\noptimization oracles into our algorithm, and bound from above the impact of\nstatic optimization errors on the regret of our algorithm. We perform numerical\nstudies to demonstrate the effectiveness of our proposed algorithm.At last, we\nextend our study by incorporating inventory carryover and the learning of\ncustomer arrival distribution.\n", "link": "http://arxiv.org/abs/2304.02022v2", "date": "2024-12-30", "relevancy": 1.3699, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4671}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4598}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4383}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Joint%20Assortment-Inventory%20Optimization%20under%20MNL%20Choices&body=Title%3A%20Online%20Joint%20Assortment-Inventory%20Optimization%20under%20MNL%20Choices%0AAuthor%3A%20Yong%20Liang%20and%20Xiaojie%20Mao%20and%20Shiyuan%20Wang%0AAbstract%3A%20%20%20We%20study%20an%20online%20joint%20assortment-inventory%20optimization%20problem%2C%20in%20which%0Awe%20assume%20that%20the%20choice%20behavior%20of%20each%20customer%20follows%20the%20Multinomial%0ALogit%20%28MNL%29%20choice%20model%2C%20and%20the%20attraction%20parameters%20are%20unknown%20a%20priori.%0AThe%20retailer%20makes%20periodic%20assortment%20and%20inventory%20decisions%20to%20dynamically%0Alearn%20from%20the%20customer%20choice%20observations%20about%20the%20attraction%20parameters%0Awhile%20maximizing%20the%20expected%20total%20profit%20over%20time.%20In%20this%20paper%2C%20we%20propose%0Aa%20novel%20algorithm%20that%20can%20effectively%20balance%20exploration%20and%20exploitation%20in%0Athe%20online%20decision-making%20of%20assortment%20and%20inventory.%20Our%20algorithm%20builds%20on%0Aa%20new%20estimator%20for%20the%20MNL%20attraction%20parameters%2C%20an%20innovative%20approach%20to%0Aincentivize%20exploration%20by%20adaptively%20tuning%20certain%20known%20and%20unknown%0Aparameters%2C%20and%20an%20optimization%20oracle%20to%20static%20single-cycle%0Aassortment-inventory%20planning%20problems%20with%20given%20parameters.%20We%20establish%20a%0Aregret%20upper%20bound%20for%20our%20algorithm%20and%20a%20lower%20bound%20for%20the%20online%20joint%0Aassortment-inventory%20optimization%20problem%2C%20suggesting%20that%20our%20algorithm%0Aachieves%20nearly%20optimal%20regret%20rate%2C%20provided%20that%20the%20static%20optimization%0Aoracle%20is%20exact.%20Then%20we%20incorporate%20more%20practical%20approximate%20static%0Aoptimization%20oracles%20into%20our%20algorithm%2C%20and%20bound%20from%20above%20the%20impact%20of%0Astatic%20optimization%20errors%20on%20the%20regret%20of%20our%20algorithm.%20We%20perform%20numerical%0Astudies%20to%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20algorithm.At%20last%2C%20we%0Aextend%20our%20study%20by%20incorporating%20inventory%20carryover%20and%20the%20learning%20of%0Acustomer%20arrival%20distribution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.02022v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Joint%2520Assortment-Inventory%2520Optimization%2520under%2520MNL%2520Choices%26entry.906535625%3DYong%2520Liang%2520and%2520Xiaojie%2520Mao%2520and%2520Shiyuan%2520Wang%26entry.1292438233%3D%2520%2520We%2520study%2520an%2520online%2520joint%2520assortment-inventory%2520optimization%2520problem%252C%2520in%2520which%250Awe%2520assume%2520that%2520the%2520choice%2520behavior%2520of%2520each%2520customer%2520follows%2520the%2520Multinomial%250ALogit%2520%2528MNL%2529%2520choice%2520model%252C%2520and%2520the%2520attraction%2520parameters%2520are%2520unknown%2520a%2520priori.%250AThe%2520retailer%2520makes%2520periodic%2520assortment%2520and%2520inventory%2520decisions%2520to%2520dynamically%250Alearn%2520from%2520the%2520customer%2520choice%2520observations%2520about%2520the%2520attraction%2520parameters%250Awhile%2520maximizing%2520the%2520expected%2520total%2520profit%2520over%2520time.%2520In%2520this%2520paper%252C%2520we%2520propose%250Aa%2520novel%2520algorithm%2520that%2520can%2520effectively%2520balance%2520exploration%2520and%2520exploitation%2520in%250Athe%2520online%2520decision-making%2520of%2520assortment%2520and%2520inventory.%2520Our%2520algorithm%2520builds%2520on%250Aa%2520new%2520estimator%2520for%2520the%2520MNL%2520attraction%2520parameters%252C%2520an%2520innovative%2520approach%2520to%250Aincentivize%2520exploration%2520by%2520adaptively%2520tuning%2520certain%2520known%2520and%2520unknown%250Aparameters%252C%2520and%2520an%2520optimization%2520oracle%2520to%2520static%2520single-cycle%250Aassortment-inventory%2520planning%2520problems%2520with%2520given%2520parameters.%2520We%2520establish%2520a%250Aregret%2520upper%2520bound%2520for%2520our%2520algorithm%2520and%2520a%2520lower%2520bound%2520for%2520the%2520online%2520joint%250Aassortment-inventory%2520optimization%2520problem%252C%2520suggesting%2520that%2520our%2520algorithm%250Aachieves%2520nearly%2520optimal%2520regret%2520rate%252C%2520provided%2520that%2520the%2520static%2520optimization%250Aoracle%2520is%2520exact.%2520Then%2520we%2520incorporate%2520more%2520practical%2520approximate%2520static%250Aoptimization%2520oracles%2520into%2520our%2520algorithm%252C%2520and%2520bound%2520from%2520above%2520the%2520impact%2520of%250Astatic%2520optimization%2520errors%2520on%2520the%2520regret%2520of%2520our%2520algorithm.%2520We%2520perform%2520numerical%250Astudies%2520to%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520algorithm.At%2520last%252C%2520we%250Aextend%2520our%2520study%2520by%2520incorporating%2520inventory%2520carryover%2520and%2520the%2520learning%2520of%250Acustomer%2520arrival%2520distribution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2304.02022v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Joint%20Assortment-Inventory%20Optimization%20under%20MNL%20Choices&entry.906535625=Yong%20Liang%20and%20Xiaojie%20Mao%20and%20Shiyuan%20Wang&entry.1292438233=%20%20We%20study%20an%20online%20joint%20assortment-inventory%20optimization%20problem%2C%20in%20which%0Awe%20assume%20that%20the%20choice%20behavior%20of%20each%20customer%20follows%20the%20Multinomial%0ALogit%20%28MNL%29%20choice%20model%2C%20and%20the%20attraction%20parameters%20are%20unknown%20a%20priori.%0AThe%20retailer%20makes%20periodic%20assortment%20and%20inventory%20decisions%20to%20dynamically%0Alearn%20from%20the%20customer%20choice%20observations%20about%20the%20attraction%20parameters%0Awhile%20maximizing%20the%20expected%20total%20profit%20over%20time.%20In%20this%20paper%2C%20we%20propose%0Aa%20novel%20algorithm%20that%20can%20effectively%20balance%20exploration%20and%20exploitation%20in%0Athe%20online%20decision-making%20of%20assortment%20and%20inventory.%20Our%20algorithm%20builds%20on%0Aa%20new%20estimator%20for%20the%20MNL%20attraction%20parameters%2C%20an%20innovative%20approach%20to%0Aincentivize%20exploration%20by%20adaptively%20tuning%20certain%20known%20and%20unknown%0Aparameters%2C%20and%20an%20optimization%20oracle%20to%20static%20single-cycle%0Aassortment-inventory%20planning%20problems%20with%20given%20parameters.%20We%20establish%20a%0Aregret%20upper%20bound%20for%20our%20algorithm%20and%20a%20lower%20bound%20for%20the%20online%20joint%0Aassortment-inventory%20optimization%20problem%2C%20suggesting%20that%20our%20algorithm%0Aachieves%20nearly%20optimal%20regret%20rate%2C%20provided%20that%20the%20static%20optimization%0Aoracle%20is%20exact.%20Then%20we%20incorporate%20more%20practical%20approximate%20static%0Aoptimization%20oracles%20into%20our%20algorithm%2C%20and%20bound%20from%20above%20the%20impact%20of%0Astatic%20optimization%20errors%20on%20the%20regret%20of%20our%20algorithm.%20We%20perform%20numerical%0Astudies%20to%20demonstrate%20the%20effectiveness%20of%20our%20proposed%20algorithm.At%20last%2C%20we%0Aextend%20our%20study%20by%20incorporating%20inventory%20carryover%20and%20the%20learning%20of%0Acustomer%20arrival%20distribution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.02022v2&entry.124074799=Read"},
{"title": "Exploring and Controlling Diversity in LLM-Agent Conversation", "author": "KuanChao Chu and Yi-Pei Chen and Hideki Nakayama", "abstract": "  Diversity is a critical aspect of multi-agent communication. In this paper,\nwe focus on controlling and exploring diversity in the context of open-domain\nmulti-agent conversations, particularly for world simulation applications. We\npropose Adaptive Prompt Pruning (APP), a novel method that dynamically adjusts\nthe content of the utterance generation prompt to control diversity using a\nsingle parameter, lambda. Through extensive experiments, we show that APP\neffectively controls the output diversity across models and datasets, with\npruning more information leading to more diverse output. We comprehensively\nanalyze the relationship between prompt content and conversational diversity.\nOur findings reveal that information from all components of the prompt\ngenerally constrains the diversity of the output, with the Memory block\nexerting the most significant influence. APP is compatible with established\ntechniques like temperature sampling and top-p sampling, providing a versatile\ntool for diversity management. To address the trade-offs of increased\ndiversity, such as inconsistencies with omitted information, we incorporate a\npost-generation correction step, which effectively balances diversity\nenhancement with output consistency. Additionally, we examine how prompt\nstructure, including component order and length, impacts diversity. This study\naddresses key questions surrounding diversity in multi-agent world simulation,\noffering insights into its control, influencing factors, and associated\ntrade-offs. Our contributions lay the foundation for systematically engineering\ndiversity in LLM-based multi-agent collaborations, advancing their\neffectiveness in real-world applications.\n", "link": "http://arxiv.org/abs/2412.21102v1", "date": "2024-12-30", "relevancy": 1.483, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4995}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4971}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20and%20Controlling%20Diversity%20in%20LLM-Agent%20Conversation&body=Title%3A%20Exploring%20and%20Controlling%20Diversity%20in%20LLM-Agent%20Conversation%0AAuthor%3A%20KuanChao%20Chu%20and%20Yi-Pei%20Chen%20and%20Hideki%20Nakayama%0AAbstract%3A%20%20%20Diversity%20is%20a%20critical%20aspect%20of%20multi-agent%20communication.%20In%20this%20paper%2C%0Awe%20focus%20on%20controlling%20and%20exploring%20diversity%20in%20the%20context%20of%20open-domain%0Amulti-agent%20conversations%2C%20particularly%20for%20world%20simulation%20applications.%20We%0Apropose%20Adaptive%20Prompt%20Pruning%20%28APP%29%2C%20a%20novel%20method%20that%20dynamically%20adjusts%0Athe%20content%20of%20the%20utterance%20generation%20prompt%20to%20control%20diversity%20using%20a%0Asingle%20parameter%2C%20lambda.%20Through%20extensive%20experiments%2C%20we%20show%20that%20APP%0Aeffectively%20controls%20the%20output%20diversity%20across%20models%20and%20datasets%2C%20with%0Apruning%20more%20information%20leading%20to%20more%20diverse%20output.%20We%20comprehensively%0Aanalyze%20the%20relationship%20between%20prompt%20content%20and%20conversational%20diversity.%0AOur%20findings%20reveal%20that%20information%20from%20all%20components%20of%20the%20prompt%0Agenerally%20constrains%20the%20diversity%20of%20the%20output%2C%20with%20the%20Memory%20block%0Aexerting%20the%20most%20significant%20influence.%20APP%20is%20compatible%20with%20established%0Atechniques%20like%20temperature%20sampling%20and%20top-p%20sampling%2C%20providing%20a%20versatile%0Atool%20for%20diversity%20management.%20To%20address%20the%20trade-offs%20of%20increased%0Adiversity%2C%20such%20as%20inconsistencies%20with%20omitted%20information%2C%20we%20incorporate%20a%0Apost-generation%20correction%20step%2C%20which%20effectively%20balances%20diversity%0Aenhancement%20with%20output%20consistency.%20Additionally%2C%20we%20examine%20how%20prompt%0Astructure%2C%20including%20component%20order%20and%20length%2C%20impacts%20diversity.%20This%20study%0Aaddresses%20key%20questions%20surrounding%20diversity%20in%20multi-agent%20world%20simulation%2C%0Aoffering%20insights%20into%20its%20control%2C%20influencing%20factors%2C%20and%20associated%0Atrade-offs.%20Our%20contributions%20lay%20the%20foundation%20for%20systematically%20engineering%0Adiversity%20in%20LLM-based%20multi-agent%20collaborations%2C%20advancing%20their%0Aeffectiveness%20in%20real-world%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.21102v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520and%2520Controlling%2520Diversity%2520in%2520LLM-Agent%2520Conversation%26entry.906535625%3DKuanChao%2520Chu%2520and%2520Yi-Pei%2520Chen%2520and%2520Hideki%2520Nakayama%26entry.1292438233%3D%2520%2520Diversity%2520is%2520a%2520critical%2520aspect%2520of%2520multi-agent%2520communication.%2520In%2520this%2520paper%252C%250Awe%2520focus%2520on%2520controlling%2520and%2520exploring%2520diversity%2520in%2520the%2520context%2520of%2520open-domain%250Amulti-agent%2520conversations%252C%2520particularly%2520for%2520world%2520simulation%2520applications.%2520We%250Apropose%2520Adaptive%2520Prompt%2520Pruning%2520%2528APP%2529%252C%2520a%2520novel%2520method%2520that%2520dynamically%2520adjusts%250Athe%2520content%2520of%2520the%2520utterance%2520generation%2520prompt%2520to%2520control%2520diversity%2520using%2520a%250Asingle%2520parameter%252C%2520lambda.%2520Through%2520extensive%2520experiments%252C%2520we%2520show%2520that%2520APP%250Aeffectively%2520controls%2520the%2520output%2520diversity%2520across%2520models%2520and%2520datasets%252C%2520with%250Apruning%2520more%2520information%2520leading%2520to%2520more%2520diverse%2520output.%2520We%2520comprehensively%250Aanalyze%2520the%2520relationship%2520between%2520prompt%2520content%2520and%2520conversational%2520diversity.%250AOur%2520findings%2520reveal%2520that%2520information%2520from%2520all%2520components%2520of%2520the%2520prompt%250Agenerally%2520constrains%2520the%2520diversity%2520of%2520the%2520output%252C%2520with%2520the%2520Memory%2520block%250Aexerting%2520the%2520most%2520significant%2520influence.%2520APP%2520is%2520compatible%2520with%2520established%250Atechniques%2520like%2520temperature%2520sampling%2520and%2520top-p%2520sampling%252C%2520providing%2520a%2520versatile%250Atool%2520for%2520diversity%2520management.%2520To%2520address%2520the%2520trade-offs%2520of%2520increased%250Adiversity%252C%2520such%2520as%2520inconsistencies%2520with%2520omitted%2520information%252C%2520we%2520incorporate%2520a%250Apost-generation%2520correction%2520step%252C%2520which%2520effectively%2520balances%2520diversity%250Aenhancement%2520with%2520output%2520consistency.%2520Additionally%252C%2520we%2520examine%2520how%2520prompt%250Astructure%252C%2520including%2520component%2520order%2520and%2520length%252C%2520impacts%2520diversity.%2520This%2520study%250Aaddresses%2520key%2520questions%2520surrounding%2520diversity%2520in%2520multi-agent%2520world%2520simulation%252C%250Aoffering%2520insights%2520into%2520its%2520control%252C%2520influencing%2520factors%252C%2520and%2520associated%250Atrade-offs.%2520Our%2520contributions%2520lay%2520the%2520foundation%2520for%2520systematically%2520engineering%250Adiversity%2520in%2520LLM-based%2520multi-agent%2520collaborations%252C%2520advancing%2520their%250Aeffectiveness%2520in%2520real-world%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.21102v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20and%20Controlling%20Diversity%20in%20LLM-Agent%20Conversation&entry.906535625=KuanChao%20Chu%20and%20Yi-Pei%20Chen%20and%20Hideki%20Nakayama&entry.1292438233=%20%20Diversity%20is%20a%20critical%20aspect%20of%20multi-agent%20communication.%20In%20this%20paper%2C%0Awe%20focus%20on%20controlling%20and%20exploring%20diversity%20in%20the%20context%20of%20open-domain%0Amulti-agent%20conversations%2C%20particularly%20for%20world%20simulation%20applications.%20We%0Apropose%20Adaptive%20Prompt%20Pruning%20%28APP%29%2C%20a%20novel%20method%20that%20dynamically%20adjusts%0Athe%20content%20of%20the%20utterance%20generation%20prompt%20to%20control%20diversity%20using%20a%0Asingle%20parameter%2C%20lambda.%20Through%20extensive%20experiments%2C%20we%20show%20that%20APP%0Aeffectively%20controls%20the%20output%20diversity%20across%20models%20and%20datasets%2C%20with%0Apruning%20more%20information%20leading%20to%20more%20diverse%20output.%20We%20comprehensively%0Aanalyze%20the%20relationship%20between%20prompt%20content%20and%20conversational%20diversity.%0AOur%20findings%20reveal%20that%20information%20from%20all%20components%20of%20the%20prompt%0Agenerally%20constrains%20the%20diversity%20of%20the%20output%2C%20with%20the%20Memory%20block%0Aexerting%20the%20most%20significant%20influence.%20APP%20is%20compatible%20with%20established%0Atechniques%20like%20temperature%20sampling%20and%20top-p%20sampling%2C%20providing%20a%20versatile%0Atool%20for%20diversity%20management.%20To%20address%20the%20trade-offs%20of%20increased%0Adiversity%2C%20such%20as%20inconsistencies%20with%20omitted%20information%2C%20we%20incorporate%20a%0Apost-generation%20correction%20step%2C%20which%20effectively%20balances%20diversity%0Aenhancement%20with%20output%20consistency.%20Additionally%2C%20we%20examine%20how%20prompt%0Astructure%2C%20including%20component%20order%20and%20length%2C%20impacts%20diversity.%20This%20study%0Aaddresses%20key%20questions%20surrounding%20diversity%20in%20multi-agent%20world%20simulation%2C%0Aoffering%20insights%20into%20its%20control%2C%20influencing%20factors%2C%20and%20associated%0Atrade-offs.%20Our%20contributions%20lay%20the%20foundation%20for%20systematically%20engineering%0Adiversity%20in%20LLM-based%20multi-agent%20collaborations%2C%20advancing%20their%0Aeffectiveness%20in%20real-world%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.21102v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


