<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240429.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "N$^{3}$-Mapping: Normal Guided Neural Non-Projective Signed Distance\n  Fields for Large-scale 3D Mapping", "author": "Shuangfu Song and Junqiao Zhao and Kai Huang and Jiaye Lin and Chen Ye and Tiantian Feng", "abstract": "  Accurate and dense mapping in large-scale environments is essential for\nvarious robot applications. Recently, implicit neural signed distance fields\n(SDFs) have shown promising advances in this task. However, most existing\napproaches employ projective distances from range data as SDF supervision,\nintroducing approximation errors and thus degrading the mapping quality. To\naddress this problem, we introduce N$^{3}$-Mapping, an implicit neural mapping\nsystem featuring normal-guided neural non-projective signed distance fields.\nSpecifically, we directly sample points along the surface normal, instead of\nthe ray, to obtain more accurate non-projective distance values from range\ndata. Then these distance values are used as supervision to train the implicit\nmap. For large-scale mapping, we apply a voxel-oriented sliding window\nmechanism to alleviate the forgetting issue with a bounded memory footprint.\nBesides, considering the uneven distribution of measured point clouds, a\nhierarchical sampling strategy is designed to improve training efficiency.\nExperiments demonstrate that our method effectively mitigates SDF approximation\nerrors and achieves state-of-the-art mapping quality compared to existing\napproaches.\n", "link": "http://arxiv.org/abs/2401.03412v2", "date": "2024-04-29", "relevancy": 2.8351, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5984}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5655}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5372}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20N%24%5E%7B3%7D%24-Mapping%3A%20Normal%20Guided%20Neural%20Non-Projective%20Signed%20Distance%0A%20%20Fields%20for%20Large-scale%203D%20Mapping&body=Title%3A%20N%24%5E%7B3%7D%24-Mapping%3A%20Normal%20Guided%20Neural%20Non-Projective%20Signed%20Distance%0A%20%20Fields%20for%20Large-scale%203D%20Mapping%0AAuthor%3A%20Shuangfu%20Song%20and%20Junqiao%20Zhao%20and%20Kai%20Huang%20and%20Jiaye%20Lin%20and%20Chen%20Ye%20and%20Tiantian%20Feng%0AAbstract%3A%20%20%20Accurate%20and%20dense%20mapping%20in%20large-scale%20environments%20is%20essential%20for%0Avarious%20robot%20applications.%20Recently%2C%20implicit%20neural%20signed%20distance%20fields%0A%28SDFs%29%20have%20shown%20promising%20advances%20in%20this%20task.%20However%2C%20most%20existing%0Aapproaches%20employ%20projective%20distances%20from%20range%20data%20as%20SDF%20supervision%2C%0Aintroducing%20approximation%20errors%20and%20thus%20degrading%20the%20mapping%20quality.%20To%0Aaddress%20this%20problem%2C%20we%20introduce%20N%24%5E%7B3%7D%24-Mapping%2C%20an%20implicit%20neural%20mapping%0Asystem%20featuring%20normal-guided%20neural%20non-projective%20signed%20distance%20fields.%0ASpecifically%2C%20we%20directly%20sample%20points%20along%20the%20surface%20normal%2C%20instead%20of%0Athe%20ray%2C%20to%20obtain%20more%20accurate%20non-projective%20distance%20values%20from%20range%0Adata.%20Then%20these%20distance%20values%20are%20used%20as%20supervision%20to%20train%20the%20implicit%0Amap.%20For%20large-scale%20mapping%2C%20we%20apply%20a%20voxel-oriented%20sliding%20window%0Amechanism%20to%20alleviate%20the%20forgetting%20issue%20with%20a%20bounded%20memory%20footprint.%0ABesides%2C%20considering%20the%20uneven%20distribution%20of%20measured%20point%20clouds%2C%20a%0Ahierarchical%20sampling%20strategy%20is%20designed%20to%20improve%20training%20efficiency.%0AExperiments%20demonstrate%20that%20our%20method%20effectively%20mitigates%20SDF%20approximation%0Aerrors%20and%20achieves%20state-of-the-art%20mapping%20quality%20compared%20to%20existing%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.03412v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=N%24%5E%7B3%7D%24-Mapping%3A%20Normal%20Guided%20Neural%20Non-Projective%20Signed%20Distance%0A%20%20Fields%20for%20Large-scale%203D%20Mapping&entry.906535625=Shuangfu%20Song%20and%20Junqiao%20Zhao%20and%20Kai%20Huang%20and%20Jiaye%20Lin%20and%20Chen%20Ye%20and%20Tiantian%20Feng&entry.1292438233=%20%20Accurate%20and%20dense%20mapping%20in%20large-scale%20environments%20is%20essential%20for%0Avarious%20robot%20applications.%20Recently%2C%20implicit%20neural%20signed%20distance%20fields%0A%28SDFs%29%20have%20shown%20promising%20advances%20in%20this%20task.%20However%2C%20most%20existing%0Aapproaches%20employ%20projective%20distances%20from%20range%20data%20as%20SDF%20supervision%2C%0Aintroducing%20approximation%20errors%20and%20thus%20degrading%20the%20mapping%20quality.%20To%0Aaddress%20this%20problem%2C%20we%20introduce%20N%24%5E%7B3%7D%24-Mapping%2C%20an%20implicit%20neural%20mapping%0Asystem%20featuring%20normal-guided%20neural%20non-projective%20signed%20distance%20fields.%0ASpecifically%2C%20we%20directly%20sample%20points%20along%20the%20surface%20normal%2C%20instead%20of%0Athe%20ray%2C%20to%20obtain%20more%20accurate%20non-projective%20distance%20values%20from%20range%0Adata.%20Then%20these%20distance%20values%20are%20used%20as%20supervision%20to%20train%20the%20implicit%0Amap.%20For%20large-scale%20mapping%2C%20we%20apply%20a%20voxel-oriented%20sliding%20window%0Amechanism%20to%20alleviate%20the%20forgetting%20issue%20with%20a%20bounded%20memory%20footprint.%0ABesides%2C%20considering%20the%20uneven%20distribution%20of%20measured%20point%20clouds%2C%20a%0Ahierarchical%20sampling%20strategy%20is%20designed%20to%20improve%20training%20efficiency.%0AExperiments%20demonstrate%20that%20our%20method%20effectively%20mitigates%20SDF%20approximation%0Aerrors%20and%20achieves%20state-of-the-art%20mapping%20quality%20compared%20to%20existing%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.03412v2&entry.124074799=Read"},
{"title": "DGE: Direct Gaussian 3D Editing by Consistent Multi-view Editing", "author": "Minghao Chen and Iro Laina and Andrea Vedaldi", "abstract": "  We consider the problem of editing 3D objects and scenes based on open-ended\nlanguage instructions. The established paradigm to solve this problem is to use\na 2D image generator or editor to guide the 3D editing process. However, this\nis often slow as it requires do update a computationally expensive 3D\nrepresentations such as a neural radiance field, and to do so by using\ncontradictory guidance from a 2D model which is inherently not multi-view\nconsistent. We thus introduce the Direct Gaussian Editor (DGE), a method that\naddresses these issues in two ways. First, we modify a given high-quality image\neditor like InstructPix2Pix to be multi-view consistent. We do so by utilizing\na training-free approach which integrates cues from the underlying 3D geometry\nof the scene. Second, given a multi-view consistent edited sequence of images\nof the object, we directly and efficiently optimize the 3D object\nrepresentation, which is based on 3D Gaussian Splatting. Because it does not\nrequire to apply edits incrementally and iteratively, DGE is significantly more\nefficient than existing approaches, and comes with other perks such as allowing\nselective editing of parts of the scene.\n", "link": "http://arxiv.org/abs/2404.18929v1", "date": "2024-04-29", "relevancy": 2.8292, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6426}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5397}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5153}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DGE%3A%20Direct%20Gaussian%203D%20Editing%20by%20Consistent%20Multi-view%20Editing&body=Title%3A%20DGE%3A%20Direct%20Gaussian%203D%20Editing%20by%20Consistent%20Multi-view%20Editing%0AAuthor%3A%20Minghao%20Chen%20and%20Iro%20Laina%20and%20Andrea%20Vedaldi%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20editing%203D%20objects%20and%20scenes%20based%20on%20open-ended%0Alanguage%20instructions.%20The%20established%20paradigm%20to%20solve%20this%20problem%20is%20to%20use%0Aa%202D%20image%20generator%20or%20editor%20to%20guide%20the%203D%20editing%20process.%20However%2C%20this%0Ais%20often%20slow%20as%20it%20requires%20do%20update%20a%20computationally%20expensive%203D%0Arepresentations%20such%20as%20a%20neural%20radiance%20field%2C%20and%20to%20do%20so%20by%20using%0Acontradictory%20guidance%20from%20a%202D%20model%20which%20is%20inherently%20not%20multi-view%0Aconsistent.%20We%20thus%20introduce%20the%20Direct%20Gaussian%20Editor%20%28DGE%29%2C%20a%20method%20that%0Aaddresses%20these%20issues%20in%20two%20ways.%20First%2C%20we%20modify%20a%20given%20high-quality%20image%0Aeditor%20like%20InstructPix2Pix%20to%20be%20multi-view%20consistent.%20We%20do%20so%20by%20utilizing%0Aa%20training-free%20approach%20which%20integrates%20cues%20from%20the%20underlying%203D%20geometry%0Aof%20the%20scene.%20Second%2C%20given%20a%20multi-view%20consistent%20edited%20sequence%20of%20images%0Aof%20the%20object%2C%20we%20directly%20and%20efficiently%20optimize%20the%203D%20object%0Arepresentation%2C%20which%20is%20based%20on%203D%20Gaussian%20Splatting.%20Because%20it%20does%20not%0Arequire%20to%20apply%20edits%20incrementally%20and%20iteratively%2C%20DGE%20is%20significantly%20more%0Aefficient%20than%20existing%20approaches%2C%20and%20comes%20with%20other%20perks%20such%20as%20allowing%0Aselective%20editing%20of%20parts%20of%20the%20scene.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18929v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DGE%3A%20Direct%20Gaussian%203D%20Editing%20by%20Consistent%20Multi-view%20Editing&entry.906535625=Minghao%20Chen%20and%20Iro%20Laina%20and%20Andrea%20Vedaldi&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20editing%203D%20objects%20and%20scenes%20based%20on%20open-ended%0Alanguage%20instructions.%20The%20established%20paradigm%20to%20solve%20this%20problem%20is%20to%20use%0Aa%202D%20image%20generator%20or%20editor%20to%20guide%20the%203D%20editing%20process.%20However%2C%20this%0Ais%20often%20slow%20as%20it%20requires%20do%20update%20a%20computationally%20expensive%203D%0Arepresentations%20such%20as%20a%20neural%20radiance%20field%2C%20and%20to%20do%20so%20by%20using%0Acontradictory%20guidance%20from%20a%202D%20model%20which%20is%20inherently%20not%20multi-view%0Aconsistent.%20We%20thus%20introduce%20the%20Direct%20Gaussian%20Editor%20%28DGE%29%2C%20a%20method%20that%0Aaddresses%20these%20issues%20in%20two%20ways.%20First%2C%20we%20modify%20a%20given%20high-quality%20image%0Aeditor%20like%20InstructPix2Pix%20to%20be%20multi-view%20consistent.%20We%20do%20so%20by%20utilizing%0Aa%20training-free%20approach%20which%20integrates%20cues%20from%20the%20underlying%203D%20geometry%0Aof%20the%20scene.%20Second%2C%20given%20a%20multi-view%20consistent%20edited%20sequence%20of%20images%0Aof%20the%20object%2C%20we%20directly%20and%20efficiently%20optimize%20the%203D%20object%0Arepresentation%2C%20which%20is%20based%20on%203D%20Gaussian%20Splatting.%20Because%20it%20does%20not%0Arequire%20to%20apply%20edits%20incrementally%20and%20iteratively%2C%20DGE%20is%20significantly%20more%0Aefficient%20than%20existing%20approaches%2C%20and%20comes%20with%20other%20perks%20such%20as%20allowing%0Aselective%20editing%20of%20parts%20of%20the%20scene.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18929v1&entry.124074799=Read"},
{"title": "Raising the Bar of AI-generated Image Detection with CLIP", "author": "Davide Cozzolino and Giovanni Poggi and Riccardo Corvi and Matthias Nie\u00dfner and Luisa Verdoliva", "abstract": "  The aim of this work is to explore the potential of pre-trained\nvision-language models (VLMs) for universal detection of AI-generated images.\nWe develop a lightweight detection strategy based on CLIP features and study\nits performance in a wide variety of challenging scenarios. We find that,\ncontrary to previous beliefs, it is neither necessary nor convenient to use a\nlarge domain-specific dataset for training. On the contrary, by using only a\nhandful of example images from a single generative model, a CLIP-based detector\nexhibits surprising generalization ability and high robustness across different\narchitectures, including recent commercial tools such as Dalle-3, Midjourney\nv5, and Firefly. We match the state-of-the-art (SoTA) on in-distribution data\nand significantly improve upon it in terms of generalization to\nout-of-distribution data (+6% AUC) and robustness to impaired/laundered data\n(+13%). Our project is available at\nhttps://grip-unina.github.io/ClipBased-SyntheticImageDetection/\n", "link": "http://arxiv.org/abs/2312.00195v2", "date": "2024-04-29", "relevancy": 2.7752, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5825}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5451}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5374}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Raising%20the%20Bar%20of%20AI-generated%20Image%20Detection%20with%20CLIP&body=Title%3A%20Raising%20the%20Bar%20of%20AI-generated%20Image%20Detection%20with%20CLIP%0AAuthor%3A%20Davide%20Cozzolino%20and%20Giovanni%20Poggi%20and%20Riccardo%20Corvi%20and%20Matthias%20Nie%C3%9Fner%20and%20Luisa%20Verdoliva%0AAbstract%3A%20%20%20The%20aim%20of%20this%20work%20is%20to%20explore%20the%20potential%20of%20pre-trained%0Avision-language%20models%20%28VLMs%29%20for%20universal%20detection%20of%20AI-generated%20images.%0AWe%20develop%20a%20lightweight%20detection%20strategy%20based%20on%20CLIP%20features%20and%20study%0Aits%20performance%20in%20a%20wide%20variety%20of%20challenging%20scenarios.%20We%20find%20that%2C%0Acontrary%20to%20previous%20beliefs%2C%20it%20is%20neither%20necessary%20nor%20convenient%20to%20use%20a%0Alarge%20domain-specific%20dataset%20for%20training.%20On%20the%20contrary%2C%20by%20using%20only%20a%0Ahandful%20of%20example%20images%20from%20a%20single%20generative%20model%2C%20a%20CLIP-based%20detector%0Aexhibits%20surprising%20generalization%20ability%20and%20high%20robustness%20across%20different%0Aarchitectures%2C%20including%20recent%20commercial%20tools%20such%20as%20Dalle-3%2C%20Midjourney%0Av5%2C%20and%20Firefly.%20We%20match%20the%20state-of-the-art%20%28SoTA%29%20on%20in-distribution%20data%0Aand%20significantly%20improve%20upon%20it%20in%20terms%20of%20generalization%20to%0Aout-of-distribution%20data%20%28%2B6%25%20AUC%29%20and%20robustness%20to%20impaired/laundered%20data%0A%28%2B13%25%29.%20Our%20project%20is%20available%20at%0Ahttps%3A//grip-unina.github.io/ClipBased-SyntheticImageDetection/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00195v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Raising%20the%20Bar%20of%20AI-generated%20Image%20Detection%20with%20CLIP&entry.906535625=Davide%20Cozzolino%20and%20Giovanni%20Poggi%20and%20Riccardo%20Corvi%20and%20Matthias%20Nie%C3%9Fner%20and%20Luisa%20Verdoliva&entry.1292438233=%20%20The%20aim%20of%20this%20work%20is%20to%20explore%20the%20potential%20of%20pre-trained%0Avision-language%20models%20%28VLMs%29%20for%20universal%20detection%20of%20AI-generated%20images.%0AWe%20develop%20a%20lightweight%20detection%20strategy%20based%20on%20CLIP%20features%20and%20study%0Aits%20performance%20in%20a%20wide%20variety%20of%20challenging%20scenarios.%20We%20find%20that%2C%0Acontrary%20to%20previous%20beliefs%2C%20it%20is%20neither%20necessary%20nor%20convenient%20to%20use%20a%0Alarge%20domain-specific%20dataset%20for%20training.%20On%20the%20contrary%2C%20by%20using%20only%20a%0Ahandful%20of%20example%20images%20from%20a%20single%20generative%20model%2C%20a%20CLIP-based%20detector%0Aexhibits%20surprising%20generalization%20ability%20and%20high%20robustness%20across%20different%0Aarchitectures%2C%20including%20recent%20commercial%20tools%20such%20as%20Dalle-3%2C%20Midjourney%0Av5%2C%20and%20Firefly.%20We%20match%20the%20state-of-the-art%20%28SoTA%29%20on%20in-distribution%20data%0Aand%20significantly%20improve%20upon%20it%20in%20terms%20of%20generalization%20to%0Aout-of-distribution%20data%20%28%2B6%25%20AUC%29%20and%20robustness%20to%20impaired/laundered%20data%0A%28%2B13%25%29.%20Our%20project%20is%20available%20at%0Ahttps%3A//grip-unina.github.io/ClipBased-SyntheticImageDetection/%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00195v2&entry.124074799=Read"},
{"title": "Non-convex Pose Graph Optimization in SLAM via Proximal Linearized\n  Riemannian ADMM", "author": "Xin Chen and Chunfeng Cui and Deren Han and Liqun Qi", "abstract": "  Pose graph optimization (PGO) is a well-known technique for solving the\npose-based simultaneous localization and mapping (SLAM) problem. In this paper,\nwe represent the rotation and translation by a unit quaternion and a\nthree-dimensional vector, and propose a new PGO model based on the von\nMises-Fisher distribution. The constraints derived from the unit quaternions\nare spherical manifolds, and the projection onto the constraints can be\ncalculated by normalization. Then a proximal linearized Riemannian alternating\ndirection method of multipliers (PieADMM) is developed to solve the proposed\nmodel, which not only has low memory requirements, but also can update the\nposes in parallel. Furthermore, we establish the iteration complexity of\n$O(1/\\epsilon^{2})$ of PieADMM for finding an $\\epsilon$-stationary solution of\nour model. The efficiency of our proposed algorithm is demonstrated by\nnumerical experiments on two synthetic and four 3D SLAM benchmark datasets.\n", "link": "http://arxiv.org/abs/2404.18560v1", "date": "2024-04-29", "relevancy": 2.7362, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5608}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5446}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5363}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Non-convex%20Pose%20Graph%20Optimization%20in%20SLAM%20via%20Proximal%20Linearized%0A%20%20Riemannian%20ADMM&body=Title%3A%20Non-convex%20Pose%20Graph%20Optimization%20in%20SLAM%20via%20Proximal%20Linearized%0A%20%20Riemannian%20ADMM%0AAuthor%3A%20Xin%20Chen%20and%20Chunfeng%20Cui%20and%20Deren%20Han%20and%20Liqun%20Qi%0AAbstract%3A%20%20%20Pose%20graph%20optimization%20%28PGO%29%20is%20a%20well-known%20technique%20for%20solving%20the%0Apose-based%20simultaneous%20localization%20and%20mapping%20%28SLAM%29%20problem.%20In%20this%20paper%2C%0Awe%20represent%20the%20rotation%20and%20translation%20by%20a%20unit%20quaternion%20and%20a%0Athree-dimensional%20vector%2C%20and%20propose%20a%20new%20PGO%20model%20based%20on%20the%20von%0AMises-Fisher%20distribution.%20The%20constraints%20derived%20from%20the%20unit%20quaternions%0Aare%20spherical%20manifolds%2C%20and%20the%20projection%20onto%20the%20constraints%20can%20be%0Acalculated%20by%20normalization.%20Then%20a%20proximal%20linearized%20Riemannian%20alternating%0Adirection%20method%20of%20multipliers%20%28PieADMM%29%20is%20developed%20to%20solve%20the%20proposed%0Amodel%2C%20which%20not%20only%20has%20low%20memory%20requirements%2C%20but%20also%20can%20update%20the%0Aposes%20in%20parallel.%20Furthermore%2C%20we%20establish%20the%20iteration%20complexity%20of%0A%24O%281/%5Cepsilon%5E%7B2%7D%29%24%20of%20PieADMM%20for%20finding%20an%20%24%5Cepsilon%24-stationary%20solution%20of%0Aour%20model.%20The%20efficiency%20of%20our%20proposed%20algorithm%20is%20demonstrated%20by%0Anumerical%20experiments%20on%20two%20synthetic%20and%20four%203D%20SLAM%20benchmark%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18560v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-convex%20Pose%20Graph%20Optimization%20in%20SLAM%20via%20Proximal%20Linearized%0A%20%20Riemannian%20ADMM&entry.906535625=Xin%20Chen%20and%20Chunfeng%20Cui%20and%20Deren%20Han%20and%20Liqun%20Qi&entry.1292438233=%20%20Pose%20graph%20optimization%20%28PGO%29%20is%20a%20well-known%20technique%20for%20solving%20the%0Apose-based%20simultaneous%20localization%20and%20mapping%20%28SLAM%29%20problem.%20In%20this%20paper%2C%0Awe%20represent%20the%20rotation%20and%20translation%20by%20a%20unit%20quaternion%20and%20a%0Athree-dimensional%20vector%2C%20and%20propose%20a%20new%20PGO%20model%20based%20on%20the%20von%0AMises-Fisher%20distribution.%20The%20constraints%20derived%20from%20the%20unit%20quaternions%0Aare%20spherical%20manifolds%2C%20and%20the%20projection%20onto%20the%20constraints%20can%20be%0Acalculated%20by%20normalization.%20Then%20a%20proximal%20linearized%20Riemannian%20alternating%0Adirection%20method%20of%20multipliers%20%28PieADMM%29%20is%20developed%20to%20solve%20the%20proposed%0Amodel%2C%20which%20not%20only%20has%20low%20memory%20requirements%2C%20but%20also%20can%20update%20the%0Aposes%20in%20parallel.%20Furthermore%2C%20we%20establish%20the%20iteration%20complexity%20of%0A%24O%281/%5Cepsilon%5E%7B2%7D%29%24%20of%20PieADMM%20for%20finding%20an%20%24%5Cepsilon%24-stationary%20solution%20of%0Aour%20model.%20The%20efficiency%20of%20our%20proposed%20algorithm%20is%20demonstrated%20by%0Anumerical%20experiments%20on%20two%20synthetic%20and%20four%203D%20SLAM%20benchmark%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18560v1&entry.124074799=Read"},
{"title": "OpenStreetView-5M: The Many Roads to Global Visual Geolocation", "author": "Guillaume Astruc and Nicolas Dufour and Ioannis Siglidis and Constantin Aronssohn and Nacim Bouia and Stephanie Fu and Romain Loiseau and Van Nguyen Nguyen and Charles Raude and Elliot Vincent and Lintao XU and Hongyu Zhou and Loic Landrieu", "abstract": "  Determining the location of an image anywhere on Earth is a complex visual\ntask, which makes it particularly relevant for evaluating computer vision\nalgorithms. Yet, the absence of standard, large-scale, open-access datasets\nwith reliably localizable images has limited its potential. To address this\nissue, we introduce OpenStreetView-5M, a large-scale, open-access dataset\ncomprising over 5.1 million geo-referenced street view images, covering 225\ncountries and territories. In contrast to existing benchmarks, we enforce a\nstrict train/test separation, allowing us to evaluate the relevance of learned\ngeographical features beyond mere memorization. To demonstrate the utility of\nour dataset, we conduct an extensive benchmark of various state-of-the-art\nimage encoders, spatial representations, and training strategies. All\nassociated codes and models can be found at https://github.com/gastruc/osv5m.\n", "link": "http://arxiv.org/abs/2404.18873v1", "date": "2024-04-29", "relevancy": 2.6267, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5665}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5062}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5033}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20OpenStreetView-5M%3A%20The%20Many%20Roads%20to%20Global%20Visual%20Geolocation&body=Title%3A%20OpenStreetView-5M%3A%20The%20Many%20Roads%20to%20Global%20Visual%20Geolocation%0AAuthor%3A%20Guillaume%20Astruc%20and%20Nicolas%20Dufour%20and%20Ioannis%20Siglidis%20and%20Constantin%20Aronssohn%20and%20Nacim%20Bouia%20and%20Stephanie%20Fu%20and%20Romain%20Loiseau%20and%20Van%20Nguyen%20Nguyen%20and%20Charles%20Raude%20and%20Elliot%20Vincent%20and%20Lintao%20XU%20and%20Hongyu%20Zhou%20and%20Loic%20Landrieu%0AAbstract%3A%20%20%20Determining%20the%20location%20of%20an%20image%20anywhere%20on%20Earth%20is%20a%20complex%20visual%0Atask%2C%20which%20makes%20it%20particularly%20relevant%20for%20evaluating%20computer%20vision%0Aalgorithms.%20Yet%2C%20the%20absence%20of%20standard%2C%20large-scale%2C%20open-access%20datasets%0Awith%20reliably%20localizable%20images%20has%20limited%20its%20potential.%20To%20address%20this%0Aissue%2C%20we%20introduce%20OpenStreetView-5M%2C%20a%20large-scale%2C%20open-access%20dataset%0Acomprising%20over%205.1%20million%20geo-referenced%20street%20view%20images%2C%20covering%20225%0Acountries%20and%20territories.%20In%20contrast%20to%20existing%20benchmarks%2C%20we%20enforce%20a%0Astrict%20train/test%20separation%2C%20allowing%20us%20to%20evaluate%20the%20relevance%20of%20learned%0Ageographical%20features%20beyond%20mere%20memorization.%20To%20demonstrate%20the%20utility%20of%0Aour%20dataset%2C%20we%20conduct%20an%20extensive%20benchmark%20of%20various%20state-of-the-art%0Aimage%20encoders%2C%20spatial%20representations%2C%20and%20training%20strategies.%20All%0Aassociated%20codes%20and%20models%20can%20be%20found%20at%20https%3A//github.com/gastruc/osv5m.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18873v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenStreetView-5M%3A%20The%20Many%20Roads%20to%20Global%20Visual%20Geolocation&entry.906535625=Guillaume%20Astruc%20and%20Nicolas%20Dufour%20and%20Ioannis%20Siglidis%20and%20Constantin%20Aronssohn%20and%20Nacim%20Bouia%20and%20Stephanie%20Fu%20and%20Romain%20Loiseau%20and%20Van%20Nguyen%20Nguyen%20and%20Charles%20Raude%20and%20Elliot%20Vincent%20and%20Lintao%20XU%20and%20Hongyu%20Zhou%20and%20Loic%20Landrieu&entry.1292438233=%20%20Determining%20the%20location%20of%20an%20image%20anywhere%20on%20Earth%20is%20a%20complex%20visual%0Atask%2C%20which%20makes%20it%20particularly%20relevant%20for%20evaluating%20computer%20vision%0Aalgorithms.%20Yet%2C%20the%20absence%20of%20standard%2C%20large-scale%2C%20open-access%20datasets%0Awith%20reliably%20localizable%20images%20has%20limited%20its%20potential.%20To%20address%20this%0Aissue%2C%20we%20introduce%20OpenStreetView-5M%2C%20a%20large-scale%2C%20open-access%20dataset%0Acomprising%20over%205.1%20million%20geo-referenced%20street%20view%20images%2C%20covering%20225%0Acountries%20and%20territories.%20In%20contrast%20to%20existing%20benchmarks%2C%20we%20enforce%20a%0Astrict%20train/test%20separation%2C%20allowing%20us%20to%20evaluate%20the%20relevance%20of%20learned%0Ageographical%20features%20beyond%20mere%20memorization.%20To%20demonstrate%20the%20utility%20of%0Aour%20dataset%2C%20we%20conduct%20an%20extensive%20benchmark%20of%20various%20state-of-the-art%0Aimage%20encoders%2C%20spatial%20representations%2C%20and%20training%20strategies.%20All%0Aassociated%20codes%20and%20models%20can%20be%20found%20at%20https%3A//github.com/gastruc/osv5m.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18873v1&entry.124074799=Read"},
{"title": "IPixMatch: Boost Semi-supervised Semantic Segmentation with Inter-Pixel\n  Relation", "author": "Kebin Wu and Wenbin Li and Xiaofei Xiao", "abstract": "  The scarcity of labeled data in real-world scenarios is a critical bottleneck\nof deep learning's effectiveness. Semi-supervised semantic segmentation has\nbeen a typical solution to achieve a desirable tradeoff between annotation cost\nand segmentation performance. However, previous approaches, whether based on\nconsistency regularization or self-training, tend to neglect the contextual\nknowledge embedded within inter-pixel relations. This negligence leads to\nsuboptimal performance and limited generalization. In this paper, we propose a\nnovel approach IPixMatch designed to mine the neglected but valuable\nInter-Pixel information for semi-supervised learning. Specifically, IPixMatch\nis constructed as an extension of the standard teacher-student network,\nincorporating additional loss terms to capture inter-pixel relations. It shines\nin low-data regimes by efficiently leveraging the limited labeled data and\nextracting maximum utility from the available unlabeled data. Furthermore,\nIPixMatch can be integrated seamlessly into most teacher-student frameworks\nwithout the need of model modification or adding additional components. Our\nstraightforward IPixMatch method demonstrates consistent performance\nimprovements across various benchmark datasets under different partitioning\nprotocols.\n", "link": "http://arxiv.org/abs/2404.18891v1", "date": "2024-04-29", "relevancy": 2.6258, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5462}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.532}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4972}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20IPixMatch%3A%20Boost%20Semi-supervised%20Semantic%20Segmentation%20with%20Inter-Pixel%0A%20%20Relation&body=Title%3A%20IPixMatch%3A%20Boost%20Semi-supervised%20Semantic%20Segmentation%20with%20Inter-Pixel%0A%20%20Relation%0AAuthor%3A%20Kebin%20Wu%20and%20Wenbin%20Li%20and%20Xiaofei%20Xiao%0AAbstract%3A%20%20%20The%20scarcity%20of%20labeled%20data%20in%20real-world%20scenarios%20is%20a%20critical%20bottleneck%0Aof%20deep%20learning%27s%20effectiveness.%20Semi-supervised%20semantic%20segmentation%20has%0Abeen%20a%20typical%20solution%20to%20achieve%20a%20desirable%20tradeoff%20between%20annotation%20cost%0Aand%20segmentation%20performance.%20However%2C%20previous%20approaches%2C%20whether%20based%20on%0Aconsistency%20regularization%20or%20self-training%2C%20tend%20to%20neglect%20the%20contextual%0Aknowledge%20embedded%20within%20inter-pixel%20relations.%20This%20negligence%20leads%20to%0Asuboptimal%20performance%20and%20limited%20generalization.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20approach%20IPixMatch%20designed%20to%20mine%20the%20neglected%20but%20valuable%0AInter-Pixel%20information%20for%20semi-supervised%20learning.%20Specifically%2C%20IPixMatch%0Ais%20constructed%20as%20an%20extension%20of%20the%20standard%20teacher-student%20network%2C%0Aincorporating%20additional%20loss%20terms%20to%20capture%20inter-pixel%20relations.%20It%20shines%0Ain%20low-data%20regimes%20by%20efficiently%20leveraging%20the%20limited%20labeled%20data%20and%0Aextracting%20maximum%20utility%20from%20the%20available%20unlabeled%20data.%20Furthermore%2C%0AIPixMatch%20can%20be%20integrated%20seamlessly%20into%20most%20teacher-student%20frameworks%0Awithout%20the%20need%20of%20model%20modification%20or%20adding%20additional%20components.%20Our%0Astraightforward%20IPixMatch%20method%20demonstrates%20consistent%20performance%0Aimprovements%20across%20various%20benchmark%20datasets%20under%20different%20partitioning%0Aprotocols.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18891v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IPixMatch%3A%20Boost%20Semi-supervised%20Semantic%20Segmentation%20with%20Inter-Pixel%0A%20%20Relation&entry.906535625=Kebin%20Wu%20and%20Wenbin%20Li%20and%20Xiaofei%20Xiao&entry.1292438233=%20%20The%20scarcity%20of%20labeled%20data%20in%20real-world%20scenarios%20is%20a%20critical%20bottleneck%0Aof%20deep%20learning%27s%20effectiveness.%20Semi-supervised%20semantic%20segmentation%20has%0Abeen%20a%20typical%20solution%20to%20achieve%20a%20desirable%20tradeoff%20between%20annotation%20cost%0Aand%20segmentation%20performance.%20However%2C%20previous%20approaches%2C%20whether%20based%20on%0Aconsistency%20regularization%20or%20self-training%2C%20tend%20to%20neglect%20the%20contextual%0Aknowledge%20embedded%20within%20inter-pixel%20relations.%20This%20negligence%20leads%20to%0Asuboptimal%20performance%20and%20limited%20generalization.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20approach%20IPixMatch%20designed%20to%20mine%20the%20neglected%20but%20valuable%0AInter-Pixel%20information%20for%20semi-supervised%20learning.%20Specifically%2C%20IPixMatch%0Ais%20constructed%20as%20an%20extension%20of%20the%20standard%20teacher-student%20network%2C%0Aincorporating%20additional%20loss%20terms%20to%20capture%20inter-pixel%20relations.%20It%20shines%0Ain%20low-data%20regimes%20by%20efficiently%20leveraging%20the%20limited%20labeled%20data%20and%0Aextracting%20maximum%20utility%20from%20the%20available%20unlabeled%20data.%20Furthermore%2C%0AIPixMatch%20can%20be%20integrated%20seamlessly%20into%20most%20teacher-student%20frameworks%0Awithout%20the%20need%20of%20model%20modification%20or%20adding%20additional%20components.%20Our%0Astraightforward%20IPixMatch%20method%20demonstrates%20consistent%20performance%0Aimprovements%20across%20various%20benchmark%20datasets%20under%20different%20partitioning%0Aprotocols.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18891v1&entry.124074799=Read"},
{"title": "Transitive Vision-Language Prompt Learning for Domain Generalization", "author": "Liyuan Wang and Yan Jin and Zhen Chen and Jinlin Wu and Mengke Li and Yang Lu and Hanzi Wang", "abstract": "  The vision-language pre-training has enabled deep models to make a huge step\nforward in generalizing across unseen domains. The recent learning method based\non the vision-language pre-training model is a great tool for domain\ngeneralization and can solve this problem to a large extent. However, there are\nstill some issues that an advancement still suffers from trading-off between\ndomain invariance and class separability, which are crucial in current DG\nproblems. However, there are still some issues that an advancement still\nsuffers from trading-off between domain invariance and class separability,\nwhich are crucial in current DG problems. In this paper, we introduce a novel\nprompt learning strategy that leverages deep vision prompts to address domain\ninvariance while utilizing language prompts to ensure class separability,\ncoupled with adaptive weighting mechanisms to balance domain invariance and\nclass separability. Extensive experiments demonstrate that deep vision prompts\neffectively extract domain-invariant features, significantly improving the\ngeneralization ability of deep models and achieving state-of-the-art\nperformance on three datasets.\n", "link": "http://arxiv.org/abs/2404.18758v1", "date": "2024-04-29", "relevancy": 2.6104, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5429}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5198}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5036}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Transitive%20Vision-Language%20Prompt%20Learning%20for%20Domain%20Generalization&body=Title%3A%20Transitive%20Vision-Language%20Prompt%20Learning%20for%20Domain%20Generalization%0AAuthor%3A%20Liyuan%20Wang%20and%20Yan%20Jin%20and%20Zhen%20Chen%20and%20Jinlin%20Wu%20and%20Mengke%20Li%20and%20Yang%20Lu%20and%20Hanzi%20Wang%0AAbstract%3A%20%20%20The%20vision-language%20pre-training%20has%20enabled%20deep%20models%20to%20make%20a%20huge%20step%0Aforward%20in%20generalizing%20across%20unseen%20domains.%20The%20recent%20learning%20method%20based%0Aon%20the%20vision-language%20pre-training%20model%20is%20a%20great%20tool%20for%20domain%0Ageneralization%20and%20can%20solve%20this%20problem%20to%20a%20large%20extent.%20However%2C%20there%20are%0Astill%20some%20issues%20that%20an%20advancement%20still%20suffers%20from%20trading-off%20between%0Adomain%20invariance%20and%20class%20separability%2C%20which%20are%20crucial%20in%20current%20DG%0Aproblems.%20However%2C%20there%20are%20still%20some%20issues%20that%20an%20advancement%20still%0Asuffers%20from%20trading-off%20between%20domain%20invariance%20and%20class%20separability%2C%0Awhich%20are%20crucial%20in%20current%20DG%20problems.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0Aprompt%20learning%20strategy%20that%20leverages%20deep%20vision%20prompts%20to%20address%20domain%0Ainvariance%20while%20utilizing%20language%20prompts%20to%20ensure%20class%20separability%2C%0Acoupled%20with%20adaptive%20weighting%20mechanisms%20to%20balance%20domain%20invariance%20and%0Aclass%20separability.%20Extensive%20experiments%20demonstrate%20that%20deep%20vision%20prompts%0Aeffectively%20extract%20domain-invariant%20features%2C%20significantly%20improving%20the%0Ageneralization%20ability%20of%20deep%20models%20and%20achieving%20state-of-the-art%0Aperformance%20on%20three%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18758v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transitive%20Vision-Language%20Prompt%20Learning%20for%20Domain%20Generalization&entry.906535625=Liyuan%20Wang%20and%20Yan%20Jin%20and%20Zhen%20Chen%20and%20Jinlin%20Wu%20and%20Mengke%20Li%20and%20Yang%20Lu%20and%20Hanzi%20Wang&entry.1292438233=%20%20The%20vision-language%20pre-training%20has%20enabled%20deep%20models%20to%20make%20a%20huge%20step%0Aforward%20in%20generalizing%20across%20unseen%20domains.%20The%20recent%20learning%20method%20based%0Aon%20the%20vision-language%20pre-training%20model%20is%20a%20great%20tool%20for%20domain%0Ageneralization%20and%20can%20solve%20this%20problem%20to%20a%20large%20extent.%20However%2C%20there%20are%0Astill%20some%20issues%20that%20an%20advancement%20still%20suffers%20from%20trading-off%20between%0Adomain%20invariance%20and%20class%20separability%2C%20which%20are%20crucial%20in%20current%20DG%0Aproblems.%20However%2C%20there%20are%20still%20some%20issues%20that%20an%20advancement%20still%0Asuffers%20from%20trading-off%20between%20domain%20invariance%20and%20class%20separability%2C%0Awhich%20are%20crucial%20in%20current%20DG%20problems.%20In%20this%20paper%2C%20we%20introduce%20a%20novel%0Aprompt%20learning%20strategy%20that%20leverages%20deep%20vision%20prompts%20to%20address%20domain%0Ainvariance%20while%20utilizing%20language%20prompts%20to%20ensure%20class%20separability%2C%0Acoupled%20with%20adaptive%20weighting%20mechanisms%20to%20balance%20domain%20invariance%20and%0Aclass%20separability.%20Extensive%20experiments%20demonstrate%20that%20deep%20vision%20prompts%0Aeffectively%20extract%20domain-invariant%20features%2C%20significantly%20improving%20the%0Ageneralization%20ability%20of%20deep%20models%20and%20achieving%20state-of-the-art%0Aperformance%20on%20three%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18758v1&entry.124074799=Read"},
{"title": "SPGNN: Recognizing Salient Subgraph Patterns via Enhanced Graph\n  Convolution and Pooling", "author": "Zehao Dong and Muhan Zhang and Yixin Chen", "abstract": "  Graph neural networks (GNNs) have revolutionized the field of machine\nlearning on non-Euclidean data such as graphs and networks. GNNs effectively\nimplement node representation learning through neighborhood aggregation and\nachieve impressive results in many graph-related tasks. However, most\nneighborhood aggregation approaches are summation-based, which can be\nproblematic as they may not be sufficiently expressive to encode informative\ngraph structures. Furthermore, though the graph pooling module is also of vital\nimportance for graph learning, especially for the task of graph classification,\nresearch on graph down-sampling mechanisms is rather limited.\n  To address the above challenges, we propose a concatenation-based graph\nconvolution mechanism that injectively updates node representations to maximize\nthe discriminative power in distinguishing non-isomorphic subgraphs. In\naddition, we design a novel graph pooling module, called WL-SortPool, to learn\nimportant subgraph patterns in a deep-learning manner. WL-SortPool layer-wise\nsorts node representations (i.e. continuous WL colors) to separately learn the\nrelative importance of subtrees with different depths for the purpose of\nclassification, thus better characterizing the complex graph topology and rich\ninformation encoded in the graph. We propose a novel Subgraph Pattern GNN\n(SPGNN) architecture that incorporates these enhancements. We test the proposed\nSPGNN architecture on many graph classification benchmarks. Experimental\nresults show that our method can achieve highly competitive results with\nstate-of-the-art graph kernels and other GNN approaches.\n", "link": "http://arxiv.org/abs/2404.13655v2", "date": "2024-04-29", "relevancy": 2.5466, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5388}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4963}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4928}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SPGNN%3A%20Recognizing%20Salient%20Subgraph%20Patterns%20via%20Enhanced%20Graph%0A%20%20Convolution%20and%20Pooling&body=Title%3A%20SPGNN%3A%20Recognizing%20Salient%20Subgraph%20Patterns%20via%20Enhanced%20Graph%0A%20%20Convolution%20and%20Pooling%0AAuthor%3A%20Zehao%20Dong%20and%20Muhan%20Zhang%20and%20Yixin%20Chen%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20revolutionized%20the%20field%20of%20machine%0Alearning%20on%20non-Euclidean%20data%20such%20as%20graphs%20and%20networks.%20GNNs%20effectively%0Aimplement%20node%20representation%20learning%20through%20neighborhood%20aggregation%20and%0Aachieve%20impressive%20results%20in%20many%20graph-related%20tasks.%20However%2C%20most%0Aneighborhood%20aggregation%20approaches%20are%20summation-based%2C%20which%20can%20be%0Aproblematic%20as%20they%20may%20not%20be%20sufficiently%20expressive%20to%20encode%20informative%0Agraph%20structures.%20Furthermore%2C%20though%20the%20graph%20pooling%20module%20is%20also%20of%20vital%0Aimportance%20for%20graph%20learning%2C%20especially%20for%20the%20task%20of%20graph%20classification%2C%0Aresearch%20on%20graph%20down-sampling%20mechanisms%20is%20rather%20limited.%0A%20%20To%20address%20the%20above%20challenges%2C%20we%20propose%20a%20concatenation-based%20graph%0Aconvolution%20mechanism%20that%20injectively%20updates%20node%20representations%20to%20maximize%0Athe%20discriminative%20power%20in%20distinguishing%20non-isomorphic%20subgraphs.%20In%0Aaddition%2C%20we%20design%20a%20novel%20graph%20pooling%20module%2C%20called%20WL-SortPool%2C%20to%20learn%0Aimportant%20subgraph%20patterns%20in%20a%20deep-learning%20manner.%20WL-SortPool%20layer-wise%0Asorts%20node%20representations%20%28i.e.%20continuous%20WL%20colors%29%20to%20separately%20learn%20the%0Arelative%20importance%20of%20subtrees%20with%20different%20depths%20for%20the%20purpose%20of%0Aclassification%2C%20thus%20better%20characterizing%20the%20complex%20graph%20topology%20and%20rich%0Ainformation%20encoded%20in%20the%20graph.%20We%20propose%20a%20novel%20Subgraph%20Pattern%20GNN%0A%28SPGNN%29%20architecture%20that%20incorporates%20these%20enhancements.%20We%20test%20the%20proposed%0ASPGNN%20architecture%20on%20many%20graph%20classification%20benchmarks.%20Experimental%0Aresults%20show%20that%20our%20method%20can%20achieve%20highly%20competitive%20results%20with%0Astate-of-the-art%20graph%20kernels%20and%20other%20GNN%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13655v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPGNN%3A%20Recognizing%20Salient%20Subgraph%20Patterns%20via%20Enhanced%20Graph%0A%20%20Convolution%20and%20Pooling&entry.906535625=Zehao%20Dong%20and%20Muhan%20Zhang%20and%20Yixin%20Chen&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20revolutionized%20the%20field%20of%20machine%0Alearning%20on%20non-Euclidean%20data%20such%20as%20graphs%20and%20networks.%20GNNs%20effectively%0Aimplement%20node%20representation%20learning%20through%20neighborhood%20aggregation%20and%0Aachieve%20impressive%20results%20in%20many%20graph-related%20tasks.%20However%2C%20most%0Aneighborhood%20aggregation%20approaches%20are%20summation-based%2C%20which%20can%20be%0Aproblematic%20as%20they%20may%20not%20be%20sufficiently%20expressive%20to%20encode%20informative%0Agraph%20structures.%20Furthermore%2C%20though%20the%20graph%20pooling%20module%20is%20also%20of%20vital%0Aimportance%20for%20graph%20learning%2C%20especially%20for%20the%20task%20of%20graph%20classification%2C%0Aresearch%20on%20graph%20down-sampling%20mechanisms%20is%20rather%20limited.%0A%20%20To%20address%20the%20above%20challenges%2C%20we%20propose%20a%20concatenation-based%20graph%0Aconvolution%20mechanism%20that%20injectively%20updates%20node%20representations%20to%20maximize%0Athe%20discriminative%20power%20in%20distinguishing%20non-isomorphic%20subgraphs.%20In%0Aaddition%2C%20we%20design%20a%20novel%20graph%20pooling%20module%2C%20called%20WL-SortPool%2C%20to%20learn%0Aimportant%20subgraph%20patterns%20in%20a%20deep-learning%20manner.%20WL-SortPool%20layer-wise%0Asorts%20node%20representations%20%28i.e.%20continuous%20WL%20colors%29%20to%20separately%20learn%20the%0Arelative%20importance%20of%20subtrees%20with%20different%20depths%20for%20the%20purpose%20of%0Aclassification%2C%20thus%20better%20characterizing%20the%20complex%20graph%20topology%20and%20rich%0Ainformation%20encoded%20in%20the%20graph.%20We%20propose%20a%20novel%20Subgraph%20Pattern%20GNN%0A%28SPGNN%29%20architecture%20that%20incorporates%20these%20enhancements.%20We%20test%20the%20proposed%0ASPGNN%20architecture%20on%20many%20graph%20classification%20benchmarks.%20Experimental%0Aresults%20show%20that%20our%20method%20can%20achieve%20highly%20competitive%20results%20with%0Astate-of-the-art%20graph%20kernels%20and%20other%20GNN%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13655v2&entry.124074799=Read"},
{"title": "Self-supervised learning for classifying paranasal anomalies in the\n  maxillary sinus", "author": "Debayan Bhattacharya and Finn Behrendt and Benjamin Tobias Becker and Lennart Maack and Dirk Beyersdorff and Elina Petersen and Marvin Petersen and Bastian Cheng and Dennis Eggert and Christian Betz and Anna Sophie Hoffmann and Alexander Schlaefer", "abstract": "  Purpose: Paranasal anomalies, frequently identified in routine radiological\nscreenings, exhibit diverse morphological characteristics. Due to the diversity\nof anomalies, supervised learning methods require large labelled dataset\nexhibiting diverse anomaly morphology. Self-supervised learning (SSL) can be\nused to learn representations from unlabelled data. However, there are no SSL\nmethods designed for the downstream task of classifying paranasal anomalies in\nthe maxillary sinus (MS).\n  Methods: Our approach uses a 3D Convolutional Autoencoder (CAE) trained in an\nunsupervised anomaly detection (UAD) framework. Initially, we train the 3D CAE\nto reduce reconstruction errors when reconstructing normal maxillary sinus (MS)\nimage. Then, this CAE is applied to an unlabelled dataset to generate coarse\nanomaly locations by creating residual MS images. Following this, a 3D\nConvolutional Neural Network (CNN) reconstructs these residual images, which\nforms our SSL task. Lastly, we fine-tune the encoder part of the 3D CNN on a\nlabelled dataset of normal and anomalous MS images.\n  Results: The proposed SSL technique exhibits superior performance compared to\nexisting generic self-supervised methods, especially in scenarios with limited\nannotated data. When trained on just 10% of the annotated dataset, our method\nachieves an Area Under the Precision-Recall Curve (AUPRC) of 0.79 for the\ndownstream classification task. This performance surpasses other methods, with\nBYOL attaining an AUPRC of 0.75, SimSiam at 0.74, SimCLR at 0.73 and Masked\nAutoencoding using SparK at 0.75.\n  Conclusion: A self-supervised learning approach that inherently focuses on\nlocalizing paranasal anomalies proves to be advantageous, particularly when the\nsubsequent task involves differentiating normal from anomalous maxillary\nsinuses. Access our code at\nhttps://github.com/mtec-tuhh/self-supervised-paranasal-anomaly\n", "link": "http://arxiv.org/abs/2404.18599v1", "date": "2024-04-29", "relevancy": 2.545, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5116}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.509}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5064}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Self-supervised%20learning%20for%20classifying%20paranasal%20anomalies%20in%20the%0A%20%20maxillary%20sinus&body=Title%3A%20Self-supervised%20learning%20for%20classifying%20paranasal%20anomalies%20in%20the%0A%20%20maxillary%20sinus%0AAuthor%3A%20Debayan%20Bhattacharya%20and%20Finn%20Behrendt%20and%20Benjamin%20Tobias%20Becker%20and%20Lennart%20Maack%20and%20Dirk%20Beyersdorff%20and%20Elina%20Petersen%20and%20Marvin%20Petersen%20and%20Bastian%20Cheng%20and%20Dennis%20Eggert%20and%20Christian%20Betz%20and%20Anna%20Sophie%20Hoffmann%20and%20Alexander%20Schlaefer%0AAbstract%3A%20%20%20Purpose%3A%20Paranasal%20anomalies%2C%20frequently%20identified%20in%20routine%20radiological%0Ascreenings%2C%20exhibit%20diverse%20morphological%20characteristics.%20Due%20to%20the%20diversity%0Aof%20anomalies%2C%20supervised%20learning%20methods%20require%20large%20labelled%20dataset%0Aexhibiting%20diverse%20anomaly%20morphology.%20Self-supervised%20learning%20%28SSL%29%20can%20be%0Aused%20to%20learn%20representations%20from%20unlabelled%20data.%20However%2C%20there%20are%20no%20SSL%0Amethods%20designed%20for%20the%20downstream%20task%20of%20classifying%20paranasal%20anomalies%20in%0Athe%20maxillary%20sinus%20%28MS%29.%0A%20%20Methods%3A%20Our%20approach%20uses%20a%203D%20Convolutional%20Autoencoder%20%28CAE%29%20trained%20in%20an%0Aunsupervised%20anomaly%20detection%20%28UAD%29%20framework.%20Initially%2C%20we%20train%20the%203D%20CAE%0Ato%20reduce%20reconstruction%20errors%20when%20reconstructing%20normal%20maxillary%20sinus%20%28MS%29%0Aimage.%20Then%2C%20this%20CAE%20is%20applied%20to%20an%20unlabelled%20dataset%20to%20generate%20coarse%0Aanomaly%20locations%20by%20creating%20residual%20MS%20images.%20Following%20this%2C%20a%203D%0AConvolutional%20Neural%20Network%20%28CNN%29%20reconstructs%20these%20residual%20images%2C%20which%0Aforms%20our%20SSL%20task.%20Lastly%2C%20we%20fine-tune%20the%20encoder%20part%20of%20the%203D%20CNN%20on%20a%0Alabelled%20dataset%20of%20normal%20and%20anomalous%20MS%20images.%0A%20%20Results%3A%20The%20proposed%20SSL%20technique%20exhibits%20superior%20performance%20compared%20to%0Aexisting%20generic%20self-supervised%20methods%2C%20especially%20in%20scenarios%20with%20limited%0Aannotated%20data.%20When%20trained%20on%20just%2010%25%20of%20the%20annotated%20dataset%2C%20our%20method%0Aachieves%20an%20Area%20Under%20the%20Precision-Recall%20Curve%20%28AUPRC%29%20of%200.79%20for%20the%0Adownstream%20classification%20task.%20This%20performance%20surpasses%20other%20methods%2C%20with%0ABYOL%20attaining%20an%20AUPRC%20of%200.75%2C%20SimSiam%20at%200.74%2C%20SimCLR%20at%200.73%20and%20Masked%0AAutoencoding%20using%20SparK%20at%200.75.%0A%20%20Conclusion%3A%20A%20self-supervised%20learning%20approach%20that%20inherently%20focuses%20on%0Alocalizing%20paranasal%20anomalies%20proves%20to%20be%20advantageous%2C%20particularly%20when%20the%0Asubsequent%20task%20involves%20differentiating%20normal%20from%20anomalous%20maxillary%0Asinuses.%20Access%20our%20code%20at%0Ahttps%3A//github.com/mtec-tuhh/self-supervised-paranasal-anomaly%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18599v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-supervised%20learning%20for%20classifying%20paranasal%20anomalies%20in%20the%0A%20%20maxillary%20sinus&entry.906535625=Debayan%20Bhattacharya%20and%20Finn%20Behrendt%20and%20Benjamin%20Tobias%20Becker%20and%20Lennart%20Maack%20and%20Dirk%20Beyersdorff%20and%20Elina%20Petersen%20and%20Marvin%20Petersen%20and%20Bastian%20Cheng%20and%20Dennis%20Eggert%20and%20Christian%20Betz%20and%20Anna%20Sophie%20Hoffmann%20and%20Alexander%20Schlaefer&entry.1292438233=%20%20Purpose%3A%20Paranasal%20anomalies%2C%20frequently%20identified%20in%20routine%20radiological%0Ascreenings%2C%20exhibit%20diverse%20morphological%20characteristics.%20Due%20to%20the%20diversity%0Aof%20anomalies%2C%20supervised%20learning%20methods%20require%20large%20labelled%20dataset%0Aexhibiting%20diverse%20anomaly%20morphology.%20Self-supervised%20learning%20%28SSL%29%20can%20be%0Aused%20to%20learn%20representations%20from%20unlabelled%20data.%20However%2C%20there%20are%20no%20SSL%0Amethods%20designed%20for%20the%20downstream%20task%20of%20classifying%20paranasal%20anomalies%20in%0Athe%20maxillary%20sinus%20%28MS%29.%0A%20%20Methods%3A%20Our%20approach%20uses%20a%203D%20Convolutional%20Autoencoder%20%28CAE%29%20trained%20in%20an%0Aunsupervised%20anomaly%20detection%20%28UAD%29%20framework.%20Initially%2C%20we%20train%20the%203D%20CAE%0Ato%20reduce%20reconstruction%20errors%20when%20reconstructing%20normal%20maxillary%20sinus%20%28MS%29%0Aimage.%20Then%2C%20this%20CAE%20is%20applied%20to%20an%20unlabelled%20dataset%20to%20generate%20coarse%0Aanomaly%20locations%20by%20creating%20residual%20MS%20images.%20Following%20this%2C%20a%203D%0AConvolutional%20Neural%20Network%20%28CNN%29%20reconstructs%20these%20residual%20images%2C%20which%0Aforms%20our%20SSL%20task.%20Lastly%2C%20we%20fine-tune%20the%20encoder%20part%20of%20the%203D%20CNN%20on%20a%0Alabelled%20dataset%20of%20normal%20and%20anomalous%20MS%20images.%0A%20%20Results%3A%20The%20proposed%20SSL%20technique%20exhibits%20superior%20performance%20compared%20to%0Aexisting%20generic%20self-supervised%20methods%2C%20especially%20in%20scenarios%20with%20limited%0Aannotated%20data.%20When%20trained%20on%20just%2010%25%20of%20the%20annotated%20dataset%2C%20our%20method%0Aachieves%20an%20Area%20Under%20the%20Precision-Recall%20Curve%20%28AUPRC%29%20of%200.79%20for%20the%0Adownstream%20classification%20task.%20This%20performance%20surpasses%20other%20methods%2C%20with%0ABYOL%20attaining%20an%20AUPRC%20of%200.75%2C%20SimSiam%20at%200.74%2C%20SimCLR%20at%200.73%20and%20Masked%0AAutoencoding%20using%20SparK%20at%200.75.%0A%20%20Conclusion%3A%20A%20self-supervised%20learning%20approach%20that%20inherently%20focuses%20on%0Alocalizing%20paranasal%20anomalies%20proves%20to%20be%20advantageous%2C%20particularly%20when%20the%0Asubsequent%20task%20involves%20differentiating%20normal%20from%20anomalous%20maxillary%0Asinuses.%20Access%20our%20code%20at%0Ahttps%3A//github.com/mtec-tuhh/self-supervised-paranasal-anomaly%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18599v1&entry.124074799=Read"},
{"title": "From Density to Geometry: YOLOv8 Instance Segmentation for Reverse\n  Engineering of Optimized Structures", "author": "Thomas Rochefort-Beaudoin and Aurelian Vadean and Sofiane Achiche and Niels Aage", "abstract": "  This paper introduces YOLOv8-TO, a novel approach for reverse engineering of\ntopology-optimized structures into interpretable geometric parameters using the\nYOLOv8 instance segmentation model. Density-based topology optimization methods\nrequire post-processing to convert the optimal density distribution into a\nparametric representation for design exploration and integration with CAD\ntools. Traditional methods such as skeletonization struggle with complex\ngeometries and require manual intervention. YOLOv8-TO addresses these\nchallenges by training a custom YOLOv8 model to automatically detect and\nreconstruct structural components from binary density distributions. The model\nis trained on a diverse dataset of both optimized and random structures\ngenerated using the Moving Morphable Components method. A custom reconstruction\nloss function based on the dice coefficient of the predicted geometry is used\nto train the new regression head of the model via self-supervised learning. The\nmethod is evaluated on test sets generated from different topology optimization\nmethods, including out-of-distribution samples, and compared against a\nskeletonization approach. Results show that YOLOv8-TO significantly outperforms\nskeletonization in reconstructing visually and structurally similar designs.\nThe method showcases an average improvement of 13.84% in the Dice coefficient,\nwith peak enhancements reaching 20.78%. The method demonstrates good\ngeneralization to complex geometries and fast inference times, making it\nsuitable for integration into design workflows using regular workstations.\nLimitations include the sensitivity to non-max suppression thresholds.\nYOLOv8-TO represents a significant advancement in topology optimization\npost-processing, enabling efficient and accurate reverse engineering of\noptimized structures for design exploration and manufacturing.\n", "link": "http://arxiv.org/abs/2404.18763v1", "date": "2024-04-29", "relevancy": 2.5075, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5109}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5003}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4934}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20From%20Density%20to%20Geometry%3A%20YOLOv8%20Instance%20Segmentation%20for%20Reverse%0A%20%20Engineering%20of%20Optimized%20Structures&body=Title%3A%20From%20Density%20to%20Geometry%3A%20YOLOv8%20Instance%20Segmentation%20for%20Reverse%0A%20%20Engineering%20of%20Optimized%20Structures%0AAuthor%3A%20Thomas%20Rochefort-Beaudoin%20and%20Aurelian%20Vadean%20and%20Sofiane%20Achiche%20and%20Niels%20Aage%0AAbstract%3A%20%20%20This%20paper%20introduces%20YOLOv8-TO%2C%20a%20novel%20approach%20for%20reverse%20engineering%20of%0Atopology-optimized%20structures%20into%20interpretable%20geometric%20parameters%20using%20the%0AYOLOv8%20instance%20segmentation%20model.%20Density-based%20topology%20optimization%20methods%0Arequire%20post-processing%20to%20convert%20the%20optimal%20density%20distribution%20into%20a%0Aparametric%20representation%20for%20design%20exploration%20and%20integration%20with%20CAD%0Atools.%20Traditional%20methods%20such%20as%20skeletonization%20struggle%20with%20complex%0Ageometries%20and%20require%20manual%20intervention.%20YOLOv8-TO%20addresses%20these%0Achallenges%20by%20training%20a%20custom%20YOLOv8%20model%20to%20automatically%20detect%20and%0Areconstruct%20structural%20components%20from%20binary%20density%20distributions.%20The%20model%0Ais%20trained%20on%20a%20diverse%20dataset%20of%20both%20optimized%20and%20random%20structures%0Agenerated%20using%20the%20Moving%20Morphable%20Components%20method.%20A%20custom%20reconstruction%0Aloss%20function%20based%20on%20the%20dice%20coefficient%20of%20the%20predicted%20geometry%20is%20used%0Ato%20train%20the%20new%20regression%20head%20of%20the%20model%20via%20self-supervised%20learning.%20The%0Amethod%20is%20evaluated%20on%20test%20sets%20generated%20from%20different%20topology%20optimization%0Amethods%2C%20including%20out-of-distribution%20samples%2C%20and%20compared%20against%20a%0Askeletonization%20approach.%20Results%20show%20that%20YOLOv8-TO%20significantly%20outperforms%0Askeletonization%20in%20reconstructing%20visually%20and%20structurally%20similar%20designs.%0AThe%20method%20showcases%20an%20average%20improvement%20of%2013.84%25%20in%20the%20Dice%20coefficient%2C%0Awith%20peak%20enhancements%20reaching%2020.78%25.%20The%20method%20demonstrates%20good%0Ageneralization%20to%20complex%20geometries%20and%20fast%20inference%20times%2C%20making%20it%0Asuitable%20for%20integration%20into%20design%20workflows%20using%20regular%20workstations.%0ALimitations%20include%20the%20sensitivity%20to%20non-max%20suppression%20thresholds.%0AYOLOv8-TO%20represents%20a%20significant%20advancement%20in%20topology%20optimization%0Apost-processing%2C%20enabling%20efficient%20and%20accurate%20reverse%20engineering%20of%0Aoptimized%20structures%20for%20design%20exploration%20and%20manufacturing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18763v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Density%20to%20Geometry%3A%20YOLOv8%20Instance%20Segmentation%20for%20Reverse%0A%20%20Engineering%20of%20Optimized%20Structures&entry.906535625=Thomas%20Rochefort-Beaudoin%20and%20Aurelian%20Vadean%20and%20Sofiane%20Achiche%20and%20Niels%20Aage&entry.1292438233=%20%20This%20paper%20introduces%20YOLOv8-TO%2C%20a%20novel%20approach%20for%20reverse%20engineering%20of%0Atopology-optimized%20structures%20into%20interpretable%20geometric%20parameters%20using%20the%0AYOLOv8%20instance%20segmentation%20model.%20Density-based%20topology%20optimization%20methods%0Arequire%20post-processing%20to%20convert%20the%20optimal%20density%20distribution%20into%20a%0Aparametric%20representation%20for%20design%20exploration%20and%20integration%20with%20CAD%0Atools.%20Traditional%20methods%20such%20as%20skeletonization%20struggle%20with%20complex%0Ageometries%20and%20require%20manual%20intervention.%20YOLOv8-TO%20addresses%20these%0Achallenges%20by%20training%20a%20custom%20YOLOv8%20model%20to%20automatically%20detect%20and%0Areconstruct%20structural%20components%20from%20binary%20density%20distributions.%20The%20model%0Ais%20trained%20on%20a%20diverse%20dataset%20of%20both%20optimized%20and%20random%20structures%0Agenerated%20using%20the%20Moving%20Morphable%20Components%20method.%20A%20custom%20reconstruction%0Aloss%20function%20based%20on%20the%20dice%20coefficient%20of%20the%20predicted%20geometry%20is%20used%0Ato%20train%20the%20new%20regression%20head%20of%20the%20model%20via%20self-supervised%20learning.%20The%0Amethod%20is%20evaluated%20on%20test%20sets%20generated%20from%20different%20topology%20optimization%0Amethods%2C%20including%20out-of-distribution%20samples%2C%20and%20compared%20against%20a%0Askeletonization%20approach.%20Results%20show%20that%20YOLOv8-TO%20significantly%20outperforms%0Askeletonization%20in%20reconstructing%20visually%20and%20structurally%20similar%20designs.%0AThe%20method%20showcases%20an%20average%20improvement%20of%2013.84%25%20in%20the%20Dice%20coefficient%2C%0Awith%20peak%20enhancements%20reaching%2020.78%25.%20The%20method%20demonstrates%20good%0Ageneralization%20to%20complex%20geometries%20and%20fast%20inference%20times%2C%20making%20it%0Asuitable%20for%20integration%20into%20design%20workflows%20using%20regular%20workstations.%0ALimitations%20include%20the%20sensitivity%20to%20non-max%20suppression%20thresholds.%0AYOLOv8-TO%20represents%20a%20significant%20advancement%20in%20topology%20optimization%0Apost-processing%2C%20enabling%20efficient%20and%20accurate%20reverse%20engineering%20of%0Aoptimized%20structures%20for%20design%20exploration%20and%20manufacturing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18763v1&entry.124074799=Read"},
{"title": "SCT: A Simple Baseline for Parameter-Efficient Fine-Tuning via Salient\n  Channels", "author": "Henry Hengyuan Zhao and Pichao Wang and Yuyang Zhao and Hao Luo and Fan Wang and Mike Zheng Shou", "abstract": "  Pre-trained vision transformers have strong representation benefits to\nvarious downstream tasks. Recently, many parameter-efficient fine-tuning (PEFT)\nmethods have been proposed, and their experiments demonstrate that tuning only\n1\\% extra parameters could surpass full fine-tuning in low-data resource\nscenarios. However, these methods overlook the task-specific information when\nfine-tuning diverse downstream tasks. In this paper, we propose a simple yet\neffective method called \"Salient Channel Tuning\" (SCT) to leverage the\ntask-specific information by forwarding the model with the task images to\nselect partial channels in a feature map that enables us to tune only 1/8\nchannels leading to significantly lower parameter costs. Experiments on 19\nvisual transfer learning downstream tasks demonstrate that our SCT outperforms\nfull fine-tuning on 18 out of 19 tasks by adding only 0.11M parameters of the\nViT-B, which is 780$\\times$ fewer than its full fine-tuning counterpart.\nFurthermore, experiments on domain generalization and few-shot classification\nfurther demonstrate the effectiveness and generic of our approach. The code is\navailable at https://github.com/showlab/SCT.\n", "link": "http://arxiv.org/abs/2309.08513v5", "date": "2024-04-29", "relevancy": 2.5074, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5056}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4998}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.499}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SCT%3A%20A%20Simple%20Baseline%20for%20Parameter-Efficient%20Fine-Tuning%20via%20Salient%0A%20%20Channels&body=Title%3A%20SCT%3A%20A%20Simple%20Baseline%20for%20Parameter-Efficient%20Fine-Tuning%20via%20Salient%0A%20%20Channels%0AAuthor%3A%20Henry%20Hengyuan%20Zhao%20and%20Pichao%20Wang%20and%20Yuyang%20Zhao%20and%20Hao%20Luo%20and%20Fan%20Wang%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20%20%20Pre-trained%20vision%20transformers%20have%20strong%20representation%20benefits%20to%0Avarious%20downstream%20tasks.%20Recently%2C%20many%20parameter-efficient%20fine-tuning%20%28PEFT%29%0Amethods%20have%20been%20proposed%2C%20and%20their%20experiments%20demonstrate%20that%20tuning%20only%0A1%5C%25%20extra%20parameters%20could%20surpass%20full%20fine-tuning%20in%20low-data%20resource%0Ascenarios.%20However%2C%20these%20methods%20overlook%20the%20task-specific%20information%20when%0Afine-tuning%20diverse%20downstream%20tasks.%20In%20this%20paper%2C%20we%20propose%20a%20simple%20yet%0Aeffective%20method%20called%20%22Salient%20Channel%20Tuning%22%20%28SCT%29%20to%20leverage%20the%0Atask-specific%20information%20by%20forwarding%20the%20model%20with%20the%20task%20images%20to%0Aselect%20partial%20channels%20in%20a%20feature%20map%20that%20enables%20us%20to%20tune%20only%201/8%0Achannels%20leading%20to%20significantly%20lower%20parameter%20costs.%20Experiments%20on%2019%0Avisual%20transfer%20learning%20downstream%20tasks%20demonstrate%20that%20our%20SCT%20outperforms%0Afull%20fine-tuning%20on%2018%20out%20of%2019%20tasks%20by%20adding%20only%200.11M%20parameters%20of%20the%0AViT-B%2C%20which%20is%20780%24%5Ctimes%24%20fewer%20than%20its%20full%20fine-tuning%20counterpart.%0AFurthermore%2C%20experiments%20on%20domain%20generalization%20and%20few-shot%20classification%0Afurther%20demonstrate%20the%20effectiveness%20and%20generic%20of%20our%20approach.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/showlab/SCT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.08513v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SCT%3A%20A%20Simple%20Baseline%20for%20Parameter-Efficient%20Fine-Tuning%20via%20Salient%0A%20%20Channels&entry.906535625=Henry%20Hengyuan%20Zhao%20and%20Pichao%20Wang%20and%20Yuyang%20Zhao%20and%20Hao%20Luo%20and%20Fan%20Wang%20and%20Mike%20Zheng%20Shou&entry.1292438233=%20%20Pre-trained%20vision%20transformers%20have%20strong%20representation%20benefits%20to%0Avarious%20downstream%20tasks.%20Recently%2C%20many%20parameter-efficient%20fine-tuning%20%28PEFT%29%0Amethods%20have%20been%20proposed%2C%20and%20their%20experiments%20demonstrate%20that%20tuning%20only%0A1%5C%25%20extra%20parameters%20could%20surpass%20full%20fine-tuning%20in%20low-data%20resource%0Ascenarios.%20However%2C%20these%20methods%20overlook%20the%20task-specific%20information%20when%0Afine-tuning%20diverse%20downstream%20tasks.%20In%20this%20paper%2C%20we%20propose%20a%20simple%20yet%0Aeffective%20method%20called%20%22Salient%20Channel%20Tuning%22%20%28SCT%29%20to%20leverage%20the%0Atask-specific%20information%20by%20forwarding%20the%20model%20with%20the%20task%20images%20to%0Aselect%20partial%20channels%20in%20a%20feature%20map%20that%20enables%20us%20to%20tune%20only%201/8%0Achannels%20leading%20to%20significantly%20lower%20parameter%20costs.%20Experiments%20on%2019%0Avisual%20transfer%20learning%20downstream%20tasks%20demonstrate%20that%20our%20SCT%20outperforms%0Afull%20fine-tuning%20on%2018%20out%20of%2019%20tasks%20by%20adding%20only%200.11M%20parameters%20of%20the%0AViT-B%2C%20which%20is%20780%24%5Ctimes%24%20fewer%20than%20its%20full%20fine-tuning%20counterpart.%0AFurthermore%2C%20experiments%20on%20domain%20generalization%20and%20few-shot%20classification%0Afurther%20demonstrate%20the%20effectiveness%20and%20generic%20of%20our%20approach.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/showlab/SCT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.08513v5&entry.124074799=Read"},
{"title": "Nightshade: Prompt-Specific Poisoning Attacks on Text-to-Image\n  Generative Models", "author": "Shawn Shan and Wenxin Ding and Josephine Passananti and Stanley Wu and Haitao Zheng and Ben Y. Zhao", "abstract": "  Data poisoning attacks manipulate training data to introduce unexpected\nbehaviors into machine learning models at training time. For text-to-image\ngenerative models with massive training datasets, current understanding of\npoisoning attacks suggests that a successful attack would require injecting\nmillions of poison samples into their training pipeline. In this paper, we show\nthat poisoning attacks can be successful on generative models. We observe that\ntraining data per concept can be quite limited in these models, making them\nvulnerable to prompt-specific poisoning attacks, which target a model's ability\nto respond to individual prompts.\n  We introduce Nightshade, an optimized prompt-specific poisoning attack where\npoison samples look visually identical to benign images with matching text\nprompts. Nightshade poison samples are also optimized for potency and can\ncorrupt an Stable Diffusion SDXL prompt in <100 poison samples. Nightshade\npoison effects \"bleed through\" to related concepts, and multiple attacks can\ncomposed together in a single prompt. Surprisingly, we show that a moderate\nnumber of Nightshade attacks can destabilize general features in a\ntext-to-image generative model, effectively disabling its ability to generate\nmeaningful images. Finally, we propose the use of Nightshade and similar tools\nas a last defense for content creators against web scrapers that ignore\nopt-out/do-not-crawl directives, and discuss possible implications for model\ntrainers and content creators.\n", "link": "http://arxiv.org/abs/2310.13828v3", "date": "2024-04-29", "relevancy": 2.4496, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5017}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4876}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4805}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Nightshade%3A%20Prompt-Specific%20Poisoning%20Attacks%20on%20Text-to-Image%0A%20%20Generative%20Models&body=Title%3A%20Nightshade%3A%20Prompt-Specific%20Poisoning%20Attacks%20on%20Text-to-Image%0A%20%20Generative%20Models%0AAuthor%3A%20Shawn%20Shan%20and%20Wenxin%20Ding%20and%20Josephine%20Passananti%20and%20Stanley%20Wu%20and%20Haitao%20Zheng%20and%20Ben%20Y.%20Zhao%0AAbstract%3A%20%20%20Data%20poisoning%20attacks%20manipulate%20training%20data%20to%20introduce%20unexpected%0Abehaviors%20into%20machine%20learning%20models%20at%20training%20time.%20For%20text-to-image%0Agenerative%20models%20with%20massive%20training%20datasets%2C%20current%20understanding%20of%0Apoisoning%20attacks%20suggests%20that%20a%20successful%20attack%20would%20require%20injecting%0Amillions%20of%20poison%20samples%20into%20their%20training%20pipeline.%20In%20this%20paper%2C%20we%20show%0Athat%20poisoning%20attacks%20can%20be%20successful%20on%20generative%20models.%20We%20observe%20that%0Atraining%20data%20per%20concept%20can%20be%20quite%20limited%20in%20these%20models%2C%20making%20them%0Avulnerable%20to%20prompt-specific%20poisoning%20attacks%2C%20which%20target%20a%20model%27s%20ability%0Ato%20respond%20to%20individual%20prompts.%0A%20%20We%20introduce%20Nightshade%2C%20an%20optimized%20prompt-specific%20poisoning%20attack%20where%0Apoison%20samples%20look%20visually%20identical%20to%20benign%20images%20with%20matching%20text%0Aprompts.%20Nightshade%20poison%20samples%20are%20also%20optimized%20for%20potency%20and%20can%0Acorrupt%20an%20Stable%20Diffusion%20SDXL%20prompt%20in%20%3C100%20poison%20samples.%20Nightshade%0Apoison%20effects%20%22bleed%20through%22%20to%20related%20concepts%2C%20and%20multiple%20attacks%20can%0Acomposed%20together%20in%20a%20single%20prompt.%20Surprisingly%2C%20we%20show%20that%20a%20moderate%0Anumber%20of%20Nightshade%20attacks%20can%20destabilize%20general%20features%20in%20a%0Atext-to-image%20generative%20model%2C%20effectively%20disabling%20its%20ability%20to%20generate%0Ameaningful%20images.%20Finally%2C%20we%20propose%20the%20use%20of%20Nightshade%20and%20similar%20tools%0Aas%20a%20last%20defense%20for%20content%20creators%20against%20web%20scrapers%20that%20ignore%0Aopt-out/do-not-crawl%20directives%2C%20and%20discuss%20possible%20implications%20for%20model%0Atrainers%20and%20content%20creators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.13828v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nightshade%3A%20Prompt-Specific%20Poisoning%20Attacks%20on%20Text-to-Image%0A%20%20Generative%20Models&entry.906535625=Shawn%20Shan%20and%20Wenxin%20Ding%20and%20Josephine%20Passananti%20and%20Stanley%20Wu%20and%20Haitao%20Zheng%20and%20Ben%20Y.%20Zhao&entry.1292438233=%20%20Data%20poisoning%20attacks%20manipulate%20training%20data%20to%20introduce%20unexpected%0Abehaviors%20into%20machine%20learning%20models%20at%20training%20time.%20For%20text-to-image%0Agenerative%20models%20with%20massive%20training%20datasets%2C%20current%20understanding%20of%0Apoisoning%20attacks%20suggests%20that%20a%20successful%20attack%20would%20require%20injecting%0Amillions%20of%20poison%20samples%20into%20their%20training%20pipeline.%20In%20this%20paper%2C%20we%20show%0Athat%20poisoning%20attacks%20can%20be%20successful%20on%20generative%20models.%20We%20observe%20that%0Atraining%20data%20per%20concept%20can%20be%20quite%20limited%20in%20these%20models%2C%20making%20them%0Avulnerable%20to%20prompt-specific%20poisoning%20attacks%2C%20which%20target%20a%20model%27s%20ability%0Ato%20respond%20to%20individual%20prompts.%0A%20%20We%20introduce%20Nightshade%2C%20an%20optimized%20prompt-specific%20poisoning%20attack%20where%0Apoison%20samples%20look%20visually%20identical%20to%20benign%20images%20with%20matching%20text%0Aprompts.%20Nightshade%20poison%20samples%20are%20also%20optimized%20for%20potency%20and%20can%0Acorrupt%20an%20Stable%20Diffusion%20SDXL%20prompt%20in%20%3C100%20poison%20samples.%20Nightshade%0Apoison%20effects%20%22bleed%20through%22%20to%20related%20concepts%2C%20and%20multiple%20attacks%20can%0Acomposed%20together%20in%20a%20single%20prompt.%20Surprisingly%2C%20we%20show%20that%20a%20moderate%0Anumber%20of%20Nightshade%20attacks%20can%20destabilize%20general%20features%20in%20a%0Atext-to-image%20generative%20model%2C%20effectively%20disabling%20its%20ability%20to%20generate%0Ameaningful%20images.%20Finally%2C%20we%20propose%20the%20use%20of%20Nightshade%20and%20similar%20tools%0Aas%20a%20last%20defense%20for%20content%20creators%20against%20web%20scrapers%20that%20ignore%0Aopt-out/do-not-crawl%20directives%2C%20and%20discuss%20possible%20implications%20for%20model%0Atrainers%20and%20content%20creators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.13828v3&entry.124074799=Read"},
{"title": "The Solution for the CVPR2024 NICE Image Captioning Challenge", "author": "Longfei Huang and Shupeng Zhong and Xiangyu Wu and Ruoxuan Li", "abstract": "  This report introduces a solution to the Topic 1 Zero-shot Image Captioning\nof 2024 NICE : New frontiers for zero-shot Image Captioning Evaluation. In\ncontrast to NICE 2023 datasets, this challenge involves new annotations by\nhumans with significant differences in caption style and content. Therefore, we\nenhance image captions effectively through retrieval augmentation and caption\ngrading methods. At the data level, we utilize high-quality captions generated\nby image caption models as training data to address the gap in text styles. At\nthe model level, we employ OFA (a large-scale visual-language pre-training\nmodel based on handcrafted templates) to perform the image captioning task.\nSubsequently, we propose caption-level strategy for the high-quality caption\ndata generated by the image caption models and integrate them with retrieval\naugmentation strategy into the template to compel the model to generate higher\nquality, more matching, and semantically enriched captions based on the\nretrieval augmentation prompts. Our approach achieves a CIDEr score of 234.11.\n", "link": "http://arxiv.org/abs/2404.12739v2", "date": "2024-04-29", "relevancy": 2.424, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4881}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4849}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4814}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20Solution%20for%20the%20CVPR2024%20NICE%20Image%20Captioning%20Challenge&body=Title%3A%20The%20Solution%20for%20the%20CVPR2024%20NICE%20Image%20Captioning%20Challenge%0AAuthor%3A%20Longfei%20Huang%20and%20Shupeng%20Zhong%20and%20Xiangyu%20Wu%20and%20Ruoxuan%20Li%0AAbstract%3A%20%20%20This%20report%20introduces%20a%20solution%20to%20the%20Topic%201%20Zero-shot%20Image%20Captioning%0Aof%202024%20NICE%20%3A%20New%20frontiers%20for%20zero-shot%20Image%20Captioning%20Evaluation.%20In%0Acontrast%20to%20NICE%202023%20datasets%2C%20this%20challenge%20involves%20new%20annotations%20by%0Ahumans%20with%20significant%20differences%20in%20caption%20style%20and%20content.%20Therefore%2C%20we%0Aenhance%20image%20captions%20effectively%20through%20retrieval%20augmentation%20and%20caption%0Agrading%20methods.%20At%20the%20data%20level%2C%20we%20utilize%20high-quality%20captions%20generated%0Aby%20image%20caption%20models%20as%20training%20data%20to%20address%20the%20gap%20in%20text%20styles.%20At%0Athe%20model%20level%2C%20we%20employ%20OFA%20%28a%20large-scale%20visual-language%20pre-training%0Amodel%20based%20on%20handcrafted%20templates%29%20to%20perform%20the%20image%20captioning%20task.%0ASubsequently%2C%20we%20propose%20caption-level%20strategy%20for%20the%20high-quality%20caption%0Adata%20generated%20by%20the%20image%20caption%20models%20and%20integrate%20them%20with%20retrieval%0Aaugmentation%20strategy%20into%20the%20template%20to%20compel%20the%20model%20to%20generate%20higher%0Aquality%2C%20more%20matching%2C%20and%20semantically%20enriched%20captions%20based%20on%20the%0Aretrieval%20augmentation%20prompts.%20Our%20approach%20achieves%20a%20CIDEr%20score%20of%20234.11.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12739v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Solution%20for%20the%20CVPR2024%20NICE%20Image%20Captioning%20Challenge&entry.906535625=Longfei%20Huang%20and%20Shupeng%20Zhong%20and%20Xiangyu%20Wu%20and%20Ruoxuan%20Li&entry.1292438233=%20%20This%20report%20introduces%20a%20solution%20to%20the%20Topic%201%20Zero-shot%20Image%20Captioning%0Aof%202024%20NICE%20%3A%20New%20frontiers%20for%20zero-shot%20Image%20Captioning%20Evaluation.%20In%0Acontrast%20to%20NICE%202023%20datasets%2C%20this%20challenge%20involves%20new%20annotations%20by%0Ahumans%20with%20significant%20differences%20in%20caption%20style%20and%20content.%20Therefore%2C%20we%0Aenhance%20image%20captions%20effectively%20through%20retrieval%20augmentation%20and%20caption%0Agrading%20methods.%20At%20the%20data%20level%2C%20we%20utilize%20high-quality%20captions%20generated%0Aby%20image%20caption%20models%20as%20training%20data%20to%20address%20the%20gap%20in%20text%20styles.%20At%0Athe%20model%20level%2C%20we%20employ%20OFA%20%28a%20large-scale%20visual-language%20pre-training%0Amodel%20based%20on%20handcrafted%20templates%29%20to%20perform%20the%20image%20captioning%20task.%0ASubsequently%2C%20we%20propose%20caption-level%20strategy%20for%20the%20high-quality%20caption%0Adata%20generated%20by%20the%20image%20caption%20models%20and%20integrate%20them%20with%20retrieval%0Aaugmentation%20strategy%20into%20the%20template%20to%20compel%20the%20model%20to%20generate%20higher%0Aquality%2C%20more%20matching%2C%20and%20semantically%20enriched%20captions%20based%20on%20the%0Aretrieval%20augmentation%20prompts.%20Our%20approach%20achieves%20a%20CIDEr%20score%20of%20234.11.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12739v2&entry.124074799=Read"},
{"title": "3Doodle: Compact Abstraction of Objects with 3D Strokes", "author": "Changwoon Choi and Jaeah Lee and Jaesik Park and Young Min Kim", "abstract": "  While free-hand sketching has long served as an efficient representation to\nconvey characteristics of an object, they are often subjective, deviating\nsignificantly from realistic representations. Moreover, sketches are not\nconsistent for arbitrary viewpoints, making it hard to catch 3D shapes. We\npropose 3Dooole, generating descriptive and view-consistent sketch images given\nmulti-view images of the target object. Our method is based on the idea that a\nset of 3D strokes can efficiently represent 3D structural information and\nrender view-consistent 2D sketches. We express 2D sketches as a union of\nview-independent and view-dependent components. 3D cubic B ezier curves\nindicate view-independent 3D feature lines, while contours of superquadrics\nexpress a smooth outline of the volume of varying viewpoints. Our pipeline\ndirectly optimizes the parameters of 3D stroke primitives to minimize\nperceptual losses in a fully differentiable manner. The resulting sparse set of\n3D strokes can be rendered as abstract sketches containing essential 3D\ncharacteristic shapes of various objects. We demonstrate that 3Doodle can\nfaithfully express concepts of the original images compared with recent sketch\ngeneration approaches.\n", "link": "http://arxiv.org/abs/2402.03690v2", "date": "2024-04-29", "relevancy": 2.4209, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4908}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4885}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4732}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%203Doodle%3A%20Compact%20Abstraction%20of%20Objects%20with%203D%20Strokes&body=Title%3A%203Doodle%3A%20Compact%20Abstraction%20of%20Objects%20with%203D%20Strokes%0AAuthor%3A%20Changwoon%20Choi%20and%20Jaeah%20Lee%20and%20Jaesik%20Park%20and%20Young%20Min%20Kim%0AAbstract%3A%20%20%20While%20free-hand%20sketching%20has%20long%20served%20as%20an%20efficient%20representation%20to%0Aconvey%20characteristics%20of%20an%20object%2C%20they%20are%20often%20subjective%2C%20deviating%0Asignificantly%20from%20realistic%20representations.%20Moreover%2C%20sketches%20are%20not%0Aconsistent%20for%20arbitrary%20viewpoints%2C%20making%20it%20hard%20to%20catch%203D%20shapes.%20We%0Apropose%203Dooole%2C%20generating%20descriptive%20and%20view-consistent%20sketch%20images%20given%0Amulti-view%20images%20of%20the%20target%20object.%20Our%20method%20is%20based%20on%20the%20idea%20that%20a%0Aset%20of%203D%20strokes%20can%20efficiently%20represent%203D%20structural%20information%20and%0Arender%20view-consistent%202D%20sketches.%20We%20express%202D%20sketches%20as%20a%20union%20of%0Aview-independent%20and%20view-dependent%20components.%203D%20cubic%20B%20ezier%20curves%0Aindicate%20view-independent%203D%20feature%20lines%2C%20while%20contours%20of%20superquadrics%0Aexpress%20a%20smooth%20outline%20of%20the%20volume%20of%20varying%20viewpoints.%20Our%20pipeline%0Adirectly%20optimizes%20the%20parameters%20of%203D%20stroke%20primitives%20to%20minimize%0Aperceptual%20losses%20in%20a%20fully%20differentiable%20manner.%20The%20resulting%20sparse%20set%20of%0A3D%20strokes%20can%20be%20rendered%20as%20abstract%20sketches%20containing%20essential%203D%0Acharacteristic%20shapes%20of%20various%20objects.%20We%20demonstrate%20that%203Doodle%20can%0Afaithfully%20express%20concepts%20of%20the%20original%20images%20compared%20with%20recent%20sketch%0Ageneration%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03690v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3Doodle%3A%20Compact%20Abstraction%20of%20Objects%20with%203D%20Strokes&entry.906535625=Changwoon%20Choi%20and%20Jaeah%20Lee%20and%20Jaesik%20Park%20and%20Young%20Min%20Kim&entry.1292438233=%20%20While%20free-hand%20sketching%20has%20long%20served%20as%20an%20efficient%20representation%20to%0Aconvey%20characteristics%20of%20an%20object%2C%20they%20are%20often%20subjective%2C%20deviating%0Asignificantly%20from%20realistic%20representations.%20Moreover%2C%20sketches%20are%20not%0Aconsistent%20for%20arbitrary%20viewpoints%2C%20making%20it%20hard%20to%20catch%203D%20shapes.%20We%0Apropose%203Dooole%2C%20generating%20descriptive%20and%20view-consistent%20sketch%20images%20given%0Amulti-view%20images%20of%20the%20target%20object.%20Our%20method%20is%20based%20on%20the%20idea%20that%20a%0Aset%20of%203D%20strokes%20can%20efficiently%20represent%203D%20structural%20information%20and%0Arender%20view-consistent%202D%20sketches.%20We%20express%202D%20sketches%20as%20a%20union%20of%0Aview-independent%20and%20view-dependent%20components.%203D%20cubic%20B%20ezier%20curves%0Aindicate%20view-independent%203D%20feature%20lines%2C%20while%20contours%20of%20superquadrics%0Aexpress%20a%20smooth%20outline%20of%20the%20volume%20of%20varying%20viewpoints.%20Our%20pipeline%0Adirectly%20optimizes%20the%20parameters%20of%203D%20stroke%20primitives%20to%20minimize%0Aperceptual%20losses%20in%20a%20fully%20differentiable%20manner.%20The%20resulting%20sparse%20set%20of%0A3D%20strokes%20can%20be%20rendered%20as%20abstract%20sketches%20containing%20essential%203D%0Acharacteristic%20shapes%20of%20various%20objects.%20We%20demonstrate%20that%203Doodle%20can%0Afaithfully%20express%20concepts%20of%20the%20original%20images%20compared%20with%20recent%20sketch%0Ageneration%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03690v2&entry.124074799=Read"},
{"title": "Learning general Gaussian mixtures with efficient score matching", "author": "Sitan Chen and Vasilis Kontonis and Kulin Shah", "abstract": "  We study the problem of learning mixtures of $k$ Gaussians in $d$ dimensions.\nWe make no separation assumptions on the underlying mixture components: we only\nrequire that the covariance matrices have bounded condition number and that the\nmeans and covariances lie in a ball of bounded radius. We give an algorithm\nthat draws $d^{\\mathrm{poly}(k/\\varepsilon)}$ samples from the target mixture,\nruns in sample-polynomial time, and constructs a sampler whose output\ndistribution is $\\varepsilon$-far from the unknown mixture in total variation.\nPrior works for this problem either (i) required exponential runtime in the\ndimension $d$, (ii) placed strong assumptions on the instance (e.g., spherical\ncovariances or clusterability), or (iii) had doubly exponential dependence on\nthe number of components $k$.\n  Our approach departs from commonly used techniques for this problem like the\nmethod of moments. Instead, we leverage a recently developed reduction, based\non diffusion models, from distribution learning to a supervised learning task\ncalled score matching. We give an algorithm for the latter by proving a\nstructural result showing that the score function of a Gaussian mixture can be\napproximated by a piecewise-polynomial function, and there is an efficient\nalgorithm for finding it. To our knowledge, this is the first example of\ndiffusion models achieving a state-of-the-art theoretical guarantee for an\nunsupervised learning task.\n", "link": "http://arxiv.org/abs/2404.18893v1", "date": "2024-04-29", "relevancy": 2.4071, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4951}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4755}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4736}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20general%20Gaussian%20mixtures%20with%20efficient%20score%20matching&body=Title%3A%20Learning%20general%20Gaussian%20mixtures%20with%20efficient%20score%20matching%0AAuthor%3A%20Sitan%20Chen%20and%20Vasilis%20Kontonis%20and%20Kulin%20Shah%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20learning%20mixtures%20of%20%24k%24%20Gaussians%20in%20%24d%24%20dimensions.%0AWe%20make%20no%20separation%20assumptions%20on%20the%20underlying%20mixture%20components%3A%20we%20only%0Arequire%20that%20the%20covariance%20matrices%20have%20bounded%20condition%20number%20and%20that%20the%0Ameans%20and%20covariances%20lie%20in%20a%20ball%20of%20bounded%20radius.%20We%20give%20an%20algorithm%0Athat%20draws%20%24d%5E%7B%5Cmathrm%7Bpoly%7D%28k/%5Cvarepsilon%29%7D%24%20samples%20from%20the%20target%20mixture%2C%0Aruns%20in%20sample-polynomial%20time%2C%20and%20constructs%20a%20sampler%20whose%20output%0Adistribution%20is%20%24%5Cvarepsilon%24-far%20from%20the%20unknown%20mixture%20in%20total%20variation.%0APrior%20works%20for%20this%20problem%20either%20%28i%29%20required%20exponential%20runtime%20in%20the%0Adimension%20%24d%24%2C%20%28ii%29%20placed%20strong%20assumptions%20on%20the%20instance%20%28e.g.%2C%20spherical%0Acovariances%20or%20clusterability%29%2C%20or%20%28iii%29%20had%20doubly%20exponential%20dependence%20on%0Athe%20number%20of%20components%20%24k%24.%0A%20%20Our%20approach%20departs%20from%20commonly%20used%20techniques%20for%20this%20problem%20like%20the%0Amethod%20of%20moments.%20Instead%2C%20we%20leverage%20a%20recently%20developed%20reduction%2C%20based%0Aon%20diffusion%20models%2C%20from%20distribution%20learning%20to%20a%20supervised%20learning%20task%0Acalled%20score%20matching.%20We%20give%20an%20algorithm%20for%20the%20latter%20by%20proving%20a%0Astructural%20result%20showing%20that%20the%20score%20function%20of%20a%20Gaussian%20mixture%20can%20be%0Aapproximated%20by%20a%20piecewise-polynomial%20function%2C%20and%20there%20is%20an%20efficient%0Aalgorithm%20for%20finding%20it.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20example%20of%0Adiffusion%20models%20achieving%20a%20state-of-the-art%20theoretical%20guarantee%20for%20an%0Aunsupervised%20learning%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18893v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20general%20Gaussian%20mixtures%20with%20efficient%20score%20matching&entry.906535625=Sitan%20Chen%20and%20Vasilis%20Kontonis%20and%20Kulin%20Shah&entry.1292438233=%20%20We%20study%20the%20problem%20of%20learning%20mixtures%20of%20%24k%24%20Gaussians%20in%20%24d%24%20dimensions.%0AWe%20make%20no%20separation%20assumptions%20on%20the%20underlying%20mixture%20components%3A%20we%20only%0Arequire%20that%20the%20covariance%20matrices%20have%20bounded%20condition%20number%20and%20that%20the%0Ameans%20and%20covariances%20lie%20in%20a%20ball%20of%20bounded%20radius.%20We%20give%20an%20algorithm%0Athat%20draws%20%24d%5E%7B%5Cmathrm%7Bpoly%7D%28k/%5Cvarepsilon%29%7D%24%20samples%20from%20the%20target%20mixture%2C%0Aruns%20in%20sample-polynomial%20time%2C%20and%20constructs%20a%20sampler%20whose%20output%0Adistribution%20is%20%24%5Cvarepsilon%24-far%20from%20the%20unknown%20mixture%20in%20total%20variation.%0APrior%20works%20for%20this%20problem%20either%20%28i%29%20required%20exponential%20runtime%20in%20the%0Adimension%20%24d%24%2C%20%28ii%29%20placed%20strong%20assumptions%20on%20the%20instance%20%28e.g.%2C%20spherical%0Acovariances%20or%20clusterability%29%2C%20or%20%28iii%29%20had%20doubly%20exponential%20dependence%20on%0Athe%20number%20of%20components%20%24k%24.%0A%20%20Our%20approach%20departs%20from%20commonly%20used%20techniques%20for%20this%20problem%20like%20the%0Amethod%20of%20moments.%20Instead%2C%20we%20leverage%20a%20recently%20developed%20reduction%2C%20based%0Aon%20diffusion%20models%2C%20from%20distribution%20learning%20to%20a%20supervised%20learning%20task%0Acalled%20score%20matching.%20We%20give%20an%20algorithm%20for%20the%20latter%20by%20proving%20a%0Astructural%20result%20showing%20that%20the%20score%20function%20of%20a%20Gaussian%20mixture%20can%20be%0Aapproximated%20by%20a%20piecewise-polynomial%20function%2C%20and%20there%20is%20an%20efficient%0Aalgorithm%20for%20finding%20it.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20example%20of%0Adiffusion%20models%20achieving%20a%20state-of-the-art%20theoretical%20guarantee%20for%20an%0Aunsupervised%20learning%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18893v1&entry.124074799=Read"},
{"title": "FlexiFilm: Long Video Generation with Flexible Conditions", "author": "Yichen Ouyang and jianhao Yuan and Hao Zhao and Gaoang Wang and Bo zhao", "abstract": "  Generating long and consistent videos has emerged as a significant yet\nchallenging problem. While most existing diffusion-based video generation\nmodels, derived from image generation models, demonstrate promising performance\nin generating short videos, their simple conditioning mechanism and sampling\nstrategy-originally designed for image generation-cause severe performance\ndegradation when adapted to long video generation. This results in prominent\ntemporal inconsistency and overexposure. Thus, in this work, we introduce\nFlexiFilm, a new diffusion model tailored for long video generation. Our\nframework incorporates a temporal conditioner to establish a more consistent\nrelationship between generation and multi-modal conditions, and a resampling\nstrategy to tackle overexposure. Empirical results demonstrate FlexiFilm\ngenerates long and consistent videos, each over 30 seconds in length,\noutperforming competitors in qualitative and quantitative analyses. Project\npage: https://y-ichen.github.io/FlexiFilm-Page/\n", "link": "http://arxiv.org/abs/2404.18620v1", "date": "2024-04-29", "relevancy": 2.3816, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6024}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5988}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5892}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FlexiFilm%3A%20Long%20Video%20Generation%20with%20Flexible%20Conditions&body=Title%3A%20FlexiFilm%3A%20Long%20Video%20Generation%20with%20Flexible%20Conditions%0AAuthor%3A%20Yichen%20Ouyang%20and%20jianhao%20Yuan%20and%20Hao%20Zhao%20and%20Gaoang%20Wang%20and%20Bo%20zhao%0AAbstract%3A%20%20%20Generating%20long%20and%20consistent%20videos%20has%20emerged%20as%20a%20significant%20yet%0Achallenging%20problem.%20While%20most%20existing%20diffusion-based%20video%20generation%0Amodels%2C%20derived%20from%20image%20generation%20models%2C%20demonstrate%20promising%20performance%0Ain%20generating%20short%20videos%2C%20their%20simple%20conditioning%20mechanism%20and%20sampling%0Astrategy-originally%20designed%20for%20image%20generation-cause%20severe%20performance%0Adegradation%20when%20adapted%20to%20long%20video%20generation.%20This%20results%20in%20prominent%0Atemporal%20inconsistency%20and%20overexposure.%20Thus%2C%20in%20this%20work%2C%20we%20introduce%0AFlexiFilm%2C%20a%20new%20diffusion%20model%20tailored%20for%20long%20video%20generation.%20Our%0Aframework%20incorporates%20a%20temporal%20conditioner%20to%20establish%20a%20more%20consistent%0Arelationship%20between%20generation%20and%20multi-modal%20conditions%2C%20and%20a%20resampling%0Astrategy%20to%20tackle%20overexposure.%20Empirical%20results%20demonstrate%20FlexiFilm%0Agenerates%20long%20and%20consistent%20videos%2C%20each%20over%2030%20seconds%20in%20length%2C%0Aoutperforming%20competitors%20in%20qualitative%20and%20quantitative%20analyses.%20Project%0Apage%3A%20https%3A//y-ichen.github.io/FlexiFilm-Page/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18620v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlexiFilm%3A%20Long%20Video%20Generation%20with%20Flexible%20Conditions&entry.906535625=Yichen%20Ouyang%20and%20jianhao%20Yuan%20and%20Hao%20Zhao%20and%20Gaoang%20Wang%20and%20Bo%20zhao&entry.1292438233=%20%20Generating%20long%20and%20consistent%20videos%20has%20emerged%20as%20a%20significant%20yet%0Achallenging%20problem.%20While%20most%20existing%20diffusion-based%20video%20generation%0Amodels%2C%20derived%20from%20image%20generation%20models%2C%20demonstrate%20promising%20performance%0Ain%20generating%20short%20videos%2C%20their%20simple%20conditioning%20mechanism%20and%20sampling%0Astrategy-originally%20designed%20for%20image%20generation-cause%20severe%20performance%0Adegradation%20when%20adapted%20to%20long%20video%20generation.%20This%20results%20in%20prominent%0Atemporal%20inconsistency%20and%20overexposure.%20Thus%2C%20in%20this%20work%2C%20we%20introduce%0AFlexiFilm%2C%20a%20new%20diffusion%20model%20tailored%20for%20long%20video%20generation.%20Our%0Aframework%20incorporates%20a%20temporal%20conditioner%20to%20establish%20a%20more%20consistent%0Arelationship%20between%20generation%20and%20multi-modal%20conditions%2C%20and%20a%20resampling%0Astrategy%20to%20tackle%20overexposure.%20Empirical%20results%20demonstrate%20FlexiFilm%0Agenerates%20long%20and%20consistent%20videos%2C%20each%20over%2030%20seconds%20in%20length%2C%0Aoutperforming%20competitors%20in%20qualitative%20and%20quantitative%20analyses.%20Project%0Apage%3A%20https%3A//y-ichen.github.io/FlexiFilm-Page/%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18620v1&entry.124074799=Read"},
{"title": "Certification of Speaker Recognition Models to Additive Perturbations", "author": "Dmitrii Korzh and Elvir Karimov and Mikhail Pautov and Oleg Y. Rogov and Ivan Oseledets", "abstract": "  Speaker recognition technology is applied in various tasks ranging from\npersonal virtual assistants to secure access systems. However, the robustness\nof these systems against adversarial attacks, particularly to additive\nperturbations, remains a significant challenge. In this paper, we pioneer\napplying robustness certification techniques to speaker recognition, originally\ndeveloped for the image domain. In our work, we cover this gap by transferring\nand improving randomized smoothing certification techniques against\nnorm-bounded additive perturbations for classification and few-shot learning\ntasks to speaker recognition. We demonstrate the effectiveness of these methods\non VoxCeleb 1 and 2 datasets for several models. We expect this work to improve\nvoice-biometry robustness, establish a new certification benchmark, and\naccelerate research of certification methods in the audio domain.\n", "link": "http://arxiv.org/abs/2404.18791v1", "date": "2024-04-29", "relevancy": 2.3802, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4803}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4742}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4737}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Certification%20of%20Speaker%20Recognition%20Models%20to%20Additive%20Perturbations&body=Title%3A%20Certification%20of%20Speaker%20Recognition%20Models%20to%20Additive%20Perturbations%0AAuthor%3A%20Dmitrii%20Korzh%20and%20Elvir%20Karimov%20and%20Mikhail%20Pautov%20and%20Oleg%20Y.%20Rogov%20and%20Ivan%20Oseledets%0AAbstract%3A%20%20%20Speaker%20recognition%20technology%20is%20applied%20in%20various%20tasks%20ranging%20from%0Apersonal%20virtual%20assistants%20to%20secure%20access%20systems.%20However%2C%20the%20robustness%0Aof%20these%20systems%20against%20adversarial%20attacks%2C%20particularly%20to%20additive%0Aperturbations%2C%20remains%20a%20significant%20challenge.%20In%20this%20paper%2C%20we%20pioneer%0Aapplying%20robustness%20certification%20techniques%20to%20speaker%20recognition%2C%20originally%0Adeveloped%20for%20the%20image%20domain.%20In%20our%20work%2C%20we%20cover%20this%20gap%20by%20transferring%0Aand%20improving%20randomized%20smoothing%20certification%20techniques%20against%0Anorm-bounded%20additive%20perturbations%20for%20classification%20and%20few-shot%20learning%0Atasks%20to%20speaker%20recognition.%20We%20demonstrate%20the%20effectiveness%20of%20these%20methods%0Aon%20VoxCeleb%201%20and%202%20datasets%20for%20several%20models.%20We%20expect%20this%20work%20to%20improve%0Avoice-biometry%20robustness%2C%20establish%20a%20new%20certification%20benchmark%2C%20and%0Aaccelerate%20research%20of%20certification%20methods%20in%20the%20audio%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18791v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Certification%20of%20Speaker%20Recognition%20Models%20to%20Additive%20Perturbations&entry.906535625=Dmitrii%20Korzh%20and%20Elvir%20Karimov%20and%20Mikhail%20Pautov%20and%20Oleg%20Y.%20Rogov%20and%20Ivan%20Oseledets&entry.1292438233=%20%20Speaker%20recognition%20technology%20is%20applied%20in%20various%20tasks%20ranging%20from%0Apersonal%20virtual%20assistants%20to%20secure%20access%20systems.%20However%2C%20the%20robustness%0Aof%20these%20systems%20against%20adversarial%20attacks%2C%20particularly%20to%20additive%0Aperturbations%2C%20remains%20a%20significant%20challenge.%20In%20this%20paper%2C%20we%20pioneer%0Aapplying%20robustness%20certification%20techniques%20to%20speaker%20recognition%2C%20originally%0Adeveloped%20for%20the%20image%20domain.%20In%20our%20work%2C%20we%20cover%20this%20gap%20by%20transferring%0Aand%20improving%20randomized%20smoothing%20certification%20techniques%20against%0Anorm-bounded%20additive%20perturbations%20for%20classification%20and%20few-shot%20learning%0Atasks%20to%20speaker%20recognition.%20We%20demonstrate%20the%20effectiveness%20of%20these%20methods%0Aon%20VoxCeleb%201%20and%202%20datasets%20for%20several%20models.%20We%20expect%20this%20work%20to%20improve%0Avoice-biometry%20robustness%2C%20establish%20a%20new%20certification%20benchmark%2C%20and%0Aaccelerate%20research%20of%20certification%20methods%20in%20the%20audio%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18791v1&entry.124074799=Read"},
{"title": "Real-time 3D semantic occupancy prediction for autonomous vehicles using\n  memory-efficient sparse convolution", "author": "Samuel Sze and Lars Kunze", "abstract": "  In autonomous vehicles, understanding the surrounding 3D environment of the\nego vehicle in real-time is essential. A compact way to represent scenes while\nencoding geometric distances and semantic object information is via 3D semantic\noccupancy maps. State of the art 3D mapping methods leverage transformers with\ncross-attention mechanisms to elevate 2D vision-centric camera features into\nthe 3D domain. However, these methods encounter significant challenges in\nreal-time applications due to their high computational demands during\ninference. This limitation is particularly problematic in autonomous vehicles,\nwhere GPU resources must be shared with other tasks such as localization and\nplanning. In this paper, we introduce an approach that extracts features from\nfront-view 2D camera images and LiDAR scans, then employs a sparse convolution\nnetwork (Minkowski Engine), for 3D semantic occupancy prediction. Given that\noutdoor scenes in autonomous driving scenarios are inherently sparse, the\nutilization of sparse convolution is particularly apt. By jointly solving the\nproblems of 3D scene completion of sparse scenes and 3D semantic segmentation,\nwe provide a more efficient learning framework suitable for real-time\napplications in autonomous vehicles. We also demonstrate competitive accuracy\non the nuScenes dataset.\n", "link": "http://arxiv.org/abs/2403.08748v2", "date": "2024-04-29", "relevancy": 2.3754, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.617}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5989}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5795}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Real-time%203D%20semantic%20occupancy%20prediction%20for%20autonomous%20vehicles%20using%0A%20%20memory-efficient%20sparse%20convolution&body=Title%3A%20Real-time%203D%20semantic%20occupancy%20prediction%20for%20autonomous%20vehicles%20using%0A%20%20memory-efficient%20sparse%20convolution%0AAuthor%3A%20Samuel%20Sze%20and%20Lars%20Kunze%0AAbstract%3A%20%20%20In%20autonomous%20vehicles%2C%20understanding%20the%20surrounding%203D%20environment%20of%20the%0Aego%20vehicle%20in%20real-time%20is%20essential.%20A%20compact%20way%20to%20represent%20scenes%20while%0Aencoding%20geometric%20distances%20and%20semantic%20object%20information%20is%20via%203D%20semantic%0Aoccupancy%20maps.%20State%20of%20the%20art%203D%20mapping%20methods%20leverage%20transformers%20with%0Across-attention%20mechanisms%20to%20elevate%202D%20vision-centric%20camera%20features%20into%0Athe%203D%20domain.%20However%2C%20these%20methods%20encounter%20significant%20challenges%20in%0Areal-time%20applications%20due%20to%20their%20high%20computational%20demands%20during%0Ainference.%20This%20limitation%20is%20particularly%20problematic%20in%20autonomous%20vehicles%2C%0Awhere%20GPU%20resources%20must%20be%20shared%20with%20other%20tasks%20such%20as%20localization%20and%0Aplanning.%20In%20this%20paper%2C%20we%20introduce%20an%20approach%20that%20extracts%20features%20from%0Afront-view%202D%20camera%20images%20and%20LiDAR%20scans%2C%20then%20employs%20a%20sparse%20convolution%0Anetwork%20%28Minkowski%20Engine%29%2C%20for%203D%20semantic%20occupancy%20prediction.%20Given%20that%0Aoutdoor%20scenes%20in%20autonomous%20driving%20scenarios%20are%20inherently%20sparse%2C%20the%0Autilization%20of%20sparse%20convolution%20is%20particularly%20apt.%20By%20jointly%20solving%20the%0Aproblems%20of%203D%20scene%20completion%20of%20sparse%20scenes%20and%203D%20semantic%20segmentation%2C%0Awe%20provide%20a%20more%20efficient%20learning%20framework%20suitable%20for%20real-time%0Aapplications%20in%20autonomous%20vehicles.%20We%20also%20demonstrate%20competitive%20accuracy%0Aon%20the%20nuScenes%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08748v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real-time%203D%20semantic%20occupancy%20prediction%20for%20autonomous%20vehicles%20using%0A%20%20memory-efficient%20sparse%20convolution&entry.906535625=Samuel%20Sze%20and%20Lars%20Kunze&entry.1292438233=%20%20In%20autonomous%20vehicles%2C%20understanding%20the%20surrounding%203D%20environment%20of%20the%0Aego%20vehicle%20in%20real-time%20is%20essential.%20A%20compact%20way%20to%20represent%20scenes%20while%0Aencoding%20geometric%20distances%20and%20semantic%20object%20information%20is%20via%203D%20semantic%0Aoccupancy%20maps.%20State%20of%20the%20art%203D%20mapping%20methods%20leverage%20transformers%20with%0Across-attention%20mechanisms%20to%20elevate%202D%20vision-centric%20camera%20features%20into%0Athe%203D%20domain.%20However%2C%20these%20methods%20encounter%20significant%20challenges%20in%0Areal-time%20applications%20due%20to%20their%20high%20computational%20demands%20during%0Ainference.%20This%20limitation%20is%20particularly%20problematic%20in%20autonomous%20vehicles%2C%0Awhere%20GPU%20resources%20must%20be%20shared%20with%20other%20tasks%20such%20as%20localization%20and%0Aplanning.%20In%20this%20paper%2C%20we%20introduce%20an%20approach%20that%20extracts%20features%20from%0Afront-view%202D%20camera%20images%20and%20LiDAR%20scans%2C%20then%20employs%20a%20sparse%20convolution%0Anetwork%20%28Minkowski%20Engine%29%2C%20for%203D%20semantic%20occupancy%20prediction.%20Given%20that%0Aoutdoor%20scenes%20in%20autonomous%20driving%20scenarios%20are%20inherently%20sparse%2C%20the%0Autilization%20of%20sparse%20convolution%20is%20particularly%20apt.%20By%20jointly%20solving%20the%0Aproblems%20of%203D%20scene%20completion%20of%20sparse%20scenes%20and%203D%20semantic%20segmentation%2C%0Awe%20provide%20a%20more%20efficient%20learning%20framework%20suitable%20for%20real-time%0Aapplications%20in%20autonomous%20vehicles.%20We%20also%20demonstrate%20competitive%20accuracy%0Aon%20the%20nuScenes%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08748v2&entry.124074799=Read"},
{"title": "Towards Stable Machine Learning Model Retraining via Slowly Varying\n  Sequences", "author": "Dimitris Bertsimas and Vassilis Digalakis Jr and Yu Ma and Phevos Paschalidis", "abstract": "  Retraining machine learning models (ML) when new batches of data become\navailable is an important task in real-world pipelines. Existing methods focus\nlargely on greedy approaches to find the best-performing model for each batch,\nwithout considering the stability of the model's structure across retraining\niterations. In this study, we propose a methodology for finding sequences of ML\nmodels that are stable across retraining iterations. We develop a mixed-integer\noptimization algorithm that is guaranteed to recover Pareto optimal models (in\nterms of the predictive power-stability trade-off) and an efficient\npolynomial-time algorithm that performs well in practice. Our method focuses on\nretaining consistent analytical insights -- which is important to model\ninterpretability, ease of implementation, and fostering trust with users -- by\nusing custom-defined distance metrics that can be directly incorporated into\nthe optimization problem. Importantly, our method shows stronger stability than\ngreedily trained models with a small, controllable sacrifice in model\nperformance in a real-world case study. Using SHAP feature importance, we show\nthat analytical insights are consistent across retraining iterations.\n", "link": "http://arxiv.org/abs/2403.19871v3", "date": "2024-04-29", "relevancy": 2.3549, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4793}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4677}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4659}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Stable%20Machine%20Learning%20Model%20Retraining%20via%20Slowly%20Varying%0A%20%20Sequences&body=Title%3A%20Towards%20Stable%20Machine%20Learning%20Model%20Retraining%20via%20Slowly%20Varying%0A%20%20Sequences%0AAuthor%3A%20Dimitris%20Bertsimas%20and%20Vassilis%20Digalakis%20Jr%20and%20Yu%20Ma%20and%20Phevos%20Paschalidis%0AAbstract%3A%20%20%20Retraining%20machine%20learning%20models%20%28ML%29%20when%20new%20batches%20of%20data%20become%0Aavailable%20is%20an%20important%20task%20in%20real-world%20pipelines.%20Existing%20methods%20focus%0Alargely%20on%20greedy%20approaches%20to%20find%20the%20best-performing%20model%20for%20each%20batch%2C%0Awithout%20considering%20the%20stability%20of%20the%20model%27s%20structure%20across%20retraining%0Aiterations.%20In%20this%20study%2C%20we%20propose%20a%20methodology%20for%20finding%20sequences%20of%20ML%0Amodels%20that%20are%20stable%20across%20retraining%20iterations.%20We%20develop%20a%20mixed-integer%0Aoptimization%20algorithm%20that%20is%20guaranteed%20to%20recover%20Pareto%20optimal%20models%20%28in%0Aterms%20of%20the%20predictive%20power-stability%20trade-off%29%20and%20an%20efficient%0Apolynomial-time%20algorithm%20that%20performs%20well%20in%20practice.%20Our%20method%20focuses%20on%0Aretaining%20consistent%20analytical%20insights%20--%20which%20is%20important%20to%20model%0Ainterpretability%2C%20ease%20of%20implementation%2C%20and%20fostering%20trust%20with%20users%20--%20by%0Ausing%20custom-defined%20distance%20metrics%20that%20can%20be%20directly%20incorporated%20into%0Athe%20optimization%20problem.%20Importantly%2C%20our%20method%20shows%20stronger%20stability%20than%0Agreedily%20trained%20models%20with%20a%20small%2C%20controllable%20sacrifice%20in%20model%0Aperformance%20in%20a%20real-world%20case%20study.%20Using%20SHAP%20feature%20importance%2C%20we%20show%0Athat%20analytical%20insights%20are%20consistent%20across%20retraining%20iterations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19871v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Stable%20Machine%20Learning%20Model%20Retraining%20via%20Slowly%20Varying%0A%20%20Sequences&entry.906535625=Dimitris%20Bertsimas%20and%20Vassilis%20Digalakis%20Jr%20and%20Yu%20Ma%20and%20Phevos%20Paschalidis&entry.1292438233=%20%20Retraining%20machine%20learning%20models%20%28ML%29%20when%20new%20batches%20of%20data%20become%0Aavailable%20is%20an%20important%20task%20in%20real-world%20pipelines.%20Existing%20methods%20focus%0Alargely%20on%20greedy%20approaches%20to%20find%20the%20best-performing%20model%20for%20each%20batch%2C%0Awithout%20considering%20the%20stability%20of%20the%20model%27s%20structure%20across%20retraining%0Aiterations.%20In%20this%20study%2C%20we%20propose%20a%20methodology%20for%20finding%20sequences%20of%20ML%0Amodels%20that%20are%20stable%20across%20retraining%20iterations.%20We%20develop%20a%20mixed-integer%0Aoptimization%20algorithm%20that%20is%20guaranteed%20to%20recover%20Pareto%20optimal%20models%20%28in%0Aterms%20of%20the%20predictive%20power-stability%20trade-off%29%20and%20an%20efficient%0Apolynomial-time%20algorithm%20that%20performs%20well%20in%20practice.%20Our%20method%20focuses%20on%0Aretaining%20consistent%20analytical%20insights%20--%20which%20is%20important%20to%20model%0Ainterpretability%2C%20ease%20of%20implementation%2C%20and%20fostering%20trust%20with%20users%20--%20by%0Ausing%20custom-defined%20distance%20metrics%20that%20can%20be%20directly%20incorporated%20into%0Athe%20optimization%20problem.%20Importantly%2C%20our%20method%20shows%20stronger%20stability%20than%0Agreedily%20trained%20models%20with%20a%20small%2C%20controllable%20sacrifice%20in%20model%0Aperformance%20in%20a%20real-world%20case%20study.%20Using%20SHAP%20feature%20importance%2C%20we%20show%0Athat%20analytical%20insights%20are%20consistent%20across%20retraining%20iterations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19871v3&entry.124074799=Read"},
{"title": "Enhancing Prosthetic Safety and Environmental Adaptability: A\n  Visual-Inertial Prosthesis Motion Estimation Approach on Uneven Terrains", "author": "Chuheng Chen and Xinxing Chen and Shucong Yin and Yuxuan Wang and Binxin Huang and Yuquan Leng and Chenglong Fu", "abstract": "  Environment awareness is crucial for enhancing walking safety and stability\nof amputee wearing powered prosthesis when crossing uneven terrains such as\nstairs and obstacles. However, existing environmental perception systems for\nprosthesis only provide terrain types and corresponding parameters, which fails\nto prevent potential collisions when crossing uneven terrains and may lead to\nfalls and other severe consequences. In this paper, a visual-inertial motion\nestimation approach is proposed for prosthesis to perceive its movement and the\nchanges of spatial relationship between the prosthesis and uneven terrain when\ntraversing them. To achieve this, we estimate the knee motion by utilizing a\ndepth camera to perceive the environment and align feature points extracted\nfrom stairs and obstacles. Subsequently, an error-state Kalman filter is\nincorporated to fuse the inertial data into visual estimations to reduce the\nfeature extraction error and obtain a more robust estimation. The motion of\nprosthetic joint and toe are derived using the prosthesis model parameters.\nExperiment conducted on our collected dataset and stair walking trials with a\npowered prosthesis shows that the proposed method can accurately tracking the\nmotion of the human leg and prosthesis with an average root-mean-square error\nof toe trajectory less than 5 cm. The proposed method is expected to enable the\nenvironmental adaptive control for prosthesis, thereby enhancing amputee's\nsafety and mobility in uneven terrains.\n", "link": "http://arxiv.org/abs/2404.18612v1", "date": "2024-04-29", "relevancy": 2.3427, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6441}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6232}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5248}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Prosthetic%20Safety%20and%20Environmental%20Adaptability%3A%20A%0A%20%20Visual-Inertial%20Prosthesis%20Motion%20Estimation%20Approach%20on%20Uneven%20Terrains&body=Title%3A%20Enhancing%20Prosthetic%20Safety%20and%20Environmental%20Adaptability%3A%20A%0A%20%20Visual-Inertial%20Prosthesis%20Motion%20Estimation%20Approach%20on%20Uneven%20Terrains%0AAuthor%3A%20Chuheng%20Chen%20and%20Xinxing%20Chen%20and%20Shucong%20Yin%20and%20Yuxuan%20Wang%20and%20Binxin%20Huang%20and%20Yuquan%20Leng%20and%20Chenglong%20Fu%0AAbstract%3A%20%20%20Environment%20awareness%20is%20crucial%20for%20enhancing%20walking%20safety%20and%20stability%0Aof%20amputee%20wearing%20powered%20prosthesis%20when%20crossing%20uneven%20terrains%20such%20as%0Astairs%20and%20obstacles.%20However%2C%20existing%20environmental%20perception%20systems%20for%0Aprosthesis%20only%20provide%20terrain%20types%20and%20corresponding%20parameters%2C%20which%20fails%0Ato%20prevent%20potential%20collisions%20when%20crossing%20uneven%20terrains%20and%20may%20lead%20to%0Afalls%20and%20other%20severe%20consequences.%20In%20this%20paper%2C%20a%20visual-inertial%20motion%0Aestimation%20approach%20is%20proposed%20for%20prosthesis%20to%20perceive%20its%20movement%20and%20the%0Achanges%20of%20spatial%20relationship%20between%20the%20prosthesis%20and%20uneven%20terrain%20when%0Atraversing%20them.%20To%20achieve%20this%2C%20we%20estimate%20the%20knee%20motion%20by%20utilizing%20a%0Adepth%20camera%20to%20perceive%20the%20environment%20and%20align%20feature%20points%20extracted%0Afrom%20stairs%20and%20obstacles.%20Subsequently%2C%20an%20error-state%20Kalman%20filter%20is%0Aincorporated%20to%20fuse%20the%20inertial%20data%20into%20visual%20estimations%20to%20reduce%20the%0Afeature%20extraction%20error%20and%20obtain%20a%20more%20robust%20estimation.%20The%20motion%20of%0Aprosthetic%20joint%20and%20toe%20are%20derived%20using%20the%20prosthesis%20model%20parameters.%0AExperiment%20conducted%20on%20our%20collected%20dataset%20and%20stair%20walking%20trials%20with%20a%0Apowered%20prosthesis%20shows%20that%20the%20proposed%20method%20can%20accurately%20tracking%20the%0Amotion%20of%20the%20human%20leg%20and%20prosthesis%20with%20an%20average%20root-mean-square%20error%0Aof%20toe%20trajectory%20less%20than%205%20cm.%20The%20proposed%20method%20is%20expected%20to%20enable%20the%0Aenvironmental%20adaptive%20control%20for%20prosthesis%2C%20thereby%20enhancing%20amputee%27s%0Asafety%20and%20mobility%20in%20uneven%20terrains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18612v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Prosthetic%20Safety%20and%20Environmental%20Adaptability%3A%20A%0A%20%20Visual-Inertial%20Prosthesis%20Motion%20Estimation%20Approach%20on%20Uneven%20Terrains&entry.906535625=Chuheng%20Chen%20and%20Xinxing%20Chen%20and%20Shucong%20Yin%20and%20Yuxuan%20Wang%20and%20Binxin%20Huang%20and%20Yuquan%20Leng%20and%20Chenglong%20Fu&entry.1292438233=%20%20Environment%20awareness%20is%20crucial%20for%20enhancing%20walking%20safety%20and%20stability%0Aof%20amputee%20wearing%20powered%20prosthesis%20when%20crossing%20uneven%20terrains%20such%20as%0Astairs%20and%20obstacles.%20However%2C%20existing%20environmental%20perception%20systems%20for%0Aprosthesis%20only%20provide%20terrain%20types%20and%20corresponding%20parameters%2C%20which%20fails%0Ato%20prevent%20potential%20collisions%20when%20crossing%20uneven%20terrains%20and%20may%20lead%20to%0Afalls%20and%20other%20severe%20consequences.%20In%20this%20paper%2C%20a%20visual-inertial%20motion%0Aestimation%20approach%20is%20proposed%20for%20prosthesis%20to%20perceive%20its%20movement%20and%20the%0Achanges%20of%20spatial%20relationship%20between%20the%20prosthesis%20and%20uneven%20terrain%20when%0Atraversing%20them.%20To%20achieve%20this%2C%20we%20estimate%20the%20knee%20motion%20by%20utilizing%20a%0Adepth%20camera%20to%20perceive%20the%20environment%20and%20align%20feature%20points%20extracted%0Afrom%20stairs%20and%20obstacles.%20Subsequently%2C%20an%20error-state%20Kalman%20filter%20is%0Aincorporated%20to%20fuse%20the%20inertial%20data%20into%20visual%20estimations%20to%20reduce%20the%0Afeature%20extraction%20error%20and%20obtain%20a%20more%20robust%20estimation.%20The%20motion%20of%0Aprosthetic%20joint%20and%20toe%20are%20derived%20using%20the%20prosthesis%20model%20parameters.%0AExperiment%20conducted%20on%20our%20collected%20dataset%20and%20stair%20walking%20trials%20with%20a%0Apowered%20prosthesis%20shows%20that%20the%20proposed%20method%20can%20accurately%20tracking%20the%0Amotion%20of%20the%20human%20leg%20and%20prosthesis%20with%20an%20average%20root-mean-square%20error%0Aof%20toe%20trajectory%20less%20than%205%20cm.%20The%20proposed%20method%20is%20expected%20to%20enable%20the%0Aenvironmental%20adaptive%20control%20for%20prosthesis%2C%20thereby%20enhancing%20amputee%27s%0Asafety%20and%20mobility%20in%20uneven%20terrains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18612v1&entry.124074799=Read"},
{"title": "VIO-DualProNet: Visual-Inertial Odometry with Learning Based Process\n  Noise Covariance", "author": "Dan Solodar and Itzik Klein", "abstract": "  Visual-inertial odometry (VIO) is a vital technique used in robotics,\naugmented reality, and autonomous vehicles. It combines visual and inertial\nmeasurements to accurately estimate position and orientation. Existing VIO\nmethods assume a fixed noise covariance for the inertial uncertainty. However,\naccurately determining in real-time the noise variance of the inertial sensors\npresents a significant challenge as the uncertainty changes throughout the\noperation leading to suboptimal performance and reduced accuracy. To circumvent\nthis, we propose VIO-DualProNet, a novel approach that utilizes deep learning\nmethods to dynamically estimate the inertial noise uncertainty in real-time. By\ndesigning and training a deep neural network to predict inertial noise\nuncertainty using only inertial sensor measurements, and integrating it into\nthe VINS-Mono algorithm, we demonstrate a substantial improvement in accuracy\nand robustness, enhancing VIO performance and potentially benefiting other\nVIO-based systems for precise localization and mapping across diverse\nconditions.\n", "link": "http://arxiv.org/abs/2308.11228v2", "date": "2024-04-29", "relevancy": 2.309, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6102}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5898}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5515}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VIO-DualProNet%3A%20Visual-Inertial%20Odometry%20with%20Learning%20Based%20Process%0A%20%20Noise%20Covariance&body=Title%3A%20VIO-DualProNet%3A%20Visual-Inertial%20Odometry%20with%20Learning%20Based%20Process%0A%20%20Noise%20Covariance%0AAuthor%3A%20Dan%20Solodar%20and%20Itzik%20Klein%0AAbstract%3A%20%20%20Visual-inertial%20odometry%20%28VIO%29%20is%20a%20vital%20technique%20used%20in%20robotics%2C%0Aaugmented%20reality%2C%20and%20autonomous%20vehicles.%20It%20combines%20visual%20and%20inertial%0Ameasurements%20to%20accurately%20estimate%20position%20and%20orientation.%20Existing%20VIO%0Amethods%20assume%20a%20fixed%20noise%20covariance%20for%20the%20inertial%20uncertainty.%20However%2C%0Aaccurately%20determining%20in%20real-time%20the%20noise%20variance%20of%20the%20inertial%20sensors%0Apresents%20a%20significant%20challenge%20as%20the%20uncertainty%20changes%20throughout%20the%0Aoperation%20leading%20to%20suboptimal%20performance%20and%20reduced%20accuracy.%20To%20circumvent%0Athis%2C%20we%20propose%20VIO-DualProNet%2C%20a%20novel%20approach%20that%20utilizes%20deep%20learning%0Amethods%20to%20dynamically%20estimate%20the%20inertial%20noise%20uncertainty%20in%20real-time.%20By%0Adesigning%20and%20training%20a%20deep%20neural%20network%20to%20predict%20inertial%20noise%0Auncertainty%20using%20only%20inertial%20sensor%20measurements%2C%20and%20integrating%20it%20into%0Athe%20VINS-Mono%20algorithm%2C%20we%20demonstrate%20a%20substantial%20improvement%20in%20accuracy%0Aand%20robustness%2C%20enhancing%20VIO%20performance%20and%20potentially%20benefiting%20other%0AVIO-based%20systems%20for%20precise%20localization%20and%20mapping%20across%20diverse%0Aconditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.11228v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIO-DualProNet%3A%20Visual-Inertial%20Odometry%20with%20Learning%20Based%20Process%0A%20%20Noise%20Covariance&entry.906535625=Dan%20Solodar%20and%20Itzik%20Klein&entry.1292438233=%20%20Visual-inertial%20odometry%20%28VIO%29%20is%20a%20vital%20technique%20used%20in%20robotics%2C%0Aaugmented%20reality%2C%20and%20autonomous%20vehicles.%20It%20combines%20visual%20and%20inertial%0Ameasurements%20to%20accurately%20estimate%20position%20and%20orientation.%20Existing%20VIO%0Amethods%20assume%20a%20fixed%20noise%20covariance%20for%20the%20inertial%20uncertainty.%20However%2C%0Aaccurately%20determining%20in%20real-time%20the%20noise%20variance%20of%20the%20inertial%20sensors%0Apresents%20a%20significant%20challenge%20as%20the%20uncertainty%20changes%20throughout%20the%0Aoperation%20leading%20to%20suboptimal%20performance%20and%20reduced%20accuracy.%20To%20circumvent%0Athis%2C%20we%20propose%20VIO-DualProNet%2C%20a%20novel%20approach%20that%20utilizes%20deep%20learning%0Amethods%20to%20dynamically%20estimate%20the%20inertial%20noise%20uncertainty%20in%20real-time.%20By%0Adesigning%20and%20training%20a%20deep%20neural%20network%20to%20predict%20inertial%20noise%0Auncertainty%20using%20only%20inertial%20sensor%20measurements%2C%20and%20integrating%20it%20into%0Athe%20VINS-Mono%20algorithm%2C%20we%20demonstrate%20a%20substantial%20improvement%20in%20accuracy%0Aand%20robustness%2C%20enhancing%20VIO%20performance%20and%20potentially%20benefiting%20other%0AVIO-based%20systems%20for%20precise%20localization%20and%20mapping%20across%20diverse%0Aconditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.11228v2&entry.124074799=Read"},
{"title": "TheaterGen: Character Management with LLM for Consistent Multi-turn\n  Image Generation", "author": "Junhao Cheng and Baiqiao Yin and Kaixin Cai and Minbin Huang and Hanhui Li and Yuxin He and Xi Lu and Yue Li and Yifei Li and Yuhao Cheng and Yiqiang Yan and Xiaodan Liang", "abstract": "  Recent advances in diffusion models can generate high-quality and stunning\nimages from text. However, multi-turn image generation, which is of high demand\nin real-world scenarios, still faces challenges in maintaining semantic\nconsistency between images and texts, as well as contextual consistency of the\nsame subject across multiple interactive turns. To address this issue, we\nintroduce TheaterGen, a training-free framework that integrates large language\nmodels (LLMs) and text-to-image (T2I) models to provide the capability of\nmulti-turn image generation. Within this framework, LLMs, acting as a\n\"Screenwriter\", engage in multi-turn interaction, generating and managing a\nstandardized prompt book that encompasses prompts and layout designs for each\ncharacter in the target image. Based on these, Theatergen generate a list of\ncharacter images and extract guidance information, akin to the \"Rehearsal\".\nSubsequently, through incorporating the prompt book and guidance information\ninto the reverse denoising process of T2I diffusion models, Theatergen generate\nthe final image, as conducting the \"Final Performance\". With the effective\nmanagement of prompt books and character images, TheaterGen significantly\nimproves semantic and contextual consistency in synthesized images.\nFurthermore, we introduce a dedicated benchmark, CMIGBench (Consistent\nMulti-turn Image Generation Benchmark) with 8000 multi-turn instructions.\nDifferent from previous multi-turn benchmarks, CMIGBench does not define\ncharacters in advance. Both the tasks of story generation and multi-turn\nediting are included on CMIGBench for comprehensive evaluation. Extensive\nexperimental results show that TheaterGen outperforms state-of-the-art methods\nsignificantly. It raises the performance bar of the cutting-edge Mini DALLE 3\nmodel by 21% in average character-character similarity and 19% in average\ntext-image similarity.\n", "link": "http://arxiv.org/abs/2404.18919v1", "date": "2024-04-29", "relevancy": 2.2975, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5881}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5756}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.537}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TheaterGen%3A%20Character%20Management%20with%20LLM%20for%20Consistent%20Multi-turn%0A%20%20Image%20Generation&body=Title%3A%20TheaterGen%3A%20Character%20Management%20with%20LLM%20for%20Consistent%20Multi-turn%0A%20%20Image%20Generation%0AAuthor%3A%20Junhao%20Cheng%20and%20Baiqiao%20Yin%20and%20Kaixin%20Cai%20and%20Minbin%20Huang%20and%20Hanhui%20Li%20and%20Yuxin%20He%20and%20Xi%20Lu%20and%20Yue%20Li%20and%20Yifei%20Li%20and%20Yuhao%20Cheng%20and%20Yiqiang%20Yan%20and%20Xiaodan%20Liang%0AAbstract%3A%20%20%20Recent%20advances%20in%20diffusion%20models%20can%20generate%20high-quality%20and%20stunning%0Aimages%20from%20text.%20However%2C%20multi-turn%20image%20generation%2C%20which%20is%20of%20high%20demand%0Ain%20real-world%20scenarios%2C%20still%20faces%20challenges%20in%20maintaining%20semantic%0Aconsistency%20between%20images%20and%20texts%2C%20as%20well%20as%20contextual%20consistency%20of%20the%0Asame%20subject%20across%20multiple%20interactive%20turns.%20To%20address%20this%20issue%2C%20we%0Aintroduce%20TheaterGen%2C%20a%20training-free%20framework%20that%20integrates%20large%20language%0Amodels%20%28LLMs%29%20and%20text-to-image%20%28T2I%29%20models%20to%20provide%20the%20capability%20of%0Amulti-turn%20image%20generation.%20Within%20this%20framework%2C%20LLMs%2C%20acting%20as%20a%0A%22Screenwriter%22%2C%20engage%20in%20multi-turn%20interaction%2C%20generating%20and%20managing%20a%0Astandardized%20prompt%20book%20that%20encompasses%20prompts%20and%20layout%20designs%20for%20each%0Acharacter%20in%20the%20target%20image.%20Based%20on%20these%2C%20Theatergen%20generate%20a%20list%20of%0Acharacter%20images%20and%20extract%20guidance%20information%2C%20akin%20to%20the%20%22Rehearsal%22.%0ASubsequently%2C%20through%20incorporating%20the%20prompt%20book%20and%20guidance%20information%0Ainto%20the%20reverse%20denoising%20process%20of%20T2I%20diffusion%20models%2C%20Theatergen%20generate%0Athe%20final%20image%2C%20as%20conducting%20the%20%22Final%20Performance%22.%20With%20the%20effective%0Amanagement%20of%20prompt%20books%20and%20character%20images%2C%20TheaterGen%20significantly%0Aimproves%20semantic%20and%20contextual%20consistency%20in%20synthesized%20images.%0AFurthermore%2C%20we%20introduce%20a%20dedicated%20benchmark%2C%20CMIGBench%20%28Consistent%0AMulti-turn%20Image%20Generation%20Benchmark%29%20with%208000%20multi-turn%20instructions.%0ADifferent%20from%20previous%20multi-turn%20benchmarks%2C%20CMIGBench%20does%20not%20define%0Acharacters%20in%20advance.%20Both%20the%20tasks%20of%20story%20generation%20and%20multi-turn%0Aediting%20are%20included%20on%20CMIGBench%20for%20comprehensive%20evaluation.%20Extensive%0Aexperimental%20results%20show%20that%20TheaterGen%20outperforms%20state-of-the-art%20methods%0Asignificantly.%20It%20raises%20the%20performance%20bar%20of%20the%20cutting-edge%20Mini%20DALLE%203%0Amodel%20by%2021%25%20in%20average%20character-character%20similarity%20and%2019%25%20in%20average%0Atext-image%20similarity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18919v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TheaterGen%3A%20Character%20Management%20with%20LLM%20for%20Consistent%20Multi-turn%0A%20%20Image%20Generation&entry.906535625=Junhao%20Cheng%20and%20Baiqiao%20Yin%20and%20Kaixin%20Cai%20and%20Minbin%20Huang%20and%20Hanhui%20Li%20and%20Yuxin%20He%20and%20Xi%20Lu%20and%20Yue%20Li%20and%20Yifei%20Li%20and%20Yuhao%20Cheng%20and%20Yiqiang%20Yan%20and%20Xiaodan%20Liang&entry.1292438233=%20%20Recent%20advances%20in%20diffusion%20models%20can%20generate%20high-quality%20and%20stunning%0Aimages%20from%20text.%20However%2C%20multi-turn%20image%20generation%2C%20which%20is%20of%20high%20demand%0Ain%20real-world%20scenarios%2C%20still%20faces%20challenges%20in%20maintaining%20semantic%0Aconsistency%20between%20images%20and%20texts%2C%20as%20well%20as%20contextual%20consistency%20of%20the%0Asame%20subject%20across%20multiple%20interactive%20turns.%20To%20address%20this%20issue%2C%20we%0Aintroduce%20TheaterGen%2C%20a%20training-free%20framework%20that%20integrates%20large%20language%0Amodels%20%28LLMs%29%20and%20text-to-image%20%28T2I%29%20models%20to%20provide%20the%20capability%20of%0Amulti-turn%20image%20generation.%20Within%20this%20framework%2C%20LLMs%2C%20acting%20as%20a%0A%22Screenwriter%22%2C%20engage%20in%20multi-turn%20interaction%2C%20generating%20and%20managing%20a%0Astandardized%20prompt%20book%20that%20encompasses%20prompts%20and%20layout%20designs%20for%20each%0Acharacter%20in%20the%20target%20image.%20Based%20on%20these%2C%20Theatergen%20generate%20a%20list%20of%0Acharacter%20images%20and%20extract%20guidance%20information%2C%20akin%20to%20the%20%22Rehearsal%22.%0ASubsequently%2C%20through%20incorporating%20the%20prompt%20book%20and%20guidance%20information%0Ainto%20the%20reverse%20denoising%20process%20of%20T2I%20diffusion%20models%2C%20Theatergen%20generate%0Athe%20final%20image%2C%20as%20conducting%20the%20%22Final%20Performance%22.%20With%20the%20effective%0Amanagement%20of%20prompt%20books%20and%20character%20images%2C%20TheaterGen%20significantly%0Aimproves%20semantic%20and%20contextual%20consistency%20in%20synthesized%20images.%0AFurthermore%2C%20we%20introduce%20a%20dedicated%20benchmark%2C%20CMIGBench%20%28Consistent%0AMulti-turn%20Image%20Generation%20Benchmark%29%20with%208000%20multi-turn%20instructions.%0ADifferent%20from%20previous%20multi-turn%20benchmarks%2C%20CMIGBench%20does%20not%20define%0Acharacters%20in%20advance.%20Both%20the%20tasks%20of%20story%20generation%20and%20multi-turn%0Aediting%20are%20included%20on%20CMIGBench%20for%20comprehensive%20evaluation.%20Extensive%0Aexperimental%20results%20show%20that%20TheaterGen%20outperforms%20state-of-the-art%20methods%0Asignificantly.%20It%20raises%20the%20performance%20bar%20of%20the%20cutting-edge%20Mini%20DALLE%203%0Amodel%20by%2021%25%20in%20average%20character-character%20similarity%20and%2019%25%20in%20average%0Atext-image%20similarity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18919v1&entry.124074799=Read"},
{"title": "CoSense3D: an Agent-based Efficient Learning Framework for Collective\n  Perception", "author": "Yunshuang Yuan and Monika Sester", "abstract": "  Collective Perception has attracted significant attention in recent years due\nto its advantage for mitigating occlusion and expanding the field-of-view,\nthereby enhancing reliability, efficiency, and, most crucially, decision-making\nsafety. However, developing collective perception models is highly resource\ndemanding due to extensive requirements of processing input data for many\nagents, usually dozens of images and point clouds for a single frame. This not\nonly slows down the model development process for collective perception but\nalso impedes the utilization of larger models. In this paper, we propose an\nagent-based training framework that handles the deep learning modules and agent\ndata separately to have a cleaner data flow structure. This framework not only\nprovides an API for flexibly prototyping the data processing pipeline and\ndefining the gradient calculation for each agent, but also provides the user\ninterface for interactive training, testing and data visualization. Training\nexperiment results of four collective object detection models on the prominent\ncollective perception benchmark OPV2V show that the agent-based training can\nsignificantly reduce the GPU memory consumption and training time while\nretaining inference performance. The framework and model implementations are\navailable at \\url{https://github.com/YuanYunshuang/CoSense3D}\n", "link": "http://arxiv.org/abs/2404.18617v1", "date": "2024-04-29", "relevancy": 2.2963, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5806}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5773}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5662}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CoSense3D%3A%20an%20Agent-based%20Efficient%20Learning%20Framework%20for%20Collective%0A%20%20Perception&body=Title%3A%20CoSense3D%3A%20an%20Agent-based%20Efficient%20Learning%20Framework%20for%20Collective%0A%20%20Perception%0AAuthor%3A%20Yunshuang%20Yuan%20and%20Monika%20Sester%0AAbstract%3A%20%20%20Collective%20Perception%20has%20attracted%20significant%20attention%20in%20recent%20years%20due%0Ato%20its%20advantage%20for%20mitigating%20occlusion%20and%20expanding%20the%20field-of-view%2C%0Athereby%20enhancing%20reliability%2C%20efficiency%2C%20and%2C%20most%20crucially%2C%20decision-making%0Asafety.%20However%2C%20developing%20collective%20perception%20models%20is%20highly%20resource%0Ademanding%20due%20to%20extensive%20requirements%20of%20processing%20input%20data%20for%20many%0Aagents%2C%20usually%20dozens%20of%20images%20and%20point%20clouds%20for%20a%20single%20frame.%20This%20not%0Aonly%20slows%20down%20the%20model%20development%20process%20for%20collective%20perception%20but%0Aalso%20impedes%20the%20utilization%20of%20larger%20models.%20In%20this%20paper%2C%20we%20propose%20an%0Aagent-based%20training%20framework%20that%20handles%20the%20deep%20learning%20modules%20and%20agent%0Adata%20separately%20to%20have%20a%20cleaner%20data%20flow%20structure.%20This%20framework%20not%20only%0Aprovides%20an%20API%20for%20flexibly%20prototyping%20the%20data%20processing%20pipeline%20and%0Adefining%20the%20gradient%20calculation%20for%20each%20agent%2C%20but%20also%20provides%20the%20user%0Ainterface%20for%20interactive%20training%2C%20testing%20and%20data%20visualization.%20Training%0Aexperiment%20results%20of%20four%20collective%20object%20detection%20models%20on%20the%20prominent%0Acollective%20perception%20benchmark%20OPV2V%20show%20that%20the%20agent-based%20training%20can%0Asignificantly%20reduce%20the%20GPU%20memory%20consumption%20and%20training%20time%20while%0Aretaining%20inference%20performance.%20The%20framework%20and%20model%20implementations%20are%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/YuanYunshuang/CoSense3D%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18617v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoSense3D%3A%20an%20Agent-based%20Efficient%20Learning%20Framework%20for%20Collective%0A%20%20Perception&entry.906535625=Yunshuang%20Yuan%20and%20Monika%20Sester&entry.1292438233=%20%20Collective%20Perception%20has%20attracted%20significant%20attention%20in%20recent%20years%20due%0Ato%20its%20advantage%20for%20mitigating%20occlusion%20and%20expanding%20the%20field-of-view%2C%0Athereby%20enhancing%20reliability%2C%20efficiency%2C%20and%2C%20most%20crucially%2C%20decision-making%0Asafety.%20However%2C%20developing%20collective%20perception%20models%20is%20highly%20resource%0Ademanding%20due%20to%20extensive%20requirements%20of%20processing%20input%20data%20for%20many%0Aagents%2C%20usually%20dozens%20of%20images%20and%20point%20clouds%20for%20a%20single%20frame.%20This%20not%0Aonly%20slows%20down%20the%20model%20development%20process%20for%20collective%20perception%20but%0Aalso%20impedes%20the%20utilization%20of%20larger%20models.%20In%20this%20paper%2C%20we%20propose%20an%0Aagent-based%20training%20framework%20that%20handles%20the%20deep%20learning%20modules%20and%20agent%0Adata%20separately%20to%20have%20a%20cleaner%20data%20flow%20structure.%20This%20framework%20not%20only%0Aprovides%20an%20API%20for%20flexibly%20prototyping%20the%20data%20processing%20pipeline%20and%0Adefining%20the%20gradient%20calculation%20for%20each%20agent%2C%20but%20also%20provides%20the%20user%0Ainterface%20for%20interactive%20training%2C%20testing%20and%20data%20visualization.%20Training%0Aexperiment%20results%20of%20four%20collective%20object%20detection%20models%20on%20the%20prominent%0Acollective%20perception%20benchmark%20OPV2V%20show%20that%20the%20agent-based%20training%20can%0Asignificantly%20reduce%20the%20GPU%20memory%20consumption%20and%20training%20time%20while%0Aretaining%20inference%20performance.%20The%20framework%20and%20model%20implementations%20are%0Aavailable%20at%20%5Curl%7Bhttps%3A//github.com/YuanYunshuang/CoSense3D%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18617v1&entry.124074799=Read"},
{"title": "Uncertainty-boosted Robust Video Activity Anticipation", "author": "Zhaobo Qi and Shuhui Wang and Weigang Zhang and Qingming Huang", "abstract": "  Video activity anticipation aims to predict what will happen in the future,\nembracing a broad application prospect ranging from robot vision and autonomous\ndriving. Despite the recent progress, the data uncertainty issue, reflected as\nthe content evolution process and dynamic correlation in event labels, has been\nsomehow ignored. This reduces the model generalization ability and deep\nunderstanding on video content, leading to serious error accumulation and\ndegraded performance. In this paper, we address the uncertainty learning\nproblem and propose an uncertainty-boosted robust video activity anticipation\nframework, which generates uncertainty values to indicate the credibility of\nthe anticipation results. The uncertainty value is used to derive a temperature\nparameter in the softmax function to modulate the predicted target activity\ndistribution. To guarantee the distribution adjustment, we construct a\nreasonable target activity label representation by incorporating the activity\nevolution from the temporal class correlation and the semantic relationship.\nMoreover, we quantify the uncertainty into relative values by comparing the\nuncertainty among sample pairs and their temporal-lengths. This relative\nstrategy provides a more accessible way in uncertainty modeling than\nquantifying the absolute uncertainty values on the whole dataset. Experiments\non multiple backbones and benchmarks show our framework achieves promising\nperformance and better robustness/interpretability. Source codes are available\nat https://github.com/qzhb/UbRV2A.\n", "link": "http://arxiv.org/abs/2404.18648v1", "date": "2024-04-29", "relevancy": 2.2921, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5736}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5728}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5725}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Uncertainty-boosted%20Robust%20Video%20Activity%20Anticipation&body=Title%3A%20Uncertainty-boosted%20Robust%20Video%20Activity%20Anticipation%0AAuthor%3A%20Zhaobo%20Qi%20and%20Shuhui%20Wang%20and%20Weigang%20Zhang%20and%20Qingming%20Huang%0AAbstract%3A%20%20%20Video%20activity%20anticipation%20aims%20to%20predict%20what%20will%20happen%20in%20the%20future%2C%0Aembracing%20a%20broad%20application%20prospect%20ranging%20from%20robot%20vision%20and%20autonomous%0Adriving.%20Despite%20the%20recent%20progress%2C%20the%20data%20uncertainty%20issue%2C%20reflected%20as%0Athe%20content%20evolution%20process%20and%20dynamic%20correlation%20in%20event%20labels%2C%20has%20been%0Asomehow%20ignored.%20This%20reduces%20the%20model%20generalization%20ability%20and%20deep%0Aunderstanding%20on%20video%20content%2C%20leading%20to%20serious%20error%20accumulation%20and%0Adegraded%20performance.%20In%20this%20paper%2C%20we%20address%20the%20uncertainty%20learning%0Aproblem%20and%20propose%20an%20uncertainty-boosted%20robust%20video%20activity%20anticipation%0Aframework%2C%20which%20generates%20uncertainty%20values%20to%20indicate%20the%20credibility%20of%0Athe%20anticipation%20results.%20The%20uncertainty%20value%20is%20used%20to%20derive%20a%20temperature%0Aparameter%20in%20the%20softmax%20function%20to%20modulate%20the%20predicted%20target%20activity%0Adistribution.%20To%20guarantee%20the%20distribution%20adjustment%2C%20we%20construct%20a%0Areasonable%20target%20activity%20label%20representation%20by%20incorporating%20the%20activity%0Aevolution%20from%20the%20temporal%20class%20correlation%20and%20the%20semantic%20relationship.%0AMoreover%2C%20we%20quantify%20the%20uncertainty%20into%20relative%20values%20by%20comparing%20the%0Auncertainty%20among%20sample%20pairs%20and%20their%20temporal-lengths.%20This%20relative%0Astrategy%20provides%20a%20more%20accessible%20way%20in%20uncertainty%20modeling%20than%0Aquantifying%20the%20absolute%20uncertainty%20values%20on%20the%20whole%20dataset.%20Experiments%0Aon%20multiple%20backbones%20and%20benchmarks%20show%20our%20framework%20achieves%20promising%0Aperformance%20and%20better%20robustness/interpretability.%20Source%20codes%20are%20available%0Aat%20https%3A//github.com/qzhb/UbRV2A.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18648v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty-boosted%20Robust%20Video%20Activity%20Anticipation&entry.906535625=Zhaobo%20Qi%20and%20Shuhui%20Wang%20and%20Weigang%20Zhang%20and%20Qingming%20Huang&entry.1292438233=%20%20Video%20activity%20anticipation%20aims%20to%20predict%20what%20will%20happen%20in%20the%20future%2C%0Aembracing%20a%20broad%20application%20prospect%20ranging%20from%20robot%20vision%20and%20autonomous%0Adriving.%20Despite%20the%20recent%20progress%2C%20the%20data%20uncertainty%20issue%2C%20reflected%20as%0Athe%20content%20evolution%20process%20and%20dynamic%20correlation%20in%20event%20labels%2C%20has%20been%0Asomehow%20ignored.%20This%20reduces%20the%20model%20generalization%20ability%20and%20deep%0Aunderstanding%20on%20video%20content%2C%20leading%20to%20serious%20error%20accumulation%20and%0Adegraded%20performance.%20In%20this%20paper%2C%20we%20address%20the%20uncertainty%20learning%0Aproblem%20and%20propose%20an%20uncertainty-boosted%20robust%20video%20activity%20anticipation%0Aframework%2C%20which%20generates%20uncertainty%20values%20to%20indicate%20the%20credibility%20of%0Athe%20anticipation%20results.%20The%20uncertainty%20value%20is%20used%20to%20derive%20a%20temperature%0Aparameter%20in%20the%20softmax%20function%20to%20modulate%20the%20predicted%20target%20activity%0Adistribution.%20To%20guarantee%20the%20distribution%20adjustment%2C%20we%20construct%20a%0Areasonable%20target%20activity%20label%20representation%20by%20incorporating%20the%20activity%0Aevolution%20from%20the%20temporal%20class%20correlation%20and%20the%20semantic%20relationship.%0AMoreover%2C%20we%20quantify%20the%20uncertainty%20into%20relative%20values%20by%20comparing%20the%0Auncertainty%20among%20sample%20pairs%20and%20their%20temporal-lengths.%20This%20relative%0Astrategy%20provides%20a%20more%20accessible%20way%20in%20uncertainty%20modeling%20than%0Aquantifying%20the%20absolute%20uncertainty%20values%20on%20the%20whole%20dataset.%20Experiments%0Aon%20multiple%20backbones%20and%20benchmarks%20show%20our%20framework%20achieves%20promising%0Aperformance%20and%20better%20robustness/interpretability.%20Source%20codes%20are%20available%0Aat%20https%3A//github.com/qzhb/UbRV2A.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18648v1&entry.124074799=Read"},
{"title": "Evaluating the Effectiveness of Video Anomaly Detection in the Wild:\n  Online Learning and Inference for Real-world Deployment", "author": "Shanle Yao and Ghazal Alinezhad Noghre and Armin Danesh Pazho and Hamed Tabkhi", "abstract": "  Video Anomaly Detection (VAD) identifies unusual activities in video streams,\na key technology with broad applications ranging from surveillance to\nhealthcare. Tackling VAD in real-life settings poses significant challenges due\nto the dynamic nature of human actions, environmental variations, and domain\nshifts. Many research initiatives neglect these complexities, often\nconcentrating on traditional testing methods that fail to account for\nperformance on unseen datasets, creating a gap between theoretical models and\ntheir real-world utility. Online learning is a potential strategy to mitigate\nthis issue by allowing models to adapt to new information continuously. This\npaper assesses how well current VAD algorithms can adjust to real-life\nconditions through an online learning framework, particularly those based on\npose analysis, for their efficiency and privacy advantages. Our proposed\nframework enables continuous model updates with streaming data from novel\nenvironments, thus mirroring actual world challenges and evaluating the models'\nability to adapt in real-time while maintaining accuracy. We investigate three\nstate-of-the-art models in this setting, focusing on their adaptability across\ndifferent domains. Our findings indicate that, even under the most challenging\nconditions, our online learning approach allows a model to preserve 89.39% of\nits original effectiveness compared to its offline-trained counterpart in a\nspecific target domain.\n", "link": "http://arxiv.org/abs/2404.18747v1", "date": "2024-04-29", "relevancy": 2.2347, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5652}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5622}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5508}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Evaluating%20the%20Effectiveness%20of%20Video%20Anomaly%20Detection%20in%20the%20Wild%3A%0A%20%20Online%20Learning%20and%20Inference%20for%20Real-world%20Deployment&body=Title%3A%20Evaluating%20the%20Effectiveness%20of%20Video%20Anomaly%20Detection%20in%20the%20Wild%3A%0A%20%20Online%20Learning%20and%20Inference%20for%20Real-world%20Deployment%0AAuthor%3A%20Shanle%20Yao%20and%20Ghazal%20Alinezhad%20Noghre%20and%20Armin%20Danesh%20Pazho%20and%20Hamed%20Tabkhi%0AAbstract%3A%20%20%20Video%20Anomaly%20Detection%20%28VAD%29%20identifies%20unusual%20activities%20in%20video%20streams%2C%0Aa%20key%20technology%20with%20broad%20applications%20ranging%20from%20surveillance%20to%0Ahealthcare.%20Tackling%20VAD%20in%20real-life%20settings%20poses%20significant%20challenges%20due%0Ato%20the%20dynamic%20nature%20of%20human%20actions%2C%20environmental%20variations%2C%20and%20domain%0Ashifts.%20Many%20research%20initiatives%20neglect%20these%20complexities%2C%20often%0Aconcentrating%20on%20traditional%20testing%20methods%20that%20fail%20to%20account%20for%0Aperformance%20on%20unseen%20datasets%2C%20creating%20a%20gap%20between%20theoretical%20models%20and%0Atheir%20real-world%20utility.%20Online%20learning%20is%20a%20potential%20strategy%20to%20mitigate%0Athis%20issue%20by%20allowing%20models%20to%20adapt%20to%20new%20information%20continuously.%20This%0Apaper%20assesses%20how%20well%20current%20VAD%20algorithms%20can%20adjust%20to%20real-life%0Aconditions%20through%20an%20online%20learning%20framework%2C%20particularly%20those%20based%20on%0Apose%20analysis%2C%20for%20their%20efficiency%20and%20privacy%20advantages.%20Our%20proposed%0Aframework%20enables%20continuous%20model%20updates%20with%20streaming%20data%20from%20novel%0Aenvironments%2C%20thus%20mirroring%20actual%20world%20challenges%20and%20evaluating%20the%20models%27%0Aability%20to%20adapt%20in%20real-time%20while%20maintaining%20accuracy.%20We%20investigate%20three%0Astate-of-the-art%20models%20in%20this%20setting%2C%20focusing%20on%20their%20adaptability%20across%0Adifferent%20domains.%20Our%20findings%20indicate%20that%2C%20even%20under%20the%20most%20challenging%0Aconditions%2C%20our%20online%20learning%20approach%20allows%20a%20model%20to%20preserve%2089.39%25%20of%0Aits%20original%20effectiveness%20compared%20to%20its%20offline-trained%20counterpart%20in%20a%0Aspecific%20target%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18747v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20the%20Effectiveness%20of%20Video%20Anomaly%20Detection%20in%20the%20Wild%3A%0A%20%20Online%20Learning%20and%20Inference%20for%20Real-world%20Deployment&entry.906535625=Shanle%20Yao%20and%20Ghazal%20Alinezhad%20Noghre%20and%20Armin%20Danesh%20Pazho%20and%20Hamed%20Tabkhi&entry.1292438233=%20%20Video%20Anomaly%20Detection%20%28VAD%29%20identifies%20unusual%20activities%20in%20video%20streams%2C%0Aa%20key%20technology%20with%20broad%20applications%20ranging%20from%20surveillance%20to%0Ahealthcare.%20Tackling%20VAD%20in%20real-life%20settings%20poses%20significant%20challenges%20due%0Ato%20the%20dynamic%20nature%20of%20human%20actions%2C%20environmental%20variations%2C%20and%20domain%0Ashifts.%20Many%20research%20initiatives%20neglect%20these%20complexities%2C%20often%0Aconcentrating%20on%20traditional%20testing%20methods%20that%20fail%20to%20account%20for%0Aperformance%20on%20unseen%20datasets%2C%20creating%20a%20gap%20between%20theoretical%20models%20and%0Atheir%20real-world%20utility.%20Online%20learning%20is%20a%20potential%20strategy%20to%20mitigate%0Athis%20issue%20by%20allowing%20models%20to%20adapt%20to%20new%20information%20continuously.%20This%0Apaper%20assesses%20how%20well%20current%20VAD%20algorithms%20can%20adjust%20to%20real-life%0Aconditions%20through%20an%20online%20learning%20framework%2C%20particularly%20those%20based%20on%0Apose%20analysis%2C%20for%20their%20efficiency%20and%20privacy%20advantages.%20Our%20proposed%0Aframework%20enables%20continuous%20model%20updates%20with%20streaming%20data%20from%20novel%0Aenvironments%2C%20thus%20mirroring%20actual%20world%20challenges%20and%20evaluating%20the%20models%27%0Aability%20to%20adapt%20in%20real-time%20while%20maintaining%20accuracy.%20We%20investigate%20three%0Astate-of-the-art%20models%20in%20this%20setting%2C%20focusing%20on%20their%20adaptability%20across%0Adifferent%20domains.%20Our%20findings%20indicate%20that%2C%20even%20under%20the%20most%20challenging%0Aconditions%2C%20our%20online%20learning%20approach%20allows%20a%20model%20to%20preserve%2089.39%25%20of%0Aits%20original%20effectiveness%20compared%20to%20its%20offline-trained%20counterpart%20in%20a%0Aspecific%20target%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18747v1&entry.124074799=Read"},
{"title": "Dual Expert Distillation Network for Generalized Zero-Shot Learning", "author": "Zhijie Rao and Jingcai Guo and Xiaocheng Lu and Jingming Liang and Jie Zhang and Haozhao Wang and Kang Wei and Xiaofeng Cao", "abstract": "  Zero-shot learning has consistently yielded remarkable progress via modeling\nnuanced one-to-one visual-attribute correlation. Existing studies resort to\nrefining a uniform mapping function to align and correlate the sample regions\nand subattributes, ignoring two crucial issues: 1) the inherent asymmetry of\nattributes; and 2) the unutilized channel information. This paper addresses\nthese issues by introducing a simple yet effective approach, dubbed Dual Expert\nDistillation Network (DEDN), where two experts are dedicated to coarse- and\nfine-grained visual-attribute modeling, respectively. Concretely, one coarse\nexpert, namely cExp, has a complete perceptual scope to coordinate\nvisual-attribute similarity metrics across dimensions, and moreover, another\nfine expert, namely fExp, consists of multiple specialized subnetworks, each\ncorresponds to an exclusive set of attributes. Two experts cooperatively\ndistill from each other to reach a mutual agreement during training. Meanwhile,\nwe further equip DEDN with a newly designed backbone network, i.e., Dual\nAttention Network (DAN), which incorporates both region and channel attention\ninformation to fully exploit and leverage visual semantic knowledge.\nExperiments on various benchmark datasets indicate a new state-of-the-art.\n", "link": "http://arxiv.org/abs/2404.16348v2", "date": "2024-04-29", "relevancy": 2.2146, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5671}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5482}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5337}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dual%20Expert%20Distillation%20Network%20for%20Generalized%20Zero-Shot%20Learning&body=Title%3A%20Dual%20Expert%20Distillation%20Network%20for%20Generalized%20Zero-Shot%20Learning%0AAuthor%3A%20Zhijie%20Rao%20and%20Jingcai%20Guo%20and%20Xiaocheng%20Lu%20and%20Jingming%20Liang%20and%20Jie%20Zhang%20and%20Haozhao%20Wang%20and%20Kang%20Wei%20and%20Xiaofeng%20Cao%0AAbstract%3A%20%20%20Zero-shot%20learning%20has%20consistently%20yielded%20remarkable%20progress%20via%20modeling%0Anuanced%20one-to-one%20visual-attribute%20correlation.%20Existing%20studies%20resort%20to%0Arefining%20a%20uniform%20mapping%20function%20to%20align%20and%20correlate%20the%20sample%20regions%0Aand%20subattributes%2C%20ignoring%20two%20crucial%20issues%3A%201%29%20the%20inherent%20asymmetry%20of%0Aattributes%3B%20and%202%29%20the%20unutilized%20channel%20information.%20This%20paper%20addresses%0Athese%20issues%20by%20introducing%20a%20simple%20yet%20effective%20approach%2C%20dubbed%20Dual%20Expert%0ADistillation%20Network%20%28DEDN%29%2C%20where%20two%20experts%20are%20dedicated%20to%20coarse-%20and%0Afine-grained%20visual-attribute%20modeling%2C%20respectively.%20Concretely%2C%20one%20coarse%0Aexpert%2C%20namely%20cExp%2C%20has%20a%20complete%20perceptual%20scope%20to%20coordinate%0Avisual-attribute%20similarity%20metrics%20across%20dimensions%2C%20and%20moreover%2C%20another%0Afine%20expert%2C%20namely%20fExp%2C%20consists%20of%20multiple%20specialized%20subnetworks%2C%20each%0Acorresponds%20to%20an%20exclusive%20set%20of%20attributes.%20Two%20experts%20cooperatively%0Adistill%20from%20each%20other%20to%20reach%20a%20mutual%20agreement%20during%20training.%20Meanwhile%2C%0Awe%20further%20equip%20DEDN%20with%20a%20newly%20designed%20backbone%20network%2C%20i.e.%2C%20Dual%0AAttention%20Network%20%28DAN%29%2C%20which%20incorporates%20both%20region%20and%20channel%20attention%0Ainformation%20to%20fully%20exploit%20and%20leverage%20visual%20semantic%20knowledge.%0AExperiments%20on%20various%20benchmark%20datasets%20indicate%20a%20new%20state-of-the-art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16348v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual%20Expert%20Distillation%20Network%20for%20Generalized%20Zero-Shot%20Learning&entry.906535625=Zhijie%20Rao%20and%20Jingcai%20Guo%20and%20Xiaocheng%20Lu%20and%20Jingming%20Liang%20and%20Jie%20Zhang%20and%20Haozhao%20Wang%20and%20Kang%20Wei%20and%20Xiaofeng%20Cao&entry.1292438233=%20%20Zero-shot%20learning%20has%20consistently%20yielded%20remarkable%20progress%20via%20modeling%0Anuanced%20one-to-one%20visual-attribute%20correlation.%20Existing%20studies%20resort%20to%0Arefining%20a%20uniform%20mapping%20function%20to%20align%20and%20correlate%20the%20sample%20regions%0Aand%20subattributes%2C%20ignoring%20two%20crucial%20issues%3A%201%29%20the%20inherent%20asymmetry%20of%0Aattributes%3B%20and%202%29%20the%20unutilized%20channel%20information.%20This%20paper%20addresses%0Athese%20issues%20by%20introducing%20a%20simple%20yet%20effective%20approach%2C%20dubbed%20Dual%20Expert%0ADistillation%20Network%20%28DEDN%29%2C%20where%20two%20experts%20are%20dedicated%20to%20coarse-%20and%0Afine-grained%20visual-attribute%20modeling%2C%20respectively.%20Concretely%2C%20one%20coarse%0Aexpert%2C%20namely%20cExp%2C%20has%20a%20complete%20perceptual%20scope%20to%20coordinate%0Avisual-attribute%20similarity%20metrics%20across%20dimensions%2C%20and%20moreover%2C%20another%0Afine%20expert%2C%20namely%20fExp%2C%20consists%20of%20multiple%20specialized%20subnetworks%2C%20each%0Acorresponds%20to%20an%20exclusive%20set%20of%20attributes.%20Two%20experts%20cooperatively%0Adistill%20from%20each%20other%20to%20reach%20a%20mutual%20agreement%20during%20training.%20Meanwhile%2C%0Awe%20further%20equip%20DEDN%20with%20a%20newly%20designed%20backbone%20network%2C%20i.e.%2C%20Dual%0AAttention%20Network%20%28DAN%29%2C%20which%20incorporates%20both%20region%20and%20channel%20attention%0Ainformation%20to%20fully%20exploit%20and%20leverage%20visual%20semantic%20knowledge.%0AExperiments%20on%20various%20benchmark%20datasets%20indicate%20a%20new%20state-of-the-art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16348v2&entry.124074799=Read"},
{"title": "Context Matters: Leveraging Spatiotemporal Metadata for Semi-Supervised\n  Learning on Remote Sensing Images", "author": "Maximilian Bernhard and Tanveer Hannan and Niklas Strau\u00df and Matthias Schubert", "abstract": "  Remote sensing projects typically generate large amounts of imagery that can\nbe used to train powerful deep neural networks. However, the amount of labeled\nimages is often small, as remote sensing applications generally require expert\nlabelers. Thus, semi-supervised learning (SSL), i.e., learning with a small\npool of labeled and a larger pool of unlabeled data, is particularly useful in\nthis domain. Current SSL approaches generate pseudo-labels from model\npredictions for unlabeled samples. As the quality of these pseudo-labels is\ncrucial for performance, utilizing additional information to improve\npseudo-label quality yields a promising direction. For remote sensing images,\ngeolocation and recording time are generally available and provide a valuable\nsource of information as semantic concepts, such as land cover, are highly\ndependent on spatiotemporal context, e.g., due to seasonal effects and\nvegetation zones. In this paper, we propose to exploit spatiotemporal\nmetainformation in SSL to improve the quality of pseudo-labels and, therefore,\nthe final model performance. We show that directly adding the available\nmetadata to the input of the predictor at test time degenerates the prediction\nquality for metadata outside the spatiotemporal distribution of the training\nset. Thus, we propose a teacher-student SSL framework where only the teacher\nnetwork uses metainformation to improve the quality of pseudo-labels on the\ntraining set. Correspondingly, our student network benefits from the improved\npseudo-labels but does not receive metadata as input, making it invariant to\nspatiotemporal shifts at test time. Furthermore, we propose methods for\nencoding and injecting spatiotemporal information into the model and introduce\na novel distillation mechanism to enhance the knowledge transfer between\nteacher and student. Our framework dubbed Spatiotemporal SSL can be easily\ncombined with several stat...\n", "link": "http://arxiv.org/abs/2404.18583v1", "date": "2024-04-29", "relevancy": 2.2051, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5804}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5322}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5263}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Context%20Matters%3A%20Leveraging%20Spatiotemporal%20Metadata%20for%20Semi-Supervised%0A%20%20Learning%20on%20Remote%20Sensing%20Images&body=Title%3A%20Context%20Matters%3A%20Leveraging%20Spatiotemporal%20Metadata%20for%20Semi-Supervised%0A%20%20Learning%20on%20Remote%20Sensing%20Images%0AAuthor%3A%20Maximilian%20Bernhard%20and%20Tanveer%20Hannan%20and%20Niklas%20Strau%C3%9F%20and%20Matthias%20Schubert%0AAbstract%3A%20%20%20Remote%20sensing%20projects%20typically%20generate%20large%20amounts%20of%20imagery%20that%20can%0Abe%20used%20to%20train%20powerful%20deep%20neural%20networks.%20However%2C%20the%20amount%20of%20labeled%0Aimages%20is%20often%20small%2C%20as%20remote%20sensing%20applications%20generally%20require%20expert%0Alabelers.%20Thus%2C%20semi-supervised%20learning%20%28SSL%29%2C%20i.e.%2C%20learning%20with%20a%20small%0Apool%20of%20labeled%20and%20a%20larger%20pool%20of%20unlabeled%20data%2C%20is%20particularly%20useful%20in%0Athis%20domain.%20Current%20SSL%20approaches%20generate%20pseudo-labels%20from%20model%0Apredictions%20for%20unlabeled%20samples.%20As%20the%20quality%20of%20these%20pseudo-labels%20is%0Acrucial%20for%20performance%2C%20utilizing%20additional%20information%20to%20improve%0Apseudo-label%20quality%20yields%20a%20promising%20direction.%20For%20remote%20sensing%20images%2C%0Ageolocation%20and%20recording%20time%20are%20generally%20available%20and%20provide%20a%20valuable%0Asource%20of%20information%20as%20semantic%20concepts%2C%20such%20as%20land%20cover%2C%20are%20highly%0Adependent%20on%20spatiotemporal%20context%2C%20e.g.%2C%20due%20to%20seasonal%20effects%20and%0Avegetation%20zones.%20In%20this%20paper%2C%20we%20propose%20to%20exploit%20spatiotemporal%0Ametainformation%20in%20SSL%20to%20improve%20the%20quality%20of%20pseudo-labels%20and%2C%20therefore%2C%0Athe%20final%20model%20performance.%20We%20show%20that%20directly%20adding%20the%20available%0Ametadata%20to%20the%20input%20of%20the%20predictor%20at%20test%20time%20degenerates%20the%20prediction%0Aquality%20for%20metadata%20outside%20the%20spatiotemporal%20distribution%20of%20the%20training%0Aset.%20Thus%2C%20we%20propose%20a%20teacher-student%20SSL%20framework%20where%20only%20the%20teacher%0Anetwork%20uses%20metainformation%20to%20improve%20the%20quality%20of%20pseudo-labels%20on%20the%0Atraining%20set.%20Correspondingly%2C%20our%20student%20network%20benefits%20from%20the%20improved%0Apseudo-labels%20but%20does%20not%20receive%20metadata%20as%20input%2C%20making%20it%20invariant%20to%0Aspatiotemporal%20shifts%20at%20test%20time.%20Furthermore%2C%20we%20propose%20methods%20for%0Aencoding%20and%20injecting%20spatiotemporal%20information%20into%20the%20model%20and%20introduce%0Aa%20novel%20distillation%20mechanism%20to%20enhance%20the%20knowledge%20transfer%20between%0Ateacher%20and%20student.%20Our%20framework%20dubbed%20Spatiotemporal%20SSL%20can%20be%20easily%0Acombined%20with%20several%20stat...%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18583v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context%20Matters%3A%20Leveraging%20Spatiotemporal%20Metadata%20for%20Semi-Supervised%0A%20%20Learning%20on%20Remote%20Sensing%20Images&entry.906535625=Maximilian%20Bernhard%20and%20Tanveer%20Hannan%20and%20Niklas%20Strau%C3%9F%20and%20Matthias%20Schubert&entry.1292438233=%20%20Remote%20sensing%20projects%20typically%20generate%20large%20amounts%20of%20imagery%20that%20can%0Abe%20used%20to%20train%20powerful%20deep%20neural%20networks.%20However%2C%20the%20amount%20of%20labeled%0Aimages%20is%20often%20small%2C%20as%20remote%20sensing%20applications%20generally%20require%20expert%0Alabelers.%20Thus%2C%20semi-supervised%20learning%20%28SSL%29%2C%20i.e.%2C%20learning%20with%20a%20small%0Apool%20of%20labeled%20and%20a%20larger%20pool%20of%20unlabeled%20data%2C%20is%20particularly%20useful%20in%0Athis%20domain.%20Current%20SSL%20approaches%20generate%20pseudo-labels%20from%20model%0Apredictions%20for%20unlabeled%20samples.%20As%20the%20quality%20of%20these%20pseudo-labels%20is%0Acrucial%20for%20performance%2C%20utilizing%20additional%20information%20to%20improve%0Apseudo-label%20quality%20yields%20a%20promising%20direction.%20For%20remote%20sensing%20images%2C%0Ageolocation%20and%20recording%20time%20are%20generally%20available%20and%20provide%20a%20valuable%0Asource%20of%20information%20as%20semantic%20concepts%2C%20such%20as%20land%20cover%2C%20are%20highly%0Adependent%20on%20spatiotemporal%20context%2C%20e.g.%2C%20due%20to%20seasonal%20effects%20and%0Avegetation%20zones.%20In%20this%20paper%2C%20we%20propose%20to%20exploit%20spatiotemporal%0Ametainformation%20in%20SSL%20to%20improve%20the%20quality%20of%20pseudo-labels%20and%2C%20therefore%2C%0Athe%20final%20model%20performance.%20We%20show%20that%20directly%20adding%20the%20available%0Ametadata%20to%20the%20input%20of%20the%20predictor%20at%20test%20time%20degenerates%20the%20prediction%0Aquality%20for%20metadata%20outside%20the%20spatiotemporal%20distribution%20of%20the%20training%0Aset.%20Thus%2C%20we%20propose%20a%20teacher-student%20SSL%20framework%20where%20only%20the%20teacher%0Anetwork%20uses%20metainformation%20to%20improve%20the%20quality%20of%20pseudo-labels%20on%20the%0Atraining%20set.%20Correspondingly%2C%20our%20student%20network%20benefits%20from%20the%20improved%0Apseudo-labels%20but%20does%20not%20receive%20metadata%20as%20input%2C%20making%20it%20invariant%20to%0Aspatiotemporal%20shifts%20at%20test%20time.%20Furthermore%2C%20we%20propose%20methods%20for%0Aencoding%20and%20injecting%20spatiotemporal%20information%20into%20the%20model%20and%20introduce%0Aa%20novel%20distillation%20mechanism%20to%20enhance%20the%20knowledge%20transfer%20between%0Ateacher%20and%20student.%20Our%20framework%20dubbed%20Spatiotemporal%20SSL%20can%20be%20easily%0Acombined%20with%20several%20stat...%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18583v1&entry.124074799=Read"},
{"title": "Dual-Modal Prompting for Sketch-Based Image Retrieval", "author": "Liying Gao and Bingliang Jiao and Peng Wang and Shizhou Zhang and Hanwang Zhang and Yanning Zhang", "abstract": "  Sketch-based image retrieval (SBIR) associates hand-drawn sketches with their\ncorresponding realistic images. In this study, we aim to tackle two major\nchallenges of this task simultaneously: i) zero-shot, dealing with unseen\ncategories, and ii) fine-grained, referring to intra-category instance-level\nretrieval. Our key innovation lies in the realization that solely addressing\nthis cross-category and fine-grained recognition task from the generalization\nperspective may be inadequate since the knowledge accumulated from limited seen\ncategories might not be fully valuable or transferable to unseen target\ncategories. Inspired by this, in this work, we propose a dual-modal prompting\nCLIP (DP-CLIP) network, in which an adaptive prompting strategy is designed.\nSpecifically, to facilitate the adaptation of our DP-CLIP toward unpredictable\ntarget categories, we employ a set of images within the target category and the\ntextual category label to respectively construct a set of category-adaptive\nprompt tokens and channel scales. By integrating the generated guidance,\nDP-CLIP could gain valuable category-centric insights, efficiently adapting to\nnovel categories and capturing unique discriminative clues for effective\nretrieval within each target category. With these designs, our DP-CLIP\noutperforms the state-of-the-art fine-grained zero-shot SBIR method by 7.3% in\nAcc.@1 on the Sketchy dataset. Meanwhile, in the other two category-level\nzero-shot SBIR benchmarks, our method also achieves promising performance.\n", "link": "http://arxiv.org/abs/2404.18695v1", "date": "2024-04-29", "relevancy": 2.176, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5564}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5432}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5152}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dual-Modal%20Prompting%20for%20Sketch-Based%20Image%20Retrieval&body=Title%3A%20Dual-Modal%20Prompting%20for%20Sketch-Based%20Image%20Retrieval%0AAuthor%3A%20Liying%20Gao%20and%20Bingliang%20Jiao%20and%20Peng%20Wang%20and%20Shizhou%20Zhang%20and%20Hanwang%20Zhang%20and%20Yanning%20Zhang%0AAbstract%3A%20%20%20Sketch-based%20image%20retrieval%20%28SBIR%29%20associates%20hand-drawn%20sketches%20with%20their%0Acorresponding%20realistic%20images.%20In%20this%20study%2C%20we%20aim%20to%20tackle%20two%20major%0Achallenges%20of%20this%20task%20simultaneously%3A%20i%29%20zero-shot%2C%20dealing%20with%20unseen%0Acategories%2C%20and%20ii%29%20fine-grained%2C%20referring%20to%20intra-category%20instance-level%0Aretrieval.%20Our%20key%20innovation%20lies%20in%20the%20realization%20that%20solely%20addressing%0Athis%20cross-category%20and%20fine-grained%20recognition%20task%20from%20the%20generalization%0Aperspective%20may%20be%20inadequate%20since%20the%20knowledge%20accumulated%20from%20limited%20seen%0Acategories%20might%20not%20be%20fully%20valuable%20or%20transferable%20to%20unseen%20target%0Acategories.%20Inspired%20by%20this%2C%20in%20this%20work%2C%20we%20propose%20a%20dual-modal%20prompting%0ACLIP%20%28DP-CLIP%29%20network%2C%20in%20which%20an%20adaptive%20prompting%20strategy%20is%20designed.%0ASpecifically%2C%20to%20facilitate%20the%20adaptation%20of%20our%20DP-CLIP%20toward%20unpredictable%0Atarget%20categories%2C%20we%20employ%20a%20set%20of%20images%20within%20the%20target%20category%20and%20the%0Atextual%20category%20label%20to%20respectively%20construct%20a%20set%20of%20category-adaptive%0Aprompt%20tokens%20and%20channel%20scales.%20By%20integrating%20the%20generated%20guidance%2C%0ADP-CLIP%20could%20gain%20valuable%20category-centric%20insights%2C%20efficiently%20adapting%20to%0Anovel%20categories%20and%20capturing%20unique%20discriminative%20clues%20for%20effective%0Aretrieval%20within%20each%20target%20category.%20With%20these%20designs%2C%20our%20DP-CLIP%0Aoutperforms%20the%20state-of-the-art%20fine-grained%20zero-shot%20SBIR%20method%20by%207.3%25%20in%0AAcc.%401%20on%20the%20Sketchy%20dataset.%20Meanwhile%2C%20in%20the%20other%20two%20category-level%0Azero-shot%20SBIR%20benchmarks%2C%20our%20method%20also%20achieves%20promising%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18695v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-Modal%20Prompting%20for%20Sketch-Based%20Image%20Retrieval&entry.906535625=Liying%20Gao%20and%20Bingliang%20Jiao%20and%20Peng%20Wang%20and%20Shizhou%20Zhang%20and%20Hanwang%20Zhang%20and%20Yanning%20Zhang&entry.1292438233=%20%20Sketch-based%20image%20retrieval%20%28SBIR%29%20associates%20hand-drawn%20sketches%20with%20their%0Acorresponding%20realistic%20images.%20In%20this%20study%2C%20we%20aim%20to%20tackle%20two%20major%0Achallenges%20of%20this%20task%20simultaneously%3A%20i%29%20zero-shot%2C%20dealing%20with%20unseen%0Acategories%2C%20and%20ii%29%20fine-grained%2C%20referring%20to%20intra-category%20instance-level%0Aretrieval.%20Our%20key%20innovation%20lies%20in%20the%20realization%20that%20solely%20addressing%0Athis%20cross-category%20and%20fine-grained%20recognition%20task%20from%20the%20generalization%0Aperspective%20may%20be%20inadequate%20since%20the%20knowledge%20accumulated%20from%20limited%20seen%0Acategories%20might%20not%20be%20fully%20valuable%20or%20transferable%20to%20unseen%20target%0Acategories.%20Inspired%20by%20this%2C%20in%20this%20work%2C%20we%20propose%20a%20dual-modal%20prompting%0ACLIP%20%28DP-CLIP%29%20network%2C%20in%20which%20an%20adaptive%20prompting%20strategy%20is%20designed.%0ASpecifically%2C%20to%20facilitate%20the%20adaptation%20of%20our%20DP-CLIP%20toward%20unpredictable%0Atarget%20categories%2C%20we%20employ%20a%20set%20of%20images%20within%20the%20target%20category%20and%20the%0Atextual%20category%20label%20to%20respectively%20construct%20a%20set%20of%20category-adaptive%0Aprompt%20tokens%20and%20channel%20scales.%20By%20integrating%20the%20generated%20guidance%2C%0ADP-CLIP%20could%20gain%20valuable%20category-centric%20insights%2C%20efficiently%20adapting%20to%0Anovel%20categories%20and%20capturing%20unique%20discriminative%20clues%20for%20effective%0Aretrieval%20within%20each%20target%20category.%20With%20these%20designs%2C%20our%20DP-CLIP%0Aoutperforms%20the%20state-of-the-art%20fine-grained%20zero-shot%20SBIR%20method%20by%207.3%25%20in%0AAcc.%401%20on%20the%20Sketchy%20dataset.%20Meanwhile%2C%20in%20the%20other%20two%20category-level%0Azero-shot%20SBIR%20benchmarks%2C%20our%20method%20also%20achieves%20promising%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18695v1&entry.124074799=Read"},
{"title": "PlanNetX: Learning an Efficient Neural Network Planner from MPC for\n  Longitudinal Control", "author": "Jasper Hoffmann and Diego Fernandez and Julien Brosseit and Julian Bernhard and Klemens Esterle and Moritz Werling and Michael Karg and Joschka Boedecker", "abstract": "  Model predictive control (MPC) is a powerful, optimization-based approach for\ncontrolling dynamical systems. However, the computational complexity of online\noptimization can be problematic on embedded devices. Especially, when we need\nto guarantee fixed control frequencies. Thus, previous work proposed to reduce\nthe computational burden using imitation learning (IL) approximating the MPC\npolicy by a neural network. In this work, we instead learn the whole planned\ntrajectory of the MPC. We introduce a combination of a novel neural network\narchitecture PlanNetX and a simple loss function based on the state trajectory\nthat leverages the parameterized optimal control structure of the MPC. We\nvalidate our approach in the context of autonomous driving by learning a\nlongitudinal planner and benchmarking it extensively in the CommonRoad\nsimulator using synthetic scenarios and scenarios derived from real data. Our\nexperimental results show that we can learn the open-loop MPC trajectory with\nhigh accuracy while improving the closed-loop performance of the learned\ncontrol policy over other baselines like behavior cloning.\n", "link": "http://arxiv.org/abs/2404.18863v1", "date": "2024-04-29", "relevancy": 2.1735, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5628}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.53}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5282}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PlanNetX%3A%20Learning%20an%20Efficient%20Neural%20Network%20Planner%20from%20MPC%20for%0A%20%20Longitudinal%20Control&body=Title%3A%20PlanNetX%3A%20Learning%20an%20Efficient%20Neural%20Network%20Planner%20from%20MPC%20for%0A%20%20Longitudinal%20Control%0AAuthor%3A%20Jasper%20Hoffmann%20and%20Diego%20Fernandez%20and%20Julien%20Brosseit%20and%20Julian%20Bernhard%20and%20Klemens%20Esterle%20and%20Moritz%20Werling%20and%20Michael%20Karg%20and%20Joschka%20Boedecker%0AAbstract%3A%20%20%20Model%20predictive%20control%20%28MPC%29%20is%20a%20powerful%2C%20optimization-based%20approach%20for%0Acontrolling%20dynamical%20systems.%20However%2C%20the%20computational%20complexity%20of%20online%0Aoptimization%20can%20be%20problematic%20on%20embedded%20devices.%20Especially%2C%20when%20we%20need%0Ato%20guarantee%20fixed%20control%20frequencies.%20Thus%2C%20previous%20work%20proposed%20to%20reduce%0Athe%20computational%20burden%20using%20imitation%20learning%20%28IL%29%20approximating%20the%20MPC%0Apolicy%20by%20a%20neural%20network.%20In%20this%20work%2C%20we%20instead%20learn%20the%20whole%20planned%0Atrajectory%20of%20the%20MPC.%20We%20introduce%20a%20combination%20of%20a%20novel%20neural%20network%0Aarchitecture%20PlanNetX%20and%20a%20simple%20loss%20function%20based%20on%20the%20state%20trajectory%0Athat%20leverages%20the%20parameterized%20optimal%20control%20structure%20of%20the%20MPC.%20We%0Avalidate%20our%20approach%20in%20the%20context%20of%20autonomous%20driving%20by%20learning%20a%0Alongitudinal%20planner%20and%20benchmarking%20it%20extensively%20in%20the%20CommonRoad%0Asimulator%20using%20synthetic%20scenarios%20and%20scenarios%20derived%20from%20real%20data.%20Our%0Aexperimental%20results%20show%20that%20we%20can%20learn%20the%20open-loop%20MPC%20trajectory%20with%0Ahigh%20accuracy%20while%20improving%20the%20closed-loop%20performance%20of%20the%20learned%0Acontrol%20policy%20over%20other%20baselines%20like%20behavior%20cloning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18863v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PlanNetX%3A%20Learning%20an%20Efficient%20Neural%20Network%20Planner%20from%20MPC%20for%0A%20%20Longitudinal%20Control&entry.906535625=Jasper%20Hoffmann%20and%20Diego%20Fernandez%20and%20Julien%20Brosseit%20and%20Julian%20Bernhard%20and%20Klemens%20Esterle%20and%20Moritz%20Werling%20and%20Michael%20Karg%20and%20Joschka%20Boedecker&entry.1292438233=%20%20Model%20predictive%20control%20%28MPC%29%20is%20a%20powerful%2C%20optimization-based%20approach%20for%0Acontrolling%20dynamical%20systems.%20However%2C%20the%20computational%20complexity%20of%20online%0Aoptimization%20can%20be%20problematic%20on%20embedded%20devices.%20Especially%2C%20when%20we%20need%0Ato%20guarantee%20fixed%20control%20frequencies.%20Thus%2C%20previous%20work%20proposed%20to%20reduce%0Athe%20computational%20burden%20using%20imitation%20learning%20%28IL%29%20approximating%20the%20MPC%0Apolicy%20by%20a%20neural%20network.%20In%20this%20work%2C%20we%20instead%20learn%20the%20whole%20planned%0Atrajectory%20of%20the%20MPC.%20We%20introduce%20a%20combination%20of%20a%20novel%20neural%20network%0Aarchitecture%20PlanNetX%20and%20a%20simple%20loss%20function%20based%20on%20the%20state%20trajectory%0Athat%20leverages%20the%20parameterized%20optimal%20control%20structure%20of%20the%20MPC.%20We%0Avalidate%20our%20approach%20in%20the%20context%20of%20autonomous%20driving%20by%20learning%20a%0Alongitudinal%20planner%20and%20benchmarking%20it%20extensively%20in%20the%20CommonRoad%0Asimulator%20using%20synthetic%20scenarios%20and%20scenarios%20derived%20from%20real%20data.%20Our%0Aexperimental%20results%20show%20that%20we%20can%20learn%20the%20open-loop%20MPC%20trajectory%20with%0Ahigh%20accuracy%20while%20improving%20the%20closed-loop%20performance%20of%20the%20learned%0Acontrol%20policy%20over%20other%20baselines%20like%20behavior%20cloning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18863v1&entry.124074799=Read"},
{"title": "Socially Adaptive Path Planning Based on Generative Adversarial Network", "author": "Yao Wang and Yuqi Kong and Wenzheng Chi and Lining Sun", "abstract": "  The natural interaction between robots and pedestrians in the process of\nautonomous navigation is crucial for the intelligent development of mobile\nrobots, which requires robots to fully consider social rules and guarantee the\npsychological comfort of pedestrians. Among the research results in the field\nof robotic path planning, the learning-based socially adaptive algorithms have\nperformed well in some specific human-robot interaction environments. However,\nhuman-robot interaction scenarios are diverse and constantly changing in daily\nlife, and the generalization of robot socially adaptive path planning remains\nto be further investigated. In order to address this issue, this work proposes\na new socially adaptive path planning algorithm by combining the generative\nadversarial network (GAN) with the Optimal Rapidly-exploring Random Tree (RRT*)\nnavigation algorithm. Firstly, a GAN model with strong generalization\nperformance is proposed to adapt the navigation algorithm to more scenarios.\nSecondly, a GAN model based Optimal Rapidly-exploring Random Tree navigation\nalgorithm (GAN-RRT*) is proposed to generate paths in human-robot interaction\nenvironments. Finally, we propose a socially adaptive path planning framework\nnamed GAN-RTIRL, which combines the GAN model with Rapidly-exploring random\nTrees Inverse Reinforcement Learning (RTIRL) to improve the homotopy rate\nbetween planned and demonstration paths. In the GAN-RTIRL framework, the\nGAN-RRT* path planner can update the GAN model from the demonstration path. In\nthis way, the robot can generate more anthropomorphic paths in human-robot\ninteraction environments and has stronger generalization in more complex\nenvironments. Experimental results reveal that our proposed method can\neffectively improve the anthropomorphic degree of robot motion planning and the\nhomotopy rate between planned and demonstration paths.\n", "link": "http://arxiv.org/abs/2404.18687v1", "date": "2024-04-29", "relevancy": 2.1624, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.566}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5362}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5348}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Socially%20Adaptive%20Path%20Planning%20Based%20on%20Generative%20Adversarial%20Network&body=Title%3A%20Socially%20Adaptive%20Path%20Planning%20Based%20on%20Generative%20Adversarial%20Network%0AAuthor%3A%20Yao%20Wang%20and%20Yuqi%20Kong%20and%20Wenzheng%20Chi%20and%20Lining%20Sun%0AAbstract%3A%20%20%20The%20natural%20interaction%20between%20robots%20and%20pedestrians%20in%20the%20process%20of%0Aautonomous%20navigation%20is%20crucial%20for%20the%20intelligent%20development%20of%20mobile%0Arobots%2C%20which%20requires%20robots%20to%20fully%20consider%20social%20rules%20and%20guarantee%20the%0Apsychological%20comfort%20of%20pedestrians.%20Among%20the%20research%20results%20in%20the%20field%0Aof%20robotic%20path%20planning%2C%20the%20learning-based%20socially%20adaptive%20algorithms%20have%0Aperformed%20well%20in%20some%20specific%20human-robot%20interaction%20environments.%20However%2C%0Ahuman-robot%20interaction%20scenarios%20are%20diverse%20and%20constantly%20changing%20in%20daily%0Alife%2C%20and%20the%20generalization%20of%20robot%20socially%20adaptive%20path%20planning%20remains%0Ato%20be%20further%20investigated.%20In%20order%20to%20address%20this%20issue%2C%20this%20work%20proposes%0Aa%20new%20socially%20adaptive%20path%20planning%20algorithm%20by%20combining%20the%20generative%0Aadversarial%20network%20%28GAN%29%20with%20the%20Optimal%20Rapidly-exploring%20Random%20Tree%20%28RRT%2A%29%0Anavigation%20algorithm.%20Firstly%2C%20a%20GAN%20model%20with%20strong%20generalization%0Aperformance%20is%20proposed%20to%20adapt%20the%20navigation%20algorithm%20to%20more%20scenarios.%0ASecondly%2C%20a%20GAN%20model%20based%20Optimal%20Rapidly-exploring%20Random%20Tree%20navigation%0Aalgorithm%20%28GAN-RRT%2A%29%20is%20proposed%20to%20generate%20paths%20in%20human-robot%20interaction%0Aenvironments.%20Finally%2C%20we%20propose%20a%20socially%20adaptive%20path%20planning%20framework%0Anamed%20GAN-RTIRL%2C%20which%20combines%20the%20GAN%20model%20with%20Rapidly-exploring%20random%0ATrees%20Inverse%20Reinforcement%20Learning%20%28RTIRL%29%20to%20improve%20the%20homotopy%20rate%0Abetween%20planned%20and%20demonstration%20paths.%20In%20the%20GAN-RTIRL%20framework%2C%20the%0AGAN-RRT%2A%20path%20planner%20can%20update%20the%20GAN%20model%20from%20the%20demonstration%20path.%20In%0Athis%20way%2C%20the%20robot%20can%20generate%20more%20anthropomorphic%20paths%20in%20human-robot%0Ainteraction%20environments%20and%20has%20stronger%20generalization%20in%20more%20complex%0Aenvironments.%20Experimental%20results%20reveal%20that%20our%20proposed%20method%20can%0Aeffectively%20improve%20the%20anthropomorphic%20degree%20of%20robot%20motion%20planning%20and%20the%0Ahomotopy%20rate%20between%20planned%20and%20demonstration%20paths.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18687v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Socially%20Adaptive%20Path%20Planning%20Based%20on%20Generative%20Adversarial%20Network&entry.906535625=Yao%20Wang%20and%20Yuqi%20Kong%20and%20Wenzheng%20Chi%20and%20Lining%20Sun&entry.1292438233=%20%20The%20natural%20interaction%20between%20robots%20and%20pedestrians%20in%20the%20process%20of%0Aautonomous%20navigation%20is%20crucial%20for%20the%20intelligent%20development%20of%20mobile%0Arobots%2C%20which%20requires%20robots%20to%20fully%20consider%20social%20rules%20and%20guarantee%20the%0Apsychological%20comfort%20of%20pedestrians.%20Among%20the%20research%20results%20in%20the%20field%0Aof%20robotic%20path%20planning%2C%20the%20learning-based%20socially%20adaptive%20algorithms%20have%0Aperformed%20well%20in%20some%20specific%20human-robot%20interaction%20environments.%20However%2C%0Ahuman-robot%20interaction%20scenarios%20are%20diverse%20and%20constantly%20changing%20in%20daily%0Alife%2C%20and%20the%20generalization%20of%20robot%20socially%20adaptive%20path%20planning%20remains%0Ato%20be%20further%20investigated.%20In%20order%20to%20address%20this%20issue%2C%20this%20work%20proposes%0Aa%20new%20socially%20adaptive%20path%20planning%20algorithm%20by%20combining%20the%20generative%0Aadversarial%20network%20%28GAN%29%20with%20the%20Optimal%20Rapidly-exploring%20Random%20Tree%20%28RRT%2A%29%0Anavigation%20algorithm.%20Firstly%2C%20a%20GAN%20model%20with%20strong%20generalization%0Aperformance%20is%20proposed%20to%20adapt%20the%20navigation%20algorithm%20to%20more%20scenarios.%0ASecondly%2C%20a%20GAN%20model%20based%20Optimal%20Rapidly-exploring%20Random%20Tree%20navigation%0Aalgorithm%20%28GAN-RRT%2A%29%20is%20proposed%20to%20generate%20paths%20in%20human-robot%20interaction%0Aenvironments.%20Finally%2C%20we%20propose%20a%20socially%20adaptive%20path%20planning%20framework%0Anamed%20GAN-RTIRL%2C%20which%20combines%20the%20GAN%20model%20with%20Rapidly-exploring%20random%0ATrees%20Inverse%20Reinforcement%20Learning%20%28RTIRL%29%20to%20improve%20the%20homotopy%20rate%0Abetween%20planned%20and%20demonstration%20paths.%20In%20the%20GAN-RTIRL%20framework%2C%20the%0AGAN-RRT%2A%20path%20planner%20can%20update%20the%20GAN%20model%20from%20the%20demonstration%20path.%20In%0Athis%20way%2C%20the%20robot%20can%20generate%20more%20anthropomorphic%20paths%20in%20human-robot%0Ainteraction%20environments%20and%20has%20stronger%20generalization%20in%20more%20complex%0Aenvironments.%20Experimental%20results%20reveal%20that%20our%20proposed%20method%20can%0Aeffectively%20improve%20the%20anthropomorphic%20degree%20of%20robot%20motion%20planning%20and%20the%0Ahomotopy%20rate%20between%20planned%20and%20demonstration%20paths.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18687v1&entry.124074799=Read"},
{"title": "Saliency Suppressed, Semantics Surfaced: Visual Transformations in\n  Neural Networks and the Brain", "author": "Gustaw Opie\u0142ka and Jessica Loke and Steven Scholte", "abstract": "  Deep learning algorithms lack human-interpretable accounts of how they\ntransform raw visual input into a robust semantic understanding, which impedes\ncomparisons between different architectures, training objectives, and the human\nbrain. In this work, we take inspiration from neuroscience and employ\nrepresentational approaches to shed light on how neural networks encode\ninformation at low (visual saliency) and high (semantic similarity) levels of\nabstraction. Moreover, we introduce a custom image dataset where we\nsystematically manipulate salient and semantic information. We find that\nResNets are more sensitive to saliency information than ViTs, when trained with\nobject classification objectives. We uncover that networks suppress saliency in\nearly layers, a process enhanced by natural language supervision (CLIP) in\nResNets. CLIP also enhances semantic encoding in both architectures. Finally,\nwe show that semantic encoding is a key factor in aligning AI with human visual\nperception, while saliency suppression is a non-brain-like strategy.\n", "link": "http://arxiv.org/abs/2404.18772v1", "date": "2024-04-29", "relevancy": 2.1602, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.547}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5367}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5311}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Saliency%20Suppressed%2C%20Semantics%20Surfaced%3A%20Visual%20Transformations%20in%0A%20%20Neural%20Networks%20and%20the%20Brain&body=Title%3A%20Saliency%20Suppressed%2C%20Semantics%20Surfaced%3A%20Visual%20Transformations%20in%0A%20%20Neural%20Networks%20and%20the%20Brain%0AAuthor%3A%20Gustaw%20Opie%C5%82ka%20and%20Jessica%20Loke%20and%20Steven%20Scholte%0AAbstract%3A%20%20%20Deep%20learning%20algorithms%20lack%20human-interpretable%20accounts%20of%20how%20they%0Atransform%20raw%20visual%20input%20into%20a%20robust%20semantic%20understanding%2C%20which%20impedes%0Acomparisons%20between%20different%20architectures%2C%20training%20objectives%2C%20and%20the%20human%0Abrain.%20In%20this%20work%2C%20we%20take%20inspiration%20from%20neuroscience%20and%20employ%0Arepresentational%20approaches%20to%20shed%20light%20on%20how%20neural%20networks%20encode%0Ainformation%20at%20low%20%28visual%20saliency%29%20and%20high%20%28semantic%20similarity%29%20levels%20of%0Aabstraction.%20Moreover%2C%20we%20introduce%20a%20custom%20image%20dataset%20where%20we%0Asystematically%20manipulate%20salient%20and%20semantic%20information.%20We%20find%20that%0AResNets%20are%20more%20sensitive%20to%20saliency%20information%20than%20ViTs%2C%20when%20trained%20with%0Aobject%20classification%20objectives.%20We%20uncover%20that%20networks%20suppress%20saliency%20in%0Aearly%20layers%2C%20a%20process%20enhanced%20by%20natural%20language%20supervision%20%28CLIP%29%20in%0AResNets.%20CLIP%20also%20enhances%20semantic%20encoding%20in%20both%20architectures.%20Finally%2C%0Awe%20show%20that%20semantic%20encoding%20is%20a%20key%20factor%20in%20aligning%20AI%20with%20human%20visual%0Aperception%2C%20while%20saliency%20suppression%20is%20a%20non-brain-like%20strategy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18772v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Saliency%20Suppressed%2C%20Semantics%20Surfaced%3A%20Visual%20Transformations%20in%0A%20%20Neural%20Networks%20and%20the%20Brain&entry.906535625=Gustaw%20Opie%C5%82ka%20and%20Jessica%20Loke%20and%20Steven%20Scholte&entry.1292438233=%20%20Deep%20learning%20algorithms%20lack%20human-interpretable%20accounts%20of%20how%20they%0Atransform%20raw%20visual%20input%20into%20a%20robust%20semantic%20understanding%2C%20which%20impedes%0Acomparisons%20between%20different%20architectures%2C%20training%20objectives%2C%20and%20the%20human%0Abrain.%20In%20this%20work%2C%20we%20take%20inspiration%20from%20neuroscience%20and%20employ%0Arepresentational%20approaches%20to%20shed%20light%20on%20how%20neural%20networks%20encode%0Ainformation%20at%20low%20%28visual%20saliency%29%20and%20high%20%28semantic%20similarity%29%20levels%20of%0Aabstraction.%20Moreover%2C%20we%20introduce%20a%20custom%20image%20dataset%20where%20we%0Asystematically%20manipulate%20salient%20and%20semantic%20information.%20We%20find%20that%0AResNets%20are%20more%20sensitive%20to%20saliency%20information%20than%20ViTs%2C%20when%20trained%20with%0Aobject%20classification%20objectives.%20We%20uncover%20that%20networks%20suppress%20saliency%20in%0Aearly%20layers%2C%20a%20process%20enhanced%20by%20natural%20language%20supervision%20%28CLIP%29%20in%0AResNets.%20CLIP%20also%20enhances%20semantic%20encoding%20in%20both%20architectures.%20Finally%2C%0Awe%20show%20that%20semantic%20encoding%20is%20a%20key%20factor%20in%20aligning%20AI%20with%20human%20visual%0Aperception%2C%20while%20saliency%20suppression%20is%20a%20non-brain-like%20strategy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18772v1&entry.124074799=Read"},
{"title": "Annotating Ambiguous Images: General Annotation Strategy for\n  High-Quality Data with Real-World Biomedical Validation", "author": "Lars Schmarje and Vasco Grossmann and Claudius Zelenka and Johannes Br\u00fcnger and Reinhard Koch", "abstract": "  In the field of image classification, existing methods often struggle with\nbiased or ambiguous data, a prevalent issue in real-world scenarios. Current\nstrategies, including semi-supervised learning and class blending, offer\npartial solutions but lack a definitive resolution. Addressing this gap, our\npaper introduces a novel strategy for generating high-quality labels in\nchallenging datasets. Central to our approach is a clearly designed flowchart,\nbased on a broad literature review, which enables the creation of reliable\nlabels. We validate our methodology through a rigorous real-world test case in\nthe biomedical field, specifically in deducing height reduction from vertebral\nimaging. Our empirical study, leveraging over 250,000 annotations, demonstrates\nthe effectiveness of our strategies decisions compared to their alternatives.\n", "link": "http://arxiv.org/abs/2306.12189v2", "date": "2024-04-29", "relevancy": 2.1574, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5911}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5377}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5203}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Annotating%20Ambiguous%20Images%3A%20General%20Annotation%20Strategy%20for%0A%20%20High-Quality%20Data%20with%20Real-World%20Biomedical%20Validation&body=Title%3A%20Annotating%20Ambiguous%20Images%3A%20General%20Annotation%20Strategy%20for%0A%20%20High-Quality%20Data%20with%20Real-World%20Biomedical%20Validation%0AAuthor%3A%20Lars%20Schmarje%20and%20Vasco%20Grossmann%20and%20Claudius%20Zelenka%20and%20Johannes%20Br%C3%BCnger%20and%20Reinhard%20Koch%0AAbstract%3A%20%20%20In%20the%20field%20of%20image%20classification%2C%20existing%20methods%20often%20struggle%20with%0Abiased%20or%20ambiguous%20data%2C%20a%20prevalent%20issue%20in%20real-world%20scenarios.%20Current%0Astrategies%2C%20including%20semi-supervised%20learning%20and%20class%20blending%2C%20offer%0Apartial%20solutions%20but%20lack%20a%20definitive%20resolution.%20Addressing%20this%20gap%2C%20our%0Apaper%20introduces%20a%20novel%20strategy%20for%20generating%20high-quality%20labels%20in%0Achallenging%20datasets.%20Central%20to%20our%20approach%20is%20a%20clearly%20designed%20flowchart%2C%0Abased%20on%20a%20broad%20literature%20review%2C%20which%20enables%20the%20creation%20of%20reliable%0Alabels.%20We%20validate%20our%20methodology%20through%20a%20rigorous%20real-world%20test%20case%20in%0Athe%20biomedical%20field%2C%20specifically%20in%20deducing%20height%20reduction%20from%20vertebral%0Aimaging.%20Our%20empirical%20study%2C%20leveraging%20over%20250%2C000%20annotations%2C%20demonstrates%0Athe%20effectiveness%20of%20our%20strategies%20decisions%20compared%20to%20their%20alternatives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.12189v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Annotating%20Ambiguous%20Images%3A%20General%20Annotation%20Strategy%20for%0A%20%20High-Quality%20Data%20with%20Real-World%20Biomedical%20Validation&entry.906535625=Lars%20Schmarje%20and%20Vasco%20Grossmann%20and%20Claudius%20Zelenka%20and%20Johannes%20Br%C3%BCnger%20and%20Reinhard%20Koch&entry.1292438233=%20%20In%20the%20field%20of%20image%20classification%2C%20existing%20methods%20often%20struggle%20with%0Abiased%20or%20ambiguous%20data%2C%20a%20prevalent%20issue%20in%20real-world%20scenarios.%20Current%0Astrategies%2C%20including%20semi-supervised%20learning%20and%20class%20blending%2C%20offer%0Apartial%20solutions%20but%20lack%20a%20definitive%20resolution.%20Addressing%20this%20gap%2C%20our%0Apaper%20introduces%20a%20novel%20strategy%20for%20generating%20high-quality%20labels%20in%0Achallenging%20datasets.%20Central%20to%20our%20approach%20is%20a%20clearly%20designed%20flowchart%2C%0Abased%20on%20a%20broad%20literature%20review%2C%20which%20enables%20the%20creation%20of%20reliable%0Alabels.%20We%20validate%20our%20methodology%20through%20a%20rigorous%20real-world%20test%20case%20in%0Athe%20biomedical%20field%2C%20specifically%20in%20deducing%20height%20reduction%20from%20vertebral%0Aimaging.%20Our%20empirical%20study%2C%20leveraging%20over%20250%2C000%20annotations%2C%20demonstrates%0Athe%20effectiveness%20of%20our%20strategies%20decisions%20compared%20to%20their%20alternatives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.12189v2&entry.124074799=Read"},
{"title": "Radarize: Enhancing Radar SLAM with Generalizable Doppler-Based Odometry", "author": "Emerson Sie and Xinyu Wu and Heyu Guo and Deepak Vasisht", "abstract": "  Millimeter-wave (mmWave) radar is increasingly being considered as an\nalternative to optical sensors for robotic primitives like simultaneous\nlocalization and mapping (SLAM). While mmWave radar overcomes some limitations\nof optical sensors, such as occlusions, poor lighting conditions, and privacy\nconcerns, it also faces unique challenges, such as missed obstacles due to\nspecular reflections or fake objects due to multipath. To address these\nchallenges, we propose Radarize, a self-contained SLAM pipeline that uses only\na commodity single-chip mmWave radar. Our radar-native approach uses techniques\nsuch as Doppler shift-based odometry and multipath artifact suppression to\nimprove performance. We evaluate our method on a large dataset of 146\ntrajectories spanning 4 buildings and mounted on 3 different platforms,\ntotaling approximately 4.7 Km of travel distance. Our results show that our\nmethod outperforms state-of-the-art radar and radar-inertial approaches by\napproximately 5x in terms of odometry and 8x in terms of end-to-end SLAM, as\nmeasured by absolute trajectory error (ATE), without the need for additional\nsensors such as IMUs or wheel encoders.\n", "link": "http://arxiv.org/abs/2311.11260v2", "date": "2024-04-29", "relevancy": 2.1529, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5763}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5724}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4888}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Radarize%3A%20Enhancing%20Radar%20SLAM%20with%20Generalizable%20Doppler-Based%20Odometry&body=Title%3A%20Radarize%3A%20Enhancing%20Radar%20SLAM%20with%20Generalizable%20Doppler-Based%20Odometry%0AAuthor%3A%20Emerson%20Sie%20and%20Xinyu%20Wu%20and%20Heyu%20Guo%20and%20Deepak%20Vasisht%0AAbstract%3A%20%20%20Millimeter-wave%20%28mmWave%29%20radar%20is%20increasingly%20being%20considered%20as%20an%0Aalternative%20to%20optical%20sensors%20for%20robotic%20primitives%20like%20simultaneous%0Alocalization%20and%20mapping%20%28SLAM%29.%20While%20mmWave%20radar%20overcomes%20some%20limitations%0Aof%20optical%20sensors%2C%20such%20as%20occlusions%2C%20poor%20lighting%20conditions%2C%20and%20privacy%0Aconcerns%2C%20it%20also%20faces%20unique%20challenges%2C%20such%20as%20missed%20obstacles%20due%20to%0Aspecular%20reflections%20or%20fake%20objects%20due%20to%20multipath.%20To%20address%20these%0Achallenges%2C%20we%20propose%20Radarize%2C%20a%20self-contained%20SLAM%20pipeline%20that%20uses%20only%0Aa%20commodity%20single-chip%20mmWave%20radar.%20Our%20radar-native%20approach%20uses%20techniques%0Asuch%20as%20Doppler%20shift-based%20odometry%20and%20multipath%20artifact%20suppression%20to%0Aimprove%20performance.%20We%20evaluate%20our%20method%20on%20a%20large%20dataset%20of%20146%0Atrajectories%20spanning%204%20buildings%20and%20mounted%20on%203%20different%20platforms%2C%0Atotaling%20approximately%204.7%20Km%20of%20travel%20distance.%20Our%20results%20show%20that%20our%0Amethod%20outperforms%20state-of-the-art%20radar%20and%20radar-inertial%20approaches%20by%0Aapproximately%205x%20in%20terms%20of%20odometry%20and%208x%20in%20terms%20of%20end-to-end%20SLAM%2C%20as%0Ameasured%20by%20absolute%20trajectory%20error%20%28ATE%29%2C%20without%20the%20need%20for%20additional%0Asensors%20such%20as%20IMUs%20or%20wheel%20encoders.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.11260v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Radarize%3A%20Enhancing%20Radar%20SLAM%20with%20Generalizable%20Doppler-Based%20Odometry&entry.906535625=Emerson%20Sie%20and%20Xinyu%20Wu%20and%20Heyu%20Guo%20and%20Deepak%20Vasisht&entry.1292438233=%20%20Millimeter-wave%20%28mmWave%29%20radar%20is%20increasingly%20being%20considered%20as%20an%0Aalternative%20to%20optical%20sensors%20for%20robotic%20primitives%20like%20simultaneous%0Alocalization%20and%20mapping%20%28SLAM%29.%20While%20mmWave%20radar%20overcomes%20some%20limitations%0Aof%20optical%20sensors%2C%20such%20as%20occlusions%2C%20poor%20lighting%20conditions%2C%20and%20privacy%0Aconcerns%2C%20it%20also%20faces%20unique%20challenges%2C%20such%20as%20missed%20obstacles%20due%20to%0Aspecular%20reflections%20or%20fake%20objects%20due%20to%20multipath.%20To%20address%20these%0Achallenges%2C%20we%20propose%20Radarize%2C%20a%20self-contained%20SLAM%20pipeline%20that%20uses%20only%0Aa%20commodity%20single-chip%20mmWave%20radar.%20Our%20radar-native%20approach%20uses%20techniques%0Asuch%20as%20Doppler%20shift-based%20odometry%20and%20multipath%20artifact%20suppression%20to%0Aimprove%20performance.%20We%20evaluate%20our%20method%20on%20a%20large%20dataset%20of%20146%0Atrajectories%20spanning%204%20buildings%20and%20mounted%20on%203%20different%20platforms%2C%0Atotaling%20approximately%204.7%20Km%20of%20travel%20distance.%20Our%20results%20show%20that%20our%0Amethod%20outperforms%20state-of-the-art%20radar%20and%20radar-inertial%20approaches%20by%0Aapproximately%205x%20in%20terms%20of%20odometry%20and%208x%20in%20terms%20of%20end-to-end%20SLAM%2C%20as%0Ameasured%20by%20absolute%20trajectory%20error%20%28ATE%29%2C%20without%20the%20need%20for%20additional%0Asensors%20such%20as%20IMUs%20or%20wheel%20encoders.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.11260v2&entry.124074799=Read"},
{"title": "Leveraging PointNet and PointNet++ for Lyft Point Cloud Classification\n  Challenge", "author": "Rajat K. Doshi", "abstract": "  This study investigates the application of PointNet and PointNet++ in the\nclassification of LiDAR-generated point cloud data, a critical component for\nachieving fully autonomous vehicles. Utilizing a modified dataset from the Lyft\n3D Object Detection Challenge, we examine the models' capabilities to handle\ndynamic and complex environments essential for autonomous navigation. Our\nanalysis shows that PointNet and PointNet++ achieved accuracy rates of 79.53%\nand 84.24%, respectively. These results underscore the models' robustness in\ninterpreting intricate environmental data, which is pivotal for the safety and\nefficiency of autonomous vehicles. Moreover, the enhanced detection accuracy,\nparticularly in distinguishing pedestrians from other objects, highlights the\npotential of these models to contribute substantially to the advancement of\nautonomous vehicle technology.\n", "link": "http://arxiv.org/abs/2404.18665v1", "date": "2024-04-29", "relevancy": 2.1401, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5615}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5173}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5131}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Leveraging%20PointNet%20and%20PointNet%2B%2B%20for%20Lyft%20Point%20Cloud%20Classification%0A%20%20Challenge&body=Title%3A%20Leveraging%20PointNet%20and%20PointNet%2B%2B%20for%20Lyft%20Point%20Cloud%20Classification%0A%20%20Challenge%0AAuthor%3A%20Rajat%20K.%20Doshi%0AAbstract%3A%20%20%20This%20study%20investigates%20the%20application%20of%20PointNet%20and%20PointNet%2B%2B%20in%20the%0Aclassification%20of%20LiDAR-generated%20point%20cloud%20data%2C%20a%20critical%20component%20for%0Aachieving%20fully%20autonomous%20vehicles.%20Utilizing%20a%20modified%20dataset%20from%20the%20Lyft%0A3D%20Object%20Detection%20Challenge%2C%20we%20examine%20the%20models%27%20capabilities%20to%20handle%0Adynamic%20and%20complex%20environments%20essential%20for%20autonomous%20navigation.%20Our%0Aanalysis%20shows%20that%20PointNet%20and%20PointNet%2B%2B%20achieved%20accuracy%20rates%20of%2079.53%25%0Aand%2084.24%25%2C%20respectively.%20These%20results%20underscore%20the%20models%27%20robustness%20in%0Ainterpreting%20intricate%20environmental%20data%2C%20which%20is%20pivotal%20for%20the%20safety%20and%0Aefficiency%20of%20autonomous%20vehicles.%20Moreover%2C%20the%20enhanced%20detection%20accuracy%2C%0Aparticularly%20in%20distinguishing%20pedestrians%20from%20other%20objects%2C%20highlights%20the%0Apotential%20of%20these%20models%20to%20contribute%20substantially%20to%20the%20advancement%20of%0Aautonomous%20vehicle%20technology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18665v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20PointNet%20and%20PointNet%2B%2B%20for%20Lyft%20Point%20Cloud%20Classification%0A%20%20Challenge&entry.906535625=Rajat%20K.%20Doshi&entry.1292438233=%20%20This%20study%20investigates%20the%20application%20of%20PointNet%20and%20PointNet%2B%2B%20in%20the%0Aclassification%20of%20LiDAR-generated%20point%20cloud%20data%2C%20a%20critical%20component%20for%0Aachieving%20fully%20autonomous%20vehicles.%20Utilizing%20a%20modified%20dataset%20from%20the%20Lyft%0A3D%20Object%20Detection%20Challenge%2C%20we%20examine%20the%20models%27%20capabilities%20to%20handle%0Adynamic%20and%20complex%20environments%20essential%20for%20autonomous%20navigation.%20Our%0Aanalysis%20shows%20that%20PointNet%20and%20PointNet%2B%2B%20achieved%20accuracy%20rates%20of%2079.53%25%0Aand%2084.24%25%2C%20respectively.%20These%20results%20underscore%20the%20models%27%20robustness%20in%0Ainterpreting%20intricate%20environmental%20data%2C%20which%20is%20pivotal%20for%20the%20safety%20and%0Aefficiency%20of%20autonomous%20vehicles.%20Moreover%2C%20the%20enhanced%20detection%20accuracy%2C%0Aparticularly%20in%20distinguishing%20pedestrians%20from%20other%20objects%2C%20highlights%20the%0Apotential%20of%20these%20models%20to%20contribute%20substantially%20to%20the%20advancement%20of%0Aautonomous%20vehicle%20technology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18665v1&entry.124074799=Read"},
{"title": "PLLaVA : Parameter-free LLaVA Extension from Images to Videos for Video\n  Dense Captioning", "author": "Lin Xu and Yilin Zhao and Daquan Zhou and Zhijie Lin and See Kiong Ng and Jiashi Feng", "abstract": "  Vision-language pre-training has significantly elevated performance across a\nwide range of image-language applications. Yet, the pre-training process for\nvideo-related tasks demands exceptionally large computational and data\nresources, which hinders the progress of video-language models. This paper\ninvestigates a straight-forward, highly efficient, and resource-light approach\nto adapting an existing image-language pre-trained model for dense video\nunderstanding. Our preliminary experiments reveal that directly fine-tuning\npre-trained image-language models with multiple frames as inputs on video\ndatasets leads to performance saturation or even a drop. Our further\ninvestigation reveals that it is largely attributed to the bias of learned\nhigh-norm visual features. Motivated by this finding, we propose a simple but\neffective pooling strategy to smooth the feature distribution along the\ntemporal dimension and thus reduce the dominant impacts from the extreme\nfeatures. The new model is termed Pooling LLaVA, or PLLaVA in short. PLLaVA\nachieves new state-of-the-art performance on modern benchmark datasets for both\nvideo question-answer and captioning tasks. Notably, on the recent popular\nVideoChatGPT benchmark, PLLaVA achieves a score of 3.48 out of 5 on average of\nfive evaluated dimensions, exceeding the previous SOTA results from GPT4V\n(IG-VLM) by 9%. On the latest multi-choice benchmark MVBench, PLLaVA achieves\n58.1% accuracy on average across 20 sub-tasks, 14.5% higher than GPT4V\n(IG-VLM). Code is available at https://pllava.github.io/\n", "link": "http://arxiv.org/abs/2404.16994v2", "date": "2024-04-29", "relevancy": 2.1397, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5449}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5389}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.527}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PLLaVA%20%3A%20Parameter-free%20LLaVA%20Extension%20from%20Images%20to%20Videos%20for%20Video%0A%20%20Dense%20Captioning&body=Title%3A%20PLLaVA%20%3A%20Parameter-free%20LLaVA%20Extension%20from%20Images%20to%20Videos%20for%20Video%0A%20%20Dense%20Captioning%0AAuthor%3A%20Lin%20Xu%20and%20Yilin%20Zhao%20and%20Daquan%20Zhou%20and%20Zhijie%20Lin%20and%20See%20Kiong%20Ng%20and%20Jiashi%20Feng%0AAbstract%3A%20%20%20Vision-language%20pre-training%20has%20significantly%20elevated%20performance%20across%20a%0Awide%20range%20of%20image-language%20applications.%20Yet%2C%20the%20pre-training%20process%20for%0Avideo-related%20tasks%20demands%20exceptionally%20large%20computational%20and%20data%0Aresources%2C%20which%20hinders%20the%20progress%20of%20video-language%20models.%20This%20paper%0Ainvestigates%20a%20straight-forward%2C%20highly%20efficient%2C%20and%20resource-light%20approach%0Ato%20adapting%20an%20existing%20image-language%20pre-trained%20model%20for%20dense%20video%0Aunderstanding.%20Our%20preliminary%20experiments%20reveal%20that%20directly%20fine-tuning%0Apre-trained%20image-language%20models%20with%20multiple%20frames%20as%20inputs%20on%20video%0Adatasets%20leads%20to%20performance%20saturation%20or%20even%20a%20drop.%20Our%20further%0Ainvestigation%20reveals%20that%20it%20is%20largely%20attributed%20to%20the%20bias%20of%20learned%0Ahigh-norm%20visual%20features.%20Motivated%20by%20this%20finding%2C%20we%20propose%20a%20simple%20but%0Aeffective%20pooling%20strategy%20to%20smooth%20the%20feature%20distribution%20along%20the%0Atemporal%20dimension%20and%20thus%20reduce%20the%20dominant%20impacts%20from%20the%20extreme%0Afeatures.%20The%20new%20model%20is%20termed%20Pooling%20LLaVA%2C%20or%20PLLaVA%20in%20short.%20PLLaVA%0Aachieves%20new%20state-of-the-art%20performance%20on%20modern%20benchmark%20datasets%20for%20both%0Avideo%20question-answer%20and%20captioning%20tasks.%20Notably%2C%20on%20the%20recent%20popular%0AVideoChatGPT%20benchmark%2C%20PLLaVA%20achieves%20a%20score%20of%203.48%20out%20of%205%20on%20average%20of%0Afive%20evaluated%20dimensions%2C%20exceeding%20the%20previous%20SOTA%20results%20from%20GPT4V%0A%28IG-VLM%29%20by%209%25.%20On%20the%20latest%20multi-choice%20benchmark%20MVBench%2C%20PLLaVA%20achieves%0A58.1%25%20accuracy%20on%20average%20across%2020%20sub-tasks%2C%2014.5%25%20higher%20than%20GPT4V%0A%28IG-VLM%29.%20Code%20is%20available%20at%20https%3A//pllava.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16994v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PLLaVA%20%3A%20Parameter-free%20LLaVA%20Extension%20from%20Images%20to%20Videos%20for%20Video%0A%20%20Dense%20Captioning&entry.906535625=Lin%20Xu%20and%20Yilin%20Zhao%20and%20Daquan%20Zhou%20and%20Zhijie%20Lin%20and%20See%20Kiong%20Ng%20and%20Jiashi%20Feng&entry.1292438233=%20%20Vision-language%20pre-training%20has%20significantly%20elevated%20performance%20across%20a%0Awide%20range%20of%20image-language%20applications.%20Yet%2C%20the%20pre-training%20process%20for%0Avideo-related%20tasks%20demands%20exceptionally%20large%20computational%20and%20data%0Aresources%2C%20which%20hinders%20the%20progress%20of%20video-language%20models.%20This%20paper%0Ainvestigates%20a%20straight-forward%2C%20highly%20efficient%2C%20and%20resource-light%20approach%0Ato%20adapting%20an%20existing%20image-language%20pre-trained%20model%20for%20dense%20video%0Aunderstanding.%20Our%20preliminary%20experiments%20reveal%20that%20directly%20fine-tuning%0Apre-trained%20image-language%20models%20with%20multiple%20frames%20as%20inputs%20on%20video%0Adatasets%20leads%20to%20performance%20saturation%20or%20even%20a%20drop.%20Our%20further%0Ainvestigation%20reveals%20that%20it%20is%20largely%20attributed%20to%20the%20bias%20of%20learned%0Ahigh-norm%20visual%20features.%20Motivated%20by%20this%20finding%2C%20we%20propose%20a%20simple%20but%0Aeffective%20pooling%20strategy%20to%20smooth%20the%20feature%20distribution%20along%20the%0Atemporal%20dimension%20and%20thus%20reduce%20the%20dominant%20impacts%20from%20the%20extreme%0Afeatures.%20The%20new%20model%20is%20termed%20Pooling%20LLaVA%2C%20or%20PLLaVA%20in%20short.%20PLLaVA%0Aachieves%20new%20state-of-the-art%20performance%20on%20modern%20benchmark%20datasets%20for%20both%0Avideo%20question-answer%20and%20captioning%20tasks.%20Notably%2C%20on%20the%20recent%20popular%0AVideoChatGPT%20benchmark%2C%20PLLaVA%20achieves%20a%20score%20of%203.48%20out%20of%205%20on%20average%20of%0Afive%20evaluated%20dimensions%2C%20exceeding%20the%20previous%20SOTA%20results%20from%20GPT4V%0A%28IG-VLM%29%20by%209%25.%20On%20the%20latest%20multi-choice%20benchmark%20MVBench%2C%20PLLaVA%20achieves%0A58.1%25%20accuracy%20on%20average%20across%2020%20sub-tasks%2C%2014.5%25%20higher%20than%20GPT4V%0A%28IG-VLM%29.%20Code%20is%20available%20at%20https%3A//pllava.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16994v2&entry.124074799=Read"},
{"title": "Bengali Document Layout Analysis -- A YOLOV8 Based Ensembling Approach", "author": "Nazmus Sakib Ahmed and Saad Sakib Noor and Ashraful Islam Shanto Sikder and Abhijit Paul", "abstract": "  This paper focuses on enhancing Bengali Document Layout Analysis (DLA) using\nthe YOLOv8 model and innovative post-processing techniques. We tackle\nchallenges unique to the complex Bengali script by employing data augmentation\nfor model robustness. After meticulous validation set evaluation, we fine-tune\nour approach on the complete dataset, leading to a two-stage prediction\nstrategy for accurate element segmentation. Our ensemble model, combined with\npost-processing, outperforms individual base architectures, addressing issues\nidentified in the BaDLAD dataset. By leveraging this approach, we aim to\nadvance Bengali document analysis, contributing to improved OCR and document\ncomprehension and BaDLAD serves as a foundational resource for this endeavor,\naiding future research in the field. Furthermore, our experiments provided key\ninsights to incorporate new strategies into the established solution.\n", "link": "http://arxiv.org/abs/2309.00848v3", "date": "2024-04-29", "relevancy": 2.1366, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5745}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5064}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5048}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Bengali%20Document%20Layout%20Analysis%20--%20A%20YOLOV8%20Based%20Ensembling%20Approach&body=Title%3A%20Bengali%20Document%20Layout%20Analysis%20--%20A%20YOLOV8%20Based%20Ensembling%20Approach%0AAuthor%3A%20Nazmus%20Sakib%20Ahmed%20and%20Saad%20Sakib%20Noor%20and%20Ashraful%20Islam%20Shanto%20Sikder%20and%20Abhijit%20Paul%0AAbstract%3A%20%20%20This%20paper%20focuses%20on%20enhancing%20Bengali%20Document%20Layout%20Analysis%20%28DLA%29%20using%0Athe%20YOLOv8%20model%20and%20innovative%20post-processing%20techniques.%20We%20tackle%0Achallenges%20unique%20to%20the%20complex%20Bengali%20script%20by%20employing%20data%20augmentation%0Afor%20model%20robustness.%20After%20meticulous%20validation%20set%20evaluation%2C%20we%20fine-tune%0Aour%20approach%20on%20the%20complete%20dataset%2C%20leading%20to%20a%20two-stage%20prediction%0Astrategy%20for%20accurate%20element%20segmentation.%20Our%20ensemble%20model%2C%20combined%20with%0Apost-processing%2C%20outperforms%20individual%20base%20architectures%2C%20addressing%20issues%0Aidentified%20in%20the%20BaDLAD%20dataset.%20By%20leveraging%20this%20approach%2C%20we%20aim%20to%0Aadvance%20Bengali%20document%20analysis%2C%20contributing%20to%20improved%20OCR%20and%20document%0Acomprehension%20and%20BaDLAD%20serves%20as%20a%20foundational%20resource%20for%20this%20endeavor%2C%0Aaiding%20future%20research%20in%20the%20field.%20Furthermore%2C%20our%20experiments%20provided%20key%0Ainsights%20to%20incorporate%20new%20strategies%20into%20the%20established%20solution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.00848v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bengali%20Document%20Layout%20Analysis%20--%20A%20YOLOV8%20Based%20Ensembling%20Approach&entry.906535625=Nazmus%20Sakib%20Ahmed%20and%20Saad%20Sakib%20Noor%20and%20Ashraful%20Islam%20Shanto%20Sikder%20and%20Abhijit%20Paul&entry.1292438233=%20%20This%20paper%20focuses%20on%20enhancing%20Bengali%20Document%20Layout%20Analysis%20%28DLA%29%20using%0Athe%20YOLOv8%20model%20and%20innovative%20post-processing%20techniques.%20We%20tackle%0Achallenges%20unique%20to%20the%20complex%20Bengali%20script%20by%20employing%20data%20augmentation%0Afor%20model%20robustness.%20After%20meticulous%20validation%20set%20evaluation%2C%20we%20fine-tune%0Aour%20approach%20on%20the%20complete%20dataset%2C%20leading%20to%20a%20two-stage%20prediction%0Astrategy%20for%20accurate%20element%20segmentation.%20Our%20ensemble%20model%2C%20combined%20with%0Apost-processing%2C%20outperforms%20individual%20base%20architectures%2C%20addressing%20issues%0Aidentified%20in%20the%20BaDLAD%20dataset.%20By%20leveraging%20this%20approach%2C%20we%20aim%20to%0Aadvance%20Bengali%20document%20analysis%2C%20contributing%20to%20improved%20OCR%20and%20document%0Acomprehension%20and%20BaDLAD%20serves%20as%20a%20foundational%20resource%20for%20this%20endeavor%2C%0Aaiding%20future%20research%20in%20the%20field.%20Furthermore%2C%20our%20experiments%20provided%20key%0Ainsights%20to%20incorporate%20new%20strategies%20into%20the%20established%20solution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.00848v3&entry.124074799=Read"},
{"title": "Hide and Seek: How Does Watermarking Impact Face Recognition?", "author": "Yuguang Yao and Steven Grosz and Sijia Liu and Anil Jain", "abstract": "  The recent progress in generative models has revolutionized the synthesis of\nhighly realistic images, including face images. This technological development\nhas undoubtedly helped face recognition, such as training data augmentation for\nhigher recognition accuracy and data privacy. However, it has also introduced\nnovel challenges concerning the responsible use and proper attribution of\ncomputer generated images. We investigate the impact of digital watermarking, a\ntechnique for embedding ownership signatures into images, on the effectiveness\nof face recognition models. We propose a comprehensive pipeline that integrates\nface image generation, watermarking, and face recognition to systematically\nexamine this question. The proposed watermarking scheme, based on an\nencoder-decoder architecture, successfully embeds and recovers signatures from\nboth real and synthetic face images while preserving their visual fidelity.\nThrough extensive experiments, we unveil that while watermarking enables robust\nimage attribution, it results in a slight decline in face recognition accuracy,\nparticularly evident for face images with challenging poses and expressions.\nAdditionally, we find that directly training face recognition models on\nwatermarked images offers only a limited alleviation of this performance\ndecline. Our findings underscore the intricate trade off between watermarking\nand face recognition accuracy. This work represents a pivotal step towards the\nresponsible utilization of generative models in face recognition and serves to\ninitiate discussions regarding the broader implications of watermarking in\nbiometrics.\n", "link": "http://arxiv.org/abs/2404.18890v1", "date": "2024-04-29", "relevancy": 2.1325, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5459}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.526}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5189}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Hide%20and%20Seek%3A%20How%20Does%20Watermarking%20Impact%20Face%20Recognition%3F&body=Title%3A%20Hide%20and%20Seek%3A%20How%20Does%20Watermarking%20Impact%20Face%20Recognition%3F%0AAuthor%3A%20Yuguang%20Yao%20and%20Steven%20Grosz%20and%20Sijia%20Liu%20and%20Anil%20Jain%0AAbstract%3A%20%20%20The%20recent%20progress%20in%20generative%20models%20has%20revolutionized%20the%20synthesis%20of%0Ahighly%20realistic%20images%2C%20including%20face%20images.%20This%20technological%20development%0Ahas%20undoubtedly%20helped%20face%20recognition%2C%20such%20as%20training%20data%20augmentation%20for%0Ahigher%20recognition%20accuracy%20and%20data%20privacy.%20However%2C%20it%20has%20also%20introduced%0Anovel%20challenges%20concerning%20the%20responsible%20use%20and%20proper%20attribution%20of%0Acomputer%20generated%20images.%20We%20investigate%20the%20impact%20of%20digital%20watermarking%2C%20a%0Atechnique%20for%20embedding%20ownership%20signatures%20into%20images%2C%20on%20the%20effectiveness%0Aof%20face%20recognition%20models.%20We%20propose%20a%20comprehensive%20pipeline%20that%20integrates%0Aface%20image%20generation%2C%20watermarking%2C%20and%20face%20recognition%20to%20systematically%0Aexamine%20this%20question.%20The%20proposed%20watermarking%20scheme%2C%20based%20on%20an%0Aencoder-decoder%20architecture%2C%20successfully%20embeds%20and%20recovers%20signatures%20from%0Aboth%20real%20and%20synthetic%20face%20images%20while%20preserving%20their%20visual%20fidelity.%0AThrough%20extensive%20experiments%2C%20we%20unveil%20that%20while%20watermarking%20enables%20robust%0Aimage%20attribution%2C%20it%20results%20in%20a%20slight%20decline%20in%20face%20recognition%20accuracy%2C%0Aparticularly%20evident%20for%20face%20images%20with%20challenging%20poses%20and%20expressions.%0AAdditionally%2C%20we%20find%20that%20directly%20training%20face%20recognition%20models%20on%0Awatermarked%20images%20offers%20only%20a%20limited%20alleviation%20of%20this%20performance%0Adecline.%20Our%20findings%20underscore%20the%20intricate%20trade%20off%20between%20watermarking%0Aand%20face%20recognition%20accuracy.%20This%20work%20represents%20a%20pivotal%20step%20towards%20the%0Aresponsible%20utilization%20of%20generative%20models%20in%20face%20recognition%20and%20serves%20to%0Ainitiate%20discussions%20regarding%20the%20broader%20implications%20of%20watermarking%20in%0Abiometrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18890v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hide%20and%20Seek%3A%20How%20Does%20Watermarking%20Impact%20Face%20Recognition%3F&entry.906535625=Yuguang%20Yao%20and%20Steven%20Grosz%20and%20Sijia%20Liu%20and%20Anil%20Jain&entry.1292438233=%20%20The%20recent%20progress%20in%20generative%20models%20has%20revolutionized%20the%20synthesis%20of%0Ahighly%20realistic%20images%2C%20including%20face%20images.%20This%20technological%20development%0Ahas%20undoubtedly%20helped%20face%20recognition%2C%20such%20as%20training%20data%20augmentation%20for%0Ahigher%20recognition%20accuracy%20and%20data%20privacy.%20However%2C%20it%20has%20also%20introduced%0Anovel%20challenges%20concerning%20the%20responsible%20use%20and%20proper%20attribution%20of%0Acomputer%20generated%20images.%20We%20investigate%20the%20impact%20of%20digital%20watermarking%2C%20a%0Atechnique%20for%20embedding%20ownership%20signatures%20into%20images%2C%20on%20the%20effectiveness%0Aof%20face%20recognition%20models.%20We%20propose%20a%20comprehensive%20pipeline%20that%20integrates%0Aface%20image%20generation%2C%20watermarking%2C%20and%20face%20recognition%20to%20systematically%0Aexamine%20this%20question.%20The%20proposed%20watermarking%20scheme%2C%20based%20on%20an%0Aencoder-decoder%20architecture%2C%20successfully%20embeds%20and%20recovers%20signatures%20from%0Aboth%20real%20and%20synthetic%20face%20images%20while%20preserving%20their%20visual%20fidelity.%0AThrough%20extensive%20experiments%2C%20we%20unveil%20that%20while%20watermarking%20enables%20robust%0Aimage%20attribution%2C%20it%20results%20in%20a%20slight%20decline%20in%20face%20recognition%20accuracy%2C%0Aparticularly%20evident%20for%20face%20images%20with%20challenging%20poses%20and%20expressions.%0AAdditionally%2C%20we%20find%20that%20directly%20training%20face%20recognition%20models%20on%0Awatermarked%20images%20offers%20only%20a%20limited%20alleviation%20of%20this%20performance%0Adecline.%20Our%20findings%20underscore%20the%20intricate%20trade%20off%20between%20watermarking%0Aand%20face%20recognition%20accuracy.%20This%20work%20represents%20a%20pivotal%20step%20towards%20the%0Aresponsible%20utilization%20of%20generative%20models%20in%20face%20recognition%20and%20serves%20to%0Ainitiate%20discussions%20regarding%20the%20broader%20implications%20of%20watermarking%20in%0Abiometrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18890v1&entry.124074799=Read"},
{"title": "CSTalk: Correlation Supervised Speech-driven 3D Emotional Facial\n  Animation Generation", "author": "Xiangyu Liang and Wenlin Zhuang and Tianyong Wang and Guangxing Geng and Guangyue Geng and Haifeng Xia and Siyu Xia", "abstract": "  Speech-driven 3D facial animation technology has been developed for years,\nbut its practical application still lacks expectations. The main challenges lie\nin data limitations, lip alignment, and the naturalness of facial expressions.\nAlthough lip alignment has seen many related studies, existing methods struggle\nto synthesize natural and realistic expressions, resulting in a mechanical and\nstiff appearance of facial animations. Even with some research extracting\nemotional features from speech, the randomness of facial movements limits the\neffective expression of emotions. To address this issue, this paper proposes a\nmethod called CSTalk (Correlation Supervised) that models the correlations\namong different regions of facial movements and supervises the training of the\ngenerative model to generate realistic expressions that conform to human facial\nmotion patterns. To generate more intricate animations, we employ a rich set of\ncontrol parameters based on the metahuman character model and capture a dataset\nfor five different emotions. We train a generative network using an autoencoder\nstructure and input an emotion embedding vector to achieve the generation of\nuser-control expressions. Experimental results demonstrate that our method\noutperforms existing state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2404.18604v1", "date": "2024-04-29", "relevancy": 2.0858, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5359}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5159}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5092}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20CSTalk%3A%20Correlation%20Supervised%20Speech-driven%203D%20Emotional%20Facial%0A%20%20Animation%20Generation&body=Title%3A%20CSTalk%3A%20Correlation%20Supervised%20Speech-driven%203D%20Emotional%20Facial%0A%20%20Animation%20Generation%0AAuthor%3A%20Xiangyu%20Liang%20and%20Wenlin%20Zhuang%20and%20Tianyong%20Wang%20and%20Guangxing%20Geng%20and%20Guangyue%20Geng%20and%20Haifeng%20Xia%20and%20Siyu%20Xia%0AAbstract%3A%20%20%20Speech-driven%203D%20facial%20animation%20technology%20has%20been%20developed%20for%20years%2C%0Abut%20its%20practical%20application%20still%20lacks%20expectations.%20The%20main%20challenges%20lie%0Ain%20data%20limitations%2C%20lip%20alignment%2C%20and%20the%20naturalness%20of%20facial%20expressions.%0AAlthough%20lip%20alignment%20has%20seen%20many%20related%20studies%2C%20existing%20methods%20struggle%0Ato%20synthesize%20natural%20and%20realistic%20expressions%2C%20resulting%20in%20a%20mechanical%20and%0Astiff%20appearance%20of%20facial%20animations.%20Even%20with%20some%20research%20extracting%0Aemotional%20features%20from%20speech%2C%20the%20randomness%20of%20facial%20movements%20limits%20the%0Aeffective%20expression%20of%20emotions.%20To%20address%20this%20issue%2C%20this%20paper%20proposes%20a%0Amethod%20called%20CSTalk%20%28Correlation%20Supervised%29%20that%20models%20the%20correlations%0Aamong%20different%20regions%20of%20facial%20movements%20and%20supervises%20the%20training%20of%20the%0Agenerative%20model%20to%20generate%20realistic%20expressions%20that%20conform%20to%20human%20facial%0Amotion%20patterns.%20To%20generate%20more%20intricate%20animations%2C%20we%20employ%20a%20rich%20set%20of%0Acontrol%20parameters%20based%20on%20the%20metahuman%20character%20model%20and%20capture%20a%20dataset%0Afor%20five%20different%20emotions.%20We%20train%20a%20generative%20network%20using%20an%20autoencoder%0Astructure%20and%20input%20an%20emotion%20embedding%20vector%20to%20achieve%20the%20generation%20of%0Auser-control%20expressions.%20Experimental%20results%20demonstrate%20that%20our%20method%0Aoutperforms%20existing%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18604v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CSTalk%3A%20Correlation%20Supervised%20Speech-driven%203D%20Emotional%20Facial%0A%20%20Animation%20Generation&entry.906535625=Xiangyu%20Liang%20and%20Wenlin%20Zhuang%20and%20Tianyong%20Wang%20and%20Guangxing%20Geng%20and%20Guangyue%20Geng%20and%20Haifeng%20Xia%20and%20Siyu%20Xia&entry.1292438233=%20%20Speech-driven%203D%20facial%20animation%20technology%20has%20been%20developed%20for%20years%2C%0Abut%20its%20practical%20application%20still%20lacks%20expectations.%20The%20main%20challenges%20lie%0Ain%20data%20limitations%2C%20lip%20alignment%2C%20and%20the%20naturalness%20of%20facial%20expressions.%0AAlthough%20lip%20alignment%20has%20seen%20many%20related%20studies%2C%20existing%20methods%20struggle%0Ato%20synthesize%20natural%20and%20realistic%20expressions%2C%20resulting%20in%20a%20mechanical%20and%0Astiff%20appearance%20of%20facial%20animations.%20Even%20with%20some%20research%20extracting%0Aemotional%20features%20from%20speech%2C%20the%20randomness%20of%20facial%20movements%20limits%20the%0Aeffective%20expression%20of%20emotions.%20To%20address%20this%20issue%2C%20this%20paper%20proposes%20a%0Amethod%20called%20CSTalk%20%28Correlation%20Supervised%29%20that%20models%20the%20correlations%0Aamong%20different%20regions%20of%20facial%20movements%20and%20supervises%20the%20training%20of%20the%0Agenerative%20model%20to%20generate%20realistic%20expressions%20that%20conform%20to%20human%20facial%0Amotion%20patterns.%20To%20generate%20more%20intricate%20animations%2C%20we%20employ%20a%20rich%20set%20of%0Acontrol%20parameters%20based%20on%20the%20metahuman%20character%20model%20and%20capture%20a%20dataset%0Afor%20five%20different%20emotions.%20We%20train%20a%20generative%20network%20using%20an%20autoencoder%0Astructure%20and%20input%20an%20emotion%20embedding%20vector%20to%20achieve%20the%20generation%20of%0Auser-control%20expressions.%20Experimental%20results%20demonstrate%20that%20our%20method%0Aoutperforms%20existing%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18604v1&entry.124074799=Read"},
{"title": "Adaptive Input-image Normalization for Solving the Mode Collapse Problem\n  in GAN-based X-ray Images", "author": "Muhammad Muneeb Saad and Mubashir Husain Rehmani and Ruairi O'Reilly", "abstract": "  Biomedical image datasets can be imbalanced due to the rarity of targeted\ndiseases. Generative Adversarial Networks play a key role in addressing this\nimbalance by enabling the generation of synthetic images to augment datasets.\nIt is important to generate synthetic images that incorporate a diverse range\nof features to accurately represent the distribution of features present in the\ntraining imagery. Furthermore, the absence of diverse features in synthetic\nimages can degrade the performance of machine learning classifiers. The mode\ncollapse problem impacts Generative Adversarial Networks' capacity to generate\ndiversified images. Mode collapse comes in two varieties: intra-class and\ninter-class. In this paper, both varieties of the mode collapse problem are\ninvestigated, and their subsequent impact on the diversity of synthetic X-ray\nimages is evaluated. This work contributes an empirical demonstration of the\nbenefits of integrating the adaptive input-image normalization with the Deep\nConvolutional GAN and Auxiliary Classifier GAN to alleviate the mode collapse\nproblems. Synthetically generated images are utilized for data augmentation and\ntraining a Vision Transformer model. The classification performance of the\nmodel is evaluated using accuracy, recall, and precision scores. Results\ndemonstrate that the DCGAN and the ACGAN with adaptive input-image\nnormalization outperform the DCGAN and ACGAN with un-normalized X-ray images as\nevidenced by the superior diversity scores and classification scores.\n", "link": "http://arxiv.org/abs/2309.12245v3", "date": "2024-04-29", "relevancy": 2.0791, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5383}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5118}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5044}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Input-image%20Normalization%20for%20Solving%20the%20Mode%20Collapse%20Problem%0A%20%20in%20GAN-based%20X-ray%20Images&body=Title%3A%20Adaptive%20Input-image%20Normalization%20for%20Solving%20the%20Mode%20Collapse%20Problem%0A%20%20in%20GAN-based%20X-ray%20Images%0AAuthor%3A%20Muhammad%20Muneeb%20Saad%20and%20Mubashir%20Husain%20Rehmani%20and%20Ruairi%20O%27Reilly%0AAbstract%3A%20%20%20Biomedical%20image%20datasets%20can%20be%20imbalanced%20due%20to%20the%20rarity%20of%20targeted%0Adiseases.%20Generative%20Adversarial%20Networks%20play%20a%20key%20role%20in%20addressing%20this%0Aimbalance%20by%20enabling%20the%20generation%20of%20synthetic%20images%20to%20augment%20datasets.%0AIt%20is%20important%20to%20generate%20synthetic%20images%20that%20incorporate%20a%20diverse%20range%0Aof%20features%20to%20accurately%20represent%20the%20distribution%20of%20features%20present%20in%20the%0Atraining%20imagery.%20Furthermore%2C%20the%20absence%20of%20diverse%20features%20in%20synthetic%0Aimages%20can%20degrade%20the%20performance%20of%20machine%20learning%20classifiers.%20The%20mode%0Acollapse%20problem%20impacts%20Generative%20Adversarial%20Networks%27%20capacity%20to%20generate%0Adiversified%20images.%20Mode%20collapse%20comes%20in%20two%20varieties%3A%20intra-class%20and%0Ainter-class.%20In%20this%20paper%2C%20both%20varieties%20of%20the%20mode%20collapse%20problem%20are%0Ainvestigated%2C%20and%20their%20subsequent%20impact%20on%20the%20diversity%20of%20synthetic%20X-ray%0Aimages%20is%20evaluated.%20This%20work%20contributes%20an%20empirical%20demonstration%20of%20the%0Abenefits%20of%20integrating%20the%20adaptive%20input-image%20normalization%20with%20the%20Deep%0AConvolutional%20GAN%20and%20Auxiliary%20Classifier%20GAN%20to%20alleviate%20the%20mode%20collapse%0Aproblems.%20Synthetically%20generated%20images%20are%20utilized%20for%20data%20augmentation%20and%0Atraining%20a%20Vision%20Transformer%20model.%20The%20classification%20performance%20of%20the%0Amodel%20is%20evaluated%20using%20accuracy%2C%20recall%2C%20and%20precision%20scores.%20Results%0Ademonstrate%20that%20the%20DCGAN%20and%20the%20ACGAN%20with%20adaptive%20input-image%0Anormalization%20outperform%20the%20DCGAN%20and%20ACGAN%20with%20un-normalized%20X-ray%20images%20as%0Aevidenced%20by%20the%20superior%20diversity%20scores%20and%20classification%20scores.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.12245v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Input-image%20Normalization%20for%20Solving%20the%20Mode%20Collapse%20Problem%0A%20%20in%20GAN-based%20X-ray%20Images&entry.906535625=Muhammad%20Muneeb%20Saad%20and%20Mubashir%20Husain%20Rehmani%20and%20Ruairi%20O%27Reilly&entry.1292438233=%20%20Biomedical%20image%20datasets%20can%20be%20imbalanced%20due%20to%20the%20rarity%20of%20targeted%0Adiseases.%20Generative%20Adversarial%20Networks%20play%20a%20key%20role%20in%20addressing%20this%0Aimbalance%20by%20enabling%20the%20generation%20of%20synthetic%20images%20to%20augment%20datasets.%0AIt%20is%20important%20to%20generate%20synthetic%20images%20that%20incorporate%20a%20diverse%20range%0Aof%20features%20to%20accurately%20represent%20the%20distribution%20of%20features%20present%20in%20the%0Atraining%20imagery.%20Furthermore%2C%20the%20absence%20of%20diverse%20features%20in%20synthetic%0Aimages%20can%20degrade%20the%20performance%20of%20machine%20learning%20classifiers.%20The%20mode%0Acollapse%20problem%20impacts%20Generative%20Adversarial%20Networks%27%20capacity%20to%20generate%0Adiversified%20images.%20Mode%20collapse%20comes%20in%20two%20varieties%3A%20intra-class%20and%0Ainter-class.%20In%20this%20paper%2C%20both%20varieties%20of%20the%20mode%20collapse%20problem%20are%0Ainvestigated%2C%20and%20their%20subsequent%20impact%20on%20the%20diversity%20of%20synthetic%20X-ray%0Aimages%20is%20evaluated.%20This%20work%20contributes%20an%20empirical%20demonstration%20of%20the%0Abenefits%20of%20integrating%20the%20adaptive%20input-image%20normalization%20with%20the%20Deep%0AConvolutional%20GAN%20and%20Auxiliary%20Classifier%20GAN%20to%20alleviate%20the%20mode%20collapse%0Aproblems.%20Synthetically%20generated%20images%20are%20utilized%20for%20data%20augmentation%20and%0Atraining%20a%20Vision%20Transformer%20model.%20The%20classification%20performance%20of%20the%0Amodel%20is%20evaluated%20using%20accuracy%2C%20recall%2C%20and%20precision%20scores.%20Results%0Ademonstrate%20that%20the%20DCGAN%20and%20the%20ACGAN%20with%20adaptive%20input-image%0Anormalization%20outperform%20the%20DCGAN%20and%20ACGAN%20with%20un-normalized%20X-ray%20images%20as%0Aevidenced%20by%20the%20superior%20diversity%20scores%20and%20classification%20scores.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.12245v3&entry.124074799=Read"},
{"title": "RBF-PINN: Non-Fourier Positional Embedding in Physics-Informed Neural\n  Networks", "author": "Chengxi Zeng and Tilo Burghardt and Alberto M Gambaruto", "abstract": "  While many recent Physics-Informed Neural Networks (PINNs) variants have had\nconsiderable success in solving Partial Differential Equations, the empirical\nbenefits of feature mapping drawn from the broader Neural Representations\nresearch have been largely overlooked. We highlight the limitations of widely\nused Fourier-based feature mapping in certain situations and suggest the use of\nthe conditionally positive definite Radial Basis Function. The empirical\nfindings demonstrate the effectiveness of our approach across a variety of\nforward and inverse problem cases. Our method can be seamlessly integrated into\ncoordinate-based input neural networks and contribute to the wider field of\nPINNs research.\n", "link": "http://arxiv.org/abs/2402.08367v2", "date": "2024-04-29", "relevancy": 2.0724, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5418}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5256}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5012}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RBF-PINN%3A%20Non-Fourier%20Positional%20Embedding%20in%20Physics-Informed%20Neural%0A%20%20Networks&body=Title%3A%20RBF-PINN%3A%20Non-Fourier%20Positional%20Embedding%20in%20Physics-Informed%20Neural%0A%20%20Networks%0AAuthor%3A%20Chengxi%20Zeng%20and%20Tilo%20Burghardt%20and%20Alberto%20M%20Gambaruto%0AAbstract%3A%20%20%20While%20many%20recent%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20variants%20have%20had%0Aconsiderable%20success%20in%20solving%20Partial%20Differential%20Equations%2C%20the%20empirical%0Abenefits%20of%20feature%20mapping%20drawn%20from%20the%20broader%20Neural%20Representations%0Aresearch%20have%20been%20largely%20overlooked.%20We%20highlight%20the%20limitations%20of%20widely%0Aused%20Fourier-based%20feature%20mapping%20in%20certain%20situations%20and%20suggest%20the%20use%20of%0Athe%20conditionally%20positive%20definite%20Radial%20Basis%20Function.%20The%20empirical%0Afindings%20demonstrate%20the%20effectiveness%20of%20our%20approach%20across%20a%20variety%20of%0Aforward%20and%20inverse%20problem%20cases.%20Our%20method%20can%20be%20seamlessly%20integrated%20into%0Acoordinate-based%20input%20neural%20networks%20and%20contribute%20to%20the%20wider%20field%20of%0APINNs%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.08367v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RBF-PINN%3A%20Non-Fourier%20Positional%20Embedding%20in%20Physics-Informed%20Neural%0A%20%20Networks&entry.906535625=Chengxi%20Zeng%20and%20Tilo%20Burghardt%20and%20Alberto%20M%20Gambaruto&entry.1292438233=%20%20While%20many%20recent%20Physics-Informed%20Neural%20Networks%20%28PINNs%29%20variants%20have%20had%0Aconsiderable%20success%20in%20solving%20Partial%20Differential%20Equations%2C%20the%20empirical%0Abenefits%20of%20feature%20mapping%20drawn%20from%20the%20broader%20Neural%20Representations%0Aresearch%20have%20been%20largely%20overlooked.%20We%20highlight%20the%20limitations%20of%20widely%0Aused%20Fourier-based%20feature%20mapping%20in%20certain%20situations%20and%20suggest%20the%20use%20of%0Athe%20conditionally%20positive%20definite%20Radial%20Basis%20Function.%20The%20empirical%0Afindings%20demonstrate%20the%20effectiveness%20of%20our%20approach%20across%20a%20variety%20of%0Aforward%20and%20inverse%20problem%20cases.%20Our%20method%20can%20be%20seamlessly%20integrated%20into%0Acoordinate-based%20input%20neural%20networks%20and%20contribute%20to%20the%20wider%20field%20of%0APINNs%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.08367v2&entry.124074799=Read"},
{"title": "Convergence Properties of Score-Based Models using Graduated\n  Optimisation for Linear Inverse Problems", "author": "Pascal Fernsel and \u017deljko Kereta and Alexander Denker", "abstract": "  The incorporation of generative models as regularisers within variational\nformulations for inverse problems has proven effective across numerous image\nreconstruction tasks. However, the resulting optimisation problem is often\nnon-convex and challenging to solve. In this work, we show that score-based\ngenerative models (SGMs) can be used in a graduated optimisation framework to\nsolve inverse problems. We show that the resulting graduated non-convexity flow\nconverge to stationary points of the original problem and provide a numerical\nconvergence analysis of a 2D toy example. We further provide experiments on\ncomputed tomography image reconstruction, where we show that this framework is\nable to recover high-quality images, independent of the initial value. The\nexperiments highlight the potential of using SGMs in graduated optimisation\nframeworks.\n", "link": "http://arxiv.org/abs/2404.18699v1", "date": "2024-04-29", "relevancy": 2.0618, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5389}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5159}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5056}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Convergence%20Properties%20of%20Score-Based%20Models%20using%20Graduated%0A%20%20Optimisation%20for%20Linear%20Inverse%20Problems&body=Title%3A%20Convergence%20Properties%20of%20Score-Based%20Models%20using%20Graduated%0A%20%20Optimisation%20for%20Linear%20Inverse%20Problems%0AAuthor%3A%20Pascal%20Fernsel%20and%20%C5%BDeljko%20Kereta%20and%20Alexander%20Denker%0AAbstract%3A%20%20%20The%20incorporation%20of%20generative%20models%20as%20regularisers%20within%20variational%0Aformulations%20for%20inverse%20problems%20has%20proven%20effective%20across%20numerous%20image%0Areconstruction%20tasks.%20However%2C%20the%20resulting%20optimisation%20problem%20is%20often%0Anon-convex%20and%20challenging%20to%20solve.%20In%20this%20work%2C%20we%20show%20that%20score-based%0Agenerative%20models%20%28SGMs%29%20can%20be%20used%20in%20a%20graduated%20optimisation%20framework%20to%0Asolve%20inverse%20problems.%20We%20show%20that%20the%20resulting%20graduated%20non-convexity%20flow%0Aconverge%20to%20stationary%20points%20of%20the%20original%20problem%20and%20provide%20a%20numerical%0Aconvergence%20analysis%20of%20a%202D%20toy%20example.%20We%20further%20provide%20experiments%20on%0Acomputed%20tomography%20image%20reconstruction%2C%20where%20we%20show%20that%20this%20framework%20is%0Aable%20to%20recover%20high-quality%20images%2C%20independent%20of%20the%20initial%20value.%20The%0Aexperiments%20highlight%20the%20potential%20of%20using%20SGMs%20in%20graduated%20optimisation%0Aframeworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18699v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convergence%20Properties%20of%20Score-Based%20Models%20using%20Graduated%0A%20%20Optimisation%20for%20Linear%20Inverse%20Problems&entry.906535625=Pascal%20Fernsel%20and%20%C5%BDeljko%20Kereta%20and%20Alexander%20Denker&entry.1292438233=%20%20The%20incorporation%20of%20generative%20models%20as%20regularisers%20within%20variational%0Aformulations%20for%20inverse%20problems%20has%20proven%20effective%20across%20numerous%20image%0Areconstruction%20tasks.%20However%2C%20the%20resulting%20optimisation%20problem%20is%20often%0Anon-convex%20and%20challenging%20to%20solve.%20In%20this%20work%2C%20we%20show%20that%20score-based%0Agenerative%20models%20%28SGMs%29%20can%20be%20used%20in%20a%20graduated%20optimisation%20framework%20to%0Asolve%20inverse%20problems.%20We%20show%20that%20the%20resulting%20graduated%20non-convexity%20flow%0Aconverge%20to%20stationary%20points%20of%20the%20original%20problem%20and%20provide%20a%20numerical%0Aconvergence%20analysis%20of%20a%202D%20toy%20example.%20We%20further%20provide%20experiments%20on%0Acomputed%20tomography%20image%20reconstruction%2C%20where%20we%20show%20that%20this%20framework%20is%0Aable%20to%20recover%20high-quality%20images%2C%20independent%20of%20the%20initial%20value.%20The%0Aexperiments%20highlight%20the%20potential%20of%20using%20SGMs%20in%20graduated%20optimisation%0Aframeworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18699v1&entry.124074799=Read"},
{"title": "A Survey on Vision Mamba: Models, Applications and Challenges", "author": "Rui Xu and Shu Yang and Yihui Wang and Bo Du and Hao Chen", "abstract": "  Mamba, a recent selective structured state space model, performs excellently\non long sequence modeling tasks. Mamba mitigates the modeling constraints of\nconvolutional neural networks and offers advanced modeling capabilities similar\nto those of Transformers, through global receptive fields and dynamic\nweighting. Crucially, it achieves this without incurring the quadratic\ncomputational complexity typically associated with Transformers. Due to its\nadvantages over the former two mainstream foundation models, Mamba exhibits\ngreat potential to be a visual foundation model. Researchers are actively\napplying Mamba to various computer vision tasks, leading to numerous emerging\nworks. To help keep pace with the rapid advancements in computer vision, this\npaper aims to provide a comprehensive review of visual Mamba approaches. This\npaper begins by delineating the formulation of the original Mamba model.\nSubsequently, our review of visual Mamba delves into several representative\nbackbone networks to elucidate the core insights of the visual Mamba. We then\ncategorize related works using different modalities, including image, video,\npoint cloud, multi-modal, and others. Specifically, for image applications, we\nfurther organize them into distinct tasks to facilitate a more structured\ndiscussion. Finally, we discuss the challenges and future research directions\nfor visual Mamba, providing insights for future research in this quickly\nevolving area. A comprehensive list of visual Mamba models reviewed in this\nwork is available at https://github.com/Ruixxxx/Awesome-Vision-Mamba-Models.\n", "link": "http://arxiv.org/abs/2404.18861v1", "date": "2024-04-29", "relevancy": 2.0365, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5394}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5049}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5012}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Vision%20Mamba%3A%20Models%2C%20Applications%20and%20Challenges&body=Title%3A%20A%20Survey%20on%20Vision%20Mamba%3A%20Models%2C%20Applications%20and%20Challenges%0AAuthor%3A%20Rui%20Xu%20and%20Shu%20Yang%20and%20Yihui%20Wang%20and%20Bo%20Du%20and%20Hao%20Chen%0AAbstract%3A%20%20%20Mamba%2C%20a%20recent%20selective%20structured%20state%20space%20model%2C%20performs%20excellently%0Aon%20long%20sequence%20modeling%20tasks.%20Mamba%20mitigates%20the%20modeling%20constraints%20of%0Aconvolutional%20neural%20networks%20and%20offers%20advanced%20modeling%20capabilities%20similar%0Ato%20those%20of%20Transformers%2C%20through%20global%20receptive%20fields%20and%20dynamic%0Aweighting.%20Crucially%2C%20it%20achieves%20this%20without%20incurring%20the%20quadratic%0Acomputational%20complexity%20typically%20associated%20with%20Transformers.%20Due%20to%20its%0Aadvantages%20over%20the%20former%20two%20mainstream%20foundation%20models%2C%20Mamba%20exhibits%0Agreat%20potential%20to%20be%20a%20visual%20foundation%20model.%20Researchers%20are%20actively%0Aapplying%20Mamba%20to%20various%20computer%20vision%20tasks%2C%20leading%20to%20numerous%20emerging%0Aworks.%20To%20help%20keep%20pace%20with%20the%20rapid%20advancements%20in%20computer%20vision%2C%20this%0Apaper%20aims%20to%20provide%20a%20comprehensive%20review%20of%20visual%20Mamba%20approaches.%20This%0Apaper%20begins%20by%20delineating%20the%20formulation%20of%20the%20original%20Mamba%20model.%0ASubsequently%2C%20our%20review%20of%20visual%20Mamba%20delves%20into%20several%20representative%0Abackbone%20networks%20to%20elucidate%20the%20core%20insights%20of%20the%20visual%20Mamba.%20We%20then%0Acategorize%20related%20works%20using%20different%20modalities%2C%20including%20image%2C%20video%2C%0Apoint%20cloud%2C%20multi-modal%2C%20and%20others.%20Specifically%2C%20for%20image%20applications%2C%20we%0Afurther%20organize%20them%20into%20distinct%20tasks%20to%20facilitate%20a%20more%20structured%0Adiscussion.%20Finally%2C%20we%20discuss%20the%20challenges%20and%20future%20research%20directions%0Afor%20visual%20Mamba%2C%20providing%20insights%20for%20future%20research%20in%20this%20quickly%0Aevolving%20area.%20A%20comprehensive%20list%20of%20visual%20Mamba%20models%20reviewed%20in%20this%0Awork%20is%20available%20at%20https%3A//github.com/Ruixxxx/Awesome-Vision-Mamba-Models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18861v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Vision%20Mamba%3A%20Models%2C%20Applications%20and%20Challenges&entry.906535625=Rui%20Xu%20and%20Shu%20Yang%20and%20Yihui%20Wang%20and%20Bo%20Du%20and%20Hao%20Chen&entry.1292438233=%20%20Mamba%2C%20a%20recent%20selective%20structured%20state%20space%20model%2C%20performs%20excellently%0Aon%20long%20sequence%20modeling%20tasks.%20Mamba%20mitigates%20the%20modeling%20constraints%20of%0Aconvolutional%20neural%20networks%20and%20offers%20advanced%20modeling%20capabilities%20similar%0Ato%20those%20of%20Transformers%2C%20through%20global%20receptive%20fields%20and%20dynamic%0Aweighting.%20Crucially%2C%20it%20achieves%20this%20without%20incurring%20the%20quadratic%0Acomputational%20complexity%20typically%20associated%20with%20Transformers.%20Due%20to%20its%0Aadvantages%20over%20the%20former%20two%20mainstream%20foundation%20models%2C%20Mamba%20exhibits%0Agreat%20potential%20to%20be%20a%20visual%20foundation%20model.%20Researchers%20are%20actively%0Aapplying%20Mamba%20to%20various%20computer%20vision%20tasks%2C%20leading%20to%20numerous%20emerging%0Aworks.%20To%20help%20keep%20pace%20with%20the%20rapid%20advancements%20in%20computer%20vision%2C%20this%0Apaper%20aims%20to%20provide%20a%20comprehensive%20review%20of%20visual%20Mamba%20approaches.%20This%0Apaper%20begins%20by%20delineating%20the%20formulation%20of%20the%20original%20Mamba%20model.%0ASubsequently%2C%20our%20review%20of%20visual%20Mamba%20delves%20into%20several%20representative%0Abackbone%20networks%20to%20elucidate%20the%20core%20insights%20of%20the%20visual%20Mamba.%20We%20then%0Acategorize%20related%20works%20using%20different%20modalities%2C%20including%20image%2C%20video%2C%0Apoint%20cloud%2C%20multi-modal%2C%20and%20others.%20Specifically%2C%20for%20image%20applications%2C%20we%0Afurther%20organize%20them%20into%20distinct%20tasks%20to%20facilitate%20a%20more%20structured%0Adiscussion.%20Finally%2C%20we%20discuss%20the%20challenges%20and%20future%20research%20directions%0Afor%20visual%20Mamba%2C%20providing%20insights%20for%20future%20research%20in%20this%20quickly%0Aevolving%20area.%20A%20comprehensive%20list%20of%20visual%20Mamba%20models%20reviewed%20in%20this%0Awork%20is%20available%20at%20https%3A//github.com/Ruixxxx/Awesome-Vision-Mamba-Models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18861v1&entry.124074799=Read"},
{"title": "Swin2-MoSE: A New Single Image Super-Resolution Model for Remote Sensing", "author": "Leonardo Rossi and Vittorio Bernuzzi and Tomaso Fontanini and Massimo Bertozzi and Andrea Prati", "abstract": "  Due to the limitations of current optical and sensor technologies and the\nhigh cost of updating them, the spectral and spatial resolution of satellites\nmay not always meet desired requirements. For these reasons, Remote-Sensing\nSingle-Image Super-Resolution (RS-SISR) techniques have gained significant\ninterest. In this paper, we propose Swin2-MoSE model, an enhanced version of\nSwin2SR. Our model introduces MoE-SM, an enhanced Mixture-of-Experts (MoE) to\nreplace the Feed-Forward inside all Transformer block. MoE-SM is designed with\nSmart-Merger, and new layer for merging the output of individual experts, and\nwith a new way to split the work between experts, defining a new per-example\nstrategy instead of the commonly used per-token one. Furthermore, we analyze\nhow positional encodings interact with each other, demonstrating that\nper-channel bias and per-head bias can positively cooperate. Finally, we\npropose to use a combination of Normalized-Cross-Correlation (NCC) and\nStructural Similarity Index Measure (SSIM) losses, to avoid typical MSE loss\nlimitations. Experimental results demonstrate that Swin2-MoSE outperforms SOTA\nby up to 0.377 ~ 0.958 dB (PSNR) on task of 2x, 3x and 4x resolution-upscaling\n(Sen2Venus and OLI2MSI datasets). We show the efficacy of Swin2-MoSE, applying\nit to a semantic segmentation task (SeasoNet dataset). Code and pretrained are\navailable on https://github.com/IMPLabUniPr/swin2-mose/tree/official_code\n", "link": "http://arxiv.org/abs/2404.18924v1", "date": "2024-04-29", "relevancy": 2.0238, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5364}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5102}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4896}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Swin2-MoSE%3A%20A%20New%20Single%20Image%20Super-Resolution%20Model%20for%20Remote%20Sensing&body=Title%3A%20Swin2-MoSE%3A%20A%20New%20Single%20Image%20Super-Resolution%20Model%20for%20Remote%20Sensing%0AAuthor%3A%20Leonardo%20Rossi%20and%20Vittorio%20Bernuzzi%20and%20Tomaso%20Fontanini%20and%20Massimo%20Bertozzi%20and%20Andrea%20Prati%0AAbstract%3A%20%20%20Due%20to%20the%20limitations%20of%20current%20optical%20and%20sensor%20technologies%20and%20the%0Ahigh%20cost%20of%20updating%20them%2C%20the%20spectral%20and%20spatial%20resolution%20of%20satellites%0Amay%20not%20always%20meet%20desired%20requirements.%20For%20these%20reasons%2C%20Remote-Sensing%0ASingle-Image%20Super-Resolution%20%28RS-SISR%29%20techniques%20have%20gained%20significant%0Ainterest.%20In%20this%20paper%2C%20we%20propose%20Swin2-MoSE%20model%2C%20an%20enhanced%20version%20of%0ASwin2SR.%20Our%20model%20introduces%20MoE-SM%2C%20an%20enhanced%20Mixture-of-Experts%20%28MoE%29%20to%0Areplace%20the%20Feed-Forward%20inside%20all%20Transformer%20block.%20MoE-SM%20is%20designed%20with%0ASmart-Merger%2C%20and%20new%20layer%20for%20merging%20the%20output%20of%20individual%20experts%2C%20and%0Awith%20a%20new%20way%20to%20split%20the%20work%20between%20experts%2C%20defining%20a%20new%20per-example%0Astrategy%20instead%20of%20the%20commonly%20used%20per-token%20one.%20Furthermore%2C%20we%20analyze%0Ahow%20positional%20encodings%20interact%20with%20each%20other%2C%20demonstrating%20that%0Aper-channel%20bias%20and%20per-head%20bias%20can%20positively%20cooperate.%20Finally%2C%20we%0Apropose%20to%20use%20a%20combination%20of%20Normalized-Cross-Correlation%20%28NCC%29%20and%0AStructural%20Similarity%20Index%20Measure%20%28SSIM%29%20losses%2C%20to%20avoid%20typical%20MSE%20loss%0Alimitations.%20Experimental%20results%20demonstrate%20that%20Swin2-MoSE%20outperforms%20SOTA%0Aby%20up%20to%200.377%20~%200.958%20dB%20%28PSNR%29%20on%20task%20of%202x%2C%203x%20and%204x%20resolution-upscaling%0A%28Sen2Venus%20and%20OLI2MSI%20datasets%29.%20We%20show%20the%20efficacy%20of%20Swin2-MoSE%2C%20applying%0Ait%20to%20a%20semantic%20segmentation%20task%20%28SeasoNet%20dataset%29.%20Code%20and%20pretrained%20are%0Aavailable%20on%20https%3A//github.com/IMPLabUniPr/swin2-mose/tree/official_code%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18924v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Swin2-MoSE%3A%20A%20New%20Single%20Image%20Super-Resolution%20Model%20for%20Remote%20Sensing&entry.906535625=Leonardo%20Rossi%20and%20Vittorio%20Bernuzzi%20and%20Tomaso%20Fontanini%20and%20Massimo%20Bertozzi%20and%20Andrea%20Prati&entry.1292438233=%20%20Due%20to%20the%20limitations%20of%20current%20optical%20and%20sensor%20technologies%20and%20the%0Ahigh%20cost%20of%20updating%20them%2C%20the%20spectral%20and%20spatial%20resolution%20of%20satellites%0Amay%20not%20always%20meet%20desired%20requirements.%20For%20these%20reasons%2C%20Remote-Sensing%0ASingle-Image%20Super-Resolution%20%28RS-SISR%29%20techniques%20have%20gained%20significant%0Ainterest.%20In%20this%20paper%2C%20we%20propose%20Swin2-MoSE%20model%2C%20an%20enhanced%20version%20of%0ASwin2SR.%20Our%20model%20introduces%20MoE-SM%2C%20an%20enhanced%20Mixture-of-Experts%20%28MoE%29%20to%0Areplace%20the%20Feed-Forward%20inside%20all%20Transformer%20block.%20MoE-SM%20is%20designed%20with%0ASmart-Merger%2C%20and%20new%20layer%20for%20merging%20the%20output%20of%20individual%20experts%2C%20and%0Awith%20a%20new%20way%20to%20split%20the%20work%20between%20experts%2C%20defining%20a%20new%20per-example%0Astrategy%20instead%20of%20the%20commonly%20used%20per-token%20one.%20Furthermore%2C%20we%20analyze%0Ahow%20positional%20encodings%20interact%20with%20each%20other%2C%20demonstrating%20that%0Aper-channel%20bias%20and%20per-head%20bias%20can%20positively%20cooperate.%20Finally%2C%20we%0Apropose%20to%20use%20a%20combination%20of%20Normalized-Cross-Correlation%20%28NCC%29%20and%0AStructural%20Similarity%20Index%20Measure%20%28SSIM%29%20losses%2C%20to%20avoid%20typical%20MSE%20loss%0Alimitations.%20Experimental%20results%20demonstrate%20that%20Swin2-MoSE%20outperforms%20SOTA%0Aby%20up%20to%200.377%20~%200.958%20dB%20%28PSNR%29%20on%20task%20of%202x%2C%203x%20and%204x%20resolution-upscaling%0A%28Sen2Venus%20and%20OLI2MSI%20datasets%29.%20We%20show%20the%20efficacy%20of%20Swin2-MoSE%2C%20applying%0Ait%20to%20a%20semantic%20segmentation%20task%20%28SeasoNet%20dataset%29.%20Code%20and%20pretrained%20are%0Aavailable%20on%20https%3A//github.com/IMPLabUniPr/swin2-mose/tree/official_code%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18924v1&entry.124074799=Read"},
{"title": "Learning Mixtures of Gaussians Using Diffusion Models", "author": "Khashayar Gatmiry and Jonathan Kelner and Holden Lee", "abstract": "  We give a new algorithm for learning mixtures of $k$ Gaussians (with identity\ncovariance in $\\mathbb{R}^n$) to TV error $\\varepsilon$, with quasi-polynomial\n($O(n^{\\text{poly log}\\left(\\frac{n+k}{\\varepsilon}\\right)})$) time and sample\ncomplexity, under a minimum weight assumption. Unlike previous approaches, most\nof which are algebraic in nature, our approach is analytic and relies on the\nframework of diffusion models. Diffusion models are a modern paradigm for\ngenerative modeling, which typically rely on learning the score function\n(gradient log-pdf) along a process transforming a pure noise distribution, in\nour case a Gaussian, to the data distribution. Despite their dazzling\nperformance in tasks such as image generation, there are few end-to-end\ntheoretical guarantees that they can efficiently learn nontrivial families of\ndistributions; we give some of the first such guarantees. We proceed by\nderiving higher-order Gaussian noise sensitivity bounds for the score functions\nfor a Gaussian mixture to show that that they can be inductively learned using\npiecewise polynomial regression (up to poly-logarithmic degree), and combine\nthis with known convergence results for diffusion models. Our results extend to\ncontinuous mixtures of Gaussians where the mixing distribution is supported on\na union of $k$ balls of constant radius. In particular, this applies to the\ncase of Gaussian convolutions of distributions on low-dimensional manifolds, or\nmore generally sets with small covering number.\n", "link": "http://arxiv.org/abs/2404.18869v1", "date": "2024-04-29", "relevancy": 2.0232, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5304}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5126}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4891}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20Mixtures%20of%20Gaussians%20Using%20Diffusion%20Models&body=Title%3A%20Learning%20Mixtures%20of%20Gaussians%20Using%20Diffusion%20Models%0AAuthor%3A%20Khashayar%20Gatmiry%20and%20Jonathan%20Kelner%20and%20Holden%20Lee%0AAbstract%3A%20%20%20We%20give%20a%20new%20algorithm%20for%20learning%20mixtures%20of%20%24k%24%20Gaussians%20%28with%20identity%0Acovariance%20in%20%24%5Cmathbb%7BR%7D%5En%24%29%20to%20TV%20error%20%24%5Cvarepsilon%24%2C%20with%20quasi-polynomial%0A%28%24O%28n%5E%7B%5Ctext%7Bpoly%20log%7D%5Cleft%28%5Cfrac%7Bn%2Bk%7D%7B%5Cvarepsilon%7D%5Cright%29%7D%29%24%29%20time%20and%20sample%0Acomplexity%2C%20under%20a%20minimum%20weight%20assumption.%20Unlike%20previous%20approaches%2C%20most%0Aof%20which%20are%20algebraic%20in%20nature%2C%20our%20approach%20is%20analytic%20and%20relies%20on%20the%0Aframework%20of%20diffusion%20models.%20Diffusion%20models%20are%20a%20modern%20paradigm%20for%0Agenerative%20modeling%2C%20which%20typically%20rely%20on%20learning%20the%20score%20function%0A%28gradient%20log-pdf%29%20along%20a%20process%20transforming%20a%20pure%20noise%20distribution%2C%20in%0Aour%20case%20a%20Gaussian%2C%20to%20the%20data%20distribution.%20Despite%20their%20dazzling%0Aperformance%20in%20tasks%20such%20as%20image%20generation%2C%20there%20are%20few%20end-to-end%0Atheoretical%20guarantees%20that%20they%20can%20efficiently%20learn%20nontrivial%20families%20of%0Adistributions%3B%20we%20give%20some%20of%20the%20first%20such%20guarantees.%20We%20proceed%20by%0Aderiving%20higher-order%20Gaussian%20noise%20sensitivity%20bounds%20for%20the%20score%20functions%0Afor%20a%20Gaussian%20mixture%20to%20show%20that%20that%20they%20can%20be%20inductively%20learned%20using%0Apiecewise%20polynomial%20regression%20%28up%20to%20poly-logarithmic%20degree%29%2C%20and%20combine%0Athis%20with%20known%20convergence%20results%20for%20diffusion%20models.%20Our%20results%20extend%20to%0Acontinuous%20mixtures%20of%20Gaussians%20where%20the%20mixing%20distribution%20is%20supported%20on%0Aa%20union%20of%20%24k%24%20balls%20of%20constant%20radius.%20In%20particular%2C%20this%20applies%20to%20the%0Acase%20of%20Gaussian%20convolutions%20of%20distributions%20on%20low-dimensional%20manifolds%2C%20or%0Amore%20generally%20sets%20with%20small%20covering%20number.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18869v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Mixtures%20of%20Gaussians%20Using%20Diffusion%20Models&entry.906535625=Khashayar%20Gatmiry%20and%20Jonathan%20Kelner%20and%20Holden%20Lee&entry.1292438233=%20%20We%20give%20a%20new%20algorithm%20for%20learning%20mixtures%20of%20%24k%24%20Gaussians%20%28with%20identity%0Acovariance%20in%20%24%5Cmathbb%7BR%7D%5En%24%29%20to%20TV%20error%20%24%5Cvarepsilon%24%2C%20with%20quasi-polynomial%0A%28%24O%28n%5E%7B%5Ctext%7Bpoly%20log%7D%5Cleft%28%5Cfrac%7Bn%2Bk%7D%7B%5Cvarepsilon%7D%5Cright%29%7D%29%24%29%20time%20and%20sample%0Acomplexity%2C%20under%20a%20minimum%20weight%20assumption.%20Unlike%20previous%20approaches%2C%20most%0Aof%20which%20are%20algebraic%20in%20nature%2C%20our%20approach%20is%20analytic%20and%20relies%20on%20the%0Aframework%20of%20diffusion%20models.%20Diffusion%20models%20are%20a%20modern%20paradigm%20for%0Agenerative%20modeling%2C%20which%20typically%20rely%20on%20learning%20the%20score%20function%0A%28gradient%20log-pdf%29%20along%20a%20process%20transforming%20a%20pure%20noise%20distribution%2C%20in%0Aour%20case%20a%20Gaussian%2C%20to%20the%20data%20distribution.%20Despite%20their%20dazzling%0Aperformance%20in%20tasks%20such%20as%20image%20generation%2C%20there%20are%20few%20end-to-end%0Atheoretical%20guarantees%20that%20they%20can%20efficiently%20learn%20nontrivial%20families%20of%0Adistributions%3B%20we%20give%20some%20of%20the%20first%20such%20guarantees.%20We%20proceed%20by%0Aderiving%20higher-order%20Gaussian%20noise%20sensitivity%20bounds%20for%20the%20score%20functions%0Afor%20a%20Gaussian%20mixture%20to%20show%20that%20that%20they%20can%20be%20inductively%20learned%20using%0Apiecewise%20polynomial%20regression%20%28up%20to%20poly-logarithmic%20degree%29%2C%20and%20combine%0Athis%20with%20known%20convergence%20results%20for%20diffusion%20models.%20Our%20results%20extend%20to%0Acontinuous%20mixtures%20of%20Gaussians%20where%20the%20mixing%20distribution%20is%20supported%20on%0Aa%20union%20of%20%24k%24%20balls%20of%20constant%20radius.%20In%20particular%2C%20this%20applies%20to%20the%0Acase%20of%20Gaussian%20convolutions%20of%20distributions%20on%20low-dimensional%20manifolds%2C%20or%0Amore%20generally%20sets%20with%20small%20covering%20number.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18869v1&entry.124074799=Read"},
{"title": "Do Vision & Language Decoders use Images and Text equally? How\n  Self-consistent are their Explanations?", "author": "Letitia Parcalabescu and Anette Frank", "abstract": "  Vision and language models (VLMs) are currently the most generally performant\narchitectures on multimodal tasks. Next to their predictions, they can also\nproduce explanations, either in post-hoc or CoT settings. However, it is not\nclear how much they use the vision and text modalities when generating\npredictions or explanations. In this work, we investigate if VLMs rely on\nmodalities differently when generating explanations as opposed to when they\nprovide answers. We also evaluate the self-consistency of VLM decoders in both\npost-hoc and CoT explanation settings, by extending existing tests and measures\nto VLM decoders. We find that VLMs are less self-consistent than LLMs. The text\ncontributions in VL decoders are much larger than the image contributions\nacross all measured tasks. And the contributions of the image are significantly\nlarger for explanation generations than for answer generation. This difference\nis even larger in CoT compared to the post-hoc explanation setting. We also\nprovide an up-to-date benchmarking of state-of-the-art VL decoders on the VALSE\nbenchmark, which to date focused only on VL encoders. We find that VL decoders\nare still struggling with most phenomena tested by VALSE.\n", "link": "http://arxiv.org/abs/2404.18624v1", "date": "2024-04-29", "relevancy": 2.0068, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5078}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5064}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4937}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Do%20Vision%20%26%20Language%20Decoders%20use%20Images%20and%20Text%20equally%3F%20How%0A%20%20Self-consistent%20are%20their%20Explanations%3F&body=Title%3A%20Do%20Vision%20%26%20Language%20Decoders%20use%20Images%20and%20Text%20equally%3F%20How%0A%20%20Self-consistent%20are%20their%20Explanations%3F%0AAuthor%3A%20Letitia%20Parcalabescu%20and%20Anette%20Frank%0AAbstract%3A%20%20%20Vision%20and%20language%20models%20%28VLMs%29%20are%20currently%20the%20most%20generally%20performant%0Aarchitectures%20on%20multimodal%20tasks.%20Next%20to%20their%20predictions%2C%20they%20can%20also%0Aproduce%20explanations%2C%20either%20in%20post-hoc%20or%20CoT%20settings.%20However%2C%20it%20is%20not%0Aclear%20how%20much%20they%20use%20the%20vision%20and%20text%20modalities%20when%20generating%0Apredictions%20or%20explanations.%20In%20this%20work%2C%20we%20investigate%20if%20VLMs%20rely%20on%0Amodalities%20differently%20when%20generating%20explanations%20as%20opposed%20to%20when%20they%0Aprovide%20answers.%20We%20also%20evaluate%20the%20self-consistency%20of%20VLM%20decoders%20in%20both%0Apost-hoc%20and%20CoT%20explanation%20settings%2C%20by%20extending%20existing%20tests%20and%20measures%0Ato%20VLM%20decoders.%20We%20find%20that%20VLMs%20are%20less%20self-consistent%20than%20LLMs.%20The%20text%0Acontributions%20in%20VL%20decoders%20are%20much%20larger%20than%20the%20image%20contributions%0Aacross%20all%20measured%20tasks.%20And%20the%20contributions%20of%20the%20image%20are%20significantly%0Alarger%20for%20explanation%20generations%20than%20for%20answer%20generation.%20This%20difference%0Ais%20even%20larger%20in%20CoT%20compared%20to%20the%20post-hoc%20explanation%20setting.%20We%20also%0Aprovide%20an%20up-to-date%20benchmarking%20of%20state-of-the-art%20VL%20decoders%20on%20the%20VALSE%0Abenchmark%2C%20which%20to%20date%20focused%20only%20on%20VL%20encoders.%20We%20find%20that%20VL%20decoders%0Aare%20still%20struggling%20with%20most%20phenomena%20tested%20by%20VALSE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18624v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Vision%20%26%20Language%20Decoders%20use%20Images%20and%20Text%20equally%3F%20How%0A%20%20Self-consistent%20are%20their%20Explanations%3F&entry.906535625=Letitia%20Parcalabescu%20and%20Anette%20Frank&entry.1292438233=%20%20Vision%20and%20language%20models%20%28VLMs%29%20are%20currently%20the%20most%20generally%20performant%0Aarchitectures%20on%20multimodal%20tasks.%20Next%20to%20their%20predictions%2C%20they%20can%20also%0Aproduce%20explanations%2C%20either%20in%20post-hoc%20or%20CoT%20settings.%20However%2C%20it%20is%20not%0Aclear%20how%20much%20they%20use%20the%20vision%20and%20text%20modalities%20when%20generating%0Apredictions%20or%20explanations.%20In%20this%20work%2C%20we%20investigate%20if%20VLMs%20rely%20on%0Amodalities%20differently%20when%20generating%20explanations%20as%20opposed%20to%20when%20they%0Aprovide%20answers.%20We%20also%20evaluate%20the%20self-consistency%20of%20VLM%20decoders%20in%20both%0Apost-hoc%20and%20CoT%20explanation%20settings%2C%20by%20extending%20existing%20tests%20and%20measures%0Ato%20VLM%20decoders.%20We%20find%20that%20VLMs%20are%20less%20self-consistent%20than%20LLMs.%20The%20text%0Acontributions%20in%20VL%20decoders%20are%20much%20larger%20than%20the%20image%20contributions%0Aacross%20all%20measured%20tasks.%20And%20the%20contributions%20of%20the%20image%20are%20significantly%0Alarger%20for%20explanation%20generations%20than%20for%20answer%20generation.%20This%20difference%0Ais%20even%20larger%20in%20CoT%20compared%20to%20the%20post-hoc%20explanation%20setting.%20We%20also%0Aprovide%20an%20up-to-date%20benchmarking%20of%20state-of-the-art%20VL%20decoders%20on%20the%20VALSE%0Abenchmark%2C%20which%20to%20date%20focused%20only%20on%20VL%20encoders.%20We%20find%20that%20VL%20decoders%0Aare%20still%20struggling%20with%20most%20phenomena%20tested%20by%20VALSE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18624v1&entry.124074799=Read"},
{"title": "ConPro: Learning Severity Representation for Medical Images using\n  Contrastive Learning and Preference Optimization", "author": "Hong Nguyen and Hoang Nguyen and Melinda Chang and Hieu Pham and Shrikanth Narayanan and Michael Pazzani", "abstract": "  Understanding the severity of conditions shown in images in medical diagnosis\nis crucial, serving as a key guide for clinical assessment, treatment, as well\nas evaluating longitudinal progression. This paper proposes Con- PrO: a novel\nrepresentation learning method for severity assessment in medical images using\nContrastive learningintegrated Preference Optimization. Different from\nconventional contrastive learning methods that maximize the distance between\nclasses, ConPrO injects into the latent vector the distance preference\nknowledge between various severity classes and the normal class. We\nsystematically examine the key components of our framework to illuminate how\ncontrastive prediction tasks acquire valuable representations. We show that our\nrepresentation learning framework offers valuable severity ordering in the\nfeature space while outperforming previous state-of-the-art methods on\nclassification tasks. We achieve a 6% and 20% relative improvement compared to\na supervised and a self-supervised baseline, respectively. In addition, we\nderived discussions on severity indicators and related applications of\npreference comparison in the medical domain.\n", "link": "http://arxiv.org/abs/2404.18831v1", "date": "2024-04-29", "relevancy": 2.0064, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5186}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4905}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4869}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ConPro%3A%20Learning%20Severity%20Representation%20for%20Medical%20Images%20using%0A%20%20Contrastive%20Learning%20and%20Preference%20Optimization&body=Title%3A%20ConPro%3A%20Learning%20Severity%20Representation%20for%20Medical%20Images%20using%0A%20%20Contrastive%20Learning%20and%20Preference%20Optimization%0AAuthor%3A%20Hong%20Nguyen%20and%20Hoang%20Nguyen%20and%20Melinda%20Chang%20and%20Hieu%20Pham%20and%20Shrikanth%20Narayanan%20and%20Michael%20Pazzani%0AAbstract%3A%20%20%20Understanding%20the%20severity%20of%20conditions%20shown%20in%20images%20in%20medical%20diagnosis%0Ais%20crucial%2C%20serving%20as%20a%20key%20guide%20for%20clinical%20assessment%2C%20treatment%2C%20as%20well%0Aas%20evaluating%20longitudinal%20progression.%20This%20paper%20proposes%20Con-%20PrO%3A%20a%20novel%0Arepresentation%20learning%20method%20for%20severity%20assessment%20in%20medical%20images%20using%0AContrastive%20learningintegrated%20Preference%20Optimization.%20Different%20from%0Aconventional%20contrastive%20learning%20methods%20that%20maximize%20the%20distance%20between%0Aclasses%2C%20ConPrO%20injects%20into%20the%20latent%20vector%20the%20distance%20preference%0Aknowledge%20between%20various%20severity%20classes%20and%20the%20normal%20class.%20We%0Asystematically%20examine%20the%20key%20components%20of%20our%20framework%20to%20illuminate%20how%0Acontrastive%20prediction%20tasks%20acquire%20valuable%20representations.%20We%20show%20that%20our%0Arepresentation%20learning%20framework%20offers%20valuable%20severity%20ordering%20in%20the%0Afeature%20space%20while%20outperforming%20previous%20state-of-the-art%20methods%20on%0Aclassification%20tasks.%20We%20achieve%20a%206%25%20and%2020%25%20relative%20improvement%20compared%20to%0Aa%20supervised%20and%20a%20self-supervised%20baseline%2C%20respectively.%20In%20addition%2C%20we%0Aderived%20discussions%20on%20severity%20indicators%20and%20related%20applications%20of%0Apreference%20comparison%20in%20the%20medical%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18831v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConPro%3A%20Learning%20Severity%20Representation%20for%20Medical%20Images%20using%0A%20%20Contrastive%20Learning%20and%20Preference%20Optimization&entry.906535625=Hong%20Nguyen%20and%20Hoang%20Nguyen%20and%20Melinda%20Chang%20and%20Hieu%20Pham%20and%20Shrikanth%20Narayanan%20and%20Michael%20Pazzani&entry.1292438233=%20%20Understanding%20the%20severity%20of%20conditions%20shown%20in%20images%20in%20medical%20diagnosis%0Ais%20crucial%2C%20serving%20as%20a%20key%20guide%20for%20clinical%20assessment%2C%20treatment%2C%20as%20well%0Aas%20evaluating%20longitudinal%20progression.%20This%20paper%20proposes%20Con-%20PrO%3A%20a%20novel%0Arepresentation%20learning%20method%20for%20severity%20assessment%20in%20medical%20images%20using%0AContrastive%20learningintegrated%20Preference%20Optimization.%20Different%20from%0Aconventional%20contrastive%20learning%20methods%20that%20maximize%20the%20distance%20between%0Aclasses%2C%20ConPrO%20injects%20into%20the%20latent%20vector%20the%20distance%20preference%0Aknowledge%20between%20various%20severity%20classes%20and%20the%20normal%20class.%20We%0Asystematically%20examine%20the%20key%20components%20of%20our%20framework%20to%20illuminate%20how%0Acontrastive%20prediction%20tasks%20acquire%20valuable%20representations.%20We%20show%20that%20our%0Arepresentation%20learning%20framework%20offers%20valuable%20severity%20ordering%20in%20the%0Afeature%20space%20while%20outperforming%20previous%20state-of-the-art%20methods%20on%0Aclassification%20tasks.%20We%20achieve%20a%206%25%20and%2020%25%20relative%20improvement%20compared%20to%0Aa%20supervised%20and%20a%20self-supervised%20baseline%2C%20respectively.%20In%20addition%2C%20we%0Aderived%20discussions%20on%20severity%20indicators%20and%20related%20applications%20of%0Apreference%20comparison%20in%20the%20medical%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18831v1&entry.124074799=Read"},
{"title": "Real Time Multi Organ Classification on Computed Tomography Images", "author": "Halid Ziya Yerebakan and Yoshihisa Shinagawa and Gerardo Hermosillo Valadez", "abstract": "  Organ segmentation is a fundamental task in medical imaging, and it is useful\nfor many clinical automation pipelines. Typically, the process involves\nsegmenting the entire volume, which can be unnecessary when the points of\ninterest are limited. In those cases, a classifier could be used instead of\nsegmentation. However, there is an inherent trade-off between the context size\nand the speed of classifiers. To address this issue, we propose a new method\nthat employs a data selection strategy with sparse sampling across a wide field\nof view without image resampling. This sparse sampling strategy makes it\npossible to classify voxels into multiple organs in real time without using\naccelerators. Although our method is an independent classifier, it can generate\nfull segmentation by querying grid locations at any resolution. We have\ncompared our method with existing segmentation techniques, demonstrating its\npotential for superior runtime in practical applications in medical imaging.\n", "link": "http://arxiv.org/abs/2404.18731v1", "date": "2024-04-29", "relevancy": 2.0046, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5215}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4871}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4854}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Real%20Time%20Multi%20Organ%20Classification%20on%20Computed%20Tomography%20Images&body=Title%3A%20Real%20Time%20Multi%20Organ%20Classification%20on%20Computed%20Tomography%20Images%0AAuthor%3A%20Halid%20Ziya%20Yerebakan%20and%20Yoshihisa%20Shinagawa%20and%20Gerardo%20Hermosillo%20Valadez%0AAbstract%3A%20%20%20Organ%20segmentation%20is%20a%20fundamental%20task%20in%20medical%20imaging%2C%20and%20it%20is%20useful%0Afor%20many%20clinical%20automation%20pipelines.%20Typically%2C%20the%20process%20involves%0Asegmenting%20the%20entire%20volume%2C%20which%20can%20be%20unnecessary%20when%20the%20points%20of%0Ainterest%20are%20limited.%20In%20those%20cases%2C%20a%20classifier%20could%20be%20used%20instead%20of%0Asegmentation.%20However%2C%20there%20is%20an%20inherent%20trade-off%20between%20the%20context%20size%0Aand%20the%20speed%20of%20classifiers.%20To%20address%20this%20issue%2C%20we%20propose%20a%20new%20method%0Athat%20employs%20a%20data%20selection%20strategy%20with%20sparse%20sampling%20across%20a%20wide%20field%0Aof%20view%20without%20image%20resampling.%20This%20sparse%20sampling%20strategy%20makes%20it%0Apossible%20to%20classify%20voxels%20into%20multiple%20organs%20in%20real%20time%20without%20using%0Aaccelerators.%20Although%20our%20method%20is%20an%20independent%20classifier%2C%20it%20can%20generate%0Afull%20segmentation%20by%20querying%20grid%20locations%20at%20any%20resolution.%20We%20have%0Acompared%20our%20method%20with%20existing%20segmentation%20techniques%2C%20demonstrating%20its%0Apotential%20for%20superior%20runtime%20in%20practical%20applications%20in%20medical%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18731v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real%20Time%20Multi%20Organ%20Classification%20on%20Computed%20Tomography%20Images&entry.906535625=Halid%20Ziya%20Yerebakan%20and%20Yoshihisa%20Shinagawa%20and%20Gerardo%20Hermosillo%20Valadez&entry.1292438233=%20%20Organ%20segmentation%20is%20a%20fundamental%20task%20in%20medical%20imaging%2C%20and%20it%20is%20useful%0Afor%20many%20clinical%20automation%20pipelines.%20Typically%2C%20the%20process%20involves%0Asegmenting%20the%20entire%20volume%2C%20which%20can%20be%20unnecessary%20when%20the%20points%20of%0Ainterest%20are%20limited.%20In%20those%20cases%2C%20a%20classifier%20could%20be%20used%20instead%20of%0Asegmentation.%20However%2C%20there%20is%20an%20inherent%20trade-off%20between%20the%20context%20size%0Aand%20the%20speed%20of%20classifiers.%20To%20address%20this%20issue%2C%20we%20propose%20a%20new%20method%0Athat%20employs%20a%20data%20selection%20strategy%20with%20sparse%20sampling%20across%20a%20wide%20field%0Aof%20view%20without%20image%20resampling.%20This%20sparse%20sampling%20strategy%20makes%20it%0Apossible%20to%20classify%20voxels%20into%20multiple%20organs%20in%20real%20time%20without%20using%0Aaccelerators.%20Although%20our%20method%20is%20an%20independent%20classifier%2C%20it%20can%20generate%0Afull%20segmentation%20by%20querying%20grid%20locations%20at%20any%20resolution.%20We%20have%0Acompared%20our%20method%20with%20existing%20segmentation%20techniques%2C%20demonstrating%20its%0Apotential%20for%20superior%20runtime%20in%20practical%20applications%20in%20medical%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18731v1&entry.124074799=Read"},
{"title": "Flow AM: Generating Point Cloud Global Explanations by Latent Alignment", "author": "Hanxiao Tan", "abstract": "  Although point cloud models have gained significant improvements in\nprediction accuracy over recent years, their trustworthiness is still not\nsufficiently investigated. In terms of global explainability, Activation\nMaximization (AM) techniques in the image domain are not directly\ntransplantable due to the special structure of the point cloud models. Existing\nstudies exploit generative models to yield global explanations that can be\nperceived by humans. However, the opacity of the generative models themselves\nand the introduction of additional priors call into question the plausibility\nand fidelity of the explanations. In this work, we demonstrate that when the\nclassifier predicts different types of instances, the intermediate layer\nactivations are differently activated, known as activation flows. Based on this\nproperty, we propose an activation flow-based AM method that generates global\nexplanations that can be perceived without incorporating any generative model.\nFurthermore, we reveal that AM based on generative models fails the sanity\nchecks and thus lack of fidelity. Extensive experiments show that our approach\ndramatically enhances the perceptibility of explanations compared to other AM\nmethods that are not based on generative models. Our code is available at:\nhttps://github.com/Explain3D/FlowAM\n", "link": "http://arxiv.org/abs/2404.18760v1", "date": "2024-04-29", "relevancy": 1.9999, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5483}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4919}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4887}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Flow%20AM%3A%20Generating%20Point%20Cloud%20Global%20Explanations%20by%20Latent%20Alignment&body=Title%3A%20Flow%20AM%3A%20Generating%20Point%20Cloud%20Global%20Explanations%20by%20Latent%20Alignment%0AAuthor%3A%20Hanxiao%20Tan%0AAbstract%3A%20%20%20Although%20point%20cloud%20models%20have%20gained%20significant%20improvements%20in%0Aprediction%20accuracy%20over%20recent%20years%2C%20their%20trustworthiness%20is%20still%20not%0Asufficiently%20investigated.%20In%20terms%20of%20global%20explainability%2C%20Activation%0AMaximization%20%28AM%29%20techniques%20in%20the%20image%20domain%20are%20not%20directly%0Atransplantable%20due%20to%20the%20special%20structure%20of%20the%20point%20cloud%20models.%20Existing%0Astudies%20exploit%20generative%20models%20to%20yield%20global%20explanations%20that%20can%20be%0Aperceived%20by%20humans.%20However%2C%20the%20opacity%20of%20the%20generative%20models%20themselves%0Aand%20the%20introduction%20of%20additional%20priors%20call%20into%20question%20the%20plausibility%0Aand%20fidelity%20of%20the%20explanations.%20In%20this%20work%2C%20we%20demonstrate%20that%20when%20the%0Aclassifier%20predicts%20different%20types%20of%20instances%2C%20the%20intermediate%20layer%0Aactivations%20are%20differently%20activated%2C%20known%20as%20activation%20flows.%20Based%20on%20this%0Aproperty%2C%20we%20propose%20an%20activation%20flow-based%20AM%20method%20that%20generates%20global%0Aexplanations%20that%20can%20be%20perceived%20without%20incorporating%20any%20generative%20model.%0AFurthermore%2C%20we%20reveal%20that%20AM%20based%20on%20generative%20models%20fails%20the%20sanity%0Achecks%20and%20thus%20lack%20of%20fidelity.%20Extensive%20experiments%20show%20that%20our%20approach%0Adramatically%20enhances%20the%20perceptibility%20of%20explanations%20compared%20to%20other%20AM%0Amethods%20that%20are%20not%20based%20on%20generative%20models.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/Explain3D/FlowAM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18760v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flow%20AM%3A%20Generating%20Point%20Cloud%20Global%20Explanations%20by%20Latent%20Alignment&entry.906535625=Hanxiao%20Tan&entry.1292438233=%20%20Although%20point%20cloud%20models%20have%20gained%20significant%20improvements%20in%0Aprediction%20accuracy%20over%20recent%20years%2C%20their%20trustworthiness%20is%20still%20not%0Asufficiently%20investigated.%20In%20terms%20of%20global%20explainability%2C%20Activation%0AMaximization%20%28AM%29%20techniques%20in%20the%20image%20domain%20are%20not%20directly%0Atransplantable%20due%20to%20the%20special%20structure%20of%20the%20point%20cloud%20models.%20Existing%0Astudies%20exploit%20generative%20models%20to%20yield%20global%20explanations%20that%20can%20be%0Aperceived%20by%20humans.%20However%2C%20the%20opacity%20of%20the%20generative%20models%20themselves%0Aand%20the%20introduction%20of%20additional%20priors%20call%20into%20question%20the%20plausibility%0Aand%20fidelity%20of%20the%20explanations.%20In%20this%20work%2C%20we%20demonstrate%20that%20when%20the%0Aclassifier%20predicts%20different%20types%20of%20instances%2C%20the%20intermediate%20layer%0Aactivations%20are%20differently%20activated%2C%20known%20as%20activation%20flows.%20Based%20on%20this%0Aproperty%2C%20we%20propose%20an%20activation%20flow-based%20AM%20method%20that%20generates%20global%0Aexplanations%20that%20can%20be%20perceived%20without%20incorporating%20any%20generative%20model.%0AFurthermore%2C%20we%20reveal%20that%20AM%20based%20on%20generative%20models%20fails%20the%20sanity%0Achecks%20and%20thus%20lack%20of%20fidelity.%20Extensive%20experiments%20show%20that%20our%20approach%0Adramatically%20enhances%20the%20perceptibility%20of%20explanations%20compared%20to%20other%20AM%0Amethods%20that%20are%20not%20based%20on%20generative%20models.%20Our%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/Explain3D/FlowAM%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18760v1&entry.124074799=Read"},
{"title": "Fast Swarming of UAVs in GNSS-denied Feature-poor Environments without\n  Explicit Communication", "author": "Jiri Horyna and Vit Kratky and Vaclav Pritzl and Tomas Baca and Eliseo Ferrante and Martin Saska", "abstract": "  A decentralized swarm approach for the fast cooperative flight of Unmanned\nAerial Vehicles (UAVs) in feature-poor environments without any external\nlocalization and communication is introduced in this paper.\n  A novel model of a UAV neighborhood is proposed to achieve robust onboard\nmutual perception and flocking state feedback control, which is designed to\ndecrease the inter-agent oscillations common in standard reactive swarm models\nemployed in fast collective motion.\n  The novel swarming methodology is supplemented with an enhanced Multi-Robot\nState Estimation (MRSE) strategy to increase the reliability of the purely\nonboard localization, which may be unreliable in real environments.\n  Although MRSE and the neighborhood model may rely on information exchange\nbetween agents, we introduce a communication-less version of the swarming\nframework based on estimating communicated states to decrease dependence on the\noften unreliable communication networks of large swarms.\n  The proposed solution has been verified by a set of complex real-world\nexperiments to demonstrate its overall capability in different conditions,\nincluding a UAV interception-motivated task with a group velocity reaching the\nphysical limits of the individual hardware platforms.\n", "link": "http://arxiv.org/abs/2404.18729v1", "date": "2024-04-29", "relevancy": 1.9965, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5404}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5002}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4575}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Fast%20Swarming%20of%20UAVs%20in%20GNSS-denied%20Feature-poor%20Environments%20without%0A%20%20Explicit%20Communication&body=Title%3A%20Fast%20Swarming%20of%20UAVs%20in%20GNSS-denied%20Feature-poor%20Environments%20without%0A%20%20Explicit%20Communication%0AAuthor%3A%20Jiri%20Horyna%20and%20Vit%20Kratky%20and%20Vaclav%20Pritzl%20and%20Tomas%20Baca%20and%20Eliseo%20Ferrante%20and%20Martin%20Saska%0AAbstract%3A%20%20%20A%20decentralized%20swarm%20approach%20for%20the%20fast%20cooperative%20flight%20of%20Unmanned%0AAerial%20Vehicles%20%28UAVs%29%20in%20feature-poor%20environments%20without%20any%20external%0Alocalization%20and%20communication%20is%20introduced%20in%20this%20paper.%0A%20%20A%20novel%20model%20of%20a%20UAV%20neighborhood%20is%20proposed%20to%20achieve%20robust%20onboard%0Amutual%20perception%20and%20flocking%20state%20feedback%20control%2C%20which%20is%20designed%20to%0Adecrease%20the%20inter-agent%20oscillations%20common%20in%20standard%20reactive%20swarm%20models%0Aemployed%20in%20fast%20collective%20motion.%0A%20%20The%20novel%20swarming%20methodology%20is%20supplemented%20with%20an%20enhanced%20Multi-Robot%0AState%20Estimation%20%28MRSE%29%20strategy%20to%20increase%20the%20reliability%20of%20the%20purely%0Aonboard%20localization%2C%20which%20may%20be%20unreliable%20in%20real%20environments.%0A%20%20Although%20MRSE%20and%20the%20neighborhood%20model%20may%20rely%20on%20information%20exchange%0Abetween%20agents%2C%20we%20introduce%20a%20communication-less%20version%20of%20the%20swarming%0Aframework%20based%20on%20estimating%20communicated%20states%20to%20decrease%20dependence%20on%20the%0Aoften%20unreliable%20communication%20networks%20of%20large%20swarms.%0A%20%20The%20proposed%20solution%20has%20been%20verified%20by%20a%20set%20of%20complex%20real-world%0Aexperiments%20to%20demonstrate%20its%20overall%20capability%20in%20different%20conditions%2C%0Aincluding%20a%20UAV%20interception-motivated%20task%20with%20a%20group%20velocity%20reaching%20the%0Aphysical%20limits%20of%20the%20individual%20hardware%20platforms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18729v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Swarming%20of%20UAVs%20in%20GNSS-denied%20Feature-poor%20Environments%20without%0A%20%20Explicit%20Communication&entry.906535625=Jiri%20Horyna%20and%20Vit%20Kratky%20and%20Vaclav%20Pritzl%20and%20Tomas%20Baca%20and%20Eliseo%20Ferrante%20and%20Martin%20Saska&entry.1292438233=%20%20A%20decentralized%20swarm%20approach%20for%20the%20fast%20cooperative%20flight%20of%20Unmanned%0AAerial%20Vehicles%20%28UAVs%29%20in%20feature-poor%20environments%20without%20any%20external%0Alocalization%20and%20communication%20is%20introduced%20in%20this%20paper.%0A%20%20A%20novel%20model%20of%20a%20UAV%20neighborhood%20is%20proposed%20to%20achieve%20robust%20onboard%0Amutual%20perception%20and%20flocking%20state%20feedback%20control%2C%20which%20is%20designed%20to%0Adecrease%20the%20inter-agent%20oscillations%20common%20in%20standard%20reactive%20swarm%20models%0Aemployed%20in%20fast%20collective%20motion.%0A%20%20The%20novel%20swarming%20methodology%20is%20supplemented%20with%20an%20enhanced%20Multi-Robot%0AState%20Estimation%20%28MRSE%29%20strategy%20to%20increase%20the%20reliability%20of%20the%20purely%0Aonboard%20localization%2C%20which%20may%20be%20unreliable%20in%20real%20environments.%0A%20%20Although%20MRSE%20and%20the%20neighborhood%20model%20may%20rely%20on%20information%20exchange%0Abetween%20agents%2C%20we%20introduce%20a%20communication-less%20version%20of%20the%20swarming%0Aframework%20based%20on%20estimating%20communicated%20states%20to%20decrease%20dependence%20on%20the%0Aoften%20unreliable%20communication%20networks%20of%20large%20swarms.%0A%20%20The%20proposed%20solution%20has%20been%20verified%20by%20a%20set%20of%20complex%20real-world%0Aexperiments%20to%20demonstrate%20its%20overall%20capability%20in%20different%20conditions%2C%0Aincluding%20a%20UAV%20interception-motivated%20task%20with%20a%20group%20velocity%20reaching%20the%0Aphysical%20limits%20of%20the%20individual%20hardware%20platforms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18729v1&entry.124074799=Read"},
{"title": "Kangaroo: Lossless Self-Speculative Decoding via Double Early Exiting", "author": "Fangcheng Liu and Yehui Tang and Zhenhua Liu and Yunsheng Ni and Kai Han and Yunhe Wang", "abstract": "  Speculative decoding has demonstrated its effectiveness in accelerating the\ninference of large language models while maintaining a consistent sampling\ndistribution. However, the conventional approach of training a separate draft\nmodel to achieve a satisfactory token acceptance rate can be costly. Drawing\ninspiration from early exiting, we propose a novel self-speculative decoding\nframework \\emph{Kangaroo}, which uses a fixed shallow sub-network as a\nself-draft model, with the remaining layers serving as the larger target model.\nWe train a lightweight and efficient adapter module on top of the sub-network\nto bridge the gap between the sub-network and the full model's representation\nability. It is noteworthy that the inference latency of the self-draft model\nmay no longer be negligible compared to the large model, necessitating\nstrategies to increase the token acceptance rate while minimizing the drafting\nsteps of the small model. To address this challenge, we introduce an additional\nearly exiting mechanism for generating draft tokens. Specifically, we halt the\nsmall model's subsequent prediction during the drafting phase once the\nconfidence level for the current token falls below a certain threshold.\nExtensive experiments on the Spec-Bench demonstrate the effectiveness of\nKangaroo. Under single-sequence verification, Kangaroo achieves speedups up to\n$1.68\\times$ on Spec-Bench, outperforming Medusa-1 with 88.7\\% fewer additional\nparameters (67M compared to 591M). The code for Kangaroo is available at\nhttps://github.com/Equationliu/Kangaroo.\n", "link": "http://arxiv.org/abs/2404.18911v1", "date": "2024-04-29", "relevancy": 1.989, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5056}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4971}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4941}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Kangaroo%3A%20Lossless%20Self-Speculative%20Decoding%20via%20Double%20Early%20Exiting&body=Title%3A%20Kangaroo%3A%20Lossless%20Self-Speculative%20Decoding%20via%20Double%20Early%20Exiting%0AAuthor%3A%20Fangcheng%20Liu%20and%20Yehui%20Tang%20and%20Zhenhua%20Liu%20and%20Yunsheng%20Ni%20and%20Kai%20Han%20and%20Yunhe%20Wang%0AAbstract%3A%20%20%20Speculative%20decoding%20has%20demonstrated%20its%20effectiveness%20in%20accelerating%20the%0Ainference%20of%20large%20language%20models%20while%20maintaining%20a%20consistent%20sampling%0Adistribution.%20However%2C%20the%20conventional%20approach%20of%20training%20a%20separate%20draft%0Amodel%20to%20achieve%20a%20satisfactory%20token%20acceptance%20rate%20can%20be%20costly.%20Drawing%0Ainspiration%20from%20early%20exiting%2C%20we%20propose%20a%20novel%20self-speculative%20decoding%0Aframework%20%5Cemph%7BKangaroo%7D%2C%20which%20uses%20a%20fixed%20shallow%20sub-network%20as%20a%0Aself-draft%20model%2C%20with%20the%20remaining%20layers%20serving%20as%20the%20larger%20target%20model.%0AWe%20train%20a%20lightweight%20and%20efficient%20adapter%20module%20on%20top%20of%20the%20sub-network%0Ato%20bridge%20the%20gap%20between%20the%20sub-network%20and%20the%20full%20model%27s%20representation%0Aability.%20It%20is%20noteworthy%20that%20the%20inference%20latency%20of%20the%20self-draft%20model%0Amay%20no%20longer%20be%20negligible%20compared%20to%20the%20large%20model%2C%20necessitating%0Astrategies%20to%20increase%20the%20token%20acceptance%20rate%20while%20minimizing%20the%20drafting%0Asteps%20of%20the%20small%20model.%20To%20address%20this%20challenge%2C%20we%20introduce%20an%20additional%0Aearly%20exiting%20mechanism%20for%20generating%20draft%20tokens.%20Specifically%2C%20we%20halt%20the%0Asmall%20model%27s%20subsequent%20prediction%20during%20the%20drafting%20phase%20once%20the%0Aconfidence%20level%20for%20the%20current%20token%20falls%20below%20a%20certain%20threshold.%0AExtensive%20experiments%20on%20the%20Spec-Bench%20demonstrate%20the%20effectiveness%20of%0AKangaroo.%20Under%20single-sequence%20verification%2C%20Kangaroo%20achieves%20speedups%20up%20to%0A%241.68%5Ctimes%24%20on%20Spec-Bench%2C%20outperforming%20Medusa-1%20with%2088.7%5C%25%20fewer%20additional%0Aparameters%20%2867M%20compared%20to%20591M%29.%20The%20code%20for%20Kangaroo%20is%20available%20at%0Ahttps%3A//github.com/Equationliu/Kangaroo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18911v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kangaroo%3A%20Lossless%20Self-Speculative%20Decoding%20via%20Double%20Early%20Exiting&entry.906535625=Fangcheng%20Liu%20and%20Yehui%20Tang%20and%20Zhenhua%20Liu%20and%20Yunsheng%20Ni%20and%20Kai%20Han%20and%20Yunhe%20Wang&entry.1292438233=%20%20Speculative%20decoding%20has%20demonstrated%20its%20effectiveness%20in%20accelerating%20the%0Ainference%20of%20large%20language%20models%20while%20maintaining%20a%20consistent%20sampling%0Adistribution.%20However%2C%20the%20conventional%20approach%20of%20training%20a%20separate%20draft%0Amodel%20to%20achieve%20a%20satisfactory%20token%20acceptance%20rate%20can%20be%20costly.%20Drawing%0Ainspiration%20from%20early%20exiting%2C%20we%20propose%20a%20novel%20self-speculative%20decoding%0Aframework%20%5Cemph%7BKangaroo%7D%2C%20which%20uses%20a%20fixed%20shallow%20sub-network%20as%20a%0Aself-draft%20model%2C%20with%20the%20remaining%20layers%20serving%20as%20the%20larger%20target%20model.%0AWe%20train%20a%20lightweight%20and%20efficient%20adapter%20module%20on%20top%20of%20the%20sub-network%0Ato%20bridge%20the%20gap%20between%20the%20sub-network%20and%20the%20full%20model%27s%20representation%0Aability.%20It%20is%20noteworthy%20that%20the%20inference%20latency%20of%20the%20self-draft%20model%0Amay%20no%20longer%20be%20negligible%20compared%20to%20the%20large%20model%2C%20necessitating%0Astrategies%20to%20increase%20the%20token%20acceptance%20rate%20while%20minimizing%20the%20drafting%0Asteps%20of%20the%20small%20model.%20To%20address%20this%20challenge%2C%20we%20introduce%20an%20additional%0Aearly%20exiting%20mechanism%20for%20generating%20draft%20tokens.%20Specifically%2C%20we%20halt%20the%0Asmall%20model%27s%20subsequent%20prediction%20during%20the%20drafting%20phase%20once%20the%0Aconfidence%20level%20for%20the%20current%20token%20falls%20below%20a%20certain%20threshold.%0AExtensive%20experiments%20on%20the%20Spec-Bench%20demonstrate%20the%20effectiveness%20of%0AKangaroo.%20Under%20single-sequence%20verification%2C%20Kangaroo%20achieves%20speedups%20up%20to%0A%241.68%5Ctimes%24%20on%20Spec-Bench%2C%20outperforming%20Medusa-1%20with%2088.7%5C%25%20fewer%20additional%0Aparameters%20%2867M%20compared%20to%20591M%29.%20The%20code%20for%20Kangaroo%20is%20available%20at%0Ahttps%3A//github.com/Equationliu/Kangaroo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18911v1&entry.124074799=Read"},
{"title": "A Multilevel Strategy to Improve People Tracking in a Real-World\n  Scenario", "author": "Cristiano B. de Oliveira and Joao C. Neves and Rafael O. Ribeiro and David Menotti", "abstract": "  The Pal\\'acio do Planalto, office of the President of Brazil, was invaded by\nprotesters on January 8, 2023. Surveillance videos taken from inside the\nbuilding were subsequently released by the Brazilian Supreme Court for public\nscrutiny. We used segments of such footage to create the UFPR-Planalto801\ndataset for people tracking and re-identification in a real-world scenario.\nThis dataset consists of more than 500,000 images. This paper presents a\ntracking approach targeting this dataset. The method proposed in this paper\nrelies on the use of known state-of-the-art trackers combined in a multilevel\nhierarchy to correct the ID association over the trajectories. We evaluated our\nmethod using IDF1, MOTA, MOTP and HOTA metrics. The results show improvements\nfor every tracker used in the experiments, with IDF1 score increasing by a\nmargin up to 9.5%.\n", "link": "http://arxiv.org/abs/2404.18876v1", "date": "2024-04-29", "relevancy": 1.9814, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5389}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4775}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4589}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Multilevel%20Strategy%20to%20Improve%20People%20Tracking%20in%20a%20Real-World%0A%20%20Scenario&body=Title%3A%20A%20Multilevel%20Strategy%20to%20Improve%20People%20Tracking%20in%20a%20Real-World%0A%20%20Scenario%0AAuthor%3A%20Cristiano%20B.%20de%20Oliveira%20and%20Joao%20C.%20Neves%20and%20Rafael%20O.%20Ribeiro%20and%20David%20Menotti%0AAbstract%3A%20%20%20The%20Pal%5C%27acio%20do%20Planalto%2C%20office%20of%20the%20President%20of%20Brazil%2C%20was%20invaded%20by%0Aprotesters%20on%20January%208%2C%202023.%20Surveillance%20videos%20taken%20from%20inside%20the%0Abuilding%20were%20subsequently%20released%20by%20the%20Brazilian%20Supreme%20Court%20for%20public%0Ascrutiny.%20We%20used%20segments%20of%20such%20footage%20to%20create%20the%20UFPR-Planalto801%0Adataset%20for%20people%20tracking%20and%20re-identification%20in%20a%20real-world%20scenario.%0AThis%20dataset%20consists%20of%20more%20than%20500%2C000%20images.%20This%20paper%20presents%20a%0Atracking%20approach%20targeting%20this%20dataset.%20The%20method%20proposed%20in%20this%20paper%0Arelies%20on%20the%20use%20of%20known%20state-of-the-art%20trackers%20combined%20in%20a%20multilevel%0Ahierarchy%20to%20correct%20the%20ID%20association%20over%20the%20trajectories.%20We%20evaluated%20our%0Amethod%20using%20IDF1%2C%20MOTA%2C%20MOTP%20and%20HOTA%20metrics.%20The%20results%20show%20improvements%0Afor%20every%20tracker%20used%20in%20the%20experiments%2C%20with%20IDF1%20score%20increasing%20by%20a%0Amargin%20up%20to%209.5%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18876v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multilevel%20Strategy%20to%20Improve%20People%20Tracking%20in%20a%20Real-World%0A%20%20Scenario&entry.906535625=Cristiano%20B.%20de%20Oliveira%20and%20Joao%20C.%20Neves%20and%20Rafael%20O.%20Ribeiro%20and%20David%20Menotti&entry.1292438233=%20%20The%20Pal%5C%27acio%20do%20Planalto%2C%20office%20of%20the%20President%20of%20Brazil%2C%20was%20invaded%20by%0Aprotesters%20on%20January%208%2C%202023.%20Surveillance%20videos%20taken%20from%20inside%20the%0Abuilding%20were%20subsequently%20released%20by%20the%20Brazilian%20Supreme%20Court%20for%20public%0Ascrutiny.%20We%20used%20segments%20of%20such%20footage%20to%20create%20the%20UFPR-Planalto801%0Adataset%20for%20people%20tracking%20and%20re-identification%20in%20a%20real-world%20scenario.%0AThis%20dataset%20consists%20of%20more%20than%20500%2C000%20images.%20This%20paper%20presents%20a%0Atracking%20approach%20targeting%20this%20dataset.%20The%20method%20proposed%20in%20this%20paper%0Arelies%20on%20the%20use%20of%20known%20state-of-the-art%20trackers%20combined%20in%20a%20multilevel%0Ahierarchy%20to%20correct%20the%20ID%20association%20over%20the%20trajectories.%20We%20evaluated%20our%0Amethod%20using%20IDF1%2C%20MOTA%2C%20MOTP%20and%20HOTA%20metrics.%20The%20results%20show%20improvements%0Afor%20every%20tracker%20used%20in%20the%20experiments%2C%20with%20IDF1%20score%20increasing%20by%20a%0Amargin%20up%20to%209.5%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18876v1&entry.124074799=Read"},
{"title": "Safe Reach Set Computation via Neural Barrier Certificates", "author": "Alessandro Abate and Sergiy Bogomolov and Alec Edwards and Kostiantyn Potomkin and Sadegh Soudjani and Paolo Zuliani", "abstract": "  We present a novel technique for online safety verification of autonomous\nsystems, which performs reachability analysis efficiently for both bounded and\nunbounded horizons by employing neural barrier certificates. Our approach uses\nbarrier certificates given by parameterized neural networks that depend on a\ngiven initial set, unsafe sets, and time horizon. Such networks are trained\nefficiently offline using system simulations sampled from regions of the state\nspace. We then employ a meta-neural network to generalize the barrier\ncertificates to state space regions that are outside the training set. These\ncertificates are generated and validated online as sound over-approximations of\nthe reachable states, thus either ensuring system safety or activating\nappropriate alternative actions in unsafe scenarios. We demonstrate our\ntechnique on case studies from linear models to nonlinear control-dependent\nmodels for online autonomous driving scenarios.\n", "link": "http://arxiv.org/abs/2404.18813v1", "date": "2024-04-29", "relevancy": 1.9799, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5166}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5031}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4782}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Safe%20Reach%20Set%20Computation%20via%20Neural%20Barrier%20Certificates&body=Title%3A%20Safe%20Reach%20Set%20Computation%20via%20Neural%20Barrier%20Certificates%0AAuthor%3A%20Alessandro%20Abate%20and%20Sergiy%20Bogomolov%20and%20Alec%20Edwards%20and%20Kostiantyn%20Potomkin%20and%20Sadegh%20Soudjani%20and%20Paolo%20Zuliani%0AAbstract%3A%20%20%20We%20present%20a%20novel%20technique%20for%20online%20safety%20verification%20of%20autonomous%0Asystems%2C%20which%20performs%20reachability%20analysis%20efficiently%20for%20both%20bounded%20and%0Aunbounded%20horizons%20by%20employing%20neural%20barrier%20certificates.%20Our%20approach%20uses%0Abarrier%20certificates%20given%20by%20parameterized%20neural%20networks%20that%20depend%20on%20a%0Agiven%20initial%20set%2C%20unsafe%20sets%2C%20and%20time%20horizon.%20Such%20networks%20are%20trained%0Aefficiently%20offline%20using%20system%20simulations%20sampled%20from%20regions%20of%20the%20state%0Aspace.%20We%20then%20employ%20a%20meta-neural%20network%20to%20generalize%20the%20barrier%0Acertificates%20to%20state%20space%20regions%20that%20are%20outside%20the%20training%20set.%20These%0Acertificates%20are%20generated%20and%20validated%20online%20as%20sound%20over-approximations%20of%0Athe%20reachable%20states%2C%20thus%20either%20ensuring%20system%20safety%20or%20activating%0Aappropriate%20alternative%20actions%20in%20unsafe%20scenarios.%20We%20demonstrate%20our%0Atechnique%20on%20case%20studies%20from%20linear%20models%20to%20nonlinear%20control-dependent%0Amodels%20for%20online%20autonomous%20driving%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18813v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20Reach%20Set%20Computation%20via%20Neural%20Barrier%20Certificates&entry.906535625=Alessandro%20Abate%20and%20Sergiy%20Bogomolov%20and%20Alec%20Edwards%20and%20Kostiantyn%20Potomkin%20and%20Sadegh%20Soudjani%20and%20Paolo%20Zuliani&entry.1292438233=%20%20We%20present%20a%20novel%20technique%20for%20online%20safety%20verification%20of%20autonomous%0Asystems%2C%20which%20performs%20reachability%20analysis%20efficiently%20for%20both%20bounded%20and%0Aunbounded%20horizons%20by%20employing%20neural%20barrier%20certificates.%20Our%20approach%20uses%0Abarrier%20certificates%20given%20by%20parameterized%20neural%20networks%20that%20depend%20on%20a%0Agiven%20initial%20set%2C%20unsafe%20sets%2C%20and%20time%20horizon.%20Such%20networks%20are%20trained%0Aefficiently%20offline%20using%20system%20simulations%20sampled%20from%20regions%20of%20the%20state%0Aspace.%20We%20then%20employ%20a%20meta-neural%20network%20to%20generalize%20the%20barrier%0Acertificates%20to%20state%20space%20regions%20that%20are%20outside%20the%20training%20set.%20These%0Acertificates%20are%20generated%20and%20validated%20online%20as%20sound%20over-approximations%20of%0Athe%20reachable%20states%2C%20thus%20either%20ensuring%20system%20safety%20or%20activating%0Aappropriate%20alternative%20actions%20in%20unsafe%20scenarios.%20We%20demonstrate%20our%0Atechnique%20on%20case%20studies%20from%20linear%20models%20to%20nonlinear%20control-dependent%0Amodels%20for%20online%20autonomous%20driving%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18813v1&entry.124074799=Read"},
{"title": "LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding", "author": "Mostafa Elhoushi and Akshat Shrivastava and Diana Liskovich and Basil Hosmer and Bram Wasti and Liangzhen Lai and Anas Mahmoud and Bilge Acun and Saurabh Agarwal and Ahmed Roman and Ahmed A Aly and Beidi Chen and Carole-Jean Wu", "abstract": "  We present LayerSkip, an end-to-end solution to speed-up inference of large\nlanguage models (LLMs). First, during training we apply layer dropout, with low\ndropout rates for earlier layers and higher dropout rates for later layers, and\nan early exit loss where all transformer layers share the same exit. Second,\nduring inference, we show that this training recipe increases the accuracy of\nearly exit at earlier layers, without adding any auxiliary layers or modules to\nthe model. Third, we present a novel self-speculative decoding solution where\nwe exit at early layers and verify and correct with remaining layers of the\nmodel. Our proposed self-speculative decoding approach has less memory\nfootprint than other speculative decoding approaches and benefits from shared\ncompute and activations of the draft and verification stages. We run\nexperiments on different Llama model sizes on different types of training:\npretraining from scratch, continual pretraining, finetuning on specific data\ndomain, and finetuning on specific task. We implement our inference solution\nand show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x\non coding, and 2.0x on TOPv2 semantic parsing task.\n", "link": "http://arxiv.org/abs/2404.16710v2", "date": "2024-04-29", "relevancy": 1.9793, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5258}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4888}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4885}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LayerSkip%3A%20Enabling%20Early%20Exit%20Inference%20and%20Self-Speculative%20Decoding&body=Title%3A%20LayerSkip%3A%20Enabling%20Early%20Exit%20Inference%20and%20Self-Speculative%20Decoding%0AAuthor%3A%20Mostafa%20Elhoushi%20and%20Akshat%20Shrivastava%20and%20Diana%20Liskovich%20and%20Basil%20Hosmer%20and%20Bram%20Wasti%20and%20Liangzhen%20Lai%20and%20Anas%20Mahmoud%20and%20Bilge%20Acun%20and%20Saurabh%20Agarwal%20and%20Ahmed%20Roman%20and%20Ahmed%20A%20Aly%20and%20Beidi%20Chen%20and%20Carole-Jean%20Wu%0AAbstract%3A%20%20%20We%20present%20LayerSkip%2C%20an%20end-to-end%20solution%20to%20speed-up%20inference%20of%20large%0Alanguage%20models%20%28LLMs%29.%20First%2C%20during%20training%20we%20apply%20layer%20dropout%2C%20with%20low%0Adropout%20rates%20for%20earlier%20layers%20and%20higher%20dropout%20rates%20for%20later%20layers%2C%20and%0Aan%20early%20exit%20loss%20where%20all%20transformer%20layers%20share%20the%20same%20exit.%20Second%2C%0Aduring%20inference%2C%20we%20show%20that%20this%20training%20recipe%20increases%20the%20accuracy%20of%0Aearly%20exit%20at%20earlier%20layers%2C%20without%20adding%20any%20auxiliary%20layers%20or%20modules%20to%0Athe%20model.%20Third%2C%20we%20present%20a%20novel%20self-speculative%20decoding%20solution%20where%0Awe%20exit%20at%20early%20layers%20and%20verify%20and%20correct%20with%20remaining%20layers%20of%20the%0Amodel.%20Our%20proposed%20self-speculative%20decoding%20approach%20has%20less%20memory%0Afootprint%20than%20other%20speculative%20decoding%20approaches%20and%20benefits%20from%20shared%0Acompute%20and%20activations%20of%20the%20draft%20and%20verification%20stages.%20We%20run%0Aexperiments%20on%20different%20Llama%20model%20sizes%20on%20different%20types%20of%20training%3A%0Apretraining%20from%20scratch%2C%20continual%20pretraining%2C%20finetuning%20on%20specific%20data%0Adomain%2C%20and%20finetuning%20on%20specific%20task.%20We%20implement%20our%20inference%20solution%0Aand%20show%20speedups%20of%20up%20to%202.16x%20on%20summarization%20for%20CNN/DM%20documents%2C%201.82x%0Aon%20coding%2C%20and%202.0x%20on%20TOPv2%20semantic%20parsing%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16710v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LayerSkip%3A%20Enabling%20Early%20Exit%20Inference%20and%20Self-Speculative%20Decoding&entry.906535625=Mostafa%20Elhoushi%20and%20Akshat%20Shrivastava%20and%20Diana%20Liskovich%20and%20Basil%20Hosmer%20and%20Bram%20Wasti%20and%20Liangzhen%20Lai%20and%20Anas%20Mahmoud%20and%20Bilge%20Acun%20and%20Saurabh%20Agarwal%20and%20Ahmed%20Roman%20and%20Ahmed%20A%20Aly%20and%20Beidi%20Chen%20and%20Carole-Jean%20Wu&entry.1292438233=%20%20We%20present%20LayerSkip%2C%20an%20end-to-end%20solution%20to%20speed-up%20inference%20of%20large%0Alanguage%20models%20%28LLMs%29.%20First%2C%20during%20training%20we%20apply%20layer%20dropout%2C%20with%20low%0Adropout%20rates%20for%20earlier%20layers%20and%20higher%20dropout%20rates%20for%20later%20layers%2C%20and%0Aan%20early%20exit%20loss%20where%20all%20transformer%20layers%20share%20the%20same%20exit.%20Second%2C%0Aduring%20inference%2C%20we%20show%20that%20this%20training%20recipe%20increases%20the%20accuracy%20of%0Aearly%20exit%20at%20earlier%20layers%2C%20without%20adding%20any%20auxiliary%20layers%20or%20modules%20to%0Athe%20model.%20Third%2C%20we%20present%20a%20novel%20self-speculative%20decoding%20solution%20where%0Awe%20exit%20at%20early%20layers%20and%20verify%20and%20correct%20with%20remaining%20layers%20of%20the%0Amodel.%20Our%20proposed%20self-speculative%20decoding%20approach%20has%20less%20memory%0Afootprint%20than%20other%20speculative%20decoding%20approaches%20and%20benefits%20from%20shared%0Acompute%20and%20activations%20of%20the%20draft%20and%20verification%20stages.%20We%20run%0Aexperiments%20on%20different%20Llama%20model%20sizes%20on%20different%20types%20of%20training%3A%0Apretraining%20from%20scratch%2C%20continual%20pretraining%2C%20finetuning%20on%20specific%20data%0Adomain%2C%20and%20finetuning%20on%20specific%20task.%20We%20implement%20our%20inference%20solution%0Aand%20show%20speedups%20of%20up%20to%202.16x%20on%20summarization%20for%20CNN/DM%20documents%2C%201.82x%0Aon%20coding%2C%20and%202.0x%20on%20TOPv2%20semantic%20parsing%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16710v2&entry.124074799=Read"},
{"title": "Over-Squashing in Graph Neural Networks: A Comprehensive survey", "author": "Singh Akansha", "abstract": "  Graph Neural Networks (GNNs) revolutionize machine learning for\ngraph-structured data, effectively capturing complex relationships. They\ndisseminate information through interconnected nodes, but long-range\ninteractions face challenges known as \"over-squashing\". This survey delves into\nthe challenge of over-squashing in Graph Neural Networks (GNNs), where\nlong-range information dissemination is hindered, impacting tasks reliant on\nintricate long-distance interactions. It comprehensively explores the causes,\nconsequences, and mitigation strategies for over-squashing. Various\nmethodologies are reviewed, including graph rewiring, novel normalization,\nspectral analysis, and curvature-based strategies, with a focus on their\ntrade-offs and effectiveness. The survey also discusses the interplay between\nover-squashing and other GNN limitations, such as over-smoothing, and provides\na taxonomy of models designed to address these issues in node and graph-level\ntasks. Benchmark datasets for performance evaluation are also detailed, making\nthis survey a valuable resource for researchers and practitioners in the GNN\nfield.\n", "link": "http://arxiv.org/abs/2308.15568v6", "date": "2024-04-29", "relevancy": 1.9627, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.493}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.491}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4894}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Over-Squashing%20in%20Graph%20Neural%20Networks%3A%20A%20Comprehensive%20survey&body=Title%3A%20Over-Squashing%20in%20Graph%20Neural%20Networks%3A%20A%20Comprehensive%20survey%0AAuthor%3A%20Singh%20Akansha%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20revolutionize%20machine%20learning%20for%0Agraph-structured%20data%2C%20effectively%20capturing%20complex%20relationships.%20They%0Adisseminate%20information%20through%20interconnected%20nodes%2C%20but%20long-range%0Ainteractions%20face%20challenges%20known%20as%20%22over-squashing%22.%20This%20survey%20delves%20into%0Athe%20challenge%20of%20over-squashing%20in%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20where%0Along-range%20information%20dissemination%20is%20hindered%2C%20impacting%20tasks%20reliant%20on%0Aintricate%20long-distance%20interactions.%20It%20comprehensively%20explores%20the%20causes%2C%0Aconsequences%2C%20and%20mitigation%20strategies%20for%20over-squashing.%20Various%0Amethodologies%20are%20reviewed%2C%20including%20graph%20rewiring%2C%20novel%20normalization%2C%0Aspectral%20analysis%2C%20and%20curvature-based%20strategies%2C%20with%20a%20focus%20on%20their%0Atrade-offs%20and%20effectiveness.%20The%20survey%20also%20discusses%20the%20interplay%20between%0Aover-squashing%20and%20other%20GNN%20limitations%2C%20such%20as%20over-smoothing%2C%20and%20provides%0Aa%20taxonomy%20of%20models%20designed%20to%20address%20these%20issues%20in%20node%20and%20graph-level%0Atasks.%20Benchmark%20datasets%20for%20performance%20evaluation%20are%20also%20detailed%2C%20making%0Athis%20survey%20a%20valuable%20resource%20for%20researchers%20and%20practitioners%20in%20the%20GNN%0Afield.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.15568v6", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Over-Squashing%20in%20Graph%20Neural%20Networks%3A%20A%20Comprehensive%20survey&entry.906535625=Singh%20Akansha&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20revolutionize%20machine%20learning%20for%0Agraph-structured%20data%2C%20effectively%20capturing%20complex%20relationships.%20They%0Adisseminate%20information%20through%20interconnected%20nodes%2C%20but%20long-range%0Ainteractions%20face%20challenges%20known%20as%20%22over-squashing%22.%20This%20survey%20delves%20into%0Athe%20challenge%20of%20over-squashing%20in%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20where%0Along-range%20information%20dissemination%20is%20hindered%2C%20impacting%20tasks%20reliant%20on%0Aintricate%20long-distance%20interactions.%20It%20comprehensively%20explores%20the%20causes%2C%0Aconsequences%2C%20and%20mitigation%20strategies%20for%20over-squashing.%20Various%0Amethodologies%20are%20reviewed%2C%20including%20graph%20rewiring%2C%20novel%20normalization%2C%0Aspectral%20analysis%2C%20and%20curvature-based%20strategies%2C%20with%20a%20focus%20on%20their%0Atrade-offs%20and%20effectiveness.%20The%20survey%20also%20discusses%20the%20interplay%20between%0Aover-squashing%20and%20other%20GNN%20limitations%2C%20such%20as%20over-smoothing%2C%20and%20provides%0Aa%20taxonomy%20of%20models%20designed%20to%20address%20these%20issues%20in%20node%20and%20graph-level%0Atasks.%20Benchmark%20datasets%20for%20performance%20evaluation%20are%20also%20detailed%2C%20making%0Athis%20survey%20a%20valuable%20resource%20for%20researchers%20and%20practitioners%20in%20the%20GNN%0Afield.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.15568v6&entry.124074799=Read"},
{"title": "Towards Quantitative Evaluation of Explainable AI Methods for Deepfake\n  Detection", "author": "Konstantinos Tsigos and Evlampios Apostolidis and Spyridon Baxevanakis and Symeon Papadopoulos and Vasileios Mezaris", "abstract": "  In this paper we propose a new framework for evaluating the performance of\nexplanation methods on the decisions of a deepfake detector. This framework\nassesses the ability of an explanation method to spot the regions of a fake\nimage with the biggest influence on the decision of the deepfake detector, by\nexamining the extent to which these regions can be modified through a set of\nadversarial attacks, in order to flip the detector's prediction or reduce its\ninitial prediction; we anticipate a larger drop in deepfake detection accuracy\nand prediction, for methods that spot these regions more accurately. Based on\nthis framework, we conduct a comparative study using a state-of-the-art model\nfor deepfake detection that has been trained on the FaceForensics++ dataset,\nand five explanation methods from the literature. The findings of our\nquantitative and qualitative evaluations document the advanced performance of\nthe LIME explanation method against the other compared ones, and indicate this\nmethod as the most appropriate for explaining the decisions of the utilized\ndeepfake detector.\n", "link": "http://arxiv.org/abs/2404.18649v1", "date": "2024-04-29", "relevancy": 1.9624, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5413}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4875}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4734}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Quantitative%20Evaluation%20of%20Explainable%20AI%20Methods%20for%20Deepfake%0A%20%20Detection&body=Title%3A%20Towards%20Quantitative%20Evaluation%20of%20Explainable%20AI%20Methods%20for%20Deepfake%0A%20%20Detection%0AAuthor%3A%20Konstantinos%20Tsigos%20and%20Evlampios%20Apostolidis%20and%20Spyridon%20Baxevanakis%20and%20Symeon%20Papadopoulos%20and%20Vasileios%20Mezaris%0AAbstract%3A%20%20%20In%20this%20paper%20we%20propose%20a%20new%20framework%20for%20evaluating%20the%20performance%20of%0Aexplanation%20methods%20on%20the%20decisions%20of%20a%20deepfake%20detector.%20This%20framework%0Aassesses%20the%20ability%20of%20an%20explanation%20method%20to%20spot%20the%20regions%20of%20a%20fake%0Aimage%20with%20the%20biggest%20influence%20on%20the%20decision%20of%20the%20deepfake%20detector%2C%20by%0Aexamining%20the%20extent%20to%20which%20these%20regions%20can%20be%20modified%20through%20a%20set%20of%0Aadversarial%20attacks%2C%20in%20order%20to%20flip%20the%20detector%27s%20prediction%20or%20reduce%20its%0Ainitial%20prediction%3B%20we%20anticipate%20a%20larger%20drop%20in%20deepfake%20detection%20accuracy%0Aand%20prediction%2C%20for%20methods%20that%20spot%20these%20regions%20more%20accurately.%20Based%20on%0Athis%20framework%2C%20we%20conduct%20a%20comparative%20study%20using%20a%20state-of-the-art%20model%0Afor%20deepfake%20detection%20that%20has%20been%20trained%20on%20the%20FaceForensics%2B%2B%20dataset%2C%0Aand%20five%20explanation%20methods%20from%20the%20literature.%20The%20findings%20of%20our%0Aquantitative%20and%20qualitative%20evaluations%20document%20the%20advanced%20performance%20of%0Athe%20LIME%20explanation%20method%20against%20the%20other%20compared%20ones%2C%20and%20indicate%20this%0Amethod%20as%20the%20most%20appropriate%20for%20explaining%20the%20decisions%20of%20the%20utilized%0Adeepfake%20detector.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18649v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Quantitative%20Evaluation%20of%20Explainable%20AI%20Methods%20for%20Deepfake%0A%20%20Detection&entry.906535625=Konstantinos%20Tsigos%20and%20Evlampios%20Apostolidis%20and%20Spyridon%20Baxevanakis%20and%20Symeon%20Papadopoulos%20and%20Vasileios%20Mezaris&entry.1292438233=%20%20In%20this%20paper%20we%20propose%20a%20new%20framework%20for%20evaluating%20the%20performance%20of%0Aexplanation%20methods%20on%20the%20decisions%20of%20a%20deepfake%20detector.%20This%20framework%0Aassesses%20the%20ability%20of%20an%20explanation%20method%20to%20spot%20the%20regions%20of%20a%20fake%0Aimage%20with%20the%20biggest%20influence%20on%20the%20decision%20of%20the%20deepfake%20detector%2C%20by%0Aexamining%20the%20extent%20to%20which%20these%20regions%20can%20be%20modified%20through%20a%20set%20of%0Aadversarial%20attacks%2C%20in%20order%20to%20flip%20the%20detector%27s%20prediction%20or%20reduce%20its%0Ainitial%20prediction%3B%20we%20anticipate%20a%20larger%20drop%20in%20deepfake%20detection%20accuracy%0Aand%20prediction%2C%20for%20methods%20that%20spot%20these%20regions%20more%20accurately.%20Based%20on%0Athis%20framework%2C%20we%20conduct%20a%20comparative%20study%20using%20a%20state-of-the-art%20model%0Afor%20deepfake%20detection%20that%20has%20been%20trained%20on%20the%20FaceForensics%2B%2B%20dataset%2C%0Aand%20five%20explanation%20methods%20from%20the%20literature.%20The%20findings%20of%20our%0Aquantitative%20and%20qualitative%20evaluations%20document%20the%20advanced%20performance%20of%0Athe%20LIME%20explanation%20method%20against%20the%20other%20compared%20ones%2C%20and%20indicate%20this%0Amethod%20as%20the%20most%20appropriate%20for%20explaining%20the%20decisions%20of%20the%20utilized%0Adeepfake%20detector.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18649v1&entry.124074799=Read"},
{"title": "Benchmarking Benchmark Leakage in Large Language Models", "author": "Ruijie Xu and Zengzhi Wang and Run-Ze Fan and Pengfei Liu", "abstract": "  Amid the expanding use of pre-training data, the phenomenon of benchmark\ndataset leakage has become increasingly prominent, exacerbated by opaque\ntraining processes and the often undisclosed inclusion of supervised data in\ncontemporary Large Language Models (LLMs). This issue skews benchmark\neffectiveness and fosters potentially unfair comparisons, impeding the field's\nhealthy development. To address this, we introduce a detection pipeline\nutilizing Perplexity and N-gram accuracy, two simple and scalable metrics that\ngauge a model's prediction precision on benchmark, to identify potential data\nleakages. By analyzing 31 LLMs under the context of mathematical reasoning, we\nreveal substantial instances of training even test set misuse, resulting in\npotentially unfair comparisons. These findings prompt us to offer several\nrecommendations regarding model documentation, benchmark setup, and future\nevaluations. Notably, we propose the \"Benchmark Transparency Card\" to encourage\nclear documentation of benchmark utilization, promoting transparency and\nhealthy developments of LLMs. we have made our leaderboard, pipeline\nimplementation, and model predictions publicly available, fostering future\nresearch.\n", "link": "http://arxiv.org/abs/2404.18824v1", "date": "2024-04-29", "relevancy": 1.9596, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5114}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4867}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4845}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Benchmark%20Leakage%20in%20Large%20Language%20Models&body=Title%3A%20Benchmarking%20Benchmark%20Leakage%20in%20Large%20Language%20Models%0AAuthor%3A%20Ruijie%20Xu%20and%20Zengzhi%20Wang%20and%20Run-Ze%20Fan%20and%20Pengfei%20Liu%0AAbstract%3A%20%20%20Amid%20the%20expanding%20use%20of%20pre-training%20data%2C%20the%20phenomenon%20of%20benchmark%0Adataset%20leakage%20has%20become%20increasingly%20prominent%2C%20exacerbated%20by%20opaque%0Atraining%20processes%20and%20the%20often%20undisclosed%20inclusion%20of%20supervised%20data%20in%0Acontemporary%20Large%20Language%20Models%20%28LLMs%29.%20This%20issue%20skews%20benchmark%0Aeffectiveness%20and%20fosters%20potentially%20unfair%20comparisons%2C%20impeding%20the%20field%27s%0Ahealthy%20development.%20To%20address%20this%2C%20we%20introduce%20a%20detection%20pipeline%0Autilizing%20Perplexity%20and%20N-gram%20accuracy%2C%20two%20simple%20and%20scalable%20metrics%20that%0Agauge%20a%20model%27s%20prediction%20precision%20on%20benchmark%2C%20to%20identify%20potential%20data%0Aleakages.%20By%20analyzing%2031%20LLMs%20under%20the%20context%20of%20mathematical%20reasoning%2C%20we%0Areveal%20substantial%20instances%20of%20training%20even%20test%20set%20misuse%2C%20resulting%20in%0Apotentially%20unfair%20comparisons.%20These%20findings%20prompt%20us%20to%20offer%20several%0Arecommendations%20regarding%20model%20documentation%2C%20benchmark%20setup%2C%20and%20future%0Aevaluations.%20Notably%2C%20we%20propose%20the%20%22Benchmark%20Transparency%20Card%22%20to%20encourage%0Aclear%20documentation%20of%20benchmark%20utilization%2C%20promoting%20transparency%20and%0Ahealthy%20developments%20of%20LLMs.%20we%20have%20made%20our%20leaderboard%2C%20pipeline%0Aimplementation%2C%20and%20model%20predictions%20publicly%20available%2C%20fostering%20future%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18824v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Benchmark%20Leakage%20in%20Large%20Language%20Models&entry.906535625=Ruijie%20Xu%20and%20Zengzhi%20Wang%20and%20Run-Ze%20Fan%20and%20Pengfei%20Liu&entry.1292438233=%20%20Amid%20the%20expanding%20use%20of%20pre-training%20data%2C%20the%20phenomenon%20of%20benchmark%0Adataset%20leakage%20has%20become%20increasingly%20prominent%2C%20exacerbated%20by%20opaque%0Atraining%20processes%20and%20the%20often%20undisclosed%20inclusion%20of%20supervised%20data%20in%0Acontemporary%20Large%20Language%20Models%20%28LLMs%29.%20This%20issue%20skews%20benchmark%0Aeffectiveness%20and%20fosters%20potentially%20unfair%20comparisons%2C%20impeding%20the%20field%27s%0Ahealthy%20development.%20To%20address%20this%2C%20we%20introduce%20a%20detection%20pipeline%0Autilizing%20Perplexity%20and%20N-gram%20accuracy%2C%20two%20simple%20and%20scalable%20metrics%20that%0Agauge%20a%20model%27s%20prediction%20precision%20on%20benchmark%2C%20to%20identify%20potential%20data%0Aleakages.%20By%20analyzing%2031%20LLMs%20under%20the%20context%20of%20mathematical%20reasoning%2C%20we%0Areveal%20substantial%20instances%20of%20training%20even%20test%20set%20misuse%2C%20resulting%20in%0Apotentially%20unfair%20comparisons.%20These%20findings%20prompt%20us%20to%20offer%20several%0Arecommendations%20regarding%20model%20documentation%2C%20benchmark%20setup%2C%20and%20future%0Aevaluations.%20Notably%2C%20we%20propose%20the%20%22Benchmark%20Transparency%20Card%22%20to%20encourage%0Aclear%20documentation%20of%20benchmark%20utilization%2C%20promoting%20transparency%20and%0Ahealthy%20developments%20of%20LLMs.%20we%20have%20made%20our%20leaderboard%2C%20pipeline%0Aimplementation%2C%20and%20model%20predictions%20publicly%20available%2C%20fostering%20future%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18824v1&entry.124074799=Read"},
{"title": "Revealing the Parametric Knowledge of Language Models: A Unified\n  Framework for Attribution Methods", "author": "Haeun Yu and Pepa Atanasova and Isabelle Augenstein", "abstract": "  Language Models (LMs) acquire parametric knowledge from their training\nprocess, embedding it within their weights. The increasing scalability of LMs,\nhowever, poses significant challenges for understanding a model's inner\nworkings and further for updating or correcting this embedded knowledge without\nthe significant cost of retraining. This underscores the importance of\nunveiling exactly what knowledge is stored and its association with specific\nmodel components. Instance Attribution (IA) and Neuron Attribution (NA) offer\ninsights into this training-acquired knowledge, though they have not been\ncompared systematically. Our study introduces a novel evaluation framework to\nquantify and compare the knowledge revealed by IA and NA. To align the results\nof the methods we introduce the attribution method NA-Instances to apply NA for\nretrieving influential training instances, and IA-Neurons to discover important\nneurons of influential instances discovered by IA. We further propose a\ncomprehensive list of faithfulness tests to evaluate the comprehensiveness and\nsufficiency of the explanations provided by both methods. Through extensive\nexperiments and analysis, we demonstrate that NA generally reveals more diverse\nand comprehensive information regarding the LM's parametric knowledge compared\nto IA. Nevertheless, IA provides unique and valuable insights into the LM's\nparametric knowledge, which are not revealed by NA. Our findings further\nsuggest the potential of a synergistic approach of combining the diverse\nfindings of IA and NA for a more holistic understanding of an LM's parametric\nknowledge.\n", "link": "http://arxiv.org/abs/2404.18655v1", "date": "2024-04-29", "relevancy": 1.9494, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5042}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4877}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4802}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Revealing%20the%20Parametric%20Knowledge%20of%20Language%20Models%3A%20A%20Unified%0A%20%20Framework%20for%20Attribution%20Methods&body=Title%3A%20Revealing%20the%20Parametric%20Knowledge%20of%20Language%20Models%3A%20A%20Unified%0A%20%20Framework%20for%20Attribution%20Methods%0AAuthor%3A%20Haeun%20Yu%20and%20Pepa%20Atanasova%20and%20Isabelle%20Augenstein%0AAbstract%3A%20%20%20Language%20Models%20%28LMs%29%20acquire%20parametric%20knowledge%20from%20their%20training%0Aprocess%2C%20embedding%20it%20within%20their%20weights.%20The%20increasing%20scalability%20of%20LMs%2C%0Ahowever%2C%20poses%20significant%20challenges%20for%20understanding%20a%20model%27s%20inner%0Aworkings%20and%20further%20for%20updating%20or%20correcting%20this%20embedded%20knowledge%20without%0Athe%20significant%20cost%20of%20retraining.%20This%20underscores%20the%20importance%20of%0Aunveiling%20exactly%20what%20knowledge%20is%20stored%20and%20its%20association%20with%20specific%0Amodel%20components.%20Instance%20Attribution%20%28IA%29%20and%20Neuron%20Attribution%20%28NA%29%20offer%0Ainsights%20into%20this%20training-acquired%20knowledge%2C%20though%20they%20have%20not%20been%0Acompared%20systematically.%20Our%20study%20introduces%20a%20novel%20evaluation%20framework%20to%0Aquantify%20and%20compare%20the%20knowledge%20revealed%20by%20IA%20and%20NA.%20To%20align%20the%20results%0Aof%20the%20methods%20we%20introduce%20the%20attribution%20method%20NA-Instances%20to%20apply%20NA%20for%0Aretrieving%20influential%20training%20instances%2C%20and%20IA-Neurons%20to%20discover%20important%0Aneurons%20of%20influential%20instances%20discovered%20by%20IA.%20We%20further%20propose%20a%0Acomprehensive%20list%20of%20faithfulness%20tests%20to%20evaluate%20the%20comprehensiveness%20and%0Asufficiency%20of%20the%20explanations%20provided%20by%20both%20methods.%20Through%20extensive%0Aexperiments%20and%20analysis%2C%20we%20demonstrate%20that%20NA%20generally%20reveals%20more%20diverse%0Aand%20comprehensive%20information%20regarding%20the%20LM%27s%20parametric%20knowledge%20compared%0Ato%20IA.%20Nevertheless%2C%20IA%20provides%20unique%20and%20valuable%20insights%20into%20the%20LM%27s%0Aparametric%20knowledge%2C%20which%20are%20not%20revealed%20by%20NA.%20Our%20findings%20further%0Asuggest%20the%20potential%20of%20a%20synergistic%20approach%20of%20combining%20the%20diverse%0Afindings%20of%20IA%20and%20NA%20for%20a%20more%20holistic%20understanding%20of%20an%20LM%27s%20parametric%0Aknowledge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18655v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revealing%20the%20Parametric%20Knowledge%20of%20Language%20Models%3A%20A%20Unified%0A%20%20Framework%20for%20Attribution%20Methods&entry.906535625=Haeun%20Yu%20and%20Pepa%20Atanasova%20and%20Isabelle%20Augenstein&entry.1292438233=%20%20Language%20Models%20%28LMs%29%20acquire%20parametric%20knowledge%20from%20their%20training%0Aprocess%2C%20embedding%20it%20within%20their%20weights.%20The%20increasing%20scalability%20of%20LMs%2C%0Ahowever%2C%20poses%20significant%20challenges%20for%20understanding%20a%20model%27s%20inner%0Aworkings%20and%20further%20for%20updating%20or%20correcting%20this%20embedded%20knowledge%20without%0Athe%20significant%20cost%20of%20retraining.%20This%20underscores%20the%20importance%20of%0Aunveiling%20exactly%20what%20knowledge%20is%20stored%20and%20its%20association%20with%20specific%0Amodel%20components.%20Instance%20Attribution%20%28IA%29%20and%20Neuron%20Attribution%20%28NA%29%20offer%0Ainsights%20into%20this%20training-acquired%20knowledge%2C%20though%20they%20have%20not%20been%0Acompared%20systematically.%20Our%20study%20introduces%20a%20novel%20evaluation%20framework%20to%0Aquantify%20and%20compare%20the%20knowledge%20revealed%20by%20IA%20and%20NA.%20To%20align%20the%20results%0Aof%20the%20methods%20we%20introduce%20the%20attribution%20method%20NA-Instances%20to%20apply%20NA%20for%0Aretrieving%20influential%20training%20instances%2C%20and%20IA-Neurons%20to%20discover%20important%0Aneurons%20of%20influential%20instances%20discovered%20by%20IA.%20We%20further%20propose%20a%0Acomprehensive%20list%20of%20faithfulness%20tests%20to%20evaluate%20the%20comprehensiveness%20and%0Asufficiency%20of%20the%20explanations%20provided%20by%20both%20methods.%20Through%20extensive%0Aexperiments%20and%20analysis%2C%20we%20demonstrate%20that%20NA%20generally%20reveals%20more%20diverse%0Aand%20comprehensive%20information%20regarding%20the%20LM%27s%20parametric%20knowledge%20compared%0Ato%20IA.%20Nevertheless%2C%20IA%20provides%20unique%20and%20valuable%20insights%20into%20the%20LM%27s%0Aparametric%20knowledge%2C%20which%20are%20not%20revealed%20by%20NA.%20Our%20findings%20further%0Asuggest%20the%20potential%20of%20a%20synergistic%20approach%20of%20combining%20the%20diverse%0Afindings%20of%20IA%20and%20NA%20for%20a%20more%20holistic%20understanding%20of%20an%20LM%27s%20parametric%0Aknowledge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18655v1&entry.124074799=Read"},
{"title": "Learning Governing Equations of Unobserved States in Dynamical Systems", "author": "Gevik Grigorian and Sandip V. George and Simon Arridge", "abstract": "  Data driven modelling and scientific machine learning have been responsible\nfor significant advances in determining suitable models to describe data.\nWithin dynamical systems, neural ordinary differential equations (ODEs), where\nthe system equations are set to be governed by a neural network, have become a\npopular tool for this challenge in recent years. However, less emphasis has\nbeen placed on systems that are only partially-observed. In this work, we\nemploy a hybrid neural ODE structure, where the system equations are governed\nby a combination of a neural network and domain-specific knowledge, together\nwith symbolic regression (SR), to learn governing equations of\npartially-observed dynamical systems. We test this approach on two case\nstudies: A 3-dimensional model of the Lotka-Volterra system and a 5-dimensional\nmodel of the Lorenz system. We demonstrate that the method is capable of\nsuccessfully learning the true underlying governing equations of unobserved\nstates within these systems, with robustness to measurement noise.\n", "link": "http://arxiv.org/abs/2404.18572v1", "date": "2024-04-29", "relevancy": 1.946, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5161}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4886}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4726}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20Governing%20Equations%20of%20Unobserved%20States%20in%20Dynamical%20Systems&body=Title%3A%20Learning%20Governing%20Equations%20of%20Unobserved%20States%20in%20Dynamical%20Systems%0AAuthor%3A%20Gevik%20Grigorian%20and%20Sandip%20V.%20George%20and%20Simon%20Arridge%0AAbstract%3A%20%20%20Data%20driven%20modelling%20and%20scientific%20machine%20learning%20have%20been%20responsible%0Afor%20significant%20advances%20in%20determining%20suitable%20models%20to%20describe%20data.%0AWithin%20dynamical%20systems%2C%20neural%20ordinary%20differential%20equations%20%28ODEs%29%2C%20where%0Athe%20system%20equations%20are%20set%20to%20be%20governed%20by%20a%20neural%20network%2C%20have%20become%20a%0Apopular%20tool%20for%20this%20challenge%20in%20recent%20years.%20However%2C%20less%20emphasis%20has%0Abeen%20placed%20on%20systems%20that%20are%20only%20partially-observed.%20In%20this%20work%2C%20we%0Aemploy%20a%20hybrid%20neural%20ODE%20structure%2C%20where%20the%20system%20equations%20are%20governed%0Aby%20a%20combination%20of%20a%20neural%20network%20and%20domain-specific%20knowledge%2C%20together%0Awith%20symbolic%20regression%20%28SR%29%2C%20to%20learn%20governing%20equations%20of%0Apartially-observed%20dynamical%20systems.%20We%20test%20this%20approach%20on%20two%20case%0Astudies%3A%20A%203-dimensional%20model%20of%20the%20Lotka-Volterra%20system%20and%20a%205-dimensional%0Amodel%20of%20the%20Lorenz%20system.%20We%20demonstrate%20that%20the%20method%20is%20capable%20of%0Asuccessfully%20learning%20the%20true%20underlying%20governing%20equations%20of%20unobserved%0Astates%20within%20these%20systems%2C%20with%20robustness%20to%20measurement%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18572v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Governing%20Equations%20of%20Unobserved%20States%20in%20Dynamical%20Systems&entry.906535625=Gevik%20Grigorian%20and%20Sandip%20V.%20George%20and%20Simon%20Arridge&entry.1292438233=%20%20Data%20driven%20modelling%20and%20scientific%20machine%20learning%20have%20been%20responsible%0Afor%20significant%20advances%20in%20determining%20suitable%20models%20to%20describe%20data.%0AWithin%20dynamical%20systems%2C%20neural%20ordinary%20differential%20equations%20%28ODEs%29%2C%20where%0Athe%20system%20equations%20are%20set%20to%20be%20governed%20by%20a%20neural%20network%2C%20have%20become%20a%0Apopular%20tool%20for%20this%20challenge%20in%20recent%20years.%20However%2C%20less%20emphasis%20has%0Abeen%20placed%20on%20systems%20that%20are%20only%20partially-observed.%20In%20this%20work%2C%20we%0Aemploy%20a%20hybrid%20neural%20ODE%20structure%2C%20where%20the%20system%20equations%20are%20governed%0Aby%20a%20combination%20of%20a%20neural%20network%20and%20domain-specific%20knowledge%2C%20together%0Awith%20symbolic%20regression%20%28SR%29%2C%20to%20learn%20governing%20equations%20of%0Apartially-observed%20dynamical%20systems.%20We%20test%20this%20approach%20on%20two%20case%0Astudies%3A%20A%203-dimensional%20model%20of%20the%20Lotka-Volterra%20system%20and%20a%205-dimensional%0Amodel%20of%20the%20Lorenz%20system.%20We%20demonstrate%20that%20the%20method%20is%20capable%20of%0Asuccessfully%20learning%20the%20true%20underlying%20governing%20equations%20of%20unobserved%0Astates%20within%20these%20systems%2C%20with%20robustness%20to%20measurement%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18572v1&entry.124074799=Read"},
{"title": "Where on Earth Do Users Say They Are?: Geo-Entity Linking for Noisy\n  Multilingual User Input", "author": "Tessa Masis and Brendan O'Connor", "abstract": "  Geo-entity linking is the task of linking a location mention to the\nreal-world geographic location. In this paper we explore the challenging task\nof geo-entity linking for noisy, multilingual social media data. There are few\nopen-source multilingual geo-entity linking tools available and existing ones\nare often rule-based, which break easily in social media settings, or\nLLM-based, which are too expensive for large-scale datasets. We present a\nmethod which represents real-world locations as averaged embeddings from\nlabeled user-input location names and allows for selective prediction via an\ninterpretable confidence score. We show that our approach improves geo-entity\nlinking on a global and multilingual social media dataset, and discuss progress\nand problems with evaluating at different geographic granularities.\n", "link": "http://arxiv.org/abs/2404.18784v1", "date": "2024-04-29", "relevancy": 1.9421, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4901}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4841}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4776}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Where%20on%20Earth%20Do%20Users%20Say%20They%20Are%3F%3A%20Geo-Entity%20Linking%20for%20Noisy%0A%20%20Multilingual%20User%20Input&body=Title%3A%20Where%20on%20Earth%20Do%20Users%20Say%20They%20Are%3F%3A%20Geo-Entity%20Linking%20for%20Noisy%0A%20%20Multilingual%20User%20Input%0AAuthor%3A%20Tessa%20Masis%20and%20Brendan%20O%27Connor%0AAbstract%3A%20%20%20Geo-entity%20linking%20is%20the%20task%20of%20linking%20a%20location%20mention%20to%20the%0Areal-world%20geographic%20location.%20In%20this%20paper%20we%20explore%20the%20challenging%20task%0Aof%20geo-entity%20linking%20for%20noisy%2C%20multilingual%20social%20media%20data.%20There%20are%20few%0Aopen-source%20multilingual%20geo-entity%20linking%20tools%20available%20and%20existing%20ones%0Aare%20often%20rule-based%2C%20which%20break%20easily%20in%20social%20media%20settings%2C%20or%0ALLM-based%2C%20which%20are%20too%20expensive%20for%20large-scale%20datasets.%20We%20present%20a%0Amethod%20which%20represents%20real-world%20locations%20as%20averaged%20embeddings%20from%0Alabeled%20user-input%20location%20names%20and%20allows%20for%20selective%20prediction%20via%20an%0Ainterpretable%20confidence%20score.%20We%20show%20that%20our%20approach%20improves%20geo-entity%0Alinking%20on%20a%20global%20and%20multilingual%20social%20media%20dataset%2C%20and%20discuss%20progress%0Aand%20problems%20with%20evaluating%20at%20different%20geographic%20granularities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18784v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Where%20on%20Earth%20Do%20Users%20Say%20They%20Are%3F%3A%20Geo-Entity%20Linking%20for%20Noisy%0A%20%20Multilingual%20User%20Input&entry.906535625=Tessa%20Masis%20and%20Brendan%20O%27Connor&entry.1292438233=%20%20Geo-entity%20linking%20is%20the%20task%20of%20linking%20a%20location%20mention%20to%20the%0Areal-world%20geographic%20location.%20In%20this%20paper%20we%20explore%20the%20challenging%20task%0Aof%20geo-entity%20linking%20for%20noisy%2C%20multilingual%20social%20media%20data.%20There%20are%20few%0Aopen-source%20multilingual%20geo-entity%20linking%20tools%20available%20and%20existing%20ones%0Aare%20often%20rule-based%2C%20which%20break%20easily%20in%20social%20media%20settings%2C%20or%0ALLM-based%2C%20which%20are%20too%20expensive%20for%20large-scale%20datasets.%20We%20present%20a%0Amethod%20which%20represents%20real-world%20locations%20as%20averaged%20embeddings%20from%0Alabeled%20user-input%20location%20names%20and%20allows%20for%20selective%20prediction%20via%20an%0Ainterpretable%20confidence%20score.%20We%20show%20that%20our%20approach%20improves%20geo-entity%0Alinking%20on%20a%20global%20and%20multilingual%20social%20media%20dataset%2C%20and%20discuss%20progress%0Aand%20problems%20with%20evaluating%20at%20different%20geographic%20granularities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18784v1&entry.124074799=Read"},
{"title": "An Incremental MaxSAT-based Model to Learn Interpretable and Balanced\n  Classification Rules", "author": "Ant\u00f4nio Carlos Souza Ferreira J\u00fanior and Thiago Alves Rocha", "abstract": "  The increasing advancements in the field of machine learning have led to the\ndevelopment of numerous applications that effectively address a wide range of\nproblems with accurate predictions. However, in certain cases, accuracy alone\nmay not be sufficient. Many real-world problems also demand explanations and\ninterpretability behind the predictions. One of the most popular interpretable\nmodels that are classification rules. This work aims to propose an incremental\nmodel for learning interpretable and balanced rules based on MaxSAT, called\nIMLIB. This new model was based on two other approaches, one based on SAT and\nthe other on MaxSAT. The one based on SAT limits the size of each generated\nrule, making it possible to balance them. We suggest that such a set of rules\nseem more natural to be understood compared to a mixture of large and small\nrules. The approach based on MaxSAT, called IMLI, presents a technique to\nincrease performance that involves learning a set of rules by incrementally\napplying the model in a dataset. Finally, IMLIB and IMLI are compared using\ndiverse databases. IMLIB obtained results comparable to IMLI in terms of\naccuracy, generating more balanced rules with smaller sizes.\n", "link": "http://arxiv.org/abs/2403.16418v2", "date": "2024-04-29", "relevancy": 1.9357, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4923}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4828}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4817}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20An%20Incremental%20MaxSAT-based%20Model%20to%20Learn%20Interpretable%20and%20Balanced%0A%20%20Classification%20Rules&body=Title%3A%20An%20Incremental%20MaxSAT-based%20Model%20to%20Learn%20Interpretable%20and%20Balanced%0A%20%20Classification%20Rules%0AAuthor%3A%20Ant%C3%B4nio%20Carlos%20Souza%20Ferreira%20J%C3%BAnior%20and%20Thiago%20Alves%20Rocha%0AAbstract%3A%20%20%20The%20increasing%20advancements%20in%20the%20field%20of%20machine%20learning%20have%20led%20to%20the%0Adevelopment%20of%20numerous%20applications%20that%20effectively%20address%20a%20wide%20range%20of%0Aproblems%20with%20accurate%20predictions.%20However%2C%20in%20certain%20cases%2C%20accuracy%20alone%0Amay%20not%20be%20sufficient.%20Many%20real-world%20problems%20also%20demand%20explanations%20and%0Ainterpretability%20behind%20the%20predictions.%20One%20of%20the%20most%20popular%20interpretable%0Amodels%20that%20are%20classification%20rules.%20This%20work%20aims%20to%20propose%20an%20incremental%0Amodel%20for%20learning%20interpretable%20and%20balanced%20rules%20based%20on%20MaxSAT%2C%20called%0AIMLIB.%20This%20new%20model%20was%20based%20on%20two%20other%20approaches%2C%20one%20based%20on%20SAT%20and%0Athe%20other%20on%20MaxSAT.%20The%20one%20based%20on%20SAT%20limits%20the%20size%20of%20each%20generated%0Arule%2C%20making%20it%20possible%20to%20balance%20them.%20We%20suggest%20that%20such%20a%20set%20of%20rules%0Aseem%20more%20natural%20to%20be%20understood%20compared%20to%20a%20mixture%20of%20large%20and%20small%0Arules.%20The%20approach%20based%20on%20MaxSAT%2C%20called%20IMLI%2C%20presents%20a%20technique%20to%0Aincrease%20performance%20that%20involves%20learning%20a%20set%20of%20rules%20by%20incrementally%0Aapplying%20the%20model%20in%20a%20dataset.%20Finally%2C%20IMLIB%20and%20IMLI%20are%20compared%20using%0Adiverse%20databases.%20IMLIB%20obtained%20results%20comparable%20to%20IMLI%20in%20terms%20of%0Aaccuracy%2C%20generating%20more%20balanced%20rules%20with%20smaller%20sizes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16418v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Incremental%20MaxSAT-based%20Model%20to%20Learn%20Interpretable%20and%20Balanced%0A%20%20Classification%20Rules&entry.906535625=Ant%C3%B4nio%20Carlos%20Souza%20Ferreira%20J%C3%BAnior%20and%20Thiago%20Alves%20Rocha&entry.1292438233=%20%20The%20increasing%20advancements%20in%20the%20field%20of%20machine%20learning%20have%20led%20to%20the%0Adevelopment%20of%20numerous%20applications%20that%20effectively%20address%20a%20wide%20range%20of%0Aproblems%20with%20accurate%20predictions.%20However%2C%20in%20certain%20cases%2C%20accuracy%20alone%0Amay%20not%20be%20sufficient.%20Many%20real-world%20problems%20also%20demand%20explanations%20and%0Ainterpretability%20behind%20the%20predictions.%20One%20of%20the%20most%20popular%20interpretable%0Amodels%20that%20are%20classification%20rules.%20This%20work%20aims%20to%20propose%20an%20incremental%0Amodel%20for%20learning%20interpretable%20and%20balanced%20rules%20based%20on%20MaxSAT%2C%20called%0AIMLIB.%20This%20new%20model%20was%20based%20on%20two%20other%20approaches%2C%20one%20based%20on%20SAT%20and%0Athe%20other%20on%20MaxSAT.%20The%20one%20based%20on%20SAT%20limits%20the%20size%20of%20each%20generated%0Arule%2C%20making%20it%20possible%20to%20balance%20them.%20We%20suggest%20that%20such%20a%20set%20of%20rules%0Aseem%20more%20natural%20to%20be%20understood%20compared%20to%20a%20mixture%20of%20large%20and%20small%0Arules.%20The%20approach%20based%20on%20MaxSAT%2C%20called%20IMLI%2C%20presents%20a%20technique%20to%0Aincrease%20performance%20that%20involves%20learning%20a%20set%20of%20rules%20by%20incrementally%0Aapplying%20the%20model%20in%20a%20dataset.%20Finally%2C%20IMLIB%20and%20IMLI%20are%20compared%20using%0Adiverse%20databases.%20IMLIB%20obtained%20results%20comparable%20to%20IMLI%20in%20terms%20of%0Aaccuracy%2C%20generating%20more%20balanced%20rules%20with%20smaller%20sizes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16418v2&entry.124074799=Read"},
{"title": "A Partial Replication of MaskFormer in TensorFlow on TPUs for the\n  TensorFlow Model Garden", "author": "Vishal Purohit and Wenxin Jiang and Akshath R. Ravikiran and James C. Davis", "abstract": "  This paper undertakes the task of replicating the MaskFormer model a\nuniversal image segmentation model originally developed using the PyTorch\nframework, within the TensorFlow ecosystem, specifically optimized for\nexecution on Tensor Processing Units (TPUs). Our implementation exploits the\nmodular constructs available within the TensorFlow Model Garden (TFMG),\nencompassing elements such as the data loader, training orchestrator, and\nvarious architectural components, tailored and adapted to meet the\nspecifications of the MaskFormer model. We address key challenges encountered\nduring the replication, non-convergence issues, slow training, adaptation of\nloss functions, and the integration of TPU-specific functionalities. We verify\nour reproduced implementation and present qualitative results on the COCO\ndataset. Although our implementation meets some of the objectives for\nend-to-end reproducibility, we encountered challenges in replicating the\nPyTorch version of MaskFormer in TensorFlow. This replication process is not\nstraightforward and requires substantial engineering efforts. Specifically, it\nnecessitates the customization of various components within the TFMG, alongside\nthorough verification and hyper-parameter tuning. The replication is available\nat:\nhttps://github.com/PurdueDualityLab/tf-maskformer/tree/main/official/projects/maskformer\n", "link": "http://arxiv.org/abs/2404.18801v1", "date": "2024-04-29", "relevancy": 1.923, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4994}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4857}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4683}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Partial%20Replication%20of%20MaskFormer%20in%20TensorFlow%20on%20TPUs%20for%20the%0A%20%20TensorFlow%20Model%20Garden&body=Title%3A%20A%20Partial%20Replication%20of%20MaskFormer%20in%20TensorFlow%20on%20TPUs%20for%20the%0A%20%20TensorFlow%20Model%20Garden%0AAuthor%3A%20Vishal%20Purohit%20and%20Wenxin%20Jiang%20and%20Akshath%20R.%20Ravikiran%20and%20James%20C.%20Davis%0AAbstract%3A%20%20%20This%20paper%20undertakes%20the%20task%20of%20replicating%20the%20MaskFormer%20model%20a%0Auniversal%20image%20segmentation%20model%20originally%20developed%20using%20the%20PyTorch%0Aframework%2C%20within%20the%20TensorFlow%20ecosystem%2C%20specifically%20optimized%20for%0Aexecution%20on%20Tensor%20Processing%20Units%20%28TPUs%29.%20Our%20implementation%20exploits%20the%0Amodular%20constructs%20available%20within%20the%20TensorFlow%20Model%20Garden%20%28TFMG%29%2C%0Aencompassing%20elements%20such%20as%20the%20data%20loader%2C%20training%20orchestrator%2C%20and%0Avarious%20architectural%20components%2C%20tailored%20and%20adapted%20to%20meet%20the%0Aspecifications%20of%20the%20MaskFormer%20model.%20We%20address%20key%20challenges%20encountered%0Aduring%20the%20replication%2C%20non-convergence%20issues%2C%20slow%20training%2C%20adaptation%20of%0Aloss%20functions%2C%20and%20the%20integration%20of%20TPU-specific%20functionalities.%20We%20verify%0Aour%20reproduced%20implementation%20and%20present%20qualitative%20results%20on%20the%20COCO%0Adataset.%20Although%20our%20implementation%20meets%20some%20of%20the%20objectives%20for%0Aend-to-end%20reproducibility%2C%20we%20encountered%20challenges%20in%20replicating%20the%0APyTorch%20version%20of%20MaskFormer%20in%20TensorFlow.%20This%20replication%20process%20is%20not%0Astraightforward%20and%20requires%20substantial%20engineering%20efforts.%20Specifically%2C%20it%0Anecessitates%20the%20customization%20of%20various%20components%20within%20the%20TFMG%2C%20alongside%0Athorough%20verification%20and%20hyper-parameter%20tuning.%20The%20replication%20is%20available%0Aat%3A%0Ahttps%3A//github.com/PurdueDualityLab/tf-maskformer/tree/main/official/projects/maskformer%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18801v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Partial%20Replication%20of%20MaskFormer%20in%20TensorFlow%20on%20TPUs%20for%20the%0A%20%20TensorFlow%20Model%20Garden&entry.906535625=Vishal%20Purohit%20and%20Wenxin%20Jiang%20and%20Akshath%20R.%20Ravikiran%20and%20James%20C.%20Davis&entry.1292438233=%20%20This%20paper%20undertakes%20the%20task%20of%20replicating%20the%20MaskFormer%20model%20a%0Auniversal%20image%20segmentation%20model%20originally%20developed%20using%20the%20PyTorch%0Aframework%2C%20within%20the%20TensorFlow%20ecosystem%2C%20specifically%20optimized%20for%0Aexecution%20on%20Tensor%20Processing%20Units%20%28TPUs%29.%20Our%20implementation%20exploits%20the%0Amodular%20constructs%20available%20within%20the%20TensorFlow%20Model%20Garden%20%28TFMG%29%2C%0Aencompassing%20elements%20such%20as%20the%20data%20loader%2C%20training%20orchestrator%2C%20and%0Avarious%20architectural%20components%2C%20tailored%20and%20adapted%20to%20meet%20the%0Aspecifications%20of%20the%20MaskFormer%20model.%20We%20address%20key%20challenges%20encountered%0Aduring%20the%20replication%2C%20non-convergence%20issues%2C%20slow%20training%2C%20adaptation%20of%0Aloss%20functions%2C%20and%20the%20integration%20of%20TPU-specific%20functionalities.%20We%20verify%0Aour%20reproduced%20implementation%20and%20present%20qualitative%20results%20on%20the%20COCO%0Adataset.%20Although%20our%20implementation%20meets%20some%20of%20the%20objectives%20for%0Aend-to-end%20reproducibility%2C%20we%20encountered%20challenges%20in%20replicating%20the%0APyTorch%20version%20of%20MaskFormer%20in%20TensorFlow.%20This%20replication%20process%20is%20not%0Astraightforward%20and%20requires%20substantial%20engineering%20efforts.%20Specifically%2C%20it%0Anecessitates%20the%20customization%20of%20various%20components%20within%20the%20TFMG%2C%20alongside%0Athorough%20verification%20and%20hyper-parameter%20tuning.%20The%20replication%20is%20available%0Aat%3A%0Ahttps%3A//github.com/PurdueDualityLab/tf-maskformer/tree/main/official/projects/maskformer%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18801v1&entry.124074799=Read"},
{"title": "Sheet Music Transformer: End-To-End Optical Music Recognition Beyond\n  Monophonic Transcription", "author": "Antonio R\u00edos-Vila and Jorge Calvo-Zaragoza and Thierry Paquet", "abstract": "  State-of-the-art end-to-end Optical Music Recognition (OMR) has, to date,\nprimarily been carried out using monophonic transcription techniques to handle\ncomplex score layouts, such as polyphony, often by resorting to simplifications\nor specific adaptations. Despite their efficacy, these approaches imply\nchallenges related to scalability and limitations. This paper presents the\nSheet Music Transformer, the first end-to-end OMR model designed to transcribe\ncomplex musical scores without relying solely on monophonic strategies. Our\nmodel employs a Transformer-based image-to-sequence framework that predicts\nscore transcriptions in a standard digital music encoding format from input\nimages. Our model has been tested on two polyphonic music datasets and has\nproven capable of handling these intricate music structures effectively. The\nexperimental outcomes not only indicate the competence of the model, but also\nshow that it is better than the state-of-the-art methods, thus contributing to\nadvancements in end-to-end OMR transcription.\n", "link": "http://arxiv.org/abs/2402.07596v2", "date": "2024-04-29", "relevancy": 1.9111, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5296}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.474}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4608}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Sheet%20Music%20Transformer%3A%20End-To-End%20Optical%20Music%20Recognition%20Beyond%0A%20%20Monophonic%20Transcription&body=Title%3A%20Sheet%20Music%20Transformer%3A%20End-To-End%20Optical%20Music%20Recognition%20Beyond%0A%20%20Monophonic%20Transcription%0AAuthor%3A%20Antonio%20R%C3%ADos-Vila%20and%20Jorge%20Calvo-Zaragoza%20and%20Thierry%20Paquet%0AAbstract%3A%20%20%20State-of-the-art%20end-to-end%20Optical%20Music%20Recognition%20%28OMR%29%20has%2C%20to%20date%2C%0Aprimarily%20been%20carried%20out%20using%20monophonic%20transcription%20techniques%20to%20handle%0Acomplex%20score%20layouts%2C%20such%20as%20polyphony%2C%20often%20by%20resorting%20to%20simplifications%0Aor%20specific%20adaptations.%20Despite%20their%20efficacy%2C%20these%20approaches%20imply%0Achallenges%20related%20to%20scalability%20and%20limitations.%20This%20paper%20presents%20the%0ASheet%20Music%20Transformer%2C%20the%20first%20end-to-end%20OMR%20model%20designed%20to%20transcribe%0Acomplex%20musical%20scores%20without%20relying%20solely%20on%20monophonic%20strategies.%20Our%0Amodel%20employs%20a%20Transformer-based%20image-to-sequence%20framework%20that%20predicts%0Ascore%20transcriptions%20in%20a%20standard%20digital%20music%20encoding%20format%20from%20input%0Aimages.%20Our%20model%20has%20been%20tested%20on%20two%20polyphonic%20music%20datasets%20and%20has%0Aproven%20capable%20of%20handling%20these%20intricate%20music%20structures%20effectively.%20The%0Aexperimental%20outcomes%20not%20only%20indicate%20the%20competence%20of%20the%20model%2C%20but%20also%0Ashow%20that%20it%20is%20better%20than%20the%20state-of-the-art%20methods%2C%20thus%20contributing%20to%0Aadvancements%20in%20end-to-end%20OMR%20transcription.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.07596v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sheet%20Music%20Transformer%3A%20End-To-End%20Optical%20Music%20Recognition%20Beyond%0A%20%20Monophonic%20Transcription&entry.906535625=Antonio%20R%C3%ADos-Vila%20and%20Jorge%20Calvo-Zaragoza%20and%20Thierry%20Paquet&entry.1292438233=%20%20State-of-the-art%20end-to-end%20Optical%20Music%20Recognition%20%28OMR%29%20has%2C%20to%20date%2C%0Aprimarily%20been%20carried%20out%20using%20monophonic%20transcription%20techniques%20to%20handle%0Acomplex%20score%20layouts%2C%20such%20as%20polyphony%2C%20often%20by%20resorting%20to%20simplifications%0Aor%20specific%20adaptations.%20Despite%20their%20efficacy%2C%20these%20approaches%20imply%0Achallenges%20related%20to%20scalability%20and%20limitations.%20This%20paper%20presents%20the%0ASheet%20Music%20Transformer%2C%20the%20first%20end-to-end%20OMR%20model%20designed%20to%20transcribe%0Acomplex%20musical%20scores%20without%20relying%20solely%20on%20monophonic%20strategies.%20Our%0Amodel%20employs%20a%20Transformer-based%20image-to-sequence%20framework%20that%20predicts%0Ascore%20transcriptions%20in%20a%20standard%20digital%20music%20encoding%20format%20from%20input%0Aimages.%20Our%20model%20has%20been%20tested%20on%20two%20polyphonic%20music%20datasets%20and%20has%0Aproven%20capable%20of%20handling%20these%20intricate%20music%20structures%20effectively.%20The%0Aexperimental%20outcomes%20not%20only%20indicate%20the%20competence%20of%20the%20model%2C%20but%20also%0Ashow%20that%20it%20is%20better%20than%20the%20state-of-the-art%20methods%2C%20thus%20contributing%20to%0Aadvancements%20in%20end-to-end%20OMR%20transcription.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.07596v2&entry.124074799=Read"},
{"title": "A Universal Metric of Dataset Similarity for Cross-silo Federated\n  Learning", "author": "Ahmed Elhussein and Gamze Gursoy", "abstract": "  Federated Learning is increasingly used in domains such as healthcare to\nfacilitate collaborative model training without data-sharing. However, datasets\nlocated in different sites are often non-identically distributed, leading to\ndegradation of model performance in FL. Most existing methods for assessing\nthese distribution shifts are limited by being dataset or task-specific.\nMoreover, these metrics can only be calculated by exchanging data, a practice\nrestricted in many FL scenarios. To address these challenges, we propose a\nnovel metric for assessing dataset similarity. Our metric exhibits several\ndesirable properties for FL: it is dataset-agnostic, is calculated in a\nprivacy-preserving manner, and is computationally efficient, requiring no model\ntraining. In this paper, we first establish a theoretical connection between\nour metric and training dynamics in FL. Next, we extensively evaluate our\nmetric on a range of datasets including synthetic, benchmark, and medical\nimaging datasets. We demonstrate that our metric shows a robust and\ninterpretable relationship with model performance and can be calculated in\nprivacy-preserving manner. As the first federated dataset similarity metric, we\nbelieve this metric can better facilitate successful collaborations between\nsites.\n", "link": "http://arxiv.org/abs/2404.18773v1", "date": "2024-04-29", "relevancy": 1.9093, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.487}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4784}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4724}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Universal%20Metric%20of%20Dataset%20Similarity%20for%20Cross-silo%20Federated%0A%20%20Learning&body=Title%3A%20A%20Universal%20Metric%20of%20Dataset%20Similarity%20for%20Cross-silo%20Federated%0A%20%20Learning%0AAuthor%3A%20Ahmed%20Elhussein%20and%20Gamze%20Gursoy%0AAbstract%3A%20%20%20Federated%20Learning%20is%20increasingly%20used%20in%20domains%20such%20as%20healthcare%20to%0Afacilitate%20collaborative%20model%20training%20without%20data-sharing.%20However%2C%20datasets%0Alocated%20in%20different%20sites%20are%20often%20non-identically%20distributed%2C%20leading%20to%0Adegradation%20of%20model%20performance%20in%20FL.%20Most%20existing%20methods%20for%20assessing%0Athese%20distribution%20shifts%20are%20limited%20by%20being%20dataset%20or%20task-specific.%0AMoreover%2C%20these%20metrics%20can%20only%20be%20calculated%20by%20exchanging%20data%2C%20a%20practice%0Arestricted%20in%20many%20FL%20scenarios.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0Anovel%20metric%20for%20assessing%20dataset%20similarity.%20Our%20metric%20exhibits%20several%0Adesirable%20properties%20for%20FL%3A%20it%20is%20dataset-agnostic%2C%20is%20calculated%20in%20a%0Aprivacy-preserving%20manner%2C%20and%20is%20computationally%20efficient%2C%20requiring%20no%20model%0Atraining.%20In%20this%20paper%2C%20we%20first%20establish%20a%20theoretical%20connection%20between%0Aour%20metric%20and%20training%20dynamics%20in%20FL.%20Next%2C%20we%20extensively%20evaluate%20our%0Ametric%20on%20a%20range%20of%20datasets%20including%20synthetic%2C%20benchmark%2C%20and%20medical%0Aimaging%20datasets.%20We%20demonstrate%20that%20our%20metric%20shows%20a%20robust%20and%0Ainterpretable%20relationship%20with%20model%20performance%20and%20can%20be%20calculated%20in%0Aprivacy-preserving%20manner.%20As%20the%20first%20federated%20dataset%20similarity%20metric%2C%20we%0Abelieve%20this%20metric%20can%20better%20facilitate%20successful%20collaborations%20between%0Asites.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18773v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Universal%20Metric%20of%20Dataset%20Similarity%20for%20Cross-silo%20Federated%0A%20%20Learning&entry.906535625=Ahmed%20Elhussein%20and%20Gamze%20Gursoy&entry.1292438233=%20%20Federated%20Learning%20is%20increasingly%20used%20in%20domains%20such%20as%20healthcare%20to%0Afacilitate%20collaborative%20model%20training%20without%20data-sharing.%20However%2C%20datasets%0Alocated%20in%20different%20sites%20are%20often%20non-identically%20distributed%2C%20leading%20to%0Adegradation%20of%20model%20performance%20in%20FL.%20Most%20existing%20methods%20for%20assessing%0Athese%20distribution%20shifts%20are%20limited%20by%20being%20dataset%20or%20task-specific.%0AMoreover%2C%20these%20metrics%20can%20only%20be%20calculated%20by%20exchanging%20data%2C%20a%20practice%0Arestricted%20in%20many%20FL%20scenarios.%20To%20address%20these%20challenges%2C%20we%20propose%20a%0Anovel%20metric%20for%20assessing%20dataset%20similarity.%20Our%20metric%20exhibits%20several%0Adesirable%20properties%20for%20FL%3A%20it%20is%20dataset-agnostic%2C%20is%20calculated%20in%20a%0Aprivacy-preserving%20manner%2C%20and%20is%20computationally%20efficient%2C%20requiring%20no%20model%0Atraining.%20In%20this%20paper%2C%20we%20first%20establish%20a%20theoretical%20connection%20between%0Aour%20metric%20and%20training%20dynamics%20in%20FL.%20Next%2C%20we%20extensively%20evaluate%20our%0Ametric%20on%20a%20range%20of%20datasets%20including%20synthetic%2C%20benchmark%2C%20and%20medical%0Aimaging%20datasets.%20We%20demonstrate%20that%20our%20metric%20shows%20a%20robust%20and%0Ainterpretable%20relationship%20with%20model%20performance%20and%20can%20be%20calculated%20in%0Aprivacy-preserving%20manner.%20As%20the%20first%20federated%20dataset%20similarity%20metric%2C%20we%0Abelieve%20this%20metric%20can%20better%20facilitate%20successful%20collaborations%20between%0Asites.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18773v1&entry.124074799=Read"},
{"title": "Graph Convolutional Networks and Graph Attention Networks for\n  Approximating Arguments Acceptability -- Technical Report", "author": "Paul Cibier and Jean-Guy Mailly", "abstract": "  Various approaches have been proposed for providing efficient computational\napproaches for abstract argumentation. Among them, neural networks have\npermitted to solve various decision problems, notably related to arguments\n(credulous or skeptical) acceptability. In this work, we push further this\nstudy in various ways. First, relying on the state-of-the-art approach AFGCN,\nwe show how we can improve the performances of the Graph Convolutional Networks\n(GCNs) regarding both runtime and accuracy. Then, we show that it is possible\nto improve even more the efficiency of the approach by modifying the\narchitecture of the network, using Graph Attention Networks (GATs) instead.\n", "link": "http://arxiv.org/abs/2404.18672v1", "date": "2024-04-29", "relevancy": 1.897, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4894}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4713}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4602}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Graph%20Convolutional%20Networks%20and%20Graph%20Attention%20Networks%20for%0A%20%20Approximating%20Arguments%20Acceptability%20--%20Technical%20Report&body=Title%3A%20Graph%20Convolutional%20Networks%20and%20Graph%20Attention%20Networks%20for%0A%20%20Approximating%20Arguments%20Acceptability%20--%20Technical%20Report%0AAuthor%3A%20Paul%20Cibier%20and%20Jean-Guy%20Mailly%0AAbstract%3A%20%20%20Various%20approaches%20have%20been%20proposed%20for%20providing%20efficient%20computational%0Aapproaches%20for%20abstract%20argumentation.%20Among%20them%2C%20neural%20networks%20have%0Apermitted%20to%20solve%20various%20decision%20problems%2C%20notably%20related%20to%20arguments%0A%28credulous%20or%20skeptical%29%20acceptability.%20In%20this%20work%2C%20we%20push%20further%20this%0Astudy%20in%20various%20ways.%20First%2C%20relying%20on%20the%20state-of-the-art%20approach%20AFGCN%2C%0Awe%20show%20how%20we%20can%20improve%20the%20performances%20of%20the%20Graph%20Convolutional%20Networks%0A%28GCNs%29%20regarding%20both%20runtime%20and%20accuracy.%20Then%2C%20we%20show%20that%20it%20is%20possible%0Ato%20improve%20even%20more%20the%20efficiency%20of%20the%20approach%20by%20modifying%20the%0Aarchitecture%20of%20the%20network%2C%20using%20Graph%20Attention%20Networks%20%28GATs%29%20instead.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18672v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Convolutional%20Networks%20and%20Graph%20Attention%20Networks%20for%0A%20%20Approximating%20Arguments%20Acceptability%20--%20Technical%20Report&entry.906535625=Paul%20Cibier%20and%20Jean-Guy%20Mailly&entry.1292438233=%20%20Various%20approaches%20have%20been%20proposed%20for%20providing%20efficient%20computational%0Aapproaches%20for%20abstract%20argumentation.%20Among%20them%2C%20neural%20networks%20have%0Apermitted%20to%20solve%20various%20decision%20problems%2C%20notably%20related%20to%20arguments%0A%28credulous%20or%20skeptical%29%20acceptability.%20In%20this%20work%2C%20we%20push%20further%20this%0Astudy%20in%20various%20ways.%20First%2C%20relying%20on%20the%20state-of-the-art%20approach%20AFGCN%2C%0Awe%20show%20how%20we%20can%20improve%20the%20performances%20of%20the%20Graph%20Convolutional%20Networks%0A%28GCNs%29%20regarding%20both%20runtime%20and%20accuracy.%20Then%2C%20we%20show%20that%20it%20is%20possible%0Ato%20improve%20even%20more%20the%20efficiency%20of%20the%20approach%20by%20modifying%20the%0Aarchitecture%20of%20the%20network%2C%20using%20Graph%20Attention%20Networks%20%28GATs%29%20instead.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18672v1&entry.124074799=Read"},
{"title": "Talking Nonsense: Probing Large Language Models' Understanding of\n  Adversarial Gibberish Inputs", "author": "Valeriia Cherepanova and James Zou", "abstract": "  Large language models (LLMs) exhibit excellent ability to understand human\nlanguages, but do they also understand their own language that appears\ngibberish to us? In this work we delve into this question, aiming to uncover\nthe mechanisms underlying such behavior in LLMs. We employ the Greedy\nCoordinate Gradient optimizer to craft prompts that compel LLMs to generate\ncoherent responses from seemingly nonsensical inputs. We call these inputs LM\nBabel and this work systematically studies the behavior of LLMs manipulated by\nthese prompts. We find that the manipulation efficiency depends on the target\ntext's length and perplexity, with the Babel prompts often located in lower\nloss minima compared to natural prompts. We further examine the structure of\nthe Babel prompts and evaluate their robustness. Notably, we find that guiding\nthe model to generate harmful texts is not more difficult than into generating\nbenign texts, suggesting lack of alignment for out-of-distribution prompts.\n", "link": "http://arxiv.org/abs/2404.17120v2", "date": "2024-04-29", "relevancy": 1.8915, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4952}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.458}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4565}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Talking%20Nonsense%3A%20Probing%20Large%20Language%20Models%27%20Understanding%20of%0A%20%20Adversarial%20Gibberish%20Inputs&body=Title%3A%20Talking%20Nonsense%3A%20Probing%20Large%20Language%20Models%27%20Understanding%20of%0A%20%20Adversarial%20Gibberish%20Inputs%0AAuthor%3A%20Valeriia%20Cherepanova%20and%20James%20Zou%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20excellent%20ability%20to%20understand%20human%0Alanguages%2C%20but%20do%20they%20also%20understand%20their%20own%20language%20that%20appears%0Agibberish%20to%20us%3F%20In%20this%20work%20we%20delve%20into%20this%20question%2C%20aiming%20to%20uncover%0Athe%20mechanisms%20underlying%20such%20behavior%20in%20LLMs.%20We%20employ%20the%20Greedy%0ACoordinate%20Gradient%20optimizer%20to%20craft%20prompts%20that%20compel%20LLMs%20to%20generate%0Acoherent%20responses%20from%20seemingly%20nonsensical%20inputs.%20We%20call%20these%20inputs%20LM%0ABabel%20and%20this%20work%20systematically%20studies%20the%20behavior%20of%20LLMs%20manipulated%20by%0Athese%20prompts.%20We%20find%20that%20the%20manipulation%20efficiency%20depends%20on%20the%20target%0Atext%27s%20length%20and%20perplexity%2C%20with%20the%20Babel%20prompts%20often%20located%20in%20lower%0Aloss%20minima%20compared%20to%20natural%20prompts.%20We%20further%20examine%20the%20structure%20of%0Athe%20Babel%20prompts%20and%20evaluate%20their%20robustness.%20Notably%2C%20we%20find%20that%20guiding%0Athe%20model%20to%20generate%20harmful%20texts%20is%20not%20more%20difficult%20than%20into%20generating%0Abenign%20texts%2C%20suggesting%20lack%20of%20alignment%20for%20out-of-distribution%20prompts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17120v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Talking%20Nonsense%3A%20Probing%20Large%20Language%20Models%27%20Understanding%20of%0A%20%20Adversarial%20Gibberish%20Inputs&entry.906535625=Valeriia%20Cherepanova%20and%20James%20Zou&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20exhibit%20excellent%20ability%20to%20understand%20human%0Alanguages%2C%20but%20do%20they%20also%20understand%20their%20own%20language%20that%20appears%0Agibberish%20to%20us%3F%20In%20this%20work%20we%20delve%20into%20this%20question%2C%20aiming%20to%20uncover%0Athe%20mechanisms%20underlying%20such%20behavior%20in%20LLMs.%20We%20employ%20the%20Greedy%0ACoordinate%20Gradient%20optimizer%20to%20craft%20prompts%20that%20compel%20LLMs%20to%20generate%0Acoherent%20responses%20from%20seemingly%20nonsensical%20inputs.%20We%20call%20these%20inputs%20LM%0ABabel%20and%20this%20work%20systematically%20studies%20the%20behavior%20of%20LLMs%20manipulated%20by%0Athese%20prompts.%20We%20find%20that%20the%20manipulation%20efficiency%20depends%20on%20the%20target%0Atext%27s%20length%20and%20perplexity%2C%20with%20the%20Babel%20prompts%20often%20located%20in%20lower%0Aloss%20minima%20compared%20to%20natural%20prompts.%20We%20further%20examine%20the%20structure%20of%0Athe%20Babel%20prompts%20and%20evaluate%20their%20robustness.%20Notably%2C%20we%20find%20that%20guiding%0Athe%20model%20to%20generate%20harmful%20texts%20is%20not%20more%20difficult%20than%20into%20generating%0Abenign%20texts%2C%20suggesting%20lack%20of%20alignment%20for%20out-of-distribution%20prompts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17120v2&entry.124074799=Read"},
{"title": "Learning with Norm Constrained, Over-parameterized, Two-layer Neural\n  Networks", "author": "Fanghui Liu and Leello Dadi and Volkan Cevher", "abstract": "  Recent studies show that a reproducing kernel Hilbert space (RKHS) is not a\nsuitable space to model functions by neural networks as the curse of\ndimensionality (CoD) cannot be evaded when trying to approximate even a single\nReLU neuron (Bach, 2017). In this paper, we study a suitable function space for\nover-parameterized two-layer neural networks with bounded norms (e.g., the path\nnorm, the Barron norm) in the perspective of sample complexity and\ngeneralization properties. First, we show that the path norm (as well as the\nBarron norm) is able to obtain width-independence sample complexity bounds,\nwhich allows for uniform convergence guarantees. Based on this result, we\nderive the improved result of metric entropy for $\\epsilon$-covering up to\n$\\mathcal{O}(\\epsilon^{-\\frac{2d}{d+2}})$ ($d$ is the input dimension and the\ndepending constant is at most polynomial order of $d$) via the convex hull\ntechnique, which demonstrates the separation with kernel methods with\n$\\Omega(\\epsilon^{-d})$ to learn the target function in a Barron space. Second,\nthis metric entropy result allows for building a sharper generalization bound\nunder a general moment hypothesis setting, achieving the rate at\n$\\mathcal{O}(n^{-\\frac{d+2}{2d+2}})$. Our analysis is novel in that it offers a\nsharper and refined estimation for metric entropy (with a clear dependence\nrelationship on the dimension $d$) and unbounded sampling in the estimation of\nthe sample error and the output error.\n", "link": "http://arxiv.org/abs/2404.18769v1", "date": "2024-04-29", "relevancy": 1.8895, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4875}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4723}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4573}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20with%20Norm%20Constrained%2C%20Over-parameterized%2C%20Two-layer%20Neural%0A%20%20Networks&body=Title%3A%20Learning%20with%20Norm%20Constrained%2C%20Over-parameterized%2C%20Two-layer%20Neural%0A%20%20Networks%0AAuthor%3A%20Fanghui%20Liu%20and%20Leello%20Dadi%20and%20Volkan%20Cevher%0AAbstract%3A%20%20%20Recent%20studies%20show%20that%20a%20reproducing%20kernel%20Hilbert%20space%20%28RKHS%29%20is%20not%20a%0Asuitable%20space%20to%20model%20functions%20by%20neural%20networks%20as%20the%20curse%20of%0Adimensionality%20%28CoD%29%20cannot%20be%20evaded%20when%20trying%20to%20approximate%20even%20a%20single%0AReLU%20neuron%20%28Bach%2C%202017%29.%20In%20this%20paper%2C%20we%20study%20a%20suitable%20function%20space%20for%0Aover-parameterized%20two-layer%20neural%20networks%20with%20bounded%20norms%20%28e.g.%2C%20the%20path%0Anorm%2C%20the%20Barron%20norm%29%20in%20the%20perspective%20of%20sample%20complexity%20and%0Ageneralization%20properties.%20First%2C%20we%20show%20that%20the%20path%20norm%20%28as%20well%20as%20the%0ABarron%20norm%29%20is%20able%20to%20obtain%20width-independence%20sample%20complexity%20bounds%2C%0Awhich%20allows%20for%20uniform%20convergence%20guarantees.%20Based%20on%20this%20result%2C%20we%0Aderive%20the%20improved%20result%20of%20metric%20entropy%20for%20%24%5Cepsilon%24-covering%20up%20to%0A%24%5Cmathcal%7BO%7D%28%5Cepsilon%5E%7B-%5Cfrac%7B2d%7D%7Bd%2B2%7D%7D%29%24%20%28%24d%24%20is%20the%20input%20dimension%20and%20the%0Adepending%20constant%20is%20at%20most%20polynomial%20order%20of%20%24d%24%29%20via%20the%20convex%20hull%0Atechnique%2C%20which%20demonstrates%20the%20separation%20with%20kernel%20methods%20with%0A%24%5COmega%28%5Cepsilon%5E%7B-d%7D%29%24%20to%20learn%20the%20target%20function%20in%20a%20Barron%20space.%20Second%2C%0Athis%20metric%20entropy%20result%20allows%20for%20building%20a%20sharper%20generalization%20bound%0Aunder%20a%20general%20moment%20hypothesis%20setting%2C%20achieving%20the%20rate%20at%0A%24%5Cmathcal%7BO%7D%28n%5E%7B-%5Cfrac%7Bd%2B2%7D%7B2d%2B2%7D%7D%29%24.%20Our%20analysis%20is%20novel%20in%20that%20it%20offers%20a%0Asharper%20and%20refined%20estimation%20for%20metric%20entropy%20%28with%20a%20clear%20dependence%0Arelationship%20on%20the%20dimension%20%24d%24%29%20and%20unbounded%20sampling%20in%20the%20estimation%20of%0Athe%20sample%20error%20and%20the%20output%20error.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18769v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20with%20Norm%20Constrained%2C%20Over-parameterized%2C%20Two-layer%20Neural%0A%20%20Networks&entry.906535625=Fanghui%20Liu%20and%20Leello%20Dadi%20and%20Volkan%20Cevher&entry.1292438233=%20%20Recent%20studies%20show%20that%20a%20reproducing%20kernel%20Hilbert%20space%20%28RKHS%29%20is%20not%20a%0Asuitable%20space%20to%20model%20functions%20by%20neural%20networks%20as%20the%20curse%20of%0Adimensionality%20%28CoD%29%20cannot%20be%20evaded%20when%20trying%20to%20approximate%20even%20a%20single%0AReLU%20neuron%20%28Bach%2C%202017%29.%20In%20this%20paper%2C%20we%20study%20a%20suitable%20function%20space%20for%0Aover-parameterized%20two-layer%20neural%20networks%20with%20bounded%20norms%20%28e.g.%2C%20the%20path%0Anorm%2C%20the%20Barron%20norm%29%20in%20the%20perspective%20of%20sample%20complexity%20and%0Ageneralization%20properties.%20First%2C%20we%20show%20that%20the%20path%20norm%20%28as%20well%20as%20the%0ABarron%20norm%29%20is%20able%20to%20obtain%20width-independence%20sample%20complexity%20bounds%2C%0Awhich%20allows%20for%20uniform%20convergence%20guarantees.%20Based%20on%20this%20result%2C%20we%0Aderive%20the%20improved%20result%20of%20metric%20entropy%20for%20%24%5Cepsilon%24-covering%20up%20to%0A%24%5Cmathcal%7BO%7D%28%5Cepsilon%5E%7B-%5Cfrac%7B2d%7D%7Bd%2B2%7D%7D%29%24%20%28%24d%24%20is%20the%20input%20dimension%20and%20the%0Adepending%20constant%20is%20at%20most%20polynomial%20order%20of%20%24d%24%29%20via%20the%20convex%20hull%0Atechnique%2C%20which%20demonstrates%20the%20separation%20with%20kernel%20methods%20with%0A%24%5COmega%28%5Cepsilon%5E%7B-d%7D%29%24%20to%20learn%20the%20target%20function%20in%20a%20Barron%20space.%20Second%2C%0Athis%20metric%20entropy%20result%20allows%20for%20building%20a%20sharper%20generalization%20bound%0Aunder%20a%20general%20moment%20hypothesis%20setting%2C%20achieving%20the%20rate%20at%0A%24%5Cmathcal%7BO%7D%28n%5E%7B-%5Cfrac%7Bd%2B2%7D%7B2d%2B2%7D%7D%29%24.%20Our%20analysis%20is%20novel%20in%20that%20it%20offers%20a%0Asharper%20and%20refined%20estimation%20for%20metric%20entropy%20%28with%20a%20clear%20dependence%0Arelationship%20on%20the%20dimension%20%24d%24%29%20and%20unbounded%20sampling%20in%20the%20estimation%20of%0Athe%20sample%20error%20and%20the%20output%20error.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18769v1&entry.124074799=Read"},
{"title": "Bayesian Reasoning for Physics Informed Neural Networks", "author": "Krzysztof M. Graczyk and Kornel Witkowski", "abstract": "  We present the application of the physics-informed neural network (PINN)\napproach in Bayesian formulation. We have adopted the Bayesian neural network\nframework to obtain posterior densities from Laplace approximation. For each\nmodel or fit, the evidence is computed, which is a measure that classifies the\nhypothesis. The optimal solution is the one with the highest value of evidence.\nWe have proposed a modification of the Bayesian algorithm to obtain\nhyperparameters of the model. We have shown that within the Bayesian framework,\none can obtain the relative weights between the boundary and equation\ncontributions to the total loss. Presented method leads to predictions\ncomparable to those obtained by sampling from the posterior distribution within\nthe Hybrid Monte Carlo algorithm (HMC). We have solved heat, wave, and Burger's\nequations, and the results obtained are in agreement with the exact solutions,\ndemonstrating the effectiveness of our approach. In Burger's equation problem,\nwe have demonstrated that the framework can combine information from\ndifferential equations and potential measurements. All solutions are provided\nwith uncertainties (induced by the model's parameter dependence) computed\nwithin the Bayesian framework.\n", "link": "http://arxiv.org/abs/2308.13222v2", "date": "2024-04-29", "relevancy": 1.8846, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5721}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4564}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4455}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Reasoning%20for%20Physics%20Informed%20Neural%20Networks&body=Title%3A%20Bayesian%20Reasoning%20for%20Physics%20Informed%20Neural%20Networks%0AAuthor%3A%20Krzysztof%20M.%20Graczyk%20and%20Kornel%20Witkowski%0AAbstract%3A%20%20%20We%20present%20the%20application%20of%20the%20physics-informed%20neural%20network%20%28PINN%29%0Aapproach%20in%20Bayesian%20formulation.%20We%20have%20adopted%20the%20Bayesian%20neural%20network%0Aframework%20to%20obtain%20posterior%20densities%20from%20Laplace%20approximation.%20For%20each%0Amodel%20or%20fit%2C%20the%20evidence%20is%20computed%2C%20which%20is%20a%20measure%20that%20classifies%20the%0Ahypothesis.%20The%20optimal%20solution%20is%20the%20one%20with%20the%20highest%20value%20of%20evidence.%0AWe%20have%20proposed%20a%20modification%20of%20the%20Bayesian%20algorithm%20to%20obtain%0Ahyperparameters%20of%20the%20model.%20We%20have%20shown%20that%20within%20the%20Bayesian%20framework%2C%0Aone%20can%20obtain%20the%20relative%20weights%20between%20the%20boundary%20and%20equation%0Acontributions%20to%20the%20total%20loss.%20Presented%20method%20leads%20to%20predictions%0Acomparable%20to%20those%20obtained%20by%20sampling%20from%20the%20posterior%20distribution%20within%0Athe%20Hybrid%20Monte%20Carlo%20algorithm%20%28HMC%29.%20We%20have%20solved%20heat%2C%20wave%2C%20and%20Burger%27s%0Aequations%2C%20and%20the%20results%20obtained%20are%20in%20agreement%20with%20the%20exact%20solutions%2C%0Ademonstrating%20the%20effectiveness%20of%20our%20approach.%20In%20Burger%27s%20equation%20problem%2C%0Awe%20have%20demonstrated%20that%20the%20framework%20can%20combine%20information%20from%0Adifferential%20equations%20and%20potential%20measurements.%20All%20solutions%20are%20provided%0Awith%20uncertainties%20%28induced%20by%20the%20model%27s%20parameter%20dependence%29%20computed%0Awithin%20the%20Bayesian%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.13222v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Reasoning%20for%20Physics%20Informed%20Neural%20Networks&entry.906535625=Krzysztof%20M.%20Graczyk%20and%20Kornel%20Witkowski&entry.1292438233=%20%20We%20present%20the%20application%20of%20the%20physics-informed%20neural%20network%20%28PINN%29%0Aapproach%20in%20Bayesian%20formulation.%20We%20have%20adopted%20the%20Bayesian%20neural%20network%0Aframework%20to%20obtain%20posterior%20densities%20from%20Laplace%20approximation.%20For%20each%0Amodel%20or%20fit%2C%20the%20evidence%20is%20computed%2C%20which%20is%20a%20measure%20that%20classifies%20the%0Ahypothesis.%20The%20optimal%20solution%20is%20the%20one%20with%20the%20highest%20value%20of%20evidence.%0AWe%20have%20proposed%20a%20modification%20of%20the%20Bayesian%20algorithm%20to%20obtain%0Ahyperparameters%20of%20the%20model.%20We%20have%20shown%20that%20within%20the%20Bayesian%20framework%2C%0Aone%20can%20obtain%20the%20relative%20weights%20between%20the%20boundary%20and%20equation%0Acontributions%20to%20the%20total%20loss.%20Presented%20method%20leads%20to%20predictions%0Acomparable%20to%20those%20obtained%20by%20sampling%20from%20the%20posterior%20distribution%20within%0Athe%20Hybrid%20Monte%20Carlo%20algorithm%20%28HMC%29.%20We%20have%20solved%20heat%2C%20wave%2C%20and%20Burger%27s%0Aequations%2C%20and%20the%20results%20obtained%20are%20in%20agreement%20with%20the%20exact%20solutions%2C%0Ademonstrating%20the%20effectiveness%20of%20our%20approach.%20In%20Burger%27s%20equation%20problem%2C%0Awe%20have%20demonstrated%20that%20the%20framework%20can%20combine%20information%20from%0Adifferential%20equations%20and%20potential%20measurements.%20All%20solutions%20are%20provided%0Awith%20uncertainties%20%28induced%20by%20the%20model%27s%20parameter%20dependence%29%20computed%0Awithin%20the%20Bayesian%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.13222v2&entry.124074799=Read"},
{"title": "DPO Meets PPO: Reinforced Token Optimization for RLHF", "author": "Han Zhong and Guhao Feng and Wei Xiong and Li Zhao and Di He and Jiang Bian and Liwei Wang", "abstract": "  In the classical Reinforcement Learning from Human Feedback (RLHF) framework,\nProximal Policy Optimization (PPO) is employed to learn from sparse,\nsentence-level rewards -- a challenging scenario in traditional deep\nreinforcement learning. Despite the great successes of PPO in the alignment of\nstate-of-the-art closed-source large language models (LLMs), its open-source\nimplementation is still largely sub-optimal, as widely reported by numerous\nresearch studies. To address these issues, we introduce a framework that models\nRLHF problems as a Markov decision process (MDP), enabling the capture of\nfine-grained token-wise information. Furthermore, we provide theoretical\ninsights that demonstrate the superiority of our MDP framework over the\nprevious sentence-level bandit formulation. Under this framework, we introduce\nan algorithm, dubbed as Reinforced Token Optimization (\\texttt{RTO}), which\nlearns the token-wise reward function from preference data and performs policy\noptimization based on this learned token-wise reward signal. Theoretically,\n\\texttt{RTO} is proven to have the capability of finding the near-optimal\npolicy sample-efficiently. For its practical implementation, \\texttt{RTO}\ninnovatively integrates Direct Preference Optimization (DPO) and PPO. DPO,\noriginally derived from sparse sentence rewards, surprisingly provides us with\na token-wise characterization of response quality, which is seamlessly\nincorporated into our subsequent PPO training stage. Extensive real-world\nalignment experiments verify the effectiveness of the proposed approach.\n", "link": "http://arxiv.org/abs/2404.18922v1", "date": "2024-04-29", "relevancy": 1.8837, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4946}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4622}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4508}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DPO%20Meets%20PPO%3A%20Reinforced%20Token%20Optimization%20for%20RLHF&body=Title%3A%20DPO%20Meets%20PPO%3A%20Reinforced%20Token%20Optimization%20for%20RLHF%0AAuthor%3A%20Han%20Zhong%20and%20Guhao%20Feng%20and%20Wei%20Xiong%20and%20Li%20Zhao%20and%20Di%20He%20and%20Jiang%20Bian%20and%20Liwei%20Wang%0AAbstract%3A%20%20%20In%20the%20classical%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20framework%2C%0AProximal%20Policy%20Optimization%20%28PPO%29%20is%20employed%20to%20learn%20from%20sparse%2C%0Asentence-level%20rewards%20--%20a%20challenging%20scenario%20in%20traditional%20deep%0Areinforcement%20learning.%20Despite%20the%20great%20successes%20of%20PPO%20in%20the%20alignment%20of%0Astate-of-the-art%20closed-source%20large%20language%20models%20%28LLMs%29%2C%20its%20open-source%0Aimplementation%20is%20still%20largely%20sub-optimal%2C%20as%20widely%20reported%20by%20numerous%0Aresearch%20studies.%20To%20address%20these%20issues%2C%20we%20introduce%20a%20framework%20that%20models%0ARLHF%20problems%20as%20a%20Markov%20decision%20process%20%28MDP%29%2C%20enabling%20the%20capture%20of%0Afine-grained%20token-wise%20information.%20Furthermore%2C%20we%20provide%20theoretical%0Ainsights%20that%20demonstrate%20the%20superiority%20of%20our%20MDP%20framework%20over%20the%0Aprevious%20sentence-level%20bandit%20formulation.%20Under%20this%20framework%2C%20we%20introduce%0Aan%20algorithm%2C%20dubbed%20as%20Reinforced%20Token%20Optimization%20%28%5Ctexttt%7BRTO%7D%29%2C%20which%0Alearns%20the%20token-wise%20reward%20function%20from%20preference%20data%20and%20performs%20policy%0Aoptimization%20based%20on%20this%20learned%20token-wise%20reward%20signal.%20Theoretically%2C%0A%5Ctexttt%7BRTO%7D%20is%20proven%20to%20have%20the%20capability%20of%20finding%20the%20near-optimal%0Apolicy%20sample-efficiently.%20For%20its%20practical%20implementation%2C%20%5Ctexttt%7BRTO%7D%0Ainnovatively%20integrates%20Direct%20Preference%20Optimization%20%28DPO%29%20and%20PPO.%20DPO%2C%0Aoriginally%20derived%20from%20sparse%20sentence%20rewards%2C%20surprisingly%20provides%20us%20with%0Aa%20token-wise%20characterization%20of%20response%20quality%2C%20which%20is%20seamlessly%0Aincorporated%20into%20our%20subsequent%20PPO%20training%20stage.%20Extensive%20real-world%0Aalignment%20experiments%20verify%20the%20effectiveness%20of%20the%20proposed%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18922v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DPO%20Meets%20PPO%3A%20Reinforced%20Token%20Optimization%20for%20RLHF&entry.906535625=Han%20Zhong%20and%20Guhao%20Feng%20and%20Wei%20Xiong%20and%20Li%20Zhao%20and%20Di%20He%20and%20Jiang%20Bian%20and%20Liwei%20Wang&entry.1292438233=%20%20In%20the%20classical%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20framework%2C%0AProximal%20Policy%20Optimization%20%28PPO%29%20is%20employed%20to%20learn%20from%20sparse%2C%0Asentence-level%20rewards%20--%20a%20challenging%20scenario%20in%20traditional%20deep%0Areinforcement%20learning.%20Despite%20the%20great%20successes%20of%20PPO%20in%20the%20alignment%20of%0Astate-of-the-art%20closed-source%20large%20language%20models%20%28LLMs%29%2C%20its%20open-source%0Aimplementation%20is%20still%20largely%20sub-optimal%2C%20as%20widely%20reported%20by%20numerous%0Aresearch%20studies.%20To%20address%20these%20issues%2C%20we%20introduce%20a%20framework%20that%20models%0ARLHF%20problems%20as%20a%20Markov%20decision%20process%20%28MDP%29%2C%20enabling%20the%20capture%20of%0Afine-grained%20token-wise%20information.%20Furthermore%2C%20we%20provide%20theoretical%0Ainsights%20that%20demonstrate%20the%20superiority%20of%20our%20MDP%20framework%20over%20the%0Aprevious%20sentence-level%20bandit%20formulation.%20Under%20this%20framework%2C%20we%20introduce%0Aan%20algorithm%2C%20dubbed%20as%20Reinforced%20Token%20Optimization%20%28%5Ctexttt%7BRTO%7D%29%2C%20which%0Alearns%20the%20token-wise%20reward%20function%20from%20preference%20data%20and%20performs%20policy%0Aoptimization%20based%20on%20this%20learned%20token-wise%20reward%20signal.%20Theoretically%2C%0A%5Ctexttt%7BRTO%7D%20is%20proven%20to%20have%20the%20capability%20of%20finding%20the%20near-optimal%0Apolicy%20sample-efficiently.%20For%20its%20practical%20implementation%2C%20%5Ctexttt%7BRTO%7D%0Ainnovatively%20integrates%20Direct%20Preference%20Optimization%20%28DPO%29%20and%20PPO.%20DPO%2C%0Aoriginally%20derived%20from%20sparse%20sentence%20rewards%2C%20surprisingly%20provides%20us%20with%0Aa%20token-wise%20characterization%20of%20response%20quality%2C%20which%20is%20seamlessly%0Aincorporated%20into%20our%20subsequent%20PPO%20training%20stage.%20Extensive%20real-world%0Aalignment%20experiments%20verify%20the%20effectiveness%20of%20the%20proposed%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18922v1&entry.124074799=Read"},
{"title": "Self-training superconducting neuromorphic circuits using reinforcement\n  learning rules", "author": "M. L. Schneider and E. M. Ju\u00e9 and M. R. Pufall and K. Segall and C. W. Anderson", "abstract": "  Reinforcement learning algorithms are used in a wide range of applications,\nfrom gaming and robotics to autonomous vehicles. In this paper we describe a\nset of reinforcement learning-based local weight update rules and their\nimplementation in superconducting hardware. Using SPICE circuit simulations, we\nimplement a small-scale neural network with a learning time of order one\nnanosecond. This network can be trained to learn new functions simply by\nchanging the target output for a given set of inputs, without the need for any\nexternal adjustments to the network. In this implementation the weights are\nadjusted based on the current state of the overall network response and locally\nstored information about the previous action. This removes the need to program\nexplicit weight values in these networks, which is one of the primary\nchallenges that analog hardware implementations of neural networks face. The\nadjustment of weights is based on a global reinforcement signal that obviates\nthe need for circuitry to back-propagate errors.\n", "link": "http://arxiv.org/abs/2404.18774v1", "date": "2024-04-29", "relevancy": 1.863, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5039}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.439}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4383}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Self-training%20superconducting%20neuromorphic%20circuits%20using%20reinforcement%0A%20%20learning%20rules&body=Title%3A%20Self-training%20superconducting%20neuromorphic%20circuits%20using%20reinforcement%0A%20%20learning%20rules%0AAuthor%3A%20M.%20L.%20Schneider%20and%20E.%20M.%20Ju%C3%A9%20and%20M.%20R.%20Pufall%20and%20K.%20Segall%20and%20C.%20W.%20Anderson%0AAbstract%3A%20%20%20Reinforcement%20learning%20algorithms%20are%20used%20in%20a%20wide%20range%20of%20applications%2C%0Afrom%20gaming%20and%20robotics%20to%20autonomous%20vehicles.%20In%20this%20paper%20we%20describe%20a%0Aset%20of%20reinforcement%20learning-based%20local%20weight%20update%20rules%20and%20their%0Aimplementation%20in%20superconducting%20hardware.%20Using%20SPICE%20circuit%20simulations%2C%20we%0Aimplement%20a%20small-scale%20neural%20network%20with%20a%20learning%20time%20of%20order%20one%0Ananosecond.%20This%20network%20can%20be%20trained%20to%20learn%20new%20functions%20simply%20by%0Achanging%20the%20target%20output%20for%20a%20given%20set%20of%20inputs%2C%20without%20the%20need%20for%20any%0Aexternal%20adjustments%20to%20the%20network.%20In%20this%20implementation%20the%20weights%20are%0Aadjusted%20based%20on%20the%20current%20state%20of%20the%20overall%20network%20response%20and%20locally%0Astored%20information%20about%20the%20previous%20action.%20This%20removes%20the%20need%20to%20program%0Aexplicit%20weight%20values%20in%20these%20networks%2C%20which%20is%20one%20of%20the%20primary%0Achallenges%20that%20analog%20hardware%20implementations%20of%20neural%20networks%20face.%20The%0Aadjustment%20of%20weights%20is%20based%20on%20a%20global%20reinforcement%20signal%20that%20obviates%0Athe%20need%20for%20circuitry%20to%20back-propagate%20errors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18774v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-training%20superconducting%20neuromorphic%20circuits%20using%20reinforcement%0A%20%20learning%20rules&entry.906535625=M.%20L.%20Schneider%20and%20E.%20M.%20Ju%C3%A9%20and%20M.%20R.%20Pufall%20and%20K.%20Segall%20and%20C.%20W.%20Anderson&entry.1292438233=%20%20Reinforcement%20learning%20algorithms%20are%20used%20in%20a%20wide%20range%20of%20applications%2C%0Afrom%20gaming%20and%20robotics%20to%20autonomous%20vehicles.%20In%20this%20paper%20we%20describe%20a%0Aset%20of%20reinforcement%20learning-based%20local%20weight%20update%20rules%20and%20their%0Aimplementation%20in%20superconducting%20hardware.%20Using%20SPICE%20circuit%20simulations%2C%20we%0Aimplement%20a%20small-scale%20neural%20network%20with%20a%20learning%20time%20of%20order%20one%0Ananosecond.%20This%20network%20can%20be%20trained%20to%20learn%20new%20functions%20simply%20by%0Achanging%20the%20target%20output%20for%20a%20given%20set%20of%20inputs%2C%20without%20the%20need%20for%20any%0Aexternal%20adjustments%20to%20the%20network.%20In%20this%20implementation%20the%20weights%20are%0Aadjusted%20based%20on%20the%20current%20state%20of%20the%20overall%20network%20response%20and%20locally%0Astored%20information%20about%20the%20previous%20action.%20This%20removes%20the%20need%20to%20program%0Aexplicit%20weight%20values%20in%20these%20networks%2C%20which%20is%20one%20of%20the%20primary%0Achallenges%20that%20analog%20hardware%20implementations%20of%20neural%20networks%20face.%20The%0Aadjustment%20of%20weights%20is%20based%20on%20a%20global%20reinforcement%20signal%20that%20obviates%0Athe%20need%20for%20circuitry%20to%20back-propagate%20errors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18774v1&entry.124074799=Read"},
{"title": "The Landscape of Unfolding with Machine Learning", "author": "Nathan Huetsch and Javier Mari\u00f1o Villadamigo and Alexander Shmakov and Sascha Diefenbacher and Vinicius Mikuni and Theo Heimel and Michael Fenton and Kevin Greif and Benjamin Nachman and Daniel Whiteson and Anja Butter and Tilman Plehn", "abstract": "  Recent innovations from machine learning allow for data unfolding, without\nbinning and including correlations across many dimensions. We describe a set of\nknown, upgraded, and new methods for ML-based unfolding. The performance of\nthese approaches are evaluated on the same two datasets. We find that all\ntechniques are capable of accurately reproducing the particle-level spectra\nacross complex observables. Given that these approaches are conceptually\ndiverse, they offer an exciting toolkit for a new class of measurements that\ncan probe the Standard Model with an unprecedented level of detail and may\nenable sensitivity to new phenomena.\n", "link": "http://arxiv.org/abs/2404.18807v1", "date": "2024-04-29", "relevancy": 1.8555, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4657}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4636}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4635}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20Landscape%20of%20Unfolding%20with%20Machine%20Learning&body=Title%3A%20The%20Landscape%20of%20Unfolding%20with%20Machine%20Learning%0AAuthor%3A%20Nathan%20Huetsch%20and%20Javier%20Mari%C3%B1o%20Villadamigo%20and%20Alexander%20Shmakov%20and%20Sascha%20Diefenbacher%20and%20Vinicius%20Mikuni%20and%20Theo%20Heimel%20and%20Michael%20Fenton%20and%20Kevin%20Greif%20and%20Benjamin%20Nachman%20and%20Daniel%20Whiteson%20and%20Anja%20Butter%20and%20Tilman%20Plehn%0AAbstract%3A%20%20%20Recent%20innovations%20from%20machine%20learning%20allow%20for%20data%20unfolding%2C%20without%0Abinning%20and%20including%20correlations%20across%20many%20dimensions.%20We%20describe%20a%20set%20of%0Aknown%2C%20upgraded%2C%20and%20new%20methods%20for%20ML-based%20unfolding.%20The%20performance%20of%0Athese%20approaches%20are%20evaluated%20on%20the%20same%20two%20datasets.%20We%20find%20that%20all%0Atechniques%20are%20capable%20of%20accurately%20reproducing%20the%20particle-level%20spectra%0Aacross%20complex%20observables.%20Given%20that%20these%20approaches%20are%20conceptually%0Adiverse%2C%20they%20offer%20an%20exciting%20toolkit%20for%20a%20new%20class%20of%20measurements%20that%0Acan%20probe%20the%20Standard%20Model%20with%20an%20unprecedented%20level%20of%20detail%20and%20may%0Aenable%20sensitivity%20to%20new%20phenomena.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18807v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Landscape%20of%20Unfolding%20with%20Machine%20Learning&entry.906535625=Nathan%20Huetsch%20and%20Javier%20Mari%C3%B1o%20Villadamigo%20and%20Alexander%20Shmakov%20and%20Sascha%20Diefenbacher%20and%20Vinicius%20Mikuni%20and%20Theo%20Heimel%20and%20Michael%20Fenton%20and%20Kevin%20Greif%20and%20Benjamin%20Nachman%20and%20Daniel%20Whiteson%20and%20Anja%20Butter%20and%20Tilman%20Plehn&entry.1292438233=%20%20Recent%20innovations%20from%20machine%20learning%20allow%20for%20data%20unfolding%2C%20without%0Abinning%20and%20including%20correlations%20across%20many%20dimensions.%20We%20describe%20a%20set%20of%0Aknown%2C%20upgraded%2C%20and%20new%20methods%20for%20ML-based%20unfolding.%20The%20performance%20of%0Athese%20approaches%20are%20evaluated%20on%20the%20same%20two%20datasets.%20We%20find%20that%20all%0Atechniques%20are%20capable%20of%20accurately%20reproducing%20the%20particle-level%20spectra%0Aacross%20complex%20observables.%20Given%20that%20these%20approaches%20are%20conceptually%0Adiverse%2C%20they%20offer%20an%20exciting%20toolkit%20for%20a%20new%20class%20of%20measurements%20that%0Acan%20probe%20the%20Standard%20Model%20with%20an%20unprecedented%20level%20of%20detail%20and%20may%0Aenable%20sensitivity%20to%20new%20phenomena.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18807v1&entry.124074799=Read"},
{"title": "Advances and Open Challenges in Federated Learning with Foundation\n  Models", "author": "Chao Ren and Han Yu and Hongyi Peng and Xiaoli Tang and Anran Li and Yulan Gao and Alysa Ziying Tan and Bo Zhao and Xiaoxiao Li and Zengxiang Li and Qiang Yang", "abstract": "  The integration of Foundation Models (FMs) with Federated Learning (FL)\npresents a transformative paradigm in Artificial Intelligence (AI), offering\nenhanced capabilities while addressing concerns of privacy, data\ndecentralization, and computational efficiency. This paper provides a\ncomprehensive survey of the emerging field of Federated Foundation Models\n(FedFM), elucidating their synergistic relationship and exploring novel\nmethodologies, challenges, and future directions that the FL research field\nneeds to focus on in order to thrive in the age of foundation models. A\nsystematic multi-tiered taxonomy is proposed, categorizing existing FedFM\napproaches for model training, aggregation, trustworthiness, and\nincentivization. Key challenges, including how to enable FL to deal with high\ncomplexity of computational demands, privacy considerations, contribution\nevaluation, and communication efficiency, are thoroughly discussed. Moreover,\nthe paper explores the intricate challenges of communication, scalability and\nsecurity inherent in training/fine-tuning FMs via FL, highlighting the\npotential of quantum computing to revolutionize the training, inference,\noptimization and data encryption processes. This survey underscores the\nimportance of further research to propel innovation in FedFM, emphasizing the\nneed for developing trustworthy solutions. It serves as a foundational guide\nfor researchers and practitioners interested in contributing to this\ninterdisciplinary and rapidly advancing field.\n", "link": "http://arxiv.org/abs/2404.15381v2", "date": "2024-04-29", "relevancy": 1.8414, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4659}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4565}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4564}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Advances%20and%20Open%20Challenges%20in%20Federated%20Learning%20with%20Foundation%0A%20%20Models&body=Title%3A%20Advances%20and%20Open%20Challenges%20in%20Federated%20Learning%20with%20Foundation%0A%20%20Models%0AAuthor%3A%20Chao%20Ren%20and%20Han%20Yu%20and%20Hongyi%20Peng%20and%20Xiaoli%20Tang%20and%20Anran%20Li%20and%20Yulan%20Gao%20and%20Alysa%20Ziying%20Tan%20and%20Bo%20Zhao%20and%20Xiaoxiao%20Li%20and%20Zengxiang%20Li%20and%20Qiang%20Yang%0AAbstract%3A%20%20%20The%20integration%20of%20Foundation%20Models%20%28FMs%29%20with%20Federated%20Learning%20%28FL%29%0Apresents%20a%20transformative%20paradigm%20in%20Artificial%20Intelligence%20%28AI%29%2C%20offering%0Aenhanced%20capabilities%20while%20addressing%20concerns%20of%20privacy%2C%20data%0Adecentralization%2C%20and%20computational%20efficiency.%20This%20paper%20provides%20a%0Acomprehensive%20survey%20of%20the%20emerging%20field%20of%20Federated%20Foundation%20Models%0A%28FedFM%29%2C%20elucidating%20their%20synergistic%20relationship%20and%20exploring%20novel%0Amethodologies%2C%20challenges%2C%20and%20future%20directions%20that%20the%20FL%20research%20field%0Aneeds%20to%20focus%20on%20in%20order%20to%20thrive%20in%20the%20age%20of%20foundation%20models.%20A%0Asystematic%20multi-tiered%20taxonomy%20is%20proposed%2C%20categorizing%20existing%20FedFM%0Aapproaches%20for%20model%20training%2C%20aggregation%2C%20trustworthiness%2C%20and%0Aincentivization.%20Key%20challenges%2C%20including%20how%20to%20enable%20FL%20to%20deal%20with%20high%0Acomplexity%20of%20computational%20demands%2C%20privacy%20considerations%2C%20contribution%0Aevaluation%2C%20and%20communication%20efficiency%2C%20are%20thoroughly%20discussed.%20Moreover%2C%0Athe%20paper%20explores%20the%20intricate%20challenges%20of%20communication%2C%20scalability%20and%0Asecurity%20inherent%20in%20training/fine-tuning%20FMs%20via%20FL%2C%20highlighting%20the%0Apotential%20of%20quantum%20computing%20to%20revolutionize%20the%20training%2C%20inference%2C%0Aoptimization%20and%20data%20encryption%20processes.%20This%20survey%20underscores%20the%0Aimportance%20of%20further%20research%20to%20propel%20innovation%20in%20FedFM%2C%20emphasizing%20the%0Aneed%20for%20developing%20trustworthy%20solutions.%20It%20serves%20as%20a%20foundational%20guide%0Afor%20researchers%20and%20practitioners%20interested%20in%20contributing%20to%20this%0Ainterdisciplinary%20and%20rapidly%20advancing%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15381v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advances%20and%20Open%20Challenges%20in%20Federated%20Learning%20with%20Foundation%0A%20%20Models&entry.906535625=Chao%20Ren%20and%20Han%20Yu%20and%20Hongyi%20Peng%20and%20Xiaoli%20Tang%20and%20Anran%20Li%20and%20Yulan%20Gao%20and%20Alysa%20Ziying%20Tan%20and%20Bo%20Zhao%20and%20Xiaoxiao%20Li%20and%20Zengxiang%20Li%20and%20Qiang%20Yang&entry.1292438233=%20%20The%20integration%20of%20Foundation%20Models%20%28FMs%29%20with%20Federated%20Learning%20%28FL%29%0Apresents%20a%20transformative%20paradigm%20in%20Artificial%20Intelligence%20%28AI%29%2C%20offering%0Aenhanced%20capabilities%20while%20addressing%20concerns%20of%20privacy%2C%20data%0Adecentralization%2C%20and%20computational%20efficiency.%20This%20paper%20provides%20a%0Acomprehensive%20survey%20of%20the%20emerging%20field%20of%20Federated%20Foundation%20Models%0A%28FedFM%29%2C%20elucidating%20their%20synergistic%20relationship%20and%20exploring%20novel%0Amethodologies%2C%20challenges%2C%20and%20future%20directions%20that%20the%20FL%20research%20field%0Aneeds%20to%20focus%20on%20in%20order%20to%20thrive%20in%20the%20age%20of%20foundation%20models.%20A%0Asystematic%20multi-tiered%20taxonomy%20is%20proposed%2C%20categorizing%20existing%20FedFM%0Aapproaches%20for%20model%20training%2C%20aggregation%2C%20trustworthiness%2C%20and%0Aincentivization.%20Key%20challenges%2C%20including%20how%20to%20enable%20FL%20to%20deal%20with%20high%0Acomplexity%20of%20computational%20demands%2C%20privacy%20considerations%2C%20contribution%0Aevaluation%2C%20and%20communication%20efficiency%2C%20are%20thoroughly%20discussed.%20Moreover%2C%0Athe%20paper%20explores%20the%20intricate%20challenges%20of%20communication%2C%20scalability%20and%0Asecurity%20inherent%20in%20training/fine-tuning%20FMs%20via%20FL%2C%20highlighting%20the%0Apotential%20of%20quantum%20computing%20to%20revolutionize%20the%20training%2C%20inference%2C%0Aoptimization%20and%20data%20encryption%20processes.%20This%20survey%20underscores%20the%0Aimportance%20of%20further%20research%20to%20propel%20innovation%20in%20FedFM%2C%20emphasizing%20the%0Aneed%20for%20developing%20trustworthy%20solutions.%20It%20serves%20as%20a%20foundational%20guide%0Afor%20researchers%20and%20practitioners%20interested%20in%20contributing%20to%20this%0Ainterdisciplinary%20and%20rapidly%20advancing%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15381v2&entry.124074799=Read"},
{"title": "Bootstrap 3D Reconstructed Scenes from 3D Gaussian Splatting", "author": "Yifei Gao and Jie Ou and Lei Wang and Jun Cheng", "abstract": "  Recent developments in neural rendering techniques have greatly enhanced the\nrendering of photo-realistic 3D scenes across both academic and commercial\nfields. The latest method, known as 3D Gaussian Splatting (3D-GS), has set new\nbenchmarks for rendering quality and speed. Nevertheless, the limitations of\n3D-GS become pronounced in synthesizing new viewpoints, especially for views\nthat greatly deviate from those seen during training. Additionally, issues such\nas dilation and aliasing arise when zooming in or out. These challenges can all\nbe traced back to a single underlying issue: insufficient sampling. In our\npaper, we present a bootstrapping method that significantly addresses this\nproblem. This approach employs a diffusion model to enhance the rendering of\nnovel views using trained 3D-GS, thereby streamlining the training process. Our\nresults indicate that bootstrapping effectively reduces artifacts, as well as\nclear enhancements on the evaluation metrics. Furthermore, we show that our\nmethod is versatile and can be easily integrated, allowing various 3D\nreconstruction projects to benefit from our approach.\n", "link": "http://arxiv.org/abs/2404.18669v1", "date": "2024-04-29", "relevancy": 1.8052, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.65}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5485}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5342}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Bootstrap%203D%20Reconstructed%20Scenes%20from%203D%20Gaussian%20Splatting&body=Title%3A%20Bootstrap%203D%20Reconstructed%20Scenes%20from%203D%20Gaussian%20Splatting%0AAuthor%3A%20Yifei%20Gao%20and%20Jie%20Ou%20and%20Lei%20Wang%20and%20Jun%20Cheng%0AAbstract%3A%20%20%20Recent%20developments%20in%20neural%20rendering%20techniques%20have%20greatly%20enhanced%20the%0Arendering%20of%20photo-realistic%203D%20scenes%20across%20both%20academic%20and%20commercial%0Afields.%20The%20latest%20method%2C%20known%20as%203D%20Gaussian%20Splatting%20%283D-GS%29%2C%20has%20set%20new%0Abenchmarks%20for%20rendering%20quality%20and%20speed.%20Nevertheless%2C%20the%20limitations%20of%0A3D-GS%20become%20pronounced%20in%20synthesizing%20new%20viewpoints%2C%20especially%20for%20views%0Athat%20greatly%20deviate%20from%20those%20seen%20during%20training.%20Additionally%2C%20issues%20such%0Aas%20dilation%20and%20aliasing%20arise%20when%20zooming%20in%20or%20out.%20These%20challenges%20can%20all%0Abe%20traced%20back%20to%20a%20single%20underlying%20issue%3A%20insufficient%20sampling.%20In%20our%0Apaper%2C%20we%20present%20a%20bootstrapping%20method%20that%20significantly%20addresses%20this%0Aproblem.%20This%20approach%20employs%20a%20diffusion%20model%20to%20enhance%20the%20rendering%20of%0Anovel%20views%20using%20trained%203D-GS%2C%20thereby%20streamlining%20the%20training%20process.%20Our%0Aresults%20indicate%20that%20bootstrapping%20effectively%20reduces%20artifacts%2C%20as%20well%20as%0Aclear%20enhancements%20on%20the%20evaluation%20metrics.%20Furthermore%2C%20we%20show%20that%20our%0Amethod%20is%20versatile%20and%20can%20be%20easily%20integrated%2C%20allowing%20various%203D%0Areconstruction%20projects%20to%20benefit%20from%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18669v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bootstrap%203D%20Reconstructed%20Scenes%20from%203D%20Gaussian%20Splatting&entry.906535625=Yifei%20Gao%20and%20Jie%20Ou%20and%20Lei%20Wang%20and%20Jun%20Cheng&entry.1292438233=%20%20Recent%20developments%20in%20neural%20rendering%20techniques%20have%20greatly%20enhanced%20the%0Arendering%20of%20photo-realistic%203D%20scenes%20across%20both%20academic%20and%20commercial%0Afields.%20The%20latest%20method%2C%20known%20as%203D%20Gaussian%20Splatting%20%283D-GS%29%2C%20has%20set%20new%0Abenchmarks%20for%20rendering%20quality%20and%20speed.%20Nevertheless%2C%20the%20limitations%20of%0A3D-GS%20become%20pronounced%20in%20synthesizing%20new%20viewpoints%2C%20especially%20for%20views%0Athat%20greatly%20deviate%20from%20those%20seen%20during%20training.%20Additionally%2C%20issues%20such%0Aas%20dilation%20and%20aliasing%20arise%20when%20zooming%20in%20or%20out.%20These%20challenges%20can%20all%0Abe%20traced%20back%20to%20a%20single%20underlying%20issue%3A%20insufficient%20sampling.%20In%20our%0Apaper%2C%20we%20present%20a%20bootstrapping%20method%20that%20significantly%20addresses%20this%0Aproblem.%20This%20approach%20employs%20a%20diffusion%20model%20to%20enhance%20the%20rendering%20of%0Anovel%20views%20using%20trained%203D-GS%2C%20thereby%20streamlining%20the%20training%20process.%20Our%0Aresults%20indicate%20that%20bootstrapping%20effectively%20reduces%20artifacts%2C%20as%20well%20as%0Aclear%20enhancements%20on%20the%20evaluation%20metrics.%20Furthermore%2C%20we%20show%20that%20our%0Amethod%20is%20versatile%20and%20can%20be%20easily%20integrated%2C%20allowing%20various%203D%0Areconstruction%20projects%20to%20benefit%20from%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18669v1&entry.124074799=Read"},
{"title": "Proof-of-Learning with Incentive Security", "author": "Zishuo Zhao and Zhixuan Fang and Xuechao Wang and Xi Chen and Yuan Zhou", "abstract": "  Most concurrent blockchain systems rely heavily on the Proof-of-Work (PoW) or\nProof-of-Stake (PoS) mechanisms for decentralized consensus and security\nassurance. However, the substantial energy expenditure stemming from\ncomputationally intensive yet meaningless tasks has raised considerable\nconcerns surrounding traditional PoW approaches, The PoS mechanism, while free\nof energy consumption, is subject to security and economic issues. Addressing\nthese issues, the paradigm of Proof-of-Useful-Work (PoUW) seeks to employ\nchallenges of practical significance as PoW, thereby imbuing energy consumption\nwith tangible value. While previous efforts in Proof of Learning (PoL) explored\nthe utilization of deep learning model training SGD tasks as PoUW challenges,\nrecent research has revealed its vulnerabilities to adversarial attacks and the\ntheoretical hardness in crafting a byzantine-secure PoL mechanism. In this\npaper, we introduce the concept of incentive-security that incentivizes\nrational provers to behave honestly for their best interest, bypassing the\nexisting hardness to design a PoL mechanism with computational efficiency, a\nprovable incentive-security guarantee and controllable difficulty.\nParticularly, our work is secure against two attacks to the recent work of Jia\net al. [2021], and also improves the computational overhead from $\\Theta(1)$ to\n$O(\\frac{\\log E}{E})$. Furthermore, while most recent research assumes trusted\nproblem providers and verifiers, our design also guarantees frontend\nincentive-security even when problem providers are untrusted, and verifier\nincentive-security that bypasses the Verifier's Dilemma. By incorporating ML\ntraining into blockchain consensus mechanisms with provable guarantees, our\nresearch not only proposes an eco-friendly solution to blockchain systems, but\nalso provides a proposal for a completely decentralized computing power market\nin the new AI age.\n", "link": "http://arxiv.org/abs/2404.09005v2", "date": "2024-04-29", "relevancy": 1.7965, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4654}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.45}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4417}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Proof-of-Learning%20with%20Incentive%20Security&body=Title%3A%20Proof-of-Learning%20with%20Incentive%20Security%0AAuthor%3A%20Zishuo%20Zhao%20and%20Zhixuan%20Fang%20and%20Xuechao%20Wang%20and%20Xi%20Chen%20and%20Yuan%20Zhou%0AAbstract%3A%20%20%20Most%20concurrent%20blockchain%20systems%20rely%20heavily%20on%20the%20Proof-of-Work%20%28PoW%29%20or%0AProof-of-Stake%20%28PoS%29%20mechanisms%20for%20decentralized%20consensus%20and%20security%0Aassurance.%20However%2C%20the%20substantial%20energy%20expenditure%20stemming%20from%0Acomputationally%20intensive%20yet%20meaningless%20tasks%20has%20raised%20considerable%0Aconcerns%20surrounding%20traditional%20PoW%20approaches%2C%20The%20PoS%20mechanism%2C%20while%20free%0Aof%20energy%20consumption%2C%20is%20subject%20to%20security%20and%20economic%20issues.%20Addressing%0Athese%20issues%2C%20the%20paradigm%20of%20Proof-of-Useful-Work%20%28PoUW%29%20seeks%20to%20employ%0Achallenges%20of%20practical%20significance%20as%20PoW%2C%20thereby%20imbuing%20energy%20consumption%0Awith%20tangible%20value.%20While%20previous%20efforts%20in%20Proof%20of%20Learning%20%28PoL%29%20explored%0Athe%20utilization%20of%20deep%20learning%20model%20training%20SGD%20tasks%20as%20PoUW%20challenges%2C%0Arecent%20research%20has%20revealed%20its%20vulnerabilities%20to%20adversarial%20attacks%20and%20the%0Atheoretical%20hardness%20in%20crafting%20a%20byzantine-secure%20PoL%20mechanism.%20In%20this%0Apaper%2C%20we%20introduce%20the%20concept%20of%20incentive-security%20that%20incentivizes%0Arational%20provers%20to%20behave%20honestly%20for%20their%20best%20interest%2C%20bypassing%20the%0Aexisting%20hardness%20to%20design%20a%20PoL%20mechanism%20with%20computational%20efficiency%2C%20a%0Aprovable%20incentive-security%20guarantee%20and%20controllable%20difficulty.%0AParticularly%2C%20our%20work%20is%20secure%20against%20two%20attacks%20to%20the%20recent%20work%20of%20Jia%0Aet%20al.%20%5B2021%5D%2C%20and%20also%20improves%20the%20computational%20overhead%20from%20%24%5CTheta%281%29%24%20to%0A%24O%28%5Cfrac%7B%5Clog%20E%7D%7BE%7D%29%24.%20Furthermore%2C%20while%20most%20recent%20research%20assumes%20trusted%0Aproblem%20providers%20and%20verifiers%2C%20our%20design%20also%20guarantees%20frontend%0Aincentive-security%20even%20when%20problem%20providers%20are%20untrusted%2C%20and%20verifier%0Aincentive-security%20that%20bypasses%20the%20Verifier%27s%20Dilemma.%20By%20incorporating%20ML%0Atraining%20into%20blockchain%20consensus%20mechanisms%20with%20provable%20guarantees%2C%20our%0Aresearch%20not%20only%20proposes%20an%20eco-friendly%20solution%20to%20blockchain%20systems%2C%20but%0Aalso%20provides%20a%20proposal%20for%20a%20completely%20decentralized%20computing%20power%20market%0Ain%20the%20new%20AI%20age.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.09005v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Proof-of-Learning%20with%20Incentive%20Security&entry.906535625=Zishuo%20Zhao%20and%20Zhixuan%20Fang%20and%20Xuechao%20Wang%20and%20Xi%20Chen%20and%20Yuan%20Zhou&entry.1292438233=%20%20Most%20concurrent%20blockchain%20systems%20rely%20heavily%20on%20the%20Proof-of-Work%20%28PoW%29%20or%0AProof-of-Stake%20%28PoS%29%20mechanisms%20for%20decentralized%20consensus%20and%20security%0Aassurance.%20However%2C%20the%20substantial%20energy%20expenditure%20stemming%20from%0Acomputationally%20intensive%20yet%20meaningless%20tasks%20has%20raised%20considerable%0Aconcerns%20surrounding%20traditional%20PoW%20approaches%2C%20The%20PoS%20mechanism%2C%20while%20free%0Aof%20energy%20consumption%2C%20is%20subject%20to%20security%20and%20economic%20issues.%20Addressing%0Athese%20issues%2C%20the%20paradigm%20of%20Proof-of-Useful-Work%20%28PoUW%29%20seeks%20to%20employ%0Achallenges%20of%20practical%20significance%20as%20PoW%2C%20thereby%20imbuing%20energy%20consumption%0Awith%20tangible%20value.%20While%20previous%20efforts%20in%20Proof%20of%20Learning%20%28PoL%29%20explored%0Athe%20utilization%20of%20deep%20learning%20model%20training%20SGD%20tasks%20as%20PoUW%20challenges%2C%0Arecent%20research%20has%20revealed%20its%20vulnerabilities%20to%20adversarial%20attacks%20and%20the%0Atheoretical%20hardness%20in%20crafting%20a%20byzantine-secure%20PoL%20mechanism.%20In%20this%0Apaper%2C%20we%20introduce%20the%20concept%20of%20incentive-security%20that%20incentivizes%0Arational%20provers%20to%20behave%20honestly%20for%20their%20best%20interest%2C%20bypassing%20the%0Aexisting%20hardness%20to%20design%20a%20PoL%20mechanism%20with%20computational%20efficiency%2C%20a%0Aprovable%20incentive-security%20guarantee%20and%20controllable%20difficulty.%0AParticularly%2C%20our%20work%20is%20secure%20against%20two%20attacks%20to%20the%20recent%20work%20of%20Jia%0Aet%20al.%20%5B2021%5D%2C%20and%20also%20improves%20the%20computational%20overhead%20from%20%24%5CTheta%281%29%24%20to%0A%24O%28%5Cfrac%7B%5Clog%20E%7D%7BE%7D%29%24.%20Furthermore%2C%20while%20most%20recent%20research%20assumes%20trusted%0Aproblem%20providers%20and%20verifiers%2C%20our%20design%20also%20guarantees%20frontend%0Aincentive-security%20even%20when%20problem%20providers%20are%20untrusted%2C%20and%20verifier%0Aincentive-security%20that%20bypasses%20the%20Verifier%27s%20Dilemma.%20By%20incorporating%20ML%0Atraining%20into%20blockchain%20consensus%20mechanisms%20with%20provable%20guarantees%2C%20our%0Aresearch%20not%20only%20proposes%20an%20eco-friendly%20solution%20to%20blockchain%20systems%2C%20but%0Aalso%20provides%20a%20proposal%20for%20a%20completely%20decentralized%20computing%20power%20market%0Ain%20the%20new%20AI%20age.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.09005v2&entry.124074799=Read"},
{"title": "Adaptive Reinforcement Learning for Robot Control", "author": "Yu Tang Liu and Nilaksh Singh and Aamir Ahmad", "abstract": "  Deep reinforcement learning (DRL) has shown remarkable success in simulation\ndomains, yet its application in designing robot controllers remains limited,\ndue to its single-task orientation and insufficient adaptability to\nenvironmental changes. To overcome these limitations, we present a novel\nadaptive agent that leverages transfer learning techniques to dynamically adapt\npolicy in response to different tasks and environmental conditions. The\napproach is validated through the blimp control challenge, where multitasking\ncapabilities and environmental adaptability are essential. The agent is trained\nusing a custom, highly parallelized simulator built on IsaacGym. We perform\nzero-shot transfer to fly the blimp in the real world to solve various tasks.\nWe share our code at\n\\url{https://github.com/robot-perception-group/adaptive\\_agent/}.\n", "link": "http://arxiv.org/abs/2404.18713v1", "date": "2024-04-29", "relevancy": 1.7931, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.6102}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5895}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5747}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Reinforcement%20Learning%20for%20Robot%20Control&body=Title%3A%20Adaptive%20Reinforcement%20Learning%20for%20Robot%20Control%0AAuthor%3A%20Yu%20Tang%20Liu%20and%20Nilaksh%20Singh%20and%20Aamir%20Ahmad%0AAbstract%3A%20%20%20Deep%20reinforcement%20learning%20%28DRL%29%20has%20shown%20remarkable%20success%20in%20simulation%0Adomains%2C%20yet%20its%20application%20in%20designing%20robot%20controllers%20remains%20limited%2C%0Adue%20to%20its%20single-task%20orientation%20and%20insufficient%20adaptability%20to%0Aenvironmental%20changes.%20To%20overcome%20these%20limitations%2C%20we%20present%20a%20novel%0Aadaptive%20agent%20that%20leverages%20transfer%20learning%20techniques%20to%20dynamically%20adapt%0Apolicy%20in%20response%20to%20different%20tasks%20and%20environmental%20conditions.%20The%0Aapproach%20is%20validated%20through%20the%20blimp%20control%20challenge%2C%20where%20multitasking%0Acapabilities%20and%20environmental%20adaptability%20are%20essential.%20The%20agent%20is%20trained%0Ausing%20a%20custom%2C%20highly%20parallelized%20simulator%20built%20on%20IsaacGym.%20We%20perform%0Azero-shot%20transfer%20to%20fly%20the%20blimp%20in%20the%20real%20world%20to%20solve%20various%20tasks.%0AWe%20share%20our%20code%20at%0A%5Curl%7Bhttps%3A//github.com/robot-perception-group/adaptive%5C_agent/%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18713v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Reinforcement%20Learning%20for%20Robot%20Control&entry.906535625=Yu%20Tang%20Liu%20and%20Nilaksh%20Singh%20and%20Aamir%20Ahmad&entry.1292438233=%20%20Deep%20reinforcement%20learning%20%28DRL%29%20has%20shown%20remarkable%20success%20in%20simulation%0Adomains%2C%20yet%20its%20application%20in%20designing%20robot%20controllers%20remains%20limited%2C%0Adue%20to%20its%20single-task%20orientation%20and%20insufficient%20adaptability%20to%0Aenvironmental%20changes.%20To%20overcome%20these%20limitations%2C%20we%20present%20a%20novel%0Aadaptive%20agent%20that%20leverages%20transfer%20learning%20techniques%20to%20dynamically%20adapt%0Apolicy%20in%20response%20to%20different%20tasks%20and%20environmental%20conditions.%20The%0Aapproach%20is%20validated%20through%20the%20blimp%20control%20challenge%2C%20where%20multitasking%0Acapabilities%20and%20environmental%20adaptability%20are%20essential.%20The%20agent%20is%20trained%0Ausing%20a%20custom%2C%20highly%20parallelized%20simulator%20built%20on%20IsaacGym.%20We%20perform%0Azero-shot%20transfer%20to%20fly%20the%20blimp%20in%20the%20real%20world%20to%20solve%20various%20tasks.%0AWe%20share%20our%20code%20at%0A%5Curl%7Bhttps%3A//github.com/robot-perception-group/adaptive%5C_agent/%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18713v1&entry.124074799=Read"},
{"title": "An FPGA-Based Accelerator for Graph Embedding using Sequential Training\n  Algorithm", "author": "Kazuki Sunaga and Keisuke Sugiura and Hiroki Matsutani", "abstract": "  A graph embedding is an emerging approach that can represent a graph\nstructure with a fixed-length low-dimensional vector. node2vec is a well-known\nalgorithm to obtain such a graph embedding by sampling neighboring nodes on a\ngiven graph with a random walk technique. However, the original node2vec\nalgorithm typically relies on a batch training of graph structures; thus, it is\nnot suited for applications in which the graph structure changes after the\ndeployment. In this paper, we focus on node2vec applications for IoT (Internet\nof Things) environments. To handle the changes of graph structures after the\nIoT devices have been deployed in edge environments, in this paper we propose\nto combine an online sequential training algorithm with node2vec. The proposed\nsequentially-trainable model is implemented on an FPGA (Field-Programmable Gate\nArray) device to demonstrate the benefits of our approach. The proposed FPGA\nimplementation achieves up to 205.25 times speedup compared to the original\nmodel on ARM Cortex-A53 CPU. Evaluation results using dynamic graphs show that\nalthough the accuracy is decreased in the original model, the proposed\nsequential model can obtain better graph embedding that achieves a higher\naccuracy even when the graph structure is changed.\n", "link": "http://arxiv.org/abs/2312.15138v2", "date": "2024-04-29", "relevancy": 1.7904, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.461}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4501}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4397}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20An%20FPGA-Based%20Accelerator%20for%20Graph%20Embedding%20using%20Sequential%20Training%0A%20%20Algorithm&body=Title%3A%20An%20FPGA-Based%20Accelerator%20for%20Graph%20Embedding%20using%20Sequential%20Training%0A%20%20Algorithm%0AAuthor%3A%20Kazuki%20Sunaga%20and%20Keisuke%20Sugiura%20and%20Hiroki%20Matsutani%0AAbstract%3A%20%20%20A%20graph%20embedding%20is%20an%20emerging%20approach%20that%20can%20represent%20a%20graph%0Astructure%20with%20a%20fixed-length%20low-dimensional%20vector.%20node2vec%20is%20a%20well-known%0Aalgorithm%20to%20obtain%20such%20a%20graph%20embedding%20by%20sampling%20neighboring%20nodes%20on%20a%0Agiven%20graph%20with%20a%20random%20walk%20technique.%20However%2C%20the%20original%20node2vec%0Aalgorithm%20typically%20relies%20on%20a%20batch%20training%20of%20graph%20structures%3B%20thus%2C%20it%20is%0Anot%20suited%20for%20applications%20in%20which%20the%20graph%20structure%20changes%20after%20the%0Adeployment.%20In%20this%20paper%2C%20we%20focus%20on%20node2vec%20applications%20for%20IoT%20%28Internet%0Aof%20Things%29%20environments.%20To%20handle%20the%20changes%20of%20graph%20structures%20after%20the%0AIoT%20devices%20have%20been%20deployed%20in%20edge%20environments%2C%20in%20this%20paper%20we%20propose%0Ato%20combine%20an%20online%20sequential%20training%20algorithm%20with%20node2vec.%20The%20proposed%0Asequentially-trainable%20model%20is%20implemented%20on%20an%20FPGA%20%28Field-Programmable%20Gate%0AArray%29%20device%20to%20demonstrate%20the%20benefits%20of%20our%20approach.%20The%20proposed%20FPGA%0Aimplementation%20achieves%20up%20to%20205.25%20times%20speedup%20compared%20to%20the%20original%0Amodel%20on%20ARM%20Cortex-A53%20CPU.%20Evaluation%20results%20using%20dynamic%20graphs%20show%20that%0Aalthough%20the%20accuracy%20is%20decreased%20in%20the%20original%20model%2C%20the%20proposed%0Asequential%20model%20can%20obtain%20better%20graph%20embedding%20that%20achieves%20a%20higher%0Aaccuracy%20even%20when%20the%20graph%20structure%20is%20changed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.15138v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20FPGA-Based%20Accelerator%20for%20Graph%20Embedding%20using%20Sequential%20Training%0A%20%20Algorithm&entry.906535625=Kazuki%20Sunaga%20and%20Keisuke%20Sugiura%20and%20Hiroki%20Matsutani&entry.1292438233=%20%20A%20graph%20embedding%20is%20an%20emerging%20approach%20that%20can%20represent%20a%20graph%0Astructure%20with%20a%20fixed-length%20low-dimensional%20vector.%20node2vec%20is%20a%20well-known%0Aalgorithm%20to%20obtain%20such%20a%20graph%20embedding%20by%20sampling%20neighboring%20nodes%20on%20a%0Agiven%20graph%20with%20a%20random%20walk%20technique.%20However%2C%20the%20original%20node2vec%0Aalgorithm%20typically%20relies%20on%20a%20batch%20training%20of%20graph%20structures%3B%20thus%2C%20it%20is%0Anot%20suited%20for%20applications%20in%20which%20the%20graph%20structure%20changes%20after%20the%0Adeployment.%20In%20this%20paper%2C%20we%20focus%20on%20node2vec%20applications%20for%20IoT%20%28Internet%0Aof%20Things%29%20environments.%20To%20handle%20the%20changes%20of%20graph%20structures%20after%20the%0AIoT%20devices%20have%20been%20deployed%20in%20edge%20environments%2C%20in%20this%20paper%20we%20propose%0Ato%20combine%20an%20online%20sequential%20training%20algorithm%20with%20node2vec.%20The%20proposed%0Asequentially-trainable%20model%20is%20implemented%20on%20an%20FPGA%20%28Field-Programmable%20Gate%0AArray%29%20device%20to%20demonstrate%20the%20benefits%20of%20our%20approach.%20The%20proposed%20FPGA%0Aimplementation%20achieves%20up%20to%20205.25%20times%20speedup%20compared%20to%20the%20original%0Amodel%20on%20ARM%20Cortex-A53%20CPU.%20Evaluation%20results%20using%20dynamic%20graphs%20show%20that%0Aalthough%20the%20accuracy%20is%20decreased%20in%20the%20original%20model%2C%20the%20proposed%0Asequential%20model%20can%20obtain%20better%20graph%20embedding%20that%20achieves%20a%20higher%0Aaccuracy%20even%20when%20the%20graph%20structure%20is%20changed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.15138v2&entry.124074799=Read"},
{"title": "Innovative Integration of Visual Foundation Model with a Robotic Arm on\n  a Mobile Platform", "author": "Shimian Zhang and Qiuhong Lu", "abstract": "  In the rapidly advancing field of robotics, the fusion of state-of-the-art\nvisual technologies with mobile robotic arms has emerged as a critical\nintegration. This paper introduces a novel system that combines the Segment\nAnything model (SAM) -- a transformer-based visual foundation model -- with a\nrobotic arm on a mobile platform. The design of integrating a depth camera on\nthe robotic arm's end-effector ensures continuous object tracking,\nsignificantly mitigating environmental uncertainties. By deploying on a mobile\nplatform, our grasping system has an enhanced mobility, playing a key role in\ndynamic environments where adaptability are critical. This synthesis enables\ndynamic object segmentation, tracking, and grasping. It also elevates user\ninteraction, allowing the robot to intuitively respond to various modalities\nsuch as clicks, drawings, or voice commands, beyond traditional robotic\nsystems. Empirical assessments in both simulated and real-world demonstrate the\nsystem's capabilities. This configuration opens avenues for wide-ranging\napplications, from industrial settings, agriculture, and household tasks, to\nspecialized assignments and beyond.\n", "link": "http://arxiv.org/abs/2404.18720v1", "date": "2024-04-29", "relevancy": 1.7887, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5987}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5968}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5922}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Innovative%20Integration%20of%20Visual%20Foundation%20Model%20with%20a%20Robotic%20Arm%20on%0A%20%20a%20Mobile%20Platform&body=Title%3A%20Innovative%20Integration%20of%20Visual%20Foundation%20Model%20with%20a%20Robotic%20Arm%20on%0A%20%20a%20Mobile%20Platform%0AAuthor%3A%20Shimian%20Zhang%20and%20Qiuhong%20Lu%0AAbstract%3A%20%20%20In%20the%20rapidly%20advancing%20field%20of%20robotics%2C%20the%20fusion%20of%20state-of-the-art%0Avisual%20technologies%20with%20mobile%20robotic%20arms%20has%20emerged%20as%20a%20critical%0Aintegration.%20This%20paper%20introduces%20a%20novel%20system%20that%20combines%20the%20Segment%0AAnything%20model%20%28SAM%29%20--%20a%20transformer-based%20visual%20foundation%20model%20--%20with%20a%0Arobotic%20arm%20on%20a%20mobile%20platform.%20The%20design%20of%20integrating%20a%20depth%20camera%20on%0Athe%20robotic%20arm%27s%20end-effector%20ensures%20continuous%20object%20tracking%2C%0Asignificantly%20mitigating%20environmental%20uncertainties.%20By%20deploying%20on%20a%20mobile%0Aplatform%2C%20our%20grasping%20system%20has%20an%20enhanced%20mobility%2C%20playing%20a%20key%20role%20in%0Adynamic%20environments%20where%20adaptability%20are%20critical.%20This%20synthesis%20enables%0Adynamic%20object%20segmentation%2C%20tracking%2C%20and%20grasping.%20It%20also%20elevates%20user%0Ainteraction%2C%20allowing%20the%20robot%20to%20intuitively%20respond%20to%20various%20modalities%0Asuch%20as%20clicks%2C%20drawings%2C%20or%20voice%20commands%2C%20beyond%20traditional%20robotic%0Asystems.%20Empirical%20assessments%20in%20both%20simulated%20and%20real-world%20demonstrate%20the%0Asystem%27s%20capabilities.%20This%20configuration%20opens%20avenues%20for%20wide-ranging%0Aapplications%2C%20from%20industrial%20settings%2C%20agriculture%2C%20and%20household%20tasks%2C%20to%0Aspecialized%20assignments%20and%20beyond.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18720v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Innovative%20Integration%20of%20Visual%20Foundation%20Model%20with%20a%20Robotic%20Arm%20on%0A%20%20a%20Mobile%20Platform&entry.906535625=Shimian%20Zhang%20and%20Qiuhong%20Lu&entry.1292438233=%20%20In%20the%20rapidly%20advancing%20field%20of%20robotics%2C%20the%20fusion%20of%20state-of-the-art%0Avisual%20technologies%20with%20mobile%20robotic%20arms%20has%20emerged%20as%20a%20critical%0Aintegration.%20This%20paper%20introduces%20a%20novel%20system%20that%20combines%20the%20Segment%0AAnything%20model%20%28SAM%29%20--%20a%20transformer-based%20visual%20foundation%20model%20--%20with%20a%0Arobotic%20arm%20on%20a%20mobile%20platform.%20The%20design%20of%20integrating%20a%20depth%20camera%20on%0Athe%20robotic%20arm%27s%20end-effector%20ensures%20continuous%20object%20tracking%2C%0Asignificantly%20mitigating%20environmental%20uncertainties.%20By%20deploying%20on%20a%20mobile%0Aplatform%2C%20our%20grasping%20system%20has%20an%20enhanced%20mobility%2C%20playing%20a%20key%20role%20in%0Adynamic%20environments%20where%20adaptability%20are%20critical.%20This%20synthesis%20enables%0Adynamic%20object%20segmentation%2C%20tracking%2C%20and%20grasping.%20It%20also%20elevates%20user%0Ainteraction%2C%20allowing%20the%20robot%20to%20intuitively%20respond%20to%20various%20modalities%0Asuch%20as%20clicks%2C%20drawings%2C%20or%20voice%20commands%2C%20beyond%20traditional%20robotic%0Asystems.%20Empirical%20assessments%20in%20both%20simulated%20and%20real-world%20demonstrate%20the%0Asystem%27s%20capabilities.%20This%20configuration%20opens%20avenues%20for%20wide-ranging%0Aapplications%2C%20from%20industrial%20settings%2C%20agriculture%2C%20and%20household%20tasks%2C%20to%0Aspecialized%20assignments%20and%20beyond.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18720v1&entry.124074799=Read"},
{"title": "Replacing Judges with Juries: Evaluating LLM Generations with a Panel of\n  Diverse Models", "author": "Pat Verga and Sebastian Hofstatter and Sophia Althammer and Yixuan Su and Aleksandra Piktus and Arkady Arkhangorodsky and Minjie Xu and Naomi White and Patrick Lewis", "abstract": "  As Large Language Models (LLMs) have become more advanced, they have outpaced\nour abilities to accurately evaluate their quality. Not only is finding data to\nadequately probe particular model properties difficult, but evaluating the\ncorrectness of a model's freeform generation alone is a challenge. To address\nthis, many evaluations now rely on using LLMs themselves as judges to score the\nquality of outputs from other LLMs. Evaluations most commonly use a single\nlarge model like GPT4. While this method has grown in popularity, it is costly,\nhas been shown to introduce intramodel bias, and in this work, we find that\nvery large models are often unnecessary. We propose instead to evaluate models\nusing a Panel of LLm evaluators (PoLL). Across three distinct judge settings\nand spanning six different datasets, we find that using a PoLL composed of a\nlarger number of smaller models outperforms a single large judge, exhibits less\nintra-model bias due to its composition of disjoint model families, and does so\nwhile being over seven times less expensive.\n", "link": "http://arxiv.org/abs/2404.18796v1", "date": "2024-04-29", "relevancy": 1.7876, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4667}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4448}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.441}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Replacing%20Judges%20with%20Juries%3A%20Evaluating%20LLM%20Generations%20with%20a%20Panel%20of%0A%20%20Diverse%20Models&body=Title%3A%20Replacing%20Judges%20with%20Juries%3A%20Evaluating%20LLM%20Generations%20with%20a%20Panel%20of%0A%20%20Diverse%20Models%0AAuthor%3A%20Pat%20Verga%20and%20Sebastian%20Hofstatter%20and%20Sophia%20Althammer%20and%20Yixuan%20Su%20and%20Aleksandra%20Piktus%20and%20Arkady%20Arkhangorodsky%20and%20Minjie%20Xu%20and%20Naomi%20White%20and%20Patrick%20Lewis%0AAbstract%3A%20%20%20As%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20more%20advanced%2C%20they%20have%20outpaced%0Aour%20abilities%20to%20accurately%20evaluate%20their%20quality.%20Not%20only%20is%20finding%20data%20to%0Aadequately%20probe%20particular%20model%20properties%20difficult%2C%20but%20evaluating%20the%0Acorrectness%20of%20a%20model%27s%20freeform%20generation%20alone%20is%20a%20challenge.%20To%20address%0Athis%2C%20many%20evaluations%20now%20rely%20on%20using%20LLMs%20themselves%20as%20judges%20to%20score%20the%0Aquality%20of%20outputs%20from%20other%20LLMs.%20Evaluations%20most%20commonly%20use%20a%20single%0Alarge%20model%20like%20GPT4.%20While%20this%20method%20has%20grown%20in%20popularity%2C%20it%20is%20costly%2C%0Ahas%20been%20shown%20to%20introduce%20intramodel%20bias%2C%20and%20in%20this%20work%2C%20we%20find%20that%0Avery%20large%20models%20are%20often%20unnecessary.%20We%20propose%20instead%20to%20evaluate%20models%0Ausing%20a%20Panel%20of%20LLm%20evaluators%20%28PoLL%29.%20Across%20three%20distinct%20judge%20settings%0Aand%20spanning%20six%20different%20datasets%2C%20we%20find%20that%20using%20a%20PoLL%20composed%20of%20a%0Alarger%20number%20of%20smaller%20models%20outperforms%20a%20single%20large%20judge%2C%20exhibits%20less%0Aintra-model%20bias%20due%20to%20its%20composition%20of%20disjoint%20model%20families%2C%20and%20does%20so%0Awhile%20being%20over%20seven%20times%20less%20expensive.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18796v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Replacing%20Judges%20with%20Juries%3A%20Evaluating%20LLM%20Generations%20with%20a%20Panel%20of%0A%20%20Diverse%20Models&entry.906535625=Pat%20Verga%20and%20Sebastian%20Hofstatter%20and%20Sophia%20Althammer%20and%20Yixuan%20Su%20and%20Aleksandra%20Piktus%20and%20Arkady%20Arkhangorodsky%20and%20Minjie%20Xu%20and%20Naomi%20White%20and%20Patrick%20Lewis&entry.1292438233=%20%20As%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20more%20advanced%2C%20they%20have%20outpaced%0Aour%20abilities%20to%20accurately%20evaluate%20their%20quality.%20Not%20only%20is%20finding%20data%20to%0Aadequately%20probe%20particular%20model%20properties%20difficult%2C%20but%20evaluating%20the%0Acorrectness%20of%20a%20model%27s%20freeform%20generation%20alone%20is%20a%20challenge.%20To%20address%0Athis%2C%20many%20evaluations%20now%20rely%20on%20using%20LLMs%20themselves%20as%20judges%20to%20score%20the%0Aquality%20of%20outputs%20from%20other%20LLMs.%20Evaluations%20most%20commonly%20use%20a%20single%0Alarge%20model%20like%20GPT4.%20While%20this%20method%20has%20grown%20in%20popularity%2C%20it%20is%20costly%2C%0Ahas%20been%20shown%20to%20introduce%20intramodel%20bias%2C%20and%20in%20this%20work%2C%20we%20find%20that%0Avery%20large%20models%20are%20often%20unnecessary.%20We%20propose%20instead%20to%20evaluate%20models%0Ausing%20a%20Panel%20of%20LLm%20evaluators%20%28PoLL%29.%20Across%20three%20distinct%20judge%20settings%0Aand%20spanning%20six%20different%20datasets%2C%20we%20find%20that%20using%20a%20PoLL%20composed%20of%20a%0Alarger%20number%20of%20smaller%20models%20outperforms%20a%20single%20large%20judge%2C%20exhibits%20less%0Aintra-model%20bias%20due%20to%20its%20composition%20of%20disjoint%20model%20families%2C%20and%20does%20so%0Awhile%20being%20over%20seven%20times%20less%20expensive.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18796v1&entry.124074799=Read"},
{"title": "Amodal Ground Truth and Completion in the Wild", "author": "Guanqi Zhan and Chuanxia Zheng and Weidi Xie and Andrew Zisserman", "abstract": "  This paper studies amodal image segmentation: predicting entire object\nsegmentation masks including both visible and invisible (occluded) parts. In\nprevious work, the amodal segmentation ground truth on real images is usually\npredicted by manual annotaton and thus is subjective. In contrast, we use 3D\ndata to establish an automatic pipeline to determine authentic ground truth\namodal masks for partially occluded objects in real images. This pipeline is\nused to construct an amodal completion evaluation benchmark, MP3D-Amodal,\nconsisting of a variety of object categories and labels. To better handle the\namodal completion task in the wild, we explore two architecture variants: a\ntwo-stage model that first infers the occluder, followed by amodal mask\ncompletion; and a one-stage model that exploits the representation power of\nStable Diffusion for amodal segmentation across many categories. Without bells\nand whistles, our method achieves a new state-of-the-art performance on Amodal\nsegmentation datasets that cover a large variety of objects, including COCOA\nand our new MP3D-Amodal dataset. The dataset, model, and code are available at\nhttps://www.robots.ox.ac.uk/~vgg/research/amodal/.\n", "link": "http://arxiv.org/abs/2312.17247v2", "date": "2024-04-29", "relevancy": 1.7869, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.616}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5727}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5677}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Amodal%20Ground%20Truth%20and%20Completion%20in%20the%20Wild&body=Title%3A%20Amodal%20Ground%20Truth%20and%20Completion%20in%20the%20Wild%0AAuthor%3A%20Guanqi%20Zhan%20and%20Chuanxia%20Zheng%20and%20Weidi%20Xie%20and%20Andrew%20Zisserman%0AAbstract%3A%20%20%20This%20paper%20studies%20amodal%20image%20segmentation%3A%20predicting%20entire%20object%0Asegmentation%20masks%20including%20both%20visible%20and%20invisible%20%28occluded%29%20parts.%20In%0Aprevious%20work%2C%20the%20amodal%20segmentation%20ground%20truth%20on%20real%20images%20is%20usually%0Apredicted%20by%20manual%20annotaton%20and%20thus%20is%20subjective.%20In%20contrast%2C%20we%20use%203D%0Adata%20to%20establish%20an%20automatic%20pipeline%20to%20determine%20authentic%20ground%20truth%0Aamodal%20masks%20for%20partially%20occluded%20objects%20in%20real%20images.%20This%20pipeline%20is%0Aused%20to%20construct%20an%20amodal%20completion%20evaluation%20benchmark%2C%20MP3D-Amodal%2C%0Aconsisting%20of%20a%20variety%20of%20object%20categories%20and%20labels.%20To%20better%20handle%20the%0Aamodal%20completion%20task%20in%20the%20wild%2C%20we%20explore%20two%20architecture%20variants%3A%20a%0Atwo-stage%20model%20that%20first%20infers%20the%20occluder%2C%20followed%20by%20amodal%20mask%0Acompletion%3B%20and%20a%20one-stage%20model%20that%20exploits%20the%20representation%20power%20of%0AStable%20Diffusion%20for%20amodal%20segmentation%20across%20many%20categories.%20Without%20bells%0Aand%20whistles%2C%20our%20method%20achieves%20a%20new%20state-of-the-art%20performance%20on%20Amodal%0Asegmentation%20datasets%20that%20cover%20a%20large%20variety%20of%20objects%2C%20including%20COCOA%0Aand%20our%20new%20MP3D-Amodal%20dataset.%20The%20dataset%2C%20model%2C%20and%20code%20are%20available%20at%0Ahttps%3A//www.robots.ox.ac.uk/~vgg/research/amodal/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.17247v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Amodal%20Ground%20Truth%20and%20Completion%20in%20the%20Wild&entry.906535625=Guanqi%20Zhan%20and%20Chuanxia%20Zheng%20and%20Weidi%20Xie%20and%20Andrew%20Zisserman&entry.1292438233=%20%20This%20paper%20studies%20amodal%20image%20segmentation%3A%20predicting%20entire%20object%0Asegmentation%20masks%20including%20both%20visible%20and%20invisible%20%28occluded%29%20parts.%20In%0Aprevious%20work%2C%20the%20amodal%20segmentation%20ground%20truth%20on%20real%20images%20is%20usually%0Apredicted%20by%20manual%20annotaton%20and%20thus%20is%20subjective.%20In%20contrast%2C%20we%20use%203D%0Adata%20to%20establish%20an%20automatic%20pipeline%20to%20determine%20authentic%20ground%20truth%0Aamodal%20masks%20for%20partially%20occluded%20objects%20in%20real%20images.%20This%20pipeline%20is%0Aused%20to%20construct%20an%20amodal%20completion%20evaluation%20benchmark%2C%20MP3D-Amodal%2C%0Aconsisting%20of%20a%20variety%20of%20object%20categories%20and%20labels.%20To%20better%20handle%20the%0Aamodal%20completion%20task%20in%20the%20wild%2C%20we%20explore%20two%20architecture%20variants%3A%20a%0Atwo-stage%20model%20that%20first%20infers%20the%20occluder%2C%20followed%20by%20amodal%20mask%0Acompletion%3B%20and%20a%20one-stage%20model%20that%20exploits%20the%20representation%20power%20of%0AStable%20Diffusion%20for%20amodal%20segmentation%20across%20many%20categories.%20Without%20bells%0Aand%20whistles%2C%20our%20method%20achieves%20a%20new%20state-of-the-art%20performance%20on%20Amodal%0Asegmentation%20datasets%20that%20cover%20a%20large%20variety%20of%20objects%2C%20including%20COCOA%0Aand%20our%20new%20MP3D-Amodal%20dataset.%20The%20dataset%2C%20model%2C%20and%20code%20are%20available%20at%0Ahttps%3A//www.robots.ox.ac.uk/~vgg/research/amodal/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.17247v2&entry.124074799=Read"},
{"title": "Predicting Safety Misbehaviours in Autonomous Driving Systems using\n  Uncertainty Quantification", "author": "Ruben Grewal and Paolo Tonella and Andrea Stocco", "abstract": "  The automated real-time recognition of unexpected situations plays a crucial\nrole in the safety of autonomous vehicles, especially in unsupported and\nunpredictable scenarios. This paper evaluates different Bayesian uncertainty\nquantification methods from the deep learning domain for the anticipatory\ntesting of safety-critical misbehaviours during system-level simulation-based\ntesting. Specifically, we compute uncertainty scores as the vehicle executes,\nfollowing the intuition that high uncertainty scores are indicative of\nunsupported runtime conditions that can be used to distinguish safe from\nfailure-inducing driving behaviors. In our study, we conducted an evaluation of\nthe effectiveness and computational overhead associated with two Bayesian\nuncertainty quantification methods, namely MC- Dropout and Deep Ensembles, for\nmisbehaviour avoidance. Overall, for three benchmarks from the Udacity\nsimulator comprising both out-of-distribution and unsafe conditions introduced\nvia mutation testing, both methods successfully detected a high number of\nout-of-bounds episodes providing early warnings several seconds in advance,\noutperforming two state-of-the-art misbehaviour prediction methods based on\nautoencoders and attention maps in terms of effectiveness and efficiency.\nNotably, Deep Ensembles detected most misbehaviours without any false alarms\nand did so even when employing a relatively small number of models, making them\ncomputationally feasible for real-time detection. Our findings suggest that\nincorporating uncertainty quantification methods is a viable approach for\nbuilding fail-safe mechanisms in deep neural network-based autonomous vehicles.\n", "link": "http://arxiv.org/abs/2404.18573v1", "date": "2024-04-29", "relevancy": 1.7831, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6032}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6006}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5884}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Predicting%20Safety%20Misbehaviours%20in%20Autonomous%20Driving%20Systems%20using%0A%20%20Uncertainty%20Quantification&body=Title%3A%20Predicting%20Safety%20Misbehaviours%20in%20Autonomous%20Driving%20Systems%20using%0A%20%20Uncertainty%20Quantification%0AAuthor%3A%20Ruben%20Grewal%20and%20Paolo%20Tonella%20and%20Andrea%20Stocco%0AAbstract%3A%20%20%20The%20automated%20real-time%20recognition%20of%20unexpected%20situations%20plays%20a%20crucial%0Arole%20in%20the%20safety%20of%20autonomous%20vehicles%2C%20especially%20in%20unsupported%20and%0Aunpredictable%20scenarios.%20This%20paper%20evaluates%20different%20Bayesian%20uncertainty%0Aquantification%20methods%20from%20the%20deep%20learning%20domain%20for%20the%20anticipatory%0Atesting%20of%20safety-critical%20misbehaviours%20during%20system-level%20simulation-based%0Atesting.%20Specifically%2C%20we%20compute%20uncertainty%20scores%20as%20the%20vehicle%20executes%2C%0Afollowing%20the%20intuition%20that%20high%20uncertainty%20scores%20are%20indicative%20of%0Aunsupported%20runtime%20conditions%20that%20can%20be%20used%20to%20distinguish%20safe%20from%0Afailure-inducing%20driving%20behaviors.%20In%20our%20study%2C%20we%20conducted%20an%20evaluation%20of%0Athe%20effectiveness%20and%20computational%20overhead%20associated%20with%20two%20Bayesian%0Auncertainty%20quantification%20methods%2C%20namely%20MC-%20Dropout%20and%20Deep%20Ensembles%2C%20for%0Amisbehaviour%20avoidance.%20Overall%2C%20for%20three%20benchmarks%20from%20the%20Udacity%0Asimulator%20comprising%20both%20out-of-distribution%20and%20unsafe%20conditions%20introduced%0Avia%20mutation%20testing%2C%20both%20methods%20successfully%20detected%20a%20high%20number%20of%0Aout-of-bounds%20episodes%20providing%20early%20warnings%20several%20seconds%20in%20advance%2C%0Aoutperforming%20two%20state-of-the-art%20misbehaviour%20prediction%20methods%20based%20on%0Aautoencoders%20and%20attention%20maps%20in%20terms%20of%20effectiveness%20and%20efficiency.%0ANotably%2C%20Deep%20Ensembles%20detected%20most%20misbehaviours%20without%20any%20false%20alarms%0Aand%20did%20so%20even%20when%20employing%20a%20relatively%20small%20number%20of%20models%2C%20making%20them%0Acomputationally%20feasible%20for%20real-time%20detection.%20Our%20findings%20suggest%20that%0Aincorporating%20uncertainty%20quantification%20methods%20is%20a%20viable%20approach%20for%0Abuilding%20fail-safe%20mechanisms%20in%20deep%20neural%20network-based%20autonomous%20vehicles.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18573v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20Safety%20Misbehaviours%20in%20Autonomous%20Driving%20Systems%20using%0A%20%20Uncertainty%20Quantification&entry.906535625=Ruben%20Grewal%20and%20Paolo%20Tonella%20and%20Andrea%20Stocco&entry.1292438233=%20%20The%20automated%20real-time%20recognition%20of%20unexpected%20situations%20plays%20a%20crucial%0Arole%20in%20the%20safety%20of%20autonomous%20vehicles%2C%20especially%20in%20unsupported%20and%0Aunpredictable%20scenarios.%20This%20paper%20evaluates%20different%20Bayesian%20uncertainty%0Aquantification%20methods%20from%20the%20deep%20learning%20domain%20for%20the%20anticipatory%0Atesting%20of%20safety-critical%20misbehaviours%20during%20system-level%20simulation-based%0Atesting.%20Specifically%2C%20we%20compute%20uncertainty%20scores%20as%20the%20vehicle%20executes%2C%0Afollowing%20the%20intuition%20that%20high%20uncertainty%20scores%20are%20indicative%20of%0Aunsupported%20runtime%20conditions%20that%20can%20be%20used%20to%20distinguish%20safe%20from%0Afailure-inducing%20driving%20behaviors.%20In%20our%20study%2C%20we%20conducted%20an%20evaluation%20of%0Athe%20effectiveness%20and%20computational%20overhead%20associated%20with%20two%20Bayesian%0Auncertainty%20quantification%20methods%2C%20namely%20MC-%20Dropout%20and%20Deep%20Ensembles%2C%20for%0Amisbehaviour%20avoidance.%20Overall%2C%20for%20three%20benchmarks%20from%20the%20Udacity%0Asimulator%20comprising%20both%20out-of-distribution%20and%20unsafe%20conditions%20introduced%0Avia%20mutation%20testing%2C%20both%20methods%20successfully%20detected%20a%20high%20number%20of%0Aout-of-bounds%20episodes%20providing%20early%20warnings%20several%20seconds%20in%20advance%2C%0Aoutperforming%20two%20state-of-the-art%20misbehaviour%20prediction%20methods%20based%20on%0Aautoencoders%20and%20attention%20maps%20in%20terms%20of%20effectiveness%20and%20efficiency.%0ANotably%2C%20Deep%20Ensembles%20detected%20most%20misbehaviours%20without%20any%20false%20alarms%0Aand%20did%20so%20even%20when%20employing%20a%20relatively%20small%20number%20of%20models%2C%20making%20them%0Acomputationally%20feasible%20for%20real-time%20detection.%20Our%20findings%20suggest%20that%0Aincorporating%20uncertainty%20quantification%20methods%20is%20a%20viable%20approach%20for%0Abuilding%20fail-safe%20mechanisms%20in%20deep%20neural%20network-based%20autonomous%20vehicles.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18573v1&entry.124074799=Read"},
{"title": "LangBiTe: A Platform for Testing Bias in Large Language Models", "author": "Sergio Morales and Robert Claris\u00f3 and Jordi Cabot", "abstract": "  The integration of Large Language Models (LLMs) into various software\napplications raises concerns about their potential biases. Typically, those\nmodels are trained on a vast amount of data scrapped from forums, websites,\nsocial media and other internet sources, which may instill harmful and\ndiscriminating behavior into the model. To address this issue, we present\nLangBiTe, a testing platform to systematically assess the presence of biases\nwithin an LLM. LangBiTe enables development teams to tailor their test\nscenarios, and automatically generate and execute the test cases according to a\nset of user-defined ethical requirements. Each test consists of a prompt fed\ninto the LLM and a corresponding test oracle that scrutinizes the LLM's\nresponse for the identification of biases. LangBite provides users with the\nbias evaluation of LLMs, and end-to-end traceability between the initial\nethical requirements and the insights obtained.\n", "link": "http://arxiv.org/abs/2404.18558v1", "date": "2024-04-29", "relevancy": 1.7775, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4674}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4438}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4358}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LangBiTe%3A%20A%20Platform%20for%20Testing%20Bias%20in%20Large%20Language%20Models&body=Title%3A%20LangBiTe%3A%20A%20Platform%20for%20Testing%20Bias%20in%20Large%20Language%20Models%0AAuthor%3A%20Sergio%20Morales%20and%20Robert%20Claris%C3%B3%20and%20Jordi%20Cabot%0AAbstract%3A%20%20%20The%20integration%20of%20Large%20Language%20Models%20%28LLMs%29%20into%20various%20software%0Aapplications%20raises%20concerns%20about%20their%20potential%20biases.%20Typically%2C%20those%0Amodels%20are%20trained%20on%20a%20vast%20amount%20of%20data%20scrapped%20from%20forums%2C%20websites%2C%0Asocial%20media%20and%20other%20internet%20sources%2C%20which%20may%20instill%20harmful%20and%0Adiscriminating%20behavior%20into%20the%20model.%20To%20address%20this%20issue%2C%20we%20present%0ALangBiTe%2C%20a%20testing%20platform%20to%20systematically%20assess%20the%20presence%20of%20biases%0Awithin%20an%20LLM.%20LangBiTe%20enables%20development%20teams%20to%20tailor%20their%20test%0Ascenarios%2C%20and%20automatically%20generate%20and%20execute%20the%20test%20cases%20according%20to%20a%0Aset%20of%20user-defined%20ethical%20requirements.%20Each%20test%20consists%20of%20a%20prompt%20fed%0Ainto%20the%20LLM%20and%20a%20corresponding%20test%20oracle%20that%20scrutinizes%20the%20LLM%27s%0Aresponse%20for%20the%20identification%20of%20biases.%20LangBite%20provides%20users%20with%20the%0Abias%20evaluation%20of%20LLMs%2C%20and%20end-to-end%20traceability%20between%20the%20initial%0Aethical%20requirements%20and%20the%20insights%20obtained.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18558v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LangBiTe%3A%20A%20Platform%20for%20Testing%20Bias%20in%20Large%20Language%20Models&entry.906535625=Sergio%20Morales%20and%20Robert%20Claris%C3%B3%20and%20Jordi%20Cabot&entry.1292438233=%20%20The%20integration%20of%20Large%20Language%20Models%20%28LLMs%29%20into%20various%20software%0Aapplications%20raises%20concerns%20about%20their%20potential%20biases.%20Typically%2C%20those%0Amodels%20are%20trained%20on%20a%20vast%20amount%20of%20data%20scrapped%20from%20forums%2C%20websites%2C%0Asocial%20media%20and%20other%20internet%20sources%2C%20which%20may%20instill%20harmful%20and%0Adiscriminating%20behavior%20into%20the%20model.%20To%20address%20this%20issue%2C%20we%20present%0ALangBiTe%2C%20a%20testing%20platform%20to%20systematically%20assess%20the%20presence%20of%20biases%0Awithin%20an%20LLM.%20LangBiTe%20enables%20development%20teams%20to%20tailor%20their%20test%0Ascenarios%2C%20and%20automatically%20generate%20and%20execute%20the%20test%20cases%20according%20to%20a%0Aset%20of%20user-defined%20ethical%20requirements.%20Each%20test%20consists%20of%20a%20prompt%20fed%0Ainto%20the%20LLM%20and%20a%20corresponding%20test%20oracle%20that%20scrutinizes%20the%20LLM%27s%0Aresponse%20for%20the%20identification%20of%20biases.%20LangBite%20provides%20users%20with%20the%0Abias%20evaluation%20of%20LLMs%2C%20and%20end-to-end%20traceability%20between%20the%20initial%0Aethical%20requirements%20and%20the%20insights%20obtained.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18558v1&entry.124074799=Read"},
{"title": "Clio: Real-time Task-Driven Open-Set 3D Scene Graphs", "author": "Dominic Maggio and Yun Chang and Nathan Hughes and Matthew Trang and Dan Griffith and Carlyn Dougherty and Eric Cristofalo and Lukas Schmid and Luca Carlone", "abstract": "  Modern tools for class-agnostic image segmentation (e.g., SegmentAnything)\nand open-set semantic understanding (e.g., CLIP) provide unprecedented\nopportunities for robot perception and mapping. While traditional closed-set\nmetric-semantic maps were restricted to tens or hundreds of semantic classes,\nwe can now build maps with a plethora of objects and countless semantic\nvariations. This leaves us with a fundamental question: what is the right\ngranularity for the objects (and, more generally, for the semantic concepts)\nthe robot has to include in its map representation? While related work\nimplicitly chooses a level of granularity by tuning thresholds for object\ndetection, we argue that such a choice is intrinsically task-dependent. The\nfirst contribution of this paper is to propose a task-driven 3D scene\nunderstanding problem, where the robot is given a list of tasks in natural\nlanguage and has to select the granularity and the subset of objects and scene\nstructure to retain in its map that is sufficient to complete the tasks. We\nshow that this problem can be naturally formulated using the Information\nBottleneck (IB), an established information-theoretic framework. The second\ncontribution is an algorithm for task-driven 3D scene understanding based on an\nAgglomerative IB approach, that is able to cluster 3D primitives in the\nenvironment into task-relevant objects and regions and executes incrementally.\nThe third contribution is to integrate our task-driven clustering algorithm\ninto a real-time pipeline, named Clio, that constructs a hierarchical 3D scene\ngraph of the environment online using only onboard compute, as the robot\nexplores it. Our final contribution is an extensive experimental campaign\nshowing that Clio not only allows real-time construction of compact open-set 3D\nscene graphs, but also improves the accuracy of task execution by limiting the\nmap to relevant semantic concepts.\n", "link": "http://arxiv.org/abs/2404.13696v3", "date": "2024-04-29", "relevancy": 1.7684, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6707}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5812}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5603}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Clio%3A%20Real-time%20Task-Driven%20Open-Set%203D%20Scene%20Graphs&body=Title%3A%20Clio%3A%20Real-time%20Task-Driven%20Open-Set%203D%20Scene%20Graphs%0AAuthor%3A%20Dominic%20Maggio%20and%20Yun%20Chang%20and%20Nathan%20Hughes%20and%20Matthew%20Trang%20and%20Dan%20Griffith%20and%20Carlyn%20Dougherty%20and%20Eric%20Cristofalo%20and%20Lukas%20Schmid%20and%20Luca%20Carlone%0AAbstract%3A%20%20%20Modern%20tools%20for%20class-agnostic%20image%20segmentation%20%28e.g.%2C%20SegmentAnything%29%0Aand%20open-set%20semantic%20understanding%20%28e.g.%2C%20CLIP%29%20provide%20unprecedented%0Aopportunities%20for%20robot%20perception%20and%20mapping.%20While%20traditional%20closed-set%0Ametric-semantic%20maps%20were%20restricted%20to%20tens%20or%20hundreds%20of%20semantic%20classes%2C%0Awe%20can%20now%20build%20maps%20with%20a%20plethora%20of%20objects%20and%20countless%20semantic%0Avariations.%20This%20leaves%20us%20with%20a%20fundamental%20question%3A%20what%20is%20the%20right%0Agranularity%20for%20the%20objects%20%28and%2C%20more%20generally%2C%20for%20the%20semantic%20concepts%29%0Athe%20robot%20has%20to%20include%20in%20its%20map%20representation%3F%20While%20related%20work%0Aimplicitly%20chooses%20a%20level%20of%20granularity%20by%20tuning%20thresholds%20for%20object%0Adetection%2C%20we%20argue%20that%20such%20a%20choice%20is%20intrinsically%20task-dependent.%20The%0Afirst%20contribution%20of%20this%20paper%20is%20to%20propose%20a%20task-driven%203D%20scene%0Aunderstanding%20problem%2C%20where%20the%20robot%20is%20given%20a%20list%20of%20tasks%20in%20natural%0Alanguage%20and%20has%20to%20select%20the%20granularity%20and%20the%20subset%20of%20objects%20and%20scene%0Astructure%20to%20retain%20in%20its%20map%20that%20is%20sufficient%20to%20complete%20the%20tasks.%20We%0Ashow%20that%20this%20problem%20can%20be%20naturally%20formulated%20using%20the%20Information%0ABottleneck%20%28IB%29%2C%20an%20established%20information-theoretic%20framework.%20The%20second%0Acontribution%20is%20an%20algorithm%20for%20task-driven%203D%20scene%20understanding%20based%20on%20an%0AAgglomerative%20IB%20approach%2C%20that%20is%20able%20to%20cluster%203D%20primitives%20in%20the%0Aenvironment%20into%20task-relevant%20objects%20and%20regions%20and%20executes%20incrementally.%0AThe%20third%20contribution%20is%20to%20integrate%20our%20task-driven%20clustering%20algorithm%0Ainto%20a%20real-time%20pipeline%2C%20named%20Clio%2C%20that%20constructs%20a%20hierarchical%203D%20scene%0Agraph%20of%20the%20environment%20online%20using%20only%20onboard%20compute%2C%20as%20the%20robot%0Aexplores%20it.%20Our%20final%20contribution%20is%20an%20extensive%20experimental%20campaign%0Ashowing%20that%20Clio%20not%20only%20allows%20real-time%20construction%20of%20compact%20open-set%203D%0Ascene%20graphs%2C%20but%20also%20improves%20the%20accuracy%20of%20task%20execution%20by%20limiting%20the%0Amap%20to%20relevant%20semantic%20concepts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.13696v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Clio%3A%20Real-time%20Task-Driven%20Open-Set%203D%20Scene%20Graphs&entry.906535625=Dominic%20Maggio%20and%20Yun%20Chang%20and%20Nathan%20Hughes%20and%20Matthew%20Trang%20and%20Dan%20Griffith%20and%20Carlyn%20Dougherty%20and%20Eric%20Cristofalo%20and%20Lukas%20Schmid%20and%20Luca%20Carlone&entry.1292438233=%20%20Modern%20tools%20for%20class-agnostic%20image%20segmentation%20%28e.g.%2C%20SegmentAnything%29%0Aand%20open-set%20semantic%20understanding%20%28e.g.%2C%20CLIP%29%20provide%20unprecedented%0Aopportunities%20for%20robot%20perception%20and%20mapping.%20While%20traditional%20closed-set%0Ametric-semantic%20maps%20were%20restricted%20to%20tens%20or%20hundreds%20of%20semantic%20classes%2C%0Awe%20can%20now%20build%20maps%20with%20a%20plethora%20of%20objects%20and%20countless%20semantic%0Avariations.%20This%20leaves%20us%20with%20a%20fundamental%20question%3A%20what%20is%20the%20right%0Agranularity%20for%20the%20objects%20%28and%2C%20more%20generally%2C%20for%20the%20semantic%20concepts%29%0Athe%20robot%20has%20to%20include%20in%20its%20map%20representation%3F%20While%20related%20work%0Aimplicitly%20chooses%20a%20level%20of%20granularity%20by%20tuning%20thresholds%20for%20object%0Adetection%2C%20we%20argue%20that%20such%20a%20choice%20is%20intrinsically%20task-dependent.%20The%0Afirst%20contribution%20of%20this%20paper%20is%20to%20propose%20a%20task-driven%203D%20scene%0Aunderstanding%20problem%2C%20where%20the%20robot%20is%20given%20a%20list%20of%20tasks%20in%20natural%0Alanguage%20and%20has%20to%20select%20the%20granularity%20and%20the%20subset%20of%20objects%20and%20scene%0Astructure%20to%20retain%20in%20its%20map%20that%20is%20sufficient%20to%20complete%20the%20tasks.%20We%0Ashow%20that%20this%20problem%20can%20be%20naturally%20formulated%20using%20the%20Information%0ABottleneck%20%28IB%29%2C%20an%20established%20information-theoretic%20framework.%20The%20second%0Acontribution%20is%20an%20algorithm%20for%20task-driven%203D%20scene%20understanding%20based%20on%20an%0AAgglomerative%20IB%20approach%2C%20that%20is%20able%20to%20cluster%203D%20primitives%20in%20the%0Aenvironment%20into%20task-relevant%20objects%20and%20regions%20and%20executes%20incrementally.%0AThe%20third%20contribution%20is%20to%20integrate%20our%20task-driven%20clustering%20algorithm%0Ainto%20a%20real-time%20pipeline%2C%20named%20Clio%2C%20that%20constructs%20a%20hierarchical%203D%20scene%0Agraph%20of%20the%20environment%20online%20using%20only%20onboard%20compute%2C%20as%20the%20robot%0Aexplores%20it.%20Our%20final%20contribution%20is%20an%20extensive%20experimental%20campaign%0Ashowing%20that%20Clio%20not%20only%20allows%20real-time%20construction%20of%20compact%20open-set%203D%0Ascene%20graphs%2C%20but%20also%20improves%20the%20accuracy%20of%20task%20execution%20by%20limiting%20the%0Amap%20to%20relevant%20semantic%20concepts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.13696v3&entry.124074799=Read"},
{"title": "A Multi-Modal Foundation Model to Assist People with Blindness and Low\n  Vision in Environmental Interaction", "author": "Yu Hao and Fan Yang and Hao Huang and Shuaihang Yuan and Sundeep Rangan and John-Ross Rizzo and Yao Wang and Yi Fang", "abstract": "  People with blindness and low vision (pBLV) encounter substantial challenges\nwhen it comes to comprehensive scene recognition and precise object\nidentification in unfamiliar environments. Additionally, due to the vision\nloss, pBLV have difficulty in accessing and identifying potential tripping\nhazards on their own. In this paper, we present a pioneering approach that\nleverages a large vision-language model to enhance visual perception for pBLV,\noffering detailed and comprehensive descriptions of the surrounding\nenvironments and providing warnings about the potential risks. Our method\nbegins by leveraging a large image tagging model (i.e., Recognize Anything\n(RAM)) to identify all common objects present in the captured images. The\nrecognition results and user query are then integrated into a prompt, tailored\nspecifically for pBLV using prompt engineering. By combining the prompt and\ninput image, a large vision-language model (i.e., InstructBLIP) generates\ndetailed and comprehensive descriptions of the environment and identifies\npotential risks in the environment by analyzing the environmental objects and\nscenes, relevant to the prompt. We evaluate our approach through experiments\nconducted on both indoor and outdoor datasets. Our results demonstrate that our\nmethod is able to recognize objects accurately and provide insightful\ndescriptions and analysis of the environment for pBLV.\n", "link": "http://arxiv.org/abs/2310.20225v2", "date": "2024-04-29", "relevancy": 1.7611, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.616}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5793}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5786}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Multi-Modal%20Foundation%20Model%20to%20Assist%20People%20with%20Blindness%20and%20Low%0A%20%20Vision%20in%20Environmental%20Interaction&body=Title%3A%20A%20Multi-Modal%20Foundation%20Model%20to%20Assist%20People%20with%20Blindness%20and%20Low%0A%20%20Vision%20in%20Environmental%20Interaction%0AAuthor%3A%20Yu%20Hao%20and%20Fan%20Yang%20and%20Hao%20Huang%20and%20Shuaihang%20Yuan%20and%20Sundeep%20Rangan%20and%20John-Ross%20Rizzo%20and%20Yao%20Wang%20and%20Yi%20Fang%0AAbstract%3A%20%20%20People%20with%20blindness%20and%20low%20vision%20%28pBLV%29%20encounter%20substantial%20challenges%0Awhen%20it%20comes%20to%20comprehensive%20scene%20recognition%20and%20precise%20object%0Aidentification%20in%20unfamiliar%20environments.%20Additionally%2C%20due%20to%20the%20vision%0Aloss%2C%20pBLV%20have%20difficulty%20in%20accessing%20and%20identifying%20potential%20tripping%0Ahazards%20on%20their%20own.%20In%20this%20paper%2C%20we%20present%20a%20pioneering%20approach%20that%0Aleverages%20a%20large%20vision-language%20model%20to%20enhance%20visual%20perception%20for%20pBLV%2C%0Aoffering%20detailed%20and%20comprehensive%20descriptions%20of%20the%20surrounding%0Aenvironments%20and%20providing%20warnings%20about%20the%20potential%20risks.%20Our%20method%0Abegins%20by%20leveraging%20a%20large%20image%20tagging%20model%20%28i.e.%2C%20Recognize%20Anything%0A%28RAM%29%29%20to%20identify%20all%20common%20objects%20present%20in%20the%20captured%20images.%20The%0Arecognition%20results%20and%20user%20query%20are%20then%20integrated%20into%20a%20prompt%2C%20tailored%0Aspecifically%20for%20pBLV%20using%20prompt%20engineering.%20By%20combining%20the%20prompt%20and%0Ainput%20image%2C%20a%20large%20vision-language%20model%20%28i.e.%2C%20InstructBLIP%29%20generates%0Adetailed%20and%20comprehensive%20descriptions%20of%20the%20environment%20and%20identifies%0Apotential%20risks%20in%20the%20environment%20by%20analyzing%20the%20environmental%20objects%20and%0Ascenes%2C%20relevant%20to%20the%20prompt.%20We%20evaluate%20our%20approach%20through%20experiments%0Aconducted%20on%20both%20indoor%20and%20outdoor%20datasets.%20Our%20results%20demonstrate%20that%20our%0Amethod%20is%20able%20to%20recognize%20objects%20accurately%20and%20provide%20insightful%0Adescriptions%20and%20analysis%20of%20the%20environment%20for%20pBLV.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.20225v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Multi-Modal%20Foundation%20Model%20to%20Assist%20People%20with%20Blindness%20and%20Low%0A%20%20Vision%20in%20Environmental%20Interaction&entry.906535625=Yu%20Hao%20and%20Fan%20Yang%20and%20Hao%20Huang%20and%20Shuaihang%20Yuan%20and%20Sundeep%20Rangan%20and%20John-Ross%20Rizzo%20and%20Yao%20Wang%20and%20Yi%20Fang&entry.1292438233=%20%20People%20with%20blindness%20and%20low%20vision%20%28pBLV%29%20encounter%20substantial%20challenges%0Awhen%20it%20comes%20to%20comprehensive%20scene%20recognition%20and%20precise%20object%0Aidentification%20in%20unfamiliar%20environments.%20Additionally%2C%20due%20to%20the%20vision%0Aloss%2C%20pBLV%20have%20difficulty%20in%20accessing%20and%20identifying%20potential%20tripping%0Ahazards%20on%20their%20own.%20In%20this%20paper%2C%20we%20present%20a%20pioneering%20approach%20that%0Aleverages%20a%20large%20vision-language%20model%20to%20enhance%20visual%20perception%20for%20pBLV%2C%0Aoffering%20detailed%20and%20comprehensive%20descriptions%20of%20the%20surrounding%0Aenvironments%20and%20providing%20warnings%20about%20the%20potential%20risks.%20Our%20method%0Abegins%20by%20leveraging%20a%20large%20image%20tagging%20model%20%28i.e.%2C%20Recognize%20Anything%0A%28RAM%29%29%20to%20identify%20all%20common%20objects%20present%20in%20the%20captured%20images.%20The%0Arecognition%20results%20and%20user%20query%20are%20then%20integrated%20into%20a%20prompt%2C%20tailored%0Aspecifically%20for%20pBLV%20using%20prompt%20engineering.%20By%20combining%20the%20prompt%20and%0Ainput%20image%2C%20a%20large%20vision-language%20model%20%28i.e.%2C%20InstructBLIP%29%20generates%0Adetailed%20and%20comprehensive%20descriptions%20of%20the%20environment%20and%20identifies%0Apotential%20risks%20in%20the%20environment%20by%20analyzing%20the%20environmental%20objects%20and%0Ascenes%2C%20relevant%20to%20the%20prompt.%20We%20evaluate%20our%20approach%20through%20experiments%0Aconducted%20on%20both%20indoor%20and%20outdoor%20datasets.%20Our%20results%20demonstrate%20that%20our%0Amethod%20is%20able%20to%20recognize%20objects%20accurately%20and%20provide%20insightful%0Adescriptions%20and%20analysis%20of%20the%20environment%20for%20pBLV.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.20225v2&entry.124074799=Read"},
{"title": "Benchmarking the CoW with the TopCoW Challenge: Topology-Aware\n  Anatomical Segmentation of the Circle of Willis for CTA and MRA", "author": "Kaiyuan Yang and Fabio Musio and Yihui Ma and Norman Juchler and Johannes C. Paetzold and Rami Al-Maskari and Luciano H\u00f6her and Hongwei Bran Li and Ibrahim Ethem Hamamci and Anjany Sekuboyina and Suprosanna Shit and Houjing Huang and Chinmay Prabhakar and Ezequiel de la Rosa and Diana Waldmannstetter and Florian Kofler and Fernando Navarro and Martin Menten and Ivan Ezhov and Daniel Rueckert and Iris Vos and Ynte Ruigrok and Birgitta Velthuis and Hugo Kuijf and Julien H\u00e4mmerli and Catherine Wurster and Philippe Bijlenga and Laura Westphal and Jeroen Bisschop and Elisa Colombo and Hakim Baazaoui and Andrew Makmur and James Hallinan and Bene Wiestler and Jan S. Kirschke and Roland Wiest and Emmanuel Montagnon and Laurent Letourneau-Guillon and Adrian Galdran and Francesco Galati and Daniele Falcetta and Maria A. Zuluaga and Chaolong Lin and Haoran Zhao and Zehan Zhang and Sinyoung Ra and Jongyun Hwang and Hyunjin Park and Junqiang Chen and Marek Wodzinski and Henning M\u00fcller and Pengcheng Shi and Wei Liu and Ting Ma and Cansu Yal\u00e7in and Rachika E. Hamadache and Joaquim Salvi and Xavier Llado and Uma Maria Lal-Trehan Estrada and Valeriia Abramova and Luca Giancardo and Arnau Oliver and Jialu Liu and Haibin Huang and Yue Cui and Zehang Lin and Yusheng Liu and Shunzhi Zhu and Tatsat R. Patel and Vincent M. Tutino and Maysam Orouskhani and Huayu Wang and Mahmud Mossa-Basha and Chengcheng Zhu and Maximilian R. Rokuss and Yannick Kirchhoff and Nico Disch and Julius Holzschuh and Fabian Isensee and Klaus Maier-Hein and Yuki Sato and Sven Hirsch and Susanne Wegener and Bjoern Menze", "abstract": "  The Circle of Willis (CoW) is an important network of arteries connecting\nmajor circulations of the brain. Its vascular architecture is believed to\naffect the risk, severity, and clinical outcome of serious neuro-vascular\ndiseases. However, characterizing the highly variable CoW anatomy is still a\nmanual and time-consuming expert task. The CoW is usually imaged by two\nangiographic imaging modalities, magnetic resonance angiography (MRA) and\ncomputed tomography angiography (CTA), but there exist limited public datasets\nwith annotations on CoW anatomy, especially for CTA. Therefore we organized the\nTopCoW Challenge in 2023 with the release of an annotated CoW dataset. The\nTopCoW dataset was the first public dataset with voxel-level annotations for\nthirteen possible CoW vessel components, enabled by virtual-reality (VR)\ntechnology. It was also the first large dataset with paired MRA and CTA from\nthe same patients. TopCoW challenge formalized the CoW characterization problem\nas a multiclass anatomical segmentation task with an emphasis on topological\nmetrics. We invited submissions worldwide for the CoW segmentation task, which\nattracted over 140 registered participants from four continents. The top\nperforming teams managed to segment many CoW components to Dice scores around\n90%, but with lower scores for communicating arteries and rare variants. There\nwere also topological mistakes for predictions with high Dice scores.\nAdditional topological analysis revealed further areas for improvement in\ndetecting certain CoW components and matching CoW variant topology accurately.\nTopCoW represented a first attempt at benchmarking the CoW anatomical\nsegmentation task for MRA and CTA, both morphologically and topologically.\n", "link": "http://arxiv.org/abs/2312.17670v3", "date": "2024-04-29", "relevancy": 1.7603, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.464}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4284}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4095}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20the%20CoW%20with%20the%20TopCoW%20Challenge%3A%20Topology-Aware%0A%20%20Anatomical%20Segmentation%20of%20the%20Circle%20of%20Willis%20for%20CTA%20and%20MRA&body=Title%3A%20Benchmarking%20the%20CoW%20with%20the%20TopCoW%20Challenge%3A%20Topology-Aware%0A%20%20Anatomical%20Segmentation%20of%20the%20Circle%20of%20Willis%20for%20CTA%20and%20MRA%0AAuthor%3A%20Kaiyuan%20Yang%20and%20Fabio%20Musio%20and%20Yihui%20Ma%20and%20Norman%20Juchler%20and%20Johannes%20C.%20Paetzold%20and%20Rami%20Al-Maskari%20and%20Luciano%20H%C3%B6her%20and%20Hongwei%20Bran%20Li%20and%20Ibrahim%20Ethem%20Hamamci%20and%20Anjany%20Sekuboyina%20and%20Suprosanna%20Shit%20and%20Houjing%20Huang%20and%20Chinmay%20Prabhakar%20and%20Ezequiel%20de%20la%20Rosa%20and%20Diana%20Waldmannstetter%20and%20Florian%20Kofler%20and%20Fernando%20Navarro%20and%20Martin%20Menten%20and%20Ivan%20Ezhov%20and%20Daniel%20Rueckert%20and%20Iris%20Vos%20and%20Ynte%20Ruigrok%20and%20Birgitta%20Velthuis%20and%20Hugo%20Kuijf%20and%20Julien%20H%C3%A4mmerli%20and%20Catherine%20Wurster%20and%20Philippe%20Bijlenga%20and%20Laura%20Westphal%20and%20Jeroen%20Bisschop%20and%20Elisa%20Colombo%20and%20Hakim%20Baazaoui%20and%20Andrew%20Makmur%20and%20James%20Hallinan%20and%20Bene%20Wiestler%20and%20Jan%20S.%20Kirschke%20and%20Roland%20Wiest%20and%20Emmanuel%20Montagnon%20and%20Laurent%20Letourneau-Guillon%20and%20Adrian%20Galdran%20and%20Francesco%20Galati%20and%20Daniele%20Falcetta%20and%20Maria%20A.%20Zuluaga%20and%20Chaolong%20Lin%20and%20Haoran%20Zhao%20and%20Zehan%20Zhang%20and%20Sinyoung%20Ra%20and%20Jongyun%20Hwang%20and%20Hyunjin%20Park%20and%20Junqiang%20Chen%20and%20Marek%20Wodzinski%20and%20Henning%20M%C3%BCller%20and%20Pengcheng%20Shi%20and%20Wei%20Liu%20and%20Ting%20Ma%20and%20Cansu%20Yal%C3%A7in%20and%20Rachika%20E.%20Hamadache%20and%20Joaquim%20Salvi%20and%20Xavier%20Llado%20and%20Uma%20Maria%20Lal-Trehan%20Estrada%20and%20Valeriia%20Abramova%20and%20Luca%20Giancardo%20and%20Arnau%20Oliver%20and%20Jialu%20Liu%20and%20Haibin%20Huang%20and%20Yue%20Cui%20and%20Zehang%20Lin%20and%20Yusheng%20Liu%20and%20Shunzhi%20Zhu%20and%20Tatsat%20R.%20Patel%20and%20Vincent%20M.%20Tutino%20and%20Maysam%20Orouskhani%20and%20Huayu%20Wang%20and%20Mahmud%20Mossa-Basha%20and%20Chengcheng%20Zhu%20and%20Maximilian%20R.%20Rokuss%20and%20Yannick%20Kirchhoff%20and%20Nico%20Disch%20and%20Julius%20Holzschuh%20and%20Fabian%20Isensee%20and%20Klaus%20Maier-Hein%20and%20Yuki%20Sato%20and%20Sven%20Hirsch%20and%20Susanne%20Wegener%20and%20Bjoern%20Menze%0AAbstract%3A%20%20%20The%20Circle%20of%20Willis%20%28CoW%29%20is%20an%20important%20network%20of%20arteries%20connecting%0Amajor%20circulations%20of%20the%20brain.%20Its%20vascular%20architecture%20is%20believed%20to%0Aaffect%20the%20risk%2C%20severity%2C%20and%20clinical%20outcome%20of%20serious%20neuro-vascular%0Adiseases.%20However%2C%20characterizing%20the%20highly%20variable%20CoW%20anatomy%20is%20still%20a%0Amanual%20and%20time-consuming%20expert%20task.%20The%20CoW%20is%20usually%20imaged%20by%20two%0Aangiographic%20imaging%20modalities%2C%20magnetic%20resonance%20angiography%20%28MRA%29%20and%0Acomputed%20tomography%20angiography%20%28CTA%29%2C%20but%20there%20exist%20limited%20public%20datasets%0Awith%20annotations%20on%20CoW%20anatomy%2C%20especially%20for%20CTA.%20Therefore%20we%20organized%20the%0ATopCoW%20Challenge%20in%202023%20with%20the%20release%20of%20an%20annotated%20CoW%20dataset.%20The%0ATopCoW%20dataset%20was%20the%20first%20public%20dataset%20with%20voxel-level%20annotations%20for%0Athirteen%20possible%20CoW%20vessel%20components%2C%20enabled%20by%20virtual-reality%20%28VR%29%0Atechnology.%20It%20was%20also%20the%20first%20large%20dataset%20with%20paired%20MRA%20and%20CTA%20from%0Athe%20same%20patients.%20TopCoW%20challenge%20formalized%20the%20CoW%20characterization%20problem%0Aas%20a%20multiclass%20anatomical%20segmentation%20task%20with%20an%20emphasis%20on%20topological%0Ametrics.%20We%20invited%20submissions%20worldwide%20for%20the%20CoW%20segmentation%20task%2C%20which%0Aattracted%20over%20140%20registered%20participants%20from%20four%20continents.%20The%20top%0Aperforming%20teams%20managed%20to%20segment%20many%20CoW%20components%20to%20Dice%20scores%20around%0A90%25%2C%20but%20with%20lower%20scores%20for%20communicating%20arteries%20and%20rare%20variants.%20There%0Awere%20also%20topological%20mistakes%20for%20predictions%20with%20high%20Dice%20scores.%0AAdditional%20topological%20analysis%20revealed%20further%20areas%20for%20improvement%20in%0Adetecting%20certain%20CoW%20components%20and%20matching%20CoW%20variant%20topology%20accurately.%0ATopCoW%20represented%20a%20first%20attempt%20at%20benchmarking%20the%20CoW%20anatomical%0Asegmentation%20task%20for%20MRA%20and%20CTA%2C%20both%20morphologically%20and%20topologically.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.17670v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20the%20CoW%20with%20the%20TopCoW%20Challenge%3A%20Topology-Aware%0A%20%20Anatomical%20Segmentation%20of%20the%20Circle%20of%20Willis%20for%20CTA%20and%20MRA&entry.906535625=Kaiyuan%20Yang%20and%20Fabio%20Musio%20and%20Yihui%20Ma%20and%20Norman%20Juchler%20and%20Johannes%20C.%20Paetzold%20and%20Rami%20Al-Maskari%20and%20Luciano%20H%C3%B6her%20and%20Hongwei%20Bran%20Li%20and%20Ibrahim%20Ethem%20Hamamci%20and%20Anjany%20Sekuboyina%20and%20Suprosanna%20Shit%20and%20Houjing%20Huang%20and%20Chinmay%20Prabhakar%20and%20Ezequiel%20de%20la%20Rosa%20and%20Diana%20Waldmannstetter%20and%20Florian%20Kofler%20and%20Fernando%20Navarro%20and%20Martin%20Menten%20and%20Ivan%20Ezhov%20and%20Daniel%20Rueckert%20and%20Iris%20Vos%20and%20Ynte%20Ruigrok%20and%20Birgitta%20Velthuis%20and%20Hugo%20Kuijf%20and%20Julien%20H%C3%A4mmerli%20and%20Catherine%20Wurster%20and%20Philippe%20Bijlenga%20and%20Laura%20Westphal%20and%20Jeroen%20Bisschop%20and%20Elisa%20Colombo%20and%20Hakim%20Baazaoui%20and%20Andrew%20Makmur%20and%20James%20Hallinan%20and%20Bene%20Wiestler%20and%20Jan%20S.%20Kirschke%20and%20Roland%20Wiest%20and%20Emmanuel%20Montagnon%20and%20Laurent%20Letourneau-Guillon%20and%20Adrian%20Galdran%20and%20Francesco%20Galati%20and%20Daniele%20Falcetta%20and%20Maria%20A.%20Zuluaga%20and%20Chaolong%20Lin%20and%20Haoran%20Zhao%20and%20Zehan%20Zhang%20and%20Sinyoung%20Ra%20and%20Jongyun%20Hwang%20and%20Hyunjin%20Park%20and%20Junqiang%20Chen%20and%20Marek%20Wodzinski%20and%20Henning%20M%C3%BCller%20and%20Pengcheng%20Shi%20and%20Wei%20Liu%20and%20Ting%20Ma%20and%20Cansu%20Yal%C3%A7in%20and%20Rachika%20E.%20Hamadache%20and%20Joaquim%20Salvi%20and%20Xavier%20Llado%20and%20Uma%20Maria%20Lal-Trehan%20Estrada%20and%20Valeriia%20Abramova%20and%20Luca%20Giancardo%20and%20Arnau%20Oliver%20and%20Jialu%20Liu%20and%20Haibin%20Huang%20and%20Yue%20Cui%20and%20Zehang%20Lin%20and%20Yusheng%20Liu%20and%20Shunzhi%20Zhu%20and%20Tatsat%20R.%20Patel%20and%20Vincent%20M.%20Tutino%20and%20Maysam%20Orouskhani%20and%20Huayu%20Wang%20and%20Mahmud%20Mossa-Basha%20and%20Chengcheng%20Zhu%20and%20Maximilian%20R.%20Rokuss%20and%20Yannick%20Kirchhoff%20and%20Nico%20Disch%20and%20Julius%20Holzschuh%20and%20Fabian%20Isensee%20and%20Klaus%20Maier-Hein%20and%20Yuki%20Sato%20and%20Sven%20Hirsch%20and%20Susanne%20Wegener%20and%20Bjoern%20Menze&entry.1292438233=%20%20The%20Circle%20of%20Willis%20%28CoW%29%20is%20an%20important%20network%20of%20arteries%20connecting%0Amajor%20circulations%20of%20the%20brain.%20Its%20vascular%20architecture%20is%20believed%20to%0Aaffect%20the%20risk%2C%20severity%2C%20and%20clinical%20outcome%20of%20serious%20neuro-vascular%0Adiseases.%20However%2C%20characterizing%20the%20highly%20variable%20CoW%20anatomy%20is%20still%20a%0Amanual%20and%20time-consuming%20expert%20task.%20The%20CoW%20is%20usually%20imaged%20by%20two%0Aangiographic%20imaging%20modalities%2C%20magnetic%20resonance%20angiography%20%28MRA%29%20and%0Acomputed%20tomography%20angiography%20%28CTA%29%2C%20but%20there%20exist%20limited%20public%20datasets%0Awith%20annotations%20on%20CoW%20anatomy%2C%20especially%20for%20CTA.%20Therefore%20we%20organized%20the%0ATopCoW%20Challenge%20in%202023%20with%20the%20release%20of%20an%20annotated%20CoW%20dataset.%20The%0ATopCoW%20dataset%20was%20the%20first%20public%20dataset%20with%20voxel-level%20annotations%20for%0Athirteen%20possible%20CoW%20vessel%20components%2C%20enabled%20by%20virtual-reality%20%28VR%29%0Atechnology.%20It%20was%20also%20the%20first%20large%20dataset%20with%20paired%20MRA%20and%20CTA%20from%0Athe%20same%20patients.%20TopCoW%20challenge%20formalized%20the%20CoW%20characterization%20problem%0Aas%20a%20multiclass%20anatomical%20segmentation%20task%20with%20an%20emphasis%20on%20topological%0Ametrics.%20We%20invited%20submissions%20worldwide%20for%20the%20CoW%20segmentation%20task%2C%20which%0Aattracted%20over%20140%20registered%20participants%20from%20four%20continents.%20The%20top%0Aperforming%20teams%20managed%20to%20segment%20many%20CoW%20components%20to%20Dice%20scores%20around%0A90%25%2C%20but%20with%20lower%20scores%20for%20communicating%20arteries%20and%20rare%20variants.%20There%0Awere%20also%20topological%20mistakes%20for%20predictions%20with%20high%20Dice%20scores.%0AAdditional%20topological%20analysis%20revealed%20further%20areas%20for%20improvement%20in%0Adetecting%20certain%20CoW%20components%20and%20matching%20CoW%20variant%20topology%20accurately.%0ATopCoW%20represented%20a%20first%20attempt%20at%20benchmarking%20the%20CoW%20anatomical%0Asegmentation%20task%20for%20MRA%20and%20CTA%2C%20both%20morphologically%20and%20topologically.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.17670v3&entry.124074799=Read"},
{"title": "Improving Automatic Text Recognition with Language Models in the PyLaia\n  Open-Source Library", "author": "Sol\u00e8ne Tarride and Yoann Schneider and Marie Generali-Lince and M\u00e9lodie Boillet and Bastien Abadie and Christopher Kermorvant", "abstract": "  PyLaia is one of the most popular open-source software for Automatic Text\nRecognition (ATR), delivering strong performance in terms of speed and\naccuracy. In this paper, we outline our recent contributions to the PyLaia\nlibrary, focusing on the incorporation of reliable confidence scores and the\nintegration of statistical language modeling during decoding. Our\nimplementation provides an easy way to combine PyLaia with n-grams language\nmodels at different levels. One of the highlights of this work is that language\nmodels are completely auto-tuned: they can be built and used easily without any\nexpert knowledge, and without requiring any additional data. To demonstrate the\nsignificance of our contribution, we evaluate PyLaia's performance on twelve\ndatasets, both with and without language modelling. The results show that\ndecoding with small language models improves the Word Error Rate by 13% and the\nCharacter Error Rate by 12% in average. Additionally, we conduct an analysis of\nconfidence scores and highlight the importance of calibration techniques. Our\nimplementation is publicly available in the official PyLaia repository at\nhttps://gitlab.teklia.com/atr/pylaia, and twelve open-source models are\nreleased on Hugging Face.\n", "link": "http://arxiv.org/abs/2404.18722v1", "date": "2024-04-29", "relevancy": 1.7576, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4492}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4418}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4331}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Improving%20Automatic%20Text%20Recognition%20with%20Language%20Models%20in%20the%20PyLaia%0A%20%20Open-Source%20Library&body=Title%3A%20Improving%20Automatic%20Text%20Recognition%20with%20Language%20Models%20in%20the%20PyLaia%0A%20%20Open-Source%20Library%0AAuthor%3A%20Sol%C3%A8ne%20Tarride%20and%20Yoann%20Schneider%20and%20Marie%20Generali-Lince%20and%20M%C3%A9lodie%20Boillet%20and%20Bastien%20Abadie%20and%20Christopher%20Kermorvant%0AAbstract%3A%20%20%20PyLaia%20is%20one%20of%20the%20most%20popular%20open-source%20software%20for%20Automatic%20Text%0ARecognition%20%28ATR%29%2C%20delivering%20strong%20performance%20in%20terms%20of%20speed%20and%0Aaccuracy.%20In%20this%20paper%2C%20we%20outline%20our%20recent%20contributions%20to%20the%20PyLaia%0Alibrary%2C%20focusing%20on%20the%20incorporation%20of%20reliable%20confidence%20scores%20and%20the%0Aintegration%20of%20statistical%20language%20modeling%20during%20decoding.%20Our%0Aimplementation%20provides%20an%20easy%20way%20to%20combine%20PyLaia%20with%20n-grams%20language%0Amodels%20at%20different%20levels.%20One%20of%20the%20highlights%20of%20this%20work%20is%20that%20language%0Amodels%20are%20completely%20auto-tuned%3A%20they%20can%20be%20built%20and%20used%20easily%20without%20any%0Aexpert%20knowledge%2C%20and%20without%20requiring%20any%20additional%20data.%20To%20demonstrate%20the%0Asignificance%20of%20our%20contribution%2C%20we%20evaluate%20PyLaia%27s%20performance%20on%20twelve%0Adatasets%2C%20both%20with%20and%20without%20language%20modelling.%20The%20results%20show%20that%0Adecoding%20with%20small%20language%20models%20improves%20the%20Word%20Error%20Rate%20by%2013%25%20and%20the%0ACharacter%20Error%20Rate%20by%2012%25%20in%20average.%20Additionally%2C%20we%20conduct%20an%20analysis%20of%0Aconfidence%20scores%20and%20highlight%20the%20importance%20of%20calibration%20techniques.%20Our%0Aimplementation%20is%20publicly%20available%20in%20the%20official%20PyLaia%20repository%20at%0Ahttps%3A//gitlab.teklia.com/atr/pylaia%2C%20and%20twelve%20open-source%20models%20are%0Areleased%20on%20Hugging%20Face.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18722v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Automatic%20Text%20Recognition%20with%20Language%20Models%20in%20the%20PyLaia%0A%20%20Open-Source%20Library&entry.906535625=Sol%C3%A8ne%20Tarride%20and%20Yoann%20Schneider%20and%20Marie%20Generali-Lince%20and%20M%C3%A9lodie%20Boillet%20and%20Bastien%20Abadie%20and%20Christopher%20Kermorvant&entry.1292438233=%20%20PyLaia%20is%20one%20of%20the%20most%20popular%20open-source%20software%20for%20Automatic%20Text%0ARecognition%20%28ATR%29%2C%20delivering%20strong%20performance%20in%20terms%20of%20speed%20and%0Aaccuracy.%20In%20this%20paper%2C%20we%20outline%20our%20recent%20contributions%20to%20the%20PyLaia%0Alibrary%2C%20focusing%20on%20the%20incorporation%20of%20reliable%20confidence%20scores%20and%20the%0Aintegration%20of%20statistical%20language%20modeling%20during%20decoding.%20Our%0Aimplementation%20provides%20an%20easy%20way%20to%20combine%20PyLaia%20with%20n-grams%20language%0Amodels%20at%20different%20levels.%20One%20of%20the%20highlights%20of%20this%20work%20is%20that%20language%0Amodels%20are%20completely%20auto-tuned%3A%20they%20can%20be%20built%20and%20used%20easily%20without%20any%0Aexpert%20knowledge%2C%20and%20without%20requiring%20any%20additional%20data.%20To%20demonstrate%20the%0Asignificance%20of%20our%20contribution%2C%20we%20evaluate%20PyLaia%27s%20performance%20on%20twelve%0Adatasets%2C%20both%20with%20and%20without%20language%20modelling.%20The%20results%20show%20that%0Adecoding%20with%20small%20language%20models%20improves%20the%20Word%20Error%20Rate%20by%2013%25%20and%20the%0ACharacter%20Error%20Rate%20by%2012%25%20in%20average.%20Additionally%2C%20we%20conduct%20an%20analysis%20of%0Aconfidence%20scores%20and%20highlight%20the%20importance%20of%20calibration%20techniques.%20Our%0Aimplementation%20is%20publicly%20available%20in%20the%20official%20PyLaia%20repository%20at%0Ahttps%3A//gitlab.teklia.com/atr/pylaia%2C%20and%20twelve%20open-source%20models%20are%0Areleased%20on%20Hugging%20Face.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18722v1&entry.124074799=Read"},
{"title": "Optimal time sampling in physics-informed neural networks", "author": "Gabriel Turinici", "abstract": "  Physics-informed neural networks (PINN) is a extremely powerful paradigm used\nto solve equations encountered in scientific computing applications. An\nimportant part of the procedure is the minimization of the equation residual\nwhich includes, when the equation is time-dependent, a time sampling. It was\nargued in the literature that the sampling need not be uniform but should\noverweight initial time instants, but no rigorous explanation was provided for\nthese choice. In this paper we take some prototypical examples and, under\nstandard hypothesis concerning the neural network convergence, we show that the\noptimal time sampling follows a truncated exponential distribution. In\nparticular we explain when the time sampling is best to be uniform and when it\nshould not be. The findings are illustrated with numerical examples on linear\nequation, Burgers' equation and the Lorenz system.\n", "link": "http://arxiv.org/abs/2404.18780v1", "date": "2024-04-29", "relevancy": 1.7472, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4415}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4335}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4334}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Optimal%20time%20sampling%20in%20physics-informed%20neural%20networks&body=Title%3A%20Optimal%20time%20sampling%20in%20physics-informed%20neural%20networks%0AAuthor%3A%20Gabriel%20Turinici%0AAbstract%3A%20%20%20Physics-informed%20neural%20networks%20%28PINN%29%20is%20a%20extremely%20powerful%20paradigm%20used%0Ato%20solve%20equations%20encountered%20in%20scientific%20computing%20applications.%20An%0Aimportant%20part%20of%20the%20procedure%20is%20the%20minimization%20of%20the%20equation%20residual%0Awhich%20includes%2C%20when%20the%20equation%20is%20time-dependent%2C%20a%20time%20sampling.%20It%20was%0Aargued%20in%20the%20literature%20that%20the%20sampling%20need%20not%20be%20uniform%20but%20should%0Aoverweight%20initial%20time%20instants%2C%20but%20no%20rigorous%20explanation%20was%20provided%20for%0Athese%20choice.%20In%20this%20paper%20we%20take%20some%20prototypical%20examples%20and%2C%20under%0Astandard%20hypothesis%20concerning%20the%20neural%20network%20convergence%2C%20we%20show%20that%20the%0Aoptimal%20time%20sampling%20follows%20a%20truncated%20exponential%20distribution.%20In%0Aparticular%20we%20explain%20when%20the%20time%20sampling%20is%20best%20to%20be%20uniform%20and%20when%20it%0Ashould%20not%20be.%20The%20findings%20are%20illustrated%20with%20numerical%20examples%20on%20linear%0Aequation%2C%20Burgers%27%20equation%20and%20the%20Lorenz%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18780v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20time%20sampling%20in%20physics-informed%20neural%20networks&entry.906535625=Gabriel%20Turinici&entry.1292438233=%20%20Physics-informed%20neural%20networks%20%28PINN%29%20is%20a%20extremely%20powerful%20paradigm%20used%0Ato%20solve%20equations%20encountered%20in%20scientific%20computing%20applications.%20An%0Aimportant%20part%20of%20the%20procedure%20is%20the%20minimization%20of%20the%20equation%20residual%0Awhich%20includes%2C%20when%20the%20equation%20is%20time-dependent%2C%20a%20time%20sampling.%20It%20was%0Aargued%20in%20the%20literature%20that%20the%20sampling%20need%20not%20be%20uniform%20but%20should%0Aoverweight%20initial%20time%20instants%2C%20but%20no%20rigorous%20explanation%20was%20provided%20for%0Athese%20choice.%20In%20this%20paper%20we%20take%20some%20prototypical%20examples%20and%2C%20under%0Astandard%20hypothesis%20concerning%20the%20neural%20network%20convergence%2C%20we%20show%20that%20the%0Aoptimal%20time%20sampling%20follows%20a%20truncated%20exponential%20distribution.%20In%0Aparticular%20we%20explain%20when%20the%20time%20sampling%20is%20best%20to%20be%20uniform%20and%20when%20it%0Ashould%20not%20be.%20The%20findings%20are%20illustrated%20with%20numerical%20examples%20on%20linear%0Aequation%2C%20Burgers%27%20equation%20and%20the%20Lorenz%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18780v1&entry.124074799=Read"},
{"title": "Point Cloud Models Improve Visual Robustness in Robotic Learners", "author": "Skand Peri and Iain Lee and Chanho Kim and Li Fuxin and Tucker Hermans and Stefan Lee", "abstract": "  Visual control policies can encounter significant performance degradation\nwhen visual conditions like lighting or camera position differ from those seen\nduring training -- often exhibiting sharp declines in capability even for minor\ndifferences. In this work, we examine robustness to a suite of these types of\nvisual changes for RGB-D and point cloud based visual control policies. To\nperform these experiments on both model-free and model-based reinforcement\nlearners, we introduce a novel Point Cloud World Model (PCWM) and point cloud\nbased control policies. Our experiments show that policies that explicitly\nencode point clouds are significantly more robust than their RGB-D\ncounterparts. Further, we find our proposed PCWM significantly outperforms\nprior works in terms of sample efficiency during training. Taken together,\nthese results suggest reasoning about the 3D scene through point clouds can\nimprove performance, reduce learning time, and increase robustness for robotic\nlearners. Project Webpage: https://pvskand.github.io/projects/PCWM\n", "link": "http://arxiv.org/abs/2404.18926v1", "date": "2024-04-29", "relevancy": 1.7381, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5909}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5864}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5434}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Point%20Cloud%20Models%20Improve%20Visual%20Robustness%20in%20Robotic%20Learners&body=Title%3A%20Point%20Cloud%20Models%20Improve%20Visual%20Robustness%20in%20Robotic%20Learners%0AAuthor%3A%20Skand%20Peri%20and%20Iain%20Lee%20and%20Chanho%20Kim%20and%20Li%20Fuxin%20and%20Tucker%20Hermans%20and%20Stefan%20Lee%0AAbstract%3A%20%20%20Visual%20control%20policies%20can%20encounter%20significant%20performance%20degradation%0Awhen%20visual%20conditions%20like%20lighting%20or%20camera%20position%20differ%20from%20those%20seen%0Aduring%20training%20--%20often%20exhibiting%20sharp%20declines%20in%20capability%20even%20for%20minor%0Adifferences.%20In%20this%20work%2C%20we%20examine%20robustness%20to%20a%20suite%20of%20these%20types%20of%0Avisual%20changes%20for%20RGB-D%20and%20point%20cloud%20based%20visual%20control%20policies.%20To%0Aperform%20these%20experiments%20on%20both%20model-free%20and%20model-based%20reinforcement%0Alearners%2C%20we%20introduce%20a%20novel%20Point%20Cloud%20World%20Model%20%28PCWM%29%20and%20point%20cloud%0Abased%20control%20policies.%20Our%20experiments%20show%20that%20policies%20that%20explicitly%0Aencode%20point%20clouds%20are%20significantly%20more%20robust%20than%20their%20RGB-D%0Acounterparts.%20Further%2C%20we%20find%20our%20proposed%20PCWM%20significantly%20outperforms%0Aprior%20works%20in%20terms%20of%20sample%20efficiency%20during%20training.%20Taken%20together%2C%0Athese%20results%20suggest%20reasoning%20about%20the%203D%20scene%20through%20point%20clouds%20can%0Aimprove%20performance%2C%20reduce%20learning%20time%2C%20and%20increase%20robustness%20for%20robotic%0Alearners.%20Project%20Webpage%3A%20https%3A//pvskand.github.io/projects/PCWM%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18926v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point%20Cloud%20Models%20Improve%20Visual%20Robustness%20in%20Robotic%20Learners&entry.906535625=Skand%20Peri%20and%20Iain%20Lee%20and%20Chanho%20Kim%20and%20Li%20Fuxin%20and%20Tucker%20Hermans%20and%20Stefan%20Lee&entry.1292438233=%20%20Visual%20control%20policies%20can%20encounter%20significant%20performance%20degradation%0Awhen%20visual%20conditions%20like%20lighting%20or%20camera%20position%20differ%20from%20those%20seen%0Aduring%20training%20--%20often%20exhibiting%20sharp%20declines%20in%20capability%20even%20for%20minor%0Adifferences.%20In%20this%20work%2C%20we%20examine%20robustness%20to%20a%20suite%20of%20these%20types%20of%0Avisual%20changes%20for%20RGB-D%20and%20point%20cloud%20based%20visual%20control%20policies.%20To%0Aperform%20these%20experiments%20on%20both%20model-free%20and%20model-based%20reinforcement%0Alearners%2C%20we%20introduce%20a%20novel%20Point%20Cloud%20World%20Model%20%28PCWM%29%20and%20point%20cloud%0Abased%20control%20policies.%20Our%20experiments%20show%20that%20policies%20that%20explicitly%0Aencode%20point%20clouds%20are%20significantly%20more%20robust%20than%20their%20RGB-D%0Acounterparts.%20Further%2C%20we%20find%20our%20proposed%20PCWM%20significantly%20outperforms%0Aprior%20works%20in%20terms%20of%20sample%20efficiency%20during%20training.%20Taken%20together%2C%0Athese%20results%20suggest%20reasoning%20about%20the%203D%20scene%20through%20point%20clouds%20can%0Aimprove%20performance%2C%20reduce%20learning%20time%2C%20and%20increase%20robustness%20for%20robotic%0Alearners.%20Project%20Webpage%3A%20https%3A//pvskand.github.io/projects/PCWM%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18926v1&entry.124074799=Read"},
{"title": "MiPa: Mixed Patch Infrared-Visible Modality Agnostic Object Detection", "author": "Heitor R. Medeiros and David Latortue and Fidel Guerrero Pena and Eric Granger and Marco Pedersoli", "abstract": "  In this paper, we present a different way to use two modalities, in which\neither one modality or the other is seen by a single model. This can be useful\nwhen adapting an unimodal model to leverage more information while respecting a\nlimited computational budget. This would mean having a single model that is\nable to deal with any modalities. To describe this, we coined the term anymodal\nlearning. An example of this, is a use case where, surveillance in a room when\nthe lights are off would be much more valuable using an infrared modality while\na visible one would provide more discriminative information when lights are on.\nThis work investigates how to efficiently leverage visible and infrared/thermal\nmodalities for transformer-based object detection backbone to create an\nanymodal architecture. Our work does not create any inference overhead during\nthe testing while exploring an effective way to exploit the two modalities\nduring the training. To accomplish such a task, we introduce the novel anymodal\ntraining technique: Mixed Patches (MiPa), in conjunction with a patch-wise\ndomain agnostic module, which is responsible of learning the best way to find a\ncommon representation of both modalities. This approach proves to be able to\nbalance modalities by reaching competitive results on individual modality\nbenchmarks with the alternative of using an unimodal architecture on three\ndifferent visible-infrared object detection datasets. Finally, our proposed\nmethod, when used as a regularization for the strongest modality, can beat the\nperformance of multimodal fusion methods while only requiring a single modality\nduring inference. Notably, MiPa became the state-of-the-art on the LLVIP\nvisible/infrared benchmark. Code: https://github.com/heitorrapela/MiPa\n", "link": "http://arxiv.org/abs/2404.18849v1", "date": "2024-04-29", "relevancy": 1.7116, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5881}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5789}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5321}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MiPa%3A%20Mixed%20Patch%20Infrared-Visible%20Modality%20Agnostic%20Object%20Detection&body=Title%3A%20MiPa%3A%20Mixed%20Patch%20Infrared-Visible%20Modality%20Agnostic%20Object%20Detection%0AAuthor%3A%20Heitor%20R.%20Medeiros%20and%20David%20Latortue%20and%20Fidel%20Guerrero%20Pena%20and%20Eric%20Granger%20and%20Marco%20Pedersoli%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20a%20different%20way%20to%20use%20two%20modalities%2C%20in%20which%0Aeither%20one%20modality%20or%20the%20other%20is%20seen%20by%20a%20single%20model.%20This%20can%20be%20useful%0Awhen%20adapting%20an%20unimodal%20model%20to%20leverage%20more%20information%20while%20respecting%20a%0Alimited%20computational%20budget.%20This%20would%20mean%20having%20a%20single%20model%20that%20is%0Aable%20to%20deal%20with%20any%20modalities.%20To%20describe%20this%2C%20we%20coined%20the%20term%20anymodal%0Alearning.%20An%20example%20of%20this%2C%20is%20a%20use%20case%20where%2C%20surveillance%20in%20a%20room%20when%0Athe%20lights%20are%20off%20would%20be%20much%20more%20valuable%20using%20an%20infrared%20modality%20while%0Aa%20visible%20one%20would%20provide%20more%20discriminative%20information%20when%20lights%20are%20on.%0AThis%20work%20investigates%20how%20to%20efficiently%20leverage%20visible%20and%20infrared/thermal%0Amodalities%20for%20transformer-based%20object%20detection%20backbone%20to%20create%20an%0Aanymodal%20architecture.%20Our%20work%20does%20not%20create%20any%20inference%20overhead%20during%0Athe%20testing%20while%20exploring%20an%20effective%20way%20to%20exploit%20the%20two%20modalities%0Aduring%20the%20training.%20To%20accomplish%20such%20a%20task%2C%20we%20introduce%20the%20novel%20anymodal%0Atraining%20technique%3A%20Mixed%20Patches%20%28MiPa%29%2C%20in%20conjunction%20with%20a%20patch-wise%0Adomain%20agnostic%20module%2C%20which%20is%20responsible%20of%20learning%20the%20best%20way%20to%20find%20a%0Acommon%20representation%20of%20both%20modalities.%20This%20approach%20proves%20to%20be%20able%20to%0Abalance%20modalities%20by%20reaching%20competitive%20results%20on%20individual%20modality%0Abenchmarks%20with%20the%20alternative%20of%20using%20an%20unimodal%20architecture%20on%20three%0Adifferent%20visible-infrared%20object%20detection%20datasets.%20Finally%2C%20our%20proposed%0Amethod%2C%20when%20used%20as%20a%20regularization%20for%20the%20strongest%20modality%2C%20can%20beat%20the%0Aperformance%20of%20multimodal%20fusion%20methods%20while%20only%20requiring%20a%20single%20modality%0Aduring%20inference.%20Notably%2C%20MiPa%20became%20the%20state-of-the-art%20on%20the%20LLVIP%0Avisible/infrared%20benchmark.%20Code%3A%20https%3A//github.com/heitorrapela/MiPa%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18849v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MiPa%3A%20Mixed%20Patch%20Infrared-Visible%20Modality%20Agnostic%20Object%20Detection&entry.906535625=Heitor%20R.%20Medeiros%20and%20David%20Latortue%20and%20Fidel%20Guerrero%20Pena%20and%20Eric%20Granger%20and%20Marco%20Pedersoli&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20a%20different%20way%20to%20use%20two%20modalities%2C%20in%20which%0Aeither%20one%20modality%20or%20the%20other%20is%20seen%20by%20a%20single%20model.%20This%20can%20be%20useful%0Awhen%20adapting%20an%20unimodal%20model%20to%20leverage%20more%20information%20while%20respecting%20a%0Alimited%20computational%20budget.%20This%20would%20mean%20having%20a%20single%20model%20that%20is%0Aable%20to%20deal%20with%20any%20modalities.%20To%20describe%20this%2C%20we%20coined%20the%20term%20anymodal%0Alearning.%20An%20example%20of%20this%2C%20is%20a%20use%20case%20where%2C%20surveillance%20in%20a%20room%20when%0Athe%20lights%20are%20off%20would%20be%20much%20more%20valuable%20using%20an%20infrared%20modality%20while%0Aa%20visible%20one%20would%20provide%20more%20discriminative%20information%20when%20lights%20are%20on.%0AThis%20work%20investigates%20how%20to%20efficiently%20leverage%20visible%20and%20infrared/thermal%0Amodalities%20for%20transformer-based%20object%20detection%20backbone%20to%20create%20an%0Aanymodal%20architecture.%20Our%20work%20does%20not%20create%20any%20inference%20overhead%20during%0Athe%20testing%20while%20exploring%20an%20effective%20way%20to%20exploit%20the%20two%20modalities%0Aduring%20the%20training.%20To%20accomplish%20such%20a%20task%2C%20we%20introduce%20the%20novel%20anymodal%0Atraining%20technique%3A%20Mixed%20Patches%20%28MiPa%29%2C%20in%20conjunction%20with%20a%20patch-wise%0Adomain%20agnostic%20module%2C%20which%20is%20responsible%20of%20learning%20the%20best%20way%20to%20find%20a%0Acommon%20representation%20of%20both%20modalities.%20This%20approach%20proves%20to%20be%20able%20to%0Abalance%20modalities%20by%20reaching%20competitive%20results%20on%20individual%20modality%0Abenchmarks%20with%20the%20alternative%20of%20using%20an%20unimodal%20architecture%20on%20three%0Adifferent%20visible-infrared%20object%20detection%20datasets.%20Finally%2C%20our%20proposed%0Amethod%2C%20when%20used%20as%20a%20regularization%20for%20the%20strongest%20modality%2C%20can%20beat%20the%0Aperformance%20of%20multimodal%20fusion%20methods%20while%20only%20requiring%20a%20single%20modality%0Aduring%20inference.%20Notably%2C%20MiPa%20became%20the%20state-of-the-art%20on%20the%20LLVIP%0Avisible/infrared%20benchmark.%20Code%3A%20https%3A//github.com/heitorrapela/MiPa%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18849v1&entry.124074799=Read"},
{"title": "Modeling Multimodal Social Interactions: New Challenges and Baselines\n  with Densely Aligned Representations", "author": "Sangmin Lee and Bolin Lai and Fiona Ryan and Bikram Boote and James M. Rehg", "abstract": "  Understanding social interactions involving both verbal and non-verbal cues\nis essential for effectively interpreting social situations. However, most\nprior works on multimodal social cues focus predominantly on single-person\nbehaviors or rely on holistic visual representations that are not aligned to\nutterances in multi-party environments. Consequently, they are limited in\nmodeling the intricate dynamics of multi-party interactions. In this paper, we\nintroduce three new challenging tasks to model the fine-grained dynamics\nbetween multiple people: speaking target identification, pronoun coreference\nresolution, and mentioned player prediction. We contribute extensive data\nannotations to curate these new challenges in social deduction game settings.\nFurthermore, we propose a novel multimodal baseline that leverages densely\naligned language-visual representations by synchronizing visual features with\ntheir corresponding utterances. This facilitates concurrently capturing verbal\nand non-verbal cues pertinent to social reasoning. Experiments demonstrate the\neffectiveness of the proposed approach with densely aligned multimodal\nrepresentations in modeling fine-grained social interactions. Project website:\nhttps://sangmin-git.github.io/projects/MMSI.\n", "link": "http://arxiv.org/abs/2403.02090v3", "date": "2024-04-29", "relevancy": 1.7081, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5843}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5764}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5368}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Modeling%20Multimodal%20Social%20Interactions%3A%20New%20Challenges%20and%20Baselines%0A%20%20with%20Densely%20Aligned%20Representations&body=Title%3A%20Modeling%20Multimodal%20Social%20Interactions%3A%20New%20Challenges%20and%20Baselines%0A%20%20with%20Densely%20Aligned%20Representations%0AAuthor%3A%20Sangmin%20Lee%20and%20Bolin%20Lai%20and%20Fiona%20Ryan%20and%20Bikram%20Boote%20and%20James%20M.%20Rehg%0AAbstract%3A%20%20%20Understanding%20social%20interactions%20involving%20both%20verbal%20and%20non-verbal%20cues%0Ais%20essential%20for%20effectively%20interpreting%20social%20situations.%20However%2C%20most%0Aprior%20works%20on%20multimodal%20social%20cues%20focus%20predominantly%20on%20single-person%0Abehaviors%20or%20rely%20on%20holistic%20visual%20representations%20that%20are%20not%20aligned%20to%0Autterances%20in%20multi-party%20environments.%20Consequently%2C%20they%20are%20limited%20in%0Amodeling%20the%20intricate%20dynamics%20of%20multi-party%20interactions.%20In%20this%20paper%2C%20we%0Aintroduce%20three%20new%20challenging%20tasks%20to%20model%20the%20fine-grained%20dynamics%0Abetween%20multiple%20people%3A%20speaking%20target%20identification%2C%20pronoun%20coreference%0Aresolution%2C%20and%20mentioned%20player%20prediction.%20We%20contribute%20extensive%20data%0Aannotations%20to%20curate%20these%20new%20challenges%20in%20social%20deduction%20game%20settings.%0AFurthermore%2C%20we%20propose%20a%20novel%20multimodal%20baseline%20that%20leverages%20densely%0Aaligned%20language-visual%20representations%20by%20synchronizing%20visual%20features%20with%0Atheir%20corresponding%20utterances.%20This%20facilitates%20concurrently%20capturing%20verbal%0Aand%20non-verbal%20cues%20pertinent%20to%20social%20reasoning.%20Experiments%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20approach%20with%20densely%20aligned%20multimodal%0Arepresentations%20in%20modeling%20fine-grained%20social%20interactions.%20Project%20website%3A%0Ahttps%3A//sangmin-git.github.io/projects/MMSI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.02090v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modeling%20Multimodal%20Social%20Interactions%3A%20New%20Challenges%20and%20Baselines%0A%20%20with%20Densely%20Aligned%20Representations&entry.906535625=Sangmin%20Lee%20and%20Bolin%20Lai%20and%20Fiona%20Ryan%20and%20Bikram%20Boote%20and%20James%20M.%20Rehg&entry.1292438233=%20%20Understanding%20social%20interactions%20involving%20both%20verbal%20and%20non-verbal%20cues%0Ais%20essential%20for%20effectively%20interpreting%20social%20situations.%20However%2C%20most%0Aprior%20works%20on%20multimodal%20social%20cues%20focus%20predominantly%20on%20single-person%0Abehaviors%20or%20rely%20on%20holistic%20visual%20representations%20that%20are%20not%20aligned%20to%0Autterances%20in%20multi-party%20environments.%20Consequently%2C%20they%20are%20limited%20in%0Amodeling%20the%20intricate%20dynamics%20of%20multi-party%20interactions.%20In%20this%20paper%2C%20we%0Aintroduce%20three%20new%20challenging%20tasks%20to%20model%20the%20fine-grained%20dynamics%0Abetween%20multiple%20people%3A%20speaking%20target%20identification%2C%20pronoun%20coreference%0Aresolution%2C%20and%20mentioned%20player%20prediction.%20We%20contribute%20extensive%20data%0Aannotations%20to%20curate%20these%20new%20challenges%20in%20social%20deduction%20game%20settings.%0AFurthermore%2C%20we%20propose%20a%20novel%20multimodal%20baseline%20that%20leverages%20densely%0Aaligned%20language-visual%20representations%20by%20synchronizing%20visual%20features%20with%0Atheir%20corresponding%20utterances.%20This%20facilitates%20concurrently%20capturing%20verbal%0Aand%20non-verbal%20cues%20pertinent%20to%20social%20reasoning.%20Experiments%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20approach%20with%20densely%20aligned%20multimodal%0Arepresentations%20in%20modeling%20fine-grained%20social%20interactions.%20Project%20website%3A%0Ahttps%3A//sangmin-git.github.io/projects/MMSI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02090v3&entry.124074799=Read"},
{"title": "FALE: Fairness-Aware ALE Plots for Auditing Bias in Subgroups", "author": "Giorgos Giannopoulos and Dimitris Sacharidis and Nikolas Theologitis and Loukas Kavouras and Ioannis Emiris", "abstract": "  Fairness is steadily becoming a crucial requirement of Machine Learning (ML)\nsystems. A particularly important notion is subgroup fairness, i.e., fairness\nin subgroups of individuals that are defined by more than one attributes.\nIdentifying bias in subgroups can become both computationally challenging, as\nwell as problematic with respect to comprehensibility and intuitiveness of the\nfinding to end users. In this work we focus on the latter aspects; we propose\nan explainability method tailored to identifying potential bias in subgroups\nand visualizing the findings in a user friendly manner to end users. In\nparticular, we extend the ALE plots explainability method, proposing FALE\n(Fairness aware Accumulated Local Effects) plots, a method for measuring the\nchange in fairness for an affected population corresponding to different values\nof a feature (attribute). We envision FALE to function as an efficient, user\nfriendly, comprehensible and reliable first-stage tool for identifying\nsubgroups with potential bias issues.\n", "link": "http://arxiv.org/abs/2404.18685v1", "date": "2024-04-29", "relevancy": 1.7046, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4347}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4203}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4199}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FALE%3A%20Fairness-Aware%20ALE%20Plots%20for%20Auditing%20Bias%20in%20Subgroups&body=Title%3A%20FALE%3A%20Fairness-Aware%20ALE%20Plots%20for%20Auditing%20Bias%20in%20Subgroups%0AAuthor%3A%20Giorgos%20Giannopoulos%20and%20Dimitris%20Sacharidis%20and%20Nikolas%20Theologitis%20and%20Loukas%20Kavouras%20and%20Ioannis%20Emiris%0AAbstract%3A%20%20%20Fairness%20is%20steadily%20becoming%20a%20crucial%20requirement%20of%20Machine%20Learning%20%28ML%29%0Asystems.%20A%20particularly%20important%20notion%20is%20subgroup%20fairness%2C%20i.e.%2C%20fairness%0Ain%20subgroups%20of%20individuals%20that%20are%20defined%20by%20more%20than%20one%20attributes.%0AIdentifying%20bias%20in%20subgroups%20can%20become%20both%20computationally%20challenging%2C%20as%0Awell%20as%20problematic%20with%20respect%20to%20comprehensibility%20and%20intuitiveness%20of%20the%0Afinding%20to%20end%20users.%20In%20this%20work%20we%20focus%20on%20the%20latter%20aspects%3B%20we%20propose%0Aan%20explainability%20method%20tailored%20to%20identifying%20potential%20bias%20in%20subgroups%0Aand%20visualizing%20the%20findings%20in%20a%20user%20friendly%20manner%20to%20end%20users.%20In%0Aparticular%2C%20we%20extend%20the%20ALE%20plots%20explainability%20method%2C%20proposing%20FALE%0A%28Fairness%20aware%20Accumulated%20Local%20Effects%29%20plots%2C%20a%20method%20for%20measuring%20the%0Achange%20in%20fairness%20for%20an%20affected%20population%20corresponding%20to%20different%20values%0Aof%20a%20feature%20%28attribute%29.%20We%20envision%20FALE%20to%20function%20as%20an%20efficient%2C%20user%0Afriendly%2C%20comprehensible%20and%20reliable%20first-stage%20tool%20for%20identifying%0Asubgroups%20with%20potential%20bias%20issues.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18685v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FALE%3A%20Fairness-Aware%20ALE%20Plots%20for%20Auditing%20Bias%20in%20Subgroups&entry.906535625=Giorgos%20Giannopoulos%20and%20Dimitris%20Sacharidis%20and%20Nikolas%20Theologitis%20and%20Loukas%20Kavouras%20and%20Ioannis%20Emiris&entry.1292438233=%20%20Fairness%20is%20steadily%20becoming%20a%20crucial%20requirement%20of%20Machine%20Learning%20%28ML%29%0Asystems.%20A%20particularly%20important%20notion%20is%20subgroup%20fairness%2C%20i.e.%2C%20fairness%0Ain%20subgroups%20of%20individuals%20that%20are%20defined%20by%20more%20than%20one%20attributes.%0AIdentifying%20bias%20in%20subgroups%20can%20become%20both%20computationally%20challenging%2C%20as%0Awell%20as%20problematic%20with%20respect%20to%20comprehensibility%20and%20intuitiveness%20of%20the%0Afinding%20to%20end%20users.%20In%20this%20work%20we%20focus%20on%20the%20latter%20aspects%3B%20we%20propose%0Aan%20explainability%20method%20tailored%20to%20identifying%20potential%20bias%20in%20subgroups%0Aand%20visualizing%20the%20findings%20in%20a%20user%20friendly%20manner%20to%20end%20users.%20In%0Aparticular%2C%20we%20extend%20the%20ALE%20plots%20explainability%20method%2C%20proposing%20FALE%0A%28Fairness%20aware%20Accumulated%20Local%20Effects%29%20plots%2C%20a%20method%20for%20measuring%20the%0Achange%20in%20fairness%20for%20an%20affected%20population%20corresponding%20to%20different%20values%0Aof%20a%20feature%20%28attribute%29.%20We%20envision%20FALE%20to%20function%20as%20an%20efficient%2C%20user%0Afriendly%2C%20comprehensible%20and%20reliable%20first-stage%20tool%20for%20identifying%0Asubgroups%20with%20potential%20bias%20issues.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18685v1&entry.124074799=Read"},
{"title": "MuseumMaker: Continual Style Customization without Catastrophic\n  Forgetting", "author": "Chenxi Liu and Gan Sun and Wenqi Liang and Jiahua Dong and Can Qin and Yang Cong", "abstract": "  Pre-trained large text-to-image (T2I) models with an appropriate text prompt\nhas attracted growing interests in customized images generation field. However,\ncatastrophic forgetting issue make it hard to continually synthesize new\nuser-provided styles while retaining the satisfying results amongst learned\nstyles. In this paper, we propose MuseumMaker, a method that enables the\nsynthesis of images by following a set of customized styles in a never-end\nmanner, and gradually accumulate these creative artistic works as a Museum.\nWhen facing with a new customization style, we develop a style distillation\nloss module to extract and learn the styles of the training data for new image\ngeneration. It can minimize the learning biases caused by content of new\ntraining images, and address the catastrophic overfitting issue induced by\nfew-shot images. To deal with catastrophic forgetting amongst past learned\nstyles, we devise a dual regularization for shared-LoRA module to optimize the\ndirection of model update, which could regularize the diffusion model from both\nweight and feature aspects, respectively. Meanwhile, to further preserve\nhistorical knowledge from past styles and address the limited representability\nof LoRA, we consider a task-wise token learning module where a unique token\nembedding is learned to denote a new style. As any new user-provided style\ncome, our MuseumMaker can capture the nuances of the new styles while\nmaintaining the details of learned styles. Experimental results on diverse\nstyle datasets validate the effectiveness of our proposed MuseumMaker method,\nshowcasing its robustness and versatility across various scenarios.\n", "link": "http://arxiv.org/abs/2404.16612v2", "date": "2024-04-29", "relevancy": 1.6916, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5989}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.582}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5426}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MuseumMaker%3A%20Continual%20Style%20Customization%20without%20Catastrophic%0A%20%20Forgetting&body=Title%3A%20MuseumMaker%3A%20Continual%20Style%20Customization%20without%20Catastrophic%0A%20%20Forgetting%0AAuthor%3A%20Chenxi%20Liu%20and%20Gan%20Sun%20and%20Wenqi%20Liang%20and%20Jiahua%20Dong%20and%20Can%20Qin%20and%20Yang%20Cong%0AAbstract%3A%20%20%20Pre-trained%20large%20text-to-image%20%28T2I%29%20models%20with%20an%20appropriate%20text%20prompt%0Ahas%20attracted%20growing%20interests%20in%20customized%20images%20generation%20field.%20However%2C%0Acatastrophic%20forgetting%20issue%20make%20it%20hard%20to%20continually%20synthesize%20new%0Auser-provided%20styles%20while%20retaining%20the%20satisfying%20results%20amongst%20learned%0Astyles.%20In%20this%20paper%2C%20we%20propose%20MuseumMaker%2C%20a%20method%20that%20enables%20the%0Asynthesis%20of%20images%20by%20following%20a%20set%20of%20customized%20styles%20in%20a%20never-end%0Amanner%2C%20and%20gradually%20accumulate%20these%20creative%20artistic%20works%20as%20a%20Museum.%0AWhen%20facing%20with%20a%20new%20customization%20style%2C%20we%20develop%20a%20style%20distillation%0Aloss%20module%20to%20extract%20and%20learn%20the%20styles%20of%20the%20training%20data%20for%20new%20image%0Ageneration.%20It%20can%20minimize%20the%20learning%20biases%20caused%20by%20content%20of%20new%0Atraining%20images%2C%20and%20address%20the%20catastrophic%20overfitting%20issue%20induced%20by%0Afew-shot%20images.%20To%20deal%20with%20catastrophic%20forgetting%20amongst%20past%20learned%0Astyles%2C%20we%20devise%20a%20dual%20regularization%20for%20shared-LoRA%20module%20to%20optimize%20the%0Adirection%20of%20model%20update%2C%20which%20could%20regularize%20the%20diffusion%20model%20from%20both%0Aweight%20and%20feature%20aspects%2C%20respectively.%20Meanwhile%2C%20to%20further%20preserve%0Ahistorical%20knowledge%20from%20past%20styles%20and%20address%20the%20limited%20representability%0Aof%20LoRA%2C%20we%20consider%20a%20task-wise%20token%20learning%20module%20where%20a%20unique%20token%0Aembedding%20is%20learned%20to%20denote%20a%20new%20style.%20As%20any%20new%20user-provided%20style%0Acome%2C%20our%20MuseumMaker%20can%20capture%20the%20nuances%20of%20the%20new%20styles%20while%0Amaintaining%20the%20details%20of%20learned%20styles.%20Experimental%20results%20on%20diverse%0Astyle%20datasets%20validate%20the%20effectiveness%20of%20our%20proposed%20MuseumMaker%20method%2C%0Ashowcasing%20its%20robustness%20and%20versatility%20across%20various%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.16612v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MuseumMaker%3A%20Continual%20Style%20Customization%20without%20Catastrophic%0A%20%20Forgetting&entry.906535625=Chenxi%20Liu%20and%20Gan%20Sun%20and%20Wenqi%20Liang%20and%20Jiahua%20Dong%20and%20Can%20Qin%20and%20Yang%20Cong&entry.1292438233=%20%20Pre-trained%20large%20text-to-image%20%28T2I%29%20models%20with%20an%20appropriate%20text%20prompt%0Ahas%20attracted%20growing%20interests%20in%20customized%20images%20generation%20field.%20However%2C%0Acatastrophic%20forgetting%20issue%20make%20it%20hard%20to%20continually%20synthesize%20new%0Auser-provided%20styles%20while%20retaining%20the%20satisfying%20results%20amongst%20learned%0Astyles.%20In%20this%20paper%2C%20we%20propose%20MuseumMaker%2C%20a%20method%20that%20enables%20the%0Asynthesis%20of%20images%20by%20following%20a%20set%20of%20customized%20styles%20in%20a%20never-end%0Amanner%2C%20and%20gradually%20accumulate%20these%20creative%20artistic%20works%20as%20a%20Museum.%0AWhen%20facing%20with%20a%20new%20customization%20style%2C%20we%20develop%20a%20style%20distillation%0Aloss%20module%20to%20extract%20and%20learn%20the%20styles%20of%20the%20training%20data%20for%20new%20image%0Ageneration.%20It%20can%20minimize%20the%20learning%20biases%20caused%20by%20content%20of%20new%0Atraining%20images%2C%20and%20address%20the%20catastrophic%20overfitting%20issue%20induced%20by%0Afew-shot%20images.%20To%20deal%20with%20catastrophic%20forgetting%20amongst%20past%20learned%0Astyles%2C%20we%20devise%20a%20dual%20regularization%20for%20shared-LoRA%20module%20to%20optimize%20the%0Adirection%20of%20model%20update%2C%20which%20could%20regularize%20the%20diffusion%20model%20from%20both%0Aweight%20and%20feature%20aspects%2C%20respectively.%20Meanwhile%2C%20to%20further%20preserve%0Ahistorical%20knowledge%20from%20past%20styles%20and%20address%20the%20limited%20representability%0Aof%20LoRA%2C%20we%20consider%20a%20task-wise%20token%20learning%20module%20where%20a%20unique%20token%0Aembedding%20is%20learned%20to%20denote%20a%20new%20style.%20As%20any%20new%20user-provided%20style%0Acome%2C%20our%20MuseumMaker%20can%20capture%20the%20nuances%20of%20the%20new%20styles%20while%0Amaintaining%20the%20details%20of%20learned%20styles.%20Experimental%20results%20on%20diverse%0Astyle%20datasets%20validate%20the%20effectiveness%20of%20our%20proposed%20MuseumMaker%20method%2C%0Ashowcasing%20its%20robustness%20and%20versatility%20across%20various%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.16612v2&entry.124074799=Read"},
{"title": "Towards Extreme Image Compression with Latent Feature Guidance and\n  Diffusion Prior", "author": "Zhiyuan Li and Yanhui Zhou and Hao Wei and Chenyang Ge and Jingwen Jiang", "abstract": "  Compressing images at extremely low bitrates (below 0.1 bits per pixel (bpp))\nis a significant challenge due to substantial information loss. Existing\nextreme image compression methods generally suffer from heavy compression\nartifacts or low-fidelity reconstructions. To address this problem, we propose\na novel extreme image compression framework that combines compressive VAEs and\npre-trained text-to-image diffusion models in an end-to-end manner.\nSpecifically, we introduce a latent feature-guided compression module based on\ncompressive VAEs. This module compresses images and initially decodes the\ncompressed information into content variables. To enhance the alignment between\ncontent variables and the diffusion space, we introduce external guidance to\nmodulate intermediate feature maps. Subsequently, we develop a conditional\ndiffusion decoding module that leverages pre-trained diffusion models to\nfurther decode these content variables. To preserve the generative capability\nof pre-trained diffusion models, we keep their parameters fixed and use a\ncontrol module to inject content information. We also design a space alignment\nloss to provide sufficient constraints for the latent feature-guided\ncompression module. Extensive experiments demonstrate that our method\noutperforms state-of-the-art approaches in terms of both visual performance and\nimage fidelity at extremely low bitrates.\n", "link": "http://arxiv.org/abs/2404.18820v1", "date": "2024-04-29", "relevancy": 1.2478, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6841}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6042}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5834}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Extreme%20Image%20Compression%20with%20Latent%20Feature%20Guidance%20and%0A%20%20Diffusion%20Prior&body=Title%3A%20Towards%20Extreme%20Image%20Compression%20with%20Latent%20Feature%20Guidance%20and%0A%20%20Diffusion%20Prior%0AAuthor%3A%20Zhiyuan%20Li%20and%20Yanhui%20Zhou%20and%20Hao%20Wei%20and%20Chenyang%20Ge%20and%20Jingwen%20Jiang%0AAbstract%3A%20%20%20Compressing%20images%20at%20extremely%20low%20bitrates%20%28below%200.1%20bits%20per%20pixel%20%28bpp%29%29%0Ais%20a%20significant%20challenge%20due%20to%20substantial%20information%20loss.%20Existing%0Aextreme%20image%20compression%20methods%20generally%20suffer%20from%20heavy%20compression%0Aartifacts%20or%20low-fidelity%20reconstructions.%20To%20address%20this%20problem%2C%20we%20propose%0Aa%20novel%20extreme%20image%20compression%20framework%20that%20combines%20compressive%20VAEs%20and%0Apre-trained%20text-to-image%20diffusion%20models%20in%20an%20end-to-end%20manner.%0ASpecifically%2C%20we%20introduce%20a%20latent%20feature-guided%20compression%20module%20based%20on%0Acompressive%20VAEs.%20This%20module%20compresses%20images%20and%20initially%20decodes%20the%0Acompressed%20information%20into%20content%20variables.%20To%20enhance%20the%20alignment%20between%0Acontent%20variables%20and%20the%20diffusion%20space%2C%20we%20introduce%20external%20guidance%20to%0Amodulate%20intermediate%20feature%20maps.%20Subsequently%2C%20we%20develop%20a%20conditional%0Adiffusion%20decoding%20module%20that%20leverages%20pre-trained%20diffusion%20models%20to%0Afurther%20decode%20these%20content%20variables.%20To%20preserve%20the%20generative%20capability%0Aof%20pre-trained%20diffusion%20models%2C%20we%20keep%20their%20parameters%20fixed%20and%20use%20a%0Acontrol%20module%20to%20inject%20content%20information.%20We%20also%20design%20a%20space%20alignment%0Aloss%20to%20provide%20sufficient%20constraints%20for%20the%20latent%20feature-guided%0Acompression%20module.%20Extensive%20experiments%20demonstrate%20that%20our%20method%0Aoutperforms%20state-of-the-art%20approaches%20in%20terms%20of%20both%20visual%20performance%20and%0Aimage%20fidelity%20at%20extremely%20low%20bitrates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18820v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Extreme%20Image%20Compression%20with%20Latent%20Feature%20Guidance%20and%0A%20%20Diffusion%20Prior&entry.906535625=Zhiyuan%20Li%20and%20Yanhui%20Zhou%20and%20Hao%20Wei%20and%20Chenyang%20Ge%20and%20Jingwen%20Jiang&entry.1292438233=%20%20Compressing%20images%20at%20extremely%20low%20bitrates%20%28below%200.1%20bits%20per%20pixel%20%28bpp%29%29%0Ais%20a%20significant%20challenge%20due%20to%20substantial%20information%20loss.%20Existing%0Aextreme%20image%20compression%20methods%20generally%20suffer%20from%20heavy%20compression%0Aartifacts%20or%20low-fidelity%20reconstructions.%20To%20address%20this%20problem%2C%20we%20propose%0Aa%20novel%20extreme%20image%20compression%20framework%20that%20combines%20compressive%20VAEs%20and%0Apre-trained%20text-to-image%20diffusion%20models%20in%20an%20end-to-end%20manner.%0ASpecifically%2C%20we%20introduce%20a%20latent%20feature-guided%20compression%20module%20based%20on%0Acompressive%20VAEs.%20This%20module%20compresses%20images%20and%20initially%20decodes%20the%0Acompressed%20information%20into%20content%20variables.%20To%20enhance%20the%20alignment%20between%0Acontent%20variables%20and%20the%20diffusion%20space%2C%20we%20introduce%20external%20guidance%20to%0Amodulate%20intermediate%20feature%20maps.%20Subsequently%2C%20we%20develop%20a%20conditional%0Adiffusion%20decoding%20module%20that%20leverages%20pre-trained%20diffusion%20models%20to%0Afurther%20decode%20these%20content%20variables.%20To%20preserve%20the%20generative%20capability%0Aof%20pre-trained%20diffusion%20models%2C%20we%20keep%20their%20parameters%20fixed%20and%20use%20a%0Acontrol%20module%20to%20inject%20content%20information.%20We%20also%20design%20a%20space%20alignment%0Aloss%20to%20provide%20sufficient%20constraints%20for%20the%20latent%20feature-guided%0Acompression%20module.%20Extensive%20experiments%20demonstrate%20that%20our%20method%0Aoutperforms%20state-of-the-art%20approaches%20in%20terms%20of%20both%20visual%20performance%20and%0Aimage%20fidelity%20at%20extremely%20low%20bitrates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18820v1&entry.124074799=Read"},
{"title": "Comparing Z3 and A3 PKM Heads: Which Is Superior and Why?", "author": "Hassen Nigatu", "abstract": "  This study presents a comparison between the Sprint Z3 and A3 head parallel\nkinematics machines, distinguished by their joint sequence. The analysis\nfocuses on performance attributes critical for precision machining\nspecifically, parasitic motion, workspace capability, stiffness performance\nover the independent and parasitic spaces, and condition number distribution.\nAlthough these machines are extensively utilized in precision machining for the\naerospace and automotive industries, a definitive superior choice has not been\nidentified for machining large components. Moreover, the distribution of\nstiffness across the configuration of parasitic space has not previously been\naddressed for either mechanism. This research reveals that despite identical\nparameters used and exhibiting similar parasitic motions, the Sprint Z3\ndemonstrates superior stiffness, workspace volume, and condition number\ndistribution. This performance advantage is attributed to variations in joint\nand link sequence, which enhance deflection resilience, crucial for\nmanufacturing large-scale components. This also results in a higher condition\nnumber and a larger workspace. The result highlights the importance of design\narchitecture in the efficacy of parallel kinematics machines and suggest\n", "link": "http://arxiv.org/abs/2404.18575v1", "date": "2024-04-29", "relevancy": 1.4769, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.375}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3678}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.364}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Comparing%20Z3%20and%20A3%20PKM%20Heads%3A%20Which%20Is%20Superior%20and%20Why%3F&body=Title%3A%20Comparing%20Z3%20and%20A3%20PKM%20Heads%3A%20Which%20Is%20Superior%20and%20Why%3F%0AAuthor%3A%20Hassen%20Nigatu%0AAbstract%3A%20%20%20This%20study%20presents%20a%20comparison%20between%20the%20Sprint%20Z3%20and%20A3%20head%20parallel%0Akinematics%20machines%2C%20distinguished%20by%20their%20joint%20sequence.%20The%20analysis%0Afocuses%20on%20performance%20attributes%20critical%20for%20precision%20machining%0Aspecifically%2C%20parasitic%20motion%2C%20workspace%20capability%2C%20stiffness%20performance%0Aover%20the%20independent%20and%20parasitic%20spaces%2C%20and%20condition%20number%20distribution.%0AAlthough%20these%20machines%20are%20extensively%20utilized%20in%20precision%20machining%20for%20the%0Aaerospace%20and%20automotive%20industries%2C%20a%20definitive%20superior%20choice%20has%20not%20been%0Aidentified%20for%20machining%20large%20components.%20Moreover%2C%20the%20distribution%20of%0Astiffness%20across%20the%20configuration%20of%20parasitic%20space%20has%20not%20previously%20been%0Aaddressed%20for%20either%20mechanism.%20This%20research%20reveals%20that%20despite%20identical%0Aparameters%20used%20and%20exhibiting%20similar%20parasitic%20motions%2C%20the%20Sprint%20Z3%0Ademonstrates%20superior%20stiffness%2C%20workspace%20volume%2C%20and%20condition%20number%0Adistribution.%20This%20performance%20advantage%20is%20attributed%20to%20variations%20in%20joint%0Aand%20link%20sequence%2C%20which%20enhance%20deflection%20resilience%2C%20crucial%20for%0Amanufacturing%20large-scale%20components.%20This%20also%20results%20in%20a%20higher%20condition%0Anumber%20and%20a%20larger%20workspace.%20The%20result%20highlights%20the%20importance%20of%20design%0Aarchitecture%20in%20the%20efficacy%20of%20parallel%20kinematics%20machines%20and%20suggest%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18575v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparing%20Z3%20and%20A3%20PKM%20Heads%3A%20Which%20Is%20Superior%20and%20Why%3F&entry.906535625=Hassen%20Nigatu&entry.1292438233=%20%20This%20study%20presents%20a%20comparison%20between%20the%20Sprint%20Z3%20and%20A3%20head%20parallel%0Akinematics%20machines%2C%20distinguished%20by%20their%20joint%20sequence.%20The%20analysis%0Afocuses%20on%20performance%20attributes%20critical%20for%20precision%20machining%0Aspecifically%2C%20parasitic%20motion%2C%20workspace%20capability%2C%20stiffness%20performance%0Aover%20the%20independent%20and%20parasitic%20spaces%2C%20and%20condition%20number%20distribution.%0AAlthough%20these%20machines%20are%20extensively%20utilized%20in%20precision%20machining%20for%20the%0Aaerospace%20and%20automotive%20industries%2C%20a%20definitive%20superior%20choice%20has%20not%20been%0Aidentified%20for%20machining%20large%20components.%20Moreover%2C%20the%20distribution%20of%0Astiffness%20across%20the%20configuration%20of%20parasitic%20space%20has%20not%20previously%20been%0Aaddressed%20for%20either%20mechanism.%20This%20research%20reveals%20that%20despite%20identical%0Aparameters%20used%20and%20exhibiting%20similar%20parasitic%20motions%2C%20the%20Sprint%20Z3%0Ademonstrates%20superior%20stiffness%2C%20workspace%20volume%2C%20and%20condition%20number%0Adistribution.%20This%20performance%20advantage%20is%20attributed%20to%20variations%20in%20joint%0Aand%20link%20sequence%2C%20which%20enhance%20deflection%20resilience%2C%20crucial%20for%0Amanufacturing%20large-scale%20components.%20This%20also%20results%20in%20a%20higher%20condition%0Anumber%20and%20a%20larger%20workspace.%20The%20result%20highlights%20the%20importance%20of%20design%0Aarchitecture%20in%20the%20efficacy%20of%20parallel%20kinematics%20machines%20and%20suggest%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18575v1&entry.124074799=Read"},
{"title": "PACC: A Passive-Arm Approach for High-Payload Collaborative Carrying\n  with Quadruped Robots Using Model Predictive Control", "author": "Giulio Turrisi and Lucas Schulze and Vivian S. Medeiros and Claudio Semini and Victor Barasuol", "abstract": "  In this paper, we introduce the concept of using passive arm structures with\nintrinsic impedance for robot-robot and human-robot collaborative carrying with\nquadruped robots. The concept is meant for a leader-follower task and takes a\nminimalist approach that focuses on exploiting the robots' payload capabilities\nand reducing energy consumption, without compromising the robot locomotion\ncapabilities. We introduce a preliminary arm mechanical design and describe how\nto use its joint displacements to guide the robot's motion. To control the\nrobot's locomotion, we propose a decentralized Model Predictive Controller that\nincorporates an approximation of the arm dynamics and the estimation of the\nexternal forces from the collaborative carrying. We validate the overall system\nexperimentally by performing both robot-robot and human-robot collaborative\ncarrying on a stair-like obstacle and on rough terrain.\n", "link": "http://arxiv.org/abs/2403.19862v2", "date": "2024-04-29", "relevancy": 1.629, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5597}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5543}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.498}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20PACC%3A%20A%20Passive-Arm%20Approach%20for%20High-Payload%20Collaborative%20Carrying%0A%20%20with%20Quadruped%20Robots%20Using%20Model%20Predictive%20Control&body=Title%3A%20PACC%3A%20A%20Passive-Arm%20Approach%20for%20High-Payload%20Collaborative%20Carrying%0A%20%20with%20Quadruped%20Robots%20Using%20Model%20Predictive%20Control%0AAuthor%3A%20Giulio%20Turrisi%20and%20Lucas%20Schulze%20and%20Vivian%20S.%20Medeiros%20and%20Claudio%20Semini%20and%20Victor%20Barasuol%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20the%20concept%20of%20using%20passive%20arm%20structures%20with%0Aintrinsic%20impedance%20for%20robot-robot%20and%20human-robot%20collaborative%20carrying%20with%0Aquadruped%20robots.%20The%20concept%20is%20meant%20for%20a%20leader-follower%20task%20and%20takes%20a%0Aminimalist%20approach%20that%20focuses%20on%20exploiting%20the%20robots%27%20payload%20capabilities%0Aand%20reducing%20energy%20consumption%2C%20without%20compromising%20the%20robot%20locomotion%0Acapabilities.%20We%20introduce%20a%20preliminary%20arm%20mechanical%20design%20and%20describe%20how%0Ato%20use%20its%20joint%20displacements%20to%20guide%20the%20robot%27s%20motion.%20To%20control%20the%0Arobot%27s%20locomotion%2C%20we%20propose%20a%20decentralized%20Model%20Predictive%20Controller%20that%0Aincorporates%20an%20approximation%20of%20the%20arm%20dynamics%20and%20the%20estimation%20of%20the%0Aexternal%20forces%20from%20the%20collaborative%20carrying.%20We%20validate%20the%20overall%20system%0Aexperimentally%20by%20performing%20both%20robot-robot%20and%20human-robot%20collaborative%0Acarrying%20on%20a%20stair-like%20obstacle%20and%20on%20rough%20terrain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19862v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PACC%3A%20A%20Passive-Arm%20Approach%20for%20High-Payload%20Collaborative%20Carrying%0A%20%20with%20Quadruped%20Robots%20Using%20Model%20Predictive%20Control&entry.906535625=Giulio%20Turrisi%20and%20Lucas%20Schulze%20and%20Vivian%20S.%20Medeiros%20and%20Claudio%20Semini%20and%20Victor%20Barasuol&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20the%20concept%20of%20using%20passive%20arm%20structures%20with%0Aintrinsic%20impedance%20for%20robot-robot%20and%20human-robot%20collaborative%20carrying%20with%0Aquadruped%20robots.%20The%20concept%20is%20meant%20for%20a%20leader-follower%20task%20and%20takes%20a%0Aminimalist%20approach%20that%20focuses%20on%20exploiting%20the%20robots%27%20payload%20capabilities%0Aand%20reducing%20energy%20consumption%2C%20without%20compromising%20the%20robot%20locomotion%0Acapabilities.%20We%20introduce%20a%20preliminary%20arm%20mechanical%20design%20and%20describe%20how%0Ato%20use%20its%20joint%20displacements%20to%20guide%20the%20robot%27s%20motion.%20To%20control%20the%0Arobot%27s%20locomotion%2C%20we%20propose%20a%20decentralized%20Model%20Predictive%20Controller%20that%0Aincorporates%20an%20approximation%20of%20the%20arm%20dynamics%20and%20the%20estimation%20of%20the%0Aexternal%20forces%20from%20the%20collaborative%20carrying.%20We%20validate%20the%20overall%20system%0Aexperimentally%20by%20performing%20both%20robot-robot%20and%20human-robot%20collaborative%0Acarrying%20on%20a%20stair-like%20obstacle%20and%20on%20rough%20terrain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19862v2&entry.124074799=Read"},
{"title": "RSCaMa: Remote Sensing Image Change Captioning with State Space Model", "author": "Chenyang Liu and Keyan Chen and Bowen Chen and Haotian Zhang and Zhengxia Zou and Zhenwei Shi", "abstract": "  Remote Sensing Image Change Captioning (RSICC) aims to identify surface\nchanges in multi-temporal remote sensing images and describe them in natural\nlanguage. Current methods typically rely on an encoder-decoder architecture and\nfocus on designing a sophisticated neck to process bi-temporal features\nextracted by the backbone. Recently, State Space Models (SSMs), especially\nMamba, have demonstrated outstanding performance in many fields, owing to their\nefficient feature-selective modelling capability. However, their potential in\nthe RSICC task remains unexplored. In this paper, we introduce Mamba into RSICC\nand propose a novel approach called RSCaMa (Remote Sensing Change Captioning\nMamba). Specifically, we utilize Siamese backbones to extract bi-temporal\nfeatures, which are then processed through multiple CaMa layers consisting of\nSpatial Difference-guided SSM (SD-SSM) and Temporal Traveling SSM (TT-SSM).\nSD-SSM uses differential features to enhance change perception, while TT-SSM\npromotes bitemporal interactions in a token-wise cross-scanning manner.\nExperimental results validate the effectiveness of CaMa layers and demonstrate\nthe superior performance of RSCaMa, as well as the potential of Mamba in the\nRSICC task. Additionally, we systematically compare the effects of three\nlanguage decoders, including Mamba, GPT-style decoder with causal attention\nmechanism, and Transformer decoder with cross-attention mechanism. This\nprovides valuable insights for future RSICC research. The code will be\navailable at https://github.com/Chen-Yang-Liu/RSCaMa\n", "link": "http://arxiv.org/abs/2404.18895v1", "date": "2024-04-29", "relevancy": 1.5257, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5216}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5108}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.49}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RSCaMa%3A%20Remote%20Sensing%20Image%20Change%20Captioning%20with%20State%20Space%20Model&body=Title%3A%20RSCaMa%3A%20Remote%20Sensing%20Image%20Change%20Captioning%20with%20State%20Space%20Model%0AAuthor%3A%20Chenyang%20Liu%20and%20Keyan%20Chen%20and%20Bowen%20Chen%20and%20Haotian%20Zhang%20and%20Zhengxia%20Zou%20and%20Zhenwei%20Shi%0AAbstract%3A%20%20%20Remote%20Sensing%20Image%20Change%20Captioning%20%28RSICC%29%20aims%20to%20identify%20surface%0Achanges%20in%20multi-temporal%20remote%20sensing%20images%20and%20describe%20them%20in%20natural%0Alanguage.%20Current%20methods%20typically%20rely%20on%20an%20encoder-decoder%20architecture%20and%0Afocus%20on%20designing%20a%20sophisticated%20neck%20to%20process%20bi-temporal%20features%0Aextracted%20by%20the%20backbone.%20Recently%2C%20State%20Space%20Models%20%28SSMs%29%2C%20especially%0AMamba%2C%20have%20demonstrated%20outstanding%20performance%20in%20many%20fields%2C%20owing%20to%20their%0Aefficient%20feature-selective%20modelling%20capability.%20However%2C%20their%20potential%20in%0Athe%20RSICC%20task%20remains%20unexplored.%20In%20this%20paper%2C%20we%20introduce%20Mamba%20into%20RSICC%0Aand%20propose%20a%20novel%20approach%20called%20RSCaMa%20%28Remote%20Sensing%20Change%20Captioning%0AMamba%29.%20Specifically%2C%20we%20utilize%20Siamese%20backbones%20to%20extract%20bi-temporal%0Afeatures%2C%20which%20are%20then%20processed%20through%20multiple%20CaMa%20layers%20consisting%20of%0ASpatial%20Difference-guided%20SSM%20%28SD-SSM%29%20and%20Temporal%20Traveling%20SSM%20%28TT-SSM%29.%0ASD-SSM%20uses%20differential%20features%20to%20enhance%20change%20perception%2C%20while%20TT-SSM%0Apromotes%20bitemporal%20interactions%20in%20a%20token-wise%20cross-scanning%20manner.%0AExperimental%20results%20validate%20the%20effectiveness%20of%20CaMa%20layers%20and%20demonstrate%0Athe%20superior%20performance%20of%20RSCaMa%2C%20as%20well%20as%20the%20potential%20of%20Mamba%20in%20the%0ARSICC%20task.%20Additionally%2C%20we%20systematically%20compare%20the%20effects%20of%20three%0Alanguage%20decoders%2C%20including%20Mamba%2C%20GPT-style%20decoder%20with%20causal%20attention%0Amechanism%2C%20and%20Transformer%20decoder%20with%20cross-attention%20mechanism.%20This%0Aprovides%20valuable%20insights%20for%20future%20RSICC%20research.%20The%20code%20will%20be%0Aavailable%20at%20https%3A//github.com/Chen-Yang-Liu/RSCaMa%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18895v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RSCaMa%3A%20Remote%20Sensing%20Image%20Change%20Captioning%20with%20State%20Space%20Model&entry.906535625=Chenyang%20Liu%20and%20Keyan%20Chen%20and%20Bowen%20Chen%20and%20Haotian%20Zhang%20and%20Zhengxia%20Zou%20and%20Zhenwei%20Shi&entry.1292438233=%20%20Remote%20Sensing%20Image%20Change%20Captioning%20%28RSICC%29%20aims%20to%20identify%20surface%0Achanges%20in%20multi-temporal%20remote%20sensing%20images%20and%20describe%20them%20in%20natural%0Alanguage.%20Current%20methods%20typically%20rely%20on%20an%20encoder-decoder%20architecture%20and%0Afocus%20on%20designing%20a%20sophisticated%20neck%20to%20process%20bi-temporal%20features%0Aextracted%20by%20the%20backbone.%20Recently%2C%20State%20Space%20Models%20%28SSMs%29%2C%20especially%0AMamba%2C%20have%20demonstrated%20outstanding%20performance%20in%20many%20fields%2C%20owing%20to%20their%0Aefficient%20feature-selective%20modelling%20capability.%20However%2C%20their%20potential%20in%0Athe%20RSICC%20task%20remains%20unexplored.%20In%20this%20paper%2C%20we%20introduce%20Mamba%20into%20RSICC%0Aand%20propose%20a%20novel%20approach%20called%20RSCaMa%20%28Remote%20Sensing%20Change%20Captioning%0AMamba%29.%20Specifically%2C%20we%20utilize%20Siamese%20backbones%20to%20extract%20bi-temporal%0Afeatures%2C%20which%20are%20then%20processed%20through%20multiple%20CaMa%20layers%20consisting%20of%0ASpatial%20Difference-guided%20SSM%20%28SD-SSM%29%20and%20Temporal%20Traveling%20SSM%20%28TT-SSM%29.%0ASD-SSM%20uses%20differential%20features%20to%20enhance%20change%20perception%2C%20while%20TT-SSM%0Apromotes%20bitemporal%20interactions%20in%20a%20token-wise%20cross-scanning%20manner.%0AExperimental%20results%20validate%20the%20effectiveness%20of%20CaMa%20layers%20and%20demonstrate%0Athe%20superior%20performance%20of%20RSCaMa%2C%20as%20well%20as%20the%20potential%20of%20Mamba%20in%20the%0ARSICC%20task.%20Additionally%2C%20we%20systematically%20compare%20the%20effects%20of%20three%0Alanguage%20decoders%2C%20including%20Mamba%2C%20GPT-style%20decoder%20with%20causal%20attention%0Amechanism%2C%20and%20Transformer%20decoder%20with%20cross-attention%20mechanism.%20This%0Aprovides%20valuable%20insights%20for%20future%20RSICC%20research.%20The%20code%20will%20be%0Aavailable%20at%20https%3A//github.com/Chen-Yang-Liu/RSCaMa%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18895v1&entry.124074799=Read"},
{"title": "Harmonic Machine Learning Models are Robust", "author": "Nicholas S. Kersting and Yi Li and Aman Mohanty and Oyindamola Obisesan and Raphael Okochu", "abstract": "  We introduce Harmonic Robustness, a powerful and intuitive method to test the\nrobustness of any machine-learning model either during training or in black-box\nreal-time inference monitoring without ground-truth labels. It is based on\nfunctional deviation from the harmonic mean value property, indicating\ninstability and lack of explainability. We show implementation examples in\nlow-dimensional trees and feedforward NNs, where the method reliably identifies\noverfitting, as well as in more complex high-dimensional models such as\nResNet-50 and Vision Transformer where it efficiently measures adversarial\nvulnerability across image classes.\n", "link": "http://arxiv.org/abs/2404.18825v1", "date": "2024-04-29", "relevancy": 1.3751, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.469}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4585}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4473}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Harmonic%20Machine%20Learning%20Models%20are%20Robust&body=Title%3A%20Harmonic%20Machine%20Learning%20Models%20are%20Robust%0AAuthor%3A%20Nicholas%20S.%20Kersting%20and%20Yi%20Li%20and%20Aman%20Mohanty%20and%20Oyindamola%20Obisesan%20and%20Raphael%20Okochu%0AAbstract%3A%20%20%20We%20introduce%20Harmonic%20Robustness%2C%20a%20powerful%20and%20intuitive%20method%20to%20test%20the%0Arobustness%20of%20any%20machine-learning%20model%20either%20during%20training%20or%20in%20black-box%0Areal-time%20inference%20monitoring%20without%20ground-truth%20labels.%20It%20is%20based%20on%0Afunctional%20deviation%20from%20the%20harmonic%20mean%20value%20property%2C%20indicating%0Ainstability%20and%20lack%20of%20explainability.%20We%20show%20implementation%20examples%20in%0Alow-dimensional%20trees%20and%20feedforward%20NNs%2C%20where%20the%20method%20reliably%20identifies%0Aoverfitting%2C%20as%20well%20as%20in%20more%20complex%20high-dimensional%20models%20such%20as%0AResNet-50%20and%20Vision%20Transformer%20where%20it%20efficiently%20measures%20adversarial%0Avulnerability%20across%20image%20classes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18825v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harmonic%20Machine%20Learning%20Models%20are%20Robust&entry.906535625=Nicholas%20S.%20Kersting%20and%20Yi%20Li%20and%20Aman%20Mohanty%20and%20Oyindamola%20Obisesan%20and%20Raphael%20Okochu&entry.1292438233=%20%20We%20introduce%20Harmonic%20Robustness%2C%20a%20powerful%20and%20intuitive%20method%20to%20test%20the%0Arobustness%20of%20any%20machine-learning%20model%20either%20during%20training%20or%20in%20black-box%0Areal-time%20inference%20monitoring%20without%20ground-truth%20labels.%20It%20is%20based%20on%0Afunctional%20deviation%20from%20the%20harmonic%20mean%20value%20property%2C%20indicating%0Ainstability%20and%20lack%20of%20explainability.%20We%20show%20implementation%20examples%20in%0Alow-dimensional%20trees%20and%20feedforward%20NNs%2C%20where%20the%20method%20reliably%20identifies%0Aoverfitting%2C%20as%20well%20as%20in%20more%20complex%20high-dimensional%20models%20such%20as%0AResNet-50%20and%20Vision%20Transformer%20where%20it%20efficiently%20measures%20adversarial%0Avulnerability%20across%20image%20classes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18825v1&entry.124074799=Read"},
{"title": "A supplemental investigation of non-linearity in quantum generative\n  models with respect to simulatability and optimization", "author": "Kaitlin Gili and Rohan S. Kumar and Mykolas Sveistrys and C. J. Ballance", "abstract": "  Recent work has demonstrated the utility of introducing non-linearity through\nrepeat-until-success (RUS) sub-routines into quantum circuits for generative\nmodeling. As a follow-up to this work, we investigate two questions of\nrelevance to the quantum algorithms and machine learning communities: Does\nintroducing this form of non-linearity make the learning model classically\nsimulatable due to the deferred measurement principle? And does introducing\nthis form of non-linearity make the overall model's training more unstable?\nWith respect to the first question, we demonstrate that the RUS sub-routines do\nnot allow us to trivially map this quantum model to a classical one, whereas a\nmodel without RUS sub-circuits containing mid-circuit measurements could be\nmapped to a classical Bayesian network due to the deferred measurement\nprinciple of quantum mechanics. This strongly suggests that the proposed form\nof non-linearity makes the model classically in-efficient to simulate. In the\npursuit of the second question, we train larger models than previously shown on\nthree different probability distributions, one continuous and two discrete, and\ncompare the training performance across multiple random trials. We see that\nwhile the model is able to perform exceptionally well in some trials, the\nvariance across trials with certain datasets quantifies its relatively poor\ntraining stability.\n", "link": "http://arxiv.org/abs/2302.00788v2", "date": "2024-04-29", "relevancy": 0.9469, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5106}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4554}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4544}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20supplemental%20investigation%20of%20non-linearity%20in%20quantum%20generative%0A%20%20models%20with%20respect%20to%20simulatability%20and%20optimization&body=Title%3A%20A%20supplemental%20investigation%20of%20non-linearity%20in%20quantum%20generative%0A%20%20models%20with%20respect%20to%20simulatability%20and%20optimization%0AAuthor%3A%20Kaitlin%20Gili%20and%20Rohan%20S.%20Kumar%20and%20Mykolas%20Sveistrys%20and%20C.%20J.%20Ballance%0AAbstract%3A%20%20%20Recent%20work%20has%20demonstrated%20the%20utility%20of%20introducing%20non-linearity%20through%0Arepeat-until-success%20%28RUS%29%20sub-routines%20into%20quantum%20circuits%20for%20generative%0Amodeling.%20As%20a%20follow-up%20to%20this%20work%2C%20we%20investigate%20two%20questions%20of%0Arelevance%20to%20the%20quantum%20algorithms%20and%20machine%20learning%20communities%3A%20Does%0Aintroducing%20this%20form%20of%20non-linearity%20make%20the%20learning%20model%20classically%0Asimulatable%20due%20to%20the%20deferred%20measurement%20principle%3F%20And%20does%20introducing%0Athis%20form%20of%20non-linearity%20make%20the%20overall%20model%27s%20training%20more%20unstable%3F%0AWith%20respect%20to%20the%20first%20question%2C%20we%20demonstrate%20that%20the%20RUS%20sub-routines%20do%0Anot%20allow%20us%20to%20trivially%20map%20this%20quantum%20model%20to%20a%20classical%20one%2C%20whereas%20a%0Amodel%20without%20RUS%20sub-circuits%20containing%20mid-circuit%20measurements%20could%20be%0Amapped%20to%20a%20classical%20Bayesian%20network%20due%20to%20the%20deferred%20measurement%0Aprinciple%20of%20quantum%20mechanics.%20This%20strongly%20suggests%20that%20the%20proposed%20form%0Aof%20non-linearity%20makes%20the%20model%20classically%20in-efficient%20to%20simulate.%20In%20the%0Apursuit%20of%20the%20second%20question%2C%20we%20train%20larger%20models%20than%20previously%20shown%20on%0Athree%20different%20probability%20distributions%2C%20one%20continuous%20and%20two%20discrete%2C%20and%0Acompare%20the%20training%20performance%20across%20multiple%20random%20trials.%20We%20see%20that%0Awhile%20the%20model%20is%20able%20to%20perform%20exceptionally%20well%20in%20some%20trials%2C%20the%0Avariance%20across%20trials%20with%20certain%20datasets%20quantifies%20its%20relatively%20poor%0Atraining%20stability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.00788v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20supplemental%20investigation%20of%20non-linearity%20in%20quantum%20generative%0A%20%20models%20with%20respect%20to%20simulatability%20and%20optimization&entry.906535625=Kaitlin%20Gili%20and%20Rohan%20S.%20Kumar%20and%20Mykolas%20Sveistrys%20and%20C.%20J.%20Ballance&entry.1292438233=%20%20Recent%20work%20has%20demonstrated%20the%20utility%20of%20introducing%20non-linearity%20through%0Arepeat-until-success%20%28RUS%29%20sub-routines%20into%20quantum%20circuits%20for%20generative%0Amodeling.%20As%20a%20follow-up%20to%20this%20work%2C%20we%20investigate%20two%20questions%20of%0Arelevance%20to%20the%20quantum%20algorithms%20and%20machine%20learning%20communities%3A%20Does%0Aintroducing%20this%20form%20of%20non-linearity%20make%20the%20learning%20model%20classically%0Asimulatable%20due%20to%20the%20deferred%20measurement%20principle%3F%20And%20does%20introducing%0Athis%20form%20of%20non-linearity%20make%20the%20overall%20model%27s%20training%20more%20unstable%3F%0AWith%20respect%20to%20the%20first%20question%2C%20we%20demonstrate%20that%20the%20RUS%20sub-routines%20do%0Anot%20allow%20us%20to%20trivially%20map%20this%20quantum%20model%20to%20a%20classical%20one%2C%20whereas%20a%0Amodel%20without%20RUS%20sub-circuits%20containing%20mid-circuit%20measurements%20could%20be%0Amapped%20to%20a%20classical%20Bayesian%20network%20due%20to%20the%20deferred%20measurement%0Aprinciple%20of%20quantum%20mechanics.%20This%20strongly%20suggests%20that%20the%20proposed%20form%0Aof%20non-linearity%20makes%20the%20model%20classically%20in-efficient%20to%20simulate.%20In%20the%0Apursuit%20of%20the%20second%20question%2C%20we%20train%20larger%20models%20than%20previously%20shown%20on%0Athree%20different%20probability%20distributions%2C%20one%20continuous%20and%20two%20discrete%2C%20and%0Acompare%20the%20training%20performance%20across%20multiple%20random%20trials.%20We%20see%20that%0Awhile%20the%20model%20is%20able%20to%20perform%20exceptionally%20well%20in%20some%20trials%2C%20the%0Avariance%20across%20trials%20with%20certain%20datasets%20quantifies%20its%20relatively%20poor%0Atraining%20stability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.00788v2&entry.124074799=Read"},
{"title": "Open-Source Drift Detection Tools in Action: Insights from Two Use Cases", "author": "Rieke M\u00fcller and Mohamed Abdelaal and Davor Stjelja", "abstract": "  Data drifts pose a critical challenge in the lifecycle of machine learning\n(ML) models, affecting their performance and reliability. In response to this\nchallenge, we present a microbenchmark study, called D3Bench, which evaluates\nthe efficacy of open-source drift detection tools. D3Bench examines the\ncapabilities of Evidently AI, NannyML, and Alibi-Detect, leveraging real-world\ndata from two smart building use cases.We prioritize assessing the functional\nsuitability of these tools to identify and analyze data drifts. Furthermore, we\nconsider a comprehensive set of non-functional criteria, such as the\nintegrability with ML pipelines, the adaptability to diverse data types,\nuser-friendliness, computational efficiency, and resource demands. Our findings\nreveal that Evidently AI stands out for its general data drift detection,\nwhereas NannyML excels at pinpointing the precise timing of shifts and\nevaluating their consequent effects on predictive accuracy.\n", "link": "http://arxiv.org/abs/2404.18673v1", "date": "2024-04-29", "relevancy": 1.4366, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5188}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4746}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4646}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Open-Source%20Drift%20Detection%20Tools%20in%20Action%3A%20Insights%20from%20Two%20Use%20Cases&body=Title%3A%20Open-Source%20Drift%20Detection%20Tools%20in%20Action%3A%20Insights%20from%20Two%20Use%20Cases%0AAuthor%3A%20Rieke%20M%C3%BCller%20and%20Mohamed%20Abdelaal%20and%20Davor%20Stjelja%0AAbstract%3A%20%20%20Data%20drifts%20pose%20a%20critical%20challenge%20in%20the%20lifecycle%20of%20machine%20learning%0A%28ML%29%20models%2C%20affecting%20their%20performance%20and%20reliability.%20In%20response%20to%20this%0Achallenge%2C%20we%20present%20a%20microbenchmark%20study%2C%20called%20D3Bench%2C%20which%20evaluates%0Athe%20efficacy%20of%20open-source%20drift%20detection%20tools.%20D3Bench%20examines%20the%0Acapabilities%20of%20Evidently%20AI%2C%20NannyML%2C%20and%20Alibi-Detect%2C%20leveraging%20real-world%0Adata%20from%20two%20smart%20building%20use%20cases.We%20prioritize%20assessing%20the%20functional%0Asuitability%20of%20these%20tools%20to%20identify%20and%20analyze%20data%20drifts.%20Furthermore%2C%20we%0Aconsider%20a%20comprehensive%20set%20of%20non-functional%20criteria%2C%20such%20as%20the%0Aintegrability%20with%20ML%20pipelines%2C%20the%20adaptability%20to%20diverse%20data%20types%2C%0Auser-friendliness%2C%20computational%20efficiency%2C%20and%20resource%20demands.%20Our%20findings%0Areveal%20that%20Evidently%20AI%20stands%20out%20for%20its%20general%20data%20drift%20detection%2C%0Awhereas%20NannyML%20excels%20at%20pinpointing%20the%20precise%20timing%20of%20shifts%20and%0Aevaluating%20their%20consequent%20effects%20on%20predictive%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18673v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-Source%20Drift%20Detection%20Tools%20in%20Action%3A%20Insights%20from%20Two%20Use%20Cases&entry.906535625=Rieke%20M%C3%BCller%20and%20Mohamed%20Abdelaal%20and%20Davor%20Stjelja&entry.1292438233=%20%20Data%20drifts%20pose%20a%20critical%20challenge%20in%20the%20lifecycle%20of%20machine%20learning%0A%28ML%29%20models%2C%20affecting%20their%20performance%20and%20reliability.%20In%20response%20to%20this%0Achallenge%2C%20we%20present%20a%20microbenchmark%20study%2C%20called%20D3Bench%2C%20which%20evaluates%0Athe%20efficacy%20of%20open-source%20drift%20detection%20tools.%20D3Bench%20examines%20the%0Acapabilities%20of%20Evidently%20AI%2C%20NannyML%2C%20and%20Alibi-Detect%2C%20leveraging%20real-world%0Adata%20from%20two%20smart%20building%20use%20cases.We%20prioritize%20assessing%20the%20functional%0Asuitability%20of%20these%20tools%20to%20identify%20and%20analyze%20data%20drifts.%20Furthermore%2C%20we%0Aconsider%20a%20comprehensive%20set%20of%20non-functional%20criteria%2C%20such%20as%20the%0Aintegrability%20with%20ML%20pipelines%2C%20the%20adaptability%20to%20diverse%20data%20types%2C%0Auser-friendliness%2C%20computational%20efficiency%2C%20and%20resource%20demands.%20Our%20findings%0Areveal%20that%20Evidently%20AI%20stands%20out%20for%20its%20general%20data%20drift%20detection%2C%0Awhereas%20NannyML%20excels%20at%20pinpointing%20the%20precise%20timing%20of%20shifts%20and%0Aevaluating%20their%20consequent%20effects%20on%20predictive%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18673v1&entry.124074799=Read"},
{"title": "Latent Space Representations of Neural Algorithmic Reasoners", "author": "Vladimir V. Mirjani\u0107 and Razvan Pascanu and Petar Veli\u010dkovi\u0107", "abstract": "  Neural Algorithmic Reasoning (NAR) is a research area focused on designing\nneural architectures that can reliably capture classical computation, usually\nby learning to execute algorithms. A typical approach is to rely on Graph\nNeural Network (GNN) architectures, which encode inputs in high-dimensional\nlatent spaces that are repeatedly transformed during the execution of the\nalgorithm. In this work we perform a detailed analysis of the structure of the\nlatent space induced by the GNN when executing algorithms. We identify two\npossible failure modes: (i) loss of resolution, making it hard to distinguish\nsimilar values; (ii) inability to deal with values outside the range observed\nduring training. We propose to solve the first issue by relying on a softmax\naggregator, and propose to decay the latent space in order to deal with\nout-of-range values. We show that these changes lead to improvements on the\nmajority of algorithms in the standard CLRS-30 benchmark when using the\nstate-of-the-art Triplet-GMPNN processor. Our code is available at\nhttps://github.com/mirjanic/nar-latent-spaces\n", "link": "http://arxiv.org/abs/2307.08874v2", "date": "2024-04-29", "relevancy": 1.4637, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5258}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4784}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4737}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Latent%20Space%20Representations%20of%20Neural%20Algorithmic%20Reasoners&body=Title%3A%20Latent%20Space%20Representations%20of%20Neural%20Algorithmic%20Reasoners%0AAuthor%3A%20Vladimir%20V.%20Mirjani%C4%87%20and%20Razvan%20Pascanu%20and%20Petar%20Veli%C4%8Dkovi%C4%87%0AAbstract%3A%20%20%20Neural%20Algorithmic%20Reasoning%20%28NAR%29%20is%20a%20research%20area%20focused%20on%20designing%0Aneural%20architectures%20that%20can%20reliably%20capture%20classical%20computation%2C%20usually%0Aby%20learning%20to%20execute%20algorithms.%20A%20typical%20approach%20is%20to%20rely%20on%20Graph%0ANeural%20Network%20%28GNN%29%20architectures%2C%20which%20encode%20inputs%20in%20high-dimensional%0Alatent%20spaces%20that%20are%20repeatedly%20transformed%20during%20the%20execution%20of%20the%0Aalgorithm.%20In%20this%20work%20we%20perform%20a%20detailed%20analysis%20of%20the%20structure%20of%20the%0Alatent%20space%20induced%20by%20the%20GNN%20when%20executing%20algorithms.%20We%20identify%20two%0Apossible%20failure%20modes%3A%20%28i%29%20loss%20of%20resolution%2C%20making%20it%20hard%20to%20distinguish%0Asimilar%20values%3B%20%28ii%29%20inability%20to%20deal%20with%20values%20outside%20the%20range%20observed%0Aduring%20training.%20We%20propose%20to%20solve%20the%20first%20issue%20by%20relying%20on%20a%20softmax%0Aaggregator%2C%20and%20propose%20to%20decay%20the%20latent%20space%20in%20order%20to%20deal%20with%0Aout-of-range%20values.%20We%20show%20that%20these%20changes%20lead%20to%20improvements%20on%20the%0Amajority%20of%20algorithms%20in%20the%20standard%20CLRS-30%20benchmark%20when%20using%20the%0Astate-of-the-art%20Triplet-GMPNN%20processor.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/mirjanic/nar-latent-spaces%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.08874v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Space%20Representations%20of%20Neural%20Algorithmic%20Reasoners&entry.906535625=Vladimir%20V.%20Mirjani%C4%87%20and%20Razvan%20Pascanu%20and%20Petar%20Veli%C4%8Dkovi%C4%87&entry.1292438233=%20%20Neural%20Algorithmic%20Reasoning%20%28NAR%29%20is%20a%20research%20area%20focused%20on%20designing%0Aneural%20architectures%20that%20can%20reliably%20capture%20classical%20computation%2C%20usually%0Aby%20learning%20to%20execute%20algorithms.%20A%20typical%20approach%20is%20to%20rely%20on%20Graph%0ANeural%20Network%20%28GNN%29%20architectures%2C%20which%20encode%20inputs%20in%20high-dimensional%0Alatent%20spaces%20that%20are%20repeatedly%20transformed%20during%20the%20execution%20of%20the%0Aalgorithm.%20In%20this%20work%20we%20perform%20a%20detailed%20analysis%20of%20the%20structure%20of%20the%0Alatent%20space%20induced%20by%20the%20GNN%20when%20executing%20algorithms.%20We%20identify%20two%0Apossible%20failure%20modes%3A%20%28i%29%20loss%20of%20resolution%2C%20making%20it%20hard%20to%20distinguish%0Asimilar%20values%3B%20%28ii%29%20inability%20to%20deal%20with%20values%20outside%20the%20range%20observed%0Aduring%20training.%20We%20propose%20to%20solve%20the%20first%20issue%20by%20relying%20on%20a%20softmax%0Aaggregator%2C%20and%20propose%20to%20decay%20the%20latent%20space%20in%20order%20to%20deal%20with%0Aout-of-range%20values.%20We%20show%20that%20these%20changes%20lead%20to%20improvements%20on%20the%0Amajority%20of%20algorithms%20in%20the%20standard%20CLRS-30%20benchmark%20when%20using%20the%0Astate-of-the-art%20Triplet-GMPNN%20processor.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/mirjanic/nar-latent-spaces%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.08874v2&entry.124074799=Read"},
{"title": "Hallucination of Multimodal Large Language Models: A Survey", "author": "Zechen Bai and Pichao Wang and Tianjun Xiao and Tong He and Zongbo Han and Zheng Zhang and Mike Zheng Shou", "abstract": "  This survey presents a comprehensive analysis of the phenomenon of\nhallucination in multimodal large language models (MLLMs), also known as Large\nVision-Language Models (LVLMs), which have demonstrated significant\nadvancements and remarkable abilities in multimodal tasks. Despite these\npromising developments, MLLMs often generate outputs that are inconsistent with\nthe visual content, a challenge known as hallucination, which poses substantial\nobstacles to their practical deployment and raises concerns regarding their\nreliability in real-world applications. This problem has attracted increasing\nattention, prompting efforts to detect and mitigate such inaccuracies. We\nreview recent advances in identifying, evaluating, and mitigating these\nhallucinations, offering a detailed overview of the underlying causes,\nevaluation benchmarks, metrics, and strategies developed to address this issue.\nAdditionally, we analyze the current challenges and limitations, formulating\nopen questions that delineate potential pathways for future research. By\ndrawing the granular classification and landscapes of hallucination causes,\nevaluation benchmarks, and mitigation methods, this survey aims to deepen the\nunderstanding of hallucinations in MLLMs and inspire further advancements in\nthe field. Through our thorough and in-depth review, we contribute to the\nongoing dialogue on enhancing the robustness and reliability of MLLMs,\nproviding valuable insights and resources for researchers and practitioners\nalike. Resources are available at:\nhttps://github.com/showlab/Awesome-MLLM-Hallucination.\n", "link": "http://arxiv.org/abs/2404.18930v1", "date": "2024-04-29", "relevancy": 1.5318, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5185}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5111}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5072}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Hallucination%20of%20Multimodal%20Large%20Language%20Models%3A%20A%20Survey&body=Title%3A%20Hallucination%20of%20Multimodal%20Large%20Language%20Models%3A%20A%20Survey%0AAuthor%3A%20Zechen%20Bai%20and%20Pichao%20Wang%20and%20Tianjun%20Xiao%20and%20Tong%20He%20and%20Zongbo%20Han%20and%20Zheng%20Zhang%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20%20%20This%20survey%20presents%20a%20comprehensive%20analysis%20of%20the%20phenomenon%20of%0Ahallucination%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%2C%20also%20known%20as%20Large%0AVision-Language%20Models%20%28LVLMs%29%2C%20which%20have%20demonstrated%20significant%0Aadvancements%20and%20remarkable%20abilities%20in%20multimodal%20tasks.%20Despite%20these%0Apromising%20developments%2C%20MLLMs%20often%20generate%20outputs%20that%20are%20inconsistent%20with%0Athe%20visual%20content%2C%20a%20challenge%20known%20as%20hallucination%2C%20which%20poses%20substantial%0Aobstacles%20to%20their%20practical%20deployment%20and%20raises%20concerns%20regarding%20their%0Areliability%20in%20real-world%20applications.%20This%20problem%20has%20attracted%20increasing%0Aattention%2C%20prompting%20efforts%20to%20detect%20and%20mitigate%20such%20inaccuracies.%20We%0Areview%20recent%20advances%20in%20identifying%2C%20evaluating%2C%20and%20mitigating%20these%0Ahallucinations%2C%20offering%20a%20detailed%20overview%20of%20the%20underlying%20causes%2C%0Aevaluation%20benchmarks%2C%20metrics%2C%20and%20strategies%20developed%20to%20address%20this%20issue.%0AAdditionally%2C%20we%20analyze%20the%20current%20challenges%20and%20limitations%2C%20formulating%0Aopen%20questions%20that%20delineate%20potential%20pathways%20for%20future%20research.%20By%0Adrawing%20the%20granular%20classification%20and%20landscapes%20of%20hallucination%20causes%2C%0Aevaluation%20benchmarks%2C%20and%20mitigation%20methods%2C%20this%20survey%20aims%20to%20deepen%20the%0Aunderstanding%20of%20hallucinations%20in%20MLLMs%20and%20inspire%20further%20advancements%20in%0Athe%20field.%20Through%20our%20thorough%20and%20in-depth%20review%2C%20we%20contribute%20to%20the%0Aongoing%20dialogue%20on%20enhancing%20the%20robustness%20and%20reliability%20of%20MLLMs%2C%0Aproviding%20valuable%20insights%20and%20resources%20for%20researchers%20and%20practitioners%0Aalike.%20Resources%20are%20available%20at%3A%0Ahttps%3A//github.com/showlab/Awesome-MLLM-Hallucination.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18930v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hallucination%20of%20Multimodal%20Large%20Language%20Models%3A%20A%20Survey&entry.906535625=Zechen%20Bai%20and%20Pichao%20Wang%20and%20Tianjun%20Xiao%20and%20Tong%20He%20and%20Zongbo%20Han%20and%20Zheng%20Zhang%20and%20Mike%20Zheng%20Shou&entry.1292438233=%20%20This%20survey%20presents%20a%20comprehensive%20analysis%20of%20the%20phenomenon%20of%0Ahallucination%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%2C%20also%20known%20as%20Large%0AVision-Language%20Models%20%28LVLMs%29%2C%20which%20have%20demonstrated%20significant%0Aadvancements%20and%20remarkable%20abilities%20in%20multimodal%20tasks.%20Despite%20these%0Apromising%20developments%2C%20MLLMs%20often%20generate%20outputs%20that%20are%20inconsistent%20with%0Athe%20visual%20content%2C%20a%20challenge%20known%20as%20hallucination%2C%20which%20poses%20substantial%0Aobstacles%20to%20their%20practical%20deployment%20and%20raises%20concerns%20regarding%20their%0Areliability%20in%20real-world%20applications.%20This%20problem%20has%20attracted%20increasing%0Aattention%2C%20prompting%20efforts%20to%20detect%20and%20mitigate%20such%20inaccuracies.%20We%0Areview%20recent%20advances%20in%20identifying%2C%20evaluating%2C%20and%20mitigating%20these%0Ahallucinations%2C%20offering%20a%20detailed%20overview%20of%20the%20underlying%20causes%2C%0Aevaluation%20benchmarks%2C%20metrics%2C%20and%20strategies%20developed%20to%20address%20this%20issue.%0AAdditionally%2C%20we%20analyze%20the%20current%20challenges%20and%20limitations%2C%20formulating%0Aopen%20questions%20that%20delineate%20potential%20pathways%20for%20future%20research.%20By%0Adrawing%20the%20granular%20classification%20and%20landscapes%20of%20hallucination%20causes%2C%0Aevaluation%20benchmarks%2C%20and%20mitigation%20methods%2C%20this%20survey%20aims%20to%20deepen%20the%0Aunderstanding%20of%20hallucinations%20in%20MLLMs%20and%20inspire%20further%20advancements%20in%0Athe%20field.%20Through%20our%20thorough%20and%20in-depth%20review%2C%20we%20contribute%20to%20the%0Aongoing%20dialogue%20on%20enhancing%20the%20robustness%20and%20reliability%20of%20MLLMs%2C%0Aproviding%20valuable%20insights%20and%20resources%20for%20researchers%20and%20practitioners%0Aalike.%20Resources%20are%20available%20at%3A%0Ahttps%3A//github.com/showlab/Awesome-MLLM-Hallucination.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18930v1&entry.124074799=Read"},
{"title": "Enhancing Uncertain Demand Prediction in Hospitals Using Simple and\n  Advanced Machine Learning", "author": "Annie Hu and Samuel Stockman and Xun Wu and Richard Wood and Bangdong Zhi and Oliver Y. Ch\u00e9n", "abstract": "  Early and timely prediction of patient care demand not only affects effective\nresource allocation but also influences clinical decision-making as well as\npatient experience. Accurately predicting patient care demand, however, is a\nubiquitous challenge for hospitals across the world due, in part, to the\ndemand's time-varying temporal variability, and, in part, to the difficulty in\nmodelling trends in advance. To address this issue, here, we develop two\nmethods, a relatively simple time-vary linear model, and a more advanced neural\nnetwork model. The former forecasts patient arrivals hourly over a week based\non factors such as day of the week and previous 7-day arrival patterns. The\nlatter leverages a long short-term memory (LSTM) model, capturing non-linear\nrelationships between past data and a three-day forecasting window. We evaluate\nthe predictive capabilities of the two proposed approaches compared to two\nna\\\"ive approaches - a reduced-rank vector autoregressive (VAR) model and the\nTBATS model. Using patient care demand data from Rambam Medical Center in\nIsrael, our results show that both proposed models effectively capture hourly\nvariations of patient demand. Additionally, the linear model is more\nexplainable thanks to its simple architecture, whereas, by accurately modelling\nweekly seasonal trends, the LSTM model delivers lower prediction errors. Taken\ntogether, our explorations suggest the utility of machine learning in\npredicting time-varying patient care demand; additionally, it is possible to\npredict patient care demand with good accuracy (around 4 patients) three days\nor a week in advance using machine learning.\n", "link": "http://arxiv.org/abs/2404.18670v1", "date": "2024-04-29", "relevancy": 1.3734, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4629}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4571}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4543}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Uncertain%20Demand%20Prediction%20in%20Hospitals%20Using%20Simple%20and%0A%20%20Advanced%20Machine%20Learning&body=Title%3A%20Enhancing%20Uncertain%20Demand%20Prediction%20in%20Hospitals%20Using%20Simple%20and%0A%20%20Advanced%20Machine%20Learning%0AAuthor%3A%20Annie%20Hu%20and%20Samuel%20Stockman%20and%20Xun%20Wu%20and%20Richard%20Wood%20and%20Bangdong%20Zhi%20and%20Oliver%20Y.%20Ch%C3%A9n%0AAbstract%3A%20%20%20Early%20and%20timely%20prediction%20of%20patient%20care%20demand%20not%20only%20affects%20effective%0Aresource%20allocation%20but%20also%20influences%20clinical%20decision-making%20as%20well%20as%0Apatient%20experience.%20Accurately%20predicting%20patient%20care%20demand%2C%20however%2C%20is%20a%0Aubiquitous%20challenge%20for%20hospitals%20across%20the%20world%20due%2C%20in%20part%2C%20to%20the%0Ademand%27s%20time-varying%20temporal%20variability%2C%20and%2C%20in%20part%2C%20to%20the%20difficulty%20in%0Amodelling%20trends%20in%20advance.%20To%20address%20this%20issue%2C%20here%2C%20we%20develop%20two%0Amethods%2C%20a%20relatively%20simple%20time-vary%20linear%20model%2C%20and%20a%20more%20advanced%20neural%0Anetwork%20model.%20The%20former%20forecasts%20patient%20arrivals%20hourly%20over%20a%20week%20based%0Aon%20factors%20such%20as%20day%20of%20the%20week%20and%20previous%207-day%20arrival%20patterns.%20The%0Alatter%20leverages%20a%20long%20short-term%20memory%20%28LSTM%29%20model%2C%20capturing%20non-linear%0Arelationships%20between%20past%20data%20and%20a%20three-day%20forecasting%20window.%20We%20evaluate%0Athe%20predictive%20capabilities%20of%20the%20two%20proposed%20approaches%20compared%20to%20two%0Ana%5C%22ive%20approaches%20-%20a%20reduced-rank%20vector%20autoregressive%20%28VAR%29%20model%20and%20the%0ATBATS%20model.%20Using%20patient%20care%20demand%20data%20from%20Rambam%20Medical%20Center%20in%0AIsrael%2C%20our%20results%20show%20that%20both%20proposed%20models%20effectively%20capture%20hourly%0Avariations%20of%20patient%20demand.%20Additionally%2C%20the%20linear%20model%20is%20more%0Aexplainable%20thanks%20to%20its%20simple%20architecture%2C%20whereas%2C%20by%20accurately%20modelling%0Aweekly%20seasonal%20trends%2C%20the%20LSTM%20model%20delivers%20lower%20prediction%20errors.%20Taken%0Atogether%2C%20our%20explorations%20suggest%20the%20utility%20of%20machine%20learning%20in%0Apredicting%20time-varying%20patient%20care%20demand%3B%20additionally%2C%20it%20is%20possible%20to%0Apredict%20patient%20care%20demand%20with%20good%20accuracy%20%28around%204%20patients%29%20three%20days%0Aor%20a%20week%20in%20advance%20using%20machine%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18670v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Uncertain%20Demand%20Prediction%20in%20Hospitals%20Using%20Simple%20and%0A%20%20Advanced%20Machine%20Learning&entry.906535625=Annie%20Hu%20and%20Samuel%20Stockman%20and%20Xun%20Wu%20and%20Richard%20Wood%20and%20Bangdong%20Zhi%20and%20Oliver%20Y.%20Ch%C3%A9n&entry.1292438233=%20%20Early%20and%20timely%20prediction%20of%20patient%20care%20demand%20not%20only%20affects%20effective%0Aresource%20allocation%20but%20also%20influences%20clinical%20decision-making%20as%20well%20as%0Apatient%20experience.%20Accurately%20predicting%20patient%20care%20demand%2C%20however%2C%20is%20a%0Aubiquitous%20challenge%20for%20hospitals%20across%20the%20world%20due%2C%20in%20part%2C%20to%20the%0Ademand%27s%20time-varying%20temporal%20variability%2C%20and%2C%20in%20part%2C%20to%20the%20difficulty%20in%0Amodelling%20trends%20in%20advance.%20To%20address%20this%20issue%2C%20here%2C%20we%20develop%20two%0Amethods%2C%20a%20relatively%20simple%20time-vary%20linear%20model%2C%20and%20a%20more%20advanced%20neural%0Anetwork%20model.%20The%20former%20forecasts%20patient%20arrivals%20hourly%20over%20a%20week%20based%0Aon%20factors%20such%20as%20day%20of%20the%20week%20and%20previous%207-day%20arrival%20patterns.%20The%0Alatter%20leverages%20a%20long%20short-term%20memory%20%28LSTM%29%20model%2C%20capturing%20non-linear%0Arelationships%20between%20past%20data%20and%20a%20three-day%20forecasting%20window.%20We%20evaluate%0Athe%20predictive%20capabilities%20of%20the%20two%20proposed%20approaches%20compared%20to%20two%0Ana%5C%22ive%20approaches%20-%20a%20reduced-rank%20vector%20autoregressive%20%28VAR%29%20model%20and%20the%0ATBATS%20model.%20Using%20patient%20care%20demand%20data%20from%20Rambam%20Medical%20Center%20in%0AIsrael%2C%20our%20results%20show%20that%20both%20proposed%20models%20effectively%20capture%20hourly%0Avariations%20of%20patient%20demand.%20Additionally%2C%20the%20linear%20model%20is%20more%0Aexplainable%20thanks%20to%20its%20simple%20architecture%2C%20whereas%2C%20by%20accurately%20modelling%0Aweekly%20seasonal%20trends%2C%20the%20LSTM%20model%20delivers%20lower%20prediction%20errors.%20Taken%0Atogether%2C%20our%20explorations%20suggest%20the%20utility%20of%20machine%20learning%20in%0Apredicting%20time-varying%20patient%20care%20demand%3B%20additionally%2C%20it%20is%20possible%20to%0Apredict%20patient%20care%20demand%20with%20good%20accuracy%20%28around%204%20patients%29%20three%20days%0Aor%20a%20week%20in%20advance%20using%20machine%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18670v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


