<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251202.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "SkelSplat: Robust Multi-view 3D Human Pose Estimation with Differentiable Gaussian Rendering", "author": "Laura Bragagnolo and Leonardo Barcellona and Stefano Ghidoni", "abstract": "Accurate 3D human pose estimation is fundamental for applications such as augmented reality and human-robot interaction. State-of-the-art multi-view methods learn to fuse predictions across views by training on large annotated datasets, leading to poor generalization when the test scenario differs. To overcome these limitations, we propose SkelSplat, a novel framework for multi-view 3D human pose estimation based on differentiable Gaussian rendering. Human pose is modeled as a skeleton of 3D Gaussians, one per joint, optimized via differentiable rendering to enable seamless fusion of arbitrary camera views without 3D ground-truth supervision. Since Gaussian Splatting was originally designed for dense scene reconstruction, we propose a novel one-hot encoding scheme that enables independent optimization of human joints. SkelSplat outperforms approaches that do not rely on 3D ground truth in Human3.6M and CMU, while reducing the cross-dataset error up to 47.8% compared to learning-based methods. Experiments on Human3.6M-Occ and Occlusion-Person demonstrate robustness to occlusions, without scenario-specific fine-tuning. Our project page is available here: https://skelsplat.github.io.", "link": "http://arxiv.org/abs/2511.08294v2", "date": "2025-12-02", "relevancy": 3.4781, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.758}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6682}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SkelSplat%3A%20Robust%20Multi-view%203D%20Human%20Pose%20Estimation%20with%20Differentiable%20Gaussian%20Rendering&body=Title%3A%20SkelSplat%3A%20Robust%20Multi-view%203D%20Human%20Pose%20Estimation%20with%20Differentiable%20Gaussian%20Rendering%0AAuthor%3A%20Laura%20Bragagnolo%20and%20Leonardo%20Barcellona%20and%20Stefano%20Ghidoni%0AAbstract%3A%20Accurate%203D%20human%20pose%20estimation%20is%20fundamental%20for%20applications%20such%20as%20augmented%20reality%20and%20human-robot%20interaction.%20State-of-the-art%20multi-view%20methods%20learn%20to%20fuse%20predictions%20across%20views%20by%20training%20on%20large%20annotated%20datasets%2C%20leading%20to%20poor%20generalization%20when%20the%20test%20scenario%20differs.%20To%20overcome%20these%20limitations%2C%20we%20propose%20SkelSplat%2C%20a%20novel%20framework%20for%20multi-view%203D%20human%20pose%20estimation%20based%20on%20differentiable%20Gaussian%20rendering.%20Human%20pose%20is%20modeled%20as%20a%20skeleton%20of%203D%20Gaussians%2C%20one%20per%20joint%2C%20optimized%20via%20differentiable%20rendering%20to%20enable%20seamless%20fusion%20of%20arbitrary%20camera%20views%20without%203D%20ground-truth%20supervision.%20Since%20Gaussian%20Splatting%20was%20originally%20designed%20for%20dense%20scene%20reconstruction%2C%20we%20propose%20a%20novel%20one-hot%20encoding%20scheme%20that%20enables%20independent%20optimization%20of%20human%20joints.%20SkelSplat%20outperforms%20approaches%20that%20do%20not%20rely%20on%203D%20ground%20truth%20in%20Human3.6M%20and%20CMU%2C%20while%20reducing%20the%20cross-dataset%20error%20up%20to%2047.8%25%20compared%20to%20learning-based%20methods.%20Experiments%20on%20Human3.6M-Occ%20and%20Occlusion-Person%20demonstrate%20robustness%20to%20occlusions%2C%20without%20scenario-specific%20fine-tuning.%20Our%20project%20page%20is%20available%20here%3A%20https%3A//skelsplat.github.io.%0ALink%3A%20http%3A//arxiv.org/abs/2511.08294v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkelSplat%253A%2520Robust%2520Multi-view%25203D%2520Human%2520Pose%2520Estimation%2520with%2520Differentiable%2520Gaussian%2520Rendering%26entry.906535625%3DLaura%2520Bragagnolo%2520and%2520Leonardo%2520Barcellona%2520and%2520Stefano%2520Ghidoni%26entry.1292438233%3DAccurate%25203D%2520human%2520pose%2520estimation%2520is%2520fundamental%2520for%2520applications%2520such%2520as%2520augmented%2520reality%2520and%2520human-robot%2520interaction.%2520State-of-the-art%2520multi-view%2520methods%2520learn%2520to%2520fuse%2520predictions%2520across%2520views%2520by%2520training%2520on%2520large%2520annotated%2520datasets%252C%2520leading%2520to%2520poor%2520generalization%2520when%2520the%2520test%2520scenario%2520differs.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520SkelSplat%252C%2520a%2520novel%2520framework%2520for%2520multi-view%25203D%2520human%2520pose%2520estimation%2520based%2520on%2520differentiable%2520Gaussian%2520rendering.%2520Human%2520pose%2520is%2520modeled%2520as%2520a%2520skeleton%2520of%25203D%2520Gaussians%252C%2520one%2520per%2520joint%252C%2520optimized%2520via%2520differentiable%2520rendering%2520to%2520enable%2520seamless%2520fusion%2520of%2520arbitrary%2520camera%2520views%2520without%25203D%2520ground-truth%2520supervision.%2520Since%2520Gaussian%2520Splatting%2520was%2520originally%2520designed%2520for%2520dense%2520scene%2520reconstruction%252C%2520we%2520propose%2520a%2520novel%2520one-hot%2520encoding%2520scheme%2520that%2520enables%2520independent%2520optimization%2520of%2520human%2520joints.%2520SkelSplat%2520outperforms%2520approaches%2520that%2520do%2520not%2520rely%2520on%25203D%2520ground%2520truth%2520in%2520Human3.6M%2520and%2520CMU%252C%2520while%2520reducing%2520the%2520cross-dataset%2520error%2520up%2520to%252047.8%2525%2520compared%2520to%2520learning-based%2520methods.%2520Experiments%2520on%2520Human3.6M-Occ%2520and%2520Occlusion-Person%2520demonstrate%2520robustness%2520to%2520occlusions%252C%2520without%2520scenario-specific%2520fine-tuning.%2520Our%2520project%2520page%2520is%2520available%2520here%253A%2520https%253A//skelsplat.github.io.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.08294v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SkelSplat%3A%20Robust%20Multi-view%203D%20Human%20Pose%20Estimation%20with%20Differentiable%20Gaussian%20Rendering&entry.906535625=Laura%20Bragagnolo%20and%20Leonardo%20Barcellona%20and%20Stefano%20Ghidoni&entry.1292438233=Accurate%203D%20human%20pose%20estimation%20is%20fundamental%20for%20applications%20such%20as%20augmented%20reality%20and%20human-robot%20interaction.%20State-of-the-art%20multi-view%20methods%20learn%20to%20fuse%20predictions%20across%20views%20by%20training%20on%20large%20annotated%20datasets%2C%20leading%20to%20poor%20generalization%20when%20the%20test%20scenario%20differs.%20To%20overcome%20these%20limitations%2C%20we%20propose%20SkelSplat%2C%20a%20novel%20framework%20for%20multi-view%203D%20human%20pose%20estimation%20based%20on%20differentiable%20Gaussian%20rendering.%20Human%20pose%20is%20modeled%20as%20a%20skeleton%20of%203D%20Gaussians%2C%20one%20per%20joint%2C%20optimized%20via%20differentiable%20rendering%20to%20enable%20seamless%20fusion%20of%20arbitrary%20camera%20views%20without%203D%20ground-truth%20supervision.%20Since%20Gaussian%20Splatting%20was%20originally%20designed%20for%20dense%20scene%20reconstruction%2C%20we%20propose%20a%20novel%20one-hot%20encoding%20scheme%20that%20enables%20independent%20optimization%20of%20human%20joints.%20SkelSplat%20outperforms%20approaches%20that%20do%20not%20rely%20on%203D%20ground%20truth%20in%20Human3.6M%20and%20CMU%2C%20while%20reducing%20the%20cross-dataset%20error%20up%20to%2047.8%25%20compared%20to%20learning-based%20methods.%20Experiments%20on%20Human3.6M-Occ%20and%20Occlusion-Person%20demonstrate%20robustness%20to%20occlusions%2C%20without%20scenario-specific%20fine-tuning.%20Our%20project%20page%20is%20available%20here%3A%20https%3A//skelsplat.github.io.&entry.1838667208=http%3A//arxiv.org/abs/2511.08294v2&entry.124074799=Read"},
{"title": "EGGS: Exchangeable 2D/3D Gaussian Splatting for Geometry-Appearance Balanced Novel View Synthesis", "author": "Yancheng Zhang and Guangyu Sun and Chen Chen", "abstract": "Novel view synthesis (NVS) is crucial in computer vision and graphics, with wide applications in AR, VR, and autonomous driving. While 3D Gaussian Splatting (3DGS) enables real-time rendering with high appearance fidelity, it suffers from multi-view inconsistencies, limiting geometric accuracy. In contrast, 2D Gaussian Splatting (2DGS) enforces multi-view consistency but compromises texture details. To address these limitations, we propose Exchangeable Gaussian Splatting (EGGS), a hybrid representation that integrates 2D and 3D Gaussians to balance appearance and geometry. To achieve this, we introduce Hybrid Gaussian Rasterization for unified rendering, Adaptive Type Exchange for dynamic adaptation between 2D and 3D Gaussians, and Frequency-Decoupled Optimization that effectively exploits the strengths of each type of Gaussian representation. Our CUDA-accelerated implementation ensures efficient training and inference. Extensive experiments demonstrate that EGGS outperforms existing methods in rendering quality, geometric accuracy, and efficiency, providing a practical solution for high-quality NVS.", "link": "http://arxiv.org/abs/2512.02932v1", "date": "2025-12-02", "relevancy": 3.2413, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6682}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6481}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EGGS%3A%20Exchangeable%202D/3D%20Gaussian%20Splatting%20for%20Geometry-Appearance%20Balanced%20Novel%20View%20Synthesis&body=Title%3A%20EGGS%3A%20Exchangeable%202D/3D%20Gaussian%20Splatting%20for%20Geometry-Appearance%20Balanced%20Novel%20View%20Synthesis%0AAuthor%3A%20Yancheng%20Zhang%20and%20Guangyu%20Sun%20and%20Chen%20Chen%0AAbstract%3A%20Novel%20view%20synthesis%20%28NVS%29%20is%20crucial%20in%20computer%20vision%20and%20graphics%2C%20with%20wide%20applications%20in%20AR%2C%20VR%2C%20and%20autonomous%20driving.%20While%203D%20Gaussian%20Splatting%20%283DGS%29%20enables%20real-time%20rendering%20with%20high%20appearance%20fidelity%2C%20it%20suffers%20from%20multi-view%20inconsistencies%2C%20limiting%20geometric%20accuracy.%20In%20contrast%2C%202D%20Gaussian%20Splatting%20%282DGS%29%20enforces%20multi-view%20consistency%20but%20compromises%20texture%20details.%20To%20address%20these%20limitations%2C%20we%20propose%20Exchangeable%20Gaussian%20Splatting%20%28EGGS%29%2C%20a%20hybrid%20representation%20that%20integrates%202D%20and%203D%20Gaussians%20to%20balance%20appearance%20and%20geometry.%20To%20achieve%20this%2C%20we%20introduce%20Hybrid%20Gaussian%20Rasterization%20for%20unified%20rendering%2C%20Adaptive%20Type%20Exchange%20for%20dynamic%20adaptation%20between%202D%20and%203D%20Gaussians%2C%20and%20Frequency-Decoupled%20Optimization%20that%20effectively%20exploits%20the%20strengths%20of%20each%20type%20of%20Gaussian%20representation.%20Our%20CUDA-accelerated%20implementation%20ensures%20efficient%20training%20and%20inference.%20Extensive%20experiments%20demonstrate%20that%20EGGS%20outperforms%20existing%20methods%20in%20rendering%20quality%2C%20geometric%20accuracy%2C%20and%20efficiency%2C%20providing%20a%20practical%20solution%20for%20high-quality%20NVS.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02932v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEGGS%253A%2520Exchangeable%25202D/3D%2520Gaussian%2520Splatting%2520for%2520Geometry-Appearance%2520Balanced%2520Novel%2520View%2520Synthesis%26entry.906535625%3DYancheng%2520Zhang%2520and%2520Guangyu%2520Sun%2520and%2520Chen%2520Chen%26entry.1292438233%3DNovel%2520view%2520synthesis%2520%2528NVS%2529%2520is%2520crucial%2520in%2520computer%2520vision%2520and%2520graphics%252C%2520with%2520wide%2520applications%2520in%2520AR%252C%2520VR%252C%2520and%2520autonomous%2520driving.%2520While%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520enables%2520real-time%2520rendering%2520with%2520high%2520appearance%2520fidelity%252C%2520it%2520suffers%2520from%2520multi-view%2520inconsistencies%252C%2520limiting%2520geometric%2520accuracy.%2520In%2520contrast%252C%25202D%2520Gaussian%2520Splatting%2520%25282DGS%2529%2520enforces%2520multi-view%2520consistency%2520but%2520compromises%2520texture%2520details.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520Exchangeable%2520Gaussian%2520Splatting%2520%2528EGGS%2529%252C%2520a%2520hybrid%2520representation%2520that%2520integrates%25202D%2520and%25203D%2520Gaussians%2520to%2520balance%2520appearance%2520and%2520geometry.%2520To%2520achieve%2520this%252C%2520we%2520introduce%2520Hybrid%2520Gaussian%2520Rasterization%2520for%2520unified%2520rendering%252C%2520Adaptive%2520Type%2520Exchange%2520for%2520dynamic%2520adaptation%2520between%25202D%2520and%25203D%2520Gaussians%252C%2520and%2520Frequency-Decoupled%2520Optimization%2520that%2520effectively%2520exploits%2520the%2520strengths%2520of%2520each%2520type%2520of%2520Gaussian%2520representation.%2520Our%2520CUDA-accelerated%2520implementation%2520ensures%2520efficient%2520training%2520and%2520inference.%2520Extensive%2520experiments%2520demonstrate%2520that%2520EGGS%2520outperforms%2520existing%2520methods%2520in%2520rendering%2520quality%252C%2520geometric%2520accuracy%252C%2520and%2520efficiency%252C%2520providing%2520a%2520practical%2520solution%2520for%2520high-quality%2520NVS.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02932v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EGGS%3A%20Exchangeable%202D/3D%20Gaussian%20Splatting%20for%20Geometry-Appearance%20Balanced%20Novel%20View%20Synthesis&entry.906535625=Yancheng%20Zhang%20and%20Guangyu%20Sun%20and%20Chen%20Chen&entry.1292438233=Novel%20view%20synthesis%20%28NVS%29%20is%20crucial%20in%20computer%20vision%20and%20graphics%2C%20with%20wide%20applications%20in%20AR%2C%20VR%2C%20and%20autonomous%20driving.%20While%203D%20Gaussian%20Splatting%20%283DGS%29%20enables%20real-time%20rendering%20with%20high%20appearance%20fidelity%2C%20it%20suffers%20from%20multi-view%20inconsistencies%2C%20limiting%20geometric%20accuracy.%20In%20contrast%2C%202D%20Gaussian%20Splatting%20%282DGS%29%20enforces%20multi-view%20consistency%20but%20compromises%20texture%20details.%20To%20address%20these%20limitations%2C%20we%20propose%20Exchangeable%20Gaussian%20Splatting%20%28EGGS%29%2C%20a%20hybrid%20representation%20that%20integrates%202D%20and%203D%20Gaussians%20to%20balance%20appearance%20and%20geometry.%20To%20achieve%20this%2C%20we%20introduce%20Hybrid%20Gaussian%20Rasterization%20for%20unified%20rendering%2C%20Adaptive%20Type%20Exchange%20for%20dynamic%20adaptation%20between%202D%20and%203D%20Gaussians%2C%20and%20Frequency-Decoupled%20Optimization%20that%20effectively%20exploits%20the%20strengths%20of%20each%20type%20of%20Gaussian%20representation.%20Our%20CUDA-accelerated%20implementation%20ensures%20efficient%20training%20and%20inference.%20Extensive%20experiments%20demonstrate%20that%20EGGS%20outperforms%20existing%20methods%20in%20rendering%20quality%2C%20geometric%20accuracy%2C%20and%20efficiency%2C%20providing%20a%20practical%20solution%20for%20high-quality%20NVS.&entry.1838667208=http%3A//arxiv.org/abs/2512.02932v1&entry.124074799=Read"},
{"title": "DehazeGS: Seeing Through Fog with 3D Gaussian Splatting", "author": "Jinze Yu and Yiqun Wang and Aiheng Jiang and Zhengda Lu and Jianwei Guo and Yong Li and Hongxing Qin and Xiaopeng Zhang", "abstract": "Current novel view synthesis methods are typically designed for high-quality and clean input images. However, in foggy scenes, scattering and attenuation can significantly degrade the quality of rendering. Although NeRF-based dehazing approaches have been developed, their reliance on deep fully connected neural networks and per-ray sampling strategies leads to high computational costs. Furthermore, NeRF's implicit representation limits its ability to recover fine-grained details from hazy scenes. To overcome these limitations, we propose learning an explicit Gaussian representation to explain the formation mechanism of foggy images through a physically forward rendering process. Our method, DehazeGS, reconstructs and renders fog-free scenes using only multi-view foggy images as input. Specifically, based on the atmospheric scattering model, we simulate the formation of fog by establishing the transmission function directly onto Gaussian primitives via depth-to-transmission mapping. During training, we jointly learn the atmospheric light and scattering coefficients while optimizing the Gaussian representation of foggy scenes. At inference time, we remove the effects of scattering and attenuation in Gaussian distributions and directly render the scene to obtain dehazed views. Experiments on both real-world and synthetic foggy datasets demonstrate that DehazeGS achieves state-of-the-art performance. visualizations are available at https://dehazegs.github.io/", "link": "http://arxiv.org/abs/2501.03659v6", "date": "2025-12-02", "relevancy": 3.2076, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6548}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6427}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.627}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DehazeGS%3A%20Seeing%20Through%20Fog%20with%203D%20Gaussian%20Splatting&body=Title%3A%20DehazeGS%3A%20Seeing%20Through%20Fog%20with%203D%20Gaussian%20Splatting%0AAuthor%3A%20Jinze%20Yu%20and%20Yiqun%20Wang%20and%20Aiheng%20Jiang%20and%20Zhengda%20Lu%20and%20Jianwei%20Guo%20and%20Yong%20Li%20and%20Hongxing%20Qin%20and%20Xiaopeng%20Zhang%0AAbstract%3A%20Current%20novel%20view%20synthesis%20methods%20are%20typically%20designed%20for%20high-quality%20and%20clean%20input%20images.%20However%2C%20in%20foggy%20scenes%2C%20scattering%20and%20attenuation%20can%20significantly%20degrade%20the%20quality%20of%20rendering.%20Although%20NeRF-based%20dehazing%20approaches%20have%20been%20developed%2C%20their%20reliance%20on%20deep%20fully%20connected%20neural%20networks%20and%20per-ray%20sampling%20strategies%20leads%20to%20high%20computational%20costs.%20Furthermore%2C%20NeRF%27s%20implicit%20representation%20limits%20its%20ability%20to%20recover%20fine-grained%20details%20from%20hazy%20scenes.%20To%20overcome%20these%20limitations%2C%20we%20propose%20learning%20an%20explicit%20Gaussian%20representation%20to%20explain%20the%20formation%20mechanism%20of%20foggy%20images%20through%20a%20physically%20forward%20rendering%20process.%20Our%20method%2C%20DehazeGS%2C%20reconstructs%20and%20renders%20fog-free%20scenes%20using%20only%20multi-view%20foggy%20images%20as%20input.%20Specifically%2C%20based%20on%20the%20atmospheric%20scattering%20model%2C%20we%20simulate%20the%20formation%20of%20fog%20by%20establishing%20the%20transmission%20function%20directly%20onto%20Gaussian%20primitives%20via%20depth-to-transmission%20mapping.%20During%20training%2C%20we%20jointly%20learn%20the%20atmospheric%20light%20and%20scattering%20coefficients%20while%20optimizing%20the%20Gaussian%20representation%20of%20foggy%20scenes.%20At%20inference%20time%2C%20we%20remove%20the%20effects%20of%20scattering%20and%20attenuation%20in%20Gaussian%20distributions%20and%20directly%20render%20the%20scene%20to%20obtain%20dehazed%20views.%20Experiments%20on%20both%20real-world%20and%20synthetic%20foggy%20datasets%20demonstrate%20that%20DehazeGS%20achieves%20state-of-the-art%20performance.%20visualizations%20are%20available%20at%20https%3A//dehazegs.github.io/%0ALink%3A%20http%3A//arxiv.org/abs/2501.03659v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDehazeGS%253A%2520Seeing%2520Through%2520Fog%2520with%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DJinze%2520Yu%2520and%2520Yiqun%2520Wang%2520and%2520Aiheng%2520Jiang%2520and%2520Zhengda%2520Lu%2520and%2520Jianwei%2520Guo%2520and%2520Yong%2520Li%2520and%2520Hongxing%2520Qin%2520and%2520Xiaopeng%2520Zhang%26entry.1292438233%3DCurrent%2520novel%2520view%2520synthesis%2520methods%2520are%2520typically%2520designed%2520for%2520high-quality%2520and%2520clean%2520input%2520images.%2520However%252C%2520in%2520foggy%2520scenes%252C%2520scattering%2520and%2520attenuation%2520can%2520significantly%2520degrade%2520the%2520quality%2520of%2520rendering.%2520Although%2520NeRF-based%2520dehazing%2520approaches%2520have%2520been%2520developed%252C%2520their%2520reliance%2520on%2520deep%2520fully%2520connected%2520neural%2520networks%2520and%2520per-ray%2520sampling%2520strategies%2520leads%2520to%2520high%2520computational%2520costs.%2520Furthermore%252C%2520NeRF%2527s%2520implicit%2520representation%2520limits%2520its%2520ability%2520to%2520recover%2520fine-grained%2520details%2520from%2520hazy%2520scenes.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520learning%2520an%2520explicit%2520Gaussian%2520representation%2520to%2520explain%2520the%2520formation%2520mechanism%2520of%2520foggy%2520images%2520through%2520a%2520physically%2520forward%2520rendering%2520process.%2520Our%2520method%252C%2520DehazeGS%252C%2520reconstructs%2520and%2520renders%2520fog-free%2520scenes%2520using%2520only%2520multi-view%2520foggy%2520images%2520as%2520input.%2520Specifically%252C%2520based%2520on%2520the%2520atmospheric%2520scattering%2520model%252C%2520we%2520simulate%2520the%2520formation%2520of%2520fog%2520by%2520establishing%2520the%2520transmission%2520function%2520directly%2520onto%2520Gaussian%2520primitives%2520via%2520depth-to-transmission%2520mapping.%2520During%2520training%252C%2520we%2520jointly%2520learn%2520the%2520atmospheric%2520light%2520and%2520scattering%2520coefficients%2520while%2520optimizing%2520the%2520Gaussian%2520representation%2520of%2520foggy%2520scenes.%2520At%2520inference%2520time%252C%2520we%2520remove%2520the%2520effects%2520of%2520scattering%2520and%2520attenuation%2520in%2520Gaussian%2520distributions%2520and%2520directly%2520render%2520the%2520scene%2520to%2520obtain%2520dehazed%2520views.%2520Experiments%2520on%2520both%2520real-world%2520and%2520synthetic%2520foggy%2520datasets%2520demonstrate%2520that%2520DehazeGS%2520achieves%2520state-of-the-art%2520performance.%2520visualizations%2520are%2520available%2520at%2520https%253A//dehazegs.github.io/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.03659v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DehazeGS%3A%20Seeing%20Through%20Fog%20with%203D%20Gaussian%20Splatting&entry.906535625=Jinze%20Yu%20and%20Yiqun%20Wang%20and%20Aiheng%20Jiang%20and%20Zhengda%20Lu%20and%20Jianwei%20Guo%20and%20Yong%20Li%20and%20Hongxing%20Qin%20and%20Xiaopeng%20Zhang&entry.1292438233=Current%20novel%20view%20synthesis%20methods%20are%20typically%20designed%20for%20high-quality%20and%20clean%20input%20images.%20However%2C%20in%20foggy%20scenes%2C%20scattering%20and%20attenuation%20can%20significantly%20degrade%20the%20quality%20of%20rendering.%20Although%20NeRF-based%20dehazing%20approaches%20have%20been%20developed%2C%20their%20reliance%20on%20deep%20fully%20connected%20neural%20networks%20and%20per-ray%20sampling%20strategies%20leads%20to%20high%20computational%20costs.%20Furthermore%2C%20NeRF%27s%20implicit%20representation%20limits%20its%20ability%20to%20recover%20fine-grained%20details%20from%20hazy%20scenes.%20To%20overcome%20these%20limitations%2C%20we%20propose%20learning%20an%20explicit%20Gaussian%20representation%20to%20explain%20the%20formation%20mechanism%20of%20foggy%20images%20through%20a%20physically%20forward%20rendering%20process.%20Our%20method%2C%20DehazeGS%2C%20reconstructs%20and%20renders%20fog-free%20scenes%20using%20only%20multi-view%20foggy%20images%20as%20input.%20Specifically%2C%20based%20on%20the%20atmospheric%20scattering%20model%2C%20we%20simulate%20the%20formation%20of%20fog%20by%20establishing%20the%20transmission%20function%20directly%20onto%20Gaussian%20primitives%20via%20depth-to-transmission%20mapping.%20During%20training%2C%20we%20jointly%20learn%20the%20atmospheric%20light%20and%20scattering%20coefficients%20while%20optimizing%20the%20Gaussian%20representation%20of%20foggy%20scenes.%20At%20inference%20time%2C%20we%20remove%20the%20effects%20of%20scattering%20and%20attenuation%20in%20Gaussian%20distributions%20and%20directly%20render%20the%20scene%20to%20obtain%20dehazed%20views.%20Experiments%20on%20both%20real-world%20and%20synthetic%20foggy%20datasets%20demonstrate%20that%20DehazeGS%20achieves%20state-of-the-art%20performance.%20visualizations%20are%20available%20at%20https%3A//dehazegs.github.io/&entry.1838667208=http%3A//arxiv.org/abs/2501.03659v6&entry.124074799=Read"},
{"title": "Content-Aware Texturing for Gaussian Splatting", "author": "Panagiotis Papantonakis and Georgios Kopanas and Fredo Durand and George Drettakis", "abstract": "Gaussian Splatting has become the method of choice for 3D reconstruction and real-time rendering of captured real scenes. However, fine appearance details need to be represented as a large number of small Gaussian primitives, which can be wasteful when geometry and appearance exhibit different frequency characteristics.\n  Inspired by the long tradition of texture mapping, we propose to use texture to represent detailed appearance where possible. Our main focus is to incorporate per-primitive texture maps that adapt to the scene in a principled manner during Gaussian Splatting optimization. We do this by proposing a new appearance representation for 2D Gaussian primitives with textures where the size of a texel is bounded by the image sampling frequency and adapted to the content of the input images. We achieve this by adaptively upscaling or downscaling the texture resolution during optimization. In addition, our approach enables control of the number of primitives during optimization based on texture resolution. We show that our approach performs favorably in image quality and total number of parameters used compared to alternative solutions for textured Gaussian primitives. Project page: https://repo-sam.inria.fr/nerphys/gs-texturing/", "link": "http://arxiv.org/abs/2512.02621v1", "date": "2025-12-02", "relevancy": 3.1825, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6705}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6319}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Content-Aware%20Texturing%20for%20Gaussian%20Splatting&body=Title%3A%20Content-Aware%20Texturing%20for%20Gaussian%20Splatting%0AAuthor%3A%20Panagiotis%20Papantonakis%20and%20Georgios%20Kopanas%20and%20Fredo%20Durand%20and%20George%20Drettakis%0AAbstract%3A%20Gaussian%20Splatting%20has%20become%20the%20method%20of%20choice%20for%203D%20reconstruction%20and%20real-time%20rendering%20of%20captured%20real%20scenes.%20However%2C%20fine%20appearance%20details%20need%20to%20be%20represented%20as%20a%20large%20number%20of%20small%20Gaussian%20primitives%2C%20which%20can%20be%20wasteful%20when%20geometry%20and%20appearance%20exhibit%20different%20frequency%20characteristics.%0A%20%20Inspired%20by%20the%20long%20tradition%20of%20texture%20mapping%2C%20we%20propose%20to%20use%20texture%20to%20represent%20detailed%20appearance%20where%20possible.%20Our%20main%20focus%20is%20to%20incorporate%20per-primitive%20texture%20maps%20that%20adapt%20to%20the%20scene%20in%20a%20principled%20manner%20during%20Gaussian%20Splatting%20optimization.%20We%20do%20this%20by%20proposing%20a%20new%20appearance%20representation%20for%202D%20Gaussian%20primitives%20with%20textures%20where%20the%20size%20of%20a%20texel%20is%20bounded%20by%20the%20image%20sampling%20frequency%20and%20adapted%20to%20the%20content%20of%20the%20input%20images.%20We%20achieve%20this%20by%20adaptively%20upscaling%20or%20downscaling%20the%20texture%20resolution%20during%20optimization.%20In%20addition%2C%20our%20approach%20enables%20control%20of%20the%20number%20of%20primitives%20during%20optimization%20based%20on%20texture%20resolution.%20We%20show%20that%20our%20approach%20performs%20favorably%20in%20image%20quality%20and%20total%20number%20of%20parameters%20used%20compared%20to%20alternative%20solutions%20for%20textured%20Gaussian%20primitives.%20Project%20page%3A%20https%3A//repo-sam.inria.fr/nerphys/gs-texturing/%0ALink%3A%20http%3A//arxiv.org/abs/2512.02621v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContent-Aware%2520Texturing%2520for%2520Gaussian%2520Splatting%26entry.906535625%3DPanagiotis%2520Papantonakis%2520and%2520Georgios%2520Kopanas%2520and%2520Fredo%2520Durand%2520and%2520George%2520Drettakis%26entry.1292438233%3DGaussian%2520Splatting%2520has%2520become%2520the%2520method%2520of%2520choice%2520for%25203D%2520reconstruction%2520and%2520real-time%2520rendering%2520of%2520captured%2520real%2520scenes.%2520However%252C%2520fine%2520appearance%2520details%2520need%2520to%2520be%2520represented%2520as%2520a%2520large%2520number%2520of%2520small%2520Gaussian%2520primitives%252C%2520which%2520can%2520be%2520wasteful%2520when%2520geometry%2520and%2520appearance%2520exhibit%2520different%2520frequency%2520characteristics.%250A%2520%2520Inspired%2520by%2520the%2520long%2520tradition%2520of%2520texture%2520mapping%252C%2520we%2520propose%2520to%2520use%2520texture%2520to%2520represent%2520detailed%2520appearance%2520where%2520possible.%2520Our%2520main%2520focus%2520is%2520to%2520incorporate%2520per-primitive%2520texture%2520maps%2520that%2520adapt%2520to%2520the%2520scene%2520in%2520a%2520principled%2520manner%2520during%2520Gaussian%2520Splatting%2520optimization.%2520We%2520do%2520this%2520by%2520proposing%2520a%2520new%2520appearance%2520representation%2520for%25202D%2520Gaussian%2520primitives%2520with%2520textures%2520where%2520the%2520size%2520of%2520a%2520texel%2520is%2520bounded%2520by%2520the%2520image%2520sampling%2520frequency%2520and%2520adapted%2520to%2520the%2520content%2520of%2520the%2520input%2520images.%2520We%2520achieve%2520this%2520by%2520adaptively%2520upscaling%2520or%2520downscaling%2520the%2520texture%2520resolution%2520during%2520optimization.%2520In%2520addition%252C%2520our%2520approach%2520enables%2520control%2520of%2520the%2520number%2520of%2520primitives%2520during%2520optimization%2520based%2520on%2520texture%2520resolution.%2520We%2520show%2520that%2520our%2520approach%2520performs%2520favorably%2520in%2520image%2520quality%2520and%2520total%2520number%2520of%2520parameters%2520used%2520compared%2520to%2520alternative%2520solutions%2520for%2520textured%2520Gaussian%2520primitives.%2520Project%2520page%253A%2520https%253A//repo-sam.inria.fr/nerphys/gs-texturing/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02621v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Content-Aware%20Texturing%20for%20Gaussian%20Splatting&entry.906535625=Panagiotis%20Papantonakis%20and%20Georgios%20Kopanas%20and%20Fredo%20Durand%20and%20George%20Drettakis&entry.1292438233=Gaussian%20Splatting%20has%20become%20the%20method%20of%20choice%20for%203D%20reconstruction%20and%20real-time%20rendering%20of%20captured%20real%20scenes.%20However%2C%20fine%20appearance%20details%20need%20to%20be%20represented%20as%20a%20large%20number%20of%20small%20Gaussian%20primitives%2C%20which%20can%20be%20wasteful%20when%20geometry%20and%20appearance%20exhibit%20different%20frequency%20characteristics.%0A%20%20Inspired%20by%20the%20long%20tradition%20of%20texture%20mapping%2C%20we%20propose%20to%20use%20texture%20to%20represent%20detailed%20appearance%20where%20possible.%20Our%20main%20focus%20is%20to%20incorporate%20per-primitive%20texture%20maps%20that%20adapt%20to%20the%20scene%20in%20a%20principled%20manner%20during%20Gaussian%20Splatting%20optimization.%20We%20do%20this%20by%20proposing%20a%20new%20appearance%20representation%20for%202D%20Gaussian%20primitives%20with%20textures%20where%20the%20size%20of%20a%20texel%20is%20bounded%20by%20the%20image%20sampling%20frequency%20and%20adapted%20to%20the%20content%20of%20the%20input%20images.%20We%20achieve%20this%20by%20adaptively%20upscaling%20or%20downscaling%20the%20texture%20resolution%20during%20optimization.%20In%20addition%2C%20our%20approach%20enables%20control%20of%20the%20number%20of%20primitives%20during%20optimization%20based%20on%20texture%20resolution.%20We%20show%20that%20our%20approach%20performs%20favorably%20in%20image%20quality%20and%20total%20number%20of%20parameters%20used%20compared%20to%20alternative%20solutions%20for%20textured%20Gaussian%20primitives.%20Project%20page%3A%20https%3A//repo-sam.inria.fr/nerphys/gs-texturing/&entry.1838667208=http%3A//arxiv.org/abs/2512.02621v1&entry.124074799=Read"},
{"title": "Ov3R: Open-Vocabulary Semantic 3D Reconstruction from RGB Videos", "author": "Ziren Gong and Xiaohan Li and Fabio Tosi and Jiawei Han and Stefano Mattoccia and Jianfei Cai and Matteo Poggi", "abstract": "We present Ov3R, a novel framework for open-vocabulary semantic 3D reconstruction from RGB video streams, designed to advance Spatial AI. The system features two key components: CLIP3R, a CLIP-informed 3D reconstruction module that predicts dense point maps from overlapping clips while embedding object-level semantics; and 2D-3D OVS, a 2D-3D open-vocabulary semantic module that lifts 2D features into 3D by learning fused descriptors integrating spatial, geometric, and semantic cues. Unlike prior methods, Ov3R incorporates CLIP semantics directly into the reconstruction process, enabling globally consistent geometry and fine-grained semantic alignment. Our framework achieves state-of-the-art performance in both dense 3D reconstruction and open-vocabulary 3D segmentation, marking a step forward toward real-time, semantics-aware Spatial AI.", "link": "http://arxiv.org/abs/2507.22052v2", "date": "2025-12-02", "relevancy": 3.1787, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6365}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6365}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ov3R%3A%20Open-Vocabulary%20Semantic%203D%20Reconstruction%20from%20RGB%20Videos&body=Title%3A%20Ov3R%3A%20Open-Vocabulary%20Semantic%203D%20Reconstruction%20from%20RGB%20Videos%0AAuthor%3A%20Ziren%20Gong%20and%20Xiaohan%20Li%20and%20Fabio%20Tosi%20and%20Jiawei%20Han%20and%20Stefano%20Mattoccia%20and%20Jianfei%20Cai%20and%20Matteo%20Poggi%0AAbstract%3A%20We%20present%20Ov3R%2C%20a%20novel%20framework%20for%20open-vocabulary%20semantic%203D%20reconstruction%20from%20RGB%20video%20streams%2C%20designed%20to%20advance%20Spatial%20AI.%20The%20system%20features%20two%20key%20components%3A%20CLIP3R%2C%20a%20CLIP-informed%203D%20reconstruction%20module%20that%20predicts%20dense%20point%20maps%20from%20overlapping%20clips%20while%20embedding%20object-level%20semantics%3B%20and%202D-3D%20OVS%2C%20a%202D-3D%20open-vocabulary%20semantic%20module%20that%20lifts%202D%20features%20into%203D%20by%20learning%20fused%20descriptors%20integrating%20spatial%2C%20geometric%2C%20and%20semantic%20cues.%20Unlike%20prior%20methods%2C%20Ov3R%20incorporates%20CLIP%20semantics%20directly%20into%20the%20reconstruction%20process%2C%20enabling%20globally%20consistent%20geometry%20and%20fine-grained%20semantic%20alignment.%20Our%20framework%20achieves%20state-of-the-art%20performance%20in%20both%20dense%203D%20reconstruction%20and%20open-vocabulary%203D%20segmentation%2C%20marking%20a%20step%20forward%20toward%20real-time%2C%20semantics-aware%20Spatial%20AI.%0ALink%3A%20http%3A//arxiv.org/abs/2507.22052v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOv3R%253A%2520Open-Vocabulary%2520Semantic%25203D%2520Reconstruction%2520from%2520RGB%2520Videos%26entry.906535625%3DZiren%2520Gong%2520and%2520Xiaohan%2520Li%2520and%2520Fabio%2520Tosi%2520and%2520Jiawei%2520Han%2520and%2520Stefano%2520Mattoccia%2520and%2520Jianfei%2520Cai%2520and%2520Matteo%2520Poggi%26entry.1292438233%3DWe%2520present%2520Ov3R%252C%2520a%2520novel%2520framework%2520for%2520open-vocabulary%2520semantic%25203D%2520reconstruction%2520from%2520RGB%2520video%2520streams%252C%2520designed%2520to%2520advance%2520Spatial%2520AI.%2520The%2520system%2520features%2520two%2520key%2520components%253A%2520CLIP3R%252C%2520a%2520CLIP-informed%25203D%2520reconstruction%2520module%2520that%2520predicts%2520dense%2520point%2520maps%2520from%2520overlapping%2520clips%2520while%2520embedding%2520object-level%2520semantics%253B%2520and%25202D-3D%2520OVS%252C%2520a%25202D-3D%2520open-vocabulary%2520semantic%2520module%2520that%2520lifts%25202D%2520features%2520into%25203D%2520by%2520learning%2520fused%2520descriptors%2520integrating%2520spatial%252C%2520geometric%252C%2520and%2520semantic%2520cues.%2520Unlike%2520prior%2520methods%252C%2520Ov3R%2520incorporates%2520CLIP%2520semantics%2520directly%2520into%2520the%2520reconstruction%2520process%252C%2520enabling%2520globally%2520consistent%2520geometry%2520and%2520fine-grained%2520semantic%2520alignment.%2520Our%2520framework%2520achieves%2520state-of-the-art%2520performance%2520in%2520both%2520dense%25203D%2520reconstruction%2520and%2520open-vocabulary%25203D%2520segmentation%252C%2520marking%2520a%2520step%2520forward%2520toward%2520real-time%252C%2520semantics-aware%2520Spatial%2520AI.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.22052v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ov3R%3A%20Open-Vocabulary%20Semantic%203D%20Reconstruction%20from%20RGB%20Videos&entry.906535625=Ziren%20Gong%20and%20Xiaohan%20Li%20and%20Fabio%20Tosi%20and%20Jiawei%20Han%20and%20Stefano%20Mattoccia%20and%20Jianfei%20Cai%20and%20Matteo%20Poggi&entry.1292438233=We%20present%20Ov3R%2C%20a%20novel%20framework%20for%20open-vocabulary%20semantic%203D%20reconstruction%20from%20RGB%20video%20streams%2C%20designed%20to%20advance%20Spatial%20AI.%20The%20system%20features%20two%20key%20components%3A%20CLIP3R%2C%20a%20CLIP-informed%203D%20reconstruction%20module%20that%20predicts%20dense%20point%20maps%20from%20overlapping%20clips%20while%20embedding%20object-level%20semantics%3B%20and%202D-3D%20OVS%2C%20a%202D-3D%20open-vocabulary%20semantic%20module%20that%20lifts%202D%20features%20into%203D%20by%20learning%20fused%20descriptors%20integrating%20spatial%2C%20geometric%2C%20and%20semantic%20cues.%20Unlike%20prior%20methods%2C%20Ov3R%20incorporates%20CLIP%20semantics%20directly%20into%20the%20reconstruction%20process%2C%20enabling%20globally%20consistent%20geometry%20and%20fine-grained%20semantic%20alignment.%20Our%20framework%20achieves%20state-of-the-art%20performance%20in%20both%20dense%203D%20reconstruction%20and%20open-vocabulary%203D%20segmentation%2C%20marking%20a%20step%20forward%20toward%20real-time%2C%20semantics-aware%20Spatial%20AI.&entry.1838667208=http%3A//arxiv.org/abs/2507.22052v2&entry.124074799=Read"},
{"title": "PoreTrack3D: A Benchmark for Dynamic 3D Gaussian Splatting in Pore-Scale Facial Trajectory Tracking", "author": "Dong Li and Jiahao Xiong and Yingda Huang and Le Chang", "abstract": "We introduce PoreTrack3D, the first benchmark for dynamic 3D Gaussian splatting in pore-scale, non-rigid 3D facial trajectory tracking. It contains over 440,000 facial trajectories in total, among which more than 52,000 are longer than 10 frames, including 68 manually reviewed trajectories that span the entire 150 frames. To the best of our knowledge, PoreTrack3D is the first benchmark dataset to capture both traditional facial landmarks and pore-scale keypoints trajectory, advancing the study of fine-grained facial expressions through the analysis of subtle skin-surface motion. We systematically evaluate state-of-the-art dynamic 3D Gaussian splatting methods on PoreTrack3D, establishing the first performance baseline in this domain. Overall, the pipeline developed for this benchmark dataset's creation establishes a new framework for high-fidelity facial motion capture and dynamic 3D reconstruction. Our dataset are publicly available at: https://github.com/JHXion9/PoreTrack3D", "link": "http://arxiv.org/abs/2512.02648v1", "date": "2025-12-02", "relevancy": 3.0476, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6116}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6105}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PoreTrack3D%3A%20A%20Benchmark%20for%20Dynamic%203D%20Gaussian%20Splatting%20in%20Pore-Scale%20Facial%20Trajectory%20Tracking&body=Title%3A%20PoreTrack3D%3A%20A%20Benchmark%20for%20Dynamic%203D%20Gaussian%20Splatting%20in%20Pore-Scale%20Facial%20Trajectory%20Tracking%0AAuthor%3A%20Dong%20Li%20and%20Jiahao%20Xiong%20and%20Yingda%20Huang%20and%20Le%20Chang%0AAbstract%3A%20We%20introduce%20PoreTrack3D%2C%20the%20first%20benchmark%20for%20dynamic%203D%20Gaussian%20splatting%20in%20pore-scale%2C%20non-rigid%203D%20facial%20trajectory%20tracking.%20It%20contains%20over%20440%2C000%20facial%20trajectories%20in%20total%2C%20among%20which%20more%20than%2052%2C000%20are%20longer%20than%2010%20frames%2C%20including%2068%20manually%20reviewed%20trajectories%20that%20span%20the%20entire%20150%20frames.%20To%20the%20best%20of%20our%20knowledge%2C%20PoreTrack3D%20is%20the%20first%20benchmark%20dataset%20to%20capture%20both%20traditional%20facial%20landmarks%20and%20pore-scale%20keypoints%20trajectory%2C%20advancing%20the%20study%20of%20fine-grained%20facial%20expressions%20through%20the%20analysis%20of%20subtle%20skin-surface%20motion.%20We%20systematically%20evaluate%20state-of-the-art%20dynamic%203D%20Gaussian%20splatting%20methods%20on%20PoreTrack3D%2C%20establishing%20the%20first%20performance%20baseline%20in%20this%20domain.%20Overall%2C%20the%20pipeline%20developed%20for%20this%20benchmark%20dataset%27s%20creation%20establishes%20a%20new%20framework%20for%20high-fidelity%20facial%20motion%20capture%20and%20dynamic%203D%20reconstruction.%20Our%20dataset%20are%20publicly%20available%20at%3A%20https%3A//github.com/JHXion9/PoreTrack3D%0ALink%3A%20http%3A//arxiv.org/abs/2512.02648v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoreTrack3D%253A%2520A%2520Benchmark%2520for%2520Dynamic%25203D%2520Gaussian%2520Splatting%2520in%2520Pore-Scale%2520Facial%2520Trajectory%2520Tracking%26entry.906535625%3DDong%2520Li%2520and%2520Jiahao%2520Xiong%2520and%2520Yingda%2520Huang%2520and%2520Le%2520Chang%26entry.1292438233%3DWe%2520introduce%2520PoreTrack3D%252C%2520the%2520first%2520benchmark%2520for%2520dynamic%25203D%2520Gaussian%2520splatting%2520in%2520pore-scale%252C%2520non-rigid%25203D%2520facial%2520trajectory%2520tracking.%2520It%2520contains%2520over%2520440%252C000%2520facial%2520trajectories%2520in%2520total%252C%2520among%2520which%2520more%2520than%252052%252C000%2520are%2520longer%2520than%252010%2520frames%252C%2520including%252068%2520manually%2520reviewed%2520trajectories%2520that%2520span%2520the%2520entire%2520150%2520frames.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520PoreTrack3D%2520is%2520the%2520first%2520benchmark%2520dataset%2520to%2520capture%2520both%2520traditional%2520facial%2520landmarks%2520and%2520pore-scale%2520keypoints%2520trajectory%252C%2520advancing%2520the%2520study%2520of%2520fine-grained%2520facial%2520expressions%2520through%2520the%2520analysis%2520of%2520subtle%2520skin-surface%2520motion.%2520We%2520systematically%2520evaluate%2520state-of-the-art%2520dynamic%25203D%2520Gaussian%2520splatting%2520methods%2520on%2520PoreTrack3D%252C%2520establishing%2520the%2520first%2520performance%2520baseline%2520in%2520this%2520domain.%2520Overall%252C%2520the%2520pipeline%2520developed%2520for%2520this%2520benchmark%2520dataset%2527s%2520creation%2520establishes%2520a%2520new%2520framework%2520for%2520high-fidelity%2520facial%2520motion%2520capture%2520and%2520dynamic%25203D%2520reconstruction.%2520Our%2520dataset%2520are%2520publicly%2520available%2520at%253A%2520https%253A//github.com/JHXion9/PoreTrack3D%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02648v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PoreTrack3D%3A%20A%20Benchmark%20for%20Dynamic%203D%20Gaussian%20Splatting%20in%20Pore-Scale%20Facial%20Trajectory%20Tracking&entry.906535625=Dong%20Li%20and%20Jiahao%20Xiong%20and%20Yingda%20Huang%20and%20Le%20Chang&entry.1292438233=We%20introduce%20PoreTrack3D%2C%20the%20first%20benchmark%20for%20dynamic%203D%20Gaussian%20splatting%20in%20pore-scale%2C%20non-rigid%203D%20facial%20trajectory%20tracking.%20It%20contains%20over%20440%2C000%20facial%20trajectories%20in%20total%2C%20among%20which%20more%20than%2052%2C000%20are%20longer%20than%2010%20frames%2C%20including%2068%20manually%20reviewed%20trajectories%20that%20span%20the%20entire%20150%20frames.%20To%20the%20best%20of%20our%20knowledge%2C%20PoreTrack3D%20is%20the%20first%20benchmark%20dataset%20to%20capture%20both%20traditional%20facial%20landmarks%20and%20pore-scale%20keypoints%20trajectory%2C%20advancing%20the%20study%20of%20fine-grained%20facial%20expressions%20through%20the%20analysis%20of%20subtle%20skin-surface%20motion.%20We%20systematically%20evaluate%20state-of-the-art%20dynamic%203D%20Gaussian%20splatting%20methods%20on%20PoreTrack3D%2C%20establishing%20the%20first%20performance%20baseline%20in%20this%20domain.%20Overall%2C%20the%20pipeline%20developed%20for%20this%20benchmark%20dataset%27s%20creation%20establishes%20a%20new%20framework%20for%20high-fidelity%20facial%20motion%20capture%20and%20dynamic%203D%20reconstruction.%20Our%20dataset%20are%20publicly%20available%20at%3A%20https%3A//github.com/JHXion9/PoreTrack3D&entry.1838667208=http%3A//arxiv.org/abs/2512.02648v1&entry.124074799=Read"},
{"title": "Polar Perspectives: Evaluating 2-D LiDAR Projections for Robust Place Recognition with Visual Foundation Models", "author": "Pierpaolo Serio and Giulio Pisaneschi and Andrea Dan Ryals and Vincenzo Infantino and Lorenzo Gentilini and Valentina Donzella and Lorenzo Pollini", "abstract": "This work presents a systematic investigation into how alternative LiDAR-to-image projections affect metric place recognition when coupled with a state-of-the-art vision foundation model. We introduce a modular retrieval pipeline that controls for backbone, aggregation, and evaluation protocol, thereby isolating the influence of the 2-D projection itself. Using consistent geometric and structural channels across multiple datasets and deployment scenarios, we identify the projection characteristics that most strongly determine discriminative power, robustness to environmental variation, and suitability for real-time autonomy. Experiments with different datasets, including integration into an operational place recognition policy, validate the practical relevance of these findings and demonstrate that carefully designed projections can serve as an effective surrogate for end-to-end 3-D learning in LiDAR place recognition.", "link": "http://arxiv.org/abs/2512.02897v1", "date": "2025-12-02", "relevancy": 3.041, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6195}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6195}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Polar%20Perspectives%3A%20Evaluating%202-D%20LiDAR%20Projections%20for%20Robust%20Place%20Recognition%20with%20Visual%20Foundation%20Models&body=Title%3A%20Polar%20Perspectives%3A%20Evaluating%202-D%20LiDAR%20Projections%20for%20Robust%20Place%20Recognition%20with%20Visual%20Foundation%20Models%0AAuthor%3A%20Pierpaolo%20Serio%20and%20Giulio%20Pisaneschi%20and%20Andrea%20Dan%20Ryals%20and%20Vincenzo%20Infantino%20and%20Lorenzo%20Gentilini%20and%20Valentina%20Donzella%20and%20Lorenzo%20Pollini%0AAbstract%3A%20This%20work%20presents%20a%20systematic%20investigation%20into%20how%20alternative%20LiDAR-to-image%20projections%20affect%20metric%20place%20recognition%20when%20coupled%20with%20a%20state-of-the-art%20vision%20foundation%20model.%20We%20introduce%20a%20modular%20retrieval%20pipeline%20that%20controls%20for%20backbone%2C%20aggregation%2C%20and%20evaluation%20protocol%2C%20thereby%20isolating%20the%20influence%20of%20the%202-D%20projection%20itself.%20Using%20consistent%20geometric%20and%20structural%20channels%20across%20multiple%20datasets%20and%20deployment%20scenarios%2C%20we%20identify%20the%20projection%20characteristics%20that%20most%20strongly%20determine%20discriminative%20power%2C%20robustness%20to%20environmental%20variation%2C%20and%20suitability%20for%20real-time%20autonomy.%20Experiments%20with%20different%20datasets%2C%20including%20integration%20into%20an%20operational%20place%20recognition%20policy%2C%20validate%20the%20practical%20relevance%20of%20these%20findings%20and%20demonstrate%20that%20carefully%20designed%20projections%20can%20serve%20as%20an%20effective%20surrogate%20for%20end-to-end%203-D%20learning%20in%20LiDAR%20place%20recognition.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02897v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPolar%2520Perspectives%253A%2520Evaluating%25202-D%2520LiDAR%2520Projections%2520for%2520Robust%2520Place%2520Recognition%2520with%2520Visual%2520Foundation%2520Models%26entry.906535625%3DPierpaolo%2520Serio%2520and%2520Giulio%2520Pisaneschi%2520and%2520Andrea%2520Dan%2520Ryals%2520and%2520Vincenzo%2520Infantino%2520and%2520Lorenzo%2520Gentilini%2520and%2520Valentina%2520Donzella%2520and%2520Lorenzo%2520Pollini%26entry.1292438233%3DThis%2520work%2520presents%2520a%2520systematic%2520investigation%2520into%2520how%2520alternative%2520LiDAR-to-image%2520projections%2520affect%2520metric%2520place%2520recognition%2520when%2520coupled%2520with%2520a%2520state-of-the-art%2520vision%2520foundation%2520model.%2520We%2520introduce%2520a%2520modular%2520retrieval%2520pipeline%2520that%2520controls%2520for%2520backbone%252C%2520aggregation%252C%2520and%2520evaluation%2520protocol%252C%2520thereby%2520isolating%2520the%2520influence%2520of%2520the%25202-D%2520projection%2520itself.%2520Using%2520consistent%2520geometric%2520and%2520structural%2520channels%2520across%2520multiple%2520datasets%2520and%2520deployment%2520scenarios%252C%2520we%2520identify%2520the%2520projection%2520characteristics%2520that%2520most%2520strongly%2520determine%2520discriminative%2520power%252C%2520robustness%2520to%2520environmental%2520variation%252C%2520and%2520suitability%2520for%2520real-time%2520autonomy.%2520Experiments%2520with%2520different%2520datasets%252C%2520including%2520integration%2520into%2520an%2520operational%2520place%2520recognition%2520policy%252C%2520validate%2520the%2520practical%2520relevance%2520of%2520these%2520findings%2520and%2520demonstrate%2520that%2520carefully%2520designed%2520projections%2520can%2520serve%2520as%2520an%2520effective%2520surrogate%2520for%2520end-to-end%25203-D%2520learning%2520in%2520LiDAR%2520place%2520recognition.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02897v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Polar%20Perspectives%3A%20Evaluating%202-D%20LiDAR%20Projections%20for%20Robust%20Place%20Recognition%20with%20Visual%20Foundation%20Models&entry.906535625=Pierpaolo%20Serio%20and%20Giulio%20Pisaneschi%20and%20Andrea%20Dan%20Ryals%20and%20Vincenzo%20Infantino%20and%20Lorenzo%20Gentilini%20and%20Valentina%20Donzella%20and%20Lorenzo%20Pollini&entry.1292438233=This%20work%20presents%20a%20systematic%20investigation%20into%20how%20alternative%20LiDAR-to-image%20projections%20affect%20metric%20place%20recognition%20when%20coupled%20with%20a%20state-of-the-art%20vision%20foundation%20model.%20We%20introduce%20a%20modular%20retrieval%20pipeline%20that%20controls%20for%20backbone%2C%20aggregation%2C%20and%20evaluation%20protocol%2C%20thereby%20isolating%20the%20influence%20of%20the%202-D%20projection%20itself.%20Using%20consistent%20geometric%20and%20structural%20channels%20across%20multiple%20datasets%20and%20deployment%20scenarios%2C%20we%20identify%20the%20projection%20characteristics%20that%20most%20strongly%20determine%20discriminative%20power%2C%20robustness%20to%20environmental%20variation%2C%20and%20suitability%20for%20real-time%20autonomy.%20Experiments%20with%20different%20datasets%2C%20including%20integration%20into%20an%20operational%20place%20recognition%20policy%2C%20validate%20the%20practical%20relevance%20of%20these%20findings%20and%20demonstrate%20that%20carefully%20designed%20projections%20can%20serve%20as%20an%20effective%20surrogate%20for%20end-to-end%203-D%20learning%20in%20LiDAR%20place%20recognition.&entry.1838667208=http%3A//arxiv.org/abs/2512.02897v1&entry.124074799=Read"},
{"title": "PolarGuide-GSDR: 3D Gaussian Splatting Driven by Polarization Priors and Deferred Reflection for Real-World Reflective Scenes", "author": "Derui Shan and Qian Qiao and Hao Lu and Tao Du and Peng Lu", "abstract": "Polarization-aware Neural Radiance Fields (NeRF) enable novel view synthesis of specular-reflection scenes but face challenges in slow training, inefficient rendering, and strong dependencies on material/viewpoint assumptions. However, 3D Gaussian Splatting (3DGS) enables real-time rendering yet struggles with accurate reflection reconstruction from reflection-geometry entanglement, adding a deferred reflection module introduces environment map dependence. We address these limitations by proposing PolarGuide-GSDR, a polarization-forward-guided paradigm establishing a bidirectional coupling mechanism between polarization and 3DGS: first 3DGS's geometric priors are leveraged to resolve polarization ambiguity, and then the refined polarization information cues are used to guide 3DGS's normal and spherical harmonic representation. This process achieves high-fidelity reflection separation and full-scene reconstruction without requiring environment maps or restrictive material assumptions. We demonstrate on public and self-collected datasets that PolarGuide-GSDR achieves state-of-the-art performance in specular reconstruction, normal estimation, and novel view synthesis, all while maintaining real-time rendering capabilities. To our knowledge, this is the first framework embedding polarization priors directly into 3DGS optimization, yielding superior interpretability and real-time performance for modeling complex reflective scenes.", "link": "http://arxiv.org/abs/2512.02664v1", "date": "2025-12-02", "relevancy": 3.0311, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.646}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6149}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PolarGuide-GSDR%3A%203D%20Gaussian%20Splatting%20Driven%20by%20Polarization%20Priors%20and%20Deferred%20Reflection%20for%20Real-World%20Reflective%20Scenes&body=Title%3A%20PolarGuide-GSDR%3A%203D%20Gaussian%20Splatting%20Driven%20by%20Polarization%20Priors%20and%20Deferred%20Reflection%20for%20Real-World%20Reflective%20Scenes%0AAuthor%3A%20Derui%20Shan%20and%20Qian%20Qiao%20and%20Hao%20Lu%20and%20Tao%20Du%20and%20Peng%20Lu%0AAbstract%3A%20Polarization-aware%20Neural%20Radiance%20Fields%20%28NeRF%29%20enable%20novel%20view%20synthesis%20of%20specular-reflection%20scenes%20but%20face%20challenges%20in%20slow%20training%2C%20inefficient%20rendering%2C%20and%20strong%20dependencies%20on%20material/viewpoint%20assumptions.%20However%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20enables%20real-time%20rendering%20yet%20struggles%20with%20accurate%20reflection%20reconstruction%20from%20reflection-geometry%20entanglement%2C%20adding%20a%20deferred%20reflection%20module%20introduces%20environment%20map%20dependence.%20We%20address%20these%20limitations%20by%20proposing%20PolarGuide-GSDR%2C%20a%20polarization-forward-guided%20paradigm%20establishing%20a%20bidirectional%20coupling%20mechanism%20between%20polarization%20and%203DGS%3A%20first%203DGS%27s%20geometric%20priors%20are%20leveraged%20to%20resolve%20polarization%20ambiguity%2C%20and%20then%20the%20refined%20polarization%20information%20cues%20are%20used%20to%20guide%203DGS%27s%20normal%20and%20spherical%20harmonic%20representation.%20This%20process%20achieves%20high-fidelity%20reflection%20separation%20and%20full-scene%20reconstruction%20without%20requiring%20environment%20maps%20or%20restrictive%20material%20assumptions.%20We%20demonstrate%20on%20public%20and%20self-collected%20datasets%20that%20PolarGuide-GSDR%20achieves%20state-of-the-art%20performance%20in%20specular%20reconstruction%2C%20normal%20estimation%2C%20and%20novel%20view%20synthesis%2C%20all%20while%20maintaining%20real-time%20rendering%20capabilities.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20framework%20embedding%20polarization%20priors%20directly%20into%203DGS%20optimization%2C%20yielding%20superior%20interpretability%20and%20real-time%20performance%20for%20modeling%20complex%20reflective%20scenes.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02664v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPolarGuide-GSDR%253A%25203D%2520Gaussian%2520Splatting%2520Driven%2520by%2520Polarization%2520Priors%2520and%2520Deferred%2520Reflection%2520for%2520Real-World%2520Reflective%2520Scenes%26entry.906535625%3DDerui%2520Shan%2520and%2520Qian%2520Qiao%2520and%2520Hao%2520Lu%2520and%2520Tao%2520Du%2520and%2520Peng%2520Lu%26entry.1292438233%3DPolarization-aware%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520enable%2520novel%2520view%2520synthesis%2520of%2520specular-reflection%2520scenes%2520but%2520face%2520challenges%2520in%2520slow%2520training%252C%2520inefficient%2520rendering%252C%2520and%2520strong%2520dependencies%2520on%2520material/viewpoint%2520assumptions.%2520However%252C%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520enables%2520real-time%2520rendering%2520yet%2520struggles%2520with%2520accurate%2520reflection%2520reconstruction%2520from%2520reflection-geometry%2520entanglement%252C%2520adding%2520a%2520deferred%2520reflection%2520module%2520introduces%2520environment%2520map%2520dependence.%2520We%2520address%2520these%2520limitations%2520by%2520proposing%2520PolarGuide-GSDR%252C%2520a%2520polarization-forward-guided%2520paradigm%2520establishing%2520a%2520bidirectional%2520coupling%2520mechanism%2520between%2520polarization%2520and%25203DGS%253A%2520first%25203DGS%2527s%2520geometric%2520priors%2520are%2520leveraged%2520to%2520resolve%2520polarization%2520ambiguity%252C%2520and%2520then%2520the%2520refined%2520polarization%2520information%2520cues%2520are%2520used%2520to%2520guide%25203DGS%2527s%2520normal%2520and%2520spherical%2520harmonic%2520representation.%2520This%2520process%2520achieves%2520high-fidelity%2520reflection%2520separation%2520and%2520full-scene%2520reconstruction%2520without%2520requiring%2520environment%2520maps%2520or%2520restrictive%2520material%2520assumptions.%2520We%2520demonstrate%2520on%2520public%2520and%2520self-collected%2520datasets%2520that%2520PolarGuide-GSDR%2520achieves%2520state-of-the-art%2520performance%2520in%2520specular%2520reconstruction%252C%2520normal%2520estimation%252C%2520and%2520novel%2520view%2520synthesis%252C%2520all%2520while%2520maintaining%2520real-time%2520rendering%2520capabilities.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520framework%2520embedding%2520polarization%2520priors%2520directly%2520into%25203DGS%2520optimization%252C%2520yielding%2520superior%2520interpretability%2520and%2520real-time%2520performance%2520for%2520modeling%2520complex%2520reflective%2520scenes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02664v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PolarGuide-GSDR%3A%203D%20Gaussian%20Splatting%20Driven%20by%20Polarization%20Priors%20and%20Deferred%20Reflection%20for%20Real-World%20Reflective%20Scenes&entry.906535625=Derui%20Shan%20and%20Qian%20Qiao%20and%20Hao%20Lu%20and%20Tao%20Du%20and%20Peng%20Lu&entry.1292438233=Polarization-aware%20Neural%20Radiance%20Fields%20%28NeRF%29%20enable%20novel%20view%20synthesis%20of%20specular-reflection%20scenes%20but%20face%20challenges%20in%20slow%20training%2C%20inefficient%20rendering%2C%20and%20strong%20dependencies%20on%20material/viewpoint%20assumptions.%20However%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20enables%20real-time%20rendering%20yet%20struggles%20with%20accurate%20reflection%20reconstruction%20from%20reflection-geometry%20entanglement%2C%20adding%20a%20deferred%20reflection%20module%20introduces%20environment%20map%20dependence.%20We%20address%20these%20limitations%20by%20proposing%20PolarGuide-GSDR%2C%20a%20polarization-forward-guided%20paradigm%20establishing%20a%20bidirectional%20coupling%20mechanism%20between%20polarization%20and%203DGS%3A%20first%203DGS%27s%20geometric%20priors%20are%20leveraged%20to%20resolve%20polarization%20ambiguity%2C%20and%20then%20the%20refined%20polarization%20information%20cues%20are%20used%20to%20guide%203DGS%27s%20normal%20and%20spherical%20harmonic%20representation.%20This%20process%20achieves%20high-fidelity%20reflection%20separation%20and%20full-scene%20reconstruction%20without%20requiring%20environment%20maps%20or%20restrictive%20material%20assumptions.%20We%20demonstrate%20on%20public%20and%20self-collected%20datasets%20that%20PolarGuide-GSDR%20achieves%20state-of-the-art%20performance%20in%20specular%20reconstruction%2C%20normal%20estimation%2C%20and%20novel%20view%20synthesis%2C%20all%20while%20maintaining%20real-time%20rendering%20capabilities.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20framework%20embedding%20polarization%20priors%20directly%20into%203DGS%20optimization%2C%20yielding%20superior%20interpretability%20and%20real-time%20performance%20for%20modeling%20complex%20reflective%20scenes.&entry.1838667208=http%3A//arxiv.org/abs/2512.02664v1&entry.124074799=Read"},
{"title": "Benchmarking Scientific Understanding and Reasoning for Video Generation using VideoScience-Bench", "author": "Lanxiang Hu and Abhilash Shankarampeta and Yixin Huang and Zilin Dai and Haoyang Yu and Yujie Zhao and Haoqiang Kang and Daniel Zhao and Tajana Rosing and Hao Zhang", "abstract": "The next frontier for video generation lies in developing models capable of zero-shot reasoning, where understanding real-world scientific laws is crucial for accurate physical outcome modeling under diverse conditions. However, existing video benchmarks are physical commonsense-based, offering limited insight into video models' scientific reasoning capability. We introduce VideoScience-Bench, a benchmark designed to evaluate undergraduate-level scientific understanding in video models. Each prompt encodes a composite scientific scenario that requires understanding and reasoning across multiple scientific concepts to generate the correct phenomenon. The benchmark comprises 200 carefully curated prompts spanning 14 topics and 103 concepts in physics and chemistry. We conduct expert-annotated evaluations across seven state-of-the-art video models in T2V and I2V settings along five dimensions: Prompt Consistency, Phenomenon Congruency, Correct Dynamism, Immutability, and Spatio-Temporal Continuity. Using a VLM-as-a-Judge to assess video generations, we observe strong correlation with human assessments. To the best of our knowledge, VideoScience-Bench is the first benchmark to evaluate video models not only as generators but also as reasoners, requiring their generations to demonstrate scientific understanding consistent with expected physical and chemical phenomena. Our data and evaluation code are available at: \\href{https://github.com/hao-ai-lab/VideoScience}{github.com/hao-ai-lab/VideoScience}.", "link": "http://arxiv.org/abs/2512.02942v1", "date": "2025-12-02", "relevancy": 2.9941, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6096}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6058}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Scientific%20Understanding%20and%20Reasoning%20for%20Video%20Generation%20using%20VideoScience-Bench&body=Title%3A%20Benchmarking%20Scientific%20Understanding%20and%20Reasoning%20for%20Video%20Generation%20using%20VideoScience-Bench%0AAuthor%3A%20Lanxiang%20Hu%20and%20Abhilash%20Shankarampeta%20and%20Yixin%20Huang%20and%20Zilin%20Dai%20and%20Haoyang%20Yu%20and%20Yujie%20Zhao%20and%20Haoqiang%20Kang%20and%20Daniel%20Zhao%20and%20Tajana%20Rosing%20and%20Hao%20Zhang%0AAbstract%3A%20The%20next%20frontier%20for%20video%20generation%20lies%20in%20developing%20models%20capable%20of%20zero-shot%20reasoning%2C%20where%20understanding%20real-world%20scientific%20laws%20is%20crucial%20for%20accurate%20physical%20outcome%20modeling%20under%20diverse%20conditions.%20However%2C%20existing%20video%20benchmarks%20are%20physical%20commonsense-based%2C%20offering%20limited%20insight%20into%20video%20models%27%20scientific%20reasoning%20capability.%20We%20introduce%20VideoScience-Bench%2C%20a%20benchmark%20designed%20to%20evaluate%20undergraduate-level%20scientific%20understanding%20in%20video%20models.%20Each%20prompt%20encodes%20a%20composite%20scientific%20scenario%20that%20requires%20understanding%20and%20reasoning%20across%20multiple%20scientific%20concepts%20to%20generate%20the%20correct%20phenomenon.%20The%20benchmark%20comprises%20200%20carefully%20curated%20prompts%20spanning%2014%20topics%20and%20103%20concepts%20in%20physics%20and%20chemistry.%20We%20conduct%20expert-annotated%20evaluations%20across%20seven%20state-of-the-art%20video%20models%20in%20T2V%20and%20I2V%20settings%20along%20five%20dimensions%3A%20Prompt%20Consistency%2C%20Phenomenon%20Congruency%2C%20Correct%20Dynamism%2C%20Immutability%2C%20and%20Spatio-Temporal%20Continuity.%20Using%20a%20VLM-as-a-Judge%20to%20assess%20video%20generations%2C%20we%20observe%20strong%20correlation%20with%20human%20assessments.%20To%20the%20best%20of%20our%20knowledge%2C%20VideoScience-Bench%20is%20the%20first%20benchmark%20to%20evaluate%20video%20models%20not%20only%20as%20generators%20but%20also%20as%20reasoners%2C%20requiring%20their%20generations%20to%20demonstrate%20scientific%20understanding%20consistent%20with%20expected%20physical%20and%20chemical%20phenomena.%20Our%20data%20and%20evaluation%20code%20are%20available%20at%3A%20%5Chref%7Bhttps%3A//github.com/hao-ai-lab/VideoScience%7D%7Bgithub.com/hao-ai-lab/VideoScience%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02942v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Scientific%2520Understanding%2520and%2520Reasoning%2520for%2520Video%2520Generation%2520using%2520VideoScience-Bench%26entry.906535625%3DLanxiang%2520Hu%2520and%2520Abhilash%2520Shankarampeta%2520and%2520Yixin%2520Huang%2520and%2520Zilin%2520Dai%2520and%2520Haoyang%2520Yu%2520and%2520Yujie%2520Zhao%2520and%2520Haoqiang%2520Kang%2520and%2520Daniel%2520Zhao%2520and%2520Tajana%2520Rosing%2520and%2520Hao%2520Zhang%26entry.1292438233%3DThe%2520next%2520frontier%2520for%2520video%2520generation%2520lies%2520in%2520developing%2520models%2520capable%2520of%2520zero-shot%2520reasoning%252C%2520where%2520understanding%2520real-world%2520scientific%2520laws%2520is%2520crucial%2520for%2520accurate%2520physical%2520outcome%2520modeling%2520under%2520diverse%2520conditions.%2520However%252C%2520existing%2520video%2520benchmarks%2520are%2520physical%2520commonsense-based%252C%2520offering%2520limited%2520insight%2520into%2520video%2520models%2527%2520scientific%2520reasoning%2520capability.%2520We%2520introduce%2520VideoScience-Bench%252C%2520a%2520benchmark%2520designed%2520to%2520evaluate%2520undergraduate-level%2520scientific%2520understanding%2520in%2520video%2520models.%2520Each%2520prompt%2520encodes%2520a%2520composite%2520scientific%2520scenario%2520that%2520requires%2520understanding%2520and%2520reasoning%2520across%2520multiple%2520scientific%2520concepts%2520to%2520generate%2520the%2520correct%2520phenomenon.%2520The%2520benchmark%2520comprises%2520200%2520carefully%2520curated%2520prompts%2520spanning%252014%2520topics%2520and%2520103%2520concepts%2520in%2520physics%2520and%2520chemistry.%2520We%2520conduct%2520expert-annotated%2520evaluations%2520across%2520seven%2520state-of-the-art%2520video%2520models%2520in%2520T2V%2520and%2520I2V%2520settings%2520along%2520five%2520dimensions%253A%2520Prompt%2520Consistency%252C%2520Phenomenon%2520Congruency%252C%2520Correct%2520Dynamism%252C%2520Immutability%252C%2520and%2520Spatio-Temporal%2520Continuity.%2520Using%2520a%2520VLM-as-a-Judge%2520to%2520assess%2520video%2520generations%252C%2520we%2520observe%2520strong%2520correlation%2520with%2520human%2520assessments.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520VideoScience-Bench%2520is%2520the%2520first%2520benchmark%2520to%2520evaluate%2520video%2520models%2520not%2520only%2520as%2520generators%2520but%2520also%2520as%2520reasoners%252C%2520requiring%2520their%2520generations%2520to%2520demonstrate%2520scientific%2520understanding%2520consistent%2520with%2520expected%2520physical%2520and%2520chemical%2520phenomena.%2520Our%2520data%2520and%2520evaluation%2520code%2520are%2520available%2520at%253A%2520%255Chref%257Bhttps%253A//github.com/hao-ai-lab/VideoScience%257D%257Bgithub.com/hao-ai-lab/VideoScience%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02942v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Scientific%20Understanding%20and%20Reasoning%20for%20Video%20Generation%20using%20VideoScience-Bench&entry.906535625=Lanxiang%20Hu%20and%20Abhilash%20Shankarampeta%20and%20Yixin%20Huang%20and%20Zilin%20Dai%20and%20Haoyang%20Yu%20and%20Yujie%20Zhao%20and%20Haoqiang%20Kang%20and%20Daniel%20Zhao%20and%20Tajana%20Rosing%20and%20Hao%20Zhang&entry.1292438233=The%20next%20frontier%20for%20video%20generation%20lies%20in%20developing%20models%20capable%20of%20zero-shot%20reasoning%2C%20where%20understanding%20real-world%20scientific%20laws%20is%20crucial%20for%20accurate%20physical%20outcome%20modeling%20under%20diverse%20conditions.%20However%2C%20existing%20video%20benchmarks%20are%20physical%20commonsense-based%2C%20offering%20limited%20insight%20into%20video%20models%27%20scientific%20reasoning%20capability.%20We%20introduce%20VideoScience-Bench%2C%20a%20benchmark%20designed%20to%20evaluate%20undergraduate-level%20scientific%20understanding%20in%20video%20models.%20Each%20prompt%20encodes%20a%20composite%20scientific%20scenario%20that%20requires%20understanding%20and%20reasoning%20across%20multiple%20scientific%20concepts%20to%20generate%20the%20correct%20phenomenon.%20The%20benchmark%20comprises%20200%20carefully%20curated%20prompts%20spanning%2014%20topics%20and%20103%20concepts%20in%20physics%20and%20chemistry.%20We%20conduct%20expert-annotated%20evaluations%20across%20seven%20state-of-the-art%20video%20models%20in%20T2V%20and%20I2V%20settings%20along%20five%20dimensions%3A%20Prompt%20Consistency%2C%20Phenomenon%20Congruency%2C%20Correct%20Dynamism%2C%20Immutability%2C%20and%20Spatio-Temporal%20Continuity.%20Using%20a%20VLM-as-a-Judge%20to%20assess%20video%20generations%2C%20we%20observe%20strong%20correlation%20with%20human%20assessments.%20To%20the%20best%20of%20our%20knowledge%2C%20VideoScience-Bench%20is%20the%20first%20benchmark%20to%20evaluate%20video%20models%20not%20only%20as%20generators%20but%20also%20as%20reasoners%2C%20requiring%20their%20generations%20to%20demonstrate%20scientific%20understanding%20consistent%20with%20expected%20physical%20and%20chemical%20phenomena.%20Our%20data%20and%20evaluation%20code%20are%20available%20at%3A%20%5Chref%7Bhttps%3A//github.com/hao-ai-lab/VideoScience%7D%7Bgithub.com/hao-ai-lab/VideoScience%7D.&entry.1838667208=http%3A//arxiv.org/abs/2512.02942v1&entry.124074799=Read"},
{"title": "Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures", "author": "Marco Bronzini and Carlo Nicolini and Bruno Lepri and Jacopo Staiano and Andrea Passerini", "abstract": "Despite their capabilities, Large Language Models (LLMs) remain opaque with limited understanding of their internal representations. Current interpretability methods either focus on input-oriented feature extraction, such as supervised probes and Sparse Autoencoders (SAEs), or on output distribution inspection, such as logit-oriented approaches. A full understanding of LLM vector spaces, however, requires integrating both perspectives, something existing approaches struggle with due to constraints on latent feature definitions. We introduce the Hyperdimensional Probe, a hybrid supervised probe that combines symbolic representations with neural probing. Leveraging Vector Symbolic Architectures (VSAs) and hypervector algebra, it unifies prior methods: the top-down interpretability of supervised probes, SAE's sparsity-driven proxy space, and output-oriented logit investigation. This allows deeper input-focused feature extraction while supporting output-oriented investigation. Our experiments show that our method consistently extracts meaningful concepts across LLMs, embedding sizes, and setups, uncovering concept-driven patterns in analogy-oriented inference and QA-focused text generation. By supporting joint input-output analysis, this work advances semantic understanding of neural representations while unifying the complementary perspectives of prior methods.", "link": "http://arxiv.org/abs/2509.25045v2", "date": "2025-12-02", "relevancy": 2.9545, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6297}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6297}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5134}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hyperdimensional%20Probe%3A%20Decoding%20LLM%20Representations%20via%20Vector%20Symbolic%20Architectures&body=Title%3A%20Hyperdimensional%20Probe%3A%20Decoding%20LLM%20Representations%20via%20Vector%20Symbolic%20Architectures%0AAuthor%3A%20Marco%20Bronzini%20and%20Carlo%20Nicolini%20and%20Bruno%20Lepri%20and%20Jacopo%20Staiano%20and%20Andrea%20Passerini%0AAbstract%3A%20Despite%20their%20capabilities%2C%20Large%20Language%20Models%20%28LLMs%29%20remain%20opaque%20with%20limited%20understanding%20of%20their%20internal%20representations.%20Current%20interpretability%20methods%20either%20focus%20on%20input-oriented%20feature%20extraction%2C%20such%20as%20supervised%20probes%20and%20Sparse%20Autoencoders%20%28SAEs%29%2C%20or%20on%20output%20distribution%20inspection%2C%20such%20as%20logit-oriented%20approaches.%20A%20full%20understanding%20of%20LLM%20vector%20spaces%2C%20however%2C%20requires%20integrating%20both%20perspectives%2C%20something%20existing%20approaches%20struggle%20with%20due%20to%20constraints%20on%20latent%20feature%20definitions.%20We%20introduce%20the%20Hyperdimensional%20Probe%2C%20a%20hybrid%20supervised%20probe%20that%20combines%20symbolic%20representations%20with%20neural%20probing.%20Leveraging%20Vector%20Symbolic%20Architectures%20%28VSAs%29%20and%20hypervector%20algebra%2C%20it%20unifies%20prior%20methods%3A%20the%20top-down%20interpretability%20of%20supervised%20probes%2C%20SAE%27s%20sparsity-driven%20proxy%20space%2C%20and%20output-oriented%20logit%20investigation.%20This%20allows%20deeper%20input-focused%20feature%20extraction%20while%20supporting%20output-oriented%20investigation.%20Our%20experiments%20show%20that%20our%20method%20consistently%20extracts%20meaningful%20concepts%20across%20LLMs%2C%20embedding%20sizes%2C%20and%20setups%2C%20uncovering%20concept-driven%20patterns%20in%20analogy-oriented%20inference%20and%20QA-focused%20text%20generation.%20By%20supporting%20joint%20input-output%20analysis%2C%20this%20work%20advances%20semantic%20understanding%20of%20neural%20representations%20while%20unifying%20the%20complementary%20perspectives%20of%20prior%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2509.25045v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperdimensional%2520Probe%253A%2520Decoding%2520LLM%2520Representations%2520via%2520Vector%2520Symbolic%2520Architectures%26entry.906535625%3DMarco%2520Bronzini%2520and%2520Carlo%2520Nicolini%2520and%2520Bruno%2520Lepri%2520and%2520Jacopo%2520Staiano%2520and%2520Andrea%2520Passerini%26entry.1292438233%3DDespite%2520their%2520capabilities%252C%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520remain%2520opaque%2520with%2520limited%2520understanding%2520of%2520their%2520internal%2520representations.%2520Current%2520interpretability%2520methods%2520either%2520focus%2520on%2520input-oriented%2520feature%2520extraction%252C%2520such%2520as%2520supervised%2520probes%2520and%2520Sparse%2520Autoencoders%2520%2528SAEs%2529%252C%2520or%2520on%2520output%2520distribution%2520inspection%252C%2520such%2520as%2520logit-oriented%2520approaches.%2520A%2520full%2520understanding%2520of%2520LLM%2520vector%2520spaces%252C%2520however%252C%2520requires%2520integrating%2520both%2520perspectives%252C%2520something%2520existing%2520approaches%2520struggle%2520with%2520due%2520to%2520constraints%2520on%2520latent%2520feature%2520definitions.%2520We%2520introduce%2520the%2520Hyperdimensional%2520Probe%252C%2520a%2520hybrid%2520supervised%2520probe%2520that%2520combines%2520symbolic%2520representations%2520with%2520neural%2520probing.%2520Leveraging%2520Vector%2520Symbolic%2520Architectures%2520%2528VSAs%2529%2520and%2520hypervector%2520algebra%252C%2520it%2520unifies%2520prior%2520methods%253A%2520the%2520top-down%2520interpretability%2520of%2520supervised%2520probes%252C%2520SAE%2527s%2520sparsity-driven%2520proxy%2520space%252C%2520and%2520output-oriented%2520logit%2520investigation.%2520This%2520allows%2520deeper%2520input-focused%2520feature%2520extraction%2520while%2520supporting%2520output-oriented%2520investigation.%2520Our%2520experiments%2520show%2520that%2520our%2520method%2520consistently%2520extracts%2520meaningful%2520concepts%2520across%2520LLMs%252C%2520embedding%2520sizes%252C%2520and%2520setups%252C%2520uncovering%2520concept-driven%2520patterns%2520in%2520analogy-oriented%2520inference%2520and%2520QA-focused%2520text%2520generation.%2520By%2520supporting%2520joint%2520input-output%2520analysis%252C%2520this%2520work%2520advances%2520semantic%2520understanding%2520of%2520neural%2520representations%2520while%2520unifying%2520the%2520complementary%2520perspectives%2520of%2520prior%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.25045v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hyperdimensional%20Probe%3A%20Decoding%20LLM%20Representations%20via%20Vector%20Symbolic%20Architectures&entry.906535625=Marco%20Bronzini%20and%20Carlo%20Nicolini%20and%20Bruno%20Lepri%20and%20Jacopo%20Staiano%20and%20Andrea%20Passerini&entry.1292438233=Despite%20their%20capabilities%2C%20Large%20Language%20Models%20%28LLMs%29%20remain%20opaque%20with%20limited%20understanding%20of%20their%20internal%20representations.%20Current%20interpretability%20methods%20either%20focus%20on%20input-oriented%20feature%20extraction%2C%20such%20as%20supervised%20probes%20and%20Sparse%20Autoencoders%20%28SAEs%29%2C%20or%20on%20output%20distribution%20inspection%2C%20such%20as%20logit-oriented%20approaches.%20A%20full%20understanding%20of%20LLM%20vector%20spaces%2C%20however%2C%20requires%20integrating%20both%20perspectives%2C%20something%20existing%20approaches%20struggle%20with%20due%20to%20constraints%20on%20latent%20feature%20definitions.%20We%20introduce%20the%20Hyperdimensional%20Probe%2C%20a%20hybrid%20supervised%20probe%20that%20combines%20symbolic%20representations%20with%20neural%20probing.%20Leveraging%20Vector%20Symbolic%20Architectures%20%28VSAs%29%20and%20hypervector%20algebra%2C%20it%20unifies%20prior%20methods%3A%20the%20top-down%20interpretability%20of%20supervised%20probes%2C%20SAE%27s%20sparsity-driven%20proxy%20space%2C%20and%20output-oriented%20logit%20investigation.%20This%20allows%20deeper%20input-focused%20feature%20extraction%20while%20supporting%20output-oriented%20investigation.%20Our%20experiments%20show%20that%20our%20method%20consistently%20extracts%20meaningful%20concepts%20across%20LLMs%2C%20embedding%20sizes%2C%20and%20setups%2C%20uncovering%20concept-driven%20patterns%20in%20analogy-oriented%20inference%20and%20QA-focused%20text%20generation.%20By%20supporting%20joint%20input-output%20analysis%2C%20this%20work%20advances%20semantic%20understanding%20of%20neural%20representations%20while%20unifying%20the%20complementary%20perspectives%20of%20prior%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2509.25045v2&entry.124074799=Read"},
{"title": "OneThinker: All-in-one Reasoning Model for Image and Video", "author": "Kaituo Feng and Manyuan Zhang and Hongyu Li and Kaixuan Fan and Shuang Chen and Yilei Jiang and Dian Zheng and Peiwen Sun and Yiyuan Zhang and Haoze Sun and Yan Feng and Peng Pei and Xunliang Cai and Xiangyu Yue", "abstract": "Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.", "link": "http://arxiv.org/abs/2512.03043v1", "date": "2025-12-02", "relevancy": 2.9237, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5882}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5882}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OneThinker%3A%20All-in-one%20Reasoning%20Model%20for%20Image%20and%20Video&body=Title%3A%20OneThinker%3A%20All-in-one%20Reasoning%20Model%20for%20Image%20and%20Video%0AAuthor%3A%20Kaituo%20Feng%20and%20Manyuan%20Zhang%20and%20Hongyu%20Li%20and%20Kaixuan%20Fan%20and%20Shuang%20Chen%20and%20Yilei%20Jiang%20and%20Dian%20Zheng%20and%20Peiwen%20Sun%20and%20Yiyuan%20Zhang%20and%20Haoze%20Sun%20and%20Yan%20Feng%20and%20Peng%20Pei%20and%20Xunliang%20Cai%20and%20Xiangyu%20Yue%0AAbstract%3A%20Reinforcement%20learning%20%28RL%29%20has%20recently%20achieved%20remarkable%20success%20in%20eliciting%20visual%20reasoning%20within%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20However%2C%20existing%20approaches%20typically%20train%20separate%20models%20for%20different%20tasks%20and%20treat%20image%20and%20video%20reasoning%20as%20disjoint%20domains.%20This%20results%20in%20limited%20scalability%20toward%20a%20multimodal%20reasoning%20generalist%2C%20which%20restricts%20practical%20versatility%20and%20hinders%20potential%20knowledge%20sharing%20across%20tasks%20and%20modalities.%20To%20this%20end%2C%20we%20propose%20OneThinker%2C%20an%20all-in-one%20reasoning%20model%20that%20unifies%20image%20and%20video%20understanding%20across%20diverse%20fundamental%20visual%20tasks%2C%20including%20question%20answering%2C%20captioning%2C%20spatial%20and%20temporal%20grounding%2C%20tracking%2C%20and%20segmentation.%20To%20achieve%20this%2C%20we%20construct%20the%20OneThinker-600k%20training%20corpus%20covering%20all%20these%20tasks%20and%20employ%20commercial%20models%20for%20CoT%20annotation%2C%20resulting%20in%20OneThinker-SFT-340k%20for%20SFT%20cold%20start.%20Furthermore%2C%20we%20propose%20EMA-GRPO%20to%20handle%20reward%20heterogeneity%20in%20multi-task%20RL%20by%20tracking%20task-wise%20moving%20averages%20of%20reward%20standard%20deviations%20for%20balanced%20optimization.%20Extensive%20experiments%20on%20diverse%20visual%20benchmarks%20show%20that%20OneThinker%20delivers%20strong%20performance%20on%2031%20benchmarks%2C%20across%2010%20fundamental%20visual%20understanding%20tasks.%20Moreover%2C%20it%20exhibits%20effective%20knowledge%20transfer%20between%20certain%20tasks%20and%20preliminary%20zero-shot%20generalization%20ability%2C%20marking%20a%20step%20toward%20a%20unified%20multimodal%20reasoning%20generalist.%20All%20code%2C%20model%2C%20and%20data%20are%20released.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03043v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOneThinker%253A%2520All-in-one%2520Reasoning%2520Model%2520for%2520Image%2520and%2520Video%26entry.906535625%3DKaituo%2520Feng%2520and%2520Manyuan%2520Zhang%2520and%2520Hongyu%2520Li%2520and%2520Kaixuan%2520Fan%2520and%2520Shuang%2520Chen%2520and%2520Yilei%2520Jiang%2520and%2520Dian%2520Zheng%2520and%2520Peiwen%2520Sun%2520and%2520Yiyuan%2520Zhang%2520and%2520Haoze%2520Sun%2520and%2520Yan%2520Feng%2520and%2520Peng%2520Pei%2520and%2520Xunliang%2520Cai%2520and%2520Xiangyu%2520Yue%26entry.1292438233%3DReinforcement%2520learning%2520%2528RL%2529%2520has%2520recently%2520achieved%2520remarkable%2520success%2520in%2520eliciting%2520visual%2520reasoning%2520within%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529.%2520However%252C%2520existing%2520approaches%2520typically%2520train%2520separate%2520models%2520for%2520different%2520tasks%2520and%2520treat%2520image%2520and%2520video%2520reasoning%2520as%2520disjoint%2520domains.%2520This%2520results%2520in%2520limited%2520scalability%2520toward%2520a%2520multimodal%2520reasoning%2520generalist%252C%2520which%2520restricts%2520practical%2520versatility%2520and%2520hinders%2520potential%2520knowledge%2520sharing%2520across%2520tasks%2520and%2520modalities.%2520To%2520this%2520end%252C%2520we%2520propose%2520OneThinker%252C%2520an%2520all-in-one%2520reasoning%2520model%2520that%2520unifies%2520image%2520and%2520video%2520understanding%2520across%2520diverse%2520fundamental%2520visual%2520tasks%252C%2520including%2520question%2520answering%252C%2520captioning%252C%2520spatial%2520and%2520temporal%2520grounding%252C%2520tracking%252C%2520and%2520segmentation.%2520To%2520achieve%2520this%252C%2520we%2520construct%2520the%2520OneThinker-600k%2520training%2520corpus%2520covering%2520all%2520these%2520tasks%2520and%2520employ%2520commercial%2520models%2520for%2520CoT%2520annotation%252C%2520resulting%2520in%2520OneThinker-SFT-340k%2520for%2520SFT%2520cold%2520start.%2520Furthermore%252C%2520we%2520propose%2520EMA-GRPO%2520to%2520handle%2520reward%2520heterogeneity%2520in%2520multi-task%2520RL%2520by%2520tracking%2520task-wise%2520moving%2520averages%2520of%2520reward%2520standard%2520deviations%2520for%2520balanced%2520optimization.%2520Extensive%2520experiments%2520on%2520diverse%2520visual%2520benchmarks%2520show%2520that%2520OneThinker%2520delivers%2520strong%2520performance%2520on%252031%2520benchmarks%252C%2520across%252010%2520fundamental%2520visual%2520understanding%2520tasks.%2520Moreover%252C%2520it%2520exhibits%2520effective%2520knowledge%2520transfer%2520between%2520certain%2520tasks%2520and%2520preliminary%2520zero-shot%2520generalization%2520ability%252C%2520marking%2520a%2520step%2520toward%2520a%2520unified%2520multimodal%2520reasoning%2520generalist.%2520All%2520code%252C%2520model%252C%2520and%2520data%2520are%2520released.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03043v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OneThinker%3A%20All-in-one%20Reasoning%20Model%20for%20Image%20and%20Video&entry.906535625=Kaituo%20Feng%20and%20Manyuan%20Zhang%20and%20Hongyu%20Li%20and%20Kaixuan%20Fan%20and%20Shuang%20Chen%20and%20Yilei%20Jiang%20and%20Dian%20Zheng%20and%20Peiwen%20Sun%20and%20Yiyuan%20Zhang%20and%20Haoze%20Sun%20and%20Yan%20Feng%20and%20Peng%20Pei%20and%20Xunliang%20Cai%20and%20Xiangyu%20Yue&entry.1292438233=Reinforcement%20learning%20%28RL%29%20has%20recently%20achieved%20remarkable%20success%20in%20eliciting%20visual%20reasoning%20within%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20However%2C%20existing%20approaches%20typically%20train%20separate%20models%20for%20different%20tasks%20and%20treat%20image%20and%20video%20reasoning%20as%20disjoint%20domains.%20This%20results%20in%20limited%20scalability%20toward%20a%20multimodal%20reasoning%20generalist%2C%20which%20restricts%20practical%20versatility%20and%20hinders%20potential%20knowledge%20sharing%20across%20tasks%20and%20modalities.%20To%20this%20end%2C%20we%20propose%20OneThinker%2C%20an%20all-in-one%20reasoning%20model%20that%20unifies%20image%20and%20video%20understanding%20across%20diverse%20fundamental%20visual%20tasks%2C%20including%20question%20answering%2C%20captioning%2C%20spatial%20and%20temporal%20grounding%2C%20tracking%2C%20and%20segmentation.%20To%20achieve%20this%2C%20we%20construct%20the%20OneThinker-600k%20training%20corpus%20covering%20all%20these%20tasks%20and%20employ%20commercial%20models%20for%20CoT%20annotation%2C%20resulting%20in%20OneThinker-SFT-340k%20for%20SFT%20cold%20start.%20Furthermore%2C%20we%20propose%20EMA-GRPO%20to%20handle%20reward%20heterogeneity%20in%20multi-task%20RL%20by%20tracking%20task-wise%20moving%20averages%20of%20reward%20standard%20deviations%20for%20balanced%20optimization.%20Extensive%20experiments%20on%20diverse%20visual%20benchmarks%20show%20that%20OneThinker%20delivers%20strong%20performance%20on%2031%20benchmarks%2C%20across%2010%20fundamental%20visual%20understanding%20tasks.%20Moreover%2C%20it%20exhibits%20effective%20knowledge%20transfer%20between%20certain%20tasks%20and%20preliminary%20zero-shot%20generalization%20ability%2C%20marking%20a%20step%20toward%20a%20unified%20multimodal%20reasoning%20generalist.%20All%20code%2C%20model%2C%20and%20data%20are%20released.&entry.1838667208=http%3A//arxiv.org/abs/2512.03043v1&entry.124074799=Read"},
{"title": "RULER-Bench: Probing Rule-based Reasoning Abilities of Next-level Video Generation Models for Vision Foundation Intelligence", "author": "Xuming He and Zehao Fan and Hengjia Li and Fan Zhuo and Hankun Xu and Senlin Cheng and Di Weng and Haifeng Liu and Can Ye and Boxi Wu", "abstract": "Recent advances in video generation have enabled the synthesis of videos with strong temporal consistency and impressive visual quality, marking a crucial step toward vision foundation models. To evaluate these video generation models, existing benchmarks primarily focus on factors related to visual perception and understanding, like visual aesthetics, instruction adherence, and temporal coherence. However, the rule-based reasoning capabilities of video generation models remain largely unexplored. Although recent studies have carried out preliminary explorations into whether video models can serve as zero-shot learners, they still lack a fine-grained decomposition of reasoning capabilities and a comprehensive evaluation protocol. To address this gap, we introduce RULER-Bench, a benchmark designed to evaluate the reasoning ability of video generation models from the perspective of cognitive rules. Built upon two fundamental paradigms: text-to-video and image-to-video, RULER-Bench covers 40 representative tasks spanning six rule categories with 622 high-quality annotated instances. For the evaluation of each generated video, we construct a checklist covering four metrics and leverage GPT-o3 to assign scores to each question, achieving 85% alignment with human judgements. Extensive experiments show that the state-of-the-art model achieves only 48.87% on the rule coherence metric, highlighting significant room for improvement in the reasoning capability of next-level video models. We expect that the insight obtained from RULER-Bench will facilitate further development of reasoning-aware video generation, advancing video generation models toward vision foundation intelligence.", "link": "http://arxiv.org/abs/2512.02622v1", "date": "2025-12-02", "relevancy": 2.9186, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5933}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5933}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RULER-Bench%3A%20Probing%20Rule-based%20Reasoning%20Abilities%20of%20Next-level%20Video%20Generation%20Models%20for%20Vision%20Foundation%20Intelligence&body=Title%3A%20RULER-Bench%3A%20Probing%20Rule-based%20Reasoning%20Abilities%20of%20Next-level%20Video%20Generation%20Models%20for%20Vision%20Foundation%20Intelligence%0AAuthor%3A%20Xuming%20He%20and%20Zehao%20Fan%20and%20Hengjia%20Li%20and%20Fan%20Zhuo%20and%20Hankun%20Xu%20and%20Senlin%20Cheng%20and%20Di%20Weng%20and%20Haifeng%20Liu%20and%20Can%20Ye%20and%20Boxi%20Wu%0AAbstract%3A%20Recent%20advances%20in%20video%20generation%20have%20enabled%20the%20synthesis%20of%20videos%20with%20strong%20temporal%20consistency%20and%20impressive%20visual%20quality%2C%20marking%20a%20crucial%20step%20toward%20vision%20foundation%20models.%20To%20evaluate%20these%20video%20generation%20models%2C%20existing%20benchmarks%20primarily%20focus%20on%20factors%20related%20to%20visual%20perception%20and%20understanding%2C%20like%20visual%20aesthetics%2C%20instruction%20adherence%2C%20and%20temporal%20coherence.%20However%2C%20the%20rule-based%20reasoning%20capabilities%20of%20video%20generation%20models%20remain%20largely%20unexplored.%20Although%20recent%20studies%20have%20carried%20out%20preliminary%20explorations%20into%20whether%20video%20models%20can%20serve%20as%20zero-shot%20learners%2C%20they%20still%20lack%20a%20fine-grained%20decomposition%20of%20reasoning%20capabilities%20and%20a%20comprehensive%20evaluation%20protocol.%20To%20address%20this%20gap%2C%20we%20introduce%20RULER-Bench%2C%20a%20benchmark%20designed%20to%20evaluate%20the%20reasoning%20ability%20of%20video%20generation%20models%20from%20the%20perspective%20of%20cognitive%20rules.%20Built%20upon%20two%20fundamental%20paradigms%3A%20text-to-video%20and%20image-to-video%2C%20RULER-Bench%20covers%2040%20representative%20tasks%20spanning%20six%20rule%20categories%20with%20622%20high-quality%20annotated%20instances.%20For%20the%20evaluation%20of%20each%20generated%20video%2C%20we%20construct%20a%20checklist%20covering%20four%20metrics%20and%20leverage%20GPT-o3%20to%20assign%20scores%20to%20each%20question%2C%20achieving%2085%25%20alignment%20with%20human%20judgements.%20Extensive%20experiments%20show%20that%20the%20state-of-the-art%20model%20achieves%20only%2048.87%25%20on%20the%20rule%20coherence%20metric%2C%20highlighting%20significant%20room%20for%20improvement%20in%20the%20reasoning%20capability%20of%20next-level%20video%20models.%20We%20expect%20that%20the%20insight%20obtained%20from%20RULER-Bench%20will%20facilitate%20further%20development%20of%20reasoning-aware%20video%20generation%2C%20advancing%20video%20generation%20models%20toward%20vision%20foundation%20intelligence.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02622v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRULER-Bench%253A%2520Probing%2520Rule-based%2520Reasoning%2520Abilities%2520of%2520Next-level%2520Video%2520Generation%2520Models%2520for%2520Vision%2520Foundation%2520Intelligence%26entry.906535625%3DXuming%2520He%2520and%2520Zehao%2520Fan%2520and%2520Hengjia%2520Li%2520and%2520Fan%2520Zhuo%2520and%2520Hankun%2520Xu%2520and%2520Senlin%2520Cheng%2520and%2520Di%2520Weng%2520and%2520Haifeng%2520Liu%2520and%2520Can%2520Ye%2520and%2520Boxi%2520Wu%26entry.1292438233%3DRecent%2520advances%2520in%2520video%2520generation%2520have%2520enabled%2520the%2520synthesis%2520of%2520videos%2520with%2520strong%2520temporal%2520consistency%2520and%2520impressive%2520visual%2520quality%252C%2520marking%2520a%2520crucial%2520step%2520toward%2520vision%2520foundation%2520models.%2520To%2520evaluate%2520these%2520video%2520generation%2520models%252C%2520existing%2520benchmarks%2520primarily%2520focus%2520on%2520factors%2520related%2520to%2520visual%2520perception%2520and%2520understanding%252C%2520like%2520visual%2520aesthetics%252C%2520instruction%2520adherence%252C%2520and%2520temporal%2520coherence.%2520However%252C%2520the%2520rule-based%2520reasoning%2520capabilities%2520of%2520video%2520generation%2520models%2520remain%2520largely%2520unexplored.%2520Although%2520recent%2520studies%2520have%2520carried%2520out%2520preliminary%2520explorations%2520into%2520whether%2520video%2520models%2520can%2520serve%2520as%2520zero-shot%2520learners%252C%2520they%2520still%2520lack%2520a%2520fine-grained%2520decomposition%2520of%2520reasoning%2520capabilities%2520and%2520a%2520comprehensive%2520evaluation%2520protocol.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520RULER-Bench%252C%2520a%2520benchmark%2520designed%2520to%2520evaluate%2520the%2520reasoning%2520ability%2520of%2520video%2520generation%2520models%2520from%2520the%2520perspective%2520of%2520cognitive%2520rules.%2520Built%2520upon%2520two%2520fundamental%2520paradigms%253A%2520text-to-video%2520and%2520image-to-video%252C%2520RULER-Bench%2520covers%252040%2520representative%2520tasks%2520spanning%2520six%2520rule%2520categories%2520with%2520622%2520high-quality%2520annotated%2520instances.%2520For%2520the%2520evaluation%2520of%2520each%2520generated%2520video%252C%2520we%2520construct%2520a%2520checklist%2520covering%2520four%2520metrics%2520and%2520leverage%2520GPT-o3%2520to%2520assign%2520scores%2520to%2520each%2520question%252C%2520achieving%252085%2525%2520alignment%2520with%2520human%2520judgements.%2520Extensive%2520experiments%2520show%2520that%2520the%2520state-of-the-art%2520model%2520achieves%2520only%252048.87%2525%2520on%2520the%2520rule%2520coherence%2520metric%252C%2520highlighting%2520significant%2520room%2520for%2520improvement%2520in%2520the%2520reasoning%2520capability%2520of%2520next-level%2520video%2520models.%2520We%2520expect%2520that%2520the%2520insight%2520obtained%2520from%2520RULER-Bench%2520will%2520facilitate%2520further%2520development%2520of%2520reasoning-aware%2520video%2520generation%252C%2520advancing%2520video%2520generation%2520models%2520toward%2520vision%2520foundation%2520intelligence.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02622v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RULER-Bench%3A%20Probing%20Rule-based%20Reasoning%20Abilities%20of%20Next-level%20Video%20Generation%20Models%20for%20Vision%20Foundation%20Intelligence&entry.906535625=Xuming%20He%20and%20Zehao%20Fan%20and%20Hengjia%20Li%20and%20Fan%20Zhuo%20and%20Hankun%20Xu%20and%20Senlin%20Cheng%20and%20Di%20Weng%20and%20Haifeng%20Liu%20and%20Can%20Ye%20and%20Boxi%20Wu&entry.1292438233=Recent%20advances%20in%20video%20generation%20have%20enabled%20the%20synthesis%20of%20videos%20with%20strong%20temporal%20consistency%20and%20impressive%20visual%20quality%2C%20marking%20a%20crucial%20step%20toward%20vision%20foundation%20models.%20To%20evaluate%20these%20video%20generation%20models%2C%20existing%20benchmarks%20primarily%20focus%20on%20factors%20related%20to%20visual%20perception%20and%20understanding%2C%20like%20visual%20aesthetics%2C%20instruction%20adherence%2C%20and%20temporal%20coherence.%20However%2C%20the%20rule-based%20reasoning%20capabilities%20of%20video%20generation%20models%20remain%20largely%20unexplored.%20Although%20recent%20studies%20have%20carried%20out%20preliminary%20explorations%20into%20whether%20video%20models%20can%20serve%20as%20zero-shot%20learners%2C%20they%20still%20lack%20a%20fine-grained%20decomposition%20of%20reasoning%20capabilities%20and%20a%20comprehensive%20evaluation%20protocol.%20To%20address%20this%20gap%2C%20we%20introduce%20RULER-Bench%2C%20a%20benchmark%20designed%20to%20evaluate%20the%20reasoning%20ability%20of%20video%20generation%20models%20from%20the%20perspective%20of%20cognitive%20rules.%20Built%20upon%20two%20fundamental%20paradigms%3A%20text-to-video%20and%20image-to-video%2C%20RULER-Bench%20covers%2040%20representative%20tasks%20spanning%20six%20rule%20categories%20with%20622%20high-quality%20annotated%20instances.%20For%20the%20evaluation%20of%20each%20generated%20video%2C%20we%20construct%20a%20checklist%20covering%20four%20metrics%20and%20leverage%20GPT-o3%20to%20assign%20scores%20to%20each%20question%2C%20achieving%2085%25%20alignment%20with%20human%20judgements.%20Extensive%20experiments%20show%20that%20the%20state-of-the-art%20model%20achieves%20only%2048.87%25%20on%20the%20rule%20coherence%20metric%2C%20highlighting%20significant%20room%20for%20improvement%20in%20the%20reasoning%20capability%20of%20next-level%20video%20models.%20We%20expect%20that%20the%20insight%20obtained%20from%20RULER-Bench%20will%20facilitate%20further%20development%20of%20reasoning-aware%20video%20generation%2C%20advancing%20video%20generation%20models%20toward%20vision%20foundation%20intelligence.&entry.1838667208=http%3A//arxiv.org/abs/2512.02622v1&entry.124074799=Read"},
{"title": "Look, Recite, Then Answer: Enhancing VLM Performance via Self-Generated Knowledge Hints", "author": "Xisheng Feng", "abstract": "Vision-Language Models (VLMs) exhibit significant performance plateaus in specialized domains like precision agriculture, primarily due to \"Reasoning-Driven Hallucination\" where linguistic priors override visual perception. A key bottleneck is the \"Modality Gap\": visual embeddings fail to reliably activate the fine-grained expert knowledge already encoded in model parameters. We propose \"Look, Recite, Then Answer,\" a parameter-efficient framework that enhances VLMs via self-generated knowledge hints while keeping backbone models frozen. The framework decouples inference into three stages: (1) Look generates objective visual descriptions and candidate sets; (2) Recite employs a lightweight 1.7B router to transform visual cues into targeted queries that trigger candidate-specific parametric knowledge; (3) Answer performs parallel evidence alignment between descriptions and recited knowledge to select the most consistent label. On AgroBench, our method achieves state-of-the-art results, improving Weed Identification accuracy by 23.52% over Qwen2-VL-72B and surpassing GPT-4o without external search overhead. This modular design mitigates hallucinations by transforming passive perception into active, controllable knowledge retrieval", "link": "http://arxiv.org/abs/2512.00882v2", "date": "2025-12-02", "relevancy": 2.8959, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6004}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6004}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Look%2C%20Recite%2C%20Then%20Answer%3A%20Enhancing%20VLM%20Performance%20via%20Self-Generated%20Knowledge%20Hints&body=Title%3A%20Look%2C%20Recite%2C%20Then%20Answer%3A%20Enhancing%20VLM%20Performance%20via%20Self-Generated%20Knowledge%20Hints%0AAuthor%3A%20Xisheng%20Feng%0AAbstract%3A%20Vision-Language%20Models%20%28VLMs%29%20exhibit%20significant%20performance%20plateaus%20in%20specialized%20domains%20like%20precision%20agriculture%2C%20primarily%20due%20to%20%22Reasoning-Driven%20Hallucination%22%20where%20linguistic%20priors%20override%20visual%20perception.%20A%20key%20bottleneck%20is%20the%20%22Modality%20Gap%22%3A%20visual%20embeddings%20fail%20to%20reliably%20activate%20the%20fine-grained%20expert%20knowledge%20already%20encoded%20in%20model%20parameters.%20We%20propose%20%22Look%2C%20Recite%2C%20Then%20Answer%2C%22%20a%20parameter-efficient%20framework%20that%20enhances%20VLMs%20via%20self-generated%20knowledge%20hints%20while%20keeping%20backbone%20models%20frozen.%20The%20framework%20decouples%20inference%20into%20three%20stages%3A%20%281%29%20Look%20generates%20objective%20visual%20descriptions%20and%20candidate%20sets%3B%20%282%29%20Recite%20employs%20a%20lightweight%201.7B%20router%20to%20transform%20visual%20cues%20into%20targeted%20queries%20that%20trigger%20candidate-specific%20parametric%20knowledge%3B%20%283%29%20Answer%20performs%20parallel%20evidence%20alignment%20between%20descriptions%20and%20recited%20knowledge%20to%20select%20the%20most%20consistent%20label.%20On%20AgroBench%2C%20our%20method%20achieves%20state-of-the-art%20results%2C%20improving%20Weed%20Identification%20accuracy%20by%2023.52%25%20over%20Qwen2-VL-72B%20and%20surpassing%20GPT-4o%20without%20external%20search%20overhead.%20This%20modular%20design%20mitigates%20hallucinations%20by%20transforming%20passive%20perception%20into%20active%2C%20controllable%20knowledge%20retrieval%0ALink%3A%20http%3A//arxiv.org/abs/2512.00882v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLook%252C%2520Recite%252C%2520Then%2520Answer%253A%2520Enhancing%2520VLM%2520Performance%2520via%2520Self-Generated%2520Knowledge%2520Hints%26entry.906535625%3DXisheng%2520Feng%26entry.1292438233%3DVision-Language%2520Models%2520%2528VLMs%2529%2520exhibit%2520significant%2520performance%2520plateaus%2520in%2520specialized%2520domains%2520like%2520precision%2520agriculture%252C%2520primarily%2520due%2520to%2520%2522Reasoning-Driven%2520Hallucination%2522%2520where%2520linguistic%2520priors%2520override%2520visual%2520perception.%2520A%2520key%2520bottleneck%2520is%2520the%2520%2522Modality%2520Gap%2522%253A%2520visual%2520embeddings%2520fail%2520to%2520reliably%2520activate%2520the%2520fine-grained%2520expert%2520knowledge%2520already%2520encoded%2520in%2520model%2520parameters.%2520We%2520propose%2520%2522Look%252C%2520Recite%252C%2520Then%2520Answer%252C%2522%2520a%2520parameter-efficient%2520framework%2520that%2520enhances%2520VLMs%2520via%2520self-generated%2520knowledge%2520hints%2520while%2520keeping%2520backbone%2520models%2520frozen.%2520The%2520framework%2520decouples%2520inference%2520into%2520three%2520stages%253A%2520%25281%2529%2520Look%2520generates%2520objective%2520visual%2520descriptions%2520and%2520candidate%2520sets%253B%2520%25282%2529%2520Recite%2520employs%2520a%2520lightweight%25201.7B%2520router%2520to%2520transform%2520visual%2520cues%2520into%2520targeted%2520queries%2520that%2520trigger%2520candidate-specific%2520parametric%2520knowledge%253B%2520%25283%2529%2520Answer%2520performs%2520parallel%2520evidence%2520alignment%2520between%2520descriptions%2520and%2520recited%2520knowledge%2520to%2520select%2520the%2520most%2520consistent%2520label.%2520On%2520AgroBench%252C%2520our%2520method%2520achieves%2520state-of-the-art%2520results%252C%2520improving%2520Weed%2520Identification%2520accuracy%2520by%252023.52%2525%2520over%2520Qwen2-VL-72B%2520and%2520surpassing%2520GPT-4o%2520without%2520external%2520search%2520overhead.%2520This%2520modular%2520design%2520mitigates%2520hallucinations%2520by%2520transforming%2520passive%2520perception%2520into%2520active%252C%2520controllable%2520knowledge%2520retrieval%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.00882v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Look%2C%20Recite%2C%20Then%20Answer%3A%20Enhancing%20VLM%20Performance%20via%20Self-Generated%20Knowledge%20Hints&entry.906535625=Xisheng%20Feng&entry.1292438233=Vision-Language%20Models%20%28VLMs%29%20exhibit%20significant%20performance%20plateaus%20in%20specialized%20domains%20like%20precision%20agriculture%2C%20primarily%20due%20to%20%22Reasoning-Driven%20Hallucination%22%20where%20linguistic%20priors%20override%20visual%20perception.%20A%20key%20bottleneck%20is%20the%20%22Modality%20Gap%22%3A%20visual%20embeddings%20fail%20to%20reliably%20activate%20the%20fine-grained%20expert%20knowledge%20already%20encoded%20in%20model%20parameters.%20We%20propose%20%22Look%2C%20Recite%2C%20Then%20Answer%2C%22%20a%20parameter-efficient%20framework%20that%20enhances%20VLMs%20via%20self-generated%20knowledge%20hints%20while%20keeping%20backbone%20models%20frozen.%20The%20framework%20decouples%20inference%20into%20three%20stages%3A%20%281%29%20Look%20generates%20objective%20visual%20descriptions%20and%20candidate%20sets%3B%20%282%29%20Recite%20employs%20a%20lightweight%201.7B%20router%20to%20transform%20visual%20cues%20into%20targeted%20queries%20that%20trigger%20candidate-specific%20parametric%20knowledge%3B%20%283%29%20Answer%20performs%20parallel%20evidence%20alignment%20between%20descriptions%20and%20recited%20knowledge%20to%20select%20the%20most%20consistent%20label.%20On%20AgroBench%2C%20our%20method%20achieves%20state-of-the-art%20results%2C%20improving%20Weed%20Identification%20accuracy%20by%2023.52%25%20over%20Qwen2-VL-72B%20and%20surpassing%20GPT-4o%20without%20external%20search%20overhead.%20This%20modular%20design%20mitigates%20hallucinations%20by%20transforming%20passive%20perception%20into%20active%2C%20controllable%20knowledge%20retrieval&entry.1838667208=http%3A//arxiv.org/abs/2512.00882v2&entry.124074799=Read"},
{"title": "DF-Mamba: Deformable State Space Modeling for 3D Hand Pose Estimation in Interactions", "author": "Yifan Zhou and Takehiko Ohkawa and Guwenxiao Zhou and Kanoko Goto and Takumi Hirose and Yusuke Sekikawa and Nakamasa Inoue", "abstract": "Modeling daily hand interactions often struggles with severe occlusions, such as when two hands overlap, which highlights the need for robust feature learning in 3D hand pose estimation (HPE). To handle such occluded hand images, it is vital to effectively learn the relationship between local image features (e.g., for occluded joints) and global context (e.g., cues from inter-joints, inter-hands, or the scene). However, most current 3D HPE methods still rely on ResNet for feature extraction, and such CNN's inductive bias may not be optimal for 3D HPE due to its limited capability to model the global context. To address this limitation, we propose an effective and efficient framework for visual feature extraction in 3D HPE using recent state space modeling (i.e., Mamba), dubbed Deformable Mamba (DF-Mamba). DF-Mamba is designed to capture global context cues beyond standard convolution through Mamba's selective state modeling and the proposed deformable state scanning. Specifically, for local features after convolution, our deformable scanning aggregates these features within an image while selectively preserving useful cues that represent the global context. This approach significantly improves the accuracy of structured 3D HPE, with comparable inference speed to ResNet-50. Our experiments involve extensive evaluations on five divergent datasets including single-hand and two-hand scenarios, hand-only and hand-object interactions, as well as RGB and depth-based estimation. DF-Mamba outperforms the latest image backbones, including VMamba and Spatial-Mamba, on all datasets and achieves state-of-the-art performance.", "link": "http://arxiv.org/abs/2512.02727v1", "date": "2025-12-02", "relevancy": 2.8728, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5864}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5822}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DF-Mamba%3A%20Deformable%20State%20Space%20Modeling%20for%203D%20Hand%20Pose%20Estimation%20in%20Interactions&body=Title%3A%20DF-Mamba%3A%20Deformable%20State%20Space%20Modeling%20for%203D%20Hand%20Pose%20Estimation%20in%20Interactions%0AAuthor%3A%20Yifan%20Zhou%20and%20Takehiko%20Ohkawa%20and%20Guwenxiao%20Zhou%20and%20Kanoko%20Goto%20and%20Takumi%20Hirose%20and%20Yusuke%20Sekikawa%20and%20Nakamasa%20Inoue%0AAbstract%3A%20Modeling%20daily%20hand%20interactions%20often%20struggles%20with%20severe%20occlusions%2C%20such%20as%20when%20two%20hands%20overlap%2C%20which%20highlights%20the%20need%20for%20robust%20feature%20learning%20in%203D%20hand%20pose%20estimation%20%28HPE%29.%20To%20handle%20such%20occluded%20hand%20images%2C%20it%20is%20vital%20to%20effectively%20learn%20the%20relationship%20between%20local%20image%20features%20%28e.g.%2C%20for%20occluded%20joints%29%20and%20global%20context%20%28e.g.%2C%20cues%20from%20inter-joints%2C%20inter-hands%2C%20or%20the%20scene%29.%20However%2C%20most%20current%203D%20HPE%20methods%20still%20rely%20on%20ResNet%20for%20feature%20extraction%2C%20and%20such%20CNN%27s%20inductive%20bias%20may%20not%20be%20optimal%20for%203D%20HPE%20due%20to%20its%20limited%20capability%20to%20model%20the%20global%20context.%20To%20address%20this%20limitation%2C%20we%20propose%20an%20effective%20and%20efficient%20framework%20for%20visual%20feature%20extraction%20in%203D%20HPE%20using%20recent%20state%20space%20modeling%20%28i.e.%2C%20Mamba%29%2C%20dubbed%20Deformable%20Mamba%20%28DF-Mamba%29.%20DF-Mamba%20is%20designed%20to%20capture%20global%20context%20cues%20beyond%20standard%20convolution%20through%20Mamba%27s%20selective%20state%20modeling%20and%20the%20proposed%20deformable%20state%20scanning.%20Specifically%2C%20for%20local%20features%20after%20convolution%2C%20our%20deformable%20scanning%20aggregates%20these%20features%20within%20an%20image%20while%20selectively%20preserving%20useful%20cues%20that%20represent%20the%20global%20context.%20This%20approach%20significantly%20improves%20the%20accuracy%20of%20structured%203D%20HPE%2C%20with%20comparable%20inference%20speed%20to%20ResNet-50.%20Our%20experiments%20involve%20extensive%20evaluations%20on%20five%20divergent%20datasets%20including%20single-hand%20and%20two-hand%20scenarios%2C%20hand-only%20and%20hand-object%20interactions%2C%20as%20well%20as%20RGB%20and%20depth-based%20estimation.%20DF-Mamba%20outperforms%20the%20latest%20image%20backbones%2C%20including%20VMamba%20and%20Spatial-Mamba%2C%20on%20all%20datasets%20and%20achieves%20state-of-the-art%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02727v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDF-Mamba%253A%2520Deformable%2520State%2520Space%2520Modeling%2520for%25203D%2520Hand%2520Pose%2520Estimation%2520in%2520Interactions%26entry.906535625%3DYifan%2520Zhou%2520and%2520Takehiko%2520Ohkawa%2520and%2520Guwenxiao%2520Zhou%2520and%2520Kanoko%2520Goto%2520and%2520Takumi%2520Hirose%2520and%2520Yusuke%2520Sekikawa%2520and%2520Nakamasa%2520Inoue%26entry.1292438233%3DModeling%2520daily%2520hand%2520interactions%2520often%2520struggles%2520with%2520severe%2520occlusions%252C%2520such%2520as%2520when%2520two%2520hands%2520overlap%252C%2520which%2520highlights%2520the%2520need%2520for%2520robust%2520feature%2520learning%2520in%25203D%2520hand%2520pose%2520estimation%2520%2528HPE%2529.%2520To%2520handle%2520such%2520occluded%2520hand%2520images%252C%2520it%2520is%2520vital%2520to%2520effectively%2520learn%2520the%2520relationship%2520between%2520local%2520image%2520features%2520%2528e.g.%252C%2520for%2520occluded%2520joints%2529%2520and%2520global%2520context%2520%2528e.g.%252C%2520cues%2520from%2520inter-joints%252C%2520inter-hands%252C%2520or%2520the%2520scene%2529.%2520However%252C%2520most%2520current%25203D%2520HPE%2520methods%2520still%2520rely%2520on%2520ResNet%2520for%2520feature%2520extraction%252C%2520and%2520such%2520CNN%2527s%2520inductive%2520bias%2520may%2520not%2520be%2520optimal%2520for%25203D%2520HPE%2520due%2520to%2520its%2520limited%2520capability%2520to%2520model%2520the%2520global%2520context.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520an%2520effective%2520and%2520efficient%2520framework%2520for%2520visual%2520feature%2520extraction%2520in%25203D%2520HPE%2520using%2520recent%2520state%2520space%2520modeling%2520%2528i.e.%252C%2520Mamba%2529%252C%2520dubbed%2520Deformable%2520Mamba%2520%2528DF-Mamba%2529.%2520DF-Mamba%2520is%2520designed%2520to%2520capture%2520global%2520context%2520cues%2520beyond%2520standard%2520convolution%2520through%2520Mamba%2527s%2520selective%2520state%2520modeling%2520and%2520the%2520proposed%2520deformable%2520state%2520scanning.%2520Specifically%252C%2520for%2520local%2520features%2520after%2520convolution%252C%2520our%2520deformable%2520scanning%2520aggregates%2520these%2520features%2520within%2520an%2520image%2520while%2520selectively%2520preserving%2520useful%2520cues%2520that%2520represent%2520the%2520global%2520context.%2520This%2520approach%2520significantly%2520improves%2520the%2520accuracy%2520of%2520structured%25203D%2520HPE%252C%2520with%2520comparable%2520inference%2520speed%2520to%2520ResNet-50.%2520Our%2520experiments%2520involve%2520extensive%2520evaluations%2520on%2520five%2520divergent%2520datasets%2520including%2520single-hand%2520and%2520two-hand%2520scenarios%252C%2520hand-only%2520and%2520hand-object%2520interactions%252C%2520as%2520well%2520as%2520RGB%2520and%2520depth-based%2520estimation.%2520DF-Mamba%2520outperforms%2520the%2520latest%2520image%2520backbones%252C%2520including%2520VMamba%2520and%2520Spatial-Mamba%252C%2520on%2520all%2520datasets%2520and%2520achieves%2520state-of-the-art%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02727v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DF-Mamba%3A%20Deformable%20State%20Space%20Modeling%20for%203D%20Hand%20Pose%20Estimation%20in%20Interactions&entry.906535625=Yifan%20Zhou%20and%20Takehiko%20Ohkawa%20and%20Guwenxiao%20Zhou%20and%20Kanoko%20Goto%20and%20Takumi%20Hirose%20and%20Yusuke%20Sekikawa%20and%20Nakamasa%20Inoue&entry.1292438233=Modeling%20daily%20hand%20interactions%20often%20struggles%20with%20severe%20occlusions%2C%20such%20as%20when%20two%20hands%20overlap%2C%20which%20highlights%20the%20need%20for%20robust%20feature%20learning%20in%203D%20hand%20pose%20estimation%20%28HPE%29.%20To%20handle%20such%20occluded%20hand%20images%2C%20it%20is%20vital%20to%20effectively%20learn%20the%20relationship%20between%20local%20image%20features%20%28e.g.%2C%20for%20occluded%20joints%29%20and%20global%20context%20%28e.g.%2C%20cues%20from%20inter-joints%2C%20inter-hands%2C%20or%20the%20scene%29.%20However%2C%20most%20current%203D%20HPE%20methods%20still%20rely%20on%20ResNet%20for%20feature%20extraction%2C%20and%20such%20CNN%27s%20inductive%20bias%20may%20not%20be%20optimal%20for%203D%20HPE%20due%20to%20its%20limited%20capability%20to%20model%20the%20global%20context.%20To%20address%20this%20limitation%2C%20we%20propose%20an%20effective%20and%20efficient%20framework%20for%20visual%20feature%20extraction%20in%203D%20HPE%20using%20recent%20state%20space%20modeling%20%28i.e.%2C%20Mamba%29%2C%20dubbed%20Deformable%20Mamba%20%28DF-Mamba%29.%20DF-Mamba%20is%20designed%20to%20capture%20global%20context%20cues%20beyond%20standard%20convolution%20through%20Mamba%27s%20selective%20state%20modeling%20and%20the%20proposed%20deformable%20state%20scanning.%20Specifically%2C%20for%20local%20features%20after%20convolution%2C%20our%20deformable%20scanning%20aggregates%20these%20features%20within%20an%20image%20while%20selectively%20preserving%20useful%20cues%20that%20represent%20the%20global%20context.%20This%20approach%20significantly%20improves%20the%20accuracy%20of%20structured%203D%20HPE%2C%20with%20comparable%20inference%20speed%20to%20ResNet-50.%20Our%20experiments%20involve%20extensive%20evaluations%20on%20five%20divergent%20datasets%20including%20single-hand%20and%20two-hand%20scenarios%2C%20hand-only%20and%20hand-object%20interactions%2C%20as%20well%20as%20RGB%20and%20depth-based%20estimation.%20DF-Mamba%20outperforms%20the%20latest%20image%20backbones%2C%20including%20VMamba%20and%20Spatial-Mamba%2C%20on%20all%20datasets%20and%20achieves%20state-of-the-art%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2512.02727v1&entry.124074799=Read"},
{"title": "SurfFill: Completion of LiDAR Point Clouds via Gaussian Surfel Splatting", "author": "Svenja Strobel and Matthias Innmann and Bernhard Egger and Marc Stamminger and Linus Franke", "abstract": "LiDAR-captured point clouds are often considered the gold standard in active 3D reconstruction. While their accuracy is exceptional in flat regions, the capturing is susceptible to miss small geometric structures and may fail with dark, absorbent materials. Alternatively, capturing multiple photos of the scene and applying 3D photogrammetry can infer these details as they often represent feature-rich regions. However, the accuracy of LiDAR for featureless regions is rarely reached. Therefore, we suggest combining the strengths of LiDAR and camera-based capture by introducing SurfFill: a Gaussian surfel-based LiDAR completion scheme. We analyze LiDAR capturings and attribute LiDAR beam divergence as a main factor for artifacts, manifesting mostly at thin structures and edges. We use this insight to introduce an ambiguity heuristic for completed scans by evaluating the change in density in the point cloud. This allows us to identify points close to missed areas, which we can then use to grow additional points from to complete the scan. For this point growing, we constrain Gaussian surfel reconstruction [Huang et al. 2024] to focus optimization and densification on these ambiguous areas. Finally, Gaussian primitives of the reconstruction in ambiguous areas are extracted and sampled for points to complete the point cloud. To address the challenges of large-scale reconstruction, we extend this pipeline with a divide-and-conquer scheme for building-sized point cloud completion. We evaluate on the task of LiDAR point cloud completion of synthetic and real-world scenes and find that our method outperforms previous reconstruction methods.", "link": "http://arxiv.org/abs/2512.03010v1", "date": "2025-12-02", "relevancy": 2.8591, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6286}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5464}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SurfFill%3A%20Completion%20of%20LiDAR%20Point%20Clouds%20via%20Gaussian%20Surfel%20Splatting&body=Title%3A%20SurfFill%3A%20Completion%20of%20LiDAR%20Point%20Clouds%20via%20Gaussian%20Surfel%20Splatting%0AAuthor%3A%20Svenja%20Strobel%20and%20Matthias%20Innmann%20and%20Bernhard%20Egger%20and%20Marc%20Stamminger%20and%20Linus%20Franke%0AAbstract%3A%20LiDAR-captured%20point%20clouds%20are%20often%20considered%20the%20gold%20standard%20in%20active%203D%20reconstruction.%20While%20their%20accuracy%20is%20exceptional%20in%20flat%20regions%2C%20the%20capturing%20is%20susceptible%20to%20miss%20small%20geometric%20structures%20and%20may%20fail%20with%20dark%2C%20absorbent%20materials.%20Alternatively%2C%20capturing%20multiple%20photos%20of%20the%20scene%20and%20applying%203D%20photogrammetry%20can%20infer%20these%20details%20as%20they%20often%20represent%20feature-rich%20regions.%20However%2C%20the%20accuracy%20of%20LiDAR%20for%20featureless%20regions%20is%20rarely%20reached.%20Therefore%2C%20we%20suggest%20combining%20the%20strengths%20of%20LiDAR%20and%20camera-based%20capture%20by%20introducing%20SurfFill%3A%20a%20Gaussian%20surfel-based%20LiDAR%20completion%20scheme.%20We%20analyze%20LiDAR%20capturings%20and%20attribute%20LiDAR%20beam%20divergence%20as%20a%20main%20factor%20for%20artifacts%2C%20manifesting%20mostly%20at%20thin%20structures%20and%20edges.%20We%20use%20this%20insight%20to%20introduce%20an%20ambiguity%20heuristic%20for%20completed%20scans%20by%20evaluating%20the%20change%20in%20density%20in%20the%20point%20cloud.%20This%20allows%20us%20to%20identify%20points%20close%20to%20missed%20areas%2C%20which%20we%20can%20then%20use%20to%20grow%20additional%20points%20from%20to%20complete%20the%20scan.%20For%20this%20point%20growing%2C%20we%20constrain%20Gaussian%20surfel%20reconstruction%20%5BHuang%20et%20al.%202024%5D%20to%20focus%20optimization%20and%20densification%20on%20these%20ambiguous%20areas.%20Finally%2C%20Gaussian%20primitives%20of%20the%20reconstruction%20in%20ambiguous%20areas%20are%20extracted%20and%20sampled%20for%20points%20to%20complete%20the%20point%20cloud.%20To%20address%20the%20challenges%20of%20large-scale%20reconstruction%2C%20we%20extend%20this%20pipeline%20with%20a%20divide-and-conquer%20scheme%20for%20building-sized%20point%20cloud%20completion.%20We%20evaluate%20on%20the%20task%20of%20LiDAR%20point%20cloud%20completion%20of%20synthetic%20and%20real-world%20scenes%20and%20find%20that%20our%20method%20outperforms%20previous%20reconstruction%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03010v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurfFill%253A%2520Completion%2520of%2520LiDAR%2520Point%2520Clouds%2520via%2520Gaussian%2520Surfel%2520Splatting%26entry.906535625%3DSvenja%2520Strobel%2520and%2520Matthias%2520Innmann%2520and%2520Bernhard%2520Egger%2520and%2520Marc%2520Stamminger%2520and%2520Linus%2520Franke%26entry.1292438233%3DLiDAR-captured%2520point%2520clouds%2520are%2520often%2520considered%2520the%2520gold%2520standard%2520in%2520active%25203D%2520reconstruction.%2520While%2520their%2520accuracy%2520is%2520exceptional%2520in%2520flat%2520regions%252C%2520the%2520capturing%2520is%2520susceptible%2520to%2520miss%2520small%2520geometric%2520structures%2520and%2520may%2520fail%2520with%2520dark%252C%2520absorbent%2520materials.%2520Alternatively%252C%2520capturing%2520multiple%2520photos%2520of%2520the%2520scene%2520and%2520applying%25203D%2520photogrammetry%2520can%2520infer%2520these%2520details%2520as%2520they%2520often%2520represent%2520feature-rich%2520regions.%2520However%252C%2520the%2520accuracy%2520of%2520LiDAR%2520for%2520featureless%2520regions%2520is%2520rarely%2520reached.%2520Therefore%252C%2520we%2520suggest%2520combining%2520the%2520strengths%2520of%2520LiDAR%2520and%2520camera-based%2520capture%2520by%2520introducing%2520SurfFill%253A%2520a%2520Gaussian%2520surfel-based%2520LiDAR%2520completion%2520scheme.%2520We%2520analyze%2520LiDAR%2520capturings%2520and%2520attribute%2520LiDAR%2520beam%2520divergence%2520as%2520a%2520main%2520factor%2520for%2520artifacts%252C%2520manifesting%2520mostly%2520at%2520thin%2520structures%2520and%2520edges.%2520We%2520use%2520this%2520insight%2520to%2520introduce%2520an%2520ambiguity%2520heuristic%2520for%2520completed%2520scans%2520by%2520evaluating%2520the%2520change%2520in%2520density%2520in%2520the%2520point%2520cloud.%2520This%2520allows%2520us%2520to%2520identify%2520points%2520close%2520to%2520missed%2520areas%252C%2520which%2520we%2520can%2520then%2520use%2520to%2520grow%2520additional%2520points%2520from%2520to%2520complete%2520the%2520scan.%2520For%2520this%2520point%2520growing%252C%2520we%2520constrain%2520Gaussian%2520surfel%2520reconstruction%2520%255BHuang%2520et%2520al.%25202024%255D%2520to%2520focus%2520optimization%2520and%2520densification%2520on%2520these%2520ambiguous%2520areas.%2520Finally%252C%2520Gaussian%2520primitives%2520of%2520the%2520reconstruction%2520in%2520ambiguous%2520areas%2520are%2520extracted%2520and%2520sampled%2520for%2520points%2520to%2520complete%2520the%2520point%2520cloud.%2520To%2520address%2520the%2520challenges%2520of%2520large-scale%2520reconstruction%252C%2520we%2520extend%2520this%2520pipeline%2520with%2520a%2520divide-and-conquer%2520scheme%2520for%2520building-sized%2520point%2520cloud%2520completion.%2520We%2520evaluate%2520on%2520the%2520task%2520of%2520LiDAR%2520point%2520cloud%2520completion%2520of%2520synthetic%2520and%2520real-world%2520scenes%2520and%2520find%2520that%2520our%2520method%2520outperforms%2520previous%2520reconstruction%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03010v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SurfFill%3A%20Completion%20of%20LiDAR%20Point%20Clouds%20via%20Gaussian%20Surfel%20Splatting&entry.906535625=Svenja%20Strobel%20and%20Matthias%20Innmann%20and%20Bernhard%20Egger%20and%20Marc%20Stamminger%20and%20Linus%20Franke&entry.1292438233=LiDAR-captured%20point%20clouds%20are%20often%20considered%20the%20gold%20standard%20in%20active%203D%20reconstruction.%20While%20their%20accuracy%20is%20exceptional%20in%20flat%20regions%2C%20the%20capturing%20is%20susceptible%20to%20miss%20small%20geometric%20structures%20and%20may%20fail%20with%20dark%2C%20absorbent%20materials.%20Alternatively%2C%20capturing%20multiple%20photos%20of%20the%20scene%20and%20applying%203D%20photogrammetry%20can%20infer%20these%20details%20as%20they%20often%20represent%20feature-rich%20regions.%20However%2C%20the%20accuracy%20of%20LiDAR%20for%20featureless%20regions%20is%20rarely%20reached.%20Therefore%2C%20we%20suggest%20combining%20the%20strengths%20of%20LiDAR%20and%20camera-based%20capture%20by%20introducing%20SurfFill%3A%20a%20Gaussian%20surfel-based%20LiDAR%20completion%20scheme.%20We%20analyze%20LiDAR%20capturings%20and%20attribute%20LiDAR%20beam%20divergence%20as%20a%20main%20factor%20for%20artifacts%2C%20manifesting%20mostly%20at%20thin%20structures%20and%20edges.%20We%20use%20this%20insight%20to%20introduce%20an%20ambiguity%20heuristic%20for%20completed%20scans%20by%20evaluating%20the%20change%20in%20density%20in%20the%20point%20cloud.%20This%20allows%20us%20to%20identify%20points%20close%20to%20missed%20areas%2C%20which%20we%20can%20then%20use%20to%20grow%20additional%20points%20from%20to%20complete%20the%20scan.%20For%20this%20point%20growing%2C%20we%20constrain%20Gaussian%20surfel%20reconstruction%20%5BHuang%20et%20al.%202024%5D%20to%20focus%20optimization%20and%20densification%20on%20these%20ambiguous%20areas.%20Finally%2C%20Gaussian%20primitives%20of%20the%20reconstruction%20in%20ambiguous%20areas%20are%20extracted%20and%20sampled%20for%20points%20to%20complete%20the%20point%20cloud.%20To%20address%20the%20challenges%20of%20large-scale%20reconstruction%2C%20we%20extend%20this%20pipeline%20with%20a%20divide-and-conquer%20scheme%20for%20building-sized%20point%20cloud%20completion.%20We%20evaluate%20on%20the%20task%20of%20LiDAR%20point%20cloud%20completion%20of%20synthetic%20and%20real-world%20scenes%20and%20find%20that%20our%20method%20outperforms%20previous%20reconstruction%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2512.03010v1&entry.124074799=Read"},
{"title": "VLA Models Are More Generalizable Than You Think: Revisiting Physical and Spatial Modeling", "author": "Weiqi Li and Quande Zhang and Ruifeng Zhai and Liang Lin and Guangrun Wang", "abstract": "Vision-language-action (VLA) models achieve strong in-distribution performance but degrade sharply under novel camera viewpoints and visual perturbations. We show that this brittleness primarily arises from misalignment in Spatial Modeling, rather than Physical Modeling. To address this, we propose a one-shot adaptation framework that recalibrates visual representations through lightweight, learnable updates. Our first method, Feature Token Modulation (FTM), applies a global affine transformation to visual tokens and improves Libero viewpoint accuracy from 48.5% to 87.1% with only 4K parameters. Building on this, Feature Linear Adaptation (FLA) introduces low-rank updates to the ViT encoder, achieving 90.8% success with 4.7M parameters -- matching LoRA-scale finetuning at far lower cost. Together, these results reveal substantial untapped robustness in pretrained VLA models and demonstrate that targeted, minimal visual adaptation is sufficient to restore viewpoint generalization.", "link": "http://arxiv.org/abs/2512.02902v1", "date": "2025-12-02", "relevancy": 2.8531, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5719}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5719}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLA%20Models%20Are%20More%20Generalizable%20Than%20You%20Think%3A%20Revisiting%20Physical%20and%20Spatial%20Modeling&body=Title%3A%20VLA%20Models%20Are%20More%20Generalizable%20Than%20You%20Think%3A%20Revisiting%20Physical%20and%20Spatial%20Modeling%0AAuthor%3A%20Weiqi%20Li%20and%20Quande%20Zhang%20and%20Ruifeng%20Zhai%20and%20Liang%20Lin%20and%20Guangrun%20Wang%0AAbstract%3A%20Vision-language-action%20%28VLA%29%20models%20achieve%20strong%20in-distribution%20performance%20but%20degrade%20sharply%20under%20novel%20camera%20viewpoints%20and%20visual%20perturbations.%20We%20show%20that%20this%20brittleness%20primarily%20arises%20from%20misalignment%20in%20Spatial%20Modeling%2C%20rather%20than%20Physical%20Modeling.%20To%20address%20this%2C%20we%20propose%20a%20one-shot%20adaptation%20framework%20that%20recalibrates%20visual%20representations%20through%20lightweight%2C%20learnable%20updates.%20Our%20first%20method%2C%20Feature%20Token%20Modulation%20%28FTM%29%2C%20applies%20a%20global%20affine%20transformation%20to%20visual%20tokens%20and%20improves%20Libero%20viewpoint%20accuracy%20from%2048.5%25%20to%2087.1%25%20with%20only%204K%20parameters.%20Building%20on%20this%2C%20Feature%20Linear%20Adaptation%20%28FLA%29%20introduces%20low-rank%20updates%20to%20the%20ViT%20encoder%2C%20achieving%2090.8%25%20success%20with%204.7M%20parameters%20--%20matching%20LoRA-scale%20finetuning%20at%20far%20lower%20cost.%20Together%2C%20these%20results%20reveal%20substantial%20untapped%20robustness%20in%20pretrained%20VLA%20models%20and%20demonstrate%20that%20targeted%2C%20minimal%20visual%20adaptation%20is%20sufficient%20to%20restore%20viewpoint%20generalization.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02902v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLA%2520Models%2520Are%2520More%2520Generalizable%2520Than%2520You%2520Think%253A%2520Revisiting%2520Physical%2520and%2520Spatial%2520Modeling%26entry.906535625%3DWeiqi%2520Li%2520and%2520Quande%2520Zhang%2520and%2520Ruifeng%2520Zhai%2520and%2520Liang%2520Lin%2520and%2520Guangrun%2520Wang%26entry.1292438233%3DVision-language-action%2520%2528VLA%2529%2520models%2520achieve%2520strong%2520in-distribution%2520performance%2520but%2520degrade%2520sharply%2520under%2520novel%2520camera%2520viewpoints%2520and%2520visual%2520perturbations.%2520We%2520show%2520that%2520this%2520brittleness%2520primarily%2520arises%2520from%2520misalignment%2520in%2520Spatial%2520Modeling%252C%2520rather%2520than%2520Physical%2520Modeling.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520one-shot%2520adaptation%2520framework%2520that%2520recalibrates%2520visual%2520representations%2520through%2520lightweight%252C%2520learnable%2520updates.%2520Our%2520first%2520method%252C%2520Feature%2520Token%2520Modulation%2520%2528FTM%2529%252C%2520applies%2520a%2520global%2520affine%2520transformation%2520to%2520visual%2520tokens%2520and%2520improves%2520Libero%2520viewpoint%2520accuracy%2520from%252048.5%2525%2520to%252087.1%2525%2520with%2520only%25204K%2520parameters.%2520Building%2520on%2520this%252C%2520Feature%2520Linear%2520Adaptation%2520%2528FLA%2529%2520introduces%2520low-rank%2520updates%2520to%2520the%2520ViT%2520encoder%252C%2520achieving%252090.8%2525%2520success%2520with%25204.7M%2520parameters%2520--%2520matching%2520LoRA-scale%2520finetuning%2520at%2520far%2520lower%2520cost.%2520Together%252C%2520these%2520results%2520reveal%2520substantial%2520untapped%2520robustness%2520in%2520pretrained%2520VLA%2520models%2520and%2520demonstrate%2520that%2520targeted%252C%2520minimal%2520visual%2520adaptation%2520is%2520sufficient%2520to%2520restore%2520viewpoint%2520generalization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02902v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLA%20Models%20Are%20More%20Generalizable%20Than%20You%20Think%3A%20Revisiting%20Physical%20and%20Spatial%20Modeling&entry.906535625=Weiqi%20Li%20and%20Quande%20Zhang%20and%20Ruifeng%20Zhai%20and%20Liang%20Lin%20and%20Guangrun%20Wang&entry.1292438233=Vision-language-action%20%28VLA%29%20models%20achieve%20strong%20in-distribution%20performance%20but%20degrade%20sharply%20under%20novel%20camera%20viewpoints%20and%20visual%20perturbations.%20We%20show%20that%20this%20brittleness%20primarily%20arises%20from%20misalignment%20in%20Spatial%20Modeling%2C%20rather%20than%20Physical%20Modeling.%20To%20address%20this%2C%20we%20propose%20a%20one-shot%20adaptation%20framework%20that%20recalibrates%20visual%20representations%20through%20lightweight%2C%20learnable%20updates.%20Our%20first%20method%2C%20Feature%20Token%20Modulation%20%28FTM%29%2C%20applies%20a%20global%20affine%20transformation%20to%20visual%20tokens%20and%20improves%20Libero%20viewpoint%20accuracy%20from%2048.5%25%20to%2087.1%25%20with%20only%204K%20parameters.%20Building%20on%20this%2C%20Feature%20Linear%20Adaptation%20%28FLA%29%20introduces%20low-rank%20updates%20to%20the%20ViT%20encoder%2C%20achieving%2090.8%25%20success%20with%204.7M%20parameters%20--%20matching%20LoRA-scale%20finetuning%20at%20far%20lower%20cost.%20Together%2C%20these%20results%20reveal%20substantial%20untapped%20robustness%20in%20pretrained%20VLA%20models%20and%20demonstrate%20that%20targeted%2C%20minimal%20visual%20adaptation%20is%20sufficient%20to%20restore%20viewpoint%20generalization.&entry.1838667208=http%3A//arxiv.org/abs/2512.02902v1&entry.124074799=Read"},
{"title": "GeoBridge: A Semantic-Anchored Multi-View Foundation Model Bridging Images and Text for Geo-Localization", "author": "Zixuan Song and Jing Zhang and Di Wang and Zidie Zhou and Wenbin Liu and Haonan Guo and En Wang and Bo Du", "abstract": "Cross-view geo-localization infers a location by retrieving geo-tagged reference images that visually correspond to a query image. However, the traditional satellite-centric paradigm limits robustness when high-resolution or up-to-date satellite imagery is unavailable. It further underexploits complementary cues across views (e.g., drone, satellite, and street) and modalities (e.g., language and image). To address these challenges, we propose GeoBridge, a foundation model that performs bidirectional matching across views and supports language-to-image retrieval. Going beyond traditional satellite-centric formulations, GeoBridge builds on a novel semantic-anchor mechanism that bridges multi-view features through textual descriptions for robust, flexible localization. In support of this task, we construct GeoLoc, the first large-scale, cross-modal, and multi-view aligned dataset comprising over 50,000 pairs of drone, street-view panorama, and satellite images as well as their textual descriptions, collected from 36 countries, ensuring both geographic and semantic alignment. We performed broad evaluations across multiple tasks. Experiments confirm that GeoLoc pre-training markedly improves geo-location accuracy for GeoBridge while promoting cross-domain generalization and cross-modal knowledge transfer. The dataset, source code, and pretrained models were released at https://github.com/MiliLab/GeoBridge.", "link": "http://arxiv.org/abs/2512.02697v1", "date": "2025-12-02", "relevancy": 2.8374, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5799}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5613}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoBridge%3A%20A%20Semantic-Anchored%20Multi-View%20Foundation%20Model%20Bridging%20Images%20and%20Text%20for%20Geo-Localization&body=Title%3A%20GeoBridge%3A%20A%20Semantic-Anchored%20Multi-View%20Foundation%20Model%20Bridging%20Images%20and%20Text%20for%20Geo-Localization%0AAuthor%3A%20Zixuan%20Song%20and%20Jing%20Zhang%20and%20Di%20Wang%20and%20Zidie%20Zhou%20and%20Wenbin%20Liu%20and%20Haonan%20Guo%20and%20En%20Wang%20and%20Bo%20Du%0AAbstract%3A%20Cross-view%20geo-localization%20infers%20a%20location%20by%20retrieving%20geo-tagged%20reference%20images%20that%20visually%20correspond%20to%20a%20query%20image.%20However%2C%20the%20traditional%20satellite-centric%20paradigm%20limits%20robustness%20when%20high-resolution%20or%20up-to-date%20satellite%20imagery%20is%20unavailable.%20It%20further%20underexploits%20complementary%20cues%20across%20views%20%28e.g.%2C%20drone%2C%20satellite%2C%20and%20street%29%20and%20modalities%20%28e.g.%2C%20language%20and%20image%29.%20To%20address%20these%20challenges%2C%20we%20propose%20GeoBridge%2C%20a%20foundation%20model%20that%20performs%20bidirectional%20matching%20across%20views%20and%20supports%20language-to-image%20retrieval.%20Going%20beyond%20traditional%20satellite-centric%20formulations%2C%20GeoBridge%20builds%20on%20a%20novel%20semantic-anchor%20mechanism%20that%20bridges%20multi-view%20features%20through%20textual%20descriptions%20for%20robust%2C%20flexible%20localization.%20In%20support%20of%20this%20task%2C%20we%20construct%20GeoLoc%2C%20the%20first%20large-scale%2C%20cross-modal%2C%20and%20multi-view%20aligned%20dataset%20comprising%20over%2050%2C000%20pairs%20of%20drone%2C%20street-view%20panorama%2C%20and%20satellite%20images%20as%20well%20as%20their%20textual%20descriptions%2C%20collected%20from%2036%20countries%2C%20ensuring%20both%20geographic%20and%20semantic%20alignment.%20We%20performed%20broad%20evaluations%20across%20multiple%20tasks.%20Experiments%20confirm%20that%20GeoLoc%20pre-training%20markedly%20improves%20geo-location%20accuracy%20for%20GeoBridge%20while%20promoting%20cross-domain%20generalization%20and%20cross-modal%20knowledge%20transfer.%20The%20dataset%2C%20source%20code%2C%20and%20pretrained%20models%20were%20released%20at%20https%3A//github.com/MiliLab/GeoBridge.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02697v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoBridge%253A%2520A%2520Semantic-Anchored%2520Multi-View%2520Foundation%2520Model%2520Bridging%2520Images%2520and%2520Text%2520for%2520Geo-Localization%26entry.906535625%3DZixuan%2520Song%2520and%2520Jing%2520Zhang%2520and%2520Di%2520Wang%2520and%2520Zidie%2520Zhou%2520and%2520Wenbin%2520Liu%2520and%2520Haonan%2520Guo%2520and%2520En%2520Wang%2520and%2520Bo%2520Du%26entry.1292438233%3DCross-view%2520geo-localization%2520infers%2520a%2520location%2520by%2520retrieving%2520geo-tagged%2520reference%2520images%2520that%2520visually%2520correspond%2520to%2520a%2520query%2520image.%2520However%252C%2520the%2520traditional%2520satellite-centric%2520paradigm%2520limits%2520robustness%2520when%2520high-resolution%2520or%2520up-to-date%2520satellite%2520imagery%2520is%2520unavailable.%2520It%2520further%2520underexploits%2520complementary%2520cues%2520across%2520views%2520%2528e.g.%252C%2520drone%252C%2520satellite%252C%2520and%2520street%2529%2520and%2520modalities%2520%2528e.g.%252C%2520language%2520and%2520image%2529.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520GeoBridge%252C%2520a%2520foundation%2520model%2520that%2520performs%2520bidirectional%2520matching%2520across%2520views%2520and%2520supports%2520language-to-image%2520retrieval.%2520Going%2520beyond%2520traditional%2520satellite-centric%2520formulations%252C%2520GeoBridge%2520builds%2520on%2520a%2520novel%2520semantic-anchor%2520mechanism%2520that%2520bridges%2520multi-view%2520features%2520through%2520textual%2520descriptions%2520for%2520robust%252C%2520flexible%2520localization.%2520In%2520support%2520of%2520this%2520task%252C%2520we%2520construct%2520GeoLoc%252C%2520the%2520first%2520large-scale%252C%2520cross-modal%252C%2520and%2520multi-view%2520aligned%2520dataset%2520comprising%2520over%252050%252C000%2520pairs%2520of%2520drone%252C%2520street-view%2520panorama%252C%2520and%2520satellite%2520images%2520as%2520well%2520as%2520their%2520textual%2520descriptions%252C%2520collected%2520from%252036%2520countries%252C%2520ensuring%2520both%2520geographic%2520and%2520semantic%2520alignment.%2520We%2520performed%2520broad%2520evaluations%2520across%2520multiple%2520tasks.%2520Experiments%2520confirm%2520that%2520GeoLoc%2520pre-training%2520markedly%2520improves%2520geo-location%2520accuracy%2520for%2520GeoBridge%2520while%2520promoting%2520cross-domain%2520generalization%2520and%2520cross-modal%2520knowledge%2520transfer.%2520The%2520dataset%252C%2520source%2520code%252C%2520and%2520pretrained%2520models%2520were%2520released%2520at%2520https%253A//github.com/MiliLab/GeoBridge.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02697v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoBridge%3A%20A%20Semantic-Anchored%20Multi-View%20Foundation%20Model%20Bridging%20Images%20and%20Text%20for%20Geo-Localization&entry.906535625=Zixuan%20Song%20and%20Jing%20Zhang%20and%20Di%20Wang%20and%20Zidie%20Zhou%20and%20Wenbin%20Liu%20and%20Haonan%20Guo%20and%20En%20Wang%20and%20Bo%20Du&entry.1292438233=Cross-view%20geo-localization%20infers%20a%20location%20by%20retrieving%20geo-tagged%20reference%20images%20that%20visually%20correspond%20to%20a%20query%20image.%20However%2C%20the%20traditional%20satellite-centric%20paradigm%20limits%20robustness%20when%20high-resolution%20or%20up-to-date%20satellite%20imagery%20is%20unavailable.%20It%20further%20underexploits%20complementary%20cues%20across%20views%20%28e.g.%2C%20drone%2C%20satellite%2C%20and%20street%29%20and%20modalities%20%28e.g.%2C%20language%20and%20image%29.%20To%20address%20these%20challenges%2C%20we%20propose%20GeoBridge%2C%20a%20foundation%20model%20that%20performs%20bidirectional%20matching%20across%20views%20and%20supports%20language-to-image%20retrieval.%20Going%20beyond%20traditional%20satellite-centric%20formulations%2C%20GeoBridge%20builds%20on%20a%20novel%20semantic-anchor%20mechanism%20that%20bridges%20multi-view%20features%20through%20textual%20descriptions%20for%20robust%2C%20flexible%20localization.%20In%20support%20of%20this%20task%2C%20we%20construct%20GeoLoc%2C%20the%20first%20large-scale%2C%20cross-modal%2C%20and%20multi-view%20aligned%20dataset%20comprising%20over%2050%2C000%20pairs%20of%20drone%2C%20street-view%20panorama%2C%20and%20satellite%20images%20as%20well%20as%20their%20textual%20descriptions%2C%20collected%20from%2036%20countries%2C%20ensuring%20both%20geographic%20and%20semantic%20alignment.%20We%20performed%20broad%20evaluations%20across%20multiple%20tasks.%20Experiments%20confirm%20that%20GeoLoc%20pre-training%20markedly%20improves%20geo-location%20accuracy%20for%20GeoBridge%20while%20promoting%20cross-domain%20generalization%20and%20cross-modal%20knowledge%20transfer.%20The%20dataset%2C%20source%20code%2C%20and%20pretrained%20models%20were%20released%20at%20https%3A//github.com/MiliLab/GeoBridge.&entry.1838667208=http%3A//arxiv.org/abs/2512.02697v1&entry.124074799=Read"},
{"title": "Beyond Paired Data: Self-Supervised UAV Geo-Localization from Reference Imagery Alone", "author": "Tristan Amadei and Enric Meinhardt-Llopis and Benedicte Bascle and Corentin Abgrall and Gabriele Facciolo", "abstract": "Image-based localization in GNSS-denied environments is critical for UAV autonomy. Existing state-of-the-art approaches rely on matching UAV images to geo-referenced satellite images; however, they typically require large-scale, paired UAV-satellite datasets for training. Such data are costly to acquire and often unavailable, limiting their applicability. To address this challenge, we adopt a training paradigm that removes the need for UAV imagery during training by learning directly from satellite-view reference images. This is achieved through a dedicated augmentation strategy that simulates the visual domain shift between satellite and real-world UAV views. We introduce CAEVL, an efficient model designed to exploit this paradigm, and validate it on ViLD, a new and challenging dataset of real-world UAV images that we release to the community. Our method achieves competitive performance compared to approaches trained with paired data, demonstrating its effectiveness and strong generalization capabilities.", "link": "http://arxiv.org/abs/2512.02737v1", "date": "2025-12-02", "relevancy": 2.8351, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5892}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5764}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Paired%20Data%3A%20Self-Supervised%20UAV%20Geo-Localization%20from%20Reference%20Imagery%20Alone&body=Title%3A%20Beyond%20Paired%20Data%3A%20Self-Supervised%20UAV%20Geo-Localization%20from%20Reference%20Imagery%20Alone%0AAuthor%3A%20Tristan%20Amadei%20and%20Enric%20Meinhardt-Llopis%20and%20Benedicte%20Bascle%20and%20Corentin%20Abgrall%20and%20Gabriele%20Facciolo%0AAbstract%3A%20Image-based%20localization%20in%20GNSS-denied%20environments%20is%20critical%20for%20UAV%20autonomy.%20Existing%20state-of-the-art%20approaches%20rely%20on%20matching%20UAV%20images%20to%20geo-referenced%20satellite%20images%3B%20however%2C%20they%20typically%20require%20large-scale%2C%20paired%20UAV-satellite%20datasets%20for%20training.%20Such%20data%20are%20costly%20to%20acquire%20and%20often%20unavailable%2C%20limiting%20their%20applicability.%20To%20address%20this%20challenge%2C%20we%20adopt%20a%20training%20paradigm%20that%20removes%20the%20need%20for%20UAV%20imagery%20during%20training%20by%20learning%20directly%20from%20satellite-view%20reference%20images.%20This%20is%20achieved%20through%20a%20dedicated%20augmentation%20strategy%20that%20simulates%20the%20visual%20domain%20shift%20between%20satellite%20and%20real-world%20UAV%20views.%20We%20introduce%20CAEVL%2C%20an%20efficient%20model%20designed%20to%20exploit%20this%20paradigm%2C%20and%20validate%20it%20on%20ViLD%2C%20a%20new%20and%20challenging%20dataset%20of%20real-world%20UAV%20images%20that%20we%20release%20to%20the%20community.%20Our%20method%20achieves%20competitive%20performance%20compared%20to%20approaches%20trained%20with%20paired%20data%2C%20demonstrating%20its%20effectiveness%20and%20strong%20generalization%20capabilities.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02737v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Paired%2520Data%253A%2520Self-Supervised%2520UAV%2520Geo-Localization%2520from%2520Reference%2520Imagery%2520Alone%26entry.906535625%3DTristan%2520Amadei%2520and%2520Enric%2520Meinhardt-Llopis%2520and%2520Benedicte%2520Bascle%2520and%2520Corentin%2520Abgrall%2520and%2520Gabriele%2520Facciolo%26entry.1292438233%3DImage-based%2520localization%2520in%2520GNSS-denied%2520environments%2520is%2520critical%2520for%2520UAV%2520autonomy.%2520Existing%2520state-of-the-art%2520approaches%2520rely%2520on%2520matching%2520UAV%2520images%2520to%2520geo-referenced%2520satellite%2520images%253B%2520however%252C%2520they%2520typically%2520require%2520large-scale%252C%2520paired%2520UAV-satellite%2520datasets%2520for%2520training.%2520Such%2520data%2520are%2520costly%2520to%2520acquire%2520and%2520often%2520unavailable%252C%2520limiting%2520their%2520applicability.%2520To%2520address%2520this%2520challenge%252C%2520we%2520adopt%2520a%2520training%2520paradigm%2520that%2520removes%2520the%2520need%2520for%2520UAV%2520imagery%2520during%2520training%2520by%2520learning%2520directly%2520from%2520satellite-view%2520reference%2520images.%2520This%2520is%2520achieved%2520through%2520a%2520dedicated%2520augmentation%2520strategy%2520that%2520simulates%2520the%2520visual%2520domain%2520shift%2520between%2520satellite%2520and%2520real-world%2520UAV%2520views.%2520We%2520introduce%2520CAEVL%252C%2520an%2520efficient%2520model%2520designed%2520to%2520exploit%2520this%2520paradigm%252C%2520and%2520validate%2520it%2520on%2520ViLD%252C%2520a%2520new%2520and%2520challenging%2520dataset%2520of%2520real-world%2520UAV%2520images%2520that%2520we%2520release%2520to%2520the%2520community.%2520Our%2520method%2520achieves%2520competitive%2520performance%2520compared%2520to%2520approaches%2520trained%2520with%2520paired%2520data%252C%2520demonstrating%2520its%2520effectiveness%2520and%2520strong%2520generalization%2520capabilities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02737v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Paired%20Data%3A%20Self-Supervised%20UAV%20Geo-Localization%20from%20Reference%20Imagery%20Alone&entry.906535625=Tristan%20Amadei%20and%20Enric%20Meinhardt-Llopis%20and%20Benedicte%20Bascle%20and%20Corentin%20Abgrall%20and%20Gabriele%20Facciolo&entry.1292438233=Image-based%20localization%20in%20GNSS-denied%20environments%20is%20critical%20for%20UAV%20autonomy.%20Existing%20state-of-the-art%20approaches%20rely%20on%20matching%20UAV%20images%20to%20geo-referenced%20satellite%20images%3B%20however%2C%20they%20typically%20require%20large-scale%2C%20paired%20UAV-satellite%20datasets%20for%20training.%20Such%20data%20are%20costly%20to%20acquire%20and%20often%20unavailable%2C%20limiting%20their%20applicability.%20To%20address%20this%20challenge%2C%20we%20adopt%20a%20training%20paradigm%20that%20removes%20the%20need%20for%20UAV%20imagery%20during%20training%20by%20learning%20directly%20from%20satellite-view%20reference%20images.%20This%20is%20achieved%20through%20a%20dedicated%20augmentation%20strategy%20that%20simulates%20the%20visual%20domain%20shift%20between%20satellite%20and%20real-world%20UAV%20views.%20We%20introduce%20CAEVL%2C%20an%20efficient%20model%20designed%20to%20exploit%20this%20paradigm%2C%20and%20validate%20it%20on%20ViLD%2C%20a%20new%20and%20challenging%20dataset%20of%20real-world%20UAV%20images%20that%20we%20release%20to%20the%20community.%20Our%20method%20achieves%20competitive%20performance%20compared%20to%20approaches%20trained%20with%20paired%20data%2C%20demonstrating%20its%20effectiveness%20and%20strong%20generalization%20capabilities.&entry.1838667208=http%3A//arxiv.org/abs/2512.02737v1&entry.124074799=Read"},
{"title": "MRD: Multi-resolution Retrieval-Detection Fusion for High-Resolution Image Understanding", "author": "Fan Yang and Kaihao Zhang", "abstract": "Understanding high-resolution images remains a significant challenge for multimodal large language models (MLLMs). Recent study address this issue by dividing the image into smaller crops and computing the semantic similarity between each crop and a query using a pretrained retrieval-augmented generation (RAG) model. The most relevant crops are then selected to localize the target object and suppress irrelevant information. However, such crop-based processing can fragment complete objects across multiple crops, thereby disrupting the computation of semantic similarity. In our experiments, we find that image crops of objects with different sizes are better handled at different resolutions. Based on this observation, we propose Multi-resolution Retrieval-Detection (MRD), a training-free framework for high-resolution image understanding. To address the issue of semantic similarity bias caused by objects being split across different image crops, we propose a multi-resolution semantic fusion method, which integrates semantic similarity maps obtained at different resolutions to produce more accurate semantic information and preserve the integrity of target objects. Furthermore, to achieve direct localization of target objects at a global scale, we introduce an open-vocalbulary object detection (OVD) model that identifies object regions using a sliding-window approach.Experiments on high-resolution image understanding benchmarks using different MLLMs demonstrate the effectiveness of our approach.", "link": "http://arxiv.org/abs/2512.02906v1", "date": "2025-12-02", "relevancy": 2.8239, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5666}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5639}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5639}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MRD%3A%20Multi-resolution%20Retrieval-Detection%20Fusion%20for%20High-Resolution%20Image%20Understanding&body=Title%3A%20MRD%3A%20Multi-resolution%20Retrieval-Detection%20Fusion%20for%20High-Resolution%20Image%20Understanding%0AAuthor%3A%20Fan%20Yang%20and%20Kaihao%20Zhang%0AAbstract%3A%20Understanding%20high-resolution%20images%20remains%20a%20significant%20challenge%20for%20multimodal%20large%20language%20models%20%28MLLMs%29.%20Recent%20study%20address%20this%20issue%20by%20dividing%20the%20image%20into%20smaller%20crops%20and%20computing%20the%20semantic%20similarity%20between%20each%20crop%20and%20a%20query%20using%20a%20pretrained%20retrieval-augmented%20generation%20%28RAG%29%20model.%20The%20most%20relevant%20crops%20are%20then%20selected%20to%20localize%20the%20target%20object%20and%20suppress%20irrelevant%20information.%20However%2C%20such%20crop-based%20processing%20can%20fragment%20complete%20objects%20across%20multiple%20crops%2C%20thereby%20disrupting%20the%20computation%20of%20semantic%20similarity.%20In%20our%20experiments%2C%20we%20find%20that%20image%20crops%20of%20objects%20with%20different%20sizes%20are%20better%20handled%20at%20different%20resolutions.%20Based%20on%20this%20observation%2C%20we%20propose%20Multi-resolution%20Retrieval-Detection%20%28MRD%29%2C%20a%20training-free%20framework%20for%20high-resolution%20image%20understanding.%20To%20address%20the%20issue%20of%20semantic%20similarity%20bias%20caused%20by%20objects%20being%20split%20across%20different%20image%20crops%2C%20we%20propose%20a%20multi-resolution%20semantic%20fusion%20method%2C%20which%20integrates%20semantic%20similarity%20maps%20obtained%20at%20different%20resolutions%20to%20produce%20more%20accurate%20semantic%20information%20and%20preserve%20the%20integrity%20of%20target%20objects.%20Furthermore%2C%20to%20achieve%20direct%20localization%20of%20target%20objects%20at%20a%20global%20scale%2C%20we%20introduce%20an%20open-vocalbulary%20object%20detection%20%28OVD%29%20model%20that%20identifies%20object%20regions%20using%20a%20sliding-window%20approach.Experiments%20on%20high-resolution%20image%20understanding%20benchmarks%20using%20different%20MLLMs%20demonstrate%20the%20effectiveness%20of%20our%20approach.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02906v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMRD%253A%2520Multi-resolution%2520Retrieval-Detection%2520Fusion%2520for%2520High-Resolution%2520Image%2520Understanding%26entry.906535625%3DFan%2520Yang%2520and%2520Kaihao%2520Zhang%26entry.1292438233%3DUnderstanding%2520high-resolution%2520images%2520remains%2520a%2520significant%2520challenge%2520for%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529.%2520Recent%2520study%2520address%2520this%2520issue%2520by%2520dividing%2520the%2520image%2520into%2520smaller%2520crops%2520and%2520computing%2520the%2520semantic%2520similarity%2520between%2520each%2520crop%2520and%2520a%2520query%2520using%2520a%2520pretrained%2520retrieval-augmented%2520generation%2520%2528RAG%2529%2520model.%2520The%2520most%2520relevant%2520crops%2520are%2520then%2520selected%2520to%2520localize%2520the%2520target%2520object%2520and%2520suppress%2520irrelevant%2520information.%2520However%252C%2520such%2520crop-based%2520processing%2520can%2520fragment%2520complete%2520objects%2520across%2520multiple%2520crops%252C%2520thereby%2520disrupting%2520the%2520computation%2520of%2520semantic%2520similarity.%2520In%2520our%2520experiments%252C%2520we%2520find%2520that%2520image%2520crops%2520of%2520objects%2520with%2520different%2520sizes%2520are%2520better%2520handled%2520at%2520different%2520resolutions.%2520Based%2520on%2520this%2520observation%252C%2520we%2520propose%2520Multi-resolution%2520Retrieval-Detection%2520%2528MRD%2529%252C%2520a%2520training-free%2520framework%2520for%2520high-resolution%2520image%2520understanding.%2520To%2520address%2520the%2520issue%2520of%2520semantic%2520similarity%2520bias%2520caused%2520by%2520objects%2520being%2520split%2520across%2520different%2520image%2520crops%252C%2520we%2520propose%2520a%2520multi-resolution%2520semantic%2520fusion%2520method%252C%2520which%2520integrates%2520semantic%2520similarity%2520maps%2520obtained%2520at%2520different%2520resolutions%2520to%2520produce%2520more%2520accurate%2520semantic%2520information%2520and%2520preserve%2520the%2520integrity%2520of%2520target%2520objects.%2520Furthermore%252C%2520to%2520achieve%2520direct%2520localization%2520of%2520target%2520objects%2520at%2520a%2520global%2520scale%252C%2520we%2520introduce%2520an%2520open-vocalbulary%2520object%2520detection%2520%2528OVD%2529%2520model%2520that%2520identifies%2520object%2520regions%2520using%2520a%2520sliding-window%2520approach.Experiments%2520on%2520high-resolution%2520image%2520understanding%2520benchmarks%2520using%2520different%2520MLLMs%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02906v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MRD%3A%20Multi-resolution%20Retrieval-Detection%20Fusion%20for%20High-Resolution%20Image%20Understanding&entry.906535625=Fan%20Yang%20and%20Kaihao%20Zhang&entry.1292438233=Understanding%20high-resolution%20images%20remains%20a%20significant%20challenge%20for%20multimodal%20large%20language%20models%20%28MLLMs%29.%20Recent%20study%20address%20this%20issue%20by%20dividing%20the%20image%20into%20smaller%20crops%20and%20computing%20the%20semantic%20similarity%20between%20each%20crop%20and%20a%20query%20using%20a%20pretrained%20retrieval-augmented%20generation%20%28RAG%29%20model.%20The%20most%20relevant%20crops%20are%20then%20selected%20to%20localize%20the%20target%20object%20and%20suppress%20irrelevant%20information.%20However%2C%20such%20crop-based%20processing%20can%20fragment%20complete%20objects%20across%20multiple%20crops%2C%20thereby%20disrupting%20the%20computation%20of%20semantic%20similarity.%20In%20our%20experiments%2C%20we%20find%20that%20image%20crops%20of%20objects%20with%20different%20sizes%20are%20better%20handled%20at%20different%20resolutions.%20Based%20on%20this%20observation%2C%20we%20propose%20Multi-resolution%20Retrieval-Detection%20%28MRD%29%2C%20a%20training-free%20framework%20for%20high-resolution%20image%20understanding.%20To%20address%20the%20issue%20of%20semantic%20similarity%20bias%20caused%20by%20objects%20being%20split%20across%20different%20image%20crops%2C%20we%20propose%20a%20multi-resolution%20semantic%20fusion%20method%2C%20which%20integrates%20semantic%20similarity%20maps%20obtained%20at%20different%20resolutions%20to%20produce%20more%20accurate%20semantic%20information%20and%20preserve%20the%20integrity%20of%20target%20objects.%20Furthermore%2C%20to%20achieve%20direct%20localization%20of%20target%20objects%20at%20a%20global%20scale%2C%20we%20introduce%20an%20open-vocalbulary%20object%20detection%20%28OVD%29%20model%20that%20identifies%20object%20regions%20using%20a%20sliding-window%20approach.Experiments%20on%20high-resolution%20image%20understanding%20benchmarks%20using%20different%20MLLMs%20demonstrate%20the%20effectiveness%20of%20our%20approach.&entry.1838667208=http%3A//arxiv.org/abs/2512.02906v1&entry.124074799=Read"},
{"title": "Multimodal LLMs See Sentiment", "author": "Neemias B. da Silva and John Harrison and Rodrigo Minetto and Myriam R. Delgado and Bogdan T. Nassu and Thiago H. Silva", "abstract": "Understanding how visual content communicates sentiment is critical in an era where online interaction is increasingly dominated by this kind of media on social platforms. However, this remains a challenging problem, as sentiment perception is closely tied to complex, scene-level semantics. In this paper, we propose an original framework, MLLMsent, to investigate the sentiment reasoning capabilities of Multimodal Large Language Models (MLLMs) through three perspectives: (1) using those MLLMs for direct sentiment classification from images; (2) associating them with pre-trained LLMs for sentiment analysis on automatically generated image descriptions; and (3) fine-tuning the LLMs on sentiment-labeled image descriptions. Experiments on a recent and established benchmark demonstrate that our proposal, particularly the fine-tuned approach, achieves state-of-the-art results outperforming Lexicon-, CNN-, and Transformer-based baselines by up to 30.9%, 64.8%, and 42.4%, respectively, across different levels of evaluators' agreement and sentiment polarity categories. Remarkably, in a cross-dataset test, without any training on these new data, our model still outperforms, by up to 8.26%, the best runner-up, which has been trained directly on them. These results highlight the potential of the proposed visual reasoning scheme for advancing affective computing, while also establishing new benchmarks for future research.", "link": "http://arxiv.org/abs/2508.16873v2", "date": "2025-12-02", "relevancy": 2.7568, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5668}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5436}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20LLMs%20See%20Sentiment&body=Title%3A%20Multimodal%20LLMs%20See%20Sentiment%0AAuthor%3A%20Neemias%20B.%20da%20Silva%20and%20John%20Harrison%20and%20Rodrigo%20Minetto%20and%20Myriam%20R.%20Delgado%20and%20Bogdan%20T.%20Nassu%20and%20Thiago%20H.%20Silva%0AAbstract%3A%20Understanding%20how%20visual%20content%20communicates%20sentiment%20is%20critical%20in%20an%20era%20where%20online%20interaction%20is%20increasingly%20dominated%20by%20this%20kind%20of%20media%20on%20social%20platforms.%20However%2C%20this%20remains%20a%20challenging%20problem%2C%20as%20sentiment%20perception%20is%20closely%20tied%20to%20complex%2C%20scene-level%20semantics.%20In%20this%20paper%2C%20we%20propose%20an%20original%20framework%2C%20MLLMsent%2C%20to%20investigate%20the%20sentiment%20reasoning%20capabilities%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20through%20three%20perspectives%3A%20%281%29%20using%20those%20MLLMs%20for%20direct%20sentiment%20classification%20from%20images%3B%20%282%29%20associating%20them%20with%20pre-trained%20LLMs%20for%20sentiment%20analysis%20on%20automatically%20generated%20image%20descriptions%3B%20and%20%283%29%20fine-tuning%20the%20LLMs%20on%20sentiment-labeled%20image%20descriptions.%20Experiments%20on%20a%20recent%20and%20established%20benchmark%20demonstrate%20that%20our%20proposal%2C%20particularly%20the%20fine-tuned%20approach%2C%20achieves%20state-of-the-art%20results%20outperforming%20Lexicon-%2C%20CNN-%2C%20and%20Transformer-based%20baselines%20by%20up%20to%2030.9%25%2C%2064.8%25%2C%20and%2042.4%25%2C%20respectively%2C%20across%20different%20levels%20of%20evaluators%27%20agreement%20and%20sentiment%20polarity%20categories.%20Remarkably%2C%20in%20a%20cross-dataset%20test%2C%20without%20any%20training%20on%20these%20new%20data%2C%20our%20model%20still%20outperforms%2C%20by%20up%20to%208.26%25%2C%20the%20best%20runner-up%2C%20which%20has%20been%20trained%20directly%20on%20them.%20These%20results%20highlight%20the%20potential%20of%20the%20proposed%20visual%20reasoning%20scheme%20for%20advancing%20affective%20computing%2C%20while%20also%20establishing%20new%20benchmarks%20for%20future%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2508.16873v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520LLMs%2520See%2520Sentiment%26entry.906535625%3DNeemias%2520B.%2520da%2520Silva%2520and%2520John%2520Harrison%2520and%2520Rodrigo%2520Minetto%2520and%2520Myriam%2520R.%2520Delgado%2520and%2520Bogdan%2520T.%2520Nassu%2520and%2520Thiago%2520H.%2520Silva%26entry.1292438233%3DUnderstanding%2520how%2520visual%2520content%2520communicates%2520sentiment%2520is%2520critical%2520in%2520an%2520era%2520where%2520online%2520interaction%2520is%2520increasingly%2520dominated%2520by%2520this%2520kind%2520of%2520media%2520on%2520social%2520platforms.%2520However%252C%2520this%2520remains%2520a%2520challenging%2520problem%252C%2520as%2520sentiment%2520perception%2520is%2520closely%2520tied%2520to%2520complex%252C%2520scene-level%2520semantics.%2520In%2520this%2520paper%252C%2520we%2520propose%2520an%2520original%2520framework%252C%2520MLLMsent%252C%2520to%2520investigate%2520the%2520sentiment%2520reasoning%2520capabilities%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520through%2520three%2520perspectives%253A%2520%25281%2529%2520using%2520those%2520MLLMs%2520for%2520direct%2520sentiment%2520classification%2520from%2520images%253B%2520%25282%2529%2520associating%2520them%2520with%2520pre-trained%2520LLMs%2520for%2520sentiment%2520analysis%2520on%2520automatically%2520generated%2520image%2520descriptions%253B%2520and%2520%25283%2529%2520fine-tuning%2520the%2520LLMs%2520on%2520sentiment-labeled%2520image%2520descriptions.%2520Experiments%2520on%2520a%2520recent%2520and%2520established%2520benchmark%2520demonstrate%2520that%2520our%2520proposal%252C%2520particularly%2520the%2520fine-tuned%2520approach%252C%2520achieves%2520state-of-the-art%2520results%2520outperforming%2520Lexicon-%252C%2520CNN-%252C%2520and%2520Transformer-based%2520baselines%2520by%2520up%2520to%252030.9%2525%252C%252064.8%2525%252C%2520and%252042.4%2525%252C%2520respectively%252C%2520across%2520different%2520levels%2520of%2520evaluators%2527%2520agreement%2520and%2520sentiment%2520polarity%2520categories.%2520Remarkably%252C%2520in%2520a%2520cross-dataset%2520test%252C%2520without%2520any%2520training%2520on%2520these%2520new%2520data%252C%2520our%2520model%2520still%2520outperforms%252C%2520by%2520up%2520to%25208.26%2525%252C%2520the%2520best%2520runner-up%252C%2520which%2520has%2520been%2520trained%2520directly%2520on%2520them.%2520These%2520results%2520highlight%2520the%2520potential%2520of%2520the%2520proposed%2520visual%2520reasoning%2520scheme%2520for%2520advancing%2520affective%2520computing%252C%2520while%2520also%2520establishing%2520new%2520benchmarks%2520for%2520future%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.16873v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20LLMs%20See%20Sentiment&entry.906535625=Neemias%20B.%20da%20Silva%20and%20John%20Harrison%20and%20Rodrigo%20Minetto%20and%20Myriam%20R.%20Delgado%20and%20Bogdan%20T.%20Nassu%20and%20Thiago%20H.%20Silva&entry.1292438233=Understanding%20how%20visual%20content%20communicates%20sentiment%20is%20critical%20in%20an%20era%20where%20online%20interaction%20is%20increasingly%20dominated%20by%20this%20kind%20of%20media%20on%20social%20platforms.%20However%2C%20this%20remains%20a%20challenging%20problem%2C%20as%20sentiment%20perception%20is%20closely%20tied%20to%20complex%2C%20scene-level%20semantics.%20In%20this%20paper%2C%20we%20propose%20an%20original%20framework%2C%20MLLMsent%2C%20to%20investigate%20the%20sentiment%20reasoning%20capabilities%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20through%20three%20perspectives%3A%20%281%29%20using%20those%20MLLMs%20for%20direct%20sentiment%20classification%20from%20images%3B%20%282%29%20associating%20them%20with%20pre-trained%20LLMs%20for%20sentiment%20analysis%20on%20automatically%20generated%20image%20descriptions%3B%20and%20%283%29%20fine-tuning%20the%20LLMs%20on%20sentiment-labeled%20image%20descriptions.%20Experiments%20on%20a%20recent%20and%20established%20benchmark%20demonstrate%20that%20our%20proposal%2C%20particularly%20the%20fine-tuned%20approach%2C%20achieves%20state-of-the-art%20results%20outperforming%20Lexicon-%2C%20CNN-%2C%20and%20Transformer-based%20baselines%20by%20up%20to%2030.9%25%2C%2064.8%25%2C%20and%2042.4%25%2C%20respectively%2C%20across%20different%20levels%20of%20evaluators%27%20agreement%20and%20sentiment%20polarity%20categories.%20Remarkably%2C%20in%20a%20cross-dataset%20test%2C%20without%20any%20training%20on%20these%20new%20data%2C%20our%20model%20still%20outperforms%2C%20by%20up%20to%208.26%25%2C%20the%20best%20runner-up%2C%20which%20has%20been%20trained%20directly%20on%20them.%20These%20results%20highlight%20the%20potential%20of%20the%20proposed%20visual%20reasoning%20scheme%20for%20advancing%20affective%20computing%2C%20while%20also%20establishing%20new%20benchmarks%20for%20future%20research.&entry.1838667208=http%3A//arxiv.org/abs/2508.16873v2&entry.124074799=Read"},
{"title": "Bias Beyond Demographics: Probing Decision Boundaries in Black-Box LVLMs via Counterfactual VQA", "author": "Zaiying Zhao and Toshihiko Yamasaki", "abstract": "Recent advances in large vision-language models (LVLMs) have amplified concerns about fairness, yet existing evaluations remain confined to demographic attributes and often conflate fairness with refusal behavior. This paper broadens the scope of fairness by introducing a counterfactual VQA benchmark that probes the decision boundaries of closed-source LVLMs under controlled context shifts. Each image pair differs in a single visual attribute that has been validated as irrelevant to the question, enabling ground-truth-free and refusal-aware analysis of reasoning stability. Comprehensive experiments reveal that non-demographic attributes, such as environmental context or social behavior, distort LVLM decision-making more strongly than demographic ones. Moreover, instruction-based debiasing shows limited effectiveness and can even amplify these asymmetries, whereas exposure to a small number of human norm validated examples from our benchmark encourages more consistent and balanced responses, highlighting its potential not only as an evaluative framework but also as a means for understanding and improving model behavior. Together, these results provide a practial basis for auditing contextual biases even in black-box LVLMs and contribute to more transparent and equitable multimodal reasoning.", "link": "http://arxiv.org/abs/2508.03079v2", "date": "2025-12-02", "relevancy": 2.6914, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5545}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5302}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5302}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bias%20Beyond%20Demographics%3A%20Probing%20Decision%20Boundaries%20in%20Black-Box%20LVLMs%20via%20Counterfactual%20VQA&body=Title%3A%20Bias%20Beyond%20Demographics%3A%20Probing%20Decision%20Boundaries%20in%20Black-Box%20LVLMs%20via%20Counterfactual%20VQA%0AAuthor%3A%20Zaiying%20Zhao%20and%20Toshihiko%20Yamasaki%0AAbstract%3A%20Recent%20advances%20in%20large%20vision-language%20models%20%28LVLMs%29%20have%20amplified%20concerns%20about%20fairness%2C%20yet%20existing%20evaluations%20remain%20confined%20to%20demographic%20attributes%20and%20often%20conflate%20fairness%20with%20refusal%20behavior.%20This%20paper%20broadens%20the%20scope%20of%20fairness%20by%20introducing%20a%20counterfactual%20VQA%20benchmark%20that%20probes%20the%20decision%20boundaries%20of%20closed-source%20LVLMs%20under%20controlled%20context%20shifts.%20Each%20image%20pair%20differs%20in%20a%20single%20visual%20attribute%20that%20has%20been%20validated%20as%20irrelevant%20to%20the%20question%2C%20enabling%20ground-truth-free%20and%20refusal-aware%20analysis%20of%20reasoning%20stability.%20Comprehensive%20experiments%20reveal%20that%20non-demographic%20attributes%2C%20such%20as%20environmental%20context%20or%20social%20behavior%2C%20distort%20LVLM%20decision-making%20more%20strongly%20than%20demographic%20ones.%20Moreover%2C%20instruction-based%20debiasing%20shows%20limited%20effectiveness%20and%20can%20even%20amplify%20these%20asymmetries%2C%20whereas%20exposure%20to%20a%20small%20number%20of%20human%20norm%20validated%20examples%20from%20our%20benchmark%20encourages%20more%20consistent%20and%20balanced%20responses%2C%20highlighting%20its%20potential%20not%20only%20as%20an%20evaluative%20framework%20but%20also%20as%20a%20means%20for%20understanding%20and%20improving%20model%20behavior.%20Together%2C%20these%20results%20provide%20a%20practial%20basis%20for%20auditing%20contextual%20biases%20even%20in%20black-box%20LVLMs%20and%20contribute%20to%20more%20transparent%20and%20equitable%20multimodal%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2508.03079v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBias%2520Beyond%2520Demographics%253A%2520Probing%2520Decision%2520Boundaries%2520in%2520Black-Box%2520LVLMs%2520via%2520Counterfactual%2520VQA%26entry.906535625%3DZaiying%2520Zhao%2520and%2520Toshihiko%2520Yamasaki%26entry.1292438233%3DRecent%2520advances%2520in%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520have%2520amplified%2520concerns%2520about%2520fairness%252C%2520yet%2520existing%2520evaluations%2520remain%2520confined%2520to%2520demographic%2520attributes%2520and%2520often%2520conflate%2520fairness%2520with%2520refusal%2520behavior.%2520This%2520paper%2520broadens%2520the%2520scope%2520of%2520fairness%2520by%2520introducing%2520a%2520counterfactual%2520VQA%2520benchmark%2520that%2520probes%2520the%2520decision%2520boundaries%2520of%2520closed-source%2520LVLMs%2520under%2520controlled%2520context%2520shifts.%2520Each%2520image%2520pair%2520differs%2520in%2520a%2520single%2520visual%2520attribute%2520that%2520has%2520been%2520validated%2520as%2520irrelevant%2520to%2520the%2520question%252C%2520enabling%2520ground-truth-free%2520and%2520refusal-aware%2520analysis%2520of%2520reasoning%2520stability.%2520Comprehensive%2520experiments%2520reveal%2520that%2520non-demographic%2520attributes%252C%2520such%2520as%2520environmental%2520context%2520or%2520social%2520behavior%252C%2520distort%2520LVLM%2520decision-making%2520more%2520strongly%2520than%2520demographic%2520ones.%2520Moreover%252C%2520instruction-based%2520debiasing%2520shows%2520limited%2520effectiveness%2520and%2520can%2520even%2520amplify%2520these%2520asymmetries%252C%2520whereas%2520exposure%2520to%2520a%2520small%2520number%2520of%2520human%2520norm%2520validated%2520examples%2520from%2520our%2520benchmark%2520encourages%2520more%2520consistent%2520and%2520balanced%2520responses%252C%2520highlighting%2520its%2520potential%2520not%2520only%2520as%2520an%2520evaluative%2520framework%2520but%2520also%2520as%2520a%2520means%2520for%2520understanding%2520and%2520improving%2520model%2520behavior.%2520Together%252C%2520these%2520results%2520provide%2520a%2520practial%2520basis%2520for%2520auditing%2520contextual%2520biases%2520even%2520in%2520black-box%2520LVLMs%2520and%2520contribute%2520to%2520more%2520transparent%2520and%2520equitable%2520multimodal%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.03079v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bias%20Beyond%20Demographics%3A%20Probing%20Decision%20Boundaries%20in%20Black-Box%20LVLMs%20via%20Counterfactual%20VQA&entry.906535625=Zaiying%20Zhao%20and%20Toshihiko%20Yamasaki&entry.1292438233=Recent%20advances%20in%20large%20vision-language%20models%20%28LVLMs%29%20have%20amplified%20concerns%20about%20fairness%2C%20yet%20existing%20evaluations%20remain%20confined%20to%20demographic%20attributes%20and%20often%20conflate%20fairness%20with%20refusal%20behavior.%20This%20paper%20broadens%20the%20scope%20of%20fairness%20by%20introducing%20a%20counterfactual%20VQA%20benchmark%20that%20probes%20the%20decision%20boundaries%20of%20closed-source%20LVLMs%20under%20controlled%20context%20shifts.%20Each%20image%20pair%20differs%20in%20a%20single%20visual%20attribute%20that%20has%20been%20validated%20as%20irrelevant%20to%20the%20question%2C%20enabling%20ground-truth-free%20and%20refusal-aware%20analysis%20of%20reasoning%20stability.%20Comprehensive%20experiments%20reveal%20that%20non-demographic%20attributes%2C%20such%20as%20environmental%20context%20or%20social%20behavior%2C%20distort%20LVLM%20decision-making%20more%20strongly%20than%20demographic%20ones.%20Moreover%2C%20instruction-based%20debiasing%20shows%20limited%20effectiveness%20and%20can%20even%20amplify%20these%20asymmetries%2C%20whereas%20exposure%20to%20a%20small%20number%20of%20human%20norm%20validated%20examples%20from%20our%20benchmark%20encourages%20more%20consistent%20and%20balanced%20responses%2C%20highlighting%20its%20potential%20not%20only%20as%20an%20evaluative%20framework%20but%20also%20as%20a%20means%20for%20understanding%20and%20improving%20model%20behavior.%20Together%2C%20these%20results%20provide%20a%20practial%20basis%20for%20auditing%20contextual%20biases%20even%20in%20black-box%20LVLMs%20and%20contribute%20to%20more%20transparent%20and%20equitable%20multimodal%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2508.03079v2&entry.124074799=Read"},
{"title": "Provable Scaling Laws of Feature Emergence from Learning Dynamics of Grokking", "author": "Yuandong Tian", "abstract": "While the phenomenon of grokking, i.e., delayed generalization, has been studied extensively, it remains an open problem whether there is a mathematical framework that characterizes what kind of features will emerge, how and in which conditions it happens, and is closely related to the gradient dynamics of the training, for complex structured inputs. We propose a novel framework, named $\\mathbf{Li}_2$, that captures three key stages for the grokking behavior of 2-layer nonlinear networks: (I) Lazy learning, (II) independent feature learning and (III) interactive feature learning. At the lazy learning stage, top layer overfits to random hidden representation and the model appears to memorize, and at the same time, the backpropagated gradient $G_F$ from the top layer now carries information about the target label, with a specific structure that enables each hidden node to learn their representation independently. Interestingly, the independent dynamics follows exactly the gradient ascent of an energy function $E$, and its local maxima are precisely the emerging features. We study whether these local-optima induced features are generalizable, their representation power, and how they change on sample size, in group arithmetic tasks. When hidden nodes start to interact in the later stage of learning, we provably show how $G_F$ changes to focus on missing features that need to be learned. Our study sheds lights on roles played by key hyperparameters such as weight decay, learning rate and sample sizes in grokking, leads to provable scaling laws of feature emergence, memorization and generalization, and reveals why recent optimizers such as Muon can be effective, from the first principles of gradient dynamics. Our analysis can be extended to multi-layers. The code is available at https://github.com/yuandong-tian/understanding/tree/main/ssl/real-dataset/cogo.", "link": "http://arxiv.org/abs/2509.21519v5", "date": "2025-12-02", "relevancy": 2.6843, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5734}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5307}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Provable%20Scaling%20Laws%20of%20Feature%20Emergence%20from%20Learning%20Dynamics%20of%20Grokking&body=Title%3A%20Provable%20Scaling%20Laws%20of%20Feature%20Emergence%20from%20Learning%20Dynamics%20of%20Grokking%0AAuthor%3A%20Yuandong%20Tian%0AAbstract%3A%20While%20the%20phenomenon%20of%20grokking%2C%20i.e.%2C%20delayed%20generalization%2C%20has%20been%20studied%20extensively%2C%20it%20remains%20an%20open%20problem%20whether%20there%20is%20a%20mathematical%20framework%20that%20characterizes%20what%20kind%20of%20features%20will%20emerge%2C%20how%20and%20in%20which%20conditions%20it%20happens%2C%20and%20is%20closely%20related%20to%20the%20gradient%20dynamics%20of%20the%20training%2C%20for%20complex%20structured%20inputs.%20We%20propose%20a%20novel%20framework%2C%20named%20%24%5Cmathbf%7BLi%7D_2%24%2C%20that%20captures%20three%20key%20stages%20for%20the%20grokking%20behavior%20of%202-layer%20nonlinear%20networks%3A%20%28I%29%20Lazy%20learning%2C%20%28II%29%20independent%20feature%20learning%20and%20%28III%29%20interactive%20feature%20learning.%20At%20the%20lazy%20learning%20stage%2C%20top%20layer%20overfits%20to%20random%20hidden%20representation%20and%20the%20model%20appears%20to%20memorize%2C%20and%20at%20the%20same%20time%2C%20the%20backpropagated%20gradient%20%24G_F%24%20from%20the%20top%20layer%20now%20carries%20information%20about%20the%20target%20label%2C%20with%20a%20specific%20structure%20that%20enables%20each%20hidden%20node%20to%20learn%20their%20representation%20independently.%20Interestingly%2C%20the%20independent%20dynamics%20follows%20exactly%20the%20gradient%20ascent%20of%20an%20energy%20function%20%24E%24%2C%20and%20its%20local%20maxima%20are%20precisely%20the%20emerging%20features.%20We%20study%20whether%20these%20local-optima%20induced%20features%20are%20generalizable%2C%20their%20representation%20power%2C%20and%20how%20they%20change%20on%20sample%20size%2C%20in%20group%20arithmetic%20tasks.%20When%20hidden%20nodes%20start%20to%20interact%20in%20the%20later%20stage%20of%20learning%2C%20we%20provably%20show%20how%20%24G_F%24%20changes%20to%20focus%20on%20missing%20features%20that%20need%20to%20be%20learned.%20Our%20study%20sheds%20lights%20on%20roles%20played%20by%20key%20hyperparameters%20such%20as%20weight%20decay%2C%20learning%20rate%20and%20sample%20sizes%20in%20grokking%2C%20leads%20to%20provable%20scaling%20laws%20of%20feature%20emergence%2C%20memorization%20and%20generalization%2C%20and%20reveals%20why%20recent%20optimizers%20such%20as%20Muon%20can%20be%20effective%2C%20from%20the%20first%20principles%20of%20gradient%20dynamics.%20Our%20analysis%20can%20be%20extended%20to%20multi-layers.%20The%20code%20is%20available%20at%20https%3A//github.com/yuandong-tian/understanding/tree/main/ssl/real-dataset/cogo.%0ALink%3A%20http%3A//arxiv.org/abs/2509.21519v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProvable%2520Scaling%2520Laws%2520of%2520Feature%2520Emergence%2520from%2520Learning%2520Dynamics%2520of%2520Grokking%26entry.906535625%3DYuandong%2520Tian%26entry.1292438233%3DWhile%2520the%2520phenomenon%2520of%2520grokking%252C%2520i.e.%252C%2520delayed%2520generalization%252C%2520has%2520been%2520studied%2520extensively%252C%2520it%2520remains%2520an%2520open%2520problem%2520whether%2520there%2520is%2520a%2520mathematical%2520framework%2520that%2520characterizes%2520what%2520kind%2520of%2520features%2520will%2520emerge%252C%2520how%2520and%2520in%2520which%2520conditions%2520it%2520happens%252C%2520and%2520is%2520closely%2520related%2520to%2520the%2520gradient%2520dynamics%2520of%2520the%2520training%252C%2520for%2520complex%2520structured%2520inputs.%2520We%2520propose%2520a%2520novel%2520framework%252C%2520named%2520%2524%255Cmathbf%257BLi%257D_2%2524%252C%2520that%2520captures%2520three%2520key%2520stages%2520for%2520the%2520grokking%2520behavior%2520of%25202-layer%2520nonlinear%2520networks%253A%2520%2528I%2529%2520Lazy%2520learning%252C%2520%2528II%2529%2520independent%2520feature%2520learning%2520and%2520%2528III%2529%2520interactive%2520feature%2520learning.%2520At%2520the%2520lazy%2520learning%2520stage%252C%2520top%2520layer%2520overfits%2520to%2520random%2520hidden%2520representation%2520and%2520the%2520model%2520appears%2520to%2520memorize%252C%2520and%2520at%2520the%2520same%2520time%252C%2520the%2520backpropagated%2520gradient%2520%2524G_F%2524%2520from%2520the%2520top%2520layer%2520now%2520carries%2520information%2520about%2520the%2520target%2520label%252C%2520with%2520a%2520specific%2520structure%2520that%2520enables%2520each%2520hidden%2520node%2520to%2520learn%2520their%2520representation%2520independently.%2520Interestingly%252C%2520the%2520independent%2520dynamics%2520follows%2520exactly%2520the%2520gradient%2520ascent%2520of%2520an%2520energy%2520function%2520%2524E%2524%252C%2520and%2520its%2520local%2520maxima%2520are%2520precisely%2520the%2520emerging%2520features.%2520We%2520study%2520whether%2520these%2520local-optima%2520induced%2520features%2520are%2520generalizable%252C%2520their%2520representation%2520power%252C%2520and%2520how%2520they%2520change%2520on%2520sample%2520size%252C%2520in%2520group%2520arithmetic%2520tasks.%2520When%2520hidden%2520nodes%2520start%2520to%2520interact%2520in%2520the%2520later%2520stage%2520of%2520learning%252C%2520we%2520provably%2520show%2520how%2520%2524G_F%2524%2520changes%2520to%2520focus%2520on%2520missing%2520features%2520that%2520need%2520to%2520be%2520learned.%2520Our%2520study%2520sheds%2520lights%2520on%2520roles%2520played%2520by%2520key%2520hyperparameters%2520such%2520as%2520weight%2520decay%252C%2520learning%2520rate%2520and%2520sample%2520sizes%2520in%2520grokking%252C%2520leads%2520to%2520provable%2520scaling%2520laws%2520of%2520feature%2520emergence%252C%2520memorization%2520and%2520generalization%252C%2520and%2520reveals%2520why%2520recent%2520optimizers%2520such%2520as%2520Muon%2520can%2520be%2520effective%252C%2520from%2520the%2520first%2520principles%2520of%2520gradient%2520dynamics.%2520Our%2520analysis%2520can%2520be%2520extended%2520to%2520multi-layers.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/yuandong-tian/understanding/tree/main/ssl/real-dataset/cogo.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.21519v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Provable%20Scaling%20Laws%20of%20Feature%20Emergence%20from%20Learning%20Dynamics%20of%20Grokking&entry.906535625=Yuandong%20Tian&entry.1292438233=While%20the%20phenomenon%20of%20grokking%2C%20i.e.%2C%20delayed%20generalization%2C%20has%20been%20studied%20extensively%2C%20it%20remains%20an%20open%20problem%20whether%20there%20is%20a%20mathematical%20framework%20that%20characterizes%20what%20kind%20of%20features%20will%20emerge%2C%20how%20and%20in%20which%20conditions%20it%20happens%2C%20and%20is%20closely%20related%20to%20the%20gradient%20dynamics%20of%20the%20training%2C%20for%20complex%20structured%20inputs.%20We%20propose%20a%20novel%20framework%2C%20named%20%24%5Cmathbf%7BLi%7D_2%24%2C%20that%20captures%20three%20key%20stages%20for%20the%20grokking%20behavior%20of%202-layer%20nonlinear%20networks%3A%20%28I%29%20Lazy%20learning%2C%20%28II%29%20independent%20feature%20learning%20and%20%28III%29%20interactive%20feature%20learning.%20At%20the%20lazy%20learning%20stage%2C%20top%20layer%20overfits%20to%20random%20hidden%20representation%20and%20the%20model%20appears%20to%20memorize%2C%20and%20at%20the%20same%20time%2C%20the%20backpropagated%20gradient%20%24G_F%24%20from%20the%20top%20layer%20now%20carries%20information%20about%20the%20target%20label%2C%20with%20a%20specific%20structure%20that%20enables%20each%20hidden%20node%20to%20learn%20their%20representation%20independently.%20Interestingly%2C%20the%20independent%20dynamics%20follows%20exactly%20the%20gradient%20ascent%20of%20an%20energy%20function%20%24E%24%2C%20and%20its%20local%20maxima%20are%20precisely%20the%20emerging%20features.%20We%20study%20whether%20these%20local-optima%20induced%20features%20are%20generalizable%2C%20their%20representation%20power%2C%20and%20how%20they%20change%20on%20sample%20size%2C%20in%20group%20arithmetic%20tasks.%20When%20hidden%20nodes%20start%20to%20interact%20in%20the%20later%20stage%20of%20learning%2C%20we%20provably%20show%20how%20%24G_F%24%20changes%20to%20focus%20on%20missing%20features%20that%20need%20to%20be%20learned.%20Our%20study%20sheds%20lights%20on%20roles%20played%20by%20key%20hyperparameters%20such%20as%20weight%20decay%2C%20learning%20rate%20and%20sample%20sizes%20in%20grokking%2C%20leads%20to%20provable%20scaling%20laws%20of%20feature%20emergence%2C%20memorization%20and%20generalization%2C%20and%20reveals%20why%20recent%20optimizers%20such%20as%20Muon%20can%20be%20effective%2C%20from%20the%20first%20principles%20of%20gradient%20dynamics.%20Our%20analysis%20can%20be%20extended%20to%20multi-layers.%20The%20code%20is%20available%20at%20https%3A//github.com/yuandong-tian/understanding/tree/main/ssl/real-dataset/cogo.&entry.1838667208=http%3A//arxiv.org/abs/2509.21519v5&entry.124074799=Read"},
{"title": "PPTBench: Towards Holistic Evaluation of Large Language Models for PowerPoint Layout and Design Understanding", "author": "Zheng Huang and Xukai Liu and Tianyu Hu and Kai Zhang and Ye Liu", "abstract": "PowerPoint presentations combine rich textual content with structured visual layouts, making them a natural testbed for evaluating the multimodal reasoning and layout understanding abilities of modern MLLMs. However, existing benchmarks focus solely on narrow subtasks while overlooking layout-centric challenges, which are central to real-world slide creation and editing. To bridge this gap, we introduce PPTBench, a comprehensive multimodal benchmark for evaluating LLMs on PowerPoint-related tasks. Leveraging a diverse source of 958 PPTX files, PPTBench evaluates models across four categories with 4,439 samples, including Detection, Understanding, Modification, and Generation. Our experiments reveal a substantial gap between semantic understanding and visual-layout reasoning in current MLLMs: models can interpret slide content but fail to produce coherent spatial arrangements. Ablation and further analysis show that current MLLMs struggle to combine visual cues with JSON-based layout structures and fail to integrate visual information into their API planning ability. And case studies visually expose systematic layout errors such as misalignment and element overlap. These findings provides a new perspective on evaluating VLLMs in PPT scenarios, highlighting challenges and directions for future research on visual-structural reasoning and coherent slide generation. All datasets and code are fully released to support reproducibility and future research.", "link": "http://arxiv.org/abs/2512.02624v1", "date": "2025-12-02", "relevancy": 2.682, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5611}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5241}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5241}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PPTBench%3A%20Towards%20Holistic%20Evaluation%20of%20Large%20Language%20Models%20for%20PowerPoint%20Layout%20and%20Design%20Understanding&body=Title%3A%20PPTBench%3A%20Towards%20Holistic%20Evaluation%20of%20Large%20Language%20Models%20for%20PowerPoint%20Layout%20and%20Design%20Understanding%0AAuthor%3A%20Zheng%20Huang%20and%20Xukai%20Liu%20and%20Tianyu%20Hu%20and%20Kai%20Zhang%20and%20Ye%20Liu%0AAbstract%3A%20PowerPoint%20presentations%20combine%20rich%20textual%20content%20with%20structured%20visual%20layouts%2C%20making%20them%20a%20natural%20testbed%20for%20evaluating%20the%20multimodal%20reasoning%20and%20layout%20understanding%20abilities%20of%20modern%20MLLMs.%20However%2C%20existing%20benchmarks%20focus%20solely%20on%20narrow%20subtasks%20while%20overlooking%20layout-centric%20challenges%2C%20which%20are%20central%20to%20real-world%20slide%20creation%20and%20editing.%20To%20bridge%20this%20gap%2C%20we%20introduce%20PPTBench%2C%20a%20comprehensive%20multimodal%20benchmark%20for%20evaluating%20LLMs%20on%20PowerPoint-related%20tasks.%20Leveraging%20a%20diverse%20source%20of%20958%20PPTX%20files%2C%20PPTBench%20evaluates%20models%20across%20four%20categories%20with%204%2C439%20samples%2C%20including%20Detection%2C%20Understanding%2C%20Modification%2C%20and%20Generation.%20Our%20experiments%20reveal%20a%20substantial%20gap%20between%20semantic%20understanding%20and%20visual-layout%20reasoning%20in%20current%20MLLMs%3A%20models%20can%20interpret%20slide%20content%20but%20fail%20to%20produce%20coherent%20spatial%20arrangements.%20Ablation%20and%20further%20analysis%20show%20that%20current%20MLLMs%20struggle%20to%20combine%20visual%20cues%20with%20JSON-based%20layout%20structures%20and%20fail%20to%20integrate%20visual%20information%20into%20their%20API%20planning%20ability.%20And%20case%20studies%20visually%20expose%20systematic%20layout%20errors%20such%20as%20misalignment%20and%20element%20overlap.%20These%20findings%20provides%20a%20new%20perspective%20on%20evaluating%20VLLMs%20in%20PPT%20scenarios%2C%20highlighting%20challenges%20and%20directions%20for%20future%20research%20on%20visual-structural%20reasoning%20and%20coherent%20slide%20generation.%20All%20datasets%20and%20code%20are%20fully%20released%20to%20support%20reproducibility%20and%20future%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02624v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPPTBench%253A%2520Towards%2520Holistic%2520Evaluation%2520of%2520Large%2520Language%2520Models%2520for%2520PowerPoint%2520Layout%2520and%2520Design%2520Understanding%26entry.906535625%3DZheng%2520Huang%2520and%2520Xukai%2520Liu%2520and%2520Tianyu%2520Hu%2520and%2520Kai%2520Zhang%2520and%2520Ye%2520Liu%26entry.1292438233%3DPowerPoint%2520presentations%2520combine%2520rich%2520textual%2520content%2520with%2520structured%2520visual%2520layouts%252C%2520making%2520them%2520a%2520natural%2520testbed%2520for%2520evaluating%2520the%2520multimodal%2520reasoning%2520and%2520layout%2520understanding%2520abilities%2520of%2520modern%2520MLLMs.%2520However%252C%2520existing%2520benchmarks%2520focus%2520solely%2520on%2520narrow%2520subtasks%2520while%2520overlooking%2520layout-centric%2520challenges%252C%2520which%2520are%2520central%2520to%2520real-world%2520slide%2520creation%2520and%2520editing.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520PPTBench%252C%2520a%2520comprehensive%2520multimodal%2520benchmark%2520for%2520evaluating%2520LLMs%2520on%2520PowerPoint-related%2520tasks.%2520Leveraging%2520a%2520diverse%2520source%2520of%2520958%2520PPTX%2520files%252C%2520PPTBench%2520evaluates%2520models%2520across%2520four%2520categories%2520with%25204%252C439%2520samples%252C%2520including%2520Detection%252C%2520Understanding%252C%2520Modification%252C%2520and%2520Generation.%2520Our%2520experiments%2520reveal%2520a%2520substantial%2520gap%2520between%2520semantic%2520understanding%2520and%2520visual-layout%2520reasoning%2520in%2520current%2520MLLMs%253A%2520models%2520can%2520interpret%2520slide%2520content%2520but%2520fail%2520to%2520produce%2520coherent%2520spatial%2520arrangements.%2520Ablation%2520and%2520further%2520analysis%2520show%2520that%2520current%2520MLLMs%2520struggle%2520to%2520combine%2520visual%2520cues%2520with%2520JSON-based%2520layout%2520structures%2520and%2520fail%2520to%2520integrate%2520visual%2520information%2520into%2520their%2520API%2520planning%2520ability.%2520And%2520case%2520studies%2520visually%2520expose%2520systematic%2520layout%2520errors%2520such%2520as%2520misalignment%2520and%2520element%2520overlap.%2520These%2520findings%2520provides%2520a%2520new%2520perspective%2520on%2520evaluating%2520VLLMs%2520in%2520PPT%2520scenarios%252C%2520highlighting%2520challenges%2520and%2520directions%2520for%2520future%2520research%2520on%2520visual-structural%2520reasoning%2520and%2520coherent%2520slide%2520generation.%2520All%2520datasets%2520and%2520code%2520are%2520fully%2520released%2520to%2520support%2520reproducibility%2520and%2520future%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02624v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PPTBench%3A%20Towards%20Holistic%20Evaluation%20of%20Large%20Language%20Models%20for%20PowerPoint%20Layout%20and%20Design%20Understanding&entry.906535625=Zheng%20Huang%20and%20Xukai%20Liu%20and%20Tianyu%20Hu%20and%20Kai%20Zhang%20and%20Ye%20Liu&entry.1292438233=PowerPoint%20presentations%20combine%20rich%20textual%20content%20with%20structured%20visual%20layouts%2C%20making%20them%20a%20natural%20testbed%20for%20evaluating%20the%20multimodal%20reasoning%20and%20layout%20understanding%20abilities%20of%20modern%20MLLMs.%20However%2C%20existing%20benchmarks%20focus%20solely%20on%20narrow%20subtasks%20while%20overlooking%20layout-centric%20challenges%2C%20which%20are%20central%20to%20real-world%20slide%20creation%20and%20editing.%20To%20bridge%20this%20gap%2C%20we%20introduce%20PPTBench%2C%20a%20comprehensive%20multimodal%20benchmark%20for%20evaluating%20LLMs%20on%20PowerPoint-related%20tasks.%20Leveraging%20a%20diverse%20source%20of%20958%20PPTX%20files%2C%20PPTBench%20evaluates%20models%20across%20four%20categories%20with%204%2C439%20samples%2C%20including%20Detection%2C%20Understanding%2C%20Modification%2C%20and%20Generation.%20Our%20experiments%20reveal%20a%20substantial%20gap%20between%20semantic%20understanding%20and%20visual-layout%20reasoning%20in%20current%20MLLMs%3A%20models%20can%20interpret%20slide%20content%20but%20fail%20to%20produce%20coherent%20spatial%20arrangements.%20Ablation%20and%20further%20analysis%20show%20that%20current%20MLLMs%20struggle%20to%20combine%20visual%20cues%20with%20JSON-based%20layout%20structures%20and%20fail%20to%20integrate%20visual%20information%20into%20their%20API%20planning%20ability.%20And%20case%20studies%20visually%20expose%20systematic%20layout%20errors%20such%20as%20misalignment%20and%20element%20overlap.%20These%20findings%20provides%20a%20new%20perspective%20on%20evaluating%20VLLMs%20in%20PPT%20scenarios%2C%20highlighting%20challenges%20and%20directions%20for%20future%20research%20on%20visual-structural%20reasoning%20and%20coherent%20slide%20generation.%20All%20datasets%20and%20code%20are%20fully%20released%20to%20support%20reproducibility%20and%20future%20research.&entry.1838667208=http%3A//arxiv.org/abs/2512.02624v1&entry.124074799=Read"},
{"title": "TCC-Bench: Benchmarking the Traditional Chinese Culture Understanding Capabilities of MLLMs", "author": "Pengju Xu and Yan Wang and Shuyuan Zhang and Xuan Zhou and Xin Li and Yue Yuan and Fengzhao Li and Shunyuan Zhou and Xingyu Wang and Yi Zhang and Haiying Zhao", "abstract": "Recent progress in Multimodal Large Language Models (MLLMs) have significantly enhanced the ability of artificial intelligence systems to understand and generate multimodal content. However, these models often exhibit limited effectiveness when applied to non-Western cultural contexts, which raises concerns about their wider applicability. To address this limitation, we propose the Traditional Chinese Culture understanding Benchmark (TCC-Bench), a bilingual (i.e., Chinese and English) Visual Question Answering (VQA) benchmark specifically designed for assessing the understanding of traditional Chinese culture by MLLMs. TCC-Bench comprises culturally rich and visually diverse data, incorporating images from museum artifacts, everyday life scenes, comics, and other culturally significant contexts. We adopt a semi-automated pipeline that utilizes GPT-4o in text-only mode to generate candidate questions, followed by human curation to ensure data quality and avoid potential data leakage. The benchmark also avoids language bias by preventing direct disclosure of cultural concepts within question texts. Experimental evaluations across a wide range of MLLMs demonstrate that current models still face significant challenges when reasoning about culturally grounded visual content. The results highlight the need for further research in developing culturally inclusive and context-aware multimodal systems. The code and data can be found at: https://tcc-bench.github.io/.", "link": "http://arxiv.org/abs/2505.11275v4", "date": "2025-12-02", "relevancy": 2.6802, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5372}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5372}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TCC-Bench%3A%20Benchmarking%20the%20Traditional%20Chinese%20Culture%20Understanding%20Capabilities%20of%20MLLMs&body=Title%3A%20TCC-Bench%3A%20Benchmarking%20the%20Traditional%20Chinese%20Culture%20Understanding%20Capabilities%20of%20MLLMs%0AAuthor%3A%20Pengju%20Xu%20and%20Yan%20Wang%20and%20Shuyuan%20Zhang%20and%20Xuan%20Zhou%20and%20Xin%20Li%20and%20Yue%20Yuan%20and%20Fengzhao%20Li%20and%20Shunyuan%20Zhou%20and%20Xingyu%20Wang%20and%20Yi%20Zhang%20and%20Haiying%20Zhao%0AAbstract%3A%20Recent%20progress%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20significantly%20enhanced%20the%20ability%20of%20artificial%20intelligence%20systems%20to%20understand%20and%20generate%20multimodal%20content.%20However%2C%20these%20models%20often%20exhibit%20limited%20effectiveness%20when%20applied%20to%20non-Western%20cultural%20contexts%2C%20which%20raises%20concerns%20about%20their%20wider%20applicability.%20To%20address%20this%20limitation%2C%20we%20propose%20the%20Traditional%20Chinese%20Culture%20understanding%20Benchmark%20%28TCC-Bench%29%2C%20a%20bilingual%20%28i.e.%2C%20Chinese%20and%20English%29%20Visual%20Question%20Answering%20%28VQA%29%20benchmark%20specifically%20designed%20for%20assessing%20the%20understanding%20of%20traditional%20Chinese%20culture%20by%20MLLMs.%20TCC-Bench%20comprises%20culturally%20rich%20and%20visually%20diverse%20data%2C%20incorporating%20images%20from%20museum%20artifacts%2C%20everyday%20life%20scenes%2C%20comics%2C%20and%20other%20culturally%20significant%20contexts.%20We%20adopt%20a%20semi-automated%20pipeline%20that%20utilizes%20GPT-4o%20in%20text-only%20mode%20to%20generate%20candidate%20questions%2C%20followed%20by%20human%20curation%20to%20ensure%20data%20quality%20and%20avoid%20potential%20data%20leakage.%20The%20benchmark%20also%20avoids%20language%20bias%20by%20preventing%20direct%20disclosure%20of%20cultural%20concepts%20within%20question%20texts.%20Experimental%20evaluations%20across%20a%20wide%20range%20of%20MLLMs%20demonstrate%20that%20current%20models%20still%20face%20significant%20challenges%20when%20reasoning%20about%20culturally%20grounded%20visual%20content.%20The%20results%20highlight%20the%20need%20for%20further%20research%20in%20developing%20culturally%20inclusive%20and%20context-aware%20multimodal%20systems.%20The%20code%20and%20data%20can%20be%20found%20at%3A%20https%3A//tcc-bench.github.io/.%0ALink%3A%20http%3A//arxiv.org/abs/2505.11275v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTCC-Bench%253A%2520Benchmarking%2520the%2520Traditional%2520Chinese%2520Culture%2520Understanding%2520Capabilities%2520of%2520MLLMs%26entry.906535625%3DPengju%2520Xu%2520and%2520Yan%2520Wang%2520and%2520Shuyuan%2520Zhang%2520and%2520Xuan%2520Zhou%2520and%2520Xin%2520Li%2520and%2520Yue%2520Yuan%2520and%2520Fengzhao%2520Li%2520and%2520Shunyuan%2520Zhou%2520and%2520Xingyu%2520Wang%2520and%2520Yi%2520Zhang%2520and%2520Haiying%2520Zhao%26entry.1292438233%3DRecent%2520progress%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520significantly%2520enhanced%2520the%2520ability%2520of%2520artificial%2520intelligence%2520systems%2520to%2520understand%2520and%2520generate%2520multimodal%2520content.%2520However%252C%2520these%2520models%2520often%2520exhibit%2520limited%2520effectiveness%2520when%2520applied%2520to%2520non-Western%2520cultural%2520contexts%252C%2520which%2520raises%2520concerns%2520about%2520their%2520wider%2520applicability.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520the%2520Traditional%2520Chinese%2520Culture%2520understanding%2520Benchmark%2520%2528TCC-Bench%2529%252C%2520a%2520bilingual%2520%2528i.e.%252C%2520Chinese%2520and%2520English%2529%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520benchmark%2520specifically%2520designed%2520for%2520assessing%2520the%2520understanding%2520of%2520traditional%2520Chinese%2520culture%2520by%2520MLLMs.%2520TCC-Bench%2520comprises%2520culturally%2520rich%2520and%2520visually%2520diverse%2520data%252C%2520incorporating%2520images%2520from%2520museum%2520artifacts%252C%2520everyday%2520life%2520scenes%252C%2520comics%252C%2520and%2520other%2520culturally%2520significant%2520contexts.%2520We%2520adopt%2520a%2520semi-automated%2520pipeline%2520that%2520utilizes%2520GPT-4o%2520in%2520text-only%2520mode%2520to%2520generate%2520candidate%2520questions%252C%2520followed%2520by%2520human%2520curation%2520to%2520ensure%2520data%2520quality%2520and%2520avoid%2520potential%2520data%2520leakage.%2520The%2520benchmark%2520also%2520avoids%2520language%2520bias%2520by%2520preventing%2520direct%2520disclosure%2520of%2520cultural%2520concepts%2520within%2520question%2520texts.%2520Experimental%2520evaluations%2520across%2520a%2520wide%2520range%2520of%2520MLLMs%2520demonstrate%2520that%2520current%2520models%2520still%2520face%2520significant%2520challenges%2520when%2520reasoning%2520about%2520culturally%2520grounded%2520visual%2520content.%2520The%2520results%2520highlight%2520the%2520need%2520for%2520further%2520research%2520in%2520developing%2520culturally%2520inclusive%2520and%2520context-aware%2520multimodal%2520systems.%2520The%2520code%2520and%2520data%2520can%2520be%2520found%2520at%253A%2520https%253A//tcc-bench.github.io/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11275v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TCC-Bench%3A%20Benchmarking%20the%20Traditional%20Chinese%20Culture%20Understanding%20Capabilities%20of%20MLLMs&entry.906535625=Pengju%20Xu%20and%20Yan%20Wang%20and%20Shuyuan%20Zhang%20and%20Xuan%20Zhou%20and%20Xin%20Li%20and%20Yue%20Yuan%20and%20Fengzhao%20Li%20and%20Shunyuan%20Zhou%20and%20Xingyu%20Wang%20and%20Yi%20Zhang%20and%20Haiying%20Zhao&entry.1292438233=Recent%20progress%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20significantly%20enhanced%20the%20ability%20of%20artificial%20intelligence%20systems%20to%20understand%20and%20generate%20multimodal%20content.%20However%2C%20these%20models%20often%20exhibit%20limited%20effectiveness%20when%20applied%20to%20non-Western%20cultural%20contexts%2C%20which%20raises%20concerns%20about%20their%20wider%20applicability.%20To%20address%20this%20limitation%2C%20we%20propose%20the%20Traditional%20Chinese%20Culture%20understanding%20Benchmark%20%28TCC-Bench%29%2C%20a%20bilingual%20%28i.e.%2C%20Chinese%20and%20English%29%20Visual%20Question%20Answering%20%28VQA%29%20benchmark%20specifically%20designed%20for%20assessing%20the%20understanding%20of%20traditional%20Chinese%20culture%20by%20MLLMs.%20TCC-Bench%20comprises%20culturally%20rich%20and%20visually%20diverse%20data%2C%20incorporating%20images%20from%20museum%20artifacts%2C%20everyday%20life%20scenes%2C%20comics%2C%20and%20other%20culturally%20significant%20contexts.%20We%20adopt%20a%20semi-automated%20pipeline%20that%20utilizes%20GPT-4o%20in%20text-only%20mode%20to%20generate%20candidate%20questions%2C%20followed%20by%20human%20curation%20to%20ensure%20data%20quality%20and%20avoid%20potential%20data%20leakage.%20The%20benchmark%20also%20avoids%20language%20bias%20by%20preventing%20direct%20disclosure%20of%20cultural%20concepts%20within%20question%20texts.%20Experimental%20evaluations%20across%20a%20wide%20range%20of%20MLLMs%20demonstrate%20that%20current%20models%20still%20face%20significant%20challenges%20when%20reasoning%20about%20culturally%20grounded%20visual%20content.%20The%20results%20highlight%20the%20need%20for%20further%20research%20in%20developing%20culturally%20inclusive%20and%20context-aware%20multimodal%20systems.%20The%20code%20and%20data%20can%20be%20found%20at%3A%20https%3A//tcc-bench.github.io/.&entry.1838667208=http%3A//arxiv.org/abs/2505.11275v4&entry.124074799=Read"},
{"title": "Spatially-Grounded Document Retrieval via Patch-to-Region Relevance Propagation", "author": "Agathoklis Georgiou", "abstract": "Vision-language models (VLMs) like ColPali achieve state-of-the-art document retrieval by embedding pages as images and computing fine-grained similarity between query tokens and visual patches. However, they return entire pages rather than specific regions, limiting utility for retrieval-augmented generation (RAG) where precise context is paramount. Conversely, OCR-based systems extract structured text with bounding box coordinates but lack semantic grounding for relevance assessment. We propose a hybrid architecture that unifies these paradigms: using ColPali's patch-level similarity scores as spatial relevance filters over OCR-extracted regions. We formalize the coordinate mapping between vision transformer patch grids and OCR bounding boxes, introduce intersection metrics for relevance propagation, and establish theoretical bounds on retrieval precision. Our approach operates at inference time without additional training. We release Snappy, an open-source implementation demonstrating practical applicability, with empirical evaluation ongoing.", "link": "http://arxiv.org/abs/2512.02660v1", "date": "2025-12-02", "relevancy": 2.6771, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5377}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5343}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5343}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatially-Grounded%20Document%20Retrieval%20via%20Patch-to-Region%20Relevance%20Propagation&body=Title%3A%20Spatially-Grounded%20Document%20Retrieval%20via%20Patch-to-Region%20Relevance%20Propagation%0AAuthor%3A%20Agathoklis%20Georgiou%0AAbstract%3A%20Vision-language%20models%20%28VLMs%29%20like%20ColPali%20achieve%20state-of-the-art%20document%20retrieval%20by%20embedding%20pages%20as%20images%20and%20computing%20fine-grained%20similarity%20between%20query%20tokens%20and%20visual%20patches.%20However%2C%20they%20return%20entire%20pages%20rather%20than%20specific%20regions%2C%20limiting%20utility%20for%20retrieval-augmented%20generation%20%28RAG%29%20where%20precise%20context%20is%20paramount.%20Conversely%2C%20OCR-based%20systems%20extract%20structured%20text%20with%20bounding%20box%20coordinates%20but%20lack%20semantic%20grounding%20for%20relevance%20assessment.%20We%20propose%20a%20hybrid%20architecture%20that%20unifies%20these%20paradigms%3A%20using%20ColPali%27s%20patch-level%20similarity%20scores%20as%20spatial%20relevance%20filters%20over%20OCR-extracted%20regions.%20We%20formalize%20the%20coordinate%20mapping%20between%20vision%20transformer%20patch%20grids%20and%20OCR%20bounding%20boxes%2C%20introduce%20intersection%20metrics%20for%20relevance%20propagation%2C%20and%20establish%20theoretical%20bounds%20on%20retrieval%20precision.%20Our%20approach%20operates%20at%20inference%20time%20without%20additional%20training.%20We%20release%20Snappy%2C%20an%20open-source%20implementation%20demonstrating%20practical%20applicability%2C%20with%20empirical%20evaluation%20ongoing.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02660v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatially-Grounded%2520Document%2520Retrieval%2520via%2520Patch-to-Region%2520Relevance%2520Propagation%26entry.906535625%3DAgathoklis%2520Georgiou%26entry.1292438233%3DVision-language%2520models%2520%2528VLMs%2529%2520like%2520ColPali%2520achieve%2520state-of-the-art%2520document%2520retrieval%2520by%2520embedding%2520pages%2520as%2520images%2520and%2520computing%2520fine-grained%2520similarity%2520between%2520query%2520tokens%2520and%2520visual%2520patches.%2520However%252C%2520they%2520return%2520entire%2520pages%2520rather%2520than%2520specific%2520regions%252C%2520limiting%2520utility%2520for%2520retrieval-augmented%2520generation%2520%2528RAG%2529%2520where%2520precise%2520context%2520is%2520paramount.%2520Conversely%252C%2520OCR-based%2520systems%2520extract%2520structured%2520text%2520with%2520bounding%2520box%2520coordinates%2520but%2520lack%2520semantic%2520grounding%2520for%2520relevance%2520assessment.%2520We%2520propose%2520a%2520hybrid%2520architecture%2520that%2520unifies%2520these%2520paradigms%253A%2520using%2520ColPali%2527s%2520patch-level%2520similarity%2520scores%2520as%2520spatial%2520relevance%2520filters%2520over%2520OCR-extracted%2520regions.%2520We%2520formalize%2520the%2520coordinate%2520mapping%2520between%2520vision%2520transformer%2520patch%2520grids%2520and%2520OCR%2520bounding%2520boxes%252C%2520introduce%2520intersection%2520metrics%2520for%2520relevance%2520propagation%252C%2520and%2520establish%2520theoretical%2520bounds%2520on%2520retrieval%2520precision.%2520Our%2520approach%2520operates%2520at%2520inference%2520time%2520without%2520additional%2520training.%2520We%2520release%2520Snappy%252C%2520an%2520open-source%2520implementation%2520demonstrating%2520practical%2520applicability%252C%2520with%2520empirical%2520evaluation%2520ongoing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02660v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatially-Grounded%20Document%20Retrieval%20via%20Patch-to-Region%20Relevance%20Propagation&entry.906535625=Agathoklis%20Georgiou&entry.1292438233=Vision-language%20models%20%28VLMs%29%20like%20ColPali%20achieve%20state-of-the-art%20document%20retrieval%20by%20embedding%20pages%20as%20images%20and%20computing%20fine-grained%20similarity%20between%20query%20tokens%20and%20visual%20patches.%20However%2C%20they%20return%20entire%20pages%20rather%20than%20specific%20regions%2C%20limiting%20utility%20for%20retrieval-augmented%20generation%20%28RAG%29%20where%20precise%20context%20is%20paramount.%20Conversely%2C%20OCR-based%20systems%20extract%20structured%20text%20with%20bounding%20box%20coordinates%20but%20lack%20semantic%20grounding%20for%20relevance%20assessment.%20We%20propose%20a%20hybrid%20architecture%20that%20unifies%20these%20paradigms%3A%20using%20ColPali%27s%20patch-level%20similarity%20scores%20as%20spatial%20relevance%20filters%20over%20OCR-extracted%20regions.%20We%20formalize%20the%20coordinate%20mapping%20between%20vision%20transformer%20patch%20grids%20and%20OCR%20bounding%20boxes%2C%20introduce%20intersection%20metrics%20for%20relevance%20propagation%2C%20and%20establish%20theoretical%20bounds%20on%20retrieval%20precision.%20Our%20approach%20operates%20at%20inference%20time%20without%20additional%20training.%20We%20release%20Snappy%2C%20an%20open-source%20implementation%20demonstrating%20practical%20applicability%2C%20with%20empirical%20evaluation%20ongoing.&entry.1838667208=http%3A//arxiv.org/abs/2512.02660v1&entry.124074799=Read"},
{"title": "DiverseAR: Boosting Diversity in Bitwise Autoregressive Image Generation", "author": "Ying Yang and Zhengyao Lv and Tianlin Pan and Haofan Wang and Binxin Yang and Hubery Yin and Chen Li and Chenyang Si", "abstract": "In this paper, we investigate the underexplored challenge of sample diversity in autoregressive (AR) generative models with bitwise visual tokenizers. We first analyze the factors that limit diversity in bitwise AR models and identify two key issues: (1) the binary classification nature of bitwise modeling, which restricts the prediction space, and (2) the overly sharp logits distribution, which causes sampling collapse and reduces diversity. Building on these insights, we propose DiverseAR, a principled and effective method that enhances image diversity without sacrificing visual quality. Specifically, we introduce an adaptive logits distribution scaling mechanism that dynamically adjusts the sharpness of the binary output distribution during sampling, resulting in smoother predictions and greater diversity. To mitigate potential fidelity loss caused by distribution smoothing, we further develop an energy-based generation path search algorithm that avoids sampling low-confidence tokens, thereby preserving high visual quality. Extensive experiments demonstrate that DiverseAR substantially improves sample diversity in bitwise autoregressive image generation.", "link": "http://arxiv.org/abs/2512.02931v1", "date": "2025-12-02", "relevancy": 2.6729, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5658}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5236}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DiverseAR%3A%20Boosting%20Diversity%20in%20Bitwise%20Autoregressive%20Image%20Generation&body=Title%3A%20DiverseAR%3A%20Boosting%20Diversity%20in%20Bitwise%20Autoregressive%20Image%20Generation%0AAuthor%3A%20Ying%20Yang%20and%20Zhengyao%20Lv%20and%20Tianlin%20Pan%20and%20Haofan%20Wang%20and%20Binxin%20Yang%20and%20Hubery%20Yin%20and%20Chen%20Li%20and%20Chenyang%20Si%0AAbstract%3A%20In%20this%20paper%2C%20we%20investigate%20the%20underexplored%20challenge%20of%20sample%20diversity%20in%20autoregressive%20%28AR%29%20generative%20models%20with%20bitwise%20visual%20tokenizers.%20We%20first%20analyze%20the%20factors%20that%20limit%20diversity%20in%20bitwise%20AR%20models%20and%20identify%20two%20key%20issues%3A%20%281%29%20the%20binary%20classification%20nature%20of%20bitwise%20modeling%2C%20which%20restricts%20the%20prediction%20space%2C%20and%20%282%29%20the%20overly%20sharp%20logits%20distribution%2C%20which%20causes%20sampling%20collapse%20and%20reduces%20diversity.%20Building%20on%20these%20insights%2C%20we%20propose%20DiverseAR%2C%20a%20principled%20and%20effective%20method%20that%20enhances%20image%20diversity%20without%20sacrificing%20visual%20quality.%20Specifically%2C%20we%20introduce%20an%20adaptive%20logits%20distribution%20scaling%20mechanism%20that%20dynamically%20adjusts%20the%20sharpness%20of%20the%20binary%20output%20distribution%20during%20sampling%2C%20resulting%20in%20smoother%20predictions%20and%20greater%20diversity.%20To%20mitigate%20potential%20fidelity%20loss%20caused%20by%20distribution%20smoothing%2C%20we%20further%20develop%20an%20energy-based%20generation%20path%20search%20algorithm%20that%20avoids%20sampling%20low-confidence%20tokens%2C%20thereby%20preserving%20high%20visual%20quality.%20Extensive%20experiments%20demonstrate%20that%20DiverseAR%20substantially%20improves%20sample%20diversity%20in%20bitwise%20autoregressive%20image%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02931v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiverseAR%253A%2520Boosting%2520Diversity%2520in%2520Bitwise%2520Autoregressive%2520Image%2520Generation%26entry.906535625%3DYing%2520Yang%2520and%2520Zhengyao%2520Lv%2520and%2520Tianlin%2520Pan%2520and%2520Haofan%2520Wang%2520and%2520Binxin%2520Yang%2520and%2520Hubery%2520Yin%2520and%2520Chen%2520Li%2520and%2520Chenyang%2520Si%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520investigate%2520the%2520underexplored%2520challenge%2520of%2520sample%2520diversity%2520in%2520autoregressive%2520%2528AR%2529%2520generative%2520models%2520with%2520bitwise%2520visual%2520tokenizers.%2520We%2520first%2520analyze%2520the%2520factors%2520that%2520limit%2520diversity%2520in%2520bitwise%2520AR%2520models%2520and%2520identify%2520two%2520key%2520issues%253A%2520%25281%2529%2520the%2520binary%2520classification%2520nature%2520of%2520bitwise%2520modeling%252C%2520which%2520restricts%2520the%2520prediction%2520space%252C%2520and%2520%25282%2529%2520the%2520overly%2520sharp%2520logits%2520distribution%252C%2520which%2520causes%2520sampling%2520collapse%2520and%2520reduces%2520diversity.%2520Building%2520on%2520these%2520insights%252C%2520we%2520propose%2520DiverseAR%252C%2520a%2520principled%2520and%2520effective%2520method%2520that%2520enhances%2520image%2520diversity%2520without%2520sacrificing%2520visual%2520quality.%2520Specifically%252C%2520we%2520introduce%2520an%2520adaptive%2520logits%2520distribution%2520scaling%2520mechanism%2520that%2520dynamically%2520adjusts%2520the%2520sharpness%2520of%2520the%2520binary%2520output%2520distribution%2520during%2520sampling%252C%2520resulting%2520in%2520smoother%2520predictions%2520and%2520greater%2520diversity.%2520To%2520mitigate%2520potential%2520fidelity%2520loss%2520caused%2520by%2520distribution%2520smoothing%252C%2520we%2520further%2520develop%2520an%2520energy-based%2520generation%2520path%2520search%2520algorithm%2520that%2520avoids%2520sampling%2520low-confidence%2520tokens%252C%2520thereby%2520preserving%2520high%2520visual%2520quality.%2520Extensive%2520experiments%2520demonstrate%2520that%2520DiverseAR%2520substantially%2520improves%2520sample%2520diversity%2520in%2520bitwise%2520autoregressive%2520image%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02931v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiverseAR%3A%20Boosting%20Diversity%20in%20Bitwise%20Autoregressive%20Image%20Generation&entry.906535625=Ying%20Yang%20and%20Zhengyao%20Lv%20and%20Tianlin%20Pan%20and%20Haofan%20Wang%20and%20Binxin%20Yang%20and%20Hubery%20Yin%20and%20Chen%20Li%20and%20Chenyang%20Si&entry.1292438233=In%20this%20paper%2C%20we%20investigate%20the%20underexplored%20challenge%20of%20sample%20diversity%20in%20autoregressive%20%28AR%29%20generative%20models%20with%20bitwise%20visual%20tokenizers.%20We%20first%20analyze%20the%20factors%20that%20limit%20diversity%20in%20bitwise%20AR%20models%20and%20identify%20two%20key%20issues%3A%20%281%29%20the%20binary%20classification%20nature%20of%20bitwise%20modeling%2C%20which%20restricts%20the%20prediction%20space%2C%20and%20%282%29%20the%20overly%20sharp%20logits%20distribution%2C%20which%20causes%20sampling%20collapse%20and%20reduces%20diversity.%20Building%20on%20these%20insights%2C%20we%20propose%20DiverseAR%2C%20a%20principled%20and%20effective%20method%20that%20enhances%20image%20diversity%20without%20sacrificing%20visual%20quality.%20Specifically%2C%20we%20introduce%20an%20adaptive%20logits%20distribution%20scaling%20mechanism%20that%20dynamically%20adjusts%20the%20sharpness%20of%20the%20binary%20output%20distribution%20during%20sampling%2C%20resulting%20in%20smoother%20predictions%20and%20greater%20diversity.%20To%20mitigate%20potential%20fidelity%20loss%20caused%20by%20distribution%20smoothing%2C%20we%20further%20develop%20an%20energy-based%20generation%20path%20search%20algorithm%20that%20avoids%20sampling%20low-confidence%20tokens%2C%20thereby%20preserving%20high%20visual%20quality.%20Extensive%20experiments%20demonstrate%20that%20DiverseAR%20substantially%20improves%20sample%20diversity%20in%20bitwise%20autoregressive%20image%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2512.02931v1&entry.124074799=Read"},
{"title": "Multilingual Pretraining for Pixel Language Models", "author": "Ilker Kesen and Jonas F. Lotz and Ingo Ziegler and Phillip Rust and Desmond Elliott", "abstract": "Pixel language models operate directly on images of rendered text, eliminating the need for a fixed vocabulary. While these models have demonstrated strong capabilities for downstream cross-lingual transfer, multilingual pretraining remains underexplored. We introduce PIXEL-M4, a model pretrained on four visually and linguistically diverse languages: English, Hindi, Ukrainian, and Simplified Chinese. Multilingual evaluations on semantic and syntactic tasks show that PIXEL-M4 outperforms an English-only counterpart on non-Latin scripts. Word-level probing analyses confirm that PIXEL-M4 captures rich linguistic features, even in languages not seen during pretraining. Furthermore, an analysis of its hidden representations shows that multilingual pretraining yields a semantic embedding space closely aligned across the languages used for pretraining. This work demonstrates that multilingual pretraining substantially enhances the capability of pixel language models to effectively support a diverse set of languages.", "link": "http://arxiv.org/abs/2505.21265v2", "date": "2025-12-02", "relevancy": 2.625, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5346}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5202}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multilingual%20Pretraining%20for%20Pixel%20Language%20Models&body=Title%3A%20Multilingual%20Pretraining%20for%20Pixel%20Language%20Models%0AAuthor%3A%20Ilker%20Kesen%20and%20Jonas%20F.%20Lotz%20and%20Ingo%20Ziegler%20and%20Phillip%20Rust%20and%20Desmond%20Elliott%0AAbstract%3A%20Pixel%20language%20models%20operate%20directly%20on%20images%20of%20rendered%20text%2C%20eliminating%20the%20need%20for%20a%20fixed%20vocabulary.%20While%20these%20models%20have%20demonstrated%20strong%20capabilities%20for%20downstream%20cross-lingual%20transfer%2C%20multilingual%20pretraining%20remains%20underexplored.%20We%20introduce%20PIXEL-M4%2C%20a%20model%20pretrained%20on%20four%20visually%20and%20linguistically%20diverse%20languages%3A%20English%2C%20Hindi%2C%20Ukrainian%2C%20and%20Simplified%20Chinese.%20Multilingual%20evaluations%20on%20semantic%20and%20syntactic%20tasks%20show%20that%20PIXEL-M4%20outperforms%20an%20English-only%20counterpart%20on%20non-Latin%20scripts.%20Word-level%20probing%20analyses%20confirm%20that%20PIXEL-M4%20captures%20rich%20linguistic%20features%2C%20even%20in%20languages%20not%20seen%20during%20pretraining.%20Furthermore%2C%20an%20analysis%20of%20its%20hidden%20representations%20shows%20that%20multilingual%20pretraining%20yields%20a%20semantic%20embedding%20space%20closely%20aligned%20across%20the%20languages%20used%20for%20pretraining.%20This%20work%20demonstrates%20that%20multilingual%20pretraining%20substantially%20enhances%20the%20capability%20of%20pixel%20language%20models%20to%20effectively%20support%20a%20diverse%20set%20of%20languages.%0ALink%3A%20http%3A//arxiv.org/abs/2505.21265v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultilingual%2520Pretraining%2520for%2520Pixel%2520Language%2520Models%26entry.906535625%3DIlker%2520Kesen%2520and%2520Jonas%2520F.%2520Lotz%2520and%2520Ingo%2520Ziegler%2520and%2520Phillip%2520Rust%2520and%2520Desmond%2520Elliott%26entry.1292438233%3DPixel%2520language%2520models%2520operate%2520directly%2520on%2520images%2520of%2520rendered%2520text%252C%2520eliminating%2520the%2520need%2520for%2520a%2520fixed%2520vocabulary.%2520While%2520these%2520models%2520have%2520demonstrated%2520strong%2520capabilities%2520for%2520downstream%2520cross-lingual%2520transfer%252C%2520multilingual%2520pretraining%2520remains%2520underexplored.%2520We%2520introduce%2520PIXEL-M4%252C%2520a%2520model%2520pretrained%2520on%2520four%2520visually%2520and%2520linguistically%2520diverse%2520languages%253A%2520English%252C%2520Hindi%252C%2520Ukrainian%252C%2520and%2520Simplified%2520Chinese.%2520Multilingual%2520evaluations%2520on%2520semantic%2520and%2520syntactic%2520tasks%2520show%2520that%2520PIXEL-M4%2520outperforms%2520an%2520English-only%2520counterpart%2520on%2520non-Latin%2520scripts.%2520Word-level%2520probing%2520analyses%2520confirm%2520that%2520PIXEL-M4%2520captures%2520rich%2520linguistic%2520features%252C%2520even%2520in%2520languages%2520not%2520seen%2520during%2520pretraining.%2520Furthermore%252C%2520an%2520analysis%2520of%2520its%2520hidden%2520representations%2520shows%2520that%2520multilingual%2520pretraining%2520yields%2520a%2520semantic%2520embedding%2520space%2520closely%2520aligned%2520across%2520the%2520languages%2520used%2520for%2520pretraining.%2520This%2520work%2520demonstrates%2520that%2520multilingual%2520pretraining%2520substantially%2520enhances%2520the%2520capability%2520of%2520pixel%2520language%2520models%2520to%2520effectively%2520support%2520a%2520diverse%2520set%2520of%2520languages.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21265v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multilingual%20Pretraining%20for%20Pixel%20Language%20Models&entry.906535625=Ilker%20Kesen%20and%20Jonas%20F.%20Lotz%20and%20Ingo%20Ziegler%20and%20Phillip%20Rust%20and%20Desmond%20Elliott&entry.1292438233=Pixel%20language%20models%20operate%20directly%20on%20images%20of%20rendered%20text%2C%20eliminating%20the%20need%20for%20a%20fixed%20vocabulary.%20While%20these%20models%20have%20demonstrated%20strong%20capabilities%20for%20downstream%20cross-lingual%20transfer%2C%20multilingual%20pretraining%20remains%20underexplored.%20We%20introduce%20PIXEL-M4%2C%20a%20model%20pretrained%20on%20four%20visually%20and%20linguistically%20diverse%20languages%3A%20English%2C%20Hindi%2C%20Ukrainian%2C%20and%20Simplified%20Chinese.%20Multilingual%20evaluations%20on%20semantic%20and%20syntactic%20tasks%20show%20that%20PIXEL-M4%20outperforms%20an%20English-only%20counterpart%20on%20non-Latin%20scripts.%20Word-level%20probing%20analyses%20confirm%20that%20PIXEL-M4%20captures%20rich%20linguistic%20features%2C%20even%20in%20languages%20not%20seen%20during%20pretraining.%20Furthermore%2C%20an%20analysis%20of%20its%20hidden%20representations%20shows%20that%20multilingual%20pretraining%20yields%20a%20semantic%20embedding%20space%20closely%20aligned%20across%20the%20languages%20used%20for%20pretraining.%20This%20work%20demonstrates%20that%20multilingual%20pretraining%20substantially%20enhances%20the%20capability%20of%20pixel%20language%20models%20to%20effectively%20support%20a%20diverse%20set%20of%20languages.&entry.1838667208=http%3A//arxiv.org/abs/2505.21265v2&entry.124074799=Read"},
{"title": "DGGT: Feedforward 4D Reconstruction of Dynamic Driving Scenes using Unposed Images", "author": "Xiaoxue Chen and Ziyi Xiong and Yuantao Chen and Gen Li and Nan Wang and Hongcheng Luo and Long Chen and Haiyang Sun and Bing Wang and Guang Chen and Hangjun Ye and Hongyang Li and Ya-Qin Zhang and Hao Zhao", "abstract": "Autonomous driving needs fast, scalable 4D reconstruction and re-simulation for training and evaluation, yet most methods for dynamic driving scenes still rely on per-scene optimization, known camera calibration, or short frame windows, making them slow and impractical. We revisit this problem from a feedforward perspective and introduce \\textbf{Driving Gaussian Grounded Transformer (DGGT)}, a unified framework for pose-free dynamic scene reconstruction. We note that the existing formulations, treating camera pose as a required input, limit flexibility and scalability. Instead, we reformulate pose as an output of the model, enabling reconstruction directly from sparse, unposed images and supporting an arbitrary number of views for long sequences. Our approach jointly predicts per-frame 3D Gaussian maps and camera parameters, disentangles dynamics with a lightweight dynamic head, and preserves temporal consistency with a lifespan head that modulates visibility over time. A diffusion-based rendering refinement further reduces motion/interpolation artifacts and improves novel-view quality under sparse inputs. The result is a single-pass, pose-free algorithm that achieves state-of-the-art performance and speed. Trained and evaluated on large-scale driving benchmarks (Waymo, nuScenes, Argoverse2), our method outperforms prior work both when trained on each dataset and in zero-shot transfer across datasets, and it scales well as the number of input frames increases.", "link": "http://arxiv.org/abs/2512.03004v1", "date": "2025-12-02", "relevancy": 2.6244, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6712}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6571}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DGGT%3A%20Feedforward%204D%20Reconstruction%20of%20Dynamic%20Driving%20Scenes%20using%20Unposed%20Images&body=Title%3A%20DGGT%3A%20Feedforward%204D%20Reconstruction%20of%20Dynamic%20Driving%20Scenes%20using%20Unposed%20Images%0AAuthor%3A%20Xiaoxue%20Chen%20and%20Ziyi%20Xiong%20and%20Yuantao%20Chen%20and%20Gen%20Li%20and%20Nan%20Wang%20and%20Hongcheng%20Luo%20and%20Long%20Chen%20and%20Haiyang%20Sun%20and%20Bing%20Wang%20and%20Guang%20Chen%20and%20Hangjun%20Ye%20and%20Hongyang%20Li%20and%20Ya-Qin%20Zhang%20and%20Hao%20Zhao%0AAbstract%3A%20Autonomous%20driving%20needs%20fast%2C%20scalable%204D%20reconstruction%20and%20re-simulation%20for%20training%20and%20evaluation%2C%20yet%20most%20methods%20for%20dynamic%20driving%20scenes%20still%20rely%20on%20per-scene%20optimization%2C%20known%20camera%20calibration%2C%20or%20short%20frame%20windows%2C%20making%20them%20slow%20and%20impractical.%20We%20revisit%20this%20problem%20from%20a%20feedforward%20perspective%20and%20introduce%20%5Ctextbf%7BDriving%20Gaussian%20Grounded%20Transformer%20%28DGGT%29%7D%2C%20a%20unified%20framework%20for%20pose-free%20dynamic%20scene%20reconstruction.%20We%20note%20that%20the%20existing%20formulations%2C%20treating%20camera%20pose%20as%20a%20required%20input%2C%20limit%20flexibility%20and%20scalability.%20Instead%2C%20we%20reformulate%20pose%20as%20an%20output%20of%20the%20model%2C%20enabling%20reconstruction%20directly%20from%20sparse%2C%20unposed%20images%20and%20supporting%20an%20arbitrary%20number%20of%20views%20for%20long%20sequences.%20Our%20approach%20jointly%20predicts%20per-frame%203D%20Gaussian%20maps%20and%20camera%20parameters%2C%20disentangles%20dynamics%20with%20a%20lightweight%20dynamic%20head%2C%20and%20preserves%20temporal%20consistency%20with%20a%20lifespan%20head%20that%20modulates%20visibility%20over%20time.%20A%20diffusion-based%20rendering%20refinement%20further%20reduces%20motion/interpolation%20artifacts%20and%20improves%20novel-view%20quality%20under%20sparse%20inputs.%20The%20result%20is%20a%20single-pass%2C%20pose-free%20algorithm%20that%20achieves%20state-of-the-art%20performance%20and%20speed.%20Trained%20and%20evaluated%20on%20large-scale%20driving%20benchmarks%20%28Waymo%2C%20nuScenes%2C%20Argoverse2%29%2C%20our%20method%20outperforms%20prior%20work%20both%20when%20trained%20on%20each%20dataset%20and%20in%20zero-shot%20transfer%20across%20datasets%2C%20and%20it%20scales%20well%20as%20the%20number%20of%20input%20frames%20increases.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03004v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDGGT%253A%2520Feedforward%25204D%2520Reconstruction%2520of%2520Dynamic%2520Driving%2520Scenes%2520using%2520Unposed%2520Images%26entry.906535625%3DXiaoxue%2520Chen%2520and%2520Ziyi%2520Xiong%2520and%2520Yuantao%2520Chen%2520and%2520Gen%2520Li%2520and%2520Nan%2520Wang%2520and%2520Hongcheng%2520Luo%2520and%2520Long%2520Chen%2520and%2520Haiyang%2520Sun%2520and%2520Bing%2520Wang%2520and%2520Guang%2520Chen%2520and%2520Hangjun%2520Ye%2520and%2520Hongyang%2520Li%2520and%2520Ya-Qin%2520Zhang%2520and%2520Hao%2520Zhao%26entry.1292438233%3DAutonomous%2520driving%2520needs%2520fast%252C%2520scalable%25204D%2520reconstruction%2520and%2520re-simulation%2520for%2520training%2520and%2520evaluation%252C%2520yet%2520most%2520methods%2520for%2520dynamic%2520driving%2520scenes%2520still%2520rely%2520on%2520per-scene%2520optimization%252C%2520known%2520camera%2520calibration%252C%2520or%2520short%2520frame%2520windows%252C%2520making%2520them%2520slow%2520and%2520impractical.%2520We%2520revisit%2520this%2520problem%2520from%2520a%2520feedforward%2520perspective%2520and%2520introduce%2520%255Ctextbf%257BDriving%2520Gaussian%2520Grounded%2520Transformer%2520%2528DGGT%2529%257D%252C%2520a%2520unified%2520framework%2520for%2520pose-free%2520dynamic%2520scene%2520reconstruction.%2520We%2520note%2520that%2520the%2520existing%2520formulations%252C%2520treating%2520camera%2520pose%2520as%2520a%2520required%2520input%252C%2520limit%2520flexibility%2520and%2520scalability.%2520Instead%252C%2520we%2520reformulate%2520pose%2520as%2520an%2520output%2520of%2520the%2520model%252C%2520enabling%2520reconstruction%2520directly%2520from%2520sparse%252C%2520unposed%2520images%2520and%2520supporting%2520an%2520arbitrary%2520number%2520of%2520views%2520for%2520long%2520sequences.%2520Our%2520approach%2520jointly%2520predicts%2520per-frame%25203D%2520Gaussian%2520maps%2520and%2520camera%2520parameters%252C%2520disentangles%2520dynamics%2520with%2520a%2520lightweight%2520dynamic%2520head%252C%2520and%2520preserves%2520temporal%2520consistency%2520with%2520a%2520lifespan%2520head%2520that%2520modulates%2520visibility%2520over%2520time.%2520A%2520diffusion-based%2520rendering%2520refinement%2520further%2520reduces%2520motion/interpolation%2520artifacts%2520and%2520improves%2520novel-view%2520quality%2520under%2520sparse%2520inputs.%2520The%2520result%2520is%2520a%2520single-pass%252C%2520pose-free%2520algorithm%2520that%2520achieves%2520state-of-the-art%2520performance%2520and%2520speed.%2520Trained%2520and%2520evaluated%2520on%2520large-scale%2520driving%2520benchmarks%2520%2528Waymo%252C%2520nuScenes%252C%2520Argoverse2%2529%252C%2520our%2520method%2520outperforms%2520prior%2520work%2520both%2520when%2520trained%2520on%2520each%2520dataset%2520and%2520in%2520zero-shot%2520transfer%2520across%2520datasets%252C%2520and%2520it%2520scales%2520well%2520as%2520the%2520number%2520of%2520input%2520frames%2520increases.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03004v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DGGT%3A%20Feedforward%204D%20Reconstruction%20of%20Dynamic%20Driving%20Scenes%20using%20Unposed%20Images&entry.906535625=Xiaoxue%20Chen%20and%20Ziyi%20Xiong%20and%20Yuantao%20Chen%20and%20Gen%20Li%20and%20Nan%20Wang%20and%20Hongcheng%20Luo%20and%20Long%20Chen%20and%20Haiyang%20Sun%20and%20Bing%20Wang%20and%20Guang%20Chen%20and%20Hangjun%20Ye%20and%20Hongyang%20Li%20and%20Ya-Qin%20Zhang%20and%20Hao%20Zhao&entry.1292438233=Autonomous%20driving%20needs%20fast%2C%20scalable%204D%20reconstruction%20and%20re-simulation%20for%20training%20and%20evaluation%2C%20yet%20most%20methods%20for%20dynamic%20driving%20scenes%20still%20rely%20on%20per-scene%20optimization%2C%20known%20camera%20calibration%2C%20or%20short%20frame%20windows%2C%20making%20them%20slow%20and%20impractical.%20We%20revisit%20this%20problem%20from%20a%20feedforward%20perspective%20and%20introduce%20%5Ctextbf%7BDriving%20Gaussian%20Grounded%20Transformer%20%28DGGT%29%7D%2C%20a%20unified%20framework%20for%20pose-free%20dynamic%20scene%20reconstruction.%20We%20note%20that%20the%20existing%20formulations%2C%20treating%20camera%20pose%20as%20a%20required%20input%2C%20limit%20flexibility%20and%20scalability.%20Instead%2C%20we%20reformulate%20pose%20as%20an%20output%20of%20the%20model%2C%20enabling%20reconstruction%20directly%20from%20sparse%2C%20unposed%20images%20and%20supporting%20an%20arbitrary%20number%20of%20views%20for%20long%20sequences.%20Our%20approach%20jointly%20predicts%20per-frame%203D%20Gaussian%20maps%20and%20camera%20parameters%2C%20disentangles%20dynamics%20with%20a%20lightweight%20dynamic%20head%2C%20and%20preserves%20temporal%20consistency%20with%20a%20lifespan%20head%20that%20modulates%20visibility%20over%20time.%20A%20diffusion-based%20rendering%20refinement%20further%20reduces%20motion/interpolation%20artifacts%20and%20improves%20novel-view%20quality%20under%20sparse%20inputs.%20The%20result%20is%20a%20single-pass%2C%20pose-free%20algorithm%20that%20achieves%20state-of-the-art%20performance%20and%20speed.%20Trained%20and%20evaluated%20on%20large-scale%20driving%20benchmarks%20%28Waymo%2C%20nuScenes%2C%20Argoverse2%29%2C%20our%20method%20outperforms%20prior%20work%20both%20when%20trained%20on%20each%20dataset%20and%20in%20zero-shot%20transfer%20across%20datasets%2C%20and%20it%20scales%20well%20as%20the%20number%20of%20input%20frames%20increases.&entry.1838667208=http%3A//arxiv.org/abs/2512.03004v1&entry.124074799=Read"},
{"title": "Reasoning-Aware Multimodal Fusion for Hateful Video Detection", "author": "Shuonan Yang and Tailin Chen and Jiangbei Yue and Guangliang Cheng and Jianbo Jiao and Zeyu Fu", "abstract": "Hate speech in online videos is posing an increasingly serious threat to digital platforms, especially as video content becomes increasingly multimodal and context-dependent. Existing methods often struggle to effectively fuse the complex semantic relationships between modalities and lack the ability to understand nuanced hateful content. To address these issues, we propose an innovative Reasoning-Aware Multimodal Fusion (RAMF) framework. To tackle the first challenge, we design Local-Global Context Fusion (LGCF) to capture both local salient cues and global temporal structures, and propose Semantic Cross Attention (SCA) to enable fine-grained multimodal semantic interaction. To tackle the second challenge, we introduce adversarial reasoning-a structured three-stage process where a vision-language model generates (i) objective descriptions, (ii) hate-assumed inferences, and (iii) non-hate-assumed inferences-providing complementary semantic perspectives that enrich the model's contextual understanding of nuanced hateful intent. Evaluations on two real-world hateful video datasets demonstrate that our method achieves robust generalisation performance, improving upon state-of-the-art methods by 3% and 7% in Macro-F1 and hate class recall, respectively. We will release the code after the anonymity period ends.", "link": "http://arxiv.org/abs/2512.02743v1", "date": "2025-12-02", "relevancy": 2.6207, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5352}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5211}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning-Aware%20Multimodal%20Fusion%20for%20Hateful%20Video%20Detection&body=Title%3A%20Reasoning-Aware%20Multimodal%20Fusion%20for%20Hateful%20Video%20Detection%0AAuthor%3A%20Shuonan%20Yang%20and%20Tailin%20Chen%20and%20Jiangbei%20Yue%20and%20Guangliang%20Cheng%20and%20Jianbo%20Jiao%20and%20Zeyu%20Fu%0AAbstract%3A%20Hate%20speech%20in%20online%20videos%20is%20posing%20an%20increasingly%20serious%20threat%20to%20digital%20platforms%2C%20especially%20as%20video%20content%20becomes%20increasingly%20multimodal%20and%20context-dependent.%20Existing%20methods%20often%20struggle%20to%20effectively%20fuse%20the%20complex%20semantic%20relationships%20between%20modalities%20and%20lack%20the%20ability%20to%20understand%20nuanced%20hateful%20content.%20To%20address%20these%20issues%2C%20we%20propose%20an%20innovative%20Reasoning-Aware%20Multimodal%20Fusion%20%28RAMF%29%20framework.%20To%20tackle%20the%20first%20challenge%2C%20we%20design%20Local-Global%20Context%20Fusion%20%28LGCF%29%20to%20capture%20both%20local%20salient%20cues%20and%20global%20temporal%20structures%2C%20and%20propose%20Semantic%20Cross%20Attention%20%28SCA%29%20to%20enable%20fine-grained%20multimodal%20semantic%20interaction.%20To%20tackle%20the%20second%20challenge%2C%20we%20introduce%20adversarial%20reasoning-a%20structured%20three-stage%20process%20where%20a%20vision-language%20model%20generates%20%28i%29%20objective%20descriptions%2C%20%28ii%29%20hate-assumed%20inferences%2C%20and%20%28iii%29%20non-hate-assumed%20inferences-providing%20complementary%20semantic%20perspectives%20that%20enrich%20the%20model%27s%20contextual%20understanding%20of%20nuanced%20hateful%20intent.%20Evaluations%20on%20two%20real-world%20hateful%20video%20datasets%20demonstrate%20that%20our%20method%20achieves%20robust%20generalisation%20performance%2C%20improving%20upon%20state-of-the-art%20methods%20by%203%25%20and%207%25%20in%20Macro-F1%20and%20hate%20class%20recall%2C%20respectively.%20We%20will%20release%20the%20code%20after%20the%20anonymity%20period%20ends.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02743v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning-Aware%2520Multimodal%2520Fusion%2520for%2520Hateful%2520Video%2520Detection%26entry.906535625%3DShuonan%2520Yang%2520and%2520Tailin%2520Chen%2520and%2520Jiangbei%2520Yue%2520and%2520Guangliang%2520Cheng%2520and%2520Jianbo%2520Jiao%2520and%2520Zeyu%2520Fu%26entry.1292438233%3DHate%2520speech%2520in%2520online%2520videos%2520is%2520posing%2520an%2520increasingly%2520serious%2520threat%2520to%2520digital%2520platforms%252C%2520especially%2520as%2520video%2520content%2520becomes%2520increasingly%2520multimodal%2520and%2520context-dependent.%2520Existing%2520methods%2520often%2520struggle%2520to%2520effectively%2520fuse%2520the%2520complex%2520semantic%2520relationships%2520between%2520modalities%2520and%2520lack%2520the%2520ability%2520to%2520understand%2520nuanced%2520hateful%2520content.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520an%2520innovative%2520Reasoning-Aware%2520Multimodal%2520Fusion%2520%2528RAMF%2529%2520framework.%2520To%2520tackle%2520the%2520first%2520challenge%252C%2520we%2520design%2520Local-Global%2520Context%2520Fusion%2520%2528LGCF%2529%2520to%2520capture%2520both%2520local%2520salient%2520cues%2520and%2520global%2520temporal%2520structures%252C%2520and%2520propose%2520Semantic%2520Cross%2520Attention%2520%2528SCA%2529%2520to%2520enable%2520fine-grained%2520multimodal%2520semantic%2520interaction.%2520To%2520tackle%2520the%2520second%2520challenge%252C%2520we%2520introduce%2520adversarial%2520reasoning-a%2520structured%2520three-stage%2520process%2520where%2520a%2520vision-language%2520model%2520generates%2520%2528i%2529%2520objective%2520descriptions%252C%2520%2528ii%2529%2520hate-assumed%2520inferences%252C%2520and%2520%2528iii%2529%2520non-hate-assumed%2520inferences-providing%2520complementary%2520semantic%2520perspectives%2520that%2520enrich%2520the%2520model%2527s%2520contextual%2520understanding%2520of%2520nuanced%2520hateful%2520intent.%2520Evaluations%2520on%2520two%2520real-world%2520hateful%2520video%2520datasets%2520demonstrate%2520that%2520our%2520method%2520achieves%2520robust%2520generalisation%2520performance%252C%2520improving%2520upon%2520state-of-the-art%2520methods%2520by%25203%2525%2520and%25207%2525%2520in%2520Macro-F1%2520and%2520hate%2520class%2520recall%252C%2520respectively.%2520We%2520will%2520release%2520the%2520code%2520after%2520the%2520anonymity%2520period%2520ends.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02743v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning-Aware%20Multimodal%20Fusion%20for%20Hateful%20Video%20Detection&entry.906535625=Shuonan%20Yang%20and%20Tailin%20Chen%20and%20Jiangbei%20Yue%20and%20Guangliang%20Cheng%20and%20Jianbo%20Jiao%20and%20Zeyu%20Fu&entry.1292438233=Hate%20speech%20in%20online%20videos%20is%20posing%20an%20increasingly%20serious%20threat%20to%20digital%20platforms%2C%20especially%20as%20video%20content%20becomes%20increasingly%20multimodal%20and%20context-dependent.%20Existing%20methods%20often%20struggle%20to%20effectively%20fuse%20the%20complex%20semantic%20relationships%20between%20modalities%20and%20lack%20the%20ability%20to%20understand%20nuanced%20hateful%20content.%20To%20address%20these%20issues%2C%20we%20propose%20an%20innovative%20Reasoning-Aware%20Multimodal%20Fusion%20%28RAMF%29%20framework.%20To%20tackle%20the%20first%20challenge%2C%20we%20design%20Local-Global%20Context%20Fusion%20%28LGCF%29%20to%20capture%20both%20local%20salient%20cues%20and%20global%20temporal%20structures%2C%20and%20propose%20Semantic%20Cross%20Attention%20%28SCA%29%20to%20enable%20fine-grained%20multimodal%20semantic%20interaction.%20To%20tackle%20the%20second%20challenge%2C%20we%20introduce%20adversarial%20reasoning-a%20structured%20three-stage%20process%20where%20a%20vision-language%20model%20generates%20%28i%29%20objective%20descriptions%2C%20%28ii%29%20hate-assumed%20inferences%2C%20and%20%28iii%29%20non-hate-assumed%20inferences-providing%20complementary%20semantic%20perspectives%20that%20enrich%20the%20model%27s%20contextual%20understanding%20of%20nuanced%20hateful%20intent.%20Evaluations%20on%20two%20real-world%20hateful%20video%20datasets%20demonstrate%20that%20our%20method%20achieves%20robust%20generalisation%20performance%2C%20improving%20upon%20state-of-the-art%20methods%20by%203%25%20and%207%25%20in%20Macro-F1%20and%20hate%20class%20recall%2C%20respectively.%20We%20will%20release%20the%20code%20after%20the%20anonymity%20period%20ends.&entry.1838667208=http%3A//arxiv.org/abs/2512.02743v1&entry.124074799=Read"},
{"title": "Pruning AMR: Efficient Visualization of Implicit Neural Representations via Weight Matrix Analysis", "author": "Jennifer Zvonek and Andrew Gillette", "abstract": "An implicit neural representation (INR) is a neural network that approximates a spatiotemporal function. Many memory-intensive visualization tasks, including modern 4D CT scanning methods, represent data natively as INRs. While INRs are prized for being more memory-efficient than traditional data stored on a lattice, many visualization tasks still require discretization to a regular grid. We present PruningAMR, an algorithm that builds a mesh with resolution adapted to geometric features encoded by the INR. To identify these geometric features, we use an interpolative decomposition pruning method on the weight matrices of the INR. The resulting pruned network is used to guide adaptive mesh refinement, enabling automatic mesh generation tailored to the underlying resolution of the function. Starting from a pre-trained INR--without access to its training data--we produce a variable resolution visualization with substantial memory savings.", "link": "http://arxiv.org/abs/2512.02967v1", "date": "2025-12-02", "relevancy": 2.6029, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5478}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5102}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5037}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pruning%20AMR%3A%20Efficient%20Visualization%20of%20Implicit%20Neural%20Representations%20via%20Weight%20Matrix%20Analysis&body=Title%3A%20Pruning%20AMR%3A%20Efficient%20Visualization%20of%20Implicit%20Neural%20Representations%20via%20Weight%20Matrix%20Analysis%0AAuthor%3A%20Jennifer%20Zvonek%20and%20Andrew%20Gillette%0AAbstract%3A%20An%20implicit%20neural%20representation%20%28INR%29%20is%20a%20neural%20network%20that%20approximates%20a%20spatiotemporal%20function.%20Many%20memory-intensive%20visualization%20tasks%2C%20including%20modern%204D%20CT%20scanning%20methods%2C%20represent%20data%20natively%20as%20INRs.%20While%20INRs%20are%20prized%20for%20being%20more%20memory-efficient%20than%20traditional%20data%20stored%20on%20a%20lattice%2C%20many%20visualization%20tasks%20still%20require%20discretization%20to%20a%20regular%20grid.%20We%20present%20PruningAMR%2C%20an%20algorithm%20that%20builds%20a%20mesh%20with%20resolution%20adapted%20to%20geometric%20features%20encoded%20by%20the%20INR.%20To%20identify%20these%20geometric%20features%2C%20we%20use%20an%20interpolative%20decomposition%20pruning%20method%20on%20the%20weight%20matrices%20of%20the%20INR.%20The%20resulting%20pruned%20network%20is%20used%20to%20guide%20adaptive%20mesh%20refinement%2C%20enabling%20automatic%20mesh%20generation%20tailored%20to%20the%20underlying%20resolution%20of%20the%20function.%20Starting%20from%20a%20pre-trained%20INR--without%20access%20to%20its%20training%20data--we%20produce%20a%20variable%20resolution%20visualization%20with%20substantial%20memory%20savings.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02967v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPruning%2520AMR%253A%2520Efficient%2520Visualization%2520of%2520Implicit%2520Neural%2520Representations%2520via%2520Weight%2520Matrix%2520Analysis%26entry.906535625%3DJennifer%2520Zvonek%2520and%2520Andrew%2520Gillette%26entry.1292438233%3DAn%2520implicit%2520neural%2520representation%2520%2528INR%2529%2520is%2520a%2520neural%2520network%2520that%2520approximates%2520a%2520spatiotemporal%2520function.%2520Many%2520memory-intensive%2520visualization%2520tasks%252C%2520including%2520modern%25204D%2520CT%2520scanning%2520methods%252C%2520represent%2520data%2520natively%2520as%2520INRs.%2520While%2520INRs%2520are%2520prized%2520for%2520being%2520more%2520memory-efficient%2520than%2520traditional%2520data%2520stored%2520on%2520a%2520lattice%252C%2520many%2520visualization%2520tasks%2520still%2520require%2520discretization%2520to%2520a%2520regular%2520grid.%2520We%2520present%2520PruningAMR%252C%2520an%2520algorithm%2520that%2520builds%2520a%2520mesh%2520with%2520resolution%2520adapted%2520to%2520geometric%2520features%2520encoded%2520by%2520the%2520INR.%2520To%2520identify%2520these%2520geometric%2520features%252C%2520we%2520use%2520an%2520interpolative%2520decomposition%2520pruning%2520method%2520on%2520the%2520weight%2520matrices%2520of%2520the%2520INR.%2520The%2520resulting%2520pruned%2520network%2520is%2520used%2520to%2520guide%2520adaptive%2520mesh%2520refinement%252C%2520enabling%2520automatic%2520mesh%2520generation%2520tailored%2520to%2520the%2520underlying%2520resolution%2520of%2520the%2520function.%2520Starting%2520from%2520a%2520pre-trained%2520INR--without%2520access%2520to%2520its%2520training%2520data--we%2520produce%2520a%2520variable%2520resolution%2520visualization%2520with%2520substantial%2520memory%2520savings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02967v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pruning%20AMR%3A%20Efficient%20Visualization%20of%20Implicit%20Neural%20Representations%20via%20Weight%20Matrix%20Analysis&entry.906535625=Jennifer%20Zvonek%20and%20Andrew%20Gillette&entry.1292438233=An%20implicit%20neural%20representation%20%28INR%29%20is%20a%20neural%20network%20that%20approximates%20a%20spatiotemporal%20function.%20Many%20memory-intensive%20visualization%20tasks%2C%20including%20modern%204D%20CT%20scanning%20methods%2C%20represent%20data%20natively%20as%20INRs.%20While%20INRs%20are%20prized%20for%20being%20more%20memory-efficient%20than%20traditional%20data%20stored%20on%20a%20lattice%2C%20many%20visualization%20tasks%20still%20require%20discretization%20to%20a%20regular%20grid.%20We%20present%20PruningAMR%2C%20an%20algorithm%20that%20builds%20a%20mesh%20with%20resolution%20adapted%20to%20geometric%20features%20encoded%20by%20the%20INR.%20To%20identify%20these%20geometric%20features%2C%20we%20use%20an%20interpolative%20decomposition%20pruning%20method%20on%20the%20weight%20matrices%20of%20the%20INR.%20The%20resulting%20pruned%20network%20is%20used%20to%20guide%20adaptive%20mesh%20refinement%2C%20enabling%20automatic%20mesh%20generation%20tailored%20to%20the%20underlying%20resolution%20of%20the%20function.%20Starting%20from%20a%20pre-trained%20INR--without%20access%20to%20its%20training%20data--we%20produce%20a%20variable%20resolution%20visualization%20with%20substantial%20memory%20savings.&entry.1838667208=http%3A//arxiv.org/abs/2512.02967v1&entry.124074799=Read"},
{"title": "Towards Responsible Development of Generative AI for Education: An Evaluation-Driven Approach", "author": "Irina Jurenka and Markus Kunesch and Kevin R. McKee and Daniel Gillick and Shaojian Zhu and Sara Wiltberger and Shubham Milind Phal and Katherine Hermann and Daniel Kasenberg and Avishkar Bhoopchand and Ankit Anand and Miruna P\u00eeslar and Stephanie Chan and Lisa Wang and Jennifer She and Parsa Mahmoudieh and Aliya Rysbek and Wei-Jen Ko and Andrea Huber and Brett Wiltshire and Gal Elidan and Roni Rabin and Jasmin Rubinovitz and Amit Pitaru and Mac McAllister and Julia Wilkowski and David Choi and Roee Engelberg and Lidan Hackmon and Adva Levin and Rachel Griffin and Michael Sears and Filip Bar and Mia Mesar and Mana Jabbour and Arslan Chaudhry and James Cohan and Sridhar Thiagarajan and Nir Levine and Ben Brown and Dilan Gorur and Svetlana Grant and Rachel Hashimshoni and Laura Weidinger and Jieru Hu and Dawn Chen and Kuba Dolecki and Canfer Akbulut and Maxwell Bileschi and Laura Culp and Wen-Xin Dong and Nahema Marchal and Kelsie Van Deman and Hema Bajaj Misra and Michael Duah and Moran Ambar and Avi Caciularu and Sandra Lefdal and Chris Summerfield and James An and Pierre-Alexandre Kamienny and Abhinit Mohdi and Theofilos Strinopoulous and Annie Hale and Wayne Anderson and Luis C. Cobo and Niv Efron and Muktha Ananda and Shakir Mohamed and Maureen Heymans and Zoubin Ghahramani and Yossi Matias and Ben Gomes and Lila Ibrahim", "abstract": "A major challenge facing the world is the provision of equitable and universal access to quality education. Recent advances in generative AI (gen AI) have created excitement about the potential of new technologies to offer a personal tutor for every learner and a teaching assistant for every teacher. The full extent of this dream, however, has not yet materialised. We argue that this is primarily due to the difficulties with verbalising pedagogical intuitions into gen AI prompts and the lack of good evaluation practices, reinforced by the challenges in defining excellent pedagogy. Here we present our work collaborating with learners and educators to translate high level principles from learning science into a pragmatic set of seven diverse educational benchmarks, spanning quantitative, qualitative, automatic and human evaluations; and to develop a new set of fine-tuning datasets to improve the pedagogical capabilities of Gemini, introducing LearnLM-Tutor. Our evaluations show that LearnLM-Tutor is consistently preferred over a prompt tuned Gemini by educators and learners on a number of pedagogical dimensions. We hope that this work can serve as a first step towards developing a comprehensive educational evaluation framework, and that this can enable rapid progress within the AI and EdTech communities towards maximising the positive impact of gen AI in education.", "link": "http://arxiv.org/abs/2407.12687v4", "date": "2025-12-02", "relevancy": 2.5989, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5668}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5092}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Responsible%20Development%20of%20Generative%20AI%20for%20Education%3A%20An%20Evaluation-Driven%20Approach&body=Title%3A%20Towards%20Responsible%20Development%20of%20Generative%20AI%20for%20Education%3A%20An%20Evaluation-Driven%20Approach%0AAuthor%3A%20Irina%20Jurenka%20and%20Markus%20Kunesch%20and%20Kevin%20R.%20McKee%20and%20Daniel%20Gillick%20and%20Shaojian%20Zhu%20and%20Sara%20Wiltberger%20and%20Shubham%20Milind%20Phal%20and%20Katherine%20Hermann%20and%20Daniel%20Kasenberg%20and%20Avishkar%20Bhoopchand%20and%20Ankit%20Anand%20and%20Miruna%20P%C3%AEslar%20and%20Stephanie%20Chan%20and%20Lisa%20Wang%20and%20Jennifer%20She%20and%20Parsa%20Mahmoudieh%20and%20Aliya%20Rysbek%20and%20Wei-Jen%20Ko%20and%20Andrea%20Huber%20and%20Brett%20Wiltshire%20and%20Gal%20Elidan%20and%20Roni%20Rabin%20and%20Jasmin%20Rubinovitz%20and%20Amit%20Pitaru%20and%20Mac%20McAllister%20and%20Julia%20Wilkowski%20and%20David%20Choi%20and%20Roee%20Engelberg%20and%20Lidan%20Hackmon%20and%20Adva%20Levin%20and%20Rachel%20Griffin%20and%20Michael%20Sears%20and%20Filip%20Bar%20and%20Mia%20Mesar%20and%20Mana%20Jabbour%20and%20Arslan%20Chaudhry%20and%20James%20Cohan%20and%20Sridhar%20Thiagarajan%20and%20Nir%20Levine%20and%20Ben%20Brown%20and%20Dilan%20Gorur%20and%20Svetlana%20Grant%20and%20Rachel%20Hashimshoni%20and%20Laura%20Weidinger%20and%20Jieru%20Hu%20and%20Dawn%20Chen%20and%20Kuba%20Dolecki%20and%20Canfer%20Akbulut%20and%20Maxwell%20Bileschi%20and%20Laura%20Culp%20and%20Wen-Xin%20Dong%20and%20Nahema%20Marchal%20and%20Kelsie%20Van%20Deman%20and%20Hema%20Bajaj%20Misra%20and%20Michael%20Duah%20and%20Moran%20Ambar%20and%20Avi%20Caciularu%20and%20Sandra%20Lefdal%20and%20Chris%20Summerfield%20and%20James%20An%20and%20Pierre-Alexandre%20Kamienny%20and%20Abhinit%20Mohdi%20and%20Theofilos%20Strinopoulous%20and%20Annie%20Hale%20and%20Wayne%20Anderson%20and%20Luis%20C.%20Cobo%20and%20Niv%20Efron%20and%20Muktha%20Ananda%20and%20Shakir%20Mohamed%20and%20Maureen%20Heymans%20and%20Zoubin%20Ghahramani%20and%20Yossi%20Matias%20and%20Ben%20Gomes%20and%20Lila%20Ibrahim%0AAbstract%3A%20A%20major%20challenge%20facing%20the%20world%20is%20the%20provision%20of%20equitable%20and%20universal%20access%20to%20quality%20education.%20Recent%20advances%20in%20generative%20AI%20%28gen%20AI%29%20have%20created%20excitement%20about%20the%20potential%20of%20new%20technologies%20to%20offer%20a%20personal%20tutor%20for%20every%20learner%20and%20a%20teaching%20assistant%20for%20every%20teacher.%20The%20full%20extent%20of%20this%20dream%2C%20however%2C%20has%20not%20yet%20materialised.%20We%20argue%20that%20this%20is%20primarily%20due%20to%20the%20difficulties%20with%20verbalising%20pedagogical%20intuitions%20into%20gen%20AI%20prompts%20and%20the%20lack%20of%20good%20evaluation%20practices%2C%20reinforced%20by%20the%20challenges%20in%20defining%20excellent%20pedagogy.%20Here%20we%20present%20our%20work%20collaborating%20with%20learners%20and%20educators%20to%20translate%20high%20level%20principles%20from%20learning%20science%20into%20a%20pragmatic%20set%20of%20seven%20diverse%20educational%20benchmarks%2C%20spanning%20quantitative%2C%20qualitative%2C%20automatic%20and%20human%20evaluations%3B%20and%20to%20develop%20a%20new%20set%20of%20fine-tuning%20datasets%20to%20improve%20the%20pedagogical%20capabilities%20of%20Gemini%2C%20introducing%20LearnLM-Tutor.%20Our%20evaluations%20show%20that%20LearnLM-Tutor%20is%20consistently%20preferred%20over%20a%20prompt%20tuned%20Gemini%20by%20educators%20and%20learners%20on%20a%20number%20of%20pedagogical%20dimensions.%20We%20hope%20that%20this%20work%20can%20serve%20as%20a%20first%20step%20towards%20developing%20a%20comprehensive%20educational%20evaluation%20framework%2C%20and%20that%20this%20can%20enable%20rapid%20progress%20within%20the%20AI%20and%20EdTech%20communities%20towards%20maximising%20the%20positive%20impact%20of%20gen%20AI%20in%20education.%0ALink%3A%20http%3A//arxiv.org/abs/2407.12687v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Responsible%2520Development%2520of%2520Generative%2520AI%2520for%2520Education%253A%2520An%2520Evaluation-Driven%2520Approach%26entry.906535625%3DIrina%2520Jurenka%2520and%2520Markus%2520Kunesch%2520and%2520Kevin%2520R.%2520McKee%2520and%2520Daniel%2520Gillick%2520and%2520Shaojian%2520Zhu%2520and%2520Sara%2520Wiltberger%2520and%2520Shubham%2520Milind%2520Phal%2520and%2520Katherine%2520Hermann%2520and%2520Daniel%2520Kasenberg%2520and%2520Avishkar%2520Bhoopchand%2520and%2520Ankit%2520Anand%2520and%2520Miruna%2520P%25C3%25AEslar%2520and%2520Stephanie%2520Chan%2520and%2520Lisa%2520Wang%2520and%2520Jennifer%2520She%2520and%2520Parsa%2520Mahmoudieh%2520and%2520Aliya%2520Rysbek%2520and%2520Wei-Jen%2520Ko%2520and%2520Andrea%2520Huber%2520and%2520Brett%2520Wiltshire%2520and%2520Gal%2520Elidan%2520and%2520Roni%2520Rabin%2520and%2520Jasmin%2520Rubinovitz%2520and%2520Amit%2520Pitaru%2520and%2520Mac%2520McAllister%2520and%2520Julia%2520Wilkowski%2520and%2520David%2520Choi%2520and%2520Roee%2520Engelberg%2520and%2520Lidan%2520Hackmon%2520and%2520Adva%2520Levin%2520and%2520Rachel%2520Griffin%2520and%2520Michael%2520Sears%2520and%2520Filip%2520Bar%2520and%2520Mia%2520Mesar%2520and%2520Mana%2520Jabbour%2520and%2520Arslan%2520Chaudhry%2520and%2520James%2520Cohan%2520and%2520Sridhar%2520Thiagarajan%2520and%2520Nir%2520Levine%2520and%2520Ben%2520Brown%2520and%2520Dilan%2520Gorur%2520and%2520Svetlana%2520Grant%2520and%2520Rachel%2520Hashimshoni%2520and%2520Laura%2520Weidinger%2520and%2520Jieru%2520Hu%2520and%2520Dawn%2520Chen%2520and%2520Kuba%2520Dolecki%2520and%2520Canfer%2520Akbulut%2520and%2520Maxwell%2520Bileschi%2520and%2520Laura%2520Culp%2520and%2520Wen-Xin%2520Dong%2520and%2520Nahema%2520Marchal%2520and%2520Kelsie%2520Van%2520Deman%2520and%2520Hema%2520Bajaj%2520Misra%2520and%2520Michael%2520Duah%2520and%2520Moran%2520Ambar%2520and%2520Avi%2520Caciularu%2520and%2520Sandra%2520Lefdal%2520and%2520Chris%2520Summerfield%2520and%2520James%2520An%2520and%2520Pierre-Alexandre%2520Kamienny%2520and%2520Abhinit%2520Mohdi%2520and%2520Theofilos%2520Strinopoulous%2520and%2520Annie%2520Hale%2520and%2520Wayne%2520Anderson%2520and%2520Luis%2520C.%2520Cobo%2520and%2520Niv%2520Efron%2520and%2520Muktha%2520Ananda%2520and%2520Shakir%2520Mohamed%2520and%2520Maureen%2520Heymans%2520and%2520Zoubin%2520Ghahramani%2520and%2520Yossi%2520Matias%2520and%2520Ben%2520Gomes%2520and%2520Lila%2520Ibrahim%26entry.1292438233%3DA%2520major%2520challenge%2520facing%2520the%2520world%2520is%2520the%2520provision%2520of%2520equitable%2520and%2520universal%2520access%2520to%2520quality%2520education.%2520Recent%2520advances%2520in%2520generative%2520AI%2520%2528gen%2520AI%2529%2520have%2520created%2520excitement%2520about%2520the%2520potential%2520of%2520new%2520technologies%2520to%2520offer%2520a%2520personal%2520tutor%2520for%2520every%2520learner%2520and%2520a%2520teaching%2520assistant%2520for%2520every%2520teacher.%2520The%2520full%2520extent%2520of%2520this%2520dream%252C%2520however%252C%2520has%2520not%2520yet%2520materialised.%2520We%2520argue%2520that%2520this%2520is%2520primarily%2520due%2520to%2520the%2520difficulties%2520with%2520verbalising%2520pedagogical%2520intuitions%2520into%2520gen%2520AI%2520prompts%2520and%2520the%2520lack%2520of%2520good%2520evaluation%2520practices%252C%2520reinforced%2520by%2520the%2520challenges%2520in%2520defining%2520excellent%2520pedagogy.%2520Here%2520we%2520present%2520our%2520work%2520collaborating%2520with%2520learners%2520and%2520educators%2520to%2520translate%2520high%2520level%2520principles%2520from%2520learning%2520science%2520into%2520a%2520pragmatic%2520set%2520of%2520seven%2520diverse%2520educational%2520benchmarks%252C%2520spanning%2520quantitative%252C%2520qualitative%252C%2520automatic%2520and%2520human%2520evaluations%253B%2520and%2520to%2520develop%2520a%2520new%2520set%2520of%2520fine-tuning%2520datasets%2520to%2520improve%2520the%2520pedagogical%2520capabilities%2520of%2520Gemini%252C%2520introducing%2520LearnLM-Tutor.%2520Our%2520evaluations%2520show%2520that%2520LearnLM-Tutor%2520is%2520consistently%2520preferred%2520over%2520a%2520prompt%2520tuned%2520Gemini%2520by%2520educators%2520and%2520learners%2520on%2520a%2520number%2520of%2520pedagogical%2520dimensions.%2520We%2520hope%2520that%2520this%2520work%2520can%2520serve%2520as%2520a%2520first%2520step%2520towards%2520developing%2520a%2520comprehensive%2520educational%2520evaluation%2520framework%252C%2520and%2520that%2520this%2520can%2520enable%2520rapid%2520progress%2520within%2520the%2520AI%2520and%2520EdTech%2520communities%2520towards%2520maximising%2520the%2520positive%2520impact%2520of%2520gen%2520AI%2520in%2520education.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12687v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Responsible%20Development%20of%20Generative%20AI%20for%20Education%3A%20An%20Evaluation-Driven%20Approach&entry.906535625=Irina%20Jurenka%20and%20Markus%20Kunesch%20and%20Kevin%20R.%20McKee%20and%20Daniel%20Gillick%20and%20Shaojian%20Zhu%20and%20Sara%20Wiltberger%20and%20Shubham%20Milind%20Phal%20and%20Katherine%20Hermann%20and%20Daniel%20Kasenberg%20and%20Avishkar%20Bhoopchand%20and%20Ankit%20Anand%20and%20Miruna%20P%C3%AEslar%20and%20Stephanie%20Chan%20and%20Lisa%20Wang%20and%20Jennifer%20She%20and%20Parsa%20Mahmoudieh%20and%20Aliya%20Rysbek%20and%20Wei-Jen%20Ko%20and%20Andrea%20Huber%20and%20Brett%20Wiltshire%20and%20Gal%20Elidan%20and%20Roni%20Rabin%20and%20Jasmin%20Rubinovitz%20and%20Amit%20Pitaru%20and%20Mac%20McAllister%20and%20Julia%20Wilkowski%20and%20David%20Choi%20and%20Roee%20Engelberg%20and%20Lidan%20Hackmon%20and%20Adva%20Levin%20and%20Rachel%20Griffin%20and%20Michael%20Sears%20and%20Filip%20Bar%20and%20Mia%20Mesar%20and%20Mana%20Jabbour%20and%20Arslan%20Chaudhry%20and%20James%20Cohan%20and%20Sridhar%20Thiagarajan%20and%20Nir%20Levine%20and%20Ben%20Brown%20and%20Dilan%20Gorur%20and%20Svetlana%20Grant%20and%20Rachel%20Hashimshoni%20and%20Laura%20Weidinger%20and%20Jieru%20Hu%20and%20Dawn%20Chen%20and%20Kuba%20Dolecki%20and%20Canfer%20Akbulut%20and%20Maxwell%20Bileschi%20and%20Laura%20Culp%20and%20Wen-Xin%20Dong%20and%20Nahema%20Marchal%20and%20Kelsie%20Van%20Deman%20and%20Hema%20Bajaj%20Misra%20and%20Michael%20Duah%20and%20Moran%20Ambar%20and%20Avi%20Caciularu%20and%20Sandra%20Lefdal%20and%20Chris%20Summerfield%20and%20James%20An%20and%20Pierre-Alexandre%20Kamienny%20and%20Abhinit%20Mohdi%20and%20Theofilos%20Strinopoulous%20and%20Annie%20Hale%20and%20Wayne%20Anderson%20and%20Luis%20C.%20Cobo%20and%20Niv%20Efron%20and%20Muktha%20Ananda%20and%20Shakir%20Mohamed%20and%20Maureen%20Heymans%20and%20Zoubin%20Ghahramani%20and%20Yossi%20Matias%20and%20Ben%20Gomes%20and%20Lila%20Ibrahim&entry.1292438233=A%20major%20challenge%20facing%20the%20world%20is%20the%20provision%20of%20equitable%20and%20universal%20access%20to%20quality%20education.%20Recent%20advances%20in%20generative%20AI%20%28gen%20AI%29%20have%20created%20excitement%20about%20the%20potential%20of%20new%20technologies%20to%20offer%20a%20personal%20tutor%20for%20every%20learner%20and%20a%20teaching%20assistant%20for%20every%20teacher.%20The%20full%20extent%20of%20this%20dream%2C%20however%2C%20has%20not%20yet%20materialised.%20We%20argue%20that%20this%20is%20primarily%20due%20to%20the%20difficulties%20with%20verbalising%20pedagogical%20intuitions%20into%20gen%20AI%20prompts%20and%20the%20lack%20of%20good%20evaluation%20practices%2C%20reinforced%20by%20the%20challenges%20in%20defining%20excellent%20pedagogy.%20Here%20we%20present%20our%20work%20collaborating%20with%20learners%20and%20educators%20to%20translate%20high%20level%20principles%20from%20learning%20science%20into%20a%20pragmatic%20set%20of%20seven%20diverse%20educational%20benchmarks%2C%20spanning%20quantitative%2C%20qualitative%2C%20automatic%20and%20human%20evaluations%3B%20and%20to%20develop%20a%20new%20set%20of%20fine-tuning%20datasets%20to%20improve%20the%20pedagogical%20capabilities%20of%20Gemini%2C%20introducing%20LearnLM-Tutor.%20Our%20evaluations%20show%20that%20LearnLM-Tutor%20is%20consistently%20preferred%20over%20a%20prompt%20tuned%20Gemini%20by%20educators%20and%20learners%20on%20a%20number%20of%20pedagogical%20dimensions.%20We%20hope%20that%20this%20work%20can%20serve%20as%20a%20first%20step%20towards%20developing%20a%20comprehensive%20educational%20evaluation%20framework%2C%20and%20that%20this%20can%20enable%20rapid%20progress%20within%20the%20AI%20and%20EdTech%20communities%20towards%20maximising%20the%20positive%20impact%20of%20gen%20AI%20in%20education.&entry.1838667208=http%3A//arxiv.org/abs/2407.12687v4&entry.124074799=Read"},
{"title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling", "author": "Kairun Wen and Yuzhi Huang and Runyu Chen and Hui Zheng and Yunlong Lin and Panwang Pan and Chenxin Li and Wenyan Cong and Jian Zhang and Junbin Lu and Chenguo Lin and Dilin Wang and Zhicheng Yan and Hongyu Xu and Justin Theiss and Yue Huang and Xinghao Ding and Rakesh Ranjan and Zhiwen Fan", "abstract": "Understanding the dynamic physical world, characterized by its evolving 3D structure, real-world motion, and semantic content with textual descriptions, is crucial for human-agent interaction and enables embodied agents to perceive and act within real environments with human-like capabilities. However, existing datasets are often derived from limited simulators or utilize traditional Structurefrom-Motion for up-to-scale annotation and offer limited descriptive captioning, which restricts the capacity of foundation models to accurately interpret real-world dynamics from monocular videos, commonly sourced from the internet. To bridge these gaps, we introduce DynamicVerse, a physical-scale, multimodal 4D world modeling framework for dynamic real-world video. We employ large vision, geometric, and multimodal models to interpret metric-scale static geometry, real-world dynamic motion, instance-level masks, and holistic descriptive captions. By integrating window-based Bundle Adjustment with global optimization, our method converts long real-world video sequences into a comprehensive 4D multimodal format. DynamicVerse delivers a large-scale dataset consists of 100K+ videos with 800K+ annotated masks and 10M+ frames from internet videos. Experimental evaluations on three benchmark tasks, namely video depth estimation, camera pose estimation, and camera intrinsics estimation, demonstrate that our 4D modeling achieves superior performance in capturing physical-scale measurements with greater global accuracy than existing methods.", "link": "http://arxiv.org/abs/2512.03000v1", "date": "2025-12-02", "relevancy": 2.597, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6508}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6508}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynamicVerse%3A%20A%20Physically-Aware%20Multimodal%20Framework%20for%204D%20World%20Modeling&body=Title%3A%20DynamicVerse%3A%20A%20Physically-Aware%20Multimodal%20Framework%20for%204D%20World%20Modeling%0AAuthor%3A%20Kairun%20Wen%20and%20Yuzhi%20Huang%20and%20Runyu%20Chen%20and%20Hui%20Zheng%20and%20Yunlong%20Lin%20and%20Panwang%20Pan%20and%20Chenxin%20Li%20and%20Wenyan%20Cong%20and%20Jian%20Zhang%20and%20Junbin%20Lu%20and%20Chenguo%20Lin%20and%20Dilin%20Wang%20and%20Zhicheng%20Yan%20and%20Hongyu%20Xu%20and%20Justin%20Theiss%20and%20Yue%20Huang%20and%20Xinghao%20Ding%20and%20Rakesh%20Ranjan%20and%20Zhiwen%20Fan%0AAbstract%3A%20Understanding%20the%20dynamic%20physical%20world%2C%20characterized%20by%20its%20evolving%203D%20structure%2C%20real-world%20motion%2C%20and%20semantic%20content%20with%20textual%20descriptions%2C%20is%20crucial%20for%20human-agent%20interaction%20and%20enables%20embodied%20agents%20to%20perceive%20and%20act%20within%20real%20environments%20with%20human-like%20capabilities.%20However%2C%20existing%20datasets%20are%20often%20derived%20from%20limited%20simulators%20or%20utilize%20traditional%20Structurefrom-Motion%20for%20up-to-scale%20annotation%20and%20offer%20limited%20descriptive%20captioning%2C%20which%20restricts%20the%20capacity%20of%20foundation%20models%20to%20accurately%20interpret%20real-world%20dynamics%20from%20monocular%20videos%2C%20commonly%20sourced%20from%20the%20internet.%20To%20bridge%20these%20gaps%2C%20we%20introduce%20DynamicVerse%2C%20a%20physical-scale%2C%20multimodal%204D%20world%20modeling%20framework%20for%20dynamic%20real-world%20video.%20We%20employ%20large%20vision%2C%20geometric%2C%20and%20multimodal%20models%20to%20interpret%20metric-scale%20static%20geometry%2C%20real-world%20dynamic%20motion%2C%20instance-level%20masks%2C%20and%20holistic%20descriptive%20captions.%20By%20integrating%20window-based%20Bundle%20Adjustment%20with%20global%20optimization%2C%20our%20method%20converts%20long%20real-world%20video%20sequences%20into%20a%20comprehensive%204D%20multimodal%20format.%20DynamicVerse%20delivers%20a%20large-scale%20dataset%20consists%20of%20100K%2B%20videos%20with%20800K%2B%20annotated%20masks%20and%2010M%2B%20frames%20from%20internet%20videos.%20Experimental%20evaluations%20on%20three%20benchmark%20tasks%2C%20namely%20video%20depth%20estimation%2C%20camera%20pose%20estimation%2C%20and%20camera%20intrinsics%20estimation%2C%20demonstrate%20that%20our%204D%20modeling%20achieves%20superior%20performance%20in%20capturing%20physical-scale%20measurements%20with%20greater%20global%20accuracy%20than%20existing%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03000v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamicVerse%253A%2520A%2520Physically-Aware%2520Multimodal%2520Framework%2520for%25204D%2520World%2520Modeling%26entry.906535625%3DKairun%2520Wen%2520and%2520Yuzhi%2520Huang%2520and%2520Runyu%2520Chen%2520and%2520Hui%2520Zheng%2520and%2520Yunlong%2520Lin%2520and%2520Panwang%2520Pan%2520and%2520Chenxin%2520Li%2520and%2520Wenyan%2520Cong%2520and%2520Jian%2520Zhang%2520and%2520Junbin%2520Lu%2520and%2520Chenguo%2520Lin%2520and%2520Dilin%2520Wang%2520and%2520Zhicheng%2520Yan%2520and%2520Hongyu%2520Xu%2520and%2520Justin%2520Theiss%2520and%2520Yue%2520Huang%2520and%2520Xinghao%2520Ding%2520and%2520Rakesh%2520Ranjan%2520and%2520Zhiwen%2520Fan%26entry.1292438233%3DUnderstanding%2520the%2520dynamic%2520physical%2520world%252C%2520characterized%2520by%2520its%2520evolving%25203D%2520structure%252C%2520real-world%2520motion%252C%2520and%2520semantic%2520content%2520with%2520textual%2520descriptions%252C%2520is%2520crucial%2520for%2520human-agent%2520interaction%2520and%2520enables%2520embodied%2520agents%2520to%2520perceive%2520and%2520act%2520within%2520real%2520environments%2520with%2520human-like%2520capabilities.%2520However%252C%2520existing%2520datasets%2520are%2520often%2520derived%2520from%2520limited%2520simulators%2520or%2520utilize%2520traditional%2520Structurefrom-Motion%2520for%2520up-to-scale%2520annotation%2520and%2520offer%2520limited%2520descriptive%2520captioning%252C%2520which%2520restricts%2520the%2520capacity%2520of%2520foundation%2520models%2520to%2520accurately%2520interpret%2520real-world%2520dynamics%2520from%2520monocular%2520videos%252C%2520commonly%2520sourced%2520from%2520the%2520internet.%2520To%2520bridge%2520these%2520gaps%252C%2520we%2520introduce%2520DynamicVerse%252C%2520a%2520physical-scale%252C%2520multimodal%25204D%2520world%2520modeling%2520framework%2520for%2520dynamic%2520real-world%2520video.%2520We%2520employ%2520large%2520vision%252C%2520geometric%252C%2520and%2520multimodal%2520models%2520to%2520interpret%2520metric-scale%2520static%2520geometry%252C%2520real-world%2520dynamic%2520motion%252C%2520instance-level%2520masks%252C%2520and%2520holistic%2520descriptive%2520captions.%2520By%2520integrating%2520window-based%2520Bundle%2520Adjustment%2520with%2520global%2520optimization%252C%2520our%2520method%2520converts%2520long%2520real-world%2520video%2520sequences%2520into%2520a%2520comprehensive%25204D%2520multimodal%2520format.%2520DynamicVerse%2520delivers%2520a%2520large-scale%2520dataset%2520consists%2520of%2520100K%252B%2520videos%2520with%2520800K%252B%2520annotated%2520masks%2520and%252010M%252B%2520frames%2520from%2520internet%2520videos.%2520Experimental%2520evaluations%2520on%2520three%2520benchmark%2520tasks%252C%2520namely%2520video%2520depth%2520estimation%252C%2520camera%2520pose%2520estimation%252C%2520and%2520camera%2520intrinsics%2520estimation%252C%2520demonstrate%2520that%2520our%25204D%2520modeling%2520achieves%2520superior%2520performance%2520in%2520capturing%2520physical-scale%2520measurements%2520with%2520greater%2520global%2520accuracy%2520than%2520existing%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03000v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynamicVerse%3A%20A%20Physically-Aware%20Multimodal%20Framework%20for%204D%20World%20Modeling&entry.906535625=Kairun%20Wen%20and%20Yuzhi%20Huang%20and%20Runyu%20Chen%20and%20Hui%20Zheng%20and%20Yunlong%20Lin%20and%20Panwang%20Pan%20and%20Chenxin%20Li%20and%20Wenyan%20Cong%20and%20Jian%20Zhang%20and%20Junbin%20Lu%20and%20Chenguo%20Lin%20and%20Dilin%20Wang%20and%20Zhicheng%20Yan%20and%20Hongyu%20Xu%20and%20Justin%20Theiss%20and%20Yue%20Huang%20and%20Xinghao%20Ding%20and%20Rakesh%20Ranjan%20and%20Zhiwen%20Fan&entry.1292438233=Understanding%20the%20dynamic%20physical%20world%2C%20characterized%20by%20its%20evolving%203D%20structure%2C%20real-world%20motion%2C%20and%20semantic%20content%20with%20textual%20descriptions%2C%20is%20crucial%20for%20human-agent%20interaction%20and%20enables%20embodied%20agents%20to%20perceive%20and%20act%20within%20real%20environments%20with%20human-like%20capabilities.%20However%2C%20existing%20datasets%20are%20often%20derived%20from%20limited%20simulators%20or%20utilize%20traditional%20Structurefrom-Motion%20for%20up-to-scale%20annotation%20and%20offer%20limited%20descriptive%20captioning%2C%20which%20restricts%20the%20capacity%20of%20foundation%20models%20to%20accurately%20interpret%20real-world%20dynamics%20from%20monocular%20videos%2C%20commonly%20sourced%20from%20the%20internet.%20To%20bridge%20these%20gaps%2C%20we%20introduce%20DynamicVerse%2C%20a%20physical-scale%2C%20multimodal%204D%20world%20modeling%20framework%20for%20dynamic%20real-world%20video.%20We%20employ%20large%20vision%2C%20geometric%2C%20and%20multimodal%20models%20to%20interpret%20metric-scale%20static%20geometry%2C%20real-world%20dynamic%20motion%2C%20instance-level%20masks%2C%20and%20holistic%20descriptive%20captions.%20By%20integrating%20window-based%20Bundle%20Adjustment%20with%20global%20optimization%2C%20our%20method%20converts%20long%20real-world%20video%20sequences%20into%20a%20comprehensive%204D%20multimodal%20format.%20DynamicVerse%20delivers%20a%20large-scale%20dataset%20consists%20of%20100K%2B%20videos%20with%20800K%2B%20annotated%20masks%20and%2010M%2B%20frames%20from%20internet%20videos.%20Experimental%20evaluations%20on%20three%20benchmark%20tasks%2C%20namely%20video%20depth%20estimation%2C%20camera%20pose%20estimation%2C%20and%20camera%20intrinsics%20estimation%2C%20demonstrate%20that%20our%204D%20modeling%20achieves%20superior%20performance%20in%20capturing%20physical-scale%20measurements%20with%20greater%20global%20accuracy%20than%20existing%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2512.03000v1&entry.124074799=Read"},
{"title": "CoT-X: An Adaptive Framework for Cross-Model Chain-of-Thought Transfer and Optimization", "author": "Ziqian Bi and Kaijie Chen and Tianyang Wang and Junfeng Hao and Benji Peng and Xinyuan Song", "abstract": "Chain-of-Thought (CoT) reasoning enhances the problem-solving ability of large language models (LLMs) but leads to substantial inference overhead, limiting deployment in resource-constrained settings. This paper investigates efficient CoT transfer across models of different scales and architectures through an adaptive reasoning summarization framework. The proposed method compresses reasoning traces via semantic segmentation with importance scoring, budget-aware dynamic compression, and coherence reconstruction, preserving critical reasoning steps while significantly reducing token usage. Experiments on 7{,}501 medical examination questions across 10 specialties show up to 40% higher accuracy than truncation under the same token budgets. Evaluations on 64 model pairs from eight LLMs (1.5B-32B parameters, including DeepSeek-R1 and Qwen3) confirm strong cross-model transferability. Furthermore, a Gaussian Process-based Bayesian optimization module reduces evaluation cost by 84% and reveals a power-law relationship between model size and cross-domain robustness. These results demonstrate that reasoning summarization provides a practical path toward efficient CoT transfer, enabling advanced reasoning under tight computational constraints. Code will be released upon publication.", "link": "http://arxiv.org/abs/2511.05747v2", "date": "2025-12-02", "relevancy": 2.5968, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5214}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5183}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5183}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoT-X%3A%20An%20Adaptive%20Framework%20for%20Cross-Model%20Chain-of-Thought%20Transfer%20and%20Optimization&body=Title%3A%20CoT-X%3A%20An%20Adaptive%20Framework%20for%20Cross-Model%20Chain-of-Thought%20Transfer%20and%20Optimization%0AAuthor%3A%20Ziqian%20Bi%20and%20Kaijie%20Chen%20and%20Tianyang%20Wang%20and%20Junfeng%20Hao%20and%20Benji%20Peng%20and%20Xinyuan%20Song%0AAbstract%3A%20Chain-of-Thought%20%28CoT%29%20reasoning%20enhances%20the%20problem-solving%20ability%20of%20large%20language%20models%20%28LLMs%29%20but%20leads%20to%20substantial%20inference%20overhead%2C%20limiting%20deployment%20in%20resource-constrained%20settings.%20This%20paper%20investigates%20efficient%20CoT%20transfer%20across%20models%20of%20different%20scales%20and%20architectures%20through%20an%20adaptive%20reasoning%20summarization%20framework.%20The%20proposed%20method%20compresses%20reasoning%20traces%20via%20semantic%20segmentation%20with%20importance%20scoring%2C%20budget-aware%20dynamic%20compression%2C%20and%20coherence%20reconstruction%2C%20preserving%20critical%20reasoning%20steps%20while%20significantly%20reducing%20token%20usage.%20Experiments%20on%207%7B%2C%7D501%20medical%20examination%20questions%20across%2010%20specialties%20show%20up%20to%2040%25%20higher%20accuracy%20than%20truncation%20under%20the%20same%20token%20budgets.%20Evaluations%20on%2064%20model%20pairs%20from%20eight%20LLMs%20%281.5B-32B%20parameters%2C%20including%20DeepSeek-R1%20and%20Qwen3%29%20confirm%20strong%20cross-model%20transferability.%20Furthermore%2C%20a%20Gaussian%20Process-based%20Bayesian%20optimization%20module%20reduces%20evaluation%20cost%20by%2084%25%20and%20reveals%20a%20power-law%20relationship%20between%20model%20size%20and%20cross-domain%20robustness.%20These%20results%20demonstrate%20that%20reasoning%20summarization%20provides%20a%20practical%20path%20toward%20efficient%20CoT%20transfer%2C%20enabling%20advanced%20reasoning%20under%20tight%20computational%20constraints.%20Code%20will%20be%20released%20upon%20publication.%0ALink%3A%20http%3A//arxiv.org/abs/2511.05747v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoT-X%253A%2520An%2520Adaptive%2520Framework%2520for%2520Cross-Model%2520Chain-of-Thought%2520Transfer%2520and%2520Optimization%26entry.906535625%3DZiqian%2520Bi%2520and%2520Kaijie%2520Chen%2520and%2520Tianyang%2520Wang%2520and%2520Junfeng%2520Hao%2520and%2520Benji%2520Peng%2520and%2520Xinyuan%2520Song%26entry.1292438233%3DChain-of-Thought%2520%2528CoT%2529%2520reasoning%2520enhances%2520the%2520problem-solving%2520ability%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520but%2520leads%2520to%2520substantial%2520inference%2520overhead%252C%2520limiting%2520deployment%2520in%2520resource-constrained%2520settings.%2520This%2520paper%2520investigates%2520efficient%2520CoT%2520transfer%2520across%2520models%2520of%2520different%2520scales%2520and%2520architectures%2520through%2520an%2520adaptive%2520reasoning%2520summarization%2520framework.%2520The%2520proposed%2520method%2520compresses%2520reasoning%2520traces%2520via%2520semantic%2520segmentation%2520with%2520importance%2520scoring%252C%2520budget-aware%2520dynamic%2520compression%252C%2520and%2520coherence%2520reconstruction%252C%2520preserving%2520critical%2520reasoning%2520steps%2520while%2520significantly%2520reducing%2520token%2520usage.%2520Experiments%2520on%25207%257B%252C%257D501%2520medical%2520examination%2520questions%2520across%252010%2520specialties%2520show%2520up%2520to%252040%2525%2520higher%2520accuracy%2520than%2520truncation%2520under%2520the%2520same%2520token%2520budgets.%2520Evaluations%2520on%252064%2520model%2520pairs%2520from%2520eight%2520LLMs%2520%25281.5B-32B%2520parameters%252C%2520including%2520DeepSeek-R1%2520and%2520Qwen3%2529%2520confirm%2520strong%2520cross-model%2520transferability.%2520Furthermore%252C%2520a%2520Gaussian%2520Process-based%2520Bayesian%2520optimization%2520module%2520reduces%2520evaluation%2520cost%2520by%252084%2525%2520and%2520reveals%2520a%2520power-law%2520relationship%2520between%2520model%2520size%2520and%2520cross-domain%2520robustness.%2520These%2520results%2520demonstrate%2520that%2520reasoning%2520summarization%2520provides%2520a%2520practical%2520path%2520toward%2520efficient%2520CoT%2520transfer%252C%2520enabling%2520advanced%2520reasoning%2520under%2520tight%2520computational%2520constraints.%2520Code%2520will%2520be%2520released%2520upon%2520publication.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.05747v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoT-X%3A%20An%20Adaptive%20Framework%20for%20Cross-Model%20Chain-of-Thought%20Transfer%20and%20Optimization&entry.906535625=Ziqian%20Bi%20and%20Kaijie%20Chen%20and%20Tianyang%20Wang%20and%20Junfeng%20Hao%20and%20Benji%20Peng%20and%20Xinyuan%20Song&entry.1292438233=Chain-of-Thought%20%28CoT%29%20reasoning%20enhances%20the%20problem-solving%20ability%20of%20large%20language%20models%20%28LLMs%29%20but%20leads%20to%20substantial%20inference%20overhead%2C%20limiting%20deployment%20in%20resource-constrained%20settings.%20This%20paper%20investigates%20efficient%20CoT%20transfer%20across%20models%20of%20different%20scales%20and%20architectures%20through%20an%20adaptive%20reasoning%20summarization%20framework.%20The%20proposed%20method%20compresses%20reasoning%20traces%20via%20semantic%20segmentation%20with%20importance%20scoring%2C%20budget-aware%20dynamic%20compression%2C%20and%20coherence%20reconstruction%2C%20preserving%20critical%20reasoning%20steps%20while%20significantly%20reducing%20token%20usage.%20Experiments%20on%207%7B%2C%7D501%20medical%20examination%20questions%20across%2010%20specialties%20show%20up%20to%2040%25%20higher%20accuracy%20than%20truncation%20under%20the%20same%20token%20budgets.%20Evaluations%20on%2064%20model%20pairs%20from%20eight%20LLMs%20%281.5B-32B%20parameters%2C%20including%20DeepSeek-R1%20and%20Qwen3%29%20confirm%20strong%20cross-model%20transferability.%20Furthermore%2C%20a%20Gaussian%20Process-based%20Bayesian%20optimization%20module%20reduces%20evaluation%20cost%20by%2084%25%20and%20reveals%20a%20power-law%20relationship%20between%20model%20size%20and%20cross-domain%20robustness.%20These%20results%20demonstrate%20that%20reasoning%20summarization%20provides%20a%20practical%20path%20toward%20efficient%20CoT%20transfer%2C%20enabling%20advanced%20reasoning%20under%20tight%20computational%20constraints.%20Code%20will%20be%20released%20upon%20publication.&entry.1838667208=http%3A//arxiv.org/abs/2511.05747v2&entry.124074799=Read"},
{"title": "Morphling: Fast, Fused, and Flexible GNN Training at Scale", "author": " Anubhab and Rupesh Nasre", "abstract": "Graph Neural Networks (GNNs) present a fundamental hardware challenge by fusing irregular, memory-bound graph traversals with regular, compute-intensive dense matrix operations. While frameworks such as PyTorch Geometric (PyG) and Deep Graph Library (DGL) prioritize high-level usability, they fail to address these divergent execution characteristics. As a result, they rely on generic kernels that suffer from poor cache locality, excessive memory movement, and substantial intermediate allocations. To address these limitations, we present Morphling, a domain-specific code synthesizer designed to bridge this gap. Morphling compiles high-level GNN specifications into portable, backend-specialized implementations targeting OpenMP, CUDA, and MPI. It achieves this by instantiating a library of optimized, architecture-aware primitives tailored to each execution environment. Morphling also incorporates a runtime sparsity-aware execution engine that dynamically selects dense or sparse execution paths using input feature statistics, reducing unnecessary computation on zero-valued entries. We evaluate Morphling on eleven real-world datasets spanning diverse graph structures, feature dimensionalities, and sparsity regimes. The results show that Morphling improves per-epoch training throughput by an average of 20X on CPUs and 19X on GPUs over PyG and DGL, with peak speedups reaching 66X. Morphling's memory-efficient layouts further reduce peak memory consumption by up to 15X, enabling large-scale GNN training on commodity hardware. These findings demonstrate that specialized, architecture-aware code synthesis provides an effective and scalable path toward high-performance GNN execution across diverse parallel and distributed platforms.", "link": "http://arxiv.org/abs/2512.01678v2", "date": "2025-12-02", "relevancy": 2.5725, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5477}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5017}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Morphling%3A%20Fast%2C%20Fused%2C%20and%20Flexible%20GNN%20Training%20at%20Scale&body=Title%3A%20Morphling%3A%20Fast%2C%20Fused%2C%20and%20Flexible%20GNN%20Training%20at%20Scale%0AAuthor%3A%20%20Anubhab%20and%20Rupesh%20Nasre%0AAbstract%3A%20Graph%20Neural%20Networks%20%28GNNs%29%20present%20a%20fundamental%20hardware%20challenge%20by%20fusing%20irregular%2C%20memory-bound%20graph%20traversals%20with%20regular%2C%20compute-intensive%20dense%20matrix%20operations.%20While%20frameworks%20such%20as%20PyTorch%20Geometric%20%28PyG%29%20and%20Deep%20Graph%20Library%20%28DGL%29%20prioritize%20high-level%20usability%2C%20they%20fail%20to%20address%20these%20divergent%20execution%20characteristics.%20As%20a%20result%2C%20they%20rely%20on%20generic%20kernels%20that%20suffer%20from%20poor%20cache%20locality%2C%20excessive%20memory%20movement%2C%20and%20substantial%20intermediate%20allocations.%20To%20address%20these%20limitations%2C%20we%20present%20Morphling%2C%20a%20domain-specific%20code%20synthesizer%20designed%20to%20bridge%20this%20gap.%20Morphling%20compiles%20high-level%20GNN%20specifications%20into%20portable%2C%20backend-specialized%20implementations%20targeting%20OpenMP%2C%20CUDA%2C%20and%20MPI.%20It%20achieves%20this%20by%20instantiating%20a%20library%20of%20optimized%2C%20architecture-aware%20primitives%20tailored%20to%20each%20execution%20environment.%20Morphling%20also%20incorporates%20a%20runtime%20sparsity-aware%20execution%20engine%20that%20dynamically%20selects%20dense%20or%20sparse%20execution%20paths%20using%20input%20feature%20statistics%2C%20reducing%20unnecessary%20computation%20on%20zero-valued%20entries.%20We%20evaluate%20Morphling%20on%20eleven%20real-world%20datasets%20spanning%20diverse%20graph%20structures%2C%20feature%20dimensionalities%2C%20and%20sparsity%20regimes.%20The%20results%20show%20that%20Morphling%20improves%20per-epoch%20training%20throughput%20by%20an%20average%20of%2020X%20on%20CPUs%20and%2019X%20on%20GPUs%20over%20PyG%20and%20DGL%2C%20with%20peak%20speedups%20reaching%2066X.%20Morphling%27s%20memory-efficient%20layouts%20further%20reduce%20peak%20memory%20consumption%20by%20up%20to%2015X%2C%20enabling%20large-scale%20GNN%20training%20on%20commodity%20hardware.%20These%20findings%20demonstrate%20that%20specialized%2C%20architecture-aware%20code%20synthesis%20provides%20an%20effective%20and%20scalable%20path%20toward%20high-performance%20GNN%20execution%20across%20diverse%20parallel%20and%20distributed%20platforms.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01678v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMorphling%253A%2520Fast%252C%2520Fused%252C%2520and%2520Flexible%2520GNN%2520Training%2520at%2520Scale%26entry.906535625%3D%2520Anubhab%2520and%2520Rupesh%2520Nasre%26entry.1292438233%3DGraph%2520Neural%2520Networks%2520%2528GNNs%2529%2520present%2520a%2520fundamental%2520hardware%2520challenge%2520by%2520fusing%2520irregular%252C%2520memory-bound%2520graph%2520traversals%2520with%2520regular%252C%2520compute-intensive%2520dense%2520matrix%2520operations.%2520While%2520frameworks%2520such%2520as%2520PyTorch%2520Geometric%2520%2528PyG%2529%2520and%2520Deep%2520Graph%2520Library%2520%2528DGL%2529%2520prioritize%2520high-level%2520usability%252C%2520they%2520fail%2520to%2520address%2520these%2520divergent%2520execution%2520characteristics.%2520As%2520a%2520result%252C%2520they%2520rely%2520on%2520generic%2520kernels%2520that%2520suffer%2520from%2520poor%2520cache%2520locality%252C%2520excessive%2520memory%2520movement%252C%2520and%2520substantial%2520intermediate%2520allocations.%2520To%2520address%2520these%2520limitations%252C%2520we%2520present%2520Morphling%252C%2520a%2520domain-specific%2520code%2520synthesizer%2520designed%2520to%2520bridge%2520this%2520gap.%2520Morphling%2520compiles%2520high-level%2520GNN%2520specifications%2520into%2520portable%252C%2520backend-specialized%2520implementations%2520targeting%2520OpenMP%252C%2520CUDA%252C%2520and%2520MPI.%2520It%2520achieves%2520this%2520by%2520instantiating%2520a%2520library%2520of%2520optimized%252C%2520architecture-aware%2520primitives%2520tailored%2520to%2520each%2520execution%2520environment.%2520Morphling%2520also%2520incorporates%2520a%2520runtime%2520sparsity-aware%2520execution%2520engine%2520that%2520dynamically%2520selects%2520dense%2520or%2520sparse%2520execution%2520paths%2520using%2520input%2520feature%2520statistics%252C%2520reducing%2520unnecessary%2520computation%2520on%2520zero-valued%2520entries.%2520We%2520evaluate%2520Morphling%2520on%2520eleven%2520real-world%2520datasets%2520spanning%2520diverse%2520graph%2520structures%252C%2520feature%2520dimensionalities%252C%2520and%2520sparsity%2520regimes.%2520The%2520results%2520show%2520that%2520Morphling%2520improves%2520per-epoch%2520training%2520throughput%2520by%2520an%2520average%2520of%252020X%2520on%2520CPUs%2520and%252019X%2520on%2520GPUs%2520over%2520PyG%2520and%2520DGL%252C%2520with%2520peak%2520speedups%2520reaching%252066X.%2520Morphling%2527s%2520memory-efficient%2520layouts%2520further%2520reduce%2520peak%2520memory%2520consumption%2520by%2520up%2520to%252015X%252C%2520enabling%2520large-scale%2520GNN%2520training%2520on%2520commodity%2520hardware.%2520These%2520findings%2520demonstrate%2520that%2520specialized%252C%2520architecture-aware%2520code%2520synthesis%2520provides%2520an%2520effective%2520and%2520scalable%2520path%2520toward%2520high-performance%2520GNN%2520execution%2520across%2520diverse%2520parallel%2520and%2520distributed%2520platforms.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01678v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Morphling%3A%20Fast%2C%20Fused%2C%20and%20Flexible%20GNN%20Training%20at%20Scale&entry.906535625=%20Anubhab%20and%20Rupesh%20Nasre&entry.1292438233=Graph%20Neural%20Networks%20%28GNNs%29%20present%20a%20fundamental%20hardware%20challenge%20by%20fusing%20irregular%2C%20memory-bound%20graph%20traversals%20with%20regular%2C%20compute-intensive%20dense%20matrix%20operations.%20While%20frameworks%20such%20as%20PyTorch%20Geometric%20%28PyG%29%20and%20Deep%20Graph%20Library%20%28DGL%29%20prioritize%20high-level%20usability%2C%20they%20fail%20to%20address%20these%20divergent%20execution%20characteristics.%20As%20a%20result%2C%20they%20rely%20on%20generic%20kernels%20that%20suffer%20from%20poor%20cache%20locality%2C%20excessive%20memory%20movement%2C%20and%20substantial%20intermediate%20allocations.%20To%20address%20these%20limitations%2C%20we%20present%20Morphling%2C%20a%20domain-specific%20code%20synthesizer%20designed%20to%20bridge%20this%20gap.%20Morphling%20compiles%20high-level%20GNN%20specifications%20into%20portable%2C%20backend-specialized%20implementations%20targeting%20OpenMP%2C%20CUDA%2C%20and%20MPI.%20It%20achieves%20this%20by%20instantiating%20a%20library%20of%20optimized%2C%20architecture-aware%20primitives%20tailored%20to%20each%20execution%20environment.%20Morphling%20also%20incorporates%20a%20runtime%20sparsity-aware%20execution%20engine%20that%20dynamically%20selects%20dense%20or%20sparse%20execution%20paths%20using%20input%20feature%20statistics%2C%20reducing%20unnecessary%20computation%20on%20zero-valued%20entries.%20We%20evaluate%20Morphling%20on%20eleven%20real-world%20datasets%20spanning%20diverse%20graph%20structures%2C%20feature%20dimensionalities%2C%20and%20sparsity%20regimes.%20The%20results%20show%20that%20Morphling%20improves%20per-epoch%20training%20throughput%20by%20an%20average%20of%2020X%20on%20CPUs%20and%2019X%20on%20GPUs%20over%20PyG%20and%20DGL%2C%20with%20peak%20speedups%20reaching%2066X.%20Morphling%27s%20memory-efficient%20layouts%20further%20reduce%20peak%20memory%20consumption%20by%20up%20to%2015X%2C%20enabling%20large-scale%20GNN%20training%20on%20commodity%20hardware.%20These%20findings%20demonstrate%20that%20specialized%2C%20architecture-aware%20code%20synthesis%20provides%20an%20effective%20and%20scalable%20path%20toward%20high-performance%20GNN%20execution%20across%20diverse%20parallel%20and%20distributed%20platforms.&entry.1838667208=http%3A//arxiv.org/abs/2512.01678v2&entry.124074799=Read"},
{"title": "VLM-Pruner: Buffering for Spatial Sparsity in an Efficient VLM Centrifugal Token Pruning Paradigm", "author": "Zhenkai Wu and Xiaowen Ma and Zhenliang Ni and Dengming Zhang and Han Shu and Xin Jiang and Xinghao Chen", "abstract": "Vision-language models (VLMs) excel at image understanding tasks, but the large number of visual tokens imposes significant computational costs, hindering deployment on mobile devices. Many pruning methods rely solely on token importance and thus overlook inter-token redundancy, retaining numerous duplicated tokens and wasting capacity. Although some redundancy-aware approaches have been proposed, they often ignore the spatial relationships among visual tokens. This can lead to overly sparse selections of retained tokens that fail to adequately cover the regions of target objects. To address these limitations, we propose VLM-Pruner, a training-free token pruning algorithm that explicitly balances redundancy and spatial sparsity. We introduce a centrifugal token pruning paradigm that enables near-to-far selection while prioritizing the preservation of fine-grained object details. Moreover, we design a Buffering for Spatial Sparsity (BSS) criterion that defers the selection of spatially distant tokens. We further adopt a parallel greedy strategy to conduct token selection efficiently. To mitigate information loss from pruning, we selectively fuse salient information from the discarded tokens into the retained ones. Comprehensive comparisons demonstrate that VLM-Pruner consistently outperforms strong baselines across five VLMs with an 88.9\\% pruning rate, while delivering an end-to-end inference speedup.", "link": "http://arxiv.org/abs/2512.02700v1", "date": "2025-12-02", "relevancy": 2.5659, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5328}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5034}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VLM-Pruner%3A%20Buffering%20for%20Spatial%20Sparsity%20in%20an%20Efficient%20VLM%20Centrifugal%20Token%20Pruning%20Paradigm&body=Title%3A%20VLM-Pruner%3A%20Buffering%20for%20Spatial%20Sparsity%20in%20an%20Efficient%20VLM%20Centrifugal%20Token%20Pruning%20Paradigm%0AAuthor%3A%20Zhenkai%20Wu%20and%20Xiaowen%20Ma%20and%20Zhenliang%20Ni%20and%20Dengming%20Zhang%20and%20Han%20Shu%20and%20Xin%20Jiang%20and%20Xinghao%20Chen%0AAbstract%3A%20Vision-language%20models%20%28VLMs%29%20excel%20at%20image%20understanding%20tasks%2C%20but%20the%20large%20number%20of%20visual%20tokens%20imposes%20significant%20computational%20costs%2C%20hindering%20deployment%20on%20mobile%20devices.%20Many%20pruning%20methods%20rely%20solely%20on%20token%20importance%20and%20thus%20overlook%20inter-token%20redundancy%2C%20retaining%20numerous%20duplicated%20tokens%20and%20wasting%20capacity.%20Although%20some%20redundancy-aware%20approaches%20have%20been%20proposed%2C%20they%20often%20ignore%20the%20spatial%20relationships%20among%20visual%20tokens.%20This%20can%20lead%20to%20overly%20sparse%20selections%20of%20retained%20tokens%20that%20fail%20to%20adequately%20cover%20the%20regions%20of%20target%20objects.%20To%20address%20these%20limitations%2C%20we%20propose%20VLM-Pruner%2C%20a%20training-free%20token%20pruning%20algorithm%20that%20explicitly%20balances%20redundancy%20and%20spatial%20sparsity.%20We%20introduce%20a%20centrifugal%20token%20pruning%20paradigm%20that%20enables%20near-to-far%20selection%20while%20prioritizing%20the%20preservation%20of%20fine-grained%20object%20details.%20Moreover%2C%20we%20design%20a%20Buffering%20for%20Spatial%20Sparsity%20%28BSS%29%20criterion%20that%20defers%20the%20selection%20of%20spatially%20distant%20tokens.%20We%20further%20adopt%20a%20parallel%20greedy%20strategy%20to%20conduct%20token%20selection%20efficiently.%20To%20mitigate%20information%20loss%20from%20pruning%2C%20we%20selectively%20fuse%20salient%20information%20from%20the%20discarded%20tokens%20into%20the%20retained%20ones.%20Comprehensive%20comparisons%20demonstrate%20that%20VLM-Pruner%20consistently%20outperforms%20strong%20baselines%20across%20five%20VLMs%20with%20an%2088.9%5C%25%20pruning%20rate%2C%20while%20delivering%20an%20end-to-end%20inference%20speedup.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVLM-Pruner%253A%2520Buffering%2520for%2520Spatial%2520Sparsity%2520in%2520an%2520Efficient%2520VLM%2520Centrifugal%2520Token%2520Pruning%2520Paradigm%26entry.906535625%3DZhenkai%2520Wu%2520and%2520Xiaowen%2520Ma%2520and%2520Zhenliang%2520Ni%2520and%2520Dengming%2520Zhang%2520and%2520Han%2520Shu%2520and%2520Xin%2520Jiang%2520and%2520Xinghao%2520Chen%26entry.1292438233%3DVision-language%2520models%2520%2528VLMs%2529%2520excel%2520at%2520image%2520understanding%2520tasks%252C%2520but%2520the%2520large%2520number%2520of%2520visual%2520tokens%2520imposes%2520significant%2520computational%2520costs%252C%2520hindering%2520deployment%2520on%2520mobile%2520devices.%2520Many%2520pruning%2520methods%2520rely%2520solely%2520on%2520token%2520importance%2520and%2520thus%2520overlook%2520inter-token%2520redundancy%252C%2520retaining%2520numerous%2520duplicated%2520tokens%2520and%2520wasting%2520capacity.%2520Although%2520some%2520redundancy-aware%2520approaches%2520have%2520been%2520proposed%252C%2520they%2520often%2520ignore%2520the%2520spatial%2520relationships%2520among%2520visual%2520tokens.%2520This%2520can%2520lead%2520to%2520overly%2520sparse%2520selections%2520of%2520retained%2520tokens%2520that%2520fail%2520to%2520adequately%2520cover%2520the%2520regions%2520of%2520target%2520objects.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520VLM-Pruner%252C%2520a%2520training-free%2520token%2520pruning%2520algorithm%2520that%2520explicitly%2520balances%2520redundancy%2520and%2520spatial%2520sparsity.%2520We%2520introduce%2520a%2520centrifugal%2520token%2520pruning%2520paradigm%2520that%2520enables%2520near-to-far%2520selection%2520while%2520prioritizing%2520the%2520preservation%2520of%2520fine-grained%2520object%2520details.%2520Moreover%252C%2520we%2520design%2520a%2520Buffering%2520for%2520Spatial%2520Sparsity%2520%2528BSS%2529%2520criterion%2520that%2520defers%2520the%2520selection%2520of%2520spatially%2520distant%2520tokens.%2520We%2520further%2520adopt%2520a%2520parallel%2520greedy%2520strategy%2520to%2520conduct%2520token%2520selection%2520efficiently.%2520To%2520mitigate%2520information%2520loss%2520from%2520pruning%252C%2520we%2520selectively%2520fuse%2520salient%2520information%2520from%2520the%2520discarded%2520tokens%2520into%2520the%2520retained%2520ones.%2520Comprehensive%2520comparisons%2520demonstrate%2520that%2520VLM-Pruner%2520consistently%2520outperforms%2520strong%2520baselines%2520across%2520five%2520VLMs%2520with%2520an%252088.9%255C%2525%2520pruning%2520rate%252C%2520while%2520delivering%2520an%2520end-to-end%2520inference%2520speedup.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VLM-Pruner%3A%20Buffering%20for%20Spatial%20Sparsity%20in%20an%20Efficient%20VLM%20Centrifugal%20Token%20Pruning%20Paradigm&entry.906535625=Zhenkai%20Wu%20and%20Xiaowen%20Ma%20and%20Zhenliang%20Ni%20and%20Dengming%20Zhang%20and%20Han%20Shu%20and%20Xin%20Jiang%20and%20Xinghao%20Chen&entry.1292438233=Vision-language%20models%20%28VLMs%29%20excel%20at%20image%20understanding%20tasks%2C%20but%20the%20large%20number%20of%20visual%20tokens%20imposes%20significant%20computational%20costs%2C%20hindering%20deployment%20on%20mobile%20devices.%20Many%20pruning%20methods%20rely%20solely%20on%20token%20importance%20and%20thus%20overlook%20inter-token%20redundancy%2C%20retaining%20numerous%20duplicated%20tokens%20and%20wasting%20capacity.%20Although%20some%20redundancy-aware%20approaches%20have%20been%20proposed%2C%20they%20often%20ignore%20the%20spatial%20relationships%20among%20visual%20tokens.%20This%20can%20lead%20to%20overly%20sparse%20selections%20of%20retained%20tokens%20that%20fail%20to%20adequately%20cover%20the%20regions%20of%20target%20objects.%20To%20address%20these%20limitations%2C%20we%20propose%20VLM-Pruner%2C%20a%20training-free%20token%20pruning%20algorithm%20that%20explicitly%20balances%20redundancy%20and%20spatial%20sparsity.%20We%20introduce%20a%20centrifugal%20token%20pruning%20paradigm%20that%20enables%20near-to-far%20selection%20while%20prioritizing%20the%20preservation%20of%20fine-grained%20object%20details.%20Moreover%2C%20we%20design%20a%20Buffering%20for%20Spatial%20Sparsity%20%28BSS%29%20criterion%20that%20defers%20the%20selection%20of%20spatially%20distant%20tokens.%20We%20further%20adopt%20a%20parallel%20greedy%20strategy%20to%20conduct%20token%20selection%20efficiently.%20To%20mitigate%20information%20loss%20from%20pruning%2C%20we%20selectively%20fuse%20salient%20information%20from%20the%20discarded%20tokens%20into%20the%20retained%20ones.%20Comprehensive%20comparisons%20demonstrate%20that%20VLM-Pruner%20consistently%20outperforms%20strong%20baselines%20across%20five%20VLMs%20with%20an%2088.9%5C%25%20pruning%20rate%2C%20while%20delivering%20an%20end-to-end%20inference%20speedup.&entry.1838667208=http%3A//arxiv.org/abs/2512.02700v1&entry.124074799=Read"},
{"title": "Diagnose, Correct, and Learn from Manipulation Failures via Visual Symbols", "author": "Xianchao Zeng and Xinyu Zhou and Youcheng Li and Jiayou Shi and Tianle Li and Liangming Chen and Lei Ren and Yong-Lu Li", "abstract": "Vision-Language-Action (VLA) models have recently achieved remarkable progress in robotic manipulation, yet they remain limited in failure diagnosis and learning from failures. Additionally, existing failure datasets are mostly generated programmatically in simulation, which limits their generalization to the real world. In light of these, we introduce ViFailback, a framework designed to diagnose robotic manipulation failures and provide both textual and visual correction guidance. Our framework utilizes explicit visual symbols to enhance annotation efficiency. We further release the ViFailback dataset, a large-scale collection of 58,126 Visual Question Answering (VQA) pairs along with their corresponding 5,202 real-world manipulation trajectories. Based on the dataset, we establish ViFailback-Bench, a benchmark of 11 fine-grained VQA tasks designed to assess the failure diagnosis and correction abilities of Vision-Language Models (VLMs), featuring ViFailback-Bench Lite for closed-ended and ViFailback-Bench Hard for open-ended evaluation. To demonstrate the effectiveness of our framework, we built the ViFailback-8B VLM, which not only achieves significant overall performance improvement on ViFailback-Bench but also generates visual symbols for corrective action guidance. Finally, by integrating ViFailback-8B with a VLA model, we conduct real-world robotic experiments demonstrating its ability to assist the VLA model in recovering from failures. Project Website: https://x1nyuzhou.github.io/vifailback.github.io/", "link": "http://arxiv.org/abs/2512.02787v1", "date": "2025-12-02", "relevancy": 2.5645, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5177}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5105}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diagnose%2C%20Correct%2C%20and%20Learn%20from%20Manipulation%20Failures%20via%20Visual%20Symbols&body=Title%3A%20Diagnose%2C%20Correct%2C%20and%20Learn%20from%20Manipulation%20Failures%20via%20Visual%20Symbols%0AAuthor%3A%20Xianchao%20Zeng%20and%20Xinyu%20Zhou%20and%20Youcheng%20Li%20and%20Jiayou%20Shi%20and%20Tianle%20Li%20and%20Liangming%20Chen%20and%20Lei%20Ren%20and%20Yong-Lu%20Li%0AAbstract%3A%20Vision-Language-Action%20%28VLA%29%20models%20have%20recently%20achieved%20remarkable%20progress%20in%20robotic%20manipulation%2C%20yet%20they%20remain%20limited%20in%20failure%20diagnosis%20and%20learning%20from%20failures.%20Additionally%2C%20existing%20failure%20datasets%20are%20mostly%20generated%20programmatically%20in%20simulation%2C%20which%20limits%20their%20generalization%20to%20the%20real%20world.%20In%20light%20of%20these%2C%20we%20introduce%20ViFailback%2C%20a%20framework%20designed%20to%20diagnose%20robotic%20manipulation%20failures%20and%20provide%20both%20textual%20and%20visual%20correction%20guidance.%20Our%20framework%20utilizes%20explicit%20visual%20symbols%20to%20enhance%20annotation%20efficiency.%20We%20further%20release%20the%20ViFailback%20dataset%2C%20a%20large-scale%20collection%20of%2058%2C126%20Visual%20Question%20Answering%20%28VQA%29%20pairs%20along%20with%20their%20corresponding%205%2C202%20real-world%20manipulation%20trajectories.%20Based%20on%20the%20dataset%2C%20we%20establish%20ViFailback-Bench%2C%20a%20benchmark%20of%2011%20fine-grained%20VQA%20tasks%20designed%20to%20assess%20the%20failure%20diagnosis%20and%20correction%20abilities%20of%20Vision-Language%20Models%20%28VLMs%29%2C%20featuring%20ViFailback-Bench%20Lite%20for%20closed-ended%20and%20ViFailback-Bench%20Hard%20for%20open-ended%20evaluation.%20To%20demonstrate%20the%20effectiveness%20of%20our%20framework%2C%20we%20built%20the%20ViFailback-8B%20VLM%2C%20which%20not%20only%20achieves%20significant%20overall%20performance%20improvement%20on%20ViFailback-Bench%20but%20also%20generates%20visual%20symbols%20for%20corrective%20action%20guidance.%20Finally%2C%20by%20integrating%20ViFailback-8B%20with%20a%20VLA%20model%2C%20we%20conduct%20real-world%20robotic%20experiments%20demonstrating%20its%20ability%20to%20assist%20the%20VLA%20model%20in%20recovering%20from%20failures.%20Project%20Website%3A%20https%3A//x1nyuzhou.github.io/vifailback.github.io/%0ALink%3A%20http%3A//arxiv.org/abs/2512.02787v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiagnose%252C%2520Correct%252C%2520and%2520Learn%2520from%2520Manipulation%2520Failures%2520via%2520Visual%2520Symbols%26entry.906535625%3DXianchao%2520Zeng%2520and%2520Xinyu%2520Zhou%2520and%2520Youcheng%2520Li%2520and%2520Jiayou%2520Shi%2520and%2520Tianle%2520Li%2520and%2520Liangming%2520Chen%2520and%2520Lei%2520Ren%2520and%2520Yong-Lu%2520Li%26entry.1292438233%3DVision-Language-Action%2520%2528VLA%2529%2520models%2520have%2520recently%2520achieved%2520remarkable%2520progress%2520in%2520robotic%2520manipulation%252C%2520yet%2520they%2520remain%2520limited%2520in%2520failure%2520diagnosis%2520and%2520learning%2520from%2520failures.%2520Additionally%252C%2520existing%2520failure%2520datasets%2520are%2520mostly%2520generated%2520programmatically%2520in%2520simulation%252C%2520which%2520limits%2520their%2520generalization%2520to%2520the%2520real%2520world.%2520In%2520light%2520of%2520these%252C%2520we%2520introduce%2520ViFailback%252C%2520a%2520framework%2520designed%2520to%2520diagnose%2520robotic%2520manipulation%2520failures%2520and%2520provide%2520both%2520textual%2520and%2520visual%2520correction%2520guidance.%2520Our%2520framework%2520utilizes%2520explicit%2520visual%2520symbols%2520to%2520enhance%2520annotation%2520efficiency.%2520We%2520further%2520release%2520the%2520ViFailback%2520dataset%252C%2520a%2520large-scale%2520collection%2520of%252058%252C126%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520pairs%2520along%2520with%2520their%2520corresponding%25205%252C202%2520real-world%2520manipulation%2520trajectories.%2520Based%2520on%2520the%2520dataset%252C%2520we%2520establish%2520ViFailback-Bench%252C%2520a%2520benchmark%2520of%252011%2520fine-grained%2520VQA%2520tasks%2520designed%2520to%2520assess%2520the%2520failure%2520diagnosis%2520and%2520correction%2520abilities%2520of%2520Vision-Language%2520Models%2520%2528VLMs%2529%252C%2520featuring%2520ViFailback-Bench%2520Lite%2520for%2520closed-ended%2520and%2520ViFailback-Bench%2520Hard%2520for%2520open-ended%2520evaluation.%2520To%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520framework%252C%2520we%2520built%2520the%2520ViFailback-8B%2520VLM%252C%2520which%2520not%2520only%2520achieves%2520significant%2520overall%2520performance%2520improvement%2520on%2520ViFailback-Bench%2520but%2520also%2520generates%2520visual%2520symbols%2520for%2520corrective%2520action%2520guidance.%2520Finally%252C%2520by%2520integrating%2520ViFailback-8B%2520with%2520a%2520VLA%2520model%252C%2520we%2520conduct%2520real-world%2520robotic%2520experiments%2520demonstrating%2520its%2520ability%2520to%2520assist%2520the%2520VLA%2520model%2520in%2520recovering%2520from%2520failures.%2520Project%2520Website%253A%2520https%253A//x1nyuzhou.github.io/vifailback.github.io/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02787v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diagnose%2C%20Correct%2C%20and%20Learn%20from%20Manipulation%20Failures%20via%20Visual%20Symbols&entry.906535625=Xianchao%20Zeng%20and%20Xinyu%20Zhou%20and%20Youcheng%20Li%20and%20Jiayou%20Shi%20and%20Tianle%20Li%20and%20Liangming%20Chen%20and%20Lei%20Ren%20and%20Yong-Lu%20Li&entry.1292438233=Vision-Language-Action%20%28VLA%29%20models%20have%20recently%20achieved%20remarkable%20progress%20in%20robotic%20manipulation%2C%20yet%20they%20remain%20limited%20in%20failure%20diagnosis%20and%20learning%20from%20failures.%20Additionally%2C%20existing%20failure%20datasets%20are%20mostly%20generated%20programmatically%20in%20simulation%2C%20which%20limits%20their%20generalization%20to%20the%20real%20world.%20In%20light%20of%20these%2C%20we%20introduce%20ViFailback%2C%20a%20framework%20designed%20to%20diagnose%20robotic%20manipulation%20failures%20and%20provide%20both%20textual%20and%20visual%20correction%20guidance.%20Our%20framework%20utilizes%20explicit%20visual%20symbols%20to%20enhance%20annotation%20efficiency.%20We%20further%20release%20the%20ViFailback%20dataset%2C%20a%20large-scale%20collection%20of%2058%2C126%20Visual%20Question%20Answering%20%28VQA%29%20pairs%20along%20with%20their%20corresponding%205%2C202%20real-world%20manipulation%20trajectories.%20Based%20on%20the%20dataset%2C%20we%20establish%20ViFailback-Bench%2C%20a%20benchmark%20of%2011%20fine-grained%20VQA%20tasks%20designed%20to%20assess%20the%20failure%20diagnosis%20and%20correction%20abilities%20of%20Vision-Language%20Models%20%28VLMs%29%2C%20featuring%20ViFailback-Bench%20Lite%20for%20closed-ended%20and%20ViFailback-Bench%20Hard%20for%20open-ended%20evaluation.%20To%20demonstrate%20the%20effectiveness%20of%20our%20framework%2C%20we%20built%20the%20ViFailback-8B%20VLM%2C%20which%20not%20only%20achieves%20significant%20overall%20performance%20improvement%20on%20ViFailback-Bench%20but%20also%20generates%20visual%20symbols%20for%20corrective%20action%20guidance.%20Finally%2C%20by%20integrating%20ViFailback-8B%20with%20a%20VLA%20model%2C%20we%20conduct%20real-world%20robotic%20experiments%20demonstrating%20its%20ability%20to%20assist%20the%20VLA%20model%20in%20recovering%20from%20failures.%20Project%20Website%3A%20https%3A//x1nyuzhou.github.io/vifailback.github.io/&entry.1838667208=http%3A//arxiv.org/abs/2512.02787v1&entry.124074799=Read"},
{"title": "Multi-View Graph Learning with Graph-Tuple", "author": "Shiyu Chen and Ningyuan Huang and Soledad Villar", "abstract": "Graph Neural Networks (GNNs) typically scale with the number of graph edges, making them well suited for sparse graphs but less efficient on dense graphs, such as point clouds or molecular interactions. A common remedy is to sparsify the graph via similarity thresholding or distance pruning, but this forces an arbitrary choice of a single interaction scale and discards crucial information from other scales. To overcome this limitation, we introduce a multi-view graph-tuple framework. Instead of a single graph, our graph-tuple framework partitions the graph into disjoint subgraphs, capturing primary local interactions and weaker, long-range connections. We then learn multi-view representations from the graph-tuple via a heterogeneous message-passing architecture inspired by the theory of non-commuting operators, which we formally prove is strictly more expressive and guarantees a lower oracle risk compared to single-graph message-passing models. We instantiate our framework on two scientific domains: molecular property prediction from feature-scarce Coulomb matrices and cosmological parameter inference from geometric point clouds. On both applications, our multi-view graph-tuple models demonstrate better performance than single-graph baselines, highlighting the power and versatility of our multi-view approach.", "link": "http://arxiv.org/abs/2510.10341v4", "date": "2025-12-02", "relevancy": 2.5491, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5123}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5123}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-View%20Graph%20Learning%20with%20Graph-Tuple&body=Title%3A%20Multi-View%20Graph%20Learning%20with%20Graph-Tuple%0AAuthor%3A%20Shiyu%20Chen%20and%20Ningyuan%20Huang%20and%20Soledad%20Villar%0AAbstract%3A%20Graph%20Neural%20Networks%20%28GNNs%29%20typically%20scale%20with%20the%20number%20of%20graph%20edges%2C%20making%20them%20well%20suited%20for%20sparse%20graphs%20but%20less%20efficient%20on%20dense%20graphs%2C%20such%20as%20point%20clouds%20or%20molecular%20interactions.%20A%20common%20remedy%20is%20to%20sparsify%20the%20graph%20via%20similarity%20thresholding%20or%20distance%20pruning%2C%20but%20this%20forces%20an%20arbitrary%20choice%20of%20a%20single%20interaction%20scale%20and%20discards%20crucial%20information%20from%20other%20scales.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20a%20multi-view%20graph-tuple%20framework.%20Instead%20of%20a%20single%20graph%2C%20our%20graph-tuple%20framework%20partitions%20the%20graph%20into%20disjoint%20subgraphs%2C%20capturing%20primary%20local%20interactions%20and%20weaker%2C%20long-range%20connections.%20We%20then%20learn%20multi-view%20representations%20from%20the%20graph-tuple%20via%20a%20heterogeneous%20message-passing%20architecture%20inspired%20by%20the%20theory%20of%20non-commuting%20operators%2C%20which%20we%20formally%20prove%20is%20strictly%20more%20expressive%20and%20guarantees%20a%20lower%20oracle%20risk%20compared%20to%20single-graph%20message-passing%20models.%20We%20instantiate%20our%20framework%20on%20two%20scientific%20domains%3A%20molecular%20property%20prediction%20from%20feature-scarce%20Coulomb%20matrices%20and%20cosmological%20parameter%20inference%20from%20geometric%20point%20clouds.%20On%20both%20applications%2C%20our%20multi-view%20graph-tuple%20models%20demonstrate%20better%20performance%20than%20single-graph%20baselines%2C%20highlighting%20the%20power%20and%20versatility%20of%20our%20multi-view%20approach.%0ALink%3A%20http%3A//arxiv.org/abs/2510.10341v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-View%2520Graph%2520Learning%2520with%2520Graph-Tuple%26entry.906535625%3DShiyu%2520Chen%2520and%2520Ningyuan%2520Huang%2520and%2520Soledad%2520Villar%26entry.1292438233%3DGraph%2520Neural%2520Networks%2520%2528GNNs%2529%2520typically%2520scale%2520with%2520the%2520number%2520of%2520graph%2520edges%252C%2520making%2520them%2520well%2520suited%2520for%2520sparse%2520graphs%2520but%2520less%2520efficient%2520on%2520dense%2520graphs%252C%2520such%2520as%2520point%2520clouds%2520or%2520molecular%2520interactions.%2520A%2520common%2520remedy%2520is%2520to%2520sparsify%2520the%2520graph%2520via%2520similarity%2520thresholding%2520or%2520distance%2520pruning%252C%2520but%2520this%2520forces%2520an%2520arbitrary%2520choice%2520of%2520a%2520single%2520interaction%2520scale%2520and%2520discards%2520crucial%2520information%2520from%2520other%2520scales.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520introduce%2520a%2520multi-view%2520graph-tuple%2520framework.%2520Instead%2520of%2520a%2520single%2520graph%252C%2520our%2520graph-tuple%2520framework%2520partitions%2520the%2520graph%2520into%2520disjoint%2520subgraphs%252C%2520capturing%2520primary%2520local%2520interactions%2520and%2520weaker%252C%2520long-range%2520connections.%2520We%2520then%2520learn%2520multi-view%2520representations%2520from%2520the%2520graph-tuple%2520via%2520a%2520heterogeneous%2520message-passing%2520architecture%2520inspired%2520by%2520the%2520theory%2520of%2520non-commuting%2520operators%252C%2520which%2520we%2520formally%2520prove%2520is%2520strictly%2520more%2520expressive%2520and%2520guarantees%2520a%2520lower%2520oracle%2520risk%2520compared%2520to%2520single-graph%2520message-passing%2520models.%2520We%2520instantiate%2520our%2520framework%2520on%2520two%2520scientific%2520domains%253A%2520molecular%2520property%2520prediction%2520from%2520feature-scarce%2520Coulomb%2520matrices%2520and%2520cosmological%2520parameter%2520inference%2520from%2520geometric%2520point%2520clouds.%2520On%2520both%2520applications%252C%2520our%2520multi-view%2520graph-tuple%2520models%2520demonstrate%2520better%2520performance%2520than%2520single-graph%2520baselines%252C%2520highlighting%2520the%2520power%2520and%2520versatility%2520of%2520our%2520multi-view%2520approach.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.10341v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-View%20Graph%20Learning%20with%20Graph-Tuple&entry.906535625=Shiyu%20Chen%20and%20Ningyuan%20Huang%20and%20Soledad%20Villar&entry.1292438233=Graph%20Neural%20Networks%20%28GNNs%29%20typically%20scale%20with%20the%20number%20of%20graph%20edges%2C%20making%20them%20well%20suited%20for%20sparse%20graphs%20but%20less%20efficient%20on%20dense%20graphs%2C%20such%20as%20point%20clouds%20or%20molecular%20interactions.%20A%20common%20remedy%20is%20to%20sparsify%20the%20graph%20via%20similarity%20thresholding%20or%20distance%20pruning%2C%20but%20this%20forces%20an%20arbitrary%20choice%20of%20a%20single%20interaction%20scale%20and%20discards%20crucial%20information%20from%20other%20scales.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20a%20multi-view%20graph-tuple%20framework.%20Instead%20of%20a%20single%20graph%2C%20our%20graph-tuple%20framework%20partitions%20the%20graph%20into%20disjoint%20subgraphs%2C%20capturing%20primary%20local%20interactions%20and%20weaker%2C%20long-range%20connections.%20We%20then%20learn%20multi-view%20representations%20from%20the%20graph-tuple%20via%20a%20heterogeneous%20message-passing%20architecture%20inspired%20by%20the%20theory%20of%20non-commuting%20operators%2C%20which%20we%20formally%20prove%20is%20strictly%20more%20expressive%20and%20guarantees%20a%20lower%20oracle%20risk%20compared%20to%20single-graph%20message-passing%20models.%20We%20instantiate%20our%20framework%20on%20two%20scientific%20domains%3A%20molecular%20property%20prediction%20from%20feature-scarce%20Coulomb%20matrices%20and%20cosmological%20parameter%20inference%20from%20geometric%20point%20clouds.%20On%20both%20applications%2C%20our%20multi-view%20graph-tuple%20models%20demonstrate%20better%20performance%20than%20single-graph%20baselines%2C%20highlighting%20the%20power%20and%20versatility%20of%20our%20multi-view%20approach.&entry.1838667208=http%3A//arxiv.org/abs/2510.10341v4&entry.124074799=Read"},
{"title": "GeoMAE: Masking Representation Learning for Spatio-Temporal Graph Forecasting with Missing Values", "author": "Songyu Ke and Chenyu Wu and Yuxuan Liang and Huiling Qin and Junbo Zhang and Yu Zheng", "abstract": "The ubiquity of missing data in urban intelligence systems, attributable to adverse environmental conditions and equipment failures, poses a significant challenge to the efficacy of downstream applications, notably in the realms of traffic forecasting and energy consumption prediction.\n  Therefore, it is imperative to develop a robust spatio-temporal learning methodology capable of extracting meaningful insights from incomplete datasets. Despite the existence of methodologies for spatio-temporal graph forecasting in the presence of missing values, unresolved issues persist.\n  Primarily, the majority of extant research is predicated on time-series analysis, thereby neglecting the dynamic spatial correlations inherent in sensor networks.\n  Additionally, the complexity of missing data patterns compounds the intricacy of the problem.\n  Furthermore, the variability in maintenance conditions results in a significant fluctuation in the ratio and pattern of missing values, thereby challenging the generalizability of predictive models.\n  In response to these challenges, this study introduces GeoMAE, a self-supervised spatio-temporal representation learning model.\n  The model is comprised of three principal components: an input preprocessing module, an attention-based spatio-temporal forecasting network (STAFN), and an auxiliary learning task, which draws inspiration from Masking AutoEncoders to enhance the robustness of spatio-temporal representation learning.\n  Empirical evaluations on real-world datasets demonstrate that GeoMAE significantly outperforms existing benchmarks, achieving up to 13.20\\% relative improvement over the best baseline models.", "link": "http://arxiv.org/abs/2508.14083v2", "date": "2025-12-02", "relevancy": 2.5443, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5145}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.51}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoMAE%3A%20Masking%20Representation%20Learning%20for%20Spatio-Temporal%20Graph%20Forecasting%20with%20Missing%20Values&body=Title%3A%20GeoMAE%3A%20Masking%20Representation%20Learning%20for%20Spatio-Temporal%20Graph%20Forecasting%20with%20Missing%20Values%0AAuthor%3A%20Songyu%20Ke%20and%20Chenyu%20Wu%20and%20Yuxuan%20Liang%20and%20Huiling%20Qin%20and%20Junbo%20Zhang%20and%20Yu%20Zheng%0AAbstract%3A%20The%20ubiquity%20of%20missing%20data%20in%20urban%20intelligence%20systems%2C%20attributable%20to%20adverse%20environmental%20conditions%20and%20equipment%20failures%2C%20poses%20a%20significant%20challenge%20to%20the%20efficacy%20of%20downstream%20applications%2C%20notably%20in%20the%20realms%20of%20traffic%20forecasting%20and%20energy%20consumption%20prediction.%0A%20%20Therefore%2C%20it%20is%20imperative%20to%20develop%20a%20robust%20spatio-temporal%20learning%20methodology%20capable%20of%20extracting%20meaningful%20insights%20from%20incomplete%20datasets.%20Despite%20the%20existence%20of%20methodologies%20for%20spatio-temporal%20graph%20forecasting%20in%20the%20presence%20of%20missing%20values%2C%20unresolved%20issues%20persist.%0A%20%20Primarily%2C%20the%20majority%20of%20extant%20research%20is%20predicated%20on%20time-series%20analysis%2C%20thereby%20neglecting%20the%20dynamic%20spatial%20correlations%20inherent%20in%20sensor%20networks.%0A%20%20Additionally%2C%20the%20complexity%20of%20missing%20data%20patterns%20compounds%20the%20intricacy%20of%20the%20problem.%0A%20%20Furthermore%2C%20the%20variability%20in%20maintenance%20conditions%20results%20in%20a%20significant%20fluctuation%20in%20the%20ratio%20and%20pattern%20of%20missing%20values%2C%20thereby%20challenging%20the%20generalizability%20of%20predictive%20models.%0A%20%20In%20response%20to%20these%20challenges%2C%20this%20study%20introduces%20GeoMAE%2C%20a%20self-supervised%20spatio-temporal%20representation%20learning%20model.%0A%20%20The%20model%20is%20comprised%20of%20three%20principal%20components%3A%20an%20input%20preprocessing%20module%2C%20an%20attention-based%20spatio-temporal%20forecasting%20network%20%28STAFN%29%2C%20and%20an%20auxiliary%20learning%20task%2C%20which%20draws%20inspiration%20from%20Masking%20AutoEncoders%20to%20enhance%20the%20robustness%20of%20spatio-temporal%20representation%20learning.%0A%20%20Empirical%20evaluations%20on%20real-world%20datasets%20demonstrate%20that%20GeoMAE%20significantly%20outperforms%20existing%20benchmarks%2C%20achieving%20up%20to%2013.20%5C%25%20relative%20improvement%20over%20the%20best%20baseline%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2508.14083v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoMAE%253A%2520Masking%2520Representation%2520Learning%2520for%2520Spatio-Temporal%2520Graph%2520Forecasting%2520with%2520Missing%2520Values%26entry.906535625%3DSongyu%2520Ke%2520and%2520Chenyu%2520Wu%2520and%2520Yuxuan%2520Liang%2520and%2520Huiling%2520Qin%2520and%2520Junbo%2520Zhang%2520and%2520Yu%2520Zheng%26entry.1292438233%3DThe%2520ubiquity%2520of%2520missing%2520data%2520in%2520urban%2520intelligence%2520systems%252C%2520attributable%2520to%2520adverse%2520environmental%2520conditions%2520and%2520equipment%2520failures%252C%2520poses%2520a%2520significant%2520challenge%2520to%2520the%2520efficacy%2520of%2520downstream%2520applications%252C%2520notably%2520in%2520the%2520realms%2520of%2520traffic%2520forecasting%2520and%2520energy%2520consumption%2520prediction.%250A%2520%2520Therefore%252C%2520it%2520is%2520imperative%2520to%2520develop%2520a%2520robust%2520spatio-temporal%2520learning%2520methodology%2520capable%2520of%2520extracting%2520meaningful%2520insights%2520from%2520incomplete%2520datasets.%2520Despite%2520the%2520existence%2520of%2520methodologies%2520for%2520spatio-temporal%2520graph%2520forecasting%2520in%2520the%2520presence%2520of%2520missing%2520values%252C%2520unresolved%2520issues%2520persist.%250A%2520%2520Primarily%252C%2520the%2520majority%2520of%2520extant%2520research%2520is%2520predicated%2520on%2520time-series%2520analysis%252C%2520thereby%2520neglecting%2520the%2520dynamic%2520spatial%2520correlations%2520inherent%2520in%2520sensor%2520networks.%250A%2520%2520Additionally%252C%2520the%2520complexity%2520of%2520missing%2520data%2520patterns%2520compounds%2520the%2520intricacy%2520of%2520the%2520problem.%250A%2520%2520Furthermore%252C%2520the%2520variability%2520in%2520maintenance%2520conditions%2520results%2520in%2520a%2520significant%2520fluctuation%2520in%2520the%2520ratio%2520and%2520pattern%2520of%2520missing%2520values%252C%2520thereby%2520challenging%2520the%2520generalizability%2520of%2520predictive%2520models.%250A%2520%2520In%2520response%2520to%2520these%2520challenges%252C%2520this%2520study%2520introduces%2520GeoMAE%252C%2520a%2520self-supervised%2520spatio-temporal%2520representation%2520learning%2520model.%250A%2520%2520The%2520model%2520is%2520comprised%2520of%2520three%2520principal%2520components%253A%2520an%2520input%2520preprocessing%2520module%252C%2520an%2520attention-based%2520spatio-temporal%2520forecasting%2520network%2520%2528STAFN%2529%252C%2520and%2520an%2520auxiliary%2520learning%2520task%252C%2520which%2520draws%2520inspiration%2520from%2520Masking%2520AutoEncoders%2520to%2520enhance%2520the%2520robustness%2520of%2520spatio-temporal%2520representation%2520learning.%250A%2520%2520Empirical%2520evaluations%2520on%2520real-world%2520datasets%2520demonstrate%2520that%2520GeoMAE%2520significantly%2520outperforms%2520existing%2520benchmarks%252C%2520achieving%2520up%2520to%252013.20%255C%2525%2520relative%2520improvement%2520over%2520the%2520best%2520baseline%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.14083v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoMAE%3A%20Masking%20Representation%20Learning%20for%20Spatio-Temporal%20Graph%20Forecasting%20with%20Missing%20Values&entry.906535625=Songyu%20Ke%20and%20Chenyu%20Wu%20and%20Yuxuan%20Liang%20and%20Huiling%20Qin%20and%20Junbo%20Zhang%20and%20Yu%20Zheng&entry.1292438233=The%20ubiquity%20of%20missing%20data%20in%20urban%20intelligence%20systems%2C%20attributable%20to%20adverse%20environmental%20conditions%20and%20equipment%20failures%2C%20poses%20a%20significant%20challenge%20to%20the%20efficacy%20of%20downstream%20applications%2C%20notably%20in%20the%20realms%20of%20traffic%20forecasting%20and%20energy%20consumption%20prediction.%0A%20%20Therefore%2C%20it%20is%20imperative%20to%20develop%20a%20robust%20spatio-temporal%20learning%20methodology%20capable%20of%20extracting%20meaningful%20insights%20from%20incomplete%20datasets.%20Despite%20the%20existence%20of%20methodologies%20for%20spatio-temporal%20graph%20forecasting%20in%20the%20presence%20of%20missing%20values%2C%20unresolved%20issues%20persist.%0A%20%20Primarily%2C%20the%20majority%20of%20extant%20research%20is%20predicated%20on%20time-series%20analysis%2C%20thereby%20neglecting%20the%20dynamic%20spatial%20correlations%20inherent%20in%20sensor%20networks.%0A%20%20Additionally%2C%20the%20complexity%20of%20missing%20data%20patterns%20compounds%20the%20intricacy%20of%20the%20problem.%0A%20%20Furthermore%2C%20the%20variability%20in%20maintenance%20conditions%20results%20in%20a%20significant%20fluctuation%20in%20the%20ratio%20and%20pattern%20of%20missing%20values%2C%20thereby%20challenging%20the%20generalizability%20of%20predictive%20models.%0A%20%20In%20response%20to%20these%20challenges%2C%20this%20study%20introduces%20GeoMAE%2C%20a%20self-supervised%20spatio-temporal%20representation%20learning%20model.%0A%20%20The%20model%20is%20comprised%20of%20three%20principal%20components%3A%20an%20input%20preprocessing%20module%2C%20an%20attention-based%20spatio-temporal%20forecasting%20network%20%28STAFN%29%2C%20and%20an%20auxiliary%20learning%20task%2C%20which%20draws%20inspiration%20from%20Masking%20AutoEncoders%20to%20enhance%20the%20robustness%20of%20spatio-temporal%20representation%20learning.%0A%20%20Empirical%20evaluations%20on%20real-world%20datasets%20demonstrate%20that%20GeoMAE%20significantly%20outperforms%20existing%20benchmarks%2C%20achieving%20up%20to%2013.20%5C%25%20relative%20improvement%20over%20the%20best%20baseline%20models.&entry.1838667208=http%3A//arxiv.org/abs/2508.14083v2&entry.124074799=Read"},
{"title": "Computational Copyright: Towards A Royalty Model for Music Generative AI", "author": "Junwei Deng and Xirui Jiang and Shiyuan Zhang and Shichang Zhang and Himabindu Lakkaraju and Ruijiang Gao and Chris Donahue and Jiaqi W. Ma", "abstract": "The rapid rise of generative AI has intensified copyright and economic tensions in creative industries, particularly in music. Current approaches addressing this challenge often focus on preventing infringement or establishing one-time licensing, which fail to provide the sustainable, recurring economic incentives necessary to maintain creative ecosystems. To address this gap, we propose Generative Content ID, a framework for scalable and faithful royalty attribution in music generative AI. Adapting the idea of YouTube's Content ID, it attributes the value of AI-generated music back to the specific training content that causally influenced its generation, a process we term as causal attribution. However, naively quantifying the causal influence requires counterfactually retraining the model on subsets of training data, which is infeasible. We address this challenge using efficient Training Data Attribution (TDA) methods to approximate causal attribution at scale.\n  We further conduct empirical analysis of the framework on public and proprietary datasets. First, we demonstrate that the scalable TDA methods provide a faithful approximation of the \"gold-standard\" but costly retraining-based causal attribution, showing the feasibility of the proposed royalty framework. Second, we investigate the relationship between the perceived similarity employed by legal practices and our causal attribution reflecting the true AI training mechanics. We find that while perceived similarity can capture the most influential samples, it fails to account for the broader data contribution that drives model utility, suggesting similarity-based legal proxies are ill-suited for royalty distribution.\n  Overall, this work provides a principled and operational foundation for royalty-based economic governance of music generative AI.", "link": "http://arxiv.org/abs/2312.06646v5", "date": "2025-12-02", "relevancy": 2.5026, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5178}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5057}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Computational%20Copyright%3A%20Towards%20A%20Royalty%20Model%20for%20Music%20Generative%20AI&body=Title%3A%20Computational%20Copyright%3A%20Towards%20A%20Royalty%20Model%20for%20Music%20Generative%20AI%0AAuthor%3A%20Junwei%20Deng%20and%20Xirui%20Jiang%20and%20Shiyuan%20Zhang%20and%20Shichang%20Zhang%20and%20Himabindu%20Lakkaraju%20and%20Ruijiang%20Gao%20and%20Chris%20Donahue%20and%20Jiaqi%20W.%20Ma%0AAbstract%3A%20The%20rapid%20rise%20of%20generative%20AI%20has%20intensified%20copyright%20and%20economic%20tensions%20in%20creative%20industries%2C%20particularly%20in%20music.%20Current%20approaches%20addressing%20this%20challenge%20often%20focus%20on%20preventing%20infringement%20or%20establishing%20one-time%20licensing%2C%20which%20fail%20to%20provide%20the%20sustainable%2C%20recurring%20economic%20incentives%20necessary%20to%20maintain%20creative%20ecosystems.%20To%20address%20this%20gap%2C%20we%20propose%20Generative%20Content%20ID%2C%20a%20framework%20for%20scalable%20and%20faithful%20royalty%20attribution%20in%20music%20generative%20AI.%20Adapting%20the%20idea%20of%20YouTube%27s%20Content%20ID%2C%20it%20attributes%20the%20value%20of%20AI-generated%20music%20back%20to%20the%20specific%20training%20content%20that%20causally%20influenced%20its%20generation%2C%20a%20process%20we%20term%20as%20causal%20attribution.%20However%2C%20naively%20quantifying%20the%20causal%20influence%20requires%20counterfactually%20retraining%20the%20model%20on%20subsets%20of%20training%20data%2C%20which%20is%20infeasible.%20We%20address%20this%20challenge%20using%20efficient%20Training%20Data%20Attribution%20%28TDA%29%20methods%20to%20approximate%20causal%20attribution%20at%20scale.%0A%20%20We%20further%20conduct%20empirical%20analysis%20of%20the%20framework%20on%20public%20and%20proprietary%20datasets.%20First%2C%20we%20demonstrate%20that%20the%20scalable%20TDA%20methods%20provide%20a%20faithful%20approximation%20of%20the%20%22gold-standard%22%20but%20costly%20retraining-based%20causal%20attribution%2C%20showing%20the%20feasibility%20of%20the%20proposed%20royalty%20framework.%20Second%2C%20we%20investigate%20the%20relationship%20between%20the%20perceived%20similarity%20employed%20by%20legal%20practices%20and%20our%20causal%20attribution%20reflecting%20the%20true%20AI%20training%20mechanics.%20We%20find%20that%20while%20perceived%20similarity%20can%20capture%20the%20most%20influential%20samples%2C%20it%20fails%20to%20account%20for%20the%20broader%20data%20contribution%20that%20drives%20model%20utility%2C%20suggesting%20similarity-based%20legal%20proxies%20are%20ill-suited%20for%20royalty%20distribution.%0A%20%20Overall%2C%20this%20work%20provides%20a%20principled%20and%20operational%20foundation%20for%20royalty-based%20economic%20governance%20of%20music%20generative%20AI.%0ALink%3A%20http%3A//arxiv.org/abs/2312.06646v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComputational%2520Copyright%253A%2520Towards%2520A%2520Royalty%2520Model%2520for%2520Music%2520Generative%2520AI%26entry.906535625%3DJunwei%2520Deng%2520and%2520Xirui%2520Jiang%2520and%2520Shiyuan%2520Zhang%2520and%2520Shichang%2520Zhang%2520and%2520Himabindu%2520Lakkaraju%2520and%2520Ruijiang%2520Gao%2520and%2520Chris%2520Donahue%2520and%2520Jiaqi%2520W.%2520Ma%26entry.1292438233%3DThe%2520rapid%2520rise%2520of%2520generative%2520AI%2520has%2520intensified%2520copyright%2520and%2520economic%2520tensions%2520in%2520creative%2520industries%252C%2520particularly%2520in%2520music.%2520Current%2520approaches%2520addressing%2520this%2520challenge%2520often%2520focus%2520on%2520preventing%2520infringement%2520or%2520establishing%2520one-time%2520licensing%252C%2520which%2520fail%2520to%2520provide%2520the%2520sustainable%252C%2520recurring%2520economic%2520incentives%2520necessary%2520to%2520maintain%2520creative%2520ecosystems.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520Generative%2520Content%2520ID%252C%2520a%2520framework%2520for%2520scalable%2520and%2520faithful%2520royalty%2520attribution%2520in%2520music%2520generative%2520AI.%2520Adapting%2520the%2520idea%2520of%2520YouTube%2527s%2520Content%2520ID%252C%2520it%2520attributes%2520the%2520value%2520of%2520AI-generated%2520music%2520back%2520to%2520the%2520specific%2520training%2520content%2520that%2520causally%2520influenced%2520its%2520generation%252C%2520a%2520process%2520we%2520term%2520as%2520causal%2520attribution.%2520However%252C%2520naively%2520quantifying%2520the%2520causal%2520influence%2520requires%2520counterfactually%2520retraining%2520the%2520model%2520on%2520subsets%2520of%2520training%2520data%252C%2520which%2520is%2520infeasible.%2520We%2520address%2520this%2520challenge%2520using%2520efficient%2520Training%2520Data%2520Attribution%2520%2528TDA%2529%2520methods%2520to%2520approximate%2520causal%2520attribution%2520at%2520scale.%250A%2520%2520We%2520further%2520conduct%2520empirical%2520analysis%2520of%2520the%2520framework%2520on%2520public%2520and%2520proprietary%2520datasets.%2520First%252C%2520we%2520demonstrate%2520that%2520the%2520scalable%2520TDA%2520methods%2520provide%2520a%2520faithful%2520approximation%2520of%2520the%2520%2522gold-standard%2522%2520but%2520costly%2520retraining-based%2520causal%2520attribution%252C%2520showing%2520the%2520feasibility%2520of%2520the%2520proposed%2520royalty%2520framework.%2520Second%252C%2520we%2520investigate%2520the%2520relationship%2520between%2520the%2520perceived%2520similarity%2520employed%2520by%2520legal%2520practices%2520and%2520our%2520causal%2520attribution%2520reflecting%2520the%2520true%2520AI%2520training%2520mechanics.%2520We%2520find%2520that%2520while%2520perceived%2520similarity%2520can%2520capture%2520the%2520most%2520influential%2520samples%252C%2520it%2520fails%2520to%2520account%2520for%2520the%2520broader%2520data%2520contribution%2520that%2520drives%2520model%2520utility%252C%2520suggesting%2520similarity-based%2520legal%2520proxies%2520are%2520ill-suited%2520for%2520royalty%2520distribution.%250A%2520%2520Overall%252C%2520this%2520work%2520provides%2520a%2520principled%2520and%2520operational%2520foundation%2520for%2520royalty-based%2520economic%2520governance%2520of%2520music%2520generative%2520AI.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.06646v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Computational%20Copyright%3A%20Towards%20A%20Royalty%20Model%20for%20Music%20Generative%20AI&entry.906535625=Junwei%20Deng%20and%20Xirui%20Jiang%20and%20Shiyuan%20Zhang%20and%20Shichang%20Zhang%20and%20Himabindu%20Lakkaraju%20and%20Ruijiang%20Gao%20and%20Chris%20Donahue%20and%20Jiaqi%20W.%20Ma&entry.1292438233=The%20rapid%20rise%20of%20generative%20AI%20has%20intensified%20copyright%20and%20economic%20tensions%20in%20creative%20industries%2C%20particularly%20in%20music.%20Current%20approaches%20addressing%20this%20challenge%20often%20focus%20on%20preventing%20infringement%20or%20establishing%20one-time%20licensing%2C%20which%20fail%20to%20provide%20the%20sustainable%2C%20recurring%20economic%20incentives%20necessary%20to%20maintain%20creative%20ecosystems.%20To%20address%20this%20gap%2C%20we%20propose%20Generative%20Content%20ID%2C%20a%20framework%20for%20scalable%20and%20faithful%20royalty%20attribution%20in%20music%20generative%20AI.%20Adapting%20the%20idea%20of%20YouTube%27s%20Content%20ID%2C%20it%20attributes%20the%20value%20of%20AI-generated%20music%20back%20to%20the%20specific%20training%20content%20that%20causally%20influenced%20its%20generation%2C%20a%20process%20we%20term%20as%20causal%20attribution.%20However%2C%20naively%20quantifying%20the%20causal%20influence%20requires%20counterfactually%20retraining%20the%20model%20on%20subsets%20of%20training%20data%2C%20which%20is%20infeasible.%20We%20address%20this%20challenge%20using%20efficient%20Training%20Data%20Attribution%20%28TDA%29%20methods%20to%20approximate%20causal%20attribution%20at%20scale.%0A%20%20We%20further%20conduct%20empirical%20analysis%20of%20the%20framework%20on%20public%20and%20proprietary%20datasets.%20First%2C%20we%20demonstrate%20that%20the%20scalable%20TDA%20methods%20provide%20a%20faithful%20approximation%20of%20the%20%22gold-standard%22%20but%20costly%20retraining-based%20causal%20attribution%2C%20showing%20the%20feasibility%20of%20the%20proposed%20royalty%20framework.%20Second%2C%20we%20investigate%20the%20relationship%20between%20the%20perceived%20similarity%20employed%20by%20legal%20practices%20and%20our%20causal%20attribution%20reflecting%20the%20true%20AI%20training%20mechanics.%20We%20find%20that%20while%20perceived%20similarity%20can%20capture%20the%20most%20influential%20samples%2C%20it%20fails%20to%20account%20for%20the%20broader%20data%20contribution%20that%20drives%20model%20utility%2C%20suggesting%20similarity-based%20legal%20proxies%20are%20ill-suited%20for%20royalty%20distribution.%0A%20%20Overall%2C%20this%20work%20provides%20a%20principled%20and%20operational%20foundation%20for%20royalty-based%20economic%20governance%20of%20music%20generative%20AI.&entry.1838667208=http%3A//arxiv.org/abs/2312.06646v5&entry.124074799=Read"},
{"title": "PrITTI: Primitive-based Generation of Controllable and Editable 3D Semantic Urban Scenes", "author": "Christina Ourania Tze and Daniel Dauner and Yiyi Liao and Dzmitry Tsishkou and Andreas Geiger", "abstract": "Existing approaches to 3D semantic urban scene generation predominantly rely on voxel-based representations, which are bound by fixed resolution, challenging to edit, and memory-intensive in their dense form. In contrast, we advocate for a primitive-based paradigm where urban scenes are represented using compact, semantically meaningful 3D elements that are easy to manipulate and compose. To this end, we introduce PrITTI, a latent diffusion model that leverages vectorized object primitives and rasterized ground surfaces for generating diverse, controllable, and editable 3D semantic urban scenes. This hybrid representation yields a structured latent space that facilitates object- and ground-level manipulation. Experiments on KITTI-360 show that primitive-based representations unlock the full capabilities of diffusion transformers, achieving state-of-the-art 3D scene generation quality with lower memory requirements, faster inference, and greater editability than voxel-based methods. Beyond generation, PrITTI supports a range of downstream applications, including scene editing, inpainting, outpainting, and photo-realistic street-view synthesis. Code and models are publicly available at $\\href{https://raniatze.github.io/pritti/}{https://raniatze.github.io/pritti}$.", "link": "http://arxiv.org/abs/2506.19117v2", "date": "2025-12-02", "relevancy": 2.4947, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6258}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6258}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PrITTI%3A%20Primitive-based%20Generation%20of%20Controllable%20and%20Editable%203D%20Semantic%20Urban%20Scenes&body=Title%3A%20PrITTI%3A%20Primitive-based%20Generation%20of%20Controllable%20and%20Editable%203D%20Semantic%20Urban%20Scenes%0AAuthor%3A%20Christina%20Ourania%20Tze%20and%20Daniel%20Dauner%20and%20Yiyi%20Liao%20and%20Dzmitry%20Tsishkou%20and%20Andreas%20Geiger%0AAbstract%3A%20Existing%20approaches%20to%203D%20semantic%20urban%20scene%20generation%20predominantly%20rely%20on%20voxel-based%20representations%2C%20which%20are%20bound%20by%20fixed%20resolution%2C%20challenging%20to%20edit%2C%20and%20memory-intensive%20in%20their%20dense%20form.%20In%20contrast%2C%20we%20advocate%20for%20a%20primitive-based%20paradigm%20where%20urban%20scenes%20are%20represented%20using%20compact%2C%20semantically%20meaningful%203D%20elements%20that%20are%20easy%20to%20manipulate%20and%20compose.%20To%20this%20end%2C%20we%20introduce%20PrITTI%2C%20a%20latent%20diffusion%20model%20that%20leverages%20vectorized%20object%20primitives%20and%20rasterized%20ground%20surfaces%20for%20generating%20diverse%2C%20controllable%2C%20and%20editable%203D%20semantic%20urban%20scenes.%20This%20hybrid%20representation%20yields%20a%20structured%20latent%20space%20that%20facilitates%20object-%20and%20ground-level%20manipulation.%20Experiments%20on%20KITTI-360%20show%20that%20primitive-based%20representations%20unlock%20the%20full%20capabilities%20of%20diffusion%20transformers%2C%20achieving%20state-of-the-art%203D%20scene%20generation%20quality%20with%20lower%20memory%20requirements%2C%20faster%20inference%2C%20and%20greater%20editability%20than%20voxel-based%20methods.%20Beyond%20generation%2C%20PrITTI%20supports%20a%20range%20of%20downstream%20applications%2C%20including%20scene%20editing%2C%20inpainting%2C%20outpainting%2C%20and%20photo-realistic%20street-view%20synthesis.%20Code%20and%20models%20are%20publicly%20available%20at%20%24%5Chref%7Bhttps%3A//raniatze.github.io/pritti/%7D%7Bhttps%3A//raniatze.github.io/pritti%7D%24.%0ALink%3A%20http%3A//arxiv.org/abs/2506.19117v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrITTI%253A%2520Primitive-based%2520Generation%2520of%2520Controllable%2520and%2520Editable%25203D%2520Semantic%2520Urban%2520Scenes%26entry.906535625%3DChristina%2520Ourania%2520Tze%2520and%2520Daniel%2520Dauner%2520and%2520Yiyi%2520Liao%2520and%2520Dzmitry%2520Tsishkou%2520and%2520Andreas%2520Geiger%26entry.1292438233%3DExisting%2520approaches%2520to%25203D%2520semantic%2520urban%2520scene%2520generation%2520predominantly%2520rely%2520on%2520voxel-based%2520representations%252C%2520which%2520are%2520bound%2520by%2520fixed%2520resolution%252C%2520challenging%2520to%2520edit%252C%2520and%2520memory-intensive%2520in%2520their%2520dense%2520form.%2520In%2520contrast%252C%2520we%2520advocate%2520for%2520a%2520primitive-based%2520paradigm%2520where%2520urban%2520scenes%2520are%2520represented%2520using%2520compact%252C%2520semantically%2520meaningful%25203D%2520elements%2520that%2520are%2520easy%2520to%2520manipulate%2520and%2520compose.%2520To%2520this%2520end%252C%2520we%2520introduce%2520PrITTI%252C%2520a%2520latent%2520diffusion%2520model%2520that%2520leverages%2520vectorized%2520object%2520primitives%2520and%2520rasterized%2520ground%2520surfaces%2520for%2520generating%2520diverse%252C%2520controllable%252C%2520and%2520editable%25203D%2520semantic%2520urban%2520scenes.%2520This%2520hybrid%2520representation%2520yields%2520a%2520structured%2520latent%2520space%2520that%2520facilitates%2520object-%2520and%2520ground-level%2520manipulation.%2520Experiments%2520on%2520KITTI-360%2520show%2520that%2520primitive-based%2520representations%2520unlock%2520the%2520full%2520capabilities%2520of%2520diffusion%2520transformers%252C%2520achieving%2520state-of-the-art%25203D%2520scene%2520generation%2520quality%2520with%2520lower%2520memory%2520requirements%252C%2520faster%2520inference%252C%2520and%2520greater%2520editability%2520than%2520voxel-based%2520methods.%2520Beyond%2520generation%252C%2520PrITTI%2520supports%2520a%2520range%2520of%2520downstream%2520applications%252C%2520including%2520scene%2520editing%252C%2520inpainting%252C%2520outpainting%252C%2520and%2520photo-realistic%2520street-view%2520synthesis.%2520Code%2520and%2520models%2520are%2520publicly%2520available%2520at%2520%2524%255Chref%257Bhttps%253A//raniatze.github.io/pritti/%257D%257Bhttps%253A//raniatze.github.io/pritti%257D%2524.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.19117v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PrITTI%3A%20Primitive-based%20Generation%20of%20Controllable%20and%20Editable%203D%20Semantic%20Urban%20Scenes&entry.906535625=Christina%20Ourania%20Tze%20and%20Daniel%20Dauner%20and%20Yiyi%20Liao%20and%20Dzmitry%20Tsishkou%20and%20Andreas%20Geiger&entry.1292438233=Existing%20approaches%20to%203D%20semantic%20urban%20scene%20generation%20predominantly%20rely%20on%20voxel-based%20representations%2C%20which%20are%20bound%20by%20fixed%20resolution%2C%20challenging%20to%20edit%2C%20and%20memory-intensive%20in%20their%20dense%20form.%20In%20contrast%2C%20we%20advocate%20for%20a%20primitive-based%20paradigm%20where%20urban%20scenes%20are%20represented%20using%20compact%2C%20semantically%20meaningful%203D%20elements%20that%20are%20easy%20to%20manipulate%20and%20compose.%20To%20this%20end%2C%20we%20introduce%20PrITTI%2C%20a%20latent%20diffusion%20model%20that%20leverages%20vectorized%20object%20primitives%20and%20rasterized%20ground%20surfaces%20for%20generating%20diverse%2C%20controllable%2C%20and%20editable%203D%20semantic%20urban%20scenes.%20This%20hybrid%20representation%20yields%20a%20structured%20latent%20space%20that%20facilitates%20object-%20and%20ground-level%20manipulation.%20Experiments%20on%20KITTI-360%20show%20that%20primitive-based%20representations%20unlock%20the%20full%20capabilities%20of%20diffusion%20transformers%2C%20achieving%20state-of-the-art%203D%20scene%20generation%20quality%20with%20lower%20memory%20requirements%2C%20faster%20inference%2C%20and%20greater%20editability%20than%20voxel-based%20methods.%20Beyond%20generation%2C%20PrITTI%20supports%20a%20range%20of%20downstream%20applications%2C%20including%20scene%20editing%2C%20inpainting%2C%20outpainting%2C%20and%20photo-realistic%20street-view%20synthesis.%20Code%20and%20models%20are%20publicly%20available%20at%20%24%5Chref%7Bhttps%3A//raniatze.github.io/pritti/%7D%7Bhttps%3A//raniatze.github.io/pritti%7D%24.&entry.1838667208=http%3A//arxiv.org/abs/2506.19117v2&entry.124074799=Read"},
{"title": "ProteinPNet: Prototypical Part Networks for Concept Learning in Spatial Proteomics", "author": "Louis McConnell and Jieran Sun and Theo Maffei and Raphael Gottardo and Marianna Rapsomaniki", "abstract": "Understanding the spatial architecture of the tumor microenvironment (TME) is critical to advance precision oncology. We present ProteinPNet, a novel framework based on prototypical part networks that discovers TME motifs from spatial proteomics data. Unlike traditional post-hoc explanability models, ProteinPNet directly learns discriminative, interpretable, faithful spatial prototypes through supervised training. We validate our approach on synthetic datasets with ground truth motifs, and further test it on a real-world lung cancer spatial proteomics dataset. ProteinPNet consistently identifies biologically meaningful prototypes aligned with different tumor subtypes. Through graphical and morphological analyses, we show that these prototypes capture interpretable features pointing to differences in immune infiltration and tissue modularity. Our results highlight the potential of prototype-based learning to reveal interpretable spatial biomarkers within the TME, with implications for mechanistic discovery in spatial omics.", "link": "http://arxiv.org/abs/2512.02983v1", "date": "2025-12-02", "relevancy": 2.493, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5048}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5041}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProteinPNet%3A%20Prototypical%20Part%20Networks%20for%20Concept%20Learning%20in%20Spatial%20Proteomics&body=Title%3A%20ProteinPNet%3A%20Prototypical%20Part%20Networks%20for%20Concept%20Learning%20in%20Spatial%20Proteomics%0AAuthor%3A%20Louis%20McConnell%20and%20Jieran%20Sun%20and%20Theo%20Maffei%20and%20Raphael%20Gottardo%20and%20Marianna%20Rapsomaniki%0AAbstract%3A%20Understanding%20the%20spatial%20architecture%20of%20the%20tumor%20microenvironment%20%28TME%29%20is%20critical%20to%20advance%20precision%20oncology.%20We%20present%20ProteinPNet%2C%20a%20novel%20framework%20based%20on%20prototypical%20part%20networks%20that%20discovers%20TME%20motifs%20from%20spatial%20proteomics%20data.%20Unlike%20traditional%20post-hoc%20explanability%20models%2C%20ProteinPNet%20directly%20learns%20discriminative%2C%20interpretable%2C%20faithful%20spatial%20prototypes%20through%20supervised%20training.%20We%20validate%20our%20approach%20on%20synthetic%20datasets%20with%20ground%20truth%20motifs%2C%20and%20further%20test%20it%20on%20a%20real-world%20lung%20cancer%20spatial%20proteomics%20dataset.%20ProteinPNet%20consistently%20identifies%20biologically%20meaningful%20prototypes%20aligned%20with%20different%20tumor%20subtypes.%20Through%20graphical%20and%20morphological%20analyses%2C%20we%20show%20that%20these%20prototypes%20capture%20interpretable%20features%20pointing%20to%20differences%20in%20immune%20infiltration%20and%20tissue%20modularity.%20Our%20results%20highlight%20the%20potential%20of%20prototype-based%20learning%20to%20reveal%20interpretable%20spatial%20biomarkers%20within%20the%20TME%2C%20with%20implications%20for%20mechanistic%20discovery%20in%20spatial%20omics.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02983v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProteinPNet%253A%2520Prototypical%2520Part%2520Networks%2520for%2520Concept%2520Learning%2520in%2520Spatial%2520Proteomics%26entry.906535625%3DLouis%2520McConnell%2520and%2520Jieran%2520Sun%2520and%2520Theo%2520Maffei%2520and%2520Raphael%2520Gottardo%2520and%2520Marianna%2520Rapsomaniki%26entry.1292438233%3DUnderstanding%2520the%2520spatial%2520architecture%2520of%2520the%2520tumor%2520microenvironment%2520%2528TME%2529%2520is%2520critical%2520to%2520advance%2520precision%2520oncology.%2520We%2520present%2520ProteinPNet%252C%2520a%2520novel%2520framework%2520based%2520on%2520prototypical%2520part%2520networks%2520that%2520discovers%2520TME%2520motifs%2520from%2520spatial%2520proteomics%2520data.%2520Unlike%2520traditional%2520post-hoc%2520explanability%2520models%252C%2520ProteinPNet%2520directly%2520learns%2520discriminative%252C%2520interpretable%252C%2520faithful%2520spatial%2520prototypes%2520through%2520supervised%2520training.%2520We%2520validate%2520our%2520approach%2520on%2520synthetic%2520datasets%2520with%2520ground%2520truth%2520motifs%252C%2520and%2520further%2520test%2520it%2520on%2520a%2520real-world%2520lung%2520cancer%2520spatial%2520proteomics%2520dataset.%2520ProteinPNet%2520consistently%2520identifies%2520biologically%2520meaningful%2520prototypes%2520aligned%2520with%2520different%2520tumor%2520subtypes.%2520Through%2520graphical%2520and%2520morphological%2520analyses%252C%2520we%2520show%2520that%2520these%2520prototypes%2520capture%2520interpretable%2520features%2520pointing%2520to%2520differences%2520in%2520immune%2520infiltration%2520and%2520tissue%2520modularity.%2520Our%2520results%2520highlight%2520the%2520potential%2520of%2520prototype-based%2520learning%2520to%2520reveal%2520interpretable%2520spatial%2520biomarkers%2520within%2520the%2520TME%252C%2520with%2520implications%2520for%2520mechanistic%2520discovery%2520in%2520spatial%2520omics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02983v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProteinPNet%3A%20Prototypical%20Part%20Networks%20for%20Concept%20Learning%20in%20Spatial%20Proteomics&entry.906535625=Louis%20McConnell%20and%20Jieran%20Sun%20and%20Theo%20Maffei%20and%20Raphael%20Gottardo%20and%20Marianna%20Rapsomaniki&entry.1292438233=Understanding%20the%20spatial%20architecture%20of%20the%20tumor%20microenvironment%20%28TME%29%20is%20critical%20to%20advance%20precision%20oncology.%20We%20present%20ProteinPNet%2C%20a%20novel%20framework%20based%20on%20prototypical%20part%20networks%20that%20discovers%20TME%20motifs%20from%20spatial%20proteomics%20data.%20Unlike%20traditional%20post-hoc%20explanability%20models%2C%20ProteinPNet%20directly%20learns%20discriminative%2C%20interpretable%2C%20faithful%20spatial%20prototypes%20through%20supervised%20training.%20We%20validate%20our%20approach%20on%20synthetic%20datasets%20with%20ground%20truth%20motifs%2C%20and%20further%20test%20it%20on%20a%20real-world%20lung%20cancer%20spatial%20proteomics%20dataset.%20ProteinPNet%20consistently%20identifies%20biologically%20meaningful%20prototypes%20aligned%20with%20different%20tumor%20subtypes.%20Through%20graphical%20and%20morphological%20analyses%2C%20we%20show%20that%20these%20prototypes%20capture%20interpretable%20features%20pointing%20to%20differences%20in%20immune%20infiltration%20and%20tissue%20modularity.%20Our%20results%20highlight%20the%20potential%20of%20prototype-based%20learning%20to%20reveal%20interpretable%20spatial%20biomarkers%20within%20the%20TME%2C%20with%20implications%20for%20mechanistic%20discovery%20in%20spatial%20omics.&entry.1838667208=http%3A//arxiv.org/abs/2512.02983v1&entry.124074799=Read"},
{"title": "OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models", "author": "Ryoto Miyamoto and Xin Fan and Fuyuko Kido and Tsuneo Matsumoto and Hayato Yamana", "abstract": "OpenLVLM-MIA is a new benchmark that highlights fundamental challenges in evaluating membership inference attacks (MIA) against large vision-language models (LVLMs). While prior work has reported high attack success rates, our analysis suggests that these results often arise from detecting distributional bias introduced during dataset construction rather than from identifying true membership status. To address this issue, we introduce a controlled benchmark of 6{,}000 images where the distributions of member and non-member samples are carefully balanced, and ground-truth membership labels are provided across three distinct training stages. Experiments using OpenLVLM-MIA demonstrated that the performance of state-of-the-art MIA methods approached chance-level. OpenLVLM-MIA, designed to be transparent and unbiased benchmark, clarifies certain limitations of MIA research on LVLMs and provides a solid foundation for developing stronger privacy-preserving techniques.", "link": "http://arxiv.org/abs/2510.16295v2", "date": "2025-12-02", "relevancy": 2.4823, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4998}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4998}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4899}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenLVLM-MIA%3A%20A%20Controlled%20Benchmark%20Revealing%20the%20Limits%20of%20Membership%20Inference%20Attacks%20on%20Large%20Vision-Language%20Models&body=Title%3A%20OpenLVLM-MIA%3A%20A%20Controlled%20Benchmark%20Revealing%20the%20Limits%20of%20Membership%20Inference%20Attacks%20on%20Large%20Vision-Language%20Models%0AAuthor%3A%20Ryoto%20Miyamoto%20and%20Xin%20Fan%20and%20Fuyuko%20Kido%20and%20Tsuneo%20Matsumoto%20and%20Hayato%20Yamana%0AAbstract%3A%20OpenLVLM-MIA%20is%20a%20new%20benchmark%20that%20highlights%20fundamental%20challenges%20in%20evaluating%20membership%20inference%20attacks%20%28MIA%29%20against%20large%20vision-language%20models%20%28LVLMs%29.%20While%20prior%20work%20has%20reported%20high%20attack%20success%20rates%2C%20our%20analysis%20suggests%20that%20these%20results%20often%20arise%20from%20detecting%20distributional%20bias%20introduced%20during%20dataset%20construction%20rather%20than%20from%20identifying%20true%20membership%20status.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20controlled%20benchmark%20of%206%7B%2C%7D000%20images%20where%20the%20distributions%20of%20member%20and%20non-member%20samples%20are%20carefully%20balanced%2C%20and%20ground-truth%20membership%20labels%20are%20provided%20across%20three%20distinct%20training%20stages.%20Experiments%20using%20OpenLVLM-MIA%20demonstrated%20that%20the%20performance%20of%20state-of-the-art%20MIA%20methods%20approached%20chance-level.%20OpenLVLM-MIA%2C%20designed%20to%20be%20transparent%20and%20unbiased%20benchmark%2C%20clarifies%20certain%20limitations%20of%20MIA%20research%20on%20LVLMs%20and%20provides%20a%20solid%20foundation%20for%20developing%20stronger%20privacy-preserving%20techniques.%0ALink%3A%20http%3A//arxiv.org/abs/2510.16295v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenLVLM-MIA%253A%2520A%2520Controlled%2520Benchmark%2520Revealing%2520the%2520Limits%2520of%2520Membership%2520Inference%2520Attacks%2520on%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DRyoto%2520Miyamoto%2520and%2520Xin%2520Fan%2520and%2520Fuyuko%2520Kido%2520and%2520Tsuneo%2520Matsumoto%2520and%2520Hayato%2520Yamana%26entry.1292438233%3DOpenLVLM-MIA%2520is%2520a%2520new%2520benchmark%2520that%2520highlights%2520fundamental%2520challenges%2520in%2520evaluating%2520membership%2520inference%2520attacks%2520%2528MIA%2529%2520against%2520large%2520vision-language%2520models%2520%2528LVLMs%2529.%2520While%2520prior%2520work%2520has%2520reported%2520high%2520attack%2520success%2520rates%252C%2520our%2520analysis%2520suggests%2520that%2520these%2520results%2520often%2520arise%2520from%2520detecting%2520distributional%2520bias%2520introduced%2520during%2520dataset%2520construction%2520rather%2520than%2520from%2520identifying%2520true%2520membership%2520status.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520a%2520controlled%2520benchmark%2520of%25206%257B%252C%257D000%2520images%2520where%2520the%2520distributions%2520of%2520member%2520and%2520non-member%2520samples%2520are%2520carefully%2520balanced%252C%2520and%2520ground-truth%2520membership%2520labels%2520are%2520provided%2520across%2520three%2520distinct%2520training%2520stages.%2520Experiments%2520using%2520OpenLVLM-MIA%2520demonstrated%2520that%2520the%2520performance%2520of%2520state-of-the-art%2520MIA%2520methods%2520approached%2520chance-level.%2520OpenLVLM-MIA%252C%2520designed%2520to%2520be%2520transparent%2520and%2520unbiased%2520benchmark%252C%2520clarifies%2520certain%2520limitations%2520of%2520MIA%2520research%2520on%2520LVLMs%2520and%2520provides%2520a%2520solid%2520foundation%2520for%2520developing%2520stronger%2520privacy-preserving%2520techniques.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.16295v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenLVLM-MIA%3A%20A%20Controlled%20Benchmark%20Revealing%20the%20Limits%20of%20Membership%20Inference%20Attacks%20on%20Large%20Vision-Language%20Models&entry.906535625=Ryoto%20Miyamoto%20and%20Xin%20Fan%20and%20Fuyuko%20Kido%20and%20Tsuneo%20Matsumoto%20and%20Hayato%20Yamana&entry.1292438233=OpenLVLM-MIA%20is%20a%20new%20benchmark%20that%20highlights%20fundamental%20challenges%20in%20evaluating%20membership%20inference%20attacks%20%28MIA%29%20against%20large%20vision-language%20models%20%28LVLMs%29.%20While%20prior%20work%20has%20reported%20high%20attack%20success%20rates%2C%20our%20analysis%20suggests%20that%20these%20results%20often%20arise%20from%20detecting%20distributional%20bias%20introduced%20during%20dataset%20construction%20rather%20than%20from%20identifying%20true%20membership%20status.%20To%20address%20this%20issue%2C%20we%20introduce%20a%20controlled%20benchmark%20of%206%7B%2C%7D000%20images%20where%20the%20distributions%20of%20member%20and%20non-member%20samples%20are%20carefully%20balanced%2C%20and%20ground-truth%20membership%20labels%20are%20provided%20across%20three%20distinct%20training%20stages.%20Experiments%20using%20OpenLVLM-MIA%20demonstrated%20that%20the%20performance%20of%20state-of-the-art%20MIA%20methods%20approached%20chance-level.%20OpenLVLM-MIA%2C%20designed%20to%20be%20transparent%20and%20unbiased%20benchmark%2C%20clarifies%20certain%20limitations%20of%20MIA%20research%20on%20LVLMs%20and%20provides%20a%20solid%20foundation%20for%20developing%20stronger%20privacy-preserving%20techniques.&entry.1838667208=http%3A//arxiv.org/abs/2510.16295v2&entry.124074799=Read"},
{"title": "Text-Queried Audio Source Separation via Hierarchical Modeling", "author": "Xinlei Yin and Xiulian Peng and Xue Jiang and Zhiwei Xiong and Yan Lu", "abstract": "Target audio source separation with natural language queries presents a promising paradigm for extracting arbitrary audio events through arbitrary text descriptions. Existing methods mainly face two challenges, the difficulty in jointly modeling acoustic-textual alignment and semantic-aware separation within a blindly-learned single-stage architecture, and the reliance on large-scale accurately-labeled training data to compensate for inefficient cross-modal learning and separation. To address these challenges, we propose a hierarchical decomposition framework, HSM-TSS, that decouples the task into global-local semantic-guided feature separation and structure-preserving acoustic reconstruction. Our approach introduces a dual-stage mechanism for semantic separation, operating on distinct global and local semantic feature spaces. We first perform global-semantic separation through a global semantic feature space aligned with text queries. A Q-Audio architecture is employed to align audio and text modalities, serving as pretrained global-semantic encoders. Conditioned on the predicted global feature, we then perform the second-stage local-semantic separation on AudioMAE features that preserve time-frequency structures, followed by acoustic reconstruction. We also propose an instruction processing pipeline to parse arbitrary text queries into structured operations, extraction or removal, coupled with audio descriptions, enabling flexible sound manipulation. Our method achieves state-of-the-art separation performance with data-efficient training while maintaining superior semantic consistency with queries in complex auditory scenes.", "link": "http://arxiv.org/abs/2505.21025v2", "date": "2025-12-02", "relevancy": 2.4742, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.498}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.498}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4884}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-Queried%20Audio%20Source%20Separation%20via%20Hierarchical%20Modeling&body=Title%3A%20Text-Queried%20Audio%20Source%20Separation%20via%20Hierarchical%20Modeling%0AAuthor%3A%20Xinlei%20Yin%20and%20Xiulian%20Peng%20and%20Xue%20Jiang%20and%20Zhiwei%20Xiong%20and%20Yan%20Lu%0AAbstract%3A%20Target%20audio%20source%20separation%20with%20natural%20language%20queries%20presents%20a%20promising%20paradigm%20for%20extracting%20arbitrary%20audio%20events%20through%20arbitrary%20text%20descriptions.%20Existing%20methods%20mainly%20face%20two%20challenges%2C%20the%20difficulty%20in%20jointly%20modeling%20acoustic-textual%20alignment%20and%20semantic-aware%20separation%20within%20a%20blindly-learned%20single-stage%20architecture%2C%20and%20the%20reliance%20on%20large-scale%20accurately-labeled%20training%20data%20to%20compensate%20for%20inefficient%20cross-modal%20learning%20and%20separation.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20hierarchical%20decomposition%20framework%2C%20HSM-TSS%2C%20that%20decouples%20the%20task%20into%20global-local%20semantic-guided%20feature%20separation%20and%20structure-preserving%20acoustic%20reconstruction.%20Our%20approach%20introduces%20a%20dual-stage%20mechanism%20for%20semantic%20separation%2C%20operating%20on%20distinct%20global%20and%20local%20semantic%20feature%20spaces.%20We%20first%20perform%20global-semantic%20separation%20through%20a%20global%20semantic%20feature%20space%20aligned%20with%20text%20queries.%20A%20Q-Audio%20architecture%20is%20employed%20to%20align%20audio%20and%20text%20modalities%2C%20serving%20as%20pretrained%20global-semantic%20encoders.%20Conditioned%20on%20the%20predicted%20global%20feature%2C%20we%20then%20perform%20the%20second-stage%20local-semantic%20separation%20on%20AudioMAE%20features%20that%20preserve%20time-frequency%20structures%2C%20followed%20by%20acoustic%20reconstruction.%20We%20also%20propose%20an%20instruction%20processing%20pipeline%20to%20parse%20arbitrary%20text%20queries%20into%20structured%20operations%2C%20extraction%20or%20removal%2C%20coupled%20with%20audio%20descriptions%2C%20enabling%20flexible%20sound%20manipulation.%20Our%20method%20achieves%20state-of-the-art%20separation%20performance%20with%20data-efficient%20training%20while%20maintaining%20superior%20semantic%20consistency%20with%20queries%20in%20complex%20auditory%20scenes.%0ALink%3A%20http%3A//arxiv.org/abs/2505.21025v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-Queried%2520Audio%2520Source%2520Separation%2520via%2520Hierarchical%2520Modeling%26entry.906535625%3DXinlei%2520Yin%2520and%2520Xiulian%2520Peng%2520and%2520Xue%2520Jiang%2520and%2520Zhiwei%2520Xiong%2520and%2520Yan%2520Lu%26entry.1292438233%3DTarget%2520audio%2520source%2520separation%2520with%2520natural%2520language%2520queries%2520presents%2520a%2520promising%2520paradigm%2520for%2520extracting%2520arbitrary%2520audio%2520events%2520through%2520arbitrary%2520text%2520descriptions.%2520Existing%2520methods%2520mainly%2520face%2520two%2520challenges%252C%2520the%2520difficulty%2520in%2520jointly%2520modeling%2520acoustic-textual%2520alignment%2520and%2520semantic-aware%2520separation%2520within%2520a%2520blindly-learned%2520single-stage%2520architecture%252C%2520and%2520the%2520reliance%2520on%2520large-scale%2520accurately-labeled%2520training%2520data%2520to%2520compensate%2520for%2520inefficient%2520cross-modal%2520learning%2520and%2520separation.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520hierarchical%2520decomposition%2520framework%252C%2520HSM-TSS%252C%2520that%2520decouples%2520the%2520task%2520into%2520global-local%2520semantic-guided%2520feature%2520separation%2520and%2520structure-preserving%2520acoustic%2520reconstruction.%2520Our%2520approach%2520introduces%2520a%2520dual-stage%2520mechanism%2520for%2520semantic%2520separation%252C%2520operating%2520on%2520distinct%2520global%2520and%2520local%2520semantic%2520feature%2520spaces.%2520We%2520first%2520perform%2520global-semantic%2520separation%2520through%2520a%2520global%2520semantic%2520feature%2520space%2520aligned%2520with%2520text%2520queries.%2520A%2520Q-Audio%2520architecture%2520is%2520employed%2520to%2520align%2520audio%2520and%2520text%2520modalities%252C%2520serving%2520as%2520pretrained%2520global-semantic%2520encoders.%2520Conditioned%2520on%2520the%2520predicted%2520global%2520feature%252C%2520we%2520then%2520perform%2520the%2520second-stage%2520local-semantic%2520separation%2520on%2520AudioMAE%2520features%2520that%2520preserve%2520time-frequency%2520structures%252C%2520followed%2520by%2520acoustic%2520reconstruction.%2520We%2520also%2520propose%2520an%2520instruction%2520processing%2520pipeline%2520to%2520parse%2520arbitrary%2520text%2520queries%2520into%2520structured%2520operations%252C%2520extraction%2520or%2520removal%252C%2520coupled%2520with%2520audio%2520descriptions%252C%2520enabling%2520flexible%2520sound%2520manipulation.%2520Our%2520method%2520achieves%2520state-of-the-art%2520separation%2520performance%2520with%2520data-efficient%2520training%2520while%2520maintaining%2520superior%2520semantic%2520consistency%2520with%2520queries%2520in%2520complex%2520auditory%2520scenes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21025v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-Queried%20Audio%20Source%20Separation%20via%20Hierarchical%20Modeling&entry.906535625=Xinlei%20Yin%20and%20Xiulian%20Peng%20and%20Xue%20Jiang%20and%20Zhiwei%20Xiong%20and%20Yan%20Lu&entry.1292438233=Target%20audio%20source%20separation%20with%20natural%20language%20queries%20presents%20a%20promising%20paradigm%20for%20extracting%20arbitrary%20audio%20events%20through%20arbitrary%20text%20descriptions.%20Existing%20methods%20mainly%20face%20two%20challenges%2C%20the%20difficulty%20in%20jointly%20modeling%20acoustic-textual%20alignment%20and%20semantic-aware%20separation%20within%20a%20blindly-learned%20single-stage%20architecture%2C%20and%20the%20reliance%20on%20large-scale%20accurately-labeled%20training%20data%20to%20compensate%20for%20inefficient%20cross-modal%20learning%20and%20separation.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20hierarchical%20decomposition%20framework%2C%20HSM-TSS%2C%20that%20decouples%20the%20task%20into%20global-local%20semantic-guided%20feature%20separation%20and%20structure-preserving%20acoustic%20reconstruction.%20Our%20approach%20introduces%20a%20dual-stage%20mechanism%20for%20semantic%20separation%2C%20operating%20on%20distinct%20global%20and%20local%20semantic%20feature%20spaces.%20We%20first%20perform%20global-semantic%20separation%20through%20a%20global%20semantic%20feature%20space%20aligned%20with%20text%20queries.%20A%20Q-Audio%20architecture%20is%20employed%20to%20align%20audio%20and%20text%20modalities%2C%20serving%20as%20pretrained%20global-semantic%20encoders.%20Conditioned%20on%20the%20predicted%20global%20feature%2C%20we%20then%20perform%20the%20second-stage%20local-semantic%20separation%20on%20AudioMAE%20features%20that%20preserve%20time-frequency%20structures%2C%20followed%20by%20acoustic%20reconstruction.%20We%20also%20propose%20an%20instruction%20processing%20pipeline%20to%20parse%20arbitrary%20text%20queries%20into%20structured%20operations%2C%20extraction%20or%20removal%2C%20coupled%20with%20audio%20descriptions%2C%20enabling%20flexible%20sound%20manipulation.%20Our%20method%20achieves%20state-of-the-art%20separation%20performance%20with%20data-efficient%20training%20while%20maintaining%20superior%20semantic%20consistency%20with%20queries%20in%20complex%20auditory%20scenes.&entry.1838667208=http%3A//arxiv.org/abs/2505.21025v2&entry.124074799=Read"},
{"title": "Are Detectors Fair to Indian IP-AIGC? A Cross-Generator Study", "author": "Vishal Dubey and Pallavi Tyagi", "abstract": "Modern image editors can produce identity-preserving AIGC (IP-AIGC), where the same person appears with new attire, background, or lighting. The robustness and fairness of current detectors in this regime remain unclear, especially for under-represented populations. We present what we believe is the first systematic study of IP-AIGC detection for Indian and South-Asian faces, quantifying cross-generator generalization and intra-population performance. We assemble Indian-focused training splits from FairFD and HAV-DF, and construct two held-out IP-AIGC test sets (HIDF-img-ip-genai and HIDF-vid-ip-genai) using commercial web-UI generators (Gemini and ChatGPT) with identity-preserving prompts. We evaluate two state-of-the-art detectors (AIDE and Effort) under pretrained (PT) and fine-tuned (FT) regimes and report AUC, AP, EER, and accuracy. Fine-tuning yields strong in-domain gains (for example, Effort AUC 0.739 to 0.944 on HAV-DF-test; AIDE EER 0.484 to 0.259), but consistently degrades performance on held-out IP-AIGC for Indian cohorts (for example, AIDE AUC 0.923 to 0.563 on HIDF-img-ip-genai; Effort 0.740 to 0.533), which indicates overfitting to training-generator cues. On non-IP HIDF images, PT performance remains high, which suggests a specific brittleness to identity-preserving edits rather than a generic distribution shift. Our study establishes IP-AIGC-Indian as a challenging and practically relevant scenario and motivates representation-preserving adaptation and India-aware benchmark curation to close generalization gaps in AIGC detection.", "link": "http://arxiv.org/abs/2512.02850v1", "date": "2025-12-02", "relevancy": 2.4736, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5454}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4708}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Detectors%20Fair%20to%20Indian%20IP-AIGC%3F%20A%20Cross-Generator%20Study&body=Title%3A%20Are%20Detectors%20Fair%20to%20Indian%20IP-AIGC%3F%20A%20Cross-Generator%20Study%0AAuthor%3A%20Vishal%20Dubey%20and%20Pallavi%20Tyagi%0AAbstract%3A%20Modern%20image%20editors%20can%20produce%20identity-preserving%20AIGC%20%28IP-AIGC%29%2C%20where%20the%20same%20person%20appears%20with%20new%20attire%2C%20background%2C%20or%20lighting.%20The%20robustness%20and%20fairness%20of%20current%20detectors%20in%20this%20regime%20remain%20unclear%2C%20especially%20for%20under-represented%20populations.%20We%20present%20what%20we%20believe%20is%20the%20first%20systematic%20study%20of%20IP-AIGC%20detection%20for%20Indian%20and%20South-Asian%20faces%2C%20quantifying%20cross-generator%20generalization%20and%20intra-population%20performance.%20We%20assemble%20Indian-focused%20training%20splits%20from%20FairFD%20and%20HAV-DF%2C%20and%20construct%20two%20held-out%20IP-AIGC%20test%20sets%20%28HIDF-img-ip-genai%20and%20HIDF-vid-ip-genai%29%20using%20commercial%20web-UI%20generators%20%28Gemini%20and%20ChatGPT%29%20with%20identity-preserving%20prompts.%20We%20evaluate%20two%20state-of-the-art%20detectors%20%28AIDE%20and%20Effort%29%20under%20pretrained%20%28PT%29%20and%20fine-tuned%20%28FT%29%20regimes%20and%20report%20AUC%2C%20AP%2C%20EER%2C%20and%20accuracy.%20Fine-tuning%20yields%20strong%20in-domain%20gains%20%28for%20example%2C%20Effort%20AUC%200.739%20to%200.944%20on%20HAV-DF-test%3B%20AIDE%20EER%200.484%20to%200.259%29%2C%20but%20consistently%20degrades%20performance%20on%20held-out%20IP-AIGC%20for%20Indian%20cohorts%20%28for%20example%2C%20AIDE%20AUC%200.923%20to%200.563%20on%20HIDF-img-ip-genai%3B%20Effort%200.740%20to%200.533%29%2C%20which%20indicates%20overfitting%20to%20training-generator%20cues.%20On%20non-IP%20HIDF%20images%2C%20PT%20performance%20remains%20high%2C%20which%20suggests%20a%20specific%20brittleness%20to%20identity-preserving%20edits%20rather%20than%20a%20generic%20distribution%20shift.%20Our%20study%20establishes%20IP-AIGC-Indian%20as%20a%20challenging%20and%20practically%20relevant%20scenario%20and%20motivates%20representation-preserving%20adaptation%20and%20India-aware%20benchmark%20curation%20to%20close%20generalization%20gaps%20in%20AIGC%20detection.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02850v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Detectors%2520Fair%2520to%2520Indian%2520IP-AIGC%253F%2520A%2520Cross-Generator%2520Study%26entry.906535625%3DVishal%2520Dubey%2520and%2520Pallavi%2520Tyagi%26entry.1292438233%3DModern%2520image%2520editors%2520can%2520produce%2520identity-preserving%2520AIGC%2520%2528IP-AIGC%2529%252C%2520where%2520the%2520same%2520person%2520appears%2520with%2520new%2520attire%252C%2520background%252C%2520or%2520lighting.%2520The%2520robustness%2520and%2520fairness%2520of%2520current%2520detectors%2520in%2520this%2520regime%2520remain%2520unclear%252C%2520especially%2520for%2520under-represented%2520populations.%2520We%2520present%2520what%2520we%2520believe%2520is%2520the%2520first%2520systematic%2520study%2520of%2520IP-AIGC%2520detection%2520for%2520Indian%2520and%2520South-Asian%2520faces%252C%2520quantifying%2520cross-generator%2520generalization%2520and%2520intra-population%2520performance.%2520We%2520assemble%2520Indian-focused%2520training%2520splits%2520from%2520FairFD%2520and%2520HAV-DF%252C%2520and%2520construct%2520two%2520held-out%2520IP-AIGC%2520test%2520sets%2520%2528HIDF-img-ip-genai%2520and%2520HIDF-vid-ip-genai%2529%2520using%2520commercial%2520web-UI%2520generators%2520%2528Gemini%2520and%2520ChatGPT%2529%2520with%2520identity-preserving%2520prompts.%2520We%2520evaluate%2520two%2520state-of-the-art%2520detectors%2520%2528AIDE%2520and%2520Effort%2529%2520under%2520pretrained%2520%2528PT%2529%2520and%2520fine-tuned%2520%2528FT%2529%2520regimes%2520and%2520report%2520AUC%252C%2520AP%252C%2520EER%252C%2520and%2520accuracy.%2520Fine-tuning%2520yields%2520strong%2520in-domain%2520gains%2520%2528for%2520example%252C%2520Effort%2520AUC%25200.739%2520to%25200.944%2520on%2520HAV-DF-test%253B%2520AIDE%2520EER%25200.484%2520to%25200.259%2529%252C%2520but%2520consistently%2520degrades%2520performance%2520on%2520held-out%2520IP-AIGC%2520for%2520Indian%2520cohorts%2520%2528for%2520example%252C%2520AIDE%2520AUC%25200.923%2520to%25200.563%2520on%2520HIDF-img-ip-genai%253B%2520Effort%25200.740%2520to%25200.533%2529%252C%2520which%2520indicates%2520overfitting%2520to%2520training-generator%2520cues.%2520On%2520non-IP%2520HIDF%2520images%252C%2520PT%2520performance%2520remains%2520high%252C%2520which%2520suggests%2520a%2520specific%2520brittleness%2520to%2520identity-preserving%2520edits%2520rather%2520than%2520a%2520generic%2520distribution%2520shift.%2520Our%2520study%2520establishes%2520IP-AIGC-Indian%2520as%2520a%2520challenging%2520and%2520practically%2520relevant%2520scenario%2520and%2520motivates%2520representation-preserving%2520adaptation%2520and%2520India-aware%2520benchmark%2520curation%2520to%2520close%2520generalization%2520gaps%2520in%2520AIGC%2520detection.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02850v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Detectors%20Fair%20to%20Indian%20IP-AIGC%3F%20A%20Cross-Generator%20Study&entry.906535625=Vishal%20Dubey%20and%20Pallavi%20Tyagi&entry.1292438233=Modern%20image%20editors%20can%20produce%20identity-preserving%20AIGC%20%28IP-AIGC%29%2C%20where%20the%20same%20person%20appears%20with%20new%20attire%2C%20background%2C%20or%20lighting.%20The%20robustness%20and%20fairness%20of%20current%20detectors%20in%20this%20regime%20remain%20unclear%2C%20especially%20for%20under-represented%20populations.%20We%20present%20what%20we%20believe%20is%20the%20first%20systematic%20study%20of%20IP-AIGC%20detection%20for%20Indian%20and%20South-Asian%20faces%2C%20quantifying%20cross-generator%20generalization%20and%20intra-population%20performance.%20We%20assemble%20Indian-focused%20training%20splits%20from%20FairFD%20and%20HAV-DF%2C%20and%20construct%20two%20held-out%20IP-AIGC%20test%20sets%20%28HIDF-img-ip-genai%20and%20HIDF-vid-ip-genai%29%20using%20commercial%20web-UI%20generators%20%28Gemini%20and%20ChatGPT%29%20with%20identity-preserving%20prompts.%20We%20evaluate%20two%20state-of-the-art%20detectors%20%28AIDE%20and%20Effort%29%20under%20pretrained%20%28PT%29%20and%20fine-tuned%20%28FT%29%20regimes%20and%20report%20AUC%2C%20AP%2C%20EER%2C%20and%20accuracy.%20Fine-tuning%20yields%20strong%20in-domain%20gains%20%28for%20example%2C%20Effort%20AUC%200.739%20to%200.944%20on%20HAV-DF-test%3B%20AIDE%20EER%200.484%20to%200.259%29%2C%20but%20consistently%20degrades%20performance%20on%20held-out%20IP-AIGC%20for%20Indian%20cohorts%20%28for%20example%2C%20AIDE%20AUC%200.923%20to%200.563%20on%20HIDF-img-ip-genai%3B%20Effort%200.740%20to%200.533%29%2C%20which%20indicates%20overfitting%20to%20training-generator%20cues.%20On%20non-IP%20HIDF%20images%2C%20PT%20performance%20remains%20high%2C%20which%20suggests%20a%20specific%20brittleness%20to%20identity-preserving%20edits%20rather%20than%20a%20generic%20distribution%20shift.%20Our%20study%20establishes%20IP-AIGC-Indian%20as%20a%20challenging%20and%20practically%20relevant%20scenario%20and%20motivates%20representation-preserving%20adaptation%20and%20India-aware%20benchmark%20curation%20to%20close%20generalization%20gaps%20in%20AIGC%20detection.&entry.1838667208=http%3A//arxiv.org/abs/2512.02850v1&entry.124074799=Read"},
{"title": "Adaptive Decentralized Federated Learning for Robust Optimization", "author": "Shuyuan Wu and Feifei Wang and Yuan Gao and Hansheng Wang", "abstract": "In decentralized federated learning (DFL), the presence of abnormal clients, often caused by noisy or poisoned data, can significantly disrupt the learning process and degrade the overall robustness of the model. Previous methods on this issue often require a sufficiently large number of normal neighboring clients or prior knowledge of reliable clients, which reduces the practical applicability of DFL. To address these limitations, we develop here a novel adaptive DFL (aDFL) approach for robust estimation. The key idea is to adaptively adjust the learning rates of clients. By assigning smaller rates to suspicious clients and larger rates to normal clients, aDFL mitigates the negative impact of abnormal clients on the global model in a fully adaptive way. Our theory does not put any stringent conditions on neighboring nodes and requires no prior knowledge. A rigorous convergence analysis is provided to guarantee the oracle property of aDFL. Extensive numerical experiments demonstrate the superior performance of the aDFL method.", "link": "http://arxiv.org/abs/2512.02852v1", "date": "2025-12-02", "relevancy": 2.4592, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5128}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4823}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4804}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Decentralized%20Federated%20Learning%20for%20Robust%20Optimization&body=Title%3A%20Adaptive%20Decentralized%20Federated%20Learning%20for%20Robust%20Optimization%0AAuthor%3A%20Shuyuan%20Wu%20and%20Feifei%20Wang%20and%20Yuan%20Gao%20and%20Hansheng%20Wang%0AAbstract%3A%20In%20decentralized%20federated%20learning%20%28DFL%29%2C%20the%20presence%20of%20abnormal%20clients%2C%20often%20caused%20by%20noisy%20or%20poisoned%20data%2C%20can%20significantly%20disrupt%20the%20learning%20process%20and%20degrade%20the%20overall%20robustness%20of%20the%20model.%20Previous%20methods%20on%20this%20issue%20often%20require%20a%20sufficiently%20large%20number%20of%20normal%20neighboring%20clients%20or%20prior%20knowledge%20of%20reliable%20clients%2C%20which%20reduces%20the%20practical%20applicability%20of%20DFL.%20To%20address%20these%20limitations%2C%20we%20develop%20here%20a%20novel%20adaptive%20DFL%20%28aDFL%29%20approach%20for%20robust%20estimation.%20The%20key%20idea%20is%20to%20adaptively%20adjust%20the%20learning%20rates%20of%20clients.%20By%20assigning%20smaller%20rates%20to%20suspicious%20clients%20and%20larger%20rates%20to%20normal%20clients%2C%20aDFL%20mitigates%20the%20negative%20impact%20of%20abnormal%20clients%20on%20the%20global%20model%20in%20a%20fully%20adaptive%20way.%20Our%20theory%20does%20not%20put%20any%20stringent%20conditions%20on%20neighboring%20nodes%20and%20requires%20no%20prior%20knowledge.%20A%20rigorous%20convergence%20analysis%20is%20provided%20to%20guarantee%20the%20oracle%20property%20of%20aDFL.%20Extensive%20numerical%20experiments%20demonstrate%20the%20superior%20performance%20of%20the%20aDFL%20method.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02852v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Decentralized%2520Federated%2520Learning%2520for%2520Robust%2520Optimization%26entry.906535625%3DShuyuan%2520Wu%2520and%2520Feifei%2520Wang%2520and%2520Yuan%2520Gao%2520and%2520Hansheng%2520Wang%26entry.1292438233%3DIn%2520decentralized%2520federated%2520learning%2520%2528DFL%2529%252C%2520the%2520presence%2520of%2520abnormal%2520clients%252C%2520often%2520caused%2520by%2520noisy%2520or%2520poisoned%2520data%252C%2520can%2520significantly%2520disrupt%2520the%2520learning%2520process%2520and%2520degrade%2520the%2520overall%2520robustness%2520of%2520the%2520model.%2520Previous%2520methods%2520on%2520this%2520issue%2520often%2520require%2520a%2520sufficiently%2520large%2520number%2520of%2520normal%2520neighboring%2520clients%2520or%2520prior%2520knowledge%2520of%2520reliable%2520clients%252C%2520which%2520reduces%2520the%2520practical%2520applicability%2520of%2520DFL.%2520To%2520address%2520these%2520limitations%252C%2520we%2520develop%2520here%2520a%2520novel%2520adaptive%2520DFL%2520%2528aDFL%2529%2520approach%2520for%2520robust%2520estimation.%2520The%2520key%2520idea%2520is%2520to%2520adaptively%2520adjust%2520the%2520learning%2520rates%2520of%2520clients.%2520By%2520assigning%2520smaller%2520rates%2520to%2520suspicious%2520clients%2520and%2520larger%2520rates%2520to%2520normal%2520clients%252C%2520aDFL%2520mitigates%2520the%2520negative%2520impact%2520of%2520abnormal%2520clients%2520on%2520the%2520global%2520model%2520in%2520a%2520fully%2520adaptive%2520way.%2520Our%2520theory%2520does%2520not%2520put%2520any%2520stringent%2520conditions%2520on%2520neighboring%2520nodes%2520and%2520requires%2520no%2520prior%2520knowledge.%2520A%2520rigorous%2520convergence%2520analysis%2520is%2520provided%2520to%2520guarantee%2520the%2520oracle%2520property%2520of%2520aDFL.%2520Extensive%2520numerical%2520experiments%2520demonstrate%2520the%2520superior%2520performance%2520of%2520the%2520aDFL%2520method.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02852v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Decentralized%20Federated%20Learning%20for%20Robust%20Optimization&entry.906535625=Shuyuan%20Wu%20and%20Feifei%20Wang%20and%20Yuan%20Gao%20and%20Hansheng%20Wang&entry.1292438233=In%20decentralized%20federated%20learning%20%28DFL%29%2C%20the%20presence%20of%20abnormal%20clients%2C%20often%20caused%20by%20noisy%20or%20poisoned%20data%2C%20can%20significantly%20disrupt%20the%20learning%20process%20and%20degrade%20the%20overall%20robustness%20of%20the%20model.%20Previous%20methods%20on%20this%20issue%20often%20require%20a%20sufficiently%20large%20number%20of%20normal%20neighboring%20clients%20or%20prior%20knowledge%20of%20reliable%20clients%2C%20which%20reduces%20the%20practical%20applicability%20of%20DFL.%20To%20address%20these%20limitations%2C%20we%20develop%20here%20a%20novel%20adaptive%20DFL%20%28aDFL%29%20approach%20for%20robust%20estimation.%20The%20key%20idea%20is%20to%20adaptively%20adjust%20the%20learning%20rates%20of%20clients.%20By%20assigning%20smaller%20rates%20to%20suspicious%20clients%20and%20larger%20rates%20to%20normal%20clients%2C%20aDFL%20mitigates%20the%20negative%20impact%20of%20abnormal%20clients%20on%20the%20global%20model%20in%20a%20fully%20adaptive%20way.%20Our%20theory%20does%20not%20put%20any%20stringent%20conditions%20on%20neighboring%20nodes%20and%20requires%20no%20prior%20knowledge.%20A%20rigorous%20convergence%20analysis%20is%20provided%20to%20guarantee%20the%20oracle%20property%20of%20aDFL.%20Extensive%20numerical%20experiments%20demonstrate%20the%20superior%20performance%20of%20the%20aDFL%20method.&entry.1838667208=http%3A//arxiv.org/abs/2512.02852v1&entry.124074799=Read"},
{"title": "CAMEO: Correspondence-Attention Alignment for Multi-View Diffusion Models", "author": "Minkyung Kwon and Jinhyeok Choi and Jiho Park and Seonghu Jeon and Jinhyuk Jang and Junyoung Seo and Minseop Kwak and Jin-Hwa Kim and Seungryong Kim", "abstract": "Multi-view diffusion models have recently emerged as a powerful paradigm for novel view synthesis, yet the underlying mechanism that enables their view-consistency remains unclear. In this work, we first verify that the attention maps of these models acquire geometric correspondence throughout training, attending to the geometrically corresponding regions across reference and target views for view-consistent generation. However, this correspondence signal remains incomplete, with its accuracy degrading under large viewpoint changes. Building on these findings, we introduce CAMEO, a simple yet effective training technique that directly supervises attention maps using geometric correspondence to enhance both the training efficiency and generation quality of multi-view diffusion models. Notably, supervising a single attention layer is sufficient to guide the model toward learning precise correspondences, thereby preserving the geometry and structure of reference images, accelerating convergence, and improving novel view synthesis performance. CAMEO reduces the number of training iterations required for convergence by half while achieving superior performance at the same iteration counts. We further demonstrate that CAMEO is model-agnostic and can be applied to any multi-view diffusion model.", "link": "http://arxiv.org/abs/2512.03045v1", "date": "2025-12-02", "relevancy": 2.4535, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6153}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6153}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6038}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAMEO%3A%20Correspondence-Attention%20Alignment%20for%20Multi-View%20Diffusion%20Models&body=Title%3A%20CAMEO%3A%20Correspondence-Attention%20Alignment%20for%20Multi-View%20Diffusion%20Models%0AAuthor%3A%20Minkyung%20Kwon%20and%20Jinhyeok%20Choi%20and%20Jiho%20Park%20and%20Seonghu%20Jeon%20and%20Jinhyuk%20Jang%20and%20Junyoung%20Seo%20and%20Minseop%20Kwak%20and%20Jin-Hwa%20Kim%20and%20Seungryong%20Kim%0AAbstract%3A%20Multi-view%20diffusion%20models%20have%20recently%20emerged%20as%20a%20powerful%20paradigm%20for%20novel%20view%20synthesis%2C%20yet%20the%20underlying%20mechanism%20that%20enables%20their%20view-consistency%20remains%20unclear.%20In%20this%20work%2C%20we%20first%20verify%20that%20the%20attention%20maps%20of%20these%20models%20acquire%20geometric%20correspondence%20throughout%20training%2C%20attending%20to%20the%20geometrically%20corresponding%20regions%20across%20reference%20and%20target%20views%20for%20view-consistent%20generation.%20However%2C%20this%20correspondence%20signal%20remains%20incomplete%2C%20with%20its%20accuracy%20degrading%20under%20large%20viewpoint%20changes.%20Building%20on%20these%20findings%2C%20we%20introduce%20CAMEO%2C%20a%20simple%20yet%20effective%20training%20technique%20that%20directly%20supervises%20attention%20maps%20using%20geometric%20correspondence%20to%20enhance%20both%20the%20training%20efficiency%20and%20generation%20quality%20of%20multi-view%20diffusion%20models.%20Notably%2C%20supervising%20a%20single%20attention%20layer%20is%20sufficient%20to%20guide%20the%20model%20toward%20learning%20precise%20correspondences%2C%20thereby%20preserving%20the%20geometry%20and%20structure%20of%20reference%20images%2C%20accelerating%20convergence%2C%20and%20improving%20novel%20view%20synthesis%20performance.%20CAMEO%20reduces%20the%20number%20of%20training%20iterations%20required%20for%20convergence%20by%20half%20while%20achieving%20superior%20performance%20at%20the%20same%20iteration%20counts.%20We%20further%20demonstrate%20that%20CAMEO%20is%20model-agnostic%20and%20can%20be%20applied%20to%20any%20multi-view%20diffusion%20model.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03045v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAMEO%253A%2520Correspondence-Attention%2520Alignment%2520for%2520Multi-View%2520Diffusion%2520Models%26entry.906535625%3DMinkyung%2520Kwon%2520and%2520Jinhyeok%2520Choi%2520and%2520Jiho%2520Park%2520and%2520Seonghu%2520Jeon%2520and%2520Jinhyuk%2520Jang%2520and%2520Junyoung%2520Seo%2520and%2520Minseop%2520Kwak%2520and%2520Jin-Hwa%2520Kim%2520and%2520Seungryong%2520Kim%26entry.1292438233%3DMulti-view%2520diffusion%2520models%2520have%2520recently%2520emerged%2520as%2520a%2520powerful%2520paradigm%2520for%2520novel%2520view%2520synthesis%252C%2520yet%2520the%2520underlying%2520mechanism%2520that%2520enables%2520their%2520view-consistency%2520remains%2520unclear.%2520In%2520this%2520work%252C%2520we%2520first%2520verify%2520that%2520the%2520attention%2520maps%2520of%2520these%2520models%2520acquire%2520geometric%2520correspondence%2520throughout%2520training%252C%2520attending%2520to%2520the%2520geometrically%2520corresponding%2520regions%2520across%2520reference%2520and%2520target%2520views%2520for%2520view-consistent%2520generation.%2520However%252C%2520this%2520correspondence%2520signal%2520remains%2520incomplete%252C%2520with%2520its%2520accuracy%2520degrading%2520under%2520large%2520viewpoint%2520changes.%2520Building%2520on%2520these%2520findings%252C%2520we%2520introduce%2520CAMEO%252C%2520a%2520simple%2520yet%2520effective%2520training%2520technique%2520that%2520directly%2520supervises%2520attention%2520maps%2520using%2520geometric%2520correspondence%2520to%2520enhance%2520both%2520the%2520training%2520efficiency%2520and%2520generation%2520quality%2520of%2520multi-view%2520diffusion%2520models.%2520Notably%252C%2520supervising%2520a%2520single%2520attention%2520layer%2520is%2520sufficient%2520to%2520guide%2520the%2520model%2520toward%2520learning%2520precise%2520correspondences%252C%2520thereby%2520preserving%2520the%2520geometry%2520and%2520structure%2520of%2520reference%2520images%252C%2520accelerating%2520convergence%252C%2520and%2520improving%2520novel%2520view%2520synthesis%2520performance.%2520CAMEO%2520reduces%2520the%2520number%2520of%2520training%2520iterations%2520required%2520for%2520convergence%2520by%2520half%2520while%2520achieving%2520superior%2520performance%2520at%2520the%2520same%2520iteration%2520counts.%2520We%2520further%2520demonstrate%2520that%2520CAMEO%2520is%2520model-agnostic%2520and%2520can%2520be%2520applied%2520to%2520any%2520multi-view%2520diffusion%2520model.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03045v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAMEO%3A%20Correspondence-Attention%20Alignment%20for%20Multi-View%20Diffusion%20Models&entry.906535625=Minkyung%20Kwon%20and%20Jinhyeok%20Choi%20and%20Jiho%20Park%20and%20Seonghu%20Jeon%20and%20Jinhyuk%20Jang%20and%20Junyoung%20Seo%20and%20Minseop%20Kwak%20and%20Jin-Hwa%20Kim%20and%20Seungryong%20Kim&entry.1292438233=Multi-view%20diffusion%20models%20have%20recently%20emerged%20as%20a%20powerful%20paradigm%20for%20novel%20view%20synthesis%2C%20yet%20the%20underlying%20mechanism%20that%20enables%20their%20view-consistency%20remains%20unclear.%20In%20this%20work%2C%20we%20first%20verify%20that%20the%20attention%20maps%20of%20these%20models%20acquire%20geometric%20correspondence%20throughout%20training%2C%20attending%20to%20the%20geometrically%20corresponding%20regions%20across%20reference%20and%20target%20views%20for%20view-consistent%20generation.%20However%2C%20this%20correspondence%20signal%20remains%20incomplete%2C%20with%20its%20accuracy%20degrading%20under%20large%20viewpoint%20changes.%20Building%20on%20these%20findings%2C%20we%20introduce%20CAMEO%2C%20a%20simple%20yet%20effective%20training%20technique%20that%20directly%20supervises%20attention%20maps%20using%20geometric%20correspondence%20to%20enhance%20both%20the%20training%20efficiency%20and%20generation%20quality%20of%20multi-view%20diffusion%20models.%20Notably%2C%20supervising%20a%20single%20attention%20layer%20is%20sufficient%20to%20guide%20the%20model%20toward%20learning%20precise%20correspondences%2C%20thereby%20preserving%20the%20geometry%20and%20structure%20of%20reference%20images%2C%20accelerating%20convergence%2C%20and%20improving%20novel%20view%20synthesis%20performance.%20CAMEO%20reduces%20the%20number%20of%20training%20iterations%20required%20for%20convergence%20by%20half%20while%20achieving%20superior%20performance%20at%20the%20same%20iteration%20counts.%20We%20further%20demonstrate%20that%20CAMEO%20is%20model-agnostic%20and%20can%20be%20applied%20to%20any%20multi-view%20diffusion%20model.&entry.1838667208=http%3A//arxiv.org/abs/2512.03045v1&entry.124074799=Read"},
{"title": "scE2TM improves single-cell embedding interpretability and reveals cellular perturbation signatures", "author": "Hegang Chen and Yuyin Lu and Yifan Zhao and Zhiming Dai and Fu Lee Wang and Qing Li and Yanghui Rao and Yue Li", "abstract": "Single-cell RNA sequencing technologies have revolutionized our understanding of cellular heterogeneity, yet computational methods often struggle to balance performance with biological interpretability. Embedded topic models have been widely used for interpretable single-cell embedding learning. However, these models suffer from the potential problem of interpretation collapse, where topics semantically collapse towards each other, resulting in redundant topics and incomplete capture of biological variation. Furthermore, the rise of single-cell foundation models creates opportunities to harness external biological knowledge for guiding model embeddings. Here, we present scE2TM, an external knowledge-guided embedded topic model that provides a high-quality cell embedding and interpretation for scRNA-seq analysis. Through embedding clustering regularization method, each topic is constrained to be the center of a separately aggregated gene cluster, enabling it to capture unique biological information. Across 20 scRNA-seq datasets, scE2TM achieves superior clustering performance compared with seven state-of-the-art methods. A comprehensive interpretability benchmark further shows that scE2TM-learned topics exhibit higher diversity and stronger consistency with underlying biological pathways. Modeling interferon-stimulated PBMCs, scE2TM simulates topic perturbations that drive control cells toward stimulated-like transcriptional states, faithfully mirroring experimental interferon responses. In melanoma, scE2TM identifies malignant-specific topics and extrapolates them to unseen patient data, revealing gene programs associated with patient survival.", "link": "http://arxiv.org/abs/2507.08355v2", "date": "2025-12-02", "relevancy": 2.4519, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5069}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5069}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20scE2TM%20improves%20single-cell%20embedding%20interpretability%20and%20reveals%20cellular%20perturbation%20signatures&body=Title%3A%20scE2TM%20improves%20single-cell%20embedding%20interpretability%20and%20reveals%20cellular%20perturbation%20signatures%0AAuthor%3A%20Hegang%20Chen%20and%20Yuyin%20Lu%20and%20Yifan%20Zhao%20and%20Zhiming%20Dai%20and%20Fu%20Lee%20Wang%20and%20Qing%20Li%20and%20Yanghui%20Rao%20and%20Yue%20Li%0AAbstract%3A%20Single-cell%20RNA%20sequencing%20technologies%20have%20revolutionized%20our%20understanding%20of%20cellular%20heterogeneity%2C%20yet%20computational%20methods%20often%20struggle%20to%20balance%20performance%20with%20biological%20interpretability.%20Embedded%20topic%20models%20have%20been%20widely%20used%20for%20interpretable%20single-cell%20embedding%20learning.%20However%2C%20these%20models%20suffer%20from%20the%20potential%20problem%20of%20interpretation%20collapse%2C%20where%20topics%20semantically%20collapse%20towards%20each%20other%2C%20resulting%20in%20redundant%20topics%20and%20incomplete%20capture%20of%20biological%20variation.%20Furthermore%2C%20the%20rise%20of%20single-cell%20foundation%20models%20creates%20opportunities%20to%20harness%20external%20biological%20knowledge%20for%20guiding%20model%20embeddings.%20Here%2C%20we%20present%20scE2TM%2C%20an%20external%20knowledge-guided%20embedded%20topic%20model%20that%20provides%20a%20high-quality%20cell%20embedding%20and%20interpretation%20for%20scRNA-seq%20analysis.%20Through%20embedding%20clustering%20regularization%20method%2C%20each%20topic%20is%20constrained%20to%20be%20the%20center%20of%20a%20separately%20aggregated%20gene%20cluster%2C%20enabling%20it%20to%20capture%20unique%20biological%20information.%20Across%2020%20scRNA-seq%20datasets%2C%20scE2TM%20achieves%20superior%20clustering%20performance%20compared%20with%20seven%20state-of-the-art%20methods.%20A%20comprehensive%20interpretability%20benchmark%20further%20shows%20that%20scE2TM-learned%20topics%20exhibit%20higher%20diversity%20and%20stronger%20consistency%20with%20underlying%20biological%20pathways.%20Modeling%20interferon-stimulated%20PBMCs%2C%20scE2TM%20simulates%20topic%20perturbations%20that%20drive%20control%20cells%20toward%20stimulated-like%20transcriptional%20states%2C%20faithfully%20mirroring%20experimental%20interferon%20responses.%20In%20melanoma%2C%20scE2TM%20identifies%20malignant-specific%20topics%20and%20extrapolates%20them%20to%20unseen%20patient%20data%2C%20revealing%20gene%20programs%20associated%20with%20patient%20survival.%0ALink%3A%20http%3A//arxiv.org/abs/2507.08355v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DscE2TM%2520improves%2520single-cell%2520embedding%2520interpretability%2520and%2520reveals%2520cellular%2520perturbation%2520signatures%26entry.906535625%3DHegang%2520Chen%2520and%2520Yuyin%2520Lu%2520and%2520Yifan%2520Zhao%2520and%2520Zhiming%2520Dai%2520and%2520Fu%2520Lee%2520Wang%2520and%2520Qing%2520Li%2520and%2520Yanghui%2520Rao%2520and%2520Yue%2520Li%26entry.1292438233%3DSingle-cell%2520RNA%2520sequencing%2520technologies%2520have%2520revolutionized%2520our%2520understanding%2520of%2520cellular%2520heterogeneity%252C%2520yet%2520computational%2520methods%2520often%2520struggle%2520to%2520balance%2520performance%2520with%2520biological%2520interpretability.%2520Embedded%2520topic%2520models%2520have%2520been%2520widely%2520used%2520for%2520interpretable%2520single-cell%2520embedding%2520learning.%2520However%252C%2520these%2520models%2520suffer%2520from%2520the%2520potential%2520problem%2520of%2520interpretation%2520collapse%252C%2520where%2520topics%2520semantically%2520collapse%2520towards%2520each%2520other%252C%2520resulting%2520in%2520redundant%2520topics%2520and%2520incomplete%2520capture%2520of%2520biological%2520variation.%2520Furthermore%252C%2520the%2520rise%2520of%2520single-cell%2520foundation%2520models%2520creates%2520opportunities%2520to%2520harness%2520external%2520biological%2520knowledge%2520for%2520guiding%2520model%2520embeddings.%2520Here%252C%2520we%2520present%2520scE2TM%252C%2520an%2520external%2520knowledge-guided%2520embedded%2520topic%2520model%2520that%2520provides%2520a%2520high-quality%2520cell%2520embedding%2520and%2520interpretation%2520for%2520scRNA-seq%2520analysis.%2520Through%2520embedding%2520clustering%2520regularization%2520method%252C%2520each%2520topic%2520is%2520constrained%2520to%2520be%2520the%2520center%2520of%2520a%2520separately%2520aggregated%2520gene%2520cluster%252C%2520enabling%2520it%2520to%2520capture%2520unique%2520biological%2520information.%2520Across%252020%2520scRNA-seq%2520datasets%252C%2520scE2TM%2520achieves%2520superior%2520clustering%2520performance%2520compared%2520with%2520seven%2520state-of-the-art%2520methods.%2520A%2520comprehensive%2520interpretability%2520benchmark%2520further%2520shows%2520that%2520scE2TM-learned%2520topics%2520exhibit%2520higher%2520diversity%2520and%2520stronger%2520consistency%2520with%2520underlying%2520biological%2520pathways.%2520Modeling%2520interferon-stimulated%2520PBMCs%252C%2520scE2TM%2520simulates%2520topic%2520perturbations%2520that%2520drive%2520control%2520cells%2520toward%2520stimulated-like%2520transcriptional%2520states%252C%2520faithfully%2520mirroring%2520experimental%2520interferon%2520responses.%2520In%2520melanoma%252C%2520scE2TM%2520identifies%2520malignant-specific%2520topics%2520and%2520extrapolates%2520them%2520to%2520unseen%2520patient%2520data%252C%2520revealing%2520gene%2520programs%2520associated%2520with%2520patient%2520survival.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08355v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=scE2TM%20improves%20single-cell%20embedding%20interpretability%20and%20reveals%20cellular%20perturbation%20signatures&entry.906535625=Hegang%20Chen%20and%20Yuyin%20Lu%20and%20Yifan%20Zhao%20and%20Zhiming%20Dai%20and%20Fu%20Lee%20Wang%20and%20Qing%20Li%20and%20Yanghui%20Rao%20and%20Yue%20Li&entry.1292438233=Single-cell%20RNA%20sequencing%20technologies%20have%20revolutionized%20our%20understanding%20of%20cellular%20heterogeneity%2C%20yet%20computational%20methods%20often%20struggle%20to%20balance%20performance%20with%20biological%20interpretability.%20Embedded%20topic%20models%20have%20been%20widely%20used%20for%20interpretable%20single-cell%20embedding%20learning.%20However%2C%20these%20models%20suffer%20from%20the%20potential%20problem%20of%20interpretation%20collapse%2C%20where%20topics%20semantically%20collapse%20towards%20each%20other%2C%20resulting%20in%20redundant%20topics%20and%20incomplete%20capture%20of%20biological%20variation.%20Furthermore%2C%20the%20rise%20of%20single-cell%20foundation%20models%20creates%20opportunities%20to%20harness%20external%20biological%20knowledge%20for%20guiding%20model%20embeddings.%20Here%2C%20we%20present%20scE2TM%2C%20an%20external%20knowledge-guided%20embedded%20topic%20model%20that%20provides%20a%20high-quality%20cell%20embedding%20and%20interpretation%20for%20scRNA-seq%20analysis.%20Through%20embedding%20clustering%20regularization%20method%2C%20each%20topic%20is%20constrained%20to%20be%20the%20center%20of%20a%20separately%20aggregated%20gene%20cluster%2C%20enabling%20it%20to%20capture%20unique%20biological%20information.%20Across%2020%20scRNA-seq%20datasets%2C%20scE2TM%20achieves%20superior%20clustering%20performance%20compared%20with%20seven%20state-of-the-art%20methods.%20A%20comprehensive%20interpretability%20benchmark%20further%20shows%20that%20scE2TM-learned%20topics%20exhibit%20higher%20diversity%20and%20stronger%20consistency%20with%20underlying%20biological%20pathways.%20Modeling%20interferon-stimulated%20PBMCs%2C%20scE2TM%20simulates%20topic%20perturbations%20that%20drive%20control%20cells%20toward%20stimulated-like%20transcriptional%20states%2C%20faithfully%20mirroring%20experimental%20interferon%20responses.%20In%20melanoma%2C%20scE2TM%20identifies%20malignant-specific%20topics%20and%20extrapolates%20them%20to%20unseen%20patient%20data%2C%20revealing%20gene%20programs%20associated%20with%20patient%20survival.&entry.1838667208=http%3A//arxiv.org/abs/2507.08355v2&entry.124074799=Read"},
{"title": "MRI Super-Resolution with Deep Learning: A Comprehensive Survey", "author": "Mohammad Khateri and Serge Vasylechko and Morteza Ghahremani and Liam Timms and Deniz Kocanaogullari and Simon K. Warfield and Camilo Jaimes and Davood Karimi and Alejandra Sierra and Jussi Tohka and Sila Kurugol and Onur Afacan", "abstract": "High-resolution (HR) magnetic resonance imaging (MRI) is crucial for many clinical and research applications. However, achieving it remains costly and constrained by technical trade-offs and experimental limitations. Super-resolution (SR) presents a promising computational approach to overcome these challenges by generating HR images from more affordable low-resolution (LR) scans, potentially improving diagnostic accuracy and efficiency without requiring additional hardware. This survey reviews recent advances in MRI SR techniques, with a focus on deep learning (DL) approaches. It examines DL-based MRI SR methods from the perspectives of computer vision, computational imaging, inverse problems, and MR physics, covering theoretical foundations, architectural designs, learning strategies, benchmark datasets, and performance metrics. We propose a systematic taxonomy to categorize these methods and present an in-depth study of both established and emerging SR techniques applicable to MRI, considering unique challenges in clinical and research contexts. We also highlight open challenges and directions that the community needs to address. Additionally, we provide a collection of essential open-access resources, tools, and tutorials, available on our GitHub: https://github.com/mkhateri/Awesome-MRI-Super-Resolution.\n  IEEE keywords: MRI, Super-Resolution, Deep Learning, Computational Imaging, Inverse Problem, Survey.", "link": "http://arxiv.org/abs/2511.16854v3", "date": "2025-12-02", "relevancy": 2.4493, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4949}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4877}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MRI%20Super-Resolution%20with%20Deep%20Learning%3A%20A%20Comprehensive%20Survey&body=Title%3A%20MRI%20Super-Resolution%20with%20Deep%20Learning%3A%20A%20Comprehensive%20Survey%0AAuthor%3A%20Mohammad%20Khateri%20and%20Serge%20Vasylechko%20and%20Morteza%20Ghahremani%20and%20Liam%20Timms%20and%20Deniz%20Kocanaogullari%20and%20Simon%20K.%20Warfield%20and%20Camilo%20Jaimes%20and%20Davood%20Karimi%20and%20Alejandra%20Sierra%20and%20Jussi%20Tohka%20and%20Sila%20Kurugol%20and%20Onur%20Afacan%0AAbstract%3A%20High-resolution%20%28HR%29%20magnetic%20resonance%20imaging%20%28MRI%29%20is%20crucial%20for%20many%20clinical%20and%20research%20applications.%20However%2C%20achieving%20it%20remains%20costly%20and%20constrained%20by%20technical%20trade-offs%20and%20experimental%20limitations.%20Super-resolution%20%28SR%29%20presents%20a%20promising%20computational%20approach%20to%20overcome%20these%20challenges%20by%20generating%20HR%20images%20from%20more%20affordable%20low-resolution%20%28LR%29%20scans%2C%20potentially%20improving%20diagnostic%20accuracy%20and%20efficiency%20without%20requiring%20additional%20hardware.%20This%20survey%20reviews%20recent%20advances%20in%20MRI%20SR%20techniques%2C%20with%20a%20focus%20on%20deep%20learning%20%28DL%29%20approaches.%20It%20examines%20DL-based%20MRI%20SR%20methods%20from%20the%20perspectives%20of%20computer%20vision%2C%20computational%20imaging%2C%20inverse%20problems%2C%20and%20MR%20physics%2C%20covering%20theoretical%20foundations%2C%20architectural%20designs%2C%20learning%20strategies%2C%20benchmark%20datasets%2C%20and%20performance%20metrics.%20We%20propose%20a%20systematic%20taxonomy%20to%20categorize%20these%20methods%20and%20present%20an%20in-depth%20study%20of%20both%20established%20and%20emerging%20SR%20techniques%20applicable%20to%20MRI%2C%20considering%20unique%20challenges%20in%20clinical%20and%20research%20contexts.%20We%20also%20highlight%20open%20challenges%20and%20directions%20that%20the%20community%20needs%20to%20address.%20Additionally%2C%20we%20provide%20a%20collection%20of%20essential%20open-access%20resources%2C%20tools%2C%20and%20tutorials%2C%20available%20on%20our%20GitHub%3A%20https%3A//github.com/mkhateri/Awesome-MRI-Super-Resolution.%0A%20%20IEEE%20keywords%3A%20MRI%2C%20Super-Resolution%2C%20Deep%20Learning%2C%20Computational%20Imaging%2C%20Inverse%20Problem%2C%20Survey.%0ALink%3A%20http%3A//arxiv.org/abs/2511.16854v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMRI%2520Super-Resolution%2520with%2520Deep%2520Learning%253A%2520A%2520Comprehensive%2520Survey%26entry.906535625%3DMohammad%2520Khateri%2520and%2520Serge%2520Vasylechko%2520and%2520Morteza%2520Ghahremani%2520and%2520Liam%2520Timms%2520and%2520Deniz%2520Kocanaogullari%2520and%2520Simon%2520K.%2520Warfield%2520and%2520Camilo%2520Jaimes%2520and%2520Davood%2520Karimi%2520and%2520Alejandra%2520Sierra%2520and%2520Jussi%2520Tohka%2520and%2520Sila%2520Kurugol%2520and%2520Onur%2520Afacan%26entry.1292438233%3DHigh-resolution%2520%2528HR%2529%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520is%2520crucial%2520for%2520many%2520clinical%2520and%2520research%2520applications.%2520However%252C%2520achieving%2520it%2520remains%2520costly%2520and%2520constrained%2520by%2520technical%2520trade-offs%2520and%2520experimental%2520limitations.%2520Super-resolution%2520%2528SR%2529%2520presents%2520a%2520promising%2520computational%2520approach%2520to%2520overcome%2520these%2520challenges%2520by%2520generating%2520HR%2520images%2520from%2520more%2520affordable%2520low-resolution%2520%2528LR%2529%2520scans%252C%2520potentially%2520improving%2520diagnostic%2520accuracy%2520and%2520efficiency%2520without%2520requiring%2520additional%2520hardware.%2520This%2520survey%2520reviews%2520recent%2520advances%2520in%2520MRI%2520SR%2520techniques%252C%2520with%2520a%2520focus%2520on%2520deep%2520learning%2520%2528DL%2529%2520approaches.%2520It%2520examines%2520DL-based%2520MRI%2520SR%2520methods%2520from%2520the%2520perspectives%2520of%2520computer%2520vision%252C%2520computational%2520imaging%252C%2520inverse%2520problems%252C%2520and%2520MR%2520physics%252C%2520covering%2520theoretical%2520foundations%252C%2520architectural%2520designs%252C%2520learning%2520strategies%252C%2520benchmark%2520datasets%252C%2520and%2520performance%2520metrics.%2520We%2520propose%2520a%2520systematic%2520taxonomy%2520to%2520categorize%2520these%2520methods%2520and%2520present%2520an%2520in-depth%2520study%2520of%2520both%2520established%2520and%2520emerging%2520SR%2520techniques%2520applicable%2520to%2520MRI%252C%2520considering%2520unique%2520challenges%2520in%2520clinical%2520and%2520research%2520contexts.%2520We%2520also%2520highlight%2520open%2520challenges%2520and%2520directions%2520that%2520the%2520community%2520needs%2520to%2520address.%2520Additionally%252C%2520we%2520provide%2520a%2520collection%2520of%2520essential%2520open-access%2520resources%252C%2520tools%252C%2520and%2520tutorials%252C%2520available%2520on%2520our%2520GitHub%253A%2520https%253A//github.com/mkhateri/Awesome-MRI-Super-Resolution.%250A%2520%2520IEEE%2520keywords%253A%2520MRI%252C%2520Super-Resolution%252C%2520Deep%2520Learning%252C%2520Computational%2520Imaging%252C%2520Inverse%2520Problem%252C%2520Survey.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.16854v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MRI%20Super-Resolution%20with%20Deep%20Learning%3A%20A%20Comprehensive%20Survey&entry.906535625=Mohammad%20Khateri%20and%20Serge%20Vasylechko%20and%20Morteza%20Ghahremani%20and%20Liam%20Timms%20and%20Deniz%20Kocanaogullari%20and%20Simon%20K.%20Warfield%20and%20Camilo%20Jaimes%20and%20Davood%20Karimi%20and%20Alejandra%20Sierra%20and%20Jussi%20Tohka%20and%20Sila%20Kurugol%20and%20Onur%20Afacan&entry.1292438233=High-resolution%20%28HR%29%20magnetic%20resonance%20imaging%20%28MRI%29%20is%20crucial%20for%20many%20clinical%20and%20research%20applications.%20However%2C%20achieving%20it%20remains%20costly%20and%20constrained%20by%20technical%20trade-offs%20and%20experimental%20limitations.%20Super-resolution%20%28SR%29%20presents%20a%20promising%20computational%20approach%20to%20overcome%20these%20challenges%20by%20generating%20HR%20images%20from%20more%20affordable%20low-resolution%20%28LR%29%20scans%2C%20potentially%20improving%20diagnostic%20accuracy%20and%20efficiency%20without%20requiring%20additional%20hardware.%20This%20survey%20reviews%20recent%20advances%20in%20MRI%20SR%20techniques%2C%20with%20a%20focus%20on%20deep%20learning%20%28DL%29%20approaches.%20It%20examines%20DL-based%20MRI%20SR%20methods%20from%20the%20perspectives%20of%20computer%20vision%2C%20computational%20imaging%2C%20inverse%20problems%2C%20and%20MR%20physics%2C%20covering%20theoretical%20foundations%2C%20architectural%20designs%2C%20learning%20strategies%2C%20benchmark%20datasets%2C%20and%20performance%20metrics.%20We%20propose%20a%20systematic%20taxonomy%20to%20categorize%20these%20methods%20and%20present%20an%20in-depth%20study%20of%20both%20established%20and%20emerging%20SR%20techniques%20applicable%20to%20MRI%2C%20considering%20unique%20challenges%20in%20clinical%20and%20research%20contexts.%20We%20also%20highlight%20open%20challenges%20and%20directions%20that%20the%20community%20needs%20to%20address.%20Additionally%2C%20we%20provide%20a%20collection%20of%20essential%20open-access%20resources%2C%20tools%2C%20and%20tutorials%2C%20available%20on%20our%20GitHub%3A%20https%3A//github.com/mkhateri/Awesome-MRI-Super-Resolution.%0A%20%20IEEE%20keywords%3A%20MRI%2C%20Super-Resolution%2C%20Deep%20Learning%2C%20Computational%20Imaging%2C%20Inverse%20Problem%2C%20Survey.&entry.1838667208=http%3A//arxiv.org/abs/2511.16854v3&entry.124074799=Read"},
{"title": "On the Temporal Question-Answering Capabilities of Large Language Models Over Anonymized Data", "author": "Alfredo Garrach\u00f3n Ruiz and Tom\u00e1s de la Rosa and Daniel Borrajo", "abstract": "The applicability of Large Language Models (LLMs) in temporal reasoning tasks over data that is not present during training is still a field that remains to be explored. In this paper we work on this topic, focusing on structured and semi-structured anonymized data. We not only develop a direct LLM pipeline, but also compare various methodologies and conduct an in-depth analysis. We identified and examined seventeen common temporal reasoning tasks in natural language, focusing on their algorithmic components. To assess LLM performance, we created the \\textit{Reasoning and Answering Temporal Ability} dataset (RATA), featuring semi-structured anonymized data to ensure reliance on reasoning rather than on prior knowledge. We compared several methodologies, involving SoTA techniques such as Tree-of-Thought, self-reflexion and code execution, tuned specifically for this scenario. Our results suggest that achieving scalable and reliable solutions requires more than just standalone LLMs, highlighting the need for integrated approaches.", "link": "http://arxiv.org/abs/2504.07646v2", "date": "2025-12-02", "relevancy": 2.4265, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4962}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4845}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Temporal%20Question-Answering%20Capabilities%20of%20Large%20Language%20Models%20Over%20Anonymized%20Data&body=Title%3A%20On%20the%20Temporal%20Question-Answering%20Capabilities%20of%20Large%20Language%20Models%20Over%20Anonymized%20Data%0AAuthor%3A%20Alfredo%20Garrach%C3%B3n%20Ruiz%20and%20Tom%C3%A1s%20de%20la%20Rosa%20and%20Daniel%20Borrajo%0AAbstract%3A%20The%20applicability%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20temporal%20reasoning%20tasks%20over%20data%20that%20is%20not%20present%20during%20training%20is%20still%20a%20field%20that%20remains%20to%20be%20explored.%20In%20this%20paper%20we%20work%20on%20this%20topic%2C%20focusing%20on%20structured%20and%20semi-structured%20anonymized%20data.%20We%20not%20only%20develop%20a%20direct%20LLM%20pipeline%2C%20but%20also%20compare%20various%20methodologies%20and%20conduct%20an%20in-depth%20analysis.%20We%20identified%20and%20examined%20seventeen%20common%20temporal%20reasoning%20tasks%20in%20natural%20language%2C%20focusing%20on%20their%20algorithmic%20components.%20To%20assess%20LLM%20performance%2C%20we%20created%20the%20%5Ctextit%7BReasoning%20and%20Answering%20Temporal%20Ability%7D%20dataset%20%28RATA%29%2C%20featuring%20semi-structured%20anonymized%20data%20to%20ensure%20reliance%20on%20reasoning%20rather%20than%20on%20prior%20knowledge.%20We%20compared%20several%20methodologies%2C%20involving%20SoTA%20techniques%20such%20as%20Tree-of-Thought%2C%20self-reflexion%20and%20code%20execution%2C%20tuned%20specifically%20for%20this%20scenario.%20Our%20results%20suggest%20that%20achieving%20scalable%20and%20reliable%20solutions%20requires%20more%20than%20just%20standalone%20LLMs%2C%20highlighting%20the%20need%20for%20integrated%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2504.07646v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Temporal%2520Question-Answering%2520Capabilities%2520of%2520Large%2520Language%2520Models%2520Over%2520Anonymized%2520Data%26entry.906535625%3DAlfredo%2520Garrach%25C3%25B3n%2520Ruiz%2520and%2520Tom%25C3%25A1s%2520de%2520la%2520Rosa%2520and%2520Daniel%2520Borrajo%26entry.1292438233%3DThe%2520applicability%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520in%2520temporal%2520reasoning%2520tasks%2520over%2520data%2520that%2520is%2520not%2520present%2520during%2520training%2520is%2520still%2520a%2520field%2520that%2520remains%2520to%2520be%2520explored.%2520In%2520this%2520paper%2520we%2520work%2520on%2520this%2520topic%252C%2520focusing%2520on%2520structured%2520and%2520semi-structured%2520anonymized%2520data.%2520We%2520not%2520only%2520develop%2520a%2520direct%2520LLM%2520pipeline%252C%2520but%2520also%2520compare%2520various%2520methodologies%2520and%2520conduct%2520an%2520in-depth%2520analysis.%2520We%2520identified%2520and%2520examined%2520seventeen%2520common%2520temporal%2520reasoning%2520tasks%2520in%2520natural%2520language%252C%2520focusing%2520on%2520their%2520algorithmic%2520components.%2520To%2520assess%2520LLM%2520performance%252C%2520we%2520created%2520the%2520%255Ctextit%257BReasoning%2520and%2520Answering%2520Temporal%2520Ability%257D%2520dataset%2520%2528RATA%2529%252C%2520featuring%2520semi-structured%2520anonymized%2520data%2520to%2520ensure%2520reliance%2520on%2520reasoning%2520rather%2520than%2520on%2520prior%2520knowledge.%2520We%2520compared%2520several%2520methodologies%252C%2520involving%2520SoTA%2520techniques%2520such%2520as%2520Tree-of-Thought%252C%2520self-reflexion%2520and%2520code%2520execution%252C%2520tuned%2520specifically%2520for%2520this%2520scenario.%2520Our%2520results%2520suggest%2520that%2520achieving%2520scalable%2520and%2520reliable%2520solutions%2520requires%2520more%2520than%2520just%2520standalone%2520LLMs%252C%2520highlighting%2520the%2520need%2520for%2520integrated%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.07646v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Temporal%20Question-Answering%20Capabilities%20of%20Large%20Language%20Models%20Over%20Anonymized%20Data&entry.906535625=Alfredo%20Garrach%C3%B3n%20Ruiz%20and%20Tom%C3%A1s%20de%20la%20Rosa%20and%20Daniel%20Borrajo&entry.1292438233=The%20applicability%20of%20Large%20Language%20Models%20%28LLMs%29%20in%20temporal%20reasoning%20tasks%20over%20data%20that%20is%20not%20present%20during%20training%20is%20still%20a%20field%20that%20remains%20to%20be%20explored.%20In%20this%20paper%20we%20work%20on%20this%20topic%2C%20focusing%20on%20structured%20and%20semi-structured%20anonymized%20data.%20We%20not%20only%20develop%20a%20direct%20LLM%20pipeline%2C%20but%20also%20compare%20various%20methodologies%20and%20conduct%20an%20in-depth%20analysis.%20We%20identified%20and%20examined%20seventeen%20common%20temporal%20reasoning%20tasks%20in%20natural%20language%2C%20focusing%20on%20their%20algorithmic%20components.%20To%20assess%20LLM%20performance%2C%20we%20created%20the%20%5Ctextit%7BReasoning%20and%20Answering%20Temporal%20Ability%7D%20dataset%20%28RATA%29%2C%20featuring%20semi-structured%20anonymized%20data%20to%20ensure%20reliance%20on%20reasoning%20rather%20than%20on%20prior%20knowledge.%20We%20compared%20several%20methodologies%2C%20involving%20SoTA%20techniques%20such%20as%20Tree-of-Thought%2C%20self-reflexion%20and%20code%20execution%2C%20tuned%20specifically%20for%20this%20scenario.%20Our%20results%20suggest%20that%20achieving%20scalable%20and%20reliable%20solutions%20requires%20more%20than%20just%20standalone%20LLMs%2C%20highlighting%20the%20need%20for%20integrated%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2504.07646v2&entry.124074799=Read"},
{"title": "Taming Camera-Controlled Video Generation with Verifiable Geometry Reward", "author": "Zhaoqing Wang and Xiaobo Xia and Zhuolin Bie and Jinlin Liu and Dongdong Yu and Jia-Wang Bian and Changhu Wang", "abstract": "Recent advances in video diffusion models have remarkably improved camera-controlled video generation, but most methods rely solely on supervised fine-tuning (SFT), leaving online reinforcement learning (RL) post-training largely underexplored. In this work, we introduce an online RL post-training framework that optimizes a pretrained video generator for precise camera control. To make RL effective in this setting, we design a verifiable geometry reward that delivers dense segment-level feedback to guide model optimization. Specifically, we estimate the 3D camera trajectories for both generated and reference videos, divide each trajectory into short segments, and compute segment-wise relative poses. The reward function then compares each generated-reference segment pair and assigns an alignment score as the reward signal, which helps alleviate reward sparsity and improve optimization efficiency. Moreover, we construct a comprehensive dataset featuring diverse large-amplitude camera motions and scenes with varied subject dynamics. Extensive experiments show that our online RL post-training clearly outperforms SFT baselines across multiple aspects, including camera-control accuracy, geometric consistency, and visual quality, demonstrating its superiority in advancing camera-controlled video generation.", "link": "http://arxiv.org/abs/2512.02870v1", "date": "2025-12-02", "relevancy": 2.422, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6516}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5987}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5939}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Taming%20Camera-Controlled%20Video%20Generation%20with%20Verifiable%20Geometry%20Reward&body=Title%3A%20Taming%20Camera-Controlled%20Video%20Generation%20with%20Verifiable%20Geometry%20Reward%0AAuthor%3A%20Zhaoqing%20Wang%20and%20Xiaobo%20Xia%20and%20Zhuolin%20Bie%20and%20Jinlin%20Liu%20and%20Dongdong%20Yu%20and%20Jia-Wang%20Bian%20and%20Changhu%20Wang%0AAbstract%3A%20Recent%20advances%20in%20video%20diffusion%20models%20have%20remarkably%20improved%20camera-controlled%20video%20generation%2C%20but%20most%20methods%20rely%20solely%20on%20supervised%20fine-tuning%20%28SFT%29%2C%20leaving%20online%20reinforcement%20learning%20%28RL%29%20post-training%20largely%20underexplored.%20In%20this%20work%2C%20we%20introduce%20an%20online%20RL%20post-training%20framework%20that%20optimizes%20a%20pretrained%20video%20generator%20for%20precise%20camera%20control.%20To%20make%20RL%20effective%20in%20this%20setting%2C%20we%20design%20a%20verifiable%20geometry%20reward%20that%20delivers%20dense%20segment-level%20feedback%20to%20guide%20model%20optimization.%20Specifically%2C%20we%20estimate%20the%203D%20camera%20trajectories%20for%20both%20generated%20and%20reference%20videos%2C%20divide%20each%20trajectory%20into%20short%20segments%2C%20and%20compute%20segment-wise%20relative%20poses.%20The%20reward%20function%20then%20compares%20each%20generated-reference%20segment%20pair%20and%20assigns%20an%20alignment%20score%20as%20the%20reward%20signal%2C%20which%20helps%20alleviate%20reward%20sparsity%20and%20improve%20optimization%20efficiency.%20Moreover%2C%20we%20construct%20a%20comprehensive%20dataset%20featuring%20diverse%20large-amplitude%20camera%20motions%20and%20scenes%20with%20varied%20subject%20dynamics.%20Extensive%20experiments%20show%20that%20our%20online%20RL%20post-training%20clearly%20outperforms%20SFT%20baselines%20across%20multiple%20aspects%2C%20including%20camera-control%20accuracy%2C%20geometric%20consistency%2C%20and%20visual%20quality%2C%20demonstrating%20its%20superiority%20in%20advancing%20camera-controlled%20video%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02870v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaming%2520Camera-Controlled%2520Video%2520Generation%2520with%2520Verifiable%2520Geometry%2520Reward%26entry.906535625%3DZhaoqing%2520Wang%2520and%2520Xiaobo%2520Xia%2520and%2520Zhuolin%2520Bie%2520and%2520Jinlin%2520Liu%2520and%2520Dongdong%2520Yu%2520and%2520Jia-Wang%2520Bian%2520and%2520Changhu%2520Wang%26entry.1292438233%3DRecent%2520advances%2520in%2520video%2520diffusion%2520models%2520have%2520remarkably%2520improved%2520camera-controlled%2520video%2520generation%252C%2520but%2520most%2520methods%2520rely%2520solely%2520on%2520supervised%2520fine-tuning%2520%2528SFT%2529%252C%2520leaving%2520online%2520reinforcement%2520learning%2520%2528RL%2529%2520post-training%2520largely%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520introduce%2520an%2520online%2520RL%2520post-training%2520framework%2520that%2520optimizes%2520a%2520pretrained%2520video%2520generator%2520for%2520precise%2520camera%2520control.%2520To%2520make%2520RL%2520effective%2520in%2520this%2520setting%252C%2520we%2520design%2520a%2520verifiable%2520geometry%2520reward%2520that%2520delivers%2520dense%2520segment-level%2520feedback%2520to%2520guide%2520model%2520optimization.%2520Specifically%252C%2520we%2520estimate%2520the%25203D%2520camera%2520trajectories%2520for%2520both%2520generated%2520and%2520reference%2520videos%252C%2520divide%2520each%2520trajectory%2520into%2520short%2520segments%252C%2520and%2520compute%2520segment-wise%2520relative%2520poses.%2520The%2520reward%2520function%2520then%2520compares%2520each%2520generated-reference%2520segment%2520pair%2520and%2520assigns%2520an%2520alignment%2520score%2520as%2520the%2520reward%2520signal%252C%2520which%2520helps%2520alleviate%2520reward%2520sparsity%2520and%2520improve%2520optimization%2520efficiency.%2520Moreover%252C%2520we%2520construct%2520a%2520comprehensive%2520dataset%2520featuring%2520diverse%2520large-amplitude%2520camera%2520motions%2520and%2520scenes%2520with%2520varied%2520subject%2520dynamics.%2520Extensive%2520experiments%2520show%2520that%2520our%2520online%2520RL%2520post-training%2520clearly%2520outperforms%2520SFT%2520baselines%2520across%2520multiple%2520aspects%252C%2520including%2520camera-control%2520accuracy%252C%2520geometric%2520consistency%252C%2520and%2520visual%2520quality%252C%2520demonstrating%2520its%2520superiority%2520in%2520advancing%2520camera-controlled%2520video%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02870v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Taming%20Camera-Controlled%20Video%20Generation%20with%20Verifiable%20Geometry%20Reward&entry.906535625=Zhaoqing%20Wang%20and%20Xiaobo%20Xia%20and%20Zhuolin%20Bie%20and%20Jinlin%20Liu%20and%20Dongdong%20Yu%20and%20Jia-Wang%20Bian%20and%20Changhu%20Wang&entry.1292438233=Recent%20advances%20in%20video%20diffusion%20models%20have%20remarkably%20improved%20camera-controlled%20video%20generation%2C%20but%20most%20methods%20rely%20solely%20on%20supervised%20fine-tuning%20%28SFT%29%2C%20leaving%20online%20reinforcement%20learning%20%28RL%29%20post-training%20largely%20underexplored.%20In%20this%20work%2C%20we%20introduce%20an%20online%20RL%20post-training%20framework%20that%20optimizes%20a%20pretrained%20video%20generator%20for%20precise%20camera%20control.%20To%20make%20RL%20effective%20in%20this%20setting%2C%20we%20design%20a%20verifiable%20geometry%20reward%20that%20delivers%20dense%20segment-level%20feedback%20to%20guide%20model%20optimization.%20Specifically%2C%20we%20estimate%20the%203D%20camera%20trajectories%20for%20both%20generated%20and%20reference%20videos%2C%20divide%20each%20trajectory%20into%20short%20segments%2C%20and%20compute%20segment-wise%20relative%20poses.%20The%20reward%20function%20then%20compares%20each%20generated-reference%20segment%20pair%20and%20assigns%20an%20alignment%20score%20as%20the%20reward%20signal%2C%20which%20helps%20alleviate%20reward%20sparsity%20and%20improve%20optimization%20efficiency.%20Moreover%2C%20we%20construct%20a%20comprehensive%20dataset%20featuring%20diverse%20large-amplitude%20camera%20motions%20and%20scenes%20with%20varied%20subject%20dynamics.%20Extensive%20experiments%20show%20that%20our%20online%20RL%20post-training%20clearly%20outperforms%20SFT%20baselines%20across%20multiple%20aspects%2C%20including%20camera-control%20accuracy%2C%20geometric%20consistency%2C%20and%20visual%20quality%2C%20demonstrating%20its%20superiority%20in%20advancing%20camera-controlled%20video%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2512.02870v1&entry.124074799=Read"},
{"title": "BEVDilation: LiDAR-Centric Multi-Modal Fusion for 3D Object Detection", "author": "Guowen Zhang and Chenhang He and Liyi Chen and Lei Zhang", "abstract": "Integrating LiDAR and camera information in the bird's eye view (BEV) representation has demonstrated its effectiveness in 3D object detection. However, because of the fundamental disparity in geometric accuracy between these sensors, indiscriminate fusion in previous methods often leads to degraded performance. In this paper, we propose BEVDilation, a novel LiDAR-centric framework that prioritizes LiDAR information in the fusion. By formulating image BEV features as implicit guidance rather than naive concatenation, our strategy effectively alleviates the spatial misalignment caused by image depth estimation errors. Furthermore, the image guidance can effectively help the LiDAR-centric paradigm to address the sparsity and semantic limitations of point clouds. Specifically, we propose a Sparse Voxel Dilation Block that mitigates the inherent point sparsity by densifying foreground voxels through image priors. Moreover, we introduce a Semantic-Guided BEV Dilation Block to enhance the LiDAR feature diffusion processing with image semantic guidance and long-range context capture. On the challenging nuScenes benchmark, BEVDilation achieves better performance than state-of-the-art methods while maintaining competitive computational efficiency. Importantly, our LiDAR-centric strategy demonstrates greater robustness to depth noise compared to naive fusion. The source code is available at https://github.com/gwenzhang/BEVDilation.", "link": "http://arxiv.org/abs/2512.02972v1", "date": "2025-12-02", "relevancy": 2.421, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6097}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6093}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5992}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BEVDilation%3A%20LiDAR-Centric%20Multi-Modal%20Fusion%20for%203D%20Object%20Detection&body=Title%3A%20BEVDilation%3A%20LiDAR-Centric%20Multi-Modal%20Fusion%20for%203D%20Object%20Detection%0AAuthor%3A%20Guowen%20Zhang%20and%20Chenhang%20He%20and%20Liyi%20Chen%20and%20Lei%20Zhang%0AAbstract%3A%20Integrating%20LiDAR%20and%20camera%20information%20in%20the%20bird%27s%20eye%20view%20%28BEV%29%20representation%20has%20demonstrated%20its%20effectiveness%20in%203D%20object%20detection.%20However%2C%20because%20of%20the%20fundamental%20disparity%20in%20geometric%20accuracy%20between%20these%20sensors%2C%20indiscriminate%20fusion%20in%20previous%20methods%20often%20leads%20to%20degraded%20performance.%20In%20this%20paper%2C%20we%20propose%20BEVDilation%2C%20a%20novel%20LiDAR-centric%20framework%20that%20prioritizes%20LiDAR%20information%20in%20the%20fusion.%20By%20formulating%20image%20BEV%20features%20as%20implicit%20guidance%20rather%20than%20naive%20concatenation%2C%20our%20strategy%20effectively%20alleviates%20the%20spatial%20misalignment%20caused%20by%20image%20depth%20estimation%20errors.%20Furthermore%2C%20the%20image%20guidance%20can%20effectively%20help%20the%20LiDAR-centric%20paradigm%20to%20address%20the%20sparsity%20and%20semantic%20limitations%20of%20point%20clouds.%20Specifically%2C%20we%20propose%20a%20Sparse%20Voxel%20Dilation%20Block%20that%20mitigates%20the%20inherent%20point%20sparsity%20by%20densifying%20foreground%20voxels%20through%20image%20priors.%20Moreover%2C%20we%20introduce%20a%20Semantic-Guided%20BEV%20Dilation%20Block%20to%20enhance%20the%20LiDAR%20feature%20diffusion%20processing%20with%20image%20semantic%20guidance%20and%20long-range%20context%20capture.%20On%20the%20challenging%20nuScenes%20benchmark%2C%20BEVDilation%20achieves%20better%20performance%20than%20state-of-the-art%20methods%20while%20maintaining%20competitive%20computational%20efficiency.%20Importantly%2C%20our%20LiDAR-centric%20strategy%20demonstrates%20greater%20robustness%20to%20depth%20noise%20compared%20to%20naive%20fusion.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/gwenzhang/BEVDilation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02972v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBEVDilation%253A%2520LiDAR-Centric%2520Multi-Modal%2520Fusion%2520for%25203D%2520Object%2520Detection%26entry.906535625%3DGuowen%2520Zhang%2520and%2520Chenhang%2520He%2520and%2520Liyi%2520Chen%2520and%2520Lei%2520Zhang%26entry.1292438233%3DIntegrating%2520LiDAR%2520and%2520camera%2520information%2520in%2520the%2520bird%2527s%2520eye%2520view%2520%2528BEV%2529%2520representation%2520has%2520demonstrated%2520its%2520effectiveness%2520in%25203D%2520object%2520detection.%2520However%252C%2520because%2520of%2520the%2520fundamental%2520disparity%2520in%2520geometric%2520accuracy%2520between%2520these%2520sensors%252C%2520indiscriminate%2520fusion%2520in%2520previous%2520methods%2520often%2520leads%2520to%2520degraded%2520performance.%2520In%2520this%2520paper%252C%2520we%2520propose%2520BEVDilation%252C%2520a%2520novel%2520LiDAR-centric%2520framework%2520that%2520prioritizes%2520LiDAR%2520information%2520in%2520the%2520fusion.%2520By%2520formulating%2520image%2520BEV%2520features%2520as%2520implicit%2520guidance%2520rather%2520than%2520naive%2520concatenation%252C%2520our%2520strategy%2520effectively%2520alleviates%2520the%2520spatial%2520misalignment%2520caused%2520by%2520image%2520depth%2520estimation%2520errors.%2520Furthermore%252C%2520the%2520image%2520guidance%2520can%2520effectively%2520help%2520the%2520LiDAR-centric%2520paradigm%2520to%2520address%2520the%2520sparsity%2520and%2520semantic%2520limitations%2520of%2520point%2520clouds.%2520Specifically%252C%2520we%2520propose%2520a%2520Sparse%2520Voxel%2520Dilation%2520Block%2520that%2520mitigates%2520the%2520inherent%2520point%2520sparsity%2520by%2520densifying%2520foreground%2520voxels%2520through%2520image%2520priors.%2520Moreover%252C%2520we%2520introduce%2520a%2520Semantic-Guided%2520BEV%2520Dilation%2520Block%2520to%2520enhance%2520the%2520LiDAR%2520feature%2520diffusion%2520processing%2520with%2520image%2520semantic%2520guidance%2520and%2520long-range%2520context%2520capture.%2520On%2520the%2520challenging%2520nuScenes%2520benchmark%252C%2520BEVDilation%2520achieves%2520better%2520performance%2520than%2520state-of-the-art%2520methods%2520while%2520maintaining%2520competitive%2520computational%2520efficiency.%2520Importantly%252C%2520our%2520LiDAR-centric%2520strategy%2520demonstrates%2520greater%2520robustness%2520to%2520depth%2520noise%2520compared%2520to%2520naive%2520fusion.%2520The%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/gwenzhang/BEVDilation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02972v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BEVDilation%3A%20LiDAR-Centric%20Multi-Modal%20Fusion%20for%203D%20Object%20Detection&entry.906535625=Guowen%20Zhang%20and%20Chenhang%20He%20and%20Liyi%20Chen%20and%20Lei%20Zhang&entry.1292438233=Integrating%20LiDAR%20and%20camera%20information%20in%20the%20bird%27s%20eye%20view%20%28BEV%29%20representation%20has%20demonstrated%20its%20effectiveness%20in%203D%20object%20detection.%20However%2C%20because%20of%20the%20fundamental%20disparity%20in%20geometric%20accuracy%20between%20these%20sensors%2C%20indiscriminate%20fusion%20in%20previous%20methods%20often%20leads%20to%20degraded%20performance.%20In%20this%20paper%2C%20we%20propose%20BEVDilation%2C%20a%20novel%20LiDAR-centric%20framework%20that%20prioritizes%20LiDAR%20information%20in%20the%20fusion.%20By%20formulating%20image%20BEV%20features%20as%20implicit%20guidance%20rather%20than%20naive%20concatenation%2C%20our%20strategy%20effectively%20alleviates%20the%20spatial%20misalignment%20caused%20by%20image%20depth%20estimation%20errors.%20Furthermore%2C%20the%20image%20guidance%20can%20effectively%20help%20the%20LiDAR-centric%20paradigm%20to%20address%20the%20sparsity%20and%20semantic%20limitations%20of%20point%20clouds.%20Specifically%2C%20we%20propose%20a%20Sparse%20Voxel%20Dilation%20Block%20that%20mitigates%20the%20inherent%20point%20sparsity%20by%20densifying%20foreground%20voxels%20through%20image%20priors.%20Moreover%2C%20we%20introduce%20a%20Semantic-Guided%20BEV%20Dilation%20Block%20to%20enhance%20the%20LiDAR%20feature%20diffusion%20processing%20with%20image%20semantic%20guidance%20and%20long-range%20context%20capture.%20On%20the%20challenging%20nuScenes%20benchmark%2C%20BEVDilation%20achieves%20better%20performance%20than%20state-of-the-art%20methods%20while%20maintaining%20competitive%20computational%20efficiency.%20Importantly%2C%20our%20LiDAR-centric%20strategy%20demonstrates%20greater%20robustness%20to%20depth%20noise%20compared%20to%20naive%20fusion.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/gwenzhang/BEVDilation.&entry.1838667208=http%3A//arxiv.org/abs/2512.02972v1&entry.124074799=Read"},
{"title": "Invasive Context Engineering to Control Large Language Models", "author": "Thomas Rivasseau", "abstract": "Current research on operator control of Large Language Models improves model robustness against adversarial attacks and misbehavior by training on preference examples, prompting, and input/output filtering. Despite good results, LLMs remain susceptible to abuse, and jailbreak probability increases with context length. There is a need for robust LLM security guarantees in long-context situations. We propose control sentences inserted into the LLM context as invasive context engineering to partially solve the problem. We suggest this technique can be generalized to the Chain-of-Thought process to prevent scheming. Invasive Context Engineering does not rely on LLM training, avoiding data shortage pitfalls which arise in training models for long context situations.", "link": "http://arxiv.org/abs/2512.03001v1", "date": "2025-12-02", "relevancy": 2.42, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4852}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4834}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Invasive%20Context%20Engineering%20to%20Control%20Large%20Language%20Models&body=Title%3A%20Invasive%20Context%20Engineering%20to%20Control%20Large%20Language%20Models%0AAuthor%3A%20Thomas%20Rivasseau%0AAbstract%3A%20Current%20research%20on%20operator%20control%20of%20Large%20Language%20Models%20improves%20model%20robustness%20against%20adversarial%20attacks%20and%20misbehavior%20by%20training%20on%20preference%20examples%2C%20prompting%2C%20and%20input/output%20filtering.%20Despite%20good%20results%2C%20LLMs%20remain%20susceptible%20to%20abuse%2C%20and%20jailbreak%20probability%20increases%20with%20context%20length.%20There%20is%20a%20need%20for%20robust%20LLM%20security%20guarantees%20in%20long-context%20situations.%20We%20propose%20control%20sentences%20inserted%20into%20the%20LLM%20context%20as%20invasive%20context%20engineering%20to%20partially%20solve%20the%20problem.%20We%20suggest%20this%20technique%20can%20be%20generalized%20to%20the%20Chain-of-Thought%20process%20to%20prevent%20scheming.%20Invasive%20Context%20Engineering%20does%20not%20rely%20on%20LLM%20training%2C%20avoiding%20data%20shortage%20pitfalls%20which%20arise%20in%20training%20models%20for%20long%20context%20situations.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03001v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvasive%2520Context%2520Engineering%2520to%2520Control%2520Large%2520Language%2520Models%26entry.906535625%3DThomas%2520Rivasseau%26entry.1292438233%3DCurrent%2520research%2520on%2520operator%2520control%2520of%2520Large%2520Language%2520Models%2520improves%2520model%2520robustness%2520against%2520adversarial%2520attacks%2520and%2520misbehavior%2520by%2520training%2520on%2520preference%2520examples%252C%2520prompting%252C%2520and%2520input/output%2520filtering.%2520Despite%2520good%2520results%252C%2520LLMs%2520remain%2520susceptible%2520to%2520abuse%252C%2520and%2520jailbreak%2520probability%2520increases%2520with%2520context%2520length.%2520There%2520is%2520a%2520need%2520for%2520robust%2520LLM%2520security%2520guarantees%2520in%2520long-context%2520situations.%2520We%2520propose%2520control%2520sentences%2520inserted%2520into%2520the%2520LLM%2520context%2520as%2520invasive%2520context%2520engineering%2520to%2520partially%2520solve%2520the%2520problem.%2520We%2520suggest%2520this%2520technique%2520can%2520be%2520generalized%2520to%2520the%2520Chain-of-Thought%2520process%2520to%2520prevent%2520scheming.%2520Invasive%2520Context%2520Engineering%2520does%2520not%2520rely%2520on%2520LLM%2520training%252C%2520avoiding%2520data%2520shortage%2520pitfalls%2520which%2520arise%2520in%2520training%2520models%2520for%2520long%2520context%2520situations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03001v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Invasive%20Context%20Engineering%20to%20Control%20Large%20Language%20Models&entry.906535625=Thomas%20Rivasseau&entry.1292438233=Current%20research%20on%20operator%20control%20of%20Large%20Language%20Models%20improves%20model%20robustness%20against%20adversarial%20attacks%20and%20misbehavior%20by%20training%20on%20preference%20examples%2C%20prompting%2C%20and%20input/output%20filtering.%20Despite%20good%20results%2C%20LLMs%20remain%20susceptible%20to%20abuse%2C%20and%20jailbreak%20probability%20increases%20with%20context%20length.%20There%20is%20a%20need%20for%20robust%20LLM%20security%20guarantees%20in%20long-context%20situations.%20We%20propose%20control%20sentences%20inserted%20into%20the%20LLM%20context%20as%20invasive%20context%20engineering%20to%20partially%20solve%20the%20problem.%20We%20suggest%20this%20technique%20can%20be%20generalized%20to%20the%20Chain-of-Thought%20process%20to%20prevent%20scheming.%20Invasive%20Context%20Engineering%20does%20not%20rely%20on%20LLM%20training%2C%20avoiding%20data%20shortage%20pitfalls%20which%20arise%20in%20training%20models%20for%20long%20context%20situations.&entry.1838667208=http%3A//arxiv.org/abs/2512.03001v1&entry.124074799=Read"},
{"title": "End-to-End Multi-Person Pose Estimation with Pose-Aware Video Transformer", "author": "Yonghui Yu and Jiahang Cai and Xun Wang and Wenwu Yang", "abstract": "Existing multi-person video pose estimation methods typically adopt a two-stage pipeline: detecting individuals in each frame, followed by temporal modeling for single person pose estimation. This design relies on heuristic operations such as detection, RoI cropping, and non-maximum suppression (NMS), limiting both accuracy and efficiency. In this paper, we present a fully end-to-end framework for multi-person 2D pose estimation in videos, effectively eliminating heuristic operations. A key challenge is to associate individuals across frames under complex and overlapping temporal trajectories. To address this, we introduce a novel Pose-Aware Video transformEr Network (PAVE-Net), which features a spatial encoder to model intra-frame relations and a spatiotemporal pose decoder to capture global dependencies across frames. To achieve accurate temporal association, we propose a pose-aware attention mechanism that enables each pose query to selectively aggregate features corresponding to the same individual across consecutive frames. Additionally, we explicitly model spatiotemporal dependencies among pose keypoints to improve accuracy. Notably, our approach is the first end-to-end method for multi-frame 2D human pose estimation. Extensive experiments show that PAVE-Net substantially outperforms prior image-based end-to-end methods, achieving a 6.0 mAP improvement on PoseTrack2017, and delivers accuracy competitive with state-of-the-art two-stage video based approaches, while offering significant gains in efficiency. Project page: https://github.com/zgspose/PAVENet.", "link": "http://arxiv.org/abs/2511.13208v2", "date": "2025-12-02", "relevancy": 2.4186, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6204}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6045}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20End-to-End%20Multi-Person%20Pose%20Estimation%20with%20Pose-Aware%20Video%20Transformer&body=Title%3A%20End-to-End%20Multi-Person%20Pose%20Estimation%20with%20Pose-Aware%20Video%20Transformer%0AAuthor%3A%20Yonghui%20Yu%20and%20Jiahang%20Cai%20and%20Xun%20Wang%20and%20Wenwu%20Yang%0AAbstract%3A%20Existing%20multi-person%20video%20pose%20estimation%20methods%20typically%20adopt%20a%20two-stage%20pipeline%3A%20detecting%20individuals%20in%20each%20frame%2C%20followed%20by%20temporal%20modeling%20for%20single%20person%20pose%20estimation.%20This%20design%20relies%20on%20heuristic%20operations%20such%20as%20detection%2C%20RoI%20cropping%2C%20and%20non-maximum%20suppression%20%28NMS%29%2C%20limiting%20both%20accuracy%20and%20efficiency.%20In%20this%20paper%2C%20we%20present%20a%20fully%20end-to-end%20framework%20for%20multi-person%202D%20pose%20estimation%20in%20videos%2C%20effectively%20eliminating%20heuristic%20operations.%20A%20key%20challenge%20is%20to%20associate%20individuals%20across%20frames%20under%20complex%20and%20overlapping%20temporal%20trajectories.%20To%20address%20this%2C%20we%20introduce%20a%20novel%20Pose-Aware%20Video%20transformEr%20Network%20%28PAVE-Net%29%2C%20which%20features%20a%20spatial%20encoder%20to%20model%20intra-frame%20relations%20and%20a%20spatiotemporal%20pose%20decoder%20to%20capture%20global%20dependencies%20across%20frames.%20To%20achieve%20accurate%20temporal%20association%2C%20we%20propose%20a%20pose-aware%20attention%20mechanism%20that%20enables%20each%20pose%20query%20to%20selectively%20aggregate%20features%20corresponding%20to%20the%20same%20individual%20across%20consecutive%20frames.%20Additionally%2C%20we%20explicitly%20model%20spatiotemporal%20dependencies%20among%20pose%20keypoints%20to%20improve%20accuracy.%20Notably%2C%20our%20approach%20is%20the%20first%20end-to-end%20method%20for%20multi-frame%202D%20human%20pose%20estimation.%20Extensive%20experiments%20show%20that%20PAVE-Net%20substantially%20outperforms%20prior%20image-based%20end-to-end%20methods%2C%20achieving%20a%206.0%20mAP%20improvement%20on%20PoseTrack2017%2C%20and%20delivers%20accuracy%20competitive%20with%20state-of-the-art%20two-stage%20video%20based%20approaches%2C%20while%20offering%20significant%20gains%20in%20efficiency.%20Project%20page%3A%20https%3A//github.com/zgspose/PAVENet.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13208v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnd-to-End%2520Multi-Person%2520Pose%2520Estimation%2520with%2520Pose-Aware%2520Video%2520Transformer%26entry.906535625%3DYonghui%2520Yu%2520and%2520Jiahang%2520Cai%2520and%2520Xun%2520Wang%2520and%2520Wenwu%2520Yang%26entry.1292438233%3DExisting%2520multi-person%2520video%2520pose%2520estimation%2520methods%2520typically%2520adopt%2520a%2520two-stage%2520pipeline%253A%2520detecting%2520individuals%2520in%2520each%2520frame%252C%2520followed%2520by%2520temporal%2520modeling%2520for%2520single%2520person%2520pose%2520estimation.%2520This%2520design%2520relies%2520on%2520heuristic%2520operations%2520such%2520as%2520detection%252C%2520RoI%2520cropping%252C%2520and%2520non-maximum%2520suppression%2520%2528NMS%2529%252C%2520limiting%2520both%2520accuracy%2520and%2520efficiency.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520fully%2520end-to-end%2520framework%2520for%2520multi-person%25202D%2520pose%2520estimation%2520in%2520videos%252C%2520effectively%2520eliminating%2520heuristic%2520operations.%2520A%2520key%2520challenge%2520is%2520to%2520associate%2520individuals%2520across%2520frames%2520under%2520complex%2520and%2520overlapping%2520temporal%2520trajectories.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520novel%2520Pose-Aware%2520Video%2520transformEr%2520Network%2520%2528PAVE-Net%2529%252C%2520which%2520features%2520a%2520spatial%2520encoder%2520to%2520model%2520intra-frame%2520relations%2520and%2520a%2520spatiotemporal%2520pose%2520decoder%2520to%2520capture%2520global%2520dependencies%2520across%2520frames.%2520To%2520achieve%2520accurate%2520temporal%2520association%252C%2520we%2520propose%2520a%2520pose-aware%2520attention%2520mechanism%2520that%2520enables%2520each%2520pose%2520query%2520to%2520selectively%2520aggregate%2520features%2520corresponding%2520to%2520the%2520same%2520individual%2520across%2520consecutive%2520frames.%2520Additionally%252C%2520we%2520explicitly%2520model%2520spatiotemporal%2520dependencies%2520among%2520pose%2520keypoints%2520to%2520improve%2520accuracy.%2520Notably%252C%2520our%2520approach%2520is%2520the%2520first%2520end-to-end%2520method%2520for%2520multi-frame%25202D%2520human%2520pose%2520estimation.%2520Extensive%2520experiments%2520show%2520that%2520PAVE-Net%2520substantially%2520outperforms%2520prior%2520image-based%2520end-to-end%2520methods%252C%2520achieving%2520a%25206.0%2520mAP%2520improvement%2520on%2520PoseTrack2017%252C%2520and%2520delivers%2520accuracy%2520competitive%2520with%2520state-of-the-art%2520two-stage%2520video%2520based%2520approaches%252C%2520while%2520offering%2520significant%2520gains%2520in%2520efficiency.%2520Project%2520page%253A%2520https%253A//github.com/zgspose/PAVENet.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13208v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End-to-End%20Multi-Person%20Pose%20Estimation%20with%20Pose-Aware%20Video%20Transformer&entry.906535625=Yonghui%20Yu%20and%20Jiahang%20Cai%20and%20Xun%20Wang%20and%20Wenwu%20Yang&entry.1292438233=Existing%20multi-person%20video%20pose%20estimation%20methods%20typically%20adopt%20a%20two-stage%20pipeline%3A%20detecting%20individuals%20in%20each%20frame%2C%20followed%20by%20temporal%20modeling%20for%20single%20person%20pose%20estimation.%20This%20design%20relies%20on%20heuristic%20operations%20such%20as%20detection%2C%20RoI%20cropping%2C%20and%20non-maximum%20suppression%20%28NMS%29%2C%20limiting%20both%20accuracy%20and%20efficiency.%20In%20this%20paper%2C%20we%20present%20a%20fully%20end-to-end%20framework%20for%20multi-person%202D%20pose%20estimation%20in%20videos%2C%20effectively%20eliminating%20heuristic%20operations.%20A%20key%20challenge%20is%20to%20associate%20individuals%20across%20frames%20under%20complex%20and%20overlapping%20temporal%20trajectories.%20To%20address%20this%2C%20we%20introduce%20a%20novel%20Pose-Aware%20Video%20transformEr%20Network%20%28PAVE-Net%29%2C%20which%20features%20a%20spatial%20encoder%20to%20model%20intra-frame%20relations%20and%20a%20spatiotemporal%20pose%20decoder%20to%20capture%20global%20dependencies%20across%20frames.%20To%20achieve%20accurate%20temporal%20association%2C%20we%20propose%20a%20pose-aware%20attention%20mechanism%20that%20enables%20each%20pose%20query%20to%20selectively%20aggregate%20features%20corresponding%20to%20the%20same%20individual%20across%20consecutive%20frames.%20Additionally%2C%20we%20explicitly%20model%20spatiotemporal%20dependencies%20among%20pose%20keypoints%20to%20improve%20accuracy.%20Notably%2C%20our%20approach%20is%20the%20first%20end-to-end%20method%20for%20multi-frame%202D%20human%20pose%20estimation.%20Extensive%20experiments%20show%20that%20PAVE-Net%20substantially%20outperforms%20prior%20image-based%20end-to-end%20methods%2C%20achieving%20a%206.0%20mAP%20improvement%20on%20PoseTrack2017%2C%20and%20delivers%20accuracy%20competitive%20with%20state-of-the-art%20two-stage%20video%20based%20approaches%2C%20while%20offering%20significant%20gains%20in%20efficiency.%20Project%20page%3A%20https%3A//github.com/zgspose/PAVENet.&entry.1838667208=http%3A//arxiv.org/abs/2511.13208v2&entry.124074799=Read"},
{"title": "Lumos: Let there be Language Model System Certification", "author": "Isha Chaudhary and Vedaant Jain and Avaljot Singh and Kavya Sachdeva and Sayan Ranu and Gagandeep Singh", "abstract": "We introduce the first principled framework, Lumos, for specifying and formally certifying Language Model System (LMS) behaviors. Lumos is an imperative probabilistic programming DSL over graphs, with constructs to generate independent and identically distributed prompts for LMS. It offers a structured view of prompt distributions via graphs, forming random prompts from sampled subgraphs. Lumos supports certifying LMS for arbitrary prompt distributions via integration with statistical certifiers. We provide hybrid (operational and denotational) semantics for Lumos, providing a rigorous way to interpret the specifications. Using only a small set of composable constructs, Lumos can encode existing LMS specifications, including complex relational and temporal specifications. It also facilitates specifying new properties - we present the first safety specifications for vision-language models (VLMs) in autonomous driving scenarios developed with Lumos. Using these, we show that the state-of-the-art VLM Qwen-VL exhibits critical safety failures, producing incorrect and unsafe responses with at least 90% probability in right-turn scenarios under rainy driving conditions, revealing substantial safety risks. Lumos's modular structure allows easy modification of the specifications, enabling LMS certification to stay abreast with the rapidly evolving threat landscape. We further demonstrate that specification programs written in Lumos enable finding specific failure cases exhibited by state-of-the-art LMS. Lumos is the first systematic and extensible language-based framework for specifying and certifying LMS behaviors, paving the way for a wider adoption of LMS certification.", "link": "http://arxiv.org/abs/2512.02966v1", "date": "2025-12-02", "relevancy": 2.4151, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4837}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4827}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4827}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lumos%3A%20Let%20there%20be%20Language%20Model%20System%20Certification&body=Title%3A%20Lumos%3A%20Let%20there%20be%20Language%20Model%20System%20Certification%0AAuthor%3A%20Isha%20Chaudhary%20and%20Vedaant%20Jain%20and%20Avaljot%20Singh%20and%20Kavya%20Sachdeva%20and%20Sayan%20Ranu%20and%20Gagandeep%20Singh%0AAbstract%3A%20We%20introduce%20the%20first%20principled%20framework%2C%20Lumos%2C%20for%20specifying%20and%20formally%20certifying%20Language%20Model%20System%20%28LMS%29%20behaviors.%20Lumos%20is%20an%20imperative%20probabilistic%20programming%20DSL%20over%20graphs%2C%20with%20constructs%20to%20generate%20independent%20and%20identically%20distributed%20prompts%20for%20LMS.%20It%20offers%20a%20structured%20view%20of%20prompt%20distributions%20via%20graphs%2C%20forming%20random%20prompts%20from%20sampled%20subgraphs.%20Lumos%20supports%20certifying%20LMS%20for%20arbitrary%20prompt%20distributions%20via%20integration%20with%20statistical%20certifiers.%20We%20provide%20hybrid%20%28operational%20and%20denotational%29%20semantics%20for%20Lumos%2C%20providing%20a%20rigorous%20way%20to%20interpret%20the%20specifications.%20Using%20only%20a%20small%20set%20of%20composable%20constructs%2C%20Lumos%20can%20encode%20existing%20LMS%20specifications%2C%20including%20complex%20relational%20and%20temporal%20specifications.%20It%20also%20facilitates%20specifying%20new%20properties%20-%20we%20present%20the%20first%20safety%20specifications%20for%20vision-language%20models%20%28VLMs%29%20in%20autonomous%20driving%20scenarios%20developed%20with%20Lumos.%20Using%20these%2C%20we%20show%20that%20the%20state-of-the-art%20VLM%20Qwen-VL%20exhibits%20critical%20safety%20failures%2C%20producing%20incorrect%20and%20unsafe%20responses%20with%20at%20least%2090%25%20probability%20in%20right-turn%20scenarios%20under%20rainy%20driving%20conditions%2C%20revealing%20substantial%20safety%20risks.%20Lumos%27s%20modular%20structure%20allows%20easy%20modification%20of%20the%20specifications%2C%20enabling%20LMS%20certification%20to%20stay%20abreast%20with%20the%20rapidly%20evolving%20threat%20landscape.%20We%20further%20demonstrate%20that%20specification%20programs%20written%20in%20Lumos%20enable%20finding%20specific%20failure%20cases%20exhibited%20by%20state-of-the-art%20LMS.%20Lumos%20is%20the%20first%20systematic%20and%20extensible%20language-based%20framework%20for%20specifying%20and%20certifying%20LMS%20behaviors%2C%20paving%20the%20way%20for%20a%20wider%20adoption%20of%20LMS%20certification.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02966v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLumos%253A%2520Let%2520there%2520be%2520Language%2520Model%2520System%2520Certification%26entry.906535625%3DIsha%2520Chaudhary%2520and%2520Vedaant%2520Jain%2520and%2520Avaljot%2520Singh%2520and%2520Kavya%2520Sachdeva%2520and%2520Sayan%2520Ranu%2520and%2520Gagandeep%2520Singh%26entry.1292438233%3DWe%2520introduce%2520the%2520first%2520principled%2520framework%252C%2520Lumos%252C%2520for%2520specifying%2520and%2520formally%2520certifying%2520Language%2520Model%2520System%2520%2528LMS%2529%2520behaviors.%2520Lumos%2520is%2520an%2520imperative%2520probabilistic%2520programming%2520DSL%2520over%2520graphs%252C%2520with%2520constructs%2520to%2520generate%2520independent%2520and%2520identically%2520distributed%2520prompts%2520for%2520LMS.%2520It%2520offers%2520a%2520structured%2520view%2520of%2520prompt%2520distributions%2520via%2520graphs%252C%2520forming%2520random%2520prompts%2520from%2520sampled%2520subgraphs.%2520Lumos%2520supports%2520certifying%2520LMS%2520for%2520arbitrary%2520prompt%2520distributions%2520via%2520integration%2520with%2520statistical%2520certifiers.%2520We%2520provide%2520hybrid%2520%2528operational%2520and%2520denotational%2529%2520semantics%2520for%2520Lumos%252C%2520providing%2520a%2520rigorous%2520way%2520to%2520interpret%2520the%2520specifications.%2520Using%2520only%2520a%2520small%2520set%2520of%2520composable%2520constructs%252C%2520Lumos%2520can%2520encode%2520existing%2520LMS%2520specifications%252C%2520including%2520complex%2520relational%2520and%2520temporal%2520specifications.%2520It%2520also%2520facilitates%2520specifying%2520new%2520properties%2520-%2520we%2520present%2520the%2520first%2520safety%2520specifications%2520for%2520vision-language%2520models%2520%2528VLMs%2529%2520in%2520autonomous%2520driving%2520scenarios%2520developed%2520with%2520Lumos.%2520Using%2520these%252C%2520we%2520show%2520that%2520the%2520state-of-the-art%2520VLM%2520Qwen-VL%2520exhibits%2520critical%2520safety%2520failures%252C%2520producing%2520incorrect%2520and%2520unsafe%2520responses%2520with%2520at%2520least%252090%2525%2520probability%2520in%2520right-turn%2520scenarios%2520under%2520rainy%2520driving%2520conditions%252C%2520revealing%2520substantial%2520safety%2520risks.%2520Lumos%2527s%2520modular%2520structure%2520allows%2520easy%2520modification%2520of%2520the%2520specifications%252C%2520enabling%2520LMS%2520certification%2520to%2520stay%2520abreast%2520with%2520the%2520rapidly%2520evolving%2520threat%2520landscape.%2520We%2520further%2520demonstrate%2520that%2520specification%2520programs%2520written%2520in%2520Lumos%2520enable%2520finding%2520specific%2520failure%2520cases%2520exhibited%2520by%2520state-of-the-art%2520LMS.%2520Lumos%2520is%2520the%2520first%2520systematic%2520and%2520extensible%2520language-based%2520framework%2520for%2520specifying%2520and%2520certifying%2520LMS%2520behaviors%252C%2520paving%2520the%2520way%2520for%2520a%2520wider%2520adoption%2520of%2520LMS%2520certification.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02966v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lumos%3A%20Let%20there%20be%20Language%20Model%20System%20Certification&entry.906535625=Isha%20Chaudhary%20and%20Vedaant%20Jain%20and%20Avaljot%20Singh%20and%20Kavya%20Sachdeva%20and%20Sayan%20Ranu%20and%20Gagandeep%20Singh&entry.1292438233=We%20introduce%20the%20first%20principled%20framework%2C%20Lumos%2C%20for%20specifying%20and%20formally%20certifying%20Language%20Model%20System%20%28LMS%29%20behaviors.%20Lumos%20is%20an%20imperative%20probabilistic%20programming%20DSL%20over%20graphs%2C%20with%20constructs%20to%20generate%20independent%20and%20identically%20distributed%20prompts%20for%20LMS.%20It%20offers%20a%20structured%20view%20of%20prompt%20distributions%20via%20graphs%2C%20forming%20random%20prompts%20from%20sampled%20subgraphs.%20Lumos%20supports%20certifying%20LMS%20for%20arbitrary%20prompt%20distributions%20via%20integration%20with%20statistical%20certifiers.%20We%20provide%20hybrid%20%28operational%20and%20denotational%29%20semantics%20for%20Lumos%2C%20providing%20a%20rigorous%20way%20to%20interpret%20the%20specifications.%20Using%20only%20a%20small%20set%20of%20composable%20constructs%2C%20Lumos%20can%20encode%20existing%20LMS%20specifications%2C%20including%20complex%20relational%20and%20temporal%20specifications.%20It%20also%20facilitates%20specifying%20new%20properties%20-%20we%20present%20the%20first%20safety%20specifications%20for%20vision-language%20models%20%28VLMs%29%20in%20autonomous%20driving%20scenarios%20developed%20with%20Lumos.%20Using%20these%2C%20we%20show%20that%20the%20state-of-the-art%20VLM%20Qwen-VL%20exhibits%20critical%20safety%20failures%2C%20producing%20incorrect%20and%20unsafe%20responses%20with%20at%20least%2090%25%20probability%20in%20right-turn%20scenarios%20under%20rainy%20driving%20conditions%2C%20revealing%20substantial%20safety%20risks.%20Lumos%27s%20modular%20structure%20allows%20easy%20modification%20of%20the%20specifications%2C%20enabling%20LMS%20certification%20to%20stay%20abreast%20with%20the%20rapidly%20evolving%20threat%20landscape.%20We%20further%20demonstrate%20that%20specification%20programs%20written%20in%20Lumos%20enable%20finding%20specific%20failure%20cases%20exhibited%20by%20state-of-the-art%20LMS.%20Lumos%20is%20the%20first%20systematic%20and%20extensible%20language-based%20framework%20for%20specifying%20and%20certifying%20LMS%20behaviors%2C%20paving%20the%20way%20for%20a%20wider%20adoption%20of%20LMS%20certification.&entry.1838667208=http%3A//arxiv.org/abs/2512.02966v1&entry.124074799=Read"},
{"title": "MagicQuillV2: Precise and Interactive Image Editing with Layered Visual Cues", "author": "Zichen Liu and Yue Yu and Hao Ouyang and Qiuyu Wang and Shuailei Ma and Ka Leong Cheng and Wen Wang and Qingyan Bai and Yuxuan Zhang and Yanhong Zeng and Yixuan Li and Xing Zhu and Yujun Shen and Qifeng Chen", "abstract": "We propose MagicQuill V2, a novel system that introduces a \\textbf{layered composition} paradigm to generative image editing, bridging the gap between the semantic power of diffusion models and the granular control of traditional graphics software. While diffusion transformers excel at holistic generation, their use of singular, monolithic prompts fails to disentangle distinct user intentions for content, position, and appearance. To overcome this, our method deconstructs creative intent into a stack of controllable visual cues: a content layer for what to create, a spatial layer for where to place it, a structural layer for how it is shaped, and a color layer for its palette. Our technical contributions include a specialized data generation pipeline for context-aware content integration, a unified control module to process all visual cues, and a fine-tuned spatial branch for precise local editing, including object removal. Extensive experiments validate that this layered approach effectively resolves the user intention gap, granting creators direct, intuitive control over the generative process.", "link": "http://arxiv.org/abs/2512.03046v1", "date": "2025-12-02", "relevancy": 2.4114, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6366}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6162}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5638}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MagicQuillV2%3A%20Precise%20and%20Interactive%20Image%20Editing%20with%20Layered%20Visual%20Cues&body=Title%3A%20MagicQuillV2%3A%20Precise%20and%20Interactive%20Image%20Editing%20with%20Layered%20Visual%20Cues%0AAuthor%3A%20Zichen%20Liu%20and%20Yue%20Yu%20and%20Hao%20Ouyang%20and%20Qiuyu%20Wang%20and%20Shuailei%20Ma%20and%20Ka%20Leong%20Cheng%20and%20Wen%20Wang%20and%20Qingyan%20Bai%20and%20Yuxuan%20Zhang%20and%20Yanhong%20Zeng%20and%20Yixuan%20Li%20and%20Xing%20Zhu%20and%20Yujun%20Shen%20and%20Qifeng%20Chen%0AAbstract%3A%20We%20propose%20MagicQuill%20V2%2C%20a%20novel%20system%20that%20introduces%20a%20%5Ctextbf%7Blayered%20composition%7D%20paradigm%20to%20generative%20image%20editing%2C%20bridging%20the%20gap%20between%20the%20semantic%20power%20of%20diffusion%20models%20and%20the%20granular%20control%20of%20traditional%20graphics%20software.%20While%20diffusion%20transformers%20excel%20at%20holistic%20generation%2C%20their%20use%20of%20singular%2C%20monolithic%20prompts%20fails%20to%20disentangle%20distinct%20user%20intentions%20for%20content%2C%20position%2C%20and%20appearance.%20To%20overcome%20this%2C%20our%20method%20deconstructs%20creative%20intent%20into%20a%20stack%20of%20controllable%20visual%20cues%3A%20a%20content%20layer%20for%20what%20to%20create%2C%20a%20spatial%20layer%20for%20where%20to%20place%20it%2C%20a%20structural%20layer%20for%20how%20it%20is%20shaped%2C%20and%20a%20color%20layer%20for%20its%20palette.%20Our%20technical%20contributions%20include%20a%20specialized%20data%20generation%20pipeline%20for%20context-aware%20content%20integration%2C%20a%20unified%20control%20module%20to%20process%20all%20visual%20cues%2C%20and%20a%20fine-tuned%20spatial%20branch%20for%20precise%20local%20editing%2C%20including%20object%20removal.%20Extensive%20experiments%20validate%20that%20this%20layered%20approach%20effectively%20resolves%20the%20user%20intention%20gap%2C%20granting%20creators%20direct%2C%20intuitive%20control%20over%20the%20generative%20process.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03046v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMagicQuillV2%253A%2520Precise%2520and%2520Interactive%2520Image%2520Editing%2520with%2520Layered%2520Visual%2520Cues%26entry.906535625%3DZichen%2520Liu%2520and%2520Yue%2520Yu%2520and%2520Hao%2520Ouyang%2520and%2520Qiuyu%2520Wang%2520and%2520Shuailei%2520Ma%2520and%2520Ka%2520Leong%2520Cheng%2520and%2520Wen%2520Wang%2520and%2520Qingyan%2520Bai%2520and%2520Yuxuan%2520Zhang%2520and%2520Yanhong%2520Zeng%2520and%2520Yixuan%2520Li%2520and%2520Xing%2520Zhu%2520and%2520Yujun%2520Shen%2520and%2520Qifeng%2520Chen%26entry.1292438233%3DWe%2520propose%2520MagicQuill%2520V2%252C%2520a%2520novel%2520system%2520that%2520introduces%2520a%2520%255Ctextbf%257Blayered%2520composition%257D%2520paradigm%2520to%2520generative%2520image%2520editing%252C%2520bridging%2520the%2520gap%2520between%2520the%2520semantic%2520power%2520of%2520diffusion%2520models%2520and%2520the%2520granular%2520control%2520of%2520traditional%2520graphics%2520software.%2520While%2520diffusion%2520transformers%2520excel%2520at%2520holistic%2520generation%252C%2520their%2520use%2520of%2520singular%252C%2520monolithic%2520prompts%2520fails%2520to%2520disentangle%2520distinct%2520user%2520intentions%2520for%2520content%252C%2520position%252C%2520and%2520appearance.%2520To%2520overcome%2520this%252C%2520our%2520method%2520deconstructs%2520creative%2520intent%2520into%2520a%2520stack%2520of%2520controllable%2520visual%2520cues%253A%2520a%2520content%2520layer%2520for%2520what%2520to%2520create%252C%2520a%2520spatial%2520layer%2520for%2520where%2520to%2520place%2520it%252C%2520a%2520structural%2520layer%2520for%2520how%2520it%2520is%2520shaped%252C%2520and%2520a%2520color%2520layer%2520for%2520its%2520palette.%2520Our%2520technical%2520contributions%2520include%2520a%2520specialized%2520data%2520generation%2520pipeline%2520for%2520context-aware%2520content%2520integration%252C%2520a%2520unified%2520control%2520module%2520to%2520process%2520all%2520visual%2520cues%252C%2520and%2520a%2520fine-tuned%2520spatial%2520branch%2520for%2520precise%2520local%2520editing%252C%2520including%2520object%2520removal.%2520Extensive%2520experiments%2520validate%2520that%2520this%2520layered%2520approach%2520effectively%2520resolves%2520the%2520user%2520intention%2520gap%252C%2520granting%2520creators%2520direct%252C%2520intuitive%2520control%2520over%2520the%2520generative%2520process.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03046v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MagicQuillV2%3A%20Precise%20and%20Interactive%20Image%20Editing%20with%20Layered%20Visual%20Cues&entry.906535625=Zichen%20Liu%20and%20Yue%20Yu%20and%20Hao%20Ouyang%20and%20Qiuyu%20Wang%20and%20Shuailei%20Ma%20and%20Ka%20Leong%20Cheng%20and%20Wen%20Wang%20and%20Qingyan%20Bai%20and%20Yuxuan%20Zhang%20and%20Yanhong%20Zeng%20and%20Yixuan%20Li%20and%20Xing%20Zhu%20and%20Yujun%20Shen%20and%20Qifeng%20Chen&entry.1292438233=We%20propose%20MagicQuill%20V2%2C%20a%20novel%20system%20that%20introduces%20a%20%5Ctextbf%7Blayered%20composition%7D%20paradigm%20to%20generative%20image%20editing%2C%20bridging%20the%20gap%20between%20the%20semantic%20power%20of%20diffusion%20models%20and%20the%20granular%20control%20of%20traditional%20graphics%20software.%20While%20diffusion%20transformers%20excel%20at%20holistic%20generation%2C%20their%20use%20of%20singular%2C%20monolithic%20prompts%20fails%20to%20disentangle%20distinct%20user%20intentions%20for%20content%2C%20position%2C%20and%20appearance.%20To%20overcome%20this%2C%20our%20method%20deconstructs%20creative%20intent%20into%20a%20stack%20of%20controllable%20visual%20cues%3A%20a%20content%20layer%20for%20what%20to%20create%2C%20a%20spatial%20layer%20for%20where%20to%20place%20it%2C%20a%20structural%20layer%20for%20how%20it%20is%20shaped%2C%20and%20a%20color%20layer%20for%20its%20palette.%20Our%20technical%20contributions%20include%20a%20specialized%20data%20generation%20pipeline%20for%20context-aware%20content%20integration%2C%20a%20unified%20control%20module%20to%20process%20all%20visual%20cues%2C%20and%20a%20fine-tuned%20spatial%20branch%20for%20precise%20local%20editing%2C%20including%20object%20removal.%20Extensive%20experiments%20validate%20that%20this%20layered%20approach%20effectively%20resolves%20the%20user%20intention%20gap%2C%20granting%20creators%20direct%2C%20intuitive%20control%20over%20the%20generative%20process.&entry.1838667208=http%3A//arxiv.org/abs/2512.03046v1&entry.124074799=Read"},
{"title": "Gaussian and Non-Gaussian Universality of Data Augmentation", "author": "Kevin Han Huang and Peter Orbanz and Morgane Austern", "abstract": "We provide universality results that quantify how data augmentation affects the variance and limiting distribution of estimates through simple surrogates, and analyze several specific models in detail. The results confirm some observations made in machine learning practice, but also lead to unexpected findings: Data augmentation may increase rather than decrease the uncertainty of estimates, such as the empirical prediction risk. It can act as a regularizer, but fails to do so in certain high-dimensional problems, and it may shift the double-descent peak of an empirical risk. Overall, the analysis shows that several properties data augmentation has been attributed with are not either true or false, but rather depend on a combination of factors -- notably the data distribution, the properties of the estimator, and the interplay of sample size, number of augmentations, and dimension. As our main theoretical tool, we develop an adaptation of Lindeberg's technique for block dependence. The resulting universality regime may be Gaussian or non-Gaussian.", "link": "http://arxiv.org/abs/2202.09134v5", "date": "2025-12-02", "relevancy": 2.4092, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4942}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4866}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20and%20Non-Gaussian%20Universality%20of%20Data%20Augmentation&body=Title%3A%20Gaussian%20and%20Non-Gaussian%20Universality%20of%20Data%20Augmentation%0AAuthor%3A%20Kevin%20Han%20Huang%20and%20Peter%20Orbanz%20and%20Morgane%20Austern%0AAbstract%3A%20We%20provide%20universality%20results%20that%20quantify%20how%20data%20augmentation%20affects%20the%20variance%20and%20limiting%20distribution%20of%20estimates%20through%20simple%20surrogates%2C%20and%20analyze%20several%20specific%20models%20in%20detail.%20The%20results%20confirm%20some%20observations%20made%20in%20machine%20learning%20practice%2C%20but%20also%20lead%20to%20unexpected%20findings%3A%20Data%20augmentation%20may%20increase%20rather%20than%20decrease%20the%20uncertainty%20of%20estimates%2C%20such%20as%20the%20empirical%20prediction%20risk.%20It%20can%20act%20as%20a%20regularizer%2C%20but%20fails%20to%20do%20so%20in%20certain%20high-dimensional%20problems%2C%20and%20it%20may%20shift%20the%20double-descent%20peak%20of%20an%20empirical%20risk.%20Overall%2C%20the%20analysis%20shows%20that%20several%20properties%20data%20augmentation%20has%20been%20attributed%20with%20are%20not%20either%20true%20or%20false%2C%20but%20rather%20depend%20on%20a%20combination%20of%20factors%20--%20notably%20the%20data%20distribution%2C%20the%20properties%20of%20the%20estimator%2C%20and%20the%20interplay%20of%20sample%20size%2C%20number%20of%20augmentations%2C%20and%20dimension.%20As%20our%20main%20theoretical%20tool%2C%20we%20develop%20an%20adaptation%20of%20Lindeberg%27s%20technique%20for%20block%20dependence.%20The%20resulting%20universality%20regime%20may%20be%20Gaussian%20or%20non-Gaussian.%0ALink%3A%20http%3A//arxiv.org/abs/2202.09134v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520and%2520Non-Gaussian%2520Universality%2520of%2520Data%2520Augmentation%26entry.906535625%3DKevin%2520Han%2520Huang%2520and%2520Peter%2520Orbanz%2520and%2520Morgane%2520Austern%26entry.1292438233%3DWe%2520provide%2520universality%2520results%2520that%2520quantify%2520how%2520data%2520augmentation%2520affects%2520the%2520variance%2520and%2520limiting%2520distribution%2520of%2520estimates%2520through%2520simple%2520surrogates%252C%2520and%2520analyze%2520several%2520specific%2520models%2520in%2520detail.%2520The%2520results%2520confirm%2520some%2520observations%2520made%2520in%2520machine%2520learning%2520practice%252C%2520but%2520also%2520lead%2520to%2520unexpected%2520findings%253A%2520Data%2520augmentation%2520may%2520increase%2520rather%2520than%2520decrease%2520the%2520uncertainty%2520of%2520estimates%252C%2520such%2520as%2520the%2520empirical%2520prediction%2520risk.%2520It%2520can%2520act%2520as%2520a%2520regularizer%252C%2520but%2520fails%2520to%2520do%2520so%2520in%2520certain%2520high-dimensional%2520problems%252C%2520and%2520it%2520may%2520shift%2520the%2520double-descent%2520peak%2520of%2520an%2520empirical%2520risk.%2520Overall%252C%2520the%2520analysis%2520shows%2520that%2520several%2520properties%2520data%2520augmentation%2520has%2520been%2520attributed%2520with%2520are%2520not%2520either%2520true%2520or%2520false%252C%2520but%2520rather%2520depend%2520on%2520a%2520combination%2520of%2520factors%2520--%2520notably%2520the%2520data%2520distribution%252C%2520the%2520properties%2520of%2520the%2520estimator%252C%2520and%2520the%2520interplay%2520of%2520sample%2520size%252C%2520number%2520of%2520augmentations%252C%2520and%2520dimension.%2520As%2520our%2520main%2520theoretical%2520tool%252C%2520we%2520develop%2520an%2520adaptation%2520of%2520Lindeberg%2527s%2520technique%2520for%2520block%2520dependence.%2520The%2520resulting%2520universality%2520regime%2520may%2520be%2520Gaussian%2520or%2520non-Gaussian.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2202.09134v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20and%20Non-Gaussian%20Universality%20of%20Data%20Augmentation&entry.906535625=Kevin%20Han%20Huang%20and%20Peter%20Orbanz%20and%20Morgane%20Austern&entry.1292438233=We%20provide%20universality%20results%20that%20quantify%20how%20data%20augmentation%20affects%20the%20variance%20and%20limiting%20distribution%20of%20estimates%20through%20simple%20surrogates%2C%20and%20analyze%20several%20specific%20models%20in%20detail.%20The%20results%20confirm%20some%20observations%20made%20in%20machine%20learning%20practice%2C%20but%20also%20lead%20to%20unexpected%20findings%3A%20Data%20augmentation%20may%20increase%20rather%20than%20decrease%20the%20uncertainty%20of%20estimates%2C%20such%20as%20the%20empirical%20prediction%20risk.%20It%20can%20act%20as%20a%20regularizer%2C%20but%20fails%20to%20do%20so%20in%20certain%20high-dimensional%20problems%2C%20and%20it%20may%20shift%20the%20double-descent%20peak%20of%20an%20empirical%20risk.%20Overall%2C%20the%20analysis%20shows%20that%20several%20properties%20data%20augmentation%20has%20been%20attributed%20with%20are%20not%20either%20true%20or%20false%2C%20but%20rather%20depend%20on%20a%20combination%20of%20factors%20--%20notably%20the%20data%20distribution%2C%20the%20properties%20of%20the%20estimator%2C%20and%20the%20interplay%20of%20sample%20size%2C%20number%20of%20augmentations%2C%20and%20dimension.%20As%20our%20main%20theoretical%20tool%2C%20we%20develop%20an%20adaptation%20of%20Lindeberg%27s%20technique%20for%20block%20dependence.%20The%20resulting%20universality%20regime%20may%20be%20Gaussian%20or%20non-Gaussian.&entry.1838667208=http%3A//arxiv.org/abs/2202.09134v5&entry.124074799=Read"},
{"title": "GraphFusion3D: Dynamic Graph Attention Convolution with Adaptive Cross-Modal Transformer for 3D Object Detection", "author": "Md Sohag Mia and Md Nahid Hasan and Tawhid Ahmed and Muhammad Abdullah Adnan", "abstract": "Despite significant progress in 3D object detection, point clouds remain challenging due to sparse data, incomplete structures, and limited semantic information. Capturing contextual relationships between distant objects presents additional difficulties. To address these challenges, we propose GraphFusion3D, a unified framework combining multi-modal fusion with advanced feature learning. Our approach introduces the Adaptive Cross-Modal Transformer (ACMT), which adaptively integrates image features into point representations to enrich both geometric and semantic information. For proposal refinement, we introduce the Graph Reasoning Module (GRM), a novel mechanism that models neighborhood relationships to simultaneously capture local geometric structures and global semantic context. The module employs multi-scale graph attention to dynamically weight both spatial proximity and feature similarity between proposals. We further employ a cascade decoder that progressively refines detections through multi-stage predictions. Extensive experiments on SUN RGB-D (70.6\\% AP$_{25}$ and 51.2\\% AP$_{50}$) and ScanNetV2 (75.1\\% AP$_{25}$ and 60.8\\% AP$_{50}$) demonstrate a substantial performance improvement over existing approaches.", "link": "http://arxiv.org/abs/2512.02991v1", "date": "2025-12-02", "relevancy": 2.4007, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6135}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5973}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphFusion3D%3A%20Dynamic%20Graph%20Attention%20Convolution%20with%20Adaptive%20Cross-Modal%20Transformer%20for%203D%20Object%20Detection&body=Title%3A%20GraphFusion3D%3A%20Dynamic%20Graph%20Attention%20Convolution%20with%20Adaptive%20Cross-Modal%20Transformer%20for%203D%20Object%20Detection%0AAuthor%3A%20Md%20Sohag%20Mia%20and%20Md%20Nahid%20Hasan%20and%20Tawhid%20Ahmed%20and%20Muhammad%20Abdullah%20Adnan%0AAbstract%3A%20Despite%20significant%20progress%20in%203D%20object%20detection%2C%20point%20clouds%20remain%20challenging%20due%20to%20sparse%20data%2C%20incomplete%20structures%2C%20and%20limited%20semantic%20information.%20Capturing%20contextual%20relationships%20between%20distant%20objects%20presents%20additional%20difficulties.%20To%20address%20these%20challenges%2C%20we%20propose%20GraphFusion3D%2C%20a%20unified%20framework%20combining%20multi-modal%20fusion%20with%20advanced%20feature%20learning.%20Our%20approach%20introduces%20the%20Adaptive%20Cross-Modal%20Transformer%20%28ACMT%29%2C%20which%20adaptively%20integrates%20image%20features%20into%20point%20representations%20to%20enrich%20both%20geometric%20and%20semantic%20information.%20For%20proposal%20refinement%2C%20we%20introduce%20the%20Graph%20Reasoning%20Module%20%28GRM%29%2C%20a%20novel%20mechanism%20that%20models%20neighborhood%20relationships%20to%20simultaneously%20capture%20local%20geometric%20structures%20and%20global%20semantic%20context.%20The%20module%20employs%20multi-scale%20graph%20attention%20to%20dynamically%20weight%20both%20spatial%20proximity%20and%20feature%20similarity%20between%20proposals.%20We%20further%20employ%20a%20cascade%20decoder%20that%20progressively%20refines%20detections%20through%20multi-stage%20predictions.%20Extensive%20experiments%20on%20SUN%20RGB-D%20%2870.6%5C%25%20AP%24_%7B25%7D%24%20and%2051.2%5C%25%20AP%24_%7B50%7D%24%29%20and%20ScanNetV2%20%2875.1%5C%25%20AP%24_%7B25%7D%24%20and%2060.8%5C%25%20AP%24_%7B50%7D%24%29%20demonstrate%20a%20substantial%20performance%20improvement%20over%20existing%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02991v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphFusion3D%253A%2520Dynamic%2520Graph%2520Attention%2520Convolution%2520with%2520Adaptive%2520Cross-Modal%2520Transformer%2520for%25203D%2520Object%2520Detection%26entry.906535625%3DMd%2520Sohag%2520Mia%2520and%2520Md%2520Nahid%2520Hasan%2520and%2520Tawhid%2520Ahmed%2520and%2520Muhammad%2520Abdullah%2520Adnan%26entry.1292438233%3DDespite%2520significant%2520progress%2520in%25203D%2520object%2520detection%252C%2520point%2520clouds%2520remain%2520challenging%2520due%2520to%2520sparse%2520data%252C%2520incomplete%2520structures%252C%2520and%2520limited%2520semantic%2520information.%2520Capturing%2520contextual%2520relationships%2520between%2520distant%2520objects%2520presents%2520additional%2520difficulties.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520GraphFusion3D%252C%2520a%2520unified%2520framework%2520combining%2520multi-modal%2520fusion%2520with%2520advanced%2520feature%2520learning.%2520Our%2520approach%2520introduces%2520the%2520Adaptive%2520Cross-Modal%2520Transformer%2520%2528ACMT%2529%252C%2520which%2520adaptively%2520integrates%2520image%2520features%2520into%2520point%2520representations%2520to%2520enrich%2520both%2520geometric%2520and%2520semantic%2520information.%2520For%2520proposal%2520refinement%252C%2520we%2520introduce%2520the%2520Graph%2520Reasoning%2520Module%2520%2528GRM%2529%252C%2520a%2520novel%2520mechanism%2520that%2520models%2520neighborhood%2520relationships%2520to%2520simultaneously%2520capture%2520local%2520geometric%2520structures%2520and%2520global%2520semantic%2520context.%2520The%2520module%2520employs%2520multi-scale%2520graph%2520attention%2520to%2520dynamically%2520weight%2520both%2520spatial%2520proximity%2520and%2520feature%2520similarity%2520between%2520proposals.%2520We%2520further%2520employ%2520a%2520cascade%2520decoder%2520that%2520progressively%2520refines%2520detections%2520through%2520multi-stage%2520predictions.%2520Extensive%2520experiments%2520on%2520SUN%2520RGB-D%2520%252870.6%255C%2525%2520AP%2524_%257B25%257D%2524%2520and%252051.2%255C%2525%2520AP%2524_%257B50%257D%2524%2529%2520and%2520ScanNetV2%2520%252875.1%255C%2525%2520AP%2524_%257B25%257D%2524%2520and%252060.8%255C%2525%2520AP%2524_%257B50%257D%2524%2529%2520demonstrate%2520a%2520substantial%2520performance%2520improvement%2520over%2520existing%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02991v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphFusion3D%3A%20Dynamic%20Graph%20Attention%20Convolution%20with%20Adaptive%20Cross-Modal%20Transformer%20for%203D%20Object%20Detection&entry.906535625=Md%20Sohag%20Mia%20and%20Md%20Nahid%20Hasan%20and%20Tawhid%20Ahmed%20and%20Muhammad%20Abdullah%20Adnan&entry.1292438233=Despite%20significant%20progress%20in%203D%20object%20detection%2C%20point%20clouds%20remain%20challenging%20due%20to%20sparse%20data%2C%20incomplete%20structures%2C%20and%20limited%20semantic%20information.%20Capturing%20contextual%20relationships%20between%20distant%20objects%20presents%20additional%20difficulties.%20To%20address%20these%20challenges%2C%20we%20propose%20GraphFusion3D%2C%20a%20unified%20framework%20combining%20multi-modal%20fusion%20with%20advanced%20feature%20learning.%20Our%20approach%20introduces%20the%20Adaptive%20Cross-Modal%20Transformer%20%28ACMT%29%2C%20which%20adaptively%20integrates%20image%20features%20into%20point%20representations%20to%20enrich%20both%20geometric%20and%20semantic%20information.%20For%20proposal%20refinement%2C%20we%20introduce%20the%20Graph%20Reasoning%20Module%20%28GRM%29%2C%20a%20novel%20mechanism%20that%20models%20neighborhood%20relationships%20to%20simultaneously%20capture%20local%20geometric%20structures%20and%20global%20semantic%20context.%20The%20module%20employs%20multi-scale%20graph%20attention%20to%20dynamically%20weight%20both%20spatial%20proximity%20and%20feature%20similarity%20between%20proposals.%20We%20further%20employ%20a%20cascade%20decoder%20that%20progressively%20refines%20detections%20through%20multi-stage%20predictions.%20Extensive%20experiments%20on%20SUN%20RGB-D%20%2870.6%5C%25%20AP%24_%7B25%7D%24%20and%2051.2%5C%25%20AP%24_%7B50%7D%24%29%20and%20ScanNetV2%20%2875.1%5C%25%20AP%24_%7B25%7D%24%20and%2060.8%5C%25%20AP%24_%7B50%7D%24%29%20demonstrate%20a%20substantial%20performance%20improvement%20over%20existing%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2512.02991v1&entry.124074799=Read"},
{"title": "Defense That Attacks: How Robust Models Become Better Attackers", "author": "Mohamed Awad and Mahmoud Akrm and Walid Gomaa", "abstract": "Deep learning has achieved great success in computer vision, but remains vulnerable to adversarial attacks. Adversarial training is the leading defense designed to improve model robustness. However, its effect on the transferability of attacks is underexplored. In this work, we ask whether adversarial training unintentionally increases the transferability of adversarial examples. To answer this, we trained a diverse zoo of 36 models, including CNNs and ViTs, and conducted comprehensive transferability experiments. Our results reveal a clear paradox: adversarially trained (AT) models produce perturbations that transfer more effectively than those from standard models, which introduce a new ecosystem risk. To enable reproducibility and further study, we release all models, code, and experimental scripts. Furthermore, we argue that robustness evaluations should assess not only the resistance of a model to transferred attacks but also its propensity to produce transferable adversarial examples.", "link": "http://arxiv.org/abs/2512.02830v1", "date": "2025-12-02", "relevancy": 2.3996, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5007}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4716}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4675}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Defense%20That%20Attacks%3A%20How%20Robust%20Models%20Become%20Better%20Attackers&body=Title%3A%20Defense%20That%20Attacks%3A%20How%20Robust%20Models%20Become%20Better%20Attackers%0AAuthor%3A%20Mohamed%20Awad%20and%20Mahmoud%20Akrm%20and%20Walid%20Gomaa%0AAbstract%3A%20Deep%20learning%20has%20achieved%20great%20success%20in%20computer%20vision%2C%20but%20remains%20vulnerable%20to%20adversarial%20attacks.%20Adversarial%20training%20is%20the%20leading%20defense%20designed%20to%20improve%20model%20robustness.%20However%2C%20its%20effect%20on%20the%20transferability%20of%20attacks%20is%20underexplored.%20In%20this%20work%2C%20we%20ask%20whether%20adversarial%20training%20unintentionally%20increases%20the%20transferability%20of%20adversarial%20examples.%20To%20answer%20this%2C%20we%20trained%20a%20diverse%20zoo%20of%2036%20models%2C%20including%20CNNs%20and%20ViTs%2C%20and%20conducted%20comprehensive%20transferability%20experiments.%20Our%20results%20reveal%20a%20clear%20paradox%3A%20adversarially%20trained%20%28AT%29%20models%20produce%20perturbations%20that%20transfer%20more%20effectively%20than%20those%20from%20standard%20models%2C%20which%20introduce%20a%20new%20ecosystem%20risk.%20To%20enable%20reproducibility%20and%20further%20study%2C%20we%20release%20all%20models%2C%20code%2C%20and%20experimental%20scripts.%20Furthermore%2C%20we%20argue%20that%20robustness%20evaluations%20should%20assess%20not%20only%20the%20resistance%20of%20a%20model%20to%20transferred%20attacks%20but%20also%20its%20propensity%20to%20produce%20transferable%20adversarial%20examples.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02830v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDefense%2520That%2520Attacks%253A%2520How%2520Robust%2520Models%2520Become%2520Better%2520Attackers%26entry.906535625%3DMohamed%2520Awad%2520and%2520Mahmoud%2520Akrm%2520and%2520Walid%2520Gomaa%26entry.1292438233%3DDeep%2520learning%2520has%2520achieved%2520great%2520success%2520in%2520computer%2520vision%252C%2520but%2520remains%2520vulnerable%2520to%2520adversarial%2520attacks.%2520Adversarial%2520training%2520is%2520the%2520leading%2520defense%2520designed%2520to%2520improve%2520model%2520robustness.%2520However%252C%2520its%2520effect%2520on%2520the%2520transferability%2520of%2520attacks%2520is%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520ask%2520whether%2520adversarial%2520training%2520unintentionally%2520increases%2520the%2520transferability%2520of%2520adversarial%2520examples.%2520To%2520answer%2520this%252C%2520we%2520trained%2520a%2520diverse%2520zoo%2520of%252036%2520models%252C%2520including%2520CNNs%2520and%2520ViTs%252C%2520and%2520conducted%2520comprehensive%2520transferability%2520experiments.%2520Our%2520results%2520reveal%2520a%2520clear%2520paradox%253A%2520adversarially%2520trained%2520%2528AT%2529%2520models%2520produce%2520perturbations%2520that%2520transfer%2520more%2520effectively%2520than%2520those%2520from%2520standard%2520models%252C%2520which%2520introduce%2520a%2520new%2520ecosystem%2520risk.%2520To%2520enable%2520reproducibility%2520and%2520further%2520study%252C%2520we%2520release%2520all%2520models%252C%2520code%252C%2520and%2520experimental%2520scripts.%2520Furthermore%252C%2520we%2520argue%2520that%2520robustness%2520evaluations%2520should%2520assess%2520not%2520only%2520the%2520resistance%2520of%2520a%2520model%2520to%2520transferred%2520attacks%2520but%2520also%2520its%2520propensity%2520to%2520produce%2520transferable%2520adversarial%2520examples.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02830v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Defense%20That%20Attacks%3A%20How%20Robust%20Models%20Become%20Better%20Attackers&entry.906535625=Mohamed%20Awad%20and%20Mahmoud%20Akrm%20and%20Walid%20Gomaa&entry.1292438233=Deep%20learning%20has%20achieved%20great%20success%20in%20computer%20vision%2C%20but%20remains%20vulnerable%20to%20adversarial%20attacks.%20Adversarial%20training%20is%20the%20leading%20defense%20designed%20to%20improve%20model%20robustness.%20However%2C%20its%20effect%20on%20the%20transferability%20of%20attacks%20is%20underexplored.%20In%20this%20work%2C%20we%20ask%20whether%20adversarial%20training%20unintentionally%20increases%20the%20transferability%20of%20adversarial%20examples.%20To%20answer%20this%2C%20we%20trained%20a%20diverse%20zoo%20of%2036%20models%2C%20including%20CNNs%20and%20ViTs%2C%20and%20conducted%20comprehensive%20transferability%20experiments.%20Our%20results%20reveal%20a%20clear%20paradox%3A%20adversarially%20trained%20%28AT%29%20models%20produce%20perturbations%20that%20transfer%20more%20effectively%20than%20those%20from%20standard%20models%2C%20which%20introduce%20a%20new%20ecosystem%20risk.%20To%20enable%20reproducibility%20and%20further%20study%2C%20we%20release%20all%20models%2C%20code%2C%20and%20experimental%20scripts.%20Furthermore%2C%20we%20argue%20that%20robustness%20evaluations%20should%20assess%20not%20only%20the%20resistance%20of%20a%20model%20to%20transferred%20attacks%20but%20also%20its%20propensity%20to%20produce%20transferable%20adversarial%20examples.&entry.1838667208=http%3A//arxiv.org/abs/2512.02830v1&entry.124074799=Read"},
{"title": "SMP: Reusable Score-Matching Motion Priors for Physics-Based Character Control", "author": "Yuxuan Mu and Ziyu Zhang and Yi Shi and Minami Matsumoto and Kotaro Imamura and Guy Tevet and Chuan Guo and Michael Taylor and Chang Shu and Pengcheng Xi and Xue Bin Peng", "abstract": "Data-driven motion priors that can guide agents toward producing naturalistic behaviors play a pivotal role in creating life-like virtual characters. Adversarial imitation learning has been a highly effective method for learning motion priors from reference motion data. However, adversarial priors, with few exceptions, need to be retrained for each new controller, thereby limiting their reusability and necessitating the retention of the reference motion data when training on downstream tasks. In this work, we present Score-Matching Motion Priors (SMP), which leverages pre-trained motion diffusion models and score distillation sampling (SDS) to create reusable task-agnostic motion priors. SMPs can be pre-trained on a motion dataset, independent of any control policy or task. Once trained, SMPs can be kept frozen and reused as general-purpose reward functions to train policies to produce naturalistic behaviors for downstream tasks. We show that a general motion prior trained on large-scale datasets can be repurposed into a variety of style-specific priors. Furthermore SMP can compose different styles to synthesize new styles not present in the original dataset. Our method produces high-quality motion comparable to state-of-the-art adversarial imitation learning methods through reusable and modular motion priors. We demonstrate the effectiveness of SMP across a diverse suite of control tasks with physically simulated humanoid characters. Video demo available at https://youtu.be/ravlZJteS20", "link": "http://arxiv.org/abs/2512.03028v1", "date": "2025-12-02", "relevancy": 2.3989, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6534}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.566}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SMP%3A%20Reusable%20Score-Matching%20Motion%20Priors%20for%20Physics-Based%20Character%20Control&body=Title%3A%20SMP%3A%20Reusable%20Score-Matching%20Motion%20Priors%20for%20Physics-Based%20Character%20Control%0AAuthor%3A%20Yuxuan%20Mu%20and%20Ziyu%20Zhang%20and%20Yi%20Shi%20and%20Minami%20Matsumoto%20and%20Kotaro%20Imamura%20and%20Guy%20Tevet%20and%20Chuan%20Guo%20and%20Michael%20Taylor%20and%20Chang%20Shu%20and%20Pengcheng%20Xi%20and%20Xue%20Bin%20Peng%0AAbstract%3A%20Data-driven%20motion%20priors%20that%20can%20guide%20agents%20toward%20producing%20naturalistic%20behaviors%20play%20a%20pivotal%20role%20in%20creating%20life-like%20virtual%20characters.%20Adversarial%20imitation%20learning%20has%20been%20a%20highly%20effective%20method%20for%20learning%20motion%20priors%20from%20reference%20motion%20data.%20However%2C%20adversarial%20priors%2C%20with%20few%20exceptions%2C%20need%20to%20be%20retrained%20for%20each%20new%20controller%2C%20thereby%20limiting%20their%20reusability%20and%20necessitating%20the%20retention%20of%20the%20reference%20motion%20data%20when%20training%20on%20downstream%20tasks.%20In%20this%20work%2C%20we%20present%20Score-Matching%20Motion%20Priors%20%28SMP%29%2C%20which%20leverages%20pre-trained%20motion%20diffusion%20models%20and%20score%20distillation%20sampling%20%28SDS%29%20to%20create%20reusable%20task-agnostic%20motion%20priors.%20SMPs%20can%20be%20pre-trained%20on%20a%20motion%20dataset%2C%20independent%20of%20any%20control%20policy%20or%20task.%20Once%20trained%2C%20SMPs%20can%20be%20kept%20frozen%20and%20reused%20as%20general-purpose%20reward%20functions%20to%20train%20policies%20to%20produce%20naturalistic%20behaviors%20for%20downstream%20tasks.%20We%20show%20that%20a%20general%20motion%20prior%20trained%20on%20large-scale%20datasets%20can%20be%20repurposed%20into%20a%20variety%20of%20style-specific%20priors.%20Furthermore%20SMP%20can%20compose%20different%20styles%20to%20synthesize%20new%20styles%20not%20present%20in%20the%20original%20dataset.%20Our%20method%20produces%20high-quality%20motion%20comparable%20to%20state-of-the-art%20adversarial%20imitation%20learning%20methods%20through%20reusable%20and%20modular%20motion%20priors.%20We%20demonstrate%20the%20effectiveness%20of%20SMP%20across%20a%20diverse%20suite%20of%20control%20tasks%20with%20physically%20simulated%20humanoid%20characters.%20Video%20demo%20available%20at%20https%3A//youtu.be/ravlZJteS20%0ALink%3A%20http%3A//arxiv.org/abs/2512.03028v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSMP%253A%2520Reusable%2520Score-Matching%2520Motion%2520Priors%2520for%2520Physics-Based%2520Character%2520Control%26entry.906535625%3DYuxuan%2520Mu%2520and%2520Ziyu%2520Zhang%2520and%2520Yi%2520Shi%2520and%2520Minami%2520Matsumoto%2520and%2520Kotaro%2520Imamura%2520and%2520Guy%2520Tevet%2520and%2520Chuan%2520Guo%2520and%2520Michael%2520Taylor%2520and%2520Chang%2520Shu%2520and%2520Pengcheng%2520Xi%2520and%2520Xue%2520Bin%2520Peng%26entry.1292438233%3DData-driven%2520motion%2520priors%2520that%2520can%2520guide%2520agents%2520toward%2520producing%2520naturalistic%2520behaviors%2520play%2520a%2520pivotal%2520role%2520in%2520creating%2520life-like%2520virtual%2520characters.%2520Adversarial%2520imitation%2520learning%2520has%2520been%2520a%2520highly%2520effective%2520method%2520for%2520learning%2520motion%2520priors%2520from%2520reference%2520motion%2520data.%2520However%252C%2520adversarial%2520priors%252C%2520with%2520few%2520exceptions%252C%2520need%2520to%2520be%2520retrained%2520for%2520each%2520new%2520controller%252C%2520thereby%2520limiting%2520their%2520reusability%2520and%2520necessitating%2520the%2520retention%2520of%2520the%2520reference%2520motion%2520data%2520when%2520training%2520on%2520downstream%2520tasks.%2520In%2520this%2520work%252C%2520we%2520present%2520Score-Matching%2520Motion%2520Priors%2520%2528SMP%2529%252C%2520which%2520leverages%2520pre-trained%2520motion%2520diffusion%2520models%2520and%2520score%2520distillation%2520sampling%2520%2528SDS%2529%2520to%2520create%2520reusable%2520task-agnostic%2520motion%2520priors.%2520SMPs%2520can%2520be%2520pre-trained%2520on%2520a%2520motion%2520dataset%252C%2520independent%2520of%2520any%2520control%2520policy%2520or%2520task.%2520Once%2520trained%252C%2520SMPs%2520can%2520be%2520kept%2520frozen%2520and%2520reused%2520as%2520general-purpose%2520reward%2520functions%2520to%2520train%2520policies%2520to%2520produce%2520naturalistic%2520behaviors%2520for%2520downstream%2520tasks.%2520We%2520show%2520that%2520a%2520general%2520motion%2520prior%2520trained%2520on%2520large-scale%2520datasets%2520can%2520be%2520repurposed%2520into%2520a%2520variety%2520of%2520style-specific%2520priors.%2520Furthermore%2520SMP%2520can%2520compose%2520different%2520styles%2520to%2520synthesize%2520new%2520styles%2520not%2520present%2520in%2520the%2520original%2520dataset.%2520Our%2520method%2520produces%2520high-quality%2520motion%2520comparable%2520to%2520state-of-the-art%2520adversarial%2520imitation%2520learning%2520methods%2520through%2520reusable%2520and%2520modular%2520motion%2520priors.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520SMP%2520across%2520a%2520diverse%2520suite%2520of%2520control%2520tasks%2520with%2520physically%2520simulated%2520humanoid%2520characters.%2520Video%2520demo%2520available%2520at%2520https%253A//youtu.be/ravlZJteS20%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03028v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SMP%3A%20Reusable%20Score-Matching%20Motion%20Priors%20for%20Physics-Based%20Character%20Control&entry.906535625=Yuxuan%20Mu%20and%20Ziyu%20Zhang%20and%20Yi%20Shi%20and%20Minami%20Matsumoto%20and%20Kotaro%20Imamura%20and%20Guy%20Tevet%20and%20Chuan%20Guo%20and%20Michael%20Taylor%20and%20Chang%20Shu%20and%20Pengcheng%20Xi%20and%20Xue%20Bin%20Peng&entry.1292438233=Data-driven%20motion%20priors%20that%20can%20guide%20agents%20toward%20producing%20naturalistic%20behaviors%20play%20a%20pivotal%20role%20in%20creating%20life-like%20virtual%20characters.%20Adversarial%20imitation%20learning%20has%20been%20a%20highly%20effective%20method%20for%20learning%20motion%20priors%20from%20reference%20motion%20data.%20However%2C%20adversarial%20priors%2C%20with%20few%20exceptions%2C%20need%20to%20be%20retrained%20for%20each%20new%20controller%2C%20thereby%20limiting%20their%20reusability%20and%20necessitating%20the%20retention%20of%20the%20reference%20motion%20data%20when%20training%20on%20downstream%20tasks.%20In%20this%20work%2C%20we%20present%20Score-Matching%20Motion%20Priors%20%28SMP%29%2C%20which%20leverages%20pre-trained%20motion%20diffusion%20models%20and%20score%20distillation%20sampling%20%28SDS%29%20to%20create%20reusable%20task-agnostic%20motion%20priors.%20SMPs%20can%20be%20pre-trained%20on%20a%20motion%20dataset%2C%20independent%20of%20any%20control%20policy%20or%20task.%20Once%20trained%2C%20SMPs%20can%20be%20kept%20frozen%20and%20reused%20as%20general-purpose%20reward%20functions%20to%20train%20policies%20to%20produce%20naturalistic%20behaviors%20for%20downstream%20tasks.%20We%20show%20that%20a%20general%20motion%20prior%20trained%20on%20large-scale%20datasets%20can%20be%20repurposed%20into%20a%20variety%20of%20style-specific%20priors.%20Furthermore%20SMP%20can%20compose%20different%20styles%20to%20synthesize%20new%20styles%20not%20present%20in%20the%20original%20dataset.%20Our%20method%20produces%20high-quality%20motion%20comparable%20to%20state-of-the-art%20adversarial%20imitation%20learning%20methods%20through%20reusable%20and%20modular%20motion%20priors.%20We%20demonstrate%20the%20effectiveness%20of%20SMP%20across%20a%20diverse%20suite%20of%20control%20tasks%20with%20physically%20simulated%20humanoid%20characters.%20Video%20demo%20available%20at%20https%3A//youtu.be/ravlZJteS20&entry.1838667208=http%3A//arxiv.org/abs/2512.03028v1&entry.124074799=Read"},
{"title": "RFOP: Rethinking Fusion and Orthogonal Projection for Face-Voice Association", "author": "Abdul Hannan and Furqan Malik and Hina Jabbar and Syed Suleman Sadiq and Mubashir Noman", "abstract": "Face-voice association in multilingual environment challenge 2026 aims to investigate the face-voice association task in multilingual scenario. The challenge introduces English-German face-voice pairs to be utilized in the evaluation phase. To this end, we revisit the fusion and orthogonal projection for face-voice association by effectively focusing on the relevant semantic information within the two modalities. Our method performs favorably on the English-German data split and ranked 3rd in the FAME 2026 challenge by achieving the EER of 33.1.", "link": "http://arxiv.org/abs/2512.02860v1", "date": "2025-12-02", "relevancy": 2.3931, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5003}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5003}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RFOP%3A%20Rethinking%20Fusion%20and%20Orthogonal%20Projection%20for%20Face-Voice%20Association&body=Title%3A%20RFOP%3A%20Rethinking%20Fusion%20and%20Orthogonal%20Projection%20for%20Face-Voice%20Association%0AAuthor%3A%20Abdul%20Hannan%20and%20Furqan%20Malik%20and%20Hina%20Jabbar%20and%20Syed%20Suleman%20Sadiq%20and%20Mubashir%20Noman%0AAbstract%3A%20Face-voice%20association%20in%20multilingual%20environment%20challenge%202026%20aims%20to%20investigate%20the%20face-voice%20association%20task%20in%20multilingual%20scenario.%20The%20challenge%20introduces%20English-German%20face-voice%20pairs%20to%20be%20utilized%20in%20the%20evaluation%20phase.%20To%20this%20end%2C%20we%20revisit%20the%20fusion%20and%20orthogonal%20projection%20for%20face-voice%20association%20by%20effectively%20focusing%20on%20the%20relevant%20semantic%20information%20within%20the%20two%20modalities.%20Our%20method%20performs%20favorably%20on%20the%20English-German%20data%20split%20and%20ranked%203rd%20in%20the%20FAME%202026%20challenge%20by%20achieving%20the%20EER%20of%2033.1.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRFOP%253A%2520Rethinking%2520Fusion%2520and%2520Orthogonal%2520Projection%2520for%2520Face-Voice%2520Association%26entry.906535625%3DAbdul%2520Hannan%2520and%2520Furqan%2520Malik%2520and%2520Hina%2520Jabbar%2520and%2520Syed%2520Suleman%2520Sadiq%2520and%2520Mubashir%2520Noman%26entry.1292438233%3DFace-voice%2520association%2520in%2520multilingual%2520environment%2520challenge%25202026%2520aims%2520to%2520investigate%2520the%2520face-voice%2520association%2520task%2520in%2520multilingual%2520scenario.%2520The%2520challenge%2520introduces%2520English-German%2520face-voice%2520pairs%2520to%2520be%2520utilized%2520in%2520the%2520evaluation%2520phase.%2520To%2520this%2520end%252C%2520we%2520revisit%2520the%2520fusion%2520and%2520orthogonal%2520projection%2520for%2520face-voice%2520association%2520by%2520effectively%2520focusing%2520on%2520the%2520relevant%2520semantic%2520information%2520within%2520the%2520two%2520modalities.%2520Our%2520method%2520performs%2520favorably%2520on%2520the%2520English-German%2520data%2520split%2520and%2520ranked%25203rd%2520in%2520the%2520FAME%25202026%2520challenge%2520by%2520achieving%2520the%2520EER%2520of%252033.1.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RFOP%3A%20Rethinking%20Fusion%20and%20Orthogonal%20Projection%20for%20Face-Voice%20Association&entry.906535625=Abdul%20Hannan%20and%20Furqan%20Malik%20and%20Hina%20Jabbar%20and%20Syed%20Suleman%20Sadiq%20and%20Mubashir%20Noman&entry.1292438233=Face-voice%20association%20in%20multilingual%20environment%20challenge%202026%20aims%20to%20investigate%20the%20face-voice%20association%20task%20in%20multilingual%20scenario.%20The%20challenge%20introduces%20English-German%20face-voice%20pairs%20to%20be%20utilized%20in%20the%20evaluation%20phase.%20To%20this%20end%2C%20we%20revisit%20the%20fusion%20and%20orthogonal%20projection%20for%20face-voice%20association%20by%20effectively%20focusing%20on%20the%20relevant%20semantic%20information%20within%20the%20two%20modalities.%20Our%20method%20performs%20favorably%20on%20the%20English-German%20data%20split%20and%20ranked%203rd%20in%20the%20FAME%202026%20challenge%20by%20achieving%20the%20EER%20of%2033.1.&entry.1838667208=http%3A//arxiv.org/abs/2512.02860v1&entry.124074799=Read"},
{"title": "SAND Challenge: Four Approaches for Dysartria Severity Classification", "author": "Gauri Deshpande and Harish Battula and Ashish Panda and Sunil Kumar Kopparapu", "abstract": "This paper presents a unified study of four distinct modeling approaches for classifying dysarthria severity in the Speech Analysis for Neurodegenerative Diseases (SAND) challenge. All models tackle the same five class classification task using a common dataset of speech recordings. We investigate: (1) a ViT-OF method leveraging a Vision Transformer on spectrogram images, (2) a 1D-CNN approach using eight 1-D CNN's with majority-vote fusion, (3) a BiLSTM-OF approach using nine BiLSTM models with majority vote fusion, and (4) a Hierarchical XGBoost ensemble that combines glottal and formant features through a two stage learning framework. Each method is described, and their performances on a validation set of 53 speakers are compared. Results show that while the feature-engineered XGBoost ensemble achieves the highest macro-F1 (0.86), the deep learning models (ViT, CNN, BiLSTM) attain competitive F1-scores (0.70) and offer complementary insights into the problem.", "link": "http://arxiv.org/abs/2512.02669v1", "date": "2025-12-02", "relevancy": 2.3723, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4844}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4844}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAND%20Challenge%3A%20Four%20Approaches%20for%20Dysartria%20Severity%20Classification&body=Title%3A%20SAND%20Challenge%3A%20Four%20Approaches%20for%20Dysartria%20Severity%20Classification%0AAuthor%3A%20Gauri%20Deshpande%20and%20Harish%20Battula%20and%20Ashish%20Panda%20and%20Sunil%20Kumar%20Kopparapu%0AAbstract%3A%20This%20paper%20presents%20a%20unified%20study%20of%20four%20distinct%20modeling%20approaches%20for%20classifying%20dysarthria%20severity%20in%20the%20Speech%20Analysis%20for%20Neurodegenerative%20Diseases%20%28SAND%29%20challenge.%20All%20models%20tackle%20the%20same%20five%20class%20classification%20task%20using%20a%20common%20dataset%20of%20speech%20recordings.%20We%20investigate%3A%20%281%29%20a%20ViT-OF%20method%20leveraging%20a%20Vision%20Transformer%20on%20spectrogram%20images%2C%20%282%29%20a%201D-CNN%20approach%20using%20eight%201-D%20CNN%27s%20with%20majority-vote%20fusion%2C%20%283%29%20a%20BiLSTM-OF%20approach%20using%20nine%20BiLSTM%20models%20with%20majority%20vote%20fusion%2C%20and%20%284%29%20a%20Hierarchical%20XGBoost%20ensemble%20that%20combines%20glottal%20and%20formant%20features%20through%20a%20two%20stage%20learning%20framework.%20Each%20method%20is%20described%2C%20and%20their%20performances%20on%20a%20validation%20set%20of%2053%20speakers%20are%20compared.%20Results%20show%20that%20while%20the%20feature-engineered%20XGBoost%20ensemble%20achieves%20the%20highest%20macro-F1%20%280.86%29%2C%20the%20deep%20learning%20models%20%28ViT%2C%20CNN%2C%20BiLSTM%29%20attain%20competitive%20F1-scores%20%280.70%29%20and%20offer%20complementary%20insights%20into%20the%20problem.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02669v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAND%2520Challenge%253A%2520Four%2520Approaches%2520for%2520Dysartria%2520Severity%2520Classification%26entry.906535625%3DGauri%2520Deshpande%2520and%2520Harish%2520Battula%2520and%2520Ashish%2520Panda%2520and%2520Sunil%2520Kumar%2520Kopparapu%26entry.1292438233%3DThis%2520paper%2520presents%2520a%2520unified%2520study%2520of%2520four%2520distinct%2520modeling%2520approaches%2520for%2520classifying%2520dysarthria%2520severity%2520in%2520the%2520Speech%2520Analysis%2520for%2520Neurodegenerative%2520Diseases%2520%2528SAND%2529%2520challenge.%2520All%2520models%2520tackle%2520the%2520same%2520five%2520class%2520classification%2520task%2520using%2520a%2520common%2520dataset%2520of%2520speech%2520recordings.%2520We%2520investigate%253A%2520%25281%2529%2520a%2520ViT-OF%2520method%2520leveraging%2520a%2520Vision%2520Transformer%2520on%2520spectrogram%2520images%252C%2520%25282%2529%2520a%25201D-CNN%2520approach%2520using%2520eight%25201-D%2520CNN%2527s%2520with%2520majority-vote%2520fusion%252C%2520%25283%2529%2520a%2520BiLSTM-OF%2520approach%2520using%2520nine%2520BiLSTM%2520models%2520with%2520majority%2520vote%2520fusion%252C%2520and%2520%25284%2529%2520a%2520Hierarchical%2520XGBoost%2520ensemble%2520that%2520combines%2520glottal%2520and%2520formant%2520features%2520through%2520a%2520two%2520stage%2520learning%2520framework.%2520Each%2520method%2520is%2520described%252C%2520and%2520their%2520performances%2520on%2520a%2520validation%2520set%2520of%252053%2520speakers%2520are%2520compared.%2520Results%2520show%2520that%2520while%2520the%2520feature-engineered%2520XGBoost%2520ensemble%2520achieves%2520the%2520highest%2520macro-F1%2520%25280.86%2529%252C%2520the%2520deep%2520learning%2520models%2520%2528ViT%252C%2520CNN%252C%2520BiLSTM%2529%2520attain%2520competitive%2520F1-scores%2520%25280.70%2529%2520and%2520offer%2520complementary%2520insights%2520into%2520the%2520problem.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02669v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAND%20Challenge%3A%20Four%20Approaches%20for%20Dysartria%20Severity%20Classification&entry.906535625=Gauri%20Deshpande%20and%20Harish%20Battula%20and%20Ashish%20Panda%20and%20Sunil%20Kumar%20Kopparapu&entry.1292438233=This%20paper%20presents%20a%20unified%20study%20of%20four%20distinct%20modeling%20approaches%20for%20classifying%20dysarthria%20severity%20in%20the%20Speech%20Analysis%20for%20Neurodegenerative%20Diseases%20%28SAND%29%20challenge.%20All%20models%20tackle%20the%20same%20five%20class%20classification%20task%20using%20a%20common%20dataset%20of%20speech%20recordings.%20We%20investigate%3A%20%281%29%20a%20ViT-OF%20method%20leveraging%20a%20Vision%20Transformer%20on%20spectrogram%20images%2C%20%282%29%20a%201D-CNN%20approach%20using%20eight%201-D%20CNN%27s%20with%20majority-vote%20fusion%2C%20%283%29%20a%20BiLSTM-OF%20approach%20using%20nine%20BiLSTM%20models%20with%20majority%20vote%20fusion%2C%20and%20%284%29%20a%20Hierarchical%20XGBoost%20ensemble%20that%20combines%20glottal%20and%20formant%20features%20through%20a%20two%20stage%20learning%20framework.%20Each%20method%20is%20described%2C%20and%20their%20performances%20on%20a%20validation%20set%20of%2053%20speakers%20are%20compared.%20Results%20show%20that%20while%20the%20feature-engineered%20XGBoost%20ensemble%20achieves%20the%20highest%20macro-F1%20%280.86%29%2C%20the%20deep%20learning%20models%20%28ViT%2C%20CNN%2C%20BiLSTM%29%20attain%20competitive%20F1-scores%20%280.70%29%20and%20offer%20complementary%20insights%20into%20the%20problem.&entry.1838667208=http%3A//arxiv.org/abs/2512.02669v1&entry.124074799=Read"},
{"title": "PRIMU: Uncertainty Estimation for Novel Views in Gaussian Splatting from Primitive-Based Representations of Error and Coverage", "author": "Thomas Gottwald and Edgar Heinert and Peter Stehr and Chamuditha Jayanga Galappaththige and Matthias Rottmann", "abstract": "We introduce Primitive-based Representations of Uncertainty (PRIMU), a post-hoc uncertainty estimation (UE) framework for Gaussian Splatting (GS). Reliable UE is essential for deploying GS in safety-critical domains such as robotics and medicine. Existing approaches typically estimate Gaussian-primitive variances and rely on the rendering process to obtain pixel-wise uncertainties. In contrast, we construct primitive-level representations of error and visibility/coverage from training views, capturing interpretable uncertainty information. These representations are obtained by projecting view-dependent training errors and coverage statistics onto the primitives. Uncertainties for novel views are inferred by rendering these primitive-level representations, producing uncertainty feature maps, which are aggregate through pixel-wise regression on holdout data. We analyze combinations of uncertainty feature maps and regression models to understand how their interactions affect prediction accuracy and generalization. PRIMU also enables an effective active view selection strategy by directly leveraging these uncertainty feature maps. Additionally, we study the effect of separating splatting into foreground and background regions. Our estimates show strong correlations with true errors, outperforming state-of-the-art methods, especially for depth UE and foreground objects. Finally, our regression models show generalization capabilities to unseen scenes, enabling UE without additional holdout data.", "link": "http://arxiv.org/abs/2508.02443v2", "date": "2025-12-02", "relevancy": 2.3715, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6188}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5795}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PRIMU%3A%20Uncertainty%20Estimation%20for%20Novel%20Views%20in%20Gaussian%20Splatting%20from%20Primitive-Based%20Representations%20of%20Error%20and%20Coverage&body=Title%3A%20PRIMU%3A%20Uncertainty%20Estimation%20for%20Novel%20Views%20in%20Gaussian%20Splatting%20from%20Primitive-Based%20Representations%20of%20Error%20and%20Coverage%0AAuthor%3A%20Thomas%20Gottwald%20and%20Edgar%20Heinert%20and%20Peter%20Stehr%20and%20Chamuditha%20Jayanga%20Galappaththige%20and%20Matthias%20Rottmann%0AAbstract%3A%20We%20introduce%20Primitive-based%20Representations%20of%20Uncertainty%20%28PRIMU%29%2C%20a%20post-hoc%20uncertainty%20estimation%20%28UE%29%20framework%20for%20Gaussian%20Splatting%20%28GS%29.%20Reliable%20UE%20is%20essential%20for%20deploying%20GS%20in%20safety-critical%20domains%20such%20as%20robotics%20and%20medicine.%20Existing%20approaches%20typically%20estimate%20Gaussian-primitive%20variances%20and%20rely%20on%20the%20rendering%20process%20to%20obtain%20pixel-wise%20uncertainties.%20In%20contrast%2C%20we%20construct%20primitive-level%20representations%20of%20error%20and%20visibility/coverage%20from%20training%20views%2C%20capturing%20interpretable%20uncertainty%20information.%20These%20representations%20are%20obtained%20by%20projecting%20view-dependent%20training%20errors%20and%20coverage%20statistics%20onto%20the%20primitives.%20Uncertainties%20for%20novel%20views%20are%20inferred%20by%20rendering%20these%20primitive-level%20representations%2C%20producing%20uncertainty%20feature%20maps%2C%20which%20are%20aggregate%20through%20pixel-wise%20regression%20on%20holdout%20data.%20We%20analyze%20combinations%20of%20uncertainty%20feature%20maps%20and%20regression%20models%20to%20understand%20how%20their%20interactions%20affect%20prediction%20accuracy%20and%20generalization.%20PRIMU%20also%20enables%20an%20effective%20active%20view%20selection%20strategy%20by%20directly%20leveraging%20these%20uncertainty%20feature%20maps.%20Additionally%2C%20we%20study%20the%20effect%20of%20separating%20splatting%20into%20foreground%20and%20background%20regions.%20Our%20estimates%20show%20strong%20correlations%20with%20true%20errors%2C%20outperforming%20state-of-the-art%20methods%2C%20especially%20for%20depth%20UE%20and%20foreground%20objects.%20Finally%2C%20our%20regression%20models%20show%20generalization%20capabilities%20to%20unseen%20scenes%2C%20enabling%20UE%20without%20additional%20holdout%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2508.02443v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPRIMU%253A%2520Uncertainty%2520Estimation%2520for%2520Novel%2520Views%2520in%2520Gaussian%2520Splatting%2520from%2520Primitive-Based%2520Representations%2520of%2520Error%2520and%2520Coverage%26entry.906535625%3DThomas%2520Gottwald%2520and%2520Edgar%2520Heinert%2520and%2520Peter%2520Stehr%2520and%2520Chamuditha%2520Jayanga%2520Galappaththige%2520and%2520Matthias%2520Rottmann%26entry.1292438233%3DWe%2520introduce%2520Primitive-based%2520Representations%2520of%2520Uncertainty%2520%2528PRIMU%2529%252C%2520a%2520post-hoc%2520uncertainty%2520estimation%2520%2528UE%2529%2520framework%2520for%2520Gaussian%2520Splatting%2520%2528GS%2529.%2520Reliable%2520UE%2520is%2520essential%2520for%2520deploying%2520GS%2520in%2520safety-critical%2520domains%2520such%2520as%2520robotics%2520and%2520medicine.%2520Existing%2520approaches%2520typically%2520estimate%2520Gaussian-primitive%2520variances%2520and%2520rely%2520on%2520the%2520rendering%2520process%2520to%2520obtain%2520pixel-wise%2520uncertainties.%2520In%2520contrast%252C%2520we%2520construct%2520primitive-level%2520representations%2520of%2520error%2520and%2520visibility/coverage%2520from%2520training%2520views%252C%2520capturing%2520interpretable%2520uncertainty%2520information.%2520These%2520representations%2520are%2520obtained%2520by%2520projecting%2520view-dependent%2520training%2520errors%2520and%2520coverage%2520statistics%2520onto%2520the%2520primitives.%2520Uncertainties%2520for%2520novel%2520views%2520are%2520inferred%2520by%2520rendering%2520these%2520primitive-level%2520representations%252C%2520producing%2520uncertainty%2520feature%2520maps%252C%2520which%2520are%2520aggregate%2520through%2520pixel-wise%2520regression%2520on%2520holdout%2520data.%2520We%2520analyze%2520combinations%2520of%2520uncertainty%2520feature%2520maps%2520and%2520regression%2520models%2520to%2520understand%2520how%2520their%2520interactions%2520affect%2520prediction%2520accuracy%2520and%2520generalization.%2520PRIMU%2520also%2520enables%2520an%2520effective%2520active%2520view%2520selection%2520strategy%2520by%2520directly%2520leveraging%2520these%2520uncertainty%2520feature%2520maps.%2520Additionally%252C%2520we%2520study%2520the%2520effect%2520of%2520separating%2520splatting%2520into%2520foreground%2520and%2520background%2520regions.%2520Our%2520estimates%2520show%2520strong%2520correlations%2520with%2520true%2520errors%252C%2520outperforming%2520state-of-the-art%2520methods%252C%2520especially%2520for%2520depth%2520UE%2520and%2520foreground%2520objects.%2520Finally%252C%2520our%2520regression%2520models%2520show%2520generalization%2520capabilities%2520to%2520unseen%2520scenes%252C%2520enabling%2520UE%2520without%2520additional%2520holdout%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02443v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRIMU%3A%20Uncertainty%20Estimation%20for%20Novel%20Views%20in%20Gaussian%20Splatting%20from%20Primitive-Based%20Representations%20of%20Error%20and%20Coverage&entry.906535625=Thomas%20Gottwald%20and%20Edgar%20Heinert%20and%20Peter%20Stehr%20and%20Chamuditha%20Jayanga%20Galappaththige%20and%20Matthias%20Rottmann&entry.1292438233=We%20introduce%20Primitive-based%20Representations%20of%20Uncertainty%20%28PRIMU%29%2C%20a%20post-hoc%20uncertainty%20estimation%20%28UE%29%20framework%20for%20Gaussian%20Splatting%20%28GS%29.%20Reliable%20UE%20is%20essential%20for%20deploying%20GS%20in%20safety-critical%20domains%20such%20as%20robotics%20and%20medicine.%20Existing%20approaches%20typically%20estimate%20Gaussian-primitive%20variances%20and%20rely%20on%20the%20rendering%20process%20to%20obtain%20pixel-wise%20uncertainties.%20In%20contrast%2C%20we%20construct%20primitive-level%20representations%20of%20error%20and%20visibility/coverage%20from%20training%20views%2C%20capturing%20interpretable%20uncertainty%20information.%20These%20representations%20are%20obtained%20by%20projecting%20view-dependent%20training%20errors%20and%20coverage%20statistics%20onto%20the%20primitives.%20Uncertainties%20for%20novel%20views%20are%20inferred%20by%20rendering%20these%20primitive-level%20representations%2C%20producing%20uncertainty%20feature%20maps%2C%20which%20are%20aggregate%20through%20pixel-wise%20regression%20on%20holdout%20data.%20We%20analyze%20combinations%20of%20uncertainty%20feature%20maps%20and%20regression%20models%20to%20understand%20how%20their%20interactions%20affect%20prediction%20accuracy%20and%20generalization.%20PRIMU%20also%20enables%20an%20effective%20active%20view%20selection%20strategy%20by%20directly%20leveraging%20these%20uncertainty%20feature%20maps.%20Additionally%2C%20we%20study%20the%20effect%20of%20separating%20splatting%20into%20foreground%20and%20background%20regions.%20Our%20estimates%20show%20strong%20correlations%20with%20true%20errors%2C%20outperforming%20state-of-the-art%20methods%2C%20especially%20for%20depth%20UE%20and%20foreground%20objects.%20Finally%2C%20our%20regression%20models%20show%20generalization%20capabilities%20to%20unseen%20scenes%2C%20enabling%20UE%20without%20additional%20holdout%20data.&entry.1838667208=http%3A//arxiv.org/abs/2508.02443v2&entry.124074799=Read"},
{"title": "Cross-Lingual Prompt Steerability: Towards Accurate and Robust LLM Behavior across Languages", "author": "Lechen Zhang and Yusheng Zhou and Tolga Ergen and Lajanugen Logeswaran and Moontae Lee and David Jurgens", "abstract": "System prompts provide a lightweight yet powerful mechanism for conditioning large language models (LLMs) at inference time. While prior work has focused on English-only settings, real-world deployments benefit from having a single prompt to operate reliably across languages. This paper presents a comprehensive study of how different system prompts steer models toward accurate and robust cross-lingual behavior. We propose a unified four-dimensional evaluation framework to assess system prompts in multilingual environments. Through large-scale experiments on five languages, three LLMs, and three benchmarks, we uncover that certain prompt components, such as CoT, emotion, and scenario, correlate with robust multilingual behavior. We develop a prompt optimization framework for multilingual settings and show it can automatically discover prompts that improve all metrics by 5-10%. Finally, we analyze over 10 million reasoning units and find that more performant system prompts induce more structured and consistent reasoning patterns, while reducing unnecessary language-switching. Together, we highlight system prompt optimization as a scalable path to accurate and robust multilingual LLM behavior.", "link": "http://arxiv.org/abs/2512.02841v1", "date": "2025-12-02", "relevancy": 2.349, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4788}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4788}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4518}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Lingual%20Prompt%20Steerability%3A%20Towards%20Accurate%20and%20Robust%20LLM%20Behavior%20across%20Languages&body=Title%3A%20Cross-Lingual%20Prompt%20Steerability%3A%20Towards%20Accurate%20and%20Robust%20LLM%20Behavior%20across%20Languages%0AAuthor%3A%20Lechen%20Zhang%20and%20Yusheng%20Zhou%20and%20Tolga%20Ergen%20and%20Lajanugen%20Logeswaran%20and%20Moontae%20Lee%20and%20David%20Jurgens%0AAbstract%3A%20System%20prompts%20provide%20a%20lightweight%20yet%20powerful%20mechanism%20for%20conditioning%20large%20language%20models%20%28LLMs%29%20at%20inference%20time.%20While%20prior%20work%20has%20focused%20on%20English-only%20settings%2C%20real-world%20deployments%20benefit%20from%20having%20a%20single%20prompt%20to%20operate%20reliably%20across%20languages.%20This%20paper%20presents%20a%20comprehensive%20study%20of%20how%20different%20system%20prompts%20steer%20models%20toward%20accurate%20and%20robust%20cross-lingual%20behavior.%20We%20propose%20a%20unified%20four-dimensional%20evaluation%20framework%20to%20assess%20system%20prompts%20in%20multilingual%20environments.%20Through%20large-scale%20experiments%20on%20five%20languages%2C%20three%20LLMs%2C%20and%20three%20benchmarks%2C%20we%20uncover%20that%20certain%20prompt%20components%2C%20such%20as%20CoT%2C%20emotion%2C%20and%20scenario%2C%20correlate%20with%20robust%20multilingual%20behavior.%20We%20develop%20a%20prompt%20optimization%20framework%20for%20multilingual%20settings%20and%20show%20it%20can%20automatically%20discover%20prompts%20that%20improve%20all%20metrics%20by%205-10%25.%20Finally%2C%20we%20analyze%20over%2010%20million%20reasoning%20units%20and%20find%20that%20more%20performant%20system%20prompts%20induce%20more%20structured%20and%20consistent%20reasoning%20patterns%2C%20while%20reducing%20unnecessary%20language-switching.%20Together%2C%20we%20highlight%20system%20prompt%20optimization%20as%20a%20scalable%20path%20to%20accurate%20and%20robust%20multilingual%20LLM%20behavior.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02841v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Lingual%2520Prompt%2520Steerability%253A%2520Towards%2520Accurate%2520and%2520Robust%2520LLM%2520Behavior%2520across%2520Languages%26entry.906535625%3DLechen%2520Zhang%2520and%2520Yusheng%2520Zhou%2520and%2520Tolga%2520Ergen%2520and%2520Lajanugen%2520Logeswaran%2520and%2520Moontae%2520Lee%2520and%2520David%2520Jurgens%26entry.1292438233%3DSystem%2520prompts%2520provide%2520a%2520lightweight%2520yet%2520powerful%2520mechanism%2520for%2520conditioning%2520large%2520language%2520models%2520%2528LLMs%2529%2520at%2520inference%2520time.%2520While%2520prior%2520work%2520has%2520focused%2520on%2520English-only%2520settings%252C%2520real-world%2520deployments%2520benefit%2520from%2520having%2520a%2520single%2520prompt%2520to%2520operate%2520reliably%2520across%2520languages.%2520This%2520paper%2520presents%2520a%2520comprehensive%2520study%2520of%2520how%2520different%2520system%2520prompts%2520steer%2520models%2520toward%2520accurate%2520and%2520robust%2520cross-lingual%2520behavior.%2520We%2520propose%2520a%2520unified%2520four-dimensional%2520evaluation%2520framework%2520to%2520assess%2520system%2520prompts%2520in%2520multilingual%2520environments.%2520Through%2520large-scale%2520experiments%2520on%2520five%2520languages%252C%2520three%2520LLMs%252C%2520and%2520three%2520benchmarks%252C%2520we%2520uncover%2520that%2520certain%2520prompt%2520components%252C%2520such%2520as%2520CoT%252C%2520emotion%252C%2520and%2520scenario%252C%2520correlate%2520with%2520robust%2520multilingual%2520behavior.%2520We%2520develop%2520a%2520prompt%2520optimization%2520framework%2520for%2520multilingual%2520settings%2520and%2520show%2520it%2520can%2520automatically%2520discover%2520prompts%2520that%2520improve%2520all%2520metrics%2520by%25205-10%2525.%2520Finally%252C%2520we%2520analyze%2520over%252010%2520million%2520reasoning%2520units%2520and%2520find%2520that%2520more%2520performant%2520system%2520prompts%2520induce%2520more%2520structured%2520and%2520consistent%2520reasoning%2520patterns%252C%2520while%2520reducing%2520unnecessary%2520language-switching.%2520Together%252C%2520we%2520highlight%2520system%2520prompt%2520optimization%2520as%2520a%2520scalable%2520path%2520to%2520accurate%2520and%2520robust%2520multilingual%2520LLM%2520behavior.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02841v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Lingual%20Prompt%20Steerability%3A%20Towards%20Accurate%20and%20Robust%20LLM%20Behavior%20across%20Languages&entry.906535625=Lechen%20Zhang%20and%20Yusheng%20Zhou%20and%20Tolga%20Ergen%20and%20Lajanugen%20Logeswaran%20and%20Moontae%20Lee%20and%20David%20Jurgens&entry.1292438233=System%20prompts%20provide%20a%20lightweight%20yet%20powerful%20mechanism%20for%20conditioning%20large%20language%20models%20%28LLMs%29%20at%20inference%20time.%20While%20prior%20work%20has%20focused%20on%20English-only%20settings%2C%20real-world%20deployments%20benefit%20from%20having%20a%20single%20prompt%20to%20operate%20reliably%20across%20languages.%20This%20paper%20presents%20a%20comprehensive%20study%20of%20how%20different%20system%20prompts%20steer%20models%20toward%20accurate%20and%20robust%20cross-lingual%20behavior.%20We%20propose%20a%20unified%20four-dimensional%20evaluation%20framework%20to%20assess%20system%20prompts%20in%20multilingual%20environments.%20Through%20large-scale%20experiments%20on%20five%20languages%2C%20three%20LLMs%2C%20and%20three%20benchmarks%2C%20we%20uncover%20that%20certain%20prompt%20components%2C%20such%20as%20CoT%2C%20emotion%2C%20and%20scenario%2C%20correlate%20with%20robust%20multilingual%20behavior.%20We%20develop%20a%20prompt%20optimization%20framework%20for%20multilingual%20settings%20and%20show%20it%20can%20automatically%20discover%20prompts%20that%20improve%20all%20metrics%20by%205-10%25.%20Finally%2C%20we%20analyze%20over%2010%20million%20reasoning%20units%20and%20find%20that%20more%20performant%20system%20prompts%20induce%20more%20structured%20and%20consistent%20reasoning%20patterns%2C%20while%20reducing%20unnecessary%20language-switching.%20Together%2C%20we%20highlight%20system%20prompt%20optimization%20as%20a%20scalable%20path%20to%20accurate%20and%20robust%20multilingual%20LLM%20behavior.&entry.1838667208=http%3A//arxiv.org/abs/2512.02841v1&entry.124074799=Read"},
{"title": "Fast Gaussian Process Approximations for Autocorrelated Data", "author": "Ahmadreza Chokhachian and Matthias Katzfuss and Yu Ding", "abstract": "This paper is concerned with the problem of how to speed up computation for Gaussian process models trained on autocorrelated data. The Gaussian process model is a powerful tool commonly used in nonlinear regression applications. Standard regression modeling assumes random samples and an independently, identically distributed noise. Various fast approximations that speed up Gaussian process regression work under this standard setting. But for autocorrelated data, failing to account for autocorrelation leads to a phenomenon known as temporal overfitting that deteriorates model performance on new test instances. To handle autocorrelated data, existing fast Gaussian process approximations have to be modified; one such approach is to segment the originally correlated data points into blocks in which the blocked data are de-correlated. This work explains how to make some of the existing Gaussian process approximations work with blocked data. Numerical experiments across diverse application datasets demonstrate that the proposed approaches can remarkably accelerate computation for Gaussian process regression on autocorrelated data without compromising model prediction performance.", "link": "http://arxiv.org/abs/2512.02925v1", "date": "2025-12-02", "relevancy": 2.3375, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4785}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4712}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Gaussian%20Process%20Approximations%20for%20Autocorrelated%20Data&body=Title%3A%20Fast%20Gaussian%20Process%20Approximations%20for%20Autocorrelated%20Data%0AAuthor%3A%20Ahmadreza%20Chokhachian%20and%20Matthias%20Katzfuss%20and%20Yu%20Ding%0AAbstract%3A%20This%20paper%20is%20concerned%20with%20the%20problem%20of%20how%20to%20speed%20up%20computation%20for%20Gaussian%20process%20models%20trained%20on%20autocorrelated%20data.%20The%20Gaussian%20process%20model%20is%20a%20powerful%20tool%20commonly%20used%20in%20nonlinear%20regression%20applications.%20Standard%20regression%20modeling%20assumes%20random%20samples%20and%20an%20independently%2C%20identically%20distributed%20noise.%20Various%20fast%20approximations%20that%20speed%20up%20Gaussian%20process%20regression%20work%20under%20this%20standard%20setting.%20But%20for%20autocorrelated%20data%2C%20failing%20to%20account%20for%20autocorrelation%20leads%20to%20a%20phenomenon%20known%20as%20temporal%20overfitting%20that%20deteriorates%20model%20performance%20on%20new%20test%20instances.%20To%20handle%20autocorrelated%20data%2C%20existing%20fast%20Gaussian%20process%20approximations%20have%20to%20be%20modified%3B%20one%20such%20approach%20is%20to%20segment%20the%20originally%20correlated%20data%20points%20into%20blocks%20in%20which%20the%20blocked%20data%20are%20de-correlated.%20This%20work%20explains%20how%20to%20make%20some%20of%20the%20existing%20Gaussian%20process%20approximations%20work%20with%20blocked%20data.%20Numerical%20experiments%20across%20diverse%20application%20datasets%20demonstrate%20that%20the%20proposed%20approaches%20can%20remarkably%20accelerate%20computation%20for%20Gaussian%20process%20regression%20on%20autocorrelated%20data%20without%20compromising%20model%20prediction%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02925v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Gaussian%2520Process%2520Approximations%2520for%2520Autocorrelated%2520Data%26entry.906535625%3DAhmadreza%2520Chokhachian%2520and%2520Matthias%2520Katzfuss%2520and%2520Yu%2520Ding%26entry.1292438233%3DThis%2520paper%2520is%2520concerned%2520with%2520the%2520problem%2520of%2520how%2520to%2520speed%2520up%2520computation%2520for%2520Gaussian%2520process%2520models%2520trained%2520on%2520autocorrelated%2520data.%2520The%2520Gaussian%2520process%2520model%2520is%2520a%2520powerful%2520tool%2520commonly%2520used%2520in%2520nonlinear%2520regression%2520applications.%2520Standard%2520regression%2520modeling%2520assumes%2520random%2520samples%2520and%2520an%2520independently%252C%2520identically%2520distributed%2520noise.%2520Various%2520fast%2520approximations%2520that%2520speed%2520up%2520Gaussian%2520process%2520regression%2520work%2520under%2520this%2520standard%2520setting.%2520But%2520for%2520autocorrelated%2520data%252C%2520failing%2520to%2520account%2520for%2520autocorrelation%2520leads%2520to%2520a%2520phenomenon%2520known%2520as%2520temporal%2520overfitting%2520that%2520deteriorates%2520model%2520performance%2520on%2520new%2520test%2520instances.%2520To%2520handle%2520autocorrelated%2520data%252C%2520existing%2520fast%2520Gaussian%2520process%2520approximations%2520have%2520to%2520be%2520modified%253B%2520one%2520such%2520approach%2520is%2520to%2520segment%2520the%2520originally%2520correlated%2520data%2520points%2520into%2520blocks%2520in%2520which%2520the%2520blocked%2520data%2520are%2520de-correlated.%2520This%2520work%2520explains%2520how%2520to%2520make%2520some%2520of%2520the%2520existing%2520Gaussian%2520process%2520approximations%2520work%2520with%2520blocked%2520data.%2520Numerical%2520experiments%2520across%2520diverse%2520application%2520datasets%2520demonstrate%2520that%2520the%2520proposed%2520approaches%2520can%2520remarkably%2520accelerate%2520computation%2520for%2520Gaussian%2520process%2520regression%2520on%2520autocorrelated%2520data%2520without%2520compromising%2520model%2520prediction%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02925v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Gaussian%20Process%20Approximations%20for%20Autocorrelated%20Data&entry.906535625=Ahmadreza%20Chokhachian%20and%20Matthias%20Katzfuss%20and%20Yu%20Ding&entry.1292438233=This%20paper%20is%20concerned%20with%20the%20problem%20of%20how%20to%20speed%20up%20computation%20for%20Gaussian%20process%20models%20trained%20on%20autocorrelated%20data.%20The%20Gaussian%20process%20model%20is%20a%20powerful%20tool%20commonly%20used%20in%20nonlinear%20regression%20applications.%20Standard%20regression%20modeling%20assumes%20random%20samples%20and%20an%20independently%2C%20identically%20distributed%20noise.%20Various%20fast%20approximations%20that%20speed%20up%20Gaussian%20process%20regression%20work%20under%20this%20standard%20setting.%20But%20for%20autocorrelated%20data%2C%20failing%20to%20account%20for%20autocorrelation%20leads%20to%20a%20phenomenon%20known%20as%20temporal%20overfitting%20that%20deteriorates%20model%20performance%20on%20new%20test%20instances.%20To%20handle%20autocorrelated%20data%2C%20existing%20fast%20Gaussian%20process%20approximations%20have%20to%20be%20modified%3B%20one%20such%20approach%20is%20to%20segment%20the%20originally%20correlated%20data%20points%20into%20blocks%20in%20which%20the%20blocked%20data%20are%20de-correlated.%20This%20work%20explains%20how%20to%20make%20some%20of%20the%20existing%20Gaussian%20process%20approximations%20work%20with%20blocked%20data.%20Numerical%20experiments%20across%20diverse%20application%20datasets%20demonstrate%20that%20the%20proposed%20approaches%20can%20remarkably%20accelerate%20computation%20for%20Gaussian%20process%20regression%20on%20autocorrelated%20data%20without%20compromising%20model%20prediction%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2512.02925v1&entry.124074799=Read"},
{"title": "Tissue-mask supported inter-subject whole-body image registration in the UK Biobank -- A method benchmarking study", "author": "Yasemin Utkueri and Elin Lundstr\u00f6m and H\u00e5kan Ahlstr\u00f6m and Johan \u00d6fverstedt and Joel Kullberg", "abstract": "The UK Biobank is a large-scale study collecting whole-body MR imaging and non-imaging health data. Robust and accurate inter-subject image registration of these whole-body MR images would enable their body-wide spatial standardization, and region-/voxel-wise correlation analysis of non-imaging data with image-derived parameters (e.g., tissue volume or fat content). We propose a sex-stratified inter-subject whole-body MR image registration approach that uses subcutaneous adipose tissue- and muscle-masks from the state-of-the-art VIBESegmentator method to augment intensity-based graph-cut registration. The proposed method was evaluated on a subset of 4000 subjects by comparing it to an intensity-only method as well as two previously published registration methods, uniGradICON and MIRTK. The evaluation comprised overlap measures applied to the 71 VIBESegmentator masks: 1) Dice scores, and 2) voxel-wise label error frequency. Additionally, voxel-wise correlation between age and each of fat content and tissue volume was studied to exemplify the usefulness for medical research. The proposed method exhibited a mean dice score of 0.77 / 0.75 across the cohort and the 71 masks for males/females, respectively. When compared to the intensity-only registration, the mean values were 6 percentage points (pp) higher for both sexes, and the label error frequency was decreased in most tissue regions. These differences were 9pp / 8pp against uniGradICON and 12pp / 13pp against MIRTK. Using the proposed method, the age-correlation maps were less noisy and showed higher anatomical alignment. In conclusion, the image registration method using two tissue masks improves whole-body registration of UK Biobank images.", "link": "http://arxiv.org/abs/2512.02702v1", "date": "2025-12-02", "relevancy": 2.3286, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5049}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4465}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tissue-mask%20supported%20inter-subject%20whole-body%20image%20registration%20in%20the%20UK%20Biobank%20--%20A%20method%20benchmarking%20study&body=Title%3A%20Tissue-mask%20supported%20inter-subject%20whole-body%20image%20registration%20in%20the%20UK%20Biobank%20--%20A%20method%20benchmarking%20study%0AAuthor%3A%20Yasemin%20Utkueri%20and%20Elin%20Lundstr%C3%B6m%20and%20H%C3%A5kan%20Ahlstr%C3%B6m%20and%20Johan%20%C3%96fverstedt%20and%20Joel%20Kullberg%0AAbstract%3A%20The%20UK%20Biobank%20is%20a%20large-scale%20study%20collecting%20whole-body%20MR%20imaging%20and%20non-imaging%20health%20data.%20Robust%20and%20accurate%20inter-subject%20image%20registration%20of%20these%20whole-body%20MR%20images%20would%20enable%20their%20body-wide%20spatial%20standardization%2C%20and%20region-/voxel-wise%20correlation%20analysis%20of%20non-imaging%20data%20with%20image-derived%20parameters%20%28e.g.%2C%20tissue%20volume%20or%20fat%20content%29.%20We%20propose%20a%20sex-stratified%20inter-subject%20whole-body%20MR%20image%20registration%20approach%20that%20uses%20subcutaneous%20adipose%20tissue-%20and%20muscle-masks%20from%20the%20state-of-the-art%20VIBESegmentator%20method%20to%20augment%20intensity-based%20graph-cut%20registration.%20The%20proposed%20method%20was%20evaluated%20on%20a%20subset%20of%204000%20subjects%20by%20comparing%20it%20to%20an%20intensity-only%20method%20as%20well%20as%20two%20previously%20published%20registration%20methods%2C%20uniGradICON%20and%20MIRTK.%20The%20evaluation%20comprised%20overlap%20measures%20applied%20to%20the%2071%20VIBESegmentator%20masks%3A%201%29%20Dice%20scores%2C%20and%202%29%20voxel-wise%20label%20error%20frequency.%20Additionally%2C%20voxel-wise%20correlation%20between%20age%20and%20each%20of%20fat%20content%20and%20tissue%20volume%20was%20studied%20to%20exemplify%20the%20usefulness%20for%20medical%20research.%20The%20proposed%20method%20exhibited%20a%20mean%20dice%20score%20of%200.77%20/%200.75%20across%20the%20cohort%20and%20the%2071%20masks%20for%20males/females%2C%20respectively.%20When%20compared%20to%20the%20intensity-only%20registration%2C%20the%20mean%20values%20were%206%20percentage%20points%20%28pp%29%20higher%20for%20both%20sexes%2C%20and%20the%20label%20error%20frequency%20was%20decreased%20in%20most%20tissue%20regions.%20These%20differences%20were%209pp%20/%208pp%20against%20uniGradICON%20and%2012pp%20/%2013pp%20against%20MIRTK.%20Using%20the%20proposed%20method%2C%20the%20age-correlation%20maps%20were%20less%20noisy%20and%20showed%20higher%20anatomical%20alignment.%20In%20conclusion%2C%20the%20image%20registration%20method%20using%20two%20tissue%20masks%20improves%20whole-body%20registration%20of%20UK%20Biobank%20images.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02702v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTissue-mask%2520supported%2520inter-subject%2520whole-body%2520image%2520registration%2520in%2520the%2520UK%2520Biobank%2520--%2520A%2520method%2520benchmarking%2520study%26entry.906535625%3DYasemin%2520Utkueri%2520and%2520Elin%2520Lundstr%25C3%25B6m%2520and%2520H%25C3%25A5kan%2520Ahlstr%25C3%25B6m%2520and%2520Johan%2520%25C3%2596fverstedt%2520and%2520Joel%2520Kullberg%26entry.1292438233%3DThe%2520UK%2520Biobank%2520is%2520a%2520large-scale%2520study%2520collecting%2520whole-body%2520MR%2520imaging%2520and%2520non-imaging%2520health%2520data.%2520Robust%2520and%2520accurate%2520inter-subject%2520image%2520registration%2520of%2520these%2520whole-body%2520MR%2520images%2520would%2520enable%2520their%2520body-wide%2520spatial%2520standardization%252C%2520and%2520region-/voxel-wise%2520correlation%2520analysis%2520of%2520non-imaging%2520data%2520with%2520image-derived%2520parameters%2520%2528e.g.%252C%2520tissue%2520volume%2520or%2520fat%2520content%2529.%2520We%2520propose%2520a%2520sex-stratified%2520inter-subject%2520whole-body%2520MR%2520image%2520registration%2520approach%2520that%2520uses%2520subcutaneous%2520adipose%2520tissue-%2520and%2520muscle-masks%2520from%2520the%2520state-of-the-art%2520VIBESegmentator%2520method%2520to%2520augment%2520intensity-based%2520graph-cut%2520registration.%2520The%2520proposed%2520method%2520was%2520evaluated%2520on%2520a%2520subset%2520of%25204000%2520subjects%2520by%2520comparing%2520it%2520to%2520an%2520intensity-only%2520method%2520as%2520well%2520as%2520two%2520previously%2520published%2520registration%2520methods%252C%2520uniGradICON%2520and%2520MIRTK.%2520The%2520evaluation%2520comprised%2520overlap%2520measures%2520applied%2520to%2520the%252071%2520VIBESegmentator%2520masks%253A%25201%2529%2520Dice%2520scores%252C%2520and%25202%2529%2520voxel-wise%2520label%2520error%2520frequency.%2520Additionally%252C%2520voxel-wise%2520correlation%2520between%2520age%2520and%2520each%2520of%2520fat%2520content%2520and%2520tissue%2520volume%2520was%2520studied%2520to%2520exemplify%2520the%2520usefulness%2520for%2520medical%2520research.%2520The%2520proposed%2520method%2520exhibited%2520a%2520mean%2520dice%2520score%2520of%25200.77%2520/%25200.75%2520across%2520the%2520cohort%2520and%2520the%252071%2520masks%2520for%2520males/females%252C%2520respectively.%2520When%2520compared%2520to%2520the%2520intensity-only%2520registration%252C%2520the%2520mean%2520values%2520were%25206%2520percentage%2520points%2520%2528pp%2529%2520higher%2520for%2520both%2520sexes%252C%2520and%2520the%2520label%2520error%2520frequency%2520was%2520decreased%2520in%2520most%2520tissue%2520regions.%2520These%2520differences%2520were%25209pp%2520/%25208pp%2520against%2520uniGradICON%2520and%252012pp%2520/%252013pp%2520against%2520MIRTK.%2520Using%2520the%2520proposed%2520method%252C%2520the%2520age-correlation%2520maps%2520were%2520less%2520noisy%2520and%2520showed%2520higher%2520anatomical%2520alignment.%2520In%2520conclusion%252C%2520the%2520image%2520registration%2520method%2520using%2520two%2520tissue%2520masks%2520improves%2520whole-body%2520registration%2520of%2520UK%2520Biobank%2520images.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02702v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tissue-mask%20supported%20inter-subject%20whole-body%20image%20registration%20in%20the%20UK%20Biobank%20--%20A%20method%20benchmarking%20study&entry.906535625=Yasemin%20Utkueri%20and%20Elin%20Lundstr%C3%B6m%20and%20H%C3%A5kan%20Ahlstr%C3%B6m%20and%20Johan%20%C3%96fverstedt%20and%20Joel%20Kullberg&entry.1292438233=The%20UK%20Biobank%20is%20a%20large-scale%20study%20collecting%20whole-body%20MR%20imaging%20and%20non-imaging%20health%20data.%20Robust%20and%20accurate%20inter-subject%20image%20registration%20of%20these%20whole-body%20MR%20images%20would%20enable%20their%20body-wide%20spatial%20standardization%2C%20and%20region-/voxel-wise%20correlation%20analysis%20of%20non-imaging%20data%20with%20image-derived%20parameters%20%28e.g.%2C%20tissue%20volume%20or%20fat%20content%29.%20We%20propose%20a%20sex-stratified%20inter-subject%20whole-body%20MR%20image%20registration%20approach%20that%20uses%20subcutaneous%20adipose%20tissue-%20and%20muscle-masks%20from%20the%20state-of-the-art%20VIBESegmentator%20method%20to%20augment%20intensity-based%20graph-cut%20registration.%20The%20proposed%20method%20was%20evaluated%20on%20a%20subset%20of%204000%20subjects%20by%20comparing%20it%20to%20an%20intensity-only%20method%20as%20well%20as%20two%20previously%20published%20registration%20methods%2C%20uniGradICON%20and%20MIRTK.%20The%20evaluation%20comprised%20overlap%20measures%20applied%20to%20the%2071%20VIBESegmentator%20masks%3A%201%29%20Dice%20scores%2C%20and%202%29%20voxel-wise%20label%20error%20frequency.%20Additionally%2C%20voxel-wise%20correlation%20between%20age%20and%20each%20of%20fat%20content%20and%20tissue%20volume%20was%20studied%20to%20exemplify%20the%20usefulness%20for%20medical%20research.%20The%20proposed%20method%20exhibited%20a%20mean%20dice%20score%20of%200.77%20/%200.75%20across%20the%20cohort%20and%20the%2071%20masks%20for%20males/females%2C%20respectively.%20When%20compared%20to%20the%20intensity-only%20registration%2C%20the%20mean%20values%20were%206%20percentage%20points%20%28pp%29%20higher%20for%20both%20sexes%2C%20and%20the%20label%20error%20frequency%20was%20decreased%20in%20most%20tissue%20regions.%20These%20differences%20were%209pp%20/%208pp%20against%20uniGradICON%20and%2012pp%20/%2013pp%20against%20MIRTK.%20Using%20the%20proposed%20method%2C%20the%20age-correlation%20maps%20were%20less%20noisy%20and%20showed%20higher%20anatomical%20alignment.%20In%20conclusion%2C%20the%20image%20registration%20method%20using%20two%20tissue%20masks%20improves%20whole-body%20registration%20of%20UK%20Biobank%20images.&entry.1838667208=http%3A//arxiv.org/abs/2512.02702v1&entry.124074799=Read"},
{"title": "Hear What Matters! Text-conditioned Selective Video-to-Audio Generation", "author": "Junwon Lee and Juhan Nam and Jiyoung Lee", "abstract": "This work introduces a new task, text-conditioned selective video-to-audio (V2A) generation, which produces only the user-intended sound from a multi-object video. This capability is especially crucial in multimedia production, where audio tracks are handled individually for each sound source for precise editing, mixing, and creative control. However, current approaches generate single source-mixed sounds at once, largely because visual features are entangled, and region cues or prompts often fail to specify the source. We propose SelVA, a novel text-conditioned V2A model that treats the text prompt as an explicit selector of target source and modulates video encoder to distinctly extract prompt-relevant video features. The proposed supplementary tokens promote cross-attention by suppressing text-irrelevant activations with efficient parameter tuning, yielding robust semantic and temporal grounding. SelVA further employs a self-augmentation scheme to overcome the lack of mono audio track supervision. We evaluate SelVA on VGG-MONOAUDIO, a curated benchmark of clean single-source videos for such a task. Extensive experiments and ablations consistently verify its effectiveness across audio quality, semantic alignment, and temporal synchronization. Code and demo are available at https://jnwnlee.github.io/selva-demo/.", "link": "http://arxiv.org/abs/2512.02650v1", "date": "2025-12-02", "relevancy": 2.3285, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5855}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5814}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hear%20What%20Matters%21%20Text-conditioned%20Selective%20Video-to-Audio%20Generation&body=Title%3A%20Hear%20What%20Matters%21%20Text-conditioned%20Selective%20Video-to-Audio%20Generation%0AAuthor%3A%20Junwon%20Lee%20and%20Juhan%20Nam%20and%20Jiyoung%20Lee%0AAbstract%3A%20This%20work%20introduces%20a%20new%20task%2C%20text-conditioned%20selective%20video-to-audio%20%28V2A%29%20generation%2C%20which%20produces%20only%20the%20user-intended%20sound%20from%20a%20multi-object%20video.%20This%20capability%20is%20especially%20crucial%20in%20multimedia%20production%2C%20where%20audio%20tracks%20are%20handled%20individually%20for%20each%20sound%20source%20for%20precise%20editing%2C%20mixing%2C%20and%20creative%20control.%20However%2C%20current%20approaches%20generate%20single%20source-mixed%20sounds%20at%20once%2C%20largely%20because%20visual%20features%20are%20entangled%2C%20and%20region%20cues%20or%20prompts%20often%20fail%20to%20specify%20the%20source.%20We%20propose%20SelVA%2C%20a%20novel%20text-conditioned%20V2A%20model%20that%20treats%20the%20text%20prompt%20as%20an%20explicit%20selector%20of%20target%20source%20and%20modulates%20video%20encoder%20to%20distinctly%20extract%20prompt-relevant%20video%20features.%20The%20proposed%20supplementary%20tokens%20promote%20cross-attention%20by%20suppressing%20text-irrelevant%20activations%20with%20efficient%20parameter%20tuning%2C%20yielding%20robust%20semantic%20and%20temporal%20grounding.%20SelVA%20further%20employs%20a%20self-augmentation%20scheme%20to%20overcome%20the%20lack%20of%20mono%20audio%20track%20supervision.%20We%20evaluate%20SelVA%20on%20VGG-MONOAUDIO%2C%20a%20curated%20benchmark%20of%20clean%20single-source%20videos%20for%20such%20a%20task.%20Extensive%20experiments%20and%20ablations%20consistently%20verify%20its%20effectiveness%20across%20audio%20quality%2C%20semantic%20alignment%2C%20and%20temporal%20synchronization.%20Code%20and%20demo%20are%20available%20at%20https%3A//jnwnlee.github.io/selva-demo/.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02650v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHear%2520What%2520Matters%2521%2520Text-conditioned%2520Selective%2520Video-to-Audio%2520Generation%26entry.906535625%3DJunwon%2520Lee%2520and%2520Juhan%2520Nam%2520and%2520Jiyoung%2520Lee%26entry.1292438233%3DThis%2520work%2520introduces%2520a%2520new%2520task%252C%2520text-conditioned%2520selective%2520video-to-audio%2520%2528V2A%2529%2520generation%252C%2520which%2520produces%2520only%2520the%2520user-intended%2520sound%2520from%2520a%2520multi-object%2520video.%2520This%2520capability%2520is%2520especially%2520crucial%2520in%2520multimedia%2520production%252C%2520where%2520audio%2520tracks%2520are%2520handled%2520individually%2520for%2520each%2520sound%2520source%2520for%2520precise%2520editing%252C%2520mixing%252C%2520and%2520creative%2520control.%2520However%252C%2520current%2520approaches%2520generate%2520single%2520source-mixed%2520sounds%2520at%2520once%252C%2520largely%2520because%2520visual%2520features%2520are%2520entangled%252C%2520and%2520region%2520cues%2520or%2520prompts%2520often%2520fail%2520to%2520specify%2520the%2520source.%2520We%2520propose%2520SelVA%252C%2520a%2520novel%2520text-conditioned%2520V2A%2520model%2520that%2520treats%2520the%2520text%2520prompt%2520as%2520an%2520explicit%2520selector%2520of%2520target%2520source%2520and%2520modulates%2520video%2520encoder%2520to%2520distinctly%2520extract%2520prompt-relevant%2520video%2520features.%2520The%2520proposed%2520supplementary%2520tokens%2520promote%2520cross-attention%2520by%2520suppressing%2520text-irrelevant%2520activations%2520with%2520efficient%2520parameter%2520tuning%252C%2520yielding%2520robust%2520semantic%2520and%2520temporal%2520grounding.%2520SelVA%2520further%2520employs%2520a%2520self-augmentation%2520scheme%2520to%2520overcome%2520the%2520lack%2520of%2520mono%2520audio%2520track%2520supervision.%2520We%2520evaluate%2520SelVA%2520on%2520VGG-MONOAUDIO%252C%2520a%2520curated%2520benchmark%2520of%2520clean%2520single-source%2520videos%2520for%2520such%2520a%2520task.%2520Extensive%2520experiments%2520and%2520ablations%2520consistently%2520verify%2520its%2520effectiveness%2520across%2520audio%2520quality%252C%2520semantic%2520alignment%252C%2520and%2520temporal%2520synchronization.%2520Code%2520and%2520demo%2520are%2520available%2520at%2520https%253A//jnwnlee.github.io/selva-demo/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02650v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hear%20What%20Matters%21%20Text-conditioned%20Selective%20Video-to-Audio%20Generation&entry.906535625=Junwon%20Lee%20and%20Juhan%20Nam%20and%20Jiyoung%20Lee&entry.1292438233=This%20work%20introduces%20a%20new%20task%2C%20text-conditioned%20selective%20video-to-audio%20%28V2A%29%20generation%2C%20which%20produces%20only%20the%20user-intended%20sound%20from%20a%20multi-object%20video.%20This%20capability%20is%20especially%20crucial%20in%20multimedia%20production%2C%20where%20audio%20tracks%20are%20handled%20individually%20for%20each%20sound%20source%20for%20precise%20editing%2C%20mixing%2C%20and%20creative%20control.%20However%2C%20current%20approaches%20generate%20single%20source-mixed%20sounds%20at%20once%2C%20largely%20because%20visual%20features%20are%20entangled%2C%20and%20region%20cues%20or%20prompts%20often%20fail%20to%20specify%20the%20source.%20We%20propose%20SelVA%2C%20a%20novel%20text-conditioned%20V2A%20model%20that%20treats%20the%20text%20prompt%20as%20an%20explicit%20selector%20of%20target%20source%20and%20modulates%20video%20encoder%20to%20distinctly%20extract%20prompt-relevant%20video%20features.%20The%20proposed%20supplementary%20tokens%20promote%20cross-attention%20by%20suppressing%20text-irrelevant%20activations%20with%20efficient%20parameter%20tuning%2C%20yielding%20robust%20semantic%20and%20temporal%20grounding.%20SelVA%20further%20employs%20a%20self-augmentation%20scheme%20to%20overcome%20the%20lack%20of%20mono%20audio%20track%20supervision.%20We%20evaluate%20SelVA%20on%20VGG-MONOAUDIO%2C%20a%20curated%20benchmark%20of%20clean%20single-source%20videos%20for%20such%20a%20task.%20Extensive%20experiments%20and%20ablations%20consistently%20verify%20its%20effectiveness%20across%20audio%20quality%2C%20semantic%20alignment%2C%20and%20temporal%20synchronization.%20Code%20and%20demo%20are%20available%20at%20https%3A//jnwnlee.github.io/selva-demo/.&entry.1838667208=http%3A//arxiv.org/abs/2512.02650v1&entry.124074799=Read"},
{"title": "GraphMatch: Fusing Language and Graph Representations in a Dynamic Two-Sided Work Marketplace", "author": "Miko\u0142aj Sacha and Hammad Jafri and Mattie Terzolo and Ayan Sinha and Andrew Rabinovich", "abstract": "Recommending matches in a text-rich, dynamic two-sided marketplace presents unique challenges due to evolving content and interaction graphs. We introduce GraphMatch, a new large-scale recommendation framework that fuses pre-trained language models with graph neural networks to overcome these challenges. Unlike prior approaches centered on standalone models, GraphMatch is a comprehensive recipe built on powerful text encoders and GNNs working in tandem. It employs adversarial negative sampling alongside point-in-time subgraph training to learn representations that capture both the fine-grained semantics of evolving text and the time-sensitive structure of the graph. We evaluated extensively on interaction data from Upwork, a leading labor marketplace, at large scale, and discuss our approach towards low-latency inference suitable for real-time use. In our experiments, GraphMatch outperforms language-only and graph-only baselines on matching tasks while being efficient at runtime. These results demonstrate that unifying language and graph representations yields a highly effective solution to text-rich, dynamic two-sided recommendations, bridging the gap between powerful pretrained LMs and large-scale graphs in practice.", "link": "http://arxiv.org/abs/2512.02849v1", "date": "2025-12-02", "relevancy": 2.3028, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4747}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4583}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphMatch%3A%20Fusing%20Language%20and%20Graph%20Representations%20in%20a%20Dynamic%20Two-Sided%20Work%20Marketplace&body=Title%3A%20GraphMatch%3A%20Fusing%20Language%20and%20Graph%20Representations%20in%20a%20Dynamic%20Two-Sided%20Work%20Marketplace%0AAuthor%3A%20Miko%C5%82aj%20Sacha%20and%20Hammad%20Jafri%20and%20Mattie%20Terzolo%20and%20Ayan%20Sinha%20and%20Andrew%20Rabinovich%0AAbstract%3A%20Recommending%20matches%20in%20a%20text-rich%2C%20dynamic%20two-sided%20marketplace%20presents%20unique%20challenges%20due%20to%20evolving%20content%20and%20interaction%20graphs.%20We%20introduce%20GraphMatch%2C%20a%20new%20large-scale%20recommendation%20framework%20that%20fuses%20pre-trained%20language%20models%20with%20graph%20neural%20networks%20to%20overcome%20these%20challenges.%20Unlike%20prior%20approaches%20centered%20on%20standalone%20models%2C%20GraphMatch%20is%20a%20comprehensive%20recipe%20built%20on%20powerful%20text%20encoders%20and%20GNNs%20working%20in%20tandem.%20It%20employs%20adversarial%20negative%20sampling%20alongside%20point-in-time%20subgraph%20training%20to%20learn%20representations%20that%20capture%20both%20the%20fine-grained%20semantics%20of%20evolving%20text%20and%20the%20time-sensitive%20structure%20of%20the%20graph.%20We%20evaluated%20extensively%20on%20interaction%20data%20from%20Upwork%2C%20a%20leading%20labor%20marketplace%2C%20at%20large%20scale%2C%20and%20discuss%20our%20approach%20towards%20low-latency%20inference%20suitable%20for%20real-time%20use.%20In%20our%20experiments%2C%20GraphMatch%20outperforms%20language-only%20and%20graph-only%20baselines%20on%20matching%20tasks%20while%20being%20efficient%20at%20runtime.%20These%20results%20demonstrate%20that%20unifying%20language%20and%20graph%20representations%20yields%20a%20highly%20effective%20solution%20to%20text-rich%2C%20dynamic%20two-sided%20recommendations%2C%20bridging%20the%20gap%20between%20powerful%20pretrained%20LMs%20and%20large-scale%20graphs%20in%20practice.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02849v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphMatch%253A%2520Fusing%2520Language%2520and%2520Graph%2520Representations%2520in%2520a%2520Dynamic%2520Two-Sided%2520Work%2520Marketplace%26entry.906535625%3DMiko%25C5%2582aj%2520Sacha%2520and%2520Hammad%2520Jafri%2520and%2520Mattie%2520Terzolo%2520and%2520Ayan%2520Sinha%2520and%2520Andrew%2520Rabinovich%26entry.1292438233%3DRecommending%2520matches%2520in%2520a%2520text-rich%252C%2520dynamic%2520two-sided%2520marketplace%2520presents%2520unique%2520challenges%2520due%2520to%2520evolving%2520content%2520and%2520interaction%2520graphs.%2520We%2520introduce%2520GraphMatch%252C%2520a%2520new%2520large-scale%2520recommendation%2520framework%2520that%2520fuses%2520pre-trained%2520language%2520models%2520with%2520graph%2520neural%2520networks%2520to%2520overcome%2520these%2520challenges.%2520Unlike%2520prior%2520approaches%2520centered%2520on%2520standalone%2520models%252C%2520GraphMatch%2520is%2520a%2520comprehensive%2520recipe%2520built%2520on%2520powerful%2520text%2520encoders%2520and%2520GNNs%2520working%2520in%2520tandem.%2520It%2520employs%2520adversarial%2520negative%2520sampling%2520alongside%2520point-in-time%2520subgraph%2520training%2520to%2520learn%2520representations%2520that%2520capture%2520both%2520the%2520fine-grained%2520semantics%2520of%2520evolving%2520text%2520and%2520the%2520time-sensitive%2520structure%2520of%2520the%2520graph.%2520We%2520evaluated%2520extensively%2520on%2520interaction%2520data%2520from%2520Upwork%252C%2520a%2520leading%2520labor%2520marketplace%252C%2520at%2520large%2520scale%252C%2520and%2520discuss%2520our%2520approach%2520towards%2520low-latency%2520inference%2520suitable%2520for%2520real-time%2520use.%2520In%2520our%2520experiments%252C%2520GraphMatch%2520outperforms%2520language-only%2520and%2520graph-only%2520baselines%2520on%2520matching%2520tasks%2520while%2520being%2520efficient%2520at%2520runtime.%2520These%2520results%2520demonstrate%2520that%2520unifying%2520language%2520and%2520graph%2520representations%2520yields%2520a%2520highly%2520effective%2520solution%2520to%2520text-rich%252C%2520dynamic%2520two-sided%2520recommendations%252C%2520bridging%2520the%2520gap%2520between%2520powerful%2520pretrained%2520LMs%2520and%2520large-scale%2520graphs%2520in%2520practice.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02849v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphMatch%3A%20Fusing%20Language%20and%20Graph%20Representations%20in%20a%20Dynamic%20Two-Sided%20Work%20Marketplace&entry.906535625=Miko%C5%82aj%20Sacha%20and%20Hammad%20Jafri%20and%20Mattie%20Terzolo%20and%20Ayan%20Sinha%20and%20Andrew%20Rabinovich&entry.1292438233=Recommending%20matches%20in%20a%20text-rich%2C%20dynamic%20two-sided%20marketplace%20presents%20unique%20challenges%20due%20to%20evolving%20content%20and%20interaction%20graphs.%20We%20introduce%20GraphMatch%2C%20a%20new%20large-scale%20recommendation%20framework%20that%20fuses%20pre-trained%20language%20models%20with%20graph%20neural%20networks%20to%20overcome%20these%20challenges.%20Unlike%20prior%20approaches%20centered%20on%20standalone%20models%2C%20GraphMatch%20is%20a%20comprehensive%20recipe%20built%20on%20powerful%20text%20encoders%20and%20GNNs%20working%20in%20tandem.%20It%20employs%20adversarial%20negative%20sampling%20alongside%20point-in-time%20subgraph%20training%20to%20learn%20representations%20that%20capture%20both%20the%20fine-grained%20semantics%20of%20evolving%20text%20and%20the%20time-sensitive%20structure%20of%20the%20graph.%20We%20evaluated%20extensively%20on%20interaction%20data%20from%20Upwork%2C%20a%20leading%20labor%20marketplace%2C%20at%20large%20scale%2C%20and%20discuss%20our%20approach%20towards%20low-latency%20inference%20suitable%20for%20real-time%20use.%20In%20our%20experiments%2C%20GraphMatch%20outperforms%20language-only%20and%20graph-only%20baselines%20on%20matching%20tasks%20while%20being%20efficient%20at%20runtime.%20These%20results%20demonstrate%20that%20unifying%20language%20and%20graph%20representations%20yields%20a%20highly%20effective%20solution%20to%20text-rich%2C%20dynamic%20two-sided%20recommendations%2C%20bridging%20the%20gap%20between%20powerful%20pretrained%20LMs%20and%20large-scale%20graphs%20in%20practice.&entry.1838667208=http%3A//arxiv.org/abs/2512.02849v1&entry.124074799=Read"},
{"title": "TEXTRIX: Latent Attribute Grid for Native Texture Generation and Beyond", "author": "Yifei Zeng and Yajie Bao and Jiachen Qian and Shuang Wu and Youtian Lin and Hao Zhu and Buyu Li and Feihu Zhang and Xun Cao and Yao Yao", "abstract": "Prevailing 3D texture generation methods, which often rely on multi-view fusion, are frequently hindered by inter-view inconsistencies and incomplete coverage of complex surfaces, limiting the fidelity and completeness of the generated content. To overcome these challenges, we introduce TEXTRIX, a native 3D attribute generation framework for high-fidelity texture synthesis and downstream applications such as precise 3D part segmentation. Our approach constructs a latent 3D attribute grid and leverages a Diffusion Transformer equipped with sparse attention, enabling direct coloring of 3D models in volumetric space and fundamentally avoiding the limitations of multi-view fusion. Built upon this native representation, the framework naturally extends to high-precision 3D segmentation by training the same architecture to predict semantic attributes on the grid. Extensive experiments demonstrate state-of-the-art performance on both tasks, producing seamless, high-fidelity textures and accurate 3D part segmentation with precise boundaries.", "link": "http://arxiv.org/abs/2512.02993v1", "date": "2025-12-02", "relevancy": 2.2968, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5897}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5766}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TEXTRIX%3A%20Latent%20Attribute%20Grid%20for%20Native%20Texture%20Generation%20and%20Beyond&body=Title%3A%20TEXTRIX%3A%20Latent%20Attribute%20Grid%20for%20Native%20Texture%20Generation%20and%20Beyond%0AAuthor%3A%20Yifei%20Zeng%20and%20Yajie%20Bao%20and%20Jiachen%20Qian%20and%20Shuang%20Wu%20and%20Youtian%20Lin%20and%20Hao%20Zhu%20and%20Buyu%20Li%20and%20Feihu%20Zhang%20and%20Xun%20Cao%20and%20Yao%20Yao%0AAbstract%3A%20Prevailing%203D%20texture%20generation%20methods%2C%20which%20often%20rely%20on%20multi-view%20fusion%2C%20are%20frequently%20hindered%20by%20inter-view%20inconsistencies%20and%20incomplete%20coverage%20of%20complex%20surfaces%2C%20limiting%20the%20fidelity%20and%20completeness%20of%20the%20generated%20content.%20To%20overcome%20these%20challenges%2C%20we%20introduce%20TEXTRIX%2C%20a%20native%203D%20attribute%20generation%20framework%20for%20high-fidelity%20texture%20synthesis%20and%20downstream%20applications%20such%20as%20precise%203D%20part%20segmentation.%20Our%20approach%20constructs%20a%20latent%203D%20attribute%20grid%20and%20leverages%20a%20Diffusion%20Transformer%20equipped%20with%20sparse%20attention%2C%20enabling%20direct%20coloring%20of%203D%20models%20in%20volumetric%20space%20and%20fundamentally%20avoiding%20the%20limitations%20of%20multi-view%20fusion.%20Built%20upon%20this%20native%20representation%2C%20the%20framework%20naturally%20extends%20to%20high-precision%203D%20segmentation%20by%20training%20the%20same%20architecture%20to%20predict%20semantic%20attributes%20on%20the%20grid.%20Extensive%20experiments%20demonstrate%20state-of-the-art%20performance%20on%20both%20tasks%2C%20producing%20seamless%2C%20high-fidelity%20textures%20and%20accurate%203D%20part%20segmentation%20with%20precise%20boundaries.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02993v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTEXTRIX%253A%2520Latent%2520Attribute%2520Grid%2520for%2520Native%2520Texture%2520Generation%2520and%2520Beyond%26entry.906535625%3DYifei%2520Zeng%2520and%2520Yajie%2520Bao%2520and%2520Jiachen%2520Qian%2520and%2520Shuang%2520Wu%2520and%2520Youtian%2520Lin%2520and%2520Hao%2520Zhu%2520and%2520Buyu%2520Li%2520and%2520Feihu%2520Zhang%2520and%2520Xun%2520Cao%2520and%2520Yao%2520Yao%26entry.1292438233%3DPrevailing%25203D%2520texture%2520generation%2520methods%252C%2520which%2520often%2520rely%2520on%2520multi-view%2520fusion%252C%2520are%2520frequently%2520hindered%2520by%2520inter-view%2520inconsistencies%2520and%2520incomplete%2520coverage%2520of%2520complex%2520surfaces%252C%2520limiting%2520the%2520fidelity%2520and%2520completeness%2520of%2520the%2520generated%2520content.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520introduce%2520TEXTRIX%252C%2520a%2520native%25203D%2520attribute%2520generation%2520framework%2520for%2520high-fidelity%2520texture%2520synthesis%2520and%2520downstream%2520applications%2520such%2520as%2520precise%25203D%2520part%2520segmentation.%2520Our%2520approach%2520constructs%2520a%2520latent%25203D%2520attribute%2520grid%2520and%2520leverages%2520a%2520Diffusion%2520Transformer%2520equipped%2520with%2520sparse%2520attention%252C%2520enabling%2520direct%2520coloring%2520of%25203D%2520models%2520in%2520volumetric%2520space%2520and%2520fundamentally%2520avoiding%2520the%2520limitations%2520of%2520multi-view%2520fusion.%2520Built%2520upon%2520this%2520native%2520representation%252C%2520the%2520framework%2520naturally%2520extends%2520to%2520high-precision%25203D%2520segmentation%2520by%2520training%2520the%2520same%2520architecture%2520to%2520predict%2520semantic%2520attributes%2520on%2520the%2520grid.%2520Extensive%2520experiments%2520demonstrate%2520state-of-the-art%2520performance%2520on%2520both%2520tasks%252C%2520producing%2520seamless%252C%2520high-fidelity%2520textures%2520and%2520accurate%25203D%2520part%2520segmentation%2520with%2520precise%2520boundaries.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02993v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TEXTRIX%3A%20Latent%20Attribute%20Grid%20for%20Native%20Texture%20Generation%20and%20Beyond&entry.906535625=Yifei%20Zeng%20and%20Yajie%20Bao%20and%20Jiachen%20Qian%20and%20Shuang%20Wu%20and%20Youtian%20Lin%20and%20Hao%20Zhu%20and%20Buyu%20Li%20and%20Feihu%20Zhang%20and%20Xun%20Cao%20and%20Yao%20Yao&entry.1292438233=Prevailing%203D%20texture%20generation%20methods%2C%20which%20often%20rely%20on%20multi-view%20fusion%2C%20are%20frequently%20hindered%20by%20inter-view%20inconsistencies%20and%20incomplete%20coverage%20of%20complex%20surfaces%2C%20limiting%20the%20fidelity%20and%20completeness%20of%20the%20generated%20content.%20To%20overcome%20these%20challenges%2C%20we%20introduce%20TEXTRIX%2C%20a%20native%203D%20attribute%20generation%20framework%20for%20high-fidelity%20texture%20synthesis%20and%20downstream%20applications%20such%20as%20precise%203D%20part%20segmentation.%20Our%20approach%20constructs%20a%20latent%203D%20attribute%20grid%20and%20leverages%20a%20Diffusion%20Transformer%20equipped%20with%20sparse%20attention%2C%20enabling%20direct%20coloring%20of%203D%20models%20in%20volumetric%20space%20and%20fundamentally%20avoiding%20the%20limitations%20of%20multi-view%20fusion.%20Built%20upon%20this%20native%20representation%2C%20the%20framework%20naturally%20extends%20to%20high-precision%203D%20segmentation%20by%20training%20the%20same%20architecture%20to%20predict%20semantic%20attributes%20on%20the%20grid.%20Extensive%20experiments%20demonstrate%20state-of-the-art%20performance%20on%20both%20tasks%2C%20producing%20seamless%2C%20high-fidelity%20textures%20and%20accurate%203D%20part%20segmentation%20with%20precise%20boundaries.&entry.1838667208=http%3A//arxiv.org/abs/2512.02993v1&entry.124074799=Read"},
{"title": "Toward Content-based Indexing and Retrieval of Head and Neck CT with Abscess Segmentation", "author": "Thao Thi Phuong Dao and Tan-Cong Nguyen and Trong-Le Do and Truong Hoang Viet and Nguyen Chi Thanh and Huynh Nguyen Thuan and Do Vo Cong Nguyen and Minh-Khoi Pham and Mai-Khiem Tran and Viet-Tham Huynh and Trong-Thuan Nguyen and Trung-Nghia Le and Vo Thanh Toan and Tam V. Nguyen and Minh-Triet Tran and Thanh Dinh Le", "abstract": "Abscesses in the head and neck represent an acute infectious process that can potentially lead to sepsis or mortality if not diagnosed and managed promptly. Accurate detection and delineation of these lesions on imaging are essential for diagnosis, treatment planning, and surgical intervention. In this study, we introduce AbscessHeNe, a curated and comprehensively annotated dataset comprising 4,926 contrast-enhanced CT slices with clinically confirmed head and neck abscesses. The dataset is designed to facilitate the development of robust semantic segmentation models that can accurately delineate abscess boundaries and evaluate deep neck space involvement, thereby supporting informed clinical decision-making. To establish performance baselines, we evaluate several state-of-the-art segmentation architectures, including CNN, Transformer, and Mamba-based models. The highest-performing model achieved a Dice Similarity Coefficient of 0.39, Intersection-over-Union of 0.27, and Normalized Surface Distance of 0.67, indicating the challenges of this task and the need for further research. Beyond segmentation, AbscessHeNe is structured for future applications in content-based multimedia indexing and case-based retrieval. Each CT scan is linked with pixel-level annotations and clinical metadata, providing a foundation for building intelligent retrieval systems and supporting knowledge-driven clinical workflows. The dataset will be made publicly available at https://github.com/drthaodao3101/AbscessHeNe.git.", "link": "http://arxiv.org/abs/2512.01589v2", "date": "2025-12-02", "relevancy": 2.2757, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4574}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4541}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Content-based%20Indexing%20and%20Retrieval%20of%20Head%20and%20Neck%20CT%20with%20Abscess%20Segmentation&body=Title%3A%20Toward%20Content-based%20Indexing%20and%20Retrieval%20of%20Head%20and%20Neck%20CT%20with%20Abscess%20Segmentation%0AAuthor%3A%20Thao%20Thi%20Phuong%20Dao%20and%20Tan-Cong%20Nguyen%20and%20Trong-Le%20Do%20and%20Truong%20Hoang%20Viet%20and%20Nguyen%20Chi%20Thanh%20and%20Huynh%20Nguyen%20Thuan%20and%20Do%20Vo%20Cong%20Nguyen%20and%20Minh-Khoi%20Pham%20and%20Mai-Khiem%20Tran%20and%20Viet-Tham%20Huynh%20and%20Trong-Thuan%20Nguyen%20and%20Trung-Nghia%20Le%20and%20Vo%20Thanh%20Toan%20and%20Tam%20V.%20Nguyen%20and%20Minh-Triet%20Tran%20and%20Thanh%20Dinh%20Le%0AAbstract%3A%20Abscesses%20in%20the%20head%20and%20neck%20represent%20an%20acute%20infectious%20process%20that%20can%20potentially%20lead%20to%20sepsis%20or%20mortality%20if%20not%20diagnosed%20and%20managed%20promptly.%20Accurate%20detection%20and%20delineation%20of%20these%20lesions%20on%20imaging%20are%20essential%20for%20diagnosis%2C%20treatment%20planning%2C%20and%20surgical%20intervention.%20In%20this%20study%2C%20we%20introduce%20AbscessHeNe%2C%20a%20curated%20and%20comprehensively%20annotated%20dataset%20comprising%204%2C926%20contrast-enhanced%20CT%20slices%20with%20clinically%20confirmed%20head%20and%20neck%20abscesses.%20The%20dataset%20is%20designed%20to%20facilitate%20the%20development%20of%20robust%20semantic%20segmentation%20models%20that%20can%20accurately%20delineate%20abscess%20boundaries%20and%20evaluate%20deep%20neck%20space%20involvement%2C%20thereby%20supporting%20informed%20clinical%20decision-making.%20To%20establish%20performance%20baselines%2C%20we%20evaluate%20several%20state-of-the-art%20segmentation%20architectures%2C%20including%20CNN%2C%20Transformer%2C%20and%20Mamba-based%20models.%20The%20highest-performing%20model%20achieved%20a%20Dice%20Similarity%20Coefficient%20of%200.39%2C%20Intersection-over-Union%20of%200.27%2C%20and%20Normalized%20Surface%20Distance%20of%200.67%2C%20indicating%20the%20challenges%20of%20this%20task%20and%20the%20need%20for%20further%20research.%20Beyond%20segmentation%2C%20AbscessHeNe%20is%20structured%20for%20future%20applications%20in%20content-based%20multimedia%20indexing%20and%20case-based%20retrieval.%20Each%20CT%20scan%20is%20linked%20with%20pixel-level%20annotations%20and%20clinical%20metadata%2C%20providing%20a%20foundation%20for%20building%20intelligent%20retrieval%20systems%20and%20supporting%20knowledge-driven%20clinical%20workflows.%20The%20dataset%20will%20be%20made%20publicly%20available%20at%20https%3A//github.com/drthaodao3101/AbscessHeNe.git.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01589v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Content-based%2520Indexing%2520and%2520Retrieval%2520of%2520Head%2520and%2520Neck%2520CT%2520with%2520Abscess%2520Segmentation%26entry.906535625%3DThao%2520Thi%2520Phuong%2520Dao%2520and%2520Tan-Cong%2520Nguyen%2520and%2520Trong-Le%2520Do%2520and%2520Truong%2520Hoang%2520Viet%2520and%2520Nguyen%2520Chi%2520Thanh%2520and%2520Huynh%2520Nguyen%2520Thuan%2520and%2520Do%2520Vo%2520Cong%2520Nguyen%2520and%2520Minh-Khoi%2520Pham%2520and%2520Mai-Khiem%2520Tran%2520and%2520Viet-Tham%2520Huynh%2520and%2520Trong-Thuan%2520Nguyen%2520and%2520Trung-Nghia%2520Le%2520and%2520Vo%2520Thanh%2520Toan%2520and%2520Tam%2520V.%2520Nguyen%2520and%2520Minh-Triet%2520Tran%2520and%2520Thanh%2520Dinh%2520Le%26entry.1292438233%3DAbscesses%2520in%2520the%2520head%2520and%2520neck%2520represent%2520an%2520acute%2520infectious%2520process%2520that%2520can%2520potentially%2520lead%2520to%2520sepsis%2520or%2520mortality%2520if%2520not%2520diagnosed%2520and%2520managed%2520promptly.%2520Accurate%2520detection%2520and%2520delineation%2520of%2520these%2520lesions%2520on%2520imaging%2520are%2520essential%2520for%2520diagnosis%252C%2520treatment%2520planning%252C%2520and%2520surgical%2520intervention.%2520In%2520this%2520study%252C%2520we%2520introduce%2520AbscessHeNe%252C%2520a%2520curated%2520and%2520comprehensively%2520annotated%2520dataset%2520comprising%25204%252C926%2520contrast-enhanced%2520CT%2520slices%2520with%2520clinically%2520confirmed%2520head%2520and%2520neck%2520abscesses.%2520The%2520dataset%2520is%2520designed%2520to%2520facilitate%2520the%2520development%2520of%2520robust%2520semantic%2520segmentation%2520models%2520that%2520can%2520accurately%2520delineate%2520abscess%2520boundaries%2520and%2520evaluate%2520deep%2520neck%2520space%2520involvement%252C%2520thereby%2520supporting%2520informed%2520clinical%2520decision-making.%2520To%2520establish%2520performance%2520baselines%252C%2520we%2520evaluate%2520several%2520state-of-the-art%2520segmentation%2520architectures%252C%2520including%2520CNN%252C%2520Transformer%252C%2520and%2520Mamba-based%2520models.%2520The%2520highest-performing%2520model%2520achieved%2520a%2520Dice%2520Similarity%2520Coefficient%2520of%25200.39%252C%2520Intersection-over-Union%2520of%25200.27%252C%2520and%2520Normalized%2520Surface%2520Distance%2520of%25200.67%252C%2520indicating%2520the%2520challenges%2520of%2520this%2520task%2520and%2520the%2520need%2520for%2520further%2520research.%2520Beyond%2520segmentation%252C%2520AbscessHeNe%2520is%2520structured%2520for%2520future%2520applications%2520in%2520content-based%2520multimedia%2520indexing%2520and%2520case-based%2520retrieval.%2520Each%2520CT%2520scan%2520is%2520linked%2520with%2520pixel-level%2520annotations%2520and%2520clinical%2520metadata%252C%2520providing%2520a%2520foundation%2520for%2520building%2520intelligent%2520retrieval%2520systems%2520and%2520supporting%2520knowledge-driven%2520clinical%2520workflows.%2520The%2520dataset%2520will%2520be%2520made%2520publicly%2520available%2520at%2520https%253A//github.com/drthaodao3101/AbscessHeNe.git.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01589v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Content-based%20Indexing%20and%20Retrieval%20of%20Head%20and%20Neck%20CT%20with%20Abscess%20Segmentation&entry.906535625=Thao%20Thi%20Phuong%20Dao%20and%20Tan-Cong%20Nguyen%20and%20Trong-Le%20Do%20and%20Truong%20Hoang%20Viet%20and%20Nguyen%20Chi%20Thanh%20and%20Huynh%20Nguyen%20Thuan%20and%20Do%20Vo%20Cong%20Nguyen%20and%20Minh-Khoi%20Pham%20and%20Mai-Khiem%20Tran%20and%20Viet-Tham%20Huynh%20and%20Trong-Thuan%20Nguyen%20and%20Trung-Nghia%20Le%20and%20Vo%20Thanh%20Toan%20and%20Tam%20V.%20Nguyen%20and%20Minh-Triet%20Tran%20and%20Thanh%20Dinh%20Le&entry.1292438233=Abscesses%20in%20the%20head%20and%20neck%20represent%20an%20acute%20infectious%20process%20that%20can%20potentially%20lead%20to%20sepsis%20or%20mortality%20if%20not%20diagnosed%20and%20managed%20promptly.%20Accurate%20detection%20and%20delineation%20of%20these%20lesions%20on%20imaging%20are%20essential%20for%20diagnosis%2C%20treatment%20planning%2C%20and%20surgical%20intervention.%20In%20this%20study%2C%20we%20introduce%20AbscessHeNe%2C%20a%20curated%20and%20comprehensively%20annotated%20dataset%20comprising%204%2C926%20contrast-enhanced%20CT%20slices%20with%20clinically%20confirmed%20head%20and%20neck%20abscesses.%20The%20dataset%20is%20designed%20to%20facilitate%20the%20development%20of%20robust%20semantic%20segmentation%20models%20that%20can%20accurately%20delineate%20abscess%20boundaries%20and%20evaluate%20deep%20neck%20space%20involvement%2C%20thereby%20supporting%20informed%20clinical%20decision-making.%20To%20establish%20performance%20baselines%2C%20we%20evaluate%20several%20state-of-the-art%20segmentation%20architectures%2C%20including%20CNN%2C%20Transformer%2C%20and%20Mamba-based%20models.%20The%20highest-performing%20model%20achieved%20a%20Dice%20Similarity%20Coefficient%20of%200.39%2C%20Intersection-over-Union%20of%200.27%2C%20and%20Normalized%20Surface%20Distance%20of%200.67%2C%20indicating%20the%20challenges%20of%20this%20task%20and%20the%20need%20for%20further%20research.%20Beyond%20segmentation%2C%20AbscessHeNe%20is%20structured%20for%20future%20applications%20in%20content-based%20multimedia%20indexing%20and%20case-based%20retrieval.%20Each%20CT%20scan%20is%20linked%20with%20pixel-level%20annotations%20and%20clinical%20metadata%2C%20providing%20a%20foundation%20for%20building%20intelligent%20retrieval%20systems%20and%20supporting%20knowledge-driven%20clinical%20workflows.%20The%20dataset%20will%20be%20made%20publicly%20available%20at%20https%3A//github.com/drthaodao3101/AbscessHeNe.git.&entry.1838667208=http%3A//arxiv.org/abs/2512.01589v2&entry.124074799=Read"},
{"title": "Fine-Tuned Large Language Models for Logical Translation: Reducing Hallucinations with Lang2Logic", "author": "Muyu Pan and Dheeraj Kodakandla and Mahfuza Farooque", "abstract": "Recent advances in natural language processing (NLP), particularly large language models (LLMs), have motivated the automatic translation of natural language statements into formal logic without human intervention. This enables automated reasoning and facilitates debugging, finding loop invariants, and adhering to specifications in software systems. However, hallucinations-incorrect outputs generated by LLMs are challenging, particularly for logical translation tasks requiring precision. This work introduces a novel framework that inputs English sentences, converts them into logical expressions, and then translates them into Conjunctive Normal Form (CNF) for satisfiability solving. It employs classical NLP techniques with self-defined grammar, symbolic computation libraries, and a fine-tuned language model to reduce hallucinations. In the early experiments, we observed that the fine-tuned model, trained on different grammar settings, could intentionally correct the same types of hallucinations made by the original model. Thus, it provides reliable CNF generation.", "link": "http://arxiv.org/abs/2512.02987v1", "date": "2025-12-02", "relevancy": 2.2596, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4551}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4551}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-Tuned%20Large%20Language%20Models%20for%20Logical%20Translation%3A%20Reducing%20Hallucinations%20with%20Lang2Logic&body=Title%3A%20Fine-Tuned%20Large%20Language%20Models%20for%20Logical%20Translation%3A%20Reducing%20Hallucinations%20with%20Lang2Logic%0AAuthor%3A%20Muyu%20Pan%20and%20Dheeraj%20Kodakandla%20and%20Mahfuza%20Farooque%0AAbstract%3A%20Recent%20advances%20in%20natural%20language%20processing%20%28NLP%29%2C%20particularly%20large%20language%20models%20%28LLMs%29%2C%20have%20motivated%20the%20automatic%20translation%20of%20natural%20language%20statements%20into%20formal%20logic%20without%20human%20intervention.%20This%20enables%20automated%20reasoning%20and%20facilitates%20debugging%2C%20finding%20loop%20invariants%2C%20and%20adhering%20to%20specifications%20in%20software%20systems.%20However%2C%20hallucinations-incorrect%20outputs%20generated%20by%20LLMs%20are%20challenging%2C%20particularly%20for%20logical%20translation%20tasks%20requiring%20precision.%20This%20work%20introduces%20a%20novel%20framework%20that%20inputs%20English%20sentences%2C%20converts%20them%20into%20logical%20expressions%2C%20and%20then%20translates%20them%20into%20Conjunctive%20Normal%20Form%20%28CNF%29%20for%20satisfiability%20solving.%20It%20employs%20classical%20NLP%20techniques%20with%20self-defined%20grammar%2C%20symbolic%20computation%20libraries%2C%20and%20a%20fine-tuned%20language%20model%20to%20reduce%20hallucinations.%20In%20the%20early%20experiments%2C%20we%20observed%20that%20the%20fine-tuned%20model%2C%20trained%20on%20different%20grammar%20settings%2C%20could%20intentionally%20correct%20the%20same%20types%20of%20hallucinations%20made%20by%20the%20original%20model.%20Thus%2C%20it%20provides%20reliable%20CNF%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02987v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-Tuned%2520Large%2520Language%2520Models%2520for%2520Logical%2520Translation%253A%2520Reducing%2520Hallucinations%2520with%2520Lang2Logic%26entry.906535625%3DMuyu%2520Pan%2520and%2520Dheeraj%2520Kodakandla%2520and%2520Mahfuza%2520Farooque%26entry.1292438233%3DRecent%2520advances%2520in%2520natural%2520language%2520processing%2520%2528NLP%2529%252C%2520particularly%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520have%2520motivated%2520the%2520automatic%2520translation%2520of%2520natural%2520language%2520statements%2520into%2520formal%2520logic%2520without%2520human%2520intervention.%2520This%2520enables%2520automated%2520reasoning%2520and%2520facilitates%2520debugging%252C%2520finding%2520loop%2520invariants%252C%2520and%2520adhering%2520to%2520specifications%2520in%2520software%2520systems.%2520However%252C%2520hallucinations-incorrect%2520outputs%2520generated%2520by%2520LLMs%2520are%2520challenging%252C%2520particularly%2520for%2520logical%2520translation%2520tasks%2520requiring%2520precision.%2520This%2520work%2520introduces%2520a%2520novel%2520framework%2520that%2520inputs%2520English%2520sentences%252C%2520converts%2520them%2520into%2520logical%2520expressions%252C%2520and%2520then%2520translates%2520them%2520into%2520Conjunctive%2520Normal%2520Form%2520%2528CNF%2529%2520for%2520satisfiability%2520solving.%2520It%2520employs%2520classical%2520NLP%2520techniques%2520with%2520self-defined%2520grammar%252C%2520symbolic%2520computation%2520libraries%252C%2520and%2520a%2520fine-tuned%2520language%2520model%2520to%2520reduce%2520hallucinations.%2520In%2520the%2520early%2520experiments%252C%2520we%2520observed%2520that%2520the%2520fine-tuned%2520model%252C%2520trained%2520on%2520different%2520grammar%2520settings%252C%2520could%2520intentionally%2520correct%2520the%2520same%2520types%2520of%2520hallucinations%2520made%2520by%2520the%2520original%2520model.%2520Thus%252C%2520it%2520provides%2520reliable%2520CNF%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02987v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-Tuned%20Large%20Language%20Models%20for%20Logical%20Translation%3A%20Reducing%20Hallucinations%20with%20Lang2Logic&entry.906535625=Muyu%20Pan%20and%20Dheeraj%20Kodakandla%20and%20Mahfuza%20Farooque&entry.1292438233=Recent%20advances%20in%20natural%20language%20processing%20%28NLP%29%2C%20particularly%20large%20language%20models%20%28LLMs%29%2C%20have%20motivated%20the%20automatic%20translation%20of%20natural%20language%20statements%20into%20formal%20logic%20without%20human%20intervention.%20This%20enables%20automated%20reasoning%20and%20facilitates%20debugging%2C%20finding%20loop%20invariants%2C%20and%20adhering%20to%20specifications%20in%20software%20systems.%20However%2C%20hallucinations-incorrect%20outputs%20generated%20by%20LLMs%20are%20challenging%2C%20particularly%20for%20logical%20translation%20tasks%20requiring%20precision.%20This%20work%20introduces%20a%20novel%20framework%20that%20inputs%20English%20sentences%2C%20converts%20them%20into%20logical%20expressions%2C%20and%20then%20translates%20them%20into%20Conjunctive%20Normal%20Form%20%28CNF%29%20for%20satisfiability%20solving.%20It%20employs%20classical%20NLP%20techniques%20with%20self-defined%20grammar%2C%20symbolic%20computation%20libraries%2C%20and%20a%20fine-tuned%20language%20model%20to%20reduce%20hallucinations.%20In%20the%20early%20experiments%2C%20we%20observed%20that%20the%20fine-tuned%20model%2C%20trained%20on%20different%20grammar%20settings%2C%20could%20intentionally%20correct%20the%20same%20types%20of%20hallucinations%20made%20by%20the%20original%20model.%20Thus%2C%20it%20provides%20reliable%20CNF%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2512.02987v1&entry.124074799=Read"},
{"title": "Layout Anything: One Transformer for Universal Room Layout Estimation", "author": "Md Sohag Mia and Muhammad Abdullah Adnan", "abstract": "We present Layout Anything, a transformer-based framework for indoor layout estimation that adapts the OneFormer's universal segmentation architecture to geometric structure prediction. Our approach integrates OneFormer's task-conditioned queries and contrastive learning with two key modules: (1) a layout degeneration strategy that augments training data while preserving Manhattan-world constraints through topology-aware transformations, and (2) differentiable geometric losses that directly enforce planar consistency and sharp boundary predictions during training. By unifying these components in an end-to-end framework, the model eliminates complex post-processing pipelines while achieving high-speed inference at 114ms. Extensive experiments demonstrate state-of-the-art performance across standard benchmarks, with pixel error (PE) of 5.43% and corner error (CE) of 4.02% on the LSUN, PE of 7.04% (CE 5.17%) on the Hedau and PE of 4.03% (CE 3.15%) on the Matterport3D-Layout datasets. The framework's combination of geometric awareness and computational efficiency makes it particularly suitable for augmented reality applications and large-scale 3D scene reconstruction tasks.", "link": "http://arxiv.org/abs/2512.02952v1", "date": "2025-12-02", "relevancy": 2.2533, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6036}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5371}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Layout%20Anything%3A%20One%20Transformer%20for%20Universal%20Room%20Layout%20Estimation&body=Title%3A%20Layout%20Anything%3A%20One%20Transformer%20for%20Universal%20Room%20Layout%20Estimation%0AAuthor%3A%20Md%20Sohag%20Mia%20and%20Muhammad%20Abdullah%20Adnan%0AAbstract%3A%20We%20present%20Layout%20Anything%2C%20a%20transformer-based%20framework%20for%20indoor%20layout%20estimation%20that%20adapts%20the%20OneFormer%27s%20universal%20segmentation%20architecture%20to%20geometric%20structure%20prediction.%20Our%20approach%20integrates%20OneFormer%27s%20task-conditioned%20queries%20and%20contrastive%20learning%20with%20two%20key%20modules%3A%20%281%29%20a%20layout%20degeneration%20strategy%20that%20augments%20training%20data%20while%20preserving%20Manhattan-world%20constraints%20through%20topology-aware%20transformations%2C%20and%20%282%29%20differentiable%20geometric%20losses%20that%20directly%20enforce%20planar%20consistency%20and%20sharp%20boundary%20predictions%20during%20training.%20By%20unifying%20these%20components%20in%20an%20end-to-end%20framework%2C%20the%20model%20eliminates%20complex%20post-processing%20pipelines%20while%20achieving%20high-speed%20inference%20at%20114ms.%20Extensive%20experiments%20demonstrate%20state-of-the-art%20performance%20across%20standard%20benchmarks%2C%20with%20pixel%20error%20%28PE%29%20of%205.43%25%20and%20corner%20error%20%28CE%29%20of%204.02%25%20on%20the%20LSUN%2C%20PE%20of%207.04%25%20%28CE%205.17%25%29%20on%20the%20Hedau%20and%20PE%20of%204.03%25%20%28CE%203.15%25%29%20on%20the%20Matterport3D-Layout%20datasets.%20The%20framework%27s%20combination%20of%20geometric%20awareness%20and%20computational%20efficiency%20makes%20it%20particularly%20suitable%20for%20augmented%20reality%20applications%20and%20large-scale%203D%20scene%20reconstruction%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02952v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLayout%2520Anything%253A%2520One%2520Transformer%2520for%2520Universal%2520Room%2520Layout%2520Estimation%26entry.906535625%3DMd%2520Sohag%2520Mia%2520and%2520Muhammad%2520Abdullah%2520Adnan%26entry.1292438233%3DWe%2520present%2520Layout%2520Anything%252C%2520a%2520transformer-based%2520framework%2520for%2520indoor%2520layout%2520estimation%2520that%2520adapts%2520the%2520OneFormer%2527s%2520universal%2520segmentation%2520architecture%2520to%2520geometric%2520structure%2520prediction.%2520Our%2520approach%2520integrates%2520OneFormer%2527s%2520task-conditioned%2520queries%2520and%2520contrastive%2520learning%2520with%2520two%2520key%2520modules%253A%2520%25281%2529%2520a%2520layout%2520degeneration%2520strategy%2520that%2520augments%2520training%2520data%2520while%2520preserving%2520Manhattan-world%2520constraints%2520through%2520topology-aware%2520transformations%252C%2520and%2520%25282%2529%2520differentiable%2520geometric%2520losses%2520that%2520directly%2520enforce%2520planar%2520consistency%2520and%2520sharp%2520boundary%2520predictions%2520during%2520training.%2520By%2520unifying%2520these%2520components%2520in%2520an%2520end-to-end%2520framework%252C%2520the%2520model%2520eliminates%2520complex%2520post-processing%2520pipelines%2520while%2520achieving%2520high-speed%2520inference%2520at%2520114ms.%2520Extensive%2520experiments%2520demonstrate%2520state-of-the-art%2520performance%2520across%2520standard%2520benchmarks%252C%2520with%2520pixel%2520error%2520%2528PE%2529%2520of%25205.43%2525%2520and%2520corner%2520error%2520%2528CE%2529%2520of%25204.02%2525%2520on%2520the%2520LSUN%252C%2520PE%2520of%25207.04%2525%2520%2528CE%25205.17%2525%2529%2520on%2520the%2520Hedau%2520and%2520PE%2520of%25204.03%2525%2520%2528CE%25203.15%2525%2529%2520on%2520the%2520Matterport3D-Layout%2520datasets.%2520The%2520framework%2527s%2520combination%2520of%2520geometric%2520awareness%2520and%2520computational%2520efficiency%2520makes%2520it%2520particularly%2520suitable%2520for%2520augmented%2520reality%2520applications%2520and%2520large-scale%25203D%2520scene%2520reconstruction%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02952v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Layout%20Anything%3A%20One%20Transformer%20for%20Universal%20Room%20Layout%20Estimation&entry.906535625=Md%20Sohag%20Mia%20and%20Muhammad%20Abdullah%20Adnan&entry.1292438233=We%20present%20Layout%20Anything%2C%20a%20transformer-based%20framework%20for%20indoor%20layout%20estimation%20that%20adapts%20the%20OneFormer%27s%20universal%20segmentation%20architecture%20to%20geometric%20structure%20prediction.%20Our%20approach%20integrates%20OneFormer%27s%20task-conditioned%20queries%20and%20contrastive%20learning%20with%20two%20key%20modules%3A%20%281%29%20a%20layout%20degeneration%20strategy%20that%20augments%20training%20data%20while%20preserving%20Manhattan-world%20constraints%20through%20topology-aware%20transformations%2C%20and%20%282%29%20differentiable%20geometric%20losses%20that%20directly%20enforce%20planar%20consistency%20and%20sharp%20boundary%20predictions%20during%20training.%20By%20unifying%20these%20components%20in%20an%20end-to-end%20framework%2C%20the%20model%20eliminates%20complex%20post-processing%20pipelines%20while%20achieving%20high-speed%20inference%20at%20114ms.%20Extensive%20experiments%20demonstrate%20state-of-the-art%20performance%20across%20standard%20benchmarks%2C%20with%20pixel%20error%20%28PE%29%20of%205.43%25%20and%20corner%20error%20%28CE%29%20of%204.02%25%20on%20the%20LSUN%2C%20PE%20of%207.04%25%20%28CE%205.17%25%29%20on%20the%20Hedau%20and%20PE%20of%204.03%25%20%28CE%203.15%25%29%20on%20the%20Matterport3D-Layout%20datasets.%20The%20framework%27s%20combination%20of%20geometric%20awareness%20and%20computational%20efficiency%20makes%20it%20particularly%20suitable%20for%20augmented%20reality%20applications%20and%20large-scale%203D%20scene%20reconstruction%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2512.02952v1&entry.124074799=Read"},
{"title": "Learning Egocentric In-Hand Object Segmentation through Weak Supervision from Human Narrations", "author": "Nicola Messina and Rosario Leonardi and Luca Ciampi and Fabio Carrara and Giovanni Maria Farinella and Fabrizio Falchi and Antonino Furnari", "abstract": "Pixel-level recognition of objects manipulated by the user from egocentric images enables key applications spanning assistive technologies, industrial safety, and activity monitoring. However, progress in this area is currently hindered by the scarcity of annotated datasets, as existing approaches rely on costly manual labels. In this paper, we propose to learn human-object interaction detection leveraging narrations $\\unicode{x2013}$ natural language descriptions of the actions performed by the camera wearer which contain clues about manipulated objects. We introduce Narration-Supervised in-Hand Object Segmentation (NS-iHOS), a novel task where models have to learn to segment in-hand objects by learning from natural-language narrations in a weakly-supervised regime. Narrations are then not employed at inference time. We showcase the potential of the task by proposing Weakly-Supervised In-hand Object Segmentation from Human Narrations (WISH), an end-to-end model distilling knowledge from narrations to learn plausible hand-object associations and enable in-hand object segmentation without using narrations at test time. We benchmark WISH against different baselines based on open-vocabulary object detectors and vision-language models. Experiments on EPIC-Kitchens and Ego4D show that WISH surpasses all baselines, recovering more than 50% of the performance of fully supervised methods, without employing fine-grained pixel-wise annotations. Code and data can be found at https://fpv-iplab.github.io/WISH.", "link": "http://arxiv.org/abs/2509.26004v2", "date": "2025-12-02", "relevancy": 2.2478, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5835}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.548}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5459}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Egocentric%20In-Hand%20Object%20Segmentation%20through%20Weak%20Supervision%20from%20Human%20Narrations&body=Title%3A%20Learning%20Egocentric%20In-Hand%20Object%20Segmentation%20through%20Weak%20Supervision%20from%20Human%20Narrations%0AAuthor%3A%20Nicola%20Messina%20and%20Rosario%20Leonardi%20and%20Luca%20Ciampi%20and%20Fabio%20Carrara%20and%20Giovanni%20Maria%20Farinella%20and%20Fabrizio%20Falchi%20and%20Antonino%20Furnari%0AAbstract%3A%20Pixel-level%20recognition%20of%20objects%20manipulated%20by%20the%20user%20from%20egocentric%20images%20enables%20key%20applications%20spanning%20assistive%20technologies%2C%20industrial%20safety%2C%20and%20activity%20monitoring.%20However%2C%20progress%20in%20this%20area%20is%20currently%20hindered%20by%20the%20scarcity%20of%20annotated%20datasets%2C%20as%20existing%20approaches%20rely%20on%20costly%20manual%20labels.%20In%20this%20paper%2C%20we%20propose%20to%20learn%20human-object%20interaction%20detection%20leveraging%20narrations%20%24%5Cunicode%7Bx2013%7D%24%20natural%20language%20descriptions%20of%20the%20actions%20performed%20by%20the%20camera%20wearer%20which%20contain%20clues%20about%20manipulated%20objects.%20We%20introduce%20Narration-Supervised%20in-Hand%20Object%20Segmentation%20%28NS-iHOS%29%2C%20a%20novel%20task%20where%20models%20have%20to%20learn%20to%20segment%20in-hand%20objects%20by%20learning%20from%20natural-language%20narrations%20in%20a%20weakly-supervised%20regime.%20Narrations%20are%20then%20not%20employed%20at%20inference%20time.%20We%20showcase%20the%20potential%20of%20the%20task%20by%20proposing%20Weakly-Supervised%20In-hand%20Object%20Segmentation%20from%20Human%20Narrations%20%28WISH%29%2C%20an%20end-to-end%20model%20distilling%20knowledge%20from%20narrations%20to%20learn%20plausible%20hand-object%20associations%20and%20enable%20in-hand%20object%20segmentation%20without%20using%20narrations%20at%20test%20time.%20We%20benchmark%20WISH%20against%20different%20baselines%20based%20on%20open-vocabulary%20object%20detectors%20and%20vision-language%20models.%20Experiments%20on%20EPIC-Kitchens%20and%20Ego4D%20show%20that%20WISH%20surpasses%20all%20baselines%2C%20recovering%20more%20than%2050%25%20of%20the%20performance%20of%20fully%20supervised%20methods%2C%20without%20employing%20fine-grained%20pixel-wise%20annotations.%20Code%20and%20data%20can%20be%20found%20at%20https%3A//fpv-iplab.github.io/WISH.%0ALink%3A%20http%3A//arxiv.org/abs/2509.26004v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Egocentric%2520In-Hand%2520Object%2520Segmentation%2520through%2520Weak%2520Supervision%2520from%2520Human%2520Narrations%26entry.906535625%3DNicola%2520Messina%2520and%2520Rosario%2520Leonardi%2520and%2520Luca%2520Ciampi%2520and%2520Fabio%2520Carrara%2520and%2520Giovanni%2520Maria%2520Farinella%2520and%2520Fabrizio%2520Falchi%2520and%2520Antonino%2520Furnari%26entry.1292438233%3DPixel-level%2520recognition%2520of%2520objects%2520manipulated%2520by%2520the%2520user%2520from%2520egocentric%2520images%2520enables%2520key%2520applications%2520spanning%2520assistive%2520technologies%252C%2520industrial%2520safety%252C%2520and%2520activity%2520monitoring.%2520However%252C%2520progress%2520in%2520this%2520area%2520is%2520currently%2520hindered%2520by%2520the%2520scarcity%2520of%2520annotated%2520datasets%252C%2520as%2520existing%2520approaches%2520rely%2520on%2520costly%2520manual%2520labels.%2520In%2520this%2520paper%252C%2520we%2520propose%2520to%2520learn%2520human-object%2520interaction%2520detection%2520leveraging%2520narrations%2520%2524%255Cunicode%257Bx2013%257D%2524%2520natural%2520language%2520descriptions%2520of%2520the%2520actions%2520performed%2520by%2520the%2520camera%2520wearer%2520which%2520contain%2520clues%2520about%2520manipulated%2520objects.%2520We%2520introduce%2520Narration-Supervised%2520in-Hand%2520Object%2520Segmentation%2520%2528NS-iHOS%2529%252C%2520a%2520novel%2520task%2520where%2520models%2520have%2520to%2520learn%2520to%2520segment%2520in-hand%2520objects%2520by%2520learning%2520from%2520natural-language%2520narrations%2520in%2520a%2520weakly-supervised%2520regime.%2520Narrations%2520are%2520then%2520not%2520employed%2520at%2520inference%2520time.%2520We%2520showcase%2520the%2520potential%2520of%2520the%2520task%2520by%2520proposing%2520Weakly-Supervised%2520In-hand%2520Object%2520Segmentation%2520from%2520Human%2520Narrations%2520%2528WISH%2529%252C%2520an%2520end-to-end%2520model%2520distilling%2520knowledge%2520from%2520narrations%2520to%2520learn%2520plausible%2520hand-object%2520associations%2520and%2520enable%2520in-hand%2520object%2520segmentation%2520without%2520using%2520narrations%2520at%2520test%2520time.%2520We%2520benchmark%2520WISH%2520against%2520different%2520baselines%2520based%2520on%2520open-vocabulary%2520object%2520detectors%2520and%2520vision-language%2520models.%2520Experiments%2520on%2520EPIC-Kitchens%2520and%2520Ego4D%2520show%2520that%2520WISH%2520surpasses%2520all%2520baselines%252C%2520recovering%2520more%2520than%252050%2525%2520of%2520the%2520performance%2520of%2520fully%2520supervised%2520methods%252C%2520without%2520employing%2520fine-grained%2520pixel-wise%2520annotations.%2520Code%2520and%2520data%2520can%2520be%2520found%2520at%2520https%253A//fpv-iplab.github.io/WISH.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26004v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Egocentric%20In-Hand%20Object%20Segmentation%20through%20Weak%20Supervision%20from%20Human%20Narrations&entry.906535625=Nicola%20Messina%20and%20Rosario%20Leonardi%20and%20Luca%20Ciampi%20and%20Fabio%20Carrara%20and%20Giovanni%20Maria%20Farinella%20and%20Fabrizio%20Falchi%20and%20Antonino%20Furnari&entry.1292438233=Pixel-level%20recognition%20of%20objects%20manipulated%20by%20the%20user%20from%20egocentric%20images%20enables%20key%20applications%20spanning%20assistive%20technologies%2C%20industrial%20safety%2C%20and%20activity%20monitoring.%20However%2C%20progress%20in%20this%20area%20is%20currently%20hindered%20by%20the%20scarcity%20of%20annotated%20datasets%2C%20as%20existing%20approaches%20rely%20on%20costly%20manual%20labels.%20In%20this%20paper%2C%20we%20propose%20to%20learn%20human-object%20interaction%20detection%20leveraging%20narrations%20%24%5Cunicode%7Bx2013%7D%24%20natural%20language%20descriptions%20of%20the%20actions%20performed%20by%20the%20camera%20wearer%20which%20contain%20clues%20about%20manipulated%20objects.%20We%20introduce%20Narration-Supervised%20in-Hand%20Object%20Segmentation%20%28NS-iHOS%29%2C%20a%20novel%20task%20where%20models%20have%20to%20learn%20to%20segment%20in-hand%20objects%20by%20learning%20from%20natural-language%20narrations%20in%20a%20weakly-supervised%20regime.%20Narrations%20are%20then%20not%20employed%20at%20inference%20time.%20We%20showcase%20the%20potential%20of%20the%20task%20by%20proposing%20Weakly-Supervised%20In-hand%20Object%20Segmentation%20from%20Human%20Narrations%20%28WISH%29%2C%20an%20end-to-end%20model%20distilling%20knowledge%20from%20narrations%20to%20learn%20plausible%20hand-object%20associations%20and%20enable%20in-hand%20object%20segmentation%20without%20using%20narrations%20at%20test%20time.%20We%20benchmark%20WISH%20against%20different%20baselines%20based%20on%20open-vocabulary%20object%20detectors%20and%20vision-language%20models.%20Experiments%20on%20EPIC-Kitchens%20and%20Ego4D%20show%20that%20WISH%20surpasses%20all%20baselines%2C%20recovering%20more%20than%2050%25%20of%20the%20performance%20of%20fully%20supervised%20methods%2C%20without%20employing%20fine-grained%20pixel-wise%20annotations.%20Code%20and%20data%20can%20be%20found%20at%20https%3A//fpv-iplab.github.io/WISH.&entry.1838667208=http%3A//arxiv.org/abs/2509.26004v2&entry.124074799=Read"},
{"title": "GeoViS: Geospatially Rewarded Visual Search for Remote Sensing Visual Grounding", "author": "Peirong Zhang and Yidan Zhang and Luxiao Xu and Jinliang Lin and Zonghao Guo and Fengxiang Wang and Xue Yang and Kaiwen Wei and Lei Wang", "abstract": "Recent advances in multimodal large language models(MLLMs) have led to remarkable progress in visual grounding, enabling fine-grained cross-modal alignment between textual queries and image regions. However, transferring such capabilities to remote sensing imagery remains challenging, as targets are often extremely small within kilometer-scale scenes, and queries typically involve intricate geospatial relations such as relative positions, spatial hierarchies, or contextual dependencies across distant objects. To address these challenges, we propose GeoViS, a Geospatially Rewarded Visual Search framework that reformulates remote sensing visual grounding as a progressive search-and-reasoning process. Rather than directly predicting the target location in a single step, GeoViS actively explores the global image through a tree-structured sequence of visual cues, integrating multimodal perception, spatial reasoning, and reward-guided exploration to refine geospatial hypotheses iteratively. This design enables the model to detect subtle small-scale targets while maintaining holistic scene awareness. Extensive experiments on five remote sensing grounding benchmarks demonstrate that GeoViS achieves precise geospatial understanding and consistently surpasses existing methods across key visual grounding metrics, highlighting its strong cross-domain generalization and interpretability.", "link": "http://arxiv.org/abs/2512.02715v1", "date": "2025-12-02", "relevancy": 2.2476, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5706}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5706}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5184}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoViS%3A%20Geospatially%20Rewarded%20Visual%20Search%20for%20Remote%20Sensing%20Visual%20Grounding&body=Title%3A%20GeoViS%3A%20Geospatially%20Rewarded%20Visual%20Search%20for%20Remote%20Sensing%20Visual%20Grounding%0AAuthor%3A%20Peirong%20Zhang%20and%20Yidan%20Zhang%20and%20Luxiao%20Xu%20and%20Jinliang%20Lin%20and%20Zonghao%20Guo%20and%20Fengxiang%20Wang%20and%20Xue%20Yang%20and%20Kaiwen%20Wei%20and%20Lei%20Wang%0AAbstract%3A%20Recent%20advances%20in%20multimodal%20large%20language%20models%28MLLMs%29%20have%20led%20to%20remarkable%20progress%20in%20visual%20grounding%2C%20enabling%20fine-grained%20cross-modal%20alignment%20between%20textual%20queries%20and%20image%20regions.%20However%2C%20transferring%20such%20capabilities%20to%20remote%20sensing%20imagery%20remains%20challenging%2C%20as%20targets%20are%20often%20extremely%20small%20within%20kilometer-scale%20scenes%2C%20and%20queries%20typically%20involve%20intricate%20geospatial%20relations%20such%20as%20relative%20positions%2C%20spatial%20hierarchies%2C%20or%20contextual%20dependencies%20across%20distant%20objects.%20To%20address%20these%20challenges%2C%20we%20propose%20GeoViS%2C%20a%20Geospatially%20Rewarded%20Visual%20Search%20framework%20that%20reformulates%20remote%20sensing%20visual%20grounding%20as%20a%20progressive%20search-and-reasoning%20process.%20Rather%20than%20directly%20predicting%20the%20target%20location%20in%20a%20single%20step%2C%20GeoViS%20actively%20explores%20the%20global%20image%20through%20a%20tree-structured%20sequence%20of%20visual%20cues%2C%20integrating%20multimodal%20perception%2C%20spatial%20reasoning%2C%20and%20reward-guided%20exploration%20to%20refine%20geospatial%20hypotheses%20iteratively.%20This%20design%20enables%20the%20model%20to%20detect%20subtle%20small-scale%20targets%20while%20maintaining%20holistic%20scene%20awareness.%20Extensive%20experiments%20on%20five%20remote%20sensing%20grounding%20benchmarks%20demonstrate%20that%20GeoViS%20achieves%20precise%20geospatial%20understanding%20and%20consistently%20surpasses%20existing%20methods%20across%20key%20visual%20grounding%20metrics%2C%20highlighting%20its%20strong%20cross-domain%20generalization%20and%20interpretability.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02715v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoViS%253A%2520Geospatially%2520Rewarded%2520Visual%2520Search%2520for%2520Remote%2520Sensing%2520Visual%2520Grounding%26entry.906535625%3DPeirong%2520Zhang%2520and%2520Yidan%2520Zhang%2520and%2520Luxiao%2520Xu%2520and%2520Jinliang%2520Lin%2520and%2520Zonghao%2520Guo%2520and%2520Fengxiang%2520Wang%2520and%2520Xue%2520Yang%2520and%2520Kaiwen%2520Wei%2520and%2520Lei%2520Wang%26entry.1292438233%3DRecent%2520advances%2520in%2520multimodal%2520large%2520language%2520models%2528MLLMs%2529%2520have%2520led%2520to%2520remarkable%2520progress%2520in%2520visual%2520grounding%252C%2520enabling%2520fine-grained%2520cross-modal%2520alignment%2520between%2520textual%2520queries%2520and%2520image%2520regions.%2520However%252C%2520transferring%2520such%2520capabilities%2520to%2520remote%2520sensing%2520imagery%2520remains%2520challenging%252C%2520as%2520targets%2520are%2520often%2520extremely%2520small%2520within%2520kilometer-scale%2520scenes%252C%2520and%2520queries%2520typically%2520involve%2520intricate%2520geospatial%2520relations%2520such%2520as%2520relative%2520positions%252C%2520spatial%2520hierarchies%252C%2520or%2520contextual%2520dependencies%2520across%2520distant%2520objects.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520GeoViS%252C%2520a%2520Geospatially%2520Rewarded%2520Visual%2520Search%2520framework%2520that%2520reformulates%2520remote%2520sensing%2520visual%2520grounding%2520as%2520a%2520progressive%2520search-and-reasoning%2520process.%2520Rather%2520than%2520directly%2520predicting%2520the%2520target%2520location%2520in%2520a%2520single%2520step%252C%2520GeoViS%2520actively%2520explores%2520the%2520global%2520image%2520through%2520a%2520tree-structured%2520sequence%2520of%2520visual%2520cues%252C%2520integrating%2520multimodal%2520perception%252C%2520spatial%2520reasoning%252C%2520and%2520reward-guided%2520exploration%2520to%2520refine%2520geospatial%2520hypotheses%2520iteratively.%2520This%2520design%2520enables%2520the%2520model%2520to%2520detect%2520subtle%2520small-scale%2520targets%2520while%2520maintaining%2520holistic%2520scene%2520awareness.%2520Extensive%2520experiments%2520on%2520five%2520remote%2520sensing%2520grounding%2520benchmarks%2520demonstrate%2520that%2520GeoViS%2520achieves%2520precise%2520geospatial%2520understanding%2520and%2520consistently%2520surpasses%2520existing%2520methods%2520across%2520key%2520visual%2520grounding%2520metrics%252C%2520highlighting%2520its%2520strong%2520cross-domain%2520generalization%2520and%2520interpretability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02715v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoViS%3A%20Geospatially%20Rewarded%20Visual%20Search%20for%20Remote%20Sensing%20Visual%20Grounding&entry.906535625=Peirong%20Zhang%20and%20Yidan%20Zhang%20and%20Luxiao%20Xu%20and%20Jinliang%20Lin%20and%20Zonghao%20Guo%20and%20Fengxiang%20Wang%20and%20Xue%20Yang%20and%20Kaiwen%20Wei%20and%20Lei%20Wang&entry.1292438233=Recent%20advances%20in%20multimodal%20large%20language%20models%28MLLMs%29%20have%20led%20to%20remarkable%20progress%20in%20visual%20grounding%2C%20enabling%20fine-grained%20cross-modal%20alignment%20between%20textual%20queries%20and%20image%20regions.%20However%2C%20transferring%20such%20capabilities%20to%20remote%20sensing%20imagery%20remains%20challenging%2C%20as%20targets%20are%20often%20extremely%20small%20within%20kilometer-scale%20scenes%2C%20and%20queries%20typically%20involve%20intricate%20geospatial%20relations%20such%20as%20relative%20positions%2C%20spatial%20hierarchies%2C%20or%20contextual%20dependencies%20across%20distant%20objects.%20To%20address%20these%20challenges%2C%20we%20propose%20GeoViS%2C%20a%20Geospatially%20Rewarded%20Visual%20Search%20framework%20that%20reformulates%20remote%20sensing%20visual%20grounding%20as%20a%20progressive%20search-and-reasoning%20process.%20Rather%20than%20directly%20predicting%20the%20target%20location%20in%20a%20single%20step%2C%20GeoViS%20actively%20explores%20the%20global%20image%20through%20a%20tree-structured%20sequence%20of%20visual%20cues%2C%20integrating%20multimodal%20perception%2C%20spatial%20reasoning%2C%20and%20reward-guided%20exploration%20to%20refine%20geospatial%20hypotheses%20iteratively.%20This%20design%20enables%20the%20model%20to%20detect%20subtle%20small-scale%20targets%20while%20maintaining%20holistic%20scene%20awareness.%20Extensive%20experiments%20on%20five%20remote%20sensing%20grounding%20benchmarks%20demonstrate%20that%20GeoViS%20achieves%20precise%20geospatial%20understanding%20and%20consistently%20surpasses%20existing%20methods%20across%20key%20visual%20grounding%20metrics%2C%20highlighting%20its%20strong%20cross-domain%20generalization%20and%20interpretability.&entry.1838667208=http%3A//arxiv.org/abs/2512.02715v1&entry.124074799=Read"},
{"title": "CogDrive: Cognition-Driven Multimodal Prediction-Planning Fusion for Safe Autonomy", "author": "Heye Huang and Yibin Yang and Mingfeng Fan and Haoran Wang and Xiaocong Zhao and Jianqiang Wang", "abstract": "Safe autonomous driving in mixed traffic requires a unified understanding of multimodal interactions and dynamic planning under uncertainty. Existing learning based approaches struggle to capture rare but safety critical behaviors, while rule based systems often lack adaptability in complex interactions. To address these limitations, CogDrive introduces a cognition driven multimodal prediction and planning framework that integrates explicit modal reasoning with safety aware trajectory optimization. The prediction module adopts cognitive representations of interaction modes based on topological motion semantics and nearest neighbor relational encoding. With a differentiable modal loss and multimodal Gaussian decoding, CogDrive learns sparse and unbalanced interaction behaviors and improves long horizon trajectory prediction. The planning module incorporates an emergency response concept and optimizes safety stabilized trajectories, where short term consistent branches ensure safety during replanning cycles and long term branches support smooth and collision free motion under low probability switching modes. Experiments on Argoverse2 and INTERACTION datasets show that CogDrive achieves strong performance in trajectory accuracy and miss rate, while closed loop simulations confirm adaptive behavior in merge and intersection scenarios. By combining cognitive multimodal prediction with safety oriented planning, CogDrive offers an interpretable and reliable paradigm for safe autonomy in complex traffic.", "link": "http://arxiv.org/abs/2512.02777v1", "date": "2025-12-02", "relevancy": 2.247, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6022}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5912}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5162}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CogDrive%3A%20Cognition-Driven%20Multimodal%20Prediction-Planning%20Fusion%20for%20Safe%20Autonomy&body=Title%3A%20CogDrive%3A%20Cognition-Driven%20Multimodal%20Prediction-Planning%20Fusion%20for%20Safe%20Autonomy%0AAuthor%3A%20Heye%20Huang%20and%20Yibin%20Yang%20and%20Mingfeng%20Fan%20and%20Haoran%20Wang%20and%20Xiaocong%20Zhao%20and%20Jianqiang%20Wang%0AAbstract%3A%20Safe%20autonomous%20driving%20in%20mixed%20traffic%20requires%20a%20unified%20understanding%20of%20multimodal%20interactions%20and%20dynamic%20planning%20under%20uncertainty.%20Existing%20learning%20based%20approaches%20struggle%20to%20capture%20rare%20but%20safety%20critical%20behaviors%2C%20while%20rule%20based%20systems%20often%20lack%20adaptability%20in%20complex%20interactions.%20To%20address%20these%20limitations%2C%20CogDrive%20introduces%20a%20cognition%20driven%20multimodal%20prediction%20and%20planning%20framework%20that%20integrates%20explicit%20modal%20reasoning%20with%20safety%20aware%20trajectory%20optimization.%20The%20prediction%20module%20adopts%20cognitive%20representations%20of%20interaction%20modes%20based%20on%20topological%20motion%20semantics%20and%20nearest%20neighbor%20relational%20encoding.%20With%20a%20differentiable%20modal%20loss%20and%20multimodal%20Gaussian%20decoding%2C%20CogDrive%20learns%20sparse%20and%20unbalanced%20interaction%20behaviors%20and%20improves%20long%20horizon%20trajectory%20prediction.%20The%20planning%20module%20incorporates%20an%20emergency%20response%20concept%20and%20optimizes%20safety%20stabilized%20trajectories%2C%20where%20short%20term%20consistent%20branches%20ensure%20safety%20during%20replanning%20cycles%20and%20long%20term%20branches%20support%20smooth%20and%20collision%20free%20motion%20under%20low%20probability%20switching%20modes.%20Experiments%20on%20Argoverse2%20and%20INTERACTION%20datasets%20show%20that%20CogDrive%20achieves%20strong%20performance%20in%20trajectory%20accuracy%20and%20miss%20rate%2C%20while%20closed%20loop%20simulations%20confirm%20adaptive%20behavior%20in%20merge%20and%20intersection%20scenarios.%20By%20combining%20cognitive%20multimodal%20prediction%20with%20safety%20oriented%20planning%2C%20CogDrive%20offers%20an%20interpretable%20and%20reliable%20paradigm%20for%20safe%20autonomy%20in%20complex%20traffic.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02777v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCogDrive%253A%2520Cognition-Driven%2520Multimodal%2520Prediction-Planning%2520Fusion%2520for%2520Safe%2520Autonomy%26entry.906535625%3DHeye%2520Huang%2520and%2520Yibin%2520Yang%2520and%2520Mingfeng%2520Fan%2520and%2520Haoran%2520Wang%2520and%2520Xiaocong%2520Zhao%2520and%2520Jianqiang%2520Wang%26entry.1292438233%3DSafe%2520autonomous%2520driving%2520in%2520mixed%2520traffic%2520requires%2520a%2520unified%2520understanding%2520of%2520multimodal%2520interactions%2520and%2520dynamic%2520planning%2520under%2520uncertainty.%2520Existing%2520learning%2520based%2520approaches%2520struggle%2520to%2520capture%2520rare%2520but%2520safety%2520critical%2520behaviors%252C%2520while%2520rule%2520based%2520systems%2520often%2520lack%2520adaptability%2520in%2520complex%2520interactions.%2520To%2520address%2520these%2520limitations%252C%2520CogDrive%2520introduces%2520a%2520cognition%2520driven%2520multimodal%2520prediction%2520and%2520planning%2520framework%2520that%2520integrates%2520explicit%2520modal%2520reasoning%2520with%2520safety%2520aware%2520trajectory%2520optimization.%2520The%2520prediction%2520module%2520adopts%2520cognitive%2520representations%2520of%2520interaction%2520modes%2520based%2520on%2520topological%2520motion%2520semantics%2520and%2520nearest%2520neighbor%2520relational%2520encoding.%2520With%2520a%2520differentiable%2520modal%2520loss%2520and%2520multimodal%2520Gaussian%2520decoding%252C%2520CogDrive%2520learns%2520sparse%2520and%2520unbalanced%2520interaction%2520behaviors%2520and%2520improves%2520long%2520horizon%2520trajectory%2520prediction.%2520The%2520planning%2520module%2520incorporates%2520an%2520emergency%2520response%2520concept%2520and%2520optimizes%2520safety%2520stabilized%2520trajectories%252C%2520where%2520short%2520term%2520consistent%2520branches%2520ensure%2520safety%2520during%2520replanning%2520cycles%2520and%2520long%2520term%2520branches%2520support%2520smooth%2520and%2520collision%2520free%2520motion%2520under%2520low%2520probability%2520switching%2520modes.%2520Experiments%2520on%2520Argoverse2%2520and%2520INTERACTION%2520datasets%2520show%2520that%2520CogDrive%2520achieves%2520strong%2520performance%2520in%2520trajectory%2520accuracy%2520and%2520miss%2520rate%252C%2520while%2520closed%2520loop%2520simulations%2520confirm%2520adaptive%2520behavior%2520in%2520merge%2520and%2520intersection%2520scenarios.%2520By%2520combining%2520cognitive%2520multimodal%2520prediction%2520with%2520safety%2520oriented%2520planning%252C%2520CogDrive%2520offers%2520an%2520interpretable%2520and%2520reliable%2520paradigm%2520for%2520safe%2520autonomy%2520in%2520complex%2520traffic.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02777v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CogDrive%3A%20Cognition-Driven%20Multimodal%20Prediction-Planning%20Fusion%20for%20Safe%20Autonomy&entry.906535625=Heye%20Huang%20and%20Yibin%20Yang%20and%20Mingfeng%20Fan%20and%20Haoran%20Wang%20and%20Xiaocong%20Zhao%20and%20Jianqiang%20Wang&entry.1292438233=Safe%20autonomous%20driving%20in%20mixed%20traffic%20requires%20a%20unified%20understanding%20of%20multimodal%20interactions%20and%20dynamic%20planning%20under%20uncertainty.%20Existing%20learning%20based%20approaches%20struggle%20to%20capture%20rare%20but%20safety%20critical%20behaviors%2C%20while%20rule%20based%20systems%20often%20lack%20adaptability%20in%20complex%20interactions.%20To%20address%20these%20limitations%2C%20CogDrive%20introduces%20a%20cognition%20driven%20multimodal%20prediction%20and%20planning%20framework%20that%20integrates%20explicit%20modal%20reasoning%20with%20safety%20aware%20trajectory%20optimization.%20The%20prediction%20module%20adopts%20cognitive%20representations%20of%20interaction%20modes%20based%20on%20topological%20motion%20semantics%20and%20nearest%20neighbor%20relational%20encoding.%20With%20a%20differentiable%20modal%20loss%20and%20multimodal%20Gaussian%20decoding%2C%20CogDrive%20learns%20sparse%20and%20unbalanced%20interaction%20behaviors%20and%20improves%20long%20horizon%20trajectory%20prediction.%20The%20planning%20module%20incorporates%20an%20emergency%20response%20concept%20and%20optimizes%20safety%20stabilized%20trajectories%2C%20where%20short%20term%20consistent%20branches%20ensure%20safety%20during%20replanning%20cycles%20and%20long%20term%20branches%20support%20smooth%20and%20collision%20free%20motion%20under%20low%20probability%20switching%20modes.%20Experiments%20on%20Argoverse2%20and%20INTERACTION%20datasets%20show%20that%20CogDrive%20achieves%20strong%20performance%20in%20trajectory%20accuracy%20and%20miss%20rate%2C%20while%20closed%20loop%20simulations%20confirm%20adaptive%20behavior%20in%20merge%20and%20intersection%20scenarios.%20By%20combining%20cognitive%20multimodal%20prediction%20with%20safety%20oriented%20planning%2C%20CogDrive%20offers%20an%20interpretable%20and%20reliable%20paradigm%20for%20safe%20autonomy%20in%20complex%20traffic.&entry.1838667208=http%3A//arxiv.org/abs/2512.02777v1&entry.124074799=Read"},
{"title": "Brain-aligning of semantic vectors improves neural decoding of visual stimuli", "author": "Shirin Vafaei and Ryohei Fukuma and Takufumi Yanagisawa and Huixiang Yang and Satoru Oshino and Naoki Tani and Hui Ming Khoo and Hidenori Sugano and Yasushi Iimura and Hiroharu Suzuki and Madoka Nakajima and Kentaro Tamura and Haruhiko Kishima", "abstract": "The development of algorithms to accurately decode neural information has long been a research focus in the field of neuroscience. Brain decoding typically involves training machine learning models to map neural data onto a preestablished vector representation of stimulus features. These vectors are usually derived from image- and/or text-based feature spaces. Nonetheless, the intrinsic characteristics of these vectors might fundamentally differ from those that are encoded by the brain, limiting the ability of decoders to accurately learn this mapping. To address this issue, we propose a framework, called brain-aligning of semantic vectors, that fine-tunes pretrained feature vectors to better align with the structure of neural representations of visual stimuli in the brain. We trained this model with functional magnetic resonance imaging (fMRI) and then performed zero-shot brain decoding on fMRI, magnetoencephalography (MEG), and electrocorticography (ECoG) data. fMRI-based brain-aligned vectors improved decoding performance across all three neuroimaging datasets when accuracy was determined by calculating the correlation coefficients between true and predicted vectors. Additionally, when decoding accuracy was determined via stimulus identification, this accuracy increased in specific category types; improvements varied depending on the original vector space that was used for brain-alignment, and consistent improvements were observed across all neuroimaging modalities.", "link": "http://arxiv.org/abs/2403.15176v4", "date": "2025-12-02", "relevancy": 2.2387, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5737}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5737}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brain-aligning%20of%20semantic%20vectors%20improves%20neural%20decoding%20of%20visual%20stimuli&body=Title%3A%20Brain-aligning%20of%20semantic%20vectors%20improves%20neural%20decoding%20of%20visual%20stimuli%0AAuthor%3A%20Shirin%20Vafaei%20and%20Ryohei%20Fukuma%20and%20Takufumi%20Yanagisawa%20and%20Huixiang%20Yang%20and%20Satoru%20Oshino%20and%20Naoki%20Tani%20and%20Hui%20Ming%20Khoo%20and%20Hidenori%20Sugano%20and%20Yasushi%20Iimura%20and%20Hiroharu%20Suzuki%20and%20Madoka%20Nakajima%20and%20Kentaro%20Tamura%20and%20Haruhiko%20Kishima%0AAbstract%3A%20The%20development%20of%20algorithms%20to%20accurately%20decode%20neural%20information%20has%20long%20been%20a%20research%20focus%20in%20the%20field%20of%20neuroscience.%20Brain%20decoding%20typically%20involves%20training%20machine%20learning%20models%20to%20map%20neural%20data%20onto%20a%20preestablished%20vector%20representation%20of%20stimulus%20features.%20These%20vectors%20are%20usually%20derived%20from%20image-%20and/or%20text-based%20feature%20spaces.%20Nonetheless%2C%20the%20intrinsic%20characteristics%20of%20these%20vectors%20might%20fundamentally%20differ%20from%20those%20that%20are%20encoded%20by%20the%20brain%2C%20limiting%20the%20ability%20of%20decoders%20to%20accurately%20learn%20this%20mapping.%20To%20address%20this%20issue%2C%20we%20propose%20a%20framework%2C%20called%20brain-aligning%20of%20semantic%20vectors%2C%20that%20fine-tunes%20pretrained%20feature%20vectors%20to%20better%20align%20with%20the%20structure%20of%20neural%20representations%20of%20visual%20stimuli%20in%20the%20brain.%20We%20trained%20this%20model%20with%20functional%20magnetic%20resonance%20imaging%20%28fMRI%29%20and%20then%20performed%20zero-shot%20brain%20decoding%20on%20fMRI%2C%20magnetoencephalography%20%28MEG%29%2C%20and%20electrocorticography%20%28ECoG%29%20data.%20fMRI-based%20brain-aligned%20vectors%20improved%20decoding%20performance%20across%20all%20three%20neuroimaging%20datasets%20when%20accuracy%20was%20determined%20by%20calculating%20the%20correlation%20coefficients%20between%20true%20and%20predicted%20vectors.%20Additionally%2C%20when%20decoding%20accuracy%20was%20determined%20via%20stimulus%20identification%2C%20this%20accuracy%20increased%20in%20specific%20category%20types%3B%20improvements%20varied%20depending%20on%20the%20original%20vector%20space%20that%20was%20used%20for%20brain-alignment%2C%20and%20consistent%20improvements%20were%20observed%20across%20all%20neuroimaging%20modalities.%0ALink%3A%20http%3A//arxiv.org/abs/2403.15176v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrain-aligning%2520of%2520semantic%2520vectors%2520improves%2520neural%2520decoding%2520of%2520visual%2520stimuli%26entry.906535625%3DShirin%2520Vafaei%2520and%2520Ryohei%2520Fukuma%2520and%2520Takufumi%2520Yanagisawa%2520and%2520Huixiang%2520Yang%2520and%2520Satoru%2520Oshino%2520and%2520Naoki%2520Tani%2520and%2520Hui%2520Ming%2520Khoo%2520and%2520Hidenori%2520Sugano%2520and%2520Yasushi%2520Iimura%2520and%2520Hiroharu%2520Suzuki%2520and%2520Madoka%2520Nakajima%2520and%2520Kentaro%2520Tamura%2520and%2520Haruhiko%2520Kishima%26entry.1292438233%3DThe%2520development%2520of%2520algorithms%2520to%2520accurately%2520decode%2520neural%2520information%2520has%2520long%2520been%2520a%2520research%2520focus%2520in%2520the%2520field%2520of%2520neuroscience.%2520Brain%2520decoding%2520typically%2520involves%2520training%2520machine%2520learning%2520models%2520to%2520map%2520neural%2520data%2520onto%2520a%2520preestablished%2520vector%2520representation%2520of%2520stimulus%2520features.%2520These%2520vectors%2520are%2520usually%2520derived%2520from%2520image-%2520and/or%2520text-based%2520feature%2520spaces.%2520Nonetheless%252C%2520the%2520intrinsic%2520characteristics%2520of%2520these%2520vectors%2520might%2520fundamentally%2520differ%2520from%2520those%2520that%2520are%2520encoded%2520by%2520the%2520brain%252C%2520limiting%2520the%2520ability%2520of%2520decoders%2520to%2520accurately%2520learn%2520this%2520mapping.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520framework%252C%2520called%2520brain-aligning%2520of%2520semantic%2520vectors%252C%2520that%2520fine-tunes%2520pretrained%2520feature%2520vectors%2520to%2520better%2520align%2520with%2520the%2520structure%2520of%2520neural%2520representations%2520of%2520visual%2520stimuli%2520in%2520the%2520brain.%2520We%2520trained%2520this%2520model%2520with%2520functional%2520magnetic%2520resonance%2520imaging%2520%2528fMRI%2529%2520and%2520then%2520performed%2520zero-shot%2520brain%2520decoding%2520on%2520fMRI%252C%2520magnetoencephalography%2520%2528MEG%2529%252C%2520and%2520electrocorticography%2520%2528ECoG%2529%2520data.%2520fMRI-based%2520brain-aligned%2520vectors%2520improved%2520decoding%2520performance%2520across%2520all%2520three%2520neuroimaging%2520datasets%2520when%2520accuracy%2520was%2520determined%2520by%2520calculating%2520the%2520correlation%2520coefficients%2520between%2520true%2520and%2520predicted%2520vectors.%2520Additionally%252C%2520when%2520decoding%2520accuracy%2520was%2520determined%2520via%2520stimulus%2520identification%252C%2520this%2520accuracy%2520increased%2520in%2520specific%2520category%2520types%253B%2520improvements%2520varied%2520depending%2520on%2520the%2520original%2520vector%2520space%2520that%2520was%2520used%2520for%2520brain-alignment%252C%2520and%2520consistent%2520improvements%2520were%2520observed%2520across%2520all%2520neuroimaging%2520modalities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15176v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brain-aligning%20of%20semantic%20vectors%20improves%20neural%20decoding%20of%20visual%20stimuli&entry.906535625=Shirin%20Vafaei%20and%20Ryohei%20Fukuma%20and%20Takufumi%20Yanagisawa%20and%20Huixiang%20Yang%20and%20Satoru%20Oshino%20and%20Naoki%20Tani%20and%20Hui%20Ming%20Khoo%20and%20Hidenori%20Sugano%20and%20Yasushi%20Iimura%20and%20Hiroharu%20Suzuki%20and%20Madoka%20Nakajima%20and%20Kentaro%20Tamura%20and%20Haruhiko%20Kishima&entry.1292438233=The%20development%20of%20algorithms%20to%20accurately%20decode%20neural%20information%20has%20long%20been%20a%20research%20focus%20in%20the%20field%20of%20neuroscience.%20Brain%20decoding%20typically%20involves%20training%20machine%20learning%20models%20to%20map%20neural%20data%20onto%20a%20preestablished%20vector%20representation%20of%20stimulus%20features.%20These%20vectors%20are%20usually%20derived%20from%20image-%20and/or%20text-based%20feature%20spaces.%20Nonetheless%2C%20the%20intrinsic%20characteristics%20of%20these%20vectors%20might%20fundamentally%20differ%20from%20those%20that%20are%20encoded%20by%20the%20brain%2C%20limiting%20the%20ability%20of%20decoders%20to%20accurately%20learn%20this%20mapping.%20To%20address%20this%20issue%2C%20we%20propose%20a%20framework%2C%20called%20brain-aligning%20of%20semantic%20vectors%2C%20that%20fine-tunes%20pretrained%20feature%20vectors%20to%20better%20align%20with%20the%20structure%20of%20neural%20representations%20of%20visual%20stimuli%20in%20the%20brain.%20We%20trained%20this%20model%20with%20functional%20magnetic%20resonance%20imaging%20%28fMRI%29%20and%20then%20performed%20zero-shot%20brain%20decoding%20on%20fMRI%2C%20magnetoencephalography%20%28MEG%29%2C%20and%20electrocorticography%20%28ECoG%29%20data.%20fMRI-based%20brain-aligned%20vectors%20improved%20decoding%20performance%20across%20all%20three%20neuroimaging%20datasets%20when%20accuracy%20was%20determined%20by%20calculating%20the%20correlation%20coefficients%20between%20true%20and%20predicted%20vectors.%20Additionally%2C%20when%20decoding%20accuracy%20was%20determined%20via%20stimulus%20identification%2C%20this%20accuracy%20increased%20in%20specific%20category%20types%3B%20improvements%20varied%20depending%20on%20the%20original%20vector%20space%20that%20was%20used%20for%20brain-alignment%2C%20and%20consistent%20improvements%20were%20observed%20across%20all%20neuroimaging%20modalities.&entry.1838667208=http%3A//arxiv.org/abs/2403.15176v4&entry.124074799=Read"},
{"title": "Training Data Attribution for Image Generation using Ontology-Aligned Knowledge Graphs", "author": "Theodoros Aivalis and Iraklis A. Klampanos and Antonis Troumpoukis and Joemon M. Jose", "abstract": "As generative models become powerful, concerns around transparency, accountability, and copyright violations have intensified. Understanding how specific training data contributes to a model's output is critical. We introduce a framework for interpreting generative outputs through the automatic construction of ontologyaligned knowledge graphs (KGs). While automatic KG construction from natural text has advanced, extracting structured and ontology-consistent representations from visual content remains challenging -- due to the richness and multi-object nature of images. Leveraging multimodal large language models (LLMs), our method extracts structured triples from images, aligned with a domain-specific ontology. By comparing the KGs of generated and training images, we can trace potential influences, enabling copyright analysis, dataset transparency, and interpretable AI. We validate our method through experiments on locally trained models via unlearning, and on large-scale models through a style-specific experiment. Our framework supports the development of AI systems that foster human collaboration, creativity and stimulate curiosity.", "link": "http://arxiv.org/abs/2512.02713v1", "date": "2025-12-02", "relevancy": 2.2376, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5621}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5577}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training%20Data%20Attribution%20for%20Image%20Generation%20using%20Ontology-Aligned%20Knowledge%20Graphs&body=Title%3A%20Training%20Data%20Attribution%20for%20Image%20Generation%20using%20Ontology-Aligned%20Knowledge%20Graphs%0AAuthor%3A%20Theodoros%20Aivalis%20and%20Iraklis%20A.%20Klampanos%20and%20Antonis%20Troumpoukis%20and%20Joemon%20M.%20Jose%0AAbstract%3A%20As%20generative%20models%20become%20powerful%2C%20concerns%20around%20transparency%2C%20accountability%2C%20and%20copyright%20violations%20have%20intensified.%20Understanding%20how%20specific%20training%20data%20contributes%20to%20a%20model%27s%20output%20is%20critical.%20We%20introduce%20a%20framework%20for%20interpreting%20generative%20outputs%20through%20the%20automatic%20construction%20of%20ontologyaligned%20knowledge%20graphs%20%28KGs%29.%20While%20automatic%20KG%20construction%20from%20natural%20text%20has%20advanced%2C%20extracting%20structured%20and%20ontology-consistent%20representations%20from%20visual%20content%20remains%20challenging%20--%20due%20to%20the%20richness%20and%20multi-object%20nature%20of%20images.%20Leveraging%20multimodal%20large%20language%20models%20%28LLMs%29%2C%20our%20method%20extracts%20structured%20triples%20from%20images%2C%20aligned%20with%20a%20domain-specific%20ontology.%20By%20comparing%20the%20KGs%20of%20generated%20and%20training%20images%2C%20we%20can%20trace%20potential%20influences%2C%20enabling%20copyright%20analysis%2C%20dataset%20transparency%2C%20and%20interpretable%20AI.%20We%20validate%20our%20method%20through%20experiments%20on%20locally%20trained%20models%20via%20unlearning%2C%20and%20on%20large-scale%20models%20through%20a%20style-specific%20experiment.%20Our%20framework%20supports%20the%20development%20of%20AI%20systems%20that%20foster%20human%20collaboration%2C%20creativity%20and%20stimulate%20curiosity.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02713v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining%2520Data%2520Attribution%2520for%2520Image%2520Generation%2520using%2520Ontology-Aligned%2520Knowledge%2520Graphs%26entry.906535625%3DTheodoros%2520Aivalis%2520and%2520Iraklis%2520A.%2520Klampanos%2520and%2520Antonis%2520Troumpoukis%2520and%2520Joemon%2520M.%2520Jose%26entry.1292438233%3DAs%2520generative%2520models%2520become%2520powerful%252C%2520concerns%2520around%2520transparency%252C%2520accountability%252C%2520and%2520copyright%2520violations%2520have%2520intensified.%2520Understanding%2520how%2520specific%2520training%2520data%2520contributes%2520to%2520a%2520model%2527s%2520output%2520is%2520critical.%2520We%2520introduce%2520a%2520framework%2520for%2520interpreting%2520generative%2520outputs%2520through%2520the%2520automatic%2520construction%2520of%2520ontologyaligned%2520knowledge%2520graphs%2520%2528KGs%2529.%2520While%2520automatic%2520KG%2520construction%2520from%2520natural%2520text%2520has%2520advanced%252C%2520extracting%2520structured%2520and%2520ontology-consistent%2520representations%2520from%2520visual%2520content%2520remains%2520challenging%2520--%2520due%2520to%2520the%2520richness%2520and%2520multi-object%2520nature%2520of%2520images.%2520Leveraging%2520multimodal%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520our%2520method%2520extracts%2520structured%2520triples%2520from%2520images%252C%2520aligned%2520with%2520a%2520domain-specific%2520ontology.%2520By%2520comparing%2520the%2520KGs%2520of%2520generated%2520and%2520training%2520images%252C%2520we%2520can%2520trace%2520potential%2520influences%252C%2520enabling%2520copyright%2520analysis%252C%2520dataset%2520transparency%252C%2520and%2520interpretable%2520AI.%2520We%2520validate%2520our%2520method%2520through%2520experiments%2520on%2520locally%2520trained%2520models%2520via%2520unlearning%252C%2520and%2520on%2520large-scale%2520models%2520through%2520a%2520style-specific%2520experiment.%2520Our%2520framework%2520supports%2520the%2520development%2520of%2520AI%2520systems%2520that%2520foster%2520human%2520collaboration%252C%2520creativity%2520and%2520stimulate%2520curiosity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02713v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training%20Data%20Attribution%20for%20Image%20Generation%20using%20Ontology-Aligned%20Knowledge%20Graphs&entry.906535625=Theodoros%20Aivalis%20and%20Iraklis%20A.%20Klampanos%20and%20Antonis%20Troumpoukis%20and%20Joemon%20M.%20Jose&entry.1292438233=As%20generative%20models%20become%20powerful%2C%20concerns%20around%20transparency%2C%20accountability%2C%20and%20copyright%20violations%20have%20intensified.%20Understanding%20how%20specific%20training%20data%20contributes%20to%20a%20model%27s%20output%20is%20critical.%20We%20introduce%20a%20framework%20for%20interpreting%20generative%20outputs%20through%20the%20automatic%20construction%20of%20ontologyaligned%20knowledge%20graphs%20%28KGs%29.%20While%20automatic%20KG%20construction%20from%20natural%20text%20has%20advanced%2C%20extracting%20structured%20and%20ontology-consistent%20representations%20from%20visual%20content%20remains%20challenging%20--%20due%20to%20the%20richness%20and%20multi-object%20nature%20of%20images.%20Leveraging%20multimodal%20large%20language%20models%20%28LLMs%29%2C%20our%20method%20extracts%20structured%20triples%20from%20images%2C%20aligned%20with%20a%20domain-specific%20ontology.%20By%20comparing%20the%20KGs%20of%20generated%20and%20training%20images%2C%20we%20can%20trace%20potential%20influences%2C%20enabling%20copyright%20analysis%2C%20dataset%20transparency%2C%20and%20interpretable%20AI.%20We%20validate%20our%20method%20through%20experiments%20on%20locally%20trained%20models%20via%20unlearning%2C%20and%20on%20large-scale%20models%20through%20a%20style-specific%20experiment.%20Our%20framework%20supports%20the%20development%20of%20AI%20systems%20that%20foster%20human%20collaboration%2C%20creativity%20and%20stimulate%20curiosity.&entry.1838667208=http%3A//arxiv.org/abs/2512.02713v1&entry.124074799=Read"},
{"title": "MAViD: A Multimodal Framework for Audio-Visual Dialogue Understanding and Generation", "author": "Youxin Pang and Jiajun Liu and Lingfeng Tan and Yong Zhang and Feng Gao and Xiang Deng and Zhuoliang Kang and Xiaoming Wei and Yebin Liu", "abstract": "We propose MAViD, a novel Multimodal framework for Audio-Visual Dialogue understanding and generation. Existing approaches primarily focus on non-interactive systems and are limited to producing constrained and unnatural human speech.The primary challenge of this task lies in effectively integrating understanding and generation capabilities, as well as achieving seamless multimodal audio-video fusion. To solve these problems, we propose a Conductor-Creator architecture that divides the dialogue system into two primary components.The Conductor is tasked with understanding, reasoning, and generating instructions by breaking them down into motion and speech components, thereby enabling fine-grained control over interactions. The Creator then delivers interactive responses based on these instructions.Furthermore, to address the difficulty of generating long videos with consistent identity, timbre, and tone using dual DiT structures, the Creator adopts a structure that combines autoregressive (AR) and diffusion models. The AR model is responsible for audio generation, while the diffusion model ensures high-quality video generation.Additionally, we propose a novel fusion module to enhance connections between contextually consecutive clips and modalities, enabling synchronized long-duration audio-visual content generation.Extensive experiments demonstrate that our framework can generate vivid and contextually coherent long-duration dialogue interactions and accurately interpret users' multimodal queries.", "link": "http://arxiv.org/abs/2512.03034v1", "date": "2025-12-02", "relevancy": 2.2343, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5975}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5341}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5295}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAViD%3A%20A%20Multimodal%20Framework%20for%20Audio-Visual%20Dialogue%20Understanding%20and%20Generation&body=Title%3A%20MAViD%3A%20A%20Multimodal%20Framework%20for%20Audio-Visual%20Dialogue%20Understanding%20and%20Generation%0AAuthor%3A%20Youxin%20Pang%20and%20Jiajun%20Liu%20and%20Lingfeng%20Tan%20and%20Yong%20Zhang%20and%20Feng%20Gao%20and%20Xiang%20Deng%20and%20Zhuoliang%20Kang%20and%20Xiaoming%20Wei%20and%20Yebin%20Liu%0AAbstract%3A%20We%20propose%20MAViD%2C%20a%20novel%20Multimodal%20framework%20for%20Audio-Visual%20Dialogue%20understanding%20and%20generation.%20Existing%20approaches%20primarily%20focus%20on%20non-interactive%20systems%20and%20are%20limited%20to%20producing%20constrained%20and%20unnatural%20human%20speech.The%20primary%20challenge%20of%20this%20task%20lies%20in%20effectively%20integrating%20understanding%20and%20generation%20capabilities%2C%20as%20well%20as%20achieving%20seamless%20multimodal%20audio-video%20fusion.%20To%20solve%20these%20problems%2C%20we%20propose%20a%20Conductor-Creator%20architecture%20that%20divides%20the%20dialogue%20system%20into%20two%20primary%20components.The%20Conductor%20is%20tasked%20with%20understanding%2C%20reasoning%2C%20and%20generating%20instructions%20by%20breaking%20them%20down%20into%20motion%20and%20speech%20components%2C%20thereby%20enabling%20fine-grained%20control%20over%20interactions.%20The%20Creator%20then%20delivers%20interactive%20responses%20based%20on%20these%20instructions.Furthermore%2C%20to%20address%20the%20difficulty%20of%20generating%20long%20videos%20with%20consistent%20identity%2C%20timbre%2C%20and%20tone%20using%20dual%20DiT%20structures%2C%20the%20Creator%20adopts%20a%20structure%20that%20combines%20autoregressive%20%28AR%29%20and%20diffusion%20models.%20The%20AR%20model%20is%20responsible%20for%20audio%20generation%2C%20while%20the%20diffusion%20model%20ensures%20high-quality%20video%20generation.Additionally%2C%20we%20propose%20a%20novel%20fusion%20module%20to%20enhance%20connections%20between%20contextually%20consecutive%20clips%20and%20modalities%2C%20enabling%20synchronized%20long-duration%20audio-visual%20content%20generation.Extensive%20experiments%20demonstrate%20that%20our%20framework%20can%20generate%20vivid%20and%20contextually%20coherent%20long-duration%20dialogue%20interactions%20and%20accurately%20interpret%20users%27%20multimodal%20queries.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03034v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAViD%253A%2520A%2520Multimodal%2520Framework%2520for%2520Audio-Visual%2520Dialogue%2520Understanding%2520and%2520Generation%26entry.906535625%3DYouxin%2520Pang%2520and%2520Jiajun%2520Liu%2520and%2520Lingfeng%2520Tan%2520and%2520Yong%2520Zhang%2520and%2520Feng%2520Gao%2520and%2520Xiang%2520Deng%2520and%2520Zhuoliang%2520Kang%2520and%2520Xiaoming%2520Wei%2520and%2520Yebin%2520Liu%26entry.1292438233%3DWe%2520propose%2520MAViD%252C%2520a%2520novel%2520Multimodal%2520framework%2520for%2520Audio-Visual%2520Dialogue%2520understanding%2520and%2520generation.%2520Existing%2520approaches%2520primarily%2520focus%2520on%2520non-interactive%2520systems%2520and%2520are%2520limited%2520to%2520producing%2520constrained%2520and%2520unnatural%2520human%2520speech.The%2520primary%2520challenge%2520of%2520this%2520task%2520lies%2520in%2520effectively%2520integrating%2520understanding%2520and%2520generation%2520capabilities%252C%2520as%2520well%2520as%2520achieving%2520seamless%2520multimodal%2520audio-video%2520fusion.%2520To%2520solve%2520these%2520problems%252C%2520we%2520propose%2520a%2520Conductor-Creator%2520architecture%2520that%2520divides%2520the%2520dialogue%2520system%2520into%2520two%2520primary%2520components.The%2520Conductor%2520is%2520tasked%2520with%2520understanding%252C%2520reasoning%252C%2520and%2520generating%2520instructions%2520by%2520breaking%2520them%2520down%2520into%2520motion%2520and%2520speech%2520components%252C%2520thereby%2520enabling%2520fine-grained%2520control%2520over%2520interactions.%2520The%2520Creator%2520then%2520delivers%2520interactive%2520responses%2520based%2520on%2520these%2520instructions.Furthermore%252C%2520to%2520address%2520the%2520difficulty%2520of%2520generating%2520long%2520videos%2520with%2520consistent%2520identity%252C%2520timbre%252C%2520and%2520tone%2520using%2520dual%2520DiT%2520structures%252C%2520the%2520Creator%2520adopts%2520a%2520structure%2520that%2520combines%2520autoregressive%2520%2528AR%2529%2520and%2520diffusion%2520models.%2520The%2520AR%2520model%2520is%2520responsible%2520for%2520audio%2520generation%252C%2520while%2520the%2520diffusion%2520model%2520ensures%2520high-quality%2520video%2520generation.Additionally%252C%2520we%2520propose%2520a%2520novel%2520fusion%2520module%2520to%2520enhance%2520connections%2520between%2520contextually%2520consecutive%2520clips%2520and%2520modalities%252C%2520enabling%2520synchronized%2520long-duration%2520audio-visual%2520content%2520generation.Extensive%2520experiments%2520demonstrate%2520that%2520our%2520framework%2520can%2520generate%2520vivid%2520and%2520contextually%2520coherent%2520long-duration%2520dialogue%2520interactions%2520and%2520accurately%2520interpret%2520users%2527%2520multimodal%2520queries.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03034v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAViD%3A%20A%20Multimodal%20Framework%20for%20Audio-Visual%20Dialogue%20Understanding%20and%20Generation&entry.906535625=Youxin%20Pang%20and%20Jiajun%20Liu%20and%20Lingfeng%20Tan%20and%20Yong%20Zhang%20and%20Feng%20Gao%20and%20Xiang%20Deng%20and%20Zhuoliang%20Kang%20and%20Xiaoming%20Wei%20and%20Yebin%20Liu&entry.1292438233=We%20propose%20MAViD%2C%20a%20novel%20Multimodal%20framework%20for%20Audio-Visual%20Dialogue%20understanding%20and%20generation.%20Existing%20approaches%20primarily%20focus%20on%20non-interactive%20systems%20and%20are%20limited%20to%20producing%20constrained%20and%20unnatural%20human%20speech.The%20primary%20challenge%20of%20this%20task%20lies%20in%20effectively%20integrating%20understanding%20and%20generation%20capabilities%2C%20as%20well%20as%20achieving%20seamless%20multimodal%20audio-video%20fusion.%20To%20solve%20these%20problems%2C%20we%20propose%20a%20Conductor-Creator%20architecture%20that%20divides%20the%20dialogue%20system%20into%20two%20primary%20components.The%20Conductor%20is%20tasked%20with%20understanding%2C%20reasoning%2C%20and%20generating%20instructions%20by%20breaking%20them%20down%20into%20motion%20and%20speech%20components%2C%20thereby%20enabling%20fine-grained%20control%20over%20interactions.%20The%20Creator%20then%20delivers%20interactive%20responses%20based%20on%20these%20instructions.Furthermore%2C%20to%20address%20the%20difficulty%20of%20generating%20long%20videos%20with%20consistent%20identity%2C%20timbre%2C%20and%20tone%20using%20dual%20DiT%20structures%2C%20the%20Creator%20adopts%20a%20structure%20that%20combines%20autoregressive%20%28AR%29%20and%20diffusion%20models.%20The%20AR%20model%20is%20responsible%20for%20audio%20generation%2C%20while%20the%20diffusion%20model%20ensures%20high-quality%20video%20generation.Additionally%2C%20we%20propose%20a%20novel%20fusion%20module%20to%20enhance%20connections%20between%20contextually%20consecutive%20clips%20and%20modalities%2C%20enabling%20synchronized%20long-duration%20audio-visual%20content%20generation.Extensive%20experiments%20demonstrate%20that%20our%20framework%20can%20generate%20vivid%20and%20contextually%20coherent%20long-duration%20dialogue%20interactions%20and%20accurately%20interpret%20users%27%20multimodal%20queries.&entry.1838667208=http%3A//arxiv.org/abs/2512.03034v1&entry.124074799=Read"},
{"title": "Adversarial Jamming for Autoencoder Distribution Matching", "author": "Waleed El-Geresy and Deniz G\u00fcnd\u00fcz", "abstract": "We propose the use of adversarial wireless jamming to regularise the latent space of an autoencoder to match a diagonal Gaussian distribution. We consider the minimisation of a mean squared error distortion, where a jammer attempts to disrupt the recovery of a Gaussian source encoded and transmitted over the adversarial channel. A straightforward consequence of existing theoretical results is the fact that the saddle point of a minimax game - involving such an encoder, its corresponding decoder, and an adversarial jammer - consists of diagonal Gaussian noise output by the jammer. We use this result as inspiration for a novel approach to distribution matching in the latent space, utilising jamming as an auxiliary objective to encourage the aggregated latent posterior to match a diagonal Gaussian distribution. Using this new technique, we achieve distribution matching comparable to standard variational autoencoders and to Wasserstein autoencoders. This approach can also be generalised to other latent distributions.", "link": "http://arxiv.org/abs/2512.02740v1", "date": "2025-12-02", "relevancy": 2.2326, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4598}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4442}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4355}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversarial%20Jamming%20for%20Autoencoder%20Distribution%20Matching&body=Title%3A%20Adversarial%20Jamming%20for%20Autoencoder%20Distribution%20Matching%0AAuthor%3A%20Waleed%20El-Geresy%20and%20Deniz%20G%C3%BCnd%C3%BCz%0AAbstract%3A%20We%20propose%20the%20use%20of%20adversarial%20wireless%20jamming%20to%20regularise%20the%20latent%20space%20of%20an%20autoencoder%20to%20match%20a%20diagonal%20Gaussian%20distribution.%20We%20consider%20the%20minimisation%20of%20a%20mean%20squared%20error%20distortion%2C%20where%20a%20jammer%20attempts%20to%20disrupt%20the%20recovery%20of%20a%20Gaussian%20source%20encoded%20and%20transmitted%20over%20the%20adversarial%20channel.%20A%20straightforward%20consequence%20of%20existing%20theoretical%20results%20is%20the%20fact%20that%20the%20saddle%20point%20of%20a%20minimax%20game%20-%20involving%20such%20an%20encoder%2C%20its%20corresponding%20decoder%2C%20and%20an%20adversarial%20jammer%20-%20consists%20of%20diagonal%20Gaussian%20noise%20output%20by%20the%20jammer.%20We%20use%20this%20result%20as%20inspiration%20for%20a%20novel%20approach%20to%20distribution%20matching%20in%20the%20latent%20space%2C%20utilising%20jamming%20as%20an%20auxiliary%20objective%20to%20encourage%20the%20aggregated%20latent%20posterior%20to%20match%20a%20diagonal%20Gaussian%20distribution.%20Using%20this%20new%20technique%2C%20we%20achieve%20distribution%20matching%20comparable%20to%20standard%20variational%20autoencoders%20and%20to%20Wasserstein%20autoencoders.%20This%20approach%20can%20also%20be%20generalised%20to%20other%20latent%20distributions.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02740v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversarial%2520Jamming%2520for%2520Autoencoder%2520Distribution%2520Matching%26entry.906535625%3DWaleed%2520El-Geresy%2520and%2520Deniz%2520G%25C3%25BCnd%25C3%25BCz%26entry.1292438233%3DWe%2520propose%2520the%2520use%2520of%2520adversarial%2520wireless%2520jamming%2520to%2520regularise%2520the%2520latent%2520space%2520of%2520an%2520autoencoder%2520to%2520match%2520a%2520diagonal%2520Gaussian%2520distribution.%2520We%2520consider%2520the%2520minimisation%2520of%2520a%2520mean%2520squared%2520error%2520distortion%252C%2520where%2520a%2520jammer%2520attempts%2520to%2520disrupt%2520the%2520recovery%2520of%2520a%2520Gaussian%2520source%2520encoded%2520and%2520transmitted%2520over%2520the%2520adversarial%2520channel.%2520A%2520straightforward%2520consequence%2520of%2520existing%2520theoretical%2520results%2520is%2520the%2520fact%2520that%2520the%2520saddle%2520point%2520of%2520a%2520minimax%2520game%2520-%2520involving%2520such%2520an%2520encoder%252C%2520its%2520corresponding%2520decoder%252C%2520and%2520an%2520adversarial%2520jammer%2520-%2520consists%2520of%2520diagonal%2520Gaussian%2520noise%2520output%2520by%2520the%2520jammer.%2520We%2520use%2520this%2520result%2520as%2520inspiration%2520for%2520a%2520novel%2520approach%2520to%2520distribution%2520matching%2520in%2520the%2520latent%2520space%252C%2520utilising%2520jamming%2520as%2520an%2520auxiliary%2520objective%2520to%2520encourage%2520the%2520aggregated%2520latent%2520posterior%2520to%2520match%2520a%2520diagonal%2520Gaussian%2520distribution.%2520Using%2520this%2520new%2520technique%252C%2520we%2520achieve%2520distribution%2520matching%2520comparable%2520to%2520standard%2520variational%2520autoencoders%2520and%2520to%2520Wasserstein%2520autoencoders.%2520This%2520approach%2520can%2520also%2520be%2520generalised%2520to%2520other%2520latent%2520distributions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02740v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversarial%20Jamming%20for%20Autoencoder%20Distribution%20Matching&entry.906535625=Waleed%20El-Geresy%20and%20Deniz%20G%C3%BCnd%C3%BCz&entry.1292438233=We%20propose%20the%20use%20of%20adversarial%20wireless%20jamming%20to%20regularise%20the%20latent%20space%20of%20an%20autoencoder%20to%20match%20a%20diagonal%20Gaussian%20distribution.%20We%20consider%20the%20minimisation%20of%20a%20mean%20squared%20error%20distortion%2C%20where%20a%20jammer%20attempts%20to%20disrupt%20the%20recovery%20of%20a%20Gaussian%20source%20encoded%20and%20transmitted%20over%20the%20adversarial%20channel.%20A%20straightforward%20consequence%20of%20existing%20theoretical%20results%20is%20the%20fact%20that%20the%20saddle%20point%20of%20a%20minimax%20game%20-%20involving%20such%20an%20encoder%2C%20its%20corresponding%20decoder%2C%20and%20an%20adversarial%20jammer%20-%20consists%20of%20diagonal%20Gaussian%20noise%20output%20by%20the%20jammer.%20We%20use%20this%20result%20as%20inspiration%20for%20a%20novel%20approach%20to%20distribution%20matching%20in%20the%20latent%20space%2C%20utilising%20jamming%20as%20an%20auxiliary%20objective%20to%20encourage%20the%20aggregated%20latent%20posterior%20to%20match%20a%20diagonal%20Gaussian%20distribution.%20Using%20this%20new%20technique%2C%20we%20achieve%20distribution%20matching%20comparable%20to%20standard%20variational%20autoencoders%20and%20to%20Wasserstein%20autoencoders.%20This%20approach%20can%20also%20be%20generalised%20to%20other%20latent%20distributions.&entry.1838667208=http%3A//arxiv.org/abs/2512.02740v1&entry.124074799=Read"},
{"title": "Unsupervised Structural Scene Decomposition via Foreground-Aware Slot Attention with Pseudo-Mask Guidance", "author": "Huankun Sheng and Ming Li and Yixiang Wei and Yeying Fan and Yu-Hui Wen and Tieliang Gong and Yong-Jin Liu", "abstract": "Recent advances in object-centric representation learning have shown that slot attention-based methods can effectively decompose visual scenes into object slot representations without supervision. However, existing approaches typically process foreground and background regions indiscriminately, often resulting in background interference and suboptimal instance discovery performance on real-world data. To address this limitation, we propose Foreground-Aware Slot Attention (FASA), a two-stage framework that explicitly separates foreground from background to enable precise object discovery. In the first stage, FASA performs a coarse scene decomposition to distinguish foreground from background regions through a dual-slot competition mechanism. These slots are initialized via a clustering-based strategy, yielding well-structured representations of salient regions. In the second stage, we introduce a masked slot attention mechanism where the first slot captures the background while the remaining slots compete to represent individual foreground objects. To further address over-segmentation of foreground objects, we incorporate pseudo-mask guidance derived from a patch affinity graph constructed with self-supervised image features to guide the learning of foreground slots. Extensive experiments on both synthetic and real-world datasets demonstrate that FASA consistently outperforms state-of-the-art methods, validating the effectiveness of explicit foreground modeling and pseudo-mask guidance for robust scene decomposition and object-coherent representation. Code will be made publicly available.", "link": "http://arxiv.org/abs/2512.02685v1", "date": "2025-12-02", "relevancy": 2.2325, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5637}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5545}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Structural%20Scene%20Decomposition%20via%20Foreground-Aware%20Slot%20Attention%20with%20Pseudo-Mask%20Guidance&body=Title%3A%20Unsupervised%20Structural%20Scene%20Decomposition%20via%20Foreground-Aware%20Slot%20Attention%20with%20Pseudo-Mask%20Guidance%0AAuthor%3A%20Huankun%20Sheng%20and%20Ming%20Li%20and%20Yixiang%20Wei%20and%20Yeying%20Fan%20and%20Yu-Hui%20Wen%20and%20Tieliang%20Gong%20and%20Yong-Jin%20Liu%0AAbstract%3A%20Recent%20advances%20in%20object-centric%20representation%20learning%20have%20shown%20that%20slot%20attention-based%20methods%20can%20effectively%20decompose%20visual%20scenes%20into%20object%20slot%20representations%20without%20supervision.%20However%2C%20existing%20approaches%20typically%20process%20foreground%20and%20background%20regions%20indiscriminately%2C%20often%20resulting%20in%20background%20interference%20and%20suboptimal%20instance%20discovery%20performance%20on%20real-world%20data.%20To%20address%20this%20limitation%2C%20we%20propose%20Foreground-Aware%20Slot%20Attention%20%28FASA%29%2C%20a%20two-stage%20framework%20that%20explicitly%20separates%20foreground%20from%20background%20to%20enable%20precise%20object%20discovery.%20In%20the%20first%20stage%2C%20FASA%20performs%20a%20coarse%20scene%20decomposition%20to%20distinguish%20foreground%20from%20background%20regions%20through%20a%20dual-slot%20competition%20mechanism.%20These%20slots%20are%20initialized%20via%20a%20clustering-based%20strategy%2C%20yielding%20well-structured%20representations%20of%20salient%20regions.%20In%20the%20second%20stage%2C%20we%20introduce%20a%20masked%20slot%20attention%20mechanism%20where%20the%20first%20slot%20captures%20the%20background%20while%20the%20remaining%20slots%20compete%20to%20represent%20individual%20foreground%20objects.%20To%20further%20address%20over-segmentation%20of%20foreground%20objects%2C%20we%20incorporate%20pseudo-mask%20guidance%20derived%20from%20a%20patch%20affinity%20graph%20constructed%20with%20self-supervised%20image%20features%20to%20guide%20the%20learning%20of%20foreground%20slots.%20Extensive%20experiments%20on%20both%20synthetic%20and%20real-world%20datasets%20demonstrate%20that%20FASA%20consistently%20outperforms%20state-of-the-art%20methods%2C%20validating%20the%20effectiveness%20of%20explicit%20foreground%20modeling%20and%20pseudo-mask%20guidance%20for%20robust%20scene%20decomposition%20and%20object-coherent%20representation.%20Code%20will%20be%20made%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02685v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Structural%2520Scene%2520Decomposition%2520via%2520Foreground-Aware%2520Slot%2520Attention%2520with%2520Pseudo-Mask%2520Guidance%26entry.906535625%3DHuankun%2520Sheng%2520and%2520Ming%2520Li%2520and%2520Yixiang%2520Wei%2520and%2520Yeying%2520Fan%2520and%2520Yu-Hui%2520Wen%2520and%2520Tieliang%2520Gong%2520and%2520Yong-Jin%2520Liu%26entry.1292438233%3DRecent%2520advances%2520in%2520object-centric%2520representation%2520learning%2520have%2520shown%2520that%2520slot%2520attention-based%2520methods%2520can%2520effectively%2520decompose%2520visual%2520scenes%2520into%2520object%2520slot%2520representations%2520without%2520supervision.%2520However%252C%2520existing%2520approaches%2520typically%2520process%2520foreground%2520and%2520background%2520regions%2520indiscriminately%252C%2520often%2520resulting%2520in%2520background%2520interference%2520and%2520suboptimal%2520instance%2520discovery%2520performance%2520on%2520real-world%2520data.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520Foreground-Aware%2520Slot%2520Attention%2520%2528FASA%2529%252C%2520a%2520two-stage%2520framework%2520that%2520explicitly%2520separates%2520foreground%2520from%2520background%2520to%2520enable%2520precise%2520object%2520discovery.%2520In%2520the%2520first%2520stage%252C%2520FASA%2520performs%2520a%2520coarse%2520scene%2520decomposition%2520to%2520distinguish%2520foreground%2520from%2520background%2520regions%2520through%2520a%2520dual-slot%2520competition%2520mechanism.%2520These%2520slots%2520are%2520initialized%2520via%2520a%2520clustering-based%2520strategy%252C%2520yielding%2520well-structured%2520representations%2520of%2520salient%2520regions.%2520In%2520the%2520second%2520stage%252C%2520we%2520introduce%2520a%2520masked%2520slot%2520attention%2520mechanism%2520where%2520the%2520first%2520slot%2520captures%2520the%2520background%2520while%2520the%2520remaining%2520slots%2520compete%2520to%2520represent%2520individual%2520foreground%2520objects.%2520To%2520further%2520address%2520over-segmentation%2520of%2520foreground%2520objects%252C%2520we%2520incorporate%2520pseudo-mask%2520guidance%2520derived%2520from%2520a%2520patch%2520affinity%2520graph%2520constructed%2520with%2520self-supervised%2520image%2520features%2520to%2520guide%2520the%2520learning%2520of%2520foreground%2520slots.%2520Extensive%2520experiments%2520on%2520both%2520synthetic%2520and%2520real-world%2520datasets%2520demonstrate%2520that%2520FASA%2520consistently%2520outperforms%2520state-of-the-art%2520methods%252C%2520validating%2520the%2520effectiveness%2520of%2520explicit%2520foreground%2520modeling%2520and%2520pseudo-mask%2520guidance%2520for%2520robust%2520scene%2520decomposition%2520and%2520object-coherent%2520representation.%2520Code%2520will%2520be%2520made%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02685v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Structural%20Scene%20Decomposition%20via%20Foreground-Aware%20Slot%20Attention%20with%20Pseudo-Mask%20Guidance&entry.906535625=Huankun%20Sheng%20and%20Ming%20Li%20and%20Yixiang%20Wei%20and%20Yeying%20Fan%20and%20Yu-Hui%20Wen%20and%20Tieliang%20Gong%20and%20Yong-Jin%20Liu&entry.1292438233=Recent%20advances%20in%20object-centric%20representation%20learning%20have%20shown%20that%20slot%20attention-based%20methods%20can%20effectively%20decompose%20visual%20scenes%20into%20object%20slot%20representations%20without%20supervision.%20However%2C%20existing%20approaches%20typically%20process%20foreground%20and%20background%20regions%20indiscriminately%2C%20often%20resulting%20in%20background%20interference%20and%20suboptimal%20instance%20discovery%20performance%20on%20real-world%20data.%20To%20address%20this%20limitation%2C%20we%20propose%20Foreground-Aware%20Slot%20Attention%20%28FASA%29%2C%20a%20two-stage%20framework%20that%20explicitly%20separates%20foreground%20from%20background%20to%20enable%20precise%20object%20discovery.%20In%20the%20first%20stage%2C%20FASA%20performs%20a%20coarse%20scene%20decomposition%20to%20distinguish%20foreground%20from%20background%20regions%20through%20a%20dual-slot%20competition%20mechanism.%20These%20slots%20are%20initialized%20via%20a%20clustering-based%20strategy%2C%20yielding%20well-structured%20representations%20of%20salient%20regions.%20In%20the%20second%20stage%2C%20we%20introduce%20a%20masked%20slot%20attention%20mechanism%20where%20the%20first%20slot%20captures%20the%20background%20while%20the%20remaining%20slots%20compete%20to%20represent%20individual%20foreground%20objects.%20To%20further%20address%20over-segmentation%20of%20foreground%20objects%2C%20we%20incorporate%20pseudo-mask%20guidance%20derived%20from%20a%20patch%20affinity%20graph%20constructed%20with%20self-supervised%20image%20features%20to%20guide%20the%20learning%20of%20foreground%20slots.%20Extensive%20experiments%20on%20both%20synthetic%20and%20real-world%20datasets%20demonstrate%20that%20FASA%20consistently%20outperforms%20state-of-the-art%20methods%2C%20validating%20the%20effectiveness%20of%20explicit%20foreground%20modeling%20and%20pseudo-mask%20guidance%20for%20robust%20scene%20decomposition%20and%20object-coherent%20representation.%20Code%20will%20be%20made%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2512.02685v1&entry.124074799=Read"},
{"title": "Graph VQ-Transformer (GVT): Fast and Accurate Molecular Generation via High-Fidelity Discrete Latents", "author": "Haozhuo Zheng and Cheng Wang and Yang Liu", "abstract": "The de novo generation of molecules with desirable properties is a critical challenge, where diffusion models are computationally intensive and autoregressive models struggle with error propagation. In this work, we introduce the Graph VQ-Transformer (GVT), a two-stage generative framework that achieves both high accuracy and efficiency. The core of our approach is a novel Graph Vector Quantized Variational Autoencoder (VQ-VAE) that compresses molecular graphs into high-fidelity discrete latent sequences. By synergistically combining a Graph Transformer with canonical Reverse Cuthill-McKee (RCM) node ordering and Rotary Positional Embeddings (RoPE), our VQ-VAE achieves near-perfect reconstruction rates. An autoregressive Transformer is then trained on these discrete latents, effectively converting graph generation into a well-structured sequence modeling problem. Crucially, this mapping of complex graphs to high-fidelity discrete sequences bridges molecular design with the powerful paradigm of large-scale sequence modeling, unlocking potential synergies with Large Language Models (LLMs). Extensive experiments show that GVT achieves state-of-the-art or highly competitive performance across major benchmarks like ZINC250k, MOSES, and GuacaMol, and notably outperforms leading diffusion models on key distribution similarity metrics such as FCD and KL Divergence. With its superior performance, efficiency, and architectural novelty, GVT not only presents a compelling alternative to diffusion models but also establishes a strong new baseline for the field, paving the way for future research in discrete latent-space molecular generation.", "link": "http://arxiv.org/abs/2512.02667v1", "date": "2025-12-02", "relevancy": 2.2234, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5842}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5506}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20VQ-Transformer%20%28GVT%29%3A%20Fast%20and%20Accurate%20Molecular%20Generation%20via%20High-Fidelity%20Discrete%20Latents&body=Title%3A%20Graph%20VQ-Transformer%20%28GVT%29%3A%20Fast%20and%20Accurate%20Molecular%20Generation%20via%20High-Fidelity%20Discrete%20Latents%0AAuthor%3A%20Haozhuo%20Zheng%20and%20Cheng%20Wang%20and%20Yang%20Liu%0AAbstract%3A%20The%20de%20novo%20generation%20of%20molecules%20with%20desirable%20properties%20is%20a%20critical%20challenge%2C%20where%20diffusion%20models%20are%20computationally%20intensive%20and%20autoregressive%20models%20struggle%20with%20error%20propagation.%20In%20this%20work%2C%20we%20introduce%20the%20Graph%20VQ-Transformer%20%28GVT%29%2C%20a%20two-stage%20generative%20framework%20that%20achieves%20both%20high%20accuracy%20and%20efficiency.%20The%20core%20of%20our%20approach%20is%20a%20novel%20Graph%20Vector%20Quantized%20Variational%20Autoencoder%20%28VQ-VAE%29%20that%20compresses%20molecular%20graphs%20into%20high-fidelity%20discrete%20latent%20sequences.%20By%20synergistically%20combining%20a%20Graph%20Transformer%20with%20canonical%20Reverse%20Cuthill-McKee%20%28RCM%29%20node%20ordering%20and%20Rotary%20Positional%20Embeddings%20%28RoPE%29%2C%20our%20VQ-VAE%20achieves%20near-perfect%20reconstruction%20rates.%20An%20autoregressive%20Transformer%20is%20then%20trained%20on%20these%20discrete%20latents%2C%20effectively%20converting%20graph%20generation%20into%20a%20well-structured%20sequence%20modeling%20problem.%20Crucially%2C%20this%20mapping%20of%20complex%20graphs%20to%20high-fidelity%20discrete%20sequences%20bridges%20molecular%20design%20with%20the%20powerful%20paradigm%20of%20large-scale%20sequence%20modeling%2C%20unlocking%20potential%20synergies%20with%20Large%20Language%20Models%20%28LLMs%29.%20Extensive%20experiments%20show%20that%20GVT%20achieves%20state-of-the-art%20or%20highly%20competitive%20performance%20across%20major%20benchmarks%20like%20ZINC250k%2C%20MOSES%2C%20and%20GuacaMol%2C%20and%20notably%20outperforms%20leading%20diffusion%20models%20on%20key%20distribution%20similarity%20metrics%20such%20as%20FCD%20and%20KL%20Divergence.%20With%20its%20superior%20performance%2C%20efficiency%2C%20and%20architectural%20novelty%2C%20GVT%20not%20only%20presents%20a%20compelling%20alternative%20to%20diffusion%20models%20but%20also%20establishes%20a%20strong%20new%20baseline%20for%20the%20field%2C%20paving%20the%20way%20for%20future%20research%20in%20discrete%20latent-space%20molecular%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02667v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520VQ-Transformer%2520%2528GVT%2529%253A%2520Fast%2520and%2520Accurate%2520Molecular%2520Generation%2520via%2520High-Fidelity%2520Discrete%2520Latents%26entry.906535625%3DHaozhuo%2520Zheng%2520and%2520Cheng%2520Wang%2520and%2520Yang%2520Liu%26entry.1292438233%3DThe%2520de%2520novo%2520generation%2520of%2520molecules%2520with%2520desirable%2520properties%2520is%2520a%2520critical%2520challenge%252C%2520where%2520diffusion%2520models%2520are%2520computationally%2520intensive%2520and%2520autoregressive%2520models%2520struggle%2520with%2520error%2520propagation.%2520In%2520this%2520work%252C%2520we%2520introduce%2520the%2520Graph%2520VQ-Transformer%2520%2528GVT%2529%252C%2520a%2520two-stage%2520generative%2520framework%2520that%2520achieves%2520both%2520high%2520accuracy%2520and%2520efficiency.%2520The%2520core%2520of%2520our%2520approach%2520is%2520a%2520novel%2520Graph%2520Vector%2520Quantized%2520Variational%2520Autoencoder%2520%2528VQ-VAE%2529%2520that%2520compresses%2520molecular%2520graphs%2520into%2520high-fidelity%2520discrete%2520latent%2520sequences.%2520By%2520synergistically%2520combining%2520a%2520Graph%2520Transformer%2520with%2520canonical%2520Reverse%2520Cuthill-McKee%2520%2528RCM%2529%2520node%2520ordering%2520and%2520Rotary%2520Positional%2520Embeddings%2520%2528RoPE%2529%252C%2520our%2520VQ-VAE%2520achieves%2520near-perfect%2520reconstruction%2520rates.%2520An%2520autoregressive%2520Transformer%2520is%2520then%2520trained%2520on%2520these%2520discrete%2520latents%252C%2520effectively%2520converting%2520graph%2520generation%2520into%2520a%2520well-structured%2520sequence%2520modeling%2520problem.%2520Crucially%252C%2520this%2520mapping%2520of%2520complex%2520graphs%2520to%2520high-fidelity%2520discrete%2520sequences%2520bridges%2520molecular%2520design%2520with%2520the%2520powerful%2520paradigm%2520of%2520large-scale%2520sequence%2520modeling%252C%2520unlocking%2520potential%2520synergies%2520with%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520Extensive%2520experiments%2520show%2520that%2520GVT%2520achieves%2520state-of-the-art%2520or%2520highly%2520competitive%2520performance%2520across%2520major%2520benchmarks%2520like%2520ZINC250k%252C%2520MOSES%252C%2520and%2520GuacaMol%252C%2520and%2520notably%2520outperforms%2520leading%2520diffusion%2520models%2520on%2520key%2520distribution%2520similarity%2520metrics%2520such%2520as%2520FCD%2520and%2520KL%2520Divergence.%2520With%2520its%2520superior%2520performance%252C%2520efficiency%252C%2520and%2520architectural%2520novelty%252C%2520GVT%2520not%2520only%2520presents%2520a%2520compelling%2520alternative%2520to%2520diffusion%2520models%2520but%2520also%2520establishes%2520a%2520strong%2520new%2520baseline%2520for%2520the%2520field%252C%2520paving%2520the%2520way%2520for%2520future%2520research%2520in%2520discrete%2520latent-space%2520molecular%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02667v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20VQ-Transformer%20%28GVT%29%3A%20Fast%20and%20Accurate%20Molecular%20Generation%20via%20High-Fidelity%20Discrete%20Latents&entry.906535625=Haozhuo%20Zheng%20and%20Cheng%20Wang%20and%20Yang%20Liu&entry.1292438233=The%20de%20novo%20generation%20of%20molecules%20with%20desirable%20properties%20is%20a%20critical%20challenge%2C%20where%20diffusion%20models%20are%20computationally%20intensive%20and%20autoregressive%20models%20struggle%20with%20error%20propagation.%20In%20this%20work%2C%20we%20introduce%20the%20Graph%20VQ-Transformer%20%28GVT%29%2C%20a%20two-stage%20generative%20framework%20that%20achieves%20both%20high%20accuracy%20and%20efficiency.%20The%20core%20of%20our%20approach%20is%20a%20novel%20Graph%20Vector%20Quantized%20Variational%20Autoencoder%20%28VQ-VAE%29%20that%20compresses%20molecular%20graphs%20into%20high-fidelity%20discrete%20latent%20sequences.%20By%20synergistically%20combining%20a%20Graph%20Transformer%20with%20canonical%20Reverse%20Cuthill-McKee%20%28RCM%29%20node%20ordering%20and%20Rotary%20Positional%20Embeddings%20%28RoPE%29%2C%20our%20VQ-VAE%20achieves%20near-perfect%20reconstruction%20rates.%20An%20autoregressive%20Transformer%20is%20then%20trained%20on%20these%20discrete%20latents%2C%20effectively%20converting%20graph%20generation%20into%20a%20well-structured%20sequence%20modeling%20problem.%20Crucially%2C%20this%20mapping%20of%20complex%20graphs%20to%20high-fidelity%20discrete%20sequences%20bridges%20molecular%20design%20with%20the%20powerful%20paradigm%20of%20large-scale%20sequence%20modeling%2C%20unlocking%20potential%20synergies%20with%20Large%20Language%20Models%20%28LLMs%29.%20Extensive%20experiments%20show%20that%20GVT%20achieves%20state-of-the-art%20or%20highly%20competitive%20performance%20across%20major%20benchmarks%20like%20ZINC250k%2C%20MOSES%2C%20and%20GuacaMol%2C%20and%20notably%20outperforms%20leading%20diffusion%20models%20on%20key%20distribution%20similarity%20metrics%20such%20as%20FCD%20and%20KL%20Divergence.%20With%20its%20superior%20performance%2C%20efficiency%2C%20and%20architectural%20novelty%2C%20GVT%20not%20only%20presents%20a%20compelling%20alternative%20to%20diffusion%20models%20but%20also%20establishes%20a%20strong%20new%20baseline%20for%20the%20field%2C%20paving%20the%20way%20for%20future%20research%20in%20discrete%20latent-space%20molecular%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2512.02667v1&entry.124074799=Read"},
{"title": "BioArc: Discovering Optimal Neural Architectures for Biological Foundation Models", "author": "Yi Fang and Haoran Xu and Jiaxin Han and Sirui Ding and Yizhi Wang and Yue Wang and Xuan Wang", "abstract": "Foundation models have revolutionized various fields such as natural language processing (NLP) and computer vision (CV). While efforts have been made to transfer the success of the foundation models in general AI domains to biology, existing works focus on directly adopting the existing foundation model architectures from general machine learning domains without a systematic design considering the unique physicochemical and structural properties of each biological data modality. This leads to suboptimal performance, as these repurposed architectures struggle to capture the long-range dependencies, sparse information, and complex underlying ``grammars'' inherent to biological data. To address this gap, we introduce BioArc, a novel framework designed to move beyond intuition-driven architecture design towards principled, automated architecture discovery for biological foundation models. Leveraging Neural Architecture Search (NAS), BioArc systematically explores a vast architecture design space, evaluating architectures across multiple biological modalities while rigorously analyzing the interplay between architecture, tokenization, and training strategies. This large-scale analysis identifies novel, high-performance architectures, allowing us to distill a set of empirical design principles to guide future model development. Furthermore, to make the best of this set of discovered principled architectures, we propose and compare several architecture prediction methods that effectively and efficiently predict optimal architectures for new biological tasks. Overall, our work provides a foundational resource and a principled methodology to guide the creation of the next generation of task-specific and foundation models for biology.", "link": "http://arxiv.org/abs/2512.00283v2", "date": "2025-12-02", "relevancy": 2.2201, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5661}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5661}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BioArc%3A%20Discovering%20Optimal%20Neural%20Architectures%20for%20Biological%20Foundation%20Models&body=Title%3A%20BioArc%3A%20Discovering%20Optimal%20Neural%20Architectures%20for%20Biological%20Foundation%20Models%0AAuthor%3A%20Yi%20Fang%20and%20Haoran%20Xu%20and%20Jiaxin%20Han%20and%20Sirui%20Ding%20and%20Yizhi%20Wang%20and%20Yue%20Wang%20and%20Xuan%20Wang%0AAbstract%3A%20Foundation%20models%20have%20revolutionized%20various%20fields%20such%20as%20natural%20language%20processing%20%28NLP%29%20and%20computer%20vision%20%28CV%29.%20While%20efforts%20have%20been%20made%20to%20transfer%20the%20success%20of%20the%20foundation%20models%20in%20general%20AI%20domains%20to%20biology%2C%20existing%20works%20focus%20on%20directly%20adopting%20the%20existing%20foundation%20model%20architectures%20from%20general%20machine%20learning%20domains%20without%20a%20systematic%20design%20considering%20the%20unique%20physicochemical%20and%20structural%20properties%20of%20each%20biological%20data%20modality.%20This%20leads%20to%20suboptimal%20performance%2C%20as%20these%20repurposed%20architectures%20struggle%20to%20capture%20the%20long-range%20dependencies%2C%20sparse%20information%2C%20and%20complex%20underlying%20%60%60grammars%27%27%20inherent%20to%20biological%20data.%20To%20address%20this%20gap%2C%20we%20introduce%20BioArc%2C%20a%20novel%20framework%20designed%20to%20move%20beyond%20intuition-driven%20architecture%20design%20towards%20principled%2C%20automated%20architecture%20discovery%20for%20biological%20foundation%20models.%20Leveraging%20Neural%20Architecture%20Search%20%28NAS%29%2C%20BioArc%20systematically%20explores%20a%20vast%20architecture%20design%20space%2C%20evaluating%20architectures%20across%20multiple%20biological%20modalities%20while%20rigorously%20analyzing%20the%20interplay%20between%20architecture%2C%20tokenization%2C%20and%20training%20strategies.%20This%20large-scale%20analysis%20identifies%20novel%2C%20high-performance%20architectures%2C%20allowing%20us%20to%20distill%20a%20set%20of%20empirical%20design%20principles%20to%20guide%20future%20model%20development.%20Furthermore%2C%20to%20make%20the%20best%20of%20this%20set%20of%20discovered%20principled%20architectures%2C%20we%20propose%20and%20compare%20several%20architecture%20prediction%20methods%20that%20effectively%20and%20efficiently%20predict%20optimal%20architectures%20for%20new%20biological%20tasks.%20Overall%2C%20our%20work%20provides%20a%20foundational%20resource%20and%20a%20principled%20methodology%20to%20guide%20the%20creation%20of%20the%20next%20generation%20of%20task-specific%20and%20foundation%20models%20for%20biology.%0ALink%3A%20http%3A//arxiv.org/abs/2512.00283v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBioArc%253A%2520Discovering%2520Optimal%2520Neural%2520Architectures%2520for%2520Biological%2520Foundation%2520Models%26entry.906535625%3DYi%2520Fang%2520and%2520Haoran%2520Xu%2520and%2520Jiaxin%2520Han%2520and%2520Sirui%2520Ding%2520and%2520Yizhi%2520Wang%2520and%2520Yue%2520Wang%2520and%2520Xuan%2520Wang%26entry.1292438233%3DFoundation%2520models%2520have%2520revolutionized%2520various%2520fields%2520such%2520as%2520natural%2520language%2520processing%2520%2528NLP%2529%2520and%2520computer%2520vision%2520%2528CV%2529.%2520While%2520efforts%2520have%2520been%2520made%2520to%2520transfer%2520the%2520success%2520of%2520the%2520foundation%2520models%2520in%2520general%2520AI%2520domains%2520to%2520biology%252C%2520existing%2520works%2520focus%2520on%2520directly%2520adopting%2520the%2520existing%2520foundation%2520model%2520architectures%2520from%2520general%2520machine%2520learning%2520domains%2520without%2520a%2520systematic%2520design%2520considering%2520the%2520unique%2520physicochemical%2520and%2520structural%2520properties%2520of%2520each%2520biological%2520data%2520modality.%2520This%2520leads%2520to%2520suboptimal%2520performance%252C%2520as%2520these%2520repurposed%2520architectures%2520struggle%2520to%2520capture%2520the%2520long-range%2520dependencies%252C%2520sparse%2520information%252C%2520and%2520complex%2520underlying%2520%2560%2560grammars%2527%2527%2520inherent%2520to%2520biological%2520data.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520BioArc%252C%2520a%2520novel%2520framework%2520designed%2520to%2520move%2520beyond%2520intuition-driven%2520architecture%2520design%2520towards%2520principled%252C%2520automated%2520architecture%2520discovery%2520for%2520biological%2520foundation%2520models.%2520Leveraging%2520Neural%2520Architecture%2520Search%2520%2528NAS%2529%252C%2520BioArc%2520systematically%2520explores%2520a%2520vast%2520architecture%2520design%2520space%252C%2520evaluating%2520architectures%2520across%2520multiple%2520biological%2520modalities%2520while%2520rigorously%2520analyzing%2520the%2520interplay%2520between%2520architecture%252C%2520tokenization%252C%2520and%2520training%2520strategies.%2520This%2520large-scale%2520analysis%2520identifies%2520novel%252C%2520high-performance%2520architectures%252C%2520allowing%2520us%2520to%2520distill%2520a%2520set%2520of%2520empirical%2520design%2520principles%2520to%2520guide%2520future%2520model%2520development.%2520Furthermore%252C%2520to%2520make%2520the%2520best%2520of%2520this%2520set%2520of%2520discovered%2520principled%2520architectures%252C%2520we%2520propose%2520and%2520compare%2520several%2520architecture%2520prediction%2520methods%2520that%2520effectively%2520and%2520efficiently%2520predict%2520optimal%2520architectures%2520for%2520new%2520biological%2520tasks.%2520Overall%252C%2520our%2520work%2520provides%2520a%2520foundational%2520resource%2520and%2520a%2520principled%2520methodology%2520to%2520guide%2520the%2520creation%2520of%2520the%2520next%2520generation%2520of%2520task-specific%2520and%2520foundation%2520models%2520for%2520biology.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.00283v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BioArc%3A%20Discovering%20Optimal%20Neural%20Architectures%20for%20Biological%20Foundation%20Models&entry.906535625=Yi%20Fang%20and%20Haoran%20Xu%20and%20Jiaxin%20Han%20and%20Sirui%20Ding%20and%20Yizhi%20Wang%20and%20Yue%20Wang%20and%20Xuan%20Wang&entry.1292438233=Foundation%20models%20have%20revolutionized%20various%20fields%20such%20as%20natural%20language%20processing%20%28NLP%29%20and%20computer%20vision%20%28CV%29.%20While%20efforts%20have%20been%20made%20to%20transfer%20the%20success%20of%20the%20foundation%20models%20in%20general%20AI%20domains%20to%20biology%2C%20existing%20works%20focus%20on%20directly%20adopting%20the%20existing%20foundation%20model%20architectures%20from%20general%20machine%20learning%20domains%20without%20a%20systematic%20design%20considering%20the%20unique%20physicochemical%20and%20structural%20properties%20of%20each%20biological%20data%20modality.%20This%20leads%20to%20suboptimal%20performance%2C%20as%20these%20repurposed%20architectures%20struggle%20to%20capture%20the%20long-range%20dependencies%2C%20sparse%20information%2C%20and%20complex%20underlying%20%60%60grammars%27%27%20inherent%20to%20biological%20data.%20To%20address%20this%20gap%2C%20we%20introduce%20BioArc%2C%20a%20novel%20framework%20designed%20to%20move%20beyond%20intuition-driven%20architecture%20design%20towards%20principled%2C%20automated%20architecture%20discovery%20for%20biological%20foundation%20models.%20Leveraging%20Neural%20Architecture%20Search%20%28NAS%29%2C%20BioArc%20systematically%20explores%20a%20vast%20architecture%20design%20space%2C%20evaluating%20architectures%20across%20multiple%20biological%20modalities%20while%20rigorously%20analyzing%20the%20interplay%20between%20architecture%2C%20tokenization%2C%20and%20training%20strategies.%20This%20large-scale%20analysis%20identifies%20novel%2C%20high-performance%20architectures%2C%20allowing%20us%20to%20distill%20a%20set%20of%20empirical%20design%20principles%20to%20guide%20future%20model%20development.%20Furthermore%2C%20to%20make%20the%20best%20of%20this%20set%20of%20discovered%20principled%20architectures%2C%20we%20propose%20and%20compare%20several%20architecture%20prediction%20methods%20that%20effectively%20and%20efficiently%20predict%20optimal%20architectures%20for%20new%20biological%20tasks.%20Overall%2C%20our%20work%20provides%20a%20foundational%20resource%20and%20a%20principled%20methodology%20to%20guide%20the%20creation%20of%20the%20next%20generation%20of%20task-specific%20and%20foundation%20models%20for%20biology.&entry.1838667208=http%3A//arxiv.org/abs/2512.00283v2&entry.124074799=Read"},
{"title": "A Lightweight Real-Time Low-Light Enhancement Network for Embedded Automotive Vision Systems", "author": "Yuhan Chen and Yicui Shi and Guofa Li and Guangrui Bai and Jinyuan Shao and Xiangfei Huang and Wenbo Chu and Keqiang Li", "abstract": "In low-light environments like nighttime driving, image degradation severely challenges in-vehicle camera safety. Since existing enhancement algorithms are often too computationally intensive for vehicular applications, we propose UltraFast-LieNET, a lightweight multi-scale shifted convolutional network for real-time low-light image enhancement. We introduce a Dynamic Shifted Convolution (DSConv) kernel with only 12 learnable parameters for efficient feature extraction. By integrating DSConv with varying shift distances, a Multi-scale Shifted Residual Block (MSRB) is constructed to significantly expand the receptive field. To mitigate lightweight network instability, a residual structure and a novel multi-level gradient-aware loss function are incorporated. UltraFast-LieNET allows flexible parameter configuration, with a minimum size of only 36 parameters. Results on the LOLI-Street dataset show a PSNR of 26.51 dB, outperforming state-of-the-art methods by 4.6 dB while utilizing only 180 parameters. Experiments across four benchmark datasets validate its superior balance of real-time performance and enhancement quality under limited resources. Code is available at https://githubhttps://github.com/YuhanChen2024/UltraFast-LiNET", "link": "http://arxiv.org/abs/2512.02965v1", "date": "2025-12-02", "relevancy": 2.2186, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.565}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5598}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Lightweight%20Real-Time%20Low-Light%20Enhancement%20Network%20for%20Embedded%20Automotive%20Vision%20Systems&body=Title%3A%20A%20Lightweight%20Real-Time%20Low-Light%20Enhancement%20Network%20for%20Embedded%20Automotive%20Vision%20Systems%0AAuthor%3A%20Yuhan%20Chen%20and%20Yicui%20Shi%20and%20Guofa%20Li%20and%20Guangrui%20Bai%20and%20Jinyuan%20Shao%20and%20Xiangfei%20Huang%20and%20Wenbo%20Chu%20and%20Keqiang%20Li%0AAbstract%3A%20In%20low-light%20environments%20like%20nighttime%20driving%2C%20image%20degradation%20severely%20challenges%20in-vehicle%20camera%20safety.%20Since%20existing%20enhancement%20algorithms%20are%20often%20too%20computationally%20intensive%20for%20vehicular%20applications%2C%20we%20propose%20UltraFast-LieNET%2C%20a%20lightweight%20multi-scale%20shifted%20convolutional%20network%20for%20real-time%20low-light%20image%20enhancement.%20We%20introduce%20a%20Dynamic%20Shifted%20Convolution%20%28DSConv%29%20kernel%20with%20only%2012%20learnable%20parameters%20for%20efficient%20feature%20extraction.%20By%20integrating%20DSConv%20with%20varying%20shift%20distances%2C%20a%20Multi-scale%20Shifted%20Residual%20Block%20%28MSRB%29%20is%20constructed%20to%20significantly%20expand%20the%20receptive%20field.%20To%20mitigate%20lightweight%20network%20instability%2C%20a%20residual%20structure%20and%20a%20novel%20multi-level%20gradient-aware%20loss%20function%20are%20incorporated.%20UltraFast-LieNET%20allows%20flexible%20parameter%20configuration%2C%20with%20a%20minimum%20size%20of%20only%2036%20parameters.%20Results%20on%20the%20LOLI-Street%20dataset%20show%20a%20PSNR%20of%2026.51%20dB%2C%20outperforming%20state-of-the-art%20methods%20by%204.6%20dB%20while%20utilizing%20only%20180%20parameters.%20Experiments%20across%20four%20benchmark%20datasets%20validate%20its%20superior%20balance%20of%20real-time%20performance%20and%20enhancement%20quality%20under%20limited%20resources.%20Code%20is%20available%20at%20https%3A//githubhttps%3A//github.com/YuhanChen2024/UltraFast-LiNET%0ALink%3A%20http%3A//arxiv.org/abs/2512.02965v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Lightweight%2520Real-Time%2520Low-Light%2520Enhancement%2520Network%2520for%2520Embedded%2520Automotive%2520Vision%2520Systems%26entry.906535625%3DYuhan%2520Chen%2520and%2520Yicui%2520Shi%2520and%2520Guofa%2520Li%2520and%2520Guangrui%2520Bai%2520and%2520Jinyuan%2520Shao%2520and%2520Xiangfei%2520Huang%2520and%2520Wenbo%2520Chu%2520and%2520Keqiang%2520Li%26entry.1292438233%3DIn%2520low-light%2520environments%2520like%2520nighttime%2520driving%252C%2520image%2520degradation%2520severely%2520challenges%2520in-vehicle%2520camera%2520safety.%2520Since%2520existing%2520enhancement%2520algorithms%2520are%2520often%2520too%2520computationally%2520intensive%2520for%2520vehicular%2520applications%252C%2520we%2520propose%2520UltraFast-LieNET%252C%2520a%2520lightweight%2520multi-scale%2520shifted%2520convolutional%2520network%2520for%2520real-time%2520low-light%2520image%2520enhancement.%2520We%2520introduce%2520a%2520Dynamic%2520Shifted%2520Convolution%2520%2528DSConv%2529%2520kernel%2520with%2520only%252012%2520learnable%2520parameters%2520for%2520efficient%2520feature%2520extraction.%2520By%2520integrating%2520DSConv%2520with%2520varying%2520shift%2520distances%252C%2520a%2520Multi-scale%2520Shifted%2520Residual%2520Block%2520%2528MSRB%2529%2520is%2520constructed%2520to%2520significantly%2520expand%2520the%2520receptive%2520field.%2520To%2520mitigate%2520lightweight%2520network%2520instability%252C%2520a%2520residual%2520structure%2520and%2520a%2520novel%2520multi-level%2520gradient-aware%2520loss%2520function%2520are%2520incorporated.%2520UltraFast-LieNET%2520allows%2520flexible%2520parameter%2520configuration%252C%2520with%2520a%2520minimum%2520size%2520of%2520only%252036%2520parameters.%2520Results%2520on%2520the%2520LOLI-Street%2520dataset%2520show%2520a%2520PSNR%2520of%252026.51%2520dB%252C%2520outperforming%2520state-of-the-art%2520methods%2520by%25204.6%2520dB%2520while%2520utilizing%2520only%2520180%2520parameters.%2520Experiments%2520across%2520four%2520benchmark%2520datasets%2520validate%2520its%2520superior%2520balance%2520of%2520real-time%2520performance%2520and%2520enhancement%2520quality%2520under%2520limited%2520resources.%2520Code%2520is%2520available%2520at%2520https%253A//githubhttps%253A//github.com/YuhanChen2024/UltraFast-LiNET%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02965v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Lightweight%20Real-Time%20Low-Light%20Enhancement%20Network%20for%20Embedded%20Automotive%20Vision%20Systems&entry.906535625=Yuhan%20Chen%20and%20Yicui%20Shi%20and%20Guofa%20Li%20and%20Guangrui%20Bai%20and%20Jinyuan%20Shao%20and%20Xiangfei%20Huang%20and%20Wenbo%20Chu%20and%20Keqiang%20Li&entry.1292438233=In%20low-light%20environments%20like%20nighttime%20driving%2C%20image%20degradation%20severely%20challenges%20in-vehicle%20camera%20safety.%20Since%20existing%20enhancement%20algorithms%20are%20often%20too%20computationally%20intensive%20for%20vehicular%20applications%2C%20we%20propose%20UltraFast-LieNET%2C%20a%20lightweight%20multi-scale%20shifted%20convolutional%20network%20for%20real-time%20low-light%20image%20enhancement.%20We%20introduce%20a%20Dynamic%20Shifted%20Convolution%20%28DSConv%29%20kernel%20with%20only%2012%20learnable%20parameters%20for%20efficient%20feature%20extraction.%20By%20integrating%20DSConv%20with%20varying%20shift%20distances%2C%20a%20Multi-scale%20Shifted%20Residual%20Block%20%28MSRB%29%20is%20constructed%20to%20significantly%20expand%20the%20receptive%20field.%20To%20mitigate%20lightweight%20network%20instability%2C%20a%20residual%20structure%20and%20a%20novel%20multi-level%20gradient-aware%20loss%20function%20are%20incorporated.%20UltraFast-LieNET%20allows%20flexible%20parameter%20configuration%2C%20with%20a%20minimum%20size%20of%20only%2036%20parameters.%20Results%20on%20the%20LOLI-Street%20dataset%20show%20a%20PSNR%20of%2026.51%20dB%2C%20outperforming%20state-of-the-art%20methods%20by%204.6%20dB%20while%20utilizing%20only%20180%20parameters.%20Experiments%20across%20four%20benchmark%20datasets%20validate%20its%20superior%20balance%20of%20real-time%20performance%20and%20enhancement%20quality%20under%20limited%20resources.%20Code%20is%20available%20at%20https%3A//githubhttps%3A//github.com/YuhanChen2024/UltraFast-LiNET&entry.1838667208=http%3A//arxiv.org/abs/2512.02965v1&entry.124074799=Read"},
{"title": "Agentic UAVs: LLM-Driven Autonomy with Integrated Tool-Calling and Cognitive Reasoning", "author": "Anis Koubaa and Khaled Gabr", "abstract": "Unmanned Aerial Vehicles (UAVs) are increasingly used in defense, surveillance, and disaster response, yet most systems still operate at SAE Level 2 to 3 autonomy. Their dependence on rule-based control and narrow AI limits adaptability in dynamic and uncertain missions. Current UAV architectures lack context-aware reasoning, autonomous decision-making, and integration with external systems. Importantly, none make use of Large Language Model (LLM) agents with tool-calling for real-time knowledge access.\n  This paper introduces the Agentic UAVs framework, a five-layer architecture consisting of Perception, Reasoning, Action, Integration, and Learning. The framework enhances UAV autonomy through LLM-driven reasoning, database querying, and interaction with third-party systems.\n  A prototype built with ROS 2 and Gazebo combines YOLOv11 for object detection with GPT-4 for reasoning and a locally deployed Gemma 3 model. In simulated search-and-rescue scenarios, agentic UAVs achieved higher detection confidence (0.79 compared to 0.72), improved person detection rates (91% compared to 75%), and a major increase in correct action recommendations (92% compared to 4.5%). These results show that modest computational overhead can enable significantly higher levels of autonomy and system-level integration.", "link": "http://arxiv.org/abs/2509.13352v2", "date": "2025-12-02", "relevancy": 2.218, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5688}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5552}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agentic%20UAVs%3A%20LLM-Driven%20Autonomy%20with%20Integrated%20Tool-Calling%20and%20Cognitive%20Reasoning&body=Title%3A%20Agentic%20UAVs%3A%20LLM-Driven%20Autonomy%20with%20Integrated%20Tool-Calling%20and%20Cognitive%20Reasoning%0AAuthor%3A%20Anis%20Koubaa%20and%20Khaled%20Gabr%0AAbstract%3A%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20are%20increasingly%20used%20in%20defense%2C%20surveillance%2C%20and%20disaster%20response%2C%20yet%20most%20systems%20still%20operate%20at%20SAE%20Level%202%20to%203%20autonomy.%20Their%20dependence%20on%20rule-based%20control%20and%20narrow%20AI%20limits%20adaptability%20in%20dynamic%20and%20uncertain%20missions.%20Current%20UAV%20architectures%20lack%20context-aware%20reasoning%2C%20autonomous%20decision-making%2C%20and%20integration%20with%20external%20systems.%20Importantly%2C%20none%20make%20use%20of%20Large%20Language%20Model%20%28LLM%29%20agents%20with%20tool-calling%20for%20real-time%20knowledge%20access.%0A%20%20This%20paper%20introduces%20the%20Agentic%20UAVs%20framework%2C%20a%20five-layer%20architecture%20consisting%20of%20Perception%2C%20Reasoning%2C%20Action%2C%20Integration%2C%20and%20Learning.%20The%20framework%20enhances%20UAV%20autonomy%20through%20LLM-driven%20reasoning%2C%20database%20querying%2C%20and%20interaction%20with%20third-party%20systems.%0A%20%20A%20prototype%20built%20with%20ROS%202%20and%20Gazebo%20combines%20YOLOv11%20for%20object%20detection%20with%20GPT-4%20for%20reasoning%20and%20a%20locally%20deployed%20Gemma%203%20model.%20In%20simulated%20search-and-rescue%20scenarios%2C%20agentic%20UAVs%20achieved%20higher%20detection%20confidence%20%280.79%20compared%20to%200.72%29%2C%20improved%20person%20detection%20rates%20%2891%25%20compared%20to%2075%25%29%2C%20and%20a%20major%20increase%20in%20correct%20action%20recommendations%20%2892%25%20compared%20to%204.5%25%29.%20These%20results%20show%20that%20modest%20computational%20overhead%20can%20enable%20significantly%20higher%20levels%20of%20autonomy%20and%20system-level%20integration.%0ALink%3A%20http%3A//arxiv.org/abs/2509.13352v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentic%2520UAVs%253A%2520LLM-Driven%2520Autonomy%2520with%2520Integrated%2520Tool-Calling%2520and%2520Cognitive%2520Reasoning%26entry.906535625%3DAnis%2520Koubaa%2520and%2520Khaled%2520Gabr%26entry.1292438233%3DUnmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529%2520are%2520increasingly%2520used%2520in%2520defense%252C%2520surveillance%252C%2520and%2520disaster%2520response%252C%2520yet%2520most%2520systems%2520still%2520operate%2520at%2520SAE%2520Level%25202%2520to%25203%2520autonomy.%2520Their%2520dependence%2520on%2520rule-based%2520control%2520and%2520narrow%2520AI%2520limits%2520adaptability%2520in%2520dynamic%2520and%2520uncertain%2520missions.%2520Current%2520UAV%2520architectures%2520lack%2520context-aware%2520reasoning%252C%2520autonomous%2520decision-making%252C%2520and%2520integration%2520with%2520external%2520systems.%2520Importantly%252C%2520none%2520make%2520use%2520of%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520agents%2520with%2520tool-calling%2520for%2520real-time%2520knowledge%2520access.%250A%2520%2520This%2520paper%2520introduces%2520the%2520Agentic%2520UAVs%2520framework%252C%2520a%2520five-layer%2520architecture%2520consisting%2520of%2520Perception%252C%2520Reasoning%252C%2520Action%252C%2520Integration%252C%2520and%2520Learning.%2520The%2520framework%2520enhances%2520UAV%2520autonomy%2520through%2520LLM-driven%2520reasoning%252C%2520database%2520querying%252C%2520and%2520interaction%2520with%2520third-party%2520systems.%250A%2520%2520A%2520prototype%2520built%2520with%2520ROS%25202%2520and%2520Gazebo%2520combines%2520YOLOv11%2520for%2520object%2520detection%2520with%2520GPT-4%2520for%2520reasoning%2520and%2520a%2520locally%2520deployed%2520Gemma%25203%2520model.%2520In%2520simulated%2520search-and-rescue%2520scenarios%252C%2520agentic%2520UAVs%2520achieved%2520higher%2520detection%2520confidence%2520%25280.79%2520compared%2520to%25200.72%2529%252C%2520improved%2520person%2520detection%2520rates%2520%252891%2525%2520compared%2520to%252075%2525%2529%252C%2520and%2520a%2520major%2520increase%2520in%2520correct%2520action%2520recommendations%2520%252892%2525%2520compared%2520to%25204.5%2525%2529.%2520These%2520results%2520show%2520that%2520modest%2520computational%2520overhead%2520can%2520enable%2520significantly%2520higher%2520levels%2520of%2520autonomy%2520and%2520system-level%2520integration.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13352v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agentic%20UAVs%3A%20LLM-Driven%20Autonomy%20with%20Integrated%20Tool-Calling%20and%20Cognitive%20Reasoning&entry.906535625=Anis%20Koubaa%20and%20Khaled%20Gabr&entry.1292438233=Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20are%20increasingly%20used%20in%20defense%2C%20surveillance%2C%20and%20disaster%20response%2C%20yet%20most%20systems%20still%20operate%20at%20SAE%20Level%202%20to%203%20autonomy.%20Their%20dependence%20on%20rule-based%20control%20and%20narrow%20AI%20limits%20adaptability%20in%20dynamic%20and%20uncertain%20missions.%20Current%20UAV%20architectures%20lack%20context-aware%20reasoning%2C%20autonomous%20decision-making%2C%20and%20integration%20with%20external%20systems.%20Importantly%2C%20none%20make%20use%20of%20Large%20Language%20Model%20%28LLM%29%20agents%20with%20tool-calling%20for%20real-time%20knowledge%20access.%0A%20%20This%20paper%20introduces%20the%20Agentic%20UAVs%20framework%2C%20a%20five-layer%20architecture%20consisting%20of%20Perception%2C%20Reasoning%2C%20Action%2C%20Integration%2C%20and%20Learning.%20The%20framework%20enhances%20UAV%20autonomy%20through%20LLM-driven%20reasoning%2C%20database%20querying%2C%20and%20interaction%20with%20third-party%20systems.%0A%20%20A%20prototype%20built%20with%20ROS%202%20and%20Gazebo%20combines%20YOLOv11%20for%20object%20detection%20with%20GPT-4%20for%20reasoning%20and%20a%20locally%20deployed%20Gemma%203%20model.%20In%20simulated%20search-and-rescue%20scenarios%2C%20agentic%20UAVs%20achieved%20higher%20detection%20confidence%20%280.79%20compared%20to%200.72%29%2C%20improved%20person%20detection%20rates%20%2891%25%20compared%20to%2075%25%29%2C%20and%20a%20major%20increase%20in%20correct%20action%20recommendations%20%2892%25%20compared%20to%204.5%25%29.%20These%20results%20show%20that%20modest%20computational%20overhead%20can%20enable%20significantly%20higher%20levels%20of%20autonomy%20and%20system-level%20integration.&entry.1838667208=http%3A//arxiv.org/abs/2509.13352v2&entry.124074799=Read"},
{"title": "ReVSeg: Incentivizing the Reasoning Chain for Video Segmentation with Reinforcement Learning", "author": "Yifan Li and Yingda Yin and Lingting Zhu and Weikai Chen and Shengju Qian and Xin Wang and Yanwei Fu", "abstract": "Reasoning-centric video object segmentation is an inherently complex task: the query often refers to dynamics, causality, and temporal interactions, rather than static appearances. Yet existing solutions generally collapse these factors into simplified reasoning with latent embeddings, rendering the reasoning chain opaque and essentially intractable. We therefore adopt an explicit decomposition perspective and introduce ReVSeg, which executes reasoning as sequential decisions in the native interface of pretrained vision language models (VLMs). Rather than folding all reasoning into a single-step prediction, ReVSeg executes three explicit operations -- semantics interpretation, temporal evidence selection, and spatial grounding -- aligning pretrained capabilities. We further employ reinforcement learning to optimize the multi-step reasoning chain, enabling the model to self-refine its decision quality from outcome-driven signals. Experimental results demonstrate that ReVSeg attains state-of-the-art performances on standard video object segmentation benchmarks and yields interpretable reasoning trajectories. Project page is available at https://clementine24.github.io/ReVSeg/ .", "link": "http://arxiv.org/abs/2512.02835v1", "date": "2025-12-02", "relevancy": 2.2167, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5591}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5591}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5295}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReVSeg%3A%20Incentivizing%20the%20Reasoning%20Chain%20for%20Video%20Segmentation%20with%20Reinforcement%20Learning&body=Title%3A%20ReVSeg%3A%20Incentivizing%20the%20Reasoning%20Chain%20for%20Video%20Segmentation%20with%20Reinforcement%20Learning%0AAuthor%3A%20Yifan%20Li%20and%20Yingda%20Yin%20and%20Lingting%20Zhu%20and%20Weikai%20Chen%20and%20Shengju%20Qian%20and%20Xin%20Wang%20and%20Yanwei%20Fu%0AAbstract%3A%20Reasoning-centric%20video%20object%20segmentation%20is%20an%20inherently%20complex%20task%3A%20the%20query%20often%20refers%20to%20dynamics%2C%20causality%2C%20and%20temporal%20interactions%2C%20rather%20than%20static%20appearances.%20Yet%20existing%20solutions%20generally%20collapse%20these%20factors%20into%20simplified%20reasoning%20with%20latent%20embeddings%2C%20rendering%20the%20reasoning%20chain%20opaque%20and%20essentially%20intractable.%20We%20therefore%20adopt%20an%20explicit%20decomposition%20perspective%20and%20introduce%20ReVSeg%2C%20which%20executes%20reasoning%20as%20sequential%20decisions%20in%20the%20native%20interface%20of%20pretrained%20vision%20language%20models%20%28VLMs%29.%20Rather%20than%20folding%20all%20reasoning%20into%20a%20single-step%20prediction%2C%20ReVSeg%20executes%20three%20explicit%20operations%20--%20semantics%20interpretation%2C%20temporal%20evidence%20selection%2C%20and%20spatial%20grounding%20--%20aligning%20pretrained%20capabilities.%20We%20further%20employ%20reinforcement%20learning%20to%20optimize%20the%20multi-step%20reasoning%20chain%2C%20enabling%20the%20model%20to%20self-refine%20its%20decision%20quality%20from%20outcome-driven%20signals.%20Experimental%20results%20demonstrate%20that%20ReVSeg%20attains%20state-of-the-art%20performances%20on%20standard%20video%20object%20segmentation%20benchmarks%20and%20yields%20interpretable%20reasoning%20trajectories.%20Project%20page%20is%20available%20at%20https%3A//clementine24.github.io/ReVSeg/%20.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02835v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReVSeg%253A%2520Incentivizing%2520the%2520Reasoning%2520Chain%2520for%2520Video%2520Segmentation%2520with%2520Reinforcement%2520Learning%26entry.906535625%3DYifan%2520Li%2520and%2520Yingda%2520Yin%2520and%2520Lingting%2520Zhu%2520and%2520Weikai%2520Chen%2520and%2520Shengju%2520Qian%2520and%2520Xin%2520Wang%2520and%2520Yanwei%2520Fu%26entry.1292438233%3DReasoning-centric%2520video%2520object%2520segmentation%2520is%2520an%2520inherently%2520complex%2520task%253A%2520the%2520query%2520often%2520refers%2520to%2520dynamics%252C%2520causality%252C%2520and%2520temporal%2520interactions%252C%2520rather%2520than%2520static%2520appearances.%2520Yet%2520existing%2520solutions%2520generally%2520collapse%2520these%2520factors%2520into%2520simplified%2520reasoning%2520with%2520latent%2520embeddings%252C%2520rendering%2520the%2520reasoning%2520chain%2520opaque%2520and%2520essentially%2520intractable.%2520We%2520therefore%2520adopt%2520an%2520explicit%2520decomposition%2520perspective%2520and%2520introduce%2520ReVSeg%252C%2520which%2520executes%2520reasoning%2520as%2520sequential%2520decisions%2520in%2520the%2520native%2520interface%2520of%2520pretrained%2520vision%2520language%2520models%2520%2528VLMs%2529.%2520Rather%2520than%2520folding%2520all%2520reasoning%2520into%2520a%2520single-step%2520prediction%252C%2520ReVSeg%2520executes%2520three%2520explicit%2520operations%2520--%2520semantics%2520interpretation%252C%2520temporal%2520evidence%2520selection%252C%2520and%2520spatial%2520grounding%2520--%2520aligning%2520pretrained%2520capabilities.%2520We%2520further%2520employ%2520reinforcement%2520learning%2520to%2520optimize%2520the%2520multi-step%2520reasoning%2520chain%252C%2520enabling%2520the%2520model%2520to%2520self-refine%2520its%2520decision%2520quality%2520from%2520outcome-driven%2520signals.%2520Experimental%2520results%2520demonstrate%2520that%2520ReVSeg%2520attains%2520state-of-the-art%2520performances%2520on%2520standard%2520video%2520object%2520segmentation%2520benchmarks%2520and%2520yields%2520interpretable%2520reasoning%2520trajectories.%2520Project%2520page%2520is%2520available%2520at%2520https%253A//clementine24.github.io/ReVSeg/%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02835v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReVSeg%3A%20Incentivizing%20the%20Reasoning%20Chain%20for%20Video%20Segmentation%20with%20Reinforcement%20Learning&entry.906535625=Yifan%20Li%20and%20Yingda%20Yin%20and%20Lingting%20Zhu%20and%20Weikai%20Chen%20and%20Shengju%20Qian%20and%20Xin%20Wang%20and%20Yanwei%20Fu&entry.1292438233=Reasoning-centric%20video%20object%20segmentation%20is%20an%20inherently%20complex%20task%3A%20the%20query%20often%20refers%20to%20dynamics%2C%20causality%2C%20and%20temporal%20interactions%2C%20rather%20than%20static%20appearances.%20Yet%20existing%20solutions%20generally%20collapse%20these%20factors%20into%20simplified%20reasoning%20with%20latent%20embeddings%2C%20rendering%20the%20reasoning%20chain%20opaque%20and%20essentially%20intractable.%20We%20therefore%20adopt%20an%20explicit%20decomposition%20perspective%20and%20introduce%20ReVSeg%2C%20which%20executes%20reasoning%20as%20sequential%20decisions%20in%20the%20native%20interface%20of%20pretrained%20vision%20language%20models%20%28VLMs%29.%20Rather%20than%20folding%20all%20reasoning%20into%20a%20single-step%20prediction%2C%20ReVSeg%20executes%20three%20explicit%20operations%20--%20semantics%20interpretation%2C%20temporal%20evidence%20selection%2C%20and%20spatial%20grounding%20--%20aligning%20pretrained%20capabilities.%20We%20further%20employ%20reinforcement%20learning%20to%20optimize%20the%20multi-step%20reasoning%20chain%2C%20enabling%20the%20model%20to%20self-refine%20its%20decision%20quality%20from%20outcome-driven%20signals.%20Experimental%20results%20demonstrate%20that%20ReVSeg%20attains%20state-of-the-art%20performances%20on%20standard%20video%20object%20segmentation%20benchmarks%20and%20yields%20interpretable%20reasoning%20trajectories.%20Project%20page%20is%20available%20at%20https%3A//clementine24.github.io/ReVSeg/%20.&entry.1838667208=http%3A//arxiv.org/abs/2512.02835v1&entry.124074799=Read"},
{"title": "Emergent Bayesian Behaviour and Optimal Cue Combination in LLMs", "author": "Julian Ma and Jun Wang and Zafeirios Fountas", "abstract": "Large language models (LLMs) excel at explicit reasoning, but their implicit computational strategies remain underexplored. Decades of psychophysics research show that humans intuitively process and integrate noisy signals using near-optimal Bayesian strategies in perceptual tasks. We ask whether LLMs exhibit similar behaviour and perform optimal multimodal integration without explicit training or instruction. Adopting the psychophysics paradigm, we infer computational principles of LLMs from systematic behavioural studies. We introduce a behavioural benchmark - BayesBench: four magnitude estimation tasks (length, location, distance, and duration) over text and image, inspired by classic psychophysics, and evaluate a diverse set of nine LLMs alongside human judgments for calibration. Through controlled ablations of noise, context, and instruction prompts, we measure performance, behaviour and efficiency in multimodal cue-combination. Beyond accuracy and efficiency metrics, we introduce a Bayesian Consistency Score that detects Bayes-consistent behavioural shifts even when accuracy saturates. Our results show that while capable models often adapt in Bayes-consistent ways, accuracy does not guarantee robustness. Notably, GPT-5 Mini achieves perfect text accuracy but fails to integrate visual cues efficiently. This reveals a critical dissociation between capability and strategy, suggesting accuracy-centric benchmarks may over-index on performance while missing brittle uncertainty handling. These findings reveal emergent principled handling of uncertainty and highlight the correlation between accuracy and Bayesian tendencies. We release our psychophysics benchmark and consistency metric (https://bayes-bench.github.io) as evaluation tools and to inform future multimodal architecture designs.", "link": "http://arxiv.org/abs/2512.02719v1", "date": "2025-12-02", "relevancy": 2.2123, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6105}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5416}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emergent%20Bayesian%20Behaviour%20and%20Optimal%20Cue%20Combination%20in%20LLMs&body=Title%3A%20Emergent%20Bayesian%20Behaviour%20and%20Optimal%20Cue%20Combination%20in%20LLMs%0AAuthor%3A%20Julian%20Ma%20and%20Jun%20Wang%20and%20Zafeirios%20Fountas%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20excel%20at%20explicit%20reasoning%2C%20but%20their%20implicit%20computational%20strategies%20remain%20underexplored.%20Decades%20of%20psychophysics%20research%20show%20that%20humans%20intuitively%20process%20and%20integrate%20noisy%20signals%20using%20near-optimal%20Bayesian%20strategies%20in%20perceptual%20tasks.%20We%20ask%20whether%20LLMs%20exhibit%20similar%20behaviour%20and%20perform%20optimal%20multimodal%20integration%20without%20explicit%20training%20or%20instruction.%20Adopting%20the%20psychophysics%20paradigm%2C%20we%20infer%20computational%20principles%20of%20LLMs%20from%20systematic%20behavioural%20studies.%20We%20introduce%20a%20behavioural%20benchmark%20-%20BayesBench%3A%20four%20magnitude%20estimation%20tasks%20%28length%2C%20location%2C%20distance%2C%20and%20duration%29%20over%20text%20and%20image%2C%20inspired%20by%20classic%20psychophysics%2C%20and%20evaluate%20a%20diverse%20set%20of%20nine%20LLMs%20alongside%20human%20judgments%20for%20calibration.%20Through%20controlled%20ablations%20of%20noise%2C%20context%2C%20and%20instruction%20prompts%2C%20we%20measure%20performance%2C%20behaviour%20and%20efficiency%20in%20multimodal%20cue-combination.%20Beyond%20accuracy%20and%20efficiency%20metrics%2C%20we%20introduce%20a%20Bayesian%20Consistency%20Score%20that%20detects%20Bayes-consistent%20behavioural%20shifts%20even%20when%20accuracy%20saturates.%20Our%20results%20show%20that%20while%20capable%20models%20often%20adapt%20in%20Bayes-consistent%20ways%2C%20accuracy%20does%20not%20guarantee%20robustness.%20Notably%2C%20GPT-5%20Mini%20achieves%20perfect%20text%20accuracy%20but%20fails%20to%20integrate%20visual%20cues%20efficiently.%20This%20reveals%20a%20critical%20dissociation%20between%20capability%20and%20strategy%2C%20suggesting%20accuracy-centric%20benchmarks%20may%20over-index%20on%20performance%20while%20missing%20brittle%20uncertainty%20handling.%20These%20findings%20reveal%20emergent%20principled%20handling%20of%20uncertainty%20and%20highlight%20the%20correlation%20between%20accuracy%20and%20Bayesian%20tendencies.%20We%20release%20our%20psychophysics%20benchmark%20and%20consistency%20metric%20%28https%3A//bayes-bench.github.io%29%20as%20evaluation%20tools%20and%20to%20inform%20future%20multimodal%20architecture%20designs.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02719v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmergent%2520Bayesian%2520Behaviour%2520and%2520Optimal%2520Cue%2520Combination%2520in%2520LLMs%26entry.906535625%3DJulian%2520Ma%2520and%2520Jun%2520Wang%2520and%2520Zafeirios%2520Fountas%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520excel%2520at%2520explicit%2520reasoning%252C%2520but%2520their%2520implicit%2520computational%2520strategies%2520remain%2520underexplored.%2520Decades%2520of%2520psychophysics%2520research%2520show%2520that%2520humans%2520intuitively%2520process%2520and%2520integrate%2520noisy%2520signals%2520using%2520near-optimal%2520Bayesian%2520strategies%2520in%2520perceptual%2520tasks.%2520We%2520ask%2520whether%2520LLMs%2520exhibit%2520similar%2520behaviour%2520and%2520perform%2520optimal%2520multimodal%2520integration%2520without%2520explicit%2520training%2520or%2520instruction.%2520Adopting%2520the%2520psychophysics%2520paradigm%252C%2520we%2520infer%2520computational%2520principles%2520of%2520LLMs%2520from%2520systematic%2520behavioural%2520studies.%2520We%2520introduce%2520a%2520behavioural%2520benchmark%2520-%2520BayesBench%253A%2520four%2520magnitude%2520estimation%2520tasks%2520%2528length%252C%2520location%252C%2520distance%252C%2520and%2520duration%2529%2520over%2520text%2520and%2520image%252C%2520inspired%2520by%2520classic%2520psychophysics%252C%2520and%2520evaluate%2520a%2520diverse%2520set%2520of%2520nine%2520LLMs%2520alongside%2520human%2520judgments%2520for%2520calibration.%2520Through%2520controlled%2520ablations%2520of%2520noise%252C%2520context%252C%2520and%2520instruction%2520prompts%252C%2520we%2520measure%2520performance%252C%2520behaviour%2520and%2520efficiency%2520in%2520multimodal%2520cue-combination.%2520Beyond%2520accuracy%2520and%2520efficiency%2520metrics%252C%2520we%2520introduce%2520a%2520Bayesian%2520Consistency%2520Score%2520that%2520detects%2520Bayes-consistent%2520behavioural%2520shifts%2520even%2520when%2520accuracy%2520saturates.%2520Our%2520results%2520show%2520that%2520while%2520capable%2520models%2520often%2520adapt%2520in%2520Bayes-consistent%2520ways%252C%2520accuracy%2520does%2520not%2520guarantee%2520robustness.%2520Notably%252C%2520GPT-5%2520Mini%2520achieves%2520perfect%2520text%2520accuracy%2520but%2520fails%2520to%2520integrate%2520visual%2520cues%2520efficiently.%2520This%2520reveals%2520a%2520critical%2520dissociation%2520between%2520capability%2520and%2520strategy%252C%2520suggesting%2520accuracy-centric%2520benchmarks%2520may%2520over-index%2520on%2520performance%2520while%2520missing%2520brittle%2520uncertainty%2520handling.%2520These%2520findings%2520reveal%2520emergent%2520principled%2520handling%2520of%2520uncertainty%2520and%2520highlight%2520the%2520correlation%2520between%2520accuracy%2520and%2520Bayesian%2520tendencies.%2520We%2520release%2520our%2520psychophysics%2520benchmark%2520and%2520consistency%2520metric%2520%2528https%253A//bayes-bench.github.io%2529%2520as%2520evaluation%2520tools%2520and%2520to%2520inform%2520future%2520multimodal%2520architecture%2520designs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02719v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emergent%20Bayesian%20Behaviour%20and%20Optimal%20Cue%20Combination%20in%20LLMs&entry.906535625=Julian%20Ma%20and%20Jun%20Wang%20and%20Zafeirios%20Fountas&entry.1292438233=Large%20language%20models%20%28LLMs%29%20excel%20at%20explicit%20reasoning%2C%20but%20their%20implicit%20computational%20strategies%20remain%20underexplored.%20Decades%20of%20psychophysics%20research%20show%20that%20humans%20intuitively%20process%20and%20integrate%20noisy%20signals%20using%20near-optimal%20Bayesian%20strategies%20in%20perceptual%20tasks.%20We%20ask%20whether%20LLMs%20exhibit%20similar%20behaviour%20and%20perform%20optimal%20multimodal%20integration%20without%20explicit%20training%20or%20instruction.%20Adopting%20the%20psychophysics%20paradigm%2C%20we%20infer%20computational%20principles%20of%20LLMs%20from%20systematic%20behavioural%20studies.%20We%20introduce%20a%20behavioural%20benchmark%20-%20BayesBench%3A%20four%20magnitude%20estimation%20tasks%20%28length%2C%20location%2C%20distance%2C%20and%20duration%29%20over%20text%20and%20image%2C%20inspired%20by%20classic%20psychophysics%2C%20and%20evaluate%20a%20diverse%20set%20of%20nine%20LLMs%20alongside%20human%20judgments%20for%20calibration.%20Through%20controlled%20ablations%20of%20noise%2C%20context%2C%20and%20instruction%20prompts%2C%20we%20measure%20performance%2C%20behaviour%20and%20efficiency%20in%20multimodal%20cue-combination.%20Beyond%20accuracy%20and%20efficiency%20metrics%2C%20we%20introduce%20a%20Bayesian%20Consistency%20Score%20that%20detects%20Bayes-consistent%20behavioural%20shifts%20even%20when%20accuracy%20saturates.%20Our%20results%20show%20that%20while%20capable%20models%20often%20adapt%20in%20Bayes-consistent%20ways%2C%20accuracy%20does%20not%20guarantee%20robustness.%20Notably%2C%20GPT-5%20Mini%20achieves%20perfect%20text%20accuracy%20but%20fails%20to%20integrate%20visual%20cues%20efficiently.%20This%20reveals%20a%20critical%20dissociation%20between%20capability%20and%20strategy%2C%20suggesting%20accuracy-centric%20benchmarks%20may%20over-index%20on%20performance%20while%20missing%20brittle%20uncertainty%20handling.%20These%20findings%20reveal%20emergent%20principled%20handling%20of%20uncertainty%20and%20highlight%20the%20correlation%20between%20accuracy%20and%20Bayesian%20tendencies.%20We%20release%20our%20psychophysics%20benchmark%20and%20consistency%20metric%20%28https%3A//bayes-bench.github.io%29%20as%20evaluation%20tools%20and%20to%20inform%20future%20multimodal%20architecture%20designs.&entry.1838667208=http%3A//arxiv.org/abs/2512.02719v1&entry.124074799=Read"},
{"title": "Is Image-based Object Pose Estimation Ready to Support Grasping?", "author": "Eric C. Joyce and Qianwen Zhao and Nathaniel Burgdorfer and Long Wang and Philippos Mordohai", "abstract": "We present a framework for evaluating 6-DoF instance-level object pose estimators, focusing on those that require a single RGB (not RGB-D) image as input. Besides gaining intuition about how accurate these estimators are, we are interested in the degree to which they can serve as the sole perception mechanism for robotic grasping. To assess this, we perform grasping trials in a physics-based simulator, using image-based pose estimates to guide a parallel gripper and an underactuated robotic hand in picking up 3D models of objects. Our experiments on a subset of the BOP (Benchmark for 6D Object Pose Estimation) dataset compare five open-source object pose estimators and provide insights that were missing from the literature.", "link": "http://arxiv.org/abs/2512.01856v2", "date": "2025-12-02", "relevancy": 2.201, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5875}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5485}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20Image-based%20Object%20Pose%20Estimation%20Ready%20to%20Support%20Grasping%3F&body=Title%3A%20Is%20Image-based%20Object%20Pose%20Estimation%20Ready%20to%20Support%20Grasping%3F%0AAuthor%3A%20Eric%20C.%20Joyce%20and%20Qianwen%20Zhao%20and%20Nathaniel%20Burgdorfer%20and%20Long%20Wang%20and%20Philippos%20Mordohai%0AAbstract%3A%20We%20present%20a%20framework%20for%20evaluating%206-DoF%20instance-level%20object%20pose%20estimators%2C%20focusing%20on%20those%20that%20require%20a%20single%20RGB%20%28not%20RGB-D%29%20image%20as%20input.%20Besides%20gaining%20intuition%20about%20how%20accurate%20these%20estimators%20are%2C%20we%20are%20interested%20in%20the%20degree%20to%20which%20they%20can%20serve%20as%20the%20sole%20perception%20mechanism%20for%20robotic%20grasping.%20To%20assess%20this%2C%20we%20perform%20grasping%20trials%20in%20a%20physics-based%20simulator%2C%20using%20image-based%20pose%20estimates%20to%20guide%20a%20parallel%20gripper%20and%20an%20underactuated%20robotic%20hand%20in%20picking%20up%203D%20models%20of%20objects.%20Our%20experiments%20on%20a%20subset%20of%20the%20BOP%20%28Benchmark%20for%206D%20Object%20Pose%20Estimation%29%20dataset%20compare%20five%20open-source%20object%20pose%20estimators%20and%20provide%20insights%20that%20were%20missing%20from%20the%20literature.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01856v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520Image-based%2520Object%2520Pose%2520Estimation%2520Ready%2520to%2520Support%2520Grasping%253F%26entry.906535625%3DEric%2520C.%2520Joyce%2520and%2520Qianwen%2520Zhao%2520and%2520Nathaniel%2520Burgdorfer%2520and%2520Long%2520Wang%2520and%2520Philippos%2520Mordohai%26entry.1292438233%3DWe%2520present%2520a%2520framework%2520for%2520evaluating%25206-DoF%2520instance-level%2520object%2520pose%2520estimators%252C%2520focusing%2520on%2520those%2520that%2520require%2520a%2520single%2520RGB%2520%2528not%2520RGB-D%2529%2520image%2520as%2520input.%2520Besides%2520gaining%2520intuition%2520about%2520how%2520accurate%2520these%2520estimators%2520are%252C%2520we%2520are%2520interested%2520in%2520the%2520degree%2520to%2520which%2520they%2520can%2520serve%2520as%2520the%2520sole%2520perception%2520mechanism%2520for%2520robotic%2520grasping.%2520To%2520assess%2520this%252C%2520we%2520perform%2520grasping%2520trials%2520in%2520a%2520physics-based%2520simulator%252C%2520using%2520image-based%2520pose%2520estimates%2520to%2520guide%2520a%2520parallel%2520gripper%2520and%2520an%2520underactuated%2520robotic%2520hand%2520in%2520picking%2520up%25203D%2520models%2520of%2520objects.%2520Our%2520experiments%2520on%2520a%2520subset%2520of%2520the%2520BOP%2520%2528Benchmark%2520for%25206D%2520Object%2520Pose%2520Estimation%2529%2520dataset%2520compare%2520five%2520open-source%2520object%2520pose%2520estimators%2520and%2520provide%2520insights%2520that%2520were%2520missing%2520from%2520the%2520literature.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01856v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20Image-based%20Object%20Pose%20Estimation%20Ready%20to%20Support%20Grasping%3F&entry.906535625=Eric%20C.%20Joyce%20and%20Qianwen%20Zhao%20and%20Nathaniel%20Burgdorfer%20and%20Long%20Wang%20and%20Philippos%20Mordohai&entry.1292438233=We%20present%20a%20framework%20for%20evaluating%206-DoF%20instance-level%20object%20pose%20estimators%2C%20focusing%20on%20those%20that%20require%20a%20single%20RGB%20%28not%20RGB-D%29%20image%20as%20input.%20Besides%20gaining%20intuition%20about%20how%20accurate%20these%20estimators%20are%2C%20we%20are%20interested%20in%20the%20degree%20to%20which%20they%20can%20serve%20as%20the%20sole%20perception%20mechanism%20for%20robotic%20grasping.%20To%20assess%20this%2C%20we%20perform%20grasping%20trials%20in%20a%20physics-based%20simulator%2C%20using%20image-based%20pose%20estimates%20to%20guide%20a%20parallel%20gripper%20and%20an%20underactuated%20robotic%20hand%20in%20picking%20up%203D%20models%20of%20objects.%20Our%20experiments%20on%20a%20subset%20of%20the%20BOP%20%28Benchmark%20for%206D%20Object%20Pose%20Estimation%29%20dataset%20compare%20five%20open-source%20object%20pose%20estimators%20and%20provide%20insights%20that%20were%20missing%20from%20the%20literature.&entry.1838667208=http%3A//arxiv.org/abs/2512.01856v2&entry.124074799=Read"},
{"title": "Can Vision-Language Models Count? A Synthetic Benchmark and Analysis of Attention-Based Interventions", "author": "Saurav Sengupta and Nazanin Moradinasab and Jiebei Liu and Donald E. Brown", "abstract": "Recent research suggests that Vision Language Models (VLMs) often rely on inherent biases learned during training when responding to queries about visual properties of images. These biases are exacerbated when VLMs are asked highly specific questions that require them to focus on particular areas of the image in tasks such as counting. We build upon this research by developing a synthetic benchmark dataset and evaluation framework to systematically determine how counting performance varies as image and prompt properties change. Using open-source VLMs, we then analyze how attention allocation fluctuates with varying input parameters (e.g. number of objects in the image, objects color, background color, objects texture, background texture, and prompt specificity). We further implement attention-based interventions to modulate focus on visual tokens at different layers and evaluate their impact on counting performance across a range of visual conditions. Our experiments reveal that while VLM counting performance remains challenging, especially under high visual or linguistic complexity, certain attention interventions can lead to modest gains in counting performance.", "link": "http://arxiv.org/abs/2511.17722v2", "date": "2025-12-02", "relevancy": 2.1996, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.561}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.561}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4941}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Vision-Language%20Models%20Count%3F%20A%20Synthetic%20Benchmark%20and%20Analysis%20of%20Attention-Based%20Interventions&body=Title%3A%20Can%20Vision-Language%20Models%20Count%3F%20A%20Synthetic%20Benchmark%20and%20Analysis%20of%20Attention-Based%20Interventions%0AAuthor%3A%20Saurav%20Sengupta%20and%20Nazanin%20Moradinasab%20and%20Jiebei%20Liu%20and%20Donald%20E.%20Brown%0AAbstract%3A%20Recent%20research%20suggests%20that%20Vision%20Language%20Models%20%28VLMs%29%20often%20rely%20on%20inherent%20biases%20learned%20during%20training%20when%20responding%20to%20queries%20about%20visual%20properties%20of%20images.%20These%20biases%20are%20exacerbated%20when%20VLMs%20are%20asked%20highly%20specific%20questions%20that%20require%20them%20to%20focus%20on%20particular%20areas%20of%20the%20image%20in%20tasks%20such%20as%20counting.%20We%20build%20upon%20this%20research%20by%20developing%20a%20synthetic%20benchmark%20dataset%20and%20evaluation%20framework%20to%20systematically%20determine%20how%20counting%20performance%20varies%20as%20image%20and%20prompt%20properties%20change.%20Using%20open-source%20VLMs%2C%20we%20then%20analyze%20how%20attention%20allocation%20fluctuates%20with%20varying%20input%20parameters%20%28e.g.%20number%20of%20objects%20in%20the%20image%2C%20objects%20color%2C%20background%20color%2C%20objects%20texture%2C%20background%20texture%2C%20and%20prompt%20specificity%29.%20We%20further%20implement%20attention-based%20interventions%20to%20modulate%20focus%20on%20visual%20tokens%20at%20different%20layers%20and%20evaluate%20their%20impact%20on%20counting%20performance%20across%20a%20range%20of%20visual%20conditions.%20Our%20experiments%20reveal%20that%20while%20VLM%20counting%20performance%20remains%20challenging%2C%20especially%20under%20high%20visual%20or%20linguistic%20complexity%2C%20certain%20attention%20interventions%20can%20lead%20to%20modest%20gains%20in%20counting%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2511.17722v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Vision-Language%2520Models%2520Count%253F%2520A%2520Synthetic%2520Benchmark%2520and%2520Analysis%2520of%2520Attention-Based%2520Interventions%26entry.906535625%3DSaurav%2520Sengupta%2520and%2520Nazanin%2520Moradinasab%2520and%2520Jiebei%2520Liu%2520and%2520Donald%2520E.%2520Brown%26entry.1292438233%3DRecent%2520research%2520suggests%2520that%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520often%2520rely%2520on%2520inherent%2520biases%2520learned%2520during%2520training%2520when%2520responding%2520to%2520queries%2520about%2520visual%2520properties%2520of%2520images.%2520These%2520biases%2520are%2520exacerbated%2520when%2520VLMs%2520are%2520asked%2520highly%2520specific%2520questions%2520that%2520require%2520them%2520to%2520focus%2520on%2520particular%2520areas%2520of%2520the%2520image%2520in%2520tasks%2520such%2520as%2520counting.%2520We%2520build%2520upon%2520this%2520research%2520by%2520developing%2520a%2520synthetic%2520benchmark%2520dataset%2520and%2520evaluation%2520framework%2520to%2520systematically%2520determine%2520how%2520counting%2520performance%2520varies%2520as%2520image%2520and%2520prompt%2520properties%2520change.%2520Using%2520open-source%2520VLMs%252C%2520we%2520then%2520analyze%2520how%2520attention%2520allocation%2520fluctuates%2520with%2520varying%2520input%2520parameters%2520%2528e.g.%2520number%2520of%2520objects%2520in%2520the%2520image%252C%2520objects%2520color%252C%2520background%2520color%252C%2520objects%2520texture%252C%2520background%2520texture%252C%2520and%2520prompt%2520specificity%2529.%2520We%2520further%2520implement%2520attention-based%2520interventions%2520to%2520modulate%2520focus%2520on%2520visual%2520tokens%2520at%2520different%2520layers%2520and%2520evaluate%2520their%2520impact%2520on%2520counting%2520performance%2520across%2520a%2520range%2520of%2520visual%2520conditions.%2520Our%2520experiments%2520reveal%2520that%2520while%2520VLM%2520counting%2520performance%2520remains%2520challenging%252C%2520especially%2520under%2520high%2520visual%2520or%2520linguistic%2520complexity%252C%2520certain%2520attention%2520interventions%2520can%2520lead%2520to%2520modest%2520gains%2520in%2520counting%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.17722v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Vision-Language%20Models%20Count%3F%20A%20Synthetic%20Benchmark%20and%20Analysis%20of%20Attention-Based%20Interventions&entry.906535625=Saurav%20Sengupta%20and%20Nazanin%20Moradinasab%20and%20Jiebei%20Liu%20and%20Donald%20E.%20Brown&entry.1292438233=Recent%20research%20suggests%20that%20Vision%20Language%20Models%20%28VLMs%29%20often%20rely%20on%20inherent%20biases%20learned%20during%20training%20when%20responding%20to%20queries%20about%20visual%20properties%20of%20images.%20These%20biases%20are%20exacerbated%20when%20VLMs%20are%20asked%20highly%20specific%20questions%20that%20require%20them%20to%20focus%20on%20particular%20areas%20of%20the%20image%20in%20tasks%20such%20as%20counting.%20We%20build%20upon%20this%20research%20by%20developing%20a%20synthetic%20benchmark%20dataset%20and%20evaluation%20framework%20to%20systematically%20determine%20how%20counting%20performance%20varies%20as%20image%20and%20prompt%20properties%20change.%20Using%20open-source%20VLMs%2C%20we%20then%20analyze%20how%20attention%20allocation%20fluctuates%20with%20varying%20input%20parameters%20%28e.g.%20number%20of%20objects%20in%20the%20image%2C%20objects%20color%2C%20background%20color%2C%20objects%20texture%2C%20background%20texture%2C%20and%20prompt%20specificity%29.%20We%20further%20implement%20attention-based%20interventions%20to%20modulate%20focus%20on%20visual%20tokens%20at%20different%20layers%20and%20evaluate%20their%20impact%20on%20counting%20performance%20across%20a%20range%20of%20visual%20conditions.%20Our%20experiments%20reveal%20that%20while%20VLM%20counting%20performance%20remains%20challenging%2C%20especially%20under%20high%20visual%20or%20linguistic%20complexity%2C%20certain%20attention%20interventions%20can%20lead%20to%20modest%20gains%20in%20counting%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2511.17722v2&entry.124074799=Read"},
{"title": "NOCTIS: Novel Object Cyclic Threshold based Instance Segmentation", "author": "Max Gandyra and Alessandro Santonicola and Michael Beetz", "abstract": "Instance segmentation of novel objects instances in RGB images, given some example images for each object, is a well known problem in computer vision. Designing a model general enough to be employed for all kinds of novel objects without (re-) training has proven to be a difficult task. To handle this, we present a new training-free framework, called: Novel Object Cyclic Threshold based Instance Segmentation (NOCTIS). NOCTIS integrates two pre-trained models: Grounded-SAM 2 for object proposals with precise bounding boxes and corresponding segmentation masks; and DINOv2 for robust class and patch embeddings, due to its zero-shot capabilities. Internally, the proposal-object matching is realized by determining an object matching score based on the similarity of the class embeddings and the average maximum similarity of the patch embeddings with a new cyclic thresholding (CT) mechanism that mitigates unstable matches caused by repetitive textures or visually similar patterns. Beyond CT, NOCTIS introduces: (i) an appearance score that is unaffected by object selection bias; (ii) the usage of the average confidence of the proposals' bounding box and mask as a scoring component; and (iii) an RGB-only pipeline that performs even better than RGB-D ones. We empirically show that NOCTIS, without further training/fine tuning, outperforms the best RGB and RGB-D methods regarding the mean AP score on the seven core datasets of the BOP 2023 challenge for the \"Model-based 2D segmentation of unseen objects\" task.", "link": "http://arxiv.org/abs/2507.01463v4", "date": "2025-12-02", "relevancy": 2.1905, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5604}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5473}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NOCTIS%3A%20Novel%20Object%20Cyclic%20Threshold%20based%20Instance%20Segmentation&body=Title%3A%20NOCTIS%3A%20Novel%20Object%20Cyclic%20Threshold%20based%20Instance%20Segmentation%0AAuthor%3A%20Max%20Gandyra%20and%20Alessandro%20Santonicola%20and%20Michael%20Beetz%0AAbstract%3A%20Instance%20segmentation%20of%20novel%20objects%20instances%20in%20RGB%20images%2C%20given%20some%20example%20images%20for%20each%20object%2C%20is%20a%20well%20known%20problem%20in%20computer%20vision.%20Designing%20a%20model%20general%20enough%20to%20be%20employed%20for%20all%20kinds%20of%20novel%20objects%20without%20%28re-%29%20training%20has%20proven%20to%20be%20a%20difficult%20task.%20To%20handle%20this%2C%20we%20present%20a%20new%20training-free%20framework%2C%20called%3A%20Novel%20Object%20Cyclic%20Threshold%20based%20Instance%20Segmentation%20%28NOCTIS%29.%20NOCTIS%20integrates%20two%20pre-trained%20models%3A%20Grounded-SAM%202%20for%20object%20proposals%20with%20precise%20bounding%20boxes%20and%20corresponding%20segmentation%20masks%3B%20and%20DINOv2%20for%20robust%20class%20and%20patch%20embeddings%2C%20due%20to%20its%20zero-shot%20capabilities.%20Internally%2C%20the%20proposal-object%20matching%20is%20realized%20by%20determining%20an%20object%20matching%20score%20based%20on%20the%20similarity%20of%20the%20class%20embeddings%20and%20the%20average%20maximum%20similarity%20of%20the%20patch%20embeddings%20with%20a%20new%20cyclic%20thresholding%20%28CT%29%20mechanism%20that%20mitigates%20unstable%20matches%20caused%20by%20repetitive%20textures%20or%20visually%20similar%20patterns.%20Beyond%20CT%2C%20NOCTIS%20introduces%3A%20%28i%29%20an%20appearance%20score%20that%20is%20unaffected%20by%20object%20selection%20bias%3B%20%28ii%29%20the%20usage%20of%20the%20average%20confidence%20of%20the%20proposals%27%20bounding%20box%20and%20mask%20as%20a%20scoring%20component%3B%20and%20%28iii%29%20an%20RGB-only%20pipeline%20that%20performs%20even%20better%20than%20RGB-D%20ones.%20We%20empirically%20show%20that%20NOCTIS%2C%20without%20further%20training/fine%20tuning%2C%20outperforms%20the%20best%20RGB%20and%20RGB-D%20methods%20regarding%20the%20mean%20AP%20score%20on%20the%20seven%20core%20datasets%20of%20the%20BOP%202023%20challenge%20for%20the%20%22Model-based%202D%20segmentation%20of%20unseen%20objects%22%20task.%0ALink%3A%20http%3A//arxiv.org/abs/2507.01463v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNOCTIS%253A%2520Novel%2520Object%2520Cyclic%2520Threshold%2520based%2520Instance%2520Segmentation%26entry.906535625%3DMax%2520Gandyra%2520and%2520Alessandro%2520Santonicola%2520and%2520Michael%2520Beetz%26entry.1292438233%3DInstance%2520segmentation%2520of%2520novel%2520objects%2520instances%2520in%2520RGB%2520images%252C%2520given%2520some%2520example%2520images%2520for%2520each%2520object%252C%2520is%2520a%2520well%2520known%2520problem%2520in%2520computer%2520vision.%2520Designing%2520a%2520model%2520general%2520enough%2520to%2520be%2520employed%2520for%2520all%2520kinds%2520of%2520novel%2520objects%2520without%2520%2528re-%2529%2520training%2520has%2520proven%2520to%2520be%2520a%2520difficult%2520task.%2520To%2520handle%2520this%252C%2520we%2520present%2520a%2520new%2520training-free%2520framework%252C%2520called%253A%2520Novel%2520Object%2520Cyclic%2520Threshold%2520based%2520Instance%2520Segmentation%2520%2528NOCTIS%2529.%2520NOCTIS%2520integrates%2520two%2520pre-trained%2520models%253A%2520Grounded-SAM%25202%2520for%2520object%2520proposals%2520with%2520precise%2520bounding%2520boxes%2520and%2520corresponding%2520segmentation%2520masks%253B%2520and%2520DINOv2%2520for%2520robust%2520class%2520and%2520patch%2520embeddings%252C%2520due%2520to%2520its%2520zero-shot%2520capabilities.%2520Internally%252C%2520the%2520proposal-object%2520matching%2520is%2520realized%2520by%2520determining%2520an%2520object%2520matching%2520score%2520based%2520on%2520the%2520similarity%2520of%2520the%2520class%2520embeddings%2520and%2520the%2520average%2520maximum%2520similarity%2520of%2520the%2520patch%2520embeddings%2520with%2520a%2520new%2520cyclic%2520thresholding%2520%2528CT%2529%2520mechanism%2520that%2520mitigates%2520unstable%2520matches%2520caused%2520by%2520repetitive%2520textures%2520or%2520visually%2520similar%2520patterns.%2520Beyond%2520CT%252C%2520NOCTIS%2520introduces%253A%2520%2528i%2529%2520an%2520appearance%2520score%2520that%2520is%2520unaffected%2520by%2520object%2520selection%2520bias%253B%2520%2528ii%2529%2520the%2520usage%2520of%2520the%2520average%2520confidence%2520of%2520the%2520proposals%2527%2520bounding%2520box%2520and%2520mask%2520as%2520a%2520scoring%2520component%253B%2520and%2520%2528iii%2529%2520an%2520RGB-only%2520pipeline%2520that%2520performs%2520even%2520better%2520than%2520RGB-D%2520ones.%2520We%2520empirically%2520show%2520that%2520NOCTIS%252C%2520without%2520further%2520training/fine%2520tuning%252C%2520outperforms%2520the%2520best%2520RGB%2520and%2520RGB-D%2520methods%2520regarding%2520the%2520mean%2520AP%2520score%2520on%2520the%2520seven%2520core%2520datasets%2520of%2520the%2520BOP%25202023%2520challenge%2520for%2520the%2520%2522Model-based%25202D%2520segmentation%2520of%2520unseen%2520objects%2522%2520task.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01463v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NOCTIS%3A%20Novel%20Object%20Cyclic%20Threshold%20based%20Instance%20Segmentation&entry.906535625=Max%20Gandyra%20and%20Alessandro%20Santonicola%20and%20Michael%20Beetz&entry.1292438233=Instance%20segmentation%20of%20novel%20objects%20instances%20in%20RGB%20images%2C%20given%20some%20example%20images%20for%20each%20object%2C%20is%20a%20well%20known%20problem%20in%20computer%20vision.%20Designing%20a%20model%20general%20enough%20to%20be%20employed%20for%20all%20kinds%20of%20novel%20objects%20without%20%28re-%29%20training%20has%20proven%20to%20be%20a%20difficult%20task.%20To%20handle%20this%2C%20we%20present%20a%20new%20training-free%20framework%2C%20called%3A%20Novel%20Object%20Cyclic%20Threshold%20based%20Instance%20Segmentation%20%28NOCTIS%29.%20NOCTIS%20integrates%20two%20pre-trained%20models%3A%20Grounded-SAM%202%20for%20object%20proposals%20with%20precise%20bounding%20boxes%20and%20corresponding%20segmentation%20masks%3B%20and%20DINOv2%20for%20robust%20class%20and%20patch%20embeddings%2C%20due%20to%20its%20zero-shot%20capabilities.%20Internally%2C%20the%20proposal-object%20matching%20is%20realized%20by%20determining%20an%20object%20matching%20score%20based%20on%20the%20similarity%20of%20the%20class%20embeddings%20and%20the%20average%20maximum%20similarity%20of%20the%20patch%20embeddings%20with%20a%20new%20cyclic%20thresholding%20%28CT%29%20mechanism%20that%20mitigates%20unstable%20matches%20caused%20by%20repetitive%20textures%20or%20visually%20similar%20patterns.%20Beyond%20CT%2C%20NOCTIS%20introduces%3A%20%28i%29%20an%20appearance%20score%20that%20is%20unaffected%20by%20object%20selection%20bias%3B%20%28ii%29%20the%20usage%20of%20the%20average%20confidence%20of%20the%20proposals%27%20bounding%20box%20and%20mask%20as%20a%20scoring%20component%3B%20and%20%28iii%29%20an%20RGB-only%20pipeline%20that%20performs%20even%20better%20than%20RGB-D%20ones.%20We%20empirically%20show%20that%20NOCTIS%2C%20without%20further%20training/fine%20tuning%2C%20outperforms%20the%20best%20RGB%20and%20RGB-D%20methods%20regarding%20the%20mean%20AP%20score%20on%20the%20seven%20core%20datasets%20of%20the%20BOP%202023%20challenge%20for%20the%20%22Model-based%202D%20segmentation%20of%20unseen%20objects%22%20task.&entry.1838667208=http%3A//arxiv.org/abs/2507.01463v4&entry.124074799=Read"},
{"title": "TrackNetV5: Residual-Driven Spatio-Temporal Refinement and Motion Direction Decoupling for Fast Object Tracking", "author": "Tang Haonan and Chen Yanjun and Jiang Lezhi", "abstract": "The TrackNet series has established a strong baseline for fast-moving small object tracking in sports. However, existing iterations face significant limitations: V1-V3 struggle with occlusions due to a reliance on purely visual cues, while TrackNetV4, despite introducing motion inputs, suffers from directional ambiguity as its absolute difference method discards motion polarity. To overcome these bottlenecks, we propose TrackNetV5, a robust architecture integrating two novel mechanisms. First, to recover lost directional priors, we introduce the Motion Direction Decoupling (MDD) module. Unlike V4, MDD decomposes temporal dynamics into signed polarity fields, explicitly encoding both movement occurrence and trajectory direction. Second, we propose the Residual-Driven Spatio-Temporal Refinement (R-STR) head. Operating on a coarse-to-fine paradigm, this Transformer-based module leverages factorized spatio-temporal contexts to estimate a corrective residual, effectively recovering occluded targets. Extensive experiments on the TrackNetV2 dataset demonstrate that TrackNetV5 achieves a new state-of-the-art F1-score of 0.9859 and an accuracy of 0.9733, significantly outperforming previous versions. Notably, this performance leap is achieved with a marginal 3.7% increase in FLOPs compared to V4, maintaining real-time inference capabilities while delivering superior tracking precision.", "link": "http://arxiv.org/abs/2512.02789v1", "date": "2025-12-02", "relevancy": 2.1838, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5629}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5383}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TrackNetV5%3A%20Residual-Driven%20Spatio-Temporal%20Refinement%20and%20Motion%20Direction%20Decoupling%20for%20Fast%20Object%20Tracking&body=Title%3A%20TrackNetV5%3A%20Residual-Driven%20Spatio-Temporal%20Refinement%20and%20Motion%20Direction%20Decoupling%20for%20Fast%20Object%20Tracking%0AAuthor%3A%20Tang%20Haonan%20and%20Chen%20Yanjun%20and%20Jiang%20Lezhi%0AAbstract%3A%20The%20TrackNet%20series%20has%20established%20a%20strong%20baseline%20for%20fast-moving%20small%20object%20tracking%20in%20sports.%20However%2C%20existing%20iterations%20face%20significant%20limitations%3A%20V1-V3%20struggle%20with%20occlusions%20due%20to%20a%20reliance%20on%20purely%20visual%20cues%2C%20while%20TrackNetV4%2C%20despite%20introducing%20motion%20inputs%2C%20suffers%20from%20directional%20ambiguity%20as%20its%20absolute%20difference%20method%20discards%20motion%20polarity.%20To%20overcome%20these%20bottlenecks%2C%20we%20propose%20TrackNetV5%2C%20a%20robust%20architecture%20integrating%20two%20novel%20mechanisms.%20First%2C%20to%20recover%20lost%20directional%20priors%2C%20we%20introduce%20the%20Motion%20Direction%20Decoupling%20%28MDD%29%20module.%20Unlike%20V4%2C%20MDD%20decomposes%20temporal%20dynamics%20into%20signed%20polarity%20fields%2C%20explicitly%20encoding%20both%20movement%20occurrence%20and%20trajectory%20direction.%20Second%2C%20we%20propose%20the%20Residual-Driven%20Spatio-Temporal%20Refinement%20%28R-STR%29%20head.%20Operating%20on%20a%20coarse-to-fine%20paradigm%2C%20this%20Transformer-based%20module%20leverages%20factorized%20spatio-temporal%20contexts%20to%20estimate%20a%20corrective%20residual%2C%20effectively%20recovering%20occluded%20targets.%20Extensive%20experiments%20on%20the%20TrackNetV2%20dataset%20demonstrate%20that%20TrackNetV5%20achieves%20a%20new%20state-of-the-art%20F1-score%20of%200.9859%20and%20an%20accuracy%20of%200.9733%2C%20significantly%20outperforming%20previous%20versions.%20Notably%2C%20this%20performance%20leap%20is%20achieved%20with%20a%20marginal%203.7%25%20increase%20in%20FLOPs%20compared%20to%20V4%2C%20maintaining%20real-time%20inference%20capabilities%20while%20delivering%20superior%20tracking%20precision.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02789v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrackNetV5%253A%2520Residual-Driven%2520Spatio-Temporal%2520Refinement%2520and%2520Motion%2520Direction%2520Decoupling%2520for%2520Fast%2520Object%2520Tracking%26entry.906535625%3DTang%2520Haonan%2520and%2520Chen%2520Yanjun%2520and%2520Jiang%2520Lezhi%26entry.1292438233%3DThe%2520TrackNet%2520series%2520has%2520established%2520a%2520strong%2520baseline%2520for%2520fast-moving%2520small%2520object%2520tracking%2520in%2520sports.%2520However%252C%2520existing%2520iterations%2520face%2520significant%2520limitations%253A%2520V1-V3%2520struggle%2520with%2520occlusions%2520due%2520to%2520a%2520reliance%2520on%2520purely%2520visual%2520cues%252C%2520while%2520TrackNetV4%252C%2520despite%2520introducing%2520motion%2520inputs%252C%2520suffers%2520from%2520directional%2520ambiguity%2520as%2520its%2520absolute%2520difference%2520method%2520discards%2520motion%2520polarity.%2520To%2520overcome%2520these%2520bottlenecks%252C%2520we%2520propose%2520TrackNetV5%252C%2520a%2520robust%2520architecture%2520integrating%2520two%2520novel%2520mechanisms.%2520First%252C%2520to%2520recover%2520lost%2520directional%2520priors%252C%2520we%2520introduce%2520the%2520Motion%2520Direction%2520Decoupling%2520%2528MDD%2529%2520module.%2520Unlike%2520V4%252C%2520MDD%2520decomposes%2520temporal%2520dynamics%2520into%2520signed%2520polarity%2520fields%252C%2520explicitly%2520encoding%2520both%2520movement%2520occurrence%2520and%2520trajectory%2520direction.%2520Second%252C%2520we%2520propose%2520the%2520Residual-Driven%2520Spatio-Temporal%2520Refinement%2520%2528R-STR%2529%2520head.%2520Operating%2520on%2520a%2520coarse-to-fine%2520paradigm%252C%2520this%2520Transformer-based%2520module%2520leverages%2520factorized%2520spatio-temporal%2520contexts%2520to%2520estimate%2520a%2520corrective%2520residual%252C%2520effectively%2520recovering%2520occluded%2520targets.%2520Extensive%2520experiments%2520on%2520the%2520TrackNetV2%2520dataset%2520demonstrate%2520that%2520TrackNetV5%2520achieves%2520a%2520new%2520state-of-the-art%2520F1-score%2520of%25200.9859%2520and%2520an%2520accuracy%2520of%25200.9733%252C%2520significantly%2520outperforming%2520previous%2520versions.%2520Notably%252C%2520this%2520performance%2520leap%2520is%2520achieved%2520with%2520a%2520marginal%25203.7%2525%2520increase%2520in%2520FLOPs%2520compared%2520to%2520V4%252C%2520maintaining%2520real-time%2520inference%2520capabilities%2520while%2520delivering%2520superior%2520tracking%2520precision.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02789v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TrackNetV5%3A%20Residual-Driven%20Spatio-Temporal%20Refinement%20and%20Motion%20Direction%20Decoupling%20for%20Fast%20Object%20Tracking&entry.906535625=Tang%20Haonan%20and%20Chen%20Yanjun%20and%20Jiang%20Lezhi&entry.1292438233=The%20TrackNet%20series%20has%20established%20a%20strong%20baseline%20for%20fast-moving%20small%20object%20tracking%20in%20sports.%20However%2C%20existing%20iterations%20face%20significant%20limitations%3A%20V1-V3%20struggle%20with%20occlusions%20due%20to%20a%20reliance%20on%20purely%20visual%20cues%2C%20while%20TrackNetV4%2C%20despite%20introducing%20motion%20inputs%2C%20suffers%20from%20directional%20ambiguity%20as%20its%20absolute%20difference%20method%20discards%20motion%20polarity.%20To%20overcome%20these%20bottlenecks%2C%20we%20propose%20TrackNetV5%2C%20a%20robust%20architecture%20integrating%20two%20novel%20mechanisms.%20First%2C%20to%20recover%20lost%20directional%20priors%2C%20we%20introduce%20the%20Motion%20Direction%20Decoupling%20%28MDD%29%20module.%20Unlike%20V4%2C%20MDD%20decomposes%20temporal%20dynamics%20into%20signed%20polarity%20fields%2C%20explicitly%20encoding%20both%20movement%20occurrence%20and%20trajectory%20direction.%20Second%2C%20we%20propose%20the%20Residual-Driven%20Spatio-Temporal%20Refinement%20%28R-STR%29%20head.%20Operating%20on%20a%20coarse-to-fine%20paradigm%2C%20this%20Transformer-based%20module%20leverages%20factorized%20spatio-temporal%20contexts%20to%20estimate%20a%20corrective%20residual%2C%20effectively%20recovering%20occluded%20targets.%20Extensive%20experiments%20on%20the%20TrackNetV2%20dataset%20demonstrate%20that%20TrackNetV5%20achieves%20a%20new%20state-of-the-art%20F1-score%20of%200.9859%20and%20an%20accuracy%20of%200.9733%2C%20significantly%20outperforming%20previous%20versions.%20Notably%2C%20this%20performance%20leap%20is%20achieved%20with%20a%20marginal%203.7%25%20increase%20in%20FLOPs%20compared%20to%20V4%2C%20maintaining%20real-time%20inference%20capabilities%20while%20delivering%20superior%20tracking%20precision.&entry.1838667208=http%3A//arxiv.org/abs/2512.02789v1&entry.124074799=Read"},
{"title": "AIDEN: Design and Pilot Study of an AI Assistant for the Visually Impaired", "author": "Luis Marquez-Carpintero and Francisco Gomez-Donoso and Zuria Bauer and Bessie Dominguez-Dager and Alvaro Belmonte-Baeza and M\u00f3nica Pina-Navarro and Francisco Morillas-Espejo and Felix Escalona and Miguel Cazorla", "abstract": "This paper presents AIDEN, an artificial intelligence-based assistant designed to enhance the autonomy and daily quality of life of visually impaired individuals, who often struggle with object identification, text reading, and navigation in unfamiliar environments. Existing solutions such as screen readers or audio-based assistants facilitate access to information but frequently lead to auditory overload and raise privacy concerns in open environments. AIDEN addresses these limitations with a hybrid architecture that integrates You Only Look Once (YOLO) for real-time object detection and a Large Language and Vision Assistant (LLaVA) for scene description and Optical Character Recognition (OCR). A key novelty of the system is a continuous haptic guidance mechanism based on a Geiger-counter metaphor, which supports object centering without occupying the auditory channel, while privacy is preserved by ensuring that no personal data are stored. Empirical evaluations with visually impaired participants assessed perceived ease of use and acceptance using the Technology Acceptance Model (TAM). Results indicate high user satisfaction, particularly regarding intuitiveness and perceived autonomy. Moreover, the ``Find an Object'' achieved effective real-time performance. These findings provide promising evidence that multimodal haptic-visual feedback can improve daily usability and independence compared to traditional audio-centric methods, motivating larger-scale clinical validations.", "link": "http://arxiv.org/abs/2511.06080v3", "date": "2025-12-02", "relevancy": 2.1647, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6022}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5117}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4919}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AIDEN%3A%20Design%20and%20Pilot%20Study%20of%20an%20AI%20Assistant%20for%20the%20Visually%20Impaired&body=Title%3A%20AIDEN%3A%20Design%20and%20Pilot%20Study%20of%20an%20AI%20Assistant%20for%20the%20Visually%20Impaired%0AAuthor%3A%20Luis%20Marquez-Carpintero%20and%20Francisco%20Gomez-Donoso%20and%20Zuria%20Bauer%20and%20Bessie%20Dominguez-Dager%20and%20Alvaro%20Belmonte-Baeza%20and%20M%C3%B3nica%20Pina-Navarro%20and%20Francisco%20Morillas-Espejo%20and%20Felix%20Escalona%20and%20Miguel%20Cazorla%0AAbstract%3A%20This%20paper%20presents%20AIDEN%2C%20an%20artificial%20intelligence-based%20assistant%20designed%20to%20enhance%20the%20autonomy%20and%20daily%20quality%20of%20life%20of%20visually%20impaired%20individuals%2C%20who%20often%20struggle%20with%20object%20identification%2C%20text%20reading%2C%20and%20navigation%20in%20unfamiliar%20environments.%20Existing%20solutions%20such%20as%20screen%20readers%20or%20audio-based%20assistants%20facilitate%20access%20to%20information%20but%20frequently%20lead%20to%20auditory%20overload%20and%20raise%20privacy%20concerns%20in%20open%20environments.%20AIDEN%20addresses%20these%20limitations%20with%20a%20hybrid%20architecture%20that%20integrates%20You%20Only%20Look%20Once%20%28YOLO%29%20for%20real-time%20object%20detection%20and%20a%20Large%20Language%20and%20Vision%20Assistant%20%28LLaVA%29%20for%20scene%20description%20and%20Optical%20Character%20Recognition%20%28OCR%29.%20A%20key%20novelty%20of%20the%20system%20is%20a%20continuous%20haptic%20guidance%20mechanism%20based%20on%20a%20Geiger-counter%20metaphor%2C%20which%20supports%20object%20centering%20without%20occupying%20the%20auditory%20channel%2C%20while%20privacy%20is%20preserved%20by%20ensuring%20that%20no%20personal%20data%20are%20stored.%20Empirical%20evaluations%20with%20visually%20impaired%20participants%20assessed%20perceived%20ease%20of%20use%20and%20acceptance%20using%20the%20Technology%20Acceptance%20Model%20%28TAM%29.%20Results%20indicate%20high%20user%20satisfaction%2C%20particularly%20regarding%20intuitiveness%20and%20perceived%20autonomy.%20Moreover%2C%20the%20%60%60Find%20an%20Object%27%27%20achieved%20effective%20real-time%20performance.%20These%20findings%20provide%20promising%20evidence%20that%20multimodal%20haptic-visual%20feedback%20can%20improve%20daily%20usability%20and%20independence%20compared%20to%20traditional%20audio-centric%20methods%2C%20motivating%20larger-scale%20clinical%20validations.%0ALink%3A%20http%3A//arxiv.org/abs/2511.06080v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAIDEN%253A%2520Design%2520and%2520Pilot%2520Study%2520of%2520an%2520AI%2520Assistant%2520for%2520the%2520Visually%2520Impaired%26entry.906535625%3DLuis%2520Marquez-Carpintero%2520and%2520Francisco%2520Gomez-Donoso%2520and%2520Zuria%2520Bauer%2520and%2520Bessie%2520Dominguez-Dager%2520and%2520Alvaro%2520Belmonte-Baeza%2520and%2520M%25C3%25B3nica%2520Pina-Navarro%2520and%2520Francisco%2520Morillas-Espejo%2520and%2520Felix%2520Escalona%2520and%2520Miguel%2520Cazorla%26entry.1292438233%3DThis%2520paper%2520presents%2520AIDEN%252C%2520an%2520artificial%2520intelligence-based%2520assistant%2520designed%2520to%2520enhance%2520the%2520autonomy%2520and%2520daily%2520quality%2520of%2520life%2520of%2520visually%2520impaired%2520individuals%252C%2520who%2520often%2520struggle%2520with%2520object%2520identification%252C%2520text%2520reading%252C%2520and%2520navigation%2520in%2520unfamiliar%2520environments.%2520Existing%2520solutions%2520such%2520as%2520screen%2520readers%2520or%2520audio-based%2520assistants%2520facilitate%2520access%2520to%2520information%2520but%2520frequently%2520lead%2520to%2520auditory%2520overload%2520and%2520raise%2520privacy%2520concerns%2520in%2520open%2520environments.%2520AIDEN%2520addresses%2520these%2520limitations%2520with%2520a%2520hybrid%2520architecture%2520that%2520integrates%2520You%2520Only%2520Look%2520Once%2520%2528YOLO%2529%2520for%2520real-time%2520object%2520detection%2520and%2520a%2520Large%2520Language%2520and%2520Vision%2520Assistant%2520%2528LLaVA%2529%2520for%2520scene%2520description%2520and%2520Optical%2520Character%2520Recognition%2520%2528OCR%2529.%2520A%2520key%2520novelty%2520of%2520the%2520system%2520is%2520a%2520continuous%2520haptic%2520guidance%2520mechanism%2520based%2520on%2520a%2520Geiger-counter%2520metaphor%252C%2520which%2520supports%2520object%2520centering%2520without%2520occupying%2520the%2520auditory%2520channel%252C%2520while%2520privacy%2520is%2520preserved%2520by%2520ensuring%2520that%2520no%2520personal%2520data%2520are%2520stored.%2520Empirical%2520evaluations%2520with%2520visually%2520impaired%2520participants%2520assessed%2520perceived%2520ease%2520of%2520use%2520and%2520acceptance%2520using%2520the%2520Technology%2520Acceptance%2520Model%2520%2528TAM%2529.%2520Results%2520indicate%2520high%2520user%2520satisfaction%252C%2520particularly%2520regarding%2520intuitiveness%2520and%2520perceived%2520autonomy.%2520Moreover%252C%2520the%2520%2560%2560Find%2520an%2520Object%2527%2527%2520achieved%2520effective%2520real-time%2520performance.%2520These%2520findings%2520provide%2520promising%2520evidence%2520that%2520multimodal%2520haptic-visual%2520feedback%2520can%2520improve%2520daily%2520usability%2520and%2520independence%2520compared%2520to%2520traditional%2520audio-centric%2520methods%252C%2520motivating%2520larger-scale%2520clinical%2520validations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.06080v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AIDEN%3A%20Design%20and%20Pilot%20Study%20of%20an%20AI%20Assistant%20for%20the%20Visually%20Impaired&entry.906535625=Luis%20Marquez-Carpintero%20and%20Francisco%20Gomez-Donoso%20and%20Zuria%20Bauer%20and%20Bessie%20Dominguez-Dager%20and%20Alvaro%20Belmonte-Baeza%20and%20M%C3%B3nica%20Pina-Navarro%20and%20Francisco%20Morillas-Espejo%20and%20Felix%20Escalona%20and%20Miguel%20Cazorla&entry.1292438233=This%20paper%20presents%20AIDEN%2C%20an%20artificial%20intelligence-based%20assistant%20designed%20to%20enhance%20the%20autonomy%20and%20daily%20quality%20of%20life%20of%20visually%20impaired%20individuals%2C%20who%20often%20struggle%20with%20object%20identification%2C%20text%20reading%2C%20and%20navigation%20in%20unfamiliar%20environments.%20Existing%20solutions%20such%20as%20screen%20readers%20or%20audio-based%20assistants%20facilitate%20access%20to%20information%20but%20frequently%20lead%20to%20auditory%20overload%20and%20raise%20privacy%20concerns%20in%20open%20environments.%20AIDEN%20addresses%20these%20limitations%20with%20a%20hybrid%20architecture%20that%20integrates%20You%20Only%20Look%20Once%20%28YOLO%29%20for%20real-time%20object%20detection%20and%20a%20Large%20Language%20and%20Vision%20Assistant%20%28LLaVA%29%20for%20scene%20description%20and%20Optical%20Character%20Recognition%20%28OCR%29.%20A%20key%20novelty%20of%20the%20system%20is%20a%20continuous%20haptic%20guidance%20mechanism%20based%20on%20a%20Geiger-counter%20metaphor%2C%20which%20supports%20object%20centering%20without%20occupying%20the%20auditory%20channel%2C%20while%20privacy%20is%20preserved%20by%20ensuring%20that%20no%20personal%20data%20are%20stored.%20Empirical%20evaluations%20with%20visually%20impaired%20participants%20assessed%20perceived%20ease%20of%20use%20and%20acceptance%20using%20the%20Technology%20Acceptance%20Model%20%28TAM%29.%20Results%20indicate%20high%20user%20satisfaction%2C%20particularly%20regarding%20intuitiveness%20and%20perceived%20autonomy.%20Moreover%2C%20the%20%60%60Find%20an%20Object%27%27%20achieved%20effective%20real-time%20performance.%20These%20findings%20provide%20promising%20evidence%20that%20multimodal%20haptic-visual%20feedback%20can%20improve%20daily%20usability%20and%20independence%20compared%20to%20traditional%20audio-centric%20methods%2C%20motivating%20larger-scale%20clinical%20validations.&entry.1838667208=http%3A//arxiv.org/abs/2511.06080v3&entry.124074799=Read"},
{"title": "Beyond Greenfield: The D3 Framework for AI-Driven Productivity in Brownfield Engineering", "author": "Krishna Kumaar Sharma", "abstract": "Brownfield engineering work involving legacy systems, incomplete documentation, and fragmented architectural knowledge poses unique challenges for the effective use of large language models (LLMs). Prior research has largely focused on greenfield or synthetic tasks, leaving a gap in structured workflows for complex, context-heavy environments. This paper introduces the Discover-Define-Deliver (D3) Framework, a disciplined LLM-assisted workflow that combines role-separated prompting strategies with applied best practices for navigating ambiguity in brownfield systems. The framework incorporates a dual-agent prompting architecture in which a Builder model generates candidate outputs and a Reviewer model provides structured critique to improve reliability. I conducted an exploratory survey study with 52 software practitioners who applied the D3 workflow to real-world engineering tasks such as legacy system exploration, documentation reconstruction, and architectural refactoring. Respondents reported perceived improvements in task clarity, documentation quality, and cognitive load, along with self-estimated productivity gains. In this exploratory study, participants reported a weighted average productivity improvement of 26.9%, reduced cognitive load for approximately 77% of participants, and 83% of participants spent less time fixing or rewriting code due to better initial planning with AI. As these findings are self-reported and not derived from controlled experiments, they should be interpreted as preliminary evidence of practitioner sentiment rather than causal effects. The results highlight both the potential and limitations of structured LLM workflows for legacy engineering systems and motivate future controlled evaluations.", "link": "http://arxiv.org/abs/2512.01155v2", "date": "2025-12-02", "relevancy": 2.0538, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5135}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5134}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5134}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Greenfield%3A%20The%20D3%20Framework%20for%20AI-Driven%20Productivity%20in%20Brownfield%20Engineering&body=Title%3A%20Beyond%20Greenfield%3A%20The%20D3%20Framework%20for%20AI-Driven%20Productivity%20in%20Brownfield%20Engineering%0AAuthor%3A%20Krishna%20Kumaar%20Sharma%0AAbstract%3A%20Brownfield%20engineering%20work%20involving%20legacy%20systems%2C%20incomplete%20documentation%2C%20and%20fragmented%20architectural%20knowledge%20poses%20unique%20challenges%20for%20the%20effective%20use%20of%20large%20language%20models%20%28LLMs%29.%20Prior%20research%20has%20largely%20focused%20on%20greenfield%20or%20synthetic%20tasks%2C%20leaving%20a%20gap%20in%20structured%20workflows%20for%20complex%2C%20context-heavy%20environments.%20This%20paper%20introduces%20the%20Discover-Define-Deliver%20%28D3%29%20Framework%2C%20a%20disciplined%20LLM-assisted%20workflow%20that%20combines%20role-separated%20prompting%20strategies%20with%20applied%20best%20practices%20for%20navigating%20ambiguity%20in%20brownfield%20systems.%20The%20framework%20incorporates%20a%20dual-agent%20prompting%20architecture%20in%20which%20a%20Builder%20model%20generates%20candidate%20outputs%20and%20a%20Reviewer%20model%20provides%20structured%20critique%20to%20improve%20reliability.%20I%20conducted%20an%20exploratory%20survey%20study%20with%2052%20software%20practitioners%20who%20applied%20the%20D3%20workflow%20to%20real-world%20engineering%20tasks%20such%20as%20legacy%20system%20exploration%2C%20documentation%20reconstruction%2C%20and%20architectural%20refactoring.%20Respondents%20reported%20perceived%20improvements%20in%20task%20clarity%2C%20documentation%20quality%2C%20and%20cognitive%20load%2C%20along%20with%20self-estimated%20productivity%20gains.%20In%20this%20exploratory%20study%2C%20participants%20reported%20a%20weighted%20average%20productivity%20improvement%20of%2026.9%25%2C%20reduced%20cognitive%20load%20for%20approximately%2077%25%20of%20participants%2C%20and%2083%25%20of%20participants%20spent%20less%20time%20fixing%20or%20rewriting%20code%20due%20to%20better%20initial%20planning%20with%20AI.%20As%20these%20findings%20are%20self-reported%20and%20not%20derived%20from%20controlled%20experiments%2C%20they%20should%20be%20interpreted%20as%20preliminary%20evidence%20of%20practitioner%20sentiment%20rather%20than%20causal%20effects.%20The%20results%20highlight%20both%20the%20potential%20and%20limitations%20of%20structured%20LLM%20workflows%20for%20legacy%20engineering%20systems%20and%20motivate%20future%20controlled%20evaluations.%0ALink%3A%20http%3A//arxiv.org/abs/2512.01155v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Greenfield%253A%2520The%2520D3%2520Framework%2520for%2520AI-Driven%2520Productivity%2520in%2520Brownfield%2520Engineering%26entry.906535625%3DKrishna%2520Kumaar%2520Sharma%26entry.1292438233%3DBrownfield%2520engineering%2520work%2520involving%2520legacy%2520systems%252C%2520incomplete%2520documentation%252C%2520and%2520fragmented%2520architectural%2520knowledge%2520poses%2520unique%2520challenges%2520for%2520the%2520effective%2520use%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Prior%2520research%2520has%2520largely%2520focused%2520on%2520greenfield%2520or%2520synthetic%2520tasks%252C%2520leaving%2520a%2520gap%2520in%2520structured%2520workflows%2520for%2520complex%252C%2520context-heavy%2520environments.%2520This%2520paper%2520introduces%2520the%2520Discover-Define-Deliver%2520%2528D3%2529%2520Framework%252C%2520a%2520disciplined%2520LLM-assisted%2520workflow%2520that%2520combines%2520role-separated%2520prompting%2520strategies%2520with%2520applied%2520best%2520practices%2520for%2520navigating%2520ambiguity%2520in%2520brownfield%2520systems.%2520The%2520framework%2520incorporates%2520a%2520dual-agent%2520prompting%2520architecture%2520in%2520which%2520a%2520Builder%2520model%2520generates%2520candidate%2520outputs%2520and%2520a%2520Reviewer%2520model%2520provides%2520structured%2520critique%2520to%2520improve%2520reliability.%2520I%2520conducted%2520an%2520exploratory%2520survey%2520study%2520with%252052%2520software%2520practitioners%2520who%2520applied%2520the%2520D3%2520workflow%2520to%2520real-world%2520engineering%2520tasks%2520such%2520as%2520legacy%2520system%2520exploration%252C%2520documentation%2520reconstruction%252C%2520and%2520architectural%2520refactoring.%2520Respondents%2520reported%2520perceived%2520improvements%2520in%2520task%2520clarity%252C%2520documentation%2520quality%252C%2520and%2520cognitive%2520load%252C%2520along%2520with%2520self-estimated%2520productivity%2520gains.%2520In%2520this%2520exploratory%2520study%252C%2520participants%2520reported%2520a%2520weighted%2520average%2520productivity%2520improvement%2520of%252026.9%2525%252C%2520reduced%2520cognitive%2520load%2520for%2520approximately%252077%2525%2520of%2520participants%252C%2520and%252083%2525%2520of%2520participants%2520spent%2520less%2520time%2520fixing%2520or%2520rewriting%2520code%2520due%2520to%2520better%2520initial%2520planning%2520with%2520AI.%2520As%2520these%2520findings%2520are%2520self-reported%2520and%2520not%2520derived%2520from%2520controlled%2520experiments%252C%2520they%2520should%2520be%2520interpreted%2520as%2520preliminary%2520evidence%2520of%2520practitioner%2520sentiment%2520rather%2520than%2520causal%2520effects.%2520The%2520results%2520highlight%2520both%2520the%2520potential%2520and%2520limitations%2520of%2520structured%2520LLM%2520workflows%2520for%2520legacy%2520engineering%2520systems%2520and%2520motivate%2520future%2520controlled%2520evaluations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.01155v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Greenfield%3A%20The%20D3%20Framework%20for%20AI-Driven%20Productivity%20in%20Brownfield%20Engineering&entry.906535625=Krishna%20Kumaar%20Sharma&entry.1292438233=Brownfield%20engineering%20work%20involving%20legacy%20systems%2C%20incomplete%20documentation%2C%20and%20fragmented%20architectural%20knowledge%20poses%20unique%20challenges%20for%20the%20effective%20use%20of%20large%20language%20models%20%28LLMs%29.%20Prior%20research%20has%20largely%20focused%20on%20greenfield%20or%20synthetic%20tasks%2C%20leaving%20a%20gap%20in%20structured%20workflows%20for%20complex%2C%20context-heavy%20environments.%20This%20paper%20introduces%20the%20Discover-Define-Deliver%20%28D3%29%20Framework%2C%20a%20disciplined%20LLM-assisted%20workflow%20that%20combines%20role-separated%20prompting%20strategies%20with%20applied%20best%20practices%20for%20navigating%20ambiguity%20in%20brownfield%20systems.%20The%20framework%20incorporates%20a%20dual-agent%20prompting%20architecture%20in%20which%20a%20Builder%20model%20generates%20candidate%20outputs%20and%20a%20Reviewer%20model%20provides%20structured%20critique%20to%20improve%20reliability.%20I%20conducted%20an%20exploratory%20survey%20study%20with%2052%20software%20practitioners%20who%20applied%20the%20D3%20workflow%20to%20real-world%20engineering%20tasks%20such%20as%20legacy%20system%20exploration%2C%20documentation%20reconstruction%2C%20and%20architectural%20refactoring.%20Respondents%20reported%20perceived%20improvements%20in%20task%20clarity%2C%20documentation%20quality%2C%20and%20cognitive%20load%2C%20along%20with%20self-estimated%20productivity%20gains.%20In%20this%20exploratory%20study%2C%20participants%20reported%20a%20weighted%20average%20productivity%20improvement%20of%2026.9%25%2C%20reduced%20cognitive%20load%20for%20approximately%2077%25%20of%20participants%2C%20and%2083%25%20of%20participants%20spent%20less%20time%20fixing%20or%20rewriting%20code%20due%20to%20better%20initial%20planning%20with%20AI.%20As%20these%20findings%20are%20self-reported%20and%20not%20derived%20from%20controlled%20experiments%2C%20they%20should%20be%20interpreted%20as%20preliminary%20evidence%20of%20practitioner%20sentiment%20rather%20than%20causal%20effects.%20The%20results%20highlight%20both%20the%20potential%20and%20limitations%20of%20structured%20LLM%20workflows%20for%20legacy%20engineering%20systems%20and%20motivate%20future%20controlled%20evaluations.&entry.1838667208=http%3A//arxiv.org/abs/2512.01155v2&entry.124074799=Read"},
{"title": "Adaptive Weighted LSSVM for Multi-View Classification", "author": "Farnaz Faramarzi Lighvan and Mehrdad Asadi and Lynn Houthuys", "abstract": "Multi-view learning integrates diverse representations of the same instances to improve performance. Most existing kernel-based multi-view learning methods use fusion techniques without enforcing an explicit collaboration type across views or co-regularization which limits global collaboration. We propose AW-LSSVM, an adaptive weighted LS-SVM that promotes complementary learning by an iterative global coupling to make each view focus on hard samples of others from previous iterations. Experiments demonstrate that AW-LSSVM outperforms existing kernel-based multi-view methods on most datasets, while keeping raw features isolated, making it also suitable for privacy-preserving scenarios.", "link": "http://arxiv.org/abs/2512.02653v1", "date": "2025-12-02", "relevancy": 1.9871, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5141}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4893}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4722}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Weighted%20LSSVM%20for%20Multi-View%20Classification&body=Title%3A%20Adaptive%20Weighted%20LSSVM%20for%20Multi-View%20Classification%0AAuthor%3A%20Farnaz%20Faramarzi%20Lighvan%20and%20Mehrdad%20Asadi%20and%20Lynn%20Houthuys%0AAbstract%3A%20Multi-view%20learning%20integrates%20diverse%20representations%20of%20the%20same%20instances%20to%20improve%20performance.%20Most%20existing%20kernel-based%20multi-view%20learning%20methods%20use%20fusion%20techniques%20without%20enforcing%20an%20explicit%20collaboration%20type%20across%20views%20or%20co-regularization%20which%20limits%20global%20collaboration.%20We%20propose%20AW-LSSVM%2C%20an%20adaptive%20weighted%20LS-SVM%20that%20promotes%20complementary%20learning%20by%20an%20iterative%20global%20coupling%20to%20make%20each%20view%20focus%20on%20hard%20samples%20of%20others%20from%20previous%20iterations.%20Experiments%20demonstrate%20that%20AW-LSSVM%20outperforms%20existing%20kernel-based%20multi-view%20methods%20on%20most%20datasets%2C%20while%20keeping%20raw%20features%20isolated%2C%20making%20it%20also%20suitable%20for%20privacy-preserving%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02653v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Weighted%2520LSSVM%2520for%2520Multi-View%2520Classification%26entry.906535625%3DFarnaz%2520Faramarzi%2520Lighvan%2520and%2520Mehrdad%2520Asadi%2520and%2520Lynn%2520Houthuys%26entry.1292438233%3DMulti-view%2520learning%2520integrates%2520diverse%2520representations%2520of%2520the%2520same%2520instances%2520to%2520improve%2520performance.%2520Most%2520existing%2520kernel-based%2520multi-view%2520learning%2520methods%2520use%2520fusion%2520techniques%2520without%2520enforcing%2520an%2520explicit%2520collaboration%2520type%2520across%2520views%2520or%2520co-regularization%2520which%2520limits%2520global%2520collaboration.%2520We%2520propose%2520AW-LSSVM%252C%2520an%2520adaptive%2520weighted%2520LS-SVM%2520that%2520promotes%2520complementary%2520learning%2520by%2520an%2520iterative%2520global%2520coupling%2520to%2520make%2520each%2520view%2520focus%2520on%2520hard%2520samples%2520of%2520others%2520from%2520previous%2520iterations.%2520Experiments%2520demonstrate%2520that%2520AW-LSSVM%2520outperforms%2520existing%2520kernel-based%2520multi-view%2520methods%2520on%2520most%2520datasets%252C%2520while%2520keeping%2520raw%2520features%2520isolated%252C%2520making%2520it%2520also%2520suitable%2520for%2520privacy-preserving%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02653v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Weighted%20LSSVM%20for%20Multi-View%20Classification&entry.906535625=Farnaz%20Faramarzi%20Lighvan%20and%20Mehrdad%20Asadi%20and%20Lynn%20Houthuys&entry.1292438233=Multi-view%20learning%20integrates%20diverse%20representations%20of%20the%20same%20instances%20to%20improve%20performance.%20Most%20existing%20kernel-based%20multi-view%20learning%20methods%20use%20fusion%20techniques%20without%20enforcing%20an%20explicit%20collaboration%20type%20across%20views%20or%20co-regularization%20which%20limits%20global%20collaboration.%20We%20propose%20AW-LSSVM%2C%20an%20adaptive%20weighted%20LS-SVM%20that%20promotes%20complementary%20learning%20by%20an%20iterative%20global%20coupling%20to%20make%20each%20view%20focus%20on%20hard%20samples%20of%20others%20from%20previous%20iterations.%20Experiments%20demonstrate%20that%20AW-LSSVM%20outperforms%20existing%20kernel-based%20multi-view%20methods%20on%20most%20datasets%2C%20while%20keeping%20raw%20features%20isolated%2C%20making%20it%20also%20suitable%20for%20privacy-preserving%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2512.02653v1&entry.124074799=Read"},
{"title": "AuditCopilot: Leveraging LLMs for Fraud Detection in Double-Entry Bookkeeping", "author": "Md Abdul Kadir and Sai Suresh Macharla Vasu and Sidharth S. Nair and Daniel Sonntag", "abstract": "Auditors rely on Journal Entry Tests (JETs) to detect anomalies in tax-related ledger records, but rule-based methods generate overwhelming false positives and struggle with subtle irregularities. We investigate whether large language models (LLMs) can serve as anomaly detectors in double-entry bookkeeping. Benchmarking SoTA LLMs such as LLaMA and Gemma on both synthetic and real-world anonymized ledgers, we compare them against JETs and machine learning baselines. Our results show that LLMs consistently outperform traditional rule-based JETs and classical ML baselines, while also providing natural-language explanations that enhance interpretability. These results highlight the potential of \\textbf{AI-augmented auditing}, where human auditors collaborate with foundation models to strengthen financial integrity.", "link": "http://arxiv.org/abs/2512.02726v1", "date": "2025-12-02", "relevancy": 1.8578, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4895}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.458}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AuditCopilot%3A%20Leveraging%20LLMs%20for%20Fraud%20Detection%20in%20Double-Entry%20Bookkeeping&body=Title%3A%20AuditCopilot%3A%20Leveraging%20LLMs%20for%20Fraud%20Detection%20in%20Double-Entry%20Bookkeeping%0AAuthor%3A%20Md%20Abdul%20Kadir%20and%20Sai%20Suresh%20Macharla%20Vasu%20and%20Sidharth%20S.%20Nair%20and%20Daniel%20Sonntag%0AAbstract%3A%20Auditors%20rely%20on%20Journal%20Entry%20Tests%20%28JETs%29%20to%20detect%20anomalies%20in%20tax-related%20ledger%20records%2C%20but%20rule-based%20methods%20generate%20overwhelming%20false%20positives%20and%20struggle%20with%20subtle%20irregularities.%20We%20investigate%20whether%20large%20language%20models%20%28LLMs%29%20can%20serve%20as%20anomaly%20detectors%20in%20double-entry%20bookkeeping.%20Benchmarking%20SoTA%20LLMs%20such%20as%20LLaMA%20and%20Gemma%20on%20both%20synthetic%20and%20real-world%20anonymized%20ledgers%2C%20we%20compare%20them%20against%20JETs%20and%20machine%20learning%20baselines.%20Our%20results%20show%20that%20LLMs%20consistently%20outperform%20traditional%20rule-based%20JETs%20and%20classical%20ML%20baselines%2C%20while%20also%20providing%20natural-language%20explanations%20that%20enhance%20interpretability.%20These%20results%20highlight%20the%20potential%20of%20%5Ctextbf%7BAI-augmented%20auditing%7D%2C%20where%20human%20auditors%20collaborate%20with%20foundation%20models%20to%20strengthen%20financial%20integrity.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02726v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAuditCopilot%253A%2520Leveraging%2520LLMs%2520for%2520Fraud%2520Detection%2520in%2520Double-Entry%2520Bookkeeping%26entry.906535625%3DMd%2520Abdul%2520Kadir%2520and%2520Sai%2520Suresh%2520Macharla%2520Vasu%2520and%2520Sidharth%2520S.%2520Nair%2520and%2520Daniel%2520Sonntag%26entry.1292438233%3DAuditors%2520rely%2520on%2520Journal%2520Entry%2520Tests%2520%2528JETs%2529%2520to%2520detect%2520anomalies%2520in%2520tax-related%2520ledger%2520records%252C%2520but%2520rule-based%2520methods%2520generate%2520overwhelming%2520false%2520positives%2520and%2520struggle%2520with%2520subtle%2520irregularities.%2520We%2520investigate%2520whether%2520large%2520language%2520models%2520%2528LLMs%2529%2520can%2520serve%2520as%2520anomaly%2520detectors%2520in%2520double-entry%2520bookkeeping.%2520Benchmarking%2520SoTA%2520LLMs%2520such%2520as%2520LLaMA%2520and%2520Gemma%2520on%2520both%2520synthetic%2520and%2520real-world%2520anonymized%2520ledgers%252C%2520we%2520compare%2520them%2520against%2520JETs%2520and%2520machine%2520learning%2520baselines.%2520Our%2520results%2520show%2520that%2520LLMs%2520consistently%2520outperform%2520traditional%2520rule-based%2520JETs%2520and%2520classical%2520ML%2520baselines%252C%2520while%2520also%2520providing%2520natural-language%2520explanations%2520that%2520enhance%2520interpretability.%2520These%2520results%2520highlight%2520the%2520potential%2520of%2520%255Ctextbf%257BAI-augmented%2520auditing%257D%252C%2520where%2520human%2520auditors%2520collaborate%2520with%2520foundation%2520models%2520to%2520strengthen%2520financial%2520integrity.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02726v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AuditCopilot%3A%20Leveraging%20LLMs%20for%20Fraud%20Detection%20in%20Double-Entry%20Bookkeeping&entry.906535625=Md%20Abdul%20Kadir%20and%20Sai%20Suresh%20Macharla%20Vasu%20and%20Sidharth%20S.%20Nair%20and%20Daniel%20Sonntag&entry.1292438233=Auditors%20rely%20on%20Journal%20Entry%20Tests%20%28JETs%29%20to%20detect%20anomalies%20in%20tax-related%20ledger%20records%2C%20but%20rule-based%20methods%20generate%20overwhelming%20false%20positives%20and%20struggle%20with%20subtle%20irregularities.%20We%20investigate%20whether%20large%20language%20models%20%28LLMs%29%20can%20serve%20as%20anomaly%20detectors%20in%20double-entry%20bookkeeping.%20Benchmarking%20SoTA%20LLMs%20such%20as%20LLaMA%20and%20Gemma%20on%20both%20synthetic%20and%20real-world%20anonymized%20ledgers%2C%20we%20compare%20them%20against%20JETs%20and%20machine%20learning%20baselines.%20Our%20results%20show%20that%20LLMs%20consistently%20outperform%20traditional%20rule-based%20JETs%20and%20classical%20ML%20baselines%2C%20while%20also%20providing%20natural-language%20explanations%20that%20enhance%20interpretability.%20These%20results%20highlight%20the%20potential%20of%20%5Ctextbf%7BAI-augmented%20auditing%7D%2C%20where%20human%20auditors%20collaborate%20with%20foundation%20models%20to%20strengthen%20financial%20integrity.&entry.1838667208=http%3A//arxiv.org/abs/2512.02726v1&entry.124074799=Read"},
{"title": "Learning Multimodal Embeddings for Traffic Accident Prediction and Causal Estimation", "author": "Ziniu Zhang and Minxuan Duan and Haris N. Koutsopoulos and Hongyang R. Zhang", "abstract": "We consider analyzing traffic accident patterns using both road network data and satellite images aligned to road graph nodes. Previous work for predicting accident occurrences relies primarily on road network structural features while overlooking physical and environmental information from the road surface and its surroundings. In this work, we construct a large multimodal dataset across six U.S. states, containing nine million traffic accident records from official sources, and one million high-resolution satellite images for each node of the road network. Additionally, every node is annotated with features such as the region's weather statistics and road type (e.g., residential vs. motorway), and each edge is annotated with traffic volume information (i.e., Average Annual Daily Traffic). Utilizing this dataset, we conduct a comprehensive evaluation of multimodal learning methods that integrate both visual and network embeddings. Our findings show that integrating both data modalities improves prediction accuracy, achieving an average AUROC of $90.1\\%$, which is a $3.7\\%$ gain over graph neural network models that only utilize graph structures. With the improved embeddings, we conduct a causal analysis based on a matching estimator to estimate the key contributing factors influencing traffic accidents. We find that accident rates rise by $24\\%$ under higher precipitation, by $22\\%$ on higher-speed roads such as motorways, and by $29\\%$ due to seasonal patterns, after adjusting for other confounding factors. Ablation studies confirm that satellite imagery features are essential for achieving accurate prediction.", "link": "http://arxiv.org/abs/2512.02920v1", "date": "2025-12-02", "relevancy": 1.5903, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5717}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5201}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5135}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Multimodal%20Embeddings%20for%20Traffic%20Accident%20Prediction%20and%20Causal%20Estimation&body=Title%3A%20Learning%20Multimodal%20Embeddings%20for%20Traffic%20Accident%20Prediction%20and%20Causal%20Estimation%0AAuthor%3A%20Ziniu%20Zhang%20and%20Minxuan%20Duan%20and%20Haris%20N.%20Koutsopoulos%20and%20Hongyang%20R.%20Zhang%0AAbstract%3A%20We%20consider%20analyzing%20traffic%20accident%20patterns%20using%20both%20road%20network%20data%20and%20satellite%20images%20aligned%20to%20road%20graph%20nodes.%20Previous%20work%20for%20predicting%20accident%20occurrences%20relies%20primarily%20on%20road%20network%20structural%20features%20while%20overlooking%20physical%20and%20environmental%20information%20from%20the%20road%20surface%20and%20its%20surroundings.%20In%20this%20work%2C%20we%20construct%20a%20large%20multimodal%20dataset%20across%20six%20U.S.%20states%2C%20containing%20nine%20million%20traffic%20accident%20records%20from%20official%20sources%2C%20and%20one%20million%20high-resolution%20satellite%20images%20for%20each%20node%20of%20the%20road%20network.%20Additionally%2C%20every%20node%20is%20annotated%20with%20features%20such%20as%20the%20region%27s%20weather%20statistics%20and%20road%20type%20%28e.g.%2C%20residential%20vs.%20motorway%29%2C%20and%20each%20edge%20is%20annotated%20with%20traffic%20volume%20information%20%28i.e.%2C%20Average%20Annual%20Daily%20Traffic%29.%20Utilizing%20this%20dataset%2C%20we%20conduct%20a%20comprehensive%20evaluation%20of%20multimodal%20learning%20methods%20that%20integrate%20both%20visual%20and%20network%20embeddings.%20Our%20findings%20show%20that%20integrating%20both%20data%20modalities%20improves%20prediction%20accuracy%2C%20achieving%20an%20average%20AUROC%20of%20%2490.1%5C%25%24%2C%20which%20is%20a%20%243.7%5C%25%24%20gain%20over%20graph%20neural%20network%20models%20that%20only%20utilize%20graph%20structures.%20With%20the%20improved%20embeddings%2C%20we%20conduct%20a%20causal%20analysis%20based%20on%20a%20matching%20estimator%20to%20estimate%20the%20key%20contributing%20factors%20influencing%20traffic%20accidents.%20We%20find%20that%20accident%20rates%20rise%20by%20%2424%5C%25%24%20under%20higher%20precipitation%2C%20by%20%2422%5C%25%24%20on%20higher-speed%20roads%20such%20as%20motorways%2C%20and%20by%20%2429%5C%25%24%20due%20to%20seasonal%20patterns%2C%20after%20adjusting%20for%20other%20confounding%20factors.%20Ablation%20studies%20confirm%20that%20satellite%20imagery%20features%20are%20essential%20for%20achieving%20accurate%20prediction.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02920v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Multimodal%2520Embeddings%2520for%2520Traffic%2520Accident%2520Prediction%2520and%2520Causal%2520Estimation%26entry.906535625%3DZiniu%2520Zhang%2520and%2520Minxuan%2520Duan%2520and%2520Haris%2520N.%2520Koutsopoulos%2520and%2520Hongyang%2520R.%2520Zhang%26entry.1292438233%3DWe%2520consider%2520analyzing%2520traffic%2520accident%2520patterns%2520using%2520both%2520road%2520network%2520data%2520and%2520satellite%2520images%2520aligned%2520to%2520road%2520graph%2520nodes.%2520Previous%2520work%2520for%2520predicting%2520accident%2520occurrences%2520relies%2520primarily%2520on%2520road%2520network%2520structural%2520features%2520while%2520overlooking%2520physical%2520and%2520environmental%2520information%2520from%2520the%2520road%2520surface%2520and%2520its%2520surroundings.%2520In%2520this%2520work%252C%2520we%2520construct%2520a%2520large%2520multimodal%2520dataset%2520across%2520six%2520U.S.%2520states%252C%2520containing%2520nine%2520million%2520traffic%2520accident%2520records%2520from%2520official%2520sources%252C%2520and%2520one%2520million%2520high-resolution%2520satellite%2520images%2520for%2520each%2520node%2520of%2520the%2520road%2520network.%2520Additionally%252C%2520every%2520node%2520is%2520annotated%2520with%2520features%2520such%2520as%2520the%2520region%2527s%2520weather%2520statistics%2520and%2520road%2520type%2520%2528e.g.%252C%2520residential%2520vs.%2520motorway%2529%252C%2520and%2520each%2520edge%2520is%2520annotated%2520with%2520traffic%2520volume%2520information%2520%2528i.e.%252C%2520Average%2520Annual%2520Daily%2520Traffic%2529.%2520Utilizing%2520this%2520dataset%252C%2520we%2520conduct%2520a%2520comprehensive%2520evaluation%2520of%2520multimodal%2520learning%2520methods%2520that%2520integrate%2520both%2520visual%2520and%2520network%2520embeddings.%2520Our%2520findings%2520show%2520that%2520integrating%2520both%2520data%2520modalities%2520improves%2520prediction%2520accuracy%252C%2520achieving%2520an%2520average%2520AUROC%2520of%2520%252490.1%255C%2525%2524%252C%2520which%2520is%2520a%2520%25243.7%255C%2525%2524%2520gain%2520over%2520graph%2520neural%2520network%2520models%2520that%2520only%2520utilize%2520graph%2520structures.%2520With%2520the%2520improved%2520embeddings%252C%2520we%2520conduct%2520a%2520causal%2520analysis%2520based%2520on%2520a%2520matching%2520estimator%2520to%2520estimate%2520the%2520key%2520contributing%2520factors%2520influencing%2520traffic%2520accidents.%2520We%2520find%2520that%2520accident%2520rates%2520rise%2520by%2520%252424%255C%2525%2524%2520under%2520higher%2520precipitation%252C%2520by%2520%252422%255C%2525%2524%2520on%2520higher-speed%2520roads%2520such%2520as%2520motorways%252C%2520and%2520by%2520%252429%255C%2525%2524%2520due%2520to%2520seasonal%2520patterns%252C%2520after%2520adjusting%2520for%2520other%2520confounding%2520factors.%2520Ablation%2520studies%2520confirm%2520that%2520satellite%2520imagery%2520features%2520are%2520essential%2520for%2520achieving%2520accurate%2520prediction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02920v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Multimodal%20Embeddings%20for%20Traffic%20Accident%20Prediction%20and%20Causal%20Estimation&entry.906535625=Ziniu%20Zhang%20and%20Minxuan%20Duan%20and%20Haris%20N.%20Koutsopoulos%20and%20Hongyang%20R.%20Zhang&entry.1292438233=We%20consider%20analyzing%20traffic%20accident%20patterns%20using%20both%20road%20network%20data%20and%20satellite%20images%20aligned%20to%20road%20graph%20nodes.%20Previous%20work%20for%20predicting%20accident%20occurrences%20relies%20primarily%20on%20road%20network%20structural%20features%20while%20overlooking%20physical%20and%20environmental%20information%20from%20the%20road%20surface%20and%20its%20surroundings.%20In%20this%20work%2C%20we%20construct%20a%20large%20multimodal%20dataset%20across%20six%20U.S.%20states%2C%20containing%20nine%20million%20traffic%20accident%20records%20from%20official%20sources%2C%20and%20one%20million%20high-resolution%20satellite%20images%20for%20each%20node%20of%20the%20road%20network.%20Additionally%2C%20every%20node%20is%20annotated%20with%20features%20such%20as%20the%20region%27s%20weather%20statistics%20and%20road%20type%20%28e.g.%2C%20residential%20vs.%20motorway%29%2C%20and%20each%20edge%20is%20annotated%20with%20traffic%20volume%20information%20%28i.e.%2C%20Average%20Annual%20Daily%20Traffic%29.%20Utilizing%20this%20dataset%2C%20we%20conduct%20a%20comprehensive%20evaluation%20of%20multimodal%20learning%20methods%20that%20integrate%20both%20visual%20and%20network%20embeddings.%20Our%20findings%20show%20that%20integrating%20both%20data%20modalities%20improves%20prediction%20accuracy%2C%20achieving%20an%20average%20AUROC%20of%20%2490.1%5C%25%24%2C%20which%20is%20a%20%243.7%5C%25%24%20gain%20over%20graph%20neural%20network%20models%20that%20only%20utilize%20graph%20structures.%20With%20the%20improved%20embeddings%2C%20we%20conduct%20a%20causal%20analysis%20based%20on%20a%20matching%20estimator%20to%20estimate%20the%20key%20contributing%20factors%20influencing%20traffic%20accidents.%20We%20find%20that%20accident%20rates%20rise%20by%20%2424%5C%25%24%20under%20higher%20precipitation%2C%20by%20%2422%5C%25%24%20on%20higher-speed%20roads%20such%20as%20motorways%2C%20and%20by%20%2429%5C%25%24%20due%20to%20seasonal%20patterns%2C%20after%20adjusting%20for%20other%20confounding%20factors.%20Ablation%20studies%20confirm%20that%20satellite%20imagery%20features%20are%20essential%20for%20achieving%20accurate%20prediction.&entry.1838667208=http%3A//arxiv.org/abs/2512.02920v1&entry.124074799=Read"},
{"title": "Limitations of Using Identical Distributions for Training and Testing When Learning Boolean Functions", "author": "Jordi P\u00e9rez-Guijarro", "abstract": "When the distributions of the training and test data do not coincide, the problem of understanding generalization becomes considerably more complex, prompting a variety of questions. Prior work has shown that, for some fixed learning methods, there are scenarios where training on a distribution different from the test distribution improves generalization. However, these results do not account for the possibility of choosing, for each training distribution, the optimal learning algorithm, leaving open whether the observed benefits stem from the mismatch itself or from suboptimality of the learner. In this work, we address this question in full generality. That is, we study whether it is always optimal for the training distribution to be identical to the test distribution when the learner is allowed to be optimally adapted to the training distribution. Surprisingly, assuming the existence of one-way functions, we find that the answer is no. That is, matching distributions is not always the best scenario. Nonetheless, we also show that when certain regularities are imposed on the target functions, the standard conclusion is recovered in the case of the uniform distribution.", "link": "http://arxiv.org/abs/2512.00791v2", "date": "2025-12-02", "relevancy": 2.02, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4294}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.3993}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.3833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Limitations%20of%20Using%20Identical%20Distributions%20for%20Training%20and%20Testing%20When%20Learning%20Boolean%20Functions&body=Title%3A%20Limitations%20of%20Using%20Identical%20Distributions%20for%20Training%20and%20Testing%20When%20Learning%20Boolean%20Functions%0AAuthor%3A%20Jordi%20P%C3%A9rez-Guijarro%0AAbstract%3A%20When%20the%20distributions%20of%20the%20training%20and%20test%20data%20do%20not%20coincide%2C%20the%20problem%20of%20understanding%20generalization%20becomes%20considerably%20more%20complex%2C%20prompting%20a%20variety%20of%20questions.%20Prior%20work%20has%20shown%20that%2C%20for%20some%20fixed%20learning%20methods%2C%20there%20are%20scenarios%20where%20training%20on%20a%20distribution%20different%20from%20the%20test%20distribution%20improves%20generalization.%20However%2C%20these%20results%20do%20not%20account%20for%20the%20possibility%20of%20choosing%2C%20for%20each%20training%20distribution%2C%20the%20optimal%20learning%20algorithm%2C%20leaving%20open%20whether%20the%20observed%20benefits%20stem%20from%20the%20mismatch%20itself%20or%20from%20suboptimality%20of%20the%20learner.%20In%20this%20work%2C%20we%20address%20this%20question%20in%20full%20generality.%20That%20is%2C%20we%20study%20whether%20it%20is%20always%20optimal%20for%20the%20training%20distribution%20to%20be%20identical%20to%20the%20test%20distribution%20when%20the%20learner%20is%20allowed%20to%20be%20optimally%20adapted%20to%20the%20training%20distribution.%20Surprisingly%2C%20assuming%20the%20existence%20of%20one-way%20functions%2C%20we%20find%20that%20the%20answer%20is%20no.%20That%20is%2C%20matching%20distributions%20is%20not%20always%20the%20best%20scenario.%20Nonetheless%2C%20we%20also%20show%20that%20when%20certain%20regularities%20are%20imposed%20on%20the%20target%20functions%2C%20the%20standard%20conclusion%20is%20recovered%20in%20the%20case%20of%20the%20uniform%20distribution.%0ALink%3A%20http%3A//arxiv.org/abs/2512.00791v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLimitations%2520of%2520Using%2520Identical%2520Distributions%2520for%2520Training%2520and%2520Testing%2520When%2520Learning%2520Boolean%2520Functions%26entry.906535625%3DJordi%2520P%25C3%25A9rez-Guijarro%26entry.1292438233%3DWhen%2520the%2520distributions%2520of%2520the%2520training%2520and%2520test%2520data%2520do%2520not%2520coincide%252C%2520the%2520problem%2520of%2520understanding%2520generalization%2520becomes%2520considerably%2520more%2520complex%252C%2520prompting%2520a%2520variety%2520of%2520questions.%2520Prior%2520work%2520has%2520shown%2520that%252C%2520for%2520some%2520fixed%2520learning%2520methods%252C%2520there%2520are%2520scenarios%2520where%2520training%2520on%2520a%2520distribution%2520different%2520from%2520the%2520test%2520distribution%2520improves%2520generalization.%2520However%252C%2520these%2520results%2520do%2520not%2520account%2520for%2520the%2520possibility%2520of%2520choosing%252C%2520for%2520each%2520training%2520distribution%252C%2520the%2520optimal%2520learning%2520algorithm%252C%2520leaving%2520open%2520whether%2520the%2520observed%2520benefits%2520stem%2520from%2520the%2520mismatch%2520itself%2520or%2520from%2520suboptimality%2520of%2520the%2520learner.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520question%2520in%2520full%2520generality.%2520That%2520is%252C%2520we%2520study%2520whether%2520it%2520is%2520always%2520optimal%2520for%2520the%2520training%2520distribution%2520to%2520be%2520identical%2520to%2520the%2520test%2520distribution%2520when%2520the%2520learner%2520is%2520allowed%2520to%2520be%2520optimally%2520adapted%2520to%2520the%2520training%2520distribution.%2520Surprisingly%252C%2520assuming%2520the%2520existence%2520of%2520one-way%2520functions%252C%2520we%2520find%2520that%2520the%2520answer%2520is%2520no.%2520That%2520is%252C%2520matching%2520distributions%2520is%2520not%2520always%2520the%2520best%2520scenario.%2520Nonetheless%252C%2520we%2520also%2520show%2520that%2520when%2520certain%2520regularities%2520are%2520imposed%2520on%2520the%2520target%2520functions%252C%2520the%2520standard%2520conclusion%2520is%2520recovered%2520in%2520the%2520case%2520of%2520the%2520uniform%2520distribution.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.00791v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Limitations%20of%20Using%20Identical%20Distributions%20for%20Training%20and%20Testing%20When%20Learning%20Boolean%20Functions&entry.906535625=Jordi%20P%C3%A9rez-Guijarro&entry.1292438233=When%20the%20distributions%20of%20the%20training%20and%20test%20data%20do%20not%20coincide%2C%20the%20problem%20of%20understanding%20generalization%20becomes%20considerably%20more%20complex%2C%20prompting%20a%20variety%20of%20questions.%20Prior%20work%20has%20shown%20that%2C%20for%20some%20fixed%20learning%20methods%2C%20there%20are%20scenarios%20where%20training%20on%20a%20distribution%20different%20from%20the%20test%20distribution%20improves%20generalization.%20However%2C%20these%20results%20do%20not%20account%20for%20the%20possibility%20of%20choosing%2C%20for%20each%20training%20distribution%2C%20the%20optimal%20learning%20algorithm%2C%20leaving%20open%20whether%20the%20observed%20benefits%20stem%20from%20the%20mismatch%20itself%20or%20from%20suboptimality%20of%20the%20learner.%20In%20this%20work%2C%20we%20address%20this%20question%20in%20full%20generality.%20That%20is%2C%20we%20study%20whether%20it%20is%20always%20optimal%20for%20the%20training%20distribution%20to%20be%20identical%20to%20the%20test%20distribution%20when%20the%20learner%20is%20allowed%20to%20be%20optimally%20adapted%20to%20the%20training%20distribution.%20Surprisingly%2C%20assuming%20the%20existence%20of%20one-way%20functions%2C%20we%20find%20that%20the%20answer%20is%20no.%20That%20is%2C%20matching%20distributions%20is%20not%20always%20the%20best%20scenario.%20Nonetheless%2C%20we%20also%20show%20that%20when%20certain%20regularities%20are%20imposed%20on%20the%20target%20functions%2C%20the%20standard%20conclusion%20is%20recovered%20in%20the%20case%20of%20the%20uniform%20distribution.&entry.1838667208=http%3A//arxiv.org/abs/2512.00791v2&entry.124074799=Read"},
{"title": "Experimental Characterization of Fingertip Trajectory following for a 3-DoF Series-Parallel Hybrid Robotic Finger", "author": "Nicholas Baiata and Nilanjan Chakraborty", "abstract": "Task-space control of robotic fingers is a critical enabler of dexterous manipulation, as manipulation objectives are most naturally specified in terms of fingertip motions and applied forces rather than individual joint angles. While task-space planning and control have been extensively studied for larger, arm-scale manipulators, demonstrations of precise task-space trajectory tracking in compact, multi-DoF robotic fingers remain scarce. In this paper, we present the physical prototyping and experimental characterization of a three-degree-of-freedom, linkage-driven, series-parallel robotic finger with analytic forward kinematics and a closed-form Jacobian. A resolved motion rate control (RMRC) scheme is implemented to achieve closed-loop task-space trajectory tracking. We experimentally evaluate the fingertip tracking performance across a variety of trajectories, including straight lines, circles, and more complex curves, and report millimeter-level accuracy. To the best of our knowledge, this work provides one of the first systematic experimental demonstrations of precise task-space trajectory tracking in a linkage-driven robotic finger, thereby establishing a benchmark for future designs aimed at dexterous in-hand manipulation.", "link": "http://arxiv.org/abs/2512.02951v1", "date": "2025-12-02", "relevancy": 2.1422, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5431}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5305}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Experimental%20Characterization%20of%20Fingertip%20Trajectory%20following%20for%20a%203-DoF%20Series-Parallel%20Hybrid%20Robotic%20Finger&body=Title%3A%20Experimental%20Characterization%20of%20Fingertip%20Trajectory%20following%20for%20a%203-DoF%20Series-Parallel%20Hybrid%20Robotic%20Finger%0AAuthor%3A%20Nicholas%20Baiata%20and%20Nilanjan%20Chakraborty%0AAbstract%3A%20Task-space%20control%20of%20robotic%20fingers%20is%20a%20critical%20enabler%20of%20dexterous%20manipulation%2C%20as%20manipulation%20objectives%20are%20most%20naturally%20specified%20in%20terms%20of%20fingertip%20motions%20and%20applied%20forces%20rather%20than%20individual%20joint%20angles.%20While%20task-space%20planning%20and%20control%20have%20been%20extensively%20studied%20for%20larger%2C%20arm-scale%20manipulators%2C%20demonstrations%20of%20precise%20task-space%20trajectory%20tracking%20in%20compact%2C%20multi-DoF%20robotic%20fingers%20remain%20scarce.%20In%20this%20paper%2C%20we%20present%20the%20physical%20prototyping%20and%20experimental%20characterization%20of%20a%20three-degree-of-freedom%2C%20linkage-driven%2C%20series-parallel%20robotic%20finger%20with%20analytic%20forward%20kinematics%20and%20a%20closed-form%20Jacobian.%20A%20resolved%20motion%20rate%20control%20%28RMRC%29%20scheme%20is%20implemented%20to%20achieve%20closed-loop%20task-space%20trajectory%20tracking.%20We%20experimentally%20evaluate%20the%20fingertip%20tracking%20performance%20across%20a%20variety%20of%20trajectories%2C%20including%20straight%20lines%2C%20circles%2C%20and%20more%20complex%20curves%2C%20and%20report%20millimeter-level%20accuracy.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20work%20provides%20one%20of%20the%20first%20systematic%20experimental%20demonstrations%20of%20precise%20task-space%20trajectory%20tracking%20in%20a%20linkage-driven%20robotic%20finger%2C%20thereby%20establishing%20a%20benchmark%20for%20future%20designs%20aimed%20at%20dexterous%20in-hand%20manipulation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02951v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExperimental%2520Characterization%2520of%2520Fingertip%2520Trajectory%2520following%2520for%2520a%25203-DoF%2520Series-Parallel%2520Hybrid%2520Robotic%2520Finger%26entry.906535625%3DNicholas%2520Baiata%2520and%2520Nilanjan%2520Chakraborty%26entry.1292438233%3DTask-space%2520control%2520of%2520robotic%2520fingers%2520is%2520a%2520critical%2520enabler%2520of%2520dexterous%2520manipulation%252C%2520as%2520manipulation%2520objectives%2520are%2520most%2520naturally%2520specified%2520in%2520terms%2520of%2520fingertip%2520motions%2520and%2520applied%2520forces%2520rather%2520than%2520individual%2520joint%2520angles.%2520While%2520task-space%2520planning%2520and%2520control%2520have%2520been%2520extensively%2520studied%2520for%2520larger%252C%2520arm-scale%2520manipulators%252C%2520demonstrations%2520of%2520precise%2520task-space%2520trajectory%2520tracking%2520in%2520compact%252C%2520multi-DoF%2520robotic%2520fingers%2520remain%2520scarce.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520physical%2520prototyping%2520and%2520experimental%2520characterization%2520of%2520a%2520three-degree-of-freedom%252C%2520linkage-driven%252C%2520series-parallel%2520robotic%2520finger%2520with%2520analytic%2520forward%2520kinematics%2520and%2520a%2520closed-form%2520Jacobian.%2520A%2520resolved%2520motion%2520rate%2520control%2520%2528RMRC%2529%2520scheme%2520is%2520implemented%2520to%2520achieve%2520closed-loop%2520task-space%2520trajectory%2520tracking.%2520We%2520experimentally%2520evaluate%2520the%2520fingertip%2520tracking%2520performance%2520across%2520a%2520variety%2520of%2520trajectories%252C%2520including%2520straight%2520lines%252C%2520circles%252C%2520and%2520more%2520complex%2520curves%252C%2520and%2520report%2520millimeter-level%2520accuracy.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520work%2520provides%2520one%2520of%2520the%2520first%2520systematic%2520experimental%2520demonstrations%2520of%2520precise%2520task-space%2520trajectory%2520tracking%2520in%2520a%2520linkage-driven%2520robotic%2520finger%252C%2520thereby%2520establishing%2520a%2520benchmark%2520for%2520future%2520designs%2520aimed%2520at%2520dexterous%2520in-hand%2520manipulation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02951v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Experimental%20Characterization%20of%20Fingertip%20Trajectory%20following%20for%20a%203-DoF%20Series-Parallel%20Hybrid%20Robotic%20Finger&entry.906535625=Nicholas%20Baiata%20and%20Nilanjan%20Chakraborty&entry.1292438233=Task-space%20control%20of%20robotic%20fingers%20is%20a%20critical%20enabler%20of%20dexterous%20manipulation%2C%20as%20manipulation%20objectives%20are%20most%20naturally%20specified%20in%20terms%20of%20fingertip%20motions%20and%20applied%20forces%20rather%20than%20individual%20joint%20angles.%20While%20task-space%20planning%20and%20control%20have%20been%20extensively%20studied%20for%20larger%2C%20arm-scale%20manipulators%2C%20demonstrations%20of%20precise%20task-space%20trajectory%20tracking%20in%20compact%2C%20multi-DoF%20robotic%20fingers%20remain%20scarce.%20In%20this%20paper%2C%20we%20present%20the%20physical%20prototyping%20and%20experimental%20characterization%20of%20a%20three-degree-of-freedom%2C%20linkage-driven%2C%20series-parallel%20robotic%20finger%20with%20analytic%20forward%20kinematics%20and%20a%20closed-form%20Jacobian.%20A%20resolved%20motion%20rate%20control%20%28RMRC%29%20scheme%20is%20implemented%20to%20achieve%20closed-loop%20task-space%20trajectory%20tracking.%20We%20experimentally%20evaluate%20the%20fingertip%20tracking%20performance%20across%20a%20variety%20of%20trajectories%2C%20including%20straight%20lines%2C%20circles%2C%20and%20more%20complex%20curves%2C%20and%20report%20millimeter-level%20accuracy.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20work%20provides%20one%20of%20the%20first%20systematic%20experimental%20demonstrations%20of%20precise%20task-space%20trajectory%20tracking%20in%20a%20linkage-driven%20robotic%20finger%2C%20thereby%20establishing%20a%20benchmark%20for%20future%20designs%20aimed%20at%20dexterous%20in-hand%20manipulation.&entry.1838667208=http%3A//arxiv.org/abs/2512.02951v1&entry.124074799=Read"},
{"title": "Towards a fully differentiable digital twin for solar cells", "author": "Marie Louise Schubert and Houssam Metni and Jan David Fischbach and Benedikt Zerulla and Marjan Krsti\u0107 and Ulrich W. Paetzold and Seyedamir Orooji and Olivier J. J. Ronsin and Yasin Ameslon and Jens Harting and Thomas Kirchartz and Sandheep Ravishankar and Chris Dreessen and Eunchi Kim and Christian Sprau and Mohamed Hussein and Alexander Colsmann and Karen Forberich and Klaus J\u00e4ger and Pascal Friederich and Carsten Rockstuhl", "abstract": "Maximizing energy yield (EY) - the total electric energy generated by a solar cell within a year at a specific location - is crucial in photovoltaics (PV), especially for emerging technologies. Computational methods provide the necessary insights and guidance for future research. However, existing simulations typically focus on only isolated aspects of solar cells. This lack of consistency highlights the need for a framework unifying all computational levels, from material to cell properties, for accurate prediction and optimization of EY prediction. To address this challenge, a differentiable digital twin, Sol(Di)$^2$T, is introduced to enable comprehensive end-to-end optimization of solar cells. The workflow starts with material properties and morphological processing parameters, followed by optical and electrical simulations. Finally, climatic conditions and geographic location are incorporated to predict the EY. Each step is either intrinsically differentiable or replaced with a machine-learned surrogate model, enabling not only accurate EY prediction but also gradient-based optimization with respect to input parameters. Consequently, Sol(Di)$^2$T extends EY predictions to previously unexplored conditions. Demonstrated for an organic solar cell, the proposed framework marks a significant step towards tailoring solar cells for specific applications while ensuring maximal performance.", "link": "http://arxiv.org/abs/2512.02904v1", "date": "2025-12-02", "relevancy": 0.9062, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4636}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.4534}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20fully%20differentiable%20digital%20twin%20for%20solar%20cells&body=Title%3A%20Towards%20a%20fully%20differentiable%20digital%20twin%20for%20solar%20cells%0AAuthor%3A%20Marie%20Louise%20Schubert%20and%20Houssam%20Metni%20and%20Jan%20David%20Fischbach%20and%20Benedikt%20Zerulla%20and%20Marjan%20Krsti%C4%87%20and%20Ulrich%20W.%20Paetzold%20and%20Seyedamir%20Orooji%20and%20Olivier%20J.%20J.%20Ronsin%20and%20Yasin%20Ameslon%20and%20Jens%20Harting%20and%20Thomas%20Kirchartz%20and%20Sandheep%20Ravishankar%20and%20Chris%20Dreessen%20and%20Eunchi%20Kim%20and%20Christian%20Sprau%20and%20Mohamed%20Hussein%20and%20Alexander%20Colsmann%20and%20Karen%20Forberich%20and%20Klaus%20J%C3%A4ger%20and%20Pascal%20Friederich%20and%20Carsten%20Rockstuhl%0AAbstract%3A%20Maximizing%20energy%20yield%20%28EY%29%20-%20the%20total%20electric%20energy%20generated%20by%20a%20solar%20cell%20within%20a%20year%20at%20a%20specific%20location%20-%20is%20crucial%20in%20photovoltaics%20%28PV%29%2C%20especially%20for%20emerging%20technologies.%20Computational%20methods%20provide%20the%20necessary%20insights%20and%20guidance%20for%20future%20research.%20However%2C%20existing%20simulations%20typically%20focus%20on%20only%20isolated%20aspects%20of%20solar%20cells.%20This%20lack%20of%20consistency%20highlights%20the%20need%20for%20a%20framework%20unifying%20all%20computational%20levels%2C%20from%20material%20to%20cell%20properties%2C%20for%20accurate%20prediction%20and%20optimization%20of%20EY%20prediction.%20To%20address%20this%20challenge%2C%20a%20differentiable%20digital%20twin%2C%20Sol%28Di%29%24%5E2%24T%2C%20is%20introduced%20to%20enable%20comprehensive%20end-to-end%20optimization%20of%20solar%20cells.%20The%20workflow%20starts%20with%20material%20properties%20and%20morphological%20processing%20parameters%2C%20followed%20by%20optical%20and%20electrical%20simulations.%20Finally%2C%20climatic%20conditions%20and%20geographic%20location%20are%20incorporated%20to%20predict%20the%20EY.%20Each%20step%20is%20either%20intrinsically%20differentiable%20or%20replaced%20with%20a%20machine-learned%20surrogate%20model%2C%20enabling%20not%20only%20accurate%20EY%20prediction%20but%20also%20gradient-based%20optimization%20with%20respect%20to%20input%20parameters.%20Consequently%2C%20Sol%28Di%29%24%5E2%24T%20extends%20EY%20predictions%20to%20previously%20unexplored%20conditions.%20Demonstrated%20for%20an%20organic%20solar%20cell%2C%20the%20proposed%20framework%20marks%20a%20significant%20step%20towards%20tailoring%20solar%20cells%20for%20specific%20applications%20while%20ensuring%20maximal%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02904v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520a%2520fully%2520differentiable%2520digital%2520twin%2520for%2520solar%2520cells%26entry.906535625%3DMarie%2520Louise%2520Schubert%2520and%2520Houssam%2520Metni%2520and%2520Jan%2520David%2520Fischbach%2520and%2520Benedikt%2520Zerulla%2520and%2520Marjan%2520Krsti%25C4%2587%2520and%2520Ulrich%2520W.%2520Paetzold%2520and%2520Seyedamir%2520Orooji%2520and%2520Olivier%2520J.%2520J.%2520Ronsin%2520and%2520Yasin%2520Ameslon%2520and%2520Jens%2520Harting%2520and%2520Thomas%2520Kirchartz%2520and%2520Sandheep%2520Ravishankar%2520and%2520Chris%2520Dreessen%2520and%2520Eunchi%2520Kim%2520and%2520Christian%2520Sprau%2520and%2520Mohamed%2520Hussein%2520and%2520Alexander%2520Colsmann%2520and%2520Karen%2520Forberich%2520and%2520Klaus%2520J%25C3%25A4ger%2520and%2520Pascal%2520Friederich%2520and%2520Carsten%2520Rockstuhl%26entry.1292438233%3DMaximizing%2520energy%2520yield%2520%2528EY%2529%2520-%2520the%2520total%2520electric%2520energy%2520generated%2520by%2520a%2520solar%2520cell%2520within%2520a%2520year%2520at%2520a%2520specific%2520location%2520-%2520is%2520crucial%2520in%2520photovoltaics%2520%2528PV%2529%252C%2520especially%2520for%2520emerging%2520technologies.%2520Computational%2520methods%2520provide%2520the%2520necessary%2520insights%2520and%2520guidance%2520for%2520future%2520research.%2520However%252C%2520existing%2520simulations%2520typically%2520focus%2520on%2520only%2520isolated%2520aspects%2520of%2520solar%2520cells.%2520This%2520lack%2520of%2520consistency%2520highlights%2520the%2520need%2520for%2520a%2520framework%2520unifying%2520all%2520computational%2520levels%252C%2520from%2520material%2520to%2520cell%2520properties%252C%2520for%2520accurate%2520prediction%2520and%2520optimization%2520of%2520EY%2520prediction.%2520To%2520address%2520this%2520challenge%252C%2520a%2520differentiable%2520digital%2520twin%252C%2520Sol%2528Di%2529%2524%255E2%2524T%252C%2520is%2520introduced%2520to%2520enable%2520comprehensive%2520end-to-end%2520optimization%2520of%2520solar%2520cells.%2520The%2520workflow%2520starts%2520with%2520material%2520properties%2520and%2520morphological%2520processing%2520parameters%252C%2520followed%2520by%2520optical%2520and%2520electrical%2520simulations.%2520Finally%252C%2520climatic%2520conditions%2520and%2520geographic%2520location%2520are%2520incorporated%2520to%2520predict%2520the%2520EY.%2520Each%2520step%2520is%2520either%2520intrinsically%2520differentiable%2520or%2520replaced%2520with%2520a%2520machine-learned%2520surrogate%2520model%252C%2520enabling%2520not%2520only%2520accurate%2520EY%2520prediction%2520but%2520also%2520gradient-based%2520optimization%2520with%2520respect%2520to%2520input%2520parameters.%2520Consequently%252C%2520Sol%2528Di%2529%2524%255E2%2524T%2520extends%2520EY%2520predictions%2520to%2520previously%2520unexplored%2520conditions.%2520Demonstrated%2520for%2520an%2520organic%2520solar%2520cell%252C%2520the%2520proposed%2520framework%2520marks%2520a%2520significant%2520step%2520towards%2520tailoring%2520solar%2520cells%2520for%2520specific%2520applications%2520while%2520ensuring%2520maximal%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02904v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20fully%20differentiable%20digital%20twin%20for%20solar%20cells&entry.906535625=Marie%20Louise%20Schubert%20and%20Houssam%20Metni%20and%20Jan%20David%20Fischbach%20and%20Benedikt%20Zerulla%20and%20Marjan%20Krsti%C4%87%20and%20Ulrich%20W.%20Paetzold%20and%20Seyedamir%20Orooji%20and%20Olivier%20J.%20J.%20Ronsin%20and%20Yasin%20Ameslon%20and%20Jens%20Harting%20and%20Thomas%20Kirchartz%20and%20Sandheep%20Ravishankar%20and%20Chris%20Dreessen%20and%20Eunchi%20Kim%20and%20Christian%20Sprau%20and%20Mohamed%20Hussein%20and%20Alexander%20Colsmann%20and%20Karen%20Forberich%20and%20Klaus%20J%C3%A4ger%20and%20Pascal%20Friederich%20and%20Carsten%20Rockstuhl&entry.1292438233=Maximizing%20energy%20yield%20%28EY%29%20-%20the%20total%20electric%20energy%20generated%20by%20a%20solar%20cell%20within%20a%20year%20at%20a%20specific%20location%20-%20is%20crucial%20in%20photovoltaics%20%28PV%29%2C%20especially%20for%20emerging%20technologies.%20Computational%20methods%20provide%20the%20necessary%20insights%20and%20guidance%20for%20future%20research.%20However%2C%20existing%20simulations%20typically%20focus%20on%20only%20isolated%20aspects%20of%20solar%20cells.%20This%20lack%20of%20consistency%20highlights%20the%20need%20for%20a%20framework%20unifying%20all%20computational%20levels%2C%20from%20material%20to%20cell%20properties%2C%20for%20accurate%20prediction%20and%20optimization%20of%20EY%20prediction.%20To%20address%20this%20challenge%2C%20a%20differentiable%20digital%20twin%2C%20Sol%28Di%29%24%5E2%24T%2C%20is%20introduced%20to%20enable%20comprehensive%20end-to-end%20optimization%20of%20solar%20cells.%20The%20workflow%20starts%20with%20material%20properties%20and%20morphological%20processing%20parameters%2C%20followed%20by%20optical%20and%20electrical%20simulations.%20Finally%2C%20climatic%20conditions%20and%20geographic%20location%20are%20incorporated%20to%20predict%20the%20EY.%20Each%20step%20is%20either%20intrinsically%20differentiable%20or%20replaced%20with%20a%20machine-learned%20surrogate%20model%2C%20enabling%20not%20only%20accurate%20EY%20prediction%20but%20also%20gradient-based%20optimization%20with%20respect%20to%20input%20parameters.%20Consequently%2C%20Sol%28Di%29%24%5E2%24T%20extends%20EY%20predictions%20to%20previously%20unexplored%20conditions.%20Demonstrated%20for%20an%20organic%20solar%20cell%2C%20the%20proposed%20framework%20marks%20a%20significant%20step%20towards%20tailoring%20solar%20cells%20for%20specific%20applications%20while%20ensuring%20maximal%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2512.02904v1&entry.124074799=Read"},
{"title": "Hidden in Plain Text: Emergence & Mitigation of Steganographic Collusion in LLMs", "author": "Yohan Mathew and Ollie Matthews and Robert McCarthy and Joan Velja and Christian Schroeder de Witt and Dylan Cope and Nandi Schoots", "abstract": "The rapid proliferation of frontier model agents promises significant societal advances but also raises concerns about systemic risks arising from unsafe interactions. Collusion to the disadvantage of others has been identified as a central form of undesirable agent cooperation. The use of information hiding (steganography) in agent communications could render such collusion practically undetectable. This underscores the need for investigations into the possibility of such behaviours emerging and the robustness corresponding countermeasures. To investigate this problem we design two approaches -- a gradient-based reinforcement learning (GBRL) method and an in-context reinforcement learning (ICRL) method -- for reliably eliciting sophisticated LLM-generated linguistic text steganography.\n  We demonstrate, for the first time, that unintended steganographic collusion in LLMs can arise due to mispecified reward incentives during training. Additionally, we find that standard mitigations -- both passive oversight of model outputs and active mitigation through communication paraphrasing -- are not fully effective at preventing this steganographic communication. Our findings imply that (i) emergence of steganographic collusion is a plausible concern that should be monitored and researched, and (ii) preventing emergence may require innovation in mitigation techniques.", "link": "http://arxiv.org/abs/2410.03768v2", "date": "2025-12-02", "relevancy": 1.8925, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4769}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4717}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hidden%20in%20Plain%20Text%3A%20Emergence%20%26%20Mitigation%20of%20Steganographic%20Collusion%20in%20LLMs&body=Title%3A%20Hidden%20in%20Plain%20Text%3A%20Emergence%20%26%20Mitigation%20of%20Steganographic%20Collusion%20in%20LLMs%0AAuthor%3A%20Yohan%20Mathew%20and%20Ollie%20Matthews%20and%20Robert%20McCarthy%20and%20Joan%20Velja%20and%20Christian%20Schroeder%20de%20Witt%20and%20Dylan%20Cope%20and%20Nandi%20Schoots%0AAbstract%3A%20The%20rapid%20proliferation%20of%20frontier%20model%20agents%20promises%20significant%20societal%20advances%20but%20also%20raises%20concerns%20about%20systemic%20risks%20arising%20from%20unsafe%20interactions.%20Collusion%20to%20the%20disadvantage%20of%20others%20has%20been%20identified%20as%20a%20central%20form%20of%20undesirable%20agent%20cooperation.%20The%20use%20of%20information%20hiding%20%28steganography%29%20in%20agent%20communications%20could%20render%20such%20collusion%20practically%20undetectable.%20This%20underscores%20the%20need%20for%20investigations%20into%20the%20possibility%20of%20such%20behaviours%20emerging%20and%20the%20robustness%20corresponding%20countermeasures.%20To%20investigate%20this%20problem%20we%20design%20two%20approaches%20--%20a%20gradient-based%20reinforcement%20learning%20%28GBRL%29%20method%20and%20an%20in-context%20reinforcement%20learning%20%28ICRL%29%20method%20--%20for%20reliably%20eliciting%20sophisticated%20LLM-generated%20linguistic%20text%20steganography.%0A%20%20We%20demonstrate%2C%20for%20the%20first%20time%2C%20that%20unintended%20steganographic%20collusion%20in%20LLMs%20can%20arise%20due%20to%20mispecified%20reward%20incentives%20during%20training.%20Additionally%2C%20we%20find%20that%20standard%20mitigations%20--%20both%20passive%20oversight%20of%20model%20outputs%20and%20active%20mitigation%20through%20communication%20paraphrasing%20--%20are%20not%20fully%20effective%20at%20preventing%20this%20steganographic%20communication.%20Our%20findings%20imply%20that%20%28i%29%20emergence%20of%20steganographic%20collusion%20is%20a%20plausible%20concern%20that%20should%20be%20monitored%20and%20researched%2C%20and%20%28ii%29%20preventing%20emergence%20may%20require%20innovation%20in%20mitigation%20techniques.%0ALink%3A%20http%3A//arxiv.org/abs/2410.03768v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHidden%2520in%2520Plain%2520Text%253A%2520Emergence%2520%2526%2520Mitigation%2520of%2520Steganographic%2520Collusion%2520in%2520LLMs%26entry.906535625%3DYohan%2520Mathew%2520and%2520Ollie%2520Matthews%2520and%2520Robert%2520McCarthy%2520and%2520Joan%2520Velja%2520and%2520Christian%2520Schroeder%2520de%2520Witt%2520and%2520Dylan%2520Cope%2520and%2520Nandi%2520Schoots%26entry.1292438233%3DThe%2520rapid%2520proliferation%2520of%2520frontier%2520model%2520agents%2520promises%2520significant%2520societal%2520advances%2520but%2520also%2520raises%2520concerns%2520about%2520systemic%2520risks%2520arising%2520from%2520unsafe%2520interactions.%2520Collusion%2520to%2520the%2520disadvantage%2520of%2520others%2520has%2520been%2520identified%2520as%2520a%2520central%2520form%2520of%2520undesirable%2520agent%2520cooperation.%2520The%2520use%2520of%2520information%2520hiding%2520%2528steganography%2529%2520in%2520agent%2520communications%2520could%2520render%2520such%2520collusion%2520practically%2520undetectable.%2520This%2520underscores%2520the%2520need%2520for%2520investigations%2520into%2520the%2520possibility%2520of%2520such%2520behaviours%2520emerging%2520and%2520the%2520robustness%2520corresponding%2520countermeasures.%2520To%2520investigate%2520this%2520problem%2520we%2520design%2520two%2520approaches%2520--%2520a%2520gradient-based%2520reinforcement%2520learning%2520%2528GBRL%2529%2520method%2520and%2520an%2520in-context%2520reinforcement%2520learning%2520%2528ICRL%2529%2520method%2520--%2520for%2520reliably%2520eliciting%2520sophisticated%2520LLM-generated%2520linguistic%2520text%2520steganography.%250A%2520%2520We%2520demonstrate%252C%2520for%2520the%2520first%2520time%252C%2520that%2520unintended%2520steganographic%2520collusion%2520in%2520LLMs%2520can%2520arise%2520due%2520to%2520mispecified%2520reward%2520incentives%2520during%2520training.%2520Additionally%252C%2520we%2520find%2520that%2520standard%2520mitigations%2520--%2520both%2520passive%2520oversight%2520of%2520model%2520outputs%2520and%2520active%2520mitigation%2520through%2520communication%2520paraphrasing%2520--%2520are%2520not%2520fully%2520effective%2520at%2520preventing%2520this%2520steganographic%2520communication.%2520Our%2520findings%2520imply%2520that%2520%2528i%2529%2520emergence%2520of%2520steganographic%2520collusion%2520is%2520a%2520plausible%2520concern%2520that%2520should%2520be%2520monitored%2520and%2520researched%252C%2520and%2520%2528ii%2529%2520preventing%2520emergence%2520may%2520require%2520innovation%2520in%2520mitigation%2520techniques.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03768v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hidden%20in%20Plain%20Text%3A%20Emergence%20%26%20Mitigation%20of%20Steganographic%20Collusion%20in%20LLMs&entry.906535625=Yohan%20Mathew%20and%20Ollie%20Matthews%20and%20Robert%20McCarthy%20and%20Joan%20Velja%20and%20Christian%20Schroeder%20de%20Witt%20and%20Dylan%20Cope%20and%20Nandi%20Schoots&entry.1292438233=The%20rapid%20proliferation%20of%20frontier%20model%20agents%20promises%20significant%20societal%20advances%20but%20also%20raises%20concerns%20about%20systemic%20risks%20arising%20from%20unsafe%20interactions.%20Collusion%20to%20the%20disadvantage%20of%20others%20has%20been%20identified%20as%20a%20central%20form%20of%20undesirable%20agent%20cooperation.%20The%20use%20of%20information%20hiding%20%28steganography%29%20in%20agent%20communications%20could%20render%20such%20collusion%20practically%20undetectable.%20This%20underscores%20the%20need%20for%20investigations%20into%20the%20possibility%20of%20such%20behaviours%20emerging%20and%20the%20robustness%20corresponding%20countermeasures.%20To%20investigate%20this%20problem%20we%20design%20two%20approaches%20--%20a%20gradient-based%20reinforcement%20learning%20%28GBRL%29%20method%20and%20an%20in-context%20reinforcement%20learning%20%28ICRL%29%20method%20--%20for%20reliably%20eliciting%20sophisticated%20LLM-generated%20linguistic%20text%20steganography.%0A%20%20We%20demonstrate%2C%20for%20the%20first%20time%2C%20that%20unintended%20steganographic%20collusion%20in%20LLMs%20can%20arise%20due%20to%20mispecified%20reward%20incentives%20during%20training.%20Additionally%2C%20we%20find%20that%20standard%20mitigations%20--%20both%20passive%20oversight%20of%20model%20outputs%20and%20active%20mitigation%20through%20communication%20paraphrasing%20--%20are%20not%20fully%20effective%20at%20preventing%20this%20steganographic%20communication.%20Our%20findings%20imply%20that%20%28i%29%20emergence%20of%20steganographic%20collusion%20is%20a%20plausible%20concern%20that%20should%20be%20monitored%20and%20researched%2C%20and%20%28ii%29%20preventing%20emergence%20may%20require%20innovation%20in%20mitigation%20techniques.&entry.1838667208=http%3A//arxiv.org/abs/2410.03768v2&entry.124074799=Read"},
{"title": "Machine Unlearning via Information Theoretic Regularization", "author": "Shizhou Xu and Thomas Strohmer", "abstract": "How can we effectively remove or ''unlearn'' undesirable information, such as specific features or the influence of individual data points, from a learning outcome while minimizing utility loss and ensuring rigorous guarantees? We introduce a unified mathematical framework based on information-theoretic regularization to address both data point unlearning and feature unlearning. For data point unlearning, we introduce the $\\textit{Marginal Unlearning Principle}$, an auditable and provable framework inspired by memory suppression studies in neuroscience. Moreover, we provide formal information-theoretic unlearning definition based on the proposed principle, named marginal unlearning, and provable guarantees on sufficiency and necessity of marginal unlearning to the existing approximate unlearning definitions. We then show the proposed framework provide natural solution to the marginal unlearning problems. For feature unlearning, the framework applies to deep learning with arbitrary training objectives. By combining flexibility in learning objectives with simplicity in regularization design, our approach is highly adaptable and practical for a wide range of machine learning and AI applications. From a mathematical perspective, we provide an unified analytic solution to the optimal feature unlearning problem with a variety of information-theoretic training objectives. Our theoretical analysis reveals intriguing connections between machine unlearning, information theory, optimal transport, and extremal sigma algebras. Numerical simulations support our theoretical finding.", "link": "http://arxiv.org/abs/2502.05684v3", "date": "2025-12-02", "relevancy": 1.8937, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4899}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4732}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Machine%20Unlearning%20via%20Information%20Theoretic%20Regularization&body=Title%3A%20Machine%20Unlearning%20via%20Information%20Theoretic%20Regularization%0AAuthor%3A%20Shizhou%20Xu%20and%20Thomas%20Strohmer%0AAbstract%3A%20How%20can%20we%20effectively%20remove%20or%20%27%27unlearn%27%27%20undesirable%20information%2C%20such%20as%20specific%20features%20or%20the%20influence%20of%20individual%20data%20points%2C%20from%20a%20learning%20outcome%20while%20minimizing%20utility%20loss%20and%20ensuring%20rigorous%20guarantees%3F%20We%20introduce%20a%20unified%20mathematical%20framework%20based%20on%20information-theoretic%20regularization%20to%20address%20both%20data%20point%20unlearning%20and%20feature%20unlearning.%20For%20data%20point%20unlearning%2C%20we%20introduce%20the%20%24%5Ctextit%7BMarginal%20Unlearning%20Principle%7D%24%2C%20an%20auditable%20and%20provable%20framework%20inspired%20by%20memory%20suppression%20studies%20in%20neuroscience.%20Moreover%2C%20we%20provide%20formal%20information-theoretic%20unlearning%20definition%20based%20on%20the%20proposed%20principle%2C%20named%20marginal%20unlearning%2C%20and%20provable%20guarantees%20on%20sufficiency%20and%20necessity%20of%20marginal%20unlearning%20to%20the%20existing%20approximate%20unlearning%20definitions.%20We%20then%20show%20the%20proposed%20framework%20provide%20natural%20solution%20to%20the%20marginal%20unlearning%20problems.%20For%20feature%20unlearning%2C%20the%20framework%20applies%20to%20deep%20learning%20with%20arbitrary%20training%20objectives.%20By%20combining%20flexibility%20in%20learning%20objectives%20with%20simplicity%20in%20regularization%20design%2C%20our%20approach%20is%20highly%20adaptable%20and%20practical%20for%20a%20wide%20range%20of%20machine%20learning%20and%20AI%20applications.%20From%20a%20mathematical%20perspective%2C%20we%20provide%20an%20unified%20analytic%20solution%20to%20the%20optimal%20feature%20unlearning%20problem%20with%20a%20variety%20of%20information-theoretic%20training%20objectives.%20Our%20theoretical%20analysis%20reveals%20intriguing%20connections%20between%20machine%20unlearning%2C%20information%20theory%2C%20optimal%20transport%2C%20and%20extremal%20sigma%20algebras.%20Numerical%20simulations%20support%20our%20theoretical%20finding.%0ALink%3A%20http%3A//arxiv.org/abs/2502.05684v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMachine%2520Unlearning%2520via%2520Information%2520Theoretic%2520Regularization%26entry.906535625%3DShizhou%2520Xu%2520and%2520Thomas%2520Strohmer%26entry.1292438233%3DHow%2520can%2520we%2520effectively%2520remove%2520or%2520%2527%2527unlearn%2527%2527%2520undesirable%2520information%252C%2520such%2520as%2520specific%2520features%2520or%2520the%2520influence%2520of%2520individual%2520data%2520points%252C%2520from%2520a%2520learning%2520outcome%2520while%2520minimizing%2520utility%2520loss%2520and%2520ensuring%2520rigorous%2520guarantees%253F%2520We%2520introduce%2520a%2520unified%2520mathematical%2520framework%2520based%2520on%2520information-theoretic%2520regularization%2520to%2520address%2520both%2520data%2520point%2520unlearning%2520and%2520feature%2520unlearning.%2520For%2520data%2520point%2520unlearning%252C%2520we%2520introduce%2520the%2520%2524%255Ctextit%257BMarginal%2520Unlearning%2520Principle%257D%2524%252C%2520an%2520auditable%2520and%2520provable%2520framework%2520inspired%2520by%2520memory%2520suppression%2520studies%2520in%2520neuroscience.%2520Moreover%252C%2520we%2520provide%2520formal%2520information-theoretic%2520unlearning%2520definition%2520based%2520on%2520the%2520proposed%2520principle%252C%2520named%2520marginal%2520unlearning%252C%2520and%2520provable%2520guarantees%2520on%2520sufficiency%2520and%2520necessity%2520of%2520marginal%2520unlearning%2520to%2520the%2520existing%2520approximate%2520unlearning%2520definitions.%2520We%2520then%2520show%2520the%2520proposed%2520framework%2520provide%2520natural%2520solution%2520to%2520the%2520marginal%2520unlearning%2520problems.%2520For%2520feature%2520unlearning%252C%2520the%2520framework%2520applies%2520to%2520deep%2520learning%2520with%2520arbitrary%2520training%2520objectives.%2520By%2520combining%2520flexibility%2520in%2520learning%2520objectives%2520with%2520simplicity%2520in%2520regularization%2520design%252C%2520our%2520approach%2520is%2520highly%2520adaptable%2520and%2520practical%2520for%2520a%2520wide%2520range%2520of%2520machine%2520learning%2520and%2520AI%2520applications.%2520From%2520a%2520mathematical%2520perspective%252C%2520we%2520provide%2520an%2520unified%2520analytic%2520solution%2520to%2520the%2520optimal%2520feature%2520unlearning%2520problem%2520with%2520a%2520variety%2520of%2520information-theoretic%2520training%2520objectives.%2520Our%2520theoretical%2520analysis%2520reveals%2520intriguing%2520connections%2520between%2520machine%2520unlearning%252C%2520information%2520theory%252C%2520optimal%2520transport%252C%2520and%2520extremal%2520sigma%2520algebras.%2520Numerical%2520simulations%2520support%2520our%2520theoretical%2520finding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.05684v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Machine%20Unlearning%20via%20Information%20Theoretic%20Regularization&entry.906535625=Shizhou%20Xu%20and%20Thomas%20Strohmer&entry.1292438233=How%20can%20we%20effectively%20remove%20or%20%27%27unlearn%27%27%20undesirable%20information%2C%20such%20as%20specific%20features%20or%20the%20influence%20of%20individual%20data%20points%2C%20from%20a%20learning%20outcome%20while%20minimizing%20utility%20loss%20and%20ensuring%20rigorous%20guarantees%3F%20We%20introduce%20a%20unified%20mathematical%20framework%20based%20on%20information-theoretic%20regularization%20to%20address%20both%20data%20point%20unlearning%20and%20feature%20unlearning.%20For%20data%20point%20unlearning%2C%20we%20introduce%20the%20%24%5Ctextit%7BMarginal%20Unlearning%20Principle%7D%24%2C%20an%20auditable%20and%20provable%20framework%20inspired%20by%20memory%20suppression%20studies%20in%20neuroscience.%20Moreover%2C%20we%20provide%20formal%20information-theoretic%20unlearning%20definition%20based%20on%20the%20proposed%20principle%2C%20named%20marginal%20unlearning%2C%20and%20provable%20guarantees%20on%20sufficiency%20and%20necessity%20of%20marginal%20unlearning%20to%20the%20existing%20approximate%20unlearning%20definitions.%20We%20then%20show%20the%20proposed%20framework%20provide%20natural%20solution%20to%20the%20marginal%20unlearning%20problems.%20For%20feature%20unlearning%2C%20the%20framework%20applies%20to%20deep%20learning%20with%20arbitrary%20training%20objectives.%20By%20combining%20flexibility%20in%20learning%20objectives%20with%20simplicity%20in%20regularization%20design%2C%20our%20approach%20is%20highly%20adaptable%20and%20practical%20for%20a%20wide%20range%20of%20machine%20learning%20and%20AI%20applications.%20From%20a%20mathematical%20perspective%2C%20we%20provide%20an%20unified%20analytic%20solution%20to%20the%20optimal%20feature%20unlearning%20problem%20with%20a%20variety%20of%20information-theoretic%20training%20objectives.%20Our%20theoretical%20analysis%20reveals%20intriguing%20connections%20between%20machine%20unlearning%2C%20information%20theory%2C%20optimal%20transport%2C%20and%20extremal%20sigma%20algebras.%20Numerical%20simulations%20support%20our%20theoretical%20finding.&entry.1838667208=http%3A//arxiv.org/abs/2502.05684v3&entry.124074799=Read"},
{"title": "ViSAudio: End-to-End Video-Driven Binaural Spatial Audio Generation", "author": "Mengchen Zhang and Qi Chen and Tong Wu and Zihan Liu and Dahua Lin", "abstract": "Despite progress in video-to-audio generation, the field focuses predominantly on mono output, lacking spatial immersion. Existing binaural approaches remain constrained by a two-stage pipeline that first generates mono audio and then performs spatialization, often resulting in error accumulation and spatio-temporal inconsistencies. To address this limitation, we introduce the task of end-to-end binaural spatial audio generation directly from silent video. To support this task, we present the BiAudio dataset, comprising approximately 97K video-binaural audio pairs spanning diverse real-world scenes and camera rotation trajectories, constructed through a semi-automated pipeline. Furthermore, we propose ViSAudio, an end-to-end framework that employs conditional flow matching with a dual-branch audio generation architecture, where two dedicated branches model the audio latent flows. Integrated with a conditional spacetime module, it balances consistency between channels while preserving distinctive spatial characteristics, ensuring precise spatio-temporal alignment between audio and the input video. Comprehensive experiments demonstrate that ViSAudio outperforms existing state-of-the-art methods across both objective metrics and subjective evaluations, generating high-quality binaural audio with spatial immersion that adapts effectively to viewpoint changes, sound-source motion, and diverse acoustic environments. Project website: https://kszpxxzmc.github.io/ViSAudio-project.", "link": "http://arxiv.org/abs/2512.03036v1", "date": "2025-12-02", "relevancy": 2.1046, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5415}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5246}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5114}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ViSAudio%3A%20End-to-End%20Video-Driven%20Binaural%20Spatial%20Audio%20Generation&body=Title%3A%20ViSAudio%3A%20End-to-End%20Video-Driven%20Binaural%20Spatial%20Audio%20Generation%0AAuthor%3A%20Mengchen%20Zhang%20and%20Qi%20Chen%20and%20Tong%20Wu%20and%20Zihan%20Liu%20and%20Dahua%20Lin%0AAbstract%3A%20Despite%20progress%20in%20video-to-audio%20generation%2C%20the%20field%20focuses%20predominantly%20on%20mono%20output%2C%20lacking%20spatial%20immersion.%20Existing%20binaural%20approaches%20remain%20constrained%20by%20a%20two-stage%20pipeline%20that%20first%20generates%20mono%20audio%20and%20then%20performs%20spatialization%2C%20often%20resulting%20in%20error%20accumulation%20and%20spatio-temporal%20inconsistencies.%20To%20address%20this%20limitation%2C%20we%20introduce%20the%20task%20of%20end-to-end%20binaural%20spatial%20audio%20generation%20directly%20from%20silent%20video.%20To%20support%20this%20task%2C%20we%20present%20the%20BiAudio%20dataset%2C%20comprising%20approximately%2097K%20video-binaural%20audio%20pairs%20spanning%20diverse%20real-world%20scenes%20and%20camera%20rotation%20trajectories%2C%20constructed%20through%20a%20semi-automated%20pipeline.%20Furthermore%2C%20we%20propose%20ViSAudio%2C%20an%20end-to-end%20framework%20that%20employs%20conditional%20flow%20matching%20with%20a%20dual-branch%20audio%20generation%20architecture%2C%20where%20two%20dedicated%20branches%20model%20the%20audio%20latent%20flows.%20Integrated%20with%20a%20conditional%20spacetime%20module%2C%20it%20balances%20consistency%20between%20channels%20while%20preserving%20distinctive%20spatial%20characteristics%2C%20ensuring%20precise%20spatio-temporal%20alignment%20between%20audio%20and%20the%20input%20video.%20Comprehensive%20experiments%20demonstrate%20that%20ViSAudio%20outperforms%20existing%20state-of-the-art%20methods%20across%20both%20objective%20metrics%20and%20subjective%20evaluations%2C%20generating%20high-quality%20binaural%20audio%20with%20spatial%20immersion%20that%20adapts%20effectively%20to%20viewpoint%20changes%2C%20sound-source%20motion%2C%20and%20diverse%20acoustic%20environments.%20Project%20website%3A%20https%3A//kszpxxzmc.github.io/ViSAudio-project.%0ALink%3A%20http%3A//arxiv.org/abs/2512.03036v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DViSAudio%253A%2520End-to-End%2520Video-Driven%2520Binaural%2520Spatial%2520Audio%2520Generation%26entry.906535625%3DMengchen%2520Zhang%2520and%2520Qi%2520Chen%2520and%2520Tong%2520Wu%2520and%2520Zihan%2520Liu%2520and%2520Dahua%2520Lin%26entry.1292438233%3DDespite%2520progress%2520in%2520video-to-audio%2520generation%252C%2520the%2520field%2520focuses%2520predominantly%2520on%2520mono%2520output%252C%2520lacking%2520spatial%2520immersion.%2520Existing%2520binaural%2520approaches%2520remain%2520constrained%2520by%2520a%2520two-stage%2520pipeline%2520that%2520first%2520generates%2520mono%2520audio%2520and%2520then%2520performs%2520spatialization%252C%2520often%2520resulting%2520in%2520error%2520accumulation%2520and%2520spatio-temporal%2520inconsistencies.%2520To%2520address%2520this%2520limitation%252C%2520we%2520introduce%2520the%2520task%2520of%2520end-to-end%2520binaural%2520spatial%2520audio%2520generation%2520directly%2520from%2520silent%2520video.%2520To%2520support%2520this%2520task%252C%2520we%2520present%2520the%2520BiAudio%2520dataset%252C%2520comprising%2520approximately%252097K%2520video-binaural%2520audio%2520pairs%2520spanning%2520diverse%2520real-world%2520scenes%2520and%2520camera%2520rotation%2520trajectories%252C%2520constructed%2520through%2520a%2520semi-automated%2520pipeline.%2520Furthermore%252C%2520we%2520propose%2520ViSAudio%252C%2520an%2520end-to-end%2520framework%2520that%2520employs%2520conditional%2520flow%2520matching%2520with%2520a%2520dual-branch%2520audio%2520generation%2520architecture%252C%2520where%2520two%2520dedicated%2520branches%2520model%2520the%2520audio%2520latent%2520flows.%2520Integrated%2520with%2520a%2520conditional%2520spacetime%2520module%252C%2520it%2520balances%2520consistency%2520between%2520channels%2520while%2520preserving%2520distinctive%2520spatial%2520characteristics%252C%2520ensuring%2520precise%2520spatio-temporal%2520alignment%2520between%2520audio%2520and%2520the%2520input%2520video.%2520Comprehensive%2520experiments%2520demonstrate%2520that%2520ViSAudio%2520outperforms%2520existing%2520state-of-the-art%2520methods%2520across%2520both%2520objective%2520metrics%2520and%2520subjective%2520evaluations%252C%2520generating%2520high-quality%2520binaural%2520audio%2520with%2520spatial%2520immersion%2520that%2520adapts%2520effectively%2520to%2520viewpoint%2520changes%252C%2520sound-source%2520motion%252C%2520and%2520diverse%2520acoustic%2520environments.%2520Project%2520website%253A%2520https%253A//kszpxxzmc.github.io/ViSAudio-project.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.03036v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ViSAudio%3A%20End-to-End%20Video-Driven%20Binaural%20Spatial%20Audio%20Generation&entry.906535625=Mengchen%20Zhang%20and%20Qi%20Chen%20and%20Tong%20Wu%20and%20Zihan%20Liu%20and%20Dahua%20Lin&entry.1292438233=Despite%20progress%20in%20video-to-audio%20generation%2C%20the%20field%20focuses%20predominantly%20on%20mono%20output%2C%20lacking%20spatial%20immersion.%20Existing%20binaural%20approaches%20remain%20constrained%20by%20a%20two-stage%20pipeline%20that%20first%20generates%20mono%20audio%20and%20then%20performs%20spatialization%2C%20often%20resulting%20in%20error%20accumulation%20and%20spatio-temporal%20inconsistencies.%20To%20address%20this%20limitation%2C%20we%20introduce%20the%20task%20of%20end-to-end%20binaural%20spatial%20audio%20generation%20directly%20from%20silent%20video.%20To%20support%20this%20task%2C%20we%20present%20the%20BiAudio%20dataset%2C%20comprising%20approximately%2097K%20video-binaural%20audio%20pairs%20spanning%20diverse%20real-world%20scenes%20and%20camera%20rotation%20trajectories%2C%20constructed%20through%20a%20semi-automated%20pipeline.%20Furthermore%2C%20we%20propose%20ViSAudio%2C%20an%20end-to-end%20framework%20that%20employs%20conditional%20flow%20matching%20with%20a%20dual-branch%20audio%20generation%20architecture%2C%20where%20two%20dedicated%20branches%20model%20the%20audio%20latent%20flows.%20Integrated%20with%20a%20conditional%20spacetime%20module%2C%20it%20balances%20consistency%20between%20channels%20while%20preserving%20distinctive%20spatial%20characteristics%2C%20ensuring%20precise%20spatio-temporal%20alignment%20between%20audio%20and%20the%20input%20video.%20Comprehensive%20experiments%20demonstrate%20that%20ViSAudio%20outperforms%20existing%20state-of-the-art%20methods%20across%20both%20objective%20metrics%20and%20subjective%20evaluations%2C%20generating%20high-quality%20binaural%20audio%20with%20spatial%20immersion%20that%20adapts%20effectively%20to%20viewpoint%20changes%2C%20sound-source%20motion%2C%20and%20diverse%20acoustic%20environments.%20Project%20website%3A%20https%3A//kszpxxzmc.github.io/ViSAudio-project.&entry.1838667208=http%3A//arxiv.org/abs/2512.03036v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


