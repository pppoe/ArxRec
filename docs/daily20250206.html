<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250205.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Dress-1-to-3: Single Image to Simulation-Ready 3D Outfit with Diffusion\n  Prior and Differentiable Physics", "author": "Xuan Li and Chang Yu and Wenxin Du and Ying Jiang and Tianyi Xie and Yunuo Chen and Yin Yang and Chenfanfu Jiang", "abstract": "  Recent advances in large models have significantly advanced image-to-3D\nreconstruction. However, the generated models are often fused into a single\npiece, limiting their applicability in downstream tasks. This paper focuses on\n3D garment generation, a key area for applications like virtual try-on with\ndynamic garment animations, which require garments to be separable and\nsimulation-ready. We introduce Dress-1-to-3, a novel pipeline that reconstructs\nphysics-plausible, simulation-ready separated garments with sewing patterns and\nhumans from an in-the-wild image. Starting with the image, our approach\ncombines a pre-trained image-to-sewing pattern generation model for creating\ncoarse sewing patterns with a pre-trained multi-view diffusion model to produce\nmulti-view images. The sewing pattern is further refined using a differentiable\ngarment simulator based on the generated multi-view images. Versatile\nexperiments demonstrate that our optimization approach substantially enhances\nthe geometric alignment of the reconstructed 3D garments and humans with the\ninput image. Furthermore, by integrating a texture generation module and a\nhuman motion generation module, we produce customized physics-plausible and\nrealistic dynamic garment demonstrations. Project page:\nhttps://dress-1-to-3.github.io/\n", "link": "http://arxiv.org/abs/2502.03449v1", "date": "2025-02-05", "relevancy": 3.7142, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.7665}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.7561}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dress-1-to-3%3A%20Single%20Image%20to%20Simulation-Ready%203D%20Outfit%20with%20Diffusion%0A%20%20Prior%20and%20Differentiable%20Physics&body=Title%3A%20Dress-1-to-3%3A%20Single%20Image%20to%20Simulation-Ready%203D%20Outfit%20with%20Diffusion%0A%20%20Prior%20and%20Differentiable%20Physics%0AAuthor%3A%20Xuan%20Li%20and%20Chang%20Yu%20and%20Wenxin%20Du%20and%20Ying%20Jiang%20and%20Tianyi%20Xie%20and%20Yunuo%20Chen%20and%20Yin%20Yang%20and%20Chenfanfu%20Jiang%0AAbstract%3A%20%20%20Recent%20advances%20in%20large%20models%20have%20significantly%20advanced%20image-to-3D%0Areconstruction.%20However%2C%20the%20generated%20models%20are%20often%20fused%20into%20a%20single%0Apiece%2C%20limiting%20their%20applicability%20in%20downstream%20tasks.%20This%20paper%20focuses%20on%0A3D%20garment%20generation%2C%20a%20key%20area%20for%20applications%20like%20virtual%20try-on%20with%0Adynamic%20garment%20animations%2C%20which%20require%20garments%20to%20be%20separable%20and%0Asimulation-ready.%20We%20introduce%20Dress-1-to-3%2C%20a%20novel%20pipeline%20that%20reconstructs%0Aphysics-plausible%2C%20simulation-ready%20separated%20garments%20with%20sewing%20patterns%20and%0Ahumans%20from%20an%20in-the-wild%20image.%20Starting%20with%20the%20image%2C%20our%20approach%0Acombines%20a%20pre-trained%20image-to-sewing%20pattern%20generation%20model%20for%20creating%0Acoarse%20sewing%20patterns%20with%20a%20pre-trained%20multi-view%20diffusion%20model%20to%20produce%0Amulti-view%20images.%20The%20sewing%20pattern%20is%20further%20refined%20using%20a%20differentiable%0Agarment%20simulator%20based%20on%20the%20generated%20multi-view%20images.%20Versatile%0Aexperiments%20demonstrate%20that%20our%20optimization%20approach%20substantially%20enhances%0Athe%20geometric%20alignment%20of%20the%20reconstructed%203D%20garments%20and%20humans%20with%20the%0Ainput%20image.%20Furthermore%2C%20by%20integrating%20a%20texture%20generation%20module%20and%20a%0Ahuman%20motion%20generation%20module%2C%20we%20produce%20customized%20physics-plausible%20and%0Arealistic%20dynamic%20garment%20demonstrations.%20Project%20page%3A%0Ahttps%3A//dress-1-to-3.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03449v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDress-1-to-3%253A%2520Single%2520Image%2520to%2520Simulation-Ready%25203D%2520Outfit%2520with%2520Diffusion%250A%2520%2520Prior%2520and%2520Differentiable%2520Physics%26entry.906535625%3DXuan%2520Li%2520and%2520Chang%2520Yu%2520and%2520Wenxin%2520Du%2520and%2520Ying%2520Jiang%2520and%2520Tianyi%2520Xie%2520and%2520Yunuo%2520Chen%2520and%2520Yin%2520Yang%2520and%2520Chenfanfu%2520Jiang%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520large%2520models%2520have%2520significantly%2520advanced%2520image-to-3D%250Areconstruction.%2520However%252C%2520the%2520generated%2520models%2520are%2520often%2520fused%2520into%2520a%2520single%250Apiece%252C%2520limiting%2520their%2520applicability%2520in%2520downstream%2520tasks.%2520This%2520paper%2520focuses%2520on%250A3D%2520garment%2520generation%252C%2520a%2520key%2520area%2520for%2520applications%2520like%2520virtual%2520try-on%2520with%250Adynamic%2520garment%2520animations%252C%2520which%2520require%2520garments%2520to%2520be%2520separable%2520and%250Asimulation-ready.%2520We%2520introduce%2520Dress-1-to-3%252C%2520a%2520novel%2520pipeline%2520that%2520reconstructs%250Aphysics-plausible%252C%2520simulation-ready%2520separated%2520garments%2520with%2520sewing%2520patterns%2520and%250Ahumans%2520from%2520an%2520in-the-wild%2520image.%2520Starting%2520with%2520the%2520image%252C%2520our%2520approach%250Acombines%2520a%2520pre-trained%2520image-to-sewing%2520pattern%2520generation%2520model%2520for%2520creating%250Acoarse%2520sewing%2520patterns%2520with%2520a%2520pre-trained%2520multi-view%2520diffusion%2520model%2520to%2520produce%250Amulti-view%2520images.%2520The%2520sewing%2520pattern%2520is%2520further%2520refined%2520using%2520a%2520differentiable%250Agarment%2520simulator%2520based%2520on%2520the%2520generated%2520multi-view%2520images.%2520Versatile%250Aexperiments%2520demonstrate%2520that%2520our%2520optimization%2520approach%2520substantially%2520enhances%250Athe%2520geometric%2520alignment%2520of%2520the%2520reconstructed%25203D%2520garments%2520and%2520humans%2520with%2520the%250Ainput%2520image.%2520Furthermore%252C%2520by%2520integrating%2520a%2520texture%2520generation%2520module%2520and%2520a%250Ahuman%2520motion%2520generation%2520module%252C%2520we%2520produce%2520customized%2520physics-plausible%2520and%250Arealistic%2520dynamic%2520garment%2520demonstrations.%2520Project%2520page%253A%250Ahttps%253A//dress-1-to-3.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03449v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dress-1-to-3%3A%20Single%20Image%20to%20Simulation-Ready%203D%20Outfit%20with%20Diffusion%0A%20%20Prior%20and%20Differentiable%20Physics&entry.906535625=Xuan%20Li%20and%20Chang%20Yu%20and%20Wenxin%20Du%20and%20Ying%20Jiang%20and%20Tianyi%20Xie%20and%20Yunuo%20Chen%20and%20Yin%20Yang%20and%20Chenfanfu%20Jiang&entry.1292438233=%20%20Recent%20advances%20in%20large%20models%20have%20significantly%20advanced%20image-to-3D%0Areconstruction.%20However%2C%20the%20generated%20models%20are%20often%20fused%20into%20a%20single%0Apiece%2C%20limiting%20their%20applicability%20in%20downstream%20tasks.%20This%20paper%20focuses%20on%0A3D%20garment%20generation%2C%20a%20key%20area%20for%20applications%20like%20virtual%20try-on%20with%0Adynamic%20garment%20animations%2C%20which%20require%20garments%20to%20be%20separable%20and%0Asimulation-ready.%20We%20introduce%20Dress-1-to-3%2C%20a%20novel%20pipeline%20that%20reconstructs%0Aphysics-plausible%2C%20simulation-ready%20separated%20garments%20with%20sewing%20patterns%20and%0Ahumans%20from%20an%20in-the-wild%20image.%20Starting%20with%20the%20image%2C%20our%20approach%0Acombines%20a%20pre-trained%20image-to-sewing%20pattern%20generation%20model%20for%20creating%0Acoarse%20sewing%20patterns%20with%20a%20pre-trained%20multi-view%20diffusion%20model%20to%20produce%0Amulti-view%20images.%20The%20sewing%20pattern%20is%20further%20refined%20using%20a%20differentiable%0Agarment%20simulator%20based%20on%20the%20generated%20multi-view%20images.%20Versatile%0Aexperiments%20demonstrate%20that%20our%20optimization%20approach%20substantially%20enhances%0Athe%20geometric%20alignment%20of%20the%20reconstructed%203D%20garments%20and%20humans%20with%20the%0Ainput%20image.%20Furthermore%2C%20by%20integrating%20a%20texture%20generation%20module%20and%20a%0Ahuman%20motion%20generation%20module%2C%20we%20produce%20customized%20physics-plausible%20and%0Arealistic%20dynamic%20garment%20demonstrations.%20Project%20page%3A%0Ahttps%3A//dress-1-to-3.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03449v1&entry.124074799=Read"},
{"title": "GP-GS: Gaussian Processes for Enhanced Gaussian Splatting", "author": "Zhihao Guo and Jingxuan Su and Shenglin Wang and Jinlong Fan and Jing Zhang and Liangxiu Han and Peng Wang", "abstract": "  3D Gaussian Splatting has emerged as an efficient photorealistic novel view\nsynthesis method. However, its reliance on sparse Structure-from-Motion (SfM)\npoint clouds consistently compromises the scene reconstruction quality. To\naddress these limitations, this paper proposes a novel 3D reconstruction\nframework Gaussian Processes Gaussian Splatting (GP-GS), where a multi-output\nGaussian Process model is developed to achieve adaptive and uncertainty-guided\ndensification of sparse SfM point clouds. Specifically, we propose a dynamic\nsampling and filtering pipeline that adaptively expands the SfM point clouds by\nleveraging GP-based predictions to infer new candidate points from the input 2D\npixels and depth maps. The pipeline utilizes uncertainty estimates to guide the\npruning of high-variance predictions, ensuring geometric consistency and\nenabling the generation of dense point clouds. The densified point clouds\nprovide high-quality initial 3D Gaussians to enhance reconstruction\nperformance. Extensive experiments conducted on synthetic and real-world\ndatasets across various scales validate the effectiveness and practicality of\nthe proposed framework.\n", "link": "http://arxiv.org/abs/2502.02283v2", "date": "2025-02-05", "relevancy": 3.5893, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.752}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7243}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GP-GS%3A%20Gaussian%20Processes%20for%20Enhanced%20Gaussian%20Splatting&body=Title%3A%20GP-GS%3A%20Gaussian%20Processes%20for%20Enhanced%20Gaussian%20Splatting%0AAuthor%3A%20Zhihao%20Guo%20and%20Jingxuan%20Su%20and%20Shenglin%20Wang%20and%20Jinlong%20Fan%20and%20Jing%20Zhang%20and%20Liangxiu%20Han%20and%20Peng%20Wang%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20has%20emerged%20as%20an%20efficient%20photorealistic%20novel%20view%0Asynthesis%20method.%20However%2C%20its%20reliance%20on%20sparse%20Structure-from-Motion%20%28SfM%29%0Apoint%20clouds%20consistently%20compromises%20the%20scene%20reconstruction%20quality.%20To%0Aaddress%20these%20limitations%2C%20this%20paper%20proposes%20a%20novel%203D%20reconstruction%0Aframework%20Gaussian%20Processes%20Gaussian%20Splatting%20%28GP-GS%29%2C%20where%20a%20multi-output%0AGaussian%20Process%20model%20is%20developed%20to%20achieve%20adaptive%20and%20uncertainty-guided%0Adensification%20of%20sparse%20SfM%20point%20clouds.%20Specifically%2C%20we%20propose%20a%20dynamic%0Asampling%20and%20filtering%20pipeline%20that%20adaptively%20expands%20the%20SfM%20point%20clouds%20by%0Aleveraging%20GP-based%20predictions%20to%20infer%20new%20candidate%20points%20from%20the%20input%202D%0Apixels%20and%20depth%20maps.%20The%20pipeline%20utilizes%20uncertainty%20estimates%20to%20guide%20the%0Apruning%20of%20high-variance%20predictions%2C%20ensuring%20geometric%20consistency%20and%0Aenabling%20the%20generation%20of%20dense%20point%20clouds.%20The%20densified%20point%20clouds%0Aprovide%20high-quality%20initial%203D%20Gaussians%20to%20enhance%20reconstruction%0Aperformance.%20Extensive%20experiments%20conducted%20on%20synthetic%20and%20real-world%0Adatasets%20across%20various%20scales%20validate%20the%20effectiveness%20and%20practicality%20of%0Athe%20proposed%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02283v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGP-GS%253A%2520Gaussian%2520Processes%2520for%2520Enhanced%2520Gaussian%2520Splatting%26entry.906535625%3DZhihao%2520Guo%2520and%2520Jingxuan%2520Su%2520and%2520Shenglin%2520Wang%2520and%2520Jinlong%2520Fan%2520and%2520Jing%2520Zhang%2520and%2520Liangxiu%2520Han%2520and%2520Peng%2520Wang%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520has%2520emerged%2520as%2520an%2520efficient%2520photorealistic%2520novel%2520view%250Asynthesis%2520method.%2520However%252C%2520its%2520reliance%2520on%2520sparse%2520Structure-from-Motion%2520%2528SfM%2529%250Apoint%2520clouds%2520consistently%2520compromises%2520the%2520scene%2520reconstruction%2520quality.%2520To%250Aaddress%2520these%2520limitations%252C%2520this%2520paper%2520proposes%2520a%2520novel%25203D%2520reconstruction%250Aframework%2520Gaussian%2520Processes%2520Gaussian%2520Splatting%2520%2528GP-GS%2529%252C%2520where%2520a%2520multi-output%250AGaussian%2520Process%2520model%2520is%2520developed%2520to%2520achieve%2520adaptive%2520and%2520uncertainty-guided%250Adensification%2520of%2520sparse%2520SfM%2520point%2520clouds.%2520Specifically%252C%2520we%2520propose%2520a%2520dynamic%250Asampling%2520and%2520filtering%2520pipeline%2520that%2520adaptively%2520expands%2520the%2520SfM%2520point%2520clouds%2520by%250Aleveraging%2520GP-based%2520predictions%2520to%2520infer%2520new%2520candidate%2520points%2520from%2520the%2520input%25202D%250Apixels%2520and%2520depth%2520maps.%2520The%2520pipeline%2520utilizes%2520uncertainty%2520estimates%2520to%2520guide%2520the%250Apruning%2520of%2520high-variance%2520predictions%252C%2520ensuring%2520geometric%2520consistency%2520and%250Aenabling%2520the%2520generation%2520of%2520dense%2520point%2520clouds.%2520The%2520densified%2520point%2520clouds%250Aprovide%2520high-quality%2520initial%25203D%2520Gaussians%2520to%2520enhance%2520reconstruction%250Aperformance.%2520Extensive%2520experiments%2520conducted%2520on%2520synthetic%2520and%2520real-world%250Adatasets%2520across%2520various%2520scales%2520validate%2520the%2520effectiveness%2520and%2520practicality%2520of%250Athe%2520proposed%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02283v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GP-GS%3A%20Gaussian%20Processes%20for%20Enhanced%20Gaussian%20Splatting&entry.906535625=Zhihao%20Guo%20and%20Jingxuan%20Su%20and%20Shenglin%20Wang%20and%20Jinlong%20Fan%20and%20Jing%20Zhang%20and%20Liangxiu%20Han%20and%20Peng%20Wang&entry.1292438233=%20%203D%20Gaussian%20Splatting%20has%20emerged%20as%20an%20efficient%20photorealistic%20novel%20view%0Asynthesis%20method.%20However%2C%20its%20reliance%20on%20sparse%20Structure-from-Motion%20%28SfM%29%0Apoint%20clouds%20consistently%20compromises%20the%20scene%20reconstruction%20quality.%20To%0Aaddress%20these%20limitations%2C%20this%20paper%20proposes%20a%20novel%203D%20reconstruction%0Aframework%20Gaussian%20Processes%20Gaussian%20Splatting%20%28GP-GS%29%2C%20where%20a%20multi-output%0AGaussian%20Process%20model%20is%20developed%20to%20achieve%20adaptive%20and%20uncertainty-guided%0Adensification%20of%20sparse%20SfM%20point%20clouds.%20Specifically%2C%20we%20propose%20a%20dynamic%0Asampling%20and%20filtering%20pipeline%20that%20adaptively%20expands%20the%20SfM%20point%20clouds%20by%0Aleveraging%20GP-based%20predictions%20to%20infer%20new%20candidate%20points%20from%20the%20input%202D%0Apixels%20and%20depth%20maps.%20The%20pipeline%20utilizes%20uncertainty%20estimates%20to%20guide%20the%0Apruning%20of%20high-variance%20predictions%2C%20ensuring%20geometric%20consistency%20and%0Aenabling%20the%20generation%20of%20dense%20point%20clouds.%20The%20densified%20point%20clouds%0Aprovide%20high-quality%20initial%203D%20Gaussians%20to%20enhance%20reconstruction%0Aperformance.%20Extensive%20experiments%20conducted%20on%20synthetic%20and%20real-world%0Adatasets%20across%20various%20scales%20validate%20the%20effectiveness%20and%20practicality%20of%0Athe%20proposed%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02283v2&entry.124074799=Read"},
{"title": "GARAD-SLAM: 3D GAussian splatting for Real-time Anti Dynamic SLAM", "author": "Mingrui Li and Weijian Chen and Na Cheng and Jingyuan Xu and Dong Li and Hongyu Wang", "abstract": "  The 3D Gaussian Splatting (3DGS)-based SLAM system has garnered widespread\nattention due to its excellent performance in real-time high-fidelity\nrendering. However, in real-world environments with dynamic objects, existing\n3DGS-based SLAM systems often face mapping errors and tracking drift issues. To\naddress these problems, we propose GARAD-SLAM, a real-time 3DGS-based SLAM\nsystem tailored for dynamic scenes. In terms of tracking, unlike traditional\nmethods, we directly perform dynamic segmentation on Gaussians and map them\nback to the front-end to obtain dynamic point labels through a Gaussian pyramid\nnetwork, achieving precise dynamic removal and robust tracking. For mapping, we\nimpose rendering penalties on dynamically labeled Gaussians, which are updated\nthrough the network, to avoid irreversible erroneous removal caused by simple\npruning. Our results on real-world datasets demonstrate that our method is\ncompetitive in tracking compared to baseline methods, generating fewer\nartifacts and higher-quality reconstructions in rendering.\n", "link": "http://arxiv.org/abs/2502.03228v1", "date": "2025-02-05", "relevancy": 3.5496, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.8033}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6675}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GARAD-SLAM%3A%203D%20GAussian%20splatting%20for%20Real-time%20Anti%20Dynamic%20SLAM&body=Title%3A%20GARAD-SLAM%3A%203D%20GAussian%20splatting%20for%20Real-time%20Anti%20Dynamic%20SLAM%0AAuthor%3A%20Mingrui%20Li%20and%20Weijian%20Chen%20and%20Na%20Cheng%20and%20Jingyuan%20Xu%20and%20Dong%20Li%20and%20Hongyu%20Wang%0AAbstract%3A%20%20%20The%203D%20Gaussian%20Splatting%20%283DGS%29-based%20SLAM%20system%20has%20garnered%20widespread%0Aattention%20due%20to%20its%20excellent%20performance%20in%20real-time%20high-fidelity%0Arendering.%20However%2C%20in%20real-world%20environments%20with%20dynamic%20objects%2C%20existing%0A3DGS-based%20SLAM%20systems%20often%20face%20mapping%20errors%20and%20tracking%20drift%20issues.%20To%0Aaddress%20these%20problems%2C%20we%20propose%20GARAD-SLAM%2C%20a%20real-time%203DGS-based%20SLAM%0Asystem%20tailored%20for%20dynamic%20scenes.%20In%20terms%20of%20tracking%2C%20unlike%20traditional%0Amethods%2C%20we%20directly%20perform%20dynamic%20segmentation%20on%20Gaussians%20and%20map%20them%0Aback%20to%20the%20front-end%20to%20obtain%20dynamic%20point%20labels%20through%20a%20Gaussian%20pyramid%0Anetwork%2C%20achieving%20precise%20dynamic%20removal%20and%20robust%20tracking.%20For%20mapping%2C%20we%0Aimpose%20rendering%20penalties%20on%20dynamically%20labeled%20Gaussians%2C%20which%20are%20updated%0Athrough%20the%20network%2C%20to%20avoid%20irreversible%20erroneous%20removal%20caused%20by%20simple%0Apruning.%20Our%20results%20on%20real-world%20datasets%20demonstrate%20that%20our%20method%20is%0Acompetitive%20in%20tracking%20compared%20to%20baseline%20methods%2C%20generating%20fewer%0Aartifacts%20and%20higher-quality%20reconstructions%20in%20rendering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03228v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGARAD-SLAM%253A%25203D%2520GAussian%2520splatting%2520for%2520Real-time%2520Anti%2520Dynamic%2520SLAM%26entry.906535625%3DMingrui%2520Li%2520and%2520Weijian%2520Chen%2520and%2520Na%2520Cheng%2520and%2520Jingyuan%2520Xu%2520and%2520Dong%2520Li%2520and%2520Hongyu%2520Wang%26entry.1292438233%3D%2520%2520The%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529-based%2520SLAM%2520system%2520has%2520garnered%2520widespread%250Aattention%2520due%2520to%2520its%2520excellent%2520performance%2520in%2520real-time%2520high-fidelity%250Arendering.%2520However%252C%2520in%2520real-world%2520environments%2520with%2520dynamic%2520objects%252C%2520existing%250A3DGS-based%2520SLAM%2520systems%2520often%2520face%2520mapping%2520errors%2520and%2520tracking%2520drift%2520issues.%2520To%250Aaddress%2520these%2520problems%252C%2520we%2520propose%2520GARAD-SLAM%252C%2520a%2520real-time%25203DGS-based%2520SLAM%250Asystem%2520tailored%2520for%2520dynamic%2520scenes.%2520In%2520terms%2520of%2520tracking%252C%2520unlike%2520traditional%250Amethods%252C%2520we%2520directly%2520perform%2520dynamic%2520segmentation%2520on%2520Gaussians%2520and%2520map%2520them%250Aback%2520to%2520the%2520front-end%2520to%2520obtain%2520dynamic%2520point%2520labels%2520through%2520a%2520Gaussian%2520pyramid%250Anetwork%252C%2520achieving%2520precise%2520dynamic%2520removal%2520and%2520robust%2520tracking.%2520For%2520mapping%252C%2520we%250Aimpose%2520rendering%2520penalties%2520on%2520dynamically%2520labeled%2520Gaussians%252C%2520which%2520are%2520updated%250Athrough%2520the%2520network%252C%2520to%2520avoid%2520irreversible%2520erroneous%2520removal%2520caused%2520by%2520simple%250Apruning.%2520Our%2520results%2520on%2520real-world%2520datasets%2520demonstrate%2520that%2520our%2520method%2520is%250Acompetitive%2520in%2520tracking%2520compared%2520to%2520baseline%2520methods%252C%2520generating%2520fewer%250Aartifacts%2520and%2520higher-quality%2520reconstructions%2520in%2520rendering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03228v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GARAD-SLAM%3A%203D%20GAussian%20splatting%20for%20Real-time%20Anti%20Dynamic%20SLAM&entry.906535625=Mingrui%20Li%20and%20Weijian%20Chen%20and%20Na%20Cheng%20and%20Jingyuan%20Xu%20and%20Dong%20Li%20and%20Hongyu%20Wang&entry.1292438233=%20%20The%203D%20Gaussian%20Splatting%20%283DGS%29-based%20SLAM%20system%20has%20garnered%20widespread%0Aattention%20due%20to%20its%20excellent%20performance%20in%20real-time%20high-fidelity%0Arendering.%20However%2C%20in%20real-world%20environments%20with%20dynamic%20objects%2C%20existing%0A3DGS-based%20SLAM%20systems%20often%20face%20mapping%20errors%20and%20tracking%20drift%20issues.%20To%0Aaddress%20these%20problems%2C%20we%20propose%20GARAD-SLAM%2C%20a%20real-time%203DGS-based%20SLAM%0Asystem%20tailored%20for%20dynamic%20scenes.%20In%20terms%20of%20tracking%2C%20unlike%20traditional%0Amethods%2C%20we%20directly%20perform%20dynamic%20segmentation%20on%20Gaussians%20and%20map%20them%0Aback%20to%20the%20front-end%20to%20obtain%20dynamic%20point%20labels%20through%20a%20Gaussian%20pyramid%0Anetwork%2C%20achieving%20precise%20dynamic%20removal%20and%20robust%20tracking.%20For%20mapping%2C%20we%0Aimpose%20rendering%20penalties%20on%20dynamically%20labeled%20Gaussians%2C%20which%20are%20updated%0Athrough%20the%20network%2C%20to%20avoid%20irreversible%20erroneous%20removal%20caused%20by%20simple%0Apruning.%20Our%20results%20on%20real-world%20datasets%20demonstrate%20that%20our%20method%20is%0Acompetitive%20in%20tracking%20compared%20to%20baseline%20methods%2C%20generating%20fewer%0Aartifacts%20and%20higher-quality%20reconstructions%20in%20rendering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03228v1&entry.124074799=Read"},
{"title": "Segment Any 3D Gaussians", "author": "Jiazhong Cen and Jiemin Fang and Chen Yang and Lingxi Xie and Xiaopeng Zhang and Wei Shen and Qi Tian", "abstract": "  This paper presents SAGA (Segment Any 3D GAussians), a highly efficient 3D\npromptable segmentation method based on 3D Gaussian Splatting (3D-GS). Given 2D\nvisual prompts as input, SAGA can segment the corresponding 3D target\nrepresented by 3D Gaussians within 4 ms. This is achieved by attaching an\nscale-gated affinity feature to each 3D Gaussian to endow it a new property\ntowards multi-granularity segmentation. Specifically, a scale-aware contrastive\ntraining strategy is proposed for the scale-gated affinity feature learning. It\n1) distills the segmentation capability of the Segment Anything Model (SAM)\nfrom 2D masks into the affinity features and 2) employs a soft scale gate\nmechanism to deal with multi-granularity ambiguity in 3D segmentation through\nadjusting the magnitude of each feature channel according to a specified 3D\nphysical scale. Evaluations demonstrate that SAGA achieves real-time\nmulti-granularity segmentation with quality comparable to state-of-the-art\nmethods. As one of the first methods addressing promptable segmentation in\n3D-GS, the simplicity and effectiveness of SAGA pave the way for future\nadvancements in this field. Our code will be released.\n", "link": "http://arxiv.org/abs/2312.00860v3", "date": "2025-02-05", "relevancy": 3.1755, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6618}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6592}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Segment%20Any%203D%20Gaussians&body=Title%3A%20Segment%20Any%203D%20Gaussians%0AAuthor%3A%20Jiazhong%20Cen%20and%20Jiemin%20Fang%20and%20Chen%20Yang%20and%20Lingxi%20Xie%20and%20Xiaopeng%20Zhang%20and%20Wei%20Shen%20and%20Qi%20Tian%0AAbstract%3A%20%20%20This%20paper%20presents%20SAGA%20%28Segment%20Any%203D%20GAussians%29%2C%20a%20highly%20efficient%203D%0Apromptable%20segmentation%20method%20based%20on%203D%20Gaussian%20Splatting%20%283D-GS%29.%20Given%202D%0Avisual%20prompts%20as%20input%2C%20SAGA%20can%20segment%20the%20corresponding%203D%20target%0Arepresented%20by%203D%20Gaussians%20within%204%20ms.%20This%20is%20achieved%20by%20attaching%20an%0Ascale-gated%20affinity%20feature%20to%20each%203D%20Gaussian%20to%20endow%20it%20a%20new%20property%0Atowards%20multi-granularity%20segmentation.%20Specifically%2C%20a%20scale-aware%20contrastive%0Atraining%20strategy%20is%20proposed%20for%20the%20scale-gated%20affinity%20feature%20learning.%20It%0A1%29%20distills%20the%20segmentation%20capability%20of%20the%20Segment%20Anything%20Model%20%28SAM%29%0Afrom%202D%20masks%20into%20the%20affinity%20features%20and%202%29%20employs%20a%20soft%20scale%20gate%0Amechanism%20to%20deal%20with%20multi-granularity%20ambiguity%20in%203D%20segmentation%20through%0Aadjusting%20the%20magnitude%20of%20each%20feature%20channel%20according%20to%20a%20specified%203D%0Aphysical%20scale.%20Evaluations%20demonstrate%20that%20SAGA%20achieves%20real-time%0Amulti-granularity%20segmentation%20with%20quality%20comparable%20to%20state-of-the-art%0Amethods.%20As%20one%20of%20the%20first%20methods%20addressing%20promptable%20segmentation%20in%0A3D-GS%2C%20the%20simplicity%20and%20effectiveness%20of%20SAGA%20pave%20the%20way%20for%20future%0Aadvancements%20in%20this%20field.%20Our%20code%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00860v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegment%2520Any%25203D%2520Gaussians%26entry.906535625%3DJiazhong%2520Cen%2520and%2520Jiemin%2520Fang%2520and%2520Chen%2520Yang%2520and%2520Lingxi%2520Xie%2520and%2520Xiaopeng%2520Zhang%2520and%2520Wei%2520Shen%2520and%2520Qi%2520Tian%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520SAGA%2520%2528Segment%2520Any%25203D%2520GAussians%2529%252C%2520a%2520highly%2520efficient%25203D%250Apromptable%2520segmentation%2520method%2520based%2520on%25203D%2520Gaussian%2520Splatting%2520%25283D-GS%2529.%2520Given%25202D%250Avisual%2520prompts%2520as%2520input%252C%2520SAGA%2520can%2520segment%2520the%2520corresponding%25203D%2520target%250Arepresented%2520by%25203D%2520Gaussians%2520within%25204%2520ms.%2520This%2520is%2520achieved%2520by%2520attaching%2520an%250Ascale-gated%2520affinity%2520feature%2520to%2520each%25203D%2520Gaussian%2520to%2520endow%2520it%2520a%2520new%2520property%250Atowards%2520multi-granularity%2520segmentation.%2520Specifically%252C%2520a%2520scale-aware%2520contrastive%250Atraining%2520strategy%2520is%2520proposed%2520for%2520the%2520scale-gated%2520affinity%2520feature%2520learning.%2520It%250A1%2529%2520distills%2520the%2520segmentation%2520capability%2520of%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%250Afrom%25202D%2520masks%2520into%2520the%2520affinity%2520features%2520and%25202%2529%2520employs%2520a%2520soft%2520scale%2520gate%250Amechanism%2520to%2520deal%2520with%2520multi-granularity%2520ambiguity%2520in%25203D%2520segmentation%2520through%250Aadjusting%2520the%2520magnitude%2520of%2520each%2520feature%2520channel%2520according%2520to%2520a%2520specified%25203D%250Aphysical%2520scale.%2520Evaluations%2520demonstrate%2520that%2520SAGA%2520achieves%2520real-time%250Amulti-granularity%2520segmentation%2520with%2520quality%2520comparable%2520to%2520state-of-the-art%250Amethods.%2520As%2520one%2520of%2520the%2520first%2520methods%2520addressing%2520promptable%2520segmentation%2520in%250A3D-GS%252C%2520the%2520simplicity%2520and%2520effectiveness%2520of%2520SAGA%2520pave%2520the%2520way%2520for%2520future%250Aadvancements%2520in%2520this%2520field.%2520Our%2520code%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.00860v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Segment%20Any%203D%20Gaussians&entry.906535625=Jiazhong%20Cen%20and%20Jiemin%20Fang%20and%20Chen%20Yang%20and%20Lingxi%20Xie%20and%20Xiaopeng%20Zhang%20and%20Wei%20Shen%20and%20Qi%20Tian&entry.1292438233=%20%20This%20paper%20presents%20SAGA%20%28Segment%20Any%203D%20GAussians%29%2C%20a%20highly%20efficient%203D%0Apromptable%20segmentation%20method%20based%20on%203D%20Gaussian%20Splatting%20%283D-GS%29.%20Given%202D%0Avisual%20prompts%20as%20input%2C%20SAGA%20can%20segment%20the%20corresponding%203D%20target%0Arepresented%20by%203D%20Gaussians%20within%204%20ms.%20This%20is%20achieved%20by%20attaching%20an%0Ascale-gated%20affinity%20feature%20to%20each%203D%20Gaussian%20to%20endow%20it%20a%20new%20property%0Atowards%20multi-granularity%20segmentation.%20Specifically%2C%20a%20scale-aware%20contrastive%0Atraining%20strategy%20is%20proposed%20for%20the%20scale-gated%20affinity%20feature%20learning.%20It%0A1%29%20distills%20the%20segmentation%20capability%20of%20the%20Segment%20Anything%20Model%20%28SAM%29%0Afrom%202D%20masks%20into%20the%20affinity%20features%20and%202%29%20employs%20a%20soft%20scale%20gate%0Amechanism%20to%20deal%20with%20multi-granularity%20ambiguity%20in%203D%20segmentation%20through%0Aadjusting%20the%20magnitude%20of%20each%20feature%20channel%20according%20to%20a%20specified%203D%0Aphysical%20scale.%20Evaluations%20demonstrate%20that%20SAGA%20achieves%20real-time%0Amulti-granularity%20segmentation%20with%20quality%20comparable%20to%20state-of-the-art%0Amethods.%20As%20one%20of%20the%20first%20methods%20addressing%20promptable%20segmentation%20in%0A3D-GS%2C%20the%20simplicity%20and%20effectiveness%20of%20SAGA%20pave%20the%20way%20for%20future%0Aadvancements%20in%20this%20field.%20Our%20code%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00860v3&entry.124074799=Read"},
{"title": "SKI Models: Skeleton Induced Vision-Language Embeddings for\n  Understanding Activities of Daily Living", "author": "Arkaprava Sinha and Dominick Reilly and Francois Bremond and Pu Wang and Srijan Das", "abstract": "  The introduction of vision-language models like CLIP has enabled the\ndevelopment of foundational video models capable of generalizing to unseen\nvideos and human actions. However, these models are typically trained on web\nvideos, which often fail to capture the challenges present in Activities of\nDaily Living (ADL) videos. Existing works address ADL-specific challenges, such\nas similar appearances, subtle motion patterns, and multiple viewpoints, by\ncombining 3D skeletons and RGB videos. However, these approaches are not\nintegrated with language, limiting their ability to generalize to unseen action\nclasses. In this paper, we introduce SKI models, which integrate 3D skeletons\ninto the vision-language embedding space. SKI models leverage a\nskeleton-language model, SkeletonCLIP, to infuse skeleton information into\nVision Language Models (VLMs) and Large Vision Language Models (LVLMs) through\ncollaborative training. Notably, SKI models do not require skeleton data during\ninference, enhancing their robustness for real-world applications. The\neffectiveness of SKI models is validated on three popular ADL datasets for\nzero-shot action recognition and video caption generation tasks.\n", "link": "http://arxiv.org/abs/2502.03459v1", "date": "2025-02-05", "relevancy": 2.9997, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6151}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6151}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SKI%20Models%3A%20Skeleton%20Induced%20Vision-Language%20Embeddings%20for%0A%20%20Understanding%20Activities%20of%20Daily%20Living&body=Title%3A%20SKI%20Models%3A%20Skeleton%20Induced%20Vision-Language%20Embeddings%20for%0A%20%20Understanding%20Activities%20of%20Daily%20Living%0AAuthor%3A%20Arkaprava%20Sinha%20and%20Dominick%20Reilly%20and%20Francois%20Bremond%20and%20Pu%20Wang%20and%20Srijan%20Das%0AAbstract%3A%20%20%20The%20introduction%20of%20vision-language%20models%20like%20CLIP%20has%20enabled%20the%0Adevelopment%20of%20foundational%20video%20models%20capable%20of%20generalizing%20to%20unseen%0Avideos%20and%20human%20actions.%20However%2C%20these%20models%20are%20typically%20trained%20on%20web%0Avideos%2C%20which%20often%20fail%20to%20capture%20the%20challenges%20present%20in%20Activities%20of%0ADaily%20Living%20%28ADL%29%20videos.%20Existing%20works%20address%20ADL-specific%20challenges%2C%20such%0Aas%20similar%20appearances%2C%20subtle%20motion%20patterns%2C%20and%20multiple%20viewpoints%2C%20by%0Acombining%203D%20skeletons%20and%20RGB%20videos.%20However%2C%20these%20approaches%20are%20not%0Aintegrated%20with%20language%2C%20limiting%20their%20ability%20to%20generalize%20to%20unseen%20action%0Aclasses.%20In%20this%20paper%2C%20we%20introduce%20SKI%20models%2C%20which%20integrate%203D%20skeletons%0Ainto%20the%20vision-language%20embedding%20space.%20SKI%20models%20leverage%20a%0Askeleton-language%20model%2C%20SkeletonCLIP%2C%20to%20infuse%20skeleton%20information%20into%0AVision%20Language%20Models%20%28VLMs%29%20and%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20through%0Acollaborative%20training.%20Notably%2C%20SKI%20models%20do%20not%20require%20skeleton%20data%20during%0Ainference%2C%20enhancing%20their%20robustness%20for%20real-world%20applications.%20The%0Aeffectiveness%20of%20SKI%20models%20is%20validated%20on%20three%20popular%20ADL%20datasets%20for%0Azero-shot%20action%20recognition%20and%20video%20caption%20generation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03459v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSKI%2520Models%253A%2520Skeleton%2520Induced%2520Vision-Language%2520Embeddings%2520for%250A%2520%2520Understanding%2520Activities%2520of%2520Daily%2520Living%26entry.906535625%3DArkaprava%2520Sinha%2520and%2520Dominick%2520Reilly%2520and%2520Francois%2520Bremond%2520and%2520Pu%2520Wang%2520and%2520Srijan%2520Das%26entry.1292438233%3D%2520%2520The%2520introduction%2520of%2520vision-language%2520models%2520like%2520CLIP%2520has%2520enabled%2520the%250Adevelopment%2520of%2520foundational%2520video%2520models%2520capable%2520of%2520generalizing%2520to%2520unseen%250Avideos%2520and%2520human%2520actions.%2520However%252C%2520these%2520models%2520are%2520typically%2520trained%2520on%2520web%250Avideos%252C%2520which%2520often%2520fail%2520to%2520capture%2520the%2520challenges%2520present%2520in%2520Activities%2520of%250ADaily%2520Living%2520%2528ADL%2529%2520videos.%2520Existing%2520works%2520address%2520ADL-specific%2520challenges%252C%2520such%250Aas%2520similar%2520appearances%252C%2520subtle%2520motion%2520patterns%252C%2520and%2520multiple%2520viewpoints%252C%2520by%250Acombining%25203D%2520skeletons%2520and%2520RGB%2520videos.%2520However%252C%2520these%2520approaches%2520are%2520not%250Aintegrated%2520with%2520language%252C%2520limiting%2520their%2520ability%2520to%2520generalize%2520to%2520unseen%2520action%250Aclasses.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520SKI%2520models%252C%2520which%2520integrate%25203D%2520skeletons%250Ainto%2520the%2520vision-language%2520embedding%2520space.%2520SKI%2520models%2520leverage%2520a%250Askeleton-language%2520model%252C%2520SkeletonCLIP%252C%2520to%2520infuse%2520skeleton%2520information%2520into%250AVision%2520Language%2520Models%2520%2528VLMs%2529%2520and%2520Large%2520Vision%2520Language%2520Models%2520%2528LVLMs%2529%2520through%250Acollaborative%2520training.%2520Notably%252C%2520SKI%2520models%2520do%2520not%2520require%2520skeleton%2520data%2520during%250Ainference%252C%2520enhancing%2520their%2520robustness%2520for%2520real-world%2520applications.%2520The%250Aeffectiveness%2520of%2520SKI%2520models%2520is%2520validated%2520on%2520three%2520popular%2520ADL%2520datasets%2520for%250Azero-shot%2520action%2520recognition%2520and%2520video%2520caption%2520generation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03459v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SKI%20Models%3A%20Skeleton%20Induced%20Vision-Language%20Embeddings%20for%0A%20%20Understanding%20Activities%20of%20Daily%20Living&entry.906535625=Arkaprava%20Sinha%20and%20Dominick%20Reilly%20and%20Francois%20Bremond%20and%20Pu%20Wang%20and%20Srijan%20Das&entry.1292438233=%20%20The%20introduction%20of%20vision-language%20models%20like%20CLIP%20has%20enabled%20the%0Adevelopment%20of%20foundational%20video%20models%20capable%20of%20generalizing%20to%20unseen%0Avideos%20and%20human%20actions.%20However%2C%20these%20models%20are%20typically%20trained%20on%20web%0Avideos%2C%20which%20often%20fail%20to%20capture%20the%20challenges%20present%20in%20Activities%20of%0ADaily%20Living%20%28ADL%29%20videos.%20Existing%20works%20address%20ADL-specific%20challenges%2C%20such%0Aas%20similar%20appearances%2C%20subtle%20motion%20patterns%2C%20and%20multiple%20viewpoints%2C%20by%0Acombining%203D%20skeletons%20and%20RGB%20videos.%20However%2C%20these%20approaches%20are%20not%0Aintegrated%20with%20language%2C%20limiting%20their%20ability%20to%20generalize%20to%20unseen%20action%0Aclasses.%20In%20this%20paper%2C%20we%20introduce%20SKI%20models%2C%20which%20integrate%203D%20skeletons%0Ainto%20the%20vision-language%20embedding%20space.%20SKI%20models%20leverage%20a%0Askeleton-language%20model%2C%20SkeletonCLIP%2C%20to%20infuse%20skeleton%20information%20into%0AVision%20Language%20Models%20%28VLMs%29%20and%20Large%20Vision%20Language%20Models%20%28LVLMs%29%20through%0Acollaborative%20training.%20Notably%2C%20SKI%20models%20do%20not%20require%20skeleton%20data%20during%0Ainference%2C%20enhancing%20their%20robustness%20for%20real-world%20applications.%20The%0Aeffectiveness%20of%20SKI%20models%20is%20validated%20on%20three%20popular%20ADL%20datasets%20for%0Azero-shot%20action%20recognition%20and%20video%20caption%20generation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03459v1&entry.124074799=Read"},
{"title": "iVISPAR -- An Interactive Visual-Spatial Reasoning Benchmark for VLMs", "author": "Julius Mayer and Mohamad Ballout and Serwan Jassim and Farbod Nosrat Nezami and Elia Bruni", "abstract": "  Vision-Language Models (VLMs) are known to struggle with spatial reasoning\nand visual alignment. To help overcome these limitations, we introduce iVISPAR,\nan interactive multi-modal benchmark designed to evaluate the spatial reasoning\ncapabilities of VLMs acting as agents. iVISPAR is based on a variant of the\nsliding tile puzzle-a classic problem that demands logical planning, spatial\nawareness, and multi-step reasoning. The benchmark supports visual 2D, 3D, and\ntext-based input modalities, enabling comprehensive assessments of VLMs'\nplanning and reasoning skills. We evaluate a broad suite of state-of-the-art\nopen-source and closed-source VLMs, comparing their performance while also\nproviding optimal path solutions and a human baseline to assess the task's\ncomplexity and feasibility for humans. Results indicate that while some VLMs\nperform well on simple spatial tasks, they encounter difficulties with more\ncomplex configurations and problem properties. Notably, while VLMs generally\nperform better in 2D vision compared to 3D or text-based representations, they\nconsistently fall short of human performance, illustrating the persistent\nchallenge of visual alignment. This highlights critical gaps in current VLM\ncapabilities, highlighting their limitations in achieving human-level\ncognition.\n", "link": "http://arxiv.org/abs/2502.03214v1", "date": "2025-02-05", "relevancy": 2.9907, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6186}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6186}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20iVISPAR%20--%20An%20Interactive%20Visual-Spatial%20Reasoning%20Benchmark%20for%20VLMs&body=Title%3A%20iVISPAR%20--%20An%20Interactive%20Visual-Spatial%20Reasoning%20Benchmark%20for%20VLMs%0AAuthor%3A%20Julius%20Mayer%20and%20Mohamad%20Ballout%20and%20Serwan%20Jassim%20and%20Farbod%20Nosrat%20Nezami%20and%20Elia%20Bruni%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20are%20known%20to%20struggle%20with%20spatial%20reasoning%0Aand%20visual%20alignment.%20To%20help%20overcome%20these%20limitations%2C%20we%20introduce%20iVISPAR%2C%0Aan%20interactive%20multi-modal%20benchmark%20designed%20to%20evaluate%20the%20spatial%20reasoning%0Acapabilities%20of%20VLMs%20acting%20as%20agents.%20iVISPAR%20is%20based%20on%20a%20variant%20of%20the%0Asliding%20tile%20puzzle-a%20classic%20problem%20that%20demands%20logical%20planning%2C%20spatial%0Aawareness%2C%20and%20multi-step%20reasoning.%20The%20benchmark%20supports%20visual%202D%2C%203D%2C%20and%0Atext-based%20input%20modalities%2C%20enabling%20comprehensive%20assessments%20of%20VLMs%27%0Aplanning%20and%20reasoning%20skills.%20We%20evaluate%20a%20broad%20suite%20of%20state-of-the-art%0Aopen-source%20and%20closed-source%20VLMs%2C%20comparing%20their%20performance%20while%20also%0Aproviding%20optimal%20path%20solutions%20and%20a%20human%20baseline%20to%20assess%20the%20task%27s%0Acomplexity%20and%20feasibility%20for%20humans.%20Results%20indicate%20that%20while%20some%20VLMs%0Aperform%20well%20on%20simple%20spatial%20tasks%2C%20they%20encounter%20difficulties%20with%20more%0Acomplex%20configurations%20and%20problem%20properties.%20Notably%2C%20while%20VLMs%20generally%0Aperform%20better%20in%202D%20vision%20compared%20to%203D%20or%20text-based%20representations%2C%20they%0Aconsistently%20fall%20short%20of%20human%20performance%2C%20illustrating%20the%20persistent%0Achallenge%20of%20visual%20alignment.%20This%20highlights%20critical%20gaps%20in%20current%20VLM%0Acapabilities%2C%20highlighting%20their%20limitations%20in%20achieving%20human-level%0Acognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03214v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DiVISPAR%2520--%2520An%2520Interactive%2520Visual-Spatial%2520Reasoning%2520Benchmark%2520for%2520VLMs%26entry.906535625%3DJulius%2520Mayer%2520and%2520Mohamad%2520Ballout%2520and%2520Serwan%2520Jassim%2520and%2520Farbod%2520Nosrat%2520Nezami%2520and%2520Elia%2520Bruni%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520are%2520known%2520to%2520struggle%2520with%2520spatial%2520reasoning%250Aand%2520visual%2520alignment.%2520To%2520help%2520overcome%2520these%2520limitations%252C%2520we%2520introduce%2520iVISPAR%252C%250Aan%2520interactive%2520multi-modal%2520benchmark%2520designed%2520to%2520evaluate%2520the%2520spatial%2520reasoning%250Acapabilities%2520of%2520VLMs%2520acting%2520as%2520agents.%2520iVISPAR%2520is%2520based%2520on%2520a%2520variant%2520of%2520the%250Asliding%2520tile%2520puzzle-a%2520classic%2520problem%2520that%2520demands%2520logical%2520planning%252C%2520spatial%250Aawareness%252C%2520and%2520multi-step%2520reasoning.%2520The%2520benchmark%2520supports%2520visual%25202D%252C%25203D%252C%2520and%250Atext-based%2520input%2520modalities%252C%2520enabling%2520comprehensive%2520assessments%2520of%2520VLMs%2527%250Aplanning%2520and%2520reasoning%2520skills.%2520We%2520evaluate%2520a%2520broad%2520suite%2520of%2520state-of-the-art%250Aopen-source%2520and%2520closed-source%2520VLMs%252C%2520comparing%2520their%2520performance%2520while%2520also%250Aproviding%2520optimal%2520path%2520solutions%2520and%2520a%2520human%2520baseline%2520to%2520assess%2520the%2520task%2527s%250Acomplexity%2520and%2520feasibility%2520for%2520humans.%2520Results%2520indicate%2520that%2520while%2520some%2520VLMs%250Aperform%2520well%2520on%2520simple%2520spatial%2520tasks%252C%2520they%2520encounter%2520difficulties%2520with%2520more%250Acomplex%2520configurations%2520and%2520problem%2520properties.%2520Notably%252C%2520while%2520VLMs%2520generally%250Aperform%2520better%2520in%25202D%2520vision%2520compared%2520to%25203D%2520or%2520text-based%2520representations%252C%2520they%250Aconsistently%2520fall%2520short%2520of%2520human%2520performance%252C%2520illustrating%2520the%2520persistent%250Achallenge%2520of%2520visual%2520alignment.%2520This%2520highlights%2520critical%2520gaps%2520in%2520current%2520VLM%250Acapabilities%252C%2520highlighting%2520their%2520limitations%2520in%2520achieving%2520human-level%250Acognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03214v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=iVISPAR%20--%20An%20Interactive%20Visual-Spatial%20Reasoning%20Benchmark%20for%20VLMs&entry.906535625=Julius%20Mayer%20and%20Mohamad%20Ballout%20and%20Serwan%20Jassim%20and%20Farbod%20Nosrat%20Nezami%20and%20Elia%20Bruni&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20are%20known%20to%20struggle%20with%20spatial%20reasoning%0Aand%20visual%20alignment.%20To%20help%20overcome%20these%20limitations%2C%20we%20introduce%20iVISPAR%2C%0Aan%20interactive%20multi-modal%20benchmark%20designed%20to%20evaluate%20the%20spatial%20reasoning%0Acapabilities%20of%20VLMs%20acting%20as%20agents.%20iVISPAR%20is%20based%20on%20a%20variant%20of%20the%0Asliding%20tile%20puzzle-a%20classic%20problem%20that%20demands%20logical%20planning%2C%20spatial%0Aawareness%2C%20and%20multi-step%20reasoning.%20The%20benchmark%20supports%20visual%202D%2C%203D%2C%20and%0Atext-based%20input%20modalities%2C%20enabling%20comprehensive%20assessments%20of%20VLMs%27%0Aplanning%20and%20reasoning%20skills.%20We%20evaluate%20a%20broad%20suite%20of%20state-of-the-art%0Aopen-source%20and%20closed-source%20VLMs%2C%20comparing%20their%20performance%20while%20also%0Aproviding%20optimal%20path%20solutions%20and%20a%20human%20baseline%20to%20assess%20the%20task%27s%0Acomplexity%20and%20feasibility%20for%20humans.%20Results%20indicate%20that%20while%20some%20VLMs%0Aperform%20well%20on%20simple%20spatial%20tasks%2C%20they%20encounter%20difficulties%20with%20more%0Acomplex%20configurations%20and%20problem%20properties.%20Notably%2C%20while%20VLMs%20generally%0Aperform%20better%20in%202D%20vision%20compared%20to%203D%20or%20text-based%20representations%2C%20they%0Aconsistently%20fall%20short%20of%20human%20performance%2C%20illustrating%20the%20persistent%0Achallenge%20of%20visual%20alignment.%20This%20highlights%20critical%20gaps%20in%20current%20VLM%0Acapabilities%2C%20highlighting%20their%20limitations%20in%20achieving%20human-level%0Acognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03214v1&entry.124074799=Read"},
{"title": "3D Face Reconstruction From Radar Images", "author": "Valentin Braeutigam and Vanessa Wirth and Ingrid Ullmann and Christian Sch\u00fc\u00dfler and Martin Vossiek and Matthias Berking and Bernhard Egger", "abstract": "  The 3D reconstruction of faces gains wide attention in computer vision and is\nused in many fields of application, for example, animation, virtual reality,\nand even forensics. This work is motivated by monitoring patients in sleep\nlaboratories. Due to their unique characteristics, sensors from the radar\ndomain have advantages compared to optical sensors, namely penetration of\nelectrically non-conductive materials and independence of light. These\nadvantages of radar signals unlock new applications and require adaptation of\n3D reconstruction frameworks. We propose a novel model-based method for 3D\nreconstruction from radar images. We generate a dataset of synthetic radar\nimages with a physics-based but non-differentiable radar renderer. This dataset\nis used to train a CNN-based encoder to estimate the parameters of a 3D\nmorphable face model. Whilst the encoder alone already leads to strong\nreconstructions of synthetic data, we extend our reconstruction in an\nAnalysis-by-Synthesis fashion to a model-based autoencoder. This is enabled by\nlearning the rendering process in the decoder, which acts as an object-specific\ndifferentiable radar renderer. Subsequently, the combination of both network\nparts is trained to minimize both, the loss of the parameters and the loss of\nthe resulting reconstructed radar image. This leads to the additional benefit,\nthat at test time the parameters can be further optimized by finetuning the\nautoencoder unsupervised on the image loss. We evaluated our framework on\ngenerated synthetic face images as well as on real radar images with 3D ground\ntruth of four individuals.\n", "link": "http://arxiv.org/abs/2412.02403v2", "date": "2025-02-05", "relevancy": 2.9482, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5961}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5864}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Face%20Reconstruction%20From%20Radar%20Images&body=Title%3A%203D%20Face%20Reconstruction%20From%20Radar%20Images%0AAuthor%3A%20Valentin%20Braeutigam%20and%20Vanessa%20Wirth%20and%20Ingrid%20Ullmann%20and%20Christian%20Sch%C3%BC%C3%9Fler%20and%20Martin%20Vossiek%20and%20Matthias%20Berking%20and%20Bernhard%20Egger%0AAbstract%3A%20%20%20The%203D%20reconstruction%20of%20faces%20gains%20wide%20attention%20in%20computer%20vision%20and%20is%0Aused%20in%20many%20fields%20of%20application%2C%20for%20example%2C%20animation%2C%20virtual%20reality%2C%0Aand%20even%20forensics.%20This%20work%20is%20motivated%20by%20monitoring%20patients%20in%20sleep%0Alaboratories.%20Due%20to%20their%20unique%20characteristics%2C%20sensors%20from%20the%20radar%0Adomain%20have%20advantages%20compared%20to%20optical%20sensors%2C%20namely%20penetration%20of%0Aelectrically%20non-conductive%20materials%20and%20independence%20of%20light.%20These%0Aadvantages%20of%20radar%20signals%20unlock%20new%20applications%20and%20require%20adaptation%20of%0A3D%20reconstruction%20frameworks.%20We%20propose%20a%20novel%20model-based%20method%20for%203D%0Areconstruction%20from%20radar%20images.%20We%20generate%20a%20dataset%20of%20synthetic%20radar%0Aimages%20with%20a%20physics-based%20but%20non-differentiable%20radar%20renderer.%20This%20dataset%0Ais%20used%20to%20train%20a%20CNN-based%20encoder%20to%20estimate%20the%20parameters%20of%20a%203D%0Amorphable%20face%20model.%20Whilst%20the%20encoder%20alone%20already%20leads%20to%20strong%0Areconstructions%20of%20synthetic%20data%2C%20we%20extend%20our%20reconstruction%20in%20an%0AAnalysis-by-Synthesis%20fashion%20to%20a%20model-based%20autoencoder.%20This%20is%20enabled%20by%0Alearning%20the%20rendering%20process%20in%20the%20decoder%2C%20which%20acts%20as%20an%20object-specific%0Adifferentiable%20radar%20renderer.%20Subsequently%2C%20the%20combination%20of%20both%20network%0Aparts%20is%20trained%20to%20minimize%20both%2C%20the%20loss%20of%20the%20parameters%20and%20the%20loss%20of%0Athe%20resulting%20reconstructed%20radar%20image.%20This%20leads%20to%20the%20additional%20benefit%2C%0Athat%20at%20test%20time%20the%20parameters%20can%20be%20further%20optimized%20by%20finetuning%20the%0Aautoencoder%20unsupervised%20on%20the%20image%20loss.%20We%20evaluated%20our%20framework%20on%0Agenerated%20synthetic%20face%20images%20as%20well%20as%20on%20real%20radar%20images%20with%203D%20ground%0Atruth%20of%20four%20individuals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.02403v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Face%2520Reconstruction%2520From%2520Radar%2520Images%26entry.906535625%3DValentin%2520Braeutigam%2520and%2520Vanessa%2520Wirth%2520and%2520Ingrid%2520Ullmann%2520and%2520Christian%2520Sch%25C3%25BC%25C3%259Fler%2520and%2520Martin%2520Vossiek%2520and%2520Matthias%2520Berking%2520and%2520Bernhard%2520Egger%26entry.1292438233%3D%2520%2520The%25203D%2520reconstruction%2520of%2520faces%2520gains%2520wide%2520attention%2520in%2520computer%2520vision%2520and%2520is%250Aused%2520in%2520many%2520fields%2520of%2520application%252C%2520for%2520example%252C%2520animation%252C%2520virtual%2520reality%252C%250Aand%2520even%2520forensics.%2520This%2520work%2520is%2520motivated%2520by%2520monitoring%2520patients%2520in%2520sleep%250Alaboratories.%2520Due%2520to%2520their%2520unique%2520characteristics%252C%2520sensors%2520from%2520the%2520radar%250Adomain%2520have%2520advantages%2520compared%2520to%2520optical%2520sensors%252C%2520namely%2520penetration%2520of%250Aelectrically%2520non-conductive%2520materials%2520and%2520independence%2520of%2520light.%2520These%250Aadvantages%2520of%2520radar%2520signals%2520unlock%2520new%2520applications%2520and%2520require%2520adaptation%2520of%250A3D%2520reconstruction%2520frameworks.%2520We%2520propose%2520a%2520novel%2520model-based%2520method%2520for%25203D%250Areconstruction%2520from%2520radar%2520images.%2520We%2520generate%2520a%2520dataset%2520of%2520synthetic%2520radar%250Aimages%2520with%2520a%2520physics-based%2520but%2520non-differentiable%2520radar%2520renderer.%2520This%2520dataset%250Ais%2520used%2520to%2520train%2520a%2520CNN-based%2520encoder%2520to%2520estimate%2520the%2520parameters%2520of%2520a%25203D%250Amorphable%2520face%2520model.%2520Whilst%2520the%2520encoder%2520alone%2520already%2520leads%2520to%2520strong%250Areconstructions%2520of%2520synthetic%2520data%252C%2520we%2520extend%2520our%2520reconstruction%2520in%2520an%250AAnalysis-by-Synthesis%2520fashion%2520to%2520a%2520model-based%2520autoencoder.%2520This%2520is%2520enabled%2520by%250Alearning%2520the%2520rendering%2520process%2520in%2520the%2520decoder%252C%2520which%2520acts%2520as%2520an%2520object-specific%250Adifferentiable%2520radar%2520renderer.%2520Subsequently%252C%2520the%2520combination%2520of%2520both%2520network%250Aparts%2520is%2520trained%2520to%2520minimize%2520both%252C%2520the%2520loss%2520of%2520the%2520parameters%2520and%2520the%2520loss%2520of%250Athe%2520resulting%2520reconstructed%2520radar%2520image.%2520This%2520leads%2520to%2520the%2520additional%2520benefit%252C%250Athat%2520at%2520test%2520time%2520the%2520parameters%2520can%2520be%2520further%2520optimized%2520by%2520finetuning%2520the%250Aautoencoder%2520unsupervised%2520on%2520the%2520image%2520loss.%2520We%2520evaluated%2520our%2520framework%2520on%250Agenerated%2520synthetic%2520face%2520images%2520as%2520well%2520as%2520on%2520real%2520radar%2520images%2520with%25203D%2520ground%250Atruth%2520of%2520four%2520individuals.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.02403v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Face%20Reconstruction%20From%20Radar%20Images&entry.906535625=Valentin%20Braeutigam%20and%20Vanessa%20Wirth%20and%20Ingrid%20Ullmann%20and%20Christian%20Sch%C3%BC%C3%9Fler%20and%20Martin%20Vossiek%20and%20Matthias%20Berking%20and%20Bernhard%20Egger&entry.1292438233=%20%20The%203D%20reconstruction%20of%20faces%20gains%20wide%20attention%20in%20computer%20vision%20and%20is%0Aused%20in%20many%20fields%20of%20application%2C%20for%20example%2C%20animation%2C%20virtual%20reality%2C%0Aand%20even%20forensics.%20This%20work%20is%20motivated%20by%20monitoring%20patients%20in%20sleep%0Alaboratories.%20Due%20to%20their%20unique%20characteristics%2C%20sensors%20from%20the%20radar%0Adomain%20have%20advantages%20compared%20to%20optical%20sensors%2C%20namely%20penetration%20of%0Aelectrically%20non-conductive%20materials%20and%20independence%20of%20light.%20These%0Aadvantages%20of%20radar%20signals%20unlock%20new%20applications%20and%20require%20adaptation%20of%0A3D%20reconstruction%20frameworks.%20We%20propose%20a%20novel%20model-based%20method%20for%203D%0Areconstruction%20from%20radar%20images.%20We%20generate%20a%20dataset%20of%20synthetic%20radar%0Aimages%20with%20a%20physics-based%20but%20non-differentiable%20radar%20renderer.%20This%20dataset%0Ais%20used%20to%20train%20a%20CNN-based%20encoder%20to%20estimate%20the%20parameters%20of%20a%203D%0Amorphable%20face%20model.%20Whilst%20the%20encoder%20alone%20already%20leads%20to%20strong%0Areconstructions%20of%20synthetic%20data%2C%20we%20extend%20our%20reconstruction%20in%20an%0AAnalysis-by-Synthesis%20fashion%20to%20a%20model-based%20autoencoder.%20This%20is%20enabled%20by%0Alearning%20the%20rendering%20process%20in%20the%20decoder%2C%20which%20acts%20as%20an%20object-specific%0Adifferentiable%20radar%20renderer.%20Subsequently%2C%20the%20combination%20of%20both%20network%0Aparts%20is%20trained%20to%20minimize%20both%2C%20the%20loss%20of%20the%20parameters%20and%20the%20loss%20of%0Athe%20resulting%20reconstructed%20radar%20image.%20This%20leads%20to%20the%20additional%20benefit%2C%0Athat%20at%20test%20time%20the%20parameters%20can%20be%20further%20optimized%20by%20finetuning%20the%0Aautoencoder%20unsupervised%20on%20the%20image%20loss.%20We%20evaluated%20our%20framework%20on%0Agenerated%20synthetic%20face%20images%20as%20well%20as%20on%20real%20radar%20images%20with%203D%20ground%0Atruth%20of%20four%20individuals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.02403v2&entry.124074799=Read"},
{"title": "Human-Aligned Image Models Improve Visual Decoding from the Brain", "author": "Nona Rajabi and Ant\u00f4nio H. Ribeiro and Miguel Vasco and Farzaneh Taleb and M\u00e5rten Bj\u00f6rkman and Danica Kragic", "abstract": "  Decoding visual images from brain activity has significant potential for\nadvancing brain-computer interaction and enhancing the understanding of human\nperception. Recent approaches align the representation spaces of images and\nbrain activity to enable visual decoding. In this paper, we introduce the use\nof human-aligned image encoders to map brain signals to images. We hypothesize\nthat these models more effectively capture perceptual attributes associated\nwith the rapid visual stimuli presentations commonly used in visual brain data\nrecording experiments. Our empirical results support this hypothesis,\ndemonstrating that this simple modification improves image retrieval accuracy\nby up to 21% compared to state-of-the-art methods. Comprehensive experiments\nconfirm consistent performance improvements across diverse EEG architectures,\nimage encoders, alignment methods, participants, and brain imaging modalities.\n", "link": "http://arxiv.org/abs/2502.03081v1", "date": "2025-02-05", "relevancy": 2.9369, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6189}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6189}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human-Aligned%20Image%20Models%20Improve%20Visual%20Decoding%20from%20the%20Brain&body=Title%3A%20Human-Aligned%20Image%20Models%20Improve%20Visual%20Decoding%20from%20the%20Brain%0AAuthor%3A%20Nona%20Rajabi%20and%20Ant%C3%B4nio%20H.%20Ribeiro%20and%20Miguel%20Vasco%20and%20Farzaneh%20Taleb%20and%20M%C3%A5rten%20Bj%C3%B6rkman%20and%20Danica%20Kragic%0AAbstract%3A%20%20%20Decoding%20visual%20images%20from%20brain%20activity%20has%20significant%20potential%20for%0Aadvancing%20brain-computer%20interaction%20and%20enhancing%20the%20understanding%20of%20human%0Aperception.%20Recent%20approaches%20align%20the%20representation%20spaces%20of%20images%20and%0Abrain%20activity%20to%20enable%20visual%20decoding.%20In%20this%20paper%2C%20we%20introduce%20the%20use%0Aof%20human-aligned%20image%20encoders%20to%20map%20brain%20signals%20to%20images.%20We%20hypothesize%0Athat%20these%20models%20more%20effectively%20capture%20perceptual%20attributes%20associated%0Awith%20the%20rapid%20visual%20stimuli%20presentations%20commonly%20used%20in%20visual%20brain%20data%0Arecording%20experiments.%20Our%20empirical%20results%20support%20this%20hypothesis%2C%0Ademonstrating%20that%20this%20simple%20modification%20improves%20image%20retrieval%20accuracy%0Aby%20up%20to%2021%25%20compared%20to%20state-of-the-art%20methods.%20Comprehensive%20experiments%0Aconfirm%20consistent%20performance%20improvements%20across%20diverse%20EEG%20architectures%2C%0Aimage%20encoders%2C%20alignment%20methods%2C%20participants%2C%20and%20brain%20imaging%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03081v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman-Aligned%2520Image%2520Models%2520Improve%2520Visual%2520Decoding%2520from%2520the%2520Brain%26entry.906535625%3DNona%2520Rajabi%2520and%2520Ant%25C3%25B4nio%2520H.%2520Ribeiro%2520and%2520Miguel%2520Vasco%2520and%2520Farzaneh%2520Taleb%2520and%2520M%25C3%25A5rten%2520Bj%25C3%25B6rkman%2520and%2520Danica%2520Kragic%26entry.1292438233%3D%2520%2520Decoding%2520visual%2520images%2520from%2520brain%2520activity%2520has%2520significant%2520potential%2520for%250Aadvancing%2520brain-computer%2520interaction%2520and%2520enhancing%2520the%2520understanding%2520of%2520human%250Aperception.%2520Recent%2520approaches%2520align%2520the%2520representation%2520spaces%2520of%2520images%2520and%250Abrain%2520activity%2520to%2520enable%2520visual%2520decoding.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520use%250Aof%2520human-aligned%2520image%2520encoders%2520to%2520map%2520brain%2520signals%2520to%2520images.%2520We%2520hypothesize%250Athat%2520these%2520models%2520more%2520effectively%2520capture%2520perceptual%2520attributes%2520associated%250Awith%2520the%2520rapid%2520visual%2520stimuli%2520presentations%2520commonly%2520used%2520in%2520visual%2520brain%2520data%250Arecording%2520experiments.%2520Our%2520empirical%2520results%2520support%2520this%2520hypothesis%252C%250Ademonstrating%2520that%2520this%2520simple%2520modification%2520improves%2520image%2520retrieval%2520accuracy%250Aby%2520up%2520to%252021%2525%2520compared%2520to%2520state-of-the-art%2520methods.%2520Comprehensive%2520experiments%250Aconfirm%2520consistent%2520performance%2520improvements%2520across%2520diverse%2520EEG%2520architectures%252C%250Aimage%2520encoders%252C%2520alignment%2520methods%252C%2520participants%252C%2520and%2520brain%2520imaging%2520modalities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03081v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human-Aligned%20Image%20Models%20Improve%20Visual%20Decoding%20from%20the%20Brain&entry.906535625=Nona%20Rajabi%20and%20Ant%C3%B4nio%20H.%20Ribeiro%20and%20Miguel%20Vasco%20and%20Farzaneh%20Taleb%20and%20M%C3%A5rten%20Bj%C3%B6rkman%20and%20Danica%20Kragic&entry.1292438233=%20%20Decoding%20visual%20images%20from%20brain%20activity%20has%20significant%20potential%20for%0Aadvancing%20brain-computer%20interaction%20and%20enhancing%20the%20understanding%20of%20human%0Aperception.%20Recent%20approaches%20align%20the%20representation%20spaces%20of%20images%20and%0Abrain%20activity%20to%20enable%20visual%20decoding.%20In%20this%20paper%2C%20we%20introduce%20the%20use%0Aof%20human-aligned%20image%20encoders%20to%20map%20brain%20signals%20to%20images.%20We%20hypothesize%0Athat%20these%20models%20more%20effectively%20capture%20perceptual%20attributes%20associated%0Awith%20the%20rapid%20visual%20stimuli%20presentations%20commonly%20used%20in%20visual%20brain%20data%0Arecording%20experiments.%20Our%20empirical%20results%20support%20this%20hypothesis%2C%0Ademonstrating%20that%20this%20simple%20modification%20improves%20image%20retrieval%20accuracy%0Aby%20up%20to%2021%25%20compared%20to%20state-of-the-art%20methods.%20Comprehensive%20experiments%0Aconfirm%20consistent%20performance%20improvements%20across%20diverse%20EEG%20architectures%2C%0Aimage%20encoders%2C%20alignment%20methods%2C%20participants%2C%20and%20brain%20imaging%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03081v1&entry.124074799=Read"},
{"title": "ZISVFM: Zero-Shot Object Instance Segmentation in Indoor Robotic\n  Environments with Vision Foundation Models", "author": "Ying Zhang and Maoliang Yin and Wenfu Bi and Haibao Yan and Shaohan Bian and Cui-Hua Zhang and Changchun Hua", "abstract": "  Service robots operating in unstructured environments must effectively\nrecognize and segment unknown objects to enhance their functionality.\nTraditional supervised learningbased segmentation techniques require extensive\nannotated datasets, which are impractical for the diversity of objects\nencountered in real-world scenarios. Unseen Object Instance Segmentation (UOIS)\nmethods aim to address this by training models on synthetic data to generalize\nto novel objects, but they often suffer from the simulation-to-reality gap.\nThis paper proposes a novel approach (ZISVFM) for solving UOIS by leveraging\nthe powerful zero-shot capability of the segment anything model (SAM) and\nexplicit visual representations from a selfsupervised vision transformer (ViT).\nThe proposed framework operates in three stages: (1) generating object-agnostic\nmask proposals from colorized depth images using SAM, (2) refining these\nproposals using attention-based features from the selfsupervised ViT to filter\nnon-object masks, and (3) applying K-Medoids clustering to generate point\nprompts that guide SAM towards precise object segmentation. Experimental\nvalidation on two benchmark datasets and a self-collected dataset demonstrates\nthe superior performance of ZISVFM in complex environments, including\nhierarchical settings such as cabinets, drawers, and handheld objects. Our\nsource code is available at https://github.com/Yinmlmaoliang/zisvfm.\n", "link": "http://arxiv.org/abs/2502.03266v1", "date": "2025-02-05", "relevancy": 2.9348, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5875}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5867}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5867}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ZISVFM%3A%20Zero-Shot%20Object%20Instance%20Segmentation%20in%20Indoor%20Robotic%0A%20%20Environments%20with%20Vision%20Foundation%20Models&body=Title%3A%20ZISVFM%3A%20Zero-Shot%20Object%20Instance%20Segmentation%20in%20Indoor%20Robotic%0A%20%20Environments%20with%20Vision%20Foundation%20Models%0AAuthor%3A%20Ying%20Zhang%20and%20Maoliang%20Yin%20and%20Wenfu%20Bi%20and%20Haibao%20Yan%20and%20Shaohan%20Bian%20and%20Cui-Hua%20Zhang%20and%20Changchun%20Hua%0AAbstract%3A%20%20%20Service%20robots%20operating%20in%20unstructured%20environments%20must%20effectively%0Arecognize%20and%20segment%20unknown%20objects%20to%20enhance%20their%20functionality.%0ATraditional%20supervised%20learningbased%20segmentation%20techniques%20require%20extensive%0Aannotated%20datasets%2C%20which%20are%20impractical%20for%20the%20diversity%20of%20objects%0Aencountered%20in%20real-world%20scenarios.%20Unseen%20Object%20Instance%20Segmentation%20%28UOIS%29%0Amethods%20aim%20to%20address%20this%20by%20training%20models%20on%20synthetic%20data%20to%20generalize%0Ato%20novel%20objects%2C%20but%20they%20often%20suffer%20from%20the%20simulation-to-reality%20gap.%0AThis%20paper%20proposes%20a%20novel%20approach%20%28ZISVFM%29%20for%20solving%20UOIS%20by%20leveraging%0Athe%20powerful%20zero-shot%20capability%20of%20the%20segment%20anything%20model%20%28SAM%29%20and%0Aexplicit%20visual%20representations%20from%20a%20selfsupervised%20vision%20transformer%20%28ViT%29.%0AThe%20proposed%20framework%20operates%20in%20three%20stages%3A%20%281%29%20generating%20object-agnostic%0Amask%20proposals%20from%20colorized%20depth%20images%20using%20SAM%2C%20%282%29%20refining%20these%0Aproposals%20using%20attention-based%20features%20from%20the%20selfsupervised%20ViT%20to%20filter%0Anon-object%20masks%2C%20and%20%283%29%20applying%20K-Medoids%20clustering%20to%20generate%20point%0Aprompts%20that%20guide%20SAM%20towards%20precise%20object%20segmentation.%20Experimental%0Avalidation%20on%20two%20benchmark%20datasets%20and%20a%20self-collected%20dataset%20demonstrates%0Athe%20superior%20performance%20of%20ZISVFM%20in%20complex%20environments%2C%20including%0Ahierarchical%20settings%20such%20as%20cabinets%2C%20drawers%2C%20and%20handheld%20objects.%20Our%0Asource%20code%20is%20available%20at%20https%3A//github.com/Yinmlmaoliang/zisvfm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03266v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZISVFM%253A%2520Zero-Shot%2520Object%2520Instance%2520Segmentation%2520in%2520Indoor%2520Robotic%250A%2520%2520Environments%2520with%2520Vision%2520Foundation%2520Models%26entry.906535625%3DYing%2520Zhang%2520and%2520Maoliang%2520Yin%2520and%2520Wenfu%2520Bi%2520and%2520Haibao%2520Yan%2520and%2520Shaohan%2520Bian%2520and%2520Cui-Hua%2520Zhang%2520and%2520Changchun%2520Hua%26entry.1292438233%3D%2520%2520Service%2520robots%2520operating%2520in%2520unstructured%2520environments%2520must%2520effectively%250Arecognize%2520and%2520segment%2520unknown%2520objects%2520to%2520enhance%2520their%2520functionality.%250ATraditional%2520supervised%2520learningbased%2520segmentation%2520techniques%2520require%2520extensive%250Aannotated%2520datasets%252C%2520which%2520are%2520impractical%2520for%2520the%2520diversity%2520of%2520objects%250Aencountered%2520in%2520real-world%2520scenarios.%2520Unseen%2520Object%2520Instance%2520Segmentation%2520%2528UOIS%2529%250Amethods%2520aim%2520to%2520address%2520this%2520by%2520training%2520models%2520on%2520synthetic%2520data%2520to%2520generalize%250Ato%2520novel%2520objects%252C%2520but%2520they%2520often%2520suffer%2520from%2520the%2520simulation-to-reality%2520gap.%250AThis%2520paper%2520proposes%2520a%2520novel%2520approach%2520%2528ZISVFM%2529%2520for%2520solving%2520UOIS%2520by%2520leveraging%250Athe%2520powerful%2520zero-shot%2520capability%2520of%2520the%2520segment%2520anything%2520model%2520%2528SAM%2529%2520and%250Aexplicit%2520visual%2520representations%2520from%2520a%2520selfsupervised%2520vision%2520transformer%2520%2528ViT%2529.%250AThe%2520proposed%2520framework%2520operates%2520in%2520three%2520stages%253A%2520%25281%2529%2520generating%2520object-agnostic%250Amask%2520proposals%2520from%2520colorized%2520depth%2520images%2520using%2520SAM%252C%2520%25282%2529%2520refining%2520these%250Aproposals%2520using%2520attention-based%2520features%2520from%2520the%2520selfsupervised%2520ViT%2520to%2520filter%250Anon-object%2520masks%252C%2520and%2520%25283%2529%2520applying%2520K-Medoids%2520clustering%2520to%2520generate%2520point%250Aprompts%2520that%2520guide%2520SAM%2520towards%2520precise%2520object%2520segmentation.%2520Experimental%250Avalidation%2520on%2520two%2520benchmark%2520datasets%2520and%2520a%2520self-collected%2520dataset%2520demonstrates%250Athe%2520superior%2520performance%2520of%2520ZISVFM%2520in%2520complex%2520environments%252C%2520including%250Ahierarchical%2520settings%2520such%2520as%2520cabinets%252C%2520drawers%252C%2520and%2520handheld%2520objects.%2520Our%250Asource%2520code%2520is%2520available%2520at%2520https%253A//github.com/Yinmlmaoliang/zisvfm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03266v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ZISVFM%3A%20Zero-Shot%20Object%20Instance%20Segmentation%20in%20Indoor%20Robotic%0A%20%20Environments%20with%20Vision%20Foundation%20Models&entry.906535625=Ying%20Zhang%20and%20Maoliang%20Yin%20and%20Wenfu%20Bi%20and%20Haibao%20Yan%20and%20Shaohan%20Bian%20and%20Cui-Hua%20Zhang%20and%20Changchun%20Hua&entry.1292438233=%20%20Service%20robots%20operating%20in%20unstructured%20environments%20must%20effectively%0Arecognize%20and%20segment%20unknown%20objects%20to%20enhance%20their%20functionality.%0ATraditional%20supervised%20learningbased%20segmentation%20techniques%20require%20extensive%0Aannotated%20datasets%2C%20which%20are%20impractical%20for%20the%20diversity%20of%20objects%0Aencountered%20in%20real-world%20scenarios.%20Unseen%20Object%20Instance%20Segmentation%20%28UOIS%29%0Amethods%20aim%20to%20address%20this%20by%20training%20models%20on%20synthetic%20data%20to%20generalize%0Ato%20novel%20objects%2C%20but%20they%20often%20suffer%20from%20the%20simulation-to-reality%20gap.%0AThis%20paper%20proposes%20a%20novel%20approach%20%28ZISVFM%29%20for%20solving%20UOIS%20by%20leveraging%0Athe%20powerful%20zero-shot%20capability%20of%20the%20segment%20anything%20model%20%28SAM%29%20and%0Aexplicit%20visual%20representations%20from%20a%20selfsupervised%20vision%20transformer%20%28ViT%29.%0AThe%20proposed%20framework%20operates%20in%20three%20stages%3A%20%281%29%20generating%20object-agnostic%0Amask%20proposals%20from%20colorized%20depth%20images%20using%20SAM%2C%20%282%29%20refining%20these%0Aproposals%20using%20attention-based%20features%20from%20the%20selfsupervised%20ViT%20to%20filter%0Anon-object%20masks%2C%20and%20%283%29%20applying%20K-Medoids%20clustering%20to%20generate%20point%0Aprompts%20that%20guide%20SAM%20towards%20precise%20object%20segmentation.%20Experimental%0Avalidation%20on%20two%20benchmark%20datasets%20and%20a%20self-collected%20dataset%20demonstrates%0Athe%20superior%20performance%20of%20ZISVFM%20in%20complex%20environments%2C%20including%0Ahierarchical%20settings%20such%20as%20cabinets%2C%20drawers%2C%20and%20handheld%20objects.%20Our%0Asource%20code%20is%20available%20at%20https%3A//github.com/Yinmlmaoliang/zisvfm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03266v1&entry.124074799=Read"},
{"title": "Tell2Reg: Establishing spatial correspondence between images by the same\n  language prompts", "author": "Wen Yan and Qianye Yang and Shiqi Huang and Yipei Wang and Shonit Punwani and Mark Emberton and Vasilis Stavrinides and Yipeng Hu and Dean Barratt", "abstract": "  Spatial correspondence can be represented by pairs of segmented regions, such\nthat the image registration networks aim to segment corresponding regions\nrather than predicting displacement fields or transformation parameters. In\nthis work, we show that such a corresponding region pair can be predicted by\nthe same language prompt on two different images using the pre-trained large\nmultimodal models based on GroundingDINO and SAM. This enables a fully\nautomated and training-free registration algorithm, potentially generalisable\nto a wide range of image registration tasks. In this paper, we present\nexperimental results using one of the challenging tasks, registering\ninter-subject prostate MR images, which involves both highly variable intensity\nand morphology between patients. Tell2Reg is training-free, eliminating the\nneed for costly and time-consuming data curation and labelling that was\npreviously required for this registration task. This approach outperforms\nunsupervised learning-based registration methods tested, and has a performance\ncomparable to weakly-supervised methods. Additional qualitative results are\nalso presented to suggest that, for the first time, there is a potential\ncorrelation between language semantics and spatial correspondence, including\nthe spatial invariance in language-prompted regions and the difference in\nlanguage prompts between the obtained local and global correspondences. Code is\navailable at https://github.com/yanwenCi/Tell2Reg.git.\n", "link": "http://arxiv.org/abs/2502.03118v1", "date": "2025-02-05", "relevancy": 2.8509, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5834}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5733}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tell2Reg%3A%20Establishing%20spatial%20correspondence%20between%20images%20by%20the%20same%0A%20%20language%20prompts&body=Title%3A%20Tell2Reg%3A%20Establishing%20spatial%20correspondence%20between%20images%20by%20the%20same%0A%20%20language%20prompts%0AAuthor%3A%20Wen%20Yan%20and%20Qianye%20Yang%20and%20Shiqi%20Huang%20and%20Yipei%20Wang%20and%20Shonit%20Punwani%20and%20Mark%20Emberton%20and%20Vasilis%20Stavrinides%20and%20Yipeng%20Hu%20and%20Dean%20Barratt%0AAbstract%3A%20%20%20Spatial%20correspondence%20can%20be%20represented%20by%20pairs%20of%20segmented%20regions%2C%20such%0Athat%20the%20image%20registration%20networks%20aim%20to%20segment%20corresponding%20regions%0Arather%20than%20predicting%20displacement%20fields%20or%20transformation%20parameters.%20In%0Athis%20work%2C%20we%20show%20that%20such%20a%20corresponding%20region%20pair%20can%20be%20predicted%20by%0Athe%20same%20language%20prompt%20on%20two%20different%20images%20using%20the%20pre-trained%20large%0Amultimodal%20models%20based%20on%20GroundingDINO%20and%20SAM.%20This%20enables%20a%20fully%0Aautomated%20and%20training-free%20registration%20algorithm%2C%20potentially%20generalisable%0Ato%20a%20wide%20range%20of%20image%20registration%20tasks.%20In%20this%20paper%2C%20we%20present%0Aexperimental%20results%20using%20one%20of%20the%20challenging%20tasks%2C%20registering%0Ainter-subject%20prostate%20MR%20images%2C%20which%20involves%20both%20highly%20variable%20intensity%0Aand%20morphology%20between%20patients.%20Tell2Reg%20is%20training-free%2C%20eliminating%20the%0Aneed%20for%20costly%20and%20time-consuming%20data%20curation%20and%20labelling%20that%20was%0Apreviously%20required%20for%20this%20registration%20task.%20This%20approach%20outperforms%0Aunsupervised%20learning-based%20registration%20methods%20tested%2C%20and%20has%20a%20performance%0Acomparable%20to%20weakly-supervised%20methods.%20Additional%20qualitative%20results%20are%0Aalso%20presented%20to%20suggest%20that%2C%20for%20the%20first%20time%2C%20there%20is%20a%20potential%0Acorrelation%20between%20language%20semantics%20and%20spatial%20correspondence%2C%20including%0Athe%20spatial%20invariance%20in%20language-prompted%20regions%20and%20the%20difference%20in%0Alanguage%20prompts%20between%20the%20obtained%20local%20and%20global%20correspondences.%20Code%20is%0Aavailable%20at%20https%3A//github.com/yanwenCi/Tell2Reg.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03118v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTell2Reg%253A%2520Establishing%2520spatial%2520correspondence%2520between%2520images%2520by%2520the%2520same%250A%2520%2520language%2520prompts%26entry.906535625%3DWen%2520Yan%2520and%2520Qianye%2520Yang%2520and%2520Shiqi%2520Huang%2520and%2520Yipei%2520Wang%2520and%2520Shonit%2520Punwani%2520and%2520Mark%2520Emberton%2520and%2520Vasilis%2520Stavrinides%2520and%2520Yipeng%2520Hu%2520and%2520Dean%2520Barratt%26entry.1292438233%3D%2520%2520Spatial%2520correspondence%2520can%2520be%2520represented%2520by%2520pairs%2520of%2520segmented%2520regions%252C%2520such%250Athat%2520the%2520image%2520registration%2520networks%2520aim%2520to%2520segment%2520corresponding%2520regions%250Arather%2520than%2520predicting%2520displacement%2520fields%2520or%2520transformation%2520parameters.%2520In%250Athis%2520work%252C%2520we%2520show%2520that%2520such%2520a%2520corresponding%2520region%2520pair%2520can%2520be%2520predicted%2520by%250Athe%2520same%2520language%2520prompt%2520on%2520two%2520different%2520images%2520using%2520the%2520pre-trained%2520large%250Amultimodal%2520models%2520based%2520on%2520GroundingDINO%2520and%2520SAM.%2520This%2520enables%2520a%2520fully%250Aautomated%2520and%2520training-free%2520registration%2520algorithm%252C%2520potentially%2520generalisable%250Ato%2520a%2520wide%2520range%2520of%2520image%2520registration%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520present%250Aexperimental%2520results%2520using%2520one%2520of%2520the%2520challenging%2520tasks%252C%2520registering%250Ainter-subject%2520prostate%2520MR%2520images%252C%2520which%2520involves%2520both%2520highly%2520variable%2520intensity%250Aand%2520morphology%2520between%2520patients.%2520Tell2Reg%2520is%2520training-free%252C%2520eliminating%2520the%250Aneed%2520for%2520costly%2520and%2520time-consuming%2520data%2520curation%2520and%2520labelling%2520that%2520was%250Apreviously%2520required%2520for%2520this%2520registration%2520task.%2520This%2520approach%2520outperforms%250Aunsupervised%2520learning-based%2520registration%2520methods%2520tested%252C%2520and%2520has%2520a%2520performance%250Acomparable%2520to%2520weakly-supervised%2520methods.%2520Additional%2520qualitative%2520results%2520are%250Aalso%2520presented%2520to%2520suggest%2520that%252C%2520for%2520the%2520first%2520time%252C%2520there%2520is%2520a%2520potential%250Acorrelation%2520between%2520language%2520semantics%2520and%2520spatial%2520correspondence%252C%2520including%250Athe%2520spatial%2520invariance%2520in%2520language-prompted%2520regions%2520and%2520the%2520difference%2520in%250Alanguage%2520prompts%2520between%2520the%2520obtained%2520local%2520and%2520global%2520correspondences.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/yanwenCi/Tell2Reg.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03118v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tell2Reg%3A%20Establishing%20spatial%20correspondence%20between%20images%20by%20the%20same%0A%20%20language%20prompts&entry.906535625=Wen%20Yan%20and%20Qianye%20Yang%20and%20Shiqi%20Huang%20and%20Yipei%20Wang%20and%20Shonit%20Punwani%20and%20Mark%20Emberton%20and%20Vasilis%20Stavrinides%20and%20Yipeng%20Hu%20and%20Dean%20Barratt&entry.1292438233=%20%20Spatial%20correspondence%20can%20be%20represented%20by%20pairs%20of%20segmented%20regions%2C%20such%0Athat%20the%20image%20registration%20networks%20aim%20to%20segment%20corresponding%20regions%0Arather%20than%20predicting%20displacement%20fields%20or%20transformation%20parameters.%20In%0Athis%20work%2C%20we%20show%20that%20such%20a%20corresponding%20region%20pair%20can%20be%20predicted%20by%0Athe%20same%20language%20prompt%20on%20two%20different%20images%20using%20the%20pre-trained%20large%0Amultimodal%20models%20based%20on%20GroundingDINO%20and%20SAM.%20This%20enables%20a%20fully%0Aautomated%20and%20training-free%20registration%20algorithm%2C%20potentially%20generalisable%0Ato%20a%20wide%20range%20of%20image%20registration%20tasks.%20In%20this%20paper%2C%20we%20present%0Aexperimental%20results%20using%20one%20of%20the%20challenging%20tasks%2C%20registering%0Ainter-subject%20prostate%20MR%20images%2C%20which%20involves%20both%20highly%20variable%20intensity%0Aand%20morphology%20between%20patients.%20Tell2Reg%20is%20training-free%2C%20eliminating%20the%0Aneed%20for%20costly%20and%20time-consuming%20data%20curation%20and%20labelling%20that%20was%0Apreviously%20required%20for%20this%20registration%20task.%20This%20approach%20outperforms%0Aunsupervised%20learning-based%20registration%20methods%20tested%2C%20and%20has%20a%20performance%0Acomparable%20to%20weakly-supervised%20methods.%20Additional%20qualitative%20results%20are%0Aalso%20presented%20to%20suggest%20that%2C%20for%20the%20first%20time%2C%20there%20is%20a%20potential%0Acorrelation%20between%20language%20semantics%20and%20spatial%20correspondence%2C%20including%0Athe%20spatial%20invariance%20in%20language-prompted%20regions%20and%20the%20difference%20in%0Alanguage%20prompts%20between%20the%20obtained%20local%20and%20global%20correspondences.%20Code%20is%0Aavailable%20at%20https%3A//github.com/yanwenCi/Tell2Reg.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03118v1&entry.124074799=Read"},
{"title": "Efficient Vision Language Model Fine-tuning for Text-based Person\n  Anomaly Search", "author": "Jiayi He and Shengeng Tang and Ao Liu and Lechao Cheng and Jingjing Wu and Yanyan Wei", "abstract": "  This paper presents the HFUT-LMC team's solution to the WWW 2025 challenge on\nText-based Person Anomaly Search (TPAS). The primary objective of this\nchallenge is to accurately identify pedestrians exhibiting either normal or\nabnormal behavior within a large library of pedestrian images. Unlike\ntraditional video analysis tasks, TPAS significantly emphasizes understanding\nand interpreting the subtle relationships between text descriptions and visual\ndata. The complexity of this task lies in the model's need to not only match\nindividuals to text descriptions in massive image datasets but also accurately\ndifferentiate between search results when faced with similar descriptions. To\novercome these challenges, we introduce the Similarity Coverage Analysis (SCA)\nstrategy to address the recognition difficulty caused by similar text\ndescriptions. This strategy effectively enhances the model's capacity to manage\nsubtle differences, thus improving both the accuracy and reliability of the\nsearch. Our proposed solution demonstrated excellent performance in this\nchallenge.\n", "link": "http://arxiv.org/abs/2502.03230v1", "date": "2025-02-05", "relevancy": 2.8314, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5782}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5782}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Vision%20Language%20Model%20Fine-tuning%20for%20Text-based%20Person%0A%20%20Anomaly%20Search&body=Title%3A%20Efficient%20Vision%20Language%20Model%20Fine-tuning%20for%20Text-based%20Person%0A%20%20Anomaly%20Search%0AAuthor%3A%20Jiayi%20He%20and%20Shengeng%20Tang%20and%20Ao%20Liu%20and%20Lechao%20Cheng%20and%20Jingjing%20Wu%20and%20Yanyan%20Wei%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20HFUT-LMC%20team%27s%20solution%20to%20the%20WWW%202025%20challenge%20on%0AText-based%20Person%20Anomaly%20Search%20%28TPAS%29.%20The%20primary%20objective%20of%20this%0Achallenge%20is%20to%20accurately%20identify%20pedestrians%20exhibiting%20either%20normal%20or%0Aabnormal%20behavior%20within%20a%20large%20library%20of%20pedestrian%20images.%20Unlike%0Atraditional%20video%20analysis%20tasks%2C%20TPAS%20significantly%20emphasizes%20understanding%0Aand%20interpreting%20the%20subtle%20relationships%20between%20text%20descriptions%20and%20visual%0Adata.%20The%20complexity%20of%20this%20task%20lies%20in%20the%20model%27s%20need%20to%20not%20only%20match%0Aindividuals%20to%20text%20descriptions%20in%20massive%20image%20datasets%20but%20also%20accurately%0Adifferentiate%20between%20search%20results%20when%20faced%20with%20similar%20descriptions.%20To%0Aovercome%20these%20challenges%2C%20we%20introduce%20the%20Similarity%20Coverage%20Analysis%20%28SCA%29%0Astrategy%20to%20address%20the%20recognition%20difficulty%20caused%20by%20similar%20text%0Adescriptions.%20This%20strategy%20effectively%20enhances%20the%20model%27s%20capacity%20to%20manage%0Asubtle%20differences%2C%20thus%20improving%20both%20the%20accuracy%20and%20reliability%20of%20the%0Asearch.%20Our%20proposed%20solution%20demonstrated%20excellent%20performance%20in%20this%0Achallenge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03230v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Vision%2520Language%2520Model%2520Fine-tuning%2520for%2520Text-based%2520Person%250A%2520%2520Anomaly%2520Search%26entry.906535625%3DJiayi%2520He%2520and%2520Shengeng%2520Tang%2520and%2520Ao%2520Liu%2520and%2520Lechao%2520Cheng%2520and%2520Jingjing%2520Wu%2520and%2520Yanyan%2520Wei%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520HFUT-LMC%2520team%2527s%2520solution%2520to%2520the%2520WWW%25202025%2520challenge%2520on%250AText-based%2520Person%2520Anomaly%2520Search%2520%2528TPAS%2529.%2520The%2520primary%2520objective%2520of%2520this%250Achallenge%2520is%2520to%2520accurately%2520identify%2520pedestrians%2520exhibiting%2520either%2520normal%2520or%250Aabnormal%2520behavior%2520within%2520a%2520large%2520library%2520of%2520pedestrian%2520images.%2520Unlike%250Atraditional%2520video%2520analysis%2520tasks%252C%2520TPAS%2520significantly%2520emphasizes%2520understanding%250Aand%2520interpreting%2520the%2520subtle%2520relationships%2520between%2520text%2520descriptions%2520and%2520visual%250Adata.%2520The%2520complexity%2520of%2520this%2520task%2520lies%2520in%2520the%2520model%2527s%2520need%2520to%2520not%2520only%2520match%250Aindividuals%2520to%2520text%2520descriptions%2520in%2520massive%2520image%2520datasets%2520but%2520also%2520accurately%250Adifferentiate%2520between%2520search%2520results%2520when%2520faced%2520with%2520similar%2520descriptions.%2520To%250Aovercome%2520these%2520challenges%252C%2520we%2520introduce%2520the%2520Similarity%2520Coverage%2520Analysis%2520%2528SCA%2529%250Astrategy%2520to%2520address%2520the%2520recognition%2520difficulty%2520caused%2520by%2520similar%2520text%250Adescriptions.%2520This%2520strategy%2520effectively%2520enhances%2520the%2520model%2527s%2520capacity%2520to%2520manage%250Asubtle%2520differences%252C%2520thus%2520improving%2520both%2520the%2520accuracy%2520and%2520reliability%2520of%2520the%250Asearch.%2520Our%2520proposed%2520solution%2520demonstrated%2520excellent%2520performance%2520in%2520this%250Achallenge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03230v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Vision%20Language%20Model%20Fine-tuning%20for%20Text-based%20Person%0A%20%20Anomaly%20Search&entry.906535625=Jiayi%20He%20and%20Shengeng%20Tang%20and%20Ao%20Liu%20and%20Lechao%20Cheng%20and%20Jingjing%20Wu%20and%20Yanyan%20Wei&entry.1292438233=%20%20This%20paper%20presents%20the%20HFUT-LMC%20team%27s%20solution%20to%20the%20WWW%202025%20challenge%20on%0AText-based%20Person%20Anomaly%20Search%20%28TPAS%29.%20The%20primary%20objective%20of%20this%0Achallenge%20is%20to%20accurately%20identify%20pedestrians%20exhibiting%20either%20normal%20or%0Aabnormal%20behavior%20within%20a%20large%20library%20of%20pedestrian%20images.%20Unlike%0Atraditional%20video%20analysis%20tasks%2C%20TPAS%20significantly%20emphasizes%20understanding%0Aand%20interpreting%20the%20subtle%20relationships%20between%20text%20descriptions%20and%20visual%0Adata.%20The%20complexity%20of%20this%20task%20lies%20in%20the%20model%27s%20need%20to%20not%20only%20match%0Aindividuals%20to%20text%20descriptions%20in%20massive%20image%20datasets%20but%20also%20accurately%0Adifferentiate%20between%20search%20results%20when%20faced%20with%20similar%20descriptions.%20To%0Aovercome%20these%20challenges%2C%20we%20introduce%20the%20Similarity%20Coverage%20Analysis%20%28SCA%29%0Astrategy%20to%20address%20the%20recognition%20difficulty%20caused%20by%20similar%20text%0Adescriptions.%20This%20strategy%20effectively%20enhances%20the%20model%27s%20capacity%20to%20manage%0Asubtle%20differences%2C%20thus%20improving%20both%20the%20accuracy%20and%20reliability%20of%20the%0Asearch.%20Our%20proposed%20solution%20demonstrated%20excellent%20performance%20in%20this%0Achallenge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03230v1&entry.124074799=Read"},
{"title": "LMOD: A Large Multimodal Ophthalmology Dataset and Benchmark for Large\n  Vision-Language Models", "author": "Zhenyue Qin and Yu Yin and Dylan Campbell and Xuansheng Wu and Ke Zou and Yih-Chung Tham and Ninghao Liu and Xiuzhen Zhang and Qingyu Chen", "abstract": "  The prevalence of vision-threatening eye diseases is a significant global\nburden, with many cases remaining undiagnosed or diagnosed too late for\neffective treatment. Large vision-language models (LVLMs) have the potential to\nassist in understanding anatomical information, diagnosing eye diseases, and\ndrafting interpretations and follow-up plans, thereby reducing the burden on\nclinicians and improving access to eye care. However, limited benchmarks are\navailable to assess LVLMs' performance in ophthalmology-specific applications.\nIn this study, we introduce LMOD, a large-scale multimodal ophthalmology\nbenchmark consisting of 21,993 instances across (1) five ophthalmic imaging\nmodalities: optical coherence tomography, color fundus photographs, scanning\nlaser ophthalmoscopy, lens photographs, and surgical scenes; (2) free-text,\ndemographic, and disease biomarker information; and (3) primary\nophthalmology-specific applications such as anatomical information\nunderstanding, disease diagnosis, and subgroup analysis. In addition, we\nbenchmarked 13 state-of-the-art LVLM representatives from closed-source,\nopen-source, and medical domains. The results demonstrate a significant\nperformance drop for LVLMs in ophthalmology compared to other domains.\nSystematic error analysis further identified six major failure modes:\nmisclassification, failure to abstain, inconsistent reasoning, hallucination,\nassertions without justification, and lack of domain-specific knowledge. In\ncontrast, supervised neural networks specifically trained on these tasks as\nbaselines demonstrated high accuracy. These findings underscore the pressing\nneed for benchmarks in the development and validation of ophthalmology-specific\nLVLMs.\n", "link": "http://arxiv.org/abs/2410.01620v5", "date": "2025-02-05", "relevancy": 2.8126, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5765}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5765}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5346}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LMOD%3A%20A%20Large%20Multimodal%20Ophthalmology%20Dataset%20and%20Benchmark%20for%20Large%0A%20%20Vision-Language%20Models&body=Title%3A%20LMOD%3A%20A%20Large%20Multimodal%20Ophthalmology%20Dataset%20and%20Benchmark%20for%20Large%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Zhenyue%20Qin%20and%20Yu%20Yin%20and%20Dylan%20Campbell%20and%20Xuansheng%20Wu%20and%20Ke%20Zou%20and%20Yih-Chung%20Tham%20and%20Ninghao%20Liu%20and%20Xiuzhen%20Zhang%20and%20Qingyu%20Chen%0AAbstract%3A%20%20%20The%20prevalence%20of%20vision-threatening%20eye%20diseases%20is%20a%20significant%20global%0Aburden%2C%20with%20many%20cases%20remaining%20undiagnosed%20or%20diagnosed%20too%20late%20for%0Aeffective%20treatment.%20Large%20vision-language%20models%20%28LVLMs%29%20have%20the%20potential%20to%0Aassist%20in%20understanding%20anatomical%20information%2C%20diagnosing%20eye%20diseases%2C%20and%0Adrafting%20interpretations%20and%20follow-up%20plans%2C%20thereby%20reducing%20the%20burden%20on%0Aclinicians%20and%20improving%20access%20to%20eye%20care.%20However%2C%20limited%20benchmarks%20are%0Aavailable%20to%20assess%20LVLMs%27%20performance%20in%20ophthalmology-specific%20applications.%0AIn%20this%20study%2C%20we%20introduce%20LMOD%2C%20a%20large-scale%20multimodal%20ophthalmology%0Abenchmark%20consisting%20of%2021%2C993%20instances%20across%20%281%29%20five%20ophthalmic%20imaging%0Amodalities%3A%20optical%20coherence%20tomography%2C%20color%20fundus%20photographs%2C%20scanning%0Alaser%20ophthalmoscopy%2C%20lens%20photographs%2C%20and%20surgical%20scenes%3B%20%282%29%20free-text%2C%0Ademographic%2C%20and%20disease%20biomarker%20information%3B%20and%20%283%29%20primary%0Aophthalmology-specific%20applications%20such%20as%20anatomical%20information%0Aunderstanding%2C%20disease%20diagnosis%2C%20and%20subgroup%20analysis.%20In%20addition%2C%20we%0Abenchmarked%2013%20state-of-the-art%20LVLM%20representatives%20from%20closed-source%2C%0Aopen-source%2C%20and%20medical%20domains.%20The%20results%20demonstrate%20a%20significant%0Aperformance%20drop%20for%20LVLMs%20in%20ophthalmology%20compared%20to%20other%20domains.%0ASystematic%20error%20analysis%20further%20identified%20six%20major%20failure%20modes%3A%0Amisclassification%2C%20failure%20to%20abstain%2C%20inconsistent%20reasoning%2C%20hallucination%2C%0Aassertions%20without%20justification%2C%20and%20lack%20of%20domain-specific%20knowledge.%20In%0Acontrast%2C%20supervised%20neural%20networks%20specifically%20trained%20on%20these%20tasks%20as%0Abaselines%20demonstrated%20high%20accuracy.%20These%20findings%20underscore%20the%20pressing%0Aneed%20for%20benchmarks%20in%20the%20development%20and%20validation%20of%20ophthalmology-specific%0ALVLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01620v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLMOD%253A%2520A%2520Large%2520Multimodal%2520Ophthalmology%2520Dataset%2520and%2520Benchmark%2520for%2520Large%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DZhenyue%2520Qin%2520and%2520Yu%2520Yin%2520and%2520Dylan%2520Campbell%2520and%2520Xuansheng%2520Wu%2520and%2520Ke%2520Zou%2520and%2520Yih-Chung%2520Tham%2520and%2520Ninghao%2520Liu%2520and%2520Xiuzhen%2520Zhang%2520and%2520Qingyu%2520Chen%26entry.1292438233%3D%2520%2520The%2520prevalence%2520of%2520vision-threatening%2520eye%2520diseases%2520is%2520a%2520significant%2520global%250Aburden%252C%2520with%2520many%2520cases%2520remaining%2520undiagnosed%2520or%2520diagnosed%2520too%2520late%2520for%250Aeffective%2520treatment.%2520Large%2520vision-language%2520models%2520%2528LVLMs%2529%2520have%2520the%2520potential%2520to%250Aassist%2520in%2520understanding%2520anatomical%2520information%252C%2520diagnosing%2520eye%2520diseases%252C%2520and%250Adrafting%2520interpretations%2520and%2520follow-up%2520plans%252C%2520thereby%2520reducing%2520the%2520burden%2520on%250Aclinicians%2520and%2520improving%2520access%2520to%2520eye%2520care.%2520However%252C%2520limited%2520benchmarks%2520are%250Aavailable%2520to%2520assess%2520LVLMs%2527%2520performance%2520in%2520ophthalmology-specific%2520applications.%250AIn%2520this%2520study%252C%2520we%2520introduce%2520LMOD%252C%2520a%2520large-scale%2520multimodal%2520ophthalmology%250Abenchmark%2520consisting%2520of%252021%252C993%2520instances%2520across%2520%25281%2529%2520five%2520ophthalmic%2520imaging%250Amodalities%253A%2520optical%2520coherence%2520tomography%252C%2520color%2520fundus%2520photographs%252C%2520scanning%250Alaser%2520ophthalmoscopy%252C%2520lens%2520photographs%252C%2520and%2520surgical%2520scenes%253B%2520%25282%2529%2520free-text%252C%250Ademographic%252C%2520and%2520disease%2520biomarker%2520information%253B%2520and%2520%25283%2529%2520primary%250Aophthalmology-specific%2520applications%2520such%2520as%2520anatomical%2520information%250Aunderstanding%252C%2520disease%2520diagnosis%252C%2520and%2520subgroup%2520analysis.%2520In%2520addition%252C%2520we%250Abenchmarked%252013%2520state-of-the-art%2520LVLM%2520representatives%2520from%2520closed-source%252C%250Aopen-source%252C%2520and%2520medical%2520domains.%2520The%2520results%2520demonstrate%2520a%2520significant%250Aperformance%2520drop%2520for%2520LVLMs%2520in%2520ophthalmology%2520compared%2520to%2520other%2520domains.%250ASystematic%2520error%2520analysis%2520further%2520identified%2520six%2520major%2520failure%2520modes%253A%250Amisclassification%252C%2520failure%2520to%2520abstain%252C%2520inconsistent%2520reasoning%252C%2520hallucination%252C%250Aassertions%2520without%2520justification%252C%2520and%2520lack%2520of%2520domain-specific%2520knowledge.%2520In%250Acontrast%252C%2520supervised%2520neural%2520networks%2520specifically%2520trained%2520on%2520these%2520tasks%2520as%250Abaselines%2520demonstrated%2520high%2520accuracy.%2520These%2520findings%2520underscore%2520the%2520pressing%250Aneed%2520for%2520benchmarks%2520in%2520the%2520development%2520and%2520validation%2520of%2520ophthalmology-specific%250ALVLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01620v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LMOD%3A%20A%20Large%20Multimodal%20Ophthalmology%20Dataset%20and%20Benchmark%20for%20Large%0A%20%20Vision-Language%20Models&entry.906535625=Zhenyue%20Qin%20and%20Yu%20Yin%20and%20Dylan%20Campbell%20and%20Xuansheng%20Wu%20and%20Ke%20Zou%20and%20Yih-Chung%20Tham%20and%20Ninghao%20Liu%20and%20Xiuzhen%20Zhang%20and%20Qingyu%20Chen&entry.1292438233=%20%20The%20prevalence%20of%20vision-threatening%20eye%20diseases%20is%20a%20significant%20global%0Aburden%2C%20with%20many%20cases%20remaining%20undiagnosed%20or%20diagnosed%20too%20late%20for%0Aeffective%20treatment.%20Large%20vision-language%20models%20%28LVLMs%29%20have%20the%20potential%20to%0Aassist%20in%20understanding%20anatomical%20information%2C%20diagnosing%20eye%20diseases%2C%20and%0Adrafting%20interpretations%20and%20follow-up%20plans%2C%20thereby%20reducing%20the%20burden%20on%0Aclinicians%20and%20improving%20access%20to%20eye%20care.%20However%2C%20limited%20benchmarks%20are%0Aavailable%20to%20assess%20LVLMs%27%20performance%20in%20ophthalmology-specific%20applications.%0AIn%20this%20study%2C%20we%20introduce%20LMOD%2C%20a%20large-scale%20multimodal%20ophthalmology%0Abenchmark%20consisting%20of%2021%2C993%20instances%20across%20%281%29%20five%20ophthalmic%20imaging%0Amodalities%3A%20optical%20coherence%20tomography%2C%20color%20fundus%20photographs%2C%20scanning%0Alaser%20ophthalmoscopy%2C%20lens%20photographs%2C%20and%20surgical%20scenes%3B%20%282%29%20free-text%2C%0Ademographic%2C%20and%20disease%20biomarker%20information%3B%20and%20%283%29%20primary%0Aophthalmology-specific%20applications%20such%20as%20anatomical%20information%0Aunderstanding%2C%20disease%20diagnosis%2C%20and%20subgroup%20analysis.%20In%20addition%2C%20we%0Abenchmarked%2013%20state-of-the-art%20LVLM%20representatives%20from%20closed-source%2C%0Aopen-source%2C%20and%20medical%20domains.%20The%20results%20demonstrate%20a%20significant%0Aperformance%20drop%20for%20LVLMs%20in%20ophthalmology%20compared%20to%20other%20domains.%0ASystematic%20error%20analysis%20further%20identified%20six%20major%20failure%20modes%3A%0Amisclassification%2C%20failure%20to%20abstain%2C%20inconsistent%20reasoning%2C%20hallucination%2C%0Aassertions%20without%20justification%2C%20and%20lack%20of%20domain-specific%20knowledge.%20In%0Acontrast%2C%20supervised%20neural%20networks%20specifically%20trained%20on%20these%20tasks%20as%0Abaselines%20demonstrated%20high%20accuracy.%20These%20findings%20underscore%20the%20pressing%0Aneed%20for%20benchmarks%20in%20the%20development%20and%20validation%20of%20ophthalmology-specific%0ALVLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01620v5&entry.124074799=Read"},
{"title": "GS-CPR: Efficient Camera Pose Refinement via 3D Gaussian Splatting", "author": "Changkun Liu and Shuai Chen and Yash Bhalgat and Siyan Hu and Ming Cheng and Zirui Wang and Victor Adrian Prisacariu and Tristan Braud", "abstract": "  We leverage 3D Gaussian Splatting (3DGS) as a scene representation and\npropose a novel test-time camera pose refinement (CPR) framework, GS-CPR. This\nframework enhances the localization accuracy of state-of-the-art absolute pose\nregression and scene coordinate regression methods. The 3DGS model renders\nhigh-quality synthetic images and depth maps to facilitate the establishment of\n2D-3D correspondences. GS-CPR obviates the need for training feature extractors\nor descriptors by operating directly on RGB images, utilizing the 3D foundation\nmodel, MASt3R, for precise 2D matching. To improve the robustness of our model\nin challenging outdoor environments, we incorporate an exposure-adaptive module\nwithin the 3DGS framework. Consequently, GS-CPR enables efficient one-shot pose\nrefinement given a single RGB query and a coarse initial pose estimation. Our\nproposed approach surpasses leading NeRF-based optimization methods in both\naccuracy and runtime across indoor and outdoor visual localization benchmarks,\nachieving new state-of-the-art accuracy on two indoor datasets. The project\npage is available at https://gsloc.active.vision.\n", "link": "http://arxiv.org/abs/2408.11085v3", "date": "2025-02-05", "relevancy": 2.8099, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7328}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7017}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GS-CPR%3A%20Efficient%20Camera%20Pose%20Refinement%20via%203D%20Gaussian%20Splatting&body=Title%3A%20GS-CPR%3A%20Efficient%20Camera%20Pose%20Refinement%20via%203D%20Gaussian%20Splatting%0AAuthor%3A%20Changkun%20Liu%20and%20Shuai%20Chen%20and%20Yash%20Bhalgat%20and%20Siyan%20Hu%20and%20Ming%20Cheng%20and%20Zirui%20Wang%20and%20Victor%20Adrian%20Prisacariu%20and%20Tristan%20Braud%0AAbstract%3A%20%20%20We%20leverage%203D%20Gaussian%20Splatting%20%283DGS%29%20as%20a%20scene%20representation%20and%0Apropose%20a%20novel%20test-time%20camera%20pose%20refinement%20%28CPR%29%20framework%2C%20GS-CPR.%20This%0Aframework%20enhances%20the%20localization%20accuracy%20of%20state-of-the-art%20absolute%20pose%0Aregression%20and%20scene%20coordinate%20regression%20methods.%20The%203DGS%20model%20renders%0Ahigh-quality%20synthetic%20images%20and%20depth%20maps%20to%20facilitate%20the%20establishment%20of%0A2D-3D%20correspondences.%20GS-CPR%20obviates%20the%20need%20for%20training%20feature%20extractors%0Aor%20descriptors%20by%20operating%20directly%20on%20RGB%20images%2C%20utilizing%20the%203D%20foundation%0Amodel%2C%20MASt3R%2C%20for%20precise%202D%20matching.%20To%20improve%20the%20robustness%20of%20our%20model%0Ain%20challenging%20outdoor%20environments%2C%20we%20incorporate%20an%20exposure-adaptive%20module%0Awithin%20the%203DGS%20framework.%20Consequently%2C%20GS-CPR%20enables%20efficient%20one-shot%20pose%0Arefinement%20given%20a%20single%20RGB%20query%20and%20a%20coarse%20initial%20pose%20estimation.%20Our%0Aproposed%20approach%20surpasses%20leading%20NeRF-based%20optimization%20methods%20in%20both%0Aaccuracy%20and%20runtime%20across%20indoor%20and%20outdoor%20visual%20localization%20benchmarks%2C%0Aachieving%20new%20state-of-the-art%20accuracy%20on%20two%20indoor%20datasets.%20The%20project%0Apage%20is%20available%20at%20https%3A//gsloc.active.vision.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.11085v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGS-CPR%253A%2520Efficient%2520Camera%2520Pose%2520Refinement%2520via%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DChangkun%2520Liu%2520and%2520Shuai%2520Chen%2520and%2520Yash%2520Bhalgat%2520and%2520Siyan%2520Hu%2520and%2520Ming%2520Cheng%2520and%2520Zirui%2520Wang%2520and%2520Victor%2520Adrian%2520Prisacariu%2520and%2520Tristan%2520Braud%26entry.1292438233%3D%2520%2520We%2520leverage%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520as%2520a%2520scene%2520representation%2520and%250Apropose%2520a%2520novel%2520test-time%2520camera%2520pose%2520refinement%2520%2528CPR%2529%2520framework%252C%2520GS-CPR.%2520This%250Aframework%2520enhances%2520the%2520localization%2520accuracy%2520of%2520state-of-the-art%2520absolute%2520pose%250Aregression%2520and%2520scene%2520coordinate%2520regression%2520methods.%2520The%25203DGS%2520model%2520renders%250Ahigh-quality%2520synthetic%2520images%2520and%2520depth%2520maps%2520to%2520facilitate%2520the%2520establishment%2520of%250A2D-3D%2520correspondences.%2520GS-CPR%2520obviates%2520the%2520need%2520for%2520training%2520feature%2520extractors%250Aor%2520descriptors%2520by%2520operating%2520directly%2520on%2520RGB%2520images%252C%2520utilizing%2520the%25203D%2520foundation%250Amodel%252C%2520MASt3R%252C%2520for%2520precise%25202D%2520matching.%2520To%2520improve%2520the%2520robustness%2520of%2520our%2520model%250Ain%2520challenging%2520outdoor%2520environments%252C%2520we%2520incorporate%2520an%2520exposure-adaptive%2520module%250Awithin%2520the%25203DGS%2520framework.%2520Consequently%252C%2520GS-CPR%2520enables%2520efficient%2520one-shot%2520pose%250Arefinement%2520given%2520a%2520single%2520RGB%2520query%2520and%2520a%2520coarse%2520initial%2520pose%2520estimation.%2520Our%250Aproposed%2520approach%2520surpasses%2520leading%2520NeRF-based%2520optimization%2520methods%2520in%2520both%250Aaccuracy%2520and%2520runtime%2520across%2520indoor%2520and%2520outdoor%2520visual%2520localization%2520benchmarks%252C%250Aachieving%2520new%2520state-of-the-art%2520accuracy%2520on%2520two%2520indoor%2520datasets.%2520The%2520project%250Apage%2520is%2520available%2520at%2520https%253A//gsloc.active.vision.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.11085v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GS-CPR%3A%20Efficient%20Camera%20Pose%20Refinement%20via%203D%20Gaussian%20Splatting&entry.906535625=Changkun%20Liu%20and%20Shuai%20Chen%20and%20Yash%20Bhalgat%20and%20Siyan%20Hu%20and%20Ming%20Cheng%20and%20Zirui%20Wang%20and%20Victor%20Adrian%20Prisacariu%20and%20Tristan%20Braud&entry.1292438233=%20%20We%20leverage%203D%20Gaussian%20Splatting%20%283DGS%29%20as%20a%20scene%20representation%20and%0Apropose%20a%20novel%20test-time%20camera%20pose%20refinement%20%28CPR%29%20framework%2C%20GS-CPR.%20This%0Aframework%20enhances%20the%20localization%20accuracy%20of%20state-of-the-art%20absolute%20pose%0Aregression%20and%20scene%20coordinate%20regression%20methods.%20The%203DGS%20model%20renders%0Ahigh-quality%20synthetic%20images%20and%20depth%20maps%20to%20facilitate%20the%20establishment%20of%0A2D-3D%20correspondences.%20GS-CPR%20obviates%20the%20need%20for%20training%20feature%20extractors%0Aor%20descriptors%20by%20operating%20directly%20on%20RGB%20images%2C%20utilizing%20the%203D%20foundation%0Amodel%2C%20MASt3R%2C%20for%20precise%202D%20matching.%20To%20improve%20the%20robustness%20of%20our%20model%0Ain%20challenging%20outdoor%20environments%2C%20we%20incorporate%20an%20exposure-adaptive%20module%0Awithin%20the%203DGS%20framework.%20Consequently%2C%20GS-CPR%20enables%20efficient%20one-shot%20pose%0Arefinement%20given%20a%20single%20RGB%20query%20and%20a%20coarse%20initial%20pose%20estimation.%20Our%0Aproposed%20approach%20surpasses%20leading%20NeRF-based%20optimization%20methods%20in%20both%0Aaccuracy%20and%20runtime%20across%20indoor%20and%20outdoor%20visual%20localization%20benchmarks%2C%0Aachieving%20new%20state-of-the-art%20accuracy%20on%20two%20indoor%20datasets.%20The%20project%0Apage%20is%20available%20at%20https%3A//gsloc.active.vision.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.11085v3&entry.124074799=Read"},
{"title": "GazeHTA: End-to-end Gaze Target Detection with Head-Target Association", "author": "Zhi-Yi Lin and Jouh Yeong Chew and Jan van Gemert and Xucong Zhang", "abstract": "  Precisely detecting which object a person is paying attention to is critical\nfor human-robot interaction since it provides important cues for the next\naction from the human user. We propose an end-to-end approach for gaze target\ndetection: predicting a head-target connection between individuals and the\ntarget image regions they are looking at. Most of the existing methods use\nindependent components such as off-the-shelf head detectors or have problems in\nestablishing associations between heads and gaze targets. In contrast, we\ninvestigate an end-to-end multi-person Gaze target detection framework with\nHeads and Targets Association (GazeHTA), which predicts multiple head-target\ninstances based solely on input scene image. GazeHTA addresses challenges in\ngaze target detection by (1) leveraging a pre-trained diffusion model to\nextract scene features for rich semantic understanding, (2) re-injecting a head\nfeature to enhance the head priors for improved head understanding, and (3)\nlearning a connection map as the explicit visual associations between heads and\ngaze targets. Our extensive experimental results demonstrate that GazeHTA\noutperforms state-of-the-art gaze target detection methods and two adapted\ndiffusion-based baselines on two standard datasets.\n", "link": "http://arxiv.org/abs/2404.10718v3", "date": "2025-02-05", "relevancy": 2.807, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5696}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5616}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GazeHTA%3A%20End-to-end%20Gaze%20Target%20Detection%20with%20Head-Target%20Association&body=Title%3A%20GazeHTA%3A%20End-to-end%20Gaze%20Target%20Detection%20with%20Head-Target%20Association%0AAuthor%3A%20Zhi-Yi%20Lin%20and%20Jouh%20Yeong%20Chew%20and%20Jan%20van%20Gemert%20and%20Xucong%20Zhang%0AAbstract%3A%20%20%20Precisely%20detecting%20which%20object%20a%20person%20is%20paying%20attention%20to%20is%20critical%0Afor%20human-robot%20interaction%20since%20it%20provides%20important%20cues%20for%20the%20next%0Aaction%20from%20the%20human%20user.%20We%20propose%20an%20end-to-end%20approach%20for%20gaze%20target%0Adetection%3A%20predicting%20a%20head-target%20connection%20between%20individuals%20and%20the%0Atarget%20image%20regions%20they%20are%20looking%20at.%20Most%20of%20the%20existing%20methods%20use%0Aindependent%20components%20such%20as%20off-the-shelf%20head%20detectors%20or%20have%20problems%20in%0Aestablishing%20associations%20between%20heads%20and%20gaze%20targets.%20In%20contrast%2C%20we%0Ainvestigate%20an%20end-to-end%20multi-person%20Gaze%20target%20detection%20framework%20with%0AHeads%20and%20Targets%20Association%20%28GazeHTA%29%2C%20which%20predicts%20multiple%20head-target%0Ainstances%20based%20solely%20on%20input%20scene%20image.%20GazeHTA%20addresses%20challenges%20in%0Agaze%20target%20detection%20by%20%281%29%20leveraging%20a%20pre-trained%20diffusion%20model%20to%0Aextract%20scene%20features%20for%20rich%20semantic%20understanding%2C%20%282%29%20re-injecting%20a%20head%0Afeature%20to%20enhance%20the%20head%20priors%20for%20improved%20head%20understanding%2C%20and%20%283%29%0Alearning%20a%20connection%20map%20as%20the%20explicit%20visual%20associations%20between%20heads%20and%0Agaze%20targets.%20Our%20extensive%20experimental%20results%20demonstrate%20that%20GazeHTA%0Aoutperforms%20state-of-the-art%20gaze%20target%20detection%20methods%20and%20two%20adapted%0Adiffusion-based%20baselines%20on%20two%20standard%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10718v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGazeHTA%253A%2520End-to-end%2520Gaze%2520Target%2520Detection%2520with%2520Head-Target%2520Association%26entry.906535625%3DZhi-Yi%2520Lin%2520and%2520Jouh%2520Yeong%2520Chew%2520and%2520Jan%2520van%2520Gemert%2520and%2520Xucong%2520Zhang%26entry.1292438233%3D%2520%2520Precisely%2520detecting%2520which%2520object%2520a%2520person%2520is%2520paying%2520attention%2520to%2520is%2520critical%250Afor%2520human-robot%2520interaction%2520since%2520it%2520provides%2520important%2520cues%2520for%2520the%2520next%250Aaction%2520from%2520the%2520human%2520user.%2520We%2520propose%2520an%2520end-to-end%2520approach%2520for%2520gaze%2520target%250Adetection%253A%2520predicting%2520a%2520head-target%2520connection%2520between%2520individuals%2520and%2520the%250Atarget%2520image%2520regions%2520they%2520are%2520looking%2520at.%2520Most%2520of%2520the%2520existing%2520methods%2520use%250Aindependent%2520components%2520such%2520as%2520off-the-shelf%2520head%2520detectors%2520or%2520have%2520problems%2520in%250Aestablishing%2520associations%2520between%2520heads%2520and%2520gaze%2520targets.%2520In%2520contrast%252C%2520we%250Ainvestigate%2520an%2520end-to-end%2520multi-person%2520Gaze%2520target%2520detection%2520framework%2520with%250AHeads%2520and%2520Targets%2520Association%2520%2528GazeHTA%2529%252C%2520which%2520predicts%2520multiple%2520head-target%250Ainstances%2520based%2520solely%2520on%2520input%2520scene%2520image.%2520GazeHTA%2520addresses%2520challenges%2520in%250Agaze%2520target%2520detection%2520by%2520%25281%2529%2520leveraging%2520a%2520pre-trained%2520diffusion%2520model%2520to%250Aextract%2520scene%2520features%2520for%2520rich%2520semantic%2520understanding%252C%2520%25282%2529%2520re-injecting%2520a%2520head%250Afeature%2520to%2520enhance%2520the%2520head%2520priors%2520for%2520improved%2520head%2520understanding%252C%2520and%2520%25283%2529%250Alearning%2520a%2520connection%2520map%2520as%2520the%2520explicit%2520visual%2520associations%2520between%2520heads%2520and%250Agaze%2520targets.%2520Our%2520extensive%2520experimental%2520results%2520demonstrate%2520that%2520GazeHTA%250Aoutperforms%2520state-of-the-art%2520gaze%2520target%2520detection%2520methods%2520and%2520two%2520adapted%250Adiffusion-based%2520baselines%2520on%2520two%2520standard%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.10718v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GazeHTA%3A%20End-to-end%20Gaze%20Target%20Detection%20with%20Head-Target%20Association&entry.906535625=Zhi-Yi%20Lin%20and%20Jouh%20Yeong%20Chew%20and%20Jan%20van%20Gemert%20and%20Xucong%20Zhang&entry.1292438233=%20%20Precisely%20detecting%20which%20object%20a%20person%20is%20paying%20attention%20to%20is%20critical%0Afor%20human-robot%20interaction%20since%20it%20provides%20important%20cues%20for%20the%20next%0Aaction%20from%20the%20human%20user.%20We%20propose%20an%20end-to-end%20approach%20for%20gaze%20target%0Adetection%3A%20predicting%20a%20head-target%20connection%20between%20individuals%20and%20the%0Atarget%20image%20regions%20they%20are%20looking%20at.%20Most%20of%20the%20existing%20methods%20use%0Aindependent%20components%20such%20as%20off-the-shelf%20head%20detectors%20or%20have%20problems%20in%0Aestablishing%20associations%20between%20heads%20and%20gaze%20targets.%20In%20contrast%2C%20we%0Ainvestigate%20an%20end-to-end%20multi-person%20Gaze%20target%20detection%20framework%20with%0AHeads%20and%20Targets%20Association%20%28GazeHTA%29%2C%20which%20predicts%20multiple%20head-target%0Ainstances%20based%20solely%20on%20input%20scene%20image.%20GazeHTA%20addresses%20challenges%20in%0Agaze%20target%20detection%20by%20%281%29%20leveraging%20a%20pre-trained%20diffusion%20model%20to%0Aextract%20scene%20features%20for%20rich%20semantic%20understanding%2C%20%282%29%20re-injecting%20a%20head%0Afeature%20to%20enhance%20the%20head%20priors%20for%20improved%20head%20understanding%2C%20and%20%283%29%0Alearning%20a%20connection%20map%20as%20the%20explicit%20visual%20associations%20between%20heads%20and%0Agaze%20targets.%20Our%20extensive%20experimental%20results%20demonstrate%20that%20GazeHTA%0Aoutperforms%20state-of-the-art%20gaze%20target%20detection%20methods%20and%20two%20adapted%0Adiffusion-based%20baselines%20on%20two%20standard%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10718v3&entry.124074799=Read"},
{"title": "Token Assorted: Mixing Latent and Text Tokens for Improved Language\n  Model Reasoning", "author": "DiJia Su and Hanlin Zhu and Yingchen Xu and Jiantao Jiao and Yuandong Tian and Qinqing Zheng", "abstract": "  Large Language Models (LLMs) excel at reasoning and planning when trained on\nchainof-thought (CoT) data, where the step-by-step thought process is\nexplicitly outlined by text tokens. However, this results in lengthy inputs\nwhere many words support textual coherence rather than core reasoning\ninformation, and processing these inputs consumes substantial computation\nresources. In this work, we propose a hybrid representation of the reasoning\nprocess, where we partially abstract away the initial reasoning steps using\nlatent discrete tokens generated by VQ-VAE, significantly reducing the length\nof reasoning traces. We explore the use of latent trace abstractions in two\nscenarios: 1) training the model from scratch for the Keys-Finding Maze\nproblem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary\nincluding unseen latent tokens, for both logical and mathematical reasoning\nproblems. To facilitate effective learning, we introduce a simple training\nprocedure that randomly mixes latent and text tokens, which enables fast\nadaptation to new latent tokens. Our approach consistently outperforms the\nbaselines methods in various benchmarks.\n", "link": "http://arxiv.org/abs/2502.03275v1", "date": "2025-02-05", "relevancy": 2.7871, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5748}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5487}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Token%20Assorted%3A%20Mixing%20Latent%20and%20Text%20Tokens%20for%20Improved%20Language%0A%20%20Model%20Reasoning&body=Title%3A%20Token%20Assorted%3A%20Mixing%20Latent%20and%20Text%20Tokens%20for%20Improved%20Language%0A%20%20Model%20Reasoning%0AAuthor%3A%20DiJia%20Su%20and%20Hanlin%20Zhu%20and%20Yingchen%20Xu%20and%20Jiantao%20Jiao%20and%20Yuandong%20Tian%20and%20Qinqing%20Zheng%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20excel%20at%20reasoning%20and%20planning%20when%20trained%20on%0Achainof-thought%20%28CoT%29%20data%2C%20where%20the%20step-by-step%20thought%20process%20is%0Aexplicitly%20outlined%20by%20text%20tokens.%20However%2C%20this%20results%20in%20lengthy%20inputs%0Awhere%20many%20words%20support%20textual%20coherence%20rather%20than%20core%20reasoning%0Ainformation%2C%20and%20processing%20these%20inputs%20consumes%20substantial%20computation%0Aresources.%20In%20this%20work%2C%20we%20propose%20a%20hybrid%20representation%20of%20the%20reasoning%0Aprocess%2C%20where%20we%20partially%20abstract%20away%20the%20initial%20reasoning%20steps%20using%0Alatent%20discrete%20tokens%20generated%20by%20VQ-VAE%2C%20significantly%20reducing%20the%20length%0Aof%20reasoning%20traces.%20We%20explore%20the%20use%20of%20latent%20trace%20abstractions%20in%20two%0Ascenarios%3A%201%29%20training%20the%20model%20from%20scratch%20for%20the%20Keys-Finding%20Maze%0Aproblem%2C%202%29%20fine-tuning%20LLMs%20on%20this%20hybrid%20data%20with%20an%20extended%20vocabulary%0Aincluding%20unseen%20latent%20tokens%2C%20for%20both%20logical%20and%20mathematical%20reasoning%0Aproblems.%20To%20facilitate%20effective%20learning%2C%20we%20introduce%20a%20simple%20training%0Aprocedure%20that%20randomly%20mixes%20latent%20and%20text%20tokens%2C%20which%20enables%20fast%0Aadaptation%20to%20new%20latent%20tokens.%20Our%20approach%20consistently%20outperforms%20the%0Abaselines%20methods%20in%20various%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03275v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToken%2520Assorted%253A%2520Mixing%2520Latent%2520and%2520Text%2520Tokens%2520for%2520Improved%2520Language%250A%2520%2520Model%2520Reasoning%26entry.906535625%3DDiJia%2520Su%2520and%2520Hanlin%2520Zhu%2520and%2520Yingchen%2520Xu%2520and%2520Jiantao%2520Jiao%2520and%2520Yuandong%2520Tian%2520and%2520Qinqing%2520Zheng%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520excel%2520at%2520reasoning%2520and%2520planning%2520when%2520trained%2520on%250Achainof-thought%2520%2528CoT%2529%2520data%252C%2520where%2520the%2520step-by-step%2520thought%2520process%2520is%250Aexplicitly%2520outlined%2520by%2520text%2520tokens.%2520However%252C%2520this%2520results%2520in%2520lengthy%2520inputs%250Awhere%2520many%2520words%2520support%2520textual%2520coherence%2520rather%2520than%2520core%2520reasoning%250Ainformation%252C%2520and%2520processing%2520these%2520inputs%2520consumes%2520substantial%2520computation%250Aresources.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520hybrid%2520representation%2520of%2520the%2520reasoning%250Aprocess%252C%2520where%2520we%2520partially%2520abstract%2520away%2520the%2520initial%2520reasoning%2520steps%2520using%250Alatent%2520discrete%2520tokens%2520generated%2520by%2520VQ-VAE%252C%2520significantly%2520reducing%2520the%2520length%250Aof%2520reasoning%2520traces.%2520We%2520explore%2520the%2520use%2520of%2520latent%2520trace%2520abstractions%2520in%2520two%250Ascenarios%253A%25201%2529%2520training%2520the%2520model%2520from%2520scratch%2520for%2520the%2520Keys-Finding%2520Maze%250Aproblem%252C%25202%2529%2520fine-tuning%2520LLMs%2520on%2520this%2520hybrid%2520data%2520with%2520an%2520extended%2520vocabulary%250Aincluding%2520unseen%2520latent%2520tokens%252C%2520for%2520both%2520logical%2520and%2520mathematical%2520reasoning%250Aproblems.%2520To%2520facilitate%2520effective%2520learning%252C%2520we%2520introduce%2520a%2520simple%2520training%250Aprocedure%2520that%2520randomly%2520mixes%2520latent%2520and%2520text%2520tokens%252C%2520which%2520enables%2520fast%250Aadaptation%2520to%2520new%2520latent%2520tokens.%2520Our%2520approach%2520consistently%2520outperforms%2520the%250Abaselines%2520methods%2520in%2520various%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03275v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Token%20Assorted%3A%20Mixing%20Latent%20and%20Text%20Tokens%20for%20Improved%20Language%0A%20%20Model%20Reasoning&entry.906535625=DiJia%20Su%20and%20Hanlin%20Zhu%20and%20Yingchen%20Xu%20and%20Jiantao%20Jiao%20and%20Yuandong%20Tian%20and%20Qinqing%20Zheng&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20excel%20at%20reasoning%20and%20planning%20when%20trained%20on%0Achainof-thought%20%28CoT%29%20data%2C%20where%20the%20step-by-step%20thought%20process%20is%0Aexplicitly%20outlined%20by%20text%20tokens.%20However%2C%20this%20results%20in%20lengthy%20inputs%0Awhere%20many%20words%20support%20textual%20coherence%20rather%20than%20core%20reasoning%0Ainformation%2C%20and%20processing%20these%20inputs%20consumes%20substantial%20computation%0Aresources.%20In%20this%20work%2C%20we%20propose%20a%20hybrid%20representation%20of%20the%20reasoning%0Aprocess%2C%20where%20we%20partially%20abstract%20away%20the%20initial%20reasoning%20steps%20using%0Alatent%20discrete%20tokens%20generated%20by%20VQ-VAE%2C%20significantly%20reducing%20the%20length%0Aof%20reasoning%20traces.%20We%20explore%20the%20use%20of%20latent%20trace%20abstractions%20in%20two%0Ascenarios%3A%201%29%20training%20the%20model%20from%20scratch%20for%20the%20Keys-Finding%20Maze%0Aproblem%2C%202%29%20fine-tuning%20LLMs%20on%20this%20hybrid%20data%20with%20an%20extended%20vocabulary%0Aincluding%20unseen%20latent%20tokens%2C%20for%20both%20logical%20and%20mathematical%20reasoning%0Aproblems.%20To%20facilitate%20effective%20learning%2C%20we%20introduce%20a%20simple%20training%0Aprocedure%20that%20randomly%20mixes%20latent%20and%20text%20tokens%2C%20which%20enables%20fast%0Aadaptation%20to%20new%20latent%20tokens.%20Our%20approach%20consistently%20outperforms%20the%0Abaselines%20methods%20in%20various%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03275v1&entry.124074799=Read"},
{"title": "MaxInfo: A Training-Free Key-Frame Selection Method Using Maximum Volume\n  for Enhanced Video Understanding", "author": "Pengyi Li and Irina Abdullaeva and Alexander Gambashidze and Andrey Kuznetsov and Ivan Oseledets", "abstract": "  Modern Video Large Language Models (VLLMs) often rely on uniform frame\nsampling for video understanding, but this approach frequently fails to capture\ncritical information due to frame redundancy and variations in video content.\nWe propose MaxInfo, a training-free method based on the maximum volume\nprinciple, which selects and retains the most representative frames from the\ninput video. By maximizing the geometric volume formed by selected embeddings,\nMaxInfo ensures that the chosen frames cover the most informative regions of\nthe embedding space, effectively reducing redundancy while preserving\ndiversity. This method enhances the quality of input representations and\nimproves long video comprehension performance across benchmarks. For instance,\nMaxInfo achieves a 3.28% improvement on LongVideoBench and a 6.4% improvement\non EgoSchema for LLaVA-Video-7B. It also achieves a 3.47% improvement for\nLLaVA-Video-72B. The approach is simple to implement and works with existing\nVLLMs without the need for additional training, making it a practical and\neffective alternative to traditional uniform sampling methods.\n", "link": "http://arxiv.org/abs/2502.03183v1", "date": "2025-02-05", "relevancy": 2.7612, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.556}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.556}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MaxInfo%3A%20A%20Training-Free%20Key-Frame%20Selection%20Method%20Using%20Maximum%20Volume%0A%20%20for%20Enhanced%20Video%20Understanding&body=Title%3A%20MaxInfo%3A%20A%20Training-Free%20Key-Frame%20Selection%20Method%20Using%20Maximum%20Volume%0A%20%20for%20Enhanced%20Video%20Understanding%0AAuthor%3A%20Pengyi%20Li%20and%20Irina%20Abdullaeva%20and%20Alexander%20Gambashidze%20and%20Andrey%20Kuznetsov%20and%20Ivan%20Oseledets%0AAbstract%3A%20%20%20Modern%20Video%20Large%20Language%20Models%20%28VLLMs%29%20often%20rely%20on%20uniform%20frame%0Asampling%20for%20video%20understanding%2C%20but%20this%20approach%20frequently%20fails%20to%20capture%0Acritical%20information%20due%20to%20frame%20redundancy%20and%20variations%20in%20video%20content.%0AWe%20propose%20MaxInfo%2C%20a%20training-free%20method%20based%20on%20the%20maximum%20volume%0Aprinciple%2C%20which%20selects%20and%20retains%20the%20most%20representative%20frames%20from%20the%0Ainput%20video.%20By%20maximizing%20the%20geometric%20volume%20formed%20by%20selected%20embeddings%2C%0AMaxInfo%20ensures%20that%20the%20chosen%20frames%20cover%20the%20most%20informative%20regions%20of%0Athe%20embedding%20space%2C%20effectively%20reducing%20redundancy%20while%20preserving%0Adiversity.%20This%20method%20enhances%20the%20quality%20of%20input%20representations%20and%0Aimproves%20long%20video%20comprehension%20performance%20across%20benchmarks.%20For%20instance%2C%0AMaxInfo%20achieves%20a%203.28%25%20improvement%20on%20LongVideoBench%20and%20a%206.4%25%20improvement%0Aon%20EgoSchema%20for%20LLaVA-Video-7B.%20It%20also%20achieves%20a%203.47%25%20improvement%20for%0ALLaVA-Video-72B.%20The%20approach%20is%20simple%20to%20implement%20and%20works%20with%20existing%0AVLLMs%20without%20the%20need%20for%20additional%20training%2C%20making%20it%20a%20practical%20and%0Aeffective%20alternative%20to%20traditional%20uniform%20sampling%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03183v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaxInfo%253A%2520A%2520Training-Free%2520Key-Frame%2520Selection%2520Method%2520Using%2520Maximum%2520Volume%250A%2520%2520for%2520Enhanced%2520Video%2520Understanding%26entry.906535625%3DPengyi%2520Li%2520and%2520Irina%2520Abdullaeva%2520and%2520Alexander%2520Gambashidze%2520and%2520Andrey%2520Kuznetsov%2520and%2520Ivan%2520Oseledets%26entry.1292438233%3D%2520%2520Modern%2520Video%2520Large%2520Language%2520Models%2520%2528VLLMs%2529%2520often%2520rely%2520on%2520uniform%2520frame%250Asampling%2520for%2520video%2520understanding%252C%2520but%2520this%2520approach%2520frequently%2520fails%2520to%2520capture%250Acritical%2520information%2520due%2520to%2520frame%2520redundancy%2520and%2520variations%2520in%2520video%2520content.%250AWe%2520propose%2520MaxInfo%252C%2520a%2520training-free%2520method%2520based%2520on%2520the%2520maximum%2520volume%250Aprinciple%252C%2520which%2520selects%2520and%2520retains%2520the%2520most%2520representative%2520frames%2520from%2520the%250Ainput%2520video.%2520By%2520maximizing%2520the%2520geometric%2520volume%2520formed%2520by%2520selected%2520embeddings%252C%250AMaxInfo%2520ensures%2520that%2520the%2520chosen%2520frames%2520cover%2520the%2520most%2520informative%2520regions%2520of%250Athe%2520embedding%2520space%252C%2520effectively%2520reducing%2520redundancy%2520while%2520preserving%250Adiversity.%2520This%2520method%2520enhances%2520the%2520quality%2520of%2520input%2520representations%2520and%250Aimproves%2520long%2520video%2520comprehension%2520performance%2520across%2520benchmarks.%2520For%2520instance%252C%250AMaxInfo%2520achieves%2520a%25203.28%2525%2520improvement%2520on%2520LongVideoBench%2520and%2520a%25206.4%2525%2520improvement%250Aon%2520EgoSchema%2520for%2520LLaVA-Video-7B.%2520It%2520also%2520achieves%2520a%25203.47%2525%2520improvement%2520for%250ALLaVA-Video-72B.%2520The%2520approach%2520is%2520simple%2520to%2520implement%2520and%2520works%2520with%2520existing%250AVLLMs%2520without%2520the%2520need%2520for%2520additional%2520training%252C%2520making%2520it%2520a%2520practical%2520and%250Aeffective%2520alternative%2520to%2520traditional%2520uniform%2520sampling%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03183v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaxInfo%3A%20A%20Training-Free%20Key-Frame%20Selection%20Method%20Using%20Maximum%20Volume%0A%20%20for%20Enhanced%20Video%20Understanding&entry.906535625=Pengyi%20Li%20and%20Irina%20Abdullaeva%20and%20Alexander%20Gambashidze%20and%20Andrey%20Kuznetsov%20and%20Ivan%20Oseledets&entry.1292438233=%20%20Modern%20Video%20Large%20Language%20Models%20%28VLLMs%29%20often%20rely%20on%20uniform%20frame%0Asampling%20for%20video%20understanding%2C%20but%20this%20approach%20frequently%20fails%20to%20capture%0Acritical%20information%20due%20to%20frame%20redundancy%20and%20variations%20in%20video%20content.%0AWe%20propose%20MaxInfo%2C%20a%20training-free%20method%20based%20on%20the%20maximum%20volume%0Aprinciple%2C%20which%20selects%20and%20retains%20the%20most%20representative%20frames%20from%20the%0Ainput%20video.%20By%20maximizing%20the%20geometric%20volume%20formed%20by%20selected%20embeddings%2C%0AMaxInfo%20ensures%20that%20the%20chosen%20frames%20cover%20the%20most%20informative%20regions%20of%0Athe%20embedding%20space%2C%20effectively%20reducing%20redundancy%20while%20preserving%0Adiversity.%20This%20method%20enhances%20the%20quality%20of%20input%20representations%20and%0Aimproves%20long%20video%20comprehension%20performance%20across%20benchmarks.%20For%20instance%2C%0AMaxInfo%20achieves%20a%203.28%25%20improvement%20on%20LongVideoBench%20and%20a%206.4%25%20improvement%0Aon%20EgoSchema%20for%20LLaVA-Video-7B.%20It%20also%20achieves%20a%203.47%25%20improvement%20for%0ALLaVA-Video-72B.%20The%20approach%20is%20simple%20to%20implement%20and%20works%20with%20existing%0AVLLMs%20without%20the%20need%20for%20additional%20training%2C%20making%20it%20a%20practical%20and%0Aeffective%20alternative%20to%20traditional%20uniform%20sampling%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03183v1&entry.124074799=Read"},
{"title": "Multi-level Supervised Contrastive Learning", "author": "Naghmeh Ghanooni and Barbod Pajoum and Harshit Rawal and Sophie Fellenz and Vo Nguyen Le Duy and Marius Kloft", "abstract": "  Contrastive learning is a well-established paradigm in representation\nlearning. The standard framework of contrastive learning minimizes the distance\nbetween \"similar\" instances and maximizes the distance between dissimilar ones\nin the projection space, disregarding the various aspects of similarity that\ncan exist between two samples. Current methods rely on a single projection\nhead, which fails to capture the full complexity of different aspects of a\nsample, leading to suboptimal performance, especially in scenarios with limited\ntraining data. In this paper, we present a novel supervised contrastive\nlearning method in a unified framework called multilevel contrastive learning\n(MLCL), that can be applied to both multi-label and hierarchical classification\ntasks. The key strength of the proposed method is the ability to capture\nsimilarities between samples across different labels and/or hierarchies using\nmultiple projection heads. Extensive experiments on text and image datasets\ndemonstrate that the proposed approach outperforms state-of-the-art contrastive\nlearning methods\n", "link": "http://arxiv.org/abs/2502.02202v2", "date": "2025-02-05", "relevancy": 2.7306, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5838}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5438}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-level%20Supervised%20Contrastive%20Learning&body=Title%3A%20Multi-level%20Supervised%20Contrastive%20Learning%0AAuthor%3A%20Naghmeh%20Ghanooni%20and%20Barbod%20Pajoum%20and%20Harshit%20Rawal%20and%20Sophie%20Fellenz%20and%20Vo%20Nguyen%20Le%20Duy%20and%20Marius%20Kloft%0AAbstract%3A%20%20%20Contrastive%20learning%20is%20a%20well-established%20paradigm%20in%20representation%0Alearning.%20The%20standard%20framework%20of%20contrastive%20learning%20minimizes%20the%20distance%0Abetween%20%22similar%22%20instances%20and%20maximizes%20the%20distance%20between%20dissimilar%20ones%0Ain%20the%20projection%20space%2C%20disregarding%20the%20various%20aspects%20of%20similarity%20that%0Acan%20exist%20between%20two%20samples.%20Current%20methods%20rely%20on%20a%20single%20projection%0Ahead%2C%20which%20fails%20to%20capture%20the%20full%20complexity%20of%20different%20aspects%20of%20a%0Asample%2C%20leading%20to%20suboptimal%20performance%2C%20especially%20in%20scenarios%20with%20limited%0Atraining%20data.%20In%20this%20paper%2C%20we%20present%20a%20novel%20supervised%20contrastive%0Alearning%20method%20in%20a%20unified%20framework%20called%20multilevel%20contrastive%20learning%0A%28MLCL%29%2C%20that%20can%20be%20applied%20to%20both%20multi-label%20and%20hierarchical%20classification%0Atasks.%20The%20key%20strength%20of%20the%20proposed%20method%20is%20the%20ability%20to%20capture%0Asimilarities%20between%20samples%20across%20different%20labels%20and/or%20hierarchies%20using%0Amultiple%20projection%20heads.%20Extensive%20experiments%20on%20text%20and%20image%20datasets%0Ademonstrate%20that%20the%20proposed%20approach%20outperforms%20state-of-the-art%20contrastive%0Alearning%20methods%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02202v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-level%2520Supervised%2520Contrastive%2520Learning%26entry.906535625%3DNaghmeh%2520Ghanooni%2520and%2520Barbod%2520Pajoum%2520and%2520Harshit%2520Rawal%2520and%2520Sophie%2520Fellenz%2520and%2520Vo%2520Nguyen%2520Le%2520Duy%2520and%2520Marius%2520Kloft%26entry.1292438233%3D%2520%2520Contrastive%2520learning%2520is%2520a%2520well-established%2520paradigm%2520in%2520representation%250Alearning.%2520The%2520standard%2520framework%2520of%2520contrastive%2520learning%2520minimizes%2520the%2520distance%250Abetween%2520%2522similar%2522%2520instances%2520and%2520maximizes%2520the%2520distance%2520between%2520dissimilar%2520ones%250Ain%2520the%2520projection%2520space%252C%2520disregarding%2520the%2520various%2520aspects%2520of%2520similarity%2520that%250Acan%2520exist%2520between%2520two%2520samples.%2520Current%2520methods%2520rely%2520on%2520a%2520single%2520projection%250Ahead%252C%2520which%2520fails%2520to%2520capture%2520the%2520full%2520complexity%2520of%2520different%2520aspects%2520of%2520a%250Asample%252C%2520leading%2520to%2520suboptimal%2520performance%252C%2520especially%2520in%2520scenarios%2520with%2520limited%250Atraining%2520data.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520supervised%2520contrastive%250Alearning%2520method%2520in%2520a%2520unified%2520framework%2520called%2520multilevel%2520contrastive%2520learning%250A%2528MLCL%2529%252C%2520that%2520can%2520be%2520applied%2520to%2520both%2520multi-label%2520and%2520hierarchical%2520classification%250Atasks.%2520The%2520key%2520strength%2520of%2520the%2520proposed%2520method%2520is%2520the%2520ability%2520to%2520capture%250Asimilarities%2520between%2520samples%2520across%2520different%2520labels%2520and/or%2520hierarchies%2520using%250Amultiple%2520projection%2520heads.%2520Extensive%2520experiments%2520on%2520text%2520and%2520image%2520datasets%250Ademonstrate%2520that%2520the%2520proposed%2520approach%2520outperforms%2520state-of-the-art%2520contrastive%250Alearning%2520methods%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02202v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-level%20Supervised%20Contrastive%20Learning&entry.906535625=Naghmeh%20Ghanooni%20and%20Barbod%20Pajoum%20and%20Harshit%20Rawal%20and%20Sophie%20Fellenz%20and%20Vo%20Nguyen%20Le%20Duy%20and%20Marius%20Kloft&entry.1292438233=%20%20Contrastive%20learning%20is%20a%20well-established%20paradigm%20in%20representation%0Alearning.%20The%20standard%20framework%20of%20contrastive%20learning%20minimizes%20the%20distance%0Abetween%20%22similar%22%20instances%20and%20maximizes%20the%20distance%20between%20dissimilar%20ones%0Ain%20the%20projection%20space%2C%20disregarding%20the%20various%20aspects%20of%20similarity%20that%0Acan%20exist%20between%20two%20samples.%20Current%20methods%20rely%20on%20a%20single%20projection%0Ahead%2C%20which%20fails%20to%20capture%20the%20full%20complexity%20of%20different%20aspects%20of%20a%0Asample%2C%20leading%20to%20suboptimal%20performance%2C%20especially%20in%20scenarios%20with%20limited%0Atraining%20data.%20In%20this%20paper%2C%20we%20present%20a%20novel%20supervised%20contrastive%0Alearning%20method%20in%20a%20unified%20framework%20called%20multilevel%20contrastive%20learning%0A%28MLCL%29%2C%20that%20can%20be%20applied%20to%20both%20multi-label%20and%20hierarchical%20classification%0Atasks.%20The%20key%20strength%20of%20the%20proposed%20method%20is%20the%20ability%20to%20capture%0Asimilarities%20between%20samples%20across%20different%20labels%20and/or%20hierarchies%20using%0Amultiple%20projection%20heads.%20Extensive%20experiments%20on%20text%20and%20image%20datasets%0Ademonstrate%20that%20the%20proposed%20approach%20outperforms%20state-of-the-art%20contrastive%0Alearning%20methods%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02202v2&entry.124074799=Read"},
{"title": "RadVLM: A Multitask Conversational Vision-Language Model for Radiology", "author": "Nicolas Deperrois and Hidetoshi Matsuo and Samuel Ruip\u00e9rez-Campillo and Moritz Vandenhirtz and Sonia Laguna and Alain Ryser and Koji Fujimoto and Mizuho Nishio and Thomas M. Sutter and Julia E. Vogt and Jonas Kluckert and Thomas Frauenfelder and Christian Bl\u00fcthgen and Farhad Nooralahzadeh and Michael Krauthammer", "abstract": "  The widespread use of chest X-rays (CXRs), coupled with a shortage of\nradiologists, has driven growing interest in automated CXR analysis and\nAI-assisted reporting. While existing vision-language models (VLMs) show\npromise in specific tasks such as report generation or abnormality detection,\nthey often lack support for interactive diagnostic capabilities. In this work\nwe present RadVLM, a compact, multitask conversational foundation model\ndesigned for CXR interpretation. To this end, we curate a large-scale\ninstruction dataset comprising over 1 million image-instruction pairs\ncontaining both single-turn tasks -- such as report generation, abnormality\nclassification, and visual grounding -- and multi-turn, multi-task\nconversational interactions. After fine-tuning RadVLM on this instruction\ndataset, we evaluate it across different tasks along with re-implemented\nbaseline VLMs. Our results show that RadVLM achieves state-of-the-art\nperformance in conversational capabilities and visual grounding while remaining\ncompetitive in other radiology tasks. Ablation studies further highlight the\nbenefit of joint training across multiple tasks, particularly for scenarios\nwith limited annotated data. Together, these findings highlight the potential\nof RadVLM as a clinically relevant AI assistant, providing structured CXR\ninterpretation and conversational capabilities to support more effective and\naccessible diagnostic workflows.\n", "link": "http://arxiv.org/abs/2502.03333v1", "date": "2025-02-05", "relevancy": 2.7252, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5488}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5488}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RadVLM%3A%20A%20Multitask%20Conversational%20Vision-Language%20Model%20for%20Radiology&body=Title%3A%20RadVLM%3A%20A%20Multitask%20Conversational%20Vision-Language%20Model%20for%20Radiology%0AAuthor%3A%20Nicolas%20Deperrois%20and%20Hidetoshi%20Matsuo%20and%20Samuel%20Ruip%C3%A9rez-Campillo%20and%20Moritz%20Vandenhirtz%20and%20Sonia%20Laguna%20and%20Alain%20Ryser%20and%20Koji%20Fujimoto%20and%20Mizuho%20Nishio%20and%20Thomas%20M.%20Sutter%20and%20Julia%20E.%20Vogt%20and%20Jonas%20Kluckert%20and%20Thomas%20Frauenfelder%20and%20Christian%20Bl%C3%BCthgen%20and%20Farhad%20Nooralahzadeh%20and%20Michael%20Krauthammer%0AAbstract%3A%20%20%20The%20widespread%20use%20of%20chest%20X-rays%20%28CXRs%29%2C%20coupled%20with%20a%20shortage%20of%0Aradiologists%2C%20has%20driven%20growing%20interest%20in%20automated%20CXR%20analysis%20and%0AAI-assisted%20reporting.%20While%20existing%20vision-language%20models%20%28VLMs%29%20show%0Apromise%20in%20specific%20tasks%20such%20as%20report%20generation%20or%20abnormality%20detection%2C%0Athey%20often%20lack%20support%20for%20interactive%20diagnostic%20capabilities.%20In%20this%20work%0Awe%20present%20RadVLM%2C%20a%20compact%2C%20multitask%20conversational%20foundation%20model%0Adesigned%20for%20CXR%20interpretation.%20To%20this%20end%2C%20we%20curate%20a%20large-scale%0Ainstruction%20dataset%20comprising%20over%201%20million%20image-instruction%20pairs%0Acontaining%20both%20single-turn%20tasks%20--%20such%20as%20report%20generation%2C%20abnormality%0Aclassification%2C%20and%20visual%20grounding%20--%20and%20multi-turn%2C%20multi-task%0Aconversational%20interactions.%20After%20fine-tuning%20RadVLM%20on%20this%20instruction%0Adataset%2C%20we%20evaluate%20it%20across%20different%20tasks%20along%20with%20re-implemented%0Abaseline%20VLMs.%20Our%20results%20show%20that%20RadVLM%20achieves%20state-of-the-art%0Aperformance%20in%20conversational%20capabilities%20and%20visual%20grounding%20while%20remaining%0Acompetitive%20in%20other%20radiology%20tasks.%20Ablation%20studies%20further%20highlight%20the%0Abenefit%20of%20joint%20training%20across%20multiple%20tasks%2C%20particularly%20for%20scenarios%0Awith%20limited%20annotated%20data.%20Together%2C%20these%20findings%20highlight%20the%20potential%0Aof%20RadVLM%20as%20a%20clinically%20relevant%20AI%20assistant%2C%20providing%20structured%20CXR%0Ainterpretation%20and%20conversational%20capabilities%20to%20support%20more%20effective%20and%0Aaccessible%20diagnostic%20workflows.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03333v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRadVLM%253A%2520A%2520Multitask%2520Conversational%2520Vision-Language%2520Model%2520for%2520Radiology%26entry.906535625%3DNicolas%2520Deperrois%2520and%2520Hidetoshi%2520Matsuo%2520and%2520Samuel%2520Ruip%25C3%25A9rez-Campillo%2520and%2520Moritz%2520Vandenhirtz%2520and%2520Sonia%2520Laguna%2520and%2520Alain%2520Ryser%2520and%2520Koji%2520Fujimoto%2520and%2520Mizuho%2520Nishio%2520and%2520Thomas%2520M.%2520Sutter%2520and%2520Julia%2520E.%2520Vogt%2520and%2520Jonas%2520Kluckert%2520and%2520Thomas%2520Frauenfelder%2520and%2520Christian%2520Bl%25C3%25BCthgen%2520and%2520Farhad%2520Nooralahzadeh%2520and%2520Michael%2520Krauthammer%26entry.1292438233%3D%2520%2520The%2520widespread%2520use%2520of%2520chest%2520X-rays%2520%2528CXRs%2529%252C%2520coupled%2520with%2520a%2520shortage%2520of%250Aradiologists%252C%2520has%2520driven%2520growing%2520interest%2520in%2520automated%2520CXR%2520analysis%2520and%250AAI-assisted%2520reporting.%2520While%2520existing%2520vision-language%2520models%2520%2528VLMs%2529%2520show%250Apromise%2520in%2520specific%2520tasks%2520such%2520as%2520report%2520generation%2520or%2520abnormality%2520detection%252C%250Athey%2520often%2520lack%2520support%2520for%2520interactive%2520diagnostic%2520capabilities.%2520In%2520this%2520work%250Awe%2520present%2520RadVLM%252C%2520a%2520compact%252C%2520multitask%2520conversational%2520foundation%2520model%250Adesigned%2520for%2520CXR%2520interpretation.%2520To%2520this%2520end%252C%2520we%2520curate%2520a%2520large-scale%250Ainstruction%2520dataset%2520comprising%2520over%25201%2520million%2520image-instruction%2520pairs%250Acontaining%2520both%2520single-turn%2520tasks%2520--%2520such%2520as%2520report%2520generation%252C%2520abnormality%250Aclassification%252C%2520and%2520visual%2520grounding%2520--%2520and%2520multi-turn%252C%2520multi-task%250Aconversational%2520interactions.%2520After%2520fine-tuning%2520RadVLM%2520on%2520this%2520instruction%250Adataset%252C%2520we%2520evaluate%2520it%2520across%2520different%2520tasks%2520along%2520with%2520re-implemented%250Abaseline%2520VLMs.%2520Our%2520results%2520show%2520that%2520RadVLM%2520achieves%2520state-of-the-art%250Aperformance%2520in%2520conversational%2520capabilities%2520and%2520visual%2520grounding%2520while%2520remaining%250Acompetitive%2520in%2520other%2520radiology%2520tasks.%2520Ablation%2520studies%2520further%2520highlight%2520the%250Abenefit%2520of%2520joint%2520training%2520across%2520multiple%2520tasks%252C%2520particularly%2520for%2520scenarios%250Awith%2520limited%2520annotated%2520data.%2520Together%252C%2520these%2520findings%2520highlight%2520the%2520potential%250Aof%2520RadVLM%2520as%2520a%2520clinically%2520relevant%2520AI%2520assistant%252C%2520providing%2520structured%2520CXR%250Ainterpretation%2520and%2520conversational%2520capabilities%2520to%2520support%2520more%2520effective%2520and%250Aaccessible%2520diagnostic%2520workflows.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03333v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RadVLM%3A%20A%20Multitask%20Conversational%20Vision-Language%20Model%20for%20Radiology&entry.906535625=Nicolas%20Deperrois%20and%20Hidetoshi%20Matsuo%20and%20Samuel%20Ruip%C3%A9rez-Campillo%20and%20Moritz%20Vandenhirtz%20and%20Sonia%20Laguna%20and%20Alain%20Ryser%20and%20Koji%20Fujimoto%20and%20Mizuho%20Nishio%20and%20Thomas%20M.%20Sutter%20and%20Julia%20E.%20Vogt%20and%20Jonas%20Kluckert%20and%20Thomas%20Frauenfelder%20and%20Christian%20Bl%C3%BCthgen%20and%20Farhad%20Nooralahzadeh%20and%20Michael%20Krauthammer&entry.1292438233=%20%20The%20widespread%20use%20of%20chest%20X-rays%20%28CXRs%29%2C%20coupled%20with%20a%20shortage%20of%0Aradiologists%2C%20has%20driven%20growing%20interest%20in%20automated%20CXR%20analysis%20and%0AAI-assisted%20reporting.%20While%20existing%20vision-language%20models%20%28VLMs%29%20show%0Apromise%20in%20specific%20tasks%20such%20as%20report%20generation%20or%20abnormality%20detection%2C%0Athey%20often%20lack%20support%20for%20interactive%20diagnostic%20capabilities.%20In%20this%20work%0Awe%20present%20RadVLM%2C%20a%20compact%2C%20multitask%20conversational%20foundation%20model%0Adesigned%20for%20CXR%20interpretation.%20To%20this%20end%2C%20we%20curate%20a%20large-scale%0Ainstruction%20dataset%20comprising%20over%201%20million%20image-instruction%20pairs%0Acontaining%20both%20single-turn%20tasks%20--%20such%20as%20report%20generation%2C%20abnormality%0Aclassification%2C%20and%20visual%20grounding%20--%20and%20multi-turn%2C%20multi-task%0Aconversational%20interactions.%20After%20fine-tuning%20RadVLM%20on%20this%20instruction%0Adataset%2C%20we%20evaluate%20it%20across%20different%20tasks%20along%20with%20re-implemented%0Abaseline%20VLMs.%20Our%20results%20show%20that%20RadVLM%20achieves%20state-of-the-art%0Aperformance%20in%20conversational%20capabilities%20and%20visual%20grounding%20while%20remaining%0Acompetitive%20in%20other%20radiology%20tasks.%20Ablation%20studies%20further%20highlight%20the%0Abenefit%20of%20joint%20training%20across%20multiple%20tasks%2C%20particularly%20for%20scenarios%0Awith%20limited%20annotated%20data.%20Together%2C%20these%20findings%20highlight%20the%20potential%0Aof%20RadVLM%20as%20a%20clinically%20relevant%20AI%20assistant%2C%20providing%20structured%20CXR%0Ainterpretation%20and%20conversational%20capabilities%20to%20support%20more%20effective%20and%0Aaccessible%20diagnostic%20workflows.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03333v1&entry.124074799=Read"},
{"title": "Double Distillation Network for Multi-Agent Reinforcement Learning", "author": "Yang Zhou and Siying Wang and Wenyu Chen and Ruoning Zhang and Zhitong Zhao and Zixuan Zhang", "abstract": "  Multi-agent reinforcement learning typically employs a centralized\ntraining-decentralized execution (CTDE) framework to alleviate the\nnon-stationarity in environment. However, the partial observability during\nexecution may lead to cumulative gap errors gathered by agents, impairing the\ntraining of effective collaborative policies. To overcome this challenge, we\nintroduce the Double Distillation Network (DDN), which incorporates two\ndistillation modules aimed at enhancing robust coordination and facilitating\nthe collaboration process under constrained information. The external\ndistillation module uses a global guiding network and a local policy network,\nemploying distillation to reconcile the gap between global training and local\nexecution. In addition, the internal distillation module introduces intrinsic\nrewards, drawn from state information, to enhance the exploration capabilities\nof agents. Extensive experiments demonstrate that DDN significantly improves\nperformance across multiple scenarios.\n", "link": "http://arxiv.org/abs/2502.03125v1", "date": "2025-02-05", "relevancy": 2.6674, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5578}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5329}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5098}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Double%20Distillation%20Network%20for%20Multi-Agent%20Reinforcement%20Learning&body=Title%3A%20Double%20Distillation%20Network%20for%20Multi-Agent%20Reinforcement%20Learning%0AAuthor%3A%20Yang%20Zhou%20and%20Siying%20Wang%20and%20Wenyu%20Chen%20and%20Ruoning%20Zhang%20and%20Zhitong%20Zhao%20and%20Zixuan%20Zhang%0AAbstract%3A%20%20%20Multi-agent%20reinforcement%20learning%20typically%20employs%20a%20centralized%0Atraining-decentralized%20execution%20%28CTDE%29%20framework%20to%20alleviate%20the%0Anon-stationarity%20in%20environment.%20However%2C%20the%20partial%20observability%20during%0Aexecution%20may%20lead%20to%20cumulative%20gap%20errors%20gathered%20by%20agents%2C%20impairing%20the%0Atraining%20of%20effective%20collaborative%20policies.%20To%20overcome%20this%20challenge%2C%20we%0Aintroduce%20the%20Double%20Distillation%20Network%20%28DDN%29%2C%20which%20incorporates%20two%0Adistillation%20modules%20aimed%20at%20enhancing%20robust%20coordination%20and%20facilitating%0Athe%20collaboration%20process%20under%20constrained%20information.%20The%20external%0Adistillation%20module%20uses%20a%20global%20guiding%20network%20and%20a%20local%20policy%20network%2C%0Aemploying%20distillation%20to%20reconcile%20the%20gap%20between%20global%20training%20and%20local%0Aexecution.%20In%20addition%2C%20the%20internal%20distillation%20module%20introduces%20intrinsic%0Arewards%2C%20drawn%20from%20state%20information%2C%20to%20enhance%20the%20exploration%20capabilities%0Aof%20agents.%20Extensive%20experiments%20demonstrate%20that%20DDN%20significantly%20improves%0Aperformance%20across%20multiple%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03125v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDouble%2520Distillation%2520Network%2520for%2520Multi-Agent%2520Reinforcement%2520Learning%26entry.906535625%3DYang%2520Zhou%2520and%2520Siying%2520Wang%2520and%2520Wenyu%2520Chen%2520and%2520Ruoning%2520Zhang%2520and%2520Zhitong%2520Zhao%2520and%2520Zixuan%2520Zhang%26entry.1292438233%3D%2520%2520Multi-agent%2520reinforcement%2520learning%2520typically%2520employs%2520a%2520centralized%250Atraining-decentralized%2520execution%2520%2528CTDE%2529%2520framework%2520to%2520alleviate%2520the%250Anon-stationarity%2520in%2520environment.%2520However%252C%2520the%2520partial%2520observability%2520during%250Aexecution%2520may%2520lead%2520to%2520cumulative%2520gap%2520errors%2520gathered%2520by%2520agents%252C%2520impairing%2520the%250Atraining%2520of%2520effective%2520collaborative%2520policies.%2520To%2520overcome%2520this%2520challenge%252C%2520we%250Aintroduce%2520the%2520Double%2520Distillation%2520Network%2520%2528DDN%2529%252C%2520which%2520incorporates%2520two%250Adistillation%2520modules%2520aimed%2520at%2520enhancing%2520robust%2520coordination%2520and%2520facilitating%250Athe%2520collaboration%2520process%2520under%2520constrained%2520information.%2520The%2520external%250Adistillation%2520module%2520uses%2520a%2520global%2520guiding%2520network%2520and%2520a%2520local%2520policy%2520network%252C%250Aemploying%2520distillation%2520to%2520reconcile%2520the%2520gap%2520between%2520global%2520training%2520and%2520local%250Aexecution.%2520In%2520addition%252C%2520the%2520internal%2520distillation%2520module%2520introduces%2520intrinsic%250Arewards%252C%2520drawn%2520from%2520state%2520information%252C%2520to%2520enhance%2520the%2520exploration%2520capabilities%250Aof%2520agents.%2520Extensive%2520experiments%2520demonstrate%2520that%2520DDN%2520significantly%2520improves%250Aperformance%2520across%2520multiple%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03125v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Double%20Distillation%20Network%20for%20Multi-Agent%20Reinforcement%20Learning&entry.906535625=Yang%20Zhou%20and%20Siying%20Wang%20and%20Wenyu%20Chen%20and%20Ruoning%20Zhang%20and%20Zhitong%20Zhao%20and%20Zixuan%20Zhang&entry.1292438233=%20%20Multi-agent%20reinforcement%20learning%20typically%20employs%20a%20centralized%0Atraining-decentralized%20execution%20%28CTDE%29%20framework%20to%20alleviate%20the%0Anon-stationarity%20in%20environment.%20However%2C%20the%20partial%20observability%20during%0Aexecution%20may%20lead%20to%20cumulative%20gap%20errors%20gathered%20by%20agents%2C%20impairing%20the%0Atraining%20of%20effective%20collaborative%20policies.%20To%20overcome%20this%20challenge%2C%20we%0Aintroduce%20the%20Double%20Distillation%20Network%20%28DDN%29%2C%20which%20incorporates%20two%0Adistillation%20modules%20aimed%20at%20enhancing%20robust%20coordination%20and%20facilitating%0Athe%20collaboration%20process%20under%20constrained%20information.%20The%20external%0Adistillation%20module%20uses%20a%20global%20guiding%20network%20and%20a%20local%20policy%20network%2C%0Aemploying%20distillation%20to%20reconcile%20the%20gap%20between%20global%20training%20and%20local%0Aexecution.%20In%20addition%2C%20the%20internal%20distillation%20module%20introduces%20intrinsic%0Arewards%2C%20drawn%20from%20state%20information%2C%20to%20enhance%20the%20exploration%20capabilities%0Aof%20agents.%20Extensive%20experiments%20demonstrate%20that%20DDN%20significantly%20improves%0Aperformance%20across%20multiple%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03125v1&entry.124074799=Read"},
{"title": "RiemannGFM: Learning a Graph Foundation Model from Riemannian Geometry", "author": "Li Sun and Zhenhao Huang and Suyang Zhou and Qiqi Wan and Hao Peng and Philip Yu", "abstract": "  The foundation model has heralded a new era in artificial intelligence,\npretraining a single model to offer cross-domain transferability on different\ndatasets. Graph neural networks excel at learning graph data, the omnipresent\nnon-Euclidean structure, but often lack the generalization capacity. Hence,\ngraph foundation model is drawing increasing attention, and recent efforts have\nbeen made to leverage Large Language Models. On the one hand, existing studies\nprimarily focus on text-attributed graphs, while a wider range of real graphs\ndo not contain fruitful textual attributes. On the other hand, the sequential\ngraph description tailored for the Large Language Model neglects the structural\ncomplexity, which is a predominant characteristic of the graph. Such\nlimitations motivate an important question: Can we go beyond Large Language\nModels, and pretrain a universal model to learn the structural knowledge for\nany graph? The answer in the language or vision domain is a shared vocabulary.\nWe observe the fact that there also exist shared substructures underlying graph\ndomain, and thereby open a new opportunity of graph foundation model with\nstructural vocabulary. The key innovation is the discovery of a simple yet\neffective structural vocabulary of trees and cycles, and we explore its\ninherent connection to Riemannian geometry. Herein, we present a universal\npretraining model, RiemannGFM. Concretely, we first construct a novel product\nbundle to incorporate the diverse geometries of the vocabulary. Then, on this\nconstructed space, we stack Riemannian layers where the structural vocabulary,\nregardless of specific graph, is learned in Riemannian manifold offering\ncross-domain transferability. Extensive experiments show the effectiveness of\nRiemannGFM on a diversity of real graphs.\n", "link": "http://arxiv.org/abs/2502.03251v1", "date": "2025-02-05", "relevancy": 2.6648, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.536}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.536}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5268}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RiemannGFM%3A%20Learning%20a%20Graph%20Foundation%20Model%20from%20Riemannian%20Geometry&body=Title%3A%20RiemannGFM%3A%20Learning%20a%20Graph%20Foundation%20Model%20from%20Riemannian%20Geometry%0AAuthor%3A%20Li%20Sun%20and%20Zhenhao%20Huang%20and%20Suyang%20Zhou%20and%20Qiqi%20Wan%20and%20Hao%20Peng%20and%20Philip%20Yu%0AAbstract%3A%20%20%20The%20foundation%20model%20has%20heralded%20a%20new%20era%20in%20artificial%20intelligence%2C%0Apretraining%20a%20single%20model%20to%20offer%20cross-domain%20transferability%20on%20different%0Adatasets.%20Graph%20neural%20networks%20excel%20at%20learning%20graph%20data%2C%20the%20omnipresent%0Anon-Euclidean%20structure%2C%20but%20often%20lack%20the%20generalization%20capacity.%20Hence%2C%0Agraph%20foundation%20model%20is%20drawing%20increasing%20attention%2C%20and%20recent%20efforts%20have%0Abeen%20made%20to%20leverage%20Large%20Language%20Models.%20On%20the%20one%20hand%2C%20existing%20studies%0Aprimarily%20focus%20on%20text-attributed%20graphs%2C%20while%20a%20wider%20range%20of%20real%20graphs%0Ado%20not%20contain%20fruitful%20textual%20attributes.%20On%20the%20other%20hand%2C%20the%20sequential%0Agraph%20description%20tailored%20for%20the%20Large%20Language%20Model%20neglects%20the%20structural%0Acomplexity%2C%20which%20is%20a%20predominant%20characteristic%20of%20the%20graph.%20Such%0Alimitations%20motivate%20an%20important%20question%3A%20Can%20we%20go%20beyond%20Large%20Language%0AModels%2C%20and%20pretrain%20a%20universal%20model%20to%20learn%20the%20structural%20knowledge%20for%0Aany%20graph%3F%20The%20answer%20in%20the%20language%20or%20vision%20domain%20is%20a%20shared%20vocabulary.%0AWe%20observe%20the%20fact%20that%20there%20also%20exist%20shared%20substructures%20underlying%20graph%0Adomain%2C%20and%20thereby%20open%20a%20new%20opportunity%20of%20graph%20foundation%20model%20with%0Astructural%20vocabulary.%20The%20key%20innovation%20is%20the%20discovery%20of%20a%20simple%20yet%0Aeffective%20structural%20vocabulary%20of%20trees%20and%20cycles%2C%20and%20we%20explore%20its%0Ainherent%20connection%20to%20Riemannian%20geometry.%20Herein%2C%20we%20present%20a%20universal%0Apretraining%20model%2C%20RiemannGFM.%20Concretely%2C%20we%20first%20construct%20a%20novel%20product%0Abundle%20to%20incorporate%20the%20diverse%20geometries%20of%20the%20vocabulary.%20Then%2C%20on%20this%0Aconstructed%20space%2C%20we%20stack%20Riemannian%20layers%20where%20the%20structural%20vocabulary%2C%0Aregardless%20of%20specific%20graph%2C%20is%20learned%20in%20Riemannian%20manifold%20offering%0Across-domain%20transferability.%20Extensive%20experiments%20show%20the%20effectiveness%20of%0ARiemannGFM%20on%20a%20diversity%20of%20real%20graphs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03251v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRiemannGFM%253A%2520Learning%2520a%2520Graph%2520Foundation%2520Model%2520from%2520Riemannian%2520Geometry%26entry.906535625%3DLi%2520Sun%2520and%2520Zhenhao%2520Huang%2520and%2520Suyang%2520Zhou%2520and%2520Qiqi%2520Wan%2520and%2520Hao%2520Peng%2520and%2520Philip%2520Yu%26entry.1292438233%3D%2520%2520The%2520foundation%2520model%2520has%2520heralded%2520a%2520new%2520era%2520in%2520artificial%2520intelligence%252C%250Apretraining%2520a%2520single%2520model%2520to%2520offer%2520cross-domain%2520transferability%2520on%2520different%250Adatasets.%2520Graph%2520neural%2520networks%2520excel%2520at%2520learning%2520graph%2520data%252C%2520the%2520omnipresent%250Anon-Euclidean%2520structure%252C%2520but%2520often%2520lack%2520the%2520generalization%2520capacity.%2520Hence%252C%250Agraph%2520foundation%2520model%2520is%2520drawing%2520increasing%2520attention%252C%2520and%2520recent%2520efforts%2520have%250Abeen%2520made%2520to%2520leverage%2520Large%2520Language%2520Models.%2520On%2520the%2520one%2520hand%252C%2520existing%2520studies%250Aprimarily%2520focus%2520on%2520text-attributed%2520graphs%252C%2520while%2520a%2520wider%2520range%2520of%2520real%2520graphs%250Ado%2520not%2520contain%2520fruitful%2520textual%2520attributes.%2520On%2520the%2520other%2520hand%252C%2520the%2520sequential%250Agraph%2520description%2520tailored%2520for%2520the%2520Large%2520Language%2520Model%2520neglects%2520the%2520structural%250Acomplexity%252C%2520which%2520is%2520a%2520predominant%2520characteristic%2520of%2520the%2520graph.%2520Such%250Alimitations%2520motivate%2520an%2520important%2520question%253A%2520Can%2520we%2520go%2520beyond%2520Large%2520Language%250AModels%252C%2520and%2520pretrain%2520a%2520universal%2520model%2520to%2520learn%2520the%2520structural%2520knowledge%2520for%250Aany%2520graph%253F%2520The%2520answer%2520in%2520the%2520language%2520or%2520vision%2520domain%2520is%2520a%2520shared%2520vocabulary.%250AWe%2520observe%2520the%2520fact%2520that%2520there%2520also%2520exist%2520shared%2520substructures%2520underlying%2520graph%250Adomain%252C%2520and%2520thereby%2520open%2520a%2520new%2520opportunity%2520of%2520graph%2520foundation%2520model%2520with%250Astructural%2520vocabulary.%2520The%2520key%2520innovation%2520is%2520the%2520discovery%2520of%2520a%2520simple%2520yet%250Aeffective%2520structural%2520vocabulary%2520of%2520trees%2520and%2520cycles%252C%2520and%2520we%2520explore%2520its%250Ainherent%2520connection%2520to%2520Riemannian%2520geometry.%2520Herein%252C%2520we%2520present%2520a%2520universal%250Apretraining%2520model%252C%2520RiemannGFM.%2520Concretely%252C%2520we%2520first%2520construct%2520a%2520novel%2520product%250Abundle%2520to%2520incorporate%2520the%2520diverse%2520geometries%2520of%2520the%2520vocabulary.%2520Then%252C%2520on%2520this%250Aconstructed%2520space%252C%2520we%2520stack%2520Riemannian%2520layers%2520where%2520the%2520structural%2520vocabulary%252C%250Aregardless%2520of%2520specific%2520graph%252C%2520is%2520learned%2520in%2520Riemannian%2520manifold%2520offering%250Across-domain%2520transferability.%2520Extensive%2520experiments%2520show%2520the%2520effectiveness%2520of%250ARiemannGFM%2520on%2520a%2520diversity%2520of%2520real%2520graphs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03251v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RiemannGFM%3A%20Learning%20a%20Graph%20Foundation%20Model%20from%20Riemannian%20Geometry&entry.906535625=Li%20Sun%20and%20Zhenhao%20Huang%20and%20Suyang%20Zhou%20and%20Qiqi%20Wan%20and%20Hao%20Peng%20and%20Philip%20Yu&entry.1292438233=%20%20The%20foundation%20model%20has%20heralded%20a%20new%20era%20in%20artificial%20intelligence%2C%0Apretraining%20a%20single%20model%20to%20offer%20cross-domain%20transferability%20on%20different%0Adatasets.%20Graph%20neural%20networks%20excel%20at%20learning%20graph%20data%2C%20the%20omnipresent%0Anon-Euclidean%20structure%2C%20but%20often%20lack%20the%20generalization%20capacity.%20Hence%2C%0Agraph%20foundation%20model%20is%20drawing%20increasing%20attention%2C%20and%20recent%20efforts%20have%0Abeen%20made%20to%20leverage%20Large%20Language%20Models.%20On%20the%20one%20hand%2C%20existing%20studies%0Aprimarily%20focus%20on%20text-attributed%20graphs%2C%20while%20a%20wider%20range%20of%20real%20graphs%0Ado%20not%20contain%20fruitful%20textual%20attributes.%20On%20the%20other%20hand%2C%20the%20sequential%0Agraph%20description%20tailored%20for%20the%20Large%20Language%20Model%20neglects%20the%20structural%0Acomplexity%2C%20which%20is%20a%20predominant%20characteristic%20of%20the%20graph.%20Such%0Alimitations%20motivate%20an%20important%20question%3A%20Can%20we%20go%20beyond%20Large%20Language%0AModels%2C%20and%20pretrain%20a%20universal%20model%20to%20learn%20the%20structural%20knowledge%20for%0Aany%20graph%3F%20The%20answer%20in%20the%20language%20or%20vision%20domain%20is%20a%20shared%20vocabulary.%0AWe%20observe%20the%20fact%20that%20there%20also%20exist%20shared%20substructures%20underlying%20graph%0Adomain%2C%20and%20thereby%20open%20a%20new%20opportunity%20of%20graph%20foundation%20model%20with%0Astructural%20vocabulary.%20The%20key%20innovation%20is%20the%20discovery%20of%20a%20simple%20yet%0Aeffective%20structural%20vocabulary%20of%20trees%20and%20cycles%2C%20and%20we%20explore%20its%0Ainherent%20connection%20to%20Riemannian%20geometry.%20Herein%2C%20we%20present%20a%20universal%0Apretraining%20model%2C%20RiemannGFM.%20Concretely%2C%20we%20first%20construct%20a%20novel%20product%0Abundle%20to%20incorporate%20the%20diverse%20geometries%20of%20the%20vocabulary.%20Then%2C%20on%20this%0Aconstructed%20space%2C%20we%20stack%20Riemannian%20layers%20where%20the%20structural%20vocabulary%2C%0Aregardless%20of%20specific%20graph%2C%20is%20learned%20in%20Riemannian%20manifold%20offering%0Across-domain%20transferability.%20Extensive%20experiments%20show%20the%20effectiveness%20of%0ARiemannGFM%20on%20a%20diversity%20of%20real%20graphs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03251v1&entry.124074799=Read"},
{"title": "Analyzing (In)Abilities of SAEs via Formal Languages", "author": "Abhinav Menon and Manish Shrivastava and David Krueger and Ekdeep Singh Lubana", "abstract": "  Autoencoders have been used for finding interpretable and disentangled\nfeatures underlying neural network representations in both image and text\ndomains. While the efficacy and pitfalls of such methods are well-studied in\nvision, there is a lack of corresponding results, both qualitative and\nquantitative, for the text domain. We aim to address this gap by training\nsparse autoencoders (SAEs) on a synthetic testbed of formal languages.\nSpecifically, we train SAEs on the hidden representations of models trained on\nformal languages (Dyck-2, Expr, and English PCFG) under a wide variety of\nhyperparameter settings, finding interpretable latents often emerge in the\nfeatures learned by our SAEs. However, similar to vision, we find performance\nturns out to be highly sensitive to inductive biases of the training pipeline.\nMoreover, we show latents correlating to certain features of the input do not\nalways induce a causal impact on model's computation. We thus argue that\ncausality has to become a central target in SAE training: learning of causal\nfeatures should be incentivized from the ground-up. Motivated by this, we\npropose and perform preliminary investigations for an approach that promotes\nlearning of causally relevant features in our formal language setting.\n", "link": "http://arxiv.org/abs/2410.11767v4", "date": "2025-02-05", "relevancy": 2.6595, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.532}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.532}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5318}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analyzing%20%28In%29Abilities%20of%20SAEs%20via%20Formal%20Languages&body=Title%3A%20Analyzing%20%28In%29Abilities%20of%20SAEs%20via%20Formal%20Languages%0AAuthor%3A%20Abhinav%20Menon%20and%20Manish%20Shrivastava%20and%20David%20Krueger%20and%20Ekdeep%20Singh%20Lubana%0AAbstract%3A%20%20%20Autoencoders%20have%20been%20used%20for%20finding%20interpretable%20and%20disentangled%0Afeatures%20underlying%20neural%20network%20representations%20in%20both%20image%20and%20text%0Adomains.%20While%20the%20efficacy%20and%20pitfalls%20of%20such%20methods%20are%20well-studied%20in%0Avision%2C%20there%20is%20a%20lack%20of%20corresponding%20results%2C%20both%20qualitative%20and%0Aquantitative%2C%20for%20the%20text%20domain.%20We%20aim%20to%20address%20this%20gap%20by%20training%0Asparse%20autoencoders%20%28SAEs%29%20on%20a%20synthetic%20testbed%20of%20formal%20languages.%0ASpecifically%2C%20we%20train%20SAEs%20on%20the%20hidden%20representations%20of%20models%20trained%20on%0Aformal%20languages%20%28Dyck-2%2C%20Expr%2C%20and%20English%20PCFG%29%20under%20a%20wide%20variety%20of%0Ahyperparameter%20settings%2C%20finding%20interpretable%20latents%20often%20emerge%20in%20the%0Afeatures%20learned%20by%20our%20SAEs.%20However%2C%20similar%20to%20vision%2C%20we%20find%20performance%0Aturns%20out%20to%20be%20highly%20sensitive%20to%20inductive%20biases%20of%20the%20training%20pipeline.%0AMoreover%2C%20we%20show%20latents%20correlating%20to%20certain%20features%20of%20the%20input%20do%20not%0Aalways%20induce%20a%20causal%20impact%20on%20model%27s%20computation.%20We%20thus%20argue%20that%0Acausality%20has%20to%20become%20a%20central%20target%20in%20SAE%20training%3A%20learning%20of%20causal%0Afeatures%20should%20be%20incentivized%20from%20the%20ground-up.%20Motivated%20by%20this%2C%20we%0Apropose%20and%20perform%20preliminary%20investigations%20for%20an%20approach%20that%20promotes%0Alearning%20of%20causally%20relevant%20features%20in%20our%20formal%20language%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.11767v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalyzing%2520%2528In%2529Abilities%2520of%2520SAEs%2520via%2520Formal%2520Languages%26entry.906535625%3DAbhinav%2520Menon%2520and%2520Manish%2520Shrivastava%2520and%2520David%2520Krueger%2520and%2520Ekdeep%2520Singh%2520Lubana%26entry.1292438233%3D%2520%2520Autoencoders%2520have%2520been%2520used%2520for%2520finding%2520interpretable%2520and%2520disentangled%250Afeatures%2520underlying%2520neural%2520network%2520representations%2520in%2520both%2520image%2520and%2520text%250Adomains.%2520While%2520the%2520efficacy%2520and%2520pitfalls%2520of%2520such%2520methods%2520are%2520well-studied%2520in%250Avision%252C%2520there%2520is%2520a%2520lack%2520of%2520corresponding%2520results%252C%2520both%2520qualitative%2520and%250Aquantitative%252C%2520for%2520the%2520text%2520domain.%2520We%2520aim%2520to%2520address%2520this%2520gap%2520by%2520training%250Asparse%2520autoencoders%2520%2528SAEs%2529%2520on%2520a%2520synthetic%2520testbed%2520of%2520formal%2520languages.%250ASpecifically%252C%2520we%2520train%2520SAEs%2520on%2520the%2520hidden%2520representations%2520of%2520models%2520trained%2520on%250Aformal%2520languages%2520%2528Dyck-2%252C%2520Expr%252C%2520and%2520English%2520PCFG%2529%2520under%2520a%2520wide%2520variety%2520of%250Ahyperparameter%2520settings%252C%2520finding%2520interpretable%2520latents%2520often%2520emerge%2520in%2520the%250Afeatures%2520learned%2520by%2520our%2520SAEs.%2520However%252C%2520similar%2520to%2520vision%252C%2520we%2520find%2520performance%250Aturns%2520out%2520to%2520be%2520highly%2520sensitive%2520to%2520inductive%2520biases%2520of%2520the%2520training%2520pipeline.%250AMoreover%252C%2520we%2520show%2520latents%2520correlating%2520to%2520certain%2520features%2520of%2520the%2520input%2520do%2520not%250Aalways%2520induce%2520a%2520causal%2520impact%2520on%2520model%2527s%2520computation.%2520We%2520thus%2520argue%2520that%250Acausality%2520has%2520to%2520become%2520a%2520central%2520target%2520in%2520SAE%2520training%253A%2520learning%2520of%2520causal%250Afeatures%2520should%2520be%2520incentivized%2520from%2520the%2520ground-up.%2520Motivated%2520by%2520this%252C%2520we%250Apropose%2520and%2520perform%2520preliminary%2520investigations%2520for%2520an%2520approach%2520that%2520promotes%250Alearning%2520of%2520causally%2520relevant%2520features%2520in%2520our%2520formal%2520language%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11767v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analyzing%20%28In%29Abilities%20of%20SAEs%20via%20Formal%20Languages&entry.906535625=Abhinav%20Menon%20and%20Manish%20Shrivastava%20and%20David%20Krueger%20and%20Ekdeep%20Singh%20Lubana&entry.1292438233=%20%20Autoencoders%20have%20been%20used%20for%20finding%20interpretable%20and%20disentangled%0Afeatures%20underlying%20neural%20network%20representations%20in%20both%20image%20and%20text%0Adomains.%20While%20the%20efficacy%20and%20pitfalls%20of%20such%20methods%20are%20well-studied%20in%0Avision%2C%20there%20is%20a%20lack%20of%20corresponding%20results%2C%20both%20qualitative%20and%0Aquantitative%2C%20for%20the%20text%20domain.%20We%20aim%20to%20address%20this%20gap%20by%20training%0Asparse%20autoencoders%20%28SAEs%29%20on%20a%20synthetic%20testbed%20of%20formal%20languages.%0ASpecifically%2C%20we%20train%20SAEs%20on%20the%20hidden%20representations%20of%20models%20trained%20on%0Aformal%20languages%20%28Dyck-2%2C%20Expr%2C%20and%20English%20PCFG%29%20under%20a%20wide%20variety%20of%0Ahyperparameter%20settings%2C%20finding%20interpretable%20latents%20often%20emerge%20in%20the%0Afeatures%20learned%20by%20our%20SAEs.%20However%2C%20similar%20to%20vision%2C%20we%20find%20performance%0Aturns%20out%20to%20be%20highly%20sensitive%20to%20inductive%20biases%20of%20the%20training%20pipeline.%0AMoreover%2C%20we%20show%20latents%20correlating%20to%20certain%20features%20of%20the%20input%20do%20not%0Aalways%20induce%20a%20causal%20impact%20on%20model%27s%20computation.%20We%20thus%20argue%20that%0Acausality%20has%20to%20become%20a%20central%20target%20in%20SAE%20training%3A%20learning%20of%20causal%0Afeatures%20should%20be%20incentivized%20from%20the%20ground-up.%20Motivated%20by%20this%2C%20we%0Apropose%20and%20perform%20preliminary%20investigations%20for%20an%20approach%20that%20promotes%0Alearning%20of%20causally%20relevant%20features%20in%20our%20formal%20language%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.11767v4&entry.124074799=Read"},
{"title": "Fast Sampling of Cosmological Initial Conditions with Gaussian Neural\n  Posterior Estimation", "author": "Oleg Savchenko and Guillermo Franco Abell\u00e1n and Florian List and Noemi Anau Montel and Christoph Weniger", "abstract": "  Knowledge of the primordial matter density field from which the large-scale\nstructure of the Universe emerged over cosmic time is of fundamental importance\nfor cosmology. However, reconstructing these cosmological initial conditions\nfrom late-time observations is a notoriously difficult task, which requires\nadvanced cosmological simulators and sophisticated statistical methods to\nexplore a multi-million-dimensional parameter space. We show how\nsimulation-based inference (SBI) can be used to tackle this problem and to\nobtain data-constrained realisations of the primordial dark matter density\nfield in a simulation-efficient way with general non-differentiable simulators.\nOur method is applicable to full high-resolution dark matter $N$-body\nsimulations and is based on modelling the posterior distribution of the\nconstrained initial conditions to be Gaussian with a diagonal covariance matrix\nin Fourier space. As a result, we can generate thousands of posterior samples\nwithin seconds on a single GPU, orders of magnitude faster than existing\nmethods, paving the way for sequential SBI for cosmological fields.\nFurthermore, we perform an analytical fit of the estimated dependence of the\ncovariance on the wavenumber, effectively transforming any point-estimator of\ninitial conditions into a fast sampler. We test the validity of our obtained\nsamples by comparing them to the true values with summary statistics and\nperforming a Bayesian consistency test.\n", "link": "http://arxiv.org/abs/2502.03139v1", "date": "2025-02-05", "relevancy": 2.6438, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5532}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5186}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5145}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fast%20Sampling%20of%20Cosmological%20Initial%20Conditions%20with%20Gaussian%20Neural%0A%20%20Posterior%20Estimation&body=Title%3A%20Fast%20Sampling%20of%20Cosmological%20Initial%20Conditions%20with%20Gaussian%20Neural%0A%20%20Posterior%20Estimation%0AAuthor%3A%20Oleg%20Savchenko%20and%20Guillermo%20Franco%20Abell%C3%A1n%20and%20Florian%20List%20and%20Noemi%20Anau%20Montel%20and%20Christoph%20Weniger%0AAbstract%3A%20%20%20Knowledge%20of%20the%20primordial%20matter%20density%20field%20from%20which%20the%20large-scale%0Astructure%20of%20the%20Universe%20emerged%20over%20cosmic%20time%20is%20of%20fundamental%20importance%0Afor%20cosmology.%20However%2C%20reconstructing%20these%20cosmological%20initial%20conditions%0Afrom%20late-time%20observations%20is%20a%20notoriously%20difficult%20task%2C%20which%20requires%0Aadvanced%20cosmological%20simulators%20and%20sophisticated%20statistical%20methods%20to%0Aexplore%20a%20multi-million-dimensional%20parameter%20space.%20We%20show%20how%0Asimulation-based%20inference%20%28SBI%29%20can%20be%20used%20to%20tackle%20this%20problem%20and%20to%0Aobtain%20data-constrained%20realisations%20of%20the%20primordial%20dark%20matter%20density%0Afield%20in%20a%20simulation-efficient%20way%20with%20general%20non-differentiable%20simulators.%0AOur%20method%20is%20applicable%20to%20full%20high-resolution%20dark%20matter%20%24N%24-body%0Asimulations%20and%20is%20based%20on%20modelling%20the%20posterior%20distribution%20of%20the%0Aconstrained%20initial%20conditions%20to%20be%20Gaussian%20with%20a%20diagonal%20covariance%20matrix%0Ain%20Fourier%20space.%20As%20a%20result%2C%20we%20can%20generate%20thousands%20of%20posterior%20samples%0Awithin%20seconds%20on%20a%20single%20GPU%2C%20orders%20of%20magnitude%20faster%20than%20existing%0Amethods%2C%20paving%20the%20way%20for%20sequential%20SBI%20for%20cosmological%20fields.%0AFurthermore%2C%20we%20perform%20an%20analytical%20fit%20of%20the%20estimated%20dependence%20of%20the%0Acovariance%20on%20the%20wavenumber%2C%20effectively%20transforming%20any%20point-estimator%20of%0Ainitial%20conditions%20into%20a%20fast%20sampler.%20We%20test%20the%20validity%20of%20our%20obtained%0Asamples%20by%20comparing%20them%20to%20the%20true%20values%20with%20summary%20statistics%20and%0Aperforming%20a%20Bayesian%20consistency%20test.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03139v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFast%2520Sampling%2520of%2520Cosmological%2520Initial%2520Conditions%2520with%2520Gaussian%2520Neural%250A%2520%2520Posterior%2520Estimation%26entry.906535625%3DOleg%2520Savchenko%2520and%2520Guillermo%2520Franco%2520Abell%25C3%25A1n%2520and%2520Florian%2520List%2520and%2520Noemi%2520Anau%2520Montel%2520and%2520Christoph%2520Weniger%26entry.1292438233%3D%2520%2520Knowledge%2520of%2520the%2520primordial%2520matter%2520density%2520field%2520from%2520which%2520the%2520large-scale%250Astructure%2520of%2520the%2520Universe%2520emerged%2520over%2520cosmic%2520time%2520is%2520of%2520fundamental%2520importance%250Afor%2520cosmology.%2520However%252C%2520reconstructing%2520these%2520cosmological%2520initial%2520conditions%250Afrom%2520late-time%2520observations%2520is%2520a%2520notoriously%2520difficult%2520task%252C%2520which%2520requires%250Aadvanced%2520cosmological%2520simulators%2520and%2520sophisticated%2520statistical%2520methods%2520to%250Aexplore%2520a%2520multi-million-dimensional%2520parameter%2520space.%2520We%2520show%2520how%250Asimulation-based%2520inference%2520%2528SBI%2529%2520can%2520be%2520used%2520to%2520tackle%2520this%2520problem%2520and%2520to%250Aobtain%2520data-constrained%2520realisations%2520of%2520the%2520primordial%2520dark%2520matter%2520density%250Afield%2520in%2520a%2520simulation-efficient%2520way%2520with%2520general%2520non-differentiable%2520simulators.%250AOur%2520method%2520is%2520applicable%2520to%2520full%2520high-resolution%2520dark%2520matter%2520%2524N%2524-body%250Asimulations%2520and%2520is%2520based%2520on%2520modelling%2520the%2520posterior%2520distribution%2520of%2520the%250Aconstrained%2520initial%2520conditions%2520to%2520be%2520Gaussian%2520with%2520a%2520diagonal%2520covariance%2520matrix%250Ain%2520Fourier%2520space.%2520As%2520a%2520result%252C%2520we%2520can%2520generate%2520thousands%2520of%2520posterior%2520samples%250Awithin%2520seconds%2520on%2520a%2520single%2520GPU%252C%2520orders%2520of%2520magnitude%2520faster%2520than%2520existing%250Amethods%252C%2520paving%2520the%2520way%2520for%2520sequential%2520SBI%2520for%2520cosmological%2520fields.%250AFurthermore%252C%2520we%2520perform%2520an%2520analytical%2520fit%2520of%2520the%2520estimated%2520dependence%2520of%2520the%250Acovariance%2520on%2520the%2520wavenumber%252C%2520effectively%2520transforming%2520any%2520point-estimator%2520of%250Ainitial%2520conditions%2520into%2520a%2520fast%2520sampler.%2520We%2520test%2520the%2520validity%2520of%2520our%2520obtained%250Asamples%2520by%2520comparing%2520them%2520to%2520the%2520true%2520values%2520with%2520summary%2520statistics%2520and%250Aperforming%2520a%2520Bayesian%2520consistency%2520test.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03139v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fast%20Sampling%20of%20Cosmological%20Initial%20Conditions%20with%20Gaussian%20Neural%0A%20%20Posterior%20Estimation&entry.906535625=Oleg%20Savchenko%20and%20Guillermo%20Franco%20Abell%C3%A1n%20and%20Florian%20List%20and%20Noemi%20Anau%20Montel%20and%20Christoph%20Weniger&entry.1292438233=%20%20Knowledge%20of%20the%20primordial%20matter%20density%20field%20from%20which%20the%20large-scale%0Astructure%20of%20the%20Universe%20emerged%20over%20cosmic%20time%20is%20of%20fundamental%20importance%0Afor%20cosmology.%20However%2C%20reconstructing%20these%20cosmological%20initial%20conditions%0Afrom%20late-time%20observations%20is%20a%20notoriously%20difficult%20task%2C%20which%20requires%0Aadvanced%20cosmological%20simulators%20and%20sophisticated%20statistical%20methods%20to%0Aexplore%20a%20multi-million-dimensional%20parameter%20space.%20We%20show%20how%0Asimulation-based%20inference%20%28SBI%29%20can%20be%20used%20to%20tackle%20this%20problem%20and%20to%0Aobtain%20data-constrained%20realisations%20of%20the%20primordial%20dark%20matter%20density%0Afield%20in%20a%20simulation-efficient%20way%20with%20general%20non-differentiable%20simulators.%0AOur%20method%20is%20applicable%20to%20full%20high-resolution%20dark%20matter%20%24N%24-body%0Asimulations%20and%20is%20based%20on%20modelling%20the%20posterior%20distribution%20of%20the%0Aconstrained%20initial%20conditions%20to%20be%20Gaussian%20with%20a%20diagonal%20covariance%20matrix%0Ain%20Fourier%20space.%20As%20a%20result%2C%20we%20can%20generate%20thousands%20of%20posterior%20samples%0Awithin%20seconds%20on%20a%20single%20GPU%2C%20orders%20of%20magnitude%20faster%20than%20existing%0Amethods%2C%20paving%20the%20way%20for%20sequential%20SBI%20for%20cosmological%20fields.%0AFurthermore%2C%20we%20perform%20an%20analytical%20fit%20of%20the%20estimated%20dependence%20of%20the%0Acovariance%20on%20the%20wavenumber%2C%20effectively%20transforming%20any%20point-estimator%20of%0Ainitial%20conditions%20into%20a%20fast%20sampler.%20We%20test%20the%20validity%20of%20our%20obtained%0Asamples%20by%20comparing%20them%20to%20the%20true%20values%20with%20summary%20statistics%20and%0Aperforming%20a%20Bayesian%20consistency%20test.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03139v1&entry.124074799=Read"},
{"title": "TruePose: Human-Parsing-guided Attention Diffusion for Full-ID\n  Preserving Pose Transfer", "author": "Zhihong Xu and Dongxia Wang and Peng Du and Yang Cao and Qing Guo", "abstract": "  Pose-Guided Person Image Synthesis (PGPIS) generates images that maintain a\nsubject's identity from a source image while adopting a specified target pose\n(e.g., skeleton). While diffusion-based PGPIS methods effectively preserve\nfacial features during pose transformation, they often struggle to accurately\nmaintain clothing details from the source image throughout the diffusion\nprocess. This limitation becomes particularly problematic when there is a\nsubstantial difference between the source and target poses, significantly\nimpacting PGPIS applications in the fashion industry where clothing style\npreservation is crucial for copyright protection. Our analysis reveals that\nthis limitation primarily stems from the conditional diffusion model's\nattention modules failing to adequately capture and preserve clothing patterns.\nTo address this limitation, we propose human-parsing-guided attention\ndiffusion, a novel approach that effectively preserves both facial and clothing\nappearance while generating high-quality results. We propose a\nhuman-parsing-aware Siamese network that consists of three key components: dual\nidentical UNets (TargetNet for diffusion denoising and SourceNet for source\nimage embedding extraction), a human-parsing-guided fusion attention (HPFA),\nand a CLIP-guided attention alignment (CAA). The HPFA and CAA modules can embed\nthe face and clothes patterns into the target image generation adaptively and\neffectively. Extensive experiments on both the in-shop clothes retrieval\nbenchmark and the latest in-the-wild human editing dataset demonstrate our\nmethod's significant advantages over 13 baseline approaches for preserving both\nfacial and clothes appearance in the source image.\n", "link": "http://arxiv.org/abs/2502.03426v1", "date": "2025-02-05", "relevancy": 2.6406, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6709}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6594}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TruePose%3A%20Human-Parsing-guided%20Attention%20Diffusion%20for%20Full-ID%0A%20%20Preserving%20Pose%20Transfer&body=Title%3A%20TruePose%3A%20Human-Parsing-guided%20Attention%20Diffusion%20for%20Full-ID%0A%20%20Preserving%20Pose%20Transfer%0AAuthor%3A%20Zhihong%20Xu%20and%20Dongxia%20Wang%20and%20Peng%20Du%20and%20Yang%20Cao%20and%20Qing%20Guo%0AAbstract%3A%20%20%20Pose-Guided%20Person%20Image%20Synthesis%20%28PGPIS%29%20generates%20images%20that%20maintain%20a%0Asubject%27s%20identity%20from%20a%20source%20image%20while%20adopting%20a%20specified%20target%20pose%0A%28e.g.%2C%20skeleton%29.%20While%20diffusion-based%20PGPIS%20methods%20effectively%20preserve%0Afacial%20features%20during%20pose%20transformation%2C%20they%20often%20struggle%20to%20accurately%0Amaintain%20clothing%20details%20from%20the%20source%20image%20throughout%20the%20diffusion%0Aprocess.%20This%20limitation%20becomes%20particularly%20problematic%20when%20there%20is%20a%0Asubstantial%20difference%20between%20the%20source%20and%20target%20poses%2C%20significantly%0Aimpacting%20PGPIS%20applications%20in%20the%20fashion%20industry%20where%20clothing%20style%0Apreservation%20is%20crucial%20for%20copyright%20protection.%20Our%20analysis%20reveals%20that%0Athis%20limitation%20primarily%20stems%20from%20the%20conditional%20diffusion%20model%27s%0Aattention%20modules%20failing%20to%20adequately%20capture%20and%20preserve%20clothing%20patterns.%0ATo%20address%20this%20limitation%2C%20we%20propose%20human-parsing-guided%20attention%0Adiffusion%2C%20a%20novel%20approach%20that%20effectively%20preserves%20both%20facial%20and%20clothing%0Aappearance%20while%20generating%20high-quality%20results.%20We%20propose%20a%0Ahuman-parsing-aware%20Siamese%20network%20that%20consists%20of%20three%20key%20components%3A%20dual%0Aidentical%20UNets%20%28TargetNet%20for%20diffusion%20denoising%20and%20SourceNet%20for%20source%0Aimage%20embedding%20extraction%29%2C%20a%20human-parsing-guided%20fusion%20attention%20%28HPFA%29%2C%0Aand%20a%20CLIP-guided%20attention%20alignment%20%28CAA%29.%20The%20HPFA%20and%20CAA%20modules%20can%20embed%0Athe%20face%20and%20clothes%20patterns%20into%20the%20target%20image%20generation%20adaptively%20and%0Aeffectively.%20Extensive%20experiments%20on%20both%20the%20in-shop%20clothes%20retrieval%0Abenchmark%20and%20the%20latest%20in-the-wild%20human%20editing%20dataset%20demonstrate%20our%0Amethod%27s%20significant%20advantages%20over%2013%20baseline%20approaches%20for%20preserving%20both%0Afacial%20and%20clothes%20appearance%20in%20the%20source%20image.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03426v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTruePose%253A%2520Human-Parsing-guided%2520Attention%2520Diffusion%2520for%2520Full-ID%250A%2520%2520Preserving%2520Pose%2520Transfer%26entry.906535625%3DZhihong%2520Xu%2520and%2520Dongxia%2520Wang%2520and%2520Peng%2520Du%2520and%2520Yang%2520Cao%2520and%2520Qing%2520Guo%26entry.1292438233%3D%2520%2520Pose-Guided%2520Person%2520Image%2520Synthesis%2520%2528PGPIS%2529%2520generates%2520images%2520that%2520maintain%2520a%250Asubject%2527s%2520identity%2520from%2520a%2520source%2520image%2520while%2520adopting%2520a%2520specified%2520target%2520pose%250A%2528e.g.%252C%2520skeleton%2529.%2520While%2520diffusion-based%2520PGPIS%2520methods%2520effectively%2520preserve%250Afacial%2520features%2520during%2520pose%2520transformation%252C%2520they%2520often%2520struggle%2520to%2520accurately%250Amaintain%2520clothing%2520details%2520from%2520the%2520source%2520image%2520throughout%2520the%2520diffusion%250Aprocess.%2520This%2520limitation%2520becomes%2520particularly%2520problematic%2520when%2520there%2520is%2520a%250Asubstantial%2520difference%2520between%2520the%2520source%2520and%2520target%2520poses%252C%2520significantly%250Aimpacting%2520PGPIS%2520applications%2520in%2520the%2520fashion%2520industry%2520where%2520clothing%2520style%250Apreservation%2520is%2520crucial%2520for%2520copyright%2520protection.%2520Our%2520analysis%2520reveals%2520that%250Athis%2520limitation%2520primarily%2520stems%2520from%2520the%2520conditional%2520diffusion%2520model%2527s%250Aattention%2520modules%2520failing%2520to%2520adequately%2520capture%2520and%2520preserve%2520clothing%2520patterns.%250ATo%2520address%2520this%2520limitation%252C%2520we%2520propose%2520human-parsing-guided%2520attention%250Adiffusion%252C%2520a%2520novel%2520approach%2520that%2520effectively%2520preserves%2520both%2520facial%2520and%2520clothing%250Aappearance%2520while%2520generating%2520high-quality%2520results.%2520We%2520propose%2520a%250Ahuman-parsing-aware%2520Siamese%2520network%2520that%2520consists%2520of%2520three%2520key%2520components%253A%2520dual%250Aidentical%2520UNets%2520%2528TargetNet%2520for%2520diffusion%2520denoising%2520and%2520SourceNet%2520for%2520source%250Aimage%2520embedding%2520extraction%2529%252C%2520a%2520human-parsing-guided%2520fusion%2520attention%2520%2528HPFA%2529%252C%250Aand%2520a%2520CLIP-guided%2520attention%2520alignment%2520%2528CAA%2529.%2520The%2520HPFA%2520and%2520CAA%2520modules%2520can%2520embed%250Athe%2520face%2520and%2520clothes%2520patterns%2520into%2520the%2520target%2520image%2520generation%2520adaptively%2520and%250Aeffectively.%2520Extensive%2520experiments%2520on%2520both%2520the%2520in-shop%2520clothes%2520retrieval%250Abenchmark%2520and%2520the%2520latest%2520in-the-wild%2520human%2520editing%2520dataset%2520demonstrate%2520our%250Amethod%2527s%2520significant%2520advantages%2520over%252013%2520baseline%2520approaches%2520for%2520preserving%2520both%250Afacial%2520and%2520clothes%2520appearance%2520in%2520the%2520source%2520image.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03426v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TruePose%3A%20Human-Parsing-guided%20Attention%20Diffusion%20for%20Full-ID%0A%20%20Preserving%20Pose%20Transfer&entry.906535625=Zhihong%20Xu%20and%20Dongxia%20Wang%20and%20Peng%20Du%20and%20Yang%20Cao%20and%20Qing%20Guo&entry.1292438233=%20%20Pose-Guided%20Person%20Image%20Synthesis%20%28PGPIS%29%20generates%20images%20that%20maintain%20a%0Asubject%27s%20identity%20from%20a%20source%20image%20while%20adopting%20a%20specified%20target%20pose%0A%28e.g.%2C%20skeleton%29.%20While%20diffusion-based%20PGPIS%20methods%20effectively%20preserve%0Afacial%20features%20during%20pose%20transformation%2C%20they%20often%20struggle%20to%20accurately%0Amaintain%20clothing%20details%20from%20the%20source%20image%20throughout%20the%20diffusion%0Aprocess.%20This%20limitation%20becomes%20particularly%20problematic%20when%20there%20is%20a%0Asubstantial%20difference%20between%20the%20source%20and%20target%20poses%2C%20significantly%0Aimpacting%20PGPIS%20applications%20in%20the%20fashion%20industry%20where%20clothing%20style%0Apreservation%20is%20crucial%20for%20copyright%20protection.%20Our%20analysis%20reveals%20that%0Athis%20limitation%20primarily%20stems%20from%20the%20conditional%20diffusion%20model%27s%0Aattention%20modules%20failing%20to%20adequately%20capture%20and%20preserve%20clothing%20patterns.%0ATo%20address%20this%20limitation%2C%20we%20propose%20human-parsing-guided%20attention%0Adiffusion%2C%20a%20novel%20approach%20that%20effectively%20preserves%20both%20facial%20and%20clothing%0Aappearance%20while%20generating%20high-quality%20results.%20We%20propose%20a%0Ahuman-parsing-aware%20Siamese%20network%20that%20consists%20of%20three%20key%20components%3A%20dual%0Aidentical%20UNets%20%28TargetNet%20for%20diffusion%20denoising%20and%20SourceNet%20for%20source%0Aimage%20embedding%20extraction%29%2C%20a%20human-parsing-guided%20fusion%20attention%20%28HPFA%29%2C%0Aand%20a%20CLIP-guided%20attention%20alignment%20%28CAA%29.%20The%20HPFA%20and%20CAA%20modules%20can%20embed%0Athe%20face%20and%20clothes%20patterns%20into%20the%20target%20image%20generation%20adaptively%20and%0Aeffectively.%20Extensive%20experiments%20on%20both%20the%20in-shop%20clothes%20retrieval%0Abenchmark%20and%20the%20latest%20in-the-wild%20human%20editing%20dataset%20demonstrate%20our%0Amethod%27s%20significant%20advantages%20over%2013%20baseline%20approaches%20for%20preserving%20both%0Afacial%20and%20clothes%20appearance%20in%20the%20source%20image.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03426v1&entry.124074799=Read"},
{"title": "SPRI: Aligning Large Language Models with Context-Situated Principles", "author": "Hongli Zhan and Muneeza Azmat and Raya Horesh and Junyi Jessy Li and Mikhail Yurochkin", "abstract": "  Aligning Large Language Models to integrate and reflect human values,\nespecially for tasks that demand intricate human oversight, is arduous since it\nis resource-intensive and time-consuming to depend on human expertise for\ncontext-specific guidance. Prior work has utilized predefined sets of rules or\nprinciples to steer the behavior of models (Bai et al., 2022; Sun et al.,\n2023). However, these principles tend to be generic, making it challenging to\nadapt them to each individual input query or context. In this work, we present\nSituated-PRInciples (SPRI), a framework requiring minimal or no human effort\nthat is designed to automatically generate guiding principles in real-time for\neach input query and utilize them to align each response. We evaluate SPRI on\nthree tasks, and show that 1) SPRI can derive principles in a complex\ndomain-specific task that leads to on-par performance as expert-crafted ones;\n2) SPRI-generated principles lead to instance-specific rubrics that outperform\nprior LLM-as-a-judge frameworks; 3) using SPRI to generate synthetic SFT data\nleads to substantial improvement on truthfulness. We release our code and model\ngenerations at https://github.com/honglizhan/SPRI-public.\n", "link": "http://arxiv.org/abs/2502.03397v1", "date": "2025-02-05", "relevancy": 2.6161, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5435}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5435}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4826}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPRI%3A%20Aligning%20Large%20Language%20Models%20with%20Context-Situated%20Principles&body=Title%3A%20SPRI%3A%20Aligning%20Large%20Language%20Models%20with%20Context-Situated%20Principles%0AAuthor%3A%20Hongli%20Zhan%20and%20Muneeza%20Azmat%20and%20Raya%20Horesh%20and%20Junyi%20Jessy%20Li%20and%20Mikhail%20Yurochkin%0AAbstract%3A%20%20%20Aligning%20Large%20Language%20Models%20to%20integrate%20and%20reflect%20human%20values%2C%0Aespecially%20for%20tasks%20that%20demand%20intricate%20human%20oversight%2C%20is%20arduous%20since%20it%0Ais%20resource-intensive%20and%20time-consuming%20to%20depend%20on%20human%20expertise%20for%0Acontext-specific%20guidance.%20Prior%20work%20has%20utilized%20predefined%20sets%20of%20rules%20or%0Aprinciples%20to%20steer%20the%20behavior%20of%20models%20%28Bai%20et%20al.%2C%202022%3B%20Sun%20et%20al.%2C%0A2023%29.%20However%2C%20these%20principles%20tend%20to%20be%20generic%2C%20making%20it%20challenging%20to%0Aadapt%20them%20to%20each%20individual%20input%20query%20or%20context.%20In%20this%20work%2C%20we%20present%0ASituated-PRInciples%20%28SPRI%29%2C%20a%20framework%20requiring%20minimal%20or%20no%20human%20effort%0Athat%20is%20designed%20to%20automatically%20generate%20guiding%20principles%20in%20real-time%20for%0Aeach%20input%20query%20and%20utilize%20them%20to%20align%20each%20response.%20We%20evaluate%20SPRI%20on%0Athree%20tasks%2C%20and%20show%20that%201%29%20SPRI%20can%20derive%20principles%20in%20a%20complex%0Adomain-specific%20task%20that%20leads%20to%20on-par%20performance%20as%20expert-crafted%20ones%3B%0A2%29%20SPRI-generated%20principles%20lead%20to%20instance-specific%20rubrics%20that%20outperform%0Aprior%20LLM-as-a-judge%20frameworks%3B%203%29%20using%20SPRI%20to%20generate%20synthetic%20SFT%20data%0Aleads%20to%20substantial%20improvement%20on%20truthfulness.%20We%20release%20our%20code%20and%20model%0Agenerations%20at%20https%3A//github.com/honglizhan/SPRI-public.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03397v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPRI%253A%2520Aligning%2520Large%2520Language%2520Models%2520with%2520Context-Situated%2520Principles%26entry.906535625%3DHongli%2520Zhan%2520and%2520Muneeza%2520Azmat%2520and%2520Raya%2520Horesh%2520and%2520Junyi%2520Jessy%2520Li%2520and%2520Mikhail%2520Yurochkin%26entry.1292438233%3D%2520%2520Aligning%2520Large%2520Language%2520Models%2520to%2520integrate%2520and%2520reflect%2520human%2520values%252C%250Aespecially%2520for%2520tasks%2520that%2520demand%2520intricate%2520human%2520oversight%252C%2520is%2520arduous%2520since%2520it%250Ais%2520resource-intensive%2520and%2520time-consuming%2520to%2520depend%2520on%2520human%2520expertise%2520for%250Acontext-specific%2520guidance.%2520Prior%2520work%2520has%2520utilized%2520predefined%2520sets%2520of%2520rules%2520or%250Aprinciples%2520to%2520steer%2520the%2520behavior%2520of%2520models%2520%2528Bai%2520et%2520al.%252C%25202022%253B%2520Sun%2520et%2520al.%252C%250A2023%2529.%2520However%252C%2520these%2520principles%2520tend%2520to%2520be%2520generic%252C%2520making%2520it%2520challenging%2520to%250Aadapt%2520them%2520to%2520each%2520individual%2520input%2520query%2520or%2520context.%2520In%2520this%2520work%252C%2520we%2520present%250ASituated-PRInciples%2520%2528SPRI%2529%252C%2520a%2520framework%2520requiring%2520minimal%2520or%2520no%2520human%2520effort%250Athat%2520is%2520designed%2520to%2520automatically%2520generate%2520guiding%2520principles%2520in%2520real-time%2520for%250Aeach%2520input%2520query%2520and%2520utilize%2520them%2520to%2520align%2520each%2520response.%2520We%2520evaluate%2520SPRI%2520on%250Athree%2520tasks%252C%2520and%2520show%2520that%25201%2529%2520SPRI%2520can%2520derive%2520principles%2520in%2520a%2520complex%250Adomain-specific%2520task%2520that%2520leads%2520to%2520on-par%2520performance%2520as%2520expert-crafted%2520ones%253B%250A2%2529%2520SPRI-generated%2520principles%2520lead%2520to%2520instance-specific%2520rubrics%2520that%2520outperform%250Aprior%2520LLM-as-a-judge%2520frameworks%253B%25203%2529%2520using%2520SPRI%2520to%2520generate%2520synthetic%2520SFT%2520data%250Aleads%2520to%2520substantial%2520improvement%2520on%2520truthfulness.%2520We%2520release%2520our%2520code%2520and%2520model%250Agenerations%2520at%2520https%253A//github.com/honglizhan/SPRI-public.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03397v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPRI%3A%20Aligning%20Large%20Language%20Models%20with%20Context-Situated%20Principles&entry.906535625=Hongli%20Zhan%20and%20Muneeza%20Azmat%20and%20Raya%20Horesh%20and%20Junyi%20Jessy%20Li%20and%20Mikhail%20Yurochkin&entry.1292438233=%20%20Aligning%20Large%20Language%20Models%20to%20integrate%20and%20reflect%20human%20values%2C%0Aespecially%20for%20tasks%20that%20demand%20intricate%20human%20oversight%2C%20is%20arduous%20since%20it%0Ais%20resource-intensive%20and%20time-consuming%20to%20depend%20on%20human%20expertise%20for%0Acontext-specific%20guidance.%20Prior%20work%20has%20utilized%20predefined%20sets%20of%20rules%20or%0Aprinciples%20to%20steer%20the%20behavior%20of%20models%20%28Bai%20et%20al.%2C%202022%3B%20Sun%20et%20al.%2C%0A2023%29.%20However%2C%20these%20principles%20tend%20to%20be%20generic%2C%20making%20it%20challenging%20to%0Aadapt%20them%20to%20each%20individual%20input%20query%20or%20context.%20In%20this%20work%2C%20we%20present%0ASituated-PRInciples%20%28SPRI%29%2C%20a%20framework%20requiring%20minimal%20or%20no%20human%20effort%0Athat%20is%20designed%20to%20automatically%20generate%20guiding%20principles%20in%20real-time%20for%0Aeach%20input%20query%20and%20utilize%20them%20to%20align%20each%20response.%20We%20evaluate%20SPRI%20on%0Athree%20tasks%2C%20and%20show%20that%201%29%20SPRI%20can%20derive%20principles%20in%20a%20complex%0Adomain-specific%20task%20that%20leads%20to%20on-par%20performance%20as%20expert-crafted%20ones%3B%0A2%29%20SPRI-generated%20principles%20lead%20to%20instance-specific%20rubrics%20that%20outperform%0Aprior%20LLM-as-a-judge%20frameworks%3B%203%29%20using%20SPRI%20to%20generate%20synthetic%20SFT%20data%0Aleads%20to%20substantial%20improvement%20on%20truthfulness.%20We%20release%20our%20code%20and%20model%0Agenerations%20at%20https%3A//github.com/honglizhan/SPRI-public.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03397v1&entry.124074799=Read"},
{"title": "LIMO: Less is More for Reasoning", "author": "Yixin Ye and Zhen Huang and Yang Xiao and Ethan Chern and Shijie Xia and Pengfei Liu", "abstract": "  We present a fundamental discovery that challenges our understanding of how\ncomplex reasoning emerges in large language models. While conventional wisdom\nsuggests that sophisticated reasoning tasks demand extensive training data\n(>100,000 examples), we demonstrate that complex mathematical reasoning\nabilities can be effectively elicited with surprisingly few examples. Through\ncomprehensive experiments, our proposed model LIMO demonstrates unprecedented\nperformance in mathematical reasoning. With merely 817 curated training\nsamples, LIMO achieves 57.1% accuracy on AIME and 94.8% on MATH, improving from\nprevious SFT-based models' 6.5% and 59.2% respectively, while only using 1% of\nthe training data required by previous approaches. LIMO demonstrates\nexceptional out-of-distribution generalization, achieving 40.5% absolute\nimprovement across 10 diverse benchmarks, outperforming models trained on 100x\nmore data, challenging the notion that SFT leads to memorization rather than\ngeneralization. Based on these results, we propose the Less-Is-More Reasoning\nHypothesis (LIMO Hypothesis): In foundation models where domain knowledge has\nbeen comprehensively encoded during pre-training, sophisticated reasoning\ncapabilities can emerge through minimal but precisely orchestrated\ndemonstrations of cognitive processes. This hypothesis posits that the\nelicitation threshold for complex reasoning is determined by two key factors:\n(1) the completeness of the model's encoded knowledge foundation during\npre-training, and (2) the effectiveness of post-training examples as \"cognitive\ntemplates\" that show the model how to utilize its knowledge base to solve\ncomplex reasoning tasks. To facilitate reproducibility and future research in\ndata-efficient reasoning, we release LIMO as a comprehensive open-source suite\nat https://github.com/GAIR-NLP/LIMO.\n", "link": "http://arxiv.org/abs/2502.03387v1", "date": "2025-02-05", "relevancy": 2.5753, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5246}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5246}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LIMO%3A%20Less%20is%20More%20for%20Reasoning&body=Title%3A%20LIMO%3A%20Less%20is%20More%20for%20Reasoning%0AAuthor%3A%20Yixin%20Ye%20and%20Zhen%20Huang%20and%20Yang%20Xiao%20and%20Ethan%20Chern%20and%20Shijie%20Xia%20and%20Pengfei%20Liu%0AAbstract%3A%20%20%20We%20present%20a%20fundamental%20discovery%20that%20challenges%20our%20understanding%20of%20how%0Acomplex%20reasoning%20emerges%20in%20large%20language%20models.%20While%20conventional%20wisdom%0Asuggests%20that%20sophisticated%20reasoning%20tasks%20demand%20extensive%20training%20data%0A%28%3E100%2C000%20examples%29%2C%20we%20demonstrate%20that%20complex%20mathematical%20reasoning%0Aabilities%20can%20be%20effectively%20elicited%20with%20surprisingly%20few%20examples.%20Through%0Acomprehensive%20experiments%2C%20our%20proposed%20model%20LIMO%20demonstrates%20unprecedented%0Aperformance%20in%20mathematical%20reasoning.%20With%20merely%20817%20curated%20training%0Asamples%2C%20LIMO%20achieves%2057.1%25%20accuracy%20on%20AIME%20and%2094.8%25%20on%20MATH%2C%20improving%20from%0Aprevious%20SFT-based%20models%27%206.5%25%20and%2059.2%25%20respectively%2C%20while%20only%20using%201%25%20of%0Athe%20training%20data%20required%20by%20previous%20approaches.%20LIMO%20demonstrates%0Aexceptional%20out-of-distribution%20generalization%2C%20achieving%2040.5%25%20absolute%0Aimprovement%20across%2010%20diverse%20benchmarks%2C%20outperforming%20models%20trained%20on%20100x%0Amore%20data%2C%20challenging%20the%20notion%20that%20SFT%20leads%20to%20memorization%20rather%20than%0Ageneralization.%20Based%20on%20these%20results%2C%20we%20propose%20the%20Less-Is-More%20Reasoning%0AHypothesis%20%28LIMO%20Hypothesis%29%3A%20In%20foundation%20models%20where%20domain%20knowledge%20has%0Abeen%20comprehensively%20encoded%20during%20pre-training%2C%20sophisticated%20reasoning%0Acapabilities%20can%20emerge%20through%20minimal%20but%20precisely%20orchestrated%0Ademonstrations%20of%20cognitive%20processes.%20This%20hypothesis%20posits%20that%20the%0Aelicitation%20threshold%20for%20complex%20reasoning%20is%20determined%20by%20two%20key%20factors%3A%0A%281%29%20the%20completeness%20of%20the%20model%27s%20encoded%20knowledge%20foundation%20during%0Apre-training%2C%20and%20%282%29%20the%20effectiveness%20of%20post-training%20examples%20as%20%22cognitive%0Atemplates%22%20that%20show%20the%20model%20how%20to%20utilize%20its%20knowledge%20base%20to%20solve%0Acomplex%20reasoning%20tasks.%20To%20facilitate%20reproducibility%20and%20future%20research%20in%0Adata-efficient%20reasoning%2C%20we%20release%20LIMO%20as%20a%20comprehensive%20open-source%20suite%0Aat%20https%3A//github.com/GAIR-NLP/LIMO.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03387v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLIMO%253A%2520Less%2520is%2520More%2520for%2520Reasoning%26entry.906535625%3DYixin%2520Ye%2520and%2520Zhen%2520Huang%2520and%2520Yang%2520Xiao%2520and%2520Ethan%2520Chern%2520and%2520Shijie%2520Xia%2520and%2520Pengfei%2520Liu%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520fundamental%2520discovery%2520that%2520challenges%2520our%2520understanding%2520of%2520how%250Acomplex%2520reasoning%2520emerges%2520in%2520large%2520language%2520models.%2520While%2520conventional%2520wisdom%250Asuggests%2520that%2520sophisticated%2520reasoning%2520tasks%2520demand%2520extensive%2520training%2520data%250A%2528%253E100%252C000%2520examples%2529%252C%2520we%2520demonstrate%2520that%2520complex%2520mathematical%2520reasoning%250Aabilities%2520can%2520be%2520effectively%2520elicited%2520with%2520surprisingly%2520few%2520examples.%2520Through%250Acomprehensive%2520experiments%252C%2520our%2520proposed%2520model%2520LIMO%2520demonstrates%2520unprecedented%250Aperformance%2520in%2520mathematical%2520reasoning.%2520With%2520merely%2520817%2520curated%2520training%250Asamples%252C%2520LIMO%2520achieves%252057.1%2525%2520accuracy%2520on%2520AIME%2520and%252094.8%2525%2520on%2520MATH%252C%2520improving%2520from%250Aprevious%2520SFT-based%2520models%2527%25206.5%2525%2520and%252059.2%2525%2520respectively%252C%2520while%2520only%2520using%25201%2525%2520of%250Athe%2520training%2520data%2520required%2520by%2520previous%2520approaches.%2520LIMO%2520demonstrates%250Aexceptional%2520out-of-distribution%2520generalization%252C%2520achieving%252040.5%2525%2520absolute%250Aimprovement%2520across%252010%2520diverse%2520benchmarks%252C%2520outperforming%2520models%2520trained%2520on%2520100x%250Amore%2520data%252C%2520challenging%2520the%2520notion%2520that%2520SFT%2520leads%2520to%2520memorization%2520rather%2520than%250Ageneralization.%2520Based%2520on%2520these%2520results%252C%2520we%2520propose%2520the%2520Less-Is-More%2520Reasoning%250AHypothesis%2520%2528LIMO%2520Hypothesis%2529%253A%2520In%2520foundation%2520models%2520where%2520domain%2520knowledge%2520has%250Abeen%2520comprehensively%2520encoded%2520during%2520pre-training%252C%2520sophisticated%2520reasoning%250Acapabilities%2520can%2520emerge%2520through%2520minimal%2520but%2520precisely%2520orchestrated%250Ademonstrations%2520of%2520cognitive%2520processes.%2520This%2520hypothesis%2520posits%2520that%2520the%250Aelicitation%2520threshold%2520for%2520complex%2520reasoning%2520is%2520determined%2520by%2520two%2520key%2520factors%253A%250A%25281%2529%2520the%2520completeness%2520of%2520the%2520model%2527s%2520encoded%2520knowledge%2520foundation%2520during%250Apre-training%252C%2520and%2520%25282%2529%2520the%2520effectiveness%2520of%2520post-training%2520examples%2520as%2520%2522cognitive%250Atemplates%2522%2520that%2520show%2520the%2520model%2520how%2520to%2520utilize%2520its%2520knowledge%2520base%2520to%2520solve%250Acomplex%2520reasoning%2520tasks.%2520To%2520facilitate%2520reproducibility%2520and%2520future%2520research%2520in%250Adata-efficient%2520reasoning%252C%2520we%2520release%2520LIMO%2520as%2520a%2520comprehensive%2520open-source%2520suite%250Aat%2520https%253A//github.com/GAIR-NLP/LIMO.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03387v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LIMO%3A%20Less%20is%20More%20for%20Reasoning&entry.906535625=Yixin%20Ye%20and%20Zhen%20Huang%20and%20Yang%20Xiao%20and%20Ethan%20Chern%20and%20Shijie%20Xia%20and%20Pengfei%20Liu&entry.1292438233=%20%20We%20present%20a%20fundamental%20discovery%20that%20challenges%20our%20understanding%20of%20how%0Acomplex%20reasoning%20emerges%20in%20large%20language%20models.%20While%20conventional%20wisdom%0Asuggests%20that%20sophisticated%20reasoning%20tasks%20demand%20extensive%20training%20data%0A%28%3E100%2C000%20examples%29%2C%20we%20demonstrate%20that%20complex%20mathematical%20reasoning%0Aabilities%20can%20be%20effectively%20elicited%20with%20surprisingly%20few%20examples.%20Through%0Acomprehensive%20experiments%2C%20our%20proposed%20model%20LIMO%20demonstrates%20unprecedented%0Aperformance%20in%20mathematical%20reasoning.%20With%20merely%20817%20curated%20training%0Asamples%2C%20LIMO%20achieves%2057.1%25%20accuracy%20on%20AIME%20and%2094.8%25%20on%20MATH%2C%20improving%20from%0Aprevious%20SFT-based%20models%27%206.5%25%20and%2059.2%25%20respectively%2C%20while%20only%20using%201%25%20of%0Athe%20training%20data%20required%20by%20previous%20approaches.%20LIMO%20demonstrates%0Aexceptional%20out-of-distribution%20generalization%2C%20achieving%2040.5%25%20absolute%0Aimprovement%20across%2010%20diverse%20benchmarks%2C%20outperforming%20models%20trained%20on%20100x%0Amore%20data%2C%20challenging%20the%20notion%20that%20SFT%20leads%20to%20memorization%20rather%20than%0Ageneralization.%20Based%20on%20these%20results%2C%20we%20propose%20the%20Less-Is-More%20Reasoning%0AHypothesis%20%28LIMO%20Hypothesis%29%3A%20In%20foundation%20models%20where%20domain%20knowledge%20has%0Abeen%20comprehensively%20encoded%20during%20pre-training%2C%20sophisticated%20reasoning%0Acapabilities%20can%20emerge%20through%20minimal%20but%20precisely%20orchestrated%0Ademonstrations%20of%20cognitive%20processes.%20This%20hypothesis%20posits%20that%20the%0Aelicitation%20threshold%20for%20complex%20reasoning%20is%20determined%20by%20two%20key%20factors%3A%0A%281%29%20the%20completeness%20of%20the%20model%27s%20encoded%20knowledge%20foundation%20during%0Apre-training%2C%20and%20%282%29%20the%20effectiveness%20of%20post-training%20examples%20as%20%22cognitive%0Atemplates%22%20that%20show%20the%20model%20how%20to%20utilize%20its%20knowledge%20base%20to%20solve%0Acomplex%20reasoning%20tasks.%20To%20facilitate%20reproducibility%20and%20future%20research%20in%0Adata-efficient%20reasoning%2C%20we%20release%20LIMO%20as%20a%20comprehensive%20open-source%20suite%0Aat%20https%3A//github.com/GAIR-NLP/LIMO.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03387v1&entry.124074799=Read"},
{"title": "Edge Attention Module for Object Classification", "author": "Santanu Roy and Ashvath Suresh and Archit Gupta", "abstract": "  A novel ``edge attention-based Convolutional Neural Network (CNN)'' is\nproposed in this research for object classification task. With the advent of\nadvanced computing technology, CNN models have achieved to remarkable success,\nparticularly in computer vision applications. Nevertheless, the efficacy of the\nconventional CNN is often hindered due to class imbalance and inter-class\nsimilarity problems, which are particularly prominent in the computer vision\nfield. In this research, we introduce for the first time an ``Edge Attention\nModule (EAM)'' consisting of a Max-Min pooling layer, followed by convolutional\nlayers. This Max-Min pooling is entirely a novel pooling technique,\nspecifically designed to capture only the edge information that is crucial for\nany object classification task. Therefore, by integrating this novel pooling\ntechnique into the attention module, the CNN network inherently prioritizes on\nessential edge features, thereby boosting the accuracy and F1-score of the\nmodel significantly. We have implemented our proposed EAM or 2EAMs on several\nstandard pre-trained CNN models for Caltech-101, Caltech-256, CIFAR-100 and\nTiny ImageNet-200 datasets. The extensive experiments reveal that our proposed\nframework (that is, EAM with CNN and 2EAMs with CNN), outperforms all\npre-trained CNN models as well as recent trend models ``Pooling-based Vision\nTransformer (PiT)'', ``Convolutional Block Attention Module (CBAM)'', and\nConvNext, by substantial margins. We have achieved the accuracy of 95.5% and\n86% by the proposed framework on Caltech-101 and Caltech-256 datasets,\nrespectively. So far, this is the best results on these datasets, to the best\nof our knowledge.\n", "link": "http://arxiv.org/abs/2502.03103v1", "date": "2025-02-05", "relevancy": 2.5714, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5457}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4991}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4981}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Edge%20Attention%20Module%20for%20Object%20Classification&body=Title%3A%20Edge%20Attention%20Module%20for%20Object%20Classification%0AAuthor%3A%20Santanu%20Roy%20and%20Ashvath%20Suresh%20and%20Archit%20Gupta%0AAbstract%3A%20%20%20A%20novel%20%60%60edge%20attention-based%20Convolutional%20Neural%20Network%20%28CNN%29%27%27%20is%0Aproposed%20in%20this%20research%20for%20object%20classification%20task.%20With%20the%20advent%20of%0Aadvanced%20computing%20technology%2C%20CNN%20models%20have%20achieved%20to%20remarkable%20success%2C%0Aparticularly%20in%20computer%20vision%20applications.%20Nevertheless%2C%20the%20efficacy%20of%20the%0Aconventional%20CNN%20is%20often%20hindered%20due%20to%20class%20imbalance%20and%20inter-class%0Asimilarity%20problems%2C%20which%20are%20particularly%20prominent%20in%20the%20computer%20vision%0Afield.%20In%20this%20research%2C%20we%20introduce%20for%20the%20first%20time%20an%20%60%60Edge%20Attention%0AModule%20%28EAM%29%27%27%20consisting%20of%20a%20Max-Min%20pooling%20layer%2C%20followed%20by%20convolutional%0Alayers.%20This%20Max-Min%20pooling%20is%20entirely%20a%20novel%20pooling%20technique%2C%0Aspecifically%20designed%20to%20capture%20only%20the%20edge%20information%20that%20is%20crucial%20for%0Aany%20object%20classification%20task.%20Therefore%2C%20by%20integrating%20this%20novel%20pooling%0Atechnique%20into%20the%20attention%20module%2C%20the%20CNN%20network%20inherently%20prioritizes%20on%0Aessential%20edge%20features%2C%20thereby%20boosting%20the%20accuracy%20and%20F1-score%20of%20the%0Amodel%20significantly.%20We%20have%20implemented%20our%20proposed%20EAM%20or%202EAMs%20on%20several%0Astandard%20pre-trained%20CNN%20models%20for%20Caltech-101%2C%20Caltech-256%2C%20CIFAR-100%20and%0ATiny%20ImageNet-200%20datasets.%20The%20extensive%20experiments%20reveal%20that%20our%20proposed%0Aframework%20%28that%20is%2C%20EAM%20with%20CNN%20and%202EAMs%20with%20CNN%29%2C%20outperforms%20all%0Apre-trained%20CNN%20models%20as%20well%20as%20recent%20trend%20models%20%60%60Pooling-based%20Vision%0ATransformer%20%28PiT%29%27%27%2C%20%60%60Convolutional%20Block%20Attention%20Module%20%28CBAM%29%27%27%2C%20and%0AConvNext%2C%20by%20substantial%20margins.%20We%20have%20achieved%20the%20accuracy%20of%2095.5%25%20and%0A86%25%20by%20the%20proposed%20framework%20on%20Caltech-101%20and%20Caltech-256%20datasets%2C%0Arespectively.%20So%20far%2C%20this%20is%20the%20best%20results%20on%20these%20datasets%2C%20to%20the%20best%0Aof%20our%20knowledge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03103v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdge%2520Attention%2520Module%2520for%2520Object%2520Classification%26entry.906535625%3DSantanu%2520Roy%2520and%2520Ashvath%2520Suresh%2520and%2520Archit%2520Gupta%26entry.1292438233%3D%2520%2520A%2520novel%2520%2560%2560edge%2520attention-based%2520Convolutional%2520Neural%2520Network%2520%2528CNN%2529%2527%2527%2520is%250Aproposed%2520in%2520this%2520research%2520for%2520object%2520classification%2520task.%2520With%2520the%2520advent%2520of%250Aadvanced%2520computing%2520technology%252C%2520CNN%2520models%2520have%2520achieved%2520to%2520remarkable%2520success%252C%250Aparticularly%2520in%2520computer%2520vision%2520applications.%2520Nevertheless%252C%2520the%2520efficacy%2520of%2520the%250Aconventional%2520CNN%2520is%2520often%2520hindered%2520due%2520to%2520class%2520imbalance%2520and%2520inter-class%250Asimilarity%2520problems%252C%2520which%2520are%2520particularly%2520prominent%2520in%2520the%2520computer%2520vision%250Afield.%2520In%2520this%2520research%252C%2520we%2520introduce%2520for%2520the%2520first%2520time%2520an%2520%2560%2560Edge%2520Attention%250AModule%2520%2528EAM%2529%2527%2527%2520consisting%2520of%2520a%2520Max-Min%2520pooling%2520layer%252C%2520followed%2520by%2520convolutional%250Alayers.%2520This%2520Max-Min%2520pooling%2520is%2520entirely%2520a%2520novel%2520pooling%2520technique%252C%250Aspecifically%2520designed%2520to%2520capture%2520only%2520the%2520edge%2520information%2520that%2520is%2520crucial%2520for%250Aany%2520object%2520classification%2520task.%2520Therefore%252C%2520by%2520integrating%2520this%2520novel%2520pooling%250Atechnique%2520into%2520the%2520attention%2520module%252C%2520the%2520CNN%2520network%2520inherently%2520prioritizes%2520on%250Aessential%2520edge%2520features%252C%2520thereby%2520boosting%2520the%2520accuracy%2520and%2520F1-score%2520of%2520the%250Amodel%2520significantly.%2520We%2520have%2520implemented%2520our%2520proposed%2520EAM%2520or%25202EAMs%2520on%2520several%250Astandard%2520pre-trained%2520CNN%2520models%2520for%2520Caltech-101%252C%2520Caltech-256%252C%2520CIFAR-100%2520and%250ATiny%2520ImageNet-200%2520datasets.%2520The%2520extensive%2520experiments%2520reveal%2520that%2520our%2520proposed%250Aframework%2520%2528that%2520is%252C%2520EAM%2520with%2520CNN%2520and%25202EAMs%2520with%2520CNN%2529%252C%2520outperforms%2520all%250Apre-trained%2520CNN%2520models%2520as%2520well%2520as%2520recent%2520trend%2520models%2520%2560%2560Pooling-based%2520Vision%250ATransformer%2520%2528PiT%2529%2527%2527%252C%2520%2560%2560Convolutional%2520Block%2520Attention%2520Module%2520%2528CBAM%2529%2527%2527%252C%2520and%250AConvNext%252C%2520by%2520substantial%2520margins.%2520We%2520have%2520achieved%2520the%2520accuracy%2520of%252095.5%2525%2520and%250A86%2525%2520by%2520the%2520proposed%2520framework%2520on%2520Caltech-101%2520and%2520Caltech-256%2520datasets%252C%250Arespectively.%2520So%2520far%252C%2520this%2520is%2520the%2520best%2520results%2520on%2520these%2520datasets%252C%2520to%2520the%2520best%250Aof%2520our%2520knowledge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03103v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Edge%20Attention%20Module%20for%20Object%20Classification&entry.906535625=Santanu%20Roy%20and%20Ashvath%20Suresh%20and%20Archit%20Gupta&entry.1292438233=%20%20A%20novel%20%60%60edge%20attention-based%20Convolutional%20Neural%20Network%20%28CNN%29%27%27%20is%0Aproposed%20in%20this%20research%20for%20object%20classification%20task.%20With%20the%20advent%20of%0Aadvanced%20computing%20technology%2C%20CNN%20models%20have%20achieved%20to%20remarkable%20success%2C%0Aparticularly%20in%20computer%20vision%20applications.%20Nevertheless%2C%20the%20efficacy%20of%20the%0Aconventional%20CNN%20is%20often%20hindered%20due%20to%20class%20imbalance%20and%20inter-class%0Asimilarity%20problems%2C%20which%20are%20particularly%20prominent%20in%20the%20computer%20vision%0Afield.%20In%20this%20research%2C%20we%20introduce%20for%20the%20first%20time%20an%20%60%60Edge%20Attention%0AModule%20%28EAM%29%27%27%20consisting%20of%20a%20Max-Min%20pooling%20layer%2C%20followed%20by%20convolutional%0Alayers.%20This%20Max-Min%20pooling%20is%20entirely%20a%20novel%20pooling%20technique%2C%0Aspecifically%20designed%20to%20capture%20only%20the%20edge%20information%20that%20is%20crucial%20for%0Aany%20object%20classification%20task.%20Therefore%2C%20by%20integrating%20this%20novel%20pooling%0Atechnique%20into%20the%20attention%20module%2C%20the%20CNN%20network%20inherently%20prioritizes%20on%0Aessential%20edge%20features%2C%20thereby%20boosting%20the%20accuracy%20and%20F1-score%20of%20the%0Amodel%20significantly.%20We%20have%20implemented%20our%20proposed%20EAM%20or%202EAMs%20on%20several%0Astandard%20pre-trained%20CNN%20models%20for%20Caltech-101%2C%20Caltech-256%2C%20CIFAR-100%20and%0ATiny%20ImageNet-200%20datasets.%20The%20extensive%20experiments%20reveal%20that%20our%20proposed%0Aframework%20%28that%20is%2C%20EAM%20with%20CNN%20and%202EAMs%20with%20CNN%29%2C%20outperforms%20all%0Apre-trained%20CNN%20models%20as%20well%20as%20recent%20trend%20models%20%60%60Pooling-based%20Vision%0ATransformer%20%28PiT%29%27%27%2C%20%60%60Convolutional%20Block%20Attention%20Module%20%28CBAM%29%27%27%2C%20and%0AConvNext%2C%20by%20substantial%20margins.%20We%20have%20achieved%20the%20accuracy%20of%2095.5%25%20and%0A86%25%20by%20the%20proposed%20framework%20on%20Caltech-101%20and%20Caltech-256%20datasets%2C%0Arespectively.%20So%20far%2C%20this%20is%20the%20best%20results%20on%20these%20datasets%2C%20to%20the%20best%0Aof%20our%20knowledge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03103v1&entry.124074799=Read"},
{"title": "DualFed: Enjoying both Generalization and Personalization in Federated\n  Learning via Hierachical Representations", "author": "Guogang Zhu and Xuefeng Liu and Jianwei Niu and Shaojie Tang and Xinghao Wu and Jiayuan Zhang", "abstract": "  In personalized federated learning (PFL), it is widely recognized that\nachieving both high model generalization and effective personalization poses a\nsignificant challenge due to their conflicting nature. As a result, existing\nPFL methods can only manage a trade-off between these two objectives. This\nraises an interesting question: Is it feasible to develop a model capable of\nachieving both objectives simultaneously? Our paper presents an affirmative\nanswer, and the key lies in the observation that deep models inherently exhibit\nhierarchical architectures, which produce representations with various levels\nof generalization and personalization at different stages. A straightforward\napproach stemming from this observation is to select multiple representations\nfrom these layers and combine them to concurrently achieve generalization and\npersonalization. However, the number of candidate representations is commonly\nhuge, which makes this method infeasible due to high computational costs.To\naddress this problem, we propose DualFed, a new method that can directly yield\ndual representations correspond to generalization and personalization\nrespectively, thereby simplifying the optimization task. Specifically, DualFed\ninserts a personalized projection network between the encoder and classifier.\nThe pre-projection representations are able to capture generalized information\nshareable across clients, and the post-projection representations are effective\nto capture task-specific information on local clients. This design minimizes\nthe mutual interference between generalization and personalization, thereby\nachieving a win-win situation. Extensive experiments show that DualFed can\noutperform other FL methods. Code is available at\nhttps://github.com/GuogangZhu/DualFed.\n", "link": "http://arxiv.org/abs/2407.17754v2", "date": "2025-02-05", "relevancy": 2.5504, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5222}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.51}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DualFed%3A%20Enjoying%20both%20Generalization%20and%20Personalization%20in%20Federated%0A%20%20Learning%20via%20Hierachical%20Representations&body=Title%3A%20DualFed%3A%20Enjoying%20both%20Generalization%20and%20Personalization%20in%20Federated%0A%20%20Learning%20via%20Hierachical%20Representations%0AAuthor%3A%20Guogang%20Zhu%20and%20Xuefeng%20Liu%20and%20Jianwei%20Niu%20and%20Shaojie%20Tang%20and%20Xinghao%20Wu%20and%20Jiayuan%20Zhang%0AAbstract%3A%20%20%20In%20personalized%20federated%20learning%20%28PFL%29%2C%20it%20is%20widely%20recognized%20that%0Aachieving%20both%20high%20model%20generalization%20and%20effective%20personalization%20poses%20a%0Asignificant%20challenge%20due%20to%20their%20conflicting%20nature.%20As%20a%20result%2C%20existing%0APFL%20methods%20can%20only%20manage%20a%20trade-off%20between%20these%20two%20objectives.%20This%0Araises%20an%20interesting%20question%3A%20Is%20it%20feasible%20to%20develop%20a%20model%20capable%20of%0Aachieving%20both%20objectives%20simultaneously%3F%20Our%20paper%20presents%20an%20affirmative%0Aanswer%2C%20and%20the%20key%20lies%20in%20the%20observation%20that%20deep%20models%20inherently%20exhibit%0Ahierarchical%20architectures%2C%20which%20produce%20representations%20with%20various%20levels%0Aof%20generalization%20and%20personalization%20at%20different%20stages.%20A%20straightforward%0Aapproach%20stemming%20from%20this%20observation%20is%20to%20select%20multiple%20representations%0Afrom%20these%20layers%20and%20combine%20them%20to%20concurrently%20achieve%20generalization%20and%0Apersonalization.%20However%2C%20the%20number%20of%20candidate%20representations%20is%20commonly%0Ahuge%2C%20which%20makes%20this%20method%20infeasible%20due%20to%20high%20computational%20costs.To%0Aaddress%20this%20problem%2C%20we%20propose%20DualFed%2C%20a%20new%20method%20that%20can%20directly%20yield%0Adual%20representations%20correspond%20to%20generalization%20and%20personalization%0Arespectively%2C%20thereby%20simplifying%20the%20optimization%20task.%20Specifically%2C%20DualFed%0Ainserts%20a%20personalized%20projection%20network%20between%20the%20encoder%20and%20classifier.%0AThe%20pre-projection%20representations%20are%20able%20to%20capture%20generalized%20information%0Ashareable%20across%20clients%2C%20and%20the%20post-projection%20representations%20are%20effective%0Ato%20capture%20task-specific%20information%20on%20local%20clients.%20This%20design%20minimizes%0Athe%20mutual%20interference%20between%20generalization%20and%20personalization%2C%20thereby%0Aachieving%20a%20win-win%20situation.%20Extensive%20experiments%20show%20that%20DualFed%20can%0Aoutperform%20other%20FL%20methods.%20Code%20is%20available%20at%0Ahttps%3A//github.com/GuogangZhu/DualFed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.17754v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDualFed%253A%2520Enjoying%2520both%2520Generalization%2520and%2520Personalization%2520in%2520Federated%250A%2520%2520Learning%2520via%2520Hierachical%2520Representations%26entry.906535625%3DGuogang%2520Zhu%2520and%2520Xuefeng%2520Liu%2520and%2520Jianwei%2520Niu%2520and%2520Shaojie%2520Tang%2520and%2520Xinghao%2520Wu%2520and%2520Jiayuan%2520Zhang%26entry.1292438233%3D%2520%2520In%2520personalized%2520federated%2520learning%2520%2528PFL%2529%252C%2520it%2520is%2520widely%2520recognized%2520that%250Aachieving%2520both%2520high%2520model%2520generalization%2520and%2520effective%2520personalization%2520poses%2520a%250Asignificant%2520challenge%2520due%2520to%2520their%2520conflicting%2520nature.%2520As%2520a%2520result%252C%2520existing%250APFL%2520methods%2520can%2520only%2520manage%2520a%2520trade-off%2520between%2520these%2520two%2520objectives.%2520This%250Araises%2520an%2520interesting%2520question%253A%2520Is%2520it%2520feasible%2520to%2520develop%2520a%2520model%2520capable%2520of%250Aachieving%2520both%2520objectives%2520simultaneously%253F%2520Our%2520paper%2520presents%2520an%2520affirmative%250Aanswer%252C%2520and%2520the%2520key%2520lies%2520in%2520the%2520observation%2520that%2520deep%2520models%2520inherently%2520exhibit%250Ahierarchical%2520architectures%252C%2520which%2520produce%2520representations%2520with%2520various%2520levels%250Aof%2520generalization%2520and%2520personalization%2520at%2520different%2520stages.%2520A%2520straightforward%250Aapproach%2520stemming%2520from%2520this%2520observation%2520is%2520to%2520select%2520multiple%2520representations%250Afrom%2520these%2520layers%2520and%2520combine%2520them%2520to%2520concurrently%2520achieve%2520generalization%2520and%250Apersonalization.%2520However%252C%2520the%2520number%2520of%2520candidate%2520representations%2520is%2520commonly%250Ahuge%252C%2520which%2520makes%2520this%2520method%2520infeasible%2520due%2520to%2520high%2520computational%2520costs.To%250Aaddress%2520this%2520problem%252C%2520we%2520propose%2520DualFed%252C%2520a%2520new%2520method%2520that%2520can%2520directly%2520yield%250Adual%2520representations%2520correspond%2520to%2520generalization%2520and%2520personalization%250Arespectively%252C%2520thereby%2520simplifying%2520the%2520optimization%2520task.%2520Specifically%252C%2520DualFed%250Ainserts%2520a%2520personalized%2520projection%2520network%2520between%2520the%2520encoder%2520and%2520classifier.%250AThe%2520pre-projection%2520representations%2520are%2520able%2520to%2520capture%2520generalized%2520information%250Ashareable%2520across%2520clients%252C%2520and%2520the%2520post-projection%2520representations%2520are%2520effective%250Ato%2520capture%2520task-specific%2520information%2520on%2520local%2520clients.%2520This%2520design%2520minimizes%250Athe%2520mutual%2520interference%2520between%2520generalization%2520and%2520personalization%252C%2520thereby%250Aachieving%2520a%2520win-win%2520situation.%2520Extensive%2520experiments%2520show%2520that%2520DualFed%2520can%250Aoutperform%2520other%2520FL%2520methods.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/GuogangZhu/DualFed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.17754v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DualFed%3A%20Enjoying%20both%20Generalization%20and%20Personalization%20in%20Federated%0A%20%20Learning%20via%20Hierachical%20Representations&entry.906535625=Guogang%20Zhu%20and%20Xuefeng%20Liu%20and%20Jianwei%20Niu%20and%20Shaojie%20Tang%20and%20Xinghao%20Wu%20and%20Jiayuan%20Zhang&entry.1292438233=%20%20In%20personalized%20federated%20learning%20%28PFL%29%2C%20it%20is%20widely%20recognized%20that%0Aachieving%20both%20high%20model%20generalization%20and%20effective%20personalization%20poses%20a%0Asignificant%20challenge%20due%20to%20their%20conflicting%20nature.%20As%20a%20result%2C%20existing%0APFL%20methods%20can%20only%20manage%20a%20trade-off%20between%20these%20two%20objectives.%20This%0Araises%20an%20interesting%20question%3A%20Is%20it%20feasible%20to%20develop%20a%20model%20capable%20of%0Aachieving%20both%20objectives%20simultaneously%3F%20Our%20paper%20presents%20an%20affirmative%0Aanswer%2C%20and%20the%20key%20lies%20in%20the%20observation%20that%20deep%20models%20inherently%20exhibit%0Ahierarchical%20architectures%2C%20which%20produce%20representations%20with%20various%20levels%0Aof%20generalization%20and%20personalization%20at%20different%20stages.%20A%20straightforward%0Aapproach%20stemming%20from%20this%20observation%20is%20to%20select%20multiple%20representations%0Afrom%20these%20layers%20and%20combine%20them%20to%20concurrently%20achieve%20generalization%20and%0Apersonalization.%20However%2C%20the%20number%20of%20candidate%20representations%20is%20commonly%0Ahuge%2C%20which%20makes%20this%20method%20infeasible%20due%20to%20high%20computational%20costs.To%0Aaddress%20this%20problem%2C%20we%20propose%20DualFed%2C%20a%20new%20method%20that%20can%20directly%20yield%0Adual%20representations%20correspond%20to%20generalization%20and%20personalization%0Arespectively%2C%20thereby%20simplifying%20the%20optimization%20task.%20Specifically%2C%20DualFed%0Ainserts%20a%20personalized%20projection%20network%20between%20the%20encoder%20and%20classifier.%0AThe%20pre-projection%20representations%20are%20able%20to%20capture%20generalized%20information%0Ashareable%20across%20clients%2C%20and%20the%20post-projection%20representations%20are%20effective%0Ato%20capture%20task-specific%20information%20on%20local%20clients.%20This%20design%20minimizes%0Athe%20mutual%20interference%20between%20generalization%20and%20personalization%2C%20thereby%0Aachieving%20a%20win-win%20situation.%20Extensive%20experiments%20show%20that%20DualFed%20can%0Aoutperform%20other%20FL%20methods.%20Code%20is%20available%20at%0Ahttps%3A//github.com/GuogangZhu/DualFed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.17754v2&entry.124074799=Read"},
{"title": "SpaceGNN: Multi-Space Graph Neural Network for Node Anomaly Detection\n  with Extremely Limited Labels", "author": "Xiangyu Dong and Xingyi Zhang and Lei Chen and Mingxuan Yuan and Sibo Wang", "abstract": "  Node Anomaly Detection (NAD) has gained significant attention in the deep\nlearning community due to its diverse applications in real-world scenarios.\nExisting NAD methods primarily embed graphs within a single Euclidean space,\nwhile overlooking the potential of non-Euclidean spaces. Besides, to address\nthe prevalent issue of limited supervision in real NAD tasks, previous methods\ntend to leverage synthetic data to collect auxiliary information, which is not\nan effective solution as shown in our experiments. To overcome these\nchallenges, we introduce a novel SpaceGNN model designed for NAD tasks with\nextremely limited labels. Specifically, we provide deeper insights into a\ntask-relevant framework by empirically analyzing the benefits of different\nspaces for node representations, based on which, we design a Learnable Space\nProjection function that effectively encodes nodes into suitable spaces.\nBesides, we introduce the concept of weighted homogeneity, which we empirically\nand theoretically validate as an effective coefficient during information\npropagation. This concept inspires the design of the Distance Aware Propagation\nmodule. Furthermore, we propose the Multiple Space Ensemble module, which\nextracts comprehensive information for NAD under conditions of extremely\nlimited supervision. Our findings indicate that this module is more beneficial\nthan data augmentation techniques for NAD. Extensive experiments conducted on 9\nreal datasets confirm the superiority of SpaceGNN, which outperforms the best\nrival by an average of 8.55% in AUC and 4.31% in F1 scores. Our code is\navailable at https://github.com/xydong127/SpaceGNN.\n", "link": "http://arxiv.org/abs/2502.03201v1", "date": "2025-02-05", "relevancy": 2.5499, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5193}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5074}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5033}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpaceGNN%3A%20Multi-Space%20Graph%20Neural%20Network%20for%20Node%20Anomaly%20Detection%0A%20%20with%20Extremely%20Limited%20Labels&body=Title%3A%20SpaceGNN%3A%20Multi-Space%20Graph%20Neural%20Network%20for%20Node%20Anomaly%20Detection%0A%20%20with%20Extremely%20Limited%20Labels%0AAuthor%3A%20Xiangyu%20Dong%20and%20Xingyi%20Zhang%20and%20Lei%20Chen%20and%20Mingxuan%20Yuan%20and%20Sibo%20Wang%0AAbstract%3A%20%20%20Node%20Anomaly%20Detection%20%28NAD%29%20has%20gained%20significant%20attention%20in%20the%20deep%0Alearning%20community%20due%20to%20its%20diverse%20applications%20in%20real-world%20scenarios.%0AExisting%20NAD%20methods%20primarily%20embed%20graphs%20within%20a%20single%20Euclidean%20space%2C%0Awhile%20overlooking%20the%20potential%20of%20non-Euclidean%20spaces.%20Besides%2C%20to%20address%0Athe%20prevalent%20issue%20of%20limited%20supervision%20in%20real%20NAD%20tasks%2C%20previous%20methods%0Atend%20to%20leverage%20synthetic%20data%20to%20collect%20auxiliary%20information%2C%20which%20is%20not%0Aan%20effective%20solution%20as%20shown%20in%20our%20experiments.%20To%20overcome%20these%0Achallenges%2C%20we%20introduce%20a%20novel%20SpaceGNN%20model%20designed%20for%20NAD%20tasks%20with%0Aextremely%20limited%20labels.%20Specifically%2C%20we%20provide%20deeper%20insights%20into%20a%0Atask-relevant%20framework%20by%20empirically%20analyzing%20the%20benefits%20of%20different%0Aspaces%20for%20node%20representations%2C%20based%20on%20which%2C%20we%20design%20a%20Learnable%20Space%0AProjection%20function%20that%20effectively%20encodes%20nodes%20into%20suitable%20spaces.%0ABesides%2C%20we%20introduce%20the%20concept%20of%20weighted%20homogeneity%2C%20which%20we%20empirically%0Aand%20theoretically%20validate%20as%20an%20effective%20coefficient%20during%20information%0Apropagation.%20This%20concept%20inspires%20the%20design%20of%20the%20Distance%20Aware%20Propagation%0Amodule.%20Furthermore%2C%20we%20propose%20the%20Multiple%20Space%20Ensemble%20module%2C%20which%0Aextracts%20comprehensive%20information%20for%20NAD%20under%20conditions%20of%20extremely%0Alimited%20supervision.%20Our%20findings%20indicate%20that%20this%20module%20is%20more%20beneficial%0Athan%20data%20augmentation%20techniques%20for%20NAD.%20Extensive%20experiments%20conducted%20on%209%0Areal%20datasets%20confirm%20the%20superiority%20of%20SpaceGNN%2C%20which%20outperforms%20the%20best%0Arival%20by%20an%20average%20of%208.55%25%20in%20AUC%20and%204.31%25%20in%20F1%20scores.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/xydong127/SpaceGNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03201v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpaceGNN%253A%2520Multi-Space%2520Graph%2520Neural%2520Network%2520for%2520Node%2520Anomaly%2520Detection%250A%2520%2520with%2520Extremely%2520Limited%2520Labels%26entry.906535625%3DXiangyu%2520Dong%2520and%2520Xingyi%2520Zhang%2520and%2520Lei%2520Chen%2520and%2520Mingxuan%2520Yuan%2520and%2520Sibo%2520Wang%26entry.1292438233%3D%2520%2520Node%2520Anomaly%2520Detection%2520%2528NAD%2529%2520has%2520gained%2520significant%2520attention%2520in%2520the%2520deep%250Alearning%2520community%2520due%2520to%2520its%2520diverse%2520applications%2520in%2520real-world%2520scenarios.%250AExisting%2520NAD%2520methods%2520primarily%2520embed%2520graphs%2520within%2520a%2520single%2520Euclidean%2520space%252C%250Awhile%2520overlooking%2520the%2520potential%2520of%2520non-Euclidean%2520spaces.%2520Besides%252C%2520to%2520address%250Athe%2520prevalent%2520issue%2520of%2520limited%2520supervision%2520in%2520real%2520NAD%2520tasks%252C%2520previous%2520methods%250Atend%2520to%2520leverage%2520synthetic%2520data%2520to%2520collect%2520auxiliary%2520information%252C%2520which%2520is%2520not%250Aan%2520effective%2520solution%2520as%2520shown%2520in%2520our%2520experiments.%2520To%2520overcome%2520these%250Achallenges%252C%2520we%2520introduce%2520a%2520novel%2520SpaceGNN%2520model%2520designed%2520for%2520NAD%2520tasks%2520with%250Aextremely%2520limited%2520labels.%2520Specifically%252C%2520we%2520provide%2520deeper%2520insights%2520into%2520a%250Atask-relevant%2520framework%2520by%2520empirically%2520analyzing%2520the%2520benefits%2520of%2520different%250Aspaces%2520for%2520node%2520representations%252C%2520based%2520on%2520which%252C%2520we%2520design%2520a%2520Learnable%2520Space%250AProjection%2520function%2520that%2520effectively%2520encodes%2520nodes%2520into%2520suitable%2520spaces.%250ABesides%252C%2520we%2520introduce%2520the%2520concept%2520of%2520weighted%2520homogeneity%252C%2520which%2520we%2520empirically%250Aand%2520theoretically%2520validate%2520as%2520an%2520effective%2520coefficient%2520during%2520information%250Apropagation.%2520This%2520concept%2520inspires%2520the%2520design%2520of%2520the%2520Distance%2520Aware%2520Propagation%250Amodule.%2520Furthermore%252C%2520we%2520propose%2520the%2520Multiple%2520Space%2520Ensemble%2520module%252C%2520which%250Aextracts%2520comprehensive%2520information%2520for%2520NAD%2520under%2520conditions%2520of%2520extremely%250Alimited%2520supervision.%2520Our%2520findings%2520indicate%2520that%2520this%2520module%2520is%2520more%2520beneficial%250Athan%2520data%2520augmentation%2520techniques%2520for%2520NAD.%2520Extensive%2520experiments%2520conducted%2520on%25209%250Areal%2520datasets%2520confirm%2520the%2520superiority%2520of%2520SpaceGNN%252C%2520which%2520outperforms%2520the%2520best%250Arival%2520by%2520an%2520average%2520of%25208.55%2525%2520in%2520AUC%2520and%25204.31%2525%2520in%2520F1%2520scores.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/xydong127/SpaceGNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03201v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpaceGNN%3A%20Multi-Space%20Graph%20Neural%20Network%20for%20Node%20Anomaly%20Detection%0A%20%20with%20Extremely%20Limited%20Labels&entry.906535625=Xiangyu%20Dong%20and%20Xingyi%20Zhang%20and%20Lei%20Chen%20and%20Mingxuan%20Yuan%20and%20Sibo%20Wang&entry.1292438233=%20%20Node%20Anomaly%20Detection%20%28NAD%29%20has%20gained%20significant%20attention%20in%20the%20deep%0Alearning%20community%20due%20to%20its%20diverse%20applications%20in%20real-world%20scenarios.%0AExisting%20NAD%20methods%20primarily%20embed%20graphs%20within%20a%20single%20Euclidean%20space%2C%0Awhile%20overlooking%20the%20potential%20of%20non-Euclidean%20spaces.%20Besides%2C%20to%20address%0Athe%20prevalent%20issue%20of%20limited%20supervision%20in%20real%20NAD%20tasks%2C%20previous%20methods%0Atend%20to%20leverage%20synthetic%20data%20to%20collect%20auxiliary%20information%2C%20which%20is%20not%0Aan%20effective%20solution%20as%20shown%20in%20our%20experiments.%20To%20overcome%20these%0Achallenges%2C%20we%20introduce%20a%20novel%20SpaceGNN%20model%20designed%20for%20NAD%20tasks%20with%0Aextremely%20limited%20labels.%20Specifically%2C%20we%20provide%20deeper%20insights%20into%20a%0Atask-relevant%20framework%20by%20empirically%20analyzing%20the%20benefits%20of%20different%0Aspaces%20for%20node%20representations%2C%20based%20on%20which%2C%20we%20design%20a%20Learnable%20Space%0AProjection%20function%20that%20effectively%20encodes%20nodes%20into%20suitable%20spaces.%0ABesides%2C%20we%20introduce%20the%20concept%20of%20weighted%20homogeneity%2C%20which%20we%20empirically%0Aand%20theoretically%20validate%20as%20an%20effective%20coefficient%20during%20information%0Apropagation.%20This%20concept%20inspires%20the%20design%20of%20the%20Distance%20Aware%20Propagation%0Amodule.%20Furthermore%2C%20we%20propose%20the%20Multiple%20Space%20Ensemble%20module%2C%20which%0Aextracts%20comprehensive%20information%20for%20NAD%20under%20conditions%20of%20extremely%0Alimited%20supervision.%20Our%20findings%20indicate%20that%20this%20module%20is%20more%20beneficial%0Athan%20data%20augmentation%20techniques%20for%20NAD.%20Extensive%20experiments%20conducted%20on%209%0Areal%20datasets%20confirm%20the%20superiority%20of%20SpaceGNN%2C%20which%20outperforms%20the%20best%0Arival%20by%20an%20average%20of%208.55%25%20in%20AUC%20and%204.31%25%20in%20F1%20scores.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/xydong127/SpaceGNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03201v1&entry.124074799=Read"},
{"title": "Optimal Task Order for Continual Learning of Multiple Tasks", "author": "Ziyan Li and Naoki Hiratani", "abstract": "  Continual learning of multiple tasks remains a major challenge for neural\nnetworks. Here, we investigate how task order influences continual learning and\npropose a strategy for optimizing it. Leveraging a linear teacher-student model\nwith latent factors, we derive an analytical expression relating task\nsimilarity and ordering to learning performance. Our analysis reveals two\nprinciples that hold under a wide parameter range: (1) tasks should be arranged\nfrom the least representative to the most typical, and (2) adjacent tasks\nshould be dissimilar. We validate these rules on both synthetic data and\nreal-world image classification datasets (Fashion-MNIST, CIFAR-10, CIFAR-100),\ndemonstrating consistent performance improvements in both multilayer\nperceptrons and convolutional neural networks. Our work thus presents a\ngeneralizable framework for task-order optimization in task-incremental\ncontinual learning.\n", "link": "http://arxiv.org/abs/2502.03350v1", "date": "2025-02-05", "relevancy": 2.5485, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5236}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.506}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Task%20Order%20for%20Continual%20Learning%20of%20Multiple%20Tasks&body=Title%3A%20Optimal%20Task%20Order%20for%20Continual%20Learning%20of%20Multiple%20Tasks%0AAuthor%3A%20Ziyan%20Li%20and%20Naoki%20Hiratani%0AAbstract%3A%20%20%20Continual%20learning%20of%20multiple%20tasks%20remains%20a%20major%20challenge%20for%20neural%0Anetworks.%20Here%2C%20we%20investigate%20how%20task%20order%20influences%20continual%20learning%20and%0Apropose%20a%20strategy%20for%20optimizing%20it.%20Leveraging%20a%20linear%20teacher-student%20model%0Awith%20latent%20factors%2C%20we%20derive%20an%20analytical%20expression%20relating%20task%0Asimilarity%20and%20ordering%20to%20learning%20performance.%20Our%20analysis%20reveals%20two%0Aprinciples%20that%20hold%20under%20a%20wide%20parameter%20range%3A%20%281%29%20tasks%20should%20be%20arranged%0Afrom%20the%20least%20representative%20to%20the%20most%20typical%2C%20and%20%282%29%20adjacent%20tasks%0Ashould%20be%20dissimilar.%20We%20validate%20these%20rules%20on%20both%20synthetic%20data%20and%0Areal-world%20image%20classification%20datasets%20%28Fashion-MNIST%2C%20CIFAR-10%2C%20CIFAR-100%29%2C%0Ademonstrating%20consistent%20performance%20improvements%20in%20both%20multilayer%0Aperceptrons%20and%20convolutional%20neural%20networks.%20Our%20work%20thus%20presents%20a%0Ageneralizable%20framework%20for%20task-order%20optimization%20in%20task-incremental%0Acontinual%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03350v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Task%2520Order%2520for%2520Continual%2520Learning%2520of%2520Multiple%2520Tasks%26entry.906535625%3DZiyan%2520Li%2520and%2520Naoki%2520Hiratani%26entry.1292438233%3D%2520%2520Continual%2520learning%2520of%2520multiple%2520tasks%2520remains%2520a%2520major%2520challenge%2520for%2520neural%250Anetworks.%2520Here%252C%2520we%2520investigate%2520how%2520task%2520order%2520influences%2520continual%2520learning%2520and%250Apropose%2520a%2520strategy%2520for%2520optimizing%2520it.%2520Leveraging%2520a%2520linear%2520teacher-student%2520model%250Awith%2520latent%2520factors%252C%2520we%2520derive%2520an%2520analytical%2520expression%2520relating%2520task%250Asimilarity%2520and%2520ordering%2520to%2520learning%2520performance.%2520Our%2520analysis%2520reveals%2520two%250Aprinciples%2520that%2520hold%2520under%2520a%2520wide%2520parameter%2520range%253A%2520%25281%2529%2520tasks%2520should%2520be%2520arranged%250Afrom%2520the%2520least%2520representative%2520to%2520the%2520most%2520typical%252C%2520and%2520%25282%2529%2520adjacent%2520tasks%250Ashould%2520be%2520dissimilar.%2520We%2520validate%2520these%2520rules%2520on%2520both%2520synthetic%2520data%2520and%250Areal-world%2520image%2520classification%2520datasets%2520%2528Fashion-MNIST%252C%2520CIFAR-10%252C%2520CIFAR-100%2529%252C%250Ademonstrating%2520consistent%2520performance%2520improvements%2520in%2520both%2520multilayer%250Aperceptrons%2520and%2520convolutional%2520neural%2520networks.%2520Our%2520work%2520thus%2520presents%2520a%250Ageneralizable%2520framework%2520for%2520task-order%2520optimization%2520in%2520task-incremental%250Acontinual%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03350v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Task%20Order%20for%20Continual%20Learning%20of%20Multiple%20Tasks&entry.906535625=Ziyan%20Li%20and%20Naoki%20Hiratani&entry.1292438233=%20%20Continual%20learning%20of%20multiple%20tasks%20remains%20a%20major%20challenge%20for%20neural%0Anetworks.%20Here%2C%20we%20investigate%20how%20task%20order%20influences%20continual%20learning%20and%0Apropose%20a%20strategy%20for%20optimizing%20it.%20Leveraging%20a%20linear%20teacher-student%20model%0Awith%20latent%20factors%2C%20we%20derive%20an%20analytical%20expression%20relating%20task%0Asimilarity%20and%20ordering%20to%20learning%20performance.%20Our%20analysis%20reveals%20two%0Aprinciples%20that%20hold%20under%20a%20wide%20parameter%20range%3A%20%281%29%20tasks%20should%20be%20arranged%0Afrom%20the%20least%20representative%20to%20the%20most%20typical%2C%20and%20%282%29%20adjacent%20tasks%0Ashould%20be%20dissimilar.%20We%20validate%20these%20rules%20on%20both%20synthetic%20data%20and%0Areal-world%20image%20classification%20datasets%20%28Fashion-MNIST%2C%20CIFAR-10%2C%20CIFAR-100%29%2C%0Ademonstrating%20consistent%20performance%20improvements%20in%20both%20multilayer%0Aperceptrons%20and%20convolutional%20neural%20networks.%20Our%20work%20thus%20presents%20a%0Ageneralizable%20framework%20for%20task-order%20optimization%20in%20task-incremental%0Acontinual%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03350v1&entry.124074799=Read"},
{"title": "Seeing World Dynamics in a Nutshell", "author": "Qiuhong Shen and Xuanyu Yi and Mingbao Lin and Hanwang Zhang and Shuicheng Yan and Xinchao Wang", "abstract": "  We consider the problem of efficiently representing casually captured\nmonocular videos in a spatially- and temporally-coherent manner. While existing\napproaches predominantly rely on 2D/2.5D techniques treating videos as\ncollections of spatiotemporal pixels, they struggle with complex motions,\nocclusions, and geometric consistency due to absence of temporal coherence and\nexplicit 3D structure. Drawing inspiration from monocular video as a projection\nof the dynamic 3D world, we explore representing videos in their intrinsic 3D\nform through continuous flows of Gaussian primitives in space-time. In this\npaper, we propose NutWorld, a novel framework that efficiently transforms\nmonocular videos into dynamic 3D Gaussian representations in a single forward\npass. At its core, NutWorld introduces a structured spatial-temporal aligned\nGaussian (STAG) representation, enabling optimization-free scene modeling with\neffective depth and flow regularization. Through comprehensive experiments, we\ndemonstrate that NutWorld achieves high-fidelity video reconstruction quality\nwhile enabling various downstream applications in real-time. Demos and code\nwill be available at https://github.com/Nut-World/NutWorld.\n", "link": "http://arxiv.org/abs/2502.03465v1", "date": "2025-02-05", "relevancy": 2.5483, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6814}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6146}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6017}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Seeing%20World%20Dynamics%20in%20a%20Nutshell&body=Title%3A%20Seeing%20World%20Dynamics%20in%20a%20Nutshell%0AAuthor%3A%20Qiuhong%20Shen%20and%20Xuanyu%20Yi%20and%20Mingbao%20Lin%20and%20Hanwang%20Zhang%20and%20Shuicheng%20Yan%20and%20Xinchao%20Wang%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20efficiently%20representing%20casually%20captured%0Amonocular%20videos%20in%20a%20spatially-%20and%20temporally-coherent%20manner.%20While%20existing%0Aapproaches%20predominantly%20rely%20on%202D/2.5D%20techniques%20treating%20videos%20as%0Acollections%20of%20spatiotemporal%20pixels%2C%20they%20struggle%20with%20complex%20motions%2C%0Aocclusions%2C%20and%20geometric%20consistency%20due%20to%20absence%20of%20temporal%20coherence%20and%0Aexplicit%203D%20structure.%20Drawing%20inspiration%20from%20monocular%20video%20as%20a%20projection%0Aof%20the%20dynamic%203D%20world%2C%20we%20explore%20representing%20videos%20in%20their%20intrinsic%203D%0Aform%20through%20continuous%20flows%20of%20Gaussian%20primitives%20in%20space-time.%20In%20this%0Apaper%2C%20we%20propose%20NutWorld%2C%20a%20novel%20framework%20that%20efficiently%20transforms%0Amonocular%20videos%20into%20dynamic%203D%20Gaussian%20representations%20in%20a%20single%20forward%0Apass.%20At%20its%20core%2C%20NutWorld%20introduces%20a%20structured%20spatial-temporal%20aligned%0AGaussian%20%28STAG%29%20representation%2C%20enabling%20optimization-free%20scene%20modeling%20with%0Aeffective%20depth%20and%20flow%20regularization.%20Through%20comprehensive%20experiments%2C%20we%0Ademonstrate%20that%20NutWorld%20achieves%20high-fidelity%20video%20reconstruction%20quality%0Awhile%20enabling%20various%20downstream%20applications%20in%20real-time.%20Demos%20and%20code%0Awill%20be%20available%20at%20https%3A//github.com/Nut-World/NutWorld.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03465v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeeing%2520World%2520Dynamics%2520in%2520a%2520Nutshell%26entry.906535625%3DQiuhong%2520Shen%2520and%2520Xuanyu%2520Yi%2520and%2520Mingbao%2520Lin%2520and%2520Hanwang%2520Zhang%2520and%2520Shuicheng%2520Yan%2520and%2520Xinchao%2520Wang%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520efficiently%2520representing%2520casually%2520captured%250Amonocular%2520videos%2520in%2520a%2520spatially-%2520and%2520temporally-coherent%2520manner.%2520While%2520existing%250Aapproaches%2520predominantly%2520rely%2520on%25202D/2.5D%2520techniques%2520treating%2520videos%2520as%250Acollections%2520of%2520spatiotemporal%2520pixels%252C%2520they%2520struggle%2520with%2520complex%2520motions%252C%250Aocclusions%252C%2520and%2520geometric%2520consistency%2520due%2520to%2520absence%2520of%2520temporal%2520coherence%2520and%250Aexplicit%25203D%2520structure.%2520Drawing%2520inspiration%2520from%2520monocular%2520video%2520as%2520a%2520projection%250Aof%2520the%2520dynamic%25203D%2520world%252C%2520we%2520explore%2520representing%2520videos%2520in%2520their%2520intrinsic%25203D%250Aform%2520through%2520continuous%2520flows%2520of%2520Gaussian%2520primitives%2520in%2520space-time.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520NutWorld%252C%2520a%2520novel%2520framework%2520that%2520efficiently%2520transforms%250Amonocular%2520videos%2520into%2520dynamic%25203D%2520Gaussian%2520representations%2520in%2520a%2520single%2520forward%250Apass.%2520At%2520its%2520core%252C%2520NutWorld%2520introduces%2520a%2520structured%2520spatial-temporal%2520aligned%250AGaussian%2520%2528STAG%2529%2520representation%252C%2520enabling%2520optimization-free%2520scene%2520modeling%2520with%250Aeffective%2520depth%2520and%2520flow%2520regularization.%2520Through%2520comprehensive%2520experiments%252C%2520we%250Ademonstrate%2520that%2520NutWorld%2520achieves%2520high-fidelity%2520video%2520reconstruction%2520quality%250Awhile%2520enabling%2520various%2520downstream%2520applications%2520in%2520real-time.%2520Demos%2520and%2520code%250Awill%2520be%2520available%2520at%2520https%253A//github.com/Nut-World/NutWorld.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03465v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Seeing%20World%20Dynamics%20in%20a%20Nutshell&entry.906535625=Qiuhong%20Shen%20and%20Xuanyu%20Yi%20and%20Mingbao%20Lin%20and%20Hanwang%20Zhang%20and%20Shuicheng%20Yan%20and%20Xinchao%20Wang&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20efficiently%20representing%20casually%20captured%0Amonocular%20videos%20in%20a%20spatially-%20and%20temporally-coherent%20manner.%20While%20existing%0Aapproaches%20predominantly%20rely%20on%202D/2.5D%20techniques%20treating%20videos%20as%0Acollections%20of%20spatiotemporal%20pixels%2C%20they%20struggle%20with%20complex%20motions%2C%0Aocclusions%2C%20and%20geometric%20consistency%20due%20to%20absence%20of%20temporal%20coherence%20and%0Aexplicit%203D%20structure.%20Drawing%20inspiration%20from%20monocular%20video%20as%20a%20projection%0Aof%20the%20dynamic%203D%20world%2C%20we%20explore%20representing%20videos%20in%20their%20intrinsic%203D%0Aform%20through%20continuous%20flows%20of%20Gaussian%20primitives%20in%20space-time.%20In%20this%0Apaper%2C%20we%20propose%20NutWorld%2C%20a%20novel%20framework%20that%20efficiently%20transforms%0Amonocular%20videos%20into%20dynamic%203D%20Gaussian%20representations%20in%20a%20single%20forward%0Apass.%20At%20its%20core%2C%20NutWorld%20introduces%20a%20structured%20spatial-temporal%20aligned%0AGaussian%20%28STAG%29%20representation%2C%20enabling%20optimization-free%20scene%20modeling%20with%0Aeffective%20depth%20and%20flow%20regularization.%20Through%20comprehensive%20experiments%2C%20we%0Ademonstrate%20that%20NutWorld%20achieves%20high-fidelity%20video%20reconstruction%20quality%0Awhile%20enabling%20various%20downstream%20applications%20in%20real-time.%20Demos%20and%20code%0Awill%20be%20available%20at%20https%3A//github.com/Nut-World/NutWorld.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03465v1&entry.124074799=Read"},
{"title": "Teaching Large Language Models Number-Focused Headline Generation With\n  Key Element Rationales", "author": "Zhen Qian and Xiuzhen Zhang and Xiaofei Xu and Feng Xia", "abstract": "  Number-focused headline generation is a summarization task requiring both\nhigh textual quality and precise numerical accuracy, which poses a unique\nchallenge for Large Language Models (LLMs). Existing studies in the literature\nfocus only on either textual quality or numerical reasoning and thus are\ninadequate to address this challenge. In this paper, we propose a novel\nchain-of-thought framework for using rationales comprising key elements of the\nTopic, Entities, and Numerical reasoning (TEN) in news articles to enhance the\ncapability for LLMs to generate topic-aligned high-quality texts with precise\nnumerical accuracy. Specifically, a teacher LLM is employed to generate TEN\nrationales as supervision data, which are then used to teach and fine-tune a\nstudent LLM. Our approach teaches the student LLM automatic generation of\nrationales with enhanced capability for numerical reasoning and topic-aligned\nnumerical headline generation. Experiments show that our approach achieves\nsuperior performance in both textual quality and numerical accuracy.\n", "link": "http://arxiv.org/abs/2502.03129v1", "date": "2025-02-05", "relevancy": 2.5051, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.512}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.512}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4791}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Teaching%20Large%20Language%20Models%20Number-Focused%20Headline%20Generation%20With%0A%20%20Key%20Element%20Rationales&body=Title%3A%20Teaching%20Large%20Language%20Models%20Number-Focused%20Headline%20Generation%20With%0A%20%20Key%20Element%20Rationales%0AAuthor%3A%20Zhen%20Qian%20and%20Xiuzhen%20Zhang%20and%20Xiaofei%20Xu%20and%20Feng%20Xia%0AAbstract%3A%20%20%20Number-focused%20headline%20generation%20is%20a%20summarization%20task%20requiring%20both%0Ahigh%20textual%20quality%20and%20precise%20numerical%20accuracy%2C%20which%20poses%20a%20unique%0Achallenge%20for%20Large%20Language%20Models%20%28LLMs%29.%20Existing%20studies%20in%20the%20literature%0Afocus%20only%20on%20either%20textual%20quality%20or%20numerical%20reasoning%20and%20thus%20are%0Ainadequate%20to%20address%20this%20challenge.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Achain-of-thought%20framework%20for%20using%20rationales%20comprising%20key%20elements%20of%20the%0ATopic%2C%20Entities%2C%20and%20Numerical%20reasoning%20%28TEN%29%20in%20news%20articles%20to%20enhance%20the%0Acapability%20for%20LLMs%20to%20generate%20topic-aligned%20high-quality%20texts%20with%20precise%0Anumerical%20accuracy.%20Specifically%2C%20a%20teacher%20LLM%20is%20employed%20to%20generate%20TEN%0Arationales%20as%20supervision%20data%2C%20which%20are%20then%20used%20to%20teach%20and%20fine-tune%20a%0Astudent%20LLM.%20Our%20approach%20teaches%20the%20student%20LLM%20automatic%20generation%20of%0Arationales%20with%20enhanced%20capability%20for%20numerical%20reasoning%20and%20topic-aligned%0Anumerical%20headline%20generation.%20Experiments%20show%20that%20our%20approach%20achieves%0Asuperior%20performance%20in%20both%20textual%20quality%20and%20numerical%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03129v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeaching%2520Large%2520Language%2520Models%2520Number-Focused%2520Headline%2520Generation%2520With%250A%2520%2520Key%2520Element%2520Rationales%26entry.906535625%3DZhen%2520Qian%2520and%2520Xiuzhen%2520Zhang%2520and%2520Xiaofei%2520Xu%2520and%2520Feng%2520Xia%26entry.1292438233%3D%2520%2520Number-focused%2520headline%2520generation%2520is%2520a%2520summarization%2520task%2520requiring%2520both%250Ahigh%2520textual%2520quality%2520and%2520precise%2520numerical%2520accuracy%252C%2520which%2520poses%2520a%2520unique%250Achallenge%2520for%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520Existing%2520studies%2520in%2520the%2520literature%250Afocus%2520only%2520on%2520either%2520textual%2520quality%2520or%2520numerical%2520reasoning%2520and%2520thus%2520are%250Ainadequate%2520to%2520address%2520this%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250Achain-of-thought%2520framework%2520for%2520using%2520rationales%2520comprising%2520key%2520elements%2520of%2520the%250ATopic%252C%2520Entities%252C%2520and%2520Numerical%2520reasoning%2520%2528TEN%2529%2520in%2520news%2520articles%2520to%2520enhance%2520the%250Acapability%2520for%2520LLMs%2520to%2520generate%2520topic-aligned%2520high-quality%2520texts%2520with%2520precise%250Anumerical%2520accuracy.%2520Specifically%252C%2520a%2520teacher%2520LLM%2520is%2520employed%2520to%2520generate%2520TEN%250Arationales%2520as%2520supervision%2520data%252C%2520which%2520are%2520then%2520used%2520to%2520teach%2520and%2520fine-tune%2520a%250Astudent%2520LLM.%2520Our%2520approach%2520teaches%2520the%2520student%2520LLM%2520automatic%2520generation%2520of%250Arationales%2520with%2520enhanced%2520capability%2520for%2520numerical%2520reasoning%2520and%2520topic-aligned%250Anumerical%2520headline%2520generation.%2520Experiments%2520show%2520that%2520our%2520approach%2520achieves%250Asuperior%2520performance%2520in%2520both%2520textual%2520quality%2520and%2520numerical%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03129v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Teaching%20Large%20Language%20Models%20Number-Focused%20Headline%20Generation%20With%0A%20%20Key%20Element%20Rationales&entry.906535625=Zhen%20Qian%20and%20Xiuzhen%20Zhang%20and%20Xiaofei%20Xu%20and%20Feng%20Xia&entry.1292438233=%20%20Number-focused%20headline%20generation%20is%20a%20summarization%20task%20requiring%20both%0Ahigh%20textual%20quality%20and%20precise%20numerical%20accuracy%2C%20which%20poses%20a%20unique%0Achallenge%20for%20Large%20Language%20Models%20%28LLMs%29.%20Existing%20studies%20in%20the%20literature%0Afocus%20only%20on%20either%20textual%20quality%20or%20numerical%20reasoning%20and%20thus%20are%0Ainadequate%20to%20address%20this%20challenge.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0Achain-of-thought%20framework%20for%20using%20rationales%20comprising%20key%20elements%20of%20the%0ATopic%2C%20Entities%2C%20and%20Numerical%20reasoning%20%28TEN%29%20in%20news%20articles%20to%20enhance%20the%0Acapability%20for%20LLMs%20to%20generate%20topic-aligned%20high-quality%20texts%20with%20precise%0Anumerical%20accuracy.%20Specifically%2C%20a%20teacher%20LLM%20is%20employed%20to%20generate%20TEN%0Arationales%20as%20supervision%20data%2C%20which%20are%20then%20used%20to%20teach%20and%20fine-tune%20a%0Astudent%20LLM.%20Our%20approach%20teaches%20the%20student%20LLM%20automatic%20generation%20of%0Arationales%20with%20enhanced%20capability%20for%20numerical%20reasoning%20and%20topic-aligned%0Anumerical%20headline%20generation.%20Experiments%20show%20that%20our%20approach%20achieves%0Asuperior%20performance%20in%20both%20textual%20quality%20and%20numerical%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03129v1&entry.124074799=Read"},
{"title": "General Time-series Model for Universal Knowledge Representation of\n  Multivariate Time-Series data", "author": "Cheng He and Xu Huang and Gangwei Jiang and Zhaoyi Li and Defu Lian and Hong Xie and Enhong Chen and Xijie Liang and Zengrong Zheng", "abstract": "  Universal knowledge representation is a central problem for multivariate time\nseries(MTS) foundation models and yet remains open. This paper investigates\nthis problem from the first principle and it makes four folds of contributions.\nFirst, a new empirical finding is revealed: time series with different time\ngranularities (or corresponding frequency resolutions) exhibit distinct joint\ndistributions in the frequency domain. This implies a crucial aspect of\nlearning universal knowledge, one that has been overlooked by previous studies.\nSecond, a novel Fourier knowledge attention mechanism is proposed to enable\nlearning time granularity-aware representations from both the temporal and\nfrequency domains. Third, an autoregressive blank infilling pre-training\nframework is incorporated to time series analysis for the first time, leading\nto a generative tasks agnostic pre-training strategy. To this end, we develop\nthe General Time-series Model (GTM), a unified MTS foundation model that\naddresses the limitation of contemporary time series models, which often\nrequire token, pre-training, or model-level customizations for downstream tasks\nadaption. Fourth, extensive experiments show that GTM outperforms\nstate-of-the-art (SOTA) methods across all generative tasks, including\nlong-term forecasting, anomaly detection, and imputation.\n", "link": "http://arxiv.org/abs/2502.03264v1", "date": "2025-02-05", "relevancy": 2.4834, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5068}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4968}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20General%20Time-series%20Model%20for%20Universal%20Knowledge%20Representation%20of%0A%20%20Multivariate%20Time-Series%20data&body=Title%3A%20General%20Time-series%20Model%20for%20Universal%20Knowledge%20Representation%20of%0A%20%20Multivariate%20Time-Series%20data%0AAuthor%3A%20Cheng%20He%20and%20Xu%20Huang%20and%20Gangwei%20Jiang%20and%20Zhaoyi%20Li%20and%20Defu%20Lian%20and%20Hong%20Xie%20and%20Enhong%20Chen%20and%20Xijie%20Liang%20and%20Zengrong%20Zheng%0AAbstract%3A%20%20%20Universal%20knowledge%20representation%20is%20a%20central%20problem%20for%20multivariate%20time%0Aseries%28MTS%29%20foundation%20models%20and%20yet%20remains%20open.%20This%20paper%20investigates%0Athis%20problem%20from%20the%20first%20principle%20and%20it%20makes%20four%20folds%20of%20contributions.%0AFirst%2C%20a%20new%20empirical%20finding%20is%20revealed%3A%20time%20series%20with%20different%20time%0Agranularities%20%28or%20corresponding%20frequency%20resolutions%29%20exhibit%20distinct%20joint%0Adistributions%20in%20the%20frequency%20domain.%20This%20implies%20a%20crucial%20aspect%20of%0Alearning%20universal%20knowledge%2C%20one%20that%20has%20been%20overlooked%20by%20previous%20studies.%0ASecond%2C%20a%20novel%20Fourier%20knowledge%20attention%20mechanism%20is%20proposed%20to%20enable%0Alearning%20time%20granularity-aware%20representations%20from%20both%20the%20temporal%20and%0Afrequency%20domains.%20Third%2C%20an%20autoregressive%20blank%20infilling%20pre-training%0Aframework%20is%20incorporated%20to%20time%20series%20analysis%20for%20the%20first%20time%2C%20leading%0Ato%20a%20generative%20tasks%20agnostic%20pre-training%20strategy.%20To%20this%20end%2C%20we%20develop%0Athe%20General%20Time-series%20Model%20%28GTM%29%2C%20a%20unified%20MTS%20foundation%20model%20that%0Aaddresses%20the%20limitation%20of%20contemporary%20time%20series%20models%2C%20which%20often%0Arequire%20token%2C%20pre-training%2C%20or%20model-level%20customizations%20for%20downstream%20tasks%0Aadaption.%20Fourth%2C%20extensive%20experiments%20show%20that%20GTM%20outperforms%0Astate-of-the-art%20%28SOTA%29%20methods%20across%20all%20generative%20tasks%2C%20including%0Along-term%20forecasting%2C%20anomaly%20detection%2C%20and%20imputation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03264v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneral%2520Time-series%2520Model%2520for%2520Universal%2520Knowledge%2520Representation%2520of%250A%2520%2520Multivariate%2520Time-Series%2520data%26entry.906535625%3DCheng%2520He%2520and%2520Xu%2520Huang%2520and%2520Gangwei%2520Jiang%2520and%2520Zhaoyi%2520Li%2520and%2520Defu%2520Lian%2520and%2520Hong%2520Xie%2520and%2520Enhong%2520Chen%2520and%2520Xijie%2520Liang%2520and%2520Zengrong%2520Zheng%26entry.1292438233%3D%2520%2520Universal%2520knowledge%2520representation%2520is%2520a%2520central%2520problem%2520for%2520multivariate%2520time%250Aseries%2528MTS%2529%2520foundation%2520models%2520and%2520yet%2520remains%2520open.%2520This%2520paper%2520investigates%250Athis%2520problem%2520from%2520the%2520first%2520principle%2520and%2520it%2520makes%2520four%2520folds%2520of%2520contributions.%250AFirst%252C%2520a%2520new%2520empirical%2520finding%2520is%2520revealed%253A%2520time%2520series%2520with%2520different%2520time%250Agranularities%2520%2528or%2520corresponding%2520frequency%2520resolutions%2529%2520exhibit%2520distinct%2520joint%250Adistributions%2520in%2520the%2520frequency%2520domain.%2520This%2520implies%2520a%2520crucial%2520aspect%2520of%250Alearning%2520universal%2520knowledge%252C%2520one%2520that%2520has%2520been%2520overlooked%2520by%2520previous%2520studies.%250ASecond%252C%2520a%2520novel%2520Fourier%2520knowledge%2520attention%2520mechanism%2520is%2520proposed%2520to%2520enable%250Alearning%2520time%2520granularity-aware%2520representations%2520from%2520both%2520the%2520temporal%2520and%250Afrequency%2520domains.%2520Third%252C%2520an%2520autoregressive%2520blank%2520infilling%2520pre-training%250Aframework%2520is%2520incorporated%2520to%2520time%2520series%2520analysis%2520for%2520the%2520first%2520time%252C%2520leading%250Ato%2520a%2520generative%2520tasks%2520agnostic%2520pre-training%2520strategy.%2520To%2520this%2520end%252C%2520we%2520develop%250Athe%2520General%2520Time-series%2520Model%2520%2528GTM%2529%252C%2520a%2520unified%2520MTS%2520foundation%2520model%2520that%250Aaddresses%2520the%2520limitation%2520of%2520contemporary%2520time%2520series%2520models%252C%2520which%2520often%250Arequire%2520token%252C%2520pre-training%252C%2520or%2520model-level%2520customizations%2520for%2520downstream%2520tasks%250Aadaption.%2520Fourth%252C%2520extensive%2520experiments%2520show%2520that%2520GTM%2520outperforms%250Astate-of-the-art%2520%2528SOTA%2529%2520methods%2520across%2520all%2520generative%2520tasks%252C%2520including%250Along-term%2520forecasting%252C%2520anomaly%2520detection%252C%2520and%2520imputation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03264v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=General%20Time-series%20Model%20for%20Universal%20Knowledge%20Representation%20of%0A%20%20Multivariate%20Time-Series%20data&entry.906535625=Cheng%20He%20and%20Xu%20Huang%20and%20Gangwei%20Jiang%20and%20Zhaoyi%20Li%20and%20Defu%20Lian%20and%20Hong%20Xie%20and%20Enhong%20Chen%20and%20Xijie%20Liang%20and%20Zengrong%20Zheng&entry.1292438233=%20%20Universal%20knowledge%20representation%20is%20a%20central%20problem%20for%20multivariate%20time%0Aseries%28MTS%29%20foundation%20models%20and%20yet%20remains%20open.%20This%20paper%20investigates%0Athis%20problem%20from%20the%20first%20principle%20and%20it%20makes%20four%20folds%20of%20contributions.%0AFirst%2C%20a%20new%20empirical%20finding%20is%20revealed%3A%20time%20series%20with%20different%20time%0Agranularities%20%28or%20corresponding%20frequency%20resolutions%29%20exhibit%20distinct%20joint%0Adistributions%20in%20the%20frequency%20domain.%20This%20implies%20a%20crucial%20aspect%20of%0Alearning%20universal%20knowledge%2C%20one%20that%20has%20been%20overlooked%20by%20previous%20studies.%0ASecond%2C%20a%20novel%20Fourier%20knowledge%20attention%20mechanism%20is%20proposed%20to%20enable%0Alearning%20time%20granularity-aware%20representations%20from%20both%20the%20temporal%20and%0Afrequency%20domains.%20Third%2C%20an%20autoregressive%20blank%20infilling%20pre-training%0Aframework%20is%20incorporated%20to%20time%20series%20analysis%20for%20the%20first%20time%2C%20leading%0Ato%20a%20generative%20tasks%20agnostic%20pre-training%20strategy.%20To%20this%20end%2C%20we%20develop%0Athe%20General%20Time-series%20Model%20%28GTM%29%2C%20a%20unified%20MTS%20foundation%20model%20that%0Aaddresses%20the%20limitation%20of%20contemporary%20time%20series%20models%2C%20which%20often%0Arequire%20token%2C%20pre-training%2C%20or%20model-level%20customizations%20for%20downstream%20tasks%0Aadaption.%20Fourth%2C%20extensive%20experiments%20show%20that%20GTM%20outperforms%0Astate-of-the-art%20%28SOTA%29%20methods%20across%20all%20generative%20tasks%2C%20including%0Along-term%20forecasting%2C%20anomaly%20detection%2C%20and%20imputation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03264v1&entry.124074799=Read"},
{"title": "Scalable In-Context Learning on Tabular Data via Retrieval-Augmented\n  Large Language Models", "author": "Xumeng Wen and Shun Zheng and Zhen Xu and Yiming Sun and Jiang Bian", "abstract": "  Recent studies have shown that large language models (LLMs), when customized\nwith post-training on tabular data, can acquire general tabular in-context\nlearning (TabICL) capabilities. These models are able to transfer effectively\nacross diverse data schemas and different task domains. However, existing\nLLM-based TabICL approaches are constrained to few-shot scenarios due to the\nsequence length limitations of LLMs, as tabular instances represented in plain\ntext consume substantial tokens. To address this limitation and enable scalable\nTabICL for any data size, we propose retrieval-augmented LLMs tailored to\ntabular data. Our approach incorporates a customized retrieval module, combined\nwith retrieval-guided instruction-tuning for LLMs. This enables LLMs to\neffectively leverage larger datasets, achieving significantly improved\nperformance across 69 widely recognized datasets and demonstrating promising\nscaling behavior. Extensive comparisons with state-of-the-art tabular models\nreveal that, while LLM-based TabICL still lags behind well-tuned numeric models\nin overall performance, it uncovers powerful algorithms under limited contexts,\nenhances ensemble diversity, and excels on specific datasets. These unique\nproperties underscore the potential of language as a universal and accessible\ninterface for scalable tabular data learning.\n", "link": "http://arxiv.org/abs/2502.03147v1", "date": "2025-02-05", "relevancy": 2.4718, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5347}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4822}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4662}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scalable%20In-Context%20Learning%20on%20Tabular%20Data%20via%20Retrieval-Augmented%0A%20%20Large%20Language%20Models&body=Title%3A%20Scalable%20In-Context%20Learning%20on%20Tabular%20Data%20via%20Retrieval-Augmented%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Xumeng%20Wen%20and%20Shun%20Zheng%20and%20Zhen%20Xu%20and%20Yiming%20Sun%20and%20Jiang%20Bian%0AAbstract%3A%20%20%20Recent%20studies%20have%20shown%20that%20large%20language%20models%20%28LLMs%29%2C%20when%20customized%0Awith%20post-training%20on%20tabular%20data%2C%20can%20acquire%20general%20tabular%20in-context%0Alearning%20%28TabICL%29%20capabilities.%20These%20models%20are%20able%20to%20transfer%20effectively%0Aacross%20diverse%20data%20schemas%20and%20different%20task%20domains.%20However%2C%20existing%0ALLM-based%20TabICL%20approaches%20are%20constrained%20to%20few-shot%20scenarios%20due%20to%20the%0Asequence%20length%20limitations%20of%20LLMs%2C%20as%20tabular%20instances%20represented%20in%20plain%0Atext%20consume%20substantial%20tokens.%20To%20address%20this%20limitation%20and%20enable%20scalable%0ATabICL%20for%20any%20data%20size%2C%20we%20propose%20retrieval-augmented%20LLMs%20tailored%20to%0Atabular%20data.%20Our%20approach%20incorporates%20a%20customized%20retrieval%20module%2C%20combined%0Awith%20retrieval-guided%20instruction-tuning%20for%20LLMs.%20This%20enables%20LLMs%20to%0Aeffectively%20leverage%20larger%20datasets%2C%20achieving%20significantly%20improved%0Aperformance%20across%2069%20widely%20recognized%20datasets%20and%20demonstrating%20promising%0Ascaling%20behavior.%20Extensive%20comparisons%20with%20state-of-the-art%20tabular%20models%0Areveal%20that%2C%20while%20LLM-based%20TabICL%20still%20lags%20behind%20well-tuned%20numeric%20models%0Ain%20overall%20performance%2C%20it%20uncovers%20powerful%20algorithms%20under%20limited%20contexts%2C%0Aenhances%20ensemble%20diversity%2C%20and%20excels%20on%20specific%20datasets.%20These%20unique%0Aproperties%20underscore%20the%20potential%20of%20language%20as%20a%20universal%20and%20accessible%0Ainterface%20for%20scalable%20tabular%20data%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03147v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScalable%2520In-Context%2520Learning%2520on%2520Tabular%2520Data%2520via%2520Retrieval-Augmented%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DXumeng%2520Wen%2520and%2520Shun%2520Zheng%2520and%2520Zhen%2520Xu%2520and%2520Yiming%2520Sun%2520and%2520Jiang%2520Bian%26entry.1292438233%3D%2520%2520Recent%2520studies%2520have%2520shown%2520that%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520when%2520customized%250Awith%2520post-training%2520on%2520tabular%2520data%252C%2520can%2520acquire%2520general%2520tabular%2520in-context%250Alearning%2520%2528TabICL%2529%2520capabilities.%2520These%2520models%2520are%2520able%2520to%2520transfer%2520effectively%250Aacross%2520diverse%2520data%2520schemas%2520and%2520different%2520task%2520domains.%2520However%252C%2520existing%250ALLM-based%2520TabICL%2520approaches%2520are%2520constrained%2520to%2520few-shot%2520scenarios%2520due%2520to%2520the%250Asequence%2520length%2520limitations%2520of%2520LLMs%252C%2520as%2520tabular%2520instances%2520represented%2520in%2520plain%250Atext%2520consume%2520substantial%2520tokens.%2520To%2520address%2520this%2520limitation%2520and%2520enable%2520scalable%250ATabICL%2520for%2520any%2520data%2520size%252C%2520we%2520propose%2520retrieval-augmented%2520LLMs%2520tailored%2520to%250Atabular%2520data.%2520Our%2520approach%2520incorporates%2520a%2520customized%2520retrieval%2520module%252C%2520combined%250Awith%2520retrieval-guided%2520instruction-tuning%2520for%2520LLMs.%2520This%2520enables%2520LLMs%2520to%250Aeffectively%2520leverage%2520larger%2520datasets%252C%2520achieving%2520significantly%2520improved%250Aperformance%2520across%252069%2520widely%2520recognized%2520datasets%2520and%2520demonstrating%2520promising%250Ascaling%2520behavior.%2520Extensive%2520comparisons%2520with%2520state-of-the-art%2520tabular%2520models%250Areveal%2520that%252C%2520while%2520LLM-based%2520TabICL%2520still%2520lags%2520behind%2520well-tuned%2520numeric%2520models%250Ain%2520overall%2520performance%252C%2520it%2520uncovers%2520powerful%2520algorithms%2520under%2520limited%2520contexts%252C%250Aenhances%2520ensemble%2520diversity%252C%2520and%2520excels%2520on%2520specific%2520datasets.%2520These%2520unique%250Aproperties%2520underscore%2520the%2520potential%2520of%2520language%2520as%2520a%2520universal%2520and%2520accessible%250Ainterface%2520for%2520scalable%2520tabular%2520data%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03147v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20In-Context%20Learning%20on%20Tabular%20Data%20via%20Retrieval-Augmented%0A%20%20Large%20Language%20Models&entry.906535625=Xumeng%20Wen%20and%20Shun%20Zheng%20and%20Zhen%20Xu%20and%20Yiming%20Sun%20and%20Jiang%20Bian&entry.1292438233=%20%20Recent%20studies%20have%20shown%20that%20large%20language%20models%20%28LLMs%29%2C%20when%20customized%0Awith%20post-training%20on%20tabular%20data%2C%20can%20acquire%20general%20tabular%20in-context%0Alearning%20%28TabICL%29%20capabilities.%20These%20models%20are%20able%20to%20transfer%20effectively%0Aacross%20diverse%20data%20schemas%20and%20different%20task%20domains.%20However%2C%20existing%0ALLM-based%20TabICL%20approaches%20are%20constrained%20to%20few-shot%20scenarios%20due%20to%20the%0Asequence%20length%20limitations%20of%20LLMs%2C%20as%20tabular%20instances%20represented%20in%20plain%0Atext%20consume%20substantial%20tokens.%20To%20address%20this%20limitation%20and%20enable%20scalable%0ATabICL%20for%20any%20data%20size%2C%20we%20propose%20retrieval-augmented%20LLMs%20tailored%20to%0Atabular%20data.%20Our%20approach%20incorporates%20a%20customized%20retrieval%20module%2C%20combined%0Awith%20retrieval-guided%20instruction-tuning%20for%20LLMs.%20This%20enables%20LLMs%20to%0Aeffectively%20leverage%20larger%20datasets%2C%20achieving%20significantly%20improved%0Aperformance%20across%2069%20widely%20recognized%20datasets%20and%20demonstrating%20promising%0Ascaling%20behavior.%20Extensive%20comparisons%20with%20state-of-the-art%20tabular%20models%0Areveal%20that%2C%20while%20LLM-based%20TabICL%20still%20lags%20behind%20well-tuned%20numeric%20models%0Ain%20overall%20performance%2C%20it%20uncovers%20powerful%20algorithms%20under%20limited%20contexts%2C%0Aenhances%20ensemble%20diversity%2C%20and%20excels%20on%20specific%20datasets.%20These%20unique%0Aproperties%20underscore%20the%20potential%20of%20language%20as%20a%20universal%20and%20accessible%0Ainterface%20for%20scalable%20tabular%20data%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03147v1&entry.124074799=Read"},
{"title": "Large Language Models Are Universal Recommendation Learners", "author": "Junguang Jiang and Yanwen Huang and Bin Liu and Xiaoyu Kong and Ziru Xu and Han Zhu and Jian Xu and Bo Zheng", "abstract": "  In real-world recommender systems, different tasks are typically addressed\nusing supervised learning on task-specific datasets with carefully designed\nmodel architectures. We demonstrate that large language models (LLMs) can\nfunction as universal recommendation learners, capable of handling multiple\ntasks within a unified input-output framework, eliminating the need for\nspecialized model designs. To improve the recommendation performance of LLMs,\nwe introduce a multimodal fusion module for item representation and a\nsequence-in-set-out approach for efficient candidate generation. When applied\nto industrial-scale data, our LLM achieves competitive results with expert\nmodels elaborately designed for different recommendation tasks. Furthermore,\nour analysis reveals that recommendation outcomes are highly sensitive to text\ninput, highlighting the potential of prompt engineering in optimizing\nindustrial-scale recommender systems.\n", "link": "http://arxiv.org/abs/2502.03041v1", "date": "2025-02-05", "relevancy": 2.4624, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4956}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4956}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4862}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20Are%20Universal%20Recommendation%20Learners&body=Title%3A%20Large%20Language%20Models%20Are%20Universal%20Recommendation%20Learners%0AAuthor%3A%20Junguang%20Jiang%20and%20Yanwen%20Huang%20and%20Bin%20Liu%20and%20Xiaoyu%20Kong%20and%20Ziru%20Xu%20and%20Han%20Zhu%20and%20Jian%20Xu%20and%20Bo%20Zheng%0AAbstract%3A%20%20%20In%20real-world%20recommender%20systems%2C%20different%20tasks%20are%20typically%20addressed%0Ausing%20supervised%20learning%20on%20task-specific%20datasets%20with%20carefully%20designed%0Amodel%20architectures.%20We%20demonstrate%20that%20large%20language%20models%20%28LLMs%29%20can%0Afunction%20as%20universal%20recommendation%20learners%2C%20capable%20of%20handling%20multiple%0Atasks%20within%20a%20unified%20input-output%20framework%2C%20eliminating%20the%20need%20for%0Aspecialized%20model%20designs.%20To%20improve%20the%20recommendation%20performance%20of%20LLMs%2C%0Awe%20introduce%20a%20multimodal%20fusion%20module%20for%20item%20representation%20and%20a%0Asequence-in-set-out%20approach%20for%20efficient%20candidate%20generation.%20When%20applied%0Ato%20industrial-scale%20data%2C%20our%20LLM%20achieves%20competitive%20results%20with%20expert%0Amodels%20elaborately%20designed%20for%20different%20recommendation%20tasks.%20Furthermore%2C%0Aour%20analysis%20reveals%20that%20recommendation%20outcomes%20are%20highly%20sensitive%20to%20text%0Ainput%2C%20highlighting%20the%20potential%20of%20prompt%20engineering%20in%20optimizing%0Aindustrial-scale%20recommender%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03041v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520Are%2520Universal%2520Recommendation%2520Learners%26entry.906535625%3DJunguang%2520Jiang%2520and%2520Yanwen%2520Huang%2520and%2520Bin%2520Liu%2520and%2520Xiaoyu%2520Kong%2520and%2520Ziru%2520Xu%2520and%2520Han%2520Zhu%2520and%2520Jian%2520Xu%2520and%2520Bo%2520Zheng%26entry.1292438233%3D%2520%2520In%2520real-world%2520recommender%2520systems%252C%2520different%2520tasks%2520are%2520typically%2520addressed%250Ausing%2520supervised%2520learning%2520on%2520task-specific%2520datasets%2520with%2520carefully%2520designed%250Amodel%2520architectures.%2520We%2520demonstrate%2520that%2520large%2520language%2520models%2520%2528LLMs%2529%2520can%250Afunction%2520as%2520universal%2520recommendation%2520learners%252C%2520capable%2520of%2520handling%2520multiple%250Atasks%2520within%2520a%2520unified%2520input-output%2520framework%252C%2520eliminating%2520the%2520need%2520for%250Aspecialized%2520model%2520designs.%2520To%2520improve%2520the%2520recommendation%2520performance%2520of%2520LLMs%252C%250Awe%2520introduce%2520a%2520multimodal%2520fusion%2520module%2520for%2520item%2520representation%2520and%2520a%250Asequence-in-set-out%2520approach%2520for%2520efficient%2520candidate%2520generation.%2520When%2520applied%250Ato%2520industrial-scale%2520data%252C%2520our%2520LLM%2520achieves%2520competitive%2520results%2520with%2520expert%250Amodels%2520elaborately%2520designed%2520for%2520different%2520recommendation%2520tasks.%2520Furthermore%252C%250Aour%2520analysis%2520reveals%2520that%2520recommendation%2520outcomes%2520are%2520highly%2520sensitive%2520to%2520text%250Ainput%252C%2520highlighting%2520the%2520potential%2520of%2520prompt%2520engineering%2520in%2520optimizing%250Aindustrial-scale%2520recommender%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03041v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20Are%20Universal%20Recommendation%20Learners&entry.906535625=Junguang%20Jiang%20and%20Yanwen%20Huang%20and%20Bin%20Liu%20and%20Xiaoyu%20Kong%20and%20Ziru%20Xu%20and%20Han%20Zhu%20and%20Jian%20Xu%20and%20Bo%20Zheng&entry.1292438233=%20%20In%20real-world%20recommender%20systems%2C%20different%20tasks%20are%20typically%20addressed%0Ausing%20supervised%20learning%20on%20task-specific%20datasets%20with%20carefully%20designed%0Amodel%20architectures.%20We%20demonstrate%20that%20large%20language%20models%20%28LLMs%29%20can%0Afunction%20as%20universal%20recommendation%20learners%2C%20capable%20of%20handling%20multiple%0Atasks%20within%20a%20unified%20input-output%20framework%2C%20eliminating%20the%20need%20for%0Aspecialized%20model%20designs.%20To%20improve%20the%20recommendation%20performance%20of%20LLMs%2C%0Awe%20introduce%20a%20multimodal%20fusion%20module%20for%20item%20representation%20and%20a%0Asequence-in-set-out%20approach%20for%20efficient%20candidate%20generation.%20When%20applied%0Ato%20industrial-scale%20data%2C%20our%20LLM%20achieves%20competitive%20results%20with%20expert%0Amodels%20elaborately%20designed%20for%20different%20recommendation%20tasks.%20Furthermore%2C%0Aour%20analysis%20reveals%20that%20recommendation%20outcomes%20are%20highly%20sensitive%20to%20text%0Ainput%2C%20highlighting%20the%20potential%20of%20prompt%20engineering%20in%20optimizing%0Aindustrial-scale%20recommender%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03041v1&entry.124074799=Read"},
{"title": "AlphaAdam:Asynchronous Masked Optimization with Dynamic Alpha for\n  Selective Updates", "author": "Da Chang and Yu Li and Ganzhao Yuan", "abstract": "  In the training of large language models (LLMs), updating parameters more\nefficiently and stably has always been an important challenge. To achieve\nefficient parameter updates, existing methods usually achieve performance\ncomparable to full parameter updates through methods such as low-dimensional\ndecomposition or layer-wise selective updates. In this work, we propose\nAlphaAdam, an optimization framework for LLM from the perspective of\nintra-layer parameter updates. By decoupling parameter updates and dynamically\nadjusting their strength, AlphaAdam accelerates convergence and improves\ntraining stability. We construct parameter masks based on the consistency of\nhistorical momentum and gradient direction and combine them with an adaptive\nmask strength strategy to ensure efficient optimization and theoretical\nconvergence guarantees, which is also applicable to most momentum-based\noptimizers. Extensive experiments show that AlphaAdam outperforms\nstate-of-the-art methods such as AdamW in terms of convergence speed and\ncomputational efficiency across tasks, including GPT-2 pre-trained and\nfine-tuned RoBERTa and Llama-7B. Our AlphaAdam implements an optimizer\nenhancement framework for LLMs through intra-layer asynchronous masked adaptive\nupdates. Our code is available in this https://github.com/MaeChd/AlphaAdam.\n", "link": "http://arxiv.org/abs/2501.18094v2", "date": "2025-02-05", "relevancy": 2.4571, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4972}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4955}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AlphaAdam%3AAsynchronous%20Masked%20Optimization%20with%20Dynamic%20Alpha%20for%0A%20%20Selective%20Updates&body=Title%3A%20AlphaAdam%3AAsynchronous%20Masked%20Optimization%20with%20Dynamic%20Alpha%20for%0A%20%20Selective%20Updates%0AAuthor%3A%20Da%20Chang%20and%20Yu%20Li%20and%20Ganzhao%20Yuan%0AAbstract%3A%20%20%20In%20the%20training%20of%20large%20language%20models%20%28LLMs%29%2C%20updating%20parameters%20more%0Aefficiently%20and%20stably%20has%20always%20been%20an%20important%20challenge.%20To%20achieve%0Aefficient%20parameter%20updates%2C%20existing%20methods%20usually%20achieve%20performance%0Acomparable%20to%20full%20parameter%20updates%20through%20methods%20such%20as%20low-dimensional%0Adecomposition%20or%20layer-wise%20selective%20updates.%20In%20this%20work%2C%20we%20propose%0AAlphaAdam%2C%20an%20optimization%20framework%20for%20LLM%20from%20the%20perspective%20of%0Aintra-layer%20parameter%20updates.%20By%20decoupling%20parameter%20updates%20and%20dynamically%0Aadjusting%20their%20strength%2C%20AlphaAdam%20accelerates%20convergence%20and%20improves%0Atraining%20stability.%20We%20construct%20parameter%20masks%20based%20on%20the%20consistency%20of%0Ahistorical%20momentum%20and%20gradient%20direction%20and%20combine%20them%20with%20an%20adaptive%0Amask%20strength%20strategy%20to%20ensure%20efficient%20optimization%20and%20theoretical%0Aconvergence%20guarantees%2C%20which%20is%20also%20applicable%20to%20most%20momentum-based%0Aoptimizers.%20Extensive%20experiments%20show%20that%20AlphaAdam%20outperforms%0Astate-of-the-art%20methods%20such%20as%20AdamW%20in%20terms%20of%20convergence%20speed%20and%0Acomputational%20efficiency%20across%20tasks%2C%20including%20GPT-2%20pre-trained%20and%0Afine-tuned%20RoBERTa%20and%20Llama-7B.%20Our%20AlphaAdam%20implements%20an%20optimizer%0Aenhancement%20framework%20for%20LLMs%20through%20intra-layer%20asynchronous%20masked%20adaptive%0Aupdates.%20Our%20code%20is%20available%20in%20this%20https%3A//github.com/MaeChd/AlphaAdam.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.18094v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlphaAdam%253AAsynchronous%2520Masked%2520Optimization%2520with%2520Dynamic%2520Alpha%2520for%250A%2520%2520Selective%2520Updates%26entry.906535625%3DDa%2520Chang%2520and%2520Yu%2520Li%2520and%2520Ganzhao%2520Yuan%26entry.1292438233%3D%2520%2520In%2520the%2520training%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520updating%2520parameters%2520more%250Aefficiently%2520and%2520stably%2520has%2520always%2520been%2520an%2520important%2520challenge.%2520To%2520achieve%250Aefficient%2520parameter%2520updates%252C%2520existing%2520methods%2520usually%2520achieve%2520performance%250Acomparable%2520to%2520full%2520parameter%2520updates%2520through%2520methods%2520such%2520as%2520low-dimensional%250Adecomposition%2520or%2520layer-wise%2520selective%2520updates.%2520In%2520this%2520work%252C%2520we%2520propose%250AAlphaAdam%252C%2520an%2520optimization%2520framework%2520for%2520LLM%2520from%2520the%2520perspective%2520of%250Aintra-layer%2520parameter%2520updates.%2520By%2520decoupling%2520parameter%2520updates%2520and%2520dynamically%250Aadjusting%2520their%2520strength%252C%2520AlphaAdam%2520accelerates%2520convergence%2520and%2520improves%250Atraining%2520stability.%2520We%2520construct%2520parameter%2520masks%2520based%2520on%2520the%2520consistency%2520of%250Ahistorical%2520momentum%2520and%2520gradient%2520direction%2520and%2520combine%2520them%2520with%2520an%2520adaptive%250Amask%2520strength%2520strategy%2520to%2520ensure%2520efficient%2520optimization%2520and%2520theoretical%250Aconvergence%2520guarantees%252C%2520which%2520is%2520also%2520applicable%2520to%2520most%2520momentum-based%250Aoptimizers.%2520Extensive%2520experiments%2520show%2520that%2520AlphaAdam%2520outperforms%250Astate-of-the-art%2520methods%2520such%2520as%2520AdamW%2520in%2520terms%2520of%2520convergence%2520speed%2520and%250Acomputational%2520efficiency%2520across%2520tasks%252C%2520including%2520GPT-2%2520pre-trained%2520and%250Afine-tuned%2520RoBERTa%2520and%2520Llama-7B.%2520Our%2520AlphaAdam%2520implements%2520an%2520optimizer%250Aenhancement%2520framework%2520for%2520LLMs%2520through%2520intra-layer%2520asynchronous%2520masked%2520adaptive%250Aupdates.%2520Our%2520code%2520is%2520available%2520in%2520this%2520https%253A//github.com/MaeChd/AlphaAdam.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.18094v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AlphaAdam%3AAsynchronous%20Masked%20Optimization%20with%20Dynamic%20Alpha%20for%0A%20%20Selective%20Updates&entry.906535625=Da%20Chang%20and%20Yu%20Li%20and%20Ganzhao%20Yuan&entry.1292438233=%20%20In%20the%20training%20of%20large%20language%20models%20%28LLMs%29%2C%20updating%20parameters%20more%0Aefficiently%20and%20stably%20has%20always%20been%20an%20important%20challenge.%20To%20achieve%0Aefficient%20parameter%20updates%2C%20existing%20methods%20usually%20achieve%20performance%0Acomparable%20to%20full%20parameter%20updates%20through%20methods%20such%20as%20low-dimensional%0Adecomposition%20or%20layer-wise%20selective%20updates.%20In%20this%20work%2C%20we%20propose%0AAlphaAdam%2C%20an%20optimization%20framework%20for%20LLM%20from%20the%20perspective%20of%0Aintra-layer%20parameter%20updates.%20By%20decoupling%20parameter%20updates%20and%20dynamically%0Aadjusting%20their%20strength%2C%20AlphaAdam%20accelerates%20convergence%20and%20improves%0Atraining%20stability.%20We%20construct%20parameter%20masks%20based%20on%20the%20consistency%20of%0Ahistorical%20momentum%20and%20gradient%20direction%20and%20combine%20them%20with%20an%20adaptive%0Amask%20strength%20strategy%20to%20ensure%20efficient%20optimization%20and%20theoretical%0Aconvergence%20guarantees%2C%20which%20is%20also%20applicable%20to%20most%20momentum-based%0Aoptimizers.%20Extensive%20experiments%20show%20that%20AlphaAdam%20outperforms%0Astate-of-the-art%20methods%20such%20as%20AdamW%20in%20terms%20of%20convergence%20speed%20and%0Acomputational%20efficiency%20across%20tasks%2C%20including%20GPT-2%20pre-trained%20and%0Afine-tuned%20RoBERTa%20and%20Llama-7B.%20Our%20AlphaAdam%20implements%20an%20optimizer%0Aenhancement%20framework%20for%20LLMs%20through%20intra-layer%20asynchronous%20masked%20adaptive%0Aupdates.%20Our%20code%20is%20available%20in%20this%20https%3A//github.com/MaeChd/AlphaAdam.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.18094v2&entry.124074799=Read"},
{"title": "Pioneer: Physics-informed Riemannian Graph ODE for Entropy-increasing\n  Dynamics", "author": "Li Sun and Ziheng Zhang and Zixi Wang and Yujie Wang and Qiqi Wan and Hao Li and Hao Peng and Philip S. Yu", "abstract": "  Dynamic interacting system modeling is important for understanding and\nsimulating real world systems. The system is typically described as a graph,\nwhere multiple objects dynamically interact with each other and evolve over\ntime. In recent years, graph Ordinary Differential Equations (ODE) receive\nincreasing research attentions. While achieving encouraging results, existing\nsolutions prioritize the traditional Euclidean space, and neglect the intrinsic\ngeometry of the system and physics laws, e.g., the principle of entropy\nincreasing. The limitations above motivate us to rethink the system dynamics\nfrom a fresh perspective of Riemannian geometry, and pose a more realistic\nproblem of physics-informed dynamic system modeling, considering the underlying\ngeometry and physics law for the first time. In this paper, we present a novel\nphysics-informed Riemannian graph ODE for a wide range of entropy-increasing\ndynamic systems (termed as Pioneer). In particular, we formulate a differential\nsystem on the Riemannian manifold, where a manifold-valued graph ODE is\ngoverned by the proposed constrained Ricci flow, and a manifold preserving\nGyro-transform aware of system geometry. Theoretically, we report the provable\nentropy non-decreasing of our formulation, obeying the physics laws. Empirical\nresults show the superiority of Pioneer on real datasets.\n", "link": "http://arxiv.org/abs/2502.03236v1", "date": "2025-02-05", "relevancy": 2.435, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.499}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4831}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pioneer%3A%20Physics-informed%20Riemannian%20Graph%20ODE%20for%20Entropy-increasing%0A%20%20Dynamics&body=Title%3A%20Pioneer%3A%20Physics-informed%20Riemannian%20Graph%20ODE%20for%20Entropy-increasing%0A%20%20Dynamics%0AAuthor%3A%20Li%20Sun%20and%20Ziheng%20Zhang%20and%20Zixi%20Wang%20and%20Yujie%20Wang%20and%20Qiqi%20Wan%20and%20Hao%20Li%20and%20Hao%20Peng%20and%20Philip%20S.%20Yu%0AAbstract%3A%20%20%20Dynamic%20interacting%20system%20modeling%20is%20important%20for%20understanding%20and%0Asimulating%20real%20world%20systems.%20The%20system%20is%20typically%20described%20as%20a%20graph%2C%0Awhere%20multiple%20objects%20dynamically%20interact%20with%20each%20other%20and%20evolve%20over%0Atime.%20In%20recent%20years%2C%20graph%20Ordinary%20Differential%20Equations%20%28ODE%29%20receive%0Aincreasing%20research%20attentions.%20While%20achieving%20encouraging%20results%2C%20existing%0Asolutions%20prioritize%20the%20traditional%20Euclidean%20space%2C%20and%20neglect%20the%20intrinsic%0Ageometry%20of%20the%20system%20and%20physics%20laws%2C%20e.g.%2C%20the%20principle%20of%20entropy%0Aincreasing.%20The%20limitations%20above%20motivate%20us%20to%20rethink%20the%20system%20dynamics%0Afrom%20a%20fresh%20perspective%20of%20Riemannian%20geometry%2C%20and%20pose%20a%20more%20realistic%0Aproblem%20of%20physics-informed%20dynamic%20system%20modeling%2C%20considering%20the%20underlying%0Ageometry%20and%20physics%20law%20for%20the%20first%20time.%20In%20this%20paper%2C%20we%20present%20a%20novel%0Aphysics-informed%20Riemannian%20graph%20ODE%20for%20a%20wide%20range%20of%20entropy-increasing%0Adynamic%20systems%20%28termed%20as%20Pioneer%29.%20In%20particular%2C%20we%20formulate%20a%20differential%0Asystem%20on%20the%20Riemannian%20manifold%2C%20where%20a%20manifold-valued%20graph%20ODE%20is%0Agoverned%20by%20the%20proposed%20constrained%20Ricci%20flow%2C%20and%20a%20manifold%20preserving%0AGyro-transform%20aware%20of%20system%20geometry.%20Theoretically%2C%20we%20report%20the%20provable%0Aentropy%20non-decreasing%20of%20our%20formulation%2C%20obeying%20the%20physics%20laws.%20Empirical%0Aresults%20show%20the%20superiority%20of%20Pioneer%20on%20real%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03236v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPioneer%253A%2520Physics-informed%2520Riemannian%2520Graph%2520ODE%2520for%2520Entropy-increasing%250A%2520%2520Dynamics%26entry.906535625%3DLi%2520Sun%2520and%2520Ziheng%2520Zhang%2520and%2520Zixi%2520Wang%2520and%2520Yujie%2520Wang%2520and%2520Qiqi%2520Wan%2520and%2520Hao%2520Li%2520and%2520Hao%2520Peng%2520and%2520Philip%2520S.%2520Yu%26entry.1292438233%3D%2520%2520Dynamic%2520interacting%2520system%2520modeling%2520is%2520important%2520for%2520understanding%2520and%250Asimulating%2520real%2520world%2520systems.%2520The%2520system%2520is%2520typically%2520described%2520as%2520a%2520graph%252C%250Awhere%2520multiple%2520objects%2520dynamically%2520interact%2520with%2520each%2520other%2520and%2520evolve%2520over%250Atime.%2520In%2520recent%2520years%252C%2520graph%2520Ordinary%2520Differential%2520Equations%2520%2528ODE%2529%2520receive%250Aincreasing%2520research%2520attentions.%2520While%2520achieving%2520encouraging%2520results%252C%2520existing%250Asolutions%2520prioritize%2520the%2520traditional%2520Euclidean%2520space%252C%2520and%2520neglect%2520the%2520intrinsic%250Ageometry%2520of%2520the%2520system%2520and%2520physics%2520laws%252C%2520e.g.%252C%2520the%2520principle%2520of%2520entropy%250Aincreasing.%2520The%2520limitations%2520above%2520motivate%2520us%2520to%2520rethink%2520the%2520system%2520dynamics%250Afrom%2520a%2520fresh%2520perspective%2520of%2520Riemannian%2520geometry%252C%2520and%2520pose%2520a%2520more%2520realistic%250Aproblem%2520of%2520physics-informed%2520dynamic%2520system%2520modeling%252C%2520considering%2520the%2520underlying%250Ageometry%2520and%2520physics%2520law%2520for%2520the%2520first%2520time.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%250Aphysics-informed%2520Riemannian%2520graph%2520ODE%2520for%2520a%2520wide%2520range%2520of%2520entropy-increasing%250Adynamic%2520systems%2520%2528termed%2520as%2520Pioneer%2529.%2520In%2520particular%252C%2520we%2520formulate%2520a%2520differential%250Asystem%2520on%2520the%2520Riemannian%2520manifold%252C%2520where%2520a%2520manifold-valued%2520graph%2520ODE%2520is%250Agoverned%2520by%2520the%2520proposed%2520constrained%2520Ricci%2520flow%252C%2520and%2520a%2520manifold%2520preserving%250AGyro-transform%2520aware%2520of%2520system%2520geometry.%2520Theoretically%252C%2520we%2520report%2520the%2520provable%250Aentropy%2520non-decreasing%2520of%2520our%2520formulation%252C%2520obeying%2520the%2520physics%2520laws.%2520Empirical%250Aresults%2520show%2520the%2520superiority%2520of%2520Pioneer%2520on%2520real%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03236v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pioneer%3A%20Physics-informed%20Riemannian%20Graph%20ODE%20for%20Entropy-increasing%0A%20%20Dynamics&entry.906535625=Li%20Sun%20and%20Ziheng%20Zhang%20and%20Zixi%20Wang%20and%20Yujie%20Wang%20and%20Qiqi%20Wan%20and%20Hao%20Li%20and%20Hao%20Peng%20and%20Philip%20S.%20Yu&entry.1292438233=%20%20Dynamic%20interacting%20system%20modeling%20is%20important%20for%20understanding%20and%0Asimulating%20real%20world%20systems.%20The%20system%20is%20typically%20described%20as%20a%20graph%2C%0Awhere%20multiple%20objects%20dynamically%20interact%20with%20each%20other%20and%20evolve%20over%0Atime.%20In%20recent%20years%2C%20graph%20Ordinary%20Differential%20Equations%20%28ODE%29%20receive%0Aincreasing%20research%20attentions.%20While%20achieving%20encouraging%20results%2C%20existing%0Asolutions%20prioritize%20the%20traditional%20Euclidean%20space%2C%20and%20neglect%20the%20intrinsic%0Ageometry%20of%20the%20system%20and%20physics%20laws%2C%20e.g.%2C%20the%20principle%20of%20entropy%0Aincreasing.%20The%20limitations%20above%20motivate%20us%20to%20rethink%20the%20system%20dynamics%0Afrom%20a%20fresh%20perspective%20of%20Riemannian%20geometry%2C%20and%20pose%20a%20more%20realistic%0Aproblem%20of%20physics-informed%20dynamic%20system%20modeling%2C%20considering%20the%20underlying%0Ageometry%20and%20physics%20law%20for%20the%20first%20time.%20In%20this%20paper%2C%20we%20present%20a%20novel%0Aphysics-informed%20Riemannian%20graph%20ODE%20for%20a%20wide%20range%20of%20entropy-increasing%0Adynamic%20systems%20%28termed%20as%20Pioneer%29.%20In%20particular%2C%20we%20formulate%20a%20differential%0Asystem%20on%20the%20Riemannian%20manifold%2C%20where%20a%20manifold-valued%20graph%20ODE%20is%0Agoverned%20by%20the%20proposed%20constrained%20Ricci%20flow%2C%20and%20a%20manifold%20preserving%0AGyro-transform%20aware%20of%20system%20geometry.%20Theoretically%2C%20we%20report%20the%20provable%0Aentropy%20non-decreasing%20of%20our%20formulation%2C%20obeying%20the%20physics%20laws.%20Empirical%0Aresults%20show%20the%20superiority%20of%20Pioneer%20on%20real%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03236v1&entry.124074799=Read"},
{"title": "Token-based Decision Criteria Are Suboptimal in In-context Learning", "author": "Hakaze Cho and Yoshihiro Sakai and Mariko Kato and Kenshiro Tanaka and Akira Ishii and Naoya Inoue", "abstract": "  In-Context Learning (ICL) typically utilizes classification criteria from\noutput probabilities of manually selected label tokens. However, we argue that\nsuch token-based classification criteria lead to suboptimal decision\nboundaries, despite delicate calibrations through translation and constrained\nrotation applied. To address this problem, we propose Hidden Calibration, which\nrenounces token probabilities and uses the nearest centroid classifier on the\nLM's last hidden states. In detail, we assign the label of the nearest centroid\npreviously estimated from a calibration set to the test sample as the predicted\nlabel. Our experiments on 6 models and 10 classification datasets indicate that\nHidden Calibration consistently outperforms current token-based baselines by\nabout 20%~50%, achieving a strong state-of-the-art in ICL. Our further analysis\ndemonstrates that Hidden Calibration finds better classification criteria with\nless inter-class overlap, and LMs provide linearly separable intra-class\nclusters with the help of demonstrations, which supports Hidden Calibration and\ngives new insights into the principle of ICL. Our official code implementation\ncan be found at https://github.com/hc495/Hidden_Calibration.\n", "link": "http://arxiv.org/abs/2406.16535v3", "date": "2025-02-05", "relevancy": 2.4051, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4875}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4786}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Token-based%20Decision%20Criteria%20Are%20Suboptimal%20in%20In-context%20Learning&body=Title%3A%20Token-based%20Decision%20Criteria%20Are%20Suboptimal%20in%20In-context%20Learning%0AAuthor%3A%20Hakaze%20Cho%20and%20Yoshihiro%20Sakai%20and%20Mariko%20Kato%20and%20Kenshiro%20Tanaka%20and%20Akira%20Ishii%20and%20Naoya%20Inoue%0AAbstract%3A%20%20%20In-Context%20Learning%20%28ICL%29%20typically%20utilizes%20classification%20criteria%20from%0Aoutput%20probabilities%20of%20manually%20selected%20label%20tokens.%20However%2C%20we%20argue%20that%0Asuch%20token-based%20classification%20criteria%20lead%20to%20suboptimal%20decision%0Aboundaries%2C%20despite%20delicate%20calibrations%20through%20translation%20and%20constrained%0Arotation%20applied.%20To%20address%20this%20problem%2C%20we%20propose%20Hidden%20Calibration%2C%20which%0Arenounces%20token%20probabilities%20and%20uses%20the%20nearest%20centroid%20classifier%20on%20the%0ALM%27s%20last%20hidden%20states.%20In%20detail%2C%20we%20assign%20the%20label%20of%20the%20nearest%20centroid%0Apreviously%20estimated%20from%20a%20calibration%20set%20to%20the%20test%20sample%20as%20the%20predicted%0Alabel.%20Our%20experiments%20on%206%20models%20and%2010%20classification%20datasets%20indicate%20that%0AHidden%20Calibration%20consistently%20outperforms%20current%20token-based%20baselines%20by%0Aabout%2020%25~50%25%2C%20achieving%20a%20strong%20state-of-the-art%20in%20ICL.%20Our%20further%20analysis%0Ademonstrates%20that%20Hidden%20Calibration%20finds%20better%20classification%20criteria%20with%0Aless%20inter-class%20overlap%2C%20and%20LMs%20provide%20linearly%20separable%20intra-class%0Aclusters%20with%20the%20help%20of%20demonstrations%2C%20which%20supports%20Hidden%20Calibration%20and%0Agives%20new%20insights%20into%20the%20principle%20of%20ICL.%20Our%20official%20code%20implementation%0Acan%20be%20found%20at%20https%3A//github.com/hc495/Hidden_Calibration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16535v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToken-based%2520Decision%2520Criteria%2520Are%2520Suboptimal%2520in%2520In-context%2520Learning%26entry.906535625%3DHakaze%2520Cho%2520and%2520Yoshihiro%2520Sakai%2520and%2520Mariko%2520Kato%2520and%2520Kenshiro%2520Tanaka%2520and%2520Akira%2520Ishii%2520and%2520Naoya%2520Inoue%26entry.1292438233%3D%2520%2520In-Context%2520Learning%2520%2528ICL%2529%2520typically%2520utilizes%2520classification%2520criteria%2520from%250Aoutput%2520probabilities%2520of%2520manually%2520selected%2520label%2520tokens.%2520However%252C%2520we%2520argue%2520that%250Asuch%2520token-based%2520classification%2520criteria%2520lead%2520to%2520suboptimal%2520decision%250Aboundaries%252C%2520despite%2520delicate%2520calibrations%2520through%2520translation%2520and%2520constrained%250Arotation%2520applied.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520Hidden%2520Calibration%252C%2520which%250Arenounces%2520token%2520probabilities%2520and%2520uses%2520the%2520nearest%2520centroid%2520classifier%2520on%2520the%250ALM%2527s%2520last%2520hidden%2520states.%2520In%2520detail%252C%2520we%2520assign%2520the%2520label%2520of%2520the%2520nearest%2520centroid%250Apreviously%2520estimated%2520from%2520a%2520calibration%2520set%2520to%2520the%2520test%2520sample%2520as%2520the%2520predicted%250Alabel.%2520Our%2520experiments%2520on%25206%2520models%2520and%252010%2520classification%2520datasets%2520indicate%2520that%250AHidden%2520Calibration%2520consistently%2520outperforms%2520current%2520token-based%2520baselines%2520by%250Aabout%252020%2525~50%2525%252C%2520achieving%2520a%2520strong%2520state-of-the-art%2520in%2520ICL.%2520Our%2520further%2520analysis%250Ademonstrates%2520that%2520Hidden%2520Calibration%2520finds%2520better%2520classification%2520criteria%2520with%250Aless%2520inter-class%2520overlap%252C%2520and%2520LMs%2520provide%2520linearly%2520separable%2520intra-class%250Aclusters%2520with%2520the%2520help%2520of%2520demonstrations%252C%2520which%2520supports%2520Hidden%2520Calibration%2520and%250Agives%2520new%2520insights%2520into%2520the%2520principle%2520of%2520ICL.%2520Our%2520official%2520code%2520implementation%250Acan%2520be%2520found%2520at%2520https%253A//github.com/hc495/Hidden_Calibration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16535v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Token-based%20Decision%20Criteria%20Are%20Suboptimal%20in%20In-context%20Learning&entry.906535625=Hakaze%20Cho%20and%20Yoshihiro%20Sakai%20and%20Mariko%20Kato%20and%20Kenshiro%20Tanaka%20and%20Akira%20Ishii%20and%20Naoya%20Inoue&entry.1292438233=%20%20In-Context%20Learning%20%28ICL%29%20typically%20utilizes%20classification%20criteria%20from%0Aoutput%20probabilities%20of%20manually%20selected%20label%20tokens.%20However%2C%20we%20argue%20that%0Asuch%20token-based%20classification%20criteria%20lead%20to%20suboptimal%20decision%0Aboundaries%2C%20despite%20delicate%20calibrations%20through%20translation%20and%20constrained%0Arotation%20applied.%20To%20address%20this%20problem%2C%20we%20propose%20Hidden%20Calibration%2C%20which%0Arenounces%20token%20probabilities%20and%20uses%20the%20nearest%20centroid%20classifier%20on%20the%0ALM%27s%20last%20hidden%20states.%20In%20detail%2C%20we%20assign%20the%20label%20of%20the%20nearest%20centroid%0Apreviously%20estimated%20from%20a%20calibration%20set%20to%20the%20test%20sample%20as%20the%20predicted%0Alabel.%20Our%20experiments%20on%206%20models%20and%2010%20classification%20datasets%20indicate%20that%0AHidden%20Calibration%20consistently%20outperforms%20current%20token-based%20baselines%20by%0Aabout%2020%25~50%25%2C%20achieving%20a%20strong%20state-of-the-art%20in%20ICL.%20Our%20further%20analysis%0Ademonstrates%20that%20Hidden%20Calibration%20finds%20better%20classification%20criteria%20with%0Aless%20inter-class%20overlap%2C%20and%20LMs%20provide%20linearly%20separable%20intra-class%0Aclusters%20with%20the%20help%20of%20demonstrations%2C%20which%20supports%20Hidden%20Calibration%20and%0Agives%20new%20insights%20into%20the%20principle%20of%20ICL.%20Our%20official%20code%20implementation%0Acan%20be%20found%20at%20https%3A//github.com/hc495/Hidden_Calibration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16535v3&entry.124074799=Read"},
{"title": "Linearized Optimal Transport pyLOT Library: A Toolkit for Machine\n  Learning on Point Clouds", "author": "Jun Linwu and Varun Khurana and Nicholas Karris and Alexander Cloninger", "abstract": "  The pyLOT library offers a Python implementation of linearized optimal\ntransport (LOT) techniques and methods to use in downstream tasks. The pipeline\nembeds probability distributions into a Hilbert space via the Optimal Transport\nmaps from a fixed reference distribution, and this linearization allows\ndownstream tasks to be completed using off the shelf (linear) machine learning\nalgorithms. We provide a case study of performing ML on 3D scans of lemur\nteeth, where the original questions of classification, clustering, dimension\nreduction, and data generation reduce to simple linear operations performed on\nthe LOT embedded representations.\n", "link": "http://arxiv.org/abs/2502.03439v1", "date": "2025-02-05", "relevancy": 2.3987, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4979}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4755}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Linearized%20Optimal%20Transport%20pyLOT%20Library%3A%20A%20Toolkit%20for%20Machine%0A%20%20Learning%20on%20Point%20Clouds&body=Title%3A%20Linearized%20Optimal%20Transport%20pyLOT%20Library%3A%20A%20Toolkit%20for%20Machine%0A%20%20Learning%20on%20Point%20Clouds%0AAuthor%3A%20Jun%20Linwu%20and%20Varun%20Khurana%20and%20Nicholas%20Karris%20and%20Alexander%20Cloninger%0AAbstract%3A%20%20%20The%20pyLOT%20library%20offers%20a%20Python%20implementation%20of%20linearized%20optimal%0Atransport%20%28LOT%29%20techniques%20and%20methods%20to%20use%20in%20downstream%20tasks.%20The%20pipeline%0Aembeds%20probability%20distributions%20into%20a%20Hilbert%20space%20via%20the%20Optimal%20Transport%0Amaps%20from%20a%20fixed%20reference%20distribution%2C%20and%20this%20linearization%20allows%0Adownstream%20tasks%20to%20be%20completed%20using%20off%20the%20shelf%20%28linear%29%20machine%20learning%0Aalgorithms.%20We%20provide%20a%20case%20study%20of%20performing%20ML%20on%203D%20scans%20of%20lemur%0Ateeth%2C%20where%20the%20original%20questions%20of%20classification%2C%20clustering%2C%20dimension%0Areduction%2C%20and%20data%20generation%20reduce%20to%20simple%20linear%20operations%20performed%20on%0Athe%20LOT%20embedded%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03439v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinearized%2520Optimal%2520Transport%2520pyLOT%2520Library%253A%2520A%2520Toolkit%2520for%2520Machine%250A%2520%2520Learning%2520on%2520Point%2520Clouds%26entry.906535625%3DJun%2520Linwu%2520and%2520Varun%2520Khurana%2520and%2520Nicholas%2520Karris%2520and%2520Alexander%2520Cloninger%26entry.1292438233%3D%2520%2520The%2520pyLOT%2520library%2520offers%2520a%2520Python%2520implementation%2520of%2520linearized%2520optimal%250Atransport%2520%2528LOT%2529%2520techniques%2520and%2520methods%2520to%2520use%2520in%2520downstream%2520tasks.%2520The%2520pipeline%250Aembeds%2520probability%2520distributions%2520into%2520a%2520Hilbert%2520space%2520via%2520the%2520Optimal%2520Transport%250Amaps%2520from%2520a%2520fixed%2520reference%2520distribution%252C%2520and%2520this%2520linearization%2520allows%250Adownstream%2520tasks%2520to%2520be%2520completed%2520using%2520off%2520the%2520shelf%2520%2528linear%2529%2520machine%2520learning%250Aalgorithms.%2520We%2520provide%2520a%2520case%2520study%2520of%2520performing%2520ML%2520on%25203D%2520scans%2520of%2520lemur%250Ateeth%252C%2520where%2520the%2520original%2520questions%2520of%2520classification%252C%2520clustering%252C%2520dimension%250Areduction%252C%2520and%2520data%2520generation%2520reduce%2520to%2520simple%2520linear%2520operations%2520performed%2520on%250Athe%2520LOT%2520embedded%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03439v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Linearized%20Optimal%20Transport%20pyLOT%20Library%3A%20A%20Toolkit%20for%20Machine%0A%20%20Learning%20on%20Point%20Clouds&entry.906535625=Jun%20Linwu%20and%20Varun%20Khurana%20and%20Nicholas%20Karris%20and%20Alexander%20Cloninger&entry.1292438233=%20%20The%20pyLOT%20library%20offers%20a%20Python%20implementation%20of%20linearized%20optimal%0Atransport%20%28LOT%29%20techniques%20and%20methods%20to%20use%20in%20downstream%20tasks.%20The%20pipeline%0Aembeds%20probability%20distributions%20into%20a%20Hilbert%20space%20via%20the%20Optimal%20Transport%0Amaps%20from%20a%20fixed%20reference%20distribution%2C%20and%20this%20linearization%20allows%0Adownstream%20tasks%20to%20be%20completed%20using%20off%20the%20shelf%20%28linear%29%20machine%20learning%0Aalgorithms.%20We%20provide%20a%20case%20study%20of%20performing%20ML%20on%203D%20scans%20of%20lemur%0Ateeth%2C%20where%20the%20original%20questions%20of%20classification%2C%20clustering%2C%20dimension%0Areduction%2C%20and%20data%20generation%20reduce%20to%20simple%20linear%20operations%20performed%20on%0Athe%20LOT%20embedded%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03439v1&entry.124074799=Read"},
{"title": "A Unified and General Humanoid Whole-Body Controller for Fine-Grained\n  Locomotion", "author": "Yufei Xue and Wentao Dong and Minghuan Liu and Weinan Zhang and Jiangmiao Pang", "abstract": "  Locomotion is a fundamental skill for humanoid robots. However, most existing\nworks made locomotion a single, tedious, unextendable, and passive movement.\nThis limits the kinematic capabilities of humanoid robots. In contrast, humans\npossess versatile athletic abilities-running, jumping, hopping, and finely\nadjusting walking parameters such as frequency, and foot height. In this paper,\nwe investigate solutions to bring such versatility into humanoid locomotion and\nthereby propose HUGWBC: a unified and general humanoid whole-body controller\nfor fine-grained locomotion. By designing a general command space in the aspect\nof tasks and behaviors, along with advanced techniques like symmetrical loss\nand intervention training for learning a whole-body humanoid controlling policy\nin simulation, HugWBC enables real-world humanoid robots to produce various\nnatural gaits, including walking (running), jumping, standing, and hopping,\nwith customizable parameters such as frequency, foot swing height, further\ncombined with different body height, waist rotation, and body pitch, all in one\nsingle policy. Beyond locomotion, HUGWBC also supports real-time interventions\nfrom external upper-body controllers like teleoperation, enabling\nloco-manipulation while maintaining precise control under any locomotive\nbehavior. Our experiments validate the high tracking accuracy and robustness of\nHUGWBC with/without upper-body intervention for all commands, and we further\nprovide an in-depth analysis of how the various commands affect humanoid\nmovement and offer insights into the relationships between these commands. To\nour knowledge, HugWBC is the first humanoid whole-body controller that supports\nsuch fine-grained locomotion behaviors with high robustness and flexibility.\n", "link": "http://arxiv.org/abs/2502.03206v1", "date": "2025-02-05", "relevancy": 2.3914, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6127}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.589}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20and%20General%20Humanoid%20Whole-Body%20Controller%20for%20Fine-Grained%0A%20%20Locomotion&body=Title%3A%20A%20Unified%20and%20General%20Humanoid%20Whole-Body%20Controller%20for%20Fine-Grained%0A%20%20Locomotion%0AAuthor%3A%20Yufei%20Xue%20and%20Wentao%20Dong%20and%20Minghuan%20Liu%20and%20Weinan%20Zhang%20and%20Jiangmiao%20Pang%0AAbstract%3A%20%20%20Locomotion%20is%20a%20fundamental%20skill%20for%20humanoid%20robots.%20However%2C%20most%20existing%0Aworks%20made%20locomotion%20a%20single%2C%20tedious%2C%20unextendable%2C%20and%20passive%20movement.%0AThis%20limits%20the%20kinematic%20capabilities%20of%20humanoid%20robots.%20In%20contrast%2C%20humans%0Apossess%20versatile%20athletic%20abilities-running%2C%20jumping%2C%20hopping%2C%20and%20finely%0Aadjusting%20walking%20parameters%20such%20as%20frequency%2C%20and%20foot%20height.%20In%20this%20paper%2C%0Awe%20investigate%20solutions%20to%20bring%20such%20versatility%20into%20humanoid%20locomotion%20and%0Athereby%20propose%20HUGWBC%3A%20a%20unified%20and%20general%20humanoid%20whole-body%20controller%0Afor%20fine-grained%20locomotion.%20By%20designing%20a%20general%20command%20space%20in%20the%20aspect%0Aof%20tasks%20and%20behaviors%2C%20along%20with%20advanced%20techniques%20like%20symmetrical%20loss%0Aand%20intervention%20training%20for%20learning%20a%20whole-body%20humanoid%20controlling%20policy%0Ain%20simulation%2C%20HugWBC%20enables%20real-world%20humanoid%20robots%20to%20produce%20various%0Anatural%20gaits%2C%20including%20walking%20%28running%29%2C%20jumping%2C%20standing%2C%20and%20hopping%2C%0Awith%20customizable%20parameters%20such%20as%20frequency%2C%20foot%20swing%20height%2C%20further%0Acombined%20with%20different%20body%20height%2C%20waist%20rotation%2C%20and%20body%20pitch%2C%20all%20in%20one%0Asingle%20policy.%20Beyond%20locomotion%2C%20HUGWBC%20also%20supports%20real-time%20interventions%0Afrom%20external%20upper-body%20controllers%20like%20teleoperation%2C%20enabling%0Aloco-manipulation%20while%20maintaining%20precise%20control%20under%20any%20locomotive%0Abehavior.%20Our%20experiments%20validate%20the%20high%20tracking%20accuracy%20and%20robustness%20of%0AHUGWBC%20with/without%20upper-body%20intervention%20for%20all%20commands%2C%20and%20we%20further%0Aprovide%20an%20in-depth%20analysis%20of%20how%20the%20various%20commands%20affect%20humanoid%0Amovement%20and%20offer%20insights%20into%20the%20relationships%20between%20these%20commands.%20To%0Aour%20knowledge%2C%20HugWBC%20is%20the%20first%20humanoid%20whole-body%20controller%20that%20supports%0Asuch%20fine-grained%20locomotion%20behaviors%20with%20high%20robustness%20and%20flexibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03206v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520and%2520General%2520Humanoid%2520Whole-Body%2520Controller%2520for%2520Fine-Grained%250A%2520%2520Locomotion%26entry.906535625%3DYufei%2520Xue%2520and%2520Wentao%2520Dong%2520and%2520Minghuan%2520Liu%2520and%2520Weinan%2520Zhang%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3D%2520%2520Locomotion%2520is%2520a%2520fundamental%2520skill%2520for%2520humanoid%2520robots.%2520However%252C%2520most%2520existing%250Aworks%2520made%2520locomotion%2520a%2520single%252C%2520tedious%252C%2520unextendable%252C%2520and%2520passive%2520movement.%250AThis%2520limits%2520the%2520kinematic%2520capabilities%2520of%2520humanoid%2520robots.%2520In%2520contrast%252C%2520humans%250Apossess%2520versatile%2520athletic%2520abilities-running%252C%2520jumping%252C%2520hopping%252C%2520and%2520finely%250Aadjusting%2520walking%2520parameters%2520such%2520as%2520frequency%252C%2520and%2520foot%2520height.%2520In%2520this%2520paper%252C%250Awe%2520investigate%2520solutions%2520to%2520bring%2520such%2520versatility%2520into%2520humanoid%2520locomotion%2520and%250Athereby%2520propose%2520HUGWBC%253A%2520a%2520unified%2520and%2520general%2520humanoid%2520whole-body%2520controller%250Afor%2520fine-grained%2520locomotion.%2520By%2520designing%2520a%2520general%2520command%2520space%2520in%2520the%2520aspect%250Aof%2520tasks%2520and%2520behaviors%252C%2520along%2520with%2520advanced%2520techniques%2520like%2520symmetrical%2520loss%250Aand%2520intervention%2520training%2520for%2520learning%2520a%2520whole-body%2520humanoid%2520controlling%2520policy%250Ain%2520simulation%252C%2520HugWBC%2520enables%2520real-world%2520humanoid%2520robots%2520to%2520produce%2520various%250Anatural%2520gaits%252C%2520including%2520walking%2520%2528running%2529%252C%2520jumping%252C%2520standing%252C%2520and%2520hopping%252C%250Awith%2520customizable%2520parameters%2520such%2520as%2520frequency%252C%2520foot%2520swing%2520height%252C%2520further%250Acombined%2520with%2520different%2520body%2520height%252C%2520waist%2520rotation%252C%2520and%2520body%2520pitch%252C%2520all%2520in%2520one%250Asingle%2520policy.%2520Beyond%2520locomotion%252C%2520HUGWBC%2520also%2520supports%2520real-time%2520interventions%250Afrom%2520external%2520upper-body%2520controllers%2520like%2520teleoperation%252C%2520enabling%250Aloco-manipulation%2520while%2520maintaining%2520precise%2520control%2520under%2520any%2520locomotive%250Abehavior.%2520Our%2520experiments%2520validate%2520the%2520high%2520tracking%2520accuracy%2520and%2520robustness%2520of%250AHUGWBC%2520with/without%2520upper-body%2520intervention%2520for%2520all%2520commands%252C%2520and%2520we%2520further%250Aprovide%2520an%2520in-depth%2520analysis%2520of%2520how%2520the%2520various%2520commands%2520affect%2520humanoid%250Amovement%2520and%2520offer%2520insights%2520into%2520the%2520relationships%2520between%2520these%2520commands.%2520To%250Aour%2520knowledge%252C%2520HugWBC%2520is%2520the%2520first%2520humanoid%2520whole-body%2520controller%2520that%2520supports%250Asuch%2520fine-grained%2520locomotion%2520behaviors%2520with%2520high%2520robustness%2520and%2520flexibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03206v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20and%20General%20Humanoid%20Whole-Body%20Controller%20for%20Fine-Grained%0A%20%20Locomotion&entry.906535625=Yufei%20Xue%20and%20Wentao%20Dong%20and%20Minghuan%20Liu%20and%20Weinan%20Zhang%20and%20Jiangmiao%20Pang&entry.1292438233=%20%20Locomotion%20is%20a%20fundamental%20skill%20for%20humanoid%20robots.%20However%2C%20most%20existing%0Aworks%20made%20locomotion%20a%20single%2C%20tedious%2C%20unextendable%2C%20and%20passive%20movement.%0AThis%20limits%20the%20kinematic%20capabilities%20of%20humanoid%20robots.%20In%20contrast%2C%20humans%0Apossess%20versatile%20athletic%20abilities-running%2C%20jumping%2C%20hopping%2C%20and%20finely%0Aadjusting%20walking%20parameters%20such%20as%20frequency%2C%20and%20foot%20height.%20In%20this%20paper%2C%0Awe%20investigate%20solutions%20to%20bring%20such%20versatility%20into%20humanoid%20locomotion%20and%0Athereby%20propose%20HUGWBC%3A%20a%20unified%20and%20general%20humanoid%20whole-body%20controller%0Afor%20fine-grained%20locomotion.%20By%20designing%20a%20general%20command%20space%20in%20the%20aspect%0Aof%20tasks%20and%20behaviors%2C%20along%20with%20advanced%20techniques%20like%20symmetrical%20loss%0Aand%20intervention%20training%20for%20learning%20a%20whole-body%20humanoid%20controlling%20policy%0Ain%20simulation%2C%20HugWBC%20enables%20real-world%20humanoid%20robots%20to%20produce%20various%0Anatural%20gaits%2C%20including%20walking%20%28running%29%2C%20jumping%2C%20standing%2C%20and%20hopping%2C%0Awith%20customizable%20parameters%20such%20as%20frequency%2C%20foot%20swing%20height%2C%20further%0Acombined%20with%20different%20body%20height%2C%20waist%20rotation%2C%20and%20body%20pitch%2C%20all%20in%20one%0Asingle%20policy.%20Beyond%20locomotion%2C%20HUGWBC%20also%20supports%20real-time%20interventions%0Afrom%20external%20upper-body%20controllers%20like%20teleoperation%2C%20enabling%0Aloco-manipulation%20while%20maintaining%20precise%20control%20under%20any%20locomotive%0Abehavior.%20Our%20experiments%20validate%20the%20high%20tracking%20accuracy%20and%20robustness%20of%0AHUGWBC%20with/without%20upper-body%20intervention%20for%20all%20commands%2C%20and%20we%20further%0Aprovide%20an%20in-depth%20analysis%20of%20how%20the%20various%20commands%20affect%20humanoid%0Amovement%20and%20offer%20insights%20into%20the%20relationships%20between%20these%20commands.%20To%0Aour%20knowledge%2C%20HugWBC%20is%20the%20first%20humanoid%20whole-body%20controller%20that%20supports%0Asuch%20fine-grained%20locomotion%20behaviors%20with%20high%20robustness%20and%20flexibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03206v1&entry.124074799=Read"},
{"title": "Spatial-Temporal Knowledge Distillation for Takeaway Recommendation", "author": "Shuyuan Zhao and Wei Chen and Boyan Shi and Liyong Zhou and Shuohao Lin and Huaiyu Wan", "abstract": "  The takeaway recommendation system aims to recommend users' future takeaway\npurchases based on their historical purchase behaviors, thereby improving user\nsatisfaction and boosting merchant sales. Existing methods focus on\nincorporating auxiliary information or leveraging knowledge graphs to alleviate\nthe sparsity issue of user purchase sequences. However, two main challenges\nlimit the performance of these approaches: (1) capturing dynamic user\npreferences on complex geospatial information and (2) efficiently integrating\nspatial-temporal knowledge from both graphs and sequence data with low\ncomputational costs. In this paper, we propose a novel spatial-temporal\nknowledge distillation model for takeaway recommendation (STKDRec) based on the\ntwo-stage training process. Specifically, during the first pre-training stage,\na spatial-temporal knowledge graph (STKG) encoder is trained to extract\nhigh-order spatial-temporal dependencies and collaborative associations from\nthe STKG. During the second spatial-temporal knowledge distillation (STKD)\nstage, a spatial-temporal Transformer (ST-Transformer) is employed to\ncomprehensively model dynamic user preferences on various types of fine-grained\ngeospatial information from a sequential perspective. Furthermore, the STKD\nstrategy is introduced to transfer graph-based spatial-temporal knowledge to\nthe ST-Transformer, facilitating the adaptive fusion of rich knowledge derived\nfrom both the STKG and sequence data while reducing computational overhead.\nExtensive experiments on three real-world datasets show that STKDRec\nsignificantly outperforms the state-of-the-art baselines.\n", "link": "http://arxiv.org/abs/2412.16502v2", "date": "2025-02-05", "relevancy": 2.3701, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4793}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4783}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial-Temporal%20Knowledge%20Distillation%20for%20Takeaway%20Recommendation&body=Title%3A%20Spatial-Temporal%20Knowledge%20Distillation%20for%20Takeaway%20Recommendation%0AAuthor%3A%20Shuyuan%20Zhao%20and%20Wei%20Chen%20and%20Boyan%20Shi%20and%20Liyong%20Zhou%20and%20Shuohao%20Lin%20and%20Huaiyu%20Wan%0AAbstract%3A%20%20%20The%20takeaway%20recommendation%20system%20aims%20to%20recommend%20users%27%20future%20takeaway%0Apurchases%20based%20on%20their%20historical%20purchase%20behaviors%2C%20thereby%20improving%20user%0Asatisfaction%20and%20boosting%20merchant%20sales.%20Existing%20methods%20focus%20on%0Aincorporating%20auxiliary%20information%20or%20leveraging%20knowledge%20graphs%20to%20alleviate%0Athe%20sparsity%20issue%20of%20user%20purchase%20sequences.%20However%2C%20two%20main%20challenges%0Alimit%20the%20performance%20of%20these%20approaches%3A%20%281%29%20capturing%20dynamic%20user%0Apreferences%20on%20complex%20geospatial%20information%20and%20%282%29%20efficiently%20integrating%0Aspatial-temporal%20knowledge%20from%20both%20graphs%20and%20sequence%20data%20with%20low%0Acomputational%20costs.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20spatial-temporal%0Aknowledge%20distillation%20model%20for%20takeaway%20recommendation%20%28STKDRec%29%20based%20on%20the%0Atwo-stage%20training%20process.%20Specifically%2C%20during%20the%20first%20pre-training%20stage%2C%0Aa%20spatial-temporal%20knowledge%20graph%20%28STKG%29%20encoder%20is%20trained%20to%20extract%0Ahigh-order%20spatial-temporal%20dependencies%20and%20collaborative%20associations%20from%0Athe%20STKG.%20During%20the%20second%20spatial-temporal%20knowledge%20distillation%20%28STKD%29%0Astage%2C%20a%20spatial-temporal%20Transformer%20%28ST-Transformer%29%20is%20employed%20to%0Acomprehensively%20model%20dynamic%20user%20preferences%20on%20various%20types%20of%20fine-grained%0Ageospatial%20information%20from%20a%20sequential%20perspective.%20Furthermore%2C%20the%20STKD%0Astrategy%20is%20introduced%20to%20transfer%20graph-based%20spatial-temporal%20knowledge%20to%0Athe%20ST-Transformer%2C%20facilitating%20the%20adaptive%20fusion%20of%20rich%20knowledge%20derived%0Afrom%20both%20the%20STKG%20and%20sequence%20data%20while%20reducing%20computational%20overhead.%0AExtensive%20experiments%20on%20three%20real-world%20datasets%20show%20that%20STKDRec%0Asignificantly%20outperforms%20the%20state-of-the-art%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.16502v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial-Temporal%2520Knowledge%2520Distillation%2520for%2520Takeaway%2520Recommendation%26entry.906535625%3DShuyuan%2520Zhao%2520and%2520Wei%2520Chen%2520and%2520Boyan%2520Shi%2520and%2520Liyong%2520Zhou%2520and%2520Shuohao%2520Lin%2520and%2520Huaiyu%2520Wan%26entry.1292438233%3D%2520%2520The%2520takeaway%2520recommendation%2520system%2520aims%2520to%2520recommend%2520users%2527%2520future%2520takeaway%250Apurchases%2520based%2520on%2520their%2520historical%2520purchase%2520behaviors%252C%2520thereby%2520improving%2520user%250Asatisfaction%2520and%2520boosting%2520merchant%2520sales.%2520Existing%2520methods%2520focus%2520on%250Aincorporating%2520auxiliary%2520information%2520or%2520leveraging%2520knowledge%2520graphs%2520to%2520alleviate%250Athe%2520sparsity%2520issue%2520of%2520user%2520purchase%2520sequences.%2520However%252C%2520two%2520main%2520challenges%250Alimit%2520the%2520performance%2520of%2520these%2520approaches%253A%2520%25281%2529%2520capturing%2520dynamic%2520user%250Apreferences%2520on%2520complex%2520geospatial%2520information%2520and%2520%25282%2529%2520efficiently%2520integrating%250Aspatial-temporal%2520knowledge%2520from%2520both%2520graphs%2520and%2520sequence%2520data%2520with%2520low%250Acomputational%2520costs.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520spatial-temporal%250Aknowledge%2520distillation%2520model%2520for%2520takeaway%2520recommendation%2520%2528STKDRec%2529%2520based%2520on%2520the%250Atwo-stage%2520training%2520process.%2520Specifically%252C%2520during%2520the%2520first%2520pre-training%2520stage%252C%250Aa%2520spatial-temporal%2520knowledge%2520graph%2520%2528STKG%2529%2520encoder%2520is%2520trained%2520to%2520extract%250Ahigh-order%2520spatial-temporal%2520dependencies%2520and%2520collaborative%2520associations%2520from%250Athe%2520STKG.%2520During%2520the%2520second%2520spatial-temporal%2520knowledge%2520distillation%2520%2528STKD%2529%250Astage%252C%2520a%2520spatial-temporal%2520Transformer%2520%2528ST-Transformer%2529%2520is%2520employed%2520to%250Acomprehensively%2520model%2520dynamic%2520user%2520preferences%2520on%2520various%2520types%2520of%2520fine-grained%250Ageospatial%2520information%2520from%2520a%2520sequential%2520perspective.%2520Furthermore%252C%2520the%2520STKD%250Astrategy%2520is%2520introduced%2520to%2520transfer%2520graph-based%2520spatial-temporal%2520knowledge%2520to%250Athe%2520ST-Transformer%252C%2520facilitating%2520the%2520adaptive%2520fusion%2520of%2520rich%2520knowledge%2520derived%250Afrom%2520both%2520the%2520STKG%2520and%2520sequence%2520data%2520while%2520reducing%2520computational%2520overhead.%250AExtensive%2520experiments%2520on%2520three%2520real-world%2520datasets%2520show%2520that%2520STKDRec%250Asignificantly%2520outperforms%2520the%2520state-of-the-art%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.16502v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial-Temporal%20Knowledge%20Distillation%20for%20Takeaway%20Recommendation&entry.906535625=Shuyuan%20Zhao%20and%20Wei%20Chen%20and%20Boyan%20Shi%20and%20Liyong%20Zhou%20and%20Shuohao%20Lin%20and%20Huaiyu%20Wan&entry.1292438233=%20%20The%20takeaway%20recommendation%20system%20aims%20to%20recommend%20users%27%20future%20takeaway%0Apurchases%20based%20on%20their%20historical%20purchase%20behaviors%2C%20thereby%20improving%20user%0Asatisfaction%20and%20boosting%20merchant%20sales.%20Existing%20methods%20focus%20on%0Aincorporating%20auxiliary%20information%20or%20leveraging%20knowledge%20graphs%20to%20alleviate%0Athe%20sparsity%20issue%20of%20user%20purchase%20sequences.%20However%2C%20two%20main%20challenges%0Alimit%20the%20performance%20of%20these%20approaches%3A%20%281%29%20capturing%20dynamic%20user%0Apreferences%20on%20complex%20geospatial%20information%20and%20%282%29%20efficiently%20integrating%0Aspatial-temporal%20knowledge%20from%20both%20graphs%20and%20sequence%20data%20with%20low%0Acomputational%20costs.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20spatial-temporal%0Aknowledge%20distillation%20model%20for%20takeaway%20recommendation%20%28STKDRec%29%20based%20on%20the%0Atwo-stage%20training%20process.%20Specifically%2C%20during%20the%20first%20pre-training%20stage%2C%0Aa%20spatial-temporal%20knowledge%20graph%20%28STKG%29%20encoder%20is%20trained%20to%20extract%0Ahigh-order%20spatial-temporal%20dependencies%20and%20collaborative%20associations%20from%0Athe%20STKG.%20During%20the%20second%20spatial-temporal%20knowledge%20distillation%20%28STKD%29%0Astage%2C%20a%20spatial-temporal%20Transformer%20%28ST-Transformer%29%20is%20employed%20to%0Acomprehensively%20model%20dynamic%20user%20preferences%20on%20various%20types%20of%20fine-grained%0Ageospatial%20information%20from%20a%20sequential%20perspective.%20Furthermore%2C%20the%20STKD%0Astrategy%20is%20introduced%20to%20transfer%20graph-based%20spatial-temporal%20knowledge%20to%0Athe%20ST-Transformer%2C%20facilitating%20the%20adaptive%20fusion%20of%20rich%20knowledge%20derived%0Afrom%20both%20the%20STKG%20and%20sequence%20data%20while%20reducing%20computational%20overhead.%0AExtensive%20experiments%20on%20three%20real-world%20datasets%20show%20that%20STKDRec%0Asignificantly%20outperforms%20the%20state-of-the-art%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.16502v2&entry.124074799=Read"},
{"title": "L-PR: Exploiting LiDAR Fiducial Marker for Unordered Low Overlap\n  Multiview Point Cloud Registration", "author": "Yibo Liu and Jinjun Shan and Amaldev Haridevan and Shuo Zhang", "abstract": "  Point cloud registration is a prerequisite for many applications in computer\nvision and robotics. Most existing methods focus on pairwise registration of\ntwo point clouds with high overlap. Although there have been some methods for\nlow overlap cases, they struggle in degraded scenarios. This paper introduces a\nnovel framework dubbed L-PR, designed to register unordered low overlap\nmultiview point clouds leveraging LiDAR fiducial markers. We refer to them as\nLiDAR fiducial markers, but they are the same as the popular AprilTag and ArUco\nmarkers, thin sheets of paper that do not affect the 3D geometry of the\nenvironment. We first propose an improved adaptive threshold marker detection\nmethod to provide robust detection results when the viewpoints among point\nclouds change dramatically. Then, we formulate the unordered multiview point\ncloud registration problem as a maximum a-posteriori (MAP) problem and develop\na framework consisting of two levels of graphs to address it. The first-level\ngraph, constructed as a weighted graph, is designed to efficiently and\noptimally infer initial values of scan poses from the unordered set. The\nsecond-level graph is constructed as a factor graph. By globally optimizing the\nvariables on the graph, including scan poses, marker poses, and marker corner\npositions, we tackle the MAP problem. We conduct both qualitative and\nquantitative experiments to demonstrate that the proposed method surpasses\nprevious state-of-the-art (SOTA) methods and to showcase that L-PR can serve as\na low-cost and efficient tool for 3D asset collection and training data\ncollection. In particular, we collect a new dataset named Livox-3DMatch using\nL-PR and incorporate it into the training of the SOTA learning-based method,\nSGHR, which brings evident improvements for SGHR on various benchmarks.\n", "link": "http://arxiv.org/abs/2406.03298v3", "date": "2025-02-05", "relevancy": 2.3638, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6133}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5854}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20L-PR%3A%20Exploiting%20LiDAR%20Fiducial%20Marker%20for%20Unordered%20Low%20Overlap%0A%20%20Multiview%20Point%20Cloud%20Registration&body=Title%3A%20L-PR%3A%20Exploiting%20LiDAR%20Fiducial%20Marker%20for%20Unordered%20Low%20Overlap%0A%20%20Multiview%20Point%20Cloud%20Registration%0AAuthor%3A%20Yibo%20Liu%20and%20Jinjun%20Shan%20and%20Amaldev%20Haridevan%20and%20Shuo%20Zhang%0AAbstract%3A%20%20%20Point%20cloud%20registration%20is%20a%20prerequisite%20for%20many%20applications%20in%20computer%0Avision%20and%20robotics.%20Most%20existing%20methods%20focus%20on%20pairwise%20registration%20of%0Atwo%20point%20clouds%20with%20high%20overlap.%20Although%20there%20have%20been%20some%20methods%20for%0Alow%20overlap%20cases%2C%20they%20struggle%20in%20degraded%20scenarios.%20This%20paper%20introduces%20a%0Anovel%20framework%20dubbed%20L-PR%2C%20designed%20to%20register%20unordered%20low%20overlap%0Amultiview%20point%20clouds%20leveraging%20LiDAR%20fiducial%20markers.%20We%20refer%20to%20them%20as%0ALiDAR%20fiducial%20markers%2C%20but%20they%20are%20the%20same%20as%20the%20popular%20AprilTag%20and%20ArUco%0Amarkers%2C%20thin%20sheets%20of%20paper%20that%20do%20not%20affect%20the%203D%20geometry%20of%20the%0Aenvironment.%20We%20first%20propose%20an%20improved%20adaptive%20threshold%20marker%20detection%0Amethod%20to%20provide%20robust%20detection%20results%20when%20the%20viewpoints%20among%20point%0Aclouds%20change%20dramatically.%20Then%2C%20we%20formulate%20the%20unordered%20multiview%20point%0Acloud%20registration%20problem%20as%20a%20maximum%20a-posteriori%20%28MAP%29%20problem%20and%20develop%0Aa%20framework%20consisting%20of%20two%20levels%20of%20graphs%20to%20address%20it.%20The%20first-level%0Agraph%2C%20constructed%20as%20a%20weighted%20graph%2C%20is%20designed%20to%20efficiently%20and%0Aoptimally%20infer%20initial%20values%20of%20scan%20poses%20from%20the%20unordered%20set.%20The%0Asecond-level%20graph%20is%20constructed%20as%20a%20factor%20graph.%20By%20globally%20optimizing%20the%0Avariables%20on%20the%20graph%2C%20including%20scan%20poses%2C%20marker%20poses%2C%20and%20marker%20corner%0Apositions%2C%20we%20tackle%20the%20MAP%20problem.%20We%20conduct%20both%20qualitative%20and%0Aquantitative%20experiments%20to%20demonstrate%20that%20the%20proposed%20method%20surpasses%0Aprevious%20state-of-the-art%20%28SOTA%29%20methods%20and%20to%20showcase%20that%20L-PR%20can%20serve%20as%0Aa%20low-cost%20and%20efficient%20tool%20for%203D%20asset%20collection%20and%20training%20data%0Acollection.%20In%20particular%2C%20we%20collect%20a%20new%20dataset%20named%20Livox-3DMatch%20using%0AL-PR%20and%20incorporate%20it%20into%20the%20training%20of%20the%20SOTA%20learning-based%20method%2C%0ASGHR%2C%20which%20brings%20evident%20improvements%20for%20SGHR%20on%20various%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03298v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DL-PR%253A%2520Exploiting%2520LiDAR%2520Fiducial%2520Marker%2520for%2520Unordered%2520Low%2520Overlap%250A%2520%2520Multiview%2520Point%2520Cloud%2520Registration%26entry.906535625%3DYibo%2520Liu%2520and%2520Jinjun%2520Shan%2520and%2520Amaldev%2520Haridevan%2520and%2520Shuo%2520Zhang%26entry.1292438233%3D%2520%2520Point%2520cloud%2520registration%2520is%2520a%2520prerequisite%2520for%2520many%2520applications%2520in%2520computer%250Avision%2520and%2520robotics.%2520Most%2520existing%2520methods%2520focus%2520on%2520pairwise%2520registration%2520of%250Atwo%2520point%2520clouds%2520with%2520high%2520overlap.%2520Although%2520there%2520have%2520been%2520some%2520methods%2520for%250Alow%2520overlap%2520cases%252C%2520they%2520struggle%2520in%2520degraded%2520scenarios.%2520This%2520paper%2520introduces%2520a%250Anovel%2520framework%2520dubbed%2520L-PR%252C%2520designed%2520to%2520register%2520unordered%2520low%2520overlap%250Amultiview%2520point%2520clouds%2520leveraging%2520LiDAR%2520fiducial%2520markers.%2520We%2520refer%2520to%2520them%2520as%250ALiDAR%2520fiducial%2520markers%252C%2520but%2520they%2520are%2520the%2520same%2520as%2520the%2520popular%2520AprilTag%2520and%2520ArUco%250Amarkers%252C%2520thin%2520sheets%2520of%2520paper%2520that%2520do%2520not%2520affect%2520the%25203D%2520geometry%2520of%2520the%250Aenvironment.%2520We%2520first%2520propose%2520an%2520improved%2520adaptive%2520threshold%2520marker%2520detection%250Amethod%2520to%2520provide%2520robust%2520detection%2520results%2520when%2520the%2520viewpoints%2520among%2520point%250Aclouds%2520change%2520dramatically.%2520Then%252C%2520we%2520formulate%2520the%2520unordered%2520multiview%2520point%250Acloud%2520registration%2520problem%2520as%2520a%2520maximum%2520a-posteriori%2520%2528MAP%2529%2520problem%2520and%2520develop%250Aa%2520framework%2520consisting%2520of%2520two%2520levels%2520of%2520graphs%2520to%2520address%2520it.%2520The%2520first-level%250Agraph%252C%2520constructed%2520as%2520a%2520weighted%2520graph%252C%2520is%2520designed%2520to%2520efficiently%2520and%250Aoptimally%2520infer%2520initial%2520values%2520of%2520scan%2520poses%2520from%2520the%2520unordered%2520set.%2520The%250Asecond-level%2520graph%2520is%2520constructed%2520as%2520a%2520factor%2520graph.%2520By%2520globally%2520optimizing%2520the%250Avariables%2520on%2520the%2520graph%252C%2520including%2520scan%2520poses%252C%2520marker%2520poses%252C%2520and%2520marker%2520corner%250Apositions%252C%2520we%2520tackle%2520the%2520MAP%2520problem.%2520We%2520conduct%2520both%2520qualitative%2520and%250Aquantitative%2520experiments%2520to%2520demonstrate%2520that%2520the%2520proposed%2520method%2520surpasses%250Aprevious%2520state-of-the-art%2520%2528SOTA%2529%2520methods%2520and%2520to%2520showcase%2520that%2520L-PR%2520can%2520serve%2520as%250Aa%2520low-cost%2520and%2520efficient%2520tool%2520for%25203D%2520asset%2520collection%2520and%2520training%2520data%250Acollection.%2520In%2520particular%252C%2520we%2520collect%2520a%2520new%2520dataset%2520named%2520Livox-3DMatch%2520using%250AL-PR%2520and%2520incorporate%2520it%2520into%2520the%2520training%2520of%2520the%2520SOTA%2520learning-based%2520method%252C%250ASGHR%252C%2520which%2520brings%2520evident%2520improvements%2520for%2520SGHR%2520on%2520various%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03298v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=L-PR%3A%20Exploiting%20LiDAR%20Fiducial%20Marker%20for%20Unordered%20Low%20Overlap%0A%20%20Multiview%20Point%20Cloud%20Registration&entry.906535625=Yibo%20Liu%20and%20Jinjun%20Shan%20and%20Amaldev%20Haridevan%20and%20Shuo%20Zhang&entry.1292438233=%20%20Point%20cloud%20registration%20is%20a%20prerequisite%20for%20many%20applications%20in%20computer%0Avision%20and%20robotics.%20Most%20existing%20methods%20focus%20on%20pairwise%20registration%20of%0Atwo%20point%20clouds%20with%20high%20overlap.%20Although%20there%20have%20been%20some%20methods%20for%0Alow%20overlap%20cases%2C%20they%20struggle%20in%20degraded%20scenarios.%20This%20paper%20introduces%20a%0Anovel%20framework%20dubbed%20L-PR%2C%20designed%20to%20register%20unordered%20low%20overlap%0Amultiview%20point%20clouds%20leveraging%20LiDAR%20fiducial%20markers.%20We%20refer%20to%20them%20as%0ALiDAR%20fiducial%20markers%2C%20but%20they%20are%20the%20same%20as%20the%20popular%20AprilTag%20and%20ArUco%0Amarkers%2C%20thin%20sheets%20of%20paper%20that%20do%20not%20affect%20the%203D%20geometry%20of%20the%0Aenvironment.%20We%20first%20propose%20an%20improved%20adaptive%20threshold%20marker%20detection%0Amethod%20to%20provide%20robust%20detection%20results%20when%20the%20viewpoints%20among%20point%0Aclouds%20change%20dramatically.%20Then%2C%20we%20formulate%20the%20unordered%20multiview%20point%0Acloud%20registration%20problem%20as%20a%20maximum%20a-posteriori%20%28MAP%29%20problem%20and%20develop%0Aa%20framework%20consisting%20of%20two%20levels%20of%20graphs%20to%20address%20it.%20The%20first-level%0Agraph%2C%20constructed%20as%20a%20weighted%20graph%2C%20is%20designed%20to%20efficiently%20and%0Aoptimally%20infer%20initial%20values%20of%20scan%20poses%20from%20the%20unordered%20set.%20The%0Asecond-level%20graph%20is%20constructed%20as%20a%20factor%20graph.%20By%20globally%20optimizing%20the%0Avariables%20on%20the%20graph%2C%20including%20scan%20poses%2C%20marker%20poses%2C%20and%20marker%20corner%0Apositions%2C%20we%20tackle%20the%20MAP%20problem.%20We%20conduct%20both%20qualitative%20and%0Aquantitative%20experiments%20to%20demonstrate%20that%20the%20proposed%20method%20surpasses%0Aprevious%20state-of-the-art%20%28SOTA%29%20methods%20and%20to%20showcase%20that%20L-PR%20can%20serve%20as%0Aa%20low-cost%20and%20efficient%20tool%20for%203D%20asset%20collection%20and%20training%20data%0Acollection.%20In%20particular%2C%20we%20collect%20a%20new%20dataset%20named%20Livox-3DMatch%20using%0AL-PR%20and%20incorporate%20it%20into%20the%20training%20of%20the%20SOTA%20learning-based%20method%2C%0ASGHR%2C%20which%20brings%20evident%20improvements%20for%20SGHR%20on%20various%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03298v3&entry.124074799=Read"},
{"title": "Learning Effective NeRFs and SDFs Representations with 3D Generative\n  Adversarial Networks for 3D Object Generation", "author": "Zheyuan Yang and Yibo Liu and Guile Wu and Tongtong Cao and Yuan Ren and Yang Liu and Bingbing Liu", "abstract": "  We present a solution for 3D object generation of ICCV 2023 OmniObject3D\nChallenge. In recent years, 3D object generation has made great process and\nachieved promising results, but it remains a challenging task due to the\ndifficulty of generating complex, textured, and high-fidelity results. To\nresolve this problem, we study learning effective NeRFs and SDFs\nrepresentations with 3D Generative Adversarial Networks (GANs) for 3D object\ngeneration. Specifically, inspired by recent works, we use the efficient\ngeometry-aware 3D GANs as the backbone incorporating with label embedding and\ncolor mapping, which enables to train the model on different taxonomies\nsimultaneously. Then, through a decoder, we aggregate the resulting features to\ngenerate Neural Radiance Fields (NeRFs) based representations for rendering\nhigh-fidelity synthetic images. Meanwhile, we optimize Signed Distance\nFunctions (SDFs) to effectively represent objects with 3D meshes. Besides, we\nobserve that this model can be effectively trained with only a few images of\neach object from a variety of classes, instead of using a great number of\nimages per object or training one model per class. With this pipeline, we can\noptimize an effective model for 3D object generation. This solution is among\nthe top 3 in the ICCV 2023 OmniObject3D Challenge.\n", "link": "http://arxiv.org/abs/2309.16110v2", "date": "2025-02-05", "relevancy": 2.3596, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.633}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5813}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Effective%20NeRFs%20and%20SDFs%20Representations%20with%203D%20Generative%0A%20%20Adversarial%20Networks%20for%203D%20Object%20Generation&body=Title%3A%20Learning%20Effective%20NeRFs%20and%20SDFs%20Representations%20with%203D%20Generative%0A%20%20Adversarial%20Networks%20for%203D%20Object%20Generation%0AAuthor%3A%20Zheyuan%20Yang%20and%20Yibo%20Liu%20and%20Guile%20Wu%20and%20Tongtong%20Cao%20and%20Yuan%20Ren%20and%20Yang%20Liu%20and%20Bingbing%20Liu%0AAbstract%3A%20%20%20We%20present%20a%20solution%20for%203D%20object%20generation%20of%20ICCV%202023%20OmniObject3D%0AChallenge.%20In%20recent%20years%2C%203D%20object%20generation%20has%20made%20great%20process%20and%0Aachieved%20promising%20results%2C%20but%20it%20remains%20a%20challenging%20task%20due%20to%20the%0Adifficulty%20of%20generating%20complex%2C%20textured%2C%20and%20high-fidelity%20results.%20To%0Aresolve%20this%20problem%2C%20we%20study%20learning%20effective%20NeRFs%20and%20SDFs%0Arepresentations%20with%203D%20Generative%20Adversarial%20Networks%20%28GANs%29%20for%203D%20object%0Ageneration.%20Specifically%2C%20inspired%20by%20recent%20works%2C%20we%20use%20the%20efficient%0Ageometry-aware%203D%20GANs%20as%20the%20backbone%20incorporating%20with%20label%20embedding%20and%0Acolor%20mapping%2C%20which%20enables%20to%20train%20the%20model%20on%20different%20taxonomies%0Asimultaneously.%20Then%2C%20through%20a%20decoder%2C%20we%20aggregate%20the%20resulting%20features%20to%0Agenerate%20Neural%20Radiance%20Fields%20%28NeRFs%29%20based%20representations%20for%20rendering%0Ahigh-fidelity%20synthetic%20images.%20Meanwhile%2C%20we%20optimize%20Signed%20Distance%0AFunctions%20%28SDFs%29%20to%20effectively%20represent%20objects%20with%203D%20meshes.%20Besides%2C%20we%0Aobserve%20that%20this%20model%20can%20be%20effectively%20trained%20with%20only%20a%20few%20images%20of%0Aeach%20object%20from%20a%20variety%20of%20classes%2C%20instead%20of%20using%20a%20great%20number%20of%0Aimages%20per%20object%20or%20training%20one%20model%20per%20class.%20With%20this%20pipeline%2C%20we%20can%0Aoptimize%20an%20effective%20model%20for%203D%20object%20generation.%20This%20solution%20is%20among%0Athe%20top%203%20in%20the%20ICCV%202023%20OmniObject3D%20Challenge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.16110v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Effective%2520NeRFs%2520and%2520SDFs%2520Representations%2520with%25203D%2520Generative%250A%2520%2520Adversarial%2520Networks%2520for%25203D%2520Object%2520Generation%26entry.906535625%3DZheyuan%2520Yang%2520and%2520Yibo%2520Liu%2520and%2520Guile%2520Wu%2520and%2520Tongtong%2520Cao%2520and%2520Yuan%2520Ren%2520and%2520Yang%2520Liu%2520and%2520Bingbing%2520Liu%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520solution%2520for%25203D%2520object%2520generation%2520of%2520ICCV%25202023%2520OmniObject3D%250AChallenge.%2520In%2520recent%2520years%252C%25203D%2520object%2520generation%2520has%2520made%2520great%2520process%2520and%250Aachieved%2520promising%2520results%252C%2520but%2520it%2520remains%2520a%2520challenging%2520task%2520due%2520to%2520the%250Adifficulty%2520of%2520generating%2520complex%252C%2520textured%252C%2520and%2520high-fidelity%2520results.%2520To%250Aresolve%2520this%2520problem%252C%2520we%2520study%2520learning%2520effective%2520NeRFs%2520and%2520SDFs%250Arepresentations%2520with%25203D%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%2520for%25203D%2520object%250Ageneration.%2520Specifically%252C%2520inspired%2520by%2520recent%2520works%252C%2520we%2520use%2520the%2520efficient%250Ageometry-aware%25203D%2520GANs%2520as%2520the%2520backbone%2520incorporating%2520with%2520label%2520embedding%2520and%250Acolor%2520mapping%252C%2520which%2520enables%2520to%2520train%2520the%2520model%2520on%2520different%2520taxonomies%250Asimultaneously.%2520Then%252C%2520through%2520a%2520decoder%252C%2520we%2520aggregate%2520the%2520resulting%2520features%2520to%250Agenerate%2520Neural%2520Radiance%2520Fields%2520%2528NeRFs%2529%2520based%2520representations%2520for%2520rendering%250Ahigh-fidelity%2520synthetic%2520images.%2520Meanwhile%252C%2520we%2520optimize%2520Signed%2520Distance%250AFunctions%2520%2528SDFs%2529%2520to%2520effectively%2520represent%2520objects%2520with%25203D%2520meshes.%2520Besides%252C%2520we%250Aobserve%2520that%2520this%2520model%2520can%2520be%2520effectively%2520trained%2520with%2520only%2520a%2520few%2520images%2520of%250Aeach%2520object%2520from%2520a%2520variety%2520of%2520classes%252C%2520instead%2520of%2520using%2520a%2520great%2520number%2520of%250Aimages%2520per%2520object%2520or%2520training%2520one%2520model%2520per%2520class.%2520With%2520this%2520pipeline%252C%2520we%2520can%250Aoptimize%2520an%2520effective%2520model%2520for%25203D%2520object%2520generation.%2520This%2520solution%2520is%2520among%250Athe%2520top%25203%2520in%2520the%2520ICCV%25202023%2520OmniObject3D%2520Challenge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.16110v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Effective%20NeRFs%20and%20SDFs%20Representations%20with%203D%20Generative%0A%20%20Adversarial%20Networks%20for%203D%20Object%20Generation&entry.906535625=Zheyuan%20Yang%20and%20Yibo%20Liu%20and%20Guile%20Wu%20and%20Tongtong%20Cao%20and%20Yuan%20Ren%20and%20Yang%20Liu%20and%20Bingbing%20Liu&entry.1292438233=%20%20We%20present%20a%20solution%20for%203D%20object%20generation%20of%20ICCV%202023%20OmniObject3D%0AChallenge.%20In%20recent%20years%2C%203D%20object%20generation%20has%20made%20great%20process%20and%0Aachieved%20promising%20results%2C%20but%20it%20remains%20a%20challenging%20task%20due%20to%20the%0Adifficulty%20of%20generating%20complex%2C%20textured%2C%20and%20high-fidelity%20results.%20To%0Aresolve%20this%20problem%2C%20we%20study%20learning%20effective%20NeRFs%20and%20SDFs%0Arepresentations%20with%203D%20Generative%20Adversarial%20Networks%20%28GANs%29%20for%203D%20object%0Ageneration.%20Specifically%2C%20inspired%20by%20recent%20works%2C%20we%20use%20the%20efficient%0Ageometry-aware%203D%20GANs%20as%20the%20backbone%20incorporating%20with%20label%20embedding%20and%0Acolor%20mapping%2C%20which%20enables%20to%20train%20the%20model%20on%20different%20taxonomies%0Asimultaneously.%20Then%2C%20through%20a%20decoder%2C%20we%20aggregate%20the%20resulting%20features%20to%0Agenerate%20Neural%20Radiance%20Fields%20%28NeRFs%29%20based%20representations%20for%20rendering%0Ahigh-fidelity%20synthetic%20images.%20Meanwhile%2C%20we%20optimize%20Signed%20Distance%0AFunctions%20%28SDFs%29%20to%20effectively%20represent%20objects%20with%203D%20meshes.%20Besides%2C%20we%0Aobserve%20that%20this%20model%20can%20be%20effectively%20trained%20with%20only%20a%20few%20images%20of%0Aeach%20object%20from%20a%20variety%20of%20classes%2C%20instead%20of%20using%20a%20great%20number%20of%0Aimages%20per%20object%20or%20training%20one%20model%20per%20class.%20With%20this%20pipeline%2C%20we%20can%0Aoptimize%20an%20effective%20model%20for%203D%20object%20generation.%20This%20solution%20is%20among%0Athe%20top%203%20in%20the%20ICCV%202023%20OmniObject3D%20Challenge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.16110v2&entry.124074799=Read"},
{"title": "Text-to-CAD Generation Through Infusing Visual Feedback in Large\n  Language Models", "author": "Ruiyu Wang and Yu Yuan and Shizhao Sun and Jiang Bian", "abstract": "  Creating Computer-Aided Design (CAD) models requires significant expertise\nand effort. Text-to-CAD, which converts textual descriptions into CAD\nparametric sequences, is crucial in streamlining this process. Recent studies\nhave utilized ground-truth parametric sequences, known as sequential signals,\nas supervision to achieve this goal. However, CAD models are inherently\nmultimodal, comprising parametric sequences and corresponding rendered visual\nobjects. Besides,the rendering process from parametric sequences to visual\nobjects is many-to-one. Therefore, both sequential and visual signals are\ncritical for effective training. In this work, we introduce CADFusion, a\nframework that uses Large Language Models (LLMs) as the backbone and alternates\nbetween two training stages: the sequential learning (SL) stage and the visual\nfeedback (VF) stage. In the SL stage, we train LLMs using ground-truth\nparametric sequences, enabling the generation of logically coherent parametric\nsequences. In the VF stage, we reward parametric sequences that render into\nvisually preferred objects and penalize those that do not, allowing LLMs to\nlearn how rendered visual objects are perceived and evaluated. These two stages\nalternate throughout the training, ensuring balanced learning and preserving\nbenefits of both signals. Experiments demonstrate that CADFusion significantly\nimproves performance, both qualitatively and quantitatively.\n", "link": "http://arxiv.org/abs/2501.19054v2", "date": "2025-02-05", "relevancy": 2.344, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6013}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5829}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5829}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text-to-CAD%20Generation%20Through%20Infusing%20Visual%20Feedback%20in%20Large%0A%20%20Language%20Models&body=Title%3A%20Text-to-CAD%20Generation%20Through%20Infusing%20Visual%20Feedback%20in%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Ruiyu%20Wang%20and%20Yu%20Yuan%20and%20Shizhao%20Sun%20and%20Jiang%20Bian%0AAbstract%3A%20%20%20Creating%20Computer-Aided%20Design%20%28CAD%29%20models%20requires%20significant%20expertise%0Aand%20effort.%20Text-to-CAD%2C%20which%20converts%20textual%20descriptions%20into%20CAD%0Aparametric%20sequences%2C%20is%20crucial%20in%20streamlining%20this%20process.%20Recent%20studies%0Ahave%20utilized%20ground-truth%20parametric%20sequences%2C%20known%20as%20sequential%20signals%2C%0Aas%20supervision%20to%20achieve%20this%20goal.%20However%2C%20CAD%20models%20are%20inherently%0Amultimodal%2C%20comprising%20parametric%20sequences%20and%20corresponding%20rendered%20visual%0Aobjects.%20Besides%2Cthe%20rendering%20process%20from%20parametric%20sequences%20to%20visual%0Aobjects%20is%20many-to-one.%20Therefore%2C%20both%20sequential%20and%20visual%20signals%20are%0Acritical%20for%20effective%20training.%20In%20this%20work%2C%20we%20introduce%20CADFusion%2C%20a%0Aframework%20that%20uses%20Large%20Language%20Models%20%28LLMs%29%20as%20the%20backbone%20and%20alternates%0Abetween%20two%20training%20stages%3A%20the%20sequential%20learning%20%28SL%29%20stage%20and%20the%20visual%0Afeedback%20%28VF%29%20stage.%20In%20the%20SL%20stage%2C%20we%20train%20LLMs%20using%20ground-truth%0Aparametric%20sequences%2C%20enabling%20the%20generation%20of%20logically%20coherent%20parametric%0Asequences.%20In%20the%20VF%20stage%2C%20we%20reward%20parametric%20sequences%20that%20render%20into%0Avisually%20preferred%20objects%20and%20penalize%20those%20that%20do%20not%2C%20allowing%20LLMs%20to%0Alearn%20how%20rendered%20visual%20objects%20are%20perceived%20and%20evaluated.%20These%20two%20stages%0Aalternate%20throughout%20the%20training%2C%20ensuring%20balanced%20learning%20and%20preserving%0Abenefits%20of%20both%20signals.%20Experiments%20demonstrate%20that%20CADFusion%20significantly%0Aimproves%20performance%2C%20both%20qualitatively%20and%20quantitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.19054v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText-to-CAD%2520Generation%2520Through%2520Infusing%2520Visual%2520Feedback%2520in%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DRuiyu%2520Wang%2520and%2520Yu%2520Yuan%2520and%2520Shizhao%2520Sun%2520and%2520Jiang%2520Bian%26entry.1292438233%3D%2520%2520Creating%2520Computer-Aided%2520Design%2520%2528CAD%2529%2520models%2520requires%2520significant%2520expertise%250Aand%2520effort.%2520Text-to-CAD%252C%2520which%2520converts%2520textual%2520descriptions%2520into%2520CAD%250Aparametric%2520sequences%252C%2520is%2520crucial%2520in%2520streamlining%2520this%2520process.%2520Recent%2520studies%250Ahave%2520utilized%2520ground-truth%2520parametric%2520sequences%252C%2520known%2520as%2520sequential%2520signals%252C%250Aas%2520supervision%2520to%2520achieve%2520this%2520goal.%2520However%252C%2520CAD%2520models%2520are%2520inherently%250Amultimodal%252C%2520comprising%2520parametric%2520sequences%2520and%2520corresponding%2520rendered%2520visual%250Aobjects.%2520Besides%252Cthe%2520rendering%2520process%2520from%2520parametric%2520sequences%2520to%2520visual%250Aobjects%2520is%2520many-to-one.%2520Therefore%252C%2520both%2520sequential%2520and%2520visual%2520signals%2520are%250Acritical%2520for%2520effective%2520training.%2520In%2520this%2520work%252C%2520we%2520introduce%2520CADFusion%252C%2520a%250Aframework%2520that%2520uses%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520as%2520the%2520backbone%2520and%2520alternates%250Abetween%2520two%2520training%2520stages%253A%2520the%2520sequential%2520learning%2520%2528SL%2529%2520stage%2520and%2520the%2520visual%250Afeedback%2520%2528VF%2529%2520stage.%2520In%2520the%2520SL%2520stage%252C%2520we%2520train%2520LLMs%2520using%2520ground-truth%250Aparametric%2520sequences%252C%2520enabling%2520the%2520generation%2520of%2520logically%2520coherent%2520parametric%250Asequences.%2520In%2520the%2520VF%2520stage%252C%2520we%2520reward%2520parametric%2520sequences%2520that%2520render%2520into%250Avisually%2520preferred%2520objects%2520and%2520penalize%2520those%2520that%2520do%2520not%252C%2520allowing%2520LLMs%2520to%250Alearn%2520how%2520rendered%2520visual%2520objects%2520are%2520perceived%2520and%2520evaluated.%2520These%2520two%2520stages%250Aalternate%2520throughout%2520the%2520training%252C%2520ensuring%2520balanced%2520learning%2520and%2520preserving%250Abenefits%2520of%2520both%2520signals.%2520Experiments%2520demonstrate%2520that%2520CADFusion%2520significantly%250Aimproves%2520performance%252C%2520both%2520qualitatively%2520and%2520quantitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.19054v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text-to-CAD%20Generation%20Through%20Infusing%20Visual%20Feedback%20in%20Large%0A%20%20Language%20Models&entry.906535625=Ruiyu%20Wang%20and%20Yu%20Yuan%20and%20Shizhao%20Sun%20and%20Jiang%20Bian&entry.1292438233=%20%20Creating%20Computer-Aided%20Design%20%28CAD%29%20models%20requires%20significant%20expertise%0Aand%20effort.%20Text-to-CAD%2C%20which%20converts%20textual%20descriptions%20into%20CAD%0Aparametric%20sequences%2C%20is%20crucial%20in%20streamlining%20this%20process.%20Recent%20studies%0Ahave%20utilized%20ground-truth%20parametric%20sequences%2C%20known%20as%20sequential%20signals%2C%0Aas%20supervision%20to%20achieve%20this%20goal.%20However%2C%20CAD%20models%20are%20inherently%0Amultimodal%2C%20comprising%20parametric%20sequences%20and%20corresponding%20rendered%20visual%0Aobjects.%20Besides%2Cthe%20rendering%20process%20from%20parametric%20sequences%20to%20visual%0Aobjects%20is%20many-to-one.%20Therefore%2C%20both%20sequential%20and%20visual%20signals%20are%0Acritical%20for%20effective%20training.%20In%20this%20work%2C%20we%20introduce%20CADFusion%2C%20a%0Aframework%20that%20uses%20Large%20Language%20Models%20%28LLMs%29%20as%20the%20backbone%20and%20alternates%0Abetween%20two%20training%20stages%3A%20the%20sequential%20learning%20%28SL%29%20stage%20and%20the%20visual%0Afeedback%20%28VF%29%20stage.%20In%20the%20SL%20stage%2C%20we%20train%20LLMs%20using%20ground-truth%0Aparametric%20sequences%2C%20enabling%20the%20generation%20of%20logically%20coherent%20parametric%0Asequences.%20In%20the%20VF%20stage%2C%20we%20reward%20parametric%20sequences%20that%20render%20into%0Avisually%20preferred%20objects%20and%20penalize%20those%20that%20do%20not%2C%20allowing%20LLMs%20to%0Alearn%20how%20rendered%20visual%20objects%20are%20perceived%20and%20evaluated.%20These%20two%20stages%0Aalternate%20throughout%20the%20training%2C%20ensuring%20balanced%20learning%20and%20preserving%0Abenefits%20of%20both%20signals.%20Experiments%20demonstrate%20that%20CADFusion%20significantly%0Aimproves%20performance%2C%20both%20qualitatively%20and%20quantitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.19054v2&entry.124074799=Read"},
{"title": "A Theoretical Analysis of Self-Supervised Learning for Vision\n  Transformers", "author": "Yu Huang and Zixin Wen and Yuejie Chi and Yingbin Liang", "abstract": "  Self-supervised learning has become a cornerstone in computer vision,\nprimarily divided into reconstruction-based methods like masked autoencoders\n(MAE) and discriminative methods such as contrastive learning (CL). Recent\nempirical observations reveal that MAE and CL capture different types of\nrepresentations: CL tends to focus on global patterns, while MAE adeptly\ncaptures both global and subtle local information simultaneously. Despite a\nflurry of recent empirical investigations to shed light on this difference,\ntheoretical understanding remains limited, especially on the dominant\narchitecture vision transformers (ViTs). In this paper, to provide rigorous\ninsights, we model the visual data distribution by considering two types of\nspatial features: dominant global features and comparatively minuscule local\nfeatures, and study the impact of imbalance among these features. We analyze\nthe training dynamics of one-layer softmax-based ViTs on both MAE and CL\nobjectives using gradient descent. Our analysis shows that as the degree of\nfeature imbalance varies, ViTs trained with the MAE objective effectively learn\nboth global and local features to achieve near-optimal reconstruction, while\nthe CL-trained ViTs favor predominantly global features, even under mild\nimbalance. These results provide a theoretical explanation for distinct\nbehaviors of MAE and CL observed in empirical studies.\n", "link": "http://arxiv.org/abs/2403.02233v3", "date": "2025-02-05", "relevancy": 2.3255, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6137}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5602}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Theoretical%20Analysis%20of%20Self-Supervised%20Learning%20for%20Vision%0A%20%20Transformers&body=Title%3A%20A%20Theoretical%20Analysis%20of%20Self-Supervised%20Learning%20for%20Vision%0A%20%20Transformers%0AAuthor%3A%20Yu%20Huang%20and%20Zixin%20Wen%20and%20Yuejie%20Chi%20and%20Yingbin%20Liang%0AAbstract%3A%20%20%20Self-supervised%20learning%20has%20become%20a%20cornerstone%20in%20computer%20vision%2C%0Aprimarily%20divided%20into%20reconstruction-based%20methods%20like%20masked%20autoencoders%0A%28MAE%29%20and%20discriminative%20methods%20such%20as%20contrastive%20learning%20%28CL%29.%20Recent%0Aempirical%20observations%20reveal%20that%20MAE%20and%20CL%20capture%20different%20types%20of%0Arepresentations%3A%20CL%20tends%20to%20focus%20on%20global%20patterns%2C%20while%20MAE%20adeptly%0Acaptures%20both%20global%20and%20subtle%20local%20information%20simultaneously.%20Despite%20a%0Aflurry%20of%20recent%20empirical%20investigations%20to%20shed%20light%20on%20this%20difference%2C%0Atheoretical%20understanding%20remains%20limited%2C%20especially%20on%20the%20dominant%0Aarchitecture%20vision%20transformers%20%28ViTs%29.%20In%20this%20paper%2C%20to%20provide%20rigorous%0Ainsights%2C%20we%20model%20the%20visual%20data%20distribution%20by%20considering%20two%20types%20of%0Aspatial%20features%3A%20dominant%20global%20features%20and%20comparatively%20minuscule%20local%0Afeatures%2C%20and%20study%20the%20impact%20of%20imbalance%20among%20these%20features.%20We%20analyze%0Athe%20training%20dynamics%20of%20one-layer%20softmax-based%20ViTs%20on%20both%20MAE%20and%20CL%0Aobjectives%20using%20gradient%20descent.%20Our%20analysis%20shows%20that%20as%20the%20degree%20of%0Afeature%20imbalance%20varies%2C%20ViTs%20trained%20with%20the%20MAE%20objective%20effectively%20learn%0Aboth%20global%20and%20local%20features%20to%20achieve%20near-optimal%20reconstruction%2C%20while%0Athe%20CL-trained%20ViTs%20favor%20predominantly%20global%20features%2C%20even%20under%20mild%0Aimbalance.%20These%20results%20provide%20a%20theoretical%20explanation%20for%20distinct%0Abehaviors%20of%20MAE%20and%20CL%20observed%20in%20empirical%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.02233v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Theoretical%2520Analysis%2520of%2520Self-Supervised%2520Learning%2520for%2520Vision%250A%2520%2520Transformers%26entry.906535625%3DYu%2520Huang%2520and%2520Zixin%2520Wen%2520and%2520Yuejie%2520Chi%2520and%2520Yingbin%2520Liang%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520has%2520become%2520a%2520cornerstone%2520in%2520computer%2520vision%252C%250Aprimarily%2520divided%2520into%2520reconstruction-based%2520methods%2520like%2520masked%2520autoencoders%250A%2528MAE%2529%2520and%2520discriminative%2520methods%2520such%2520as%2520contrastive%2520learning%2520%2528CL%2529.%2520Recent%250Aempirical%2520observations%2520reveal%2520that%2520MAE%2520and%2520CL%2520capture%2520different%2520types%2520of%250Arepresentations%253A%2520CL%2520tends%2520to%2520focus%2520on%2520global%2520patterns%252C%2520while%2520MAE%2520adeptly%250Acaptures%2520both%2520global%2520and%2520subtle%2520local%2520information%2520simultaneously.%2520Despite%2520a%250Aflurry%2520of%2520recent%2520empirical%2520investigations%2520to%2520shed%2520light%2520on%2520this%2520difference%252C%250Atheoretical%2520understanding%2520remains%2520limited%252C%2520especially%2520on%2520the%2520dominant%250Aarchitecture%2520vision%2520transformers%2520%2528ViTs%2529.%2520In%2520this%2520paper%252C%2520to%2520provide%2520rigorous%250Ainsights%252C%2520we%2520model%2520the%2520visual%2520data%2520distribution%2520by%2520considering%2520two%2520types%2520of%250Aspatial%2520features%253A%2520dominant%2520global%2520features%2520and%2520comparatively%2520minuscule%2520local%250Afeatures%252C%2520and%2520study%2520the%2520impact%2520of%2520imbalance%2520among%2520these%2520features.%2520We%2520analyze%250Athe%2520training%2520dynamics%2520of%2520one-layer%2520softmax-based%2520ViTs%2520on%2520both%2520MAE%2520and%2520CL%250Aobjectives%2520using%2520gradient%2520descent.%2520Our%2520analysis%2520shows%2520that%2520as%2520the%2520degree%2520of%250Afeature%2520imbalance%2520varies%252C%2520ViTs%2520trained%2520with%2520the%2520MAE%2520objective%2520effectively%2520learn%250Aboth%2520global%2520and%2520local%2520features%2520to%2520achieve%2520near-optimal%2520reconstruction%252C%2520while%250Athe%2520CL-trained%2520ViTs%2520favor%2520predominantly%2520global%2520features%252C%2520even%2520under%2520mild%250Aimbalance.%2520These%2520results%2520provide%2520a%2520theoretical%2520explanation%2520for%2520distinct%250Abehaviors%2520of%2520MAE%2520and%2520CL%2520observed%2520in%2520empirical%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.02233v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Theoretical%20Analysis%20of%20Self-Supervised%20Learning%20for%20Vision%0A%20%20Transformers&entry.906535625=Yu%20Huang%20and%20Zixin%20Wen%20and%20Yuejie%20Chi%20and%20Yingbin%20Liang&entry.1292438233=%20%20Self-supervised%20learning%20has%20become%20a%20cornerstone%20in%20computer%20vision%2C%0Aprimarily%20divided%20into%20reconstruction-based%20methods%20like%20masked%20autoencoders%0A%28MAE%29%20and%20discriminative%20methods%20such%20as%20contrastive%20learning%20%28CL%29.%20Recent%0Aempirical%20observations%20reveal%20that%20MAE%20and%20CL%20capture%20different%20types%20of%0Arepresentations%3A%20CL%20tends%20to%20focus%20on%20global%20patterns%2C%20while%20MAE%20adeptly%0Acaptures%20both%20global%20and%20subtle%20local%20information%20simultaneously.%20Despite%20a%0Aflurry%20of%20recent%20empirical%20investigations%20to%20shed%20light%20on%20this%20difference%2C%0Atheoretical%20understanding%20remains%20limited%2C%20especially%20on%20the%20dominant%0Aarchitecture%20vision%20transformers%20%28ViTs%29.%20In%20this%20paper%2C%20to%20provide%20rigorous%0Ainsights%2C%20we%20model%20the%20visual%20data%20distribution%20by%20considering%20two%20types%20of%0Aspatial%20features%3A%20dominant%20global%20features%20and%20comparatively%20minuscule%20local%0Afeatures%2C%20and%20study%20the%20impact%20of%20imbalance%20among%20these%20features.%20We%20analyze%0Athe%20training%20dynamics%20of%20one-layer%20softmax-based%20ViTs%20on%20both%20MAE%20and%20CL%0Aobjectives%20using%20gradient%20descent.%20Our%20analysis%20shows%20that%20as%20the%20degree%20of%0Afeature%20imbalance%20varies%2C%20ViTs%20trained%20with%20the%20MAE%20objective%20effectively%20learn%0Aboth%20global%20and%20local%20features%20to%20achieve%20near-optimal%20reconstruction%2C%20while%0Athe%20CL-trained%20ViTs%20favor%20predominantly%20global%20features%2C%20even%20under%20mild%0Aimbalance.%20These%20results%20provide%20a%20theoretical%20explanation%20for%20distinct%0Abehaviors%20of%20MAE%20and%20CL%20observed%20in%20empirical%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.02233v3&entry.124074799=Read"},
{"title": "Shot2Story: A New Benchmark for Comprehensive Understanding of\n  Multi-shot Videos", "author": "Mingfei Han and Linjie Yang and Xiaojun Chang and Lina Yao and Heng Wang", "abstract": "  A short clip of video may contain progression of multiple events and an\ninteresting story line. A human need to capture both the event in every shot\nand associate them together to understand the story behind it. In this work, we\npresent a new multi-shot video understanding benchmark Shot2Story with detailed\nshot-level captions, comprehensive video summaries and question-answering\npairs. To facilitate better semantic understanding of videos, we provide\ncaptions for both visual signals and human narrations. We design several\ndistinct tasks including single-shot video captioning, multi-shot video\nsummarization, and multi-shot video question answering. Preliminary experiments\nshow some challenges to generate a long and comprehensive video summary for\nmulti-shot videos. Nevertheless, the generated imperfect summaries can already\nachieve competitive performance on existing video understanding tasks such as\nvideo question-answering, promoting an under-explored setting of video\nunderstanding with detailed summaries.\n", "link": "http://arxiv.org/abs/2312.10300v3", "date": "2025-02-05", "relevancy": 2.3145, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5814}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5814}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shot2Story%3A%20A%20New%20Benchmark%20for%20Comprehensive%20Understanding%20of%0A%20%20Multi-shot%20Videos&body=Title%3A%20Shot2Story%3A%20A%20New%20Benchmark%20for%20Comprehensive%20Understanding%20of%0A%20%20Multi-shot%20Videos%0AAuthor%3A%20Mingfei%20Han%20and%20Linjie%20Yang%20and%20Xiaojun%20Chang%20and%20Lina%20Yao%20and%20Heng%20Wang%0AAbstract%3A%20%20%20A%20short%20clip%20of%20video%20may%20contain%20progression%20of%20multiple%20events%20and%20an%0Ainteresting%20story%20line.%20A%20human%20need%20to%20capture%20both%20the%20event%20in%20every%20shot%0Aand%20associate%20them%20together%20to%20understand%20the%20story%20behind%20it.%20In%20this%20work%2C%20we%0Apresent%20a%20new%20multi-shot%20video%20understanding%20benchmark%20Shot2Story%20with%20detailed%0Ashot-level%20captions%2C%20comprehensive%20video%20summaries%20and%20question-answering%0Apairs.%20To%20facilitate%20better%20semantic%20understanding%20of%20videos%2C%20we%20provide%0Acaptions%20for%20both%20visual%20signals%20and%20human%20narrations.%20We%20design%20several%0Adistinct%20tasks%20including%20single-shot%20video%20captioning%2C%20multi-shot%20video%0Asummarization%2C%20and%20multi-shot%20video%20question%20answering.%20Preliminary%20experiments%0Ashow%20some%20challenges%20to%20generate%20a%20long%20and%20comprehensive%20video%20summary%20for%0Amulti-shot%20videos.%20Nevertheless%2C%20the%20generated%20imperfect%20summaries%20can%20already%0Aachieve%20competitive%20performance%20on%20existing%20video%20understanding%20tasks%20such%20as%0Avideo%20question-answering%2C%20promoting%20an%20under-explored%20setting%20of%20video%0Aunderstanding%20with%20detailed%20summaries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.10300v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShot2Story%253A%2520A%2520New%2520Benchmark%2520for%2520Comprehensive%2520Understanding%2520of%250A%2520%2520Multi-shot%2520Videos%26entry.906535625%3DMingfei%2520Han%2520and%2520Linjie%2520Yang%2520and%2520Xiaojun%2520Chang%2520and%2520Lina%2520Yao%2520and%2520Heng%2520Wang%26entry.1292438233%3D%2520%2520A%2520short%2520clip%2520of%2520video%2520may%2520contain%2520progression%2520of%2520multiple%2520events%2520and%2520an%250Ainteresting%2520story%2520line.%2520A%2520human%2520need%2520to%2520capture%2520both%2520the%2520event%2520in%2520every%2520shot%250Aand%2520associate%2520them%2520together%2520to%2520understand%2520the%2520story%2520behind%2520it.%2520In%2520this%2520work%252C%2520we%250Apresent%2520a%2520new%2520multi-shot%2520video%2520understanding%2520benchmark%2520Shot2Story%2520with%2520detailed%250Ashot-level%2520captions%252C%2520comprehensive%2520video%2520summaries%2520and%2520question-answering%250Apairs.%2520To%2520facilitate%2520better%2520semantic%2520understanding%2520of%2520videos%252C%2520we%2520provide%250Acaptions%2520for%2520both%2520visual%2520signals%2520and%2520human%2520narrations.%2520We%2520design%2520several%250Adistinct%2520tasks%2520including%2520single-shot%2520video%2520captioning%252C%2520multi-shot%2520video%250Asummarization%252C%2520and%2520multi-shot%2520video%2520question%2520answering.%2520Preliminary%2520experiments%250Ashow%2520some%2520challenges%2520to%2520generate%2520a%2520long%2520and%2520comprehensive%2520video%2520summary%2520for%250Amulti-shot%2520videos.%2520Nevertheless%252C%2520the%2520generated%2520imperfect%2520summaries%2520can%2520already%250Aachieve%2520competitive%2520performance%2520on%2520existing%2520video%2520understanding%2520tasks%2520such%2520as%250Avideo%2520question-answering%252C%2520promoting%2520an%2520under-explored%2520setting%2520of%2520video%250Aunderstanding%2520with%2520detailed%2520summaries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.10300v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shot2Story%3A%20A%20New%20Benchmark%20for%20Comprehensive%20Understanding%20of%0A%20%20Multi-shot%20Videos&entry.906535625=Mingfei%20Han%20and%20Linjie%20Yang%20and%20Xiaojun%20Chang%20and%20Lina%20Yao%20and%20Heng%20Wang&entry.1292438233=%20%20A%20short%20clip%20of%20video%20may%20contain%20progression%20of%20multiple%20events%20and%20an%0Ainteresting%20story%20line.%20A%20human%20need%20to%20capture%20both%20the%20event%20in%20every%20shot%0Aand%20associate%20them%20together%20to%20understand%20the%20story%20behind%20it.%20In%20this%20work%2C%20we%0Apresent%20a%20new%20multi-shot%20video%20understanding%20benchmark%20Shot2Story%20with%20detailed%0Ashot-level%20captions%2C%20comprehensive%20video%20summaries%20and%20question-answering%0Apairs.%20To%20facilitate%20better%20semantic%20understanding%20of%20videos%2C%20we%20provide%0Acaptions%20for%20both%20visual%20signals%20and%20human%20narrations.%20We%20design%20several%0Adistinct%20tasks%20including%20single-shot%20video%20captioning%2C%20multi-shot%20video%0Asummarization%2C%20and%20multi-shot%20video%20question%20answering.%20Preliminary%20experiments%0Ashow%20some%20challenges%20to%20generate%20a%20long%20and%20comprehensive%20video%20summary%20for%0Amulti-shot%20videos.%20Nevertheless%2C%20the%20generated%20imperfect%20summaries%20can%20already%0Aachieve%20competitive%20performance%20on%20existing%20video%20understanding%20tasks%20such%20as%0Avideo%20question-answering%2C%20promoting%20an%20under-explored%20setting%20of%20video%0Aunderstanding%20with%20detailed%20summaries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.10300v3&entry.124074799=Read"},
{"title": "When Pre-trained Visual Representations Fall Short: Limitations in\n  Visuo-Motor Robot Learning", "author": "Nikolaos Tsagkas and Andreas Sochopoulos and Duolikun Danier and Chris Xiaoxuan Lu and Oisin Mac Aodha", "abstract": "  The integration of pre-trained visual representations (PVRs) into visuo-motor\nrobot learning has emerged as a promising alternative to training visual\nencoders from scratch. However, PVRs face critical challenges in the context of\npolicy learning, including temporal entanglement and an inability to generalise\neven in the presence of minor scene perturbations. These limitations hinder\nperformance in tasks requiring temporal awareness and robustness to scene\nchanges. This work identifies these shortcomings and proposes solutions to\naddress them. First, we augment PVR features with temporal perception and a\nsense of task completion, effectively disentangling them in time. Second, we\nintroduce a module that learns to selectively attend to task-relevant local\nfeatures, enhancing robustness when evaluated on out-of-distribution scenes.\nOur experiments demonstrate significant performance improvements, particularly\nin PVRs trained with masking objectives, and validate the effectiveness of our\nenhancements in addressing PVR-specific limitations.\n", "link": "http://arxiv.org/abs/2502.03270v1", "date": "2025-02-05", "relevancy": 2.3095, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5931}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5742}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Pre-trained%20Visual%20Representations%20Fall%20Short%3A%20Limitations%20in%0A%20%20Visuo-Motor%20Robot%20Learning&body=Title%3A%20When%20Pre-trained%20Visual%20Representations%20Fall%20Short%3A%20Limitations%20in%0A%20%20Visuo-Motor%20Robot%20Learning%0AAuthor%3A%20Nikolaos%20Tsagkas%20and%20Andreas%20Sochopoulos%20and%20Duolikun%20Danier%20and%20Chris%20Xiaoxuan%20Lu%20and%20Oisin%20Mac%20Aodha%0AAbstract%3A%20%20%20The%20integration%20of%20pre-trained%20visual%20representations%20%28PVRs%29%20into%20visuo-motor%0Arobot%20learning%20has%20emerged%20as%20a%20promising%20alternative%20to%20training%20visual%0Aencoders%20from%20scratch.%20However%2C%20PVRs%20face%20critical%20challenges%20in%20the%20context%20of%0Apolicy%20learning%2C%20including%20temporal%20entanglement%20and%20an%20inability%20to%20generalise%0Aeven%20in%20the%20presence%20of%20minor%20scene%20perturbations.%20These%20limitations%20hinder%0Aperformance%20in%20tasks%20requiring%20temporal%20awareness%20and%20robustness%20to%20scene%0Achanges.%20This%20work%20identifies%20these%20shortcomings%20and%20proposes%20solutions%20to%0Aaddress%20them.%20First%2C%20we%20augment%20PVR%20features%20with%20temporal%20perception%20and%20a%0Asense%20of%20task%20completion%2C%20effectively%20disentangling%20them%20in%20time.%20Second%2C%20we%0Aintroduce%20a%20module%20that%20learns%20to%20selectively%20attend%20to%20task-relevant%20local%0Afeatures%2C%20enhancing%20robustness%20when%20evaluated%20on%20out-of-distribution%20scenes.%0AOur%20experiments%20demonstrate%20significant%20performance%20improvements%2C%20particularly%0Ain%20PVRs%20trained%20with%20masking%20objectives%2C%20and%20validate%20the%20effectiveness%20of%20our%0Aenhancements%20in%20addressing%20PVR-specific%20limitations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03270v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Pre-trained%2520Visual%2520Representations%2520Fall%2520Short%253A%2520Limitations%2520in%250A%2520%2520Visuo-Motor%2520Robot%2520Learning%26entry.906535625%3DNikolaos%2520Tsagkas%2520and%2520Andreas%2520Sochopoulos%2520and%2520Duolikun%2520Danier%2520and%2520Chris%2520Xiaoxuan%2520Lu%2520and%2520Oisin%2520Mac%2520Aodha%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520pre-trained%2520visual%2520representations%2520%2528PVRs%2529%2520into%2520visuo-motor%250Arobot%2520learning%2520has%2520emerged%2520as%2520a%2520promising%2520alternative%2520to%2520training%2520visual%250Aencoders%2520from%2520scratch.%2520However%252C%2520PVRs%2520face%2520critical%2520challenges%2520in%2520the%2520context%2520of%250Apolicy%2520learning%252C%2520including%2520temporal%2520entanglement%2520and%2520an%2520inability%2520to%2520generalise%250Aeven%2520in%2520the%2520presence%2520of%2520minor%2520scene%2520perturbations.%2520These%2520limitations%2520hinder%250Aperformance%2520in%2520tasks%2520requiring%2520temporal%2520awareness%2520and%2520robustness%2520to%2520scene%250Achanges.%2520This%2520work%2520identifies%2520these%2520shortcomings%2520and%2520proposes%2520solutions%2520to%250Aaddress%2520them.%2520First%252C%2520we%2520augment%2520PVR%2520features%2520with%2520temporal%2520perception%2520and%2520a%250Asense%2520of%2520task%2520completion%252C%2520effectively%2520disentangling%2520them%2520in%2520time.%2520Second%252C%2520we%250Aintroduce%2520a%2520module%2520that%2520learns%2520to%2520selectively%2520attend%2520to%2520task-relevant%2520local%250Afeatures%252C%2520enhancing%2520robustness%2520when%2520evaluated%2520on%2520out-of-distribution%2520scenes.%250AOur%2520experiments%2520demonstrate%2520significant%2520performance%2520improvements%252C%2520particularly%250Ain%2520PVRs%2520trained%2520with%2520masking%2520objectives%252C%2520and%2520validate%2520the%2520effectiveness%2520of%2520our%250Aenhancements%2520in%2520addressing%2520PVR-specific%2520limitations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03270v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Pre-trained%20Visual%20Representations%20Fall%20Short%3A%20Limitations%20in%0A%20%20Visuo-Motor%20Robot%20Learning&entry.906535625=Nikolaos%20Tsagkas%20and%20Andreas%20Sochopoulos%20and%20Duolikun%20Danier%20and%20Chris%20Xiaoxuan%20Lu%20and%20Oisin%20Mac%20Aodha&entry.1292438233=%20%20The%20integration%20of%20pre-trained%20visual%20representations%20%28PVRs%29%20into%20visuo-motor%0Arobot%20learning%20has%20emerged%20as%20a%20promising%20alternative%20to%20training%20visual%0Aencoders%20from%20scratch.%20However%2C%20PVRs%20face%20critical%20challenges%20in%20the%20context%20of%0Apolicy%20learning%2C%20including%20temporal%20entanglement%20and%20an%20inability%20to%20generalise%0Aeven%20in%20the%20presence%20of%20minor%20scene%20perturbations.%20These%20limitations%20hinder%0Aperformance%20in%20tasks%20requiring%20temporal%20awareness%20and%20robustness%20to%20scene%0Achanges.%20This%20work%20identifies%20these%20shortcomings%20and%20proposes%20solutions%20to%0Aaddress%20them.%20First%2C%20we%20augment%20PVR%20features%20with%20temporal%20perception%20and%20a%0Asense%20of%20task%20completion%2C%20effectively%20disentangling%20them%20in%20time.%20Second%2C%20we%0Aintroduce%20a%20module%20that%20learns%20to%20selectively%20attend%20to%20task-relevant%20local%0Afeatures%2C%20enhancing%20robustness%20when%20evaluated%20on%20out-of-distribution%20scenes.%0AOur%20experiments%20demonstrate%20significant%20performance%20improvements%2C%20particularly%0Ain%20PVRs%20trained%20with%20masking%20objectives%2C%20and%20validate%20the%20effectiveness%20of%20our%0Aenhancements%20in%20addressing%20PVR-specific%20limitations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03270v1&entry.124074799=Read"},
{"title": "AAD-DCE: An Aggregated Multimodal Attention Mechanism for Early and Late\n  Dynamic Contrast Enhanced Prostate MRI Synthesis", "author": "Divya Bharti and Sriprabha Ramanarayanan and Sadhana S and Kishore Kumar M and Keerthi Ram and Harsh Agarwal and Ramesh Venkatesan and Mohanasankar Sivaprakasam", "abstract": "  Dynamic Contrast-Enhanced Magnetic Resonance Imaging (DCE-MRI) is a medical\nimaging technique that plays a crucial role in the detailed visualization and\nidentification of tissue perfusion in abnormal lesions and radiological\nsuggestions for biopsy. However, DCE-MRI involves the administration of a\nGadolinium based (Gad) contrast agent, which is associated with a risk of\ntoxicity in the body. Previous deep learning approaches that synthesize DCE-MR\nimages employ unimodal non-contrast or low-dose contrast MRI images lacking\nfocus on the local perfusion information within the anatomy of interest. We\npropose AAD-DCE, a generative adversarial network (GAN) with an aggregated\nattention discriminator module consisting of global and local discriminators.\nThe discriminators provide a spatial embedded attention map to drive the\ngenerator to synthesize early and late response DCE-MRI images. Our method\nemploys multimodal inputs - T2 weighted (T2W), Apparent Diffusion Coefficient\n(ADC), and T1 pre-contrast for image synthesis. Extensive comparative and\nablation studies on the ProstateX dataset show that our model (i) is agnostic\nto various generator benchmarks and (ii) outperforms other DCE-MRI synthesis\napproaches with improvement margins of +0.64 dB PSNR, +0.0518 SSIM, -0.015 MAE\nfor early response and +0.1 dB PSNR, +0.0424 SSIM, -0.021 MAE for late\nresponse, and (ii) emphasize the importance of attention ensembling. Our code\nis available at https://github.com/bhartidivya/AAD-DCE.\n", "link": "http://arxiv.org/abs/2502.02555v2", "date": "2025-02-05", "relevancy": 2.2946, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5811}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5701}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AAD-DCE%3A%20An%20Aggregated%20Multimodal%20Attention%20Mechanism%20for%20Early%20and%20Late%0A%20%20Dynamic%20Contrast%20Enhanced%20Prostate%20MRI%20Synthesis&body=Title%3A%20AAD-DCE%3A%20An%20Aggregated%20Multimodal%20Attention%20Mechanism%20for%20Early%20and%20Late%0A%20%20Dynamic%20Contrast%20Enhanced%20Prostate%20MRI%20Synthesis%0AAuthor%3A%20Divya%20Bharti%20and%20Sriprabha%20Ramanarayanan%20and%20Sadhana%20S%20and%20Kishore%20Kumar%20M%20and%20Keerthi%20Ram%20and%20Harsh%20Agarwal%20and%20Ramesh%20Venkatesan%20and%20Mohanasankar%20Sivaprakasam%0AAbstract%3A%20%20%20Dynamic%20Contrast-Enhanced%20Magnetic%20Resonance%20Imaging%20%28DCE-MRI%29%20is%20a%20medical%0Aimaging%20technique%20that%20plays%20a%20crucial%20role%20in%20the%20detailed%20visualization%20and%0Aidentification%20of%20tissue%20perfusion%20in%20abnormal%20lesions%20and%20radiological%0Asuggestions%20for%20biopsy.%20However%2C%20DCE-MRI%20involves%20the%20administration%20of%20a%0AGadolinium%20based%20%28Gad%29%20contrast%20agent%2C%20which%20is%20associated%20with%20a%20risk%20of%0Atoxicity%20in%20the%20body.%20Previous%20deep%20learning%20approaches%20that%20synthesize%20DCE-MR%0Aimages%20employ%20unimodal%20non-contrast%20or%20low-dose%20contrast%20MRI%20images%20lacking%0Afocus%20on%20the%20local%20perfusion%20information%20within%20the%20anatomy%20of%20interest.%20We%0Apropose%20AAD-DCE%2C%20a%20generative%20adversarial%20network%20%28GAN%29%20with%20an%20aggregated%0Aattention%20discriminator%20module%20consisting%20of%20global%20and%20local%20discriminators.%0AThe%20discriminators%20provide%20a%20spatial%20embedded%20attention%20map%20to%20drive%20the%0Agenerator%20to%20synthesize%20early%20and%20late%20response%20DCE-MRI%20images.%20Our%20method%0Aemploys%20multimodal%20inputs%20-%20T2%20weighted%20%28T2W%29%2C%20Apparent%20Diffusion%20Coefficient%0A%28ADC%29%2C%20and%20T1%20pre-contrast%20for%20image%20synthesis.%20Extensive%20comparative%20and%0Aablation%20studies%20on%20the%20ProstateX%20dataset%20show%20that%20our%20model%20%28i%29%20is%20agnostic%0Ato%20various%20generator%20benchmarks%20and%20%28ii%29%20outperforms%20other%20DCE-MRI%20synthesis%0Aapproaches%20with%20improvement%20margins%20of%20%2B0.64%20dB%20PSNR%2C%20%2B0.0518%20SSIM%2C%20-0.015%20MAE%0Afor%20early%20response%20and%20%2B0.1%20dB%20PSNR%2C%20%2B0.0424%20SSIM%2C%20-0.021%20MAE%20for%20late%0Aresponse%2C%20and%20%28ii%29%20emphasize%20the%20importance%20of%20attention%20ensembling.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/bhartidivya/AAD-DCE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02555v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAAD-DCE%253A%2520An%2520Aggregated%2520Multimodal%2520Attention%2520Mechanism%2520for%2520Early%2520and%2520Late%250A%2520%2520Dynamic%2520Contrast%2520Enhanced%2520Prostate%2520MRI%2520Synthesis%26entry.906535625%3DDivya%2520Bharti%2520and%2520Sriprabha%2520Ramanarayanan%2520and%2520Sadhana%2520S%2520and%2520Kishore%2520Kumar%2520M%2520and%2520Keerthi%2520Ram%2520and%2520Harsh%2520Agarwal%2520and%2520Ramesh%2520Venkatesan%2520and%2520Mohanasankar%2520Sivaprakasam%26entry.1292438233%3D%2520%2520Dynamic%2520Contrast-Enhanced%2520Magnetic%2520Resonance%2520Imaging%2520%2528DCE-MRI%2529%2520is%2520a%2520medical%250Aimaging%2520technique%2520that%2520plays%2520a%2520crucial%2520role%2520in%2520the%2520detailed%2520visualization%2520and%250Aidentification%2520of%2520tissue%2520perfusion%2520in%2520abnormal%2520lesions%2520and%2520radiological%250Asuggestions%2520for%2520biopsy.%2520However%252C%2520DCE-MRI%2520involves%2520the%2520administration%2520of%2520a%250AGadolinium%2520based%2520%2528Gad%2529%2520contrast%2520agent%252C%2520which%2520is%2520associated%2520with%2520a%2520risk%2520of%250Atoxicity%2520in%2520the%2520body.%2520Previous%2520deep%2520learning%2520approaches%2520that%2520synthesize%2520DCE-MR%250Aimages%2520employ%2520unimodal%2520non-contrast%2520or%2520low-dose%2520contrast%2520MRI%2520images%2520lacking%250Afocus%2520on%2520the%2520local%2520perfusion%2520information%2520within%2520the%2520anatomy%2520of%2520interest.%2520We%250Apropose%2520AAD-DCE%252C%2520a%2520generative%2520adversarial%2520network%2520%2528GAN%2529%2520with%2520an%2520aggregated%250Aattention%2520discriminator%2520module%2520consisting%2520of%2520global%2520and%2520local%2520discriminators.%250AThe%2520discriminators%2520provide%2520a%2520spatial%2520embedded%2520attention%2520map%2520to%2520drive%2520the%250Agenerator%2520to%2520synthesize%2520early%2520and%2520late%2520response%2520DCE-MRI%2520images.%2520Our%2520method%250Aemploys%2520multimodal%2520inputs%2520-%2520T2%2520weighted%2520%2528T2W%2529%252C%2520Apparent%2520Diffusion%2520Coefficient%250A%2528ADC%2529%252C%2520and%2520T1%2520pre-contrast%2520for%2520image%2520synthesis.%2520Extensive%2520comparative%2520and%250Aablation%2520studies%2520on%2520the%2520ProstateX%2520dataset%2520show%2520that%2520our%2520model%2520%2528i%2529%2520is%2520agnostic%250Ato%2520various%2520generator%2520benchmarks%2520and%2520%2528ii%2529%2520outperforms%2520other%2520DCE-MRI%2520synthesis%250Aapproaches%2520with%2520improvement%2520margins%2520of%2520%252B0.64%2520dB%2520PSNR%252C%2520%252B0.0518%2520SSIM%252C%2520-0.015%2520MAE%250Afor%2520early%2520response%2520and%2520%252B0.1%2520dB%2520PSNR%252C%2520%252B0.0424%2520SSIM%252C%2520-0.021%2520MAE%2520for%2520late%250Aresponse%252C%2520and%2520%2528ii%2529%2520emphasize%2520the%2520importance%2520of%2520attention%2520ensembling.%2520Our%2520code%250Ais%2520available%2520at%2520https%253A//github.com/bhartidivya/AAD-DCE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02555v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AAD-DCE%3A%20An%20Aggregated%20Multimodal%20Attention%20Mechanism%20for%20Early%20and%20Late%0A%20%20Dynamic%20Contrast%20Enhanced%20Prostate%20MRI%20Synthesis&entry.906535625=Divya%20Bharti%20and%20Sriprabha%20Ramanarayanan%20and%20Sadhana%20S%20and%20Kishore%20Kumar%20M%20and%20Keerthi%20Ram%20and%20Harsh%20Agarwal%20and%20Ramesh%20Venkatesan%20and%20Mohanasankar%20Sivaprakasam&entry.1292438233=%20%20Dynamic%20Contrast-Enhanced%20Magnetic%20Resonance%20Imaging%20%28DCE-MRI%29%20is%20a%20medical%0Aimaging%20technique%20that%20plays%20a%20crucial%20role%20in%20the%20detailed%20visualization%20and%0Aidentification%20of%20tissue%20perfusion%20in%20abnormal%20lesions%20and%20radiological%0Asuggestions%20for%20biopsy.%20However%2C%20DCE-MRI%20involves%20the%20administration%20of%20a%0AGadolinium%20based%20%28Gad%29%20contrast%20agent%2C%20which%20is%20associated%20with%20a%20risk%20of%0Atoxicity%20in%20the%20body.%20Previous%20deep%20learning%20approaches%20that%20synthesize%20DCE-MR%0Aimages%20employ%20unimodal%20non-contrast%20or%20low-dose%20contrast%20MRI%20images%20lacking%0Afocus%20on%20the%20local%20perfusion%20information%20within%20the%20anatomy%20of%20interest.%20We%0Apropose%20AAD-DCE%2C%20a%20generative%20adversarial%20network%20%28GAN%29%20with%20an%20aggregated%0Aattention%20discriminator%20module%20consisting%20of%20global%20and%20local%20discriminators.%0AThe%20discriminators%20provide%20a%20spatial%20embedded%20attention%20map%20to%20drive%20the%0Agenerator%20to%20synthesize%20early%20and%20late%20response%20DCE-MRI%20images.%20Our%20method%0Aemploys%20multimodal%20inputs%20-%20T2%20weighted%20%28T2W%29%2C%20Apparent%20Diffusion%20Coefficient%0A%28ADC%29%2C%20and%20T1%20pre-contrast%20for%20image%20synthesis.%20Extensive%20comparative%20and%0Aablation%20studies%20on%20the%20ProstateX%20dataset%20show%20that%20our%20model%20%28i%29%20is%20agnostic%0Ato%20various%20generator%20benchmarks%20and%20%28ii%29%20outperforms%20other%20DCE-MRI%20synthesis%0Aapproaches%20with%20improvement%20margins%20of%20%2B0.64%20dB%20PSNR%2C%20%2B0.0518%20SSIM%2C%20-0.015%20MAE%0Afor%20early%20response%20and%20%2B0.1%20dB%20PSNR%2C%20%2B0.0424%20SSIM%2C%20-0.021%20MAE%20for%20late%0Aresponse%2C%20and%20%28ii%29%20emphasize%20the%20importance%20of%20attention%20ensembling.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/bhartidivya/AAD-DCE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02555v2&entry.124074799=Read"},
{"title": "Explain Yourself, Briefly! Self-Explaining Neural Networks with Concise\n  Sufficient Reasons", "author": "Shahaf Bassan and Shlomit Gur and Ron Eliav", "abstract": "  Minimal sufficient reasons represent a prevalent form of explanation - the\nsmallest subset of input features which, when held constant at their\ncorresponding values, ensure that the prediction remains unchanged. Previous\npost-hoc methods attempt to obtain such explanations but face two main\nlimitations: (1) Obtaining these subsets poses a computational challenge,\nleading most scalable methods to converge towards suboptimal, less meaningful\nsubsets; (2) These methods heavily rely on sampling out-of-distribution input\nassignments, potentially resulting in counterintuitive behaviors. To tackle\nthese limitations, we propose in this work a self-supervised training approach,\nwhich we term *sufficient subset training* (SST). Using SST, we train models to\ngenerate concise sufficient reasons for their predictions as an integral part\nof their output. Our results indicate that our framework produces succinct and\nfaithful subsets substantially more efficiently than competing post-hoc\nmethods, while maintaining comparable predictive performance.\n", "link": "http://arxiv.org/abs/2502.03391v1", "date": "2025-02-05", "relevancy": 2.2893, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4677}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4602}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explain%20Yourself%2C%20Briefly%21%20Self-Explaining%20Neural%20Networks%20with%20Concise%0A%20%20Sufficient%20Reasons&body=Title%3A%20Explain%20Yourself%2C%20Briefly%21%20Self-Explaining%20Neural%20Networks%20with%20Concise%0A%20%20Sufficient%20Reasons%0AAuthor%3A%20Shahaf%20Bassan%20and%20Shlomit%20Gur%20and%20Ron%20Eliav%0AAbstract%3A%20%20%20Minimal%20sufficient%20reasons%20represent%20a%20prevalent%20form%20of%20explanation%20-%20the%0Asmallest%20subset%20of%20input%20features%20which%2C%20when%20held%20constant%20at%20their%0Acorresponding%20values%2C%20ensure%20that%20the%20prediction%20remains%20unchanged.%20Previous%0Apost-hoc%20methods%20attempt%20to%20obtain%20such%20explanations%20but%20face%20two%20main%0Alimitations%3A%20%281%29%20Obtaining%20these%20subsets%20poses%20a%20computational%20challenge%2C%0Aleading%20most%20scalable%20methods%20to%20converge%20towards%20suboptimal%2C%20less%20meaningful%0Asubsets%3B%20%282%29%20These%20methods%20heavily%20rely%20on%20sampling%20out-of-distribution%20input%0Aassignments%2C%20potentially%20resulting%20in%20counterintuitive%20behaviors.%20To%20tackle%0Athese%20limitations%2C%20we%20propose%20in%20this%20work%20a%20self-supervised%20training%20approach%2C%0Awhich%20we%20term%20%2Asufficient%20subset%20training%2A%20%28SST%29.%20Using%20SST%2C%20we%20train%20models%20to%0Agenerate%20concise%20sufficient%20reasons%20for%20their%20predictions%20as%20an%20integral%20part%0Aof%20their%20output.%20Our%20results%20indicate%20that%20our%20framework%20produces%20succinct%20and%0Afaithful%20subsets%20substantially%20more%20efficiently%20than%20competing%20post-hoc%0Amethods%2C%20while%20maintaining%20comparable%20predictive%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03391v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplain%2520Yourself%252C%2520Briefly%2521%2520Self-Explaining%2520Neural%2520Networks%2520with%2520Concise%250A%2520%2520Sufficient%2520Reasons%26entry.906535625%3DShahaf%2520Bassan%2520and%2520Shlomit%2520Gur%2520and%2520Ron%2520Eliav%26entry.1292438233%3D%2520%2520Minimal%2520sufficient%2520reasons%2520represent%2520a%2520prevalent%2520form%2520of%2520explanation%2520-%2520the%250Asmallest%2520subset%2520of%2520input%2520features%2520which%252C%2520when%2520held%2520constant%2520at%2520their%250Acorresponding%2520values%252C%2520ensure%2520that%2520the%2520prediction%2520remains%2520unchanged.%2520Previous%250Apost-hoc%2520methods%2520attempt%2520to%2520obtain%2520such%2520explanations%2520but%2520face%2520two%2520main%250Alimitations%253A%2520%25281%2529%2520Obtaining%2520these%2520subsets%2520poses%2520a%2520computational%2520challenge%252C%250Aleading%2520most%2520scalable%2520methods%2520to%2520converge%2520towards%2520suboptimal%252C%2520less%2520meaningful%250Asubsets%253B%2520%25282%2529%2520These%2520methods%2520heavily%2520rely%2520on%2520sampling%2520out-of-distribution%2520input%250Aassignments%252C%2520potentially%2520resulting%2520in%2520counterintuitive%2520behaviors.%2520To%2520tackle%250Athese%2520limitations%252C%2520we%2520propose%2520in%2520this%2520work%2520a%2520self-supervised%2520training%2520approach%252C%250Awhich%2520we%2520term%2520%252Asufficient%2520subset%2520training%252A%2520%2528SST%2529.%2520Using%2520SST%252C%2520we%2520train%2520models%2520to%250Agenerate%2520concise%2520sufficient%2520reasons%2520for%2520their%2520predictions%2520as%2520an%2520integral%2520part%250Aof%2520their%2520output.%2520Our%2520results%2520indicate%2520that%2520our%2520framework%2520produces%2520succinct%2520and%250Afaithful%2520subsets%2520substantially%2520more%2520efficiently%2520than%2520competing%2520post-hoc%250Amethods%252C%2520while%2520maintaining%2520comparable%2520predictive%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03391v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explain%20Yourself%2C%20Briefly%21%20Self-Explaining%20Neural%20Networks%20with%20Concise%0A%20%20Sufficient%20Reasons&entry.906535625=Shahaf%20Bassan%20and%20Shlomit%20Gur%20and%20Ron%20Eliav&entry.1292438233=%20%20Minimal%20sufficient%20reasons%20represent%20a%20prevalent%20form%20of%20explanation%20-%20the%0Asmallest%20subset%20of%20input%20features%20which%2C%20when%20held%20constant%20at%20their%0Acorresponding%20values%2C%20ensure%20that%20the%20prediction%20remains%20unchanged.%20Previous%0Apost-hoc%20methods%20attempt%20to%20obtain%20such%20explanations%20but%20face%20two%20main%0Alimitations%3A%20%281%29%20Obtaining%20these%20subsets%20poses%20a%20computational%20challenge%2C%0Aleading%20most%20scalable%20methods%20to%20converge%20towards%20suboptimal%2C%20less%20meaningful%0Asubsets%3B%20%282%29%20These%20methods%20heavily%20rely%20on%20sampling%20out-of-distribution%20input%0Aassignments%2C%20potentially%20resulting%20in%20counterintuitive%20behaviors.%20To%20tackle%0Athese%20limitations%2C%20we%20propose%20in%20this%20work%20a%20self-supervised%20training%20approach%2C%0Awhich%20we%20term%20%2Asufficient%20subset%20training%2A%20%28SST%29.%20Using%20SST%2C%20we%20train%20models%20to%0Agenerate%20concise%20sufficient%20reasons%20for%20their%20predictions%20as%20an%20integral%20part%0Aof%20their%20output.%20Our%20results%20indicate%20that%20our%20framework%20produces%20succinct%20and%0Afaithful%20subsets%20substantially%20more%20efficiently%20than%20competing%20post-hoc%0Amethods%2C%20while%20maintaining%20comparable%20predictive%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03391v1&entry.124074799=Read"},
{"title": "Long-tailed Medical Diagnosis with Relation-aware Representation\n  Learning and Iterative Classifier Calibration", "author": "Li Pan and Yupei Zhang and Qiushi Yang and Tan Li and Zhen Chen", "abstract": "  Recently computer-aided diagnosis has demonstrated promising performance,\neffectively alleviating the workload of clinicians. However, the inherent\nsample imbalance among different diseases leads algorithms biased to the\nmajority categories, leading to poor performance for rare categories. Existing\nworks formulated this challenge as a long-tailed problem and attempted to\ntackle it by decoupling the feature representation and classification. Yet, due\nto the imbalanced distribution and limited samples from tail classes, these\nworks are prone to biased representation learning and insufficient classifier\ncalibration. To tackle these problems, we propose a new Long-tailed Medical\nDiagnosis (LMD) framework for balanced medical image classification on\nlong-tailed datasets. In the initial stage, we develop a Relation-aware\nRepresentation Learning (RRL) scheme to boost the representation ability by\nencouraging the encoder to capture intrinsic semantic features through\ndifferent data augmentations. In the subsequent stage, we propose an Iterative\nClassifier Calibration (ICC) scheme to calibrate the classifier iteratively.\nThis is achieved by generating a large number of balanced virtual features and\nfine-tuning the encoder using an Expectation-Maximization manner. The proposed\nICC compensates for minority categories to facilitate unbiased classifier\noptimization while maintaining the diagnostic knowledge in majority classes.\nComprehensive experiments on three public long-tailed medical datasets\ndemonstrate that our LMD framework significantly surpasses state-of-the-art\napproaches. The source code can be accessed at\nhttps://github.com/peterlipan/LMD.\n", "link": "http://arxiv.org/abs/2502.03238v1", "date": "2025-02-05", "relevancy": 2.2775, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5923}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5623}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Long-tailed%20Medical%20Diagnosis%20with%20Relation-aware%20Representation%0A%20%20Learning%20and%20Iterative%20Classifier%20Calibration&body=Title%3A%20Long-tailed%20Medical%20Diagnosis%20with%20Relation-aware%20Representation%0A%20%20Learning%20and%20Iterative%20Classifier%20Calibration%0AAuthor%3A%20Li%20Pan%20and%20Yupei%20Zhang%20and%20Qiushi%20Yang%20and%20Tan%20Li%20and%20Zhen%20Chen%0AAbstract%3A%20%20%20Recently%20computer-aided%20diagnosis%20has%20demonstrated%20promising%20performance%2C%0Aeffectively%20alleviating%20the%20workload%20of%20clinicians.%20However%2C%20the%20inherent%0Asample%20imbalance%20among%20different%20diseases%20leads%20algorithms%20biased%20to%20the%0Amajority%20categories%2C%20leading%20to%20poor%20performance%20for%20rare%20categories.%20Existing%0Aworks%20formulated%20this%20challenge%20as%20a%20long-tailed%20problem%20and%20attempted%20to%0Atackle%20it%20by%20decoupling%20the%20feature%20representation%20and%20classification.%20Yet%2C%20due%0Ato%20the%20imbalanced%20distribution%20and%20limited%20samples%20from%20tail%20classes%2C%20these%0Aworks%20are%20prone%20to%20biased%20representation%20learning%20and%20insufficient%20classifier%0Acalibration.%20To%20tackle%20these%20problems%2C%20we%20propose%20a%20new%20Long-tailed%20Medical%0ADiagnosis%20%28LMD%29%20framework%20for%20balanced%20medical%20image%20classification%20on%0Along-tailed%20datasets.%20In%20the%20initial%20stage%2C%20we%20develop%20a%20Relation-aware%0ARepresentation%20Learning%20%28RRL%29%20scheme%20to%20boost%20the%20representation%20ability%20by%0Aencouraging%20the%20encoder%20to%20capture%20intrinsic%20semantic%20features%20through%0Adifferent%20data%20augmentations.%20In%20the%20subsequent%20stage%2C%20we%20propose%20an%20Iterative%0AClassifier%20Calibration%20%28ICC%29%20scheme%20to%20calibrate%20the%20classifier%20iteratively.%0AThis%20is%20achieved%20by%20generating%20a%20large%20number%20of%20balanced%20virtual%20features%20and%0Afine-tuning%20the%20encoder%20using%20an%20Expectation-Maximization%20manner.%20The%20proposed%0AICC%20compensates%20for%20minority%20categories%20to%20facilitate%20unbiased%20classifier%0Aoptimization%20while%20maintaining%20the%20diagnostic%20knowledge%20in%20majority%20classes.%0AComprehensive%20experiments%20on%20three%20public%20long-tailed%20medical%20datasets%0Ademonstrate%20that%20our%20LMD%20framework%20significantly%20surpasses%20state-of-the-art%0Aapproaches.%20The%20source%20code%20can%20be%20accessed%20at%0Ahttps%3A//github.com/peterlipan/LMD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03238v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLong-tailed%2520Medical%2520Diagnosis%2520with%2520Relation-aware%2520Representation%250A%2520%2520Learning%2520and%2520Iterative%2520Classifier%2520Calibration%26entry.906535625%3DLi%2520Pan%2520and%2520Yupei%2520Zhang%2520and%2520Qiushi%2520Yang%2520and%2520Tan%2520Li%2520and%2520Zhen%2520Chen%26entry.1292438233%3D%2520%2520Recently%2520computer-aided%2520diagnosis%2520has%2520demonstrated%2520promising%2520performance%252C%250Aeffectively%2520alleviating%2520the%2520workload%2520of%2520clinicians.%2520However%252C%2520the%2520inherent%250Asample%2520imbalance%2520among%2520different%2520diseases%2520leads%2520algorithms%2520biased%2520to%2520the%250Amajority%2520categories%252C%2520leading%2520to%2520poor%2520performance%2520for%2520rare%2520categories.%2520Existing%250Aworks%2520formulated%2520this%2520challenge%2520as%2520a%2520long-tailed%2520problem%2520and%2520attempted%2520to%250Atackle%2520it%2520by%2520decoupling%2520the%2520feature%2520representation%2520and%2520classification.%2520Yet%252C%2520due%250Ato%2520the%2520imbalanced%2520distribution%2520and%2520limited%2520samples%2520from%2520tail%2520classes%252C%2520these%250Aworks%2520are%2520prone%2520to%2520biased%2520representation%2520learning%2520and%2520insufficient%2520classifier%250Acalibration.%2520To%2520tackle%2520these%2520problems%252C%2520we%2520propose%2520a%2520new%2520Long-tailed%2520Medical%250ADiagnosis%2520%2528LMD%2529%2520framework%2520for%2520balanced%2520medical%2520image%2520classification%2520on%250Along-tailed%2520datasets.%2520In%2520the%2520initial%2520stage%252C%2520we%2520develop%2520a%2520Relation-aware%250ARepresentation%2520Learning%2520%2528RRL%2529%2520scheme%2520to%2520boost%2520the%2520representation%2520ability%2520by%250Aencouraging%2520the%2520encoder%2520to%2520capture%2520intrinsic%2520semantic%2520features%2520through%250Adifferent%2520data%2520augmentations.%2520In%2520the%2520subsequent%2520stage%252C%2520we%2520propose%2520an%2520Iterative%250AClassifier%2520Calibration%2520%2528ICC%2529%2520scheme%2520to%2520calibrate%2520the%2520classifier%2520iteratively.%250AThis%2520is%2520achieved%2520by%2520generating%2520a%2520large%2520number%2520of%2520balanced%2520virtual%2520features%2520and%250Afine-tuning%2520the%2520encoder%2520using%2520an%2520Expectation-Maximization%2520manner.%2520The%2520proposed%250AICC%2520compensates%2520for%2520minority%2520categories%2520to%2520facilitate%2520unbiased%2520classifier%250Aoptimization%2520while%2520maintaining%2520the%2520diagnostic%2520knowledge%2520in%2520majority%2520classes.%250AComprehensive%2520experiments%2520on%2520three%2520public%2520long-tailed%2520medical%2520datasets%250Ademonstrate%2520that%2520our%2520LMD%2520framework%2520significantly%2520surpasses%2520state-of-the-art%250Aapproaches.%2520The%2520source%2520code%2520can%2520be%2520accessed%2520at%250Ahttps%253A//github.com/peterlipan/LMD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03238v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Long-tailed%20Medical%20Diagnosis%20with%20Relation-aware%20Representation%0A%20%20Learning%20and%20Iterative%20Classifier%20Calibration&entry.906535625=Li%20Pan%20and%20Yupei%20Zhang%20and%20Qiushi%20Yang%20and%20Tan%20Li%20and%20Zhen%20Chen&entry.1292438233=%20%20Recently%20computer-aided%20diagnosis%20has%20demonstrated%20promising%20performance%2C%0Aeffectively%20alleviating%20the%20workload%20of%20clinicians.%20However%2C%20the%20inherent%0Asample%20imbalance%20among%20different%20diseases%20leads%20algorithms%20biased%20to%20the%0Amajority%20categories%2C%20leading%20to%20poor%20performance%20for%20rare%20categories.%20Existing%0Aworks%20formulated%20this%20challenge%20as%20a%20long-tailed%20problem%20and%20attempted%20to%0Atackle%20it%20by%20decoupling%20the%20feature%20representation%20and%20classification.%20Yet%2C%20due%0Ato%20the%20imbalanced%20distribution%20and%20limited%20samples%20from%20tail%20classes%2C%20these%0Aworks%20are%20prone%20to%20biased%20representation%20learning%20and%20insufficient%20classifier%0Acalibration.%20To%20tackle%20these%20problems%2C%20we%20propose%20a%20new%20Long-tailed%20Medical%0ADiagnosis%20%28LMD%29%20framework%20for%20balanced%20medical%20image%20classification%20on%0Along-tailed%20datasets.%20In%20the%20initial%20stage%2C%20we%20develop%20a%20Relation-aware%0ARepresentation%20Learning%20%28RRL%29%20scheme%20to%20boost%20the%20representation%20ability%20by%0Aencouraging%20the%20encoder%20to%20capture%20intrinsic%20semantic%20features%20through%0Adifferent%20data%20augmentations.%20In%20the%20subsequent%20stage%2C%20we%20propose%20an%20Iterative%0AClassifier%20Calibration%20%28ICC%29%20scheme%20to%20calibrate%20the%20classifier%20iteratively.%0AThis%20is%20achieved%20by%20generating%20a%20large%20number%20of%20balanced%20virtual%20features%20and%0Afine-tuning%20the%20encoder%20using%20an%20Expectation-Maximization%20manner.%20The%20proposed%0AICC%20compensates%20for%20minority%20categories%20to%20facilitate%20unbiased%20classifier%0Aoptimization%20while%20maintaining%20the%20diagnostic%20knowledge%20in%20majority%20classes.%0AComprehensive%20experiments%20on%20three%20public%20long-tailed%20medical%20datasets%0Ademonstrate%20that%20our%20LMD%20framework%20significantly%20surpasses%20state-of-the-art%0Aapproaches.%20The%20source%20code%20can%20be%20accessed%20at%0Ahttps%3A//github.com/peterlipan/LMD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03238v1&entry.124074799=Read"},
{"title": "A Schema-Guided Reason-while-Retrieve framework for Reasoning on Scene\n  Graphs with Large-Language-Models (LLMs)", "author": "Yiye Chen and Harpreet Sawhney and Nicholas Gyd\u00e9 and Yanan Jian and Jack Saunders and Patricio Vela and Ben Lundell", "abstract": "  Scene graphs have emerged as a structured and serializable environment\nrepresentation for grounded spatial reasoning with Large Language Models\n(LLMs). In this work, we propose SG-RwR, a Schema-Guided Retrieve-while-Reason\nframework for reasoning and planning with scene graphs. Our approach employs\ntwo cooperative, code-writing LLM agents: a (1) Reasoner for task planning and\ninformation queries generation, and a (2) Retriever for extracting\ncorresponding graph information following the queries. Two agents collaborate\niteratively, enabling sequential reasoning and adaptive attention to graph\ninformation. Unlike prior works, both agents are prompted only with the scene\ngraph schema rather than the full graph data, which reduces the hallucination\nby limiting input tokens, and drives the Reasoner to generate reasoning trace\nabstractly.Following the trace, the Retriever programmatically query the scene\ngraph data based on the schema understanding, allowing dynamic and global\nattention on the graph that enhances alignment between reasoning and retrieval.\nThrough experiments in multiple simulation environments, we show that our\nframework surpasses existing LLM-based approaches in numerical Q\\&A and\nplanning tasks, and can benefit from task-level few-shot examples, even in the\nabsence of agent-level demonstrations. Project code will be released.\n", "link": "http://arxiv.org/abs/2502.03450v1", "date": "2025-02-05", "relevancy": 2.2743, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.625}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5573}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5573}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Schema-Guided%20Reason-while-Retrieve%20framework%20for%20Reasoning%20on%20Scene%0A%20%20Graphs%20with%20Large-Language-Models%20%28LLMs%29&body=Title%3A%20A%20Schema-Guided%20Reason-while-Retrieve%20framework%20for%20Reasoning%20on%20Scene%0A%20%20Graphs%20with%20Large-Language-Models%20%28LLMs%29%0AAuthor%3A%20Yiye%20Chen%20and%20Harpreet%20Sawhney%20and%20Nicholas%20Gyd%C3%A9%20and%20Yanan%20Jian%20and%20Jack%20Saunders%20and%20Patricio%20Vela%20and%20Ben%20Lundell%0AAbstract%3A%20%20%20Scene%20graphs%20have%20emerged%20as%20a%20structured%20and%20serializable%20environment%0Arepresentation%20for%20grounded%20spatial%20reasoning%20with%20Large%20Language%20Models%0A%28LLMs%29.%20In%20this%20work%2C%20we%20propose%20SG-RwR%2C%20a%20Schema-Guided%20Retrieve-while-Reason%0Aframework%20for%20reasoning%20and%20planning%20with%20scene%20graphs.%20Our%20approach%20employs%0Atwo%20cooperative%2C%20code-writing%20LLM%20agents%3A%20a%20%281%29%20Reasoner%20for%20task%20planning%20and%0Ainformation%20queries%20generation%2C%20and%20a%20%282%29%20Retriever%20for%20extracting%0Acorresponding%20graph%20information%20following%20the%20queries.%20Two%20agents%20collaborate%0Aiteratively%2C%20enabling%20sequential%20reasoning%20and%20adaptive%20attention%20to%20graph%0Ainformation.%20Unlike%20prior%20works%2C%20both%20agents%20are%20prompted%20only%20with%20the%20scene%0Agraph%20schema%20rather%20than%20the%20full%20graph%20data%2C%20which%20reduces%20the%20hallucination%0Aby%20limiting%20input%20tokens%2C%20and%20drives%20the%20Reasoner%20to%20generate%20reasoning%20trace%0Aabstractly.Following%20the%20trace%2C%20the%20Retriever%20programmatically%20query%20the%20scene%0Agraph%20data%20based%20on%20the%20schema%20understanding%2C%20allowing%20dynamic%20and%20global%0Aattention%20on%20the%20graph%20that%20enhances%20alignment%20between%20reasoning%20and%20retrieval.%0AThrough%20experiments%20in%20multiple%20simulation%20environments%2C%20we%20show%20that%20our%0Aframework%20surpasses%20existing%20LLM-based%20approaches%20in%20numerical%20Q%5C%26A%20and%0Aplanning%20tasks%2C%20and%20can%20benefit%20from%20task-level%20few-shot%20examples%2C%20even%20in%20the%0Aabsence%20of%20agent-level%20demonstrations.%20Project%20code%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03450v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Schema-Guided%2520Reason-while-Retrieve%2520framework%2520for%2520Reasoning%2520on%2520Scene%250A%2520%2520Graphs%2520with%2520Large-Language-Models%2520%2528LLMs%2529%26entry.906535625%3DYiye%2520Chen%2520and%2520Harpreet%2520Sawhney%2520and%2520Nicholas%2520Gyd%25C3%25A9%2520and%2520Yanan%2520Jian%2520and%2520Jack%2520Saunders%2520and%2520Patricio%2520Vela%2520and%2520Ben%2520Lundell%26entry.1292438233%3D%2520%2520Scene%2520graphs%2520have%2520emerged%2520as%2520a%2520structured%2520and%2520serializable%2520environment%250Arepresentation%2520for%2520grounded%2520spatial%2520reasoning%2520with%2520Large%2520Language%2520Models%250A%2528LLMs%2529.%2520In%2520this%2520work%252C%2520we%2520propose%2520SG-RwR%252C%2520a%2520Schema-Guided%2520Retrieve-while-Reason%250Aframework%2520for%2520reasoning%2520and%2520planning%2520with%2520scene%2520graphs.%2520Our%2520approach%2520employs%250Atwo%2520cooperative%252C%2520code-writing%2520LLM%2520agents%253A%2520a%2520%25281%2529%2520Reasoner%2520for%2520task%2520planning%2520and%250Ainformation%2520queries%2520generation%252C%2520and%2520a%2520%25282%2529%2520Retriever%2520for%2520extracting%250Acorresponding%2520graph%2520information%2520following%2520the%2520queries.%2520Two%2520agents%2520collaborate%250Aiteratively%252C%2520enabling%2520sequential%2520reasoning%2520and%2520adaptive%2520attention%2520to%2520graph%250Ainformation.%2520Unlike%2520prior%2520works%252C%2520both%2520agents%2520are%2520prompted%2520only%2520with%2520the%2520scene%250Agraph%2520schema%2520rather%2520than%2520the%2520full%2520graph%2520data%252C%2520which%2520reduces%2520the%2520hallucination%250Aby%2520limiting%2520input%2520tokens%252C%2520and%2520drives%2520the%2520Reasoner%2520to%2520generate%2520reasoning%2520trace%250Aabstractly.Following%2520the%2520trace%252C%2520the%2520Retriever%2520programmatically%2520query%2520the%2520scene%250Agraph%2520data%2520based%2520on%2520the%2520schema%2520understanding%252C%2520allowing%2520dynamic%2520and%2520global%250Aattention%2520on%2520the%2520graph%2520that%2520enhances%2520alignment%2520between%2520reasoning%2520and%2520retrieval.%250AThrough%2520experiments%2520in%2520multiple%2520simulation%2520environments%252C%2520we%2520show%2520that%2520our%250Aframework%2520surpasses%2520existing%2520LLM-based%2520approaches%2520in%2520numerical%2520Q%255C%2526A%2520and%250Aplanning%2520tasks%252C%2520and%2520can%2520benefit%2520from%2520task-level%2520few-shot%2520examples%252C%2520even%2520in%2520the%250Aabsence%2520of%2520agent-level%2520demonstrations.%2520Project%2520code%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03450v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Schema-Guided%20Reason-while-Retrieve%20framework%20for%20Reasoning%20on%20Scene%0A%20%20Graphs%20with%20Large-Language-Models%20%28LLMs%29&entry.906535625=Yiye%20Chen%20and%20Harpreet%20Sawhney%20and%20Nicholas%20Gyd%C3%A9%20and%20Yanan%20Jian%20and%20Jack%20Saunders%20and%20Patricio%20Vela%20and%20Ben%20Lundell&entry.1292438233=%20%20Scene%20graphs%20have%20emerged%20as%20a%20structured%20and%20serializable%20environment%0Arepresentation%20for%20grounded%20spatial%20reasoning%20with%20Large%20Language%20Models%0A%28LLMs%29.%20In%20this%20work%2C%20we%20propose%20SG-RwR%2C%20a%20Schema-Guided%20Retrieve-while-Reason%0Aframework%20for%20reasoning%20and%20planning%20with%20scene%20graphs.%20Our%20approach%20employs%0Atwo%20cooperative%2C%20code-writing%20LLM%20agents%3A%20a%20%281%29%20Reasoner%20for%20task%20planning%20and%0Ainformation%20queries%20generation%2C%20and%20a%20%282%29%20Retriever%20for%20extracting%0Acorresponding%20graph%20information%20following%20the%20queries.%20Two%20agents%20collaborate%0Aiteratively%2C%20enabling%20sequential%20reasoning%20and%20adaptive%20attention%20to%20graph%0Ainformation.%20Unlike%20prior%20works%2C%20both%20agents%20are%20prompted%20only%20with%20the%20scene%0Agraph%20schema%20rather%20than%20the%20full%20graph%20data%2C%20which%20reduces%20the%20hallucination%0Aby%20limiting%20input%20tokens%2C%20and%20drives%20the%20Reasoner%20to%20generate%20reasoning%20trace%0Aabstractly.Following%20the%20trace%2C%20the%20Retriever%20programmatically%20query%20the%20scene%0Agraph%20data%20based%20on%20the%20schema%20understanding%2C%20allowing%20dynamic%20and%20global%0Aattention%20on%20the%20graph%20that%20enhances%20alignment%20between%20reasoning%20and%20retrieval.%0AThrough%20experiments%20in%20multiple%20simulation%20environments%2C%20we%20show%20that%20our%0Aframework%20surpasses%20existing%20LLM-based%20approaches%20in%20numerical%20Q%5C%26A%20and%0Aplanning%20tasks%2C%20and%20can%20benefit%20from%20task-level%20few-shot%20examples%2C%20even%20in%20the%0Aabsence%20of%20agent-level%20demonstrations.%20Project%20code%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03450v1&entry.124074799=Read"},
{"title": "Implicit Communication in Human-Robot Collaborative Transport", "author": "Elvin Yang and Christoforos Mavrogiannis", "abstract": "  We focus on human-robot collaborative transport, in which a robot and a user\ncollaboratively move an object to a goal pose. In the absence of explicit\ncommunication, this problem is challenging because it demands tight implicit\ncoordination between two heterogeneous agents, who have very different sensing,\nactuation, and reasoning capabilities. Our key insight is that the two agents\ncan coordinate fluently by encoding subtle, communicative signals into actions\nthat affect the state of the transported object. To this end, we design an\ninference mechanism that probabilistically maps observations of joint actions\nexecuted by the two agents to a set of joint strategies of workspace traversal.\nBased on this mechanism, we define a cost representing the human's uncertainty\nover the unfolding traversal strategy and introduce it into a model predictive\ncontroller that balances between uncertainty minimization and efficiency\nmaximization. We deploy our framework on a mobile manipulator (Hello Robot\nStretch) and evaluate it in a within-subjects lab study (N=24). We show that\nour framework enables greater team performance and empowers the robot to be\nperceived as a significantly more fluent and competent partner compared to\nbaselines lacking a communicative mechanism.\n", "link": "http://arxiv.org/abs/2502.03346v1", "date": "2025-02-05", "relevancy": 2.2606, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.587}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5611}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implicit%20Communication%20in%20Human-Robot%20Collaborative%20Transport&body=Title%3A%20Implicit%20Communication%20in%20Human-Robot%20Collaborative%20Transport%0AAuthor%3A%20Elvin%20Yang%20and%20Christoforos%20Mavrogiannis%0AAbstract%3A%20%20%20We%20focus%20on%20human-robot%20collaborative%20transport%2C%20in%20which%20a%20robot%20and%20a%20user%0Acollaboratively%20move%20an%20object%20to%20a%20goal%20pose.%20In%20the%20absence%20of%20explicit%0Acommunication%2C%20this%20problem%20is%20challenging%20because%20it%20demands%20tight%20implicit%0Acoordination%20between%20two%20heterogeneous%20agents%2C%20who%20have%20very%20different%20sensing%2C%0Aactuation%2C%20and%20reasoning%20capabilities.%20Our%20key%20insight%20is%20that%20the%20two%20agents%0Acan%20coordinate%20fluently%20by%20encoding%20subtle%2C%20communicative%20signals%20into%20actions%0Athat%20affect%20the%20state%20of%20the%20transported%20object.%20To%20this%20end%2C%20we%20design%20an%0Ainference%20mechanism%20that%20probabilistically%20maps%20observations%20of%20joint%20actions%0Aexecuted%20by%20the%20two%20agents%20to%20a%20set%20of%20joint%20strategies%20of%20workspace%20traversal.%0ABased%20on%20this%20mechanism%2C%20we%20define%20a%20cost%20representing%20the%20human%27s%20uncertainty%0Aover%20the%20unfolding%20traversal%20strategy%20and%20introduce%20it%20into%20a%20model%20predictive%0Acontroller%20that%20balances%20between%20uncertainty%20minimization%20and%20efficiency%0Amaximization.%20We%20deploy%20our%20framework%20on%20a%20mobile%20manipulator%20%28Hello%20Robot%0AStretch%29%20and%20evaluate%20it%20in%20a%20within-subjects%20lab%20study%20%28N%3D24%29.%20We%20show%20that%0Aour%20framework%20enables%20greater%20team%20performance%20and%20empowers%20the%20robot%20to%20be%0Aperceived%20as%20a%20significantly%20more%20fluent%20and%20competent%20partner%20compared%20to%0Abaselines%20lacking%20a%20communicative%20mechanism.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03346v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplicit%2520Communication%2520in%2520Human-Robot%2520Collaborative%2520Transport%26entry.906535625%3DElvin%2520Yang%2520and%2520Christoforos%2520Mavrogiannis%26entry.1292438233%3D%2520%2520We%2520focus%2520on%2520human-robot%2520collaborative%2520transport%252C%2520in%2520which%2520a%2520robot%2520and%2520a%2520user%250Acollaboratively%2520move%2520an%2520object%2520to%2520a%2520goal%2520pose.%2520In%2520the%2520absence%2520of%2520explicit%250Acommunication%252C%2520this%2520problem%2520is%2520challenging%2520because%2520it%2520demands%2520tight%2520implicit%250Acoordination%2520between%2520two%2520heterogeneous%2520agents%252C%2520who%2520have%2520very%2520different%2520sensing%252C%250Aactuation%252C%2520and%2520reasoning%2520capabilities.%2520Our%2520key%2520insight%2520is%2520that%2520the%2520two%2520agents%250Acan%2520coordinate%2520fluently%2520by%2520encoding%2520subtle%252C%2520communicative%2520signals%2520into%2520actions%250Athat%2520affect%2520the%2520state%2520of%2520the%2520transported%2520object.%2520To%2520this%2520end%252C%2520we%2520design%2520an%250Ainference%2520mechanism%2520that%2520probabilistically%2520maps%2520observations%2520of%2520joint%2520actions%250Aexecuted%2520by%2520the%2520two%2520agents%2520to%2520a%2520set%2520of%2520joint%2520strategies%2520of%2520workspace%2520traversal.%250ABased%2520on%2520this%2520mechanism%252C%2520we%2520define%2520a%2520cost%2520representing%2520the%2520human%2527s%2520uncertainty%250Aover%2520the%2520unfolding%2520traversal%2520strategy%2520and%2520introduce%2520it%2520into%2520a%2520model%2520predictive%250Acontroller%2520that%2520balances%2520between%2520uncertainty%2520minimization%2520and%2520efficiency%250Amaximization.%2520We%2520deploy%2520our%2520framework%2520on%2520a%2520mobile%2520manipulator%2520%2528Hello%2520Robot%250AStretch%2529%2520and%2520evaluate%2520it%2520in%2520a%2520within-subjects%2520lab%2520study%2520%2528N%253D24%2529.%2520We%2520show%2520that%250Aour%2520framework%2520enables%2520greater%2520team%2520performance%2520and%2520empowers%2520the%2520robot%2520to%2520be%250Aperceived%2520as%2520a%2520significantly%2520more%2520fluent%2520and%2520competent%2520partner%2520compared%2520to%250Abaselines%2520lacking%2520a%2520communicative%2520mechanism.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03346v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20Communication%20in%20Human-Robot%20Collaborative%20Transport&entry.906535625=Elvin%20Yang%20and%20Christoforos%20Mavrogiannis&entry.1292438233=%20%20We%20focus%20on%20human-robot%20collaborative%20transport%2C%20in%20which%20a%20robot%20and%20a%20user%0Acollaboratively%20move%20an%20object%20to%20a%20goal%20pose.%20In%20the%20absence%20of%20explicit%0Acommunication%2C%20this%20problem%20is%20challenging%20because%20it%20demands%20tight%20implicit%0Acoordination%20between%20two%20heterogeneous%20agents%2C%20who%20have%20very%20different%20sensing%2C%0Aactuation%2C%20and%20reasoning%20capabilities.%20Our%20key%20insight%20is%20that%20the%20two%20agents%0Acan%20coordinate%20fluently%20by%20encoding%20subtle%2C%20communicative%20signals%20into%20actions%0Athat%20affect%20the%20state%20of%20the%20transported%20object.%20To%20this%20end%2C%20we%20design%20an%0Ainference%20mechanism%20that%20probabilistically%20maps%20observations%20of%20joint%20actions%0Aexecuted%20by%20the%20two%20agents%20to%20a%20set%20of%20joint%20strategies%20of%20workspace%20traversal.%0ABased%20on%20this%20mechanism%2C%20we%20define%20a%20cost%20representing%20the%20human%27s%20uncertainty%0Aover%20the%20unfolding%20traversal%20strategy%20and%20introduce%20it%20into%20a%20model%20predictive%0Acontroller%20that%20balances%20between%20uncertainty%20minimization%20and%20efficiency%0Amaximization.%20We%20deploy%20our%20framework%20on%20a%20mobile%20manipulator%20%28Hello%20Robot%0AStretch%29%20and%20evaluate%20it%20in%20a%20within-subjects%20lab%20study%20%28N%3D24%29.%20We%20show%20that%0Aour%20framework%20enables%20greater%20team%20performance%20and%20empowers%20the%20robot%20to%20be%0Aperceived%20as%20a%20significantly%20more%20fluent%20and%20competent%20partner%20compared%20to%0Abaselines%20lacking%20a%20communicative%20mechanism.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03346v1&entry.124074799=Read"},
{"title": "ECM: A Unified Electronic Circuit Model for Explaining the Emergence of\n  In-Context Learning and Chain-of-Thought in Large Language Model", "author": "Qiguang Chen and Libo Qin and Jinhao Liu and Dengyun Peng and Jiaqi Wang and Mengkang Hu and Zhi Chen and Wanxiang Che and Ting Liu", "abstract": "  Recent advancements in large language models (LLMs) have led to significant\nsuccesses across various applications, where the most noticeable is to a series\nof emerging capabilities, particularly in the areas of In-Context Learning\n(ICL) and Chain-of-Thought (CoT). To better understand and control model\nperformance, many studies have begun investigating the underlying causes of\nthese phenomena and their impact on task outcomes. However, existing\nexplanatory frameworks predominantly focus on isolating and explaining ICL and\nCoT independently, leading to an incomplete understanding of their combined\ninfluence on model performance. To address this gap, we propose the Electronic\nCircuit Model (ECM), which provides a foundation for developing scalable,\nlearnable policies and improving the management of AI-generated content.\nSpecifically, ECM conceptualizes model behavior as an electronic circuit: ICL\nis represented as semantic magnetic field to providing an additional voltage\nfollowing Faraday's Law, while CoT is modeled as series resistors to constrain\nthe model output performance following Ohm's Law. Experimental results\ndemonstrate that the ECM effectively predicts and explains LLM performance\nacross a variety of prompting strategies. Furthermore, we apply ECM to advanced\nreasoning strategy optimization on a series of tasks, such as the International\nOlympiad in Informatics (IOI) and the International Mathematical Olympiad\n(IMO), achieving competitive performance that surpasses nearly 80% of top human\ncompetitors.\n", "link": "http://arxiv.org/abs/2502.03325v1", "date": "2025-02-05", "relevancy": 2.2547, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5691}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5691}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ECM%3A%20A%20Unified%20Electronic%20Circuit%20Model%20for%20Explaining%20the%20Emergence%20of%0A%20%20In-Context%20Learning%20and%20Chain-of-Thought%20in%20Large%20Language%20Model&body=Title%3A%20ECM%3A%20A%20Unified%20Electronic%20Circuit%20Model%20for%20Explaining%20the%20Emergence%20of%0A%20%20In-Context%20Learning%20and%20Chain-of-Thought%20in%20Large%20Language%20Model%0AAuthor%3A%20Qiguang%20Chen%20and%20Libo%20Qin%20and%20Jinhao%20Liu%20and%20Dengyun%20Peng%20and%20Jiaqi%20Wang%20and%20Mengkang%20Hu%20and%20Zhi%20Chen%20and%20Wanxiang%20Che%20and%20Ting%20Liu%0AAbstract%3A%20%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20led%20to%20significant%0Asuccesses%20across%20various%20applications%2C%20where%20the%20most%20noticeable%20is%20to%20a%20series%0Aof%20emerging%20capabilities%2C%20particularly%20in%20the%20areas%20of%20In-Context%20Learning%0A%28ICL%29%20and%20Chain-of-Thought%20%28CoT%29.%20To%20better%20understand%20and%20control%20model%0Aperformance%2C%20many%20studies%20have%20begun%20investigating%20the%20underlying%20causes%20of%0Athese%20phenomena%20and%20their%20impact%20on%20task%20outcomes.%20However%2C%20existing%0Aexplanatory%20frameworks%20predominantly%20focus%20on%20isolating%20and%20explaining%20ICL%20and%0ACoT%20independently%2C%20leading%20to%20an%20incomplete%20understanding%20of%20their%20combined%0Ainfluence%20on%20model%20performance.%20To%20address%20this%20gap%2C%20we%20propose%20the%20Electronic%0ACircuit%20Model%20%28ECM%29%2C%20which%20provides%20a%20foundation%20for%20developing%20scalable%2C%0Alearnable%20policies%20and%20improving%20the%20management%20of%20AI-generated%20content.%0ASpecifically%2C%20ECM%20conceptualizes%20model%20behavior%20as%20an%20electronic%20circuit%3A%20ICL%0Ais%20represented%20as%20semantic%20magnetic%20field%20to%20providing%20an%20additional%20voltage%0Afollowing%20Faraday%27s%20Law%2C%20while%20CoT%20is%20modeled%20as%20series%20resistors%20to%20constrain%0Athe%20model%20output%20performance%20following%20Ohm%27s%20Law.%20Experimental%20results%0Ademonstrate%20that%20the%20ECM%20effectively%20predicts%20and%20explains%20LLM%20performance%0Aacross%20a%20variety%20of%20prompting%20strategies.%20Furthermore%2C%20we%20apply%20ECM%20to%20advanced%0Areasoning%20strategy%20optimization%20on%20a%20series%20of%20tasks%2C%20such%20as%20the%20International%0AOlympiad%20in%20Informatics%20%28IOI%29%20and%20the%20International%20Mathematical%20Olympiad%0A%28IMO%29%2C%20achieving%20competitive%20performance%20that%20surpasses%20nearly%2080%25%20of%20top%20human%0Acompetitors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03325v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DECM%253A%2520A%2520Unified%2520Electronic%2520Circuit%2520Model%2520for%2520Explaining%2520the%2520Emergence%2520of%250A%2520%2520In-Context%2520Learning%2520and%2520Chain-of-Thought%2520in%2520Large%2520Language%2520Model%26entry.906535625%3DQiguang%2520Chen%2520and%2520Libo%2520Qin%2520and%2520Jinhao%2520Liu%2520and%2520Dengyun%2520Peng%2520and%2520Jiaqi%2520Wang%2520and%2520Mengkang%2520Hu%2520and%2520Zhi%2520Chen%2520and%2520Wanxiang%2520Che%2520and%2520Ting%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520led%2520to%2520significant%250Asuccesses%2520across%2520various%2520applications%252C%2520where%2520the%2520most%2520noticeable%2520is%2520to%2520a%2520series%250Aof%2520emerging%2520capabilities%252C%2520particularly%2520in%2520the%2520areas%2520of%2520In-Context%2520Learning%250A%2528ICL%2529%2520and%2520Chain-of-Thought%2520%2528CoT%2529.%2520To%2520better%2520understand%2520and%2520control%2520model%250Aperformance%252C%2520many%2520studies%2520have%2520begun%2520investigating%2520the%2520underlying%2520causes%2520of%250Athese%2520phenomena%2520and%2520their%2520impact%2520on%2520task%2520outcomes.%2520However%252C%2520existing%250Aexplanatory%2520frameworks%2520predominantly%2520focus%2520on%2520isolating%2520and%2520explaining%2520ICL%2520and%250ACoT%2520independently%252C%2520leading%2520to%2520an%2520incomplete%2520understanding%2520of%2520their%2520combined%250Ainfluence%2520on%2520model%2520performance.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520the%2520Electronic%250ACircuit%2520Model%2520%2528ECM%2529%252C%2520which%2520provides%2520a%2520foundation%2520for%2520developing%2520scalable%252C%250Alearnable%2520policies%2520and%2520improving%2520the%2520management%2520of%2520AI-generated%2520content.%250ASpecifically%252C%2520ECM%2520conceptualizes%2520model%2520behavior%2520as%2520an%2520electronic%2520circuit%253A%2520ICL%250Ais%2520represented%2520as%2520semantic%2520magnetic%2520field%2520to%2520providing%2520an%2520additional%2520voltage%250Afollowing%2520Faraday%2527s%2520Law%252C%2520while%2520CoT%2520is%2520modeled%2520as%2520series%2520resistors%2520to%2520constrain%250Athe%2520model%2520output%2520performance%2520following%2520Ohm%2527s%2520Law.%2520Experimental%2520results%250Ademonstrate%2520that%2520the%2520ECM%2520effectively%2520predicts%2520and%2520explains%2520LLM%2520performance%250Aacross%2520a%2520variety%2520of%2520prompting%2520strategies.%2520Furthermore%252C%2520we%2520apply%2520ECM%2520to%2520advanced%250Areasoning%2520strategy%2520optimization%2520on%2520a%2520series%2520of%2520tasks%252C%2520such%2520as%2520the%2520International%250AOlympiad%2520in%2520Informatics%2520%2528IOI%2529%2520and%2520the%2520International%2520Mathematical%2520Olympiad%250A%2528IMO%2529%252C%2520achieving%2520competitive%2520performance%2520that%2520surpasses%2520nearly%252080%2525%2520of%2520top%2520human%250Acompetitors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03325v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ECM%3A%20A%20Unified%20Electronic%20Circuit%20Model%20for%20Explaining%20the%20Emergence%20of%0A%20%20In-Context%20Learning%20and%20Chain-of-Thought%20in%20Large%20Language%20Model&entry.906535625=Qiguang%20Chen%20and%20Libo%20Qin%20and%20Jinhao%20Liu%20and%20Dengyun%20Peng%20and%20Jiaqi%20Wang%20and%20Mengkang%20Hu%20and%20Zhi%20Chen%20and%20Wanxiang%20Che%20and%20Ting%20Liu&entry.1292438233=%20%20Recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20led%20to%20significant%0Asuccesses%20across%20various%20applications%2C%20where%20the%20most%20noticeable%20is%20to%20a%20series%0Aof%20emerging%20capabilities%2C%20particularly%20in%20the%20areas%20of%20In-Context%20Learning%0A%28ICL%29%20and%20Chain-of-Thought%20%28CoT%29.%20To%20better%20understand%20and%20control%20model%0Aperformance%2C%20many%20studies%20have%20begun%20investigating%20the%20underlying%20causes%20of%0Athese%20phenomena%20and%20their%20impact%20on%20task%20outcomes.%20However%2C%20existing%0Aexplanatory%20frameworks%20predominantly%20focus%20on%20isolating%20and%20explaining%20ICL%20and%0ACoT%20independently%2C%20leading%20to%20an%20incomplete%20understanding%20of%20their%20combined%0Ainfluence%20on%20model%20performance.%20To%20address%20this%20gap%2C%20we%20propose%20the%20Electronic%0ACircuit%20Model%20%28ECM%29%2C%20which%20provides%20a%20foundation%20for%20developing%20scalable%2C%0Alearnable%20policies%20and%20improving%20the%20management%20of%20AI-generated%20content.%0ASpecifically%2C%20ECM%20conceptualizes%20model%20behavior%20as%20an%20electronic%20circuit%3A%20ICL%0Ais%20represented%20as%20semantic%20magnetic%20field%20to%20providing%20an%20additional%20voltage%0Afollowing%20Faraday%27s%20Law%2C%20while%20CoT%20is%20modeled%20as%20series%20resistors%20to%20constrain%0Athe%20model%20output%20performance%20following%20Ohm%27s%20Law.%20Experimental%20results%0Ademonstrate%20that%20the%20ECM%20effectively%20predicts%20and%20explains%20LLM%20performance%0Aacross%20a%20variety%20of%20prompting%20strategies.%20Furthermore%2C%20we%20apply%20ECM%20to%20advanced%0Areasoning%20strategy%20optimization%20on%20a%20series%20of%20tasks%2C%20such%20as%20the%20International%0AOlympiad%20in%20Informatics%20%28IOI%29%20and%20the%20International%20Mathematical%20Olympiad%0A%28IMO%29%2C%20achieving%20competitive%20performance%20that%20surpasses%20nearly%2080%25%20of%20top%20human%0Acompetitors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03325v1&entry.124074799=Read"},
{"title": "HiLo: Learning Whole-Body Human-like Locomotion with Motion Tracking\n  Controller", "author": "Qiyuan Zhang and Chenfan Weng and Guanwu Li and Fulai He and Yusheng Cai", "abstract": "  Deep Reinforcement Learning (RL) has emerged as a promising method to develop\nhumanoid robot locomotion controllers. Despite the robust and stable locomotion\ndemonstrated by previous RL controllers, their behavior often lacks the natural\nand agile motion patterns necessary for human-centric scenarios. In this work,\nwe propose HiLo (human-like locomotion with motion tracking), an effective\nframework designed to learn RL policies that perform human-like locomotion. The\nprimary challenges of human-like locomotion are complex reward engineering and\ndomain randomization. HiLo overcomes these issues by developing an RL-based\nmotion tracking controller and simple domain randomization through random force\ninjection and action delay. Within the framework of HiLo, the whole-body\ncontrol problem can be decomposed into two components: One part is solved using\nan open-loop control method, while the residual part is addressed with RL\npolicies. A distributional value function is also implemented to stabilize the\ntraining process by improving the estimation of cumulative rewards under\nperturbed dynamics. Our experiments demonstrate that the motion tracking\ncontroller trained using HiLo can perform natural and agile human-like\nlocomotion while exhibiting resilience to external disturbances in real-world\nsystems. Furthermore, we show that the motion patterns of humanoid robots can\nbe adapted through the residual mechanism without fine-tuning, allowing quick\nadjustments to task requirements.\n", "link": "http://arxiv.org/abs/2502.03122v1", "date": "2025-02-05", "relevancy": 2.2525, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5869}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5777}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiLo%3A%20Learning%20Whole-Body%20Human-like%20Locomotion%20with%20Motion%20Tracking%0A%20%20Controller&body=Title%3A%20HiLo%3A%20Learning%20Whole-Body%20Human-like%20Locomotion%20with%20Motion%20Tracking%0A%20%20Controller%0AAuthor%3A%20Qiyuan%20Zhang%20and%20Chenfan%20Weng%20and%20Guanwu%20Li%20and%20Fulai%20He%20and%20Yusheng%20Cai%0AAbstract%3A%20%20%20Deep%20Reinforcement%20Learning%20%28RL%29%20has%20emerged%20as%20a%20promising%20method%20to%20develop%0Ahumanoid%20robot%20locomotion%20controllers.%20Despite%20the%20robust%20and%20stable%20locomotion%0Ademonstrated%20by%20previous%20RL%20controllers%2C%20their%20behavior%20often%20lacks%20the%20natural%0Aand%20agile%20motion%20patterns%20necessary%20for%20human-centric%20scenarios.%20In%20this%20work%2C%0Awe%20propose%20HiLo%20%28human-like%20locomotion%20with%20motion%20tracking%29%2C%20an%20effective%0Aframework%20designed%20to%20learn%20RL%20policies%20that%20perform%20human-like%20locomotion.%20The%0Aprimary%20challenges%20of%20human-like%20locomotion%20are%20complex%20reward%20engineering%20and%0Adomain%20randomization.%20HiLo%20overcomes%20these%20issues%20by%20developing%20an%20RL-based%0Amotion%20tracking%20controller%20and%20simple%20domain%20randomization%20through%20random%20force%0Ainjection%20and%20action%20delay.%20Within%20the%20framework%20of%20HiLo%2C%20the%20whole-body%0Acontrol%20problem%20can%20be%20decomposed%20into%20two%20components%3A%20One%20part%20is%20solved%20using%0Aan%20open-loop%20control%20method%2C%20while%20the%20residual%20part%20is%20addressed%20with%20RL%0Apolicies.%20A%20distributional%20value%20function%20is%20also%20implemented%20to%20stabilize%20the%0Atraining%20process%20by%20improving%20the%20estimation%20of%20cumulative%20rewards%20under%0Aperturbed%20dynamics.%20Our%20experiments%20demonstrate%20that%20the%20motion%20tracking%0Acontroller%20trained%20using%20HiLo%20can%20perform%20natural%20and%20agile%20human-like%0Alocomotion%20while%20exhibiting%20resilience%20to%20external%20disturbances%20in%20real-world%0Asystems.%20Furthermore%2C%20we%20show%20that%20the%20motion%20patterns%20of%20humanoid%20robots%20can%0Abe%20adapted%20through%20the%20residual%20mechanism%20without%20fine-tuning%2C%20allowing%20quick%0Aadjustments%20to%20task%20requirements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03122v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiLo%253A%2520Learning%2520Whole-Body%2520Human-like%2520Locomotion%2520with%2520Motion%2520Tracking%250A%2520%2520Controller%26entry.906535625%3DQiyuan%2520Zhang%2520and%2520Chenfan%2520Weng%2520and%2520Guanwu%2520Li%2520and%2520Fulai%2520He%2520and%2520Yusheng%2520Cai%26entry.1292438233%3D%2520%2520Deep%2520Reinforcement%2520Learning%2520%2528RL%2529%2520has%2520emerged%2520as%2520a%2520promising%2520method%2520to%2520develop%250Ahumanoid%2520robot%2520locomotion%2520controllers.%2520Despite%2520the%2520robust%2520and%2520stable%2520locomotion%250Ademonstrated%2520by%2520previous%2520RL%2520controllers%252C%2520their%2520behavior%2520often%2520lacks%2520the%2520natural%250Aand%2520agile%2520motion%2520patterns%2520necessary%2520for%2520human-centric%2520scenarios.%2520In%2520this%2520work%252C%250Awe%2520propose%2520HiLo%2520%2528human-like%2520locomotion%2520with%2520motion%2520tracking%2529%252C%2520an%2520effective%250Aframework%2520designed%2520to%2520learn%2520RL%2520policies%2520that%2520perform%2520human-like%2520locomotion.%2520The%250Aprimary%2520challenges%2520of%2520human-like%2520locomotion%2520are%2520complex%2520reward%2520engineering%2520and%250Adomain%2520randomization.%2520HiLo%2520overcomes%2520these%2520issues%2520by%2520developing%2520an%2520RL-based%250Amotion%2520tracking%2520controller%2520and%2520simple%2520domain%2520randomization%2520through%2520random%2520force%250Ainjection%2520and%2520action%2520delay.%2520Within%2520the%2520framework%2520of%2520HiLo%252C%2520the%2520whole-body%250Acontrol%2520problem%2520can%2520be%2520decomposed%2520into%2520two%2520components%253A%2520One%2520part%2520is%2520solved%2520using%250Aan%2520open-loop%2520control%2520method%252C%2520while%2520the%2520residual%2520part%2520is%2520addressed%2520with%2520RL%250Apolicies.%2520A%2520distributional%2520value%2520function%2520is%2520also%2520implemented%2520to%2520stabilize%2520the%250Atraining%2520process%2520by%2520improving%2520the%2520estimation%2520of%2520cumulative%2520rewards%2520under%250Aperturbed%2520dynamics.%2520Our%2520experiments%2520demonstrate%2520that%2520the%2520motion%2520tracking%250Acontroller%2520trained%2520using%2520HiLo%2520can%2520perform%2520natural%2520and%2520agile%2520human-like%250Alocomotion%2520while%2520exhibiting%2520resilience%2520to%2520external%2520disturbances%2520in%2520real-world%250Asystems.%2520Furthermore%252C%2520we%2520show%2520that%2520the%2520motion%2520patterns%2520of%2520humanoid%2520robots%2520can%250Abe%2520adapted%2520through%2520the%2520residual%2520mechanism%2520without%2520fine-tuning%252C%2520allowing%2520quick%250Aadjustments%2520to%2520task%2520requirements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03122v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiLo%3A%20Learning%20Whole-Body%20Human-like%20Locomotion%20with%20Motion%20Tracking%0A%20%20Controller&entry.906535625=Qiyuan%20Zhang%20and%20Chenfan%20Weng%20and%20Guanwu%20Li%20and%20Fulai%20He%20and%20Yusheng%20Cai&entry.1292438233=%20%20Deep%20Reinforcement%20Learning%20%28RL%29%20has%20emerged%20as%20a%20promising%20method%20to%20develop%0Ahumanoid%20robot%20locomotion%20controllers.%20Despite%20the%20robust%20and%20stable%20locomotion%0Ademonstrated%20by%20previous%20RL%20controllers%2C%20their%20behavior%20often%20lacks%20the%20natural%0Aand%20agile%20motion%20patterns%20necessary%20for%20human-centric%20scenarios.%20In%20this%20work%2C%0Awe%20propose%20HiLo%20%28human-like%20locomotion%20with%20motion%20tracking%29%2C%20an%20effective%0Aframework%20designed%20to%20learn%20RL%20policies%20that%20perform%20human-like%20locomotion.%20The%0Aprimary%20challenges%20of%20human-like%20locomotion%20are%20complex%20reward%20engineering%20and%0Adomain%20randomization.%20HiLo%20overcomes%20these%20issues%20by%20developing%20an%20RL-based%0Amotion%20tracking%20controller%20and%20simple%20domain%20randomization%20through%20random%20force%0Ainjection%20and%20action%20delay.%20Within%20the%20framework%20of%20HiLo%2C%20the%20whole-body%0Acontrol%20problem%20can%20be%20decomposed%20into%20two%20components%3A%20One%20part%20is%20solved%20using%0Aan%20open-loop%20control%20method%2C%20while%20the%20residual%20part%20is%20addressed%20with%20RL%0Apolicies.%20A%20distributional%20value%20function%20is%20also%20implemented%20to%20stabilize%20the%0Atraining%20process%20by%20improving%20the%20estimation%20of%20cumulative%20rewards%20under%0Aperturbed%20dynamics.%20Our%20experiments%20demonstrate%20that%20the%20motion%20tracking%0Acontroller%20trained%20using%20HiLo%20can%20perform%20natural%20and%20agile%20human-like%0Alocomotion%20while%20exhibiting%20resilience%20to%20external%20disturbances%20in%20real-world%0Asystems.%20Furthermore%2C%20we%20show%20that%20the%20motion%20patterns%20of%20humanoid%20robots%20can%0Abe%20adapted%20through%20the%20residual%20mechanism%20without%20fine-tuning%2C%20allowing%20quick%0Aadjustments%20to%20task%20requirements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03122v1&entry.124074799=Read"},
{"title": "OverThink: Slowdown Attacks on Reasoning LLMs", "author": "Abhinav Kumar and Jaechul Roh and Ali Naseh and Marzena Karpinska and Mohit Iyyer and Amir Houmansadr and Eugene Bagdasarian", "abstract": "  We increase overhead for applications that rely on reasoning LLMs-we force\nmodels to spend an amplified number of reasoning tokens, i.e., \"overthink\", to\nrespond to the user query while providing contextually correct answers. The\nadversary performs an OVERTHINK attack by injecting decoy reasoning problems\ninto the public content that is used by the reasoning LLM (e.g., for RAG\napplications) during inference time. Due to the nature of our decoy problems\n(e.g., a Markov Decision Process), modified texts do not violate safety\nguardrails. We evaluated our attack across closed-(OpenAI o1, o1-mini, o3-mini)\nand open-(DeepSeek R1) weights reasoning models on the FreshQA and SQuAD\ndatasets. Our results show up to 18x slowdown on FreshQA dataset and 46x\nslowdown on SQuAD dataset. The attack also shows high transferability across\nmodels. To protect applications, we discuss and implement defenses leveraging\nLLM-based and system design approaches. Finally, we discuss societal,\nfinancial, and energy impacts of OVERTHINK attack which could amplify the costs\nfor third-party applications operating reasoning models.\n", "link": "http://arxiv.org/abs/2502.02542v2", "date": "2025-02-05", "relevancy": 2.2403, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4706}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4368}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OverThink%3A%20Slowdown%20Attacks%20on%20Reasoning%20LLMs&body=Title%3A%20OverThink%3A%20Slowdown%20Attacks%20on%20Reasoning%20LLMs%0AAuthor%3A%20Abhinav%20Kumar%20and%20Jaechul%20Roh%20and%20Ali%20Naseh%20and%20Marzena%20Karpinska%20and%20Mohit%20Iyyer%20and%20Amir%20Houmansadr%20and%20Eugene%20Bagdasarian%0AAbstract%3A%20%20%20We%20increase%20overhead%20for%20applications%20that%20rely%20on%20reasoning%20LLMs-we%20force%0Amodels%20to%20spend%20an%20amplified%20number%20of%20reasoning%20tokens%2C%20i.e.%2C%20%22overthink%22%2C%20to%0Arespond%20to%20the%20user%20query%20while%20providing%20contextually%20correct%20answers.%20The%0Aadversary%20performs%20an%20OVERTHINK%20attack%20by%20injecting%20decoy%20reasoning%20problems%0Ainto%20the%20public%20content%20that%20is%20used%20by%20the%20reasoning%20LLM%20%28e.g.%2C%20for%20RAG%0Aapplications%29%20during%20inference%20time.%20Due%20to%20the%20nature%20of%20our%20decoy%20problems%0A%28e.g.%2C%20a%20Markov%20Decision%20Process%29%2C%20modified%20texts%20do%20not%20violate%20safety%0Aguardrails.%20We%20evaluated%20our%20attack%20across%20closed-%28OpenAI%20o1%2C%20o1-mini%2C%20o3-mini%29%0Aand%20open-%28DeepSeek%20R1%29%20weights%20reasoning%20models%20on%20the%20FreshQA%20and%20SQuAD%0Adatasets.%20Our%20results%20show%20up%20to%2018x%20slowdown%20on%20FreshQA%20dataset%20and%2046x%0Aslowdown%20on%20SQuAD%20dataset.%20The%20attack%20also%20shows%20high%20transferability%20across%0Amodels.%20To%20protect%20applications%2C%20we%20discuss%20and%20implement%20defenses%20leveraging%0ALLM-based%20and%20system%20design%20approaches.%20Finally%2C%20we%20discuss%20societal%2C%0Afinancial%2C%20and%20energy%20impacts%20of%20OVERTHINK%20attack%20which%20could%20amplify%20the%20costs%0Afor%20third-party%20applications%20operating%20reasoning%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02542v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOverThink%253A%2520Slowdown%2520Attacks%2520on%2520Reasoning%2520LLMs%26entry.906535625%3DAbhinav%2520Kumar%2520and%2520Jaechul%2520Roh%2520and%2520Ali%2520Naseh%2520and%2520Marzena%2520Karpinska%2520and%2520Mohit%2520Iyyer%2520and%2520Amir%2520Houmansadr%2520and%2520Eugene%2520Bagdasarian%26entry.1292438233%3D%2520%2520We%2520increase%2520overhead%2520for%2520applications%2520that%2520rely%2520on%2520reasoning%2520LLMs-we%2520force%250Amodels%2520to%2520spend%2520an%2520amplified%2520number%2520of%2520reasoning%2520tokens%252C%2520i.e.%252C%2520%2522overthink%2522%252C%2520to%250Arespond%2520to%2520the%2520user%2520query%2520while%2520providing%2520contextually%2520correct%2520answers.%2520The%250Aadversary%2520performs%2520an%2520OVERTHINK%2520attack%2520by%2520injecting%2520decoy%2520reasoning%2520problems%250Ainto%2520the%2520public%2520content%2520that%2520is%2520used%2520by%2520the%2520reasoning%2520LLM%2520%2528e.g.%252C%2520for%2520RAG%250Aapplications%2529%2520during%2520inference%2520time.%2520Due%2520to%2520the%2520nature%2520of%2520our%2520decoy%2520problems%250A%2528e.g.%252C%2520a%2520Markov%2520Decision%2520Process%2529%252C%2520modified%2520texts%2520do%2520not%2520violate%2520safety%250Aguardrails.%2520We%2520evaluated%2520our%2520attack%2520across%2520closed-%2528OpenAI%2520o1%252C%2520o1-mini%252C%2520o3-mini%2529%250Aand%2520open-%2528DeepSeek%2520R1%2529%2520weights%2520reasoning%2520models%2520on%2520the%2520FreshQA%2520and%2520SQuAD%250Adatasets.%2520Our%2520results%2520show%2520up%2520to%252018x%2520slowdown%2520on%2520FreshQA%2520dataset%2520and%252046x%250Aslowdown%2520on%2520SQuAD%2520dataset.%2520The%2520attack%2520also%2520shows%2520high%2520transferability%2520across%250Amodels.%2520To%2520protect%2520applications%252C%2520we%2520discuss%2520and%2520implement%2520defenses%2520leveraging%250ALLM-based%2520and%2520system%2520design%2520approaches.%2520Finally%252C%2520we%2520discuss%2520societal%252C%250Afinancial%252C%2520and%2520energy%2520impacts%2520of%2520OVERTHINK%2520attack%2520which%2520could%2520amplify%2520the%2520costs%250Afor%2520third-party%2520applications%2520operating%2520reasoning%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02542v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OverThink%3A%20Slowdown%20Attacks%20on%20Reasoning%20LLMs&entry.906535625=Abhinav%20Kumar%20and%20Jaechul%20Roh%20and%20Ali%20Naseh%20and%20Marzena%20Karpinska%20and%20Mohit%20Iyyer%20and%20Amir%20Houmansadr%20and%20Eugene%20Bagdasarian&entry.1292438233=%20%20We%20increase%20overhead%20for%20applications%20that%20rely%20on%20reasoning%20LLMs-we%20force%0Amodels%20to%20spend%20an%20amplified%20number%20of%20reasoning%20tokens%2C%20i.e.%2C%20%22overthink%22%2C%20to%0Arespond%20to%20the%20user%20query%20while%20providing%20contextually%20correct%20answers.%20The%0Aadversary%20performs%20an%20OVERTHINK%20attack%20by%20injecting%20decoy%20reasoning%20problems%0Ainto%20the%20public%20content%20that%20is%20used%20by%20the%20reasoning%20LLM%20%28e.g.%2C%20for%20RAG%0Aapplications%29%20during%20inference%20time.%20Due%20to%20the%20nature%20of%20our%20decoy%20problems%0A%28e.g.%2C%20a%20Markov%20Decision%20Process%29%2C%20modified%20texts%20do%20not%20violate%20safety%0Aguardrails.%20We%20evaluated%20our%20attack%20across%20closed-%28OpenAI%20o1%2C%20o1-mini%2C%20o3-mini%29%0Aand%20open-%28DeepSeek%20R1%29%20weights%20reasoning%20models%20on%20the%20FreshQA%20and%20SQuAD%0Adatasets.%20Our%20results%20show%20up%20to%2018x%20slowdown%20on%20FreshQA%20dataset%20and%2046x%0Aslowdown%20on%20SQuAD%20dataset.%20The%20attack%20also%20shows%20high%20transferability%20across%0Amodels.%20To%20protect%20applications%2C%20we%20discuss%20and%20implement%20defenses%20leveraging%0ALLM-based%20and%20system%20design%20approaches.%20Finally%2C%20we%20discuss%20societal%2C%0Afinancial%2C%20and%20energy%20impacts%20of%20OVERTHINK%20attack%20which%20could%20amplify%20the%20costs%0Afor%20third-party%20applications%20operating%20reasoning%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02542v2&entry.124074799=Read"},
{"title": "Interaction-Aware Gaussian Weighting for Clustered Federated Learning", "author": "Alessandro Licciardi and Davide Leo and Eros Fan\u00ed and Barbara Caputo and Marco Ciccone", "abstract": "  Federated Learning (FL) emerged as a decentralized paradigm to train models\nwhile preserving privacy. However, conventional FL struggles with data\nheterogeneity and class imbalance, which degrade model performance. Clustered\nFL balances personalization and decentralized training by grouping clients with\nanalogous data distributions, enabling improved accuracy while adhering to\nprivacy constraints. This approach effectively mitigates the adverse impact of\nheterogeneity in FL. In this work, we propose a novel clustered FL method,\nFedGWC (Federated Gaussian Weighting Clustering), which groups clients based on\ntheir data distribution, allowing training of a more robust and personalized\nmodel on the identified clusters. FedGWC identifies homogeneous clusters by\ntransforming individual empirical losses to model client interactions with a\nGaussian reward mechanism. Additionally, we introduce the Wasserstein Adjusted\nScore, a new clustering metric for FL to evaluate cluster cohesion with respect\nto the individual class distribution. Our experiments on benchmark datasets\nshow that FedGWC outperforms existing FL algorithms in cluster quality and\nclassification accuracy, validating the efficacy of our approach.\n", "link": "http://arxiv.org/abs/2502.03340v1", "date": "2025-02-05", "relevancy": 2.22, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4512}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4441}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interaction-Aware%20Gaussian%20Weighting%20for%20Clustered%20Federated%20Learning&body=Title%3A%20Interaction-Aware%20Gaussian%20Weighting%20for%20Clustered%20Federated%20Learning%0AAuthor%3A%20Alessandro%20Licciardi%20and%20Davide%20Leo%20and%20Eros%20Fan%C3%AD%20and%20Barbara%20Caputo%20and%20Marco%20Ciccone%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20emerged%20as%20a%20decentralized%20paradigm%20to%20train%20models%0Awhile%20preserving%20privacy.%20However%2C%20conventional%20FL%20struggles%20with%20data%0Aheterogeneity%20and%20class%20imbalance%2C%20which%20degrade%20model%20performance.%20Clustered%0AFL%20balances%20personalization%20and%20decentralized%20training%20by%20grouping%20clients%20with%0Aanalogous%20data%20distributions%2C%20enabling%20improved%20accuracy%20while%20adhering%20to%0Aprivacy%20constraints.%20This%20approach%20effectively%20mitigates%20the%20adverse%20impact%20of%0Aheterogeneity%20in%20FL.%20In%20this%20work%2C%20we%20propose%20a%20novel%20clustered%20FL%20method%2C%0AFedGWC%20%28Federated%20Gaussian%20Weighting%20Clustering%29%2C%20which%20groups%20clients%20based%20on%0Atheir%20data%20distribution%2C%20allowing%20training%20of%20a%20more%20robust%20and%20personalized%0Amodel%20on%20the%20identified%20clusters.%20FedGWC%20identifies%20homogeneous%20clusters%20by%0Atransforming%20individual%20empirical%20losses%20to%20model%20client%20interactions%20with%20a%0AGaussian%20reward%20mechanism.%20Additionally%2C%20we%20introduce%20the%20Wasserstein%20Adjusted%0AScore%2C%20a%20new%20clustering%20metric%20for%20FL%20to%20evaluate%20cluster%20cohesion%20with%20respect%0Ato%20the%20individual%20class%20distribution.%20Our%20experiments%20on%20benchmark%20datasets%0Ashow%20that%20FedGWC%20outperforms%20existing%20FL%20algorithms%20in%20cluster%20quality%20and%0Aclassification%20accuracy%2C%20validating%20the%20efficacy%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03340v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInteraction-Aware%2520Gaussian%2520Weighting%2520for%2520Clustered%2520Federated%2520Learning%26entry.906535625%3DAlessandro%2520Licciardi%2520and%2520Davide%2520Leo%2520and%2520Eros%2520Fan%25C3%25AD%2520and%2520Barbara%2520Caputo%2520and%2520Marco%2520Ciccone%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520emerged%2520as%2520a%2520decentralized%2520paradigm%2520to%2520train%2520models%250Awhile%2520preserving%2520privacy.%2520However%252C%2520conventional%2520FL%2520struggles%2520with%2520data%250Aheterogeneity%2520and%2520class%2520imbalance%252C%2520which%2520degrade%2520model%2520performance.%2520Clustered%250AFL%2520balances%2520personalization%2520and%2520decentralized%2520training%2520by%2520grouping%2520clients%2520with%250Aanalogous%2520data%2520distributions%252C%2520enabling%2520improved%2520accuracy%2520while%2520adhering%2520to%250Aprivacy%2520constraints.%2520This%2520approach%2520effectively%2520mitigates%2520the%2520adverse%2520impact%2520of%250Aheterogeneity%2520in%2520FL.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520clustered%2520FL%2520method%252C%250AFedGWC%2520%2528Federated%2520Gaussian%2520Weighting%2520Clustering%2529%252C%2520which%2520groups%2520clients%2520based%2520on%250Atheir%2520data%2520distribution%252C%2520allowing%2520training%2520of%2520a%2520more%2520robust%2520and%2520personalized%250Amodel%2520on%2520the%2520identified%2520clusters.%2520FedGWC%2520identifies%2520homogeneous%2520clusters%2520by%250Atransforming%2520individual%2520empirical%2520losses%2520to%2520model%2520client%2520interactions%2520with%2520a%250AGaussian%2520reward%2520mechanism.%2520Additionally%252C%2520we%2520introduce%2520the%2520Wasserstein%2520Adjusted%250AScore%252C%2520a%2520new%2520clustering%2520metric%2520for%2520FL%2520to%2520evaluate%2520cluster%2520cohesion%2520with%2520respect%250Ato%2520the%2520individual%2520class%2520distribution.%2520Our%2520experiments%2520on%2520benchmark%2520datasets%250Ashow%2520that%2520FedGWC%2520outperforms%2520existing%2520FL%2520algorithms%2520in%2520cluster%2520quality%2520and%250Aclassification%2520accuracy%252C%2520validating%2520the%2520efficacy%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03340v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interaction-Aware%20Gaussian%20Weighting%20for%20Clustered%20Federated%20Learning&entry.906535625=Alessandro%20Licciardi%20and%20Davide%20Leo%20and%20Eros%20Fan%C3%AD%20and%20Barbara%20Caputo%20and%20Marco%20Ciccone&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20emerged%20as%20a%20decentralized%20paradigm%20to%20train%20models%0Awhile%20preserving%20privacy.%20However%2C%20conventional%20FL%20struggles%20with%20data%0Aheterogeneity%20and%20class%20imbalance%2C%20which%20degrade%20model%20performance.%20Clustered%0AFL%20balances%20personalization%20and%20decentralized%20training%20by%20grouping%20clients%20with%0Aanalogous%20data%20distributions%2C%20enabling%20improved%20accuracy%20while%20adhering%20to%0Aprivacy%20constraints.%20This%20approach%20effectively%20mitigates%20the%20adverse%20impact%20of%0Aheterogeneity%20in%20FL.%20In%20this%20work%2C%20we%20propose%20a%20novel%20clustered%20FL%20method%2C%0AFedGWC%20%28Federated%20Gaussian%20Weighting%20Clustering%29%2C%20which%20groups%20clients%20based%20on%0Atheir%20data%20distribution%2C%20allowing%20training%20of%20a%20more%20robust%20and%20personalized%0Amodel%20on%20the%20identified%20clusters.%20FedGWC%20identifies%20homogeneous%20clusters%20by%0Atransforming%20individual%20empirical%20losses%20to%20model%20client%20interactions%20with%20a%0AGaussian%20reward%20mechanism.%20Additionally%2C%20we%20introduce%20the%20Wasserstein%20Adjusted%0AScore%2C%20a%20new%20clustering%20metric%20for%20FL%20to%20evaluate%20cluster%20cohesion%20with%20respect%0Ato%20the%20individual%20class%20distribution.%20Our%20experiments%20on%20benchmark%20datasets%0Ashow%20that%20FedGWC%20outperforms%20existing%20FL%20algorithms%20in%20cluster%20quality%20and%0Aclassification%20accuracy%2C%20validating%20the%20efficacy%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03340v1&entry.124074799=Read"},
{"title": "A Beam's Eye View to Fluence Maps 3D Network for Ultra Fast VMAT\n  Radiotherapy Planning", "author": "Simon Arberet and Florin C. Ghesu and Riqiang Gao and Martin Kraus and Jonathan Sackett and Esa Kuusela and Ali Kamen", "abstract": "  Volumetric Modulated Arc Therapy (VMAT) revolutionizes cancer treatment by\nprecisely delivering radiation while sparing healthy tissues. Fluence maps\ngeneration, crucial in VMAT planning, traditionally involves complex and\niterative, and thus time consuming processes. These fluence maps are\nsubsequently leveraged for leaf-sequence. The deep-learning approach presented\nin this article aims to expedite this by directly predicting fluence maps from\npatient data. We developed a 3D network which we trained in a supervised way\nusing a combination of L1 and L2 losses, and RT plans generated by Eclipse and\nfrom the REQUITE dataset, taking the RT dose map as input and the fluence maps\ncomputed from the corresponding RT plans as target. Our network predicts\njointly the 180 fluence maps corresponding to the 180 control points (CP) of\nsingle arc VMAT plans. In order to help the network, we pre-process the input\ndose by computing the projections of the 3D dose map to the beam's eye view\n(BEV) of the 180 CPs, in the same coordinate system as the fluence maps. We\ngenerated over 2000 VMAT plans using Eclipse to scale up the dataset size.\nAdditionally, we evaluated various network architectures and analyzed the\nimpact of increasing the dataset size. We are measuring the performance in the\n2D fluence maps domain using image metrics (PSNR, SSIM), as well as in the 3D\ndose domain using the dose-volume histogram (DVH) on a validation dataset. The\nnetwork inference, which does not include the data loading and processing, is\nless than 20ms. Using our proposed 3D network architecture as well as\nincreasing the dataset size using Eclipse improved the fluence map\nreconstruction performance by approximately 8 dB in PSNR compared to a U-Net\narchitecture trained on the original REQUITE dataset. The resulting DVHs are\nvery close to the one of the input target dose.\n", "link": "http://arxiv.org/abs/2502.03360v1", "date": "2025-02-05", "relevancy": 2.1935, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5493}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5482}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Beam%27s%20Eye%20View%20to%20Fluence%20Maps%203D%20Network%20for%20Ultra%20Fast%20VMAT%0A%20%20Radiotherapy%20Planning&body=Title%3A%20A%20Beam%27s%20Eye%20View%20to%20Fluence%20Maps%203D%20Network%20for%20Ultra%20Fast%20VMAT%0A%20%20Radiotherapy%20Planning%0AAuthor%3A%20Simon%20Arberet%20and%20Florin%20C.%20Ghesu%20and%20Riqiang%20Gao%20and%20Martin%20Kraus%20and%20Jonathan%20Sackett%20and%20Esa%20Kuusela%20and%20Ali%20Kamen%0AAbstract%3A%20%20%20Volumetric%20Modulated%20Arc%20Therapy%20%28VMAT%29%20revolutionizes%20cancer%20treatment%20by%0Aprecisely%20delivering%20radiation%20while%20sparing%20healthy%20tissues.%20Fluence%20maps%0Ageneration%2C%20crucial%20in%20VMAT%20planning%2C%20traditionally%20involves%20complex%20and%0Aiterative%2C%20and%20thus%20time%20consuming%20processes.%20These%20fluence%20maps%20are%0Asubsequently%20leveraged%20for%20leaf-sequence.%20The%20deep-learning%20approach%20presented%0Ain%20this%20article%20aims%20to%20expedite%20this%20by%20directly%20predicting%20fluence%20maps%20from%0Apatient%20data.%20We%20developed%20a%203D%20network%20which%20we%20trained%20in%20a%20supervised%20way%0Ausing%20a%20combination%20of%20L1%20and%20L2%20losses%2C%20and%20RT%20plans%20generated%20by%20Eclipse%20and%0Afrom%20the%20REQUITE%20dataset%2C%20taking%20the%20RT%20dose%20map%20as%20input%20and%20the%20fluence%20maps%0Acomputed%20from%20the%20corresponding%20RT%20plans%20as%20target.%20Our%20network%20predicts%0Ajointly%20the%20180%20fluence%20maps%20corresponding%20to%20the%20180%20control%20points%20%28CP%29%20of%0Asingle%20arc%20VMAT%20plans.%20In%20order%20to%20help%20the%20network%2C%20we%20pre-process%20the%20input%0Adose%20by%20computing%20the%20projections%20of%20the%203D%20dose%20map%20to%20the%20beam%27s%20eye%20view%0A%28BEV%29%20of%20the%20180%20CPs%2C%20in%20the%20same%20coordinate%20system%20as%20the%20fluence%20maps.%20We%0Agenerated%20over%202000%20VMAT%20plans%20using%20Eclipse%20to%20scale%20up%20the%20dataset%20size.%0AAdditionally%2C%20we%20evaluated%20various%20network%20architectures%20and%20analyzed%20the%0Aimpact%20of%20increasing%20the%20dataset%20size.%20We%20are%20measuring%20the%20performance%20in%20the%0A2D%20fluence%20maps%20domain%20using%20image%20metrics%20%28PSNR%2C%20SSIM%29%2C%20as%20well%20as%20in%20the%203D%0Adose%20domain%20using%20the%20dose-volume%20histogram%20%28DVH%29%20on%20a%20validation%20dataset.%20The%0Anetwork%20inference%2C%20which%20does%20not%20include%20the%20data%20loading%20and%20processing%2C%20is%0Aless%20than%2020ms.%20Using%20our%20proposed%203D%20network%20architecture%20as%20well%20as%0Aincreasing%20the%20dataset%20size%20using%20Eclipse%20improved%20the%20fluence%20map%0Areconstruction%20performance%20by%20approximately%208%20dB%20in%20PSNR%20compared%20to%20a%20U-Net%0Aarchitecture%20trained%20on%20the%20original%20REQUITE%20dataset.%20The%20resulting%20DVHs%20are%0Avery%20close%20to%20the%20one%20of%20the%20input%20target%20dose.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03360v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Beam%2527s%2520Eye%2520View%2520to%2520Fluence%2520Maps%25203D%2520Network%2520for%2520Ultra%2520Fast%2520VMAT%250A%2520%2520Radiotherapy%2520Planning%26entry.906535625%3DSimon%2520Arberet%2520and%2520Florin%2520C.%2520Ghesu%2520and%2520Riqiang%2520Gao%2520and%2520Martin%2520Kraus%2520and%2520Jonathan%2520Sackett%2520and%2520Esa%2520Kuusela%2520and%2520Ali%2520Kamen%26entry.1292438233%3D%2520%2520Volumetric%2520Modulated%2520Arc%2520Therapy%2520%2528VMAT%2529%2520revolutionizes%2520cancer%2520treatment%2520by%250Aprecisely%2520delivering%2520radiation%2520while%2520sparing%2520healthy%2520tissues.%2520Fluence%2520maps%250Ageneration%252C%2520crucial%2520in%2520VMAT%2520planning%252C%2520traditionally%2520involves%2520complex%2520and%250Aiterative%252C%2520and%2520thus%2520time%2520consuming%2520processes.%2520These%2520fluence%2520maps%2520are%250Asubsequently%2520leveraged%2520for%2520leaf-sequence.%2520The%2520deep-learning%2520approach%2520presented%250Ain%2520this%2520article%2520aims%2520to%2520expedite%2520this%2520by%2520directly%2520predicting%2520fluence%2520maps%2520from%250Apatient%2520data.%2520We%2520developed%2520a%25203D%2520network%2520which%2520we%2520trained%2520in%2520a%2520supervised%2520way%250Ausing%2520a%2520combination%2520of%2520L1%2520and%2520L2%2520losses%252C%2520and%2520RT%2520plans%2520generated%2520by%2520Eclipse%2520and%250Afrom%2520the%2520REQUITE%2520dataset%252C%2520taking%2520the%2520RT%2520dose%2520map%2520as%2520input%2520and%2520the%2520fluence%2520maps%250Acomputed%2520from%2520the%2520corresponding%2520RT%2520plans%2520as%2520target.%2520Our%2520network%2520predicts%250Ajointly%2520the%2520180%2520fluence%2520maps%2520corresponding%2520to%2520the%2520180%2520control%2520points%2520%2528CP%2529%2520of%250Asingle%2520arc%2520VMAT%2520plans.%2520In%2520order%2520to%2520help%2520the%2520network%252C%2520we%2520pre-process%2520the%2520input%250Adose%2520by%2520computing%2520the%2520projections%2520of%2520the%25203D%2520dose%2520map%2520to%2520the%2520beam%2527s%2520eye%2520view%250A%2528BEV%2529%2520of%2520the%2520180%2520CPs%252C%2520in%2520the%2520same%2520coordinate%2520system%2520as%2520the%2520fluence%2520maps.%2520We%250Agenerated%2520over%25202000%2520VMAT%2520plans%2520using%2520Eclipse%2520to%2520scale%2520up%2520the%2520dataset%2520size.%250AAdditionally%252C%2520we%2520evaluated%2520various%2520network%2520architectures%2520and%2520analyzed%2520the%250Aimpact%2520of%2520increasing%2520the%2520dataset%2520size.%2520We%2520are%2520measuring%2520the%2520performance%2520in%2520the%250A2D%2520fluence%2520maps%2520domain%2520using%2520image%2520metrics%2520%2528PSNR%252C%2520SSIM%2529%252C%2520as%2520well%2520as%2520in%2520the%25203D%250Adose%2520domain%2520using%2520the%2520dose-volume%2520histogram%2520%2528DVH%2529%2520on%2520a%2520validation%2520dataset.%2520The%250Anetwork%2520inference%252C%2520which%2520does%2520not%2520include%2520the%2520data%2520loading%2520and%2520processing%252C%2520is%250Aless%2520than%252020ms.%2520Using%2520our%2520proposed%25203D%2520network%2520architecture%2520as%2520well%2520as%250Aincreasing%2520the%2520dataset%2520size%2520using%2520Eclipse%2520improved%2520the%2520fluence%2520map%250Areconstruction%2520performance%2520by%2520approximately%25208%2520dB%2520in%2520PSNR%2520compared%2520to%2520a%2520U-Net%250Aarchitecture%2520trained%2520on%2520the%2520original%2520REQUITE%2520dataset.%2520The%2520resulting%2520DVHs%2520are%250Avery%2520close%2520to%2520the%2520one%2520of%2520the%2520input%2520target%2520dose.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03360v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Beam%27s%20Eye%20View%20to%20Fluence%20Maps%203D%20Network%20for%20Ultra%20Fast%20VMAT%0A%20%20Radiotherapy%20Planning&entry.906535625=Simon%20Arberet%20and%20Florin%20C.%20Ghesu%20and%20Riqiang%20Gao%20and%20Martin%20Kraus%20and%20Jonathan%20Sackett%20and%20Esa%20Kuusela%20and%20Ali%20Kamen&entry.1292438233=%20%20Volumetric%20Modulated%20Arc%20Therapy%20%28VMAT%29%20revolutionizes%20cancer%20treatment%20by%0Aprecisely%20delivering%20radiation%20while%20sparing%20healthy%20tissues.%20Fluence%20maps%0Ageneration%2C%20crucial%20in%20VMAT%20planning%2C%20traditionally%20involves%20complex%20and%0Aiterative%2C%20and%20thus%20time%20consuming%20processes.%20These%20fluence%20maps%20are%0Asubsequently%20leveraged%20for%20leaf-sequence.%20The%20deep-learning%20approach%20presented%0Ain%20this%20article%20aims%20to%20expedite%20this%20by%20directly%20predicting%20fluence%20maps%20from%0Apatient%20data.%20We%20developed%20a%203D%20network%20which%20we%20trained%20in%20a%20supervised%20way%0Ausing%20a%20combination%20of%20L1%20and%20L2%20losses%2C%20and%20RT%20plans%20generated%20by%20Eclipse%20and%0Afrom%20the%20REQUITE%20dataset%2C%20taking%20the%20RT%20dose%20map%20as%20input%20and%20the%20fluence%20maps%0Acomputed%20from%20the%20corresponding%20RT%20plans%20as%20target.%20Our%20network%20predicts%0Ajointly%20the%20180%20fluence%20maps%20corresponding%20to%20the%20180%20control%20points%20%28CP%29%20of%0Asingle%20arc%20VMAT%20plans.%20In%20order%20to%20help%20the%20network%2C%20we%20pre-process%20the%20input%0Adose%20by%20computing%20the%20projections%20of%20the%203D%20dose%20map%20to%20the%20beam%27s%20eye%20view%0A%28BEV%29%20of%20the%20180%20CPs%2C%20in%20the%20same%20coordinate%20system%20as%20the%20fluence%20maps.%20We%0Agenerated%20over%202000%20VMAT%20plans%20using%20Eclipse%20to%20scale%20up%20the%20dataset%20size.%0AAdditionally%2C%20we%20evaluated%20various%20network%20architectures%20and%20analyzed%20the%0Aimpact%20of%20increasing%20the%20dataset%20size.%20We%20are%20measuring%20the%20performance%20in%20the%0A2D%20fluence%20maps%20domain%20using%20image%20metrics%20%28PSNR%2C%20SSIM%29%2C%20as%20well%20as%20in%20the%203D%0Adose%20domain%20using%20the%20dose-volume%20histogram%20%28DVH%29%20on%20a%20validation%20dataset.%20The%0Anetwork%20inference%2C%20which%20does%20not%20include%20the%20data%20loading%20and%20processing%2C%20is%0Aless%20than%2020ms.%20Using%20our%20proposed%203D%20network%20architecture%20as%20well%20as%0Aincreasing%20the%20dataset%20size%20using%20Eclipse%20improved%20the%20fluence%20map%0Areconstruction%20performance%20by%20approximately%208%20dB%20in%20PSNR%20compared%20to%20a%20U-Net%0Aarchitecture%20trained%20on%20the%20original%20REQUITE%20dataset.%20The%20resulting%20DVHs%20are%0Avery%20close%20to%20the%20one%20of%20the%20input%20target%20dose.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03360v1&entry.124074799=Read"},
{"title": "An Optimized Toolbox for Advanced Image Processing with Tsetlin Machine\n  Composites", "author": "Ylva Gr\u00f8nnings\u00e6ter and Halvor S. Sm\u00f8rvik and Ole-Christoffer Granmo", "abstract": "  The Tsetlin Machine (TM) has achieved competitive results on several image\nclassification benchmarks, including MNIST, K-MNIST, F-MNIST, and CIFAR-2.\nHowever, color image classification is arguably still in its infancy for TMs,\nwith CIFAR-10 being a focal point for tracking progress. Over the past few\nyears, TM's CIFAR-10 accuracy has increased from around 61% in 2020 to 75.1% in\n2023 with the introduction of Drop Clause. In this paper, we leverage the\nrecently proposed TM Composites architecture and introduce a range of TM\nSpecialists that use various image processing techniques. These include Canny\nedge detection, Histogram of Oriented Gradients, adaptive mean thresholding,\nadaptive Gaussian thresholding, Otsu's thresholding, color thermometers, and\nadaptive color thermometers. In addition, we conduct a rigorous hyperparameter\nsearch, where we uncover optimal hyperparameters for several of the TM\nSpecialists. The result is a toolbox that provides new state-of-the-art results\non CIFAR-10 for TMs with an accuracy of 82.8%. In conclusion, our toolbox of TM\nSpecialists forms a foundation for new TM applications and a landmark for\nfurther research on TM Composites in image analysis.\n", "link": "http://arxiv.org/abs/2406.00704v2", "date": "2025-02-05", "relevancy": 2.1892, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5795}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5274}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Optimized%20Toolbox%20for%20Advanced%20Image%20Processing%20with%20Tsetlin%20Machine%0A%20%20Composites&body=Title%3A%20An%20Optimized%20Toolbox%20for%20Advanced%20Image%20Processing%20with%20Tsetlin%20Machine%0A%20%20Composites%0AAuthor%3A%20Ylva%20Gr%C3%B8nnings%C3%A6ter%20and%20Halvor%20S.%20Sm%C3%B8rvik%20and%20Ole-Christoffer%20Granmo%0AAbstract%3A%20%20%20The%20Tsetlin%20Machine%20%28TM%29%20has%20achieved%20competitive%20results%20on%20several%20image%0Aclassification%20benchmarks%2C%20including%20MNIST%2C%20K-MNIST%2C%20F-MNIST%2C%20and%20CIFAR-2.%0AHowever%2C%20color%20image%20classification%20is%20arguably%20still%20in%20its%20infancy%20for%20TMs%2C%0Awith%20CIFAR-10%20being%20a%20focal%20point%20for%20tracking%20progress.%20Over%20the%20past%20few%0Ayears%2C%20TM%27s%20CIFAR-10%20accuracy%20has%20increased%20from%20around%2061%25%20in%202020%20to%2075.1%25%20in%0A2023%20with%20the%20introduction%20of%20Drop%20Clause.%20In%20this%20paper%2C%20we%20leverage%20the%0Arecently%20proposed%20TM%20Composites%20architecture%20and%20introduce%20a%20range%20of%20TM%0ASpecialists%20that%20use%20various%20image%20processing%20techniques.%20These%20include%20Canny%0Aedge%20detection%2C%20Histogram%20of%20Oriented%20Gradients%2C%20adaptive%20mean%20thresholding%2C%0Aadaptive%20Gaussian%20thresholding%2C%20Otsu%27s%20thresholding%2C%20color%20thermometers%2C%20and%0Aadaptive%20color%20thermometers.%20In%20addition%2C%20we%20conduct%20a%20rigorous%20hyperparameter%0Asearch%2C%20where%20we%20uncover%20optimal%20hyperparameters%20for%20several%20of%20the%20TM%0ASpecialists.%20The%20result%20is%20a%20toolbox%20that%20provides%20new%20state-of-the-art%20results%0Aon%20CIFAR-10%20for%20TMs%20with%20an%20accuracy%20of%2082.8%25.%20In%20conclusion%2C%20our%20toolbox%20of%20TM%0ASpecialists%20forms%20a%20foundation%20for%20new%20TM%20applications%20and%20a%20landmark%20for%0Afurther%20research%20on%20TM%20Composites%20in%20image%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.00704v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Optimized%2520Toolbox%2520for%2520Advanced%2520Image%2520Processing%2520with%2520Tsetlin%2520Machine%250A%2520%2520Composites%26entry.906535625%3DYlva%2520Gr%25C3%25B8nnings%25C3%25A6ter%2520and%2520Halvor%2520S.%2520Sm%25C3%25B8rvik%2520and%2520Ole-Christoffer%2520Granmo%26entry.1292438233%3D%2520%2520The%2520Tsetlin%2520Machine%2520%2528TM%2529%2520has%2520achieved%2520competitive%2520results%2520on%2520several%2520image%250Aclassification%2520benchmarks%252C%2520including%2520MNIST%252C%2520K-MNIST%252C%2520F-MNIST%252C%2520and%2520CIFAR-2.%250AHowever%252C%2520color%2520image%2520classification%2520is%2520arguably%2520still%2520in%2520its%2520infancy%2520for%2520TMs%252C%250Awith%2520CIFAR-10%2520being%2520a%2520focal%2520point%2520for%2520tracking%2520progress.%2520Over%2520the%2520past%2520few%250Ayears%252C%2520TM%2527s%2520CIFAR-10%2520accuracy%2520has%2520increased%2520from%2520around%252061%2525%2520in%25202020%2520to%252075.1%2525%2520in%250A2023%2520with%2520the%2520introduction%2520of%2520Drop%2520Clause.%2520In%2520this%2520paper%252C%2520we%2520leverage%2520the%250Arecently%2520proposed%2520TM%2520Composites%2520architecture%2520and%2520introduce%2520a%2520range%2520of%2520TM%250ASpecialists%2520that%2520use%2520various%2520image%2520processing%2520techniques.%2520These%2520include%2520Canny%250Aedge%2520detection%252C%2520Histogram%2520of%2520Oriented%2520Gradients%252C%2520adaptive%2520mean%2520thresholding%252C%250Aadaptive%2520Gaussian%2520thresholding%252C%2520Otsu%2527s%2520thresholding%252C%2520color%2520thermometers%252C%2520and%250Aadaptive%2520color%2520thermometers.%2520In%2520addition%252C%2520we%2520conduct%2520a%2520rigorous%2520hyperparameter%250Asearch%252C%2520where%2520we%2520uncover%2520optimal%2520hyperparameters%2520for%2520several%2520of%2520the%2520TM%250ASpecialists.%2520The%2520result%2520is%2520a%2520toolbox%2520that%2520provides%2520new%2520state-of-the-art%2520results%250Aon%2520CIFAR-10%2520for%2520TMs%2520with%2520an%2520accuracy%2520of%252082.8%2525.%2520In%2520conclusion%252C%2520our%2520toolbox%2520of%2520TM%250ASpecialists%2520forms%2520a%2520foundation%2520for%2520new%2520TM%2520applications%2520and%2520a%2520landmark%2520for%250Afurther%2520research%2520on%2520TM%2520Composites%2520in%2520image%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.00704v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Optimized%20Toolbox%20for%20Advanced%20Image%20Processing%20with%20Tsetlin%20Machine%0A%20%20Composites&entry.906535625=Ylva%20Gr%C3%B8nnings%C3%A6ter%20and%20Halvor%20S.%20Sm%C3%B8rvik%20and%20Ole-Christoffer%20Granmo&entry.1292438233=%20%20The%20Tsetlin%20Machine%20%28TM%29%20has%20achieved%20competitive%20results%20on%20several%20image%0Aclassification%20benchmarks%2C%20including%20MNIST%2C%20K-MNIST%2C%20F-MNIST%2C%20and%20CIFAR-2.%0AHowever%2C%20color%20image%20classification%20is%20arguably%20still%20in%20its%20infancy%20for%20TMs%2C%0Awith%20CIFAR-10%20being%20a%20focal%20point%20for%20tracking%20progress.%20Over%20the%20past%20few%0Ayears%2C%20TM%27s%20CIFAR-10%20accuracy%20has%20increased%20from%20around%2061%25%20in%202020%20to%2075.1%25%20in%0A2023%20with%20the%20introduction%20of%20Drop%20Clause.%20In%20this%20paper%2C%20we%20leverage%20the%0Arecently%20proposed%20TM%20Composites%20architecture%20and%20introduce%20a%20range%20of%20TM%0ASpecialists%20that%20use%20various%20image%20processing%20techniques.%20These%20include%20Canny%0Aedge%20detection%2C%20Histogram%20of%20Oriented%20Gradients%2C%20adaptive%20mean%20thresholding%2C%0Aadaptive%20Gaussian%20thresholding%2C%20Otsu%27s%20thresholding%2C%20color%20thermometers%2C%20and%0Aadaptive%20color%20thermometers.%20In%20addition%2C%20we%20conduct%20a%20rigorous%20hyperparameter%0Asearch%2C%20where%20we%20uncover%20optimal%20hyperparameters%20for%20several%20of%20the%20TM%0ASpecialists.%20The%20result%20is%20a%20toolbox%20that%20provides%20new%20state-of-the-art%20results%0Aon%20CIFAR-10%20for%20TMs%20with%20an%20accuracy%20of%2082.8%25.%20In%20conclusion%2C%20our%20toolbox%20of%20TM%0ASpecialists%20forms%20a%20foundation%20for%20new%20TM%20applications%20and%20a%20landmark%20for%0Afurther%20research%20on%20TM%20Composites%20in%20image%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.00704v2&entry.124074799=Read"},
{"title": "SimSort: A Powerful Framework for Spike Sorting by Large-Scale\n  Electrophysiology Simulation", "author": "Yimu Zhang and Dongqi Han and Yansen Wang and Yu Gu and Dongsheng Li", "abstract": "  Spike sorting is an essential process in neural recording, which identifies\nand separates electrical signals from individual neurons recorded by electrodes\nin the brain, enabling researchers to study how specific neurons communicate\nand process information. Although there exist a number of spike sorting methods\nwhich have contributed to significant neuroscientific breakthroughs, many are\nheuristically designed, making it challenging to verify their correctness due\nto the difficulty of obtaining ground truth labels from real-world neural\nrecordings. In this work, we explore a data-driven, deep learning-based\napproach. We begin by creating a large-scale dataset through electrophysiology\nsimulations using biologically realistic computational models. We then present\n\\textbf{SimSort}, a pretraining framework for spike sorting. Remarkably, when\ntrained on our simulated dataset, SimSort demonstrates strong zero-shot\ngeneralization to real-world spike sorting tasks, significantly outperforming\nexisting methods. Our findings underscore the potential of data-driven\ntechniques to enhance the reliability and scalability of spike sorting in\nexperimental neuroscience.\n", "link": "http://arxiv.org/abs/2502.03198v1", "date": "2025-02-05", "relevancy": 2.1822, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4519}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4289}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SimSort%3A%20A%20Powerful%20Framework%20for%20Spike%20Sorting%20by%20Large-Scale%0A%20%20Electrophysiology%20Simulation&body=Title%3A%20SimSort%3A%20A%20Powerful%20Framework%20for%20Spike%20Sorting%20by%20Large-Scale%0A%20%20Electrophysiology%20Simulation%0AAuthor%3A%20Yimu%20Zhang%20and%20Dongqi%20Han%20and%20Yansen%20Wang%20and%20Yu%20Gu%20and%20Dongsheng%20Li%0AAbstract%3A%20%20%20Spike%20sorting%20is%20an%20essential%20process%20in%20neural%20recording%2C%20which%20identifies%0Aand%20separates%20electrical%20signals%20from%20individual%20neurons%20recorded%20by%20electrodes%0Ain%20the%20brain%2C%20enabling%20researchers%20to%20study%20how%20specific%20neurons%20communicate%0Aand%20process%20information.%20Although%20there%20exist%20a%20number%20of%20spike%20sorting%20methods%0Awhich%20have%20contributed%20to%20significant%20neuroscientific%20breakthroughs%2C%20many%20are%0Aheuristically%20designed%2C%20making%20it%20challenging%20to%20verify%20their%20correctness%20due%0Ato%20the%20difficulty%20of%20obtaining%20ground%20truth%20labels%20from%20real-world%20neural%0Arecordings.%20In%20this%20work%2C%20we%20explore%20a%20data-driven%2C%20deep%20learning-based%0Aapproach.%20We%20begin%20by%20creating%20a%20large-scale%20dataset%20through%20electrophysiology%0Asimulations%20using%20biologically%20realistic%20computational%20models.%20We%20then%20present%0A%5Ctextbf%7BSimSort%7D%2C%20a%20pretraining%20framework%20for%20spike%20sorting.%20Remarkably%2C%20when%0Atrained%20on%20our%20simulated%20dataset%2C%20SimSort%20demonstrates%20strong%20zero-shot%0Ageneralization%20to%20real-world%20spike%20sorting%20tasks%2C%20significantly%20outperforming%0Aexisting%20methods.%20Our%20findings%20underscore%20the%20potential%20of%20data-driven%0Atechniques%20to%20enhance%20the%20reliability%20and%20scalability%20of%20spike%20sorting%20in%0Aexperimental%20neuroscience.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03198v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimSort%253A%2520A%2520Powerful%2520Framework%2520for%2520Spike%2520Sorting%2520by%2520Large-Scale%250A%2520%2520Electrophysiology%2520Simulation%26entry.906535625%3DYimu%2520Zhang%2520and%2520Dongqi%2520Han%2520and%2520Yansen%2520Wang%2520and%2520Yu%2520Gu%2520and%2520Dongsheng%2520Li%26entry.1292438233%3D%2520%2520Spike%2520sorting%2520is%2520an%2520essential%2520process%2520in%2520neural%2520recording%252C%2520which%2520identifies%250Aand%2520separates%2520electrical%2520signals%2520from%2520individual%2520neurons%2520recorded%2520by%2520electrodes%250Ain%2520the%2520brain%252C%2520enabling%2520researchers%2520to%2520study%2520how%2520specific%2520neurons%2520communicate%250Aand%2520process%2520information.%2520Although%2520there%2520exist%2520a%2520number%2520of%2520spike%2520sorting%2520methods%250Awhich%2520have%2520contributed%2520to%2520significant%2520neuroscientific%2520breakthroughs%252C%2520many%2520are%250Aheuristically%2520designed%252C%2520making%2520it%2520challenging%2520to%2520verify%2520their%2520correctness%2520due%250Ato%2520the%2520difficulty%2520of%2520obtaining%2520ground%2520truth%2520labels%2520from%2520real-world%2520neural%250Arecordings.%2520In%2520this%2520work%252C%2520we%2520explore%2520a%2520data-driven%252C%2520deep%2520learning-based%250Aapproach.%2520We%2520begin%2520by%2520creating%2520a%2520large-scale%2520dataset%2520through%2520electrophysiology%250Asimulations%2520using%2520biologically%2520realistic%2520computational%2520models.%2520We%2520then%2520present%250A%255Ctextbf%257BSimSort%257D%252C%2520a%2520pretraining%2520framework%2520for%2520spike%2520sorting.%2520Remarkably%252C%2520when%250Atrained%2520on%2520our%2520simulated%2520dataset%252C%2520SimSort%2520demonstrates%2520strong%2520zero-shot%250Ageneralization%2520to%2520real-world%2520spike%2520sorting%2520tasks%252C%2520significantly%2520outperforming%250Aexisting%2520methods.%2520Our%2520findings%2520underscore%2520the%2520potential%2520of%2520data-driven%250Atechniques%2520to%2520enhance%2520the%2520reliability%2520and%2520scalability%2520of%2520spike%2520sorting%2520in%250Aexperimental%2520neuroscience.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03198v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SimSort%3A%20A%20Powerful%20Framework%20for%20Spike%20Sorting%20by%20Large-Scale%0A%20%20Electrophysiology%20Simulation&entry.906535625=Yimu%20Zhang%20and%20Dongqi%20Han%20and%20Yansen%20Wang%20and%20Yu%20Gu%20and%20Dongsheng%20Li&entry.1292438233=%20%20Spike%20sorting%20is%20an%20essential%20process%20in%20neural%20recording%2C%20which%20identifies%0Aand%20separates%20electrical%20signals%20from%20individual%20neurons%20recorded%20by%20electrodes%0Ain%20the%20brain%2C%20enabling%20researchers%20to%20study%20how%20specific%20neurons%20communicate%0Aand%20process%20information.%20Although%20there%20exist%20a%20number%20of%20spike%20sorting%20methods%0Awhich%20have%20contributed%20to%20significant%20neuroscientific%20breakthroughs%2C%20many%20are%0Aheuristically%20designed%2C%20making%20it%20challenging%20to%20verify%20their%20correctness%20due%0Ato%20the%20difficulty%20of%20obtaining%20ground%20truth%20labels%20from%20real-world%20neural%0Arecordings.%20In%20this%20work%2C%20we%20explore%20a%20data-driven%2C%20deep%20learning-based%0Aapproach.%20We%20begin%20by%20creating%20a%20large-scale%20dataset%20through%20electrophysiology%0Asimulations%20using%20biologically%20realistic%20computational%20models.%20We%20then%20present%0A%5Ctextbf%7BSimSort%7D%2C%20a%20pretraining%20framework%20for%20spike%20sorting.%20Remarkably%2C%20when%0Atrained%20on%20our%20simulated%20dataset%2C%20SimSort%20demonstrates%20strong%20zero-shot%0Ageneralization%20to%20real-world%20spike%20sorting%20tasks%2C%20significantly%20outperforming%0Aexisting%20methods.%20Our%20findings%20underscore%20the%20potential%20of%20data-driven%0Atechniques%20to%20enhance%20the%20reliability%20and%20scalability%20of%20spike%20sorting%20in%0Aexperimental%20neuroscience.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03198v1&entry.124074799=Read"},
{"title": "Learning from Active Human Involvement through Proxy Value Propagation", "author": "Zhenghao Peng and Wenjie Mo and Chenda Duan and Quanyi Li and Bolei Zhou", "abstract": "  Learning from active human involvement enables the human subject to actively\nintervene and demonstrate to the AI agent during training. The interaction and\ncorrective feedback from human brings safety and AI alignment to the learning\nprocess. In this work, we propose a new reward-free active human involvement\nmethod called Proxy Value Propagation for policy optimization. Our key insight\nis that a proxy value function can be designed to express human intents,\nwherein state-action pairs in the human demonstration are labeled with high\nvalues, while those agents' actions that are intervened receive low values.\nThrough the TD-learning framework, labeled values of demonstrated state-action\npairs are further propagated to other unlabeled data generated from agents'\nexploration. The proxy value function thus induces a policy that faithfully\nemulates human behaviors. Human-in-the-loop experiments show the generality and\nefficiency of our method. With minimal modification to existing reinforcement\nlearning algorithms, our method can learn to solve continuous and discrete\ncontrol tasks with various human control devices, including the challenging\ntask of driving in Grand Theft Auto V. Demo video and code are available at:\nhttps://metadriverse.github.io/pvp\n", "link": "http://arxiv.org/abs/2502.03369v1", "date": "2025-02-05", "relevancy": 2.1764, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5665}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5618}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5146}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Active%20Human%20Involvement%20through%20Proxy%20Value%20Propagation&body=Title%3A%20Learning%20from%20Active%20Human%20Involvement%20through%20Proxy%20Value%20Propagation%0AAuthor%3A%20Zhenghao%20Peng%20and%20Wenjie%20Mo%20and%20Chenda%20Duan%20and%20Quanyi%20Li%20and%20Bolei%20Zhou%0AAbstract%3A%20%20%20Learning%20from%20active%20human%20involvement%20enables%20the%20human%20subject%20to%20actively%0Aintervene%20and%20demonstrate%20to%20the%20AI%20agent%20during%20training.%20The%20interaction%20and%0Acorrective%20feedback%20from%20human%20brings%20safety%20and%20AI%20alignment%20to%20the%20learning%0Aprocess.%20In%20this%20work%2C%20we%20propose%20a%20new%20reward-free%20active%20human%20involvement%0Amethod%20called%20Proxy%20Value%20Propagation%20for%20policy%20optimization.%20Our%20key%20insight%0Ais%20that%20a%20proxy%20value%20function%20can%20be%20designed%20to%20express%20human%20intents%2C%0Awherein%20state-action%20pairs%20in%20the%20human%20demonstration%20are%20labeled%20with%20high%0Avalues%2C%20while%20those%20agents%27%20actions%20that%20are%20intervened%20receive%20low%20values.%0AThrough%20the%20TD-learning%20framework%2C%20labeled%20values%20of%20demonstrated%20state-action%0Apairs%20are%20further%20propagated%20to%20other%20unlabeled%20data%20generated%20from%20agents%27%0Aexploration.%20The%20proxy%20value%20function%20thus%20induces%20a%20policy%20that%20faithfully%0Aemulates%20human%20behaviors.%20Human-in-the-loop%20experiments%20show%20the%20generality%20and%0Aefficiency%20of%20our%20method.%20With%20minimal%20modification%20to%20existing%20reinforcement%0Alearning%20algorithms%2C%20our%20method%20can%20learn%20to%20solve%20continuous%20and%20discrete%0Acontrol%20tasks%20with%20various%20human%20control%20devices%2C%20including%20the%20challenging%0Atask%20of%20driving%20in%20Grand%20Theft%20Auto%20V.%20Demo%20video%20and%20code%20are%20available%20at%3A%0Ahttps%3A//metadriverse.github.io/pvp%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03369v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Active%2520Human%2520Involvement%2520through%2520Proxy%2520Value%2520Propagation%26entry.906535625%3DZhenghao%2520Peng%2520and%2520Wenjie%2520Mo%2520and%2520Chenda%2520Duan%2520and%2520Quanyi%2520Li%2520and%2520Bolei%2520Zhou%26entry.1292438233%3D%2520%2520Learning%2520from%2520active%2520human%2520involvement%2520enables%2520the%2520human%2520subject%2520to%2520actively%250Aintervene%2520and%2520demonstrate%2520to%2520the%2520AI%2520agent%2520during%2520training.%2520The%2520interaction%2520and%250Acorrective%2520feedback%2520from%2520human%2520brings%2520safety%2520and%2520AI%2520alignment%2520to%2520the%2520learning%250Aprocess.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520new%2520reward-free%2520active%2520human%2520involvement%250Amethod%2520called%2520Proxy%2520Value%2520Propagation%2520for%2520policy%2520optimization.%2520Our%2520key%2520insight%250Ais%2520that%2520a%2520proxy%2520value%2520function%2520can%2520be%2520designed%2520to%2520express%2520human%2520intents%252C%250Awherein%2520state-action%2520pairs%2520in%2520the%2520human%2520demonstration%2520are%2520labeled%2520with%2520high%250Avalues%252C%2520while%2520those%2520agents%2527%2520actions%2520that%2520are%2520intervened%2520receive%2520low%2520values.%250AThrough%2520the%2520TD-learning%2520framework%252C%2520labeled%2520values%2520of%2520demonstrated%2520state-action%250Apairs%2520are%2520further%2520propagated%2520to%2520other%2520unlabeled%2520data%2520generated%2520from%2520agents%2527%250Aexploration.%2520The%2520proxy%2520value%2520function%2520thus%2520induces%2520a%2520policy%2520that%2520faithfully%250Aemulates%2520human%2520behaviors.%2520Human-in-the-loop%2520experiments%2520show%2520the%2520generality%2520and%250Aefficiency%2520of%2520our%2520method.%2520With%2520minimal%2520modification%2520to%2520existing%2520reinforcement%250Alearning%2520algorithms%252C%2520our%2520method%2520can%2520learn%2520to%2520solve%2520continuous%2520and%2520discrete%250Acontrol%2520tasks%2520with%2520various%2520human%2520control%2520devices%252C%2520including%2520the%2520challenging%250Atask%2520of%2520driving%2520in%2520Grand%2520Theft%2520Auto%2520V.%2520Demo%2520video%2520and%2520code%2520are%2520available%2520at%253A%250Ahttps%253A//metadriverse.github.io/pvp%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03369v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Active%20Human%20Involvement%20through%20Proxy%20Value%20Propagation&entry.906535625=Zhenghao%20Peng%20and%20Wenjie%20Mo%20and%20Chenda%20Duan%20and%20Quanyi%20Li%20and%20Bolei%20Zhou&entry.1292438233=%20%20Learning%20from%20active%20human%20involvement%20enables%20the%20human%20subject%20to%20actively%0Aintervene%20and%20demonstrate%20to%20the%20AI%20agent%20during%20training.%20The%20interaction%20and%0Acorrective%20feedback%20from%20human%20brings%20safety%20and%20AI%20alignment%20to%20the%20learning%0Aprocess.%20In%20this%20work%2C%20we%20propose%20a%20new%20reward-free%20active%20human%20involvement%0Amethod%20called%20Proxy%20Value%20Propagation%20for%20policy%20optimization.%20Our%20key%20insight%0Ais%20that%20a%20proxy%20value%20function%20can%20be%20designed%20to%20express%20human%20intents%2C%0Awherein%20state-action%20pairs%20in%20the%20human%20demonstration%20are%20labeled%20with%20high%0Avalues%2C%20while%20those%20agents%27%20actions%20that%20are%20intervened%20receive%20low%20values.%0AThrough%20the%20TD-learning%20framework%2C%20labeled%20values%20of%20demonstrated%20state-action%0Apairs%20are%20further%20propagated%20to%20other%20unlabeled%20data%20generated%20from%20agents%27%0Aexploration.%20The%20proxy%20value%20function%20thus%20induces%20a%20policy%20that%20faithfully%0Aemulates%20human%20behaviors.%20Human-in-the-loop%20experiments%20show%20the%20generality%20and%0Aefficiency%20of%20our%20method.%20With%20minimal%20modification%20to%20existing%20reinforcement%0Alearning%20algorithms%2C%20our%20method%20can%20learn%20to%20solve%20continuous%20and%20discrete%0Acontrol%20tasks%20with%20various%20human%20control%20devices%2C%20including%20the%20challenging%0Atask%20of%20driving%20in%20Grand%20Theft%20Auto%20V.%20Demo%20video%20and%20code%20are%20available%20at%3A%0Ahttps%3A//metadriverse.github.io/pvp%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03369v1&entry.124074799=Read"},
{"title": "A Temporal Convolutional Network-Based Approach and a Benchmark Dataset\n  for Colonoscopy Video Temporal Segmentation", "author": "Carlo Biffi and Giorgio Roffo and Pietro Salvagnini and Andrea Cherubini", "abstract": "  Following recent advancements in computer-aided detection and diagnosis\nsystems for colonoscopy, the automated reporting of colonoscopy procedures is\nset to further revolutionize clinical practice. A crucial yet underexplored\naspect in the development of these systems is the creation of computer vision\nmodels capable of autonomously segmenting full-procedure colonoscopy videos\ninto anatomical sections and procedural phases. In this work, we aim to create\nthe first open-access dataset for this task and propose a state-of-the-art\napproach, benchmarked against competitive models. We annotated the publicly\navailable REAL-Colon dataset, consisting of 2.7 million frames from 60 complete\ncolonoscopy videos, with frame-level labels for anatomical locations and\ncolonoscopy phases across nine categories. We then present ColonTCN, a\nlearning-based architecture that employs custom temporal convolutional blocks\ndesigned to efficiently capture long temporal dependencies for the temporal\nsegmentation of colonoscopy videos. We also propose a dual k-fold\ncross-validation evaluation protocol for this benchmark, which includes model\nassessment on unseen, multi-center data.ColonTCN achieves state-of-the-art\nperformance in classification accuracy while maintaining a low parameter count\nwhen evaluated using the two proposed k-fold cross-validation settings,\noutperforming competitive models. We report ablation studies to provide\ninsights into the challenges of this task and highlight the benefits of the\ncustom temporal convolutional blocks, which enhance learning and improve model\nefficiency. We believe that the proposed open-access benchmark and the ColonTCN\napproach represent a significant advancement in the temporal segmentation of\ncolonoscopy procedures, fostering further open-access research to address this\nclinical need.\n", "link": "http://arxiv.org/abs/2502.03430v1", "date": "2025-02-05", "relevancy": 2.1752, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5551}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5438}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5393}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Temporal%20Convolutional%20Network-Based%20Approach%20and%20a%20Benchmark%20Dataset%0A%20%20for%20Colonoscopy%20Video%20Temporal%20Segmentation&body=Title%3A%20A%20Temporal%20Convolutional%20Network-Based%20Approach%20and%20a%20Benchmark%20Dataset%0A%20%20for%20Colonoscopy%20Video%20Temporal%20Segmentation%0AAuthor%3A%20Carlo%20Biffi%20and%20Giorgio%20Roffo%20and%20Pietro%20Salvagnini%20and%20Andrea%20Cherubini%0AAbstract%3A%20%20%20Following%20recent%20advancements%20in%20computer-aided%20detection%20and%20diagnosis%0Asystems%20for%20colonoscopy%2C%20the%20automated%20reporting%20of%20colonoscopy%20procedures%20is%0Aset%20to%20further%20revolutionize%20clinical%20practice.%20A%20crucial%20yet%20underexplored%0Aaspect%20in%20the%20development%20of%20these%20systems%20is%20the%20creation%20of%20computer%20vision%0Amodels%20capable%20of%20autonomously%20segmenting%20full-procedure%20colonoscopy%20videos%0Ainto%20anatomical%20sections%20and%20procedural%20phases.%20In%20this%20work%2C%20we%20aim%20to%20create%0Athe%20first%20open-access%20dataset%20for%20this%20task%20and%20propose%20a%20state-of-the-art%0Aapproach%2C%20benchmarked%20against%20competitive%20models.%20We%20annotated%20the%20publicly%0Aavailable%20REAL-Colon%20dataset%2C%20consisting%20of%202.7%20million%20frames%20from%2060%20complete%0Acolonoscopy%20videos%2C%20with%20frame-level%20labels%20for%20anatomical%20locations%20and%0Acolonoscopy%20phases%20across%20nine%20categories.%20We%20then%20present%20ColonTCN%2C%20a%0Alearning-based%20architecture%20that%20employs%20custom%20temporal%20convolutional%20blocks%0Adesigned%20to%20efficiently%20capture%20long%20temporal%20dependencies%20for%20the%20temporal%0Asegmentation%20of%20colonoscopy%20videos.%20We%20also%20propose%20a%20dual%20k-fold%0Across-validation%20evaluation%20protocol%20for%20this%20benchmark%2C%20which%20includes%20model%0Aassessment%20on%20unseen%2C%20multi-center%20data.ColonTCN%20achieves%20state-of-the-art%0Aperformance%20in%20classification%20accuracy%20while%20maintaining%20a%20low%20parameter%20count%0Awhen%20evaluated%20using%20the%20two%20proposed%20k-fold%20cross-validation%20settings%2C%0Aoutperforming%20competitive%20models.%20We%20report%20ablation%20studies%20to%20provide%0Ainsights%20into%20the%20challenges%20of%20this%20task%20and%20highlight%20the%20benefits%20of%20the%0Acustom%20temporal%20convolutional%20blocks%2C%20which%20enhance%20learning%20and%20improve%20model%0Aefficiency.%20We%20believe%20that%20the%20proposed%20open-access%20benchmark%20and%20the%20ColonTCN%0Aapproach%20represent%20a%20significant%20advancement%20in%20the%20temporal%20segmentation%20of%0Acolonoscopy%20procedures%2C%20fostering%20further%20open-access%20research%20to%20address%20this%0Aclinical%20need.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03430v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Temporal%2520Convolutional%2520Network-Based%2520Approach%2520and%2520a%2520Benchmark%2520Dataset%250A%2520%2520for%2520Colonoscopy%2520Video%2520Temporal%2520Segmentation%26entry.906535625%3DCarlo%2520Biffi%2520and%2520Giorgio%2520Roffo%2520and%2520Pietro%2520Salvagnini%2520and%2520Andrea%2520Cherubini%26entry.1292438233%3D%2520%2520Following%2520recent%2520advancements%2520in%2520computer-aided%2520detection%2520and%2520diagnosis%250Asystems%2520for%2520colonoscopy%252C%2520the%2520automated%2520reporting%2520of%2520colonoscopy%2520procedures%2520is%250Aset%2520to%2520further%2520revolutionize%2520clinical%2520practice.%2520A%2520crucial%2520yet%2520underexplored%250Aaspect%2520in%2520the%2520development%2520of%2520these%2520systems%2520is%2520the%2520creation%2520of%2520computer%2520vision%250Amodels%2520capable%2520of%2520autonomously%2520segmenting%2520full-procedure%2520colonoscopy%2520videos%250Ainto%2520anatomical%2520sections%2520and%2520procedural%2520phases.%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520create%250Athe%2520first%2520open-access%2520dataset%2520for%2520this%2520task%2520and%2520propose%2520a%2520state-of-the-art%250Aapproach%252C%2520benchmarked%2520against%2520competitive%2520models.%2520We%2520annotated%2520the%2520publicly%250Aavailable%2520REAL-Colon%2520dataset%252C%2520consisting%2520of%25202.7%2520million%2520frames%2520from%252060%2520complete%250Acolonoscopy%2520videos%252C%2520with%2520frame-level%2520labels%2520for%2520anatomical%2520locations%2520and%250Acolonoscopy%2520phases%2520across%2520nine%2520categories.%2520We%2520then%2520present%2520ColonTCN%252C%2520a%250Alearning-based%2520architecture%2520that%2520employs%2520custom%2520temporal%2520convolutional%2520blocks%250Adesigned%2520to%2520efficiently%2520capture%2520long%2520temporal%2520dependencies%2520for%2520the%2520temporal%250Asegmentation%2520of%2520colonoscopy%2520videos.%2520We%2520also%2520propose%2520a%2520dual%2520k-fold%250Across-validation%2520evaluation%2520protocol%2520for%2520this%2520benchmark%252C%2520which%2520includes%2520model%250Aassessment%2520on%2520unseen%252C%2520multi-center%2520data.ColonTCN%2520achieves%2520state-of-the-art%250Aperformance%2520in%2520classification%2520accuracy%2520while%2520maintaining%2520a%2520low%2520parameter%2520count%250Awhen%2520evaluated%2520using%2520the%2520two%2520proposed%2520k-fold%2520cross-validation%2520settings%252C%250Aoutperforming%2520competitive%2520models.%2520We%2520report%2520ablation%2520studies%2520to%2520provide%250Ainsights%2520into%2520the%2520challenges%2520of%2520this%2520task%2520and%2520highlight%2520the%2520benefits%2520of%2520the%250Acustom%2520temporal%2520convolutional%2520blocks%252C%2520which%2520enhance%2520learning%2520and%2520improve%2520model%250Aefficiency.%2520We%2520believe%2520that%2520the%2520proposed%2520open-access%2520benchmark%2520and%2520the%2520ColonTCN%250Aapproach%2520represent%2520a%2520significant%2520advancement%2520in%2520the%2520temporal%2520segmentation%2520of%250Acolonoscopy%2520procedures%252C%2520fostering%2520further%2520open-access%2520research%2520to%2520address%2520this%250Aclinical%2520need.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03430v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Temporal%20Convolutional%20Network-Based%20Approach%20and%20a%20Benchmark%20Dataset%0A%20%20for%20Colonoscopy%20Video%20Temporal%20Segmentation&entry.906535625=Carlo%20Biffi%20and%20Giorgio%20Roffo%20and%20Pietro%20Salvagnini%20and%20Andrea%20Cherubini&entry.1292438233=%20%20Following%20recent%20advancements%20in%20computer-aided%20detection%20and%20diagnosis%0Asystems%20for%20colonoscopy%2C%20the%20automated%20reporting%20of%20colonoscopy%20procedures%20is%0Aset%20to%20further%20revolutionize%20clinical%20practice.%20A%20crucial%20yet%20underexplored%0Aaspect%20in%20the%20development%20of%20these%20systems%20is%20the%20creation%20of%20computer%20vision%0Amodels%20capable%20of%20autonomously%20segmenting%20full-procedure%20colonoscopy%20videos%0Ainto%20anatomical%20sections%20and%20procedural%20phases.%20In%20this%20work%2C%20we%20aim%20to%20create%0Athe%20first%20open-access%20dataset%20for%20this%20task%20and%20propose%20a%20state-of-the-art%0Aapproach%2C%20benchmarked%20against%20competitive%20models.%20We%20annotated%20the%20publicly%0Aavailable%20REAL-Colon%20dataset%2C%20consisting%20of%202.7%20million%20frames%20from%2060%20complete%0Acolonoscopy%20videos%2C%20with%20frame-level%20labels%20for%20anatomical%20locations%20and%0Acolonoscopy%20phases%20across%20nine%20categories.%20We%20then%20present%20ColonTCN%2C%20a%0Alearning-based%20architecture%20that%20employs%20custom%20temporal%20convolutional%20blocks%0Adesigned%20to%20efficiently%20capture%20long%20temporal%20dependencies%20for%20the%20temporal%0Asegmentation%20of%20colonoscopy%20videos.%20We%20also%20propose%20a%20dual%20k-fold%0Across-validation%20evaluation%20protocol%20for%20this%20benchmark%2C%20which%20includes%20model%0Aassessment%20on%20unseen%2C%20multi-center%20data.ColonTCN%20achieves%20state-of-the-art%0Aperformance%20in%20classification%20accuracy%20while%20maintaining%20a%20low%20parameter%20count%0Awhen%20evaluated%20using%20the%20two%20proposed%20k-fold%20cross-validation%20settings%2C%0Aoutperforming%20competitive%20models.%20We%20report%20ablation%20studies%20to%20provide%0Ainsights%20into%20the%20challenges%20of%20this%20task%20and%20highlight%20the%20benefits%20of%20the%0Acustom%20temporal%20convolutional%20blocks%2C%20which%20enhance%20learning%20and%20improve%20model%0Aefficiency.%20We%20believe%20that%20the%20proposed%20open-access%20benchmark%20and%20the%20ColonTCN%0Aapproach%20represent%20a%20significant%20advancement%20in%20the%20temporal%20segmentation%20of%0Acolonoscopy%20procedures%2C%20fostering%20further%20open-access%20research%20to%20address%20this%0Aclinical%20need.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03430v1&entry.124074799=Read"},
{"title": "Taking a Big Step: Large Learning Rates in Denoising Score Matching\n  Prevent Memorization", "author": "Yu-Han Wu and Pierre Marion and G\u00e9rard Biau and Claire Boyer", "abstract": "  Denoising score matching plays a pivotal role in the performance of\ndiffusion-based generative models. However, the empirical optimal score--the\nexact solution to the denoising score matching--leads to memorization, where\ngenerated samples replicate the training data. Yet, in practice, only a\nmoderate degree of memorization is observed, even without explicit\nregularization. In this paper, we investigate this phenomenon by uncovering an\nimplicit regularization mechanism driven by large learning rates. Specifically,\nwe show that in the small-noise regime, the empirical optimal score exhibits\nhigh irregularity. We then prove that, when trained by stochastic gradient\ndescent with a large enough learning rate, neural networks cannot stably\nconverge to a local minimum with arbitrarily small excess risk. Consequently,\nthe learned score cannot be arbitrarily close to the empirical optimal score,\nthereby mitigating memorization. To make the analysis tractable, we consider\none-dimensional data and two-layer neural networks. Experiments validate the\ncrucial role of the learning rate in preventing memorization, even beyond the\none-dimensional setting.\n", "link": "http://arxiv.org/abs/2502.03435v1", "date": "2025-02-05", "relevancy": 2.1644, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5569}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5343}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Taking%20a%20Big%20Step%3A%20Large%20Learning%20Rates%20in%20Denoising%20Score%20Matching%0A%20%20Prevent%20Memorization&body=Title%3A%20Taking%20a%20Big%20Step%3A%20Large%20Learning%20Rates%20in%20Denoising%20Score%20Matching%0A%20%20Prevent%20Memorization%0AAuthor%3A%20Yu-Han%20Wu%20and%20Pierre%20Marion%20and%20G%C3%A9rard%20Biau%20and%20Claire%20Boyer%0AAbstract%3A%20%20%20Denoising%20score%20matching%20plays%20a%20pivotal%20role%20in%20the%20performance%20of%0Adiffusion-based%20generative%20models.%20However%2C%20the%20empirical%20optimal%20score--the%0Aexact%20solution%20to%20the%20denoising%20score%20matching--leads%20to%20memorization%2C%20where%0Agenerated%20samples%20replicate%20the%20training%20data.%20Yet%2C%20in%20practice%2C%20only%20a%0Amoderate%20degree%20of%20memorization%20is%20observed%2C%20even%20without%20explicit%0Aregularization.%20In%20this%20paper%2C%20we%20investigate%20this%20phenomenon%20by%20uncovering%20an%0Aimplicit%20regularization%20mechanism%20driven%20by%20large%20learning%20rates.%20Specifically%2C%0Awe%20show%20that%20in%20the%20small-noise%20regime%2C%20the%20empirical%20optimal%20score%20exhibits%0Ahigh%20irregularity.%20We%20then%20prove%20that%2C%20when%20trained%20by%20stochastic%20gradient%0Adescent%20with%20a%20large%20enough%20learning%20rate%2C%20neural%20networks%20cannot%20stably%0Aconverge%20to%20a%20local%20minimum%20with%20arbitrarily%20small%20excess%20risk.%20Consequently%2C%0Athe%20learned%20score%20cannot%20be%20arbitrarily%20close%20to%20the%20empirical%20optimal%20score%2C%0Athereby%20mitigating%20memorization.%20To%20make%20the%20analysis%20tractable%2C%20we%20consider%0Aone-dimensional%20data%20and%20two-layer%20neural%20networks.%20Experiments%20validate%20the%0Acrucial%20role%20of%20the%20learning%20rate%20in%20preventing%20memorization%2C%20even%20beyond%20the%0Aone-dimensional%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03435v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaking%2520a%2520Big%2520Step%253A%2520Large%2520Learning%2520Rates%2520in%2520Denoising%2520Score%2520Matching%250A%2520%2520Prevent%2520Memorization%26entry.906535625%3DYu-Han%2520Wu%2520and%2520Pierre%2520Marion%2520and%2520G%25C3%25A9rard%2520Biau%2520and%2520Claire%2520Boyer%26entry.1292438233%3D%2520%2520Denoising%2520score%2520matching%2520plays%2520a%2520pivotal%2520role%2520in%2520the%2520performance%2520of%250Adiffusion-based%2520generative%2520models.%2520However%252C%2520the%2520empirical%2520optimal%2520score--the%250Aexact%2520solution%2520to%2520the%2520denoising%2520score%2520matching--leads%2520to%2520memorization%252C%2520where%250Agenerated%2520samples%2520replicate%2520the%2520training%2520data.%2520Yet%252C%2520in%2520practice%252C%2520only%2520a%250Amoderate%2520degree%2520of%2520memorization%2520is%2520observed%252C%2520even%2520without%2520explicit%250Aregularization.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520this%2520phenomenon%2520by%2520uncovering%2520an%250Aimplicit%2520regularization%2520mechanism%2520driven%2520by%2520large%2520learning%2520rates.%2520Specifically%252C%250Awe%2520show%2520that%2520in%2520the%2520small-noise%2520regime%252C%2520the%2520empirical%2520optimal%2520score%2520exhibits%250Ahigh%2520irregularity.%2520We%2520then%2520prove%2520that%252C%2520when%2520trained%2520by%2520stochastic%2520gradient%250Adescent%2520with%2520a%2520large%2520enough%2520learning%2520rate%252C%2520neural%2520networks%2520cannot%2520stably%250Aconverge%2520to%2520a%2520local%2520minimum%2520with%2520arbitrarily%2520small%2520excess%2520risk.%2520Consequently%252C%250Athe%2520learned%2520score%2520cannot%2520be%2520arbitrarily%2520close%2520to%2520the%2520empirical%2520optimal%2520score%252C%250Athereby%2520mitigating%2520memorization.%2520To%2520make%2520the%2520analysis%2520tractable%252C%2520we%2520consider%250Aone-dimensional%2520data%2520and%2520two-layer%2520neural%2520networks.%2520Experiments%2520validate%2520the%250Acrucial%2520role%2520of%2520the%2520learning%2520rate%2520in%2520preventing%2520memorization%252C%2520even%2520beyond%2520the%250Aone-dimensional%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03435v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Taking%20a%20Big%20Step%3A%20Large%20Learning%20Rates%20in%20Denoising%20Score%20Matching%0A%20%20Prevent%20Memorization&entry.906535625=Yu-Han%20Wu%20and%20Pierre%20Marion%20and%20G%C3%A9rard%20Biau%20and%20Claire%20Boyer&entry.1292438233=%20%20Denoising%20score%20matching%20plays%20a%20pivotal%20role%20in%20the%20performance%20of%0Adiffusion-based%20generative%20models.%20However%2C%20the%20empirical%20optimal%20score--the%0Aexact%20solution%20to%20the%20denoising%20score%20matching--leads%20to%20memorization%2C%20where%0Agenerated%20samples%20replicate%20the%20training%20data.%20Yet%2C%20in%20practice%2C%20only%20a%0Amoderate%20degree%20of%20memorization%20is%20observed%2C%20even%20without%20explicit%0Aregularization.%20In%20this%20paper%2C%20we%20investigate%20this%20phenomenon%20by%20uncovering%20an%0Aimplicit%20regularization%20mechanism%20driven%20by%20large%20learning%20rates.%20Specifically%2C%0Awe%20show%20that%20in%20the%20small-noise%20regime%2C%20the%20empirical%20optimal%20score%20exhibits%0Ahigh%20irregularity.%20We%20then%20prove%20that%2C%20when%20trained%20by%20stochastic%20gradient%0Adescent%20with%20a%20large%20enough%20learning%20rate%2C%20neural%20networks%20cannot%20stably%0Aconverge%20to%20a%20local%20minimum%20with%20arbitrarily%20small%20excess%20risk.%20Consequently%2C%0Athe%20learned%20score%20cannot%20be%20arbitrarily%20close%20to%20the%20empirical%20optimal%20score%2C%0Athereby%20mitigating%20memorization.%20To%20make%20the%20analysis%20tractable%2C%20we%20consider%0Aone-dimensional%20data%20and%20two-layer%20neural%20networks.%20Experiments%20validate%20the%0Acrucial%20role%20of%20the%20learning%20rate%20in%20preventing%20memorization%2C%20even%20beyond%20the%0Aone-dimensional%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03435v1&entry.124074799=Read"},
{"title": "Can Text-to-Image Generative Models Accurately Depict Age? A Comparative\n  Study on Synthetic Portrait Generation and Age Estimation", "author": "Alexey A. Novikov and Miroslav Vranka and Fran\u00e7ois David and Artem Voronin", "abstract": "  Text-to-image generative models have shown remarkable progress in producing\ndiverse and photorealistic outputs. In this paper, we present a comprehensive\nanalysis of their effectiveness in creating synthetic portraits that accurately\nrepresent various demographic attributes, with a special focus on age,\nnationality, and gender. Our evaluation employs prompts specifying detailed\nprofiles (e.g., Photorealistic selfie photo of a 32-year-old Canadian male),\ncovering a broad spectrum of 212 nationalities, 30 distinct ages from 10 to 78,\nand balanced gender representation. We compare the generated images against\nground truth age estimates from two established age estimation models to assess\nhow faithfully age is depicted. Our findings reveal that although text-to-image\nmodels can consistently generate faces reflecting different identities, the\naccuracy with which they capture specific ages and do so across diverse\ndemographic backgrounds remains highly variable. These results suggest that\ncurrent synthetic data may be insufficiently reliable for high-stakes\nage-related tasks requiring robust precision, unless practitioners are prepared\nto invest in significant filtering and curation. Nevertheless, they may still\nbe useful in less sensitive or exploratory applications, where absolute age\nprecision is not critical.\n", "link": "http://arxiv.org/abs/2502.03420v1", "date": "2025-02-05", "relevancy": 2.1618, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.548}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5416}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Text-to-Image%20Generative%20Models%20Accurately%20Depict%20Age%3F%20A%20Comparative%0A%20%20Study%20on%20Synthetic%20Portrait%20Generation%20and%20Age%20Estimation&body=Title%3A%20Can%20Text-to-Image%20Generative%20Models%20Accurately%20Depict%20Age%3F%20A%20Comparative%0A%20%20Study%20on%20Synthetic%20Portrait%20Generation%20and%20Age%20Estimation%0AAuthor%3A%20Alexey%20A.%20Novikov%20and%20Miroslav%20Vranka%20and%20Fran%C3%A7ois%20David%20and%20Artem%20Voronin%0AAbstract%3A%20%20%20Text-to-image%20generative%20models%20have%20shown%20remarkable%20progress%20in%20producing%0Adiverse%20and%20photorealistic%20outputs.%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%0Aanalysis%20of%20their%20effectiveness%20in%20creating%20synthetic%20portraits%20that%20accurately%0Arepresent%20various%20demographic%20attributes%2C%20with%20a%20special%20focus%20on%20age%2C%0Anationality%2C%20and%20gender.%20Our%20evaluation%20employs%20prompts%20specifying%20detailed%0Aprofiles%20%28e.g.%2C%20Photorealistic%20selfie%20photo%20of%20a%2032-year-old%20Canadian%20male%29%2C%0Acovering%20a%20broad%20spectrum%20of%20212%20nationalities%2C%2030%20distinct%20ages%20from%2010%20to%2078%2C%0Aand%20balanced%20gender%20representation.%20We%20compare%20the%20generated%20images%20against%0Aground%20truth%20age%20estimates%20from%20two%20established%20age%20estimation%20models%20to%20assess%0Ahow%20faithfully%20age%20is%20depicted.%20Our%20findings%20reveal%20that%20although%20text-to-image%0Amodels%20can%20consistently%20generate%20faces%20reflecting%20different%20identities%2C%20the%0Aaccuracy%20with%20which%20they%20capture%20specific%20ages%20and%20do%20so%20across%20diverse%0Ademographic%20backgrounds%20remains%20highly%20variable.%20These%20results%20suggest%20that%0Acurrent%20synthetic%20data%20may%20be%20insufficiently%20reliable%20for%20high-stakes%0Aage-related%20tasks%20requiring%20robust%20precision%2C%20unless%20practitioners%20are%20prepared%0Ato%20invest%20in%20significant%20filtering%20and%20curation.%20Nevertheless%2C%20they%20may%20still%0Abe%20useful%20in%20less%20sensitive%20or%20exploratory%20applications%2C%20where%20absolute%20age%0Aprecision%20is%20not%20critical.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03420v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Text-to-Image%2520Generative%2520Models%2520Accurately%2520Depict%2520Age%253F%2520A%2520Comparative%250A%2520%2520Study%2520on%2520Synthetic%2520Portrait%2520Generation%2520and%2520Age%2520Estimation%26entry.906535625%3DAlexey%2520A.%2520Novikov%2520and%2520Miroslav%2520Vranka%2520and%2520Fran%25C3%25A7ois%2520David%2520and%2520Artem%2520Voronin%26entry.1292438233%3D%2520%2520Text-to-image%2520generative%2520models%2520have%2520shown%2520remarkable%2520progress%2520in%2520producing%250Adiverse%2520and%2520photorealistic%2520outputs.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520comprehensive%250Aanalysis%2520of%2520their%2520effectiveness%2520in%2520creating%2520synthetic%2520portraits%2520that%2520accurately%250Arepresent%2520various%2520demographic%2520attributes%252C%2520with%2520a%2520special%2520focus%2520on%2520age%252C%250Anationality%252C%2520and%2520gender.%2520Our%2520evaluation%2520employs%2520prompts%2520specifying%2520detailed%250Aprofiles%2520%2528e.g.%252C%2520Photorealistic%2520selfie%2520photo%2520of%2520a%252032-year-old%2520Canadian%2520male%2529%252C%250Acovering%2520a%2520broad%2520spectrum%2520of%2520212%2520nationalities%252C%252030%2520distinct%2520ages%2520from%252010%2520to%252078%252C%250Aand%2520balanced%2520gender%2520representation.%2520We%2520compare%2520the%2520generated%2520images%2520against%250Aground%2520truth%2520age%2520estimates%2520from%2520two%2520established%2520age%2520estimation%2520models%2520to%2520assess%250Ahow%2520faithfully%2520age%2520is%2520depicted.%2520Our%2520findings%2520reveal%2520that%2520although%2520text-to-image%250Amodels%2520can%2520consistently%2520generate%2520faces%2520reflecting%2520different%2520identities%252C%2520the%250Aaccuracy%2520with%2520which%2520they%2520capture%2520specific%2520ages%2520and%2520do%2520so%2520across%2520diverse%250Ademographic%2520backgrounds%2520remains%2520highly%2520variable.%2520These%2520results%2520suggest%2520that%250Acurrent%2520synthetic%2520data%2520may%2520be%2520insufficiently%2520reliable%2520for%2520high-stakes%250Aage-related%2520tasks%2520requiring%2520robust%2520precision%252C%2520unless%2520practitioners%2520are%2520prepared%250Ato%2520invest%2520in%2520significant%2520filtering%2520and%2520curation.%2520Nevertheless%252C%2520they%2520may%2520still%250Abe%2520useful%2520in%2520less%2520sensitive%2520or%2520exploratory%2520applications%252C%2520where%2520absolute%2520age%250Aprecision%2520is%2520not%2520critical.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03420v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Text-to-Image%20Generative%20Models%20Accurately%20Depict%20Age%3F%20A%20Comparative%0A%20%20Study%20on%20Synthetic%20Portrait%20Generation%20and%20Age%20Estimation&entry.906535625=Alexey%20A.%20Novikov%20and%20Miroslav%20Vranka%20and%20Fran%C3%A7ois%20David%20and%20Artem%20Voronin&entry.1292438233=%20%20Text-to-image%20generative%20models%20have%20shown%20remarkable%20progress%20in%20producing%0Adiverse%20and%20photorealistic%20outputs.%20In%20this%20paper%2C%20we%20present%20a%20comprehensive%0Aanalysis%20of%20their%20effectiveness%20in%20creating%20synthetic%20portraits%20that%20accurately%0Arepresent%20various%20demographic%20attributes%2C%20with%20a%20special%20focus%20on%20age%2C%0Anationality%2C%20and%20gender.%20Our%20evaluation%20employs%20prompts%20specifying%20detailed%0Aprofiles%20%28e.g.%2C%20Photorealistic%20selfie%20photo%20of%20a%2032-year-old%20Canadian%20male%29%2C%0Acovering%20a%20broad%20spectrum%20of%20212%20nationalities%2C%2030%20distinct%20ages%20from%2010%20to%2078%2C%0Aand%20balanced%20gender%20representation.%20We%20compare%20the%20generated%20images%20against%0Aground%20truth%20age%20estimates%20from%20two%20established%20age%20estimation%20models%20to%20assess%0Ahow%20faithfully%20age%20is%20depicted.%20Our%20findings%20reveal%20that%20although%20text-to-image%0Amodels%20can%20consistently%20generate%20faces%20reflecting%20different%20identities%2C%20the%0Aaccuracy%20with%20which%20they%20capture%20specific%20ages%20and%20do%20so%20across%20diverse%0Ademographic%20backgrounds%20remains%20highly%20variable.%20These%20results%20suggest%20that%0Acurrent%20synthetic%20data%20may%20be%20insufficiently%20reliable%20for%20high-stakes%0Aage-related%20tasks%20requiring%20robust%20precision%2C%20unless%20practitioners%20are%20prepared%0Ato%20invest%20in%20significant%20filtering%20and%20curation.%20Nevertheless%2C%20they%20may%20still%0Abe%20useful%20in%20less%20sensitive%20or%20exploratory%20applications%2C%20where%20absolute%20age%0Aprecision%20is%20not%20critical.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03420v1&entry.124074799=Read"},
{"title": "Disentanglement in Difference: Directly Learning Semantically\n  Disentangled Representations by Maximizing Inter-Factor Differences", "author": "Xingshen Zhang and Shuangrong Liu and Xintao Lu and Chaoran Pang and Lin Wang and Bo Yang", "abstract": "  In this study, Disentanglement in Difference(DiD) is proposed to address the\ninherent inconsistency between the statistical independence of latent variables\nand the goal of semantic disentanglement in disentanglement representation\nlearning. Conventional disentanglement methods achieve disentanglement\nrepresentation by improving statistical independence among latent variables.\nHowever, the statistical independence of latent variables does not necessarily\nimply that they are semantically unrelated, thus, improving statistical\nindependence does not always enhance disentanglement performance. To address\nthe above issue, DiD is proposed to directly learn semantic differences rather\nthan the statistical independence of latent variables. In the DiD, a Difference\nEncoder is designed to measure the semantic differences; a contrastive loss\nfunction is established to facilitate inter-dimensional comparison. Both of\nthem allow the model to directly differentiate and disentangle distinct\nsemantic factors, thereby resolving the inconsistency between statistical\nindependence and semantic disentanglement. Experimental results on the dSprites\nand 3DShapes datasets demonstrate that the proposed DiD outperforms existing\nmainstream methods across various disentanglement metrics.\n", "link": "http://arxiv.org/abs/2502.03123v1", "date": "2025-02-05", "relevancy": 2.1598, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5442}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5442}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5184}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentanglement%20in%20Difference%3A%20Directly%20Learning%20Semantically%0A%20%20Disentangled%20Representations%20by%20Maximizing%20Inter-Factor%20Differences&body=Title%3A%20Disentanglement%20in%20Difference%3A%20Directly%20Learning%20Semantically%0A%20%20Disentangled%20Representations%20by%20Maximizing%20Inter-Factor%20Differences%0AAuthor%3A%20Xingshen%20Zhang%20and%20Shuangrong%20Liu%20and%20Xintao%20Lu%20and%20Chaoran%20Pang%20and%20Lin%20Wang%20and%20Bo%20Yang%0AAbstract%3A%20%20%20In%20this%20study%2C%20Disentanglement%20in%20Difference%28DiD%29%20is%20proposed%20to%20address%20the%0Ainherent%20inconsistency%20between%20the%20statistical%20independence%20of%20latent%20variables%0Aand%20the%20goal%20of%20semantic%20disentanglement%20in%20disentanglement%20representation%0Alearning.%20Conventional%20disentanglement%20methods%20achieve%20disentanglement%0Arepresentation%20by%20improving%20statistical%20independence%20among%20latent%20variables.%0AHowever%2C%20the%20statistical%20independence%20of%20latent%20variables%20does%20not%20necessarily%0Aimply%20that%20they%20are%20semantically%20unrelated%2C%20thus%2C%20improving%20statistical%0Aindependence%20does%20not%20always%20enhance%20disentanglement%20performance.%20To%20address%0Athe%20above%20issue%2C%20DiD%20is%20proposed%20to%20directly%20learn%20semantic%20differences%20rather%0Athan%20the%20statistical%20independence%20of%20latent%20variables.%20In%20the%20DiD%2C%20a%20Difference%0AEncoder%20is%20designed%20to%20measure%20the%20semantic%20differences%3B%20a%20contrastive%20loss%0Afunction%20is%20established%20to%20facilitate%20inter-dimensional%20comparison.%20Both%20of%0Athem%20allow%20the%20model%20to%20directly%20differentiate%20and%20disentangle%20distinct%0Asemantic%20factors%2C%20thereby%20resolving%20the%20inconsistency%20between%20statistical%0Aindependence%20and%20semantic%20disentanglement.%20Experimental%20results%20on%20the%20dSprites%0Aand%203DShapes%20datasets%20demonstrate%20that%20the%20proposed%20DiD%20outperforms%20existing%0Amainstream%20methods%20across%20various%20disentanglement%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03123v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentanglement%2520in%2520Difference%253A%2520Directly%2520Learning%2520Semantically%250A%2520%2520Disentangled%2520Representations%2520by%2520Maximizing%2520Inter-Factor%2520Differences%26entry.906535625%3DXingshen%2520Zhang%2520and%2520Shuangrong%2520Liu%2520and%2520Xintao%2520Lu%2520and%2520Chaoran%2520Pang%2520and%2520Lin%2520Wang%2520and%2520Bo%2520Yang%26entry.1292438233%3D%2520%2520In%2520this%2520study%252C%2520Disentanglement%2520in%2520Difference%2528DiD%2529%2520is%2520proposed%2520to%2520address%2520the%250Ainherent%2520inconsistency%2520between%2520the%2520statistical%2520independence%2520of%2520latent%2520variables%250Aand%2520the%2520goal%2520of%2520semantic%2520disentanglement%2520in%2520disentanglement%2520representation%250Alearning.%2520Conventional%2520disentanglement%2520methods%2520achieve%2520disentanglement%250Arepresentation%2520by%2520improving%2520statistical%2520independence%2520among%2520latent%2520variables.%250AHowever%252C%2520the%2520statistical%2520independence%2520of%2520latent%2520variables%2520does%2520not%2520necessarily%250Aimply%2520that%2520they%2520are%2520semantically%2520unrelated%252C%2520thus%252C%2520improving%2520statistical%250Aindependence%2520does%2520not%2520always%2520enhance%2520disentanglement%2520performance.%2520To%2520address%250Athe%2520above%2520issue%252C%2520DiD%2520is%2520proposed%2520to%2520directly%2520learn%2520semantic%2520differences%2520rather%250Athan%2520the%2520statistical%2520independence%2520of%2520latent%2520variables.%2520In%2520the%2520DiD%252C%2520a%2520Difference%250AEncoder%2520is%2520designed%2520to%2520measure%2520the%2520semantic%2520differences%253B%2520a%2520contrastive%2520loss%250Afunction%2520is%2520established%2520to%2520facilitate%2520inter-dimensional%2520comparison.%2520Both%2520of%250Athem%2520allow%2520the%2520model%2520to%2520directly%2520differentiate%2520and%2520disentangle%2520distinct%250Asemantic%2520factors%252C%2520thereby%2520resolving%2520the%2520inconsistency%2520between%2520statistical%250Aindependence%2520and%2520semantic%2520disentanglement.%2520Experimental%2520results%2520on%2520the%2520dSprites%250Aand%25203DShapes%2520datasets%2520demonstrate%2520that%2520the%2520proposed%2520DiD%2520outperforms%2520existing%250Amainstream%2520methods%2520across%2520various%2520disentanglement%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03123v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentanglement%20in%20Difference%3A%20Directly%20Learning%20Semantically%0A%20%20Disentangled%20Representations%20by%20Maximizing%20Inter-Factor%20Differences&entry.906535625=Xingshen%20Zhang%20and%20Shuangrong%20Liu%20and%20Xintao%20Lu%20and%20Chaoran%20Pang%20and%20Lin%20Wang%20and%20Bo%20Yang&entry.1292438233=%20%20In%20this%20study%2C%20Disentanglement%20in%20Difference%28DiD%29%20is%20proposed%20to%20address%20the%0Ainherent%20inconsistency%20between%20the%20statistical%20independence%20of%20latent%20variables%0Aand%20the%20goal%20of%20semantic%20disentanglement%20in%20disentanglement%20representation%0Alearning.%20Conventional%20disentanglement%20methods%20achieve%20disentanglement%0Arepresentation%20by%20improving%20statistical%20independence%20among%20latent%20variables.%0AHowever%2C%20the%20statistical%20independence%20of%20latent%20variables%20does%20not%20necessarily%0Aimply%20that%20they%20are%20semantically%20unrelated%2C%20thus%2C%20improving%20statistical%0Aindependence%20does%20not%20always%20enhance%20disentanglement%20performance.%20To%20address%0Athe%20above%20issue%2C%20DiD%20is%20proposed%20to%20directly%20learn%20semantic%20differences%20rather%0Athan%20the%20statistical%20independence%20of%20latent%20variables.%20In%20the%20DiD%2C%20a%20Difference%0AEncoder%20is%20designed%20to%20measure%20the%20semantic%20differences%3B%20a%20contrastive%20loss%0Afunction%20is%20established%20to%20facilitate%20inter-dimensional%20comparison.%20Both%20of%0Athem%20allow%20the%20model%20to%20directly%20differentiate%20and%20disentangle%20distinct%0Asemantic%20factors%2C%20thereby%20resolving%20the%20inconsistency%20between%20statistical%0Aindependence%20and%20semantic%20disentanglement.%20Experimental%20results%20on%20the%20dSprites%0Aand%203DShapes%20datasets%20demonstrate%20that%20the%20proposed%20DiD%20outperforms%20existing%0Amainstream%20methods%20across%20various%20disentanglement%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03123v1&entry.124074799=Read"},
{"title": "More Experts Than Galaxies: Conditionally-overlapping Experts With\n  Biologically-Inspired Fixed Routing", "author": "Sagi Shaier and Francisco Pereira and Katharina von der Wense and Lawrence E Hunter and Matt Jones", "abstract": "  The evolution of biological neural systems has led to both modularity and\nsparse coding, which enables energy efficiency and robustness across the\ndiversity of tasks in the lifespan. In contrast, standard neural networks rely\non dense, non-specialized architectures, where all model parameters are\nsimultaneously updated to learn multiple tasks, leading to interference.\nCurrent sparse neural network approaches aim to alleviate this issue but are\nhindered by limitations such as 1) trainable gating functions that cause\nrepresentation collapse, 2) disjoint experts that result in redundant\ncomputation and slow learning, and 3) reliance on explicit input or task IDs\nthat limit flexibility and scalability. In this paper we propose Conditionally\nOverlapping Mixture of ExperTs (COMET), a general deep learning method that\naddresses these challenges by inducing a modular, sparse architecture with an\nexponential number of overlapping experts. COMET replaces the trainable gating\nfunction used in Sparse Mixture of Experts with a fixed, biologically inspired\nrandom projection applied to individual input representations. This design\ncauses the degree of expert overlap to depend on input similarity, so that\nsimilar inputs tend to share more parameters. This results in faster learning\nper update step and improved out-of-sample generalization. We demonstrate the\neffectiveness of COMET on a range of tasks, including image classification,\nlanguage modeling, and regression, using several popular deep learning\narchitectures.\n", "link": "http://arxiv.org/abs/2410.08003v5", "date": "2025-02-05", "relevancy": 2.1565, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5613}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5451}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5243}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20More%20Experts%20Than%20Galaxies%3A%20Conditionally-overlapping%20Experts%20With%0A%20%20Biologically-Inspired%20Fixed%20Routing&body=Title%3A%20More%20Experts%20Than%20Galaxies%3A%20Conditionally-overlapping%20Experts%20With%0A%20%20Biologically-Inspired%20Fixed%20Routing%0AAuthor%3A%20Sagi%20Shaier%20and%20Francisco%20Pereira%20and%20Katharina%20von%20der%20Wense%20and%20Lawrence%20E%20Hunter%20and%20Matt%20Jones%0AAbstract%3A%20%20%20The%20evolution%20of%20biological%20neural%20systems%20has%20led%20to%20both%20modularity%20and%0Asparse%20coding%2C%20which%20enables%20energy%20efficiency%20and%20robustness%20across%20the%0Adiversity%20of%20tasks%20in%20the%20lifespan.%20In%20contrast%2C%20standard%20neural%20networks%20rely%0Aon%20dense%2C%20non-specialized%20architectures%2C%20where%20all%20model%20parameters%20are%0Asimultaneously%20updated%20to%20learn%20multiple%20tasks%2C%20leading%20to%20interference.%0ACurrent%20sparse%20neural%20network%20approaches%20aim%20to%20alleviate%20this%20issue%20but%20are%0Ahindered%20by%20limitations%20such%20as%201%29%20trainable%20gating%20functions%20that%20cause%0Arepresentation%20collapse%2C%202%29%20disjoint%20experts%20that%20result%20in%20redundant%0Acomputation%20and%20slow%20learning%2C%20and%203%29%20reliance%20on%20explicit%20input%20or%20task%20IDs%0Athat%20limit%20flexibility%20and%20scalability.%20In%20this%20paper%20we%20propose%20Conditionally%0AOverlapping%20Mixture%20of%20ExperTs%20%28COMET%29%2C%20a%20general%20deep%20learning%20method%20that%0Aaddresses%20these%20challenges%20by%20inducing%20a%20modular%2C%20sparse%20architecture%20with%20an%0Aexponential%20number%20of%20overlapping%20experts.%20COMET%20replaces%20the%20trainable%20gating%0Afunction%20used%20in%20Sparse%20Mixture%20of%20Experts%20with%20a%20fixed%2C%20biologically%20inspired%0Arandom%20projection%20applied%20to%20individual%20input%20representations.%20This%20design%0Acauses%20the%20degree%20of%20expert%20overlap%20to%20depend%20on%20input%20similarity%2C%20so%20that%0Asimilar%20inputs%20tend%20to%20share%20more%20parameters.%20This%20results%20in%20faster%20learning%0Aper%20update%20step%20and%20improved%20out-of-sample%20generalization.%20We%20demonstrate%20the%0Aeffectiveness%20of%20COMET%20on%20a%20range%20of%20tasks%2C%20including%20image%20classification%2C%0Alanguage%20modeling%2C%20and%20regression%2C%20using%20several%20popular%20deep%20learning%0Aarchitectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.08003v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMore%2520Experts%2520Than%2520Galaxies%253A%2520Conditionally-overlapping%2520Experts%2520With%250A%2520%2520Biologically-Inspired%2520Fixed%2520Routing%26entry.906535625%3DSagi%2520Shaier%2520and%2520Francisco%2520Pereira%2520and%2520Katharina%2520von%2520der%2520Wense%2520and%2520Lawrence%2520E%2520Hunter%2520and%2520Matt%2520Jones%26entry.1292438233%3D%2520%2520The%2520evolution%2520of%2520biological%2520neural%2520systems%2520has%2520led%2520to%2520both%2520modularity%2520and%250Asparse%2520coding%252C%2520which%2520enables%2520energy%2520efficiency%2520and%2520robustness%2520across%2520the%250Adiversity%2520of%2520tasks%2520in%2520the%2520lifespan.%2520In%2520contrast%252C%2520standard%2520neural%2520networks%2520rely%250Aon%2520dense%252C%2520non-specialized%2520architectures%252C%2520where%2520all%2520model%2520parameters%2520are%250Asimultaneously%2520updated%2520to%2520learn%2520multiple%2520tasks%252C%2520leading%2520to%2520interference.%250ACurrent%2520sparse%2520neural%2520network%2520approaches%2520aim%2520to%2520alleviate%2520this%2520issue%2520but%2520are%250Ahindered%2520by%2520limitations%2520such%2520as%25201%2529%2520trainable%2520gating%2520functions%2520that%2520cause%250Arepresentation%2520collapse%252C%25202%2529%2520disjoint%2520experts%2520that%2520result%2520in%2520redundant%250Acomputation%2520and%2520slow%2520learning%252C%2520and%25203%2529%2520reliance%2520on%2520explicit%2520input%2520or%2520task%2520IDs%250Athat%2520limit%2520flexibility%2520and%2520scalability.%2520In%2520this%2520paper%2520we%2520propose%2520Conditionally%250AOverlapping%2520Mixture%2520of%2520ExperTs%2520%2528COMET%2529%252C%2520a%2520general%2520deep%2520learning%2520method%2520that%250Aaddresses%2520these%2520challenges%2520by%2520inducing%2520a%2520modular%252C%2520sparse%2520architecture%2520with%2520an%250Aexponential%2520number%2520of%2520overlapping%2520experts.%2520COMET%2520replaces%2520the%2520trainable%2520gating%250Afunction%2520used%2520in%2520Sparse%2520Mixture%2520of%2520Experts%2520with%2520a%2520fixed%252C%2520biologically%2520inspired%250Arandom%2520projection%2520applied%2520to%2520individual%2520input%2520representations.%2520This%2520design%250Acauses%2520the%2520degree%2520of%2520expert%2520overlap%2520to%2520depend%2520on%2520input%2520similarity%252C%2520so%2520that%250Asimilar%2520inputs%2520tend%2520to%2520share%2520more%2520parameters.%2520This%2520results%2520in%2520faster%2520learning%250Aper%2520update%2520step%2520and%2520improved%2520out-of-sample%2520generalization.%2520We%2520demonstrate%2520the%250Aeffectiveness%2520of%2520COMET%2520on%2520a%2520range%2520of%2520tasks%252C%2520including%2520image%2520classification%252C%250Alanguage%2520modeling%252C%2520and%2520regression%252C%2520using%2520several%2520popular%2520deep%2520learning%250Aarchitectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.08003v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=More%20Experts%20Than%20Galaxies%3A%20Conditionally-overlapping%20Experts%20With%0A%20%20Biologically-Inspired%20Fixed%20Routing&entry.906535625=Sagi%20Shaier%20and%20Francisco%20Pereira%20and%20Katharina%20von%20der%20Wense%20and%20Lawrence%20E%20Hunter%20and%20Matt%20Jones&entry.1292438233=%20%20The%20evolution%20of%20biological%20neural%20systems%20has%20led%20to%20both%20modularity%20and%0Asparse%20coding%2C%20which%20enables%20energy%20efficiency%20and%20robustness%20across%20the%0Adiversity%20of%20tasks%20in%20the%20lifespan.%20In%20contrast%2C%20standard%20neural%20networks%20rely%0Aon%20dense%2C%20non-specialized%20architectures%2C%20where%20all%20model%20parameters%20are%0Asimultaneously%20updated%20to%20learn%20multiple%20tasks%2C%20leading%20to%20interference.%0ACurrent%20sparse%20neural%20network%20approaches%20aim%20to%20alleviate%20this%20issue%20but%20are%0Ahindered%20by%20limitations%20such%20as%201%29%20trainable%20gating%20functions%20that%20cause%0Arepresentation%20collapse%2C%202%29%20disjoint%20experts%20that%20result%20in%20redundant%0Acomputation%20and%20slow%20learning%2C%20and%203%29%20reliance%20on%20explicit%20input%20or%20task%20IDs%0Athat%20limit%20flexibility%20and%20scalability.%20In%20this%20paper%20we%20propose%20Conditionally%0AOverlapping%20Mixture%20of%20ExperTs%20%28COMET%29%2C%20a%20general%20deep%20learning%20method%20that%0Aaddresses%20these%20challenges%20by%20inducing%20a%20modular%2C%20sparse%20architecture%20with%20an%0Aexponential%20number%20of%20overlapping%20experts.%20COMET%20replaces%20the%20trainable%20gating%0Afunction%20used%20in%20Sparse%20Mixture%20of%20Experts%20with%20a%20fixed%2C%20biologically%20inspired%0Arandom%20projection%20applied%20to%20individual%20input%20representations.%20This%20design%0Acauses%20the%20degree%20of%20expert%20overlap%20to%20depend%20on%20input%20similarity%2C%20so%20that%0Asimilar%20inputs%20tend%20to%20share%20more%20parameters.%20This%20results%20in%20faster%20learning%0Aper%20update%20step%20and%20improved%20out-of-sample%20generalization.%20We%20demonstrate%20the%0Aeffectiveness%20of%20COMET%20on%20a%20range%20of%20tasks%2C%20including%20image%20classification%2C%0Alanguage%20modeling%2C%20and%20regression%2C%20using%20several%20popular%20deep%20learning%0Aarchitectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.08003v5&entry.124074799=Read"},
{"title": "Lightweight Authenticated Task Offloading in 6G-Cloud Vehicular Twin\n  Networks", "author": "Sarah Al-Shareeda and Fusun Ozguner and Keith Redmill and Trung Q. Duong and Berk Canberk", "abstract": "  Task offloading management in 6G vehicular networks is crucial for\nmaintaining network efficiency, particularly as vehicles generate substantial\ndata. Integrating secure communication through authentication introduces\nadditional computational and communication overhead, significantly impacting\noffloading efficiency and latency. This paper presents a unified framework\nincorporating lightweight Identity-Based Cryptographic (IBC) authentication\ninto task offloading within cloud-based 6G Vehicular Twin Networks (VTNs).\nUtilizing Proximal Policy Optimization (PPO) in Deep Reinforcement Learning\n(DRL), our approach optimizes authenticated offloading decisions to minimize\nlatency and enhance resource allocation. Performance evaluation under varying\nnetwork sizes, task sizes, and data rates reveals that IBC authentication can\nreduce offloading efficiency by up to 50% due to the added overhead. Besides,\nincreasing network size and task size can further reduce offloading efficiency\nby up to 91.7%. As a countermeasure, increasing the transmission data rate can\nimprove the offloading performance by as much as 63%, even in the presence of\nauthentication overhead. The code for the simulations and experiments detailed\nin this paper is available on GitHub for further reference and reproducibility\n[1].\n", "link": "http://arxiv.org/abs/2502.03403v1", "date": "2025-02-05", "relevancy": 2.1531, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4404}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4284}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lightweight%20Authenticated%20Task%20Offloading%20in%206G-Cloud%20Vehicular%20Twin%0A%20%20Networks&body=Title%3A%20Lightweight%20Authenticated%20Task%20Offloading%20in%206G-Cloud%20Vehicular%20Twin%0A%20%20Networks%0AAuthor%3A%20Sarah%20Al-Shareeda%20and%20Fusun%20Ozguner%20and%20Keith%20Redmill%20and%20Trung%20Q.%20Duong%20and%20Berk%20Canberk%0AAbstract%3A%20%20%20Task%20offloading%20management%20in%206G%20vehicular%20networks%20is%20crucial%20for%0Amaintaining%20network%20efficiency%2C%20particularly%20as%20vehicles%20generate%20substantial%0Adata.%20Integrating%20secure%20communication%20through%20authentication%20introduces%0Aadditional%20computational%20and%20communication%20overhead%2C%20significantly%20impacting%0Aoffloading%20efficiency%20and%20latency.%20This%20paper%20presents%20a%20unified%20framework%0Aincorporating%20lightweight%20Identity-Based%20Cryptographic%20%28IBC%29%20authentication%0Ainto%20task%20offloading%20within%20cloud-based%206G%20Vehicular%20Twin%20Networks%20%28VTNs%29.%0AUtilizing%20Proximal%20Policy%20Optimization%20%28PPO%29%20in%20Deep%20Reinforcement%20Learning%0A%28DRL%29%2C%20our%20approach%20optimizes%20authenticated%20offloading%20decisions%20to%20minimize%0Alatency%20and%20enhance%20resource%20allocation.%20Performance%20evaluation%20under%20varying%0Anetwork%20sizes%2C%20task%20sizes%2C%20and%20data%20rates%20reveals%20that%20IBC%20authentication%20can%0Areduce%20offloading%20efficiency%20by%20up%20to%2050%25%20due%20to%20the%20added%20overhead.%20Besides%2C%0Aincreasing%20network%20size%20and%20task%20size%20can%20further%20reduce%20offloading%20efficiency%0Aby%20up%20to%2091.7%25.%20As%20a%20countermeasure%2C%20increasing%20the%20transmission%20data%20rate%20can%0Aimprove%20the%20offloading%20performance%20by%20as%20much%20as%2063%25%2C%20even%20in%20the%20presence%20of%0Aauthentication%20overhead.%20The%20code%20for%20the%20simulations%20and%20experiments%20detailed%0Ain%20this%20paper%20is%20available%20on%20GitHub%20for%20further%20reference%20and%20reproducibility%0A%5B1%5D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03403v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightweight%2520Authenticated%2520Task%2520Offloading%2520in%25206G-Cloud%2520Vehicular%2520Twin%250A%2520%2520Networks%26entry.906535625%3DSarah%2520Al-Shareeda%2520and%2520Fusun%2520Ozguner%2520and%2520Keith%2520Redmill%2520and%2520Trung%2520Q.%2520Duong%2520and%2520Berk%2520Canberk%26entry.1292438233%3D%2520%2520Task%2520offloading%2520management%2520in%25206G%2520vehicular%2520networks%2520is%2520crucial%2520for%250Amaintaining%2520network%2520efficiency%252C%2520particularly%2520as%2520vehicles%2520generate%2520substantial%250Adata.%2520Integrating%2520secure%2520communication%2520through%2520authentication%2520introduces%250Aadditional%2520computational%2520and%2520communication%2520overhead%252C%2520significantly%2520impacting%250Aoffloading%2520efficiency%2520and%2520latency.%2520This%2520paper%2520presents%2520a%2520unified%2520framework%250Aincorporating%2520lightweight%2520Identity-Based%2520Cryptographic%2520%2528IBC%2529%2520authentication%250Ainto%2520task%2520offloading%2520within%2520cloud-based%25206G%2520Vehicular%2520Twin%2520Networks%2520%2528VTNs%2529.%250AUtilizing%2520Proximal%2520Policy%2520Optimization%2520%2528PPO%2529%2520in%2520Deep%2520Reinforcement%2520Learning%250A%2528DRL%2529%252C%2520our%2520approach%2520optimizes%2520authenticated%2520offloading%2520decisions%2520to%2520minimize%250Alatency%2520and%2520enhance%2520resource%2520allocation.%2520Performance%2520evaluation%2520under%2520varying%250Anetwork%2520sizes%252C%2520task%2520sizes%252C%2520and%2520data%2520rates%2520reveals%2520that%2520IBC%2520authentication%2520can%250Areduce%2520offloading%2520efficiency%2520by%2520up%2520to%252050%2525%2520due%2520to%2520the%2520added%2520overhead.%2520Besides%252C%250Aincreasing%2520network%2520size%2520and%2520task%2520size%2520can%2520further%2520reduce%2520offloading%2520efficiency%250Aby%2520up%2520to%252091.7%2525.%2520As%2520a%2520countermeasure%252C%2520increasing%2520the%2520transmission%2520data%2520rate%2520can%250Aimprove%2520the%2520offloading%2520performance%2520by%2520as%2520much%2520as%252063%2525%252C%2520even%2520in%2520the%2520presence%2520of%250Aauthentication%2520overhead.%2520The%2520code%2520for%2520the%2520simulations%2520and%2520experiments%2520detailed%250Ain%2520this%2520paper%2520is%2520available%2520on%2520GitHub%2520for%2520further%2520reference%2520and%2520reproducibility%250A%255B1%255D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03403v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lightweight%20Authenticated%20Task%20Offloading%20in%206G-Cloud%20Vehicular%20Twin%0A%20%20Networks&entry.906535625=Sarah%20Al-Shareeda%20and%20Fusun%20Ozguner%20and%20Keith%20Redmill%20and%20Trung%20Q.%20Duong%20and%20Berk%20Canberk&entry.1292438233=%20%20Task%20offloading%20management%20in%206G%20vehicular%20networks%20is%20crucial%20for%0Amaintaining%20network%20efficiency%2C%20particularly%20as%20vehicles%20generate%20substantial%0Adata.%20Integrating%20secure%20communication%20through%20authentication%20introduces%0Aadditional%20computational%20and%20communication%20overhead%2C%20significantly%20impacting%0Aoffloading%20efficiency%20and%20latency.%20This%20paper%20presents%20a%20unified%20framework%0Aincorporating%20lightweight%20Identity-Based%20Cryptographic%20%28IBC%29%20authentication%0Ainto%20task%20offloading%20within%20cloud-based%206G%20Vehicular%20Twin%20Networks%20%28VTNs%29.%0AUtilizing%20Proximal%20Policy%20Optimization%20%28PPO%29%20in%20Deep%20Reinforcement%20Learning%0A%28DRL%29%2C%20our%20approach%20optimizes%20authenticated%20offloading%20decisions%20to%20minimize%0Alatency%20and%20enhance%20resource%20allocation.%20Performance%20evaluation%20under%20varying%0Anetwork%20sizes%2C%20task%20sizes%2C%20and%20data%20rates%20reveals%20that%20IBC%20authentication%20can%0Areduce%20offloading%20efficiency%20by%20up%20to%2050%25%20due%20to%20the%20added%20overhead.%20Besides%2C%0Aincreasing%20network%20size%20and%20task%20size%20can%20further%20reduce%20offloading%20efficiency%0Aby%20up%20to%2091.7%25.%20As%20a%20countermeasure%2C%20increasing%20the%20transmission%20data%20rate%20can%0Aimprove%20the%20offloading%20performance%20by%20as%20much%20as%2063%25%2C%20even%20in%20the%20presence%20of%0Aauthentication%20overhead.%20The%20code%20for%20the%20simulations%20and%20experiments%20detailed%0Ain%20this%20paper%20is%20available%20on%20GitHub%20for%20further%20reference%20and%20reproducibility%0A%5B1%5D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03403v1&entry.124074799=Read"},
{"title": "Metis: A Foundation Speech Generation Model with Masked Generative\n  Pre-training", "author": "Yuancheng Wang and Jiachen Zheng and Junan Zhang and Xueyao Zhang and Huan Liao and Zhizheng Wu", "abstract": "  We introduce Metis, a foundation model for unified speech generation. Unlike\nprevious task-specific or multi-task models, Metis follows a pre-training and\nfine-tuning paradigm. It is pre-trained on large-scale unlabeled speech data\nusing masked generative modeling and then fine-tuned to adapt to diverse speech\ngeneration tasks. Specifically, 1) Metis utilizes two discrete speech\nrepresentations: SSL tokens derived from speech self-supervised learning (SSL)\nfeatures, and acoustic tokens directly quantized from waveforms. 2) Metis\nperforms masked generative pre-training on SSL tokens, utilizing 300K hours of\ndiverse speech data, without any additional condition. 3) Through fine-tuning\nwith task-specific conditions, Metis achieves efficient adaptation to various\nspeech generation tasks while supporting multimodal input, even when using\nlimited data and trainable parameters. Experiments demonstrate that Metis can\nserve as a foundation model for unified speech generation: Metis outperforms\nstate-of-the-art task-specific or multi-task systems across five speech\ngeneration tasks, including zero-shot text-to-speech, voice conversion, target\nspeaker extraction, speech enhancement, and lip-to-speech, even with fewer than\n20M trainable parameters or 300 times less training data. Audio samples are are\navailable at https://metis-demo.github.io/.\n", "link": "http://arxiv.org/abs/2502.03128v1", "date": "2025-02-05", "relevancy": 2.1466, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5533}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5256}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Metis%3A%20A%20Foundation%20Speech%20Generation%20Model%20with%20Masked%20Generative%0A%20%20Pre-training&body=Title%3A%20Metis%3A%20A%20Foundation%20Speech%20Generation%20Model%20with%20Masked%20Generative%0A%20%20Pre-training%0AAuthor%3A%20Yuancheng%20Wang%20and%20Jiachen%20Zheng%20and%20Junan%20Zhang%20and%20Xueyao%20Zhang%20and%20Huan%20Liao%20and%20Zhizheng%20Wu%0AAbstract%3A%20%20%20We%20introduce%20Metis%2C%20a%20foundation%20model%20for%20unified%20speech%20generation.%20Unlike%0Aprevious%20task-specific%20or%20multi-task%20models%2C%20Metis%20follows%20a%20pre-training%20and%0Afine-tuning%20paradigm.%20It%20is%20pre-trained%20on%20large-scale%20unlabeled%20speech%20data%0Ausing%20masked%20generative%20modeling%20and%20then%20fine-tuned%20to%20adapt%20to%20diverse%20speech%0Ageneration%20tasks.%20Specifically%2C%201%29%20Metis%20utilizes%20two%20discrete%20speech%0Arepresentations%3A%20SSL%20tokens%20derived%20from%20speech%20self-supervised%20learning%20%28SSL%29%0Afeatures%2C%20and%20acoustic%20tokens%20directly%20quantized%20from%20waveforms.%202%29%20Metis%0Aperforms%20masked%20generative%20pre-training%20on%20SSL%20tokens%2C%20utilizing%20300K%20hours%20of%0Adiverse%20speech%20data%2C%20without%20any%20additional%20condition.%203%29%20Through%20fine-tuning%0Awith%20task-specific%20conditions%2C%20Metis%20achieves%20efficient%20adaptation%20to%20various%0Aspeech%20generation%20tasks%20while%20supporting%20multimodal%20input%2C%20even%20when%20using%0Alimited%20data%20and%20trainable%20parameters.%20Experiments%20demonstrate%20that%20Metis%20can%0Aserve%20as%20a%20foundation%20model%20for%20unified%20speech%20generation%3A%20Metis%20outperforms%0Astate-of-the-art%20task-specific%20or%20multi-task%20systems%20across%20five%20speech%0Ageneration%20tasks%2C%20including%20zero-shot%20text-to-speech%2C%20voice%20conversion%2C%20target%0Aspeaker%20extraction%2C%20speech%20enhancement%2C%20and%20lip-to-speech%2C%20even%20with%20fewer%20than%0A20M%20trainable%20parameters%20or%20300%20times%20less%20training%20data.%20Audio%20samples%20are%20are%0Aavailable%20at%20https%3A//metis-demo.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03128v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetis%253A%2520A%2520Foundation%2520Speech%2520Generation%2520Model%2520with%2520Masked%2520Generative%250A%2520%2520Pre-training%26entry.906535625%3DYuancheng%2520Wang%2520and%2520Jiachen%2520Zheng%2520and%2520Junan%2520Zhang%2520and%2520Xueyao%2520Zhang%2520and%2520Huan%2520Liao%2520and%2520Zhizheng%2520Wu%26entry.1292438233%3D%2520%2520We%2520introduce%2520Metis%252C%2520a%2520foundation%2520model%2520for%2520unified%2520speech%2520generation.%2520Unlike%250Aprevious%2520task-specific%2520or%2520multi-task%2520models%252C%2520Metis%2520follows%2520a%2520pre-training%2520and%250Afine-tuning%2520paradigm.%2520It%2520is%2520pre-trained%2520on%2520large-scale%2520unlabeled%2520speech%2520data%250Ausing%2520masked%2520generative%2520modeling%2520and%2520then%2520fine-tuned%2520to%2520adapt%2520to%2520diverse%2520speech%250Ageneration%2520tasks.%2520Specifically%252C%25201%2529%2520Metis%2520utilizes%2520two%2520discrete%2520speech%250Arepresentations%253A%2520SSL%2520tokens%2520derived%2520from%2520speech%2520self-supervised%2520learning%2520%2528SSL%2529%250Afeatures%252C%2520and%2520acoustic%2520tokens%2520directly%2520quantized%2520from%2520waveforms.%25202%2529%2520Metis%250Aperforms%2520masked%2520generative%2520pre-training%2520on%2520SSL%2520tokens%252C%2520utilizing%2520300K%2520hours%2520of%250Adiverse%2520speech%2520data%252C%2520without%2520any%2520additional%2520condition.%25203%2529%2520Through%2520fine-tuning%250Awith%2520task-specific%2520conditions%252C%2520Metis%2520achieves%2520efficient%2520adaptation%2520to%2520various%250Aspeech%2520generation%2520tasks%2520while%2520supporting%2520multimodal%2520input%252C%2520even%2520when%2520using%250Alimited%2520data%2520and%2520trainable%2520parameters.%2520Experiments%2520demonstrate%2520that%2520Metis%2520can%250Aserve%2520as%2520a%2520foundation%2520model%2520for%2520unified%2520speech%2520generation%253A%2520Metis%2520outperforms%250Astate-of-the-art%2520task-specific%2520or%2520multi-task%2520systems%2520across%2520five%2520speech%250Ageneration%2520tasks%252C%2520including%2520zero-shot%2520text-to-speech%252C%2520voice%2520conversion%252C%2520target%250Aspeaker%2520extraction%252C%2520speech%2520enhancement%252C%2520and%2520lip-to-speech%252C%2520even%2520with%2520fewer%2520than%250A20M%2520trainable%2520parameters%2520or%2520300%2520times%2520less%2520training%2520data.%2520Audio%2520samples%2520are%2520are%250Aavailable%2520at%2520https%253A//metis-demo.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03128v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Metis%3A%20A%20Foundation%20Speech%20Generation%20Model%20with%20Masked%20Generative%0A%20%20Pre-training&entry.906535625=Yuancheng%20Wang%20and%20Jiachen%20Zheng%20and%20Junan%20Zhang%20and%20Xueyao%20Zhang%20and%20Huan%20Liao%20and%20Zhizheng%20Wu&entry.1292438233=%20%20We%20introduce%20Metis%2C%20a%20foundation%20model%20for%20unified%20speech%20generation.%20Unlike%0Aprevious%20task-specific%20or%20multi-task%20models%2C%20Metis%20follows%20a%20pre-training%20and%0Afine-tuning%20paradigm.%20It%20is%20pre-trained%20on%20large-scale%20unlabeled%20speech%20data%0Ausing%20masked%20generative%20modeling%20and%20then%20fine-tuned%20to%20adapt%20to%20diverse%20speech%0Ageneration%20tasks.%20Specifically%2C%201%29%20Metis%20utilizes%20two%20discrete%20speech%0Arepresentations%3A%20SSL%20tokens%20derived%20from%20speech%20self-supervised%20learning%20%28SSL%29%0Afeatures%2C%20and%20acoustic%20tokens%20directly%20quantized%20from%20waveforms.%202%29%20Metis%0Aperforms%20masked%20generative%20pre-training%20on%20SSL%20tokens%2C%20utilizing%20300K%20hours%20of%0Adiverse%20speech%20data%2C%20without%20any%20additional%20condition.%203%29%20Through%20fine-tuning%0Awith%20task-specific%20conditions%2C%20Metis%20achieves%20efficient%20adaptation%20to%20various%0Aspeech%20generation%20tasks%20while%20supporting%20multimodal%20input%2C%20even%20when%20using%0Alimited%20data%20and%20trainable%20parameters.%20Experiments%20demonstrate%20that%20Metis%20can%0Aserve%20as%20a%20foundation%20model%20for%20unified%20speech%20generation%3A%20Metis%20outperforms%0Astate-of-the-art%20task-specific%20or%20multi-task%20systems%20across%20five%20speech%0Ageneration%20tasks%2C%20including%20zero-shot%20text-to-speech%2C%20voice%20conversion%2C%20target%0Aspeaker%20extraction%2C%20speech%20enhancement%2C%20and%20lip-to-speech%2C%20even%20with%20fewer%20than%0A20M%20trainable%20parameters%20or%20300%20times%20less%20training%20data.%20Audio%20samples%20are%20are%0Aavailable%20at%20https%3A//metis-demo.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03128v1&entry.124074799=Read"},
{"title": "MAP Image Recovery with Guarantees using Locally Convex Multi-Scale\n  Energy (LC-MUSE) Model", "author": "Jyothi Rikhab Chand and Mathews Jacob", "abstract": "  We propose a multi-scale deep energy model that is strongly convex in the\nlocal neighbourhood around the data manifold to represent its probability\ndensity, with application in inverse problems. In particular, we represent the\nnegative log-prior as a multi-scale energy model parameterized by a\nConvolutional Neural Network (CNN). We restrict the gradient of the CNN to be\nlocally monotone, which constrains the model as a Locally Convex Multi-Scale\nEnergy (LC-MuSE). We use the learned energy model in image-based inverse\nproblems, where the formulation offers several desirable properties: i)\nuniqueness of the solution, ii) convergence guarantees to a minimum of the\ninverse problem, and iii) robustness to input perturbations. In the context of\nparallel Magnetic Resonance (MR) image reconstruction, we show that the\nproposed method performs better than the state-of-the-art convex regularizers,\nwhile the performance is comparable to plug-and-play regularizers and\nend-to-end trained methods.\n", "link": "http://arxiv.org/abs/2502.03302v1", "date": "2025-02-05", "relevancy": 2.1396, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5562}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5522}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAP%20Image%20Recovery%20with%20Guarantees%20using%20Locally%20Convex%20Multi-Scale%0A%20%20Energy%20%28LC-MUSE%29%20Model&body=Title%3A%20MAP%20Image%20Recovery%20with%20Guarantees%20using%20Locally%20Convex%20Multi-Scale%0A%20%20Energy%20%28LC-MUSE%29%20Model%0AAuthor%3A%20Jyothi%20Rikhab%20Chand%20and%20Mathews%20Jacob%0AAbstract%3A%20%20%20We%20propose%20a%20multi-scale%20deep%20energy%20model%20that%20is%20strongly%20convex%20in%20the%0Alocal%20neighbourhood%20around%20the%20data%20manifold%20to%20represent%20its%20probability%0Adensity%2C%20with%20application%20in%20inverse%20problems.%20In%20particular%2C%20we%20represent%20the%0Anegative%20log-prior%20as%20a%20multi-scale%20energy%20model%20parameterized%20by%20a%0AConvolutional%20Neural%20Network%20%28CNN%29.%20We%20restrict%20the%20gradient%20of%20the%20CNN%20to%20be%0Alocally%20monotone%2C%20which%20constrains%20the%20model%20as%20a%20Locally%20Convex%20Multi-Scale%0AEnergy%20%28LC-MuSE%29.%20We%20use%20the%20learned%20energy%20model%20in%20image-based%20inverse%0Aproblems%2C%20where%20the%20formulation%20offers%20several%20desirable%20properties%3A%20i%29%0Auniqueness%20of%20the%20solution%2C%20ii%29%20convergence%20guarantees%20to%20a%20minimum%20of%20the%0Ainverse%20problem%2C%20and%20iii%29%20robustness%20to%20input%20perturbations.%20In%20the%20context%20of%0Aparallel%20Magnetic%20Resonance%20%28MR%29%20image%20reconstruction%2C%20we%20show%20that%20the%0Aproposed%20method%20performs%20better%20than%20the%20state-of-the-art%20convex%20regularizers%2C%0Awhile%20the%20performance%20is%20comparable%20to%20plug-and-play%20regularizers%20and%0Aend-to-end%20trained%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03302v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAP%2520Image%2520Recovery%2520with%2520Guarantees%2520using%2520Locally%2520Convex%2520Multi-Scale%250A%2520%2520Energy%2520%2528LC-MUSE%2529%2520Model%26entry.906535625%3DJyothi%2520Rikhab%2520Chand%2520and%2520Mathews%2520Jacob%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520multi-scale%2520deep%2520energy%2520model%2520that%2520is%2520strongly%2520convex%2520in%2520the%250Alocal%2520neighbourhood%2520around%2520the%2520data%2520manifold%2520to%2520represent%2520its%2520probability%250Adensity%252C%2520with%2520application%2520in%2520inverse%2520problems.%2520In%2520particular%252C%2520we%2520represent%2520the%250Anegative%2520log-prior%2520as%2520a%2520multi-scale%2520energy%2520model%2520parameterized%2520by%2520a%250AConvolutional%2520Neural%2520Network%2520%2528CNN%2529.%2520We%2520restrict%2520the%2520gradient%2520of%2520the%2520CNN%2520to%2520be%250Alocally%2520monotone%252C%2520which%2520constrains%2520the%2520model%2520as%2520a%2520Locally%2520Convex%2520Multi-Scale%250AEnergy%2520%2528LC-MuSE%2529.%2520We%2520use%2520the%2520learned%2520energy%2520model%2520in%2520image-based%2520inverse%250Aproblems%252C%2520where%2520the%2520formulation%2520offers%2520several%2520desirable%2520properties%253A%2520i%2529%250Auniqueness%2520of%2520the%2520solution%252C%2520ii%2529%2520convergence%2520guarantees%2520to%2520a%2520minimum%2520of%2520the%250Ainverse%2520problem%252C%2520and%2520iii%2529%2520robustness%2520to%2520input%2520perturbations.%2520In%2520the%2520context%2520of%250Aparallel%2520Magnetic%2520Resonance%2520%2528MR%2529%2520image%2520reconstruction%252C%2520we%2520show%2520that%2520the%250Aproposed%2520method%2520performs%2520better%2520than%2520the%2520state-of-the-art%2520convex%2520regularizers%252C%250Awhile%2520the%2520performance%2520is%2520comparable%2520to%2520plug-and-play%2520regularizers%2520and%250Aend-to-end%2520trained%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03302v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAP%20Image%20Recovery%20with%20Guarantees%20using%20Locally%20Convex%20Multi-Scale%0A%20%20Energy%20%28LC-MUSE%29%20Model&entry.906535625=Jyothi%20Rikhab%20Chand%20and%20Mathews%20Jacob&entry.1292438233=%20%20We%20propose%20a%20multi-scale%20deep%20energy%20model%20that%20is%20strongly%20convex%20in%20the%0Alocal%20neighbourhood%20around%20the%20data%20manifold%20to%20represent%20its%20probability%0Adensity%2C%20with%20application%20in%20inverse%20problems.%20In%20particular%2C%20we%20represent%20the%0Anegative%20log-prior%20as%20a%20multi-scale%20energy%20model%20parameterized%20by%20a%0AConvolutional%20Neural%20Network%20%28CNN%29.%20We%20restrict%20the%20gradient%20of%20the%20CNN%20to%20be%0Alocally%20monotone%2C%20which%20constrains%20the%20model%20as%20a%20Locally%20Convex%20Multi-Scale%0AEnergy%20%28LC-MuSE%29.%20We%20use%20the%20learned%20energy%20model%20in%20image-based%20inverse%0Aproblems%2C%20where%20the%20formulation%20offers%20several%20desirable%20properties%3A%20i%29%0Auniqueness%20of%20the%20solution%2C%20ii%29%20convergence%20guarantees%20to%20a%20minimum%20of%20the%0Ainverse%20problem%2C%20and%20iii%29%20robustness%20to%20input%20perturbations.%20In%20the%20context%20of%0Aparallel%20Magnetic%20Resonance%20%28MR%29%20image%20reconstruction%2C%20we%20show%20that%20the%0Aproposed%20method%20performs%20better%20than%20the%20state-of-the-art%20convex%20regularizers%2C%0Awhile%20the%20performance%20is%20comparable%20to%20plug-and-play%20regularizers%20and%0Aend-to-end%20trained%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03302v1&entry.124074799=Read"},
{"title": "Learning with SASQuaTCh: a Novel Variational Quantum Transformer\n  Architecture with Kernel-Based Self-Attention", "author": "Ethan N. Evans and Matthew Cook and Zachary P. Bradshaw and Margarite L. LaBorde", "abstract": "  The recent exploding growth in size of state-of-the-art machine learning\nmodels highlights a well-known issue where exponential parameter growth, which\nhas grown to trillions as in the case of the Generative Pre-trained Transformer\n(GPT), leads to training time and memory requirements which limit their\nadvancement in the near term. The predominant models use the so-called\ntransformer network and have a large field of applicability, including\npredicting text and images, classification, and even predicting solutions to\nthe dynamics of physical systems. Here we present a variational quantum circuit\narchitecture named Self-Attention Sequential Quantum Transformer Channel\n(SASQuaTCh), which builds networks of qubits that perform analogous operations\nof the transformer network, namely the keystone self-attention operation, and\nleads to an exponential improvement in parameter complexity and run-time\ncomplexity over its classical counterpart. Our approach leverages recent\ninsights from kernel-based operator learning in the context of predicting\nspatiotemporal systems to represent deep layers of a vision transformer network\nusing simple gate operations and a set of multi-dimensional quantum Fourier\ntransforms. To validate our approach, we consider image classification tasks in\nsimulation and with hardware, where with only 9 qubits and a handful of\nparameters we are able to simultaneously embed and classify a grayscale image\nof handwritten digits with high accuracy.\n", "link": "http://arxiv.org/abs/2403.14753v2", "date": "2025-02-05", "relevancy": 2.1352, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5735}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.53}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20with%20SASQuaTCh%3A%20a%20Novel%20Variational%20Quantum%20Transformer%0A%20%20Architecture%20with%20Kernel-Based%20Self-Attention&body=Title%3A%20Learning%20with%20SASQuaTCh%3A%20a%20Novel%20Variational%20Quantum%20Transformer%0A%20%20Architecture%20with%20Kernel-Based%20Self-Attention%0AAuthor%3A%20Ethan%20N.%20Evans%20and%20Matthew%20Cook%20and%20Zachary%20P.%20Bradshaw%20and%20Margarite%20L.%20LaBorde%0AAbstract%3A%20%20%20The%20recent%20exploding%20growth%20in%20size%20of%20state-of-the-art%20machine%20learning%0Amodels%20highlights%20a%20well-known%20issue%20where%20exponential%20parameter%20growth%2C%20which%0Ahas%20grown%20to%20trillions%20as%20in%20the%20case%20of%20the%20Generative%20Pre-trained%20Transformer%0A%28GPT%29%2C%20leads%20to%20training%20time%20and%20memory%20requirements%20which%20limit%20their%0Aadvancement%20in%20the%20near%20term.%20The%20predominant%20models%20use%20the%20so-called%0Atransformer%20network%20and%20have%20a%20large%20field%20of%20applicability%2C%20including%0Apredicting%20text%20and%20images%2C%20classification%2C%20and%20even%20predicting%20solutions%20to%0Athe%20dynamics%20of%20physical%20systems.%20Here%20we%20present%20a%20variational%20quantum%20circuit%0Aarchitecture%20named%20Self-Attention%20Sequential%20Quantum%20Transformer%20Channel%0A%28SASQuaTCh%29%2C%20which%20builds%20networks%20of%20qubits%20that%20perform%20analogous%20operations%0Aof%20the%20transformer%20network%2C%20namely%20the%20keystone%20self-attention%20operation%2C%20and%0Aleads%20to%20an%20exponential%20improvement%20in%20parameter%20complexity%20and%20run-time%0Acomplexity%20over%20its%20classical%20counterpart.%20Our%20approach%20leverages%20recent%0Ainsights%20from%20kernel-based%20operator%20learning%20in%20the%20context%20of%20predicting%0Aspatiotemporal%20systems%20to%20represent%20deep%20layers%20of%20a%20vision%20transformer%20network%0Ausing%20simple%20gate%20operations%20and%20a%20set%20of%20multi-dimensional%20quantum%20Fourier%0Atransforms.%20To%20validate%20our%20approach%2C%20we%20consider%20image%20classification%20tasks%20in%0Asimulation%20and%20with%20hardware%2C%20where%20with%20only%209%20qubits%20and%20a%20handful%20of%0Aparameters%20we%20are%20able%20to%20simultaneously%20embed%20and%20classify%20a%20grayscale%20image%0Aof%20handwritten%20digits%20with%20high%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14753v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520with%2520SASQuaTCh%253A%2520a%2520Novel%2520Variational%2520Quantum%2520Transformer%250A%2520%2520Architecture%2520with%2520Kernel-Based%2520Self-Attention%26entry.906535625%3DEthan%2520N.%2520Evans%2520and%2520Matthew%2520Cook%2520and%2520Zachary%2520P.%2520Bradshaw%2520and%2520Margarite%2520L.%2520LaBorde%26entry.1292438233%3D%2520%2520The%2520recent%2520exploding%2520growth%2520in%2520size%2520of%2520state-of-the-art%2520machine%2520learning%250Amodels%2520highlights%2520a%2520well-known%2520issue%2520where%2520exponential%2520parameter%2520growth%252C%2520which%250Ahas%2520grown%2520to%2520trillions%2520as%2520in%2520the%2520case%2520of%2520the%2520Generative%2520Pre-trained%2520Transformer%250A%2528GPT%2529%252C%2520leads%2520to%2520training%2520time%2520and%2520memory%2520requirements%2520which%2520limit%2520their%250Aadvancement%2520in%2520the%2520near%2520term.%2520The%2520predominant%2520models%2520use%2520the%2520so-called%250Atransformer%2520network%2520and%2520have%2520a%2520large%2520field%2520of%2520applicability%252C%2520including%250Apredicting%2520text%2520and%2520images%252C%2520classification%252C%2520and%2520even%2520predicting%2520solutions%2520to%250Athe%2520dynamics%2520of%2520physical%2520systems.%2520Here%2520we%2520present%2520a%2520variational%2520quantum%2520circuit%250Aarchitecture%2520named%2520Self-Attention%2520Sequential%2520Quantum%2520Transformer%2520Channel%250A%2528SASQuaTCh%2529%252C%2520which%2520builds%2520networks%2520of%2520qubits%2520that%2520perform%2520analogous%2520operations%250Aof%2520the%2520transformer%2520network%252C%2520namely%2520the%2520keystone%2520self-attention%2520operation%252C%2520and%250Aleads%2520to%2520an%2520exponential%2520improvement%2520in%2520parameter%2520complexity%2520and%2520run-time%250Acomplexity%2520over%2520its%2520classical%2520counterpart.%2520Our%2520approach%2520leverages%2520recent%250Ainsights%2520from%2520kernel-based%2520operator%2520learning%2520in%2520the%2520context%2520of%2520predicting%250Aspatiotemporal%2520systems%2520to%2520represent%2520deep%2520layers%2520of%2520a%2520vision%2520transformer%2520network%250Ausing%2520simple%2520gate%2520operations%2520and%2520a%2520set%2520of%2520multi-dimensional%2520quantum%2520Fourier%250Atransforms.%2520To%2520validate%2520our%2520approach%252C%2520we%2520consider%2520image%2520classification%2520tasks%2520in%250Asimulation%2520and%2520with%2520hardware%252C%2520where%2520with%2520only%25209%2520qubits%2520and%2520a%2520handful%2520of%250Aparameters%2520we%2520are%2520able%2520to%2520simultaneously%2520embed%2520and%2520classify%2520a%2520grayscale%2520image%250Aof%2520handwritten%2520digits%2520with%2520high%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.14753v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20with%20SASQuaTCh%3A%20a%20Novel%20Variational%20Quantum%20Transformer%0A%20%20Architecture%20with%20Kernel-Based%20Self-Attention&entry.906535625=Ethan%20N.%20Evans%20and%20Matthew%20Cook%20and%20Zachary%20P.%20Bradshaw%20and%20Margarite%20L.%20LaBorde&entry.1292438233=%20%20The%20recent%20exploding%20growth%20in%20size%20of%20state-of-the-art%20machine%20learning%0Amodels%20highlights%20a%20well-known%20issue%20where%20exponential%20parameter%20growth%2C%20which%0Ahas%20grown%20to%20trillions%20as%20in%20the%20case%20of%20the%20Generative%20Pre-trained%20Transformer%0A%28GPT%29%2C%20leads%20to%20training%20time%20and%20memory%20requirements%20which%20limit%20their%0Aadvancement%20in%20the%20near%20term.%20The%20predominant%20models%20use%20the%20so-called%0Atransformer%20network%20and%20have%20a%20large%20field%20of%20applicability%2C%20including%0Apredicting%20text%20and%20images%2C%20classification%2C%20and%20even%20predicting%20solutions%20to%0Athe%20dynamics%20of%20physical%20systems.%20Here%20we%20present%20a%20variational%20quantum%20circuit%0Aarchitecture%20named%20Self-Attention%20Sequential%20Quantum%20Transformer%20Channel%0A%28SASQuaTCh%29%2C%20which%20builds%20networks%20of%20qubits%20that%20perform%20analogous%20operations%0Aof%20the%20transformer%20network%2C%20namely%20the%20keystone%20self-attention%20operation%2C%20and%0Aleads%20to%20an%20exponential%20improvement%20in%20parameter%20complexity%20and%20run-time%0Acomplexity%20over%20its%20classical%20counterpart.%20Our%20approach%20leverages%20recent%0Ainsights%20from%20kernel-based%20operator%20learning%20in%20the%20context%20of%20predicting%0Aspatiotemporal%20systems%20to%20represent%20deep%20layers%20of%20a%20vision%20transformer%20network%0Ausing%20simple%20gate%20operations%20and%20a%20set%20of%20multi-dimensional%20quantum%20Fourier%0Atransforms.%20To%20validate%20our%20approach%2C%20we%20consider%20image%20classification%20tasks%20in%0Asimulation%20and%20with%20hardware%2C%20where%20with%20only%209%20qubits%20and%20a%20handful%20of%0Aparameters%20we%20are%20able%20to%20simultaneously%20embed%20and%20classify%20a%20grayscale%20image%0Aof%20handwritten%20digits%20with%20high%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14753v2&entry.124074799=Read"},
{"title": "Conditional Prediction by Simulation for Automated Driving", "author": "Fabian Konstantinidis and Moritz Sackmann and Ulrich Hofmann and Christoph Stiller", "abstract": "  Modular automated driving systems commonly handle prediction and planning as\nsequential, separate tasks, thereby prohibiting cooperative maneuvers. To\nenable cooperative planning, this work introduces a prediction model that\nmodels the conditional dependencies between trajectories. For this, predictions\nare generated by a microscopic traffic simulation, with the individual traffic\nparticipants being controlled by a realistic behavior model trained via\nAdversarial Inverse Reinforcement Learning. By assuming various candidate\ntrajectories for the automated vehicle, we generate predictions conditioned on\neach of them. Furthermore, our approach allows the candidate trajectories to\nadapt dynamically during the prediction rollout. Several example scenarios are\navailable at https://conditionalpredictionbysimulation.github.io/.\n", "link": "http://arxiv.org/abs/2502.03286v1", "date": "2025-02-05", "relevancy": 2.1321, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.616}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5204}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5125}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conditional%20Prediction%20by%20Simulation%20for%20Automated%20Driving&body=Title%3A%20Conditional%20Prediction%20by%20Simulation%20for%20Automated%20Driving%0AAuthor%3A%20Fabian%20Konstantinidis%20and%20Moritz%20Sackmann%20and%20Ulrich%20Hofmann%20and%20Christoph%20Stiller%0AAbstract%3A%20%20%20Modular%20automated%20driving%20systems%20commonly%20handle%20prediction%20and%20planning%20as%0Asequential%2C%20separate%20tasks%2C%20thereby%20prohibiting%20cooperative%20maneuvers.%20To%0Aenable%20cooperative%20planning%2C%20this%20work%20introduces%20a%20prediction%20model%20that%0Amodels%20the%20conditional%20dependencies%20between%20trajectories.%20For%20this%2C%20predictions%0Aare%20generated%20by%20a%20microscopic%20traffic%20simulation%2C%20with%20the%20individual%20traffic%0Aparticipants%20being%20controlled%20by%20a%20realistic%20behavior%20model%20trained%20via%0AAdversarial%20Inverse%20Reinforcement%20Learning.%20By%20assuming%20various%20candidate%0Atrajectories%20for%20the%20automated%20vehicle%2C%20we%20generate%20predictions%20conditioned%20on%0Aeach%20of%20them.%20Furthermore%2C%20our%20approach%20allows%20the%20candidate%20trajectories%20to%0Aadapt%20dynamically%20during%20the%20prediction%20rollout.%20Several%20example%20scenarios%20are%0Aavailable%20at%20https%3A//conditionalpredictionbysimulation.github.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03286v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConditional%2520Prediction%2520by%2520Simulation%2520for%2520Automated%2520Driving%26entry.906535625%3DFabian%2520Konstantinidis%2520and%2520Moritz%2520Sackmann%2520and%2520Ulrich%2520Hofmann%2520and%2520Christoph%2520Stiller%26entry.1292438233%3D%2520%2520Modular%2520automated%2520driving%2520systems%2520commonly%2520handle%2520prediction%2520and%2520planning%2520as%250Asequential%252C%2520separate%2520tasks%252C%2520thereby%2520prohibiting%2520cooperative%2520maneuvers.%2520To%250Aenable%2520cooperative%2520planning%252C%2520this%2520work%2520introduces%2520a%2520prediction%2520model%2520that%250Amodels%2520the%2520conditional%2520dependencies%2520between%2520trajectories.%2520For%2520this%252C%2520predictions%250Aare%2520generated%2520by%2520a%2520microscopic%2520traffic%2520simulation%252C%2520with%2520the%2520individual%2520traffic%250Aparticipants%2520being%2520controlled%2520by%2520a%2520realistic%2520behavior%2520model%2520trained%2520via%250AAdversarial%2520Inverse%2520Reinforcement%2520Learning.%2520By%2520assuming%2520various%2520candidate%250Atrajectories%2520for%2520the%2520automated%2520vehicle%252C%2520we%2520generate%2520predictions%2520conditioned%2520on%250Aeach%2520of%2520them.%2520Furthermore%252C%2520our%2520approach%2520allows%2520the%2520candidate%2520trajectories%2520to%250Aadapt%2520dynamically%2520during%2520the%2520prediction%2520rollout.%2520Several%2520example%2520scenarios%2520are%250Aavailable%2520at%2520https%253A//conditionalpredictionbysimulation.github.io/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03286v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conditional%20Prediction%20by%20Simulation%20for%20Automated%20Driving&entry.906535625=Fabian%20Konstantinidis%20and%20Moritz%20Sackmann%20and%20Ulrich%20Hofmann%20and%20Christoph%20Stiller&entry.1292438233=%20%20Modular%20automated%20driving%20systems%20commonly%20handle%20prediction%20and%20planning%20as%0Asequential%2C%20separate%20tasks%2C%20thereby%20prohibiting%20cooperative%20maneuvers.%20To%0Aenable%20cooperative%20planning%2C%20this%20work%20introduces%20a%20prediction%20model%20that%0Amodels%20the%20conditional%20dependencies%20between%20trajectories.%20For%20this%2C%20predictions%0Aare%20generated%20by%20a%20microscopic%20traffic%20simulation%2C%20with%20the%20individual%20traffic%0Aparticipants%20being%20controlled%20by%20a%20realistic%20behavior%20model%20trained%20via%0AAdversarial%20Inverse%20Reinforcement%20Learning.%20By%20assuming%20various%20candidate%0Atrajectories%20for%20the%20automated%20vehicle%2C%20we%20generate%20predictions%20conditioned%20on%0Aeach%20of%20them.%20Furthermore%2C%20our%20approach%20allows%20the%20candidate%20trajectories%20to%0Aadapt%20dynamically%20during%20the%20prediction%20rollout.%20Several%20example%20scenarios%20are%0Aavailable%20at%20https%3A//conditionalpredictionbysimulation.github.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03286v1&entry.124074799=Read"},
{"title": "A User's Guide to Sampling Strategies for Sliced Optimal Transport", "author": "Keanu Sisouk and Julie Delon and Julien Tierny", "abstract": "  This paper serves as a user's guide to sampling strategies for sliced optimal\ntransport. We provide reminders and additional regularity results on the Sliced\nWasserstein distance. We detail the construction methods, generation time\ncomplexity, theoretical guarantees, and conditions for each strategy.\nAdditionally, we provide insights into their suitability for sliced optimal\ntransport in theory. Extensive experiments on both simulated and real-world\ndata offer a representative comparison of the strategies, culminating in\npractical recommendations for their best usage.\n", "link": "http://arxiv.org/abs/2502.02275v2", "date": "2025-02-05", "relevancy": 2.1314, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.443}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4226}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20User%27s%20Guide%20to%20Sampling%20Strategies%20for%20Sliced%20Optimal%20Transport&body=Title%3A%20A%20User%27s%20Guide%20to%20Sampling%20Strategies%20for%20Sliced%20Optimal%20Transport%0AAuthor%3A%20Keanu%20Sisouk%20and%20Julie%20Delon%20and%20Julien%20Tierny%0AAbstract%3A%20%20%20This%20paper%20serves%20as%20a%20user%27s%20guide%20to%20sampling%20strategies%20for%20sliced%20optimal%0Atransport.%20We%20provide%20reminders%20and%20additional%20regularity%20results%20on%20the%20Sliced%0AWasserstein%20distance.%20We%20detail%20the%20construction%20methods%2C%20generation%20time%0Acomplexity%2C%20theoretical%20guarantees%2C%20and%20conditions%20for%20each%20strategy.%0AAdditionally%2C%20we%20provide%20insights%20into%20their%20suitability%20for%20sliced%20optimal%0Atransport%20in%20theory.%20Extensive%20experiments%20on%20both%20simulated%20and%20real-world%0Adata%20offer%20a%20representative%20comparison%20of%20the%20strategies%2C%20culminating%20in%0Apractical%20recommendations%20for%20their%20best%20usage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02275v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520User%2527s%2520Guide%2520to%2520Sampling%2520Strategies%2520for%2520Sliced%2520Optimal%2520Transport%26entry.906535625%3DKeanu%2520Sisouk%2520and%2520Julie%2520Delon%2520and%2520Julien%2520Tierny%26entry.1292438233%3D%2520%2520This%2520paper%2520serves%2520as%2520a%2520user%2527s%2520guide%2520to%2520sampling%2520strategies%2520for%2520sliced%2520optimal%250Atransport.%2520We%2520provide%2520reminders%2520and%2520additional%2520regularity%2520results%2520on%2520the%2520Sliced%250AWasserstein%2520distance.%2520We%2520detail%2520the%2520construction%2520methods%252C%2520generation%2520time%250Acomplexity%252C%2520theoretical%2520guarantees%252C%2520and%2520conditions%2520for%2520each%2520strategy.%250AAdditionally%252C%2520we%2520provide%2520insights%2520into%2520their%2520suitability%2520for%2520sliced%2520optimal%250Atransport%2520in%2520theory.%2520Extensive%2520experiments%2520on%2520both%2520simulated%2520and%2520real-world%250Adata%2520offer%2520a%2520representative%2520comparison%2520of%2520the%2520strategies%252C%2520culminating%2520in%250Apractical%2520recommendations%2520for%2520their%2520best%2520usage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02275v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20User%27s%20Guide%20to%20Sampling%20Strategies%20for%20Sliced%20Optimal%20Transport&entry.906535625=Keanu%20Sisouk%20and%20Julie%20Delon%20and%20Julien%20Tierny&entry.1292438233=%20%20This%20paper%20serves%20as%20a%20user%27s%20guide%20to%20sampling%20strategies%20for%20sliced%20optimal%0Atransport.%20We%20provide%20reminders%20and%20additional%20regularity%20results%20on%20the%20Sliced%0AWasserstein%20distance.%20We%20detail%20the%20construction%20methods%2C%20generation%20time%0Acomplexity%2C%20theoretical%20guarantees%2C%20and%20conditions%20for%20each%20strategy.%0AAdditionally%2C%20we%20provide%20insights%20into%20their%20suitability%20for%20sliced%20optimal%0Atransport%20in%20theory.%20Extensive%20experiments%20on%20both%20simulated%20and%20real-world%0Adata%20offer%20a%20representative%20comparison%20of%20the%20strategies%2C%20culminating%20in%0Apractical%20recommendations%20for%20their%20best%20usage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02275v2&entry.124074799=Read"},
{"title": "Learnable Expansion of Graph Operators for Multi-Modal Feature Fusion", "author": "Dexuan Ding and Lei Wang and Liyun Zhu and Tom Gedeon and Piotr Koniusz", "abstract": "  In computer vision tasks, features often come from diverse representations,\ndomains (e.g., indoor and outdoor), and modalities (e.g., text, images, and\nvideos). Effectively fusing these features is essential for robust performance,\nespecially with the availability of powerful pre-trained models like\nvision-language models. However, common fusion methods, such as concatenation,\nelement-wise operations, and non-linear techniques, often fail to capture\nstructural relationships, deep feature interactions, and suffer from\ninefficiency or misalignment of features across domains or modalities. In this\npaper, we shift from high-dimensional feature space to a lower-dimensional,\ninterpretable graph space by constructing relationship graphs that encode\nfeature relationships at different levels, e.g., clip, frame, patch, token,\netc. To capture deeper interactions, we use graph power expansions and\nintroduce a learnable graph fusion operator to combine these graph powers for\nmore effective fusion. Our approach is relationship-centric, operates in a\nhomogeneous space, and is mathematically principled, resembling element-wise\nrelationship score aggregation via multilinear polynomials. We demonstrate the\neffectiveness of our graph-based fusion method on video anomaly detection,\nshowing strong performance across multi-representational, multi-modal, and\nmulti-domain feature fusion tasks.\n", "link": "http://arxiv.org/abs/2410.01506v3", "date": "2025-02-05", "relevancy": 2.1262, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5423}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5273}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learnable%20Expansion%20of%20Graph%20Operators%20for%20Multi-Modal%20Feature%20Fusion&body=Title%3A%20Learnable%20Expansion%20of%20Graph%20Operators%20for%20Multi-Modal%20Feature%20Fusion%0AAuthor%3A%20Dexuan%20Ding%20and%20Lei%20Wang%20and%20Liyun%20Zhu%20and%20Tom%20Gedeon%20and%20Piotr%20Koniusz%0AAbstract%3A%20%20%20In%20computer%20vision%20tasks%2C%20features%20often%20come%20from%20diverse%20representations%2C%0Adomains%20%28e.g.%2C%20indoor%20and%20outdoor%29%2C%20and%20modalities%20%28e.g.%2C%20text%2C%20images%2C%20and%0Avideos%29.%20Effectively%20fusing%20these%20features%20is%20essential%20for%20robust%20performance%2C%0Aespecially%20with%20the%20availability%20of%20powerful%20pre-trained%20models%20like%0Avision-language%20models.%20However%2C%20common%20fusion%20methods%2C%20such%20as%20concatenation%2C%0Aelement-wise%20operations%2C%20and%20non-linear%20techniques%2C%20often%20fail%20to%20capture%0Astructural%20relationships%2C%20deep%20feature%20interactions%2C%20and%20suffer%20from%0Ainefficiency%20or%20misalignment%20of%20features%20across%20domains%20or%20modalities.%20In%20this%0Apaper%2C%20we%20shift%20from%20high-dimensional%20feature%20space%20to%20a%20lower-dimensional%2C%0Ainterpretable%20graph%20space%20by%20constructing%20relationship%20graphs%20that%20encode%0Afeature%20relationships%20at%20different%20levels%2C%20e.g.%2C%20clip%2C%20frame%2C%20patch%2C%20token%2C%0Aetc.%20To%20capture%20deeper%20interactions%2C%20we%20use%20graph%20power%20expansions%20and%0Aintroduce%20a%20learnable%20graph%20fusion%20operator%20to%20combine%20these%20graph%20powers%20for%0Amore%20effective%20fusion.%20Our%20approach%20is%20relationship-centric%2C%20operates%20in%20a%0Ahomogeneous%20space%2C%20and%20is%20mathematically%20principled%2C%20resembling%20element-wise%0Arelationship%20score%20aggregation%20via%20multilinear%20polynomials.%20We%20demonstrate%20the%0Aeffectiveness%20of%20our%20graph-based%20fusion%20method%20on%20video%20anomaly%20detection%2C%0Ashowing%20strong%20performance%20across%20multi-representational%2C%20multi-modal%2C%20and%0Amulti-domain%20feature%20fusion%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01506v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearnable%2520Expansion%2520of%2520Graph%2520Operators%2520for%2520Multi-Modal%2520Feature%2520Fusion%26entry.906535625%3DDexuan%2520Ding%2520and%2520Lei%2520Wang%2520and%2520Liyun%2520Zhu%2520and%2520Tom%2520Gedeon%2520and%2520Piotr%2520Koniusz%26entry.1292438233%3D%2520%2520In%2520computer%2520vision%2520tasks%252C%2520features%2520often%2520come%2520from%2520diverse%2520representations%252C%250Adomains%2520%2528e.g.%252C%2520indoor%2520and%2520outdoor%2529%252C%2520and%2520modalities%2520%2528e.g.%252C%2520text%252C%2520images%252C%2520and%250Avideos%2529.%2520Effectively%2520fusing%2520these%2520features%2520is%2520essential%2520for%2520robust%2520performance%252C%250Aespecially%2520with%2520the%2520availability%2520of%2520powerful%2520pre-trained%2520models%2520like%250Avision-language%2520models.%2520However%252C%2520common%2520fusion%2520methods%252C%2520such%2520as%2520concatenation%252C%250Aelement-wise%2520operations%252C%2520and%2520non-linear%2520techniques%252C%2520often%2520fail%2520to%2520capture%250Astructural%2520relationships%252C%2520deep%2520feature%2520interactions%252C%2520and%2520suffer%2520from%250Ainefficiency%2520or%2520misalignment%2520of%2520features%2520across%2520domains%2520or%2520modalities.%2520In%2520this%250Apaper%252C%2520we%2520shift%2520from%2520high-dimensional%2520feature%2520space%2520to%2520a%2520lower-dimensional%252C%250Ainterpretable%2520graph%2520space%2520by%2520constructing%2520relationship%2520graphs%2520that%2520encode%250Afeature%2520relationships%2520at%2520different%2520levels%252C%2520e.g.%252C%2520clip%252C%2520frame%252C%2520patch%252C%2520token%252C%250Aetc.%2520To%2520capture%2520deeper%2520interactions%252C%2520we%2520use%2520graph%2520power%2520expansions%2520and%250Aintroduce%2520a%2520learnable%2520graph%2520fusion%2520operator%2520to%2520combine%2520these%2520graph%2520powers%2520for%250Amore%2520effective%2520fusion.%2520Our%2520approach%2520is%2520relationship-centric%252C%2520operates%2520in%2520a%250Ahomogeneous%2520space%252C%2520and%2520is%2520mathematically%2520principled%252C%2520resembling%2520element-wise%250Arelationship%2520score%2520aggregation%2520via%2520multilinear%2520polynomials.%2520We%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520graph-based%2520fusion%2520method%2520on%2520video%2520anomaly%2520detection%252C%250Ashowing%2520strong%2520performance%2520across%2520multi-representational%252C%2520multi-modal%252C%2520and%250Amulti-domain%2520feature%2520fusion%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01506v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learnable%20Expansion%20of%20Graph%20Operators%20for%20Multi-Modal%20Feature%20Fusion&entry.906535625=Dexuan%20Ding%20and%20Lei%20Wang%20and%20Liyun%20Zhu%20and%20Tom%20Gedeon%20and%20Piotr%20Koniusz&entry.1292438233=%20%20In%20computer%20vision%20tasks%2C%20features%20often%20come%20from%20diverse%20representations%2C%0Adomains%20%28e.g.%2C%20indoor%20and%20outdoor%29%2C%20and%20modalities%20%28e.g.%2C%20text%2C%20images%2C%20and%0Avideos%29.%20Effectively%20fusing%20these%20features%20is%20essential%20for%20robust%20performance%2C%0Aespecially%20with%20the%20availability%20of%20powerful%20pre-trained%20models%20like%0Avision-language%20models.%20However%2C%20common%20fusion%20methods%2C%20such%20as%20concatenation%2C%0Aelement-wise%20operations%2C%20and%20non-linear%20techniques%2C%20often%20fail%20to%20capture%0Astructural%20relationships%2C%20deep%20feature%20interactions%2C%20and%20suffer%20from%0Ainefficiency%20or%20misalignment%20of%20features%20across%20domains%20or%20modalities.%20In%20this%0Apaper%2C%20we%20shift%20from%20high-dimensional%20feature%20space%20to%20a%20lower-dimensional%2C%0Ainterpretable%20graph%20space%20by%20constructing%20relationship%20graphs%20that%20encode%0Afeature%20relationships%20at%20different%20levels%2C%20e.g.%2C%20clip%2C%20frame%2C%20patch%2C%20token%2C%0Aetc.%20To%20capture%20deeper%20interactions%2C%20we%20use%20graph%20power%20expansions%20and%0Aintroduce%20a%20learnable%20graph%20fusion%20operator%20to%20combine%20these%20graph%20powers%20for%0Amore%20effective%20fusion.%20Our%20approach%20is%20relationship-centric%2C%20operates%20in%20a%0Ahomogeneous%20space%2C%20and%20is%20mathematically%20principled%2C%20resembling%20element-wise%0Arelationship%20score%20aggregation%20via%20multilinear%20polynomials.%20We%20demonstrate%20the%0Aeffectiveness%20of%20our%20graph-based%20fusion%20method%20on%20video%20anomaly%20detection%2C%0Ashowing%20strong%20performance%20across%20multi-representational%2C%20multi-modal%2C%20and%0Amulti-domain%20feature%20fusion%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01506v3&entry.124074799=Read"},
{"title": "A Unified Framework for Semi-Supervised Image Segmentation and\n  Registration", "author": "Ruizhe Li and Grazziela Figueredo and Dorothee Auer and Rob Dineen and Paul Morgan and Xin Chen", "abstract": "  Semi-supervised learning, which leverages both annotated and unannotated\ndata, is an efficient approach for medical image segmentation, where obtaining\nannotations for the whole dataset is time-consuming and costly. Traditional\nsemi-supervised methods primarily focus on extracting features and learning\ndata distributions from unannotated data to enhance model training. In this\npaper, we introduce a novel approach incorporating an image registration model\nto generate pseudo-labels for the unannotated data, producing more\ngeometrically correct pseudo-labels to improve the model training. Our method\nwas evaluated on a 2D brain data set, showing excellent performance even using\nonly 1\\% of the annotated data. The results show that our approach outperforms\nconventional semi-supervised segmentation methods (e.g. teacher-student model),\nparticularly in a low percentage of annotation scenario. GitHub:\nhttps://github.com/ruizhe-l/UniSegReg.\n", "link": "http://arxiv.org/abs/2502.03229v1", "date": "2025-02-05", "relevancy": 2.1128, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5546}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5341}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Framework%20for%20Semi-Supervised%20Image%20Segmentation%20and%0A%20%20Registration&body=Title%3A%20A%20Unified%20Framework%20for%20Semi-Supervised%20Image%20Segmentation%20and%0A%20%20Registration%0AAuthor%3A%20Ruizhe%20Li%20and%20Grazziela%20Figueredo%20and%20Dorothee%20Auer%20and%20Rob%20Dineen%20and%20Paul%20Morgan%20and%20Xin%20Chen%0AAbstract%3A%20%20%20Semi-supervised%20learning%2C%20which%20leverages%20both%20annotated%20and%20unannotated%0Adata%2C%20is%20an%20efficient%20approach%20for%20medical%20image%20segmentation%2C%20where%20obtaining%0Aannotations%20for%20the%20whole%20dataset%20is%20time-consuming%20and%20costly.%20Traditional%0Asemi-supervised%20methods%20primarily%20focus%20on%20extracting%20features%20and%20learning%0Adata%20distributions%20from%20unannotated%20data%20to%20enhance%20model%20training.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20approach%20incorporating%20an%20image%20registration%20model%0Ato%20generate%20pseudo-labels%20for%20the%20unannotated%20data%2C%20producing%20more%0Ageometrically%20correct%20pseudo-labels%20to%20improve%20the%20model%20training.%20Our%20method%0Awas%20evaluated%20on%20a%202D%20brain%20data%20set%2C%20showing%20excellent%20performance%20even%20using%0Aonly%201%5C%25%20of%20the%20annotated%20data.%20The%20results%20show%20that%20our%20approach%20outperforms%0Aconventional%20semi-supervised%20segmentation%20methods%20%28e.g.%20teacher-student%20model%29%2C%0Aparticularly%20in%20a%20low%20percentage%20of%20annotation%20scenario.%20GitHub%3A%0Ahttps%3A//github.com/ruizhe-l/UniSegReg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03229v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Framework%2520for%2520Semi-Supervised%2520Image%2520Segmentation%2520and%250A%2520%2520Registration%26entry.906535625%3DRuizhe%2520Li%2520and%2520Grazziela%2520Figueredo%2520and%2520Dorothee%2520Auer%2520and%2520Rob%2520Dineen%2520and%2520Paul%2520Morgan%2520and%2520Xin%2520Chen%26entry.1292438233%3D%2520%2520Semi-supervised%2520learning%252C%2520which%2520leverages%2520both%2520annotated%2520and%2520unannotated%250Adata%252C%2520is%2520an%2520efficient%2520approach%2520for%2520medical%2520image%2520segmentation%252C%2520where%2520obtaining%250Aannotations%2520for%2520the%2520whole%2520dataset%2520is%2520time-consuming%2520and%2520costly.%2520Traditional%250Asemi-supervised%2520methods%2520primarily%2520focus%2520on%2520extracting%2520features%2520and%2520learning%250Adata%2520distributions%2520from%2520unannotated%2520data%2520to%2520enhance%2520model%2520training.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520a%2520novel%2520approach%2520incorporating%2520an%2520image%2520registration%2520model%250Ato%2520generate%2520pseudo-labels%2520for%2520the%2520unannotated%2520data%252C%2520producing%2520more%250Ageometrically%2520correct%2520pseudo-labels%2520to%2520improve%2520the%2520model%2520training.%2520Our%2520method%250Awas%2520evaluated%2520on%2520a%25202D%2520brain%2520data%2520set%252C%2520showing%2520excellent%2520performance%2520even%2520using%250Aonly%25201%255C%2525%2520of%2520the%2520annotated%2520data.%2520The%2520results%2520show%2520that%2520our%2520approach%2520outperforms%250Aconventional%2520semi-supervised%2520segmentation%2520methods%2520%2528e.g.%2520teacher-student%2520model%2529%252C%250Aparticularly%2520in%2520a%2520low%2520percentage%2520of%2520annotation%2520scenario.%2520GitHub%253A%250Ahttps%253A//github.com/ruizhe-l/UniSegReg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03229v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Framework%20for%20Semi-Supervised%20Image%20Segmentation%20and%0A%20%20Registration&entry.906535625=Ruizhe%20Li%20and%20Grazziela%20Figueredo%20and%20Dorothee%20Auer%20and%20Rob%20Dineen%20and%20Paul%20Morgan%20and%20Xin%20Chen&entry.1292438233=%20%20Semi-supervised%20learning%2C%20which%20leverages%20both%20annotated%20and%20unannotated%0Adata%2C%20is%20an%20efficient%20approach%20for%20medical%20image%20segmentation%2C%20where%20obtaining%0Aannotations%20for%20the%20whole%20dataset%20is%20time-consuming%20and%20costly.%20Traditional%0Asemi-supervised%20methods%20primarily%20focus%20on%20extracting%20features%20and%20learning%0Adata%20distributions%20from%20unannotated%20data%20to%20enhance%20model%20training.%20In%20this%0Apaper%2C%20we%20introduce%20a%20novel%20approach%20incorporating%20an%20image%20registration%20model%0Ato%20generate%20pseudo-labels%20for%20the%20unannotated%20data%2C%20producing%20more%0Ageometrically%20correct%20pseudo-labels%20to%20improve%20the%20model%20training.%20Our%20method%0Awas%20evaluated%20on%20a%202D%20brain%20data%20set%2C%20showing%20excellent%20performance%20even%20using%0Aonly%201%5C%25%20of%20the%20annotated%20data.%20The%20results%20show%20that%20our%20approach%20outperforms%0Aconventional%20semi-supervised%20segmentation%20methods%20%28e.g.%20teacher-student%20model%29%2C%0Aparticularly%20in%20a%20low%20percentage%20of%20annotation%20scenario.%20GitHub%3A%0Ahttps%3A//github.com/ruizhe-l/UniSegReg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03229v1&entry.124074799=Read"},
{"title": "GHOST: Gaussian Hypothesis Open-Set Technique", "author": "Ryan Rabinowitz and Steve Cruz and Manuel G\u00fcnther and Terrance E. Boult", "abstract": "  Evaluations of large-scale recognition methods typically focus on overall\nperformance. While this approach is common, it often fails to provide insights\ninto performance across individual classes, which can lead to fairness issues\nand misrepresentation. Addressing these gaps is crucial for accurately\nassessing how well methods handle novel or unseen classes and ensuring a fair\nevaluation. To address fairness in Open-Set Recognition (OSR), we demonstrate\nthat per-class performance can vary dramatically. We introduce Gaussian\nHypothesis Open Set Technique (GHOST), a novel hyperparameter-free algorithm\nthat models deep features using class-wise multivariate Gaussian distributions\nwith diagonal covariance matrices. We apply Z-score normalization to logits to\nmitigate the impact of feature magnitudes that deviate from the model's\nexpectations, thereby reducing the likelihood of the network assigning a high\nscore to an unknown sample. We evaluate GHOST across multiple ImageNet-1K\npre-trained deep networks and test it with four different unknown datasets.\nUsing standard metrics such as AUOSCR, AUROC and FPR95, we achieve\nstatistically significant improvements, advancing the state-of-the-art in\nlarge-scale OSR. Source code is provided online.\n", "link": "http://arxiv.org/abs/2502.03359v1", "date": "2025-02-05", "relevancy": 2.1125, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5317}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5282}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GHOST%3A%20Gaussian%20Hypothesis%20Open-Set%20Technique&body=Title%3A%20GHOST%3A%20Gaussian%20Hypothesis%20Open-Set%20Technique%0AAuthor%3A%20Ryan%20Rabinowitz%20and%20Steve%20Cruz%20and%20Manuel%20G%C3%BCnther%20and%20Terrance%20E.%20Boult%0AAbstract%3A%20%20%20Evaluations%20of%20large-scale%20recognition%20methods%20typically%20focus%20on%20overall%0Aperformance.%20While%20this%20approach%20is%20common%2C%20it%20often%20fails%20to%20provide%20insights%0Ainto%20performance%20across%20individual%20classes%2C%20which%20can%20lead%20to%20fairness%20issues%0Aand%20misrepresentation.%20Addressing%20these%20gaps%20is%20crucial%20for%20accurately%0Aassessing%20how%20well%20methods%20handle%20novel%20or%20unseen%20classes%20and%20ensuring%20a%20fair%0Aevaluation.%20To%20address%20fairness%20in%20Open-Set%20Recognition%20%28OSR%29%2C%20we%20demonstrate%0Athat%20per-class%20performance%20can%20vary%20dramatically.%20We%20introduce%20Gaussian%0AHypothesis%20Open%20Set%20Technique%20%28GHOST%29%2C%20a%20novel%20hyperparameter-free%20algorithm%0Athat%20models%20deep%20features%20using%20class-wise%20multivariate%20Gaussian%20distributions%0Awith%20diagonal%20covariance%20matrices.%20We%20apply%20Z-score%20normalization%20to%20logits%20to%0Amitigate%20the%20impact%20of%20feature%20magnitudes%20that%20deviate%20from%20the%20model%27s%0Aexpectations%2C%20thereby%20reducing%20the%20likelihood%20of%20the%20network%20assigning%20a%20high%0Ascore%20to%20an%20unknown%20sample.%20We%20evaluate%20GHOST%20across%20multiple%20ImageNet-1K%0Apre-trained%20deep%20networks%20and%20test%20it%20with%20four%20different%20unknown%20datasets.%0AUsing%20standard%20metrics%20such%20as%20AUOSCR%2C%20AUROC%20and%20FPR95%2C%20we%20achieve%0Astatistically%20significant%20improvements%2C%20advancing%20the%20state-of-the-art%20in%0Alarge-scale%20OSR.%20Source%20code%20is%20provided%20online.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03359v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGHOST%253A%2520Gaussian%2520Hypothesis%2520Open-Set%2520Technique%26entry.906535625%3DRyan%2520Rabinowitz%2520and%2520Steve%2520Cruz%2520and%2520Manuel%2520G%25C3%25BCnther%2520and%2520Terrance%2520E.%2520Boult%26entry.1292438233%3D%2520%2520Evaluations%2520of%2520large-scale%2520recognition%2520methods%2520typically%2520focus%2520on%2520overall%250Aperformance.%2520While%2520this%2520approach%2520is%2520common%252C%2520it%2520often%2520fails%2520to%2520provide%2520insights%250Ainto%2520performance%2520across%2520individual%2520classes%252C%2520which%2520can%2520lead%2520to%2520fairness%2520issues%250Aand%2520misrepresentation.%2520Addressing%2520these%2520gaps%2520is%2520crucial%2520for%2520accurately%250Aassessing%2520how%2520well%2520methods%2520handle%2520novel%2520or%2520unseen%2520classes%2520and%2520ensuring%2520a%2520fair%250Aevaluation.%2520To%2520address%2520fairness%2520in%2520Open-Set%2520Recognition%2520%2528OSR%2529%252C%2520we%2520demonstrate%250Athat%2520per-class%2520performance%2520can%2520vary%2520dramatically.%2520We%2520introduce%2520Gaussian%250AHypothesis%2520Open%2520Set%2520Technique%2520%2528GHOST%2529%252C%2520a%2520novel%2520hyperparameter-free%2520algorithm%250Athat%2520models%2520deep%2520features%2520using%2520class-wise%2520multivariate%2520Gaussian%2520distributions%250Awith%2520diagonal%2520covariance%2520matrices.%2520We%2520apply%2520Z-score%2520normalization%2520to%2520logits%2520to%250Amitigate%2520the%2520impact%2520of%2520feature%2520magnitudes%2520that%2520deviate%2520from%2520the%2520model%2527s%250Aexpectations%252C%2520thereby%2520reducing%2520the%2520likelihood%2520of%2520the%2520network%2520assigning%2520a%2520high%250Ascore%2520to%2520an%2520unknown%2520sample.%2520We%2520evaluate%2520GHOST%2520across%2520multiple%2520ImageNet-1K%250Apre-trained%2520deep%2520networks%2520and%2520test%2520it%2520with%2520four%2520different%2520unknown%2520datasets.%250AUsing%2520standard%2520metrics%2520such%2520as%2520AUOSCR%252C%2520AUROC%2520and%2520FPR95%252C%2520we%2520achieve%250Astatistically%2520significant%2520improvements%252C%2520advancing%2520the%2520state-of-the-art%2520in%250Alarge-scale%2520OSR.%2520Source%2520code%2520is%2520provided%2520online.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03359v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GHOST%3A%20Gaussian%20Hypothesis%20Open-Set%20Technique&entry.906535625=Ryan%20Rabinowitz%20and%20Steve%20Cruz%20and%20Manuel%20G%C3%BCnther%20and%20Terrance%20E.%20Boult&entry.1292438233=%20%20Evaluations%20of%20large-scale%20recognition%20methods%20typically%20focus%20on%20overall%0Aperformance.%20While%20this%20approach%20is%20common%2C%20it%20often%20fails%20to%20provide%20insights%0Ainto%20performance%20across%20individual%20classes%2C%20which%20can%20lead%20to%20fairness%20issues%0Aand%20misrepresentation.%20Addressing%20these%20gaps%20is%20crucial%20for%20accurately%0Aassessing%20how%20well%20methods%20handle%20novel%20or%20unseen%20classes%20and%20ensuring%20a%20fair%0Aevaluation.%20To%20address%20fairness%20in%20Open-Set%20Recognition%20%28OSR%29%2C%20we%20demonstrate%0Athat%20per-class%20performance%20can%20vary%20dramatically.%20We%20introduce%20Gaussian%0AHypothesis%20Open%20Set%20Technique%20%28GHOST%29%2C%20a%20novel%20hyperparameter-free%20algorithm%0Athat%20models%20deep%20features%20using%20class-wise%20multivariate%20Gaussian%20distributions%0Awith%20diagonal%20covariance%20matrices.%20We%20apply%20Z-score%20normalization%20to%20logits%20to%0Amitigate%20the%20impact%20of%20feature%20magnitudes%20that%20deviate%20from%20the%20model%27s%0Aexpectations%2C%20thereby%20reducing%20the%20likelihood%20of%20the%20network%20assigning%20a%20high%0Ascore%20to%20an%20unknown%20sample.%20We%20evaluate%20GHOST%20across%20multiple%20ImageNet-1K%0Apre-trained%20deep%20networks%20and%20test%20it%20with%20four%20different%20unknown%20datasets.%0AUsing%20standard%20metrics%20such%20as%20AUOSCR%2C%20AUROC%20and%20FPR95%2C%20we%20achieve%0Astatistically%20significant%20improvements%2C%20advancing%20the%20state-of-the-art%20in%0Alarge-scale%20OSR.%20Source%20code%20is%20provided%20online.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03359v1&entry.124074799=Read"},
{"title": "Scaling laws in wearable human activity recognition", "author": "Tom Hoddes and Alex Bijamov and Saket Joshi and Daniel Roggen and Ali Etemad and Robert Harle and David Racz", "abstract": "  Many deep architectures and self-supervised pre-training techniques have been\nproposed for human activity recognition (HAR) from wearable multimodal sensors.\nScaling laws have the potential to help move towards more principled design by\nlinking model capacity with pre-training data volume. Yet, scaling laws have\nnot been established for HAR to the same extent as in language and vision. By\nconducting an exhaustive grid search on both amount of pre-training data and\nTransformer architectures, we establish the first known scaling laws for HAR.\nWe show that pre-training loss scales with a power law relationship to amount\nof data and parameter count and that increasing the number of users in a\ndataset results in a steeper improvement in performance than increasing data\nper user, indicating that diversity of pre-training data is important, which\ncontrasts to some previously reported findings in self-supervised HAR. We show\nthat these scaling laws translate to downstream performance improvements on\nthree HAR benchmark datasets of postures, modes of locomotion and activities of\ndaily living: UCI HAR and WISDM Phone and WISDM Watch. Finally, we suggest some\npreviously published works should be revisited in light of these scaling laws\nwith more adequate model capacities.\n", "link": "http://arxiv.org/abs/2502.03364v1", "date": "2025-02-05", "relevancy": 2.0868, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5599}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5366}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20laws%20in%20wearable%20human%20activity%20recognition&body=Title%3A%20Scaling%20laws%20in%20wearable%20human%20activity%20recognition%0AAuthor%3A%20Tom%20Hoddes%20and%20Alex%20Bijamov%20and%20Saket%20Joshi%20and%20Daniel%20Roggen%20and%20Ali%20Etemad%20and%20Robert%20Harle%20and%20David%20Racz%0AAbstract%3A%20%20%20Many%20deep%20architectures%20and%20self-supervised%20pre-training%20techniques%20have%20been%0Aproposed%20for%20human%20activity%20recognition%20%28HAR%29%20from%20wearable%20multimodal%20sensors.%0AScaling%20laws%20have%20the%20potential%20to%20help%20move%20towards%20more%20principled%20design%20by%0Alinking%20model%20capacity%20with%20pre-training%20data%20volume.%20Yet%2C%20scaling%20laws%20have%0Anot%20been%20established%20for%20HAR%20to%20the%20same%20extent%20as%20in%20language%20and%20vision.%20By%0Aconducting%20an%20exhaustive%20grid%20search%20on%20both%20amount%20of%20pre-training%20data%20and%0ATransformer%20architectures%2C%20we%20establish%20the%20first%20known%20scaling%20laws%20for%20HAR.%0AWe%20show%20that%20pre-training%20loss%20scales%20with%20a%20power%20law%20relationship%20to%20amount%0Aof%20data%20and%20parameter%20count%20and%20that%20increasing%20the%20number%20of%20users%20in%20a%0Adataset%20results%20in%20a%20steeper%20improvement%20in%20performance%20than%20increasing%20data%0Aper%20user%2C%20indicating%20that%20diversity%20of%20pre-training%20data%20is%20important%2C%20which%0Acontrasts%20to%20some%20previously%20reported%20findings%20in%20self-supervised%20HAR.%20We%20show%0Athat%20these%20scaling%20laws%20translate%20to%20downstream%20performance%20improvements%20on%0Athree%20HAR%20benchmark%20datasets%20of%20postures%2C%20modes%20of%20locomotion%20and%20activities%20of%0Adaily%20living%3A%20UCI%20HAR%20and%20WISDM%20Phone%20and%20WISDM%20Watch.%20Finally%2C%20we%20suggest%20some%0Apreviously%20published%20works%20should%20be%20revisited%20in%20light%20of%20these%20scaling%20laws%0Awith%20more%20adequate%20model%20capacities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03364v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520laws%2520in%2520wearable%2520human%2520activity%2520recognition%26entry.906535625%3DTom%2520Hoddes%2520and%2520Alex%2520Bijamov%2520and%2520Saket%2520Joshi%2520and%2520Daniel%2520Roggen%2520and%2520Ali%2520Etemad%2520and%2520Robert%2520Harle%2520and%2520David%2520Racz%26entry.1292438233%3D%2520%2520Many%2520deep%2520architectures%2520and%2520self-supervised%2520pre-training%2520techniques%2520have%2520been%250Aproposed%2520for%2520human%2520activity%2520recognition%2520%2528HAR%2529%2520from%2520wearable%2520multimodal%2520sensors.%250AScaling%2520laws%2520have%2520the%2520potential%2520to%2520help%2520move%2520towards%2520more%2520principled%2520design%2520by%250Alinking%2520model%2520capacity%2520with%2520pre-training%2520data%2520volume.%2520Yet%252C%2520scaling%2520laws%2520have%250Anot%2520been%2520established%2520for%2520HAR%2520to%2520the%2520same%2520extent%2520as%2520in%2520language%2520and%2520vision.%2520By%250Aconducting%2520an%2520exhaustive%2520grid%2520search%2520on%2520both%2520amount%2520of%2520pre-training%2520data%2520and%250ATransformer%2520architectures%252C%2520we%2520establish%2520the%2520first%2520known%2520scaling%2520laws%2520for%2520HAR.%250AWe%2520show%2520that%2520pre-training%2520loss%2520scales%2520with%2520a%2520power%2520law%2520relationship%2520to%2520amount%250Aof%2520data%2520and%2520parameter%2520count%2520and%2520that%2520increasing%2520the%2520number%2520of%2520users%2520in%2520a%250Adataset%2520results%2520in%2520a%2520steeper%2520improvement%2520in%2520performance%2520than%2520increasing%2520data%250Aper%2520user%252C%2520indicating%2520that%2520diversity%2520of%2520pre-training%2520data%2520is%2520important%252C%2520which%250Acontrasts%2520to%2520some%2520previously%2520reported%2520findings%2520in%2520self-supervised%2520HAR.%2520We%2520show%250Athat%2520these%2520scaling%2520laws%2520translate%2520to%2520downstream%2520performance%2520improvements%2520on%250Athree%2520HAR%2520benchmark%2520datasets%2520of%2520postures%252C%2520modes%2520of%2520locomotion%2520and%2520activities%2520of%250Adaily%2520living%253A%2520UCI%2520HAR%2520and%2520WISDM%2520Phone%2520and%2520WISDM%2520Watch.%2520Finally%252C%2520we%2520suggest%2520some%250Apreviously%2520published%2520works%2520should%2520be%2520revisited%2520in%2520light%2520of%2520these%2520scaling%2520laws%250Awith%2520more%2520adequate%2520model%2520capacities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03364v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20laws%20in%20wearable%20human%20activity%20recognition&entry.906535625=Tom%20Hoddes%20and%20Alex%20Bijamov%20and%20Saket%20Joshi%20and%20Daniel%20Roggen%20and%20Ali%20Etemad%20and%20Robert%20Harle%20and%20David%20Racz&entry.1292438233=%20%20Many%20deep%20architectures%20and%20self-supervised%20pre-training%20techniques%20have%20been%0Aproposed%20for%20human%20activity%20recognition%20%28HAR%29%20from%20wearable%20multimodal%20sensors.%0AScaling%20laws%20have%20the%20potential%20to%20help%20move%20towards%20more%20principled%20design%20by%0Alinking%20model%20capacity%20with%20pre-training%20data%20volume.%20Yet%2C%20scaling%20laws%20have%0Anot%20been%20established%20for%20HAR%20to%20the%20same%20extent%20as%20in%20language%20and%20vision.%20By%0Aconducting%20an%20exhaustive%20grid%20search%20on%20both%20amount%20of%20pre-training%20data%20and%0ATransformer%20architectures%2C%20we%20establish%20the%20first%20known%20scaling%20laws%20for%20HAR.%0AWe%20show%20that%20pre-training%20loss%20scales%20with%20a%20power%20law%20relationship%20to%20amount%0Aof%20data%20and%20parameter%20count%20and%20that%20increasing%20the%20number%20of%20users%20in%20a%0Adataset%20results%20in%20a%20steeper%20improvement%20in%20performance%20than%20increasing%20data%0Aper%20user%2C%20indicating%20that%20diversity%20of%20pre-training%20data%20is%20important%2C%20which%0Acontrasts%20to%20some%20previously%20reported%20findings%20in%20self-supervised%20HAR.%20We%20show%0Athat%20these%20scaling%20laws%20translate%20to%20downstream%20performance%20improvements%20on%0Athree%20HAR%20benchmark%20datasets%20of%20postures%2C%20modes%20of%20locomotion%20and%20activities%20of%0Adaily%20living%3A%20UCI%20HAR%20and%20WISDM%20Phone%20and%20WISDM%20Watch.%20Finally%2C%20we%20suggest%20some%0Apreviously%20published%20works%20should%20be%20revisited%20in%20light%20of%20these%20scaling%20laws%0Awith%20more%20adequate%20model%20capacities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03364v1&entry.124074799=Read"},
{"title": "Automatic Prompt Optimization Techniques: Exploring the Potential for\n  Synthetic Data Generation", "author": "Nina Freise and Marius Heitlinger and Ruben Nuredini and Gerrit Meixner", "abstract": "  Artificial Intelligence (AI) advancement is heavily dependent on access to\nlarge-scale, high-quality training data. However, in specialized domains such\nas healthcare, data acquisition faces significant constraints due to privacy\nregulations, ethical considerations, and limited availability. While synthetic\ndata generation offers a promising solution, conventional approaches typically\nrequire substantial real data for training generative models. The emergence of\nlarge-scale prompt-based models presents new opportunities for synthetic data\ngeneration without direct access to protected data. However, crafting effective\nprompts for domain-specific data generation remains challenging, and manual\nprompt engineering proves insufficient for achieving output with sufficient\nprecision and authenticity. We review recent developments in automatic prompt\noptimization, following PRISMA guidelines. We analyze six peer-reviewed studies\npublished between 2020 and 2024 that focus on automatic data-free prompt\noptimization methods. Our analysis reveals three approaches: feedback-driven,\nerror-based, and control-theoretic. Although all approaches demonstrate\npromising capabilities in prompt refinement and adaptation, our findings\nsuggest the need for an integrated framework that combines complementary\noptimization techniques to enhance synthetic data generation while minimizing\nmanual intervention. We propose future research directions toward developing\nrobust, iterative prompt optimization frameworks capable of improving the\nquality of synthetic data. This advancement can be particularly crucial for\nsensitive fields and in specialized domains where data access is restricted,\npotentially transforming how we approach synthetic data generation for AI\ndevelopment.\n", "link": "http://arxiv.org/abs/2502.03078v1", "date": "2025-02-05", "relevancy": 2.0713, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.524}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5147}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Prompt%20Optimization%20Techniques%3A%20Exploring%20the%20Potential%20for%0A%20%20Synthetic%20Data%20Generation&body=Title%3A%20Automatic%20Prompt%20Optimization%20Techniques%3A%20Exploring%20the%20Potential%20for%0A%20%20Synthetic%20Data%20Generation%0AAuthor%3A%20Nina%20Freise%20and%20Marius%20Heitlinger%20and%20Ruben%20Nuredini%20and%20Gerrit%20Meixner%0AAbstract%3A%20%20%20Artificial%20Intelligence%20%28AI%29%20advancement%20is%20heavily%20dependent%20on%20access%20to%0Alarge-scale%2C%20high-quality%20training%20data.%20However%2C%20in%20specialized%20domains%20such%0Aas%20healthcare%2C%20data%20acquisition%20faces%20significant%20constraints%20due%20to%20privacy%0Aregulations%2C%20ethical%20considerations%2C%20and%20limited%20availability.%20While%20synthetic%0Adata%20generation%20offers%20a%20promising%20solution%2C%20conventional%20approaches%20typically%0Arequire%20substantial%20real%20data%20for%20training%20generative%20models.%20The%20emergence%20of%0Alarge-scale%20prompt-based%20models%20presents%20new%20opportunities%20for%20synthetic%20data%0Ageneration%20without%20direct%20access%20to%20protected%20data.%20However%2C%20crafting%20effective%0Aprompts%20for%20domain-specific%20data%20generation%20remains%20challenging%2C%20and%20manual%0Aprompt%20engineering%20proves%20insufficient%20for%20achieving%20output%20with%20sufficient%0Aprecision%20and%20authenticity.%20We%20review%20recent%20developments%20in%20automatic%20prompt%0Aoptimization%2C%20following%20PRISMA%20guidelines.%20We%20analyze%20six%20peer-reviewed%20studies%0Apublished%20between%202020%20and%202024%20that%20focus%20on%20automatic%20data-free%20prompt%0Aoptimization%20methods.%20Our%20analysis%20reveals%20three%20approaches%3A%20feedback-driven%2C%0Aerror-based%2C%20and%20control-theoretic.%20Although%20all%20approaches%20demonstrate%0Apromising%20capabilities%20in%20prompt%20refinement%20and%20adaptation%2C%20our%20findings%0Asuggest%20the%20need%20for%20an%20integrated%20framework%20that%20combines%20complementary%0Aoptimization%20techniques%20to%20enhance%20synthetic%20data%20generation%20while%20minimizing%0Amanual%20intervention.%20We%20propose%20future%20research%20directions%20toward%20developing%0Arobust%2C%20iterative%20prompt%20optimization%20frameworks%20capable%20of%20improving%20the%0Aquality%20of%20synthetic%20data.%20This%20advancement%20can%20be%20particularly%20crucial%20for%0Asensitive%20fields%20and%20in%20specialized%20domains%20where%20data%20access%20is%20restricted%2C%0Apotentially%20transforming%20how%20we%20approach%20synthetic%20data%20generation%20for%20AI%0Adevelopment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03078v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Prompt%2520Optimization%2520Techniques%253A%2520Exploring%2520the%2520Potential%2520for%250A%2520%2520Synthetic%2520Data%2520Generation%26entry.906535625%3DNina%2520Freise%2520and%2520Marius%2520Heitlinger%2520and%2520Ruben%2520Nuredini%2520and%2520Gerrit%2520Meixner%26entry.1292438233%3D%2520%2520Artificial%2520Intelligence%2520%2528AI%2529%2520advancement%2520is%2520heavily%2520dependent%2520on%2520access%2520to%250Alarge-scale%252C%2520high-quality%2520training%2520data.%2520However%252C%2520in%2520specialized%2520domains%2520such%250Aas%2520healthcare%252C%2520data%2520acquisition%2520faces%2520significant%2520constraints%2520due%2520to%2520privacy%250Aregulations%252C%2520ethical%2520considerations%252C%2520and%2520limited%2520availability.%2520While%2520synthetic%250Adata%2520generation%2520offers%2520a%2520promising%2520solution%252C%2520conventional%2520approaches%2520typically%250Arequire%2520substantial%2520real%2520data%2520for%2520training%2520generative%2520models.%2520The%2520emergence%2520of%250Alarge-scale%2520prompt-based%2520models%2520presents%2520new%2520opportunities%2520for%2520synthetic%2520data%250Ageneration%2520without%2520direct%2520access%2520to%2520protected%2520data.%2520However%252C%2520crafting%2520effective%250Aprompts%2520for%2520domain-specific%2520data%2520generation%2520remains%2520challenging%252C%2520and%2520manual%250Aprompt%2520engineering%2520proves%2520insufficient%2520for%2520achieving%2520output%2520with%2520sufficient%250Aprecision%2520and%2520authenticity.%2520We%2520review%2520recent%2520developments%2520in%2520automatic%2520prompt%250Aoptimization%252C%2520following%2520PRISMA%2520guidelines.%2520We%2520analyze%2520six%2520peer-reviewed%2520studies%250Apublished%2520between%25202020%2520and%25202024%2520that%2520focus%2520on%2520automatic%2520data-free%2520prompt%250Aoptimization%2520methods.%2520Our%2520analysis%2520reveals%2520three%2520approaches%253A%2520feedback-driven%252C%250Aerror-based%252C%2520and%2520control-theoretic.%2520Although%2520all%2520approaches%2520demonstrate%250Apromising%2520capabilities%2520in%2520prompt%2520refinement%2520and%2520adaptation%252C%2520our%2520findings%250Asuggest%2520the%2520need%2520for%2520an%2520integrated%2520framework%2520that%2520combines%2520complementary%250Aoptimization%2520techniques%2520to%2520enhance%2520synthetic%2520data%2520generation%2520while%2520minimizing%250Amanual%2520intervention.%2520We%2520propose%2520future%2520research%2520directions%2520toward%2520developing%250Arobust%252C%2520iterative%2520prompt%2520optimization%2520frameworks%2520capable%2520of%2520improving%2520the%250Aquality%2520of%2520synthetic%2520data.%2520This%2520advancement%2520can%2520be%2520particularly%2520crucial%2520for%250Asensitive%2520fields%2520and%2520in%2520specialized%2520domains%2520where%2520data%2520access%2520is%2520restricted%252C%250Apotentially%2520transforming%2520how%2520we%2520approach%2520synthetic%2520data%2520generation%2520for%2520AI%250Adevelopment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03078v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Prompt%20Optimization%20Techniques%3A%20Exploring%20the%20Potential%20for%0A%20%20Synthetic%20Data%20Generation&entry.906535625=Nina%20Freise%20and%20Marius%20Heitlinger%20and%20Ruben%20Nuredini%20and%20Gerrit%20Meixner&entry.1292438233=%20%20Artificial%20Intelligence%20%28AI%29%20advancement%20is%20heavily%20dependent%20on%20access%20to%0Alarge-scale%2C%20high-quality%20training%20data.%20However%2C%20in%20specialized%20domains%20such%0Aas%20healthcare%2C%20data%20acquisition%20faces%20significant%20constraints%20due%20to%20privacy%0Aregulations%2C%20ethical%20considerations%2C%20and%20limited%20availability.%20While%20synthetic%0Adata%20generation%20offers%20a%20promising%20solution%2C%20conventional%20approaches%20typically%0Arequire%20substantial%20real%20data%20for%20training%20generative%20models.%20The%20emergence%20of%0Alarge-scale%20prompt-based%20models%20presents%20new%20opportunities%20for%20synthetic%20data%0Ageneration%20without%20direct%20access%20to%20protected%20data.%20However%2C%20crafting%20effective%0Aprompts%20for%20domain-specific%20data%20generation%20remains%20challenging%2C%20and%20manual%0Aprompt%20engineering%20proves%20insufficient%20for%20achieving%20output%20with%20sufficient%0Aprecision%20and%20authenticity.%20We%20review%20recent%20developments%20in%20automatic%20prompt%0Aoptimization%2C%20following%20PRISMA%20guidelines.%20We%20analyze%20six%20peer-reviewed%20studies%0Apublished%20between%202020%20and%202024%20that%20focus%20on%20automatic%20data-free%20prompt%0Aoptimization%20methods.%20Our%20analysis%20reveals%20three%20approaches%3A%20feedback-driven%2C%0Aerror-based%2C%20and%20control-theoretic.%20Although%20all%20approaches%20demonstrate%0Apromising%20capabilities%20in%20prompt%20refinement%20and%20adaptation%2C%20our%20findings%0Asuggest%20the%20need%20for%20an%20integrated%20framework%20that%20combines%20complementary%0Aoptimization%20techniques%20to%20enhance%20synthetic%20data%20generation%20while%20minimizing%0Amanual%20intervention.%20We%20propose%20future%20research%20directions%20toward%20developing%0Arobust%2C%20iterative%20prompt%20optimization%20frameworks%20capable%20of%20improving%20the%0Aquality%20of%20synthetic%20data.%20This%20advancement%20can%20be%20particularly%20crucial%20for%0Asensitive%20fields%20and%20in%20specialized%20domains%20where%20data%20access%20is%20restricted%2C%0Apotentially%20transforming%20how%20we%20approach%20synthetic%20data%20generation%20for%20AI%0Adevelopment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03078v1&entry.124074799=Read"},
{"title": "DP-SGD-Global-Adapt-V2-S: Triad Improvements of Privacy, Accuracy and\n  Fairness via Step Decay Noise Multiplier and Step Decay Upper Clipping\n  Threshold", "author": "Sai Venkatesh Chilukoti and Md Imran Hossen and Liqun Shan and Vijay Srinivas Tida and Mahathir Mohammad Bappy and Wenmeng Tian and Xiai Hei", "abstract": "  Differentially Private Stochastic Gradient Descent (DP-SGD) has become a\nwidely used technique for safeguarding sensitive information in deep learning\napplications. Unfortunately, DPSGD's per-sample gradient clipping and uniform\nnoise addition during training can significantly degrade model utility and\nfairness. We observe that the latest DP-SGD-Global-Adapt's average gradient\nnorm is the same throughout the training. Even when it is integrated with the\nexisting linear decay noise multiplier, it has little or no advantage.\nMoreover, we notice that its upper clipping threshold increases exponentially\ntowards the end of training, potentially impacting the models convergence.\nOther algorithms, DP-PSAC, Auto-S, DP-SGD-Global, and DP-F, have utility and\nfairness that are similar to or worse than DP-SGD, as demonstrated in\nexperiments. To overcome these problems and improve utility and fairness, we\ndeveloped the DP-SGD-Global-Adapt-V2-S. It has a step-decay noise multiplier\nand an upper clipping threshold that is also decayed step-wise.\nDP-SGD-Global-Adapt-V2-S with a privacy budget ($\\epsilon$) of 1 improves\naccuracy by 0.9795\\%, 0.6786\\%, and 4.0130\\% in MNIST, CIFAR10, and CIFAR100,\nrespectively. It also reduces the privacy cost gap ($\\pi$) by 89.8332% and\n60.5541% in unbalanced MNIST and Thinwall datasets, respectively. Finally, we\ndevelop mathematical expressions to compute the privacy budget using truncated\nconcentrated differential privacy (tCDP) for DP-SGD-Global-Adapt-V2-T and\nDP-SGD-Global-Adapt-V2-S.\n", "link": "http://arxiv.org/abs/2312.02400v2", "date": "2025-02-05", "relevancy": 2.0678, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5606}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5221}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DP-SGD-Global-Adapt-V2-S%3A%20Triad%20Improvements%20of%20Privacy%2C%20Accuracy%20and%0A%20%20Fairness%20via%20Step%20Decay%20Noise%20Multiplier%20and%20Step%20Decay%20Upper%20Clipping%0A%20%20Threshold&body=Title%3A%20DP-SGD-Global-Adapt-V2-S%3A%20Triad%20Improvements%20of%20Privacy%2C%20Accuracy%20and%0A%20%20Fairness%20via%20Step%20Decay%20Noise%20Multiplier%20and%20Step%20Decay%20Upper%20Clipping%0A%20%20Threshold%0AAuthor%3A%20Sai%20Venkatesh%20Chilukoti%20and%20Md%20Imran%20Hossen%20and%20Liqun%20Shan%20and%20Vijay%20Srinivas%20Tida%20and%20Mahathir%20Mohammad%20Bappy%20and%20Wenmeng%20Tian%20and%20Xiai%20Hei%0AAbstract%3A%20%20%20Differentially%20Private%20Stochastic%20Gradient%20Descent%20%28DP-SGD%29%20has%20become%20a%0Awidely%20used%20technique%20for%20safeguarding%20sensitive%20information%20in%20deep%20learning%0Aapplications.%20Unfortunately%2C%20DPSGD%27s%20per-sample%20gradient%20clipping%20and%20uniform%0Anoise%20addition%20during%20training%20can%20significantly%20degrade%20model%20utility%20and%0Afairness.%20We%20observe%20that%20the%20latest%20DP-SGD-Global-Adapt%27s%20average%20gradient%0Anorm%20is%20the%20same%20throughout%20the%20training.%20Even%20when%20it%20is%20integrated%20with%20the%0Aexisting%20linear%20decay%20noise%20multiplier%2C%20it%20has%20little%20or%20no%20advantage.%0AMoreover%2C%20we%20notice%20that%20its%20upper%20clipping%20threshold%20increases%20exponentially%0Atowards%20the%20end%20of%20training%2C%20potentially%20impacting%20the%20models%20convergence.%0AOther%20algorithms%2C%20DP-PSAC%2C%20Auto-S%2C%20DP-SGD-Global%2C%20and%20DP-F%2C%20have%20utility%20and%0Afairness%20that%20are%20similar%20to%20or%20worse%20than%20DP-SGD%2C%20as%20demonstrated%20in%0Aexperiments.%20To%20overcome%20these%20problems%20and%20improve%20utility%20and%20fairness%2C%20we%0Adeveloped%20the%20DP-SGD-Global-Adapt-V2-S.%20It%20has%20a%20step-decay%20noise%20multiplier%0Aand%20an%20upper%20clipping%20threshold%20that%20is%20also%20decayed%20step-wise.%0ADP-SGD-Global-Adapt-V2-S%20with%20a%20privacy%20budget%20%28%24%5Cepsilon%24%29%20of%201%20improves%0Aaccuracy%20by%200.9795%5C%25%2C%200.6786%5C%25%2C%20and%204.0130%5C%25%20in%20MNIST%2C%20CIFAR10%2C%20and%20CIFAR100%2C%0Arespectively.%20It%20also%20reduces%20the%20privacy%20cost%20gap%20%28%24%5Cpi%24%29%20by%2089.8332%25%20and%0A60.5541%25%20in%20unbalanced%20MNIST%20and%20Thinwall%20datasets%2C%20respectively.%20Finally%2C%20we%0Adevelop%20mathematical%20expressions%20to%20compute%20the%20privacy%20budget%20using%20truncated%0Aconcentrated%20differential%20privacy%20%28tCDP%29%20for%20DP-SGD-Global-Adapt-V2-T%20and%0ADP-SGD-Global-Adapt-V2-S.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.02400v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDP-SGD-Global-Adapt-V2-S%253A%2520Triad%2520Improvements%2520of%2520Privacy%252C%2520Accuracy%2520and%250A%2520%2520Fairness%2520via%2520Step%2520Decay%2520Noise%2520Multiplier%2520and%2520Step%2520Decay%2520Upper%2520Clipping%250A%2520%2520Threshold%26entry.906535625%3DSai%2520Venkatesh%2520Chilukoti%2520and%2520Md%2520Imran%2520Hossen%2520and%2520Liqun%2520Shan%2520and%2520Vijay%2520Srinivas%2520Tida%2520and%2520Mahathir%2520Mohammad%2520Bappy%2520and%2520Wenmeng%2520Tian%2520and%2520Xiai%2520Hei%26entry.1292438233%3D%2520%2520Differentially%2520Private%2520Stochastic%2520Gradient%2520Descent%2520%2528DP-SGD%2529%2520has%2520become%2520a%250Awidely%2520used%2520technique%2520for%2520safeguarding%2520sensitive%2520information%2520in%2520deep%2520learning%250Aapplications.%2520Unfortunately%252C%2520DPSGD%2527s%2520per-sample%2520gradient%2520clipping%2520and%2520uniform%250Anoise%2520addition%2520during%2520training%2520can%2520significantly%2520degrade%2520model%2520utility%2520and%250Afairness.%2520We%2520observe%2520that%2520the%2520latest%2520DP-SGD-Global-Adapt%2527s%2520average%2520gradient%250Anorm%2520is%2520the%2520same%2520throughout%2520the%2520training.%2520Even%2520when%2520it%2520is%2520integrated%2520with%2520the%250Aexisting%2520linear%2520decay%2520noise%2520multiplier%252C%2520it%2520has%2520little%2520or%2520no%2520advantage.%250AMoreover%252C%2520we%2520notice%2520that%2520its%2520upper%2520clipping%2520threshold%2520increases%2520exponentially%250Atowards%2520the%2520end%2520of%2520training%252C%2520potentially%2520impacting%2520the%2520models%2520convergence.%250AOther%2520algorithms%252C%2520DP-PSAC%252C%2520Auto-S%252C%2520DP-SGD-Global%252C%2520and%2520DP-F%252C%2520have%2520utility%2520and%250Afairness%2520that%2520are%2520similar%2520to%2520or%2520worse%2520than%2520DP-SGD%252C%2520as%2520demonstrated%2520in%250Aexperiments.%2520To%2520overcome%2520these%2520problems%2520and%2520improve%2520utility%2520and%2520fairness%252C%2520we%250Adeveloped%2520the%2520DP-SGD-Global-Adapt-V2-S.%2520It%2520has%2520a%2520step-decay%2520noise%2520multiplier%250Aand%2520an%2520upper%2520clipping%2520threshold%2520that%2520is%2520also%2520decayed%2520step-wise.%250ADP-SGD-Global-Adapt-V2-S%2520with%2520a%2520privacy%2520budget%2520%2528%2524%255Cepsilon%2524%2529%2520of%25201%2520improves%250Aaccuracy%2520by%25200.9795%255C%2525%252C%25200.6786%255C%2525%252C%2520and%25204.0130%255C%2525%2520in%2520MNIST%252C%2520CIFAR10%252C%2520and%2520CIFAR100%252C%250Arespectively.%2520It%2520also%2520reduces%2520the%2520privacy%2520cost%2520gap%2520%2528%2524%255Cpi%2524%2529%2520by%252089.8332%2525%2520and%250A60.5541%2525%2520in%2520unbalanced%2520MNIST%2520and%2520Thinwall%2520datasets%252C%2520respectively.%2520Finally%252C%2520we%250Adevelop%2520mathematical%2520expressions%2520to%2520compute%2520the%2520privacy%2520budget%2520using%2520truncated%250Aconcentrated%2520differential%2520privacy%2520%2528tCDP%2529%2520for%2520DP-SGD-Global-Adapt-V2-T%2520and%250ADP-SGD-Global-Adapt-V2-S.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.02400v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DP-SGD-Global-Adapt-V2-S%3A%20Triad%20Improvements%20of%20Privacy%2C%20Accuracy%20and%0A%20%20Fairness%20via%20Step%20Decay%20Noise%20Multiplier%20and%20Step%20Decay%20Upper%20Clipping%0A%20%20Threshold&entry.906535625=Sai%20Venkatesh%20Chilukoti%20and%20Md%20Imran%20Hossen%20and%20Liqun%20Shan%20and%20Vijay%20Srinivas%20Tida%20and%20Mahathir%20Mohammad%20Bappy%20and%20Wenmeng%20Tian%20and%20Xiai%20Hei&entry.1292438233=%20%20Differentially%20Private%20Stochastic%20Gradient%20Descent%20%28DP-SGD%29%20has%20become%20a%0Awidely%20used%20technique%20for%20safeguarding%20sensitive%20information%20in%20deep%20learning%0Aapplications.%20Unfortunately%2C%20DPSGD%27s%20per-sample%20gradient%20clipping%20and%20uniform%0Anoise%20addition%20during%20training%20can%20significantly%20degrade%20model%20utility%20and%0Afairness.%20We%20observe%20that%20the%20latest%20DP-SGD-Global-Adapt%27s%20average%20gradient%0Anorm%20is%20the%20same%20throughout%20the%20training.%20Even%20when%20it%20is%20integrated%20with%20the%0Aexisting%20linear%20decay%20noise%20multiplier%2C%20it%20has%20little%20or%20no%20advantage.%0AMoreover%2C%20we%20notice%20that%20its%20upper%20clipping%20threshold%20increases%20exponentially%0Atowards%20the%20end%20of%20training%2C%20potentially%20impacting%20the%20models%20convergence.%0AOther%20algorithms%2C%20DP-PSAC%2C%20Auto-S%2C%20DP-SGD-Global%2C%20and%20DP-F%2C%20have%20utility%20and%0Afairness%20that%20are%20similar%20to%20or%20worse%20than%20DP-SGD%2C%20as%20demonstrated%20in%0Aexperiments.%20To%20overcome%20these%20problems%20and%20improve%20utility%20and%20fairness%2C%20we%0Adeveloped%20the%20DP-SGD-Global-Adapt-V2-S.%20It%20has%20a%20step-decay%20noise%20multiplier%0Aand%20an%20upper%20clipping%20threshold%20that%20is%20also%20decayed%20step-wise.%0ADP-SGD-Global-Adapt-V2-S%20with%20a%20privacy%20budget%20%28%24%5Cepsilon%24%29%20of%201%20improves%0Aaccuracy%20by%200.9795%5C%25%2C%200.6786%5C%25%2C%20and%204.0130%5C%25%20in%20MNIST%2C%20CIFAR10%2C%20and%20CIFAR100%2C%0Arespectively.%20It%20also%20reduces%20the%20privacy%20cost%20gap%20%28%24%5Cpi%24%29%20by%2089.8332%25%20and%0A60.5541%25%20in%20unbalanced%20MNIST%20and%20Thinwall%20datasets%2C%20respectively.%20Finally%2C%20we%0Adevelop%20mathematical%20expressions%20to%20compute%20the%20privacy%20budget%20using%20truncated%0Aconcentrated%20differential%20privacy%20%28tCDP%29%20for%20DP-SGD-Global-Adapt-V2-T%20and%0ADP-SGD-Global-Adapt-V2-S.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.02400v2&entry.124074799=Read"},
{"title": "Prediction of the Most Fire-Sensitive Point in Building Structures with\n  Differentiable Agents for Thermal Simulators", "author": "Yuan Xinjie and Khalid M. Mosalam", "abstract": "  Fire safety is a critical area of research in civil and mechanical\nengineering, particularly in ensuring the structural stability of buildings\nduring fire events. The Most Fire-Sensitive Point (MFSP) in a structure is the\nlocation where a fire would cause the greatest impact on structural stability.\nAccurate prediction of the MFSP is vital for streamlining structural\nassessments and optimizing the design process. This paper presents a novel\nframework for MFSP prediction using a neural network-based approach that\nintegrates fire dynamics and finite element analysis through a differentiable\nagent model. The framework focuses on predicting the Maximum Interstory Drift\nRatio (MIDR), a key indicator of structural performance under fire conditions.\nBy leveraging the differentiable agent model, we efficiently generate labeled\ndata for MFSP and directly train a predictor for this critical metric. To\nachieve this, we generated extensive simulation data encompassing structural\nand fire scenarios and employed graph neural networks to represent the building\nstructures. Transfer learning was applied to optimize the training process, and\nan edge update mechanism was introduced to dynamically adjust edge attributes,\nreflecting property changes under fire conditions. The proposed model was\nrigorously evaluated on simulation data, demonstrating strong performance in\naccurately predicting both MIDR and MFSP, thus advancing fire safety analysis\nfor building structures.\n", "link": "http://arxiv.org/abs/2502.03424v1", "date": "2025-02-05", "relevancy": 2.065, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5728}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5199}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prediction%20of%20the%20Most%20Fire-Sensitive%20Point%20in%20Building%20Structures%20with%0A%20%20Differentiable%20Agents%20for%20Thermal%20Simulators&body=Title%3A%20Prediction%20of%20the%20Most%20Fire-Sensitive%20Point%20in%20Building%20Structures%20with%0A%20%20Differentiable%20Agents%20for%20Thermal%20Simulators%0AAuthor%3A%20Yuan%20Xinjie%20and%20Khalid%20M.%20Mosalam%0AAbstract%3A%20%20%20Fire%20safety%20is%20a%20critical%20area%20of%20research%20in%20civil%20and%20mechanical%0Aengineering%2C%20particularly%20in%20ensuring%20the%20structural%20stability%20of%20buildings%0Aduring%20fire%20events.%20The%20Most%20Fire-Sensitive%20Point%20%28MFSP%29%20in%20a%20structure%20is%20the%0Alocation%20where%20a%20fire%20would%20cause%20the%20greatest%20impact%20on%20structural%20stability.%0AAccurate%20prediction%20of%20the%20MFSP%20is%20vital%20for%20streamlining%20structural%0Aassessments%20and%20optimizing%20the%20design%20process.%20This%20paper%20presents%20a%20novel%0Aframework%20for%20MFSP%20prediction%20using%20a%20neural%20network-based%20approach%20that%0Aintegrates%20fire%20dynamics%20and%20finite%20element%20analysis%20through%20a%20differentiable%0Aagent%20model.%20The%20framework%20focuses%20on%20predicting%20the%20Maximum%20Interstory%20Drift%0ARatio%20%28MIDR%29%2C%20a%20key%20indicator%20of%20structural%20performance%20under%20fire%20conditions.%0ABy%20leveraging%20the%20differentiable%20agent%20model%2C%20we%20efficiently%20generate%20labeled%0Adata%20for%20MFSP%20and%20directly%20train%20a%20predictor%20for%20this%20critical%20metric.%20To%0Aachieve%20this%2C%20we%20generated%20extensive%20simulation%20data%20encompassing%20structural%0Aand%20fire%20scenarios%20and%20employed%20graph%20neural%20networks%20to%20represent%20the%20building%0Astructures.%20Transfer%20learning%20was%20applied%20to%20optimize%20the%20training%20process%2C%20and%0Aan%20edge%20update%20mechanism%20was%20introduced%20to%20dynamically%20adjust%20edge%20attributes%2C%0Areflecting%20property%20changes%20under%20fire%20conditions.%20The%20proposed%20model%20was%0Arigorously%20evaluated%20on%20simulation%20data%2C%20demonstrating%20strong%20performance%20in%0Aaccurately%20predicting%20both%20MIDR%20and%20MFSP%2C%20thus%20advancing%20fire%20safety%20analysis%0Afor%20building%20structures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03424v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrediction%2520of%2520the%2520Most%2520Fire-Sensitive%2520Point%2520in%2520Building%2520Structures%2520with%250A%2520%2520Differentiable%2520Agents%2520for%2520Thermal%2520Simulators%26entry.906535625%3DYuan%2520Xinjie%2520and%2520Khalid%2520M.%2520Mosalam%26entry.1292438233%3D%2520%2520Fire%2520safety%2520is%2520a%2520critical%2520area%2520of%2520research%2520in%2520civil%2520and%2520mechanical%250Aengineering%252C%2520particularly%2520in%2520ensuring%2520the%2520structural%2520stability%2520of%2520buildings%250Aduring%2520fire%2520events.%2520The%2520Most%2520Fire-Sensitive%2520Point%2520%2528MFSP%2529%2520in%2520a%2520structure%2520is%2520the%250Alocation%2520where%2520a%2520fire%2520would%2520cause%2520the%2520greatest%2520impact%2520on%2520structural%2520stability.%250AAccurate%2520prediction%2520of%2520the%2520MFSP%2520is%2520vital%2520for%2520streamlining%2520structural%250Aassessments%2520and%2520optimizing%2520the%2520design%2520process.%2520This%2520paper%2520presents%2520a%2520novel%250Aframework%2520for%2520MFSP%2520prediction%2520using%2520a%2520neural%2520network-based%2520approach%2520that%250Aintegrates%2520fire%2520dynamics%2520and%2520finite%2520element%2520analysis%2520through%2520a%2520differentiable%250Aagent%2520model.%2520The%2520framework%2520focuses%2520on%2520predicting%2520the%2520Maximum%2520Interstory%2520Drift%250ARatio%2520%2528MIDR%2529%252C%2520a%2520key%2520indicator%2520of%2520structural%2520performance%2520under%2520fire%2520conditions.%250ABy%2520leveraging%2520the%2520differentiable%2520agent%2520model%252C%2520we%2520efficiently%2520generate%2520labeled%250Adata%2520for%2520MFSP%2520and%2520directly%2520train%2520a%2520predictor%2520for%2520this%2520critical%2520metric.%2520To%250Aachieve%2520this%252C%2520we%2520generated%2520extensive%2520simulation%2520data%2520encompassing%2520structural%250Aand%2520fire%2520scenarios%2520and%2520employed%2520graph%2520neural%2520networks%2520to%2520represent%2520the%2520building%250Astructures.%2520Transfer%2520learning%2520was%2520applied%2520to%2520optimize%2520the%2520training%2520process%252C%2520and%250Aan%2520edge%2520update%2520mechanism%2520was%2520introduced%2520to%2520dynamically%2520adjust%2520edge%2520attributes%252C%250Areflecting%2520property%2520changes%2520under%2520fire%2520conditions.%2520The%2520proposed%2520model%2520was%250Arigorously%2520evaluated%2520on%2520simulation%2520data%252C%2520demonstrating%2520strong%2520performance%2520in%250Aaccurately%2520predicting%2520both%2520MIDR%2520and%2520MFSP%252C%2520thus%2520advancing%2520fire%2520safety%2520analysis%250Afor%2520building%2520structures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03424v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prediction%20of%20the%20Most%20Fire-Sensitive%20Point%20in%20Building%20Structures%20with%0A%20%20Differentiable%20Agents%20for%20Thermal%20Simulators&entry.906535625=Yuan%20Xinjie%20and%20Khalid%20M.%20Mosalam&entry.1292438233=%20%20Fire%20safety%20is%20a%20critical%20area%20of%20research%20in%20civil%20and%20mechanical%0Aengineering%2C%20particularly%20in%20ensuring%20the%20structural%20stability%20of%20buildings%0Aduring%20fire%20events.%20The%20Most%20Fire-Sensitive%20Point%20%28MFSP%29%20in%20a%20structure%20is%20the%0Alocation%20where%20a%20fire%20would%20cause%20the%20greatest%20impact%20on%20structural%20stability.%0AAccurate%20prediction%20of%20the%20MFSP%20is%20vital%20for%20streamlining%20structural%0Aassessments%20and%20optimizing%20the%20design%20process.%20This%20paper%20presents%20a%20novel%0Aframework%20for%20MFSP%20prediction%20using%20a%20neural%20network-based%20approach%20that%0Aintegrates%20fire%20dynamics%20and%20finite%20element%20analysis%20through%20a%20differentiable%0Aagent%20model.%20The%20framework%20focuses%20on%20predicting%20the%20Maximum%20Interstory%20Drift%0ARatio%20%28MIDR%29%2C%20a%20key%20indicator%20of%20structural%20performance%20under%20fire%20conditions.%0ABy%20leveraging%20the%20differentiable%20agent%20model%2C%20we%20efficiently%20generate%20labeled%0Adata%20for%20MFSP%20and%20directly%20train%20a%20predictor%20for%20this%20critical%20metric.%20To%0Aachieve%20this%2C%20we%20generated%20extensive%20simulation%20data%20encompassing%20structural%0Aand%20fire%20scenarios%20and%20employed%20graph%20neural%20networks%20to%20represent%20the%20building%0Astructures.%20Transfer%20learning%20was%20applied%20to%20optimize%20the%20training%20process%2C%20and%0Aan%20edge%20update%20mechanism%20was%20introduced%20to%20dynamically%20adjust%20edge%20attributes%2C%0Areflecting%20property%20changes%20under%20fire%20conditions.%20The%20proposed%20model%20was%0Arigorously%20evaluated%20on%20simulation%20data%2C%20demonstrating%20strong%20performance%20in%0Aaccurately%20predicting%20both%20MIDR%20and%20MFSP%2C%20thus%20advancing%20fire%20safety%20analysis%0Afor%20building%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03424v1&entry.124074799=Read"},
{"title": "Improve Decoding Factuality by Token-wise Cross Layer Entropy of Large\n  Language Models", "author": "Jialiang Wu and Yi Shen and Sijia Liu and Yi Tang and Sen Song and Xiaoyi Wang and Longjun Cai", "abstract": "  Despite their impressive capacities, Large language models (LLMs) often\nstruggle with the hallucination issue of generating inaccurate or fabricated\ncontent even when they possess correct knowledge. In this paper, we extend the\nexploration of the correlation between hidden-state prediction changes and\noutput factuality into a deeper, token-wise level. Based on the insights , we\npropose cross-layer Entropy eNhanced Decoding (END), a decoding method that\nmitigates hallucinations without requiring extra training. END leverages inner\nprobability changes across layers to individually quantify the factual\nknowledge required for each candidate token, and adjusts the final predicting\ndistribution to prioritize tokens with higher factuality. Experiments on both\nhallucination and QA benchmarks demonstrate that END significantly enhances the\ntruthfulness and informativeness of generated content while maintaining robust\nQA accuracy. Moreover, our work provides a deeper perspective on understanding\nthe correlations between inherent knowledge and output factuality.\n", "link": "http://arxiv.org/abs/2502.03199v1", "date": "2025-02-05", "relevancy": 2.063, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5237}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5142}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5142}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improve%20Decoding%20Factuality%20by%20Token-wise%20Cross%20Layer%20Entropy%20of%20Large%0A%20%20Language%20Models&body=Title%3A%20Improve%20Decoding%20Factuality%20by%20Token-wise%20Cross%20Layer%20Entropy%20of%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Jialiang%20Wu%20and%20Yi%20Shen%20and%20Sijia%20Liu%20and%20Yi%20Tang%20and%20Sen%20Song%20and%20Xiaoyi%20Wang%20and%20Longjun%20Cai%0AAbstract%3A%20%20%20Despite%20their%20impressive%20capacities%2C%20Large%20language%20models%20%28LLMs%29%20often%0Astruggle%20with%20the%20hallucination%20issue%20of%20generating%20inaccurate%20or%20fabricated%0Acontent%20even%20when%20they%20possess%20correct%20knowledge.%20In%20this%20paper%2C%20we%20extend%20the%0Aexploration%20of%20the%20correlation%20between%20hidden-state%20prediction%20changes%20and%0Aoutput%20factuality%20into%20a%20deeper%2C%20token-wise%20level.%20Based%20on%20the%20insights%20%2C%20we%0Apropose%20cross-layer%20Entropy%20eNhanced%20Decoding%20%28END%29%2C%20a%20decoding%20method%20that%0Amitigates%20hallucinations%20without%20requiring%20extra%20training.%20END%20leverages%20inner%0Aprobability%20changes%20across%20layers%20to%20individually%20quantify%20the%20factual%0Aknowledge%20required%20for%20each%20candidate%20token%2C%20and%20adjusts%20the%20final%20predicting%0Adistribution%20to%20prioritize%20tokens%20with%20higher%20factuality.%20Experiments%20on%20both%0Ahallucination%20and%20QA%20benchmarks%20demonstrate%20that%20END%20significantly%20enhances%20the%0Atruthfulness%20and%20informativeness%20of%20generated%20content%20while%20maintaining%20robust%0AQA%20accuracy.%20Moreover%2C%20our%20work%20provides%20a%20deeper%20perspective%20on%20understanding%0Athe%20correlations%20between%20inherent%20knowledge%20and%20output%20factuality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03199v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImprove%2520Decoding%2520Factuality%2520by%2520Token-wise%2520Cross%2520Layer%2520Entropy%2520of%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DJialiang%2520Wu%2520and%2520Yi%2520Shen%2520and%2520Sijia%2520Liu%2520and%2520Yi%2520Tang%2520and%2520Sen%2520Song%2520and%2520Xiaoyi%2520Wang%2520and%2520Longjun%2520Cai%26entry.1292438233%3D%2520%2520Despite%2520their%2520impressive%2520capacities%252C%2520Large%2520language%2520models%2520%2528LLMs%2529%2520often%250Astruggle%2520with%2520the%2520hallucination%2520issue%2520of%2520generating%2520inaccurate%2520or%2520fabricated%250Acontent%2520even%2520when%2520they%2520possess%2520correct%2520knowledge.%2520In%2520this%2520paper%252C%2520we%2520extend%2520the%250Aexploration%2520of%2520the%2520correlation%2520between%2520hidden-state%2520prediction%2520changes%2520and%250Aoutput%2520factuality%2520into%2520a%2520deeper%252C%2520token-wise%2520level.%2520Based%2520on%2520the%2520insights%2520%252C%2520we%250Apropose%2520cross-layer%2520Entropy%2520eNhanced%2520Decoding%2520%2528END%2529%252C%2520a%2520decoding%2520method%2520that%250Amitigates%2520hallucinations%2520without%2520requiring%2520extra%2520training.%2520END%2520leverages%2520inner%250Aprobability%2520changes%2520across%2520layers%2520to%2520individually%2520quantify%2520the%2520factual%250Aknowledge%2520required%2520for%2520each%2520candidate%2520token%252C%2520and%2520adjusts%2520the%2520final%2520predicting%250Adistribution%2520to%2520prioritize%2520tokens%2520with%2520higher%2520factuality.%2520Experiments%2520on%2520both%250Ahallucination%2520and%2520QA%2520benchmarks%2520demonstrate%2520that%2520END%2520significantly%2520enhances%2520the%250Atruthfulness%2520and%2520informativeness%2520of%2520generated%2520content%2520while%2520maintaining%2520robust%250AQA%2520accuracy.%2520Moreover%252C%2520our%2520work%2520provides%2520a%2520deeper%2520perspective%2520on%2520understanding%250Athe%2520correlations%2520between%2520inherent%2520knowledge%2520and%2520output%2520factuality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03199v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improve%20Decoding%20Factuality%20by%20Token-wise%20Cross%20Layer%20Entropy%20of%20Large%0A%20%20Language%20Models&entry.906535625=Jialiang%20Wu%20and%20Yi%20Shen%20and%20Sijia%20Liu%20and%20Yi%20Tang%20and%20Sen%20Song%20and%20Xiaoyi%20Wang%20and%20Longjun%20Cai&entry.1292438233=%20%20Despite%20their%20impressive%20capacities%2C%20Large%20language%20models%20%28LLMs%29%20often%0Astruggle%20with%20the%20hallucination%20issue%20of%20generating%20inaccurate%20or%20fabricated%0Acontent%20even%20when%20they%20possess%20correct%20knowledge.%20In%20this%20paper%2C%20we%20extend%20the%0Aexploration%20of%20the%20correlation%20between%20hidden-state%20prediction%20changes%20and%0Aoutput%20factuality%20into%20a%20deeper%2C%20token-wise%20level.%20Based%20on%20the%20insights%20%2C%20we%0Apropose%20cross-layer%20Entropy%20eNhanced%20Decoding%20%28END%29%2C%20a%20decoding%20method%20that%0Amitigates%20hallucinations%20without%20requiring%20extra%20training.%20END%20leverages%20inner%0Aprobability%20changes%20across%20layers%20to%20individually%20quantify%20the%20factual%0Aknowledge%20required%20for%20each%20candidate%20token%2C%20and%20adjusts%20the%20final%20predicting%0Adistribution%20to%20prioritize%20tokens%20with%20higher%20factuality.%20Experiments%20on%20both%0Ahallucination%20and%20QA%20benchmarks%20demonstrate%20that%20END%20significantly%20enhances%20the%0Atruthfulness%20and%20informativeness%20of%20generated%20content%20while%20maintaining%20robust%0AQA%20accuracy.%20Moreover%2C%20our%20work%20provides%20a%20deeper%20perspective%20on%20understanding%0Athe%20correlations%20between%20inherent%20knowledge%20and%20output%20factuality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03199v1&entry.124074799=Read"},
{"title": "Kineto-Dynamical Planning and Accurate Execution of Minimum-Time\n  Maneuvers on Three-Dimensional Circuits", "author": "Mattia Piccinini and Sebastiano Taddei and Johannes Betz and Francesco Biral", "abstract": "  Online planning and execution of minimum-time maneuvers on three-dimensional\n(3D) circuits is an open challenge in autonomous vehicle racing. In this paper,\nwe present an artificial race driver (ARD) to learn the vehicle dynamics, plan\nand execute minimum-time maneuvers on a 3D track. ARD integrates a novel\nkineto-dynamical (KD) vehicle model for trajectory planning with economic\nnonlinear model predictive control (E-NMPC). We use a high-fidelity vehicle\nsimulator (VS) to compare the closed-loop ARD results with a minimum-lap-time\noptimal control problem (MLT-VS), solved offline with the same VS. Our ARD sets\nlap times close to the MLT-VS, and the new KD model outperforms a literature\nbenchmark. Finally, we study the vehicle trajectories, to assess the\nre-planning capabilities of ARD under execution errors. A video with the main\nresults is available as supplementary material.\n", "link": "http://arxiv.org/abs/2502.03454v1", "date": "2025-02-05", "relevancy": 2.0627, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5174}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5162}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Kineto-Dynamical%20Planning%20and%20Accurate%20Execution%20of%20Minimum-Time%0A%20%20Maneuvers%20on%20Three-Dimensional%20Circuits&body=Title%3A%20Kineto-Dynamical%20Planning%20and%20Accurate%20Execution%20of%20Minimum-Time%0A%20%20Maneuvers%20on%20Three-Dimensional%20Circuits%0AAuthor%3A%20Mattia%20Piccinini%20and%20Sebastiano%20Taddei%20and%20Johannes%20Betz%20and%20Francesco%20Biral%0AAbstract%3A%20%20%20Online%20planning%20and%20execution%20of%20minimum-time%20maneuvers%20on%20three-dimensional%0A%283D%29%20circuits%20is%20an%20open%20challenge%20in%20autonomous%20vehicle%20racing.%20In%20this%20paper%2C%0Awe%20present%20an%20artificial%20race%20driver%20%28ARD%29%20to%20learn%20the%20vehicle%20dynamics%2C%20plan%0Aand%20execute%20minimum-time%20maneuvers%20on%20a%203D%20track.%20ARD%20integrates%20a%20novel%0Akineto-dynamical%20%28KD%29%20vehicle%20model%20for%20trajectory%20planning%20with%20economic%0Anonlinear%20model%20predictive%20control%20%28E-NMPC%29.%20We%20use%20a%20high-fidelity%20vehicle%0Asimulator%20%28VS%29%20to%20compare%20the%20closed-loop%20ARD%20results%20with%20a%20minimum-lap-time%0Aoptimal%20control%20problem%20%28MLT-VS%29%2C%20solved%20offline%20with%20the%20same%20VS.%20Our%20ARD%20sets%0Alap%20times%20close%20to%20the%20MLT-VS%2C%20and%20the%20new%20KD%20model%20outperforms%20a%20literature%0Abenchmark.%20Finally%2C%20we%20study%20the%20vehicle%20trajectories%2C%20to%20assess%20the%0Are-planning%20capabilities%20of%20ARD%20under%20execution%20errors.%20A%20video%20with%20the%20main%0Aresults%20is%20available%20as%20supplementary%20material.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03454v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKineto-Dynamical%2520Planning%2520and%2520Accurate%2520Execution%2520of%2520Minimum-Time%250A%2520%2520Maneuvers%2520on%2520Three-Dimensional%2520Circuits%26entry.906535625%3DMattia%2520Piccinini%2520and%2520Sebastiano%2520Taddei%2520and%2520Johannes%2520Betz%2520and%2520Francesco%2520Biral%26entry.1292438233%3D%2520%2520Online%2520planning%2520and%2520execution%2520of%2520minimum-time%2520maneuvers%2520on%2520three-dimensional%250A%25283D%2529%2520circuits%2520is%2520an%2520open%2520challenge%2520in%2520autonomous%2520vehicle%2520racing.%2520In%2520this%2520paper%252C%250Awe%2520present%2520an%2520artificial%2520race%2520driver%2520%2528ARD%2529%2520to%2520learn%2520the%2520vehicle%2520dynamics%252C%2520plan%250Aand%2520execute%2520minimum-time%2520maneuvers%2520on%2520a%25203D%2520track.%2520ARD%2520integrates%2520a%2520novel%250Akineto-dynamical%2520%2528KD%2529%2520vehicle%2520model%2520for%2520trajectory%2520planning%2520with%2520economic%250Anonlinear%2520model%2520predictive%2520control%2520%2528E-NMPC%2529.%2520We%2520use%2520a%2520high-fidelity%2520vehicle%250Asimulator%2520%2528VS%2529%2520to%2520compare%2520the%2520closed-loop%2520ARD%2520results%2520with%2520a%2520minimum-lap-time%250Aoptimal%2520control%2520problem%2520%2528MLT-VS%2529%252C%2520solved%2520offline%2520with%2520the%2520same%2520VS.%2520Our%2520ARD%2520sets%250Alap%2520times%2520close%2520to%2520the%2520MLT-VS%252C%2520and%2520the%2520new%2520KD%2520model%2520outperforms%2520a%2520literature%250Abenchmark.%2520Finally%252C%2520we%2520study%2520the%2520vehicle%2520trajectories%252C%2520to%2520assess%2520the%250Are-planning%2520capabilities%2520of%2520ARD%2520under%2520execution%2520errors.%2520A%2520video%2520with%2520the%2520main%250Aresults%2520is%2520available%2520as%2520supplementary%2520material.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03454v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Kineto-Dynamical%20Planning%20and%20Accurate%20Execution%20of%20Minimum-Time%0A%20%20Maneuvers%20on%20Three-Dimensional%20Circuits&entry.906535625=Mattia%20Piccinini%20and%20Sebastiano%20Taddei%20and%20Johannes%20Betz%20and%20Francesco%20Biral&entry.1292438233=%20%20Online%20planning%20and%20execution%20of%20minimum-time%20maneuvers%20on%20three-dimensional%0A%283D%29%20circuits%20is%20an%20open%20challenge%20in%20autonomous%20vehicle%20racing.%20In%20this%20paper%2C%0Awe%20present%20an%20artificial%20race%20driver%20%28ARD%29%20to%20learn%20the%20vehicle%20dynamics%2C%20plan%0Aand%20execute%20minimum-time%20maneuvers%20on%20a%203D%20track.%20ARD%20integrates%20a%20novel%0Akineto-dynamical%20%28KD%29%20vehicle%20model%20for%20trajectory%20planning%20with%20economic%0Anonlinear%20model%20predictive%20control%20%28E-NMPC%29.%20We%20use%20a%20high-fidelity%20vehicle%0Asimulator%20%28VS%29%20to%20compare%20the%20closed-loop%20ARD%20results%20with%20a%20minimum-lap-time%0Aoptimal%20control%20problem%20%28MLT-VS%29%2C%20solved%20offline%20with%20the%20same%20VS.%20Our%20ARD%20sets%0Alap%20times%20close%20to%20the%20MLT-VS%2C%20and%20the%20new%20KD%20model%20outperforms%20a%20literature%0Abenchmark.%20Finally%2C%20we%20study%20the%20vehicle%20trajectories%2C%20to%20assess%20the%0Are-planning%20capabilities%20of%20ARD%20under%20execution%20errors.%20A%20video%20with%20the%20main%0Aresults%20is%20available%20as%20supplementary%20material.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03454v1&entry.124074799=Read"},
{"title": "Brief analysis of DeepSeek R1 and its implications for Generative AI", "author": "Sarah Mercer and Samuel Spillard and Daniel P. Martin", "abstract": "  In late January 2025, DeepSeek released their new reasoning model (DeepSeek\nR1); which was developed at a fraction of the cost yet remains competitive with\nOpenAI's models, despite the US's GPU export ban. This report discusses the\nmodel, and what its release means for the field of Generative AI more widely.\nWe briefly discuss other models released from China in recent weeks, their\nsimilarities; innovative use of Mixture of Experts (MoE), Reinforcement\nLearning (RL) and clever engineering appear to be key factors in the\ncapabilities of these models. This think piece has been written to a tight\ntimescale, providing broad coverage of the topic, and serves as introductory\nmaterial for those looking to understand the model's technical advancements, as\nwell as its place in the ecosystem. Several further areas of research are\nidentified.\n", "link": "http://arxiv.org/abs/2502.02523v2", "date": "2025-02-05", "relevancy": 2.0607, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5225}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5118}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brief%20analysis%20of%20DeepSeek%20R1%20and%20its%20implications%20for%20Generative%20AI&body=Title%3A%20Brief%20analysis%20of%20DeepSeek%20R1%20and%20its%20implications%20for%20Generative%20AI%0AAuthor%3A%20Sarah%20Mercer%20and%20Samuel%20Spillard%20and%20Daniel%20P.%20Martin%0AAbstract%3A%20%20%20In%20late%20January%202025%2C%20DeepSeek%20released%20their%20new%20reasoning%20model%20%28DeepSeek%0AR1%29%3B%20which%20was%20developed%20at%20a%20fraction%20of%20the%20cost%20yet%20remains%20competitive%20with%0AOpenAI%27s%20models%2C%20despite%20the%20US%27s%20GPU%20export%20ban.%20This%20report%20discusses%20the%0Amodel%2C%20and%20what%20its%20release%20means%20for%20the%20field%20of%20Generative%20AI%20more%20widely.%0AWe%20briefly%20discuss%20other%20models%20released%20from%20China%20in%20recent%20weeks%2C%20their%0Asimilarities%3B%20innovative%20use%20of%20Mixture%20of%20Experts%20%28MoE%29%2C%20Reinforcement%0ALearning%20%28RL%29%20and%20clever%20engineering%20appear%20to%20be%20key%20factors%20in%20the%0Acapabilities%20of%20these%20models.%20This%20think%20piece%20has%20been%20written%20to%20a%20tight%0Atimescale%2C%20providing%20broad%20coverage%20of%20the%20topic%2C%20and%20serves%20as%20introductory%0Amaterial%20for%20those%20looking%20to%20understand%20the%20model%27s%20technical%20advancements%2C%20as%0Awell%20as%20its%20place%20in%20the%20ecosystem.%20Several%20further%20areas%20of%20research%20are%0Aidentified.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.02523v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrief%2520analysis%2520of%2520DeepSeek%2520R1%2520and%2520its%2520implications%2520for%2520Generative%2520AI%26entry.906535625%3DSarah%2520Mercer%2520and%2520Samuel%2520Spillard%2520and%2520Daniel%2520P.%2520Martin%26entry.1292438233%3D%2520%2520In%2520late%2520January%25202025%252C%2520DeepSeek%2520released%2520their%2520new%2520reasoning%2520model%2520%2528DeepSeek%250AR1%2529%253B%2520which%2520was%2520developed%2520at%2520a%2520fraction%2520of%2520the%2520cost%2520yet%2520remains%2520competitive%2520with%250AOpenAI%2527s%2520models%252C%2520despite%2520the%2520US%2527s%2520GPU%2520export%2520ban.%2520This%2520report%2520discusses%2520the%250Amodel%252C%2520and%2520what%2520its%2520release%2520means%2520for%2520the%2520field%2520of%2520Generative%2520AI%2520more%2520widely.%250AWe%2520briefly%2520discuss%2520other%2520models%2520released%2520from%2520China%2520in%2520recent%2520weeks%252C%2520their%250Asimilarities%253B%2520innovative%2520use%2520of%2520Mixture%2520of%2520Experts%2520%2528MoE%2529%252C%2520Reinforcement%250ALearning%2520%2528RL%2529%2520and%2520clever%2520engineering%2520appear%2520to%2520be%2520key%2520factors%2520in%2520the%250Acapabilities%2520of%2520these%2520models.%2520This%2520think%2520piece%2520has%2520been%2520written%2520to%2520a%2520tight%250Atimescale%252C%2520providing%2520broad%2520coverage%2520of%2520the%2520topic%252C%2520and%2520serves%2520as%2520introductory%250Amaterial%2520for%2520those%2520looking%2520to%2520understand%2520the%2520model%2527s%2520technical%2520advancements%252C%2520as%250Awell%2520as%2520its%2520place%2520in%2520the%2520ecosystem.%2520Several%2520further%2520areas%2520of%2520research%2520are%250Aidentified.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.02523v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brief%20analysis%20of%20DeepSeek%20R1%20and%20its%20implications%20for%20Generative%20AI&entry.906535625=Sarah%20Mercer%20and%20Samuel%20Spillard%20and%20Daniel%20P.%20Martin&entry.1292438233=%20%20In%20late%20January%202025%2C%20DeepSeek%20released%20their%20new%20reasoning%20model%20%28DeepSeek%0AR1%29%3B%20which%20was%20developed%20at%20a%20fraction%20of%20the%20cost%20yet%20remains%20competitive%20with%0AOpenAI%27s%20models%2C%20despite%20the%20US%27s%20GPU%20export%20ban.%20This%20report%20discusses%20the%0Amodel%2C%20and%20what%20its%20release%20means%20for%20the%20field%20of%20Generative%20AI%20more%20widely.%0AWe%20briefly%20discuss%20other%20models%20released%20from%20China%20in%20recent%20weeks%2C%20their%0Asimilarities%3B%20innovative%20use%20of%20Mixture%20of%20Experts%20%28MoE%29%2C%20Reinforcement%0ALearning%20%28RL%29%20and%20clever%20engineering%20appear%20to%20be%20key%20factors%20in%20the%0Acapabilities%20of%20these%20models.%20This%20think%20piece%20has%20been%20written%20to%20a%20tight%0Atimescale%2C%20providing%20broad%20coverage%20of%20the%20topic%2C%20and%20serves%20as%20introductory%0Amaterial%20for%20those%20looking%20to%20understand%20the%20model%27s%20technical%20advancements%2C%20as%0Awell%20as%20its%20place%20in%20the%20ecosystem.%20Several%20further%20areas%20of%20research%20are%0Aidentified.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.02523v2&entry.124074799=Read"},
{"title": "Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient\n  Zeroth-order LLM Fine-tuning", "author": "Qitao Tan and Jun Liu and Zheng Zhan and Caiwei Ding and Yanzhi Wang and Jin Lu and Geng Yuan", "abstract": "  Large language models (LLMs) excel across various tasks, but standard\nfirst-order (FO) fine-tuning demands considerable memory, significantly\nlimiting real-world deployment. Recently, zeroth-order (ZO) optimization stood\nout as a promising memory-efficient training paradigm, avoiding backward passes\nand relying solely on forward passes for gradient estimation, making it\nattractive for resource-constrained scenarios. However, ZO method lags far\nbehind FO method in both convergence speed and accuracy. To bridge the gap, we\nintroduce a novel layer-wise divergence analysis that uncovers the distinct\nupdate pattern of FO and ZO optimization. Aiming to resemble the learning\ncapacity of FO method from the findings, we propose \\textbf{Di}vergence-driven\n\\textbf{Z}eroth-\\textbf{O}rder (\\textbf{DiZO}) optimization. DiZO conducts\ndivergence-driven layer adaptation by incorporating projections to ZO updates,\ngenerating diverse-magnitude updates precisely scaled to layer-wise individual\noptimization needs. Our results demonstrate that DiZO significantly reduces the\nneeded iterations for convergence without sacrificing throughput, cutting\ntraining GPU hours by up to 48\\% on various datasets. Moreover, DiZO\nconsistently outperforms the representative ZO baselines in fine-tuning\nRoBERTa-large, OPT-series, and Llama-series on downstream tasks and, in some\ncases, even surpasses memory-intensive FO fine-tuning.\n", "link": "http://arxiv.org/abs/2502.03304v1", "date": "2025-02-05", "relevancy": 2.0481, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5152}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5109}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harmony%20in%20Divergence%3A%20Towards%20Fast%2C%20Accurate%2C%20and%20Memory-efficient%0A%20%20Zeroth-order%20LLM%20Fine-tuning&body=Title%3A%20Harmony%20in%20Divergence%3A%20Towards%20Fast%2C%20Accurate%2C%20and%20Memory-efficient%0A%20%20Zeroth-order%20LLM%20Fine-tuning%0AAuthor%3A%20Qitao%20Tan%20and%20Jun%20Liu%20and%20Zheng%20Zhan%20and%20Caiwei%20Ding%20and%20Yanzhi%20Wang%20and%20Jin%20Lu%20and%20Geng%20Yuan%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20excel%20across%20various%20tasks%2C%20but%20standard%0Afirst-order%20%28FO%29%20fine-tuning%20demands%20considerable%20memory%2C%20significantly%0Alimiting%20real-world%20deployment.%20Recently%2C%20zeroth-order%20%28ZO%29%20optimization%20stood%0Aout%20as%20a%20promising%20memory-efficient%20training%20paradigm%2C%20avoiding%20backward%20passes%0Aand%20relying%20solely%20on%20forward%20passes%20for%20gradient%20estimation%2C%20making%20it%0Aattractive%20for%20resource-constrained%20scenarios.%20However%2C%20ZO%20method%20lags%20far%0Abehind%20FO%20method%20in%20both%20convergence%20speed%20and%20accuracy.%20To%20bridge%20the%20gap%2C%20we%0Aintroduce%20a%20novel%20layer-wise%20divergence%20analysis%20that%20uncovers%20the%20distinct%0Aupdate%20pattern%20of%20FO%20and%20ZO%20optimization.%20Aiming%20to%20resemble%20the%20learning%0Acapacity%20of%20FO%20method%20from%20the%20findings%2C%20we%20propose%20%5Ctextbf%7BDi%7Dvergence-driven%0A%5Ctextbf%7BZ%7Deroth-%5Ctextbf%7BO%7Drder%20%28%5Ctextbf%7BDiZO%7D%29%20optimization.%20DiZO%20conducts%0Adivergence-driven%20layer%20adaptation%20by%20incorporating%20projections%20to%20ZO%20updates%2C%0Agenerating%20diverse-magnitude%20updates%20precisely%20scaled%20to%20layer-wise%20individual%0Aoptimization%20needs.%20Our%20results%20demonstrate%20that%20DiZO%20significantly%20reduces%20the%0Aneeded%20iterations%20for%20convergence%20without%20sacrificing%20throughput%2C%20cutting%0Atraining%20GPU%20hours%20by%20up%20to%2048%5C%25%20on%20various%20datasets.%20Moreover%2C%20DiZO%0Aconsistently%20outperforms%20the%20representative%20ZO%20baselines%20in%20fine-tuning%0ARoBERTa-large%2C%20OPT-series%2C%20and%20Llama-series%20on%20downstream%20tasks%20and%2C%20in%20some%0Acases%2C%20even%20surpasses%20memory-intensive%20FO%20fine-tuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03304v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarmony%2520in%2520Divergence%253A%2520Towards%2520Fast%252C%2520Accurate%252C%2520and%2520Memory-efficient%250A%2520%2520Zeroth-order%2520LLM%2520Fine-tuning%26entry.906535625%3DQitao%2520Tan%2520and%2520Jun%2520Liu%2520and%2520Zheng%2520Zhan%2520and%2520Caiwei%2520Ding%2520and%2520Yanzhi%2520Wang%2520and%2520Jin%2520Lu%2520and%2520Geng%2520Yuan%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520excel%2520across%2520various%2520tasks%252C%2520but%2520standard%250Afirst-order%2520%2528FO%2529%2520fine-tuning%2520demands%2520considerable%2520memory%252C%2520significantly%250Alimiting%2520real-world%2520deployment.%2520Recently%252C%2520zeroth-order%2520%2528ZO%2529%2520optimization%2520stood%250Aout%2520as%2520a%2520promising%2520memory-efficient%2520training%2520paradigm%252C%2520avoiding%2520backward%2520passes%250Aand%2520relying%2520solely%2520on%2520forward%2520passes%2520for%2520gradient%2520estimation%252C%2520making%2520it%250Aattractive%2520for%2520resource-constrained%2520scenarios.%2520However%252C%2520ZO%2520method%2520lags%2520far%250Abehind%2520FO%2520method%2520in%2520both%2520convergence%2520speed%2520and%2520accuracy.%2520To%2520bridge%2520the%2520gap%252C%2520we%250Aintroduce%2520a%2520novel%2520layer-wise%2520divergence%2520analysis%2520that%2520uncovers%2520the%2520distinct%250Aupdate%2520pattern%2520of%2520FO%2520and%2520ZO%2520optimization.%2520Aiming%2520to%2520resemble%2520the%2520learning%250Acapacity%2520of%2520FO%2520method%2520from%2520the%2520findings%252C%2520we%2520propose%2520%255Ctextbf%257BDi%257Dvergence-driven%250A%255Ctextbf%257BZ%257Deroth-%255Ctextbf%257BO%257Drder%2520%2528%255Ctextbf%257BDiZO%257D%2529%2520optimization.%2520DiZO%2520conducts%250Adivergence-driven%2520layer%2520adaptation%2520by%2520incorporating%2520projections%2520to%2520ZO%2520updates%252C%250Agenerating%2520diverse-magnitude%2520updates%2520precisely%2520scaled%2520to%2520layer-wise%2520individual%250Aoptimization%2520needs.%2520Our%2520results%2520demonstrate%2520that%2520DiZO%2520significantly%2520reduces%2520the%250Aneeded%2520iterations%2520for%2520convergence%2520without%2520sacrificing%2520throughput%252C%2520cutting%250Atraining%2520GPU%2520hours%2520by%2520up%2520to%252048%255C%2525%2520on%2520various%2520datasets.%2520Moreover%252C%2520DiZO%250Aconsistently%2520outperforms%2520the%2520representative%2520ZO%2520baselines%2520in%2520fine-tuning%250ARoBERTa-large%252C%2520OPT-series%252C%2520and%2520Llama-series%2520on%2520downstream%2520tasks%2520and%252C%2520in%2520some%250Acases%252C%2520even%2520surpasses%2520memory-intensive%2520FO%2520fine-tuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03304v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harmony%20in%20Divergence%3A%20Towards%20Fast%2C%20Accurate%2C%20and%20Memory-efficient%0A%20%20Zeroth-order%20LLM%20Fine-tuning&entry.906535625=Qitao%20Tan%20and%20Jun%20Liu%20and%20Zheng%20Zhan%20and%20Caiwei%20Ding%20and%20Yanzhi%20Wang%20and%20Jin%20Lu%20and%20Geng%20Yuan&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20excel%20across%20various%20tasks%2C%20but%20standard%0Afirst-order%20%28FO%29%20fine-tuning%20demands%20considerable%20memory%2C%20significantly%0Alimiting%20real-world%20deployment.%20Recently%2C%20zeroth-order%20%28ZO%29%20optimization%20stood%0Aout%20as%20a%20promising%20memory-efficient%20training%20paradigm%2C%20avoiding%20backward%20passes%0Aand%20relying%20solely%20on%20forward%20passes%20for%20gradient%20estimation%2C%20making%20it%0Aattractive%20for%20resource-constrained%20scenarios.%20However%2C%20ZO%20method%20lags%20far%0Abehind%20FO%20method%20in%20both%20convergence%20speed%20and%20accuracy.%20To%20bridge%20the%20gap%2C%20we%0Aintroduce%20a%20novel%20layer-wise%20divergence%20analysis%20that%20uncovers%20the%20distinct%0Aupdate%20pattern%20of%20FO%20and%20ZO%20optimization.%20Aiming%20to%20resemble%20the%20learning%0Acapacity%20of%20FO%20method%20from%20the%20findings%2C%20we%20propose%20%5Ctextbf%7BDi%7Dvergence-driven%0A%5Ctextbf%7BZ%7Deroth-%5Ctextbf%7BO%7Drder%20%28%5Ctextbf%7BDiZO%7D%29%20optimization.%20DiZO%20conducts%0Adivergence-driven%20layer%20adaptation%20by%20incorporating%20projections%20to%20ZO%20updates%2C%0Agenerating%20diverse-magnitude%20updates%20precisely%20scaled%20to%20layer-wise%20individual%0Aoptimization%20needs.%20Our%20results%20demonstrate%20that%20DiZO%20significantly%20reduces%20the%0Aneeded%20iterations%20for%20convergence%20without%20sacrificing%20throughput%2C%20cutting%0Atraining%20GPU%20hours%20by%20up%20to%2048%5C%25%20on%20various%20datasets.%20Moreover%2C%20DiZO%0Aconsistently%20outperforms%20the%20representative%20ZO%20baselines%20in%20fine-tuning%0ARoBERTa-large%2C%20OPT-series%2C%20and%20Llama-series%20on%20downstream%20tasks%20and%2C%20in%20some%0Acases%2C%20even%20surpasses%20memory-intensive%20FO%20fine-tuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03304v1&entry.124074799=Read"},
{"title": "Concept Based Explanations and Class Contrasting", "author": "Rudolf Herdt and Daniel Otero Baguer", "abstract": "  Explaining deep neural networks is challenging, due to their large size and\nnon-linearity. In this paper, we introduce a concept-based explanation method,\nin order to explain the prediction for an individual class, as well as\ncontrasting any two classes, i.e. explain why the model predicts one class over\nthe other. We test it on several openly available classification models trained\non ImageNet1K, as well as on a segmentation model trained to detect tumor in\nstained tissue samples. We perform both qualitative and quantitative tests. For\nexample, for a ResNet50 model from pytorch model zoo, we can use the\nexplanation for why the model predicts a class 'A' to automatically select six\ndataset crops where the model does not predict class 'A'. The model then\npredicts class 'A' again for the newly combined image in 71\\% of the cases\n(works for 710 out of the 1000 classes). The code including an .ipynb example\nis available on git:\nhttps://github.com/rherdt185/concept-based-explanations-and-class-contrasting.\n", "link": "http://arxiv.org/abs/2502.03422v1", "date": "2025-02-05", "relevancy": 2.046, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5264}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.517}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4945}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Concept%20Based%20Explanations%20and%20Class%20Contrasting&body=Title%3A%20Concept%20Based%20Explanations%20and%20Class%20Contrasting%0AAuthor%3A%20Rudolf%20Herdt%20and%20Daniel%20Otero%20Baguer%0AAbstract%3A%20%20%20Explaining%20deep%20neural%20networks%20is%20challenging%2C%20due%20to%20their%20large%20size%20and%0Anon-linearity.%20In%20this%20paper%2C%20we%20introduce%20a%20concept-based%20explanation%20method%2C%0Ain%20order%20to%20explain%20the%20prediction%20for%20an%20individual%20class%2C%20as%20well%20as%0Acontrasting%20any%20two%20classes%2C%20i.e.%20explain%20why%20the%20model%20predicts%20one%20class%20over%0Athe%20other.%20We%20test%20it%20on%20several%20openly%20available%20classification%20models%20trained%0Aon%20ImageNet1K%2C%20as%20well%20as%20on%20a%20segmentation%20model%20trained%20to%20detect%20tumor%20in%0Astained%20tissue%20samples.%20We%20perform%20both%20qualitative%20and%20quantitative%20tests.%20For%0Aexample%2C%20for%20a%20ResNet50%20model%20from%20pytorch%20model%20zoo%2C%20we%20can%20use%20the%0Aexplanation%20for%20why%20the%20model%20predicts%20a%20class%20%27A%27%20to%20automatically%20select%20six%0Adataset%20crops%20where%20the%20model%20does%20not%20predict%20class%20%27A%27.%20The%20model%20then%0Apredicts%20class%20%27A%27%20again%20for%20the%20newly%20combined%20image%20in%2071%5C%25%20of%20the%20cases%0A%28works%20for%20710%20out%20of%20the%201000%20classes%29.%20The%20code%20including%20an%20.ipynb%20example%0Ais%20available%20on%20git%3A%0Ahttps%3A//github.com/rherdt185/concept-based-explanations-and-class-contrasting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03422v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConcept%2520Based%2520Explanations%2520and%2520Class%2520Contrasting%26entry.906535625%3DRudolf%2520Herdt%2520and%2520Daniel%2520Otero%2520Baguer%26entry.1292438233%3D%2520%2520Explaining%2520deep%2520neural%2520networks%2520is%2520challenging%252C%2520due%2520to%2520their%2520large%2520size%2520and%250Anon-linearity.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520concept-based%2520explanation%2520method%252C%250Ain%2520order%2520to%2520explain%2520the%2520prediction%2520for%2520an%2520individual%2520class%252C%2520as%2520well%2520as%250Acontrasting%2520any%2520two%2520classes%252C%2520i.e.%2520explain%2520why%2520the%2520model%2520predicts%2520one%2520class%2520over%250Athe%2520other.%2520We%2520test%2520it%2520on%2520several%2520openly%2520available%2520classification%2520models%2520trained%250Aon%2520ImageNet1K%252C%2520as%2520well%2520as%2520on%2520a%2520segmentation%2520model%2520trained%2520to%2520detect%2520tumor%2520in%250Astained%2520tissue%2520samples.%2520We%2520perform%2520both%2520qualitative%2520and%2520quantitative%2520tests.%2520For%250Aexample%252C%2520for%2520a%2520ResNet50%2520model%2520from%2520pytorch%2520model%2520zoo%252C%2520we%2520can%2520use%2520the%250Aexplanation%2520for%2520why%2520the%2520model%2520predicts%2520a%2520class%2520%2527A%2527%2520to%2520automatically%2520select%2520six%250Adataset%2520crops%2520where%2520the%2520model%2520does%2520not%2520predict%2520class%2520%2527A%2527.%2520The%2520model%2520then%250Apredicts%2520class%2520%2527A%2527%2520again%2520for%2520the%2520newly%2520combined%2520image%2520in%252071%255C%2525%2520of%2520the%2520cases%250A%2528works%2520for%2520710%2520out%2520of%2520the%25201000%2520classes%2529.%2520The%2520code%2520including%2520an%2520.ipynb%2520example%250Ais%2520available%2520on%2520git%253A%250Ahttps%253A//github.com/rherdt185/concept-based-explanations-and-class-contrasting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03422v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Concept%20Based%20Explanations%20and%20Class%20Contrasting&entry.906535625=Rudolf%20Herdt%20and%20Daniel%20Otero%20Baguer&entry.1292438233=%20%20Explaining%20deep%20neural%20networks%20is%20challenging%2C%20due%20to%20their%20large%20size%20and%0Anon-linearity.%20In%20this%20paper%2C%20we%20introduce%20a%20concept-based%20explanation%20method%2C%0Ain%20order%20to%20explain%20the%20prediction%20for%20an%20individual%20class%2C%20as%20well%20as%0Acontrasting%20any%20two%20classes%2C%20i.e.%20explain%20why%20the%20model%20predicts%20one%20class%20over%0Athe%20other.%20We%20test%20it%20on%20several%20openly%20available%20classification%20models%20trained%0Aon%20ImageNet1K%2C%20as%20well%20as%20on%20a%20segmentation%20model%20trained%20to%20detect%20tumor%20in%0Astained%20tissue%20samples.%20We%20perform%20both%20qualitative%20and%20quantitative%20tests.%20For%0Aexample%2C%20for%20a%20ResNet50%20model%20from%20pytorch%20model%20zoo%2C%20we%20can%20use%20the%0Aexplanation%20for%20why%20the%20model%20predicts%20a%20class%20%27A%27%20to%20automatically%20select%20six%0Adataset%20crops%20where%20the%20model%20does%20not%20predict%20class%20%27A%27.%20The%20model%20then%0Apredicts%20class%20%27A%27%20again%20for%20the%20newly%20combined%20image%20in%2071%5C%25%20of%20the%20cases%0A%28works%20for%20710%20out%20of%20the%201000%20classes%29.%20The%20code%20including%20an%20.ipynb%20example%0Ais%20available%20on%20git%3A%0Ahttps%3A//github.com/rherdt185/concept-based-explanations-and-class-contrasting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03422v1&entry.124074799=Read"},
{"title": "LNQ 2023 challenge: Benchmark of weakly-supervised techniques for\n  mediastinal lymph node quantification", "author": "Reuben Dorent and Roya Khajavi and Tagwa Idris and Erik Ziegler and Bhanusupriya Somarouthu and Heather Jacene and Ann LaCasce and Jonathan Deissler and Jan Ehrhardt and Sofija Engelson and Stefan M. Fischer and Yun Gu and Heinz Handels and Satoshi Kasai and Satoshi Kondo and Klaus Maier-Hein and Julia A. Schnabel and Guotai Wang and Litingyu Wang and Tassilo Wald and Guang-Zhong Yang and Hanxiao Zhang and Minghui Zhang and Steve Pieper and Gordon Harris and Ron Kikinis and Tina Kapur", "abstract": "  Accurate assessment of lymph node size in 3D CT scans is crucial for cancer\nstaging, therapeutic management, and monitoring treatment response. Existing\nstate-of-the-art segmentation frameworks in medical imaging often rely on fully\nannotated datasets. However, for lymph node segmentation, these datasets are\ntypically small due to the extensive time and expertise required to annotate\nthe numerous lymph nodes in 3D CT scans. Weakly-supervised learning, which\nleverages incomplete or noisy annotations, has recently gained interest in the\nmedical imaging community as a potential solution. Despite the variety of\nweakly-supervised techniques proposed, most have been validated only on private\ndatasets or small publicly available datasets. To address this limitation, the\nMediastinal Lymph Node Quantification (LNQ) challenge was organized in\nconjunction with the 26th International Conference on Medical Image Computing\nand Computer Assisted Intervention (MICCAI 2023). This challenge aimed to\nadvance weakly-supervised segmentation methods by providing a new, partially\nannotated dataset and a robust evaluation framework. A total of 16 teams from 5\ncountries submitted predictions to the validation leaderboard, and 6 teams from\n3 countries participated in the evaluation phase. The results highlighted both\nthe potential and the current limitations of weakly-supervised approaches. On\none hand, weakly-supervised approaches obtained relatively good performance\nwith a median Dice score of $61.0\\%$. On the other hand, top-ranked teams, with\na median Dice score exceeding $70\\%$, boosted their performance by leveraging\nsmaller but fully annotated datasets to combine weak supervision and full\nsupervision. This highlights both the promise of weakly-supervised methods and\nthe ongoing need for high-quality, fully annotated data to achieve higher\nsegmentation performance.\n", "link": "http://arxiv.org/abs/2408.10069v2", "date": "2025-02-05", "relevancy": 2.0457, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5549}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5072}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4983}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LNQ%202023%20challenge%3A%20Benchmark%20of%20weakly-supervised%20techniques%20for%0A%20%20mediastinal%20lymph%20node%20quantification&body=Title%3A%20LNQ%202023%20challenge%3A%20Benchmark%20of%20weakly-supervised%20techniques%20for%0A%20%20mediastinal%20lymph%20node%20quantification%0AAuthor%3A%20Reuben%20Dorent%20and%20Roya%20Khajavi%20and%20Tagwa%20Idris%20and%20Erik%20Ziegler%20and%20Bhanusupriya%20Somarouthu%20and%20Heather%20Jacene%20and%20Ann%20LaCasce%20and%20Jonathan%20Deissler%20and%20Jan%20Ehrhardt%20and%20Sofija%20Engelson%20and%20Stefan%20M.%20Fischer%20and%20Yun%20Gu%20and%20Heinz%20Handels%20and%20Satoshi%20Kasai%20and%20Satoshi%20Kondo%20and%20Klaus%20Maier-Hein%20and%20Julia%20A.%20Schnabel%20and%20Guotai%20Wang%20and%20Litingyu%20Wang%20and%20Tassilo%20Wald%20and%20Guang-Zhong%20Yang%20and%20Hanxiao%20Zhang%20and%20Minghui%20Zhang%20and%20Steve%20Pieper%20and%20Gordon%20Harris%20and%20Ron%20Kikinis%20and%20Tina%20Kapur%0AAbstract%3A%20%20%20Accurate%20assessment%20of%20lymph%20node%20size%20in%203D%20CT%20scans%20is%20crucial%20for%20cancer%0Astaging%2C%20therapeutic%20management%2C%20and%20monitoring%20treatment%20response.%20Existing%0Astate-of-the-art%20segmentation%20frameworks%20in%20medical%20imaging%20often%20rely%20on%20fully%0Aannotated%20datasets.%20However%2C%20for%20lymph%20node%20segmentation%2C%20these%20datasets%20are%0Atypically%20small%20due%20to%20the%20extensive%20time%20and%20expertise%20required%20to%20annotate%0Athe%20numerous%20lymph%20nodes%20in%203D%20CT%20scans.%20Weakly-supervised%20learning%2C%20which%0Aleverages%20incomplete%20or%20noisy%20annotations%2C%20has%20recently%20gained%20interest%20in%20the%0Amedical%20imaging%20community%20as%20a%20potential%20solution.%20Despite%20the%20variety%20of%0Aweakly-supervised%20techniques%20proposed%2C%20most%20have%20been%20validated%20only%20on%20private%0Adatasets%20or%20small%20publicly%20available%20datasets.%20To%20address%20this%20limitation%2C%20the%0AMediastinal%20Lymph%20Node%20Quantification%20%28LNQ%29%20challenge%20was%20organized%20in%0Aconjunction%20with%20the%2026th%20International%20Conference%20on%20Medical%20Image%20Computing%0Aand%20Computer%20Assisted%20Intervention%20%28MICCAI%202023%29.%20This%20challenge%20aimed%20to%0Aadvance%20weakly-supervised%20segmentation%20methods%20by%20providing%20a%20new%2C%20partially%0Aannotated%20dataset%20and%20a%20robust%20evaluation%20framework.%20A%20total%20of%2016%20teams%20from%205%0Acountries%20submitted%20predictions%20to%20the%20validation%20leaderboard%2C%20and%206%20teams%20from%0A3%20countries%20participated%20in%20the%20evaluation%20phase.%20The%20results%20highlighted%20both%0Athe%20potential%20and%20the%20current%20limitations%20of%20weakly-supervised%20approaches.%20On%0Aone%20hand%2C%20weakly-supervised%20approaches%20obtained%20relatively%20good%20performance%0Awith%20a%20median%20Dice%20score%20of%20%2461.0%5C%25%24.%20On%20the%20other%20hand%2C%20top-ranked%20teams%2C%20with%0Aa%20median%20Dice%20score%20exceeding%20%2470%5C%25%24%2C%20boosted%20their%20performance%20by%20leveraging%0Asmaller%20but%20fully%20annotated%20datasets%20to%20combine%20weak%20supervision%20and%20full%0Asupervision.%20This%20highlights%20both%20the%20promise%20of%20weakly-supervised%20methods%20and%0Athe%20ongoing%20need%20for%20high-quality%2C%20fully%20annotated%20data%20to%20achieve%20higher%0Asegmentation%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.10069v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLNQ%25202023%2520challenge%253A%2520Benchmark%2520of%2520weakly-supervised%2520techniques%2520for%250A%2520%2520mediastinal%2520lymph%2520node%2520quantification%26entry.906535625%3DReuben%2520Dorent%2520and%2520Roya%2520Khajavi%2520and%2520Tagwa%2520Idris%2520and%2520Erik%2520Ziegler%2520and%2520Bhanusupriya%2520Somarouthu%2520and%2520Heather%2520Jacene%2520and%2520Ann%2520LaCasce%2520and%2520Jonathan%2520Deissler%2520and%2520Jan%2520Ehrhardt%2520and%2520Sofija%2520Engelson%2520and%2520Stefan%2520M.%2520Fischer%2520and%2520Yun%2520Gu%2520and%2520Heinz%2520Handels%2520and%2520Satoshi%2520Kasai%2520and%2520Satoshi%2520Kondo%2520and%2520Klaus%2520Maier-Hein%2520and%2520Julia%2520A.%2520Schnabel%2520and%2520Guotai%2520Wang%2520and%2520Litingyu%2520Wang%2520and%2520Tassilo%2520Wald%2520and%2520Guang-Zhong%2520Yang%2520and%2520Hanxiao%2520Zhang%2520and%2520Minghui%2520Zhang%2520and%2520Steve%2520Pieper%2520and%2520Gordon%2520Harris%2520and%2520Ron%2520Kikinis%2520and%2520Tina%2520Kapur%26entry.1292438233%3D%2520%2520Accurate%2520assessment%2520of%2520lymph%2520node%2520size%2520in%25203D%2520CT%2520scans%2520is%2520crucial%2520for%2520cancer%250Astaging%252C%2520therapeutic%2520management%252C%2520and%2520monitoring%2520treatment%2520response.%2520Existing%250Astate-of-the-art%2520segmentation%2520frameworks%2520in%2520medical%2520imaging%2520often%2520rely%2520on%2520fully%250Aannotated%2520datasets.%2520However%252C%2520for%2520lymph%2520node%2520segmentation%252C%2520these%2520datasets%2520are%250Atypically%2520small%2520due%2520to%2520the%2520extensive%2520time%2520and%2520expertise%2520required%2520to%2520annotate%250Athe%2520numerous%2520lymph%2520nodes%2520in%25203D%2520CT%2520scans.%2520Weakly-supervised%2520learning%252C%2520which%250Aleverages%2520incomplete%2520or%2520noisy%2520annotations%252C%2520has%2520recently%2520gained%2520interest%2520in%2520the%250Amedical%2520imaging%2520community%2520as%2520a%2520potential%2520solution.%2520Despite%2520the%2520variety%2520of%250Aweakly-supervised%2520techniques%2520proposed%252C%2520most%2520have%2520been%2520validated%2520only%2520on%2520private%250Adatasets%2520or%2520small%2520publicly%2520available%2520datasets.%2520To%2520address%2520this%2520limitation%252C%2520the%250AMediastinal%2520Lymph%2520Node%2520Quantification%2520%2528LNQ%2529%2520challenge%2520was%2520organized%2520in%250Aconjunction%2520with%2520the%252026th%2520International%2520Conference%2520on%2520Medical%2520Image%2520Computing%250Aand%2520Computer%2520Assisted%2520Intervention%2520%2528MICCAI%25202023%2529.%2520This%2520challenge%2520aimed%2520to%250Aadvance%2520weakly-supervised%2520segmentation%2520methods%2520by%2520providing%2520a%2520new%252C%2520partially%250Aannotated%2520dataset%2520and%2520a%2520robust%2520evaluation%2520framework.%2520A%2520total%2520of%252016%2520teams%2520from%25205%250Acountries%2520submitted%2520predictions%2520to%2520the%2520validation%2520leaderboard%252C%2520and%25206%2520teams%2520from%250A3%2520countries%2520participated%2520in%2520the%2520evaluation%2520phase.%2520The%2520results%2520highlighted%2520both%250Athe%2520potential%2520and%2520the%2520current%2520limitations%2520of%2520weakly-supervised%2520approaches.%2520On%250Aone%2520hand%252C%2520weakly-supervised%2520approaches%2520obtained%2520relatively%2520good%2520performance%250Awith%2520a%2520median%2520Dice%2520score%2520of%2520%252461.0%255C%2525%2524.%2520On%2520the%2520other%2520hand%252C%2520top-ranked%2520teams%252C%2520with%250Aa%2520median%2520Dice%2520score%2520exceeding%2520%252470%255C%2525%2524%252C%2520boosted%2520their%2520performance%2520by%2520leveraging%250Asmaller%2520but%2520fully%2520annotated%2520datasets%2520to%2520combine%2520weak%2520supervision%2520and%2520full%250Asupervision.%2520This%2520highlights%2520both%2520the%2520promise%2520of%2520weakly-supervised%2520methods%2520and%250Athe%2520ongoing%2520need%2520for%2520high-quality%252C%2520fully%2520annotated%2520data%2520to%2520achieve%2520higher%250Asegmentation%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10069v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LNQ%202023%20challenge%3A%20Benchmark%20of%20weakly-supervised%20techniques%20for%0A%20%20mediastinal%20lymph%20node%20quantification&entry.906535625=Reuben%20Dorent%20and%20Roya%20Khajavi%20and%20Tagwa%20Idris%20and%20Erik%20Ziegler%20and%20Bhanusupriya%20Somarouthu%20and%20Heather%20Jacene%20and%20Ann%20LaCasce%20and%20Jonathan%20Deissler%20and%20Jan%20Ehrhardt%20and%20Sofija%20Engelson%20and%20Stefan%20M.%20Fischer%20and%20Yun%20Gu%20and%20Heinz%20Handels%20and%20Satoshi%20Kasai%20and%20Satoshi%20Kondo%20and%20Klaus%20Maier-Hein%20and%20Julia%20A.%20Schnabel%20and%20Guotai%20Wang%20and%20Litingyu%20Wang%20and%20Tassilo%20Wald%20and%20Guang-Zhong%20Yang%20and%20Hanxiao%20Zhang%20and%20Minghui%20Zhang%20and%20Steve%20Pieper%20and%20Gordon%20Harris%20and%20Ron%20Kikinis%20and%20Tina%20Kapur&entry.1292438233=%20%20Accurate%20assessment%20of%20lymph%20node%20size%20in%203D%20CT%20scans%20is%20crucial%20for%20cancer%0Astaging%2C%20therapeutic%20management%2C%20and%20monitoring%20treatment%20response.%20Existing%0Astate-of-the-art%20segmentation%20frameworks%20in%20medical%20imaging%20often%20rely%20on%20fully%0Aannotated%20datasets.%20However%2C%20for%20lymph%20node%20segmentation%2C%20these%20datasets%20are%0Atypically%20small%20due%20to%20the%20extensive%20time%20and%20expertise%20required%20to%20annotate%0Athe%20numerous%20lymph%20nodes%20in%203D%20CT%20scans.%20Weakly-supervised%20learning%2C%20which%0Aleverages%20incomplete%20or%20noisy%20annotations%2C%20has%20recently%20gained%20interest%20in%20the%0Amedical%20imaging%20community%20as%20a%20potential%20solution.%20Despite%20the%20variety%20of%0Aweakly-supervised%20techniques%20proposed%2C%20most%20have%20been%20validated%20only%20on%20private%0Adatasets%20or%20small%20publicly%20available%20datasets.%20To%20address%20this%20limitation%2C%20the%0AMediastinal%20Lymph%20Node%20Quantification%20%28LNQ%29%20challenge%20was%20organized%20in%0Aconjunction%20with%20the%2026th%20International%20Conference%20on%20Medical%20Image%20Computing%0Aand%20Computer%20Assisted%20Intervention%20%28MICCAI%202023%29.%20This%20challenge%20aimed%20to%0Aadvance%20weakly-supervised%20segmentation%20methods%20by%20providing%20a%20new%2C%20partially%0Aannotated%20dataset%20and%20a%20robust%20evaluation%20framework.%20A%20total%20of%2016%20teams%20from%205%0Acountries%20submitted%20predictions%20to%20the%20validation%20leaderboard%2C%20and%206%20teams%20from%0A3%20countries%20participated%20in%20the%20evaluation%20phase.%20The%20results%20highlighted%20both%0Athe%20potential%20and%20the%20current%20limitations%20of%20weakly-supervised%20approaches.%20On%0Aone%20hand%2C%20weakly-supervised%20approaches%20obtained%20relatively%20good%20performance%0Awith%20a%20median%20Dice%20score%20of%20%2461.0%5C%25%24.%20On%20the%20other%20hand%2C%20top-ranked%20teams%2C%20with%0Aa%20median%20Dice%20score%20exceeding%20%2470%5C%25%24%2C%20boosted%20their%20performance%20by%20leveraging%0Asmaller%20but%20fully%20annotated%20datasets%20to%20combine%20weak%20supervision%20and%20full%0Asupervision.%20This%20highlights%20both%20the%20promise%20of%20weakly-supervised%20methods%20and%0Athe%20ongoing%20need%20for%20high-quality%2C%20fully%20annotated%20data%20to%20achieve%20higher%0Asegmentation%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.10069v2&entry.124074799=Read"},
{"title": "A Structured Reasoning Framework for Unbalanced Data Classification\n  Using Probabilistic Models", "author": "Junliang Du and Shiyu Dou and Bohuan Yang and Jiacheng Hu and Tai An", "abstract": "  This paper studies a Markov network model for unbalanced data, aiming to\nsolve the problems of classification bias and insufficient minority class\nrecognition ability of traditional machine learning models in environments with\nuneven class distribution. By constructing joint probability distribution and\nconditional dependency, the model can achieve global modeling and reasoning\noptimization of sample categories. The study introduced marginal probability\nestimation and weighted loss optimization strategies, combined with\nregularization constraints and structured reasoning methods, effectively\nimproving the generalization ability and robustness of the model. In the\nexperimental stage, a real credit card fraud detection dataset was selected and\ncompared with models such as logistic regression, support vector machine,\nrandom forest and XGBoost. The experimental results show that the Markov\nnetwork performs well in indicators such as weighted accuracy, F1 score, and\nAUC-ROC, significantly outperforming traditional classification models,\ndemonstrating its strong decision-making ability and applicability in\nunbalanced data scenarios. Future research can focus on efficient model\ntraining, structural optimization, and deep learning integration in large-scale\nunbalanced data environments and promote its wide application in practical\napplications such as financial risk control, medical diagnosis, and intelligent\nmonitoring.\n", "link": "http://arxiv.org/abs/2502.03386v1", "date": "2025-02-05", "relevancy": 2.0439, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5454}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5167}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Structured%20Reasoning%20Framework%20for%20Unbalanced%20Data%20Classification%0A%20%20Using%20Probabilistic%20Models&body=Title%3A%20A%20Structured%20Reasoning%20Framework%20for%20Unbalanced%20Data%20Classification%0A%20%20Using%20Probabilistic%20Models%0AAuthor%3A%20Junliang%20Du%20and%20Shiyu%20Dou%20and%20Bohuan%20Yang%20and%20Jiacheng%20Hu%20and%20Tai%20An%0AAbstract%3A%20%20%20This%20paper%20studies%20a%20Markov%20network%20model%20for%20unbalanced%20data%2C%20aiming%20to%0Asolve%20the%20problems%20of%20classification%20bias%20and%20insufficient%20minority%20class%0Arecognition%20ability%20of%20traditional%20machine%20learning%20models%20in%20environments%20with%0Auneven%20class%20distribution.%20By%20constructing%20joint%20probability%20distribution%20and%0Aconditional%20dependency%2C%20the%20model%20can%20achieve%20global%20modeling%20and%20reasoning%0Aoptimization%20of%20sample%20categories.%20The%20study%20introduced%20marginal%20probability%0Aestimation%20and%20weighted%20loss%20optimization%20strategies%2C%20combined%20with%0Aregularization%20constraints%20and%20structured%20reasoning%20methods%2C%20effectively%0Aimproving%20the%20generalization%20ability%20and%20robustness%20of%20the%20model.%20In%20the%0Aexperimental%20stage%2C%20a%20real%20credit%20card%20fraud%20detection%20dataset%20was%20selected%20and%0Acompared%20with%20models%20such%20as%20logistic%20regression%2C%20support%20vector%20machine%2C%0Arandom%20forest%20and%20XGBoost.%20The%20experimental%20results%20show%20that%20the%20Markov%0Anetwork%20performs%20well%20in%20indicators%20such%20as%20weighted%20accuracy%2C%20F1%20score%2C%20and%0AAUC-ROC%2C%20significantly%20outperforming%20traditional%20classification%20models%2C%0Ademonstrating%20its%20strong%20decision-making%20ability%20and%20applicability%20in%0Aunbalanced%20data%20scenarios.%20Future%20research%20can%20focus%20on%20efficient%20model%0Atraining%2C%20structural%20optimization%2C%20and%20deep%20learning%20integration%20in%20large-scale%0Aunbalanced%20data%20environments%20and%20promote%20its%20wide%20application%20in%20practical%0Aapplications%20such%20as%20financial%20risk%20control%2C%20medical%20diagnosis%2C%20and%20intelligent%0Amonitoring.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03386v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Structured%2520Reasoning%2520Framework%2520for%2520Unbalanced%2520Data%2520Classification%250A%2520%2520Using%2520Probabilistic%2520Models%26entry.906535625%3DJunliang%2520Du%2520and%2520Shiyu%2520Dou%2520and%2520Bohuan%2520Yang%2520and%2520Jiacheng%2520Hu%2520and%2520Tai%2520An%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520a%2520Markov%2520network%2520model%2520for%2520unbalanced%2520data%252C%2520aiming%2520to%250Asolve%2520the%2520problems%2520of%2520classification%2520bias%2520and%2520insufficient%2520minority%2520class%250Arecognition%2520ability%2520of%2520traditional%2520machine%2520learning%2520models%2520in%2520environments%2520with%250Auneven%2520class%2520distribution.%2520By%2520constructing%2520joint%2520probability%2520distribution%2520and%250Aconditional%2520dependency%252C%2520the%2520model%2520can%2520achieve%2520global%2520modeling%2520and%2520reasoning%250Aoptimization%2520of%2520sample%2520categories.%2520The%2520study%2520introduced%2520marginal%2520probability%250Aestimation%2520and%2520weighted%2520loss%2520optimization%2520strategies%252C%2520combined%2520with%250Aregularization%2520constraints%2520and%2520structured%2520reasoning%2520methods%252C%2520effectively%250Aimproving%2520the%2520generalization%2520ability%2520and%2520robustness%2520of%2520the%2520model.%2520In%2520the%250Aexperimental%2520stage%252C%2520a%2520real%2520credit%2520card%2520fraud%2520detection%2520dataset%2520was%2520selected%2520and%250Acompared%2520with%2520models%2520such%2520as%2520logistic%2520regression%252C%2520support%2520vector%2520machine%252C%250Arandom%2520forest%2520and%2520XGBoost.%2520The%2520experimental%2520results%2520show%2520that%2520the%2520Markov%250Anetwork%2520performs%2520well%2520in%2520indicators%2520such%2520as%2520weighted%2520accuracy%252C%2520F1%2520score%252C%2520and%250AAUC-ROC%252C%2520significantly%2520outperforming%2520traditional%2520classification%2520models%252C%250Ademonstrating%2520its%2520strong%2520decision-making%2520ability%2520and%2520applicability%2520in%250Aunbalanced%2520data%2520scenarios.%2520Future%2520research%2520can%2520focus%2520on%2520efficient%2520model%250Atraining%252C%2520structural%2520optimization%252C%2520and%2520deep%2520learning%2520integration%2520in%2520large-scale%250Aunbalanced%2520data%2520environments%2520and%2520promote%2520its%2520wide%2520application%2520in%2520practical%250Aapplications%2520such%2520as%2520financial%2520risk%2520control%252C%2520medical%2520diagnosis%252C%2520and%2520intelligent%250Amonitoring.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03386v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Structured%20Reasoning%20Framework%20for%20Unbalanced%20Data%20Classification%0A%20%20Using%20Probabilistic%20Models&entry.906535625=Junliang%20Du%20and%20Shiyu%20Dou%20and%20Bohuan%20Yang%20and%20Jiacheng%20Hu%20and%20Tai%20An&entry.1292438233=%20%20This%20paper%20studies%20a%20Markov%20network%20model%20for%20unbalanced%20data%2C%20aiming%20to%0Asolve%20the%20problems%20of%20classification%20bias%20and%20insufficient%20minority%20class%0Arecognition%20ability%20of%20traditional%20machine%20learning%20models%20in%20environments%20with%0Auneven%20class%20distribution.%20By%20constructing%20joint%20probability%20distribution%20and%0Aconditional%20dependency%2C%20the%20model%20can%20achieve%20global%20modeling%20and%20reasoning%0Aoptimization%20of%20sample%20categories.%20The%20study%20introduced%20marginal%20probability%0Aestimation%20and%20weighted%20loss%20optimization%20strategies%2C%20combined%20with%0Aregularization%20constraints%20and%20structured%20reasoning%20methods%2C%20effectively%0Aimproving%20the%20generalization%20ability%20and%20robustness%20of%20the%20model.%20In%20the%0Aexperimental%20stage%2C%20a%20real%20credit%20card%20fraud%20detection%20dataset%20was%20selected%20and%0Acompared%20with%20models%20such%20as%20logistic%20regression%2C%20support%20vector%20machine%2C%0Arandom%20forest%20and%20XGBoost.%20The%20experimental%20results%20show%20that%20the%20Markov%0Anetwork%20performs%20well%20in%20indicators%20such%20as%20weighted%20accuracy%2C%20F1%20score%2C%20and%0AAUC-ROC%2C%20significantly%20outperforming%20traditional%20classification%20models%2C%0Ademonstrating%20its%20strong%20decision-making%20ability%20and%20applicability%20in%0Aunbalanced%20data%20scenarios.%20Future%20research%20can%20focus%20on%20efficient%20model%0Atraining%2C%20structural%20optimization%2C%20and%20deep%20learning%20integration%20in%20large-scale%0Aunbalanced%20data%20environments%20and%20promote%20its%20wide%20application%20in%20practical%0Aapplications%20such%20as%20financial%20risk%20control%2C%20medical%20diagnosis%2C%20and%20intelligent%0Amonitoring.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03386v1&entry.124074799=Read"},
{"title": "AttNS: Attention-Inspired Numerical Solving For Limited Data Scenarios", "author": "Zhongzhan Huang and Mingfu Liang and Shanshan Zhong and Liang Lin", "abstract": "  We propose the attention-inspired numerical solver (AttNS), a concise method\nthat helps the generalization and robustness issues faced by the AI-Hybrid\nnumerical solver in solving differential equations due to limited data. AttNS\nis inspired by the effectiveness of attention modules in Residual Neural\nNetworks (ResNet) in enhancing model generalization and robustness for\nconventional deep learning tasks. Drawing from the dynamical system perspective\nof ResNet, we seamlessly incorporate attention mechanisms into the design of\nnumerical methods tailored for the characteristics of solving differential\nequations. Our results on benchmarks, ranging from high-dimensional problems to\nchaotic systems, showcases AttNS consistently enhancing various numerical\nsolvers without any intricate model crafting. Finally, we analyze AttNS\nexperimentally and theoretically, demonstrating its ability to achieve strong\ngeneralization and robustness while ensuring the convergence of the solver.\nThis includes requiring less data compared to other advanced methods to achieve\ncomparable generalization errors and better prevention of numerical explosion\nissues when solving differential equations.\n", "link": "http://arxiv.org/abs/2302.10184v2", "date": "2025-02-05", "relevancy": 2.0418, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5314}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5118}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AttNS%3A%20Attention-Inspired%20Numerical%20Solving%20For%20Limited%20Data%20Scenarios&body=Title%3A%20AttNS%3A%20Attention-Inspired%20Numerical%20Solving%20For%20Limited%20Data%20Scenarios%0AAuthor%3A%20Zhongzhan%20Huang%20and%20Mingfu%20Liang%20and%20Shanshan%20Zhong%20and%20Liang%20Lin%0AAbstract%3A%20%20%20We%20propose%20the%20attention-inspired%20numerical%20solver%20%28AttNS%29%2C%20a%20concise%20method%0Athat%20helps%20the%20generalization%20and%20robustness%20issues%20faced%20by%20the%20AI-Hybrid%0Anumerical%20solver%20in%20solving%20differential%20equations%20due%20to%20limited%20data.%20AttNS%0Ais%20inspired%20by%20the%20effectiveness%20of%20attention%20modules%20in%20Residual%20Neural%0ANetworks%20%28ResNet%29%20in%20enhancing%20model%20generalization%20and%20robustness%20for%0Aconventional%20deep%20learning%20tasks.%20Drawing%20from%20the%20dynamical%20system%20perspective%0Aof%20ResNet%2C%20we%20seamlessly%20incorporate%20attention%20mechanisms%20into%20the%20design%20of%0Anumerical%20methods%20tailored%20for%20the%20characteristics%20of%20solving%20differential%0Aequations.%20Our%20results%20on%20benchmarks%2C%20ranging%20from%20high-dimensional%20problems%20to%0Achaotic%20systems%2C%20showcases%20AttNS%20consistently%20enhancing%20various%20numerical%0Asolvers%20without%20any%20intricate%20model%20crafting.%20Finally%2C%20we%20analyze%20AttNS%0Aexperimentally%20and%20theoretically%2C%20demonstrating%20its%20ability%20to%20achieve%20strong%0Ageneralization%20and%20robustness%20while%20ensuring%20the%20convergence%20of%20the%20solver.%0AThis%20includes%20requiring%20less%20data%20compared%20to%20other%20advanced%20methods%20to%20achieve%0Acomparable%20generalization%20errors%20and%20better%20prevention%20of%20numerical%20explosion%0Aissues%20when%20solving%20differential%20equations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.10184v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttNS%253A%2520Attention-Inspired%2520Numerical%2520Solving%2520For%2520Limited%2520Data%2520Scenarios%26entry.906535625%3DZhongzhan%2520Huang%2520and%2520Mingfu%2520Liang%2520and%2520Shanshan%2520Zhong%2520and%2520Liang%2520Lin%26entry.1292438233%3D%2520%2520We%2520propose%2520the%2520attention-inspired%2520numerical%2520solver%2520%2528AttNS%2529%252C%2520a%2520concise%2520method%250Athat%2520helps%2520the%2520generalization%2520and%2520robustness%2520issues%2520faced%2520by%2520the%2520AI-Hybrid%250Anumerical%2520solver%2520in%2520solving%2520differential%2520equations%2520due%2520to%2520limited%2520data.%2520AttNS%250Ais%2520inspired%2520by%2520the%2520effectiveness%2520of%2520attention%2520modules%2520in%2520Residual%2520Neural%250ANetworks%2520%2528ResNet%2529%2520in%2520enhancing%2520model%2520generalization%2520and%2520robustness%2520for%250Aconventional%2520deep%2520learning%2520tasks.%2520Drawing%2520from%2520the%2520dynamical%2520system%2520perspective%250Aof%2520ResNet%252C%2520we%2520seamlessly%2520incorporate%2520attention%2520mechanisms%2520into%2520the%2520design%2520of%250Anumerical%2520methods%2520tailored%2520for%2520the%2520characteristics%2520of%2520solving%2520differential%250Aequations.%2520Our%2520results%2520on%2520benchmarks%252C%2520ranging%2520from%2520high-dimensional%2520problems%2520to%250Achaotic%2520systems%252C%2520showcases%2520AttNS%2520consistently%2520enhancing%2520various%2520numerical%250Asolvers%2520without%2520any%2520intricate%2520model%2520crafting.%2520Finally%252C%2520we%2520analyze%2520AttNS%250Aexperimentally%2520and%2520theoretically%252C%2520demonstrating%2520its%2520ability%2520to%2520achieve%2520strong%250Ageneralization%2520and%2520robustness%2520while%2520ensuring%2520the%2520convergence%2520of%2520the%2520solver.%250AThis%2520includes%2520requiring%2520less%2520data%2520compared%2520to%2520other%2520advanced%2520methods%2520to%2520achieve%250Acomparable%2520generalization%2520errors%2520and%2520better%2520prevention%2520of%2520numerical%2520explosion%250Aissues%2520when%2520solving%2520differential%2520equations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.10184v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AttNS%3A%20Attention-Inspired%20Numerical%20Solving%20For%20Limited%20Data%20Scenarios&entry.906535625=Zhongzhan%20Huang%20and%20Mingfu%20Liang%20and%20Shanshan%20Zhong%20and%20Liang%20Lin&entry.1292438233=%20%20We%20propose%20the%20attention-inspired%20numerical%20solver%20%28AttNS%29%2C%20a%20concise%20method%0Athat%20helps%20the%20generalization%20and%20robustness%20issues%20faced%20by%20the%20AI-Hybrid%0Anumerical%20solver%20in%20solving%20differential%20equations%20due%20to%20limited%20data.%20AttNS%0Ais%20inspired%20by%20the%20effectiveness%20of%20attention%20modules%20in%20Residual%20Neural%0ANetworks%20%28ResNet%29%20in%20enhancing%20model%20generalization%20and%20robustness%20for%0Aconventional%20deep%20learning%20tasks.%20Drawing%20from%20the%20dynamical%20system%20perspective%0Aof%20ResNet%2C%20we%20seamlessly%20incorporate%20attention%20mechanisms%20into%20the%20design%20of%0Anumerical%20methods%20tailored%20for%20the%20characteristics%20of%20solving%20differential%0Aequations.%20Our%20results%20on%20benchmarks%2C%20ranging%20from%20high-dimensional%20problems%20to%0Achaotic%20systems%2C%20showcases%20AttNS%20consistently%20enhancing%20various%20numerical%0Asolvers%20without%20any%20intricate%20model%20crafting.%20Finally%2C%20we%20analyze%20AttNS%0Aexperimentally%20and%20theoretically%2C%20demonstrating%20its%20ability%20to%20achieve%20strong%0Ageneralization%20and%20robustness%20while%20ensuring%20the%20convergence%20of%20the%20solver.%0AThis%20includes%20requiring%20less%20data%20compared%20to%20other%20advanced%20methods%20to%20achieve%0Acomparable%20generalization%20errors%20and%20better%20prevention%20of%20numerical%20explosion%0Aissues%20when%20solving%20differential%20equations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.10184v2&entry.124074799=Read"},
{"title": "Simple Is Effective: The Roles of Graphs and Large Language Models in\n  Knowledge-Graph-Based Retrieval-Augmented Generation", "author": "Mufei Li and Siqi Miao and Pan Li", "abstract": "  Large Language Models (LLMs) demonstrate strong reasoning abilities but face\nlimitations such as hallucinations and outdated knowledge. Knowledge Graph\n(KG)-based Retrieval-Augmented Generation (RAG) addresses these issues by\ngrounding LLM outputs in structured external knowledge from KGs. However,\ncurrent KG-based RAG frameworks still struggle to optimize the trade-off\nbetween retrieval effectiveness and efficiency in identifying a suitable amount\nof relevant graph information for the LLM to digest. We introduce SubgraphRAG,\nextending the KG-based RAG framework that retrieves subgraphs and leverages\nLLMs for reasoning and answer prediction. Our approach innovatively integrates\na lightweight multilayer perceptron with a parallel triple-scoring mechanism\nfor efficient and flexible subgraph retrieval while encoding directional\nstructural distances to enhance retrieval effectiveness. The size of retrieved\nsubgraphs can be flexibly adjusted to match the query's need and the downstream\nLLM's capabilities. This design strikes a balance between model complexity and\nreasoning power, enabling scalable and generalizable retrieval processes.\nNotably, based on our retrieved subgraphs, smaller LLMs like\nLlama3.1-8B-Instruct deliver competitive results with explainable reasoning,\nwhile larger models like GPT-4o achieve state-of-the-art accuracy compared with\nprevious baselines -- all without fine-tuning. Extensive evaluations on the\nWebQSP and CWQ benchmarks highlight SubgraphRAG's strengths in efficiency,\naccuracy, and reliability by reducing hallucinations and improving response\ngrounding.\n", "link": "http://arxiv.org/abs/2410.20724v4", "date": "2025-02-05", "relevancy": 2.0313, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5218}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5103}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simple%20Is%20Effective%3A%20The%20Roles%20of%20Graphs%20and%20Large%20Language%20Models%20in%0A%20%20Knowledge-Graph-Based%20Retrieval-Augmented%20Generation&body=Title%3A%20Simple%20Is%20Effective%3A%20The%20Roles%20of%20Graphs%20and%20Large%20Language%20Models%20in%0A%20%20Knowledge-Graph-Based%20Retrieval-Augmented%20Generation%0AAuthor%3A%20Mufei%20Li%20and%20Siqi%20Miao%20and%20Pan%20Li%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20strong%20reasoning%20abilities%20but%20face%0Alimitations%20such%20as%20hallucinations%20and%20outdated%20knowledge.%20Knowledge%20Graph%0A%28KG%29-based%20Retrieval-Augmented%20Generation%20%28RAG%29%20addresses%20these%20issues%20by%0Agrounding%20LLM%20outputs%20in%20structured%20external%20knowledge%20from%20KGs.%20However%2C%0Acurrent%20KG-based%20RAG%20frameworks%20still%20struggle%20to%20optimize%20the%20trade-off%0Abetween%20retrieval%20effectiveness%20and%20efficiency%20in%20identifying%20a%20suitable%20amount%0Aof%20relevant%20graph%20information%20for%20the%20LLM%20to%20digest.%20We%20introduce%20SubgraphRAG%2C%0Aextending%20the%20KG-based%20RAG%20framework%20that%20retrieves%20subgraphs%20and%20leverages%0ALLMs%20for%20reasoning%20and%20answer%20prediction.%20Our%20approach%20innovatively%20integrates%0Aa%20lightweight%20multilayer%20perceptron%20with%20a%20parallel%20triple-scoring%20mechanism%0Afor%20efficient%20and%20flexible%20subgraph%20retrieval%20while%20encoding%20directional%0Astructural%20distances%20to%20enhance%20retrieval%20effectiveness.%20The%20size%20of%20retrieved%0Asubgraphs%20can%20be%20flexibly%20adjusted%20to%20match%20the%20query%27s%20need%20and%20the%20downstream%0ALLM%27s%20capabilities.%20This%20design%20strikes%20a%20balance%20between%20model%20complexity%20and%0Areasoning%20power%2C%20enabling%20scalable%20and%20generalizable%20retrieval%20processes.%0ANotably%2C%20based%20on%20our%20retrieved%20subgraphs%2C%20smaller%20LLMs%20like%0ALlama3.1-8B-Instruct%20deliver%20competitive%20results%20with%20explainable%20reasoning%2C%0Awhile%20larger%20models%20like%20GPT-4o%20achieve%20state-of-the-art%20accuracy%20compared%20with%0Aprevious%20baselines%20--%20all%20without%20fine-tuning.%20Extensive%20evaluations%20on%20the%0AWebQSP%20and%20CWQ%20benchmarks%20highlight%20SubgraphRAG%27s%20strengths%20in%20efficiency%2C%0Aaccuracy%2C%20and%20reliability%20by%20reducing%20hallucinations%20and%20improving%20response%0Agrounding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.20724v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimple%2520Is%2520Effective%253A%2520The%2520Roles%2520of%2520Graphs%2520and%2520Large%2520Language%2520Models%2520in%250A%2520%2520Knowledge-Graph-Based%2520Retrieval-Augmented%2520Generation%26entry.906535625%3DMufei%2520Li%2520and%2520Siqi%2520Miao%2520and%2520Pan%2520Li%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520demonstrate%2520strong%2520reasoning%2520abilities%2520but%2520face%250Alimitations%2520such%2520as%2520hallucinations%2520and%2520outdated%2520knowledge.%2520Knowledge%2520Graph%250A%2528KG%2529-based%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520addresses%2520these%2520issues%2520by%250Agrounding%2520LLM%2520outputs%2520in%2520structured%2520external%2520knowledge%2520from%2520KGs.%2520However%252C%250Acurrent%2520KG-based%2520RAG%2520frameworks%2520still%2520struggle%2520to%2520optimize%2520the%2520trade-off%250Abetween%2520retrieval%2520effectiveness%2520and%2520efficiency%2520in%2520identifying%2520a%2520suitable%2520amount%250Aof%2520relevant%2520graph%2520information%2520for%2520the%2520LLM%2520to%2520digest.%2520We%2520introduce%2520SubgraphRAG%252C%250Aextending%2520the%2520KG-based%2520RAG%2520framework%2520that%2520retrieves%2520subgraphs%2520and%2520leverages%250ALLMs%2520for%2520reasoning%2520and%2520answer%2520prediction.%2520Our%2520approach%2520innovatively%2520integrates%250Aa%2520lightweight%2520multilayer%2520perceptron%2520with%2520a%2520parallel%2520triple-scoring%2520mechanism%250Afor%2520efficient%2520and%2520flexible%2520subgraph%2520retrieval%2520while%2520encoding%2520directional%250Astructural%2520distances%2520to%2520enhance%2520retrieval%2520effectiveness.%2520The%2520size%2520of%2520retrieved%250Asubgraphs%2520can%2520be%2520flexibly%2520adjusted%2520to%2520match%2520the%2520query%2527s%2520need%2520and%2520the%2520downstream%250ALLM%2527s%2520capabilities.%2520This%2520design%2520strikes%2520a%2520balance%2520between%2520model%2520complexity%2520and%250Areasoning%2520power%252C%2520enabling%2520scalable%2520and%2520generalizable%2520retrieval%2520processes.%250ANotably%252C%2520based%2520on%2520our%2520retrieved%2520subgraphs%252C%2520smaller%2520LLMs%2520like%250ALlama3.1-8B-Instruct%2520deliver%2520competitive%2520results%2520with%2520explainable%2520reasoning%252C%250Awhile%2520larger%2520models%2520like%2520GPT-4o%2520achieve%2520state-of-the-art%2520accuracy%2520compared%2520with%250Aprevious%2520baselines%2520--%2520all%2520without%2520fine-tuning.%2520Extensive%2520evaluations%2520on%2520the%250AWebQSP%2520and%2520CWQ%2520benchmarks%2520highlight%2520SubgraphRAG%2527s%2520strengths%2520in%2520efficiency%252C%250Aaccuracy%252C%2520and%2520reliability%2520by%2520reducing%2520hallucinations%2520and%2520improving%2520response%250Agrounding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.20724v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simple%20Is%20Effective%3A%20The%20Roles%20of%20Graphs%20and%20Large%20Language%20Models%20in%0A%20%20Knowledge-Graph-Based%20Retrieval-Augmented%20Generation&entry.906535625=Mufei%20Li%20and%20Siqi%20Miao%20and%20Pan%20Li&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20demonstrate%20strong%20reasoning%20abilities%20but%20face%0Alimitations%20such%20as%20hallucinations%20and%20outdated%20knowledge.%20Knowledge%20Graph%0A%28KG%29-based%20Retrieval-Augmented%20Generation%20%28RAG%29%20addresses%20these%20issues%20by%0Agrounding%20LLM%20outputs%20in%20structured%20external%20knowledge%20from%20KGs.%20However%2C%0Acurrent%20KG-based%20RAG%20frameworks%20still%20struggle%20to%20optimize%20the%20trade-off%0Abetween%20retrieval%20effectiveness%20and%20efficiency%20in%20identifying%20a%20suitable%20amount%0Aof%20relevant%20graph%20information%20for%20the%20LLM%20to%20digest.%20We%20introduce%20SubgraphRAG%2C%0Aextending%20the%20KG-based%20RAG%20framework%20that%20retrieves%20subgraphs%20and%20leverages%0ALLMs%20for%20reasoning%20and%20answer%20prediction.%20Our%20approach%20innovatively%20integrates%0Aa%20lightweight%20multilayer%20perceptron%20with%20a%20parallel%20triple-scoring%20mechanism%0Afor%20efficient%20and%20flexible%20subgraph%20retrieval%20while%20encoding%20directional%0Astructural%20distances%20to%20enhance%20retrieval%20effectiveness.%20The%20size%20of%20retrieved%0Asubgraphs%20can%20be%20flexibly%20adjusted%20to%20match%20the%20query%27s%20need%20and%20the%20downstream%0ALLM%27s%20capabilities.%20This%20design%20strikes%20a%20balance%20between%20model%20complexity%20and%0Areasoning%20power%2C%20enabling%20scalable%20and%20generalizable%20retrieval%20processes.%0ANotably%2C%20based%20on%20our%20retrieved%20subgraphs%2C%20smaller%20LLMs%20like%0ALlama3.1-8B-Instruct%20deliver%20competitive%20results%20with%20explainable%20reasoning%2C%0Awhile%20larger%20models%20like%20GPT-4o%20achieve%20state-of-the-art%20accuracy%20compared%20with%0Aprevious%20baselines%20--%20all%20without%20fine-tuning.%20Extensive%20evaluations%20on%20the%0AWebQSP%20and%20CWQ%20benchmarks%20highlight%20SubgraphRAG%27s%20strengths%20in%20efficiency%2C%0Aaccuracy%2C%20and%20reliability%20by%20reducing%20hallucinations%20and%20improving%20response%0Agrounding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.20724v4&entry.124074799=Read"},
{"title": "Demystifying Long Chain-of-Thought Reasoning in LLMs", "author": "Edward Yeo and Yuxuan Tong and Morry Niu and Graham Neubig and Xiang Yue", "abstract": "  Scaling inference compute enhances reasoning in large language models (LLMs),\nwith long chains-of-thought (CoTs) enabling strategies like backtracking and\nerror correction. Reinforcement learning (RL) has emerged as a crucial method\nfor developing these capabilities, yet the conditions under which long CoTs\nemerge remain unclear, and RL training requires careful design choices. In this\nstudy, we systematically investigate the mechanics of long CoT reasoning,\nidentifying the key factors that enable models to generate long CoT\ntrajectories. Through extensive supervised fine-tuning (SFT) and RL\nexperiments, we present four main findings: (1) While SFT is not strictly\nnecessary, it simplifies training and improves efficiency; (2) Reasoning\ncapabilities tend to emerge with increased training compute, but their\ndevelopment is not guaranteed, making reward shaping crucial for stabilizing\nCoT length growth; (3) Scaling verifiable reward signals is critical for RL. We\nfind that leveraging noisy, web-extracted solutions with filtering mechanisms\nshows strong potential, particularly for out-of-distribution (OOD) tasks such\nas STEM reasoning; and (4) Core abilities like error correction are inherently\npresent in base models, but incentivizing these skills effectively for complex\ntasks via RL demands significant compute, and measuring their emergence\nrequires a nuanced approach. These insights provide practical guidance for\noptimizing training strategies to enhance long CoT reasoning in LLMs. Our code\nis available at: https://github.com/eddycmu/demystify-long-cot.\n", "link": "http://arxiv.org/abs/2502.03373v1", "date": "2025-02-05", "relevancy": 2.0275, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5116}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5116}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4833}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Demystifying%20Long%20Chain-of-Thought%20Reasoning%20in%20LLMs&body=Title%3A%20Demystifying%20Long%20Chain-of-Thought%20Reasoning%20in%20LLMs%0AAuthor%3A%20Edward%20Yeo%20and%20Yuxuan%20Tong%20and%20Morry%20Niu%20and%20Graham%20Neubig%20and%20Xiang%20Yue%0AAbstract%3A%20%20%20Scaling%20inference%20compute%20enhances%20reasoning%20in%20large%20language%20models%20%28LLMs%29%2C%0Awith%20long%20chains-of-thought%20%28CoTs%29%20enabling%20strategies%20like%20backtracking%20and%0Aerror%20correction.%20Reinforcement%20learning%20%28RL%29%20has%20emerged%20as%20a%20crucial%20method%0Afor%20developing%20these%20capabilities%2C%20yet%20the%20conditions%20under%20which%20long%20CoTs%0Aemerge%20remain%20unclear%2C%20and%20RL%20training%20requires%20careful%20design%20choices.%20In%20this%0Astudy%2C%20we%20systematically%20investigate%20the%20mechanics%20of%20long%20CoT%20reasoning%2C%0Aidentifying%20the%20key%20factors%20that%20enable%20models%20to%20generate%20long%20CoT%0Atrajectories.%20Through%20extensive%20supervised%20fine-tuning%20%28SFT%29%20and%20RL%0Aexperiments%2C%20we%20present%20four%20main%20findings%3A%20%281%29%20While%20SFT%20is%20not%20strictly%0Anecessary%2C%20it%20simplifies%20training%20and%20improves%20efficiency%3B%20%282%29%20Reasoning%0Acapabilities%20tend%20to%20emerge%20with%20increased%20training%20compute%2C%20but%20their%0Adevelopment%20is%20not%20guaranteed%2C%20making%20reward%20shaping%20crucial%20for%20stabilizing%0ACoT%20length%20growth%3B%20%283%29%20Scaling%20verifiable%20reward%20signals%20is%20critical%20for%20RL.%20We%0Afind%20that%20leveraging%20noisy%2C%20web-extracted%20solutions%20with%20filtering%20mechanisms%0Ashows%20strong%20potential%2C%20particularly%20for%20out-of-distribution%20%28OOD%29%20tasks%20such%0Aas%20STEM%20reasoning%3B%20and%20%284%29%20Core%20abilities%20like%20error%20correction%20are%20inherently%0Apresent%20in%20base%20models%2C%20but%20incentivizing%20these%20skills%20effectively%20for%20complex%0Atasks%20via%20RL%20demands%20significant%20compute%2C%20and%20measuring%20their%20emergence%0Arequires%20a%20nuanced%20approach.%20These%20insights%20provide%20practical%20guidance%20for%0Aoptimizing%20training%20strategies%20to%20enhance%20long%20CoT%20reasoning%20in%20LLMs.%20Our%20code%0Ais%20available%20at%3A%20https%3A//github.com/eddycmu/demystify-long-cot.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03373v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDemystifying%2520Long%2520Chain-of-Thought%2520Reasoning%2520in%2520LLMs%26entry.906535625%3DEdward%2520Yeo%2520and%2520Yuxuan%2520Tong%2520and%2520Morry%2520Niu%2520and%2520Graham%2520Neubig%2520and%2520Xiang%2520Yue%26entry.1292438233%3D%2520%2520Scaling%2520inference%2520compute%2520enhances%2520reasoning%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%252C%250Awith%2520long%2520chains-of-thought%2520%2528CoTs%2529%2520enabling%2520strategies%2520like%2520backtracking%2520and%250Aerror%2520correction.%2520Reinforcement%2520learning%2520%2528RL%2529%2520has%2520emerged%2520as%2520a%2520crucial%2520method%250Afor%2520developing%2520these%2520capabilities%252C%2520yet%2520the%2520conditions%2520under%2520which%2520long%2520CoTs%250Aemerge%2520remain%2520unclear%252C%2520and%2520RL%2520training%2520requires%2520careful%2520design%2520choices.%2520In%2520this%250Astudy%252C%2520we%2520systematically%2520investigate%2520the%2520mechanics%2520of%2520long%2520CoT%2520reasoning%252C%250Aidentifying%2520the%2520key%2520factors%2520that%2520enable%2520models%2520to%2520generate%2520long%2520CoT%250Atrajectories.%2520Through%2520extensive%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520and%2520RL%250Aexperiments%252C%2520we%2520present%2520four%2520main%2520findings%253A%2520%25281%2529%2520While%2520SFT%2520is%2520not%2520strictly%250Anecessary%252C%2520it%2520simplifies%2520training%2520and%2520improves%2520efficiency%253B%2520%25282%2529%2520Reasoning%250Acapabilities%2520tend%2520to%2520emerge%2520with%2520increased%2520training%2520compute%252C%2520but%2520their%250Adevelopment%2520is%2520not%2520guaranteed%252C%2520making%2520reward%2520shaping%2520crucial%2520for%2520stabilizing%250ACoT%2520length%2520growth%253B%2520%25283%2529%2520Scaling%2520verifiable%2520reward%2520signals%2520is%2520critical%2520for%2520RL.%2520We%250Afind%2520that%2520leveraging%2520noisy%252C%2520web-extracted%2520solutions%2520with%2520filtering%2520mechanisms%250Ashows%2520strong%2520potential%252C%2520particularly%2520for%2520out-of-distribution%2520%2528OOD%2529%2520tasks%2520such%250Aas%2520STEM%2520reasoning%253B%2520and%2520%25284%2529%2520Core%2520abilities%2520like%2520error%2520correction%2520are%2520inherently%250Apresent%2520in%2520base%2520models%252C%2520but%2520incentivizing%2520these%2520skills%2520effectively%2520for%2520complex%250Atasks%2520via%2520RL%2520demands%2520significant%2520compute%252C%2520and%2520measuring%2520their%2520emergence%250Arequires%2520a%2520nuanced%2520approach.%2520These%2520insights%2520provide%2520practical%2520guidance%2520for%250Aoptimizing%2520training%2520strategies%2520to%2520enhance%2520long%2520CoT%2520reasoning%2520in%2520LLMs.%2520Our%2520code%250Ais%2520available%2520at%253A%2520https%253A//github.com/eddycmu/demystify-long-cot.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03373v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Demystifying%20Long%20Chain-of-Thought%20Reasoning%20in%20LLMs&entry.906535625=Edward%20Yeo%20and%20Yuxuan%20Tong%20and%20Morry%20Niu%20and%20Graham%20Neubig%20and%20Xiang%20Yue&entry.1292438233=%20%20Scaling%20inference%20compute%20enhances%20reasoning%20in%20large%20language%20models%20%28LLMs%29%2C%0Awith%20long%20chains-of-thought%20%28CoTs%29%20enabling%20strategies%20like%20backtracking%20and%0Aerror%20correction.%20Reinforcement%20learning%20%28RL%29%20has%20emerged%20as%20a%20crucial%20method%0Afor%20developing%20these%20capabilities%2C%20yet%20the%20conditions%20under%20which%20long%20CoTs%0Aemerge%20remain%20unclear%2C%20and%20RL%20training%20requires%20careful%20design%20choices.%20In%20this%0Astudy%2C%20we%20systematically%20investigate%20the%20mechanics%20of%20long%20CoT%20reasoning%2C%0Aidentifying%20the%20key%20factors%20that%20enable%20models%20to%20generate%20long%20CoT%0Atrajectories.%20Through%20extensive%20supervised%20fine-tuning%20%28SFT%29%20and%20RL%0Aexperiments%2C%20we%20present%20four%20main%20findings%3A%20%281%29%20While%20SFT%20is%20not%20strictly%0Anecessary%2C%20it%20simplifies%20training%20and%20improves%20efficiency%3B%20%282%29%20Reasoning%0Acapabilities%20tend%20to%20emerge%20with%20increased%20training%20compute%2C%20but%20their%0Adevelopment%20is%20not%20guaranteed%2C%20making%20reward%20shaping%20crucial%20for%20stabilizing%0ACoT%20length%20growth%3B%20%283%29%20Scaling%20verifiable%20reward%20signals%20is%20critical%20for%20RL.%20We%0Afind%20that%20leveraging%20noisy%2C%20web-extracted%20solutions%20with%20filtering%20mechanisms%0Ashows%20strong%20potential%2C%20particularly%20for%20out-of-distribution%20%28OOD%29%20tasks%20such%0Aas%20STEM%20reasoning%3B%20and%20%284%29%20Core%20abilities%20like%20error%20correction%20are%20inherently%0Apresent%20in%20base%20models%2C%20but%20incentivizing%20these%20skills%20effectively%20for%20complex%0Atasks%20via%20RL%20demands%20significant%20compute%2C%20and%20measuring%20their%20emergence%0Arequires%20a%20nuanced%20approach.%20These%20insights%20provide%20practical%20guidance%20for%0Aoptimizing%20training%20strategies%20to%20enhance%20long%20CoT%20reasoning%20in%20LLMs.%20Our%20code%0Ais%20available%20at%3A%20https%3A//github.com/eddycmu/demystify-long-cot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03373v1&entry.124074799=Read"},
{"title": "IRIS: An Immersive Robot Interaction System", "author": "Xinkai Jiang and Qihao Yuan and Enes Ulas Dincer and Hongyi Zhou and Ge Li and Xueyin Li and Julius Haag and Nicolas Schreiber and Kailai Li and Gerhard Neumann and Rudolf Lioutikov", "abstract": "  This paper introduces IRIS, an immersive Robot Interaction System leveraging\nExtended Reality (XR), designed for robot data collection and interaction\nacross multiple simulators, benchmarks, and real-world scenarios. While\nexisting XR-based data collection systems provide efficient and intuitive\nsolutions for large-scale data collection, they are often challenging to\nreproduce and reuse. This limitation arises because current systems are highly\ntailored to simulator-specific use cases and environments. IRIS is a novel,\neasily extendable framework that already supports multiple simulators,\nbenchmarks, and even headsets. Furthermore, IRIS is able to include additional\ninformation from real-world sensors, such as point clouds captured through\ndepth cameras. A unified scene specification is generated directly from\nsimulators or real-world sensors and transmitted to XR headsets, creating\nidentical scenes in XR. This specification allows IRIS to support any of the\nobjects, assets, and robots provided by the simulators. In addition, IRIS\nintroduces shared spatial anchors and a robust communication protocol that\nlinks simulations between multiple XR headsets. This feature enables multiple\nXR headsets to share a synchronized scene, facilitating collaborative and\nmulti-user data collection. IRIS can be deployed on any device that supports\nthe Unity Framework, encompassing the vast majority of commercially available\nheadsets. In this work, IRIS was deployed and tested on the Meta Quest 3 and\nthe HoloLens 2. IRIS showcased its versatility across a wide range of\nreal-world and simulated scenarios, using current popular robot simulators such\nas MuJoCo, IsaacSim, CoppeliaSim, and Genesis. In addition, a user study\nevaluates IRIS on a data collection task for the LIBERO benchmark. The study\nshows that IRIS significantly outperforms the baseline in both objective and\nsubjective metrics.\n", "link": "http://arxiv.org/abs/2502.03297v1", "date": "2025-02-05", "relevancy": 2.0269, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5642}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4967}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IRIS%3A%20An%20Immersive%20Robot%20Interaction%20System&body=Title%3A%20IRIS%3A%20An%20Immersive%20Robot%20Interaction%20System%0AAuthor%3A%20Xinkai%20Jiang%20and%20Qihao%20Yuan%20and%20Enes%20Ulas%20Dincer%20and%20Hongyi%20Zhou%20and%20Ge%20Li%20and%20Xueyin%20Li%20and%20Julius%20Haag%20and%20Nicolas%20Schreiber%20and%20Kailai%20Li%20and%20Gerhard%20Neumann%20and%20Rudolf%20Lioutikov%0AAbstract%3A%20%20%20This%20paper%20introduces%20IRIS%2C%20an%20immersive%20Robot%20Interaction%20System%20leveraging%0AExtended%20Reality%20%28XR%29%2C%20designed%20for%20robot%20data%20collection%20and%20interaction%0Aacross%20multiple%20simulators%2C%20benchmarks%2C%20and%20real-world%20scenarios.%20While%0Aexisting%20XR-based%20data%20collection%20systems%20provide%20efficient%20and%20intuitive%0Asolutions%20for%20large-scale%20data%20collection%2C%20they%20are%20often%20challenging%20to%0Areproduce%20and%20reuse.%20This%20limitation%20arises%20because%20current%20systems%20are%20highly%0Atailored%20to%20simulator-specific%20use%20cases%20and%20environments.%20IRIS%20is%20a%20novel%2C%0Aeasily%20extendable%20framework%20that%20already%20supports%20multiple%20simulators%2C%0Abenchmarks%2C%20and%20even%20headsets.%20Furthermore%2C%20IRIS%20is%20able%20to%20include%20additional%0Ainformation%20from%20real-world%20sensors%2C%20such%20as%20point%20clouds%20captured%20through%0Adepth%20cameras.%20A%20unified%20scene%20specification%20is%20generated%20directly%20from%0Asimulators%20or%20real-world%20sensors%20and%20transmitted%20to%20XR%20headsets%2C%20creating%0Aidentical%20scenes%20in%20XR.%20This%20specification%20allows%20IRIS%20to%20support%20any%20of%20the%0Aobjects%2C%20assets%2C%20and%20robots%20provided%20by%20the%20simulators.%20In%20addition%2C%20IRIS%0Aintroduces%20shared%20spatial%20anchors%20and%20a%20robust%20communication%20protocol%20that%0Alinks%20simulations%20between%20multiple%20XR%20headsets.%20This%20feature%20enables%20multiple%0AXR%20headsets%20to%20share%20a%20synchronized%20scene%2C%20facilitating%20collaborative%20and%0Amulti-user%20data%20collection.%20IRIS%20can%20be%20deployed%20on%20any%20device%20that%20supports%0Athe%20Unity%20Framework%2C%20encompassing%20the%20vast%20majority%20of%20commercially%20available%0Aheadsets.%20In%20this%20work%2C%20IRIS%20was%20deployed%20and%20tested%20on%20the%20Meta%20Quest%203%20and%0Athe%20HoloLens%202.%20IRIS%20showcased%20its%20versatility%20across%20a%20wide%20range%20of%0Areal-world%20and%20simulated%20scenarios%2C%20using%20current%20popular%20robot%20simulators%20such%0Aas%20MuJoCo%2C%20IsaacSim%2C%20CoppeliaSim%2C%20and%20Genesis.%20In%20addition%2C%20a%20user%20study%0Aevaluates%20IRIS%20on%20a%20data%20collection%20task%20for%20the%20LIBERO%20benchmark.%20The%20study%0Ashows%20that%20IRIS%20significantly%20outperforms%20the%20baseline%20in%20both%20objective%20and%0Asubjective%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03297v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIRIS%253A%2520An%2520Immersive%2520Robot%2520Interaction%2520System%26entry.906535625%3DXinkai%2520Jiang%2520and%2520Qihao%2520Yuan%2520and%2520Enes%2520Ulas%2520Dincer%2520and%2520Hongyi%2520Zhou%2520and%2520Ge%2520Li%2520and%2520Xueyin%2520Li%2520and%2520Julius%2520Haag%2520and%2520Nicolas%2520Schreiber%2520and%2520Kailai%2520Li%2520and%2520Gerhard%2520Neumann%2520and%2520Rudolf%2520Lioutikov%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520IRIS%252C%2520an%2520immersive%2520Robot%2520Interaction%2520System%2520leveraging%250AExtended%2520Reality%2520%2528XR%2529%252C%2520designed%2520for%2520robot%2520data%2520collection%2520and%2520interaction%250Aacross%2520multiple%2520simulators%252C%2520benchmarks%252C%2520and%2520real-world%2520scenarios.%2520While%250Aexisting%2520XR-based%2520data%2520collection%2520systems%2520provide%2520efficient%2520and%2520intuitive%250Asolutions%2520for%2520large-scale%2520data%2520collection%252C%2520they%2520are%2520often%2520challenging%2520to%250Areproduce%2520and%2520reuse.%2520This%2520limitation%2520arises%2520because%2520current%2520systems%2520are%2520highly%250Atailored%2520to%2520simulator-specific%2520use%2520cases%2520and%2520environments.%2520IRIS%2520is%2520a%2520novel%252C%250Aeasily%2520extendable%2520framework%2520that%2520already%2520supports%2520multiple%2520simulators%252C%250Abenchmarks%252C%2520and%2520even%2520headsets.%2520Furthermore%252C%2520IRIS%2520is%2520able%2520to%2520include%2520additional%250Ainformation%2520from%2520real-world%2520sensors%252C%2520such%2520as%2520point%2520clouds%2520captured%2520through%250Adepth%2520cameras.%2520A%2520unified%2520scene%2520specification%2520is%2520generated%2520directly%2520from%250Asimulators%2520or%2520real-world%2520sensors%2520and%2520transmitted%2520to%2520XR%2520headsets%252C%2520creating%250Aidentical%2520scenes%2520in%2520XR.%2520This%2520specification%2520allows%2520IRIS%2520to%2520support%2520any%2520of%2520the%250Aobjects%252C%2520assets%252C%2520and%2520robots%2520provided%2520by%2520the%2520simulators.%2520In%2520addition%252C%2520IRIS%250Aintroduces%2520shared%2520spatial%2520anchors%2520and%2520a%2520robust%2520communication%2520protocol%2520that%250Alinks%2520simulations%2520between%2520multiple%2520XR%2520headsets.%2520This%2520feature%2520enables%2520multiple%250AXR%2520headsets%2520to%2520share%2520a%2520synchronized%2520scene%252C%2520facilitating%2520collaborative%2520and%250Amulti-user%2520data%2520collection.%2520IRIS%2520can%2520be%2520deployed%2520on%2520any%2520device%2520that%2520supports%250Athe%2520Unity%2520Framework%252C%2520encompassing%2520the%2520vast%2520majority%2520of%2520commercially%2520available%250Aheadsets.%2520In%2520this%2520work%252C%2520IRIS%2520was%2520deployed%2520and%2520tested%2520on%2520the%2520Meta%2520Quest%25203%2520and%250Athe%2520HoloLens%25202.%2520IRIS%2520showcased%2520its%2520versatility%2520across%2520a%2520wide%2520range%2520of%250Areal-world%2520and%2520simulated%2520scenarios%252C%2520using%2520current%2520popular%2520robot%2520simulators%2520such%250Aas%2520MuJoCo%252C%2520IsaacSim%252C%2520CoppeliaSim%252C%2520and%2520Genesis.%2520In%2520addition%252C%2520a%2520user%2520study%250Aevaluates%2520IRIS%2520on%2520a%2520data%2520collection%2520task%2520for%2520the%2520LIBERO%2520benchmark.%2520The%2520study%250Ashows%2520that%2520IRIS%2520significantly%2520outperforms%2520the%2520baseline%2520in%2520both%2520objective%2520and%250Asubjective%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03297v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IRIS%3A%20An%20Immersive%20Robot%20Interaction%20System&entry.906535625=Xinkai%20Jiang%20and%20Qihao%20Yuan%20and%20Enes%20Ulas%20Dincer%20and%20Hongyi%20Zhou%20and%20Ge%20Li%20and%20Xueyin%20Li%20and%20Julius%20Haag%20and%20Nicolas%20Schreiber%20and%20Kailai%20Li%20and%20Gerhard%20Neumann%20and%20Rudolf%20Lioutikov&entry.1292438233=%20%20This%20paper%20introduces%20IRIS%2C%20an%20immersive%20Robot%20Interaction%20System%20leveraging%0AExtended%20Reality%20%28XR%29%2C%20designed%20for%20robot%20data%20collection%20and%20interaction%0Aacross%20multiple%20simulators%2C%20benchmarks%2C%20and%20real-world%20scenarios.%20While%0Aexisting%20XR-based%20data%20collection%20systems%20provide%20efficient%20and%20intuitive%0Asolutions%20for%20large-scale%20data%20collection%2C%20they%20are%20often%20challenging%20to%0Areproduce%20and%20reuse.%20This%20limitation%20arises%20because%20current%20systems%20are%20highly%0Atailored%20to%20simulator-specific%20use%20cases%20and%20environments.%20IRIS%20is%20a%20novel%2C%0Aeasily%20extendable%20framework%20that%20already%20supports%20multiple%20simulators%2C%0Abenchmarks%2C%20and%20even%20headsets.%20Furthermore%2C%20IRIS%20is%20able%20to%20include%20additional%0Ainformation%20from%20real-world%20sensors%2C%20such%20as%20point%20clouds%20captured%20through%0Adepth%20cameras.%20A%20unified%20scene%20specification%20is%20generated%20directly%20from%0Asimulators%20or%20real-world%20sensors%20and%20transmitted%20to%20XR%20headsets%2C%20creating%0Aidentical%20scenes%20in%20XR.%20This%20specification%20allows%20IRIS%20to%20support%20any%20of%20the%0Aobjects%2C%20assets%2C%20and%20robots%20provided%20by%20the%20simulators.%20In%20addition%2C%20IRIS%0Aintroduces%20shared%20spatial%20anchors%20and%20a%20robust%20communication%20protocol%20that%0Alinks%20simulations%20between%20multiple%20XR%20headsets.%20This%20feature%20enables%20multiple%0AXR%20headsets%20to%20share%20a%20synchronized%20scene%2C%20facilitating%20collaborative%20and%0Amulti-user%20data%20collection.%20IRIS%20can%20be%20deployed%20on%20any%20device%20that%20supports%0Athe%20Unity%20Framework%2C%20encompassing%20the%20vast%20majority%20of%20commercially%20available%0Aheadsets.%20In%20this%20work%2C%20IRIS%20was%20deployed%20and%20tested%20on%20the%20Meta%20Quest%203%20and%0Athe%20HoloLens%202.%20IRIS%20showcased%20its%20versatility%20across%20a%20wide%20range%20of%0Areal-world%20and%20simulated%20scenarios%2C%20using%20current%20popular%20robot%20simulators%20such%0Aas%20MuJoCo%2C%20IsaacSim%2C%20CoppeliaSim%2C%20and%20Genesis.%20In%20addition%2C%20a%20user%20study%0Aevaluates%20IRIS%20on%20a%20data%20collection%20task%20for%20the%20LIBERO%20benchmark.%20The%20study%0Ashows%20that%20IRIS%20significantly%20outperforms%20the%20baseline%20in%20both%20objective%20and%0Asubjective%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03297v1&entry.124074799=Read"},
{"title": "From Features to Transformers: Redefining Ranking for Scalable Impact", "author": "Fedor Borisyuk and Lars Hertel and Ganesh Parameswaran and Gaurav Srivastava and Sudarshan Srinivasa Ramanujam and Borja Ocejo and Peng Du and Andrei Akterskii and Neil Daftary and Shao Tang and Daqi Sun and Qiang Charles Xiao and Deepesh Nathani and Mohit Kothari and Yun Dai and Aman Gupta", "abstract": "  We present LiGR, a large-scale ranking framework developed at LinkedIn that\nbrings state-of-the-art transformer-based modeling architectures into\nproduction. We introduce a modified transformer architecture that incorporates\nlearned normalization and simultaneous set-wise attention to user history and\nranked items. This architecture enables several breakthrough achievements,\nincluding: (1) the deprecation of most manually designed feature engineering,\noutperforming the prior state-of-the-art system using only few features\n(compared to hundreds in the baseline), (2) validation of the scaling law for\nranking systems, showing improved performance with larger models, more training\ndata, and longer context sequences, and (3) simultaneous joint scoring of items\nin a set-wise manner, leading to automated improvements in diversity. To enable\nefficient serving of large ranking models, we describe techniques to scale\ninference effectively using single-pass processing of user history and set-wise\nattention. We also summarize key insights from various ablation studies and A/B\ntests, highlighting the most impactful technical approaches.\n", "link": "http://arxiv.org/abs/2502.03417v1", "date": "2025-02-05", "relevancy": 2.0242, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5971}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5019}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Features%20to%20Transformers%3A%20Redefining%20Ranking%20for%20Scalable%20Impact&body=Title%3A%20From%20Features%20to%20Transformers%3A%20Redefining%20Ranking%20for%20Scalable%20Impact%0AAuthor%3A%20Fedor%20Borisyuk%20and%20Lars%20Hertel%20and%20Ganesh%20Parameswaran%20and%20Gaurav%20Srivastava%20and%20Sudarshan%20Srinivasa%20Ramanujam%20and%20Borja%20Ocejo%20and%20Peng%20Du%20and%20Andrei%20Akterskii%20and%20Neil%20Daftary%20and%20Shao%20Tang%20and%20Daqi%20Sun%20and%20Qiang%20Charles%20Xiao%20and%20Deepesh%20Nathani%20and%20Mohit%20Kothari%20and%20Yun%20Dai%20and%20Aman%20Gupta%0AAbstract%3A%20%20%20We%20present%20LiGR%2C%20a%20large-scale%20ranking%20framework%20developed%20at%20LinkedIn%20that%0Abrings%20state-of-the-art%20transformer-based%20modeling%20architectures%20into%0Aproduction.%20We%20introduce%20a%20modified%20transformer%20architecture%20that%20incorporates%0Alearned%20normalization%20and%20simultaneous%20set-wise%20attention%20to%20user%20history%20and%0Aranked%20items.%20This%20architecture%20enables%20several%20breakthrough%20achievements%2C%0Aincluding%3A%20%281%29%20the%20deprecation%20of%20most%20manually%20designed%20feature%20engineering%2C%0Aoutperforming%20the%20prior%20state-of-the-art%20system%20using%20only%20few%20features%0A%28compared%20to%20hundreds%20in%20the%20baseline%29%2C%20%282%29%20validation%20of%20the%20scaling%20law%20for%0Aranking%20systems%2C%20showing%20improved%20performance%20with%20larger%20models%2C%20more%20training%0Adata%2C%20and%20longer%20context%20sequences%2C%20and%20%283%29%20simultaneous%20joint%20scoring%20of%20items%0Ain%20a%20set-wise%20manner%2C%20leading%20to%20automated%20improvements%20in%20diversity.%20To%20enable%0Aefficient%20serving%20of%20large%20ranking%20models%2C%20we%20describe%20techniques%20to%20scale%0Ainference%20effectively%20using%20single-pass%20processing%20of%20user%20history%20and%20set-wise%0Aattention.%20We%20also%20summarize%20key%20insights%20from%20various%20ablation%20studies%20and%20A/B%0Atests%2C%20highlighting%20the%20most%20impactful%20technical%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03417v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Features%2520to%2520Transformers%253A%2520Redefining%2520Ranking%2520for%2520Scalable%2520Impact%26entry.906535625%3DFedor%2520Borisyuk%2520and%2520Lars%2520Hertel%2520and%2520Ganesh%2520Parameswaran%2520and%2520Gaurav%2520Srivastava%2520and%2520Sudarshan%2520Srinivasa%2520Ramanujam%2520and%2520Borja%2520Ocejo%2520and%2520Peng%2520Du%2520and%2520Andrei%2520Akterskii%2520and%2520Neil%2520Daftary%2520and%2520Shao%2520Tang%2520and%2520Daqi%2520Sun%2520and%2520Qiang%2520Charles%2520Xiao%2520and%2520Deepesh%2520Nathani%2520and%2520Mohit%2520Kothari%2520and%2520Yun%2520Dai%2520and%2520Aman%2520Gupta%26entry.1292438233%3D%2520%2520We%2520present%2520LiGR%252C%2520a%2520large-scale%2520ranking%2520framework%2520developed%2520at%2520LinkedIn%2520that%250Abrings%2520state-of-the-art%2520transformer-based%2520modeling%2520architectures%2520into%250Aproduction.%2520We%2520introduce%2520a%2520modified%2520transformer%2520architecture%2520that%2520incorporates%250Alearned%2520normalization%2520and%2520simultaneous%2520set-wise%2520attention%2520to%2520user%2520history%2520and%250Aranked%2520items.%2520This%2520architecture%2520enables%2520several%2520breakthrough%2520achievements%252C%250Aincluding%253A%2520%25281%2529%2520the%2520deprecation%2520of%2520most%2520manually%2520designed%2520feature%2520engineering%252C%250Aoutperforming%2520the%2520prior%2520state-of-the-art%2520system%2520using%2520only%2520few%2520features%250A%2528compared%2520to%2520hundreds%2520in%2520the%2520baseline%2529%252C%2520%25282%2529%2520validation%2520of%2520the%2520scaling%2520law%2520for%250Aranking%2520systems%252C%2520showing%2520improved%2520performance%2520with%2520larger%2520models%252C%2520more%2520training%250Adata%252C%2520and%2520longer%2520context%2520sequences%252C%2520and%2520%25283%2529%2520simultaneous%2520joint%2520scoring%2520of%2520items%250Ain%2520a%2520set-wise%2520manner%252C%2520leading%2520to%2520automated%2520improvements%2520in%2520diversity.%2520To%2520enable%250Aefficient%2520serving%2520of%2520large%2520ranking%2520models%252C%2520we%2520describe%2520techniques%2520to%2520scale%250Ainference%2520effectively%2520using%2520single-pass%2520processing%2520of%2520user%2520history%2520and%2520set-wise%250Aattention.%2520We%2520also%2520summarize%2520key%2520insights%2520from%2520various%2520ablation%2520studies%2520and%2520A/B%250Atests%252C%2520highlighting%2520the%2520most%2520impactful%2520technical%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03417v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Features%20to%20Transformers%3A%20Redefining%20Ranking%20for%20Scalable%20Impact&entry.906535625=Fedor%20Borisyuk%20and%20Lars%20Hertel%20and%20Ganesh%20Parameswaran%20and%20Gaurav%20Srivastava%20and%20Sudarshan%20Srinivasa%20Ramanujam%20and%20Borja%20Ocejo%20and%20Peng%20Du%20and%20Andrei%20Akterskii%20and%20Neil%20Daftary%20and%20Shao%20Tang%20and%20Daqi%20Sun%20and%20Qiang%20Charles%20Xiao%20and%20Deepesh%20Nathani%20and%20Mohit%20Kothari%20and%20Yun%20Dai%20and%20Aman%20Gupta&entry.1292438233=%20%20We%20present%20LiGR%2C%20a%20large-scale%20ranking%20framework%20developed%20at%20LinkedIn%20that%0Abrings%20state-of-the-art%20transformer-based%20modeling%20architectures%20into%0Aproduction.%20We%20introduce%20a%20modified%20transformer%20architecture%20that%20incorporates%0Alearned%20normalization%20and%20simultaneous%20set-wise%20attention%20to%20user%20history%20and%0Aranked%20items.%20This%20architecture%20enables%20several%20breakthrough%20achievements%2C%0Aincluding%3A%20%281%29%20the%20deprecation%20of%20most%20manually%20designed%20feature%20engineering%2C%0Aoutperforming%20the%20prior%20state-of-the-art%20system%20using%20only%20few%20features%0A%28compared%20to%20hundreds%20in%20the%20baseline%29%2C%20%282%29%20validation%20of%20the%20scaling%20law%20for%0Aranking%20systems%2C%20showing%20improved%20performance%20with%20larger%20models%2C%20more%20training%0Adata%2C%20and%20longer%20context%20sequences%2C%20and%20%283%29%20simultaneous%20joint%20scoring%20of%20items%0Ain%20a%20set-wise%20manner%2C%20leading%20to%20automated%20improvements%20in%20diversity.%20To%20enable%0Aefficient%20serving%20of%20large%20ranking%20models%2C%20we%20describe%20techniques%20to%20scale%0Ainference%20effectively%20using%20single-pass%20processing%20of%20user%20history%20and%20set-wise%0Aattention.%20We%20also%20summarize%20key%20insights%20from%20various%20ablation%20studies%20and%20A/B%0Atests%2C%20highlighting%20the%20most%20impactful%20technical%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03417v1&entry.124074799=Read"},
{"title": "Adapt-Pruner: Adaptive Structural Pruning for Efficient Small Language\n  Model Training", "author": "Boyao Wang and Rui Pan and Shizhe Diao and Xingyuan Pan and Jipeng Zhang and Renjie Pi and Tong Zhang", "abstract": "  Small language models (SLMs) have attracted considerable attention from both\nacademia and industry due to their broad range of applications in edge devices.\nTo obtain SLMs with strong performance, conventional approaches either\npre-train the models from scratch, which incurs substantial computational\ncosts, or compress/prune existing large language models (LLMs), which results\nin performance drops and falls short in comparison to pre-training. In this\npaper, we investigate the family of acceleration methods that involve both\nstructured pruning and model training. We found 1) layer-wise adaptive pruning\n(Adapt-Pruner) is extremely effective in LLMs and yields significant\nimprovements over existing pruning techniques, 2) adaptive pruning equipped\nwith further training leads to models comparable to those pre-training from\nscratch, 3) incremental pruning brings non-trivial performance gain by\ninterleaving pruning with training and only removing a small portion of neurons\n($\\sim$5%) at a time. Experimental results on LLaMA-3.1-8B demonstrate that\nAdapt-Pruner outperforms conventional pruning methods, such as LLM-Pruner,\nFLAP, and SliceGPT, by an average of 1%-7% in accuracy on commonsense\nbenchmarks. Additionally, Adapt-Pruner restores the performance of\nMobileLLM-125M to 600M on the MMLU benchmark with 200$\\times$ fewer tokens via\npruning from its larger counterparts, and discovers a new 1B model that\nsurpasses LLaMA-3.2-1B in multiple benchmarks.\n", "link": "http://arxiv.org/abs/2502.03460v1", "date": "2025-02-05", "relevancy": 2.0169, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5288}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4913}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adapt-Pruner%3A%20Adaptive%20Structural%20Pruning%20for%20Efficient%20Small%20Language%0A%20%20Model%20Training&body=Title%3A%20Adapt-Pruner%3A%20Adaptive%20Structural%20Pruning%20for%20Efficient%20Small%20Language%0A%20%20Model%20Training%0AAuthor%3A%20Boyao%20Wang%20and%20Rui%20Pan%20and%20Shizhe%20Diao%20and%20Xingyuan%20Pan%20and%20Jipeng%20Zhang%20and%20Renjie%20Pi%20and%20Tong%20Zhang%0AAbstract%3A%20%20%20Small%20language%20models%20%28SLMs%29%20have%20attracted%20considerable%20attention%20from%20both%0Aacademia%20and%20industry%20due%20to%20their%20broad%20range%20of%20applications%20in%20edge%20devices.%0ATo%20obtain%20SLMs%20with%20strong%20performance%2C%20conventional%20approaches%20either%0Apre-train%20the%20models%20from%20scratch%2C%20which%20incurs%20substantial%20computational%0Acosts%2C%20or%20compress/prune%20existing%20large%20language%20models%20%28LLMs%29%2C%20which%20results%0Ain%20performance%20drops%20and%20falls%20short%20in%20comparison%20to%20pre-training.%20In%20this%0Apaper%2C%20we%20investigate%20the%20family%20of%20acceleration%20methods%20that%20involve%20both%0Astructured%20pruning%20and%20model%20training.%20We%20found%201%29%20layer-wise%20adaptive%20pruning%0A%28Adapt-Pruner%29%20is%20extremely%20effective%20in%20LLMs%20and%20yields%20significant%0Aimprovements%20over%20existing%20pruning%20techniques%2C%202%29%20adaptive%20pruning%20equipped%0Awith%20further%20training%20leads%20to%20models%20comparable%20to%20those%20pre-training%20from%0Ascratch%2C%203%29%20incremental%20pruning%20brings%20non-trivial%20performance%20gain%20by%0Ainterleaving%20pruning%20with%20training%20and%20only%20removing%20a%20small%20portion%20of%20neurons%0A%28%24%5Csim%245%25%29%20at%20a%20time.%20Experimental%20results%20on%20LLaMA-3.1-8B%20demonstrate%20that%0AAdapt-Pruner%20outperforms%20conventional%20pruning%20methods%2C%20such%20as%20LLM-Pruner%2C%0AFLAP%2C%20and%20SliceGPT%2C%20by%20an%20average%20of%201%25-7%25%20in%20accuracy%20on%20commonsense%0Abenchmarks.%20Additionally%2C%20Adapt-Pruner%20restores%20the%20performance%20of%0AMobileLLM-125M%20to%20600M%20on%20the%20MMLU%20benchmark%20with%20200%24%5Ctimes%24%20fewer%20tokens%20via%0Apruning%20from%20its%20larger%20counterparts%2C%20and%20discovers%20a%20new%201B%20model%20that%0Asurpasses%20LLaMA-3.2-1B%20in%20multiple%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdapt-Pruner%253A%2520Adaptive%2520Structural%2520Pruning%2520for%2520Efficient%2520Small%2520Language%250A%2520%2520Model%2520Training%26entry.906535625%3DBoyao%2520Wang%2520and%2520Rui%2520Pan%2520and%2520Shizhe%2520Diao%2520and%2520Xingyuan%2520Pan%2520and%2520Jipeng%2520Zhang%2520and%2520Renjie%2520Pi%2520and%2520Tong%2520Zhang%26entry.1292438233%3D%2520%2520Small%2520language%2520models%2520%2528SLMs%2529%2520have%2520attracted%2520considerable%2520attention%2520from%2520both%250Aacademia%2520and%2520industry%2520due%2520to%2520their%2520broad%2520range%2520of%2520applications%2520in%2520edge%2520devices.%250ATo%2520obtain%2520SLMs%2520with%2520strong%2520performance%252C%2520conventional%2520approaches%2520either%250Apre-train%2520the%2520models%2520from%2520scratch%252C%2520which%2520incurs%2520substantial%2520computational%250Acosts%252C%2520or%2520compress/prune%2520existing%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520which%2520results%250Ain%2520performance%2520drops%2520and%2520falls%2520short%2520in%2520comparison%2520to%2520pre-training.%2520In%2520this%250Apaper%252C%2520we%2520investigate%2520the%2520family%2520of%2520acceleration%2520methods%2520that%2520involve%2520both%250Astructured%2520pruning%2520and%2520model%2520training.%2520We%2520found%25201%2529%2520layer-wise%2520adaptive%2520pruning%250A%2528Adapt-Pruner%2529%2520is%2520extremely%2520effective%2520in%2520LLMs%2520and%2520yields%2520significant%250Aimprovements%2520over%2520existing%2520pruning%2520techniques%252C%25202%2529%2520adaptive%2520pruning%2520equipped%250Awith%2520further%2520training%2520leads%2520to%2520models%2520comparable%2520to%2520those%2520pre-training%2520from%250Ascratch%252C%25203%2529%2520incremental%2520pruning%2520brings%2520non-trivial%2520performance%2520gain%2520by%250Ainterleaving%2520pruning%2520with%2520training%2520and%2520only%2520removing%2520a%2520small%2520portion%2520of%2520neurons%250A%2528%2524%255Csim%25245%2525%2529%2520at%2520a%2520time.%2520Experimental%2520results%2520on%2520LLaMA-3.1-8B%2520demonstrate%2520that%250AAdapt-Pruner%2520outperforms%2520conventional%2520pruning%2520methods%252C%2520such%2520as%2520LLM-Pruner%252C%250AFLAP%252C%2520and%2520SliceGPT%252C%2520by%2520an%2520average%2520of%25201%2525-7%2525%2520in%2520accuracy%2520on%2520commonsense%250Abenchmarks.%2520Additionally%252C%2520Adapt-Pruner%2520restores%2520the%2520performance%2520of%250AMobileLLM-125M%2520to%2520600M%2520on%2520the%2520MMLU%2520benchmark%2520with%2520200%2524%255Ctimes%2524%2520fewer%2520tokens%2520via%250Apruning%2520from%2520its%2520larger%2520counterparts%252C%2520and%2520discovers%2520a%2520new%25201B%2520model%2520that%250Asurpasses%2520LLaMA-3.2-1B%2520in%2520multiple%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adapt-Pruner%3A%20Adaptive%20Structural%20Pruning%20for%20Efficient%20Small%20Language%0A%20%20Model%20Training&entry.906535625=Boyao%20Wang%20and%20Rui%20Pan%20and%20Shizhe%20Diao%20and%20Xingyuan%20Pan%20and%20Jipeng%20Zhang%20and%20Renjie%20Pi%20and%20Tong%20Zhang&entry.1292438233=%20%20Small%20language%20models%20%28SLMs%29%20have%20attracted%20considerable%20attention%20from%20both%0Aacademia%20and%20industry%20due%20to%20their%20broad%20range%20of%20applications%20in%20edge%20devices.%0ATo%20obtain%20SLMs%20with%20strong%20performance%2C%20conventional%20approaches%20either%0Apre-train%20the%20models%20from%20scratch%2C%20which%20incurs%20substantial%20computational%0Acosts%2C%20or%20compress/prune%20existing%20large%20language%20models%20%28LLMs%29%2C%20which%20results%0Ain%20performance%20drops%20and%20falls%20short%20in%20comparison%20to%20pre-training.%20In%20this%0Apaper%2C%20we%20investigate%20the%20family%20of%20acceleration%20methods%20that%20involve%20both%0Astructured%20pruning%20and%20model%20training.%20We%20found%201%29%20layer-wise%20adaptive%20pruning%0A%28Adapt-Pruner%29%20is%20extremely%20effective%20in%20LLMs%20and%20yields%20significant%0Aimprovements%20over%20existing%20pruning%20techniques%2C%202%29%20adaptive%20pruning%20equipped%0Awith%20further%20training%20leads%20to%20models%20comparable%20to%20those%20pre-training%20from%0Ascratch%2C%203%29%20incremental%20pruning%20brings%20non-trivial%20performance%20gain%20by%0Ainterleaving%20pruning%20with%20training%20and%20only%20removing%20a%20small%20portion%20of%20neurons%0A%28%24%5Csim%245%25%29%20at%20a%20time.%20Experimental%20results%20on%20LLaMA-3.1-8B%20demonstrate%20that%0AAdapt-Pruner%20outperforms%20conventional%20pruning%20methods%2C%20such%20as%20LLM-Pruner%2C%0AFLAP%2C%20and%20SliceGPT%2C%20by%20an%20average%20of%201%25-7%25%20in%20accuracy%20on%20commonsense%0Abenchmarks.%20Additionally%2C%20Adapt-Pruner%20restores%20the%20performance%20of%0AMobileLLM-125M%20to%20600M%20on%20the%20MMLU%20benchmark%20with%20200%24%5Ctimes%24%20fewer%20tokens%20via%0Apruning%20from%20its%20larger%20counterparts%2C%20and%20discovers%20a%20new%201B%20model%20that%0Asurpasses%20LLaMA-3.2-1B%20in%20multiple%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03460v1&entry.124074799=Read"},
{"title": "An Algebraically Converging Stochastic Gradient Descent Algorithm for\n  Global Optimization", "author": "Bj\u00f6rn Engquist and Kui Ren and Yunan Yang", "abstract": "  We propose a new gradient descent algorithm with added stochastic terms for\nfinding the global optimizers of nonconvex optimization problems. A key\ncomponent in the algorithm is the adaptive tuning of the randomness based on\nthe value of the objective function. In the language of simulated annealing,\nthe temperature is state-dependent. With this, we prove the global convergence\nof the algorithm with an algebraic rate both in probability and in the\nparameter space. This is a significant improvement over the classical rate from\nusing a more straightforward control of the noise term. The convergence proof\nis based on the actual discrete setup of the algorithm, not just its continuous\nlimit as often done in the literature. We also present several numerical\nexamples to demonstrate the efficiency and robustness of the algorithm for\nreasonably complex objective functions.\n", "link": "http://arxiv.org/abs/2204.05923v4", "date": "2025-02-05", "relevancy": 1.7396, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4393}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4347}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Algebraically%20Converging%20Stochastic%20Gradient%20Descent%20Algorithm%20for%0A%20%20Global%20Optimization&body=Title%3A%20An%20Algebraically%20Converging%20Stochastic%20Gradient%20Descent%20Algorithm%20for%0A%20%20Global%20Optimization%0AAuthor%3A%20Bj%C3%B6rn%20Engquist%20and%20Kui%20Ren%20and%20Yunan%20Yang%0AAbstract%3A%20%20%20We%20propose%20a%20new%20gradient%20descent%20algorithm%20with%20added%20stochastic%20terms%20for%0Afinding%20the%20global%20optimizers%20of%20nonconvex%20optimization%20problems.%20A%20key%0Acomponent%20in%20the%20algorithm%20is%20the%20adaptive%20tuning%20of%20the%20randomness%20based%20on%0Athe%20value%20of%20the%20objective%20function.%20In%20the%20language%20of%20simulated%20annealing%2C%0Athe%20temperature%20is%20state-dependent.%20With%20this%2C%20we%20prove%20the%20global%20convergence%0Aof%20the%20algorithm%20with%20an%20algebraic%20rate%20both%20in%20probability%20and%20in%20the%0Aparameter%20space.%20This%20is%20a%20significant%20improvement%20over%20the%20classical%20rate%20from%0Ausing%20a%20more%20straightforward%20control%20of%20the%20noise%20term.%20The%20convergence%20proof%0Ais%20based%20on%20the%20actual%20discrete%20setup%20of%20the%20algorithm%2C%20not%20just%20its%20continuous%0Alimit%20as%20often%20done%20in%20the%20literature.%20We%20also%20present%20several%20numerical%0Aexamples%20to%20demonstrate%20the%20efficiency%20and%20robustness%20of%20the%20algorithm%20for%0Areasonably%20complex%20objective%20functions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2204.05923v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Algebraically%2520Converging%2520Stochastic%2520Gradient%2520Descent%2520Algorithm%2520for%250A%2520%2520Global%2520Optimization%26entry.906535625%3DBj%25C3%25B6rn%2520Engquist%2520and%2520Kui%2520Ren%2520and%2520Yunan%2520Yang%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520new%2520gradient%2520descent%2520algorithm%2520with%2520added%2520stochastic%2520terms%2520for%250Afinding%2520the%2520global%2520optimizers%2520of%2520nonconvex%2520optimization%2520problems.%2520A%2520key%250Acomponent%2520in%2520the%2520algorithm%2520is%2520the%2520adaptive%2520tuning%2520of%2520the%2520randomness%2520based%2520on%250Athe%2520value%2520of%2520the%2520objective%2520function.%2520In%2520the%2520language%2520of%2520simulated%2520annealing%252C%250Athe%2520temperature%2520is%2520state-dependent.%2520With%2520this%252C%2520we%2520prove%2520the%2520global%2520convergence%250Aof%2520the%2520algorithm%2520with%2520an%2520algebraic%2520rate%2520both%2520in%2520probability%2520and%2520in%2520the%250Aparameter%2520space.%2520This%2520is%2520a%2520significant%2520improvement%2520over%2520the%2520classical%2520rate%2520from%250Ausing%2520a%2520more%2520straightforward%2520control%2520of%2520the%2520noise%2520term.%2520The%2520convergence%2520proof%250Ais%2520based%2520on%2520the%2520actual%2520discrete%2520setup%2520of%2520the%2520algorithm%252C%2520not%2520just%2520its%2520continuous%250Alimit%2520as%2520often%2520done%2520in%2520the%2520literature.%2520We%2520also%2520present%2520several%2520numerical%250Aexamples%2520to%2520demonstrate%2520the%2520efficiency%2520and%2520robustness%2520of%2520the%2520algorithm%2520for%250Areasonably%2520complex%2520objective%2520functions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2204.05923v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Algebraically%20Converging%20Stochastic%20Gradient%20Descent%20Algorithm%20for%0A%20%20Global%20Optimization&entry.906535625=Bj%C3%B6rn%20Engquist%20and%20Kui%20Ren%20and%20Yunan%20Yang&entry.1292438233=%20%20We%20propose%20a%20new%20gradient%20descent%20algorithm%20with%20added%20stochastic%20terms%20for%0Afinding%20the%20global%20optimizers%20of%20nonconvex%20optimization%20problems.%20A%20key%0Acomponent%20in%20the%20algorithm%20is%20the%20adaptive%20tuning%20of%20the%20randomness%20based%20on%0Athe%20value%20of%20the%20objective%20function.%20In%20the%20language%20of%20simulated%20annealing%2C%0Athe%20temperature%20is%20state-dependent.%20With%20this%2C%20we%20prove%20the%20global%20convergence%0Aof%20the%20algorithm%20with%20an%20algebraic%20rate%20both%20in%20probability%20and%20in%20the%0Aparameter%20space.%20This%20is%20a%20significant%20improvement%20over%20the%20classical%20rate%20from%0Ausing%20a%20more%20straightforward%20control%20of%20the%20noise%20term.%20The%20convergence%20proof%0Ais%20based%20on%20the%20actual%20discrete%20setup%20of%20the%20algorithm%2C%20not%20just%20its%20continuous%0Alimit%20as%20often%20done%20in%20the%20literature.%20We%20also%20present%20several%20numerical%0Aexamples%20to%20demonstrate%20the%20efficiency%20and%20robustness%20of%20the%20algorithm%20for%0Areasonably%20complex%20objective%20functions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2204.05923v4&entry.124074799=Read"},
{"title": "On Fairness of Unified Multimodal Large Language Model for Image\n  Generation", "author": "Ming Liu and Hao Chen and Jindong Wang and Liwen Wang and Bhiksha Raj Ramakrishnan and Wensheng Zhang", "abstract": "  Unified multimodal large language models (U-MLLMs) have demonstrated\nimpressive performance in visual understanding and generation in an end-to-end\npipeline. Compared with generation-only models (e.g., Stable Diffusion),\nU-MLLMs may raise new questions about bias in their outputs, which can be\naffected by their unified capabilities. This gap is particularly concerning\ngiven the under-explored risk of propagating harmful stereotypes. In this\npaper, we benchmark the latest U-MLLMs and find that most exhibit significant\ndemographic biases, such as gender and race bias. To better understand and\nmitigate this issue, we propose a locate-then-fix strategy, where we audit and\nshow how the individual model component is affected by bias. Our analysis shows\nthat bias originates primarily from the language model. More interestingly, we\nobserve a \"partial alignment\" phenomenon in U-MLLMs, where understanding bias\nappears minimal, but generation bias remains substantial. Thus, we propose a\nnovel balanced preference model to balance the demographic distribution with\nsynthetic data. Experiments demonstrate that our approach reduces demographic\nbias while preserving semantic fidelity. We hope our findings underscore the\nneed for more holistic interpretation and debiasing strategies of U-MLLMs in\nthe future.\n", "link": "http://arxiv.org/abs/2502.03429v1", "date": "2025-02-05", "relevancy": 1.1113, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5671}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5519}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.548}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Fairness%20of%20Unified%20Multimodal%20Large%20Language%20Model%20for%20Image%0A%20%20Generation&body=Title%3A%20On%20Fairness%20of%20Unified%20Multimodal%20Large%20Language%20Model%20for%20Image%0A%20%20Generation%0AAuthor%3A%20Ming%20Liu%20and%20Hao%20Chen%20and%20Jindong%20Wang%20and%20Liwen%20Wang%20and%20Bhiksha%20Raj%20Ramakrishnan%20and%20Wensheng%20Zhang%0AAbstract%3A%20%20%20Unified%20multimodal%20large%20language%20models%20%28U-MLLMs%29%20have%20demonstrated%0Aimpressive%20performance%20in%20visual%20understanding%20and%20generation%20in%20an%20end-to-end%0Apipeline.%20Compared%20with%20generation-only%20models%20%28e.g.%2C%20Stable%20Diffusion%29%2C%0AU-MLLMs%20may%20raise%20new%20questions%20about%20bias%20in%20their%20outputs%2C%20which%20can%20be%0Aaffected%20by%20their%20unified%20capabilities.%20This%20gap%20is%20particularly%20concerning%0Agiven%20the%20under-explored%20risk%20of%20propagating%20harmful%20stereotypes.%20In%20this%0Apaper%2C%20we%20benchmark%20the%20latest%20U-MLLMs%20and%20find%20that%20most%20exhibit%20significant%0Ademographic%20biases%2C%20such%20as%20gender%20and%20race%20bias.%20To%20better%20understand%20and%0Amitigate%20this%20issue%2C%20we%20propose%20a%20locate-then-fix%20strategy%2C%20where%20we%20audit%20and%0Ashow%20how%20the%20individual%20model%20component%20is%20affected%20by%20bias.%20Our%20analysis%20shows%0Athat%20bias%20originates%20primarily%20from%20the%20language%20model.%20More%20interestingly%2C%20we%0Aobserve%20a%20%22partial%20alignment%22%20phenomenon%20in%20U-MLLMs%2C%20where%20understanding%20bias%0Aappears%20minimal%2C%20but%20generation%20bias%20remains%20substantial.%20Thus%2C%20we%20propose%20a%0Anovel%20balanced%20preference%20model%20to%20balance%20the%20demographic%20distribution%20with%0Asynthetic%20data.%20Experiments%20demonstrate%20that%20our%20approach%20reduces%20demographic%0Abias%20while%20preserving%20semantic%20fidelity.%20We%20hope%20our%20findings%20underscore%20the%0Aneed%20for%20more%20holistic%20interpretation%20and%20debiasing%20strategies%20of%20U-MLLMs%20in%0Athe%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03429v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Fairness%2520of%2520Unified%2520Multimodal%2520Large%2520Language%2520Model%2520for%2520Image%250A%2520%2520Generation%26entry.906535625%3DMing%2520Liu%2520and%2520Hao%2520Chen%2520and%2520Jindong%2520Wang%2520and%2520Liwen%2520Wang%2520and%2520Bhiksha%2520Raj%2520Ramakrishnan%2520and%2520Wensheng%2520Zhang%26entry.1292438233%3D%2520%2520Unified%2520multimodal%2520large%2520language%2520models%2520%2528U-MLLMs%2529%2520have%2520demonstrated%250Aimpressive%2520performance%2520in%2520visual%2520understanding%2520and%2520generation%2520in%2520an%2520end-to-end%250Apipeline.%2520Compared%2520with%2520generation-only%2520models%2520%2528e.g.%252C%2520Stable%2520Diffusion%2529%252C%250AU-MLLMs%2520may%2520raise%2520new%2520questions%2520about%2520bias%2520in%2520their%2520outputs%252C%2520which%2520can%2520be%250Aaffected%2520by%2520their%2520unified%2520capabilities.%2520This%2520gap%2520is%2520particularly%2520concerning%250Agiven%2520the%2520under-explored%2520risk%2520of%2520propagating%2520harmful%2520stereotypes.%2520In%2520this%250Apaper%252C%2520we%2520benchmark%2520the%2520latest%2520U-MLLMs%2520and%2520find%2520that%2520most%2520exhibit%2520significant%250Ademographic%2520biases%252C%2520such%2520as%2520gender%2520and%2520race%2520bias.%2520To%2520better%2520understand%2520and%250Amitigate%2520this%2520issue%252C%2520we%2520propose%2520a%2520locate-then-fix%2520strategy%252C%2520where%2520we%2520audit%2520and%250Ashow%2520how%2520the%2520individual%2520model%2520component%2520is%2520affected%2520by%2520bias.%2520Our%2520analysis%2520shows%250Athat%2520bias%2520originates%2520primarily%2520from%2520the%2520language%2520model.%2520More%2520interestingly%252C%2520we%250Aobserve%2520a%2520%2522partial%2520alignment%2522%2520phenomenon%2520in%2520U-MLLMs%252C%2520where%2520understanding%2520bias%250Aappears%2520minimal%252C%2520but%2520generation%2520bias%2520remains%2520substantial.%2520Thus%252C%2520we%2520propose%2520a%250Anovel%2520balanced%2520preference%2520model%2520to%2520balance%2520the%2520demographic%2520distribution%2520with%250Asynthetic%2520data.%2520Experiments%2520demonstrate%2520that%2520our%2520approach%2520reduces%2520demographic%250Abias%2520while%2520preserving%2520semantic%2520fidelity.%2520We%2520hope%2520our%2520findings%2520underscore%2520the%250Aneed%2520for%2520more%2520holistic%2520interpretation%2520and%2520debiasing%2520strategies%2520of%2520U-MLLMs%2520in%250Athe%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03429v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Fairness%20of%20Unified%20Multimodal%20Large%20Language%20Model%20for%20Image%0A%20%20Generation&entry.906535625=Ming%20Liu%20and%20Hao%20Chen%20and%20Jindong%20Wang%20and%20Liwen%20Wang%20and%20Bhiksha%20Raj%20Ramakrishnan%20and%20Wensheng%20Zhang&entry.1292438233=%20%20Unified%20multimodal%20large%20language%20models%20%28U-MLLMs%29%20have%20demonstrated%0Aimpressive%20performance%20in%20visual%20understanding%20and%20generation%20in%20an%20end-to-end%0Apipeline.%20Compared%20with%20generation-only%20models%20%28e.g.%2C%20Stable%20Diffusion%29%2C%0AU-MLLMs%20may%20raise%20new%20questions%20about%20bias%20in%20their%20outputs%2C%20which%20can%20be%0Aaffected%20by%20their%20unified%20capabilities.%20This%20gap%20is%20particularly%20concerning%0Agiven%20the%20under-explored%20risk%20of%20propagating%20harmful%20stereotypes.%20In%20this%0Apaper%2C%20we%20benchmark%20the%20latest%20U-MLLMs%20and%20find%20that%20most%20exhibit%20significant%0Ademographic%20biases%2C%20such%20as%20gender%20and%20race%20bias.%20To%20better%20understand%20and%0Amitigate%20this%20issue%2C%20we%20propose%20a%20locate-then-fix%20strategy%2C%20where%20we%20audit%20and%0Ashow%20how%20the%20individual%20model%20component%20is%20affected%20by%20bias.%20Our%20analysis%20shows%0Athat%20bias%20originates%20primarily%20from%20the%20language%20model.%20More%20interestingly%2C%20we%0Aobserve%20a%20%22partial%20alignment%22%20phenomenon%20in%20U-MLLMs%2C%20where%20understanding%20bias%0Aappears%20minimal%2C%20but%20generation%20bias%20remains%20substantial.%20Thus%2C%20we%20propose%20a%0Anovel%20balanced%20preference%20model%20to%20balance%20the%20demographic%20distribution%20with%0Asynthetic%20data.%20Experiments%20demonstrate%20that%20our%20approach%20reduces%20demographic%0Abias%20while%20preserving%20semantic%20fidelity.%20We%20hope%20our%20findings%20underscore%20the%0Aneed%20for%20more%20holistic%20interpretation%20and%20debiasing%20strategies%20of%20U-MLLMs%20in%0Athe%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03429v1&entry.124074799=Read"},
{"title": "Symmetry-Aware Bayesian Flow Networks for Crystal Generation", "author": "Laura Ruple and Luca Torresi and Henrik Schopmans and Pascal Friederich", "abstract": "  The discovery of new crystalline materials is essential to scientific and\ntechnological progress. However, traditional trial-and-error approaches are\ninefficient due to the vast search space. Recent advancements in machine\nlearning have enabled generative models to predict new stable materials by\nincorporating structural symmetries and to condition the generation on desired\nproperties. In this work, we introduce SymmBFN, a novel symmetry-aware Bayesian\nFlow Network (BFN) for crystalline material generation that accurately\nreproduces the distribution of space groups found in experimentally observed\ncrystals. SymmBFN substantially improves efficiency, generating stable\nstructures at least 50 times faster than the next-best method. Furthermore, we\ndemonstrate its capability for property-conditioned generation, enabling the\ndesign of materials with tailored properties. Our findings establish BFNs as an\neffective tool for accelerating the discovery of crystalline materials.\n", "link": "http://arxiv.org/abs/2502.03146v1", "date": "2025-02-05", "relevancy": 1.3711, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4969}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4541}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Symmetry-Aware%20Bayesian%20Flow%20Networks%20for%20Crystal%20Generation&body=Title%3A%20Symmetry-Aware%20Bayesian%20Flow%20Networks%20for%20Crystal%20Generation%0AAuthor%3A%20Laura%20Ruple%20and%20Luca%20Torresi%20and%20Henrik%20Schopmans%20and%20Pascal%20Friederich%0AAbstract%3A%20%20%20The%20discovery%20of%20new%20crystalline%20materials%20is%20essential%20to%20scientific%20and%0Atechnological%20progress.%20However%2C%20traditional%20trial-and-error%20approaches%20are%0Ainefficient%20due%20to%20the%20vast%20search%20space.%20Recent%20advancements%20in%20machine%0Alearning%20have%20enabled%20generative%20models%20to%20predict%20new%20stable%20materials%20by%0Aincorporating%20structural%20symmetries%20and%20to%20condition%20the%20generation%20on%20desired%0Aproperties.%20In%20this%20work%2C%20we%20introduce%20SymmBFN%2C%20a%20novel%20symmetry-aware%20Bayesian%0AFlow%20Network%20%28BFN%29%20for%20crystalline%20material%20generation%20that%20accurately%0Areproduces%20the%20distribution%20of%20space%20groups%20found%20in%20experimentally%20observed%0Acrystals.%20SymmBFN%20substantially%20improves%20efficiency%2C%20generating%20stable%0Astructures%20at%20least%2050%20times%20faster%20than%20the%20next-best%20method.%20Furthermore%2C%20we%0Ademonstrate%20its%20capability%20for%20property-conditioned%20generation%2C%20enabling%20the%0Adesign%20of%20materials%20with%20tailored%20properties.%20Our%20findings%20establish%20BFNs%20as%20an%0Aeffective%20tool%20for%20accelerating%20the%20discovery%20of%20crystalline%20materials.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03146v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSymmetry-Aware%2520Bayesian%2520Flow%2520Networks%2520for%2520Crystal%2520Generation%26entry.906535625%3DLaura%2520Ruple%2520and%2520Luca%2520Torresi%2520and%2520Henrik%2520Schopmans%2520and%2520Pascal%2520Friederich%26entry.1292438233%3D%2520%2520The%2520discovery%2520of%2520new%2520crystalline%2520materials%2520is%2520essential%2520to%2520scientific%2520and%250Atechnological%2520progress.%2520However%252C%2520traditional%2520trial-and-error%2520approaches%2520are%250Ainefficient%2520due%2520to%2520the%2520vast%2520search%2520space.%2520Recent%2520advancements%2520in%2520machine%250Alearning%2520have%2520enabled%2520generative%2520models%2520to%2520predict%2520new%2520stable%2520materials%2520by%250Aincorporating%2520structural%2520symmetries%2520and%2520to%2520condition%2520the%2520generation%2520on%2520desired%250Aproperties.%2520In%2520this%2520work%252C%2520we%2520introduce%2520SymmBFN%252C%2520a%2520novel%2520symmetry-aware%2520Bayesian%250AFlow%2520Network%2520%2528BFN%2529%2520for%2520crystalline%2520material%2520generation%2520that%2520accurately%250Areproduces%2520the%2520distribution%2520of%2520space%2520groups%2520found%2520in%2520experimentally%2520observed%250Acrystals.%2520SymmBFN%2520substantially%2520improves%2520efficiency%252C%2520generating%2520stable%250Astructures%2520at%2520least%252050%2520times%2520faster%2520than%2520the%2520next-best%2520method.%2520Furthermore%252C%2520we%250Ademonstrate%2520its%2520capability%2520for%2520property-conditioned%2520generation%252C%2520enabling%2520the%250Adesign%2520of%2520materials%2520with%2520tailored%2520properties.%2520Our%2520findings%2520establish%2520BFNs%2520as%2520an%250Aeffective%2520tool%2520for%2520accelerating%2520the%2520discovery%2520of%2520crystalline%2520materials.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03146v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Symmetry-Aware%20Bayesian%20Flow%20Networks%20for%20Crystal%20Generation&entry.906535625=Laura%20Ruple%20and%20Luca%20Torresi%20and%20Henrik%20Schopmans%20and%20Pascal%20Friederich&entry.1292438233=%20%20The%20discovery%20of%20new%20crystalline%20materials%20is%20essential%20to%20scientific%20and%0Atechnological%20progress.%20However%2C%20traditional%20trial-and-error%20approaches%20are%0Ainefficient%20due%20to%20the%20vast%20search%20space.%20Recent%20advancements%20in%20machine%0Alearning%20have%20enabled%20generative%20models%20to%20predict%20new%20stable%20materials%20by%0Aincorporating%20structural%20symmetries%20and%20to%20condition%20the%20generation%20on%20desired%0Aproperties.%20In%20this%20work%2C%20we%20introduce%20SymmBFN%2C%20a%20novel%20symmetry-aware%20Bayesian%0AFlow%20Network%20%28BFN%29%20for%20crystalline%20material%20generation%20that%20accurately%0Areproduces%20the%20distribution%20of%20space%20groups%20found%20in%20experimentally%20observed%0Acrystals.%20SymmBFN%20substantially%20improves%20efficiency%2C%20generating%20stable%0Astructures%20at%20least%2050%20times%20faster%20than%20the%20next-best%20method.%20Furthermore%2C%20we%0Ademonstrate%20its%20capability%20for%20property-conditioned%20generation%2C%20enabling%20the%0Adesign%20of%20materials%20with%20tailored%20properties.%20Our%20findings%20establish%20BFNs%20as%20an%0Aeffective%20tool%20for%20accelerating%20the%20discovery%20of%20crystalline%20materials.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03146v1&entry.124074799=Read"},
{"title": "Context in Public Health for Underserved Communities: A Bayesian\n  Approach to Online Restless Bandits", "author": "Biyonka Liang and Lily Xu and Aparna Taneja and Milind Tambe and Lucas Janson", "abstract": "  Public health programs often provide interventions to encourage program\nadherence, and effectively allocating interventions is vital for producing the\ngreatest overall health outcomes, especially in underserved communities where\nresources are limited. Such resource allocation problems are often modeled as\nrestless multi-armed bandits (RMABs) with unknown underlying transition\ndynamics, hence requiring online reinforcement learning (RL). We present\nBayesian Learning for Contextual RMABs (BCoR), an online RL approach for RMABs\nthat novelly combines techniques in Bayesian modeling with Thompson sampling to\nflexibly model the complex RMAB settings present in public health program\nadherence problems, namely context and non-stationarity. BCoR's key strength is\nthe ability to leverage shared information within and between arms to learn the\nunknown RMAB transition dynamics quickly in intervention-scarce settings with\nrelatively short time horizons, which is common in public health applications.\nEmpirically, BCoR achieves substantially higher finite-sample performance over\na range of experimental settings, including a setting using real-world\nadherence data that was developed in collaboration with ARMMAN, an NGO in India\nwhich runs a large-scale maternal mHealth program, showcasing BCoR practical\nutility and potential for real-world deployment.\n", "link": "http://arxiv.org/abs/2402.04933v3", "date": "2025-02-05", "relevancy": 1.4031, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5369}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.464}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context%20in%20Public%20Health%20for%20Underserved%20Communities%3A%20A%20Bayesian%0A%20%20Approach%20to%20Online%20Restless%20Bandits&body=Title%3A%20Context%20in%20Public%20Health%20for%20Underserved%20Communities%3A%20A%20Bayesian%0A%20%20Approach%20to%20Online%20Restless%20Bandits%0AAuthor%3A%20Biyonka%20Liang%20and%20Lily%20Xu%20and%20Aparna%20Taneja%20and%20Milind%20Tambe%20and%20Lucas%20Janson%0AAbstract%3A%20%20%20Public%20health%20programs%20often%20provide%20interventions%20to%20encourage%20program%0Aadherence%2C%20and%20effectively%20allocating%20interventions%20is%20vital%20for%20producing%20the%0Agreatest%20overall%20health%20outcomes%2C%20especially%20in%20underserved%20communities%20where%0Aresources%20are%20limited.%20Such%20resource%20allocation%20problems%20are%20often%20modeled%20as%0Arestless%20multi-armed%20bandits%20%28RMABs%29%20with%20unknown%20underlying%20transition%0Adynamics%2C%20hence%20requiring%20online%20reinforcement%20learning%20%28RL%29.%20We%20present%0ABayesian%20Learning%20for%20Contextual%20RMABs%20%28BCoR%29%2C%20an%20online%20RL%20approach%20for%20RMABs%0Athat%20novelly%20combines%20techniques%20in%20Bayesian%20modeling%20with%20Thompson%20sampling%20to%0Aflexibly%20model%20the%20complex%20RMAB%20settings%20present%20in%20public%20health%20program%0Aadherence%20problems%2C%20namely%20context%20and%20non-stationarity.%20BCoR%27s%20key%20strength%20is%0Athe%20ability%20to%20leverage%20shared%20information%20within%20and%20between%20arms%20to%20learn%20the%0Aunknown%20RMAB%20transition%20dynamics%20quickly%20in%20intervention-scarce%20settings%20with%0Arelatively%20short%20time%20horizons%2C%20which%20is%20common%20in%20public%20health%20applications.%0AEmpirically%2C%20BCoR%20achieves%20substantially%20higher%20finite-sample%20performance%20over%0Aa%20range%20of%20experimental%20settings%2C%20including%20a%20setting%20using%20real-world%0Aadherence%20data%20that%20was%20developed%20in%20collaboration%20with%20ARMMAN%2C%20an%20NGO%20in%20India%0Awhich%20runs%20a%20large-scale%20maternal%20mHealth%20program%2C%20showcasing%20BCoR%20practical%0Autility%20and%20potential%20for%20real-world%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.04933v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext%2520in%2520Public%2520Health%2520for%2520Underserved%2520Communities%253A%2520A%2520Bayesian%250A%2520%2520Approach%2520to%2520Online%2520Restless%2520Bandits%26entry.906535625%3DBiyonka%2520Liang%2520and%2520Lily%2520Xu%2520and%2520Aparna%2520Taneja%2520and%2520Milind%2520Tambe%2520and%2520Lucas%2520Janson%26entry.1292438233%3D%2520%2520Public%2520health%2520programs%2520often%2520provide%2520interventions%2520to%2520encourage%2520program%250Aadherence%252C%2520and%2520effectively%2520allocating%2520interventions%2520is%2520vital%2520for%2520producing%2520the%250Agreatest%2520overall%2520health%2520outcomes%252C%2520especially%2520in%2520underserved%2520communities%2520where%250Aresources%2520are%2520limited.%2520Such%2520resource%2520allocation%2520problems%2520are%2520often%2520modeled%2520as%250Arestless%2520multi-armed%2520bandits%2520%2528RMABs%2529%2520with%2520unknown%2520underlying%2520transition%250Adynamics%252C%2520hence%2520requiring%2520online%2520reinforcement%2520learning%2520%2528RL%2529.%2520We%2520present%250ABayesian%2520Learning%2520for%2520Contextual%2520RMABs%2520%2528BCoR%2529%252C%2520an%2520online%2520RL%2520approach%2520for%2520RMABs%250Athat%2520novelly%2520combines%2520techniques%2520in%2520Bayesian%2520modeling%2520with%2520Thompson%2520sampling%2520to%250Aflexibly%2520model%2520the%2520complex%2520RMAB%2520settings%2520present%2520in%2520public%2520health%2520program%250Aadherence%2520problems%252C%2520namely%2520context%2520and%2520non-stationarity.%2520BCoR%2527s%2520key%2520strength%2520is%250Athe%2520ability%2520to%2520leverage%2520shared%2520information%2520within%2520and%2520between%2520arms%2520to%2520learn%2520the%250Aunknown%2520RMAB%2520transition%2520dynamics%2520quickly%2520in%2520intervention-scarce%2520settings%2520with%250Arelatively%2520short%2520time%2520horizons%252C%2520which%2520is%2520common%2520in%2520public%2520health%2520applications.%250AEmpirically%252C%2520BCoR%2520achieves%2520substantially%2520higher%2520finite-sample%2520performance%2520over%250Aa%2520range%2520of%2520experimental%2520settings%252C%2520including%2520a%2520setting%2520using%2520real-world%250Aadherence%2520data%2520that%2520was%2520developed%2520in%2520collaboration%2520with%2520ARMMAN%252C%2520an%2520NGO%2520in%2520India%250Awhich%2520runs%2520a%2520large-scale%2520maternal%2520mHealth%2520program%252C%2520showcasing%2520BCoR%2520practical%250Autility%2520and%2520potential%2520for%2520real-world%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.04933v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context%20in%20Public%20Health%20for%20Underserved%20Communities%3A%20A%20Bayesian%0A%20%20Approach%20to%20Online%20Restless%20Bandits&entry.906535625=Biyonka%20Liang%20and%20Lily%20Xu%20and%20Aparna%20Taneja%20and%20Milind%20Tambe%20and%20Lucas%20Janson&entry.1292438233=%20%20Public%20health%20programs%20often%20provide%20interventions%20to%20encourage%20program%0Aadherence%2C%20and%20effectively%20allocating%20interventions%20is%20vital%20for%20producing%20the%0Agreatest%20overall%20health%20outcomes%2C%20especially%20in%20underserved%20communities%20where%0Aresources%20are%20limited.%20Such%20resource%20allocation%20problems%20are%20often%20modeled%20as%0Arestless%20multi-armed%20bandits%20%28RMABs%29%20with%20unknown%20underlying%20transition%0Adynamics%2C%20hence%20requiring%20online%20reinforcement%20learning%20%28RL%29.%20We%20present%0ABayesian%20Learning%20for%20Contextual%20RMABs%20%28BCoR%29%2C%20an%20online%20RL%20approach%20for%20RMABs%0Athat%20novelly%20combines%20techniques%20in%20Bayesian%20modeling%20with%20Thompson%20sampling%20to%0Aflexibly%20model%20the%20complex%20RMAB%20settings%20present%20in%20public%20health%20program%0Aadherence%20problems%2C%20namely%20context%20and%20non-stationarity.%20BCoR%27s%20key%20strength%20is%0Athe%20ability%20to%20leverage%20shared%20information%20within%20and%20between%20arms%20to%20learn%20the%0Aunknown%20RMAB%20transition%20dynamics%20quickly%20in%20intervention-scarce%20settings%20with%0Arelatively%20short%20time%20horizons%2C%20which%20is%20common%20in%20public%20health%20applications.%0AEmpirically%2C%20BCoR%20achieves%20substantially%20higher%20finite-sample%20performance%20over%0Aa%20range%20of%20experimental%20settings%2C%20including%20a%20setting%20using%20real-world%0Aadherence%20data%20that%20was%20developed%20in%20collaboration%20with%20ARMMAN%2C%20an%20NGO%20in%20India%0Awhich%20runs%20a%20large-scale%20maternal%20mHealth%20program%2C%20showcasing%20BCoR%20practical%0Autility%20and%20potential%20for%20real-world%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.04933v3&entry.124074799=Read"},
{"title": "Rough kernel hedging", "author": "Nicola Muca Cirone and Cristopher Salvi", "abstract": "  Building on the functional-analytic framework of operator-valued kernels and\nun-truncated signature kernels, we propose a scalable, provably convergent\nsignature-based algorithm for a broad class of high-dimensional, path-dependent\nhedging problems. We make minimal assumptions about market dynamics by\nmodelling them as general geometric rough paths, yielding a fully model-free\napproach. Furthermore, through a representer theorem, we provide theoretical\nguarantees on the existence and uniqueness of a global minimum for the\nresulting optimization problem and derive an analytic solution under highly\ngeneral loss functions. Similar to the popular deep hedging approach, but in a\nmore rigorous fashion, our method can also incorporate additional features via\nthe underlying operator-valued kernel, such as trading signals, news analytics,\nand past hedging decisions, closely aligning with true machine-learning\npractice.\n", "link": "http://arxiv.org/abs/2501.09683v2", "date": "2025-02-05", "relevancy": 1.7081, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.457}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4227}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4194}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rough%20kernel%20hedging&body=Title%3A%20Rough%20kernel%20hedging%0AAuthor%3A%20Nicola%20Muca%20Cirone%20and%20Cristopher%20Salvi%0AAbstract%3A%20%20%20Building%20on%20the%20functional-analytic%20framework%20of%20operator-valued%20kernels%20and%0Aun-truncated%20signature%20kernels%2C%20we%20propose%20a%20scalable%2C%20provably%20convergent%0Asignature-based%20algorithm%20for%20a%20broad%20class%20of%20high-dimensional%2C%20path-dependent%0Ahedging%20problems.%20We%20make%20minimal%20assumptions%20about%20market%20dynamics%20by%0Amodelling%20them%20as%20general%20geometric%20rough%20paths%2C%20yielding%20a%20fully%20model-free%0Aapproach.%20Furthermore%2C%20through%20a%20representer%20theorem%2C%20we%20provide%20theoretical%0Aguarantees%20on%20the%20existence%20and%20uniqueness%20of%20a%20global%20minimum%20for%20the%0Aresulting%20optimization%20problem%20and%20derive%20an%20analytic%20solution%20under%20highly%0Ageneral%20loss%20functions.%20Similar%20to%20the%20popular%20deep%20hedging%20approach%2C%20but%20in%20a%0Amore%20rigorous%20fashion%2C%20our%20method%20can%20also%20incorporate%20additional%20features%20via%0Athe%20underlying%20operator-valued%20kernel%2C%20such%20as%20trading%20signals%2C%20news%20analytics%2C%0Aand%20past%20hedging%20decisions%2C%20closely%20aligning%20with%20true%20machine-learning%0Apractice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09683v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRough%2520kernel%2520hedging%26entry.906535625%3DNicola%2520Muca%2520Cirone%2520and%2520Cristopher%2520Salvi%26entry.1292438233%3D%2520%2520Building%2520on%2520the%2520functional-analytic%2520framework%2520of%2520operator-valued%2520kernels%2520and%250Aun-truncated%2520signature%2520kernels%252C%2520we%2520propose%2520a%2520scalable%252C%2520provably%2520convergent%250Asignature-based%2520algorithm%2520for%2520a%2520broad%2520class%2520of%2520high-dimensional%252C%2520path-dependent%250Ahedging%2520problems.%2520We%2520make%2520minimal%2520assumptions%2520about%2520market%2520dynamics%2520by%250Amodelling%2520them%2520as%2520general%2520geometric%2520rough%2520paths%252C%2520yielding%2520a%2520fully%2520model-free%250Aapproach.%2520Furthermore%252C%2520through%2520a%2520representer%2520theorem%252C%2520we%2520provide%2520theoretical%250Aguarantees%2520on%2520the%2520existence%2520and%2520uniqueness%2520of%2520a%2520global%2520minimum%2520for%2520the%250Aresulting%2520optimization%2520problem%2520and%2520derive%2520an%2520analytic%2520solution%2520under%2520highly%250Ageneral%2520loss%2520functions.%2520Similar%2520to%2520the%2520popular%2520deep%2520hedging%2520approach%252C%2520but%2520in%2520a%250Amore%2520rigorous%2520fashion%252C%2520our%2520method%2520can%2520also%2520incorporate%2520additional%2520features%2520via%250Athe%2520underlying%2520operator-valued%2520kernel%252C%2520such%2520as%2520trading%2520signals%252C%2520news%2520analytics%252C%250Aand%2520past%2520hedging%2520decisions%252C%2520closely%2520aligning%2520with%2520true%2520machine-learning%250Apractice.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09683v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rough%20kernel%20hedging&entry.906535625=Nicola%20Muca%20Cirone%20and%20Cristopher%20Salvi&entry.1292438233=%20%20Building%20on%20the%20functional-analytic%20framework%20of%20operator-valued%20kernels%20and%0Aun-truncated%20signature%20kernels%2C%20we%20propose%20a%20scalable%2C%20provably%20convergent%0Asignature-based%20algorithm%20for%20a%20broad%20class%20of%20high-dimensional%2C%20path-dependent%0Ahedging%20problems.%20We%20make%20minimal%20assumptions%20about%20market%20dynamics%20by%0Amodelling%20them%20as%20general%20geometric%20rough%20paths%2C%20yielding%20a%20fully%20model-free%0Aapproach.%20Furthermore%2C%20through%20a%20representer%20theorem%2C%20we%20provide%20theoretical%0Aguarantees%20on%20the%20existence%20and%20uniqueness%20of%20a%20global%20minimum%20for%20the%0Aresulting%20optimization%20problem%20and%20derive%20an%20analytic%20solution%20under%20highly%0Ageneral%20loss%20functions.%20Similar%20to%20the%20popular%20deep%20hedging%20approach%2C%20but%20in%20a%0Amore%20rigorous%20fashion%2C%20our%20method%20can%20also%20incorporate%20additional%20features%20via%0Athe%20underlying%20operator-valued%20kernel%2C%20such%20as%20trading%20signals%2C%20news%20analytics%2C%0Aand%20past%20hedging%20decisions%2C%20closely%20aligning%20with%20true%20machine-learning%0Apractice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09683v2&entry.124074799=Read"},
{"title": "Asynchronous Stochastic Gradient Descent with Decoupled Backpropagation\n  and Layer-Wise Updates", "author": "Cabrel Teguemne Fokam and Khaleelulla Khan Nazeer and Lukas K\u00f6nig and David Kappel and Anand Subramoney", "abstract": "  The increasing size of deep learning models has made distributed training\nacross multiple devices essential. However, current methods such as distributed\ndata-parallel training suffer from large communication and synchronization\noverheads when training across devices, leading to longer training times as a\nresult of suboptimal hardware utilization. Asynchronous stochastic gradient\ndescent (ASGD) methods can improve training speed, but are sensitive to delays\ndue to both communication and differences throughput. Moreover, the\nbackpropagation algorithm used within ASGD workers is bottlenecked by the\ninterlocking between its forward and backward passes. Current methods also do\nnot take advantage of the large differences in the computation required for the\nforward and backward passes. Therefore, we propose an extension to ASGD called\nPartial Decoupled ASGD (PD-ASGD) that addresses these issues. PD-ASGD uses\nseparate threads for the forward and backward passes, decoupling the updates\nand allowing for a higher ratio of forward to backward threads than the usual\n1:1 ratio, leading to higher throughput. PD-ASGD also performs layer-wise\n(partial) model updates concurrently across multiple threads. This reduces\nparameter staleness and consequently improves robustness to delays. Our\napproach yields close to state-of-the-art results while running up to\n$5.95\\times$ faster than synchronous data parallelism in the presence of\ndelays, and up to $2.14\\times$ times faster than comparable ASGD algorithms by\nachieving higher model flops utilization. We mathematically describe the\ngradient bias introduced by our method, establish an upper bound, and prove\nconvergence.\n", "link": "http://arxiv.org/abs/2410.05985v2", "date": "2025-02-05", "relevancy": 1.6234, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5657}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.538}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Asynchronous%20Stochastic%20Gradient%20Descent%20with%20Decoupled%20Backpropagation%0A%20%20and%20Layer-Wise%20Updates&body=Title%3A%20Asynchronous%20Stochastic%20Gradient%20Descent%20with%20Decoupled%20Backpropagation%0A%20%20and%20Layer-Wise%20Updates%0AAuthor%3A%20Cabrel%20Teguemne%20Fokam%20and%20Khaleelulla%20Khan%20Nazeer%20and%20Lukas%20K%C3%B6nig%20and%20David%20Kappel%20and%20Anand%20Subramoney%0AAbstract%3A%20%20%20The%20increasing%20size%20of%20deep%20learning%20models%20has%20made%20distributed%20training%0Aacross%20multiple%20devices%20essential.%20However%2C%20current%20methods%20such%20as%20distributed%0Adata-parallel%20training%20suffer%20from%20large%20communication%20and%20synchronization%0Aoverheads%20when%20training%20across%20devices%2C%20leading%20to%20longer%20training%20times%20as%20a%0Aresult%20of%20suboptimal%20hardware%20utilization.%20Asynchronous%20stochastic%20gradient%0Adescent%20%28ASGD%29%20methods%20can%20improve%20training%20speed%2C%20but%20are%20sensitive%20to%20delays%0Adue%20to%20both%20communication%20and%20differences%20throughput.%20Moreover%2C%20the%0Abackpropagation%20algorithm%20used%20within%20ASGD%20workers%20is%20bottlenecked%20by%20the%0Ainterlocking%20between%20its%20forward%20and%20backward%20passes.%20Current%20methods%20also%20do%0Anot%20take%20advantage%20of%20the%20large%20differences%20in%20the%20computation%20required%20for%20the%0Aforward%20and%20backward%20passes.%20Therefore%2C%20we%20propose%20an%20extension%20to%20ASGD%20called%0APartial%20Decoupled%20ASGD%20%28PD-ASGD%29%20that%20addresses%20these%20issues.%20PD-ASGD%20uses%0Aseparate%20threads%20for%20the%20forward%20and%20backward%20passes%2C%20decoupling%20the%20updates%0Aand%20allowing%20for%20a%20higher%20ratio%20of%20forward%20to%20backward%20threads%20than%20the%20usual%0A1%3A1%20ratio%2C%20leading%20to%20higher%20throughput.%20PD-ASGD%20also%20performs%20layer-wise%0A%28partial%29%20model%20updates%20concurrently%20across%20multiple%20threads.%20This%20reduces%0Aparameter%20staleness%20and%20consequently%20improves%20robustness%20to%20delays.%20Our%0Aapproach%20yields%20close%20to%20state-of-the-art%20results%20while%20running%20up%20to%0A%245.95%5Ctimes%24%20faster%20than%20synchronous%20data%20parallelism%20in%20the%20presence%20of%0Adelays%2C%20and%20up%20to%20%242.14%5Ctimes%24%20times%20faster%20than%20comparable%20ASGD%20algorithms%20by%0Aachieving%20higher%20model%20flops%20utilization.%20We%20mathematically%20describe%20the%0Agradient%20bias%20introduced%20by%20our%20method%2C%20establish%20an%20upper%20bound%2C%20and%20prove%0Aconvergence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05985v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAsynchronous%2520Stochastic%2520Gradient%2520Descent%2520with%2520Decoupled%2520Backpropagation%250A%2520%2520and%2520Layer-Wise%2520Updates%26entry.906535625%3DCabrel%2520Teguemne%2520Fokam%2520and%2520Khaleelulla%2520Khan%2520Nazeer%2520and%2520Lukas%2520K%25C3%25B6nig%2520and%2520David%2520Kappel%2520and%2520Anand%2520Subramoney%26entry.1292438233%3D%2520%2520The%2520increasing%2520size%2520of%2520deep%2520learning%2520models%2520has%2520made%2520distributed%2520training%250Aacross%2520multiple%2520devices%2520essential.%2520However%252C%2520current%2520methods%2520such%2520as%2520distributed%250Adata-parallel%2520training%2520suffer%2520from%2520large%2520communication%2520and%2520synchronization%250Aoverheads%2520when%2520training%2520across%2520devices%252C%2520leading%2520to%2520longer%2520training%2520times%2520as%2520a%250Aresult%2520of%2520suboptimal%2520hardware%2520utilization.%2520Asynchronous%2520stochastic%2520gradient%250Adescent%2520%2528ASGD%2529%2520methods%2520can%2520improve%2520training%2520speed%252C%2520but%2520are%2520sensitive%2520to%2520delays%250Adue%2520to%2520both%2520communication%2520and%2520differences%2520throughput.%2520Moreover%252C%2520the%250Abackpropagation%2520algorithm%2520used%2520within%2520ASGD%2520workers%2520is%2520bottlenecked%2520by%2520the%250Ainterlocking%2520between%2520its%2520forward%2520and%2520backward%2520passes.%2520Current%2520methods%2520also%2520do%250Anot%2520take%2520advantage%2520of%2520the%2520large%2520differences%2520in%2520the%2520computation%2520required%2520for%2520the%250Aforward%2520and%2520backward%2520passes.%2520Therefore%252C%2520we%2520propose%2520an%2520extension%2520to%2520ASGD%2520called%250APartial%2520Decoupled%2520ASGD%2520%2528PD-ASGD%2529%2520that%2520addresses%2520these%2520issues.%2520PD-ASGD%2520uses%250Aseparate%2520threads%2520for%2520the%2520forward%2520and%2520backward%2520passes%252C%2520decoupling%2520the%2520updates%250Aand%2520allowing%2520for%2520a%2520higher%2520ratio%2520of%2520forward%2520to%2520backward%2520threads%2520than%2520the%2520usual%250A1%253A1%2520ratio%252C%2520leading%2520to%2520higher%2520throughput.%2520PD-ASGD%2520also%2520performs%2520layer-wise%250A%2528partial%2529%2520model%2520updates%2520concurrently%2520across%2520multiple%2520threads.%2520This%2520reduces%250Aparameter%2520staleness%2520and%2520consequently%2520improves%2520robustness%2520to%2520delays.%2520Our%250Aapproach%2520yields%2520close%2520to%2520state-of-the-art%2520results%2520while%2520running%2520up%2520to%250A%25245.95%255Ctimes%2524%2520faster%2520than%2520synchronous%2520data%2520parallelism%2520in%2520the%2520presence%2520of%250Adelays%252C%2520and%2520up%2520to%2520%25242.14%255Ctimes%2524%2520times%2520faster%2520than%2520comparable%2520ASGD%2520algorithms%2520by%250Aachieving%2520higher%2520model%2520flops%2520utilization.%2520We%2520mathematically%2520describe%2520the%250Agradient%2520bias%2520introduced%2520by%2520our%2520method%252C%2520establish%2520an%2520upper%2520bound%252C%2520and%2520prove%250Aconvergence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05985v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Asynchronous%20Stochastic%20Gradient%20Descent%20with%20Decoupled%20Backpropagation%0A%20%20and%20Layer-Wise%20Updates&entry.906535625=Cabrel%20Teguemne%20Fokam%20and%20Khaleelulla%20Khan%20Nazeer%20and%20Lukas%20K%C3%B6nig%20and%20David%20Kappel%20and%20Anand%20Subramoney&entry.1292438233=%20%20The%20increasing%20size%20of%20deep%20learning%20models%20has%20made%20distributed%20training%0Aacross%20multiple%20devices%20essential.%20However%2C%20current%20methods%20such%20as%20distributed%0Adata-parallel%20training%20suffer%20from%20large%20communication%20and%20synchronization%0Aoverheads%20when%20training%20across%20devices%2C%20leading%20to%20longer%20training%20times%20as%20a%0Aresult%20of%20suboptimal%20hardware%20utilization.%20Asynchronous%20stochastic%20gradient%0Adescent%20%28ASGD%29%20methods%20can%20improve%20training%20speed%2C%20but%20are%20sensitive%20to%20delays%0Adue%20to%20both%20communication%20and%20differences%20throughput.%20Moreover%2C%20the%0Abackpropagation%20algorithm%20used%20within%20ASGD%20workers%20is%20bottlenecked%20by%20the%0Ainterlocking%20between%20its%20forward%20and%20backward%20passes.%20Current%20methods%20also%20do%0Anot%20take%20advantage%20of%20the%20large%20differences%20in%20the%20computation%20required%20for%20the%0Aforward%20and%20backward%20passes.%20Therefore%2C%20we%20propose%20an%20extension%20to%20ASGD%20called%0APartial%20Decoupled%20ASGD%20%28PD-ASGD%29%20that%20addresses%20these%20issues.%20PD-ASGD%20uses%0Aseparate%20threads%20for%20the%20forward%20and%20backward%20passes%2C%20decoupling%20the%20updates%0Aand%20allowing%20for%20a%20higher%20ratio%20of%20forward%20to%20backward%20threads%20than%20the%20usual%0A1%3A1%20ratio%2C%20leading%20to%20higher%20throughput.%20PD-ASGD%20also%20performs%20layer-wise%0A%28partial%29%20model%20updates%20concurrently%20across%20multiple%20threads.%20This%20reduces%0Aparameter%20staleness%20and%20consequently%20improves%20robustness%20to%20delays.%20Our%0Aapproach%20yields%20close%20to%20state-of-the-art%20results%20while%20running%20up%20to%0A%245.95%5Ctimes%24%20faster%20than%20synchronous%20data%20parallelism%20in%20the%20presence%20of%0Adelays%2C%20and%20up%20to%20%242.14%5Ctimes%24%20times%20faster%20than%20comparable%20ASGD%20algorithms%20by%0Aachieving%20higher%20model%20flops%20utilization.%20We%20mathematically%20describe%20the%0Agradient%20bias%20introduced%20by%20our%20method%2C%20establish%20an%20upper%20bound%2C%20and%20prove%0Aconvergence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05985v2&entry.124074799=Read"},
{"title": "Is In-Context Universality Enough? MLPs are Also Universal In-Context", "author": "Anastasis Kratsios and Takashi Furuya", "abstract": "  The success of transformers is often linked to their ability to perform\nin-context learning. Recent work shows that transformers are universal in\ncontext, capable of approximating any real-valued continuous function of a\ncontext (a probability measure over $\\mathcal{X}\\subseteq \\mathbb{R}^d$) and a\nquery $x\\in \\mathcal{X}$. This raises the question: Does in-context\nuniversality explain their advantage over classical models? We answer this in\nthe negative by proving that MLPs with trainable activation functions are also\nuniversal in-context. This suggests the transformer's success is likely due to\nother factors like inductive bias or training stability.\n", "link": "http://arxiv.org/abs/2502.03327v1", "date": "2025-02-05", "relevancy": 1.818, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4559}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4559}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20In-Context%20Universality%20Enough%3F%20MLPs%20are%20Also%20Universal%20In-Context&body=Title%3A%20Is%20In-Context%20Universality%20Enough%3F%20MLPs%20are%20Also%20Universal%20In-Context%0AAuthor%3A%20Anastasis%20Kratsios%20and%20Takashi%20Furuya%0AAbstract%3A%20%20%20The%20success%20of%20transformers%20is%20often%20linked%20to%20their%20ability%20to%20perform%0Ain-context%20learning.%20Recent%20work%20shows%20that%20transformers%20are%20universal%20in%0Acontext%2C%20capable%20of%20approximating%20any%20real-valued%20continuous%20function%20of%20a%0Acontext%20%28a%20probability%20measure%20over%20%24%5Cmathcal%7BX%7D%5Csubseteq%20%5Cmathbb%7BR%7D%5Ed%24%29%20and%20a%0Aquery%20%24x%5Cin%20%5Cmathcal%7BX%7D%24.%20This%20raises%20the%20question%3A%20Does%20in-context%0Auniversality%20explain%20their%20advantage%20over%20classical%20models%3F%20We%20answer%20this%20in%0Athe%20negative%20by%20proving%20that%20MLPs%20with%20trainable%20activation%20functions%20are%20also%0Auniversal%20in-context.%20This%20suggests%20the%20transformer%27s%20success%20is%20likely%20due%20to%0Aother%20factors%20like%20inductive%20bias%20or%20training%20stability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03327v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520In-Context%2520Universality%2520Enough%253F%2520MLPs%2520are%2520Also%2520Universal%2520In-Context%26entry.906535625%3DAnastasis%2520Kratsios%2520and%2520Takashi%2520Furuya%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520transformers%2520is%2520often%2520linked%2520to%2520their%2520ability%2520to%2520perform%250Ain-context%2520learning.%2520Recent%2520work%2520shows%2520that%2520transformers%2520are%2520universal%2520in%250Acontext%252C%2520capable%2520of%2520approximating%2520any%2520real-valued%2520continuous%2520function%2520of%2520a%250Acontext%2520%2528a%2520probability%2520measure%2520over%2520%2524%255Cmathcal%257BX%257D%255Csubseteq%2520%255Cmathbb%257BR%257D%255Ed%2524%2529%2520and%2520a%250Aquery%2520%2524x%255Cin%2520%255Cmathcal%257BX%257D%2524.%2520This%2520raises%2520the%2520question%253A%2520Does%2520in-context%250Auniversality%2520explain%2520their%2520advantage%2520over%2520classical%2520models%253F%2520We%2520answer%2520this%2520in%250Athe%2520negative%2520by%2520proving%2520that%2520MLPs%2520with%2520trainable%2520activation%2520functions%2520are%2520also%250Auniversal%2520in-context.%2520This%2520suggests%2520the%2520transformer%2527s%2520success%2520is%2520likely%2520due%2520to%250Aother%2520factors%2520like%2520inductive%2520bias%2520or%2520training%2520stability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03327v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20In-Context%20Universality%20Enough%3F%20MLPs%20are%20Also%20Universal%20In-Context&entry.906535625=Anastasis%20Kratsios%20and%20Takashi%20Furuya&entry.1292438233=%20%20The%20success%20of%20transformers%20is%20often%20linked%20to%20their%20ability%20to%20perform%0Ain-context%20learning.%20Recent%20work%20shows%20that%20transformers%20are%20universal%20in%0Acontext%2C%20capable%20of%20approximating%20any%20real-valued%20continuous%20function%20of%20a%0Acontext%20%28a%20probability%20measure%20over%20%24%5Cmathcal%7BX%7D%5Csubseteq%20%5Cmathbb%7BR%7D%5Ed%24%29%20and%20a%0Aquery%20%24x%5Cin%20%5Cmathcal%7BX%7D%24.%20This%20raises%20the%20question%3A%20Does%20in-context%0Auniversality%20explain%20their%20advantage%20over%20classical%20models%3F%20We%20answer%20this%20in%0Athe%20negative%20by%20proving%20that%20MLPs%20with%20trainable%20activation%20functions%20are%20also%0Auniversal%20in-context.%20This%20suggests%20the%20transformer%27s%20success%20is%20likely%20due%20to%0Aother%20factors%20like%20inductive%20bias%20or%20training%20stability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03327v1&entry.124074799=Read"},
{"title": "STEM: Spatial-Temporal Mapping Tool For Spiking Neural Networks", "author": "Sherif Eissa and Sander Stuijk and Floran De Putter and Andrea Nardi-Dei and Federico Corradi and Henk Corporaal", "abstract": "  Spiking Neural Networks (SNNs) are promising bio-inspired third-generation\nneural networks. Recent research has trained deep SNN models with accuracy on\npar with Artificial Neural Networks (ANNs). Although the event-driven and\nsparse nature of SNNs show potential for more energy efficient computation than\nANNs, SNN neurons have internal states which evolve over time. Keeping track of\nSNN states can significantly increase data movement and storage requirements,\npotentially losing its advantages with respect to ANNs. This paper investigates\nthe energy effects of having neuron states, and how it is influenced by the\nchosen mapping to realistic hardware architectures with advanced memory\nhierarchies. Therefore, we develop STEMS, a mapping design space exploration\ntool for SNNs. STEMS models SNN's stateful behavior and explores intra-layer\nand inter-layer mapping optimizations to minimize data movement, considering\nboth spatial and temporal SNN dimensions. Using STEMS, we show up to 12x\nreduction in off-chip data movement and 5x reduction in energy (on top of\nintra-layer optimizations), on two event-based vision SNN benchmarks. Finally,\nneuron states may not be needed for all SNN layers. By optimizing neuron states\nfor one of our benchmarks, we show 20x reduction in neuron states and 1.4x\nbetter performance without accuracy loss.\n", "link": "http://arxiv.org/abs/2502.03287v1", "date": "2025-02-05", "relevancy": 1.9399, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4991}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4762}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STEM%3A%20Spatial-Temporal%20Mapping%20Tool%20For%20Spiking%20Neural%20Networks&body=Title%3A%20STEM%3A%20Spatial-Temporal%20Mapping%20Tool%20For%20Spiking%20Neural%20Networks%0AAuthor%3A%20Sherif%20Eissa%20and%20Sander%20Stuijk%20and%20Floran%20De%20Putter%20and%20Andrea%20Nardi-Dei%20and%20Federico%20Corradi%20and%20Henk%20Corporaal%0AAbstract%3A%20%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20are%20promising%20bio-inspired%20third-generation%0Aneural%20networks.%20Recent%20research%20has%20trained%20deep%20SNN%20models%20with%20accuracy%20on%0Apar%20with%20Artificial%20Neural%20Networks%20%28ANNs%29.%20Although%20the%20event-driven%20and%0Asparse%20nature%20of%20SNNs%20show%20potential%20for%20more%20energy%20efficient%20computation%20than%0AANNs%2C%20SNN%20neurons%20have%20internal%20states%20which%20evolve%20over%20time.%20Keeping%20track%20of%0ASNN%20states%20can%20significantly%20increase%20data%20movement%20and%20storage%20requirements%2C%0Apotentially%20losing%20its%20advantages%20with%20respect%20to%20ANNs.%20This%20paper%20investigates%0Athe%20energy%20effects%20of%20having%20neuron%20states%2C%20and%20how%20it%20is%20influenced%20by%20the%0Achosen%20mapping%20to%20realistic%20hardware%20architectures%20with%20advanced%20memory%0Ahierarchies.%20Therefore%2C%20we%20develop%20STEMS%2C%20a%20mapping%20design%20space%20exploration%0Atool%20for%20SNNs.%20STEMS%20models%20SNN%27s%20stateful%20behavior%20and%20explores%20intra-layer%0Aand%20inter-layer%20mapping%20optimizations%20to%20minimize%20data%20movement%2C%20considering%0Aboth%20spatial%20and%20temporal%20SNN%20dimensions.%20Using%20STEMS%2C%20we%20show%20up%20to%2012x%0Areduction%20in%20off-chip%20data%20movement%20and%205x%20reduction%20in%20energy%20%28on%20top%20of%0Aintra-layer%20optimizations%29%2C%20on%20two%20event-based%20vision%20SNN%20benchmarks.%20Finally%2C%0Aneuron%20states%20may%20not%20be%20needed%20for%20all%20SNN%20layers.%20By%20optimizing%20neuron%20states%0Afor%20one%20of%20our%20benchmarks%2C%20we%20show%2020x%20reduction%20in%20neuron%20states%20and%201.4x%0Abetter%20performance%20without%20accuracy%20loss.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03287v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTEM%253A%2520Spatial-Temporal%2520Mapping%2520Tool%2520For%2520Spiking%2520Neural%2520Networks%26entry.906535625%3DSherif%2520Eissa%2520and%2520Sander%2520Stuijk%2520and%2520Floran%2520De%2520Putter%2520and%2520Andrea%2520Nardi-Dei%2520and%2520Federico%2520Corradi%2520and%2520Henk%2520Corporaal%26entry.1292438233%3D%2520%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520are%2520promising%2520bio-inspired%2520third-generation%250Aneural%2520networks.%2520Recent%2520research%2520has%2520trained%2520deep%2520SNN%2520models%2520with%2520accuracy%2520on%250Apar%2520with%2520Artificial%2520Neural%2520Networks%2520%2528ANNs%2529.%2520Although%2520the%2520event-driven%2520and%250Asparse%2520nature%2520of%2520SNNs%2520show%2520potential%2520for%2520more%2520energy%2520efficient%2520computation%2520than%250AANNs%252C%2520SNN%2520neurons%2520have%2520internal%2520states%2520which%2520evolve%2520over%2520time.%2520Keeping%2520track%2520of%250ASNN%2520states%2520can%2520significantly%2520increase%2520data%2520movement%2520and%2520storage%2520requirements%252C%250Apotentially%2520losing%2520its%2520advantages%2520with%2520respect%2520to%2520ANNs.%2520This%2520paper%2520investigates%250Athe%2520energy%2520effects%2520of%2520having%2520neuron%2520states%252C%2520and%2520how%2520it%2520is%2520influenced%2520by%2520the%250Achosen%2520mapping%2520to%2520realistic%2520hardware%2520architectures%2520with%2520advanced%2520memory%250Ahierarchies.%2520Therefore%252C%2520we%2520develop%2520STEMS%252C%2520a%2520mapping%2520design%2520space%2520exploration%250Atool%2520for%2520SNNs.%2520STEMS%2520models%2520SNN%2527s%2520stateful%2520behavior%2520and%2520explores%2520intra-layer%250Aand%2520inter-layer%2520mapping%2520optimizations%2520to%2520minimize%2520data%2520movement%252C%2520considering%250Aboth%2520spatial%2520and%2520temporal%2520SNN%2520dimensions.%2520Using%2520STEMS%252C%2520we%2520show%2520up%2520to%252012x%250Areduction%2520in%2520off-chip%2520data%2520movement%2520and%25205x%2520reduction%2520in%2520energy%2520%2528on%2520top%2520of%250Aintra-layer%2520optimizations%2529%252C%2520on%2520two%2520event-based%2520vision%2520SNN%2520benchmarks.%2520Finally%252C%250Aneuron%2520states%2520may%2520not%2520be%2520needed%2520for%2520all%2520SNN%2520layers.%2520By%2520optimizing%2520neuron%2520states%250Afor%2520one%2520of%2520our%2520benchmarks%252C%2520we%2520show%252020x%2520reduction%2520in%2520neuron%2520states%2520and%25201.4x%250Abetter%2520performance%2520without%2520accuracy%2520loss.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03287v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STEM%3A%20Spatial-Temporal%20Mapping%20Tool%20For%20Spiking%20Neural%20Networks&entry.906535625=Sherif%20Eissa%20and%20Sander%20Stuijk%20and%20Floran%20De%20Putter%20and%20Andrea%20Nardi-Dei%20and%20Federico%20Corradi%20and%20Henk%20Corporaal&entry.1292438233=%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20are%20promising%20bio-inspired%20third-generation%0Aneural%20networks.%20Recent%20research%20has%20trained%20deep%20SNN%20models%20with%20accuracy%20on%0Apar%20with%20Artificial%20Neural%20Networks%20%28ANNs%29.%20Although%20the%20event-driven%20and%0Asparse%20nature%20of%20SNNs%20show%20potential%20for%20more%20energy%20efficient%20computation%20than%0AANNs%2C%20SNN%20neurons%20have%20internal%20states%20which%20evolve%20over%20time.%20Keeping%20track%20of%0ASNN%20states%20can%20significantly%20increase%20data%20movement%20and%20storage%20requirements%2C%0Apotentially%20losing%20its%20advantages%20with%20respect%20to%20ANNs.%20This%20paper%20investigates%0Athe%20energy%20effects%20of%20having%20neuron%20states%2C%20and%20how%20it%20is%20influenced%20by%20the%0Achosen%20mapping%20to%20realistic%20hardware%20architectures%20with%20advanced%20memory%0Ahierarchies.%20Therefore%2C%20we%20develop%20STEMS%2C%20a%20mapping%20design%20space%20exploration%0Atool%20for%20SNNs.%20STEMS%20models%20SNN%27s%20stateful%20behavior%20and%20explores%20intra-layer%0Aand%20inter-layer%20mapping%20optimizations%20to%20minimize%20data%20movement%2C%20considering%0Aboth%20spatial%20and%20temporal%20SNN%20dimensions.%20Using%20STEMS%2C%20we%20show%20up%20to%2012x%0Areduction%20in%20off-chip%20data%20movement%20and%205x%20reduction%20in%20energy%20%28on%20top%20of%0Aintra-layer%20optimizations%29%2C%20on%20two%20event-based%20vision%20SNN%20benchmarks.%20Finally%2C%0Aneuron%20states%20may%20not%20be%20needed%20for%20all%20SNN%20layers.%20By%20optimizing%20neuron%20states%0Afor%20one%20of%20our%20benchmarks%2C%20we%20show%2020x%20reduction%20in%20neuron%20states%20and%201.4x%0Abetter%20performance%20without%20accuracy%20loss.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03287v1&entry.124074799=Read"},
{"title": "Understanding and Enhancing the Transferability of Jailbreaking Attacks", "author": "Runqi Lin and Bo Han and Fengwang Li and Tongling Liu", "abstract": "  Jailbreaking attacks can effectively manipulate open-source large language\nmodels (LLMs) to produce harmful responses. However, these attacks exhibit\nlimited transferability, failing to disrupt proprietary LLMs consistently. To\nreliably identify vulnerabilities in proprietary LLMs, this work investigates\nthe transferability of jailbreaking attacks by analysing their impact on the\nmodel's intent perception. By incorporating adversarial sequences, these\nattacks can redirect the source LLM's focus away from malicious-intent tokens\nin the original input, thereby obstructing the model's intent recognition and\neliciting harmful responses. Nevertheless, these adversarial sequences fail to\nmislead the target LLM's intent perception, allowing the target LLM to refocus\non malicious-intent tokens and abstain from responding. Our analysis further\nreveals the inherent distributional dependency within the generated adversarial\nsequences, whose effectiveness stems from overfitting the source LLM's\nparameters, resulting in limited transferability to target LLMs. To this end,\nwe propose the Perceived-importance Flatten (PiF) method, which uniformly\ndisperses the model's focus across neutral-intent tokens in the original input,\nthus obscuring malicious-intent tokens without relying on overfitted\nadversarial sequences. Extensive experiments demonstrate that PiF provides an\neffective and efficient red-teaming evaluation for proprietary LLMs.\n", "link": "http://arxiv.org/abs/2502.03052v1", "date": "2025-02-05", "relevancy": 1.7115, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4284}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4284}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20and%20Enhancing%20the%20Transferability%20of%20Jailbreaking%20Attacks&body=Title%3A%20Understanding%20and%20Enhancing%20the%20Transferability%20of%20Jailbreaking%20Attacks%0AAuthor%3A%20Runqi%20Lin%20and%20Bo%20Han%20and%20Fengwang%20Li%20and%20Tongling%20Liu%0AAbstract%3A%20%20%20Jailbreaking%20attacks%20can%20effectively%20manipulate%20open-source%20large%20language%0Amodels%20%28LLMs%29%20to%20produce%20harmful%20responses.%20However%2C%20these%20attacks%20exhibit%0Alimited%20transferability%2C%20failing%20to%20disrupt%20proprietary%20LLMs%20consistently.%20To%0Areliably%20identify%20vulnerabilities%20in%20proprietary%20LLMs%2C%20this%20work%20investigates%0Athe%20transferability%20of%20jailbreaking%20attacks%20by%20analysing%20their%20impact%20on%20the%0Amodel%27s%20intent%20perception.%20By%20incorporating%20adversarial%20sequences%2C%20these%0Aattacks%20can%20redirect%20the%20source%20LLM%27s%20focus%20away%20from%20malicious-intent%20tokens%0Ain%20the%20original%20input%2C%20thereby%20obstructing%20the%20model%27s%20intent%20recognition%20and%0Aeliciting%20harmful%20responses.%20Nevertheless%2C%20these%20adversarial%20sequences%20fail%20to%0Amislead%20the%20target%20LLM%27s%20intent%20perception%2C%20allowing%20the%20target%20LLM%20to%20refocus%0Aon%20malicious-intent%20tokens%20and%20abstain%20from%20responding.%20Our%20analysis%20further%0Areveals%20the%20inherent%20distributional%20dependency%20within%20the%20generated%20adversarial%0Asequences%2C%20whose%20effectiveness%20stems%20from%20overfitting%20the%20source%20LLM%27s%0Aparameters%2C%20resulting%20in%20limited%20transferability%20to%20target%20LLMs.%20To%20this%20end%2C%0Awe%20propose%20the%20Perceived-importance%20Flatten%20%28PiF%29%20method%2C%20which%20uniformly%0Adisperses%20the%20model%27s%20focus%20across%20neutral-intent%20tokens%20in%20the%20original%20input%2C%0Athus%20obscuring%20malicious-intent%20tokens%20without%20relying%20on%20overfitted%0Aadversarial%20sequences.%20Extensive%20experiments%20demonstrate%20that%20PiF%20provides%20an%0Aeffective%20and%20efficient%20red-teaming%20evaluation%20for%20proprietary%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.03052v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520and%2520Enhancing%2520the%2520Transferability%2520of%2520Jailbreaking%2520Attacks%26entry.906535625%3DRunqi%2520Lin%2520and%2520Bo%2520Han%2520and%2520Fengwang%2520Li%2520and%2520Tongling%2520Liu%26entry.1292438233%3D%2520%2520Jailbreaking%2520attacks%2520can%2520effectively%2520manipulate%2520open-source%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520to%2520produce%2520harmful%2520responses.%2520However%252C%2520these%2520attacks%2520exhibit%250Alimited%2520transferability%252C%2520failing%2520to%2520disrupt%2520proprietary%2520LLMs%2520consistently.%2520To%250Areliably%2520identify%2520vulnerabilities%2520in%2520proprietary%2520LLMs%252C%2520this%2520work%2520investigates%250Athe%2520transferability%2520of%2520jailbreaking%2520attacks%2520by%2520analysing%2520their%2520impact%2520on%2520the%250Amodel%2527s%2520intent%2520perception.%2520By%2520incorporating%2520adversarial%2520sequences%252C%2520these%250Aattacks%2520can%2520redirect%2520the%2520source%2520LLM%2527s%2520focus%2520away%2520from%2520malicious-intent%2520tokens%250Ain%2520the%2520original%2520input%252C%2520thereby%2520obstructing%2520the%2520model%2527s%2520intent%2520recognition%2520and%250Aeliciting%2520harmful%2520responses.%2520Nevertheless%252C%2520these%2520adversarial%2520sequences%2520fail%2520to%250Amislead%2520the%2520target%2520LLM%2527s%2520intent%2520perception%252C%2520allowing%2520the%2520target%2520LLM%2520to%2520refocus%250Aon%2520malicious-intent%2520tokens%2520and%2520abstain%2520from%2520responding.%2520Our%2520analysis%2520further%250Areveals%2520the%2520inherent%2520distributional%2520dependency%2520within%2520the%2520generated%2520adversarial%250Asequences%252C%2520whose%2520effectiveness%2520stems%2520from%2520overfitting%2520the%2520source%2520LLM%2527s%250Aparameters%252C%2520resulting%2520in%2520limited%2520transferability%2520to%2520target%2520LLMs.%2520To%2520this%2520end%252C%250Awe%2520propose%2520the%2520Perceived-importance%2520Flatten%2520%2528PiF%2529%2520method%252C%2520which%2520uniformly%250Adisperses%2520the%2520model%2527s%2520focus%2520across%2520neutral-intent%2520tokens%2520in%2520the%2520original%2520input%252C%250Athus%2520obscuring%2520malicious-intent%2520tokens%2520without%2520relying%2520on%2520overfitted%250Aadversarial%2520sequences.%2520Extensive%2520experiments%2520demonstrate%2520that%2520PiF%2520provides%2520an%250Aeffective%2520and%2520efficient%2520red-teaming%2520evaluation%2520for%2520proprietary%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.03052v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20and%20Enhancing%20the%20Transferability%20of%20Jailbreaking%20Attacks&entry.906535625=Runqi%20Lin%20and%20Bo%20Han%20and%20Fengwang%20Li%20and%20Tongling%20Liu&entry.1292438233=%20%20Jailbreaking%20attacks%20can%20effectively%20manipulate%20open-source%20large%20language%0Amodels%20%28LLMs%29%20to%20produce%20harmful%20responses.%20However%2C%20these%20attacks%20exhibit%0Alimited%20transferability%2C%20failing%20to%20disrupt%20proprietary%20LLMs%20consistently.%20To%0Areliably%20identify%20vulnerabilities%20in%20proprietary%20LLMs%2C%20this%20work%20investigates%0Athe%20transferability%20of%20jailbreaking%20attacks%20by%20analysing%20their%20impact%20on%20the%0Amodel%27s%20intent%20perception.%20By%20incorporating%20adversarial%20sequences%2C%20these%0Aattacks%20can%20redirect%20the%20source%20LLM%27s%20focus%20away%20from%20malicious-intent%20tokens%0Ain%20the%20original%20input%2C%20thereby%20obstructing%20the%20model%27s%20intent%20recognition%20and%0Aeliciting%20harmful%20responses.%20Nevertheless%2C%20these%20adversarial%20sequences%20fail%20to%0Amislead%20the%20target%20LLM%27s%20intent%20perception%2C%20allowing%20the%20target%20LLM%20to%20refocus%0Aon%20malicious-intent%20tokens%20and%20abstain%20from%20responding.%20Our%20analysis%20further%0Areveals%20the%20inherent%20distributional%20dependency%20within%20the%20generated%20adversarial%0Asequences%2C%20whose%20effectiveness%20stems%20from%20overfitting%20the%20source%20LLM%27s%0Aparameters%2C%20resulting%20in%20limited%20transferability%20to%20target%20LLMs.%20To%20this%20end%2C%0Awe%20propose%20the%20Perceived-importance%20Flatten%20%28PiF%29%20method%2C%20which%20uniformly%0Adisperses%20the%20model%27s%20focus%20across%20neutral-intent%20tokens%20in%20the%20original%20input%2C%0Athus%20obscuring%20malicious-intent%20tokens%20without%20relying%20on%20overfitted%0Aadversarial%20sequences.%20Extensive%20experiments%20demonstrate%20that%20PiF%20provides%20an%0Aeffective%20and%20efficient%20red-teaming%20evaluation%20for%20proprietary%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.03052v1&entry.124074799=Read"},
{"title": "Can Large Language Models Predict the Outcome of Judicial Decisions?", "author": "Mohamed Bayan Kmainasi and Ali Ezzat Shahroor and Amani Al-Ghraibah", "abstract": "  Large Language Models (LLMs) have shown exceptional capabilities in Natural\nLanguage Processing (NLP) across diverse domains. However, their application in\nspecialized tasks such as Legal Judgment Prediction (LJP) for low-resource\nlanguages like Arabic remains underexplored. In this work, we address this gap\nby developing an Arabic LJP dataset, collected and preprocessed from Saudi\ncommercial court judgments. We benchmark state-of-the-art open-source LLMs,\nincluding LLaMA-3.2-3B and LLaMA-3.1-8B, under varying configurations such as\nzero-shot, one-shot, and fine-tuning using QLoRA. Additionally, we used a\ncomprehensive evaluation framework combining quantitative metrics (BLEU and\nROUGE) and qualitative assessments (Coherence, legal language, clarity). Our\nresults demonstrate that fine-tuned smaller models achieve comparable\nperformance to larger models in task-specific contexts while offering\nsignificant resource efficiency. Furthermore, we investigate the effects of\nprompt engineering and fine-tuning on model outputs, providing insights into\nperformance variability and instruction sensitivity. By making the dataset,\nimplementation code, and models publicly available, we establish a robust\nfoundation for future research in Arabic legal NLP.\n", "link": "http://arxiv.org/abs/2501.09768v2", "date": "2025-02-05", "relevancy": 1.8429, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4918}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4545}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Large%20Language%20Models%20Predict%20the%20Outcome%20of%20Judicial%20Decisions%3F&body=Title%3A%20Can%20Large%20Language%20Models%20Predict%20the%20Outcome%20of%20Judicial%20Decisions%3F%0AAuthor%3A%20Mohamed%20Bayan%20Kmainasi%20and%20Ali%20Ezzat%20Shahroor%20and%20Amani%20Al-Ghraibah%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20exceptional%20capabilities%20in%20Natural%0ALanguage%20Processing%20%28NLP%29%20across%20diverse%20domains.%20However%2C%20their%20application%20in%0Aspecialized%20tasks%20such%20as%20Legal%20Judgment%20Prediction%20%28LJP%29%20for%20low-resource%0Alanguages%20like%20Arabic%20remains%20underexplored.%20In%20this%20work%2C%20we%20address%20this%20gap%0Aby%20developing%20an%20Arabic%20LJP%20dataset%2C%20collected%20and%20preprocessed%20from%20Saudi%0Acommercial%20court%20judgments.%20We%20benchmark%20state-of-the-art%20open-source%20LLMs%2C%0Aincluding%20LLaMA-3.2-3B%20and%20LLaMA-3.1-8B%2C%20under%20varying%20configurations%20such%20as%0Azero-shot%2C%20one-shot%2C%20and%20fine-tuning%20using%20QLoRA.%20Additionally%2C%20we%20used%20a%0Acomprehensive%20evaluation%20framework%20combining%20quantitative%20metrics%20%28BLEU%20and%0AROUGE%29%20and%20qualitative%20assessments%20%28Coherence%2C%20legal%20language%2C%20clarity%29.%20Our%0Aresults%20demonstrate%20that%20fine-tuned%20smaller%20models%20achieve%20comparable%0Aperformance%20to%20larger%20models%20in%20task-specific%20contexts%20while%20offering%0Asignificant%20resource%20efficiency.%20Furthermore%2C%20we%20investigate%20the%20effects%20of%0Aprompt%20engineering%20and%20fine-tuning%20on%20model%20outputs%2C%20providing%20insights%20into%0Aperformance%20variability%20and%20instruction%20sensitivity.%20By%20making%20the%20dataset%2C%0Aimplementation%20code%2C%20and%20models%20publicly%20available%2C%20we%20establish%20a%20robust%0Afoundation%20for%20future%20research%20in%20Arabic%20legal%20NLP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.09768v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Large%2520Language%2520Models%2520Predict%2520the%2520Outcome%2520of%2520Judicial%2520Decisions%253F%26entry.906535625%3DMohamed%2520Bayan%2520Kmainasi%2520and%2520Ali%2520Ezzat%2520Shahroor%2520and%2520Amani%2520Al-Ghraibah%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520exceptional%2520capabilities%2520in%2520Natural%250ALanguage%2520Processing%2520%2528NLP%2529%2520across%2520diverse%2520domains.%2520However%252C%2520their%2520application%2520in%250Aspecialized%2520tasks%2520such%2520as%2520Legal%2520Judgment%2520Prediction%2520%2528LJP%2529%2520for%2520low-resource%250Alanguages%2520like%2520Arabic%2520remains%2520underexplored.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520gap%250Aby%2520developing%2520an%2520Arabic%2520LJP%2520dataset%252C%2520collected%2520and%2520preprocessed%2520from%2520Saudi%250Acommercial%2520court%2520judgments.%2520We%2520benchmark%2520state-of-the-art%2520open-source%2520LLMs%252C%250Aincluding%2520LLaMA-3.2-3B%2520and%2520LLaMA-3.1-8B%252C%2520under%2520varying%2520configurations%2520such%2520as%250Azero-shot%252C%2520one-shot%252C%2520and%2520fine-tuning%2520using%2520QLoRA.%2520Additionally%252C%2520we%2520used%2520a%250Acomprehensive%2520evaluation%2520framework%2520combining%2520quantitative%2520metrics%2520%2528BLEU%2520and%250AROUGE%2529%2520and%2520qualitative%2520assessments%2520%2528Coherence%252C%2520legal%2520language%252C%2520clarity%2529.%2520Our%250Aresults%2520demonstrate%2520that%2520fine-tuned%2520smaller%2520models%2520achieve%2520comparable%250Aperformance%2520to%2520larger%2520models%2520in%2520task-specific%2520contexts%2520while%2520offering%250Asignificant%2520resource%2520efficiency.%2520Furthermore%252C%2520we%2520investigate%2520the%2520effects%2520of%250Aprompt%2520engineering%2520and%2520fine-tuning%2520on%2520model%2520outputs%252C%2520providing%2520insights%2520into%250Aperformance%2520variability%2520and%2520instruction%2520sensitivity.%2520By%2520making%2520the%2520dataset%252C%250Aimplementation%2520code%252C%2520and%2520models%2520publicly%2520available%252C%2520we%2520establish%2520a%2520robust%250Afoundation%2520for%2520future%2520research%2520in%2520Arabic%2520legal%2520NLP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.09768v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Large%20Language%20Models%20Predict%20the%20Outcome%20of%20Judicial%20Decisions%3F&entry.906535625=Mohamed%20Bayan%20Kmainasi%20and%20Ali%20Ezzat%20Shahroor%20and%20Amani%20Al-Ghraibah&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20exceptional%20capabilities%20in%20Natural%0ALanguage%20Processing%20%28NLP%29%20across%20diverse%20domains.%20However%2C%20their%20application%20in%0Aspecialized%20tasks%20such%20as%20Legal%20Judgment%20Prediction%20%28LJP%29%20for%20low-resource%0Alanguages%20like%20Arabic%20remains%20underexplored.%20In%20this%20work%2C%20we%20address%20this%20gap%0Aby%20developing%20an%20Arabic%20LJP%20dataset%2C%20collected%20and%20preprocessed%20from%20Saudi%0Acommercial%20court%20judgments.%20We%20benchmark%20state-of-the-art%20open-source%20LLMs%2C%0Aincluding%20LLaMA-3.2-3B%20and%20LLaMA-3.1-8B%2C%20under%20varying%20configurations%20such%20as%0Azero-shot%2C%20one-shot%2C%20and%20fine-tuning%20using%20QLoRA.%20Additionally%2C%20we%20used%20a%0Acomprehensive%20evaluation%20framework%20combining%20quantitative%20metrics%20%28BLEU%20and%0AROUGE%29%20and%20qualitative%20assessments%20%28Coherence%2C%20legal%20language%2C%20clarity%29.%20Our%0Aresults%20demonstrate%20that%20fine-tuned%20smaller%20models%20achieve%20comparable%0Aperformance%20to%20larger%20models%20in%20task-specific%20contexts%20while%20offering%0Asignificant%20resource%20efficiency.%20Furthermore%2C%20we%20investigate%20the%20effects%20of%0Aprompt%20engineering%20and%20fine-tuning%20on%20model%20outputs%2C%20providing%20insights%20into%0Aperformance%20variability%20and%20instruction%20sensitivity.%20By%20making%20the%20dataset%2C%0Aimplementation%20code%2C%20and%20models%20publicly%20available%2C%20we%20establish%20a%20robust%0Afoundation%20for%20future%20research%20in%20Arabic%20legal%20NLP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.09768v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


