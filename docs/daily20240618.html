<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240617.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "RetinaGS: Scalable Training for Dense Scene Rendering with Billion-Scale\n  3D Gaussians", "author": "Bingling Li and Shengyi Chen and Luchao Wang and Kaimin He and Sijie Yan and Yuanjun Xiong", "abstract": "  In this work, we explore the possibility of training high-parameter 3D\nGaussian splatting (3DGS) models on large-scale, high-resolution datasets. We\ndesign a general model parallel training method for 3DGS, named RetinaGS, which\nuses a proper rendering equation and can be applied to any scene and arbitrary\ndistribution of Gaussian primitives. It enables us to explore the scaling\nbehavior of 3DGS in terms of primitive numbers and training resolutions that\nwere difficult to explore before and surpass previous state-of-the-art\nreconstruction quality. We observe a clear positive trend of increasing visual\nquality when increasing primitive numbers with our method. We also demonstrate\nthe first attempt at training a 3DGS model with more than one billion\nprimitives on the full MatrixCity dataset that attains a promising visual\nquality.\n", "link": "http://arxiv.org/abs/2406.11836v1", "date": "2024-06-17", "relevancy": 3.2224, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7038}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6494}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RetinaGS%3A%20Scalable%20Training%20for%20Dense%20Scene%20Rendering%20with%20Billion-Scale%0A%20%203D%20Gaussians&body=Title%3A%20RetinaGS%3A%20Scalable%20Training%20for%20Dense%20Scene%20Rendering%20with%20Billion-Scale%0A%20%203D%20Gaussians%0AAuthor%3A%20Bingling%20Li%20and%20Shengyi%20Chen%20and%20Luchao%20Wang%20and%20Kaimin%20He%20and%20Sijie%20Yan%20and%20Yuanjun%20Xiong%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20explore%20the%20possibility%20of%20training%20high-parameter%203D%0AGaussian%20splatting%20%283DGS%29%20models%20on%20large-scale%2C%20high-resolution%20datasets.%20We%0Adesign%20a%20general%20model%20parallel%20training%20method%20for%203DGS%2C%20named%20RetinaGS%2C%20which%0Auses%20a%20proper%20rendering%20equation%20and%20can%20be%20applied%20to%20any%20scene%20and%20arbitrary%0Adistribution%20of%20Gaussian%20primitives.%20It%20enables%20us%20to%20explore%20the%20scaling%0Abehavior%20of%203DGS%20in%20terms%20of%20primitive%20numbers%20and%20training%20resolutions%20that%0Awere%20difficult%20to%20explore%20before%20and%20surpass%20previous%20state-of-the-art%0Areconstruction%20quality.%20We%20observe%20a%20clear%20positive%20trend%20of%20increasing%20visual%0Aquality%20when%20increasing%20primitive%20numbers%20with%20our%20method.%20We%20also%20demonstrate%0Athe%20first%20attempt%20at%20training%20a%203DGS%20model%20with%20more%20than%20one%20billion%0Aprimitives%20on%20the%20full%20MatrixCity%20dataset%20that%20attains%20a%20promising%20visual%0Aquality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11836v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetinaGS%253A%2520Scalable%2520Training%2520for%2520Dense%2520Scene%2520Rendering%2520with%2520Billion-Scale%250A%2520%25203D%2520Gaussians%26entry.906535625%3DBingling%2520Li%2520and%2520Shengyi%2520Chen%2520and%2520Luchao%2520Wang%2520and%2520Kaimin%2520He%2520and%2520Sijie%2520Yan%2520and%2520Yuanjun%2520Xiong%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520explore%2520the%2520possibility%2520of%2520training%2520high-parameter%25203D%250AGaussian%2520splatting%2520%25283DGS%2529%2520models%2520on%2520large-scale%252C%2520high-resolution%2520datasets.%2520We%250Adesign%2520a%2520general%2520model%2520parallel%2520training%2520method%2520for%25203DGS%252C%2520named%2520RetinaGS%252C%2520which%250Auses%2520a%2520proper%2520rendering%2520equation%2520and%2520can%2520be%2520applied%2520to%2520any%2520scene%2520and%2520arbitrary%250Adistribution%2520of%2520Gaussian%2520primitives.%2520It%2520enables%2520us%2520to%2520explore%2520the%2520scaling%250Abehavior%2520of%25203DGS%2520in%2520terms%2520of%2520primitive%2520numbers%2520and%2520training%2520resolutions%2520that%250Awere%2520difficult%2520to%2520explore%2520before%2520and%2520surpass%2520previous%2520state-of-the-art%250Areconstruction%2520quality.%2520We%2520observe%2520a%2520clear%2520positive%2520trend%2520of%2520increasing%2520visual%250Aquality%2520when%2520increasing%2520primitive%2520numbers%2520with%2520our%2520method.%2520We%2520also%2520demonstrate%250Athe%2520first%2520attempt%2520at%2520training%2520a%25203DGS%2520model%2520with%2520more%2520than%2520one%2520billion%250Aprimitives%2520on%2520the%2520full%2520MatrixCity%2520dataset%2520that%2520attains%2520a%2520promising%2520visual%250Aquality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11836v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RetinaGS%3A%20Scalable%20Training%20for%20Dense%20Scene%20Rendering%20with%20Billion-Scale%0A%20%203D%20Gaussians&entry.906535625=Bingling%20Li%20and%20Shengyi%20Chen%20and%20Luchao%20Wang%20and%20Kaimin%20He%20and%20Sijie%20Yan%20and%20Yuanjun%20Xiong&entry.1292438233=%20%20In%20this%20work%2C%20we%20explore%20the%20possibility%20of%20training%20high-parameter%203D%0AGaussian%20splatting%20%283DGS%29%20models%20on%20large-scale%2C%20high-resolution%20datasets.%20We%0Adesign%20a%20general%20model%20parallel%20training%20method%20for%203DGS%2C%20named%20RetinaGS%2C%20which%0Auses%20a%20proper%20rendering%20equation%20and%20can%20be%20applied%20to%20any%20scene%20and%20arbitrary%0Adistribution%20of%20Gaussian%20primitives.%20It%20enables%20us%20to%20explore%20the%20scaling%0Abehavior%20of%203DGS%20in%20terms%20of%20primitive%20numbers%20and%20training%20resolutions%20that%0Awere%20difficult%20to%20explore%20before%20and%20surpass%20previous%20state-of-the-art%0Areconstruction%20quality.%20We%20observe%20a%20clear%20positive%20trend%20of%20increasing%20visual%0Aquality%20when%20increasing%20primitive%20numbers%20with%20our%20method.%20We%20also%20demonstrate%0Athe%20first%20attempt%20at%20training%20a%203DGS%20model%20with%20more%20than%20one%20billion%0Aprimitives%20on%20the%20full%20MatrixCity%20dataset%20that%20attains%20a%20promising%20visual%0Aquality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11836v1&entry.124074799=Read"},
{"title": "Effective Rank Analysis and Regularization for Enhanced 3D Gaussian\n  Splatting", "author": "Junha Hyung and Susung Hong and Sungwon Hwang and Jaeseong Lee and Jaegul Choo and Jin-Hwa Kim", "abstract": "  3D reconstruction from multi-view images is one of the fundamental challenges\nin computer vision and graphics. Recently, 3D Gaussian Splatting (3DGS) has\nemerged as a promising technique capable of real-time rendering with\nhigh-quality 3D reconstruction. This method utilizes 3D Gaussian representation\nand tile-based splatting techniques, bypassing the expensive neural field\nquerying. Despite its potential, 3DGS encounters challenges, including\nneedle-like artifacts, suboptimal geometries, and inaccurate normals, due to\nthe Gaussians converging into anisotropic Gaussians with one dominant variance.\nWe propose using effective rank analysis to examine the shape statistics of 3D\nGaussian primitives, and identify the Gaussians indeed converge into\nneedle-like shapes with the effective rank 1. To address this, we introduce\neffective rank as a regularization, which constrains the structure of the\nGaussians. Our new regularization method enhances normal and geometry\nreconstruction while reducing needle-like artifacts. The approach can be\nintegrated as an add-on module to other 3DGS variants, improving their quality\nwithout compromising visual fidelity.\n", "link": "http://arxiv.org/abs/2406.11672v1", "date": "2024-06-17", "relevancy": 3.194, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7045}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6515}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5604}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Effective%20Rank%20Analysis%20and%20Regularization%20for%20Enhanced%203D%20Gaussian%0A%20%20Splatting&body=Title%3A%20Effective%20Rank%20Analysis%20and%20Regularization%20for%20Enhanced%203D%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Junha%20Hyung%20and%20Susung%20Hong%20and%20Sungwon%20Hwang%20and%20Jaeseong%20Lee%20and%20Jaegul%20Choo%20and%20Jin-Hwa%20Kim%0AAbstract%3A%20%20%203D%20reconstruction%20from%20multi-view%20images%20is%20one%20of%20the%20fundamental%20challenges%0Ain%20computer%20vision%20and%20graphics.%20Recently%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%0Aemerged%20as%20a%20promising%20technique%20capable%20of%20real-time%20rendering%20with%0Ahigh-quality%203D%20reconstruction.%20This%20method%20utilizes%203D%20Gaussian%20representation%0Aand%20tile-based%20splatting%20techniques%2C%20bypassing%20the%20expensive%20neural%20field%0Aquerying.%20Despite%20its%20potential%2C%203DGS%20encounters%20challenges%2C%20including%0Aneedle-like%20artifacts%2C%20suboptimal%20geometries%2C%20and%20inaccurate%20normals%2C%20due%20to%0Athe%20Gaussians%20converging%20into%20anisotropic%20Gaussians%20with%20one%20dominant%20variance.%0AWe%20propose%20using%20effective%20rank%20analysis%20to%20examine%20the%20shape%20statistics%20of%203D%0AGaussian%20primitives%2C%20and%20identify%20the%20Gaussians%20indeed%20converge%20into%0Aneedle-like%20shapes%20with%20the%20effective%20rank%201.%20To%20address%20this%2C%20we%20introduce%0Aeffective%20rank%20as%20a%20regularization%2C%20which%20constrains%20the%20structure%20of%20the%0AGaussians.%20Our%20new%20regularization%20method%20enhances%20normal%20and%20geometry%0Areconstruction%20while%20reducing%20needle-like%20artifacts.%20The%20approach%20can%20be%0Aintegrated%20as%20an%20add-on%20module%20to%20other%203DGS%20variants%2C%20improving%20their%20quality%0Awithout%20compromising%20visual%20fidelity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11672v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEffective%2520Rank%2520Analysis%2520and%2520Regularization%2520for%2520Enhanced%25203D%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DJunha%2520Hyung%2520and%2520Susung%2520Hong%2520and%2520Sungwon%2520Hwang%2520and%2520Jaeseong%2520Lee%2520and%2520Jaegul%2520Choo%2520and%2520Jin-Hwa%2520Kim%26entry.1292438233%3D%2520%25203D%2520reconstruction%2520from%2520multi-view%2520images%2520is%2520one%2520of%2520the%2520fundamental%2520challenges%250Ain%2520computer%2520vision%2520and%2520graphics.%2520Recently%252C%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%250Aemerged%2520as%2520a%2520promising%2520technique%2520capable%2520of%2520real-time%2520rendering%2520with%250Ahigh-quality%25203D%2520reconstruction.%2520This%2520method%2520utilizes%25203D%2520Gaussian%2520representation%250Aand%2520tile-based%2520splatting%2520techniques%252C%2520bypassing%2520the%2520expensive%2520neural%2520field%250Aquerying.%2520Despite%2520its%2520potential%252C%25203DGS%2520encounters%2520challenges%252C%2520including%250Aneedle-like%2520artifacts%252C%2520suboptimal%2520geometries%252C%2520and%2520inaccurate%2520normals%252C%2520due%2520to%250Athe%2520Gaussians%2520converging%2520into%2520anisotropic%2520Gaussians%2520with%2520one%2520dominant%2520variance.%250AWe%2520propose%2520using%2520effective%2520rank%2520analysis%2520to%2520examine%2520the%2520shape%2520statistics%2520of%25203D%250AGaussian%2520primitives%252C%2520and%2520identify%2520the%2520Gaussians%2520indeed%2520converge%2520into%250Aneedle-like%2520shapes%2520with%2520the%2520effective%2520rank%25201.%2520To%2520address%2520this%252C%2520we%2520introduce%250Aeffective%2520rank%2520as%2520a%2520regularization%252C%2520which%2520constrains%2520the%2520structure%2520of%2520the%250AGaussians.%2520Our%2520new%2520regularization%2520method%2520enhances%2520normal%2520and%2520geometry%250Areconstruction%2520while%2520reducing%2520needle-like%2520artifacts.%2520The%2520approach%2520can%2520be%250Aintegrated%2520as%2520an%2520add-on%2520module%2520to%2520other%25203DGS%2520variants%252C%2520improving%2520their%2520quality%250Awithout%2520compromising%2520visual%2520fidelity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11672v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Effective%20Rank%20Analysis%20and%20Regularization%20for%20Enhanced%203D%20Gaussian%0A%20%20Splatting&entry.906535625=Junha%20Hyung%20and%20Susung%20Hong%20and%20Sungwon%20Hwang%20and%20Jaeseong%20Lee%20and%20Jaegul%20Choo%20and%20Jin-Hwa%20Kim&entry.1292438233=%20%203D%20reconstruction%20from%20multi-view%20images%20is%20one%20of%20the%20fundamental%20challenges%0Ain%20computer%20vision%20and%20graphics.%20Recently%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%0Aemerged%20as%20a%20promising%20technique%20capable%20of%20real-time%20rendering%20with%0Ahigh-quality%203D%20reconstruction.%20This%20method%20utilizes%203D%20Gaussian%20representation%0Aand%20tile-based%20splatting%20techniques%2C%20bypassing%20the%20expensive%20neural%20field%0Aquerying.%20Despite%20its%20potential%2C%203DGS%20encounters%20challenges%2C%20including%0Aneedle-like%20artifacts%2C%20suboptimal%20geometries%2C%20and%20inaccurate%20normals%2C%20due%20to%0Athe%20Gaussians%20converging%20into%20anisotropic%20Gaussians%20with%20one%20dominant%20variance.%0AWe%20propose%20using%20effective%20rank%20analysis%20to%20examine%20the%20shape%20statistics%20of%203D%0AGaussian%20primitives%2C%20and%20identify%20the%20Gaussians%20indeed%20converge%20into%0Aneedle-like%20shapes%20with%20the%20effective%20rank%201.%20To%20address%20this%2C%20we%20introduce%0Aeffective%20rank%20as%20a%20regularization%2C%20which%20constrains%20the%20structure%20of%20the%0AGaussians.%20Our%20new%20regularization%20method%20enhances%20normal%20and%20geometry%0Areconstruction%20while%20reducing%20needle-like%20artifacts.%20The%20approach%20can%20be%0Aintegrated%20as%20an%20add-on%20module%20to%20other%203DGS%20variants%2C%20improving%20their%20quality%0Awithout%20compromising%20visual%20fidelity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11672v1&entry.124074799=Read"},
{"title": "Duoduo CLIP: Efficient 3D Understanding with Multi-View Images", "author": "Han-Hung Lee and Yiming Zhang and Angel X. Chang", "abstract": "  We introduce Duoduo CLIP, a model for 3D representation learning that learns\nshape encodings from multi-view images instead of point-clouds. The choice of\nmulti-view images allows us to leverage 2D priors from off-the-shelf CLIP\nmodels to facilitate fine-tuning with 3D data. Our approach not only shows\nbetter generalization compared to existing point cloud methods, but also\nreduces GPU requirements and training time. In addition, we modify the model\nwith cross-view attention to leverage information across multiple frames of the\nobject which further boosts performance. Compared to the current SOTA point\ncloud method that requires 480 A100 hours to train 1 billion model parameters\nwe only require 57 A5000 hours and 87 million parameters. Multi-view images\nalso provide more flexibility in use cases compared to point clouds. This\nincludes being able to encode objects with a variable number of images, with\nbetter performance when more views are used. This is in contrast to point cloud\nbased methods, where an entire scan or model of an object is required. We\nshowcase this flexibility with object retrieval from images of real-world\nobjects. Our model also achieves better performance in more fine-grained text\nto shape retrieval, demonstrating better text-and-shape alignment than point\ncloud based models.\n", "link": "http://arxiv.org/abs/2406.11579v1", "date": "2024-06-17", "relevancy": 3.1604, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6861}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6087}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Duoduo%20CLIP%3A%20Efficient%203D%20Understanding%20with%20Multi-View%20Images&body=Title%3A%20Duoduo%20CLIP%3A%20Efficient%203D%20Understanding%20with%20Multi-View%20Images%0AAuthor%3A%20Han-Hung%20Lee%20and%20Yiming%20Zhang%20and%20Angel%20X.%20Chang%0AAbstract%3A%20%20%20We%20introduce%20Duoduo%20CLIP%2C%20a%20model%20for%203D%20representation%20learning%20that%20learns%0Ashape%20encodings%20from%20multi-view%20images%20instead%20of%20point-clouds.%20The%20choice%20of%0Amulti-view%20images%20allows%20us%20to%20leverage%202D%20priors%20from%20off-the-shelf%20CLIP%0Amodels%20to%20facilitate%20fine-tuning%20with%203D%20data.%20Our%20approach%20not%20only%20shows%0Abetter%20generalization%20compared%20to%20existing%20point%20cloud%20methods%2C%20but%20also%0Areduces%20GPU%20requirements%20and%20training%20time.%20In%20addition%2C%20we%20modify%20the%20model%0Awith%20cross-view%20attention%20to%20leverage%20information%20across%20multiple%20frames%20of%20the%0Aobject%20which%20further%20boosts%20performance.%20Compared%20to%20the%20current%20SOTA%20point%0Acloud%20method%20that%20requires%20480%20A100%20hours%20to%20train%201%20billion%20model%20parameters%0Awe%20only%20require%2057%20A5000%20hours%20and%2087%20million%20parameters.%20Multi-view%20images%0Aalso%20provide%20more%20flexibility%20in%20use%20cases%20compared%20to%20point%20clouds.%20This%0Aincludes%20being%20able%20to%20encode%20objects%20with%20a%20variable%20number%20of%20images%2C%20with%0Abetter%20performance%20when%20more%20views%20are%20used.%20This%20is%20in%20contrast%20to%20point%20cloud%0Abased%20methods%2C%20where%20an%20entire%20scan%20or%20model%20of%20an%20object%20is%20required.%20We%0Ashowcase%20this%20flexibility%20with%20object%20retrieval%20from%20images%20of%20real-world%0Aobjects.%20Our%20model%20also%20achieves%20better%20performance%20in%20more%20fine-grained%20text%0Ato%20shape%20retrieval%2C%20demonstrating%20better%20text-and-shape%20alignment%20than%20point%0Acloud%20based%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11579v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDuoduo%2520CLIP%253A%2520Efficient%25203D%2520Understanding%2520with%2520Multi-View%2520Images%26entry.906535625%3DHan-Hung%2520Lee%2520and%2520Yiming%2520Zhang%2520and%2520Angel%2520X.%2520Chang%26entry.1292438233%3D%2520%2520We%2520introduce%2520Duoduo%2520CLIP%252C%2520a%2520model%2520for%25203D%2520representation%2520learning%2520that%2520learns%250Ashape%2520encodings%2520from%2520multi-view%2520images%2520instead%2520of%2520point-clouds.%2520The%2520choice%2520of%250Amulti-view%2520images%2520allows%2520us%2520to%2520leverage%25202D%2520priors%2520from%2520off-the-shelf%2520CLIP%250Amodels%2520to%2520facilitate%2520fine-tuning%2520with%25203D%2520data.%2520Our%2520approach%2520not%2520only%2520shows%250Abetter%2520generalization%2520compared%2520to%2520existing%2520point%2520cloud%2520methods%252C%2520but%2520also%250Areduces%2520GPU%2520requirements%2520and%2520training%2520time.%2520In%2520addition%252C%2520we%2520modify%2520the%2520model%250Awith%2520cross-view%2520attention%2520to%2520leverage%2520information%2520across%2520multiple%2520frames%2520of%2520the%250Aobject%2520which%2520further%2520boosts%2520performance.%2520Compared%2520to%2520the%2520current%2520SOTA%2520point%250Acloud%2520method%2520that%2520requires%2520480%2520A100%2520hours%2520to%2520train%25201%2520billion%2520model%2520parameters%250Awe%2520only%2520require%252057%2520A5000%2520hours%2520and%252087%2520million%2520parameters.%2520Multi-view%2520images%250Aalso%2520provide%2520more%2520flexibility%2520in%2520use%2520cases%2520compared%2520to%2520point%2520clouds.%2520This%250Aincludes%2520being%2520able%2520to%2520encode%2520objects%2520with%2520a%2520variable%2520number%2520of%2520images%252C%2520with%250Abetter%2520performance%2520when%2520more%2520views%2520are%2520used.%2520This%2520is%2520in%2520contrast%2520to%2520point%2520cloud%250Abased%2520methods%252C%2520where%2520an%2520entire%2520scan%2520or%2520model%2520of%2520an%2520object%2520is%2520required.%2520We%250Ashowcase%2520this%2520flexibility%2520with%2520object%2520retrieval%2520from%2520images%2520of%2520real-world%250Aobjects.%2520Our%2520model%2520also%2520achieves%2520better%2520performance%2520in%2520more%2520fine-grained%2520text%250Ato%2520shape%2520retrieval%252C%2520demonstrating%2520better%2520text-and-shape%2520alignment%2520than%2520point%250Acloud%2520based%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11579v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Duoduo%20CLIP%3A%20Efficient%203D%20Understanding%20with%20Multi-View%20Images&entry.906535625=Han-Hung%20Lee%20and%20Yiming%20Zhang%20and%20Angel%20X.%20Chang&entry.1292438233=%20%20We%20introduce%20Duoduo%20CLIP%2C%20a%20model%20for%203D%20representation%20learning%20that%20learns%0Ashape%20encodings%20from%20multi-view%20images%20instead%20of%20point-clouds.%20The%20choice%20of%0Amulti-view%20images%20allows%20us%20to%20leverage%202D%20priors%20from%20off-the-shelf%20CLIP%0Amodels%20to%20facilitate%20fine-tuning%20with%203D%20data.%20Our%20approach%20not%20only%20shows%0Abetter%20generalization%20compared%20to%20existing%20point%20cloud%20methods%2C%20but%20also%0Areduces%20GPU%20requirements%20and%20training%20time.%20In%20addition%2C%20we%20modify%20the%20model%0Awith%20cross-view%20attention%20to%20leverage%20information%20across%20multiple%20frames%20of%20the%0Aobject%20which%20further%20boosts%20performance.%20Compared%20to%20the%20current%20SOTA%20point%0Acloud%20method%20that%20requires%20480%20A100%20hours%20to%20train%201%20billion%20model%20parameters%0Awe%20only%20require%2057%20A5000%20hours%20and%2087%20million%20parameters.%20Multi-view%20images%0Aalso%20provide%20more%20flexibility%20in%20use%20cases%20compared%20to%20point%20clouds.%20This%0Aincludes%20being%20able%20to%20encode%20objects%20with%20a%20variable%20number%20of%20images%2C%20with%0Abetter%20performance%20when%20more%20views%20are%20used.%20This%20is%20in%20contrast%20to%20point%20cloud%0Abased%20methods%2C%20where%20an%20entire%20scan%20or%20model%20of%20an%20object%20is%20required.%20We%0Ashowcase%20this%20flexibility%20with%20object%20retrieval%20from%20images%20of%20real-world%0Aobjects.%20Our%20model%20also%20achieves%20better%20performance%20in%20more%20fine-grained%20text%0Ato%20shape%20retrieval%2C%20demonstrating%20better%20text-and-shape%20alignment%20than%20point%0Acloud%20based%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11579v1&entry.124074799=Read"},
{"title": "MegaScenes: Scene-Level View Synthesis at Scale", "author": "Joseph Tung and Gene Chou and Ruojin Cai and Guandao Yang and Kai Zhang and Gordon Wetzstein and Bharath Hariharan and Noah Snavely", "abstract": "  Scene-level novel view synthesis (NVS) is fundamental to many vision and\ngraphics applications. Recently, pose-conditioned diffusion models have led to\nsignificant progress by extracting 3D information from 2D foundation models,\nbut these methods are limited by the lack of scene-level training data. Common\ndataset choices either consist of isolated objects (Objaverse), or of\nobject-centric scenes with limited pose distributions (DTU, CO3D). In this\npaper, we create a large-scale scene-level dataset from Internet photo\ncollections, called MegaScenes, which contains over 100K structure from motion\n(SfM) reconstructions from around the world. Internet photos represent a\nscalable data source but come with challenges such as lighting and transient\nobjects. We address these issues to further create a subset suitable for the\ntask of NVS. Additionally, we analyze failure cases of state-of-the-art NVS\nmethods and significantly improve generation consistency. Through extensive\nexperiments, we validate the effectiveness of both our dataset and method on\ngenerating in-the-wild scenes. For details on the dataset and code, see our\nproject page at https://megascenes.github.io .\n", "link": "http://arxiv.org/abs/2406.11819v1", "date": "2024-06-17", "relevancy": 3.1128, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6433}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6433}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MegaScenes%3A%20Scene-Level%20View%20Synthesis%20at%20Scale&body=Title%3A%20MegaScenes%3A%20Scene-Level%20View%20Synthesis%20at%20Scale%0AAuthor%3A%20Joseph%20Tung%20and%20Gene%20Chou%20and%20Ruojin%20Cai%20and%20Guandao%20Yang%20and%20Kai%20Zhang%20and%20Gordon%20Wetzstein%20and%20Bharath%20Hariharan%20and%20Noah%20Snavely%0AAbstract%3A%20%20%20Scene-level%20novel%20view%20synthesis%20%28NVS%29%20is%20fundamental%20to%20many%20vision%20and%0Agraphics%20applications.%20Recently%2C%20pose-conditioned%20diffusion%20models%20have%20led%20to%0Asignificant%20progress%20by%20extracting%203D%20information%20from%202D%20foundation%20models%2C%0Abut%20these%20methods%20are%20limited%20by%20the%20lack%20of%20scene-level%20training%20data.%20Common%0Adataset%20choices%20either%20consist%20of%20isolated%20objects%20%28Objaverse%29%2C%20or%20of%0Aobject-centric%20scenes%20with%20limited%20pose%20distributions%20%28DTU%2C%20CO3D%29.%20In%20this%0Apaper%2C%20we%20create%20a%20large-scale%20scene-level%20dataset%20from%20Internet%20photo%0Acollections%2C%20called%20MegaScenes%2C%20which%20contains%20over%20100K%20structure%20from%20motion%0A%28SfM%29%20reconstructions%20from%20around%20the%20world.%20Internet%20photos%20represent%20a%0Ascalable%20data%20source%20but%20come%20with%20challenges%20such%20as%20lighting%20and%20transient%0Aobjects.%20We%20address%20these%20issues%20to%20further%20create%20a%20subset%20suitable%20for%20the%0Atask%20of%20NVS.%20Additionally%2C%20we%20analyze%20failure%20cases%20of%20state-of-the-art%20NVS%0Amethods%20and%20significantly%20improve%20generation%20consistency.%20Through%20extensive%0Aexperiments%2C%20we%20validate%20the%20effectiveness%20of%20both%20our%20dataset%20and%20method%20on%0Agenerating%20in-the-wild%20scenes.%20For%20details%20on%20the%20dataset%20and%20code%2C%20see%20our%0Aproject%20page%20at%20https%3A//megascenes.github.io%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11819v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMegaScenes%253A%2520Scene-Level%2520View%2520Synthesis%2520at%2520Scale%26entry.906535625%3DJoseph%2520Tung%2520and%2520Gene%2520Chou%2520and%2520Ruojin%2520Cai%2520and%2520Guandao%2520Yang%2520and%2520Kai%2520Zhang%2520and%2520Gordon%2520Wetzstein%2520and%2520Bharath%2520Hariharan%2520and%2520Noah%2520Snavely%26entry.1292438233%3D%2520%2520Scene-level%2520novel%2520view%2520synthesis%2520%2528NVS%2529%2520is%2520fundamental%2520to%2520many%2520vision%2520and%250Agraphics%2520applications.%2520Recently%252C%2520pose-conditioned%2520diffusion%2520models%2520have%2520led%2520to%250Asignificant%2520progress%2520by%2520extracting%25203D%2520information%2520from%25202D%2520foundation%2520models%252C%250Abut%2520these%2520methods%2520are%2520limited%2520by%2520the%2520lack%2520of%2520scene-level%2520training%2520data.%2520Common%250Adataset%2520choices%2520either%2520consist%2520of%2520isolated%2520objects%2520%2528Objaverse%2529%252C%2520or%2520of%250Aobject-centric%2520scenes%2520with%2520limited%2520pose%2520distributions%2520%2528DTU%252C%2520CO3D%2529.%2520In%2520this%250Apaper%252C%2520we%2520create%2520a%2520large-scale%2520scene-level%2520dataset%2520from%2520Internet%2520photo%250Acollections%252C%2520called%2520MegaScenes%252C%2520which%2520contains%2520over%2520100K%2520structure%2520from%2520motion%250A%2528SfM%2529%2520reconstructions%2520from%2520around%2520the%2520world.%2520Internet%2520photos%2520represent%2520a%250Ascalable%2520data%2520source%2520but%2520come%2520with%2520challenges%2520such%2520as%2520lighting%2520and%2520transient%250Aobjects.%2520We%2520address%2520these%2520issues%2520to%2520further%2520create%2520a%2520subset%2520suitable%2520for%2520the%250Atask%2520of%2520NVS.%2520Additionally%252C%2520we%2520analyze%2520failure%2520cases%2520of%2520state-of-the-art%2520NVS%250Amethods%2520and%2520significantly%2520improve%2520generation%2520consistency.%2520Through%2520extensive%250Aexperiments%252C%2520we%2520validate%2520the%2520effectiveness%2520of%2520both%2520our%2520dataset%2520and%2520method%2520on%250Agenerating%2520in-the-wild%2520scenes.%2520For%2520details%2520on%2520the%2520dataset%2520and%2520code%252C%2520see%2520our%250Aproject%2520page%2520at%2520https%253A//megascenes.github.io%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11819v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MegaScenes%3A%20Scene-Level%20View%20Synthesis%20at%20Scale&entry.906535625=Joseph%20Tung%20and%20Gene%20Chou%20and%20Ruojin%20Cai%20and%20Guandao%20Yang%20and%20Kai%20Zhang%20and%20Gordon%20Wetzstein%20and%20Bharath%20Hariharan%20and%20Noah%20Snavely&entry.1292438233=%20%20Scene-level%20novel%20view%20synthesis%20%28NVS%29%20is%20fundamental%20to%20many%20vision%20and%0Agraphics%20applications.%20Recently%2C%20pose-conditioned%20diffusion%20models%20have%20led%20to%0Asignificant%20progress%20by%20extracting%203D%20information%20from%202D%20foundation%20models%2C%0Abut%20these%20methods%20are%20limited%20by%20the%20lack%20of%20scene-level%20training%20data.%20Common%0Adataset%20choices%20either%20consist%20of%20isolated%20objects%20%28Objaverse%29%2C%20or%20of%0Aobject-centric%20scenes%20with%20limited%20pose%20distributions%20%28DTU%2C%20CO3D%29.%20In%20this%0Apaper%2C%20we%20create%20a%20large-scale%20scene-level%20dataset%20from%20Internet%20photo%0Acollections%2C%20called%20MegaScenes%2C%20which%20contains%20over%20100K%20structure%20from%20motion%0A%28SfM%29%20reconstructions%20from%20around%20the%20world.%20Internet%20photos%20represent%20a%0Ascalable%20data%20source%20but%20come%20with%20challenges%20such%20as%20lighting%20and%20transient%0Aobjects.%20We%20address%20these%20issues%20to%20further%20create%20a%20subset%20suitable%20for%20the%0Atask%20of%20NVS.%20Additionally%2C%20we%20analyze%20failure%20cases%20of%20state-of-the-art%20NVS%0Amethods%20and%20significantly%20improve%20generation%20consistency.%20Through%20extensive%0Aexperiments%2C%20we%20validate%20the%20effectiveness%20of%20both%20our%20dataset%20and%20method%20on%0Agenerating%20in-the-wild%20scenes.%20For%20details%20on%20the%20dataset%20and%20code%2C%20see%20our%0Aproject%20page%20at%20https%3A//megascenes.github.io%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11819v1&entry.124074799=Read"},
{"title": "Matching Query Image Against Selected NeRF Feature for Efficient and\n  Scalable Localization", "author": "Huaiji Zhou and Bing Wang and Changhao Chen", "abstract": "  Neural implicit representations such as NeRF have revolutionized 3D scene\nrepresentation with photo-realistic quality. However, existing methods for\nvisual localization within NeRF representations suffer from inefficiency and\nscalability issues, particularly in large-scale environments. This work\nproposes MatLoc-NeRF, a novel matching-based localization framework using\nselected NeRF features. It addresses efficiency by employing a learnable\nfeature selection mechanism that identifies informative NeRF features for\nmatching with query images. This eliminates the need for all NeRF features or\nadditional descriptors, leading to faster and more accurate pose estimation. To\ntackle large-scale scenes, MatLoc-NeRF utilizes a pose-aware scene partitioning\nstrategy. It ensures that only the most relevant NeRF sub-block generates key\nfeatures for a specific pose. Additionally, scene segmentation and a place\npredictor provide fast coarse initial pose estimation. Evaluations on public\nlarge-scale datasets demonstrate that MatLoc-NeRF achieves superior efficiency\nand accuracy compared to existing NeRF-based localization methods.\n", "link": "http://arxiv.org/abs/2406.11766v1", "date": "2024-06-17", "relevancy": 3.0304, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6719}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5977}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Matching%20Query%20Image%20Against%20Selected%20NeRF%20Feature%20for%20Efficient%20and%0A%20%20Scalable%20Localization&body=Title%3A%20Matching%20Query%20Image%20Against%20Selected%20NeRF%20Feature%20for%20Efficient%20and%0A%20%20Scalable%20Localization%0AAuthor%3A%20Huaiji%20Zhou%20and%20Bing%20Wang%20and%20Changhao%20Chen%0AAbstract%3A%20%20%20Neural%20implicit%20representations%20such%20as%20NeRF%20have%20revolutionized%203D%20scene%0Arepresentation%20with%20photo-realistic%20quality.%20However%2C%20existing%20methods%20for%0Avisual%20localization%20within%20NeRF%20representations%20suffer%20from%20inefficiency%20and%0Ascalability%20issues%2C%20particularly%20in%20large-scale%20environments.%20This%20work%0Aproposes%20MatLoc-NeRF%2C%20a%20novel%20matching-based%20localization%20framework%20using%0Aselected%20NeRF%20features.%20It%20addresses%20efficiency%20by%20employing%20a%20learnable%0Afeature%20selection%20mechanism%20that%20identifies%20informative%20NeRF%20features%20for%0Amatching%20with%20query%20images.%20This%20eliminates%20the%20need%20for%20all%20NeRF%20features%20or%0Aadditional%20descriptors%2C%20leading%20to%20faster%20and%20more%20accurate%20pose%20estimation.%20To%0Atackle%20large-scale%20scenes%2C%20MatLoc-NeRF%20utilizes%20a%20pose-aware%20scene%20partitioning%0Astrategy.%20It%20ensures%20that%20only%20the%20most%20relevant%20NeRF%20sub-block%20generates%20key%0Afeatures%20for%20a%20specific%20pose.%20Additionally%2C%20scene%20segmentation%20and%20a%20place%0Apredictor%20provide%20fast%20coarse%20initial%20pose%20estimation.%20Evaluations%20on%20public%0Alarge-scale%20datasets%20demonstrate%20that%20MatLoc-NeRF%20achieves%20superior%20efficiency%0Aand%20accuracy%20compared%20to%20existing%20NeRF-based%20localization%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11766v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatching%2520Query%2520Image%2520Against%2520Selected%2520NeRF%2520Feature%2520for%2520Efficient%2520and%250A%2520%2520Scalable%2520Localization%26entry.906535625%3DHuaiji%2520Zhou%2520and%2520Bing%2520Wang%2520and%2520Changhao%2520Chen%26entry.1292438233%3D%2520%2520Neural%2520implicit%2520representations%2520such%2520as%2520NeRF%2520have%2520revolutionized%25203D%2520scene%250Arepresentation%2520with%2520photo-realistic%2520quality.%2520However%252C%2520existing%2520methods%2520for%250Avisual%2520localization%2520within%2520NeRF%2520representations%2520suffer%2520from%2520inefficiency%2520and%250Ascalability%2520issues%252C%2520particularly%2520in%2520large-scale%2520environments.%2520This%2520work%250Aproposes%2520MatLoc-NeRF%252C%2520a%2520novel%2520matching-based%2520localization%2520framework%2520using%250Aselected%2520NeRF%2520features.%2520It%2520addresses%2520efficiency%2520by%2520employing%2520a%2520learnable%250Afeature%2520selection%2520mechanism%2520that%2520identifies%2520informative%2520NeRF%2520features%2520for%250Amatching%2520with%2520query%2520images.%2520This%2520eliminates%2520the%2520need%2520for%2520all%2520NeRF%2520features%2520or%250Aadditional%2520descriptors%252C%2520leading%2520to%2520faster%2520and%2520more%2520accurate%2520pose%2520estimation.%2520To%250Atackle%2520large-scale%2520scenes%252C%2520MatLoc-NeRF%2520utilizes%2520a%2520pose-aware%2520scene%2520partitioning%250Astrategy.%2520It%2520ensures%2520that%2520only%2520the%2520most%2520relevant%2520NeRF%2520sub-block%2520generates%2520key%250Afeatures%2520for%2520a%2520specific%2520pose.%2520Additionally%252C%2520scene%2520segmentation%2520and%2520a%2520place%250Apredictor%2520provide%2520fast%2520coarse%2520initial%2520pose%2520estimation.%2520Evaluations%2520on%2520public%250Alarge-scale%2520datasets%2520demonstrate%2520that%2520MatLoc-NeRF%2520achieves%2520superior%2520efficiency%250Aand%2520accuracy%2520compared%2520to%2520existing%2520NeRF-based%2520localization%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11766v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Matching%20Query%20Image%20Against%20Selected%20NeRF%20Feature%20for%20Efficient%20and%0A%20%20Scalable%20Localization&entry.906535625=Huaiji%20Zhou%20and%20Bing%20Wang%20and%20Changhao%20Chen&entry.1292438233=%20%20Neural%20implicit%20representations%20such%20as%20NeRF%20have%20revolutionized%203D%20scene%0Arepresentation%20with%20photo-realistic%20quality.%20However%2C%20existing%20methods%20for%0Avisual%20localization%20within%20NeRF%20representations%20suffer%20from%20inefficiency%20and%0Ascalability%20issues%2C%20particularly%20in%20large-scale%20environments.%20This%20work%0Aproposes%20MatLoc-NeRF%2C%20a%20novel%20matching-based%20localization%20framework%20using%0Aselected%20NeRF%20features.%20It%20addresses%20efficiency%20by%20employing%20a%20learnable%0Afeature%20selection%20mechanism%20that%20identifies%20informative%20NeRF%20features%20for%0Amatching%20with%20query%20images.%20This%20eliminates%20the%20need%20for%20all%20NeRF%20features%20or%0Aadditional%20descriptors%2C%20leading%20to%20faster%20and%20more%20accurate%20pose%20estimation.%20To%0Atackle%20large-scale%20scenes%2C%20MatLoc-NeRF%20utilizes%20a%20pose-aware%20scene%20partitioning%0Astrategy.%20It%20ensures%20that%20only%20the%20most%20relevant%20NeRF%20sub-block%20generates%20key%0Afeatures%20for%20a%20specific%20pose.%20Additionally%2C%20scene%20segmentation%20and%20a%20place%0Apredictor%20provide%20fast%20coarse%20initial%20pose%20estimation.%20Evaluations%20on%20public%0Alarge-scale%20datasets%20demonstrate%20that%20MatLoc-NeRF%20achieves%20superior%20efficiency%0Aand%20accuracy%20compared%20to%20existing%20NeRF-based%20localization%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11766v1&entry.124074799=Read"},
{"title": "Human Mesh Recovery from Arbitrary Multi-view Images", "author": "Xiaoben Li and Mancheng Meng and Ziyan Wu and Terrence Chen and Fan Yang and Dinggang Shen", "abstract": "  Human mesh recovery from arbitrary multi-view images involves two\ncharacteristics: the arbitrary camera poses and arbitrary number of camera\nviews. Because of the variability, designing a unified framework to tackle this\ntask is challenging. The challenges can be summarized as the dilemma of being\nable to simultaneously estimate arbitrary camera poses and recover human mesh\nfrom arbitrary multi-view images while maintaining flexibility. To solve this\ndilemma, we propose a divide and conquer framework for Unified Human Mesh\nRecovery (U-HMR) from arbitrary multi-view images. In particular, U-HMR\nconsists of a decoupled structure and two main components: camera and body\ndecoupling (CBD), camera pose estimation (CPE), and arbitrary view fusion\n(AVF). As camera poses and human body mesh are independent of each other, CBD\nsplits the estimation of them into two sub-tasks for two individual\nsub-networks (ie, CPE and AVF) to handle respectively, thus the two sub-tasks\nare disentangled. In CPE, since each camera pose is unrelated to the others, we\nadopt a shared MLP to process all views in a parallel way. In AVF, in order to\nfuse multi-view information and make the fusion operation independent of the\nnumber of views, we introduce a transformer decoder with a SMPL parameters\nquery token to extract cross-view features for mesh recovery. To demonstrate\nthe efficacy and flexibility of the proposed framework and effect of each\ncomponent, we conduct extensive experiments on three public datasets:\nHuman3.6M, MPI-INF-3DHP, and TotalCapture.\n", "link": "http://arxiv.org/abs/2403.12434v4", "date": "2024-06-17", "relevancy": 3.0266, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6134}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6013}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Human%20Mesh%20Recovery%20from%20Arbitrary%20Multi-view%20Images&body=Title%3A%20Human%20Mesh%20Recovery%20from%20Arbitrary%20Multi-view%20Images%0AAuthor%3A%20Xiaoben%20Li%20and%20Mancheng%20Meng%20and%20Ziyan%20Wu%20and%20Terrence%20Chen%20and%20Fan%20Yang%20and%20Dinggang%20Shen%0AAbstract%3A%20%20%20Human%20mesh%20recovery%20from%20arbitrary%20multi-view%20images%20involves%20two%0Acharacteristics%3A%20the%20arbitrary%20camera%20poses%20and%20arbitrary%20number%20of%20camera%0Aviews.%20Because%20of%20the%20variability%2C%20designing%20a%20unified%20framework%20to%20tackle%20this%0Atask%20is%20challenging.%20The%20challenges%20can%20be%20summarized%20as%20the%20dilemma%20of%20being%0Aable%20to%20simultaneously%20estimate%20arbitrary%20camera%20poses%20and%20recover%20human%20mesh%0Afrom%20arbitrary%20multi-view%20images%20while%20maintaining%20flexibility.%20To%20solve%20this%0Adilemma%2C%20we%20propose%20a%20divide%20and%20conquer%20framework%20for%20Unified%20Human%20Mesh%0ARecovery%20%28U-HMR%29%20from%20arbitrary%20multi-view%20images.%20In%20particular%2C%20U-HMR%0Aconsists%20of%20a%20decoupled%20structure%20and%20two%20main%20components%3A%20camera%20and%20body%0Adecoupling%20%28CBD%29%2C%20camera%20pose%20estimation%20%28CPE%29%2C%20and%20arbitrary%20view%20fusion%0A%28AVF%29.%20As%20camera%20poses%20and%20human%20body%20mesh%20are%20independent%20of%20each%20other%2C%20CBD%0Asplits%20the%20estimation%20of%20them%20into%20two%20sub-tasks%20for%20two%20individual%0Asub-networks%20%28ie%2C%20CPE%20and%20AVF%29%20to%20handle%20respectively%2C%20thus%20the%20two%20sub-tasks%0Aare%20disentangled.%20In%20CPE%2C%20since%20each%20camera%20pose%20is%20unrelated%20to%20the%20others%2C%20we%0Aadopt%20a%20shared%20MLP%20to%20process%20all%20views%20in%20a%20parallel%20way.%20In%20AVF%2C%20in%20order%20to%0Afuse%20multi-view%20information%20and%20make%20the%20fusion%20operation%20independent%20of%20the%0Anumber%20of%20views%2C%20we%20introduce%20a%20transformer%20decoder%20with%20a%20SMPL%20parameters%0Aquery%20token%20to%20extract%20cross-view%20features%20for%20mesh%20recovery.%20To%20demonstrate%0Athe%20efficacy%20and%20flexibility%20of%20the%20proposed%20framework%20and%20effect%20of%20each%0Acomponent%2C%20we%20conduct%20extensive%20experiments%20on%20three%20public%20datasets%3A%0AHuman3.6M%2C%20MPI-INF-3DHP%2C%20and%20TotalCapture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12434v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHuman%2520Mesh%2520Recovery%2520from%2520Arbitrary%2520Multi-view%2520Images%26entry.906535625%3DXiaoben%2520Li%2520and%2520Mancheng%2520Meng%2520and%2520Ziyan%2520Wu%2520and%2520Terrence%2520Chen%2520and%2520Fan%2520Yang%2520and%2520Dinggang%2520Shen%26entry.1292438233%3D%2520%2520Human%2520mesh%2520recovery%2520from%2520arbitrary%2520multi-view%2520images%2520involves%2520two%250Acharacteristics%253A%2520the%2520arbitrary%2520camera%2520poses%2520and%2520arbitrary%2520number%2520of%2520camera%250Aviews.%2520Because%2520of%2520the%2520variability%252C%2520designing%2520a%2520unified%2520framework%2520to%2520tackle%2520this%250Atask%2520is%2520challenging.%2520The%2520challenges%2520can%2520be%2520summarized%2520as%2520the%2520dilemma%2520of%2520being%250Aable%2520to%2520simultaneously%2520estimate%2520arbitrary%2520camera%2520poses%2520and%2520recover%2520human%2520mesh%250Afrom%2520arbitrary%2520multi-view%2520images%2520while%2520maintaining%2520flexibility.%2520To%2520solve%2520this%250Adilemma%252C%2520we%2520propose%2520a%2520divide%2520and%2520conquer%2520framework%2520for%2520Unified%2520Human%2520Mesh%250ARecovery%2520%2528U-HMR%2529%2520from%2520arbitrary%2520multi-view%2520images.%2520In%2520particular%252C%2520U-HMR%250Aconsists%2520of%2520a%2520decoupled%2520structure%2520and%2520two%2520main%2520components%253A%2520camera%2520and%2520body%250Adecoupling%2520%2528CBD%2529%252C%2520camera%2520pose%2520estimation%2520%2528CPE%2529%252C%2520and%2520arbitrary%2520view%2520fusion%250A%2528AVF%2529.%2520As%2520camera%2520poses%2520and%2520human%2520body%2520mesh%2520are%2520independent%2520of%2520each%2520other%252C%2520CBD%250Asplits%2520the%2520estimation%2520of%2520them%2520into%2520two%2520sub-tasks%2520for%2520two%2520individual%250Asub-networks%2520%2528ie%252C%2520CPE%2520and%2520AVF%2529%2520to%2520handle%2520respectively%252C%2520thus%2520the%2520two%2520sub-tasks%250Aare%2520disentangled.%2520In%2520CPE%252C%2520since%2520each%2520camera%2520pose%2520is%2520unrelated%2520to%2520the%2520others%252C%2520we%250Aadopt%2520a%2520shared%2520MLP%2520to%2520process%2520all%2520views%2520in%2520a%2520parallel%2520way.%2520In%2520AVF%252C%2520in%2520order%2520to%250Afuse%2520multi-view%2520information%2520and%2520make%2520the%2520fusion%2520operation%2520independent%2520of%2520the%250Anumber%2520of%2520views%252C%2520we%2520introduce%2520a%2520transformer%2520decoder%2520with%2520a%2520SMPL%2520parameters%250Aquery%2520token%2520to%2520extract%2520cross-view%2520features%2520for%2520mesh%2520recovery.%2520To%2520demonstrate%250Athe%2520efficacy%2520and%2520flexibility%2520of%2520the%2520proposed%2520framework%2520and%2520effect%2520of%2520each%250Acomponent%252C%2520we%2520conduct%2520extensive%2520experiments%2520on%2520three%2520public%2520datasets%253A%250AHuman3.6M%252C%2520MPI-INF-3DHP%252C%2520and%2520TotalCapture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12434v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Human%20Mesh%20Recovery%20from%20Arbitrary%20Multi-view%20Images&entry.906535625=Xiaoben%20Li%20and%20Mancheng%20Meng%20and%20Ziyan%20Wu%20and%20Terrence%20Chen%20and%20Fan%20Yang%20and%20Dinggang%20Shen&entry.1292438233=%20%20Human%20mesh%20recovery%20from%20arbitrary%20multi-view%20images%20involves%20two%0Acharacteristics%3A%20the%20arbitrary%20camera%20poses%20and%20arbitrary%20number%20of%20camera%0Aviews.%20Because%20of%20the%20variability%2C%20designing%20a%20unified%20framework%20to%20tackle%20this%0Atask%20is%20challenging.%20The%20challenges%20can%20be%20summarized%20as%20the%20dilemma%20of%20being%0Aable%20to%20simultaneously%20estimate%20arbitrary%20camera%20poses%20and%20recover%20human%20mesh%0Afrom%20arbitrary%20multi-view%20images%20while%20maintaining%20flexibility.%20To%20solve%20this%0Adilemma%2C%20we%20propose%20a%20divide%20and%20conquer%20framework%20for%20Unified%20Human%20Mesh%0ARecovery%20%28U-HMR%29%20from%20arbitrary%20multi-view%20images.%20In%20particular%2C%20U-HMR%0Aconsists%20of%20a%20decoupled%20structure%20and%20two%20main%20components%3A%20camera%20and%20body%0Adecoupling%20%28CBD%29%2C%20camera%20pose%20estimation%20%28CPE%29%2C%20and%20arbitrary%20view%20fusion%0A%28AVF%29.%20As%20camera%20poses%20and%20human%20body%20mesh%20are%20independent%20of%20each%20other%2C%20CBD%0Asplits%20the%20estimation%20of%20them%20into%20two%20sub-tasks%20for%20two%20individual%0Asub-networks%20%28ie%2C%20CPE%20and%20AVF%29%20to%20handle%20respectively%2C%20thus%20the%20two%20sub-tasks%0Aare%20disentangled.%20In%20CPE%2C%20since%20each%20camera%20pose%20is%20unrelated%20to%20the%20others%2C%20we%0Aadopt%20a%20shared%20MLP%20to%20process%20all%20views%20in%20a%20parallel%20way.%20In%20AVF%2C%20in%20order%20to%0Afuse%20multi-view%20information%20and%20make%20the%20fusion%20operation%20independent%20of%20the%0Anumber%20of%20views%2C%20we%20introduce%20a%20transformer%20decoder%20with%20a%20SMPL%20parameters%0Aquery%20token%20to%20extract%20cross-view%20features%20for%20mesh%20recovery.%20To%20demonstrate%0Athe%20efficacy%20and%20flexibility%20of%20the%20proposed%20framework%20and%20effect%20of%20each%0Acomponent%2C%20we%20conduct%20extensive%20experiments%20on%20three%20public%20datasets%3A%0AHuman3.6M%2C%20MPI-INF-3DHP%2C%20and%20TotalCapture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12434v4&entry.124074799=Read"},
{"title": "TetSphere Splatting: Representing High-Quality Geometry with Lagrangian\n  Volumetric Meshes", "author": "Minghao Guo and Bohan Wang and Kaiming He and Wojciech Matusik", "abstract": "  We present TetSphere splatting, an explicit, Lagrangian representation for\nreconstructing 3D shapes with high-quality geometry. In contrast to\nconventional object reconstruction methods which predominantly use Eulerian\nrepresentations, including both neural implicit (e.g., NeRF, NeuS) and explicit\nrepresentations (e.g., DMTet), and often struggle with high computational\ndemands and suboptimal mesh quality, TetSphere splatting utilizes an underused\nbut highly effective geometric primitive -- tetrahedral meshes. This approach\ndirectly yields superior mesh quality without relying on neural networks or\npost-processing. It deforms multiple initial tetrahedral spheres to accurately\nreconstruct the 3D shape through a combination of differentiable rendering and\ngeometric energy optimization, resulting in significant computational\nefficiency. Serving as a robust and versatile geometry representation,\nTet-Sphere splatting seamlessly integrates into diverse applications, including\nsingle-view 3D reconstruction, image-/text-to-3D content generation.\nExperimental results demonstrate that TetSphere splatting outperforms existing\nrepresentations, delivering faster optimization speed, enhanced mesh quality,\nand reliable preservation of thin structures.\n", "link": "http://arxiv.org/abs/2405.20283v2", "date": "2024-06-17", "relevancy": 2.9648, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6399}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6106}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5284}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TetSphere%20Splatting%3A%20Representing%20High-Quality%20Geometry%20with%20Lagrangian%0A%20%20Volumetric%20Meshes&body=Title%3A%20TetSphere%20Splatting%3A%20Representing%20High-Quality%20Geometry%20with%20Lagrangian%0A%20%20Volumetric%20Meshes%0AAuthor%3A%20Minghao%20Guo%20and%20Bohan%20Wang%20and%20Kaiming%20He%20and%20Wojciech%20Matusik%0AAbstract%3A%20%20%20We%20present%20TetSphere%20splatting%2C%20an%20explicit%2C%20Lagrangian%20representation%20for%0Areconstructing%203D%20shapes%20with%20high-quality%20geometry.%20In%20contrast%20to%0Aconventional%20object%20reconstruction%20methods%20which%20predominantly%20use%20Eulerian%0Arepresentations%2C%20including%20both%20neural%20implicit%20%28e.g.%2C%20NeRF%2C%20NeuS%29%20and%20explicit%0Arepresentations%20%28e.g.%2C%20DMTet%29%2C%20and%20often%20struggle%20with%20high%20computational%0Ademands%20and%20suboptimal%20mesh%20quality%2C%20TetSphere%20splatting%20utilizes%20an%20underused%0Abut%20highly%20effective%20geometric%20primitive%20--%20tetrahedral%20meshes.%20This%20approach%0Adirectly%20yields%20superior%20mesh%20quality%20without%20relying%20on%20neural%20networks%20or%0Apost-processing.%20It%20deforms%20multiple%20initial%20tetrahedral%20spheres%20to%20accurately%0Areconstruct%20the%203D%20shape%20through%20a%20combination%20of%20differentiable%20rendering%20and%0Ageometric%20energy%20optimization%2C%20resulting%20in%20significant%20computational%0Aefficiency.%20Serving%20as%20a%20robust%20and%20versatile%20geometry%20representation%2C%0ATet-Sphere%20splatting%20seamlessly%20integrates%20into%20diverse%20applications%2C%20including%0Asingle-view%203D%20reconstruction%2C%20image-/text-to-3D%20content%20generation.%0AExperimental%20results%20demonstrate%20that%20TetSphere%20splatting%20outperforms%20existing%0Arepresentations%2C%20delivering%20faster%20optimization%20speed%2C%20enhanced%20mesh%20quality%2C%0Aand%20reliable%20preservation%20of%20thin%20structures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20283v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTetSphere%2520Splatting%253A%2520Representing%2520High-Quality%2520Geometry%2520with%2520Lagrangian%250A%2520%2520Volumetric%2520Meshes%26entry.906535625%3DMinghao%2520Guo%2520and%2520Bohan%2520Wang%2520and%2520Kaiming%2520He%2520and%2520Wojciech%2520Matusik%26entry.1292438233%3D%2520%2520We%2520present%2520TetSphere%2520splatting%252C%2520an%2520explicit%252C%2520Lagrangian%2520representation%2520for%250Areconstructing%25203D%2520shapes%2520with%2520high-quality%2520geometry.%2520In%2520contrast%2520to%250Aconventional%2520object%2520reconstruction%2520methods%2520which%2520predominantly%2520use%2520Eulerian%250Arepresentations%252C%2520including%2520both%2520neural%2520implicit%2520%2528e.g.%252C%2520NeRF%252C%2520NeuS%2529%2520and%2520explicit%250Arepresentations%2520%2528e.g.%252C%2520DMTet%2529%252C%2520and%2520often%2520struggle%2520with%2520high%2520computational%250Ademands%2520and%2520suboptimal%2520mesh%2520quality%252C%2520TetSphere%2520splatting%2520utilizes%2520an%2520underused%250Abut%2520highly%2520effective%2520geometric%2520primitive%2520--%2520tetrahedral%2520meshes.%2520This%2520approach%250Adirectly%2520yields%2520superior%2520mesh%2520quality%2520without%2520relying%2520on%2520neural%2520networks%2520or%250Apost-processing.%2520It%2520deforms%2520multiple%2520initial%2520tetrahedral%2520spheres%2520to%2520accurately%250Areconstruct%2520the%25203D%2520shape%2520through%2520a%2520combination%2520of%2520differentiable%2520rendering%2520and%250Ageometric%2520energy%2520optimization%252C%2520resulting%2520in%2520significant%2520computational%250Aefficiency.%2520Serving%2520as%2520a%2520robust%2520and%2520versatile%2520geometry%2520representation%252C%250ATet-Sphere%2520splatting%2520seamlessly%2520integrates%2520into%2520diverse%2520applications%252C%2520including%250Asingle-view%25203D%2520reconstruction%252C%2520image-/text-to-3D%2520content%2520generation.%250AExperimental%2520results%2520demonstrate%2520that%2520TetSphere%2520splatting%2520outperforms%2520existing%250Arepresentations%252C%2520delivering%2520faster%2520optimization%2520speed%252C%2520enhanced%2520mesh%2520quality%252C%250Aand%2520reliable%2520preservation%2520of%2520thin%2520structures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20283v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TetSphere%20Splatting%3A%20Representing%20High-Quality%20Geometry%20with%20Lagrangian%0A%20%20Volumetric%20Meshes&entry.906535625=Minghao%20Guo%20and%20Bohan%20Wang%20and%20Kaiming%20He%20and%20Wojciech%20Matusik&entry.1292438233=%20%20We%20present%20TetSphere%20splatting%2C%20an%20explicit%2C%20Lagrangian%20representation%20for%0Areconstructing%203D%20shapes%20with%20high-quality%20geometry.%20In%20contrast%20to%0Aconventional%20object%20reconstruction%20methods%20which%20predominantly%20use%20Eulerian%0Arepresentations%2C%20including%20both%20neural%20implicit%20%28e.g.%2C%20NeRF%2C%20NeuS%29%20and%20explicit%0Arepresentations%20%28e.g.%2C%20DMTet%29%2C%20and%20often%20struggle%20with%20high%20computational%0Ademands%20and%20suboptimal%20mesh%20quality%2C%20TetSphere%20splatting%20utilizes%20an%20underused%0Abut%20highly%20effective%20geometric%20primitive%20--%20tetrahedral%20meshes.%20This%20approach%0Adirectly%20yields%20superior%20mesh%20quality%20without%20relying%20on%20neural%20networks%20or%0Apost-processing.%20It%20deforms%20multiple%20initial%20tetrahedral%20spheres%20to%20accurately%0Areconstruct%20the%203D%20shape%20through%20a%20combination%20of%20differentiable%20rendering%20and%0Ageometric%20energy%20optimization%2C%20resulting%20in%20significant%20computational%0Aefficiency.%20Serving%20as%20a%20robust%20and%20versatile%20geometry%20representation%2C%0ATet-Sphere%20splatting%20seamlessly%20integrates%20into%20diverse%20applications%2C%20including%0Asingle-view%203D%20reconstruction%2C%20image-/text-to-3D%20content%20generation.%0AExperimental%20results%20demonstrate%20that%20TetSphere%20splatting%20outperforms%20existing%0Arepresentations%2C%20delivering%20faster%20optimization%20speed%2C%20enhanced%20mesh%20quality%2C%0Aand%20reliable%20preservation%20of%20thin%20structures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20283v2&entry.124074799=Read"},
{"title": "RoMe: Towards Large Scale Road Surface Reconstruction via Mesh\n  Representation", "author": "Ruohong Mei and Wei Sui and Jiaxin Zhang and Xue Qin and Gang Wang and Tao Peng and Cong Yang", "abstract": "  In autonomous driving applications, accurate and efficient road surface\nreconstruction is paramount. This paper introduces RoMe, a novel framework\ndesigned for the robust reconstruction of large-scale road surfaces. Leveraging\na unique mesh representation, RoMe ensures that the reconstructed road surfaces\nare accurate and seamlessly aligned with semantics. To address challenges in\ncomputational efficiency, we propose a waypoint sampling strategy, enabling\nRoMe to reconstruct vast environments by focusing on sub-areas and subsequently\nmerging them. Furthermore, we incorporate an extrinsic optimization module to\nenhance the robustness against inaccuracies in extrinsic calibration. Our\nextensive evaluations of both public datasets and wild data underscore RoMe's\nsuperiority in terms of speed, accuracy, and robustness. For instance, it costs\nonly 2 GPU hours to recover a road surface of 600*600 square meters from\nthousands of images. Notably, RoMe's capability extends beyond mere\nreconstruction, offering significant value for autolabeling tasks in autonomous\ndriving applications. All related data and code are available at\nhttps://github.com/DRosemei/RoMe.\n", "link": "http://arxiv.org/abs/2306.11368v3", "date": "2024-06-17", "relevancy": 2.8305, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6007}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5741}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5235}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RoMe%3A%20Towards%20Large%20Scale%20Road%20Surface%20Reconstruction%20via%20Mesh%0A%20%20Representation&body=Title%3A%20RoMe%3A%20Towards%20Large%20Scale%20Road%20Surface%20Reconstruction%20via%20Mesh%0A%20%20Representation%0AAuthor%3A%20Ruohong%20Mei%20and%20Wei%20Sui%20and%20Jiaxin%20Zhang%20and%20Xue%20Qin%20and%20Gang%20Wang%20and%20Tao%20Peng%20and%20Cong%20Yang%0AAbstract%3A%20%20%20In%20autonomous%20driving%20applications%2C%20accurate%20and%20efficient%20road%20surface%0Areconstruction%20is%20paramount.%20This%20paper%20introduces%20RoMe%2C%20a%20novel%20framework%0Adesigned%20for%20the%20robust%20reconstruction%20of%20large-scale%20road%20surfaces.%20Leveraging%0Aa%20unique%20mesh%20representation%2C%20RoMe%20ensures%20that%20the%20reconstructed%20road%20surfaces%0Aare%20accurate%20and%20seamlessly%20aligned%20with%20semantics.%20To%20address%20challenges%20in%0Acomputational%20efficiency%2C%20we%20propose%20a%20waypoint%20sampling%20strategy%2C%20enabling%0ARoMe%20to%20reconstruct%20vast%20environments%20by%20focusing%20on%20sub-areas%20and%20subsequently%0Amerging%20them.%20Furthermore%2C%20we%20incorporate%20an%20extrinsic%20optimization%20module%20to%0Aenhance%20the%20robustness%20against%20inaccuracies%20in%20extrinsic%20calibration.%20Our%0Aextensive%20evaluations%20of%20both%20public%20datasets%20and%20wild%20data%20underscore%20RoMe%27s%0Asuperiority%20in%20terms%20of%20speed%2C%20accuracy%2C%20and%20robustness.%20For%20instance%2C%20it%20costs%0Aonly%202%20GPU%20hours%20to%20recover%20a%20road%20surface%20of%20600%2A600%20square%20meters%20from%0Athousands%20of%20images.%20Notably%2C%20RoMe%27s%20capability%20extends%20beyond%20mere%0Areconstruction%2C%20offering%20significant%20value%20for%20autolabeling%20tasks%20in%20autonomous%0Adriving%20applications.%20All%20related%20data%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/DRosemei/RoMe.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.11368v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRoMe%253A%2520Towards%2520Large%2520Scale%2520Road%2520Surface%2520Reconstruction%2520via%2520Mesh%250A%2520%2520Representation%26entry.906535625%3DRuohong%2520Mei%2520and%2520Wei%2520Sui%2520and%2520Jiaxin%2520Zhang%2520and%2520Xue%2520Qin%2520and%2520Gang%2520Wang%2520and%2520Tao%2520Peng%2520and%2520Cong%2520Yang%26entry.1292438233%3D%2520%2520In%2520autonomous%2520driving%2520applications%252C%2520accurate%2520and%2520efficient%2520road%2520surface%250Areconstruction%2520is%2520paramount.%2520This%2520paper%2520introduces%2520RoMe%252C%2520a%2520novel%2520framework%250Adesigned%2520for%2520the%2520robust%2520reconstruction%2520of%2520large-scale%2520road%2520surfaces.%2520Leveraging%250Aa%2520unique%2520mesh%2520representation%252C%2520RoMe%2520ensures%2520that%2520the%2520reconstructed%2520road%2520surfaces%250Aare%2520accurate%2520and%2520seamlessly%2520aligned%2520with%2520semantics.%2520To%2520address%2520challenges%2520in%250Acomputational%2520efficiency%252C%2520we%2520propose%2520a%2520waypoint%2520sampling%2520strategy%252C%2520enabling%250ARoMe%2520to%2520reconstruct%2520vast%2520environments%2520by%2520focusing%2520on%2520sub-areas%2520and%2520subsequently%250Amerging%2520them.%2520Furthermore%252C%2520we%2520incorporate%2520an%2520extrinsic%2520optimization%2520module%2520to%250Aenhance%2520the%2520robustness%2520against%2520inaccuracies%2520in%2520extrinsic%2520calibration.%2520Our%250Aextensive%2520evaluations%2520of%2520both%2520public%2520datasets%2520and%2520wild%2520data%2520underscore%2520RoMe%2527s%250Asuperiority%2520in%2520terms%2520of%2520speed%252C%2520accuracy%252C%2520and%2520robustness.%2520For%2520instance%252C%2520it%2520costs%250Aonly%25202%2520GPU%2520hours%2520to%2520recover%2520a%2520road%2520surface%2520of%2520600%252A600%2520square%2520meters%2520from%250Athousands%2520of%2520images.%2520Notably%252C%2520RoMe%2527s%2520capability%2520extends%2520beyond%2520mere%250Areconstruction%252C%2520offering%2520significant%2520value%2520for%2520autolabeling%2520tasks%2520in%2520autonomous%250Adriving%2520applications.%2520All%2520related%2520data%2520and%2520code%2520are%2520available%2520at%250Ahttps%253A//github.com/DRosemei/RoMe.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.11368v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoMe%3A%20Towards%20Large%20Scale%20Road%20Surface%20Reconstruction%20via%20Mesh%0A%20%20Representation&entry.906535625=Ruohong%20Mei%20and%20Wei%20Sui%20and%20Jiaxin%20Zhang%20and%20Xue%20Qin%20and%20Gang%20Wang%20and%20Tao%20Peng%20and%20Cong%20Yang&entry.1292438233=%20%20In%20autonomous%20driving%20applications%2C%20accurate%20and%20efficient%20road%20surface%0Areconstruction%20is%20paramount.%20This%20paper%20introduces%20RoMe%2C%20a%20novel%20framework%0Adesigned%20for%20the%20robust%20reconstruction%20of%20large-scale%20road%20surfaces.%20Leveraging%0Aa%20unique%20mesh%20representation%2C%20RoMe%20ensures%20that%20the%20reconstructed%20road%20surfaces%0Aare%20accurate%20and%20seamlessly%20aligned%20with%20semantics.%20To%20address%20challenges%20in%0Acomputational%20efficiency%2C%20we%20propose%20a%20waypoint%20sampling%20strategy%2C%20enabling%0ARoMe%20to%20reconstruct%20vast%20environments%20by%20focusing%20on%20sub-areas%20and%20subsequently%0Amerging%20them.%20Furthermore%2C%20we%20incorporate%20an%20extrinsic%20optimization%20module%20to%0Aenhance%20the%20robustness%20against%20inaccuracies%20in%20extrinsic%20calibration.%20Our%0Aextensive%20evaluations%20of%20both%20public%20datasets%20and%20wild%20data%20underscore%20RoMe%27s%0Asuperiority%20in%20terms%20of%20speed%2C%20accuracy%2C%20and%20robustness.%20For%20instance%2C%20it%20costs%0Aonly%202%20GPU%20hours%20to%20recover%20a%20road%20surface%20of%20600%2A600%20square%20meters%20from%0Athousands%20of%20images.%20Notably%2C%20RoMe%27s%20capability%20extends%20beyond%20mere%0Areconstruction%2C%20offering%20significant%20value%20for%20autolabeling%20tasks%20in%20autonomous%0Adriving%20applications.%20All%20related%20data%20and%20code%20are%20available%20at%0Ahttps%3A//github.com/DRosemei/RoMe.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.11368v3&entry.124074799=Read"},
{"title": "Simple Yet Efficient: Towards Self-Supervised FG-SBIR with Unified\n  Sample Feature Alignment", "author": "Jianan Jiang and Di Wu and Zhilin Jiang and Weiren Yu", "abstract": "  Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) aims to minimize the\ndistance between sketches and corresponding images in the embedding space.\nHowever, scalability is hindered by the growing complexity of solutions, mainly\ndue to the abstract nature of fine-grained sketches. In this paper, we propose\na simple yet efficient approach to narrow the gap between the two modes. It\nmainly facilitates unified mutual information sharing both intra- and\ninter-samples, rather than treating them as a single feature alignment problem\nbetween modalities. Specifically, our approach includes: (i) Employing dual\nweight-sharing networks to optimize alignment within sketch and image domain,\nwhich also effectively mitigates model learning saturation issues. (ii)\nIntroducing an objective optimization function based on contrastive loss to\nenhance the model's ability to align features intra- and inter-samples. (iii)\nPresenting a learnable TRSM combined of self-attention and cross-attention to\npromote feature representations among tokens, further enhancing sample\nalignment in the embedding space. Our framework achieves excellent results on\nCNN- and ViT-based backbones. Extensive experiments demonstrate its superiority\nover existing methods. We also introduce Cloths-V1, the first professional\nfashion sketches and images dataset, utilized to validate our method and will\nbe beneficial for other applications.\n", "link": "http://arxiv.org/abs/2406.11551v1", "date": "2024-06-17", "relevancy": 2.8202, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5949}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5593}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simple%20Yet%20Efficient%3A%20Towards%20Self-Supervised%20FG-SBIR%20with%20Unified%0A%20%20Sample%20Feature%20Alignment&body=Title%3A%20Simple%20Yet%20Efficient%3A%20Towards%20Self-Supervised%20FG-SBIR%20with%20Unified%0A%20%20Sample%20Feature%20Alignment%0AAuthor%3A%20Jianan%20Jiang%20and%20Di%20Wu%20and%20Zhilin%20Jiang%20and%20Weiren%20Yu%0AAbstract%3A%20%20%20Fine-Grained%20Sketch-Based%20Image%20Retrieval%20%28FG-SBIR%29%20aims%20to%20minimize%20the%0Adistance%20between%20sketches%20and%20corresponding%20images%20in%20the%20embedding%20space.%0AHowever%2C%20scalability%20is%20hindered%20by%20the%20growing%20complexity%20of%20solutions%2C%20mainly%0Adue%20to%20the%20abstract%20nature%20of%20fine-grained%20sketches.%20In%20this%20paper%2C%20we%20propose%0Aa%20simple%20yet%20efficient%20approach%20to%20narrow%20the%20gap%20between%20the%20two%20modes.%20It%0Amainly%20facilitates%20unified%20mutual%20information%20sharing%20both%20intra-%20and%0Ainter-samples%2C%20rather%20than%20treating%20them%20as%20a%20single%20feature%20alignment%20problem%0Abetween%20modalities.%20Specifically%2C%20our%20approach%20includes%3A%20%28i%29%20Employing%20dual%0Aweight-sharing%20networks%20to%20optimize%20alignment%20within%20sketch%20and%20image%20domain%2C%0Awhich%20also%20effectively%20mitigates%20model%20learning%20saturation%20issues.%20%28ii%29%0AIntroducing%20an%20objective%20optimization%20function%20based%20on%20contrastive%20loss%20to%0Aenhance%20the%20model%27s%20ability%20to%20align%20features%20intra-%20and%20inter-samples.%20%28iii%29%0APresenting%20a%20learnable%20TRSM%20combined%20of%20self-attention%20and%20cross-attention%20to%0Apromote%20feature%20representations%20among%20tokens%2C%20further%20enhancing%20sample%0Aalignment%20in%20the%20embedding%20space.%20Our%20framework%20achieves%20excellent%20results%20on%0ACNN-%20and%20ViT-based%20backbones.%20Extensive%20experiments%20demonstrate%20its%20superiority%0Aover%20existing%20methods.%20We%20also%20introduce%20Cloths-V1%2C%20the%20first%20professional%0Afashion%20sketches%20and%20images%20dataset%2C%20utilized%20to%20validate%20our%20method%20and%20will%0Abe%20beneficial%20for%20other%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11551v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimple%2520Yet%2520Efficient%253A%2520Towards%2520Self-Supervised%2520FG-SBIR%2520with%2520Unified%250A%2520%2520Sample%2520Feature%2520Alignment%26entry.906535625%3DJianan%2520Jiang%2520and%2520Di%2520Wu%2520and%2520Zhilin%2520Jiang%2520and%2520Weiren%2520Yu%26entry.1292438233%3D%2520%2520Fine-Grained%2520Sketch-Based%2520Image%2520Retrieval%2520%2528FG-SBIR%2529%2520aims%2520to%2520minimize%2520the%250Adistance%2520between%2520sketches%2520and%2520corresponding%2520images%2520in%2520the%2520embedding%2520space.%250AHowever%252C%2520scalability%2520is%2520hindered%2520by%2520the%2520growing%2520complexity%2520of%2520solutions%252C%2520mainly%250Adue%2520to%2520the%2520abstract%2520nature%2520of%2520fine-grained%2520sketches.%2520In%2520this%2520paper%252C%2520we%2520propose%250Aa%2520simple%2520yet%2520efficient%2520approach%2520to%2520narrow%2520the%2520gap%2520between%2520the%2520two%2520modes.%2520It%250Amainly%2520facilitates%2520unified%2520mutual%2520information%2520sharing%2520both%2520intra-%2520and%250Ainter-samples%252C%2520rather%2520than%2520treating%2520them%2520as%2520a%2520single%2520feature%2520alignment%2520problem%250Abetween%2520modalities.%2520Specifically%252C%2520our%2520approach%2520includes%253A%2520%2528i%2529%2520Employing%2520dual%250Aweight-sharing%2520networks%2520to%2520optimize%2520alignment%2520within%2520sketch%2520and%2520image%2520domain%252C%250Awhich%2520also%2520effectively%2520mitigates%2520model%2520learning%2520saturation%2520issues.%2520%2528ii%2529%250AIntroducing%2520an%2520objective%2520optimization%2520function%2520based%2520on%2520contrastive%2520loss%2520to%250Aenhance%2520the%2520model%2527s%2520ability%2520to%2520align%2520features%2520intra-%2520and%2520inter-samples.%2520%2528iii%2529%250APresenting%2520a%2520learnable%2520TRSM%2520combined%2520of%2520self-attention%2520and%2520cross-attention%2520to%250Apromote%2520feature%2520representations%2520among%2520tokens%252C%2520further%2520enhancing%2520sample%250Aalignment%2520in%2520the%2520embedding%2520space.%2520Our%2520framework%2520achieves%2520excellent%2520results%2520on%250ACNN-%2520and%2520ViT-based%2520backbones.%2520Extensive%2520experiments%2520demonstrate%2520its%2520superiority%250Aover%2520existing%2520methods.%2520We%2520also%2520introduce%2520Cloths-V1%252C%2520the%2520first%2520professional%250Afashion%2520sketches%2520and%2520images%2520dataset%252C%2520utilized%2520to%2520validate%2520our%2520method%2520and%2520will%250Abe%2520beneficial%2520for%2520other%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11551v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simple%20Yet%20Efficient%3A%20Towards%20Self-Supervised%20FG-SBIR%20with%20Unified%0A%20%20Sample%20Feature%20Alignment&entry.906535625=Jianan%20Jiang%20and%20Di%20Wu%20and%20Zhilin%20Jiang%20and%20Weiren%20Yu&entry.1292438233=%20%20Fine-Grained%20Sketch-Based%20Image%20Retrieval%20%28FG-SBIR%29%20aims%20to%20minimize%20the%0Adistance%20between%20sketches%20and%20corresponding%20images%20in%20the%20embedding%20space.%0AHowever%2C%20scalability%20is%20hindered%20by%20the%20growing%20complexity%20of%20solutions%2C%20mainly%0Adue%20to%20the%20abstract%20nature%20of%20fine-grained%20sketches.%20In%20this%20paper%2C%20we%20propose%0Aa%20simple%20yet%20efficient%20approach%20to%20narrow%20the%20gap%20between%20the%20two%20modes.%20It%0Amainly%20facilitates%20unified%20mutual%20information%20sharing%20both%20intra-%20and%0Ainter-samples%2C%20rather%20than%20treating%20them%20as%20a%20single%20feature%20alignment%20problem%0Abetween%20modalities.%20Specifically%2C%20our%20approach%20includes%3A%20%28i%29%20Employing%20dual%0Aweight-sharing%20networks%20to%20optimize%20alignment%20within%20sketch%20and%20image%20domain%2C%0Awhich%20also%20effectively%20mitigates%20model%20learning%20saturation%20issues.%20%28ii%29%0AIntroducing%20an%20objective%20optimization%20function%20based%20on%20contrastive%20loss%20to%0Aenhance%20the%20model%27s%20ability%20to%20align%20features%20intra-%20and%20inter-samples.%20%28iii%29%0APresenting%20a%20learnable%20TRSM%20combined%20of%20self-attention%20and%20cross-attention%20to%0Apromote%20feature%20representations%20among%20tokens%2C%20further%20enhancing%20sample%0Aalignment%20in%20the%20embedding%20space.%20Our%20framework%20achieves%20excellent%20results%20on%0ACNN-%20and%20ViT-based%20backbones.%20Extensive%20experiments%20demonstrate%20its%20superiority%0Aover%20existing%20methods.%20We%20also%20introduce%20Cloths-V1%2C%20the%20first%20professional%0Afashion%20sketches%20and%20images%20dataset%2C%20utilized%20to%20validate%20our%20method%20and%20will%0Abe%20beneficial%20for%20other%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11551v1&entry.124074799=Read"},
{"title": "Domain Generalization for In-Orbit 6D Pose Estimation", "author": "Antoine Legrand and Renaud Detry and Christophe De Vleeschouwer", "abstract": "  We address the problem of estimating the relative 6D pose, i.e., position and\norientation, of a target spacecraft, from a monocular image, a key capability\nfor future autonomous Rendezvous and Proximity Operations. Due to the\ndifficulty of acquiring large sets of real images, spacecraft pose estimation\nnetworks are exclusively trained on synthetic ones. However, because those\nimages do not capture the illumination conditions encountered in orbit, pose\nestimation networks face a domain gap problem, i.e., they do not generalize to\nreal images. Our work introduces a method that bridges this domain gap. It\nrelies on a novel, end-to-end, neural-based architecture as well as a novel\nlearning strategy. This strategy improves the domain generalization abilities\nof the network through multi-task learning and aggressive data augmentation\npolicies, thereby enforcing the network to learn domain-invariant features. We\ndemonstrate that our method effectively closes the domain gap, achieving\nstate-of-the-art accuracy on the widespread SPEED+ dataset. Finally, ablation\nstudies assess the impact of key components of our method on its generalization\nabilities.\n", "link": "http://arxiv.org/abs/2406.11743v1", "date": "2024-06-17", "relevancy": 2.7845, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5903}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5455}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain%20Generalization%20for%20In-Orbit%206D%20Pose%20Estimation&body=Title%3A%20Domain%20Generalization%20for%20In-Orbit%206D%20Pose%20Estimation%0AAuthor%3A%20Antoine%20Legrand%20and%20Renaud%20Detry%20and%20Christophe%20De%20Vleeschouwer%0AAbstract%3A%20%20%20We%20address%20the%20problem%20of%20estimating%20the%20relative%206D%20pose%2C%20i.e.%2C%20position%20and%0Aorientation%2C%20of%20a%20target%20spacecraft%2C%20from%20a%20monocular%20image%2C%20a%20key%20capability%0Afor%20future%20autonomous%20Rendezvous%20and%20Proximity%20Operations.%20Due%20to%20the%0Adifficulty%20of%20acquiring%20large%20sets%20of%20real%20images%2C%20spacecraft%20pose%20estimation%0Anetworks%20are%20exclusively%20trained%20on%20synthetic%20ones.%20However%2C%20because%20those%0Aimages%20do%20not%20capture%20the%20illumination%20conditions%20encountered%20in%20orbit%2C%20pose%0Aestimation%20networks%20face%20a%20domain%20gap%20problem%2C%20i.e.%2C%20they%20do%20not%20generalize%20to%0Areal%20images.%20Our%20work%20introduces%20a%20method%20that%20bridges%20this%20domain%20gap.%20It%0Arelies%20on%20a%20novel%2C%20end-to-end%2C%20neural-based%20architecture%20as%20well%20as%20a%20novel%0Alearning%20strategy.%20This%20strategy%20improves%20the%20domain%20generalization%20abilities%0Aof%20the%20network%20through%20multi-task%20learning%20and%20aggressive%20data%20augmentation%0Apolicies%2C%20thereby%20enforcing%20the%20network%20to%20learn%20domain-invariant%20features.%20We%0Ademonstrate%20that%20our%20method%20effectively%20closes%20the%20domain%20gap%2C%20achieving%0Astate-of-the-art%20accuracy%20on%20the%20widespread%20SPEED%2B%20dataset.%20Finally%2C%20ablation%0Astudies%20assess%20the%20impact%20of%20key%20components%20of%20our%20method%20on%20its%20generalization%0Aabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11743v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain%2520Generalization%2520for%2520In-Orbit%25206D%2520Pose%2520Estimation%26entry.906535625%3DAntoine%2520Legrand%2520and%2520Renaud%2520Detry%2520and%2520Christophe%2520De%2520Vleeschouwer%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520problem%2520of%2520estimating%2520the%2520relative%25206D%2520pose%252C%2520i.e.%252C%2520position%2520and%250Aorientation%252C%2520of%2520a%2520target%2520spacecraft%252C%2520from%2520a%2520monocular%2520image%252C%2520a%2520key%2520capability%250Afor%2520future%2520autonomous%2520Rendezvous%2520and%2520Proximity%2520Operations.%2520Due%2520to%2520the%250Adifficulty%2520of%2520acquiring%2520large%2520sets%2520of%2520real%2520images%252C%2520spacecraft%2520pose%2520estimation%250Anetworks%2520are%2520exclusively%2520trained%2520on%2520synthetic%2520ones.%2520However%252C%2520because%2520those%250Aimages%2520do%2520not%2520capture%2520the%2520illumination%2520conditions%2520encountered%2520in%2520orbit%252C%2520pose%250Aestimation%2520networks%2520face%2520a%2520domain%2520gap%2520problem%252C%2520i.e.%252C%2520they%2520do%2520not%2520generalize%2520to%250Areal%2520images.%2520Our%2520work%2520introduces%2520a%2520method%2520that%2520bridges%2520this%2520domain%2520gap.%2520It%250Arelies%2520on%2520a%2520novel%252C%2520end-to-end%252C%2520neural-based%2520architecture%2520as%2520well%2520as%2520a%2520novel%250Alearning%2520strategy.%2520This%2520strategy%2520improves%2520the%2520domain%2520generalization%2520abilities%250Aof%2520the%2520network%2520through%2520multi-task%2520learning%2520and%2520aggressive%2520data%2520augmentation%250Apolicies%252C%2520thereby%2520enforcing%2520the%2520network%2520to%2520learn%2520domain-invariant%2520features.%2520We%250Ademonstrate%2520that%2520our%2520method%2520effectively%2520closes%2520the%2520domain%2520gap%252C%2520achieving%250Astate-of-the-art%2520accuracy%2520on%2520the%2520widespread%2520SPEED%252B%2520dataset.%2520Finally%252C%2520ablation%250Astudies%2520assess%2520the%2520impact%2520of%2520key%2520components%2520of%2520our%2520method%2520on%2520its%2520generalization%250Aabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11743v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain%20Generalization%20for%20In-Orbit%206D%20Pose%20Estimation&entry.906535625=Antoine%20Legrand%20and%20Renaud%20Detry%20and%20Christophe%20De%20Vleeschouwer&entry.1292438233=%20%20We%20address%20the%20problem%20of%20estimating%20the%20relative%206D%20pose%2C%20i.e.%2C%20position%20and%0Aorientation%2C%20of%20a%20target%20spacecraft%2C%20from%20a%20monocular%20image%2C%20a%20key%20capability%0Afor%20future%20autonomous%20Rendezvous%20and%20Proximity%20Operations.%20Due%20to%20the%0Adifficulty%20of%20acquiring%20large%20sets%20of%20real%20images%2C%20spacecraft%20pose%20estimation%0Anetworks%20are%20exclusively%20trained%20on%20synthetic%20ones.%20However%2C%20because%20those%0Aimages%20do%20not%20capture%20the%20illumination%20conditions%20encountered%20in%20orbit%2C%20pose%0Aestimation%20networks%20face%20a%20domain%20gap%20problem%2C%20i.e.%2C%20they%20do%20not%20generalize%20to%0Areal%20images.%20Our%20work%20introduces%20a%20method%20that%20bridges%20this%20domain%20gap.%20It%0Arelies%20on%20a%20novel%2C%20end-to-end%2C%20neural-based%20architecture%20as%20well%20as%20a%20novel%0Alearning%20strategy.%20This%20strategy%20improves%20the%20domain%20generalization%20abilities%0Aof%20the%20network%20through%20multi-task%20learning%20and%20aggressive%20data%20augmentation%0Apolicies%2C%20thereby%20enforcing%20the%20network%20to%20learn%20domain-invariant%20features.%20We%0Ademonstrate%20that%20our%20method%20effectively%20closes%20the%20domain%20gap%2C%20achieving%0Astate-of-the-art%20accuracy%20on%20the%20widespread%20SPEED%2B%20dataset.%20Finally%2C%20ablation%0Astudies%20assess%20the%20impact%20of%20key%20components%20of%20our%20method%20on%20its%20generalization%0Aabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11743v1&entry.124074799=Read"},
{"title": "Infinigen Indoors: Photorealistic Indoor Scenes using Procedural\n  Generation", "author": "Alexander Raistrick and Lingjie Mei and Karhan Kayan and David Yan and Yiming Zuo and Beining Han and Hongyu Wen and Meenal Parakh and Stamatis Alexandropoulos and Lahav Lipson and Zeyu Ma and Jia Deng", "abstract": "  We introduce Infinigen Indoors, a Blender-based procedural generator of\nphotorealistic indoor scenes. It builds upon the existing Infinigen system,\nwhich focuses on natural scenes, but expands its coverage to indoor scenes by\nintroducing a diverse library of procedural indoor assets, including furniture,\narchitecture elements, appliances, and other day-to-day objects. It also\nintroduces a constraint-based arrangement system, which consists of a\ndomain-specific language for expressing diverse constraints on scene\ncomposition, and a solver that generates scene compositions that maximally\nsatisfy the constraints. We provide an export tool that allows the generated 3D\nobjects and scenes to be directly used for training embodied agents in\nreal-time simulators such as Omniverse and Unreal. Infinigen Indoors is\nopen-sourced under the BSD license. Please visit https://infinigen.org for code\nand videos.\n", "link": "http://arxiv.org/abs/2406.11824v1", "date": "2024-06-17", "relevancy": 2.7455, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5675}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5399}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Infinigen%20Indoors%3A%20Photorealistic%20Indoor%20Scenes%20using%20Procedural%0A%20%20Generation&body=Title%3A%20Infinigen%20Indoors%3A%20Photorealistic%20Indoor%20Scenes%20using%20Procedural%0A%20%20Generation%0AAuthor%3A%20Alexander%20Raistrick%20and%20Lingjie%20Mei%20and%20Karhan%20Kayan%20and%20David%20Yan%20and%20Yiming%20Zuo%20and%20Beining%20Han%20and%20Hongyu%20Wen%20and%20Meenal%20Parakh%20and%20Stamatis%20Alexandropoulos%20and%20Lahav%20Lipson%20and%20Zeyu%20Ma%20and%20Jia%20Deng%0AAbstract%3A%20%20%20We%20introduce%20Infinigen%20Indoors%2C%20a%20Blender-based%20procedural%20generator%20of%0Aphotorealistic%20indoor%20scenes.%20It%20builds%20upon%20the%20existing%20Infinigen%20system%2C%0Awhich%20focuses%20on%20natural%20scenes%2C%20but%20expands%20its%20coverage%20to%20indoor%20scenes%20by%0Aintroducing%20a%20diverse%20library%20of%20procedural%20indoor%20assets%2C%20including%20furniture%2C%0Aarchitecture%20elements%2C%20appliances%2C%20and%20other%20day-to-day%20objects.%20It%20also%0Aintroduces%20a%20constraint-based%20arrangement%20system%2C%20which%20consists%20of%20a%0Adomain-specific%20language%20for%20expressing%20diverse%20constraints%20on%20scene%0Acomposition%2C%20and%20a%20solver%20that%20generates%20scene%20compositions%20that%20maximally%0Asatisfy%20the%20constraints.%20We%20provide%20an%20export%20tool%20that%20allows%20the%20generated%203D%0Aobjects%20and%20scenes%20to%20be%20directly%20used%20for%20training%20embodied%20agents%20in%0Areal-time%20simulators%20such%20as%20Omniverse%20and%20Unreal.%20Infinigen%20Indoors%20is%0Aopen-sourced%20under%20the%20BSD%20license.%20Please%20visit%20https%3A//infinigen.org%20for%20code%0Aand%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11824v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfinigen%2520Indoors%253A%2520Photorealistic%2520Indoor%2520Scenes%2520using%2520Procedural%250A%2520%2520Generation%26entry.906535625%3DAlexander%2520Raistrick%2520and%2520Lingjie%2520Mei%2520and%2520Karhan%2520Kayan%2520and%2520David%2520Yan%2520and%2520Yiming%2520Zuo%2520and%2520Beining%2520Han%2520and%2520Hongyu%2520Wen%2520and%2520Meenal%2520Parakh%2520and%2520Stamatis%2520Alexandropoulos%2520and%2520Lahav%2520Lipson%2520and%2520Zeyu%2520Ma%2520and%2520Jia%2520Deng%26entry.1292438233%3D%2520%2520We%2520introduce%2520Infinigen%2520Indoors%252C%2520a%2520Blender-based%2520procedural%2520generator%2520of%250Aphotorealistic%2520indoor%2520scenes.%2520It%2520builds%2520upon%2520the%2520existing%2520Infinigen%2520system%252C%250Awhich%2520focuses%2520on%2520natural%2520scenes%252C%2520but%2520expands%2520its%2520coverage%2520to%2520indoor%2520scenes%2520by%250Aintroducing%2520a%2520diverse%2520library%2520of%2520procedural%2520indoor%2520assets%252C%2520including%2520furniture%252C%250Aarchitecture%2520elements%252C%2520appliances%252C%2520and%2520other%2520day-to-day%2520objects.%2520It%2520also%250Aintroduces%2520a%2520constraint-based%2520arrangement%2520system%252C%2520which%2520consists%2520of%2520a%250Adomain-specific%2520language%2520for%2520expressing%2520diverse%2520constraints%2520on%2520scene%250Acomposition%252C%2520and%2520a%2520solver%2520that%2520generates%2520scene%2520compositions%2520that%2520maximally%250Asatisfy%2520the%2520constraints.%2520We%2520provide%2520an%2520export%2520tool%2520that%2520allows%2520the%2520generated%25203D%250Aobjects%2520and%2520scenes%2520to%2520be%2520directly%2520used%2520for%2520training%2520embodied%2520agents%2520in%250Areal-time%2520simulators%2520such%2520as%2520Omniverse%2520and%2520Unreal.%2520Infinigen%2520Indoors%2520is%250Aopen-sourced%2520under%2520the%2520BSD%2520license.%2520Please%2520visit%2520https%253A//infinigen.org%2520for%2520code%250Aand%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11824v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Infinigen%20Indoors%3A%20Photorealistic%20Indoor%20Scenes%20using%20Procedural%0A%20%20Generation&entry.906535625=Alexander%20Raistrick%20and%20Lingjie%20Mei%20and%20Karhan%20Kayan%20and%20David%20Yan%20and%20Yiming%20Zuo%20and%20Beining%20Han%20and%20Hongyu%20Wen%20and%20Meenal%20Parakh%20and%20Stamatis%20Alexandropoulos%20and%20Lahav%20Lipson%20and%20Zeyu%20Ma%20and%20Jia%20Deng&entry.1292438233=%20%20We%20introduce%20Infinigen%20Indoors%2C%20a%20Blender-based%20procedural%20generator%20of%0Aphotorealistic%20indoor%20scenes.%20It%20builds%20upon%20the%20existing%20Infinigen%20system%2C%0Awhich%20focuses%20on%20natural%20scenes%2C%20but%20expands%20its%20coverage%20to%20indoor%20scenes%20by%0Aintroducing%20a%20diverse%20library%20of%20procedural%20indoor%20assets%2C%20including%20furniture%2C%0Aarchitecture%20elements%2C%20appliances%2C%20and%20other%20day-to-day%20objects.%20It%20also%0Aintroduces%20a%20constraint-based%20arrangement%20system%2C%20which%20consists%20of%20a%0Adomain-specific%20language%20for%20expressing%20diverse%20constraints%20on%20scene%0Acomposition%2C%20and%20a%20solver%20that%20generates%20scene%20compositions%20that%20maximally%0Asatisfy%20the%20constraints.%20We%20provide%20an%20export%20tool%20that%20allows%20the%20generated%203D%0Aobjects%20and%20scenes%20to%20be%20directly%20used%20for%20training%20embodied%20agents%20in%0Areal-time%20simulators%20such%20as%20Omniverse%20and%20Unreal.%20Infinigen%20Indoors%20is%0Aopen-sourced%20under%20the%20BSD%20license.%20Please%20visit%20https%3A//infinigen.org%20for%20code%0Aand%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11824v1&entry.124074799=Read"},
{"title": "OGNI-DC: Robust Depth Completion with Optimization-Guided Neural\n  Iterations", "author": "Yiming Zuo and Jia Deng", "abstract": "  Depth completion is the task of generating a dense depth map given an image\nand a sparse depth map as inputs. It has important applications in various\ndownstream tasks. In this paper, we present OGNI-DC, a novel framework for\ndepth completion. The key to our method is \"Optimization-Guided Neural\nIterations\" (OGNI). It consists of a recurrent unit that refines a depth\ngradient field and a differentiable depth integrator that integrates the depth\ngradients into a depth map. OGNI-DC exhibits strong generalization,\noutperforming baselines by a large margin on unseen datasets and across various\nsparsity levels. Moreover, OGNI-DC has high accuracy, achieving\nstate-of-the-art performance on the NYUv2 and the KITTI benchmarks. Code is\navailable at https://github.com/princeton-vl/OGNI-DC.\n", "link": "http://arxiv.org/abs/2406.11711v1", "date": "2024-06-17", "relevancy": 2.7366, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5603}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5422}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OGNI-DC%3A%20Robust%20Depth%20Completion%20with%20Optimization-Guided%20Neural%0A%20%20Iterations&body=Title%3A%20OGNI-DC%3A%20Robust%20Depth%20Completion%20with%20Optimization-Guided%20Neural%0A%20%20Iterations%0AAuthor%3A%20Yiming%20Zuo%20and%20Jia%20Deng%0AAbstract%3A%20%20%20Depth%20completion%20is%20the%20task%20of%20generating%20a%20dense%20depth%20map%20given%20an%20image%0Aand%20a%20sparse%20depth%20map%20as%20inputs.%20It%20has%20important%20applications%20in%20various%0Adownstream%20tasks.%20In%20this%20paper%2C%20we%20present%20OGNI-DC%2C%20a%20novel%20framework%20for%0Adepth%20completion.%20The%20key%20to%20our%20method%20is%20%22Optimization-Guided%20Neural%0AIterations%22%20%28OGNI%29.%20It%20consists%20of%20a%20recurrent%20unit%20that%20refines%20a%20depth%0Agradient%20field%20and%20a%20differentiable%20depth%20integrator%20that%20integrates%20the%20depth%0Agradients%20into%20a%20depth%20map.%20OGNI-DC%20exhibits%20strong%20generalization%2C%0Aoutperforming%20baselines%20by%20a%20large%20margin%20on%20unseen%20datasets%20and%20across%20various%0Asparsity%20levels.%20Moreover%2C%20OGNI-DC%20has%20high%20accuracy%2C%20achieving%0Astate-of-the-art%20performance%20on%20the%20NYUv2%20and%20the%20KITTI%20benchmarks.%20Code%20is%0Aavailable%20at%20https%3A//github.com/princeton-vl/OGNI-DC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11711v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOGNI-DC%253A%2520Robust%2520Depth%2520Completion%2520with%2520Optimization-Guided%2520Neural%250A%2520%2520Iterations%26entry.906535625%3DYiming%2520Zuo%2520and%2520Jia%2520Deng%26entry.1292438233%3D%2520%2520Depth%2520completion%2520is%2520the%2520task%2520of%2520generating%2520a%2520dense%2520depth%2520map%2520given%2520an%2520image%250Aand%2520a%2520sparse%2520depth%2520map%2520as%2520inputs.%2520It%2520has%2520important%2520applications%2520in%2520various%250Adownstream%2520tasks.%2520In%2520this%2520paper%252C%2520we%2520present%2520OGNI-DC%252C%2520a%2520novel%2520framework%2520for%250Adepth%2520completion.%2520The%2520key%2520to%2520our%2520method%2520is%2520%2522Optimization-Guided%2520Neural%250AIterations%2522%2520%2528OGNI%2529.%2520It%2520consists%2520of%2520a%2520recurrent%2520unit%2520that%2520refines%2520a%2520depth%250Agradient%2520field%2520and%2520a%2520differentiable%2520depth%2520integrator%2520that%2520integrates%2520the%2520depth%250Agradients%2520into%2520a%2520depth%2520map.%2520OGNI-DC%2520exhibits%2520strong%2520generalization%252C%250Aoutperforming%2520baselines%2520by%2520a%2520large%2520margin%2520on%2520unseen%2520datasets%2520and%2520across%2520various%250Asparsity%2520levels.%2520Moreover%252C%2520OGNI-DC%2520has%2520high%2520accuracy%252C%2520achieving%250Astate-of-the-art%2520performance%2520on%2520the%2520NYUv2%2520and%2520the%2520KITTI%2520benchmarks.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/princeton-vl/OGNI-DC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11711v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OGNI-DC%3A%20Robust%20Depth%20Completion%20with%20Optimization-Guided%20Neural%0A%20%20Iterations&entry.906535625=Yiming%20Zuo%20and%20Jia%20Deng&entry.1292438233=%20%20Depth%20completion%20is%20the%20task%20of%20generating%20a%20dense%20depth%20map%20given%20an%20image%0Aand%20a%20sparse%20depth%20map%20as%20inputs.%20It%20has%20important%20applications%20in%20various%0Adownstream%20tasks.%20In%20this%20paper%2C%20we%20present%20OGNI-DC%2C%20a%20novel%20framework%20for%0Adepth%20completion.%20The%20key%20to%20our%20method%20is%20%22Optimization-Guided%20Neural%0AIterations%22%20%28OGNI%29.%20It%20consists%20of%20a%20recurrent%20unit%20that%20refines%20a%20depth%0Agradient%20field%20and%20a%20differentiable%20depth%20integrator%20that%20integrates%20the%20depth%0Agradients%20into%20a%20depth%20map.%20OGNI-DC%20exhibits%20strong%20generalization%2C%0Aoutperforming%20baselines%20by%20a%20large%20margin%20on%20unseen%20datasets%20and%20across%20various%0Asparsity%20levels.%20Moreover%2C%20OGNI-DC%20has%20high%20accuracy%2C%20achieving%0Astate-of-the-art%20performance%20on%20the%20NYUv2%20and%20the%20KITTI%20benchmarks.%20Code%20is%0Aavailable%20at%20https%3A//github.com/princeton-vl/OGNI-DC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11711v1&entry.124074799=Read"},
{"title": "SWCF-Net: Similarity-weighted Convolution and Local-global Fusion for\n  Efficient Large-scale Point Cloud Semantic Segmentation", "author": "Zhenchao Lin and Li He and Hongqiang Yang and Xiaoqun Sun and Cuojin Zhang and Weinan Chen and Yisheng Guan and Hong Zhang", "abstract": "  Large-scale point cloud consists of a multitude of individual objects,\nthereby encompassing rich structural and underlying semantic contextual\ninformation, resulting in a challenging problem in efficiently segmenting a\npoint cloud. Most existing researches mainly focus on capturing intricate local\nfeatures without giving due consideration to global ones, thus failing to\nleverage semantic context. In this paper, we propose a Similarity-Weighted\nConvolution and local-global Fusion Network, named SWCF-Net, which takes into\naccount both local and global features. We propose a Similarity-Weighted\nConvolution (SWConv) to effectively extract local features, where similarity\nweights are incorporated into the convolution operation to enhance the\ngeneralization capabilities. Then, we employ a downsampling operation on the K\nand V channels within the attention module, thereby reducing the quadratic\ncomplexity to linear, enabling the Transformer to deal with large-scale point\nclouds. At last, orthogonal components are extracted in the global features and\nthen aggregated with local features, thereby eliminating redundant information\nbetween local and global features and consequently promoting efficiency. We\nevaluate SWCF-Net on large-scale outdoor datasets SemanticKITTI and Toronto3D.\nOur experimental results demonstrate the effectiveness of the proposed network.\nOur method achieves a competitive result with less computational cost, and is\nable to handle large-scale point clouds efficiently.\n", "link": "http://arxiv.org/abs/2406.11441v1", "date": "2024-06-17", "relevancy": 2.7159, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5518}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5404}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5373}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SWCF-Net%3A%20Similarity-weighted%20Convolution%20and%20Local-global%20Fusion%20for%0A%20%20Efficient%20Large-scale%20Point%20Cloud%20Semantic%20Segmentation&body=Title%3A%20SWCF-Net%3A%20Similarity-weighted%20Convolution%20and%20Local-global%20Fusion%20for%0A%20%20Efficient%20Large-scale%20Point%20Cloud%20Semantic%20Segmentation%0AAuthor%3A%20Zhenchao%20Lin%20and%20Li%20He%20and%20Hongqiang%20Yang%20and%20Xiaoqun%20Sun%20and%20Cuojin%20Zhang%20and%20Weinan%20Chen%20and%20Yisheng%20Guan%20and%20Hong%20Zhang%0AAbstract%3A%20%20%20Large-scale%20point%20cloud%20consists%20of%20a%20multitude%20of%20individual%20objects%2C%0Athereby%20encompassing%20rich%20structural%20and%20underlying%20semantic%20contextual%0Ainformation%2C%20resulting%20in%20a%20challenging%20problem%20in%20efficiently%20segmenting%20a%0Apoint%20cloud.%20Most%20existing%20researches%20mainly%20focus%20on%20capturing%20intricate%20local%0Afeatures%20without%20giving%20due%20consideration%20to%20global%20ones%2C%20thus%20failing%20to%0Aleverage%20semantic%20context.%20In%20this%20paper%2C%20we%20propose%20a%20Similarity-Weighted%0AConvolution%20and%20local-global%20Fusion%20Network%2C%20named%20SWCF-Net%2C%20which%20takes%20into%0Aaccount%20both%20local%20and%20global%20features.%20We%20propose%20a%20Similarity-Weighted%0AConvolution%20%28SWConv%29%20to%20effectively%20extract%20local%20features%2C%20where%20similarity%0Aweights%20are%20incorporated%20into%20the%20convolution%20operation%20to%20enhance%20the%0Ageneralization%20capabilities.%20Then%2C%20we%20employ%20a%20downsampling%20operation%20on%20the%20K%0Aand%20V%20channels%20within%20the%20attention%20module%2C%20thereby%20reducing%20the%20quadratic%0Acomplexity%20to%20linear%2C%20enabling%20the%20Transformer%20to%20deal%20with%20large-scale%20point%0Aclouds.%20At%20last%2C%20orthogonal%20components%20are%20extracted%20in%20the%20global%20features%20and%0Athen%20aggregated%20with%20local%20features%2C%20thereby%20eliminating%20redundant%20information%0Abetween%20local%20and%20global%20features%20and%20consequently%20promoting%20efficiency.%20We%0Aevaluate%20SWCF-Net%20on%20large-scale%20outdoor%20datasets%20SemanticKITTI%20and%20Toronto3D.%0AOur%20experimental%20results%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20network.%0AOur%20method%20achieves%20a%20competitive%20result%20with%20less%20computational%20cost%2C%20and%20is%0Aable%20to%20handle%20large-scale%20point%20clouds%20efficiently.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11441v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSWCF-Net%253A%2520Similarity-weighted%2520Convolution%2520and%2520Local-global%2520Fusion%2520for%250A%2520%2520Efficient%2520Large-scale%2520Point%2520Cloud%2520Semantic%2520Segmentation%26entry.906535625%3DZhenchao%2520Lin%2520and%2520Li%2520He%2520and%2520Hongqiang%2520Yang%2520and%2520Xiaoqun%2520Sun%2520and%2520Cuojin%2520Zhang%2520and%2520Weinan%2520Chen%2520and%2520Yisheng%2520Guan%2520and%2520Hong%2520Zhang%26entry.1292438233%3D%2520%2520Large-scale%2520point%2520cloud%2520consists%2520of%2520a%2520multitude%2520of%2520individual%2520objects%252C%250Athereby%2520encompassing%2520rich%2520structural%2520and%2520underlying%2520semantic%2520contextual%250Ainformation%252C%2520resulting%2520in%2520a%2520challenging%2520problem%2520in%2520efficiently%2520segmenting%2520a%250Apoint%2520cloud.%2520Most%2520existing%2520researches%2520mainly%2520focus%2520on%2520capturing%2520intricate%2520local%250Afeatures%2520without%2520giving%2520due%2520consideration%2520to%2520global%2520ones%252C%2520thus%2520failing%2520to%250Aleverage%2520semantic%2520context.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520Similarity-Weighted%250AConvolution%2520and%2520local-global%2520Fusion%2520Network%252C%2520named%2520SWCF-Net%252C%2520which%2520takes%2520into%250Aaccount%2520both%2520local%2520and%2520global%2520features.%2520We%2520propose%2520a%2520Similarity-Weighted%250AConvolution%2520%2528SWConv%2529%2520to%2520effectively%2520extract%2520local%2520features%252C%2520where%2520similarity%250Aweights%2520are%2520incorporated%2520into%2520the%2520convolution%2520operation%2520to%2520enhance%2520the%250Ageneralization%2520capabilities.%2520Then%252C%2520we%2520employ%2520a%2520downsampling%2520operation%2520on%2520the%2520K%250Aand%2520V%2520channels%2520within%2520the%2520attention%2520module%252C%2520thereby%2520reducing%2520the%2520quadratic%250Acomplexity%2520to%2520linear%252C%2520enabling%2520the%2520Transformer%2520to%2520deal%2520with%2520large-scale%2520point%250Aclouds.%2520At%2520last%252C%2520orthogonal%2520components%2520are%2520extracted%2520in%2520the%2520global%2520features%2520and%250Athen%2520aggregated%2520with%2520local%2520features%252C%2520thereby%2520eliminating%2520redundant%2520information%250Abetween%2520local%2520and%2520global%2520features%2520and%2520consequently%2520promoting%2520efficiency.%2520We%250Aevaluate%2520SWCF-Net%2520on%2520large-scale%2520outdoor%2520datasets%2520SemanticKITTI%2520and%2520Toronto3D.%250AOur%2520experimental%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520network.%250AOur%2520method%2520achieves%2520a%2520competitive%2520result%2520with%2520less%2520computational%2520cost%252C%2520and%2520is%250Aable%2520to%2520handle%2520large-scale%2520point%2520clouds%2520efficiently.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11441v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SWCF-Net%3A%20Similarity-weighted%20Convolution%20and%20Local-global%20Fusion%20for%0A%20%20Efficient%20Large-scale%20Point%20Cloud%20Semantic%20Segmentation&entry.906535625=Zhenchao%20Lin%20and%20Li%20He%20and%20Hongqiang%20Yang%20and%20Xiaoqun%20Sun%20and%20Cuojin%20Zhang%20and%20Weinan%20Chen%20and%20Yisheng%20Guan%20and%20Hong%20Zhang&entry.1292438233=%20%20Large-scale%20point%20cloud%20consists%20of%20a%20multitude%20of%20individual%20objects%2C%0Athereby%20encompassing%20rich%20structural%20and%20underlying%20semantic%20contextual%0Ainformation%2C%20resulting%20in%20a%20challenging%20problem%20in%20efficiently%20segmenting%20a%0Apoint%20cloud.%20Most%20existing%20researches%20mainly%20focus%20on%20capturing%20intricate%20local%0Afeatures%20without%20giving%20due%20consideration%20to%20global%20ones%2C%20thus%20failing%20to%0Aleverage%20semantic%20context.%20In%20this%20paper%2C%20we%20propose%20a%20Similarity-Weighted%0AConvolution%20and%20local-global%20Fusion%20Network%2C%20named%20SWCF-Net%2C%20which%20takes%20into%0Aaccount%20both%20local%20and%20global%20features.%20We%20propose%20a%20Similarity-Weighted%0AConvolution%20%28SWConv%29%20to%20effectively%20extract%20local%20features%2C%20where%20similarity%0Aweights%20are%20incorporated%20into%20the%20convolution%20operation%20to%20enhance%20the%0Ageneralization%20capabilities.%20Then%2C%20we%20employ%20a%20downsampling%20operation%20on%20the%20K%0Aand%20V%20channels%20within%20the%20attention%20module%2C%20thereby%20reducing%20the%20quadratic%0Acomplexity%20to%20linear%2C%20enabling%20the%20Transformer%20to%20deal%20with%20large-scale%20point%0Aclouds.%20At%20last%2C%20orthogonal%20components%20are%20extracted%20in%20the%20global%20features%20and%0Athen%20aggregated%20with%20local%20features%2C%20thereby%20eliminating%20redundant%20information%0Abetween%20local%20and%20global%20features%20and%20consequently%20promoting%20efficiency.%20We%0Aevaluate%20SWCF-Net%20on%20large-scale%20outdoor%20datasets%20SemanticKITTI%20and%20Toronto3D.%0AOur%20experimental%20results%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20network.%0AOur%20method%20achieves%20a%20competitive%20result%20with%20less%20computational%20cost%2C%20and%20is%0Aable%20to%20handle%20large-scale%20point%20clouds%20efficiently.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11441v1&entry.124074799=Read"},
{"title": "IDVT: Interest-aware Denoising and View-guided Tuning for Social\n  Recommendation", "author": "Dezhao Yang and Jianghong Ma and Shanshan Feng and Haijun Zhang and Zhao Zhang", "abstract": "  In the information age, recommendation systems are vital for efficiently\nfiltering information and identifying user preferences. Online social platforms\nhave enriched these systems by providing valuable auxiliary information.\nSocially connected users are assumed to share similar preferences, enhancing\nrecommendation accuracy and addressing cold start issues. However, empirical\nfindings challenge the assumption, revealing that certain social connections\ncan actually harm system performance. Our statistical analysis indicates a\nsignificant amount of noise in the social network, where many socially\nconnected users do not share common interests. To address this issue, we\npropose an innovative \\underline{I}nterest-aware \\underline{D}enoising and\n\\underline{V}iew-guided \\underline{T}uning (IDVT) method for the social\nrecommendation. The first ID part effectively denoises social connections.\nSpecifically, the denoising process considers both social network structure and\nuser interaction interests in a global view. Moreover, in this global view, we\nalso integrate denoised social information (social domain) into the propagation\nof the user-item interactions (collaborative domain) and aggregate user\nrepresentations from two domains using a gating mechanism. To tackle potential\nuser interest loss and enhance model robustness within the global view, our\nsecond VT part introduces two additional views (local view and dropout-enhanced\nview) for fine-tuning user representations in the global view through\ncontrastive learning. Extensive evaluations on real-world datasets with varying\nnoise ratios demonstrate the superiority of IDVT over state-of-the-art social\nrecommendation methods.\n", "link": "http://arxiv.org/abs/2308.15926v2", "date": "2024-06-17", "relevancy": 2.6651, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5643}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5287}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5061}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IDVT%3A%20Interest-aware%20Denoising%20and%20View-guided%20Tuning%20for%20Social%0A%20%20Recommendation&body=Title%3A%20IDVT%3A%20Interest-aware%20Denoising%20and%20View-guided%20Tuning%20for%20Social%0A%20%20Recommendation%0AAuthor%3A%20Dezhao%20Yang%20and%20Jianghong%20Ma%20and%20Shanshan%20Feng%20and%20Haijun%20Zhang%20and%20Zhao%20Zhang%0AAbstract%3A%20%20%20In%20the%20information%20age%2C%20recommendation%20systems%20are%20vital%20for%20efficiently%0Afiltering%20information%20and%20identifying%20user%20preferences.%20Online%20social%20platforms%0Ahave%20enriched%20these%20systems%20by%20providing%20valuable%20auxiliary%20information.%0ASocially%20connected%20users%20are%20assumed%20to%20share%20similar%20preferences%2C%20enhancing%0Arecommendation%20accuracy%20and%20addressing%20cold%20start%20issues.%20However%2C%20empirical%0Afindings%20challenge%20the%20assumption%2C%20revealing%20that%20certain%20social%20connections%0Acan%20actually%20harm%20system%20performance.%20Our%20statistical%20analysis%20indicates%20a%0Asignificant%20amount%20of%20noise%20in%20the%20social%20network%2C%20where%20many%20socially%0Aconnected%20users%20do%20not%20share%20common%20interests.%20To%20address%20this%20issue%2C%20we%0Apropose%20an%20innovative%20%5Cunderline%7BI%7Dnterest-aware%20%5Cunderline%7BD%7Denoising%20and%0A%5Cunderline%7BV%7Diew-guided%20%5Cunderline%7BT%7Duning%20%28IDVT%29%20method%20for%20the%20social%0Arecommendation.%20The%20first%20ID%20part%20effectively%20denoises%20social%20connections.%0ASpecifically%2C%20the%20denoising%20process%20considers%20both%20social%20network%20structure%20and%0Auser%20interaction%20interests%20in%20a%20global%20view.%20Moreover%2C%20in%20this%20global%20view%2C%20we%0Aalso%20integrate%20denoised%20social%20information%20%28social%20domain%29%20into%20the%20propagation%0Aof%20the%20user-item%20interactions%20%28collaborative%20domain%29%20and%20aggregate%20user%0Arepresentations%20from%20two%20domains%20using%20a%20gating%20mechanism.%20To%20tackle%20potential%0Auser%20interest%20loss%20and%20enhance%20model%20robustness%20within%20the%20global%20view%2C%20our%0Asecond%20VT%20part%20introduces%20two%20additional%20views%20%28local%20view%20and%20dropout-enhanced%0Aview%29%20for%20fine-tuning%20user%20representations%20in%20the%20global%20view%20through%0Acontrastive%20learning.%20Extensive%20evaluations%20on%20real-world%20datasets%20with%20varying%0Anoise%20ratios%20demonstrate%20the%20superiority%20of%20IDVT%20over%20state-of-the-art%20social%0Arecommendation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.15926v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIDVT%253A%2520Interest-aware%2520Denoising%2520and%2520View-guided%2520Tuning%2520for%2520Social%250A%2520%2520Recommendation%26entry.906535625%3DDezhao%2520Yang%2520and%2520Jianghong%2520Ma%2520and%2520Shanshan%2520Feng%2520and%2520Haijun%2520Zhang%2520and%2520Zhao%2520Zhang%26entry.1292438233%3D%2520%2520In%2520the%2520information%2520age%252C%2520recommendation%2520systems%2520are%2520vital%2520for%2520efficiently%250Afiltering%2520information%2520and%2520identifying%2520user%2520preferences.%2520Online%2520social%2520platforms%250Ahave%2520enriched%2520these%2520systems%2520by%2520providing%2520valuable%2520auxiliary%2520information.%250ASocially%2520connected%2520users%2520are%2520assumed%2520to%2520share%2520similar%2520preferences%252C%2520enhancing%250Arecommendation%2520accuracy%2520and%2520addressing%2520cold%2520start%2520issues.%2520However%252C%2520empirical%250Afindings%2520challenge%2520the%2520assumption%252C%2520revealing%2520that%2520certain%2520social%2520connections%250Acan%2520actually%2520harm%2520system%2520performance.%2520Our%2520statistical%2520analysis%2520indicates%2520a%250Asignificant%2520amount%2520of%2520noise%2520in%2520the%2520social%2520network%252C%2520where%2520many%2520socially%250Aconnected%2520users%2520do%2520not%2520share%2520common%2520interests.%2520To%2520address%2520this%2520issue%252C%2520we%250Apropose%2520an%2520innovative%2520%255Cunderline%257BI%257Dnterest-aware%2520%255Cunderline%257BD%257Denoising%2520and%250A%255Cunderline%257BV%257Diew-guided%2520%255Cunderline%257BT%257Duning%2520%2528IDVT%2529%2520method%2520for%2520the%2520social%250Arecommendation.%2520The%2520first%2520ID%2520part%2520effectively%2520denoises%2520social%2520connections.%250ASpecifically%252C%2520the%2520denoising%2520process%2520considers%2520both%2520social%2520network%2520structure%2520and%250Auser%2520interaction%2520interests%2520in%2520a%2520global%2520view.%2520Moreover%252C%2520in%2520this%2520global%2520view%252C%2520we%250Aalso%2520integrate%2520denoised%2520social%2520information%2520%2528social%2520domain%2529%2520into%2520the%2520propagation%250Aof%2520the%2520user-item%2520interactions%2520%2528collaborative%2520domain%2529%2520and%2520aggregate%2520user%250Arepresentations%2520from%2520two%2520domains%2520using%2520a%2520gating%2520mechanism.%2520To%2520tackle%2520potential%250Auser%2520interest%2520loss%2520and%2520enhance%2520model%2520robustness%2520within%2520the%2520global%2520view%252C%2520our%250Asecond%2520VT%2520part%2520introduces%2520two%2520additional%2520views%2520%2528local%2520view%2520and%2520dropout-enhanced%250Aview%2529%2520for%2520fine-tuning%2520user%2520representations%2520in%2520the%2520global%2520view%2520through%250Acontrastive%2520learning.%2520Extensive%2520evaluations%2520on%2520real-world%2520datasets%2520with%2520varying%250Anoise%2520ratios%2520demonstrate%2520the%2520superiority%2520of%2520IDVT%2520over%2520state-of-the-art%2520social%250Arecommendation%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.15926v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IDVT%3A%20Interest-aware%20Denoising%20and%20View-guided%20Tuning%20for%20Social%0A%20%20Recommendation&entry.906535625=Dezhao%20Yang%20and%20Jianghong%20Ma%20and%20Shanshan%20Feng%20and%20Haijun%20Zhang%20and%20Zhao%20Zhang&entry.1292438233=%20%20In%20the%20information%20age%2C%20recommendation%20systems%20are%20vital%20for%20efficiently%0Afiltering%20information%20and%20identifying%20user%20preferences.%20Online%20social%20platforms%0Ahave%20enriched%20these%20systems%20by%20providing%20valuable%20auxiliary%20information.%0ASocially%20connected%20users%20are%20assumed%20to%20share%20similar%20preferences%2C%20enhancing%0Arecommendation%20accuracy%20and%20addressing%20cold%20start%20issues.%20However%2C%20empirical%0Afindings%20challenge%20the%20assumption%2C%20revealing%20that%20certain%20social%20connections%0Acan%20actually%20harm%20system%20performance.%20Our%20statistical%20analysis%20indicates%20a%0Asignificant%20amount%20of%20noise%20in%20the%20social%20network%2C%20where%20many%20socially%0Aconnected%20users%20do%20not%20share%20common%20interests.%20To%20address%20this%20issue%2C%20we%0Apropose%20an%20innovative%20%5Cunderline%7BI%7Dnterest-aware%20%5Cunderline%7BD%7Denoising%20and%0A%5Cunderline%7BV%7Diew-guided%20%5Cunderline%7BT%7Duning%20%28IDVT%29%20method%20for%20the%20social%0Arecommendation.%20The%20first%20ID%20part%20effectively%20denoises%20social%20connections.%0ASpecifically%2C%20the%20denoising%20process%20considers%20both%20social%20network%20structure%20and%0Auser%20interaction%20interests%20in%20a%20global%20view.%20Moreover%2C%20in%20this%20global%20view%2C%20we%0Aalso%20integrate%20denoised%20social%20information%20%28social%20domain%29%20into%20the%20propagation%0Aof%20the%20user-item%20interactions%20%28collaborative%20domain%29%20and%20aggregate%20user%0Arepresentations%20from%20two%20domains%20using%20a%20gating%20mechanism.%20To%20tackle%20potential%0Auser%20interest%20loss%20and%20enhance%20model%20robustness%20within%20the%20global%20view%2C%20our%0Asecond%20VT%20part%20introduces%20two%20additional%20views%20%28local%20view%20and%20dropout-enhanced%0Aview%29%20for%20fine-tuning%20user%20representations%20in%20the%20global%20view%20through%0Acontrastive%20learning.%20Extensive%20evaluations%20on%20real-world%20datasets%20with%20varying%0Anoise%20ratios%20demonstrate%20the%20superiority%20of%20IDVT%20over%20state-of-the-art%20social%0Arecommendation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.15926v2&entry.124074799=Read"},
{"title": "t-DGR: A Trajectory-Based Deep Generative Replay Method for Continual\n  Learning in Decision Making", "author": "William Yue and Bo Liu and Peter Stone", "abstract": "  Deep generative replay has emerged as a promising approach for continual\nlearning in decision-making tasks. This approach addresses the problem of\ncatastrophic forgetting by leveraging the generation of trajectories from\npreviously encountered tasks to augment the current dataset. However, existing\ndeep generative replay methods for continual learning rely on autoregressive\nmodels, which suffer from compounding errors in the generated trajectories. In\nthis paper, we propose a simple, scalable, and non-autoregressive method for\ncontinual learning in decision-making tasks using a generative model that\ngenerates task samples conditioned on the trajectory timestep. We evaluate our\nmethod on Continual World benchmarks and find that our approach achieves\nstate-of-the-art performance on the average success rate metric among continual\nlearning methods. Code is available at https://github.com/WilliamYue37/t-DGR.\n", "link": "http://arxiv.org/abs/2401.02576v2", "date": "2024-06-17", "relevancy": 2.6585, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5389}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5324}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5238}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20t-DGR%3A%20A%20Trajectory-Based%20Deep%20Generative%20Replay%20Method%20for%20Continual%0A%20%20Learning%20in%20Decision%20Making&body=Title%3A%20t-DGR%3A%20A%20Trajectory-Based%20Deep%20Generative%20Replay%20Method%20for%20Continual%0A%20%20Learning%20in%20Decision%20Making%0AAuthor%3A%20William%20Yue%20and%20Bo%20Liu%20and%20Peter%20Stone%0AAbstract%3A%20%20%20Deep%20generative%20replay%20has%20emerged%20as%20a%20promising%20approach%20for%20continual%0Alearning%20in%20decision-making%20tasks.%20This%20approach%20addresses%20the%20problem%20of%0Acatastrophic%20forgetting%20by%20leveraging%20the%20generation%20of%20trajectories%20from%0Apreviously%20encountered%20tasks%20to%20augment%20the%20current%20dataset.%20However%2C%20existing%0Adeep%20generative%20replay%20methods%20for%20continual%20learning%20rely%20on%20autoregressive%0Amodels%2C%20which%20suffer%20from%20compounding%20errors%20in%20the%20generated%20trajectories.%20In%0Athis%20paper%2C%20we%20propose%20a%20simple%2C%20scalable%2C%20and%20non-autoregressive%20method%20for%0Acontinual%20learning%20in%20decision-making%20tasks%20using%20a%20generative%20model%20that%0Agenerates%20task%20samples%20conditioned%20on%20the%20trajectory%20timestep.%20We%20evaluate%20our%0Amethod%20on%20Continual%20World%20benchmarks%20and%20find%20that%20our%20approach%20achieves%0Astate-of-the-art%20performance%20on%20the%20average%20success%20rate%20metric%20among%20continual%0Alearning%20methods.%20Code%20is%20available%20at%20https%3A//github.com/WilliamYue37/t-DGR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.02576v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dt-DGR%253A%2520A%2520Trajectory-Based%2520Deep%2520Generative%2520Replay%2520Method%2520for%2520Continual%250A%2520%2520Learning%2520in%2520Decision%2520Making%26entry.906535625%3DWilliam%2520Yue%2520and%2520Bo%2520Liu%2520and%2520Peter%2520Stone%26entry.1292438233%3D%2520%2520Deep%2520generative%2520replay%2520has%2520emerged%2520as%2520a%2520promising%2520approach%2520for%2520continual%250Alearning%2520in%2520decision-making%2520tasks.%2520This%2520approach%2520addresses%2520the%2520problem%2520of%250Acatastrophic%2520forgetting%2520by%2520leveraging%2520the%2520generation%2520of%2520trajectories%2520from%250Apreviously%2520encountered%2520tasks%2520to%2520augment%2520the%2520current%2520dataset.%2520However%252C%2520existing%250Adeep%2520generative%2520replay%2520methods%2520for%2520continual%2520learning%2520rely%2520on%2520autoregressive%250Amodels%252C%2520which%2520suffer%2520from%2520compounding%2520errors%2520in%2520the%2520generated%2520trajectories.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520a%2520simple%252C%2520scalable%252C%2520and%2520non-autoregressive%2520method%2520for%250Acontinual%2520learning%2520in%2520decision-making%2520tasks%2520using%2520a%2520generative%2520model%2520that%250Agenerates%2520task%2520samples%2520conditioned%2520on%2520the%2520trajectory%2520timestep.%2520We%2520evaluate%2520our%250Amethod%2520on%2520Continual%2520World%2520benchmarks%2520and%2520find%2520that%2520our%2520approach%2520achieves%250Astate-of-the-art%2520performance%2520on%2520the%2520average%2520success%2520rate%2520metric%2520among%2520continual%250Alearning%2520methods.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/WilliamYue37/t-DGR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.02576v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=t-DGR%3A%20A%20Trajectory-Based%20Deep%20Generative%20Replay%20Method%20for%20Continual%0A%20%20Learning%20in%20Decision%20Making&entry.906535625=William%20Yue%20and%20Bo%20Liu%20and%20Peter%20Stone&entry.1292438233=%20%20Deep%20generative%20replay%20has%20emerged%20as%20a%20promising%20approach%20for%20continual%0Alearning%20in%20decision-making%20tasks.%20This%20approach%20addresses%20the%20problem%20of%0Acatastrophic%20forgetting%20by%20leveraging%20the%20generation%20of%20trajectories%20from%0Apreviously%20encountered%20tasks%20to%20augment%20the%20current%20dataset.%20However%2C%20existing%0Adeep%20generative%20replay%20methods%20for%20continual%20learning%20rely%20on%20autoregressive%0Amodels%2C%20which%20suffer%20from%20compounding%20errors%20in%20the%20generated%20trajectories.%20In%0Athis%20paper%2C%20we%20propose%20a%20simple%2C%20scalable%2C%20and%20non-autoregressive%20method%20for%0Acontinual%20learning%20in%20decision-making%20tasks%20using%20a%20generative%20model%20that%0Agenerates%20task%20samples%20conditioned%20on%20the%20trajectory%20timestep.%20We%20evaluate%20our%0Amethod%20on%20Continual%20World%20benchmarks%20and%20find%20that%20our%20approach%20achieves%0Astate-of-the-art%20performance%20on%20the%20average%20success%20rate%20metric%20among%20continual%0Alearning%20methods.%20Code%20is%20available%20at%20https%3A//github.com/WilliamYue37/t-DGR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.02576v2&entry.124074799=Read"},
{"title": "Composing Object Relations and Attributes for Image-Text Matching", "author": "Khoi Pham and Chuong Huynh and Ser-Nam Lim and Abhinav Shrivastava", "abstract": "  We study the visual semantic embedding problem for image-text matching. Most\nexisting work utilizes a tailored cross-attention mechanism to perform local\nalignment across the two image and text modalities. This is computationally\nexpensive, even though it is more powerful than the unimodal dual-encoder\napproach. This work introduces a dual-encoder image-text matching model,\nleveraging a scene graph to represent captions with nodes for objects and\nattributes interconnected by relational edges. Utilizing a graph attention\nnetwork, our model efficiently encodes object-attribute and object-object\nsemantic relations, resulting in a robust and fast-performing system.\nRepresenting caption as a scene graph offers the ability to utilize the strong\nrelational inductive bias of graph neural networks to learn object-attribute\nand object-object relations effectively. To train the model, we propose losses\nthat align the image and caption both at the holistic level (image-caption) and\nthe local level (image-object entity), which we show is key to the success of\nthe model. Our model is termed Composition model for Object Relations and\nAttributes, CORA. Experimental results on two prominent image-text retrieval\nbenchmarks, Flickr30K and MSCOCO, demonstrate that CORA outperforms existing\nstate-of-the-art computationally expensive cross-attention methods regarding\nrecall score while achieving fast computation speed of the dual encoder.\n", "link": "http://arxiv.org/abs/2406.11820v1", "date": "2024-06-17", "relevancy": 2.6551, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5485}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5356}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5089}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Composing%20Object%20Relations%20and%20Attributes%20for%20Image-Text%20Matching&body=Title%3A%20Composing%20Object%20Relations%20and%20Attributes%20for%20Image-Text%20Matching%0AAuthor%3A%20Khoi%20Pham%20and%20Chuong%20Huynh%20and%20Ser-Nam%20Lim%20and%20Abhinav%20Shrivastava%0AAbstract%3A%20%20%20We%20study%20the%20visual%20semantic%20embedding%20problem%20for%20image-text%20matching.%20Most%0Aexisting%20work%20utilizes%20a%20tailored%20cross-attention%20mechanism%20to%20perform%20local%0Aalignment%20across%20the%20two%20image%20and%20text%20modalities.%20This%20is%20computationally%0Aexpensive%2C%20even%20though%20it%20is%20more%20powerful%20than%20the%20unimodal%20dual-encoder%0Aapproach.%20This%20work%20introduces%20a%20dual-encoder%20image-text%20matching%20model%2C%0Aleveraging%20a%20scene%20graph%20to%20represent%20captions%20with%20nodes%20for%20objects%20and%0Aattributes%20interconnected%20by%20relational%20edges.%20Utilizing%20a%20graph%20attention%0Anetwork%2C%20our%20model%20efficiently%20encodes%20object-attribute%20and%20object-object%0Asemantic%20relations%2C%20resulting%20in%20a%20robust%20and%20fast-performing%20system.%0ARepresenting%20caption%20as%20a%20scene%20graph%20offers%20the%20ability%20to%20utilize%20the%20strong%0Arelational%20inductive%20bias%20of%20graph%20neural%20networks%20to%20learn%20object-attribute%0Aand%20object-object%20relations%20effectively.%20To%20train%20the%20model%2C%20we%20propose%20losses%0Athat%20align%20the%20image%20and%20caption%20both%20at%20the%20holistic%20level%20%28image-caption%29%20and%0Athe%20local%20level%20%28image-object%20entity%29%2C%20which%20we%20show%20is%20key%20to%20the%20success%20of%0Athe%20model.%20Our%20model%20is%20termed%20Composition%20model%20for%20Object%20Relations%20and%0AAttributes%2C%20CORA.%20Experimental%20results%20on%20two%20prominent%20image-text%20retrieval%0Abenchmarks%2C%20Flickr30K%20and%20MSCOCO%2C%20demonstrate%20that%20CORA%20outperforms%20existing%0Astate-of-the-art%20computationally%20expensive%20cross-attention%20methods%20regarding%0Arecall%20score%20while%20achieving%20fast%20computation%20speed%20of%20the%20dual%20encoder.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11820v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComposing%2520Object%2520Relations%2520and%2520Attributes%2520for%2520Image-Text%2520Matching%26entry.906535625%3DKhoi%2520Pham%2520and%2520Chuong%2520Huynh%2520and%2520Ser-Nam%2520Lim%2520and%2520Abhinav%2520Shrivastava%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520visual%2520semantic%2520embedding%2520problem%2520for%2520image-text%2520matching.%2520Most%250Aexisting%2520work%2520utilizes%2520a%2520tailored%2520cross-attention%2520mechanism%2520to%2520perform%2520local%250Aalignment%2520across%2520the%2520two%2520image%2520and%2520text%2520modalities.%2520This%2520is%2520computationally%250Aexpensive%252C%2520even%2520though%2520it%2520is%2520more%2520powerful%2520than%2520the%2520unimodal%2520dual-encoder%250Aapproach.%2520This%2520work%2520introduces%2520a%2520dual-encoder%2520image-text%2520matching%2520model%252C%250Aleveraging%2520a%2520scene%2520graph%2520to%2520represent%2520captions%2520with%2520nodes%2520for%2520objects%2520and%250Aattributes%2520interconnected%2520by%2520relational%2520edges.%2520Utilizing%2520a%2520graph%2520attention%250Anetwork%252C%2520our%2520model%2520efficiently%2520encodes%2520object-attribute%2520and%2520object-object%250Asemantic%2520relations%252C%2520resulting%2520in%2520a%2520robust%2520and%2520fast-performing%2520system.%250ARepresenting%2520caption%2520as%2520a%2520scene%2520graph%2520offers%2520the%2520ability%2520to%2520utilize%2520the%2520strong%250Arelational%2520inductive%2520bias%2520of%2520graph%2520neural%2520networks%2520to%2520learn%2520object-attribute%250Aand%2520object-object%2520relations%2520effectively.%2520To%2520train%2520the%2520model%252C%2520we%2520propose%2520losses%250Athat%2520align%2520the%2520image%2520and%2520caption%2520both%2520at%2520the%2520holistic%2520level%2520%2528image-caption%2529%2520and%250Athe%2520local%2520level%2520%2528image-object%2520entity%2529%252C%2520which%2520we%2520show%2520is%2520key%2520to%2520the%2520success%2520of%250Athe%2520model.%2520Our%2520model%2520is%2520termed%2520Composition%2520model%2520for%2520Object%2520Relations%2520and%250AAttributes%252C%2520CORA.%2520Experimental%2520results%2520on%2520two%2520prominent%2520image-text%2520retrieval%250Abenchmarks%252C%2520Flickr30K%2520and%2520MSCOCO%252C%2520demonstrate%2520that%2520CORA%2520outperforms%2520existing%250Astate-of-the-art%2520computationally%2520expensive%2520cross-attention%2520methods%2520regarding%250Arecall%2520score%2520while%2520achieving%2520fast%2520computation%2520speed%2520of%2520the%2520dual%2520encoder.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11820v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Composing%20Object%20Relations%20and%20Attributes%20for%20Image-Text%20Matching&entry.906535625=Khoi%20Pham%20and%20Chuong%20Huynh%20and%20Ser-Nam%20Lim%20and%20Abhinav%20Shrivastava&entry.1292438233=%20%20We%20study%20the%20visual%20semantic%20embedding%20problem%20for%20image-text%20matching.%20Most%0Aexisting%20work%20utilizes%20a%20tailored%20cross-attention%20mechanism%20to%20perform%20local%0Aalignment%20across%20the%20two%20image%20and%20text%20modalities.%20This%20is%20computationally%0Aexpensive%2C%20even%20though%20it%20is%20more%20powerful%20than%20the%20unimodal%20dual-encoder%0Aapproach.%20This%20work%20introduces%20a%20dual-encoder%20image-text%20matching%20model%2C%0Aleveraging%20a%20scene%20graph%20to%20represent%20captions%20with%20nodes%20for%20objects%20and%0Aattributes%20interconnected%20by%20relational%20edges.%20Utilizing%20a%20graph%20attention%0Anetwork%2C%20our%20model%20efficiently%20encodes%20object-attribute%20and%20object-object%0Asemantic%20relations%2C%20resulting%20in%20a%20robust%20and%20fast-performing%20system.%0ARepresenting%20caption%20as%20a%20scene%20graph%20offers%20the%20ability%20to%20utilize%20the%20strong%0Arelational%20inductive%20bias%20of%20graph%20neural%20networks%20to%20learn%20object-attribute%0Aand%20object-object%20relations%20effectively.%20To%20train%20the%20model%2C%20we%20propose%20losses%0Athat%20align%20the%20image%20and%20caption%20both%20at%20the%20holistic%20level%20%28image-caption%29%20and%0Athe%20local%20level%20%28image-object%20entity%29%2C%20which%20we%20show%20is%20key%20to%20the%20success%20of%0Athe%20model.%20Our%20model%20is%20termed%20Composition%20model%20for%20Object%20Relations%20and%0AAttributes%2C%20CORA.%20Experimental%20results%20on%20two%20prominent%20image-text%20retrieval%0Abenchmarks%2C%20Flickr30K%20and%20MSCOCO%2C%20demonstrate%20that%20CORA%20outperforms%20existing%0Astate-of-the-art%20computationally%20expensive%20cross-attention%20methods%20regarding%0Arecall%20score%20while%20achieving%20fast%20computation%20speed%20of%20the%20dual%20encoder.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11820v1&entry.124074799=Read"},
{"title": "Task Me Anything", "author": "Jieyu Zhang and Weikai Huang and Zixian Ma and Oscar Michel and Dong He and Tanmay Gupta and Wei-Chiu Ma and Ali Farhadi and Aniruddha Kembhavi and Ranjay Krishna", "abstract": "  Benchmarks for large multimodal language models (MLMs) now serve to\nsimultaneously assess the general capabilities of models instead of evaluating\nfor a specific capability. As a result, when a developer wants to identify\nwhich models to use for their application, they are overwhelmed by the number\nof benchmarks and remain uncertain about which benchmark's results are most\nreflective of their specific use case. This paper introduces Task-Me-Anything,\na benchmark generation engine which produces a benchmark tailored to a user's\nneeds. Task-Me-Anything maintains an extendable taxonomy of visual assets and\ncan programmatically generate a vast number of task instances. Additionally, it\nalgorithmically addresses user queries regarding MLM performance efficiently\nwithin a computational budget. It contains 113K images, 10K videos, 2K 3D\nobject assets, over 365 object categories, 655 attributes, and 335\nrelationships. It can generate 750M image/video question-answering pairs, which\nfocus on evaluating MLM perceptual capabilities. Task-Me-Anything reveals\ncritical insights: open-source MLMs excel in object and attribute recognition\nbut lack spatial and temporal understanding; each model exhibits unique\nstrengths and weaknesses; larger models generally perform better, though\nexceptions exist; and GPT4o demonstrates challenges in recognizing\nrotating/moving objects and distinguishing colors.\n", "link": "http://arxiv.org/abs/2406.11775v1", "date": "2024-06-17", "relevancy": 2.6336, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5469}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5319}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5014}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task%20Me%20Anything&body=Title%3A%20Task%20Me%20Anything%0AAuthor%3A%20Jieyu%20Zhang%20and%20Weikai%20Huang%20and%20Zixian%20Ma%20and%20Oscar%20Michel%20and%20Dong%20He%20and%20Tanmay%20Gupta%20and%20Wei-Chiu%20Ma%20and%20Ali%20Farhadi%20and%20Aniruddha%20Kembhavi%20and%20Ranjay%20Krishna%0AAbstract%3A%20%20%20Benchmarks%20for%20large%20multimodal%20language%20models%20%28MLMs%29%20now%20serve%20to%0Asimultaneously%20assess%20the%20general%20capabilities%20of%20models%20instead%20of%20evaluating%0Afor%20a%20specific%20capability.%20As%20a%20result%2C%20when%20a%20developer%20wants%20to%20identify%0Awhich%20models%20to%20use%20for%20their%20application%2C%20they%20are%20overwhelmed%20by%20the%20number%0Aof%20benchmarks%20and%20remain%20uncertain%20about%20which%20benchmark%27s%20results%20are%20most%0Areflective%20of%20their%20specific%20use%20case.%20This%20paper%20introduces%20Task-Me-Anything%2C%0Aa%20benchmark%20generation%20engine%20which%20produces%20a%20benchmark%20tailored%20to%20a%20user%27s%0Aneeds.%20Task-Me-Anything%20maintains%20an%20extendable%20taxonomy%20of%20visual%20assets%20and%0Acan%20programmatically%20generate%20a%20vast%20number%20of%20task%20instances.%20Additionally%2C%20it%0Aalgorithmically%20addresses%20user%20queries%20regarding%20MLM%20performance%20efficiently%0Awithin%20a%20computational%20budget.%20It%20contains%20113K%20images%2C%2010K%20videos%2C%202K%203D%0Aobject%20assets%2C%20over%20365%20object%20categories%2C%20655%20attributes%2C%20and%20335%0Arelationships.%20It%20can%20generate%20750M%20image/video%20question-answering%20pairs%2C%20which%0Afocus%20on%20evaluating%20MLM%20perceptual%20capabilities.%20Task-Me-Anything%20reveals%0Acritical%20insights%3A%20open-source%20MLMs%20excel%20in%20object%20and%20attribute%20recognition%0Abut%20lack%20spatial%20and%20temporal%20understanding%3B%20each%20model%20exhibits%20unique%0Astrengths%20and%20weaknesses%3B%20larger%20models%20generally%20perform%20better%2C%20though%0Aexceptions%20exist%3B%20and%20GPT4o%20demonstrates%20challenges%20in%20recognizing%0Arotating/moving%20objects%20and%20distinguishing%20colors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11775v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask%2520Me%2520Anything%26entry.906535625%3DJieyu%2520Zhang%2520and%2520Weikai%2520Huang%2520and%2520Zixian%2520Ma%2520and%2520Oscar%2520Michel%2520and%2520Dong%2520He%2520and%2520Tanmay%2520Gupta%2520and%2520Wei-Chiu%2520Ma%2520and%2520Ali%2520Farhadi%2520and%2520Aniruddha%2520Kembhavi%2520and%2520Ranjay%2520Krishna%26entry.1292438233%3D%2520%2520Benchmarks%2520for%2520large%2520multimodal%2520language%2520models%2520%2528MLMs%2529%2520now%2520serve%2520to%250Asimultaneously%2520assess%2520the%2520general%2520capabilities%2520of%2520models%2520instead%2520of%2520evaluating%250Afor%2520a%2520specific%2520capability.%2520As%2520a%2520result%252C%2520when%2520a%2520developer%2520wants%2520to%2520identify%250Awhich%2520models%2520to%2520use%2520for%2520their%2520application%252C%2520they%2520are%2520overwhelmed%2520by%2520the%2520number%250Aof%2520benchmarks%2520and%2520remain%2520uncertain%2520about%2520which%2520benchmark%2527s%2520results%2520are%2520most%250Areflective%2520of%2520their%2520specific%2520use%2520case.%2520This%2520paper%2520introduces%2520Task-Me-Anything%252C%250Aa%2520benchmark%2520generation%2520engine%2520which%2520produces%2520a%2520benchmark%2520tailored%2520to%2520a%2520user%2527s%250Aneeds.%2520Task-Me-Anything%2520maintains%2520an%2520extendable%2520taxonomy%2520of%2520visual%2520assets%2520and%250Acan%2520programmatically%2520generate%2520a%2520vast%2520number%2520of%2520task%2520instances.%2520Additionally%252C%2520it%250Aalgorithmically%2520addresses%2520user%2520queries%2520regarding%2520MLM%2520performance%2520efficiently%250Awithin%2520a%2520computational%2520budget.%2520It%2520contains%2520113K%2520images%252C%252010K%2520videos%252C%25202K%25203D%250Aobject%2520assets%252C%2520over%2520365%2520object%2520categories%252C%2520655%2520attributes%252C%2520and%2520335%250Arelationships.%2520It%2520can%2520generate%2520750M%2520image/video%2520question-answering%2520pairs%252C%2520which%250Afocus%2520on%2520evaluating%2520MLM%2520perceptual%2520capabilities.%2520Task-Me-Anything%2520reveals%250Acritical%2520insights%253A%2520open-source%2520MLMs%2520excel%2520in%2520object%2520and%2520attribute%2520recognition%250Abut%2520lack%2520spatial%2520and%2520temporal%2520understanding%253B%2520each%2520model%2520exhibits%2520unique%250Astrengths%2520and%2520weaknesses%253B%2520larger%2520models%2520generally%2520perform%2520better%252C%2520though%250Aexceptions%2520exist%253B%2520and%2520GPT4o%2520demonstrates%2520challenges%2520in%2520recognizing%250Arotating/moving%2520objects%2520and%2520distinguishing%2520colors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11775v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task%20Me%20Anything&entry.906535625=Jieyu%20Zhang%20and%20Weikai%20Huang%20and%20Zixian%20Ma%20and%20Oscar%20Michel%20and%20Dong%20He%20and%20Tanmay%20Gupta%20and%20Wei-Chiu%20Ma%20and%20Ali%20Farhadi%20and%20Aniruddha%20Kembhavi%20and%20Ranjay%20Krishna&entry.1292438233=%20%20Benchmarks%20for%20large%20multimodal%20language%20models%20%28MLMs%29%20now%20serve%20to%0Asimultaneously%20assess%20the%20general%20capabilities%20of%20models%20instead%20of%20evaluating%0Afor%20a%20specific%20capability.%20As%20a%20result%2C%20when%20a%20developer%20wants%20to%20identify%0Awhich%20models%20to%20use%20for%20their%20application%2C%20they%20are%20overwhelmed%20by%20the%20number%0Aof%20benchmarks%20and%20remain%20uncertain%20about%20which%20benchmark%27s%20results%20are%20most%0Areflective%20of%20their%20specific%20use%20case.%20This%20paper%20introduces%20Task-Me-Anything%2C%0Aa%20benchmark%20generation%20engine%20which%20produces%20a%20benchmark%20tailored%20to%20a%20user%27s%0Aneeds.%20Task-Me-Anything%20maintains%20an%20extendable%20taxonomy%20of%20visual%20assets%20and%0Acan%20programmatically%20generate%20a%20vast%20number%20of%20task%20instances.%20Additionally%2C%20it%0Aalgorithmically%20addresses%20user%20queries%20regarding%20MLM%20performance%20efficiently%0Awithin%20a%20computational%20budget.%20It%20contains%20113K%20images%2C%2010K%20videos%2C%202K%203D%0Aobject%20assets%2C%20over%20365%20object%20categories%2C%20655%20attributes%2C%20and%20335%0Arelationships.%20It%20can%20generate%20750M%20image/video%20question-answering%20pairs%2C%20which%0Afocus%20on%20evaluating%20MLM%20perceptual%20capabilities.%20Task-Me-Anything%20reveals%0Acritical%20insights%3A%20open-source%20MLMs%20excel%20in%20object%20and%20attribute%20recognition%0Abut%20lack%20spatial%20and%20temporal%20understanding%3B%20each%20model%20exhibits%20unique%0Astrengths%20and%20weaknesses%3B%20larger%20models%20generally%20perform%20better%2C%20though%0Aexceptions%20exist%3B%20and%20GPT4o%20demonstrates%20challenges%20in%20recognizing%0Arotating/moving%20objects%20and%20distinguishing%20colors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11775v1&entry.124074799=Read"},
{"title": "Harmonizing Feature Maps: A Graph Convolutional Approach for Enhancing\n  Adversarial Robustness", "author": "Kejia Zhang and Juanjuan Weng and Junwei Wu and Guoqing Yang and Shaozi Li and Zhiming Luo", "abstract": "  The vulnerability of Deep Neural Networks to adversarial perturbations\npresents significant security concerns, as the imperceptible perturbations can\ncontaminate the feature space and lead to incorrect predictions. Recent studies\nhave attempted to calibrate contaminated features by either suppressing or\nover-activating particular channels. Despite these efforts, we claim that\nadversarial attacks exhibit varying disruption levels across individual\nchannels. Furthermore, we argue that harmonizing feature maps via graph and\nemploying graph convolution can calibrate contaminated features. To this end,\nwe introduce an innovative plug-and-play module called Feature Map-based\nReconstructed Graph Convolution (FMR-GC). FMR-GC harmonizes feature maps in the\nchannel dimension to reconstruct the graph, then employs graph convolution to\ncapture neighborhood information, effectively calibrating contaminated\nfeatures. Extensive experiments have demonstrated the superior performance and\nscalability of FMR-GC. Moreover, our model can be combined with advanced\nadversarial training methods to considerably enhance robustness without\ncompromising the model's clean accuracy.\n", "link": "http://arxiv.org/abs/2406.11576v1", "date": "2024-06-17", "relevancy": 2.5649, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5218}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5104}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harmonizing%20Feature%20Maps%3A%20A%20Graph%20Convolutional%20Approach%20for%20Enhancing%0A%20%20Adversarial%20Robustness&body=Title%3A%20Harmonizing%20Feature%20Maps%3A%20A%20Graph%20Convolutional%20Approach%20for%20Enhancing%0A%20%20Adversarial%20Robustness%0AAuthor%3A%20Kejia%20Zhang%20and%20Juanjuan%20Weng%20and%20Junwei%20Wu%20and%20Guoqing%20Yang%20and%20Shaozi%20Li%20and%20Zhiming%20Luo%0AAbstract%3A%20%20%20The%20vulnerability%20of%20Deep%20Neural%20Networks%20to%20adversarial%20perturbations%0Apresents%20significant%20security%20concerns%2C%20as%20the%20imperceptible%20perturbations%20can%0Acontaminate%20the%20feature%20space%20and%20lead%20to%20incorrect%20predictions.%20Recent%20studies%0Ahave%20attempted%20to%20calibrate%20contaminated%20features%20by%20either%20suppressing%20or%0Aover-activating%20particular%20channels.%20Despite%20these%20efforts%2C%20we%20claim%20that%0Aadversarial%20attacks%20exhibit%20varying%20disruption%20levels%20across%20individual%0Achannels.%20Furthermore%2C%20we%20argue%20that%20harmonizing%20feature%20maps%20via%20graph%20and%0Aemploying%20graph%20convolution%20can%20calibrate%20contaminated%20features.%20To%20this%20end%2C%0Awe%20introduce%20an%20innovative%20plug-and-play%20module%20called%20Feature%20Map-based%0AReconstructed%20Graph%20Convolution%20%28FMR-GC%29.%20FMR-GC%20harmonizes%20feature%20maps%20in%20the%0Achannel%20dimension%20to%20reconstruct%20the%20graph%2C%20then%20employs%20graph%20convolution%20to%0Acapture%20neighborhood%20information%2C%20effectively%20calibrating%20contaminated%0Afeatures.%20Extensive%20experiments%20have%20demonstrated%20the%20superior%20performance%20and%0Ascalability%20of%20FMR-GC.%20Moreover%2C%20our%20model%20can%20be%20combined%20with%20advanced%0Aadversarial%20training%20methods%20to%20considerably%20enhance%20robustness%20without%0Acompromising%20the%20model%27s%20clean%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11576v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarmonizing%2520Feature%2520Maps%253A%2520A%2520Graph%2520Convolutional%2520Approach%2520for%2520Enhancing%250A%2520%2520Adversarial%2520Robustness%26entry.906535625%3DKejia%2520Zhang%2520and%2520Juanjuan%2520Weng%2520and%2520Junwei%2520Wu%2520and%2520Guoqing%2520Yang%2520and%2520Shaozi%2520Li%2520and%2520Zhiming%2520Luo%26entry.1292438233%3D%2520%2520The%2520vulnerability%2520of%2520Deep%2520Neural%2520Networks%2520to%2520adversarial%2520perturbations%250Apresents%2520significant%2520security%2520concerns%252C%2520as%2520the%2520imperceptible%2520perturbations%2520can%250Acontaminate%2520the%2520feature%2520space%2520and%2520lead%2520to%2520incorrect%2520predictions.%2520Recent%2520studies%250Ahave%2520attempted%2520to%2520calibrate%2520contaminated%2520features%2520by%2520either%2520suppressing%2520or%250Aover-activating%2520particular%2520channels.%2520Despite%2520these%2520efforts%252C%2520we%2520claim%2520that%250Aadversarial%2520attacks%2520exhibit%2520varying%2520disruption%2520levels%2520across%2520individual%250Achannels.%2520Furthermore%252C%2520we%2520argue%2520that%2520harmonizing%2520feature%2520maps%2520via%2520graph%2520and%250Aemploying%2520graph%2520convolution%2520can%2520calibrate%2520contaminated%2520features.%2520To%2520this%2520end%252C%250Awe%2520introduce%2520an%2520innovative%2520plug-and-play%2520module%2520called%2520Feature%2520Map-based%250AReconstructed%2520Graph%2520Convolution%2520%2528FMR-GC%2529.%2520FMR-GC%2520harmonizes%2520feature%2520maps%2520in%2520the%250Achannel%2520dimension%2520to%2520reconstruct%2520the%2520graph%252C%2520then%2520employs%2520graph%2520convolution%2520to%250Acapture%2520neighborhood%2520information%252C%2520effectively%2520calibrating%2520contaminated%250Afeatures.%2520Extensive%2520experiments%2520have%2520demonstrated%2520the%2520superior%2520performance%2520and%250Ascalability%2520of%2520FMR-GC.%2520Moreover%252C%2520our%2520model%2520can%2520be%2520combined%2520with%2520advanced%250Aadversarial%2520training%2520methods%2520to%2520considerably%2520enhance%2520robustness%2520without%250Acompromising%2520the%2520model%2527s%2520clean%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11576v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harmonizing%20Feature%20Maps%3A%20A%20Graph%20Convolutional%20Approach%20for%20Enhancing%0A%20%20Adversarial%20Robustness&entry.906535625=Kejia%20Zhang%20and%20Juanjuan%20Weng%20and%20Junwei%20Wu%20and%20Guoqing%20Yang%20and%20Shaozi%20Li%20and%20Zhiming%20Luo&entry.1292438233=%20%20The%20vulnerability%20of%20Deep%20Neural%20Networks%20to%20adversarial%20perturbations%0Apresents%20significant%20security%20concerns%2C%20as%20the%20imperceptible%20perturbations%20can%0Acontaminate%20the%20feature%20space%20and%20lead%20to%20incorrect%20predictions.%20Recent%20studies%0Ahave%20attempted%20to%20calibrate%20contaminated%20features%20by%20either%20suppressing%20or%0Aover-activating%20particular%20channels.%20Despite%20these%20efforts%2C%20we%20claim%20that%0Aadversarial%20attacks%20exhibit%20varying%20disruption%20levels%20across%20individual%0Achannels.%20Furthermore%2C%20we%20argue%20that%20harmonizing%20feature%20maps%20via%20graph%20and%0Aemploying%20graph%20convolution%20can%20calibrate%20contaminated%20features.%20To%20this%20end%2C%0Awe%20introduce%20an%20innovative%20plug-and-play%20module%20called%20Feature%20Map-based%0AReconstructed%20Graph%20Convolution%20%28FMR-GC%29.%20FMR-GC%20harmonizes%20feature%20maps%20in%20the%0Achannel%20dimension%20to%20reconstruct%20the%20graph%2C%20then%20employs%20graph%20convolution%20to%0Acapture%20neighborhood%20information%2C%20effectively%20calibrating%20contaminated%0Afeatures.%20Extensive%20experiments%20have%20demonstrated%20the%20superior%20performance%20and%0Ascalability%20of%20FMR-GC.%20Moreover%2C%20our%20model%20can%20be%20combined%20with%20advanced%0Aadversarial%20training%20methods%20to%20considerably%20enhance%20robustness%20without%0Acompromising%20the%20model%27s%20clean%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11576v1&entry.124074799=Read"},
{"title": "A Clipped Trip: the Dynamics of SGD with Gradient Clipping in\n  High-Dimensions", "author": "Noah Marshall and Ke Liang Xiao and Atish Agarwala and Elliot Paquette", "abstract": "  The success of modern machine learning is due in part to the adaptive\noptimization methods that have been developed to deal with the difficulties of\ntraining large models over complex datasets. One such method is gradient\nclipping: a practical procedure with limited theoretical underpinnings. In this\nwork, we study clipping in a least squares problem under streaming SGD. We\ndevelop a theoretical analysis of the learning dynamics in the limit of large\nintrinsic dimension-a model and dataset dependent notion of dimensionality. In\nthis limit we find a deterministic equation that describes the evolution of the\nloss. We show that with Gaussian noise clipping cannot improve SGD performance.\nYet, in other noisy settings, clipping can provide benefits with tuning of the\nclipping threshold. In these cases, clipping biases updates in a way beneficial\nto training which cannot be recovered by SGD under any schedule. We conclude\nwith a discussion about the links between high-dimensional clipping and neural\nnetwork training.\n", "link": "http://arxiv.org/abs/2406.11733v1", "date": "2024-06-17", "relevancy": 2.5615, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5216}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.51}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5053}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Clipped%20Trip%3A%20the%20Dynamics%20of%20SGD%20with%20Gradient%20Clipping%20in%0A%20%20High-Dimensions&body=Title%3A%20A%20Clipped%20Trip%3A%20the%20Dynamics%20of%20SGD%20with%20Gradient%20Clipping%20in%0A%20%20High-Dimensions%0AAuthor%3A%20Noah%20Marshall%20and%20Ke%20Liang%20Xiao%20and%20Atish%20Agarwala%20and%20Elliot%20Paquette%0AAbstract%3A%20%20%20The%20success%20of%20modern%20machine%20learning%20is%20due%20in%20part%20to%20the%20adaptive%0Aoptimization%20methods%20that%20have%20been%20developed%20to%20deal%20with%20the%20difficulties%20of%0Atraining%20large%20models%20over%20complex%20datasets.%20One%20such%20method%20is%20gradient%0Aclipping%3A%20a%20practical%20procedure%20with%20limited%20theoretical%20underpinnings.%20In%20this%0Awork%2C%20we%20study%20clipping%20in%20a%20least%20squares%20problem%20under%20streaming%20SGD.%20We%0Adevelop%20a%20theoretical%20analysis%20of%20the%20learning%20dynamics%20in%20the%20limit%20of%20large%0Aintrinsic%20dimension-a%20model%20and%20dataset%20dependent%20notion%20of%20dimensionality.%20In%0Athis%20limit%20we%20find%20a%20deterministic%20equation%20that%20describes%20the%20evolution%20of%20the%0Aloss.%20We%20show%20that%20with%20Gaussian%20noise%20clipping%20cannot%20improve%20SGD%20performance.%0AYet%2C%20in%20other%20noisy%20settings%2C%20clipping%20can%20provide%20benefits%20with%20tuning%20of%20the%0Aclipping%20threshold.%20In%20these%20cases%2C%20clipping%20biases%20updates%20in%20a%20way%20beneficial%0Ato%20training%20which%20cannot%20be%20recovered%20by%20SGD%20under%20any%20schedule.%20We%20conclude%0Awith%20a%20discussion%20about%20the%20links%20between%20high-dimensional%20clipping%20and%20neural%0Anetwork%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11733v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Clipped%2520Trip%253A%2520the%2520Dynamics%2520of%2520SGD%2520with%2520Gradient%2520Clipping%2520in%250A%2520%2520High-Dimensions%26entry.906535625%3DNoah%2520Marshall%2520and%2520Ke%2520Liang%2520Xiao%2520and%2520Atish%2520Agarwala%2520and%2520Elliot%2520Paquette%26entry.1292438233%3D%2520%2520The%2520success%2520of%2520modern%2520machine%2520learning%2520is%2520due%2520in%2520part%2520to%2520the%2520adaptive%250Aoptimization%2520methods%2520that%2520have%2520been%2520developed%2520to%2520deal%2520with%2520the%2520difficulties%2520of%250Atraining%2520large%2520models%2520over%2520complex%2520datasets.%2520One%2520such%2520method%2520is%2520gradient%250Aclipping%253A%2520a%2520practical%2520procedure%2520with%2520limited%2520theoretical%2520underpinnings.%2520In%2520this%250Awork%252C%2520we%2520study%2520clipping%2520in%2520a%2520least%2520squares%2520problem%2520under%2520streaming%2520SGD.%2520We%250Adevelop%2520a%2520theoretical%2520analysis%2520of%2520the%2520learning%2520dynamics%2520in%2520the%2520limit%2520of%2520large%250Aintrinsic%2520dimension-a%2520model%2520and%2520dataset%2520dependent%2520notion%2520of%2520dimensionality.%2520In%250Athis%2520limit%2520we%2520find%2520a%2520deterministic%2520equation%2520that%2520describes%2520the%2520evolution%2520of%2520the%250Aloss.%2520We%2520show%2520that%2520with%2520Gaussian%2520noise%2520clipping%2520cannot%2520improve%2520SGD%2520performance.%250AYet%252C%2520in%2520other%2520noisy%2520settings%252C%2520clipping%2520can%2520provide%2520benefits%2520with%2520tuning%2520of%2520the%250Aclipping%2520threshold.%2520In%2520these%2520cases%252C%2520clipping%2520biases%2520updates%2520in%2520a%2520way%2520beneficial%250Ato%2520training%2520which%2520cannot%2520be%2520recovered%2520by%2520SGD%2520under%2520any%2520schedule.%2520We%2520conclude%250Awith%2520a%2520discussion%2520about%2520the%2520links%2520between%2520high-dimensional%2520clipping%2520and%2520neural%250Anetwork%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11733v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Clipped%20Trip%3A%20the%20Dynamics%20of%20SGD%20with%20Gradient%20Clipping%20in%0A%20%20High-Dimensions&entry.906535625=Noah%20Marshall%20and%20Ke%20Liang%20Xiao%20and%20Atish%20Agarwala%20and%20Elliot%20Paquette&entry.1292438233=%20%20The%20success%20of%20modern%20machine%20learning%20is%20due%20in%20part%20to%20the%20adaptive%0Aoptimization%20methods%20that%20have%20been%20developed%20to%20deal%20with%20the%20difficulties%20of%0Atraining%20large%20models%20over%20complex%20datasets.%20One%20such%20method%20is%20gradient%0Aclipping%3A%20a%20practical%20procedure%20with%20limited%20theoretical%20underpinnings.%20In%20this%0Awork%2C%20we%20study%20clipping%20in%20a%20least%20squares%20problem%20under%20streaming%20SGD.%20We%0Adevelop%20a%20theoretical%20analysis%20of%20the%20learning%20dynamics%20in%20the%20limit%20of%20large%0Aintrinsic%20dimension-a%20model%20and%20dataset%20dependent%20notion%20of%20dimensionality.%20In%0Athis%20limit%20we%20find%20a%20deterministic%20equation%20that%20describes%20the%20evolution%20of%20the%0Aloss.%20We%20show%20that%20with%20Gaussian%20noise%20clipping%20cannot%20improve%20SGD%20performance.%0AYet%2C%20in%20other%20noisy%20settings%2C%20clipping%20can%20provide%20benefits%20with%20tuning%20of%20the%0Aclipping%20threshold.%20In%20these%20cases%2C%20clipping%20biases%20updates%20in%20a%20way%20beneficial%0Ato%20training%20which%20cannot%20be%20recovered%20by%20SGD%20under%20any%20schedule.%20We%20conclude%0Awith%20a%20discussion%20about%20the%20links%20between%20high-dimensional%20clipping%20and%20neural%0Anetwork%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11733v1&entry.124074799=Read"},
{"title": "Cross-domain Open-world Discovery", "author": "Shuo Wen and Maria Brbic", "abstract": "  In many real-world applications, test data may commonly exhibit categorical\nshifts, characterized by the emergence of novel classes, as well as\ndistribution shifts arising from feature distributions different from the ones\nthe model was trained on. However, existing methods either discover novel\nclasses in the open-world setting or assume domain shifts without the ability\nto discover novel classes. In this work, we consider a cross-domain open-world\ndiscovery setting, where the goal is to assign samples to seen classes and\ndiscover unseen classes under a domain shift. To address this challenging\nproblem, we present CROW, a prototype-based approach that introduces a\ncluster-then-match strategy enabled by a well-structured representation space\nof foundation models. In this way, CROW discovers novel classes by robustly\nmatching clusters with previously seen classes, followed by fine-tuning the\nrepresentation space using an objective designed for cross-domain open-world\ndiscovery. Extensive experimental results on image classification benchmark\ndatasets demonstrate that CROW outperforms alternative baselines, achieving an\n8% average performance improvement across 75 experimental settings.\n", "link": "http://arxiv.org/abs/2406.11422v1", "date": "2024-06-17", "relevancy": 2.5486, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5185}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5077}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.503}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-domain%20Open-world%20Discovery&body=Title%3A%20Cross-domain%20Open-world%20Discovery%0AAuthor%3A%20Shuo%20Wen%20and%20Maria%20Brbic%0AAbstract%3A%20%20%20In%20many%20real-world%20applications%2C%20test%20data%20may%20commonly%20exhibit%20categorical%0Ashifts%2C%20characterized%20by%20the%20emergence%20of%20novel%20classes%2C%20as%20well%20as%0Adistribution%20shifts%20arising%20from%20feature%20distributions%20different%20from%20the%20ones%0Athe%20model%20was%20trained%20on.%20However%2C%20existing%20methods%20either%20discover%20novel%0Aclasses%20in%20the%20open-world%20setting%20or%20assume%20domain%20shifts%20without%20the%20ability%0Ato%20discover%20novel%20classes.%20In%20this%20work%2C%20we%20consider%20a%20cross-domain%20open-world%0Adiscovery%20setting%2C%20where%20the%20goal%20is%20to%20assign%20samples%20to%20seen%20classes%20and%0Adiscover%20unseen%20classes%20under%20a%20domain%20shift.%20To%20address%20this%20challenging%0Aproblem%2C%20we%20present%20CROW%2C%20a%20prototype-based%20approach%20that%20introduces%20a%0Acluster-then-match%20strategy%20enabled%20by%20a%20well-structured%20representation%20space%0Aof%20foundation%20models.%20In%20this%20way%2C%20CROW%20discovers%20novel%20classes%20by%20robustly%0Amatching%20clusters%20with%20previously%20seen%20classes%2C%20followed%20by%20fine-tuning%20the%0Arepresentation%20space%20using%20an%20objective%20designed%20for%20cross-domain%20open-world%0Adiscovery.%20Extensive%20experimental%20results%20on%20image%20classification%20benchmark%0Adatasets%20demonstrate%20that%20CROW%20outperforms%20alternative%20baselines%2C%20achieving%20an%0A8%25%20average%20performance%20improvement%20across%2075%20experimental%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11422v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-domain%2520Open-world%2520Discovery%26entry.906535625%3DShuo%2520Wen%2520and%2520Maria%2520Brbic%26entry.1292438233%3D%2520%2520In%2520many%2520real-world%2520applications%252C%2520test%2520data%2520may%2520commonly%2520exhibit%2520categorical%250Ashifts%252C%2520characterized%2520by%2520the%2520emergence%2520of%2520novel%2520classes%252C%2520as%2520well%2520as%250Adistribution%2520shifts%2520arising%2520from%2520feature%2520distributions%2520different%2520from%2520the%2520ones%250Athe%2520model%2520was%2520trained%2520on.%2520However%252C%2520existing%2520methods%2520either%2520discover%2520novel%250Aclasses%2520in%2520the%2520open-world%2520setting%2520or%2520assume%2520domain%2520shifts%2520without%2520the%2520ability%250Ato%2520discover%2520novel%2520classes.%2520In%2520this%2520work%252C%2520we%2520consider%2520a%2520cross-domain%2520open-world%250Adiscovery%2520setting%252C%2520where%2520the%2520goal%2520is%2520to%2520assign%2520samples%2520to%2520seen%2520classes%2520and%250Adiscover%2520unseen%2520classes%2520under%2520a%2520domain%2520shift.%2520To%2520address%2520this%2520challenging%250Aproblem%252C%2520we%2520present%2520CROW%252C%2520a%2520prototype-based%2520approach%2520that%2520introduces%2520a%250Acluster-then-match%2520strategy%2520enabled%2520by%2520a%2520well-structured%2520representation%2520space%250Aof%2520foundation%2520models.%2520In%2520this%2520way%252C%2520CROW%2520discovers%2520novel%2520classes%2520by%2520robustly%250Amatching%2520clusters%2520with%2520previously%2520seen%2520classes%252C%2520followed%2520by%2520fine-tuning%2520the%250Arepresentation%2520space%2520using%2520an%2520objective%2520designed%2520for%2520cross-domain%2520open-world%250Adiscovery.%2520Extensive%2520experimental%2520results%2520on%2520image%2520classification%2520benchmark%250Adatasets%2520demonstrate%2520that%2520CROW%2520outperforms%2520alternative%2520baselines%252C%2520achieving%2520an%250A8%2525%2520average%2520performance%2520improvement%2520across%252075%2520experimental%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11422v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-domain%20Open-world%20Discovery&entry.906535625=Shuo%20Wen%20and%20Maria%20Brbic&entry.1292438233=%20%20In%20many%20real-world%20applications%2C%20test%20data%20may%20commonly%20exhibit%20categorical%0Ashifts%2C%20characterized%20by%20the%20emergence%20of%20novel%20classes%2C%20as%20well%20as%0Adistribution%20shifts%20arising%20from%20feature%20distributions%20different%20from%20the%20ones%0Athe%20model%20was%20trained%20on.%20However%2C%20existing%20methods%20either%20discover%20novel%0Aclasses%20in%20the%20open-world%20setting%20or%20assume%20domain%20shifts%20without%20the%20ability%0Ato%20discover%20novel%20classes.%20In%20this%20work%2C%20we%20consider%20a%20cross-domain%20open-world%0Adiscovery%20setting%2C%20where%20the%20goal%20is%20to%20assign%20samples%20to%20seen%20classes%20and%0Adiscover%20unseen%20classes%20under%20a%20domain%20shift.%20To%20address%20this%20challenging%0Aproblem%2C%20we%20present%20CROW%2C%20a%20prototype-based%20approach%20that%20introduces%20a%0Acluster-then-match%20strategy%20enabled%20by%20a%20well-structured%20representation%20space%0Aof%20foundation%20models.%20In%20this%20way%2C%20CROW%20discovers%20novel%20classes%20by%20robustly%0Amatching%20clusters%20with%20previously%20seen%20classes%2C%20followed%20by%20fine-tuning%20the%0Arepresentation%20space%20using%20an%20objective%20designed%20for%20cross-domain%20open-world%0Adiscovery.%20Extensive%20experimental%20results%20on%20image%20classification%20benchmark%0Adatasets%20demonstrate%20that%20CROW%20outperforms%20alternative%20baselines%2C%20achieving%20an%0A8%25%20average%20performance%20improvement%20across%2075%20experimental%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11422v1&entry.124074799=Read"},
{"title": "Unveiling Encoder-Free Vision-Language Models", "author": "Haiwen Diao and Yufeng Cui and Xiaotong Li and Yueze Wang and Huchuan Lu and Xinlong Wang", "abstract": "  Existing vision-language models (VLMs) mostly rely on vision encoders to\nextract visual features followed by large language models (LLMs) for\nvisual-language tasks. However, the vision encoders set a strong inductive bias\nin abstracting visual representation, e.g., resolution, aspect ratio, and\nsemantic priors, which could impede the flexibility and efficiency of the VLMs.\nTraining pure VLMs that accept the seamless vision and language inputs, i.e.,\nwithout vision encoders, remains challenging and rarely explored. Empirical\nobservations reveal that direct training without encoders results in slow\nconvergence and large performance gaps. In this work, we bridge the gap between\nencoder-based and encoder-free models, and present a simple yet effective\ntraining recipe towards pure VLMs. Specifically, we unveil the key aspects of\ntraining encoder-free VLMs efficiently via thorough experiments: (1) Bridging\nvision-language representation inside one unified decoder; (2) Enhancing visual\nrecognition capability via extra supervision. With these strategies, we launch\nEVE, an encoder-free vision-language model that can be trained and forwarded\nefficiently. Notably, solely utilizing 35M publicly accessible data, EVE can\nimpressively rival the encoder-based VLMs of similar capacities across multiple\nvision-language benchmarks. It significantly outperforms the counterpart\nFuyu-8B with mysterious training procedures and undisclosed training data. We\nbelieve that EVE provides a transparent and efficient route for developing a\npure decoder-only architecture across modalities. Our code and models are\npublicly available at: https://github.com/baaivision/EVE.\n", "link": "http://arxiv.org/abs/2406.11832v1", "date": "2024-06-17", "relevancy": 2.5433, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5099}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5085}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5076}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20Encoder-Free%20Vision-Language%20Models&body=Title%3A%20Unveiling%20Encoder-Free%20Vision-Language%20Models%0AAuthor%3A%20Haiwen%20Diao%20and%20Yufeng%20Cui%20and%20Xiaotong%20Li%20and%20Yueze%20Wang%20and%20Huchuan%20Lu%20and%20Xinlong%20Wang%0AAbstract%3A%20%20%20Existing%20vision-language%20models%20%28VLMs%29%20mostly%20rely%20on%20vision%20encoders%20to%0Aextract%20visual%20features%20followed%20by%20large%20language%20models%20%28LLMs%29%20for%0Avisual-language%20tasks.%20However%2C%20the%20vision%20encoders%20set%20a%20strong%20inductive%20bias%0Ain%20abstracting%20visual%20representation%2C%20e.g.%2C%20resolution%2C%20aspect%20ratio%2C%20and%0Asemantic%20priors%2C%20which%20could%20impede%20the%20flexibility%20and%20efficiency%20of%20the%20VLMs.%0ATraining%20pure%20VLMs%20that%20accept%20the%20seamless%20vision%20and%20language%20inputs%2C%20i.e.%2C%0Awithout%20vision%20encoders%2C%20remains%20challenging%20and%20rarely%20explored.%20Empirical%0Aobservations%20reveal%20that%20direct%20training%20without%20encoders%20results%20in%20slow%0Aconvergence%20and%20large%20performance%20gaps.%20In%20this%20work%2C%20we%20bridge%20the%20gap%20between%0Aencoder-based%20and%20encoder-free%20models%2C%20and%20present%20a%20simple%20yet%20effective%0Atraining%20recipe%20towards%20pure%20VLMs.%20Specifically%2C%20we%20unveil%20the%20key%20aspects%20of%0Atraining%20encoder-free%20VLMs%20efficiently%20via%20thorough%20experiments%3A%20%281%29%20Bridging%0Avision-language%20representation%20inside%20one%20unified%20decoder%3B%20%282%29%20Enhancing%20visual%0Arecognition%20capability%20via%20extra%20supervision.%20With%20these%20strategies%2C%20we%20launch%0AEVE%2C%20an%20encoder-free%20vision-language%20model%20that%20can%20be%20trained%20and%20forwarded%0Aefficiently.%20Notably%2C%20solely%20utilizing%2035M%20publicly%20accessible%20data%2C%20EVE%20can%0Aimpressively%20rival%20the%20encoder-based%20VLMs%20of%20similar%20capacities%20across%20multiple%0Avision-language%20benchmarks.%20It%20significantly%20outperforms%20the%20counterpart%0AFuyu-8B%20with%20mysterious%20training%20procedures%20and%20undisclosed%20training%20data.%20We%0Abelieve%20that%20EVE%20provides%20a%20transparent%20and%20efficient%20route%20for%20developing%20a%0Apure%20decoder-only%20architecture%20across%20modalities.%20Our%20code%20and%20models%20are%0Apublicly%20available%20at%3A%20https%3A//github.com/baaivision/EVE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11832v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520Encoder-Free%2520Vision-Language%2520Models%26entry.906535625%3DHaiwen%2520Diao%2520and%2520Yufeng%2520Cui%2520and%2520Xiaotong%2520Li%2520and%2520Yueze%2520Wang%2520and%2520Huchuan%2520Lu%2520and%2520Xinlong%2520Wang%26entry.1292438233%3D%2520%2520Existing%2520vision-language%2520models%2520%2528VLMs%2529%2520mostly%2520rely%2520on%2520vision%2520encoders%2520to%250Aextract%2520visual%2520features%2520followed%2520by%2520large%2520language%2520models%2520%2528LLMs%2529%2520for%250Avisual-language%2520tasks.%2520However%252C%2520the%2520vision%2520encoders%2520set%2520a%2520strong%2520inductive%2520bias%250Ain%2520abstracting%2520visual%2520representation%252C%2520e.g.%252C%2520resolution%252C%2520aspect%2520ratio%252C%2520and%250Asemantic%2520priors%252C%2520which%2520could%2520impede%2520the%2520flexibility%2520and%2520efficiency%2520of%2520the%2520VLMs.%250ATraining%2520pure%2520VLMs%2520that%2520accept%2520the%2520seamless%2520vision%2520and%2520language%2520inputs%252C%2520i.e.%252C%250Awithout%2520vision%2520encoders%252C%2520remains%2520challenging%2520and%2520rarely%2520explored.%2520Empirical%250Aobservations%2520reveal%2520that%2520direct%2520training%2520without%2520encoders%2520results%2520in%2520slow%250Aconvergence%2520and%2520large%2520performance%2520gaps.%2520In%2520this%2520work%252C%2520we%2520bridge%2520the%2520gap%2520between%250Aencoder-based%2520and%2520encoder-free%2520models%252C%2520and%2520present%2520a%2520simple%2520yet%2520effective%250Atraining%2520recipe%2520towards%2520pure%2520VLMs.%2520Specifically%252C%2520we%2520unveil%2520the%2520key%2520aspects%2520of%250Atraining%2520encoder-free%2520VLMs%2520efficiently%2520via%2520thorough%2520experiments%253A%2520%25281%2529%2520Bridging%250Avision-language%2520representation%2520inside%2520one%2520unified%2520decoder%253B%2520%25282%2529%2520Enhancing%2520visual%250Arecognition%2520capability%2520via%2520extra%2520supervision.%2520With%2520these%2520strategies%252C%2520we%2520launch%250AEVE%252C%2520an%2520encoder-free%2520vision-language%2520model%2520that%2520can%2520be%2520trained%2520and%2520forwarded%250Aefficiently.%2520Notably%252C%2520solely%2520utilizing%252035M%2520publicly%2520accessible%2520data%252C%2520EVE%2520can%250Aimpressively%2520rival%2520the%2520encoder-based%2520VLMs%2520of%2520similar%2520capacities%2520across%2520multiple%250Avision-language%2520benchmarks.%2520It%2520significantly%2520outperforms%2520the%2520counterpart%250AFuyu-8B%2520with%2520mysterious%2520training%2520procedures%2520and%2520undisclosed%2520training%2520data.%2520We%250Abelieve%2520that%2520EVE%2520provides%2520a%2520transparent%2520and%2520efficient%2520route%2520for%2520developing%2520a%250Apure%2520decoder-only%2520architecture%2520across%2520modalities.%2520Our%2520code%2520and%2520models%2520are%250Apublicly%2520available%2520at%253A%2520https%253A//github.com/baaivision/EVE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11832v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20Encoder-Free%20Vision-Language%20Models&entry.906535625=Haiwen%20Diao%20and%20Yufeng%20Cui%20and%20Xiaotong%20Li%20and%20Yueze%20Wang%20and%20Huchuan%20Lu%20and%20Xinlong%20Wang&entry.1292438233=%20%20Existing%20vision-language%20models%20%28VLMs%29%20mostly%20rely%20on%20vision%20encoders%20to%0Aextract%20visual%20features%20followed%20by%20large%20language%20models%20%28LLMs%29%20for%0Avisual-language%20tasks.%20However%2C%20the%20vision%20encoders%20set%20a%20strong%20inductive%20bias%0Ain%20abstracting%20visual%20representation%2C%20e.g.%2C%20resolution%2C%20aspect%20ratio%2C%20and%0Asemantic%20priors%2C%20which%20could%20impede%20the%20flexibility%20and%20efficiency%20of%20the%20VLMs.%0ATraining%20pure%20VLMs%20that%20accept%20the%20seamless%20vision%20and%20language%20inputs%2C%20i.e.%2C%0Awithout%20vision%20encoders%2C%20remains%20challenging%20and%20rarely%20explored.%20Empirical%0Aobservations%20reveal%20that%20direct%20training%20without%20encoders%20results%20in%20slow%0Aconvergence%20and%20large%20performance%20gaps.%20In%20this%20work%2C%20we%20bridge%20the%20gap%20between%0Aencoder-based%20and%20encoder-free%20models%2C%20and%20present%20a%20simple%20yet%20effective%0Atraining%20recipe%20towards%20pure%20VLMs.%20Specifically%2C%20we%20unveil%20the%20key%20aspects%20of%0Atraining%20encoder-free%20VLMs%20efficiently%20via%20thorough%20experiments%3A%20%281%29%20Bridging%0Avision-language%20representation%20inside%20one%20unified%20decoder%3B%20%282%29%20Enhancing%20visual%0Arecognition%20capability%20via%20extra%20supervision.%20With%20these%20strategies%2C%20we%20launch%0AEVE%2C%20an%20encoder-free%20vision-language%20model%20that%20can%20be%20trained%20and%20forwarded%0Aefficiently.%20Notably%2C%20solely%20utilizing%2035M%20publicly%20accessible%20data%2C%20EVE%20can%0Aimpressively%20rival%20the%20encoder-based%20VLMs%20of%20similar%20capacities%20across%20multiple%0Avision-language%20benchmarks.%20It%20significantly%20outperforms%20the%20counterpart%0AFuyu-8B%20with%20mysterious%20training%20procedures%20and%20undisclosed%20training%20data.%20We%0Abelieve%20that%20EVE%20provides%20a%20transparent%20and%20efficient%20route%20for%20developing%20a%0Apure%20decoder-only%20architecture%20across%20modalities.%20Our%20code%20and%20models%20are%0Apublicly%20available%20at%3A%20https%3A//github.com/baaivision/EVE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11832v1&entry.124074799=Read"},
{"title": "YOLO-FEDER FusionNet: A Novel Deep Learning Architecture for Drone\n  Detection", "author": "Tamara R. Lenhard and Andreas Weinmann and Stefan J\u00e4ger and Tobias Koch", "abstract": "  Predominant methods for image-based drone detection frequently rely on\nemploying generic object detection algorithms like YOLOv5. While proficient in\nidentifying drones against homogeneous backgrounds, these algorithms often\nstruggle in complex, highly textured environments. In such scenarios, drones\nseamlessly integrate into the background, creating camouflage effects that\nadversely affect the detection quality. To address this issue, we introduce a\nnovel deep learning architecture called YOLO-FEDER FusionNet. Unlike\nconventional approaches, YOLO-FEDER FusionNet combines generic object detection\nmethods with the specialized strength of camouflage object detection techniques\nto enhance drone detection capabilities. Comprehensive evaluations of\nYOLO-FEDER FusionNet show the efficiency of the proposed model and demonstrate\nsubstantial improvements in both reducing missed detections and false alarms.\n", "link": "http://arxiv.org/abs/2406.11641v1", "date": "2024-06-17", "relevancy": 2.5395, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5107}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5075}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5054}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20YOLO-FEDER%20FusionNet%3A%20A%20Novel%20Deep%20Learning%20Architecture%20for%20Drone%0A%20%20Detection&body=Title%3A%20YOLO-FEDER%20FusionNet%3A%20A%20Novel%20Deep%20Learning%20Architecture%20for%20Drone%0A%20%20Detection%0AAuthor%3A%20Tamara%20R.%20Lenhard%20and%20Andreas%20Weinmann%20and%20Stefan%20J%C3%A4ger%20and%20Tobias%20Koch%0AAbstract%3A%20%20%20Predominant%20methods%20for%20image-based%20drone%20detection%20frequently%20rely%20on%0Aemploying%20generic%20object%20detection%20algorithms%20like%20YOLOv5.%20While%20proficient%20in%0Aidentifying%20drones%20against%20homogeneous%20backgrounds%2C%20these%20algorithms%20often%0Astruggle%20in%20complex%2C%20highly%20textured%20environments.%20In%20such%20scenarios%2C%20drones%0Aseamlessly%20integrate%20into%20the%20background%2C%20creating%20camouflage%20effects%20that%0Aadversely%20affect%20the%20detection%20quality.%20To%20address%20this%20issue%2C%20we%20introduce%20a%0Anovel%20deep%20learning%20architecture%20called%20YOLO-FEDER%20FusionNet.%20Unlike%0Aconventional%20approaches%2C%20YOLO-FEDER%20FusionNet%20combines%20generic%20object%20detection%0Amethods%20with%20the%20specialized%20strength%20of%20camouflage%20object%20detection%20techniques%0Ato%20enhance%20drone%20detection%20capabilities.%20Comprehensive%20evaluations%20of%0AYOLO-FEDER%20FusionNet%20show%20the%20efficiency%20of%20the%20proposed%20model%20and%20demonstrate%0Asubstantial%20improvements%20in%20both%20reducing%20missed%20detections%20and%20false%20alarms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11641v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYOLO-FEDER%2520FusionNet%253A%2520A%2520Novel%2520Deep%2520Learning%2520Architecture%2520for%2520Drone%250A%2520%2520Detection%26entry.906535625%3DTamara%2520R.%2520Lenhard%2520and%2520Andreas%2520Weinmann%2520and%2520Stefan%2520J%25C3%25A4ger%2520and%2520Tobias%2520Koch%26entry.1292438233%3D%2520%2520Predominant%2520methods%2520for%2520image-based%2520drone%2520detection%2520frequently%2520rely%2520on%250Aemploying%2520generic%2520object%2520detection%2520algorithms%2520like%2520YOLOv5.%2520While%2520proficient%2520in%250Aidentifying%2520drones%2520against%2520homogeneous%2520backgrounds%252C%2520these%2520algorithms%2520often%250Astruggle%2520in%2520complex%252C%2520highly%2520textured%2520environments.%2520In%2520such%2520scenarios%252C%2520drones%250Aseamlessly%2520integrate%2520into%2520the%2520background%252C%2520creating%2520camouflage%2520effects%2520that%250Aadversely%2520affect%2520the%2520detection%2520quality.%2520To%2520address%2520this%2520issue%252C%2520we%2520introduce%2520a%250Anovel%2520deep%2520learning%2520architecture%2520called%2520YOLO-FEDER%2520FusionNet.%2520Unlike%250Aconventional%2520approaches%252C%2520YOLO-FEDER%2520FusionNet%2520combines%2520generic%2520object%2520detection%250Amethods%2520with%2520the%2520specialized%2520strength%2520of%2520camouflage%2520object%2520detection%2520techniques%250Ato%2520enhance%2520drone%2520detection%2520capabilities.%2520Comprehensive%2520evaluations%2520of%250AYOLO-FEDER%2520FusionNet%2520show%2520the%2520efficiency%2520of%2520the%2520proposed%2520model%2520and%2520demonstrate%250Asubstantial%2520improvements%2520in%2520both%2520reducing%2520missed%2520detections%2520and%2520false%2520alarms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11641v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=YOLO-FEDER%20FusionNet%3A%20A%20Novel%20Deep%20Learning%20Architecture%20for%20Drone%0A%20%20Detection&entry.906535625=Tamara%20R.%20Lenhard%20and%20Andreas%20Weinmann%20and%20Stefan%20J%C3%A4ger%20and%20Tobias%20Koch&entry.1292438233=%20%20Predominant%20methods%20for%20image-based%20drone%20detection%20frequently%20rely%20on%0Aemploying%20generic%20object%20detection%20algorithms%20like%20YOLOv5.%20While%20proficient%20in%0Aidentifying%20drones%20against%20homogeneous%20backgrounds%2C%20these%20algorithms%20often%0Astruggle%20in%20complex%2C%20highly%20textured%20environments.%20In%20such%20scenarios%2C%20drones%0Aseamlessly%20integrate%20into%20the%20background%2C%20creating%20camouflage%20effects%20that%0Aadversely%20affect%20the%20detection%20quality.%20To%20address%20this%20issue%2C%20we%20introduce%20a%0Anovel%20deep%20learning%20architecture%20called%20YOLO-FEDER%20FusionNet.%20Unlike%0Aconventional%20approaches%2C%20YOLO-FEDER%20FusionNet%20combines%20generic%20object%20detection%0Amethods%20with%20the%20specialized%20strength%20of%20camouflage%20object%20detection%20techniques%0Ato%20enhance%20drone%20detection%20capabilities.%20Comprehensive%20evaluations%20of%0AYOLO-FEDER%20FusionNet%20show%20the%20efficiency%20of%20the%20proposed%20model%20and%20demonstrate%0Asubstantial%20improvements%20in%20both%20reducing%20missed%20detections%20and%20false%20alarms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11641v1&entry.124074799=Read"},
{"title": "MADA: Meta-Adaptive Optimizers through hyper-gradient Descent", "author": "Kaan Ozkara and Can Karakus and Parameswaran Raman and Mingyi Hong and Shoham Sabach and Branislav Kveton and Volkan Cevher", "abstract": "  Following the introduction of Adam, several novel adaptive optimizers for\ndeep learning have been proposed. These optimizers typically excel in some\ntasks but may not outperform Adam uniformly across all tasks. In this work, we\nintroduce Meta-Adaptive Optimizers (MADA), a unified optimizer framework that\ncan generalize several known optimizers and dynamically learn the most suitable\none during training. The key idea in MADA is to parameterize the space of\noptimizers and dynamically search through it using hyper-gradient descent\nduring training. We empirically compare MADA to other popular optimizers on\nvision and language tasks, and find that MADA consistently outperforms Adam and\nother popular optimizers, and is robust against sub-optimally tuned\nhyper-parameters. MADA achieves a greater validation performance improvement\nover Adam compared to other popular optimizers during GPT-2 training and\nfine-tuning. We also propose AVGrad, a modification of AMSGrad that replaces\nthe maximum operator with averaging, which is more suitable for hyper-gradient\noptimization. Finally, we provide a convergence analysis to show that\nparameterized interpolations of optimizers can improve their error bounds (up\nto constants), hinting at an advantage for meta-optimizers.\n", "link": "http://arxiv.org/abs/2401.08893v3", "date": "2024-06-17", "relevancy": 2.5321, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5141}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5058}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4994}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MADA%3A%20Meta-Adaptive%20Optimizers%20through%20hyper-gradient%20Descent&body=Title%3A%20MADA%3A%20Meta-Adaptive%20Optimizers%20through%20hyper-gradient%20Descent%0AAuthor%3A%20Kaan%20Ozkara%20and%20Can%20Karakus%20and%20Parameswaran%20Raman%20and%20Mingyi%20Hong%20and%20Shoham%20Sabach%20and%20Branislav%20Kveton%20and%20Volkan%20Cevher%0AAbstract%3A%20%20%20Following%20the%20introduction%20of%20Adam%2C%20several%20novel%20adaptive%20optimizers%20for%0Adeep%20learning%20have%20been%20proposed.%20These%20optimizers%20typically%20excel%20in%20some%0Atasks%20but%20may%20not%20outperform%20Adam%20uniformly%20across%20all%20tasks.%20In%20this%20work%2C%20we%0Aintroduce%20Meta-Adaptive%20Optimizers%20%28MADA%29%2C%20a%20unified%20optimizer%20framework%20that%0Acan%20generalize%20several%20known%20optimizers%20and%20dynamically%20learn%20the%20most%20suitable%0Aone%20during%20training.%20The%20key%20idea%20in%20MADA%20is%20to%20parameterize%20the%20space%20of%0Aoptimizers%20and%20dynamically%20search%20through%20it%20using%20hyper-gradient%20descent%0Aduring%20training.%20We%20empirically%20compare%20MADA%20to%20other%20popular%20optimizers%20on%0Avision%20and%20language%20tasks%2C%20and%20find%20that%20MADA%20consistently%20outperforms%20Adam%20and%0Aother%20popular%20optimizers%2C%20and%20is%20robust%20against%20sub-optimally%20tuned%0Ahyper-parameters.%20MADA%20achieves%20a%20greater%20validation%20performance%20improvement%0Aover%20Adam%20compared%20to%20other%20popular%20optimizers%20during%20GPT-2%20training%20and%0Afine-tuning.%20We%20also%20propose%20AVGrad%2C%20a%20modification%20of%20AMSGrad%20that%20replaces%0Athe%20maximum%20operator%20with%20averaging%2C%20which%20is%20more%20suitable%20for%20hyper-gradient%0Aoptimization.%20Finally%2C%20we%20provide%20a%20convergence%20analysis%20to%20show%20that%0Aparameterized%20interpolations%20of%20optimizers%20can%20improve%20their%20error%20bounds%20%28up%0Ato%20constants%29%2C%20hinting%20at%20an%20advantage%20for%20meta-optimizers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.08893v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMADA%253A%2520Meta-Adaptive%2520Optimizers%2520through%2520hyper-gradient%2520Descent%26entry.906535625%3DKaan%2520Ozkara%2520and%2520Can%2520Karakus%2520and%2520Parameswaran%2520Raman%2520and%2520Mingyi%2520Hong%2520and%2520Shoham%2520Sabach%2520and%2520Branislav%2520Kveton%2520and%2520Volkan%2520Cevher%26entry.1292438233%3D%2520%2520Following%2520the%2520introduction%2520of%2520Adam%252C%2520several%2520novel%2520adaptive%2520optimizers%2520for%250Adeep%2520learning%2520have%2520been%2520proposed.%2520These%2520optimizers%2520typically%2520excel%2520in%2520some%250Atasks%2520but%2520may%2520not%2520outperform%2520Adam%2520uniformly%2520across%2520all%2520tasks.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520Meta-Adaptive%2520Optimizers%2520%2528MADA%2529%252C%2520a%2520unified%2520optimizer%2520framework%2520that%250Acan%2520generalize%2520several%2520known%2520optimizers%2520and%2520dynamically%2520learn%2520the%2520most%2520suitable%250Aone%2520during%2520training.%2520The%2520key%2520idea%2520in%2520MADA%2520is%2520to%2520parameterize%2520the%2520space%2520of%250Aoptimizers%2520and%2520dynamically%2520search%2520through%2520it%2520using%2520hyper-gradient%2520descent%250Aduring%2520training.%2520We%2520empirically%2520compare%2520MADA%2520to%2520other%2520popular%2520optimizers%2520on%250Avision%2520and%2520language%2520tasks%252C%2520and%2520find%2520that%2520MADA%2520consistently%2520outperforms%2520Adam%2520and%250Aother%2520popular%2520optimizers%252C%2520and%2520is%2520robust%2520against%2520sub-optimally%2520tuned%250Ahyper-parameters.%2520MADA%2520achieves%2520a%2520greater%2520validation%2520performance%2520improvement%250Aover%2520Adam%2520compared%2520to%2520other%2520popular%2520optimizers%2520during%2520GPT-2%2520training%2520and%250Afine-tuning.%2520We%2520also%2520propose%2520AVGrad%252C%2520a%2520modification%2520of%2520AMSGrad%2520that%2520replaces%250Athe%2520maximum%2520operator%2520with%2520averaging%252C%2520which%2520is%2520more%2520suitable%2520for%2520hyper-gradient%250Aoptimization.%2520Finally%252C%2520we%2520provide%2520a%2520convergence%2520analysis%2520to%2520show%2520that%250Aparameterized%2520interpolations%2520of%2520optimizers%2520can%2520improve%2520their%2520error%2520bounds%2520%2528up%250Ato%2520constants%2529%252C%2520hinting%2520at%2520an%2520advantage%2520for%2520meta-optimizers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.08893v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MADA%3A%20Meta-Adaptive%20Optimizers%20through%20hyper-gradient%20Descent&entry.906535625=Kaan%20Ozkara%20and%20Can%20Karakus%20and%20Parameswaran%20Raman%20and%20Mingyi%20Hong%20and%20Shoham%20Sabach%20and%20Branislav%20Kveton%20and%20Volkan%20Cevher&entry.1292438233=%20%20Following%20the%20introduction%20of%20Adam%2C%20several%20novel%20adaptive%20optimizers%20for%0Adeep%20learning%20have%20been%20proposed.%20These%20optimizers%20typically%20excel%20in%20some%0Atasks%20but%20may%20not%20outperform%20Adam%20uniformly%20across%20all%20tasks.%20In%20this%20work%2C%20we%0Aintroduce%20Meta-Adaptive%20Optimizers%20%28MADA%29%2C%20a%20unified%20optimizer%20framework%20that%0Acan%20generalize%20several%20known%20optimizers%20and%20dynamically%20learn%20the%20most%20suitable%0Aone%20during%20training.%20The%20key%20idea%20in%20MADA%20is%20to%20parameterize%20the%20space%20of%0Aoptimizers%20and%20dynamically%20search%20through%20it%20using%20hyper-gradient%20descent%0Aduring%20training.%20We%20empirically%20compare%20MADA%20to%20other%20popular%20optimizers%20on%0Avision%20and%20language%20tasks%2C%20and%20find%20that%20MADA%20consistently%20outperforms%20Adam%20and%0Aother%20popular%20optimizers%2C%20and%20is%20robust%20against%20sub-optimally%20tuned%0Ahyper-parameters.%20MADA%20achieves%20a%20greater%20validation%20performance%20improvement%0Aover%20Adam%20compared%20to%20other%20popular%20optimizers%20during%20GPT-2%20training%20and%0Afine-tuning.%20We%20also%20propose%20AVGrad%2C%20a%20modification%20of%20AMSGrad%20that%20replaces%0Athe%20maximum%20operator%20with%20averaging%2C%20which%20is%20more%20suitable%20for%20hyper-gradient%0Aoptimization.%20Finally%2C%20we%20provide%20a%20convergence%20analysis%20to%20show%20that%0Aparameterized%20interpolations%20of%20optimizers%20can%20improve%20their%20error%20bounds%20%28up%0Ato%20constants%29%2C%20hinting%20at%20an%20advantage%20for%20meta-optimizers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.08893v3&entry.124074799=Read"},
{"title": "AnyMaker: Zero-shot General Object Customization via Decoupled\n  Dual-Level ID Injection", "author": "Lingjie Kong and Kai Wu and Xiaobin Hu and Wenhui Han and Jinlong Peng and Chengming Xu and Donghao Luo and Jiangning Zhang and Chengjie Wang and Yanwei Fu", "abstract": "  Text-to-image based object customization, aiming to generate images with the\nsame identity (ID) as objects of interest in accordance with text prompts and\nreference images, has made significant progress. However, recent customizing\nresearch is dominated by specialized tasks, such as human customization or\nvirtual try-on, leaving a gap in general object customization. To this end, we\nintroduce AnyMaker, an innovative zero-shot object customization framework\ncapable of generating general objects with high ID fidelity and flexible text\neditability. The efficacy of AnyMaker stems from its novel general ID\nextraction, dual-level ID injection, and ID-aware decoupling. Specifically, the\ngeneral ID extraction module extracts sufficient ID information with an\nensemble of self-supervised models to tackle the diverse customization tasks\nfor general objects. Then, to provide the diffusion UNet with the extracted ID\nas much while not damaging the text editability in the generation process, we\ndesign a global-local dual-level ID injection module, in which the global-level\nsemantic ID is injected into text descriptions while the local-level ID details\nare injected directly into the model through newly added cross-attention\nmodules. In addition, we propose an ID-aware decoupling module to disentangle\nID-related information from non-ID elements in the extracted representations\nfor high-fidelity generation of both identity and text descriptions. To\nvalidate our approach and boost the research of general object customization,\nwe create the first large-scale general ID dataset, Multi-Category\nID-Consistent (MC-IDC) dataset, with 315k text-image samples and 10k\ncategories. Experiments show that AnyMaker presents remarkable performance in\ngeneral object customization and outperforms specialized methods in\ncorresponding tasks. Code and dataset will be released soon.\n", "link": "http://arxiv.org/abs/2406.11643v1", "date": "2024-06-17", "relevancy": 2.5198, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6572}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6181}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5914}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnyMaker%3A%20Zero-shot%20General%20Object%20Customization%20via%20Decoupled%0A%20%20Dual-Level%20ID%20Injection&body=Title%3A%20AnyMaker%3A%20Zero-shot%20General%20Object%20Customization%20via%20Decoupled%0A%20%20Dual-Level%20ID%20Injection%0AAuthor%3A%20Lingjie%20Kong%20and%20Kai%20Wu%20and%20Xiaobin%20Hu%20and%20Wenhui%20Han%20and%20Jinlong%20Peng%20and%20Chengming%20Xu%20and%20Donghao%20Luo%20and%20Jiangning%20Zhang%20and%20Chengjie%20Wang%20and%20Yanwei%20Fu%0AAbstract%3A%20%20%20Text-to-image%20based%20object%20customization%2C%20aiming%20to%20generate%20images%20with%20the%0Asame%20identity%20%28ID%29%20as%20objects%20of%20interest%20in%20accordance%20with%20text%20prompts%20and%0Areference%20images%2C%20has%20made%20significant%20progress.%20However%2C%20recent%20customizing%0Aresearch%20is%20dominated%20by%20specialized%20tasks%2C%20such%20as%20human%20customization%20or%0Avirtual%20try-on%2C%20leaving%20a%20gap%20in%20general%20object%20customization.%20To%20this%20end%2C%20we%0Aintroduce%20AnyMaker%2C%20an%20innovative%20zero-shot%20object%20customization%20framework%0Acapable%20of%20generating%20general%20objects%20with%20high%20ID%20fidelity%20and%20flexible%20text%0Aeditability.%20The%20efficacy%20of%20AnyMaker%20stems%20from%20its%20novel%20general%20ID%0Aextraction%2C%20dual-level%20ID%20injection%2C%20and%20ID-aware%20decoupling.%20Specifically%2C%20the%0Ageneral%20ID%20extraction%20module%20extracts%20sufficient%20ID%20information%20with%20an%0Aensemble%20of%20self-supervised%20models%20to%20tackle%20the%20diverse%20customization%20tasks%0Afor%20general%20objects.%20Then%2C%20to%20provide%20the%20diffusion%20UNet%20with%20the%20extracted%20ID%0Aas%20much%20while%20not%20damaging%20the%20text%20editability%20in%20the%20generation%20process%2C%20we%0Adesign%20a%20global-local%20dual-level%20ID%20injection%20module%2C%20in%20which%20the%20global-level%0Asemantic%20ID%20is%20injected%20into%20text%20descriptions%20while%20the%20local-level%20ID%20details%0Aare%20injected%20directly%20into%20the%20model%20through%20newly%20added%20cross-attention%0Amodules.%20In%20addition%2C%20we%20propose%20an%20ID-aware%20decoupling%20module%20to%20disentangle%0AID-related%20information%20from%20non-ID%20elements%20in%20the%20extracted%20representations%0Afor%20high-fidelity%20generation%20of%20both%20identity%20and%20text%20descriptions.%20To%0Avalidate%20our%20approach%20and%20boost%20the%20research%20of%20general%20object%20customization%2C%0Awe%20create%20the%20first%20large-scale%20general%20ID%20dataset%2C%20Multi-Category%0AID-Consistent%20%28MC-IDC%29%20dataset%2C%20with%20315k%20text-image%20samples%20and%2010k%0Acategories.%20Experiments%20show%20that%20AnyMaker%20presents%20remarkable%20performance%20in%0Ageneral%20object%20customization%20and%20outperforms%20specialized%20methods%20in%0Acorresponding%20tasks.%20Code%20and%20dataset%20will%20be%20released%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnyMaker%253A%2520Zero-shot%2520General%2520Object%2520Customization%2520via%2520Decoupled%250A%2520%2520Dual-Level%2520ID%2520Injection%26entry.906535625%3DLingjie%2520Kong%2520and%2520Kai%2520Wu%2520and%2520Xiaobin%2520Hu%2520and%2520Wenhui%2520Han%2520and%2520Jinlong%2520Peng%2520and%2520Chengming%2520Xu%2520and%2520Donghao%2520Luo%2520and%2520Jiangning%2520Zhang%2520and%2520Chengjie%2520Wang%2520and%2520Yanwei%2520Fu%26entry.1292438233%3D%2520%2520Text-to-image%2520based%2520object%2520customization%252C%2520aiming%2520to%2520generate%2520images%2520with%2520the%250Asame%2520identity%2520%2528ID%2529%2520as%2520objects%2520of%2520interest%2520in%2520accordance%2520with%2520text%2520prompts%2520and%250Areference%2520images%252C%2520has%2520made%2520significant%2520progress.%2520However%252C%2520recent%2520customizing%250Aresearch%2520is%2520dominated%2520by%2520specialized%2520tasks%252C%2520such%2520as%2520human%2520customization%2520or%250Avirtual%2520try-on%252C%2520leaving%2520a%2520gap%2520in%2520general%2520object%2520customization.%2520To%2520this%2520end%252C%2520we%250Aintroduce%2520AnyMaker%252C%2520an%2520innovative%2520zero-shot%2520object%2520customization%2520framework%250Acapable%2520of%2520generating%2520general%2520objects%2520with%2520high%2520ID%2520fidelity%2520and%2520flexible%2520text%250Aeditability.%2520The%2520efficacy%2520of%2520AnyMaker%2520stems%2520from%2520its%2520novel%2520general%2520ID%250Aextraction%252C%2520dual-level%2520ID%2520injection%252C%2520and%2520ID-aware%2520decoupling.%2520Specifically%252C%2520the%250Ageneral%2520ID%2520extraction%2520module%2520extracts%2520sufficient%2520ID%2520information%2520with%2520an%250Aensemble%2520of%2520self-supervised%2520models%2520to%2520tackle%2520the%2520diverse%2520customization%2520tasks%250Afor%2520general%2520objects.%2520Then%252C%2520to%2520provide%2520the%2520diffusion%2520UNet%2520with%2520the%2520extracted%2520ID%250Aas%2520much%2520while%2520not%2520damaging%2520the%2520text%2520editability%2520in%2520the%2520generation%2520process%252C%2520we%250Adesign%2520a%2520global-local%2520dual-level%2520ID%2520injection%2520module%252C%2520in%2520which%2520the%2520global-level%250Asemantic%2520ID%2520is%2520injected%2520into%2520text%2520descriptions%2520while%2520the%2520local-level%2520ID%2520details%250Aare%2520injected%2520directly%2520into%2520the%2520model%2520through%2520newly%2520added%2520cross-attention%250Amodules.%2520In%2520addition%252C%2520we%2520propose%2520an%2520ID-aware%2520decoupling%2520module%2520to%2520disentangle%250AID-related%2520information%2520from%2520non-ID%2520elements%2520in%2520the%2520extracted%2520representations%250Afor%2520high-fidelity%2520generation%2520of%2520both%2520identity%2520and%2520text%2520descriptions.%2520To%250Avalidate%2520our%2520approach%2520and%2520boost%2520the%2520research%2520of%2520general%2520object%2520customization%252C%250Awe%2520create%2520the%2520first%2520large-scale%2520general%2520ID%2520dataset%252C%2520Multi-Category%250AID-Consistent%2520%2528MC-IDC%2529%2520dataset%252C%2520with%2520315k%2520text-image%2520samples%2520and%252010k%250Acategories.%2520Experiments%2520show%2520that%2520AnyMaker%2520presents%2520remarkable%2520performance%2520in%250Ageneral%2520object%2520customization%2520and%2520outperforms%2520specialized%2520methods%2520in%250Acorresponding%2520tasks.%2520Code%2520and%2520dataset%2520will%2520be%2520released%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnyMaker%3A%20Zero-shot%20General%20Object%20Customization%20via%20Decoupled%0A%20%20Dual-Level%20ID%20Injection&entry.906535625=Lingjie%20Kong%20and%20Kai%20Wu%20and%20Xiaobin%20Hu%20and%20Wenhui%20Han%20and%20Jinlong%20Peng%20and%20Chengming%20Xu%20and%20Donghao%20Luo%20and%20Jiangning%20Zhang%20and%20Chengjie%20Wang%20and%20Yanwei%20Fu&entry.1292438233=%20%20Text-to-image%20based%20object%20customization%2C%20aiming%20to%20generate%20images%20with%20the%0Asame%20identity%20%28ID%29%20as%20objects%20of%20interest%20in%20accordance%20with%20text%20prompts%20and%0Areference%20images%2C%20has%20made%20significant%20progress.%20However%2C%20recent%20customizing%0Aresearch%20is%20dominated%20by%20specialized%20tasks%2C%20such%20as%20human%20customization%20or%0Avirtual%20try-on%2C%20leaving%20a%20gap%20in%20general%20object%20customization.%20To%20this%20end%2C%20we%0Aintroduce%20AnyMaker%2C%20an%20innovative%20zero-shot%20object%20customization%20framework%0Acapable%20of%20generating%20general%20objects%20with%20high%20ID%20fidelity%20and%20flexible%20text%0Aeditability.%20The%20efficacy%20of%20AnyMaker%20stems%20from%20its%20novel%20general%20ID%0Aextraction%2C%20dual-level%20ID%20injection%2C%20and%20ID-aware%20decoupling.%20Specifically%2C%20the%0Ageneral%20ID%20extraction%20module%20extracts%20sufficient%20ID%20information%20with%20an%0Aensemble%20of%20self-supervised%20models%20to%20tackle%20the%20diverse%20customization%20tasks%0Afor%20general%20objects.%20Then%2C%20to%20provide%20the%20diffusion%20UNet%20with%20the%20extracted%20ID%0Aas%20much%20while%20not%20damaging%20the%20text%20editability%20in%20the%20generation%20process%2C%20we%0Adesign%20a%20global-local%20dual-level%20ID%20injection%20module%2C%20in%20which%20the%20global-level%0Asemantic%20ID%20is%20injected%20into%20text%20descriptions%20while%20the%20local-level%20ID%20details%0Aare%20injected%20directly%20into%20the%20model%20through%20newly%20added%20cross-attention%0Amodules.%20In%20addition%2C%20we%20propose%20an%20ID-aware%20decoupling%20module%20to%20disentangle%0AID-related%20information%20from%20non-ID%20elements%20in%20the%20extracted%20representations%0Afor%20high-fidelity%20generation%20of%20both%20identity%20and%20text%20descriptions.%20To%0Avalidate%20our%20approach%20and%20boost%20the%20research%20of%20general%20object%20customization%2C%0Awe%20create%20the%20first%20large-scale%20general%20ID%20dataset%2C%20Multi-Category%0AID-Consistent%20%28MC-IDC%29%20dataset%2C%20with%20315k%20text-image%20samples%20and%2010k%0Acategories.%20Experiments%20show%20that%20AnyMaker%20presents%20remarkable%20performance%20in%0Ageneral%20object%20customization%20and%20outperforms%20specialized%20methods%20in%0Acorresponding%20tasks.%20Code%20and%20dataset%20will%20be%20released%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11643v1&entry.124074799=Read"},
{"title": "Novel Fundus Image Preprocessing for Retcam Images to Improve Deep\n  Learning Classification of Retinopathy of Prematurity", "author": "Sajid Rahim and Kourosh Sabri and Anna Ells and Alan Wassyng and Mark Lawford and Linyang Chu and Wenbo He", "abstract": "  Retinopathy of Prematurity (ROP) is a potentially blinding eye disorder\nbecause of damage to the eye's retina which can affect babies born prematurely.\nScreening of ROP is essential for early detection and treatment. This is a\nlaborious and manual process which requires trained physician performing\ndilated ophthalmological examination which can be subjective resulting in lower\ndiagnosis success for clinically significant disease. Automated diagnostic\nmethods can assist ophthalmologists increase diagnosis accuracy using deep\nlearning. Several research groups have highlighted various approaches. Captured\nROP Retcam images suffer from poor quality. This paper proposes the use of\nimproved novel fundus preprocessing methods using pretrained transfer learning\nframeworks to create hybrid models to give higher diagnosis accuracy. Once\ntrained and validated, the evaluations showed that these novel methods in\ncomparison to traditional imaging processing contribute to better and in many\naspects higher accuracy in classifying Plus disease, Stages of ROP and Zones in\ncomparison to peer papers.\n", "link": "http://arxiv.org/abs/2302.02524v5", "date": "2024-06-17", "relevancy": 2.5123, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5144}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4967}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Novel%20Fundus%20Image%20Preprocessing%20for%20Retcam%20Images%20to%20Improve%20Deep%0A%20%20Learning%20Classification%20of%20Retinopathy%20of%20Prematurity&body=Title%3A%20Novel%20Fundus%20Image%20Preprocessing%20for%20Retcam%20Images%20to%20Improve%20Deep%0A%20%20Learning%20Classification%20of%20Retinopathy%20of%20Prematurity%0AAuthor%3A%20Sajid%20Rahim%20and%20Kourosh%20Sabri%20and%20Anna%20Ells%20and%20Alan%20Wassyng%20and%20Mark%20Lawford%20and%20Linyang%20Chu%20and%20Wenbo%20He%0AAbstract%3A%20%20%20Retinopathy%20of%20Prematurity%20%28ROP%29%20is%20a%20potentially%20blinding%20eye%20disorder%0Abecause%20of%20damage%20to%20the%20eye%27s%20retina%20which%20can%20affect%20babies%20born%20prematurely.%0AScreening%20of%20ROP%20is%20essential%20for%20early%20detection%20and%20treatment.%20This%20is%20a%0Alaborious%20and%20manual%20process%20which%20requires%20trained%20physician%20performing%0Adilated%20ophthalmological%20examination%20which%20can%20be%20subjective%20resulting%20in%20lower%0Adiagnosis%20success%20for%20clinically%20significant%20disease.%20Automated%20diagnostic%0Amethods%20can%20assist%20ophthalmologists%20increase%20diagnosis%20accuracy%20using%20deep%0Alearning.%20Several%20research%20groups%20have%20highlighted%20various%20approaches.%20Captured%0AROP%20Retcam%20images%20suffer%20from%20poor%20quality.%20This%20paper%20proposes%20the%20use%20of%0Aimproved%20novel%20fundus%20preprocessing%20methods%20using%20pretrained%20transfer%20learning%0Aframeworks%20to%20create%20hybrid%20models%20to%20give%20higher%20diagnosis%20accuracy.%20Once%0Atrained%20and%20validated%2C%20the%20evaluations%20showed%20that%20these%20novel%20methods%20in%0Acomparison%20to%20traditional%20imaging%20processing%20contribute%20to%20better%20and%20in%20many%0Aaspects%20higher%20accuracy%20in%20classifying%20Plus%20disease%2C%20Stages%20of%20ROP%20and%20Zones%20in%0Acomparison%20to%20peer%20papers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.02524v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNovel%2520Fundus%2520Image%2520Preprocessing%2520for%2520Retcam%2520Images%2520to%2520Improve%2520Deep%250A%2520%2520Learning%2520Classification%2520of%2520Retinopathy%2520of%2520Prematurity%26entry.906535625%3DSajid%2520Rahim%2520and%2520Kourosh%2520Sabri%2520and%2520Anna%2520Ells%2520and%2520Alan%2520Wassyng%2520and%2520Mark%2520Lawford%2520and%2520Linyang%2520Chu%2520and%2520Wenbo%2520He%26entry.1292438233%3D%2520%2520Retinopathy%2520of%2520Prematurity%2520%2528ROP%2529%2520is%2520a%2520potentially%2520blinding%2520eye%2520disorder%250Abecause%2520of%2520damage%2520to%2520the%2520eye%2527s%2520retina%2520which%2520can%2520affect%2520babies%2520born%2520prematurely.%250AScreening%2520of%2520ROP%2520is%2520essential%2520for%2520early%2520detection%2520and%2520treatment.%2520This%2520is%2520a%250Alaborious%2520and%2520manual%2520process%2520which%2520requires%2520trained%2520physician%2520performing%250Adilated%2520ophthalmological%2520examination%2520which%2520can%2520be%2520subjective%2520resulting%2520in%2520lower%250Adiagnosis%2520success%2520for%2520clinically%2520significant%2520disease.%2520Automated%2520diagnostic%250Amethods%2520can%2520assist%2520ophthalmologists%2520increase%2520diagnosis%2520accuracy%2520using%2520deep%250Alearning.%2520Several%2520research%2520groups%2520have%2520highlighted%2520various%2520approaches.%2520Captured%250AROP%2520Retcam%2520images%2520suffer%2520from%2520poor%2520quality.%2520This%2520paper%2520proposes%2520the%2520use%2520of%250Aimproved%2520novel%2520fundus%2520preprocessing%2520methods%2520using%2520pretrained%2520transfer%2520learning%250Aframeworks%2520to%2520create%2520hybrid%2520models%2520to%2520give%2520higher%2520diagnosis%2520accuracy.%2520Once%250Atrained%2520and%2520validated%252C%2520the%2520evaluations%2520showed%2520that%2520these%2520novel%2520methods%2520in%250Acomparison%2520to%2520traditional%2520imaging%2520processing%2520contribute%2520to%2520better%2520and%2520in%2520many%250Aaspects%2520higher%2520accuracy%2520in%2520classifying%2520Plus%2520disease%252C%2520Stages%2520of%2520ROP%2520and%2520Zones%2520in%250Acomparison%2520to%2520peer%2520papers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.02524v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Novel%20Fundus%20Image%20Preprocessing%20for%20Retcam%20Images%20to%20Improve%20Deep%0A%20%20Learning%20Classification%20of%20Retinopathy%20of%20Prematurity&entry.906535625=Sajid%20Rahim%20and%20Kourosh%20Sabri%20and%20Anna%20Ells%20and%20Alan%20Wassyng%20and%20Mark%20Lawford%20and%20Linyang%20Chu%20and%20Wenbo%20He&entry.1292438233=%20%20Retinopathy%20of%20Prematurity%20%28ROP%29%20is%20a%20potentially%20blinding%20eye%20disorder%0Abecause%20of%20damage%20to%20the%20eye%27s%20retina%20which%20can%20affect%20babies%20born%20prematurely.%0AScreening%20of%20ROP%20is%20essential%20for%20early%20detection%20and%20treatment.%20This%20is%20a%0Alaborious%20and%20manual%20process%20which%20requires%20trained%20physician%20performing%0Adilated%20ophthalmological%20examination%20which%20can%20be%20subjective%20resulting%20in%20lower%0Adiagnosis%20success%20for%20clinically%20significant%20disease.%20Automated%20diagnostic%0Amethods%20can%20assist%20ophthalmologists%20increase%20diagnosis%20accuracy%20using%20deep%0Alearning.%20Several%20research%20groups%20have%20highlighted%20various%20approaches.%20Captured%0AROP%20Retcam%20images%20suffer%20from%20poor%20quality.%20This%20paper%20proposes%20the%20use%20of%0Aimproved%20novel%20fundus%20preprocessing%20methods%20using%20pretrained%20transfer%20learning%0Aframeworks%20to%20create%20hybrid%20models%20to%20give%20higher%20diagnosis%20accuracy.%20Once%0Atrained%20and%20validated%2C%20the%20evaluations%20showed%20that%20these%20novel%20methods%20in%0Acomparison%20to%20traditional%20imaging%20processing%20contribute%20to%20better%20and%20in%20many%0Aaspects%20higher%20accuracy%20in%20classifying%20Plus%20disease%2C%20Stages%20of%20ROP%20and%20Zones%20in%0Acomparison%20to%20peer%20papers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.02524v5&entry.124074799=Read"},
{"title": "Pre-Training and Personalized Fine-Tuning via Over-the-Air Federated\n  Meta-Learning: Convergence-Generalization Trade-Offs", "author": "Haifeng Wen and Hong Xing and Osvaldo Simeone", "abstract": "  For modern artificial intelligence (AI) applications such as large language\nmodels (LLMs), the training paradigm has recently shifted to pre-training\nfollowed by fine-tuning. Furthermore, owing to dwindling open repositories of\ndata and thanks to efforts to democratize access to AI models, pre-training is\nexpected to increasingly migrate from the current centralized deployments to\nfederated learning (FL) implementations. Meta-learning provides a general\nframework in which pre-training and fine-tuning can be formalized.\nMeta-learning-based personalized FL (meta-pFL) moves beyond basic\npersonalization by targeting generalization to new agents and tasks. This paper\nstudies the generalization performance of meta-pFL for a wireless setting in\nwhich the agents participating in the pre-training phase, i.e., meta-learning,\nare connected via a shared wireless channel to the server. Adopting\nover-the-air computing, we study the trade-off between generalization to new\nagents and tasks, on the one hand, and convergence, on the other hand. The\ntrade-off arises from the fact that channel impairments may enhance\ngeneralization, while degrading convergence. Extensive numerical results\nvalidate the theory.\n", "link": "http://arxiv.org/abs/2406.11569v1", "date": "2024-06-17", "relevancy": 2.5117, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5259}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4933}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pre-Training%20and%20Personalized%20Fine-Tuning%20via%20Over-the-Air%20Federated%0A%20%20Meta-Learning%3A%20Convergence-Generalization%20Trade-Offs&body=Title%3A%20Pre-Training%20and%20Personalized%20Fine-Tuning%20via%20Over-the-Air%20Federated%0A%20%20Meta-Learning%3A%20Convergence-Generalization%20Trade-Offs%0AAuthor%3A%20Haifeng%20Wen%20and%20Hong%20Xing%20and%20Osvaldo%20Simeone%0AAbstract%3A%20%20%20For%20modern%20artificial%20intelligence%20%28AI%29%20applications%20such%20as%20large%20language%0Amodels%20%28LLMs%29%2C%20the%20training%20paradigm%20has%20recently%20shifted%20to%20pre-training%0Afollowed%20by%20fine-tuning.%20Furthermore%2C%20owing%20to%20dwindling%20open%20repositories%20of%0Adata%20and%20thanks%20to%20efforts%20to%20democratize%20access%20to%20AI%20models%2C%20pre-training%20is%0Aexpected%20to%20increasingly%20migrate%20from%20the%20current%20centralized%20deployments%20to%0Afederated%20learning%20%28FL%29%20implementations.%20Meta-learning%20provides%20a%20general%0Aframework%20in%20which%20pre-training%20and%20fine-tuning%20can%20be%20formalized.%0AMeta-learning-based%20personalized%20FL%20%28meta-pFL%29%20moves%20beyond%20basic%0Apersonalization%20by%20targeting%20generalization%20to%20new%20agents%20and%20tasks.%20This%20paper%0Astudies%20the%20generalization%20performance%20of%20meta-pFL%20for%20a%20wireless%20setting%20in%0Awhich%20the%20agents%20participating%20in%20the%20pre-training%20phase%2C%20i.e.%2C%20meta-learning%2C%0Aare%20connected%20via%20a%20shared%20wireless%20channel%20to%20the%20server.%20Adopting%0Aover-the-air%20computing%2C%20we%20study%20the%20trade-off%20between%20generalization%20to%20new%0Aagents%20and%20tasks%2C%20on%20the%20one%20hand%2C%20and%20convergence%2C%20on%20the%20other%20hand.%20The%0Atrade-off%20arises%20from%20the%20fact%20that%20channel%20impairments%20may%20enhance%0Ageneralization%2C%20while%20degrading%20convergence.%20Extensive%20numerical%20results%0Avalidate%20the%20theory.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11569v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPre-Training%2520and%2520Personalized%2520Fine-Tuning%2520via%2520Over-the-Air%2520Federated%250A%2520%2520Meta-Learning%253A%2520Convergence-Generalization%2520Trade-Offs%26entry.906535625%3DHaifeng%2520Wen%2520and%2520Hong%2520Xing%2520and%2520Osvaldo%2520Simeone%26entry.1292438233%3D%2520%2520For%2520modern%2520artificial%2520intelligence%2520%2528AI%2529%2520applications%2520such%2520as%2520large%2520language%250Amodels%2520%2528LLMs%2529%252C%2520the%2520training%2520paradigm%2520has%2520recently%2520shifted%2520to%2520pre-training%250Afollowed%2520by%2520fine-tuning.%2520Furthermore%252C%2520owing%2520to%2520dwindling%2520open%2520repositories%2520of%250Adata%2520and%2520thanks%2520to%2520efforts%2520to%2520democratize%2520access%2520to%2520AI%2520models%252C%2520pre-training%2520is%250Aexpected%2520to%2520increasingly%2520migrate%2520from%2520the%2520current%2520centralized%2520deployments%2520to%250Afederated%2520learning%2520%2528FL%2529%2520implementations.%2520Meta-learning%2520provides%2520a%2520general%250Aframework%2520in%2520which%2520pre-training%2520and%2520fine-tuning%2520can%2520be%2520formalized.%250AMeta-learning-based%2520personalized%2520FL%2520%2528meta-pFL%2529%2520moves%2520beyond%2520basic%250Apersonalization%2520by%2520targeting%2520generalization%2520to%2520new%2520agents%2520and%2520tasks.%2520This%2520paper%250Astudies%2520the%2520generalization%2520performance%2520of%2520meta-pFL%2520for%2520a%2520wireless%2520setting%2520in%250Awhich%2520the%2520agents%2520participating%2520in%2520the%2520pre-training%2520phase%252C%2520i.e.%252C%2520meta-learning%252C%250Aare%2520connected%2520via%2520a%2520shared%2520wireless%2520channel%2520to%2520the%2520server.%2520Adopting%250Aover-the-air%2520computing%252C%2520we%2520study%2520the%2520trade-off%2520between%2520generalization%2520to%2520new%250Aagents%2520and%2520tasks%252C%2520on%2520the%2520one%2520hand%252C%2520and%2520convergence%252C%2520on%2520the%2520other%2520hand.%2520The%250Atrade-off%2520arises%2520from%2520the%2520fact%2520that%2520channel%2520impairments%2520may%2520enhance%250Ageneralization%252C%2520while%2520degrading%2520convergence.%2520Extensive%2520numerical%2520results%250Avalidate%2520the%2520theory.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11569v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pre-Training%20and%20Personalized%20Fine-Tuning%20via%20Over-the-Air%20Federated%0A%20%20Meta-Learning%3A%20Convergence-Generalization%20Trade-Offs&entry.906535625=Haifeng%20Wen%20and%20Hong%20Xing%20and%20Osvaldo%20Simeone&entry.1292438233=%20%20For%20modern%20artificial%20intelligence%20%28AI%29%20applications%20such%20as%20large%20language%0Amodels%20%28LLMs%29%2C%20the%20training%20paradigm%20has%20recently%20shifted%20to%20pre-training%0Afollowed%20by%20fine-tuning.%20Furthermore%2C%20owing%20to%20dwindling%20open%20repositories%20of%0Adata%20and%20thanks%20to%20efforts%20to%20democratize%20access%20to%20AI%20models%2C%20pre-training%20is%0Aexpected%20to%20increasingly%20migrate%20from%20the%20current%20centralized%20deployments%20to%0Afederated%20learning%20%28FL%29%20implementations.%20Meta-learning%20provides%20a%20general%0Aframework%20in%20which%20pre-training%20and%20fine-tuning%20can%20be%20formalized.%0AMeta-learning-based%20personalized%20FL%20%28meta-pFL%29%20moves%20beyond%20basic%0Apersonalization%20by%20targeting%20generalization%20to%20new%20agents%20and%20tasks.%20This%20paper%0Astudies%20the%20generalization%20performance%20of%20meta-pFL%20for%20a%20wireless%20setting%20in%0Awhich%20the%20agents%20participating%20in%20the%20pre-training%20phase%2C%20i.e.%2C%20meta-learning%2C%0Aare%20connected%20via%20a%20shared%20wireless%20channel%20to%20the%20server.%20Adopting%0Aover-the-air%20computing%2C%20we%20study%20the%20trade-off%20between%20generalization%20to%20new%0Aagents%20and%20tasks%2C%20on%20the%20one%20hand%2C%20and%20convergence%2C%20on%20the%20other%20hand.%20The%0Atrade-off%20arises%20from%20the%20fact%20that%20channel%20impairments%20may%20enhance%0Ageneralization%2C%20while%20degrading%20convergence.%20Extensive%20numerical%20results%0Avalidate%20the%20theory.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11569v1&entry.124074799=Read"},
{"title": "CAT: A Causally Graph Attention Network for Trimming Heterophilic Graph", "author": "Silu He and Qinyao Luo and Xinsha Fu and Ling Zhao and Ronghua Du and Haifeng Li", "abstract": "  Local Attention-guided Message Passing Mechanism (LAMP) adopted in Graph\nAttention Networks (GATs) is designed to adaptively learn the importance of\nneighboring nodes for better local aggregation on the graph, which can bring\nthe representations of similar neighbors closer effectively, thus showing\nstronger discrimination ability. However, existing GATs suffer from a\nsignificant discrimination ability decline in heterophilic graphs because the\nhigh proportion of dissimilar neighbors can weaken the self-attention of the\ncentral node, jointly resulting in the deviation of the central node from\nsimilar nodes in the representation space. This kind of effect generated by\nneighboring nodes is called the Distraction Effect (DE) in this paper. To\nestimate and weaken the DE of neighboring nodes, we propose a Causally graph\nAttention network for Trimming heterophilic graph (CAT). To estimate the DE,\nsince the DE are generated through two paths (grab the attention assigned to\nneighbors and reduce the self-attention of the central node), we use Total\nEffect to model DE, which is a kind of causal estimand and can be estimated\nfrom intervened data; To weaken the DE, we identify the neighbors with the\nhighest DE (we call them Distraction Neighbors) and remove them. We adopt three\nrepresentative GATs as the base model within the proposed CAT framework and\nconduct experiments on seven heterophilic datasets in three different sizes.\nComparative experiments show that CAT can improve the node classification\naccuracy of all base GAT models. Ablation experiments and visualization further\nvalidate the enhancement of discrimination ability brought by CAT. The source\ncode is available at https://github.com/GeoX-Lab/CAT.\n", "link": "http://arxiv.org/abs/2312.08672v3", "date": "2024-06-17", "relevancy": 2.502, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5109}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.497}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4933}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAT%3A%20A%20Causally%20Graph%20Attention%20Network%20for%20Trimming%20Heterophilic%20Graph&body=Title%3A%20CAT%3A%20A%20Causally%20Graph%20Attention%20Network%20for%20Trimming%20Heterophilic%20Graph%0AAuthor%3A%20Silu%20He%20and%20Qinyao%20Luo%20and%20Xinsha%20Fu%20and%20Ling%20Zhao%20and%20Ronghua%20Du%20and%20Haifeng%20Li%0AAbstract%3A%20%20%20Local%20Attention-guided%20Message%20Passing%20Mechanism%20%28LAMP%29%20adopted%20in%20Graph%0AAttention%20Networks%20%28GATs%29%20is%20designed%20to%20adaptively%20learn%20the%20importance%20of%0Aneighboring%20nodes%20for%20better%20local%20aggregation%20on%20the%20graph%2C%20which%20can%20bring%0Athe%20representations%20of%20similar%20neighbors%20closer%20effectively%2C%20thus%20showing%0Astronger%20discrimination%20ability.%20However%2C%20existing%20GATs%20suffer%20from%20a%0Asignificant%20discrimination%20ability%20decline%20in%20heterophilic%20graphs%20because%20the%0Ahigh%20proportion%20of%20dissimilar%20neighbors%20can%20weaken%20the%20self-attention%20of%20the%0Acentral%20node%2C%20jointly%20resulting%20in%20the%20deviation%20of%20the%20central%20node%20from%0Asimilar%20nodes%20in%20the%20representation%20space.%20This%20kind%20of%20effect%20generated%20by%0Aneighboring%20nodes%20is%20called%20the%20Distraction%20Effect%20%28DE%29%20in%20this%20paper.%20To%0Aestimate%20and%20weaken%20the%20DE%20of%20neighboring%20nodes%2C%20we%20propose%20a%20Causally%20graph%0AAttention%20network%20for%20Trimming%20heterophilic%20graph%20%28CAT%29.%20To%20estimate%20the%20DE%2C%0Asince%20the%20DE%20are%20generated%20through%20two%20paths%20%28grab%20the%20attention%20assigned%20to%0Aneighbors%20and%20reduce%20the%20self-attention%20of%20the%20central%20node%29%2C%20we%20use%20Total%0AEffect%20to%20model%20DE%2C%20which%20is%20a%20kind%20of%20causal%20estimand%20and%20can%20be%20estimated%0Afrom%20intervened%20data%3B%20To%20weaken%20the%20DE%2C%20we%20identify%20the%20neighbors%20with%20the%0Ahighest%20DE%20%28we%20call%20them%20Distraction%20Neighbors%29%20and%20remove%20them.%20We%20adopt%20three%0Arepresentative%20GATs%20as%20the%20base%20model%20within%20the%20proposed%20CAT%20framework%20and%0Aconduct%20experiments%20on%20seven%20heterophilic%20datasets%20in%20three%20different%20sizes.%0AComparative%20experiments%20show%20that%20CAT%20can%20improve%20the%20node%20classification%0Aaccuracy%20of%20all%20base%20GAT%20models.%20Ablation%20experiments%20and%20visualization%20further%0Avalidate%20the%20enhancement%20of%20discrimination%20ability%20brought%20by%20CAT.%20The%20source%0Acode%20is%20available%20at%20https%3A//github.com/GeoX-Lab/CAT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.08672v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAT%253A%2520A%2520Causally%2520Graph%2520Attention%2520Network%2520for%2520Trimming%2520Heterophilic%2520Graph%26entry.906535625%3DSilu%2520He%2520and%2520Qinyao%2520Luo%2520and%2520Xinsha%2520Fu%2520and%2520Ling%2520Zhao%2520and%2520Ronghua%2520Du%2520and%2520Haifeng%2520Li%26entry.1292438233%3D%2520%2520Local%2520Attention-guided%2520Message%2520Passing%2520Mechanism%2520%2528LAMP%2529%2520adopted%2520in%2520Graph%250AAttention%2520Networks%2520%2528GATs%2529%2520is%2520designed%2520to%2520adaptively%2520learn%2520the%2520importance%2520of%250Aneighboring%2520nodes%2520for%2520better%2520local%2520aggregation%2520on%2520the%2520graph%252C%2520which%2520can%2520bring%250Athe%2520representations%2520of%2520similar%2520neighbors%2520closer%2520effectively%252C%2520thus%2520showing%250Astronger%2520discrimination%2520ability.%2520However%252C%2520existing%2520GATs%2520suffer%2520from%2520a%250Asignificant%2520discrimination%2520ability%2520decline%2520in%2520heterophilic%2520graphs%2520because%2520the%250Ahigh%2520proportion%2520of%2520dissimilar%2520neighbors%2520can%2520weaken%2520the%2520self-attention%2520of%2520the%250Acentral%2520node%252C%2520jointly%2520resulting%2520in%2520the%2520deviation%2520of%2520the%2520central%2520node%2520from%250Asimilar%2520nodes%2520in%2520the%2520representation%2520space.%2520This%2520kind%2520of%2520effect%2520generated%2520by%250Aneighboring%2520nodes%2520is%2520called%2520the%2520Distraction%2520Effect%2520%2528DE%2529%2520in%2520this%2520paper.%2520To%250Aestimate%2520and%2520weaken%2520the%2520DE%2520of%2520neighboring%2520nodes%252C%2520we%2520propose%2520a%2520Causally%2520graph%250AAttention%2520network%2520for%2520Trimming%2520heterophilic%2520graph%2520%2528CAT%2529.%2520To%2520estimate%2520the%2520DE%252C%250Asince%2520the%2520DE%2520are%2520generated%2520through%2520two%2520paths%2520%2528grab%2520the%2520attention%2520assigned%2520to%250Aneighbors%2520and%2520reduce%2520the%2520self-attention%2520of%2520the%2520central%2520node%2529%252C%2520we%2520use%2520Total%250AEffect%2520to%2520model%2520DE%252C%2520which%2520is%2520a%2520kind%2520of%2520causal%2520estimand%2520and%2520can%2520be%2520estimated%250Afrom%2520intervened%2520data%253B%2520To%2520weaken%2520the%2520DE%252C%2520we%2520identify%2520the%2520neighbors%2520with%2520the%250Ahighest%2520DE%2520%2528we%2520call%2520them%2520Distraction%2520Neighbors%2529%2520and%2520remove%2520them.%2520We%2520adopt%2520three%250Arepresentative%2520GATs%2520as%2520the%2520base%2520model%2520within%2520the%2520proposed%2520CAT%2520framework%2520and%250Aconduct%2520experiments%2520on%2520seven%2520heterophilic%2520datasets%2520in%2520three%2520different%2520sizes.%250AComparative%2520experiments%2520show%2520that%2520CAT%2520can%2520improve%2520the%2520node%2520classification%250Aaccuracy%2520of%2520all%2520base%2520GAT%2520models.%2520Ablation%2520experiments%2520and%2520visualization%2520further%250Avalidate%2520the%2520enhancement%2520of%2520discrimination%2520ability%2520brought%2520by%2520CAT.%2520The%2520source%250Acode%2520is%2520available%2520at%2520https%253A//github.com/GeoX-Lab/CAT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.08672v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAT%3A%20A%20Causally%20Graph%20Attention%20Network%20for%20Trimming%20Heterophilic%20Graph&entry.906535625=Silu%20He%20and%20Qinyao%20Luo%20and%20Xinsha%20Fu%20and%20Ling%20Zhao%20and%20Ronghua%20Du%20and%20Haifeng%20Li&entry.1292438233=%20%20Local%20Attention-guided%20Message%20Passing%20Mechanism%20%28LAMP%29%20adopted%20in%20Graph%0AAttention%20Networks%20%28GATs%29%20is%20designed%20to%20adaptively%20learn%20the%20importance%20of%0Aneighboring%20nodes%20for%20better%20local%20aggregation%20on%20the%20graph%2C%20which%20can%20bring%0Athe%20representations%20of%20similar%20neighbors%20closer%20effectively%2C%20thus%20showing%0Astronger%20discrimination%20ability.%20However%2C%20existing%20GATs%20suffer%20from%20a%0Asignificant%20discrimination%20ability%20decline%20in%20heterophilic%20graphs%20because%20the%0Ahigh%20proportion%20of%20dissimilar%20neighbors%20can%20weaken%20the%20self-attention%20of%20the%0Acentral%20node%2C%20jointly%20resulting%20in%20the%20deviation%20of%20the%20central%20node%20from%0Asimilar%20nodes%20in%20the%20representation%20space.%20This%20kind%20of%20effect%20generated%20by%0Aneighboring%20nodes%20is%20called%20the%20Distraction%20Effect%20%28DE%29%20in%20this%20paper.%20To%0Aestimate%20and%20weaken%20the%20DE%20of%20neighboring%20nodes%2C%20we%20propose%20a%20Causally%20graph%0AAttention%20network%20for%20Trimming%20heterophilic%20graph%20%28CAT%29.%20To%20estimate%20the%20DE%2C%0Asince%20the%20DE%20are%20generated%20through%20two%20paths%20%28grab%20the%20attention%20assigned%20to%0Aneighbors%20and%20reduce%20the%20self-attention%20of%20the%20central%20node%29%2C%20we%20use%20Total%0AEffect%20to%20model%20DE%2C%20which%20is%20a%20kind%20of%20causal%20estimand%20and%20can%20be%20estimated%0Afrom%20intervened%20data%3B%20To%20weaken%20the%20DE%2C%20we%20identify%20the%20neighbors%20with%20the%0Ahighest%20DE%20%28we%20call%20them%20Distraction%20Neighbors%29%20and%20remove%20them.%20We%20adopt%20three%0Arepresentative%20GATs%20as%20the%20base%20model%20within%20the%20proposed%20CAT%20framework%20and%0Aconduct%20experiments%20on%20seven%20heterophilic%20datasets%20in%20three%20different%20sizes.%0AComparative%20experiments%20show%20that%20CAT%20can%20improve%20the%20node%20classification%0Aaccuracy%20of%20all%20base%20GAT%20models.%20Ablation%20experiments%20and%20visualization%20further%0Avalidate%20the%20enhancement%20of%20discrimination%20ability%20brought%20by%20CAT.%20The%20source%0Acode%20is%20available%20at%20https%3A//github.com/GeoX-Lab/CAT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.08672v3&entry.124074799=Read"},
{"title": "Learning to Check: Unleashing Potentials for Self-Correction in Large\n  Language Models", "author": "Che Zhang and Zhenyang Xiao and Chengcheng Han and Yixin Lian and Yuejian Fang", "abstract": "  Self-correction has achieved impressive results in enhancing the style and\nsecurity of the generated output from large language models (LLMs). However,\nrecent studies suggest that self-correction might be limited or even\ncounterproductive in reasoning tasks due to LLMs' difficulties in identifying\nlogical mistakes.\n  In this paper, we aim to enhance the self-checking capabilities of LLMs by\nconstructing training data for checking tasks. Specifically, we apply the Chain\nof Thought (CoT) methodology to self-checking tasks, utilizing fine-grained\nstep-level analyses and explanations to assess the correctness of reasoning\npaths. We propose a specialized checking format called \"Step CoT Check\".\nFollowing this format, we construct a checking-correction dataset that includes\ndetailed step-by-step analysis and checking. Then we fine-tune LLMs to enhance\ntheir error detection and correction abilities.\n  Our experiments demonstrate that fine-tuning with the \"Step CoT Check\" format\nsignificantly improves the self-checking and self-correction abilities of LLMs\nacross multiple benchmarks. This approach outperforms other formats, especially\nin locating the incorrect position, with greater benefits observed in larger\nmodels.\n  For reproducibility, all the datasets and code are provided in\nhttps://github.com/bammt/Learn-to-check.\n", "link": "http://arxiv.org/abs/2402.13035v3", "date": "2024-06-17", "relevancy": 2.4712, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5011}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.492}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Check%3A%20Unleashing%20Potentials%20for%20Self-Correction%20in%20Large%0A%20%20Language%20Models&body=Title%3A%20Learning%20to%20Check%3A%20Unleashing%20Potentials%20for%20Self-Correction%20in%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Che%20Zhang%20and%20Zhenyang%20Xiao%20and%20Chengcheng%20Han%20and%20Yixin%20Lian%20and%20Yuejian%20Fang%0AAbstract%3A%20%20%20Self-correction%20has%20achieved%20impressive%20results%20in%20enhancing%20the%20style%20and%0Asecurity%20of%20the%20generated%20output%20from%20large%20language%20models%20%28LLMs%29.%20However%2C%0Arecent%20studies%20suggest%20that%20self-correction%20might%20be%20limited%20or%20even%0Acounterproductive%20in%20reasoning%20tasks%20due%20to%20LLMs%27%20difficulties%20in%20identifying%0Alogical%20mistakes.%0A%20%20In%20this%20paper%2C%20we%20aim%20to%20enhance%20the%20self-checking%20capabilities%20of%20LLMs%20by%0Aconstructing%20training%20data%20for%20checking%20tasks.%20Specifically%2C%20we%20apply%20the%20Chain%0Aof%20Thought%20%28CoT%29%20methodology%20to%20self-checking%20tasks%2C%20utilizing%20fine-grained%0Astep-level%20analyses%20and%20explanations%20to%20assess%20the%20correctness%20of%20reasoning%0Apaths.%20We%20propose%20a%20specialized%20checking%20format%20called%20%22Step%20CoT%20Check%22.%0AFollowing%20this%20format%2C%20we%20construct%20a%20checking-correction%20dataset%20that%20includes%0Adetailed%20step-by-step%20analysis%20and%20checking.%20Then%20we%20fine-tune%20LLMs%20to%20enhance%0Atheir%20error%20detection%20and%20correction%20abilities.%0A%20%20Our%20experiments%20demonstrate%20that%20fine-tuning%20with%20the%20%22Step%20CoT%20Check%22%20format%0Asignificantly%20improves%20the%20self-checking%20and%20self-correction%20abilities%20of%20LLMs%0Aacross%20multiple%20benchmarks.%20This%20approach%20outperforms%20other%20formats%2C%20especially%0Ain%20locating%20the%20incorrect%20position%2C%20with%20greater%20benefits%20observed%20in%20larger%0Amodels.%0A%20%20For%20reproducibility%2C%20all%20the%20datasets%20and%20code%20are%20provided%20in%0Ahttps%3A//github.com/bammt/Learn-to-check.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.13035v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Check%253A%2520Unleashing%2520Potentials%2520for%2520Self-Correction%2520in%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DChe%2520Zhang%2520and%2520Zhenyang%2520Xiao%2520and%2520Chengcheng%2520Han%2520and%2520Yixin%2520Lian%2520and%2520Yuejian%2520Fang%26entry.1292438233%3D%2520%2520Self-correction%2520has%2520achieved%2520impressive%2520results%2520in%2520enhancing%2520the%2520style%2520and%250Asecurity%2520of%2520the%2520generated%2520output%2520from%2520large%2520language%2520models%2520%2528LLMs%2529.%2520However%252C%250Arecent%2520studies%2520suggest%2520that%2520self-correction%2520might%2520be%2520limited%2520or%2520even%250Acounterproductive%2520in%2520reasoning%2520tasks%2520due%2520to%2520LLMs%2527%2520difficulties%2520in%2520identifying%250Alogical%2520mistakes.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520aim%2520to%2520enhance%2520the%2520self-checking%2520capabilities%2520of%2520LLMs%2520by%250Aconstructing%2520training%2520data%2520for%2520checking%2520tasks.%2520Specifically%252C%2520we%2520apply%2520the%2520Chain%250Aof%2520Thought%2520%2528CoT%2529%2520methodology%2520to%2520self-checking%2520tasks%252C%2520utilizing%2520fine-grained%250Astep-level%2520analyses%2520and%2520explanations%2520to%2520assess%2520the%2520correctness%2520of%2520reasoning%250Apaths.%2520We%2520propose%2520a%2520specialized%2520checking%2520format%2520called%2520%2522Step%2520CoT%2520Check%2522.%250AFollowing%2520this%2520format%252C%2520we%2520construct%2520a%2520checking-correction%2520dataset%2520that%2520includes%250Adetailed%2520step-by-step%2520analysis%2520and%2520checking.%2520Then%2520we%2520fine-tune%2520LLMs%2520to%2520enhance%250Atheir%2520error%2520detection%2520and%2520correction%2520abilities.%250A%2520%2520Our%2520experiments%2520demonstrate%2520that%2520fine-tuning%2520with%2520the%2520%2522Step%2520CoT%2520Check%2522%2520format%250Asignificantly%2520improves%2520the%2520self-checking%2520and%2520self-correction%2520abilities%2520of%2520LLMs%250Aacross%2520multiple%2520benchmarks.%2520This%2520approach%2520outperforms%2520other%2520formats%252C%2520especially%250Ain%2520locating%2520the%2520incorrect%2520position%252C%2520with%2520greater%2520benefits%2520observed%2520in%2520larger%250Amodels.%250A%2520%2520For%2520reproducibility%252C%2520all%2520the%2520datasets%2520and%2520code%2520are%2520provided%2520in%250Ahttps%253A//github.com/bammt/Learn-to-check.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.13035v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Check%3A%20Unleashing%20Potentials%20for%20Self-Correction%20in%20Large%0A%20%20Language%20Models&entry.906535625=Che%20Zhang%20and%20Zhenyang%20Xiao%20and%20Chengcheng%20Han%20and%20Yixin%20Lian%20and%20Yuejian%20Fang&entry.1292438233=%20%20Self-correction%20has%20achieved%20impressive%20results%20in%20enhancing%20the%20style%20and%0Asecurity%20of%20the%20generated%20output%20from%20large%20language%20models%20%28LLMs%29.%20However%2C%0Arecent%20studies%20suggest%20that%20self-correction%20might%20be%20limited%20or%20even%0Acounterproductive%20in%20reasoning%20tasks%20due%20to%20LLMs%27%20difficulties%20in%20identifying%0Alogical%20mistakes.%0A%20%20In%20this%20paper%2C%20we%20aim%20to%20enhance%20the%20self-checking%20capabilities%20of%20LLMs%20by%0Aconstructing%20training%20data%20for%20checking%20tasks.%20Specifically%2C%20we%20apply%20the%20Chain%0Aof%20Thought%20%28CoT%29%20methodology%20to%20self-checking%20tasks%2C%20utilizing%20fine-grained%0Astep-level%20analyses%20and%20explanations%20to%20assess%20the%20correctness%20of%20reasoning%0Apaths.%20We%20propose%20a%20specialized%20checking%20format%20called%20%22Step%20CoT%20Check%22.%0AFollowing%20this%20format%2C%20we%20construct%20a%20checking-correction%20dataset%20that%20includes%0Adetailed%20step-by-step%20analysis%20and%20checking.%20Then%20we%20fine-tune%20LLMs%20to%20enhance%0Atheir%20error%20detection%20and%20correction%20abilities.%0A%20%20Our%20experiments%20demonstrate%20that%20fine-tuning%20with%20the%20%22Step%20CoT%20Check%22%20format%0Asignificantly%20improves%20the%20self-checking%20and%20self-correction%20abilities%20of%20LLMs%0Aacross%20multiple%20benchmarks.%20This%20approach%20outperforms%20other%20formats%2C%20especially%0Ain%20locating%20the%20incorrect%20position%2C%20with%20greater%20benefits%20observed%20in%20larger%0Amodels.%0A%20%20For%20reproducibility%2C%20all%20the%20datasets%20and%20code%20are%20provided%20in%0Ahttps%3A//github.com/bammt/Learn-to-check.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13035v3&entry.124074799=Read"},
{"title": "Vocabulary Expansion for Low-resource Cross-lingual Transfer", "author": "Atsuki Yamaguchi and Aline Villavicencio and Nikolaos Aletras", "abstract": "  Large language models (LLMs) have shown remarkable capabilities in many\nlanguages beyond English. Yet, LLMs require more inference steps when\ngenerating non-English text due to their reliance on English-centric\ntokenizers, vocabulary, and pre-training data, resulting in higher usage costs\nto non-English speakers. Vocabulary expansion with target language tokens is a\nwidely used cross-lingual vocabulary adaptation approach to remedy this issue.\nDespite its effectiveness in inference speedup, the majority of previous work\nhas focused on high-resource settings assuming access to a substantial amount\nof target language data to effectively initialize the embeddings of the new\ntokens and adapt the LLM to the target language. However, vocabulary expansion\nfor LLMs in low-resource settings (i.e. languages and compute) has yet to be\nexplored. In this paper, we investigate sample-efficient adaptation strategies\nfrom different angles, including target vocabulary size and initialization\nmethods, and the amount of target data available for adaptation. Extensive\nexperiments across typologically diverse languages, tasks and models show that\nsimpler heuristic-based embedding initialization is more efficient and robust\nto changes in target vocabulary size and adaptation data in low-resource\nsettings, outperforming a popular random initialization and a more\nsophisticated state-of-the-art approach that relies on external data and model.\n", "link": "http://arxiv.org/abs/2406.11477v1", "date": "2024-06-17", "relevancy": 2.4673, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5097}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5006}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.47}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vocabulary%20Expansion%20for%20Low-resource%20Cross-lingual%20Transfer&body=Title%3A%20Vocabulary%20Expansion%20for%20Low-resource%20Cross-lingual%20Transfer%0AAuthor%3A%20Atsuki%20Yamaguchi%20and%20Aline%20Villavicencio%20and%20Nikolaos%20Aletras%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%20in%20many%0Alanguages%20beyond%20English.%20Yet%2C%20LLMs%20require%20more%20inference%20steps%20when%0Agenerating%20non-English%20text%20due%20to%20their%20reliance%20on%20English-centric%0Atokenizers%2C%20vocabulary%2C%20and%20pre-training%20data%2C%20resulting%20in%20higher%20usage%20costs%0Ato%20non-English%20speakers.%20Vocabulary%20expansion%20with%20target%20language%20tokens%20is%20a%0Awidely%20used%20cross-lingual%20vocabulary%20adaptation%20approach%20to%20remedy%20this%20issue.%0ADespite%20its%20effectiveness%20in%20inference%20speedup%2C%20the%20majority%20of%20previous%20work%0Ahas%20focused%20on%20high-resource%20settings%20assuming%20access%20to%20a%20substantial%20amount%0Aof%20target%20language%20data%20to%20effectively%20initialize%20the%20embeddings%20of%20the%20new%0Atokens%20and%20adapt%20the%20LLM%20to%20the%20target%20language.%20However%2C%20vocabulary%20expansion%0Afor%20LLMs%20in%20low-resource%20settings%20%28i.e.%20languages%20and%20compute%29%20has%20yet%20to%20be%0Aexplored.%20In%20this%20paper%2C%20we%20investigate%20sample-efficient%20adaptation%20strategies%0Afrom%20different%20angles%2C%20including%20target%20vocabulary%20size%20and%20initialization%0Amethods%2C%20and%20the%20amount%20of%20target%20data%20available%20for%20adaptation.%20Extensive%0Aexperiments%20across%20typologically%20diverse%20languages%2C%20tasks%20and%20models%20show%20that%0Asimpler%20heuristic-based%20embedding%20initialization%20is%20more%20efficient%20and%20robust%0Ato%20changes%20in%20target%20vocabulary%20size%20and%20adaptation%20data%20in%20low-resource%0Asettings%2C%20outperforming%20a%20popular%20random%20initialization%20and%20a%20more%0Asophisticated%20state-of-the-art%20approach%20that%20relies%20on%20external%20data%20and%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11477v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVocabulary%2520Expansion%2520for%2520Low-resource%2520Cross-lingual%2520Transfer%26entry.906535625%3DAtsuki%2520Yamaguchi%2520and%2520Aline%2520Villavicencio%2520and%2520Nikolaos%2520Aletras%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520remarkable%2520capabilities%2520in%2520many%250Alanguages%2520beyond%2520English.%2520Yet%252C%2520LLMs%2520require%2520more%2520inference%2520steps%2520when%250Agenerating%2520non-English%2520text%2520due%2520to%2520their%2520reliance%2520on%2520English-centric%250Atokenizers%252C%2520vocabulary%252C%2520and%2520pre-training%2520data%252C%2520resulting%2520in%2520higher%2520usage%2520costs%250Ato%2520non-English%2520speakers.%2520Vocabulary%2520expansion%2520with%2520target%2520language%2520tokens%2520is%2520a%250Awidely%2520used%2520cross-lingual%2520vocabulary%2520adaptation%2520approach%2520to%2520remedy%2520this%2520issue.%250ADespite%2520its%2520effectiveness%2520in%2520inference%2520speedup%252C%2520the%2520majority%2520of%2520previous%2520work%250Ahas%2520focused%2520on%2520high-resource%2520settings%2520assuming%2520access%2520to%2520a%2520substantial%2520amount%250Aof%2520target%2520language%2520data%2520to%2520effectively%2520initialize%2520the%2520embeddings%2520of%2520the%2520new%250Atokens%2520and%2520adapt%2520the%2520LLM%2520to%2520the%2520target%2520language.%2520However%252C%2520vocabulary%2520expansion%250Afor%2520LLMs%2520in%2520low-resource%2520settings%2520%2528i.e.%2520languages%2520and%2520compute%2529%2520has%2520yet%2520to%2520be%250Aexplored.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520sample-efficient%2520adaptation%2520strategies%250Afrom%2520different%2520angles%252C%2520including%2520target%2520vocabulary%2520size%2520and%2520initialization%250Amethods%252C%2520and%2520the%2520amount%2520of%2520target%2520data%2520available%2520for%2520adaptation.%2520Extensive%250Aexperiments%2520across%2520typologically%2520diverse%2520languages%252C%2520tasks%2520and%2520models%2520show%2520that%250Asimpler%2520heuristic-based%2520embedding%2520initialization%2520is%2520more%2520efficient%2520and%2520robust%250Ato%2520changes%2520in%2520target%2520vocabulary%2520size%2520and%2520adaptation%2520data%2520in%2520low-resource%250Asettings%252C%2520outperforming%2520a%2520popular%2520random%2520initialization%2520and%2520a%2520more%250Asophisticated%2520state-of-the-art%2520approach%2520that%2520relies%2520on%2520external%2520data%2520and%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11477v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vocabulary%20Expansion%20for%20Low-resource%20Cross-lingual%20Transfer&entry.906535625=Atsuki%20Yamaguchi%20and%20Aline%20Villavicencio%20and%20Nikolaos%20Aletras&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%20in%20many%0Alanguages%20beyond%20English.%20Yet%2C%20LLMs%20require%20more%20inference%20steps%20when%0Agenerating%20non-English%20text%20due%20to%20their%20reliance%20on%20English-centric%0Atokenizers%2C%20vocabulary%2C%20and%20pre-training%20data%2C%20resulting%20in%20higher%20usage%20costs%0Ato%20non-English%20speakers.%20Vocabulary%20expansion%20with%20target%20language%20tokens%20is%20a%0Awidely%20used%20cross-lingual%20vocabulary%20adaptation%20approach%20to%20remedy%20this%20issue.%0ADespite%20its%20effectiveness%20in%20inference%20speedup%2C%20the%20majority%20of%20previous%20work%0Ahas%20focused%20on%20high-resource%20settings%20assuming%20access%20to%20a%20substantial%20amount%0Aof%20target%20language%20data%20to%20effectively%20initialize%20the%20embeddings%20of%20the%20new%0Atokens%20and%20adapt%20the%20LLM%20to%20the%20target%20language.%20However%2C%20vocabulary%20expansion%0Afor%20LLMs%20in%20low-resource%20settings%20%28i.e.%20languages%20and%20compute%29%20has%20yet%20to%20be%0Aexplored.%20In%20this%20paper%2C%20we%20investigate%20sample-efficient%20adaptation%20strategies%0Afrom%20different%20angles%2C%20including%20target%20vocabulary%20size%20and%20initialization%0Amethods%2C%20and%20the%20amount%20of%20target%20data%20available%20for%20adaptation.%20Extensive%0Aexperiments%20across%20typologically%20diverse%20languages%2C%20tasks%20and%20models%20show%20that%0Asimpler%20heuristic-based%20embedding%20initialization%20is%20more%20efficient%20and%20robust%0Ato%20changes%20in%20target%20vocabulary%20size%20and%20adaptation%20data%20in%20low-resource%0Asettings%2C%20outperforming%20a%20popular%20random%20initialization%20and%20a%20more%0Asophisticated%20state-of-the-art%20approach%20that%20relies%20on%20external%20data%20and%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11477v1&entry.124074799=Read"},
{"title": "Edge Classification on Graphs: New Directions in Topological Imbalance", "author": "Xueqi Cheng and Yu Wang and  Yunchao and  Liu and Yuying Zhao and Charu C. Aggarwal and Tyler Derr", "abstract": "  Recent years have witnessed the remarkable success of applying Graph machine\nlearning (GML) to node/graph classification and link prediction. However, edge\nclassification task that enjoys numerous real-world applications such as social\nnetwork analysis and cybersecurity, has not seen significant advancement. To\naddress this gap, our study pioneers a comprehensive approach to edge\nclassification. We identify a novel `Topological Imbalance Issue', which arises\nfrom the skewed distribution of edges across different classes, affecting the\nlocal subgraph of each edge and harming the performance of edge\nclassifications. Inspired by the recent studies in node classification that the\nperformance discrepancy exists with varying local structural patterns, we aim\nto investigate if the performance discrepancy in topological imbalanced edge\nclassification can also be mitigated by characterizing the local class\ndistribution variance. To overcome this challenge, we introduce Topological\nEntropy (TE), a novel topological-based metric that measures the topological\nimbalance for each edge. Our empirical studies confirm that TE effectively\nmeasures local class distribution variance, and indicate that prioritizing\nedges with high TE values can help address the issue of topological imbalance.\nBased on this, we develop two strategies - Topological Reweighting and TE\nWedge-based Mixup - to focus training on (synthetic) edges based on their TEs.\nWhile topological reweighting directly manipulates training edge weights\naccording to TE, our wedge-based mixup interpolates synthetic edges between\nhigh TE wedges. Ultimately, we integrate these strategies into a novel\ntopological imbalance strategy for edge classification: TopoEdge. Through\nextensive experiments, we demonstrate the efficacy of our proposed strategies\non newly curated datasets and thus establish a new benchmark for (imbalanced)\nedge classification.\n", "link": "http://arxiv.org/abs/2406.11685v1", "date": "2024-06-17", "relevancy": 2.4625, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5078}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4972}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Edge%20Classification%20on%20Graphs%3A%20New%20Directions%20in%20Topological%20Imbalance&body=Title%3A%20Edge%20Classification%20on%20Graphs%3A%20New%20Directions%20in%20Topological%20Imbalance%0AAuthor%3A%20Xueqi%20Cheng%20and%20Yu%20Wang%20and%20%20Yunchao%20and%20%20Liu%20and%20Yuying%20Zhao%20and%20Charu%20C.%20Aggarwal%20and%20Tyler%20Derr%0AAbstract%3A%20%20%20Recent%20years%20have%20witnessed%20the%20remarkable%20success%20of%20applying%20Graph%20machine%0Alearning%20%28GML%29%20to%20node/graph%20classification%20and%20link%20prediction.%20However%2C%20edge%0Aclassification%20task%20that%20enjoys%20numerous%20real-world%20applications%20such%20as%20social%0Anetwork%20analysis%20and%20cybersecurity%2C%20has%20not%20seen%20significant%20advancement.%20To%0Aaddress%20this%20gap%2C%20our%20study%20pioneers%20a%20comprehensive%20approach%20to%20edge%0Aclassification.%20We%20identify%20a%20novel%20%60Topological%20Imbalance%20Issue%27%2C%20which%20arises%0Afrom%20the%20skewed%20distribution%20of%20edges%20across%20different%20classes%2C%20affecting%20the%0Alocal%20subgraph%20of%20each%20edge%20and%20harming%20the%20performance%20of%20edge%0Aclassifications.%20Inspired%20by%20the%20recent%20studies%20in%20node%20classification%20that%20the%0Aperformance%20discrepancy%20exists%20with%20varying%20local%20structural%20patterns%2C%20we%20aim%0Ato%20investigate%20if%20the%20performance%20discrepancy%20in%20topological%20imbalanced%20edge%0Aclassification%20can%20also%20be%20mitigated%20by%20characterizing%20the%20local%20class%0Adistribution%20variance.%20To%20overcome%20this%20challenge%2C%20we%20introduce%20Topological%0AEntropy%20%28TE%29%2C%20a%20novel%20topological-based%20metric%20that%20measures%20the%20topological%0Aimbalance%20for%20each%20edge.%20Our%20empirical%20studies%20confirm%20that%20TE%20effectively%0Ameasures%20local%20class%20distribution%20variance%2C%20and%20indicate%20that%20prioritizing%0Aedges%20with%20high%20TE%20values%20can%20help%20address%20the%20issue%20of%20topological%20imbalance.%0ABased%20on%20this%2C%20we%20develop%20two%20strategies%20-%20Topological%20Reweighting%20and%20TE%0AWedge-based%20Mixup%20-%20to%20focus%20training%20on%20%28synthetic%29%20edges%20based%20on%20their%20TEs.%0AWhile%20topological%20reweighting%20directly%20manipulates%20training%20edge%20weights%0Aaccording%20to%20TE%2C%20our%20wedge-based%20mixup%20interpolates%20synthetic%20edges%20between%0Ahigh%20TE%20wedges.%20Ultimately%2C%20we%20integrate%20these%20strategies%20into%20a%20novel%0Atopological%20imbalance%20strategy%20for%20edge%20classification%3A%20TopoEdge.%20Through%0Aextensive%20experiments%2C%20we%20demonstrate%20the%20efficacy%20of%20our%20proposed%20strategies%0Aon%20newly%20curated%20datasets%20and%20thus%20establish%20a%20new%20benchmark%20for%20%28imbalanced%29%0Aedge%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11685v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEdge%2520Classification%2520on%2520Graphs%253A%2520New%2520Directions%2520in%2520Topological%2520Imbalance%26entry.906535625%3DXueqi%2520Cheng%2520and%2520Yu%2520Wang%2520and%2520%2520Yunchao%2520and%2520%2520Liu%2520and%2520Yuying%2520Zhao%2520and%2520Charu%2520C.%2520Aggarwal%2520and%2520Tyler%2520Derr%26entry.1292438233%3D%2520%2520Recent%2520years%2520have%2520witnessed%2520the%2520remarkable%2520success%2520of%2520applying%2520Graph%2520machine%250Alearning%2520%2528GML%2529%2520to%2520node/graph%2520classification%2520and%2520link%2520prediction.%2520However%252C%2520edge%250Aclassification%2520task%2520that%2520enjoys%2520numerous%2520real-world%2520applications%2520such%2520as%2520social%250Anetwork%2520analysis%2520and%2520cybersecurity%252C%2520has%2520not%2520seen%2520significant%2520advancement.%2520To%250Aaddress%2520this%2520gap%252C%2520our%2520study%2520pioneers%2520a%2520comprehensive%2520approach%2520to%2520edge%250Aclassification.%2520We%2520identify%2520a%2520novel%2520%2560Topological%2520Imbalance%2520Issue%2527%252C%2520which%2520arises%250Afrom%2520the%2520skewed%2520distribution%2520of%2520edges%2520across%2520different%2520classes%252C%2520affecting%2520the%250Alocal%2520subgraph%2520of%2520each%2520edge%2520and%2520harming%2520the%2520performance%2520of%2520edge%250Aclassifications.%2520Inspired%2520by%2520the%2520recent%2520studies%2520in%2520node%2520classification%2520that%2520the%250Aperformance%2520discrepancy%2520exists%2520with%2520varying%2520local%2520structural%2520patterns%252C%2520we%2520aim%250Ato%2520investigate%2520if%2520the%2520performance%2520discrepancy%2520in%2520topological%2520imbalanced%2520edge%250Aclassification%2520can%2520also%2520be%2520mitigated%2520by%2520characterizing%2520the%2520local%2520class%250Adistribution%2520variance.%2520To%2520overcome%2520this%2520challenge%252C%2520we%2520introduce%2520Topological%250AEntropy%2520%2528TE%2529%252C%2520a%2520novel%2520topological-based%2520metric%2520that%2520measures%2520the%2520topological%250Aimbalance%2520for%2520each%2520edge.%2520Our%2520empirical%2520studies%2520confirm%2520that%2520TE%2520effectively%250Ameasures%2520local%2520class%2520distribution%2520variance%252C%2520and%2520indicate%2520that%2520prioritizing%250Aedges%2520with%2520high%2520TE%2520values%2520can%2520help%2520address%2520the%2520issue%2520of%2520topological%2520imbalance.%250ABased%2520on%2520this%252C%2520we%2520develop%2520two%2520strategies%2520-%2520Topological%2520Reweighting%2520and%2520TE%250AWedge-based%2520Mixup%2520-%2520to%2520focus%2520training%2520on%2520%2528synthetic%2529%2520edges%2520based%2520on%2520their%2520TEs.%250AWhile%2520topological%2520reweighting%2520directly%2520manipulates%2520training%2520edge%2520weights%250Aaccording%2520to%2520TE%252C%2520our%2520wedge-based%2520mixup%2520interpolates%2520synthetic%2520edges%2520between%250Ahigh%2520TE%2520wedges.%2520Ultimately%252C%2520we%2520integrate%2520these%2520strategies%2520into%2520a%2520novel%250Atopological%2520imbalance%2520strategy%2520for%2520edge%2520classification%253A%2520TopoEdge.%2520Through%250Aextensive%2520experiments%252C%2520we%2520demonstrate%2520the%2520efficacy%2520of%2520our%2520proposed%2520strategies%250Aon%2520newly%2520curated%2520datasets%2520and%2520thus%2520establish%2520a%2520new%2520benchmark%2520for%2520%2528imbalanced%2529%250Aedge%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11685v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Edge%20Classification%20on%20Graphs%3A%20New%20Directions%20in%20Topological%20Imbalance&entry.906535625=Xueqi%20Cheng%20and%20Yu%20Wang%20and%20%20Yunchao%20and%20%20Liu%20and%20Yuying%20Zhao%20and%20Charu%20C.%20Aggarwal%20and%20Tyler%20Derr&entry.1292438233=%20%20Recent%20years%20have%20witnessed%20the%20remarkable%20success%20of%20applying%20Graph%20machine%0Alearning%20%28GML%29%20to%20node/graph%20classification%20and%20link%20prediction.%20However%2C%20edge%0Aclassification%20task%20that%20enjoys%20numerous%20real-world%20applications%20such%20as%20social%0Anetwork%20analysis%20and%20cybersecurity%2C%20has%20not%20seen%20significant%20advancement.%20To%0Aaddress%20this%20gap%2C%20our%20study%20pioneers%20a%20comprehensive%20approach%20to%20edge%0Aclassification.%20We%20identify%20a%20novel%20%60Topological%20Imbalance%20Issue%27%2C%20which%20arises%0Afrom%20the%20skewed%20distribution%20of%20edges%20across%20different%20classes%2C%20affecting%20the%0Alocal%20subgraph%20of%20each%20edge%20and%20harming%20the%20performance%20of%20edge%0Aclassifications.%20Inspired%20by%20the%20recent%20studies%20in%20node%20classification%20that%20the%0Aperformance%20discrepancy%20exists%20with%20varying%20local%20structural%20patterns%2C%20we%20aim%0Ato%20investigate%20if%20the%20performance%20discrepancy%20in%20topological%20imbalanced%20edge%0Aclassification%20can%20also%20be%20mitigated%20by%20characterizing%20the%20local%20class%0Adistribution%20variance.%20To%20overcome%20this%20challenge%2C%20we%20introduce%20Topological%0AEntropy%20%28TE%29%2C%20a%20novel%20topological-based%20metric%20that%20measures%20the%20topological%0Aimbalance%20for%20each%20edge.%20Our%20empirical%20studies%20confirm%20that%20TE%20effectively%0Ameasures%20local%20class%20distribution%20variance%2C%20and%20indicate%20that%20prioritizing%0Aedges%20with%20high%20TE%20values%20can%20help%20address%20the%20issue%20of%20topological%20imbalance.%0ABased%20on%20this%2C%20we%20develop%20two%20strategies%20-%20Topological%20Reweighting%20and%20TE%0AWedge-based%20Mixup%20-%20to%20focus%20training%20on%20%28synthetic%29%20edges%20based%20on%20their%20TEs.%0AWhile%20topological%20reweighting%20directly%20manipulates%20training%20edge%20weights%0Aaccording%20to%20TE%2C%20our%20wedge-based%20mixup%20interpolates%20synthetic%20edges%20between%0Ahigh%20TE%20wedges.%20Ultimately%2C%20we%20integrate%20these%20strategies%20into%20a%20novel%0Atopological%20imbalance%20strategy%20for%20edge%20classification%3A%20TopoEdge.%20Through%0Aextensive%20experiments%2C%20we%20demonstrate%20the%20efficacy%20of%20our%20proposed%20strategies%0Aon%20newly%20curated%20datasets%20and%20thus%20establish%20a%20new%20benchmark%20for%20%28imbalanced%29%0Aedge%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11685v1&entry.124074799=Read"},
{"title": "CodeGemma: Open Code Models Based on Gemma", "author": " CodeGemma Team", "abstract": "  This paper introduces CodeGemma, a collection of specialized open code models\nbuilt on top of Gemma, capable of a variety of code and natural language\ngeneration tasks. We release three model variants. CodeGemma 7B pretrained (PT)\nand instruction-tuned (IT) variants have remarkably resilient natural language\nunderstanding, excel in mathematical reasoning, and match code capabilities of\nother open models. CodeGemma 2B is a state-of-the-art code completion model\ndesigned for fast code infilling and open-ended generation in latency-sensitive\nsettings.\n", "link": "http://arxiv.org/abs/2406.11409v1", "date": "2024-06-17", "relevancy": 2.4537, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5124}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4956}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4642}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CodeGemma%3A%20Open%20Code%20Models%20Based%20on%20Gemma&body=Title%3A%20CodeGemma%3A%20Open%20Code%20Models%20Based%20on%20Gemma%0AAuthor%3A%20%20CodeGemma%20Team%0AAbstract%3A%20%20%20This%20paper%20introduces%20CodeGemma%2C%20a%20collection%20of%20specialized%20open%20code%20models%0Abuilt%20on%20top%20of%20Gemma%2C%20capable%20of%20a%20variety%20of%20code%20and%20natural%20language%0Ageneration%20tasks.%20We%20release%20three%20model%20variants.%20CodeGemma%207B%20pretrained%20%28PT%29%0Aand%20instruction-tuned%20%28IT%29%20variants%20have%20remarkably%20resilient%20natural%20language%0Aunderstanding%2C%20excel%20in%20mathematical%20reasoning%2C%20and%20match%20code%20capabilities%20of%0Aother%20open%20models.%20CodeGemma%202B%20is%20a%20state-of-the-art%20code%20completion%20model%0Adesigned%20for%20fast%20code%20infilling%20and%20open-ended%20generation%20in%20latency-sensitive%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11409v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCodeGemma%253A%2520Open%2520Code%2520Models%2520Based%2520on%2520Gemma%26entry.906535625%3D%2520CodeGemma%2520Team%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520CodeGemma%252C%2520a%2520collection%2520of%2520specialized%2520open%2520code%2520models%250Abuilt%2520on%2520top%2520of%2520Gemma%252C%2520capable%2520of%2520a%2520variety%2520of%2520code%2520and%2520natural%2520language%250Ageneration%2520tasks.%2520We%2520release%2520three%2520model%2520variants.%2520CodeGemma%25207B%2520pretrained%2520%2528PT%2529%250Aand%2520instruction-tuned%2520%2528IT%2529%2520variants%2520have%2520remarkably%2520resilient%2520natural%2520language%250Aunderstanding%252C%2520excel%2520in%2520mathematical%2520reasoning%252C%2520and%2520match%2520code%2520capabilities%2520of%250Aother%2520open%2520models.%2520CodeGemma%25202B%2520is%2520a%2520state-of-the-art%2520code%2520completion%2520model%250Adesigned%2520for%2520fast%2520code%2520infilling%2520and%2520open-ended%2520generation%2520in%2520latency-sensitive%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11409v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CodeGemma%3A%20Open%20Code%20Models%20Based%20on%20Gemma&entry.906535625=%20CodeGemma%20Team&entry.1292438233=%20%20This%20paper%20introduces%20CodeGemma%2C%20a%20collection%20of%20specialized%20open%20code%20models%0Abuilt%20on%20top%20of%20Gemma%2C%20capable%20of%20a%20variety%20of%20code%20and%20natural%20language%0Ageneration%20tasks.%20We%20release%20three%20model%20variants.%20CodeGemma%207B%20pretrained%20%28PT%29%0Aand%20instruction-tuned%20%28IT%29%20variants%20have%20remarkably%20resilient%20natural%20language%0Aunderstanding%2C%20excel%20in%20mathematical%20reasoning%2C%20and%20match%20code%20capabilities%20of%0Aother%20open%20models.%20CodeGemma%202B%20is%20a%20state-of-the-art%20code%20completion%20model%0Adesigned%20for%20fast%20code%20infilling%20and%20open-ended%20generation%20in%20latency-sensitive%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11409v1&entry.124074799=Read"},
{"title": "Correspondence Free Multivector Cloud Registration using Conformal\n  Geometric Algebra", "author": "Francisco Xavier Vasconcelos and Jacinto C. Nascimento", "abstract": "  We present, for the first time, a novel theoretical approach to address the\nproblem of correspondence free multivector cloud registration in conformal\ngeometric algebra. Such formalism achieves several favorable properties.\nPrimarily, it forms an orthogonal automorphism that extends beyond the typical\nvector space to the entire conformal geometric algebra while respecting the\nmultivector grading. Concretely, the registration can be viewed as an\northogonal transformation (\\it i.e., scale, translation, rotation) belonging to\n$SO(4,1)$ - group of special orthogonal transformations in conformal geometric\nalgebra. We will show that such formalism is able to: $(i)$ perform the\nregistration without directly accessing the input multivectors. Instead, we use\nprimitives or geometric objects provided by the conformal model - the\nmultivectors, $(ii)$ the geometric objects are obtained by solving a\nmultilinear eigenvalue problem to find sets of eigenmultivectors. In this way,\nwe can explicitly avoid solving the correspondences in the registration\nprocess. Most importantly, this offers rotation and translation equivariant\nproperties between the input multivectors and the eigenmultivectors.\nExperimental evaluation is conducted in datasets commonly used in point cloud\nregistration, to testify the usefulness of the approach with emphasis to\nambiguities arising from high levels of noise. The code is available at\nhttps://github.com/Numerical-Geometric-Algebra/RegistrationGA . This work was\nsubmitted to the International Journal of Computer Vision and is currently\nunder review.\n", "link": "http://arxiv.org/abs/2406.11732v1", "date": "2024-06-17", "relevancy": 2.4503, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.513}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4928}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4643}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Correspondence%20Free%20Multivector%20Cloud%20Registration%20using%20Conformal%0A%20%20Geometric%20Algebra&body=Title%3A%20Correspondence%20Free%20Multivector%20Cloud%20Registration%20using%20Conformal%0A%20%20Geometric%20Algebra%0AAuthor%3A%20Francisco%20Xavier%20Vasconcelos%20and%20Jacinto%20C.%20Nascimento%0AAbstract%3A%20%20%20We%20present%2C%20for%20the%20first%20time%2C%20a%20novel%20theoretical%20approach%20to%20address%20the%0Aproblem%20of%20correspondence%20free%20multivector%20cloud%20registration%20in%20conformal%0Ageometric%20algebra.%20Such%20formalism%20achieves%20several%20favorable%20properties.%0APrimarily%2C%20it%20forms%20an%20orthogonal%20automorphism%20that%20extends%20beyond%20the%20typical%0Avector%20space%20to%20the%20entire%20conformal%20geometric%20algebra%20while%20respecting%20the%0Amultivector%20grading.%20Concretely%2C%20the%20registration%20can%20be%20viewed%20as%20an%0Aorthogonal%20transformation%20%28%5Cit%20i.e.%2C%20scale%2C%20translation%2C%20rotation%29%20belonging%20to%0A%24SO%284%2C1%29%24%20-%20group%20of%20special%20orthogonal%20transformations%20in%20conformal%20geometric%0Aalgebra.%20We%20will%20show%20that%20such%20formalism%20is%20able%20to%3A%20%24%28i%29%24%20perform%20the%0Aregistration%20without%20directly%20accessing%20the%20input%20multivectors.%20Instead%2C%20we%20use%0Aprimitives%20or%20geometric%20objects%20provided%20by%20the%20conformal%20model%20-%20the%0Amultivectors%2C%20%24%28ii%29%24%20the%20geometric%20objects%20are%20obtained%20by%20solving%20a%0Amultilinear%20eigenvalue%20problem%20to%20find%20sets%20of%20eigenmultivectors.%20In%20this%20way%2C%0Awe%20can%20explicitly%20avoid%20solving%20the%20correspondences%20in%20the%20registration%0Aprocess.%20Most%20importantly%2C%20this%20offers%20rotation%20and%20translation%20equivariant%0Aproperties%20between%20the%20input%20multivectors%20and%20the%20eigenmultivectors.%0AExperimental%20evaluation%20is%20conducted%20in%20datasets%20commonly%20used%20in%20point%20cloud%0Aregistration%2C%20to%20testify%20the%20usefulness%20of%20the%20approach%20with%20emphasis%20to%0Aambiguities%20arising%20from%20high%20levels%20of%20noise.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Numerical-Geometric-Algebra/RegistrationGA%20.%20This%20work%20was%0Asubmitted%20to%20the%20International%20Journal%20of%20Computer%20Vision%20and%20is%20currently%0Aunder%20review.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11732v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCorrespondence%2520Free%2520Multivector%2520Cloud%2520Registration%2520using%2520Conformal%250A%2520%2520Geometric%2520Algebra%26entry.906535625%3DFrancisco%2520Xavier%2520Vasconcelos%2520and%2520Jacinto%2520C.%2520Nascimento%26entry.1292438233%3D%2520%2520We%2520present%252C%2520for%2520the%2520first%2520time%252C%2520a%2520novel%2520theoretical%2520approach%2520to%2520address%2520the%250Aproblem%2520of%2520correspondence%2520free%2520multivector%2520cloud%2520registration%2520in%2520conformal%250Ageometric%2520algebra.%2520Such%2520formalism%2520achieves%2520several%2520favorable%2520properties.%250APrimarily%252C%2520it%2520forms%2520an%2520orthogonal%2520automorphism%2520that%2520extends%2520beyond%2520the%2520typical%250Avector%2520space%2520to%2520the%2520entire%2520conformal%2520geometric%2520algebra%2520while%2520respecting%2520the%250Amultivector%2520grading.%2520Concretely%252C%2520the%2520registration%2520can%2520be%2520viewed%2520as%2520an%250Aorthogonal%2520transformation%2520%2528%255Cit%2520i.e.%252C%2520scale%252C%2520translation%252C%2520rotation%2529%2520belonging%2520to%250A%2524SO%25284%252C1%2529%2524%2520-%2520group%2520of%2520special%2520orthogonal%2520transformations%2520in%2520conformal%2520geometric%250Aalgebra.%2520We%2520will%2520show%2520that%2520such%2520formalism%2520is%2520able%2520to%253A%2520%2524%2528i%2529%2524%2520perform%2520the%250Aregistration%2520without%2520directly%2520accessing%2520the%2520input%2520multivectors.%2520Instead%252C%2520we%2520use%250Aprimitives%2520or%2520geometric%2520objects%2520provided%2520by%2520the%2520conformal%2520model%2520-%2520the%250Amultivectors%252C%2520%2524%2528ii%2529%2524%2520the%2520geometric%2520objects%2520are%2520obtained%2520by%2520solving%2520a%250Amultilinear%2520eigenvalue%2520problem%2520to%2520find%2520sets%2520of%2520eigenmultivectors.%2520In%2520this%2520way%252C%250Awe%2520can%2520explicitly%2520avoid%2520solving%2520the%2520correspondences%2520in%2520the%2520registration%250Aprocess.%2520Most%2520importantly%252C%2520this%2520offers%2520rotation%2520and%2520translation%2520equivariant%250Aproperties%2520between%2520the%2520input%2520multivectors%2520and%2520the%2520eigenmultivectors.%250AExperimental%2520evaluation%2520is%2520conducted%2520in%2520datasets%2520commonly%2520used%2520in%2520point%2520cloud%250Aregistration%252C%2520to%2520testify%2520the%2520usefulness%2520of%2520the%2520approach%2520with%2520emphasis%2520to%250Aambiguities%2520arising%2520from%2520high%2520levels%2520of%2520noise.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Numerical-Geometric-Algebra/RegistrationGA%2520.%2520This%2520work%2520was%250Asubmitted%2520to%2520the%2520International%2520Journal%2520of%2520Computer%2520Vision%2520and%2520is%2520currently%250Aunder%2520review.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11732v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Correspondence%20Free%20Multivector%20Cloud%20Registration%20using%20Conformal%0A%20%20Geometric%20Algebra&entry.906535625=Francisco%20Xavier%20Vasconcelos%20and%20Jacinto%20C.%20Nascimento&entry.1292438233=%20%20We%20present%2C%20for%20the%20first%20time%2C%20a%20novel%20theoretical%20approach%20to%20address%20the%0Aproblem%20of%20correspondence%20free%20multivector%20cloud%20registration%20in%20conformal%0Ageometric%20algebra.%20Such%20formalism%20achieves%20several%20favorable%20properties.%0APrimarily%2C%20it%20forms%20an%20orthogonal%20automorphism%20that%20extends%20beyond%20the%20typical%0Avector%20space%20to%20the%20entire%20conformal%20geometric%20algebra%20while%20respecting%20the%0Amultivector%20grading.%20Concretely%2C%20the%20registration%20can%20be%20viewed%20as%20an%0Aorthogonal%20transformation%20%28%5Cit%20i.e.%2C%20scale%2C%20translation%2C%20rotation%29%20belonging%20to%0A%24SO%284%2C1%29%24%20-%20group%20of%20special%20orthogonal%20transformations%20in%20conformal%20geometric%0Aalgebra.%20We%20will%20show%20that%20such%20formalism%20is%20able%20to%3A%20%24%28i%29%24%20perform%20the%0Aregistration%20without%20directly%20accessing%20the%20input%20multivectors.%20Instead%2C%20we%20use%0Aprimitives%20or%20geometric%20objects%20provided%20by%20the%20conformal%20model%20-%20the%0Amultivectors%2C%20%24%28ii%29%24%20the%20geometric%20objects%20are%20obtained%20by%20solving%20a%0Amultilinear%20eigenvalue%20problem%20to%20find%20sets%20of%20eigenmultivectors.%20In%20this%20way%2C%0Awe%20can%20explicitly%20avoid%20solving%20the%20correspondences%20in%20the%20registration%0Aprocess.%20Most%20importantly%2C%20this%20offers%20rotation%20and%20translation%20equivariant%0Aproperties%20between%20the%20input%20multivectors%20and%20the%20eigenmultivectors.%0AExperimental%20evaluation%20is%20conducted%20in%20datasets%20commonly%20used%20in%20point%20cloud%0Aregistration%2C%20to%20testify%20the%20usefulness%20of%20the%20approach%20with%20emphasis%20to%0Aambiguities%20arising%20from%20high%20levels%20of%20noise.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Numerical-Geometric-Algebra/RegistrationGA%20.%20This%20work%20was%0Asubmitted%20to%20the%20International%20Journal%20of%20Computer%20Vision%20and%20is%20currently%0Aunder%20review.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11732v1&entry.124074799=Read"},
{"title": "Zero-Shot Generalization during Instruction Tuning: Insights from\n  Similarity and Granularity", "author": "Bingxiang He and Ning Ding and Cheng Qian and Jia Deng and Ganqu Cui and Lifan Yuan and Huan-ang Gao and Huimin Chen and Zhiyuan Liu and Maosong Sun", "abstract": "  Understanding alignment techniques begins with comprehending zero-shot\ngeneralization brought by instruction tuning, but little of the mechanism has\nbeen understood. Existing work has largely been confined to the task level,\nwithout considering that tasks are artificially defined and, to LLMs, merely\nconsist of tokens and representations. This line of research has been limited\nto examining transfer between tasks from a task-pair perspective, with few\nstudies focusing on understanding zero-shot generalization from the perspective\nof the data itself. To bridge this gap, we first demonstrate through multiple\nmetrics that zero-shot generalization during instruction tuning happens very\nearly. Next, we investigate the facilitation of zero-shot generalization from\nboth data similarity and granularity perspectives, confirming that encountering\nhighly similar and fine-grained training data earlier during instruction\ntuning, without the constraints of defined \"tasks\", enables better\ngeneralization. Finally, we propose a more grounded training data arrangement\nmethod, Test-centric Multi-turn Arrangement, and show its effectiveness in\npromoting continual learning and further loss reduction. For the first time, we\nshow that zero-shot generalization during instruction tuning is a form of\nsimilarity-based generalization between training and test data at the instance\nlevel. We hope our analysis will advance the understanding of zero-shot\ngeneralization during instruction tuning and contribute to the development of\nmore aligned LLMs. Our code is released at\nhttps://github.com/HBX-hbx/dynamics_of_zero-shot_generalization.\n", "link": "http://arxiv.org/abs/2406.11721v1", "date": "2024-06-17", "relevancy": 2.4497, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5063}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4823}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Generalization%20during%20Instruction%20Tuning%3A%20Insights%20from%0A%20%20Similarity%20and%20Granularity&body=Title%3A%20Zero-Shot%20Generalization%20during%20Instruction%20Tuning%3A%20Insights%20from%0A%20%20Similarity%20and%20Granularity%0AAuthor%3A%20Bingxiang%20He%20and%20Ning%20Ding%20and%20Cheng%20Qian%20and%20Jia%20Deng%20and%20Ganqu%20Cui%20and%20Lifan%20Yuan%20and%20Huan-ang%20Gao%20and%20Huimin%20Chen%20and%20Zhiyuan%20Liu%20and%20Maosong%20Sun%0AAbstract%3A%20%20%20Understanding%20alignment%20techniques%20begins%20with%20comprehending%20zero-shot%0Ageneralization%20brought%20by%20instruction%20tuning%2C%20but%20little%20of%20the%20mechanism%20has%0Abeen%20understood.%20Existing%20work%20has%20largely%20been%20confined%20to%20the%20task%20level%2C%0Awithout%20considering%20that%20tasks%20are%20artificially%20defined%20and%2C%20to%20LLMs%2C%20merely%0Aconsist%20of%20tokens%20and%20representations.%20This%20line%20of%20research%20has%20been%20limited%0Ato%20examining%20transfer%20between%20tasks%20from%20a%20task-pair%20perspective%2C%20with%20few%0Astudies%20focusing%20on%20understanding%20zero-shot%20generalization%20from%20the%20perspective%0Aof%20the%20data%20itself.%20To%20bridge%20this%20gap%2C%20we%20first%20demonstrate%20through%20multiple%0Ametrics%20that%20zero-shot%20generalization%20during%20instruction%20tuning%20happens%20very%0Aearly.%20Next%2C%20we%20investigate%20the%20facilitation%20of%20zero-shot%20generalization%20from%0Aboth%20data%20similarity%20and%20granularity%20perspectives%2C%20confirming%20that%20encountering%0Ahighly%20similar%20and%20fine-grained%20training%20data%20earlier%20during%20instruction%0Atuning%2C%20without%20the%20constraints%20of%20defined%20%22tasks%22%2C%20enables%20better%0Ageneralization.%20Finally%2C%20we%20propose%20a%20more%20grounded%20training%20data%20arrangement%0Amethod%2C%20Test-centric%20Multi-turn%20Arrangement%2C%20and%20show%20its%20effectiveness%20in%0Apromoting%20continual%20learning%20and%20further%20loss%20reduction.%20For%20the%20first%20time%2C%20we%0Ashow%20that%20zero-shot%20generalization%20during%20instruction%20tuning%20is%20a%20form%20of%0Asimilarity-based%20generalization%20between%20training%20and%20test%20data%20at%20the%20instance%0Alevel.%20We%20hope%20our%20analysis%20will%20advance%20the%20understanding%20of%20zero-shot%0Ageneralization%20during%20instruction%20tuning%20and%20contribute%20to%20the%20development%20of%0Amore%20aligned%20LLMs.%20Our%20code%20is%20released%20at%0Ahttps%3A//github.com/HBX-hbx/dynamics_of_zero-shot_generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11721v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Generalization%2520during%2520Instruction%2520Tuning%253A%2520Insights%2520from%250A%2520%2520Similarity%2520and%2520Granularity%26entry.906535625%3DBingxiang%2520He%2520and%2520Ning%2520Ding%2520and%2520Cheng%2520Qian%2520and%2520Jia%2520Deng%2520and%2520Ganqu%2520Cui%2520and%2520Lifan%2520Yuan%2520and%2520Huan-ang%2520Gao%2520and%2520Huimin%2520Chen%2520and%2520Zhiyuan%2520Liu%2520and%2520Maosong%2520Sun%26entry.1292438233%3D%2520%2520Understanding%2520alignment%2520techniques%2520begins%2520with%2520comprehending%2520zero-shot%250Ageneralization%2520brought%2520by%2520instruction%2520tuning%252C%2520but%2520little%2520of%2520the%2520mechanism%2520has%250Abeen%2520understood.%2520Existing%2520work%2520has%2520largely%2520been%2520confined%2520to%2520the%2520task%2520level%252C%250Awithout%2520considering%2520that%2520tasks%2520are%2520artificially%2520defined%2520and%252C%2520to%2520LLMs%252C%2520merely%250Aconsist%2520of%2520tokens%2520and%2520representations.%2520This%2520line%2520of%2520research%2520has%2520been%2520limited%250Ato%2520examining%2520transfer%2520between%2520tasks%2520from%2520a%2520task-pair%2520perspective%252C%2520with%2520few%250Astudies%2520focusing%2520on%2520understanding%2520zero-shot%2520generalization%2520from%2520the%2520perspective%250Aof%2520the%2520data%2520itself.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520first%2520demonstrate%2520through%2520multiple%250Ametrics%2520that%2520zero-shot%2520generalization%2520during%2520instruction%2520tuning%2520happens%2520very%250Aearly.%2520Next%252C%2520we%2520investigate%2520the%2520facilitation%2520of%2520zero-shot%2520generalization%2520from%250Aboth%2520data%2520similarity%2520and%2520granularity%2520perspectives%252C%2520confirming%2520that%2520encountering%250Ahighly%2520similar%2520and%2520fine-grained%2520training%2520data%2520earlier%2520during%2520instruction%250Atuning%252C%2520without%2520the%2520constraints%2520of%2520defined%2520%2522tasks%2522%252C%2520enables%2520better%250Ageneralization.%2520Finally%252C%2520we%2520propose%2520a%2520more%2520grounded%2520training%2520data%2520arrangement%250Amethod%252C%2520Test-centric%2520Multi-turn%2520Arrangement%252C%2520and%2520show%2520its%2520effectiveness%2520in%250Apromoting%2520continual%2520learning%2520and%2520further%2520loss%2520reduction.%2520For%2520the%2520first%2520time%252C%2520we%250Ashow%2520that%2520zero-shot%2520generalization%2520during%2520instruction%2520tuning%2520is%2520a%2520form%2520of%250Asimilarity-based%2520generalization%2520between%2520training%2520and%2520test%2520data%2520at%2520the%2520instance%250Alevel.%2520We%2520hope%2520our%2520analysis%2520will%2520advance%2520the%2520understanding%2520of%2520zero-shot%250Ageneralization%2520during%2520instruction%2520tuning%2520and%2520contribute%2520to%2520the%2520development%2520of%250Amore%2520aligned%2520LLMs.%2520Our%2520code%2520is%2520released%2520at%250Ahttps%253A//github.com/HBX-hbx/dynamics_of_zero-shot_generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11721v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Generalization%20during%20Instruction%20Tuning%3A%20Insights%20from%0A%20%20Similarity%20and%20Granularity&entry.906535625=Bingxiang%20He%20and%20Ning%20Ding%20and%20Cheng%20Qian%20and%20Jia%20Deng%20and%20Ganqu%20Cui%20and%20Lifan%20Yuan%20and%20Huan-ang%20Gao%20and%20Huimin%20Chen%20and%20Zhiyuan%20Liu%20and%20Maosong%20Sun&entry.1292438233=%20%20Understanding%20alignment%20techniques%20begins%20with%20comprehending%20zero-shot%0Ageneralization%20brought%20by%20instruction%20tuning%2C%20but%20little%20of%20the%20mechanism%20has%0Abeen%20understood.%20Existing%20work%20has%20largely%20been%20confined%20to%20the%20task%20level%2C%0Awithout%20considering%20that%20tasks%20are%20artificially%20defined%20and%2C%20to%20LLMs%2C%20merely%0Aconsist%20of%20tokens%20and%20representations.%20This%20line%20of%20research%20has%20been%20limited%0Ato%20examining%20transfer%20between%20tasks%20from%20a%20task-pair%20perspective%2C%20with%20few%0Astudies%20focusing%20on%20understanding%20zero-shot%20generalization%20from%20the%20perspective%0Aof%20the%20data%20itself.%20To%20bridge%20this%20gap%2C%20we%20first%20demonstrate%20through%20multiple%0Ametrics%20that%20zero-shot%20generalization%20during%20instruction%20tuning%20happens%20very%0Aearly.%20Next%2C%20we%20investigate%20the%20facilitation%20of%20zero-shot%20generalization%20from%0Aboth%20data%20similarity%20and%20granularity%20perspectives%2C%20confirming%20that%20encountering%0Ahighly%20similar%20and%20fine-grained%20training%20data%20earlier%20during%20instruction%0Atuning%2C%20without%20the%20constraints%20of%20defined%20%22tasks%22%2C%20enables%20better%0Ageneralization.%20Finally%2C%20we%20propose%20a%20more%20grounded%20training%20data%20arrangement%0Amethod%2C%20Test-centric%20Multi-turn%20Arrangement%2C%20and%20show%20its%20effectiveness%20in%0Apromoting%20continual%20learning%20and%20further%20loss%20reduction.%20For%20the%20first%20time%2C%20we%0Ashow%20that%20zero-shot%20generalization%20during%20instruction%20tuning%20is%20a%20form%20of%0Asimilarity-based%20generalization%20between%20training%20and%20test%20data%20at%20the%20instance%0Alevel.%20We%20hope%20our%20analysis%20will%20advance%20the%20understanding%20of%20zero-shot%0Ageneralization%20during%20instruction%20tuning%20and%20contribute%20to%20the%20development%20of%0Amore%20aligned%20LLMs.%20Our%20code%20is%20released%20at%0Ahttps%3A//github.com/HBX-hbx/dynamics_of_zero-shot_generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11721v1&entry.124074799=Read"},
{"title": "P-TA: Using Proximal Policy Optimization to Enhance Tabular Data\n  Augmentation via Large Language Models", "author": "Shuo Yang and Chenchen Yuan and Yao Rong and Felix Steinbauer and Gjergji Kasneci", "abstract": "  A multitude of industries depend on accurate and reasonable tabular data\naugmentation for their business processes. Contemporary methodologies in\ngenerating tabular data revolve around utilizing Generative Adversarial\nNetworks (GAN) or fine-tuning Large Language Models (LLM). However, GAN-based\napproaches are documented to produce samples with common-sense errors\nattributed to the absence of external knowledge. On the other hand, LLM-based\nmethods exhibit a limited capacity to capture the disparities between\nsynthesized and actual data distribution due to the absence of feedback from a\ndiscriminator during training. Furthermore, the decoding of LLM-based\ngeneration introduces gradient breakpoints, impeding the backpropagation of\nloss from a discriminator, thereby complicating the integration of these two\napproaches. To solve this challenge, we propose using proximal policy\noptimization (PPO) to apply GANs, guiding LLMs to enhance the probability\ndistribution of tabular features. This approach enables the utilization of LLMs\nas generators for GANs in synthesizing tabular data. Our experiments\ndemonstrate that PPO leads to an approximately 4\\% improvement in the accuracy\nof models trained on synthetically generated data over state-of-the-art across\nthree real-world datasets.\n", "link": "http://arxiv.org/abs/2406.11391v1", "date": "2024-06-17", "relevancy": 2.4494, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4961}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4884}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20P-TA%3A%20Using%20Proximal%20Policy%20Optimization%20to%20Enhance%20Tabular%20Data%0A%20%20Augmentation%20via%20Large%20Language%20Models&body=Title%3A%20P-TA%3A%20Using%20Proximal%20Policy%20Optimization%20to%20Enhance%20Tabular%20Data%0A%20%20Augmentation%20via%20Large%20Language%20Models%0AAuthor%3A%20Shuo%20Yang%20and%20Chenchen%20Yuan%20and%20Yao%20Rong%20and%20Felix%20Steinbauer%20and%20Gjergji%20Kasneci%0AAbstract%3A%20%20%20A%20multitude%20of%20industries%20depend%20on%20accurate%20and%20reasonable%20tabular%20data%0Aaugmentation%20for%20their%20business%20processes.%20Contemporary%20methodologies%20in%0Agenerating%20tabular%20data%20revolve%20around%20utilizing%20Generative%20Adversarial%0ANetworks%20%28GAN%29%20or%20fine-tuning%20Large%20Language%20Models%20%28LLM%29.%20However%2C%20GAN-based%0Aapproaches%20are%20documented%20to%20produce%20samples%20with%20common-sense%20errors%0Aattributed%20to%20the%20absence%20of%20external%20knowledge.%20On%20the%20other%20hand%2C%20LLM-based%0Amethods%20exhibit%20a%20limited%20capacity%20to%20capture%20the%20disparities%20between%0Asynthesized%20and%20actual%20data%20distribution%20due%20to%20the%20absence%20of%20feedback%20from%20a%0Adiscriminator%20during%20training.%20Furthermore%2C%20the%20decoding%20of%20LLM-based%0Ageneration%20introduces%20gradient%20breakpoints%2C%20impeding%20the%20backpropagation%20of%0Aloss%20from%20a%20discriminator%2C%20thereby%20complicating%20the%20integration%20of%20these%20two%0Aapproaches.%20To%20solve%20this%20challenge%2C%20we%20propose%20using%20proximal%20policy%0Aoptimization%20%28PPO%29%20to%20apply%20GANs%2C%20guiding%20LLMs%20to%20enhance%20the%20probability%0Adistribution%20of%20tabular%20features.%20This%20approach%20enables%20the%20utilization%20of%20LLMs%0Aas%20generators%20for%20GANs%20in%20synthesizing%20tabular%20data.%20Our%20experiments%0Ademonstrate%20that%20PPO%20leads%20to%20an%20approximately%204%5C%25%20improvement%20in%20the%20accuracy%0Aof%20models%20trained%20on%20synthetically%20generated%20data%20over%20state-of-the-art%20across%0Athree%20real-world%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11391v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DP-TA%253A%2520Using%2520Proximal%2520Policy%2520Optimization%2520to%2520Enhance%2520Tabular%2520Data%250A%2520%2520Augmentation%2520via%2520Large%2520Language%2520Models%26entry.906535625%3DShuo%2520Yang%2520and%2520Chenchen%2520Yuan%2520and%2520Yao%2520Rong%2520and%2520Felix%2520Steinbauer%2520and%2520Gjergji%2520Kasneci%26entry.1292438233%3D%2520%2520A%2520multitude%2520of%2520industries%2520depend%2520on%2520accurate%2520and%2520reasonable%2520tabular%2520data%250Aaugmentation%2520for%2520their%2520business%2520processes.%2520Contemporary%2520methodologies%2520in%250Agenerating%2520tabular%2520data%2520revolve%2520around%2520utilizing%2520Generative%2520Adversarial%250ANetworks%2520%2528GAN%2529%2520or%2520fine-tuning%2520Large%2520Language%2520Models%2520%2528LLM%2529.%2520However%252C%2520GAN-based%250Aapproaches%2520are%2520documented%2520to%2520produce%2520samples%2520with%2520common-sense%2520errors%250Aattributed%2520to%2520the%2520absence%2520of%2520external%2520knowledge.%2520On%2520the%2520other%2520hand%252C%2520LLM-based%250Amethods%2520exhibit%2520a%2520limited%2520capacity%2520to%2520capture%2520the%2520disparities%2520between%250Asynthesized%2520and%2520actual%2520data%2520distribution%2520due%2520to%2520the%2520absence%2520of%2520feedback%2520from%2520a%250Adiscriminator%2520during%2520training.%2520Furthermore%252C%2520the%2520decoding%2520of%2520LLM-based%250Ageneration%2520introduces%2520gradient%2520breakpoints%252C%2520impeding%2520the%2520backpropagation%2520of%250Aloss%2520from%2520a%2520discriminator%252C%2520thereby%2520complicating%2520the%2520integration%2520of%2520these%2520two%250Aapproaches.%2520To%2520solve%2520this%2520challenge%252C%2520we%2520propose%2520using%2520proximal%2520policy%250Aoptimization%2520%2528PPO%2529%2520to%2520apply%2520GANs%252C%2520guiding%2520LLMs%2520to%2520enhance%2520the%2520probability%250Adistribution%2520of%2520tabular%2520features.%2520This%2520approach%2520enables%2520the%2520utilization%2520of%2520LLMs%250Aas%2520generators%2520for%2520GANs%2520in%2520synthesizing%2520tabular%2520data.%2520Our%2520experiments%250Ademonstrate%2520that%2520PPO%2520leads%2520to%2520an%2520approximately%25204%255C%2525%2520improvement%2520in%2520the%2520accuracy%250Aof%2520models%2520trained%2520on%2520synthetically%2520generated%2520data%2520over%2520state-of-the-art%2520across%250Athree%2520real-world%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11391v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=P-TA%3A%20Using%20Proximal%20Policy%20Optimization%20to%20Enhance%20Tabular%20Data%0A%20%20Augmentation%20via%20Large%20Language%20Models&entry.906535625=Shuo%20Yang%20and%20Chenchen%20Yuan%20and%20Yao%20Rong%20and%20Felix%20Steinbauer%20and%20Gjergji%20Kasneci&entry.1292438233=%20%20A%20multitude%20of%20industries%20depend%20on%20accurate%20and%20reasonable%20tabular%20data%0Aaugmentation%20for%20their%20business%20processes.%20Contemporary%20methodologies%20in%0Agenerating%20tabular%20data%20revolve%20around%20utilizing%20Generative%20Adversarial%0ANetworks%20%28GAN%29%20or%20fine-tuning%20Large%20Language%20Models%20%28LLM%29.%20However%2C%20GAN-based%0Aapproaches%20are%20documented%20to%20produce%20samples%20with%20common-sense%20errors%0Aattributed%20to%20the%20absence%20of%20external%20knowledge.%20On%20the%20other%20hand%2C%20LLM-based%0Amethods%20exhibit%20a%20limited%20capacity%20to%20capture%20the%20disparities%20between%0Asynthesized%20and%20actual%20data%20distribution%20due%20to%20the%20absence%20of%20feedback%20from%20a%0Adiscriminator%20during%20training.%20Furthermore%2C%20the%20decoding%20of%20LLM-based%0Ageneration%20introduces%20gradient%20breakpoints%2C%20impeding%20the%20backpropagation%20of%0Aloss%20from%20a%20discriminator%2C%20thereby%20complicating%20the%20integration%20of%20these%20two%0Aapproaches.%20To%20solve%20this%20challenge%2C%20we%20propose%20using%20proximal%20policy%0Aoptimization%20%28PPO%29%20to%20apply%20GANs%2C%20guiding%20LLMs%20to%20enhance%20the%20probability%0Adistribution%20of%20tabular%20features.%20This%20approach%20enables%20the%20utilization%20of%20LLMs%0Aas%20generators%20for%20GANs%20in%20synthesizing%20tabular%20data.%20Our%20experiments%0Ademonstrate%20that%20PPO%20leads%20to%20an%20approximately%204%5C%25%20improvement%20in%20the%20accuracy%0Aof%20models%20trained%20on%20synthetically%20generated%20data%20over%20state-of-the-art%20across%0Athree%20real-world%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11391v1&entry.124074799=Read"},
{"title": "ChildDiffusion: Unlocking the Potential of Generative AI and\n  Controllable Augmentations for Child Facial Data using Stable Diffusion and\n  Large Language Models", "author": "Muhammad Ali Farooq and Wang Yao and Peter Corcoran", "abstract": "  In this research work we have proposed high-level ChildDiffusion framework\ncapable of generating photorealistic child facial samples and further embedding\nseveral intelligent augmentations on child facial data using short text\nprompts, detailed textual guidance from LLMs, and further image to image\ntransformation using text guidance control conditioning thus providing an\nopportunity to curate fully synthetic large scale child datasets. The framework\nis validated by rendering high-quality child faces representing ethnicity data,\nmicro expressions, face pose variations, eye blinking effects, facial\naccessories, different hair colours and styles, aging, multiple and different\nchild gender subjects in a single frame. Addressing privacy concerns regarding\nchild data acquisition requires a comprehensive approach that involves legal,\nethical, and technological considerations. Keeping this in view this framework\ncan be adapted to synthesise child facial data which can be effectively used\nfor numerous downstream machine learning tasks. The proposed method circumvents\ncommon issues encountered in generative AI tools, such as temporal\ninconsistency and limited control over the rendered outputs. As an exemplary\nuse case we have open-sourced child ethnicity data consisting of 2.5k child\nfacial samples of five different classes which includes African, Asian, White,\nSouth Asian/ Indian, and Hispanic races by deploying the model in production\ninference phase. The rendered data undergoes rigorous qualitative as well as\nquantitative tests to cross validate its efficacy and further fine-tuning Yolo\narchitecture for detecting and classifying child ethnicity as an exemplary\ndownstream machine learning task.\n", "link": "http://arxiv.org/abs/2406.11592v1", "date": "2024-06-17", "relevancy": 2.4347, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.636}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5925}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChildDiffusion%3A%20Unlocking%20the%20Potential%20of%20Generative%20AI%20and%0A%20%20Controllable%20Augmentations%20for%20Child%20Facial%20Data%20using%20Stable%20Diffusion%20and%0A%20%20Large%20Language%20Models&body=Title%3A%20ChildDiffusion%3A%20Unlocking%20the%20Potential%20of%20Generative%20AI%20and%0A%20%20Controllable%20Augmentations%20for%20Child%20Facial%20Data%20using%20Stable%20Diffusion%20and%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Muhammad%20Ali%20Farooq%20and%20Wang%20Yao%20and%20Peter%20Corcoran%0AAbstract%3A%20%20%20In%20this%20research%20work%20we%20have%20proposed%20high-level%20ChildDiffusion%20framework%0Acapable%20of%20generating%20photorealistic%20child%20facial%20samples%20and%20further%20embedding%0Aseveral%20intelligent%20augmentations%20on%20child%20facial%20data%20using%20short%20text%0Aprompts%2C%20detailed%20textual%20guidance%20from%20LLMs%2C%20and%20further%20image%20to%20image%0Atransformation%20using%20text%20guidance%20control%20conditioning%20thus%20providing%20an%0Aopportunity%20to%20curate%20fully%20synthetic%20large%20scale%20child%20datasets.%20The%20framework%0Ais%20validated%20by%20rendering%20high-quality%20child%20faces%20representing%20ethnicity%20data%2C%0Amicro%20expressions%2C%20face%20pose%20variations%2C%20eye%20blinking%20effects%2C%20facial%0Aaccessories%2C%20different%20hair%20colours%20and%20styles%2C%20aging%2C%20multiple%20and%20different%0Achild%20gender%20subjects%20in%20a%20single%20frame.%20Addressing%20privacy%20concerns%20regarding%0Achild%20data%20acquisition%20requires%20a%20comprehensive%20approach%20that%20involves%20legal%2C%0Aethical%2C%20and%20technological%20considerations.%20Keeping%20this%20in%20view%20this%20framework%0Acan%20be%20adapted%20to%20synthesise%20child%20facial%20data%20which%20can%20be%20effectively%20used%0Afor%20numerous%20downstream%20machine%20learning%20tasks.%20The%20proposed%20method%20circumvents%0Acommon%20issues%20encountered%20in%20generative%20AI%20tools%2C%20such%20as%20temporal%0Ainconsistency%20and%20limited%20control%20over%20the%20rendered%20outputs.%20As%20an%20exemplary%0Ause%20case%20we%20have%20open-sourced%20child%20ethnicity%20data%20consisting%20of%202.5k%20child%0Afacial%20samples%20of%20five%20different%20classes%20which%20includes%20African%2C%20Asian%2C%20White%2C%0ASouth%20Asian/%20Indian%2C%20and%20Hispanic%20races%20by%20deploying%20the%20model%20in%20production%0Ainference%20phase.%20The%20rendered%20data%20undergoes%20rigorous%20qualitative%20as%20well%20as%0Aquantitative%20tests%20to%20cross%20validate%20its%20efficacy%20and%20further%20fine-tuning%20Yolo%0Aarchitecture%20for%20detecting%20and%20classifying%20child%20ethnicity%20as%20an%20exemplary%0Adownstream%20machine%20learning%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11592v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChildDiffusion%253A%2520Unlocking%2520the%2520Potential%2520of%2520Generative%2520AI%2520and%250A%2520%2520Controllable%2520Augmentations%2520for%2520Child%2520Facial%2520Data%2520using%2520Stable%2520Diffusion%2520and%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DMuhammad%2520Ali%2520Farooq%2520and%2520Wang%2520Yao%2520and%2520Peter%2520Corcoran%26entry.1292438233%3D%2520%2520In%2520this%2520research%2520work%2520we%2520have%2520proposed%2520high-level%2520ChildDiffusion%2520framework%250Acapable%2520of%2520generating%2520photorealistic%2520child%2520facial%2520samples%2520and%2520further%2520embedding%250Aseveral%2520intelligent%2520augmentations%2520on%2520child%2520facial%2520data%2520using%2520short%2520text%250Aprompts%252C%2520detailed%2520textual%2520guidance%2520from%2520LLMs%252C%2520and%2520further%2520image%2520to%2520image%250Atransformation%2520using%2520text%2520guidance%2520control%2520conditioning%2520thus%2520providing%2520an%250Aopportunity%2520to%2520curate%2520fully%2520synthetic%2520large%2520scale%2520child%2520datasets.%2520The%2520framework%250Ais%2520validated%2520by%2520rendering%2520high-quality%2520child%2520faces%2520representing%2520ethnicity%2520data%252C%250Amicro%2520expressions%252C%2520face%2520pose%2520variations%252C%2520eye%2520blinking%2520effects%252C%2520facial%250Aaccessories%252C%2520different%2520hair%2520colours%2520and%2520styles%252C%2520aging%252C%2520multiple%2520and%2520different%250Achild%2520gender%2520subjects%2520in%2520a%2520single%2520frame.%2520Addressing%2520privacy%2520concerns%2520regarding%250Achild%2520data%2520acquisition%2520requires%2520a%2520comprehensive%2520approach%2520that%2520involves%2520legal%252C%250Aethical%252C%2520and%2520technological%2520considerations.%2520Keeping%2520this%2520in%2520view%2520this%2520framework%250Acan%2520be%2520adapted%2520to%2520synthesise%2520child%2520facial%2520data%2520which%2520can%2520be%2520effectively%2520used%250Afor%2520numerous%2520downstream%2520machine%2520learning%2520tasks.%2520The%2520proposed%2520method%2520circumvents%250Acommon%2520issues%2520encountered%2520in%2520generative%2520AI%2520tools%252C%2520such%2520as%2520temporal%250Ainconsistency%2520and%2520limited%2520control%2520over%2520the%2520rendered%2520outputs.%2520As%2520an%2520exemplary%250Ause%2520case%2520we%2520have%2520open-sourced%2520child%2520ethnicity%2520data%2520consisting%2520of%25202.5k%2520child%250Afacial%2520samples%2520of%2520five%2520different%2520classes%2520which%2520includes%2520African%252C%2520Asian%252C%2520White%252C%250ASouth%2520Asian/%2520Indian%252C%2520and%2520Hispanic%2520races%2520by%2520deploying%2520the%2520model%2520in%2520production%250Ainference%2520phase.%2520The%2520rendered%2520data%2520undergoes%2520rigorous%2520qualitative%2520as%2520well%2520as%250Aquantitative%2520tests%2520to%2520cross%2520validate%2520its%2520efficacy%2520and%2520further%2520fine-tuning%2520Yolo%250Aarchitecture%2520for%2520detecting%2520and%2520classifying%2520child%2520ethnicity%2520as%2520an%2520exemplary%250Adownstream%2520machine%2520learning%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11592v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChildDiffusion%3A%20Unlocking%20the%20Potential%20of%20Generative%20AI%20and%0A%20%20Controllable%20Augmentations%20for%20Child%20Facial%20Data%20using%20Stable%20Diffusion%20and%0A%20%20Large%20Language%20Models&entry.906535625=Muhammad%20Ali%20Farooq%20and%20Wang%20Yao%20and%20Peter%20Corcoran&entry.1292438233=%20%20In%20this%20research%20work%20we%20have%20proposed%20high-level%20ChildDiffusion%20framework%0Acapable%20of%20generating%20photorealistic%20child%20facial%20samples%20and%20further%20embedding%0Aseveral%20intelligent%20augmentations%20on%20child%20facial%20data%20using%20short%20text%0Aprompts%2C%20detailed%20textual%20guidance%20from%20LLMs%2C%20and%20further%20image%20to%20image%0Atransformation%20using%20text%20guidance%20control%20conditioning%20thus%20providing%20an%0Aopportunity%20to%20curate%20fully%20synthetic%20large%20scale%20child%20datasets.%20The%20framework%0Ais%20validated%20by%20rendering%20high-quality%20child%20faces%20representing%20ethnicity%20data%2C%0Amicro%20expressions%2C%20face%20pose%20variations%2C%20eye%20blinking%20effects%2C%20facial%0Aaccessories%2C%20different%20hair%20colours%20and%20styles%2C%20aging%2C%20multiple%20and%20different%0Achild%20gender%20subjects%20in%20a%20single%20frame.%20Addressing%20privacy%20concerns%20regarding%0Achild%20data%20acquisition%20requires%20a%20comprehensive%20approach%20that%20involves%20legal%2C%0Aethical%2C%20and%20technological%20considerations.%20Keeping%20this%20in%20view%20this%20framework%0Acan%20be%20adapted%20to%20synthesise%20child%20facial%20data%20which%20can%20be%20effectively%20used%0Afor%20numerous%20downstream%20machine%20learning%20tasks.%20The%20proposed%20method%20circumvents%0Acommon%20issues%20encountered%20in%20generative%20AI%20tools%2C%20such%20as%20temporal%0Ainconsistency%20and%20limited%20control%20over%20the%20rendered%20outputs.%20As%20an%20exemplary%0Ause%20case%20we%20have%20open-sourced%20child%20ethnicity%20data%20consisting%20of%202.5k%20child%0Afacial%20samples%20of%20five%20different%20classes%20which%20includes%20African%2C%20Asian%2C%20White%2C%0ASouth%20Asian/%20Indian%2C%20and%20Hispanic%20races%20by%20deploying%20the%20model%20in%20production%0Ainference%20phase.%20The%20rendered%20data%20undergoes%20rigorous%20qualitative%20as%20well%20as%0Aquantitative%20tests%20to%20cross%20validate%20its%20efficacy%20and%20further%20fine-tuning%20Yolo%0Aarchitecture%20for%20detecting%20and%20classifying%20child%20ethnicity%20as%20an%20exemplary%0Adownstream%20machine%20learning%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11592v1&entry.124074799=Read"},
{"title": "Beyond Bare Queries: Open-Vocabulary Object Retrieval with 3D Scene\n  Graph", "author": "Sergey Linok and Tatiana Zemskova and Svetlana Ladanova and Roman Titkov and Dmitry Yudin", "abstract": "  Locating objects referred to in natural language poses a significant\nchallenge for autonomous agents. Existing CLIP-based open-vocabulary methods\nsuccessfully perform 3D object retrieval with simple (bare) queries but cannot\ncope with ambiguous descriptions that demand an understanding of object\nrelations. To tackle this problem, we propose a modular approach called BBQ\n(Beyond Bare Queries), which constructs 3D scene spatial graph representation\nwith metric edges and utilizes a large language model as a human-to-agent\ninterface through our deductive scene reasoning algorithm. BBQ employs robust\nDINO-powered associations to form 3D objects, an advanced raycasting algorithm\nto project them to 2D, and a vision-language model to describe them as graph\nnodes. On Replica and ScanNet datasets, we show that the designed method\naccurately constructs 3D object-centric maps. We have demonstrated that their\nquality takes a leading place for open-vocabulary 3D semantic segmentation\nagainst other zero-shot methods. Also, we show that leveraging spatial\nrelations is especially effective for scenes containing multiple entities of\nthe same semantic class. On Sr3D and Nr3D benchmarks, our deductive approach\ndemonstrates a significant improvement, enabling retrieving objects by complex\nqueries compared to other state-of-the-art methods. Considering our design\nsolutions, we achieved a processing speed approximately x3 times faster than\nthe closest analog. This promising performance enables our approach for usage\nin applied intelligent robotics projects. We make the code publicly available\nat linukc.github.io/bbq/.\n", "link": "http://arxiv.org/abs/2406.07113v2", "date": "2024-06-17", "relevancy": 2.4245, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6439}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5986}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Bare%20Queries%3A%20Open-Vocabulary%20Object%20Retrieval%20with%203D%20Scene%0A%20%20Graph&body=Title%3A%20Beyond%20Bare%20Queries%3A%20Open-Vocabulary%20Object%20Retrieval%20with%203D%20Scene%0A%20%20Graph%0AAuthor%3A%20Sergey%20Linok%20and%20Tatiana%20Zemskova%20and%20Svetlana%20Ladanova%20and%20Roman%20Titkov%20and%20Dmitry%20Yudin%0AAbstract%3A%20%20%20Locating%20objects%20referred%20to%20in%20natural%20language%20poses%20a%20significant%0Achallenge%20for%20autonomous%20agents.%20Existing%20CLIP-based%20open-vocabulary%20methods%0Asuccessfully%20perform%203D%20object%20retrieval%20with%20simple%20%28bare%29%20queries%20but%20cannot%0Acope%20with%20ambiguous%20descriptions%20that%20demand%20an%20understanding%20of%20object%0Arelations.%20To%20tackle%20this%20problem%2C%20we%20propose%20a%20modular%20approach%20called%20BBQ%0A%28Beyond%20Bare%20Queries%29%2C%20which%20constructs%203D%20scene%20spatial%20graph%20representation%0Awith%20metric%20edges%20and%20utilizes%20a%20large%20language%20model%20as%20a%20human-to-agent%0Ainterface%20through%20our%20deductive%20scene%20reasoning%20algorithm.%20BBQ%20employs%20robust%0ADINO-powered%20associations%20to%20form%203D%20objects%2C%20an%20advanced%20raycasting%20algorithm%0Ato%20project%20them%20to%202D%2C%20and%20a%20vision-language%20model%20to%20describe%20them%20as%20graph%0Anodes.%20On%20Replica%20and%20ScanNet%20datasets%2C%20we%20show%20that%20the%20designed%20method%0Aaccurately%20constructs%203D%20object-centric%20maps.%20We%20have%20demonstrated%20that%20their%0Aquality%20takes%20a%20leading%20place%20for%20open-vocabulary%203D%20semantic%20segmentation%0Aagainst%20other%20zero-shot%20methods.%20Also%2C%20we%20show%20that%20leveraging%20spatial%0Arelations%20is%20especially%20effective%20for%20scenes%20containing%20multiple%20entities%20of%0Athe%20same%20semantic%20class.%20On%20Sr3D%20and%20Nr3D%20benchmarks%2C%20our%20deductive%20approach%0Ademonstrates%20a%20significant%20improvement%2C%20enabling%20retrieving%20objects%20by%20complex%0Aqueries%20compared%20to%20other%20state-of-the-art%20methods.%20Considering%20our%20design%0Asolutions%2C%20we%20achieved%20a%20processing%20speed%20approximately%20x3%20times%20faster%20than%0Athe%20closest%20analog.%20This%20promising%20performance%20enables%20our%20approach%20for%20usage%0Ain%20applied%20intelligent%20robotics%20projects.%20We%20make%20the%20code%20publicly%20available%0Aat%20linukc.github.io/bbq/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07113v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Bare%2520Queries%253A%2520Open-Vocabulary%2520Object%2520Retrieval%2520with%25203D%2520Scene%250A%2520%2520Graph%26entry.906535625%3DSergey%2520Linok%2520and%2520Tatiana%2520Zemskova%2520and%2520Svetlana%2520Ladanova%2520and%2520Roman%2520Titkov%2520and%2520Dmitry%2520Yudin%26entry.1292438233%3D%2520%2520Locating%2520objects%2520referred%2520to%2520in%2520natural%2520language%2520poses%2520a%2520significant%250Achallenge%2520for%2520autonomous%2520agents.%2520Existing%2520CLIP-based%2520open-vocabulary%2520methods%250Asuccessfully%2520perform%25203D%2520object%2520retrieval%2520with%2520simple%2520%2528bare%2529%2520queries%2520but%2520cannot%250Acope%2520with%2520ambiguous%2520descriptions%2520that%2520demand%2520an%2520understanding%2520of%2520object%250Arelations.%2520To%2520tackle%2520this%2520problem%252C%2520we%2520propose%2520a%2520modular%2520approach%2520called%2520BBQ%250A%2528Beyond%2520Bare%2520Queries%2529%252C%2520which%2520constructs%25203D%2520scene%2520spatial%2520graph%2520representation%250Awith%2520metric%2520edges%2520and%2520utilizes%2520a%2520large%2520language%2520model%2520as%2520a%2520human-to-agent%250Ainterface%2520through%2520our%2520deductive%2520scene%2520reasoning%2520algorithm.%2520BBQ%2520employs%2520robust%250ADINO-powered%2520associations%2520to%2520form%25203D%2520objects%252C%2520an%2520advanced%2520raycasting%2520algorithm%250Ato%2520project%2520them%2520to%25202D%252C%2520and%2520a%2520vision-language%2520model%2520to%2520describe%2520them%2520as%2520graph%250Anodes.%2520On%2520Replica%2520and%2520ScanNet%2520datasets%252C%2520we%2520show%2520that%2520the%2520designed%2520method%250Aaccurately%2520constructs%25203D%2520object-centric%2520maps.%2520We%2520have%2520demonstrated%2520that%2520their%250Aquality%2520takes%2520a%2520leading%2520place%2520for%2520open-vocabulary%25203D%2520semantic%2520segmentation%250Aagainst%2520other%2520zero-shot%2520methods.%2520Also%252C%2520we%2520show%2520that%2520leveraging%2520spatial%250Arelations%2520is%2520especially%2520effective%2520for%2520scenes%2520containing%2520multiple%2520entities%2520of%250Athe%2520same%2520semantic%2520class.%2520On%2520Sr3D%2520and%2520Nr3D%2520benchmarks%252C%2520our%2520deductive%2520approach%250Ademonstrates%2520a%2520significant%2520improvement%252C%2520enabling%2520retrieving%2520objects%2520by%2520complex%250Aqueries%2520compared%2520to%2520other%2520state-of-the-art%2520methods.%2520Considering%2520our%2520design%250Asolutions%252C%2520we%2520achieved%2520a%2520processing%2520speed%2520approximately%2520x3%2520times%2520faster%2520than%250Athe%2520closest%2520analog.%2520This%2520promising%2520performance%2520enables%2520our%2520approach%2520for%2520usage%250Ain%2520applied%2520intelligent%2520robotics%2520projects.%2520We%2520make%2520the%2520code%2520publicly%2520available%250Aat%2520linukc.github.io/bbq/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07113v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Bare%20Queries%3A%20Open-Vocabulary%20Object%20Retrieval%20with%203D%20Scene%0A%20%20Graph&entry.906535625=Sergey%20Linok%20and%20Tatiana%20Zemskova%20and%20Svetlana%20Ladanova%20and%20Roman%20Titkov%20and%20Dmitry%20Yudin&entry.1292438233=%20%20Locating%20objects%20referred%20to%20in%20natural%20language%20poses%20a%20significant%0Achallenge%20for%20autonomous%20agents.%20Existing%20CLIP-based%20open-vocabulary%20methods%0Asuccessfully%20perform%203D%20object%20retrieval%20with%20simple%20%28bare%29%20queries%20but%20cannot%0Acope%20with%20ambiguous%20descriptions%20that%20demand%20an%20understanding%20of%20object%0Arelations.%20To%20tackle%20this%20problem%2C%20we%20propose%20a%20modular%20approach%20called%20BBQ%0A%28Beyond%20Bare%20Queries%29%2C%20which%20constructs%203D%20scene%20spatial%20graph%20representation%0Awith%20metric%20edges%20and%20utilizes%20a%20large%20language%20model%20as%20a%20human-to-agent%0Ainterface%20through%20our%20deductive%20scene%20reasoning%20algorithm.%20BBQ%20employs%20robust%0ADINO-powered%20associations%20to%20form%203D%20objects%2C%20an%20advanced%20raycasting%20algorithm%0Ato%20project%20them%20to%202D%2C%20and%20a%20vision-language%20model%20to%20describe%20them%20as%20graph%0Anodes.%20On%20Replica%20and%20ScanNet%20datasets%2C%20we%20show%20that%20the%20designed%20method%0Aaccurately%20constructs%203D%20object-centric%20maps.%20We%20have%20demonstrated%20that%20their%0Aquality%20takes%20a%20leading%20place%20for%20open-vocabulary%203D%20semantic%20segmentation%0Aagainst%20other%20zero-shot%20methods.%20Also%2C%20we%20show%20that%20leveraging%20spatial%0Arelations%20is%20especially%20effective%20for%20scenes%20containing%20multiple%20entities%20of%0Athe%20same%20semantic%20class.%20On%20Sr3D%20and%20Nr3D%20benchmarks%2C%20our%20deductive%20approach%0Ademonstrates%20a%20significant%20improvement%2C%20enabling%20retrieving%20objects%20by%20complex%0Aqueries%20compared%20to%20other%20state-of-the-art%20methods.%20Considering%20our%20design%0Asolutions%2C%20we%20achieved%20a%20processing%20speed%20approximately%20x3%20times%20faster%20than%0Athe%20closest%20analog.%20This%20promising%20performance%20enables%20our%20approach%20for%20usage%0Ain%20applied%20intelligent%20robotics%20projects.%20We%20make%20the%20code%20publicly%20available%0Aat%20linukc.github.io/bbq/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07113v2&entry.124074799=Read"},
{"title": "An Interactive Agent Foundation Model", "author": "Zane Durante and Bidipta Sarkar and Ran Gong and Rohan Taori and Yusuke Noda and Paul Tang and Ehsan Adeli and Shrinidhi Kowshika Lakshmikanth and Kevin Schulman and Arnold Milstein and Demetri Terzopoulos and Ade Famoti and Noboru Kuno and Ashley Llorens and Hoi Vo and Katsu Ikeuchi and Li Fei-Fei and Jianfeng Gao and Naoki Wake and Qiuyuan Huang", "abstract": "  The development of artificial intelligence systems is transitioning from\ncreating static, task-specific models to dynamic, agent-based systems capable\nof performing well in a wide range of applications. We propose an Interactive\nAgent Foundation Model that uses a novel multi-task agent training paradigm for\ntraining AI agents across a wide range of domains, datasets, and tasks. Our\ntraining paradigm unifies diverse pre-training strategies, including visual\nmasked auto-encoders, language modeling, and next-action prediction, enabling a\nversatile and adaptable AI framework. We demonstrate the performance of our\nframework across three separate domains -- Robotics, Gaming AI, and Healthcare.\nOur model demonstrates its ability to generate meaningful and contextually\nrelevant outputs in each area. The strength of our approach lies in its\ngenerality, leveraging a variety of data sources such as robotics sequences,\ngameplay data, large-scale video datasets, and textual information for\neffective multimodal and multi-task learning. Our approach provides a promising\navenue for developing generalist, action-taking, multimodal systems.\n", "link": "http://arxiv.org/abs/2402.05929v2", "date": "2024-06-17", "relevancy": 2.4243, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.65}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5768}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Interactive%20Agent%20Foundation%20Model&body=Title%3A%20An%20Interactive%20Agent%20Foundation%20Model%0AAuthor%3A%20Zane%20Durante%20and%20Bidipta%20Sarkar%20and%20Ran%20Gong%20and%20Rohan%20Taori%20and%20Yusuke%20Noda%20and%20Paul%20Tang%20and%20Ehsan%20Adeli%20and%20Shrinidhi%20Kowshika%20Lakshmikanth%20and%20Kevin%20Schulman%20and%20Arnold%20Milstein%20and%20Demetri%20Terzopoulos%20and%20Ade%20Famoti%20and%20Noboru%20Kuno%20and%20Ashley%20Llorens%20and%20Hoi%20Vo%20and%20Katsu%20Ikeuchi%20and%20Li%20Fei-Fei%20and%20Jianfeng%20Gao%20and%20Naoki%20Wake%20and%20Qiuyuan%20Huang%0AAbstract%3A%20%20%20The%20development%20of%20artificial%20intelligence%20systems%20is%20transitioning%20from%0Acreating%20static%2C%20task-specific%20models%20to%20dynamic%2C%20agent-based%20systems%20capable%0Aof%20performing%20well%20in%20a%20wide%20range%20of%20applications.%20We%20propose%20an%20Interactive%0AAgent%20Foundation%20Model%20that%20uses%20a%20novel%20multi-task%20agent%20training%20paradigm%20for%0Atraining%20AI%20agents%20across%20a%20wide%20range%20of%20domains%2C%20datasets%2C%20and%20tasks.%20Our%0Atraining%20paradigm%20unifies%20diverse%20pre-training%20strategies%2C%20including%20visual%0Amasked%20auto-encoders%2C%20language%20modeling%2C%20and%20next-action%20prediction%2C%20enabling%20a%0Aversatile%20and%20adaptable%20AI%20framework.%20We%20demonstrate%20the%20performance%20of%20our%0Aframework%20across%20three%20separate%20domains%20--%20Robotics%2C%20Gaming%20AI%2C%20and%20Healthcare.%0AOur%20model%20demonstrates%20its%20ability%20to%20generate%20meaningful%20and%20contextually%0Arelevant%20outputs%20in%20each%20area.%20The%20strength%20of%20our%20approach%20lies%20in%20its%0Agenerality%2C%20leveraging%20a%20variety%20of%20data%20sources%20such%20as%20robotics%20sequences%2C%0Agameplay%20data%2C%20large-scale%20video%20datasets%2C%20and%20textual%20information%20for%0Aeffective%20multimodal%20and%20multi-task%20learning.%20Our%20approach%20provides%20a%20promising%0Aavenue%20for%20developing%20generalist%2C%20action-taking%2C%20multimodal%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.05929v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Interactive%2520Agent%2520Foundation%2520Model%26entry.906535625%3DZane%2520Durante%2520and%2520Bidipta%2520Sarkar%2520and%2520Ran%2520Gong%2520and%2520Rohan%2520Taori%2520and%2520Yusuke%2520Noda%2520and%2520Paul%2520Tang%2520and%2520Ehsan%2520Adeli%2520and%2520Shrinidhi%2520Kowshika%2520Lakshmikanth%2520and%2520Kevin%2520Schulman%2520and%2520Arnold%2520Milstein%2520and%2520Demetri%2520Terzopoulos%2520and%2520Ade%2520Famoti%2520and%2520Noboru%2520Kuno%2520and%2520Ashley%2520Llorens%2520and%2520Hoi%2520Vo%2520and%2520Katsu%2520Ikeuchi%2520and%2520Li%2520Fei-Fei%2520and%2520Jianfeng%2520Gao%2520and%2520Naoki%2520Wake%2520and%2520Qiuyuan%2520Huang%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520artificial%2520intelligence%2520systems%2520is%2520transitioning%2520from%250Acreating%2520static%252C%2520task-specific%2520models%2520to%2520dynamic%252C%2520agent-based%2520systems%2520capable%250Aof%2520performing%2520well%2520in%2520a%2520wide%2520range%2520of%2520applications.%2520We%2520propose%2520an%2520Interactive%250AAgent%2520Foundation%2520Model%2520that%2520uses%2520a%2520novel%2520multi-task%2520agent%2520training%2520paradigm%2520for%250Atraining%2520AI%2520agents%2520across%2520a%2520wide%2520range%2520of%2520domains%252C%2520datasets%252C%2520and%2520tasks.%2520Our%250Atraining%2520paradigm%2520unifies%2520diverse%2520pre-training%2520strategies%252C%2520including%2520visual%250Amasked%2520auto-encoders%252C%2520language%2520modeling%252C%2520and%2520next-action%2520prediction%252C%2520enabling%2520a%250Aversatile%2520and%2520adaptable%2520AI%2520framework.%2520We%2520demonstrate%2520the%2520performance%2520of%2520our%250Aframework%2520across%2520three%2520separate%2520domains%2520--%2520Robotics%252C%2520Gaming%2520AI%252C%2520and%2520Healthcare.%250AOur%2520model%2520demonstrates%2520its%2520ability%2520to%2520generate%2520meaningful%2520and%2520contextually%250Arelevant%2520outputs%2520in%2520each%2520area.%2520The%2520strength%2520of%2520our%2520approach%2520lies%2520in%2520its%250Agenerality%252C%2520leveraging%2520a%2520variety%2520of%2520data%2520sources%2520such%2520as%2520robotics%2520sequences%252C%250Agameplay%2520data%252C%2520large-scale%2520video%2520datasets%252C%2520and%2520textual%2520information%2520for%250Aeffective%2520multimodal%2520and%2520multi-task%2520learning.%2520Our%2520approach%2520provides%2520a%2520promising%250Aavenue%2520for%2520developing%2520generalist%252C%2520action-taking%252C%2520multimodal%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.05929v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Interactive%20Agent%20Foundation%20Model&entry.906535625=Zane%20Durante%20and%20Bidipta%20Sarkar%20and%20Ran%20Gong%20and%20Rohan%20Taori%20and%20Yusuke%20Noda%20and%20Paul%20Tang%20and%20Ehsan%20Adeli%20and%20Shrinidhi%20Kowshika%20Lakshmikanth%20and%20Kevin%20Schulman%20and%20Arnold%20Milstein%20and%20Demetri%20Terzopoulos%20and%20Ade%20Famoti%20and%20Noboru%20Kuno%20and%20Ashley%20Llorens%20and%20Hoi%20Vo%20and%20Katsu%20Ikeuchi%20and%20Li%20Fei-Fei%20and%20Jianfeng%20Gao%20and%20Naoki%20Wake%20and%20Qiuyuan%20Huang&entry.1292438233=%20%20The%20development%20of%20artificial%20intelligence%20systems%20is%20transitioning%20from%0Acreating%20static%2C%20task-specific%20models%20to%20dynamic%2C%20agent-based%20systems%20capable%0Aof%20performing%20well%20in%20a%20wide%20range%20of%20applications.%20We%20propose%20an%20Interactive%0AAgent%20Foundation%20Model%20that%20uses%20a%20novel%20multi-task%20agent%20training%20paradigm%20for%0Atraining%20AI%20agents%20across%20a%20wide%20range%20of%20domains%2C%20datasets%2C%20and%20tasks.%20Our%0Atraining%20paradigm%20unifies%20diverse%20pre-training%20strategies%2C%20including%20visual%0Amasked%20auto-encoders%2C%20language%20modeling%2C%20and%20next-action%20prediction%2C%20enabling%20a%0Aversatile%20and%20adaptable%20AI%20framework.%20We%20demonstrate%20the%20performance%20of%20our%0Aframework%20across%20three%20separate%20domains%20--%20Robotics%2C%20Gaming%20AI%2C%20and%20Healthcare.%0AOur%20model%20demonstrates%20its%20ability%20to%20generate%20meaningful%20and%20contextually%0Arelevant%20outputs%20in%20each%20area.%20The%20strength%20of%20our%20approach%20lies%20in%20its%0Agenerality%2C%20leveraging%20a%20variety%20of%20data%20sources%20such%20as%20robotics%20sequences%2C%0Agameplay%20data%2C%20large-scale%20video%20datasets%2C%20and%20textual%20information%20for%0Aeffective%20multimodal%20and%20multi-task%20learning.%20Our%20approach%20provides%20a%20promising%0Aavenue%20for%20developing%20generalist%2C%20action-taking%2C%20multimodal%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.05929v2&entry.124074799=Read"},
{"title": "An Imitative Reinforcement Learning Framework for Autonomous Dogfight", "author": "Siyuan Li and Rongchang Zuo and Peng Liu and Yingnan Zhao", "abstract": "  Unmanned Combat Aerial Vehicle (UCAV) dogfight, which refers to a fight\nbetween two or more UCAVs usually at close quarters, plays a decisive role on\nthe aerial battlefields. With the evolution of artificial intelligence,\ndogfight progressively transits towards intelligent and autonomous modes.\nHowever, the development of autonomous dogfight policy learning is hindered by\nchallenges such as weak exploration capabilities, low learning efficiency, and\nunrealistic simulated environments. To overcome these challenges, this paper\nproposes a novel imitative reinforcement learning framework, which efficiently\nleverages expert data while enabling autonomous exploration. The proposed\nframework not only enhances learning efficiency through expert imitation, but\nalso ensures adaptability to dynamic environments via autonomous exploration\nwith reinforcement learning. Therefore, the proposed framework can learn a\nsuccessful dogfight policy of 'pursuit-lock-launch' for UCAVs. To support\ndata-driven learning, we establish a dogfight environment based on the\nHarfang3D sandbox, where we conduct extensive experiments. The results indicate\nthat the proposed framework excels in multistage dogfight, significantly\noutperforms state-of-the-art reinforcement learning and imitation learning\nmethods. Thanks to the ability of imitating experts and autonomous exploration,\nour framework can quickly learn the critical knowledge in complex aerial combat\ntasks, achieving up to a 100% success rate and demonstrating excellent\nrobustness.\n", "link": "http://arxiv.org/abs/2406.11562v1", "date": "2024-06-17", "relevancy": 2.421, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4923}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4823}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Imitative%20Reinforcement%20Learning%20Framework%20for%20Autonomous%20Dogfight&body=Title%3A%20An%20Imitative%20Reinforcement%20Learning%20Framework%20for%20Autonomous%20Dogfight%0AAuthor%3A%20Siyuan%20Li%20and%20Rongchang%20Zuo%20and%20Peng%20Liu%20and%20Yingnan%20Zhao%0AAbstract%3A%20%20%20Unmanned%20Combat%20Aerial%20Vehicle%20%28UCAV%29%20dogfight%2C%20which%20refers%20to%20a%20fight%0Abetween%20two%20or%20more%20UCAVs%20usually%20at%20close%20quarters%2C%20plays%20a%20decisive%20role%20on%0Athe%20aerial%20battlefields.%20With%20the%20evolution%20of%20artificial%20intelligence%2C%0Adogfight%20progressively%20transits%20towards%20intelligent%20and%20autonomous%20modes.%0AHowever%2C%20the%20development%20of%20autonomous%20dogfight%20policy%20learning%20is%20hindered%20by%0Achallenges%20such%20as%20weak%20exploration%20capabilities%2C%20low%20learning%20efficiency%2C%20and%0Aunrealistic%20simulated%20environments.%20To%20overcome%20these%20challenges%2C%20this%20paper%0Aproposes%20a%20novel%20imitative%20reinforcement%20learning%20framework%2C%20which%20efficiently%0Aleverages%20expert%20data%20while%20enabling%20autonomous%20exploration.%20The%20proposed%0Aframework%20not%20only%20enhances%20learning%20efficiency%20through%20expert%20imitation%2C%20but%0Aalso%20ensures%20adaptability%20to%20dynamic%20environments%20via%20autonomous%20exploration%0Awith%20reinforcement%20learning.%20Therefore%2C%20the%20proposed%20framework%20can%20learn%20a%0Asuccessful%20dogfight%20policy%20of%20%27pursuit-lock-launch%27%20for%20UCAVs.%20To%20support%0Adata-driven%20learning%2C%20we%20establish%20a%20dogfight%20environment%20based%20on%20the%0AHarfang3D%20sandbox%2C%20where%20we%20conduct%20extensive%20experiments.%20The%20results%20indicate%0Athat%20the%20proposed%20framework%20excels%20in%20multistage%20dogfight%2C%20significantly%0Aoutperforms%20state-of-the-art%20reinforcement%20learning%20and%20imitation%20learning%0Amethods.%20Thanks%20to%20the%20ability%20of%20imitating%20experts%20and%20autonomous%20exploration%2C%0Aour%20framework%20can%20quickly%20learn%20the%20critical%20knowledge%20in%20complex%20aerial%20combat%0Atasks%2C%20achieving%20up%20to%20a%20100%25%20success%20rate%20and%20demonstrating%20excellent%0Arobustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11562v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Imitative%2520Reinforcement%2520Learning%2520Framework%2520for%2520Autonomous%2520Dogfight%26entry.906535625%3DSiyuan%2520Li%2520and%2520Rongchang%2520Zuo%2520and%2520Peng%2520Liu%2520and%2520Yingnan%2520Zhao%26entry.1292438233%3D%2520%2520Unmanned%2520Combat%2520Aerial%2520Vehicle%2520%2528UCAV%2529%2520dogfight%252C%2520which%2520refers%2520to%2520a%2520fight%250Abetween%2520two%2520or%2520more%2520UCAVs%2520usually%2520at%2520close%2520quarters%252C%2520plays%2520a%2520decisive%2520role%2520on%250Athe%2520aerial%2520battlefields.%2520With%2520the%2520evolution%2520of%2520artificial%2520intelligence%252C%250Adogfight%2520progressively%2520transits%2520towards%2520intelligent%2520and%2520autonomous%2520modes.%250AHowever%252C%2520the%2520development%2520of%2520autonomous%2520dogfight%2520policy%2520learning%2520is%2520hindered%2520by%250Achallenges%2520such%2520as%2520weak%2520exploration%2520capabilities%252C%2520low%2520learning%2520efficiency%252C%2520and%250Aunrealistic%2520simulated%2520environments.%2520To%2520overcome%2520these%2520challenges%252C%2520this%2520paper%250Aproposes%2520a%2520novel%2520imitative%2520reinforcement%2520learning%2520framework%252C%2520which%2520efficiently%250Aleverages%2520expert%2520data%2520while%2520enabling%2520autonomous%2520exploration.%2520The%2520proposed%250Aframework%2520not%2520only%2520enhances%2520learning%2520efficiency%2520through%2520expert%2520imitation%252C%2520but%250Aalso%2520ensures%2520adaptability%2520to%2520dynamic%2520environments%2520via%2520autonomous%2520exploration%250Awith%2520reinforcement%2520learning.%2520Therefore%252C%2520the%2520proposed%2520framework%2520can%2520learn%2520a%250Asuccessful%2520dogfight%2520policy%2520of%2520%2527pursuit-lock-launch%2527%2520for%2520UCAVs.%2520To%2520support%250Adata-driven%2520learning%252C%2520we%2520establish%2520a%2520dogfight%2520environment%2520based%2520on%2520the%250AHarfang3D%2520sandbox%252C%2520where%2520we%2520conduct%2520extensive%2520experiments.%2520The%2520results%2520indicate%250Athat%2520the%2520proposed%2520framework%2520excels%2520in%2520multistage%2520dogfight%252C%2520significantly%250Aoutperforms%2520state-of-the-art%2520reinforcement%2520learning%2520and%2520imitation%2520learning%250Amethods.%2520Thanks%2520to%2520the%2520ability%2520of%2520imitating%2520experts%2520and%2520autonomous%2520exploration%252C%250Aour%2520framework%2520can%2520quickly%2520learn%2520the%2520critical%2520knowledge%2520in%2520complex%2520aerial%2520combat%250Atasks%252C%2520achieving%2520up%2520to%2520a%2520100%2525%2520success%2520rate%2520and%2520demonstrating%2520excellent%250Arobustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11562v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Imitative%20Reinforcement%20Learning%20Framework%20for%20Autonomous%20Dogfight&entry.906535625=Siyuan%20Li%20and%20Rongchang%20Zuo%20and%20Peng%20Liu%20and%20Yingnan%20Zhao&entry.1292438233=%20%20Unmanned%20Combat%20Aerial%20Vehicle%20%28UCAV%29%20dogfight%2C%20which%20refers%20to%20a%20fight%0Abetween%20two%20or%20more%20UCAVs%20usually%20at%20close%20quarters%2C%20plays%20a%20decisive%20role%20on%0Athe%20aerial%20battlefields.%20With%20the%20evolution%20of%20artificial%20intelligence%2C%0Adogfight%20progressively%20transits%20towards%20intelligent%20and%20autonomous%20modes.%0AHowever%2C%20the%20development%20of%20autonomous%20dogfight%20policy%20learning%20is%20hindered%20by%0Achallenges%20such%20as%20weak%20exploration%20capabilities%2C%20low%20learning%20efficiency%2C%20and%0Aunrealistic%20simulated%20environments.%20To%20overcome%20these%20challenges%2C%20this%20paper%0Aproposes%20a%20novel%20imitative%20reinforcement%20learning%20framework%2C%20which%20efficiently%0Aleverages%20expert%20data%20while%20enabling%20autonomous%20exploration.%20The%20proposed%0Aframework%20not%20only%20enhances%20learning%20efficiency%20through%20expert%20imitation%2C%20but%0Aalso%20ensures%20adaptability%20to%20dynamic%20environments%20via%20autonomous%20exploration%0Awith%20reinforcement%20learning.%20Therefore%2C%20the%20proposed%20framework%20can%20learn%20a%0Asuccessful%20dogfight%20policy%20of%20%27pursuit-lock-launch%27%20for%20UCAVs.%20To%20support%0Adata-driven%20learning%2C%20we%20establish%20a%20dogfight%20environment%20based%20on%20the%0AHarfang3D%20sandbox%2C%20where%20we%20conduct%20extensive%20experiments.%20The%20results%20indicate%0Athat%20the%20proposed%20framework%20excels%20in%20multistage%20dogfight%2C%20significantly%0Aoutperforms%20state-of-the-art%20reinforcement%20learning%20and%20imitation%20learning%0Amethods.%20Thanks%20to%20the%20ability%20of%20imitating%20experts%20and%20autonomous%20exploration%2C%0Aour%20framework%20can%20quickly%20learn%20the%20critical%20knowledge%20in%20complex%20aerial%20combat%0Atasks%2C%20achieving%20up%20to%20a%20100%25%20success%20rate%20and%20demonstrating%20excellent%0Arobustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11562v1&entry.124074799=Read"},
{"title": "Uncovering Challenges of Solving the Continuous Gromov-Wasserstein\n  Problem", "author": "Xavier Aramayo Carrasco and Maksim Nekrashevich and Petr Mokrov and Evgeny Burnaev and Alexander Korotin", "abstract": "  Recently, the Gromov-Wasserstein Optimal Transport (GWOT) problem has\nattracted the special attention of the ML community. In this problem, given two\ndistributions supported on two (possibly different) spaces, one has to find the\nmost isometric map between them. In the discrete variant of GWOT, the task is\nto learn an assignment between given discrete sets of points. In the more\nadvanced continuous formulation, one aims at recovering a parametric mapping\nbetween unknown continuous distributions based on i.i.d. samples derived from\nthem. The clear geometrical intuition behind the GWOT makes it a natural choice\nfor several practical use cases, giving rise to a number of proposed solvers.\nSome of them claim to solve the continuous version of the problem. At the same\ntime, GWOT is notoriously hard, both theoretically and numerically. Moreover,\nall existing continuous GWOT solvers still heavily rely on discrete techniques.\nNatural questions arise: to what extent existing methods unravel GWOT problem,\nwhat difficulties they encounter, and under which conditions they are\nsuccessful. Our benchmark paper is an attempt to answer these questions. We\nspecifically focus on the continuous GWOT as the most interesting and debatable\nsetup. We crash-test existing continuous GWOT approaches on different\nscenarios, carefully record and analyze the obtained results, and identify\nissues. Our findings experimentally testify that the scientific community is\nstill missing a reliable continuous GWOT solver, which necessitates further\nresearch efforts. As the first step in this direction, we propose a new\ncontinuous GWOT method which does not rely on discrete techniques and partially\nsolves some of the problems of the competitors. Our code is available at\nhttps://github.com/Ark-130994/GW-Solvers.\n", "link": "http://arxiv.org/abs/2303.05978v2", "date": "2024-06-17", "relevancy": 2.4025, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4956}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4758}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncovering%20Challenges%20of%20Solving%20the%20Continuous%20Gromov-Wasserstein%0A%20%20Problem&body=Title%3A%20Uncovering%20Challenges%20of%20Solving%20the%20Continuous%20Gromov-Wasserstein%0A%20%20Problem%0AAuthor%3A%20Xavier%20Aramayo%20Carrasco%20and%20Maksim%20Nekrashevich%20and%20Petr%20Mokrov%20and%20Evgeny%20Burnaev%20and%20Alexander%20Korotin%0AAbstract%3A%20%20%20Recently%2C%20the%20Gromov-Wasserstein%20Optimal%20Transport%20%28GWOT%29%20problem%20has%0Aattracted%20the%20special%20attention%20of%20the%20ML%20community.%20In%20this%20problem%2C%20given%20two%0Adistributions%20supported%20on%20two%20%28possibly%20different%29%20spaces%2C%20one%20has%20to%20find%20the%0Amost%20isometric%20map%20between%20them.%20In%20the%20discrete%20variant%20of%20GWOT%2C%20the%20task%20is%0Ato%20learn%20an%20assignment%20between%20given%20discrete%20sets%20of%20points.%20In%20the%20more%0Aadvanced%20continuous%20formulation%2C%20one%20aims%20at%20recovering%20a%20parametric%20mapping%0Abetween%20unknown%20continuous%20distributions%20based%20on%20i.i.d.%20samples%20derived%20from%0Athem.%20The%20clear%20geometrical%20intuition%20behind%20the%20GWOT%20makes%20it%20a%20natural%20choice%0Afor%20several%20practical%20use%20cases%2C%20giving%20rise%20to%20a%20number%20of%20proposed%20solvers.%0ASome%20of%20them%20claim%20to%20solve%20the%20continuous%20version%20of%20the%20problem.%20At%20the%20same%0Atime%2C%20GWOT%20is%20notoriously%20hard%2C%20both%20theoretically%20and%20numerically.%20Moreover%2C%0Aall%20existing%20continuous%20GWOT%20solvers%20still%20heavily%20rely%20on%20discrete%20techniques.%0ANatural%20questions%20arise%3A%20to%20what%20extent%20existing%20methods%20unravel%20GWOT%20problem%2C%0Awhat%20difficulties%20they%20encounter%2C%20and%20under%20which%20conditions%20they%20are%0Asuccessful.%20Our%20benchmark%20paper%20is%20an%20attempt%20to%20answer%20these%20questions.%20We%0Aspecifically%20focus%20on%20the%20continuous%20GWOT%20as%20the%20most%20interesting%20and%20debatable%0Asetup.%20We%20crash-test%20existing%20continuous%20GWOT%20approaches%20on%20different%0Ascenarios%2C%20carefully%20record%20and%20analyze%20the%20obtained%20results%2C%20and%20identify%0Aissues.%20Our%20findings%20experimentally%20testify%20that%20the%20scientific%20community%20is%0Astill%20missing%20a%20reliable%20continuous%20GWOT%20solver%2C%20which%20necessitates%20further%0Aresearch%20efforts.%20As%20the%20first%20step%20in%20this%20direction%2C%20we%20propose%20a%20new%0Acontinuous%20GWOT%20method%20which%20does%20not%20rely%20on%20discrete%20techniques%20and%20partially%0Asolves%20some%20of%20the%20problems%20of%20the%20competitors.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Ark-130994/GW-Solvers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.05978v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncovering%2520Challenges%2520of%2520Solving%2520the%2520Continuous%2520Gromov-Wasserstein%250A%2520%2520Problem%26entry.906535625%3DXavier%2520Aramayo%2520Carrasco%2520and%2520Maksim%2520Nekrashevich%2520and%2520Petr%2520Mokrov%2520and%2520Evgeny%2520Burnaev%2520and%2520Alexander%2520Korotin%26entry.1292438233%3D%2520%2520Recently%252C%2520the%2520Gromov-Wasserstein%2520Optimal%2520Transport%2520%2528GWOT%2529%2520problem%2520has%250Aattracted%2520the%2520special%2520attention%2520of%2520the%2520ML%2520community.%2520In%2520this%2520problem%252C%2520given%2520two%250Adistributions%2520supported%2520on%2520two%2520%2528possibly%2520different%2529%2520spaces%252C%2520one%2520has%2520to%2520find%2520the%250Amost%2520isometric%2520map%2520between%2520them.%2520In%2520the%2520discrete%2520variant%2520of%2520GWOT%252C%2520the%2520task%2520is%250Ato%2520learn%2520an%2520assignment%2520between%2520given%2520discrete%2520sets%2520of%2520points.%2520In%2520the%2520more%250Aadvanced%2520continuous%2520formulation%252C%2520one%2520aims%2520at%2520recovering%2520a%2520parametric%2520mapping%250Abetween%2520unknown%2520continuous%2520distributions%2520based%2520on%2520i.i.d.%2520samples%2520derived%2520from%250Athem.%2520The%2520clear%2520geometrical%2520intuition%2520behind%2520the%2520GWOT%2520makes%2520it%2520a%2520natural%2520choice%250Afor%2520several%2520practical%2520use%2520cases%252C%2520giving%2520rise%2520to%2520a%2520number%2520of%2520proposed%2520solvers.%250ASome%2520of%2520them%2520claim%2520to%2520solve%2520the%2520continuous%2520version%2520of%2520the%2520problem.%2520At%2520the%2520same%250Atime%252C%2520GWOT%2520is%2520notoriously%2520hard%252C%2520both%2520theoretically%2520and%2520numerically.%2520Moreover%252C%250Aall%2520existing%2520continuous%2520GWOT%2520solvers%2520still%2520heavily%2520rely%2520on%2520discrete%2520techniques.%250ANatural%2520questions%2520arise%253A%2520to%2520what%2520extent%2520existing%2520methods%2520unravel%2520GWOT%2520problem%252C%250Awhat%2520difficulties%2520they%2520encounter%252C%2520and%2520under%2520which%2520conditions%2520they%2520are%250Asuccessful.%2520Our%2520benchmark%2520paper%2520is%2520an%2520attempt%2520to%2520answer%2520these%2520questions.%2520We%250Aspecifically%2520focus%2520on%2520the%2520continuous%2520GWOT%2520as%2520the%2520most%2520interesting%2520and%2520debatable%250Asetup.%2520We%2520crash-test%2520existing%2520continuous%2520GWOT%2520approaches%2520on%2520different%250Ascenarios%252C%2520carefully%2520record%2520and%2520analyze%2520the%2520obtained%2520results%252C%2520and%2520identify%250Aissues.%2520Our%2520findings%2520experimentally%2520testify%2520that%2520the%2520scientific%2520community%2520is%250Astill%2520missing%2520a%2520reliable%2520continuous%2520GWOT%2520solver%252C%2520which%2520necessitates%2520further%250Aresearch%2520efforts.%2520As%2520the%2520first%2520step%2520in%2520this%2520direction%252C%2520we%2520propose%2520a%2520new%250Acontinuous%2520GWOT%2520method%2520which%2520does%2520not%2520rely%2520on%2520discrete%2520techniques%2520and%2520partially%250Asolves%2520some%2520of%2520the%2520problems%2520of%2520the%2520competitors.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Ark-130994/GW-Solvers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.05978v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncovering%20Challenges%20of%20Solving%20the%20Continuous%20Gromov-Wasserstein%0A%20%20Problem&entry.906535625=Xavier%20Aramayo%20Carrasco%20and%20Maksim%20Nekrashevich%20and%20Petr%20Mokrov%20and%20Evgeny%20Burnaev%20and%20Alexander%20Korotin&entry.1292438233=%20%20Recently%2C%20the%20Gromov-Wasserstein%20Optimal%20Transport%20%28GWOT%29%20problem%20has%0Aattracted%20the%20special%20attention%20of%20the%20ML%20community.%20In%20this%20problem%2C%20given%20two%0Adistributions%20supported%20on%20two%20%28possibly%20different%29%20spaces%2C%20one%20has%20to%20find%20the%0Amost%20isometric%20map%20between%20them.%20In%20the%20discrete%20variant%20of%20GWOT%2C%20the%20task%20is%0Ato%20learn%20an%20assignment%20between%20given%20discrete%20sets%20of%20points.%20In%20the%20more%0Aadvanced%20continuous%20formulation%2C%20one%20aims%20at%20recovering%20a%20parametric%20mapping%0Abetween%20unknown%20continuous%20distributions%20based%20on%20i.i.d.%20samples%20derived%20from%0Athem.%20The%20clear%20geometrical%20intuition%20behind%20the%20GWOT%20makes%20it%20a%20natural%20choice%0Afor%20several%20practical%20use%20cases%2C%20giving%20rise%20to%20a%20number%20of%20proposed%20solvers.%0ASome%20of%20them%20claim%20to%20solve%20the%20continuous%20version%20of%20the%20problem.%20At%20the%20same%0Atime%2C%20GWOT%20is%20notoriously%20hard%2C%20both%20theoretically%20and%20numerically.%20Moreover%2C%0Aall%20existing%20continuous%20GWOT%20solvers%20still%20heavily%20rely%20on%20discrete%20techniques.%0ANatural%20questions%20arise%3A%20to%20what%20extent%20existing%20methods%20unravel%20GWOT%20problem%2C%0Awhat%20difficulties%20they%20encounter%2C%20and%20under%20which%20conditions%20they%20are%0Asuccessful.%20Our%20benchmark%20paper%20is%20an%20attempt%20to%20answer%20these%20questions.%20We%0Aspecifically%20focus%20on%20the%20continuous%20GWOT%20as%20the%20most%20interesting%20and%20debatable%0Asetup.%20We%20crash-test%20existing%20continuous%20GWOT%20approaches%20on%20different%0Ascenarios%2C%20carefully%20record%20and%20analyze%20the%20obtained%20results%2C%20and%20identify%0Aissues.%20Our%20findings%20experimentally%20testify%20that%20the%20scientific%20community%20is%0Astill%20missing%20a%20reliable%20continuous%20GWOT%20solver%2C%20which%20necessitates%20further%0Aresearch%20efforts.%20As%20the%20first%20step%20in%20this%20direction%2C%20we%20propose%20a%20new%0Acontinuous%20GWOT%20method%20which%20does%20not%20rely%20on%20discrete%20techniques%20and%20partially%0Asolves%20some%20of%20the%20problems%20of%20the%20competitors.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Ark-130994/GW-Solvers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.05978v2&entry.124074799=Read"},
{"title": "Joint Linked Component Analysis for Multiview Data", "author": "Lin Xiao and Luo Xiao", "abstract": "  In this work, we propose the joint linked component analysis (joint\\_LCA) for\nmultiview data. Unlike classic methods which extract the shared components in a\nsequential manner, the objective of joint\\_LCA is to identify the view-specific\nloading matrices and the rank of the common latent subspace simultaneously. We\nformulate a matrix decomposition model where a joint structure and an\nindividual structure are present in each data view, which enables us to arrive\nat a clean svd representation for the cross covariance between any pair of data\nviews. An objective function with a novel penalty term is then proposed to\nachieve simultaneous estimation and rank selection. In addition, a refitting\nprocedure is employed as a remedy to reduce the shrinkage bias caused by the\npenalization.\n", "link": "http://arxiv.org/abs/2406.11761v1", "date": "2024-06-17", "relevancy": 2.4008, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4811}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4811}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4783}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20Linked%20Component%20Analysis%20for%20Multiview%20Data&body=Title%3A%20Joint%20Linked%20Component%20Analysis%20for%20Multiview%20Data%0AAuthor%3A%20Lin%20Xiao%20and%20Luo%20Xiao%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20propose%20the%20joint%20linked%20component%20analysis%20%28joint%5C_LCA%29%20for%0Amultiview%20data.%20Unlike%20classic%20methods%20which%20extract%20the%20shared%20components%20in%20a%0Asequential%20manner%2C%20the%20objective%20of%20joint%5C_LCA%20is%20to%20identify%20the%20view-specific%0Aloading%20matrices%20and%20the%20rank%20of%20the%20common%20latent%20subspace%20simultaneously.%20We%0Aformulate%20a%20matrix%20decomposition%20model%20where%20a%20joint%20structure%20and%20an%0Aindividual%20structure%20are%20present%20in%20each%20data%20view%2C%20which%20enables%20us%20to%20arrive%0Aat%20a%20clean%20svd%20representation%20for%20the%20cross%20covariance%20between%20any%20pair%20of%20data%0Aviews.%20An%20objective%20function%20with%20a%20novel%20penalty%20term%20is%20then%20proposed%20to%0Aachieve%20simultaneous%20estimation%20and%20rank%20selection.%20In%20addition%2C%20a%20refitting%0Aprocedure%20is%20employed%20as%20a%20remedy%20to%20reduce%20the%20shrinkage%20bias%20caused%20by%20the%0Apenalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11761v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520Linked%2520Component%2520Analysis%2520for%2520Multiview%2520Data%26entry.906535625%3DLin%2520Xiao%2520and%2520Luo%2520Xiao%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520joint%2520linked%2520component%2520analysis%2520%2528joint%255C_LCA%2529%2520for%250Amultiview%2520data.%2520Unlike%2520classic%2520methods%2520which%2520extract%2520the%2520shared%2520components%2520in%2520a%250Asequential%2520manner%252C%2520the%2520objective%2520of%2520joint%255C_LCA%2520is%2520to%2520identify%2520the%2520view-specific%250Aloading%2520matrices%2520and%2520the%2520rank%2520of%2520the%2520common%2520latent%2520subspace%2520simultaneously.%2520We%250Aformulate%2520a%2520matrix%2520decomposition%2520model%2520where%2520a%2520joint%2520structure%2520and%2520an%250Aindividual%2520structure%2520are%2520present%2520in%2520each%2520data%2520view%252C%2520which%2520enables%2520us%2520to%2520arrive%250Aat%2520a%2520clean%2520svd%2520representation%2520for%2520the%2520cross%2520covariance%2520between%2520any%2520pair%2520of%2520data%250Aviews.%2520An%2520objective%2520function%2520with%2520a%2520novel%2520penalty%2520term%2520is%2520then%2520proposed%2520to%250Aachieve%2520simultaneous%2520estimation%2520and%2520rank%2520selection.%2520In%2520addition%252C%2520a%2520refitting%250Aprocedure%2520is%2520employed%2520as%2520a%2520remedy%2520to%2520reduce%2520the%2520shrinkage%2520bias%2520caused%2520by%2520the%250Apenalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11761v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Linked%20Component%20Analysis%20for%20Multiview%20Data&entry.906535625=Lin%20Xiao%20and%20Luo%20Xiao&entry.1292438233=%20%20In%20this%20work%2C%20we%20propose%20the%20joint%20linked%20component%20analysis%20%28joint%5C_LCA%29%20for%0Amultiview%20data.%20Unlike%20classic%20methods%20which%20extract%20the%20shared%20components%20in%20a%0Asequential%20manner%2C%20the%20objective%20of%20joint%5C_LCA%20is%20to%20identify%20the%20view-specific%0Aloading%20matrices%20and%20the%20rank%20of%20the%20common%20latent%20subspace%20simultaneously.%20We%0Aformulate%20a%20matrix%20decomposition%20model%20where%20a%20joint%20structure%20and%20an%0Aindividual%20structure%20are%20present%20in%20each%20data%20view%2C%20which%20enables%20us%20to%20arrive%0Aat%20a%20clean%20svd%20representation%20for%20the%20cross%20covariance%20between%20any%20pair%20of%20data%0Aviews.%20An%20objective%20function%20with%20a%20novel%20penalty%20term%20is%20then%20proposed%20to%0Aachieve%20simultaneous%20estimation%20and%20rank%20selection.%20In%20addition%2C%20a%20refitting%0Aprocedure%20is%20employed%20as%20a%20remedy%20to%20reduce%20the%20shrinkage%20bias%20caused%20by%20the%0Apenalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11761v1&entry.124074799=Read"},
{"title": "InterControl: Zero-shot Human Interaction Generation by Controlling\n  Every Joint", "author": "Zhenzhi Wang and Jingbo Wang and Yixuan Li and Dahua Lin and Bo Dai", "abstract": "  Text-conditioned motion synthesis has made remarkable progress with the\nemergence of diffusion models. However, the majority of these motion diffusion\nmodels are primarily designed for a single character and overlook multi-human\ninteractions. In our approach, we strive to explore this problem by\nsynthesizing human motion with interactions for a group of characters of any\nsize in a zero-shot manner. The key aspect of our approach is the adaptation of\nhuman-wise interactions as pairs of human joints that can be either in contact\nor separated by a desired distance. In contrast to existing methods that\nnecessitate training motion generation models on multi-human motion datasets\nwith a fixed number of characters, our approach inherently possesses the\nflexibility to model human interactions involving an arbitrary number of\nindividuals, thereby transcending the limitations imposed by the training data.\nWe introduce a novel controllable motion generation method, InterControl, to\nencourage the synthesized motions maintaining the desired distance between\njoint pairs. It consists of a motion controller and an inverse kinematics\nguidance module that realistically and accurately aligns the joints of\nsynthesized characters to the desired location. Furthermore, we demonstrate\nthat the distance between joint pairs for human-wise interactions can be\ngenerated using an off-the-shelf Large Language Model (LLM). Experimental\nresults highlight the capability of our framework to generate interactions with\nmultiple human characters and its potential to work with off-the-shelf\nphysics-based character simulators.\n", "link": "http://arxiv.org/abs/2311.15864v3", "date": "2024-06-17", "relevancy": 2.4, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6585}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5644}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InterControl%3A%20Zero-shot%20Human%20Interaction%20Generation%20by%20Controlling%0A%20%20Every%20Joint&body=Title%3A%20InterControl%3A%20Zero-shot%20Human%20Interaction%20Generation%20by%20Controlling%0A%20%20Every%20Joint%0AAuthor%3A%20Zhenzhi%20Wang%20and%20Jingbo%20Wang%20and%20Yixuan%20Li%20and%20Dahua%20Lin%20and%20Bo%20Dai%0AAbstract%3A%20%20%20Text-conditioned%20motion%20synthesis%20has%20made%20remarkable%20progress%20with%20the%0Aemergence%20of%20diffusion%20models.%20However%2C%20the%20majority%20of%20these%20motion%20diffusion%0Amodels%20are%20primarily%20designed%20for%20a%20single%20character%20and%20overlook%20multi-human%0Ainteractions.%20In%20our%20approach%2C%20we%20strive%20to%20explore%20this%20problem%20by%0Asynthesizing%20human%20motion%20with%20interactions%20for%20a%20group%20of%20characters%20of%20any%0Asize%20in%20a%20zero-shot%20manner.%20The%20key%20aspect%20of%20our%20approach%20is%20the%20adaptation%20of%0Ahuman-wise%20interactions%20as%20pairs%20of%20human%20joints%20that%20can%20be%20either%20in%20contact%0Aor%20separated%20by%20a%20desired%20distance.%20In%20contrast%20to%20existing%20methods%20that%0Anecessitate%20training%20motion%20generation%20models%20on%20multi-human%20motion%20datasets%0Awith%20a%20fixed%20number%20of%20characters%2C%20our%20approach%20inherently%20possesses%20the%0Aflexibility%20to%20model%20human%20interactions%20involving%20an%20arbitrary%20number%20of%0Aindividuals%2C%20thereby%20transcending%20the%20limitations%20imposed%20by%20the%20training%20data.%0AWe%20introduce%20a%20novel%20controllable%20motion%20generation%20method%2C%20InterControl%2C%20to%0Aencourage%20the%20synthesized%20motions%20maintaining%20the%20desired%20distance%20between%0Ajoint%20pairs.%20It%20consists%20of%20a%20motion%20controller%20and%20an%20inverse%20kinematics%0Aguidance%20module%20that%20realistically%20and%20accurately%20aligns%20the%20joints%20of%0Asynthesized%20characters%20to%20the%20desired%20location.%20Furthermore%2C%20we%20demonstrate%0Athat%20the%20distance%20between%20joint%20pairs%20for%20human-wise%20interactions%20can%20be%0Agenerated%20using%20an%20off-the-shelf%20Large%20Language%20Model%20%28LLM%29.%20Experimental%0Aresults%20highlight%20the%20capability%20of%20our%20framework%20to%20generate%20interactions%20with%0Amultiple%20human%20characters%20and%20its%20potential%20to%20work%20with%20off-the-shelf%0Aphysics-based%20character%20simulators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.15864v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterControl%253A%2520Zero-shot%2520Human%2520Interaction%2520Generation%2520by%2520Controlling%250A%2520%2520Every%2520Joint%26entry.906535625%3DZhenzhi%2520Wang%2520and%2520Jingbo%2520Wang%2520and%2520Yixuan%2520Li%2520and%2520Dahua%2520Lin%2520and%2520Bo%2520Dai%26entry.1292438233%3D%2520%2520Text-conditioned%2520motion%2520synthesis%2520has%2520made%2520remarkable%2520progress%2520with%2520the%250Aemergence%2520of%2520diffusion%2520models.%2520However%252C%2520the%2520majority%2520of%2520these%2520motion%2520diffusion%250Amodels%2520are%2520primarily%2520designed%2520for%2520a%2520single%2520character%2520and%2520overlook%2520multi-human%250Ainteractions.%2520In%2520our%2520approach%252C%2520we%2520strive%2520to%2520explore%2520this%2520problem%2520by%250Asynthesizing%2520human%2520motion%2520with%2520interactions%2520for%2520a%2520group%2520of%2520characters%2520of%2520any%250Asize%2520in%2520a%2520zero-shot%2520manner.%2520The%2520key%2520aspect%2520of%2520our%2520approach%2520is%2520the%2520adaptation%2520of%250Ahuman-wise%2520interactions%2520as%2520pairs%2520of%2520human%2520joints%2520that%2520can%2520be%2520either%2520in%2520contact%250Aor%2520separated%2520by%2520a%2520desired%2520distance.%2520In%2520contrast%2520to%2520existing%2520methods%2520that%250Anecessitate%2520training%2520motion%2520generation%2520models%2520on%2520multi-human%2520motion%2520datasets%250Awith%2520a%2520fixed%2520number%2520of%2520characters%252C%2520our%2520approach%2520inherently%2520possesses%2520the%250Aflexibility%2520to%2520model%2520human%2520interactions%2520involving%2520an%2520arbitrary%2520number%2520of%250Aindividuals%252C%2520thereby%2520transcending%2520the%2520limitations%2520imposed%2520by%2520the%2520training%2520data.%250AWe%2520introduce%2520a%2520novel%2520controllable%2520motion%2520generation%2520method%252C%2520InterControl%252C%2520to%250Aencourage%2520the%2520synthesized%2520motions%2520maintaining%2520the%2520desired%2520distance%2520between%250Ajoint%2520pairs.%2520It%2520consists%2520of%2520a%2520motion%2520controller%2520and%2520an%2520inverse%2520kinematics%250Aguidance%2520module%2520that%2520realistically%2520and%2520accurately%2520aligns%2520the%2520joints%2520of%250Asynthesized%2520characters%2520to%2520the%2520desired%2520location.%2520Furthermore%252C%2520we%2520demonstrate%250Athat%2520the%2520distance%2520between%2520joint%2520pairs%2520for%2520human-wise%2520interactions%2520can%2520be%250Agenerated%2520using%2520an%2520off-the-shelf%2520Large%2520Language%2520Model%2520%2528LLM%2529.%2520Experimental%250Aresults%2520highlight%2520the%2520capability%2520of%2520our%2520framework%2520to%2520generate%2520interactions%2520with%250Amultiple%2520human%2520characters%2520and%2520its%2520potential%2520to%2520work%2520with%2520off-the-shelf%250Aphysics-based%2520character%2520simulators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.15864v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InterControl%3A%20Zero-shot%20Human%20Interaction%20Generation%20by%20Controlling%0A%20%20Every%20Joint&entry.906535625=Zhenzhi%20Wang%20and%20Jingbo%20Wang%20and%20Yixuan%20Li%20and%20Dahua%20Lin%20and%20Bo%20Dai&entry.1292438233=%20%20Text-conditioned%20motion%20synthesis%20has%20made%20remarkable%20progress%20with%20the%0Aemergence%20of%20diffusion%20models.%20However%2C%20the%20majority%20of%20these%20motion%20diffusion%0Amodels%20are%20primarily%20designed%20for%20a%20single%20character%20and%20overlook%20multi-human%0Ainteractions.%20In%20our%20approach%2C%20we%20strive%20to%20explore%20this%20problem%20by%0Asynthesizing%20human%20motion%20with%20interactions%20for%20a%20group%20of%20characters%20of%20any%0Asize%20in%20a%20zero-shot%20manner.%20The%20key%20aspect%20of%20our%20approach%20is%20the%20adaptation%20of%0Ahuman-wise%20interactions%20as%20pairs%20of%20human%20joints%20that%20can%20be%20either%20in%20contact%0Aor%20separated%20by%20a%20desired%20distance.%20In%20contrast%20to%20existing%20methods%20that%0Anecessitate%20training%20motion%20generation%20models%20on%20multi-human%20motion%20datasets%0Awith%20a%20fixed%20number%20of%20characters%2C%20our%20approach%20inherently%20possesses%20the%0Aflexibility%20to%20model%20human%20interactions%20involving%20an%20arbitrary%20number%20of%0Aindividuals%2C%20thereby%20transcending%20the%20limitations%20imposed%20by%20the%20training%20data.%0AWe%20introduce%20a%20novel%20controllable%20motion%20generation%20method%2C%20InterControl%2C%20to%0Aencourage%20the%20synthesized%20motions%20maintaining%20the%20desired%20distance%20between%0Ajoint%20pairs.%20It%20consists%20of%20a%20motion%20controller%20and%20an%20inverse%20kinematics%0Aguidance%20module%20that%20realistically%20and%20accurately%20aligns%20the%20joints%20of%0Asynthesized%20characters%20to%20the%20desired%20location.%20Furthermore%2C%20we%20demonstrate%0Athat%20the%20distance%20between%20joint%20pairs%20for%20human-wise%20interactions%20can%20be%0Agenerated%20using%20an%20off-the-shelf%20Large%20Language%20Model%20%28LLM%29.%20Experimental%0Aresults%20highlight%20the%20capability%20of%20our%20framework%20to%20generate%20interactions%20with%0Amultiple%20human%20characters%20and%20its%20potential%20to%20work%20with%20off-the-shelf%0Aphysics-based%20character%20simulators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.15864v3&entry.124074799=Read"},
{"title": "RO-SVD: A Reconfigurable Hardware Copyright Protection Framework for\n  AIGC Applications", "author": "Zhuoheng Ran and Muhammad A. A. Abdelgawad and Zekai Zhang and Ray C. C. Cheung and Hong Yan", "abstract": "  The dramatic surge in the utilisation of generative artificial intelligence\n(GenAI) underscores the need for a secure and efficient mechanism to\nresponsibly manage, use and disseminate multi-dimensional data generated by\nartificial intelligence (AI). In this paper, we propose a blockchain-based\ncopyright traceability framework called ring oscillator-singular value\ndecomposition (RO-SVD), which introduces decomposition computing to approximate\nlow-rank matrices generated from hardware entropy sources and establishes an\nAI-generated content (AIGC) copyright traceability mechanism at the device\nlevel. By leveraging the parallelism and reconfigurability of\nfield-programmable gate arrays (FPGAs), our framework can be easily constructed\non existing AI-accelerated devices and provide a low-cost solution to emerging\ncopyright issues of AIGC. We developed a hardware-software (HW/SW) co-design\nprototype based on comprehensive analysis and on-board experiments with\nmultiple AI-applicable FPGAs. Using AI-generated images as a case study, our\nframework demonstrated effectiveness and emphasised customisation,\nunpredictability, efficiency, management and reconfigurability. To the best of\nour knowledge, this is the first practical hardware study discussing and\nimplementing copyright traceability specifically for AI-generated content.\n", "link": "http://arxiv.org/abs/2406.11536v1", "date": "2024-06-17", "relevancy": 2.3904, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.489}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4794}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4658}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RO-SVD%3A%20A%20Reconfigurable%20Hardware%20Copyright%20Protection%20Framework%20for%0A%20%20AIGC%20Applications&body=Title%3A%20RO-SVD%3A%20A%20Reconfigurable%20Hardware%20Copyright%20Protection%20Framework%20for%0A%20%20AIGC%20Applications%0AAuthor%3A%20Zhuoheng%20Ran%20and%20Muhammad%20A.%20A.%20Abdelgawad%20and%20Zekai%20Zhang%20and%20Ray%20C.%20C.%20Cheung%20and%20Hong%20Yan%0AAbstract%3A%20%20%20The%20dramatic%20surge%20in%20the%20utilisation%20of%20generative%20artificial%20intelligence%0A%28GenAI%29%20underscores%20the%20need%20for%20a%20secure%20and%20efficient%20mechanism%20to%0Aresponsibly%20manage%2C%20use%20and%20disseminate%20multi-dimensional%20data%20generated%20by%0Aartificial%20intelligence%20%28AI%29.%20In%20this%20paper%2C%20we%20propose%20a%20blockchain-based%0Acopyright%20traceability%20framework%20called%20ring%20oscillator-singular%20value%0Adecomposition%20%28RO-SVD%29%2C%20which%20introduces%20decomposition%20computing%20to%20approximate%0Alow-rank%20matrices%20generated%20from%20hardware%20entropy%20sources%20and%20establishes%20an%0AAI-generated%20content%20%28AIGC%29%20copyright%20traceability%20mechanism%20at%20the%20device%0Alevel.%20By%20leveraging%20the%20parallelism%20and%20reconfigurability%20of%0Afield-programmable%20gate%20arrays%20%28FPGAs%29%2C%20our%20framework%20can%20be%20easily%20constructed%0Aon%20existing%20AI-accelerated%20devices%20and%20provide%20a%20low-cost%20solution%20to%20emerging%0Acopyright%20issues%20of%20AIGC.%20We%20developed%20a%20hardware-software%20%28HW/SW%29%20co-design%0Aprototype%20based%20on%20comprehensive%20analysis%20and%20on-board%20experiments%20with%0Amultiple%20AI-applicable%20FPGAs.%20Using%20AI-generated%20images%20as%20a%20case%20study%2C%20our%0Aframework%20demonstrated%20effectiveness%20and%20emphasised%20customisation%2C%0Aunpredictability%2C%20efficiency%2C%20management%20and%20reconfigurability.%20To%20the%20best%20of%0Aour%20knowledge%2C%20this%20is%20the%20first%20practical%20hardware%20study%20discussing%20and%0Aimplementing%20copyright%20traceability%20specifically%20for%20AI-generated%20content.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11536v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRO-SVD%253A%2520A%2520Reconfigurable%2520Hardware%2520Copyright%2520Protection%2520Framework%2520for%250A%2520%2520AIGC%2520Applications%26entry.906535625%3DZhuoheng%2520Ran%2520and%2520Muhammad%2520A.%2520A.%2520Abdelgawad%2520and%2520Zekai%2520Zhang%2520and%2520Ray%2520C.%2520C.%2520Cheung%2520and%2520Hong%2520Yan%26entry.1292438233%3D%2520%2520The%2520dramatic%2520surge%2520in%2520the%2520utilisation%2520of%2520generative%2520artificial%2520intelligence%250A%2528GenAI%2529%2520underscores%2520the%2520need%2520for%2520a%2520secure%2520and%2520efficient%2520mechanism%2520to%250Aresponsibly%2520manage%252C%2520use%2520and%2520disseminate%2520multi-dimensional%2520data%2520generated%2520by%250Aartificial%2520intelligence%2520%2528AI%2529.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520blockchain-based%250Acopyright%2520traceability%2520framework%2520called%2520ring%2520oscillator-singular%2520value%250Adecomposition%2520%2528RO-SVD%2529%252C%2520which%2520introduces%2520decomposition%2520computing%2520to%2520approximate%250Alow-rank%2520matrices%2520generated%2520from%2520hardware%2520entropy%2520sources%2520and%2520establishes%2520an%250AAI-generated%2520content%2520%2528AIGC%2529%2520copyright%2520traceability%2520mechanism%2520at%2520the%2520device%250Alevel.%2520By%2520leveraging%2520the%2520parallelism%2520and%2520reconfigurability%2520of%250Afield-programmable%2520gate%2520arrays%2520%2528FPGAs%2529%252C%2520our%2520framework%2520can%2520be%2520easily%2520constructed%250Aon%2520existing%2520AI-accelerated%2520devices%2520and%2520provide%2520a%2520low-cost%2520solution%2520to%2520emerging%250Acopyright%2520issues%2520of%2520AIGC.%2520We%2520developed%2520a%2520hardware-software%2520%2528HW/SW%2529%2520co-design%250Aprototype%2520based%2520on%2520comprehensive%2520analysis%2520and%2520on-board%2520experiments%2520with%250Amultiple%2520AI-applicable%2520FPGAs.%2520Using%2520AI-generated%2520images%2520as%2520a%2520case%2520study%252C%2520our%250Aframework%2520demonstrated%2520effectiveness%2520and%2520emphasised%2520customisation%252C%250Aunpredictability%252C%2520efficiency%252C%2520management%2520and%2520reconfigurability.%2520To%2520the%2520best%2520of%250Aour%2520knowledge%252C%2520this%2520is%2520the%2520first%2520practical%2520hardware%2520study%2520discussing%2520and%250Aimplementing%2520copyright%2520traceability%2520specifically%2520for%2520AI-generated%2520content.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11536v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RO-SVD%3A%20A%20Reconfigurable%20Hardware%20Copyright%20Protection%20Framework%20for%0A%20%20AIGC%20Applications&entry.906535625=Zhuoheng%20Ran%20and%20Muhammad%20A.%20A.%20Abdelgawad%20and%20Zekai%20Zhang%20and%20Ray%20C.%20C.%20Cheung%20and%20Hong%20Yan&entry.1292438233=%20%20The%20dramatic%20surge%20in%20the%20utilisation%20of%20generative%20artificial%20intelligence%0A%28GenAI%29%20underscores%20the%20need%20for%20a%20secure%20and%20efficient%20mechanism%20to%0Aresponsibly%20manage%2C%20use%20and%20disseminate%20multi-dimensional%20data%20generated%20by%0Aartificial%20intelligence%20%28AI%29.%20In%20this%20paper%2C%20we%20propose%20a%20blockchain-based%0Acopyright%20traceability%20framework%20called%20ring%20oscillator-singular%20value%0Adecomposition%20%28RO-SVD%29%2C%20which%20introduces%20decomposition%20computing%20to%20approximate%0Alow-rank%20matrices%20generated%20from%20hardware%20entropy%20sources%20and%20establishes%20an%0AAI-generated%20content%20%28AIGC%29%20copyright%20traceability%20mechanism%20at%20the%20device%0Alevel.%20By%20leveraging%20the%20parallelism%20and%20reconfigurability%20of%0Afield-programmable%20gate%20arrays%20%28FPGAs%29%2C%20our%20framework%20can%20be%20easily%20constructed%0Aon%20existing%20AI-accelerated%20devices%20and%20provide%20a%20low-cost%20solution%20to%20emerging%0Acopyright%20issues%20of%20AIGC.%20We%20developed%20a%20hardware-software%20%28HW/SW%29%20co-design%0Aprototype%20based%20on%20comprehensive%20analysis%20and%20on-board%20experiments%20with%0Amultiple%20AI-applicable%20FPGAs.%20Using%20AI-generated%20images%20as%20a%20case%20study%2C%20our%0Aframework%20demonstrated%20effectiveness%20and%20emphasised%20customisation%2C%0Aunpredictability%2C%20efficiency%2C%20management%20and%20reconfigurability.%20To%20the%20best%20of%0Aour%20knowledge%2C%20this%20is%20the%20first%20practical%20hardware%20study%20discussing%20and%0Aimplementing%20copyright%20traceability%20specifically%20for%20AI-generated%20content.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11536v1&entry.124074799=Read"},
{"title": "Benchmarking General Purpose In-Context Learning", "author": "Fan Wang and Chuan Lin and Yang Cao and Yu Kang", "abstract": "  In-context learning (ICL) is becoming increasingly appealing to the AI\ncommunity due to its flexibility, generality, sample efficiency, and exemption\nfrom artificial optimization skills. It is desirable to further enhance the\ngenerality and capability of ICL, which gives rise to the concept of\ngeneral-purpose in-context learning (GPICL). We aim to extend ICL to address a\nbroader range of tasks with an extended learning horizon and higher improvement\npotential, albeit with relatively limited zero-shot generalization. To this\nend, we introduce two lightweight but insightful benchmarks specifically\ncrafted to train and evaluate GPICL functionalities. Each benchmark includes a\nvast number of tasks characterized by significant task variance, featuring\nminimal inductive bias. These tasks are also designed to facilitate lifelong\nin-context learning through continuous generation and interaction. These\nfeatures pose significant challenges for models that rely on context or\ninteractions to improve their proficiency, including language models, decision\nmodels, and world models. Our experiments reveal that the scale of parameters\nalone may not be crucial for ICL or GPICL, suggesting alternative approaches\nsuch as increasing the scale of contexts and memory states.\n", "link": "http://arxiv.org/abs/2405.17234v4", "date": "2024-06-17", "relevancy": 2.3904, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.511}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4696}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20General%20Purpose%20In-Context%20Learning&body=Title%3A%20Benchmarking%20General%20Purpose%20In-Context%20Learning%0AAuthor%3A%20Fan%20Wang%20and%20Chuan%20Lin%20and%20Yang%20Cao%20and%20Yu%20Kang%0AAbstract%3A%20%20%20In-context%20learning%20%28ICL%29%20is%20becoming%20increasingly%20appealing%20to%20the%20AI%0Acommunity%20due%20to%20its%20flexibility%2C%20generality%2C%20sample%20efficiency%2C%20and%20exemption%0Afrom%20artificial%20optimization%20skills.%20It%20is%20desirable%20to%20further%20enhance%20the%0Agenerality%20and%20capability%20of%20ICL%2C%20which%20gives%20rise%20to%20the%20concept%20of%0Ageneral-purpose%20in-context%20learning%20%28GPICL%29.%20We%20aim%20to%20extend%20ICL%20to%20address%20a%0Abroader%20range%20of%20tasks%20with%20an%20extended%20learning%20horizon%20and%20higher%20improvement%0Apotential%2C%20albeit%20with%20relatively%20limited%20zero-shot%20generalization.%20To%20this%0Aend%2C%20we%20introduce%20two%20lightweight%20but%20insightful%20benchmarks%20specifically%0Acrafted%20to%20train%20and%20evaluate%20GPICL%20functionalities.%20Each%20benchmark%20includes%20a%0Avast%20number%20of%20tasks%20characterized%20by%20significant%20task%20variance%2C%20featuring%0Aminimal%20inductive%20bias.%20These%20tasks%20are%20also%20designed%20to%20facilitate%20lifelong%0Ain-context%20learning%20through%20continuous%20generation%20and%20interaction.%20These%0Afeatures%20pose%20significant%20challenges%20for%20models%20that%20rely%20on%20context%20or%0Ainteractions%20to%20improve%20their%20proficiency%2C%20including%20language%20models%2C%20decision%0Amodels%2C%20and%20world%20models.%20Our%20experiments%20reveal%20that%20the%20scale%20of%20parameters%0Aalone%20may%20not%20be%20crucial%20for%20ICL%20or%20GPICL%2C%20suggesting%20alternative%20approaches%0Asuch%20as%20increasing%20the%20scale%20of%20contexts%20and%20memory%20states.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.17234v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520General%2520Purpose%2520In-Context%2520Learning%26entry.906535625%3DFan%2520Wang%2520and%2520Chuan%2520Lin%2520and%2520Yang%2520Cao%2520and%2520Yu%2520Kang%26entry.1292438233%3D%2520%2520In-context%2520learning%2520%2528ICL%2529%2520is%2520becoming%2520increasingly%2520appealing%2520to%2520the%2520AI%250Acommunity%2520due%2520to%2520its%2520flexibility%252C%2520generality%252C%2520sample%2520efficiency%252C%2520and%2520exemption%250Afrom%2520artificial%2520optimization%2520skills.%2520It%2520is%2520desirable%2520to%2520further%2520enhance%2520the%250Agenerality%2520and%2520capability%2520of%2520ICL%252C%2520which%2520gives%2520rise%2520to%2520the%2520concept%2520of%250Ageneral-purpose%2520in-context%2520learning%2520%2528GPICL%2529.%2520We%2520aim%2520to%2520extend%2520ICL%2520to%2520address%2520a%250Abroader%2520range%2520of%2520tasks%2520with%2520an%2520extended%2520learning%2520horizon%2520and%2520higher%2520improvement%250Apotential%252C%2520albeit%2520with%2520relatively%2520limited%2520zero-shot%2520generalization.%2520To%2520this%250Aend%252C%2520we%2520introduce%2520two%2520lightweight%2520but%2520insightful%2520benchmarks%2520specifically%250Acrafted%2520to%2520train%2520and%2520evaluate%2520GPICL%2520functionalities.%2520Each%2520benchmark%2520includes%2520a%250Avast%2520number%2520of%2520tasks%2520characterized%2520by%2520significant%2520task%2520variance%252C%2520featuring%250Aminimal%2520inductive%2520bias.%2520These%2520tasks%2520are%2520also%2520designed%2520to%2520facilitate%2520lifelong%250Ain-context%2520learning%2520through%2520continuous%2520generation%2520and%2520interaction.%2520These%250Afeatures%2520pose%2520significant%2520challenges%2520for%2520models%2520that%2520rely%2520on%2520context%2520or%250Ainteractions%2520to%2520improve%2520their%2520proficiency%252C%2520including%2520language%2520models%252C%2520decision%250Amodels%252C%2520and%2520world%2520models.%2520Our%2520experiments%2520reveal%2520that%2520the%2520scale%2520of%2520parameters%250Aalone%2520may%2520not%2520be%2520crucial%2520for%2520ICL%2520or%2520GPICL%252C%2520suggesting%2520alternative%2520approaches%250Asuch%2520as%2520increasing%2520the%2520scale%2520of%2520contexts%2520and%2520memory%2520states.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.17234v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20General%20Purpose%20In-Context%20Learning&entry.906535625=Fan%20Wang%20and%20Chuan%20Lin%20and%20Yang%20Cao%20and%20Yu%20Kang&entry.1292438233=%20%20In-context%20learning%20%28ICL%29%20is%20becoming%20increasingly%20appealing%20to%20the%20AI%0Acommunity%20due%20to%20its%20flexibility%2C%20generality%2C%20sample%20efficiency%2C%20and%20exemption%0Afrom%20artificial%20optimization%20skills.%20It%20is%20desirable%20to%20further%20enhance%20the%0Agenerality%20and%20capability%20of%20ICL%2C%20which%20gives%20rise%20to%20the%20concept%20of%0Ageneral-purpose%20in-context%20learning%20%28GPICL%29.%20We%20aim%20to%20extend%20ICL%20to%20address%20a%0Abroader%20range%20of%20tasks%20with%20an%20extended%20learning%20horizon%20and%20higher%20improvement%0Apotential%2C%20albeit%20with%20relatively%20limited%20zero-shot%20generalization.%20To%20this%0Aend%2C%20we%20introduce%20two%20lightweight%20but%20insightful%20benchmarks%20specifically%0Acrafted%20to%20train%20and%20evaluate%20GPICL%20functionalities.%20Each%20benchmark%20includes%20a%0Avast%20number%20of%20tasks%20characterized%20by%20significant%20task%20variance%2C%20featuring%0Aminimal%20inductive%20bias.%20These%20tasks%20are%20also%20designed%20to%20facilitate%20lifelong%0Ain-context%20learning%20through%20continuous%20generation%20and%20interaction.%20These%0Afeatures%20pose%20significant%20challenges%20for%20models%20that%20rely%20on%20context%20or%0Ainteractions%20to%20improve%20their%20proficiency%2C%20including%20language%20models%2C%20decision%0Amodels%2C%20and%20world%20models.%20Our%20experiments%20reveal%20that%20the%20scale%20of%20parameters%0Aalone%20may%20not%20be%20crucial%20for%20ICL%20or%20GPICL%2C%20suggesting%20alternative%20approaches%0Asuch%20as%20increasing%20the%20scale%20of%20contexts%20and%20memory%20states.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.17234v4&entry.124074799=Read"},
{"title": "Adversaries With Incentives: A Strategic Alternative to Adversarial\n  Robustness", "author": "Maayan Ehrenberg and Roy Ganz and Nir Rosenfeld", "abstract": "  Adversarial training aims to defend against *adversaries*: malicious\nopponents whose sole aim is to harm predictive performance in any way possible\n- a rather harsh perspective, which we assert results in unnecessarily\nconservative models. Instead, we propose to model opponents as simply pursuing\ntheir own goals, rather than working directly against the classifier. Employing\ntools from strategic modeling, our approach uses knowledge or beliefs regarding\nthe opponent's possible incentives as inductive bias for learning. Our method\nof *strategic training* is designed to defend against opponents within an\n*incentive uncertainty set*: this resorts to adversarial learning when the set\nis maximal, but offers potential gains when it can be appropriately reduced. We\nconduct a series of experiments that show how even mild knowledge regarding the\nadversary's incentives can be useful, and that the degree of potential gains\ndepends on how incentives relate to the structure of the learning task.\n", "link": "http://arxiv.org/abs/2406.11458v1", "date": "2024-06-17", "relevancy": 2.3785, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4881}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4736}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4655}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adversaries%20With%20Incentives%3A%20A%20Strategic%20Alternative%20to%20Adversarial%0A%20%20Robustness&body=Title%3A%20Adversaries%20With%20Incentives%3A%20A%20Strategic%20Alternative%20to%20Adversarial%0A%20%20Robustness%0AAuthor%3A%20Maayan%20Ehrenberg%20and%20Roy%20Ganz%20and%20Nir%20Rosenfeld%0AAbstract%3A%20%20%20Adversarial%20training%20aims%20to%20defend%20against%20%2Aadversaries%2A%3A%20malicious%0Aopponents%20whose%20sole%20aim%20is%20to%20harm%20predictive%20performance%20in%20any%20way%20possible%0A-%20a%20rather%20harsh%20perspective%2C%20which%20we%20assert%20results%20in%20unnecessarily%0Aconservative%20models.%20Instead%2C%20we%20propose%20to%20model%20opponents%20as%20simply%20pursuing%0Atheir%20own%20goals%2C%20rather%20than%20working%20directly%20against%20the%20classifier.%20Employing%0Atools%20from%20strategic%20modeling%2C%20our%20approach%20uses%20knowledge%20or%20beliefs%20regarding%0Athe%20opponent%27s%20possible%20incentives%20as%20inductive%20bias%20for%20learning.%20Our%20method%0Aof%20%2Astrategic%20training%2A%20is%20designed%20to%20defend%20against%20opponents%20within%20an%0A%2Aincentive%20uncertainty%20set%2A%3A%20this%20resorts%20to%20adversarial%20learning%20when%20the%20set%0Ais%20maximal%2C%20but%20offers%20potential%20gains%20when%20it%20can%20be%20appropriately%20reduced.%20We%0Aconduct%20a%20series%20of%20experiments%20that%20show%20how%20even%20mild%20knowledge%20regarding%20the%0Aadversary%27s%20incentives%20can%20be%20useful%2C%20and%20that%20the%20degree%20of%20potential%20gains%0Adepends%20on%20how%20incentives%20relate%20to%20the%20structure%20of%20the%20learning%20task.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11458v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdversaries%2520With%2520Incentives%253A%2520A%2520Strategic%2520Alternative%2520to%2520Adversarial%250A%2520%2520Robustness%26entry.906535625%3DMaayan%2520Ehrenberg%2520and%2520Roy%2520Ganz%2520and%2520Nir%2520Rosenfeld%26entry.1292438233%3D%2520%2520Adversarial%2520training%2520aims%2520to%2520defend%2520against%2520%252Aadversaries%252A%253A%2520malicious%250Aopponents%2520whose%2520sole%2520aim%2520is%2520to%2520harm%2520predictive%2520performance%2520in%2520any%2520way%2520possible%250A-%2520a%2520rather%2520harsh%2520perspective%252C%2520which%2520we%2520assert%2520results%2520in%2520unnecessarily%250Aconservative%2520models.%2520Instead%252C%2520we%2520propose%2520to%2520model%2520opponents%2520as%2520simply%2520pursuing%250Atheir%2520own%2520goals%252C%2520rather%2520than%2520working%2520directly%2520against%2520the%2520classifier.%2520Employing%250Atools%2520from%2520strategic%2520modeling%252C%2520our%2520approach%2520uses%2520knowledge%2520or%2520beliefs%2520regarding%250Athe%2520opponent%2527s%2520possible%2520incentives%2520as%2520inductive%2520bias%2520for%2520learning.%2520Our%2520method%250Aof%2520%252Astrategic%2520training%252A%2520is%2520designed%2520to%2520defend%2520against%2520opponents%2520within%2520an%250A%252Aincentive%2520uncertainty%2520set%252A%253A%2520this%2520resorts%2520to%2520adversarial%2520learning%2520when%2520the%2520set%250Ais%2520maximal%252C%2520but%2520offers%2520potential%2520gains%2520when%2520it%2520can%2520be%2520appropriately%2520reduced.%2520We%250Aconduct%2520a%2520series%2520of%2520experiments%2520that%2520show%2520how%2520even%2520mild%2520knowledge%2520regarding%2520the%250Aadversary%2527s%2520incentives%2520can%2520be%2520useful%252C%2520and%2520that%2520the%2520degree%2520of%2520potential%2520gains%250Adepends%2520on%2520how%2520incentives%2520relate%2520to%2520the%2520structure%2520of%2520the%2520learning%2520task.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11458v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adversaries%20With%20Incentives%3A%20A%20Strategic%20Alternative%20to%20Adversarial%0A%20%20Robustness&entry.906535625=Maayan%20Ehrenberg%20and%20Roy%20Ganz%20and%20Nir%20Rosenfeld&entry.1292438233=%20%20Adversarial%20training%20aims%20to%20defend%20against%20%2Aadversaries%2A%3A%20malicious%0Aopponents%20whose%20sole%20aim%20is%20to%20harm%20predictive%20performance%20in%20any%20way%20possible%0A-%20a%20rather%20harsh%20perspective%2C%20which%20we%20assert%20results%20in%20unnecessarily%0Aconservative%20models.%20Instead%2C%20we%20propose%20to%20model%20opponents%20as%20simply%20pursuing%0Atheir%20own%20goals%2C%20rather%20than%20working%20directly%20against%20the%20classifier.%20Employing%0Atools%20from%20strategic%20modeling%2C%20our%20approach%20uses%20knowledge%20or%20beliefs%20regarding%0Athe%20opponent%27s%20possible%20incentives%20as%20inductive%20bias%20for%20learning.%20Our%20method%0Aof%20%2Astrategic%20training%2A%20is%20designed%20to%20defend%20against%20opponents%20within%20an%0A%2Aincentive%20uncertainty%20set%2A%3A%20this%20resorts%20to%20adversarial%20learning%20when%20the%20set%0Ais%20maximal%2C%20but%20offers%20potential%20gains%20when%20it%20can%20be%20appropriately%20reduced.%20We%0Aconduct%20a%20series%20of%20experiments%20that%20show%20how%20even%20mild%20knowledge%20regarding%20the%0Aadversary%27s%20incentives%20can%20be%20useful%2C%20and%20that%20the%20degree%20of%20potential%20gains%0Adepends%20on%20how%20incentives%20relate%20to%20the%20structure%20of%20the%20learning%20task.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11458v1&entry.124074799=Read"},
{"title": "Pointer Networks with Q-Learning for Combinatorial Optimization", "author": "Alessandro Barro", "abstract": "  We introduce the Pointer Q-Network (PQN), a hybrid neural architecture that\nintegrates model-free Q-value policy approximation with Pointer Networks\n(Ptr-Nets) to enhance the optimality of attention-based sequence generation,\nfocusing on long-term outcomes. This integration proves particularly effective\nin solving combinatorial optimization (CO) tasks, especially the Travelling\nSalesman Problem (TSP), which is the focus of our study. We address this\nchallenge by defining a Markov Decision Process (MDP) compatible with PQN,\nwhich involves iterative graph embedding, encoding and decoding by an\nLSTM-based recurrent neural network. This process generates a context vector\nand computes raw attention scores, which are dynamically adjusted by Q-values\ncalculated for all available state-action pairs before applying softmax. The\nresulting attention vector is utilized as an action distribution, with actions\nselected hinged to exploration-exploitation dynamic adaptibility of PQN. Our\nempirical results demonstrate the efficacy of this approach, also testing the\nmodel in unstable environments.\n", "link": "http://arxiv.org/abs/2311.02629v3", "date": "2024-06-17", "relevancy": 2.3599, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4759}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4722}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pointer%20Networks%20with%20Q-Learning%20for%20Combinatorial%20Optimization&body=Title%3A%20Pointer%20Networks%20with%20Q-Learning%20for%20Combinatorial%20Optimization%0AAuthor%3A%20Alessandro%20Barro%0AAbstract%3A%20%20%20We%20introduce%20the%20Pointer%20Q-Network%20%28PQN%29%2C%20a%20hybrid%20neural%20architecture%20that%0Aintegrates%20model-free%20Q-value%20policy%20approximation%20with%20Pointer%20Networks%0A%28Ptr-Nets%29%20to%20enhance%20the%20optimality%20of%20attention-based%20sequence%20generation%2C%0Afocusing%20on%20long-term%20outcomes.%20This%20integration%20proves%20particularly%20effective%0Ain%20solving%20combinatorial%20optimization%20%28CO%29%20tasks%2C%20especially%20the%20Travelling%0ASalesman%20Problem%20%28TSP%29%2C%20which%20is%20the%20focus%20of%20our%20study.%20We%20address%20this%0Achallenge%20by%20defining%20a%20Markov%20Decision%20Process%20%28MDP%29%20compatible%20with%20PQN%2C%0Awhich%20involves%20iterative%20graph%20embedding%2C%20encoding%20and%20decoding%20by%20an%0ALSTM-based%20recurrent%20neural%20network.%20This%20process%20generates%20a%20context%20vector%0Aand%20computes%20raw%20attention%20scores%2C%20which%20are%20dynamically%20adjusted%20by%20Q-values%0Acalculated%20for%20all%20available%20state-action%20pairs%20before%20applying%20softmax.%20The%0Aresulting%20attention%20vector%20is%20utilized%20as%20an%20action%20distribution%2C%20with%20actions%0Aselected%20hinged%20to%20exploration-exploitation%20dynamic%20adaptibility%20of%20PQN.%20Our%0Aempirical%20results%20demonstrate%20the%20efficacy%20of%20this%20approach%2C%20also%20testing%20the%0Amodel%20in%20unstable%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.02629v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPointer%2520Networks%2520with%2520Q-Learning%2520for%2520Combinatorial%2520Optimization%26entry.906535625%3DAlessandro%2520Barro%26entry.1292438233%3D%2520%2520We%2520introduce%2520the%2520Pointer%2520Q-Network%2520%2528PQN%2529%252C%2520a%2520hybrid%2520neural%2520architecture%2520that%250Aintegrates%2520model-free%2520Q-value%2520policy%2520approximation%2520with%2520Pointer%2520Networks%250A%2528Ptr-Nets%2529%2520to%2520enhance%2520the%2520optimality%2520of%2520attention-based%2520sequence%2520generation%252C%250Afocusing%2520on%2520long-term%2520outcomes.%2520This%2520integration%2520proves%2520particularly%2520effective%250Ain%2520solving%2520combinatorial%2520optimization%2520%2528CO%2529%2520tasks%252C%2520especially%2520the%2520Travelling%250ASalesman%2520Problem%2520%2528TSP%2529%252C%2520which%2520is%2520the%2520focus%2520of%2520our%2520study.%2520We%2520address%2520this%250Achallenge%2520by%2520defining%2520a%2520Markov%2520Decision%2520Process%2520%2528MDP%2529%2520compatible%2520with%2520PQN%252C%250Awhich%2520involves%2520iterative%2520graph%2520embedding%252C%2520encoding%2520and%2520decoding%2520by%2520an%250ALSTM-based%2520recurrent%2520neural%2520network.%2520This%2520process%2520generates%2520a%2520context%2520vector%250Aand%2520computes%2520raw%2520attention%2520scores%252C%2520which%2520are%2520dynamically%2520adjusted%2520by%2520Q-values%250Acalculated%2520for%2520all%2520available%2520state-action%2520pairs%2520before%2520applying%2520softmax.%2520The%250Aresulting%2520attention%2520vector%2520is%2520utilized%2520as%2520an%2520action%2520distribution%252C%2520with%2520actions%250Aselected%2520hinged%2520to%2520exploration-exploitation%2520dynamic%2520adaptibility%2520of%2520PQN.%2520Our%250Aempirical%2520results%2520demonstrate%2520the%2520efficacy%2520of%2520this%2520approach%252C%2520also%2520testing%2520the%250Amodel%2520in%2520unstable%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.02629v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pointer%20Networks%20with%20Q-Learning%20for%20Combinatorial%20Optimization&entry.906535625=Alessandro%20Barro&entry.1292438233=%20%20We%20introduce%20the%20Pointer%20Q-Network%20%28PQN%29%2C%20a%20hybrid%20neural%20architecture%20that%0Aintegrates%20model-free%20Q-value%20policy%20approximation%20with%20Pointer%20Networks%0A%28Ptr-Nets%29%20to%20enhance%20the%20optimality%20of%20attention-based%20sequence%20generation%2C%0Afocusing%20on%20long-term%20outcomes.%20This%20integration%20proves%20particularly%20effective%0Ain%20solving%20combinatorial%20optimization%20%28CO%29%20tasks%2C%20especially%20the%20Travelling%0ASalesman%20Problem%20%28TSP%29%2C%20which%20is%20the%20focus%20of%20our%20study.%20We%20address%20this%0Achallenge%20by%20defining%20a%20Markov%20Decision%20Process%20%28MDP%29%20compatible%20with%20PQN%2C%0Awhich%20involves%20iterative%20graph%20embedding%2C%20encoding%20and%20decoding%20by%20an%0ALSTM-based%20recurrent%20neural%20network.%20This%20process%20generates%20a%20context%20vector%0Aand%20computes%20raw%20attention%20scores%2C%20which%20are%20dynamically%20adjusted%20by%20Q-values%0Acalculated%20for%20all%20available%20state-action%20pairs%20before%20applying%20softmax.%20The%0Aresulting%20attention%20vector%20is%20utilized%20as%20an%20action%20distribution%2C%20with%20actions%0Aselected%20hinged%20to%20exploration-exploitation%20dynamic%20adaptibility%20of%20PQN.%20Our%0Aempirical%20results%20demonstrate%20the%20efficacy%20of%20this%20approach%2C%20also%20testing%20the%0Amodel%20in%20unstable%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.02629v3&entry.124074799=Read"},
{"title": "HARE: HumAn pRiors, a key to small language model Efficiency", "author": "Lingyun Zhang and Bin jin and Gaojian Ge and Lunhui Liu and Xuewen Shen and Mingyong Wu and Houqian Zhang and Yongneng Jiang and Shiqi Chen and Shi Pu", "abstract": "  Human priors play a crucial role in efficiently utilizing data in deep\nlearning. However, with the development of large language models (LLMs), there\nis an increasing emphasis on scaling both model size and data volume, which\noften diminishes the importance of human priors in data construction.\nInfluenced by these trends, existing Small Language Models (SLMs) mainly rely\non web-scraped large-scale training data, neglecting the proper incorporation\nof human priors. This oversight limits the training efficiency of language\nmodels in resource-constrained settings. In this paper, we propose a principle\nto leverage human priors for data construction. This principle emphasizes\nachieving high-performance SLMs by training on a concise dataset that\naccommodates both semantic diversity and data quality consistency, while\navoiding benchmark data leakage. Following this principle, we train an SLM\nnamed HARE-1.1B. Extensive experiments on large-scale benchmark datasets\ndemonstrate that HARE-1.1B performs favorably against state-of-the-art SLMs,\nvalidating the effectiveness of the proposed principle. Additionally, this\nprovides new insights into efficient language model training in\nresource-constrained environments from the view of human priors.\n", "link": "http://arxiv.org/abs/2406.11410v1", "date": "2024-06-17", "relevancy": 2.3392, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4902}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4592}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HARE%3A%20HumAn%20pRiors%2C%20a%20key%20to%20small%20language%20model%20Efficiency&body=Title%3A%20HARE%3A%20HumAn%20pRiors%2C%20a%20key%20to%20small%20language%20model%20Efficiency%0AAuthor%3A%20Lingyun%20Zhang%20and%20Bin%20jin%20and%20Gaojian%20Ge%20and%20Lunhui%20Liu%20and%20Xuewen%20Shen%20and%20Mingyong%20Wu%20and%20Houqian%20Zhang%20and%20Yongneng%20Jiang%20and%20Shiqi%20Chen%20and%20Shi%20Pu%0AAbstract%3A%20%20%20Human%20priors%20play%20a%20crucial%20role%20in%20efficiently%20utilizing%20data%20in%20deep%0Alearning.%20However%2C%20with%20the%20development%20of%20large%20language%20models%20%28LLMs%29%2C%20there%0Ais%20an%20increasing%20emphasis%20on%20scaling%20both%20model%20size%20and%20data%20volume%2C%20which%0Aoften%20diminishes%20the%20importance%20of%20human%20priors%20in%20data%20construction.%0AInfluenced%20by%20these%20trends%2C%20existing%20Small%20Language%20Models%20%28SLMs%29%20mainly%20rely%0Aon%20web-scraped%20large-scale%20training%20data%2C%20neglecting%20the%20proper%20incorporation%0Aof%20human%20priors.%20This%20oversight%20limits%20the%20training%20efficiency%20of%20language%0Amodels%20in%20resource-constrained%20settings.%20In%20this%20paper%2C%20we%20propose%20a%20principle%0Ato%20leverage%20human%20priors%20for%20data%20construction.%20This%20principle%20emphasizes%0Aachieving%20high-performance%20SLMs%20by%20training%20on%20a%20concise%20dataset%20that%0Aaccommodates%20both%20semantic%20diversity%20and%20data%20quality%20consistency%2C%20while%0Aavoiding%20benchmark%20data%20leakage.%20Following%20this%20principle%2C%20we%20train%20an%20SLM%0Anamed%20HARE-1.1B.%20Extensive%20experiments%20on%20large-scale%20benchmark%20datasets%0Ademonstrate%20that%20HARE-1.1B%20performs%20favorably%20against%20state-of-the-art%20SLMs%2C%0Avalidating%20the%20effectiveness%20of%20the%20proposed%20principle.%20Additionally%2C%20this%0Aprovides%20new%20insights%20into%20efficient%20language%20model%20training%20in%0Aresource-constrained%20environments%20from%20the%20view%20of%20human%20priors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11410v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHARE%253A%2520HumAn%2520pRiors%252C%2520a%2520key%2520to%2520small%2520language%2520model%2520Efficiency%26entry.906535625%3DLingyun%2520Zhang%2520and%2520Bin%2520jin%2520and%2520Gaojian%2520Ge%2520and%2520Lunhui%2520Liu%2520and%2520Xuewen%2520Shen%2520and%2520Mingyong%2520Wu%2520and%2520Houqian%2520Zhang%2520and%2520Yongneng%2520Jiang%2520and%2520Shiqi%2520Chen%2520and%2520Shi%2520Pu%26entry.1292438233%3D%2520%2520Human%2520priors%2520play%2520a%2520crucial%2520role%2520in%2520efficiently%2520utilizing%2520data%2520in%2520deep%250Alearning.%2520However%252C%2520with%2520the%2520development%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520there%250Ais%2520an%2520increasing%2520emphasis%2520on%2520scaling%2520both%2520model%2520size%2520and%2520data%2520volume%252C%2520which%250Aoften%2520diminishes%2520the%2520importance%2520of%2520human%2520priors%2520in%2520data%2520construction.%250AInfluenced%2520by%2520these%2520trends%252C%2520existing%2520Small%2520Language%2520Models%2520%2528SLMs%2529%2520mainly%2520rely%250Aon%2520web-scraped%2520large-scale%2520training%2520data%252C%2520neglecting%2520the%2520proper%2520incorporation%250Aof%2520human%2520priors.%2520This%2520oversight%2520limits%2520the%2520training%2520efficiency%2520of%2520language%250Amodels%2520in%2520resource-constrained%2520settings.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520principle%250Ato%2520leverage%2520human%2520priors%2520for%2520data%2520construction.%2520This%2520principle%2520emphasizes%250Aachieving%2520high-performance%2520SLMs%2520by%2520training%2520on%2520a%2520concise%2520dataset%2520that%250Aaccommodates%2520both%2520semantic%2520diversity%2520and%2520data%2520quality%2520consistency%252C%2520while%250Aavoiding%2520benchmark%2520data%2520leakage.%2520Following%2520this%2520principle%252C%2520we%2520train%2520an%2520SLM%250Anamed%2520HARE-1.1B.%2520Extensive%2520experiments%2520on%2520large-scale%2520benchmark%2520datasets%250Ademonstrate%2520that%2520HARE-1.1B%2520performs%2520favorably%2520against%2520state-of-the-art%2520SLMs%252C%250Avalidating%2520the%2520effectiveness%2520of%2520the%2520proposed%2520principle.%2520Additionally%252C%2520this%250Aprovides%2520new%2520insights%2520into%2520efficient%2520language%2520model%2520training%2520in%250Aresource-constrained%2520environments%2520from%2520the%2520view%2520of%2520human%2520priors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11410v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HARE%3A%20HumAn%20pRiors%2C%20a%20key%20to%20small%20language%20model%20Efficiency&entry.906535625=Lingyun%20Zhang%20and%20Bin%20jin%20and%20Gaojian%20Ge%20and%20Lunhui%20Liu%20and%20Xuewen%20Shen%20and%20Mingyong%20Wu%20and%20Houqian%20Zhang%20and%20Yongneng%20Jiang%20and%20Shiqi%20Chen%20and%20Shi%20Pu&entry.1292438233=%20%20Human%20priors%20play%20a%20crucial%20role%20in%20efficiently%20utilizing%20data%20in%20deep%0Alearning.%20However%2C%20with%20the%20development%20of%20large%20language%20models%20%28LLMs%29%2C%20there%0Ais%20an%20increasing%20emphasis%20on%20scaling%20both%20model%20size%20and%20data%20volume%2C%20which%0Aoften%20diminishes%20the%20importance%20of%20human%20priors%20in%20data%20construction.%0AInfluenced%20by%20these%20trends%2C%20existing%20Small%20Language%20Models%20%28SLMs%29%20mainly%20rely%0Aon%20web-scraped%20large-scale%20training%20data%2C%20neglecting%20the%20proper%20incorporation%0Aof%20human%20priors.%20This%20oversight%20limits%20the%20training%20efficiency%20of%20language%0Amodels%20in%20resource-constrained%20settings.%20In%20this%20paper%2C%20we%20propose%20a%20principle%0Ato%20leverage%20human%20priors%20for%20data%20construction.%20This%20principle%20emphasizes%0Aachieving%20high-performance%20SLMs%20by%20training%20on%20a%20concise%20dataset%20that%0Aaccommodates%20both%20semantic%20diversity%20and%20data%20quality%20consistency%2C%20while%0Aavoiding%20benchmark%20data%20leakage.%20Following%20this%20principle%2C%20we%20train%20an%20SLM%0Anamed%20HARE-1.1B.%20Extensive%20experiments%20on%20large-scale%20benchmark%20datasets%0Ademonstrate%20that%20HARE-1.1B%20performs%20favorably%20against%20state-of-the-art%20SLMs%2C%0Avalidating%20the%20effectiveness%20of%20the%20proposed%20principle.%20Additionally%2C%20this%0Aprovides%20new%20insights%20into%20efficient%20language%20model%20training%20in%0Aresource-constrained%20environments%20from%20the%20view%20of%20human%20priors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11410v1&entry.124074799=Read"},
{"title": "Learning from Exemplars for Interactive Image Segmentation", "author": "Kun Li and Hao Cheng and George Vosselman and Michael Ying Yang", "abstract": "  Interactive image segmentation enables users to interact minimally with a\nmachine, facilitating the gradual refinement of the segmentation mask for a\ntarget of interest. Previous studies have demonstrated impressive performance\nin extracting a single target mask through interactive segmentation. However,\nthe information cues of previously interacted objects have been overlooked in\nthe existing methods, which can be further explored to speed up interactive\nsegmentation for multiple targets in the same category. To this end, we\nintroduce novel interactive segmentation frameworks for both a single object\nand multiple objects in the same category. Specifically, our model leverages\ntransformer backbones to extract interaction-focused visual features from the\nimage and the interactions to obtain a satisfactory mask of a target as an\nexemplar. For multiple objects, we propose an exemplar-informed module to\nenhance the learning of similarities among the objects of the target category.\nTo combine attended features from different modules, we incorporate\ncross-attention blocks followed by a feature fusion module. Experiments\nconducted on mainstream benchmarks demonstrate that our models achieve superior\nperformance compared to previous methods. Particularly, our model reduces\nusers' labor by around 15\\%, requiring two fewer clicks to achieve target IoUs\n85\\% and 90\\%. The results highlight our models' potential as a flexible and\npractical annotation tool. The source code will be released after publication.\n", "link": "http://arxiv.org/abs/2406.11472v1", "date": "2024-06-17", "relevancy": 2.2997, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5884}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5686}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Exemplars%20for%20Interactive%20Image%20Segmentation&body=Title%3A%20Learning%20from%20Exemplars%20for%20Interactive%20Image%20Segmentation%0AAuthor%3A%20Kun%20Li%20and%20Hao%20Cheng%20and%20George%20Vosselman%20and%20Michael%20Ying%20Yang%0AAbstract%3A%20%20%20Interactive%20image%20segmentation%20enables%20users%20to%20interact%20minimally%20with%20a%0Amachine%2C%20facilitating%20the%20gradual%20refinement%20of%20the%20segmentation%20mask%20for%20a%0Atarget%20of%20interest.%20Previous%20studies%20have%20demonstrated%20impressive%20performance%0Ain%20extracting%20a%20single%20target%20mask%20through%20interactive%20segmentation.%20However%2C%0Athe%20information%20cues%20of%20previously%20interacted%20objects%20have%20been%20overlooked%20in%0Athe%20existing%20methods%2C%20which%20can%20be%20further%20explored%20to%20speed%20up%20interactive%0Asegmentation%20for%20multiple%20targets%20in%20the%20same%20category.%20To%20this%20end%2C%20we%0Aintroduce%20novel%20interactive%20segmentation%20frameworks%20for%20both%20a%20single%20object%0Aand%20multiple%20objects%20in%20the%20same%20category.%20Specifically%2C%20our%20model%20leverages%0Atransformer%20backbones%20to%20extract%20interaction-focused%20visual%20features%20from%20the%0Aimage%20and%20the%20interactions%20to%20obtain%20a%20satisfactory%20mask%20of%20a%20target%20as%20an%0Aexemplar.%20For%20multiple%20objects%2C%20we%20propose%20an%20exemplar-informed%20module%20to%0Aenhance%20the%20learning%20of%20similarities%20among%20the%20objects%20of%20the%20target%20category.%0ATo%20combine%20attended%20features%20from%20different%20modules%2C%20we%20incorporate%0Across-attention%20blocks%20followed%20by%20a%20feature%20fusion%20module.%20Experiments%0Aconducted%20on%20mainstream%20benchmarks%20demonstrate%20that%20our%20models%20achieve%20superior%0Aperformance%20compared%20to%20previous%20methods.%20Particularly%2C%20our%20model%20reduces%0Ausers%27%20labor%20by%20around%2015%5C%25%2C%20requiring%20two%20fewer%20clicks%20to%20achieve%20target%20IoUs%0A85%5C%25%20and%2090%5C%25.%20The%20results%20highlight%20our%20models%27%20potential%20as%20a%20flexible%20and%0Apractical%20annotation%20tool.%20The%20source%20code%20will%20be%20released%20after%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11472v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Exemplars%2520for%2520Interactive%2520Image%2520Segmentation%26entry.906535625%3DKun%2520Li%2520and%2520Hao%2520Cheng%2520and%2520George%2520Vosselman%2520and%2520Michael%2520Ying%2520Yang%26entry.1292438233%3D%2520%2520Interactive%2520image%2520segmentation%2520enables%2520users%2520to%2520interact%2520minimally%2520with%2520a%250Amachine%252C%2520facilitating%2520the%2520gradual%2520refinement%2520of%2520the%2520segmentation%2520mask%2520for%2520a%250Atarget%2520of%2520interest.%2520Previous%2520studies%2520have%2520demonstrated%2520impressive%2520performance%250Ain%2520extracting%2520a%2520single%2520target%2520mask%2520through%2520interactive%2520segmentation.%2520However%252C%250Athe%2520information%2520cues%2520of%2520previously%2520interacted%2520objects%2520have%2520been%2520overlooked%2520in%250Athe%2520existing%2520methods%252C%2520which%2520can%2520be%2520further%2520explored%2520to%2520speed%2520up%2520interactive%250Asegmentation%2520for%2520multiple%2520targets%2520in%2520the%2520same%2520category.%2520To%2520this%2520end%252C%2520we%250Aintroduce%2520novel%2520interactive%2520segmentation%2520frameworks%2520for%2520both%2520a%2520single%2520object%250Aand%2520multiple%2520objects%2520in%2520the%2520same%2520category.%2520Specifically%252C%2520our%2520model%2520leverages%250Atransformer%2520backbones%2520to%2520extract%2520interaction-focused%2520visual%2520features%2520from%2520the%250Aimage%2520and%2520the%2520interactions%2520to%2520obtain%2520a%2520satisfactory%2520mask%2520of%2520a%2520target%2520as%2520an%250Aexemplar.%2520For%2520multiple%2520objects%252C%2520we%2520propose%2520an%2520exemplar-informed%2520module%2520to%250Aenhance%2520the%2520learning%2520of%2520similarities%2520among%2520the%2520objects%2520of%2520the%2520target%2520category.%250ATo%2520combine%2520attended%2520features%2520from%2520different%2520modules%252C%2520we%2520incorporate%250Across-attention%2520blocks%2520followed%2520by%2520a%2520feature%2520fusion%2520module.%2520Experiments%250Aconducted%2520on%2520mainstream%2520benchmarks%2520demonstrate%2520that%2520our%2520models%2520achieve%2520superior%250Aperformance%2520compared%2520to%2520previous%2520methods.%2520Particularly%252C%2520our%2520model%2520reduces%250Ausers%2527%2520labor%2520by%2520around%252015%255C%2525%252C%2520requiring%2520two%2520fewer%2520clicks%2520to%2520achieve%2520target%2520IoUs%250A85%255C%2525%2520and%252090%255C%2525.%2520The%2520results%2520highlight%2520our%2520models%2527%2520potential%2520as%2520a%2520flexible%2520and%250Apractical%2520annotation%2520tool.%2520The%2520source%2520code%2520will%2520be%2520released%2520after%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11472v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Exemplars%20for%20Interactive%20Image%20Segmentation&entry.906535625=Kun%20Li%20and%20Hao%20Cheng%20and%20George%20Vosselman%20and%20Michael%20Ying%20Yang&entry.1292438233=%20%20Interactive%20image%20segmentation%20enables%20users%20to%20interact%20minimally%20with%20a%0Amachine%2C%20facilitating%20the%20gradual%20refinement%20of%20the%20segmentation%20mask%20for%20a%0Atarget%20of%20interest.%20Previous%20studies%20have%20demonstrated%20impressive%20performance%0Ain%20extracting%20a%20single%20target%20mask%20through%20interactive%20segmentation.%20However%2C%0Athe%20information%20cues%20of%20previously%20interacted%20objects%20have%20been%20overlooked%20in%0Athe%20existing%20methods%2C%20which%20can%20be%20further%20explored%20to%20speed%20up%20interactive%0Asegmentation%20for%20multiple%20targets%20in%20the%20same%20category.%20To%20this%20end%2C%20we%0Aintroduce%20novel%20interactive%20segmentation%20frameworks%20for%20both%20a%20single%20object%0Aand%20multiple%20objects%20in%20the%20same%20category.%20Specifically%2C%20our%20model%20leverages%0Atransformer%20backbones%20to%20extract%20interaction-focused%20visual%20features%20from%20the%0Aimage%20and%20the%20interactions%20to%20obtain%20a%20satisfactory%20mask%20of%20a%20target%20as%20an%0Aexemplar.%20For%20multiple%20objects%2C%20we%20propose%20an%20exemplar-informed%20module%20to%0Aenhance%20the%20learning%20of%20similarities%20among%20the%20objects%20of%20the%20target%20category.%0ATo%20combine%20attended%20features%20from%20different%20modules%2C%20we%20incorporate%0Across-attention%20blocks%20followed%20by%20a%20feature%20fusion%20module.%20Experiments%0Aconducted%20on%20mainstream%20benchmarks%20demonstrate%20that%20our%20models%20achieve%20superior%0Aperformance%20compared%20to%20previous%20methods.%20Particularly%2C%20our%20model%20reduces%0Ausers%27%20labor%20by%20around%2015%5C%25%2C%20requiring%20two%20fewer%20clicks%20to%20achieve%20target%20IoUs%0A85%5C%25%20and%2090%5C%25.%20The%20results%20highlight%20our%20models%27%20potential%20as%20a%20flexible%20and%0Apractical%20annotation%20tool.%20The%20source%20code%20will%20be%20released%20after%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11472v1&entry.124074799=Read"},
{"title": "Transcendence: Generative Models Can Outperform The Experts That Train\n  Them", "author": "Edwin Zhang and Vincent Zhu and Naomi Saphra and Anat Kleiman and Benjamin L. Edelman and Milind Tambe and Sham M. Kakade and Eran Malach", "abstract": "  Generative models are trained with the simple objective of imitating the\nconditional probability distribution induced by the data they are trained on.\nTherefore, when trained on data generated by humans, we may not expect the\nartificial model to outperform the humans on their original objectives. In this\nwork, we study the phenomenon of transcendence: when a generative model\nachieves capabilities that surpass the abilities of the experts generating its\ndata. We demonstrate transcendence by training an autoregressive transformer to\nplay chess from game transcripts, and show that the trained model can sometimes\nachieve better performance than all players in the dataset. We theoretically\nprove that transcendence is enabled by low-temperature sampling, and rigorously\nassess this experimentally. Finally, we discuss other sources of transcendence,\nlaying the groundwork for future investigation of this phenomenon in a broader\nsetting.\n", "link": "http://arxiv.org/abs/2406.11741v1", "date": "2024-06-17", "relevancy": 2.2934, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6039}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.598}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5364}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transcendence%3A%20Generative%20Models%20Can%20Outperform%20The%20Experts%20That%20Train%0A%20%20Them&body=Title%3A%20Transcendence%3A%20Generative%20Models%20Can%20Outperform%20The%20Experts%20That%20Train%0A%20%20Them%0AAuthor%3A%20Edwin%20Zhang%20and%20Vincent%20Zhu%20and%20Naomi%20Saphra%20and%20Anat%20Kleiman%20and%20Benjamin%20L.%20Edelman%20and%20Milind%20Tambe%20and%20Sham%20M.%20Kakade%20and%20Eran%20Malach%0AAbstract%3A%20%20%20Generative%20models%20are%20trained%20with%20the%20simple%20objective%20of%20imitating%20the%0Aconditional%20probability%20distribution%20induced%20by%20the%20data%20they%20are%20trained%20on.%0ATherefore%2C%20when%20trained%20on%20data%20generated%20by%20humans%2C%20we%20may%20not%20expect%20the%0Aartificial%20model%20to%20outperform%20the%20humans%20on%20their%20original%20objectives.%20In%20this%0Awork%2C%20we%20study%20the%20phenomenon%20of%20transcendence%3A%20when%20a%20generative%20model%0Aachieves%20capabilities%20that%20surpass%20the%20abilities%20of%20the%20experts%20generating%20its%0Adata.%20We%20demonstrate%20transcendence%20by%20training%20an%20autoregressive%20transformer%20to%0Aplay%20chess%20from%20game%20transcripts%2C%20and%20show%20that%20the%20trained%20model%20can%20sometimes%0Aachieve%20better%20performance%20than%20all%20players%20in%20the%20dataset.%20We%20theoretically%0Aprove%20that%20transcendence%20is%20enabled%20by%20low-temperature%20sampling%2C%20and%20rigorously%0Aassess%20this%20experimentally.%20Finally%2C%20we%20discuss%20other%20sources%20of%20transcendence%2C%0Alaying%20the%20groundwork%20for%20future%20investigation%20of%20this%20phenomenon%20in%20a%20broader%0Asetting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11741v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTranscendence%253A%2520Generative%2520Models%2520Can%2520Outperform%2520The%2520Experts%2520That%2520Train%250A%2520%2520Them%26entry.906535625%3DEdwin%2520Zhang%2520and%2520Vincent%2520Zhu%2520and%2520Naomi%2520Saphra%2520and%2520Anat%2520Kleiman%2520and%2520Benjamin%2520L.%2520Edelman%2520and%2520Milind%2520Tambe%2520and%2520Sham%2520M.%2520Kakade%2520and%2520Eran%2520Malach%26entry.1292438233%3D%2520%2520Generative%2520models%2520are%2520trained%2520with%2520the%2520simple%2520objective%2520of%2520imitating%2520the%250Aconditional%2520probability%2520distribution%2520induced%2520by%2520the%2520data%2520they%2520are%2520trained%2520on.%250ATherefore%252C%2520when%2520trained%2520on%2520data%2520generated%2520by%2520humans%252C%2520we%2520may%2520not%2520expect%2520the%250Aartificial%2520model%2520to%2520outperform%2520the%2520humans%2520on%2520their%2520original%2520objectives.%2520In%2520this%250Awork%252C%2520we%2520study%2520the%2520phenomenon%2520of%2520transcendence%253A%2520when%2520a%2520generative%2520model%250Aachieves%2520capabilities%2520that%2520surpass%2520the%2520abilities%2520of%2520the%2520experts%2520generating%2520its%250Adata.%2520We%2520demonstrate%2520transcendence%2520by%2520training%2520an%2520autoregressive%2520transformer%2520to%250Aplay%2520chess%2520from%2520game%2520transcripts%252C%2520and%2520show%2520that%2520the%2520trained%2520model%2520can%2520sometimes%250Aachieve%2520better%2520performance%2520than%2520all%2520players%2520in%2520the%2520dataset.%2520We%2520theoretically%250Aprove%2520that%2520transcendence%2520is%2520enabled%2520by%2520low-temperature%2520sampling%252C%2520and%2520rigorously%250Aassess%2520this%2520experimentally.%2520Finally%252C%2520we%2520discuss%2520other%2520sources%2520of%2520transcendence%252C%250Alaying%2520the%2520groundwork%2520for%2520future%2520investigation%2520of%2520this%2520phenomenon%2520in%2520a%2520broader%250Asetting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11741v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transcendence%3A%20Generative%20Models%20Can%20Outperform%20The%20Experts%20That%20Train%0A%20%20Them&entry.906535625=Edwin%20Zhang%20and%20Vincent%20Zhu%20and%20Naomi%20Saphra%20and%20Anat%20Kleiman%20and%20Benjamin%20L.%20Edelman%20and%20Milind%20Tambe%20and%20Sham%20M.%20Kakade%20and%20Eran%20Malach&entry.1292438233=%20%20Generative%20models%20are%20trained%20with%20the%20simple%20objective%20of%20imitating%20the%0Aconditional%20probability%20distribution%20induced%20by%20the%20data%20they%20are%20trained%20on.%0ATherefore%2C%20when%20trained%20on%20data%20generated%20by%20humans%2C%20we%20may%20not%20expect%20the%0Aartificial%20model%20to%20outperform%20the%20humans%20on%20their%20original%20objectives.%20In%20this%0Awork%2C%20we%20study%20the%20phenomenon%20of%20transcendence%3A%20when%20a%20generative%20model%0Aachieves%20capabilities%20that%20surpass%20the%20abilities%20of%20the%20experts%20generating%20its%0Adata.%20We%20demonstrate%20transcendence%20by%20training%20an%20autoregressive%20transformer%20to%0Aplay%20chess%20from%20game%20transcripts%2C%20and%20show%20that%20the%20trained%20model%20can%20sometimes%0Aachieve%20better%20performance%20than%20all%20players%20in%20the%20dataset.%20We%20theoretically%0Aprove%20that%20transcendence%20is%20enabled%20by%20low-temperature%20sampling%2C%20and%20rigorously%0Aassess%20this%20experimentally.%20Finally%2C%20we%20discuss%20other%20sources%20of%20transcendence%2C%0Alaying%20the%20groundwork%20for%20future%20investigation%20of%20this%20phenomenon%20in%20a%20broader%0Asetting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11741v1&entry.124074799=Read"},
{"title": "Multi-Camera Hand-Eye Calibration for Human-Robot Collaboration in\n  Industrial Robotic Workcells", "author": "Davide Allegro and Matteo Terreran and Stefano Ghidoni", "abstract": "  In industrial scenarios, effective human-robot collaboration relies on\nmulti-camera systems to robustly monitor human operators despite the occlusions\nthat typically show up in a robotic workcell. In this scenario, precise\nlocalization of the person in the robot coordinate system is essential, making\nthe hand-eye calibration of the camera network critical. This process presents\nsignificant challenges when high calibration accuracy should be achieved in\nshort time to minimize production downtime, and when dealing with extensive\ncamera networks used for monitoring wide areas, such as industrial robotic\nworkcells. Our paper introduces an innovative and robust multi-camera hand-eye\ncalibration method, designed to optimize each camera's pose relative to both\nthe robot's base and to each other camera. This optimization integrates two\ntypes of key constraints: i) a single board-to-end-effector transformation, and\nii) the relative camera-to-camera transformations. We demonstrate the superior\nperformance of our method through comprehensive experiments employing the\nMETRIC dataset and real-world data collected on industrial scenarios, showing\nnotable advancements over state-of-the-art techniques even using less than 10\nimages. Additionally, we release an open-source version of our multi-camera\nhand-eye calibration algorithm at\nhttps://github.com/davidea97/Multi-Camera-Hand-Eye-Calibration.git.\n", "link": "http://arxiv.org/abs/2406.11392v1", "date": "2024-06-17", "relevancy": 2.2863, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5872}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5675}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Camera%20Hand-Eye%20Calibration%20for%20Human-Robot%20Collaboration%20in%0A%20%20Industrial%20Robotic%20Workcells&body=Title%3A%20Multi-Camera%20Hand-Eye%20Calibration%20for%20Human-Robot%20Collaboration%20in%0A%20%20Industrial%20Robotic%20Workcells%0AAuthor%3A%20Davide%20Allegro%20and%20Matteo%20Terreran%20and%20Stefano%20Ghidoni%0AAbstract%3A%20%20%20In%20industrial%20scenarios%2C%20effective%20human-robot%20collaboration%20relies%20on%0Amulti-camera%20systems%20to%20robustly%20monitor%20human%20operators%20despite%20the%20occlusions%0Athat%20typically%20show%20up%20in%20a%20robotic%20workcell.%20In%20this%20scenario%2C%20precise%0Alocalization%20of%20the%20person%20in%20the%20robot%20coordinate%20system%20is%20essential%2C%20making%0Athe%20hand-eye%20calibration%20of%20the%20camera%20network%20critical.%20This%20process%20presents%0Asignificant%20challenges%20when%20high%20calibration%20accuracy%20should%20be%20achieved%20in%0Ashort%20time%20to%20minimize%20production%20downtime%2C%20and%20when%20dealing%20with%20extensive%0Acamera%20networks%20used%20for%20monitoring%20wide%20areas%2C%20such%20as%20industrial%20robotic%0Aworkcells.%20Our%20paper%20introduces%20an%20innovative%20and%20robust%20multi-camera%20hand-eye%0Acalibration%20method%2C%20designed%20to%20optimize%20each%20camera%27s%20pose%20relative%20to%20both%0Athe%20robot%27s%20base%20and%20to%20each%20other%20camera.%20This%20optimization%20integrates%20two%0Atypes%20of%20key%20constraints%3A%20i%29%20a%20single%20board-to-end-effector%20transformation%2C%20and%0Aii%29%20the%20relative%20camera-to-camera%20transformations.%20We%20demonstrate%20the%20superior%0Aperformance%20of%20our%20method%20through%20comprehensive%20experiments%20employing%20the%0AMETRIC%20dataset%20and%20real-world%20data%20collected%20on%20industrial%20scenarios%2C%20showing%0Anotable%20advancements%20over%20state-of-the-art%20techniques%20even%20using%20less%20than%2010%0Aimages.%20Additionally%2C%20we%20release%20an%20open-source%20version%20of%20our%20multi-camera%0Ahand-eye%20calibration%20algorithm%20at%0Ahttps%3A//github.com/davidea97/Multi-Camera-Hand-Eye-Calibration.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11392v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Camera%2520Hand-Eye%2520Calibration%2520for%2520Human-Robot%2520Collaboration%2520in%250A%2520%2520Industrial%2520Robotic%2520Workcells%26entry.906535625%3DDavide%2520Allegro%2520and%2520Matteo%2520Terreran%2520and%2520Stefano%2520Ghidoni%26entry.1292438233%3D%2520%2520In%2520industrial%2520scenarios%252C%2520effective%2520human-robot%2520collaboration%2520relies%2520on%250Amulti-camera%2520systems%2520to%2520robustly%2520monitor%2520human%2520operators%2520despite%2520the%2520occlusions%250Athat%2520typically%2520show%2520up%2520in%2520a%2520robotic%2520workcell.%2520In%2520this%2520scenario%252C%2520precise%250Alocalization%2520of%2520the%2520person%2520in%2520the%2520robot%2520coordinate%2520system%2520is%2520essential%252C%2520making%250Athe%2520hand-eye%2520calibration%2520of%2520the%2520camera%2520network%2520critical.%2520This%2520process%2520presents%250Asignificant%2520challenges%2520when%2520high%2520calibration%2520accuracy%2520should%2520be%2520achieved%2520in%250Ashort%2520time%2520to%2520minimize%2520production%2520downtime%252C%2520and%2520when%2520dealing%2520with%2520extensive%250Acamera%2520networks%2520used%2520for%2520monitoring%2520wide%2520areas%252C%2520such%2520as%2520industrial%2520robotic%250Aworkcells.%2520Our%2520paper%2520introduces%2520an%2520innovative%2520and%2520robust%2520multi-camera%2520hand-eye%250Acalibration%2520method%252C%2520designed%2520to%2520optimize%2520each%2520camera%2527s%2520pose%2520relative%2520to%2520both%250Athe%2520robot%2527s%2520base%2520and%2520to%2520each%2520other%2520camera.%2520This%2520optimization%2520integrates%2520two%250Atypes%2520of%2520key%2520constraints%253A%2520i%2529%2520a%2520single%2520board-to-end-effector%2520transformation%252C%2520and%250Aii%2529%2520the%2520relative%2520camera-to-camera%2520transformations.%2520We%2520demonstrate%2520the%2520superior%250Aperformance%2520of%2520our%2520method%2520through%2520comprehensive%2520experiments%2520employing%2520the%250AMETRIC%2520dataset%2520and%2520real-world%2520data%2520collected%2520on%2520industrial%2520scenarios%252C%2520showing%250Anotable%2520advancements%2520over%2520state-of-the-art%2520techniques%2520even%2520using%2520less%2520than%252010%250Aimages.%2520Additionally%252C%2520we%2520release%2520an%2520open-source%2520version%2520of%2520our%2520multi-camera%250Ahand-eye%2520calibration%2520algorithm%2520at%250Ahttps%253A//github.com/davidea97/Multi-Camera-Hand-Eye-Calibration.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11392v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Camera%20Hand-Eye%20Calibration%20for%20Human-Robot%20Collaboration%20in%0A%20%20Industrial%20Robotic%20Workcells&entry.906535625=Davide%20Allegro%20and%20Matteo%20Terreran%20and%20Stefano%20Ghidoni&entry.1292438233=%20%20In%20industrial%20scenarios%2C%20effective%20human-robot%20collaboration%20relies%20on%0Amulti-camera%20systems%20to%20robustly%20monitor%20human%20operators%20despite%20the%20occlusions%0Athat%20typically%20show%20up%20in%20a%20robotic%20workcell.%20In%20this%20scenario%2C%20precise%0Alocalization%20of%20the%20person%20in%20the%20robot%20coordinate%20system%20is%20essential%2C%20making%0Athe%20hand-eye%20calibration%20of%20the%20camera%20network%20critical.%20This%20process%20presents%0Asignificant%20challenges%20when%20high%20calibration%20accuracy%20should%20be%20achieved%20in%0Ashort%20time%20to%20minimize%20production%20downtime%2C%20and%20when%20dealing%20with%20extensive%0Acamera%20networks%20used%20for%20monitoring%20wide%20areas%2C%20such%20as%20industrial%20robotic%0Aworkcells.%20Our%20paper%20introduces%20an%20innovative%20and%20robust%20multi-camera%20hand-eye%0Acalibration%20method%2C%20designed%20to%20optimize%20each%20camera%27s%20pose%20relative%20to%20both%0Athe%20robot%27s%20base%20and%20to%20each%20other%20camera.%20This%20optimization%20integrates%20two%0Atypes%20of%20key%20constraints%3A%20i%29%20a%20single%20board-to-end-effector%20transformation%2C%20and%0Aii%29%20the%20relative%20camera-to-camera%20transformations.%20We%20demonstrate%20the%20superior%0Aperformance%20of%20our%20method%20through%20comprehensive%20experiments%20employing%20the%0AMETRIC%20dataset%20and%20real-world%20data%20collected%20on%20industrial%20scenarios%2C%20showing%0Anotable%20advancements%20over%20state-of-the-art%20techniques%20even%20using%20less%20than%2010%0Aimages.%20Additionally%2C%20we%20release%20an%20open-source%20version%20of%20our%20multi-camera%0Ahand-eye%20calibration%20algorithm%20at%0Ahttps%3A//github.com/davidea97/Multi-Camera-Hand-Eye-Calibration.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11392v1&entry.124074799=Read"},
{"title": "Unfolding Time: Generative Modeling for Turbulent Flows in 4D", "author": "Abdullah Saydemir and Marten Lienen and Stephan G\u00fcnnemann", "abstract": "  A recent study in turbulent flow simulation demonstrated the potential of\ngenerative diffusion models for fast 3D surrogate modeling. This approach\neliminates the need for specifying initial states or performing lengthy\nsimulations, significantly accelerating the process. While adept at sampling\nindividual frames from the learned manifold of turbulent flow states, the\nprevious model lacks the capability to generate sequences, hindering analysis\nof dynamic phenomena. This work addresses this limitation by introducing a 4D\ngenerative diffusion model and a physics-informed guidance technique that\nenables the generation of realistic sequences of flow states. Our findings\nindicate that the proposed method can successfully sample entire subsequences\nfrom the turbulent manifold, even though generalizing from individual frames to\nsequences remains a challenging task. This advancement opens doors for the\napplication of generative modeling in analyzing the temporal evolution of\nturbulent flows, providing valuable insights into their complex dynamics.\n", "link": "http://arxiv.org/abs/2406.11390v1", "date": "2024-06-17", "relevancy": 2.2788, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6263}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5584}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unfolding%20Time%3A%20Generative%20Modeling%20for%20Turbulent%20Flows%20in%204D&body=Title%3A%20Unfolding%20Time%3A%20Generative%20Modeling%20for%20Turbulent%20Flows%20in%204D%0AAuthor%3A%20Abdullah%20Saydemir%20and%20Marten%20Lienen%20and%20Stephan%20G%C3%BCnnemann%0AAbstract%3A%20%20%20A%20recent%20study%20in%20turbulent%20flow%20simulation%20demonstrated%20the%20potential%20of%0Agenerative%20diffusion%20models%20for%20fast%203D%20surrogate%20modeling.%20This%20approach%0Aeliminates%20the%20need%20for%20specifying%20initial%20states%20or%20performing%20lengthy%0Asimulations%2C%20significantly%20accelerating%20the%20process.%20While%20adept%20at%20sampling%0Aindividual%20frames%20from%20the%20learned%20manifold%20of%20turbulent%20flow%20states%2C%20the%0Aprevious%20model%20lacks%20the%20capability%20to%20generate%20sequences%2C%20hindering%20analysis%0Aof%20dynamic%20phenomena.%20This%20work%20addresses%20this%20limitation%20by%20introducing%20a%204D%0Agenerative%20diffusion%20model%20and%20a%20physics-informed%20guidance%20technique%20that%0Aenables%20the%20generation%20of%20realistic%20sequences%20of%20flow%20states.%20Our%20findings%0Aindicate%20that%20the%20proposed%20method%20can%20successfully%20sample%20entire%20subsequences%0Afrom%20the%20turbulent%20manifold%2C%20even%20though%20generalizing%20from%20individual%20frames%20to%0Asequences%20remains%20a%20challenging%20task.%20This%20advancement%20opens%20doors%20for%20the%0Aapplication%20of%20generative%20modeling%20in%20analyzing%20the%20temporal%20evolution%20of%0Aturbulent%20flows%2C%20providing%20valuable%20insights%20into%20their%20complex%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11390v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnfolding%2520Time%253A%2520Generative%2520Modeling%2520for%2520Turbulent%2520Flows%2520in%25204D%26entry.906535625%3DAbdullah%2520Saydemir%2520and%2520Marten%2520Lienen%2520and%2520Stephan%2520G%25C3%25BCnnemann%26entry.1292438233%3D%2520%2520A%2520recent%2520study%2520in%2520turbulent%2520flow%2520simulation%2520demonstrated%2520the%2520potential%2520of%250Agenerative%2520diffusion%2520models%2520for%2520fast%25203D%2520surrogate%2520modeling.%2520This%2520approach%250Aeliminates%2520the%2520need%2520for%2520specifying%2520initial%2520states%2520or%2520performing%2520lengthy%250Asimulations%252C%2520significantly%2520accelerating%2520the%2520process.%2520While%2520adept%2520at%2520sampling%250Aindividual%2520frames%2520from%2520the%2520learned%2520manifold%2520of%2520turbulent%2520flow%2520states%252C%2520the%250Aprevious%2520model%2520lacks%2520the%2520capability%2520to%2520generate%2520sequences%252C%2520hindering%2520analysis%250Aof%2520dynamic%2520phenomena.%2520This%2520work%2520addresses%2520this%2520limitation%2520by%2520introducing%2520a%25204D%250Agenerative%2520diffusion%2520model%2520and%2520a%2520physics-informed%2520guidance%2520technique%2520that%250Aenables%2520the%2520generation%2520of%2520realistic%2520sequences%2520of%2520flow%2520states.%2520Our%2520findings%250Aindicate%2520that%2520the%2520proposed%2520method%2520can%2520successfully%2520sample%2520entire%2520subsequences%250Afrom%2520the%2520turbulent%2520manifold%252C%2520even%2520though%2520generalizing%2520from%2520individual%2520frames%2520to%250Asequences%2520remains%2520a%2520challenging%2520task.%2520This%2520advancement%2520opens%2520doors%2520for%2520the%250Aapplication%2520of%2520generative%2520modeling%2520in%2520analyzing%2520the%2520temporal%2520evolution%2520of%250Aturbulent%2520flows%252C%2520providing%2520valuable%2520insights%2520into%2520their%2520complex%2520dynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11390v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unfolding%20Time%3A%20Generative%20Modeling%20for%20Turbulent%20Flows%20in%204D&entry.906535625=Abdullah%20Saydemir%20and%20Marten%20Lienen%20and%20Stephan%20G%C3%BCnnemann&entry.1292438233=%20%20A%20recent%20study%20in%20turbulent%20flow%20simulation%20demonstrated%20the%20potential%20of%0Agenerative%20diffusion%20models%20for%20fast%203D%20surrogate%20modeling.%20This%20approach%0Aeliminates%20the%20need%20for%20specifying%20initial%20states%20or%20performing%20lengthy%0Asimulations%2C%20significantly%20accelerating%20the%20process.%20While%20adept%20at%20sampling%0Aindividual%20frames%20from%20the%20learned%20manifold%20of%20turbulent%20flow%20states%2C%20the%0Aprevious%20model%20lacks%20the%20capability%20to%20generate%20sequences%2C%20hindering%20analysis%0Aof%20dynamic%20phenomena.%20This%20work%20addresses%20this%20limitation%20by%20introducing%20a%204D%0Agenerative%20diffusion%20model%20and%20a%20physics-informed%20guidance%20technique%20that%0Aenables%20the%20generation%20of%20realistic%20sequences%20of%20flow%20states.%20Our%20findings%0Aindicate%20that%20the%20proposed%20method%20can%20successfully%20sample%20entire%20subsequences%0Afrom%20the%20turbulent%20manifold%2C%20even%20though%20generalizing%20from%20individual%20frames%20to%0Asequences%20remains%20a%20challenging%20task.%20This%20advancement%20opens%20doors%20for%20the%0Aapplication%20of%20generative%20modeling%20in%20analyzing%20the%20temporal%20evolution%20of%0Aturbulent%20flows%2C%20providing%20valuable%20insights%20into%20their%20complex%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11390v1&entry.124074799=Read"},
{"title": "GRID-FAST: A Grid-based Intersection Detection for Fast Semantic\n  Topometric Mapping", "author": "Scott Fredriksson and Akshit Saradagi and George Nikolakopoulos", "abstract": "  This article introduces a novel approach to constructing a topometric map\nthat allows for efficient navigation and decision-making in mobile robotics\napplications. The method generates the topometric map from a 2D grid-based map.\nThe topometric map segments areas of the input map into different\nstructural-semantic classes: intersections, pathways, dead ends, and pathways\nleading to unexplored areas. This method is grounded in a new technique for\nintersection detection that identifies the area and the openings of\nintersections in a semantically meaningful way. The framework introduces two\nlevels of pre-filtering with minimal computational cost to eliminate small\nopenings and objects from the map which are unimportant in the context of\nhigh-level map segmentation and decision making. The topological map generated\nby GRID-FAST enables fast navigation in large-scale environments, and the\nstructural semantics can aid in mission planning, autonomous exploration, and\nhuman-to-robot cooperation. The efficacy of the proposed method is demonstrated\nthrough validation on real maps gathered from robotic experiments: 1) a\nstructured indoor environment, 2) an unstructured cave-like subterranean\nenvironment, and 3) a large-scale outdoor environment, which comprises\npathways, buildings, and scattered objects. Additionally, the proposed\nframework has been compared with state-of-the-art topological mapping solutions\nand is able to produce a topometric and topological map with up to \\blue92%\nfewer nodes than the next best solution.\n", "link": "http://arxiv.org/abs/2406.11635v1", "date": "2024-06-17", "relevancy": 2.2506, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5756}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5586}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRID-FAST%3A%20A%20Grid-based%20Intersection%20Detection%20for%20Fast%20Semantic%0A%20%20Topometric%20Mapping&body=Title%3A%20GRID-FAST%3A%20A%20Grid-based%20Intersection%20Detection%20for%20Fast%20Semantic%0A%20%20Topometric%20Mapping%0AAuthor%3A%20Scott%20Fredriksson%20and%20Akshit%20Saradagi%20and%20George%20Nikolakopoulos%0AAbstract%3A%20%20%20This%20article%20introduces%20a%20novel%20approach%20to%20constructing%20a%20topometric%20map%0Athat%20allows%20for%20efficient%20navigation%20and%20decision-making%20in%20mobile%20robotics%0Aapplications.%20The%20method%20generates%20the%20topometric%20map%20from%20a%202D%20grid-based%20map.%0AThe%20topometric%20map%20segments%20areas%20of%20the%20input%20map%20into%20different%0Astructural-semantic%20classes%3A%20intersections%2C%20pathways%2C%20dead%20ends%2C%20and%20pathways%0Aleading%20to%20unexplored%20areas.%20This%20method%20is%20grounded%20in%20a%20new%20technique%20for%0Aintersection%20detection%20that%20identifies%20the%20area%20and%20the%20openings%20of%0Aintersections%20in%20a%20semantically%20meaningful%20way.%20The%20framework%20introduces%20two%0Alevels%20of%20pre-filtering%20with%20minimal%20computational%20cost%20to%20eliminate%20small%0Aopenings%20and%20objects%20from%20the%20map%20which%20are%20unimportant%20in%20the%20context%20of%0Ahigh-level%20map%20segmentation%20and%20decision%20making.%20The%20topological%20map%20generated%0Aby%20GRID-FAST%20enables%20fast%20navigation%20in%20large-scale%20environments%2C%20and%20the%0Astructural%20semantics%20can%20aid%20in%20mission%20planning%2C%20autonomous%20exploration%2C%20and%0Ahuman-to-robot%20cooperation.%20The%20efficacy%20of%20the%20proposed%20method%20is%20demonstrated%0Athrough%20validation%20on%20real%20maps%20gathered%20from%20robotic%20experiments%3A%201%29%20a%0Astructured%20indoor%20environment%2C%202%29%20an%20unstructured%20cave-like%20subterranean%0Aenvironment%2C%20and%203%29%20a%20large-scale%20outdoor%20environment%2C%20which%20comprises%0Apathways%2C%20buildings%2C%20and%20scattered%20objects.%20Additionally%2C%20the%20proposed%0Aframework%20has%20been%20compared%20with%20state-of-the-art%20topological%20mapping%20solutions%0Aand%20is%20able%20to%20produce%20a%20topometric%20and%20topological%20map%20with%20up%20to%20%5Cblue92%25%0Afewer%20nodes%20than%20the%20next%20best%20solution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11635v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRID-FAST%253A%2520A%2520Grid-based%2520Intersection%2520Detection%2520for%2520Fast%2520Semantic%250A%2520%2520Topometric%2520Mapping%26entry.906535625%3DScott%2520Fredriksson%2520and%2520Akshit%2520Saradagi%2520and%2520George%2520Nikolakopoulos%26entry.1292438233%3D%2520%2520This%2520article%2520introduces%2520a%2520novel%2520approach%2520to%2520constructing%2520a%2520topometric%2520map%250Athat%2520allows%2520for%2520efficient%2520navigation%2520and%2520decision-making%2520in%2520mobile%2520robotics%250Aapplications.%2520The%2520method%2520generates%2520the%2520topometric%2520map%2520from%2520a%25202D%2520grid-based%2520map.%250AThe%2520topometric%2520map%2520segments%2520areas%2520of%2520the%2520input%2520map%2520into%2520different%250Astructural-semantic%2520classes%253A%2520intersections%252C%2520pathways%252C%2520dead%2520ends%252C%2520and%2520pathways%250Aleading%2520to%2520unexplored%2520areas.%2520This%2520method%2520is%2520grounded%2520in%2520a%2520new%2520technique%2520for%250Aintersection%2520detection%2520that%2520identifies%2520the%2520area%2520and%2520the%2520openings%2520of%250Aintersections%2520in%2520a%2520semantically%2520meaningful%2520way.%2520The%2520framework%2520introduces%2520two%250Alevels%2520of%2520pre-filtering%2520with%2520minimal%2520computational%2520cost%2520to%2520eliminate%2520small%250Aopenings%2520and%2520objects%2520from%2520the%2520map%2520which%2520are%2520unimportant%2520in%2520the%2520context%2520of%250Ahigh-level%2520map%2520segmentation%2520and%2520decision%2520making.%2520The%2520topological%2520map%2520generated%250Aby%2520GRID-FAST%2520enables%2520fast%2520navigation%2520in%2520large-scale%2520environments%252C%2520and%2520the%250Astructural%2520semantics%2520can%2520aid%2520in%2520mission%2520planning%252C%2520autonomous%2520exploration%252C%2520and%250Ahuman-to-robot%2520cooperation.%2520The%2520efficacy%2520of%2520the%2520proposed%2520method%2520is%2520demonstrated%250Athrough%2520validation%2520on%2520real%2520maps%2520gathered%2520from%2520robotic%2520experiments%253A%25201%2529%2520a%250Astructured%2520indoor%2520environment%252C%25202%2529%2520an%2520unstructured%2520cave-like%2520subterranean%250Aenvironment%252C%2520and%25203%2529%2520a%2520large-scale%2520outdoor%2520environment%252C%2520which%2520comprises%250Apathways%252C%2520buildings%252C%2520and%2520scattered%2520objects.%2520Additionally%252C%2520the%2520proposed%250Aframework%2520has%2520been%2520compared%2520with%2520state-of-the-art%2520topological%2520mapping%2520solutions%250Aand%2520is%2520able%2520to%2520produce%2520a%2520topometric%2520and%2520topological%2520map%2520with%2520up%2520to%2520%255Cblue92%2525%250Afewer%2520nodes%2520than%2520the%2520next%2520best%2520solution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11635v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRID-FAST%3A%20A%20Grid-based%20Intersection%20Detection%20for%20Fast%20Semantic%0A%20%20Topometric%20Mapping&entry.906535625=Scott%20Fredriksson%20and%20Akshit%20Saradagi%20and%20George%20Nikolakopoulos&entry.1292438233=%20%20This%20article%20introduces%20a%20novel%20approach%20to%20constructing%20a%20topometric%20map%0Athat%20allows%20for%20efficient%20navigation%20and%20decision-making%20in%20mobile%20robotics%0Aapplications.%20The%20method%20generates%20the%20topometric%20map%20from%20a%202D%20grid-based%20map.%0AThe%20topometric%20map%20segments%20areas%20of%20the%20input%20map%20into%20different%0Astructural-semantic%20classes%3A%20intersections%2C%20pathways%2C%20dead%20ends%2C%20and%20pathways%0Aleading%20to%20unexplored%20areas.%20This%20method%20is%20grounded%20in%20a%20new%20technique%20for%0Aintersection%20detection%20that%20identifies%20the%20area%20and%20the%20openings%20of%0Aintersections%20in%20a%20semantically%20meaningful%20way.%20The%20framework%20introduces%20two%0Alevels%20of%20pre-filtering%20with%20minimal%20computational%20cost%20to%20eliminate%20small%0Aopenings%20and%20objects%20from%20the%20map%20which%20are%20unimportant%20in%20the%20context%20of%0Ahigh-level%20map%20segmentation%20and%20decision%20making.%20The%20topological%20map%20generated%0Aby%20GRID-FAST%20enables%20fast%20navigation%20in%20large-scale%20environments%2C%20and%20the%0Astructural%20semantics%20can%20aid%20in%20mission%20planning%2C%20autonomous%20exploration%2C%20and%0Ahuman-to-robot%20cooperation.%20The%20efficacy%20of%20the%20proposed%20method%20is%20demonstrated%0Athrough%20validation%20on%20real%20maps%20gathered%20from%20robotic%20experiments%3A%201%29%20a%0Astructured%20indoor%20environment%2C%202%29%20an%20unstructured%20cave-like%20subterranean%0Aenvironment%2C%20and%203%29%20a%20large-scale%20outdoor%20environment%2C%20which%20comprises%0Apathways%2C%20buildings%2C%20and%20scattered%20objects.%20Additionally%2C%20the%20proposed%0Aframework%20has%20been%20compared%20with%20state-of-the-art%20topological%20mapping%20solutions%0Aand%20is%20able%20to%20produce%20a%20topometric%20and%20topological%20map%20with%20up%20to%20%5Cblue92%25%0Afewer%20nodes%20than%20the%20next%20best%20solution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11635v1&entry.124074799=Read"},
{"title": "On GNN explanability with activation rules", "author": "Luca Veyrin-Forrer and Ataollah Kamal and Stefan Duffner and Marc Plantevit and C\u00e9line Robardet", "abstract": "  GNNs are powerful models based on node representation learning that perform\nparticularly well in many machine learning problems related to graphs. The\nmajor obstacle to the deployment of GNNs is mostly a problem of societal\nacceptability and trustworthiness, properties which require making explicit the\ninternal functioning of such models. Here, we propose to mine activation rules\nin the hidden layers to understand how the GNNs perceive the world. The problem\nis not to discover activation rules that are individually highly discriminating\nfor an output of the model. Instead, the challenge is to provide a small set of\nrules that cover all input graphs. To this end, we introduce the subjective\nactivation pattern domain. We define an effective and principled algorithm to\nenumerate activations rules in each hidden layer. The proposed approach for\nquantifying the interest of these rules is rooted in information theory and is\nable to account for background knowledge on the input graph data. The\nactivation rules can then be redescribed thanks to pattern languages involving\ninterpretable features. We show that the activation rules provide insights on\nthe characteristics used by the GNN to classify the graphs. Especially, this\nallows to identify the hidden features built by the GNN through its different\nlayers. Also, these rules can subsequently be used for explaining GNN\ndecisions. Experiments on both synthetic and real-life datasets show highly\ncompetitive performance, with up to 200% improvement in fidelity on explaining\ngraph classification over the SOTA methods.\n", "link": "http://arxiv.org/abs/2406.11594v1", "date": "2024-06-17", "relevancy": 2.2469, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4613}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4472}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20GNN%20explanability%20with%20activation%20rules&body=Title%3A%20On%20GNN%20explanability%20with%20activation%20rules%0AAuthor%3A%20Luca%20Veyrin-Forrer%20and%20Ataollah%20Kamal%20and%20Stefan%20Duffner%20and%20Marc%20Plantevit%20and%20C%C3%A9line%20Robardet%0AAbstract%3A%20%20%20GNNs%20are%20powerful%20models%20based%20on%20node%20representation%20learning%20that%20perform%0Aparticularly%20well%20in%20many%20machine%20learning%20problems%20related%20to%20graphs.%20The%0Amajor%20obstacle%20to%20the%20deployment%20of%20GNNs%20is%20mostly%20a%20problem%20of%20societal%0Aacceptability%20and%20trustworthiness%2C%20properties%20which%20require%20making%20explicit%20the%0Ainternal%20functioning%20of%20such%20models.%20Here%2C%20we%20propose%20to%20mine%20activation%20rules%0Ain%20the%20hidden%20layers%20to%20understand%20how%20the%20GNNs%20perceive%20the%20world.%20The%20problem%0Ais%20not%20to%20discover%20activation%20rules%20that%20are%20individually%20highly%20discriminating%0Afor%20an%20output%20of%20the%20model.%20Instead%2C%20the%20challenge%20is%20to%20provide%20a%20small%20set%20of%0Arules%20that%20cover%20all%20input%20graphs.%20To%20this%20end%2C%20we%20introduce%20the%20subjective%0Aactivation%20pattern%20domain.%20We%20define%20an%20effective%20and%20principled%20algorithm%20to%0Aenumerate%20activations%20rules%20in%20each%20hidden%20layer.%20The%20proposed%20approach%20for%0Aquantifying%20the%20interest%20of%20these%20rules%20is%20rooted%20in%20information%20theory%20and%20is%0Aable%20to%20account%20for%20background%20knowledge%20on%20the%20input%20graph%20data.%20The%0Aactivation%20rules%20can%20then%20be%20redescribed%20thanks%20to%20pattern%20languages%20involving%0Ainterpretable%20features.%20We%20show%20that%20the%20activation%20rules%20provide%20insights%20on%0Athe%20characteristics%20used%20by%20the%20GNN%20to%20classify%20the%20graphs.%20Especially%2C%20this%0Aallows%20to%20identify%20the%20hidden%20features%20built%20by%20the%20GNN%20through%20its%20different%0Alayers.%20Also%2C%20these%20rules%20can%20subsequently%20be%20used%20for%20explaining%20GNN%0Adecisions.%20Experiments%20on%20both%20synthetic%20and%20real-life%20datasets%20show%20highly%0Acompetitive%20performance%2C%20with%20up%20to%20200%25%20improvement%20in%20fidelity%20on%20explaining%0Agraph%20classification%20over%20the%20SOTA%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11594v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520GNN%2520explanability%2520with%2520activation%2520rules%26entry.906535625%3DLuca%2520Veyrin-Forrer%2520and%2520Ataollah%2520Kamal%2520and%2520Stefan%2520Duffner%2520and%2520Marc%2520Plantevit%2520and%2520C%25C3%25A9line%2520Robardet%26entry.1292438233%3D%2520%2520GNNs%2520are%2520powerful%2520models%2520based%2520on%2520node%2520representation%2520learning%2520that%2520perform%250Aparticularly%2520well%2520in%2520many%2520machine%2520learning%2520problems%2520related%2520to%2520graphs.%2520The%250Amajor%2520obstacle%2520to%2520the%2520deployment%2520of%2520GNNs%2520is%2520mostly%2520a%2520problem%2520of%2520societal%250Aacceptability%2520and%2520trustworthiness%252C%2520properties%2520which%2520require%2520making%2520explicit%2520the%250Ainternal%2520functioning%2520of%2520such%2520models.%2520Here%252C%2520we%2520propose%2520to%2520mine%2520activation%2520rules%250Ain%2520the%2520hidden%2520layers%2520to%2520understand%2520how%2520the%2520GNNs%2520perceive%2520the%2520world.%2520The%2520problem%250Ais%2520not%2520to%2520discover%2520activation%2520rules%2520that%2520are%2520individually%2520highly%2520discriminating%250Afor%2520an%2520output%2520of%2520the%2520model.%2520Instead%252C%2520the%2520challenge%2520is%2520to%2520provide%2520a%2520small%2520set%2520of%250Arules%2520that%2520cover%2520all%2520input%2520graphs.%2520To%2520this%2520end%252C%2520we%2520introduce%2520the%2520subjective%250Aactivation%2520pattern%2520domain.%2520We%2520define%2520an%2520effective%2520and%2520principled%2520algorithm%2520to%250Aenumerate%2520activations%2520rules%2520in%2520each%2520hidden%2520layer.%2520The%2520proposed%2520approach%2520for%250Aquantifying%2520the%2520interest%2520of%2520these%2520rules%2520is%2520rooted%2520in%2520information%2520theory%2520and%2520is%250Aable%2520to%2520account%2520for%2520background%2520knowledge%2520on%2520the%2520input%2520graph%2520data.%2520The%250Aactivation%2520rules%2520can%2520then%2520be%2520redescribed%2520thanks%2520to%2520pattern%2520languages%2520involving%250Ainterpretable%2520features.%2520We%2520show%2520that%2520the%2520activation%2520rules%2520provide%2520insights%2520on%250Athe%2520characteristics%2520used%2520by%2520the%2520GNN%2520to%2520classify%2520the%2520graphs.%2520Especially%252C%2520this%250Aallows%2520to%2520identify%2520the%2520hidden%2520features%2520built%2520by%2520the%2520GNN%2520through%2520its%2520different%250Alayers.%2520Also%252C%2520these%2520rules%2520can%2520subsequently%2520be%2520used%2520for%2520explaining%2520GNN%250Adecisions.%2520Experiments%2520on%2520both%2520synthetic%2520and%2520real-life%2520datasets%2520show%2520highly%250Acompetitive%2520performance%252C%2520with%2520up%2520to%2520200%2525%2520improvement%2520in%2520fidelity%2520on%2520explaining%250Agraph%2520classification%2520over%2520the%2520SOTA%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11594v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20GNN%20explanability%20with%20activation%20rules&entry.906535625=Luca%20Veyrin-Forrer%20and%20Ataollah%20Kamal%20and%20Stefan%20Duffner%20and%20Marc%20Plantevit%20and%20C%C3%A9line%20Robardet&entry.1292438233=%20%20GNNs%20are%20powerful%20models%20based%20on%20node%20representation%20learning%20that%20perform%0Aparticularly%20well%20in%20many%20machine%20learning%20problems%20related%20to%20graphs.%20The%0Amajor%20obstacle%20to%20the%20deployment%20of%20GNNs%20is%20mostly%20a%20problem%20of%20societal%0Aacceptability%20and%20trustworthiness%2C%20properties%20which%20require%20making%20explicit%20the%0Ainternal%20functioning%20of%20such%20models.%20Here%2C%20we%20propose%20to%20mine%20activation%20rules%0Ain%20the%20hidden%20layers%20to%20understand%20how%20the%20GNNs%20perceive%20the%20world.%20The%20problem%0Ais%20not%20to%20discover%20activation%20rules%20that%20are%20individually%20highly%20discriminating%0Afor%20an%20output%20of%20the%20model.%20Instead%2C%20the%20challenge%20is%20to%20provide%20a%20small%20set%20of%0Arules%20that%20cover%20all%20input%20graphs.%20To%20this%20end%2C%20we%20introduce%20the%20subjective%0Aactivation%20pattern%20domain.%20We%20define%20an%20effective%20and%20principled%20algorithm%20to%0Aenumerate%20activations%20rules%20in%20each%20hidden%20layer.%20The%20proposed%20approach%20for%0Aquantifying%20the%20interest%20of%20these%20rules%20is%20rooted%20in%20information%20theory%20and%20is%0Aable%20to%20account%20for%20background%20knowledge%20on%20the%20input%20graph%20data.%20The%0Aactivation%20rules%20can%20then%20be%20redescribed%20thanks%20to%20pattern%20languages%20involving%0Ainterpretable%20features.%20We%20show%20that%20the%20activation%20rules%20provide%20insights%20on%0Athe%20characteristics%20used%20by%20the%20GNN%20to%20classify%20the%20graphs.%20Especially%2C%20this%0Aallows%20to%20identify%20the%20hidden%20features%20built%20by%20the%20GNN%20through%20its%20different%0Alayers.%20Also%2C%20these%20rules%20can%20subsequently%20be%20used%20for%20explaining%20GNN%0Adecisions.%20Experiments%20on%20both%20synthetic%20and%20real-life%20datasets%20show%20highly%0Acompetitive%20performance%2C%20with%20up%20to%20200%25%20improvement%20in%20fidelity%20on%20explaining%0Agraph%20classification%20over%20the%20SOTA%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11594v1&entry.124074799=Read"},
{"title": "Efficient Discovery of Significant Patterns with Few-Shot Resampling", "author": "Leonardo Pellegrina and Fabio Vandin", "abstract": "  Significant pattern mining is a fundamental task in mining transactional\ndata, requiring to identify patterns significantly associated with the value of\na given feature, the target. In several applications, such as biomedicine,\nbasket market analysis, and social networks, the goal is to discover patterns\nwhose association with the target is defined with respect to an underlying\npopulation, or process, of which the dataset represents only a collection of\nobservations, or samples. A natural way to capture the association of a pattern\nwith the target is to consider its statistical significance, assessing its\ndeviation from the (null) hypothesis of independence between the pattern and\nthe target. While several algorithms have been proposed to find statistically\nsignificant patterns, it remains a computationally demanding task, and for\ncomplex patterns such as subgroups, no efficient solution exists.\n  We present FSR, an efficient algorithm to identify statistically significant\npatterns with rigorous guarantees on the probability of false discoveries. FSR\nbuilds on a novel general framework for mining significant patterns that\ncaptures some of the most commonly considered patterns, including itemsets,\nsequential patterns, and subgroups. FSR uses a small number of resampled\ndatasets, obtained by assigning i.i.d. labels to each transaction, to\nrigorously bound the supremum deviation of a quality statistic measuring the\nsignificance of patterns. FSR builds on novel tight bounds on the supremum\ndeviation that require to mine a small number of resampled datasets, while\nproviding a high effectiveness in discovering significant patterns. As a test\ncase, we consider significant subgroup mining, and our evaluation on several\nreal datasets shows that FSR is effective in discovering significant subgroups,\nwhile requiring a small number of resampled datasets.\n", "link": "http://arxiv.org/abs/2406.11803v1", "date": "2024-06-17", "relevancy": 2.2465, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4734}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4518}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Discovery%20of%20Significant%20Patterns%20with%20Few-Shot%20Resampling&body=Title%3A%20Efficient%20Discovery%20of%20Significant%20Patterns%20with%20Few-Shot%20Resampling%0AAuthor%3A%20Leonardo%20Pellegrina%20and%20Fabio%20Vandin%0AAbstract%3A%20%20%20Significant%20pattern%20mining%20is%20a%20fundamental%20task%20in%20mining%20transactional%0Adata%2C%20requiring%20to%20identify%20patterns%20significantly%20associated%20with%20the%20value%20of%0Aa%20given%20feature%2C%20the%20target.%20In%20several%20applications%2C%20such%20as%20biomedicine%2C%0Abasket%20market%20analysis%2C%20and%20social%20networks%2C%20the%20goal%20is%20to%20discover%20patterns%0Awhose%20association%20with%20the%20target%20is%20defined%20with%20respect%20to%20an%20underlying%0Apopulation%2C%20or%20process%2C%20of%20which%20the%20dataset%20represents%20only%20a%20collection%20of%0Aobservations%2C%20or%20samples.%20A%20natural%20way%20to%20capture%20the%20association%20of%20a%20pattern%0Awith%20the%20target%20is%20to%20consider%20its%20statistical%20significance%2C%20assessing%20its%0Adeviation%20from%20the%20%28null%29%20hypothesis%20of%20independence%20between%20the%20pattern%20and%0Athe%20target.%20While%20several%20algorithms%20have%20been%20proposed%20to%20find%20statistically%0Asignificant%20patterns%2C%20it%20remains%20a%20computationally%20demanding%20task%2C%20and%20for%0Acomplex%20patterns%20such%20as%20subgroups%2C%20no%20efficient%20solution%20exists.%0A%20%20We%20present%20FSR%2C%20an%20efficient%20algorithm%20to%20identify%20statistically%20significant%0Apatterns%20with%20rigorous%20guarantees%20on%20the%20probability%20of%20false%20discoveries.%20FSR%0Abuilds%20on%20a%20novel%20general%20framework%20for%20mining%20significant%20patterns%20that%0Acaptures%20some%20of%20the%20most%20commonly%20considered%20patterns%2C%20including%20itemsets%2C%0Asequential%20patterns%2C%20and%20subgroups.%20FSR%20uses%20a%20small%20number%20of%20resampled%0Adatasets%2C%20obtained%20by%20assigning%20i.i.d.%20labels%20to%20each%20transaction%2C%20to%0Arigorously%20bound%20the%20supremum%20deviation%20of%20a%20quality%20statistic%20measuring%20the%0Asignificance%20of%20patterns.%20FSR%20builds%20on%20novel%20tight%20bounds%20on%20the%20supremum%0Adeviation%20that%20require%20to%20mine%20a%20small%20number%20of%20resampled%20datasets%2C%20while%0Aproviding%20a%20high%20effectiveness%20in%20discovering%20significant%20patterns.%20As%20a%20test%0Acase%2C%20we%20consider%20significant%20subgroup%20mining%2C%20and%20our%20evaluation%20on%20several%0Areal%20datasets%20shows%20that%20FSR%20is%20effective%20in%20discovering%20significant%20subgroups%2C%0Awhile%20requiring%20a%20small%20number%20of%20resampled%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11803v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Discovery%2520of%2520Significant%2520Patterns%2520with%2520Few-Shot%2520Resampling%26entry.906535625%3DLeonardo%2520Pellegrina%2520and%2520Fabio%2520Vandin%26entry.1292438233%3D%2520%2520Significant%2520pattern%2520mining%2520is%2520a%2520fundamental%2520task%2520in%2520mining%2520transactional%250Adata%252C%2520requiring%2520to%2520identify%2520patterns%2520significantly%2520associated%2520with%2520the%2520value%2520of%250Aa%2520given%2520feature%252C%2520the%2520target.%2520In%2520several%2520applications%252C%2520such%2520as%2520biomedicine%252C%250Abasket%2520market%2520analysis%252C%2520and%2520social%2520networks%252C%2520the%2520goal%2520is%2520to%2520discover%2520patterns%250Awhose%2520association%2520with%2520the%2520target%2520is%2520defined%2520with%2520respect%2520to%2520an%2520underlying%250Apopulation%252C%2520or%2520process%252C%2520of%2520which%2520the%2520dataset%2520represents%2520only%2520a%2520collection%2520of%250Aobservations%252C%2520or%2520samples.%2520A%2520natural%2520way%2520to%2520capture%2520the%2520association%2520of%2520a%2520pattern%250Awith%2520the%2520target%2520is%2520to%2520consider%2520its%2520statistical%2520significance%252C%2520assessing%2520its%250Adeviation%2520from%2520the%2520%2528null%2529%2520hypothesis%2520of%2520independence%2520between%2520the%2520pattern%2520and%250Athe%2520target.%2520While%2520several%2520algorithms%2520have%2520been%2520proposed%2520to%2520find%2520statistically%250Asignificant%2520patterns%252C%2520it%2520remains%2520a%2520computationally%2520demanding%2520task%252C%2520and%2520for%250Acomplex%2520patterns%2520such%2520as%2520subgroups%252C%2520no%2520efficient%2520solution%2520exists.%250A%2520%2520We%2520present%2520FSR%252C%2520an%2520efficient%2520algorithm%2520to%2520identify%2520statistically%2520significant%250Apatterns%2520with%2520rigorous%2520guarantees%2520on%2520the%2520probability%2520of%2520false%2520discoveries.%2520FSR%250Abuilds%2520on%2520a%2520novel%2520general%2520framework%2520for%2520mining%2520significant%2520patterns%2520that%250Acaptures%2520some%2520of%2520the%2520most%2520commonly%2520considered%2520patterns%252C%2520including%2520itemsets%252C%250Asequential%2520patterns%252C%2520and%2520subgroups.%2520FSR%2520uses%2520a%2520small%2520number%2520of%2520resampled%250Adatasets%252C%2520obtained%2520by%2520assigning%2520i.i.d.%2520labels%2520to%2520each%2520transaction%252C%2520to%250Arigorously%2520bound%2520the%2520supremum%2520deviation%2520of%2520a%2520quality%2520statistic%2520measuring%2520the%250Asignificance%2520of%2520patterns.%2520FSR%2520builds%2520on%2520novel%2520tight%2520bounds%2520on%2520the%2520supremum%250Adeviation%2520that%2520require%2520to%2520mine%2520a%2520small%2520number%2520of%2520resampled%2520datasets%252C%2520while%250Aproviding%2520a%2520high%2520effectiveness%2520in%2520discovering%2520significant%2520patterns.%2520As%2520a%2520test%250Acase%252C%2520we%2520consider%2520significant%2520subgroup%2520mining%252C%2520and%2520our%2520evaluation%2520on%2520several%250Areal%2520datasets%2520shows%2520that%2520FSR%2520is%2520effective%2520in%2520discovering%2520significant%2520subgroups%252C%250Awhile%2520requiring%2520a%2520small%2520number%2520of%2520resampled%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11803v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Discovery%20of%20Significant%20Patterns%20with%20Few-Shot%20Resampling&entry.906535625=Leonardo%20Pellegrina%20and%20Fabio%20Vandin&entry.1292438233=%20%20Significant%20pattern%20mining%20is%20a%20fundamental%20task%20in%20mining%20transactional%0Adata%2C%20requiring%20to%20identify%20patterns%20significantly%20associated%20with%20the%20value%20of%0Aa%20given%20feature%2C%20the%20target.%20In%20several%20applications%2C%20such%20as%20biomedicine%2C%0Abasket%20market%20analysis%2C%20and%20social%20networks%2C%20the%20goal%20is%20to%20discover%20patterns%0Awhose%20association%20with%20the%20target%20is%20defined%20with%20respect%20to%20an%20underlying%0Apopulation%2C%20or%20process%2C%20of%20which%20the%20dataset%20represents%20only%20a%20collection%20of%0Aobservations%2C%20or%20samples.%20A%20natural%20way%20to%20capture%20the%20association%20of%20a%20pattern%0Awith%20the%20target%20is%20to%20consider%20its%20statistical%20significance%2C%20assessing%20its%0Adeviation%20from%20the%20%28null%29%20hypothesis%20of%20independence%20between%20the%20pattern%20and%0Athe%20target.%20While%20several%20algorithms%20have%20been%20proposed%20to%20find%20statistically%0Asignificant%20patterns%2C%20it%20remains%20a%20computationally%20demanding%20task%2C%20and%20for%0Acomplex%20patterns%20such%20as%20subgroups%2C%20no%20efficient%20solution%20exists.%0A%20%20We%20present%20FSR%2C%20an%20efficient%20algorithm%20to%20identify%20statistically%20significant%0Apatterns%20with%20rigorous%20guarantees%20on%20the%20probability%20of%20false%20discoveries.%20FSR%0Abuilds%20on%20a%20novel%20general%20framework%20for%20mining%20significant%20patterns%20that%0Acaptures%20some%20of%20the%20most%20commonly%20considered%20patterns%2C%20including%20itemsets%2C%0Asequential%20patterns%2C%20and%20subgroups.%20FSR%20uses%20a%20small%20number%20of%20resampled%0Adatasets%2C%20obtained%20by%20assigning%20i.i.d.%20labels%20to%20each%20transaction%2C%20to%0Arigorously%20bound%20the%20supremum%20deviation%20of%20a%20quality%20statistic%20measuring%20the%0Asignificance%20of%20patterns.%20FSR%20builds%20on%20novel%20tight%20bounds%20on%20the%20supremum%0Adeviation%20that%20require%20to%20mine%20a%20small%20number%20of%20resampled%20datasets%2C%20while%0Aproviding%20a%20high%20effectiveness%20in%20discovering%20significant%20patterns.%20As%20a%20test%0Acase%2C%20we%20consider%20significant%20subgroup%20mining%2C%20and%20our%20evaluation%20on%20several%0Areal%20datasets%20shows%20that%20FSR%20is%20effective%20in%20discovering%20significant%20subgroups%2C%0Awhile%20requiring%20a%20small%20number%20of%20resampled%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11803v1&entry.124074799=Read"},
{"title": "Embedded Hierarchical MPC for Autonomous Navigation", "author": "Dennis Benders and Johannes K\u00f6hler and Thijs Niesten and Robert Babu\u0161ka and Javier Alonso-Mora and Laura Ferranti", "abstract": "  To efficiently deploy robotic systems in society, mobile robots need to\nautonomously and safely move through complex environments. Nonlinear model\npredictive control (MPC) methods provide a natural way to find a dynamically\nfeasible trajectory through the environment without colliding with nearby\nobstacles. However, the limited computation power available on typical embedded\nrobotic systems, such as quadrotors, poses a challenge to running MPC in\nreal-time, including its most expensive tasks: constraints generation and\noptimization. To address this problem, we propose a novel hierarchical MPC\nscheme that interconnects a planning and a tracking layer. The planner\nconstructs a trajectory with a long prediction horizon at a slow rate, while\nthe tracker ensures trajectory tracking at a relatively fast rate. We prove\nthat the proposed framework avoids collisions and is recursively feasible.\nFurthermore, we demonstrate its effectiveness in simulations and lab\nexperiments with a quadrotor that needs to reach a goal position in a complex\nstatic environment. The code is efficiently implemented on the quadrotor's\nembedded computer to ensure real-time feasibility. Compared to a\nstate-of-the-art single-layer MPC formulation, this allows us to increase the\nplanning horizon by a factor of 5, which results in significantly better\nperformance.\n", "link": "http://arxiv.org/abs/2406.11506v1", "date": "2024-06-17", "relevancy": 2.2366, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5848}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5767}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5314}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embedded%20Hierarchical%20MPC%20for%20Autonomous%20Navigation&body=Title%3A%20Embedded%20Hierarchical%20MPC%20for%20Autonomous%20Navigation%0AAuthor%3A%20Dennis%20Benders%20and%20Johannes%20K%C3%B6hler%20and%20Thijs%20Niesten%20and%20Robert%20Babu%C5%A1ka%20and%20Javier%20Alonso-Mora%20and%20Laura%20Ferranti%0AAbstract%3A%20%20%20To%20efficiently%20deploy%20robotic%20systems%20in%20society%2C%20mobile%20robots%20need%20to%0Aautonomously%20and%20safely%20move%20through%20complex%20environments.%20Nonlinear%20model%0Apredictive%20control%20%28MPC%29%20methods%20provide%20a%20natural%20way%20to%20find%20a%20dynamically%0Afeasible%20trajectory%20through%20the%20environment%20without%20colliding%20with%20nearby%0Aobstacles.%20However%2C%20the%20limited%20computation%20power%20available%20on%20typical%20embedded%0Arobotic%20systems%2C%20such%20as%20quadrotors%2C%20poses%20a%20challenge%20to%20running%20MPC%20in%0Areal-time%2C%20including%20its%20most%20expensive%20tasks%3A%20constraints%20generation%20and%0Aoptimization.%20To%20address%20this%20problem%2C%20we%20propose%20a%20novel%20hierarchical%20MPC%0Ascheme%20that%20interconnects%20a%20planning%20and%20a%20tracking%20layer.%20The%20planner%0Aconstructs%20a%20trajectory%20with%20a%20long%20prediction%20horizon%20at%20a%20slow%20rate%2C%20while%0Athe%20tracker%20ensures%20trajectory%20tracking%20at%20a%20relatively%20fast%20rate.%20We%20prove%0Athat%20the%20proposed%20framework%20avoids%20collisions%20and%20is%20recursively%20feasible.%0AFurthermore%2C%20we%20demonstrate%20its%20effectiveness%20in%20simulations%20and%20lab%0Aexperiments%20with%20a%20quadrotor%20that%20needs%20to%20reach%20a%20goal%20position%20in%20a%20complex%0Astatic%20environment.%20The%20code%20is%20efficiently%20implemented%20on%20the%20quadrotor%27s%0Aembedded%20computer%20to%20ensure%20real-time%20feasibility.%20Compared%20to%20a%0Astate-of-the-art%20single-layer%20MPC%20formulation%2C%20this%20allows%20us%20to%20increase%20the%0Aplanning%20horizon%20by%20a%20factor%20of%205%2C%20which%20results%20in%20significantly%20better%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11506v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbedded%2520Hierarchical%2520MPC%2520for%2520Autonomous%2520Navigation%26entry.906535625%3DDennis%2520Benders%2520and%2520Johannes%2520K%25C3%25B6hler%2520and%2520Thijs%2520Niesten%2520and%2520Robert%2520Babu%25C5%25A1ka%2520and%2520Javier%2520Alonso-Mora%2520and%2520Laura%2520Ferranti%26entry.1292438233%3D%2520%2520To%2520efficiently%2520deploy%2520robotic%2520systems%2520in%2520society%252C%2520mobile%2520robots%2520need%2520to%250Aautonomously%2520and%2520safely%2520move%2520through%2520complex%2520environments.%2520Nonlinear%2520model%250Apredictive%2520control%2520%2528MPC%2529%2520methods%2520provide%2520a%2520natural%2520way%2520to%2520find%2520a%2520dynamically%250Afeasible%2520trajectory%2520through%2520the%2520environment%2520without%2520colliding%2520with%2520nearby%250Aobstacles.%2520However%252C%2520the%2520limited%2520computation%2520power%2520available%2520on%2520typical%2520embedded%250Arobotic%2520systems%252C%2520such%2520as%2520quadrotors%252C%2520poses%2520a%2520challenge%2520to%2520running%2520MPC%2520in%250Areal-time%252C%2520including%2520its%2520most%2520expensive%2520tasks%253A%2520constraints%2520generation%2520and%250Aoptimization.%2520To%2520address%2520this%2520problem%252C%2520we%2520propose%2520a%2520novel%2520hierarchical%2520MPC%250Ascheme%2520that%2520interconnects%2520a%2520planning%2520and%2520a%2520tracking%2520layer.%2520The%2520planner%250Aconstructs%2520a%2520trajectory%2520with%2520a%2520long%2520prediction%2520horizon%2520at%2520a%2520slow%2520rate%252C%2520while%250Athe%2520tracker%2520ensures%2520trajectory%2520tracking%2520at%2520a%2520relatively%2520fast%2520rate.%2520We%2520prove%250Athat%2520the%2520proposed%2520framework%2520avoids%2520collisions%2520and%2520is%2520recursively%2520feasible.%250AFurthermore%252C%2520we%2520demonstrate%2520its%2520effectiveness%2520in%2520simulations%2520and%2520lab%250Aexperiments%2520with%2520a%2520quadrotor%2520that%2520needs%2520to%2520reach%2520a%2520goal%2520position%2520in%2520a%2520complex%250Astatic%2520environment.%2520The%2520code%2520is%2520efficiently%2520implemented%2520on%2520the%2520quadrotor%2527s%250Aembedded%2520computer%2520to%2520ensure%2520real-time%2520feasibility.%2520Compared%2520to%2520a%250Astate-of-the-art%2520single-layer%2520MPC%2520formulation%252C%2520this%2520allows%2520us%2520to%2520increase%2520the%250Aplanning%2520horizon%2520by%2520a%2520factor%2520of%25205%252C%2520which%2520results%2520in%2520significantly%2520better%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11506v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embedded%20Hierarchical%20MPC%20for%20Autonomous%20Navigation&entry.906535625=Dennis%20Benders%20and%20Johannes%20K%C3%B6hler%20and%20Thijs%20Niesten%20and%20Robert%20Babu%C5%A1ka%20and%20Javier%20Alonso-Mora%20and%20Laura%20Ferranti&entry.1292438233=%20%20To%20efficiently%20deploy%20robotic%20systems%20in%20society%2C%20mobile%20robots%20need%20to%0Aautonomously%20and%20safely%20move%20through%20complex%20environments.%20Nonlinear%20model%0Apredictive%20control%20%28MPC%29%20methods%20provide%20a%20natural%20way%20to%20find%20a%20dynamically%0Afeasible%20trajectory%20through%20the%20environment%20without%20colliding%20with%20nearby%0Aobstacles.%20However%2C%20the%20limited%20computation%20power%20available%20on%20typical%20embedded%0Arobotic%20systems%2C%20such%20as%20quadrotors%2C%20poses%20a%20challenge%20to%20running%20MPC%20in%0Areal-time%2C%20including%20its%20most%20expensive%20tasks%3A%20constraints%20generation%20and%0Aoptimization.%20To%20address%20this%20problem%2C%20we%20propose%20a%20novel%20hierarchical%20MPC%0Ascheme%20that%20interconnects%20a%20planning%20and%20a%20tracking%20layer.%20The%20planner%0Aconstructs%20a%20trajectory%20with%20a%20long%20prediction%20horizon%20at%20a%20slow%20rate%2C%20while%0Athe%20tracker%20ensures%20trajectory%20tracking%20at%20a%20relatively%20fast%20rate.%20We%20prove%0Athat%20the%20proposed%20framework%20avoids%20collisions%20and%20is%20recursively%20feasible.%0AFurthermore%2C%20we%20demonstrate%20its%20effectiveness%20in%20simulations%20and%20lab%0Aexperiments%20with%20a%20quadrotor%20that%20needs%20to%20reach%20a%20goal%20position%20in%20a%20complex%0Astatic%20environment.%20The%20code%20is%20efficiently%20implemented%20on%20the%20quadrotor%27s%0Aembedded%20computer%20to%20ensure%20real-time%20feasibility.%20Compared%20to%20a%0Astate-of-the-art%20single-layer%20MPC%20formulation%2C%20this%20allows%20us%20to%20increase%20the%0Aplanning%20horizon%20by%20a%20factor%20of%205%2C%20which%20results%20in%20significantly%20better%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11506v1&entry.124074799=Read"},
{"title": "SeaFormer++: Squeeze-enhanced Axial Transformer for Mobile Visual\n  Recognition", "author": "Qiang Wan and Zilong Huang and Jiachen Lu and Gang Yu and Li Zhang", "abstract": "  Since the introduction of Vision Transformers, the landscape of many computer\nvision tasks (e.g., semantic segmentation), which has been overwhelmingly\ndominated by CNNs, recently has significantly revolutionized. However, the\ncomputational cost and memory requirement renders these methods unsuitable on\nthe mobile device. In this paper, we introduce a new method squeeze-enhanced\nAxial Transformer (SeaFormer) for mobile visual recognition. Specifically, we\ndesign a generic attention block characterized by the formulation of squeeze\nAxial and detail enhancement. It can be further used to create a family of\nbackbone architectures with superior cost-effectiveness. Coupled with a light\nsegmentation head, we achieve the best trade-off between segmentation accuracy\nand latency on the ARM-based mobile devices on the ADE20K, Cityscapes, Pascal\nContext and COCO-Stuff datasets. Critically, we beat both the mobilefriendly\nrivals and Transformer-based counterparts with better performance and lower\nlatency without bells and whistles. Furthermore, we incorporate a feature\nupsampling-based multi-resolution distillation technique, further reducing the\ninference latency of the proposed framework. Beyond semantic segmentation, we\nfurther apply the proposed SeaFormer architecture to image classification and\nobject detection problems, demonstrating the potential of serving as a\nversatile mobile-friendly backbone. Our code and models are made publicly\navailable at https://github.com/fudan-zvg/SeaFormer.\n", "link": "http://arxiv.org/abs/2301.13156v5", "date": "2024-06-17", "relevancy": 2.2075, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5664}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5529}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SeaFormer%2B%2B%3A%20Squeeze-enhanced%20Axial%20Transformer%20for%20Mobile%20Visual%0A%20%20Recognition&body=Title%3A%20SeaFormer%2B%2B%3A%20Squeeze-enhanced%20Axial%20Transformer%20for%20Mobile%20Visual%0A%20%20Recognition%0AAuthor%3A%20Qiang%20Wan%20and%20Zilong%20Huang%20and%20Jiachen%20Lu%20and%20Gang%20Yu%20and%20Li%20Zhang%0AAbstract%3A%20%20%20Since%20the%20introduction%20of%20Vision%20Transformers%2C%20the%20landscape%20of%20many%20computer%0Avision%20tasks%20%28e.g.%2C%20semantic%20segmentation%29%2C%20which%20has%20been%20overwhelmingly%0Adominated%20by%20CNNs%2C%20recently%20has%20significantly%20revolutionized.%20However%2C%20the%0Acomputational%20cost%20and%20memory%20requirement%20renders%20these%20methods%20unsuitable%20on%0Athe%20mobile%20device.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20method%20squeeze-enhanced%0AAxial%20Transformer%20%28SeaFormer%29%20for%20mobile%20visual%20recognition.%20Specifically%2C%20we%0Adesign%20a%20generic%20attention%20block%20characterized%20by%20the%20formulation%20of%20squeeze%0AAxial%20and%20detail%20enhancement.%20It%20can%20be%20further%20used%20to%20create%20a%20family%20of%0Abackbone%20architectures%20with%20superior%20cost-effectiveness.%20Coupled%20with%20a%20light%0Asegmentation%20head%2C%20we%20achieve%20the%20best%20trade-off%20between%20segmentation%20accuracy%0Aand%20latency%20on%20the%20ARM-based%20mobile%20devices%20on%20the%20ADE20K%2C%20Cityscapes%2C%20Pascal%0AContext%20and%20COCO-Stuff%20datasets.%20Critically%2C%20we%20beat%20both%20the%20mobilefriendly%0Arivals%20and%20Transformer-based%20counterparts%20with%20better%20performance%20and%20lower%0Alatency%20without%20bells%20and%20whistles.%20Furthermore%2C%20we%20incorporate%20a%20feature%0Aupsampling-based%20multi-resolution%20distillation%20technique%2C%20further%20reducing%20the%0Ainference%20latency%20of%20the%20proposed%20framework.%20Beyond%20semantic%20segmentation%2C%20we%0Afurther%20apply%20the%20proposed%20SeaFormer%20architecture%20to%20image%20classification%20and%0Aobject%20detection%20problems%2C%20demonstrating%20the%20potential%20of%20serving%20as%20a%0Aversatile%20mobile-friendly%20backbone.%20Our%20code%20and%20models%20are%20made%20publicly%0Aavailable%20at%20https%3A//github.com/fudan-zvg/SeaFormer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.13156v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeaFormer%252B%252B%253A%2520Squeeze-enhanced%2520Axial%2520Transformer%2520for%2520Mobile%2520Visual%250A%2520%2520Recognition%26entry.906535625%3DQiang%2520Wan%2520and%2520Zilong%2520Huang%2520and%2520Jiachen%2520Lu%2520and%2520Gang%2520Yu%2520and%2520Li%2520Zhang%26entry.1292438233%3D%2520%2520Since%2520the%2520introduction%2520of%2520Vision%2520Transformers%252C%2520the%2520landscape%2520of%2520many%2520computer%250Avision%2520tasks%2520%2528e.g.%252C%2520semantic%2520segmentation%2529%252C%2520which%2520has%2520been%2520overwhelmingly%250Adominated%2520by%2520CNNs%252C%2520recently%2520has%2520significantly%2520revolutionized.%2520However%252C%2520the%250Acomputational%2520cost%2520and%2520memory%2520requirement%2520renders%2520these%2520methods%2520unsuitable%2520on%250Athe%2520mobile%2520device.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%2520method%2520squeeze-enhanced%250AAxial%2520Transformer%2520%2528SeaFormer%2529%2520for%2520mobile%2520visual%2520recognition.%2520Specifically%252C%2520we%250Adesign%2520a%2520generic%2520attention%2520block%2520characterized%2520by%2520the%2520formulation%2520of%2520squeeze%250AAxial%2520and%2520detail%2520enhancement.%2520It%2520can%2520be%2520further%2520used%2520to%2520create%2520a%2520family%2520of%250Abackbone%2520architectures%2520with%2520superior%2520cost-effectiveness.%2520Coupled%2520with%2520a%2520light%250Asegmentation%2520head%252C%2520we%2520achieve%2520the%2520best%2520trade-off%2520between%2520segmentation%2520accuracy%250Aand%2520latency%2520on%2520the%2520ARM-based%2520mobile%2520devices%2520on%2520the%2520ADE20K%252C%2520Cityscapes%252C%2520Pascal%250AContext%2520and%2520COCO-Stuff%2520datasets.%2520Critically%252C%2520we%2520beat%2520both%2520the%2520mobilefriendly%250Arivals%2520and%2520Transformer-based%2520counterparts%2520with%2520better%2520performance%2520and%2520lower%250Alatency%2520without%2520bells%2520and%2520whistles.%2520Furthermore%252C%2520we%2520incorporate%2520a%2520feature%250Aupsampling-based%2520multi-resolution%2520distillation%2520technique%252C%2520further%2520reducing%2520the%250Ainference%2520latency%2520of%2520the%2520proposed%2520framework.%2520Beyond%2520semantic%2520segmentation%252C%2520we%250Afurther%2520apply%2520the%2520proposed%2520SeaFormer%2520architecture%2520to%2520image%2520classification%2520and%250Aobject%2520detection%2520problems%252C%2520demonstrating%2520the%2520potential%2520of%2520serving%2520as%2520a%250Aversatile%2520mobile-friendly%2520backbone.%2520Our%2520code%2520and%2520models%2520are%2520made%2520publicly%250Aavailable%2520at%2520https%253A//github.com/fudan-zvg/SeaFormer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.13156v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SeaFormer%2B%2B%3A%20Squeeze-enhanced%20Axial%20Transformer%20for%20Mobile%20Visual%0A%20%20Recognition&entry.906535625=Qiang%20Wan%20and%20Zilong%20Huang%20and%20Jiachen%20Lu%20and%20Gang%20Yu%20and%20Li%20Zhang&entry.1292438233=%20%20Since%20the%20introduction%20of%20Vision%20Transformers%2C%20the%20landscape%20of%20many%20computer%0Avision%20tasks%20%28e.g.%2C%20semantic%20segmentation%29%2C%20which%20has%20been%20overwhelmingly%0Adominated%20by%20CNNs%2C%20recently%20has%20significantly%20revolutionized.%20However%2C%20the%0Acomputational%20cost%20and%20memory%20requirement%20renders%20these%20methods%20unsuitable%20on%0Athe%20mobile%20device.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20method%20squeeze-enhanced%0AAxial%20Transformer%20%28SeaFormer%29%20for%20mobile%20visual%20recognition.%20Specifically%2C%20we%0Adesign%20a%20generic%20attention%20block%20characterized%20by%20the%20formulation%20of%20squeeze%0AAxial%20and%20detail%20enhancement.%20It%20can%20be%20further%20used%20to%20create%20a%20family%20of%0Abackbone%20architectures%20with%20superior%20cost-effectiveness.%20Coupled%20with%20a%20light%0Asegmentation%20head%2C%20we%20achieve%20the%20best%20trade-off%20between%20segmentation%20accuracy%0Aand%20latency%20on%20the%20ARM-based%20mobile%20devices%20on%20the%20ADE20K%2C%20Cityscapes%2C%20Pascal%0AContext%20and%20COCO-Stuff%20datasets.%20Critically%2C%20we%20beat%20both%20the%20mobilefriendly%0Arivals%20and%20Transformer-based%20counterparts%20with%20better%20performance%20and%20lower%0Alatency%20without%20bells%20and%20whistles.%20Furthermore%2C%20we%20incorporate%20a%20feature%0Aupsampling-based%20multi-resolution%20distillation%20technique%2C%20further%20reducing%20the%0Ainference%20latency%20of%20the%20proposed%20framework.%20Beyond%20semantic%20segmentation%2C%20we%0Afurther%20apply%20the%20proposed%20SeaFormer%20architecture%20to%20image%20classification%20and%0Aobject%20detection%20problems%2C%20demonstrating%20the%20potential%20of%20serving%20as%20a%0Aversatile%20mobile-friendly%20backbone.%20Our%20code%20and%20models%20are%20made%20publicly%0Aavailable%20at%20https%3A//github.com/fudan-zvg/SeaFormer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.13156v5&entry.124074799=Read"},
{"title": "Multimodal Learning To Improve Segmentation With Intraoperative CBCT &\n  Preoperative CT", "author": "Maximilian E. Tschuchnig and Philipp Steininger and Michael Gadermayr", "abstract": "  Intraoperative medical imaging, particularly Cone-beam computed tomography\n(CBCT), is an important tool facilitating computer aided interventions, despite\na lower visual quality. While this degraded image quality can affect downstream\nsegmentation, the availability of high quality preoperative scans represents\npotential for improvements. Here we consider a setting where preoperative CT\nand intraoperative CBCT scans are available, however, the alignment\n(registration) between the scans is imperfect. We propose a multimodal learning\nmethod that fuses roughly aligned CBCT and CT scans and investigate the effect\nof CBCT quality and misalignment (affine and elastic transformations\nfacilitating misalignment) on the final segmentation performance. As an\napplication scenario, we focus on the segmentation of liver and liver tumor\nsemantic segmentation and evaluate the effect of intraoperative image quality\nand misalignment on segmentation performance. To accomplish this, high quality,\nlabelled CTs are defined as preoperative and used as a basis to simulate\nintraoperative CBCT. We show that the fusion of preoperative CT and simulated,\nintraoperative CBCT mostly improves segmentation performance and that even\nclearly misaligned preoperative data has the potential to improve segmentation\nperformance.\n", "link": "http://arxiv.org/abs/2406.11650v1", "date": "2024-06-17", "relevancy": 2.1958, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5844}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5268}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Learning%20To%20Improve%20Segmentation%20With%20Intraoperative%20CBCT%20%26%0A%20%20Preoperative%20CT&body=Title%3A%20Multimodal%20Learning%20To%20Improve%20Segmentation%20With%20Intraoperative%20CBCT%20%26%0A%20%20Preoperative%20CT%0AAuthor%3A%20Maximilian%20E.%20Tschuchnig%20and%20Philipp%20Steininger%20and%20Michael%20Gadermayr%0AAbstract%3A%20%20%20Intraoperative%20medical%20imaging%2C%20particularly%20Cone-beam%20computed%20tomography%0A%28CBCT%29%2C%20is%20an%20important%20tool%20facilitating%20computer%20aided%20interventions%2C%20despite%0Aa%20lower%20visual%20quality.%20While%20this%20degraded%20image%20quality%20can%20affect%20downstream%0Asegmentation%2C%20the%20availability%20of%20high%20quality%20preoperative%20scans%20represents%0Apotential%20for%20improvements.%20Here%20we%20consider%20a%20setting%20where%20preoperative%20CT%0Aand%20intraoperative%20CBCT%20scans%20are%20available%2C%20however%2C%20the%20alignment%0A%28registration%29%20between%20the%20scans%20is%20imperfect.%20We%20propose%20a%20multimodal%20learning%0Amethod%20that%20fuses%20roughly%20aligned%20CBCT%20and%20CT%20scans%20and%20investigate%20the%20effect%0Aof%20CBCT%20quality%20and%20misalignment%20%28affine%20and%20elastic%20transformations%0Afacilitating%20misalignment%29%20on%20the%20final%20segmentation%20performance.%20As%20an%0Aapplication%20scenario%2C%20we%20focus%20on%20the%20segmentation%20of%20liver%20and%20liver%20tumor%0Asemantic%20segmentation%20and%20evaluate%20the%20effect%20of%20intraoperative%20image%20quality%0Aand%20misalignment%20on%20segmentation%20performance.%20To%20accomplish%20this%2C%20high%20quality%2C%0Alabelled%20CTs%20are%20defined%20as%20preoperative%20and%20used%20as%20a%20basis%20to%20simulate%0Aintraoperative%20CBCT.%20We%20show%20that%20the%20fusion%20of%20preoperative%20CT%20and%20simulated%2C%0Aintraoperative%20CBCT%20mostly%20improves%20segmentation%20performance%20and%20that%20even%0Aclearly%20misaligned%20preoperative%20data%20has%20the%20potential%20to%20improve%20segmentation%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11650v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Learning%2520To%2520Improve%2520Segmentation%2520With%2520Intraoperative%2520CBCT%2520%2526%250A%2520%2520Preoperative%2520CT%26entry.906535625%3DMaximilian%2520E.%2520Tschuchnig%2520and%2520Philipp%2520Steininger%2520and%2520Michael%2520Gadermayr%26entry.1292438233%3D%2520%2520Intraoperative%2520medical%2520imaging%252C%2520particularly%2520Cone-beam%2520computed%2520tomography%250A%2528CBCT%2529%252C%2520is%2520an%2520important%2520tool%2520facilitating%2520computer%2520aided%2520interventions%252C%2520despite%250Aa%2520lower%2520visual%2520quality.%2520While%2520this%2520degraded%2520image%2520quality%2520can%2520affect%2520downstream%250Asegmentation%252C%2520the%2520availability%2520of%2520high%2520quality%2520preoperative%2520scans%2520represents%250Apotential%2520for%2520improvements.%2520Here%2520we%2520consider%2520a%2520setting%2520where%2520preoperative%2520CT%250Aand%2520intraoperative%2520CBCT%2520scans%2520are%2520available%252C%2520however%252C%2520the%2520alignment%250A%2528registration%2529%2520between%2520the%2520scans%2520is%2520imperfect.%2520We%2520propose%2520a%2520multimodal%2520learning%250Amethod%2520that%2520fuses%2520roughly%2520aligned%2520CBCT%2520and%2520CT%2520scans%2520and%2520investigate%2520the%2520effect%250Aof%2520CBCT%2520quality%2520and%2520misalignment%2520%2528affine%2520and%2520elastic%2520transformations%250Afacilitating%2520misalignment%2529%2520on%2520the%2520final%2520segmentation%2520performance.%2520As%2520an%250Aapplication%2520scenario%252C%2520we%2520focus%2520on%2520the%2520segmentation%2520of%2520liver%2520and%2520liver%2520tumor%250Asemantic%2520segmentation%2520and%2520evaluate%2520the%2520effect%2520of%2520intraoperative%2520image%2520quality%250Aand%2520misalignment%2520on%2520segmentation%2520performance.%2520To%2520accomplish%2520this%252C%2520high%2520quality%252C%250Alabelled%2520CTs%2520are%2520defined%2520as%2520preoperative%2520and%2520used%2520as%2520a%2520basis%2520to%2520simulate%250Aintraoperative%2520CBCT.%2520We%2520show%2520that%2520the%2520fusion%2520of%2520preoperative%2520CT%2520and%2520simulated%252C%250Aintraoperative%2520CBCT%2520mostly%2520improves%2520segmentation%2520performance%2520and%2520that%2520even%250Aclearly%2520misaligned%2520preoperative%2520data%2520has%2520the%2520potential%2520to%2520improve%2520segmentation%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11650v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Learning%20To%20Improve%20Segmentation%20With%20Intraoperative%20CBCT%20%26%0A%20%20Preoperative%20CT&entry.906535625=Maximilian%20E.%20Tschuchnig%20and%20Philipp%20Steininger%20and%20Michael%20Gadermayr&entry.1292438233=%20%20Intraoperative%20medical%20imaging%2C%20particularly%20Cone-beam%20computed%20tomography%0A%28CBCT%29%2C%20is%20an%20important%20tool%20facilitating%20computer%20aided%20interventions%2C%20despite%0Aa%20lower%20visual%20quality.%20While%20this%20degraded%20image%20quality%20can%20affect%20downstream%0Asegmentation%2C%20the%20availability%20of%20high%20quality%20preoperative%20scans%20represents%0Apotential%20for%20improvements.%20Here%20we%20consider%20a%20setting%20where%20preoperative%20CT%0Aand%20intraoperative%20CBCT%20scans%20are%20available%2C%20however%2C%20the%20alignment%0A%28registration%29%20between%20the%20scans%20is%20imperfect.%20We%20propose%20a%20multimodal%20learning%0Amethod%20that%20fuses%20roughly%20aligned%20CBCT%20and%20CT%20scans%20and%20investigate%20the%20effect%0Aof%20CBCT%20quality%20and%20misalignment%20%28affine%20and%20elastic%20transformations%0Afacilitating%20misalignment%29%20on%20the%20final%20segmentation%20performance.%20As%20an%0Aapplication%20scenario%2C%20we%20focus%20on%20the%20segmentation%20of%20liver%20and%20liver%20tumor%0Asemantic%20segmentation%20and%20evaluate%20the%20effect%20of%20intraoperative%20image%20quality%0Aand%20misalignment%20on%20segmentation%20performance.%20To%20accomplish%20this%2C%20high%20quality%2C%0Alabelled%20CTs%20are%20defined%20as%20preoperative%20and%20used%20as%20a%20basis%20to%20simulate%0Aintraoperative%20CBCT.%20We%20show%20that%20the%20fusion%20of%20preoperative%20CT%20and%20simulated%2C%0Aintraoperative%20CBCT%20mostly%20improves%20segmentation%20performance%20and%20that%20even%0Aclearly%20misaligned%20preoperative%20data%20has%20the%20potential%20to%20improve%20segmentation%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11650v1&entry.124074799=Read"},
{"title": "On the Feasibility of Fidelity$^-$ for Graph Pruning", "author": "Yong-Min Shin and Won-Yong Shin", "abstract": "  As one of popular quantitative metrics to assess the quality of explanation\nof graph neural networks (GNNs), fidelity measures the output difference after\nremoving unimportant parts of the input graph. Fidelity has been widely used\ndue to its straightforward interpretation that the underlying model should\nproduce similar predictions when features deemed unimportant from the\nexplanation are removed. This raises a natural question: \"Does fidelity induce\na global (soft) mask for graph pruning?\" To solve this, we aim to explore the\npotential of the fidelity measure to be used for graph pruning, eventually\nenhancing the GNN models for better efficiency. To this end, we propose\nFidelity$^-$-inspired Pruning (FiP), an effective framework to construct global\nedge masks from local explanations. Our empirical observations using 7 edge\nattribution methods demonstrate that, surprisingly, general eXplainable AI\nmethods outperform methods tailored to GNNs in terms of graph pruning\nperformance.\n", "link": "http://arxiv.org/abs/2406.11504v1", "date": "2024-06-17", "relevancy": 2.1808, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4557}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4339}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Feasibility%20of%20Fidelity%24%5E-%24%20for%20Graph%20Pruning&body=Title%3A%20On%20the%20Feasibility%20of%20Fidelity%24%5E-%24%20for%20Graph%20Pruning%0AAuthor%3A%20Yong-Min%20Shin%20and%20Won-Yong%20Shin%0AAbstract%3A%20%20%20As%20one%20of%20popular%20quantitative%20metrics%20to%20assess%20the%20quality%20of%20explanation%0Aof%20graph%20neural%20networks%20%28GNNs%29%2C%20fidelity%20measures%20the%20output%20difference%20after%0Aremoving%20unimportant%20parts%20of%20the%20input%20graph.%20Fidelity%20has%20been%20widely%20used%0Adue%20to%20its%20straightforward%20interpretation%20that%20the%20underlying%20model%20should%0Aproduce%20similar%20predictions%20when%20features%20deemed%20unimportant%20from%20the%0Aexplanation%20are%20removed.%20This%20raises%20a%20natural%20question%3A%20%22Does%20fidelity%20induce%0Aa%20global%20%28soft%29%20mask%20for%20graph%20pruning%3F%22%20To%20solve%20this%2C%20we%20aim%20to%20explore%20the%0Apotential%20of%20the%20fidelity%20measure%20to%20be%20used%20for%20graph%20pruning%2C%20eventually%0Aenhancing%20the%20GNN%20models%20for%20better%20efficiency.%20To%20this%20end%2C%20we%20propose%0AFidelity%24%5E-%24-inspired%20Pruning%20%28FiP%29%2C%20an%20effective%20framework%20to%20construct%20global%0Aedge%20masks%20from%20local%20explanations.%20Our%20empirical%20observations%20using%207%20edge%0Aattribution%20methods%20demonstrate%20that%2C%20surprisingly%2C%20general%20eXplainable%20AI%0Amethods%20outperform%20methods%20tailored%20to%20GNNs%20in%20terms%20of%20graph%20pruning%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11504v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Feasibility%2520of%2520Fidelity%2524%255E-%2524%2520for%2520Graph%2520Pruning%26entry.906535625%3DYong-Min%2520Shin%2520and%2520Won-Yong%2520Shin%26entry.1292438233%3D%2520%2520As%2520one%2520of%2520popular%2520quantitative%2520metrics%2520to%2520assess%2520the%2520quality%2520of%2520explanation%250Aof%2520graph%2520neural%2520networks%2520%2528GNNs%2529%252C%2520fidelity%2520measures%2520the%2520output%2520difference%2520after%250Aremoving%2520unimportant%2520parts%2520of%2520the%2520input%2520graph.%2520Fidelity%2520has%2520been%2520widely%2520used%250Adue%2520to%2520its%2520straightforward%2520interpretation%2520that%2520the%2520underlying%2520model%2520should%250Aproduce%2520similar%2520predictions%2520when%2520features%2520deemed%2520unimportant%2520from%2520the%250Aexplanation%2520are%2520removed.%2520This%2520raises%2520a%2520natural%2520question%253A%2520%2522Does%2520fidelity%2520induce%250Aa%2520global%2520%2528soft%2529%2520mask%2520for%2520graph%2520pruning%253F%2522%2520To%2520solve%2520this%252C%2520we%2520aim%2520to%2520explore%2520the%250Apotential%2520of%2520the%2520fidelity%2520measure%2520to%2520be%2520used%2520for%2520graph%2520pruning%252C%2520eventually%250Aenhancing%2520the%2520GNN%2520models%2520for%2520better%2520efficiency.%2520To%2520this%2520end%252C%2520we%2520propose%250AFidelity%2524%255E-%2524-inspired%2520Pruning%2520%2528FiP%2529%252C%2520an%2520effective%2520framework%2520to%2520construct%2520global%250Aedge%2520masks%2520from%2520local%2520explanations.%2520Our%2520empirical%2520observations%2520using%25207%2520edge%250Aattribution%2520methods%2520demonstrate%2520that%252C%2520surprisingly%252C%2520general%2520eXplainable%2520AI%250Amethods%2520outperform%2520methods%2520tailored%2520to%2520GNNs%2520in%2520terms%2520of%2520graph%2520pruning%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11504v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Feasibility%20of%20Fidelity%24%5E-%24%20for%20Graph%20Pruning&entry.906535625=Yong-Min%20Shin%20and%20Won-Yong%20Shin&entry.1292438233=%20%20As%20one%20of%20popular%20quantitative%20metrics%20to%20assess%20the%20quality%20of%20explanation%0Aof%20graph%20neural%20networks%20%28GNNs%29%2C%20fidelity%20measures%20the%20output%20difference%20after%0Aremoving%20unimportant%20parts%20of%20the%20input%20graph.%20Fidelity%20has%20been%20widely%20used%0Adue%20to%20its%20straightforward%20interpretation%20that%20the%20underlying%20model%20should%0Aproduce%20similar%20predictions%20when%20features%20deemed%20unimportant%20from%20the%0Aexplanation%20are%20removed.%20This%20raises%20a%20natural%20question%3A%20%22Does%20fidelity%20induce%0Aa%20global%20%28soft%29%20mask%20for%20graph%20pruning%3F%22%20To%20solve%20this%2C%20we%20aim%20to%20explore%20the%0Apotential%20of%20the%20fidelity%20measure%20to%20be%20used%20for%20graph%20pruning%2C%20eventually%0Aenhancing%20the%20GNN%20models%20for%20better%20efficiency.%20To%20this%20end%2C%20we%20propose%0AFidelity%24%5E-%24-inspired%20Pruning%20%28FiP%29%2C%20an%20effective%20framework%20to%20construct%20global%0Aedge%20masks%20from%20local%20explanations.%20Our%20empirical%20observations%20using%207%20edge%0Aattribution%20methods%20demonstrate%20that%2C%20surprisingly%2C%20general%20eXplainable%20AI%0Amethods%20outperform%20methods%20tailored%20to%20GNNs%20in%20terms%20of%20graph%20pruning%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11504v1&entry.124074799=Read"},
{"title": "Probabilistic Constrained Reinforcement Learning with Formal\n  Interpretability", "author": "Yanran Wang and Qiuchen Qian and David Boyle", "abstract": "  Reinforcement learning can provide effective reasoning for sequential\ndecision-making problems with variable dynamics. Such reasoning in practical\nimplementation, however, poses a persistent challenge in interpreting the\nreward function and the corresponding optimal policy. Consequently,\nrepresenting sequential decision-making problems as probabilistic inference can\nhave considerable value, as, in principle, the inference offers diverse and\npowerful mathematical tools to infer the stochastic dynamics whilst suggesting\na probabilistic interpretation of policy optimization. In this study, we\npropose a novel Adaptive Wasserstein Variational Optimization, namely AWaVO, to\ntackle these interpretability challenges. Our approach uses formal methods to\nachieve the interpretability for convergence guarantee, training transparency,\nand intrinsic decision-interpretation. To demonstrate its practicality, we\nshowcase guaranteed interpretability with an optimal global convergence rate in\nsimulation and in practical quadrotor tasks. In comparison with\nstate-of-the-art benchmarks including TRPO-IPO, PCPO and CRPO, we empirically\nverify that AWaVO offers a reasonable trade-off between high performance and\nsufficient interpretability.\n", "link": "http://arxiv.org/abs/2307.07084v4", "date": "2024-06-17", "relevancy": 2.1788, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5832}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.542}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5319}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Constrained%20Reinforcement%20Learning%20with%20Formal%0A%20%20Interpretability&body=Title%3A%20Probabilistic%20Constrained%20Reinforcement%20Learning%20with%20Formal%0A%20%20Interpretability%0AAuthor%3A%20Yanran%20Wang%20and%20Qiuchen%20Qian%20and%20David%20Boyle%0AAbstract%3A%20%20%20Reinforcement%20learning%20can%20provide%20effective%20reasoning%20for%20sequential%0Adecision-making%20problems%20with%20variable%20dynamics.%20Such%20reasoning%20in%20practical%0Aimplementation%2C%20however%2C%20poses%20a%20persistent%20challenge%20in%20interpreting%20the%0Areward%20function%20and%20the%20corresponding%20optimal%20policy.%20Consequently%2C%0Arepresenting%20sequential%20decision-making%20problems%20as%20probabilistic%20inference%20can%0Ahave%20considerable%20value%2C%20as%2C%20in%20principle%2C%20the%20inference%20offers%20diverse%20and%0Apowerful%20mathematical%20tools%20to%20infer%20the%20stochastic%20dynamics%20whilst%20suggesting%0Aa%20probabilistic%20interpretation%20of%20policy%20optimization.%20In%20this%20study%2C%20we%0Apropose%20a%20novel%20Adaptive%20Wasserstein%20Variational%20Optimization%2C%20namely%20AWaVO%2C%20to%0Atackle%20these%20interpretability%20challenges.%20Our%20approach%20uses%20formal%20methods%20to%0Aachieve%20the%20interpretability%20for%20convergence%20guarantee%2C%20training%20transparency%2C%0Aand%20intrinsic%20decision-interpretation.%20To%20demonstrate%20its%20practicality%2C%20we%0Ashowcase%20guaranteed%20interpretability%20with%20an%20optimal%20global%20convergence%20rate%20in%0Asimulation%20and%20in%20practical%20quadrotor%20tasks.%20In%20comparison%20with%0Astate-of-the-art%20benchmarks%20including%20TRPO-IPO%2C%20PCPO%20and%20CRPO%2C%20we%20empirically%0Averify%20that%20AWaVO%20offers%20a%20reasonable%20trade-off%20between%20high%20performance%20and%0Asufficient%20interpretability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.07084v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProbabilistic%2520Constrained%2520Reinforcement%2520Learning%2520with%2520Formal%250A%2520%2520Interpretability%26entry.906535625%3DYanran%2520Wang%2520and%2520Qiuchen%2520Qian%2520and%2520David%2520Boyle%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520can%2520provide%2520effective%2520reasoning%2520for%2520sequential%250Adecision-making%2520problems%2520with%2520variable%2520dynamics.%2520Such%2520reasoning%2520in%2520practical%250Aimplementation%252C%2520however%252C%2520poses%2520a%2520persistent%2520challenge%2520in%2520interpreting%2520the%250Areward%2520function%2520and%2520the%2520corresponding%2520optimal%2520policy.%2520Consequently%252C%250Arepresenting%2520sequential%2520decision-making%2520problems%2520as%2520probabilistic%2520inference%2520can%250Ahave%2520considerable%2520value%252C%2520as%252C%2520in%2520principle%252C%2520the%2520inference%2520offers%2520diverse%2520and%250Apowerful%2520mathematical%2520tools%2520to%2520infer%2520the%2520stochastic%2520dynamics%2520whilst%2520suggesting%250Aa%2520probabilistic%2520interpretation%2520of%2520policy%2520optimization.%2520In%2520this%2520study%252C%2520we%250Apropose%2520a%2520novel%2520Adaptive%2520Wasserstein%2520Variational%2520Optimization%252C%2520namely%2520AWaVO%252C%2520to%250Atackle%2520these%2520interpretability%2520challenges.%2520Our%2520approach%2520uses%2520formal%2520methods%2520to%250Aachieve%2520the%2520interpretability%2520for%2520convergence%2520guarantee%252C%2520training%2520transparency%252C%250Aand%2520intrinsic%2520decision-interpretation.%2520To%2520demonstrate%2520its%2520practicality%252C%2520we%250Ashowcase%2520guaranteed%2520interpretability%2520with%2520an%2520optimal%2520global%2520convergence%2520rate%2520in%250Asimulation%2520and%2520in%2520practical%2520quadrotor%2520tasks.%2520In%2520comparison%2520with%250Astate-of-the-art%2520benchmarks%2520including%2520TRPO-IPO%252C%2520PCPO%2520and%2520CRPO%252C%2520we%2520empirically%250Averify%2520that%2520AWaVO%2520offers%2520a%2520reasonable%2520trade-off%2520between%2520high%2520performance%2520and%250Asufficient%2520interpretability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.07084v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Constrained%20Reinforcement%20Learning%20with%20Formal%0A%20%20Interpretability&entry.906535625=Yanran%20Wang%20and%20Qiuchen%20Qian%20and%20David%20Boyle&entry.1292438233=%20%20Reinforcement%20learning%20can%20provide%20effective%20reasoning%20for%20sequential%0Adecision-making%20problems%20with%20variable%20dynamics.%20Such%20reasoning%20in%20practical%0Aimplementation%2C%20however%2C%20poses%20a%20persistent%20challenge%20in%20interpreting%20the%0Areward%20function%20and%20the%20corresponding%20optimal%20policy.%20Consequently%2C%0Arepresenting%20sequential%20decision-making%20problems%20as%20probabilistic%20inference%20can%0Ahave%20considerable%20value%2C%20as%2C%20in%20principle%2C%20the%20inference%20offers%20diverse%20and%0Apowerful%20mathematical%20tools%20to%20infer%20the%20stochastic%20dynamics%20whilst%20suggesting%0Aa%20probabilistic%20interpretation%20of%20policy%20optimization.%20In%20this%20study%2C%20we%0Apropose%20a%20novel%20Adaptive%20Wasserstein%20Variational%20Optimization%2C%20namely%20AWaVO%2C%20to%0Atackle%20these%20interpretability%20challenges.%20Our%20approach%20uses%20formal%20methods%20to%0Aachieve%20the%20interpretability%20for%20convergence%20guarantee%2C%20training%20transparency%2C%0Aand%20intrinsic%20decision-interpretation.%20To%20demonstrate%20its%20practicality%2C%20we%0Ashowcase%20guaranteed%20interpretability%20with%20an%20optimal%20global%20convergence%20rate%20in%0Asimulation%20and%20in%20practical%20quadrotor%20tasks.%20In%20comparison%20with%0Astate-of-the-art%20benchmarks%20including%20TRPO-IPO%2C%20PCPO%20and%20CRPO%2C%20we%20empirically%0Averify%20that%20AWaVO%20offers%20a%20reasonable%20trade-off%20between%20high%20performance%20and%0Asufficient%20interpretability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.07084v4&entry.124074799=Read"},
{"title": "ART: Automatic Red-teaming for Text-to-Image Models to Protect Benign\n  Users", "author": "Guanlin Li and Kangjie Chen and Shudong Zhang and Jie Zhang and Tianwei Zhang", "abstract": "  Large-scale pre-trained generative models are taking the world by storm, due\nto their abilities in generating creative content. Meanwhile, safeguards for\nthese generative models are developed, to protect users' rights and safety,\nmost of which are designed for large language models. Existing methods\nprimarily focus on jailbreak and adversarial attacks, which mainly evaluate the\nmodel's safety under malicious prompts. Recent work found that manually crafted\nsafe prompts can unintentionally trigger unsafe generations. To further\nsystematically evaluate the safety risks of text-to-image models, we propose a\nnovel Automatic Red-Teaming framework, ART. Our method leverages both vision\nlanguage model and large language model to establish a connection between\nunsafe generations and their prompts, thereby more efficiently identifying the\nmodel's vulnerabilities. With our comprehensive experiments, we reveal the\ntoxicity of the popular open-source text-to-image models. The experiments also\nvalidate the effectiveness, adaptability, and great diversity of ART.\nAdditionally, we introduce three large-scale red-teaming datasets for studying\nthe safety risks associated with text-to-image models. Datasets and models can\nbe found in https://github.com/GuanlinLee/ART.\n", "link": "http://arxiv.org/abs/2405.19360v2", "date": "2024-06-17", "relevancy": 2.1762, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5737}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5403}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ART%3A%20Automatic%20Red-teaming%20for%20Text-to-Image%20Models%20to%20Protect%20Benign%0A%20%20Users&body=Title%3A%20ART%3A%20Automatic%20Red-teaming%20for%20Text-to-Image%20Models%20to%20Protect%20Benign%0A%20%20Users%0AAuthor%3A%20Guanlin%20Li%20and%20Kangjie%20Chen%20and%20Shudong%20Zhang%20and%20Jie%20Zhang%20and%20Tianwei%20Zhang%0AAbstract%3A%20%20%20Large-scale%20pre-trained%20generative%20models%20are%20taking%20the%20world%20by%20storm%2C%20due%0Ato%20their%20abilities%20in%20generating%20creative%20content.%20Meanwhile%2C%20safeguards%20for%0Athese%20generative%20models%20are%20developed%2C%20to%20protect%20users%27%20rights%20and%20safety%2C%0Amost%20of%20which%20are%20designed%20for%20large%20language%20models.%20Existing%20methods%0Aprimarily%20focus%20on%20jailbreak%20and%20adversarial%20attacks%2C%20which%20mainly%20evaluate%20the%0Amodel%27s%20safety%20under%20malicious%20prompts.%20Recent%20work%20found%20that%20manually%20crafted%0Asafe%20prompts%20can%20unintentionally%20trigger%20unsafe%20generations.%20To%20further%0Asystematically%20evaluate%20the%20safety%20risks%20of%20text-to-image%20models%2C%20we%20propose%20a%0Anovel%20Automatic%20Red-Teaming%20framework%2C%20ART.%20Our%20method%20leverages%20both%20vision%0Alanguage%20model%20and%20large%20language%20model%20to%20establish%20a%20connection%20between%0Aunsafe%20generations%20and%20their%20prompts%2C%20thereby%20more%20efficiently%20identifying%20the%0Amodel%27s%20vulnerabilities.%20With%20our%20comprehensive%20experiments%2C%20we%20reveal%20the%0Atoxicity%20of%20the%20popular%20open-source%20text-to-image%20models.%20The%20experiments%20also%0Avalidate%20the%20effectiveness%2C%20adaptability%2C%20and%20great%20diversity%20of%20ART.%0AAdditionally%2C%20we%20introduce%20three%20large-scale%20red-teaming%20datasets%20for%20studying%0Athe%20safety%20risks%20associated%20with%20text-to-image%20models.%20Datasets%20and%20models%20can%0Abe%20found%20in%20https%3A//github.com/GuanlinLee/ART.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19360v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DART%253A%2520Automatic%2520Red-teaming%2520for%2520Text-to-Image%2520Models%2520to%2520Protect%2520Benign%250A%2520%2520Users%26entry.906535625%3DGuanlin%2520Li%2520and%2520Kangjie%2520Chen%2520and%2520Shudong%2520Zhang%2520and%2520Jie%2520Zhang%2520and%2520Tianwei%2520Zhang%26entry.1292438233%3D%2520%2520Large-scale%2520pre-trained%2520generative%2520models%2520are%2520taking%2520the%2520world%2520by%2520storm%252C%2520due%250Ato%2520their%2520abilities%2520in%2520generating%2520creative%2520content.%2520Meanwhile%252C%2520safeguards%2520for%250Athese%2520generative%2520models%2520are%2520developed%252C%2520to%2520protect%2520users%2527%2520rights%2520and%2520safety%252C%250Amost%2520of%2520which%2520are%2520designed%2520for%2520large%2520language%2520models.%2520Existing%2520methods%250Aprimarily%2520focus%2520on%2520jailbreak%2520and%2520adversarial%2520attacks%252C%2520which%2520mainly%2520evaluate%2520the%250Amodel%2527s%2520safety%2520under%2520malicious%2520prompts.%2520Recent%2520work%2520found%2520that%2520manually%2520crafted%250Asafe%2520prompts%2520can%2520unintentionally%2520trigger%2520unsafe%2520generations.%2520To%2520further%250Asystematically%2520evaluate%2520the%2520safety%2520risks%2520of%2520text-to-image%2520models%252C%2520we%2520propose%2520a%250Anovel%2520Automatic%2520Red-Teaming%2520framework%252C%2520ART.%2520Our%2520method%2520leverages%2520both%2520vision%250Alanguage%2520model%2520and%2520large%2520language%2520model%2520to%2520establish%2520a%2520connection%2520between%250Aunsafe%2520generations%2520and%2520their%2520prompts%252C%2520thereby%2520more%2520efficiently%2520identifying%2520the%250Amodel%2527s%2520vulnerabilities.%2520With%2520our%2520comprehensive%2520experiments%252C%2520we%2520reveal%2520the%250Atoxicity%2520of%2520the%2520popular%2520open-source%2520text-to-image%2520models.%2520The%2520experiments%2520also%250Avalidate%2520the%2520effectiveness%252C%2520adaptability%252C%2520and%2520great%2520diversity%2520of%2520ART.%250AAdditionally%252C%2520we%2520introduce%2520three%2520large-scale%2520red-teaming%2520datasets%2520for%2520studying%250Athe%2520safety%2520risks%2520associated%2520with%2520text-to-image%2520models.%2520Datasets%2520and%2520models%2520can%250Abe%2520found%2520in%2520https%253A//github.com/GuanlinLee/ART.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19360v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ART%3A%20Automatic%20Red-teaming%20for%20Text-to-Image%20Models%20to%20Protect%20Benign%0A%20%20Users&entry.906535625=Guanlin%20Li%20and%20Kangjie%20Chen%20and%20Shudong%20Zhang%20and%20Jie%20Zhang%20and%20Tianwei%20Zhang&entry.1292438233=%20%20Large-scale%20pre-trained%20generative%20models%20are%20taking%20the%20world%20by%20storm%2C%20due%0Ato%20their%20abilities%20in%20generating%20creative%20content.%20Meanwhile%2C%20safeguards%20for%0Athese%20generative%20models%20are%20developed%2C%20to%20protect%20users%27%20rights%20and%20safety%2C%0Amost%20of%20which%20are%20designed%20for%20large%20language%20models.%20Existing%20methods%0Aprimarily%20focus%20on%20jailbreak%20and%20adversarial%20attacks%2C%20which%20mainly%20evaluate%20the%0Amodel%27s%20safety%20under%20malicious%20prompts.%20Recent%20work%20found%20that%20manually%20crafted%0Asafe%20prompts%20can%20unintentionally%20trigger%20unsafe%20generations.%20To%20further%0Asystematically%20evaluate%20the%20safety%20risks%20of%20text-to-image%20models%2C%20we%20propose%20a%0Anovel%20Automatic%20Red-Teaming%20framework%2C%20ART.%20Our%20method%20leverages%20both%20vision%0Alanguage%20model%20and%20large%20language%20model%20to%20establish%20a%20connection%20between%0Aunsafe%20generations%20and%20their%20prompts%2C%20thereby%20more%20efficiently%20identifying%20the%0Amodel%27s%20vulnerabilities.%20With%20our%20comprehensive%20experiments%2C%20we%20reveal%20the%0Atoxicity%20of%20the%20popular%20open-source%20text-to-image%20models.%20The%20experiments%20also%0Avalidate%20the%20effectiveness%2C%20adaptability%2C%20and%20great%20diversity%20of%20ART.%0AAdditionally%2C%20we%20introduce%20three%20large-scale%20red-teaming%20datasets%20for%20studying%0Athe%20safety%20risks%20associated%20with%20text-to-image%20models.%20Datasets%20and%20models%20can%0Abe%20found%20in%20https%3A//github.com/GuanlinLee/ART.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19360v2&entry.124074799=Read"},
{"title": "Topology-aware Federated Learning in Edge Computing: A Comprehensive\n  Survey", "author": "Jiajun Wu and Steve Drew and Fan Dong and Zhuangdi Zhu and Jiayu Zhou", "abstract": "  The ultra-low latency requirements of 5G/6G applications and privacy\nconstraints call for distributed machine learning systems to be deployed at the\nedge. With its simple yet effective approach, federated learning (FL) is a\nnatural solution for massive user-owned devices in edge computing with\ndistributed and private training data. FL methods based on FedAvg typically\nfollow a naive star topology, ignoring the heterogeneity and hierarchy of the\nvolatile edge computing architectures and topologies in reality. Several other\nnetwork topologies exist and can address the limitations and bottlenecks of the\nstar topology. This motivates us to survey network topology-related FL\nsolutions. In this paper, we conduct a comprehensive survey of the existing FL\nworks focusing on network topologies. After a brief overview of FL and edge\ncomputing networks, we discuss various edge network topologies as well as their\nadvantages and disadvantages. Lastly, we discuss the remaining challenges and\nfuture works for applying FL to topology-specific edge networks.\n", "link": "http://arxiv.org/abs/2302.02573v2", "date": "2024-06-17", "relevancy": 2.1731, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4596}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4286}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4156}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topology-aware%20Federated%20Learning%20in%20Edge%20Computing%3A%20A%20Comprehensive%0A%20%20Survey&body=Title%3A%20Topology-aware%20Federated%20Learning%20in%20Edge%20Computing%3A%20A%20Comprehensive%0A%20%20Survey%0AAuthor%3A%20Jiajun%20Wu%20and%20Steve%20Drew%20and%20Fan%20Dong%20and%20Zhuangdi%20Zhu%20and%20Jiayu%20Zhou%0AAbstract%3A%20%20%20The%20ultra-low%20latency%20requirements%20of%205G/6G%20applications%20and%20privacy%0Aconstraints%20call%20for%20distributed%20machine%20learning%20systems%20to%20be%20deployed%20at%20the%0Aedge.%20With%20its%20simple%20yet%20effective%20approach%2C%20federated%20learning%20%28FL%29%20is%20a%0Anatural%20solution%20for%20massive%20user-owned%20devices%20in%20edge%20computing%20with%0Adistributed%20and%20private%20training%20data.%20FL%20methods%20based%20on%20FedAvg%20typically%0Afollow%20a%20naive%20star%20topology%2C%20ignoring%20the%20heterogeneity%20and%20hierarchy%20of%20the%0Avolatile%20edge%20computing%20architectures%20and%20topologies%20in%20reality.%20Several%20other%0Anetwork%20topologies%20exist%20and%20can%20address%20the%20limitations%20and%20bottlenecks%20of%20the%0Astar%20topology.%20This%20motivates%20us%20to%20survey%20network%20topology-related%20FL%0Asolutions.%20In%20this%20paper%2C%20we%20conduct%20a%20comprehensive%20survey%20of%20the%20existing%20FL%0Aworks%20focusing%20on%20network%20topologies.%20After%20a%20brief%20overview%20of%20FL%20and%20edge%0Acomputing%20networks%2C%20we%20discuss%20various%20edge%20network%20topologies%20as%20well%20as%20their%0Aadvantages%20and%20disadvantages.%20Lastly%2C%20we%20discuss%20the%20remaining%20challenges%20and%0Afuture%20works%20for%20applying%20FL%20to%20topology-specific%20edge%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.02573v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopology-aware%2520Federated%2520Learning%2520in%2520Edge%2520Computing%253A%2520A%2520Comprehensive%250A%2520%2520Survey%26entry.906535625%3DJiajun%2520Wu%2520and%2520Steve%2520Drew%2520and%2520Fan%2520Dong%2520and%2520Zhuangdi%2520Zhu%2520and%2520Jiayu%2520Zhou%26entry.1292438233%3D%2520%2520The%2520ultra-low%2520latency%2520requirements%2520of%25205G/6G%2520applications%2520and%2520privacy%250Aconstraints%2520call%2520for%2520distributed%2520machine%2520learning%2520systems%2520to%2520be%2520deployed%2520at%2520the%250Aedge.%2520With%2520its%2520simple%2520yet%2520effective%2520approach%252C%2520federated%2520learning%2520%2528FL%2529%2520is%2520a%250Anatural%2520solution%2520for%2520massive%2520user-owned%2520devices%2520in%2520edge%2520computing%2520with%250Adistributed%2520and%2520private%2520training%2520data.%2520FL%2520methods%2520based%2520on%2520FedAvg%2520typically%250Afollow%2520a%2520naive%2520star%2520topology%252C%2520ignoring%2520the%2520heterogeneity%2520and%2520hierarchy%2520of%2520the%250Avolatile%2520edge%2520computing%2520architectures%2520and%2520topologies%2520in%2520reality.%2520Several%2520other%250Anetwork%2520topologies%2520exist%2520and%2520can%2520address%2520the%2520limitations%2520and%2520bottlenecks%2520of%2520the%250Astar%2520topology.%2520This%2520motivates%2520us%2520to%2520survey%2520network%2520topology-related%2520FL%250Asolutions.%2520In%2520this%2520paper%252C%2520we%2520conduct%2520a%2520comprehensive%2520survey%2520of%2520the%2520existing%2520FL%250Aworks%2520focusing%2520on%2520network%2520topologies.%2520After%2520a%2520brief%2520overview%2520of%2520FL%2520and%2520edge%250Acomputing%2520networks%252C%2520we%2520discuss%2520various%2520edge%2520network%2520topologies%2520as%2520well%2520as%2520their%250Aadvantages%2520and%2520disadvantages.%2520Lastly%252C%2520we%2520discuss%2520the%2520remaining%2520challenges%2520and%250Afuture%2520works%2520for%2520applying%2520FL%2520to%2520topology-specific%2520edge%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.02573v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topology-aware%20Federated%20Learning%20in%20Edge%20Computing%3A%20A%20Comprehensive%0A%20%20Survey&entry.906535625=Jiajun%20Wu%20and%20Steve%20Drew%20and%20Fan%20Dong%20and%20Zhuangdi%20Zhu%20and%20Jiayu%20Zhou&entry.1292438233=%20%20The%20ultra-low%20latency%20requirements%20of%205G/6G%20applications%20and%20privacy%0Aconstraints%20call%20for%20distributed%20machine%20learning%20systems%20to%20be%20deployed%20at%20the%0Aedge.%20With%20its%20simple%20yet%20effective%20approach%2C%20federated%20learning%20%28FL%29%20is%20a%0Anatural%20solution%20for%20massive%20user-owned%20devices%20in%20edge%20computing%20with%0Adistributed%20and%20private%20training%20data.%20FL%20methods%20based%20on%20FedAvg%20typically%0Afollow%20a%20naive%20star%20topology%2C%20ignoring%20the%20heterogeneity%20and%20hierarchy%20of%20the%0Avolatile%20edge%20computing%20architectures%20and%20topologies%20in%20reality.%20Several%20other%0Anetwork%20topologies%20exist%20and%20can%20address%20the%20limitations%20and%20bottlenecks%20of%20the%0Astar%20topology.%20This%20motivates%20us%20to%20survey%20network%20topology-related%20FL%0Asolutions.%20In%20this%20paper%2C%20we%20conduct%20a%20comprehensive%20survey%20of%20the%20existing%20FL%0Aworks%20focusing%20on%20network%20topologies.%20After%20a%20brief%20overview%20of%20FL%20and%20edge%0Acomputing%20networks%2C%20we%20discuss%20various%20edge%20network%20topologies%20as%20well%20as%20their%0Aadvantages%20and%20disadvantages.%20Lastly%2C%20we%20discuss%20the%20remaining%20challenges%20and%0Afuture%20works%20for%20applying%20FL%20to%20topology-specific%20edge%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.02573v2&entry.124074799=Read"},
{"title": "Exploring the Benefits of Vision Foundation Models for Unsupervised\n  Domain Adaptation", "author": "Brun\u00f3 B. Englert and Fabrizio J. Piva and Tommie Kerssies and Daan de Geus and Gijs Dubbelman", "abstract": "  Achieving robust generalization across diverse data domains remains a\nsignificant challenge in computer vision. This challenge is important in\nsafety-critical applications, where deep-neural-network-based systems must\nperform reliably under various environmental conditions not seen during\ntraining. Our study investigates whether the generalization capabilities of\nVision Foundation Models (VFMs) and Unsupervised Domain Adaptation (UDA)\nmethods for the semantic segmentation task are complementary. Results show that\ncombining VFMs with UDA has two main benefits: (a) it allows for better UDA\nperformance while maintaining the out-of-distribution performance of VFMs, and\n(b) it makes certain time-consuming UDA components redundant, thus enabling\nsignificant inference speedups. Specifically, with equivalent model sizes, the\nresulting VFM-UDA method achieves an 8.4$\\times$ speed increase over the prior\nnon-VFM state of the art, while also improving performance by +1.2 mIoU in the\nUDA setting and by +6.1 mIoU in terms of out-of-distribution generalization.\nMoreover, when we use a VFM with 3.6$\\times$ more parameters, the VFM-UDA\napproach maintains a 3.3$\\times$ speed up, while improving the UDA performance\nby +3.1 mIoU and the out-of-distribution performance by +10.3 mIoU. These\nresults underscore the significant benefits of combining VFMs with UDA, setting\nnew standards and baselines for Unsupervised Domain Adaptation in semantic\nsegmentation.\n", "link": "http://arxiv.org/abs/2406.09896v2", "date": "2024-06-17", "relevancy": 2.1731, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5447}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5437}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5416}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Benefits%20of%20Vision%20Foundation%20Models%20for%20Unsupervised%0A%20%20Domain%20Adaptation&body=Title%3A%20Exploring%20the%20Benefits%20of%20Vision%20Foundation%20Models%20for%20Unsupervised%0A%20%20Domain%20Adaptation%0AAuthor%3A%20Brun%C3%B3%20B.%20Englert%20and%20Fabrizio%20J.%20Piva%20and%20Tommie%20Kerssies%20and%20Daan%20de%20Geus%20and%20Gijs%20Dubbelman%0AAbstract%3A%20%20%20Achieving%20robust%20generalization%20across%20diverse%20data%20domains%20remains%20a%0Asignificant%20challenge%20in%20computer%20vision.%20This%20challenge%20is%20important%20in%0Asafety-critical%20applications%2C%20where%20deep-neural-network-based%20systems%20must%0Aperform%20reliably%20under%20various%20environmental%20conditions%20not%20seen%20during%0Atraining.%20Our%20study%20investigates%20whether%20the%20generalization%20capabilities%20of%0AVision%20Foundation%20Models%20%28VFMs%29%20and%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%0Amethods%20for%20the%20semantic%20segmentation%20task%20are%20complementary.%20Results%20show%20that%0Acombining%20VFMs%20with%20UDA%20has%20two%20main%20benefits%3A%20%28a%29%20it%20allows%20for%20better%20UDA%0Aperformance%20while%20maintaining%20the%20out-of-distribution%20performance%20of%20VFMs%2C%20and%0A%28b%29%20it%20makes%20certain%20time-consuming%20UDA%20components%20redundant%2C%20thus%20enabling%0Asignificant%20inference%20speedups.%20Specifically%2C%20with%20equivalent%20model%20sizes%2C%20the%0Aresulting%20VFM-UDA%20method%20achieves%20an%208.4%24%5Ctimes%24%20speed%20increase%20over%20the%20prior%0Anon-VFM%20state%20of%20the%20art%2C%20while%20also%20improving%20performance%20by%20%2B1.2%20mIoU%20in%20the%0AUDA%20setting%20and%20by%20%2B6.1%20mIoU%20in%20terms%20of%20out-of-distribution%20generalization.%0AMoreover%2C%20when%20we%20use%20a%20VFM%20with%203.6%24%5Ctimes%24%20more%20parameters%2C%20the%20VFM-UDA%0Aapproach%20maintains%20a%203.3%24%5Ctimes%24%20speed%20up%2C%20while%20improving%20the%20UDA%20performance%0Aby%20%2B3.1%20mIoU%20and%20the%20out-of-distribution%20performance%20by%20%2B10.3%20mIoU.%20These%0Aresults%20underscore%20the%20significant%20benefits%20of%20combining%20VFMs%20with%20UDA%2C%20setting%0Anew%20standards%20and%20baselines%20for%20Unsupervised%20Domain%20Adaptation%20in%20semantic%0Asegmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09896v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Benefits%2520of%2520Vision%2520Foundation%2520Models%2520for%2520Unsupervised%250A%2520%2520Domain%2520Adaptation%26entry.906535625%3DBrun%25C3%25B3%2520B.%2520Englert%2520and%2520Fabrizio%2520J.%2520Piva%2520and%2520Tommie%2520Kerssies%2520and%2520Daan%2520de%2520Geus%2520and%2520Gijs%2520Dubbelman%26entry.1292438233%3D%2520%2520Achieving%2520robust%2520generalization%2520across%2520diverse%2520data%2520domains%2520remains%2520a%250Asignificant%2520challenge%2520in%2520computer%2520vision.%2520This%2520challenge%2520is%2520important%2520in%250Asafety-critical%2520applications%252C%2520where%2520deep-neural-network-based%2520systems%2520must%250Aperform%2520reliably%2520under%2520various%2520environmental%2520conditions%2520not%2520seen%2520during%250Atraining.%2520Our%2520study%2520investigates%2520whether%2520the%2520generalization%2520capabilities%2520of%250AVision%2520Foundation%2520Models%2520%2528VFMs%2529%2520and%2520Unsupervised%2520Domain%2520Adaptation%2520%2528UDA%2529%250Amethods%2520for%2520the%2520semantic%2520segmentation%2520task%2520are%2520complementary.%2520Results%2520show%2520that%250Acombining%2520VFMs%2520with%2520UDA%2520has%2520two%2520main%2520benefits%253A%2520%2528a%2529%2520it%2520allows%2520for%2520better%2520UDA%250Aperformance%2520while%2520maintaining%2520the%2520out-of-distribution%2520performance%2520of%2520VFMs%252C%2520and%250A%2528b%2529%2520it%2520makes%2520certain%2520time-consuming%2520UDA%2520components%2520redundant%252C%2520thus%2520enabling%250Asignificant%2520inference%2520speedups.%2520Specifically%252C%2520with%2520equivalent%2520model%2520sizes%252C%2520the%250Aresulting%2520VFM-UDA%2520method%2520achieves%2520an%25208.4%2524%255Ctimes%2524%2520speed%2520increase%2520over%2520the%2520prior%250Anon-VFM%2520state%2520of%2520the%2520art%252C%2520while%2520also%2520improving%2520performance%2520by%2520%252B1.2%2520mIoU%2520in%2520the%250AUDA%2520setting%2520and%2520by%2520%252B6.1%2520mIoU%2520in%2520terms%2520of%2520out-of-distribution%2520generalization.%250AMoreover%252C%2520when%2520we%2520use%2520a%2520VFM%2520with%25203.6%2524%255Ctimes%2524%2520more%2520parameters%252C%2520the%2520VFM-UDA%250Aapproach%2520maintains%2520a%25203.3%2524%255Ctimes%2524%2520speed%2520up%252C%2520while%2520improving%2520the%2520UDA%2520performance%250Aby%2520%252B3.1%2520mIoU%2520and%2520the%2520out-of-distribution%2520performance%2520by%2520%252B10.3%2520mIoU.%2520These%250Aresults%2520underscore%2520the%2520significant%2520benefits%2520of%2520combining%2520VFMs%2520with%2520UDA%252C%2520setting%250Anew%2520standards%2520and%2520baselines%2520for%2520Unsupervised%2520Domain%2520Adaptation%2520in%2520semantic%250Asegmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09896v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Benefits%20of%20Vision%20Foundation%20Models%20for%20Unsupervised%0A%20%20Domain%20Adaptation&entry.906535625=Brun%C3%B3%20B.%20Englert%20and%20Fabrizio%20J.%20Piva%20and%20Tommie%20Kerssies%20and%20Daan%20de%20Geus%20and%20Gijs%20Dubbelman&entry.1292438233=%20%20Achieving%20robust%20generalization%20across%20diverse%20data%20domains%20remains%20a%0Asignificant%20challenge%20in%20computer%20vision.%20This%20challenge%20is%20important%20in%0Asafety-critical%20applications%2C%20where%20deep-neural-network-based%20systems%20must%0Aperform%20reliably%20under%20various%20environmental%20conditions%20not%20seen%20during%0Atraining.%20Our%20study%20investigates%20whether%20the%20generalization%20capabilities%20of%0AVision%20Foundation%20Models%20%28VFMs%29%20and%20Unsupervised%20Domain%20Adaptation%20%28UDA%29%0Amethods%20for%20the%20semantic%20segmentation%20task%20are%20complementary.%20Results%20show%20that%0Acombining%20VFMs%20with%20UDA%20has%20two%20main%20benefits%3A%20%28a%29%20it%20allows%20for%20better%20UDA%0Aperformance%20while%20maintaining%20the%20out-of-distribution%20performance%20of%20VFMs%2C%20and%0A%28b%29%20it%20makes%20certain%20time-consuming%20UDA%20components%20redundant%2C%20thus%20enabling%0Asignificant%20inference%20speedups.%20Specifically%2C%20with%20equivalent%20model%20sizes%2C%20the%0Aresulting%20VFM-UDA%20method%20achieves%20an%208.4%24%5Ctimes%24%20speed%20increase%20over%20the%20prior%0Anon-VFM%20state%20of%20the%20art%2C%20while%20also%20improving%20performance%20by%20%2B1.2%20mIoU%20in%20the%0AUDA%20setting%20and%20by%20%2B6.1%20mIoU%20in%20terms%20of%20out-of-distribution%20generalization.%0AMoreover%2C%20when%20we%20use%20a%20VFM%20with%203.6%24%5Ctimes%24%20more%20parameters%2C%20the%20VFM-UDA%0Aapproach%20maintains%20a%203.3%24%5Ctimes%24%20speed%20up%2C%20while%20improving%20the%20UDA%20performance%0Aby%20%2B3.1%20mIoU%20and%20the%20out-of-distribution%20performance%20by%20%2B10.3%20mIoU.%20These%0Aresults%20underscore%20the%20significant%20benefits%20of%20combining%20VFMs%20with%20UDA%2C%20setting%0Anew%20standards%20and%20baselines%20for%20Unsupervised%20Domain%20Adaptation%20in%20semantic%0Asegmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09896v2&entry.124074799=Read"},
{"title": "Ovis: Structural Embedding Alignment for Multimodal Large Language Model", "author": "Shiyin Lu and Yang Li and Qing-Guo Chen and Zhao Xu and Weihua Luo and Kaifu Zhang and Han-Jia Ye", "abstract": "  Current Multimodal Large Language Models (MLLMs) typically integrate a\npre-trained LLM with another pre-trained vision transformer through a\nconnector, such as an MLP, endowing the LLM with visual capabilities. However,\nthe misalignment between two embedding strategies in MLLMs -- the structural\ntextual embeddings based on an embedding look-up table and the continuous\nembeddings generated directly by the vision encoder -- makes challenges for a\nmore seamless fusion of visual and textual information. We propose Ovis, a\nnovel MLLM architecture designed to structurally align visual and textual\nembeddings. Ovis integrates an additional learnable visual embedding table into\nthe visual encoder's process. To capture rich visual semantics, each image\npatch indexes the visual embedding table multiple times, resulting in a final\nvisual embedding that is a probabilistic combination of the indexed embeddings.\nThis structural approach mirrors the method used for generating textual\nembeddings. Empirical evaluations on various multimodal benchmarks show that\nOvis outperforms open-source MLLMs of similar parameter scales and even\nsurpasses the proprietary model Qwen-VL-Plus overall. These results highlight\nthe potential of Ovis' structured visual representation for advancing MLLM\narchitectural design and promoting more effective multimodal learning. Code,\ndatasets, and models are available at https://github.com/AIDC-AI/Ovis.\n", "link": "http://arxiv.org/abs/2405.20797v2", "date": "2024-06-17", "relevancy": 2.1664, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5923}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5058}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ovis%3A%20Structural%20Embedding%20Alignment%20for%20Multimodal%20Large%20Language%20Model&body=Title%3A%20Ovis%3A%20Structural%20Embedding%20Alignment%20for%20Multimodal%20Large%20Language%20Model%0AAuthor%3A%20Shiyin%20Lu%20and%20Yang%20Li%20and%20Qing-Guo%20Chen%20and%20Zhao%20Xu%20and%20Weihua%20Luo%20and%20Kaifu%20Zhang%20and%20Han-Jia%20Ye%0AAbstract%3A%20%20%20Current%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20typically%20integrate%20a%0Apre-trained%20LLM%20with%20another%20pre-trained%20vision%20transformer%20through%20a%0Aconnector%2C%20such%20as%20an%20MLP%2C%20endowing%20the%20LLM%20with%20visual%20capabilities.%20However%2C%0Athe%20misalignment%20between%20two%20embedding%20strategies%20in%20MLLMs%20--%20the%20structural%0Atextual%20embeddings%20based%20on%20an%20embedding%20look-up%20table%20and%20the%20continuous%0Aembeddings%20generated%20directly%20by%20the%20vision%20encoder%20--%20makes%20challenges%20for%20a%0Amore%20seamless%20fusion%20of%20visual%20and%20textual%20information.%20We%20propose%20Ovis%2C%20a%0Anovel%20MLLM%20architecture%20designed%20to%20structurally%20align%20visual%20and%20textual%0Aembeddings.%20Ovis%20integrates%20an%20additional%20learnable%20visual%20embedding%20table%20into%0Athe%20visual%20encoder%27s%20process.%20To%20capture%20rich%20visual%20semantics%2C%20each%20image%0Apatch%20indexes%20the%20visual%20embedding%20table%20multiple%20times%2C%20resulting%20in%20a%20final%0Avisual%20embedding%20that%20is%20a%20probabilistic%20combination%20of%20the%20indexed%20embeddings.%0AThis%20structural%20approach%20mirrors%20the%20method%20used%20for%20generating%20textual%0Aembeddings.%20Empirical%20evaluations%20on%20various%20multimodal%20benchmarks%20show%20that%0AOvis%20outperforms%20open-source%20MLLMs%20of%20similar%20parameter%20scales%20and%20even%0Asurpasses%20the%20proprietary%20model%20Qwen-VL-Plus%20overall.%20These%20results%20highlight%0Athe%20potential%20of%20Ovis%27%20structured%20visual%20representation%20for%20advancing%20MLLM%0Aarchitectural%20design%20and%20promoting%20more%20effective%20multimodal%20learning.%20Code%2C%0Adatasets%2C%20and%20models%20are%20available%20at%20https%3A//github.com/AIDC-AI/Ovis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.20797v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOvis%253A%2520Structural%2520Embedding%2520Alignment%2520for%2520Multimodal%2520Large%2520Language%2520Model%26entry.906535625%3DShiyin%2520Lu%2520and%2520Yang%2520Li%2520and%2520Qing-Guo%2520Chen%2520and%2520Zhao%2520Xu%2520and%2520Weihua%2520Luo%2520and%2520Kaifu%2520Zhang%2520and%2520Han-Jia%2520Ye%26entry.1292438233%3D%2520%2520Current%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520typically%2520integrate%2520a%250Apre-trained%2520LLM%2520with%2520another%2520pre-trained%2520vision%2520transformer%2520through%2520a%250Aconnector%252C%2520such%2520as%2520an%2520MLP%252C%2520endowing%2520the%2520LLM%2520with%2520visual%2520capabilities.%2520However%252C%250Athe%2520misalignment%2520between%2520two%2520embedding%2520strategies%2520in%2520MLLMs%2520--%2520the%2520structural%250Atextual%2520embeddings%2520based%2520on%2520an%2520embedding%2520look-up%2520table%2520and%2520the%2520continuous%250Aembeddings%2520generated%2520directly%2520by%2520the%2520vision%2520encoder%2520--%2520makes%2520challenges%2520for%2520a%250Amore%2520seamless%2520fusion%2520of%2520visual%2520and%2520textual%2520information.%2520We%2520propose%2520Ovis%252C%2520a%250Anovel%2520MLLM%2520architecture%2520designed%2520to%2520structurally%2520align%2520visual%2520and%2520textual%250Aembeddings.%2520Ovis%2520integrates%2520an%2520additional%2520learnable%2520visual%2520embedding%2520table%2520into%250Athe%2520visual%2520encoder%2527s%2520process.%2520To%2520capture%2520rich%2520visual%2520semantics%252C%2520each%2520image%250Apatch%2520indexes%2520the%2520visual%2520embedding%2520table%2520multiple%2520times%252C%2520resulting%2520in%2520a%2520final%250Avisual%2520embedding%2520that%2520is%2520a%2520probabilistic%2520combination%2520of%2520the%2520indexed%2520embeddings.%250AThis%2520structural%2520approach%2520mirrors%2520the%2520method%2520used%2520for%2520generating%2520textual%250Aembeddings.%2520Empirical%2520evaluations%2520on%2520various%2520multimodal%2520benchmarks%2520show%2520that%250AOvis%2520outperforms%2520open-source%2520MLLMs%2520of%2520similar%2520parameter%2520scales%2520and%2520even%250Asurpasses%2520the%2520proprietary%2520model%2520Qwen-VL-Plus%2520overall.%2520These%2520results%2520highlight%250Athe%2520potential%2520of%2520Ovis%2527%2520structured%2520visual%2520representation%2520for%2520advancing%2520MLLM%250Aarchitectural%2520design%2520and%2520promoting%2520more%2520effective%2520multimodal%2520learning.%2520Code%252C%250Adatasets%252C%2520and%2520models%2520are%2520available%2520at%2520https%253A//github.com/AIDC-AI/Ovis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.20797v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ovis%3A%20Structural%20Embedding%20Alignment%20for%20Multimodal%20Large%20Language%20Model&entry.906535625=Shiyin%20Lu%20and%20Yang%20Li%20and%20Qing-Guo%20Chen%20and%20Zhao%20Xu%20and%20Weihua%20Luo%20and%20Kaifu%20Zhang%20and%20Han-Jia%20Ye&entry.1292438233=%20%20Current%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20typically%20integrate%20a%0Apre-trained%20LLM%20with%20another%20pre-trained%20vision%20transformer%20through%20a%0Aconnector%2C%20such%20as%20an%20MLP%2C%20endowing%20the%20LLM%20with%20visual%20capabilities.%20However%2C%0Athe%20misalignment%20between%20two%20embedding%20strategies%20in%20MLLMs%20--%20the%20structural%0Atextual%20embeddings%20based%20on%20an%20embedding%20look-up%20table%20and%20the%20continuous%0Aembeddings%20generated%20directly%20by%20the%20vision%20encoder%20--%20makes%20challenges%20for%20a%0Amore%20seamless%20fusion%20of%20visual%20and%20textual%20information.%20We%20propose%20Ovis%2C%20a%0Anovel%20MLLM%20architecture%20designed%20to%20structurally%20align%20visual%20and%20textual%0Aembeddings.%20Ovis%20integrates%20an%20additional%20learnable%20visual%20embedding%20table%20into%0Athe%20visual%20encoder%27s%20process.%20To%20capture%20rich%20visual%20semantics%2C%20each%20image%0Apatch%20indexes%20the%20visual%20embedding%20table%20multiple%20times%2C%20resulting%20in%20a%20final%0Avisual%20embedding%20that%20is%20a%20probabilistic%20combination%20of%20the%20indexed%20embeddings.%0AThis%20structural%20approach%20mirrors%20the%20method%20used%20for%20generating%20textual%0Aembeddings.%20Empirical%20evaluations%20on%20various%20multimodal%20benchmarks%20show%20that%0AOvis%20outperforms%20open-source%20MLLMs%20of%20similar%20parameter%20scales%20and%20even%0Asurpasses%20the%20proprietary%20model%20Qwen-VL-Plus%20overall.%20These%20results%20highlight%0Athe%20potential%20of%20Ovis%27%20structured%20visual%20representation%20for%20advancing%20MLLM%0Aarchitectural%20design%20and%20promoting%20more%20effective%20multimodal%20learning.%20Code%2C%0Adatasets%2C%20and%20models%20are%20available%20at%20https%3A//github.com/AIDC-AI/Ovis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.20797v2&entry.124074799=Read"},
{"title": "Guiding the Last Centimeter: Novel Anatomy-Aware Probe Servoing for\n  Standardized Imaging Plane Navigation in Robotic Lung Ultrasound", "author": "Xihan Ma and Mingjie Zeng and Jeffrey C. Hill and Beatrice Hoffmann and Ziming Zhang and Haichong K. Zhang", "abstract": "  Navigating the ultrasound (US) probe to the standardized imaging plane (SIP)\nfor image acquisition is a critical but operator-dependent task in conventional\nfreehand diagnostic US. Robotic US systems (RUSS) offer the potential to\nenhance imaging consistency by leveraging real-time US image feedback to\noptimize the probe pose, thereby reducing reliance on operator expertise.\nHowever, determining the proper approach to extracting generalizable features\nfrom the US images for probe pose adjustment remain challenging. In this work,\nwe propose a SIP navigation framework for RUSS, exemplified in the context of\nrobotic lung ultrasound (LUS). This framework facilitates automatic probe\nadjustment when in proximity to the SIP. This is achieved by explicitly\nextracting multiple anatomical features presented in real-time LUS images and\nperforming non-patient-specific template matching to generate probe motion\ntowards the SIP using image-based visual servoing (IBVS). This framework is\nfurther integrated with the active-sensing end-effector (A-SEE), a customized\nrobot end-effector that leverages patient external body geometry to maintain\noptimal probe alignment with the contact surface, thus preserving US signal\nquality throughout the navigation. The proposed approach ensures procedural\ninterpretability and inter-patient adaptability. Validation is conducted\nthrough anatomy-mimicking phantom and in-vivo evaluations involving five human\nsubjects. The results show the framework's high navigation precision with the\nprobe correctly located at the SIP for all cases, exhibiting positioning error\nof under 2 mm in translation and under 2 degree in rotation. These results\ndemonstrate the navigation process's capability to accomondate anatomical\nvariations among patients.\n", "link": "http://arxiv.org/abs/2406.11523v1", "date": "2024-06-17", "relevancy": 2.1598, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5502}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5424}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5082}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Guiding%20the%20Last%20Centimeter%3A%20Novel%20Anatomy-Aware%20Probe%20Servoing%20for%0A%20%20Standardized%20Imaging%20Plane%20Navigation%20in%20Robotic%20Lung%20Ultrasound&body=Title%3A%20Guiding%20the%20Last%20Centimeter%3A%20Novel%20Anatomy-Aware%20Probe%20Servoing%20for%0A%20%20Standardized%20Imaging%20Plane%20Navigation%20in%20Robotic%20Lung%20Ultrasound%0AAuthor%3A%20Xihan%20Ma%20and%20Mingjie%20Zeng%20and%20Jeffrey%20C.%20Hill%20and%20Beatrice%20Hoffmann%20and%20Ziming%20Zhang%20and%20Haichong%20K.%20Zhang%0AAbstract%3A%20%20%20Navigating%20the%20ultrasound%20%28US%29%20probe%20to%20the%20standardized%20imaging%20plane%20%28SIP%29%0Afor%20image%20acquisition%20is%20a%20critical%20but%20operator-dependent%20task%20in%20conventional%0Afreehand%20diagnostic%20US.%20Robotic%20US%20systems%20%28RUSS%29%20offer%20the%20potential%20to%0Aenhance%20imaging%20consistency%20by%20leveraging%20real-time%20US%20image%20feedback%20to%0Aoptimize%20the%20probe%20pose%2C%20thereby%20reducing%20reliance%20on%20operator%20expertise.%0AHowever%2C%20determining%20the%20proper%20approach%20to%20extracting%20generalizable%20features%0Afrom%20the%20US%20images%20for%20probe%20pose%20adjustment%20remain%20challenging.%20In%20this%20work%2C%0Awe%20propose%20a%20SIP%20navigation%20framework%20for%20RUSS%2C%20exemplified%20in%20the%20context%20of%0Arobotic%20lung%20ultrasound%20%28LUS%29.%20This%20framework%20facilitates%20automatic%20probe%0Aadjustment%20when%20in%20proximity%20to%20the%20SIP.%20This%20is%20achieved%20by%20explicitly%0Aextracting%20multiple%20anatomical%20features%20presented%20in%20real-time%20LUS%20images%20and%0Aperforming%20non-patient-specific%20template%20matching%20to%20generate%20probe%20motion%0Atowards%20the%20SIP%20using%20image-based%20visual%20servoing%20%28IBVS%29.%20This%20framework%20is%0Afurther%20integrated%20with%20the%20active-sensing%20end-effector%20%28A-SEE%29%2C%20a%20customized%0Arobot%20end-effector%20that%20leverages%20patient%20external%20body%20geometry%20to%20maintain%0Aoptimal%20probe%20alignment%20with%20the%20contact%20surface%2C%20thus%20preserving%20US%20signal%0Aquality%20throughout%20the%20navigation.%20The%20proposed%20approach%20ensures%20procedural%0Ainterpretability%20and%20inter-patient%20adaptability.%20Validation%20is%20conducted%0Athrough%20anatomy-mimicking%20phantom%20and%20in-vivo%20evaluations%20involving%20five%20human%0Asubjects.%20The%20results%20show%20the%20framework%27s%20high%20navigation%20precision%20with%20the%0Aprobe%20correctly%20located%20at%20the%20SIP%20for%20all%20cases%2C%20exhibiting%20positioning%20error%0Aof%20under%202%20mm%20in%20translation%20and%20under%202%20degree%20in%20rotation.%20These%20results%0Ademonstrate%20the%20navigation%20process%27s%20capability%20to%20accomondate%20anatomical%0Avariations%20among%20patients.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11523v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuiding%2520the%2520Last%2520Centimeter%253A%2520Novel%2520Anatomy-Aware%2520Probe%2520Servoing%2520for%250A%2520%2520Standardized%2520Imaging%2520Plane%2520Navigation%2520in%2520Robotic%2520Lung%2520Ultrasound%26entry.906535625%3DXihan%2520Ma%2520and%2520Mingjie%2520Zeng%2520and%2520Jeffrey%2520C.%2520Hill%2520and%2520Beatrice%2520Hoffmann%2520and%2520Ziming%2520Zhang%2520and%2520Haichong%2520K.%2520Zhang%26entry.1292438233%3D%2520%2520Navigating%2520the%2520ultrasound%2520%2528US%2529%2520probe%2520to%2520the%2520standardized%2520imaging%2520plane%2520%2528SIP%2529%250Afor%2520image%2520acquisition%2520is%2520a%2520critical%2520but%2520operator-dependent%2520task%2520in%2520conventional%250Afreehand%2520diagnostic%2520US.%2520Robotic%2520US%2520systems%2520%2528RUSS%2529%2520offer%2520the%2520potential%2520to%250Aenhance%2520imaging%2520consistency%2520by%2520leveraging%2520real-time%2520US%2520image%2520feedback%2520to%250Aoptimize%2520the%2520probe%2520pose%252C%2520thereby%2520reducing%2520reliance%2520on%2520operator%2520expertise.%250AHowever%252C%2520determining%2520the%2520proper%2520approach%2520to%2520extracting%2520generalizable%2520features%250Afrom%2520the%2520US%2520images%2520for%2520probe%2520pose%2520adjustment%2520remain%2520challenging.%2520In%2520this%2520work%252C%250Awe%2520propose%2520a%2520SIP%2520navigation%2520framework%2520for%2520RUSS%252C%2520exemplified%2520in%2520the%2520context%2520of%250Arobotic%2520lung%2520ultrasound%2520%2528LUS%2529.%2520This%2520framework%2520facilitates%2520automatic%2520probe%250Aadjustment%2520when%2520in%2520proximity%2520to%2520the%2520SIP.%2520This%2520is%2520achieved%2520by%2520explicitly%250Aextracting%2520multiple%2520anatomical%2520features%2520presented%2520in%2520real-time%2520LUS%2520images%2520and%250Aperforming%2520non-patient-specific%2520template%2520matching%2520to%2520generate%2520probe%2520motion%250Atowards%2520the%2520SIP%2520using%2520image-based%2520visual%2520servoing%2520%2528IBVS%2529.%2520This%2520framework%2520is%250Afurther%2520integrated%2520with%2520the%2520active-sensing%2520end-effector%2520%2528A-SEE%2529%252C%2520a%2520customized%250Arobot%2520end-effector%2520that%2520leverages%2520patient%2520external%2520body%2520geometry%2520to%2520maintain%250Aoptimal%2520probe%2520alignment%2520with%2520the%2520contact%2520surface%252C%2520thus%2520preserving%2520US%2520signal%250Aquality%2520throughout%2520the%2520navigation.%2520The%2520proposed%2520approach%2520ensures%2520procedural%250Ainterpretability%2520and%2520inter-patient%2520adaptability.%2520Validation%2520is%2520conducted%250Athrough%2520anatomy-mimicking%2520phantom%2520and%2520in-vivo%2520evaluations%2520involving%2520five%2520human%250Asubjects.%2520The%2520results%2520show%2520the%2520framework%2527s%2520high%2520navigation%2520precision%2520with%2520the%250Aprobe%2520correctly%2520located%2520at%2520the%2520SIP%2520for%2520all%2520cases%252C%2520exhibiting%2520positioning%2520error%250Aof%2520under%25202%2520mm%2520in%2520translation%2520and%2520under%25202%2520degree%2520in%2520rotation.%2520These%2520results%250Ademonstrate%2520the%2520navigation%2520process%2527s%2520capability%2520to%2520accomondate%2520anatomical%250Avariations%2520among%2520patients.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11523v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guiding%20the%20Last%20Centimeter%3A%20Novel%20Anatomy-Aware%20Probe%20Servoing%20for%0A%20%20Standardized%20Imaging%20Plane%20Navigation%20in%20Robotic%20Lung%20Ultrasound&entry.906535625=Xihan%20Ma%20and%20Mingjie%20Zeng%20and%20Jeffrey%20C.%20Hill%20and%20Beatrice%20Hoffmann%20and%20Ziming%20Zhang%20and%20Haichong%20K.%20Zhang&entry.1292438233=%20%20Navigating%20the%20ultrasound%20%28US%29%20probe%20to%20the%20standardized%20imaging%20plane%20%28SIP%29%0Afor%20image%20acquisition%20is%20a%20critical%20but%20operator-dependent%20task%20in%20conventional%0Afreehand%20diagnostic%20US.%20Robotic%20US%20systems%20%28RUSS%29%20offer%20the%20potential%20to%0Aenhance%20imaging%20consistency%20by%20leveraging%20real-time%20US%20image%20feedback%20to%0Aoptimize%20the%20probe%20pose%2C%20thereby%20reducing%20reliance%20on%20operator%20expertise.%0AHowever%2C%20determining%20the%20proper%20approach%20to%20extracting%20generalizable%20features%0Afrom%20the%20US%20images%20for%20probe%20pose%20adjustment%20remain%20challenging.%20In%20this%20work%2C%0Awe%20propose%20a%20SIP%20navigation%20framework%20for%20RUSS%2C%20exemplified%20in%20the%20context%20of%0Arobotic%20lung%20ultrasound%20%28LUS%29.%20This%20framework%20facilitates%20automatic%20probe%0Aadjustment%20when%20in%20proximity%20to%20the%20SIP.%20This%20is%20achieved%20by%20explicitly%0Aextracting%20multiple%20anatomical%20features%20presented%20in%20real-time%20LUS%20images%20and%0Aperforming%20non-patient-specific%20template%20matching%20to%20generate%20probe%20motion%0Atowards%20the%20SIP%20using%20image-based%20visual%20servoing%20%28IBVS%29.%20This%20framework%20is%0Afurther%20integrated%20with%20the%20active-sensing%20end-effector%20%28A-SEE%29%2C%20a%20customized%0Arobot%20end-effector%20that%20leverages%20patient%20external%20body%20geometry%20to%20maintain%0Aoptimal%20probe%20alignment%20with%20the%20contact%20surface%2C%20thus%20preserving%20US%20signal%0Aquality%20throughout%20the%20navigation.%20The%20proposed%20approach%20ensures%20procedural%0Ainterpretability%20and%20inter-patient%20adaptability.%20Validation%20is%20conducted%0Athrough%20anatomy-mimicking%20phantom%20and%20in-vivo%20evaluations%20involving%20five%20human%0Asubjects.%20The%20results%20show%20the%20framework%27s%20high%20navigation%20precision%20with%20the%0Aprobe%20correctly%20located%20at%20the%20SIP%20for%20all%20cases%2C%20exhibiting%20positioning%20error%0Aof%20under%202%20mm%20in%20translation%20and%20under%202%20degree%20in%20rotation.%20These%20results%0Ademonstrate%20the%20navigation%20process%27s%20capability%20to%20accomondate%20anatomical%0Avariations%20among%20patients.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11523v1&entry.124074799=Read"},
{"title": "Interactive Evolution: A Neural-Symbolic Self-Training Framework For\n  Large Language Models", "author": "Fangzhi Xu and Qiushi Sun and Kanzhi Cheng and Jun Liu and Yu Qiao and Zhiyong Wu", "abstract": "  One of the primary driving forces contributing to the superior performance of\nLarge Language Models (LLMs) is the extensive availability of human-annotated\nnatural language data, which is used for alignment fine-tuning. This inspired\nresearchers to investigate self-training methods to mitigate the extensive\nreliance on human annotations. However, the current success of self-training\nhas been primarily observed in natural language scenarios, rather than in the\nincreasingly important neural-symbolic scenarios. To this end, we propose an\nenvironment-guided neural-symbolic self-training framework named ENVISIONS. It\naims to overcome two main challenges: (1) the scarcity of symbolic data, and\n(2) the limited proficiency of LLMs in processing symbolic language. Extensive\nevaluations conducted on three distinct domains demonstrate the effectiveness\nof our approach. Additionally, we have conducted a comprehensive analysis to\nuncover the factors contributing to ENVISIONS's success, thereby offering\nvaluable insights for future research in this area. Code will be available at\n\\url{https://github.com/xufangzhi/ENVISIONS}.\n", "link": "http://arxiv.org/abs/2406.11736v1", "date": "2024-06-17", "relevancy": 2.1581, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5622}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5341}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interactive%20Evolution%3A%20A%20Neural-Symbolic%20Self-Training%20Framework%20For%0A%20%20Large%20Language%20Models&body=Title%3A%20Interactive%20Evolution%3A%20A%20Neural-Symbolic%20Self-Training%20Framework%20For%0A%20%20Large%20Language%20Models%0AAuthor%3A%20Fangzhi%20Xu%20and%20Qiushi%20Sun%20and%20Kanzhi%20Cheng%20and%20Jun%20Liu%20and%20Yu%20Qiao%20and%20Zhiyong%20Wu%0AAbstract%3A%20%20%20One%20of%20the%20primary%20driving%20forces%20contributing%20to%20the%20superior%20performance%20of%0ALarge%20Language%20Models%20%28LLMs%29%20is%20the%20extensive%20availability%20of%20human-annotated%0Anatural%20language%20data%2C%20which%20is%20used%20for%20alignment%20fine-tuning.%20This%20inspired%0Aresearchers%20to%20investigate%20self-training%20methods%20to%20mitigate%20the%20extensive%0Areliance%20on%20human%20annotations.%20However%2C%20the%20current%20success%20of%20self-training%0Ahas%20been%20primarily%20observed%20in%20natural%20language%20scenarios%2C%20rather%20than%20in%20the%0Aincreasingly%20important%20neural-symbolic%20scenarios.%20To%20this%20end%2C%20we%20propose%20an%0Aenvironment-guided%20neural-symbolic%20self-training%20framework%20named%20ENVISIONS.%20It%0Aaims%20to%20overcome%20two%20main%20challenges%3A%20%281%29%20the%20scarcity%20of%20symbolic%20data%2C%20and%0A%282%29%20the%20limited%20proficiency%20of%20LLMs%20in%20processing%20symbolic%20language.%20Extensive%0Aevaluations%20conducted%20on%20three%20distinct%20domains%20demonstrate%20the%20effectiveness%0Aof%20our%20approach.%20Additionally%2C%20we%20have%20conducted%20a%20comprehensive%20analysis%20to%0Auncover%20the%20factors%20contributing%20to%20ENVISIONS%27s%20success%2C%20thereby%20offering%0Avaluable%20insights%20for%20future%20research%20in%20this%20area.%20Code%20will%20be%20available%20at%0A%5Curl%7Bhttps%3A//github.com/xufangzhi/ENVISIONS%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11736v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInteractive%2520Evolution%253A%2520A%2520Neural-Symbolic%2520Self-Training%2520Framework%2520For%250A%2520%2520Large%2520Language%2520Models%26entry.906535625%3DFangzhi%2520Xu%2520and%2520Qiushi%2520Sun%2520and%2520Kanzhi%2520Cheng%2520and%2520Jun%2520Liu%2520and%2520Yu%2520Qiao%2520and%2520Zhiyong%2520Wu%26entry.1292438233%3D%2520%2520One%2520of%2520the%2520primary%2520driving%2520forces%2520contributing%2520to%2520the%2520superior%2520performance%2520of%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520is%2520the%2520extensive%2520availability%2520of%2520human-annotated%250Anatural%2520language%2520data%252C%2520which%2520is%2520used%2520for%2520alignment%2520fine-tuning.%2520This%2520inspired%250Aresearchers%2520to%2520investigate%2520self-training%2520methods%2520to%2520mitigate%2520the%2520extensive%250Areliance%2520on%2520human%2520annotations.%2520However%252C%2520the%2520current%2520success%2520of%2520self-training%250Ahas%2520been%2520primarily%2520observed%2520in%2520natural%2520language%2520scenarios%252C%2520rather%2520than%2520in%2520the%250Aincreasingly%2520important%2520neural-symbolic%2520scenarios.%2520To%2520this%2520end%252C%2520we%2520propose%2520an%250Aenvironment-guided%2520neural-symbolic%2520self-training%2520framework%2520named%2520ENVISIONS.%2520It%250Aaims%2520to%2520overcome%2520two%2520main%2520challenges%253A%2520%25281%2529%2520the%2520scarcity%2520of%2520symbolic%2520data%252C%2520and%250A%25282%2529%2520the%2520limited%2520proficiency%2520of%2520LLMs%2520in%2520processing%2520symbolic%2520language.%2520Extensive%250Aevaluations%2520conducted%2520on%2520three%2520distinct%2520domains%2520demonstrate%2520the%2520effectiveness%250Aof%2520our%2520approach.%2520Additionally%252C%2520we%2520have%2520conducted%2520a%2520comprehensive%2520analysis%2520to%250Auncover%2520the%2520factors%2520contributing%2520to%2520ENVISIONS%2527s%2520success%252C%2520thereby%2520offering%250Avaluable%2520insights%2520for%2520future%2520research%2520in%2520this%2520area.%2520Code%2520will%2520be%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/xufangzhi/ENVISIONS%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11736v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interactive%20Evolution%3A%20A%20Neural-Symbolic%20Self-Training%20Framework%20For%0A%20%20Large%20Language%20Models&entry.906535625=Fangzhi%20Xu%20and%20Qiushi%20Sun%20and%20Kanzhi%20Cheng%20and%20Jun%20Liu%20and%20Yu%20Qiao%20and%20Zhiyong%20Wu&entry.1292438233=%20%20One%20of%20the%20primary%20driving%20forces%20contributing%20to%20the%20superior%20performance%20of%0ALarge%20Language%20Models%20%28LLMs%29%20is%20the%20extensive%20availability%20of%20human-annotated%0Anatural%20language%20data%2C%20which%20is%20used%20for%20alignment%20fine-tuning.%20This%20inspired%0Aresearchers%20to%20investigate%20self-training%20methods%20to%20mitigate%20the%20extensive%0Areliance%20on%20human%20annotations.%20However%2C%20the%20current%20success%20of%20self-training%0Ahas%20been%20primarily%20observed%20in%20natural%20language%20scenarios%2C%20rather%20than%20in%20the%0Aincreasingly%20important%20neural-symbolic%20scenarios.%20To%20this%20end%2C%20we%20propose%20an%0Aenvironment-guided%20neural-symbolic%20self-training%20framework%20named%20ENVISIONS.%20It%0Aaims%20to%20overcome%20two%20main%20challenges%3A%20%281%29%20the%20scarcity%20of%20symbolic%20data%2C%20and%0A%282%29%20the%20limited%20proficiency%20of%20LLMs%20in%20processing%20symbolic%20language.%20Extensive%0Aevaluations%20conducted%20on%20three%20distinct%20domains%20demonstrate%20the%20effectiveness%0Aof%20our%20approach.%20Additionally%2C%20we%20have%20conducted%20a%20comprehensive%20analysis%20to%0Auncover%20the%20factors%20contributing%20to%20ENVISIONS%27s%20success%2C%20thereby%20offering%0Avaluable%20insights%20for%20future%20research%20in%20this%20area.%20Code%20will%20be%20available%20at%0A%5Curl%7Bhttps%3A//github.com/xufangzhi/ENVISIONS%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11736v1&entry.124074799=Read"},
{"title": "VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio\n  Understanding in Video-LLMs", "author": "Zesen Cheng and Sicong Leng and Hang Zhang and Yifei Xin and Xin Li and Guanzheng Chen and Yongxin Zhu and Wenqi Zhang and Ziyang Luo and Deli Zhao and Lidong Bing", "abstract": "  In this paper, we present the VideoLLaMA 2, a set of Video Large Language\nModels (Video-LLMs) designed to enhance spatial-temporal modeling and audio\nunderstanding in video and audio-oriented tasks. Building upon its predecessor,\nVideoLLaMA 2 incorporates a tailor-made Spatial-Temporal Convolution (STC)\nconnector, which effectively captures the intricate spatial and temporal\ndynamics of video data. Additionally, we integrate an Audio Branch into the\nmodel through joint training, thereby enriching the multimodal understanding\ncapabilities of the model by seamlessly incorporating audio cues. Comprehensive\nevaluations on multiple-choice video question answering (MC-VQA), open-ended\nvideo question answering (OE-VQA), and video captioning (VC) tasks demonstrate\nthat VideoLLaMA 2 consistently achieves competitive results among open-source\nmodels and even gets close to some proprietary models on several benchmarks.\nFurthermore, VideoLLaMA 2 exhibits reasonable improvements in audio-only and\naudio-video question-answering (AQA & OE-AVQA) benchmarks over existing models.\nThese advancements underline VideoLLaMA 2's superior performance in multimodal\ncomprehension, setting a new standard for intelligent video analysis systems.\nAll models are public to facilitate further research.\n", "link": "http://arxiv.org/abs/2406.07476v2", "date": "2024-06-17", "relevancy": 2.1524, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5564}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5541}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5134}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoLLaMA%202%3A%20Advancing%20Spatial-Temporal%20Modeling%20and%20Audio%0A%20%20Understanding%20in%20Video-LLMs&body=Title%3A%20VideoLLaMA%202%3A%20Advancing%20Spatial-Temporal%20Modeling%20and%20Audio%0A%20%20Understanding%20in%20Video-LLMs%0AAuthor%3A%20Zesen%20Cheng%20and%20Sicong%20Leng%20and%20Hang%20Zhang%20and%20Yifei%20Xin%20and%20Xin%20Li%20and%20Guanzheng%20Chen%20and%20Yongxin%20Zhu%20and%20Wenqi%20Zhang%20and%20Ziyang%20Luo%20and%20Deli%20Zhao%20and%20Lidong%20Bing%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20the%20VideoLLaMA%202%2C%20a%20set%20of%20Video%20Large%20Language%0AModels%20%28Video-LLMs%29%20designed%20to%20enhance%20spatial-temporal%20modeling%20and%20audio%0Aunderstanding%20in%20video%20and%20audio-oriented%20tasks.%20Building%20upon%20its%20predecessor%2C%0AVideoLLaMA%202%20incorporates%20a%20tailor-made%20Spatial-Temporal%20Convolution%20%28STC%29%0Aconnector%2C%20which%20effectively%20captures%20the%20intricate%20spatial%20and%20temporal%0Adynamics%20of%20video%20data.%20Additionally%2C%20we%20integrate%20an%20Audio%20Branch%20into%20the%0Amodel%20through%20joint%20training%2C%20thereby%20enriching%20the%20multimodal%20understanding%0Acapabilities%20of%20the%20model%20by%20seamlessly%20incorporating%20audio%20cues.%20Comprehensive%0Aevaluations%20on%20multiple-choice%20video%20question%20answering%20%28MC-VQA%29%2C%20open-ended%0Avideo%20question%20answering%20%28OE-VQA%29%2C%20and%20video%20captioning%20%28VC%29%20tasks%20demonstrate%0Athat%20VideoLLaMA%202%20consistently%20achieves%20competitive%20results%20among%20open-source%0Amodels%20and%20even%20gets%20close%20to%20some%20proprietary%20models%20on%20several%20benchmarks.%0AFurthermore%2C%20VideoLLaMA%202%20exhibits%20reasonable%20improvements%20in%20audio-only%20and%0Aaudio-video%20question-answering%20%28AQA%20%26%20OE-AVQA%29%20benchmarks%20over%20existing%20models.%0AThese%20advancements%20underline%20VideoLLaMA%202%27s%20superior%20performance%20in%20multimodal%0Acomprehension%2C%20setting%20a%20new%20standard%20for%20intelligent%20video%20analysis%20systems.%0AAll%20models%20are%20public%20to%20facilitate%20further%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07476v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoLLaMA%25202%253A%2520Advancing%2520Spatial-Temporal%2520Modeling%2520and%2520Audio%250A%2520%2520Understanding%2520in%2520Video-LLMs%26entry.906535625%3DZesen%2520Cheng%2520and%2520Sicong%2520Leng%2520and%2520Hang%2520Zhang%2520and%2520Yifei%2520Xin%2520and%2520Xin%2520Li%2520and%2520Guanzheng%2520Chen%2520and%2520Yongxin%2520Zhu%2520and%2520Wenqi%2520Zhang%2520and%2520Ziyang%2520Luo%2520and%2520Deli%2520Zhao%2520and%2520Lidong%2520Bing%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520VideoLLaMA%25202%252C%2520a%2520set%2520of%2520Video%2520Large%2520Language%250AModels%2520%2528Video-LLMs%2529%2520designed%2520to%2520enhance%2520spatial-temporal%2520modeling%2520and%2520audio%250Aunderstanding%2520in%2520video%2520and%2520audio-oriented%2520tasks.%2520Building%2520upon%2520its%2520predecessor%252C%250AVideoLLaMA%25202%2520incorporates%2520a%2520tailor-made%2520Spatial-Temporal%2520Convolution%2520%2528STC%2529%250Aconnector%252C%2520which%2520effectively%2520captures%2520the%2520intricate%2520spatial%2520and%2520temporal%250Adynamics%2520of%2520video%2520data.%2520Additionally%252C%2520we%2520integrate%2520an%2520Audio%2520Branch%2520into%2520the%250Amodel%2520through%2520joint%2520training%252C%2520thereby%2520enriching%2520the%2520multimodal%2520understanding%250Acapabilities%2520of%2520the%2520model%2520by%2520seamlessly%2520incorporating%2520audio%2520cues.%2520Comprehensive%250Aevaluations%2520on%2520multiple-choice%2520video%2520question%2520answering%2520%2528MC-VQA%2529%252C%2520open-ended%250Avideo%2520question%2520answering%2520%2528OE-VQA%2529%252C%2520and%2520video%2520captioning%2520%2528VC%2529%2520tasks%2520demonstrate%250Athat%2520VideoLLaMA%25202%2520consistently%2520achieves%2520competitive%2520results%2520among%2520open-source%250Amodels%2520and%2520even%2520gets%2520close%2520to%2520some%2520proprietary%2520models%2520on%2520several%2520benchmarks.%250AFurthermore%252C%2520VideoLLaMA%25202%2520exhibits%2520reasonable%2520improvements%2520in%2520audio-only%2520and%250Aaudio-video%2520question-answering%2520%2528AQA%2520%2526%2520OE-AVQA%2529%2520benchmarks%2520over%2520existing%2520models.%250AThese%2520advancements%2520underline%2520VideoLLaMA%25202%2527s%2520superior%2520performance%2520in%2520multimodal%250Acomprehension%252C%2520setting%2520a%2520new%2520standard%2520for%2520intelligent%2520video%2520analysis%2520systems.%250AAll%2520models%2520are%2520public%2520to%2520facilitate%2520further%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07476v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoLLaMA%202%3A%20Advancing%20Spatial-Temporal%20Modeling%20and%20Audio%0A%20%20Understanding%20in%20Video-LLMs&entry.906535625=Zesen%20Cheng%20and%20Sicong%20Leng%20and%20Hang%20Zhang%20and%20Yifei%20Xin%20and%20Xin%20Li%20and%20Guanzheng%20Chen%20and%20Yongxin%20Zhu%20and%20Wenqi%20Zhang%20and%20Ziyang%20Luo%20and%20Deli%20Zhao%20and%20Lidong%20Bing&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20the%20VideoLLaMA%202%2C%20a%20set%20of%20Video%20Large%20Language%0AModels%20%28Video-LLMs%29%20designed%20to%20enhance%20spatial-temporal%20modeling%20and%20audio%0Aunderstanding%20in%20video%20and%20audio-oriented%20tasks.%20Building%20upon%20its%20predecessor%2C%0AVideoLLaMA%202%20incorporates%20a%20tailor-made%20Spatial-Temporal%20Convolution%20%28STC%29%0Aconnector%2C%20which%20effectively%20captures%20the%20intricate%20spatial%20and%20temporal%0Adynamics%20of%20video%20data.%20Additionally%2C%20we%20integrate%20an%20Audio%20Branch%20into%20the%0Amodel%20through%20joint%20training%2C%20thereby%20enriching%20the%20multimodal%20understanding%0Acapabilities%20of%20the%20model%20by%20seamlessly%20incorporating%20audio%20cues.%20Comprehensive%0Aevaluations%20on%20multiple-choice%20video%20question%20answering%20%28MC-VQA%29%2C%20open-ended%0Avideo%20question%20answering%20%28OE-VQA%29%2C%20and%20video%20captioning%20%28VC%29%20tasks%20demonstrate%0Athat%20VideoLLaMA%202%20consistently%20achieves%20competitive%20results%20among%20open-source%0Amodels%20and%20even%20gets%20close%20to%20some%20proprietary%20models%20on%20several%20benchmarks.%0AFurthermore%2C%20VideoLLaMA%202%20exhibits%20reasonable%20improvements%20in%20audio-only%20and%0Aaudio-video%20question-answering%20%28AQA%20%26%20OE-AVQA%29%20benchmarks%20over%20existing%20models.%0AThese%20advancements%20underline%20VideoLLaMA%202%27s%20superior%20performance%20in%20multimodal%0Acomprehension%2C%20setting%20a%20new%20standard%20for%20intelligent%20video%20analysis%20systems.%0AAll%20models%20are%20public%20to%20facilitate%20further%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07476v2&entry.124074799=Read"},
{"title": "GeoGPT4V: Towards Geometric Multi-modal Large Language Models with\n  Geometric Image Generation", "author": "Shihao Cai and Keqin Bao and Hangyu Guo and Jizhi Zhang and Jun Song and Bo Zheng", "abstract": "  Large language models have seen widespread adoption in math problem-solving.\nHowever, in geometry problems that usually require visual aids for better\nunderstanding, even the most advanced multi-modal models currently still face\nchallenges in effectively using image information. High-quality data is crucial\nfor enhancing the geometric capabilities of multi-modal models, yet existing\nopen-source datasets and related efforts are either too challenging for direct\nmodel learning or suffer from misalignment between text and images. To overcome\nthis issue, we introduce a novel pipeline that leverages GPT-4 and GPT-4V to\ngenerate relatively basic geometry problems with aligned text and images,\nfacilitating model learning. We have produced a dataset of 4.9K geometry\nproblems and combined it with 19K open-source data to form our GeoGPT4V\ndataset. Experimental results demonstrate that the GeoGPT4V dataset\nsignificantly improves the geometry performance of various models on the\nMathVista and MathVision benchmarks. The code is available at\nhttps://github.com/Lanyu0303/GeoGPT4V_Project\n", "link": "http://arxiv.org/abs/2406.11503v1", "date": "2024-06-17", "relevancy": 2.1519, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5713}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5151}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoGPT4V%3A%20Towards%20Geometric%20Multi-modal%20Large%20Language%20Models%20with%0A%20%20Geometric%20Image%20Generation&body=Title%3A%20GeoGPT4V%3A%20Towards%20Geometric%20Multi-modal%20Large%20Language%20Models%20with%0A%20%20Geometric%20Image%20Generation%0AAuthor%3A%20Shihao%20Cai%20and%20Keqin%20Bao%20and%20Hangyu%20Guo%20and%20Jizhi%20Zhang%20and%20Jun%20Song%20and%20Bo%20Zheng%0AAbstract%3A%20%20%20Large%20language%20models%20have%20seen%20widespread%20adoption%20in%20math%20problem-solving.%0AHowever%2C%20in%20geometry%20problems%20that%20usually%20require%20visual%20aids%20for%20better%0Aunderstanding%2C%20even%20the%20most%20advanced%20multi-modal%20models%20currently%20still%20face%0Achallenges%20in%20effectively%20using%20image%20information.%20High-quality%20data%20is%20crucial%0Afor%20enhancing%20the%20geometric%20capabilities%20of%20multi-modal%20models%2C%20yet%20existing%0Aopen-source%20datasets%20and%20related%20efforts%20are%20either%20too%20challenging%20for%20direct%0Amodel%20learning%20or%20suffer%20from%20misalignment%20between%20text%20and%20images.%20To%20overcome%0Athis%20issue%2C%20we%20introduce%20a%20novel%20pipeline%20that%20leverages%20GPT-4%20and%20GPT-4V%20to%0Agenerate%20relatively%20basic%20geometry%20problems%20with%20aligned%20text%20and%20images%2C%0Afacilitating%20model%20learning.%20We%20have%20produced%20a%20dataset%20of%204.9K%20geometry%0Aproblems%20and%20combined%20it%20with%2019K%20open-source%20data%20to%20form%20our%20GeoGPT4V%0Adataset.%20Experimental%20results%20demonstrate%20that%20the%20GeoGPT4V%20dataset%0Asignificantly%20improves%20the%20geometry%20performance%20of%20various%20models%20on%20the%0AMathVista%20and%20MathVision%20benchmarks.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Lanyu0303/GeoGPT4V_Project%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11503v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoGPT4V%253A%2520Towards%2520Geometric%2520Multi-modal%2520Large%2520Language%2520Models%2520with%250A%2520%2520Geometric%2520Image%2520Generation%26entry.906535625%3DShihao%2520Cai%2520and%2520Keqin%2520Bao%2520and%2520Hangyu%2520Guo%2520and%2520Jizhi%2520Zhang%2520and%2520Jun%2520Song%2520and%2520Bo%2520Zheng%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520have%2520seen%2520widespread%2520adoption%2520in%2520math%2520problem-solving.%250AHowever%252C%2520in%2520geometry%2520problems%2520that%2520usually%2520require%2520visual%2520aids%2520for%2520better%250Aunderstanding%252C%2520even%2520the%2520most%2520advanced%2520multi-modal%2520models%2520currently%2520still%2520face%250Achallenges%2520in%2520effectively%2520using%2520image%2520information.%2520High-quality%2520data%2520is%2520crucial%250Afor%2520enhancing%2520the%2520geometric%2520capabilities%2520of%2520multi-modal%2520models%252C%2520yet%2520existing%250Aopen-source%2520datasets%2520and%2520related%2520efforts%2520are%2520either%2520too%2520challenging%2520for%2520direct%250Amodel%2520learning%2520or%2520suffer%2520from%2520misalignment%2520between%2520text%2520and%2520images.%2520To%2520overcome%250Athis%2520issue%252C%2520we%2520introduce%2520a%2520novel%2520pipeline%2520that%2520leverages%2520GPT-4%2520and%2520GPT-4V%2520to%250Agenerate%2520relatively%2520basic%2520geometry%2520problems%2520with%2520aligned%2520text%2520and%2520images%252C%250Afacilitating%2520model%2520learning.%2520We%2520have%2520produced%2520a%2520dataset%2520of%25204.9K%2520geometry%250Aproblems%2520and%2520combined%2520it%2520with%252019K%2520open-source%2520data%2520to%2520form%2520our%2520GeoGPT4V%250Adataset.%2520Experimental%2520results%2520demonstrate%2520that%2520the%2520GeoGPT4V%2520dataset%250Asignificantly%2520improves%2520the%2520geometry%2520performance%2520of%2520various%2520models%2520on%2520the%250AMathVista%2520and%2520MathVision%2520benchmarks.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Lanyu0303/GeoGPT4V_Project%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11503v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoGPT4V%3A%20Towards%20Geometric%20Multi-modal%20Large%20Language%20Models%20with%0A%20%20Geometric%20Image%20Generation&entry.906535625=Shihao%20Cai%20and%20Keqin%20Bao%20and%20Hangyu%20Guo%20and%20Jizhi%20Zhang%20and%20Jun%20Song%20and%20Bo%20Zheng&entry.1292438233=%20%20Large%20language%20models%20have%20seen%20widespread%20adoption%20in%20math%20problem-solving.%0AHowever%2C%20in%20geometry%20problems%20that%20usually%20require%20visual%20aids%20for%20better%0Aunderstanding%2C%20even%20the%20most%20advanced%20multi-modal%20models%20currently%20still%20face%0Achallenges%20in%20effectively%20using%20image%20information.%20High-quality%20data%20is%20crucial%0Afor%20enhancing%20the%20geometric%20capabilities%20of%20multi-modal%20models%2C%20yet%20existing%0Aopen-source%20datasets%20and%20related%20efforts%20are%20either%20too%20challenging%20for%20direct%0Amodel%20learning%20or%20suffer%20from%20misalignment%20between%20text%20and%20images.%20To%20overcome%0Athis%20issue%2C%20we%20introduce%20a%20novel%20pipeline%20that%20leverages%20GPT-4%20and%20GPT-4V%20to%0Agenerate%20relatively%20basic%20geometry%20problems%20with%20aligned%20text%20and%20images%2C%0Afacilitating%20model%20learning.%20We%20have%20produced%20a%20dataset%20of%204.9K%20geometry%0Aproblems%20and%20combined%20it%20with%2019K%20open-source%20data%20to%20form%20our%20GeoGPT4V%0Adataset.%20Experimental%20results%20demonstrate%20that%20the%20GeoGPT4V%20dataset%0Asignificantly%20improves%20the%20geometry%20performance%20of%20various%20models%20on%20the%0AMathVista%20and%20MathVision%20benchmarks.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Lanyu0303/GeoGPT4V_Project%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11503v1&entry.124074799=Read"},
{"title": "A Picture is Worth More Than 77 Text Tokens: Evaluating CLIP-Style\n  Models on Dense Captions", "author": "Jack Urbanek and Florian Bordes and Pietro Astolfi and Mary Williamson and Vasu Sharma and Adriana Romero-Soriano", "abstract": "  Curation methods for massive vision-language datasets trade off between\ndataset size and quality. However, even the highest quality of available\ncurated captions are far too short to capture the rich visual detail in an\nimage. To show the value of dense and highly-aligned image-text pairs, we\ncollect the Densely Captioned Images (DCI) dataset, containing 7805 natural\nimages human-annotated with mask-aligned descriptions averaging above 1000\nwords each. With precise and reliable captions associated with specific parts\nof an image, we can evaluate vision-language models' (VLMs) understanding of\nimage content with a novel task that matches each caption with its\ncorresponding subcrop. As current models are often limited to 77 text tokens,\nwe also introduce a summarized version (sDCI) in which each caption length is\nlimited. We show that modern techniques that make progress on standard\nbenchmarks do not correspond with significant improvement on our sDCI based\nbenchmark. Lastly, we finetune CLIP using sDCI and show significant\nimprovements over the baseline despite a small training set. By releasing the\nfirst human annotated dense image captioning dataset, we hope to enable the\ndevelopment of new benchmarks or fine-tuning recipes for the next generation of\nVLMs to come.\n", "link": "http://arxiv.org/abs/2312.08578v2", "date": "2024-06-17", "relevancy": 2.1424, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5533}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5316}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5194}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Picture%20is%20Worth%20More%20Than%2077%20Text%20Tokens%3A%20Evaluating%20CLIP-Style%0A%20%20Models%20on%20Dense%20Captions&body=Title%3A%20A%20Picture%20is%20Worth%20More%20Than%2077%20Text%20Tokens%3A%20Evaluating%20CLIP-Style%0A%20%20Models%20on%20Dense%20Captions%0AAuthor%3A%20Jack%20Urbanek%20and%20Florian%20Bordes%20and%20Pietro%20Astolfi%20and%20Mary%20Williamson%20and%20Vasu%20Sharma%20and%20Adriana%20Romero-Soriano%0AAbstract%3A%20%20%20Curation%20methods%20for%20massive%20vision-language%20datasets%20trade%20off%20between%0Adataset%20size%20and%20quality.%20However%2C%20even%20the%20highest%20quality%20of%20available%0Acurated%20captions%20are%20far%20too%20short%20to%20capture%20the%20rich%20visual%20detail%20in%20an%0Aimage.%20To%20show%20the%20value%20of%20dense%20and%20highly-aligned%20image-text%20pairs%2C%20we%0Acollect%20the%20Densely%20Captioned%20Images%20%28DCI%29%20dataset%2C%20containing%207805%20natural%0Aimages%20human-annotated%20with%20mask-aligned%20descriptions%20averaging%20above%201000%0Awords%20each.%20With%20precise%20and%20reliable%20captions%20associated%20with%20specific%20parts%0Aof%20an%20image%2C%20we%20can%20evaluate%20vision-language%20models%27%20%28VLMs%29%20understanding%20of%0Aimage%20content%20with%20a%20novel%20task%20that%20matches%20each%20caption%20with%20its%0Acorresponding%20subcrop.%20As%20current%20models%20are%20often%20limited%20to%2077%20text%20tokens%2C%0Awe%20also%20introduce%20a%20summarized%20version%20%28sDCI%29%20in%20which%20each%20caption%20length%20is%0Alimited.%20We%20show%20that%20modern%20techniques%20that%20make%20progress%20on%20standard%0Abenchmarks%20do%20not%20correspond%20with%20significant%20improvement%20on%20our%20sDCI%20based%0Abenchmark.%20Lastly%2C%20we%20finetune%20CLIP%20using%20sDCI%20and%20show%20significant%0Aimprovements%20over%20the%20baseline%20despite%20a%20small%20training%20set.%20By%20releasing%20the%0Afirst%20human%20annotated%20dense%20image%20captioning%20dataset%2C%20we%20hope%20to%20enable%20the%0Adevelopment%20of%20new%20benchmarks%20or%20fine-tuning%20recipes%20for%20the%20next%20generation%20of%0AVLMs%20to%20come.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.08578v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Picture%2520is%2520Worth%2520More%2520Than%252077%2520Text%2520Tokens%253A%2520Evaluating%2520CLIP-Style%250A%2520%2520Models%2520on%2520Dense%2520Captions%26entry.906535625%3DJack%2520Urbanek%2520and%2520Florian%2520Bordes%2520and%2520Pietro%2520Astolfi%2520and%2520Mary%2520Williamson%2520and%2520Vasu%2520Sharma%2520and%2520Adriana%2520Romero-Soriano%26entry.1292438233%3D%2520%2520Curation%2520methods%2520for%2520massive%2520vision-language%2520datasets%2520trade%2520off%2520between%250Adataset%2520size%2520and%2520quality.%2520However%252C%2520even%2520the%2520highest%2520quality%2520of%2520available%250Acurated%2520captions%2520are%2520far%2520too%2520short%2520to%2520capture%2520the%2520rich%2520visual%2520detail%2520in%2520an%250Aimage.%2520To%2520show%2520the%2520value%2520of%2520dense%2520and%2520highly-aligned%2520image-text%2520pairs%252C%2520we%250Acollect%2520the%2520Densely%2520Captioned%2520Images%2520%2528DCI%2529%2520dataset%252C%2520containing%25207805%2520natural%250Aimages%2520human-annotated%2520with%2520mask-aligned%2520descriptions%2520averaging%2520above%25201000%250Awords%2520each.%2520With%2520precise%2520and%2520reliable%2520captions%2520associated%2520with%2520specific%2520parts%250Aof%2520an%2520image%252C%2520we%2520can%2520evaluate%2520vision-language%2520models%2527%2520%2528VLMs%2529%2520understanding%2520of%250Aimage%2520content%2520with%2520a%2520novel%2520task%2520that%2520matches%2520each%2520caption%2520with%2520its%250Acorresponding%2520subcrop.%2520As%2520current%2520models%2520are%2520often%2520limited%2520to%252077%2520text%2520tokens%252C%250Awe%2520also%2520introduce%2520a%2520summarized%2520version%2520%2528sDCI%2529%2520in%2520which%2520each%2520caption%2520length%2520is%250Alimited.%2520We%2520show%2520that%2520modern%2520techniques%2520that%2520make%2520progress%2520on%2520standard%250Abenchmarks%2520do%2520not%2520correspond%2520with%2520significant%2520improvement%2520on%2520our%2520sDCI%2520based%250Abenchmark.%2520Lastly%252C%2520we%2520finetune%2520CLIP%2520using%2520sDCI%2520and%2520show%2520significant%250Aimprovements%2520over%2520the%2520baseline%2520despite%2520a%2520small%2520training%2520set.%2520By%2520releasing%2520the%250Afirst%2520human%2520annotated%2520dense%2520image%2520captioning%2520dataset%252C%2520we%2520hope%2520to%2520enable%2520the%250Adevelopment%2520of%2520new%2520benchmarks%2520or%2520fine-tuning%2520recipes%2520for%2520the%2520next%2520generation%2520of%250AVLMs%2520to%2520come.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.08578v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Picture%20is%20Worth%20More%20Than%2077%20Text%20Tokens%3A%20Evaluating%20CLIP-Style%0A%20%20Models%20on%20Dense%20Captions&entry.906535625=Jack%20Urbanek%20and%20Florian%20Bordes%20and%20Pietro%20Astolfi%20and%20Mary%20Williamson%20and%20Vasu%20Sharma%20and%20Adriana%20Romero-Soriano&entry.1292438233=%20%20Curation%20methods%20for%20massive%20vision-language%20datasets%20trade%20off%20between%0Adataset%20size%20and%20quality.%20However%2C%20even%20the%20highest%20quality%20of%20available%0Acurated%20captions%20are%20far%20too%20short%20to%20capture%20the%20rich%20visual%20detail%20in%20an%0Aimage.%20To%20show%20the%20value%20of%20dense%20and%20highly-aligned%20image-text%20pairs%2C%20we%0Acollect%20the%20Densely%20Captioned%20Images%20%28DCI%29%20dataset%2C%20containing%207805%20natural%0Aimages%20human-annotated%20with%20mask-aligned%20descriptions%20averaging%20above%201000%0Awords%20each.%20With%20precise%20and%20reliable%20captions%20associated%20with%20specific%20parts%0Aof%20an%20image%2C%20we%20can%20evaluate%20vision-language%20models%27%20%28VLMs%29%20understanding%20of%0Aimage%20content%20with%20a%20novel%20task%20that%20matches%20each%20caption%20with%20its%0Acorresponding%20subcrop.%20As%20current%20models%20are%20often%20limited%20to%2077%20text%20tokens%2C%0Awe%20also%20introduce%20a%20summarized%20version%20%28sDCI%29%20in%20which%20each%20caption%20length%20is%0Alimited.%20We%20show%20that%20modern%20techniques%20that%20make%20progress%20on%20standard%0Abenchmarks%20do%20not%20correspond%20with%20significant%20improvement%20on%20our%20sDCI%20based%0Abenchmark.%20Lastly%2C%20we%20finetune%20CLIP%20using%20sDCI%20and%20show%20significant%0Aimprovements%20over%20the%20baseline%20despite%20a%20small%20training%20set.%20By%20releasing%20the%0Afirst%20human%20annotated%20dense%20image%20captioning%20dataset%2C%20we%20hope%20to%20enable%20the%0Adevelopment%20of%20new%20benchmarks%20or%20fine-tuning%20recipes%20for%20the%20next%20generation%20of%0AVLMs%20to%20come.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.08578v2&entry.124074799=Read"},
{"title": "Beyond Embeddings: The Promise of Visual Table in Visual Reasoning", "author": "Yiwu Zhong and Zi-Yuan Hu and Michael R. Lyu and Liwei Wang", "abstract": "  Visual representation learning has been a cornerstone in computer vision,\ninvolving typical forms such as visual embeddings, structural symbols, and\ntext-based representations. Despite the success of CLIP-type visual embeddings,\nthey often lack access to world knowledge critical for visual reasoning. In\nthis work, we propose Visual Table, a novel form of visual representation\ntailored for visual reasoning. Visual tables are constructed as hierarchical\ndescriptions of visual scenes, featuring a scene description and multiple\nobject-centric descriptions covering categories, attributes, and knowledge.\nThanks to the structural and textual formats, visual tables offer unique\nadvantages over mere visual embeddings, such as interpretability and\ncontrollable editing. Furthermore, they deliver instance-level world knowledge\nand detailed attributes that are essential for visual reasoning. To create\nvisual tables, we develop a generator trained on the dataset with collected,\nsmall-scale annotations. Extensive results on 11 visual reasoning benchmarks\ndemonstrate that the generated visual tables significantly outperform previous\nstructural and text-based representations. Moreover, they consistently enhance\nstate-of-the-art multimodal large language models across diverse benchmarks,\nshowcasing their potential for advancing visual reasoning tasks. Our code is\navailable at https://github.com/LaVi-Lab/Visual-Table.\n", "link": "http://arxiv.org/abs/2403.18252v2", "date": "2024-06-17", "relevancy": 2.1375, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5404}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5331}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5225}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Embeddings%3A%20The%20Promise%20of%20Visual%20Table%20in%20Visual%20Reasoning&body=Title%3A%20Beyond%20Embeddings%3A%20The%20Promise%20of%20Visual%20Table%20in%20Visual%20Reasoning%0AAuthor%3A%20Yiwu%20Zhong%20and%20Zi-Yuan%20Hu%20and%20Michael%20R.%20Lyu%20and%20Liwei%20Wang%0AAbstract%3A%20%20%20Visual%20representation%20learning%20has%20been%20a%20cornerstone%20in%20computer%20vision%2C%0Ainvolving%20typical%20forms%20such%20as%20visual%20embeddings%2C%20structural%20symbols%2C%20and%0Atext-based%20representations.%20Despite%20the%20success%20of%20CLIP-type%20visual%20embeddings%2C%0Athey%20often%20lack%20access%20to%20world%20knowledge%20critical%20for%20visual%20reasoning.%20In%0Athis%20work%2C%20we%20propose%20Visual%20Table%2C%20a%20novel%20form%20of%20visual%20representation%0Atailored%20for%20visual%20reasoning.%20Visual%20tables%20are%20constructed%20as%20hierarchical%0Adescriptions%20of%20visual%20scenes%2C%20featuring%20a%20scene%20description%20and%20multiple%0Aobject-centric%20descriptions%20covering%20categories%2C%20attributes%2C%20and%20knowledge.%0AThanks%20to%20the%20structural%20and%20textual%20formats%2C%20visual%20tables%20offer%20unique%0Aadvantages%20over%20mere%20visual%20embeddings%2C%20such%20as%20interpretability%20and%0Acontrollable%20editing.%20Furthermore%2C%20they%20deliver%20instance-level%20world%20knowledge%0Aand%20detailed%20attributes%20that%20are%20essential%20for%20visual%20reasoning.%20To%20create%0Avisual%20tables%2C%20we%20develop%20a%20generator%20trained%20on%20the%20dataset%20with%20collected%2C%0Asmall-scale%20annotations.%20Extensive%20results%20on%2011%20visual%20reasoning%20benchmarks%0Ademonstrate%20that%20the%20generated%20visual%20tables%20significantly%20outperform%20previous%0Astructural%20and%20text-based%20representations.%20Moreover%2C%20they%20consistently%20enhance%0Astate-of-the-art%20multimodal%20large%20language%20models%20across%20diverse%20benchmarks%2C%0Ashowcasing%20their%20potential%20for%20advancing%20visual%20reasoning%20tasks.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/LaVi-Lab/Visual-Table.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18252v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Embeddings%253A%2520The%2520Promise%2520of%2520Visual%2520Table%2520in%2520Visual%2520Reasoning%26entry.906535625%3DYiwu%2520Zhong%2520and%2520Zi-Yuan%2520Hu%2520and%2520Michael%2520R.%2520Lyu%2520and%2520Liwei%2520Wang%26entry.1292438233%3D%2520%2520Visual%2520representation%2520learning%2520has%2520been%2520a%2520cornerstone%2520in%2520computer%2520vision%252C%250Ainvolving%2520typical%2520forms%2520such%2520as%2520visual%2520embeddings%252C%2520structural%2520symbols%252C%2520and%250Atext-based%2520representations.%2520Despite%2520the%2520success%2520of%2520CLIP-type%2520visual%2520embeddings%252C%250Athey%2520often%2520lack%2520access%2520to%2520world%2520knowledge%2520critical%2520for%2520visual%2520reasoning.%2520In%250Athis%2520work%252C%2520we%2520propose%2520Visual%2520Table%252C%2520a%2520novel%2520form%2520of%2520visual%2520representation%250Atailored%2520for%2520visual%2520reasoning.%2520Visual%2520tables%2520are%2520constructed%2520as%2520hierarchical%250Adescriptions%2520of%2520visual%2520scenes%252C%2520featuring%2520a%2520scene%2520description%2520and%2520multiple%250Aobject-centric%2520descriptions%2520covering%2520categories%252C%2520attributes%252C%2520and%2520knowledge.%250AThanks%2520to%2520the%2520structural%2520and%2520textual%2520formats%252C%2520visual%2520tables%2520offer%2520unique%250Aadvantages%2520over%2520mere%2520visual%2520embeddings%252C%2520such%2520as%2520interpretability%2520and%250Acontrollable%2520editing.%2520Furthermore%252C%2520they%2520deliver%2520instance-level%2520world%2520knowledge%250Aand%2520detailed%2520attributes%2520that%2520are%2520essential%2520for%2520visual%2520reasoning.%2520To%2520create%250Avisual%2520tables%252C%2520we%2520develop%2520a%2520generator%2520trained%2520on%2520the%2520dataset%2520with%2520collected%252C%250Asmall-scale%2520annotations.%2520Extensive%2520results%2520on%252011%2520visual%2520reasoning%2520benchmarks%250Ademonstrate%2520that%2520the%2520generated%2520visual%2520tables%2520significantly%2520outperform%2520previous%250Astructural%2520and%2520text-based%2520representations.%2520Moreover%252C%2520they%2520consistently%2520enhance%250Astate-of-the-art%2520multimodal%2520large%2520language%2520models%2520across%2520diverse%2520benchmarks%252C%250Ashowcasing%2520their%2520potential%2520for%2520advancing%2520visual%2520reasoning%2520tasks.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/LaVi-Lab/Visual-Table.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.18252v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Embeddings%3A%20The%20Promise%20of%20Visual%20Table%20in%20Visual%20Reasoning&entry.906535625=Yiwu%20Zhong%20and%20Zi-Yuan%20Hu%20and%20Michael%20R.%20Lyu%20and%20Liwei%20Wang&entry.1292438233=%20%20Visual%20representation%20learning%20has%20been%20a%20cornerstone%20in%20computer%20vision%2C%0Ainvolving%20typical%20forms%20such%20as%20visual%20embeddings%2C%20structural%20symbols%2C%20and%0Atext-based%20representations.%20Despite%20the%20success%20of%20CLIP-type%20visual%20embeddings%2C%0Athey%20often%20lack%20access%20to%20world%20knowledge%20critical%20for%20visual%20reasoning.%20In%0Athis%20work%2C%20we%20propose%20Visual%20Table%2C%20a%20novel%20form%20of%20visual%20representation%0Atailored%20for%20visual%20reasoning.%20Visual%20tables%20are%20constructed%20as%20hierarchical%0Adescriptions%20of%20visual%20scenes%2C%20featuring%20a%20scene%20description%20and%20multiple%0Aobject-centric%20descriptions%20covering%20categories%2C%20attributes%2C%20and%20knowledge.%0AThanks%20to%20the%20structural%20and%20textual%20formats%2C%20visual%20tables%20offer%20unique%0Aadvantages%20over%20mere%20visual%20embeddings%2C%20such%20as%20interpretability%20and%0Acontrollable%20editing.%20Furthermore%2C%20they%20deliver%20instance-level%20world%20knowledge%0Aand%20detailed%20attributes%20that%20are%20essential%20for%20visual%20reasoning.%20To%20create%0Avisual%20tables%2C%20we%20develop%20a%20generator%20trained%20on%20the%20dataset%20with%20collected%2C%0Asmall-scale%20annotations.%20Extensive%20results%20on%2011%20visual%20reasoning%20benchmarks%0Ademonstrate%20that%20the%20generated%20visual%20tables%20significantly%20outperform%20previous%0Astructural%20and%20text-based%20representations.%20Moreover%2C%20they%20consistently%20enhance%0Astate-of-the-art%20multimodal%20large%20language%20models%20across%20diverse%20benchmarks%2C%0Ashowcasing%20their%20potential%20for%20advancing%20visual%20reasoning%20tasks.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/LaVi-Lab/Visual-Table.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18252v2&entry.124074799=Read"},
{"title": "pFedLVM: A Large Vision Model (LVM)-Driven and Latent Feature-Based\n  Personalized Federated Learning Framework in Autonomous Driving", "author": "Wei-Bin Kou and Qingfeng Lin and Ming Tang and Sheng Xu and Rongguang Ye and Yang Leng and Shuai Wang and Guofa Li and Zhenyu Chen and Guangxu Zhu and Yik-Chung Wu", "abstract": "  Deep learning-based Autonomous Driving (AD) models often exhibit poor\ngeneralization due to data heterogeneity in an ever domain-shifting\nenvironment. While Federated Learning (FL) could improve the generalization of\nan AD model (known as FedAD system), conventional models often struggle with\nunder-fitting as the amount of accumulated training data progressively\nincreases. To address this issue, instead of conventional small models,\nemploying Large Vision Models (LVMs) in FedAD is a viable option for better\nlearning of representations from a vast volume of data. However, implementing\nLVMs in FedAD introduces three challenges: (I) the extremely high communication\noverheads associated with transmitting LVMs between participating vehicles and\na central server; (II) lack of computing resource to deploy LVMs on each\nvehicle; (III) the performance drop due to LVM focusing on shared features but\noverlooking local vehicle characteristics. To overcome these challenges, we\npropose pFedLVM, a LVM-Driven, Latent Feature-Based Personalized Federated\nLearning framework. In this approach, the LVM is deployed only on central\nserver, which effectively alleviates the computational burden on individual\nvehicles. Furthermore, the exchange between central server and vehicles are the\nlearned features rather than the LVM parameters, which significantly reduces\ncommunication overhead. In addition, we utilize both shared features from all\nparticipating vehicles and individual characteristics from each vehicle to\nestablish a personalized learning mechanism. This enables each vehicle's model\nto learn features from others while preserving its personalized\ncharacteristics, thereby outperforming globally shared models trained in\ngeneral FL. Extensive experiments demonstrate that pFedLVM outperforms the\nexisting state-of-the-art approaches.\n", "link": "http://arxiv.org/abs/2405.04146v2", "date": "2024-06-17", "relevancy": 2.1359, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.558}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5201}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20pFedLVM%3A%20A%20Large%20Vision%20Model%20%28LVM%29-Driven%20and%20Latent%20Feature-Based%0A%20%20Personalized%20Federated%20Learning%20Framework%20in%20Autonomous%20Driving&body=Title%3A%20pFedLVM%3A%20A%20Large%20Vision%20Model%20%28LVM%29-Driven%20and%20Latent%20Feature-Based%0A%20%20Personalized%20Federated%20Learning%20Framework%20in%20Autonomous%20Driving%0AAuthor%3A%20Wei-Bin%20Kou%20and%20Qingfeng%20Lin%20and%20Ming%20Tang%20and%20Sheng%20Xu%20and%20Rongguang%20Ye%20and%20Yang%20Leng%20and%20Shuai%20Wang%20and%20Guofa%20Li%20and%20Zhenyu%20Chen%20and%20Guangxu%20Zhu%20and%20Yik-Chung%20Wu%0AAbstract%3A%20%20%20Deep%20learning-based%20Autonomous%20Driving%20%28AD%29%20models%20often%20exhibit%20poor%0Ageneralization%20due%20to%20data%20heterogeneity%20in%20an%20ever%20domain-shifting%0Aenvironment.%20While%20Federated%20Learning%20%28FL%29%20could%20improve%20the%20generalization%20of%0Aan%20AD%20model%20%28known%20as%20FedAD%20system%29%2C%20conventional%20models%20often%20struggle%20with%0Aunder-fitting%20as%20the%20amount%20of%20accumulated%20training%20data%20progressively%0Aincreases.%20To%20address%20this%20issue%2C%20instead%20of%20conventional%20small%20models%2C%0Aemploying%20Large%20Vision%20Models%20%28LVMs%29%20in%20FedAD%20is%20a%20viable%20option%20for%20better%0Alearning%20of%20representations%20from%20a%20vast%20volume%20of%20data.%20However%2C%20implementing%0ALVMs%20in%20FedAD%20introduces%20three%20challenges%3A%20%28I%29%20the%20extremely%20high%20communication%0Aoverheads%20associated%20with%20transmitting%20LVMs%20between%20participating%20vehicles%20and%0Aa%20central%20server%3B%20%28II%29%20lack%20of%20computing%20resource%20to%20deploy%20LVMs%20on%20each%0Avehicle%3B%20%28III%29%20the%20performance%20drop%20due%20to%20LVM%20focusing%20on%20shared%20features%20but%0Aoverlooking%20local%20vehicle%20characteristics.%20To%20overcome%20these%20challenges%2C%20we%0Apropose%20pFedLVM%2C%20a%20LVM-Driven%2C%20Latent%20Feature-Based%20Personalized%20Federated%0ALearning%20framework.%20In%20this%20approach%2C%20the%20LVM%20is%20deployed%20only%20on%20central%0Aserver%2C%20which%20effectively%20alleviates%20the%20computational%20burden%20on%20individual%0Avehicles.%20Furthermore%2C%20the%20exchange%20between%20central%20server%20and%20vehicles%20are%20the%0Alearned%20features%20rather%20than%20the%20LVM%20parameters%2C%20which%20significantly%20reduces%0Acommunication%20overhead.%20In%20addition%2C%20we%20utilize%20both%20shared%20features%20from%20all%0Aparticipating%20vehicles%20and%20individual%20characteristics%20from%20each%20vehicle%20to%0Aestablish%20a%20personalized%20learning%20mechanism.%20This%20enables%20each%20vehicle%27s%20model%0Ato%20learn%20features%20from%20others%20while%20preserving%20its%20personalized%0Acharacteristics%2C%20thereby%20outperforming%20globally%20shared%20models%20trained%20in%0Ageneral%20FL.%20Extensive%20experiments%20demonstrate%20that%20pFedLVM%20outperforms%20the%0Aexisting%20state-of-the-art%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.04146v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DpFedLVM%253A%2520A%2520Large%2520Vision%2520Model%2520%2528LVM%2529-Driven%2520and%2520Latent%2520Feature-Based%250A%2520%2520Personalized%2520Federated%2520Learning%2520Framework%2520in%2520Autonomous%2520Driving%26entry.906535625%3DWei-Bin%2520Kou%2520and%2520Qingfeng%2520Lin%2520and%2520Ming%2520Tang%2520and%2520Sheng%2520Xu%2520and%2520Rongguang%2520Ye%2520and%2520Yang%2520Leng%2520and%2520Shuai%2520Wang%2520and%2520Guofa%2520Li%2520and%2520Zhenyu%2520Chen%2520and%2520Guangxu%2520Zhu%2520and%2520Yik-Chung%2520Wu%26entry.1292438233%3D%2520%2520Deep%2520learning-based%2520Autonomous%2520Driving%2520%2528AD%2529%2520models%2520often%2520exhibit%2520poor%250Ageneralization%2520due%2520to%2520data%2520heterogeneity%2520in%2520an%2520ever%2520domain-shifting%250Aenvironment.%2520While%2520Federated%2520Learning%2520%2528FL%2529%2520could%2520improve%2520the%2520generalization%2520of%250Aan%2520AD%2520model%2520%2528known%2520as%2520FedAD%2520system%2529%252C%2520conventional%2520models%2520often%2520struggle%2520with%250Aunder-fitting%2520as%2520the%2520amount%2520of%2520accumulated%2520training%2520data%2520progressively%250Aincreases.%2520To%2520address%2520this%2520issue%252C%2520instead%2520of%2520conventional%2520small%2520models%252C%250Aemploying%2520Large%2520Vision%2520Models%2520%2528LVMs%2529%2520in%2520FedAD%2520is%2520a%2520viable%2520option%2520for%2520better%250Alearning%2520of%2520representations%2520from%2520a%2520vast%2520volume%2520of%2520data.%2520However%252C%2520implementing%250ALVMs%2520in%2520FedAD%2520introduces%2520three%2520challenges%253A%2520%2528I%2529%2520the%2520extremely%2520high%2520communication%250Aoverheads%2520associated%2520with%2520transmitting%2520LVMs%2520between%2520participating%2520vehicles%2520and%250Aa%2520central%2520server%253B%2520%2528II%2529%2520lack%2520of%2520computing%2520resource%2520to%2520deploy%2520LVMs%2520on%2520each%250Avehicle%253B%2520%2528III%2529%2520the%2520performance%2520drop%2520due%2520to%2520LVM%2520focusing%2520on%2520shared%2520features%2520but%250Aoverlooking%2520local%2520vehicle%2520characteristics.%2520To%2520overcome%2520these%2520challenges%252C%2520we%250Apropose%2520pFedLVM%252C%2520a%2520LVM-Driven%252C%2520Latent%2520Feature-Based%2520Personalized%2520Federated%250ALearning%2520framework.%2520In%2520this%2520approach%252C%2520the%2520LVM%2520is%2520deployed%2520only%2520on%2520central%250Aserver%252C%2520which%2520effectively%2520alleviates%2520the%2520computational%2520burden%2520on%2520individual%250Avehicles.%2520Furthermore%252C%2520the%2520exchange%2520between%2520central%2520server%2520and%2520vehicles%2520are%2520the%250Alearned%2520features%2520rather%2520than%2520the%2520LVM%2520parameters%252C%2520which%2520significantly%2520reduces%250Acommunication%2520overhead.%2520In%2520addition%252C%2520we%2520utilize%2520both%2520shared%2520features%2520from%2520all%250Aparticipating%2520vehicles%2520and%2520individual%2520characteristics%2520from%2520each%2520vehicle%2520to%250Aestablish%2520a%2520personalized%2520learning%2520mechanism.%2520This%2520enables%2520each%2520vehicle%2527s%2520model%250Ato%2520learn%2520features%2520from%2520others%2520while%2520preserving%2520its%2520personalized%250Acharacteristics%252C%2520thereby%2520outperforming%2520globally%2520shared%2520models%2520trained%2520in%250Ageneral%2520FL.%2520Extensive%2520experiments%2520demonstrate%2520that%2520pFedLVM%2520outperforms%2520the%250Aexisting%2520state-of-the-art%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.04146v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=pFedLVM%3A%20A%20Large%20Vision%20Model%20%28LVM%29-Driven%20and%20Latent%20Feature-Based%0A%20%20Personalized%20Federated%20Learning%20Framework%20in%20Autonomous%20Driving&entry.906535625=Wei-Bin%20Kou%20and%20Qingfeng%20Lin%20and%20Ming%20Tang%20and%20Sheng%20Xu%20and%20Rongguang%20Ye%20and%20Yang%20Leng%20and%20Shuai%20Wang%20and%20Guofa%20Li%20and%20Zhenyu%20Chen%20and%20Guangxu%20Zhu%20and%20Yik-Chung%20Wu&entry.1292438233=%20%20Deep%20learning-based%20Autonomous%20Driving%20%28AD%29%20models%20often%20exhibit%20poor%0Ageneralization%20due%20to%20data%20heterogeneity%20in%20an%20ever%20domain-shifting%0Aenvironment.%20While%20Federated%20Learning%20%28FL%29%20could%20improve%20the%20generalization%20of%0Aan%20AD%20model%20%28known%20as%20FedAD%20system%29%2C%20conventional%20models%20often%20struggle%20with%0Aunder-fitting%20as%20the%20amount%20of%20accumulated%20training%20data%20progressively%0Aincreases.%20To%20address%20this%20issue%2C%20instead%20of%20conventional%20small%20models%2C%0Aemploying%20Large%20Vision%20Models%20%28LVMs%29%20in%20FedAD%20is%20a%20viable%20option%20for%20better%0Alearning%20of%20representations%20from%20a%20vast%20volume%20of%20data.%20However%2C%20implementing%0ALVMs%20in%20FedAD%20introduces%20three%20challenges%3A%20%28I%29%20the%20extremely%20high%20communication%0Aoverheads%20associated%20with%20transmitting%20LVMs%20between%20participating%20vehicles%20and%0Aa%20central%20server%3B%20%28II%29%20lack%20of%20computing%20resource%20to%20deploy%20LVMs%20on%20each%0Avehicle%3B%20%28III%29%20the%20performance%20drop%20due%20to%20LVM%20focusing%20on%20shared%20features%20but%0Aoverlooking%20local%20vehicle%20characteristics.%20To%20overcome%20these%20challenges%2C%20we%0Apropose%20pFedLVM%2C%20a%20LVM-Driven%2C%20Latent%20Feature-Based%20Personalized%20Federated%0ALearning%20framework.%20In%20this%20approach%2C%20the%20LVM%20is%20deployed%20only%20on%20central%0Aserver%2C%20which%20effectively%20alleviates%20the%20computational%20burden%20on%20individual%0Avehicles.%20Furthermore%2C%20the%20exchange%20between%20central%20server%20and%20vehicles%20are%20the%0Alearned%20features%20rather%20than%20the%20LVM%20parameters%2C%20which%20significantly%20reduces%0Acommunication%20overhead.%20In%20addition%2C%20we%20utilize%20both%20shared%20features%20from%20all%0Aparticipating%20vehicles%20and%20individual%20characteristics%20from%20each%20vehicle%20to%0Aestablish%20a%20personalized%20learning%20mechanism.%20This%20enables%20each%20vehicle%27s%20model%0Ato%20learn%20features%20from%20others%20while%20preserving%20its%20personalized%0Acharacteristics%2C%20thereby%20outperforming%20globally%20shared%20models%20trained%20in%0Ageneral%20FL.%20Extensive%20experiments%20demonstrate%20that%20pFedLVM%20outperforms%20the%0Aexisting%20state-of-the-art%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.04146v2&entry.124074799=Read"},
{"title": "Lightweight Model Pre-training via Language Guided Knowledge\n  Distillation", "author": "Mingsheng Li and Lin Zhang and Mingzhen Zhu and Zilong Huang and Gang Yu and Jiayuan Fan and Tao Chen", "abstract": "  This paper studies the problem of pre-training for small models, which is\nessential for many mobile devices. Current state-of-the-art methods on this\nproblem transfer the representational knowledge of a large network (as a\nTeacher) into a smaller model (as a Student) using self-supervised\ndistillation, improving the performance of the small model on downstream tasks.\nHowever, existing approaches are insufficient in extracting the crucial\nknowledge that is useful for discerning categories in downstream tasks during\nthe distillation process. In this paper, for the first time, we introduce\nlanguage guidance to the distillation process and propose a new method named\nLanguage-Guided Distillation (LGD) system, which uses category names of the\ntarget downstream task to help refine the knowledge transferred between the\nteacher and student. To this end, we utilize a pre-trained text encoder to\nextract semantic embeddings from language and construct a textual semantic\nspace called Textual Semantics Bank (TSB). Furthermore, we design a\nLanguage-Guided Knowledge Aggregation (LGKA) module to construct the visual\nsemantic space, also named Visual Semantics Bank (VSB). The task-related\nknowledge is transferred by driving a student encoder to mimic the similarity\nscore distribution inferred by a teacher over TSB and VSB. Compared with other\nsmall models obtained by either ImageNet pre-training or self-supervised\ndistillation, experiment results show that the distilled lightweight model\nusing the proposed LGD method presents state-of-the-art performance and is\nvalidated on various downstream tasks, including classification, detection, and\nsegmentation. We have made the code available at https://github.com/mZhenz/LGD.\n", "link": "http://arxiv.org/abs/2406.11689v1", "date": "2024-06-17", "relevancy": 2.1274, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5376}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5278}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lightweight%20Model%20Pre-training%20via%20Language%20Guided%20Knowledge%0A%20%20Distillation&body=Title%3A%20Lightweight%20Model%20Pre-training%20via%20Language%20Guided%20Knowledge%0A%20%20Distillation%0AAuthor%3A%20Mingsheng%20Li%20and%20Lin%20Zhang%20and%20Mingzhen%20Zhu%20and%20Zilong%20Huang%20and%20Gang%20Yu%20and%20Jiayuan%20Fan%20and%20Tao%20Chen%0AAbstract%3A%20%20%20This%20paper%20studies%20the%20problem%20of%20pre-training%20for%20small%20models%2C%20which%20is%0Aessential%20for%20many%20mobile%20devices.%20Current%20state-of-the-art%20methods%20on%20this%0Aproblem%20transfer%20the%20representational%20knowledge%20of%20a%20large%20network%20%28as%20a%0ATeacher%29%20into%20a%20smaller%20model%20%28as%20a%20Student%29%20using%20self-supervised%0Adistillation%2C%20improving%20the%20performance%20of%20the%20small%20model%20on%20downstream%20tasks.%0AHowever%2C%20existing%20approaches%20are%20insufficient%20in%20extracting%20the%20crucial%0Aknowledge%20that%20is%20useful%20for%20discerning%20categories%20in%20downstream%20tasks%20during%0Athe%20distillation%20process.%20In%20this%20paper%2C%20for%20the%20first%20time%2C%20we%20introduce%0Alanguage%20guidance%20to%20the%20distillation%20process%20and%20propose%20a%20new%20method%20named%0ALanguage-Guided%20Distillation%20%28LGD%29%20system%2C%20which%20uses%20category%20names%20of%20the%0Atarget%20downstream%20task%20to%20help%20refine%20the%20knowledge%20transferred%20between%20the%0Ateacher%20and%20student.%20To%20this%20end%2C%20we%20utilize%20a%20pre-trained%20text%20encoder%20to%0Aextract%20semantic%20embeddings%20from%20language%20and%20construct%20a%20textual%20semantic%0Aspace%20called%20Textual%20Semantics%20Bank%20%28TSB%29.%20Furthermore%2C%20we%20design%20a%0ALanguage-Guided%20Knowledge%20Aggregation%20%28LGKA%29%20module%20to%20construct%20the%20visual%0Asemantic%20space%2C%20also%20named%20Visual%20Semantics%20Bank%20%28VSB%29.%20The%20task-related%0Aknowledge%20is%20transferred%20by%20driving%20a%20student%20encoder%20to%20mimic%20the%20similarity%0Ascore%20distribution%20inferred%20by%20a%20teacher%20over%20TSB%20and%20VSB.%20Compared%20with%20other%0Asmall%20models%20obtained%20by%20either%20ImageNet%20pre-training%20or%20self-supervised%0Adistillation%2C%20experiment%20results%20show%20that%20the%20distilled%20lightweight%20model%0Ausing%20the%20proposed%20LGD%20method%20presents%20state-of-the-art%20performance%20and%20is%0Avalidated%20on%20various%20downstream%20tasks%2C%20including%20classification%2C%20detection%2C%20and%0Asegmentation.%20We%20have%20made%20the%20code%20available%20at%20https%3A//github.com/mZhenz/LGD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11689v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightweight%2520Model%2520Pre-training%2520via%2520Language%2520Guided%2520Knowledge%250A%2520%2520Distillation%26entry.906535625%3DMingsheng%2520Li%2520and%2520Lin%2520Zhang%2520and%2520Mingzhen%2520Zhu%2520and%2520Zilong%2520Huang%2520and%2520Gang%2520Yu%2520and%2520Jiayuan%2520Fan%2520and%2520Tao%2520Chen%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520the%2520problem%2520of%2520pre-training%2520for%2520small%2520models%252C%2520which%2520is%250Aessential%2520for%2520many%2520mobile%2520devices.%2520Current%2520state-of-the-art%2520methods%2520on%2520this%250Aproblem%2520transfer%2520the%2520representational%2520knowledge%2520of%2520a%2520large%2520network%2520%2528as%2520a%250ATeacher%2529%2520into%2520a%2520smaller%2520model%2520%2528as%2520a%2520Student%2529%2520using%2520self-supervised%250Adistillation%252C%2520improving%2520the%2520performance%2520of%2520the%2520small%2520model%2520on%2520downstream%2520tasks.%250AHowever%252C%2520existing%2520approaches%2520are%2520insufficient%2520in%2520extracting%2520the%2520crucial%250Aknowledge%2520that%2520is%2520useful%2520for%2520discerning%2520categories%2520in%2520downstream%2520tasks%2520during%250Athe%2520distillation%2520process.%2520In%2520this%2520paper%252C%2520for%2520the%2520first%2520time%252C%2520we%2520introduce%250Alanguage%2520guidance%2520to%2520the%2520distillation%2520process%2520and%2520propose%2520a%2520new%2520method%2520named%250ALanguage-Guided%2520Distillation%2520%2528LGD%2529%2520system%252C%2520which%2520uses%2520category%2520names%2520of%2520the%250Atarget%2520downstream%2520task%2520to%2520help%2520refine%2520the%2520knowledge%2520transferred%2520between%2520the%250Ateacher%2520and%2520student.%2520To%2520this%2520end%252C%2520we%2520utilize%2520a%2520pre-trained%2520text%2520encoder%2520to%250Aextract%2520semantic%2520embeddings%2520from%2520language%2520and%2520construct%2520a%2520textual%2520semantic%250Aspace%2520called%2520Textual%2520Semantics%2520Bank%2520%2528TSB%2529.%2520Furthermore%252C%2520we%2520design%2520a%250ALanguage-Guided%2520Knowledge%2520Aggregation%2520%2528LGKA%2529%2520module%2520to%2520construct%2520the%2520visual%250Asemantic%2520space%252C%2520also%2520named%2520Visual%2520Semantics%2520Bank%2520%2528VSB%2529.%2520The%2520task-related%250Aknowledge%2520is%2520transferred%2520by%2520driving%2520a%2520student%2520encoder%2520to%2520mimic%2520the%2520similarity%250Ascore%2520distribution%2520inferred%2520by%2520a%2520teacher%2520over%2520TSB%2520and%2520VSB.%2520Compared%2520with%2520other%250Asmall%2520models%2520obtained%2520by%2520either%2520ImageNet%2520pre-training%2520or%2520self-supervised%250Adistillation%252C%2520experiment%2520results%2520show%2520that%2520the%2520distilled%2520lightweight%2520model%250Ausing%2520the%2520proposed%2520LGD%2520method%2520presents%2520state-of-the-art%2520performance%2520and%2520is%250Avalidated%2520on%2520various%2520downstream%2520tasks%252C%2520including%2520classification%252C%2520detection%252C%2520and%250Asegmentation.%2520We%2520have%2520made%2520the%2520code%2520available%2520at%2520https%253A//github.com/mZhenz/LGD.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11689v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lightweight%20Model%20Pre-training%20via%20Language%20Guided%20Knowledge%0A%20%20Distillation&entry.906535625=Mingsheng%20Li%20and%20Lin%20Zhang%20and%20Mingzhen%20Zhu%20and%20Zilong%20Huang%20and%20Gang%20Yu%20and%20Jiayuan%20Fan%20and%20Tao%20Chen&entry.1292438233=%20%20This%20paper%20studies%20the%20problem%20of%20pre-training%20for%20small%20models%2C%20which%20is%0Aessential%20for%20many%20mobile%20devices.%20Current%20state-of-the-art%20methods%20on%20this%0Aproblem%20transfer%20the%20representational%20knowledge%20of%20a%20large%20network%20%28as%20a%0ATeacher%29%20into%20a%20smaller%20model%20%28as%20a%20Student%29%20using%20self-supervised%0Adistillation%2C%20improving%20the%20performance%20of%20the%20small%20model%20on%20downstream%20tasks.%0AHowever%2C%20existing%20approaches%20are%20insufficient%20in%20extracting%20the%20crucial%0Aknowledge%20that%20is%20useful%20for%20discerning%20categories%20in%20downstream%20tasks%20during%0Athe%20distillation%20process.%20In%20this%20paper%2C%20for%20the%20first%20time%2C%20we%20introduce%0Alanguage%20guidance%20to%20the%20distillation%20process%20and%20propose%20a%20new%20method%20named%0ALanguage-Guided%20Distillation%20%28LGD%29%20system%2C%20which%20uses%20category%20names%20of%20the%0Atarget%20downstream%20task%20to%20help%20refine%20the%20knowledge%20transferred%20between%20the%0Ateacher%20and%20student.%20To%20this%20end%2C%20we%20utilize%20a%20pre-trained%20text%20encoder%20to%0Aextract%20semantic%20embeddings%20from%20language%20and%20construct%20a%20textual%20semantic%0Aspace%20called%20Textual%20Semantics%20Bank%20%28TSB%29.%20Furthermore%2C%20we%20design%20a%0ALanguage-Guided%20Knowledge%20Aggregation%20%28LGKA%29%20module%20to%20construct%20the%20visual%0Asemantic%20space%2C%20also%20named%20Visual%20Semantics%20Bank%20%28VSB%29.%20The%20task-related%0Aknowledge%20is%20transferred%20by%20driving%20a%20student%20encoder%20to%20mimic%20the%20similarity%0Ascore%20distribution%20inferred%20by%20a%20teacher%20over%20TSB%20and%20VSB.%20Compared%20with%20other%0Asmall%20models%20obtained%20by%20either%20ImageNet%20pre-training%20or%20self-supervised%0Adistillation%2C%20experiment%20results%20show%20that%20the%20distilled%20lightweight%20model%0Ausing%20the%20proposed%20LGD%20method%20presents%20state-of-the-art%20performance%20and%20is%0Avalidated%20on%20various%20downstream%20tasks%2C%20including%20classification%2C%20detection%2C%20and%0Asegmentation.%20We%20have%20made%20the%20code%20available%20at%20https%3A//github.com/mZhenz/LGD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11689v1&entry.124074799=Read"},
{"title": "Multimodal Structured Generation: CVPR's 2nd MMFM Challenge Technical\n  Report", "author": "Franz Louis Cesista", "abstract": "  Multimodal Foundation Models (MMFMs) have shown remarkable performance on\nvarious computer vision and natural language processing tasks. However, their\nperformance on particular tasks such as document understanding is still\nlimited. They also require more compute, time, and engineering resources to\nfinetune and deploy compared to traditional, unimodal models. In this report,\nwe present Multimodal Structured Generation, a general framework which\nconstrains the output logits of frozen MMFMs to force them to reason before\nresponding with structured outputs that downstream APIs can parse and use. We\nprovide a detailed account of our approach, including the technical details,\ntheoretical discussions, and final evaluation results in the 2nd Multimodal\nFoundation Models Challenge hosted by the Computer Vision and Pattern\nRecognition (CVPR) conference. Our approach achieved the second highest score\nin the hidden test set for Phase 2 and third highest overall. This shows the\nmethod's ability to generalize to unseen tasks. And that simple engineering can\nbeat expensive & complicated modelling steps as we first discussed in our\npaper, Retrieval Augmented Structured Generation: Business Document Information\nExtraction as Tool Use. All of our scripts, deployment steps, and evaluation\nresults can be accessed in https://github.com/leloykun/MMFM-Challenge\n", "link": "http://arxiv.org/abs/2406.11403v1", "date": "2024-06-17", "relevancy": 2.1249, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5383}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5317}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Structured%20Generation%3A%20CVPR%27s%202nd%20MMFM%20Challenge%20Technical%0A%20%20Report&body=Title%3A%20Multimodal%20Structured%20Generation%3A%20CVPR%27s%202nd%20MMFM%20Challenge%20Technical%0A%20%20Report%0AAuthor%3A%20Franz%20Louis%20Cesista%0AAbstract%3A%20%20%20Multimodal%20Foundation%20Models%20%28MMFMs%29%20have%20shown%20remarkable%20performance%20on%0Avarious%20computer%20vision%20and%20natural%20language%20processing%20tasks.%20However%2C%20their%0Aperformance%20on%20particular%20tasks%20such%20as%20document%20understanding%20is%20still%0Alimited.%20They%20also%20require%20more%20compute%2C%20time%2C%20and%20engineering%20resources%20to%0Afinetune%20and%20deploy%20compared%20to%20traditional%2C%20unimodal%20models.%20In%20this%20report%2C%0Awe%20present%20Multimodal%20Structured%20Generation%2C%20a%20general%20framework%20which%0Aconstrains%20the%20output%20logits%20of%20frozen%20MMFMs%20to%20force%20them%20to%20reason%20before%0Aresponding%20with%20structured%20outputs%20that%20downstream%20APIs%20can%20parse%20and%20use.%20We%0Aprovide%20a%20detailed%20account%20of%20our%20approach%2C%20including%20the%20technical%20details%2C%0Atheoretical%20discussions%2C%20and%20final%20evaluation%20results%20in%20the%202nd%20Multimodal%0AFoundation%20Models%20Challenge%20hosted%20by%20the%20Computer%20Vision%20and%20Pattern%0ARecognition%20%28CVPR%29%20conference.%20Our%20approach%20achieved%20the%20second%20highest%20score%0Ain%20the%20hidden%20test%20set%20for%20Phase%202%20and%20third%20highest%20overall.%20This%20shows%20the%0Amethod%27s%20ability%20to%20generalize%20to%20unseen%20tasks.%20And%20that%20simple%20engineering%20can%0Abeat%20expensive%20%26%20complicated%20modelling%20steps%20as%20we%20first%20discussed%20in%20our%0Apaper%2C%20Retrieval%20Augmented%20Structured%20Generation%3A%20Business%20Document%20Information%0AExtraction%20as%20Tool%20Use.%20All%20of%20our%20scripts%2C%20deployment%20steps%2C%20and%20evaluation%0Aresults%20can%20be%20accessed%20in%20https%3A//github.com/leloykun/MMFM-Challenge%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11403v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Structured%2520Generation%253A%2520CVPR%2527s%25202nd%2520MMFM%2520Challenge%2520Technical%250A%2520%2520Report%26entry.906535625%3DFranz%2520Louis%2520Cesista%26entry.1292438233%3D%2520%2520Multimodal%2520Foundation%2520Models%2520%2528MMFMs%2529%2520have%2520shown%2520remarkable%2520performance%2520on%250Avarious%2520computer%2520vision%2520and%2520natural%2520language%2520processing%2520tasks.%2520However%252C%2520their%250Aperformance%2520on%2520particular%2520tasks%2520such%2520as%2520document%2520understanding%2520is%2520still%250Alimited.%2520They%2520also%2520require%2520more%2520compute%252C%2520time%252C%2520and%2520engineering%2520resources%2520to%250Afinetune%2520and%2520deploy%2520compared%2520to%2520traditional%252C%2520unimodal%2520models.%2520In%2520this%2520report%252C%250Awe%2520present%2520Multimodal%2520Structured%2520Generation%252C%2520a%2520general%2520framework%2520which%250Aconstrains%2520the%2520output%2520logits%2520of%2520frozen%2520MMFMs%2520to%2520force%2520them%2520to%2520reason%2520before%250Aresponding%2520with%2520structured%2520outputs%2520that%2520downstream%2520APIs%2520can%2520parse%2520and%2520use.%2520We%250Aprovide%2520a%2520detailed%2520account%2520of%2520our%2520approach%252C%2520including%2520the%2520technical%2520details%252C%250Atheoretical%2520discussions%252C%2520and%2520final%2520evaluation%2520results%2520in%2520the%25202nd%2520Multimodal%250AFoundation%2520Models%2520Challenge%2520hosted%2520by%2520the%2520Computer%2520Vision%2520and%2520Pattern%250ARecognition%2520%2528CVPR%2529%2520conference.%2520Our%2520approach%2520achieved%2520the%2520second%2520highest%2520score%250Ain%2520the%2520hidden%2520test%2520set%2520for%2520Phase%25202%2520and%2520third%2520highest%2520overall.%2520This%2520shows%2520the%250Amethod%2527s%2520ability%2520to%2520generalize%2520to%2520unseen%2520tasks.%2520And%2520that%2520simple%2520engineering%2520can%250Abeat%2520expensive%2520%2526%2520complicated%2520modelling%2520steps%2520as%2520we%2520first%2520discussed%2520in%2520our%250Apaper%252C%2520Retrieval%2520Augmented%2520Structured%2520Generation%253A%2520Business%2520Document%2520Information%250AExtraction%2520as%2520Tool%2520Use.%2520All%2520of%2520our%2520scripts%252C%2520deployment%2520steps%252C%2520and%2520evaluation%250Aresults%2520can%2520be%2520accessed%2520in%2520https%253A//github.com/leloykun/MMFM-Challenge%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11403v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Structured%20Generation%3A%20CVPR%27s%202nd%20MMFM%20Challenge%20Technical%0A%20%20Report&entry.906535625=Franz%20Louis%20Cesista&entry.1292438233=%20%20Multimodal%20Foundation%20Models%20%28MMFMs%29%20have%20shown%20remarkable%20performance%20on%0Avarious%20computer%20vision%20and%20natural%20language%20processing%20tasks.%20However%2C%20their%0Aperformance%20on%20particular%20tasks%20such%20as%20document%20understanding%20is%20still%0Alimited.%20They%20also%20require%20more%20compute%2C%20time%2C%20and%20engineering%20resources%20to%0Afinetune%20and%20deploy%20compared%20to%20traditional%2C%20unimodal%20models.%20In%20this%20report%2C%0Awe%20present%20Multimodal%20Structured%20Generation%2C%20a%20general%20framework%20which%0Aconstrains%20the%20output%20logits%20of%20frozen%20MMFMs%20to%20force%20them%20to%20reason%20before%0Aresponding%20with%20structured%20outputs%20that%20downstream%20APIs%20can%20parse%20and%20use.%20We%0Aprovide%20a%20detailed%20account%20of%20our%20approach%2C%20including%20the%20technical%20details%2C%0Atheoretical%20discussions%2C%20and%20final%20evaluation%20results%20in%20the%202nd%20Multimodal%0AFoundation%20Models%20Challenge%20hosted%20by%20the%20Computer%20Vision%20and%20Pattern%0ARecognition%20%28CVPR%29%20conference.%20Our%20approach%20achieved%20the%20second%20highest%20score%0Ain%20the%20hidden%20test%20set%20for%20Phase%202%20and%20third%20highest%20overall.%20This%20shows%20the%0Amethod%27s%20ability%20to%20generalize%20to%20unseen%20tasks.%20And%20that%20simple%20engineering%20can%0Abeat%20expensive%20%26%20complicated%20modelling%20steps%20as%20we%20first%20discussed%20in%20our%0Apaper%2C%20Retrieval%20Augmented%20Structured%20Generation%3A%20Business%20Document%20Information%0AExtraction%20as%20Tool%20Use.%20All%20of%20our%20scripts%2C%20deployment%20steps%2C%20and%20evaluation%0Aresults%20can%20be%20accessed%20in%20https%3A//github.com/leloykun/MMFM-Challenge%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11403v1&entry.124074799=Read"},
{"title": "Understanding Multi-Granularity for Open-Vocabulary Part Segmentation", "author": "Jiho Choi and Seonho Lee and Seungho Lee and Minhyun Lee and Hyunjung Shim", "abstract": "  Open-vocabulary part segmentation (OVPS) is an emerging research area focused\non segmenting fine-grained entities based on diverse and previously unseen\nvocabularies. Our study highlights the inherent complexities of part\nsegmentation due to intricate boundaries and diverse granularity, reflecting\nthe knowledge-based nature of part identification. To address these challenges,\nwe propose PartCLIPSeg, a novel framework utilizing generalized parts and\nobject-level contexts to mitigate the lack of generalization in fine-grained\nparts. PartCLIPSeg integrates competitive part relationships and attention\ncontrol techniques, alleviating ambiguous boundaries and underrepresented\nparts. Experimental results demonstrate that PartCLIPSeg outperforms existing\nstate-of-the-art OVPS methods, offering refined segmentation and an advanced\nunderstanding of part relationships in images. Through extensive experiments,\nour model demonstrated an improvement over the state-of-the-art models on the\nPascal-Part-116, ADE20K-Part-234, and PartImageNet datasets.\n", "link": "http://arxiv.org/abs/2406.11384v1", "date": "2024-06-17", "relevancy": 2.124, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5519}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5223}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5005}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Multi-Granularity%20for%20Open-Vocabulary%20Part%20Segmentation&body=Title%3A%20Understanding%20Multi-Granularity%20for%20Open-Vocabulary%20Part%20Segmentation%0AAuthor%3A%20Jiho%20Choi%20and%20Seonho%20Lee%20and%20Seungho%20Lee%20and%20Minhyun%20Lee%20and%20Hyunjung%20Shim%0AAbstract%3A%20%20%20Open-vocabulary%20part%20segmentation%20%28OVPS%29%20is%20an%20emerging%20research%20area%20focused%0Aon%20segmenting%20fine-grained%20entities%20based%20on%20diverse%20and%20previously%20unseen%0Avocabularies.%20Our%20study%20highlights%20the%20inherent%20complexities%20of%20part%0Asegmentation%20due%20to%20intricate%20boundaries%20and%20diverse%20granularity%2C%20reflecting%0Athe%20knowledge-based%20nature%20of%20part%20identification.%20To%20address%20these%20challenges%2C%0Awe%20propose%20PartCLIPSeg%2C%20a%20novel%20framework%20utilizing%20generalized%20parts%20and%0Aobject-level%20contexts%20to%20mitigate%20the%20lack%20of%20generalization%20in%20fine-grained%0Aparts.%20PartCLIPSeg%20integrates%20competitive%20part%20relationships%20and%20attention%0Acontrol%20techniques%2C%20alleviating%20ambiguous%20boundaries%20and%20underrepresented%0Aparts.%20Experimental%20results%20demonstrate%20that%20PartCLIPSeg%20outperforms%20existing%0Astate-of-the-art%20OVPS%20methods%2C%20offering%20refined%20segmentation%20and%20an%20advanced%0Aunderstanding%20of%20part%20relationships%20in%20images.%20Through%20extensive%20experiments%2C%0Aour%20model%20demonstrated%20an%20improvement%20over%20the%20state-of-the-art%20models%20on%20the%0APascal-Part-116%2C%20ADE20K-Part-234%2C%20and%20PartImageNet%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11384v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Multi-Granularity%2520for%2520Open-Vocabulary%2520Part%2520Segmentation%26entry.906535625%3DJiho%2520Choi%2520and%2520Seonho%2520Lee%2520and%2520Seungho%2520Lee%2520and%2520Minhyun%2520Lee%2520and%2520Hyunjung%2520Shim%26entry.1292438233%3D%2520%2520Open-vocabulary%2520part%2520segmentation%2520%2528OVPS%2529%2520is%2520an%2520emerging%2520research%2520area%2520focused%250Aon%2520segmenting%2520fine-grained%2520entities%2520based%2520on%2520diverse%2520and%2520previously%2520unseen%250Avocabularies.%2520Our%2520study%2520highlights%2520the%2520inherent%2520complexities%2520of%2520part%250Asegmentation%2520due%2520to%2520intricate%2520boundaries%2520and%2520diverse%2520granularity%252C%2520reflecting%250Athe%2520knowledge-based%2520nature%2520of%2520part%2520identification.%2520To%2520address%2520these%2520challenges%252C%250Awe%2520propose%2520PartCLIPSeg%252C%2520a%2520novel%2520framework%2520utilizing%2520generalized%2520parts%2520and%250Aobject-level%2520contexts%2520to%2520mitigate%2520the%2520lack%2520of%2520generalization%2520in%2520fine-grained%250Aparts.%2520PartCLIPSeg%2520integrates%2520competitive%2520part%2520relationships%2520and%2520attention%250Acontrol%2520techniques%252C%2520alleviating%2520ambiguous%2520boundaries%2520and%2520underrepresented%250Aparts.%2520Experimental%2520results%2520demonstrate%2520that%2520PartCLIPSeg%2520outperforms%2520existing%250Astate-of-the-art%2520OVPS%2520methods%252C%2520offering%2520refined%2520segmentation%2520and%2520an%2520advanced%250Aunderstanding%2520of%2520part%2520relationships%2520in%2520images.%2520Through%2520extensive%2520experiments%252C%250Aour%2520model%2520demonstrated%2520an%2520improvement%2520over%2520the%2520state-of-the-art%2520models%2520on%2520the%250APascal-Part-116%252C%2520ADE20K-Part-234%252C%2520and%2520PartImageNet%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11384v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Multi-Granularity%20for%20Open-Vocabulary%20Part%20Segmentation&entry.906535625=Jiho%20Choi%20and%20Seonho%20Lee%20and%20Seungho%20Lee%20and%20Minhyun%20Lee%20and%20Hyunjung%20Shim&entry.1292438233=%20%20Open-vocabulary%20part%20segmentation%20%28OVPS%29%20is%20an%20emerging%20research%20area%20focused%0Aon%20segmenting%20fine-grained%20entities%20based%20on%20diverse%20and%20previously%20unseen%0Avocabularies.%20Our%20study%20highlights%20the%20inherent%20complexities%20of%20part%0Asegmentation%20due%20to%20intricate%20boundaries%20and%20diverse%20granularity%2C%20reflecting%0Athe%20knowledge-based%20nature%20of%20part%20identification.%20To%20address%20these%20challenges%2C%0Awe%20propose%20PartCLIPSeg%2C%20a%20novel%20framework%20utilizing%20generalized%20parts%20and%0Aobject-level%20contexts%20to%20mitigate%20the%20lack%20of%20generalization%20in%20fine-grained%0Aparts.%20PartCLIPSeg%20integrates%20competitive%20part%20relationships%20and%20attention%0Acontrol%20techniques%2C%20alleviating%20ambiguous%20boundaries%20and%20underrepresented%0Aparts.%20Experimental%20results%20demonstrate%20that%20PartCLIPSeg%20outperforms%20existing%0Astate-of-the-art%20OVPS%20methods%2C%20offering%20refined%20segmentation%20and%20an%20advanced%0Aunderstanding%20of%20part%20relationships%20in%20images.%20Through%20extensive%20experiments%2C%0Aour%20model%20demonstrated%20an%20improvement%20over%20the%20state-of-the-art%20models%20on%20the%0APascal-Part-116%2C%20ADE20K-Part-234%2C%20and%20PartImageNet%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11384v1&entry.124074799=Read"},
{"title": "MOWA: Multiple-in-One Image Warping Model", "author": "Kang Liao and Zongsheng Yue and Zhonghua Wu and Chen Change Loy", "abstract": "  While recent image warping approaches achieved remarkable success on existing\nbenchmarks, they still require training separate models for each specific task\nand cannot generalize well to different camera models or customized\nmanipulations. To address diverse types of warping in practice, we propose a\nMultiple-in-One image WArping model (named MOWA) in this work. Specifically, we\nmitigate the difficulty of multi-task learning by disentangling the motion\nestimation at both the region level and pixel level. To further enable dynamic\ntask-aware image warping, we introduce a lightweight point-based classifier\nthat predicts the task type, serving as prompts to modulate the feature maps\nfor more accurate estimation. To our knowledge, this is the first work that\nsolves multiple practical warping tasks in one single model. Extensive\nexperiments demonstrate that our MOWA, which is trained on six tasks for\nmultiple-in-one image warping, outperforms state-of-the-art task-specific\nmodels across most tasks. Moreover, MOWA also exhibits promising potential to\ngeneralize into unseen scenes, as evidenced by cross-domain and zero-shot\nevaluations. The code and more visual results can be found on the project page:\nhttps://kangliao929.github.io/projects/mowa/.\n", "link": "http://arxiv.org/abs/2404.10716v2", "date": "2024-06-17", "relevancy": 2.1205, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5449}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5261}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MOWA%3A%20Multiple-in-One%20Image%20Warping%20Model&body=Title%3A%20MOWA%3A%20Multiple-in-One%20Image%20Warping%20Model%0AAuthor%3A%20Kang%20Liao%20and%20Zongsheng%20Yue%20and%20Zhonghua%20Wu%20and%20Chen%20Change%20Loy%0AAbstract%3A%20%20%20While%20recent%20image%20warping%20approaches%20achieved%20remarkable%20success%20on%20existing%0Abenchmarks%2C%20they%20still%20require%20training%20separate%20models%20for%20each%20specific%20task%0Aand%20cannot%20generalize%20well%20to%20different%20camera%20models%20or%20customized%0Amanipulations.%20To%20address%20diverse%20types%20of%20warping%20in%20practice%2C%20we%20propose%20a%0AMultiple-in-One%20image%20WArping%20model%20%28named%20MOWA%29%20in%20this%20work.%20Specifically%2C%20we%0Amitigate%20the%20difficulty%20of%20multi-task%20learning%20by%20disentangling%20the%20motion%0Aestimation%20at%20both%20the%20region%20level%20and%20pixel%20level.%20To%20further%20enable%20dynamic%0Atask-aware%20image%20warping%2C%20we%20introduce%20a%20lightweight%20point-based%20classifier%0Athat%20predicts%20the%20task%20type%2C%20serving%20as%20prompts%20to%20modulate%20the%20feature%20maps%0Afor%20more%20accurate%20estimation.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20work%20that%0Asolves%20multiple%20practical%20warping%20tasks%20in%20one%20single%20model.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20MOWA%2C%20which%20is%20trained%20on%20six%20tasks%20for%0Amultiple-in-one%20image%20warping%2C%20outperforms%20state-of-the-art%20task-specific%0Amodels%20across%20most%20tasks.%20Moreover%2C%20MOWA%20also%20exhibits%20promising%20potential%20to%0Ageneralize%20into%20unseen%20scenes%2C%20as%20evidenced%20by%20cross-domain%20and%20zero-shot%0Aevaluations.%20The%20code%20and%20more%20visual%20results%20can%20be%20found%20on%20the%20project%20page%3A%0Ahttps%3A//kangliao929.github.io/projects/mowa/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10716v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMOWA%253A%2520Multiple-in-One%2520Image%2520Warping%2520Model%26entry.906535625%3DKang%2520Liao%2520and%2520Zongsheng%2520Yue%2520and%2520Zhonghua%2520Wu%2520and%2520Chen%2520Change%2520Loy%26entry.1292438233%3D%2520%2520While%2520recent%2520image%2520warping%2520approaches%2520achieved%2520remarkable%2520success%2520on%2520existing%250Abenchmarks%252C%2520they%2520still%2520require%2520training%2520separate%2520models%2520for%2520each%2520specific%2520task%250Aand%2520cannot%2520generalize%2520well%2520to%2520different%2520camera%2520models%2520or%2520customized%250Amanipulations.%2520To%2520address%2520diverse%2520types%2520of%2520warping%2520in%2520practice%252C%2520we%2520propose%2520a%250AMultiple-in-One%2520image%2520WArping%2520model%2520%2528named%2520MOWA%2529%2520in%2520this%2520work.%2520Specifically%252C%2520we%250Amitigate%2520the%2520difficulty%2520of%2520multi-task%2520learning%2520by%2520disentangling%2520the%2520motion%250Aestimation%2520at%2520both%2520the%2520region%2520level%2520and%2520pixel%2520level.%2520To%2520further%2520enable%2520dynamic%250Atask-aware%2520image%2520warping%252C%2520we%2520introduce%2520a%2520lightweight%2520point-based%2520classifier%250Athat%2520predicts%2520the%2520task%2520type%252C%2520serving%2520as%2520prompts%2520to%2520modulate%2520the%2520feature%2520maps%250Afor%2520more%2520accurate%2520estimation.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520work%2520that%250Asolves%2520multiple%2520practical%2520warping%2520tasks%2520in%2520one%2520single%2520model.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520MOWA%252C%2520which%2520is%2520trained%2520on%2520six%2520tasks%2520for%250Amultiple-in-one%2520image%2520warping%252C%2520outperforms%2520state-of-the-art%2520task-specific%250Amodels%2520across%2520most%2520tasks.%2520Moreover%252C%2520MOWA%2520also%2520exhibits%2520promising%2520potential%2520to%250Ageneralize%2520into%2520unseen%2520scenes%252C%2520as%2520evidenced%2520by%2520cross-domain%2520and%2520zero-shot%250Aevaluations.%2520The%2520code%2520and%2520more%2520visual%2520results%2520can%2520be%2520found%2520on%2520the%2520project%2520page%253A%250Ahttps%253A//kangliao929.github.io/projects/mowa/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.10716v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MOWA%3A%20Multiple-in-One%20Image%20Warping%20Model&entry.906535625=Kang%20Liao%20and%20Zongsheng%20Yue%20and%20Zhonghua%20Wu%20and%20Chen%20Change%20Loy&entry.1292438233=%20%20While%20recent%20image%20warping%20approaches%20achieved%20remarkable%20success%20on%20existing%0Abenchmarks%2C%20they%20still%20require%20training%20separate%20models%20for%20each%20specific%20task%0Aand%20cannot%20generalize%20well%20to%20different%20camera%20models%20or%20customized%0Amanipulations.%20To%20address%20diverse%20types%20of%20warping%20in%20practice%2C%20we%20propose%20a%0AMultiple-in-One%20image%20WArping%20model%20%28named%20MOWA%29%20in%20this%20work.%20Specifically%2C%20we%0Amitigate%20the%20difficulty%20of%20multi-task%20learning%20by%20disentangling%20the%20motion%0Aestimation%20at%20both%20the%20region%20level%20and%20pixel%20level.%20To%20further%20enable%20dynamic%0Atask-aware%20image%20warping%2C%20we%20introduce%20a%20lightweight%20point-based%20classifier%0Athat%20predicts%20the%20task%20type%2C%20serving%20as%20prompts%20to%20modulate%20the%20feature%20maps%0Afor%20more%20accurate%20estimation.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20work%20that%0Asolves%20multiple%20practical%20warping%20tasks%20in%20one%20single%20model.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20MOWA%2C%20which%20is%20trained%20on%20six%20tasks%20for%0Amultiple-in-one%20image%20warping%2C%20outperforms%20state-of-the-art%20task-specific%0Amodels%20across%20most%20tasks.%20Moreover%2C%20MOWA%20also%20exhibits%20promising%20potential%20to%0Ageneralize%20into%20unseen%20scenes%2C%20as%20evidenced%20by%20cross-domain%20and%20zero-shot%0Aevaluations.%20The%20code%20and%20more%20visual%20results%20can%20be%20found%20on%20the%20project%20page%3A%0Ahttps%3A//kangliao929.github.io/projects/mowa/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10716v2&entry.124074799=Read"},
{"title": "Input Conditioned Graph Generation for Language Agents", "author": "Lukas Vierling and Jie Fu and Kai Chen", "abstract": "  Recent progress in Large Language Models (LLMs) and language agents has\ndemonstrated significant promise for various future applications across\nmultiple disciplines. While traditional approaches to language agents often\nrely on fixed, handcrafted designs, our research aims to develop both learnable\nand dynamic agents. Our method uses an existing framework that abstracts\nlanguage agents as graphs. Within this graph framework, we aim to learn a model\nthat can generate edges for every given input to the language agent. This\nallows us to generate edges that represent the flow of communication within the\ngraph based on the given input, thereby adjusting the internal communication of\na language agent. We learn to generate these edges using a pretrained LLM that\nis fine-tuned with reinforcement learning. This LLM can be fine-tuned on\nseveral datasets simultaneously, and we hypothesize that the model learns to\nadapt to these different domains during training, achieving good overall\nperformance when encountering data from different domains during deployment. We\ndemonstrate that our approach surpasses the previous static approach by nearly\n6% accuracy on a combined dataset of MMLU and CMMLU, and by more than 10% when\ntrained with a sparsity-inducing loss. It also performs superior in additional\nexperiments conducted with the MMLU and Mini Crossword Puzzles datasets. The\ncode is available at https://github.com/lukasVierling/DynamicGPTSwarm.\n", "link": "http://arxiv.org/abs/2406.11555v1", "date": "2024-06-17", "relevancy": 2.1204, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5474}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.53}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Input%20Conditioned%20Graph%20Generation%20for%20Language%20Agents&body=Title%3A%20Input%20Conditioned%20Graph%20Generation%20for%20Language%20Agents%0AAuthor%3A%20Lukas%20Vierling%20and%20Jie%20Fu%20and%20Kai%20Chen%0AAbstract%3A%20%20%20Recent%20progress%20in%20Large%20Language%20Models%20%28LLMs%29%20and%20language%20agents%20has%0Ademonstrated%20significant%20promise%20for%20various%20future%20applications%20across%0Amultiple%20disciplines.%20While%20traditional%20approaches%20to%20language%20agents%20often%0Arely%20on%20fixed%2C%20handcrafted%20designs%2C%20our%20research%20aims%20to%20develop%20both%20learnable%0Aand%20dynamic%20agents.%20Our%20method%20uses%20an%20existing%20framework%20that%20abstracts%0Alanguage%20agents%20as%20graphs.%20Within%20this%20graph%20framework%2C%20we%20aim%20to%20learn%20a%20model%0Athat%20can%20generate%20edges%20for%20every%20given%20input%20to%20the%20language%20agent.%20This%0Aallows%20us%20to%20generate%20edges%20that%20represent%20the%20flow%20of%20communication%20within%20the%0Agraph%20based%20on%20the%20given%20input%2C%20thereby%20adjusting%20the%20internal%20communication%20of%0Aa%20language%20agent.%20We%20learn%20to%20generate%20these%20edges%20using%20a%20pretrained%20LLM%20that%0Ais%20fine-tuned%20with%20reinforcement%20learning.%20This%20LLM%20can%20be%20fine-tuned%20on%0Aseveral%20datasets%20simultaneously%2C%20and%20we%20hypothesize%20that%20the%20model%20learns%20to%0Aadapt%20to%20these%20different%20domains%20during%20training%2C%20achieving%20good%20overall%0Aperformance%20when%20encountering%20data%20from%20different%20domains%20during%20deployment.%20We%0Ademonstrate%20that%20our%20approach%20surpasses%20the%20previous%20static%20approach%20by%20nearly%0A6%25%20accuracy%20on%20a%20combined%20dataset%20of%20MMLU%20and%20CMMLU%2C%20and%20by%20more%20than%2010%25%20when%0Atrained%20with%20a%20sparsity-inducing%20loss.%20It%20also%20performs%20superior%20in%20additional%0Aexperiments%20conducted%20with%20the%20MMLU%20and%20Mini%20Crossword%20Puzzles%20datasets.%20The%0Acode%20is%20available%20at%20https%3A//github.com/lukasVierling/DynamicGPTSwarm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11555v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInput%2520Conditioned%2520Graph%2520Generation%2520for%2520Language%2520Agents%26entry.906535625%3DLukas%2520Vierling%2520and%2520Jie%2520Fu%2520and%2520Kai%2520Chen%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520language%2520agents%2520has%250Ademonstrated%2520significant%2520promise%2520for%2520various%2520future%2520applications%2520across%250Amultiple%2520disciplines.%2520While%2520traditional%2520approaches%2520to%2520language%2520agents%2520often%250Arely%2520on%2520fixed%252C%2520handcrafted%2520designs%252C%2520our%2520research%2520aims%2520to%2520develop%2520both%2520learnable%250Aand%2520dynamic%2520agents.%2520Our%2520method%2520uses%2520an%2520existing%2520framework%2520that%2520abstracts%250Alanguage%2520agents%2520as%2520graphs.%2520Within%2520this%2520graph%2520framework%252C%2520we%2520aim%2520to%2520learn%2520a%2520model%250Athat%2520can%2520generate%2520edges%2520for%2520every%2520given%2520input%2520to%2520the%2520language%2520agent.%2520This%250Aallows%2520us%2520to%2520generate%2520edges%2520that%2520represent%2520the%2520flow%2520of%2520communication%2520within%2520the%250Agraph%2520based%2520on%2520the%2520given%2520input%252C%2520thereby%2520adjusting%2520the%2520internal%2520communication%2520of%250Aa%2520language%2520agent.%2520We%2520learn%2520to%2520generate%2520these%2520edges%2520using%2520a%2520pretrained%2520LLM%2520that%250Ais%2520fine-tuned%2520with%2520reinforcement%2520learning.%2520This%2520LLM%2520can%2520be%2520fine-tuned%2520on%250Aseveral%2520datasets%2520simultaneously%252C%2520and%2520we%2520hypothesize%2520that%2520the%2520model%2520learns%2520to%250Aadapt%2520to%2520these%2520different%2520domains%2520during%2520training%252C%2520achieving%2520good%2520overall%250Aperformance%2520when%2520encountering%2520data%2520from%2520different%2520domains%2520during%2520deployment.%2520We%250Ademonstrate%2520that%2520our%2520approach%2520surpasses%2520the%2520previous%2520static%2520approach%2520by%2520nearly%250A6%2525%2520accuracy%2520on%2520a%2520combined%2520dataset%2520of%2520MMLU%2520and%2520CMMLU%252C%2520and%2520by%2520more%2520than%252010%2525%2520when%250Atrained%2520with%2520a%2520sparsity-inducing%2520loss.%2520It%2520also%2520performs%2520superior%2520in%2520additional%250Aexperiments%2520conducted%2520with%2520the%2520MMLU%2520and%2520Mini%2520Crossword%2520Puzzles%2520datasets.%2520The%250Acode%2520is%2520available%2520at%2520https%253A//github.com/lukasVierling/DynamicGPTSwarm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11555v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Input%20Conditioned%20Graph%20Generation%20for%20Language%20Agents&entry.906535625=Lukas%20Vierling%20and%20Jie%20Fu%20and%20Kai%20Chen&entry.1292438233=%20%20Recent%20progress%20in%20Large%20Language%20Models%20%28LLMs%29%20and%20language%20agents%20has%0Ademonstrated%20significant%20promise%20for%20various%20future%20applications%20across%0Amultiple%20disciplines.%20While%20traditional%20approaches%20to%20language%20agents%20often%0Arely%20on%20fixed%2C%20handcrafted%20designs%2C%20our%20research%20aims%20to%20develop%20both%20learnable%0Aand%20dynamic%20agents.%20Our%20method%20uses%20an%20existing%20framework%20that%20abstracts%0Alanguage%20agents%20as%20graphs.%20Within%20this%20graph%20framework%2C%20we%20aim%20to%20learn%20a%20model%0Athat%20can%20generate%20edges%20for%20every%20given%20input%20to%20the%20language%20agent.%20This%0Aallows%20us%20to%20generate%20edges%20that%20represent%20the%20flow%20of%20communication%20within%20the%0Agraph%20based%20on%20the%20given%20input%2C%20thereby%20adjusting%20the%20internal%20communication%20of%0Aa%20language%20agent.%20We%20learn%20to%20generate%20these%20edges%20using%20a%20pretrained%20LLM%20that%0Ais%20fine-tuned%20with%20reinforcement%20learning.%20This%20LLM%20can%20be%20fine-tuned%20on%0Aseveral%20datasets%20simultaneously%2C%20and%20we%20hypothesize%20that%20the%20model%20learns%20to%0Aadapt%20to%20these%20different%20domains%20during%20training%2C%20achieving%20good%20overall%0Aperformance%20when%20encountering%20data%20from%20different%20domains%20during%20deployment.%20We%0Ademonstrate%20that%20our%20approach%20surpasses%20the%20previous%20static%20approach%20by%20nearly%0A6%25%20accuracy%20on%20a%20combined%20dataset%20of%20MMLU%20and%20CMMLU%2C%20and%20by%20more%20than%2010%25%20when%0Atrained%20with%20a%20sparsity-inducing%20loss.%20It%20also%20performs%20superior%20in%20additional%0Aexperiments%20conducted%20with%20the%20MMLU%20and%20Mini%20Crossword%20Puzzles%20datasets.%20The%0Acode%20is%20available%20at%20https%3A//github.com/lukasVierling/DynamicGPTSwarm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11555v1&entry.124074799=Read"},
{"title": "Eyes Wide Unshut: Unsupervised Mistake Detection in Egocentric Video by\n  Detecting Unpredictable Gaze", "author": "Michele Mazzamuto and Antonino Furnari and Giovanni Maria Farinella", "abstract": "  In this paper, we address the challenge of unsupervised mistake detection in\negocentric video through the analysis of gaze signals, a critical component for\nadvancing user assistance in smart glasses. Traditional supervised methods,\nreliant on manually labeled mistakes, suffer from domain-dependence and\nscalability issues. This research introduces an unsupervised method for\ndetecting mistakes in videos of human activities, overcoming the challenges of\ndomain-specific requirements and the necessity for annotated data. By analyzing\nunusual gaze patterns that signal user disorientation during tasks, we propose\na gaze completion model that forecasts eye gaze trajectories from incomplete\ninputs. The difference between the anticipated and observed gaze paths acts as\nan indicator for identifying errors. Our method is validated on the EPIC-Tent\ndataset, showing its superiority compared to current one-class supervised and\nunsupervised techniques.\n", "link": "http://arxiv.org/abs/2406.08379v2", "date": "2024-06-17", "relevancy": 2.1195, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5367}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5341}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Eyes%20Wide%20Unshut%3A%20Unsupervised%20Mistake%20Detection%20in%20Egocentric%20Video%20by%0A%20%20Detecting%20Unpredictable%20Gaze&body=Title%3A%20Eyes%20Wide%20Unshut%3A%20Unsupervised%20Mistake%20Detection%20in%20Egocentric%20Video%20by%0A%20%20Detecting%20Unpredictable%20Gaze%0AAuthor%3A%20Michele%20Mazzamuto%20and%20Antonino%20Furnari%20and%20Giovanni%20Maria%20Farinella%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20address%20the%20challenge%20of%20unsupervised%20mistake%20detection%20in%0Aegocentric%20video%20through%20the%20analysis%20of%20gaze%20signals%2C%20a%20critical%20component%20for%0Aadvancing%20user%20assistance%20in%20smart%20glasses.%20Traditional%20supervised%20methods%2C%0Areliant%20on%20manually%20labeled%20mistakes%2C%20suffer%20from%20domain-dependence%20and%0Ascalability%20issues.%20This%20research%20introduces%20an%20unsupervised%20method%20for%0Adetecting%20mistakes%20in%20videos%20of%20human%20activities%2C%20overcoming%20the%20challenges%20of%0Adomain-specific%20requirements%20and%20the%20necessity%20for%20annotated%20data.%20By%20analyzing%0Aunusual%20gaze%20patterns%20that%20signal%20user%20disorientation%20during%20tasks%2C%20we%20propose%0Aa%20gaze%20completion%20model%20that%20forecasts%20eye%20gaze%20trajectories%20from%20incomplete%0Ainputs.%20The%20difference%20between%20the%20anticipated%20and%20observed%20gaze%20paths%20acts%20as%0Aan%20indicator%20for%20identifying%20errors.%20Our%20method%20is%20validated%20on%20the%20EPIC-Tent%0Adataset%2C%20showing%20its%20superiority%20compared%20to%20current%20one-class%20supervised%20and%0Aunsupervised%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.08379v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEyes%2520Wide%2520Unshut%253A%2520Unsupervised%2520Mistake%2520Detection%2520in%2520Egocentric%2520Video%2520by%250A%2520%2520Detecting%2520Unpredictable%2520Gaze%26entry.906535625%3DMichele%2520Mazzamuto%2520and%2520Antonino%2520Furnari%2520and%2520Giovanni%2520Maria%2520Farinella%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520address%2520the%2520challenge%2520of%2520unsupervised%2520mistake%2520detection%2520in%250Aegocentric%2520video%2520through%2520the%2520analysis%2520of%2520gaze%2520signals%252C%2520a%2520critical%2520component%2520for%250Aadvancing%2520user%2520assistance%2520in%2520smart%2520glasses.%2520Traditional%2520supervised%2520methods%252C%250Areliant%2520on%2520manually%2520labeled%2520mistakes%252C%2520suffer%2520from%2520domain-dependence%2520and%250Ascalability%2520issues.%2520This%2520research%2520introduces%2520an%2520unsupervised%2520method%2520for%250Adetecting%2520mistakes%2520in%2520videos%2520of%2520human%2520activities%252C%2520overcoming%2520the%2520challenges%2520of%250Adomain-specific%2520requirements%2520and%2520the%2520necessity%2520for%2520annotated%2520data.%2520By%2520analyzing%250Aunusual%2520gaze%2520patterns%2520that%2520signal%2520user%2520disorientation%2520during%2520tasks%252C%2520we%2520propose%250Aa%2520gaze%2520completion%2520model%2520that%2520forecasts%2520eye%2520gaze%2520trajectories%2520from%2520incomplete%250Ainputs.%2520The%2520difference%2520between%2520the%2520anticipated%2520and%2520observed%2520gaze%2520paths%2520acts%2520as%250Aan%2520indicator%2520for%2520identifying%2520errors.%2520Our%2520method%2520is%2520validated%2520on%2520the%2520EPIC-Tent%250Adataset%252C%2520showing%2520its%2520superiority%2520compared%2520to%2520current%2520one-class%2520supervised%2520and%250Aunsupervised%2520techniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.08379v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Eyes%20Wide%20Unshut%3A%20Unsupervised%20Mistake%20Detection%20in%20Egocentric%20Video%20by%0A%20%20Detecting%20Unpredictable%20Gaze&entry.906535625=Michele%20Mazzamuto%20and%20Antonino%20Furnari%20and%20Giovanni%20Maria%20Farinella&entry.1292438233=%20%20In%20this%20paper%2C%20we%20address%20the%20challenge%20of%20unsupervised%20mistake%20detection%20in%0Aegocentric%20video%20through%20the%20analysis%20of%20gaze%20signals%2C%20a%20critical%20component%20for%0Aadvancing%20user%20assistance%20in%20smart%20glasses.%20Traditional%20supervised%20methods%2C%0Areliant%20on%20manually%20labeled%20mistakes%2C%20suffer%20from%20domain-dependence%20and%0Ascalability%20issues.%20This%20research%20introduces%20an%20unsupervised%20method%20for%0Adetecting%20mistakes%20in%20videos%20of%20human%20activities%2C%20overcoming%20the%20challenges%20of%0Adomain-specific%20requirements%20and%20the%20necessity%20for%20annotated%20data.%20By%20analyzing%0Aunusual%20gaze%20patterns%20that%20signal%20user%20disorientation%20during%20tasks%2C%20we%20propose%0Aa%20gaze%20completion%20model%20that%20forecasts%20eye%20gaze%20trajectories%20from%20incomplete%0Ainputs.%20The%20difference%20between%20the%20anticipated%20and%20observed%20gaze%20paths%20acts%20as%0Aan%20indicator%20for%20identifying%20errors.%20Our%20method%20is%20validated%20on%20the%20EPIC-Tent%0Adataset%2C%20showing%20its%20superiority%20compared%20to%20current%20one-class%20supervised%20and%0Aunsupervised%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.08379v2&entry.124074799=Read"},
{"title": "VideoLLM-online: Online Video Large Language Model for Streaming Video", "author": "Joya Chen and Zhaoyang Lv and Shiwei Wu and Kevin Qinghong Lin and Chenan Song and Difei Gao and Jia-Wei Liu and Ziteng Gao and Dongxing Mao and Mike Zheng Shou", "abstract": "  Recent Large Language Models have been enhanced with vision capabilities,\nenabling them to comprehend images, videos, and interleaved vision-language\ncontent. However, the learning methods of these large multimodal models\ntypically treat videos as predetermined clips, making them less effective and\nefficient at handling streaming video inputs. In this paper, we propose a novel\nLearning-In-Video-Stream (LIVE) framework, which enables temporally aligned,\nlong-context, and real-time conversation within a continuous video stream. Our\nLIVE framework comprises comprehensive approaches to achieve video streaming\ndialogue, encompassing: (1) a training objective designed to perform language\nmodeling for continuous streaming inputs, (2) a data generation scheme that\nconverts offline temporal annotations into a streaming dialogue format, and (3)\nan optimized inference pipeline to speed up the model responses in real-world\nvideo streams. With our LIVE framework, we built VideoLLM-online model upon\nLlama-2/Llama-3 and demonstrate its significant advantages in processing\nstreaming videos. For instance, on average, our model can support streaming\ndialogue in a 5-minute video clip at over 10 FPS on an A100 GPU. Moreover, it\nalso showcases state-of-the-art performance on public offline video benchmarks,\nsuch as recognition, captioning, and forecasting. The code, model, data, and\ndemo have been made available at https://showlab.github.io/videollm-online.\n", "link": "http://arxiv.org/abs/2406.11816v1", "date": "2024-06-17", "relevancy": 2.1047, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5355}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.532}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5167}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoLLM-online%3A%20Online%20Video%20Large%20Language%20Model%20for%20Streaming%20Video&body=Title%3A%20VideoLLM-online%3A%20Online%20Video%20Large%20Language%20Model%20for%20Streaming%20Video%0AAuthor%3A%20Joya%20Chen%20and%20Zhaoyang%20Lv%20and%20Shiwei%20Wu%20and%20Kevin%20Qinghong%20Lin%20and%20Chenan%20Song%20and%20Difei%20Gao%20and%20Jia-Wei%20Liu%20and%20Ziteng%20Gao%20and%20Dongxing%20Mao%20and%20Mike%20Zheng%20Shou%0AAbstract%3A%20%20%20Recent%20Large%20Language%20Models%20have%20been%20enhanced%20with%20vision%20capabilities%2C%0Aenabling%20them%20to%20comprehend%20images%2C%20videos%2C%20and%20interleaved%20vision-language%0Acontent.%20However%2C%20the%20learning%20methods%20of%20these%20large%20multimodal%20models%0Atypically%20treat%20videos%20as%20predetermined%20clips%2C%20making%20them%20less%20effective%20and%0Aefficient%20at%20handling%20streaming%20video%20inputs.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0ALearning-In-Video-Stream%20%28LIVE%29%20framework%2C%20which%20enables%20temporally%20aligned%2C%0Along-context%2C%20and%20real-time%20conversation%20within%20a%20continuous%20video%20stream.%20Our%0ALIVE%20framework%20comprises%20comprehensive%20approaches%20to%20achieve%20video%20streaming%0Adialogue%2C%20encompassing%3A%20%281%29%20a%20training%20objective%20designed%20to%20perform%20language%0Amodeling%20for%20continuous%20streaming%20inputs%2C%20%282%29%20a%20data%20generation%20scheme%20that%0Aconverts%20offline%20temporal%20annotations%20into%20a%20streaming%20dialogue%20format%2C%20and%20%283%29%0Aan%20optimized%20inference%20pipeline%20to%20speed%20up%20the%20model%20responses%20in%20real-world%0Avideo%20streams.%20With%20our%20LIVE%20framework%2C%20we%20built%20VideoLLM-online%20model%20upon%0ALlama-2/Llama-3%20and%20demonstrate%20its%20significant%20advantages%20in%20processing%0Astreaming%20videos.%20For%20instance%2C%20on%20average%2C%20our%20model%20can%20support%20streaming%0Adialogue%20in%20a%205-minute%20video%20clip%20at%20over%2010%20FPS%20on%20an%20A100%20GPU.%20Moreover%2C%20it%0Aalso%20showcases%20state-of-the-art%20performance%20on%20public%20offline%20video%20benchmarks%2C%0Asuch%20as%20recognition%2C%20captioning%2C%20and%20forecasting.%20The%20code%2C%20model%2C%20data%2C%20and%0Ademo%20have%20been%20made%20available%20at%20https%3A//showlab.github.io/videollm-online.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11816v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoLLM-online%253A%2520Online%2520Video%2520Large%2520Language%2520Model%2520for%2520Streaming%2520Video%26entry.906535625%3DJoya%2520Chen%2520and%2520Zhaoyang%2520Lv%2520and%2520Shiwei%2520Wu%2520and%2520Kevin%2520Qinghong%2520Lin%2520and%2520Chenan%2520Song%2520and%2520Difei%2520Gao%2520and%2520Jia-Wei%2520Liu%2520and%2520Ziteng%2520Gao%2520and%2520Dongxing%2520Mao%2520and%2520Mike%2520Zheng%2520Shou%26entry.1292438233%3D%2520%2520Recent%2520Large%2520Language%2520Models%2520have%2520been%2520enhanced%2520with%2520vision%2520capabilities%252C%250Aenabling%2520them%2520to%2520comprehend%2520images%252C%2520videos%252C%2520and%2520interleaved%2520vision-language%250Acontent.%2520However%252C%2520the%2520learning%2520methods%2520of%2520these%2520large%2520multimodal%2520models%250Atypically%2520treat%2520videos%2520as%2520predetermined%2520clips%252C%2520making%2520them%2520less%2520effective%2520and%250Aefficient%2520at%2520handling%2520streaming%2520video%2520inputs.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%250ALearning-In-Video-Stream%2520%2528LIVE%2529%2520framework%252C%2520which%2520enables%2520temporally%2520aligned%252C%250Along-context%252C%2520and%2520real-time%2520conversation%2520within%2520a%2520continuous%2520video%2520stream.%2520Our%250ALIVE%2520framework%2520comprises%2520comprehensive%2520approaches%2520to%2520achieve%2520video%2520streaming%250Adialogue%252C%2520encompassing%253A%2520%25281%2529%2520a%2520training%2520objective%2520designed%2520to%2520perform%2520language%250Amodeling%2520for%2520continuous%2520streaming%2520inputs%252C%2520%25282%2529%2520a%2520data%2520generation%2520scheme%2520that%250Aconverts%2520offline%2520temporal%2520annotations%2520into%2520a%2520streaming%2520dialogue%2520format%252C%2520and%2520%25283%2529%250Aan%2520optimized%2520inference%2520pipeline%2520to%2520speed%2520up%2520the%2520model%2520responses%2520in%2520real-world%250Avideo%2520streams.%2520With%2520our%2520LIVE%2520framework%252C%2520we%2520built%2520VideoLLM-online%2520model%2520upon%250ALlama-2/Llama-3%2520and%2520demonstrate%2520its%2520significant%2520advantages%2520in%2520processing%250Astreaming%2520videos.%2520For%2520instance%252C%2520on%2520average%252C%2520our%2520model%2520can%2520support%2520streaming%250Adialogue%2520in%2520a%25205-minute%2520video%2520clip%2520at%2520over%252010%2520FPS%2520on%2520an%2520A100%2520GPU.%2520Moreover%252C%2520it%250Aalso%2520showcases%2520state-of-the-art%2520performance%2520on%2520public%2520offline%2520video%2520benchmarks%252C%250Asuch%2520as%2520recognition%252C%2520captioning%252C%2520and%2520forecasting.%2520The%2520code%252C%2520model%252C%2520data%252C%2520and%250Ademo%2520have%2520been%2520made%2520available%2520at%2520https%253A//showlab.github.io/videollm-online.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11816v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoLLM-online%3A%20Online%20Video%20Large%20Language%20Model%20for%20Streaming%20Video&entry.906535625=Joya%20Chen%20and%20Zhaoyang%20Lv%20and%20Shiwei%20Wu%20and%20Kevin%20Qinghong%20Lin%20and%20Chenan%20Song%20and%20Difei%20Gao%20and%20Jia-Wei%20Liu%20and%20Ziteng%20Gao%20and%20Dongxing%20Mao%20and%20Mike%20Zheng%20Shou&entry.1292438233=%20%20Recent%20Large%20Language%20Models%20have%20been%20enhanced%20with%20vision%20capabilities%2C%0Aenabling%20them%20to%20comprehend%20images%2C%20videos%2C%20and%20interleaved%20vision-language%0Acontent.%20However%2C%20the%20learning%20methods%20of%20these%20large%20multimodal%20models%0Atypically%20treat%20videos%20as%20predetermined%20clips%2C%20making%20them%20less%20effective%20and%0Aefficient%20at%20handling%20streaming%20video%20inputs.%20In%20this%20paper%2C%20we%20propose%20a%20novel%0ALearning-In-Video-Stream%20%28LIVE%29%20framework%2C%20which%20enables%20temporally%20aligned%2C%0Along-context%2C%20and%20real-time%20conversation%20within%20a%20continuous%20video%20stream.%20Our%0ALIVE%20framework%20comprises%20comprehensive%20approaches%20to%20achieve%20video%20streaming%0Adialogue%2C%20encompassing%3A%20%281%29%20a%20training%20objective%20designed%20to%20perform%20language%0Amodeling%20for%20continuous%20streaming%20inputs%2C%20%282%29%20a%20data%20generation%20scheme%20that%0Aconverts%20offline%20temporal%20annotations%20into%20a%20streaming%20dialogue%20format%2C%20and%20%283%29%0Aan%20optimized%20inference%20pipeline%20to%20speed%20up%20the%20model%20responses%20in%20real-world%0Avideo%20streams.%20With%20our%20LIVE%20framework%2C%20we%20built%20VideoLLM-online%20model%20upon%0ALlama-2/Llama-3%20and%20demonstrate%20its%20significant%20advantages%20in%20processing%0Astreaming%20videos.%20For%20instance%2C%20on%20average%2C%20our%20model%20can%20support%20streaming%0Adialogue%20in%20a%205-minute%20video%20clip%20at%20over%2010%20FPS%20on%20an%20A100%20GPU.%20Moreover%2C%20it%0Aalso%20showcases%20state-of-the-art%20performance%20on%20public%20offline%20video%20benchmarks%2C%0Asuch%20as%20recognition%2C%20captioning%2C%20and%20forecasting.%20The%20code%2C%20model%2C%20data%2C%20and%0Ademo%20have%20been%20made%20available%20at%20https%3A//showlab.github.io/videollm-online.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11816v1&entry.124074799=Read"},
{"title": "PhyBench: A Physical Commonsense Benchmark for Evaluating Text-to-Image\n  Models", "author": "Fanqing Meng and Wenqi Shao and Lixin Luo and Yahong Wang and Yiran Chen and Quanfeng Lu and Yue Yang and Tianshuo Yang and Kaipeng Zhang and Yu Qiao and Ping Luo", "abstract": "  Text-to-image (T2I) models have made substantial progress in generating\nimages from textual prompts. However, they frequently fail to produce images\nconsistent with physical commonsense, a vital capability for applications in\nworld simulation and everyday tasks. Current T2I evaluation benchmarks focus on\nmetrics such as accuracy, bias, and safety, neglecting the evaluation of\nmodels' internal knowledge, particularly physical commonsense. To address this\nissue, we introduce PhyBench, a comprehensive T2I evaluation dataset comprising\n700 prompts across 4 primary categories: mechanics, optics, thermodynamics, and\nmaterial properties, encompassing 31 distinct physical scenarios. We assess 6\nprominent T2I models, including proprietary models DALLE3 and Gemini, and\ndemonstrate that incorporating physical principles into prompts enhances the\nmodels' ability to generate physically accurate images. Our findings reveal\nthat: (1) even advanced models frequently err in various physical scenarios,\nexcept for optics; (2) GPT-4o, with item-specific scoring instructions,\neffectively evaluates the models' understanding of physical commonsense,\nclosely aligning with human assessments; and (3) current T2I models are\nprimarily focused on text-to-image translation, lacking profound reasoning\nregarding physical commonsense. We advocate for increased attention to the\ninherent knowledge within T2I models, beyond their utility as mere image\ngeneration tools. The code and data are available at\nhttps://github.com/OpenGVLab/PhyBench.\n", "link": "http://arxiv.org/abs/2406.11802v1", "date": "2024-06-17", "relevancy": 2.0979, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.528}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5244}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhyBench%3A%20A%20Physical%20Commonsense%20Benchmark%20for%20Evaluating%20Text-to-Image%0A%20%20Models&body=Title%3A%20PhyBench%3A%20A%20Physical%20Commonsense%20Benchmark%20for%20Evaluating%20Text-to-Image%0A%20%20Models%0AAuthor%3A%20Fanqing%20Meng%20and%20Wenqi%20Shao%20and%20Lixin%20Luo%20and%20Yahong%20Wang%20and%20Yiran%20Chen%20and%20Quanfeng%20Lu%20and%20Yue%20Yang%20and%20Tianshuo%20Yang%20and%20Kaipeng%20Zhang%20and%20Yu%20Qiao%20and%20Ping%20Luo%0AAbstract%3A%20%20%20Text-to-image%20%28T2I%29%20models%20have%20made%20substantial%20progress%20in%20generating%0Aimages%20from%20textual%20prompts.%20However%2C%20they%20frequently%20fail%20to%20produce%20images%0Aconsistent%20with%20physical%20commonsense%2C%20a%20vital%20capability%20for%20applications%20in%0Aworld%20simulation%20and%20everyday%20tasks.%20Current%20T2I%20evaluation%20benchmarks%20focus%20on%0Ametrics%20such%20as%20accuracy%2C%20bias%2C%20and%20safety%2C%20neglecting%20the%20evaluation%20of%0Amodels%27%20internal%20knowledge%2C%20particularly%20physical%20commonsense.%20To%20address%20this%0Aissue%2C%20we%20introduce%20PhyBench%2C%20a%20comprehensive%20T2I%20evaluation%20dataset%20comprising%0A700%20prompts%20across%204%20primary%20categories%3A%20mechanics%2C%20optics%2C%20thermodynamics%2C%20and%0Amaterial%20properties%2C%20encompassing%2031%20distinct%20physical%20scenarios.%20We%20assess%206%0Aprominent%20T2I%20models%2C%20including%20proprietary%20models%20DALLE3%20and%20Gemini%2C%20and%0Ademonstrate%20that%20incorporating%20physical%20principles%20into%20prompts%20enhances%20the%0Amodels%27%20ability%20to%20generate%20physically%20accurate%20images.%20Our%20findings%20reveal%0Athat%3A%20%281%29%20even%20advanced%20models%20frequently%20err%20in%20various%20physical%20scenarios%2C%0Aexcept%20for%20optics%3B%20%282%29%20GPT-4o%2C%20with%20item-specific%20scoring%20instructions%2C%0Aeffectively%20evaluates%20the%20models%27%20understanding%20of%20physical%20commonsense%2C%0Aclosely%20aligning%20with%20human%20assessments%3B%20and%20%283%29%20current%20T2I%20models%20are%0Aprimarily%20focused%20on%20text-to-image%20translation%2C%20lacking%20profound%20reasoning%0Aregarding%20physical%20commonsense.%20We%20advocate%20for%20increased%20attention%20to%20the%0Ainherent%20knowledge%20within%20T2I%20models%2C%20beyond%20their%20utility%20as%20mere%20image%0Ageneration%20tools.%20The%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/OpenGVLab/PhyBench.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhyBench%253A%2520A%2520Physical%2520Commonsense%2520Benchmark%2520for%2520Evaluating%2520Text-to-Image%250A%2520%2520Models%26entry.906535625%3DFanqing%2520Meng%2520and%2520Wenqi%2520Shao%2520and%2520Lixin%2520Luo%2520and%2520Yahong%2520Wang%2520and%2520Yiran%2520Chen%2520and%2520Quanfeng%2520Lu%2520and%2520Yue%2520Yang%2520and%2520Tianshuo%2520Yang%2520and%2520Kaipeng%2520Zhang%2520and%2520Yu%2520Qiao%2520and%2520Ping%2520Luo%26entry.1292438233%3D%2520%2520Text-to-image%2520%2528T2I%2529%2520models%2520have%2520made%2520substantial%2520progress%2520in%2520generating%250Aimages%2520from%2520textual%2520prompts.%2520However%252C%2520they%2520frequently%2520fail%2520to%2520produce%2520images%250Aconsistent%2520with%2520physical%2520commonsense%252C%2520a%2520vital%2520capability%2520for%2520applications%2520in%250Aworld%2520simulation%2520and%2520everyday%2520tasks.%2520Current%2520T2I%2520evaluation%2520benchmarks%2520focus%2520on%250Ametrics%2520such%2520as%2520accuracy%252C%2520bias%252C%2520and%2520safety%252C%2520neglecting%2520the%2520evaluation%2520of%250Amodels%2527%2520internal%2520knowledge%252C%2520particularly%2520physical%2520commonsense.%2520To%2520address%2520this%250Aissue%252C%2520we%2520introduce%2520PhyBench%252C%2520a%2520comprehensive%2520T2I%2520evaluation%2520dataset%2520comprising%250A700%2520prompts%2520across%25204%2520primary%2520categories%253A%2520mechanics%252C%2520optics%252C%2520thermodynamics%252C%2520and%250Amaterial%2520properties%252C%2520encompassing%252031%2520distinct%2520physical%2520scenarios.%2520We%2520assess%25206%250Aprominent%2520T2I%2520models%252C%2520including%2520proprietary%2520models%2520DALLE3%2520and%2520Gemini%252C%2520and%250Ademonstrate%2520that%2520incorporating%2520physical%2520principles%2520into%2520prompts%2520enhances%2520the%250Amodels%2527%2520ability%2520to%2520generate%2520physically%2520accurate%2520images.%2520Our%2520findings%2520reveal%250Athat%253A%2520%25281%2529%2520even%2520advanced%2520models%2520frequently%2520err%2520in%2520various%2520physical%2520scenarios%252C%250Aexcept%2520for%2520optics%253B%2520%25282%2529%2520GPT-4o%252C%2520with%2520item-specific%2520scoring%2520instructions%252C%250Aeffectively%2520evaluates%2520the%2520models%2527%2520understanding%2520of%2520physical%2520commonsense%252C%250Aclosely%2520aligning%2520with%2520human%2520assessments%253B%2520and%2520%25283%2529%2520current%2520T2I%2520models%2520are%250Aprimarily%2520focused%2520on%2520text-to-image%2520translation%252C%2520lacking%2520profound%2520reasoning%250Aregarding%2520physical%2520commonsense.%2520We%2520advocate%2520for%2520increased%2520attention%2520to%2520the%250Ainherent%2520knowledge%2520within%2520T2I%2520models%252C%2520beyond%2520their%2520utility%2520as%2520mere%2520image%250Ageneration%2520tools.%2520The%2520code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/OpenGVLab/PhyBench.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhyBench%3A%20A%20Physical%20Commonsense%20Benchmark%20for%20Evaluating%20Text-to-Image%0A%20%20Models&entry.906535625=Fanqing%20Meng%20and%20Wenqi%20Shao%20and%20Lixin%20Luo%20and%20Yahong%20Wang%20and%20Yiran%20Chen%20and%20Quanfeng%20Lu%20and%20Yue%20Yang%20and%20Tianshuo%20Yang%20and%20Kaipeng%20Zhang%20and%20Yu%20Qiao%20and%20Ping%20Luo&entry.1292438233=%20%20Text-to-image%20%28T2I%29%20models%20have%20made%20substantial%20progress%20in%20generating%0Aimages%20from%20textual%20prompts.%20However%2C%20they%20frequently%20fail%20to%20produce%20images%0Aconsistent%20with%20physical%20commonsense%2C%20a%20vital%20capability%20for%20applications%20in%0Aworld%20simulation%20and%20everyday%20tasks.%20Current%20T2I%20evaluation%20benchmarks%20focus%20on%0Ametrics%20such%20as%20accuracy%2C%20bias%2C%20and%20safety%2C%20neglecting%20the%20evaluation%20of%0Amodels%27%20internal%20knowledge%2C%20particularly%20physical%20commonsense.%20To%20address%20this%0Aissue%2C%20we%20introduce%20PhyBench%2C%20a%20comprehensive%20T2I%20evaluation%20dataset%20comprising%0A700%20prompts%20across%204%20primary%20categories%3A%20mechanics%2C%20optics%2C%20thermodynamics%2C%20and%0Amaterial%20properties%2C%20encompassing%2031%20distinct%20physical%20scenarios.%20We%20assess%206%0Aprominent%20T2I%20models%2C%20including%20proprietary%20models%20DALLE3%20and%20Gemini%2C%20and%0Ademonstrate%20that%20incorporating%20physical%20principles%20into%20prompts%20enhances%20the%0Amodels%27%20ability%20to%20generate%20physically%20accurate%20images.%20Our%20findings%20reveal%0Athat%3A%20%281%29%20even%20advanced%20models%20frequently%20err%20in%20various%20physical%20scenarios%2C%0Aexcept%20for%20optics%3B%20%282%29%20GPT-4o%2C%20with%20item-specific%20scoring%20instructions%2C%0Aeffectively%20evaluates%20the%20models%27%20understanding%20of%20physical%20commonsense%2C%0Aclosely%20aligning%20with%20human%20assessments%3B%20and%20%283%29%20current%20T2I%20models%20are%0Aprimarily%20focused%20on%20text-to-image%20translation%2C%20lacking%20profound%20reasoning%0Aregarding%20physical%20commonsense.%20We%20advocate%20for%20increased%20attention%20to%20the%0Ainherent%20knowledge%20within%20T2I%20models%2C%20beyond%20their%20utility%20as%20mere%20image%0Ageneration%20tools.%20The%20code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/OpenGVLab/PhyBench.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11802v1&entry.124074799=Read"},
{"title": "LLaNA: Large Language and NeRF Assistant", "author": "Andrea Amaduzzi and Pierluigi Zama Ramirez and Giuseppe Lisanti and Samuele Salti and Luigi Di Stefano", "abstract": "  Multimodal Large Language Models (MLLMs) have demonstrated an excellent\nunderstanding of images and 3D data. However, both modalities have shortcomings\nin holistically capturing the appearance and geometry of objects. Meanwhile,\nNeural Radiance Fields (NeRFs), which encode information within the weights of\na simple Multi-Layer Perceptron (MLP), have emerged as an increasingly\nwidespread modality that simultaneously encodes the geometry and photorealistic\nappearance of objects. This paper investigates the feasibility and\neffectiveness of ingesting NeRF into MLLM. We create LLaNA, the first\ngeneral-purpose NeRF-language assistant capable of performing new tasks such as\nNeRF captioning and Q\\&A. Notably, our method directly processes the weights of\nthe NeRF's MLP to extract information about the represented objects without the\nneed to render images or materialize 3D data structures. Moreover, we build a\ndataset of NeRFs with text annotations for various NeRF-language tasks with no\nhuman intervention. Based on this dataset, we develop a benchmark to evaluate\nthe NeRF understanding capability of our method. Results show that processing\nNeRF weights performs favourably against extracting 2D or 3D representations\nfrom NeRFs.\n", "link": "http://arxiv.org/abs/2406.11840v1", "date": "2024-06-17", "relevancy": 2.0947, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5274}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5253}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLaNA%3A%20Large%20Language%20and%20NeRF%20Assistant&body=Title%3A%20LLaNA%3A%20Large%20Language%20and%20NeRF%20Assistant%0AAuthor%3A%20Andrea%20Amaduzzi%20and%20Pierluigi%20Zama%20Ramirez%20and%20Giuseppe%20Lisanti%20and%20Samuele%20Salti%20and%20Luigi%20Di%20Stefano%0AAbstract%3A%20%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20an%20excellent%0Aunderstanding%20of%20images%20and%203D%20data.%20However%2C%20both%20modalities%20have%20shortcomings%0Ain%20holistically%20capturing%20the%20appearance%20and%20geometry%20of%20objects.%20Meanwhile%2C%0ANeural%20Radiance%20Fields%20%28NeRFs%29%2C%20which%20encode%20information%20within%20the%20weights%20of%0Aa%20simple%20Multi-Layer%20Perceptron%20%28MLP%29%2C%20have%20emerged%20as%20an%20increasingly%0Awidespread%20modality%20that%20simultaneously%20encodes%20the%20geometry%20and%20photorealistic%0Aappearance%20of%20objects.%20This%20paper%20investigates%20the%20feasibility%20and%0Aeffectiveness%20of%20ingesting%20NeRF%20into%20MLLM.%20We%20create%20LLaNA%2C%20the%20first%0Ageneral-purpose%20NeRF-language%20assistant%20capable%20of%20performing%20new%20tasks%20such%20as%0ANeRF%20captioning%20and%20Q%5C%26A.%20Notably%2C%20our%20method%20directly%20processes%20the%20weights%20of%0Athe%20NeRF%27s%20MLP%20to%20extract%20information%20about%20the%20represented%20objects%20without%20the%0Aneed%20to%20render%20images%20or%20materialize%203D%20data%20structures.%20Moreover%2C%20we%20build%20a%0Adataset%20of%20NeRFs%20with%20text%20annotations%20for%20various%20NeRF-language%20tasks%20with%20no%0Ahuman%20intervention.%20Based%20on%20this%20dataset%2C%20we%20develop%20a%20benchmark%20to%20evaluate%0Athe%20NeRF%20understanding%20capability%20of%20our%20method.%20Results%20show%20that%20processing%0ANeRF%20weights%20performs%20favourably%20against%20extracting%202D%20or%203D%20representations%0Afrom%20NeRFs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11840v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLaNA%253A%2520Large%2520Language%2520and%2520NeRF%2520Assistant%26entry.906535625%3DAndrea%2520Amaduzzi%2520and%2520Pierluigi%2520Zama%2520Ramirez%2520and%2520Giuseppe%2520Lisanti%2520and%2520Samuele%2520Salti%2520and%2520Luigi%2520Di%2520Stefano%26entry.1292438233%3D%2520%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520demonstrated%2520an%2520excellent%250Aunderstanding%2520of%2520images%2520and%25203D%2520data.%2520However%252C%2520both%2520modalities%2520have%2520shortcomings%250Ain%2520holistically%2520capturing%2520the%2520appearance%2520and%2520geometry%2520of%2520objects.%2520Meanwhile%252C%250ANeural%2520Radiance%2520Fields%2520%2528NeRFs%2529%252C%2520which%2520encode%2520information%2520within%2520the%2520weights%2520of%250Aa%2520simple%2520Multi-Layer%2520Perceptron%2520%2528MLP%2529%252C%2520have%2520emerged%2520as%2520an%2520increasingly%250Awidespread%2520modality%2520that%2520simultaneously%2520encodes%2520the%2520geometry%2520and%2520photorealistic%250Aappearance%2520of%2520objects.%2520This%2520paper%2520investigates%2520the%2520feasibility%2520and%250Aeffectiveness%2520of%2520ingesting%2520NeRF%2520into%2520MLLM.%2520We%2520create%2520LLaNA%252C%2520the%2520first%250Ageneral-purpose%2520NeRF-language%2520assistant%2520capable%2520of%2520performing%2520new%2520tasks%2520such%2520as%250ANeRF%2520captioning%2520and%2520Q%255C%2526A.%2520Notably%252C%2520our%2520method%2520directly%2520processes%2520the%2520weights%2520of%250Athe%2520NeRF%2527s%2520MLP%2520to%2520extract%2520information%2520about%2520the%2520represented%2520objects%2520without%2520the%250Aneed%2520to%2520render%2520images%2520or%2520materialize%25203D%2520data%2520structures.%2520Moreover%252C%2520we%2520build%2520a%250Adataset%2520of%2520NeRFs%2520with%2520text%2520annotations%2520for%2520various%2520NeRF-language%2520tasks%2520with%2520no%250Ahuman%2520intervention.%2520Based%2520on%2520this%2520dataset%252C%2520we%2520develop%2520a%2520benchmark%2520to%2520evaluate%250Athe%2520NeRF%2520understanding%2520capability%2520of%2520our%2520method.%2520Results%2520show%2520that%2520processing%250ANeRF%2520weights%2520performs%2520favourably%2520against%2520extracting%25202D%2520or%25203D%2520representations%250Afrom%2520NeRFs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11840v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLaNA%3A%20Large%20Language%20and%20NeRF%20Assistant&entry.906535625=Andrea%20Amaduzzi%20and%20Pierluigi%20Zama%20Ramirez%20and%20Giuseppe%20Lisanti%20and%20Samuele%20Salti%20and%20Luigi%20Di%20Stefano&entry.1292438233=%20%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20demonstrated%20an%20excellent%0Aunderstanding%20of%20images%20and%203D%20data.%20However%2C%20both%20modalities%20have%20shortcomings%0Ain%20holistically%20capturing%20the%20appearance%20and%20geometry%20of%20objects.%20Meanwhile%2C%0ANeural%20Radiance%20Fields%20%28NeRFs%29%2C%20which%20encode%20information%20within%20the%20weights%20of%0Aa%20simple%20Multi-Layer%20Perceptron%20%28MLP%29%2C%20have%20emerged%20as%20an%20increasingly%0Awidespread%20modality%20that%20simultaneously%20encodes%20the%20geometry%20and%20photorealistic%0Aappearance%20of%20objects.%20This%20paper%20investigates%20the%20feasibility%20and%0Aeffectiveness%20of%20ingesting%20NeRF%20into%20MLLM.%20We%20create%20LLaNA%2C%20the%20first%0Ageneral-purpose%20NeRF-language%20assistant%20capable%20of%20performing%20new%20tasks%20such%20as%0ANeRF%20captioning%20and%20Q%5C%26A.%20Notably%2C%20our%20method%20directly%20processes%20the%20weights%20of%0Athe%20NeRF%27s%20MLP%20to%20extract%20information%20about%20the%20represented%20objects%20without%20the%0Aneed%20to%20render%20images%20or%20materialize%203D%20data%20structures.%20Moreover%2C%20we%20build%20a%0Adataset%20of%20NeRFs%20with%20text%20annotations%20for%20various%20NeRF-language%20tasks%20with%20no%0Ahuman%20intervention.%20Based%20on%20this%20dataset%2C%20we%20develop%20a%20benchmark%20to%20evaluate%0Athe%20NeRF%20understanding%20capability%20of%20our%20method.%20Results%20show%20that%20processing%0ANeRF%20weights%20performs%20favourably%20against%20extracting%202D%20or%203D%20representations%0Afrom%20NeRFs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11840v1&entry.124074799=Read"},
{"title": "Learning Hierarchical Semantic Classification by Grounding on Consistent\n  Image Segmentations", "author": "Seulki Park and Youren Zhang and Stella X. Yu and Sara Beery and Jonathan Huang", "abstract": "  Hierarchical semantic classification requires the prediction of a taxonomy\ntree instead of a single flat level of the tree, where both accuracies at\nindividual levels and consistency across levels matter. We can train\nclassifiers for individual levels, which has accuracy but not consistency, or\nwe can train only the finest level classification and infer higher levels,\nwhich has consistency but not accuracy. Our key insight is that hierarchical\nrecognition should not be treated as multi-task classification, as each level\nis essentially a different task and they would have to compromise with each\nother, but be grounded on image segmentations that are consistent across\nsemantic granularities. Consistency can in fact improve accuracy. We build upon\nrecent work on learning hierarchical segmentation for flat-level recognition,\nand extend it to hierarchical recognition. It naturally captures the intuition\nthat fine-grained recognition requires fine image segmentation whereas\ncoarse-grained recognition requires coarse segmentation; they can all be\nintegrated into one recognition model that drives fine-to-coarse internal\nvisual parsing.Additionally, we introduce a Tree-path KL Divergence loss to\nenforce consistent accurate predictions across levels. Our extensive\nexperimentation and analysis demonstrate our significant gains on predicting an\naccurate and consistent taxonomy tree.\n", "link": "http://arxiv.org/abs/2406.11608v1", "date": "2024-06-17", "relevancy": 2.0843, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5286}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5207}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5184}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Hierarchical%20Semantic%20Classification%20by%20Grounding%20on%20Consistent%0A%20%20Image%20Segmentations&body=Title%3A%20Learning%20Hierarchical%20Semantic%20Classification%20by%20Grounding%20on%20Consistent%0A%20%20Image%20Segmentations%0AAuthor%3A%20Seulki%20Park%20and%20Youren%20Zhang%20and%20Stella%20X.%20Yu%20and%20Sara%20Beery%20and%20Jonathan%20Huang%0AAbstract%3A%20%20%20Hierarchical%20semantic%20classification%20requires%20the%20prediction%20of%20a%20taxonomy%0Atree%20instead%20of%20a%20single%20flat%20level%20of%20the%20tree%2C%20where%20both%20accuracies%20at%0Aindividual%20levels%20and%20consistency%20across%20levels%20matter.%20We%20can%20train%0Aclassifiers%20for%20individual%20levels%2C%20which%20has%20accuracy%20but%20not%20consistency%2C%20or%0Awe%20can%20train%20only%20the%20finest%20level%20classification%20and%20infer%20higher%20levels%2C%0Awhich%20has%20consistency%20but%20not%20accuracy.%20Our%20key%20insight%20is%20that%20hierarchical%0Arecognition%20should%20not%20be%20treated%20as%20multi-task%20classification%2C%20as%20each%20level%0Ais%20essentially%20a%20different%20task%20and%20they%20would%20have%20to%20compromise%20with%20each%0Aother%2C%20but%20be%20grounded%20on%20image%20segmentations%20that%20are%20consistent%20across%0Asemantic%20granularities.%20Consistency%20can%20in%20fact%20improve%20accuracy.%20We%20build%20upon%0Arecent%20work%20on%20learning%20hierarchical%20segmentation%20for%20flat-level%20recognition%2C%0Aand%20extend%20it%20to%20hierarchical%20recognition.%20It%20naturally%20captures%20the%20intuition%0Athat%20fine-grained%20recognition%20requires%20fine%20image%20segmentation%20whereas%0Acoarse-grained%20recognition%20requires%20coarse%20segmentation%3B%20they%20can%20all%20be%0Aintegrated%20into%20one%20recognition%20model%20that%20drives%20fine-to-coarse%20internal%0Avisual%20parsing.Additionally%2C%20we%20introduce%20a%20Tree-path%20KL%20Divergence%20loss%20to%0Aenforce%20consistent%20accurate%20predictions%20across%20levels.%20Our%20extensive%0Aexperimentation%20and%20analysis%20demonstrate%20our%20significant%20gains%20on%20predicting%20an%0Aaccurate%20and%20consistent%20taxonomy%20tree.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11608v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Hierarchical%2520Semantic%2520Classification%2520by%2520Grounding%2520on%2520Consistent%250A%2520%2520Image%2520Segmentations%26entry.906535625%3DSeulki%2520Park%2520and%2520Youren%2520Zhang%2520and%2520Stella%2520X.%2520Yu%2520and%2520Sara%2520Beery%2520and%2520Jonathan%2520Huang%26entry.1292438233%3D%2520%2520Hierarchical%2520semantic%2520classification%2520requires%2520the%2520prediction%2520of%2520a%2520taxonomy%250Atree%2520instead%2520of%2520a%2520single%2520flat%2520level%2520of%2520the%2520tree%252C%2520where%2520both%2520accuracies%2520at%250Aindividual%2520levels%2520and%2520consistency%2520across%2520levels%2520matter.%2520We%2520can%2520train%250Aclassifiers%2520for%2520individual%2520levels%252C%2520which%2520has%2520accuracy%2520but%2520not%2520consistency%252C%2520or%250Awe%2520can%2520train%2520only%2520the%2520finest%2520level%2520classification%2520and%2520infer%2520higher%2520levels%252C%250Awhich%2520has%2520consistency%2520but%2520not%2520accuracy.%2520Our%2520key%2520insight%2520is%2520that%2520hierarchical%250Arecognition%2520should%2520not%2520be%2520treated%2520as%2520multi-task%2520classification%252C%2520as%2520each%2520level%250Ais%2520essentially%2520a%2520different%2520task%2520and%2520they%2520would%2520have%2520to%2520compromise%2520with%2520each%250Aother%252C%2520but%2520be%2520grounded%2520on%2520image%2520segmentations%2520that%2520are%2520consistent%2520across%250Asemantic%2520granularities.%2520Consistency%2520can%2520in%2520fact%2520improve%2520accuracy.%2520We%2520build%2520upon%250Arecent%2520work%2520on%2520learning%2520hierarchical%2520segmentation%2520for%2520flat-level%2520recognition%252C%250Aand%2520extend%2520it%2520to%2520hierarchical%2520recognition.%2520It%2520naturally%2520captures%2520the%2520intuition%250Athat%2520fine-grained%2520recognition%2520requires%2520fine%2520image%2520segmentation%2520whereas%250Acoarse-grained%2520recognition%2520requires%2520coarse%2520segmentation%253B%2520they%2520can%2520all%2520be%250Aintegrated%2520into%2520one%2520recognition%2520model%2520that%2520drives%2520fine-to-coarse%2520internal%250Avisual%2520parsing.Additionally%252C%2520we%2520introduce%2520a%2520Tree-path%2520KL%2520Divergence%2520loss%2520to%250Aenforce%2520consistent%2520accurate%2520predictions%2520across%2520levels.%2520Our%2520extensive%250Aexperimentation%2520and%2520analysis%2520demonstrate%2520our%2520significant%2520gains%2520on%2520predicting%2520an%250Aaccurate%2520and%2520consistent%2520taxonomy%2520tree.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11608v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Hierarchical%20Semantic%20Classification%20by%20Grounding%20on%20Consistent%0A%20%20Image%20Segmentations&entry.906535625=Seulki%20Park%20and%20Youren%20Zhang%20and%20Stella%20X.%20Yu%20and%20Sara%20Beery%20and%20Jonathan%20Huang&entry.1292438233=%20%20Hierarchical%20semantic%20classification%20requires%20the%20prediction%20of%20a%20taxonomy%0Atree%20instead%20of%20a%20single%20flat%20level%20of%20the%20tree%2C%20where%20both%20accuracies%20at%0Aindividual%20levels%20and%20consistency%20across%20levels%20matter.%20We%20can%20train%0Aclassifiers%20for%20individual%20levels%2C%20which%20has%20accuracy%20but%20not%20consistency%2C%20or%0Awe%20can%20train%20only%20the%20finest%20level%20classification%20and%20infer%20higher%20levels%2C%0Awhich%20has%20consistency%20but%20not%20accuracy.%20Our%20key%20insight%20is%20that%20hierarchical%0Arecognition%20should%20not%20be%20treated%20as%20multi-task%20classification%2C%20as%20each%20level%0Ais%20essentially%20a%20different%20task%20and%20they%20would%20have%20to%20compromise%20with%20each%0Aother%2C%20but%20be%20grounded%20on%20image%20segmentations%20that%20are%20consistent%20across%0Asemantic%20granularities.%20Consistency%20can%20in%20fact%20improve%20accuracy.%20We%20build%20upon%0Arecent%20work%20on%20learning%20hierarchical%20segmentation%20for%20flat-level%20recognition%2C%0Aand%20extend%20it%20to%20hierarchical%20recognition.%20It%20naturally%20captures%20the%20intuition%0Athat%20fine-grained%20recognition%20requires%20fine%20image%20segmentation%20whereas%0Acoarse-grained%20recognition%20requires%20coarse%20segmentation%3B%20they%20can%20all%20be%0Aintegrated%20into%20one%20recognition%20model%20that%20drives%20fine-to-coarse%20internal%0Avisual%20parsing.Additionally%2C%20we%20introduce%20a%20Tree-path%20KL%20Divergence%20loss%20to%0Aenforce%20consistent%20accurate%20predictions%20across%20levels.%20Our%20extensive%0Aexperimentation%20and%20analysis%20demonstrate%20our%20significant%20gains%20on%20predicting%20an%0Aaccurate%20and%20consistent%20taxonomy%20tree.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11608v1&entry.124074799=Read"},
{"title": "SeamPose: Repurposing Seams as Capacitive Sensors in a Shirt for\n  Upper-Body Pose Tracking", "author": "Tianhong Catherine Yu and  Manru and  Zhang and Peter He and Chi-Jung Lee and Cassidy Cheesman and Saif Mahmud and Ruidong Zhang and Fran\u00e7ois Guimbreti\u00e8re and Cheng Zhang", "abstract": "  Seams are areas of overlapping fabric formed by stitching two or more pieces\nof fabric together in the cut-and-sew apparel manufacturing process. In\nSeamPose, we repurposed seams as capacitive sensors in a shirt for continuous\nupper-body pose estimation. Compared to previous all-textile motion-capturing\ngarments that place the electrodes on the surface of clothing, our solution\nleverages existing seams inside of a shirt by machine-sewing insulated\nconductive threads over the seams. The unique invisibilities and placements of\nthe seams afford the sensing shirt to look and wear the same as a conventional\nshirt while providing exciting pose-tracking capabilities. To validate this\napproach, we implemented a proof-of-concept untethered shirt. With eight\ncapacitive sensing seams, our customized deep-learning pipeline accurately\nestimates the upper-body 3D joint positions relative to the pelvis. With a\n12-participant user study, we demonstrated promising cross-user and\ncross-session tracking performance. SeamPose represents a step towards\nunobtrusive integration of smart clothing for everyday pose estimation.\n", "link": "http://arxiv.org/abs/2406.11645v1", "date": "2024-06-17", "relevancy": 2.0814, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.581}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5073}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.4649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SeamPose%3A%20Repurposing%20Seams%20as%20Capacitive%20Sensors%20in%20a%20Shirt%20for%0A%20%20Upper-Body%20Pose%20Tracking&body=Title%3A%20SeamPose%3A%20Repurposing%20Seams%20as%20Capacitive%20Sensors%20in%20a%20Shirt%20for%0A%20%20Upper-Body%20Pose%20Tracking%0AAuthor%3A%20Tianhong%20Catherine%20Yu%20and%20%20Manru%20and%20%20Zhang%20and%20Peter%20He%20and%20Chi-Jung%20Lee%20and%20Cassidy%20Cheesman%20and%20Saif%20Mahmud%20and%20Ruidong%20Zhang%20and%20Fran%C3%A7ois%20Guimbreti%C3%A8re%20and%20Cheng%20Zhang%0AAbstract%3A%20%20%20Seams%20are%20areas%20of%20overlapping%20fabric%20formed%20by%20stitching%20two%20or%20more%20pieces%0Aof%20fabric%20together%20in%20the%20cut-and-sew%20apparel%20manufacturing%20process.%20In%0ASeamPose%2C%20we%20repurposed%20seams%20as%20capacitive%20sensors%20in%20a%20shirt%20for%20continuous%0Aupper-body%20pose%20estimation.%20Compared%20to%20previous%20all-textile%20motion-capturing%0Agarments%20that%20place%20the%20electrodes%20on%20the%20surface%20of%20clothing%2C%20our%20solution%0Aleverages%20existing%20seams%20inside%20of%20a%20shirt%20by%20machine-sewing%20insulated%0Aconductive%20threads%20over%20the%20seams.%20The%20unique%20invisibilities%20and%20placements%20of%0Athe%20seams%20afford%20the%20sensing%20shirt%20to%20look%20and%20wear%20the%20same%20as%20a%20conventional%0Ashirt%20while%20providing%20exciting%20pose-tracking%20capabilities.%20To%20validate%20this%0Aapproach%2C%20we%20implemented%20a%20proof-of-concept%20untethered%20shirt.%20With%20eight%0Acapacitive%20sensing%20seams%2C%20our%20customized%20deep-learning%20pipeline%20accurately%0Aestimates%20the%20upper-body%203D%20joint%20positions%20relative%20to%20the%20pelvis.%20With%20a%0A12-participant%20user%20study%2C%20we%20demonstrated%20promising%20cross-user%20and%0Across-session%20tracking%20performance.%20SeamPose%20represents%20a%20step%20towards%0Aunobtrusive%20integration%20of%20smart%20clothing%20for%20everyday%20pose%20estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11645v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeamPose%253A%2520Repurposing%2520Seams%2520as%2520Capacitive%2520Sensors%2520in%2520a%2520Shirt%2520for%250A%2520%2520Upper-Body%2520Pose%2520Tracking%26entry.906535625%3DTianhong%2520Catherine%2520Yu%2520and%2520%2520Manru%2520and%2520%2520Zhang%2520and%2520Peter%2520He%2520and%2520Chi-Jung%2520Lee%2520and%2520Cassidy%2520Cheesman%2520and%2520Saif%2520Mahmud%2520and%2520Ruidong%2520Zhang%2520and%2520Fran%25C3%25A7ois%2520Guimbreti%25C3%25A8re%2520and%2520Cheng%2520Zhang%26entry.1292438233%3D%2520%2520Seams%2520are%2520areas%2520of%2520overlapping%2520fabric%2520formed%2520by%2520stitching%2520two%2520or%2520more%2520pieces%250Aof%2520fabric%2520together%2520in%2520the%2520cut-and-sew%2520apparel%2520manufacturing%2520process.%2520In%250ASeamPose%252C%2520we%2520repurposed%2520seams%2520as%2520capacitive%2520sensors%2520in%2520a%2520shirt%2520for%2520continuous%250Aupper-body%2520pose%2520estimation.%2520Compared%2520to%2520previous%2520all-textile%2520motion-capturing%250Agarments%2520that%2520place%2520the%2520electrodes%2520on%2520the%2520surface%2520of%2520clothing%252C%2520our%2520solution%250Aleverages%2520existing%2520seams%2520inside%2520of%2520a%2520shirt%2520by%2520machine-sewing%2520insulated%250Aconductive%2520threads%2520over%2520the%2520seams.%2520The%2520unique%2520invisibilities%2520and%2520placements%2520of%250Athe%2520seams%2520afford%2520the%2520sensing%2520shirt%2520to%2520look%2520and%2520wear%2520the%2520same%2520as%2520a%2520conventional%250Ashirt%2520while%2520providing%2520exciting%2520pose-tracking%2520capabilities.%2520To%2520validate%2520this%250Aapproach%252C%2520we%2520implemented%2520a%2520proof-of-concept%2520untethered%2520shirt.%2520With%2520eight%250Acapacitive%2520sensing%2520seams%252C%2520our%2520customized%2520deep-learning%2520pipeline%2520accurately%250Aestimates%2520the%2520upper-body%25203D%2520joint%2520positions%2520relative%2520to%2520the%2520pelvis.%2520With%2520a%250A12-participant%2520user%2520study%252C%2520we%2520demonstrated%2520promising%2520cross-user%2520and%250Across-session%2520tracking%2520performance.%2520SeamPose%2520represents%2520a%2520step%2520towards%250Aunobtrusive%2520integration%2520of%2520smart%2520clothing%2520for%2520everyday%2520pose%2520estimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11645v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SeamPose%3A%20Repurposing%20Seams%20as%20Capacitive%20Sensors%20in%20a%20Shirt%20for%0A%20%20Upper-Body%20Pose%20Tracking&entry.906535625=Tianhong%20Catherine%20Yu%20and%20%20Manru%20and%20%20Zhang%20and%20Peter%20He%20and%20Chi-Jung%20Lee%20and%20Cassidy%20Cheesman%20and%20Saif%20Mahmud%20and%20Ruidong%20Zhang%20and%20Fran%C3%A7ois%20Guimbreti%C3%A8re%20and%20Cheng%20Zhang&entry.1292438233=%20%20Seams%20are%20areas%20of%20overlapping%20fabric%20formed%20by%20stitching%20two%20or%20more%20pieces%0Aof%20fabric%20together%20in%20the%20cut-and-sew%20apparel%20manufacturing%20process.%20In%0ASeamPose%2C%20we%20repurposed%20seams%20as%20capacitive%20sensors%20in%20a%20shirt%20for%20continuous%0Aupper-body%20pose%20estimation.%20Compared%20to%20previous%20all-textile%20motion-capturing%0Agarments%20that%20place%20the%20electrodes%20on%20the%20surface%20of%20clothing%2C%20our%20solution%0Aleverages%20existing%20seams%20inside%20of%20a%20shirt%20by%20machine-sewing%20insulated%0Aconductive%20threads%20over%20the%20seams.%20The%20unique%20invisibilities%20and%20placements%20of%0Athe%20seams%20afford%20the%20sensing%20shirt%20to%20look%20and%20wear%20the%20same%20as%20a%20conventional%0Ashirt%20while%20providing%20exciting%20pose-tracking%20capabilities.%20To%20validate%20this%0Aapproach%2C%20we%20implemented%20a%20proof-of-concept%20untethered%20shirt.%20With%20eight%0Acapacitive%20sensing%20seams%2C%20our%20customized%20deep-learning%20pipeline%20accurately%0Aestimates%20the%20upper-body%203D%20joint%20positions%20relative%20to%20the%20pelvis.%20With%20a%0A12-participant%20user%20study%2C%20we%20demonstrated%20promising%20cross-user%20and%0Across-session%20tracking%20performance.%20SeamPose%20represents%20a%20step%20towards%0Aunobtrusive%20integration%20of%20smart%20clothing%20for%20everyday%20pose%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11645v1&entry.124074799=Read"},
{"title": "Generative Pretrained Structured Transformers: Unsupervised Syntactic\n  Language Models at Scale", "author": "Xiang Hu and Pengyu Ji and Qingyang Zhu and Wei Wu and Kewei Tu", "abstract": "  A syntactic language model (SLM) incrementally generates a sentence with its\nsyntactic tree in a left-to-right manner. We present Generative Pretrained\nStructured Transformers (GPST), an unsupervised SLM at scale capable of being\npre-trained from scratch on raw texts with high parallelism. GPST circumvents\nthe limitations of previous SLMs such as relying on gold trees and sequential\ntraining. It consists of two components, a usual SLM supervised by a\nuni-directional language modeling loss, and an additional composition model,\nwhich induces syntactic parse trees and computes constituent representations,\nsupervised by a bi-directional language modeling loss. We propose a\nrepresentation surrogate to enable joint parallel training of the two models in\na hard-EM fashion. We pre-train GPST on OpenWebText, a corpus with $9$ billion\ntokens, and demonstrate the superiority of GPST over GPT-2 with a comparable\nsize in numerous tasks covering both language understanding and language\ngeneration. Meanwhile, GPST also significantly outperforms existing\nunsupervised SLMs on left-to-right grammar induction, while holding a\nsubstantial acceleration on training.\n", "link": "http://arxiv.org/abs/2403.08293v3", "date": "2024-06-17", "relevancy": 2.08, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5473}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5428}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Pretrained%20Structured%20Transformers%3A%20Unsupervised%20Syntactic%0A%20%20Language%20Models%20at%20Scale&body=Title%3A%20Generative%20Pretrained%20Structured%20Transformers%3A%20Unsupervised%20Syntactic%0A%20%20Language%20Models%20at%20Scale%0AAuthor%3A%20Xiang%20Hu%20and%20Pengyu%20Ji%20and%20Qingyang%20Zhu%20and%20Wei%20Wu%20and%20Kewei%20Tu%0AAbstract%3A%20%20%20A%20syntactic%20language%20model%20%28SLM%29%20incrementally%20generates%20a%20sentence%20with%20its%0Asyntactic%20tree%20in%20a%20left-to-right%20manner.%20We%20present%20Generative%20Pretrained%0AStructured%20Transformers%20%28GPST%29%2C%20an%20unsupervised%20SLM%20at%20scale%20capable%20of%20being%0Apre-trained%20from%20scratch%20on%20raw%20texts%20with%20high%20parallelism.%20GPST%20circumvents%0Athe%20limitations%20of%20previous%20SLMs%20such%20as%20relying%20on%20gold%20trees%20and%20sequential%0Atraining.%20It%20consists%20of%20two%20components%2C%20a%20usual%20SLM%20supervised%20by%20a%0Auni-directional%20language%20modeling%20loss%2C%20and%20an%20additional%20composition%20model%2C%0Awhich%20induces%20syntactic%20parse%20trees%20and%20computes%20constituent%20representations%2C%0Asupervised%20by%20a%20bi-directional%20language%20modeling%20loss.%20We%20propose%20a%0Arepresentation%20surrogate%20to%20enable%20joint%20parallel%20training%20of%20the%20two%20models%20in%0Aa%20hard-EM%20fashion.%20We%20pre-train%20GPST%20on%20OpenWebText%2C%20a%20corpus%20with%20%249%24%20billion%0Atokens%2C%20and%20demonstrate%20the%20superiority%20of%20GPST%20over%20GPT-2%20with%20a%20comparable%0Asize%20in%20numerous%20tasks%20covering%20both%20language%20understanding%20and%20language%0Ageneration.%20Meanwhile%2C%20GPST%20also%20significantly%20outperforms%20existing%0Aunsupervised%20SLMs%20on%20left-to-right%20grammar%20induction%2C%20while%20holding%20a%0Asubstantial%20acceleration%20on%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08293v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Pretrained%2520Structured%2520Transformers%253A%2520Unsupervised%2520Syntactic%250A%2520%2520Language%2520Models%2520at%2520Scale%26entry.906535625%3DXiang%2520Hu%2520and%2520Pengyu%2520Ji%2520and%2520Qingyang%2520Zhu%2520and%2520Wei%2520Wu%2520and%2520Kewei%2520Tu%26entry.1292438233%3D%2520%2520A%2520syntactic%2520language%2520model%2520%2528SLM%2529%2520incrementally%2520generates%2520a%2520sentence%2520with%2520its%250Asyntactic%2520tree%2520in%2520a%2520left-to-right%2520manner.%2520We%2520present%2520Generative%2520Pretrained%250AStructured%2520Transformers%2520%2528GPST%2529%252C%2520an%2520unsupervised%2520SLM%2520at%2520scale%2520capable%2520of%2520being%250Apre-trained%2520from%2520scratch%2520on%2520raw%2520texts%2520with%2520high%2520parallelism.%2520GPST%2520circumvents%250Athe%2520limitations%2520of%2520previous%2520SLMs%2520such%2520as%2520relying%2520on%2520gold%2520trees%2520and%2520sequential%250Atraining.%2520It%2520consists%2520of%2520two%2520components%252C%2520a%2520usual%2520SLM%2520supervised%2520by%2520a%250Auni-directional%2520language%2520modeling%2520loss%252C%2520and%2520an%2520additional%2520composition%2520model%252C%250Awhich%2520induces%2520syntactic%2520parse%2520trees%2520and%2520computes%2520constituent%2520representations%252C%250Asupervised%2520by%2520a%2520bi-directional%2520language%2520modeling%2520loss.%2520We%2520propose%2520a%250Arepresentation%2520surrogate%2520to%2520enable%2520joint%2520parallel%2520training%2520of%2520the%2520two%2520models%2520in%250Aa%2520hard-EM%2520fashion.%2520We%2520pre-train%2520GPST%2520on%2520OpenWebText%252C%2520a%2520corpus%2520with%2520%25249%2524%2520billion%250Atokens%252C%2520and%2520demonstrate%2520the%2520superiority%2520of%2520GPST%2520over%2520GPT-2%2520with%2520a%2520comparable%250Asize%2520in%2520numerous%2520tasks%2520covering%2520both%2520language%2520understanding%2520and%2520language%250Ageneration.%2520Meanwhile%252C%2520GPST%2520also%2520significantly%2520outperforms%2520existing%250Aunsupervised%2520SLMs%2520on%2520left-to-right%2520grammar%2520induction%252C%2520while%2520holding%2520a%250Asubstantial%2520acceleration%2520on%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.08293v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Pretrained%20Structured%20Transformers%3A%20Unsupervised%20Syntactic%0A%20%20Language%20Models%20at%20Scale&entry.906535625=Xiang%20Hu%20and%20Pengyu%20Ji%20and%20Qingyang%20Zhu%20and%20Wei%20Wu%20and%20Kewei%20Tu&entry.1292438233=%20%20A%20syntactic%20language%20model%20%28SLM%29%20incrementally%20generates%20a%20sentence%20with%20its%0Asyntactic%20tree%20in%20a%20left-to-right%20manner.%20We%20present%20Generative%20Pretrained%0AStructured%20Transformers%20%28GPST%29%2C%20an%20unsupervised%20SLM%20at%20scale%20capable%20of%20being%0Apre-trained%20from%20scratch%20on%20raw%20texts%20with%20high%20parallelism.%20GPST%20circumvents%0Athe%20limitations%20of%20previous%20SLMs%20such%20as%20relying%20on%20gold%20trees%20and%20sequential%0Atraining.%20It%20consists%20of%20two%20components%2C%20a%20usual%20SLM%20supervised%20by%20a%0Auni-directional%20language%20modeling%20loss%2C%20and%20an%20additional%20composition%20model%2C%0Awhich%20induces%20syntactic%20parse%20trees%20and%20computes%20constituent%20representations%2C%0Asupervised%20by%20a%20bi-directional%20language%20modeling%20loss.%20We%20propose%20a%0Arepresentation%20surrogate%20to%20enable%20joint%20parallel%20training%20of%20the%20two%20models%20in%0Aa%20hard-EM%20fashion.%20We%20pre-train%20GPST%20on%20OpenWebText%2C%20a%20corpus%20with%20%249%24%20billion%0Atokens%2C%20and%20demonstrate%20the%20superiority%20of%20GPST%20over%20GPT-2%20with%20a%20comparable%0Asize%20in%20numerous%20tasks%20covering%20both%20language%20understanding%20and%20language%0Ageneration.%20Meanwhile%2C%20GPST%20also%20significantly%20outperforms%20existing%0Aunsupervised%20SLMs%20on%20left-to-right%20grammar%20induction%2C%20while%20holding%20a%0Asubstantial%20acceleration%20on%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08293v3&entry.124074799=Read"},
{"title": "Measurement Simplification in \u03c1-POMDP with Performance Guarantees", "author": "Tom Yotam and Vadim Indelman", "abstract": "  Decision making under uncertainty is at the heart of any autonomous system\nacting with imperfect information. The cost of solving the decision making\nproblem is exponential in the action and observation spaces, thus rendering it\nunfeasible for many online systems. This paper introduces a novel approach to\nefficient decision-making, by partitioning the high-dimensional observation\nspace. Using the partitioned observation space, we formulate analytical bounds\non the expected information-theoretic reward, for general belief distributions.\nThese bounds are then used to plan efficiently while keeping performance\nguarantees. We show that the bounds are adaptive, computationally efficient,\nand that they converge to the original solution. We extend the partitioning\nparadigm and present a hierarchy of partitioned spaces that allows greater\nefficiency in planning. We then propose a specific variant of these bounds for\nGaussian beliefs and show a theoretical performance improvement of at least a\nfactor of 4. Finally, we compare our novel method to other state of the art\nalgorithms in active SLAM scenarios, in simulation and in real experiments. In\nboth cases we show a significant speed-up in planning with performance\nguarantees.\n", "link": "http://arxiv.org/abs/2309.10701v2", "date": "2024-06-17", "relevancy": 2.0745, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.553}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.55}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Measurement%20Simplification%20in%20%CF%81-POMDP%20with%20Performance%20Guarantees&body=Title%3A%20Measurement%20Simplification%20in%20%CF%81-POMDP%20with%20Performance%20Guarantees%0AAuthor%3A%20Tom%20Yotam%20and%20Vadim%20Indelman%0AAbstract%3A%20%20%20Decision%20making%20under%20uncertainty%20is%20at%20the%20heart%20of%20any%20autonomous%20system%0Aacting%20with%20imperfect%20information.%20The%20cost%20of%20solving%20the%20decision%20making%0Aproblem%20is%20exponential%20in%20the%20action%20and%20observation%20spaces%2C%20thus%20rendering%20it%0Aunfeasible%20for%20many%20online%20systems.%20This%20paper%20introduces%20a%20novel%20approach%20to%0Aefficient%20decision-making%2C%20by%20partitioning%20the%20high-dimensional%20observation%0Aspace.%20Using%20the%20partitioned%20observation%20space%2C%20we%20formulate%20analytical%20bounds%0Aon%20the%20expected%20information-theoretic%20reward%2C%20for%20general%20belief%20distributions.%0AThese%20bounds%20are%20then%20used%20to%20plan%20efficiently%20while%20keeping%20performance%0Aguarantees.%20We%20show%20that%20the%20bounds%20are%20adaptive%2C%20computationally%20efficient%2C%0Aand%20that%20they%20converge%20to%20the%20original%20solution.%20We%20extend%20the%20partitioning%0Aparadigm%20and%20present%20a%20hierarchy%20of%20partitioned%20spaces%20that%20allows%20greater%0Aefficiency%20in%20planning.%20We%20then%20propose%20a%20specific%20variant%20of%20these%20bounds%20for%0AGaussian%20beliefs%20and%20show%20a%20theoretical%20performance%20improvement%20of%20at%20least%20a%0Afactor%20of%204.%20Finally%2C%20we%20compare%20our%20novel%20method%20to%20other%20state%20of%20the%20art%0Aalgorithms%20in%20active%20SLAM%20scenarios%2C%20in%20simulation%20and%20in%20real%20experiments.%20In%0Aboth%20cases%20we%20show%20a%20significant%20speed-up%20in%20planning%20with%20performance%0Aguarantees.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.10701v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeasurement%2520Simplification%2520in%2520%25CF%2581-POMDP%2520with%2520Performance%2520Guarantees%26entry.906535625%3DTom%2520Yotam%2520and%2520Vadim%2520Indelman%26entry.1292438233%3D%2520%2520Decision%2520making%2520under%2520uncertainty%2520is%2520at%2520the%2520heart%2520of%2520any%2520autonomous%2520system%250Aacting%2520with%2520imperfect%2520information.%2520The%2520cost%2520of%2520solving%2520the%2520decision%2520making%250Aproblem%2520is%2520exponential%2520in%2520the%2520action%2520and%2520observation%2520spaces%252C%2520thus%2520rendering%2520it%250Aunfeasible%2520for%2520many%2520online%2520systems.%2520This%2520paper%2520introduces%2520a%2520novel%2520approach%2520to%250Aefficient%2520decision-making%252C%2520by%2520partitioning%2520the%2520high-dimensional%2520observation%250Aspace.%2520Using%2520the%2520partitioned%2520observation%2520space%252C%2520we%2520formulate%2520analytical%2520bounds%250Aon%2520the%2520expected%2520information-theoretic%2520reward%252C%2520for%2520general%2520belief%2520distributions.%250AThese%2520bounds%2520are%2520then%2520used%2520to%2520plan%2520efficiently%2520while%2520keeping%2520performance%250Aguarantees.%2520We%2520show%2520that%2520the%2520bounds%2520are%2520adaptive%252C%2520computationally%2520efficient%252C%250Aand%2520that%2520they%2520converge%2520to%2520the%2520original%2520solution.%2520We%2520extend%2520the%2520partitioning%250Aparadigm%2520and%2520present%2520a%2520hierarchy%2520of%2520partitioned%2520spaces%2520that%2520allows%2520greater%250Aefficiency%2520in%2520planning.%2520We%2520then%2520propose%2520a%2520specific%2520variant%2520of%2520these%2520bounds%2520for%250AGaussian%2520beliefs%2520and%2520show%2520a%2520theoretical%2520performance%2520improvement%2520of%2520at%2520least%2520a%250Afactor%2520of%25204.%2520Finally%252C%2520we%2520compare%2520our%2520novel%2520method%2520to%2520other%2520state%2520of%2520the%2520art%250Aalgorithms%2520in%2520active%2520SLAM%2520scenarios%252C%2520in%2520simulation%2520and%2520in%2520real%2520experiments.%2520In%250Aboth%2520cases%2520we%2520show%2520a%2520significant%2520speed-up%2520in%2520planning%2520with%2520performance%250Aguarantees.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.10701v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Measurement%20Simplification%20in%20%CF%81-POMDP%20with%20Performance%20Guarantees&entry.906535625=Tom%20Yotam%20and%20Vadim%20Indelman&entry.1292438233=%20%20Decision%20making%20under%20uncertainty%20is%20at%20the%20heart%20of%20any%20autonomous%20system%0Aacting%20with%20imperfect%20information.%20The%20cost%20of%20solving%20the%20decision%20making%0Aproblem%20is%20exponential%20in%20the%20action%20and%20observation%20spaces%2C%20thus%20rendering%20it%0Aunfeasible%20for%20many%20online%20systems.%20This%20paper%20introduces%20a%20novel%20approach%20to%0Aefficient%20decision-making%2C%20by%20partitioning%20the%20high-dimensional%20observation%0Aspace.%20Using%20the%20partitioned%20observation%20space%2C%20we%20formulate%20analytical%20bounds%0Aon%20the%20expected%20information-theoretic%20reward%2C%20for%20general%20belief%20distributions.%0AThese%20bounds%20are%20then%20used%20to%20plan%20efficiently%20while%20keeping%20performance%0Aguarantees.%20We%20show%20that%20the%20bounds%20are%20adaptive%2C%20computationally%20efficient%2C%0Aand%20that%20they%20converge%20to%20the%20original%20solution.%20We%20extend%20the%20partitioning%0Aparadigm%20and%20present%20a%20hierarchy%20of%20partitioned%20spaces%20that%20allows%20greater%0Aefficiency%20in%20planning.%20We%20then%20propose%20a%20specific%20variant%20of%20these%20bounds%20for%0AGaussian%20beliefs%20and%20show%20a%20theoretical%20performance%20improvement%20of%20at%20least%20a%0Afactor%20of%204.%20Finally%2C%20we%20compare%20our%20novel%20method%20to%20other%20state%20of%20the%20art%0Aalgorithms%20in%20active%20SLAM%20scenarios%2C%20in%20simulation%20and%20in%20real%20experiments.%20In%0Aboth%20cases%20we%20show%20a%20significant%20speed-up%20in%20planning%20with%20performance%0Aguarantees.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.10701v2&entry.124074799=Read"},
{"title": "Discriminative Hamiltonian Variational Autoencoder for Accurate Tumor\n  Segmentation in Data-Scarce Regimes", "author": "Aghiles Kebaili and J\u00e9r\u00f4me Lapuyade-Lahorgue and Pierre Vera and Su Ruan", "abstract": "  Deep learning has gained significant attention in medical image segmentation.\nHowever, the limited availability of annotated training data presents a\nchallenge to achieving accurate results. In efforts to overcome this challenge,\ndata augmentation techniques have been proposed. However, the majority of these\napproaches primarily focus on image generation. For segmentation tasks,\nproviding both images and their corresponding target masks is crucial, and the\ngeneration of diverse and realistic samples remains a complex task, especially\nwhen working with limited training datasets. To this end, we propose a new\nend-to-end hybrid architecture based on Hamiltonian Variational Autoencoders\n(HVAE) and a discriminative regularization to improve the quality of generated\nimages. Our method provides an accuracte estimation of the joint distribution\nof the images and masks, resulting in the generation of realistic medical\nimages with reduced artifacts and off-distribution instances. As generating 3D\nvolumes requires substantial time and memory, our architecture operates on a\nslice-by-slice basis to segment 3D volumes, capitilizing on the richly\naugmented dataset. Experiments conducted on two public datasets, BRATS (MRI\nmodality) and HECKTOR (PET modality), demonstrate the efficacy of our proposed\nmethod on different medical imaging modalities with limited data.\n", "link": "http://arxiv.org/abs/2406.11659v1", "date": "2024-06-17", "relevancy": 2.0694, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5442}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5163}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5077}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discriminative%20Hamiltonian%20Variational%20Autoencoder%20for%20Accurate%20Tumor%0A%20%20Segmentation%20in%20Data-Scarce%20Regimes&body=Title%3A%20Discriminative%20Hamiltonian%20Variational%20Autoencoder%20for%20Accurate%20Tumor%0A%20%20Segmentation%20in%20Data-Scarce%20Regimes%0AAuthor%3A%20Aghiles%20Kebaili%20and%20J%C3%A9r%C3%B4me%20Lapuyade-Lahorgue%20and%20Pierre%20Vera%20and%20Su%20Ruan%0AAbstract%3A%20%20%20Deep%20learning%20has%20gained%20significant%20attention%20in%20medical%20image%20segmentation.%0AHowever%2C%20the%20limited%20availability%20of%20annotated%20training%20data%20presents%20a%0Achallenge%20to%20achieving%20accurate%20results.%20In%20efforts%20to%20overcome%20this%20challenge%2C%0Adata%20augmentation%20techniques%20have%20been%20proposed.%20However%2C%20the%20majority%20of%20these%0Aapproaches%20primarily%20focus%20on%20image%20generation.%20For%20segmentation%20tasks%2C%0Aproviding%20both%20images%20and%20their%20corresponding%20target%20masks%20is%20crucial%2C%20and%20the%0Ageneration%20of%20diverse%20and%20realistic%20samples%20remains%20a%20complex%20task%2C%20especially%0Awhen%20working%20with%20limited%20training%20datasets.%20To%20this%20end%2C%20we%20propose%20a%20new%0Aend-to-end%20hybrid%20architecture%20based%20on%20Hamiltonian%20Variational%20Autoencoders%0A%28HVAE%29%20and%20a%20discriminative%20regularization%20to%20improve%20the%20quality%20of%20generated%0Aimages.%20Our%20method%20provides%20an%20accuracte%20estimation%20of%20the%20joint%20distribution%0Aof%20the%20images%20and%20masks%2C%20resulting%20in%20the%20generation%20of%20realistic%20medical%0Aimages%20with%20reduced%20artifacts%20and%20off-distribution%20instances.%20As%20generating%203D%0Avolumes%20requires%20substantial%20time%20and%20memory%2C%20our%20architecture%20operates%20on%20a%0Aslice-by-slice%20basis%20to%20segment%203D%20volumes%2C%20capitilizing%20on%20the%20richly%0Aaugmented%20dataset.%20Experiments%20conducted%20on%20two%20public%20datasets%2C%20BRATS%20%28MRI%0Amodality%29%20and%20HECKTOR%20%28PET%20modality%29%2C%20demonstrate%20the%20efficacy%20of%20our%20proposed%0Amethod%20on%20different%20medical%20imaging%20modalities%20with%20limited%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11659v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscriminative%2520Hamiltonian%2520Variational%2520Autoencoder%2520for%2520Accurate%2520Tumor%250A%2520%2520Segmentation%2520in%2520Data-Scarce%2520Regimes%26entry.906535625%3DAghiles%2520Kebaili%2520and%2520J%25C3%25A9r%25C3%25B4me%2520Lapuyade-Lahorgue%2520and%2520Pierre%2520Vera%2520and%2520Su%2520Ruan%26entry.1292438233%3D%2520%2520Deep%2520learning%2520has%2520gained%2520significant%2520attention%2520in%2520medical%2520image%2520segmentation.%250AHowever%252C%2520the%2520limited%2520availability%2520of%2520annotated%2520training%2520data%2520presents%2520a%250Achallenge%2520to%2520achieving%2520accurate%2520results.%2520In%2520efforts%2520to%2520overcome%2520this%2520challenge%252C%250Adata%2520augmentation%2520techniques%2520have%2520been%2520proposed.%2520However%252C%2520the%2520majority%2520of%2520these%250Aapproaches%2520primarily%2520focus%2520on%2520image%2520generation.%2520For%2520segmentation%2520tasks%252C%250Aproviding%2520both%2520images%2520and%2520their%2520corresponding%2520target%2520masks%2520is%2520crucial%252C%2520and%2520the%250Ageneration%2520of%2520diverse%2520and%2520realistic%2520samples%2520remains%2520a%2520complex%2520task%252C%2520especially%250Awhen%2520working%2520with%2520limited%2520training%2520datasets.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520new%250Aend-to-end%2520hybrid%2520architecture%2520based%2520on%2520Hamiltonian%2520Variational%2520Autoencoders%250A%2528HVAE%2529%2520and%2520a%2520discriminative%2520regularization%2520to%2520improve%2520the%2520quality%2520of%2520generated%250Aimages.%2520Our%2520method%2520provides%2520an%2520accuracte%2520estimation%2520of%2520the%2520joint%2520distribution%250Aof%2520the%2520images%2520and%2520masks%252C%2520resulting%2520in%2520the%2520generation%2520of%2520realistic%2520medical%250Aimages%2520with%2520reduced%2520artifacts%2520and%2520off-distribution%2520instances.%2520As%2520generating%25203D%250Avolumes%2520requires%2520substantial%2520time%2520and%2520memory%252C%2520our%2520architecture%2520operates%2520on%2520a%250Aslice-by-slice%2520basis%2520to%2520segment%25203D%2520volumes%252C%2520capitilizing%2520on%2520the%2520richly%250Aaugmented%2520dataset.%2520Experiments%2520conducted%2520on%2520two%2520public%2520datasets%252C%2520BRATS%2520%2528MRI%250Amodality%2529%2520and%2520HECKTOR%2520%2528PET%2520modality%2529%252C%2520demonstrate%2520the%2520efficacy%2520of%2520our%2520proposed%250Amethod%2520on%2520different%2520medical%2520imaging%2520modalities%2520with%2520limited%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11659v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discriminative%20Hamiltonian%20Variational%20Autoencoder%20for%20Accurate%20Tumor%0A%20%20Segmentation%20in%20Data-Scarce%20Regimes&entry.906535625=Aghiles%20Kebaili%20and%20J%C3%A9r%C3%B4me%20Lapuyade-Lahorgue%20and%20Pierre%20Vera%20and%20Su%20Ruan&entry.1292438233=%20%20Deep%20learning%20has%20gained%20significant%20attention%20in%20medical%20image%20segmentation.%0AHowever%2C%20the%20limited%20availability%20of%20annotated%20training%20data%20presents%20a%0Achallenge%20to%20achieving%20accurate%20results.%20In%20efforts%20to%20overcome%20this%20challenge%2C%0Adata%20augmentation%20techniques%20have%20been%20proposed.%20However%2C%20the%20majority%20of%20these%0Aapproaches%20primarily%20focus%20on%20image%20generation.%20For%20segmentation%20tasks%2C%0Aproviding%20both%20images%20and%20their%20corresponding%20target%20masks%20is%20crucial%2C%20and%20the%0Ageneration%20of%20diverse%20and%20realistic%20samples%20remains%20a%20complex%20task%2C%20especially%0Awhen%20working%20with%20limited%20training%20datasets.%20To%20this%20end%2C%20we%20propose%20a%20new%0Aend-to-end%20hybrid%20architecture%20based%20on%20Hamiltonian%20Variational%20Autoencoders%0A%28HVAE%29%20and%20a%20discriminative%20regularization%20to%20improve%20the%20quality%20of%20generated%0Aimages.%20Our%20method%20provides%20an%20accuracte%20estimation%20of%20the%20joint%20distribution%0Aof%20the%20images%20and%20masks%2C%20resulting%20in%20the%20generation%20of%20realistic%20medical%0Aimages%20with%20reduced%20artifacts%20and%20off-distribution%20instances.%20As%20generating%203D%0Avolumes%20requires%20substantial%20time%20and%20memory%2C%20our%20architecture%20operates%20on%20a%0Aslice-by-slice%20basis%20to%20segment%203D%20volumes%2C%20capitilizing%20on%20the%20richly%0Aaugmented%20dataset.%20Experiments%20conducted%20on%20two%20public%20datasets%2C%20BRATS%20%28MRI%0Amodality%29%20and%20HECKTOR%20%28PET%20modality%29%2C%20demonstrate%20the%20efficacy%20of%20our%20proposed%0Amethod%20on%20different%20medical%20imaging%20modalities%20with%20limited%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11659v1&entry.124074799=Read"},
{"title": "A First Physical-World Trajectory Prediction Attack via LiDAR-induced\n  Deceptions in Autonomous Driving", "author": "Yang Lou and Yi Zhu and Qun Song and Rui Tan and Chunming Qiao and Wei-Bin Lee and Jianping Wang", "abstract": "  Trajectory prediction forecasts nearby agents' moves based on their\nhistorical trajectories. Accurate trajectory prediction is crucial for\nautonomous vehicles. Existing attacks compromise the prediction model of a\nvictim AV by directly manipulating the historical trajectory of an attacker AV,\nwhich has limited real-world applicability. This paper, for the first time,\nexplores an indirect attack approach that induces prediction errors via attacks\nagainst the perception module of a victim AV. Although it has been shown that\nphysically realizable attacks against LiDAR-based perception are possible by\nplacing a few objects at strategic locations, it is still an open challenge to\nfind an object location from the vast search space in order to launch effective\nattacks against prediction under varying victim AV velocities.\n  Through analysis, we observe that a prediction model is prone to an attack\nfocusing on a single point in the scene. Consequently, we propose a novel\ntwo-stage attack framework to realize the single-point attack. The first stage\nof prediction-side attack efficiently identifies, guided by the distribution of\ndetection results under object-based attacks against perception, the state\nperturbations for the prediction model that are effective and\nvelocity-insensitive. In the second stage of location matching, we match the\nfeasible object locations with the found state perturbations. Our evaluation\nusing a public autonomous driving dataset shows that our attack causes a\ncollision rate of up to 63% and various hazardous responses of the victim AV.\nThe effectiveness of our attack is also demonstrated on a real testbed car. To\nthe best of our knowledge, this study is the first security analysis spanning\nfrom LiDAR-based perception to prediction in autonomous driving, leading to a\nrealistic attack on prediction. To counteract the proposed attack, potential\ndefenses are discussed.\n", "link": "http://arxiv.org/abs/2406.11707v1", "date": "2024-06-17", "relevancy": 2.0676, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5617}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5293}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20First%20Physical-World%20Trajectory%20Prediction%20Attack%20via%20LiDAR-induced%0A%20%20Deceptions%20in%20Autonomous%20Driving&body=Title%3A%20A%20First%20Physical-World%20Trajectory%20Prediction%20Attack%20via%20LiDAR-induced%0A%20%20Deceptions%20in%20Autonomous%20Driving%0AAuthor%3A%20Yang%20Lou%20and%20Yi%20Zhu%20and%20Qun%20Song%20and%20Rui%20Tan%20and%20Chunming%20Qiao%20and%20Wei-Bin%20Lee%20and%20Jianping%20Wang%0AAbstract%3A%20%20%20Trajectory%20prediction%20forecasts%20nearby%20agents%27%20moves%20based%20on%20their%0Ahistorical%20trajectories.%20Accurate%20trajectory%20prediction%20is%20crucial%20for%0Aautonomous%20vehicles.%20Existing%20attacks%20compromise%20the%20prediction%20model%20of%20a%0Avictim%20AV%20by%20directly%20manipulating%20the%20historical%20trajectory%20of%20an%20attacker%20AV%2C%0Awhich%20has%20limited%20real-world%20applicability.%20This%20paper%2C%20for%20the%20first%20time%2C%0Aexplores%20an%20indirect%20attack%20approach%20that%20induces%20prediction%20errors%20via%20attacks%0Aagainst%20the%20perception%20module%20of%20a%20victim%20AV.%20Although%20it%20has%20been%20shown%20that%0Aphysically%20realizable%20attacks%20against%20LiDAR-based%20perception%20are%20possible%20by%0Aplacing%20a%20few%20objects%20at%20strategic%20locations%2C%20it%20is%20still%20an%20open%20challenge%20to%0Afind%20an%20object%20location%20from%20the%20vast%20search%20space%20in%20order%20to%20launch%20effective%0Aattacks%20against%20prediction%20under%20varying%20victim%20AV%20velocities.%0A%20%20Through%20analysis%2C%20we%20observe%20that%20a%20prediction%20model%20is%20prone%20to%20an%20attack%0Afocusing%20on%20a%20single%20point%20in%20the%20scene.%20Consequently%2C%20we%20propose%20a%20novel%0Atwo-stage%20attack%20framework%20to%20realize%20the%20single-point%20attack.%20The%20first%20stage%0Aof%20prediction-side%20attack%20efficiently%20identifies%2C%20guided%20by%20the%20distribution%20of%0Adetection%20results%20under%20object-based%20attacks%20against%20perception%2C%20the%20state%0Aperturbations%20for%20the%20prediction%20model%20that%20are%20effective%20and%0Avelocity-insensitive.%20In%20the%20second%20stage%20of%20location%20matching%2C%20we%20match%20the%0Afeasible%20object%20locations%20with%20the%20found%20state%20perturbations.%20Our%20evaluation%0Ausing%20a%20public%20autonomous%20driving%20dataset%20shows%20that%20our%20attack%20causes%20a%0Acollision%20rate%20of%20up%20to%2063%25%20and%20various%20hazardous%20responses%20of%20the%20victim%20AV.%0AThe%20effectiveness%20of%20our%20attack%20is%20also%20demonstrated%20on%20a%20real%20testbed%20car.%20To%0Athe%20best%20of%20our%20knowledge%2C%20this%20study%20is%20the%20first%20security%20analysis%20spanning%0Afrom%20LiDAR-based%20perception%20to%20prediction%20in%20autonomous%20driving%2C%20leading%20to%20a%0Arealistic%20attack%20on%20prediction.%20To%20counteract%20the%20proposed%20attack%2C%20potential%0Adefenses%20are%20discussed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11707v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520First%2520Physical-World%2520Trajectory%2520Prediction%2520Attack%2520via%2520LiDAR-induced%250A%2520%2520Deceptions%2520in%2520Autonomous%2520Driving%26entry.906535625%3DYang%2520Lou%2520and%2520Yi%2520Zhu%2520and%2520Qun%2520Song%2520and%2520Rui%2520Tan%2520and%2520Chunming%2520Qiao%2520and%2520Wei-Bin%2520Lee%2520and%2520Jianping%2520Wang%26entry.1292438233%3D%2520%2520Trajectory%2520prediction%2520forecasts%2520nearby%2520agents%2527%2520moves%2520based%2520on%2520their%250Ahistorical%2520trajectories.%2520Accurate%2520trajectory%2520prediction%2520is%2520crucial%2520for%250Aautonomous%2520vehicles.%2520Existing%2520attacks%2520compromise%2520the%2520prediction%2520model%2520of%2520a%250Avictim%2520AV%2520by%2520directly%2520manipulating%2520the%2520historical%2520trajectory%2520of%2520an%2520attacker%2520AV%252C%250Awhich%2520has%2520limited%2520real-world%2520applicability.%2520This%2520paper%252C%2520for%2520the%2520first%2520time%252C%250Aexplores%2520an%2520indirect%2520attack%2520approach%2520that%2520induces%2520prediction%2520errors%2520via%2520attacks%250Aagainst%2520the%2520perception%2520module%2520of%2520a%2520victim%2520AV.%2520Although%2520it%2520has%2520been%2520shown%2520that%250Aphysically%2520realizable%2520attacks%2520against%2520LiDAR-based%2520perception%2520are%2520possible%2520by%250Aplacing%2520a%2520few%2520objects%2520at%2520strategic%2520locations%252C%2520it%2520is%2520still%2520an%2520open%2520challenge%2520to%250Afind%2520an%2520object%2520location%2520from%2520the%2520vast%2520search%2520space%2520in%2520order%2520to%2520launch%2520effective%250Aattacks%2520against%2520prediction%2520under%2520varying%2520victim%2520AV%2520velocities.%250A%2520%2520Through%2520analysis%252C%2520we%2520observe%2520that%2520a%2520prediction%2520model%2520is%2520prone%2520to%2520an%2520attack%250Afocusing%2520on%2520a%2520single%2520point%2520in%2520the%2520scene.%2520Consequently%252C%2520we%2520propose%2520a%2520novel%250Atwo-stage%2520attack%2520framework%2520to%2520realize%2520the%2520single-point%2520attack.%2520The%2520first%2520stage%250Aof%2520prediction-side%2520attack%2520efficiently%2520identifies%252C%2520guided%2520by%2520the%2520distribution%2520of%250Adetection%2520results%2520under%2520object-based%2520attacks%2520against%2520perception%252C%2520the%2520state%250Aperturbations%2520for%2520the%2520prediction%2520model%2520that%2520are%2520effective%2520and%250Avelocity-insensitive.%2520In%2520the%2520second%2520stage%2520of%2520location%2520matching%252C%2520we%2520match%2520the%250Afeasible%2520object%2520locations%2520with%2520the%2520found%2520state%2520perturbations.%2520Our%2520evaluation%250Ausing%2520a%2520public%2520autonomous%2520driving%2520dataset%2520shows%2520that%2520our%2520attack%2520causes%2520a%250Acollision%2520rate%2520of%2520up%2520to%252063%2525%2520and%2520various%2520hazardous%2520responses%2520of%2520the%2520victim%2520AV.%250AThe%2520effectiveness%2520of%2520our%2520attack%2520is%2520also%2520demonstrated%2520on%2520a%2520real%2520testbed%2520car.%2520To%250Athe%2520best%2520of%2520our%2520knowledge%252C%2520this%2520study%2520is%2520the%2520first%2520security%2520analysis%2520spanning%250Afrom%2520LiDAR-based%2520perception%2520to%2520prediction%2520in%2520autonomous%2520driving%252C%2520leading%2520to%2520a%250Arealistic%2520attack%2520on%2520prediction.%2520To%2520counteract%2520the%2520proposed%2520attack%252C%2520potential%250Adefenses%2520are%2520discussed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11707v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20First%20Physical-World%20Trajectory%20Prediction%20Attack%20via%20LiDAR-induced%0A%20%20Deceptions%20in%20Autonomous%20Driving&entry.906535625=Yang%20Lou%20and%20Yi%20Zhu%20and%20Qun%20Song%20and%20Rui%20Tan%20and%20Chunming%20Qiao%20and%20Wei-Bin%20Lee%20and%20Jianping%20Wang&entry.1292438233=%20%20Trajectory%20prediction%20forecasts%20nearby%20agents%27%20moves%20based%20on%20their%0Ahistorical%20trajectories.%20Accurate%20trajectory%20prediction%20is%20crucial%20for%0Aautonomous%20vehicles.%20Existing%20attacks%20compromise%20the%20prediction%20model%20of%20a%0Avictim%20AV%20by%20directly%20manipulating%20the%20historical%20trajectory%20of%20an%20attacker%20AV%2C%0Awhich%20has%20limited%20real-world%20applicability.%20This%20paper%2C%20for%20the%20first%20time%2C%0Aexplores%20an%20indirect%20attack%20approach%20that%20induces%20prediction%20errors%20via%20attacks%0Aagainst%20the%20perception%20module%20of%20a%20victim%20AV.%20Although%20it%20has%20been%20shown%20that%0Aphysically%20realizable%20attacks%20against%20LiDAR-based%20perception%20are%20possible%20by%0Aplacing%20a%20few%20objects%20at%20strategic%20locations%2C%20it%20is%20still%20an%20open%20challenge%20to%0Afind%20an%20object%20location%20from%20the%20vast%20search%20space%20in%20order%20to%20launch%20effective%0Aattacks%20against%20prediction%20under%20varying%20victim%20AV%20velocities.%0A%20%20Through%20analysis%2C%20we%20observe%20that%20a%20prediction%20model%20is%20prone%20to%20an%20attack%0Afocusing%20on%20a%20single%20point%20in%20the%20scene.%20Consequently%2C%20we%20propose%20a%20novel%0Atwo-stage%20attack%20framework%20to%20realize%20the%20single-point%20attack.%20The%20first%20stage%0Aof%20prediction-side%20attack%20efficiently%20identifies%2C%20guided%20by%20the%20distribution%20of%0Adetection%20results%20under%20object-based%20attacks%20against%20perception%2C%20the%20state%0Aperturbations%20for%20the%20prediction%20model%20that%20are%20effective%20and%0Avelocity-insensitive.%20In%20the%20second%20stage%20of%20location%20matching%2C%20we%20match%20the%0Afeasible%20object%20locations%20with%20the%20found%20state%20perturbations.%20Our%20evaluation%0Ausing%20a%20public%20autonomous%20driving%20dataset%20shows%20that%20our%20attack%20causes%20a%0Acollision%20rate%20of%20up%20to%2063%25%20and%20various%20hazardous%20responses%20of%20the%20victim%20AV.%0AThe%20effectiveness%20of%20our%20attack%20is%20also%20demonstrated%20on%20a%20real%20testbed%20car.%20To%0Athe%20best%20of%20our%20knowledge%2C%20this%20study%20is%20the%20first%20security%20analysis%20spanning%0Afrom%20LiDAR-based%20perception%20to%20prediction%20in%20autonomous%20driving%2C%20leading%20to%20a%0Arealistic%20attack%20on%20prediction.%20To%20counteract%20the%20proposed%20attack%2C%20potential%0Adefenses%20are%20discussed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11707v1&entry.124074799=Read"},
{"title": "Mix-Domain Contrastive Learning for Unpaired H&E-to-IHC Stain\n  Translation", "author": "Song Wang and Zhong Zhang and Huan Yan and Ming Xu and Guanghui Wang", "abstract": "  H&E-to-IHC stain translation techniques offer a promising solution for\nprecise cancer diagnosis, especially in low-resource regions where there is a\nshortage of health professionals and limited access to expensive equipment.\nConsidering the pixel-level misalignment of H&E-IHC image pairs, current\nresearch explores the pathological consistency between patches from the same\npositions of the image pair. However, most of them overemphasize the\ncorrespondence between domains or patches, overlooking the side information\nprovided by the non-corresponding objects. In this paper, we propose a\nMix-Domain Contrastive Learning (MDCL) method to leverage the supervision\ninformation in unpaired H&E-to-IHC stain translation. Specifically, the\nproposed MDCL method aggregates the inter-domain and intra-domain pathology\ninformation by estimating the correlation between the anchor patch and all the\npatches from the matching images, encouraging the network to learn additional\ncontrastive knowledge from mixed domains. With the mix-domain pathology\ninformation aggregation, MDCL enhances the pathological consistency between the\ncorresponding patches and the component discrepancy of the patches from the\ndifferent positions of the generated IHC image. Extensive experiments on two\nH&E-to-IHC stain translation datasets, namely MIST and BCI, demonstrate that\nthe proposed method achieves state-of-the-art performance across multiple\nmetrics.\n", "link": "http://arxiv.org/abs/2406.11799v1", "date": "2024-06-17", "relevancy": 2.0659, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5246}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5131}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5048}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mix-Domain%20Contrastive%20Learning%20for%20Unpaired%20H%26E-to-IHC%20Stain%0A%20%20Translation&body=Title%3A%20Mix-Domain%20Contrastive%20Learning%20for%20Unpaired%20H%26E-to-IHC%20Stain%0A%20%20Translation%0AAuthor%3A%20Song%20Wang%20and%20Zhong%20Zhang%20and%20Huan%20Yan%20and%20Ming%20Xu%20and%20Guanghui%20Wang%0AAbstract%3A%20%20%20H%26E-to-IHC%20stain%20translation%20techniques%20offer%20a%20promising%20solution%20for%0Aprecise%20cancer%20diagnosis%2C%20especially%20in%20low-resource%20regions%20where%20there%20is%20a%0Ashortage%20of%20health%20professionals%20and%20limited%20access%20to%20expensive%20equipment.%0AConsidering%20the%20pixel-level%20misalignment%20of%20H%26E-IHC%20image%20pairs%2C%20current%0Aresearch%20explores%20the%20pathological%20consistency%20between%20patches%20from%20the%20same%0Apositions%20of%20the%20image%20pair.%20However%2C%20most%20of%20them%20overemphasize%20the%0Acorrespondence%20between%20domains%20or%20patches%2C%20overlooking%20the%20side%20information%0Aprovided%20by%20the%20non-corresponding%20objects.%20In%20this%20paper%2C%20we%20propose%20a%0AMix-Domain%20Contrastive%20Learning%20%28MDCL%29%20method%20to%20leverage%20the%20supervision%0Ainformation%20in%20unpaired%20H%26E-to-IHC%20stain%20translation.%20Specifically%2C%20the%0Aproposed%20MDCL%20method%20aggregates%20the%20inter-domain%20and%20intra-domain%20pathology%0Ainformation%20by%20estimating%20the%20correlation%20between%20the%20anchor%20patch%20and%20all%20the%0Apatches%20from%20the%20matching%20images%2C%20encouraging%20the%20network%20to%20learn%20additional%0Acontrastive%20knowledge%20from%20mixed%20domains.%20With%20the%20mix-domain%20pathology%0Ainformation%20aggregation%2C%20MDCL%20enhances%20the%20pathological%20consistency%20between%20the%0Acorresponding%20patches%20and%20the%20component%20discrepancy%20of%20the%20patches%20from%20the%0Adifferent%20positions%20of%20the%20generated%20IHC%20image.%20Extensive%20experiments%20on%20two%0AH%26E-to-IHC%20stain%20translation%20datasets%2C%20namely%20MIST%20and%20BCI%2C%20demonstrate%20that%0Athe%20proposed%20method%20achieves%20state-of-the-art%20performance%20across%20multiple%0Ametrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11799v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMix-Domain%2520Contrastive%2520Learning%2520for%2520Unpaired%2520H%2526E-to-IHC%2520Stain%250A%2520%2520Translation%26entry.906535625%3DSong%2520Wang%2520and%2520Zhong%2520Zhang%2520and%2520Huan%2520Yan%2520and%2520Ming%2520Xu%2520and%2520Guanghui%2520Wang%26entry.1292438233%3D%2520%2520H%2526E-to-IHC%2520stain%2520translation%2520techniques%2520offer%2520a%2520promising%2520solution%2520for%250Aprecise%2520cancer%2520diagnosis%252C%2520especially%2520in%2520low-resource%2520regions%2520where%2520there%2520is%2520a%250Ashortage%2520of%2520health%2520professionals%2520and%2520limited%2520access%2520to%2520expensive%2520equipment.%250AConsidering%2520the%2520pixel-level%2520misalignment%2520of%2520H%2526E-IHC%2520image%2520pairs%252C%2520current%250Aresearch%2520explores%2520the%2520pathological%2520consistency%2520between%2520patches%2520from%2520the%2520same%250Apositions%2520of%2520the%2520image%2520pair.%2520However%252C%2520most%2520of%2520them%2520overemphasize%2520the%250Acorrespondence%2520between%2520domains%2520or%2520patches%252C%2520overlooking%2520the%2520side%2520information%250Aprovided%2520by%2520the%2520non-corresponding%2520objects.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250AMix-Domain%2520Contrastive%2520Learning%2520%2528MDCL%2529%2520method%2520to%2520leverage%2520the%2520supervision%250Ainformation%2520in%2520unpaired%2520H%2526E-to-IHC%2520stain%2520translation.%2520Specifically%252C%2520the%250Aproposed%2520MDCL%2520method%2520aggregates%2520the%2520inter-domain%2520and%2520intra-domain%2520pathology%250Ainformation%2520by%2520estimating%2520the%2520correlation%2520between%2520the%2520anchor%2520patch%2520and%2520all%2520the%250Apatches%2520from%2520the%2520matching%2520images%252C%2520encouraging%2520the%2520network%2520to%2520learn%2520additional%250Acontrastive%2520knowledge%2520from%2520mixed%2520domains.%2520With%2520the%2520mix-domain%2520pathology%250Ainformation%2520aggregation%252C%2520MDCL%2520enhances%2520the%2520pathological%2520consistency%2520between%2520the%250Acorresponding%2520patches%2520and%2520the%2520component%2520discrepancy%2520of%2520the%2520patches%2520from%2520the%250Adifferent%2520positions%2520of%2520the%2520generated%2520IHC%2520image.%2520Extensive%2520experiments%2520on%2520two%250AH%2526E-to-IHC%2520stain%2520translation%2520datasets%252C%2520namely%2520MIST%2520and%2520BCI%252C%2520demonstrate%2520that%250Athe%2520proposed%2520method%2520achieves%2520state-of-the-art%2520performance%2520across%2520multiple%250Ametrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11799v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mix-Domain%20Contrastive%20Learning%20for%20Unpaired%20H%26E-to-IHC%20Stain%0A%20%20Translation&entry.906535625=Song%20Wang%20and%20Zhong%20Zhang%20and%20Huan%20Yan%20and%20Ming%20Xu%20and%20Guanghui%20Wang&entry.1292438233=%20%20H%26E-to-IHC%20stain%20translation%20techniques%20offer%20a%20promising%20solution%20for%0Aprecise%20cancer%20diagnosis%2C%20especially%20in%20low-resource%20regions%20where%20there%20is%20a%0Ashortage%20of%20health%20professionals%20and%20limited%20access%20to%20expensive%20equipment.%0AConsidering%20the%20pixel-level%20misalignment%20of%20H%26E-IHC%20image%20pairs%2C%20current%0Aresearch%20explores%20the%20pathological%20consistency%20between%20patches%20from%20the%20same%0Apositions%20of%20the%20image%20pair.%20However%2C%20most%20of%20them%20overemphasize%20the%0Acorrespondence%20between%20domains%20or%20patches%2C%20overlooking%20the%20side%20information%0Aprovided%20by%20the%20non-corresponding%20objects.%20In%20this%20paper%2C%20we%20propose%20a%0AMix-Domain%20Contrastive%20Learning%20%28MDCL%29%20method%20to%20leverage%20the%20supervision%0Ainformation%20in%20unpaired%20H%26E-to-IHC%20stain%20translation.%20Specifically%2C%20the%0Aproposed%20MDCL%20method%20aggregates%20the%20inter-domain%20and%20intra-domain%20pathology%0Ainformation%20by%20estimating%20the%20correlation%20between%20the%20anchor%20patch%20and%20all%20the%0Apatches%20from%20the%20matching%20images%2C%20encouraging%20the%20network%20to%20learn%20additional%0Acontrastive%20knowledge%20from%20mixed%20domains.%20With%20the%20mix-domain%20pathology%0Ainformation%20aggregation%2C%20MDCL%20enhances%20the%20pathological%20consistency%20between%20the%0Acorresponding%20patches%20and%20the%20component%20discrepancy%20of%20the%20patches%20from%20the%0Adifferent%20positions%20of%20the%20generated%20IHC%20image.%20Extensive%20experiments%20on%20two%0AH%26E-to-IHC%20stain%20translation%20datasets%2C%20namely%20MIST%20and%20BCI%2C%20demonstrate%20that%0Athe%20proposed%20method%20achieves%20state-of-the-art%20performance%20across%20multiple%0Ametrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11799v1&entry.124074799=Read"},
{"title": "Too Many Frames, not all Useful:Efficient Strategies for Long-Form Video\n  QA", "author": "Jongwoo Park and Kanchana Ranasinghe and Kumara Kahatapitiya and Wonjeong Ryoo and Donghyun Kim and Michael S. Ryoo", "abstract": "  Long-form videos that span across wide temporal intervals are highly\ninformation redundant and contain multiple distinct events or entities that are\noften loosely-related. Therefore, when performing long-form video question\nanswering (LVQA),all information necessary to generate a correct response can\noften be contained within a small subset of frames. Recent literature explore\nthe use of large language models (LLMs) in LVQA benchmarks, achieving\nexceptional performance, while relying on vision language models (VLMs) to\nconvert all visual content within videos into natural language. Such VLMs often\nindependently caption a large number of frames uniformly sampled from long\nvideos, which is not efficient and can mostly be redundant. Questioning these\ndecision choices, we explore optimal strategies for key-frame selection and\nsequence-aware captioning, that can significantly reduce these redundancies. We\npropose two novel approaches that improve each of aspects, namely Hierarchical\nKeyframe Selector and Sequential Visual LLM. Our resulting framework termed\nLVNet achieves state-of-the-art performance across three benchmark LVQA\ndatasets. Our code will be released publicly.\n", "link": "http://arxiv.org/abs/2406.09396v2", "date": "2024-06-17", "relevancy": 2.0631, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5354}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5141}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Too%20Many%20Frames%2C%20not%20all%20Useful%3AEfficient%20Strategies%20for%20Long-Form%20Video%0A%20%20QA&body=Title%3A%20Too%20Many%20Frames%2C%20not%20all%20Useful%3AEfficient%20Strategies%20for%20Long-Form%20Video%0A%20%20QA%0AAuthor%3A%20Jongwoo%20Park%20and%20Kanchana%20Ranasinghe%20and%20Kumara%20Kahatapitiya%20and%20Wonjeong%20Ryoo%20and%20Donghyun%20Kim%20and%20Michael%20S.%20Ryoo%0AAbstract%3A%20%20%20Long-form%20videos%20that%20span%20across%20wide%20temporal%20intervals%20are%20highly%0Ainformation%20redundant%20and%20contain%20multiple%20distinct%20events%20or%20entities%20that%20are%0Aoften%20loosely-related.%20Therefore%2C%20when%20performing%20long-form%20video%20question%0Aanswering%20%28LVQA%29%2Call%20information%20necessary%20to%20generate%20a%20correct%20response%20can%0Aoften%20be%20contained%20within%20a%20small%20subset%20of%20frames.%20Recent%20literature%20explore%0Athe%20use%20of%20large%20language%20models%20%28LLMs%29%20in%20LVQA%20benchmarks%2C%20achieving%0Aexceptional%20performance%2C%20while%20relying%20on%20vision%20language%20models%20%28VLMs%29%20to%0Aconvert%20all%20visual%20content%20within%20videos%20into%20natural%20language.%20Such%20VLMs%20often%0Aindependently%20caption%20a%20large%20number%20of%20frames%20uniformly%20sampled%20from%20long%0Avideos%2C%20which%20is%20not%20efficient%20and%20can%20mostly%20be%20redundant.%20Questioning%20these%0Adecision%20choices%2C%20we%20explore%20optimal%20strategies%20for%20key-frame%20selection%20and%0Asequence-aware%20captioning%2C%20that%20can%20significantly%20reduce%20these%20redundancies.%20We%0Apropose%20two%20novel%20approaches%20that%20improve%20each%20of%20aspects%2C%20namely%20Hierarchical%0AKeyframe%20Selector%20and%20Sequential%20Visual%20LLM.%20Our%20resulting%20framework%20termed%0ALVNet%20achieves%20state-of-the-art%20performance%20across%20three%20benchmark%20LVQA%0Adatasets.%20Our%20code%20will%20be%20released%20publicly.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09396v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToo%2520Many%2520Frames%252C%2520not%2520all%2520Useful%253AEfficient%2520Strategies%2520for%2520Long-Form%2520Video%250A%2520%2520QA%26entry.906535625%3DJongwoo%2520Park%2520and%2520Kanchana%2520Ranasinghe%2520and%2520Kumara%2520Kahatapitiya%2520and%2520Wonjeong%2520Ryoo%2520and%2520Donghyun%2520Kim%2520and%2520Michael%2520S.%2520Ryoo%26entry.1292438233%3D%2520%2520Long-form%2520videos%2520that%2520span%2520across%2520wide%2520temporal%2520intervals%2520are%2520highly%250Ainformation%2520redundant%2520and%2520contain%2520multiple%2520distinct%2520events%2520or%2520entities%2520that%2520are%250Aoften%2520loosely-related.%2520Therefore%252C%2520when%2520performing%2520long-form%2520video%2520question%250Aanswering%2520%2528LVQA%2529%252Call%2520information%2520necessary%2520to%2520generate%2520a%2520correct%2520response%2520can%250Aoften%2520be%2520contained%2520within%2520a%2520small%2520subset%2520of%2520frames.%2520Recent%2520literature%2520explore%250Athe%2520use%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520in%2520LVQA%2520benchmarks%252C%2520achieving%250Aexceptional%2520performance%252C%2520while%2520relying%2520on%2520vision%2520language%2520models%2520%2528VLMs%2529%2520to%250Aconvert%2520all%2520visual%2520content%2520within%2520videos%2520into%2520natural%2520language.%2520Such%2520VLMs%2520often%250Aindependently%2520caption%2520a%2520large%2520number%2520of%2520frames%2520uniformly%2520sampled%2520from%2520long%250Avideos%252C%2520which%2520is%2520not%2520efficient%2520and%2520can%2520mostly%2520be%2520redundant.%2520Questioning%2520these%250Adecision%2520choices%252C%2520we%2520explore%2520optimal%2520strategies%2520for%2520key-frame%2520selection%2520and%250Asequence-aware%2520captioning%252C%2520that%2520can%2520significantly%2520reduce%2520these%2520redundancies.%2520We%250Apropose%2520two%2520novel%2520approaches%2520that%2520improve%2520each%2520of%2520aspects%252C%2520namely%2520Hierarchical%250AKeyframe%2520Selector%2520and%2520Sequential%2520Visual%2520LLM.%2520Our%2520resulting%2520framework%2520termed%250ALVNet%2520achieves%2520state-of-the-art%2520performance%2520across%2520three%2520benchmark%2520LVQA%250Adatasets.%2520Our%2520code%2520will%2520be%2520released%2520publicly.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09396v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Too%20Many%20Frames%2C%20not%20all%20Useful%3AEfficient%20Strategies%20for%20Long-Form%20Video%0A%20%20QA&entry.906535625=Jongwoo%20Park%20and%20Kanchana%20Ranasinghe%20and%20Kumara%20Kahatapitiya%20and%20Wonjeong%20Ryoo%20and%20Donghyun%20Kim%20and%20Michael%20S.%20Ryoo&entry.1292438233=%20%20Long-form%20videos%20that%20span%20across%20wide%20temporal%20intervals%20are%20highly%0Ainformation%20redundant%20and%20contain%20multiple%20distinct%20events%20or%20entities%20that%20are%0Aoften%20loosely-related.%20Therefore%2C%20when%20performing%20long-form%20video%20question%0Aanswering%20%28LVQA%29%2Call%20information%20necessary%20to%20generate%20a%20correct%20response%20can%0Aoften%20be%20contained%20within%20a%20small%20subset%20of%20frames.%20Recent%20literature%20explore%0Athe%20use%20of%20large%20language%20models%20%28LLMs%29%20in%20LVQA%20benchmarks%2C%20achieving%0Aexceptional%20performance%2C%20while%20relying%20on%20vision%20language%20models%20%28VLMs%29%20to%0Aconvert%20all%20visual%20content%20within%20videos%20into%20natural%20language.%20Such%20VLMs%20often%0Aindependently%20caption%20a%20large%20number%20of%20frames%20uniformly%20sampled%20from%20long%0Avideos%2C%20which%20is%20not%20efficient%20and%20can%20mostly%20be%20redundant.%20Questioning%20these%0Adecision%20choices%2C%20we%20explore%20optimal%20strategies%20for%20key-frame%20selection%20and%0Asequence-aware%20captioning%2C%20that%20can%20significantly%20reduce%20these%20redundancies.%20We%0Apropose%20two%20novel%20approaches%20that%20improve%20each%20of%20aspects%2C%20namely%20Hierarchical%0AKeyframe%20Selector%20and%20Sequential%20Visual%20LLM.%20Our%20resulting%20framework%20termed%0ALVNet%20achieves%20state-of-the-art%20performance%20across%20three%20benchmark%20LVQA%0Adatasets.%20Our%20code%20will%20be%20released%20publicly.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09396v2&entry.124074799=Read"},
{"title": "Scaling the Codebook Size of VQGAN to 100,000 with a Utilization Rate of\n  99%", "author": "Lei Zhu and Fangyun Wei and Yanye Lu and Dong Chen", "abstract": "  In the realm of image quantization exemplified by VQGAN, the process encodes\nimages into discrete tokens drawn from a codebook with a predefined size.\nRecent advancements, particularly with LLAMA 3, reveal that enlarging the\ncodebook significantly enhances model performance. However, VQGAN and its\nderivatives, such as VQGAN-FC (Factorized Codes) and VQGAN-EMA, continue to\ngrapple with challenges related to expanding the codebook size and enhancing\ncodebook utilization. For instance, VQGAN-FC is restricted to learning a\ncodebook with a maximum size of 16,384, maintaining a typically low utilization\nrate of less than 12% on ImageNet. In this work, we propose a novel image\nquantization model named VQGAN-LC (Large Codebook), which extends the codebook\nsize to 100,000, achieving an utilization rate exceeding 99%. Unlike previous\nmethods that optimize each codebook entry, our approach begins with a codebook\ninitialized with 100,000 features extracted by a pre-trained vision encoder.\nOptimization then focuses on training a projector that aligns the entire\ncodebook with the feature distributions of the encoder in VQGAN-LC. We\ndemonstrate the superior performance of our model over its counterparts across\na variety of tasks, including image reconstruction, image classification,\nauto-regressive image generation using GPT, and image creation with diffusion-\nand flow-based generative models. Code and models are available at\nhttps://github.com/zh460045050/VQGAN-LC.\n", "link": "http://arxiv.org/abs/2406.11837v1", "date": "2024-06-17", "relevancy": 2.0602, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5227}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5148}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scaling%20the%20Codebook%20Size%20of%20VQGAN%20to%20100%2C000%20with%20a%20Utilization%20Rate%20of%0A%20%2099%25&body=Title%3A%20Scaling%20the%20Codebook%20Size%20of%20VQGAN%20to%20100%2C000%20with%20a%20Utilization%20Rate%20of%0A%20%2099%25%0AAuthor%3A%20Lei%20Zhu%20and%20Fangyun%20Wei%20and%20Yanye%20Lu%20and%20Dong%20Chen%0AAbstract%3A%20%20%20In%20the%20realm%20of%20image%20quantization%20exemplified%20by%20VQGAN%2C%20the%20process%20encodes%0Aimages%20into%20discrete%20tokens%20drawn%20from%20a%20codebook%20with%20a%20predefined%20size.%0ARecent%20advancements%2C%20particularly%20with%20LLAMA%203%2C%20reveal%20that%20enlarging%20the%0Acodebook%20significantly%20enhances%20model%20performance.%20However%2C%20VQGAN%20and%20its%0Aderivatives%2C%20such%20as%20VQGAN-FC%20%28Factorized%20Codes%29%20and%20VQGAN-EMA%2C%20continue%20to%0Agrapple%20with%20challenges%20related%20to%20expanding%20the%20codebook%20size%20and%20enhancing%0Acodebook%20utilization.%20For%20instance%2C%20VQGAN-FC%20is%20restricted%20to%20learning%20a%0Acodebook%20with%20a%20maximum%20size%20of%2016%2C384%2C%20maintaining%20a%20typically%20low%20utilization%0Arate%20of%20less%20than%2012%25%20on%20ImageNet.%20In%20this%20work%2C%20we%20propose%20a%20novel%20image%0Aquantization%20model%20named%20VQGAN-LC%20%28Large%20Codebook%29%2C%20which%20extends%20the%20codebook%0Asize%20to%20100%2C000%2C%20achieving%20an%20utilization%20rate%20exceeding%2099%25.%20Unlike%20previous%0Amethods%20that%20optimize%20each%20codebook%20entry%2C%20our%20approach%20begins%20with%20a%20codebook%0Ainitialized%20with%20100%2C000%20features%20extracted%20by%20a%20pre-trained%20vision%20encoder.%0AOptimization%20then%20focuses%20on%20training%20a%20projector%20that%20aligns%20the%20entire%0Acodebook%20with%20the%20feature%20distributions%20of%20the%20encoder%20in%20VQGAN-LC.%20We%0Ademonstrate%20the%20superior%20performance%20of%20our%20model%20over%20its%20counterparts%20across%0Aa%20variety%20of%20tasks%2C%20including%20image%20reconstruction%2C%20image%20classification%2C%0Aauto-regressive%20image%20generation%20using%20GPT%2C%20and%20image%20creation%20with%20diffusion-%0Aand%20flow-based%20generative%20models.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/zh460045050/VQGAN-LC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11837v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaling%2520the%2520Codebook%2520Size%2520of%2520VQGAN%2520to%2520100%252C000%2520with%2520a%2520Utilization%2520Rate%2520of%250A%2520%252099%2525%26entry.906535625%3DLei%2520Zhu%2520and%2520Fangyun%2520Wei%2520and%2520Yanye%2520Lu%2520and%2520Dong%2520Chen%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520image%2520quantization%2520exemplified%2520by%2520VQGAN%252C%2520the%2520process%2520encodes%250Aimages%2520into%2520discrete%2520tokens%2520drawn%2520from%2520a%2520codebook%2520with%2520a%2520predefined%2520size.%250ARecent%2520advancements%252C%2520particularly%2520with%2520LLAMA%25203%252C%2520reveal%2520that%2520enlarging%2520the%250Acodebook%2520significantly%2520enhances%2520model%2520performance.%2520However%252C%2520VQGAN%2520and%2520its%250Aderivatives%252C%2520such%2520as%2520VQGAN-FC%2520%2528Factorized%2520Codes%2529%2520and%2520VQGAN-EMA%252C%2520continue%2520to%250Agrapple%2520with%2520challenges%2520related%2520to%2520expanding%2520the%2520codebook%2520size%2520and%2520enhancing%250Acodebook%2520utilization.%2520For%2520instance%252C%2520VQGAN-FC%2520is%2520restricted%2520to%2520learning%2520a%250Acodebook%2520with%2520a%2520maximum%2520size%2520of%252016%252C384%252C%2520maintaining%2520a%2520typically%2520low%2520utilization%250Arate%2520of%2520less%2520than%252012%2525%2520on%2520ImageNet.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520image%250Aquantization%2520model%2520named%2520VQGAN-LC%2520%2528Large%2520Codebook%2529%252C%2520which%2520extends%2520the%2520codebook%250Asize%2520to%2520100%252C000%252C%2520achieving%2520an%2520utilization%2520rate%2520exceeding%252099%2525.%2520Unlike%2520previous%250Amethods%2520that%2520optimize%2520each%2520codebook%2520entry%252C%2520our%2520approach%2520begins%2520with%2520a%2520codebook%250Ainitialized%2520with%2520100%252C000%2520features%2520extracted%2520by%2520a%2520pre-trained%2520vision%2520encoder.%250AOptimization%2520then%2520focuses%2520on%2520training%2520a%2520projector%2520that%2520aligns%2520the%2520entire%250Acodebook%2520with%2520the%2520feature%2520distributions%2520of%2520the%2520encoder%2520in%2520VQGAN-LC.%2520We%250Ademonstrate%2520the%2520superior%2520performance%2520of%2520our%2520model%2520over%2520its%2520counterparts%2520across%250Aa%2520variety%2520of%2520tasks%252C%2520including%2520image%2520reconstruction%252C%2520image%2520classification%252C%250Aauto-regressive%2520image%2520generation%2520using%2520GPT%252C%2520and%2520image%2520creation%2520with%2520diffusion-%250Aand%2520flow-based%2520generative%2520models.%2520Code%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/zh460045050/VQGAN-LC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11837v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scaling%20the%20Codebook%20Size%20of%20VQGAN%20to%20100%2C000%20with%20a%20Utilization%20Rate%20of%0A%20%2099%25&entry.906535625=Lei%20Zhu%20and%20Fangyun%20Wei%20and%20Yanye%20Lu%20and%20Dong%20Chen&entry.1292438233=%20%20In%20the%20realm%20of%20image%20quantization%20exemplified%20by%20VQGAN%2C%20the%20process%20encodes%0Aimages%20into%20discrete%20tokens%20drawn%20from%20a%20codebook%20with%20a%20predefined%20size.%0ARecent%20advancements%2C%20particularly%20with%20LLAMA%203%2C%20reveal%20that%20enlarging%20the%0Acodebook%20significantly%20enhances%20model%20performance.%20However%2C%20VQGAN%20and%20its%0Aderivatives%2C%20such%20as%20VQGAN-FC%20%28Factorized%20Codes%29%20and%20VQGAN-EMA%2C%20continue%20to%0Agrapple%20with%20challenges%20related%20to%20expanding%20the%20codebook%20size%20and%20enhancing%0Acodebook%20utilization.%20For%20instance%2C%20VQGAN-FC%20is%20restricted%20to%20learning%20a%0Acodebook%20with%20a%20maximum%20size%20of%2016%2C384%2C%20maintaining%20a%20typically%20low%20utilization%0Arate%20of%20less%20than%2012%25%20on%20ImageNet.%20In%20this%20work%2C%20we%20propose%20a%20novel%20image%0Aquantization%20model%20named%20VQGAN-LC%20%28Large%20Codebook%29%2C%20which%20extends%20the%20codebook%0Asize%20to%20100%2C000%2C%20achieving%20an%20utilization%20rate%20exceeding%2099%25.%20Unlike%20previous%0Amethods%20that%20optimize%20each%20codebook%20entry%2C%20our%20approach%20begins%20with%20a%20codebook%0Ainitialized%20with%20100%2C000%20features%20extracted%20by%20a%20pre-trained%20vision%20encoder.%0AOptimization%20then%20focuses%20on%20training%20a%20projector%20that%20aligns%20the%20entire%0Acodebook%20with%20the%20feature%20distributions%20of%20the%20encoder%20in%20VQGAN-LC.%20We%0Ademonstrate%20the%20superior%20performance%20of%20our%20model%20over%20its%20counterparts%20across%0Aa%20variety%20of%20tasks%2C%20including%20image%20reconstruction%2C%20image%20classification%2C%0Aauto-regressive%20image%20generation%20using%20GPT%2C%20and%20image%20creation%20with%20diffusion-%0Aand%20flow-based%20generative%20models.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/zh460045050/VQGAN-LC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11837v1&entry.124074799=Read"},
{"title": "ArtWhisperer: A Dataset for Characterizing Human-AI Interactions in\n  Artistic Creations", "author": "Kailas Vodrahalli and James Zou", "abstract": "  As generative AI becomes more prevalent, it is important to study how human\nusers interact with such models. In this work, we investigate how people use\ntext-to-image models to generate desired target images. To study this\ninteraction, we created ArtWhisperer, an online game where users are given a\ntarget image and are tasked with iteratively finding a prompt that creates a\nsimilar-looking image as the target. Through this game, we recorded over 50,000\nhuman-AI interactions; each interaction corresponds to one text prompt created\nby a user and the corresponding generated image. The majority of these are\nrepeated interactions where a user iterates to find the best prompt for their\ntarget image, making this a unique sequential dataset for studying human-AI\ncollaborations. In an initial analysis of this dataset, we identify several\ncharacteristics of prompt interactions and user strategies. People submit\ndiverse prompts and are able to discover a variety of text descriptions that\ngenerate similar images. Interestingly, prompt diversity does not decrease as\nusers find better prompts. We further propose a new metric to quantify the\nsteerability of AI using our dataset. We define steerability as the expected\nnumber of interactions required to adequately complete a task. We estimate this\nvalue by fitting a Markov chain for each target task and calculating the\nexpected time to reach an adequate score in the Markov chain. We quantify and\ncompare AI steerability across different types of target images and two\ndifferent models, finding that images of cities and natural world images are\nmore steerable than artistic and fantasy images. These findings provide\ninsights into human-AI interaction behavior, present a concrete method of\nassessing AI steerability, and demonstrate the general utility of the\nArtWhisperer dataset.\n", "link": "http://arxiv.org/abs/2306.08141v4", "date": "2024-06-17", "relevancy": 2.0573, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5173}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5149}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ArtWhisperer%3A%20A%20Dataset%20for%20Characterizing%20Human-AI%20Interactions%20in%0A%20%20Artistic%20Creations&body=Title%3A%20ArtWhisperer%3A%20A%20Dataset%20for%20Characterizing%20Human-AI%20Interactions%20in%0A%20%20Artistic%20Creations%0AAuthor%3A%20Kailas%20Vodrahalli%20and%20James%20Zou%0AAbstract%3A%20%20%20As%20generative%20AI%20becomes%20more%20prevalent%2C%20it%20is%20important%20to%20study%20how%20human%0Ausers%20interact%20with%20such%20models.%20In%20this%20work%2C%20we%20investigate%20how%20people%20use%0Atext-to-image%20models%20to%20generate%20desired%20target%20images.%20To%20study%20this%0Ainteraction%2C%20we%20created%20ArtWhisperer%2C%20an%20online%20game%20where%20users%20are%20given%20a%0Atarget%20image%20and%20are%20tasked%20with%20iteratively%20finding%20a%20prompt%20that%20creates%20a%0Asimilar-looking%20image%20as%20the%20target.%20Through%20this%20game%2C%20we%20recorded%20over%2050%2C000%0Ahuman-AI%20interactions%3B%20each%20interaction%20corresponds%20to%20one%20text%20prompt%20created%0Aby%20a%20user%20and%20the%20corresponding%20generated%20image.%20The%20majority%20of%20these%20are%0Arepeated%20interactions%20where%20a%20user%20iterates%20to%20find%20the%20best%20prompt%20for%20their%0Atarget%20image%2C%20making%20this%20a%20unique%20sequential%20dataset%20for%20studying%20human-AI%0Acollaborations.%20In%20an%20initial%20analysis%20of%20this%20dataset%2C%20we%20identify%20several%0Acharacteristics%20of%20prompt%20interactions%20and%20user%20strategies.%20People%20submit%0Adiverse%20prompts%20and%20are%20able%20to%20discover%20a%20variety%20of%20text%20descriptions%20that%0Agenerate%20similar%20images.%20Interestingly%2C%20prompt%20diversity%20does%20not%20decrease%20as%0Ausers%20find%20better%20prompts.%20We%20further%20propose%20a%20new%20metric%20to%20quantify%20the%0Asteerability%20of%20AI%20using%20our%20dataset.%20We%20define%20steerability%20as%20the%20expected%0Anumber%20of%20interactions%20required%20to%20adequately%20complete%20a%20task.%20We%20estimate%20this%0Avalue%20by%20fitting%20a%20Markov%20chain%20for%20each%20target%20task%20and%20calculating%20the%0Aexpected%20time%20to%20reach%20an%20adequate%20score%20in%20the%20Markov%20chain.%20We%20quantify%20and%0Acompare%20AI%20steerability%20across%20different%20types%20of%20target%20images%20and%20two%0Adifferent%20models%2C%20finding%20that%20images%20of%20cities%20and%20natural%20world%20images%20are%0Amore%20steerable%20than%20artistic%20and%20fantasy%20images.%20These%20findings%20provide%0Ainsights%20into%20human-AI%20interaction%20behavior%2C%20present%20a%20concrete%20method%20of%0Aassessing%20AI%20steerability%2C%20and%20demonstrate%20the%20general%20utility%20of%20the%0AArtWhisperer%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.08141v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtWhisperer%253A%2520A%2520Dataset%2520for%2520Characterizing%2520Human-AI%2520Interactions%2520in%250A%2520%2520Artistic%2520Creations%26entry.906535625%3DKailas%2520Vodrahalli%2520and%2520James%2520Zou%26entry.1292438233%3D%2520%2520As%2520generative%2520AI%2520becomes%2520more%2520prevalent%252C%2520it%2520is%2520important%2520to%2520study%2520how%2520human%250Ausers%2520interact%2520with%2520such%2520models.%2520In%2520this%2520work%252C%2520we%2520investigate%2520how%2520people%2520use%250Atext-to-image%2520models%2520to%2520generate%2520desired%2520target%2520images.%2520To%2520study%2520this%250Ainteraction%252C%2520we%2520created%2520ArtWhisperer%252C%2520an%2520online%2520game%2520where%2520users%2520are%2520given%2520a%250Atarget%2520image%2520and%2520are%2520tasked%2520with%2520iteratively%2520finding%2520a%2520prompt%2520that%2520creates%2520a%250Asimilar-looking%2520image%2520as%2520the%2520target.%2520Through%2520this%2520game%252C%2520we%2520recorded%2520over%252050%252C000%250Ahuman-AI%2520interactions%253B%2520each%2520interaction%2520corresponds%2520to%2520one%2520text%2520prompt%2520created%250Aby%2520a%2520user%2520and%2520the%2520corresponding%2520generated%2520image.%2520The%2520majority%2520of%2520these%2520are%250Arepeated%2520interactions%2520where%2520a%2520user%2520iterates%2520to%2520find%2520the%2520best%2520prompt%2520for%2520their%250Atarget%2520image%252C%2520making%2520this%2520a%2520unique%2520sequential%2520dataset%2520for%2520studying%2520human-AI%250Acollaborations.%2520In%2520an%2520initial%2520analysis%2520of%2520this%2520dataset%252C%2520we%2520identify%2520several%250Acharacteristics%2520of%2520prompt%2520interactions%2520and%2520user%2520strategies.%2520People%2520submit%250Adiverse%2520prompts%2520and%2520are%2520able%2520to%2520discover%2520a%2520variety%2520of%2520text%2520descriptions%2520that%250Agenerate%2520similar%2520images.%2520Interestingly%252C%2520prompt%2520diversity%2520does%2520not%2520decrease%2520as%250Ausers%2520find%2520better%2520prompts.%2520We%2520further%2520propose%2520a%2520new%2520metric%2520to%2520quantify%2520the%250Asteerability%2520of%2520AI%2520using%2520our%2520dataset.%2520We%2520define%2520steerability%2520as%2520the%2520expected%250Anumber%2520of%2520interactions%2520required%2520to%2520adequately%2520complete%2520a%2520task.%2520We%2520estimate%2520this%250Avalue%2520by%2520fitting%2520a%2520Markov%2520chain%2520for%2520each%2520target%2520task%2520and%2520calculating%2520the%250Aexpected%2520time%2520to%2520reach%2520an%2520adequate%2520score%2520in%2520the%2520Markov%2520chain.%2520We%2520quantify%2520and%250Acompare%2520AI%2520steerability%2520across%2520different%2520types%2520of%2520target%2520images%2520and%2520two%250Adifferent%2520models%252C%2520finding%2520that%2520images%2520of%2520cities%2520and%2520natural%2520world%2520images%2520are%250Amore%2520steerable%2520than%2520artistic%2520and%2520fantasy%2520images.%2520These%2520findings%2520provide%250Ainsights%2520into%2520human-AI%2520interaction%2520behavior%252C%2520present%2520a%2520concrete%2520method%2520of%250Aassessing%2520AI%2520steerability%252C%2520and%2520demonstrate%2520the%2520general%2520utility%2520of%2520the%250AArtWhisperer%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.08141v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ArtWhisperer%3A%20A%20Dataset%20for%20Characterizing%20Human-AI%20Interactions%20in%0A%20%20Artistic%20Creations&entry.906535625=Kailas%20Vodrahalli%20and%20James%20Zou&entry.1292438233=%20%20As%20generative%20AI%20becomes%20more%20prevalent%2C%20it%20is%20important%20to%20study%20how%20human%0Ausers%20interact%20with%20such%20models.%20In%20this%20work%2C%20we%20investigate%20how%20people%20use%0Atext-to-image%20models%20to%20generate%20desired%20target%20images.%20To%20study%20this%0Ainteraction%2C%20we%20created%20ArtWhisperer%2C%20an%20online%20game%20where%20users%20are%20given%20a%0Atarget%20image%20and%20are%20tasked%20with%20iteratively%20finding%20a%20prompt%20that%20creates%20a%0Asimilar-looking%20image%20as%20the%20target.%20Through%20this%20game%2C%20we%20recorded%20over%2050%2C000%0Ahuman-AI%20interactions%3B%20each%20interaction%20corresponds%20to%20one%20text%20prompt%20created%0Aby%20a%20user%20and%20the%20corresponding%20generated%20image.%20The%20majority%20of%20these%20are%0Arepeated%20interactions%20where%20a%20user%20iterates%20to%20find%20the%20best%20prompt%20for%20their%0Atarget%20image%2C%20making%20this%20a%20unique%20sequential%20dataset%20for%20studying%20human-AI%0Acollaborations.%20In%20an%20initial%20analysis%20of%20this%20dataset%2C%20we%20identify%20several%0Acharacteristics%20of%20prompt%20interactions%20and%20user%20strategies.%20People%20submit%0Adiverse%20prompts%20and%20are%20able%20to%20discover%20a%20variety%20of%20text%20descriptions%20that%0Agenerate%20similar%20images.%20Interestingly%2C%20prompt%20diversity%20does%20not%20decrease%20as%0Ausers%20find%20better%20prompts.%20We%20further%20propose%20a%20new%20metric%20to%20quantify%20the%0Asteerability%20of%20AI%20using%20our%20dataset.%20We%20define%20steerability%20as%20the%20expected%0Anumber%20of%20interactions%20required%20to%20adequately%20complete%20a%20task.%20We%20estimate%20this%0Avalue%20by%20fitting%20a%20Markov%20chain%20for%20each%20target%20task%20and%20calculating%20the%0Aexpected%20time%20to%20reach%20an%20adequate%20score%20in%20the%20Markov%20chain.%20We%20quantify%20and%0Acompare%20AI%20steerability%20across%20different%20types%20of%20target%20images%20and%20two%0Adifferent%20models%2C%20finding%20that%20images%20of%20cities%20and%20natural%20world%20images%20are%0Amore%20steerable%20than%20artistic%20and%20fantasy%20images.%20These%20findings%20provide%0Ainsights%20into%20human-AI%20interaction%20behavior%2C%20present%20a%20concrete%20method%20of%0Aassessing%20AI%20steerability%2C%20and%20demonstrate%20the%20general%20utility%20of%20the%0AArtWhisperer%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.08141v4&entry.124074799=Read"},
{"title": "Quaternion Generative Adversarial Neural Networks and Applications to\n  Color Image Inpainting", "author": "Duan Wang and Dandan Zhu and Meixiang Zhao and Zhigang Jia", "abstract": "  Color image inpainting is a challenging task in imaging science. The existing\nmethod is based on real operation, and the red, green and blue channels of the\ncolor image are processed separately, ignoring the correlation between each\nchannel. In order to make full use of the correlation between each channel,\nthis paper proposes a Quaternion Generative Adversarial Neural Network (QGAN)\nmodel and related theory, and applies it to solve the problem of color image\ninpainting with large area missing. Firstly, the definition of quaternion\ndeconvolution is given and the quaternion batch normalization is proposed.\nSecondly, the above two innovative modules are applied to generate adversarial\nnetworks to improve stability. Finally, QGAN is applied to color image\ninpainting and compared with other state-of-the-art algorithms. The\nexperimental results show that QGAN has superiority in color image inpainting\nwith large area missing.\n", "link": "http://arxiv.org/abs/2406.11567v1", "date": "2024-06-17", "relevancy": 2.0564, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5266}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5121}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quaternion%20Generative%20Adversarial%20Neural%20Networks%20and%20Applications%20to%0A%20%20Color%20Image%20Inpainting&body=Title%3A%20Quaternion%20Generative%20Adversarial%20Neural%20Networks%20and%20Applications%20to%0A%20%20Color%20Image%20Inpainting%0AAuthor%3A%20Duan%20Wang%20and%20Dandan%20Zhu%20and%20Meixiang%20Zhao%20and%20Zhigang%20Jia%0AAbstract%3A%20%20%20Color%20image%20inpainting%20is%20a%20challenging%20task%20in%20imaging%20science.%20The%20existing%0Amethod%20is%20based%20on%20real%20operation%2C%20and%20the%20red%2C%20green%20and%20blue%20channels%20of%20the%0Acolor%20image%20are%20processed%20separately%2C%20ignoring%20the%20correlation%20between%20each%0Achannel.%20In%20order%20to%20make%20full%20use%20of%20the%20correlation%20between%20each%20channel%2C%0Athis%20paper%20proposes%20a%20Quaternion%20Generative%20Adversarial%20Neural%20Network%20%28QGAN%29%0Amodel%20and%20related%20theory%2C%20and%20applies%20it%20to%20solve%20the%20problem%20of%20color%20image%0Ainpainting%20with%20large%20area%20missing.%20Firstly%2C%20the%20definition%20of%20quaternion%0Adeconvolution%20is%20given%20and%20the%20quaternion%20batch%20normalization%20is%20proposed.%0ASecondly%2C%20the%20above%20two%20innovative%20modules%20are%20applied%20to%20generate%20adversarial%0Anetworks%20to%20improve%20stability.%20Finally%2C%20QGAN%20is%20applied%20to%20color%20image%0Ainpainting%20and%20compared%20with%20other%20state-of-the-art%20algorithms.%20The%0Aexperimental%20results%20show%20that%20QGAN%20has%20superiority%20in%20color%20image%20inpainting%0Awith%20large%20area%20missing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11567v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuaternion%2520Generative%2520Adversarial%2520Neural%2520Networks%2520and%2520Applications%2520to%250A%2520%2520Color%2520Image%2520Inpainting%26entry.906535625%3DDuan%2520Wang%2520and%2520Dandan%2520Zhu%2520and%2520Meixiang%2520Zhao%2520and%2520Zhigang%2520Jia%26entry.1292438233%3D%2520%2520Color%2520image%2520inpainting%2520is%2520a%2520challenging%2520task%2520in%2520imaging%2520science.%2520The%2520existing%250Amethod%2520is%2520based%2520on%2520real%2520operation%252C%2520and%2520the%2520red%252C%2520green%2520and%2520blue%2520channels%2520of%2520the%250Acolor%2520image%2520are%2520processed%2520separately%252C%2520ignoring%2520the%2520correlation%2520between%2520each%250Achannel.%2520In%2520order%2520to%2520make%2520full%2520use%2520of%2520the%2520correlation%2520between%2520each%2520channel%252C%250Athis%2520paper%2520proposes%2520a%2520Quaternion%2520Generative%2520Adversarial%2520Neural%2520Network%2520%2528QGAN%2529%250Amodel%2520and%2520related%2520theory%252C%2520and%2520applies%2520it%2520to%2520solve%2520the%2520problem%2520of%2520color%2520image%250Ainpainting%2520with%2520large%2520area%2520missing.%2520Firstly%252C%2520the%2520definition%2520of%2520quaternion%250Adeconvolution%2520is%2520given%2520and%2520the%2520quaternion%2520batch%2520normalization%2520is%2520proposed.%250ASecondly%252C%2520the%2520above%2520two%2520innovative%2520modules%2520are%2520applied%2520to%2520generate%2520adversarial%250Anetworks%2520to%2520improve%2520stability.%2520Finally%252C%2520QGAN%2520is%2520applied%2520to%2520color%2520image%250Ainpainting%2520and%2520compared%2520with%2520other%2520state-of-the-art%2520algorithms.%2520The%250Aexperimental%2520results%2520show%2520that%2520QGAN%2520has%2520superiority%2520in%2520color%2520image%2520inpainting%250Awith%2520large%2520area%2520missing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11567v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quaternion%20Generative%20Adversarial%20Neural%20Networks%20and%20Applications%20to%0A%20%20Color%20Image%20Inpainting&entry.906535625=Duan%20Wang%20and%20Dandan%20Zhu%20and%20Meixiang%20Zhao%20and%20Zhigang%20Jia&entry.1292438233=%20%20Color%20image%20inpainting%20is%20a%20challenging%20task%20in%20imaging%20science.%20The%20existing%0Amethod%20is%20based%20on%20real%20operation%2C%20and%20the%20red%2C%20green%20and%20blue%20channels%20of%20the%0Acolor%20image%20are%20processed%20separately%2C%20ignoring%20the%20correlation%20between%20each%0Achannel.%20In%20order%20to%20make%20full%20use%20of%20the%20correlation%20between%20each%20channel%2C%0Athis%20paper%20proposes%20a%20Quaternion%20Generative%20Adversarial%20Neural%20Network%20%28QGAN%29%0Amodel%20and%20related%20theory%2C%20and%20applies%20it%20to%20solve%20the%20problem%20of%20color%20image%0Ainpainting%20with%20large%20area%20missing.%20Firstly%2C%20the%20definition%20of%20quaternion%0Adeconvolution%20is%20given%20and%20the%20quaternion%20batch%20normalization%20is%20proposed.%0ASecondly%2C%20the%20above%20two%20innovative%20modules%20are%20applied%20to%20generate%20adversarial%0Anetworks%20to%20improve%20stability.%20Finally%2C%20QGAN%20is%20applied%20to%20color%20image%0Ainpainting%20and%20compared%20with%20other%20state-of-the-art%20algorithms.%20The%0Aexperimental%20results%20show%20that%20QGAN%20has%20superiority%20in%20color%20image%20inpainting%0Awith%20large%20area%20missing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11567v1&entry.124074799=Read"},
{"title": "BLoB: Bayesian Low-Rank Adaptation by Backpropagation for Large Language\n  Models", "author": "Yibin Wang and Haizhou Shi and Ligong Han and Dimitris Metaxas and Hao Wang", "abstract": "  Large Language Models (LLMs) often suffer from overconfidence during\ninference, particularly when adapted to downstream domain-specific tasks with\nlimited data. Previous work addresses this issue by employing approximate\nBayesian estimation after the LLMs are trained, enabling them to quantify\nuncertainty. However, such post-training approaches' performance is severely\nlimited by the parameters learned during training. In this paper, we go beyond\npost-training Bayesianization and propose Bayesian Low-Rank Adaptation by\nBackpropagation (BLoB), an algorithm that continuously and jointly adjusts both\nthe mean and covariance of LLM parameters throughout the whole fine-tuning\nprocess. Our empirical results verify the effectiveness of BLoB in terms of\ngeneralization and uncertainty estimation, when evaluated on both\nin-distribution and out-of-distribution data.\n", "link": "http://arxiv.org/abs/2406.11675v1", "date": "2024-06-17", "relevancy": 2.0544, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5307}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.511}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4975}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BLoB%3A%20Bayesian%20Low-Rank%20Adaptation%20by%20Backpropagation%20for%20Large%20Language%0A%20%20Models&body=Title%3A%20BLoB%3A%20Bayesian%20Low-Rank%20Adaptation%20by%20Backpropagation%20for%20Large%20Language%0A%20%20Models%0AAuthor%3A%20Yibin%20Wang%20and%20Haizhou%20Shi%20and%20Ligong%20Han%20and%20Dimitris%20Metaxas%20and%20Hao%20Wang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20often%20suffer%20from%20overconfidence%20during%0Ainference%2C%20particularly%20when%20adapted%20to%20downstream%20domain-specific%20tasks%20with%0Alimited%20data.%20Previous%20work%20addresses%20this%20issue%20by%20employing%20approximate%0ABayesian%20estimation%20after%20the%20LLMs%20are%20trained%2C%20enabling%20them%20to%20quantify%0Auncertainty.%20However%2C%20such%20post-training%20approaches%27%20performance%20is%20severely%0Alimited%20by%20the%20parameters%20learned%20during%20training.%20In%20this%20paper%2C%20we%20go%20beyond%0Apost-training%20Bayesianization%20and%20propose%20Bayesian%20Low-Rank%20Adaptation%20by%0ABackpropagation%20%28BLoB%29%2C%20an%20algorithm%20that%20continuously%20and%20jointly%20adjusts%20both%0Athe%20mean%20and%20covariance%20of%20LLM%20parameters%20throughout%20the%20whole%20fine-tuning%0Aprocess.%20Our%20empirical%20results%20verify%20the%20effectiveness%20of%20BLoB%20in%20terms%20of%0Ageneralization%20and%20uncertainty%20estimation%2C%20when%20evaluated%20on%20both%0Ain-distribution%20and%20out-of-distribution%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11675v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBLoB%253A%2520Bayesian%2520Low-Rank%2520Adaptation%2520by%2520Backpropagation%2520for%2520Large%2520Language%250A%2520%2520Models%26entry.906535625%3DYibin%2520Wang%2520and%2520Haizhou%2520Shi%2520and%2520Ligong%2520Han%2520and%2520Dimitris%2520Metaxas%2520and%2520Hao%2520Wang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520often%2520suffer%2520from%2520overconfidence%2520during%250Ainference%252C%2520particularly%2520when%2520adapted%2520to%2520downstream%2520domain-specific%2520tasks%2520with%250Alimited%2520data.%2520Previous%2520work%2520addresses%2520this%2520issue%2520by%2520employing%2520approximate%250ABayesian%2520estimation%2520after%2520the%2520LLMs%2520are%2520trained%252C%2520enabling%2520them%2520to%2520quantify%250Auncertainty.%2520However%252C%2520such%2520post-training%2520approaches%2527%2520performance%2520is%2520severely%250Alimited%2520by%2520the%2520parameters%2520learned%2520during%2520training.%2520In%2520this%2520paper%252C%2520we%2520go%2520beyond%250Apost-training%2520Bayesianization%2520and%2520propose%2520Bayesian%2520Low-Rank%2520Adaptation%2520by%250ABackpropagation%2520%2528BLoB%2529%252C%2520an%2520algorithm%2520that%2520continuously%2520and%2520jointly%2520adjusts%2520both%250Athe%2520mean%2520and%2520covariance%2520of%2520LLM%2520parameters%2520throughout%2520the%2520whole%2520fine-tuning%250Aprocess.%2520Our%2520empirical%2520results%2520verify%2520the%2520effectiveness%2520of%2520BLoB%2520in%2520terms%2520of%250Ageneralization%2520and%2520uncertainty%2520estimation%252C%2520when%2520evaluated%2520on%2520both%250Ain-distribution%2520and%2520out-of-distribution%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11675v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BLoB%3A%20Bayesian%20Low-Rank%20Adaptation%20by%20Backpropagation%20for%20Large%20Language%0A%20%20Models&entry.906535625=Yibin%20Wang%20and%20Haizhou%20Shi%20and%20Ligong%20Han%20and%20Dimitris%20Metaxas%20and%20Hao%20Wang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20often%20suffer%20from%20overconfidence%20during%0Ainference%2C%20particularly%20when%20adapted%20to%20downstream%20domain-specific%20tasks%20with%0Alimited%20data.%20Previous%20work%20addresses%20this%20issue%20by%20employing%20approximate%0ABayesian%20estimation%20after%20the%20LLMs%20are%20trained%2C%20enabling%20them%20to%20quantify%0Auncertainty.%20However%2C%20such%20post-training%20approaches%27%20performance%20is%20severely%0Alimited%20by%20the%20parameters%20learned%20during%20training.%20In%20this%20paper%2C%20we%20go%20beyond%0Apost-training%20Bayesianization%20and%20propose%20Bayesian%20Low-Rank%20Adaptation%20by%0ABackpropagation%20%28BLoB%29%2C%20an%20algorithm%20that%20continuously%20and%20jointly%20adjusts%20both%0Athe%20mean%20and%20covariance%20of%20LLM%20parameters%20throughout%20the%20whole%20fine-tuning%0Aprocess.%20Our%20empirical%20results%20verify%20the%20effectiveness%20of%20BLoB%20in%20terms%20of%0Ageneralization%20and%20uncertainty%20estimation%2C%20when%20evaluated%20on%20both%0Ain-distribution%20and%20out-of-distribution%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11675v1&entry.124074799=Read"},
{"title": "PrAViC: Probabilistic Adaptation Framework for Real-Time Video\n  Classification", "author": "Magdalena Tr\u0119dowicz and \u0141ukasz Struski and Marcin Mazur and Szymon Janusz and Arkadiusz Lewicki and Jacek Tabor", "abstract": "  Video processing is generally divided into two main categories: processing of\nthe entire video, which typically yields optimal classification outcomes, and\nreal-time processing, where the objective is to make a decision as promptly as\npossible. The latter is often driven by the need to identify rapidly potential\ncritical or dangerous situations. These could include machine failure, traffic\naccidents, heart problems, or dangerous behavior. Although the models dedicated\nto the processing of entire videos are typically well-defined and clearly\npresented in the literature, this is not the case for online processing, where\na plethora of hand-devised methods exist. To address this, we present \\our{}, a\nnovel, unified, and theoretically-based adaptation framework for dealing with\nthe online classification problem for video data. The initial phase of our\nstudy is to establish a robust mathematical foundation for the theory of\nclassification of sequential data, with the potential to make a decision at an\nearly stage. This allows us to construct a natural function that encourages the\nmodel to return an outcome much faster. The subsequent phase is to demonstrate\na straightforward and readily implementable method for adapting offline models\nto online and recurrent operations. Finally, by comparing the proposed approach\nto the non-online state-of-the-art baseline, it is demonstrated that the use of\n\\our{} encourages the network to make earlier classification decisions without\ncompromising accuracy.\n", "link": "http://arxiv.org/abs/2406.11443v1", "date": "2024-06-17", "relevancy": 1.6898, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5793}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5727}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5531}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PrAViC%3A%20Probabilistic%20Adaptation%20Framework%20for%20Real-Time%20Video%0A%20%20Classification&body=Title%3A%20PrAViC%3A%20Probabilistic%20Adaptation%20Framework%20for%20Real-Time%20Video%0A%20%20Classification%0AAuthor%3A%20Magdalena%20Tr%C4%99dowicz%20and%20%C5%81ukasz%20Struski%20and%20Marcin%20Mazur%20and%20Szymon%20Janusz%20and%20Arkadiusz%20Lewicki%20and%20Jacek%20Tabor%0AAbstract%3A%20%20%20Video%20processing%20is%20generally%20divided%20into%20two%20main%20categories%3A%20processing%20of%0Athe%20entire%20video%2C%20which%20typically%20yields%20optimal%20classification%20outcomes%2C%20and%0Areal-time%20processing%2C%20where%20the%20objective%20is%20to%20make%20a%20decision%20as%20promptly%20as%0Apossible.%20The%20latter%20is%20often%20driven%20by%20the%20need%20to%20identify%20rapidly%20potential%0Acritical%20or%20dangerous%20situations.%20These%20could%20include%20machine%20failure%2C%20traffic%0Aaccidents%2C%20heart%20problems%2C%20or%20dangerous%20behavior.%20Although%20the%20models%20dedicated%0Ato%20the%20processing%20of%20entire%20videos%20are%20typically%20well-defined%20and%20clearly%0Apresented%20in%20the%20literature%2C%20this%20is%20not%20the%20case%20for%20online%20processing%2C%20where%0Aa%20plethora%20of%20hand-devised%20methods%20exist.%20To%20address%20this%2C%20we%20present%20%5Cour%7B%7D%2C%20a%0Anovel%2C%20unified%2C%20and%20theoretically-based%20adaptation%20framework%20for%20dealing%20with%0Athe%20online%20classification%20problem%20for%20video%20data.%20The%20initial%20phase%20of%20our%0Astudy%20is%20to%20establish%20a%20robust%20mathematical%20foundation%20for%20the%20theory%20of%0Aclassification%20of%20sequential%20data%2C%20with%20the%20potential%20to%20make%20a%20decision%20at%20an%0Aearly%20stage.%20This%20allows%20us%20to%20construct%20a%20natural%20function%20that%20encourages%20the%0Amodel%20to%20return%20an%20outcome%20much%20faster.%20The%20subsequent%20phase%20is%20to%20demonstrate%0Aa%20straightforward%20and%20readily%20implementable%20method%20for%20adapting%20offline%20models%0Ato%20online%20and%20recurrent%20operations.%20Finally%2C%20by%20comparing%20the%20proposed%20approach%0Ato%20the%20non-online%20state-of-the-art%20baseline%2C%20it%20is%20demonstrated%20that%20the%20use%20of%0A%5Cour%7B%7D%20encourages%20the%20network%20to%20make%20earlier%20classification%20decisions%20without%0Acompromising%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11443v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrAViC%253A%2520Probabilistic%2520Adaptation%2520Framework%2520for%2520Real-Time%2520Video%250A%2520%2520Classification%26entry.906535625%3DMagdalena%2520Tr%25C4%2599dowicz%2520and%2520%25C5%2581ukasz%2520Struski%2520and%2520Marcin%2520Mazur%2520and%2520Szymon%2520Janusz%2520and%2520Arkadiusz%2520Lewicki%2520and%2520Jacek%2520Tabor%26entry.1292438233%3D%2520%2520Video%2520processing%2520is%2520generally%2520divided%2520into%2520two%2520main%2520categories%253A%2520processing%2520of%250Athe%2520entire%2520video%252C%2520which%2520typically%2520yields%2520optimal%2520classification%2520outcomes%252C%2520and%250Areal-time%2520processing%252C%2520where%2520the%2520objective%2520is%2520to%2520make%2520a%2520decision%2520as%2520promptly%2520as%250Apossible.%2520The%2520latter%2520is%2520often%2520driven%2520by%2520the%2520need%2520to%2520identify%2520rapidly%2520potential%250Acritical%2520or%2520dangerous%2520situations.%2520These%2520could%2520include%2520machine%2520failure%252C%2520traffic%250Aaccidents%252C%2520heart%2520problems%252C%2520or%2520dangerous%2520behavior.%2520Although%2520the%2520models%2520dedicated%250Ato%2520the%2520processing%2520of%2520entire%2520videos%2520are%2520typically%2520well-defined%2520and%2520clearly%250Apresented%2520in%2520the%2520literature%252C%2520this%2520is%2520not%2520the%2520case%2520for%2520online%2520processing%252C%2520where%250Aa%2520plethora%2520of%2520hand-devised%2520methods%2520exist.%2520To%2520address%2520this%252C%2520we%2520present%2520%255Cour%257B%257D%252C%2520a%250Anovel%252C%2520unified%252C%2520and%2520theoretically-based%2520adaptation%2520framework%2520for%2520dealing%2520with%250Athe%2520online%2520classification%2520problem%2520for%2520video%2520data.%2520The%2520initial%2520phase%2520of%2520our%250Astudy%2520is%2520to%2520establish%2520a%2520robust%2520mathematical%2520foundation%2520for%2520the%2520theory%2520of%250Aclassification%2520of%2520sequential%2520data%252C%2520with%2520the%2520potential%2520to%2520make%2520a%2520decision%2520at%2520an%250Aearly%2520stage.%2520This%2520allows%2520us%2520to%2520construct%2520a%2520natural%2520function%2520that%2520encourages%2520the%250Amodel%2520to%2520return%2520an%2520outcome%2520much%2520faster.%2520The%2520subsequent%2520phase%2520is%2520to%2520demonstrate%250Aa%2520straightforward%2520and%2520readily%2520implementable%2520method%2520for%2520adapting%2520offline%2520models%250Ato%2520online%2520and%2520recurrent%2520operations.%2520Finally%252C%2520by%2520comparing%2520the%2520proposed%2520approach%250Ato%2520the%2520non-online%2520state-of-the-art%2520baseline%252C%2520it%2520is%2520demonstrated%2520that%2520the%2520use%2520of%250A%255Cour%257B%257D%2520encourages%2520the%2520network%2520to%2520make%2520earlier%2520classification%2520decisions%2520without%250Acompromising%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11443v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PrAViC%3A%20Probabilistic%20Adaptation%20Framework%20for%20Real-Time%20Video%0A%20%20Classification&entry.906535625=Magdalena%20Tr%C4%99dowicz%20and%20%C5%81ukasz%20Struski%20and%20Marcin%20Mazur%20and%20Szymon%20Janusz%20and%20Arkadiusz%20Lewicki%20and%20Jacek%20Tabor&entry.1292438233=%20%20Video%20processing%20is%20generally%20divided%20into%20two%20main%20categories%3A%20processing%20of%0Athe%20entire%20video%2C%20which%20typically%20yields%20optimal%20classification%20outcomes%2C%20and%0Areal-time%20processing%2C%20where%20the%20objective%20is%20to%20make%20a%20decision%20as%20promptly%20as%0Apossible.%20The%20latter%20is%20often%20driven%20by%20the%20need%20to%20identify%20rapidly%20potential%0Acritical%20or%20dangerous%20situations.%20These%20could%20include%20machine%20failure%2C%20traffic%0Aaccidents%2C%20heart%20problems%2C%20or%20dangerous%20behavior.%20Although%20the%20models%20dedicated%0Ato%20the%20processing%20of%20entire%20videos%20are%20typically%20well-defined%20and%20clearly%0Apresented%20in%20the%20literature%2C%20this%20is%20not%20the%20case%20for%20online%20processing%2C%20where%0Aa%20plethora%20of%20hand-devised%20methods%20exist.%20To%20address%20this%2C%20we%20present%20%5Cour%7B%7D%2C%20a%0Anovel%2C%20unified%2C%20and%20theoretically-based%20adaptation%20framework%20for%20dealing%20with%0Athe%20online%20classification%20problem%20for%20video%20data.%20The%20initial%20phase%20of%20our%0Astudy%20is%20to%20establish%20a%20robust%20mathematical%20foundation%20for%20the%20theory%20of%0Aclassification%20of%20sequential%20data%2C%20with%20the%20potential%20to%20make%20a%20decision%20at%20an%0Aearly%20stage.%20This%20allows%20us%20to%20construct%20a%20natural%20function%20that%20encourages%20the%0Amodel%20to%20return%20an%20outcome%20much%20faster.%20The%20subsequent%20phase%20is%20to%20demonstrate%0Aa%20straightforward%20and%20readily%20implementable%20method%20for%20adapting%20offline%20models%0Ato%20online%20and%20recurrent%20operations.%20Finally%2C%20by%20comparing%20the%20proposed%20approach%0Ato%20the%20non-online%20state-of-the-art%20baseline%2C%20it%20is%20demonstrated%20that%20the%20use%20of%0A%5Cour%7B%7D%20encourages%20the%20network%20to%20make%20earlier%20classification%20decisions%20without%0Acompromising%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11443v1&entry.124074799=Read"},
{"title": "Score-fPINN: Fractional Score-Based Physics-Informed Neural Networks for\n  High-Dimensional Fokker-Planck-Levy Equations", "author": "Zheyuan Hu and Zhongqiang Zhang and George Em Karniadakis and Kenji Kawaguchi", "abstract": "  We introduce an innovative approach for solving high-dimensional\nFokker-Planck-L\\'evy (FPL) equations in modeling non-Brownian processes across\ndisciplines such as physics, finance, and ecology. We utilize a fractional\nscore function and Physical-informed neural networks (PINN) to lift the curse\nof dimensionality (CoD) and alleviate numerical overflow from exponentially\ndecaying solutions with dimensions. The introduction of a fractional score\nfunction allows us to transform the FPL equation into a second-order partial\ndifferential equation without fractional Laplacian and thus can be readily\nsolved with standard physics-informed neural networks (PINNs). We propose two\nmethods to obtain a fractional score function: fractional score matching (FSM)\nand score-fPINN for fitting the fractional score function. While FSM is more\ncost-effective, it relies on known conditional distributions. On the other\nhand, score-fPINN is independent of specific stochastic differential equations\n(SDEs) but requires evaluating the PINN model's derivatives, which may be more\ncostly. We conduct our experiments on various SDEs and demonstrate numerical\nstability and effectiveness of our method in dealing with high-dimensional\nproblems, marking a significant advancement in addressing the CoD in FPL\nequations.\n", "link": "http://arxiv.org/abs/2406.11676v1", "date": "2024-06-17", "relevancy": 1.7547, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4631}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4394}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4282}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Score-fPINN%3A%20Fractional%20Score-Based%20Physics-Informed%20Neural%20Networks%20for%0A%20%20High-Dimensional%20Fokker-Planck-Levy%20Equations&body=Title%3A%20Score-fPINN%3A%20Fractional%20Score-Based%20Physics-Informed%20Neural%20Networks%20for%0A%20%20High-Dimensional%20Fokker-Planck-Levy%20Equations%0AAuthor%3A%20Zheyuan%20Hu%20and%20Zhongqiang%20Zhang%20and%20George%20Em%20Karniadakis%20and%20Kenji%20Kawaguchi%0AAbstract%3A%20%20%20We%20introduce%20an%20innovative%20approach%20for%20solving%20high-dimensional%0AFokker-Planck-L%5C%27evy%20%28FPL%29%20equations%20in%20modeling%20non-Brownian%20processes%20across%0Adisciplines%20such%20as%20physics%2C%20finance%2C%20and%20ecology.%20We%20utilize%20a%20fractional%0Ascore%20function%20and%20Physical-informed%20neural%20networks%20%28PINN%29%20to%20lift%20the%20curse%0Aof%20dimensionality%20%28CoD%29%20and%20alleviate%20numerical%20overflow%20from%20exponentially%0Adecaying%20solutions%20with%20dimensions.%20The%20introduction%20of%20a%20fractional%20score%0Afunction%20allows%20us%20to%20transform%20the%20FPL%20equation%20into%20a%20second-order%20partial%0Adifferential%20equation%20without%20fractional%20Laplacian%20and%20thus%20can%20be%20readily%0Asolved%20with%20standard%20physics-informed%20neural%20networks%20%28PINNs%29.%20We%20propose%20two%0Amethods%20to%20obtain%20a%20fractional%20score%20function%3A%20fractional%20score%20matching%20%28FSM%29%0Aand%20score-fPINN%20for%20fitting%20the%20fractional%20score%20function.%20While%20FSM%20is%20more%0Acost-effective%2C%20it%20relies%20on%20known%20conditional%20distributions.%20On%20the%20other%0Ahand%2C%20score-fPINN%20is%20independent%20of%20specific%20stochastic%20differential%20equations%0A%28SDEs%29%20but%20requires%20evaluating%20the%20PINN%20model%27s%20derivatives%2C%20which%20may%20be%20more%0Acostly.%20We%20conduct%20our%20experiments%20on%20various%20SDEs%20and%20demonstrate%20numerical%0Astability%20and%20effectiveness%20of%20our%20method%20in%20dealing%20with%20high-dimensional%0Aproblems%2C%20marking%20a%20significant%20advancement%20in%20addressing%20the%20CoD%20in%20FPL%0Aequations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11676v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScore-fPINN%253A%2520Fractional%2520Score-Based%2520Physics-Informed%2520Neural%2520Networks%2520for%250A%2520%2520High-Dimensional%2520Fokker-Planck-Levy%2520Equations%26entry.906535625%3DZheyuan%2520Hu%2520and%2520Zhongqiang%2520Zhang%2520and%2520George%2520Em%2520Karniadakis%2520and%2520Kenji%2520Kawaguchi%26entry.1292438233%3D%2520%2520We%2520introduce%2520an%2520innovative%2520approach%2520for%2520solving%2520high-dimensional%250AFokker-Planck-L%255C%2527evy%2520%2528FPL%2529%2520equations%2520in%2520modeling%2520non-Brownian%2520processes%2520across%250Adisciplines%2520such%2520as%2520physics%252C%2520finance%252C%2520and%2520ecology.%2520We%2520utilize%2520a%2520fractional%250Ascore%2520function%2520and%2520Physical-informed%2520neural%2520networks%2520%2528PINN%2529%2520to%2520lift%2520the%2520curse%250Aof%2520dimensionality%2520%2528CoD%2529%2520and%2520alleviate%2520numerical%2520overflow%2520from%2520exponentially%250Adecaying%2520solutions%2520with%2520dimensions.%2520The%2520introduction%2520of%2520a%2520fractional%2520score%250Afunction%2520allows%2520us%2520to%2520transform%2520the%2520FPL%2520equation%2520into%2520a%2520second-order%2520partial%250Adifferential%2520equation%2520without%2520fractional%2520Laplacian%2520and%2520thus%2520can%2520be%2520readily%250Asolved%2520with%2520standard%2520physics-informed%2520neural%2520networks%2520%2528PINNs%2529.%2520We%2520propose%2520two%250Amethods%2520to%2520obtain%2520a%2520fractional%2520score%2520function%253A%2520fractional%2520score%2520matching%2520%2528FSM%2529%250Aand%2520score-fPINN%2520for%2520fitting%2520the%2520fractional%2520score%2520function.%2520While%2520FSM%2520is%2520more%250Acost-effective%252C%2520it%2520relies%2520on%2520known%2520conditional%2520distributions.%2520On%2520the%2520other%250Ahand%252C%2520score-fPINN%2520is%2520independent%2520of%2520specific%2520stochastic%2520differential%2520equations%250A%2528SDEs%2529%2520but%2520requires%2520evaluating%2520the%2520PINN%2520model%2527s%2520derivatives%252C%2520which%2520may%2520be%2520more%250Acostly.%2520We%2520conduct%2520our%2520experiments%2520on%2520various%2520SDEs%2520and%2520demonstrate%2520numerical%250Astability%2520and%2520effectiveness%2520of%2520our%2520method%2520in%2520dealing%2520with%2520high-dimensional%250Aproblems%252C%2520marking%2520a%2520significant%2520advancement%2520in%2520addressing%2520the%2520CoD%2520in%2520FPL%250Aequations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11676v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Score-fPINN%3A%20Fractional%20Score-Based%20Physics-Informed%20Neural%20Networks%20for%0A%20%20High-Dimensional%20Fokker-Planck-Levy%20Equations&entry.906535625=Zheyuan%20Hu%20and%20Zhongqiang%20Zhang%20and%20George%20Em%20Karniadakis%20and%20Kenji%20Kawaguchi&entry.1292438233=%20%20We%20introduce%20an%20innovative%20approach%20for%20solving%20high-dimensional%0AFokker-Planck-L%5C%27evy%20%28FPL%29%20equations%20in%20modeling%20non-Brownian%20processes%20across%0Adisciplines%20such%20as%20physics%2C%20finance%2C%20and%20ecology.%20We%20utilize%20a%20fractional%0Ascore%20function%20and%20Physical-informed%20neural%20networks%20%28PINN%29%20to%20lift%20the%20curse%0Aof%20dimensionality%20%28CoD%29%20and%20alleviate%20numerical%20overflow%20from%20exponentially%0Adecaying%20solutions%20with%20dimensions.%20The%20introduction%20of%20a%20fractional%20score%0Afunction%20allows%20us%20to%20transform%20the%20FPL%20equation%20into%20a%20second-order%20partial%0Adifferential%20equation%20without%20fractional%20Laplacian%20and%20thus%20can%20be%20readily%0Asolved%20with%20standard%20physics-informed%20neural%20networks%20%28PINNs%29.%20We%20propose%20two%0Amethods%20to%20obtain%20a%20fractional%20score%20function%3A%20fractional%20score%20matching%20%28FSM%29%0Aand%20score-fPINN%20for%20fitting%20the%20fractional%20score%20function.%20While%20FSM%20is%20more%0Acost-effective%2C%20it%20relies%20on%20known%20conditional%20distributions.%20On%20the%20other%0Ahand%2C%20score-fPINN%20is%20independent%20of%20specific%20stochastic%20differential%20equations%0A%28SDEs%29%20but%20requires%20evaluating%20the%20PINN%20model%27s%20derivatives%2C%20which%20may%20be%20more%0Acostly.%20We%20conduct%20our%20experiments%20on%20various%20SDEs%20and%20demonstrate%20numerical%0Astability%20and%20effectiveness%20of%20our%20method%20in%20dealing%20with%20high-dimensional%0Aproblems%2C%20marking%20a%20significant%20advancement%20in%20addressing%20the%20CoD%20in%20FPL%0Aequations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11676v1&entry.124074799=Read"},
{"title": "Prompts as Auto-Optimized Training Hyperparameters: Training\n  Best-in-Class IR Models from Scratch with 10 Gold Labels", "author": "Jasper Xian and Saron Samuel and Faraz Khoubsirat and Ronak Pradeep and Md Arafat Sultan and Radu Florian and Salim Roukos and Avirup Sil and Christopher Potts and Omar Khattab", "abstract": "  We develop a method for training small-scale (under 100M parameter) neural\ninformation retrieval models with as few as 10 gold relevance labels. The\nmethod depends on generating synthetic queries for documents using a language\nmodel (LM), and the key step is that we automatically optimize the LM prompt\nthat is used to generate these queries based on training quality. In\nexperiments with the BIRCO benchmark, we find that models trained with our\nmethod outperform RankZephyr and are competitive with RankLLama, both of which\nare 7B parameter models trained on over 100K labels. These findings point to\nthe power of automatic prompt optimization for synthetic dataset generation.\n", "link": "http://arxiv.org/abs/2406.11706v1", "date": "2024-06-17", "relevancy": 1.8847, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4831}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4677}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prompts%20as%20Auto-Optimized%20Training%20Hyperparameters%3A%20Training%0A%20%20Best-in-Class%20IR%20Models%20from%20Scratch%20with%2010%20Gold%20Labels&body=Title%3A%20Prompts%20as%20Auto-Optimized%20Training%20Hyperparameters%3A%20Training%0A%20%20Best-in-Class%20IR%20Models%20from%20Scratch%20with%2010%20Gold%20Labels%0AAuthor%3A%20Jasper%20Xian%20and%20Saron%20Samuel%20and%20Faraz%20Khoubsirat%20and%20Ronak%20Pradeep%20and%20Md%20Arafat%20Sultan%20and%20Radu%20Florian%20and%20Salim%20Roukos%20and%20Avirup%20Sil%20and%20Christopher%20Potts%20and%20Omar%20Khattab%0AAbstract%3A%20%20%20We%20develop%20a%20method%20for%20training%20small-scale%20%28under%20100M%20parameter%29%20neural%0Ainformation%20retrieval%20models%20with%20as%20few%20as%2010%20gold%20relevance%20labels.%20The%0Amethod%20depends%20on%20generating%20synthetic%20queries%20for%20documents%20using%20a%20language%0Amodel%20%28LM%29%2C%20and%20the%20key%20step%20is%20that%20we%20automatically%20optimize%20the%20LM%20prompt%0Athat%20is%20used%20to%20generate%20these%20queries%20based%20on%20training%20quality.%20In%0Aexperiments%20with%20the%20BIRCO%20benchmark%2C%20we%20find%20that%20models%20trained%20with%20our%0Amethod%20outperform%20RankZephyr%20and%20are%20competitive%20with%20RankLLama%2C%20both%20of%20which%0Aare%207B%20parameter%20models%20trained%20on%20over%20100K%20labels.%20These%20findings%20point%20to%0Athe%20power%20of%20automatic%20prompt%20optimization%20for%20synthetic%20dataset%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11706v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrompts%2520as%2520Auto-Optimized%2520Training%2520Hyperparameters%253A%2520Training%250A%2520%2520Best-in-Class%2520IR%2520Models%2520from%2520Scratch%2520with%252010%2520Gold%2520Labels%26entry.906535625%3DJasper%2520Xian%2520and%2520Saron%2520Samuel%2520and%2520Faraz%2520Khoubsirat%2520and%2520Ronak%2520Pradeep%2520and%2520Md%2520Arafat%2520Sultan%2520and%2520Radu%2520Florian%2520and%2520Salim%2520Roukos%2520and%2520Avirup%2520Sil%2520and%2520Christopher%2520Potts%2520and%2520Omar%2520Khattab%26entry.1292438233%3D%2520%2520We%2520develop%2520a%2520method%2520for%2520training%2520small-scale%2520%2528under%2520100M%2520parameter%2529%2520neural%250Ainformation%2520retrieval%2520models%2520with%2520as%2520few%2520as%252010%2520gold%2520relevance%2520labels.%2520The%250Amethod%2520depends%2520on%2520generating%2520synthetic%2520queries%2520for%2520documents%2520using%2520a%2520language%250Amodel%2520%2528LM%2529%252C%2520and%2520the%2520key%2520step%2520is%2520that%2520we%2520automatically%2520optimize%2520the%2520LM%2520prompt%250Athat%2520is%2520used%2520to%2520generate%2520these%2520queries%2520based%2520on%2520training%2520quality.%2520In%250Aexperiments%2520with%2520the%2520BIRCO%2520benchmark%252C%2520we%2520find%2520that%2520models%2520trained%2520with%2520our%250Amethod%2520outperform%2520RankZephyr%2520and%2520are%2520competitive%2520with%2520RankLLama%252C%2520both%2520of%2520which%250Aare%25207B%2520parameter%2520models%2520trained%2520on%2520over%2520100K%2520labels.%2520These%2520findings%2520point%2520to%250Athe%2520power%2520of%2520automatic%2520prompt%2520optimization%2520for%2520synthetic%2520dataset%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11706v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prompts%20as%20Auto-Optimized%20Training%20Hyperparameters%3A%20Training%0A%20%20Best-in-Class%20IR%20Models%20from%20Scratch%20with%2010%20Gold%20Labels&entry.906535625=Jasper%20Xian%20and%20Saron%20Samuel%20and%20Faraz%20Khoubsirat%20and%20Ronak%20Pradeep%20and%20Md%20Arafat%20Sultan%20and%20Radu%20Florian%20and%20Salim%20Roukos%20and%20Avirup%20Sil%20and%20Christopher%20Potts%20and%20Omar%20Khattab&entry.1292438233=%20%20We%20develop%20a%20method%20for%20training%20small-scale%20%28under%20100M%20parameter%29%20neural%0Ainformation%20retrieval%20models%20with%20as%20few%20as%2010%20gold%20relevance%20labels.%20The%0Amethod%20depends%20on%20generating%20synthetic%20queries%20for%20documents%20using%20a%20language%0Amodel%20%28LM%29%2C%20and%20the%20key%20step%20is%20that%20we%20automatically%20optimize%20the%20LM%20prompt%0Athat%20is%20used%20to%20generate%20these%20queries%20based%20on%20training%20quality.%20In%0Aexperiments%20with%20the%20BIRCO%20benchmark%2C%20we%20find%20that%20models%20trained%20with%20our%0Amethod%20outperform%20RankZephyr%20and%20are%20competitive%20with%20RankLLama%2C%20both%20of%20which%0Aare%207B%20parameter%20models%20trained%20on%20over%20100K%20labels.%20These%20findings%20point%20to%0Athe%20power%20of%20automatic%20prompt%20optimization%20for%20synthetic%20dataset%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11706v1&entry.124074799=Read"},
{"title": "Optimal Transport-Assisted Risk-Sensitive Q-Learning", "author": "Zahra Shahrooei and Ali Baheri", "abstract": "  The primary goal of reinforcement learning is to develop decision-making\npolicies that prioritize optimal performance without considering risk or\nsafety. In contrast, safe reinforcement learning aims to mitigate or avoid\nunsafe states. This paper presents a risk-sensitive Q-learning algorithm that\nleverages optimal transport theory to enhance the agent safety. By integrating\noptimal transport into the Q-learning framework, our approach seeks to optimize\nthe policy's expected return while minimizing the Wasserstein distance between\nthe policy's stationary distribution and a predefined risk distribution, which\nencapsulates safety preferences from domain experts. We validate the proposed\nalgorithm in a Gridworld environment. The results indicate that our method\nsignificantly reduces the frequency of visits to risky states and achieves\nfaster convergence to a stable policy compared to the traditional Q-learning\nalgorithm.\n", "link": "http://arxiv.org/abs/2406.11774v1", "date": "2024-06-17", "relevancy": 1.9603, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5175}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4715}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4701}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Transport-Assisted%20Risk-Sensitive%20Q-Learning&body=Title%3A%20Optimal%20Transport-Assisted%20Risk-Sensitive%20Q-Learning%0AAuthor%3A%20Zahra%20Shahrooei%20and%20Ali%20Baheri%0AAbstract%3A%20%20%20The%20primary%20goal%20of%20reinforcement%20learning%20is%20to%20develop%20decision-making%0Apolicies%20that%20prioritize%20optimal%20performance%20without%20considering%20risk%20or%0Asafety.%20In%20contrast%2C%20safe%20reinforcement%20learning%20aims%20to%20mitigate%20or%20avoid%0Aunsafe%20states.%20This%20paper%20presents%20a%20risk-sensitive%20Q-learning%20algorithm%20that%0Aleverages%20optimal%20transport%20theory%20to%20enhance%20the%20agent%20safety.%20By%20integrating%0Aoptimal%20transport%20into%20the%20Q-learning%20framework%2C%20our%20approach%20seeks%20to%20optimize%0Athe%20policy%27s%20expected%20return%20while%20minimizing%20the%20Wasserstein%20distance%20between%0Athe%20policy%27s%20stationary%20distribution%20and%20a%20predefined%20risk%20distribution%2C%20which%0Aencapsulates%20safety%20preferences%20from%20domain%20experts.%20We%20validate%20the%20proposed%0Aalgorithm%20in%20a%20Gridworld%20environment.%20The%20results%20indicate%20that%20our%20method%0Asignificantly%20reduces%20the%20frequency%20of%20visits%20to%20risky%20states%20and%20achieves%0Afaster%20convergence%20to%20a%20stable%20policy%20compared%20to%20the%20traditional%20Q-learning%0Aalgorithm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11774v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Transport-Assisted%2520Risk-Sensitive%2520Q-Learning%26entry.906535625%3DZahra%2520Shahrooei%2520and%2520Ali%2520Baheri%26entry.1292438233%3D%2520%2520The%2520primary%2520goal%2520of%2520reinforcement%2520learning%2520is%2520to%2520develop%2520decision-making%250Apolicies%2520that%2520prioritize%2520optimal%2520performance%2520without%2520considering%2520risk%2520or%250Asafety.%2520In%2520contrast%252C%2520safe%2520reinforcement%2520learning%2520aims%2520to%2520mitigate%2520or%2520avoid%250Aunsafe%2520states.%2520This%2520paper%2520presents%2520a%2520risk-sensitive%2520Q-learning%2520algorithm%2520that%250Aleverages%2520optimal%2520transport%2520theory%2520to%2520enhance%2520the%2520agent%2520safety.%2520By%2520integrating%250Aoptimal%2520transport%2520into%2520the%2520Q-learning%2520framework%252C%2520our%2520approach%2520seeks%2520to%2520optimize%250Athe%2520policy%2527s%2520expected%2520return%2520while%2520minimizing%2520the%2520Wasserstein%2520distance%2520between%250Athe%2520policy%2527s%2520stationary%2520distribution%2520and%2520a%2520predefined%2520risk%2520distribution%252C%2520which%250Aencapsulates%2520safety%2520preferences%2520from%2520domain%2520experts.%2520We%2520validate%2520the%2520proposed%250Aalgorithm%2520in%2520a%2520Gridworld%2520environment.%2520The%2520results%2520indicate%2520that%2520our%2520method%250Asignificantly%2520reduces%2520the%2520frequency%2520of%2520visits%2520to%2520risky%2520states%2520and%2520achieves%250Afaster%2520convergence%2520to%2520a%2520stable%2520policy%2520compared%2520to%2520the%2520traditional%2520Q-learning%250Aalgorithm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11774v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Transport-Assisted%20Risk-Sensitive%20Q-Learning&entry.906535625=Zahra%20Shahrooei%20and%20Ali%20Baheri&entry.1292438233=%20%20The%20primary%20goal%20of%20reinforcement%20learning%20is%20to%20develop%20decision-making%0Apolicies%20that%20prioritize%20optimal%20performance%20without%20considering%20risk%20or%0Asafety.%20In%20contrast%2C%20safe%20reinforcement%20learning%20aims%20to%20mitigate%20or%20avoid%0Aunsafe%20states.%20This%20paper%20presents%20a%20risk-sensitive%20Q-learning%20algorithm%20that%0Aleverages%20optimal%20transport%20theory%20to%20enhance%20the%20agent%20safety.%20By%20integrating%0Aoptimal%20transport%20into%20the%20Q-learning%20framework%2C%20our%20approach%20seeks%20to%20optimize%0Athe%20policy%27s%20expected%20return%20while%20minimizing%20the%20Wasserstein%20distance%20between%0Athe%20policy%27s%20stationary%20distribution%20and%20a%20predefined%20risk%20distribution%2C%20which%0Aencapsulates%20safety%20preferences%20from%20domain%20experts.%20We%20validate%20the%20proposed%0Aalgorithm%20in%20a%20Gridworld%20environment.%20The%20results%20indicate%20that%20our%20method%0Asignificantly%20reduces%20the%20frequency%20of%20visits%20to%20risky%20states%20and%20achieves%0Afaster%20convergence%20to%20a%20stable%20policy%20compared%20to%20the%20traditional%20Q-learning%0Aalgorithm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11774v1&entry.124074799=Read"},
{"title": "Dual Degradation Representation for Joint Deraining and Low-Light\n  Enhancement in the Dark", "author": "Xin Lin and Jingtong Yue and Sixian Ding and Chao Ren and Lu Qi and Ming-Hsuan Yang", "abstract": "  Rain in the dark poses a significant challenge to deploying real-world\napplications such as autonomous driving, surveillance systems, and night\nphotography. Existing low-light enhancement or deraining methods struggle to\nbrighten low-light conditions and remove rain simultaneously. Additionally,\ncascade approaches like ``deraining followed by low-light enhancement'' or the\nreverse often result in problematic rain patterns or overly blurred and\noverexposed images. To address these challenges, we introduce an end-to-end\nmodel called L$^{2}$RIRNet, designed to manage both low-light enhancement and\nderaining in real-world settings. Our model features two main components: a\nDual Degradation Representation Network (DDR-Net) and a Restoration Network.\nThe DDR-Net independently learns degradation representations for luminance\neffects in dark areas and rain patterns in light areas, employing dual\ndegradation loss to guide the training process. The Restoration Network\nrestores the degraded image using a Fourier Detail Guidance (FDG) module, which\nleverages near-rainless detailed images, focusing on texture details in\nfrequency and spatial domains to inform the restoration process. Furthermore,\nwe contribute a dataset containing both synthetic and real-world\nlow-light-rainy images. Extensive experiments demonstrate that our\nL$^{2}$RIRNet performs favorably against existing methods in both synthetic and\ncomplex real-world scenarios. All the code and dataset can be found in\n\\url{https://github.com/linxin0/Low_light_rainy}.\n", "link": "http://arxiv.org/abs/2305.03997v3", "date": "2024-06-17", "relevancy": 1.6767, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5851}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5569}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual%20Degradation%20Representation%20for%20Joint%20Deraining%20and%20Low-Light%0A%20%20Enhancement%20in%20the%20Dark&body=Title%3A%20Dual%20Degradation%20Representation%20for%20Joint%20Deraining%20and%20Low-Light%0A%20%20Enhancement%20in%20the%20Dark%0AAuthor%3A%20Xin%20Lin%20and%20Jingtong%20Yue%20and%20Sixian%20Ding%20and%20Chao%20Ren%20and%20Lu%20Qi%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20%20%20Rain%20in%20the%20dark%20poses%20a%20significant%20challenge%20to%20deploying%20real-world%0Aapplications%20such%20as%20autonomous%20driving%2C%20surveillance%20systems%2C%20and%20night%0Aphotography.%20Existing%20low-light%20enhancement%20or%20deraining%20methods%20struggle%20to%0Abrighten%20low-light%20conditions%20and%20remove%20rain%20simultaneously.%20Additionally%2C%0Acascade%20approaches%20like%20%60%60deraining%20followed%20by%20low-light%20enhancement%27%27%20or%20the%0Areverse%20often%20result%20in%20problematic%20rain%20patterns%20or%20overly%20blurred%20and%0Aoverexposed%20images.%20To%20address%20these%20challenges%2C%20we%20introduce%20an%20end-to-end%0Amodel%20called%20L%24%5E%7B2%7D%24RIRNet%2C%20designed%20to%20manage%20both%20low-light%20enhancement%20and%0Aderaining%20in%20real-world%20settings.%20Our%20model%20features%20two%20main%20components%3A%20a%0ADual%20Degradation%20Representation%20Network%20%28DDR-Net%29%20and%20a%20Restoration%20Network.%0AThe%20DDR-Net%20independently%20learns%20degradation%20representations%20for%20luminance%0Aeffects%20in%20dark%20areas%20and%20rain%20patterns%20in%20light%20areas%2C%20employing%20dual%0Adegradation%20loss%20to%20guide%20the%20training%20process.%20The%20Restoration%20Network%0Arestores%20the%20degraded%20image%20using%20a%20Fourier%20Detail%20Guidance%20%28FDG%29%20module%2C%20which%0Aleverages%20near-rainless%20detailed%20images%2C%20focusing%20on%20texture%20details%20in%0Afrequency%20and%20spatial%20domains%20to%20inform%20the%20restoration%20process.%20Furthermore%2C%0Awe%20contribute%20a%20dataset%20containing%20both%20synthetic%20and%20real-world%0Alow-light-rainy%20images.%20Extensive%20experiments%20demonstrate%20that%20our%0AL%24%5E%7B2%7D%24RIRNet%20performs%20favorably%20against%20existing%20methods%20in%20both%20synthetic%20and%0Acomplex%20real-world%20scenarios.%20All%20the%20code%20and%20dataset%20can%20be%20found%20in%0A%5Curl%7Bhttps%3A//github.com/linxin0/Low_light_rainy%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.03997v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual%2520Degradation%2520Representation%2520for%2520Joint%2520Deraining%2520and%2520Low-Light%250A%2520%2520Enhancement%2520in%2520the%2520Dark%26entry.906535625%3DXin%2520Lin%2520and%2520Jingtong%2520Yue%2520and%2520Sixian%2520Ding%2520and%2520Chao%2520Ren%2520and%2520Lu%2520Qi%2520and%2520Ming-Hsuan%2520Yang%26entry.1292438233%3D%2520%2520Rain%2520in%2520the%2520dark%2520poses%2520a%2520significant%2520challenge%2520to%2520deploying%2520real-world%250Aapplications%2520such%2520as%2520autonomous%2520driving%252C%2520surveillance%2520systems%252C%2520and%2520night%250Aphotography.%2520Existing%2520low-light%2520enhancement%2520or%2520deraining%2520methods%2520struggle%2520to%250Abrighten%2520low-light%2520conditions%2520and%2520remove%2520rain%2520simultaneously.%2520Additionally%252C%250Acascade%2520approaches%2520like%2520%2560%2560deraining%2520followed%2520by%2520low-light%2520enhancement%2527%2527%2520or%2520the%250Areverse%2520often%2520result%2520in%2520problematic%2520rain%2520patterns%2520or%2520overly%2520blurred%2520and%250Aoverexposed%2520images.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520an%2520end-to-end%250Amodel%2520called%2520L%2524%255E%257B2%257D%2524RIRNet%252C%2520designed%2520to%2520manage%2520both%2520low-light%2520enhancement%2520and%250Aderaining%2520in%2520real-world%2520settings.%2520Our%2520model%2520features%2520two%2520main%2520components%253A%2520a%250ADual%2520Degradation%2520Representation%2520Network%2520%2528DDR-Net%2529%2520and%2520a%2520Restoration%2520Network.%250AThe%2520DDR-Net%2520independently%2520learns%2520degradation%2520representations%2520for%2520luminance%250Aeffects%2520in%2520dark%2520areas%2520and%2520rain%2520patterns%2520in%2520light%2520areas%252C%2520employing%2520dual%250Adegradation%2520loss%2520to%2520guide%2520the%2520training%2520process.%2520The%2520Restoration%2520Network%250Arestores%2520the%2520degraded%2520image%2520using%2520a%2520Fourier%2520Detail%2520Guidance%2520%2528FDG%2529%2520module%252C%2520which%250Aleverages%2520near-rainless%2520detailed%2520images%252C%2520focusing%2520on%2520texture%2520details%2520in%250Afrequency%2520and%2520spatial%2520domains%2520to%2520inform%2520the%2520restoration%2520process.%2520Furthermore%252C%250Awe%2520contribute%2520a%2520dataset%2520containing%2520both%2520synthetic%2520and%2520real-world%250Alow-light-rainy%2520images.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%250AL%2524%255E%257B2%257D%2524RIRNet%2520performs%2520favorably%2520against%2520existing%2520methods%2520in%2520both%2520synthetic%2520and%250Acomplex%2520real-world%2520scenarios.%2520All%2520the%2520code%2520and%2520dataset%2520can%2520be%2520found%2520in%250A%255Curl%257Bhttps%253A//github.com/linxin0/Low_light_rainy%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.03997v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual%20Degradation%20Representation%20for%20Joint%20Deraining%20and%20Low-Light%0A%20%20Enhancement%20in%20the%20Dark&entry.906535625=Xin%20Lin%20and%20Jingtong%20Yue%20and%20Sixian%20Ding%20and%20Chao%20Ren%20and%20Lu%20Qi%20and%20Ming-Hsuan%20Yang&entry.1292438233=%20%20Rain%20in%20the%20dark%20poses%20a%20significant%20challenge%20to%20deploying%20real-world%0Aapplications%20such%20as%20autonomous%20driving%2C%20surveillance%20systems%2C%20and%20night%0Aphotography.%20Existing%20low-light%20enhancement%20or%20deraining%20methods%20struggle%20to%0Abrighten%20low-light%20conditions%20and%20remove%20rain%20simultaneously.%20Additionally%2C%0Acascade%20approaches%20like%20%60%60deraining%20followed%20by%20low-light%20enhancement%27%27%20or%20the%0Areverse%20often%20result%20in%20problematic%20rain%20patterns%20or%20overly%20blurred%20and%0Aoverexposed%20images.%20To%20address%20these%20challenges%2C%20we%20introduce%20an%20end-to-end%0Amodel%20called%20L%24%5E%7B2%7D%24RIRNet%2C%20designed%20to%20manage%20both%20low-light%20enhancement%20and%0Aderaining%20in%20real-world%20settings.%20Our%20model%20features%20two%20main%20components%3A%20a%0ADual%20Degradation%20Representation%20Network%20%28DDR-Net%29%20and%20a%20Restoration%20Network.%0AThe%20DDR-Net%20independently%20learns%20degradation%20representations%20for%20luminance%0Aeffects%20in%20dark%20areas%20and%20rain%20patterns%20in%20light%20areas%2C%20employing%20dual%0Adegradation%20loss%20to%20guide%20the%20training%20process.%20The%20Restoration%20Network%0Arestores%20the%20degraded%20image%20using%20a%20Fourier%20Detail%20Guidance%20%28FDG%29%20module%2C%20which%0Aleverages%20near-rainless%20detailed%20images%2C%20focusing%20on%20texture%20details%20in%0Afrequency%20and%20spatial%20domains%20to%20inform%20the%20restoration%20process.%20Furthermore%2C%0Awe%20contribute%20a%20dataset%20containing%20both%20synthetic%20and%20real-world%0Alow-light-rainy%20images.%20Extensive%20experiments%20demonstrate%20that%20our%0AL%24%5E%7B2%7D%24RIRNet%20performs%20favorably%20against%20existing%20methods%20in%20both%20synthetic%20and%0Acomplex%20real-world%20scenarios.%20All%20the%20code%20and%20dataset%20can%20be%20found%20in%0A%5Curl%7Bhttps%3A//github.com/linxin0/Low_light_rainy%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.03997v3&entry.124074799=Read"},
{"title": "Improving Generalization of Neural Vehicle Routing Problem Solvers\n  Through the Lens of Model Architecture", "author": "Yubin Xiao and Di Wang and Xuan Wu and Yuesong Wu and Boyang Li and Wei Du and Liupu Wang and You Zhou", "abstract": "  Neural models produce promising results when solving Vehicle Routing Problems\n(VRPs), but often fall short in generalization. Recent attempts to enhance\nmodel generalization often incur unnecessarily large training cost or cannot be\ndirectly applied to other models solving different VRP variants. To address\nthese issues, we take a novel perspective on model architecture in this study.\nSpecifically, we propose a plug-and-play Entropy-based Scaling Factor (ESF) and\na Distribution-Specific (DS) decoder to enhance the size and distribution\ngeneralization, respectively. ESF adjusts the attention weight pattern of the\nmodel towards familiar ones discovered during training when solving VRPs of\nvarying sizes. The DS decoder explicitly models VRPs of multiple training\ndistribution patterns through multiple auxiliary light decoders, expanding the\nmodel representation space to encompass a broader range of distributional\nscenarios. We conduct extensive experiments on both synthetic and widely\nrecognized real-world benchmarking datasets and compare the performance with\nseven baseline models. The results demonstrate the effectiveness of using ESF\nand DS decoder to obtain a more generalizable model and showcase their\napplicability to solve different VRP variants, i.e., travelling salesman\nproblem and capacitated VRP. Notably, our proposed generic components require\nminimal computational resources, and can be effortlessly integrated into\nconventional generalization strategies to further elevate model generalization.\n", "link": "http://arxiv.org/abs/2406.06652v2", "date": "2024-06-17", "relevancy": 1.5745, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5486}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5197}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Generalization%20of%20Neural%20Vehicle%20Routing%20Problem%20Solvers%0A%20%20Through%20the%20Lens%20of%20Model%20Architecture&body=Title%3A%20Improving%20Generalization%20of%20Neural%20Vehicle%20Routing%20Problem%20Solvers%0A%20%20Through%20the%20Lens%20of%20Model%20Architecture%0AAuthor%3A%20Yubin%20Xiao%20and%20Di%20Wang%20and%20Xuan%20Wu%20and%20Yuesong%20Wu%20and%20Boyang%20Li%20and%20Wei%20Du%20and%20Liupu%20Wang%20and%20You%20Zhou%0AAbstract%3A%20%20%20Neural%20models%20produce%20promising%20results%20when%20solving%20Vehicle%20Routing%20Problems%0A%28VRPs%29%2C%20but%20often%20fall%20short%20in%20generalization.%20Recent%20attempts%20to%20enhance%0Amodel%20generalization%20often%20incur%20unnecessarily%20large%20training%20cost%20or%20cannot%20be%0Adirectly%20applied%20to%20other%20models%20solving%20different%20VRP%20variants.%20To%20address%0Athese%20issues%2C%20we%20take%20a%20novel%20perspective%20on%20model%20architecture%20in%20this%20study.%0ASpecifically%2C%20we%20propose%20a%20plug-and-play%20Entropy-based%20Scaling%20Factor%20%28ESF%29%20and%0Aa%20Distribution-Specific%20%28DS%29%20decoder%20to%20enhance%20the%20size%20and%20distribution%0Ageneralization%2C%20respectively.%20ESF%20adjusts%20the%20attention%20weight%20pattern%20of%20the%0Amodel%20towards%20familiar%20ones%20discovered%20during%20training%20when%20solving%20VRPs%20of%0Avarying%20sizes.%20The%20DS%20decoder%20explicitly%20models%20VRPs%20of%20multiple%20training%0Adistribution%20patterns%20through%20multiple%20auxiliary%20light%20decoders%2C%20expanding%20the%0Amodel%20representation%20space%20to%20encompass%20a%20broader%20range%20of%20distributional%0Ascenarios.%20We%20conduct%20extensive%20experiments%20on%20both%20synthetic%20and%20widely%0Arecognized%20real-world%20benchmarking%20datasets%20and%20compare%20the%20performance%20with%0Aseven%20baseline%20models.%20The%20results%20demonstrate%20the%20effectiveness%20of%20using%20ESF%0Aand%20DS%20decoder%20to%20obtain%20a%20more%20generalizable%20model%20and%20showcase%20their%0Aapplicability%20to%20solve%20different%20VRP%20variants%2C%20i.e.%2C%20travelling%20salesman%0Aproblem%20and%20capacitated%20VRP.%20Notably%2C%20our%20proposed%20generic%20components%20require%0Aminimal%20computational%20resources%2C%20and%20can%20be%20effortlessly%20integrated%20into%0Aconventional%20generalization%20strategies%20to%20further%20elevate%20model%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06652v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Generalization%2520of%2520Neural%2520Vehicle%2520Routing%2520Problem%2520Solvers%250A%2520%2520Through%2520the%2520Lens%2520of%2520Model%2520Architecture%26entry.906535625%3DYubin%2520Xiao%2520and%2520Di%2520Wang%2520and%2520Xuan%2520Wu%2520and%2520Yuesong%2520Wu%2520and%2520Boyang%2520Li%2520and%2520Wei%2520Du%2520and%2520Liupu%2520Wang%2520and%2520You%2520Zhou%26entry.1292438233%3D%2520%2520Neural%2520models%2520produce%2520promising%2520results%2520when%2520solving%2520Vehicle%2520Routing%2520Problems%250A%2528VRPs%2529%252C%2520but%2520often%2520fall%2520short%2520in%2520generalization.%2520Recent%2520attempts%2520to%2520enhance%250Amodel%2520generalization%2520often%2520incur%2520unnecessarily%2520large%2520training%2520cost%2520or%2520cannot%2520be%250Adirectly%2520applied%2520to%2520other%2520models%2520solving%2520different%2520VRP%2520variants.%2520To%2520address%250Athese%2520issues%252C%2520we%2520take%2520a%2520novel%2520perspective%2520on%2520model%2520architecture%2520in%2520this%2520study.%250ASpecifically%252C%2520we%2520propose%2520a%2520plug-and-play%2520Entropy-based%2520Scaling%2520Factor%2520%2528ESF%2529%2520and%250Aa%2520Distribution-Specific%2520%2528DS%2529%2520decoder%2520to%2520enhance%2520the%2520size%2520and%2520distribution%250Ageneralization%252C%2520respectively.%2520ESF%2520adjusts%2520the%2520attention%2520weight%2520pattern%2520of%2520the%250Amodel%2520towards%2520familiar%2520ones%2520discovered%2520during%2520training%2520when%2520solving%2520VRPs%2520of%250Avarying%2520sizes.%2520The%2520DS%2520decoder%2520explicitly%2520models%2520VRPs%2520of%2520multiple%2520training%250Adistribution%2520patterns%2520through%2520multiple%2520auxiliary%2520light%2520decoders%252C%2520expanding%2520the%250Amodel%2520representation%2520space%2520to%2520encompass%2520a%2520broader%2520range%2520of%2520distributional%250Ascenarios.%2520We%2520conduct%2520extensive%2520experiments%2520on%2520both%2520synthetic%2520and%2520widely%250Arecognized%2520real-world%2520benchmarking%2520datasets%2520and%2520compare%2520the%2520performance%2520with%250Aseven%2520baseline%2520models.%2520The%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520using%2520ESF%250Aand%2520DS%2520decoder%2520to%2520obtain%2520a%2520more%2520generalizable%2520model%2520and%2520showcase%2520their%250Aapplicability%2520to%2520solve%2520different%2520VRP%2520variants%252C%2520i.e.%252C%2520travelling%2520salesman%250Aproblem%2520and%2520capacitated%2520VRP.%2520Notably%252C%2520our%2520proposed%2520generic%2520components%2520require%250Aminimal%2520computational%2520resources%252C%2520and%2520can%2520be%2520effortlessly%2520integrated%2520into%250Aconventional%2520generalization%2520strategies%2520to%2520further%2520elevate%2520model%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06652v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Generalization%20of%20Neural%20Vehicle%20Routing%20Problem%20Solvers%0A%20%20Through%20the%20Lens%20of%20Model%20Architecture&entry.906535625=Yubin%20Xiao%20and%20Di%20Wang%20and%20Xuan%20Wu%20and%20Yuesong%20Wu%20and%20Boyang%20Li%20and%20Wei%20Du%20and%20Liupu%20Wang%20and%20You%20Zhou&entry.1292438233=%20%20Neural%20models%20produce%20promising%20results%20when%20solving%20Vehicle%20Routing%20Problems%0A%28VRPs%29%2C%20but%20often%20fall%20short%20in%20generalization.%20Recent%20attempts%20to%20enhance%0Amodel%20generalization%20often%20incur%20unnecessarily%20large%20training%20cost%20or%20cannot%20be%0Adirectly%20applied%20to%20other%20models%20solving%20different%20VRP%20variants.%20To%20address%0Athese%20issues%2C%20we%20take%20a%20novel%20perspective%20on%20model%20architecture%20in%20this%20study.%0ASpecifically%2C%20we%20propose%20a%20plug-and-play%20Entropy-based%20Scaling%20Factor%20%28ESF%29%20and%0Aa%20Distribution-Specific%20%28DS%29%20decoder%20to%20enhance%20the%20size%20and%20distribution%0Ageneralization%2C%20respectively.%20ESF%20adjusts%20the%20attention%20weight%20pattern%20of%20the%0Amodel%20towards%20familiar%20ones%20discovered%20during%20training%20when%20solving%20VRPs%20of%0Avarying%20sizes.%20The%20DS%20decoder%20explicitly%20models%20VRPs%20of%20multiple%20training%0Adistribution%20patterns%20through%20multiple%20auxiliary%20light%20decoders%2C%20expanding%20the%0Amodel%20representation%20space%20to%20encompass%20a%20broader%20range%20of%20distributional%0Ascenarios.%20We%20conduct%20extensive%20experiments%20on%20both%20synthetic%20and%20widely%0Arecognized%20real-world%20benchmarking%20datasets%20and%20compare%20the%20performance%20with%0Aseven%20baseline%20models.%20The%20results%20demonstrate%20the%20effectiveness%20of%20using%20ESF%0Aand%20DS%20decoder%20to%20obtain%20a%20more%20generalizable%20model%20and%20showcase%20their%0Aapplicability%20to%20solve%20different%20VRP%20variants%2C%20i.e.%2C%20travelling%20salesman%0Aproblem%20and%20capacitated%20VRP.%20Notably%2C%20our%20proposed%20generic%20components%20require%0Aminimal%20computational%20resources%2C%20and%20can%20be%20effortlessly%20integrated%20into%0Aconventional%20generalization%20strategies%20to%20further%20elevate%20model%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06652v2&entry.124074799=Read"},
{"title": "KnowledgeHub: An end-to-end Tool for Assisted Scientific Discovery", "author": "Shinnosuke Tanaka and James Barry and Vishnudev Kuruvanthodi and Movina Moses and Maxwell J. Giammona and Nathan Herr and Mohab Elkaref and Geeth De Mel", "abstract": "  This paper describes the KnowledgeHub tool, a scientific literature\nInformation Extraction (IE) and Question Answering (QA) pipeline. This is\nachieved by supporting the ingestion of PDF documents that are converted to\ntext and structured representations. An ontology can then be constructed where\na user defines the types of entities and relationships they want to capture. A\nbrowser-based annotation tool enables annotating the contents of the PDF\ndocuments according to the ontology. Named Entity Recognition (NER) and\nRelation Classification (RC) models can be trained on the resulting annotations\nand can be used to annotate the unannotated portion of the documents. A\nknowledge graph is constructed from these entity and relation triples which can\nbe queried to obtain insights from the data. Furthermore, we integrate a suite\nof Large Language Models (LLMs) that can be used for QA and summarisation that\nis grounded in the included documents via a retrieval component. KnowledgeHub\nis a unique tool that supports annotation, IE and QA, which gives the user full\ninsight into the knowledge discovery pipeline.\n", "link": "http://arxiv.org/abs/2406.00008v2", "date": "2024-06-17", "relevancy": 0.8664, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.456}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4299}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KnowledgeHub%3A%20An%20end-to-end%20Tool%20for%20Assisted%20Scientific%20Discovery&body=Title%3A%20KnowledgeHub%3A%20An%20end-to-end%20Tool%20for%20Assisted%20Scientific%20Discovery%0AAuthor%3A%20Shinnosuke%20Tanaka%20and%20James%20Barry%20and%20Vishnudev%20Kuruvanthodi%20and%20Movina%20Moses%20and%20Maxwell%20J.%20Giammona%20and%20Nathan%20Herr%20and%20Mohab%20Elkaref%20and%20Geeth%20De%20Mel%0AAbstract%3A%20%20%20This%20paper%20describes%20the%20KnowledgeHub%20tool%2C%20a%20scientific%20literature%0AInformation%20Extraction%20%28IE%29%20and%20Question%20Answering%20%28QA%29%20pipeline.%20This%20is%0Aachieved%20by%20supporting%20the%20ingestion%20of%20PDF%20documents%20that%20are%20converted%20to%0Atext%20and%20structured%20representations.%20An%20ontology%20can%20then%20be%20constructed%20where%0Aa%20user%20defines%20the%20types%20of%20entities%20and%20relationships%20they%20want%20to%20capture.%20A%0Abrowser-based%20annotation%20tool%20enables%20annotating%20the%20contents%20of%20the%20PDF%0Adocuments%20according%20to%20the%20ontology.%20Named%20Entity%20Recognition%20%28NER%29%20and%0ARelation%20Classification%20%28RC%29%20models%20can%20be%20trained%20on%20the%20resulting%20annotations%0Aand%20can%20be%20used%20to%20annotate%20the%20unannotated%20portion%20of%20the%20documents.%20A%0Aknowledge%20graph%20is%20constructed%20from%20these%20entity%20and%20relation%20triples%20which%20can%0Abe%20queried%20to%20obtain%20insights%20from%20the%20data.%20Furthermore%2C%20we%20integrate%20a%20suite%0Aof%20Large%20Language%20Models%20%28LLMs%29%20that%20can%20be%20used%20for%20QA%20and%20summarisation%20that%0Ais%20grounded%20in%20the%20included%20documents%20via%20a%20retrieval%20component.%20KnowledgeHub%0Ais%20a%20unique%20tool%20that%20supports%20annotation%2C%20IE%20and%20QA%2C%20which%20gives%20the%20user%20full%0Ainsight%20into%20the%20knowledge%20discovery%20pipeline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.00008v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledgeHub%253A%2520An%2520end-to-end%2520Tool%2520for%2520Assisted%2520Scientific%2520Discovery%26entry.906535625%3DShinnosuke%2520Tanaka%2520and%2520James%2520Barry%2520and%2520Vishnudev%2520Kuruvanthodi%2520and%2520Movina%2520Moses%2520and%2520Maxwell%2520J.%2520Giammona%2520and%2520Nathan%2520Herr%2520and%2520Mohab%2520Elkaref%2520and%2520Geeth%2520De%2520Mel%26entry.1292438233%3D%2520%2520This%2520paper%2520describes%2520the%2520KnowledgeHub%2520tool%252C%2520a%2520scientific%2520literature%250AInformation%2520Extraction%2520%2528IE%2529%2520and%2520Question%2520Answering%2520%2528QA%2529%2520pipeline.%2520This%2520is%250Aachieved%2520by%2520supporting%2520the%2520ingestion%2520of%2520PDF%2520documents%2520that%2520are%2520converted%2520to%250Atext%2520and%2520structured%2520representations.%2520An%2520ontology%2520can%2520then%2520be%2520constructed%2520where%250Aa%2520user%2520defines%2520the%2520types%2520of%2520entities%2520and%2520relationships%2520they%2520want%2520to%2520capture.%2520A%250Abrowser-based%2520annotation%2520tool%2520enables%2520annotating%2520the%2520contents%2520of%2520the%2520PDF%250Adocuments%2520according%2520to%2520the%2520ontology.%2520Named%2520Entity%2520Recognition%2520%2528NER%2529%2520and%250ARelation%2520Classification%2520%2528RC%2529%2520models%2520can%2520be%2520trained%2520on%2520the%2520resulting%2520annotations%250Aand%2520can%2520be%2520used%2520to%2520annotate%2520the%2520unannotated%2520portion%2520of%2520the%2520documents.%2520A%250Aknowledge%2520graph%2520is%2520constructed%2520from%2520these%2520entity%2520and%2520relation%2520triples%2520which%2520can%250Abe%2520queried%2520to%2520obtain%2520insights%2520from%2520the%2520data.%2520Furthermore%252C%2520we%2520integrate%2520a%2520suite%250Aof%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520that%2520can%2520be%2520used%2520for%2520QA%2520and%2520summarisation%2520that%250Ais%2520grounded%2520in%2520the%2520included%2520documents%2520via%2520a%2520retrieval%2520component.%2520KnowledgeHub%250Ais%2520a%2520unique%2520tool%2520that%2520supports%2520annotation%252C%2520IE%2520and%2520QA%252C%2520which%2520gives%2520the%2520user%2520full%250Ainsight%2520into%2520the%2520knowledge%2520discovery%2520pipeline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.00008v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KnowledgeHub%3A%20An%20end-to-end%20Tool%20for%20Assisted%20Scientific%20Discovery&entry.906535625=Shinnosuke%20Tanaka%20and%20James%20Barry%20and%20Vishnudev%20Kuruvanthodi%20and%20Movina%20Moses%20and%20Maxwell%20J.%20Giammona%20and%20Nathan%20Herr%20and%20Mohab%20Elkaref%20and%20Geeth%20De%20Mel&entry.1292438233=%20%20This%20paper%20describes%20the%20KnowledgeHub%20tool%2C%20a%20scientific%20literature%0AInformation%20Extraction%20%28IE%29%20and%20Question%20Answering%20%28QA%29%20pipeline.%20This%20is%0Aachieved%20by%20supporting%20the%20ingestion%20of%20PDF%20documents%20that%20are%20converted%20to%0Atext%20and%20structured%20representations.%20An%20ontology%20can%20then%20be%20constructed%20where%0Aa%20user%20defines%20the%20types%20of%20entities%20and%20relationships%20they%20want%20to%20capture.%20A%0Abrowser-based%20annotation%20tool%20enables%20annotating%20the%20contents%20of%20the%20PDF%0Adocuments%20according%20to%20the%20ontology.%20Named%20Entity%20Recognition%20%28NER%29%20and%0ARelation%20Classification%20%28RC%29%20models%20can%20be%20trained%20on%20the%20resulting%20annotations%0Aand%20can%20be%20used%20to%20annotate%20the%20unannotated%20portion%20of%20the%20documents.%20A%0Aknowledge%20graph%20is%20constructed%20from%20these%20entity%20and%20relation%20triples%20which%20can%0Abe%20queried%20to%20obtain%20insights%20from%20the%20data.%20Furthermore%2C%20we%20integrate%20a%20suite%0Aof%20Large%20Language%20Models%20%28LLMs%29%20that%20can%20be%20used%20for%20QA%20and%20summarisation%20that%0Ais%20grounded%20in%20the%20included%20documents%20via%20a%20retrieval%20component.%20KnowledgeHub%0Ais%20a%20unique%20tool%20that%20supports%20annotation%2C%20IE%20and%20QA%2C%20which%20gives%20the%20user%20full%0Ainsight%20into%20the%20knowledge%20discovery%20pipeline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.00008v2&entry.124074799=Read"},
{"title": "Split, Unlearn, Merge: Leveraging Data Attributes for More Effective\n  Unlearning in LLMs", "author": "Swanand Ravindra Kadhe and Farhan Ahmed and Dennis Wei and Nathalie Baracaldo and Inkit Padhi", "abstract": "  Large language models (LLMs) have shown to pose social and ethical risks such\nas generating toxic language or facilitating malicious use of hazardous\nknowledge. Machine unlearning is a promising approach to improve LLM safety by\ndirectly removing harmful behaviors and knowledge. In this paper, we propose\n\"SPlit, UNlearn, MerGE\" (SPUNGE), a framework that can be used with any\nunlearning method to amplify its effectiveness. SPUNGE leverages data\nattributes during unlearning by splitting unlearning data into subsets based on\nspecific attribute values, unlearning each subset separately, and merging the\nunlearned models. We empirically demonstrate that SPUNGE significantly improves\nthe performance of two recent unlearning methods on state-of-the-art LLMs while\nmaintaining their general capabilities on standard academic benchmarks.\n", "link": "http://arxiv.org/abs/2406.11780v1", "date": "2024-06-17", "relevancy": 1.9053, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5086}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4704}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Split%2C%20Unlearn%2C%20Merge%3A%20Leveraging%20Data%20Attributes%20for%20More%20Effective%0A%20%20Unlearning%20in%20LLMs&body=Title%3A%20Split%2C%20Unlearn%2C%20Merge%3A%20Leveraging%20Data%20Attributes%20for%20More%20Effective%0A%20%20Unlearning%20in%20LLMs%0AAuthor%3A%20Swanand%20Ravindra%20Kadhe%20and%20Farhan%20Ahmed%20and%20Dennis%20Wei%20and%20Nathalie%20Baracaldo%20and%20Inkit%20Padhi%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20to%20pose%20social%20and%20ethical%20risks%20such%0Aas%20generating%20toxic%20language%20or%20facilitating%20malicious%20use%20of%20hazardous%0Aknowledge.%20Machine%20unlearning%20is%20a%20promising%20approach%20to%20improve%20LLM%20safety%20by%0Adirectly%20removing%20harmful%20behaviors%20and%20knowledge.%20In%20this%20paper%2C%20we%20propose%0A%22SPlit%2C%20UNlearn%2C%20MerGE%22%20%28SPUNGE%29%2C%20a%20framework%20that%20can%20be%20used%20with%20any%0Aunlearning%20method%20to%20amplify%20its%20effectiveness.%20SPUNGE%20leverages%20data%0Aattributes%20during%20unlearning%20by%20splitting%20unlearning%20data%20into%20subsets%20based%20on%0Aspecific%20attribute%20values%2C%20unlearning%20each%20subset%20separately%2C%20and%20merging%20the%0Aunlearned%20models.%20We%20empirically%20demonstrate%20that%20SPUNGE%20significantly%20improves%0Athe%20performance%20of%20two%20recent%20unlearning%20methods%20on%20state-of-the-art%20LLMs%20while%0Amaintaining%20their%20general%20capabilities%20on%20standard%20academic%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11780v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSplit%252C%2520Unlearn%252C%2520Merge%253A%2520Leveraging%2520Data%2520Attributes%2520for%2520More%2520Effective%250A%2520%2520Unlearning%2520in%2520LLMs%26entry.906535625%3DSwanand%2520Ravindra%2520Kadhe%2520and%2520Farhan%2520Ahmed%2520and%2520Dennis%2520Wei%2520and%2520Nathalie%2520Baracaldo%2520and%2520Inkit%2520Padhi%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%2520to%2520pose%2520social%2520and%2520ethical%2520risks%2520such%250Aas%2520generating%2520toxic%2520language%2520or%2520facilitating%2520malicious%2520use%2520of%2520hazardous%250Aknowledge.%2520Machine%2520unlearning%2520is%2520a%2520promising%2520approach%2520to%2520improve%2520LLM%2520safety%2520by%250Adirectly%2520removing%2520harmful%2520behaviors%2520and%2520knowledge.%2520In%2520this%2520paper%252C%2520we%2520propose%250A%2522SPlit%252C%2520UNlearn%252C%2520MerGE%2522%2520%2528SPUNGE%2529%252C%2520a%2520framework%2520that%2520can%2520be%2520used%2520with%2520any%250Aunlearning%2520method%2520to%2520amplify%2520its%2520effectiveness.%2520SPUNGE%2520leverages%2520data%250Aattributes%2520during%2520unlearning%2520by%2520splitting%2520unlearning%2520data%2520into%2520subsets%2520based%2520on%250Aspecific%2520attribute%2520values%252C%2520unlearning%2520each%2520subset%2520separately%252C%2520and%2520merging%2520the%250Aunlearned%2520models.%2520We%2520empirically%2520demonstrate%2520that%2520SPUNGE%2520significantly%2520improves%250Athe%2520performance%2520of%2520two%2520recent%2520unlearning%2520methods%2520on%2520state-of-the-art%2520LLMs%2520while%250Amaintaining%2520their%2520general%2520capabilities%2520on%2520standard%2520academic%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11780v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Split%2C%20Unlearn%2C%20Merge%3A%20Leveraging%20Data%20Attributes%20for%20More%20Effective%0A%20%20Unlearning%20in%20LLMs&entry.906535625=Swanand%20Ravindra%20Kadhe%20and%20Farhan%20Ahmed%20and%20Dennis%20Wei%20and%20Nathalie%20Baracaldo%20and%20Inkit%20Padhi&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20shown%20to%20pose%20social%20and%20ethical%20risks%20such%0Aas%20generating%20toxic%20language%20or%20facilitating%20malicious%20use%20of%20hazardous%0Aknowledge.%20Machine%20unlearning%20is%20a%20promising%20approach%20to%20improve%20LLM%20safety%20by%0Adirectly%20removing%20harmful%20behaviors%20and%20knowledge.%20In%20this%20paper%2C%20we%20propose%0A%22SPlit%2C%20UNlearn%2C%20MerGE%22%20%28SPUNGE%29%2C%20a%20framework%20that%20can%20be%20used%20with%20any%0Aunlearning%20method%20to%20amplify%20its%20effectiveness.%20SPUNGE%20leverages%20data%0Aattributes%20during%20unlearning%20by%20splitting%20unlearning%20data%20into%20subsets%20based%20on%0Aspecific%20attribute%20values%2C%20unlearning%20each%20subset%20separately%2C%20and%20merging%20the%0Aunlearned%20models.%20We%20empirically%20demonstrate%20that%20SPUNGE%20significantly%20improves%0Athe%20performance%20of%20two%20recent%20unlearning%20methods%20on%20state-of-the-art%20LLMs%20while%0Amaintaining%20their%20general%20capabilities%20on%20standard%20academic%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11780v1&entry.124074799=Read"},
{"title": "Physics-informed Neural Network Estimation of Material Properties in\n  Soft Tissue Nonlinear Biomechanical Models", "author": "Federica Caforio and Francesco Regazzoni and Stefano Pagani and Elias Karabelas and Christoph Augustin and Gundolf Haase and Gernot Plank and Alfio Quarteroni", "abstract": "  The development of biophysical models for clinical applications is rapidly\nadvancing in the research community, thanks to their predictive nature and\ntheir ability to assist the interpretation of clinical data. However,\nhigh-resolution and accurate multi-physics computational models are\ncomputationally expensive and their personalisation involves fine calibration\nof a large number of parameters, which may be space-dependent, challenging\ntheir clinical translation. In this work, we propose a new approach which\nrelies on the combination of physics-informed neural networks (PINNs) with\nthree-dimensional soft tissue nonlinear biomechanical models, capable of\nreconstructing displacement fields and estimating heterogeneous\npatient-specific biophysical properties. The proposed learning algorithm\nencodes information from a limited amount of displacement and, in some cases,\nstrain data, that can be routinely acquired in the clinical setting, and\ncombines it with the physics of the problem, represented by a mathematical\nmodel based on partial differential equations, to regularise the problem and\nimprove its convergence properties. Several benchmarks are presented to show\nthe accuracy and robustness of the proposed method and its great potential to\nenable the robust and effective identification of patient-specific,\nheterogeneous physical properties, s.a. tissue stiffness properties. In\nparticular, we demonstrate the capability of the PINN to detect the presence,\nlocation and severity of scar tissue, which is beneficial to develop\npersonalised simulation models for disease diagnosis, especially for cardiac\napplications.\n", "link": "http://arxiv.org/abs/2312.09787v3", "date": "2024-06-17", "relevancy": 1.4828, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5839}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4814}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4636}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-informed%20Neural%20Network%20Estimation%20of%20Material%20Properties%20in%0A%20%20Soft%20Tissue%20Nonlinear%20Biomechanical%20Models&body=Title%3A%20Physics-informed%20Neural%20Network%20Estimation%20of%20Material%20Properties%20in%0A%20%20Soft%20Tissue%20Nonlinear%20Biomechanical%20Models%0AAuthor%3A%20Federica%20Caforio%20and%20Francesco%20Regazzoni%20and%20Stefano%20Pagani%20and%20Elias%20Karabelas%20and%20Christoph%20Augustin%20and%20Gundolf%20Haase%20and%20Gernot%20Plank%20and%20Alfio%20Quarteroni%0AAbstract%3A%20%20%20The%20development%20of%20biophysical%20models%20for%20clinical%20applications%20is%20rapidly%0Aadvancing%20in%20the%20research%20community%2C%20thanks%20to%20their%20predictive%20nature%20and%0Atheir%20ability%20to%20assist%20the%20interpretation%20of%20clinical%20data.%20However%2C%0Ahigh-resolution%20and%20accurate%20multi-physics%20computational%20models%20are%0Acomputationally%20expensive%20and%20their%20personalisation%20involves%20fine%20calibration%0Aof%20a%20large%20number%20of%20parameters%2C%20which%20may%20be%20space-dependent%2C%20challenging%0Atheir%20clinical%20translation.%20In%20this%20work%2C%20we%20propose%20a%20new%20approach%20which%0Arelies%20on%20the%20combination%20of%20physics-informed%20neural%20networks%20%28PINNs%29%20with%0Athree-dimensional%20soft%20tissue%20nonlinear%20biomechanical%20models%2C%20capable%20of%0Areconstructing%20displacement%20fields%20and%20estimating%20heterogeneous%0Apatient-specific%20biophysical%20properties.%20The%20proposed%20learning%20algorithm%0Aencodes%20information%20from%20a%20limited%20amount%20of%20displacement%20and%2C%20in%20some%20cases%2C%0Astrain%20data%2C%20that%20can%20be%20routinely%20acquired%20in%20the%20clinical%20setting%2C%20and%0Acombines%20it%20with%20the%20physics%20of%20the%20problem%2C%20represented%20by%20a%20mathematical%0Amodel%20based%20on%20partial%20differential%20equations%2C%20to%20regularise%20the%20problem%20and%0Aimprove%20its%20convergence%20properties.%20Several%20benchmarks%20are%20presented%20to%20show%0Athe%20accuracy%20and%20robustness%20of%20the%20proposed%20method%20and%20its%20great%20potential%20to%0Aenable%20the%20robust%20and%20effective%20identification%20of%20patient-specific%2C%0Aheterogeneous%20physical%20properties%2C%20s.a.%20tissue%20stiffness%20properties.%20In%0Aparticular%2C%20we%20demonstrate%20the%20capability%20of%20the%20PINN%20to%20detect%20the%20presence%2C%0Alocation%20and%20severity%20of%20scar%20tissue%2C%20which%20is%20beneficial%20to%20develop%0Apersonalised%20simulation%20models%20for%20disease%20diagnosis%2C%20especially%20for%20cardiac%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.09787v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-informed%2520Neural%2520Network%2520Estimation%2520of%2520Material%2520Properties%2520in%250A%2520%2520Soft%2520Tissue%2520Nonlinear%2520Biomechanical%2520Models%26entry.906535625%3DFederica%2520Caforio%2520and%2520Francesco%2520Regazzoni%2520and%2520Stefano%2520Pagani%2520and%2520Elias%2520Karabelas%2520and%2520Christoph%2520Augustin%2520and%2520Gundolf%2520Haase%2520and%2520Gernot%2520Plank%2520and%2520Alfio%2520Quarteroni%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520biophysical%2520models%2520for%2520clinical%2520applications%2520is%2520rapidly%250Aadvancing%2520in%2520the%2520research%2520community%252C%2520thanks%2520to%2520their%2520predictive%2520nature%2520and%250Atheir%2520ability%2520to%2520assist%2520the%2520interpretation%2520of%2520clinical%2520data.%2520However%252C%250Ahigh-resolution%2520and%2520accurate%2520multi-physics%2520computational%2520models%2520are%250Acomputationally%2520expensive%2520and%2520their%2520personalisation%2520involves%2520fine%2520calibration%250Aof%2520a%2520large%2520number%2520of%2520parameters%252C%2520which%2520may%2520be%2520space-dependent%252C%2520challenging%250Atheir%2520clinical%2520translation.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520new%2520approach%2520which%250Arelies%2520on%2520the%2520combination%2520of%2520physics-informed%2520neural%2520networks%2520%2528PINNs%2529%2520with%250Athree-dimensional%2520soft%2520tissue%2520nonlinear%2520biomechanical%2520models%252C%2520capable%2520of%250Areconstructing%2520displacement%2520fields%2520and%2520estimating%2520heterogeneous%250Apatient-specific%2520biophysical%2520properties.%2520The%2520proposed%2520learning%2520algorithm%250Aencodes%2520information%2520from%2520a%2520limited%2520amount%2520of%2520displacement%2520and%252C%2520in%2520some%2520cases%252C%250Astrain%2520data%252C%2520that%2520can%2520be%2520routinely%2520acquired%2520in%2520the%2520clinical%2520setting%252C%2520and%250Acombines%2520it%2520with%2520the%2520physics%2520of%2520the%2520problem%252C%2520represented%2520by%2520a%2520mathematical%250Amodel%2520based%2520on%2520partial%2520differential%2520equations%252C%2520to%2520regularise%2520the%2520problem%2520and%250Aimprove%2520its%2520convergence%2520properties.%2520Several%2520benchmarks%2520are%2520presented%2520to%2520show%250Athe%2520accuracy%2520and%2520robustness%2520of%2520the%2520proposed%2520method%2520and%2520its%2520great%2520potential%2520to%250Aenable%2520the%2520robust%2520and%2520effective%2520identification%2520of%2520patient-specific%252C%250Aheterogeneous%2520physical%2520properties%252C%2520s.a.%2520tissue%2520stiffness%2520properties.%2520In%250Aparticular%252C%2520we%2520demonstrate%2520the%2520capability%2520of%2520the%2520PINN%2520to%2520detect%2520the%2520presence%252C%250Alocation%2520and%2520severity%2520of%2520scar%2520tissue%252C%2520which%2520is%2520beneficial%2520to%2520develop%250Apersonalised%2520simulation%2520models%2520for%2520disease%2520diagnosis%252C%2520especially%2520for%2520cardiac%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.09787v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-informed%20Neural%20Network%20Estimation%20of%20Material%20Properties%20in%0A%20%20Soft%20Tissue%20Nonlinear%20Biomechanical%20Models&entry.906535625=Federica%20Caforio%20and%20Francesco%20Regazzoni%20and%20Stefano%20Pagani%20and%20Elias%20Karabelas%20and%20Christoph%20Augustin%20and%20Gundolf%20Haase%20and%20Gernot%20Plank%20and%20Alfio%20Quarteroni&entry.1292438233=%20%20The%20development%20of%20biophysical%20models%20for%20clinical%20applications%20is%20rapidly%0Aadvancing%20in%20the%20research%20community%2C%20thanks%20to%20their%20predictive%20nature%20and%0Atheir%20ability%20to%20assist%20the%20interpretation%20of%20clinical%20data.%20However%2C%0Ahigh-resolution%20and%20accurate%20multi-physics%20computational%20models%20are%0Acomputationally%20expensive%20and%20their%20personalisation%20involves%20fine%20calibration%0Aof%20a%20large%20number%20of%20parameters%2C%20which%20may%20be%20space-dependent%2C%20challenging%0Atheir%20clinical%20translation.%20In%20this%20work%2C%20we%20propose%20a%20new%20approach%20which%0Arelies%20on%20the%20combination%20of%20physics-informed%20neural%20networks%20%28PINNs%29%20with%0Athree-dimensional%20soft%20tissue%20nonlinear%20biomechanical%20models%2C%20capable%20of%0Areconstructing%20displacement%20fields%20and%20estimating%20heterogeneous%0Apatient-specific%20biophysical%20properties.%20The%20proposed%20learning%20algorithm%0Aencodes%20information%20from%20a%20limited%20amount%20of%20displacement%20and%2C%20in%20some%20cases%2C%0Astrain%20data%2C%20that%20can%20be%20routinely%20acquired%20in%20the%20clinical%20setting%2C%20and%0Acombines%20it%20with%20the%20physics%20of%20the%20problem%2C%20represented%20by%20a%20mathematical%0Amodel%20based%20on%20partial%20differential%20equations%2C%20to%20regularise%20the%20problem%20and%0Aimprove%20its%20convergence%20properties.%20Several%20benchmarks%20are%20presented%20to%20show%0Athe%20accuracy%20and%20robustness%20of%20the%20proposed%20method%20and%20its%20great%20potential%20to%0Aenable%20the%20robust%20and%20effective%20identification%20of%20patient-specific%2C%0Aheterogeneous%20physical%20properties%2C%20s.a.%20tissue%20stiffness%20properties.%20In%0Aparticular%2C%20we%20demonstrate%20the%20capability%20of%20the%20PINN%20to%20detect%20the%20presence%2C%0Alocation%20and%20severity%20of%20scar%20tissue%2C%20which%20is%20beneficial%20to%20develop%0Apersonalised%20simulation%20models%20for%20disease%20diagnosis%2C%20especially%20for%20cardiac%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.09787v3&entry.124074799=Read"},
{"title": "SIMPLOT: Enhancing Chart Question Answering by Distilling Essentials", "author": "Wonjoong Kim and Sangwu Park and Yeonjun In and Seokwon Han and Chanyoung Park", "abstract": "  Recently, interpreting complex charts with logical reasoning has emerged as\nchallenges due to the development of vision-language models. A prior\nstate-of-the-art (SOTA) model has presented an end-to-end method that leverages\nthe vision-language model to convert charts into table format utilizing Large\nLanguage Model (LLM) for reasoning. However, unlike natural images, charts\ncontain a mix of essential and irrelevant information required for chart\nreasoning, and we discover that this characteristic can lower the performance\nof chart-to-table extraction. In this paper, we introduce SIMPLOT, a method\ndesigned to extract only the elements necessary for chart reasoning. The\nproposed method involves two steps: 1) training to mimic a simple plot that\ncontains only the essential information from a complex chart for table\nextraction, followed by 2) performing reasoning based on the table. Our model\nenables accurate chart reasoning without the need for additional annotations or\ndatasets, and its effectiveness is demonstrated through various experiments.\nFurthermore, we propose a novel prompt mimicking how human interpret charts for\nmore accurate reasoning. Our source code is available at\nhttps://github.com/sangwu99/Simplot.\n", "link": "http://arxiv.org/abs/2405.00021v2", "date": "2024-06-17", "relevancy": 1.83, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4786}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.466}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SIMPLOT%3A%20Enhancing%20Chart%20Question%20Answering%20by%20Distilling%20Essentials&body=Title%3A%20SIMPLOT%3A%20Enhancing%20Chart%20Question%20Answering%20by%20Distilling%20Essentials%0AAuthor%3A%20Wonjoong%20Kim%20and%20Sangwu%20Park%20and%20Yeonjun%20In%20and%20Seokwon%20Han%20and%20Chanyoung%20Park%0AAbstract%3A%20%20%20Recently%2C%20interpreting%20complex%20charts%20with%20logical%20reasoning%20has%20emerged%20as%0Achallenges%20due%20to%20the%20development%20of%20vision-language%20models.%20A%20prior%0Astate-of-the-art%20%28SOTA%29%20model%20has%20presented%20an%20end-to-end%20method%20that%20leverages%0Athe%20vision-language%20model%20to%20convert%20charts%20into%20table%20format%20utilizing%20Large%0ALanguage%20Model%20%28LLM%29%20for%20reasoning.%20However%2C%20unlike%20natural%20images%2C%20charts%0Acontain%20a%20mix%20of%20essential%20and%20irrelevant%20information%20required%20for%20chart%0Areasoning%2C%20and%20we%20discover%20that%20this%20characteristic%20can%20lower%20the%20performance%0Aof%20chart-to-table%20extraction.%20In%20this%20paper%2C%20we%20introduce%20SIMPLOT%2C%20a%20method%0Adesigned%20to%20extract%20only%20the%20elements%20necessary%20for%20chart%20reasoning.%20The%0Aproposed%20method%20involves%20two%20steps%3A%201%29%20training%20to%20mimic%20a%20simple%20plot%20that%0Acontains%20only%20the%20essential%20information%20from%20a%20complex%20chart%20for%20table%0Aextraction%2C%20followed%20by%202%29%20performing%20reasoning%20based%20on%20the%20table.%20Our%20model%0Aenables%20accurate%20chart%20reasoning%20without%20the%20need%20for%20additional%20annotations%20or%0Adatasets%2C%20and%20its%20effectiveness%20is%20demonstrated%20through%20various%20experiments.%0AFurthermore%2C%20we%20propose%20a%20novel%20prompt%20mimicking%20how%20human%20interpret%20charts%20for%0Amore%20accurate%20reasoning.%20Our%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/sangwu99/Simplot.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00021v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSIMPLOT%253A%2520Enhancing%2520Chart%2520Question%2520Answering%2520by%2520Distilling%2520Essentials%26entry.906535625%3DWonjoong%2520Kim%2520and%2520Sangwu%2520Park%2520and%2520Yeonjun%2520In%2520and%2520Seokwon%2520Han%2520and%2520Chanyoung%2520Park%26entry.1292438233%3D%2520%2520Recently%252C%2520interpreting%2520complex%2520charts%2520with%2520logical%2520reasoning%2520has%2520emerged%2520as%250Achallenges%2520due%2520to%2520the%2520development%2520of%2520vision-language%2520models.%2520A%2520prior%250Astate-of-the-art%2520%2528SOTA%2529%2520model%2520has%2520presented%2520an%2520end-to-end%2520method%2520that%2520leverages%250Athe%2520vision-language%2520model%2520to%2520convert%2520charts%2520into%2520table%2520format%2520utilizing%2520Large%250ALanguage%2520Model%2520%2528LLM%2529%2520for%2520reasoning.%2520However%252C%2520unlike%2520natural%2520images%252C%2520charts%250Acontain%2520a%2520mix%2520of%2520essential%2520and%2520irrelevant%2520information%2520required%2520for%2520chart%250Areasoning%252C%2520and%2520we%2520discover%2520that%2520this%2520characteristic%2520can%2520lower%2520the%2520performance%250Aof%2520chart-to-table%2520extraction.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520SIMPLOT%252C%2520a%2520method%250Adesigned%2520to%2520extract%2520only%2520the%2520elements%2520necessary%2520for%2520chart%2520reasoning.%2520The%250Aproposed%2520method%2520involves%2520two%2520steps%253A%25201%2529%2520training%2520to%2520mimic%2520a%2520simple%2520plot%2520that%250Acontains%2520only%2520the%2520essential%2520information%2520from%2520a%2520complex%2520chart%2520for%2520table%250Aextraction%252C%2520followed%2520by%25202%2529%2520performing%2520reasoning%2520based%2520on%2520the%2520table.%2520Our%2520model%250Aenables%2520accurate%2520chart%2520reasoning%2520without%2520the%2520need%2520for%2520additional%2520annotations%2520or%250Adatasets%252C%2520and%2520its%2520effectiveness%2520is%2520demonstrated%2520through%2520various%2520experiments.%250AFurthermore%252C%2520we%2520propose%2520a%2520novel%2520prompt%2520mimicking%2520how%2520human%2520interpret%2520charts%2520for%250Amore%2520accurate%2520reasoning.%2520Our%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/sangwu99/Simplot.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.00021v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SIMPLOT%3A%20Enhancing%20Chart%20Question%20Answering%20by%20Distilling%20Essentials&entry.906535625=Wonjoong%20Kim%20and%20Sangwu%20Park%20and%20Yeonjun%20In%20and%20Seokwon%20Han%20and%20Chanyoung%20Park&entry.1292438233=%20%20Recently%2C%20interpreting%20complex%20charts%20with%20logical%20reasoning%20has%20emerged%20as%0Achallenges%20due%20to%20the%20development%20of%20vision-language%20models.%20A%20prior%0Astate-of-the-art%20%28SOTA%29%20model%20has%20presented%20an%20end-to-end%20method%20that%20leverages%0Athe%20vision-language%20model%20to%20convert%20charts%20into%20table%20format%20utilizing%20Large%0ALanguage%20Model%20%28LLM%29%20for%20reasoning.%20However%2C%20unlike%20natural%20images%2C%20charts%0Acontain%20a%20mix%20of%20essential%20and%20irrelevant%20information%20required%20for%20chart%0Areasoning%2C%20and%20we%20discover%20that%20this%20characteristic%20can%20lower%20the%20performance%0Aof%20chart-to-table%20extraction.%20In%20this%20paper%2C%20we%20introduce%20SIMPLOT%2C%20a%20method%0Adesigned%20to%20extract%20only%20the%20elements%20necessary%20for%20chart%20reasoning.%20The%0Aproposed%20method%20involves%20two%20steps%3A%201%29%20training%20to%20mimic%20a%20simple%20plot%20that%0Acontains%20only%20the%20essential%20information%20from%20a%20complex%20chart%20for%20table%0Aextraction%2C%20followed%20by%202%29%20performing%20reasoning%20based%20on%20the%20table.%20Our%20model%0Aenables%20accurate%20chart%20reasoning%20without%20the%20need%20for%20additional%20annotations%20or%0Adatasets%2C%20and%20its%20effectiveness%20is%20demonstrated%20through%20various%20experiments.%0AFurthermore%2C%20we%20propose%20a%20novel%20prompt%20mimicking%20how%20human%20interpret%20charts%20for%0Amore%20accurate%20reasoning.%20Our%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/sangwu99/Simplot.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00021v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


