<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241113.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GaussianObject: High-Quality 3D Object Reconstruction from Four Views\n  with Gaussian Splatting", "author": "Chen Yang and Sikuang Li and Jiemin Fang and Ruofan Liang and Lingxi Xie and Xiaopeng Zhang and Wei Shen and Qi Tian", "abstract": "  Reconstructing and rendering 3D objects from highly sparse views is of\ncritical importance for promoting applications of 3D vision techniques and\nimproving user experience. However, images from sparse views only contain very\nlimited 3D information, leading to two significant challenges: 1) Difficulty in\nbuilding multi-view consistency as images for matching are too few; 2)\nPartially omitted or highly compressed object information as view coverage is\ninsufficient. To tackle these challenges, we propose GaussianObject, a\nframework to represent and render the 3D object with Gaussian splatting that\nachieves high rendering quality with only 4 input images. We first introduce\ntechniques of visual hull and floater elimination, which explicitly inject\nstructure priors into the initial optimization process to help build multi-view\nconsistency, yielding a coarse 3D Gaussian representation. Then we construct a\nGaussian repair model based on diffusion models to supplement the omitted\nobject information, where Gaussians are further refined. We design a\nself-generating strategy to obtain image pairs for training the repair model.\nWe further design a COLMAP-free variant, where pre-given accurate camera poses\nare not required, which achieves competitive quality and facilitates wider\napplications. GaussianObject is evaluated on several challenging datasets,\nincluding MipNeRF360, OmniObject3D, OpenIllumination, and our-collected unposed\nimages, achieving superior performance from only four views and significantly\noutperforming previous SOTA methods. Our demo is available at\nhttps://gaussianobject.github.io/, and the code has been released at\nhttps://github.com/GaussianObject/GaussianObject.\n", "link": "http://arxiv.org/abs/2402.10259v4", "date": "2024-11-13", "relevancy": 3.5594, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7164}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7156}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaussianObject%3A%20High-Quality%203D%20Object%20Reconstruction%20from%20Four%20Views%0A%20%20with%20Gaussian%20Splatting&body=Title%3A%20GaussianObject%3A%20High-Quality%203D%20Object%20Reconstruction%20from%20Four%20Views%0A%20%20with%20Gaussian%20Splatting%0AAuthor%3A%20Chen%20Yang%20and%20Sikuang%20Li%20and%20Jiemin%20Fang%20and%20Ruofan%20Liang%20and%20Lingxi%20Xie%20and%20Xiaopeng%20Zhang%20and%20Wei%20Shen%20and%20Qi%20Tian%0AAbstract%3A%20%20%20Reconstructing%20and%20rendering%203D%20objects%20from%20highly%20sparse%20views%20is%20of%0Acritical%20importance%20for%20promoting%20applications%20of%203D%20vision%20techniques%20and%0Aimproving%20user%20experience.%20However%2C%20images%20from%20sparse%20views%20only%20contain%20very%0Alimited%203D%20information%2C%20leading%20to%20two%20significant%20challenges%3A%201%29%20Difficulty%20in%0Abuilding%20multi-view%20consistency%20as%20images%20for%20matching%20are%20too%20few%3B%202%29%0APartially%20omitted%20or%20highly%20compressed%20object%20information%20as%20view%20coverage%20is%0Ainsufficient.%20To%20tackle%20these%20challenges%2C%20we%20propose%20GaussianObject%2C%20a%0Aframework%20to%20represent%20and%20render%20the%203D%20object%20with%20Gaussian%20splatting%20that%0Aachieves%20high%20rendering%20quality%20with%20only%204%20input%20images.%20We%20first%20introduce%0Atechniques%20of%20visual%20hull%20and%20floater%20elimination%2C%20which%20explicitly%20inject%0Astructure%20priors%20into%20the%20initial%20optimization%20process%20to%20help%20build%20multi-view%0Aconsistency%2C%20yielding%20a%20coarse%203D%20Gaussian%20representation.%20Then%20we%20construct%20a%0AGaussian%20repair%20model%20based%20on%20diffusion%20models%20to%20supplement%20the%20omitted%0Aobject%20information%2C%20where%20Gaussians%20are%20further%20refined.%20We%20design%20a%0Aself-generating%20strategy%20to%20obtain%20image%20pairs%20for%20training%20the%20repair%20model.%0AWe%20further%20design%20a%20COLMAP-free%20variant%2C%20where%20pre-given%20accurate%20camera%20poses%0Aare%20not%20required%2C%20which%20achieves%20competitive%20quality%20and%20facilitates%20wider%0Aapplications.%20GaussianObject%20is%20evaluated%20on%20several%20challenging%20datasets%2C%0Aincluding%20MipNeRF360%2C%20OmniObject3D%2C%20OpenIllumination%2C%20and%20our-collected%20unposed%0Aimages%2C%20achieving%20superior%20performance%20from%20only%20four%20views%20and%20significantly%0Aoutperforming%20previous%20SOTA%20methods.%20Our%20demo%20is%20available%20at%0Ahttps%3A//gaussianobject.github.io/%2C%20and%20the%20code%20has%20been%20released%20at%0Ahttps%3A//github.com/GaussianObject/GaussianObject.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10259v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussianObject%253A%2520High-Quality%25203D%2520Object%2520Reconstruction%2520from%2520Four%2520Views%250A%2520%2520with%2520Gaussian%2520Splatting%26entry.906535625%3DChen%2520Yang%2520and%2520Sikuang%2520Li%2520and%2520Jiemin%2520Fang%2520and%2520Ruofan%2520Liang%2520and%2520Lingxi%2520Xie%2520and%2520Xiaopeng%2520Zhang%2520and%2520Wei%2520Shen%2520and%2520Qi%2520Tian%26entry.1292438233%3D%2520%2520Reconstructing%2520and%2520rendering%25203D%2520objects%2520from%2520highly%2520sparse%2520views%2520is%2520of%250Acritical%2520importance%2520for%2520promoting%2520applications%2520of%25203D%2520vision%2520techniques%2520and%250Aimproving%2520user%2520experience.%2520However%252C%2520images%2520from%2520sparse%2520views%2520only%2520contain%2520very%250Alimited%25203D%2520information%252C%2520leading%2520to%2520two%2520significant%2520challenges%253A%25201%2529%2520Difficulty%2520in%250Abuilding%2520multi-view%2520consistency%2520as%2520images%2520for%2520matching%2520are%2520too%2520few%253B%25202%2529%250APartially%2520omitted%2520or%2520highly%2520compressed%2520object%2520information%2520as%2520view%2520coverage%2520is%250Ainsufficient.%2520To%2520tackle%2520these%2520challenges%252C%2520we%2520propose%2520GaussianObject%252C%2520a%250Aframework%2520to%2520represent%2520and%2520render%2520the%25203D%2520object%2520with%2520Gaussian%2520splatting%2520that%250Aachieves%2520high%2520rendering%2520quality%2520with%2520only%25204%2520input%2520images.%2520We%2520first%2520introduce%250Atechniques%2520of%2520visual%2520hull%2520and%2520floater%2520elimination%252C%2520which%2520explicitly%2520inject%250Astructure%2520priors%2520into%2520the%2520initial%2520optimization%2520process%2520to%2520help%2520build%2520multi-view%250Aconsistency%252C%2520yielding%2520a%2520coarse%25203D%2520Gaussian%2520representation.%2520Then%2520we%2520construct%2520a%250AGaussian%2520repair%2520model%2520based%2520on%2520diffusion%2520models%2520to%2520supplement%2520the%2520omitted%250Aobject%2520information%252C%2520where%2520Gaussians%2520are%2520further%2520refined.%2520We%2520design%2520a%250Aself-generating%2520strategy%2520to%2520obtain%2520image%2520pairs%2520for%2520training%2520the%2520repair%2520model.%250AWe%2520further%2520design%2520a%2520COLMAP-free%2520variant%252C%2520where%2520pre-given%2520accurate%2520camera%2520poses%250Aare%2520not%2520required%252C%2520which%2520achieves%2520competitive%2520quality%2520and%2520facilitates%2520wider%250Aapplications.%2520GaussianObject%2520is%2520evaluated%2520on%2520several%2520challenging%2520datasets%252C%250Aincluding%2520MipNeRF360%252C%2520OmniObject3D%252C%2520OpenIllumination%252C%2520and%2520our-collected%2520unposed%250Aimages%252C%2520achieving%2520superior%2520performance%2520from%2520only%2520four%2520views%2520and%2520significantly%250Aoutperforming%2520previous%2520SOTA%2520methods.%2520Our%2520demo%2520is%2520available%2520at%250Ahttps%253A//gaussianobject.github.io/%252C%2520and%2520the%2520code%2520has%2520been%2520released%2520at%250Ahttps%253A//github.com/GaussianObject/GaussianObject.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10259v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussianObject%3A%20High-Quality%203D%20Object%20Reconstruction%20from%20Four%20Views%0A%20%20with%20Gaussian%20Splatting&entry.906535625=Chen%20Yang%20and%20Sikuang%20Li%20and%20Jiemin%20Fang%20and%20Ruofan%20Liang%20and%20Lingxi%20Xie%20and%20Xiaopeng%20Zhang%20and%20Wei%20Shen%20and%20Qi%20Tian&entry.1292438233=%20%20Reconstructing%20and%20rendering%203D%20objects%20from%20highly%20sparse%20views%20is%20of%0Acritical%20importance%20for%20promoting%20applications%20of%203D%20vision%20techniques%20and%0Aimproving%20user%20experience.%20However%2C%20images%20from%20sparse%20views%20only%20contain%20very%0Alimited%203D%20information%2C%20leading%20to%20two%20significant%20challenges%3A%201%29%20Difficulty%20in%0Abuilding%20multi-view%20consistency%20as%20images%20for%20matching%20are%20too%20few%3B%202%29%0APartially%20omitted%20or%20highly%20compressed%20object%20information%20as%20view%20coverage%20is%0Ainsufficient.%20To%20tackle%20these%20challenges%2C%20we%20propose%20GaussianObject%2C%20a%0Aframework%20to%20represent%20and%20render%20the%203D%20object%20with%20Gaussian%20splatting%20that%0Aachieves%20high%20rendering%20quality%20with%20only%204%20input%20images.%20We%20first%20introduce%0Atechniques%20of%20visual%20hull%20and%20floater%20elimination%2C%20which%20explicitly%20inject%0Astructure%20priors%20into%20the%20initial%20optimization%20process%20to%20help%20build%20multi-view%0Aconsistency%2C%20yielding%20a%20coarse%203D%20Gaussian%20representation.%20Then%20we%20construct%20a%0AGaussian%20repair%20model%20based%20on%20diffusion%20models%20to%20supplement%20the%20omitted%0Aobject%20information%2C%20where%20Gaussians%20are%20further%20refined.%20We%20design%20a%0Aself-generating%20strategy%20to%20obtain%20image%20pairs%20for%20training%20the%20repair%20model.%0AWe%20further%20design%20a%20COLMAP-free%20variant%2C%20where%20pre-given%20accurate%20camera%20poses%0Aare%20not%20required%2C%20which%20achieves%20competitive%20quality%20and%20facilitates%20wider%0Aapplications.%20GaussianObject%20is%20evaluated%20on%20several%20challenging%20datasets%2C%0Aincluding%20MipNeRF360%2C%20OmniObject3D%2C%20OpenIllumination%2C%20and%20our-collected%20unposed%0Aimages%2C%20achieving%20superior%20performance%20from%20only%20four%20views%20and%20significantly%0Aoutperforming%20previous%20SOTA%20methods.%20Our%20demo%20is%20available%20at%0Ahttps%3A//gaussianobject.github.io/%2C%20and%20the%20code%20has%20been%20released%20at%0Ahttps%3A//github.com/GaussianObject/GaussianObject.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10259v4&entry.124074799=Read"},
{"title": "4D Gaussian Splatting in the Wild with Uncertainty-Aware Regularization", "author": "Mijeong Kim and Jongwoo Lim and Bohyung Han", "abstract": "  Novel view synthesis of dynamic scenes is becoming important in various\napplications, including augmented and virtual reality. We propose a novel 4D\nGaussian Splatting (4DGS) algorithm for dynamic scenes from casually recorded\nmonocular videos. To overcome the overfitting problem of existing work for\nthese real-world videos, we introduce an uncertainty-aware regularization that\nidentifies uncertain regions with few observations and selectively imposes\nadditional priors based on diffusion models and depth smoothness on such\nregions. This approach improves both the performance of novel view synthesis\nand the quality of training image reconstruction. We also identify the\ninitialization problem of 4DGS in fast-moving dynamic regions, where the\nStructure from Motion (SfM) algorithm fails to provide reliable 3D landmarks.\nTo initialize Gaussian primitives in such regions, we present a dynamic region\ndensification method using the estimated depth maps and scene flow. Our\nexperiments show that the proposed method improves the performance of 4DGS\nreconstruction from a video captured by a handheld monocular camera and also\nexhibits promising results in few-shot static scene reconstruction.\n", "link": "http://arxiv.org/abs/2411.08879v1", "date": "2024-11-13", "relevancy": 3.5279, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.719}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6992}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204D%20Gaussian%20Splatting%20in%20the%20Wild%20with%20Uncertainty-Aware%20Regularization&body=Title%3A%204D%20Gaussian%20Splatting%20in%20the%20Wild%20with%20Uncertainty-Aware%20Regularization%0AAuthor%3A%20Mijeong%20Kim%20and%20Jongwoo%20Lim%20and%20Bohyung%20Han%0AAbstract%3A%20%20%20Novel%20view%20synthesis%20of%20dynamic%20scenes%20is%20becoming%20important%20in%20various%0Aapplications%2C%20including%20augmented%20and%20virtual%20reality.%20We%20propose%20a%20novel%204D%0AGaussian%20Splatting%20%284DGS%29%20algorithm%20for%20dynamic%20scenes%20from%20casually%20recorded%0Amonocular%20videos.%20To%20overcome%20the%20overfitting%20problem%20of%20existing%20work%20for%0Athese%20real-world%20videos%2C%20we%20introduce%20an%20uncertainty-aware%20regularization%20that%0Aidentifies%20uncertain%20regions%20with%20few%20observations%20and%20selectively%20imposes%0Aadditional%20priors%20based%20on%20diffusion%20models%20and%20depth%20smoothness%20on%20such%0Aregions.%20This%20approach%20improves%20both%20the%20performance%20of%20novel%20view%20synthesis%0Aand%20the%20quality%20of%20training%20image%20reconstruction.%20We%20also%20identify%20the%0Ainitialization%20problem%20of%204DGS%20in%20fast-moving%20dynamic%20regions%2C%20where%20the%0AStructure%20from%20Motion%20%28SfM%29%20algorithm%20fails%20to%20provide%20reliable%203D%20landmarks.%0ATo%20initialize%20Gaussian%20primitives%20in%20such%20regions%2C%20we%20present%20a%20dynamic%20region%0Adensification%20method%20using%20the%20estimated%20depth%20maps%20and%20scene%20flow.%20Our%0Aexperiments%20show%20that%20the%20proposed%20method%20improves%20the%20performance%20of%204DGS%0Areconstruction%20from%20a%20video%20captured%20by%20a%20handheld%20monocular%20camera%20and%20also%0Aexhibits%20promising%20results%20in%20few-shot%20static%20scene%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08879v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4D%2520Gaussian%2520Splatting%2520in%2520the%2520Wild%2520with%2520Uncertainty-Aware%2520Regularization%26entry.906535625%3DMijeong%2520Kim%2520and%2520Jongwoo%2520Lim%2520and%2520Bohyung%2520Han%26entry.1292438233%3D%2520%2520Novel%2520view%2520synthesis%2520of%2520dynamic%2520scenes%2520is%2520becoming%2520important%2520in%2520various%250Aapplications%252C%2520including%2520augmented%2520and%2520virtual%2520reality.%2520We%2520propose%2520a%2520novel%25204D%250AGaussian%2520Splatting%2520%25284DGS%2529%2520algorithm%2520for%2520dynamic%2520scenes%2520from%2520casually%2520recorded%250Amonocular%2520videos.%2520To%2520overcome%2520the%2520overfitting%2520problem%2520of%2520existing%2520work%2520for%250Athese%2520real-world%2520videos%252C%2520we%2520introduce%2520an%2520uncertainty-aware%2520regularization%2520that%250Aidentifies%2520uncertain%2520regions%2520with%2520few%2520observations%2520and%2520selectively%2520imposes%250Aadditional%2520priors%2520based%2520on%2520diffusion%2520models%2520and%2520depth%2520smoothness%2520on%2520such%250Aregions.%2520This%2520approach%2520improves%2520both%2520the%2520performance%2520of%2520novel%2520view%2520synthesis%250Aand%2520the%2520quality%2520of%2520training%2520image%2520reconstruction.%2520We%2520also%2520identify%2520the%250Ainitialization%2520problem%2520of%25204DGS%2520in%2520fast-moving%2520dynamic%2520regions%252C%2520where%2520the%250AStructure%2520from%2520Motion%2520%2528SfM%2529%2520algorithm%2520fails%2520to%2520provide%2520reliable%25203D%2520landmarks.%250ATo%2520initialize%2520Gaussian%2520primitives%2520in%2520such%2520regions%252C%2520we%2520present%2520a%2520dynamic%2520region%250Adensification%2520method%2520using%2520the%2520estimated%2520depth%2520maps%2520and%2520scene%2520flow.%2520Our%250Aexperiments%2520show%2520that%2520the%2520proposed%2520method%2520improves%2520the%2520performance%2520of%25204DGS%250Areconstruction%2520from%2520a%2520video%2520captured%2520by%2520a%2520handheld%2520monocular%2520camera%2520and%2520also%250Aexhibits%2520promising%2520results%2520in%2520few-shot%2520static%2520scene%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08879v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4D%20Gaussian%20Splatting%20in%20the%20Wild%20with%20Uncertainty-Aware%20Regularization&entry.906535625=Mijeong%20Kim%20and%20Jongwoo%20Lim%20and%20Bohyung%20Han&entry.1292438233=%20%20Novel%20view%20synthesis%20of%20dynamic%20scenes%20is%20becoming%20important%20in%20various%0Aapplications%2C%20including%20augmented%20and%20virtual%20reality.%20We%20propose%20a%20novel%204D%0AGaussian%20Splatting%20%284DGS%29%20algorithm%20for%20dynamic%20scenes%20from%20casually%20recorded%0Amonocular%20videos.%20To%20overcome%20the%20overfitting%20problem%20of%20existing%20work%20for%0Athese%20real-world%20videos%2C%20we%20introduce%20an%20uncertainty-aware%20regularization%20that%0Aidentifies%20uncertain%20regions%20with%20few%20observations%20and%20selectively%20imposes%0Aadditional%20priors%20based%20on%20diffusion%20models%20and%20depth%20smoothness%20on%20such%0Aregions.%20This%20approach%20improves%20both%20the%20performance%20of%20novel%20view%20synthesis%0Aand%20the%20quality%20of%20training%20image%20reconstruction.%20We%20also%20identify%20the%0Ainitialization%20problem%20of%204DGS%20in%20fast-moving%20dynamic%20regions%2C%20where%20the%0AStructure%20from%20Motion%20%28SfM%29%20algorithm%20fails%20to%20provide%20reliable%203D%20landmarks.%0ATo%20initialize%20Gaussian%20primitives%20in%20such%20regions%2C%20we%20present%20a%20dynamic%20region%0Adensification%20method%20using%20the%20estimated%20depth%20maps%20and%20scene%20flow.%20Our%0Aexperiments%20show%20that%20the%20proposed%20method%20improves%20the%20performance%20of%204DGS%0Areconstruction%20from%20a%20video%20captured%20by%20a%20handheld%20monocular%20camera%20and%20also%0Aexhibits%20promising%20results%20in%20few-shot%20static%20scene%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08879v1&entry.124074799=Read"},
{"title": "Textured-GS: Gaussian Splatting with Spatially Defined Color and Opacity", "author": "Zhentao Huang and Minglun Gong", "abstract": "  In this paper, we introduce Textured-GS, an innovative method for rendering\nGaussian splatting that incorporates spatially defined color and opacity\nvariations using Spherical Harmonics (SH). This approach enables each Gaussian\nto exhibit a richer representation by accommodating varying colors and\nopacities across its surface, significantly enhancing rendering quality\ncompared to traditional methods. To demonstrate the merits of our approach, we\nhave adapted the Mini-Splatting architecture to integrate textured Gaussians\nwithout increasing the number of Gaussians. Our experiments across multiple\nreal-world datasets show that Textured-GS consistently outperforms both the\nbaseline Mini-Splatting and standard 3DGS in terms of visual fidelity. The\nresults highlight the potential of Textured-GS to advance Gaussian-based\nrendering technologies, promising more efficient and high-quality scene\nreconstructions. Our implementation is available at\nhttps://github.com/ZhentaoHuang/Textured-GS.\n", "link": "http://arxiv.org/abs/2407.09733v3", "date": "2024-11-13", "relevancy": 3.1891, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6535}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6369}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Textured-GS%3A%20Gaussian%20Splatting%20with%20Spatially%20Defined%20Color%20and%20Opacity&body=Title%3A%20Textured-GS%3A%20Gaussian%20Splatting%20with%20Spatially%20Defined%20Color%20and%20Opacity%0AAuthor%3A%20Zhentao%20Huang%20and%20Minglun%20Gong%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20Textured-GS%2C%20an%20innovative%20method%20for%20rendering%0AGaussian%20splatting%20that%20incorporates%20spatially%20defined%20color%20and%20opacity%0Avariations%20using%20Spherical%20Harmonics%20%28SH%29.%20This%20approach%20enables%20each%20Gaussian%0Ato%20exhibit%20a%20richer%20representation%20by%20accommodating%20varying%20colors%20and%0Aopacities%20across%20its%20surface%2C%20significantly%20enhancing%20rendering%20quality%0Acompared%20to%20traditional%20methods.%20To%20demonstrate%20the%20merits%20of%20our%20approach%2C%20we%0Ahave%20adapted%20the%20Mini-Splatting%20architecture%20to%20integrate%20textured%20Gaussians%0Awithout%20increasing%20the%20number%20of%20Gaussians.%20Our%20experiments%20across%20multiple%0Areal-world%20datasets%20show%20that%20Textured-GS%20consistently%20outperforms%20both%20the%0Abaseline%20Mini-Splatting%20and%20standard%203DGS%20in%20terms%20of%20visual%20fidelity.%20The%0Aresults%20highlight%20the%20potential%20of%20Textured-GS%20to%20advance%20Gaussian-based%0Arendering%20technologies%2C%20promising%20more%20efficient%20and%20high-quality%20scene%0Areconstructions.%20Our%20implementation%20is%20available%20at%0Ahttps%3A//github.com/ZhentaoHuang/Textured-GS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09733v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTextured-GS%253A%2520Gaussian%2520Splatting%2520with%2520Spatially%2520Defined%2520Color%2520and%2520Opacity%26entry.906535625%3DZhentao%2520Huang%2520and%2520Minglun%2520Gong%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Textured-GS%252C%2520an%2520innovative%2520method%2520for%2520rendering%250AGaussian%2520splatting%2520that%2520incorporates%2520spatially%2520defined%2520color%2520and%2520opacity%250Avariations%2520using%2520Spherical%2520Harmonics%2520%2528SH%2529.%2520This%2520approach%2520enables%2520each%2520Gaussian%250Ato%2520exhibit%2520a%2520richer%2520representation%2520by%2520accommodating%2520varying%2520colors%2520and%250Aopacities%2520across%2520its%2520surface%252C%2520significantly%2520enhancing%2520rendering%2520quality%250Acompared%2520to%2520traditional%2520methods.%2520To%2520demonstrate%2520the%2520merits%2520of%2520our%2520approach%252C%2520we%250Ahave%2520adapted%2520the%2520Mini-Splatting%2520architecture%2520to%2520integrate%2520textured%2520Gaussians%250Awithout%2520increasing%2520the%2520number%2520of%2520Gaussians.%2520Our%2520experiments%2520across%2520multiple%250Areal-world%2520datasets%2520show%2520that%2520Textured-GS%2520consistently%2520outperforms%2520both%2520the%250Abaseline%2520Mini-Splatting%2520and%2520standard%25203DGS%2520in%2520terms%2520of%2520visual%2520fidelity.%2520The%250Aresults%2520highlight%2520the%2520potential%2520of%2520Textured-GS%2520to%2520advance%2520Gaussian-based%250Arendering%2520technologies%252C%2520promising%2520more%2520efficient%2520and%2520high-quality%2520scene%250Areconstructions.%2520Our%2520implementation%2520is%2520available%2520at%250Ahttps%253A//github.com/ZhentaoHuang/Textured-GS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09733v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Textured-GS%3A%20Gaussian%20Splatting%20with%20Spatially%20Defined%20Color%20and%20Opacity&entry.906535625=Zhentao%20Huang%20and%20Minglun%20Gong&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20Textured-GS%2C%20an%20innovative%20method%20for%20rendering%0AGaussian%20splatting%20that%20incorporates%20spatially%20defined%20color%20and%20opacity%0Avariations%20using%20Spherical%20Harmonics%20%28SH%29.%20This%20approach%20enables%20each%20Gaussian%0Ato%20exhibit%20a%20richer%20representation%20by%20accommodating%20varying%20colors%20and%0Aopacities%20across%20its%20surface%2C%20significantly%20enhancing%20rendering%20quality%0Acompared%20to%20traditional%20methods.%20To%20demonstrate%20the%20merits%20of%20our%20approach%2C%20we%0Ahave%20adapted%20the%20Mini-Splatting%20architecture%20to%20integrate%20textured%20Gaussians%0Awithout%20increasing%20the%20number%20of%20Gaussians.%20Our%20experiments%20across%20multiple%0Areal-world%20datasets%20show%20that%20Textured-GS%20consistently%20outperforms%20both%20the%0Abaseline%20Mini-Splatting%20and%20standard%203DGS%20in%20terms%20of%20visual%20fidelity.%20The%0Aresults%20highlight%20the%20potential%20of%20Textured-GS%20to%20advance%20Gaussian-based%0Arendering%20technologies%2C%20promising%20more%20efficient%20and%20high-quality%20scene%0Areconstructions.%20Our%20implementation%20is%20available%20at%0Ahttps%3A//github.com/ZhentaoHuang/Textured-GS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09733v3&entry.124074799=Read"},
{"title": "BillBoard Splatting (BBSplat): Learnable Textured Primitives for Novel\n  View Synthesis", "author": "David Svitov and Pietro Morerio and Lourdes Agapito and Alessio Del Bue", "abstract": "  We present billboard Splatting (BBSplat) - a novel approach for 3D scene\nrepresentation based on textured geometric primitives. BBSplat represents the\nscene as a set of optimizable textured planar primitives with learnable RGB\ntextures and alpha-maps to control their shape. BBSplat primitives can be used\nin any Gaussian Splatting pipeline as drop-in replacements for Gaussians. Our\nmethod's qualitative and quantitative improvements over 3D and 2D Gaussians are\nmost noticeable when fewer primitives are used, when BBSplat achieves over 1200\nFPS. Our novel regularization term encourages textures to have a sparser\nstructure, unlocking an efficient compression that leads to a reduction in\nstorage space of the model. Our experiments show the efficiency of BBSplat on\nstandard datasets of real indoor and outdoor scenes such as Tanks&Temples, DTU,\nand Mip-NeRF-360. We demonstrate improvements on PSNR, SSIM, and LPIPS metrics\ncompared to the state-of-the-art, especially for the case when fewer primitives\nare used, which, on the other hand, leads to up to 2 times inference speed\nimprovement for the same rendering quality.\n", "link": "http://arxiv.org/abs/2411.08508v1", "date": "2024-11-13", "relevancy": 3.1266, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6578}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6291}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BillBoard%20Splatting%20%28BBSplat%29%3A%20Learnable%20Textured%20Primitives%20for%20Novel%0A%20%20View%20Synthesis&body=Title%3A%20BillBoard%20Splatting%20%28BBSplat%29%3A%20Learnable%20Textured%20Primitives%20for%20Novel%0A%20%20View%20Synthesis%0AAuthor%3A%20David%20Svitov%20and%20Pietro%20Morerio%20and%20Lourdes%20Agapito%20and%20Alessio%20Del%20Bue%0AAbstract%3A%20%20%20We%20present%20billboard%20Splatting%20%28BBSplat%29%20-%20a%20novel%20approach%20for%203D%20scene%0Arepresentation%20based%20on%20textured%20geometric%20primitives.%20BBSplat%20represents%20the%0Ascene%20as%20a%20set%20of%20optimizable%20textured%20planar%20primitives%20with%20learnable%20RGB%0Atextures%20and%20alpha-maps%20to%20control%20their%20shape.%20BBSplat%20primitives%20can%20be%20used%0Ain%20any%20Gaussian%20Splatting%20pipeline%20as%20drop-in%20replacements%20for%20Gaussians.%20Our%0Amethod%27s%20qualitative%20and%20quantitative%20improvements%20over%203D%20and%202D%20Gaussians%20are%0Amost%20noticeable%20when%20fewer%20primitives%20are%20used%2C%20when%20BBSplat%20achieves%20over%201200%0AFPS.%20Our%20novel%20regularization%20term%20encourages%20textures%20to%20have%20a%20sparser%0Astructure%2C%20unlocking%20an%20efficient%20compression%20that%20leads%20to%20a%20reduction%20in%0Astorage%20space%20of%20the%20model.%20Our%20experiments%20show%20the%20efficiency%20of%20BBSplat%20on%0Astandard%20datasets%20of%20real%20indoor%20and%20outdoor%20scenes%20such%20as%20Tanks%26Temples%2C%20DTU%2C%0Aand%20Mip-NeRF-360.%20We%20demonstrate%20improvements%20on%20PSNR%2C%20SSIM%2C%20and%20LPIPS%20metrics%0Acompared%20to%20the%20state-of-the-art%2C%20especially%20for%20the%20case%20when%20fewer%20primitives%0Aare%20used%2C%20which%2C%20on%20the%20other%20hand%2C%20leads%20to%20up%20to%202%20times%20inference%20speed%0Aimprovement%20for%20the%20same%20rendering%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08508v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBillBoard%2520Splatting%2520%2528BBSplat%2529%253A%2520Learnable%2520Textured%2520Primitives%2520for%2520Novel%250A%2520%2520View%2520Synthesis%26entry.906535625%3DDavid%2520Svitov%2520and%2520Pietro%2520Morerio%2520and%2520Lourdes%2520Agapito%2520and%2520Alessio%2520Del%2520Bue%26entry.1292438233%3D%2520%2520We%2520present%2520billboard%2520Splatting%2520%2528BBSplat%2529%2520-%2520a%2520novel%2520approach%2520for%25203D%2520scene%250Arepresentation%2520based%2520on%2520textured%2520geometric%2520primitives.%2520BBSplat%2520represents%2520the%250Ascene%2520as%2520a%2520set%2520of%2520optimizable%2520textured%2520planar%2520primitives%2520with%2520learnable%2520RGB%250Atextures%2520and%2520alpha-maps%2520to%2520control%2520their%2520shape.%2520BBSplat%2520primitives%2520can%2520be%2520used%250Ain%2520any%2520Gaussian%2520Splatting%2520pipeline%2520as%2520drop-in%2520replacements%2520for%2520Gaussians.%2520Our%250Amethod%2527s%2520qualitative%2520and%2520quantitative%2520improvements%2520over%25203D%2520and%25202D%2520Gaussians%2520are%250Amost%2520noticeable%2520when%2520fewer%2520primitives%2520are%2520used%252C%2520when%2520BBSplat%2520achieves%2520over%25201200%250AFPS.%2520Our%2520novel%2520regularization%2520term%2520encourages%2520textures%2520to%2520have%2520a%2520sparser%250Astructure%252C%2520unlocking%2520an%2520efficient%2520compression%2520that%2520leads%2520to%2520a%2520reduction%2520in%250Astorage%2520space%2520of%2520the%2520model.%2520Our%2520experiments%2520show%2520the%2520efficiency%2520of%2520BBSplat%2520on%250Astandard%2520datasets%2520of%2520real%2520indoor%2520and%2520outdoor%2520scenes%2520such%2520as%2520Tanks%2526Temples%252C%2520DTU%252C%250Aand%2520Mip-NeRF-360.%2520We%2520demonstrate%2520improvements%2520on%2520PSNR%252C%2520SSIM%252C%2520and%2520LPIPS%2520metrics%250Acompared%2520to%2520the%2520state-of-the-art%252C%2520especially%2520for%2520the%2520case%2520when%2520fewer%2520primitives%250Aare%2520used%252C%2520which%252C%2520on%2520the%2520other%2520hand%252C%2520leads%2520to%2520up%2520to%25202%2520times%2520inference%2520speed%250Aimprovement%2520for%2520the%2520same%2520rendering%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08508v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BillBoard%20Splatting%20%28BBSplat%29%3A%20Learnable%20Textured%20Primitives%20for%20Novel%0A%20%20View%20Synthesis&entry.906535625=David%20Svitov%20and%20Pietro%20Morerio%20and%20Lourdes%20Agapito%20and%20Alessio%20Del%20Bue&entry.1292438233=%20%20We%20present%20billboard%20Splatting%20%28BBSplat%29%20-%20a%20novel%20approach%20for%203D%20scene%0Arepresentation%20based%20on%20textured%20geometric%20primitives.%20BBSplat%20represents%20the%0Ascene%20as%20a%20set%20of%20optimizable%20textured%20planar%20primitives%20with%20learnable%20RGB%0Atextures%20and%20alpha-maps%20to%20control%20their%20shape.%20BBSplat%20primitives%20can%20be%20used%0Ain%20any%20Gaussian%20Splatting%20pipeline%20as%20drop-in%20replacements%20for%20Gaussians.%20Our%0Amethod%27s%20qualitative%20and%20quantitative%20improvements%20over%203D%20and%202D%20Gaussians%20are%0Amost%20noticeable%20when%20fewer%20primitives%20are%20used%2C%20when%20BBSplat%20achieves%20over%201200%0AFPS.%20Our%20novel%20regularization%20term%20encourages%20textures%20to%20have%20a%20sparser%0Astructure%2C%20unlocking%20an%20efficient%20compression%20that%20leads%20to%20a%20reduction%20in%0Astorage%20space%20of%20the%20model.%20Our%20experiments%20show%20the%20efficiency%20of%20BBSplat%20on%0Astandard%20datasets%20of%20real%20indoor%20and%20outdoor%20scenes%20such%20as%20Tanks%26Temples%2C%20DTU%2C%0Aand%20Mip-NeRF-360.%20We%20demonstrate%20improvements%20on%20PSNR%2C%20SSIM%2C%20and%20LPIPS%20metrics%0Acompared%20to%20the%20state-of-the-art%2C%20especially%20for%20the%20case%20when%20fewer%20primitives%0Aare%20used%2C%20which%2C%20on%20the%20other%20hand%2C%20leads%20to%20up%20to%202%20times%20inference%20speed%0Aimprovement%20for%20the%20same%20rendering%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08508v1&entry.124074799=Read"},
{"title": "Learning Locally Adaptive Metrics that Enhance Structural Representation\n  with $\\texttt{LAMINAR}$", "author": "Christian Kleiber and William H. Oliver and Tobias Buck", "abstract": "  We present $\\texttt{LAMINAR}$, a novel unsupervised machine learning pipeline\ndesigned to enhance the representation of structure within data via producing a\nmore-informative distance metric. Analysis methods in the physical sciences\noften rely on standard metrics to define geometric relationships in data, which\nmay fail to capture the underlying structure of complex data sets.\n$\\texttt{LAMINAR}$ addresses this by using a continuous-normalising-flow and\ninverse-transform-sampling to define a Riemannian manifold in the data space\nwithout the need for the user to specify a metric over the data a-priori. The\nresult is a locally-adaptive-metric that produces structurally-informative\ndensity-based distances. We demonstrate the utility of $\\texttt{LAMINAR}$ by\ncomparing its output to the Euclidean metric for structured data sets.\n", "link": "http://arxiv.org/abs/2411.08557v1", "date": "2024-11-13", "relevancy": 2.8591, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5933}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5677}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Locally%20Adaptive%20Metrics%20that%20Enhance%20Structural%20Representation%0A%20%20with%20%24%5Ctexttt%7BLAMINAR%7D%24&body=Title%3A%20Learning%20Locally%20Adaptive%20Metrics%20that%20Enhance%20Structural%20Representation%0A%20%20with%20%24%5Ctexttt%7BLAMINAR%7D%24%0AAuthor%3A%20Christian%20Kleiber%20and%20William%20H.%20Oliver%20and%20Tobias%20Buck%0AAbstract%3A%20%20%20We%20present%20%24%5Ctexttt%7BLAMINAR%7D%24%2C%20a%20novel%20unsupervised%20machine%20learning%20pipeline%0Adesigned%20to%20enhance%20the%20representation%20of%20structure%20within%20data%20via%20producing%20a%0Amore-informative%20distance%20metric.%20Analysis%20methods%20in%20the%20physical%20sciences%0Aoften%20rely%20on%20standard%20metrics%20to%20define%20geometric%20relationships%20in%20data%2C%20which%0Amay%20fail%20to%20capture%20the%20underlying%20structure%20of%20complex%20data%20sets.%0A%24%5Ctexttt%7BLAMINAR%7D%24%20addresses%20this%20by%20using%20a%20continuous-normalising-flow%20and%0Ainverse-transform-sampling%20to%20define%20a%20Riemannian%20manifold%20in%20the%20data%20space%0Awithout%20the%20need%20for%20the%20user%20to%20specify%20a%20metric%20over%20the%20data%20a-priori.%20The%0Aresult%20is%20a%20locally-adaptive-metric%20that%20produces%20structurally-informative%0Adensity-based%20distances.%20We%20demonstrate%20the%20utility%20of%20%24%5Ctexttt%7BLAMINAR%7D%24%20by%0Acomparing%20its%20output%20to%20the%20Euclidean%20metric%20for%20structured%20data%20sets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08557v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Locally%2520Adaptive%2520Metrics%2520that%2520Enhance%2520Structural%2520Representation%250A%2520%2520with%2520%2524%255Ctexttt%257BLAMINAR%257D%2524%26entry.906535625%3DChristian%2520Kleiber%2520and%2520William%2520H.%2520Oliver%2520and%2520Tobias%2520Buck%26entry.1292438233%3D%2520%2520We%2520present%2520%2524%255Ctexttt%257BLAMINAR%257D%2524%252C%2520a%2520novel%2520unsupervised%2520machine%2520learning%2520pipeline%250Adesigned%2520to%2520enhance%2520the%2520representation%2520of%2520structure%2520within%2520data%2520via%2520producing%2520a%250Amore-informative%2520distance%2520metric.%2520Analysis%2520methods%2520in%2520the%2520physical%2520sciences%250Aoften%2520rely%2520on%2520standard%2520metrics%2520to%2520define%2520geometric%2520relationships%2520in%2520data%252C%2520which%250Amay%2520fail%2520to%2520capture%2520the%2520underlying%2520structure%2520of%2520complex%2520data%2520sets.%250A%2524%255Ctexttt%257BLAMINAR%257D%2524%2520addresses%2520this%2520by%2520using%2520a%2520continuous-normalising-flow%2520and%250Ainverse-transform-sampling%2520to%2520define%2520a%2520Riemannian%2520manifold%2520in%2520the%2520data%2520space%250Awithout%2520the%2520need%2520for%2520the%2520user%2520to%2520specify%2520a%2520metric%2520over%2520the%2520data%2520a-priori.%2520The%250Aresult%2520is%2520a%2520locally-adaptive-metric%2520that%2520produces%2520structurally-informative%250Adensity-based%2520distances.%2520We%2520demonstrate%2520the%2520utility%2520of%2520%2524%255Ctexttt%257BLAMINAR%257D%2524%2520by%250Acomparing%2520its%2520output%2520to%2520the%2520Euclidean%2520metric%2520for%2520structured%2520data%2520sets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08557v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Locally%20Adaptive%20Metrics%20that%20Enhance%20Structural%20Representation%0A%20%20with%20%24%5Ctexttt%7BLAMINAR%7D%24&entry.906535625=Christian%20Kleiber%20and%20William%20H.%20Oliver%20and%20Tobias%20Buck&entry.1292438233=%20%20We%20present%20%24%5Ctexttt%7BLAMINAR%7D%24%2C%20a%20novel%20unsupervised%20machine%20learning%20pipeline%0Adesigned%20to%20enhance%20the%20representation%20of%20structure%20within%20data%20via%20producing%20a%0Amore-informative%20distance%20metric.%20Analysis%20methods%20in%20the%20physical%20sciences%0Aoften%20rely%20on%20standard%20metrics%20to%20define%20geometric%20relationships%20in%20data%2C%20which%0Amay%20fail%20to%20capture%20the%20underlying%20structure%20of%20complex%20data%20sets.%0A%24%5Ctexttt%7BLAMINAR%7D%24%20addresses%20this%20by%20using%20a%20continuous-normalising-flow%20and%0Ainverse-transform-sampling%20to%20define%20a%20Riemannian%20manifold%20in%20the%20data%20space%0Awithout%20the%20need%20for%20the%20user%20to%20specify%20a%20metric%20over%20the%20data%20a-priori.%20The%0Aresult%20is%20a%20locally-adaptive-metric%20that%20produces%20structurally-informative%0Adensity-based%20distances.%20We%20demonstrate%20the%20utility%20of%20%24%5Ctexttt%7BLAMINAR%7D%24%20by%0Acomparing%20its%20output%20to%20the%20Euclidean%20metric%20for%20structured%20data%20sets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08557v1&entry.124074799=Read"},
{"title": "The Limited Impact of Medical Adaptation of Large Language and\n  Vision-Language Models", "author": "Daniel P. Jeong and Pranav Mani and Saurabh Garg and Zachary C. Lipton and Michael Oberst", "abstract": "  Several recent works seek to develop foundation models specifically for\nmedical applications, adapting general-purpose large language models (LLMs) and\nvision-language models (VLMs) via continued pretraining on publicly available\nbiomedical corpora. These works typically claim that such domain-adaptive\npretraining (DAPT) improves performance on downstream medical tasks, such as\nanswering medical licensing exam questions. In this paper, we compare ten\npublic \"medical\" LLMs and two VLMs against their corresponding base models,\narriving at a different conclusion: all medical VLMs and nearly all medical\nLLMs fail to consistently improve over their base models in the zero-/few-shot\nprompting and supervised fine-tuning regimes for medical question-answering\n(QA). For instance, across all tasks and model pairs we consider in the 3-shot\nsetting, medical LLMs only outperform their base models in 22.7% of cases,\nreach a (statistical) tie in 36.8% of cases, and are significantly worse than\ntheir base models in the remaining 40.5% of cases. Our conclusions are based on\n(i) comparing each medical model head-to-head, directly against the\ncorresponding base model; (ii) optimizing the prompts for each model separately\nin zero-/few-shot prompting; and (iii) accounting for statistical uncertainty\nin comparisons. While these basic practices are not consistently adopted in the\nliterature, our ablations show that they substantially impact conclusions.\nMeanwhile, we find that after fine-tuning on specific QA tasks, medical LLMs\ncan show performance improvements, but the benefits do not carry over to tasks\nbased on clinical notes. Our findings suggest that state-of-the-art\ngeneral-domain models may already exhibit strong medical knowledge and\nreasoning capabilities, and offer recommendations to strengthen the conclusions\nof future studies.\n", "link": "http://arxiv.org/abs/2411.08870v1", "date": "2024-11-13", "relevancy": 2.7972, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5686}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5686}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5411}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Limited%20Impact%20of%20Medical%20Adaptation%20of%20Large%20Language%20and%0A%20%20Vision-Language%20Models&body=Title%3A%20The%20Limited%20Impact%20of%20Medical%20Adaptation%20of%20Large%20Language%20and%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Daniel%20P.%20Jeong%20and%20Pranav%20Mani%20and%20Saurabh%20Garg%20and%20Zachary%20C.%20Lipton%20and%20Michael%20Oberst%0AAbstract%3A%20%20%20Several%20recent%20works%20seek%20to%20develop%20foundation%20models%20specifically%20for%0Amedical%20applications%2C%20adapting%20general-purpose%20large%20language%20models%20%28LLMs%29%20and%0Avision-language%20models%20%28VLMs%29%20via%20continued%20pretraining%20on%20publicly%20available%0Abiomedical%20corpora.%20These%20works%20typically%20claim%20that%20such%20domain-adaptive%0Apretraining%20%28DAPT%29%20improves%20performance%20on%20downstream%20medical%20tasks%2C%20such%20as%0Aanswering%20medical%20licensing%20exam%20questions.%20In%20this%20paper%2C%20we%20compare%20ten%0Apublic%20%22medical%22%20LLMs%20and%20two%20VLMs%20against%20their%20corresponding%20base%20models%2C%0Aarriving%20at%20a%20different%20conclusion%3A%20all%20medical%20VLMs%20and%20nearly%20all%20medical%0ALLMs%20fail%20to%20consistently%20improve%20over%20their%20base%20models%20in%20the%20zero-/few-shot%0Aprompting%20and%20supervised%20fine-tuning%20regimes%20for%20medical%20question-answering%0A%28QA%29.%20For%20instance%2C%20across%20all%20tasks%20and%20model%20pairs%20we%20consider%20in%20the%203-shot%0Asetting%2C%20medical%20LLMs%20only%20outperform%20their%20base%20models%20in%2022.7%25%20of%20cases%2C%0Areach%20a%20%28statistical%29%20tie%20in%2036.8%25%20of%20cases%2C%20and%20are%20significantly%20worse%20than%0Atheir%20base%20models%20in%20the%20remaining%2040.5%25%20of%20cases.%20Our%20conclusions%20are%20based%20on%0A%28i%29%20comparing%20each%20medical%20model%20head-to-head%2C%20directly%20against%20the%0Acorresponding%20base%20model%3B%20%28ii%29%20optimizing%20the%20prompts%20for%20each%20model%20separately%0Ain%20zero-/few-shot%20prompting%3B%20and%20%28iii%29%20accounting%20for%20statistical%20uncertainty%0Ain%20comparisons.%20While%20these%20basic%20practices%20are%20not%20consistently%20adopted%20in%20the%0Aliterature%2C%20our%20ablations%20show%20that%20they%20substantially%20impact%20conclusions.%0AMeanwhile%2C%20we%20find%20that%20after%20fine-tuning%20on%20specific%20QA%20tasks%2C%20medical%20LLMs%0Acan%20show%20performance%20improvements%2C%20but%20the%20benefits%20do%20not%20carry%20over%20to%20tasks%0Abased%20on%20clinical%20notes.%20Our%20findings%20suggest%20that%20state-of-the-art%0Ageneral-domain%20models%20may%20already%20exhibit%20strong%20medical%20knowledge%20and%0Areasoning%20capabilities%2C%20and%20offer%20recommendations%20to%20strengthen%20the%20conclusions%0Aof%20future%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08870v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Limited%2520Impact%2520of%2520Medical%2520Adaptation%2520of%2520Large%2520Language%2520and%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DDaniel%2520P.%2520Jeong%2520and%2520Pranav%2520Mani%2520and%2520Saurabh%2520Garg%2520and%2520Zachary%2520C.%2520Lipton%2520and%2520Michael%2520Oberst%26entry.1292438233%3D%2520%2520Several%2520recent%2520works%2520seek%2520to%2520develop%2520foundation%2520models%2520specifically%2520for%250Amedical%2520applications%252C%2520adapting%2520general-purpose%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%250Avision-language%2520models%2520%2528VLMs%2529%2520via%2520continued%2520pretraining%2520on%2520publicly%2520available%250Abiomedical%2520corpora.%2520These%2520works%2520typically%2520claim%2520that%2520such%2520domain-adaptive%250Apretraining%2520%2528DAPT%2529%2520improves%2520performance%2520on%2520downstream%2520medical%2520tasks%252C%2520such%2520as%250Aanswering%2520medical%2520licensing%2520exam%2520questions.%2520In%2520this%2520paper%252C%2520we%2520compare%2520ten%250Apublic%2520%2522medical%2522%2520LLMs%2520and%2520two%2520VLMs%2520against%2520their%2520corresponding%2520base%2520models%252C%250Aarriving%2520at%2520a%2520different%2520conclusion%253A%2520all%2520medical%2520VLMs%2520and%2520nearly%2520all%2520medical%250ALLMs%2520fail%2520to%2520consistently%2520improve%2520over%2520their%2520base%2520models%2520in%2520the%2520zero-/few-shot%250Aprompting%2520and%2520supervised%2520fine-tuning%2520regimes%2520for%2520medical%2520question-answering%250A%2528QA%2529.%2520For%2520instance%252C%2520across%2520all%2520tasks%2520and%2520model%2520pairs%2520we%2520consider%2520in%2520the%25203-shot%250Asetting%252C%2520medical%2520LLMs%2520only%2520outperform%2520their%2520base%2520models%2520in%252022.7%2525%2520of%2520cases%252C%250Areach%2520a%2520%2528statistical%2529%2520tie%2520in%252036.8%2525%2520of%2520cases%252C%2520and%2520are%2520significantly%2520worse%2520than%250Atheir%2520base%2520models%2520in%2520the%2520remaining%252040.5%2525%2520of%2520cases.%2520Our%2520conclusions%2520are%2520based%2520on%250A%2528i%2529%2520comparing%2520each%2520medical%2520model%2520head-to-head%252C%2520directly%2520against%2520the%250Acorresponding%2520base%2520model%253B%2520%2528ii%2529%2520optimizing%2520the%2520prompts%2520for%2520each%2520model%2520separately%250Ain%2520zero-/few-shot%2520prompting%253B%2520and%2520%2528iii%2529%2520accounting%2520for%2520statistical%2520uncertainty%250Ain%2520comparisons.%2520While%2520these%2520basic%2520practices%2520are%2520not%2520consistently%2520adopted%2520in%2520the%250Aliterature%252C%2520our%2520ablations%2520show%2520that%2520they%2520substantially%2520impact%2520conclusions.%250AMeanwhile%252C%2520we%2520find%2520that%2520after%2520fine-tuning%2520on%2520specific%2520QA%2520tasks%252C%2520medical%2520LLMs%250Acan%2520show%2520performance%2520improvements%252C%2520but%2520the%2520benefits%2520do%2520not%2520carry%2520over%2520to%2520tasks%250Abased%2520on%2520clinical%2520notes.%2520Our%2520findings%2520suggest%2520that%2520state-of-the-art%250Ageneral-domain%2520models%2520may%2520already%2520exhibit%2520strong%2520medical%2520knowledge%2520and%250Areasoning%2520capabilities%252C%2520and%2520offer%2520recommendations%2520to%2520strengthen%2520the%2520conclusions%250Aof%2520future%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08870v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Limited%20Impact%20of%20Medical%20Adaptation%20of%20Large%20Language%20and%0A%20%20Vision-Language%20Models&entry.906535625=Daniel%20P.%20Jeong%20and%20Pranav%20Mani%20and%20Saurabh%20Garg%20and%20Zachary%20C.%20Lipton%20and%20Michael%20Oberst&entry.1292438233=%20%20Several%20recent%20works%20seek%20to%20develop%20foundation%20models%20specifically%20for%0Amedical%20applications%2C%20adapting%20general-purpose%20large%20language%20models%20%28LLMs%29%20and%0Avision-language%20models%20%28VLMs%29%20via%20continued%20pretraining%20on%20publicly%20available%0Abiomedical%20corpora.%20These%20works%20typically%20claim%20that%20such%20domain-adaptive%0Apretraining%20%28DAPT%29%20improves%20performance%20on%20downstream%20medical%20tasks%2C%20such%20as%0Aanswering%20medical%20licensing%20exam%20questions.%20In%20this%20paper%2C%20we%20compare%20ten%0Apublic%20%22medical%22%20LLMs%20and%20two%20VLMs%20against%20their%20corresponding%20base%20models%2C%0Aarriving%20at%20a%20different%20conclusion%3A%20all%20medical%20VLMs%20and%20nearly%20all%20medical%0ALLMs%20fail%20to%20consistently%20improve%20over%20their%20base%20models%20in%20the%20zero-/few-shot%0Aprompting%20and%20supervised%20fine-tuning%20regimes%20for%20medical%20question-answering%0A%28QA%29.%20For%20instance%2C%20across%20all%20tasks%20and%20model%20pairs%20we%20consider%20in%20the%203-shot%0Asetting%2C%20medical%20LLMs%20only%20outperform%20their%20base%20models%20in%2022.7%25%20of%20cases%2C%0Areach%20a%20%28statistical%29%20tie%20in%2036.8%25%20of%20cases%2C%20and%20are%20significantly%20worse%20than%0Atheir%20base%20models%20in%20the%20remaining%2040.5%25%20of%20cases.%20Our%20conclusions%20are%20based%20on%0A%28i%29%20comparing%20each%20medical%20model%20head-to-head%2C%20directly%20against%20the%0Acorresponding%20base%20model%3B%20%28ii%29%20optimizing%20the%20prompts%20for%20each%20model%20separately%0Ain%20zero-/few-shot%20prompting%3B%20and%20%28iii%29%20accounting%20for%20statistical%20uncertainty%0Ain%20comparisons.%20While%20these%20basic%20practices%20are%20not%20consistently%20adopted%20in%20the%0Aliterature%2C%20our%20ablations%20show%20that%20they%20substantially%20impact%20conclusions.%0AMeanwhile%2C%20we%20find%20that%20after%20fine-tuning%20on%20specific%20QA%20tasks%2C%20medical%20LLMs%0Acan%20show%20performance%20improvements%2C%20but%20the%20benefits%20do%20not%20carry%20over%20to%20tasks%0Abased%20on%20clinical%20notes.%20Our%20findings%20suggest%20that%20state-of-the-art%0Ageneral-domain%20models%20may%20already%20exhibit%20strong%20medical%20knowledge%20and%0Areasoning%20capabilities%2C%20and%20offer%20recommendations%20to%20strengthen%20the%20conclusions%0Aof%20future%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08870v1&entry.124074799=Read"},
{"title": "V-LoL: A Diagnostic Dataset for Visual Logical Learning", "author": "Lukas Helff and Wolfgang Stammer and Hikaru Shindo and Devendra Singh Dhami and Kristian Kersting", "abstract": "  Despite the successes of recent developments in visual AI, different\nshortcomings still exist; from missing exact logical reasoning, to abstract\ngeneralization abilities, to understanding complex and noisy scenes.\nUnfortunately, existing benchmarks, were not designed to capture more than a\nfew of these aspects. Whereas deep learning datasets focus on visually complex\ndata but simple visual reasoning tasks, inductive logic datasets involve\ncomplex logical learning tasks, however, lack the visual component. To address\nthis, we propose the diagnostic visual logical learning dataset, V-LoL, that\nseamlessly combines visual and logical challenges. Notably, we introduce the\nfirst instantiation of V-LoL, V-LoL-Train, - a visual rendition of a classic\nbenchmark in symbolic AI, the Michalski train problem. By incorporating\nintricate visual scenes and flexible logical reasoning tasks within a versatile\nframework, V-LoL-Train provides a platform for investigating a wide range of\nvisual logical learning challenges. We evaluate a variety of AI systems\nincluding traditional symbolic AI, neural AI, as well as neuro-symbolic AI. Our\nevaluations demonstrate that even SOTA AI faces difficulties in dealing with\nvisual logical learning challenges, highlighting unique advantages and\nlimitations of each methodology. Overall, V-LoL opens up new avenues for\nunderstanding and enhancing current abilities in visual logical learning for AI\nsystems.\n", "link": "http://arxiv.org/abs/2306.07743v3", "date": "2024-11-13", "relevancy": 2.7884, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5662}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5662}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5407}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V-LoL%3A%20A%20Diagnostic%20Dataset%20for%20Visual%20Logical%20Learning&body=Title%3A%20V-LoL%3A%20A%20Diagnostic%20Dataset%20for%20Visual%20Logical%20Learning%0AAuthor%3A%20Lukas%20Helff%20and%20Wolfgang%20Stammer%20and%20Hikaru%20Shindo%20and%20Devendra%20Singh%20Dhami%20and%20Kristian%20Kersting%0AAbstract%3A%20%20%20Despite%20the%20successes%20of%20recent%20developments%20in%20visual%20AI%2C%20different%0Ashortcomings%20still%20exist%3B%20from%20missing%20exact%20logical%20reasoning%2C%20to%20abstract%0Ageneralization%20abilities%2C%20to%20understanding%20complex%20and%20noisy%20scenes.%0AUnfortunately%2C%20existing%20benchmarks%2C%20were%20not%20designed%20to%20capture%20more%20than%20a%0Afew%20of%20these%20aspects.%20Whereas%20deep%20learning%20datasets%20focus%20on%20visually%20complex%0Adata%20but%20simple%20visual%20reasoning%20tasks%2C%20inductive%20logic%20datasets%20involve%0Acomplex%20logical%20learning%20tasks%2C%20however%2C%20lack%20the%20visual%20component.%20To%20address%0Athis%2C%20we%20propose%20the%20diagnostic%20visual%20logical%20learning%20dataset%2C%20V-LoL%2C%20that%0Aseamlessly%20combines%20visual%20and%20logical%20challenges.%20Notably%2C%20we%20introduce%20the%0Afirst%20instantiation%20of%20V-LoL%2C%20V-LoL-Train%2C%20-%20a%20visual%20rendition%20of%20a%20classic%0Abenchmark%20in%20symbolic%20AI%2C%20the%20Michalski%20train%20problem.%20By%20incorporating%0Aintricate%20visual%20scenes%20and%20flexible%20logical%20reasoning%20tasks%20within%20a%20versatile%0Aframework%2C%20V-LoL-Train%20provides%20a%20platform%20for%20investigating%20a%20wide%20range%20of%0Avisual%20logical%20learning%20challenges.%20We%20evaluate%20a%20variety%20of%20AI%20systems%0Aincluding%20traditional%20symbolic%20AI%2C%20neural%20AI%2C%20as%20well%20as%20neuro-symbolic%20AI.%20Our%0Aevaluations%20demonstrate%20that%20even%20SOTA%20AI%20faces%20difficulties%20in%20dealing%20with%0Avisual%20logical%20learning%20challenges%2C%20highlighting%20unique%20advantages%20and%0Alimitations%20of%20each%20methodology.%20Overall%2C%20V-LoL%20opens%20up%20new%20avenues%20for%0Aunderstanding%20and%20enhancing%20current%20abilities%20in%20visual%20logical%20learning%20for%20AI%0Asystems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.07743v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV-LoL%253A%2520A%2520Diagnostic%2520Dataset%2520for%2520Visual%2520Logical%2520Learning%26entry.906535625%3DLukas%2520Helff%2520and%2520Wolfgang%2520Stammer%2520and%2520Hikaru%2520Shindo%2520and%2520Devendra%2520Singh%2520Dhami%2520and%2520Kristian%2520Kersting%26entry.1292438233%3D%2520%2520Despite%2520the%2520successes%2520of%2520recent%2520developments%2520in%2520visual%2520AI%252C%2520different%250Ashortcomings%2520still%2520exist%253B%2520from%2520missing%2520exact%2520logical%2520reasoning%252C%2520to%2520abstract%250Ageneralization%2520abilities%252C%2520to%2520understanding%2520complex%2520and%2520noisy%2520scenes.%250AUnfortunately%252C%2520existing%2520benchmarks%252C%2520were%2520not%2520designed%2520to%2520capture%2520more%2520than%2520a%250Afew%2520of%2520these%2520aspects.%2520Whereas%2520deep%2520learning%2520datasets%2520focus%2520on%2520visually%2520complex%250Adata%2520but%2520simple%2520visual%2520reasoning%2520tasks%252C%2520inductive%2520logic%2520datasets%2520involve%250Acomplex%2520logical%2520learning%2520tasks%252C%2520however%252C%2520lack%2520the%2520visual%2520component.%2520To%2520address%250Athis%252C%2520we%2520propose%2520the%2520diagnostic%2520visual%2520logical%2520learning%2520dataset%252C%2520V-LoL%252C%2520that%250Aseamlessly%2520combines%2520visual%2520and%2520logical%2520challenges.%2520Notably%252C%2520we%2520introduce%2520the%250Afirst%2520instantiation%2520of%2520V-LoL%252C%2520V-LoL-Train%252C%2520-%2520a%2520visual%2520rendition%2520of%2520a%2520classic%250Abenchmark%2520in%2520symbolic%2520AI%252C%2520the%2520Michalski%2520train%2520problem.%2520By%2520incorporating%250Aintricate%2520visual%2520scenes%2520and%2520flexible%2520logical%2520reasoning%2520tasks%2520within%2520a%2520versatile%250Aframework%252C%2520V-LoL-Train%2520provides%2520a%2520platform%2520for%2520investigating%2520a%2520wide%2520range%2520of%250Avisual%2520logical%2520learning%2520challenges.%2520We%2520evaluate%2520a%2520variety%2520of%2520AI%2520systems%250Aincluding%2520traditional%2520symbolic%2520AI%252C%2520neural%2520AI%252C%2520as%2520well%2520as%2520neuro-symbolic%2520AI.%2520Our%250Aevaluations%2520demonstrate%2520that%2520even%2520SOTA%2520AI%2520faces%2520difficulties%2520in%2520dealing%2520with%250Avisual%2520logical%2520learning%2520challenges%252C%2520highlighting%2520unique%2520advantages%2520and%250Alimitations%2520of%2520each%2520methodology.%2520Overall%252C%2520V-LoL%2520opens%2520up%2520new%2520avenues%2520for%250Aunderstanding%2520and%2520enhancing%2520current%2520abilities%2520in%2520visual%2520logical%2520learning%2520for%2520AI%250Asystems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.07743v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V-LoL%3A%20A%20Diagnostic%20Dataset%20for%20Visual%20Logical%20Learning&entry.906535625=Lukas%20Helff%20and%20Wolfgang%20Stammer%20and%20Hikaru%20Shindo%20and%20Devendra%20Singh%20Dhami%20and%20Kristian%20Kersting&entry.1292438233=%20%20Despite%20the%20successes%20of%20recent%20developments%20in%20visual%20AI%2C%20different%0Ashortcomings%20still%20exist%3B%20from%20missing%20exact%20logical%20reasoning%2C%20to%20abstract%0Ageneralization%20abilities%2C%20to%20understanding%20complex%20and%20noisy%20scenes.%0AUnfortunately%2C%20existing%20benchmarks%2C%20were%20not%20designed%20to%20capture%20more%20than%20a%0Afew%20of%20these%20aspects.%20Whereas%20deep%20learning%20datasets%20focus%20on%20visually%20complex%0Adata%20but%20simple%20visual%20reasoning%20tasks%2C%20inductive%20logic%20datasets%20involve%0Acomplex%20logical%20learning%20tasks%2C%20however%2C%20lack%20the%20visual%20component.%20To%20address%0Athis%2C%20we%20propose%20the%20diagnostic%20visual%20logical%20learning%20dataset%2C%20V-LoL%2C%20that%0Aseamlessly%20combines%20visual%20and%20logical%20challenges.%20Notably%2C%20we%20introduce%20the%0Afirst%20instantiation%20of%20V-LoL%2C%20V-LoL-Train%2C%20-%20a%20visual%20rendition%20of%20a%20classic%0Abenchmark%20in%20symbolic%20AI%2C%20the%20Michalski%20train%20problem.%20By%20incorporating%0Aintricate%20visual%20scenes%20and%20flexible%20logical%20reasoning%20tasks%20within%20a%20versatile%0Aframework%2C%20V-LoL-Train%20provides%20a%20platform%20for%20investigating%20a%20wide%20range%20of%0Avisual%20logical%20learning%20challenges.%20We%20evaluate%20a%20variety%20of%20AI%20systems%0Aincluding%20traditional%20symbolic%20AI%2C%20neural%20AI%2C%20as%20well%20as%20neuro-symbolic%20AI.%20Our%0Aevaluations%20demonstrate%20that%20even%20SOTA%20AI%20faces%20difficulties%20in%20dealing%20with%0Avisual%20logical%20learning%20challenges%2C%20highlighting%20unique%20advantages%20and%0Alimitations%20of%20each%20methodology.%20Overall%2C%20V-LoL%20opens%20up%20new%20avenues%20for%0Aunderstanding%20and%20enhancing%20current%20abilities%20in%20visual%20logical%20learning%20for%20AI%0Asystems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.07743v3&entry.124074799=Read"},
{"title": "AstroM$^3$: A self-supervised multimodal model for astronomy", "author": "Mariia Rizhko and Joshua S. Bloom", "abstract": "  While machine-learned models are now routinely employed to facilitate\nastronomical inquiry, model inputs tend to be limited to a primary data source\n(namely images or time series) and, in the more advanced approaches, some\nmetadata. Yet with the growing use of wide-field, multiplexed observational\nresources, individual sources of interest often have a broad range of\nobservational modes available. Here we construct an astronomical multimodal\ndataset and propose AstroM$^3$, a self-supervised pre-training approach that\nenables a model to learn from multiple modalities simultaneously. Specifically,\nwe extend the CLIP (Contrastive Language-Image Pretraining) model to a trimodal\nsetting, allowing the integration of time-series photometry data, spectra, and\nastrophysical metadata. In a fine-tuning supervised setting, our results\ndemonstrate that CLIP pre-training improves classification performance for\ntime-series photometry, where accuracy increases from 84.6% to 91.5%.\nFurthermore, CLIP boosts classification accuracy by up to 12.6% when the\navailability of labeled data is limited, showing the effectiveness of\nleveraging larger corpora of unlabeled data. In addition to fine-tuned\nclassification, we can use the trained model in other downstream tasks that are\nnot explicitly contemplated during the construction of the self-supervised\nmodel. In particular we show the efficacy of using the learned embeddings for\nmisclassifications identification, similarity search, and anomaly detection.\nOne surprising highlight is the \"rediscovery\" of Mira subtypes and two\nRotational variable subclasses using manifold learning and dimension reduction\nalgorithm. To our knowledge this is the first construction of an $n>2$ mode\nmodel in astronomy. Extensions to $n>3$ modes is naturally anticipated with\nthis approach.\n", "link": "http://arxiv.org/abs/2411.08842v1", "date": "2024-11-13", "relevancy": 2.753, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6408}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5055}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5055}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AstroM%24%5E3%24%3A%20A%20self-supervised%20multimodal%20model%20for%20astronomy&body=Title%3A%20AstroM%24%5E3%24%3A%20A%20self-supervised%20multimodal%20model%20for%20astronomy%0AAuthor%3A%20Mariia%20Rizhko%20and%20Joshua%20S.%20Bloom%0AAbstract%3A%20%20%20While%20machine-learned%20models%20are%20now%20routinely%20employed%20to%20facilitate%0Aastronomical%20inquiry%2C%20model%20inputs%20tend%20to%20be%20limited%20to%20a%20primary%20data%20source%0A%28namely%20images%20or%20time%20series%29%20and%2C%20in%20the%20more%20advanced%20approaches%2C%20some%0Ametadata.%20Yet%20with%20the%20growing%20use%20of%20wide-field%2C%20multiplexed%20observational%0Aresources%2C%20individual%20sources%20of%20interest%20often%20have%20a%20broad%20range%20of%0Aobservational%20modes%20available.%20Here%20we%20construct%20an%20astronomical%20multimodal%0Adataset%20and%20propose%20AstroM%24%5E3%24%2C%20a%20self-supervised%20pre-training%20approach%20that%0Aenables%20a%20model%20to%20learn%20from%20multiple%20modalities%20simultaneously.%20Specifically%2C%0Awe%20extend%20the%20CLIP%20%28Contrastive%20Language-Image%20Pretraining%29%20model%20to%20a%20trimodal%0Asetting%2C%20allowing%20the%20integration%20of%20time-series%20photometry%20data%2C%20spectra%2C%20and%0Aastrophysical%20metadata.%20In%20a%20fine-tuning%20supervised%20setting%2C%20our%20results%0Ademonstrate%20that%20CLIP%20pre-training%20improves%20classification%20performance%20for%0Atime-series%20photometry%2C%20where%20accuracy%20increases%20from%2084.6%25%20to%2091.5%25.%0AFurthermore%2C%20CLIP%20boosts%20classification%20accuracy%20by%20up%20to%2012.6%25%20when%20the%0Aavailability%20of%20labeled%20data%20is%20limited%2C%20showing%20the%20effectiveness%20of%0Aleveraging%20larger%20corpora%20of%20unlabeled%20data.%20In%20addition%20to%20fine-tuned%0Aclassification%2C%20we%20can%20use%20the%20trained%20model%20in%20other%20downstream%20tasks%20that%20are%0Anot%20explicitly%20contemplated%20during%20the%20construction%20of%20the%20self-supervised%0Amodel.%20In%20particular%20we%20show%20the%20efficacy%20of%20using%20the%20learned%20embeddings%20for%0Amisclassifications%20identification%2C%20similarity%20search%2C%20and%20anomaly%20detection.%0AOne%20surprising%20highlight%20is%20the%20%22rediscovery%22%20of%20Mira%20subtypes%20and%20two%0ARotational%20variable%20subclasses%20using%20manifold%20learning%20and%20dimension%20reduction%0Aalgorithm.%20To%20our%20knowledge%20this%20is%20the%20first%20construction%20of%20an%20%24n%3E2%24%20mode%0Amodel%20in%20astronomy.%20Extensions%20to%20%24n%3E3%24%20modes%20is%20naturally%20anticipated%20with%0Athis%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08842v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAstroM%2524%255E3%2524%253A%2520A%2520self-supervised%2520multimodal%2520model%2520for%2520astronomy%26entry.906535625%3DMariia%2520Rizhko%2520and%2520Joshua%2520S.%2520Bloom%26entry.1292438233%3D%2520%2520While%2520machine-learned%2520models%2520are%2520now%2520routinely%2520employed%2520to%2520facilitate%250Aastronomical%2520inquiry%252C%2520model%2520inputs%2520tend%2520to%2520be%2520limited%2520to%2520a%2520primary%2520data%2520source%250A%2528namely%2520images%2520or%2520time%2520series%2529%2520and%252C%2520in%2520the%2520more%2520advanced%2520approaches%252C%2520some%250Ametadata.%2520Yet%2520with%2520the%2520growing%2520use%2520of%2520wide-field%252C%2520multiplexed%2520observational%250Aresources%252C%2520individual%2520sources%2520of%2520interest%2520often%2520have%2520a%2520broad%2520range%2520of%250Aobservational%2520modes%2520available.%2520Here%2520we%2520construct%2520an%2520astronomical%2520multimodal%250Adataset%2520and%2520propose%2520AstroM%2524%255E3%2524%252C%2520a%2520self-supervised%2520pre-training%2520approach%2520that%250Aenables%2520a%2520model%2520to%2520learn%2520from%2520multiple%2520modalities%2520simultaneously.%2520Specifically%252C%250Awe%2520extend%2520the%2520CLIP%2520%2528Contrastive%2520Language-Image%2520Pretraining%2529%2520model%2520to%2520a%2520trimodal%250Asetting%252C%2520allowing%2520the%2520integration%2520of%2520time-series%2520photometry%2520data%252C%2520spectra%252C%2520and%250Aastrophysical%2520metadata.%2520In%2520a%2520fine-tuning%2520supervised%2520setting%252C%2520our%2520results%250Ademonstrate%2520that%2520CLIP%2520pre-training%2520improves%2520classification%2520performance%2520for%250Atime-series%2520photometry%252C%2520where%2520accuracy%2520increases%2520from%252084.6%2525%2520to%252091.5%2525.%250AFurthermore%252C%2520CLIP%2520boosts%2520classification%2520accuracy%2520by%2520up%2520to%252012.6%2525%2520when%2520the%250Aavailability%2520of%2520labeled%2520data%2520is%2520limited%252C%2520showing%2520the%2520effectiveness%2520of%250Aleveraging%2520larger%2520corpora%2520of%2520unlabeled%2520data.%2520In%2520addition%2520to%2520fine-tuned%250Aclassification%252C%2520we%2520can%2520use%2520the%2520trained%2520model%2520in%2520other%2520downstream%2520tasks%2520that%2520are%250Anot%2520explicitly%2520contemplated%2520during%2520the%2520construction%2520of%2520the%2520self-supervised%250Amodel.%2520In%2520particular%2520we%2520show%2520the%2520efficacy%2520of%2520using%2520the%2520learned%2520embeddings%2520for%250Amisclassifications%2520identification%252C%2520similarity%2520search%252C%2520and%2520anomaly%2520detection.%250AOne%2520surprising%2520highlight%2520is%2520the%2520%2522rediscovery%2522%2520of%2520Mira%2520subtypes%2520and%2520two%250ARotational%2520variable%2520subclasses%2520using%2520manifold%2520learning%2520and%2520dimension%2520reduction%250Aalgorithm.%2520To%2520our%2520knowledge%2520this%2520is%2520the%2520first%2520construction%2520of%2520an%2520%2524n%253E2%2524%2520mode%250Amodel%2520in%2520astronomy.%2520Extensions%2520to%2520%2524n%253E3%2524%2520modes%2520is%2520naturally%2520anticipated%2520with%250Athis%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08842v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AstroM%24%5E3%24%3A%20A%20self-supervised%20multimodal%20model%20for%20astronomy&entry.906535625=Mariia%20Rizhko%20and%20Joshua%20S.%20Bloom&entry.1292438233=%20%20While%20machine-learned%20models%20are%20now%20routinely%20employed%20to%20facilitate%0Aastronomical%20inquiry%2C%20model%20inputs%20tend%20to%20be%20limited%20to%20a%20primary%20data%20source%0A%28namely%20images%20or%20time%20series%29%20and%2C%20in%20the%20more%20advanced%20approaches%2C%20some%0Ametadata.%20Yet%20with%20the%20growing%20use%20of%20wide-field%2C%20multiplexed%20observational%0Aresources%2C%20individual%20sources%20of%20interest%20often%20have%20a%20broad%20range%20of%0Aobservational%20modes%20available.%20Here%20we%20construct%20an%20astronomical%20multimodal%0Adataset%20and%20propose%20AstroM%24%5E3%24%2C%20a%20self-supervised%20pre-training%20approach%20that%0Aenables%20a%20model%20to%20learn%20from%20multiple%20modalities%20simultaneously.%20Specifically%2C%0Awe%20extend%20the%20CLIP%20%28Contrastive%20Language-Image%20Pretraining%29%20model%20to%20a%20trimodal%0Asetting%2C%20allowing%20the%20integration%20of%20time-series%20photometry%20data%2C%20spectra%2C%20and%0Aastrophysical%20metadata.%20In%20a%20fine-tuning%20supervised%20setting%2C%20our%20results%0Ademonstrate%20that%20CLIP%20pre-training%20improves%20classification%20performance%20for%0Atime-series%20photometry%2C%20where%20accuracy%20increases%20from%2084.6%25%20to%2091.5%25.%0AFurthermore%2C%20CLIP%20boosts%20classification%20accuracy%20by%20up%20to%2012.6%25%20when%20the%0Aavailability%20of%20labeled%20data%20is%20limited%2C%20showing%20the%20effectiveness%20of%0Aleveraging%20larger%20corpora%20of%20unlabeled%20data.%20In%20addition%20to%20fine-tuned%0Aclassification%2C%20we%20can%20use%20the%20trained%20model%20in%20other%20downstream%20tasks%20that%20are%0Anot%20explicitly%20contemplated%20during%20the%20construction%20of%20the%20self-supervised%0Amodel.%20In%20particular%20we%20show%20the%20efficacy%20of%20using%20the%20learned%20embeddings%20for%0Amisclassifications%20identification%2C%20similarity%20search%2C%20and%20anomaly%20detection.%0AOne%20surprising%20highlight%20is%20the%20%22rediscovery%22%20of%20Mira%20subtypes%20and%20two%0ARotational%20variable%20subclasses%20using%20manifold%20learning%20and%20dimension%20reduction%0Aalgorithm.%20To%20our%20knowledge%20this%20is%20the%20first%20construction%20of%20an%20%24n%3E2%24%20mode%0Amodel%20in%20astronomy.%20Extensions%20to%20%24n%3E3%24%20modes%20is%20naturally%20anticipated%20with%0Athis%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08842v1&entry.124074799=Read"},
{"title": "Gaussian Mixture Models Based Augmentation Enhances GNN Generalization", "author": "Yassine Abbahaddou and Fragkiskos D. Malliaros and Johannes F. Lutzeyer and Amine Mohamed Aboussalah and Michalis Vazirgiannis", "abstract": "  Graph Neural Networks (GNNs) have shown great promise in tasks like node and\ngraph classification, but they often struggle to generalize, particularly to\nunseen or out-of-distribution (OOD) data. These challenges are exacerbated when\ntraining data is limited in size or diversity. To address these issues, we\nintroduce a theoretical framework using Rademacher complexity to compute a\nregret bound on the generalization error and then characterize the effect of\ndata augmentation. This framework informs the design of GMM-GDA, an efficient\ngraph data augmentation (GDA) algorithm leveraging the capability of Gaussian\nMixture Models (GMMs) to approximate any distribution. Our approach not only\noutperforms existing augmentation techniques in terms of generalization but\nalso offers improved time complexity, making it highly suitable for real-world\napplications.\n", "link": "http://arxiv.org/abs/2411.08638v1", "date": "2024-11-13", "relevancy": 2.7267, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5618}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5375}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5367}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Mixture%20Models%20Based%20Augmentation%20Enhances%20GNN%20Generalization&body=Title%3A%20Gaussian%20Mixture%20Models%20Based%20Augmentation%20Enhances%20GNN%20Generalization%0AAuthor%3A%20Yassine%20Abbahaddou%20and%20Fragkiskos%20D.%20Malliaros%20and%20Johannes%20F.%20Lutzeyer%20and%20Amine%20Mohamed%20Aboussalah%20and%20Michalis%20Vazirgiannis%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20shown%20great%20promise%20in%20tasks%20like%20node%20and%0Agraph%20classification%2C%20but%20they%20often%20struggle%20to%20generalize%2C%20particularly%20to%0Aunseen%20or%20out-of-distribution%20%28OOD%29%20data.%20These%20challenges%20are%20exacerbated%20when%0Atraining%20data%20is%20limited%20in%20size%20or%20diversity.%20To%20address%20these%20issues%2C%20we%0Aintroduce%20a%20theoretical%20framework%20using%20Rademacher%20complexity%20to%20compute%20a%0Aregret%20bound%20on%20the%20generalization%20error%20and%20then%20characterize%20the%20effect%20of%0Adata%20augmentation.%20This%20framework%20informs%20the%20design%20of%20GMM-GDA%2C%20an%20efficient%0Agraph%20data%20augmentation%20%28GDA%29%20algorithm%20leveraging%20the%20capability%20of%20Gaussian%0AMixture%20Models%20%28GMMs%29%20to%20approximate%20any%20distribution.%20Our%20approach%20not%20only%0Aoutperforms%20existing%20augmentation%20techniques%20in%20terms%20of%20generalization%20but%0Aalso%20offers%20improved%20time%20complexity%2C%20making%20it%20highly%20suitable%20for%20real-world%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08638v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Mixture%2520Models%2520Based%2520Augmentation%2520Enhances%2520GNN%2520Generalization%26entry.906535625%3DYassine%2520Abbahaddou%2520and%2520Fragkiskos%2520D.%2520Malliaros%2520and%2520Johannes%2520F.%2520Lutzeyer%2520and%2520Amine%2520Mohamed%2520Aboussalah%2520and%2520Michalis%2520Vazirgiannis%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520shown%2520great%2520promise%2520in%2520tasks%2520like%2520node%2520and%250Agraph%2520classification%252C%2520but%2520they%2520often%2520struggle%2520to%2520generalize%252C%2520particularly%2520to%250Aunseen%2520or%2520out-of-distribution%2520%2528OOD%2529%2520data.%2520These%2520challenges%2520are%2520exacerbated%2520when%250Atraining%2520data%2520is%2520limited%2520in%2520size%2520or%2520diversity.%2520To%2520address%2520these%2520issues%252C%2520we%250Aintroduce%2520a%2520theoretical%2520framework%2520using%2520Rademacher%2520complexity%2520to%2520compute%2520a%250Aregret%2520bound%2520on%2520the%2520generalization%2520error%2520and%2520then%2520characterize%2520the%2520effect%2520of%250Adata%2520augmentation.%2520This%2520framework%2520informs%2520the%2520design%2520of%2520GMM-GDA%252C%2520an%2520efficient%250Agraph%2520data%2520augmentation%2520%2528GDA%2529%2520algorithm%2520leveraging%2520the%2520capability%2520of%2520Gaussian%250AMixture%2520Models%2520%2528GMMs%2529%2520to%2520approximate%2520any%2520distribution.%2520Our%2520approach%2520not%2520only%250Aoutperforms%2520existing%2520augmentation%2520techniques%2520in%2520terms%2520of%2520generalization%2520but%250Aalso%2520offers%2520improved%2520time%2520complexity%252C%2520making%2520it%2520highly%2520suitable%2520for%2520real-world%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08638v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Mixture%20Models%20Based%20Augmentation%20Enhances%20GNN%20Generalization&entry.906535625=Yassine%20Abbahaddou%20and%20Fragkiskos%20D.%20Malliaros%20and%20Johannes%20F.%20Lutzeyer%20and%20Amine%20Mohamed%20Aboussalah%20and%20Michalis%20Vazirgiannis&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20shown%20great%20promise%20in%20tasks%20like%20node%20and%0Agraph%20classification%2C%20but%20they%20often%20struggle%20to%20generalize%2C%20particularly%20to%0Aunseen%20or%20out-of-distribution%20%28OOD%29%20data.%20These%20challenges%20are%20exacerbated%20when%0Atraining%20data%20is%20limited%20in%20size%20or%20diversity.%20To%20address%20these%20issues%2C%20we%0Aintroduce%20a%20theoretical%20framework%20using%20Rademacher%20complexity%20to%20compute%20a%0Aregret%20bound%20on%20the%20generalization%20error%20and%20then%20characterize%20the%20effect%20of%0Adata%20augmentation.%20This%20framework%20informs%20the%20design%20of%20GMM-GDA%2C%20an%20efficient%0Agraph%20data%20augmentation%20%28GDA%29%20algorithm%20leveraging%20the%20capability%20of%20Gaussian%0AMixture%20Models%20%28GMMs%29%20to%20approximate%20any%20distribution.%20Our%20approach%20not%20only%0Aoutperforms%20existing%20augmentation%20techniques%20in%20terms%20of%20generalization%20but%0Aalso%20offers%20improved%20time%20complexity%2C%20making%20it%20highly%20suitable%20for%20real-world%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08638v1&entry.124074799=Read"},
{"title": "Evaluating World Models with LLM for Decision Making", "author": "Chang Yang and Xinrun Wang and Junzhe Jiang and Qinggang Zhang and Xiao Huang", "abstract": "  World model emerges as a key module in decision making, where MuZero and\nDreamer achieve remarkable successes in complex tasks. Recent work leverages\nLarge Language Models (LLMs) as general world simulators to simulate the\ndynamics of the world due to their generalizability. LLMs also serve as the\nworld model for deliberative reasoning in Reasoning via Planning (RAP) and Tree\nof Thought (ToT). However, the world models are either evaluated as a general\nworld simulator, or as a functional module of the agent, i.e., predicting the\ntransitions to assist the planning. In this work, we propose a comprehensive\nevaluation of the world models with LLMs from the decision making perspective.\nSpecifically, we leverage the 31 diverse environments from (Wang et al.,\n2023;2024) and curate the rule-based policy of each environment for the diverse\nevaluation. Then, we design three main tasks, i.e., policy verification, action\nproposal, and policy planning, where the world models can be used for decision\nmaking solely. Finally, we conduct the comprehensive evaluation of the advanced\nLLMs, i.e., GPT-4o and GPT-4o-mini, on the environments for the three main\ntasks under various settings. The key observations include: i) GPT-4o\nsignificantly outperforms GPT-4o-mini on the three main tasks, especially for\nthe tasks which require the domain knowledge, ii) the performance of the world\nmodel with LLM will be decreased for long-term decision-making tasks, and iii)\nthe combination of different functionalities of the world model will brings\nadditional unstabilities of the performance.\n", "link": "http://arxiv.org/abs/2411.08794v1", "date": "2024-11-13", "relevancy": 2.7049, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5556}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5556}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5118}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20World%20Models%20with%20LLM%20for%20Decision%20Making&body=Title%3A%20Evaluating%20World%20Models%20with%20LLM%20for%20Decision%20Making%0AAuthor%3A%20Chang%20Yang%20and%20Xinrun%20Wang%20and%20Junzhe%20Jiang%20and%20Qinggang%20Zhang%20and%20Xiao%20Huang%0AAbstract%3A%20%20%20World%20model%20emerges%20as%20a%20key%20module%20in%20decision%20making%2C%20where%20MuZero%20and%0ADreamer%20achieve%20remarkable%20successes%20in%20complex%20tasks.%20Recent%20work%20leverages%0ALarge%20Language%20Models%20%28LLMs%29%20as%20general%20world%20simulators%20to%20simulate%20the%0Adynamics%20of%20the%20world%20due%20to%20their%20generalizability.%20LLMs%20also%20serve%20as%20the%0Aworld%20model%20for%20deliberative%20reasoning%20in%20Reasoning%20via%20Planning%20%28RAP%29%20and%20Tree%0Aof%20Thought%20%28ToT%29.%20However%2C%20the%20world%20models%20are%20either%20evaluated%20as%20a%20general%0Aworld%20simulator%2C%20or%20as%20a%20functional%20module%20of%20the%20agent%2C%20i.e.%2C%20predicting%20the%0Atransitions%20to%20assist%20the%20planning.%20In%20this%20work%2C%20we%20propose%20a%20comprehensive%0Aevaluation%20of%20the%20world%20models%20with%20LLMs%20from%20the%20decision%20making%20perspective.%0ASpecifically%2C%20we%20leverage%20the%2031%20diverse%20environments%20from%20%28Wang%20et%20al.%2C%0A2023%3B2024%29%20and%20curate%20the%20rule-based%20policy%20of%20each%20environment%20for%20the%20diverse%0Aevaluation.%20Then%2C%20we%20design%20three%20main%20tasks%2C%20i.e.%2C%20policy%20verification%2C%20action%0Aproposal%2C%20and%20policy%20planning%2C%20where%20the%20world%20models%20can%20be%20used%20for%20decision%0Amaking%20solely.%20Finally%2C%20we%20conduct%20the%20comprehensive%20evaluation%20of%20the%20advanced%0ALLMs%2C%20i.e.%2C%20GPT-4o%20and%20GPT-4o-mini%2C%20on%20the%20environments%20for%20the%20three%20main%0Atasks%20under%20various%20settings.%20The%20key%20observations%20include%3A%20i%29%20GPT-4o%0Asignificantly%20outperforms%20GPT-4o-mini%20on%20the%20three%20main%20tasks%2C%20especially%20for%0Athe%20tasks%20which%20require%20the%20domain%20knowledge%2C%20ii%29%20the%20performance%20of%20the%20world%0Amodel%20with%20LLM%20will%20be%20decreased%20for%20long-term%20decision-making%20tasks%2C%20and%20iii%29%0Athe%20combination%20of%20different%20functionalities%20of%20the%20world%20model%20will%20brings%0Aadditional%20unstabilities%20of%20the%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08794v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520World%2520Models%2520with%2520LLM%2520for%2520Decision%2520Making%26entry.906535625%3DChang%2520Yang%2520and%2520Xinrun%2520Wang%2520and%2520Junzhe%2520Jiang%2520and%2520Qinggang%2520Zhang%2520and%2520Xiao%2520Huang%26entry.1292438233%3D%2520%2520World%2520model%2520emerges%2520as%2520a%2520key%2520module%2520in%2520decision%2520making%252C%2520where%2520MuZero%2520and%250ADreamer%2520achieve%2520remarkable%2520successes%2520in%2520complex%2520tasks.%2520Recent%2520work%2520leverages%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520as%2520general%2520world%2520simulators%2520to%2520simulate%2520the%250Adynamics%2520of%2520the%2520world%2520due%2520to%2520their%2520generalizability.%2520LLMs%2520also%2520serve%2520as%2520the%250Aworld%2520model%2520for%2520deliberative%2520reasoning%2520in%2520Reasoning%2520via%2520Planning%2520%2528RAP%2529%2520and%2520Tree%250Aof%2520Thought%2520%2528ToT%2529.%2520However%252C%2520the%2520world%2520models%2520are%2520either%2520evaluated%2520as%2520a%2520general%250Aworld%2520simulator%252C%2520or%2520as%2520a%2520functional%2520module%2520of%2520the%2520agent%252C%2520i.e.%252C%2520predicting%2520the%250Atransitions%2520to%2520assist%2520the%2520planning.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520comprehensive%250Aevaluation%2520of%2520the%2520world%2520models%2520with%2520LLMs%2520from%2520the%2520decision%2520making%2520perspective.%250ASpecifically%252C%2520we%2520leverage%2520the%252031%2520diverse%2520environments%2520from%2520%2528Wang%2520et%2520al.%252C%250A2023%253B2024%2529%2520and%2520curate%2520the%2520rule-based%2520policy%2520of%2520each%2520environment%2520for%2520the%2520diverse%250Aevaluation.%2520Then%252C%2520we%2520design%2520three%2520main%2520tasks%252C%2520i.e.%252C%2520policy%2520verification%252C%2520action%250Aproposal%252C%2520and%2520policy%2520planning%252C%2520where%2520the%2520world%2520models%2520can%2520be%2520used%2520for%2520decision%250Amaking%2520solely.%2520Finally%252C%2520we%2520conduct%2520the%2520comprehensive%2520evaluation%2520of%2520the%2520advanced%250ALLMs%252C%2520i.e.%252C%2520GPT-4o%2520and%2520GPT-4o-mini%252C%2520on%2520the%2520environments%2520for%2520the%2520three%2520main%250Atasks%2520under%2520various%2520settings.%2520The%2520key%2520observations%2520include%253A%2520i%2529%2520GPT-4o%250Asignificantly%2520outperforms%2520GPT-4o-mini%2520on%2520the%2520three%2520main%2520tasks%252C%2520especially%2520for%250Athe%2520tasks%2520which%2520require%2520the%2520domain%2520knowledge%252C%2520ii%2529%2520the%2520performance%2520of%2520the%2520world%250Amodel%2520with%2520LLM%2520will%2520be%2520decreased%2520for%2520long-term%2520decision-making%2520tasks%252C%2520and%2520iii%2529%250Athe%2520combination%2520of%2520different%2520functionalities%2520of%2520the%2520world%2520model%2520will%2520brings%250Aadditional%2520unstabilities%2520of%2520the%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08794v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20World%20Models%20with%20LLM%20for%20Decision%20Making&entry.906535625=Chang%20Yang%20and%20Xinrun%20Wang%20and%20Junzhe%20Jiang%20and%20Qinggang%20Zhang%20and%20Xiao%20Huang&entry.1292438233=%20%20World%20model%20emerges%20as%20a%20key%20module%20in%20decision%20making%2C%20where%20MuZero%20and%0ADreamer%20achieve%20remarkable%20successes%20in%20complex%20tasks.%20Recent%20work%20leverages%0ALarge%20Language%20Models%20%28LLMs%29%20as%20general%20world%20simulators%20to%20simulate%20the%0Adynamics%20of%20the%20world%20due%20to%20their%20generalizability.%20LLMs%20also%20serve%20as%20the%0Aworld%20model%20for%20deliberative%20reasoning%20in%20Reasoning%20via%20Planning%20%28RAP%29%20and%20Tree%0Aof%20Thought%20%28ToT%29.%20However%2C%20the%20world%20models%20are%20either%20evaluated%20as%20a%20general%0Aworld%20simulator%2C%20or%20as%20a%20functional%20module%20of%20the%20agent%2C%20i.e.%2C%20predicting%20the%0Atransitions%20to%20assist%20the%20planning.%20In%20this%20work%2C%20we%20propose%20a%20comprehensive%0Aevaluation%20of%20the%20world%20models%20with%20LLMs%20from%20the%20decision%20making%20perspective.%0ASpecifically%2C%20we%20leverage%20the%2031%20diverse%20environments%20from%20%28Wang%20et%20al.%2C%0A2023%3B2024%29%20and%20curate%20the%20rule-based%20policy%20of%20each%20environment%20for%20the%20diverse%0Aevaluation.%20Then%2C%20we%20design%20three%20main%20tasks%2C%20i.e.%2C%20policy%20verification%2C%20action%0Aproposal%2C%20and%20policy%20planning%2C%20where%20the%20world%20models%20can%20be%20used%20for%20decision%0Amaking%20solely.%20Finally%2C%20we%20conduct%20the%20comprehensive%20evaluation%20of%20the%20advanced%0ALLMs%2C%20i.e.%2C%20GPT-4o%20and%20GPT-4o-mini%2C%20on%20the%20environments%20for%20the%20three%20main%0Atasks%20under%20various%20settings.%20The%20key%20observations%20include%3A%20i%29%20GPT-4o%0Asignificantly%20outperforms%20GPT-4o-mini%20on%20the%20three%20main%20tasks%2C%20especially%20for%0Athe%20tasks%20which%20require%20the%20domain%20knowledge%2C%20ii%29%20the%20performance%20of%20the%20world%0Amodel%20with%20LLM%20will%20be%20decreased%20for%20long-term%20decision-making%20tasks%2C%20and%20iii%29%0Athe%20combination%20of%20different%20functionalities%20of%20the%20world%20model%20will%20brings%0Aadditional%20unstabilities%20of%20the%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08794v1&entry.124074799=Read"},
{"title": "SLYKLatent: A Learning Framework for Gaze Estimation Using Deep Facial\n  Feature Learning", "author": "Samuel Adebayo and Joost C. Dessing and Se\u00e1n McLoone", "abstract": "  In this research, we present SLYKLatent, a novel approach for enhancing gaze\nestimation by addressing appearance instability challenges in datasets due to\naleatoric uncertainties, covariant shifts, and test domain generalization.\nSLYKLatent utilizes Self-Supervised Learning for initial training with facial\nexpression datasets, followed by refinement with a patch-based tri-branch\nnetwork and an inverse explained variance-weighted training loss function. Our\nevaluation on benchmark datasets achieves a 10.9% improvement on Gaze360,\nsupersedes top MPIIFaceGaze results with 3.8%, and leads on a subset of\nETH-XGaze by 11.6%, surpassing existing methods by significant margins.\nAdaptability tests on RAF-DB and Affectnet show 86.4% and 60.9% accuracies,\nrespectively. Ablation studies confirm the effectiveness of SLYKLatent's novel\ncomponents.\n", "link": "http://arxiv.org/abs/2402.01555v2", "date": "2024-11-13", "relevancy": 2.7008, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5472}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5434}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SLYKLatent%3A%20A%20Learning%20Framework%20for%20Gaze%20Estimation%20Using%20Deep%20Facial%0A%20%20Feature%20Learning&body=Title%3A%20SLYKLatent%3A%20A%20Learning%20Framework%20for%20Gaze%20Estimation%20Using%20Deep%20Facial%0A%20%20Feature%20Learning%0AAuthor%3A%20Samuel%20Adebayo%20and%20Joost%20C.%20Dessing%20and%20Se%C3%A1n%20McLoone%0AAbstract%3A%20%20%20In%20this%20research%2C%20we%20present%20SLYKLatent%2C%20a%20novel%20approach%20for%20enhancing%20gaze%0Aestimation%20by%20addressing%20appearance%20instability%20challenges%20in%20datasets%20due%20to%0Aaleatoric%20uncertainties%2C%20covariant%20shifts%2C%20and%20test%20domain%20generalization.%0ASLYKLatent%20utilizes%20Self-Supervised%20Learning%20for%20initial%20training%20with%20facial%0Aexpression%20datasets%2C%20followed%20by%20refinement%20with%20a%20patch-based%20tri-branch%0Anetwork%20and%20an%20inverse%20explained%20variance-weighted%20training%20loss%20function.%20Our%0Aevaluation%20on%20benchmark%20datasets%20achieves%20a%2010.9%25%20improvement%20on%20Gaze360%2C%0Asupersedes%20top%20MPIIFaceGaze%20results%20with%203.8%25%2C%20and%20leads%20on%20a%20subset%20of%0AETH-XGaze%20by%2011.6%25%2C%20surpassing%20existing%20methods%20by%20significant%20margins.%0AAdaptability%20tests%20on%20RAF-DB%20and%20Affectnet%20show%2086.4%25%20and%2060.9%25%20accuracies%2C%0Arespectively.%20Ablation%20studies%20confirm%20the%20effectiveness%20of%20SLYKLatent%27s%20novel%0Acomponents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01555v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSLYKLatent%253A%2520A%2520Learning%2520Framework%2520for%2520Gaze%2520Estimation%2520Using%2520Deep%2520Facial%250A%2520%2520Feature%2520Learning%26entry.906535625%3DSamuel%2520Adebayo%2520and%2520Joost%2520C.%2520Dessing%2520and%2520Se%25C3%25A1n%2520McLoone%26entry.1292438233%3D%2520%2520In%2520this%2520research%252C%2520we%2520present%2520SLYKLatent%252C%2520a%2520novel%2520approach%2520for%2520enhancing%2520gaze%250Aestimation%2520by%2520addressing%2520appearance%2520instability%2520challenges%2520in%2520datasets%2520due%2520to%250Aaleatoric%2520uncertainties%252C%2520covariant%2520shifts%252C%2520and%2520test%2520domain%2520generalization.%250ASLYKLatent%2520utilizes%2520Self-Supervised%2520Learning%2520for%2520initial%2520training%2520with%2520facial%250Aexpression%2520datasets%252C%2520followed%2520by%2520refinement%2520with%2520a%2520patch-based%2520tri-branch%250Anetwork%2520and%2520an%2520inverse%2520explained%2520variance-weighted%2520training%2520loss%2520function.%2520Our%250Aevaluation%2520on%2520benchmark%2520datasets%2520achieves%2520a%252010.9%2525%2520improvement%2520on%2520Gaze360%252C%250Asupersedes%2520top%2520MPIIFaceGaze%2520results%2520with%25203.8%2525%252C%2520and%2520leads%2520on%2520a%2520subset%2520of%250AETH-XGaze%2520by%252011.6%2525%252C%2520surpassing%2520existing%2520methods%2520by%2520significant%2520margins.%250AAdaptability%2520tests%2520on%2520RAF-DB%2520and%2520Affectnet%2520show%252086.4%2525%2520and%252060.9%2525%2520accuracies%252C%250Arespectively.%2520Ablation%2520studies%2520confirm%2520the%2520effectiveness%2520of%2520SLYKLatent%2527s%2520novel%250Acomponents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.01555v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SLYKLatent%3A%20A%20Learning%20Framework%20for%20Gaze%20Estimation%20Using%20Deep%20Facial%0A%20%20Feature%20Learning&entry.906535625=Samuel%20Adebayo%20and%20Joost%20C.%20Dessing%20and%20Se%C3%A1n%20McLoone&entry.1292438233=%20%20In%20this%20research%2C%20we%20present%20SLYKLatent%2C%20a%20novel%20approach%20for%20enhancing%20gaze%0Aestimation%20by%20addressing%20appearance%20instability%20challenges%20in%20datasets%20due%20to%0Aaleatoric%20uncertainties%2C%20covariant%20shifts%2C%20and%20test%20domain%20generalization.%0ASLYKLatent%20utilizes%20Self-Supervised%20Learning%20for%20initial%20training%20with%20facial%0Aexpression%20datasets%2C%20followed%20by%20refinement%20with%20a%20patch-based%20tri-branch%0Anetwork%20and%20an%20inverse%20explained%20variance-weighted%20training%20loss%20function.%20Our%0Aevaluation%20on%20benchmark%20datasets%20achieves%20a%2010.9%25%20improvement%20on%20Gaze360%2C%0Asupersedes%20top%20MPIIFaceGaze%20results%20with%203.8%25%2C%20and%20leads%20on%20a%20subset%20of%0AETH-XGaze%20by%2011.6%25%2C%20surpassing%20existing%20methods%20by%20significant%20margins.%0AAdaptability%20tests%20on%20RAF-DB%20and%20Affectnet%20show%2086.4%25%20and%2060.9%25%20accuracies%2C%0Arespectively.%20Ablation%20studies%20confirm%20the%20effectiveness%20of%20SLYKLatent%27s%20novel%0Acomponents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01555v2&entry.124074799=Read"},
{"title": "Snakes and Ladders: Two Steps Up for VideoMamba", "author": "Hui Lu and Albert Ali Salah and Ronald Poppe", "abstract": "  Video understanding requires the extraction of rich spatio-temporal\nrepresentations, which transformer models achieve through self-attention.\nUnfortunately, self-attention poses a computational burden. In NLP, Mamba has\nsurfaced as an efficient alternative for transformers. However, Mamba's\nsuccesses do not trivially extend to vision tasks, including those in video\nanalysis. In this paper, we theoretically analyze the differences between\nself-attention and Mamba. We identify two limitations in Mamba's token\nprocessing: historical decay and element contradiction. We propose\nVideoMambaPro (VMP) that solves the identified limitations by adding masked\nbackward computation and elemental residual connections to a VideoMamba\nbackbone. Differently sized VideoMambaPro models surpass VideoMamba by 1.6-2.8%\nand 1.1-1.9% top-1 on Kinetics-400 and Something-Something V2, respectively.\nEven without extensive pre-training, our models present an increasingly\nattractive and efficient alternative to current transformer models. Moreover,\nour two solutions are orthogonal to recent advances in Vision Mamba models, and\nare likely to provide further improvements in future models.\n", "link": "http://arxiv.org/abs/2406.19006v4", "date": "2024-11-13", "relevancy": 2.661, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.541}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5278}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5278}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Snakes%20and%20Ladders%3A%20Two%20Steps%20Up%20for%20VideoMamba&body=Title%3A%20Snakes%20and%20Ladders%3A%20Two%20Steps%20Up%20for%20VideoMamba%0AAuthor%3A%20Hui%20Lu%20and%20Albert%20Ali%20Salah%20and%20Ronald%20Poppe%0AAbstract%3A%20%20%20Video%20understanding%20requires%20the%20extraction%20of%20rich%20spatio-temporal%0Arepresentations%2C%20which%20transformer%20models%20achieve%20through%20self-attention.%0AUnfortunately%2C%20self-attention%20poses%20a%20computational%20burden.%20In%20NLP%2C%20Mamba%20has%0Asurfaced%20as%20an%20efficient%20alternative%20for%20transformers.%20However%2C%20Mamba%27s%0Asuccesses%20do%20not%20trivially%20extend%20to%20vision%20tasks%2C%20including%20those%20in%20video%0Aanalysis.%20In%20this%20paper%2C%20we%20theoretically%20analyze%20the%20differences%20between%0Aself-attention%20and%20Mamba.%20We%20identify%20two%20limitations%20in%20Mamba%27s%20token%0Aprocessing%3A%20historical%20decay%20and%20element%20contradiction.%20We%20propose%0AVideoMambaPro%20%28VMP%29%20that%20solves%20the%20identified%20limitations%20by%20adding%20masked%0Abackward%20computation%20and%20elemental%20residual%20connections%20to%20a%20VideoMamba%0Abackbone.%20Differently%20sized%20VideoMambaPro%20models%20surpass%20VideoMamba%20by%201.6-2.8%25%0Aand%201.1-1.9%25%20top-1%20on%20Kinetics-400%20and%20Something-Something%20V2%2C%20respectively.%0AEven%20without%20extensive%20pre-training%2C%20our%20models%20present%20an%20increasingly%0Aattractive%20and%20efficient%20alternative%20to%20current%20transformer%20models.%20Moreover%2C%0Aour%20two%20solutions%20are%20orthogonal%20to%20recent%20advances%20in%20Vision%20Mamba%20models%2C%20and%0Aare%20likely%20to%20provide%20further%20improvements%20in%20future%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19006v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSnakes%2520and%2520Ladders%253A%2520Two%2520Steps%2520Up%2520for%2520VideoMamba%26entry.906535625%3DHui%2520Lu%2520and%2520Albert%2520Ali%2520Salah%2520and%2520Ronald%2520Poppe%26entry.1292438233%3D%2520%2520Video%2520understanding%2520requires%2520the%2520extraction%2520of%2520rich%2520spatio-temporal%250Arepresentations%252C%2520which%2520transformer%2520models%2520achieve%2520through%2520self-attention.%250AUnfortunately%252C%2520self-attention%2520poses%2520a%2520computational%2520burden.%2520In%2520NLP%252C%2520Mamba%2520has%250Asurfaced%2520as%2520an%2520efficient%2520alternative%2520for%2520transformers.%2520However%252C%2520Mamba%2527s%250Asuccesses%2520do%2520not%2520trivially%2520extend%2520to%2520vision%2520tasks%252C%2520including%2520those%2520in%2520video%250Aanalysis.%2520In%2520this%2520paper%252C%2520we%2520theoretically%2520analyze%2520the%2520differences%2520between%250Aself-attention%2520and%2520Mamba.%2520We%2520identify%2520two%2520limitations%2520in%2520Mamba%2527s%2520token%250Aprocessing%253A%2520historical%2520decay%2520and%2520element%2520contradiction.%2520We%2520propose%250AVideoMambaPro%2520%2528VMP%2529%2520that%2520solves%2520the%2520identified%2520limitations%2520by%2520adding%2520masked%250Abackward%2520computation%2520and%2520elemental%2520residual%2520connections%2520to%2520a%2520VideoMamba%250Abackbone.%2520Differently%2520sized%2520VideoMambaPro%2520models%2520surpass%2520VideoMamba%2520by%25201.6-2.8%2525%250Aand%25201.1-1.9%2525%2520top-1%2520on%2520Kinetics-400%2520and%2520Something-Something%2520V2%252C%2520respectively.%250AEven%2520without%2520extensive%2520pre-training%252C%2520our%2520models%2520present%2520an%2520increasingly%250Aattractive%2520and%2520efficient%2520alternative%2520to%2520current%2520transformer%2520models.%2520Moreover%252C%250Aour%2520two%2520solutions%2520are%2520orthogonal%2520to%2520recent%2520advances%2520in%2520Vision%2520Mamba%2520models%252C%2520and%250Aare%2520likely%2520to%2520provide%2520further%2520improvements%2520in%2520future%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19006v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Snakes%20and%20Ladders%3A%20Two%20Steps%20Up%20for%20VideoMamba&entry.906535625=Hui%20Lu%20and%20Albert%20Ali%20Salah%20and%20Ronald%20Poppe&entry.1292438233=%20%20Video%20understanding%20requires%20the%20extraction%20of%20rich%20spatio-temporal%0Arepresentations%2C%20which%20transformer%20models%20achieve%20through%20self-attention.%0AUnfortunately%2C%20self-attention%20poses%20a%20computational%20burden.%20In%20NLP%2C%20Mamba%20has%0Asurfaced%20as%20an%20efficient%20alternative%20for%20transformers.%20However%2C%20Mamba%27s%0Asuccesses%20do%20not%20trivially%20extend%20to%20vision%20tasks%2C%20including%20those%20in%20video%0Aanalysis.%20In%20this%20paper%2C%20we%20theoretically%20analyze%20the%20differences%20between%0Aself-attention%20and%20Mamba.%20We%20identify%20two%20limitations%20in%20Mamba%27s%20token%0Aprocessing%3A%20historical%20decay%20and%20element%20contradiction.%20We%20propose%0AVideoMambaPro%20%28VMP%29%20that%20solves%20the%20identified%20limitations%20by%20adding%20masked%0Abackward%20computation%20and%20elemental%20residual%20connections%20to%20a%20VideoMamba%0Abackbone.%20Differently%20sized%20VideoMambaPro%20models%20surpass%20VideoMamba%20by%201.6-2.8%25%0Aand%201.1-1.9%25%20top-1%20on%20Kinetics-400%20and%20Something-Something%20V2%2C%20respectively.%0AEven%20without%20extensive%20pre-training%2C%20our%20models%20present%20an%20increasingly%0Aattractive%20and%20efficient%20alternative%20to%20current%20transformer%20models.%20Moreover%2C%0Aour%20two%20solutions%20are%20orthogonal%20to%20recent%20advances%20in%20Vision%20Mamba%20models%2C%20and%0Aare%20likely%20to%20provide%20further%20improvements%20in%20future%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19006v4&entry.124074799=Read"},
{"title": "BoQ: A Place is Worth a Bag of Learnable Queries", "author": "Amar Ali-Bey and Brahim Chaib-draa and Philippe Gigu\u00e8re", "abstract": "  In visual place recognition, accurately identifying and matching images of\nlocations under varying environmental conditions and viewpoints remains a\nsignificant challenge. In this paper, we introduce a new technique, called\nBag-of-Queries (BoQ), which learns a set of global queries designed to capture\nuniversal place-specific attributes. Unlike existing methods that employ\nself-attention and generate the queries directly from the input features, BoQ\nemploys distinct learnable global queries, which probe the input features via\ncross-attention, ensuring consistent information aggregation. In addition, our\ntechnique provides an interpretable attention mechanism and integrates with\nboth CNN and Vision Transformer backbones. The performance of BoQ is\ndemonstrated through extensive experiments on 14 large-scale benchmarks. It\nconsistently outperforms current state-of-the-art techniques including NetVLAD,\nMixVPR and EigenPlaces. Moreover, as a global retrieval technique (one-stage),\nBoQ surpasses two-stage retrieval methods, such as Patch-NetVLAD, TransVPR and\nR2Former, all while being orders of magnitude faster and more efficient. The\ncode and model weights are publicly available at\nhttps://github.com/amaralibey/Bag-of-Queries.\n", "link": "http://arxiv.org/abs/2405.07364v3", "date": "2024-11-13", "relevancy": 2.6152, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5399}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.515}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5141}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BoQ%3A%20A%20Place%20is%20Worth%20a%20Bag%20of%20Learnable%20Queries&body=Title%3A%20BoQ%3A%20A%20Place%20is%20Worth%20a%20Bag%20of%20Learnable%20Queries%0AAuthor%3A%20Amar%20Ali-Bey%20and%20Brahim%20Chaib-draa%20and%20Philippe%20Gigu%C3%A8re%0AAbstract%3A%20%20%20In%20visual%20place%20recognition%2C%20accurately%20identifying%20and%20matching%20images%20of%0Alocations%20under%20varying%20environmental%20conditions%20and%20viewpoints%20remains%20a%0Asignificant%20challenge.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20technique%2C%20called%0ABag-of-Queries%20%28BoQ%29%2C%20which%20learns%20a%20set%20of%20global%20queries%20designed%20to%20capture%0Auniversal%20place-specific%20attributes.%20Unlike%20existing%20methods%20that%20employ%0Aself-attention%20and%20generate%20the%20queries%20directly%20from%20the%20input%20features%2C%20BoQ%0Aemploys%20distinct%20learnable%20global%20queries%2C%20which%20probe%20the%20input%20features%20via%0Across-attention%2C%20ensuring%20consistent%20information%20aggregation.%20In%20addition%2C%20our%0Atechnique%20provides%20an%20interpretable%20attention%20mechanism%20and%20integrates%20with%0Aboth%20CNN%20and%20Vision%20Transformer%20backbones.%20The%20performance%20of%20BoQ%20is%0Ademonstrated%20through%20extensive%20experiments%20on%2014%20large-scale%20benchmarks.%20It%0Aconsistently%20outperforms%20current%20state-of-the-art%20techniques%20including%20NetVLAD%2C%0AMixVPR%20and%20EigenPlaces.%20Moreover%2C%20as%20a%20global%20retrieval%20technique%20%28one-stage%29%2C%0ABoQ%20surpasses%20two-stage%20retrieval%20methods%2C%20such%20as%20Patch-NetVLAD%2C%20TransVPR%20and%0AR2Former%2C%20all%20while%20being%20orders%20of%20magnitude%20faster%20and%20more%20efficient.%20The%0Acode%20and%20model%20weights%20are%20publicly%20available%20at%0Ahttps%3A//github.com/amaralibey/Bag-of-Queries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.07364v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoQ%253A%2520A%2520Place%2520is%2520Worth%2520a%2520Bag%2520of%2520Learnable%2520Queries%26entry.906535625%3DAmar%2520Ali-Bey%2520and%2520Brahim%2520Chaib-draa%2520and%2520Philippe%2520Gigu%25C3%25A8re%26entry.1292438233%3D%2520%2520In%2520visual%2520place%2520recognition%252C%2520accurately%2520identifying%2520and%2520matching%2520images%2520of%250Alocations%2520under%2520varying%2520environmental%2520conditions%2520and%2520viewpoints%2520remains%2520a%250Asignificant%2520challenge.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%2520technique%252C%2520called%250ABag-of-Queries%2520%2528BoQ%2529%252C%2520which%2520learns%2520a%2520set%2520of%2520global%2520queries%2520designed%2520to%2520capture%250Auniversal%2520place-specific%2520attributes.%2520Unlike%2520existing%2520methods%2520that%2520employ%250Aself-attention%2520and%2520generate%2520the%2520queries%2520directly%2520from%2520the%2520input%2520features%252C%2520BoQ%250Aemploys%2520distinct%2520learnable%2520global%2520queries%252C%2520which%2520probe%2520the%2520input%2520features%2520via%250Across-attention%252C%2520ensuring%2520consistent%2520information%2520aggregation.%2520In%2520addition%252C%2520our%250Atechnique%2520provides%2520an%2520interpretable%2520attention%2520mechanism%2520and%2520integrates%2520with%250Aboth%2520CNN%2520and%2520Vision%2520Transformer%2520backbones.%2520The%2520performance%2520of%2520BoQ%2520is%250Ademonstrated%2520through%2520extensive%2520experiments%2520on%252014%2520large-scale%2520benchmarks.%2520It%250Aconsistently%2520outperforms%2520current%2520state-of-the-art%2520techniques%2520including%2520NetVLAD%252C%250AMixVPR%2520and%2520EigenPlaces.%2520Moreover%252C%2520as%2520a%2520global%2520retrieval%2520technique%2520%2528one-stage%2529%252C%250ABoQ%2520surpasses%2520two-stage%2520retrieval%2520methods%252C%2520such%2520as%2520Patch-NetVLAD%252C%2520TransVPR%2520and%250AR2Former%252C%2520all%2520while%2520being%2520orders%2520of%2520magnitude%2520faster%2520and%2520more%2520efficient.%2520The%250Acode%2520and%2520model%2520weights%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/amaralibey/Bag-of-Queries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.07364v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BoQ%3A%20A%20Place%20is%20Worth%20a%20Bag%20of%20Learnable%20Queries&entry.906535625=Amar%20Ali-Bey%20and%20Brahim%20Chaib-draa%20and%20Philippe%20Gigu%C3%A8re&entry.1292438233=%20%20In%20visual%20place%20recognition%2C%20accurately%20identifying%20and%20matching%20images%20of%0Alocations%20under%20varying%20environmental%20conditions%20and%20viewpoints%20remains%20a%0Asignificant%20challenge.%20In%20this%20paper%2C%20we%20introduce%20a%20new%20technique%2C%20called%0ABag-of-Queries%20%28BoQ%29%2C%20which%20learns%20a%20set%20of%20global%20queries%20designed%20to%20capture%0Auniversal%20place-specific%20attributes.%20Unlike%20existing%20methods%20that%20employ%0Aself-attention%20and%20generate%20the%20queries%20directly%20from%20the%20input%20features%2C%20BoQ%0Aemploys%20distinct%20learnable%20global%20queries%2C%20which%20probe%20the%20input%20features%20via%0Across-attention%2C%20ensuring%20consistent%20information%20aggregation.%20In%20addition%2C%20our%0Atechnique%20provides%20an%20interpretable%20attention%20mechanism%20and%20integrates%20with%0Aboth%20CNN%20and%20Vision%20Transformer%20backbones.%20The%20performance%20of%20BoQ%20is%0Ademonstrated%20through%20extensive%20experiments%20on%2014%20large-scale%20benchmarks.%20It%0Aconsistently%20outperforms%20current%20state-of-the-art%20techniques%20including%20NetVLAD%2C%0AMixVPR%20and%20EigenPlaces.%20Moreover%2C%20as%20a%20global%20retrieval%20technique%20%28one-stage%29%2C%0ABoQ%20surpasses%20two-stage%20retrieval%20methods%2C%20such%20as%20Patch-NetVLAD%2C%20TransVPR%20and%0AR2Former%2C%20all%20while%20being%20orders%20of%20magnitude%20faster%20and%20more%20efficient.%20The%0Acode%20and%20model%20weights%20are%20publicly%20available%20at%0Ahttps%3A//github.com/amaralibey/Bag-of-Queries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.07364v3&entry.124074799=Read"},
{"title": "Separating Tongue from Thought: Activation Patching Reveals\n  Language-Agnostic Concept Representations in Transformers", "author": "Cl\u00e9ment Dumas and Chris Wendler and Veniamin Veselovsky and Giovanni Monea and Robert West", "abstract": "  A central question in multilingual language modeling is whether large\nlanguage models (LLMs) develop a universal concept representation, disentangled\nfrom specific languages. In this paper, we address this question by analyzing\nlatent representations (latents) during a word translation task in\ntransformer-based LLMs. We strategically extract latents from a source\ntranslation prompt and insert them into the forward pass on a target\ntranslation prompt. By doing so, we find that the output language is encoded in\nthe latent at an earlier layer than the concept to be translated. Building on\nthis insight, we conduct two key experiments. First, we demonstrate that we can\nchange the concept without changing the language and vice versa through\nactivation patching alone. Second, we show that patching with the mean over\nlatents across different languages does not impair and instead improves the\nmodels' performance in translating the concept. Our results provide evidence\nfor the existence of language-agnostic concept representations within the\ninvestigated models.\n", "link": "http://arxiv.org/abs/2411.08745v1", "date": "2024-11-13", "relevancy": 2.6105, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5281}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5281}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.51}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Separating%20Tongue%20from%20Thought%3A%20Activation%20Patching%20Reveals%0A%20%20Language-Agnostic%20Concept%20Representations%20in%20Transformers&body=Title%3A%20Separating%20Tongue%20from%20Thought%3A%20Activation%20Patching%20Reveals%0A%20%20Language-Agnostic%20Concept%20Representations%20in%20Transformers%0AAuthor%3A%20Cl%C3%A9ment%20Dumas%20and%20Chris%20Wendler%20and%20Veniamin%20Veselovsky%20and%20Giovanni%20Monea%20and%20Robert%20West%0AAbstract%3A%20%20%20A%20central%20question%20in%20multilingual%20language%20modeling%20is%20whether%20large%0Alanguage%20models%20%28LLMs%29%20develop%20a%20universal%20concept%20representation%2C%20disentangled%0Afrom%20specific%20languages.%20In%20this%20paper%2C%20we%20address%20this%20question%20by%20analyzing%0Alatent%20representations%20%28latents%29%20during%20a%20word%20translation%20task%20in%0Atransformer-based%20LLMs.%20We%20strategically%20extract%20latents%20from%20a%20source%0Atranslation%20prompt%20and%20insert%20them%20into%20the%20forward%20pass%20on%20a%20target%0Atranslation%20prompt.%20By%20doing%20so%2C%20we%20find%20that%20the%20output%20language%20is%20encoded%20in%0Athe%20latent%20at%20an%20earlier%20layer%20than%20the%20concept%20to%20be%20translated.%20Building%20on%0Athis%20insight%2C%20we%20conduct%20two%20key%20experiments.%20First%2C%20we%20demonstrate%20that%20we%20can%0Achange%20the%20concept%20without%20changing%20the%20language%20and%20vice%20versa%20through%0Aactivation%20patching%20alone.%20Second%2C%20we%20show%20that%20patching%20with%20the%20mean%20over%0Alatents%20across%20different%20languages%20does%20not%20impair%20and%20instead%20improves%20the%0Amodels%27%20performance%20in%20translating%20the%20concept.%20Our%20results%20provide%20evidence%0Afor%20the%20existence%20of%20language-agnostic%20concept%20representations%20within%20the%0Ainvestigated%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08745v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSeparating%2520Tongue%2520from%2520Thought%253A%2520Activation%2520Patching%2520Reveals%250A%2520%2520Language-Agnostic%2520Concept%2520Representations%2520in%2520Transformers%26entry.906535625%3DCl%25C3%25A9ment%2520Dumas%2520and%2520Chris%2520Wendler%2520and%2520Veniamin%2520Veselovsky%2520and%2520Giovanni%2520Monea%2520and%2520Robert%2520West%26entry.1292438233%3D%2520%2520A%2520central%2520question%2520in%2520multilingual%2520language%2520modeling%2520is%2520whether%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520develop%2520a%2520universal%2520concept%2520representation%252C%2520disentangled%250Afrom%2520specific%2520languages.%2520In%2520this%2520paper%252C%2520we%2520address%2520this%2520question%2520by%2520analyzing%250Alatent%2520representations%2520%2528latents%2529%2520during%2520a%2520word%2520translation%2520task%2520in%250Atransformer-based%2520LLMs.%2520We%2520strategically%2520extract%2520latents%2520from%2520a%2520source%250Atranslation%2520prompt%2520and%2520insert%2520them%2520into%2520the%2520forward%2520pass%2520on%2520a%2520target%250Atranslation%2520prompt.%2520By%2520doing%2520so%252C%2520we%2520find%2520that%2520the%2520output%2520language%2520is%2520encoded%2520in%250Athe%2520latent%2520at%2520an%2520earlier%2520layer%2520than%2520the%2520concept%2520to%2520be%2520translated.%2520Building%2520on%250Athis%2520insight%252C%2520we%2520conduct%2520two%2520key%2520experiments.%2520First%252C%2520we%2520demonstrate%2520that%2520we%2520can%250Achange%2520the%2520concept%2520without%2520changing%2520the%2520language%2520and%2520vice%2520versa%2520through%250Aactivation%2520patching%2520alone.%2520Second%252C%2520we%2520show%2520that%2520patching%2520with%2520the%2520mean%2520over%250Alatents%2520across%2520different%2520languages%2520does%2520not%2520impair%2520and%2520instead%2520improves%2520the%250Amodels%2527%2520performance%2520in%2520translating%2520the%2520concept.%2520Our%2520results%2520provide%2520evidence%250Afor%2520the%2520existence%2520of%2520language-agnostic%2520concept%2520representations%2520within%2520the%250Ainvestigated%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08745v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Separating%20Tongue%20from%20Thought%3A%20Activation%20Patching%20Reveals%0A%20%20Language-Agnostic%20Concept%20Representations%20in%20Transformers&entry.906535625=Cl%C3%A9ment%20Dumas%20and%20Chris%20Wendler%20and%20Veniamin%20Veselovsky%20and%20Giovanni%20Monea%20and%20Robert%20West&entry.1292438233=%20%20A%20central%20question%20in%20multilingual%20language%20modeling%20is%20whether%20large%0Alanguage%20models%20%28LLMs%29%20develop%20a%20universal%20concept%20representation%2C%20disentangled%0Afrom%20specific%20languages.%20In%20this%20paper%2C%20we%20address%20this%20question%20by%20analyzing%0Alatent%20representations%20%28latents%29%20during%20a%20word%20translation%20task%20in%0Atransformer-based%20LLMs.%20We%20strategically%20extract%20latents%20from%20a%20source%0Atranslation%20prompt%20and%20insert%20them%20into%20the%20forward%20pass%20on%20a%20target%0Atranslation%20prompt.%20By%20doing%20so%2C%20we%20find%20that%20the%20output%20language%20is%20encoded%20in%0Athe%20latent%20at%20an%20earlier%20layer%20than%20the%20concept%20to%20be%20translated.%20Building%20on%0Athis%20insight%2C%20we%20conduct%20two%20key%20experiments.%20First%2C%20we%20demonstrate%20that%20we%20can%0Achange%20the%20concept%20without%20changing%20the%20language%20and%20vice%20versa%20through%0Aactivation%20patching%20alone.%20Second%2C%20we%20show%20that%20patching%20with%20the%20mean%20over%0Alatents%20across%20different%20languages%20does%20not%20impair%20and%20instead%20improves%20the%0Amodels%27%20performance%20in%20translating%20the%20concept.%20Our%20results%20provide%20evidence%0Afor%20the%20existence%20of%20language-agnostic%20concept%20representations%20within%20the%0Ainvestigated%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08745v1&entry.124074799=Read"},
{"title": "Retrieval Augmented Recipe Generation", "author": "Guoshan Liu and Hailong Yin and Bin Zhu and Jingjing Chen and Chong-Wah Ngo and Yu-Gang Jiang", "abstract": "  Given the potential applications of generating recipes from food images, this\narea has garnered significant attention from researchers in recent years.\nExisting works for recipe generation primarily utilize a two-stage training\nmethod, first generating ingredients and then obtaining instructions from both\nthe image and ingredients. Large Multi-modal Models (LMMs), which have achieved\nnotable success across a variety of vision and language tasks, shed light to\ngenerating both ingredients and instructions directly from images.\nNevertheless, LMMs still face the common issue of hallucinations during recipe\ngeneration, leading to suboptimal performance. To tackle this, we propose a\nretrieval augmented large multimodal model for recipe generation. We first\nintroduce Stochastic Diversified Retrieval Augmentation (SDRA) to retrieve\nrecipes semantically related to the image from an existing datastore as a\nsupplement, integrating them into the prompt to add diverse and rich context to\nthe input image. Additionally, Self-Consistency Ensemble Voting mechanism is\nproposed to determine the most confident prediction recipes as the final\noutput. It calculates the consistency among generated recipe candidates, which\nuse different retrieval recipes as context for generation. Extensive\nexperiments validate the effectiveness of our proposed method, which\ndemonstrates state-of-the-art (SOTA) performance in recipe generation tasks on\nthe Recipe1M dataset.\n", "link": "http://arxiv.org/abs/2411.08715v1", "date": "2024-11-13", "relevancy": 2.5709, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5204}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.517}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Retrieval%20Augmented%20Recipe%20Generation&body=Title%3A%20Retrieval%20Augmented%20Recipe%20Generation%0AAuthor%3A%20Guoshan%20Liu%20and%20Hailong%20Yin%20and%20Bin%20Zhu%20and%20Jingjing%20Chen%20and%20Chong-Wah%20Ngo%20and%20Yu-Gang%20Jiang%0AAbstract%3A%20%20%20Given%20the%20potential%20applications%20of%20generating%20recipes%20from%20food%20images%2C%20this%0Aarea%20has%20garnered%20significant%20attention%20from%20researchers%20in%20recent%20years.%0AExisting%20works%20for%20recipe%20generation%20primarily%20utilize%20a%20two-stage%20training%0Amethod%2C%20first%20generating%20ingredients%20and%20then%20obtaining%20instructions%20from%20both%0Athe%20image%20and%20ingredients.%20Large%20Multi-modal%20Models%20%28LMMs%29%2C%20which%20have%20achieved%0Anotable%20success%20across%20a%20variety%20of%20vision%20and%20language%20tasks%2C%20shed%20light%20to%0Agenerating%20both%20ingredients%20and%20instructions%20directly%20from%20images.%0ANevertheless%2C%20LMMs%20still%20face%20the%20common%20issue%20of%20hallucinations%20during%20recipe%0Ageneration%2C%20leading%20to%20suboptimal%20performance.%20To%20tackle%20this%2C%20we%20propose%20a%0Aretrieval%20augmented%20large%20multimodal%20model%20for%20recipe%20generation.%20We%20first%0Aintroduce%20Stochastic%20Diversified%20Retrieval%20Augmentation%20%28SDRA%29%20to%20retrieve%0Arecipes%20semantically%20related%20to%20the%20image%20from%20an%20existing%20datastore%20as%20a%0Asupplement%2C%20integrating%20them%20into%20the%20prompt%20to%20add%20diverse%20and%20rich%20context%20to%0Athe%20input%20image.%20Additionally%2C%20Self-Consistency%20Ensemble%20Voting%20mechanism%20is%0Aproposed%20to%20determine%20the%20most%20confident%20prediction%20recipes%20as%20the%20final%0Aoutput.%20It%20calculates%20the%20consistency%20among%20generated%20recipe%20candidates%2C%20which%0Ause%20different%20retrieval%20recipes%20as%20context%20for%20generation.%20Extensive%0Aexperiments%20validate%20the%20effectiveness%20of%20our%20proposed%20method%2C%20which%0Ademonstrates%20state-of-the-art%20%28SOTA%29%20performance%20in%20recipe%20generation%20tasks%20on%0Athe%20Recipe1M%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08715v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRetrieval%2520Augmented%2520Recipe%2520Generation%26entry.906535625%3DGuoshan%2520Liu%2520and%2520Hailong%2520Yin%2520and%2520Bin%2520Zhu%2520and%2520Jingjing%2520Chen%2520and%2520Chong-Wah%2520Ngo%2520and%2520Yu-Gang%2520Jiang%26entry.1292438233%3D%2520%2520Given%2520the%2520potential%2520applications%2520of%2520generating%2520recipes%2520from%2520food%2520images%252C%2520this%250Aarea%2520has%2520garnered%2520significant%2520attention%2520from%2520researchers%2520in%2520recent%2520years.%250AExisting%2520works%2520for%2520recipe%2520generation%2520primarily%2520utilize%2520a%2520two-stage%2520training%250Amethod%252C%2520first%2520generating%2520ingredients%2520and%2520then%2520obtaining%2520instructions%2520from%2520both%250Athe%2520image%2520and%2520ingredients.%2520Large%2520Multi-modal%2520Models%2520%2528LMMs%2529%252C%2520which%2520have%2520achieved%250Anotable%2520success%2520across%2520a%2520variety%2520of%2520vision%2520and%2520language%2520tasks%252C%2520shed%2520light%2520to%250Agenerating%2520both%2520ingredients%2520and%2520instructions%2520directly%2520from%2520images.%250ANevertheless%252C%2520LMMs%2520still%2520face%2520the%2520common%2520issue%2520of%2520hallucinations%2520during%2520recipe%250Ageneration%252C%2520leading%2520to%2520suboptimal%2520performance.%2520To%2520tackle%2520this%252C%2520we%2520propose%2520a%250Aretrieval%2520augmented%2520large%2520multimodal%2520model%2520for%2520recipe%2520generation.%2520We%2520first%250Aintroduce%2520Stochastic%2520Diversified%2520Retrieval%2520Augmentation%2520%2528SDRA%2529%2520to%2520retrieve%250Arecipes%2520semantically%2520related%2520to%2520the%2520image%2520from%2520an%2520existing%2520datastore%2520as%2520a%250Asupplement%252C%2520integrating%2520them%2520into%2520the%2520prompt%2520to%2520add%2520diverse%2520and%2520rich%2520context%2520to%250Athe%2520input%2520image.%2520Additionally%252C%2520Self-Consistency%2520Ensemble%2520Voting%2520mechanism%2520is%250Aproposed%2520to%2520determine%2520the%2520most%2520confident%2520prediction%2520recipes%2520as%2520the%2520final%250Aoutput.%2520It%2520calculates%2520the%2520consistency%2520among%2520generated%2520recipe%2520candidates%252C%2520which%250Ause%2520different%2520retrieval%2520recipes%2520as%2520context%2520for%2520generation.%2520Extensive%250Aexperiments%2520validate%2520the%2520effectiveness%2520of%2520our%2520proposed%2520method%252C%2520which%250Ademonstrates%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520in%2520recipe%2520generation%2520tasks%2520on%250Athe%2520Recipe1M%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08715v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retrieval%20Augmented%20Recipe%20Generation&entry.906535625=Guoshan%20Liu%20and%20Hailong%20Yin%20and%20Bin%20Zhu%20and%20Jingjing%20Chen%20and%20Chong-Wah%20Ngo%20and%20Yu-Gang%20Jiang&entry.1292438233=%20%20Given%20the%20potential%20applications%20of%20generating%20recipes%20from%20food%20images%2C%20this%0Aarea%20has%20garnered%20significant%20attention%20from%20researchers%20in%20recent%20years.%0AExisting%20works%20for%20recipe%20generation%20primarily%20utilize%20a%20two-stage%20training%0Amethod%2C%20first%20generating%20ingredients%20and%20then%20obtaining%20instructions%20from%20both%0Athe%20image%20and%20ingredients.%20Large%20Multi-modal%20Models%20%28LMMs%29%2C%20which%20have%20achieved%0Anotable%20success%20across%20a%20variety%20of%20vision%20and%20language%20tasks%2C%20shed%20light%20to%0Agenerating%20both%20ingredients%20and%20instructions%20directly%20from%20images.%0ANevertheless%2C%20LMMs%20still%20face%20the%20common%20issue%20of%20hallucinations%20during%20recipe%0Ageneration%2C%20leading%20to%20suboptimal%20performance.%20To%20tackle%20this%2C%20we%20propose%20a%0Aretrieval%20augmented%20large%20multimodal%20model%20for%20recipe%20generation.%20We%20first%0Aintroduce%20Stochastic%20Diversified%20Retrieval%20Augmentation%20%28SDRA%29%20to%20retrieve%0Arecipes%20semantically%20related%20to%20the%20image%20from%20an%20existing%20datastore%20as%20a%0Asupplement%2C%20integrating%20them%20into%20the%20prompt%20to%20add%20diverse%20and%20rich%20context%20to%0Athe%20input%20image.%20Additionally%2C%20Self-Consistency%20Ensemble%20Voting%20mechanism%20is%0Aproposed%20to%20determine%20the%20most%20confident%20prediction%20recipes%20as%20the%20final%0Aoutput.%20It%20calculates%20the%20consistency%20among%20generated%20recipe%20candidates%2C%20which%0Ause%20different%20retrieval%20recipes%20as%20context%20for%20generation.%20Extensive%0Aexperiments%20validate%20the%20effectiveness%20of%20our%20proposed%20method%2C%20which%0Ademonstrates%20state-of-the-art%20%28SOTA%29%20performance%20in%20recipe%20generation%20tasks%20on%0Athe%20Recipe1M%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08715v1&entry.124074799=Read"},
{"title": "Hopfield-Fenchel-Young Networks: A Unified Framework for Associative\n  Memory Retrieval", "author": "Saul Santos and Vlad Niculae and Daniel McNamee and Andr\u00e9 F. T. Martins", "abstract": "  Associative memory models, such as Hopfield networks and their modern\nvariants, have garnered renewed interest due to advancements in memory capacity\nand connections with self-attention in transformers. In this work, we introduce\na unified framework-Hopfield-Fenchel-Young networks-which generalizes these\nmodels to a broader family of energy functions. Our energies are formulated as\nthe difference between two Fenchel-Young losses: one, parameterized by a\ngeneralized entropy, defines the Hopfield scoring mechanism, while the other\napplies a post-transformation to the Hopfield output. By utilizing Tsallis and\nnorm entropies, we derive end-to-end differentiable update rules that enable\nsparse transformations, uncovering new connections between loss margins,\nsparsity, and exact retrieval of single memory patterns. We further extend this\nframework to structured Hopfield networks using the SparseMAP transformation,\nallowing the retrieval of pattern associations rather than a single pattern.\nOur framework unifies and extends traditional and modern Hopfield networks and\nprovides an energy minimization perspective for widely used\npost-transformations like $\\ell_2$-normalization and layer normalization-all\nthrough suitable choices of Fenchel-Young losses and by using convex analysis\nas a building block. Finally, we validate our Hopfield-Fenchel-Young networks\non diverse memory recall tasks, including free and sequential recall.\nExperiments on simulated data, image retrieval, multiple instance learning, and\ntext rationalization demonstrate the effectiveness of our approach.\n", "link": "http://arxiv.org/abs/2411.08590v1", "date": "2024-11-13", "relevancy": 2.5566, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5199}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5071}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hopfield-Fenchel-Young%20Networks%3A%20A%20Unified%20Framework%20for%20Associative%0A%20%20Memory%20Retrieval&body=Title%3A%20Hopfield-Fenchel-Young%20Networks%3A%20A%20Unified%20Framework%20for%20Associative%0A%20%20Memory%20Retrieval%0AAuthor%3A%20Saul%20Santos%20and%20Vlad%20Niculae%20and%20Daniel%20McNamee%20and%20Andr%C3%A9%20F.%20T.%20Martins%0AAbstract%3A%20%20%20Associative%20memory%20models%2C%20such%20as%20Hopfield%20networks%20and%20their%20modern%0Avariants%2C%20have%20garnered%20renewed%20interest%20due%20to%20advancements%20in%20memory%20capacity%0Aand%20connections%20with%20self-attention%20in%20transformers.%20In%20this%20work%2C%20we%20introduce%0Aa%20unified%20framework-Hopfield-Fenchel-Young%20networks-which%20generalizes%20these%0Amodels%20to%20a%20broader%20family%20of%20energy%20functions.%20Our%20energies%20are%20formulated%20as%0Athe%20difference%20between%20two%20Fenchel-Young%20losses%3A%20one%2C%20parameterized%20by%20a%0Ageneralized%20entropy%2C%20defines%20the%20Hopfield%20scoring%20mechanism%2C%20while%20the%20other%0Aapplies%20a%20post-transformation%20to%20the%20Hopfield%20output.%20By%20utilizing%20Tsallis%20and%0Anorm%20entropies%2C%20we%20derive%20end-to-end%20differentiable%20update%20rules%20that%20enable%0Asparse%20transformations%2C%20uncovering%20new%20connections%20between%20loss%20margins%2C%0Asparsity%2C%20and%20exact%20retrieval%20of%20single%20memory%20patterns.%20We%20further%20extend%20this%0Aframework%20to%20structured%20Hopfield%20networks%20using%20the%20SparseMAP%20transformation%2C%0Aallowing%20the%20retrieval%20of%20pattern%20associations%20rather%20than%20a%20single%20pattern.%0AOur%20framework%20unifies%20and%20extends%20traditional%20and%20modern%20Hopfield%20networks%20and%0Aprovides%20an%20energy%20minimization%20perspective%20for%20widely%20used%0Apost-transformations%20like%20%24%5Cell_2%24-normalization%20and%20layer%20normalization-all%0Athrough%20suitable%20choices%20of%20Fenchel-Young%20losses%20and%20by%20using%20convex%20analysis%0Aas%20a%20building%20block.%20Finally%2C%20we%20validate%20our%20Hopfield-Fenchel-Young%20networks%0Aon%20diverse%20memory%20recall%20tasks%2C%20including%20free%20and%20sequential%20recall.%0AExperiments%20on%20simulated%20data%2C%20image%20retrieval%2C%20multiple%20instance%20learning%2C%20and%0Atext%20rationalization%20demonstrate%20the%20effectiveness%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08590v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHopfield-Fenchel-Young%2520Networks%253A%2520A%2520Unified%2520Framework%2520for%2520Associative%250A%2520%2520Memory%2520Retrieval%26entry.906535625%3DSaul%2520Santos%2520and%2520Vlad%2520Niculae%2520and%2520Daniel%2520McNamee%2520and%2520Andr%25C3%25A9%2520F.%2520T.%2520Martins%26entry.1292438233%3D%2520%2520Associative%2520memory%2520models%252C%2520such%2520as%2520Hopfield%2520networks%2520and%2520their%2520modern%250Avariants%252C%2520have%2520garnered%2520renewed%2520interest%2520due%2520to%2520advancements%2520in%2520memory%2520capacity%250Aand%2520connections%2520with%2520self-attention%2520in%2520transformers.%2520In%2520this%2520work%252C%2520we%2520introduce%250Aa%2520unified%2520framework-Hopfield-Fenchel-Young%2520networks-which%2520generalizes%2520these%250Amodels%2520to%2520a%2520broader%2520family%2520of%2520energy%2520functions.%2520Our%2520energies%2520are%2520formulated%2520as%250Athe%2520difference%2520between%2520two%2520Fenchel-Young%2520losses%253A%2520one%252C%2520parameterized%2520by%2520a%250Ageneralized%2520entropy%252C%2520defines%2520the%2520Hopfield%2520scoring%2520mechanism%252C%2520while%2520the%2520other%250Aapplies%2520a%2520post-transformation%2520to%2520the%2520Hopfield%2520output.%2520By%2520utilizing%2520Tsallis%2520and%250Anorm%2520entropies%252C%2520we%2520derive%2520end-to-end%2520differentiable%2520update%2520rules%2520that%2520enable%250Asparse%2520transformations%252C%2520uncovering%2520new%2520connections%2520between%2520loss%2520margins%252C%250Asparsity%252C%2520and%2520exact%2520retrieval%2520of%2520single%2520memory%2520patterns.%2520We%2520further%2520extend%2520this%250Aframework%2520to%2520structured%2520Hopfield%2520networks%2520using%2520the%2520SparseMAP%2520transformation%252C%250Aallowing%2520the%2520retrieval%2520of%2520pattern%2520associations%2520rather%2520than%2520a%2520single%2520pattern.%250AOur%2520framework%2520unifies%2520and%2520extends%2520traditional%2520and%2520modern%2520Hopfield%2520networks%2520and%250Aprovides%2520an%2520energy%2520minimization%2520perspective%2520for%2520widely%2520used%250Apost-transformations%2520like%2520%2524%255Cell_2%2524-normalization%2520and%2520layer%2520normalization-all%250Athrough%2520suitable%2520choices%2520of%2520Fenchel-Young%2520losses%2520and%2520by%2520using%2520convex%2520analysis%250Aas%2520a%2520building%2520block.%2520Finally%252C%2520we%2520validate%2520our%2520Hopfield-Fenchel-Young%2520networks%250Aon%2520diverse%2520memory%2520recall%2520tasks%252C%2520including%2520free%2520and%2520sequential%2520recall.%250AExperiments%2520on%2520simulated%2520data%252C%2520image%2520retrieval%252C%2520multiple%2520instance%2520learning%252C%2520and%250Atext%2520rationalization%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08590v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hopfield-Fenchel-Young%20Networks%3A%20A%20Unified%20Framework%20for%20Associative%0A%20%20Memory%20Retrieval&entry.906535625=Saul%20Santos%20and%20Vlad%20Niculae%20and%20Daniel%20McNamee%20and%20Andr%C3%A9%20F.%20T.%20Martins&entry.1292438233=%20%20Associative%20memory%20models%2C%20such%20as%20Hopfield%20networks%20and%20their%20modern%0Avariants%2C%20have%20garnered%20renewed%20interest%20due%20to%20advancements%20in%20memory%20capacity%0Aand%20connections%20with%20self-attention%20in%20transformers.%20In%20this%20work%2C%20we%20introduce%0Aa%20unified%20framework-Hopfield-Fenchel-Young%20networks-which%20generalizes%20these%0Amodels%20to%20a%20broader%20family%20of%20energy%20functions.%20Our%20energies%20are%20formulated%20as%0Athe%20difference%20between%20two%20Fenchel-Young%20losses%3A%20one%2C%20parameterized%20by%20a%0Ageneralized%20entropy%2C%20defines%20the%20Hopfield%20scoring%20mechanism%2C%20while%20the%20other%0Aapplies%20a%20post-transformation%20to%20the%20Hopfield%20output.%20By%20utilizing%20Tsallis%20and%0Anorm%20entropies%2C%20we%20derive%20end-to-end%20differentiable%20update%20rules%20that%20enable%0Asparse%20transformations%2C%20uncovering%20new%20connections%20between%20loss%20margins%2C%0Asparsity%2C%20and%20exact%20retrieval%20of%20single%20memory%20patterns.%20We%20further%20extend%20this%0Aframework%20to%20structured%20Hopfield%20networks%20using%20the%20SparseMAP%20transformation%2C%0Aallowing%20the%20retrieval%20of%20pattern%20associations%20rather%20than%20a%20single%20pattern.%0AOur%20framework%20unifies%20and%20extends%20traditional%20and%20modern%20Hopfield%20networks%20and%0Aprovides%20an%20energy%20minimization%20perspective%20for%20widely%20used%0Apost-transformations%20like%20%24%5Cell_2%24-normalization%20and%20layer%20normalization-all%0Athrough%20suitable%20choices%20of%20Fenchel-Young%20losses%20and%20by%20using%20convex%20analysis%0Aas%20a%20building%20block.%20Finally%2C%20we%20validate%20our%20Hopfield-Fenchel-Young%20networks%0Aon%20diverse%20memory%20recall%20tasks%2C%20including%20free%20and%20sequential%20recall.%0AExperiments%20on%20simulated%20data%2C%20image%20retrieval%2C%20multiple%20instance%20learning%2C%20and%0Atext%20rationalization%20demonstrate%20the%20effectiveness%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08590v1&entry.124074799=Read"},
{"title": "Zero-shot capability of SAM-family models for bone segmentation in CT\n  scans", "author": "Caroline Magg and Hoel Kervadec and Clara I. S\u00e1nchez", "abstract": "  The Segment Anything Model (SAM) and similar models build a family of\npromptable foundation models (FMs) for image and video segmentation. The object\nof interest is identified using prompts, such as bounding boxes or points. With\nthese FMs becoming part of medical image segmentation, extensive evaluation\nstudies are required to assess their strengths and weaknesses in clinical\nsetting. Since the performance is highly dependent on the chosen prompting\nstrategy, it is important to investigate different prompting techniques to\ndefine optimal guidelines that ensure effective use in medical image\nsegmentation. Currently, no dedicated evaluation studies exist specifically for\nbone segmentation in CT scans, leaving a gap in understanding the performance\nfor this task. Thus, we use non-iterative, ``optimal'' prompting strategies\ncomposed of bounding box, points and combinations to test the zero-shot\ncapability of SAM-family models for bone CT segmentation on three different\nskeletal regions. Our results show that the best settings depend on the model\ntype and size, dataset characteristics and objective to optimize. Overall, SAM\nand SAM2 prompted with a bounding box in combination with the center point for\nall the components of an object yield the best results across all tested\nsettings. As the results depend on multiple factors, we provide a guideline for\ninformed decision-making in 2D prompting with non-interactive, ''optimal''\nprompts.\n", "link": "http://arxiv.org/abs/2411.08629v1", "date": "2024-11-13", "relevancy": 2.5366, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5161}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5161}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4897}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-shot%20capability%20of%20SAM-family%20models%20for%20bone%20segmentation%20in%20CT%0A%20%20scans&body=Title%3A%20Zero-shot%20capability%20of%20SAM-family%20models%20for%20bone%20segmentation%20in%20CT%0A%20%20scans%0AAuthor%3A%20Caroline%20Magg%20and%20Hoel%20Kervadec%20and%20Clara%20I.%20S%C3%A1nchez%0AAbstract%3A%20%20%20The%20Segment%20Anything%20Model%20%28SAM%29%20and%20similar%20models%20build%20a%20family%20of%0Apromptable%20foundation%20models%20%28FMs%29%20for%20image%20and%20video%20segmentation.%20The%20object%0Aof%20interest%20is%20identified%20using%20prompts%2C%20such%20as%20bounding%20boxes%20or%20points.%20With%0Athese%20FMs%20becoming%20part%20of%20medical%20image%20segmentation%2C%20extensive%20evaluation%0Astudies%20are%20required%20to%20assess%20their%20strengths%20and%20weaknesses%20in%20clinical%0Asetting.%20Since%20the%20performance%20is%20highly%20dependent%20on%20the%20chosen%20prompting%0Astrategy%2C%20it%20is%20important%20to%20investigate%20different%20prompting%20techniques%20to%0Adefine%20optimal%20guidelines%20that%20ensure%20effective%20use%20in%20medical%20image%0Asegmentation.%20Currently%2C%20no%20dedicated%20evaluation%20studies%20exist%20specifically%20for%0Abone%20segmentation%20in%20CT%20scans%2C%20leaving%20a%20gap%20in%20understanding%20the%20performance%0Afor%20this%20task.%20Thus%2C%20we%20use%20non-iterative%2C%20%60%60optimal%27%27%20prompting%20strategies%0Acomposed%20of%20bounding%20box%2C%20points%20and%20combinations%20to%20test%20the%20zero-shot%0Acapability%20of%20SAM-family%20models%20for%20bone%20CT%20segmentation%20on%20three%20different%0Askeletal%20regions.%20Our%20results%20show%20that%20the%20best%20settings%20depend%20on%20the%20model%0Atype%20and%20size%2C%20dataset%20characteristics%20and%20objective%20to%20optimize.%20Overall%2C%20SAM%0Aand%20SAM2%20prompted%20with%20a%20bounding%20box%20in%20combination%20with%20the%20center%20point%20for%0Aall%20the%20components%20of%20an%20object%20yield%20the%20best%20results%20across%20all%20tested%0Asettings.%20As%20the%20results%20depend%20on%20multiple%20factors%2C%20we%20provide%20a%20guideline%20for%0Ainformed%20decision-making%20in%202D%20prompting%20with%20non-interactive%2C%20%27%27optimal%27%27%0Aprompts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08629v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-shot%2520capability%2520of%2520SAM-family%2520models%2520for%2520bone%2520segmentation%2520in%2520CT%250A%2520%2520scans%26entry.906535625%3DCaroline%2520Magg%2520and%2520Hoel%2520Kervadec%2520and%2520Clara%2520I.%2520S%25C3%25A1nchez%26entry.1292438233%3D%2520%2520The%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520and%2520similar%2520models%2520build%2520a%2520family%2520of%250Apromptable%2520foundation%2520models%2520%2528FMs%2529%2520for%2520image%2520and%2520video%2520segmentation.%2520The%2520object%250Aof%2520interest%2520is%2520identified%2520using%2520prompts%252C%2520such%2520as%2520bounding%2520boxes%2520or%2520points.%2520With%250Athese%2520FMs%2520becoming%2520part%2520of%2520medical%2520image%2520segmentation%252C%2520extensive%2520evaluation%250Astudies%2520are%2520required%2520to%2520assess%2520their%2520strengths%2520and%2520weaknesses%2520in%2520clinical%250Asetting.%2520Since%2520the%2520performance%2520is%2520highly%2520dependent%2520on%2520the%2520chosen%2520prompting%250Astrategy%252C%2520it%2520is%2520important%2520to%2520investigate%2520different%2520prompting%2520techniques%2520to%250Adefine%2520optimal%2520guidelines%2520that%2520ensure%2520effective%2520use%2520in%2520medical%2520image%250Asegmentation.%2520Currently%252C%2520no%2520dedicated%2520evaluation%2520studies%2520exist%2520specifically%2520for%250Abone%2520segmentation%2520in%2520CT%2520scans%252C%2520leaving%2520a%2520gap%2520in%2520understanding%2520the%2520performance%250Afor%2520this%2520task.%2520Thus%252C%2520we%2520use%2520non-iterative%252C%2520%2560%2560optimal%2527%2527%2520prompting%2520strategies%250Acomposed%2520of%2520bounding%2520box%252C%2520points%2520and%2520combinations%2520to%2520test%2520the%2520zero-shot%250Acapability%2520of%2520SAM-family%2520models%2520for%2520bone%2520CT%2520segmentation%2520on%2520three%2520different%250Askeletal%2520regions.%2520Our%2520results%2520show%2520that%2520the%2520best%2520settings%2520depend%2520on%2520the%2520model%250Atype%2520and%2520size%252C%2520dataset%2520characteristics%2520and%2520objective%2520to%2520optimize.%2520Overall%252C%2520SAM%250Aand%2520SAM2%2520prompted%2520with%2520a%2520bounding%2520box%2520in%2520combination%2520with%2520the%2520center%2520point%2520for%250Aall%2520the%2520components%2520of%2520an%2520object%2520yield%2520the%2520best%2520results%2520across%2520all%2520tested%250Asettings.%2520As%2520the%2520results%2520depend%2520on%2520multiple%2520factors%252C%2520we%2520provide%2520a%2520guideline%2520for%250Ainformed%2520decision-making%2520in%25202D%2520prompting%2520with%2520non-interactive%252C%2520%2527%2527optimal%2527%2527%250Aprompts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08629v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-shot%20capability%20of%20SAM-family%20models%20for%20bone%20segmentation%20in%20CT%0A%20%20scans&entry.906535625=Caroline%20Magg%20and%20Hoel%20Kervadec%20and%20Clara%20I.%20S%C3%A1nchez&entry.1292438233=%20%20The%20Segment%20Anything%20Model%20%28SAM%29%20and%20similar%20models%20build%20a%20family%20of%0Apromptable%20foundation%20models%20%28FMs%29%20for%20image%20and%20video%20segmentation.%20The%20object%0Aof%20interest%20is%20identified%20using%20prompts%2C%20such%20as%20bounding%20boxes%20or%20points.%20With%0Athese%20FMs%20becoming%20part%20of%20medical%20image%20segmentation%2C%20extensive%20evaluation%0Astudies%20are%20required%20to%20assess%20their%20strengths%20and%20weaknesses%20in%20clinical%0Asetting.%20Since%20the%20performance%20is%20highly%20dependent%20on%20the%20chosen%20prompting%0Astrategy%2C%20it%20is%20important%20to%20investigate%20different%20prompting%20techniques%20to%0Adefine%20optimal%20guidelines%20that%20ensure%20effective%20use%20in%20medical%20image%0Asegmentation.%20Currently%2C%20no%20dedicated%20evaluation%20studies%20exist%20specifically%20for%0Abone%20segmentation%20in%20CT%20scans%2C%20leaving%20a%20gap%20in%20understanding%20the%20performance%0Afor%20this%20task.%20Thus%2C%20we%20use%20non-iterative%2C%20%60%60optimal%27%27%20prompting%20strategies%0Acomposed%20of%20bounding%20box%2C%20points%20and%20combinations%20to%20test%20the%20zero-shot%0Acapability%20of%20SAM-family%20models%20for%20bone%20CT%20segmentation%20on%20three%20different%0Askeletal%20regions.%20Our%20results%20show%20that%20the%20best%20settings%20depend%20on%20the%20model%0Atype%20and%20size%2C%20dataset%20characteristics%20and%20objective%20to%20optimize.%20Overall%2C%20SAM%0Aand%20SAM2%20prompted%20with%20a%20bounding%20box%20in%20combination%20with%20the%20center%20point%20for%0Aall%20the%20components%20of%20an%20object%20yield%20the%20best%20results%20across%20all%20tested%0Asettings.%20As%20the%20results%20depend%20on%20multiple%20factors%2C%20we%20provide%20a%20guideline%20for%0Ainformed%20decision-making%20in%202D%20prompting%20with%20non-interactive%2C%20%27%27optimal%27%27%0Aprompts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08629v1&entry.124074799=Read"},
{"title": "OSMLoc: Single Image-Based Visual Localization in OpenStreetMap with\n  Geometric and Semantic Guidances", "author": "Youqi Liao and Xieyuanli Chen and Shuhao Kang and Jianping Li and Zhen Dong and Hongchao Fan and Bisheng Yang", "abstract": "  OpenStreetMap (OSM), an online and versatile source of volunteered geographic\ninformation (VGI), is widely used for human self-localization by matching\nnearby visual observations with vectorized map data. However, due to the\ndivergence in modalities and views, image-to-OSM (I2O) matching and\nlocalization remain challenging for robots, preventing the full utilization of\nVGI data in the unmanned ground vehicles and logistic industry. Inspired by the\nfact that the human brain relies on geometric and semantic understanding of\nsensory information for spatial localization tasks, we propose the OSMLoc in\nthis paper. OSMLoc is a brain-inspired single-image visual localization method\nwith semantic and geometric guidance to improve accuracy, robustness, and\ngeneralization ability. First, we equip the OSMLoc with the visual foundational\nmodel to extract powerful image features. Second, a geometry-guided depth\ndistribution adapter is proposed to bridge the monocular depth estimation and\ncamera-to-BEV transform. Thirdly, the semantic embeddings from the OSM data are\nutilized as auxiliary guidance for image-to-OSM feature matching. To validate\nthe proposed OSMLoc, we collect a worldwide cross-area and cross-condition (CC)\nbenchmark for extensive evaluation. Experiments on the MGL dataset, CC\nvalidation benchmark, and KITTI dataset have demonstrated the superiority of\nour method. Code, pre-trained models, CC validation benchmark, and additional\nresults are available on: https://github.com/WHU-USI3DV/OSMLoc\n", "link": "http://arxiv.org/abs/2411.08665v1", "date": "2024-11-13", "relevancy": 2.5295, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6585}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6197}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6113}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OSMLoc%3A%20Single%20Image-Based%20Visual%20Localization%20in%20OpenStreetMap%20with%0A%20%20Geometric%20and%20Semantic%20Guidances&body=Title%3A%20OSMLoc%3A%20Single%20Image-Based%20Visual%20Localization%20in%20OpenStreetMap%20with%0A%20%20Geometric%20and%20Semantic%20Guidances%0AAuthor%3A%20Youqi%20Liao%20and%20Xieyuanli%20Chen%20and%20Shuhao%20Kang%20and%20Jianping%20Li%20and%20Zhen%20Dong%20and%20Hongchao%20Fan%20and%20Bisheng%20Yang%0AAbstract%3A%20%20%20OpenStreetMap%20%28OSM%29%2C%20an%20online%20and%20versatile%20source%20of%20volunteered%20geographic%0Ainformation%20%28VGI%29%2C%20is%20widely%20used%20for%20human%20self-localization%20by%20matching%0Anearby%20visual%20observations%20with%20vectorized%20map%20data.%20However%2C%20due%20to%20the%0Adivergence%20in%20modalities%20and%20views%2C%20image-to-OSM%20%28I2O%29%20matching%20and%0Alocalization%20remain%20challenging%20for%20robots%2C%20preventing%20the%20full%20utilization%20of%0AVGI%20data%20in%20the%20unmanned%20ground%20vehicles%20and%20logistic%20industry.%20Inspired%20by%20the%0Afact%20that%20the%20human%20brain%20relies%20on%20geometric%20and%20semantic%20understanding%20of%0Asensory%20information%20for%20spatial%20localization%20tasks%2C%20we%20propose%20the%20OSMLoc%20in%0Athis%20paper.%20OSMLoc%20is%20a%20brain-inspired%20single-image%20visual%20localization%20method%0Awith%20semantic%20and%20geometric%20guidance%20to%20improve%20accuracy%2C%20robustness%2C%20and%0Ageneralization%20ability.%20First%2C%20we%20equip%20the%20OSMLoc%20with%20the%20visual%20foundational%0Amodel%20to%20extract%20powerful%20image%20features.%20Second%2C%20a%20geometry-guided%20depth%0Adistribution%20adapter%20is%20proposed%20to%20bridge%20the%20monocular%20depth%20estimation%20and%0Acamera-to-BEV%20transform.%20Thirdly%2C%20the%20semantic%20embeddings%20from%20the%20OSM%20data%20are%0Autilized%20as%20auxiliary%20guidance%20for%20image-to-OSM%20feature%20matching.%20To%20validate%0Athe%20proposed%20OSMLoc%2C%20we%20collect%20a%20worldwide%20cross-area%20and%20cross-condition%20%28CC%29%0Abenchmark%20for%20extensive%20evaluation.%20Experiments%20on%20the%20MGL%20dataset%2C%20CC%0Avalidation%20benchmark%2C%20and%20KITTI%20dataset%20have%20demonstrated%20the%20superiority%20of%0Aour%20method.%20Code%2C%20pre-trained%20models%2C%20CC%20validation%20benchmark%2C%20and%20additional%0Aresults%20are%20available%20on%3A%20https%3A//github.com/WHU-USI3DV/OSMLoc%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08665v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOSMLoc%253A%2520Single%2520Image-Based%2520Visual%2520Localization%2520in%2520OpenStreetMap%2520with%250A%2520%2520Geometric%2520and%2520Semantic%2520Guidances%26entry.906535625%3DYouqi%2520Liao%2520and%2520Xieyuanli%2520Chen%2520and%2520Shuhao%2520Kang%2520and%2520Jianping%2520Li%2520and%2520Zhen%2520Dong%2520and%2520Hongchao%2520Fan%2520and%2520Bisheng%2520Yang%26entry.1292438233%3D%2520%2520OpenStreetMap%2520%2528OSM%2529%252C%2520an%2520online%2520and%2520versatile%2520source%2520of%2520volunteered%2520geographic%250Ainformation%2520%2528VGI%2529%252C%2520is%2520widely%2520used%2520for%2520human%2520self-localization%2520by%2520matching%250Anearby%2520visual%2520observations%2520with%2520vectorized%2520map%2520data.%2520However%252C%2520due%2520to%2520the%250Adivergence%2520in%2520modalities%2520and%2520views%252C%2520image-to-OSM%2520%2528I2O%2529%2520matching%2520and%250Alocalization%2520remain%2520challenging%2520for%2520robots%252C%2520preventing%2520the%2520full%2520utilization%2520of%250AVGI%2520data%2520in%2520the%2520unmanned%2520ground%2520vehicles%2520and%2520logistic%2520industry.%2520Inspired%2520by%2520the%250Afact%2520that%2520the%2520human%2520brain%2520relies%2520on%2520geometric%2520and%2520semantic%2520understanding%2520of%250Asensory%2520information%2520for%2520spatial%2520localization%2520tasks%252C%2520we%2520propose%2520the%2520OSMLoc%2520in%250Athis%2520paper.%2520OSMLoc%2520is%2520a%2520brain-inspired%2520single-image%2520visual%2520localization%2520method%250Awith%2520semantic%2520and%2520geometric%2520guidance%2520to%2520improve%2520accuracy%252C%2520robustness%252C%2520and%250Ageneralization%2520ability.%2520First%252C%2520we%2520equip%2520the%2520OSMLoc%2520with%2520the%2520visual%2520foundational%250Amodel%2520to%2520extract%2520powerful%2520image%2520features.%2520Second%252C%2520a%2520geometry-guided%2520depth%250Adistribution%2520adapter%2520is%2520proposed%2520to%2520bridge%2520the%2520monocular%2520depth%2520estimation%2520and%250Acamera-to-BEV%2520transform.%2520Thirdly%252C%2520the%2520semantic%2520embeddings%2520from%2520the%2520OSM%2520data%2520are%250Autilized%2520as%2520auxiliary%2520guidance%2520for%2520image-to-OSM%2520feature%2520matching.%2520To%2520validate%250Athe%2520proposed%2520OSMLoc%252C%2520we%2520collect%2520a%2520worldwide%2520cross-area%2520and%2520cross-condition%2520%2528CC%2529%250Abenchmark%2520for%2520extensive%2520evaluation.%2520Experiments%2520on%2520the%2520MGL%2520dataset%252C%2520CC%250Avalidation%2520benchmark%252C%2520and%2520KITTI%2520dataset%2520have%2520demonstrated%2520the%2520superiority%2520of%250Aour%2520method.%2520Code%252C%2520pre-trained%2520models%252C%2520CC%2520validation%2520benchmark%252C%2520and%2520additional%250Aresults%2520are%2520available%2520on%253A%2520https%253A//github.com/WHU-USI3DV/OSMLoc%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08665v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OSMLoc%3A%20Single%20Image-Based%20Visual%20Localization%20in%20OpenStreetMap%20with%0A%20%20Geometric%20and%20Semantic%20Guidances&entry.906535625=Youqi%20Liao%20and%20Xieyuanli%20Chen%20and%20Shuhao%20Kang%20and%20Jianping%20Li%20and%20Zhen%20Dong%20and%20Hongchao%20Fan%20and%20Bisheng%20Yang&entry.1292438233=%20%20OpenStreetMap%20%28OSM%29%2C%20an%20online%20and%20versatile%20source%20of%20volunteered%20geographic%0Ainformation%20%28VGI%29%2C%20is%20widely%20used%20for%20human%20self-localization%20by%20matching%0Anearby%20visual%20observations%20with%20vectorized%20map%20data.%20However%2C%20due%20to%20the%0Adivergence%20in%20modalities%20and%20views%2C%20image-to-OSM%20%28I2O%29%20matching%20and%0Alocalization%20remain%20challenging%20for%20robots%2C%20preventing%20the%20full%20utilization%20of%0AVGI%20data%20in%20the%20unmanned%20ground%20vehicles%20and%20logistic%20industry.%20Inspired%20by%20the%0Afact%20that%20the%20human%20brain%20relies%20on%20geometric%20and%20semantic%20understanding%20of%0Asensory%20information%20for%20spatial%20localization%20tasks%2C%20we%20propose%20the%20OSMLoc%20in%0Athis%20paper.%20OSMLoc%20is%20a%20brain-inspired%20single-image%20visual%20localization%20method%0Awith%20semantic%20and%20geometric%20guidance%20to%20improve%20accuracy%2C%20robustness%2C%20and%0Ageneralization%20ability.%20First%2C%20we%20equip%20the%20OSMLoc%20with%20the%20visual%20foundational%0Amodel%20to%20extract%20powerful%20image%20features.%20Second%2C%20a%20geometry-guided%20depth%0Adistribution%20adapter%20is%20proposed%20to%20bridge%20the%20monocular%20depth%20estimation%20and%0Acamera-to-BEV%20transform.%20Thirdly%2C%20the%20semantic%20embeddings%20from%20the%20OSM%20data%20are%0Autilized%20as%20auxiliary%20guidance%20for%20image-to-OSM%20feature%20matching.%20To%20validate%0Athe%20proposed%20OSMLoc%2C%20we%20collect%20a%20worldwide%20cross-area%20and%20cross-condition%20%28CC%29%0Abenchmark%20for%20extensive%20evaluation.%20Experiments%20on%20the%20MGL%20dataset%2C%20CC%0Avalidation%20benchmark%2C%20and%20KITTI%20dataset%20have%20demonstrated%20the%20superiority%20of%0Aour%20method.%20Code%2C%20pre-trained%20models%2C%20CC%20validation%20benchmark%2C%20and%20additional%0Aresults%20are%20available%20on%3A%20https%3A//github.com/WHU-USI3DV/OSMLoc%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08665v1&entry.124074799=Read"},
{"title": "Measuring similarity between embedding spaces using induced neighborhood\n  graphs", "author": "Tiago F. Tavares and Fabio Ayres and Paris Smaragdis", "abstract": "  Deep Learning techniques have excelled at generating embedding spaces that\ncapture semantic similarities between items. Often these representations are\npaired, enabling experiments with analogies (pairs within the same domain) and\ncross-modality (pairs across domains). These experiments are based on specific\nassumptions about the geometry of embedding spaces, which allow finding paired\nitems by extrapolating the positional relationships between embedding pairs in\nthe training dataset, allowing for tasks such as finding new analogies, and\nmultimodal zero-shot classification. In this work, we propose a metric to\nevaluate the similarity between paired item representations. Our proposal is\nbuilt from the structural similarity between the nearest-neighbors induced\ngraphs of each representation, and can be configured to compare spaces based on\ndifferent distance metrics and on different neighborhood sizes. We demonstrate\nthat our proposal can be used to identify similar structures at different\nscales, which is hard to achieve with kernel methods such as Centered Kernel\nAlignment (CKA). We further illustrate our method with two case studies: an\nanalogy task using GloVe embeddings, and zero-shot classification in the\nCIFAR-100 dataset using CLIP embeddings. Our results show that accuracy in both\nanalogy and zero-shot classification tasks correlates with the embedding\nsimilarity. These findings can help explain performance differences in these\ntasks, and may lead to improved design of paired-embedding models in the\nfuture.\n", "link": "http://arxiv.org/abs/2411.08687v1", "date": "2024-11-13", "relevancy": 2.5209, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5255}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4938}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4933}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Measuring%20similarity%20between%20embedding%20spaces%20using%20induced%20neighborhood%0A%20%20graphs&body=Title%3A%20Measuring%20similarity%20between%20embedding%20spaces%20using%20induced%20neighborhood%0A%20%20graphs%0AAuthor%3A%20Tiago%20F.%20Tavares%20and%20Fabio%20Ayres%20and%20Paris%20Smaragdis%0AAbstract%3A%20%20%20Deep%20Learning%20techniques%20have%20excelled%20at%20generating%20embedding%20spaces%20that%0Acapture%20semantic%20similarities%20between%20items.%20Often%20these%20representations%20are%0Apaired%2C%20enabling%20experiments%20with%20analogies%20%28pairs%20within%20the%20same%20domain%29%20and%0Across-modality%20%28pairs%20across%20domains%29.%20These%20experiments%20are%20based%20on%20specific%0Aassumptions%20about%20the%20geometry%20of%20embedding%20spaces%2C%20which%20allow%20finding%20paired%0Aitems%20by%20extrapolating%20the%20positional%20relationships%20between%20embedding%20pairs%20in%0Athe%20training%20dataset%2C%20allowing%20for%20tasks%20such%20as%20finding%20new%20analogies%2C%20and%0Amultimodal%20zero-shot%20classification.%20In%20this%20work%2C%20we%20propose%20a%20metric%20to%0Aevaluate%20the%20similarity%20between%20paired%20item%20representations.%20Our%20proposal%20is%0Abuilt%20from%20the%20structural%20similarity%20between%20the%20nearest-neighbors%20induced%0Agraphs%20of%20each%20representation%2C%20and%20can%20be%20configured%20to%20compare%20spaces%20based%20on%0Adifferent%20distance%20metrics%20and%20on%20different%20neighborhood%20sizes.%20We%20demonstrate%0Athat%20our%20proposal%20can%20be%20used%20to%20identify%20similar%20structures%20at%20different%0Ascales%2C%20which%20is%20hard%20to%20achieve%20with%20kernel%20methods%20such%20as%20Centered%20Kernel%0AAlignment%20%28CKA%29.%20We%20further%20illustrate%20our%20method%20with%20two%20case%20studies%3A%20an%0Aanalogy%20task%20using%20GloVe%20embeddings%2C%20and%20zero-shot%20classification%20in%20the%0ACIFAR-100%20dataset%20using%20CLIP%20embeddings.%20Our%20results%20show%20that%20accuracy%20in%20both%0Aanalogy%20and%20zero-shot%20classification%20tasks%20correlates%20with%20the%20embedding%0Asimilarity.%20These%20findings%20can%20help%20explain%20performance%20differences%20in%20these%0Atasks%2C%20and%20may%20lead%20to%20improved%20design%20of%20paired-embedding%20models%20in%20the%0Afuture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08687v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeasuring%2520similarity%2520between%2520embedding%2520spaces%2520using%2520induced%2520neighborhood%250A%2520%2520graphs%26entry.906535625%3DTiago%2520F.%2520Tavares%2520and%2520Fabio%2520Ayres%2520and%2520Paris%2520Smaragdis%26entry.1292438233%3D%2520%2520Deep%2520Learning%2520techniques%2520have%2520excelled%2520at%2520generating%2520embedding%2520spaces%2520that%250Acapture%2520semantic%2520similarities%2520between%2520items.%2520Often%2520these%2520representations%2520are%250Apaired%252C%2520enabling%2520experiments%2520with%2520analogies%2520%2528pairs%2520within%2520the%2520same%2520domain%2529%2520and%250Across-modality%2520%2528pairs%2520across%2520domains%2529.%2520These%2520experiments%2520are%2520based%2520on%2520specific%250Aassumptions%2520about%2520the%2520geometry%2520of%2520embedding%2520spaces%252C%2520which%2520allow%2520finding%2520paired%250Aitems%2520by%2520extrapolating%2520the%2520positional%2520relationships%2520between%2520embedding%2520pairs%2520in%250Athe%2520training%2520dataset%252C%2520allowing%2520for%2520tasks%2520such%2520as%2520finding%2520new%2520analogies%252C%2520and%250Amultimodal%2520zero-shot%2520classification.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520metric%2520to%250Aevaluate%2520the%2520similarity%2520between%2520paired%2520item%2520representations.%2520Our%2520proposal%2520is%250Abuilt%2520from%2520the%2520structural%2520similarity%2520between%2520the%2520nearest-neighbors%2520induced%250Agraphs%2520of%2520each%2520representation%252C%2520and%2520can%2520be%2520configured%2520to%2520compare%2520spaces%2520based%2520on%250Adifferent%2520distance%2520metrics%2520and%2520on%2520different%2520neighborhood%2520sizes.%2520We%2520demonstrate%250Athat%2520our%2520proposal%2520can%2520be%2520used%2520to%2520identify%2520similar%2520structures%2520at%2520different%250Ascales%252C%2520which%2520is%2520hard%2520to%2520achieve%2520with%2520kernel%2520methods%2520such%2520as%2520Centered%2520Kernel%250AAlignment%2520%2528CKA%2529.%2520We%2520further%2520illustrate%2520our%2520method%2520with%2520two%2520case%2520studies%253A%2520an%250Aanalogy%2520task%2520using%2520GloVe%2520embeddings%252C%2520and%2520zero-shot%2520classification%2520in%2520the%250ACIFAR-100%2520dataset%2520using%2520CLIP%2520embeddings.%2520Our%2520results%2520show%2520that%2520accuracy%2520in%2520both%250Aanalogy%2520and%2520zero-shot%2520classification%2520tasks%2520correlates%2520with%2520the%2520embedding%250Asimilarity.%2520These%2520findings%2520can%2520help%2520explain%2520performance%2520differences%2520in%2520these%250Atasks%252C%2520and%2520may%2520lead%2520to%2520improved%2520design%2520of%2520paired-embedding%2520models%2520in%2520the%250Afuture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08687v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Measuring%20similarity%20between%20embedding%20spaces%20using%20induced%20neighborhood%0A%20%20graphs&entry.906535625=Tiago%20F.%20Tavares%20and%20Fabio%20Ayres%20and%20Paris%20Smaragdis&entry.1292438233=%20%20Deep%20Learning%20techniques%20have%20excelled%20at%20generating%20embedding%20spaces%20that%0Acapture%20semantic%20similarities%20between%20items.%20Often%20these%20representations%20are%0Apaired%2C%20enabling%20experiments%20with%20analogies%20%28pairs%20within%20the%20same%20domain%29%20and%0Across-modality%20%28pairs%20across%20domains%29.%20These%20experiments%20are%20based%20on%20specific%0Aassumptions%20about%20the%20geometry%20of%20embedding%20spaces%2C%20which%20allow%20finding%20paired%0Aitems%20by%20extrapolating%20the%20positional%20relationships%20between%20embedding%20pairs%20in%0Athe%20training%20dataset%2C%20allowing%20for%20tasks%20such%20as%20finding%20new%20analogies%2C%20and%0Amultimodal%20zero-shot%20classification.%20In%20this%20work%2C%20we%20propose%20a%20metric%20to%0Aevaluate%20the%20similarity%20between%20paired%20item%20representations.%20Our%20proposal%20is%0Abuilt%20from%20the%20structural%20similarity%20between%20the%20nearest-neighbors%20induced%0Agraphs%20of%20each%20representation%2C%20and%20can%20be%20configured%20to%20compare%20spaces%20based%20on%0Adifferent%20distance%20metrics%20and%20on%20different%20neighborhood%20sizes.%20We%20demonstrate%0Athat%20our%20proposal%20can%20be%20used%20to%20identify%20similar%20structures%20at%20different%0Ascales%2C%20which%20is%20hard%20to%20achieve%20with%20kernel%20methods%20such%20as%20Centered%20Kernel%0AAlignment%20%28CKA%29.%20We%20further%20illustrate%20our%20method%20with%20two%20case%20studies%3A%20an%0Aanalogy%20task%20using%20GloVe%20embeddings%2C%20and%20zero-shot%20classification%20in%20the%0ACIFAR-100%20dataset%20using%20CLIP%20embeddings.%20Our%20results%20show%20that%20accuracy%20in%20both%0Aanalogy%20and%20zero-shot%20classification%20tasks%20correlates%20with%20the%20embedding%0Asimilarity.%20These%20findings%20can%20help%20explain%20performance%20differences%20in%20these%0Atasks%2C%20and%20may%20lead%20to%20improved%20design%20of%20paired-embedding%20models%20in%20the%0Afuture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08687v1&entry.124074799=Read"},
{"title": "CLASS-M: Adaptive stain separation-based contrastive learning with\n  pseudo-labeling for histopathological image classification", "author": "Bodong Zhang and Hamid Manoochehri and Man Minh Ho and Fahimeh Fooladgar and Yosep Chong and Beatrice S. Knudsen and Deepika Sirohi and Tolga Tasdizen", "abstract": "  Histopathological image classification is an important task in medical image\nanalysis. Recent approaches generally rely on weakly supervised learning due to\nthe ease of acquiring case-level labels from pathology reports. However,\npatch-level classification is preferable in applications where only a limited\nnumber of cases are available or when local prediction accuracy is critical. On\nthe other hand, acquiring extensive datasets with localized labels for training\nis not feasible. In this paper, we propose a semi-supervised patch-level\nhistopathological image classification model, named CLASS-M, that does not\nrequire extensively labeled datasets. CLASS-M is formed by two main parts: a\ncontrastive learning module that uses separated Hematoxylin and Eosin images\ngenerated through an adaptive stain separation process, and a module with\npseudo-labels using MixUp. We compare our model with other state-of-the-art\nmodels on two clear cell renal cell carcinoma datasets. We demonstrate that our\nCLASS-M model has the best performance on both datasets. Our code is available\nat github.com/BzhangURU/Paper_CLASS-M/tree/main\n", "link": "http://arxiv.org/abs/2312.06978v4", "date": "2024-11-13", "relevancy": 2.5197, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5154}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5094}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLASS-M%3A%20Adaptive%20stain%20separation-based%20contrastive%20learning%20with%0A%20%20pseudo-labeling%20for%20histopathological%20image%20classification&body=Title%3A%20CLASS-M%3A%20Adaptive%20stain%20separation-based%20contrastive%20learning%20with%0A%20%20pseudo-labeling%20for%20histopathological%20image%20classification%0AAuthor%3A%20Bodong%20Zhang%20and%20Hamid%20Manoochehri%20and%20Man%20Minh%20Ho%20and%20Fahimeh%20Fooladgar%20and%20Yosep%20Chong%20and%20Beatrice%20S.%20Knudsen%20and%20Deepika%20Sirohi%20and%20Tolga%20Tasdizen%0AAbstract%3A%20%20%20Histopathological%20image%20classification%20is%20an%20important%20task%20in%20medical%20image%0Aanalysis.%20Recent%20approaches%20generally%20rely%20on%20weakly%20supervised%20learning%20due%20to%0Athe%20ease%20of%20acquiring%20case-level%20labels%20from%20pathology%20reports.%20However%2C%0Apatch-level%20classification%20is%20preferable%20in%20applications%20where%20only%20a%20limited%0Anumber%20of%20cases%20are%20available%20or%20when%20local%20prediction%20accuracy%20is%20critical.%20On%0Athe%20other%20hand%2C%20acquiring%20extensive%20datasets%20with%20localized%20labels%20for%20training%0Ais%20not%20feasible.%20In%20this%20paper%2C%20we%20propose%20a%20semi-supervised%20patch-level%0Ahistopathological%20image%20classification%20model%2C%20named%20CLASS-M%2C%20that%20does%20not%0Arequire%20extensively%20labeled%20datasets.%20CLASS-M%20is%20formed%20by%20two%20main%20parts%3A%20a%0Acontrastive%20learning%20module%20that%20uses%20separated%20Hematoxylin%20and%20Eosin%20images%0Agenerated%20through%20an%20adaptive%20stain%20separation%20process%2C%20and%20a%20module%20with%0Apseudo-labels%20using%20MixUp.%20We%20compare%20our%20model%20with%20other%20state-of-the-art%0Amodels%20on%20two%20clear%20cell%20renal%20cell%20carcinoma%20datasets.%20We%20demonstrate%20that%20our%0ACLASS-M%20model%20has%20the%20best%20performance%20on%20both%20datasets.%20Our%20code%20is%20available%0Aat%20github.com/BzhangURU/Paper_CLASS-M/tree/main%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.06978v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLASS-M%253A%2520Adaptive%2520stain%2520separation-based%2520contrastive%2520learning%2520with%250A%2520%2520pseudo-labeling%2520for%2520histopathological%2520image%2520classification%26entry.906535625%3DBodong%2520Zhang%2520and%2520Hamid%2520Manoochehri%2520and%2520Man%2520Minh%2520Ho%2520and%2520Fahimeh%2520Fooladgar%2520and%2520Yosep%2520Chong%2520and%2520Beatrice%2520S.%2520Knudsen%2520and%2520Deepika%2520Sirohi%2520and%2520Tolga%2520Tasdizen%26entry.1292438233%3D%2520%2520Histopathological%2520image%2520classification%2520is%2520an%2520important%2520task%2520in%2520medical%2520image%250Aanalysis.%2520Recent%2520approaches%2520generally%2520rely%2520on%2520weakly%2520supervised%2520learning%2520due%2520to%250Athe%2520ease%2520of%2520acquiring%2520case-level%2520labels%2520from%2520pathology%2520reports.%2520However%252C%250Apatch-level%2520classification%2520is%2520preferable%2520in%2520applications%2520where%2520only%2520a%2520limited%250Anumber%2520of%2520cases%2520are%2520available%2520or%2520when%2520local%2520prediction%2520accuracy%2520is%2520critical.%2520On%250Athe%2520other%2520hand%252C%2520acquiring%2520extensive%2520datasets%2520with%2520localized%2520labels%2520for%2520training%250Ais%2520not%2520feasible.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520semi-supervised%2520patch-level%250Ahistopathological%2520image%2520classification%2520model%252C%2520named%2520CLASS-M%252C%2520that%2520does%2520not%250Arequire%2520extensively%2520labeled%2520datasets.%2520CLASS-M%2520is%2520formed%2520by%2520two%2520main%2520parts%253A%2520a%250Acontrastive%2520learning%2520module%2520that%2520uses%2520separated%2520Hematoxylin%2520and%2520Eosin%2520images%250Agenerated%2520through%2520an%2520adaptive%2520stain%2520separation%2520process%252C%2520and%2520a%2520module%2520with%250Apseudo-labels%2520using%2520MixUp.%2520We%2520compare%2520our%2520model%2520with%2520other%2520state-of-the-art%250Amodels%2520on%2520two%2520clear%2520cell%2520renal%2520cell%2520carcinoma%2520datasets.%2520We%2520demonstrate%2520that%2520our%250ACLASS-M%2520model%2520has%2520the%2520best%2520performance%2520on%2520both%2520datasets.%2520Our%2520code%2520is%2520available%250Aat%2520github.com/BzhangURU/Paper_CLASS-M/tree/main%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.06978v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLASS-M%3A%20Adaptive%20stain%20separation-based%20contrastive%20learning%20with%0A%20%20pseudo-labeling%20for%20histopathological%20image%20classification&entry.906535625=Bodong%20Zhang%20and%20Hamid%20Manoochehri%20and%20Man%20Minh%20Ho%20and%20Fahimeh%20Fooladgar%20and%20Yosep%20Chong%20and%20Beatrice%20S.%20Knudsen%20and%20Deepika%20Sirohi%20and%20Tolga%20Tasdizen&entry.1292438233=%20%20Histopathological%20image%20classification%20is%20an%20important%20task%20in%20medical%20image%0Aanalysis.%20Recent%20approaches%20generally%20rely%20on%20weakly%20supervised%20learning%20due%20to%0Athe%20ease%20of%20acquiring%20case-level%20labels%20from%20pathology%20reports.%20However%2C%0Apatch-level%20classification%20is%20preferable%20in%20applications%20where%20only%20a%20limited%0Anumber%20of%20cases%20are%20available%20or%20when%20local%20prediction%20accuracy%20is%20critical.%20On%0Athe%20other%20hand%2C%20acquiring%20extensive%20datasets%20with%20localized%20labels%20for%20training%0Ais%20not%20feasible.%20In%20this%20paper%2C%20we%20propose%20a%20semi-supervised%20patch-level%0Ahistopathological%20image%20classification%20model%2C%20named%20CLASS-M%2C%20that%20does%20not%0Arequire%20extensively%20labeled%20datasets.%20CLASS-M%20is%20formed%20by%20two%20main%20parts%3A%20a%0Acontrastive%20learning%20module%20that%20uses%20separated%20Hematoxylin%20and%20Eosin%20images%0Agenerated%20through%20an%20adaptive%20stain%20separation%20process%2C%20and%20a%20module%20with%0Apseudo-labels%20using%20MixUp.%20We%20compare%20our%20model%20with%20other%20state-of-the-art%0Amodels%20on%20two%20clear%20cell%20renal%20cell%20carcinoma%20datasets.%20We%20demonstrate%20that%20our%0ACLASS-M%20model%20has%20the%20best%20performance%20on%20both%20datasets.%20Our%20code%20is%20available%0Aat%20github.com/BzhangURU/Paper_CLASS-M/tree/main%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.06978v4&entry.124074799=Read"},
{"title": "Can sparse autoencoders be used to decompose and interpret steering\n  vectors?", "author": "Harry Mayne and Yushi Yang and Adam Mahdi", "abstract": "  Steering vectors are a promising approach to control the behaviour of large\nlanguage models. However, their underlying mechanisms remain poorly understood.\nWhile sparse autoencoders (SAEs) may offer a potential method to interpret\nsteering vectors, recent findings show that SAE-reconstructed vectors often\nlack the steering properties of the original vectors. This paper investigates\nwhy directly applying SAEs to steering vectors yields misleading\ndecompositions, identifying two reasons: (1) steering vectors fall outside the\ninput distribution for which SAEs are designed, and (2) steering vectors can\nhave meaningful negative projections in feature directions, which SAEs are not\ndesigned to accommodate. These limitations hinder the direct use of SAEs for\ninterpreting steering vectors.\n", "link": "http://arxiv.org/abs/2411.08790v1", "date": "2024-11-13", "relevancy": 2.5161, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5064}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5064}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4968}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20sparse%20autoencoders%20be%20used%20to%20decompose%20and%20interpret%20steering%0A%20%20vectors%3F&body=Title%3A%20Can%20sparse%20autoencoders%20be%20used%20to%20decompose%20and%20interpret%20steering%0A%20%20vectors%3F%0AAuthor%3A%20Harry%20Mayne%20and%20Yushi%20Yang%20and%20Adam%20Mahdi%0AAbstract%3A%20%20%20Steering%20vectors%20are%20a%20promising%20approach%20to%20control%20the%20behaviour%20of%20large%0Alanguage%20models.%20However%2C%20their%20underlying%20mechanisms%20remain%20poorly%20understood.%0AWhile%20sparse%20autoencoders%20%28SAEs%29%20may%20offer%20a%20potential%20method%20to%20interpret%0Asteering%20vectors%2C%20recent%20findings%20show%20that%20SAE-reconstructed%20vectors%20often%0Alack%20the%20steering%20properties%20of%20the%20original%20vectors.%20This%20paper%20investigates%0Awhy%20directly%20applying%20SAEs%20to%20steering%20vectors%20yields%20misleading%0Adecompositions%2C%20identifying%20two%20reasons%3A%20%281%29%20steering%20vectors%20fall%20outside%20the%0Ainput%20distribution%20for%20which%20SAEs%20are%20designed%2C%20and%20%282%29%20steering%20vectors%20can%0Ahave%20meaningful%20negative%20projections%20in%20feature%20directions%2C%20which%20SAEs%20are%20not%0Adesigned%20to%20accommodate.%20These%20limitations%20hinder%20the%20direct%20use%20of%20SAEs%20for%0Ainterpreting%20steering%20vectors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08790v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520sparse%2520autoencoders%2520be%2520used%2520to%2520decompose%2520and%2520interpret%2520steering%250A%2520%2520vectors%253F%26entry.906535625%3DHarry%2520Mayne%2520and%2520Yushi%2520Yang%2520and%2520Adam%2520Mahdi%26entry.1292438233%3D%2520%2520Steering%2520vectors%2520are%2520a%2520promising%2520approach%2520to%2520control%2520the%2520behaviour%2520of%2520large%250Alanguage%2520models.%2520However%252C%2520their%2520underlying%2520mechanisms%2520remain%2520poorly%2520understood.%250AWhile%2520sparse%2520autoencoders%2520%2528SAEs%2529%2520may%2520offer%2520a%2520potential%2520method%2520to%2520interpret%250Asteering%2520vectors%252C%2520recent%2520findings%2520show%2520that%2520SAE-reconstructed%2520vectors%2520often%250Alack%2520the%2520steering%2520properties%2520of%2520the%2520original%2520vectors.%2520This%2520paper%2520investigates%250Awhy%2520directly%2520applying%2520SAEs%2520to%2520steering%2520vectors%2520yields%2520misleading%250Adecompositions%252C%2520identifying%2520two%2520reasons%253A%2520%25281%2529%2520steering%2520vectors%2520fall%2520outside%2520the%250Ainput%2520distribution%2520for%2520which%2520SAEs%2520are%2520designed%252C%2520and%2520%25282%2529%2520steering%2520vectors%2520can%250Ahave%2520meaningful%2520negative%2520projections%2520in%2520feature%2520directions%252C%2520which%2520SAEs%2520are%2520not%250Adesigned%2520to%2520accommodate.%2520These%2520limitations%2520hinder%2520the%2520direct%2520use%2520of%2520SAEs%2520for%250Ainterpreting%2520steering%2520vectors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08790v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20sparse%20autoencoders%20be%20used%20to%20decompose%20and%20interpret%20steering%0A%20%20vectors%3F&entry.906535625=Harry%20Mayne%20and%20Yushi%20Yang%20and%20Adam%20Mahdi&entry.1292438233=%20%20Steering%20vectors%20are%20a%20promising%20approach%20to%20control%20the%20behaviour%20of%20large%0Alanguage%20models.%20However%2C%20their%20underlying%20mechanisms%20remain%20poorly%20understood.%0AWhile%20sparse%20autoencoders%20%28SAEs%29%20may%20offer%20a%20potential%20method%20to%20interpret%0Asteering%20vectors%2C%20recent%20findings%20show%20that%20SAE-reconstructed%20vectors%20often%0Alack%20the%20steering%20properties%20of%20the%20original%20vectors.%20This%20paper%20investigates%0Awhy%20directly%20applying%20SAEs%20to%20steering%20vectors%20yields%20misleading%0Adecompositions%2C%20identifying%20two%20reasons%3A%20%281%29%20steering%20vectors%20fall%20outside%20the%0Ainput%20distribution%20for%20which%20SAEs%20are%20designed%2C%20and%20%282%29%20steering%20vectors%20can%0Ahave%20meaningful%20negative%20projections%20in%20feature%20directions%2C%20which%20SAEs%20are%20not%0Adesigned%20to%20accommodate.%20These%20limitations%20hinder%20the%20direct%20use%20of%20SAEs%20for%0Ainterpreting%20steering%20vectors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08790v1&entry.124074799=Read"},
{"title": "Saliency Map-based Image Retrieval using Invariant Krawtchouk Moments", "author": "Ashkan Nejad and Mohammad Reza Faraji and Xiaojun Qi", "abstract": "  With the widespread adoption of digital devices equipped with cameras and the\nrapid development of Internet technology, numerous content-based image\nretrieval systems and novel image feature extraction techniques have emerged in\nrecent years. This paper introduces a saliency map-based image retrieval\napproach using invariant Krawtchouk moments (SM-IKM) to enhance retrieval speed\nand accuracy. The proposed method applies a global contrast-based salient\nregion detection algorithm to create a saliency map that effectively isolates\nthe foreground from the background. It then combines multiple orders of\ninvariant Krawtchouk moments (IKM) with local binary patterns (LBPs) and color\nhistograms to comprehensively represent the foreground and background.\nAdditionally, it incorporates LBPs derived from the saliency map to improve\ndiscriminative power, facilitating more precise image differentiation. A\nbag-of-visual-words (BoVW) model is employed to generate a codebook for\nclassification and discrimination. By using compact IKMs in the BoVW framework\nand integrating a range of region-based feature-including color histograms,\nLBPs, and saliency map-enhanced LBPs, our proposed SM-IKM achieves efficient\nand accurate image retrieval. xtensive experiments on publicly available\ndatasets, such as Caltech 101 and Wang, demonstrate that SM-IKM outperforms\nrecent state-of-the-art retrieval methods. The source code for SM-IKM is\navailable at github.com/arnejad/SMIKM.\n", "link": "http://arxiv.org/abs/2411.08567v1", "date": "2024-11-13", "relevancy": 2.5136, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5158}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4985}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Saliency%20Map-based%20Image%20Retrieval%20using%20Invariant%20Krawtchouk%20Moments&body=Title%3A%20Saliency%20Map-based%20Image%20Retrieval%20using%20Invariant%20Krawtchouk%20Moments%0AAuthor%3A%20Ashkan%20Nejad%20and%20Mohammad%20Reza%20Faraji%20and%20Xiaojun%20Qi%0AAbstract%3A%20%20%20With%20the%20widespread%20adoption%20of%20digital%20devices%20equipped%20with%20cameras%20and%20the%0Arapid%20development%20of%20Internet%20technology%2C%20numerous%20content-based%20image%0Aretrieval%20systems%20and%20novel%20image%20feature%20extraction%20techniques%20have%20emerged%20in%0Arecent%20years.%20This%20paper%20introduces%20a%20saliency%20map-based%20image%20retrieval%0Aapproach%20using%20invariant%20Krawtchouk%20moments%20%28SM-IKM%29%20to%20enhance%20retrieval%20speed%0Aand%20accuracy.%20The%20proposed%20method%20applies%20a%20global%20contrast-based%20salient%0Aregion%20detection%20algorithm%20to%20create%20a%20saliency%20map%20that%20effectively%20isolates%0Athe%20foreground%20from%20the%20background.%20It%20then%20combines%20multiple%20orders%20of%0Ainvariant%20Krawtchouk%20moments%20%28IKM%29%20with%20local%20binary%20patterns%20%28LBPs%29%20and%20color%0Ahistograms%20to%20comprehensively%20represent%20the%20foreground%20and%20background.%0AAdditionally%2C%20it%20incorporates%20LBPs%20derived%20from%20the%20saliency%20map%20to%20improve%0Adiscriminative%20power%2C%20facilitating%20more%20precise%20image%20differentiation.%20A%0Abag-of-visual-words%20%28BoVW%29%20model%20is%20employed%20to%20generate%20a%20codebook%20for%0Aclassification%20and%20discrimination.%20By%20using%20compact%20IKMs%20in%20the%20BoVW%20framework%0Aand%20integrating%20a%20range%20of%20region-based%20feature-including%20color%20histograms%2C%0ALBPs%2C%20and%20saliency%20map-enhanced%20LBPs%2C%20our%20proposed%20SM-IKM%20achieves%20efficient%0Aand%20accurate%20image%20retrieval.%20xtensive%20experiments%20on%20publicly%20available%0Adatasets%2C%20such%20as%20Caltech%20101%20and%20Wang%2C%20demonstrate%20that%20SM-IKM%20outperforms%0Arecent%20state-of-the-art%20retrieval%20methods.%20The%20source%20code%20for%20SM-IKM%20is%0Aavailable%20at%20github.com/arnejad/SMIKM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08567v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSaliency%2520Map-based%2520Image%2520Retrieval%2520using%2520Invariant%2520Krawtchouk%2520Moments%26entry.906535625%3DAshkan%2520Nejad%2520and%2520Mohammad%2520Reza%2520Faraji%2520and%2520Xiaojun%2520Qi%26entry.1292438233%3D%2520%2520With%2520the%2520widespread%2520adoption%2520of%2520digital%2520devices%2520equipped%2520with%2520cameras%2520and%2520the%250Arapid%2520development%2520of%2520Internet%2520technology%252C%2520numerous%2520content-based%2520image%250Aretrieval%2520systems%2520and%2520novel%2520image%2520feature%2520extraction%2520techniques%2520have%2520emerged%2520in%250Arecent%2520years.%2520This%2520paper%2520introduces%2520a%2520saliency%2520map-based%2520image%2520retrieval%250Aapproach%2520using%2520invariant%2520Krawtchouk%2520moments%2520%2528SM-IKM%2529%2520to%2520enhance%2520retrieval%2520speed%250Aand%2520accuracy.%2520The%2520proposed%2520method%2520applies%2520a%2520global%2520contrast-based%2520salient%250Aregion%2520detection%2520algorithm%2520to%2520create%2520a%2520saliency%2520map%2520that%2520effectively%2520isolates%250Athe%2520foreground%2520from%2520the%2520background.%2520It%2520then%2520combines%2520multiple%2520orders%2520of%250Ainvariant%2520Krawtchouk%2520moments%2520%2528IKM%2529%2520with%2520local%2520binary%2520patterns%2520%2528LBPs%2529%2520and%2520color%250Ahistograms%2520to%2520comprehensively%2520represent%2520the%2520foreground%2520and%2520background.%250AAdditionally%252C%2520it%2520incorporates%2520LBPs%2520derived%2520from%2520the%2520saliency%2520map%2520to%2520improve%250Adiscriminative%2520power%252C%2520facilitating%2520more%2520precise%2520image%2520differentiation.%2520A%250Abag-of-visual-words%2520%2528BoVW%2529%2520model%2520is%2520employed%2520to%2520generate%2520a%2520codebook%2520for%250Aclassification%2520and%2520discrimination.%2520By%2520using%2520compact%2520IKMs%2520in%2520the%2520BoVW%2520framework%250Aand%2520integrating%2520a%2520range%2520of%2520region-based%2520feature-including%2520color%2520histograms%252C%250ALBPs%252C%2520and%2520saliency%2520map-enhanced%2520LBPs%252C%2520our%2520proposed%2520SM-IKM%2520achieves%2520efficient%250Aand%2520accurate%2520image%2520retrieval.%2520xtensive%2520experiments%2520on%2520publicly%2520available%250Adatasets%252C%2520such%2520as%2520Caltech%2520101%2520and%2520Wang%252C%2520demonstrate%2520that%2520SM-IKM%2520outperforms%250Arecent%2520state-of-the-art%2520retrieval%2520methods.%2520The%2520source%2520code%2520for%2520SM-IKM%2520is%250Aavailable%2520at%2520github.com/arnejad/SMIKM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08567v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Saliency%20Map-based%20Image%20Retrieval%20using%20Invariant%20Krawtchouk%20Moments&entry.906535625=Ashkan%20Nejad%20and%20Mohammad%20Reza%20Faraji%20and%20Xiaojun%20Qi&entry.1292438233=%20%20With%20the%20widespread%20adoption%20of%20digital%20devices%20equipped%20with%20cameras%20and%20the%0Arapid%20development%20of%20Internet%20technology%2C%20numerous%20content-based%20image%0Aretrieval%20systems%20and%20novel%20image%20feature%20extraction%20techniques%20have%20emerged%20in%0Arecent%20years.%20This%20paper%20introduces%20a%20saliency%20map-based%20image%20retrieval%0Aapproach%20using%20invariant%20Krawtchouk%20moments%20%28SM-IKM%29%20to%20enhance%20retrieval%20speed%0Aand%20accuracy.%20The%20proposed%20method%20applies%20a%20global%20contrast-based%20salient%0Aregion%20detection%20algorithm%20to%20create%20a%20saliency%20map%20that%20effectively%20isolates%0Athe%20foreground%20from%20the%20background.%20It%20then%20combines%20multiple%20orders%20of%0Ainvariant%20Krawtchouk%20moments%20%28IKM%29%20with%20local%20binary%20patterns%20%28LBPs%29%20and%20color%0Ahistograms%20to%20comprehensively%20represent%20the%20foreground%20and%20background.%0AAdditionally%2C%20it%20incorporates%20LBPs%20derived%20from%20the%20saliency%20map%20to%20improve%0Adiscriminative%20power%2C%20facilitating%20more%20precise%20image%20differentiation.%20A%0Abag-of-visual-words%20%28BoVW%29%20model%20is%20employed%20to%20generate%20a%20codebook%20for%0Aclassification%20and%20discrimination.%20By%20using%20compact%20IKMs%20in%20the%20BoVW%20framework%0Aand%20integrating%20a%20range%20of%20region-based%20feature-including%20color%20histograms%2C%0ALBPs%2C%20and%20saliency%20map-enhanced%20LBPs%2C%20our%20proposed%20SM-IKM%20achieves%20efficient%0Aand%20accurate%20image%20retrieval.%20xtensive%20experiments%20on%20publicly%20available%0Adatasets%2C%20such%20as%20Caltech%20101%20and%20Wang%2C%20demonstrate%20that%20SM-IKM%20outperforms%0Arecent%20state-of-the-art%20retrieval%20methods.%20The%20source%20code%20for%20SM-IKM%20is%0Aavailable%20at%20github.com/arnejad/SMIKM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08567v1&entry.124074799=Read"},
{"title": "Learning Model Agnostic Explanations via Constraint Programming", "author": "Frederic Koriche and Jean-Marie Lagniez and Stefan Mengel and Chi Tran", "abstract": "  Interpretable Machine Learning faces a recurring challenge of explaining the\npredictions made by opaque classifiers such as ensemble models, kernel methods,\nor neural networks in terms that are understandable to humans. When the model\nis viewed as a black box, the objective is to identify a small set of features\nthat jointly determine the black box response with minimal error. However,\nfinding such model-agnostic explanations is computationally demanding, as the\nproblem is intractable even for binary classifiers. In this paper, the task is\nframed as a Constraint Optimization Problem, where the constraint solver seeks\nan explanation of minimum error and bounded size for an input data instance and\na set of samples generated by the black box. From a theoretical perspective,\nthis constraint programming approach offers PAC-style guarantees for the output\nexplanation. We evaluate the approach empirically on various datasets and show\nthat it statistically outperforms the state-of-the-art heuristic Anchors\nmethod.\n", "link": "http://arxiv.org/abs/2411.08478v1", "date": "2024-11-13", "relevancy": 2.5134, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5047}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5047}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Model%20Agnostic%20Explanations%20via%20Constraint%20Programming&body=Title%3A%20Learning%20Model%20Agnostic%20Explanations%20via%20Constraint%20Programming%0AAuthor%3A%20Frederic%20Koriche%20and%20Jean-Marie%20Lagniez%20and%20Stefan%20Mengel%20and%20Chi%20Tran%0AAbstract%3A%20%20%20Interpretable%20Machine%20Learning%20faces%20a%20recurring%20challenge%20of%20explaining%20the%0Apredictions%20made%20by%20opaque%20classifiers%20such%20as%20ensemble%20models%2C%20kernel%20methods%2C%0Aor%20neural%20networks%20in%20terms%20that%20are%20understandable%20to%20humans.%20When%20the%20model%0Ais%20viewed%20as%20a%20black%20box%2C%20the%20objective%20is%20to%20identify%20a%20small%20set%20of%20features%0Athat%20jointly%20determine%20the%20black%20box%20response%20with%20minimal%20error.%20However%2C%0Afinding%20such%20model-agnostic%20explanations%20is%20computationally%20demanding%2C%20as%20the%0Aproblem%20is%20intractable%20even%20for%20binary%20classifiers.%20In%20this%20paper%2C%20the%20task%20is%0Aframed%20as%20a%20Constraint%20Optimization%20Problem%2C%20where%20the%20constraint%20solver%20seeks%0Aan%20explanation%20of%20minimum%20error%20and%20bounded%20size%20for%20an%20input%20data%20instance%20and%0Aa%20set%20of%20samples%20generated%20by%20the%20black%20box.%20From%20a%20theoretical%20perspective%2C%0Athis%20constraint%20programming%20approach%20offers%20PAC-style%20guarantees%20for%20the%20output%0Aexplanation.%20We%20evaluate%20the%20approach%20empirically%20on%20various%20datasets%20and%20show%0Athat%20it%20statistically%20outperforms%20the%20state-of-the-art%20heuristic%20Anchors%0Amethod.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08478v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Model%2520Agnostic%2520Explanations%2520via%2520Constraint%2520Programming%26entry.906535625%3DFrederic%2520Koriche%2520and%2520Jean-Marie%2520Lagniez%2520and%2520Stefan%2520Mengel%2520and%2520Chi%2520Tran%26entry.1292438233%3D%2520%2520Interpretable%2520Machine%2520Learning%2520faces%2520a%2520recurring%2520challenge%2520of%2520explaining%2520the%250Apredictions%2520made%2520by%2520opaque%2520classifiers%2520such%2520as%2520ensemble%2520models%252C%2520kernel%2520methods%252C%250Aor%2520neural%2520networks%2520in%2520terms%2520that%2520are%2520understandable%2520to%2520humans.%2520When%2520the%2520model%250Ais%2520viewed%2520as%2520a%2520black%2520box%252C%2520the%2520objective%2520is%2520to%2520identify%2520a%2520small%2520set%2520of%2520features%250Athat%2520jointly%2520determine%2520the%2520black%2520box%2520response%2520with%2520minimal%2520error.%2520However%252C%250Afinding%2520such%2520model-agnostic%2520explanations%2520is%2520computationally%2520demanding%252C%2520as%2520the%250Aproblem%2520is%2520intractable%2520even%2520for%2520binary%2520classifiers.%2520In%2520this%2520paper%252C%2520the%2520task%2520is%250Aframed%2520as%2520a%2520Constraint%2520Optimization%2520Problem%252C%2520where%2520the%2520constraint%2520solver%2520seeks%250Aan%2520explanation%2520of%2520minimum%2520error%2520and%2520bounded%2520size%2520for%2520an%2520input%2520data%2520instance%2520and%250Aa%2520set%2520of%2520samples%2520generated%2520by%2520the%2520black%2520box.%2520From%2520a%2520theoretical%2520perspective%252C%250Athis%2520constraint%2520programming%2520approach%2520offers%2520PAC-style%2520guarantees%2520for%2520the%2520output%250Aexplanation.%2520We%2520evaluate%2520the%2520approach%2520empirically%2520on%2520various%2520datasets%2520and%2520show%250Athat%2520it%2520statistically%2520outperforms%2520the%2520state-of-the-art%2520heuristic%2520Anchors%250Amethod.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08478v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Model%20Agnostic%20Explanations%20via%20Constraint%20Programming&entry.906535625=Frederic%20Koriche%20and%20Jean-Marie%20Lagniez%20and%20Stefan%20Mengel%20and%20Chi%20Tran&entry.1292438233=%20%20Interpretable%20Machine%20Learning%20faces%20a%20recurring%20challenge%20of%20explaining%20the%0Apredictions%20made%20by%20opaque%20classifiers%20such%20as%20ensemble%20models%2C%20kernel%20methods%2C%0Aor%20neural%20networks%20in%20terms%20that%20are%20understandable%20to%20humans.%20When%20the%20model%0Ais%20viewed%20as%20a%20black%20box%2C%20the%20objective%20is%20to%20identify%20a%20small%20set%20of%20features%0Athat%20jointly%20determine%20the%20black%20box%20response%20with%20minimal%20error.%20However%2C%0Afinding%20such%20model-agnostic%20explanations%20is%20computationally%20demanding%2C%20as%20the%0Aproblem%20is%20intractable%20even%20for%20binary%20classifiers.%20In%20this%20paper%2C%20the%20task%20is%0Aframed%20as%20a%20Constraint%20Optimization%20Problem%2C%20where%20the%20constraint%20solver%20seeks%0Aan%20explanation%20of%20minimum%20error%20and%20bounded%20size%20for%20an%20input%20data%20instance%20and%0Aa%20set%20of%20samples%20generated%20by%20the%20black%20box.%20From%20a%20theoretical%20perspective%2C%0Athis%20constraint%20programming%20approach%20offers%20PAC-style%20guarantees%20for%20the%20output%0Aexplanation.%20We%20evaluate%20the%20approach%20empirically%20on%20various%20datasets%20and%20show%0Athat%20it%20statistically%20outperforms%20the%20state-of-the-art%20heuristic%20Anchors%0Amethod.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08478v1&entry.124074799=Read"},
{"title": "Toward Human Understanding with Controllable Synthesis", "author": "Hanz Cuevas-Velasquez and Priyanka Patel and Haiwen Feng and Michael Black", "abstract": "  Training methods to perform robust 3D human pose and shape (HPS) estimation\nrequires diverse training images with accurate ground truth. While BEDLAM\ndemonstrates the potential of traditional procedural graphics to generate such\ndata, the training images are clearly synthetic. In contrast, generative image\nmodels produce highly realistic images but without ground truth. Putting these\nmethods together seems straightforward: use a generative model with the body\nground truth as controlling signal. However, we find that, the more realistic\nthe generated images, the more they deviate from the ground truth, making them\ninappropriate for training and evaluation. Enhancements of realistic details,\nsuch as clothing and facial expressions, can lead to subtle yet significant\ndeviations from the ground truth, potentially misleading training models. We\nempirically verify that this misalignment causes the accuracy of HPS networks\nto decline when trained with generated images. To address this, we design a\ncontrollable synthesis method that effectively balances image realism with\nprecise ground truth. We use this to create the Generative BEDLAM (Gen-B)\ndataset, which improves the realism of the existing synthetic BEDLAM dataset\nwhile preserving ground truth accuracy. We perform extensive experiments, with\nvarious noise-conditioning strategies, to evaluate the tradeoff between visual\nrealism and HPS accuracy. We show, for the first time, that generative image\nmodels can be controlled by traditional graphics methods to produce training\ndata that increases the accuracy of HPS methods.\n", "link": "http://arxiv.org/abs/2411.08663v1", "date": "2024-11-13", "relevancy": 2.5034, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6409}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6336}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Human%20Understanding%20with%20Controllable%20Synthesis&body=Title%3A%20Toward%20Human%20Understanding%20with%20Controllable%20Synthesis%0AAuthor%3A%20Hanz%20Cuevas-Velasquez%20and%20Priyanka%20Patel%20and%20Haiwen%20Feng%20and%20Michael%20Black%0AAbstract%3A%20%20%20Training%20methods%20to%20perform%20robust%203D%20human%20pose%20and%20shape%20%28HPS%29%20estimation%0Arequires%20diverse%20training%20images%20with%20accurate%20ground%20truth.%20While%20BEDLAM%0Ademonstrates%20the%20potential%20of%20traditional%20procedural%20graphics%20to%20generate%20such%0Adata%2C%20the%20training%20images%20are%20clearly%20synthetic.%20In%20contrast%2C%20generative%20image%0Amodels%20produce%20highly%20realistic%20images%20but%20without%20ground%20truth.%20Putting%20these%0Amethods%20together%20seems%20straightforward%3A%20use%20a%20generative%20model%20with%20the%20body%0Aground%20truth%20as%20controlling%20signal.%20However%2C%20we%20find%20that%2C%20the%20more%20realistic%0Athe%20generated%20images%2C%20the%20more%20they%20deviate%20from%20the%20ground%20truth%2C%20making%20them%0Ainappropriate%20for%20training%20and%20evaluation.%20Enhancements%20of%20realistic%20details%2C%0Asuch%20as%20clothing%20and%20facial%20expressions%2C%20can%20lead%20to%20subtle%20yet%20significant%0Adeviations%20from%20the%20ground%20truth%2C%20potentially%20misleading%20training%20models.%20We%0Aempirically%20verify%20that%20this%20misalignment%20causes%20the%20accuracy%20of%20HPS%20networks%0Ato%20decline%20when%20trained%20with%20generated%20images.%20To%20address%20this%2C%20we%20design%20a%0Acontrollable%20synthesis%20method%20that%20effectively%20balances%20image%20realism%20with%0Aprecise%20ground%20truth.%20We%20use%20this%20to%20create%20the%20Generative%20BEDLAM%20%28Gen-B%29%0Adataset%2C%20which%20improves%20the%20realism%20of%20the%20existing%20synthetic%20BEDLAM%20dataset%0Awhile%20preserving%20ground%20truth%20accuracy.%20We%20perform%20extensive%20experiments%2C%20with%0Avarious%20noise-conditioning%20strategies%2C%20to%20evaluate%20the%20tradeoff%20between%20visual%0Arealism%20and%20HPS%20accuracy.%20We%20show%2C%20for%20the%20first%20time%2C%20that%20generative%20image%0Amodels%20can%20be%20controlled%20by%20traditional%20graphics%20methods%20to%20produce%20training%0Adata%20that%20increases%20the%20accuracy%20of%20HPS%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08663v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Human%2520Understanding%2520with%2520Controllable%2520Synthesis%26entry.906535625%3DHanz%2520Cuevas-Velasquez%2520and%2520Priyanka%2520Patel%2520and%2520Haiwen%2520Feng%2520and%2520Michael%2520Black%26entry.1292438233%3D%2520%2520Training%2520methods%2520to%2520perform%2520robust%25203D%2520human%2520pose%2520and%2520shape%2520%2528HPS%2529%2520estimation%250Arequires%2520diverse%2520training%2520images%2520with%2520accurate%2520ground%2520truth.%2520While%2520BEDLAM%250Ademonstrates%2520the%2520potential%2520of%2520traditional%2520procedural%2520graphics%2520to%2520generate%2520such%250Adata%252C%2520the%2520training%2520images%2520are%2520clearly%2520synthetic.%2520In%2520contrast%252C%2520generative%2520image%250Amodels%2520produce%2520highly%2520realistic%2520images%2520but%2520without%2520ground%2520truth.%2520Putting%2520these%250Amethods%2520together%2520seems%2520straightforward%253A%2520use%2520a%2520generative%2520model%2520with%2520the%2520body%250Aground%2520truth%2520as%2520controlling%2520signal.%2520However%252C%2520we%2520find%2520that%252C%2520the%2520more%2520realistic%250Athe%2520generated%2520images%252C%2520the%2520more%2520they%2520deviate%2520from%2520the%2520ground%2520truth%252C%2520making%2520them%250Ainappropriate%2520for%2520training%2520and%2520evaluation.%2520Enhancements%2520of%2520realistic%2520details%252C%250Asuch%2520as%2520clothing%2520and%2520facial%2520expressions%252C%2520can%2520lead%2520to%2520subtle%2520yet%2520significant%250Adeviations%2520from%2520the%2520ground%2520truth%252C%2520potentially%2520misleading%2520training%2520models.%2520We%250Aempirically%2520verify%2520that%2520this%2520misalignment%2520causes%2520the%2520accuracy%2520of%2520HPS%2520networks%250Ato%2520decline%2520when%2520trained%2520with%2520generated%2520images.%2520To%2520address%2520this%252C%2520we%2520design%2520a%250Acontrollable%2520synthesis%2520method%2520that%2520effectively%2520balances%2520image%2520realism%2520with%250Aprecise%2520ground%2520truth.%2520We%2520use%2520this%2520to%2520create%2520the%2520Generative%2520BEDLAM%2520%2528Gen-B%2529%250Adataset%252C%2520which%2520improves%2520the%2520realism%2520of%2520the%2520existing%2520synthetic%2520BEDLAM%2520dataset%250Awhile%2520preserving%2520ground%2520truth%2520accuracy.%2520We%2520perform%2520extensive%2520experiments%252C%2520with%250Avarious%2520noise-conditioning%2520strategies%252C%2520to%2520evaluate%2520the%2520tradeoff%2520between%2520visual%250Arealism%2520and%2520HPS%2520accuracy.%2520We%2520show%252C%2520for%2520the%2520first%2520time%252C%2520that%2520generative%2520image%250Amodels%2520can%2520be%2520controlled%2520by%2520traditional%2520graphics%2520methods%2520to%2520produce%2520training%250Adata%2520that%2520increases%2520the%2520accuracy%2520of%2520HPS%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08663v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Human%20Understanding%20with%20Controllable%20Synthesis&entry.906535625=Hanz%20Cuevas-Velasquez%20and%20Priyanka%20Patel%20and%20Haiwen%20Feng%20and%20Michael%20Black&entry.1292438233=%20%20Training%20methods%20to%20perform%20robust%203D%20human%20pose%20and%20shape%20%28HPS%29%20estimation%0Arequires%20diverse%20training%20images%20with%20accurate%20ground%20truth.%20While%20BEDLAM%0Ademonstrates%20the%20potential%20of%20traditional%20procedural%20graphics%20to%20generate%20such%0Adata%2C%20the%20training%20images%20are%20clearly%20synthetic.%20In%20contrast%2C%20generative%20image%0Amodels%20produce%20highly%20realistic%20images%20but%20without%20ground%20truth.%20Putting%20these%0Amethods%20together%20seems%20straightforward%3A%20use%20a%20generative%20model%20with%20the%20body%0Aground%20truth%20as%20controlling%20signal.%20However%2C%20we%20find%20that%2C%20the%20more%20realistic%0Athe%20generated%20images%2C%20the%20more%20they%20deviate%20from%20the%20ground%20truth%2C%20making%20them%0Ainappropriate%20for%20training%20and%20evaluation.%20Enhancements%20of%20realistic%20details%2C%0Asuch%20as%20clothing%20and%20facial%20expressions%2C%20can%20lead%20to%20subtle%20yet%20significant%0Adeviations%20from%20the%20ground%20truth%2C%20potentially%20misleading%20training%20models.%20We%0Aempirically%20verify%20that%20this%20misalignment%20causes%20the%20accuracy%20of%20HPS%20networks%0Ato%20decline%20when%20trained%20with%20generated%20images.%20To%20address%20this%2C%20we%20design%20a%0Acontrollable%20synthesis%20method%20that%20effectively%20balances%20image%20realism%20with%0Aprecise%20ground%20truth.%20We%20use%20this%20to%20create%20the%20Generative%20BEDLAM%20%28Gen-B%29%0Adataset%2C%20which%20improves%20the%20realism%20of%20the%20existing%20synthetic%20BEDLAM%20dataset%0Awhile%20preserving%20ground%20truth%20accuracy.%20We%20perform%20extensive%20experiments%2C%20with%0Avarious%20noise-conditioning%20strategies%2C%20to%20evaluate%20the%20tradeoff%20between%20visual%0Arealism%20and%20HPS%20accuracy.%20We%20show%2C%20for%20the%20first%20time%2C%20that%20generative%20image%0Amodels%20can%20be%20controlled%20by%20traditional%20graphics%20methods%20to%20produce%20training%0Adata%20that%20increases%20the%20accuracy%20of%20HPS%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08663v1&entry.124074799=Read"},
{"title": "Multiple noncooperative targets encirclement by relative distance-based\n  positioning and neural antisynchronization control", "author": "Fen Liu and Shenghai Yuan and Wei Meng and Rong Su and Lihua Xie", "abstract": "  From prehistoric encirclement for hunting to GPS orbiting the earth for\npositioning, target encirclement has numerous real world applications. However,\nencircling multiple non-cooperative targets in GPS-denied environments remains\nchallenging. In this work, multiple targets encirclement by using a minimum of\ntwo tasking agents, is considered where the relative distance measurements\nbetween the agents and the targets can be obtained by using onboard sensors.\nBased on the measurements, the center of all the targets is estimated directly\nby a fuzzy wavelet neural network (FWNN) and the least squares fit method.\nThen, a new distributed anti-synchronization controller (DASC) is designed so\nthat the two tasking agents are able to encircle all targets while staying\nopposite to each other. In particular, the radius of the desired encirclement\ntrajectory can be dynamically determined to avoid potential collisions between\nthe two agents and all targets. Based on the Lyapunov stability analysis\nmethod, the convergence proofs of the neural network prediction error, the\ntarget-center position estimation error, and the controller error are addressed\nrespectively. Finally, both numerical simulations and UAV flight experiments\nare conducted to demonstrate the validity of the encirclement algorithms. The\nflight tests recorded video and other simulation results can be found in\nhttps://youtu.be/B8uTorBNrl4.\n", "link": "http://arxiv.org/abs/2411.07590v2", "date": "2024-11-13", "relevancy": 2.4966, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5169}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5003}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multiple%20noncooperative%20targets%20encirclement%20by%20relative%20distance-based%0A%20%20positioning%20and%20neural%20antisynchronization%20control&body=Title%3A%20Multiple%20noncooperative%20targets%20encirclement%20by%20relative%20distance-based%0A%20%20positioning%20and%20neural%20antisynchronization%20control%0AAuthor%3A%20Fen%20Liu%20and%20Shenghai%20Yuan%20and%20Wei%20Meng%20and%20Rong%20Su%20and%20Lihua%20Xie%0AAbstract%3A%20%20%20From%20prehistoric%20encirclement%20for%20hunting%20to%20GPS%20orbiting%20the%20earth%20for%0Apositioning%2C%20target%20encirclement%20has%20numerous%20real%20world%20applications.%20However%2C%0Aencircling%20multiple%20non-cooperative%20targets%20in%20GPS-denied%20environments%20remains%0Achallenging.%20In%20this%20work%2C%20multiple%20targets%20encirclement%20by%20using%20a%20minimum%20of%0Atwo%20tasking%20agents%2C%20is%20considered%20where%20the%20relative%20distance%20measurements%0Abetween%20the%20agents%20and%20the%20targets%20can%20be%20obtained%20by%20using%20onboard%20sensors.%0ABased%20on%20the%20measurements%2C%20the%20center%20of%20all%20the%20targets%20is%20estimated%20directly%0Aby%20a%20fuzzy%20wavelet%20neural%20network%20%28FWNN%29%20and%20the%20least%20squares%20fit%20method.%0AThen%2C%20a%20new%20distributed%20anti-synchronization%20controller%20%28DASC%29%20is%20designed%20so%0Athat%20the%20two%20tasking%20agents%20are%20able%20to%20encircle%20all%20targets%20while%20staying%0Aopposite%20to%20each%20other.%20In%20particular%2C%20the%20radius%20of%20the%20desired%20encirclement%0Atrajectory%20can%20be%20dynamically%20determined%20to%20avoid%20potential%20collisions%20between%0Athe%20two%20agents%20and%20all%20targets.%20Based%20on%20the%20Lyapunov%20stability%20analysis%0Amethod%2C%20the%20convergence%20proofs%20of%20the%20neural%20network%20prediction%20error%2C%20the%0Atarget-center%20position%20estimation%20error%2C%20and%20the%20controller%20error%20are%20addressed%0Arespectively.%20Finally%2C%20both%20numerical%20simulations%20and%20UAV%20flight%20experiments%0Aare%20conducted%20to%20demonstrate%20the%20validity%20of%20the%20encirclement%20algorithms.%20The%0Aflight%20tests%20recorded%20video%20and%20other%20simulation%20results%20can%20be%20found%20in%0Ahttps%3A//youtu.be/B8uTorBNrl4.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07590v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiple%2520noncooperative%2520targets%2520encirclement%2520by%2520relative%2520distance-based%250A%2520%2520positioning%2520and%2520neural%2520antisynchronization%2520control%26entry.906535625%3DFen%2520Liu%2520and%2520Shenghai%2520Yuan%2520and%2520Wei%2520Meng%2520and%2520Rong%2520Su%2520and%2520Lihua%2520Xie%26entry.1292438233%3D%2520%2520From%2520prehistoric%2520encirclement%2520for%2520hunting%2520to%2520GPS%2520orbiting%2520the%2520earth%2520for%250Apositioning%252C%2520target%2520encirclement%2520has%2520numerous%2520real%2520world%2520applications.%2520However%252C%250Aencircling%2520multiple%2520non-cooperative%2520targets%2520in%2520GPS-denied%2520environments%2520remains%250Achallenging.%2520In%2520this%2520work%252C%2520multiple%2520targets%2520encirclement%2520by%2520using%2520a%2520minimum%2520of%250Atwo%2520tasking%2520agents%252C%2520is%2520considered%2520where%2520the%2520relative%2520distance%2520measurements%250Abetween%2520the%2520agents%2520and%2520the%2520targets%2520can%2520be%2520obtained%2520by%2520using%2520onboard%2520sensors.%250ABased%2520on%2520the%2520measurements%252C%2520the%2520center%2520of%2520all%2520the%2520targets%2520is%2520estimated%2520directly%250Aby%2520a%2520fuzzy%2520wavelet%2520neural%2520network%2520%2528FWNN%2529%2520and%2520the%2520least%2520squares%2520fit%2520method.%250AThen%252C%2520a%2520new%2520distributed%2520anti-synchronization%2520controller%2520%2528DASC%2529%2520is%2520designed%2520so%250Athat%2520the%2520two%2520tasking%2520agents%2520are%2520able%2520to%2520encircle%2520all%2520targets%2520while%2520staying%250Aopposite%2520to%2520each%2520other.%2520In%2520particular%252C%2520the%2520radius%2520of%2520the%2520desired%2520encirclement%250Atrajectory%2520can%2520be%2520dynamically%2520determined%2520to%2520avoid%2520potential%2520collisions%2520between%250Athe%2520two%2520agents%2520and%2520all%2520targets.%2520Based%2520on%2520the%2520Lyapunov%2520stability%2520analysis%250Amethod%252C%2520the%2520convergence%2520proofs%2520of%2520the%2520neural%2520network%2520prediction%2520error%252C%2520the%250Atarget-center%2520position%2520estimation%2520error%252C%2520and%2520the%2520controller%2520error%2520are%2520addressed%250Arespectively.%2520Finally%252C%2520both%2520numerical%2520simulations%2520and%2520UAV%2520flight%2520experiments%250Aare%2520conducted%2520to%2520demonstrate%2520the%2520validity%2520of%2520the%2520encirclement%2520algorithms.%2520The%250Aflight%2520tests%2520recorded%2520video%2520and%2520other%2520simulation%2520results%2520can%2520be%2520found%2520in%250Ahttps%253A//youtu.be/B8uTorBNrl4.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07590v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multiple%20noncooperative%20targets%20encirclement%20by%20relative%20distance-based%0A%20%20positioning%20and%20neural%20antisynchronization%20control&entry.906535625=Fen%20Liu%20and%20Shenghai%20Yuan%20and%20Wei%20Meng%20and%20Rong%20Su%20and%20Lihua%20Xie&entry.1292438233=%20%20From%20prehistoric%20encirclement%20for%20hunting%20to%20GPS%20orbiting%20the%20earth%20for%0Apositioning%2C%20target%20encirclement%20has%20numerous%20real%20world%20applications.%20However%2C%0Aencircling%20multiple%20non-cooperative%20targets%20in%20GPS-denied%20environments%20remains%0Achallenging.%20In%20this%20work%2C%20multiple%20targets%20encirclement%20by%20using%20a%20minimum%20of%0Atwo%20tasking%20agents%2C%20is%20considered%20where%20the%20relative%20distance%20measurements%0Abetween%20the%20agents%20and%20the%20targets%20can%20be%20obtained%20by%20using%20onboard%20sensors.%0ABased%20on%20the%20measurements%2C%20the%20center%20of%20all%20the%20targets%20is%20estimated%20directly%0Aby%20a%20fuzzy%20wavelet%20neural%20network%20%28FWNN%29%20and%20the%20least%20squares%20fit%20method.%0AThen%2C%20a%20new%20distributed%20anti-synchronization%20controller%20%28DASC%29%20is%20designed%20so%0Athat%20the%20two%20tasking%20agents%20are%20able%20to%20encircle%20all%20targets%20while%20staying%0Aopposite%20to%20each%20other.%20In%20particular%2C%20the%20radius%20of%20the%20desired%20encirclement%0Atrajectory%20can%20be%20dynamically%20determined%20to%20avoid%20potential%20collisions%20between%0Athe%20two%20agents%20and%20all%20targets.%20Based%20on%20the%20Lyapunov%20stability%20analysis%0Amethod%2C%20the%20convergence%20proofs%20of%20the%20neural%20network%20prediction%20error%2C%20the%0Atarget-center%20position%20estimation%20error%2C%20and%20the%20controller%20error%20are%20addressed%0Arespectively.%20Finally%2C%20both%20numerical%20simulations%20and%20UAV%20flight%20experiments%0Aare%20conducted%20to%20demonstrate%20the%20validity%20of%20the%20encirclement%20algorithms.%20The%0Aflight%20tests%20recorded%20video%20and%20other%20simulation%20results%20can%20be%20found%20in%0Ahttps%3A//youtu.be/B8uTorBNrl4.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07590v2&entry.124074799=Read"},
{"title": "Voxeland: Probabilistic Instance-Aware Semantic Mapping with\n  Evidence-based Uncertainty Quantification", "author": "Jose-Luis Matez-Bandera and Pepe Ojeda and Javier Monroy and Javier Gonzalez-Jimenez and Jose-Raul Ruiz-Sarmiento", "abstract": "  Robots in human-centered environments require accurate scene understanding to\nperform high-level tasks effectively. This understanding can be achieved\nthrough instance-aware semantic mapping, which involves reconstructing elements\nat the level of individual instances. Neural networks, the de facto solution\nfor scene understanding, still face limitations such as overconfident incorrect\npredictions with out-of-distribution objects or generating inaccurate\nmasks.Placing excessive reliance on these predictions makes the reconstruction\nsusceptible to errors, reducing the robustness of the resulting maps and\nhampering robot operation. In this work, we propose Voxeland, a probabilistic\nframework for incrementally building instance-aware semantic maps. Inspired by\nthe Theory of Evidence, Voxeland treats neural network predictions as\nsubjective opinions regarding map instances at both geometric and semantic\nlevels. These opinions are aggregated over time to form evidences, which are\nformalized through a probabilistic model. This enables us to quantify\nuncertainty in the reconstruction process, facilitating the identification of\nmap areas requiring improvement (e.g. reobservation or reclassification). As\none strategy to exploit this, we incorporate a Large Vision-Language Model\n(LVLM) to perform semantic level disambiguation for instances with high\nuncertainty. Results from the standard benchmarking on the publicly available\nSceneNN dataset demonstrate that Voxeland outperforms state-of-the-art methods,\nhighlighting the benefits of incorporating and leveraging both instance- and\nsemantic-level uncertainties to enhance reconstruction robustness. This is\nfurther validated through qualitative experiments conducted on the real-world\nScanNet dataset.\n", "link": "http://arxiv.org/abs/2411.08727v1", "date": "2024-11-13", "relevancy": 2.4964, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6522}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6246}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Voxeland%3A%20Probabilistic%20Instance-Aware%20Semantic%20Mapping%20with%0A%20%20Evidence-based%20Uncertainty%20Quantification&body=Title%3A%20Voxeland%3A%20Probabilistic%20Instance-Aware%20Semantic%20Mapping%20with%0A%20%20Evidence-based%20Uncertainty%20Quantification%0AAuthor%3A%20Jose-Luis%20Matez-Bandera%20and%20Pepe%20Ojeda%20and%20Javier%20Monroy%20and%20Javier%20Gonzalez-Jimenez%20and%20Jose-Raul%20Ruiz-Sarmiento%0AAbstract%3A%20%20%20Robots%20in%20human-centered%20environments%20require%20accurate%20scene%20understanding%20to%0Aperform%20high-level%20tasks%20effectively.%20This%20understanding%20can%20be%20achieved%0Athrough%20instance-aware%20semantic%20mapping%2C%20which%20involves%20reconstructing%20elements%0Aat%20the%20level%20of%20individual%20instances.%20Neural%20networks%2C%20the%20de%20facto%20solution%0Afor%20scene%20understanding%2C%20still%20face%20limitations%20such%20as%20overconfident%20incorrect%0Apredictions%20with%20out-of-distribution%20objects%20or%20generating%20inaccurate%0Amasks.Placing%20excessive%20reliance%20on%20these%20predictions%20makes%20the%20reconstruction%0Asusceptible%20to%20errors%2C%20reducing%20the%20robustness%20of%20the%20resulting%20maps%20and%0Ahampering%20robot%20operation.%20In%20this%20work%2C%20we%20propose%20Voxeland%2C%20a%20probabilistic%0Aframework%20for%20incrementally%20building%20instance-aware%20semantic%20maps.%20Inspired%20by%0Athe%20Theory%20of%20Evidence%2C%20Voxeland%20treats%20neural%20network%20predictions%20as%0Asubjective%20opinions%20regarding%20map%20instances%20at%20both%20geometric%20and%20semantic%0Alevels.%20These%20opinions%20are%20aggregated%20over%20time%20to%20form%20evidences%2C%20which%20are%0Aformalized%20through%20a%20probabilistic%20model.%20This%20enables%20us%20to%20quantify%0Auncertainty%20in%20the%20reconstruction%20process%2C%20facilitating%20the%20identification%20of%0Amap%20areas%20requiring%20improvement%20%28e.g.%20reobservation%20or%20reclassification%29.%20As%0Aone%20strategy%20to%20exploit%20this%2C%20we%20incorporate%20a%20Large%20Vision-Language%20Model%0A%28LVLM%29%20to%20perform%20semantic%20level%20disambiguation%20for%20instances%20with%20high%0Auncertainty.%20Results%20from%20the%20standard%20benchmarking%20on%20the%20publicly%20available%0ASceneNN%20dataset%20demonstrate%20that%20Voxeland%20outperforms%20state-of-the-art%20methods%2C%0Ahighlighting%20the%20benefits%20of%20incorporating%20and%20leveraging%20both%20instance-%20and%0Asemantic-level%20uncertainties%20to%20enhance%20reconstruction%20robustness.%20This%20is%0Afurther%20validated%20through%20qualitative%20experiments%20conducted%20on%20the%20real-world%0AScanNet%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08727v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVoxeland%253A%2520Probabilistic%2520Instance-Aware%2520Semantic%2520Mapping%2520with%250A%2520%2520Evidence-based%2520Uncertainty%2520Quantification%26entry.906535625%3DJose-Luis%2520Matez-Bandera%2520and%2520Pepe%2520Ojeda%2520and%2520Javier%2520Monroy%2520and%2520Javier%2520Gonzalez-Jimenez%2520and%2520Jose-Raul%2520Ruiz-Sarmiento%26entry.1292438233%3D%2520%2520Robots%2520in%2520human-centered%2520environments%2520require%2520accurate%2520scene%2520understanding%2520to%250Aperform%2520high-level%2520tasks%2520effectively.%2520This%2520understanding%2520can%2520be%2520achieved%250Athrough%2520instance-aware%2520semantic%2520mapping%252C%2520which%2520involves%2520reconstructing%2520elements%250Aat%2520the%2520level%2520of%2520individual%2520instances.%2520Neural%2520networks%252C%2520the%2520de%2520facto%2520solution%250Afor%2520scene%2520understanding%252C%2520still%2520face%2520limitations%2520such%2520as%2520overconfident%2520incorrect%250Apredictions%2520with%2520out-of-distribution%2520objects%2520or%2520generating%2520inaccurate%250Amasks.Placing%2520excessive%2520reliance%2520on%2520these%2520predictions%2520makes%2520the%2520reconstruction%250Asusceptible%2520to%2520errors%252C%2520reducing%2520the%2520robustness%2520of%2520the%2520resulting%2520maps%2520and%250Ahampering%2520robot%2520operation.%2520In%2520this%2520work%252C%2520we%2520propose%2520Voxeland%252C%2520a%2520probabilistic%250Aframework%2520for%2520incrementally%2520building%2520instance-aware%2520semantic%2520maps.%2520Inspired%2520by%250Athe%2520Theory%2520of%2520Evidence%252C%2520Voxeland%2520treats%2520neural%2520network%2520predictions%2520as%250Asubjective%2520opinions%2520regarding%2520map%2520instances%2520at%2520both%2520geometric%2520and%2520semantic%250Alevels.%2520These%2520opinions%2520are%2520aggregated%2520over%2520time%2520to%2520form%2520evidences%252C%2520which%2520are%250Aformalized%2520through%2520a%2520probabilistic%2520model.%2520This%2520enables%2520us%2520to%2520quantify%250Auncertainty%2520in%2520the%2520reconstruction%2520process%252C%2520facilitating%2520the%2520identification%2520of%250Amap%2520areas%2520requiring%2520improvement%2520%2528e.g.%2520reobservation%2520or%2520reclassification%2529.%2520As%250Aone%2520strategy%2520to%2520exploit%2520this%252C%2520we%2520incorporate%2520a%2520Large%2520Vision-Language%2520Model%250A%2528LVLM%2529%2520to%2520perform%2520semantic%2520level%2520disambiguation%2520for%2520instances%2520with%2520high%250Auncertainty.%2520Results%2520from%2520the%2520standard%2520benchmarking%2520on%2520the%2520publicly%2520available%250ASceneNN%2520dataset%2520demonstrate%2520that%2520Voxeland%2520outperforms%2520state-of-the-art%2520methods%252C%250Ahighlighting%2520the%2520benefits%2520of%2520incorporating%2520and%2520leveraging%2520both%2520instance-%2520and%250Asemantic-level%2520uncertainties%2520to%2520enhance%2520reconstruction%2520robustness.%2520This%2520is%250Afurther%2520validated%2520through%2520qualitative%2520experiments%2520conducted%2520on%2520the%2520real-world%250AScanNet%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08727v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Voxeland%3A%20Probabilistic%20Instance-Aware%20Semantic%20Mapping%20with%0A%20%20Evidence-based%20Uncertainty%20Quantification&entry.906535625=Jose-Luis%20Matez-Bandera%20and%20Pepe%20Ojeda%20and%20Javier%20Monroy%20and%20Javier%20Gonzalez-Jimenez%20and%20Jose-Raul%20Ruiz-Sarmiento&entry.1292438233=%20%20Robots%20in%20human-centered%20environments%20require%20accurate%20scene%20understanding%20to%0Aperform%20high-level%20tasks%20effectively.%20This%20understanding%20can%20be%20achieved%0Athrough%20instance-aware%20semantic%20mapping%2C%20which%20involves%20reconstructing%20elements%0Aat%20the%20level%20of%20individual%20instances.%20Neural%20networks%2C%20the%20de%20facto%20solution%0Afor%20scene%20understanding%2C%20still%20face%20limitations%20such%20as%20overconfident%20incorrect%0Apredictions%20with%20out-of-distribution%20objects%20or%20generating%20inaccurate%0Amasks.Placing%20excessive%20reliance%20on%20these%20predictions%20makes%20the%20reconstruction%0Asusceptible%20to%20errors%2C%20reducing%20the%20robustness%20of%20the%20resulting%20maps%20and%0Ahampering%20robot%20operation.%20In%20this%20work%2C%20we%20propose%20Voxeland%2C%20a%20probabilistic%0Aframework%20for%20incrementally%20building%20instance-aware%20semantic%20maps.%20Inspired%20by%0Athe%20Theory%20of%20Evidence%2C%20Voxeland%20treats%20neural%20network%20predictions%20as%0Asubjective%20opinions%20regarding%20map%20instances%20at%20both%20geometric%20and%20semantic%0Alevels.%20These%20opinions%20are%20aggregated%20over%20time%20to%20form%20evidences%2C%20which%20are%0Aformalized%20through%20a%20probabilistic%20model.%20This%20enables%20us%20to%20quantify%0Auncertainty%20in%20the%20reconstruction%20process%2C%20facilitating%20the%20identification%20of%0Amap%20areas%20requiring%20improvement%20%28e.g.%20reobservation%20or%20reclassification%29.%20As%0Aone%20strategy%20to%20exploit%20this%2C%20we%20incorporate%20a%20Large%20Vision-Language%20Model%0A%28LVLM%29%20to%20perform%20semantic%20level%20disambiguation%20for%20instances%20with%20high%0Auncertainty.%20Results%20from%20the%20standard%20benchmarking%20on%20the%20publicly%20available%0ASceneNN%20dataset%20demonstrate%20that%20Voxeland%20outperforms%20state-of-the-art%20methods%2C%0Ahighlighting%20the%20benefits%20of%20incorporating%20and%20leveraging%20both%20instance-%20and%0Asemantic-level%20uncertainties%20to%20enhance%20reconstruction%20robustness.%20This%20is%0Afurther%20validated%20through%20qualitative%20experiments%20conducted%20on%20the%20real-world%0AScanNet%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08727v1&entry.124074799=Read"},
{"title": "Are Large Language Models Table-based Fact-Checkers?", "author": "Hanwen Zhang and Qingyi Si and Peng Fu and Zheng Lin and Weiping Wang", "abstract": "  Table-based Fact Verification (TFV) aims to extract the entailment relation\nbetween statements and structured tables. Existing TFV methods based on\nsmall-scaled models suffer from insufficient labeled data and weak zero-shot\nability. Recently, the appearance of Large Language Models (LLMs) has gained\nlots of attraction in research fields. They have shown powerful zero-shot and\nin-context learning abilities on several NLP tasks, but their potential on TFV\nis still unknown. In this work, we implement a preliminary study about whether\nLLMs are table-based fact-checkers. In detail, we design diverse prompts to\nexplore how the in-context learning can help LLMs in TFV, i.e., zero-shot and\nfew-shot TFV capability. Besides, we carefully design and construct TFV\ninstructions to study the performance gain brought by the instruction tuning of\nLLMs. Experimental results demonstrate that LLMs can achieve acceptable results\non zero-shot and few-shot TFV with prompt engineering, while instruction-tuning\ncan stimulate the TFV capability significantly. We also make some valuable\nfindings about the format of zero-shot prompts and the number of in-context\nexamples. Finally, we analyze some possible directions to promote the accuracy\nof TFV via LLMs, which is beneficial to further research of table reasoning.\n", "link": "http://arxiv.org/abs/2402.02549v2", "date": "2024-11-13", "relevancy": 2.4747, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5003}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5003}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Large%20Language%20Models%20Table-based%20Fact-Checkers%3F&body=Title%3A%20Are%20Large%20Language%20Models%20Table-based%20Fact-Checkers%3F%0AAuthor%3A%20Hanwen%20Zhang%20and%20Qingyi%20Si%20and%20Peng%20Fu%20and%20Zheng%20Lin%20and%20Weiping%20Wang%0AAbstract%3A%20%20%20Table-based%20Fact%20Verification%20%28TFV%29%20aims%20to%20extract%20the%20entailment%20relation%0Abetween%20statements%20and%20structured%20tables.%20Existing%20TFV%20methods%20based%20on%0Asmall-scaled%20models%20suffer%20from%20insufficient%20labeled%20data%20and%20weak%20zero-shot%0Aability.%20Recently%2C%20the%20appearance%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20gained%0Alots%20of%20attraction%20in%20research%20fields.%20They%20have%20shown%20powerful%20zero-shot%20and%0Ain-context%20learning%20abilities%20on%20several%20NLP%20tasks%2C%20but%20their%20potential%20on%20TFV%0Ais%20still%20unknown.%20In%20this%20work%2C%20we%20implement%20a%20preliminary%20study%20about%20whether%0ALLMs%20are%20table-based%20fact-checkers.%20In%20detail%2C%20we%20design%20diverse%20prompts%20to%0Aexplore%20how%20the%20in-context%20learning%20can%20help%20LLMs%20in%20TFV%2C%20i.e.%2C%20zero-shot%20and%0Afew-shot%20TFV%20capability.%20Besides%2C%20we%20carefully%20design%20and%20construct%20TFV%0Ainstructions%20to%20study%20the%20performance%20gain%20brought%20by%20the%20instruction%20tuning%20of%0ALLMs.%20Experimental%20results%20demonstrate%20that%20LLMs%20can%20achieve%20acceptable%20results%0Aon%20zero-shot%20and%20few-shot%20TFV%20with%20prompt%20engineering%2C%20while%20instruction-tuning%0Acan%20stimulate%20the%20TFV%20capability%20significantly.%20We%20also%20make%20some%20valuable%0Afindings%20about%20the%20format%20of%20zero-shot%20prompts%20and%20the%20number%20of%20in-context%0Aexamples.%20Finally%2C%20we%20analyze%20some%20possible%20directions%20to%20promote%20the%20accuracy%0Aof%20TFV%20via%20LLMs%2C%20which%20is%20beneficial%20to%20further%20research%20of%20table%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.02549v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Large%2520Language%2520Models%2520Table-based%2520Fact-Checkers%253F%26entry.906535625%3DHanwen%2520Zhang%2520and%2520Qingyi%2520Si%2520and%2520Peng%2520Fu%2520and%2520Zheng%2520Lin%2520and%2520Weiping%2520Wang%26entry.1292438233%3D%2520%2520Table-based%2520Fact%2520Verification%2520%2528TFV%2529%2520aims%2520to%2520extract%2520the%2520entailment%2520relation%250Abetween%2520statements%2520and%2520structured%2520tables.%2520Existing%2520TFV%2520methods%2520based%2520on%250Asmall-scaled%2520models%2520suffer%2520from%2520insufficient%2520labeled%2520data%2520and%2520weak%2520zero-shot%250Aability.%2520Recently%252C%2520the%2520appearance%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520gained%250Alots%2520of%2520attraction%2520in%2520research%2520fields.%2520They%2520have%2520shown%2520powerful%2520zero-shot%2520and%250Ain-context%2520learning%2520abilities%2520on%2520several%2520NLP%2520tasks%252C%2520but%2520their%2520potential%2520on%2520TFV%250Ais%2520still%2520unknown.%2520In%2520this%2520work%252C%2520we%2520implement%2520a%2520preliminary%2520study%2520about%2520whether%250ALLMs%2520are%2520table-based%2520fact-checkers.%2520In%2520detail%252C%2520we%2520design%2520diverse%2520prompts%2520to%250Aexplore%2520how%2520the%2520in-context%2520learning%2520can%2520help%2520LLMs%2520in%2520TFV%252C%2520i.e.%252C%2520zero-shot%2520and%250Afew-shot%2520TFV%2520capability.%2520Besides%252C%2520we%2520carefully%2520design%2520and%2520construct%2520TFV%250Ainstructions%2520to%2520study%2520the%2520performance%2520gain%2520brought%2520by%2520the%2520instruction%2520tuning%2520of%250ALLMs.%2520Experimental%2520results%2520demonstrate%2520that%2520LLMs%2520can%2520achieve%2520acceptable%2520results%250Aon%2520zero-shot%2520and%2520few-shot%2520TFV%2520with%2520prompt%2520engineering%252C%2520while%2520instruction-tuning%250Acan%2520stimulate%2520the%2520TFV%2520capability%2520significantly.%2520We%2520also%2520make%2520some%2520valuable%250Afindings%2520about%2520the%2520format%2520of%2520zero-shot%2520prompts%2520and%2520the%2520number%2520of%2520in-context%250Aexamples.%2520Finally%252C%2520we%2520analyze%2520some%2520possible%2520directions%2520to%2520promote%2520the%2520accuracy%250Aof%2520TFV%2520via%2520LLMs%252C%2520which%2520is%2520beneficial%2520to%2520further%2520research%2520of%2520table%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.02549v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Large%20Language%20Models%20Table-based%20Fact-Checkers%3F&entry.906535625=Hanwen%20Zhang%20and%20Qingyi%20Si%20and%20Peng%20Fu%20and%20Zheng%20Lin%20and%20Weiping%20Wang&entry.1292438233=%20%20Table-based%20Fact%20Verification%20%28TFV%29%20aims%20to%20extract%20the%20entailment%20relation%0Abetween%20statements%20and%20structured%20tables.%20Existing%20TFV%20methods%20based%20on%0Asmall-scaled%20models%20suffer%20from%20insufficient%20labeled%20data%20and%20weak%20zero-shot%0Aability.%20Recently%2C%20the%20appearance%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20gained%0Alots%20of%20attraction%20in%20research%20fields.%20They%20have%20shown%20powerful%20zero-shot%20and%0Ain-context%20learning%20abilities%20on%20several%20NLP%20tasks%2C%20but%20their%20potential%20on%20TFV%0Ais%20still%20unknown.%20In%20this%20work%2C%20we%20implement%20a%20preliminary%20study%20about%20whether%0ALLMs%20are%20table-based%20fact-checkers.%20In%20detail%2C%20we%20design%20diverse%20prompts%20to%0Aexplore%20how%20the%20in-context%20learning%20can%20help%20LLMs%20in%20TFV%2C%20i.e.%2C%20zero-shot%20and%0Afew-shot%20TFV%20capability.%20Besides%2C%20we%20carefully%20design%20and%20construct%20TFV%0Ainstructions%20to%20study%20the%20performance%20gain%20brought%20by%20the%20instruction%20tuning%20of%0ALLMs.%20Experimental%20results%20demonstrate%20that%20LLMs%20can%20achieve%20acceptable%20results%0Aon%20zero-shot%20and%20few-shot%20TFV%20with%20prompt%20engineering%2C%20while%20instruction-tuning%0Acan%20stimulate%20the%20TFV%20capability%20significantly.%20We%20also%20make%20some%20valuable%0Afindings%20about%20the%20format%20of%20zero-shot%20prompts%20and%20the%20number%20of%20in-context%0Aexamples.%20Finally%2C%20we%20analyze%20some%20possible%20directions%20to%20promote%20the%20accuracy%0Aof%20TFV%20via%20LLMs%2C%20which%20is%20beneficial%20to%20further%20research%20of%20table%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.02549v2&entry.124074799=Read"},
{"title": "Rethinking negative sampling in content-based news recommendation", "author": "Miguel \u00c2ngelo Rebelo and Jo\u00e3o Vinagre and Ivo Pereira and \u00c1lvaro Figueira", "abstract": "  News recommender systems are hindered by the brief lifespan of articles, as\nthey undergo rapid relevance decay. Recent studies have demonstrated the\npotential of content-based neural techniques in tackling this problem. However,\nthese models often involve complex neural architectures and often lack\nconsideration for negative examples. In this study, we posit that the careful\nsampling of negative examples has a big impact on the model's outcome. We\ndevise a negative sampling technique that not only improves the accuracy of the\nmodel but also facilitates the decentralization of the recommendation system.\nThe experimental results obtained using the MIND dataset demonstrate that the\naccuracy of the method under consideration can compete with that of\nState-of-the-Art models. The utilization of the sampling technique is essential\nin reducing model complexity and accelerating the training process, while\nmaintaining a high level of accuracy. Finally, we discuss how decentralized\nmodels can help improve privacy and scalability.\n", "link": "http://arxiv.org/abs/2411.08700v1", "date": "2024-11-13", "relevancy": 2.4687, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5112}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.488}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4819}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20negative%20sampling%20in%20content-based%20news%20recommendation&body=Title%3A%20Rethinking%20negative%20sampling%20in%20content-based%20news%20recommendation%0AAuthor%3A%20Miguel%20%C3%82ngelo%20Rebelo%20and%20Jo%C3%A3o%20Vinagre%20and%20Ivo%20Pereira%20and%20%C3%81lvaro%20Figueira%0AAbstract%3A%20%20%20News%20recommender%20systems%20are%20hindered%20by%20the%20brief%20lifespan%20of%20articles%2C%20as%0Athey%20undergo%20rapid%20relevance%20decay.%20Recent%20studies%20have%20demonstrated%20the%0Apotential%20of%20content-based%20neural%20techniques%20in%20tackling%20this%20problem.%20However%2C%0Athese%20models%20often%20involve%20complex%20neural%20architectures%20and%20often%20lack%0Aconsideration%20for%20negative%20examples.%20In%20this%20study%2C%20we%20posit%20that%20the%20careful%0Asampling%20of%20negative%20examples%20has%20a%20big%20impact%20on%20the%20model%27s%20outcome.%20We%0Adevise%20a%20negative%20sampling%20technique%20that%20not%20only%20improves%20the%20accuracy%20of%20the%0Amodel%20but%20also%20facilitates%20the%20decentralization%20of%20the%20recommendation%20system.%0AThe%20experimental%20results%20obtained%20using%20the%20MIND%20dataset%20demonstrate%20that%20the%0Aaccuracy%20of%20the%20method%20under%20consideration%20can%20compete%20with%20that%20of%0AState-of-the-Art%20models.%20The%20utilization%20of%20the%20sampling%20technique%20is%20essential%0Ain%20reducing%20model%20complexity%20and%20accelerating%20the%20training%20process%2C%20while%0Amaintaining%20a%20high%20level%20of%20accuracy.%20Finally%2C%20we%20discuss%20how%20decentralized%0Amodels%20can%20help%20improve%20privacy%20and%20scalability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520negative%2520sampling%2520in%2520content-based%2520news%2520recommendation%26entry.906535625%3DMiguel%2520%25C3%2582ngelo%2520Rebelo%2520and%2520Jo%25C3%25A3o%2520Vinagre%2520and%2520Ivo%2520Pereira%2520and%2520%25C3%2581lvaro%2520Figueira%26entry.1292438233%3D%2520%2520News%2520recommender%2520systems%2520are%2520hindered%2520by%2520the%2520brief%2520lifespan%2520of%2520articles%252C%2520as%250Athey%2520undergo%2520rapid%2520relevance%2520decay.%2520Recent%2520studies%2520have%2520demonstrated%2520the%250Apotential%2520of%2520content-based%2520neural%2520techniques%2520in%2520tackling%2520this%2520problem.%2520However%252C%250Athese%2520models%2520often%2520involve%2520complex%2520neural%2520architectures%2520and%2520often%2520lack%250Aconsideration%2520for%2520negative%2520examples.%2520In%2520this%2520study%252C%2520we%2520posit%2520that%2520the%2520careful%250Asampling%2520of%2520negative%2520examples%2520has%2520a%2520big%2520impact%2520on%2520the%2520model%2527s%2520outcome.%2520We%250Adevise%2520a%2520negative%2520sampling%2520technique%2520that%2520not%2520only%2520improves%2520the%2520accuracy%2520of%2520the%250Amodel%2520but%2520also%2520facilitates%2520the%2520decentralization%2520of%2520the%2520recommendation%2520system.%250AThe%2520experimental%2520results%2520obtained%2520using%2520the%2520MIND%2520dataset%2520demonstrate%2520that%2520the%250Aaccuracy%2520of%2520the%2520method%2520under%2520consideration%2520can%2520compete%2520with%2520that%2520of%250AState-of-the-Art%2520models.%2520The%2520utilization%2520of%2520the%2520sampling%2520technique%2520is%2520essential%250Ain%2520reducing%2520model%2520complexity%2520and%2520accelerating%2520the%2520training%2520process%252C%2520while%250Amaintaining%2520a%2520high%2520level%2520of%2520accuracy.%2520Finally%252C%2520we%2520discuss%2520how%2520decentralized%250Amodels%2520can%2520help%2520improve%2520privacy%2520and%2520scalability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20negative%20sampling%20in%20content-based%20news%20recommendation&entry.906535625=Miguel%20%C3%82ngelo%20Rebelo%20and%20Jo%C3%A3o%20Vinagre%20and%20Ivo%20Pereira%20and%20%C3%81lvaro%20Figueira&entry.1292438233=%20%20News%20recommender%20systems%20are%20hindered%20by%20the%20brief%20lifespan%20of%20articles%2C%20as%0Athey%20undergo%20rapid%20relevance%20decay.%20Recent%20studies%20have%20demonstrated%20the%0Apotential%20of%20content-based%20neural%20techniques%20in%20tackling%20this%20problem.%20However%2C%0Athese%20models%20often%20involve%20complex%20neural%20architectures%20and%20often%20lack%0Aconsideration%20for%20negative%20examples.%20In%20this%20study%2C%20we%20posit%20that%20the%20careful%0Asampling%20of%20negative%20examples%20has%20a%20big%20impact%20on%20the%20model%27s%20outcome.%20We%0Adevise%20a%20negative%20sampling%20technique%20that%20not%20only%20improves%20the%20accuracy%20of%20the%0Amodel%20but%20also%20facilitates%20the%20decentralization%20of%20the%20recommendation%20system.%0AThe%20experimental%20results%20obtained%20using%20the%20MIND%20dataset%20demonstrate%20that%20the%0Aaccuracy%20of%20the%20method%20under%20consideration%20can%20compete%20with%20that%20of%0AState-of-the-Art%20models.%20The%20utilization%20of%20the%20sampling%20technique%20is%20essential%0Ain%20reducing%20model%20complexity%20and%20accelerating%20the%20training%20process%2C%20while%0Amaintaining%20a%20high%20level%20of%20accuracy.%20Finally%2C%20we%20discuss%20how%20decentralized%0Amodels%20can%20help%20improve%20privacy%20and%20scalability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08700v1&entry.124074799=Read"},
{"title": "A Universal Deep Learning Framework for Materials X-ray Absorption\n  Spectra", "author": "Shubha R. Kharel and Fanchen Meng and Xiaohui Qu and Matthew R. Carbone and Deyu Lu", "abstract": "  X-ray absorption spectroscopy (XAS) is a powerful characterization technique\nfor probing the local chemical environment of absorbing atoms. However,\nanalyzing XAS data presents significant challenges, often requiring extensive,\ncomputationally intensive simulations, as well as significant domain expertise.\nThese limitations hinder the development of fast, robust XAS analysis pipelines\nthat are essential in high-throughput studies and for autonomous\nexperimentation. We address these challenges with OmniXAS, a framework that\ncontains a suite of transfer learning approaches for XAS prediction, each\ncontributing to improved accuracy and efficiency, as demonstrated on K-edge\nspectra database covering eight 3d transition metals (Ti-Cu). The OmniXAS\nframework is built upon three distinct strategies. First, we use M3GNet to\nderive latent representations of the local chemical environment of absorption\nsites as input for XAS prediction, achieving up to order-of-magnitude\nimprovements over conventional featurization techniques. Second, we employ a\nhierarchical transfer learning strategy, training a universal multi-task model\nacross elements before fine-tuning for element-specific predictions. Models\nbased on this cascaded approach after element-wise fine-tuning outperform\nelement-specific models by up to 69%. Third, we implement cross-fidelity\ntransfer learning, adapting a universal model to predict spectra generated by\nsimulation of a different fidelity with a higher computational cost. This\napproach improves prediction accuracy by up to 11% over models trained on the\ntarget fidelity alone. Our approach boosts the throughput of XAS modeling by\norders of magnitude versus first-principles simulations and is extendable to\nXAS prediction for a broader range of elements. This transfer learning\nframework is generalizable to enhance deep-learning models that target other\nproperties in materials research.\n", "link": "http://arxiv.org/abs/2409.19552v2", "date": "2024-11-13", "relevancy": 2.4235, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4859}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4859}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Universal%20Deep%20Learning%20Framework%20for%20Materials%20X-ray%20Absorption%0A%20%20Spectra&body=Title%3A%20A%20Universal%20Deep%20Learning%20Framework%20for%20Materials%20X-ray%20Absorption%0A%20%20Spectra%0AAuthor%3A%20Shubha%20R.%20Kharel%20and%20Fanchen%20Meng%20and%20Xiaohui%20Qu%20and%20Matthew%20R.%20Carbone%20and%20Deyu%20Lu%0AAbstract%3A%20%20%20X-ray%20absorption%20spectroscopy%20%28XAS%29%20is%20a%20powerful%20characterization%20technique%0Afor%20probing%20the%20local%20chemical%20environment%20of%20absorbing%20atoms.%20However%2C%0Aanalyzing%20XAS%20data%20presents%20significant%20challenges%2C%20often%20requiring%20extensive%2C%0Acomputationally%20intensive%20simulations%2C%20as%20well%20as%20significant%20domain%20expertise.%0AThese%20limitations%20hinder%20the%20development%20of%20fast%2C%20robust%20XAS%20analysis%20pipelines%0Athat%20are%20essential%20in%20high-throughput%20studies%20and%20for%20autonomous%0Aexperimentation.%20We%20address%20these%20challenges%20with%20OmniXAS%2C%20a%20framework%20that%0Acontains%20a%20suite%20of%20transfer%20learning%20approaches%20for%20XAS%20prediction%2C%20each%0Acontributing%20to%20improved%20accuracy%20and%20efficiency%2C%20as%20demonstrated%20on%20K-edge%0Aspectra%20database%20covering%20eight%203d%20transition%20metals%20%28Ti-Cu%29.%20The%20OmniXAS%0Aframework%20is%20built%20upon%20three%20distinct%20strategies.%20First%2C%20we%20use%20M3GNet%20to%0Aderive%20latent%20representations%20of%20the%20local%20chemical%20environment%20of%20absorption%0Asites%20as%20input%20for%20XAS%20prediction%2C%20achieving%20up%20to%20order-of-magnitude%0Aimprovements%20over%20conventional%20featurization%20techniques.%20Second%2C%20we%20employ%20a%0Ahierarchical%20transfer%20learning%20strategy%2C%20training%20a%20universal%20multi-task%20model%0Aacross%20elements%20before%20fine-tuning%20for%20element-specific%20predictions.%20Models%0Abased%20on%20this%20cascaded%20approach%20after%20element-wise%20fine-tuning%20outperform%0Aelement-specific%20models%20by%20up%20to%2069%25.%20Third%2C%20we%20implement%20cross-fidelity%0Atransfer%20learning%2C%20adapting%20a%20universal%20model%20to%20predict%20spectra%20generated%20by%0Asimulation%20of%20a%20different%20fidelity%20with%20a%20higher%20computational%20cost.%20This%0Aapproach%20improves%20prediction%20accuracy%20by%20up%20to%2011%25%20over%20models%20trained%20on%20the%0Atarget%20fidelity%20alone.%20Our%20approach%20boosts%20the%20throughput%20of%20XAS%20modeling%20by%0Aorders%20of%20magnitude%20versus%20first-principles%20simulations%20and%20is%20extendable%20to%0AXAS%20prediction%20for%20a%20broader%20range%20of%20elements.%20This%20transfer%20learning%0Aframework%20is%20generalizable%20to%20enhance%20deep-learning%20models%20that%20target%20other%0Aproperties%20in%20materials%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.19552v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Universal%2520Deep%2520Learning%2520Framework%2520for%2520Materials%2520X-ray%2520Absorption%250A%2520%2520Spectra%26entry.906535625%3DShubha%2520R.%2520Kharel%2520and%2520Fanchen%2520Meng%2520and%2520Xiaohui%2520Qu%2520and%2520Matthew%2520R.%2520Carbone%2520and%2520Deyu%2520Lu%26entry.1292438233%3D%2520%2520X-ray%2520absorption%2520spectroscopy%2520%2528XAS%2529%2520is%2520a%2520powerful%2520characterization%2520technique%250Afor%2520probing%2520the%2520local%2520chemical%2520environment%2520of%2520absorbing%2520atoms.%2520However%252C%250Aanalyzing%2520XAS%2520data%2520presents%2520significant%2520challenges%252C%2520often%2520requiring%2520extensive%252C%250Acomputationally%2520intensive%2520simulations%252C%2520as%2520well%2520as%2520significant%2520domain%2520expertise.%250AThese%2520limitations%2520hinder%2520the%2520development%2520of%2520fast%252C%2520robust%2520XAS%2520analysis%2520pipelines%250Athat%2520are%2520essential%2520in%2520high-throughput%2520studies%2520and%2520for%2520autonomous%250Aexperimentation.%2520We%2520address%2520these%2520challenges%2520with%2520OmniXAS%252C%2520a%2520framework%2520that%250Acontains%2520a%2520suite%2520of%2520transfer%2520learning%2520approaches%2520for%2520XAS%2520prediction%252C%2520each%250Acontributing%2520to%2520improved%2520accuracy%2520and%2520efficiency%252C%2520as%2520demonstrated%2520on%2520K-edge%250Aspectra%2520database%2520covering%2520eight%25203d%2520transition%2520metals%2520%2528Ti-Cu%2529.%2520The%2520OmniXAS%250Aframework%2520is%2520built%2520upon%2520three%2520distinct%2520strategies.%2520First%252C%2520we%2520use%2520M3GNet%2520to%250Aderive%2520latent%2520representations%2520of%2520the%2520local%2520chemical%2520environment%2520of%2520absorption%250Asites%2520as%2520input%2520for%2520XAS%2520prediction%252C%2520achieving%2520up%2520to%2520order-of-magnitude%250Aimprovements%2520over%2520conventional%2520featurization%2520techniques.%2520Second%252C%2520we%2520employ%2520a%250Ahierarchical%2520transfer%2520learning%2520strategy%252C%2520training%2520a%2520universal%2520multi-task%2520model%250Aacross%2520elements%2520before%2520fine-tuning%2520for%2520element-specific%2520predictions.%2520Models%250Abased%2520on%2520this%2520cascaded%2520approach%2520after%2520element-wise%2520fine-tuning%2520outperform%250Aelement-specific%2520models%2520by%2520up%2520to%252069%2525.%2520Third%252C%2520we%2520implement%2520cross-fidelity%250Atransfer%2520learning%252C%2520adapting%2520a%2520universal%2520model%2520to%2520predict%2520spectra%2520generated%2520by%250Asimulation%2520of%2520a%2520different%2520fidelity%2520with%2520a%2520higher%2520computational%2520cost.%2520This%250Aapproach%2520improves%2520prediction%2520accuracy%2520by%2520up%2520to%252011%2525%2520over%2520models%2520trained%2520on%2520the%250Atarget%2520fidelity%2520alone.%2520Our%2520approach%2520boosts%2520the%2520throughput%2520of%2520XAS%2520modeling%2520by%250Aorders%2520of%2520magnitude%2520versus%2520first-principles%2520simulations%2520and%2520is%2520extendable%2520to%250AXAS%2520prediction%2520for%2520a%2520broader%2520range%2520of%2520elements.%2520This%2520transfer%2520learning%250Aframework%2520is%2520generalizable%2520to%2520enhance%2520deep-learning%2520models%2520that%2520target%2520other%250Aproperties%2520in%2520materials%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.19552v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Universal%20Deep%20Learning%20Framework%20for%20Materials%20X-ray%20Absorption%0A%20%20Spectra&entry.906535625=Shubha%20R.%20Kharel%20and%20Fanchen%20Meng%20and%20Xiaohui%20Qu%20and%20Matthew%20R.%20Carbone%20and%20Deyu%20Lu&entry.1292438233=%20%20X-ray%20absorption%20spectroscopy%20%28XAS%29%20is%20a%20powerful%20characterization%20technique%0Afor%20probing%20the%20local%20chemical%20environment%20of%20absorbing%20atoms.%20However%2C%0Aanalyzing%20XAS%20data%20presents%20significant%20challenges%2C%20often%20requiring%20extensive%2C%0Acomputationally%20intensive%20simulations%2C%20as%20well%20as%20significant%20domain%20expertise.%0AThese%20limitations%20hinder%20the%20development%20of%20fast%2C%20robust%20XAS%20analysis%20pipelines%0Athat%20are%20essential%20in%20high-throughput%20studies%20and%20for%20autonomous%0Aexperimentation.%20We%20address%20these%20challenges%20with%20OmniXAS%2C%20a%20framework%20that%0Acontains%20a%20suite%20of%20transfer%20learning%20approaches%20for%20XAS%20prediction%2C%20each%0Acontributing%20to%20improved%20accuracy%20and%20efficiency%2C%20as%20demonstrated%20on%20K-edge%0Aspectra%20database%20covering%20eight%203d%20transition%20metals%20%28Ti-Cu%29.%20The%20OmniXAS%0Aframework%20is%20built%20upon%20three%20distinct%20strategies.%20First%2C%20we%20use%20M3GNet%20to%0Aderive%20latent%20representations%20of%20the%20local%20chemical%20environment%20of%20absorption%0Asites%20as%20input%20for%20XAS%20prediction%2C%20achieving%20up%20to%20order-of-magnitude%0Aimprovements%20over%20conventional%20featurization%20techniques.%20Second%2C%20we%20employ%20a%0Ahierarchical%20transfer%20learning%20strategy%2C%20training%20a%20universal%20multi-task%20model%0Aacross%20elements%20before%20fine-tuning%20for%20element-specific%20predictions.%20Models%0Abased%20on%20this%20cascaded%20approach%20after%20element-wise%20fine-tuning%20outperform%0Aelement-specific%20models%20by%20up%20to%2069%25.%20Third%2C%20we%20implement%20cross-fidelity%0Atransfer%20learning%2C%20adapting%20a%20universal%20model%20to%20predict%20spectra%20generated%20by%0Asimulation%20of%20a%20different%20fidelity%20with%20a%20higher%20computational%20cost.%20This%0Aapproach%20improves%20prediction%20accuracy%20by%20up%20to%2011%25%20over%20models%20trained%20on%20the%0Atarget%20fidelity%20alone.%20Our%20approach%20boosts%20the%20throughput%20of%20XAS%20modeling%20by%0Aorders%20of%20magnitude%20versus%20first-principles%20simulations%20and%20is%20extendable%20to%0AXAS%20prediction%20for%20a%20broader%20range%20of%20elements.%20This%20transfer%20learning%0Aframework%20is%20generalizable%20to%20enhance%20deep-learning%20models%20that%20target%20other%0Aproperties%20in%20materials%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.19552v2&entry.124074799=Read"},
{"title": "NavAgent: Multi-scale Urban Street View Fusion For UAV Embodied\n  Vision-and-Language Navigation", "author": "Youzhi Liu and Fanglong Yao and Yuanchang Yue and Guangluan Xu and Xian Sun and Kun Fu", "abstract": "  Vision-and-Language Navigation (VLN), as a widely discussed research\ndirection in embodied intelligence, aims to enable embodied agents to navigate\nin complicated visual environments through natural language commands. Most\nexisting VLN methods focus on indoor ground robot scenarios. However, when\napplied to UAV VLN in outdoor urban scenes, it faces two significant\nchallenges. First, urban scenes contain numerous objects, which makes it\nchallenging to match fine-grained landmarks in images with complex textual\ndescriptions of these landmarks. Second, overall environmental information\nencompasses multiple modal dimensions, and the diversity of representations\nsignificantly increases the complexity of the encoding process. To address\nthese challenges, we propose NavAgent, the first urban UAV embodied navigation\nmodel driven by a large Vision-Language Model. NavAgent undertakes navigation\ntasks by synthesizing multi-scale environmental information, including\ntopological maps (global), panoramas (medium), and fine-grained landmarks\n(local). Specifically, we utilize GLIP to build a visual recognizer for\nlandmark capable of identifying and linguisticizing fine-grained landmarks.\nSubsequently, we develop dynamically growing scene topology map that integrate\nenvironmental information and employ Graph Convolutional Networks to encode\nglobal environmental data. In addition, to train the visual recognizer for\nlandmark, we develop NavAgent-Landmark2K, the first fine-grained landmark\ndataset for real urban street scenes. In experiments conducted on the Touchdown\nand Map2seq datasets, NavAgent outperforms strong baseline models. The code and\ndataset will be released to the community to facilitate the exploration and\ndevelopment of outdoor VLN.\n", "link": "http://arxiv.org/abs/2411.08579v1", "date": "2024-11-13", "relevancy": 2.4087, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6375}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.577}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NavAgent%3A%20Multi-scale%20Urban%20Street%20View%20Fusion%20For%20UAV%20Embodied%0A%20%20Vision-and-Language%20Navigation&body=Title%3A%20NavAgent%3A%20Multi-scale%20Urban%20Street%20View%20Fusion%20For%20UAV%20Embodied%0A%20%20Vision-and-Language%20Navigation%0AAuthor%3A%20Youzhi%20Liu%20and%20Fanglong%20Yao%20and%20Yuanchang%20Yue%20and%20Guangluan%20Xu%20and%20Xian%20Sun%20and%20Kun%20Fu%0AAbstract%3A%20%20%20Vision-and-Language%20Navigation%20%28VLN%29%2C%20as%20a%20widely%20discussed%20research%0Adirection%20in%20embodied%20intelligence%2C%20aims%20to%20enable%20embodied%20agents%20to%20navigate%0Ain%20complicated%20visual%20environments%20through%20natural%20language%20commands.%20Most%0Aexisting%20VLN%20methods%20focus%20on%20indoor%20ground%20robot%20scenarios.%20However%2C%20when%0Aapplied%20to%20UAV%20VLN%20in%20outdoor%20urban%20scenes%2C%20it%20faces%20two%20significant%0Achallenges.%20First%2C%20urban%20scenes%20contain%20numerous%20objects%2C%20which%20makes%20it%0Achallenging%20to%20match%20fine-grained%20landmarks%20in%20images%20with%20complex%20textual%0Adescriptions%20of%20these%20landmarks.%20Second%2C%20overall%20environmental%20information%0Aencompasses%20multiple%20modal%20dimensions%2C%20and%20the%20diversity%20of%20representations%0Asignificantly%20increases%20the%20complexity%20of%20the%20encoding%20process.%20To%20address%0Athese%20challenges%2C%20we%20propose%20NavAgent%2C%20the%20first%20urban%20UAV%20embodied%20navigation%0Amodel%20driven%20by%20a%20large%20Vision-Language%20Model.%20NavAgent%20undertakes%20navigation%0Atasks%20by%20synthesizing%20multi-scale%20environmental%20information%2C%20including%0Atopological%20maps%20%28global%29%2C%20panoramas%20%28medium%29%2C%20and%20fine-grained%20landmarks%0A%28local%29.%20Specifically%2C%20we%20utilize%20GLIP%20to%20build%20a%20visual%20recognizer%20for%0Alandmark%20capable%20of%20identifying%20and%20linguisticizing%20fine-grained%20landmarks.%0ASubsequently%2C%20we%20develop%20dynamically%20growing%20scene%20topology%20map%20that%20integrate%0Aenvironmental%20information%20and%20employ%20Graph%20Convolutional%20Networks%20to%20encode%0Aglobal%20environmental%20data.%20In%20addition%2C%20to%20train%20the%20visual%20recognizer%20for%0Alandmark%2C%20we%20develop%20NavAgent-Landmark2K%2C%20the%20first%20fine-grained%20landmark%0Adataset%20for%20real%20urban%20street%20scenes.%20In%20experiments%20conducted%20on%20the%20Touchdown%0Aand%20Map2seq%20datasets%2C%20NavAgent%20outperforms%20strong%20baseline%20models.%20The%20code%20and%0Adataset%20will%20be%20released%20to%20the%20community%20to%20facilitate%20the%20exploration%20and%0Adevelopment%20of%20outdoor%20VLN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08579v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNavAgent%253A%2520Multi-scale%2520Urban%2520Street%2520View%2520Fusion%2520For%2520UAV%2520Embodied%250A%2520%2520Vision-and-Language%2520Navigation%26entry.906535625%3DYouzhi%2520Liu%2520and%2520Fanglong%2520Yao%2520and%2520Yuanchang%2520Yue%2520and%2520Guangluan%2520Xu%2520and%2520Xian%2520Sun%2520and%2520Kun%2520Fu%26entry.1292438233%3D%2520%2520Vision-and-Language%2520Navigation%2520%2528VLN%2529%252C%2520as%2520a%2520widely%2520discussed%2520research%250Adirection%2520in%2520embodied%2520intelligence%252C%2520aims%2520to%2520enable%2520embodied%2520agents%2520to%2520navigate%250Ain%2520complicated%2520visual%2520environments%2520through%2520natural%2520language%2520commands.%2520Most%250Aexisting%2520VLN%2520methods%2520focus%2520on%2520indoor%2520ground%2520robot%2520scenarios.%2520However%252C%2520when%250Aapplied%2520to%2520UAV%2520VLN%2520in%2520outdoor%2520urban%2520scenes%252C%2520it%2520faces%2520two%2520significant%250Achallenges.%2520First%252C%2520urban%2520scenes%2520contain%2520numerous%2520objects%252C%2520which%2520makes%2520it%250Achallenging%2520to%2520match%2520fine-grained%2520landmarks%2520in%2520images%2520with%2520complex%2520textual%250Adescriptions%2520of%2520these%2520landmarks.%2520Second%252C%2520overall%2520environmental%2520information%250Aencompasses%2520multiple%2520modal%2520dimensions%252C%2520and%2520the%2520diversity%2520of%2520representations%250Asignificantly%2520increases%2520the%2520complexity%2520of%2520the%2520encoding%2520process.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520propose%2520NavAgent%252C%2520the%2520first%2520urban%2520UAV%2520embodied%2520navigation%250Amodel%2520driven%2520by%2520a%2520large%2520Vision-Language%2520Model.%2520NavAgent%2520undertakes%2520navigation%250Atasks%2520by%2520synthesizing%2520multi-scale%2520environmental%2520information%252C%2520including%250Atopological%2520maps%2520%2528global%2529%252C%2520panoramas%2520%2528medium%2529%252C%2520and%2520fine-grained%2520landmarks%250A%2528local%2529.%2520Specifically%252C%2520we%2520utilize%2520GLIP%2520to%2520build%2520a%2520visual%2520recognizer%2520for%250Alandmark%2520capable%2520of%2520identifying%2520and%2520linguisticizing%2520fine-grained%2520landmarks.%250ASubsequently%252C%2520we%2520develop%2520dynamically%2520growing%2520scene%2520topology%2520map%2520that%2520integrate%250Aenvironmental%2520information%2520and%2520employ%2520Graph%2520Convolutional%2520Networks%2520to%2520encode%250Aglobal%2520environmental%2520data.%2520In%2520addition%252C%2520to%2520train%2520the%2520visual%2520recognizer%2520for%250Alandmark%252C%2520we%2520develop%2520NavAgent-Landmark2K%252C%2520the%2520first%2520fine-grained%2520landmark%250Adataset%2520for%2520real%2520urban%2520street%2520scenes.%2520In%2520experiments%2520conducted%2520on%2520the%2520Touchdown%250Aand%2520Map2seq%2520datasets%252C%2520NavAgent%2520outperforms%2520strong%2520baseline%2520models.%2520The%2520code%2520and%250Adataset%2520will%2520be%2520released%2520to%2520the%2520community%2520to%2520facilitate%2520the%2520exploration%2520and%250Adevelopment%2520of%2520outdoor%2520VLN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08579v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NavAgent%3A%20Multi-scale%20Urban%20Street%20View%20Fusion%20For%20UAV%20Embodied%0A%20%20Vision-and-Language%20Navigation&entry.906535625=Youzhi%20Liu%20and%20Fanglong%20Yao%20and%20Yuanchang%20Yue%20and%20Guangluan%20Xu%20and%20Xian%20Sun%20and%20Kun%20Fu&entry.1292438233=%20%20Vision-and-Language%20Navigation%20%28VLN%29%2C%20as%20a%20widely%20discussed%20research%0Adirection%20in%20embodied%20intelligence%2C%20aims%20to%20enable%20embodied%20agents%20to%20navigate%0Ain%20complicated%20visual%20environments%20through%20natural%20language%20commands.%20Most%0Aexisting%20VLN%20methods%20focus%20on%20indoor%20ground%20robot%20scenarios.%20However%2C%20when%0Aapplied%20to%20UAV%20VLN%20in%20outdoor%20urban%20scenes%2C%20it%20faces%20two%20significant%0Achallenges.%20First%2C%20urban%20scenes%20contain%20numerous%20objects%2C%20which%20makes%20it%0Achallenging%20to%20match%20fine-grained%20landmarks%20in%20images%20with%20complex%20textual%0Adescriptions%20of%20these%20landmarks.%20Second%2C%20overall%20environmental%20information%0Aencompasses%20multiple%20modal%20dimensions%2C%20and%20the%20diversity%20of%20representations%0Asignificantly%20increases%20the%20complexity%20of%20the%20encoding%20process.%20To%20address%0Athese%20challenges%2C%20we%20propose%20NavAgent%2C%20the%20first%20urban%20UAV%20embodied%20navigation%0Amodel%20driven%20by%20a%20large%20Vision-Language%20Model.%20NavAgent%20undertakes%20navigation%0Atasks%20by%20synthesizing%20multi-scale%20environmental%20information%2C%20including%0Atopological%20maps%20%28global%29%2C%20panoramas%20%28medium%29%2C%20and%20fine-grained%20landmarks%0A%28local%29.%20Specifically%2C%20we%20utilize%20GLIP%20to%20build%20a%20visual%20recognizer%20for%0Alandmark%20capable%20of%20identifying%20and%20linguisticizing%20fine-grained%20landmarks.%0ASubsequently%2C%20we%20develop%20dynamically%20growing%20scene%20topology%20map%20that%20integrate%0Aenvironmental%20information%20and%20employ%20Graph%20Convolutional%20Networks%20to%20encode%0Aglobal%20environmental%20data.%20In%20addition%2C%20to%20train%20the%20visual%20recognizer%20for%0Alandmark%2C%20we%20develop%20NavAgent-Landmark2K%2C%20the%20first%20fine-grained%20landmark%0Adataset%20for%20real%20urban%20street%20scenes.%20In%20experiments%20conducted%20on%20the%20Touchdown%0Aand%20Map2seq%20datasets%2C%20NavAgent%20outperforms%20strong%20baseline%20models.%20The%20code%20and%0Adataset%20will%20be%20released%20to%20the%20community%20to%20facilitate%20the%20exploration%20and%0Adevelopment%20of%20outdoor%20VLN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08579v1&entry.124074799=Read"},
{"title": "AudioProtoPNet: An interpretable deep learning model for bird sound\n  classification", "author": "Ren\u00e9 Heinrich and Lukas Rauch and Bernhard Sick and Christoph Scholz", "abstract": "  Deep learning models have significantly advanced acoustic bird monitoring by\nbeing able to recognize numerous bird species based on their vocalizations.\nHowever, traditional deep learning models are black boxes that provide no\ninsight into their underlying computations, limiting their usefulness to\nornithologists and machine learning engineers. Explainable models could\nfacilitate debugging, knowledge discovery, trust, and interdisciplinary\ncollaboration. This study introduces AudioProtoPNet, an adaptation of the\nPrototypical Part Network (ProtoPNet) for multi-label bird sound\nclassification. It is an inherently interpretable model that uses a ConvNeXt\nbackbone to extract embeddings, with the classification layer replaced by a\nprototype learning classifier trained on these embeddings. The classifier\nlearns prototypical patterns of each bird species' vocalizations from\nspectrograms of training instances. During inference, audio recordings are\nclassified by comparing them to the learned prototypes in the embedding space,\nproviding explanations for the model's decisions and insights into the most\ninformative embeddings of each bird species. The model was trained on the\nBirdSet training dataset, which consists of 9,734 bird species and over 6,800\nhours of recordings. Its performance was evaluated on the seven test datasets\nof BirdSet, covering different geographical regions. AudioProtoPNet\noutperformed the state-of-the-art model Perch, achieving an average AUROC of\n0.90 and a cmAP of 0.42, with relative improvements of 7.1% and 16.7% over\nPerch, respectively. These results demonstrate that even for the challenging\ntask of multi-label bird sound classification, it is possible to develop\npowerful yet inherently interpretable deep learning models that provide\nvaluable insights for ornithologists and machine learning engineers.\n", "link": "http://arxiv.org/abs/2404.10420v3", "date": "2024-11-13", "relevancy": 2.3833, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4846}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4846}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AudioProtoPNet%3A%20An%20interpretable%20deep%20learning%20model%20for%20bird%20sound%0A%20%20classification&body=Title%3A%20AudioProtoPNet%3A%20An%20interpretable%20deep%20learning%20model%20for%20bird%20sound%0A%20%20classification%0AAuthor%3A%20Ren%C3%A9%20Heinrich%20and%20Lukas%20Rauch%20and%20Bernhard%20Sick%20and%20Christoph%20Scholz%0AAbstract%3A%20%20%20Deep%20learning%20models%20have%20significantly%20advanced%20acoustic%20bird%20monitoring%20by%0Abeing%20able%20to%20recognize%20numerous%20bird%20species%20based%20on%20their%20vocalizations.%0AHowever%2C%20traditional%20deep%20learning%20models%20are%20black%20boxes%20that%20provide%20no%0Ainsight%20into%20their%20underlying%20computations%2C%20limiting%20their%20usefulness%20to%0Aornithologists%20and%20machine%20learning%20engineers.%20Explainable%20models%20could%0Afacilitate%20debugging%2C%20knowledge%20discovery%2C%20trust%2C%20and%20interdisciplinary%0Acollaboration.%20This%20study%20introduces%20AudioProtoPNet%2C%20an%20adaptation%20of%20the%0APrototypical%20Part%20Network%20%28ProtoPNet%29%20for%20multi-label%20bird%20sound%0Aclassification.%20It%20is%20an%20inherently%20interpretable%20model%20that%20uses%20a%20ConvNeXt%0Abackbone%20to%20extract%20embeddings%2C%20with%20the%20classification%20layer%20replaced%20by%20a%0Aprototype%20learning%20classifier%20trained%20on%20these%20embeddings.%20The%20classifier%0Alearns%20prototypical%20patterns%20of%20each%20bird%20species%27%20vocalizations%20from%0Aspectrograms%20of%20training%20instances.%20During%20inference%2C%20audio%20recordings%20are%0Aclassified%20by%20comparing%20them%20to%20the%20learned%20prototypes%20in%20the%20embedding%20space%2C%0Aproviding%20explanations%20for%20the%20model%27s%20decisions%20and%20insights%20into%20the%20most%0Ainformative%20embeddings%20of%20each%20bird%20species.%20The%20model%20was%20trained%20on%20the%0ABirdSet%20training%20dataset%2C%20which%20consists%20of%209%2C734%20bird%20species%20and%20over%206%2C800%0Ahours%20of%20recordings.%20Its%20performance%20was%20evaluated%20on%20the%20seven%20test%20datasets%0Aof%20BirdSet%2C%20covering%20different%20geographical%20regions.%20AudioProtoPNet%0Aoutperformed%20the%20state-of-the-art%20model%20Perch%2C%20achieving%20an%20average%20AUROC%20of%0A0.90%20and%20a%20cmAP%20of%200.42%2C%20with%20relative%20improvements%20of%207.1%25%20and%2016.7%25%20over%0APerch%2C%20respectively.%20These%20results%20demonstrate%20that%20even%20for%20the%20challenging%0Atask%20of%20multi-label%20bird%20sound%20classification%2C%20it%20is%20possible%20to%20develop%0Apowerful%20yet%20inherently%20interpretable%20deep%20learning%20models%20that%20provide%0Avaluable%20insights%20for%20ornithologists%20and%20machine%20learning%20engineers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.10420v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudioProtoPNet%253A%2520An%2520interpretable%2520deep%2520learning%2520model%2520for%2520bird%2520sound%250A%2520%2520classification%26entry.906535625%3DRen%25C3%25A9%2520Heinrich%2520and%2520Lukas%2520Rauch%2520and%2520Bernhard%2520Sick%2520and%2520Christoph%2520Scholz%26entry.1292438233%3D%2520%2520Deep%2520learning%2520models%2520have%2520significantly%2520advanced%2520acoustic%2520bird%2520monitoring%2520by%250Abeing%2520able%2520to%2520recognize%2520numerous%2520bird%2520species%2520based%2520on%2520their%2520vocalizations.%250AHowever%252C%2520traditional%2520deep%2520learning%2520models%2520are%2520black%2520boxes%2520that%2520provide%2520no%250Ainsight%2520into%2520their%2520underlying%2520computations%252C%2520limiting%2520their%2520usefulness%2520to%250Aornithologists%2520and%2520machine%2520learning%2520engineers.%2520Explainable%2520models%2520could%250Afacilitate%2520debugging%252C%2520knowledge%2520discovery%252C%2520trust%252C%2520and%2520interdisciplinary%250Acollaboration.%2520This%2520study%2520introduces%2520AudioProtoPNet%252C%2520an%2520adaptation%2520of%2520the%250APrototypical%2520Part%2520Network%2520%2528ProtoPNet%2529%2520for%2520multi-label%2520bird%2520sound%250Aclassification.%2520It%2520is%2520an%2520inherently%2520interpretable%2520model%2520that%2520uses%2520a%2520ConvNeXt%250Abackbone%2520to%2520extract%2520embeddings%252C%2520with%2520the%2520classification%2520layer%2520replaced%2520by%2520a%250Aprototype%2520learning%2520classifier%2520trained%2520on%2520these%2520embeddings.%2520The%2520classifier%250Alearns%2520prototypical%2520patterns%2520of%2520each%2520bird%2520species%2527%2520vocalizations%2520from%250Aspectrograms%2520of%2520training%2520instances.%2520During%2520inference%252C%2520audio%2520recordings%2520are%250Aclassified%2520by%2520comparing%2520them%2520to%2520the%2520learned%2520prototypes%2520in%2520the%2520embedding%2520space%252C%250Aproviding%2520explanations%2520for%2520the%2520model%2527s%2520decisions%2520and%2520insights%2520into%2520the%2520most%250Ainformative%2520embeddings%2520of%2520each%2520bird%2520species.%2520The%2520model%2520was%2520trained%2520on%2520the%250ABirdSet%2520training%2520dataset%252C%2520which%2520consists%2520of%25209%252C734%2520bird%2520species%2520and%2520over%25206%252C800%250Ahours%2520of%2520recordings.%2520Its%2520performance%2520was%2520evaluated%2520on%2520the%2520seven%2520test%2520datasets%250Aof%2520BirdSet%252C%2520covering%2520different%2520geographical%2520regions.%2520AudioProtoPNet%250Aoutperformed%2520the%2520state-of-the-art%2520model%2520Perch%252C%2520achieving%2520an%2520average%2520AUROC%2520of%250A0.90%2520and%2520a%2520cmAP%2520of%25200.42%252C%2520with%2520relative%2520improvements%2520of%25207.1%2525%2520and%252016.7%2525%2520over%250APerch%252C%2520respectively.%2520These%2520results%2520demonstrate%2520that%2520even%2520for%2520the%2520challenging%250Atask%2520of%2520multi-label%2520bird%2520sound%2520classification%252C%2520it%2520is%2520possible%2520to%2520develop%250Apowerful%2520yet%2520inherently%2520interpretable%2520deep%2520learning%2520models%2520that%2520provide%250Avaluable%2520insights%2520for%2520ornithologists%2520and%2520machine%2520learning%2520engineers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.10420v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AudioProtoPNet%3A%20An%20interpretable%20deep%20learning%20model%20for%20bird%20sound%0A%20%20classification&entry.906535625=Ren%C3%A9%20Heinrich%20and%20Lukas%20Rauch%20and%20Bernhard%20Sick%20and%20Christoph%20Scholz&entry.1292438233=%20%20Deep%20learning%20models%20have%20significantly%20advanced%20acoustic%20bird%20monitoring%20by%0Abeing%20able%20to%20recognize%20numerous%20bird%20species%20based%20on%20their%20vocalizations.%0AHowever%2C%20traditional%20deep%20learning%20models%20are%20black%20boxes%20that%20provide%20no%0Ainsight%20into%20their%20underlying%20computations%2C%20limiting%20their%20usefulness%20to%0Aornithologists%20and%20machine%20learning%20engineers.%20Explainable%20models%20could%0Afacilitate%20debugging%2C%20knowledge%20discovery%2C%20trust%2C%20and%20interdisciplinary%0Acollaboration.%20This%20study%20introduces%20AudioProtoPNet%2C%20an%20adaptation%20of%20the%0APrototypical%20Part%20Network%20%28ProtoPNet%29%20for%20multi-label%20bird%20sound%0Aclassification.%20It%20is%20an%20inherently%20interpretable%20model%20that%20uses%20a%20ConvNeXt%0Abackbone%20to%20extract%20embeddings%2C%20with%20the%20classification%20layer%20replaced%20by%20a%0Aprototype%20learning%20classifier%20trained%20on%20these%20embeddings.%20The%20classifier%0Alearns%20prototypical%20patterns%20of%20each%20bird%20species%27%20vocalizations%20from%0Aspectrograms%20of%20training%20instances.%20During%20inference%2C%20audio%20recordings%20are%0Aclassified%20by%20comparing%20them%20to%20the%20learned%20prototypes%20in%20the%20embedding%20space%2C%0Aproviding%20explanations%20for%20the%20model%27s%20decisions%20and%20insights%20into%20the%20most%0Ainformative%20embeddings%20of%20each%20bird%20species.%20The%20model%20was%20trained%20on%20the%0ABirdSet%20training%20dataset%2C%20which%20consists%20of%209%2C734%20bird%20species%20and%20over%206%2C800%0Ahours%20of%20recordings.%20Its%20performance%20was%20evaluated%20on%20the%20seven%20test%20datasets%0Aof%20BirdSet%2C%20covering%20different%20geographical%20regions.%20AudioProtoPNet%0Aoutperformed%20the%20state-of-the-art%20model%20Perch%2C%20achieving%20an%20average%20AUROC%20of%0A0.90%20and%20a%20cmAP%20of%200.42%2C%20with%20relative%20improvements%20of%207.1%25%20and%2016.7%25%20over%0APerch%2C%20respectively.%20These%20results%20demonstrate%20that%20even%20for%20the%20challenging%0Atask%20of%20multi-label%20bird%20sound%20classification%2C%20it%20is%20possible%20to%20develop%0Apowerful%20yet%20inherently%20interpretable%20deep%20learning%20models%20that%20provide%0Avaluable%20insights%20for%20ornithologists%20and%20machine%20learning%20engineers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.10420v3&entry.124074799=Read"},
{"title": "XiYan-SQL: A Multi-Generator Ensemble Framework for Text-to-SQL", "author": "Yingqi Gao and Yifu Liu and Xiaoxia Li and Xiaorong Shi and Yin Zhu and Yiming Wang and Shiqi Li and Wei Li and Yuntao Hong and Zhiling Luo and Jinyang Gao and Liyu Mou and Yu Li", "abstract": "  To tackle the challenges of large language model performance in natural\nlanguage to SQL tasks, we introduce XiYan-SQL, an innovative framework that\nemploys a multi-generator ensemble strategy to improve candidate generation. We\nintroduce M-Schema, a semi-structured schema representation method designed to\nenhance the understanding of database structures. To enhance the quality and\ndiversity of generated candidate SQL queries, XiYan-SQL integrates the\nsignificant potential of in-context learning (ICL) with the precise control of\nsupervised fine-tuning. On one hand, we propose a series of training strategies\nto fine-tune models to generate high-quality candidates with diverse\npreferences. On the other hand, we implement the ICL approach with an example\nselection method based on named entity recognition to prevent overemphasis on\nentities. The refiner optimizes each candidate by correcting logical or\nsyntactical errors. To address the challenge of identifying the best candidate,\nwe fine-tune a selection model to distinguish nuances of candidate SQL queries.\nThe experimental results on multiple dialect datasets demonstrate the\nrobustness of XiYan-SQL in addressing challenges across different scenarios.\nOverall, our proposed XiYan-SQL achieves the state-of-the-art execution\naccuracy of 89.65% on the Spider test set, 69.86% on SQL-Eval, 41.20% on\nNL2GQL, and a competitive score of 72.23% on the Bird development benchmark.\nThe proposed framework not only enhances the quality and diversity of SQL\nqueries but also outperforms previous methods.\n", "link": "http://arxiv.org/abs/2411.08599v1", "date": "2024-11-13", "relevancy": 2.3761, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.479}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4733}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20XiYan-SQL%3A%20A%20Multi-Generator%20Ensemble%20Framework%20for%20Text-to-SQL&body=Title%3A%20XiYan-SQL%3A%20A%20Multi-Generator%20Ensemble%20Framework%20for%20Text-to-SQL%0AAuthor%3A%20Yingqi%20Gao%20and%20Yifu%20Liu%20and%20Xiaoxia%20Li%20and%20Xiaorong%20Shi%20and%20Yin%20Zhu%20and%20Yiming%20Wang%20and%20Shiqi%20Li%20and%20Wei%20Li%20and%20Yuntao%20Hong%20and%20Zhiling%20Luo%20and%20Jinyang%20Gao%20and%20Liyu%20Mou%20and%20Yu%20Li%0AAbstract%3A%20%20%20To%20tackle%20the%20challenges%20of%20large%20language%20model%20performance%20in%20natural%0Alanguage%20to%20SQL%20tasks%2C%20we%20introduce%20XiYan-SQL%2C%20an%20innovative%20framework%20that%0Aemploys%20a%20multi-generator%20ensemble%20strategy%20to%20improve%20candidate%20generation.%20We%0Aintroduce%20M-Schema%2C%20a%20semi-structured%20schema%20representation%20method%20designed%20to%0Aenhance%20the%20understanding%20of%20database%20structures.%20To%20enhance%20the%20quality%20and%0Adiversity%20of%20generated%20candidate%20SQL%20queries%2C%20XiYan-SQL%20integrates%20the%0Asignificant%20potential%20of%20in-context%20learning%20%28ICL%29%20with%20the%20precise%20control%20of%0Asupervised%20fine-tuning.%20On%20one%20hand%2C%20we%20propose%20a%20series%20of%20training%20strategies%0Ato%20fine-tune%20models%20to%20generate%20high-quality%20candidates%20with%20diverse%0Apreferences.%20On%20the%20other%20hand%2C%20we%20implement%20the%20ICL%20approach%20with%20an%20example%0Aselection%20method%20based%20on%20named%20entity%20recognition%20to%20prevent%20overemphasis%20on%0Aentities.%20The%20refiner%20optimizes%20each%20candidate%20by%20correcting%20logical%20or%0Asyntactical%20errors.%20To%20address%20the%20challenge%20of%20identifying%20the%20best%20candidate%2C%0Awe%20fine-tune%20a%20selection%20model%20to%20distinguish%20nuances%20of%20candidate%20SQL%20queries.%0AThe%20experimental%20results%20on%20multiple%20dialect%20datasets%20demonstrate%20the%0Arobustness%20of%20XiYan-SQL%20in%20addressing%20challenges%20across%20different%20scenarios.%0AOverall%2C%20our%20proposed%20XiYan-SQL%20achieves%20the%20state-of-the-art%20execution%0Aaccuracy%20of%2089.65%25%20on%20the%20Spider%20test%20set%2C%2069.86%25%20on%20SQL-Eval%2C%2041.20%25%20on%0ANL2GQL%2C%20and%20a%20competitive%20score%20of%2072.23%25%20on%20the%20Bird%20development%20benchmark.%0AThe%20proposed%20framework%20not%20only%20enhances%20the%20quality%20and%20diversity%20of%20SQL%0Aqueries%20but%20also%20outperforms%20previous%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08599v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXiYan-SQL%253A%2520A%2520Multi-Generator%2520Ensemble%2520Framework%2520for%2520Text-to-SQL%26entry.906535625%3DYingqi%2520Gao%2520and%2520Yifu%2520Liu%2520and%2520Xiaoxia%2520Li%2520and%2520Xiaorong%2520Shi%2520and%2520Yin%2520Zhu%2520and%2520Yiming%2520Wang%2520and%2520Shiqi%2520Li%2520and%2520Wei%2520Li%2520and%2520Yuntao%2520Hong%2520and%2520Zhiling%2520Luo%2520and%2520Jinyang%2520Gao%2520and%2520Liyu%2520Mou%2520and%2520Yu%2520Li%26entry.1292438233%3D%2520%2520To%2520tackle%2520the%2520challenges%2520of%2520large%2520language%2520model%2520performance%2520in%2520natural%250Alanguage%2520to%2520SQL%2520tasks%252C%2520we%2520introduce%2520XiYan-SQL%252C%2520an%2520innovative%2520framework%2520that%250Aemploys%2520a%2520multi-generator%2520ensemble%2520strategy%2520to%2520improve%2520candidate%2520generation.%2520We%250Aintroduce%2520M-Schema%252C%2520a%2520semi-structured%2520schema%2520representation%2520method%2520designed%2520to%250Aenhance%2520the%2520understanding%2520of%2520database%2520structures.%2520To%2520enhance%2520the%2520quality%2520and%250Adiversity%2520of%2520generated%2520candidate%2520SQL%2520queries%252C%2520XiYan-SQL%2520integrates%2520the%250Asignificant%2520potential%2520of%2520in-context%2520learning%2520%2528ICL%2529%2520with%2520the%2520precise%2520control%2520of%250Asupervised%2520fine-tuning.%2520On%2520one%2520hand%252C%2520we%2520propose%2520a%2520series%2520of%2520training%2520strategies%250Ato%2520fine-tune%2520models%2520to%2520generate%2520high-quality%2520candidates%2520with%2520diverse%250Apreferences.%2520On%2520the%2520other%2520hand%252C%2520we%2520implement%2520the%2520ICL%2520approach%2520with%2520an%2520example%250Aselection%2520method%2520based%2520on%2520named%2520entity%2520recognition%2520to%2520prevent%2520overemphasis%2520on%250Aentities.%2520The%2520refiner%2520optimizes%2520each%2520candidate%2520by%2520correcting%2520logical%2520or%250Asyntactical%2520errors.%2520To%2520address%2520the%2520challenge%2520of%2520identifying%2520the%2520best%2520candidate%252C%250Awe%2520fine-tune%2520a%2520selection%2520model%2520to%2520distinguish%2520nuances%2520of%2520candidate%2520SQL%2520queries.%250AThe%2520experimental%2520results%2520on%2520multiple%2520dialect%2520datasets%2520demonstrate%2520the%250Arobustness%2520of%2520XiYan-SQL%2520in%2520addressing%2520challenges%2520across%2520different%2520scenarios.%250AOverall%252C%2520our%2520proposed%2520XiYan-SQL%2520achieves%2520the%2520state-of-the-art%2520execution%250Aaccuracy%2520of%252089.65%2525%2520on%2520the%2520Spider%2520test%2520set%252C%252069.86%2525%2520on%2520SQL-Eval%252C%252041.20%2525%2520on%250ANL2GQL%252C%2520and%2520a%2520competitive%2520score%2520of%252072.23%2525%2520on%2520the%2520Bird%2520development%2520benchmark.%250AThe%2520proposed%2520framework%2520not%2520only%2520enhances%2520the%2520quality%2520and%2520diversity%2520of%2520SQL%250Aqueries%2520but%2520also%2520outperforms%2520previous%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08599v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=XiYan-SQL%3A%20A%20Multi-Generator%20Ensemble%20Framework%20for%20Text-to-SQL&entry.906535625=Yingqi%20Gao%20and%20Yifu%20Liu%20and%20Xiaoxia%20Li%20and%20Xiaorong%20Shi%20and%20Yin%20Zhu%20and%20Yiming%20Wang%20and%20Shiqi%20Li%20and%20Wei%20Li%20and%20Yuntao%20Hong%20and%20Zhiling%20Luo%20and%20Jinyang%20Gao%20and%20Liyu%20Mou%20and%20Yu%20Li&entry.1292438233=%20%20To%20tackle%20the%20challenges%20of%20large%20language%20model%20performance%20in%20natural%0Alanguage%20to%20SQL%20tasks%2C%20we%20introduce%20XiYan-SQL%2C%20an%20innovative%20framework%20that%0Aemploys%20a%20multi-generator%20ensemble%20strategy%20to%20improve%20candidate%20generation.%20We%0Aintroduce%20M-Schema%2C%20a%20semi-structured%20schema%20representation%20method%20designed%20to%0Aenhance%20the%20understanding%20of%20database%20structures.%20To%20enhance%20the%20quality%20and%0Adiversity%20of%20generated%20candidate%20SQL%20queries%2C%20XiYan-SQL%20integrates%20the%0Asignificant%20potential%20of%20in-context%20learning%20%28ICL%29%20with%20the%20precise%20control%20of%0Asupervised%20fine-tuning.%20On%20one%20hand%2C%20we%20propose%20a%20series%20of%20training%20strategies%0Ato%20fine-tune%20models%20to%20generate%20high-quality%20candidates%20with%20diverse%0Apreferences.%20On%20the%20other%20hand%2C%20we%20implement%20the%20ICL%20approach%20with%20an%20example%0Aselection%20method%20based%20on%20named%20entity%20recognition%20to%20prevent%20overemphasis%20on%0Aentities.%20The%20refiner%20optimizes%20each%20candidate%20by%20correcting%20logical%20or%0Asyntactical%20errors.%20To%20address%20the%20challenge%20of%20identifying%20the%20best%20candidate%2C%0Awe%20fine-tune%20a%20selection%20model%20to%20distinguish%20nuances%20of%20candidate%20SQL%20queries.%0AThe%20experimental%20results%20on%20multiple%20dialect%20datasets%20demonstrate%20the%0Arobustness%20of%20XiYan-SQL%20in%20addressing%20challenges%20across%20different%20scenarios.%0AOverall%2C%20our%20proposed%20XiYan-SQL%20achieves%20the%20state-of-the-art%20execution%0Aaccuracy%20of%2089.65%25%20on%20the%20Spider%20test%20set%2C%2069.86%25%20on%20SQL-Eval%2C%2041.20%25%20on%0ANL2GQL%2C%20and%20a%20competitive%20score%20of%2072.23%25%20on%20the%20Bird%20development%20benchmark.%0AThe%20proposed%20framework%20not%20only%20enhances%20the%20quality%20and%20diversity%20of%20SQL%0Aqueries%20but%20also%20outperforms%20previous%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08599v1&entry.124074799=Read"},
{"title": "Dynamic Subset Tuning: Expanding the Operational Range of\n  Parameter-Efficient Training for Large Language Models", "author": "Felix Stahlberg and Jared Lichtarge and Shankar Kumar", "abstract": "  We propose a novel parameter-efficient training (PET) method for large\nlanguage models that adapts models to downstream tasks by optimizing a small\nsubset of the existing model parameters. Unlike prior methods, this subset is\nnot fixed in location but rather which parameters are modified evolves over the\ncourse of training. This dynamic parameter selection can yield good performance\nwith many fewer parameters than extant methods. Our method enables a seamless\nscaling of the subset size across an arbitrary proportion of the total model\nsize, while popular PET approaches like prompt tuning and LoRA cover only a\nsmall part of this spectrum. We match or outperform prompt tuning and LoRA in\nmost cases on a variety of NLP tasks (MT, QA, GSM8K, SuperGLUE) for a given\nparameter budget across different model families and sizes.\n", "link": "http://arxiv.org/abs/2411.08610v1", "date": "2024-11-13", "relevancy": 2.3561, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4905}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4629}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Subset%20Tuning%3A%20Expanding%20the%20Operational%20Range%20of%0A%20%20Parameter-Efficient%20Training%20for%20Large%20Language%20Models&body=Title%3A%20Dynamic%20Subset%20Tuning%3A%20Expanding%20the%20Operational%20Range%20of%0A%20%20Parameter-Efficient%20Training%20for%20Large%20Language%20Models%0AAuthor%3A%20Felix%20Stahlberg%20and%20Jared%20Lichtarge%20and%20Shankar%20Kumar%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20parameter-efficient%20training%20%28PET%29%20method%20for%20large%0Alanguage%20models%20that%20adapts%20models%20to%20downstream%20tasks%20by%20optimizing%20a%20small%0Asubset%20of%20the%20existing%20model%20parameters.%20Unlike%20prior%20methods%2C%20this%20subset%20is%0Anot%20fixed%20in%20location%20but%20rather%20which%20parameters%20are%20modified%20evolves%20over%20the%0Acourse%20of%20training.%20This%20dynamic%20parameter%20selection%20can%20yield%20good%20performance%0Awith%20many%20fewer%20parameters%20than%20extant%20methods.%20Our%20method%20enables%20a%20seamless%0Ascaling%20of%20the%20subset%20size%20across%20an%20arbitrary%20proportion%20of%20the%20total%20model%0Asize%2C%20while%20popular%20PET%20approaches%20like%20prompt%20tuning%20and%20LoRA%20cover%20only%20a%0Asmall%20part%20of%20this%20spectrum.%20We%20match%20or%20outperform%20prompt%20tuning%20and%20LoRA%20in%0Amost%20cases%20on%20a%20variety%20of%20NLP%20tasks%20%28MT%2C%20QA%2C%20GSM8K%2C%20SuperGLUE%29%20for%20a%20given%0Aparameter%20budget%20across%20different%20model%20families%20and%20sizes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08610v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Subset%2520Tuning%253A%2520Expanding%2520the%2520Operational%2520Range%2520of%250A%2520%2520Parameter-Efficient%2520Training%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DFelix%2520Stahlberg%2520and%2520Jared%2520Lichtarge%2520and%2520Shankar%2520Kumar%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520novel%2520parameter-efficient%2520training%2520%2528PET%2529%2520method%2520for%2520large%250Alanguage%2520models%2520that%2520adapts%2520models%2520to%2520downstream%2520tasks%2520by%2520optimizing%2520a%2520small%250Asubset%2520of%2520the%2520existing%2520model%2520parameters.%2520Unlike%2520prior%2520methods%252C%2520this%2520subset%2520is%250Anot%2520fixed%2520in%2520location%2520but%2520rather%2520which%2520parameters%2520are%2520modified%2520evolves%2520over%2520the%250Acourse%2520of%2520training.%2520This%2520dynamic%2520parameter%2520selection%2520can%2520yield%2520good%2520performance%250Awith%2520many%2520fewer%2520parameters%2520than%2520extant%2520methods.%2520Our%2520method%2520enables%2520a%2520seamless%250Ascaling%2520of%2520the%2520subset%2520size%2520across%2520an%2520arbitrary%2520proportion%2520of%2520the%2520total%2520model%250Asize%252C%2520while%2520popular%2520PET%2520approaches%2520like%2520prompt%2520tuning%2520and%2520LoRA%2520cover%2520only%2520a%250Asmall%2520part%2520of%2520this%2520spectrum.%2520We%2520match%2520or%2520outperform%2520prompt%2520tuning%2520and%2520LoRA%2520in%250Amost%2520cases%2520on%2520a%2520variety%2520of%2520NLP%2520tasks%2520%2528MT%252C%2520QA%252C%2520GSM8K%252C%2520SuperGLUE%2529%2520for%2520a%2520given%250Aparameter%2520budget%2520across%2520different%2520model%2520families%2520and%2520sizes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08610v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Subset%20Tuning%3A%20Expanding%20the%20Operational%20Range%20of%0A%20%20Parameter-Efficient%20Training%20for%20Large%20Language%20Models&entry.906535625=Felix%20Stahlberg%20and%20Jared%20Lichtarge%20and%20Shankar%20Kumar&entry.1292438233=%20%20We%20propose%20a%20novel%20parameter-efficient%20training%20%28PET%29%20method%20for%20large%0Alanguage%20models%20that%20adapts%20models%20to%20downstream%20tasks%20by%20optimizing%20a%20small%0Asubset%20of%20the%20existing%20model%20parameters.%20Unlike%20prior%20methods%2C%20this%20subset%20is%0Anot%20fixed%20in%20location%20but%20rather%20which%20parameters%20are%20modified%20evolves%20over%20the%0Acourse%20of%20training.%20This%20dynamic%20parameter%20selection%20can%20yield%20good%20performance%0Awith%20many%20fewer%20parameters%20than%20extant%20methods.%20Our%20method%20enables%20a%20seamless%0Ascaling%20of%20the%20subset%20size%20across%20an%20arbitrary%20proportion%20of%20the%20total%20model%0Asize%2C%20while%20popular%20PET%20approaches%20like%20prompt%20tuning%20and%20LoRA%20cover%20only%20a%0Asmall%20part%20of%20this%20spectrum.%20We%20match%20or%20outperform%20prompt%20tuning%20and%20LoRA%20in%0Amost%20cases%20on%20a%20variety%20of%20NLP%20tasks%20%28MT%2C%20QA%2C%20GSM8K%2C%20SuperGLUE%29%20for%20a%20given%0Aparameter%20budget%20across%20different%20model%20families%20and%20sizes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08610v1&entry.124074799=Read"},
{"title": "A Single Transformer for Scalable Vision-Language Modeling", "author": "Yangyi Chen and Xingyao Wang and Hao Peng and Heng Ji", "abstract": "  We present SOLO, a single transformer for Scalable visiOn-Language mOdeling.\nCurrent large vision-language models (LVLMs) such as LLaVA mostly employ\nheterogeneous architectures that connect pre-trained visual encoders with large\nlanguage models (LLMs) to facilitate visual recognition and complex reasoning.\nAlthough achieving remarkable performance with relatively lightweight training,\nwe identify four primary scalability limitations: (1) The visual capacity is\nconstrained by pre-trained visual encoders, which are typically an order of\nmagnitude smaller than LLMs. (2) The heterogeneous architecture complicates the\nuse of established hardware and software infrastructure. (3) Study of scaling\nlaws on such architecture must consider three separate components - visual\nencoder, connector, and LLMs, which complicates the analysis. (4) The use of\nexisting visual encoders typically requires following a pre-defined\nspecification of image inputs pre-processing, for example, by reshaping inputs\nto fixed-resolution square images, which presents difficulties in processing\nand training on high-resolution images or those with unusual aspect ratio. A\nunified single Transformer architecture, like SOLO, effectively addresses these\nscalability concerns in LVLMs; however, its limited adoption in the modern\ncontext likely stems from the absence of reliable training recipes that balance\nboth modalities and ensure stable training for billion-scale models. In this\npaper, we introduce the first open-source training recipe for developing SOLO,\nan open-source 7B LVLM using moderate academic resources. The training recipe\ninvolves initializing from LLMs, sequential pre-training on ImageNet and\nweb-scale data, and instruction fine-tuning on our curated high-quality\ndatasets. On extensive evaluation, SOLO demonstrates performance comparable to\nLLaVA-v1.5-7B, particularly excelling in visual mathematical reasoning.\n", "link": "http://arxiv.org/abs/2407.06438v2", "date": "2024-11-13", "relevancy": 2.3153, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5865}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5773}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5773}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Single%20Transformer%20for%20Scalable%20Vision-Language%20Modeling&body=Title%3A%20A%20Single%20Transformer%20for%20Scalable%20Vision-Language%20Modeling%0AAuthor%3A%20Yangyi%20Chen%20and%20Xingyao%20Wang%20and%20Hao%20Peng%20and%20Heng%20Ji%0AAbstract%3A%20%20%20We%20present%20SOLO%2C%20a%20single%20transformer%20for%20Scalable%20visiOn-Language%20mOdeling.%0ACurrent%20large%20vision-language%20models%20%28LVLMs%29%20such%20as%20LLaVA%20mostly%20employ%0Aheterogeneous%20architectures%20that%20connect%20pre-trained%20visual%20encoders%20with%20large%0Alanguage%20models%20%28LLMs%29%20to%20facilitate%20visual%20recognition%20and%20complex%20reasoning.%0AAlthough%20achieving%20remarkable%20performance%20with%20relatively%20lightweight%20training%2C%0Awe%20identify%20four%20primary%20scalability%20limitations%3A%20%281%29%20The%20visual%20capacity%20is%0Aconstrained%20by%20pre-trained%20visual%20encoders%2C%20which%20are%20typically%20an%20order%20of%0Amagnitude%20smaller%20than%20LLMs.%20%282%29%20The%20heterogeneous%20architecture%20complicates%20the%0Ause%20of%20established%20hardware%20and%20software%20infrastructure.%20%283%29%20Study%20of%20scaling%0Alaws%20on%20such%20architecture%20must%20consider%20three%20separate%20components%20-%20visual%0Aencoder%2C%20connector%2C%20and%20LLMs%2C%20which%20complicates%20the%20analysis.%20%284%29%20The%20use%20of%0Aexisting%20visual%20encoders%20typically%20requires%20following%20a%20pre-defined%0Aspecification%20of%20image%20inputs%20pre-processing%2C%20for%20example%2C%20by%20reshaping%20inputs%0Ato%20fixed-resolution%20square%20images%2C%20which%20presents%20difficulties%20in%20processing%0Aand%20training%20on%20high-resolution%20images%20or%20those%20with%20unusual%20aspect%20ratio.%20A%0Aunified%20single%20Transformer%20architecture%2C%20like%20SOLO%2C%20effectively%20addresses%20these%0Ascalability%20concerns%20in%20LVLMs%3B%20however%2C%20its%20limited%20adoption%20in%20the%20modern%0Acontext%20likely%20stems%20from%20the%20absence%20of%20reliable%20training%20recipes%20that%20balance%0Aboth%20modalities%20and%20ensure%20stable%20training%20for%20billion-scale%20models.%20In%20this%0Apaper%2C%20we%20introduce%20the%20first%20open-source%20training%20recipe%20for%20developing%20SOLO%2C%0Aan%20open-source%207B%20LVLM%20using%20moderate%20academic%20resources.%20The%20training%20recipe%0Ainvolves%20initializing%20from%20LLMs%2C%20sequential%20pre-training%20on%20ImageNet%20and%0Aweb-scale%20data%2C%20and%20instruction%20fine-tuning%20on%20our%20curated%20high-quality%0Adatasets.%20On%20extensive%20evaluation%2C%20SOLO%20demonstrates%20performance%20comparable%20to%0ALLaVA-v1.5-7B%2C%20particularly%20excelling%20in%20visual%20mathematical%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.06438v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Single%2520Transformer%2520for%2520Scalable%2520Vision-Language%2520Modeling%26entry.906535625%3DYangyi%2520Chen%2520and%2520Xingyao%2520Wang%2520and%2520Hao%2520Peng%2520and%2520Heng%2520Ji%26entry.1292438233%3D%2520%2520We%2520present%2520SOLO%252C%2520a%2520single%2520transformer%2520for%2520Scalable%2520visiOn-Language%2520mOdeling.%250ACurrent%2520large%2520vision-language%2520models%2520%2528LVLMs%2529%2520such%2520as%2520LLaVA%2520mostly%2520employ%250Aheterogeneous%2520architectures%2520that%2520connect%2520pre-trained%2520visual%2520encoders%2520with%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520to%2520facilitate%2520visual%2520recognition%2520and%2520complex%2520reasoning.%250AAlthough%2520achieving%2520remarkable%2520performance%2520with%2520relatively%2520lightweight%2520training%252C%250Awe%2520identify%2520four%2520primary%2520scalability%2520limitations%253A%2520%25281%2529%2520The%2520visual%2520capacity%2520is%250Aconstrained%2520by%2520pre-trained%2520visual%2520encoders%252C%2520which%2520are%2520typically%2520an%2520order%2520of%250Amagnitude%2520smaller%2520than%2520LLMs.%2520%25282%2529%2520The%2520heterogeneous%2520architecture%2520complicates%2520the%250Ause%2520of%2520established%2520hardware%2520and%2520software%2520infrastructure.%2520%25283%2529%2520Study%2520of%2520scaling%250Alaws%2520on%2520such%2520architecture%2520must%2520consider%2520three%2520separate%2520components%2520-%2520visual%250Aencoder%252C%2520connector%252C%2520and%2520LLMs%252C%2520which%2520complicates%2520the%2520analysis.%2520%25284%2529%2520The%2520use%2520of%250Aexisting%2520visual%2520encoders%2520typically%2520requires%2520following%2520a%2520pre-defined%250Aspecification%2520of%2520image%2520inputs%2520pre-processing%252C%2520for%2520example%252C%2520by%2520reshaping%2520inputs%250Ato%2520fixed-resolution%2520square%2520images%252C%2520which%2520presents%2520difficulties%2520in%2520processing%250Aand%2520training%2520on%2520high-resolution%2520images%2520or%2520those%2520with%2520unusual%2520aspect%2520ratio.%2520A%250Aunified%2520single%2520Transformer%2520architecture%252C%2520like%2520SOLO%252C%2520effectively%2520addresses%2520these%250Ascalability%2520concerns%2520in%2520LVLMs%253B%2520however%252C%2520its%2520limited%2520adoption%2520in%2520the%2520modern%250Acontext%2520likely%2520stems%2520from%2520the%2520absence%2520of%2520reliable%2520training%2520recipes%2520that%2520balance%250Aboth%2520modalities%2520and%2520ensure%2520stable%2520training%2520for%2520billion-scale%2520models.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520the%2520first%2520open-source%2520training%2520recipe%2520for%2520developing%2520SOLO%252C%250Aan%2520open-source%25207B%2520LVLM%2520using%2520moderate%2520academic%2520resources.%2520The%2520training%2520recipe%250Ainvolves%2520initializing%2520from%2520LLMs%252C%2520sequential%2520pre-training%2520on%2520ImageNet%2520and%250Aweb-scale%2520data%252C%2520and%2520instruction%2520fine-tuning%2520on%2520our%2520curated%2520high-quality%250Adatasets.%2520On%2520extensive%2520evaluation%252C%2520SOLO%2520demonstrates%2520performance%2520comparable%2520to%250ALLaVA-v1.5-7B%252C%2520particularly%2520excelling%2520in%2520visual%2520mathematical%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.06438v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Single%20Transformer%20for%20Scalable%20Vision-Language%20Modeling&entry.906535625=Yangyi%20Chen%20and%20Xingyao%20Wang%20and%20Hao%20Peng%20and%20Heng%20Ji&entry.1292438233=%20%20We%20present%20SOLO%2C%20a%20single%20transformer%20for%20Scalable%20visiOn-Language%20mOdeling.%0ACurrent%20large%20vision-language%20models%20%28LVLMs%29%20such%20as%20LLaVA%20mostly%20employ%0Aheterogeneous%20architectures%20that%20connect%20pre-trained%20visual%20encoders%20with%20large%0Alanguage%20models%20%28LLMs%29%20to%20facilitate%20visual%20recognition%20and%20complex%20reasoning.%0AAlthough%20achieving%20remarkable%20performance%20with%20relatively%20lightweight%20training%2C%0Awe%20identify%20four%20primary%20scalability%20limitations%3A%20%281%29%20The%20visual%20capacity%20is%0Aconstrained%20by%20pre-trained%20visual%20encoders%2C%20which%20are%20typically%20an%20order%20of%0Amagnitude%20smaller%20than%20LLMs.%20%282%29%20The%20heterogeneous%20architecture%20complicates%20the%0Ause%20of%20established%20hardware%20and%20software%20infrastructure.%20%283%29%20Study%20of%20scaling%0Alaws%20on%20such%20architecture%20must%20consider%20three%20separate%20components%20-%20visual%0Aencoder%2C%20connector%2C%20and%20LLMs%2C%20which%20complicates%20the%20analysis.%20%284%29%20The%20use%20of%0Aexisting%20visual%20encoders%20typically%20requires%20following%20a%20pre-defined%0Aspecification%20of%20image%20inputs%20pre-processing%2C%20for%20example%2C%20by%20reshaping%20inputs%0Ato%20fixed-resolution%20square%20images%2C%20which%20presents%20difficulties%20in%20processing%0Aand%20training%20on%20high-resolution%20images%20or%20those%20with%20unusual%20aspect%20ratio.%20A%0Aunified%20single%20Transformer%20architecture%2C%20like%20SOLO%2C%20effectively%20addresses%20these%0Ascalability%20concerns%20in%20LVLMs%3B%20however%2C%20its%20limited%20adoption%20in%20the%20modern%0Acontext%20likely%20stems%20from%20the%20absence%20of%20reliable%20training%20recipes%20that%20balance%0Aboth%20modalities%20and%20ensure%20stable%20training%20for%20billion-scale%20models.%20In%20this%0Apaper%2C%20we%20introduce%20the%20first%20open-source%20training%20recipe%20for%20developing%20SOLO%2C%0Aan%20open-source%207B%20LVLM%20using%20moderate%20academic%20resources.%20The%20training%20recipe%0Ainvolves%20initializing%20from%20LLMs%2C%20sequential%20pre-training%20on%20ImageNet%20and%0Aweb-scale%20data%2C%20and%20instruction%20fine-tuning%20on%20our%20curated%20high-quality%0Adatasets.%20On%20extensive%20evaluation%2C%20SOLO%20demonstrates%20performance%20comparable%20to%0ALLaVA-v1.5-7B%2C%20particularly%20excelling%20in%20visual%20mathematical%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.06438v2&entry.124074799=Read"},
{"title": "Generalized Pose Space Embeddings for Training In-the-Wild using\n  Anaylis-by-Synthesis", "author": "Dominik Borer and Jakob Buhmann and Martin Guay", "abstract": "  Modern pose estimation models are trained on large, manually-labelled\ndatasets which are costly and may not cover the full extent of human poses and\nappearances in the real world. With advances in neural rendering,\nanalysis-by-synthesis and the ability to not only predict, but also render the\npose, is becoming an appealing framework, which could alleviate the need for\nlarge scale manual labelling efforts. While recent work have shown the\nfeasibility of this approach, the predictions admit many flips due to a\nsimplistic intermediate skeleton representation, resulting in low precision and\ninhibiting the acquisition of any downstream knowledge such as\nthree-dimensional positioning. We solve this problem with a more expressive\nintermediate skeleton representation capable of capturing the semantics of the\npose (left and right), which significantly reduces flips. To successfully train\nthis new representation, we extend the analysis-by-synthesis framework with a\ntraining protocol based on synthetic data. We show that our representation\nresults in less flips and more accurate predictions. Our approach outperforms\nprevious models trained with analysis-by-synthesis on standard benchmarks.\n", "link": "http://arxiv.org/abs/2411.08603v1", "date": "2024-11-13", "relevancy": 2.3142, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5863}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5836}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalized%20Pose%20Space%20Embeddings%20for%20Training%20In-the-Wild%20using%0A%20%20Anaylis-by-Synthesis&body=Title%3A%20Generalized%20Pose%20Space%20Embeddings%20for%20Training%20In-the-Wild%20using%0A%20%20Anaylis-by-Synthesis%0AAuthor%3A%20Dominik%20Borer%20and%20Jakob%20Buhmann%20and%20Martin%20Guay%0AAbstract%3A%20%20%20Modern%20pose%20estimation%20models%20are%20trained%20on%20large%2C%20manually-labelled%0Adatasets%20which%20are%20costly%20and%20may%20not%20cover%20the%20full%20extent%20of%20human%20poses%20and%0Aappearances%20in%20the%20real%20world.%20With%20advances%20in%20neural%20rendering%2C%0Aanalysis-by-synthesis%20and%20the%20ability%20to%20not%20only%20predict%2C%20but%20also%20render%20the%0Apose%2C%20is%20becoming%20an%20appealing%20framework%2C%20which%20could%20alleviate%20the%20need%20for%0Alarge%20scale%20manual%20labelling%20efforts.%20While%20recent%20work%20have%20shown%20the%0Afeasibility%20of%20this%20approach%2C%20the%20predictions%20admit%20many%20flips%20due%20to%20a%0Asimplistic%20intermediate%20skeleton%20representation%2C%20resulting%20in%20low%20precision%20and%0Ainhibiting%20the%20acquisition%20of%20any%20downstream%20knowledge%20such%20as%0Athree-dimensional%20positioning.%20We%20solve%20this%20problem%20with%20a%20more%20expressive%0Aintermediate%20skeleton%20representation%20capable%20of%20capturing%20the%20semantics%20of%20the%0Apose%20%28left%20and%20right%29%2C%20which%20significantly%20reduces%20flips.%20To%20successfully%20train%0Athis%20new%20representation%2C%20we%20extend%20the%20analysis-by-synthesis%20framework%20with%20a%0Atraining%20protocol%20based%20on%20synthetic%20data.%20We%20show%20that%20our%20representation%0Aresults%20in%20less%20flips%20and%20more%20accurate%20predictions.%20Our%20approach%20outperforms%0Aprevious%20models%20trained%20with%20analysis-by-synthesis%20on%20standard%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08603v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralized%2520Pose%2520Space%2520Embeddings%2520for%2520Training%2520In-the-Wild%2520using%250A%2520%2520Anaylis-by-Synthesis%26entry.906535625%3DDominik%2520Borer%2520and%2520Jakob%2520Buhmann%2520and%2520Martin%2520Guay%26entry.1292438233%3D%2520%2520Modern%2520pose%2520estimation%2520models%2520are%2520trained%2520on%2520large%252C%2520manually-labelled%250Adatasets%2520which%2520are%2520costly%2520and%2520may%2520not%2520cover%2520the%2520full%2520extent%2520of%2520human%2520poses%2520and%250Aappearances%2520in%2520the%2520real%2520world.%2520With%2520advances%2520in%2520neural%2520rendering%252C%250Aanalysis-by-synthesis%2520and%2520the%2520ability%2520to%2520not%2520only%2520predict%252C%2520but%2520also%2520render%2520the%250Apose%252C%2520is%2520becoming%2520an%2520appealing%2520framework%252C%2520which%2520could%2520alleviate%2520the%2520need%2520for%250Alarge%2520scale%2520manual%2520labelling%2520efforts.%2520While%2520recent%2520work%2520have%2520shown%2520the%250Afeasibility%2520of%2520this%2520approach%252C%2520the%2520predictions%2520admit%2520many%2520flips%2520due%2520to%2520a%250Asimplistic%2520intermediate%2520skeleton%2520representation%252C%2520resulting%2520in%2520low%2520precision%2520and%250Ainhibiting%2520the%2520acquisition%2520of%2520any%2520downstream%2520knowledge%2520such%2520as%250Athree-dimensional%2520positioning.%2520We%2520solve%2520this%2520problem%2520with%2520a%2520more%2520expressive%250Aintermediate%2520skeleton%2520representation%2520capable%2520of%2520capturing%2520the%2520semantics%2520of%2520the%250Apose%2520%2528left%2520and%2520right%2529%252C%2520which%2520significantly%2520reduces%2520flips.%2520To%2520successfully%2520train%250Athis%2520new%2520representation%252C%2520we%2520extend%2520the%2520analysis-by-synthesis%2520framework%2520with%2520a%250Atraining%2520protocol%2520based%2520on%2520synthetic%2520data.%2520We%2520show%2520that%2520our%2520representation%250Aresults%2520in%2520less%2520flips%2520and%2520more%2520accurate%2520predictions.%2520Our%2520approach%2520outperforms%250Aprevious%2520models%2520trained%2520with%2520analysis-by-synthesis%2520on%2520standard%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08603v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Pose%20Space%20Embeddings%20for%20Training%20In-the-Wild%20using%0A%20%20Anaylis-by-Synthesis&entry.906535625=Dominik%20Borer%20and%20Jakob%20Buhmann%20and%20Martin%20Guay&entry.1292438233=%20%20Modern%20pose%20estimation%20models%20are%20trained%20on%20large%2C%20manually-labelled%0Adatasets%20which%20are%20costly%20and%20may%20not%20cover%20the%20full%20extent%20of%20human%20poses%20and%0Aappearances%20in%20the%20real%20world.%20With%20advances%20in%20neural%20rendering%2C%0Aanalysis-by-synthesis%20and%20the%20ability%20to%20not%20only%20predict%2C%20but%20also%20render%20the%0Apose%2C%20is%20becoming%20an%20appealing%20framework%2C%20which%20could%20alleviate%20the%20need%20for%0Alarge%20scale%20manual%20labelling%20efforts.%20While%20recent%20work%20have%20shown%20the%0Afeasibility%20of%20this%20approach%2C%20the%20predictions%20admit%20many%20flips%20due%20to%20a%0Asimplistic%20intermediate%20skeleton%20representation%2C%20resulting%20in%20low%20precision%20and%0Ainhibiting%20the%20acquisition%20of%20any%20downstream%20knowledge%20such%20as%0Athree-dimensional%20positioning.%20We%20solve%20this%20problem%20with%20a%20more%20expressive%0Aintermediate%20skeleton%20representation%20capable%20of%20capturing%20the%20semantics%20of%20the%0Apose%20%28left%20and%20right%29%2C%20which%20significantly%20reduces%20flips.%20To%20successfully%20train%0Athis%20new%20representation%2C%20we%20extend%20the%20analysis-by-synthesis%20framework%20with%20a%0Atraining%20protocol%20based%20on%20synthetic%20data.%20We%20show%20that%20our%20representation%0Aresults%20in%20less%20flips%20and%20more%20accurate%20predictions.%20Our%20approach%20outperforms%0Aprevious%20models%20trained%20with%20analysis-by-synthesis%20on%20standard%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08603v1&entry.124074799=Read"},
{"title": "VoxelKeypointFusion: Generalizable Multi-View Multi-Person Pose\n  Estimation", "author": "Daniel Bermuth and Alexander Poeppel and Wolfgang Reif", "abstract": "  In the rapidly evolving field of computer vision, the task of accurately\nestimating the poses of multiple individuals from various viewpoints presents a\nformidable challenge, especially if the estimations should be reliable as well.\nThis work presents an extensive evaluation of the generalization capabilities\nof multi-view multi-person pose estimators to unseen datasets and presents a\nnew algorithm with strong performance in this task. It also studies the\nimprovements by additionally using depth information. Since the new approach\ncan not only generalize well to unseen datasets, but also to different\nkeypoints, the first multi-view multi-person whole-body estimator is presented.\nTo support further research on those topics, all of the work is publicly\naccessible.\n", "link": "http://arxiv.org/abs/2410.18723v2", "date": "2024-11-13", "relevancy": 2.3087, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5908}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5892}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5598}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VoxelKeypointFusion%3A%20Generalizable%20Multi-View%20Multi-Person%20Pose%0A%20%20Estimation&body=Title%3A%20VoxelKeypointFusion%3A%20Generalizable%20Multi-View%20Multi-Person%20Pose%0A%20%20Estimation%0AAuthor%3A%20Daniel%20Bermuth%20and%20Alexander%20Poeppel%20and%20Wolfgang%20Reif%0AAbstract%3A%20%20%20In%20the%20rapidly%20evolving%20field%20of%20computer%20vision%2C%20the%20task%20of%20accurately%0Aestimating%20the%20poses%20of%20multiple%20individuals%20from%20various%20viewpoints%20presents%20a%0Aformidable%20challenge%2C%20especially%20if%20the%20estimations%20should%20be%20reliable%20as%20well.%0AThis%20work%20presents%20an%20extensive%20evaluation%20of%20the%20generalization%20capabilities%0Aof%20multi-view%20multi-person%20pose%20estimators%20to%20unseen%20datasets%20and%20presents%20a%0Anew%20algorithm%20with%20strong%20performance%20in%20this%20task.%20It%20also%20studies%20the%0Aimprovements%20by%20additionally%20using%20depth%20information.%20Since%20the%20new%20approach%0Acan%20not%20only%20generalize%20well%20to%20unseen%20datasets%2C%20but%20also%20to%20different%0Akeypoints%2C%20the%20first%20multi-view%20multi-person%20whole-body%20estimator%20is%20presented.%0ATo%20support%20further%20research%20on%20those%20topics%2C%20all%20of%20the%20work%20is%20publicly%0Aaccessible.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18723v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVoxelKeypointFusion%253A%2520Generalizable%2520Multi-View%2520Multi-Person%2520Pose%250A%2520%2520Estimation%26entry.906535625%3DDaniel%2520Bermuth%2520and%2520Alexander%2520Poeppel%2520and%2520Wolfgang%2520Reif%26entry.1292438233%3D%2520%2520In%2520the%2520rapidly%2520evolving%2520field%2520of%2520computer%2520vision%252C%2520the%2520task%2520of%2520accurately%250Aestimating%2520the%2520poses%2520of%2520multiple%2520individuals%2520from%2520various%2520viewpoints%2520presents%2520a%250Aformidable%2520challenge%252C%2520especially%2520if%2520the%2520estimations%2520should%2520be%2520reliable%2520as%2520well.%250AThis%2520work%2520presents%2520an%2520extensive%2520evaluation%2520of%2520the%2520generalization%2520capabilities%250Aof%2520multi-view%2520multi-person%2520pose%2520estimators%2520to%2520unseen%2520datasets%2520and%2520presents%2520a%250Anew%2520algorithm%2520with%2520strong%2520performance%2520in%2520this%2520task.%2520It%2520also%2520studies%2520the%250Aimprovements%2520by%2520additionally%2520using%2520depth%2520information.%2520Since%2520the%2520new%2520approach%250Acan%2520not%2520only%2520generalize%2520well%2520to%2520unseen%2520datasets%252C%2520but%2520also%2520to%2520different%250Akeypoints%252C%2520the%2520first%2520multi-view%2520multi-person%2520whole-body%2520estimator%2520is%2520presented.%250ATo%2520support%2520further%2520research%2520on%2520those%2520topics%252C%2520all%2520of%2520the%2520work%2520is%2520publicly%250Aaccessible.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18723v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VoxelKeypointFusion%3A%20Generalizable%20Multi-View%20Multi-Person%20Pose%0A%20%20Estimation&entry.906535625=Daniel%20Bermuth%20and%20Alexander%20Poeppel%20and%20Wolfgang%20Reif&entry.1292438233=%20%20In%20the%20rapidly%20evolving%20field%20of%20computer%20vision%2C%20the%20task%20of%20accurately%0Aestimating%20the%20poses%20of%20multiple%20individuals%20from%20various%20viewpoints%20presents%20a%0Aformidable%20challenge%2C%20especially%20if%20the%20estimations%20should%20be%20reliable%20as%20well.%0AThis%20work%20presents%20an%20extensive%20evaluation%20of%20the%20generalization%20capabilities%0Aof%20multi-view%20multi-person%20pose%20estimators%20to%20unseen%20datasets%20and%20presents%20a%0Anew%20algorithm%20with%20strong%20performance%20in%20this%20task.%20It%20also%20studies%20the%0Aimprovements%20by%20additionally%20using%20depth%20information.%20Since%20the%20new%20approach%0Acan%20not%20only%20generalize%20well%20to%20unseen%20datasets%2C%20but%20also%20to%20different%0Akeypoints%2C%20the%20first%20multi-view%20multi-person%20whole-body%20estimator%20is%20presented.%0ATo%20support%20further%20research%20on%20those%20topics%2C%20all%20of%20the%20work%20is%20publicly%0Aaccessible.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18723v2&entry.124074799=Read"},
{"title": "Quantifying and Mitigating Unimodal Biases in Multimodal Large Language\n  Models: A Causal Perspective", "author": "Meiqi Chen and Yixin Cao and Yan Zhang and Chaochao Lu", "abstract": "  Recent advancements in Large Language Models (LLMs) have facilitated the\ndevelopment of Multimodal LLMs (MLLMs). Despite their impressive capabilities,\nMLLMs often suffer from over-reliance on unimodal biases (e.g., language bias\nand vision bias), leading to incorrect answers or hallucinations in complex\nmultimodal tasks. To investigate this issue, we propose a causal framework to\ninterpret the biases in Visual Question Answering (VQA) problems. Within this\nframework, we conduct an in-depth causal analysis to assess the causal effect\nof these biases on MLLM predictions. Based on the analysis, we introduce 1) a\nnovel MORE dataset with 12,000 challenging VQA instances requiring multi-hop\nreasoning and overcoming unimodal biases. 2) a causality-enhanced agent\nframework CAVE that guides models to comprehensively integrate information from\ndifferent modalities and mitigate biases. Our experiments show that MLLMs\nperform poorly on MORE, indicating strong unimodal biases and limited semantic\nunderstanding. However, when integrated with our CAVE, promising improvements\nin reasoning and bias mitigation can be seen. These findings provide important\ninsights for the development of more robust MLLMs and contribute to the broader\ngoal of advancing multimodal AI systems capable of deeper understanding and\nreasoning. Our project page is at https://github.com/OpenCausaLab/MORE.\n", "link": "http://arxiv.org/abs/2403.18346v4", "date": "2024-11-13", "relevancy": 2.3019, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5804}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5745}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5745}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantifying%20and%20Mitigating%20Unimodal%20Biases%20in%20Multimodal%20Large%20Language%0A%20%20Models%3A%20A%20Causal%20Perspective&body=Title%3A%20Quantifying%20and%20Mitigating%20Unimodal%20Biases%20in%20Multimodal%20Large%20Language%0A%20%20Models%3A%20A%20Causal%20Perspective%0AAuthor%3A%20Meiqi%20Chen%20and%20Yixin%20Cao%20and%20Yan%20Zhang%20and%20Chaochao%20Lu%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20facilitated%20the%0Adevelopment%20of%20Multimodal%20LLMs%20%28MLLMs%29.%20Despite%20their%20impressive%20capabilities%2C%0AMLLMs%20often%20suffer%20from%20over-reliance%20on%20unimodal%20biases%20%28e.g.%2C%20language%20bias%0Aand%20vision%20bias%29%2C%20leading%20to%20incorrect%20answers%20or%20hallucinations%20in%20complex%0Amultimodal%20tasks.%20To%20investigate%20this%20issue%2C%20we%20propose%20a%20causal%20framework%20to%0Ainterpret%20the%20biases%20in%20Visual%20Question%20Answering%20%28VQA%29%20problems.%20Within%20this%0Aframework%2C%20we%20conduct%20an%20in-depth%20causal%20analysis%20to%20assess%20the%20causal%20effect%0Aof%20these%20biases%20on%20MLLM%20predictions.%20Based%20on%20the%20analysis%2C%20we%20introduce%201%29%20a%0Anovel%20MORE%20dataset%20with%2012%2C000%20challenging%20VQA%20instances%20requiring%20multi-hop%0Areasoning%20and%20overcoming%20unimodal%20biases.%202%29%20a%20causality-enhanced%20agent%0Aframework%20CAVE%20that%20guides%20models%20to%20comprehensively%20integrate%20information%20from%0Adifferent%20modalities%20and%20mitigate%20biases.%20Our%20experiments%20show%20that%20MLLMs%0Aperform%20poorly%20on%20MORE%2C%20indicating%20strong%20unimodal%20biases%20and%20limited%20semantic%0Aunderstanding.%20However%2C%20when%20integrated%20with%20our%20CAVE%2C%20promising%20improvements%0Ain%20reasoning%20and%20bias%20mitigation%20can%20be%20seen.%20These%20findings%20provide%20important%0Ainsights%20for%20the%20development%20of%20more%20robust%20MLLMs%20and%20contribute%20to%20the%20broader%0Agoal%20of%20advancing%20multimodal%20AI%20systems%20capable%20of%20deeper%20understanding%20and%0Areasoning.%20Our%20project%20page%20is%20at%20https%3A//github.com/OpenCausaLab/MORE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.18346v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantifying%2520and%2520Mitigating%2520Unimodal%2520Biases%2520in%2520Multimodal%2520Large%2520Language%250A%2520%2520Models%253A%2520A%2520Causal%2520Perspective%26entry.906535625%3DMeiqi%2520Chen%2520and%2520Yixin%2520Cao%2520and%2520Yan%2520Zhang%2520and%2520Chaochao%2520Lu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520facilitated%2520the%250Adevelopment%2520of%2520Multimodal%2520LLMs%2520%2528MLLMs%2529.%2520Despite%2520their%2520impressive%2520capabilities%252C%250AMLLMs%2520often%2520suffer%2520from%2520over-reliance%2520on%2520unimodal%2520biases%2520%2528e.g.%252C%2520language%2520bias%250Aand%2520vision%2520bias%2529%252C%2520leading%2520to%2520incorrect%2520answers%2520or%2520hallucinations%2520in%2520complex%250Amultimodal%2520tasks.%2520To%2520investigate%2520this%2520issue%252C%2520we%2520propose%2520a%2520causal%2520framework%2520to%250Ainterpret%2520the%2520biases%2520in%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520problems.%2520Within%2520this%250Aframework%252C%2520we%2520conduct%2520an%2520in-depth%2520causal%2520analysis%2520to%2520assess%2520the%2520causal%2520effect%250Aof%2520these%2520biases%2520on%2520MLLM%2520predictions.%2520Based%2520on%2520the%2520analysis%252C%2520we%2520introduce%25201%2529%2520a%250Anovel%2520MORE%2520dataset%2520with%252012%252C000%2520challenging%2520VQA%2520instances%2520requiring%2520multi-hop%250Areasoning%2520and%2520overcoming%2520unimodal%2520biases.%25202%2529%2520a%2520causality-enhanced%2520agent%250Aframework%2520CAVE%2520that%2520guides%2520models%2520to%2520comprehensively%2520integrate%2520information%2520from%250Adifferent%2520modalities%2520and%2520mitigate%2520biases.%2520Our%2520experiments%2520show%2520that%2520MLLMs%250Aperform%2520poorly%2520on%2520MORE%252C%2520indicating%2520strong%2520unimodal%2520biases%2520and%2520limited%2520semantic%250Aunderstanding.%2520However%252C%2520when%2520integrated%2520with%2520our%2520CAVE%252C%2520promising%2520improvements%250Ain%2520reasoning%2520and%2520bias%2520mitigation%2520can%2520be%2520seen.%2520These%2520findings%2520provide%2520important%250Ainsights%2520for%2520the%2520development%2520of%2520more%2520robust%2520MLLMs%2520and%2520contribute%2520to%2520the%2520broader%250Agoal%2520of%2520advancing%2520multimodal%2520AI%2520systems%2520capable%2520of%2520deeper%2520understanding%2520and%250Areasoning.%2520Our%2520project%2520page%2520is%2520at%2520https%253A//github.com/OpenCausaLab/MORE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.18346v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantifying%20and%20Mitigating%20Unimodal%20Biases%20in%20Multimodal%20Large%20Language%0A%20%20Models%3A%20A%20Causal%20Perspective&entry.906535625=Meiqi%20Chen%20and%20Yixin%20Cao%20and%20Yan%20Zhang%20and%20Chaochao%20Lu&entry.1292438233=%20%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20facilitated%20the%0Adevelopment%20of%20Multimodal%20LLMs%20%28MLLMs%29.%20Despite%20their%20impressive%20capabilities%2C%0AMLLMs%20often%20suffer%20from%20over-reliance%20on%20unimodal%20biases%20%28e.g.%2C%20language%20bias%0Aand%20vision%20bias%29%2C%20leading%20to%20incorrect%20answers%20or%20hallucinations%20in%20complex%0Amultimodal%20tasks.%20To%20investigate%20this%20issue%2C%20we%20propose%20a%20causal%20framework%20to%0Ainterpret%20the%20biases%20in%20Visual%20Question%20Answering%20%28VQA%29%20problems.%20Within%20this%0Aframework%2C%20we%20conduct%20an%20in-depth%20causal%20analysis%20to%20assess%20the%20causal%20effect%0Aof%20these%20biases%20on%20MLLM%20predictions.%20Based%20on%20the%20analysis%2C%20we%20introduce%201%29%20a%0Anovel%20MORE%20dataset%20with%2012%2C000%20challenging%20VQA%20instances%20requiring%20multi-hop%0Areasoning%20and%20overcoming%20unimodal%20biases.%202%29%20a%20causality-enhanced%20agent%0Aframework%20CAVE%20that%20guides%20models%20to%20comprehensively%20integrate%20information%20from%0Adifferent%20modalities%20and%20mitigate%20biases.%20Our%20experiments%20show%20that%20MLLMs%0Aperform%20poorly%20on%20MORE%2C%20indicating%20strong%20unimodal%20biases%20and%20limited%20semantic%0Aunderstanding.%20However%2C%20when%20integrated%20with%20our%20CAVE%2C%20promising%20improvements%0Ain%20reasoning%20and%20bias%20mitigation%20can%20be%20seen.%20These%20findings%20provide%20important%0Ainsights%20for%20the%20development%20of%20more%20robust%20MLLMs%20and%20contribute%20to%20the%20broader%0Agoal%20of%20advancing%20multimodal%20AI%20systems%20capable%20of%20deeper%20understanding%20and%0Areasoning.%20Our%20project%20page%20is%20at%20https%3A//github.com/OpenCausaLab/MORE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.18346v4&entry.124074799=Read"},
{"title": "Which Viewpoint Shows it Best? Language for Weakly Supervising View\n  Selection in Multi-view Videos", "author": "Sagnik Majumder and Tushar Nagarajan and Ziad Al-Halah and Reina Pradhan and Kristen Grauman", "abstract": "  Given a multi-view video, which viewpoint is most informative for a human\nobserver? Existing methods rely on heuristics or expensive ``best-view\"\nsupervision to answer this question, limiting their applicability. We propose a\nweakly supervised approach that leverages language accompanying an\ninstructional multi-view video as a means to recover its most informative\nviewpoint(s). Our key hypothesis is that the more accurately an individual view\ncan predict a view-agnostic text summary, the more informative it is. To put\nthis into action, we propose a framework that uses the relative accuracy of\nview-dependent caption predictions as a proxy for best view pseudo-labels.\nThen, those pseudo-labels are used to train a view selector, together with an\nauxiliary camera pose predictor that enhances view-sensitivity. During\ninference, our model takes as input only a multi-view video -- no language or\ncamera poses -- and returns the best viewpoint to watch at each timestep. On\ntwo challenging datasets comprised of diverse multi-camera setups and how-to\nactivities, our model consistently outperforms state-of-the-art baselines, both\nwith quantitative metrics and human evaluation.\n", "link": "http://arxiv.org/abs/2411.08753v1", "date": "2024-11-13", "relevancy": 2.2894, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5764}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5764}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Which%20Viewpoint%20Shows%20it%20Best%3F%20Language%20for%20Weakly%20Supervising%20View%0A%20%20Selection%20in%20Multi-view%20Videos&body=Title%3A%20Which%20Viewpoint%20Shows%20it%20Best%3F%20Language%20for%20Weakly%20Supervising%20View%0A%20%20Selection%20in%20Multi-view%20Videos%0AAuthor%3A%20Sagnik%20Majumder%20and%20Tushar%20Nagarajan%20and%20Ziad%20Al-Halah%20and%20Reina%20Pradhan%20and%20Kristen%20Grauman%0AAbstract%3A%20%20%20Given%20a%20multi-view%20video%2C%20which%20viewpoint%20is%20most%20informative%20for%20a%20human%0Aobserver%3F%20Existing%20methods%20rely%20on%20heuristics%20or%20expensive%20%60%60best-view%22%0Asupervision%20to%20answer%20this%20question%2C%20limiting%20their%20applicability.%20We%20propose%20a%0Aweakly%20supervised%20approach%20that%20leverages%20language%20accompanying%20an%0Ainstructional%20multi-view%20video%20as%20a%20means%20to%20recover%20its%20most%20informative%0Aviewpoint%28s%29.%20Our%20key%20hypothesis%20is%20that%20the%20more%20accurately%20an%20individual%20view%0Acan%20predict%20a%20view-agnostic%20text%20summary%2C%20the%20more%20informative%20it%20is.%20To%20put%0Athis%20into%20action%2C%20we%20propose%20a%20framework%20that%20uses%20the%20relative%20accuracy%20of%0Aview-dependent%20caption%20predictions%20as%20a%20proxy%20for%20best%20view%20pseudo-labels.%0AThen%2C%20those%20pseudo-labels%20are%20used%20to%20train%20a%20view%20selector%2C%20together%20with%20an%0Aauxiliary%20camera%20pose%20predictor%20that%20enhances%20view-sensitivity.%20During%0Ainference%2C%20our%20model%20takes%20as%20input%20only%20a%20multi-view%20video%20--%20no%20language%20or%0Acamera%20poses%20--%20and%20returns%20the%20best%20viewpoint%20to%20watch%20at%20each%20timestep.%20On%0Atwo%20challenging%20datasets%20comprised%20of%20diverse%20multi-camera%20setups%20and%20how-to%0Aactivities%2C%20our%20model%20consistently%20outperforms%20state-of-the-art%20baselines%2C%20both%0Awith%20quantitative%20metrics%20and%20human%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08753v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhich%2520Viewpoint%2520Shows%2520it%2520Best%253F%2520Language%2520for%2520Weakly%2520Supervising%2520View%250A%2520%2520Selection%2520in%2520Multi-view%2520Videos%26entry.906535625%3DSagnik%2520Majumder%2520and%2520Tushar%2520Nagarajan%2520and%2520Ziad%2520Al-Halah%2520and%2520Reina%2520Pradhan%2520and%2520Kristen%2520Grauman%26entry.1292438233%3D%2520%2520Given%2520a%2520multi-view%2520video%252C%2520which%2520viewpoint%2520is%2520most%2520informative%2520for%2520a%2520human%250Aobserver%253F%2520Existing%2520methods%2520rely%2520on%2520heuristics%2520or%2520expensive%2520%2560%2560best-view%2522%250Asupervision%2520to%2520answer%2520this%2520question%252C%2520limiting%2520their%2520applicability.%2520We%2520propose%2520a%250Aweakly%2520supervised%2520approach%2520that%2520leverages%2520language%2520accompanying%2520an%250Ainstructional%2520multi-view%2520video%2520as%2520a%2520means%2520to%2520recover%2520its%2520most%2520informative%250Aviewpoint%2528s%2529.%2520Our%2520key%2520hypothesis%2520is%2520that%2520the%2520more%2520accurately%2520an%2520individual%2520view%250Acan%2520predict%2520a%2520view-agnostic%2520text%2520summary%252C%2520the%2520more%2520informative%2520it%2520is.%2520To%2520put%250Athis%2520into%2520action%252C%2520we%2520propose%2520a%2520framework%2520that%2520uses%2520the%2520relative%2520accuracy%2520of%250Aview-dependent%2520caption%2520predictions%2520as%2520a%2520proxy%2520for%2520best%2520view%2520pseudo-labels.%250AThen%252C%2520those%2520pseudo-labels%2520are%2520used%2520to%2520train%2520a%2520view%2520selector%252C%2520together%2520with%2520an%250Aauxiliary%2520camera%2520pose%2520predictor%2520that%2520enhances%2520view-sensitivity.%2520During%250Ainference%252C%2520our%2520model%2520takes%2520as%2520input%2520only%2520a%2520multi-view%2520video%2520--%2520no%2520language%2520or%250Acamera%2520poses%2520--%2520and%2520returns%2520the%2520best%2520viewpoint%2520to%2520watch%2520at%2520each%2520timestep.%2520On%250Atwo%2520challenging%2520datasets%2520comprised%2520of%2520diverse%2520multi-camera%2520setups%2520and%2520how-to%250Aactivities%252C%2520our%2520model%2520consistently%2520outperforms%2520state-of-the-art%2520baselines%252C%2520both%250Awith%2520quantitative%2520metrics%2520and%2520human%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08753v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Which%20Viewpoint%20Shows%20it%20Best%3F%20Language%20for%20Weakly%20Supervising%20View%0A%20%20Selection%20in%20Multi-view%20Videos&entry.906535625=Sagnik%20Majumder%20and%20Tushar%20Nagarajan%20and%20Ziad%20Al-Halah%20and%20Reina%20Pradhan%20and%20Kristen%20Grauman&entry.1292438233=%20%20Given%20a%20multi-view%20video%2C%20which%20viewpoint%20is%20most%20informative%20for%20a%20human%0Aobserver%3F%20Existing%20methods%20rely%20on%20heuristics%20or%20expensive%20%60%60best-view%22%0Asupervision%20to%20answer%20this%20question%2C%20limiting%20their%20applicability.%20We%20propose%20a%0Aweakly%20supervised%20approach%20that%20leverages%20language%20accompanying%20an%0Ainstructional%20multi-view%20video%20as%20a%20means%20to%20recover%20its%20most%20informative%0Aviewpoint%28s%29.%20Our%20key%20hypothesis%20is%20that%20the%20more%20accurately%20an%20individual%20view%0Acan%20predict%20a%20view-agnostic%20text%20summary%2C%20the%20more%20informative%20it%20is.%20To%20put%0Athis%20into%20action%2C%20we%20propose%20a%20framework%20that%20uses%20the%20relative%20accuracy%20of%0Aview-dependent%20caption%20predictions%20as%20a%20proxy%20for%20best%20view%20pseudo-labels.%0AThen%2C%20those%20pseudo-labels%20are%20used%20to%20train%20a%20view%20selector%2C%20together%20with%20an%0Aauxiliary%20camera%20pose%20predictor%20that%20enhances%20view-sensitivity.%20During%0Ainference%2C%20our%20model%20takes%20as%20input%20only%20a%20multi-view%20video%20--%20no%20language%20or%0Acamera%20poses%20--%20and%20returns%20the%20best%20viewpoint%20to%20watch%20at%20each%20timestep.%20On%0Atwo%20challenging%20datasets%20comprised%20of%20diverse%20multi-camera%20setups%20and%20how-to%0Aactivities%2C%20our%20model%20consistently%20outperforms%20state-of-the-art%20baselines%2C%20both%0Awith%20quantitative%20metrics%20and%20human%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08753v1&entry.124074799=Read"},
{"title": "CGRclust: Chaos Game Representation for Twin Contrastive Clustering of\n  Unlabelled DNA Sequences", "author": "Fatemeh Alipour and Kathleen A. Hill and Lila Kari", "abstract": "  This study proposes CGRclust, a novel combination of unsupervised twin\ncontrastive clustering of Chaos Game Representations (CGR) of DNA sequences,\nwith convolutional neural networks (CNNs). To the best of our knowledge,\nCGRclust is the first method to use unsupervised learning for image\nclassification (herein applied to two-dimensional CGR images) for clustering\ndatasets of DNA sequences. CGRclust overcomes the limitations of traditional\nsequence classification methods by leveraging unsupervised twin contrastive\nlearning to detect distinctive sequence patterns, without requiring DNA\nsequence alignment or biological/taxonomic labels. CGRclust accurately\nclustered twenty-five diverse datasets, with sequence lengths ranging from 664\nbp to 100 kbp, including mitochondrial genomes of fish, fungi, and protists, as\nwell as viral whole genome assemblies and synthetic DNA sequences. Compared\nwith three recent clustering methods for DNA sequences (DeLUCS, iDeLUCS, and\nMeShClust v3.0.), CGRclust is the only method that surpasses 81.70% accuracy\nacross all four taxonomic levels tested for mitochondrial DNA genomes of fish.\nMoreover, CGRclust also consistently demonstrates superior performance across\nall the viral genomic datasets. The high clustering accuracy of CGRclust on\nthese twenty-five datasets, which vary significantly in terms of sequence\nlength, number of genomes, number of clusters, and level of taxonomy,\ndemonstrates its robustness, scalability, and versatility.\n", "link": "http://arxiv.org/abs/2407.02538v2", "date": "2024-11-13", "relevancy": 2.2794, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4599}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4563}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CGRclust%3A%20Chaos%20Game%20Representation%20for%20Twin%20Contrastive%20Clustering%20of%0A%20%20Unlabelled%20DNA%20Sequences&body=Title%3A%20CGRclust%3A%20Chaos%20Game%20Representation%20for%20Twin%20Contrastive%20Clustering%20of%0A%20%20Unlabelled%20DNA%20Sequences%0AAuthor%3A%20Fatemeh%20Alipour%20and%20Kathleen%20A.%20Hill%20and%20Lila%20Kari%0AAbstract%3A%20%20%20This%20study%20proposes%20CGRclust%2C%20a%20novel%20combination%20of%20unsupervised%20twin%0Acontrastive%20clustering%20of%20Chaos%20Game%20Representations%20%28CGR%29%20of%20DNA%20sequences%2C%0Awith%20convolutional%20neural%20networks%20%28CNNs%29.%20To%20the%20best%20of%20our%20knowledge%2C%0ACGRclust%20is%20the%20first%20method%20to%20use%20unsupervised%20learning%20for%20image%0Aclassification%20%28herein%20applied%20to%20two-dimensional%20CGR%20images%29%20for%20clustering%0Adatasets%20of%20DNA%20sequences.%20CGRclust%20overcomes%20the%20limitations%20of%20traditional%0Asequence%20classification%20methods%20by%20leveraging%20unsupervised%20twin%20contrastive%0Alearning%20to%20detect%20distinctive%20sequence%20patterns%2C%20without%20requiring%20DNA%0Asequence%20alignment%20or%20biological/taxonomic%20labels.%20CGRclust%20accurately%0Aclustered%20twenty-five%20diverse%20datasets%2C%20with%20sequence%20lengths%20ranging%20from%20664%0Abp%20to%20100%20kbp%2C%20including%20mitochondrial%20genomes%20of%20fish%2C%20fungi%2C%20and%20protists%2C%20as%0Awell%20as%20viral%20whole%20genome%20assemblies%20and%20synthetic%20DNA%20sequences.%20Compared%0Awith%20three%20recent%20clustering%20methods%20for%20DNA%20sequences%20%28DeLUCS%2C%20iDeLUCS%2C%20and%0AMeShClust%20v3.0.%29%2C%20CGRclust%20is%20the%20only%20method%20that%20surpasses%2081.70%25%20accuracy%0Aacross%20all%20four%20taxonomic%20levels%20tested%20for%20mitochondrial%20DNA%20genomes%20of%20fish.%0AMoreover%2C%20CGRclust%20also%20consistently%20demonstrates%20superior%20performance%20across%0Aall%20the%20viral%20genomic%20datasets.%20The%20high%20clustering%20accuracy%20of%20CGRclust%20on%0Athese%20twenty-five%20datasets%2C%20which%20vary%20significantly%20in%20terms%20of%20sequence%0Alength%2C%20number%20of%20genomes%2C%20number%20of%20clusters%2C%20and%20level%20of%20taxonomy%2C%0Ademonstrates%20its%20robustness%2C%20scalability%2C%20and%20versatility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02538v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCGRclust%253A%2520Chaos%2520Game%2520Representation%2520for%2520Twin%2520Contrastive%2520Clustering%2520of%250A%2520%2520Unlabelled%2520DNA%2520Sequences%26entry.906535625%3DFatemeh%2520Alipour%2520and%2520Kathleen%2520A.%2520Hill%2520and%2520Lila%2520Kari%26entry.1292438233%3D%2520%2520This%2520study%2520proposes%2520CGRclust%252C%2520a%2520novel%2520combination%2520of%2520unsupervised%2520twin%250Acontrastive%2520clustering%2520of%2520Chaos%2520Game%2520Representations%2520%2528CGR%2529%2520of%2520DNA%2520sequences%252C%250Awith%2520convolutional%2520neural%2520networks%2520%2528CNNs%2529.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%250ACGRclust%2520is%2520the%2520first%2520method%2520to%2520use%2520unsupervised%2520learning%2520for%2520image%250Aclassification%2520%2528herein%2520applied%2520to%2520two-dimensional%2520CGR%2520images%2529%2520for%2520clustering%250Adatasets%2520of%2520DNA%2520sequences.%2520CGRclust%2520overcomes%2520the%2520limitations%2520of%2520traditional%250Asequence%2520classification%2520methods%2520by%2520leveraging%2520unsupervised%2520twin%2520contrastive%250Alearning%2520to%2520detect%2520distinctive%2520sequence%2520patterns%252C%2520without%2520requiring%2520DNA%250Asequence%2520alignment%2520or%2520biological/taxonomic%2520labels.%2520CGRclust%2520accurately%250Aclustered%2520twenty-five%2520diverse%2520datasets%252C%2520with%2520sequence%2520lengths%2520ranging%2520from%2520664%250Abp%2520to%2520100%2520kbp%252C%2520including%2520mitochondrial%2520genomes%2520of%2520fish%252C%2520fungi%252C%2520and%2520protists%252C%2520as%250Awell%2520as%2520viral%2520whole%2520genome%2520assemblies%2520and%2520synthetic%2520DNA%2520sequences.%2520Compared%250Awith%2520three%2520recent%2520clustering%2520methods%2520for%2520DNA%2520sequences%2520%2528DeLUCS%252C%2520iDeLUCS%252C%2520and%250AMeShClust%2520v3.0.%2529%252C%2520CGRclust%2520is%2520the%2520only%2520method%2520that%2520surpasses%252081.70%2525%2520accuracy%250Aacross%2520all%2520four%2520taxonomic%2520levels%2520tested%2520for%2520mitochondrial%2520DNA%2520genomes%2520of%2520fish.%250AMoreover%252C%2520CGRclust%2520also%2520consistently%2520demonstrates%2520superior%2520performance%2520across%250Aall%2520the%2520viral%2520genomic%2520datasets.%2520The%2520high%2520clustering%2520accuracy%2520of%2520CGRclust%2520on%250Athese%2520twenty-five%2520datasets%252C%2520which%2520vary%2520significantly%2520in%2520terms%2520of%2520sequence%250Alength%252C%2520number%2520of%2520genomes%252C%2520number%2520of%2520clusters%252C%2520and%2520level%2520of%2520taxonomy%252C%250Ademonstrates%2520its%2520robustness%252C%2520scalability%252C%2520and%2520versatility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02538v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CGRclust%3A%20Chaos%20Game%20Representation%20for%20Twin%20Contrastive%20Clustering%20of%0A%20%20Unlabelled%20DNA%20Sequences&entry.906535625=Fatemeh%20Alipour%20and%20Kathleen%20A.%20Hill%20and%20Lila%20Kari&entry.1292438233=%20%20This%20study%20proposes%20CGRclust%2C%20a%20novel%20combination%20of%20unsupervised%20twin%0Acontrastive%20clustering%20of%20Chaos%20Game%20Representations%20%28CGR%29%20of%20DNA%20sequences%2C%0Awith%20convolutional%20neural%20networks%20%28CNNs%29.%20To%20the%20best%20of%20our%20knowledge%2C%0ACGRclust%20is%20the%20first%20method%20to%20use%20unsupervised%20learning%20for%20image%0Aclassification%20%28herein%20applied%20to%20two-dimensional%20CGR%20images%29%20for%20clustering%0Adatasets%20of%20DNA%20sequences.%20CGRclust%20overcomes%20the%20limitations%20of%20traditional%0Asequence%20classification%20methods%20by%20leveraging%20unsupervised%20twin%20contrastive%0Alearning%20to%20detect%20distinctive%20sequence%20patterns%2C%20without%20requiring%20DNA%0Asequence%20alignment%20or%20biological/taxonomic%20labels.%20CGRclust%20accurately%0Aclustered%20twenty-five%20diverse%20datasets%2C%20with%20sequence%20lengths%20ranging%20from%20664%0Abp%20to%20100%20kbp%2C%20including%20mitochondrial%20genomes%20of%20fish%2C%20fungi%2C%20and%20protists%2C%20as%0Awell%20as%20viral%20whole%20genome%20assemblies%20and%20synthetic%20DNA%20sequences.%20Compared%0Awith%20three%20recent%20clustering%20methods%20for%20DNA%20sequences%20%28DeLUCS%2C%20iDeLUCS%2C%20and%0AMeShClust%20v3.0.%29%2C%20CGRclust%20is%20the%20only%20method%20that%20surpasses%2081.70%25%20accuracy%0Aacross%20all%20four%20taxonomic%20levels%20tested%20for%20mitochondrial%20DNA%20genomes%20of%20fish.%0AMoreover%2C%20CGRclust%20also%20consistently%20demonstrates%20superior%20performance%20across%0Aall%20the%20viral%20genomic%20datasets.%20The%20high%20clustering%20accuracy%20of%20CGRclust%20on%0Athese%20twenty-five%20datasets%2C%20which%20vary%20significantly%20in%20terms%20of%20sequence%0Alength%2C%20number%20of%20genomes%2C%20number%20of%20clusters%2C%20and%20level%20of%20taxonomy%2C%0Ademonstrates%20its%20robustness%2C%20scalability%2C%20and%20versatility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02538v2&entry.124074799=Read"},
{"title": "UNSCT-HRNet: Modeling Anatomical Uncertainty for Landmark Detection in\n  Total Hip Arthroplasty", "author": "Jiaxin Wan and Lin Liu and Haoran Wang and Liangwei Li and Wei Li and Shuheng Kou and Runtian Li and Jiayi Tang and Juanxiu Liu and Jing Zhang and Xiaohui Du and Ruqian Hao", "abstract": "  Total hip arthroplasty (THA) relies on accurate landmark detection from\nradiographic images, but unstructured data caused by irregular patient postures\nor occluded anatomical markers pose significant challenges for existing\nmethods. To address this, we propose UNSCT-HRNet (Unstructured CT -\nHigh-Resolution Net), a deep learning-based framework that integrates a Spatial\nRelationship Fusion (SRF) module and an Uncertainty Estimation (UE) module. The\nSRF module, utilizing coordinate convolution and polarized attention, enhances\nthe model's ability to capture complex spatial relationships. Meanwhile, the UE\nmodule which based on entropy ensures predictions are anatomically relevant.\nFor unstructured data, the proposed method can predict landmarks without\nrelying on the fixed number of points, which shows higher accuracy and better\nrobustness comparing with the existing methods. Our UNSCT-HRNet demonstrates\nover a 60% improvement across multiple metrics in unstructured data. The\nexperimental results also reveal that our approach maintains good performance\non the structured dataset. Overall, the proposed UNSCT-HRNet has the potential\nto be used as a new reliable, automated solution for THA surgical planning and\npostoperative monitoring.\n", "link": "http://arxiv.org/abs/2411.08488v1", "date": "2024-11-13", "relevancy": 2.2785, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5763}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.575}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UNSCT-HRNet%3A%20Modeling%20Anatomical%20Uncertainty%20for%20Landmark%20Detection%20in%0A%20%20Total%20Hip%20Arthroplasty&body=Title%3A%20UNSCT-HRNet%3A%20Modeling%20Anatomical%20Uncertainty%20for%20Landmark%20Detection%20in%0A%20%20Total%20Hip%20Arthroplasty%0AAuthor%3A%20Jiaxin%20Wan%20and%20Lin%20Liu%20and%20Haoran%20Wang%20and%20Liangwei%20Li%20and%20Wei%20Li%20and%20Shuheng%20Kou%20and%20Runtian%20Li%20and%20Jiayi%20Tang%20and%20Juanxiu%20Liu%20and%20Jing%20Zhang%20and%20Xiaohui%20Du%20and%20Ruqian%20Hao%0AAbstract%3A%20%20%20Total%20hip%20arthroplasty%20%28THA%29%20relies%20on%20accurate%20landmark%20detection%20from%0Aradiographic%20images%2C%20but%20unstructured%20data%20caused%20by%20irregular%20patient%20postures%0Aor%20occluded%20anatomical%20markers%20pose%20significant%20challenges%20for%20existing%0Amethods.%20To%20address%20this%2C%20we%20propose%20UNSCT-HRNet%20%28Unstructured%20CT%20-%0AHigh-Resolution%20Net%29%2C%20a%20deep%20learning-based%20framework%20that%20integrates%20a%20Spatial%0ARelationship%20Fusion%20%28SRF%29%20module%20and%20an%20Uncertainty%20Estimation%20%28UE%29%20module.%20The%0ASRF%20module%2C%20utilizing%20coordinate%20convolution%20and%20polarized%20attention%2C%20enhances%0Athe%20model%27s%20ability%20to%20capture%20complex%20spatial%20relationships.%20Meanwhile%2C%20the%20UE%0Amodule%20which%20based%20on%20entropy%20ensures%20predictions%20are%20anatomically%20relevant.%0AFor%20unstructured%20data%2C%20the%20proposed%20method%20can%20predict%20landmarks%20without%0Arelying%20on%20the%20fixed%20number%20of%20points%2C%20which%20shows%20higher%20accuracy%20and%20better%0Arobustness%20comparing%20with%20the%20existing%20methods.%20Our%20UNSCT-HRNet%20demonstrates%0Aover%20a%2060%25%20improvement%20across%20multiple%20metrics%20in%20unstructured%20data.%20The%0Aexperimental%20results%20also%20reveal%20that%20our%20approach%20maintains%20good%20performance%0Aon%20the%20structured%20dataset.%20Overall%2C%20the%20proposed%20UNSCT-HRNet%20has%20the%20potential%0Ato%20be%20used%20as%20a%20new%20reliable%2C%20automated%20solution%20for%20THA%20surgical%20planning%20and%0Apostoperative%20monitoring.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08488v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUNSCT-HRNet%253A%2520Modeling%2520Anatomical%2520Uncertainty%2520for%2520Landmark%2520Detection%2520in%250A%2520%2520Total%2520Hip%2520Arthroplasty%26entry.906535625%3DJiaxin%2520Wan%2520and%2520Lin%2520Liu%2520and%2520Haoran%2520Wang%2520and%2520Liangwei%2520Li%2520and%2520Wei%2520Li%2520and%2520Shuheng%2520Kou%2520and%2520Runtian%2520Li%2520and%2520Jiayi%2520Tang%2520and%2520Juanxiu%2520Liu%2520and%2520Jing%2520Zhang%2520and%2520Xiaohui%2520Du%2520and%2520Ruqian%2520Hao%26entry.1292438233%3D%2520%2520Total%2520hip%2520arthroplasty%2520%2528THA%2529%2520relies%2520on%2520accurate%2520landmark%2520detection%2520from%250Aradiographic%2520images%252C%2520but%2520unstructured%2520data%2520caused%2520by%2520irregular%2520patient%2520postures%250Aor%2520occluded%2520anatomical%2520markers%2520pose%2520significant%2520challenges%2520for%2520existing%250Amethods.%2520To%2520address%2520this%252C%2520we%2520propose%2520UNSCT-HRNet%2520%2528Unstructured%2520CT%2520-%250AHigh-Resolution%2520Net%2529%252C%2520a%2520deep%2520learning-based%2520framework%2520that%2520integrates%2520a%2520Spatial%250ARelationship%2520Fusion%2520%2528SRF%2529%2520module%2520and%2520an%2520Uncertainty%2520Estimation%2520%2528UE%2529%2520module.%2520The%250ASRF%2520module%252C%2520utilizing%2520coordinate%2520convolution%2520and%2520polarized%2520attention%252C%2520enhances%250Athe%2520model%2527s%2520ability%2520to%2520capture%2520complex%2520spatial%2520relationships.%2520Meanwhile%252C%2520the%2520UE%250Amodule%2520which%2520based%2520on%2520entropy%2520ensures%2520predictions%2520are%2520anatomically%2520relevant.%250AFor%2520unstructured%2520data%252C%2520the%2520proposed%2520method%2520can%2520predict%2520landmarks%2520without%250Arelying%2520on%2520the%2520fixed%2520number%2520of%2520points%252C%2520which%2520shows%2520higher%2520accuracy%2520and%2520better%250Arobustness%2520comparing%2520with%2520the%2520existing%2520methods.%2520Our%2520UNSCT-HRNet%2520demonstrates%250Aover%2520a%252060%2525%2520improvement%2520across%2520multiple%2520metrics%2520in%2520unstructured%2520data.%2520The%250Aexperimental%2520results%2520also%2520reveal%2520that%2520our%2520approach%2520maintains%2520good%2520performance%250Aon%2520the%2520structured%2520dataset.%2520Overall%252C%2520the%2520proposed%2520UNSCT-HRNet%2520has%2520the%2520potential%250Ato%2520be%2520used%2520as%2520a%2520new%2520reliable%252C%2520automated%2520solution%2520for%2520THA%2520surgical%2520planning%2520and%250Apostoperative%2520monitoring.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08488v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UNSCT-HRNet%3A%20Modeling%20Anatomical%20Uncertainty%20for%20Landmark%20Detection%20in%0A%20%20Total%20Hip%20Arthroplasty&entry.906535625=Jiaxin%20Wan%20and%20Lin%20Liu%20and%20Haoran%20Wang%20and%20Liangwei%20Li%20and%20Wei%20Li%20and%20Shuheng%20Kou%20and%20Runtian%20Li%20and%20Jiayi%20Tang%20and%20Juanxiu%20Liu%20and%20Jing%20Zhang%20and%20Xiaohui%20Du%20and%20Ruqian%20Hao&entry.1292438233=%20%20Total%20hip%20arthroplasty%20%28THA%29%20relies%20on%20accurate%20landmark%20detection%20from%0Aradiographic%20images%2C%20but%20unstructured%20data%20caused%20by%20irregular%20patient%20postures%0Aor%20occluded%20anatomical%20markers%20pose%20significant%20challenges%20for%20existing%0Amethods.%20To%20address%20this%2C%20we%20propose%20UNSCT-HRNet%20%28Unstructured%20CT%20-%0AHigh-Resolution%20Net%29%2C%20a%20deep%20learning-based%20framework%20that%20integrates%20a%20Spatial%0ARelationship%20Fusion%20%28SRF%29%20module%20and%20an%20Uncertainty%20Estimation%20%28UE%29%20module.%20The%0ASRF%20module%2C%20utilizing%20coordinate%20convolution%20and%20polarized%20attention%2C%20enhances%0Athe%20model%27s%20ability%20to%20capture%20complex%20spatial%20relationships.%20Meanwhile%2C%20the%20UE%0Amodule%20which%20based%20on%20entropy%20ensures%20predictions%20are%20anatomically%20relevant.%0AFor%20unstructured%20data%2C%20the%20proposed%20method%20can%20predict%20landmarks%20without%0Arelying%20on%20the%20fixed%20number%20of%20points%2C%20which%20shows%20higher%20accuracy%20and%20better%0Arobustness%20comparing%20with%20the%20existing%20methods.%20Our%20UNSCT-HRNet%20demonstrates%0Aover%20a%2060%25%20improvement%20across%20multiple%20metrics%20in%20unstructured%20data.%20The%0Aexperimental%20results%20also%20reveal%20that%20our%20approach%20maintains%20good%20performance%0Aon%20the%20structured%20dataset.%20Overall%2C%20the%20proposed%20UNSCT-HRNet%20has%20the%20potential%0Ato%20be%20used%20as%20a%20new%20reliable%2C%20automated%20solution%20for%20THA%20surgical%20planning%20and%0Apostoperative%20monitoring.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08488v1&entry.124074799=Read"},
{"title": "Optimal Oblivious Subspace Embeddings with Near-optimal Sparsity", "author": "Shabarish Chenakkod and Micha\u0142 Derezi\u0144ski and Xiaoyu Dong", "abstract": "  An oblivious subspace embedding is a random $m\\times n$ matrix $\\Pi$ such\nthat, for any $d$-dimensional subspace, with high probability $\\Pi$ preserves\nthe norms of all vectors in that subspace within a $1\\pm\\epsilon$ factor. In\nthis work, we give an oblivious subspace embedding with the optimal dimension\n$m=\\Theta(d/\\epsilon^2)$ that has a near-optimal sparsity of $\\tilde\nO(1/\\epsilon)$ non-zero entries per column of $\\Pi$. This is the first result\nto nearly match the conjecture of Nelson and Nguyen [FOCS 2013] in terms of the\nbest sparsity attainable by an optimal oblivious subspace embedding, improving\non a prior bound of $\\tilde O(1/\\epsilon^6)$ non-zeros per column [Chenakkod et\nal., STOC 2024]. We further extend our approach to the non-oblivious setting,\nproposing a new family of Leverage Score Sparsified embeddings with Independent\nColumns, which yield faster runtimes for matrix approximation and regression\ntasks.\n  In our analysis, we develop a new method which uses a decoupling argument\ntogether with the cumulant method for bounding the edge universality error of\nisotropic random matrices. To achieve near-optimal sparsity, we combine this\ngeneral-purpose approach with new traces inequalities that leverage the\nspecific structure of our subspace embedding construction.\n", "link": "http://arxiv.org/abs/2411.08773v1", "date": "2024-11-13", "relevancy": 2.2779, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4647}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4594}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Oblivious%20Subspace%20Embeddings%20with%20Near-optimal%20Sparsity&body=Title%3A%20Optimal%20Oblivious%20Subspace%20Embeddings%20with%20Near-optimal%20Sparsity%0AAuthor%3A%20Shabarish%20Chenakkod%20and%20Micha%C5%82%20Derezi%C5%84ski%20and%20Xiaoyu%20Dong%0AAbstract%3A%20%20%20An%20oblivious%20subspace%20embedding%20is%20a%20random%20%24m%5Ctimes%20n%24%20matrix%20%24%5CPi%24%20such%0Athat%2C%20for%20any%20%24d%24-dimensional%20subspace%2C%20with%20high%20probability%20%24%5CPi%24%20preserves%0Athe%20norms%20of%20all%20vectors%20in%20that%20subspace%20within%20a%20%241%5Cpm%5Cepsilon%24%20factor.%20In%0Athis%20work%2C%20we%20give%20an%20oblivious%20subspace%20embedding%20with%20the%20optimal%20dimension%0A%24m%3D%5CTheta%28d/%5Cepsilon%5E2%29%24%20that%20has%20a%20near-optimal%20sparsity%20of%20%24%5Ctilde%0AO%281/%5Cepsilon%29%24%20non-zero%20entries%20per%20column%20of%20%24%5CPi%24.%20This%20is%20the%20first%20result%0Ato%20nearly%20match%20the%20conjecture%20of%20Nelson%20and%20Nguyen%20%5BFOCS%202013%5D%20in%20terms%20of%20the%0Abest%20sparsity%20attainable%20by%20an%20optimal%20oblivious%20subspace%20embedding%2C%20improving%0Aon%20a%20prior%20bound%20of%20%24%5Ctilde%20O%281/%5Cepsilon%5E6%29%24%20non-zeros%20per%20column%20%5BChenakkod%20et%0Aal.%2C%20STOC%202024%5D.%20We%20further%20extend%20our%20approach%20to%20the%20non-oblivious%20setting%2C%0Aproposing%20a%20new%20family%20of%20Leverage%20Score%20Sparsified%20embeddings%20with%20Independent%0AColumns%2C%20which%20yield%20faster%20runtimes%20for%20matrix%20approximation%20and%20regression%0Atasks.%0A%20%20In%20our%20analysis%2C%20we%20develop%20a%20new%20method%20which%20uses%20a%20decoupling%20argument%0Atogether%20with%20the%20cumulant%20method%20for%20bounding%20the%20edge%20universality%20error%20of%0Aisotropic%20random%20matrices.%20To%20achieve%20near-optimal%20sparsity%2C%20we%20combine%20this%0Ageneral-purpose%20approach%20with%20new%20traces%20inequalities%20that%20leverage%20the%0Aspecific%20structure%20of%20our%20subspace%20embedding%20construction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08773v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Oblivious%2520Subspace%2520Embeddings%2520with%2520Near-optimal%2520Sparsity%26entry.906535625%3DShabarish%2520Chenakkod%2520and%2520Micha%25C5%2582%2520Derezi%25C5%2584ski%2520and%2520Xiaoyu%2520Dong%26entry.1292438233%3D%2520%2520An%2520oblivious%2520subspace%2520embedding%2520is%2520a%2520random%2520%2524m%255Ctimes%2520n%2524%2520matrix%2520%2524%255CPi%2524%2520such%250Athat%252C%2520for%2520any%2520%2524d%2524-dimensional%2520subspace%252C%2520with%2520high%2520probability%2520%2524%255CPi%2524%2520preserves%250Athe%2520norms%2520of%2520all%2520vectors%2520in%2520that%2520subspace%2520within%2520a%2520%25241%255Cpm%255Cepsilon%2524%2520factor.%2520In%250Athis%2520work%252C%2520we%2520give%2520an%2520oblivious%2520subspace%2520embedding%2520with%2520the%2520optimal%2520dimension%250A%2524m%253D%255CTheta%2528d/%255Cepsilon%255E2%2529%2524%2520that%2520has%2520a%2520near-optimal%2520sparsity%2520of%2520%2524%255Ctilde%250AO%25281/%255Cepsilon%2529%2524%2520non-zero%2520entries%2520per%2520column%2520of%2520%2524%255CPi%2524.%2520This%2520is%2520the%2520first%2520result%250Ato%2520nearly%2520match%2520the%2520conjecture%2520of%2520Nelson%2520and%2520Nguyen%2520%255BFOCS%25202013%255D%2520in%2520terms%2520of%2520the%250Abest%2520sparsity%2520attainable%2520by%2520an%2520optimal%2520oblivious%2520subspace%2520embedding%252C%2520improving%250Aon%2520a%2520prior%2520bound%2520of%2520%2524%255Ctilde%2520O%25281/%255Cepsilon%255E6%2529%2524%2520non-zeros%2520per%2520column%2520%255BChenakkod%2520et%250Aal.%252C%2520STOC%25202024%255D.%2520We%2520further%2520extend%2520our%2520approach%2520to%2520the%2520non-oblivious%2520setting%252C%250Aproposing%2520a%2520new%2520family%2520of%2520Leverage%2520Score%2520Sparsified%2520embeddings%2520with%2520Independent%250AColumns%252C%2520which%2520yield%2520faster%2520runtimes%2520for%2520matrix%2520approximation%2520and%2520regression%250Atasks.%250A%2520%2520In%2520our%2520analysis%252C%2520we%2520develop%2520a%2520new%2520method%2520which%2520uses%2520a%2520decoupling%2520argument%250Atogether%2520with%2520the%2520cumulant%2520method%2520for%2520bounding%2520the%2520edge%2520universality%2520error%2520of%250Aisotropic%2520random%2520matrices.%2520To%2520achieve%2520near-optimal%2520sparsity%252C%2520we%2520combine%2520this%250Ageneral-purpose%2520approach%2520with%2520new%2520traces%2520inequalities%2520that%2520leverage%2520the%250Aspecific%2520structure%2520of%2520our%2520subspace%2520embedding%2520construction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08773v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Oblivious%20Subspace%20Embeddings%20with%20Near-optimal%20Sparsity&entry.906535625=Shabarish%20Chenakkod%20and%20Micha%C5%82%20Derezi%C5%84ski%20and%20Xiaoyu%20Dong&entry.1292438233=%20%20An%20oblivious%20subspace%20embedding%20is%20a%20random%20%24m%5Ctimes%20n%24%20matrix%20%24%5CPi%24%20such%0Athat%2C%20for%20any%20%24d%24-dimensional%20subspace%2C%20with%20high%20probability%20%24%5CPi%24%20preserves%0Athe%20norms%20of%20all%20vectors%20in%20that%20subspace%20within%20a%20%241%5Cpm%5Cepsilon%24%20factor.%20In%0Athis%20work%2C%20we%20give%20an%20oblivious%20subspace%20embedding%20with%20the%20optimal%20dimension%0A%24m%3D%5CTheta%28d/%5Cepsilon%5E2%29%24%20that%20has%20a%20near-optimal%20sparsity%20of%20%24%5Ctilde%0AO%281/%5Cepsilon%29%24%20non-zero%20entries%20per%20column%20of%20%24%5CPi%24.%20This%20is%20the%20first%20result%0Ato%20nearly%20match%20the%20conjecture%20of%20Nelson%20and%20Nguyen%20%5BFOCS%202013%5D%20in%20terms%20of%20the%0Abest%20sparsity%20attainable%20by%20an%20optimal%20oblivious%20subspace%20embedding%2C%20improving%0Aon%20a%20prior%20bound%20of%20%24%5Ctilde%20O%281/%5Cepsilon%5E6%29%24%20non-zeros%20per%20column%20%5BChenakkod%20et%0Aal.%2C%20STOC%202024%5D.%20We%20further%20extend%20our%20approach%20to%20the%20non-oblivious%20setting%2C%0Aproposing%20a%20new%20family%20of%20Leverage%20Score%20Sparsified%20embeddings%20with%20Independent%0AColumns%2C%20which%20yield%20faster%20runtimes%20for%20matrix%20approximation%20and%20regression%0Atasks.%0A%20%20In%20our%20analysis%2C%20we%20develop%20a%20new%20method%20which%20uses%20a%20decoupling%20argument%0Atogether%20with%20the%20cumulant%20method%20for%20bounding%20the%20edge%20universality%20error%20of%0Aisotropic%20random%20matrices.%20To%20achieve%20near-optimal%20sparsity%2C%20we%20combine%20this%0Ageneral-purpose%20approach%20with%20new%20traces%20inequalities%20that%20leverage%20the%0Aspecific%20structure%20of%20our%20subspace%20embedding%20construction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08773v1&entry.124074799=Read"},
{"title": "A Short Note on Evaluating RepNet for Temporal Repetition Counting in\n  Videos", "author": "Debidatta Dwibedi and Yusuf Aytar and Jonathan Tompson and Pierre Sermanet and Andrew Zisserman", "abstract": "  We discuss some consistent issues on how RepNet has been evaluated in various\npapers. As a way to mitigate these issues, we report RepNet performance results\non different datasets, and release evaluation code and the RepNet checkpoint to\nobtain these results. Code URL:\nhttps://github.com/google-research/google-research/blob/master/repnet/\n", "link": "http://arxiv.org/abs/2411.08878v1", "date": "2024-11-13", "relevancy": 2.2746, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4595}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4579}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Short%20Note%20on%20Evaluating%20RepNet%20for%20Temporal%20Repetition%20Counting%20in%0A%20%20Videos&body=Title%3A%20A%20Short%20Note%20on%20Evaluating%20RepNet%20for%20Temporal%20Repetition%20Counting%20in%0A%20%20Videos%0AAuthor%3A%20Debidatta%20Dwibedi%20and%20Yusuf%20Aytar%20and%20Jonathan%20Tompson%20and%20Pierre%20Sermanet%20and%20Andrew%20Zisserman%0AAbstract%3A%20%20%20We%20discuss%20some%20consistent%20issues%20on%20how%20RepNet%20has%20been%20evaluated%20in%20various%0Apapers.%20As%20a%20way%20to%20mitigate%20these%20issues%2C%20we%20report%20RepNet%20performance%20results%0Aon%20different%20datasets%2C%20and%20release%20evaluation%20code%20and%20the%20RepNet%20checkpoint%20to%0Aobtain%20these%20results.%20Code%20URL%3A%0Ahttps%3A//github.com/google-research/google-research/blob/master/repnet/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08878v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Short%2520Note%2520on%2520Evaluating%2520RepNet%2520for%2520Temporal%2520Repetition%2520Counting%2520in%250A%2520%2520Videos%26entry.906535625%3DDebidatta%2520Dwibedi%2520and%2520Yusuf%2520Aytar%2520and%2520Jonathan%2520Tompson%2520and%2520Pierre%2520Sermanet%2520and%2520Andrew%2520Zisserman%26entry.1292438233%3D%2520%2520We%2520discuss%2520some%2520consistent%2520issues%2520on%2520how%2520RepNet%2520has%2520been%2520evaluated%2520in%2520various%250Apapers.%2520As%2520a%2520way%2520to%2520mitigate%2520these%2520issues%252C%2520we%2520report%2520RepNet%2520performance%2520results%250Aon%2520different%2520datasets%252C%2520and%2520release%2520evaluation%2520code%2520and%2520the%2520RepNet%2520checkpoint%2520to%250Aobtain%2520these%2520results.%2520Code%2520URL%253A%250Ahttps%253A//github.com/google-research/google-research/blob/master/repnet/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08878v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Short%20Note%20on%20Evaluating%20RepNet%20for%20Temporal%20Repetition%20Counting%20in%0A%20%20Videos&entry.906535625=Debidatta%20Dwibedi%20and%20Yusuf%20Aytar%20and%20Jonathan%20Tompson%20and%20Pierre%20Sermanet%20and%20Andrew%20Zisserman&entry.1292438233=%20%20We%20discuss%20some%20consistent%20issues%20on%20how%20RepNet%20has%20been%20evaluated%20in%20various%0Apapers.%20As%20a%20way%20to%20mitigate%20these%20issues%2C%20we%20report%20RepNet%20performance%20results%0Aon%20different%20datasets%2C%20and%20release%20evaluation%20code%20and%20the%20RepNet%20checkpoint%20to%0Aobtain%20these%20results.%20Code%20URL%3A%0Ahttps%3A//github.com/google-research/google-research/blob/master/repnet/%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08878v1&entry.124074799=Read"},
{"title": "Towards More Accurate Fake Detection on Images Generated from Advanced\n  Generative and Neural Rendering Models", "author": "Chengdong Dong and Vijayakumar Bhagavatula and Zhenyu Zhou and Ajay Kumar", "abstract": "  The remarkable progress in neural-network-driven visual data generation,\nespecially with neural rendering techniques like Neural Radiance Fields and 3D\nGaussian splatting, offers a powerful alternative to GANs and diffusion models.\nThese methods can produce high-fidelity images and lifelike avatars,\nhighlighting the need for robust detection methods. In response, an\nunsupervised training technique is proposed that enables the model to extract\ncomprehensive features from the Fourier spectrum magnitude, thereby overcoming\nthe challenges of reconstructing the spectrum due to its centrosymmetric\nproperties. By leveraging the spectral domain and dynamically combining it with\nspatial domain information, we create a robust multimodal detector that\ndemonstrates superior generalization capabilities in identifying challenging\nsynthetic images generated by the latest image synthesis techniques. To address\nthe absence of a 3D neural rendering-based fake image database, we develop a\ncomprehensive database that includes images generated by diverse neural\nrendering techniques, providing a robust foundation for evaluating and\nadvancing detection methods.\n", "link": "http://arxiv.org/abs/2411.08642v1", "date": "2024-11-13", "relevancy": 2.2575, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5688}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5687}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5582}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20More%20Accurate%20Fake%20Detection%20on%20Images%20Generated%20from%20Advanced%0A%20%20Generative%20and%20Neural%20Rendering%20Models&body=Title%3A%20Towards%20More%20Accurate%20Fake%20Detection%20on%20Images%20Generated%20from%20Advanced%0A%20%20Generative%20and%20Neural%20Rendering%20Models%0AAuthor%3A%20Chengdong%20Dong%20and%20Vijayakumar%20Bhagavatula%20and%20Zhenyu%20Zhou%20and%20Ajay%20Kumar%0AAbstract%3A%20%20%20The%20remarkable%20progress%20in%20neural-network-driven%20visual%20data%20generation%2C%0Aespecially%20with%20neural%20rendering%20techniques%20like%20Neural%20Radiance%20Fields%20and%203D%0AGaussian%20splatting%2C%20offers%20a%20powerful%20alternative%20to%20GANs%20and%20diffusion%20models.%0AThese%20methods%20can%20produce%20high-fidelity%20images%20and%20lifelike%20avatars%2C%0Ahighlighting%20the%20need%20for%20robust%20detection%20methods.%20In%20response%2C%20an%0Aunsupervised%20training%20technique%20is%20proposed%20that%20enables%20the%20model%20to%20extract%0Acomprehensive%20features%20from%20the%20Fourier%20spectrum%20magnitude%2C%20thereby%20overcoming%0Athe%20challenges%20of%20reconstructing%20the%20spectrum%20due%20to%20its%20centrosymmetric%0Aproperties.%20By%20leveraging%20the%20spectral%20domain%20and%20dynamically%20combining%20it%20with%0Aspatial%20domain%20information%2C%20we%20create%20a%20robust%20multimodal%20detector%20that%0Ademonstrates%20superior%20generalization%20capabilities%20in%20identifying%20challenging%0Asynthetic%20images%20generated%20by%20the%20latest%20image%20synthesis%20techniques.%20To%20address%0Athe%20absence%20of%20a%203D%20neural%20rendering-based%20fake%20image%20database%2C%20we%20develop%20a%0Acomprehensive%20database%20that%20includes%20images%20generated%20by%20diverse%20neural%0Arendering%20techniques%2C%20providing%20a%20robust%20foundation%20for%20evaluating%20and%0Aadvancing%20detection%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08642v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520More%2520Accurate%2520Fake%2520Detection%2520on%2520Images%2520Generated%2520from%2520Advanced%250A%2520%2520Generative%2520and%2520Neural%2520Rendering%2520Models%26entry.906535625%3DChengdong%2520Dong%2520and%2520Vijayakumar%2520Bhagavatula%2520and%2520Zhenyu%2520Zhou%2520and%2520Ajay%2520Kumar%26entry.1292438233%3D%2520%2520The%2520remarkable%2520progress%2520in%2520neural-network-driven%2520visual%2520data%2520generation%252C%250Aespecially%2520with%2520neural%2520rendering%2520techniques%2520like%2520Neural%2520Radiance%2520Fields%2520and%25203D%250AGaussian%2520splatting%252C%2520offers%2520a%2520powerful%2520alternative%2520to%2520GANs%2520and%2520diffusion%2520models.%250AThese%2520methods%2520can%2520produce%2520high-fidelity%2520images%2520and%2520lifelike%2520avatars%252C%250Ahighlighting%2520the%2520need%2520for%2520robust%2520detection%2520methods.%2520In%2520response%252C%2520an%250Aunsupervised%2520training%2520technique%2520is%2520proposed%2520that%2520enables%2520the%2520model%2520to%2520extract%250Acomprehensive%2520features%2520from%2520the%2520Fourier%2520spectrum%2520magnitude%252C%2520thereby%2520overcoming%250Athe%2520challenges%2520of%2520reconstructing%2520the%2520spectrum%2520due%2520to%2520its%2520centrosymmetric%250Aproperties.%2520By%2520leveraging%2520the%2520spectral%2520domain%2520and%2520dynamically%2520combining%2520it%2520with%250Aspatial%2520domain%2520information%252C%2520we%2520create%2520a%2520robust%2520multimodal%2520detector%2520that%250Ademonstrates%2520superior%2520generalization%2520capabilities%2520in%2520identifying%2520challenging%250Asynthetic%2520images%2520generated%2520by%2520the%2520latest%2520image%2520synthesis%2520techniques.%2520To%2520address%250Athe%2520absence%2520of%2520a%25203D%2520neural%2520rendering-based%2520fake%2520image%2520database%252C%2520we%2520develop%2520a%250Acomprehensive%2520database%2520that%2520includes%2520images%2520generated%2520by%2520diverse%2520neural%250Arendering%2520techniques%252C%2520providing%2520a%2520robust%2520foundation%2520for%2520evaluating%2520and%250Aadvancing%2520detection%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08642v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20More%20Accurate%20Fake%20Detection%20on%20Images%20Generated%20from%20Advanced%0A%20%20Generative%20and%20Neural%20Rendering%20Models&entry.906535625=Chengdong%20Dong%20and%20Vijayakumar%20Bhagavatula%20and%20Zhenyu%20Zhou%20and%20Ajay%20Kumar&entry.1292438233=%20%20The%20remarkable%20progress%20in%20neural-network-driven%20visual%20data%20generation%2C%0Aespecially%20with%20neural%20rendering%20techniques%20like%20Neural%20Radiance%20Fields%20and%203D%0AGaussian%20splatting%2C%20offers%20a%20powerful%20alternative%20to%20GANs%20and%20diffusion%20models.%0AThese%20methods%20can%20produce%20high-fidelity%20images%20and%20lifelike%20avatars%2C%0Ahighlighting%20the%20need%20for%20robust%20detection%20methods.%20In%20response%2C%20an%0Aunsupervised%20training%20technique%20is%20proposed%20that%20enables%20the%20model%20to%20extract%0Acomprehensive%20features%20from%20the%20Fourier%20spectrum%20magnitude%2C%20thereby%20overcoming%0Athe%20challenges%20of%20reconstructing%20the%20spectrum%20due%20to%20its%20centrosymmetric%0Aproperties.%20By%20leveraging%20the%20spectral%20domain%20and%20dynamically%20combining%20it%20with%0Aspatial%20domain%20information%2C%20we%20create%20a%20robust%20multimodal%20detector%20that%0Ademonstrates%20superior%20generalization%20capabilities%20in%20identifying%20challenging%0Asynthetic%20images%20generated%20by%20the%20latest%20image%20synthesis%20techniques.%20To%20address%0Athe%20absence%20of%20a%203D%20neural%20rendering-based%20fake%20image%20database%2C%20we%20develop%20a%0Acomprehensive%20database%20that%20includes%20images%20generated%20by%20diverse%20neural%0Arendering%20techniques%2C%20providing%20a%20robust%20foundation%20for%20evaluating%20and%0Aadvancing%20detection%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08642v1&entry.124074799=Read"},
{"title": "LG-Gaze: Learning Geometry-aware Continuous Prompts for Language-Guided\n  Gaze Estimation", "author": "Pengwei Yin and Jingjing Wang and Guanzhong Zeng and Di Xie and Jiang Zhu", "abstract": "  The ability of gaze estimation models to generalize is often significantly\nhindered by various factors unrelated to gaze, especially when the training\ndataset is limited. Current strategies aim to address this challenge through\ndifferent domain generalization techniques, yet they have had limited success\ndue to the risk of overfitting when solely relying on value labels for\nregression. Recent progress in pre-trained vision-language models has motivated\nus to capitalize on the abundant semantic information available. We propose a\nnovel approach in this paper, reframing the gaze estimation task as a\nvision-language alignment issue. Our proposed framework, named Language-Guided\nGaze Estimation (LG-Gaze), learns continuous and geometry-sensitive features\nfor gaze estimation benefit from the rich prior knowledges of vision-language\nmodels. Specifically, LG-Gaze aligns gaze features with continuous linguistic\nfeatures through our proposed multimodal contrastive regression loss, which\ncustomizes adaptive weights for different negative samples. Furthermore, to\nbetter adapt to the labels for gaze estimation task, we propose a\ngeometry-aware interpolation method to obtain more precise gaze embeddings.\nThrough extensive experiments, we validate the efficacy of our framework in\nfour different cross-domain evaluation tasks.\n", "link": "http://arxiv.org/abs/2411.08606v1", "date": "2024-11-13", "relevancy": 2.2428, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5634}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5611}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5529}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LG-Gaze%3A%20Learning%20Geometry-aware%20Continuous%20Prompts%20for%20Language-Guided%0A%20%20Gaze%20Estimation&body=Title%3A%20LG-Gaze%3A%20Learning%20Geometry-aware%20Continuous%20Prompts%20for%20Language-Guided%0A%20%20Gaze%20Estimation%0AAuthor%3A%20Pengwei%20Yin%20and%20Jingjing%20Wang%20and%20Guanzhong%20Zeng%20and%20Di%20Xie%20and%20Jiang%20Zhu%0AAbstract%3A%20%20%20The%20ability%20of%20gaze%20estimation%20models%20to%20generalize%20is%20often%20significantly%0Ahindered%20by%20various%20factors%20unrelated%20to%20gaze%2C%20especially%20when%20the%20training%0Adataset%20is%20limited.%20Current%20strategies%20aim%20to%20address%20this%20challenge%20through%0Adifferent%20domain%20generalization%20techniques%2C%20yet%20they%20have%20had%20limited%20success%0Adue%20to%20the%20risk%20of%20overfitting%20when%20solely%20relying%20on%20value%20labels%20for%0Aregression.%20Recent%20progress%20in%20pre-trained%20vision-language%20models%20has%20motivated%0Aus%20to%20capitalize%20on%20the%20abundant%20semantic%20information%20available.%20We%20propose%20a%0Anovel%20approach%20in%20this%20paper%2C%20reframing%20the%20gaze%20estimation%20task%20as%20a%0Avision-language%20alignment%20issue.%20Our%20proposed%20framework%2C%20named%20Language-Guided%0AGaze%20Estimation%20%28LG-Gaze%29%2C%20learns%20continuous%20and%20geometry-sensitive%20features%0Afor%20gaze%20estimation%20benefit%20from%20the%20rich%20prior%20knowledges%20of%20vision-language%0Amodels.%20Specifically%2C%20LG-Gaze%20aligns%20gaze%20features%20with%20continuous%20linguistic%0Afeatures%20through%20our%20proposed%20multimodal%20contrastive%20regression%20loss%2C%20which%0Acustomizes%20adaptive%20weights%20for%20different%20negative%20samples.%20Furthermore%2C%20to%0Abetter%20adapt%20to%20the%20labels%20for%20gaze%20estimation%20task%2C%20we%20propose%20a%0Ageometry-aware%20interpolation%20method%20to%20obtain%20more%20precise%20gaze%20embeddings.%0AThrough%20extensive%20experiments%2C%20we%20validate%20the%20efficacy%20of%20our%20framework%20in%0Afour%20different%20cross-domain%20evaluation%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08606v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLG-Gaze%253A%2520Learning%2520Geometry-aware%2520Continuous%2520Prompts%2520for%2520Language-Guided%250A%2520%2520Gaze%2520Estimation%26entry.906535625%3DPengwei%2520Yin%2520and%2520Jingjing%2520Wang%2520and%2520Guanzhong%2520Zeng%2520and%2520Di%2520Xie%2520and%2520Jiang%2520Zhu%26entry.1292438233%3D%2520%2520The%2520ability%2520of%2520gaze%2520estimation%2520models%2520to%2520generalize%2520is%2520often%2520significantly%250Ahindered%2520by%2520various%2520factors%2520unrelated%2520to%2520gaze%252C%2520especially%2520when%2520the%2520training%250Adataset%2520is%2520limited.%2520Current%2520strategies%2520aim%2520to%2520address%2520this%2520challenge%2520through%250Adifferent%2520domain%2520generalization%2520techniques%252C%2520yet%2520they%2520have%2520had%2520limited%2520success%250Adue%2520to%2520the%2520risk%2520of%2520overfitting%2520when%2520solely%2520relying%2520on%2520value%2520labels%2520for%250Aregression.%2520Recent%2520progress%2520in%2520pre-trained%2520vision-language%2520models%2520has%2520motivated%250Aus%2520to%2520capitalize%2520on%2520the%2520abundant%2520semantic%2520information%2520available.%2520We%2520propose%2520a%250Anovel%2520approach%2520in%2520this%2520paper%252C%2520reframing%2520the%2520gaze%2520estimation%2520task%2520as%2520a%250Avision-language%2520alignment%2520issue.%2520Our%2520proposed%2520framework%252C%2520named%2520Language-Guided%250AGaze%2520Estimation%2520%2528LG-Gaze%2529%252C%2520learns%2520continuous%2520and%2520geometry-sensitive%2520features%250Afor%2520gaze%2520estimation%2520benefit%2520from%2520the%2520rich%2520prior%2520knowledges%2520of%2520vision-language%250Amodels.%2520Specifically%252C%2520LG-Gaze%2520aligns%2520gaze%2520features%2520with%2520continuous%2520linguistic%250Afeatures%2520through%2520our%2520proposed%2520multimodal%2520contrastive%2520regression%2520loss%252C%2520which%250Acustomizes%2520adaptive%2520weights%2520for%2520different%2520negative%2520samples.%2520Furthermore%252C%2520to%250Abetter%2520adapt%2520to%2520the%2520labels%2520for%2520gaze%2520estimation%2520task%252C%2520we%2520propose%2520a%250Ageometry-aware%2520interpolation%2520method%2520to%2520obtain%2520more%2520precise%2520gaze%2520embeddings.%250AThrough%2520extensive%2520experiments%252C%2520we%2520validate%2520the%2520efficacy%2520of%2520our%2520framework%2520in%250Afour%2520different%2520cross-domain%2520evaluation%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08606v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LG-Gaze%3A%20Learning%20Geometry-aware%20Continuous%20Prompts%20for%20Language-Guided%0A%20%20Gaze%20Estimation&entry.906535625=Pengwei%20Yin%20and%20Jingjing%20Wang%20and%20Guanzhong%20Zeng%20and%20Di%20Xie%20and%20Jiang%20Zhu&entry.1292438233=%20%20The%20ability%20of%20gaze%20estimation%20models%20to%20generalize%20is%20often%20significantly%0Ahindered%20by%20various%20factors%20unrelated%20to%20gaze%2C%20especially%20when%20the%20training%0Adataset%20is%20limited.%20Current%20strategies%20aim%20to%20address%20this%20challenge%20through%0Adifferent%20domain%20generalization%20techniques%2C%20yet%20they%20have%20had%20limited%20success%0Adue%20to%20the%20risk%20of%20overfitting%20when%20solely%20relying%20on%20value%20labels%20for%0Aregression.%20Recent%20progress%20in%20pre-trained%20vision-language%20models%20has%20motivated%0Aus%20to%20capitalize%20on%20the%20abundant%20semantic%20information%20available.%20We%20propose%20a%0Anovel%20approach%20in%20this%20paper%2C%20reframing%20the%20gaze%20estimation%20task%20as%20a%0Avision-language%20alignment%20issue.%20Our%20proposed%20framework%2C%20named%20Language-Guided%0AGaze%20Estimation%20%28LG-Gaze%29%2C%20learns%20continuous%20and%20geometry-sensitive%20features%0Afor%20gaze%20estimation%20benefit%20from%20the%20rich%20prior%20knowledges%20of%20vision-language%0Amodels.%20Specifically%2C%20LG-Gaze%20aligns%20gaze%20features%20with%20continuous%20linguistic%0Afeatures%20through%20our%20proposed%20multimodal%20contrastive%20regression%20loss%2C%20which%0Acustomizes%20adaptive%20weights%20for%20different%20negative%20samples.%20Furthermore%2C%20to%0Abetter%20adapt%20to%20the%20labels%20for%20gaze%20estimation%20task%2C%20we%20propose%20a%0Ageometry-aware%20interpolation%20method%20to%20obtain%20more%20precise%20gaze%20embeddings.%0AThrough%20extensive%20experiments%2C%20we%20validate%20the%20efficacy%20of%20our%20framework%20in%0Afour%20different%20cross-domain%20evaluation%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08606v1&entry.124074799=Read"},
{"title": "Masked Image Modeling Boosting Semi-Supervised Semantic Segmentation", "author": "Yangyang Li and Xuanting Hao and Ronghua Shang and Licheng Jiao", "abstract": "  In view of the fact that semi- and self-supervised learning share a\nfundamental principle, effectively modeling knowledge from unlabeled data,\nvarious semi-supervised semantic segmentation methods have integrated\nrepresentative self-supervised learning paradigms for further regularization.\nHowever, the potential of the state-of-the-art generative self-supervised\nparadigm, masked image modeling, has been scarcely studied. This paradigm\nlearns the knowledge through establishing connections between the masked and\nvisible parts of masked image, during the pixel reconstruction process. By\ninheriting and extending this insight, we successfully leverage masked image\nmodeling to boost semi-supervised semantic segmentation. Specifically, we\nintroduce a novel class-wise masked image modeling that independently\nreconstructs different image regions according to their respective classes. In\nthis way, the mask-induced connections are established within each class,\nmitigating the semantic confusion that arises from plainly reconstructing\nimages in basic masked image modeling. To strengthen these intra-class\nconnections, we further develop a feature aggregation strategy that minimizes\nthe distances between features corresponding to the masked and visible parts\nwithin the same class. Additionally, in semantic space, we explore the\napplication of masked image modeling to enhance regularization. Extensive\nexperiments conducted on well-known benchmarks demonstrate that our approach\nachieves state-of-the-art performance. The code will be available at\nhttps://github.com/haoxt/S4MIM.\n", "link": "http://arxiv.org/abs/2411.08756v1", "date": "2024-11-13", "relevancy": 2.2337, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5637}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5604}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Masked%20Image%20Modeling%20Boosting%20Semi-Supervised%20Semantic%20Segmentation&body=Title%3A%20Masked%20Image%20Modeling%20Boosting%20Semi-Supervised%20Semantic%20Segmentation%0AAuthor%3A%20Yangyang%20Li%20and%20Xuanting%20Hao%20and%20Ronghua%20Shang%20and%20Licheng%20Jiao%0AAbstract%3A%20%20%20In%20view%20of%20the%20fact%20that%20semi-%20and%20self-supervised%20learning%20share%20a%0Afundamental%20principle%2C%20effectively%20modeling%20knowledge%20from%20unlabeled%20data%2C%0Avarious%20semi-supervised%20semantic%20segmentation%20methods%20have%20integrated%0Arepresentative%20self-supervised%20learning%20paradigms%20for%20further%20regularization.%0AHowever%2C%20the%20potential%20of%20the%20state-of-the-art%20generative%20self-supervised%0Aparadigm%2C%20masked%20image%20modeling%2C%20has%20been%20scarcely%20studied.%20This%20paradigm%0Alearns%20the%20knowledge%20through%20establishing%20connections%20between%20the%20masked%20and%0Avisible%20parts%20of%20masked%20image%2C%20during%20the%20pixel%20reconstruction%20process.%20By%0Ainheriting%20and%20extending%20this%20insight%2C%20we%20successfully%20leverage%20masked%20image%0Amodeling%20to%20boost%20semi-supervised%20semantic%20segmentation.%20Specifically%2C%20we%0Aintroduce%20a%20novel%20class-wise%20masked%20image%20modeling%20that%20independently%0Areconstructs%20different%20image%20regions%20according%20to%20their%20respective%20classes.%20In%0Athis%20way%2C%20the%20mask-induced%20connections%20are%20established%20within%20each%20class%2C%0Amitigating%20the%20semantic%20confusion%20that%20arises%20from%20plainly%20reconstructing%0Aimages%20in%20basic%20masked%20image%20modeling.%20To%20strengthen%20these%20intra-class%0Aconnections%2C%20we%20further%20develop%20a%20feature%20aggregation%20strategy%20that%20minimizes%0Athe%20distances%20between%20features%20corresponding%20to%20the%20masked%20and%20visible%20parts%0Awithin%20the%20same%20class.%20Additionally%2C%20in%20semantic%20space%2C%20we%20explore%20the%0Aapplication%20of%20masked%20image%20modeling%20to%20enhance%20regularization.%20Extensive%0Aexperiments%20conducted%20on%20well-known%20benchmarks%20demonstrate%20that%20our%20approach%0Aachieves%20state-of-the-art%20performance.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/haoxt/S4MIM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08756v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMasked%2520Image%2520Modeling%2520Boosting%2520Semi-Supervised%2520Semantic%2520Segmentation%26entry.906535625%3DYangyang%2520Li%2520and%2520Xuanting%2520Hao%2520and%2520Ronghua%2520Shang%2520and%2520Licheng%2520Jiao%26entry.1292438233%3D%2520%2520In%2520view%2520of%2520the%2520fact%2520that%2520semi-%2520and%2520self-supervised%2520learning%2520share%2520a%250Afundamental%2520principle%252C%2520effectively%2520modeling%2520knowledge%2520from%2520unlabeled%2520data%252C%250Avarious%2520semi-supervised%2520semantic%2520segmentation%2520methods%2520have%2520integrated%250Arepresentative%2520self-supervised%2520learning%2520paradigms%2520for%2520further%2520regularization.%250AHowever%252C%2520the%2520potential%2520of%2520the%2520state-of-the-art%2520generative%2520self-supervised%250Aparadigm%252C%2520masked%2520image%2520modeling%252C%2520has%2520been%2520scarcely%2520studied.%2520This%2520paradigm%250Alearns%2520the%2520knowledge%2520through%2520establishing%2520connections%2520between%2520the%2520masked%2520and%250Avisible%2520parts%2520of%2520masked%2520image%252C%2520during%2520the%2520pixel%2520reconstruction%2520process.%2520By%250Ainheriting%2520and%2520extending%2520this%2520insight%252C%2520we%2520successfully%2520leverage%2520masked%2520image%250Amodeling%2520to%2520boost%2520semi-supervised%2520semantic%2520segmentation.%2520Specifically%252C%2520we%250Aintroduce%2520a%2520novel%2520class-wise%2520masked%2520image%2520modeling%2520that%2520independently%250Areconstructs%2520different%2520image%2520regions%2520according%2520to%2520their%2520respective%2520classes.%2520In%250Athis%2520way%252C%2520the%2520mask-induced%2520connections%2520are%2520established%2520within%2520each%2520class%252C%250Amitigating%2520the%2520semantic%2520confusion%2520that%2520arises%2520from%2520plainly%2520reconstructing%250Aimages%2520in%2520basic%2520masked%2520image%2520modeling.%2520To%2520strengthen%2520these%2520intra-class%250Aconnections%252C%2520we%2520further%2520develop%2520a%2520feature%2520aggregation%2520strategy%2520that%2520minimizes%250Athe%2520distances%2520between%2520features%2520corresponding%2520to%2520the%2520masked%2520and%2520visible%2520parts%250Awithin%2520the%2520same%2520class.%2520Additionally%252C%2520in%2520semantic%2520space%252C%2520we%2520explore%2520the%250Aapplication%2520of%2520masked%2520image%2520modeling%2520to%2520enhance%2520regularization.%2520Extensive%250Aexperiments%2520conducted%2520on%2520well-known%2520benchmarks%2520demonstrate%2520that%2520our%2520approach%250Aachieves%2520state-of-the-art%2520performance.%2520The%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/haoxt/S4MIM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08756v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Masked%20Image%20Modeling%20Boosting%20Semi-Supervised%20Semantic%20Segmentation&entry.906535625=Yangyang%20Li%20and%20Xuanting%20Hao%20and%20Ronghua%20Shang%20and%20Licheng%20Jiao&entry.1292438233=%20%20In%20view%20of%20the%20fact%20that%20semi-%20and%20self-supervised%20learning%20share%20a%0Afundamental%20principle%2C%20effectively%20modeling%20knowledge%20from%20unlabeled%20data%2C%0Avarious%20semi-supervised%20semantic%20segmentation%20methods%20have%20integrated%0Arepresentative%20self-supervised%20learning%20paradigms%20for%20further%20regularization.%0AHowever%2C%20the%20potential%20of%20the%20state-of-the-art%20generative%20self-supervised%0Aparadigm%2C%20masked%20image%20modeling%2C%20has%20been%20scarcely%20studied.%20This%20paradigm%0Alearns%20the%20knowledge%20through%20establishing%20connections%20between%20the%20masked%20and%0Avisible%20parts%20of%20masked%20image%2C%20during%20the%20pixel%20reconstruction%20process.%20By%0Ainheriting%20and%20extending%20this%20insight%2C%20we%20successfully%20leverage%20masked%20image%0Amodeling%20to%20boost%20semi-supervised%20semantic%20segmentation.%20Specifically%2C%20we%0Aintroduce%20a%20novel%20class-wise%20masked%20image%20modeling%20that%20independently%0Areconstructs%20different%20image%20regions%20according%20to%20their%20respective%20classes.%20In%0Athis%20way%2C%20the%20mask-induced%20connections%20are%20established%20within%20each%20class%2C%0Amitigating%20the%20semantic%20confusion%20that%20arises%20from%20plainly%20reconstructing%0Aimages%20in%20basic%20masked%20image%20modeling.%20To%20strengthen%20these%20intra-class%0Aconnections%2C%20we%20further%20develop%20a%20feature%20aggregation%20strategy%20that%20minimizes%0Athe%20distances%20between%20features%20corresponding%20to%20the%20masked%20and%20visible%20parts%0Awithin%20the%20same%20class.%20Additionally%2C%20in%20semantic%20space%2C%20we%20explore%20the%0Aapplication%20of%20masked%20image%20modeling%20to%20enhance%20regularization.%20Extensive%0Aexperiments%20conducted%20on%20well-known%20benchmarks%20demonstrate%20that%20our%20approach%0Aachieves%20state-of-the-art%20performance.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/haoxt/S4MIM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08756v1&entry.124074799=Read"},
{"title": "GeSubNet: Gene Interaction Inference for Disease Subtype Network\n  Generation", "author": "Ziwei Yang and Zheng Chen and Xin Liu and Rikuto Kotoge and Peng Chen and Yasuko Matsubara and Yasushi Sakurai and Jimeng Sun", "abstract": "  Retrieving gene functional networks from knowledge databases presents a\nchallenge due to the mismatch between disease networks and subtype-specific\nvariations. Current solutions, including statistical and deep learning methods,\noften fail to effectively integrate gene interaction knowledge from databases\nor explicitly learn subtype-specific interactions. To address this mismatch, we\npropose GeSubNet, which learns a unified representation capable of predicting\ngene interactions while distinguishing between different disease subtypes.\nGraphs generated by such representations can be considered subtype-specific\nnetworks. GeSubNet is a multi-step representation learning framework with three\nmodules: First, a deep generative model learns distinct disease subtypes from\npatient gene expression profiles. Second, a graph neural network captures\nrepresentations of prior gene networks from knowledge databases, ensuring\naccurate physical gene interactions. Finally, we integrate these two\nrepresentations using an inference loss that leverages graph generation\ncapabilities, conditioned on the patient separation loss, to refine\nsubtype-specific information in the learned representation. GeSubNet\nconsistently outperforms traditional methods, with average improvements of\n30.6%, 21.0%, 20.1%, and 56.6% across four graph evaluation metrics, averaged\nover four cancer datasets. Particularly, we conduct a biological simulation\nexperiment to assess how the behavior of selected genes from over 11,000\ncandidates affects subtypes or patient distributions. The results show that the\ngenerated network has the potential to identify subtype-specific genes with an\n83% likelihood of impacting patient distribution shifts. The GeSubNet resource\nis available: https://anonymous.4open.science/r/GeSubNet/\n", "link": "http://arxiv.org/abs/2410.13178v2", "date": "2024-11-13", "relevancy": 2.226, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4641}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4359}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeSubNet%3A%20Gene%20Interaction%20Inference%20for%20Disease%20Subtype%20Network%0A%20%20Generation&body=Title%3A%20GeSubNet%3A%20Gene%20Interaction%20Inference%20for%20Disease%20Subtype%20Network%0A%20%20Generation%0AAuthor%3A%20Ziwei%20Yang%20and%20Zheng%20Chen%20and%20Xin%20Liu%20and%20Rikuto%20Kotoge%20and%20Peng%20Chen%20and%20Yasuko%20Matsubara%20and%20Yasushi%20Sakurai%20and%20Jimeng%20Sun%0AAbstract%3A%20%20%20Retrieving%20gene%20functional%20networks%20from%20knowledge%20databases%20presents%20a%0Achallenge%20due%20to%20the%20mismatch%20between%20disease%20networks%20and%20subtype-specific%0Avariations.%20Current%20solutions%2C%20including%20statistical%20and%20deep%20learning%20methods%2C%0Aoften%20fail%20to%20effectively%20integrate%20gene%20interaction%20knowledge%20from%20databases%0Aor%20explicitly%20learn%20subtype-specific%20interactions.%20To%20address%20this%20mismatch%2C%20we%0Apropose%20GeSubNet%2C%20which%20learns%20a%20unified%20representation%20capable%20of%20predicting%0Agene%20interactions%20while%20distinguishing%20between%20different%20disease%20subtypes.%0AGraphs%20generated%20by%20such%20representations%20can%20be%20considered%20subtype-specific%0Anetworks.%20GeSubNet%20is%20a%20multi-step%20representation%20learning%20framework%20with%20three%0Amodules%3A%20First%2C%20a%20deep%20generative%20model%20learns%20distinct%20disease%20subtypes%20from%0Apatient%20gene%20expression%20profiles.%20Second%2C%20a%20graph%20neural%20network%20captures%0Arepresentations%20of%20prior%20gene%20networks%20from%20knowledge%20databases%2C%20ensuring%0Aaccurate%20physical%20gene%20interactions.%20Finally%2C%20we%20integrate%20these%20two%0Arepresentations%20using%20an%20inference%20loss%20that%20leverages%20graph%20generation%0Acapabilities%2C%20conditioned%20on%20the%20patient%20separation%20loss%2C%20to%20refine%0Asubtype-specific%20information%20in%20the%20learned%20representation.%20GeSubNet%0Aconsistently%20outperforms%20traditional%20methods%2C%20with%20average%20improvements%20of%0A30.6%25%2C%2021.0%25%2C%2020.1%25%2C%20and%2056.6%25%20across%20four%20graph%20evaluation%20metrics%2C%20averaged%0Aover%20four%20cancer%20datasets.%20Particularly%2C%20we%20conduct%20a%20biological%20simulation%0Aexperiment%20to%20assess%20how%20the%20behavior%20of%20selected%20genes%20from%20over%2011%2C000%0Acandidates%20affects%20subtypes%20or%20patient%20distributions.%20The%20results%20show%20that%20the%0Agenerated%20network%20has%20the%20potential%20to%20identify%20subtype-specific%20genes%20with%20an%0A83%25%20likelihood%20of%20impacting%20patient%20distribution%20shifts.%20The%20GeSubNet%20resource%0Ais%20available%3A%20https%3A//anonymous.4open.science/r/GeSubNet/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13178v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeSubNet%253A%2520Gene%2520Interaction%2520Inference%2520for%2520Disease%2520Subtype%2520Network%250A%2520%2520Generation%26entry.906535625%3DZiwei%2520Yang%2520and%2520Zheng%2520Chen%2520and%2520Xin%2520Liu%2520and%2520Rikuto%2520Kotoge%2520and%2520Peng%2520Chen%2520and%2520Yasuko%2520Matsubara%2520and%2520Yasushi%2520Sakurai%2520and%2520Jimeng%2520Sun%26entry.1292438233%3D%2520%2520Retrieving%2520gene%2520functional%2520networks%2520from%2520knowledge%2520databases%2520presents%2520a%250Achallenge%2520due%2520to%2520the%2520mismatch%2520between%2520disease%2520networks%2520and%2520subtype-specific%250Avariations.%2520Current%2520solutions%252C%2520including%2520statistical%2520and%2520deep%2520learning%2520methods%252C%250Aoften%2520fail%2520to%2520effectively%2520integrate%2520gene%2520interaction%2520knowledge%2520from%2520databases%250Aor%2520explicitly%2520learn%2520subtype-specific%2520interactions.%2520To%2520address%2520this%2520mismatch%252C%2520we%250Apropose%2520GeSubNet%252C%2520which%2520learns%2520a%2520unified%2520representation%2520capable%2520of%2520predicting%250Agene%2520interactions%2520while%2520distinguishing%2520between%2520different%2520disease%2520subtypes.%250AGraphs%2520generated%2520by%2520such%2520representations%2520can%2520be%2520considered%2520subtype-specific%250Anetworks.%2520GeSubNet%2520is%2520a%2520multi-step%2520representation%2520learning%2520framework%2520with%2520three%250Amodules%253A%2520First%252C%2520a%2520deep%2520generative%2520model%2520learns%2520distinct%2520disease%2520subtypes%2520from%250Apatient%2520gene%2520expression%2520profiles.%2520Second%252C%2520a%2520graph%2520neural%2520network%2520captures%250Arepresentations%2520of%2520prior%2520gene%2520networks%2520from%2520knowledge%2520databases%252C%2520ensuring%250Aaccurate%2520physical%2520gene%2520interactions.%2520Finally%252C%2520we%2520integrate%2520these%2520two%250Arepresentations%2520using%2520an%2520inference%2520loss%2520that%2520leverages%2520graph%2520generation%250Acapabilities%252C%2520conditioned%2520on%2520the%2520patient%2520separation%2520loss%252C%2520to%2520refine%250Asubtype-specific%2520information%2520in%2520the%2520learned%2520representation.%2520GeSubNet%250Aconsistently%2520outperforms%2520traditional%2520methods%252C%2520with%2520average%2520improvements%2520of%250A30.6%2525%252C%252021.0%2525%252C%252020.1%2525%252C%2520and%252056.6%2525%2520across%2520four%2520graph%2520evaluation%2520metrics%252C%2520averaged%250Aover%2520four%2520cancer%2520datasets.%2520Particularly%252C%2520we%2520conduct%2520a%2520biological%2520simulation%250Aexperiment%2520to%2520assess%2520how%2520the%2520behavior%2520of%2520selected%2520genes%2520from%2520over%252011%252C000%250Acandidates%2520affects%2520subtypes%2520or%2520patient%2520distributions.%2520The%2520results%2520show%2520that%2520the%250Agenerated%2520network%2520has%2520the%2520potential%2520to%2520identify%2520subtype-specific%2520genes%2520with%2520an%250A83%2525%2520likelihood%2520of%2520impacting%2520patient%2520distribution%2520shifts.%2520The%2520GeSubNet%2520resource%250Ais%2520available%253A%2520https%253A//anonymous.4open.science/r/GeSubNet/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13178v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeSubNet%3A%20Gene%20Interaction%20Inference%20for%20Disease%20Subtype%20Network%0A%20%20Generation&entry.906535625=Ziwei%20Yang%20and%20Zheng%20Chen%20and%20Xin%20Liu%20and%20Rikuto%20Kotoge%20and%20Peng%20Chen%20and%20Yasuko%20Matsubara%20and%20Yasushi%20Sakurai%20and%20Jimeng%20Sun&entry.1292438233=%20%20Retrieving%20gene%20functional%20networks%20from%20knowledge%20databases%20presents%20a%0Achallenge%20due%20to%20the%20mismatch%20between%20disease%20networks%20and%20subtype-specific%0Avariations.%20Current%20solutions%2C%20including%20statistical%20and%20deep%20learning%20methods%2C%0Aoften%20fail%20to%20effectively%20integrate%20gene%20interaction%20knowledge%20from%20databases%0Aor%20explicitly%20learn%20subtype-specific%20interactions.%20To%20address%20this%20mismatch%2C%20we%0Apropose%20GeSubNet%2C%20which%20learns%20a%20unified%20representation%20capable%20of%20predicting%0Agene%20interactions%20while%20distinguishing%20between%20different%20disease%20subtypes.%0AGraphs%20generated%20by%20such%20representations%20can%20be%20considered%20subtype-specific%0Anetworks.%20GeSubNet%20is%20a%20multi-step%20representation%20learning%20framework%20with%20three%0Amodules%3A%20First%2C%20a%20deep%20generative%20model%20learns%20distinct%20disease%20subtypes%20from%0Apatient%20gene%20expression%20profiles.%20Second%2C%20a%20graph%20neural%20network%20captures%0Arepresentations%20of%20prior%20gene%20networks%20from%20knowledge%20databases%2C%20ensuring%0Aaccurate%20physical%20gene%20interactions.%20Finally%2C%20we%20integrate%20these%20two%0Arepresentations%20using%20an%20inference%20loss%20that%20leverages%20graph%20generation%0Acapabilities%2C%20conditioned%20on%20the%20patient%20separation%20loss%2C%20to%20refine%0Asubtype-specific%20information%20in%20the%20learned%20representation.%20GeSubNet%0Aconsistently%20outperforms%20traditional%20methods%2C%20with%20average%20improvements%20of%0A30.6%25%2C%2021.0%25%2C%2020.1%25%2C%20and%2056.6%25%20across%20four%20graph%20evaluation%20metrics%2C%20averaged%0Aover%20four%20cancer%20datasets.%20Particularly%2C%20we%20conduct%20a%20biological%20simulation%0Aexperiment%20to%20assess%20how%20the%20behavior%20of%20selected%20genes%20from%20over%2011%2C000%0Acandidates%20affects%20subtypes%20or%20patient%20distributions.%20The%20results%20show%20that%20the%0Agenerated%20network%20has%20the%20potential%20to%20identify%20subtype-specific%20genes%20with%20an%0A83%25%20likelihood%20of%20impacting%20patient%20distribution%20shifts.%20The%20GeSubNet%20resource%0Ais%20available%3A%20https%3A//anonymous.4open.science/r/GeSubNet/%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13178v2&entry.124074799=Read"},
{"title": "A Survey on Vision Autoregressive Model", "author": "Kai Jiang and Jiaxing Huang", "abstract": "  Autoregressive models have demonstrated great performance in natural language\nprocessing (NLP) with impressive scalability, adaptability and\ngeneralizability. Inspired by their notable success in NLP field,\nautoregressive models have been intensively investigated recently for computer\nvision, which perform next-token predictions by representing visual data as\nvisual tokens and enables autoregressive modelling for a wide range of vision\ntasks, ranging from visual generation and visual understanding to the very\nrecent multimodal generation that unifies visual generation and understanding\nwith a single autoregressive model. This paper provides a systematic review of\nvision autoregressive models, including the development of a taxonomy of\nexisting methods and highlighting their major contributions, strengths, and\nlimitations, covering various vision tasks such as image generation, video\ngeneration, image editing, motion generation, medical image analysis, 3D\ngeneration, robotic manipulation, unified multimodal generation, etc. Besides,\nwe investigate and analyze the latest advancements in autoregressive models,\nincluding thorough benchmarking and discussion of existing methods across\nvarious evaluation datasets. Finally, we outline key challenges and promising\ndirections for future research, offering a roadmap to guide further\nadvancements in vision autoregressive models.\n", "link": "http://arxiv.org/abs/2411.08666v1", "date": "2024-11-13", "relevancy": 2.2245, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5621}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5621}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Vision%20Autoregressive%20Model&body=Title%3A%20A%20Survey%20on%20Vision%20Autoregressive%20Model%0AAuthor%3A%20Kai%20Jiang%20and%20Jiaxing%20Huang%0AAbstract%3A%20%20%20Autoregressive%20models%20have%20demonstrated%20great%20performance%20in%20natural%20language%0Aprocessing%20%28NLP%29%20with%20impressive%20scalability%2C%20adaptability%20and%0Ageneralizability.%20Inspired%20by%20their%20notable%20success%20in%20NLP%20field%2C%0Aautoregressive%20models%20have%20been%20intensively%20investigated%20recently%20for%20computer%0Avision%2C%20which%20perform%20next-token%20predictions%20by%20representing%20visual%20data%20as%0Avisual%20tokens%20and%20enables%20autoregressive%20modelling%20for%20a%20wide%20range%20of%20vision%0Atasks%2C%20ranging%20from%20visual%20generation%20and%20visual%20understanding%20to%20the%20very%0Arecent%20multimodal%20generation%20that%20unifies%20visual%20generation%20and%20understanding%0Awith%20a%20single%20autoregressive%20model.%20This%20paper%20provides%20a%20systematic%20review%20of%0Avision%20autoregressive%20models%2C%20including%20the%20development%20of%20a%20taxonomy%20of%0Aexisting%20methods%20and%20highlighting%20their%20major%20contributions%2C%20strengths%2C%20and%0Alimitations%2C%20covering%20various%20vision%20tasks%20such%20as%20image%20generation%2C%20video%0Ageneration%2C%20image%20editing%2C%20motion%20generation%2C%20medical%20image%20analysis%2C%203D%0Ageneration%2C%20robotic%20manipulation%2C%20unified%20multimodal%20generation%2C%20etc.%20Besides%2C%0Awe%20investigate%20and%20analyze%20the%20latest%20advancements%20in%20autoregressive%20models%2C%0Aincluding%20thorough%20benchmarking%20and%20discussion%20of%20existing%20methods%20across%0Avarious%20evaluation%20datasets.%20Finally%2C%20we%20outline%20key%20challenges%20and%20promising%0Adirections%20for%20future%20research%2C%20offering%20a%20roadmap%20to%20guide%20further%0Aadvancements%20in%20vision%20autoregressive%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520on%2520Vision%2520Autoregressive%2520Model%26entry.906535625%3DKai%2520Jiang%2520and%2520Jiaxing%2520Huang%26entry.1292438233%3D%2520%2520Autoregressive%2520models%2520have%2520demonstrated%2520great%2520performance%2520in%2520natural%2520language%250Aprocessing%2520%2528NLP%2529%2520with%2520impressive%2520scalability%252C%2520adaptability%2520and%250Ageneralizability.%2520Inspired%2520by%2520their%2520notable%2520success%2520in%2520NLP%2520field%252C%250Aautoregressive%2520models%2520have%2520been%2520intensively%2520investigated%2520recently%2520for%2520computer%250Avision%252C%2520which%2520perform%2520next-token%2520predictions%2520by%2520representing%2520visual%2520data%2520as%250Avisual%2520tokens%2520and%2520enables%2520autoregressive%2520modelling%2520for%2520a%2520wide%2520range%2520of%2520vision%250Atasks%252C%2520ranging%2520from%2520visual%2520generation%2520and%2520visual%2520understanding%2520to%2520the%2520very%250Arecent%2520multimodal%2520generation%2520that%2520unifies%2520visual%2520generation%2520and%2520understanding%250Awith%2520a%2520single%2520autoregressive%2520model.%2520This%2520paper%2520provides%2520a%2520systematic%2520review%2520of%250Avision%2520autoregressive%2520models%252C%2520including%2520the%2520development%2520of%2520a%2520taxonomy%2520of%250Aexisting%2520methods%2520and%2520highlighting%2520their%2520major%2520contributions%252C%2520strengths%252C%2520and%250Alimitations%252C%2520covering%2520various%2520vision%2520tasks%2520such%2520as%2520image%2520generation%252C%2520video%250Ageneration%252C%2520image%2520editing%252C%2520motion%2520generation%252C%2520medical%2520image%2520analysis%252C%25203D%250Ageneration%252C%2520robotic%2520manipulation%252C%2520unified%2520multimodal%2520generation%252C%2520etc.%2520Besides%252C%250Awe%2520investigate%2520and%2520analyze%2520the%2520latest%2520advancements%2520in%2520autoregressive%2520models%252C%250Aincluding%2520thorough%2520benchmarking%2520and%2520discussion%2520of%2520existing%2520methods%2520across%250Avarious%2520evaluation%2520datasets.%2520Finally%252C%2520we%2520outline%2520key%2520challenges%2520and%2520promising%250Adirections%2520for%2520future%2520research%252C%2520offering%2520a%2520roadmap%2520to%2520guide%2520further%250Aadvancements%2520in%2520vision%2520autoregressive%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Vision%20Autoregressive%20Model&entry.906535625=Kai%20Jiang%20and%20Jiaxing%20Huang&entry.1292438233=%20%20Autoregressive%20models%20have%20demonstrated%20great%20performance%20in%20natural%20language%0Aprocessing%20%28NLP%29%20with%20impressive%20scalability%2C%20adaptability%20and%0Ageneralizability.%20Inspired%20by%20their%20notable%20success%20in%20NLP%20field%2C%0Aautoregressive%20models%20have%20been%20intensively%20investigated%20recently%20for%20computer%0Avision%2C%20which%20perform%20next-token%20predictions%20by%20representing%20visual%20data%20as%0Avisual%20tokens%20and%20enables%20autoregressive%20modelling%20for%20a%20wide%20range%20of%20vision%0Atasks%2C%20ranging%20from%20visual%20generation%20and%20visual%20understanding%20to%20the%20very%0Arecent%20multimodal%20generation%20that%20unifies%20visual%20generation%20and%20understanding%0Awith%20a%20single%20autoregressive%20model.%20This%20paper%20provides%20a%20systematic%20review%20of%0Avision%20autoregressive%20models%2C%20including%20the%20development%20of%20a%20taxonomy%20of%0Aexisting%20methods%20and%20highlighting%20their%20major%20contributions%2C%20strengths%2C%20and%0Alimitations%2C%20covering%20various%20vision%20tasks%20such%20as%20image%20generation%2C%20video%0Ageneration%2C%20image%20editing%2C%20motion%20generation%2C%20medical%20image%20analysis%2C%203D%0Ageneration%2C%20robotic%20manipulation%2C%20unified%20multimodal%20generation%2C%20etc.%20Besides%2C%0Awe%20investigate%20and%20analyze%20the%20latest%20advancements%20in%20autoregressive%20models%2C%0Aincluding%20thorough%20benchmarking%20and%20discussion%20of%20existing%20methods%20across%0Avarious%20evaluation%20datasets.%20Finally%2C%20we%20outline%20key%20challenges%20and%20promising%0Adirections%20for%20future%20research%2C%20offering%20a%20roadmap%20to%20guide%20further%0Aadvancements%20in%20vision%20autoregressive%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08666v1&entry.124074799=Read"},
{"title": "Weakly-Supervised Anomaly Detection in Surveillance Videos Based on\n  Two-Stream I3D Convolution Network", "author": "Sareh Soltani Nejad and Anwar Haque", "abstract": "  The widespread implementation of urban surveillance systems has necessitated\nmore sophisticated techniques for anomaly detection to ensure enhanced public\nsafety. This paper presents a significant advancement in the field of anomaly\ndetection through the application of Two-Stream Inflated 3D (I3D) Convolutional\nNetworks. These networks substantially outperform traditional 3D Convolutional\nNetworks (C3D) by more effectively extracting spatial and temporal features\nfrom surveillance videos, thus improving the precision of anomaly detection.\nOur research advances the field by implementing a weakly supervised learning\nframework based on Multiple Instance Learning (MIL), which uniquely\nconceptualizes surveillance videos as collections of 'bags' that contain\ninstances (video clips). Each instance is innovatively processed through a\nranking mechanism that prioritizes clips based on their potential to display\nanomalies. This novel strategy not only enhances the accuracy and precision of\nanomaly detection but also significantly diminishes the dependency on extensive\nmanual annotations. Moreover, through meticulous optimization of model\nsettings, including the choice of optimizer, our approach not only establishes\nnew benchmarks in the performance of anomaly detection systems but also offers\na scalable and efficient solution for real-world surveillance applications.\nThis paper contributes significantly to the field of computer vision by\ndelivering a more adaptable, efficient, and context-aware anomaly detection\nsystem, which is poised to redefine practices in urban surveillance.\n", "link": "http://arxiv.org/abs/2411.08755v1", "date": "2024-11-13", "relevancy": 2.2227, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5583}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.556}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weakly-Supervised%20Anomaly%20Detection%20in%20Surveillance%20Videos%20Based%20on%0A%20%20Two-Stream%20I3D%20Convolution%20Network&body=Title%3A%20Weakly-Supervised%20Anomaly%20Detection%20in%20Surveillance%20Videos%20Based%20on%0A%20%20Two-Stream%20I3D%20Convolution%20Network%0AAuthor%3A%20Sareh%20Soltani%20Nejad%20and%20Anwar%20Haque%0AAbstract%3A%20%20%20The%20widespread%20implementation%20of%20urban%20surveillance%20systems%20has%20necessitated%0Amore%20sophisticated%20techniques%20for%20anomaly%20detection%20to%20ensure%20enhanced%20public%0Asafety.%20This%20paper%20presents%20a%20significant%20advancement%20in%20the%20field%20of%20anomaly%0Adetection%20through%20the%20application%20of%20Two-Stream%20Inflated%203D%20%28I3D%29%20Convolutional%0ANetworks.%20These%20networks%20substantially%20outperform%20traditional%203D%20Convolutional%0ANetworks%20%28C3D%29%20by%20more%20effectively%20extracting%20spatial%20and%20temporal%20features%0Afrom%20surveillance%20videos%2C%20thus%20improving%20the%20precision%20of%20anomaly%20detection.%0AOur%20research%20advances%20the%20field%20by%20implementing%20a%20weakly%20supervised%20learning%0Aframework%20based%20on%20Multiple%20Instance%20Learning%20%28MIL%29%2C%20which%20uniquely%0Aconceptualizes%20surveillance%20videos%20as%20collections%20of%20%27bags%27%20that%20contain%0Ainstances%20%28video%20clips%29.%20Each%20instance%20is%20innovatively%20processed%20through%20a%0Aranking%20mechanism%20that%20prioritizes%20clips%20based%20on%20their%20potential%20to%20display%0Aanomalies.%20This%20novel%20strategy%20not%20only%20enhances%20the%20accuracy%20and%20precision%20of%0Aanomaly%20detection%20but%20also%20significantly%20diminishes%20the%20dependency%20on%20extensive%0Amanual%20annotations.%20Moreover%2C%20through%20meticulous%20optimization%20of%20model%0Asettings%2C%20including%20the%20choice%20of%20optimizer%2C%20our%20approach%20not%20only%20establishes%0Anew%20benchmarks%20in%20the%20performance%20of%20anomaly%20detection%20systems%20but%20also%20offers%0Aa%20scalable%20and%20efficient%20solution%20for%20real-world%20surveillance%20applications.%0AThis%20paper%20contributes%20significantly%20to%20the%20field%20of%20computer%20vision%20by%0Adelivering%20a%20more%20adaptable%2C%20efficient%2C%20and%20context-aware%20anomaly%20detection%0Asystem%2C%20which%20is%20poised%20to%20redefine%20practices%20in%20urban%20surveillance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08755v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeakly-Supervised%2520Anomaly%2520Detection%2520in%2520Surveillance%2520Videos%2520Based%2520on%250A%2520%2520Two-Stream%2520I3D%2520Convolution%2520Network%26entry.906535625%3DSareh%2520Soltani%2520Nejad%2520and%2520Anwar%2520Haque%26entry.1292438233%3D%2520%2520The%2520widespread%2520implementation%2520of%2520urban%2520surveillance%2520systems%2520has%2520necessitated%250Amore%2520sophisticated%2520techniques%2520for%2520anomaly%2520detection%2520to%2520ensure%2520enhanced%2520public%250Asafety.%2520This%2520paper%2520presents%2520a%2520significant%2520advancement%2520in%2520the%2520field%2520of%2520anomaly%250Adetection%2520through%2520the%2520application%2520of%2520Two-Stream%2520Inflated%25203D%2520%2528I3D%2529%2520Convolutional%250ANetworks.%2520These%2520networks%2520substantially%2520outperform%2520traditional%25203D%2520Convolutional%250ANetworks%2520%2528C3D%2529%2520by%2520more%2520effectively%2520extracting%2520spatial%2520and%2520temporal%2520features%250Afrom%2520surveillance%2520videos%252C%2520thus%2520improving%2520the%2520precision%2520of%2520anomaly%2520detection.%250AOur%2520research%2520advances%2520the%2520field%2520by%2520implementing%2520a%2520weakly%2520supervised%2520learning%250Aframework%2520based%2520on%2520Multiple%2520Instance%2520Learning%2520%2528MIL%2529%252C%2520which%2520uniquely%250Aconceptualizes%2520surveillance%2520videos%2520as%2520collections%2520of%2520%2527bags%2527%2520that%2520contain%250Ainstances%2520%2528video%2520clips%2529.%2520Each%2520instance%2520is%2520innovatively%2520processed%2520through%2520a%250Aranking%2520mechanism%2520that%2520prioritizes%2520clips%2520based%2520on%2520their%2520potential%2520to%2520display%250Aanomalies.%2520This%2520novel%2520strategy%2520not%2520only%2520enhances%2520the%2520accuracy%2520and%2520precision%2520of%250Aanomaly%2520detection%2520but%2520also%2520significantly%2520diminishes%2520the%2520dependency%2520on%2520extensive%250Amanual%2520annotations.%2520Moreover%252C%2520through%2520meticulous%2520optimization%2520of%2520model%250Asettings%252C%2520including%2520the%2520choice%2520of%2520optimizer%252C%2520our%2520approach%2520not%2520only%2520establishes%250Anew%2520benchmarks%2520in%2520the%2520performance%2520of%2520anomaly%2520detection%2520systems%2520but%2520also%2520offers%250Aa%2520scalable%2520and%2520efficient%2520solution%2520for%2520real-world%2520surveillance%2520applications.%250AThis%2520paper%2520contributes%2520significantly%2520to%2520the%2520field%2520of%2520computer%2520vision%2520by%250Adelivering%2520a%2520more%2520adaptable%252C%2520efficient%252C%2520and%2520context-aware%2520anomaly%2520detection%250Asystem%252C%2520which%2520is%2520poised%2520to%2520redefine%2520practices%2520in%2520urban%2520surveillance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08755v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weakly-Supervised%20Anomaly%20Detection%20in%20Surveillance%20Videos%20Based%20on%0A%20%20Two-Stream%20I3D%20Convolution%20Network&entry.906535625=Sareh%20Soltani%20Nejad%20and%20Anwar%20Haque&entry.1292438233=%20%20The%20widespread%20implementation%20of%20urban%20surveillance%20systems%20has%20necessitated%0Amore%20sophisticated%20techniques%20for%20anomaly%20detection%20to%20ensure%20enhanced%20public%0Asafety.%20This%20paper%20presents%20a%20significant%20advancement%20in%20the%20field%20of%20anomaly%0Adetection%20through%20the%20application%20of%20Two-Stream%20Inflated%203D%20%28I3D%29%20Convolutional%0ANetworks.%20These%20networks%20substantially%20outperform%20traditional%203D%20Convolutional%0ANetworks%20%28C3D%29%20by%20more%20effectively%20extracting%20spatial%20and%20temporal%20features%0Afrom%20surveillance%20videos%2C%20thus%20improving%20the%20precision%20of%20anomaly%20detection.%0AOur%20research%20advances%20the%20field%20by%20implementing%20a%20weakly%20supervised%20learning%0Aframework%20based%20on%20Multiple%20Instance%20Learning%20%28MIL%29%2C%20which%20uniquely%0Aconceptualizes%20surveillance%20videos%20as%20collections%20of%20%27bags%27%20that%20contain%0Ainstances%20%28video%20clips%29.%20Each%20instance%20is%20innovatively%20processed%20through%20a%0Aranking%20mechanism%20that%20prioritizes%20clips%20based%20on%20their%20potential%20to%20display%0Aanomalies.%20This%20novel%20strategy%20not%20only%20enhances%20the%20accuracy%20and%20precision%20of%0Aanomaly%20detection%20but%20also%20significantly%20diminishes%20the%20dependency%20on%20extensive%0Amanual%20annotations.%20Moreover%2C%20through%20meticulous%20optimization%20of%20model%0Asettings%2C%20including%20the%20choice%20of%20optimizer%2C%20our%20approach%20not%20only%20establishes%0Anew%20benchmarks%20in%20the%20performance%20of%20anomaly%20detection%20systems%20but%20also%20offers%0Aa%20scalable%20and%20efficient%20solution%20for%20real-world%20surveillance%20applications.%0AThis%20paper%20contributes%20significantly%20to%20the%20field%20of%20computer%20vision%20by%0Adelivering%20a%20more%20adaptable%2C%20efficient%2C%20and%20context-aware%20anomaly%20detection%0Asystem%2C%20which%20is%20poised%20to%20redefine%20practices%20in%20urban%20surveillance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08755v1&entry.124074799=Read"},
{"title": "LUDO: Low-Latency Understanding of Highly Deformable Objects using Point\n  Cloud Occupancy Functions", "author": "Pit Henrich and Franziska Mathis-Ullrich and Paul Maria Scheikl", "abstract": "  Accurately determining the shape and location of internal structures within\ndeformable objects is crucial for medical tasks that require precise targeting,\nsuch as robotic biopsies. We introduce LUDO, a method for accurate low-latency\nunderstanding of deformable objects. LUDO reconstructs objects in their\ndeformed state, including their internal structures, from a single-view point\ncloud observation in under 30 ms using occupancy networks. We demonstrate\nLUDO's abilities for autonomous targeting of internal regions of interest\n(ROIs) in highly deformable objects. Additionally, LUDO provides uncertainty\nestimates and explainability for its predictions, both of which are important\nin safety-critical applications such as surgical interventions. We evaluate\nLUDO in real-world robotic experiments, achieving a success rate of 98.9% for\npuncturing various ROIs inside highly deformable objects. LUDO demonstrates the\npotential to interact with deformable objects without the need for deformable\nregistration methods.\n", "link": "http://arxiv.org/abs/2411.08777v1", "date": "2024-11-13", "relevancy": 2.2182, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5815}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5788}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5195}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LUDO%3A%20Low-Latency%20Understanding%20of%20Highly%20Deformable%20Objects%20using%20Point%0A%20%20Cloud%20Occupancy%20Functions&body=Title%3A%20LUDO%3A%20Low-Latency%20Understanding%20of%20Highly%20Deformable%20Objects%20using%20Point%0A%20%20Cloud%20Occupancy%20Functions%0AAuthor%3A%20Pit%20Henrich%20and%20Franziska%20Mathis-Ullrich%20and%20Paul%20Maria%20Scheikl%0AAbstract%3A%20%20%20Accurately%20determining%20the%20shape%20and%20location%20of%20internal%20structures%20within%0Adeformable%20objects%20is%20crucial%20for%20medical%20tasks%20that%20require%20precise%20targeting%2C%0Asuch%20as%20robotic%20biopsies.%20We%20introduce%20LUDO%2C%20a%20method%20for%20accurate%20low-latency%0Aunderstanding%20of%20deformable%20objects.%20LUDO%20reconstructs%20objects%20in%20their%0Adeformed%20state%2C%20including%20their%20internal%20structures%2C%20from%20a%20single-view%20point%0Acloud%20observation%20in%20under%2030%20ms%20using%20occupancy%20networks.%20We%20demonstrate%0ALUDO%27s%20abilities%20for%20autonomous%20targeting%20of%20internal%20regions%20of%20interest%0A%28ROIs%29%20in%20highly%20deformable%20objects.%20Additionally%2C%20LUDO%20provides%20uncertainty%0Aestimates%20and%20explainability%20for%20its%20predictions%2C%20both%20of%20which%20are%20important%0Ain%20safety-critical%20applications%20such%20as%20surgical%20interventions.%20We%20evaluate%0ALUDO%20in%20real-world%20robotic%20experiments%2C%20achieving%20a%20success%20rate%20of%2098.9%25%20for%0Apuncturing%20various%20ROIs%20inside%20highly%20deformable%20objects.%20LUDO%20demonstrates%20the%0Apotential%20to%20interact%20with%20deformable%20objects%20without%20the%20need%20for%20deformable%0Aregistration%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08777v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLUDO%253A%2520Low-Latency%2520Understanding%2520of%2520Highly%2520Deformable%2520Objects%2520using%2520Point%250A%2520%2520Cloud%2520Occupancy%2520Functions%26entry.906535625%3DPit%2520Henrich%2520and%2520Franziska%2520Mathis-Ullrich%2520and%2520Paul%2520Maria%2520Scheikl%26entry.1292438233%3D%2520%2520Accurately%2520determining%2520the%2520shape%2520and%2520location%2520of%2520internal%2520structures%2520within%250Adeformable%2520objects%2520is%2520crucial%2520for%2520medical%2520tasks%2520that%2520require%2520precise%2520targeting%252C%250Asuch%2520as%2520robotic%2520biopsies.%2520We%2520introduce%2520LUDO%252C%2520a%2520method%2520for%2520accurate%2520low-latency%250Aunderstanding%2520of%2520deformable%2520objects.%2520LUDO%2520reconstructs%2520objects%2520in%2520their%250Adeformed%2520state%252C%2520including%2520their%2520internal%2520structures%252C%2520from%2520a%2520single-view%2520point%250Acloud%2520observation%2520in%2520under%252030%2520ms%2520using%2520occupancy%2520networks.%2520We%2520demonstrate%250ALUDO%2527s%2520abilities%2520for%2520autonomous%2520targeting%2520of%2520internal%2520regions%2520of%2520interest%250A%2528ROIs%2529%2520in%2520highly%2520deformable%2520objects.%2520Additionally%252C%2520LUDO%2520provides%2520uncertainty%250Aestimates%2520and%2520explainability%2520for%2520its%2520predictions%252C%2520both%2520of%2520which%2520are%2520important%250Ain%2520safety-critical%2520applications%2520such%2520as%2520surgical%2520interventions.%2520We%2520evaluate%250ALUDO%2520in%2520real-world%2520robotic%2520experiments%252C%2520achieving%2520a%2520success%2520rate%2520of%252098.9%2525%2520for%250Apuncturing%2520various%2520ROIs%2520inside%2520highly%2520deformable%2520objects.%2520LUDO%2520demonstrates%2520the%250Apotential%2520to%2520interact%2520with%2520deformable%2520objects%2520without%2520the%2520need%2520for%2520deformable%250Aregistration%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08777v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LUDO%3A%20Low-Latency%20Understanding%20of%20Highly%20Deformable%20Objects%20using%20Point%0A%20%20Cloud%20Occupancy%20Functions&entry.906535625=Pit%20Henrich%20and%20Franziska%20Mathis-Ullrich%20and%20Paul%20Maria%20Scheikl&entry.1292438233=%20%20Accurately%20determining%20the%20shape%20and%20location%20of%20internal%20structures%20within%0Adeformable%20objects%20is%20crucial%20for%20medical%20tasks%20that%20require%20precise%20targeting%2C%0Asuch%20as%20robotic%20biopsies.%20We%20introduce%20LUDO%2C%20a%20method%20for%20accurate%20low-latency%0Aunderstanding%20of%20deformable%20objects.%20LUDO%20reconstructs%20objects%20in%20their%0Adeformed%20state%2C%20including%20their%20internal%20structures%2C%20from%20a%20single-view%20point%0Acloud%20observation%20in%20under%2030%20ms%20using%20occupancy%20networks.%20We%20demonstrate%0ALUDO%27s%20abilities%20for%20autonomous%20targeting%20of%20internal%20regions%20of%20interest%0A%28ROIs%29%20in%20highly%20deformable%20objects.%20Additionally%2C%20LUDO%20provides%20uncertainty%0Aestimates%20and%20explainability%20for%20its%20predictions%2C%20both%20of%20which%20are%20important%0Ain%20safety-critical%20applications%20such%20as%20surgical%20interventions.%20We%20evaluate%0ALUDO%20in%20real-world%20robotic%20experiments%2C%20achieving%20a%20success%20rate%20of%2098.9%25%20for%0Apuncturing%20various%20ROIs%20inside%20highly%20deformable%20objects.%20LUDO%20demonstrates%20the%0Apotential%20to%20interact%20with%20deformable%20objects%20without%20the%20need%20for%20deformable%0Aregistration%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08777v1&entry.124074799=Read"},
{"title": "Unsupervised Parameter-free Outlier Detection using HDBSCAN* Outlier\n  Profiles", "author": "Kushankur Ghosh and Murilo Coelho Naldi and J\u00f6rg Sander and Euijin Choo", "abstract": "  In machine learning and data mining, outliers are data points that\nsignificantly differ from the dataset and often introduce irrelevant\ninformation that can induce bias in its statistics and models. Therefore,\nunsupervised methods are crucial to detect outliers if there is limited or no\ninformation about them. Global-Local Outlier Scores based on Hierarchies\n(GLOSH) is an unsupervised outlier detection method within HDBSCAN*, a\nstate-of-the-art hierarchical clustering method. GLOSH estimates outlier scores\nfor each data point by comparing its density to the highest density of the\nregion they reside in the HDBSCAN* hierarchy. GLOSH may be sensitive to\nHDBSCAN*'s minpts parameter that influences density estimation. With limited\nknowledge about the data, choosing an appropriate minpts value beforehand is\nchallenging as one or some minpts values may better represent the underlying\ncluster structure than others. Additionally, in the process of searching for\n``potential outliers'', one has to define the number of outliers n a dataset\nhas, which may be impractical and is often unknown. In this paper, we propose\nan unsupervised strategy to find the ``best'' minpts value, leveraging the\nrange of GLOSH scores across minpts values to identify the value for which\nGLOSH scores can best identify outliers from the rest of the dataset. Moreover,\nwe propose an unsupervised strategy to estimate a threshold for classifying\npoints into inliers and (potential) outliers without the need to pre-define any\nvalue. Our experiments show that our strategies can automatically find the\nminpts value and threshold that yield the best or near best outlier detection\nresults using GLOSH.\n", "link": "http://arxiv.org/abs/2411.08867v1", "date": "2024-11-13", "relevancy": 2.1775, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4716}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4202}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Parameter-free%20Outlier%20Detection%20using%20HDBSCAN%2A%20Outlier%0A%20%20Profiles&body=Title%3A%20Unsupervised%20Parameter-free%20Outlier%20Detection%20using%20HDBSCAN%2A%20Outlier%0A%20%20Profiles%0AAuthor%3A%20Kushankur%20Ghosh%20and%20Murilo%20Coelho%20Naldi%20and%20J%C3%B6rg%20Sander%20and%20Euijin%20Choo%0AAbstract%3A%20%20%20In%20machine%20learning%20and%20data%20mining%2C%20outliers%20are%20data%20points%20that%0Asignificantly%20differ%20from%20the%20dataset%20and%20often%20introduce%20irrelevant%0Ainformation%20that%20can%20induce%20bias%20in%20its%20statistics%20and%20models.%20Therefore%2C%0Aunsupervised%20methods%20are%20crucial%20to%20detect%20outliers%20if%20there%20is%20limited%20or%20no%0Ainformation%20about%20them.%20Global-Local%20Outlier%20Scores%20based%20on%20Hierarchies%0A%28GLOSH%29%20is%20an%20unsupervised%20outlier%20detection%20method%20within%20HDBSCAN%2A%2C%20a%0Astate-of-the-art%20hierarchical%20clustering%20method.%20GLOSH%20estimates%20outlier%20scores%0Afor%20each%20data%20point%20by%20comparing%20its%20density%20to%20the%20highest%20density%20of%20the%0Aregion%20they%20reside%20in%20the%20HDBSCAN%2A%20hierarchy.%20GLOSH%20may%20be%20sensitive%20to%0AHDBSCAN%2A%27s%20minpts%20parameter%20that%20influences%20density%20estimation.%20With%20limited%0Aknowledge%20about%20the%20data%2C%20choosing%20an%20appropriate%20minpts%20value%20beforehand%20is%0Achallenging%20as%20one%20or%20some%20minpts%20values%20may%20better%20represent%20the%20underlying%0Acluster%20structure%20than%20others.%20Additionally%2C%20in%20the%20process%20of%20searching%20for%0A%60%60potential%20outliers%27%27%2C%20one%20has%20to%20define%20the%20number%20of%20outliers%20n%20a%20dataset%0Ahas%2C%20which%20may%20be%20impractical%20and%20is%20often%20unknown.%20In%20this%20paper%2C%20we%20propose%0Aan%20unsupervised%20strategy%20to%20find%20the%20%60%60best%27%27%20minpts%20value%2C%20leveraging%20the%0Arange%20of%20GLOSH%20scores%20across%20minpts%20values%20to%20identify%20the%20value%20for%20which%0AGLOSH%20scores%20can%20best%20identify%20outliers%20from%20the%20rest%20of%20the%20dataset.%20Moreover%2C%0Awe%20propose%20an%20unsupervised%20strategy%20to%20estimate%20a%20threshold%20for%20classifying%0Apoints%20into%20inliers%20and%20%28potential%29%20outliers%20without%20the%20need%20to%20pre-define%20any%0Avalue.%20Our%20experiments%20show%20that%20our%20strategies%20can%20automatically%20find%20the%0Aminpts%20value%20and%20threshold%20that%20yield%20the%20best%20or%20near%20best%20outlier%20detection%0Aresults%20using%20GLOSH.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08867v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Parameter-free%2520Outlier%2520Detection%2520using%2520HDBSCAN%252A%2520Outlier%250A%2520%2520Profiles%26entry.906535625%3DKushankur%2520Ghosh%2520and%2520Murilo%2520Coelho%2520Naldi%2520and%2520J%25C3%25B6rg%2520Sander%2520and%2520Euijin%2520Choo%26entry.1292438233%3D%2520%2520In%2520machine%2520learning%2520and%2520data%2520mining%252C%2520outliers%2520are%2520data%2520points%2520that%250Asignificantly%2520differ%2520from%2520the%2520dataset%2520and%2520often%2520introduce%2520irrelevant%250Ainformation%2520that%2520can%2520induce%2520bias%2520in%2520its%2520statistics%2520and%2520models.%2520Therefore%252C%250Aunsupervised%2520methods%2520are%2520crucial%2520to%2520detect%2520outliers%2520if%2520there%2520is%2520limited%2520or%2520no%250Ainformation%2520about%2520them.%2520Global-Local%2520Outlier%2520Scores%2520based%2520on%2520Hierarchies%250A%2528GLOSH%2529%2520is%2520an%2520unsupervised%2520outlier%2520detection%2520method%2520within%2520HDBSCAN%252A%252C%2520a%250Astate-of-the-art%2520hierarchical%2520clustering%2520method.%2520GLOSH%2520estimates%2520outlier%2520scores%250Afor%2520each%2520data%2520point%2520by%2520comparing%2520its%2520density%2520to%2520the%2520highest%2520density%2520of%2520the%250Aregion%2520they%2520reside%2520in%2520the%2520HDBSCAN%252A%2520hierarchy.%2520GLOSH%2520may%2520be%2520sensitive%2520to%250AHDBSCAN%252A%2527s%2520minpts%2520parameter%2520that%2520influences%2520density%2520estimation.%2520With%2520limited%250Aknowledge%2520about%2520the%2520data%252C%2520choosing%2520an%2520appropriate%2520minpts%2520value%2520beforehand%2520is%250Achallenging%2520as%2520one%2520or%2520some%2520minpts%2520values%2520may%2520better%2520represent%2520the%2520underlying%250Acluster%2520structure%2520than%2520others.%2520Additionally%252C%2520in%2520the%2520process%2520of%2520searching%2520for%250A%2560%2560potential%2520outliers%2527%2527%252C%2520one%2520has%2520to%2520define%2520the%2520number%2520of%2520outliers%2520n%2520a%2520dataset%250Ahas%252C%2520which%2520may%2520be%2520impractical%2520and%2520is%2520often%2520unknown.%2520In%2520this%2520paper%252C%2520we%2520propose%250Aan%2520unsupervised%2520strategy%2520to%2520find%2520the%2520%2560%2560best%2527%2527%2520minpts%2520value%252C%2520leveraging%2520the%250Arange%2520of%2520GLOSH%2520scores%2520across%2520minpts%2520values%2520to%2520identify%2520the%2520value%2520for%2520which%250AGLOSH%2520scores%2520can%2520best%2520identify%2520outliers%2520from%2520the%2520rest%2520of%2520the%2520dataset.%2520Moreover%252C%250Awe%2520propose%2520an%2520unsupervised%2520strategy%2520to%2520estimate%2520a%2520threshold%2520for%2520classifying%250Apoints%2520into%2520inliers%2520and%2520%2528potential%2529%2520outliers%2520without%2520the%2520need%2520to%2520pre-define%2520any%250Avalue.%2520Our%2520experiments%2520show%2520that%2520our%2520strategies%2520can%2520automatically%2520find%2520the%250Aminpts%2520value%2520and%2520threshold%2520that%2520yield%2520the%2520best%2520or%2520near%2520best%2520outlier%2520detection%250Aresults%2520using%2520GLOSH.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08867v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Parameter-free%20Outlier%20Detection%20using%20HDBSCAN%2A%20Outlier%0A%20%20Profiles&entry.906535625=Kushankur%20Ghosh%20and%20Murilo%20Coelho%20Naldi%20and%20J%C3%B6rg%20Sander%20and%20Euijin%20Choo&entry.1292438233=%20%20In%20machine%20learning%20and%20data%20mining%2C%20outliers%20are%20data%20points%20that%0Asignificantly%20differ%20from%20the%20dataset%20and%20often%20introduce%20irrelevant%0Ainformation%20that%20can%20induce%20bias%20in%20its%20statistics%20and%20models.%20Therefore%2C%0Aunsupervised%20methods%20are%20crucial%20to%20detect%20outliers%20if%20there%20is%20limited%20or%20no%0Ainformation%20about%20them.%20Global-Local%20Outlier%20Scores%20based%20on%20Hierarchies%0A%28GLOSH%29%20is%20an%20unsupervised%20outlier%20detection%20method%20within%20HDBSCAN%2A%2C%20a%0Astate-of-the-art%20hierarchical%20clustering%20method.%20GLOSH%20estimates%20outlier%20scores%0Afor%20each%20data%20point%20by%20comparing%20its%20density%20to%20the%20highest%20density%20of%20the%0Aregion%20they%20reside%20in%20the%20HDBSCAN%2A%20hierarchy.%20GLOSH%20may%20be%20sensitive%20to%0AHDBSCAN%2A%27s%20minpts%20parameter%20that%20influences%20density%20estimation.%20With%20limited%0Aknowledge%20about%20the%20data%2C%20choosing%20an%20appropriate%20minpts%20value%20beforehand%20is%0Achallenging%20as%20one%20or%20some%20minpts%20values%20may%20better%20represent%20the%20underlying%0Acluster%20structure%20than%20others.%20Additionally%2C%20in%20the%20process%20of%20searching%20for%0A%60%60potential%20outliers%27%27%2C%20one%20has%20to%20define%20the%20number%20of%20outliers%20n%20a%20dataset%0Ahas%2C%20which%20may%20be%20impractical%20and%20is%20often%20unknown.%20In%20this%20paper%2C%20we%20propose%0Aan%20unsupervised%20strategy%20to%20find%20the%20%60%60best%27%27%20minpts%20value%2C%20leveraging%20the%0Arange%20of%20GLOSH%20scores%20across%20minpts%20values%20to%20identify%20the%20value%20for%20which%0AGLOSH%20scores%20can%20best%20identify%20outliers%20from%20the%20rest%20of%20the%20dataset.%20Moreover%2C%0Awe%20propose%20an%20unsupervised%20strategy%20to%20estimate%20a%20threshold%20for%20classifying%0Apoints%20into%20inliers%20and%20%28potential%29%20outliers%20without%20the%20need%20to%20pre-define%20any%0Avalue.%20Our%20experiments%20show%20that%20our%20strategies%20can%20automatically%20find%20the%0Aminpts%20value%20and%20threshold%20that%20yield%20the%20best%20or%20near%20best%20outlier%20detection%0Aresults%20using%20GLOSH.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08867v1&entry.124074799=Read"},
{"title": "MikuDance: Animating Character Art with Mixed Motion Dynamics", "author": "Jiaxu Zhang and Xianfang Zeng and Xin Chen and Wei Zuo and Gang Yu and Zhigang Tu", "abstract": "  We propose MikuDance, a diffusion-based pipeline incorporating mixed motion\ndynamics to animate stylized character art. MikuDance consists of two key\ntechniques: Mixed Motion Modeling and Mixed-Control Diffusion, to address the\nchallenges of high-dynamic motion and reference-guidance misalignment in\ncharacter art animation. Specifically, a Scene Motion Tracking strategy is\npresented to explicitly model the dynamic camera in pixel-wise space, enabling\nunified character-scene motion modeling. Building on this, the Mixed-Control\nDiffusion implicitly aligns the scale and body shape of diverse characters with\nmotion guidance, allowing flexible control of local character motion.\nSubsequently, a Motion-Adaptive Normalization module is incorporated to\neffectively inject global scene motion, paving the way for comprehensive\ncharacter art animation. Through extensive experiments, we demonstrate the\neffectiveness and generalizability of MikuDance across various character art\nand motion guidance, consistently producing high-quality animations with\nremarkable motion dynamics.\n", "link": "http://arxiv.org/abs/2411.08656v1", "date": "2024-11-13", "relevancy": 2.1666, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.588}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5122}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5071}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MikuDance%3A%20Animating%20Character%20Art%20with%20Mixed%20Motion%20Dynamics&body=Title%3A%20MikuDance%3A%20Animating%20Character%20Art%20with%20Mixed%20Motion%20Dynamics%0AAuthor%3A%20Jiaxu%20Zhang%20and%20Xianfang%20Zeng%20and%20Xin%20Chen%20and%20Wei%20Zuo%20and%20Gang%20Yu%20and%20Zhigang%20Tu%0AAbstract%3A%20%20%20We%20propose%20MikuDance%2C%20a%20diffusion-based%20pipeline%20incorporating%20mixed%20motion%0Adynamics%20to%20animate%20stylized%20character%20art.%20MikuDance%20consists%20of%20two%20key%0Atechniques%3A%20Mixed%20Motion%20Modeling%20and%20Mixed-Control%20Diffusion%2C%20to%20address%20the%0Achallenges%20of%20high-dynamic%20motion%20and%20reference-guidance%20misalignment%20in%0Acharacter%20art%20animation.%20Specifically%2C%20a%20Scene%20Motion%20Tracking%20strategy%20is%0Apresented%20to%20explicitly%20model%20the%20dynamic%20camera%20in%20pixel-wise%20space%2C%20enabling%0Aunified%20character-scene%20motion%20modeling.%20Building%20on%20this%2C%20the%20Mixed-Control%0ADiffusion%20implicitly%20aligns%20the%20scale%20and%20body%20shape%20of%20diverse%20characters%20with%0Amotion%20guidance%2C%20allowing%20flexible%20control%20of%20local%20character%20motion.%0ASubsequently%2C%20a%20Motion-Adaptive%20Normalization%20module%20is%20incorporated%20to%0Aeffectively%20inject%20global%20scene%20motion%2C%20paving%20the%20way%20for%20comprehensive%0Acharacter%20art%20animation.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20the%0Aeffectiveness%20and%20generalizability%20of%20MikuDance%20across%20various%20character%20art%0Aand%20motion%20guidance%2C%20consistently%20producing%20high-quality%20animations%20with%0Aremarkable%20motion%20dynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08656v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMikuDance%253A%2520Animating%2520Character%2520Art%2520with%2520Mixed%2520Motion%2520Dynamics%26entry.906535625%3DJiaxu%2520Zhang%2520and%2520Xianfang%2520Zeng%2520and%2520Xin%2520Chen%2520and%2520Wei%2520Zuo%2520and%2520Gang%2520Yu%2520and%2520Zhigang%2520Tu%26entry.1292438233%3D%2520%2520We%2520propose%2520MikuDance%252C%2520a%2520diffusion-based%2520pipeline%2520incorporating%2520mixed%2520motion%250Adynamics%2520to%2520animate%2520stylized%2520character%2520art.%2520MikuDance%2520consists%2520of%2520two%2520key%250Atechniques%253A%2520Mixed%2520Motion%2520Modeling%2520and%2520Mixed-Control%2520Diffusion%252C%2520to%2520address%2520the%250Achallenges%2520of%2520high-dynamic%2520motion%2520and%2520reference-guidance%2520misalignment%2520in%250Acharacter%2520art%2520animation.%2520Specifically%252C%2520a%2520Scene%2520Motion%2520Tracking%2520strategy%2520is%250Apresented%2520to%2520explicitly%2520model%2520the%2520dynamic%2520camera%2520in%2520pixel-wise%2520space%252C%2520enabling%250Aunified%2520character-scene%2520motion%2520modeling.%2520Building%2520on%2520this%252C%2520the%2520Mixed-Control%250ADiffusion%2520implicitly%2520aligns%2520the%2520scale%2520and%2520body%2520shape%2520of%2520diverse%2520characters%2520with%250Amotion%2520guidance%252C%2520allowing%2520flexible%2520control%2520of%2520local%2520character%2520motion.%250ASubsequently%252C%2520a%2520Motion-Adaptive%2520Normalization%2520module%2520is%2520incorporated%2520to%250Aeffectively%2520inject%2520global%2520scene%2520motion%252C%2520paving%2520the%2520way%2520for%2520comprehensive%250Acharacter%2520art%2520animation.%2520Through%2520extensive%2520experiments%252C%2520we%2520demonstrate%2520the%250Aeffectiveness%2520and%2520generalizability%2520of%2520MikuDance%2520across%2520various%2520character%2520art%250Aand%2520motion%2520guidance%252C%2520consistently%2520producing%2520high-quality%2520animations%2520with%250Aremarkable%2520motion%2520dynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08656v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MikuDance%3A%20Animating%20Character%20Art%20with%20Mixed%20Motion%20Dynamics&entry.906535625=Jiaxu%20Zhang%20and%20Xianfang%20Zeng%20and%20Xin%20Chen%20and%20Wei%20Zuo%20and%20Gang%20Yu%20and%20Zhigang%20Tu&entry.1292438233=%20%20We%20propose%20MikuDance%2C%20a%20diffusion-based%20pipeline%20incorporating%20mixed%20motion%0Adynamics%20to%20animate%20stylized%20character%20art.%20MikuDance%20consists%20of%20two%20key%0Atechniques%3A%20Mixed%20Motion%20Modeling%20and%20Mixed-Control%20Diffusion%2C%20to%20address%20the%0Achallenges%20of%20high-dynamic%20motion%20and%20reference-guidance%20misalignment%20in%0Acharacter%20art%20animation.%20Specifically%2C%20a%20Scene%20Motion%20Tracking%20strategy%20is%0Apresented%20to%20explicitly%20model%20the%20dynamic%20camera%20in%20pixel-wise%20space%2C%20enabling%0Aunified%20character-scene%20motion%20modeling.%20Building%20on%20this%2C%20the%20Mixed-Control%0ADiffusion%20implicitly%20aligns%20the%20scale%20and%20body%20shape%20of%20diverse%20characters%20with%0Amotion%20guidance%2C%20allowing%20flexible%20control%20of%20local%20character%20motion.%0ASubsequently%2C%20a%20Motion-Adaptive%20Normalization%20module%20is%20incorporated%20to%0Aeffectively%20inject%20global%20scene%20motion%2C%20paving%20the%20way%20for%20comprehensive%0Acharacter%20art%20animation.%20Through%20extensive%20experiments%2C%20we%20demonstrate%20the%0Aeffectiveness%20and%20generalizability%20of%20MikuDance%20across%20various%20character%20art%0Aand%20motion%20guidance%2C%20consistently%20producing%20high-quality%20animations%20with%0Aremarkable%20motion%20dynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08656v1&entry.124074799=Read"},
{"title": "Methodology for a Statistical Analysis of Influencing Factors on 3D\n  Object Detection Performance", "author": "Anton Kuznietsov and Dirk Schweickard and Steven Peters", "abstract": "  In autonomous driving, object detection is an essential task to perceive the\nenvironment by localizing and classifying objects. Most object detection\nalgorithms rely on deep learning for their superior performance. However, their\nblack box nature makes it challenging to ensure safety. In this paper, we\npropose a first-of-its-kind methodology for statistical analysis of the\ninfluence of various factors related to the objects to detect or the\nenvironment on the detection performance of both LiDAR- and camera-based 3D\nobject detectors. We perform a univariate analysis between each of the factors\nand the detection error in order to compare the strength of influence. To\nbetter identify potential sources of detection errors, we also analyze the\nperformance in dependency of the influencing factors and examine the\ninterdependencies between the different influencing factors. Recognizing the\nfactors that influence detection performance helps identify robustness issues\nin the trained object detector and supports the safety approval of object\ndetection systems.\n", "link": "http://arxiv.org/abs/2411.08482v1", "date": "2024-11-13", "relevancy": 2.1636, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5659}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5359}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Methodology%20for%20a%20Statistical%20Analysis%20of%20Influencing%20Factors%20on%203D%0A%20%20Object%20Detection%20Performance&body=Title%3A%20Methodology%20for%20a%20Statistical%20Analysis%20of%20Influencing%20Factors%20on%203D%0A%20%20Object%20Detection%20Performance%0AAuthor%3A%20Anton%20Kuznietsov%20and%20Dirk%20Schweickard%20and%20Steven%20Peters%0AAbstract%3A%20%20%20In%20autonomous%20driving%2C%20object%20detection%20is%20an%20essential%20task%20to%20perceive%20the%0Aenvironment%20by%20localizing%20and%20classifying%20objects.%20Most%20object%20detection%0Aalgorithms%20rely%20on%20deep%20learning%20for%20their%20superior%20performance.%20However%2C%20their%0Ablack%20box%20nature%20makes%20it%20challenging%20to%20ensure%20safety.%20In%20this%20paper%2C%20we%0Apropose%20a%20first-of-its-kind%20methodology%20for%20statistical%20analysis%20of%20the%0Ainfluence%20of%20various%20factors%20related%20to%20the%20objects%20to%20detect%20or%20the%0Aenvironment%20on%20the%20detection%20performance%20of%20both%20LiDAR-%20and%20camera-based%203D%0Aobject%20detectors.%20We%20perform%20a%20univariate%20analysis%20between%20each%20of%20the%20factors%0Aand%20the%20detection%20error%20in%20order%20to%20compare%20the%20strength%20of%20influence.%20To%0Abetter%20identify%20potential%20sources%20of%20detection%20errors%2C%20we%20also%20analyze%20the%0Aperformance%20in%20dependency%20of%20the%20influencing%20factors%20and%20examine%20the%0Ainterdependencies%20between%20the%20different%20influencing%20factors.%20Recognizing%20the%0Afactors%20that%20influence%20detection%20performance%20helps%20identify%20robustness%20issues%0Ain%20the%20trained%20object%20detector%20and%20supports%20the%20safety%20approval%20of%20object%0Adetection%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08482v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMethodology%2520for%2520a%2520Statistical%2520Analysis%2520of%2520Influencing%2520Factors%2520on%25203D%250A%2520%2520Object%2520Detection%2520Performance%26entry.906535625%3DAnton%2520Kuznietsov%2520and%2520Dirk%2520Schweickard%2520and%2520Steven%2520Peters%26entry.1292438233%3D%2520%2520In%2520autonomous%2520driving%252C%2520object%2520detection%2520is%2520an%2520essential%2520task%2520to%2520perceive%2520the%250Aenvironment%2520by%2520localizing%2520and%2520classifying%2520objects.%2520Most%2520object%2520detection%250Aalgorithms%2520rely%2520on%2520deep%2520learning%2520for%2520their%2520superior%2520performance.%2520However%252C%2520their%250Ablack%2520box%2520nature%2520makes%2520it%2520challenging%2520to%2520ensure%2520safety.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520first-of-its-kind%2520methodology%2520for%2520statistical%2520analysis%2520of%2520the%250Ainfluence%2520of%2520various%2520factors%2520related%2520to%2520the%2520objects%2520to%2520detect%2520or%2520the%250Aenvironment%2520on%2520the%2520detection%2520performance%2520of%2520both%2520LiDAR-%2520and%2520camera-based%25203D%250Aobject%2520detectors.%2520We%2520perform%2520a%2520univariate%2520analysis%2520between%2520each%2520of%2520the%2520factors%250Aand%2520the%2520detection%2520error%2520in%2520order%2520to%2520compare%2520the%2520strength%2520of%2520influence.%2520To%250Abetter%2520identify%2520potential%2520sources%2520of%2520detection%2520errors%252C%2520we%2520also%2520analyze%2520the%250Aperformance%2520in%2520dependency%2520of%2520the%2520influencing%2520factors%2520and%2520examine%2520the%250Ainterdependencies%2520between%2520the%2520different%2520influencing%2520factors.%2520Recognizing%2520the%250Afactors%2520that%2520influence%2520detection%2520performance%2520helps%2520identify%2520robustness%2520issues%250Ain%2520the%2520trained%2520object%2520detector%2520and%2520supports%2520the%2520safety%2520approval%2520of%2520object%250Adetection%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08482v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Methodology%20for%20a%20Statistical%20Analysis%20of%20Influencing%20Factors%20on%203D%0A%20%20Object%20Detection%20Performance&entry.906535625=Anton%20Kuznietsov%20and%20Dirk%20Schweickard%20and%20Steven%20Peters&entry.1292438233=%20%20In%20autonomous%20driving%2C%20object%20detection%20is%20an%20essential%20task%20to%20perceive%20the%0Aenvironment%20by%20localizing%20and%20classifying%20objects.%20Most%20object%20detection%0Aalgorithms%20rely%20on%20deep%20learning%20for%20their%20superior%20performance.%20However%2C%20their%0Ablack%20box%20nature%20makes%20it%20challenging%20to%20ensure%20safety.%20In%20this%20paper%2C%20we%0Apropose%20a%20first-of-its-kind%20methodology%20for%20statistical%20analysis%20of%20the%0Ainfluence%20of%20various%20factors%20related%20to%20the%20objects%20to%20detect%20or%20the%0Aenvironment%20on%20the%20detection%20performance%20of%20both%20LiDAR-%20and%20camera-based%203D%0Aobject%20detectors.%20We%20perform%20a%20univariate%20analysis%20between%20each%20of%20the%20factors%0Aand%20the%20detection%20error%20in%20order%20to%20compare%20the%20strength%20of%20influence.%20To%0Abetter%20identify%20potential%20sources%20of%20detection%20errors%2C%20we%20also%20analyze%20the%0Aperformance%20in%20dependency%20of%20the%20influencing%20factors%20and%20examine%20the%0Ainterdependencies%20between%20the%20different%20influencing%20factors.%20Recognizing%20the%0Afactors%20that%20influence%20detection%20performance%20helps%20identify%20robustness%20issues%0Ain%20the%20trained%20object%20detector%20and%20supports%20the%20safety%20approval%20of%20object%0Adetection%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08482v1&entry.124074799=Read"},
{"title": "HiFi-Syn: Hierarchical Granularity Discrimination for High-Fidelity\n  Synthesis of MR Images with Structure Preservation", "author": "Ziqi Yu and Botao Zhao and Shengjie Zhang and Xiang Chen and Jianfeng Feng and Tingying Peng and Xiao-Yong Zhang", "abstract": "  Synthesizing medical images while preserving their structural information is\ncrucial in medical research. In such scenarios, the preservation of anatomical\ncontent becomes especially important. Although recent advances have been made\nby incorporating instance-level information to guide translation, these methods\noverlook the spatial coherence of structural-level representation and the\nanatomical invariance of content during translation. To address these issues,\nwe introduce hierarchical granularity discrimination, which exploits various\nlevels of semantic information present in medical images. Our strategy utilizes\nthree levels of discrimination granularity: pixel-level discrimination using a\nBrain Memory Bank, structure-level discrimination on each brain structure with\na re-weighting strategy to focus on hard samples, and global-level\ndiscrimination to ensure anatomical consistency during translation. The image\ntranslation performance of our strategy has been evaluated on three independent\ndatasets (UK Biobank, IXI, and BraTS 2018), and it has outperformed\nstate-of-the-art algorithms. Particularly, our model excels not only in\nsynthesizing normal structures but also in handling abnormal (pathological)\nstructures, such as brain tumors, despite the variations in contrast observed\nacross different imaging modalities due to their pathological characteristics.\nThe diagnostic value of synthesized MR images containing brain tumors has been\nevaluated by radiologists. This indicates that our model may offer an\nalternative solution in scenarios where specific MR modalities of patients are\nunavailable. Extensive experiments further demonstrate the versatility of our\nmethod, providing unique insights into medical image translation.\n", "link": "http://arxiv.org/abs/2311.12461v2", "date": "2024-11-13", "relevancy": 2.1617, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5475}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5427}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiFi-Syn%3A%20Hierarchical%20Granularity%20Discrimination%20for%20High-Fidelity%0A%20%20Synthesis%20of%20MR%20Images%20with%20Structure%20Preservation&body=Title%3A%20HiFi-Syn%3A%20Hierarchical%20Granularity%20Discrimination%20for%20High-Fidelity%0A%20%20Synthesis%20of%20MR%20Images%20with%20Structure%20Preservation%0AAuthor%3A%20Ziqi%20Yu%20and%20Botao%20Zhao%20and%20Shengjie%20Zhang%20and%20Xiang%20Chen%20and%20Jianfeng%20Feng%20and%20Tingying%20Peng%20and%20Xiao-Yong%20Zhang%0AAbstract%3A%20%20%20Synthesizing%20medical%20images%20while%20preserving%20their%20structural%20information%20is%0Acrucial%20in%20medical%20research.%20In%20such%20scenarios%2C%20the%20preservation%20of%20anatomical%0Acontent%20becomes%20especially%20important.%20Although%20recent%20advances%20have%20been%20made%0Aby%20incorporating%20instance-level%20information%20to%20guide%20translation%2C%20these%20methods%0Aoverlook%20the%20spatial%20coherence%20of%20structural-level%20representation%20and%20the%0Aanatomical%20invariance%20of%20content%20during%20translation.%20To%20address%20these%20issues%2C%0Awe%20introduce%20hierarchical%20granularity%20discrimination%2C%20which%20exploits%20various%0Alevels%20of%20semantic%20information%20present%20in%20medical%20images.%20Our%20strategy%20utilizes%0Athree%20levels%20of%20discrimination%20granularity%3A%20pixel-level%20discrimination%20using%20a%0ABrain%20Memory%20Bank%2C%20structure-level%20discrimination%20on%20each%20brain%20structure%20with%0Aa%20re-weighting%20strategy%20to%20focus%20on%20hard%20samples%2C%20and%20global-level%0Adiscrimination%20to%20ensure%20anatomical%20consistency%20during%20translation.%20The%20image%0Atranslation%20performance%20of%20our%20strategy%20has%20been%20evaluated%20on%20three%20independent%0Adatasets%20%28UK%20Biobank%2C%20IXI%2C%20and%20BraTS%202018%29%2C%20and%20it%20has%20outperformed%0Astate-of-the-art%20algorithms.%20Particularly%2C%20our%20model%20excels%20not%20only%20in%0Asynthesizing%20normal%20structures%20but%20also%20in%20handling%20abnormal%20%28pathological%29%0Astructures%2C%20such%20as%20brain%20tumors%2C%20despite%20the%20variations%20in%20contrast%20observed%0Aacross%20different%20imaging%20modalities%20due%20to%20their%20pathological%20characteristics.%0AThe%20diagnostic%20value%20of%20synthesized%20MR%20images%20containing%20brain%20tumors%20has%20been%0Aevaluated%20by%20radiologists.%20This%20indicates%20that%20our%20model%20may%20offer%20an%0Aalternative%20solution%20in%20scenarios%20where%20specific%20MR%20modalities%20of%20patients%20are%0Aunavailable.%20Extensive%20experiments%20further%20demonstrate%20the%20versatility%20of%20our%0Amethod%2C%20providing%20unique%20insights%20into%20medical%20image%20translation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.12461v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiFi-Syn%253A%2520Hierarchical%2520Granularity%2520Discrimination%2520for%2520High-Fidelity%250A%2520%2520Synthesis%2520of%2520MR%2520Images%2520with%2520Structure%2520Preservation%26entry.906535625%3DZiqi%2520Yu%2520and%2520Botao%2520Zhao%2520and%2520Shengjie%2520Zhang%2520and%2520Xiang%2520Chen%2520and%2520Jianfeng%2520Feng%2520and%2520Tingying%2520Peng%2520and%2520Xiao-Yong%2520Zhang%26entry.1292438233%3D%2520%2520Synthesizing%2520medical%2520images%2520while%2520preserving%2520their%2520structural%2520information%2520is%250Acrucial%2520in%2520medical%2520research.%2520In%2520such%2520scenarios%252C%2520the%2520preservation%2520of%2520anatomical%250Acontent%2520becomes%2520especially%2520important.%2520Although%2520recent%2520advances%2520have%2520been%2520made%250Aby%2520incorporating%2520instance-level%2520information%2520to%2520guide%2520translation%252C%2520these%2520methods%250Aoverlook%2520the%2520spatial%2520coherence%2520of%2520structural-level%2520representation%2520and%2520the%250Aanatomical%2520invariance%2520of%2520content%2520during%2520translation.%2520To%2520address%2520these%2520issues%252C%250Awe%2520introduce%2520hierarchical%2520granularity%2520discrimination%252C%2520which%2520exploits%2520various%250Alevels%2520of%2520semantic%2520information%2520present%2520in%2520medical%2520images.%2520Our%2520strategy%2520utilizes%250Athree%2520levels%2520of%2520discrimination%2520granularity%253A%2520pixel-level%2520discrimination%2520using%2520a%250ABrain%2520Memory%2520Bank%252C%2520structure-level%2520discrimination%2520on%2520each%2520brain%2520structure%2520with%250Aa%2520re-weighting%2520strategy%2520to%2520focus%2520on%2520hard%2520samples%252C%2520and%2520global-level%250Adiscrimination%2520to%2520ensure%2520anatomical%2520consistency%2520during%2520translation.%2520The%2520image%250Atranslation%2520performance%2520of%2520our%2520strategy%2520has%2520been%2520evaluated%2520on%2520three%2520independent%250Adatasets%2520%2528UK%2520Biobank%252C%2520IXI%252C%2520and%2520BraTS%25202018%2529%252C%2520and%2520it%2520has%2520outperformed%250Astate-of-the-art%2520algorithms.%2520Particularly%252C%2520our%2520model%2520excels%2520not%2520only%2520in%250Asynthesizing%2520normal%2520structures%2520but%2520also%2520in%2520handling%2520abnormal%2520%2528pathological%2529%250Astructures%252C%2520such%2520as%2520brain%2520tumors%252C%2520despite%2520the%2520variations%2520in%2520contrast%2520observed%250Aacross%2520different%2520imaging%2520modalities%2520due%2520to%2520their%2520pathological%2520characteristics.%250AThe%2520diagnostic%2520value%2520of%2520synthesized%2520MR%2520images%2520containing%2520brain%2520tumors%2520has%2520been%250Aevaluated%2520by%2520radiologists.%2520This%2520indicates%2520that%2520our%2520model%2520may%2520offer%2520an%250Aalternative%2520solution%2520in%2520scenarios%2520where%2520specific%2520MR%2520modalities%2520of%2520patients%2520are%250Aunavailable.%2520Extensive%2520experiments%2520further%2520demonstrate%2520the%2520versatility%2520of%2520our%250Amethod%252C%2520providing%2520unique%2520insights%2520into%2520medical%2520image%2520translation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.12461v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiFi-Syn%3A%20Hierarchical%20Granularity%20Discrimination%20for%20High-Fidelity%0A%20%20Synthesis%20of%20MR%20Images%20with%20Structure%20Preservation&entry.906535625=Ziqi%20Yu%20and%20Botao%20Zhao%20and%20Shengjie%20Zhang%20and%20Xiang%20Chen%20and%20Jianfeng%20Feng%20and%20Tingying%20Peng%20and%20Xiao-Yong%20Zhang&entry.1292438233=%20%20Synthesizing%20medical%20images%20while%20preserving%20their%20structural%20information%20is%0Acrucial%20in%20medical%20research.%20In%20such%20scenarios%2C%20the%20preservation%20of%20anatomical%0Acontent%20becomes%20especially%20important.%20Although%20recent%20advances%20have%20been%20made%0Aby%20incorporating%20instance-level%20information%20to%20guide%20translation%2C%20these%20methods%0Aoverlook%20the%20spatial%20coherence%20of%20structural-level%20representation%20and%20the%0Aanatomical%20invariance%20of%20content%20during%20translation.%20To%20address%20these%20issues%2C%0Awe%20introduce%20hierarchical%20granularity%20discrimination%2C%20which%20exploits%20various%0Alevels%20of%20semantic%20information%20present%20in%20medical%20images.%20Our%20strategy%20utilizes%0Athree%20levels%20of%20discrimination%20granularity%3A%20pixel-level%20discrimination%20using%20a%0ABrain%20Memory%20Bank%2C%20structure-level%20discrimination%20on%20each%20brain%20structure%20with%0Aa%20re-weighting%20strategy%20to%20focus%20on%20hard%20samples%2C%20and%20global-level%0Adiscrimination%20to%20ensure%20anatomical%20consistency%20during%20translation.%20The%20image%0Atranslation%20performance%20of%20our%20strategy%20has%20been%20evaluated%20on%20three%20independent%0Adatasets%20%28UK%20Biobank%2C%20IXI%2C%20and%20BraTS%202018%29%2C%20and%20it%20has%20outperformed%0Astate-of-the-art%20algorithms.%20Particularly%2C%20our%20model%20excels%20not%20only%20in%0Asynthesizing%20normal%20structures%20but%20also%20in%20handling%20abnormal%20%28pathological%29%0Astructures%2C%20such%20as%20brain%20tumors%2C%20despite%20the%20variations%20in%20contrast%20observed%0Aacross%20different%20imaging%20modalities%20due%20to%20their%20pathological%20characteristics.%0AThe%20diagnostic%20value%20of%20synthesized%20MR%20images%20containing%20brain%20tumors%20has%20been%0Aevaluated%20by%20radiologists.%20This%20indicates%20that%20our%20model%20may%20offer%20an%0Aalternative%20solution%20in%20scenarios%20where%20specific%20MR%20modalities%20of%20patients%20are%0Aunavailable.%20Extensive%20experiments%20further%20demonstrate%20the%20versatility%20of%20our%0Amethod%2C%20providing%20unique%20insights%20into%20medical%20image%20translation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.12461v2&entry.124074799=Read"},
{"title": "Flow reconstruction in time-varying geometries using graph neural\n  networks", "author": "Bogdan A. Danciu and Vito A. Pagone and Benjamin B\u00f6hm and Marius Schmidt and Christos E. Frouzakis", "abstract": "  The paper presents a Graph Attention Convolutional Network (GACN) for flow\nreconstruction from very sparse data in time-varying geometries. The model\nincorporates a feature propagation algorithm as a preprocessing step to handle\nextremely sparse inputs, leveraging information from neighboring nodes to\ninitialize missing features. In addition, a binary indicator is introduced as a\nvalidity mask to distinguish between the original and propagated data points,\nenabling more effective learning from sparse inputs. Trained on a unique data\nset of Direct Numerical Simulations (DNS) of a motored engine at a technically\nrelevant operating condition, the GACN shows robust performance across\ndifferent resolutions and domain sizes and can effectively handle unstructured\ndata and variable input sizes. The model is tested on previously unseen DNS\ndata as well as on an experimental data set from Particle Image Velocimetry\n(PIV) measurements that were not considered during training. A comparative\nanalysis shows that the GACN consistently outperforms both a conventional\nConvolutional Neural Network (CNN) and cubic interpolation methods on the DNS\nand PIV test sets by achieving lower reconstruction errors and better capturing\nfine-scale turbulent structures. In particular, the GACN effectively\nreconstructs flow fields from domains up to 14 times larger than those observed\nduring training, with the performance advantage increasing for larger domains.\n", "link": "http://arxiv.org/abs/2411.08764v1", "date": "2024-11-13", "relevancy": 2.1609, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5994}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5578}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Flow%20reconstruction%20in%20time-varying%20geometries%20using%20graph%20neural%0A%20%20networks&body=Title%3A%20Flow%20reconstruction%20in%20time-varying%20geometries%20using%20graph%20neural%0A%20%20networks%0AAuthor%3A%20Bogdan%20A.%20Danciu%20and%20Vito%20A.%20Pagone%20and%20Benjamin%20B%C3%B6hm%20and%20Marius%20Schmidt%20and%20Christos%20E.%20Frouzakis%0AAbstract%3A%20%20%20The%20paper%20presents%20a%20Graph%20Attention%20Convolutional%20Network%20%28GACN%29%20for%20flow%0Areconstruction%20from%20very%20sparse%20data%20in%20time-varying%20geometries.%20The%20model%0Aincorporates%20a%20feature%20propagation%20algorithm%20as%20a%20preprocessing%20step%20to%20handle%0Aextremely%20sparse%20inputs%2C%20leveraging%20information%20from%20neighboring%20nodes%20to%0Ainitialize%20missing%20features.%20In%20addition%2C%20a%20binary%20indicator%20is%20introduced%20as%20a%0Avalidity%20mask%20to%20distinguish%20between%20the%20original%20and%20propagated%20data%20points%2C%0Aenabling%20more%20effective%20learning%20from%20sparse%20inputs.%20Trained%20on%20a%20unique%20data%0Aset%20of%20Direct%20Numerical%20Simulations%20%28DNS%29%20of%20a%20motored%20engine%20at%20a%20technically%0Arelevant%20operating%20condition%2C%20the%20GACN%20shows%20robust%20performance%20across%0Adifferent%20resolutions%20and%20domain%20sizes%20and%20can%20effectively%20handle%20unstructured%0Adata%20and%20variable%20input%20sizes.%20The%20model%20is%20tested%20on%20previously%20unseen%20DNS%0Adata%20as%20well%20as%20on%20an%20experimental%20data%20set%20from%20Particle%20Image%20Velocimetry%0A%28PIV%29%20measurements%20that%20were%20not%20considered%20during%20training.%20A%20comparative%0Aanalysis%20shows%20that%20the%20GACN%20consistently%20outperforms%20both%20a%20conventional%0AConvolutional%20Neural%20Network%20%28CNN%29%20and%20cubic%20interpolation%20methods%20on%20the%20DNS%0Aand%20PIV%20test%20sets%20by%20achieving%20lower%20reconstruction%20errors%20and%20better%20capturing%0Afine-scale%20turbulent%20structures.%20In%20particular%2C%20the%20GACN%20effectively%0Areconstructs%20flow%20fields%20from%20domains%20up%20to%2014%20times%20larger%20than%20those%20observed%0Aduring%20training%2C%20with%20the%20performance%20advantage%20increasing%20for%20larger%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08764v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlow%2520reconstruction%2520in%2520time-varying%2520geometries%2520using%2520graph%2520neural%250A%2520%2520networks%26entry.906535625%3DBogdan%2520A.%2520Danciu%2520and%2520Vito%2520A.%2520Pagone%2520and%2520Benjamin%2520B%25C3%25B6hm%2520and%2520Marius%2520Schmidt%2520and%2520Christos%2520E.%2520Frouzakis%26entry.1292438233%3D%2520%2520The%2520paper%2520presents%2520a%2520Graph%2520Attention%2520Convolutional%2520Network%2520%2528GACN%2529%2520for%2520flow%250Areconstruction%2520from%2520very%2520sparse%2520data%2520in%2520time-varying%2520geometries.%2520The%2520model%250Aincorporates%2520a%2520feature%2520propagation%2520algorithm%2520as%2520a%2520preprocessing%2520step%2520to%2520handle%250Aextremely%2520sparse%2520inputs%252C%2520leveraging%2520information%2520from%2520neighboring%2520nodes%2520to%250Ainitialize%2520missing%2520features.%2520In%2520addition%252C%2520a%2520binary%2520indicator%2520is%2520introduced%2520as%2520a%250Avalidity%2520mask%2520to%2520distinguish%2520between%2520the%2520original%2520and%2520propagated%2520data%2520points%252C%250Aenabling%2520more%2520effective%2520learning%2520from%2520sparse%2520inputs.%2520Trained%2520on%2520a%2520unique%2520data%250Aset%2520of%2520Direct%2520Numerical%2520Simulations%2520%2528DNS%2529%2520of%2520a%2520motored%2520engine%2520at%2520a%2520technically%250Arelevant%2520operating%2520condition%252C%2520the%2520GACN%2520shows%2520robust%2520performance%2520across%250Adifferent%2520resolutions%2520and%2520domain%2520sizes%2520and%2520can%2520effectively%2520handle%2520unstructured%250Adata%2520and%2520variable%2520input%2520sizes.%2520The%2520model%2520is%2520tested%2520on%2520previously%2520unseen%2520DNS%250Adata%2520as%2520well%2520as%2520on%2520an%2520experimental%2520data%2520set%2520from%2520Particle%2520Image%2520Velocimetry%250A%2528PIV%2529%2520measurements%2520that%2520were%2520not%2520considered%2520during%2520training.%2520A%2520comparative%250Aanalysis%2520shows%2520that%2520the%2520GACN%2520consistently%2520outperforms%2520both%2520a%2520conventional%250AConvolutional%2520Neural%2520Network%2520%2528CNN%2529%2520and%2520cubic%2520interpolation%2520methods%2520on%2520the%2520DNS%250Aand%2520PIV%2520test%2520sets%2520by%2520achieving%2520lower%2520reconstruction%2520errors%2520and%2520better%2520capturing%250Afine-scale%2520turbulent%2520structures.%2520In%2520particular%252C%2520the%2520GACN%2520effectively%250Areconstructs%2520flow%2520fields%2520from%2520domains%2520up%2520to%252014%2520times%2520larger%2520than%2520those%2520observed%250Aduring%2520training%252C%2520with%2520the%2520performance%2520advantage%2520increasing%2520for%2520larger%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08764v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Flow%20reconstruction%20in%20time-varying%20geometries%20using%20graph%20neural%0A%20%20networks&entry.906535625=Bogdan%20A.%20Danciu%20and%20Vito%20A.%20Pagone%20and%20Benjamin%20B%C3%B6hm%20and%20Marius%20Schmidt%20and%20Christos%20E.%20Frouzakis&entry.1292438233=%20%20The%20paper%20presents%20a%20Graph%20Attention%20Convolutional%20Network%20%28GACN%29%20for%20flow%0Areconstruction%20from%20very%20sparse%20data%20in%20time-varying%20geometries.%20The%20model%0Aincorporates%20a%20feature%20propagation%20algorithm%20as%20a%20preprocessing%20step%20to%20handle%0Aextremely%20sparse%20inputs%2C%20leveraging%20information%20from%20neighboring%20nodes%20to%0Ainitialize%20missing%20features.%20In%20addition%2C%20a%20binary%20indicator%20is%20introduced%20as%20a%0Avalidity%20mask%20to%20distinguish%20between%20the%20original%20and%20propagated%20data%20points%2C%0Aenabling%20more%20effective%20learning%20from%20sparse%20inputs.%20Trained%20on%20a%20unique%20data%0Aset%20of%20Direct%20Numerical%20Simulations%20%28DNS%29%20of%20a%20motored%20engine%20at%20a%20technically%0Arelevant%20operating%20condition%2C%20the%20GACN%20shows%20robust%20performance%20across%0Adifferent%20resolutions%20and%20domain%20sizes%20and%20can%20effectively%20handle%20unstructured%0Adata%20and%20variable%20input%20sizes.%20The%20model%20is%20tested%20on%20previously%20unseen%20DNS%0Adata%20as%20well%20as%20on%20an%20experimental%20data%20set%20from%20Particle%20Image%20Velocimetry%0A%28PIV%29%20measurements%20that%20were%20not%20considered%20during%20training.%20A%20comparative%0Aanalysis%20shows%20that%20the%20GACN%20consistently%20outperforms%20both%20a%20conventional%0AConvolutional%20Neural%20Network%20%28CNN%29%20and%20cubic%20interpolation%20methods%20on%20the%20DNS%0Aand%20PIV%20test%20sets%20by%20achieving%20lower%20reconstruction%20errors%20and%20better%20capturing%0Afine-scale%20turbulent%20structures.%20In%20particular%2C%20the%20GACN%20effectively%0Areconstructs%20flow%20fields%20from%20domains%20up%20to%2014%20times%20larger%20than%20those%20observed%0Aduring%20training%2C%20with%20the%20performance%20advantage%20increasing%20for%20larger%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08764v1&entry.124074799=Read"},
{"title": "UIFormer: A Unified Transformer-based Framework for Incremental Few-Shot\n  Object Detection and Instance Segmentation", "author": "Chengyuan Zhang and Yilin Zhang and Lei Zhu and Deyin Liu and Lin Wu and Bo Li and Shichao Zhang and Mohammed Bennamoun and Farid Boussaid", "abstract": "  This paper introduces a novel framework for unified incremental few-shot\nobject detection (iFSOD) and instance segmentation (iFSIS) using the\nTransformer architecture. Our goal is to create an optimal solution for\nsituations where only a few examples of novel object classes are available,\nwith no access to training data for base or old classes, while maintaining high\nperformance across both base and novel classes. To achieve this, We extend\nMask-DINO into a two-stage incremental learning framework. Stage 1 focuses on\noptimizing the model using the base dataset, while Stage 2 involves fine-tuning\nthe model on novel classes. Besides, we incorporate a classifier selection\nstrategy that assigns appropriate classifiers to the encoder and decoder\naccording to their distinct functions. Empirical evidence indicates that this\napproach effectively mitigates the over-fitting on novel classes learning.\nFurthermore, we implement knowledge distillation to prevent catastrophic\nforgetting of base classes. Comprehensive evaluations on the COCO and LVIS\ndatasets for both iFSIS and iFSOD tasks demonstrate that our method\nsignificantly outperforms state-of-the-art approaches.\n", "link": "http://arxiv.org/abs/2411.08569v1", "date": "2024-11-13", "relevancy": 2.1559, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5391}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5391}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UIFormer%3A%20A%20Unified%20Transformer-based%20Framework%20for%20Incremental%20Few-Shot%0A%20%20Object%20Detection%20and%20Instance%20Segmentation&body=Title%3A%20UIFormer%3A%20A%20Unified%20Transformer-based%20Framework%20for%20Incremental%20Few-Shot%0A%20%20Object%20Detection%20and%20Instance%20Segmentation%0AAuthor%3A%20Chengyuan%20Zhang%20and%20Yilin%20Zhang%20and%20Lei%20Zhu%20and%20Deyin%20Liu%20and%20Lin%20Wu%20and%20Bo%20Li%20and%20Shichao%20Zhang%20and%20Mohammed%20Bennamoun%20and%20Farid%20Boussaid%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20framework%20for%20unified%20incremental%20few-shot%0Aobject%20detection%20%28iFSOD%29%20and%20instance%20segmentation%20%28iFSIS%29%20using%20the%0ATransformer%20architecture.%20Our%20goal%20is%20to%20create%20an%20optimal%20solution%20for%0Asituations%20where%20only%20a%20few%20examples%20of%20novel%20object%20classes%20are%20available%2C%0Awith%20no%20access%20to%20training%20data%20for%20base%20or%20old%20classes%2C%20while%20maintaining%20high%0Aperformance%20across%20both%20base%20and%20novel%20classes.%20To%20achieve%20this%2C%20We%20extend%0AMask-DINO%20into%20a%20two-stage%20incremental%20learning%20framework.%20Stage%201%20focuses%20on%0Aoptimizing%20the%20model%20using%20the%20base%20dataset%2C%20while%20Stage%202%20involves%20fine-tuning%0Athe%20model%20on%20novel%20classes.%20Besides%2C%20we%20incorporate%20a%20classifier%20selection%0Astrategy%20that%20assigns%20appropriate%20classifiers%20to%20the%20encoder%20and%20decoder%0Aaccording%20to%20their%20distinct%20functions.%20Empirical%20evidence%20indicates%20that%20this%0Aapproach%20effectively%20mitigates%20the%20over-fitting%20on%20novel%20classes%20learning.%0AFurthermore%2C%20we%20implement%20knowledge%20distillation%20to%20prevent%20catastrophic%0Aforgetting%20of%20base%20classes.%20Comprehensive%20evaluations%20on%20the%20COCO%20and%20LVIS%0Adatasets%20for%20both%20iFSIS%20and%20iFSOD%20tasks%20demonstrate%20that%20our%20method%0Asignificantly%20outperforms%20state-of-the-art%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08569v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUIFormer%253A%2520A%2520Unified%2520Transformer-based%2520Framework%2520for%2520Incremental%2520Few-Shot%250A%2520%2520Object%2520Detection%2520and%2520Instance%2520Segmentation%26entry.906535625%3DChengyuan%2520Zhang%2520and%2520Yilin%2520Zhang%2520and%2520Lei%2520Zhu%2520and%2520Deyin%2520Liu%2520and%2520Lin%2520Wu%2520and%2520Bo%2520Li%2520and%2520Shichao%2520Zhang%2520and%2520Mohammed%2520Bennamoun%2520and%2520Farid%2520Boussaid%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520framework%2520for%2520unified%2520incremental%2520few-shot%250Aobject%2520detection%2520%2528iFSOD%2529%2520and%2520instance%2520segmentation%2520%2528iFSIS%2529%2520using%2520the%250ATransformer%2520architecture.%2520Our%2520goal%2520is%2520to%2520create%2520an%2520optimal%2520solution%2520for%250Asituations%2520where%2520only%2520a%2520few%2520examples%2520of%2520novel%2520object%2520classes%2520are%2520available%252C%250Awith%2520no%2520access%2520to%2520training%2520data%2520for%2520base%2520or%2520old%2520classes%252C%2520while%2520maintaining%2520high%250Aperformance%2520across%2520both%2520base%2520and%2520novel%2520classes.%2520To%2520achieve%2520this%252C%2520We%2520extend%250AMask-DINO%2520into%2520a%2520two-stage%2520incremental%2520learning%2520framework.%2520Stage%25201%2520focuses%2520on%250Aoptimizing%2520the%2520model%2520using%2520the%2520base%2520dataset%252C%2520while%2520Stage%25202%2520involves%2520fine-tuning%250Athe%2520model%2520on%2520novel%2520classes.%2520Besides%252C%2520we%2520incorporate%2520a%2520classifier%2520selection%250Astrategy%2520that%2520assigns%2520appropriate%2520classifiers%2520to%2520the%2520encoder%2520and%2520decoder%250Aaccording%2520to%2520their%2520distinct%2520functions.%2520Empirical%2520evidence%2520indicates%2520that%2520this%250Aapproach%2520effectively%2520mitigates%2520the%2520over-fitting%2520on%2520novel%2520classes%2520learning.%250AFurthermore%252C%2520we%2520implement%2520knowledge%2520distillation%2520to%2520prevent%2520catastrophic%250Aforgetting%2520of%2520base%2520classes.%2520Comprehensive%2520evaluations%2520on%2520the%2520COCO%2520and%2520LVIS%250Adatasets%2520for%2520both%2520iFSIS%2520and%2520iFSOD%2520tasks%2520demonstrate%2520that%2520our%2520method%250Asignificantly%2520outperforms%2520state-of-the-art%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08569v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UIFormer%3A%20A%20Unified%20Transformer-based%20Framework%20for%20Incremental%20Few-Shot%0A%20%20Object%20Detection%20and%20Instance%20Segmentation&entry.906535625=Chengyuan%20Zhang%20and%20Yilin%20Zhang%20and%20Lei%20Zhu%20and%20Deyin%20Liu%20and%20Lin%20Wu%20and%20Bo%20Li%20and%20Shichao%20Zhang%20and%20Mohammed%20Bennamoun%20and%20Farid%20Boussaid&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20framework%20for%20unified%20incremental%20few-shot%0Aobject%20detection%20%28iFSOD%29%20and%20instance%20segmentation%20%28iFSIS%29%20using%20the%0ATransformer%20architecture.%20Our%20goal%20is%20to%20create%20an%20optimal%20solution%20for%0Asituations%20where%20only%20a%20few%20examples%20of%20novel%20object%20classes%20are%20available%2C%0Awith%20no%20access%20to%20training%20data%20for%20base%20or%20old%20classes%2C%20while%20maintaining%20high%0Aperformance%20across%20both%20base%20and%20novel%20classes.%20To%20achieve%20this%2C%20We%20extend%0AMask-DINO%20into%20a%20two-stage%20incremental%20learning%20framework.%20Stage%201%20focuses%20on%0Aoptimizing%20the%20model%20using%20the%20base%20dataset%2C%20while%20Stage%202%20involves%20fine-tuning%0Athe%20model%20on%20novel%20classes.%20Besides%2C%20we%20incorporate%20a%20classifier%20selection%0Astrategy%20that%20assigns%20appropriate%20classifiers%20to%20the%20encoder%20and%20decoder%0Aaccording%20to%20their%20distinct%20functions.%20Empirical%20evidence%20indicates%20that%20this%0Aapproach%20effectively%20mitigates%20the%20over-fitting%20on%20novel%20classes%20learning.%0AFurthermore%2C%20we%20implement%20knowledge%20distillation%20to%20prevent%20catastrophic%0Aforgetting%20of%20base%20classes.%20Comprehensive%20evaluations%20on%20the%20COCO%20and%20LVIS%0Adatasets%20for%20both%20iFSIS%20and%20iFSOD%20tasks%20demonstrate%20that%20our%20method%0Asignificantly%20outperforms%20state-of-the-art%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08569v1&entry.124074799=Read"},
{"title": "Slender Object Scene Segmentation in Remote Sensing Image Based on\n  Learnable Morphological Skeleton with Segment Anything Model", "author": "Jun Xie and Wenxiao Li and Faqiang Wang and Liqiang Zhang and Zhengyang Hou and Jun Liu", "abstract": "  Morphological methods play a crucial role in remote sensing image processing,\ndue to their ability to capture and preserve small structural details. However,\nmost of the existing deep learning models for semantic segmentation are based\non the encoder-decoder architecture including U-net and Segment Anything Model\n(SAM), where the downsampling process tends to discard fine details. In this\npaper, we propose a new approach that integrates learnable morphological\nskeleton prior into deep neural networks using the variational method. To\naddress the difficulty in backpropagation in neural networks caused by the\nnon-differentiability presented in classical morphological operations, we\nprovide a smooth representation of the morphological skeleton and design a\nvariational segmentation model integrating morphological skeleton prior by\nemploying operator splitting and dual methods. Then, we integrate this model\ninto the network architecture of SAM, which is achieved by adding a token to\nmask decoder and modifying the final sigmoid layer, ensuring the final\nsegmentation results preserve the skeleton structure as much as possible.\nExperimental results on remote sensing datasets, including buildings and roads,\ndemonstrate that our method outperforms the original SAM on slender object\nsegmentation and exhibits better generalization capability.\n", "link": "http://arxiv.org/abs/2411.08592v1", "date": "2024-11-13", "relevancy": 2.1303, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5346}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5343}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Slender%20Object%20Scene%20Segmentation%20in%20Remote%20Sensing%20Image%20Based%20on%0A%20%20Learnable%20Morphological%20Skeleton%20with%20Segment%20Anything%20Model&body=Title%3A%20Slender%20Object%20Scene%20Segmentation%20in%20Remote%20Sensing%20Image%20Based%20on%0A%20%20Learnable%20Morphological%20Skeleton%20with%20Segment%20Anything%20Model%0AAuthor%3A%20Jun%20Xie%20and%20Wenxiao%20Li%20and%20Faqiang%20Wang%20and%20Liqiang%20Zhang%20and%20Zhengyang%20Hou%20and%20Jun%20Liu%0AAbstract%3A%20%20%20Morphological%20methods%20play%20a%20crucial%20role%20in%20remote%20sensing%20image%20processing%2C%0Adue%20to%20their%20ability%20to%20capture%20and%20preserve%20small%20structural%20details.%20However%2C%0Amost%20of%20the%20existing%20deep%20learning%20models%20for%20semantic%20segmentation%20are%20based%0Aon%20the%20encoder-decoder%20architecture%20including%20U-net%20and%20Segment%20Anything%20Model%0A%28SAM%29%2C%20where%20the%20downsampling%20process%20tends%20to%20discard%20fine%20details.%20In%20this%0Apaper%2C%20we%20propose%20a%20new%20approach%20that%20integrates%20learnable%20morphological%0Askeleton%20prior%20into%20deep%20neural%20networks%20using%20the%20variational%20method.%20To%0Aaddress%20the%20difficulty%20in%20backpropagation%20in%20neural%20networks%20caused%20by%20the%0Anon-differentiability%20presented%20in%20classical%20morphological%20operations%2C%20we%0Aprovide%20a%20smooth%20representation%20of%20the%20morphological%20skeleton%20and%20design%20a%0Avariational%20segmentation%20model%20integrating%20morphological%20skeleton%20prior%20by%0Aemploying%20operator%20splitting%20and%20dual%20methods.%20Then%2C%20we%20integrate%20this%20model%0Ainto%20the%20network%20architecture%20of%20SAM%2C%20which%20is%20achieved%20by%20adding%20a%20token%20to%0Amask%20decoder%20and%20modifying%20the%20final%20sigmoid%20layer%2C%20ensuring%20the%20final%0Asegmentation%20results%20preserve%20the%20skeleton%20structure%20as%20much%20as%20possible.%0AExperimental%20results%20on%20remote%20sensing%20datasets%2C%20including%20buildings%20and%20roads%2C%0Ademonstrate%20that%20our%20method%20outperforms%20the%20original%20SAM%20on%20slender%20object%0Asegmentation%20and%20exhibits%20better%20generalization%20capability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08592v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSlender%2520Object%2520Scene%2520Segmentation%2520in%2520Remote%2520Sensing%2520Image%2520Based%2520on%250A%2520%2520Learnable%2520Morphological%2520Skeleton%2520with%2520Segment%2520Anything%2520Model%26entry.906535625%3DJun%2520Xie%2520and%2520Wenxiao%2520Li%2520and%2520Faqiang%2520Wang%2520and%2520Liqiang%2520Zhang%2520and%2520Zhengyang%2520Hou%2520and%2520Jun%2520Liu%26entry.1292438233%3D%2520%2520Morphological%2520methods%2520play%2520a%2520crucial%2520role%2520in%2520remote%2520sensing%2520image%2520processing%252C%250Adue%2520to%2520their%2520ability%2520to%2520capture%2520and%2520preserve%2520small%2520structural%2520details.%2520However%252C%250Amost%2520of%2520the%2520existing%2520deep%2520learning%2520models%2520for%2520semantic%2520segmentation%2520are%2520based%250Aon%2520the%2520encoder-decoder%2520architecture%2520including%2520U-net%2520and%2520Segment%2520Anything%2520Model%250A%2528SAM%2529%252C%2520where%2520the%2520downsampling%2520process%2520tends%2520to%2520discard%2520fine%2520details.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520new%2520approach%2520that%2520integrates%2520learnable%2520morphological%250Askeleton%2520prior%2520into%2520deep%2520neural%2520networks%2520using%2520the%2520variational%2520method.%2520To%250Aaddress%2520the%2520difficulty%2520in%2520backpropagation%2520in%2520neural%2520networks%2520caused%2520by%2520the%250Anon-differentiability%2520presented%2520in%2520classical%2520morphological%2520operations%252C%2520we%250Aprovide%2520a%2520smooth%2520representation%2520of%2520the%2520morphological%2520skeleton%2520and%2520design%2520a%250Avariational%2520segmentation%2520model%2520integrating%2520morphological%2520skeleton%2520prior%2520by%250Aemploying%2520operator%2520splitting%2520and%2520dual%2520methods.%2520Then%252C%2520we%2520integrate%2520this%2520model%250Ainto%2520the%2520network%2520architecture%2520of%2520SAM%252C%2520which%2520is%2520achieved%2520by%2520adding%2520a%2520token%2520to%250Amask%2520decoder%2520and%2520modifying%2520the%2520final%2520sigmoid%2520layer%252C%2520ensuring%2520the%2520final%250Asegmentation%2520results%2520preserve%2520the%2520skeleton%2520structure%2520as%2520much%2520as%2520possible.%250AExperimental%2520results%2520on%2520remote%2520sensing%2520datasets%252C%2520including%2520buildings%2520and%2520roads%252C%250Ademonstrate%2520that%2520our%2520method%2520outperforms%2520the%2520original%2520SAM%2520on%2520slender%2520object%250Asegmentation%2520and%2520exhibits%2520better%2520generalization%2520capability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08592v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Slender%20Object%20Scene%20Segmentation%20in%20Remote%20Sensing%20Image%20Based%20on%0A%20%20Learnable%20Morphological%20Skeleton%20with%20Segment%20Anything%20Model&entry.906535625=Jun%20Xie%20and%20Wenxiao%20Li%20and%20Faqiang%20Wang%20and%20Liqiang%20Zhang%20and%20Zhengyang%20Hou%20and%20Jun%20Liu&entry.1292438233=%20%20Morphological%20methods%20play%20a%20crucial%20role%20in%20remote%20sensing%20image%20processing%2C%0Adue%20to%20their%20ability%20to%20capture%20and%20preserve%20small%20structural%20details.%20However%2C%0Amost%20of%20the%20existing%20deep%20learning%20models%20for%20semantic%20segmentation%20are%20based%0Aon%20the%20encoder-decoder%20architecture%20including%20U-net%20and%20Segment%20Anything%20Model%0A%28SAM%29%2C%20where%20the%20downsampling%20process%20tends%20to%20discard%20fine%20details.%20In%20this%0Apaper%2C%20we%20propose%20a%20new%20approach%20that%20integrates%20learnable%20morphological%0Askeleton%20prior%20into%20deep%20neural%20networks%20using%20the%20variational%20method.%20To%0Aaddress%20the%20difficulty%20in%20backpropagation%20in%20neural%20networks%20caused%20by%20the%0Anon-differentiability%20presented%20in%20classical%20morphological%20operations%2C%20we%0Aprovide%20a%20smooth%20representation%20of%20the%20morphological%20skeleton%20and%20design%20a%0Avariational%20segmentation%20model%20integrating%20morphological%20skeleton%20prior%20by%0Aemploying%20operator%20splitting%20and%20dual%20methods.%20Then%2C%20we%20integrate%20this%20model%0Ainto%20the%20network%20architecture%20of%20SAM%2C%20which%20is%20achieved%20by%20adding%20a%20token%20to%0Amask%20decoder%20and%20modifying%20the%20final%20sigmoid%20layer%2C%20ensuring%20the%20final%0Asegmentation%20results%20preserve%20the%20skeleton%20structure%20as%20much%20as%20possible.%0AExperimental%20results%20on%20remote%20sensing%20datasets%2C%20including%20buildings%20and%20roads%2C%0Ademonstrate%20that%20our%20method%20outperforms%20the%20original%20SAM%20on%20slender%20object%0Asegmentation%20and%20exhibits%20better%20generalization%20capability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08592v1&entry.124074799=Read"},
{"title": "ScaleNet: Scale Invariance Learning in Directed Graphs", "author": "Qin Jiang and Chengjia Wang and Michael Lones and Wei Pang", "abstract": "  Graph Neural Networks (GNNs) have advanced relational data analysis but lack\ninvariance learning techniques common in image classification. In node\nclassification with GNNs, it is actually the ego-graph of the center node that\nis classified. This research extends the scale invariance concept to node\nclassification by drawing an analogy to image processing: just as scale\ninvariance being used in image classification to capture multi-scale features,\nwe propose the concept of ``scaled ego-graphs''. Scaled ego-graphs generalize\ntraditional ego-graphs by replacing undirected single-edges with\n``scaled-edges'', which are ordered sequences of multiple directed edges. We\nempirically assess the performance of the proposed scale invariance in graphs\non seven benchmark datasets, across both homophilic and heterophilic\nstructures. Our scale-invariance-based graph learning outperforms inception\nmodels derived from random walks by being simpler, faster, and more accurate.\nThe scale invariance explains inception models' success on homophilic graphs\nand limitations on heterophilic graphs. To ensure applicability of inception\nmodel to heterophilic graphs as well, we further present ScaleNet, an\narchitecture that leverages multi-scaled features. ScaleNet achieves\nstate-of-the-art results on five out of seven datasets (four homophilic and one\nheterophilic) and matches top performance on the remaining two, demonstrating\nits excellent applicability. This represents a significant advance in graph\nlearning, offering a unified framework that enhances node classification across\nvarious graph types. Our code is available at\nhttps://github.com/Qin87/ScaleNet/tree/July25.\n", "link": "http://arxiv.org/abs/2411.08758v1", "date": "2024-11-13", "relevancy": 2.1205, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5347}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5302}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ScaleNet%3A%20Scale%20Invariance%20Learning%20in%20Directed%20Graphs&body=Title%3A%20ScaleNet%3A%20Scale%20Invariance%20Learning%20in%20Directed%20Graphs%0AAuthor%3A%20Qin%20Jiang%20and%20Chengjia%20Wang%20and%20Michael%20Lones%20and%20Wei%20Pang%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20advanced%20relational%20data%20analysis%20but%20lack%0Ainvariance%20learning%20techniques%20common%20in%20image%20classification.%20In%20node%0Aclassification%20with%20GNNs%2C%20it%20is%20actually%20the%20ego-graph%20of%20the%20center%20node%20that%0Ais%20classified.%20This%20research%20extends%20the%20scale%20invariance%20concept%20to%20node%0Aclassification%20by%20drawing%20an%20analogy%20to%20image%20processing%3A%20just%20as%20scale%0Ainvariance%20being%20used%20in%20image%20classification%20to%20capture%20multi-scale%20features%2C%0Awe%20propose%20the%20concept%20of%20%60%60scaled%20ego-graphs%27%27.%20Scaled%20ego-graphs%20generalize%0Atraditional%20ego-graphs%20by%20replacing%20undirected%20single-edges%20with%0A%60%60scaled-edges%27%27%2C%20which%20are%20ordered%20sequences%20of%20multiple%20directed%20edges.%20We%0Aempirically%20assess%20the%20performance%20of%20the%20proposed%20scale%20invariance%20in%20graphs%0Aon%20seven%20benchmark%20datasets%2C%20across%20both%20homophilic%20and%20heterophilic%0Astructures.%20Our%20scale-invariance-based%20graph%20learning%20outperforms%20inception%0Amodels%20derived%20from%20random%20walks%20by%20being%20simpler%2C%20faster%2C%20and%20more%20accurate.%0AThe%20scale%20invariance%20explains%20inception%20models%27%20success%20on%20homophilic%20graphs%0Aand%20limitations%20on%20heterophilic%20graphs.%20To%20ensure%20applicability%20of%20inception%0Amodel%20to%20heterophilic%20graphs%20as%20well%2C%20we%20further%20present%20ScaleNet%2C%20an%0Aarchitecture%20that%20leverages%20multi-scaled%20features.%20ScaleNet%20achieves%0Astate-of-the-art%20results%20on%20five%20out%20of%20seven%20datasets%20%28four%20homophilic%20and%20one%0Aheterophilic%29%20and%20matches%20top%20performance%20on%20the%20remaining%20two%2C%20demonstrating%0Aits%20excellent%20applicability.%20This%20represents%20a%20significant%20advance%20in%20graph%0Alearning%2C%20offering%20a%20unified%20framework%20that%20enhances%20node%20classification%20across%0Avarious%20graph%20types.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Qin87/ScaleNet/tree/July25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08758v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScaleNet%253A%2520Scale%2520Invariance%2520Learning%2520in%2520Directed%2520Graphs%26entry.906535625%3DQin%2520Jiang%2520and%2520Chengjia%2520Wang%2520and%2520Michael%2520Lones%2520and%2520Wei%2520Pang%26entry.1292438233%3D%2520%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520have%2520advanced%2520relational%2520data%2520analysis%2520but%2520lack%250Ainvariance%2520learning%2520techniques%2520common%2520in%2520image%2520classification.%2520In%2520node%250Aclassification%2520with%2520GNNs%252C%2520it%2520is%2520actually%2520the%2520ego-graph%2520of%2520the%2520center%2520node%2520that%250Ais%2520classified.%2520This%2520research%2520extends%2520the%2520scale%2520invariance%2520concept%2520to%2520node%250Aclassification%2520by%2520drawing%2520an%2520analogy%2520to%2520image%2520processing%253A%2520just%2520as%2520scale%250Ainvariance%2520being%2520used%2520in%2520image%2520classification%2520to%2520capture%2520multi-scale%2520features%252C%250Awe%2520propose%2520the%2520concept%2520of%2520%2560%2560scaled%2520ego-graphs%2527%2527.%2520Scaled%2520ego-graphs%2520generalize%250Atraditional%2520ego-graphs%2520by%2520replacing%2520undirected%2520single-edges%2520with%250A%2560%2560scaled-edges%2527%2527%252C%2520which%2520are%2520ordered%2520sequences%2520of%2520multiple%2520directed%2520edges.%2520We%250Aempirically%2520assess%2520the%2520performance%2520of%2520the%2520proposed%2520scale%2520invariance%2520in%2520graphs%250Aon%2520seven%2520benchmark%2520datasets%252C%2520across%2520both%2520homophilic%2520and%2520heterophilic%250Astructures.%2520Our%2520scale-invariance-based%2520graph%2520learning%2520outperforms%2520inception%250Amodels%2520derived%2520from%2520random%2520walks%2520by%2520being%2520simpler%252C%2520faster%252C%2520and%2520more%2520accurate.%250AThe%2520scale%2520invariance%2520explains%2520inception%2520models%2527%2520success%2520on%2520homophilic%2520graphs%250Aand%2520limitations%2520on%2520heterophilic%2520graphs.%2520To%2520ensure%2520applicability%2520of%2520inception%250Amodel%2520to%2520heterophilic%2520graphs%2520as%2520well%252C%2520we%2520further%2520present%2520ScaleNet%252C%2520an%250Aarchitecture%2520that%2520leverages%2520multi-scaled%2520features.%2520ScaleNet%2520achieves%250Astate-of-the-art%2520results%2520on%2520five%2520out%2520of%2520seven%2520datasets%2520%2528four%2520homophilic%2520and%2520one%250Aheterophilic%2529%2520and%2520matches%2520top%2520performance%2520on%2520the%2520remaining%2520two%252C%2520demonstrating%250Aits%2520excellent%2520applicability.%2520This%2520represents%2520a%2520significant%2520advance%2520in%2520graph%250Alearning%252C%2520offering%2520a%2520unified%2520framework%2520that%2520enhances%2520node%2520classification%2520across%250Avarious%2520graph%2520types.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Qin87/ScaleNet/tree/July25.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08758v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ScaleNet%3A%20Scale%20Invariance%20Learning%20in%20Directed%20Graphs&entry.906535625=Qin%20Jiang%20and%20Chengjia%20Wang%20and%20Michael%20Lones%20and%20Wei%20Pang&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20advanced%20relational%20data%20analysis%20but%20lack%0Ainvariance%20learning%20techniques%20common%20in%20image%20classification.%20In%20node%0Aclassification%20with%20GNNs%2C%20it%20is%20actually%20the%20ego-graph%20of%20the%20center%20node%20that%0Ais%20classified.%20This%20research%20extends%20the%20scale%20invariance%20concept%20to%20node%0Aclassification%20by%20drawing%20an%20analogy%20to%20image%20processing%3A%20just%20as%20scale%0Ainvariance%20being%20used%20in%20image%20classification%20to%20capture%20multi-scale%20features%2C%0Awe%20propose%20the%20concept%20of%20%60%60scaled%20ego-graphs%27%27.%20Scaled%20ego-graphs%20generalize%0Atraditional%20ego-graphs%20by%20replacing%20undirected%20single-edges%20with%0A%60%60scaled-edges%27%27%2C%20which%20are%20ordered%20sequences%20of%20multiple%20directed%20edges.%20We%0Aempirically%20assess%20the%20performance%20of%20the%20proposed%20scale%20invariance%20in%20graphs%0Aon%20seven%20benchmark%20datasets%2C%20across%20both%20homophilic%20and%20heterophilic%0Astructures.%20Our%20scale-invariance-based%20graph%20learning%20outperforms%20inception%0Amodels%20derived%20from%20random%20walks%20by%20being%20simpler%2C%20faster%2C%20and%20more%20accurate.%0AThe%20scale%20invariance%20explains%20inception%20models%27%20success%20on%20homophilic%20graphs%0Aand%20limitations%20on%20heterophilic%20graphs.%20To%20ensure%20applicability%20of%20inception%0Amodel%20to%20heterophilic%20graphs%20as%20well%2C%20we%20further%20present%20ScaleNet%2C%20an%0Aarchitecture%20that%20leverages%20multi-scaled%20features.%20ScaleNet%20achieves%0Astate-of-the-art%20results%20on%20five%20out%20of%20seven%20datasets%20%28four%20homophilic%20and%20one%0Aheterophilic%29%20and%20matches%20top%20performance%20on%20the%20remaining%20two%2C%20demonstrating%0Aits%20excellent%20applicability.%20This%20represents%20a%20significant%20advance%20in%20graph%0Alearning%2C%20offering%20a%20unified%20framework%20that%20enhances%20node%20classification%20across%0Avarious%20graph%20types.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/Qin87/ScaleNet/tree/July25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08758v1&entry.124074799=Read"},
{"title": "Physics-Informed Geometry-Aware Neural Operator", "author": "Weiheng Zhong and Hadi Meidani", "abstract": "  Engineering design problems often involve solving parametric Partial\nDifferential Equations (PDEs) under variable PDE parameters and domain\ngeometry. Recently, neural operators have shown promise in learning PDE\noperators and quickly predicting the PDE solutions. However, training these\nneural operators typically requires large datasets, the acquisition of which\ncan be prohibitively expensive. To overcome this, physics-informed training\noffers an alternative way of building neural operators, eliminating the high\ncomputational costs associated with Finite Element generation of training data.\nNevertheless, current physics-informed neural operators struggle with\nlimitations, either in handling varying domain geometries or varying PDE\nparameters. In this research, we introduce a novel method, the Physics-Informed\nGeometry-Aware Neural Operator (PI-GANO), designed to simultaneously generalize\nacross both PDE parameters and domain geometries. We adopt a geometry encoder\nto capture the domain geometry features, and design a novel pipeline to\nintegrate this component within the existing DCON architecture. Numerical\nresults demonstrate the accuracy and efficiency of the proposed method. All the\ncodes and data related to this work are available on GitHub:\nhttps://github.com/WeihengZ/Physics-informed-Neural-Foundation-Operator.\n", "link": "http://arxiv.org/abs/2408.01600v3", "date": "2024-11-13", "relevancy": 2.1192, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5322}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5291}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Physics-Informed%20Geometry-Aware%20Neural%20Operator&body=Title%3A%20Physics-Informed%20Geometry-Aware%20Neural%20Operator%0AAuthor%3A%20Weiheng%20Zhong%20and%20Hadi%20Meidani%0AAbstract%3A%20%20%20Engineering%20design%20problems%20often%20involve%20solving%20parametric%20Partial%0ADifferential%20Equations%20%28PDEs%29%20under%20variable%20PDE%20parameters%20and%20domain%0Ageometry.%20Recently%2C%20neural%20operators%20have%20shown%20promise%20in%20learning%20PDE%0Aoperators%20and%20quickly%20predicting%20the%20PDE%20solutions.%20However%2C%20training%20these%0Aneural%20operators%20typically%20requires%20large%20datasets%2C%20the%20acquisition%20of%20which%0Acan%20be%20prohibitively%20expensive.%20To%20overcome%20this%2C%20physics-informed%20training%0Aoffers%20an%20alternative%20way%20of%20building%20neural%20operators%2C%20eliminating%20the%20high%0Acomputational%20costs%20associated%20with%20Finite%20Element%20generation%20of%20training%20data.%0ANevertheless%2C%20current%20physics-informed%20neural%20operators%20struggle%20with%0Alimitations%2C%20either%20in%20handling%20varying%20domain%20geometries%20or%20varying%20PDE%0Aparameters.%20In%20this%20research%2C%20we%20introduce%20a%20novel%20method%2C%20the%20Physics-Informed%0AGeometry-Aware%20Neural%20Operator%20%28PI-GANO%29%2C%20designed%20to%20simultaneously%20generalize%0Aacross%20both%20PDE%20parameters%20and%20domain%20geometries.%20We%20adopt%20a%20geometry%20encoder%0Ato%20capture%20the%20domain%20geometry%20features%2C%20and%20design%20a%20novel%20pipeline%20to%0Aintegrate%20this%20component%20within%20the%20existing%20DCON%20architecture.%20Numerical%0Aresults%20demonstrate%20the%20accuracy%20and%20efficiency%20of%20the%20proposed%20method.%20All%20the%0Acodes%20and%20data%20related%20to%20this%20work%20are%20available%20on%20GitHub%3A%0Ahttps%3A//github.com/WeihengZ/Physics-informed-Neural-Foundation-Operator.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01600v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysics-Informed%2520Geometry-Aware%2520Neural%2520Operator%26entry.906535625%3DWeiheng%2520Zhong%2520and%2520Hadi%2520Meidani%26entry.1292438233%3D%2520%2520Engineering%2520design%2520problems%2520often%2520involve%2520solving%2520parametric%2520Partial%250ADifferential%2520Equations%2520%2528PDEs%2529%2520under%2520variable%2520PDE%2520parameters%2520and%2520domain%250Ageometry.%2520Recently%252C%2520neural%2520operators%2520have%2520shown%2520promise%2520in%2520learning%2520PDE%250Aoperators%2520and%2520quickly%2520predicting%2520the%2520PDE%2520solutions.%2520However%252C%2520training%2520these%250Aneural%2520operators%2520typically%2520requires%2520large%2520datasets%252C%2520the%2520acquisition%2520of%2520which%250Acan%2520be%2520prohibitively%2520expensive.%2520To%2520overcome%2520this%252C%2520physics-informed%2520training%250Aoffers%2520an%2520alternative%2520way%2520of%2520building%2520neural%2520operators%252C%2520eliminating%2520the%2520high%250Acomputational%2520costs%2520associated%2520with%2520Finite%2520Element%2520generation%2520of%2520training%2520data.%250ANevertheless%252C%2520current%2520physics-informed%2520neural%2520operators%2520struggle%2520with%250Alimitations%252C%2520either%2520in%2520handling%2520varying%2520domain%2520geometries%2520or%2520varying%2520PDE%250Aparameters.%2520In%2520this%2520research%252C%2520we%2520introduce%2520a%2520novel%2520method%252C%2520the%2520Physics-Informed%250AGeometry-Aware%2520Neural%2520Operator%2520%2528PI-GANO%2529%252C%2520designed%2520to%2520simultaneously%2520generalize%250Aacross%2520both%2520PDE%2520parameters%2520and%2520domain%2520geometries.%2520We%2520adopt%2520a%2520geometry%2520encoder%250Ato%2520capture%2520the%2520domain%2520geometry%2520features%252C%2520and%2520design%2520a%2520novel%2520pipeline%2520to%250Aintegrate%2520this%2520component%2520within%2520the%2520existing%2520DCON%2520architecture.%2520Numerical%250Aresults%2520demonstrate%2520the%2520accuracy%2520and%2520efficiency%2520of%2520the%2520proposed%2520method.%2520All%2520the%250Acodes%2520and%2520data%2520related%2520to%2520this%2520work%2520are%2520available%2520on%2520GitHub%253A%250Ahttps%253A//github.com/WeihengZ/Physics-informed-Neural-Foundation-Operator.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01600v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-Informed%20Geometry-Aware%20Neural%20Operator&entry.906535625=Weiheng%20Zhong%20and%20Hadi%20Meidani&entry.1292438233=%20%20Engineering%20design%20problems%20often%20involve%20solving%20parametric%20Partial%0ADifferential%20Equations%20%28PDEs%29%20under%20variable%20PDE%20parameters%20and%20domain%0Ageometry.%20Recently%2C%20neural%20operators%20have%20shown%20promise%20in%20learning%20PDE%0Aoperators%20and%20quickly%20predicting%20the%20PDE%20solutions.%20However%2C%20training%20these%0Aneural%20operators%20typically%20requires%20large%20datasets%2C%20the%20acquisition%20of%20which%0Acan%20be%20prohibitively%20expensive.%20To%20overcome%20this%2C%20physics-informed%20training%0Aoffers%20an%20alternative%20way%20of%20building%20neural%20operators%2C%20eliminating%20the%20high%0Acomputational%20costs%20associated%20with%20Finite%20Element%20generation%20of%20training%20data.%0ANevertheless%2C%20current%20physics-informed%20neural%20operators%20struggle%20with%0Alimitations%2C%20either%20in%20handling%20varying%20domain%20geometries%20or%20varying%20PDE%0Aparameters.%20In%20this%20research%2C%20we%20introduce%20a%20novel%20method%2C%20the%20Physics-Informed%0AGeometry-Aware%20Neural%20Operator%20%28PI-GANO%29%2C%20designed%20to%20simultaneously%20generalize%0Aacross%20both%20PDE%20parameters%20and%20domain%20geometries.%20We%20adopt%20a%20geometry%20encoder%0Ato%20capture%20the%20domain%20geometry%20features%2C%20and%20design%20a%20novel%20pipeline%20to%0Aintegrate%20this%20component%20within%20the%20existing%20DCON%20architecture.%20Numerical%0Aresults%20demonstrate%20the%20accuracy%20and%20efficiency%20of%20the%20proposed%20method.%20All%20the%0Acodes%20and%20data%20related%20to%20this%20work%20are%20available%20on%20GitHub%3A%0Ahttps%3A//github.com/WeihengZ/Physics-informed-Neural-Foundation-Operator.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01600v3&entry.124074799=Read"},
{"title": "DeepUQ: Assessing the Aleatoric Uncertainties from two Deep Learning\n  Methods", "author": "Rebecca Nevin and Aleksandra \u0106iprijanovi\u0107 and Brian D. Nord", "abstract": "  Assessing the quality of aleatoric uncertainty estimates from uncertainty\nquantification (UQ) deep learning methods is important in scientific contexts,\nwhere uncertainty is physically meaningful and important to characterize and\ninterpret exactly. We systematically compare aleatoric uncertainty measured by\ntwo UQ techniques, Deep Ensembles (DE) and Deep Evidential Regression (DER).\nOur method focuses on both zero-dimensional (0D) and two-dimensional (2D) data,\nto explore how the UQ methods function for different data dimensionalities. We\ninvestigate uncertainty injected on the input and output variables and include\na method to propagate uncertainty in the case of input uncertainty so that we\ncan compare the predicted aleatoric uncertainty to the known values. We\nexperiment with three levels of noise. The aleatoric uncertainty predicted\nacross all models and experiments scales with the injected noise level.\nHowever, the predicted uncertainty is miscalibrated to $\\rm{std}(\\sigma_{\\rm\nal})$ with the true uncertainty for half of the DE experiments and almost all\nof the DER experiments. The predicted uncertainty is the least accurate for\nboth UQ methods for the 2D input uncertainty experiment and the high-noise\nlevel. While these results do not apply to more complex data, they highlight\nthat further research on post-facto calibration for these methods would be\nbeneficial, particularly for high-noise and high-dimensional settings.\n", "link": "http://arxiv.org/abs/2411.08587v1", "date": "2024-11-13", "relevancy": 2.0936, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6504}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5281}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeepUQ%3A%20Assessing%20the%20Aleatoric%20Uncertainties%20from%20two%20Deep%20Learning%0A%20%20Methods&body=Title%3A%20DeepUQ%3A%20Assessing%20the%20Aleatoric%20Uncertainties%20from%20two%20Deep%20Learning%0A%20%20Methods%0AAuthor%3A%20Rebecca%20Nevin%20and%20Aleksandra%20%C4%86iprijanovi%C4%87%20and%20Brian%20D.%20Nord%0AAbstract%3A%20%20%20Assessing%20the%20quality%20of%20aleatoric%20uncertainty%20estimates%20from%20uncertainty%0Aquantification%20%28UQ%29%20deep%20learning%20methods%20is%20important%20in%20scientific%20contexts%2C%0Awhere%20uncertainty%20is%20physically%20meaningful%20and%20important%20to%20characterize%20and%0Ainterpret%20exactly.%20We%20systematically%20compare%20aleatoric%20uncertainty%20measured%20by%0Atwo%20UQ%20techniques%2C%20Deep%20Ensembles%20%28DE%29%20and%20Deep%20Evidential%20Regression%20%28DER%29.%0AOur%20method%20focuses%20on%20both%20zero-dimensional%20%280D%29%20and%20two-dimensional%20%282D%29%20data%2C%0Ato%20explore%20how%20the%20UQ%20methods%20function%20for%20different%20data%20dimensionalities.%20We%0Ainvestigate%20uncertainty%20injected%20on%20the%20input%20and%20output%20variables%20and%20include%0Aa%20method%20to%20propagate%20uncertainty%20in%20the%20case%20of%20input%20uncertainty%20so%20that%20we%0Acan%20compare%20the%20predicted%20aleatoric%20uncertainty%20to%20the%20known%20values.%20We%0Aexperiment%20with%20three%20levels%20of%20noise.%20The%20aleatoric%20uncertainty%20predicted%0Aacross%20all%20models%20and%20experiments%20scales%20with%20the%20injected%20noise%20level.%0AHowever%2C%20the%20predicted%20uncertainty%20is%20miscalibrated%20to%20%24%5Crm%7Bstd%7D%28%5Csigma_%7B%5Crm%0Aal%7D%29%24%20with%20the%20true%20uncertainty%20for%20half%20of%20the%20DE%20experiments%20and%20almost%20all%0Aof%20the%20DER%20experiments.%20The%20predicted%20uncertainty%20is%20the%20least%20accurate%20for%0Aboth%20UQ%20methods%20for%20the%202D%20input%20uncertainty%20experiment%20and%20the%20high-noise%0Alevel.%20While%20these%20results%20do%20not%20apply%20to%20more%20complex%20data%2C%20they%20highlight%0Athat%20further%20research%20on%20post-facto%20calibration%20for%20these%20methods%20would%20be%0Abeneficial%2C%20particularly%20for%20high-noise%20and%20high-dimensional%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08587v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeepUQ%253A%2520Assessing%2520the%2520Aleatoric%2520Uncertainties%2520from%2520two%2520Deep%2520Learning%250A%2520%2520Methods%26entry.906535625%3DRebecca%2520Nevin%2520and%2520Aleksandra%2520%25C4%2586iprijanovi%25C4%2587%2520and%2520Brian%2520D.%2520Nord%26entry.1292438233%3D%2520%2520Assessing%2520the%2520quality%2520of%2520aleatoric%2520uncertainty%2520estimates%2520from%2520uncertainty%250Aquantification%2520%2528UQ%2529%2520deep%2520learning%2520methods%2520is%2520important%2520in%2520scientific%2520contexts%252C%250Awhere%2520uncertainty%2520is%2520physically%2520meaningful%2520and%2520important%2520to%2520characterize%2520and%250Ainterpret%2520exactly.%2520We%2520systematically%2520compare%2520aleatoric%2520uncertainty%2520measured%2520by%250Atwo%2520UQ%2520techniques%252C%2520Deep%2520Ensembles%2520%2528DE%2529%2520and%2520Deep%2520Evidential%2520Regression%2520%2528DER%2529.%250AOur%2520method%2520focuses%2520on%2520both%2520zero-dimensional%2520%25280D%2529%2520and%2520two-dimensional%2520%25282D%2529%2520data%252C%250Ato%2520explore%2520how%2520the%2520UQ%2520methods%2520function%2520for%2520different%2520data%2520dimensionalities.%2520We%250Ainvestigate%2520uncertainty%2520injected%2520on%2520the%2520input%2520and%2520output%2520variables%2520and%2520include%250Aa%2520method%2520to%2520propagate%2520uncertainty%2520in%2520the%2520case%2520of%2520input%2520uncertainty%2520so%2520that%2520we%250Acan%2520compare%2520the%2520predicted%2520aleatoric%2520uncertainty%2520to%2520the%2520known%2520values.%2520We%250Aexperiment%2520with%2520three%2520levels%2520of%2520noise.%2520The%2520aleatoric%2520uncertainty%2520predicted%250Aacross%2520all%2520models%2520and%2520experiments%2520scales%2520with%2520the%2520injected%2520noise%2520level.%250AHowever%252C%2520the%2520predicted%2520uncertainty%2520is%2520miscalibrated%2520to%2520%2524%255Crm%257Bstd%257D%2528%255Csigma_%257B%255Crm%250Aal%257D%2529%2524%2520with%2520the%2520true%2520uncertainty%2520for%2520half%2520of%2520the%2520DE%2520experiments%2520and%2520almost%2520all%250Aof%2520the%2520DER%2520experiments.%2520The%2520predicted%2520uncertainty%2520is%2520the%2520least%2520accurate%2520for%250Aboth%2520UQ%2520methods%2520for%2520the%25202D%2520input%2520uncertainty%2520experiment%2520and%2520the%2520high-noise%250Alevel.%2520While%2520these%2520results%2520do%2520not%2520apply%2520to%2520more%2520complex%2520data%252C%2520they%2520highlight%250Athat%2520further%2520research%2520on%2520post-facto%2520calibration%2520for%2520these%2520methods%2520would%2520be%250Abeneficial%252C%2520particularly%2520for%2520high-noise%2520and%2520high-dimensional%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08587v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepUQ%3A%20Assessing%20the%20Aleatoric%20Uncertainties%20from%20two%20Deep%20Learning%0A%20%20Methods&entry.906535625=Rebecca%20Nevin%20and%20Aleksandra%20%C4%86iprijanovi%C4%87%20and%20Brian%20D.%20Nord&entry.1292438233=%20%20Assessing%20the%20quality%20of%20aleatoric%20uncertainty%20estimates%20from%20uncertainty%0Aquantification%20%28UQ%29%20deep%20learning%20methods%20is%20important%20in%20scientific%20contexts%2C%0Awhere%20uncertainty%20is%20physically%20meaningful%20and%20important%20to%20characterize%20and%0Ainterpret%20exactly.%20We%20systematically%20compare%20aleatoric%20uncertainty%20measured%20by%0Atwo%20UQ%20techniques%2C%20Deep%20Ensembles%20%28DE%29%20and%20Deep%20Evidential%20Regression%20%28DER%29.%0AOur%20method%20focuses%20on%20both%20zero-dimensional%20%280D%29%20and%20two-dimensional%20%282D%29%20data%2C%0Ato%20explore%20how%20the%20UQ%20methods%20function%20for%20different%20data%20dimensionalities.%20We%0Ainvestigate%20uncertainty%20injected%20on%20the%20input%20and%20output%20variables%20and%20include%0Aa%20method%20to%20propagate%20uncertainty%20in%20the%20case%20of%20input%20uncertainty%20so%20that%20we%0Acan%20compare%20the%20predicted%20aleatoric%20uncertainty%20to%20the%20known%20values.%20We%0Aexperiment%20with%20three%20levels%20of%20noise.%20The%20aleatoric%20uncertainty%20predicted%0Aacross%20all%20models%20and%20experiments%20scales%20with%20the%20injected%20noise%20level.%0AHowever%2C%20the%20predicted%20uncertainty%20is%20miscalibrated%20to%20%24%5Crm%7Bstd%7D%28%5Csigma_%7B%5Crm%0Aal%7D%29%24%20with%20the%20true%20uncertainty%20for%20half%20of%20the%20DE%20experiments%20and%20almost%20all%0Aof%20the%20DER%20experiments.%20The%20predicted%20uncertainty%20is%20the%20least%20accurate%20for%0Aboth%20UQ%20methods%20for%20the%202D%20input%20uncertainty%20experiment%20and%20the%20high-noise%0Alevel.%20While%20these%20results%20do%20not%20apply%20to%20more%20complex%20data%2C%20they%20highlight%0Athat%20further%20research%20on%20post-facto%20calibration%20for%20these%20methods%20would%20be%0Abeneficial%2C%20particularly%20for%20high-noise%20and%20high-dimensional%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08587v1&entry.124074799=Read"},
{"title": "Exact, Tractable Gauss-Newton Optimization in Deep Reversible\n  Architectures Reveal Poor Generalization", "author": "Davide Buffelli and Jamie McGowan and Wangkun Xu and Alexandru Cioba and Da-shan Shiu and Guillaume Hennequin and Alberto Bernacchia", "abstract": "  Second-order optimization has been shown to accelerate the training of deep\nneural networks in many applications, often yielding faster progress per\niteration on the training loss compared to first-order optimizers. However, the\ngeneralization properties of second-order methods are still being debated.\nTheoretical investigations have proved difficult to carry out outside the\ntractable settings of heavily simplified model classes -- thus, the relevance\nof existing theories to practical deep learning applications remains unclear.\nSimilarly, empirical studies in large-scale models and real datasets are\nsignificantly confounded by the necessity to approximate second-order updates\nin practice. It is often unclear whether the observed generalization behaviour\narises specifically from the second-order nature of the parameter updates, or\ninstead reflects the specific structured (e.g.\\ Kronecker) approximations used\nor any damping-based interpolation towards first-order updates. Here, we show\nfor the first time that exact Gauss-Newton (GN) updates take on a tractable\nform in a class of deep reversible architectures that are sufficiently\nexpressive to be meaningfully applied to common benchmark datasets. We exploit\nthis novel setting to study the training and generalization properties of the\nGN optimizer. We find that exact GN generalizes poorly. In the mini-batch\ntraining setting, this manifests as rapidly saturating progress even on the\n\\emph{training} loss, with parameter updates found to overfit each\nmini-batchatch without producing the features that would support generalization\nto other mini-batches. We show that our experiments run in the ``lazy'' regime,\nin which the neural tangent kernel (NTK) changes very little during the course\nof training. This behaviour is associated with having no significant changes in\nneural representations, explaining the lack of generalization.\n", "link": "http://arxiv.org/abs/2411.07979v2", "date": "2024-11-13", "relevancy": 2.0928, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5282}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5209}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5166}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exact%2C%20Tractable%20Gauss-Newton%20Optimization%20in%20Deep%20Reversible%0A%20%20Architectures%20Reveal%20Poor%20Generalization&body=Title%3A%20Exact%2C%20Tractable%20Gauss-Newton%20Optimization%20in%20Deep%20Reversible%0A%20%20Architectures%20Reveal%20Poor%20Generalization%0AAuthor%3A%20Davide%20Buffelli%20and%20Jamie%20McGowan%20and%20Wangkun%20Xu%20and%20Alexandru%20Cioba%20and%20Da-shan%20Shiu%20and%20Guillaume%20Hennequin%20and%20Alberto%20Bernacchia%0AAbstract%3A%20%20%20Second-order%20optimization%20has%20been%20shown%20to%20accelerate%20the%20training%20of%20deep%0Aneural%20networks%20in%20many%20applications%2C%20often%20yielding%20faster%20progress%20per%0Aiteration%20on%20the%20training%20loss%20compared%20to%20first-order%20optimizers.%20However%2C%20the%0Ageneralization%20properties%20of%20second-order%20methods%20are%20still%20being%20debated.%0ATheoretical%20investigations%20have%20proved%20difficult%20to%20carry%20out%20outside%20the%0Atractable%20settings%20of%20heavily%20simplified%20model%20classes%20--%20thus%2C%20the%20relevance%0Aof%20existing%20theories%20to%20practical%20deep%20learning%20applications%20remains%20unclear.%0ASimilarly%2C%20empirical%20studies%20in%20large-scale%20models%20and%20real%20datasets%20are%0Asignificantly%20confounded%20by%20the%20necessity%20to%20approximate%20second-order%20updates%0Ain%20practice.%20It%20is%20often%20unclear%20whether%20the%20observed%20generalization%20behaviour%0Aarises%20specifically%20from%20the%20second-order%20nature%20of%20the%20parameter%20updates%2C%20or%0Ainstead%20reflects%20the%20specific%20structured%20%28e.g.%5C%20Kronecker%29%20approximations%20used%0Aor%20any%20damping-based%20interpolation%20towards%20first-order%20updates.%20Here%2C%20we%20show%0Afor%20the%20first%20time%20that%20exact%20Gauss-Newton%20%28GN%29%20updates%20take%20on%20a%20tractable%0Aform%20in%20a%20class%20of%20deep%20reversible%20architectures%20that%20are%20sufficiently%0Aexpressive%20to%20be%20meaningfully%20applied%20to%20common%20benchmark%20datasets.%20We%20exploit%0Athis%20novel%20setting%20to%20study%20the%20training%20and%20generalization%20properties%20of%20the%0AGN%20optimizer.%20We%20find%20that%20exact%20GN%20generalizes%20poorly.%20In%20the%20mini-batch%0Atraining%20setting%2C%20this%20manifests%20as%20rapidly%20saturating%20progress%20even%20on%20the%0A%5Cemph%7Btraining%7D%20loss%2C%20with%20parameter%20updates%20found%20to%20overfit%20each%0Amini-batchatch%20without%20producing%20the%20features%20that%20would%20support%20generalization%0Ato%20other%20mini-batches.%20We%20show%20that%20our%20experiments%20run%20in%20the%20%60%60lazy%27%27%20regime%2C%0Ain%20which%20the%20neural%20tangent%20kernel%20%28NTK%29%20changes%20very%20little%20during%20the%20course%0Aof%20training.%20This%20behaviour%20is%20associated%20with%20having%20no%20significant%20changes%20in%0Aneural%20representations%2C%20explaining%20the%20lack%20of%20generalization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07979v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExact%252C%2520Tractable%2520Gauss-Newton%2520Optimization%2520in%2520Deep%2520Reversible%250A%2520%2520Architectures%2520Reveal%2520Poor%2520Generalization%26entry.906535625%3DDavide%2520Buffelli%2520and%2520Jamie%2520McGowan%2520and%2520Wangkun%2520Xu%2520and%2520Alexandru%2520Cioba%2520and%2520Da-shan%2520Shiu%2520and%2520Guillaume%2520Hennequin%2520and%2520Alberto%2520Bernacchia%26entry.1292438233%3D%2520%2520Second-order%2520optimization%2520has%2520been%2520shown%2520to%2520accelerate%2520the%2520training%2520of%2520deep%250Aneural%2520networks%2520in%2520many%2520applications%252C%2520often%2520yielding%2520faster%2520progress%2520per%250Aiteration%2520on%2520the%2520training%2520loss%2520compared%2520to%2520first-order%2520optimizers.%2520However%252C%2520the%250Ageneralization%2520properties%2520of%2520second-order%2520methods%2520are%2520still%2520being%2520debated.%250ATheoretical%2520investigations%2520have%2520proved%2520difficult%2520to%2520carry%2520out%2520outside%2520the%250Atractable%2520settings%2520of%2520heavily%2520simplified%2520model%2520classes%2520--%2520thus%252C%2520the%2520relevance%250Aof%2520existing%2520theories%2520to%2520practical%2520deep%2520learning%2520applications%2520remains%2520unclear.%250ASimilarly%252C%2520empirical%2520studies%2520in%2520large-scale%2520models%2520and%2520real%2520datasets%2520are%250Asignificantly%2520confounded%2520by%2520the%2520necessity%2520to%2520approximate%2520second-order%2520updates%250Ain%2520practice.%2520It%2520is%2520often%2520unclear%2520whether%2520the%2520observed%2520generalization%2520behaviour%250Aarises%2520specifically%2520from%2520the%2520second-order%2520nature%2520of%2520the%2520parameter%2520updates%252C%2520or%250Ainstead%2520reflects%2520the%2520specific%2520structured%2520%2528e.g.%255C%2520Kronecker%2529%2520approximations%2520used%250Aor%2520any%2520damping-based%2520interpolation%2520towards%2520first-order%2520updates.%2520Here%252C%2520we%2520show%250Afor%2520the%2520first%2520time%2520that%2520exact%2520Gauss-Newton%2520%2528GN%2529%2520updates%2520take%2520on%2520a%2520tractable%250Aform%2520in%2520a%2520class%2520of%2520deep%2520reversible%2520architectures%2520that%2520are%2520sufficiently%250Aexpressive%2520to%2520be%2520meaningfully%2520applied%2520to%2520common%2520benchmark%2520datasets.%2520We%2520exploit%250Athis%2520novel%2520setting%2520to%2520study%2520the%2520training%2520and%2520generalization%2520properties%2520of%2520the%250AGN%2520optimizer.%2520We%2520find%2520that%2520exact%2520GN%2520generalizes%2520poorly.%2520In%2520the%2520mini-batch%250Atraining%2520setting%252C%2520this%2520manifests%2520as%2520rapidly%2520saturating%2520progress%2520even%2520on%2520the%250A%255Cemph%257Btraining%257D%2520loss%252C%2520with%2520parameter%2520updates%2520found%2520to%2520overfit%2520each%250Amini-batchatch%2520without%2520producing%2520the%2520features%2520that%2520would%2520support%2520generalization%250Ato%2520other%2520mini-batches.%2520We%2520show%2520that%2520our%2520experiments%2520run%2520in%2520the%2520%2560%2560lazy%2527%2527%2520regime%252C%250Ain%2520which%2520the%2520neural%2520tangent%2520kernel%2520%2528NTK%2529%2520changes%2520very%2520little%2520during%2520the%2520course%250Aof%2520training.%2520This%2520behaviour%2520is%2520associated%2520with%2520having%2520no%2520significant%2520changes%2520in%250Aneural%2520representations%252C%2520explaining%2520the%2520lack%2520of%2520generalization.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07979v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exact%2C%20Tractable%20Gauss-Newton%20Optimization%20in%20Deep%20Reversible%0A%20%20Architectures%20Reveal%20Poor%20Generalization&entry.906535625=Davide%20Buffelli%20and%20Jamie%20McGowan%20and%20Wangkun%20Xu%20and%20Alexandru%20Cioba%20and%20Da-shan%20Shiu%20and%20Guillaume%20Hennequin%20and%20Alberto%20Bernacchia&entry.1292438233=%20%20Second-order%20optimization%20has%20been%20shown%20to%20accelerate%20the%20training%20of%20deep%0Aneural%20networks%20in%20many%20applications%2C%20often%20yielding%20faster%20progress%20per%0Aiteration%20on%20the%20training%20loss%20compared%20to%20first-order%20optimizers.%20However%2C%20the%0Ageneralization%20properties%20of%20second-order%20methods%20are%20still%20being%20debated.%0ATheoretical%20investigations%20have%20proved%20difficult%20to%20carry%20out%20outside%20the%0Atractable%20settings%20of%20heavily%20simplified%20model%20classes%20--%20thus%2C%20the%20relevance%0Aof%20existing%20theories%20to%20practical%20deep%20learning%20applications%20remains%20unclear.%0ASimilarly%2C%20empirical%20studies%20in%20large-scale%20models%20and%20real%20datasets%20are%0Asignificantly%20confounded%20by%20the%20necessity%20to%20approximate%20second-order%20updates%0Ain%20practice.%20It%20is%20often%20unclear%20whether%20the%20observed%20generalization%20behaviour%0Aarises%20specifically%20from%20the%20second-order%20nature%20of%20the%20parameter%20updates%2C%20or%0Ainstead%20reflects%20the%20specific%20structured%20%28e.g.%5C%20Kronecker%29%20approximations%20used%0Aor%20any%20damping-based%20interpolation%20towards%20first-order%20updates.%20Here%2C%20we%20show%0Afor%20the%20first%20time%20that%20exact%20Gauss-Newton%20%28GN%29%20updates%20take%20on%20a%20tractable%0Aform%20in%20a%20class%20of%20deep%20reversible%20architectures%20that%20are%20sufficiently%0Aexpressive%20to%20be%20meaningfully%20applied%20to%20common%20benchmark%20datasets.%20We%20exploit%0Athis%20novel%20setting%20to%20study%20the%20training%20and%20generalization%20properties%20of%20the%0AGN%20optimizer.%20We%20find%20that%20exact%20GN%20generalizes%20poorly.%20In%20the%20mini-batch%0Atraining%20setting%2C%20this%20manifests%20as%20rapidly%20saturating%20progress%20even%20on%20the%0A%5Cemph%7Btraining%7D%20loss%2C%20with%20parameter%20updates%20found%20to%20overfit%20each%0Amini-batchatch%20without%20producing%20the%20features%20that%20would%20support%20generalization%0Ato%20other%20mini-batches.%20We%20show%20that%20our%20experiments%20run%20in%20the%20%60%60lazy%27%27%20regime%2C%0Ain%20which%20the%20neural%20tangent%20kernel%20%28NTK%29%20changes%20very%20little%20during%20the%20course%0Aof%20training.%20This%20behaviour%20is%20associated%20with%20having%20no%20significant%20changes%20in%0Aneural%20representations%2C%20explaining%20the%20lack%20of%20generalization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07979v2&entry.124074799=Read"},
{"title": "High-resolution optical and acoustic remote sensing datasets of the Puck\n  Lagoon, Southern Baltic", "author": "\u0141ukasz Janowski and Dimitrios Skarlatos and Panagiotis Agrafiotis and Pawe\u0142 Tysi\u0105c and Andrzej Pydyn and Mateusz Popek and Anna M. Kotarba-Morley and Gottfried Mandlburger and \u0141ukasz Gajewski and Mateusz Ko\u0142akowski and Alexandra Papadaki and Juliusz Gajewski", "abstract": "  The very shallow marine basin of Puck Lagoon in the southern Baltic Sea, on\nthe Northern coast of Poland, hosts valuable benthic habitats and cultural\nheritage sites. These include, among others, protected Zostera marina meadows,\none of the Baltic's major medieval harbours, a ship graveyard, and likely other\nsubmerged features that are yet to be discovered. Prior to this project, no\ncomprehensive high-resolution remote sensing data were available for this area.\nThis article describes the first Digital Elevation Models (DEMs) derived from a\ncombination of airborne bathymetric LiDAR, multibeam echosounder, airborne\nphotogrammetry and satellite imagery. These datasets also include multibeam\nechosounder backscatter and LiDAR intensity, allowing determination of the\ncharacter and properties of the seafloor. Combined, these datasets are a vital\nresource for assessing and understanding seafloor morphology, benthic habitats,\ncultural heritage, and submerged landscapes. Given the significance of Puck\nLagoon's hydrographical, ecological, geological, and archaeological environs,\nthe high-resolution bathymetry, acquired by our project, can provide the\nfoundation for sustainable management and informed decision-making for this\narea of interest.\n", "link": "http://arxiv.org/abs/2411.08712v1", "date": "2024-11-13", "relevancy": 2.0667, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4342}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4029}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4029}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-resolution%20optical%20and%20acoustic%20remote%20sensing%20datasets%20of%20the%20Puck%0A%20%20Lagoon%2C%20Southern%20Baltic&body=Title%3A%20High-resolution%20optical%20and%20acoustic%20remote%20sensing%20datasets%20of%20the%20Puck%0A%20%20Lagoon%2C%20Southern%20Baltic%0AAuthor%3A%20%C5%81ukasz%20Janowski%20and%20Dimitrios%20Skarlatos%20and%20Panagiotis%20Agrafiotis%20and%20Pawe%C5%82%20Tysi%C4%85c%20and%20Andrzej%20Pydyn%20and%20Mateusz%20Popek%20and%20Anna%20M.%20Kotarba-Morley%20and%20Gottfried%20Mandlburger%20and%20%C5%81ukasz%20Gajewski%20and%20Mateusz%20Ko%C5%82akowski%20and%20Alexandra%20Papadaki%20and%20Juliusz%20Gajewski%0AAbstract%3A%20%20%20The%20very%20shallow%20marine%20basin%20of%20Puck%20Lagoon%20in%20the%20southern%20Baltic%20Sea%2C%20on%0Athe%20Northern%20coast%20of%20Poland%2C%20hosts%20valuable%20benthic%20habitats%20and%20cultural%0Aheritage%20sites.%20These%20include%2C%20among%20others%2C%20protected%20Zostera%20marina%20meadows%2C%0Aone%20of%20the%20Baltic%27s%20major%20medieval%20harbours%2C%20a%20ship%20graveyard%2C%20and%20likely%20other%0Asubmerged%20features%20that%20are%20yet%20to%20be%20discovered.%20Prior%20to%20this%20project%2C%20no%0Acomprehensive%20high-resolution%20remote%20sensing%20data%20were%20available%20for%20this%20area.%0AThis%20article%20describes%20the%20first%20Digital%20Elevation%20Models%20%28DEMs%29%20derived%20from%20a%0Acombination%20of%20airborne%20bathymetric%20LiDAR%2C%20multibeam%20echosounder%2C%20airborne%0Aphotogrammetry%20and%20satellite%20imagery.%20These%20datasets%20also%20include%20multibeam%0Aechosounder%20backscatter%20and%20LiDAR%20intensity%2C%20allowing%20determination%20of%20the%0Acharacter%20and%20properties%20of%20the%20seafloor.%20Combined%2C%20these%20datasets%20are%20a%20vital%0Aresource%20for%20assessing%20and%20understanding%20seafloor%20morphology%2C%20benthic%20habitats%2C%0Acultural%20heritage%2C%20and%20submerged%20landscapes.%20Given%20the%20significance%20of%20Puck%0ALagoon%27s%20hydrographical%2C%20ecological%2C%20geological%2C%20and%20archaeological%20environs%2C%0Athe%20high-resolution%20bathymetry%2C%20acquired%20by%20our%20project%2C%20can%20provide%20the%0Afoundation%20for%20sustainable%20management%20and%20informed%20decision-making%20for%20this%0Aarea%20of%20interest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08712v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-resolution%2520optical%2520and%2520acoustic%2520remote%2520sensing%2520datasets%2520of%2520the%2520Puck%250A%2520%2520Lagoon%252C%2520Southern%2520Baltic%26entry.906535625%3D%25C5%2581ukasz%2520Janowski%2520and%2520Dimitrios%2520Skarlatos%2520and%2520Panagiotis%2520Agrafiotis%2520and%2520Pawe%25C5%2582%2520Tysi%25C4%2585c%2520and%2520Andrzej%2520Pydyn%2520and%2520Mateusz%2520Popek%2520and%2520Anna%2520M.%2520Kotarba-Morley%2520and%2520Gottfried%2520Mandlburger%2520and%2520%25C5%2581ukasz%2520Gajewski%2520and%2520Mateusz%2520Ko%25C5%2582akowski%2520and%2520Alexandra%2520Papadaki%2520and%2520Juliusz%2520Gajewski%26entry.1292438233%3D%2520%2520The%2520very%2520shallow%2520marine%2520basin%2520of%2520Puck%2520Lagoon%2520in%2520the%2520southern%2520Baltic%2520Sea%252C%2520on%250Athe%2520Northern%2520coast%2520of%2520Poland%252C%2520hosts%2520valuable%2520benthic%2520habitats%2520and%2520cultural%250Aheritage%2520sites.%2520These%2520include%252C%2520among%2520others%252C%2520protected%2520Zostera%2520marina%2520meadows%252C%250Aone%2520of%2520the%2520Baltic%2527s%2520major%2520medieval%2520harbours%252C%2520a%2520ship%2520graveyard%252C%2520and%2520likely%2520other%250Asubmerged%2520features%2520that%2520are%2520yet%2520to%2520be%2520discovered.%2520Prior%2520to%2520this%2520project%252C%2520no%250Acomprehensive%2520high-resolution%2520remote%2520sensing%2520data%2520were%2520available%2520for%2520this%2520area.%250AThis%2520article%2520describes%2520the%2520first%2520Digital%2520Elevation%2520Models%2520%2528DEMs%2529%2520derived%2520from%2520a%250Acombination%2520of%2520airborne%2520bathymetric%2520LiDAR%252C%2520multibeam%2520echosounder%252C%2520airborne%250Aphotogrammetry%2520and%2520satellite%2520imagery.%2520These%2520datasets%2520also%2520include%2520multibeam%250Aechosounder%2520backscatter%2520and%2520LiDAR%2520intensity%252C%2520allowing%2520determination%2520of%2520the%250Acharacter%2520and%2520properties%2520of%2520the%2520seafloor.%2520Combined%252C%2520these%2520datasets%2520are%2520a%2520vital%250Aresource%2520for%2520assessing%2520and%2520understanding%2520seafloor%2520morphology%252C%2520benthic%2520habitats%252C%250Acultural%2520heritage%252C%2520and%2520submerged%2520landscapes.%2520Given%2520the%2520significance%2520of%2520Puck%250ALagoon%2527s%2520hydrographical%252C%2520ecological%252C%2520geological%252C%2520and%2520archaeological%2520environs%252C%250Athe%2520high-resolution%2520bathymetry%252C%2520acquired%2520by%2520our%2520project%252C%2520can%2520provide%2520the%250Afoundation%2520for%2520sustainable%2520management%2520and%2520informed%2520decision-making%2520for%2520this%250Aarea%2520of%2520interest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08712v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-resolution%20optical%20and%20acoustic%20remote%20sensing%20datasets%20of%20the%20Puck%0A%20%20Lagoon%2C%20Southern%20Baltic&entry.906535625=%C5%81ukasz%20Janowski%20and%20Dimitrios%20Skarlatos%20and%20Panagiotis%20Agrafiotis%20and%20Pawe%C5%82%20Tysi%C4%85c%20and%20Andrzej%20Pydyn%20and%20Mateusz%20Popek%20and%20Anna%20M.%20Kotarba-Morley%20and%20Gottfried%20Mandlburger%20and%20%C5%81ukasz%20Gajewski%20and%20Mateusz%20Ko%C5%82akowski%20and%20Alexandra%20Papadaki%20and%20Juliusz%20Gajewski&entry.1292438233=%20%20The%20very%20shallow%20marine%20basin%20of%20Puck%20Lagoon%20in%20the%20southern%20Baltic%20Sea%2C%20on%0Athe%20Northern%20coast%20of%20Poland%2C%20hosts%20valuable%20benthic%20habitats%20and%20cultural%0Aheritage%20sites.%20These%20include%2C%20among%20others%2C%20protected%20Zostera%20marina%20meadows%2C%0Aone%20of%20the%20Baltic%27s%20major%20medieval%20harbours%2C%20a%20ship%20graveyard%2C%20and%20likely%20other%0Asubmerged%20features%20that%20are%20yet%20to%20be%20discovered.%20Prior%20to%20this%20project%2C%20no%0Acomprehensive%20high-resolution%20remote%20sensing%20data%20were%20available%20for%20this%20area.%0AThis%20article%20describes%20the%20first%20Digital%20Elevation%20Models%20%28DEMs%29%20derived%20from%20a%0Acombination%20of%20airborne%20bathymetric%20LiDAR%2C%20multibeam%20echosounder%2C%20airborne%0Aphotogrammetry%20and%20satellite%20imagery.%20These%20datasets%20also%20include%20multibeam%0Aechosounder%20backscatter%20and%20LiDAR%20intensity%2C%20allowing%20determination%20of%20the%0Acharacter%20and%20properties%20of%20the%20seafloor.%20Combined%2C%20these%20datasets%20are%20a%20vital%0Aresource%20for%20assessing%20and%20understanding%20seafloor%20morphology%2C%20benthic%20habitats%2C%0Acultural%20heritage%2C%20and%20submerged%20landscapes.%20Given%20the%20significance%20of%20Puck%0ALagoon%27s%20hydrographical%2C%20ecological%2C%20geological%2C%20and%20archaeological%20environs%2C%0Athe%20high-resolution%20bathymetry%2C%20acquired%20by%20our%20project%2C%20can%20provide%20the%0Afoundation%20for%20sustainable%20management%20and%20informed%20decision-making%20for%20this%0Aarea%20of%20interest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08712v1&entry.124074799=Read"},
{"title": "Neural Persistence Dynamics", "author": "Sebastian Zeng and Florian Graf and Martin Uray and Stefan Huber and Roland Kwitt", "abstract": "  We consider the problem of learning the dynamics in the topology of\ntime-evolving point clouds, the prevalent spatiotemporal model for systems\nexhibiting collective behavior, such as swarms of insects and birds or\nparticles in physics. In such systems, patterns emerge from (local)\ninteractions among self-propelled entities. While several well-understood\ngoverning equations for motion and interaction exist, they are notoriously\ndifficult to fit to data, as most prior work requires knowledge about\nindividual motion trajectories, i.e., a requirement that is challenging to\nsatisfy with an increasing number of entities. To evade such confounding\nfactors, we investigate collective behavior from a $\\textit{topological\nperspective}$, but instead of summarizing entire observation sequences (as done\npreviously), we propose learning a latent dynamical model from topological\nfeatures $\\textit{per time point}$. The latter is then used to formulate a\ndownstream regression task to predict the parametrization of some a priori\nspecified governing equation. We implement this idea based on a latent ODE\nlearned from vectorized (static) persistence diagrams and show that a\ncombination of recent stability results for persistent homology justifies this\nmodeling choice. Various (ablation) experiments not only demonstrate the\nrelevance of each model component but provide compelling empirical evidence\nthat our proposed model - $\\textit{Neural Persistence Dynamics}$ -\nsubstantially outperforms the state-of-the-art across a diverse set of\nparameter regression tasks.\n", "link": "http://arxiv.org/abs/2405.15732v2", "date": "2024-11-13", "relevancy": 2.0523, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5274}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5082}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Persistence%20Dynamics&body=Title%3A%20Neural%20Persistence%20Dynamics%0AAuthor%3A%20Sebastian%20Zeng%20and%20Florian%20Graf%20and%20Martin%20Uray%20and%20Stefan%20Huber%20and%20Roland%20Kwitt%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20learning%20the%20dynamics%20in%20the%20topology%20of%0Atime-evolving%20point%20clouds%2C%20the%20prevalent%20spatiotemporal%20model%20for%20systems%0Aexhibiting%20collective%20behavior%2C%20such%20as%20swarms%20of%20insects%20and%20birds%20or%0Aparticles%20in%20physics.%20In%20such%20systems%2C%20patterns%20emerge%20from%20%28local%29%0Ainteractions%20among%20self-propelled%20entities.%20While%20several%20well-understood%0Agoverning%20equations%20for%20motion%20and%20interaction%20exist%2C%20they%20are%20notoriously%0Adifficult%20to%20fit%20to%20data%2C%20as%20most%20prior%20work%20requires%20knowledge%20about%0Aindividual%20motion%20trajectories%2C%20i.e.%2C%20a%20requirement%20that%20is%20challenging%20to%0Asatisfy%20with%20an%20increasing%20number%20of%20entities.%20To%20evade%20such%20confounding%0Afactors%2C%20we%20investigate%20collective%20behavior%20from%20a%20%24%5Ctextit%7Btopological%0Aperspective%7D%24%2C%20but%20instead%20of%20summarizing%20entire%20observation%20sequences%20%28as%20done%0Apreviously%29%2C%20we%20propose%20learning%20a%20latent%20dynamical%20model%20from%20topological%0Afeatures%20%24%5Ctextit%7Bper%20time%20point%7D%24.%20The%20latter%20is%20then%20used%20to%20formulate%20a%0Adownstream%20regression%20task%20to%20predict%20the%20parametrization%20of%20some%20a%20priori%0Aspecified%20governing%20equation.%20We%20implement%20this%20idea%20based%20on%20a%20latent%20ODE%0Alearned%20from%20vectorized%20%28static%29%20persistence%20diagrams%20and%20show%20that%20a%0Acombination%20of%20recent%20stability%20results%20for%20persistent%20homology%20justifies%20this%0Amodeling%20choice.%20Various%20%28ablation%29%20experiments%20not%20only%20demonstrate%20the%0Arelevance%20of%20each%20model%20component%20but%20provide%20compelling%20empirical%20evidence%0Athat%20our%20proposed%20model%20-%20%24%5Ctextit%7BNeural%20Persistence%20Dynamics%7D%24%20-%0Asubstantially%20outperforms%20the%20state-of-the-art%20across%20a%20diverse%20set%20of%0Aparameter%20regression%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15732v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Persistence%2520Dynamics%26entry.906535625%3DSebastian%2520Zeng%2520and%2520Florian%2520Graf%2520and%2520Martin%2520Uray%2520and%2520Stefan%2520Huber%2520and%2520Roland%2520Kwitt%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520learning%2520the%2520dynamics%2520in%2520the%2520topology%2520of%250Atime-evolving%2520point%2520clouds%252C%2520the%2520prevalent%2520spatiotemporal%2520model%2520for%2520systems%250Aexhibiting%2520collective%2520behavior%252C%2520such%2520as%2520swarms%2520of%2520insects%2520and%2520birds%2520or%250Aparticles%2520in%2520physics.%2520In%2520such%2520systems%252C%2520patterns%2520emerge%2520from%2520%2528local%2529%250Ainteractions%2520among%2520self-propelled%2520entities.%2520While%2520several%2520well-understood%250Agoverning%2520equations%2520for%2520motion%2520and%2520interaction%2520exist%252C%2520they%2520are%2520notoriously%250Adifficult%2520to%2520fit%2520to%2520data%252C%2520as%2520most%2520prior%2520work%2520requires%2520knowledge%2520about%250Aindividual%2520motion%2520trajectories%252C%2520i.e.%252C%2520a%2520requirement%2520that%2520is%2520challenging%2520to%250Asatisfy%2520with%2520an%2520increasing%2520number%2520of%2520entities.%2520To%2520evade%2520such%2520confounding%250Afactors%252C%2520we%2520investigate%2520collective%2520behavior%2520from%2520a%2520%2524%255Ctextit%257Btopological%250Aperspective%257D%2524%252C%2520but%2520instead%2520of%2520summarizing%2520entire%2520observation%2520sequences%2520%2528as%2520done%250Apreviously%2529%252C%2520we%2520propose%2520learning%2520a%2520latent%2520dynamical%2520model%2520from%2520topological%250Afeatures%2520%2524%255Ctextit%257Bper%2520time%2520point%257D%2524.%2520The%2520latter%2520is%2520then%2520used%2520to%2520formulate%2520a%250Adownstream%2520regression%2520task%2520to%2520predict%2520the%2520parametrization%2520of%2520some%2520a%2520priori%250Aspecified%2520governing%2520equation.%2520We%2520implement%2520this%2520idea%2520based%2520on%2520a%2520latent%2520ODE%250Alearned%2520from%2520vectorized%2520%2528static%2529%2520persistence%2520diagrams%2520and%2520show%2520that%2520a%250Acombination%2520of%2520recent%2520stability%2520results%2520for%2520persistent%2520homology%2520justifies%2520this%250Amodeling%2520choice.%2520Various%2520%2528ablation%2529%2520experiments%2520not%2520only%2520demonstrate%2520the%250Arelevance%2520of%2520each%2520model%2520component%2520but%2520provide%2520compelling%2520empirical%2520evidence%250Athat%2520our%2520proposed%2520model%2520-%2520%2524%255Ctextit%257BNeural%2520Persistence%2520Dynamics%257D%2524%2520-%250Asubstantially%2520outperforms%2520the%2520state-of-the-art%2520across%2520a%2520diverse%2520set%2520of%250Aparameter%2520regression%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15732v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Persistence%20Dynamics&entry.906535625=Sebastian%20Zeng%20and%20Florian%20Graf%20and%20Martin%20Uray%20and%20Stefan%20Huber%20and%20Roland%20Kwitt&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20learning%20the%20dynamics%20in%20the%20topology%20of%0Atime-evolving%20point%20clouds%2C%20the%20prevalent%20spatiotemporal%20model%20for%20systems%0Aexhibiting%20collective%20behavior%2C%20such%20as%20swarms%20of%20insects%20and%20birds%20or%0Aparticles%20in%20physics.%20In%20such%20systems%2C%20patterns%20emerge%20from%20%28local%29%0Ainteractions%20among%20self-propelled%20entities.%20While%20several%20well-understood%0Agoverning%20equations%20for%20motion%20and%20interaction%20exist%2C%20they%20are%20notoriously%0Adifficult%20to%20fit%20to%20data%2C%20as%20most%20prior%20work%20requires%20knowledge%20about%0Aindividual%20motion%20trajectories%2C%20i.e.%2C%20a%20requirement%20that%20is%20challenging%20to%0Asatisfy%20with%20an%20increasing%20number%20of%20entities.%20To%20evade%20such%20confounding%0Afactors%2C%20we%20investigate%20collective%20behavior%20from%20a%20%24%5Ctextit%7Btopological%0Aperspective%7D%24%2C%20but%20instead%20of%20summarizing%20entire%20observation%20sequences%20%28as%20done%0Apreviously%29%2C%20we%20propose%20learning%20a%20latent%20dynamical%20model%20from%20topological%0Afeatures%20%24%5Ctextit%7Bper%20time%20point%7D%24.%20The%20latter%20is%20then%20used%20to%20formulate%20a%0Adownstream%20regression%20task%20to%20predict%20the%20parametrization%20of%20some%20a%20priori%0Aspecified%20governing%20equation.%20We%20implement%20this%20idea%20based%20on%20a%20latent%20ODE%0Alearned%20from%20vectorized%20%28static%29%20persistence%20diagrams%20and%20show%20that%20a%0Acombination%20of%20recent%20stability%20results%20for%20persistent%20homology%20justifies%20this%0Amodeling%20choice.%20Various%20%28ablation%29%20experiments%20not%20only%20demonstrate%20the%0Arelevance%20of%20each%20model%20component%20but%20provide%20compelling%20empirical%20evidence%0Athat%20our%20proposed%20model%20-%20%24%5Ctextit%7BNeural%20Persistence%20Dynamics%7D%24%20-%0Asubstantially%20outperforms%20the%20state-of-the-art%20across%20a%20diverse%20set%20of%0Aparameter%20regression%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15732v2&entry.124074799=Read"},
{"title": "MLV$^2$-Net: Rater-Based Majority-Label Voting for Consistent Meningeal\n  Lymphatic Vessel Segmentation", "author": "Fabian Bongratz and Markus Karmann and Adrian Holz and Moritz Bonhoeffer and Viktor Neumaier and Sarah Deli and Benita Schmitz-Koep and Claus Zimmer and Christian Sorg and Melissa Thalhammer and Dennis M Hedderich and Christian Wachinger", "abstract": "  Meningeal lymphatic vessels (MLVs) are responsible for the drainage of waste\nproducts from the human brain. An impairment in their functionality has been\nassociated with aging as well as brain disorders like multiple sclerosis and\nAlzheimer's disease. However, MLVs have only recently been described for the\nfirst time in magnetic resonance imaging (MRI), and their ramified structure\nrenders manual segmentation particularly difficult. Further, as there is no\nconsistent notion of their appearance, human-annotated MLV structures contain a\nhigh inter-rater variability that most automatic segmentation methods cannot\ntake into account. In this work, we propose a new rater-aware training scheme\nfor the popular nnU-Net model, and we explore rater-based ensembling strategies\nfor accurate and consistent segmentation of MLVs. This enables us to boost\nnnU-Net's performance while obtaining explicit predictions in different\nannotation styles and a rater-based uncertainty estimation. Our final model,\nMLV$^2$-Net, achieves a Dice similarity coefficient of 0.806 with respect to\nthe human reference standard. The model further matches the human inter-rater\nreliability and replicates age-related associations with MLV volume.\n", "link": "http://arxiv.org/abs/2411.08537v1", "date": "2024-11-13", "relevancy": 2.0491, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.558}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5087}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4976}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MLV%24%5E2%24-Net%3A%20Rater-Based%20Majority-Label%20Voting%20for%20Consistent%20Meningeal%0A%20%20Lymphatic%20Vessel%20Segmentation&body=Title%3A%20MLV%24%5E2%24-Net%3A%20Rater-Based%20Majority-Label%20Voting%20for%20Consistent%20Meningeal%0A%20%20Lymphatic%20Vessel%20Segmentation%0AAuthor%3A%20Fabian%20Bongratz%20and%20Markus%20Karmann%20and%20Adrian%20Holz%20and%20Moritz%20Bonhoeffer%20and%20Viktor%20Neumaier%20and%20Sarah%20Deli%20and%20Benita%20Schmitz-Koep%20and%20Claus%20Zimmer%20and%20Christian%20Sorg%20and%20Melissa%20Thalhammer%20and%20Dennis%20M%20Hedderich%20and%20Christian%20Wachinger%0AAbstract%3A%20%20%20Meningeal%20lymphatic%20vessels%20%28MLVs%29%20are%20responsible%20for%20the%20drainage%20of%20waste%0Aproducts%20from%20the%20human%20brain.%20An%20impairment%20in%20their%20functionality%20has%20been%0Aassociated%20with%20aging%20as%20well%20as%20brain%20disorders%20like%20multiple%20sclerosis%20and%0AAlzheimer%27s%20disease.%20However%2C%20MLVs%20have%20only%20recently%20been%20described%20for%20the%0Afirst%20time%20in%20magnetic%20resonance%20imaging%20%28MRI%29%2C%20and%20their%20ramified%20structure%0Arenders%20manual%20segmentation%20particularly%20difficult.%20Further%2C%20as%20there%20is%20no%0Aconsistent%20notion%20of%20their%20appearance%2C%20human-annotated%20MLV%20structures%20contain%20a%0Ahigh%20inter-rater%20variability%20that%20most%20automatic%20segmentation%20methods%20cannot%0Atake%20into%20account.%20In%20this%20work%2C%20we%20propose%20a%20new%20rater-aware%20training%20scheme%0Afor%20the%20popular%20nnU-Net%20model%2C%20and%20we%20explore%20rater-based%20ensembling%20strategies%0Afor%20accurate%20and%20consistent%20segmentation%20of%20MLVs.%20This%20enables%20us%20to%20boost%0AnnU-Net%27s%20performance%20while%20obtaining%20explicit%20predictions%20in%20different%0Aannotation%20styles%20and%20a%20rater-based%20uncertainty%20estimation.%20Our%20final%20model%2C%0AMLV%24%5E2%24-Net%2C%20achieves%20a%20Dice%20similarity%20coefficient%20of%200.806%20with%20respect%20to%0Athe%20human%20reference%20standard.%20The%20model%20further%20matches%20the%20human%20inter-rater%0Areliability%20and%20replicates%20age-related%20associations%20with%20MLV%20volume.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08537v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMLV%2524%255E2%2524-Net%253A%2520Rater-Based%2520Majority-Label%2520Voting%2520for%2520Consistent%2520Meningeal%250A%2520%2520Lymphatic%2520Vessel%2520Segmentation%26entry.906535625%3DFabian%2520Bongratz%2520and%2520Markus%2520Karmann%2520and%2520Adrian%2520Holz%2520and%2520Moritz%2520Bonhoeffer%2520and%2520Viktor%2520Neumaier%2520and%2520Sarah%2520Deli%2520and%2520Benita%2520Schmitz-Koep%2520and%2520Claus%2520Zimmer%2520and%2520Christian%2520Sorg%2520and%2520Melissa%2520Thalhammer%2520and%2520Dennis%2520M%2520Hedderich%2520and%2520Christian%2520Wachinger%26entry.1292438233%3D%2520%2520Meningeal%2520lymphatic%2520vessels%2520%2528MLVs%2529%2520are%2520responsible%2520for%2520the%2520drainage%2520of%2520waste%250Aproducts%2520from%2520the%2520human%2520brain.%2520An%2520impairment%2520in%2520their%2520functionality%2520has%2520been%250Aassociated%2520with%2520aging%2520as%2520well%2520as%2520brain%2520disorders%2520like%2520multiple%2520sclerosis%2520and%250AAlzheimer%2527s%2520disease.%2520However%252C%2520MLVs%2520have%2520only%2520recently%2520been%2520described%2520for%2520the%250Afirst%2520time%2520in%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%252C%2520and%2520their%2520ramified%2520structure%250Arenders%2520manual%2520segmentation%2520particularly%2520difficult.%2520Further%252C%2520as%2520there%2520is%2520no%250Aconsistent%2520notion%2520of%2520their%2520appearance%252C%2520human-annotated%2520MLV%2520structures%2520contain%2520a%250Ahigh%2520inter-rater%2520variability%2520that%2520most%2520automatic%2520segmentation%2520methods%2520cannot%250Atake%2520into%2520account.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520new%2520rater-aware%2520training%2520scheme%250Afor%2520the%2520popular%2520nnU-Net%2520model%252C%2520and%2520we%2520explore%2520rater-based%2520ensembling%2520strategies%250Afor%2520accurate%2520and%2520consistent%2520segmentation%2520of%2520MLVs.%2520This%2520enables%2520us%2520to%2520boost%250AnnU-Net%2527s%2520performance%2520while%2520obtaining%2520explicit%2520predictions%2520in%2520different%250Aannotation%2520styles%2520and%2520a%2520rater-based%2520uncertainty%2520estimation.%2520Our%2520final%2520model%252C%250AMLV%2524%255E2%2524-Net%252C%2520achieves%2520a%2520Dice%2520similarity%2520coefficient%2520of%25200.806%2520with%2520respect%2520to%250Athe%2520human%2520reference%2520standard.%2520The%2520model%2520further%2520matches%2520the%2520human%2520inter-rater%250Areliability%2520and%2520replicates%2520age-related%2520associations%2520with%2520MLV%2520volume.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08537v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MLV%24%5E2%24-Net%3A%20Rater-Based%20Majority-Label%20Voting%20for%20Consistent%20Meningeal%0A%20%20Lymphatic%20Vessel%20Segmentation&entry.906535625=Fabian%20Bongratz%20and%20Markus%20Karmann%20and%20Adrian%20Holz%20and%20Moritz%20Bonhoeffer%20and%20Viktor%20Neumaier%20and%20Sarah%20Deli%20and%20Benita%20Schmitz-Koep%20and%20Claus%20Zimmer%20and%20Christian%20Sorg%20and%20Melissa%20Thalhammer%20and%20Dennis%20M%20Hedderich%20and%20Christian%20Wachinger&entry.1292438233=%20%20Meningeal%20lymphatic%20vessels%20%28MLVs%29%20are%20responsible%20for%20the%20drainage%20of%20waste%0Aproducts%20from%20the%20human%20brain.%20An%20impairment%20in%20their%20functionality%20has%20been%0Aassociated%20with%20aging%20as%20well%20as%20brain%20disorders%20like%20multiple%20sclerosis%20and%0AAlzheimer%27s%20disease.%20However%2C%20MLVs%20have%20only%20recently%20been%20described%20for%20the%0Afirst%20time%20in%20magnetic%20resonance%20imaging%20%28MRI%29%2C%20and%20their%20ramified%20structure%0Arenders%20manual%20segmentation%20particularly%20difficult.%20Further%2C%20as%20there%20is%20no%0Aconsistent%20notion%20of%20their%20appearance%2C%20human-annotated%20MLV%20structures%20contain%20a%0Ahigh%20inter-rater%20variability%20that%20most%20automatic%20segmentation%20methods%20cannot%0Atake%20into%20account.%20In%20this%20work%2C%20we%20propose%20a%20new%20rater-aware%20training%20scheme%0Afor%20the%20popular%20nnU-Net%20model%2C%20and%20we%20explore%20rater-based%20ensembling%20strategies%0Afor%20accurate%20and%20consistent%20segmentation%20of%20MLVs.%20This%20enables%20us%20to%20boost%0AnnU-Net%27s%20performance%20while%20obtaining%20explicit%20predictions%20in%20different%0Aannotation%20styles%20and%20a%20rater-based%20uncertainty%20estimation.%20Our%20final%20model%2C%0AMLV%24%5E2%24-Net%2C%20achieves%20a%20Dice%20similarity%20coefficient%20of%200.806%20with%20respect%20to%0Athe%20human%20reference%20standard.%20The%20model%20further%20matches%20the%20human%20inter-rater%0Areliability%20and%20replicates%20age-related%20associations%20with%20MLV%20volume.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08537v1&entry.124074799=Read"},
{"title": "On the Effects of Data Scale on UI Control Agents", "author": "Wei Li and William Bishop and Alice Li and Chris Rawles and Folawiyo Campbell-Ajala and Divya Tyamagundlu and Oriana Riva", "abstract": "  Autonomous agents that control computer interfaces to accomplish human tasks\nare emerging. Leveraging LLMs to power such agents has been of special\ninterest, but unless fine-tuned on human-collected task demonstrations,\nperformance is still relatively low. In this work we study whether fine-tuning\nalone is a viable approach for building real-world computer control agents. In\nparticularly, we investigate how performance measured on both high and\nlow-level tasks in domain and out of domain scales as more training data is\ncollected. To this end we collect and release a new dataset, AndroidControl,\nconsisting of 15,283 demonstrations of everyday tasks with Android apps.\nCompared to existing datasets, each AndroidControl task instance includes both\nhigh and low-level human-generated instructions, allowing us to explore the\nlevel of task complexity an agent can handle. Moreover, AndroidControl is the\nmost diverse computer control dataset to date, including 14,548 unique tasks\nover 833 Android apps, thus allowing us to conduct in-depth analysis of the\nmodel performance in and out of the domain of the training data. Using the\ndataset, we find that when tested in domain fine-tuned models outperform zero\nand few-shot baselines and scale in such a way that robust performance might\nfeasibly be obtained simply by collecting more data. Out of domain, performance\nscales significantly more slowly and suggests that in particular for high-level\ntasks, fine-tuning on more data alone may be insufficient for achieving robust\nout-of-domain performance.\n", "link": "http://arxiv.org/abs/2406.03679v6", "date": "2024-11-13", "relevancy": 2.0426, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5414}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4921}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4873}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Effects%20of%20Data%20Scale%20on%20UI%20Control%20Agents&body=Title%3A%20On%20the%20Effects%20of%20Data%20Scale%20on%20UI%20Control%20Agents%0AAuthor%3A%20Wei%20Li%20and%20William%20Bishop%20and%20Alice%20Li%20and%20Chris%20Rawles%20and%20Folawiyo%20Campbell-Ajala%20and%20Divya%20Tyamagundlu%20and%20Oriana%20Riva%0AAbstract%3A%20%20%20Autonomous%20agents%20that%20control%20computer%20interfaces%20to%20accomplish%20human%20tasks%0Aare%20emerging.%20Leveraging%20LLMs%20to%20power%20such%20agents%20has%20been%20of%20special%0Ainterest%2C%20but%20unless%20fine-tuned%20on%20human-collected%20task%20demonstrations%2C%0Aperformance%20is%20still%20relatively%20low.%20In%20this%20work%20we%20study%20whether%20fine-tuning%0Aalone%20is%20a%20viable%20approach%20for%20building%20real-world%20computer%20control%20agents.%20In%0Aparticularly%2C%20we%20investigate%20how%20performance%20measured%20on%20both%20high%20and%0Alow-level%20tasks%20in%20domain%20and%20out%20of%20domain%20scales%20as%20more%20training%20data%20is%0Acollected.%20To%20this%20end%20we%20collect%20and%20release%20a%20new%20dataset%2C%20AndroidControl%2C%0Aconsisting%20of%2015%2C283%20demonstrations%20of%20everyday%20tasks%20with%20Android%20apps.%0ACompared%20to%20existing%20datasets%2C%20each%20AndroidControl%20task%20instance%20includes%20both%0Ahigh%20and%20low-level%20human-generated%20instructions%2C%20allowing%20us%20to%20explore%20the%0Alevel%20of%20task%20complexity%20an%20agent%20can%20handle.%20Moreover%2C%20AndroidControl%20is%20the%0Amost%20diverse%20computer%20control%20dataset%20to%20date%2C%20including%2014%2C548%20unique%20tasks%0Aover%20833%20Android%20apps%2C%20thus%20allowing%20us%20to%20conduct%20in-depth%20analysis%20of%20the%0Amodel%20performance%20in%20and%20out%20of%20the%20domain%20of%20the%20training%20data.%20Using%20the%0Adataset%2C%20we%20find%20that%20when%20tested%20in%20domain%20fine-tuned%20models%20outperform%20zero%0Aand%20few-shot%20baselines%20and%20scale%20in%20such%20a%20way%20that%20robust%20performance%20might%0Afeasibly%20be%20obtained%20simply%20by%20collecting%20more%20data.%20Out%20of%20domain%2C%20performance%0Ascales%20significantly%20more%20slowly%20and%20suggests%20that%20in%20particular%20for%20high-level%0Atasks%2C%20fine-tuning%20on%20more%20data%20alone%20may%20be%20insufficient%20for%20achieving%20robust%0Aout-of-domain%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03679v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Effects%2520of%2520Data%2520Scale%2520on%2520UI%2520Control%2520Agents%26entry.906535625%3DWei%2520Li%2520and%2520William%2520Bishop%2520and%2520Alice%2520Li%2520and%2520Chris%2520Rawles%2520and%2520Folawiyo%2520Campbell-Ajala%2520and%2520Divya%2520Tyamagundlu%2520and%2520Oriana%2520Riva%26entry.1292438233%3D%2520%2520Autonomous%2520agents%2520that%2520control%2520computer%2520interfaces%2520to%2520accomplish%2520human%2520tasks%250Aare%2520emerging.%2520Leveraging%2520LLMs%2520to%2520power%2520such%2520agents%2520has%2520been%2520of%2520special%250Ainterest%252C%2520but%2520unless%2520fine-tuned%2520on%2520human-collected%2520task%2520demonstrations%252C%250Aperformance%2520is%2520still%2520relatively%2520low.%2520In%2520this%2520work%2520we%2520study%2520whether%2520fine-tuning%250Aalone%2520is%2520a%2520viable%2520approach%2520for%2520building%2520real-world%2520computer%2520control%2520agents.%2520In%250Aparticularly%252C%2520we%2520investigate%2520how%2520performance%2520measured%2520on%2520both%2520high%2520and%250Alow-level%2520tasks%2520in%2520domain%2520and%2520out%2520of%2520domain%2520scales%2520as%2520more%2520training%2520data%2520is%250Acollected.%2520To%2520this%2520end%2520we%2520collect%2520and%2520release%2520a%2520new%2520dataset%252C%2520AndroidControl%252C%250Aconsisting%2520of%252015%252C283%2520demonstrations%2520of%2520everyday%2520tasks%2520with%2520Android%2520apps.%250ACompared%2520to%2520existing%2520datasets%252C%2520each%2520AndroidControl%2520task%2520instance%2520includes%2520both%250Ahigh%2520and%2520low-level%2520human-generated%2520instructions%252C%2520allowing%2520us%2520to%2520explore%2520the%250Alevel%2520of%2520task%2520complexity%2520an%2520agent%2520can%2520handle.%2520Moreover%252C%2520AndroidControl%2520is%2520the%250Amost%2520diverse%2520computer%2520control%2520dataset%2520to%2520date%252C%2520including%252014%252C548%2520unique%2520tasks%250Aover%2520833%2520Android%2520apps%252C%2520thus%2520allowing%2520us%2520to%2520conduct%2520in-depth%2520analysis%2520of%2520the%250Amodel%2520performance%2520in%2520and%2520out%2520of%2520the%2520domain%2520of%2520the%2520training%2520data.%2520Using%2520the%250Adataset%252C%2520we%2520find%2520that%2520when%2520tested%2520in%2520domain%2520fine-tuned%2520models%2520outperform%2520zero%250Aand%2520few-shot%2520baselines%2520and%2520scale%2520in%2520such%2520a%2520way%2520that%2520robust%2520performance%2520might%250Afeasibly%2520be%2520obtained%2520simply%2520by%2520collecting%2520more%2520data.%2520Out%2520of%2520domain%252C%2520performance%250Ascales%2520significantly%2520more%2520slowly%2520and%2520suggests%2520that%2520in%2520particular%2520for%2520high-level%250Atasks%252C%2520fine-tuning%2520on%2520more%2520data%2520alone%2520may%2520be%2520insufficient%2520for%2520achieving%2520robust%250Aout-of-domain%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03679v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Effects%20of%20Data%20Scale%20on%20UI%20Control%20Agents&entry.906535625=Wei%20Li%20and%20William%20Bishop%20and%20Alice%20Li%20and%20Chris%20Rawles%20and%20Folawiyo%20Campbell-Ajala%20and%20Divya%20Tyamagundlu%20and%20Oriana%20Riva&entry.1292438233=%20%20Autonomous%20agents%20that%20control%20computer%20interfaces%20to%20accomplish%20human%20tasks%0Aare%20emerging.%20Leveraging%20LLMs%20to%20power%20such%20agents%20has%20been%20of%20special%0Ainterest%2C%20but%20unless%20fine-tuned%20on%20human-collected%20task%20demonstrations%2C%0Aperformance%20is%20still%20relatively%20low.%20In%20this%20work%20we%20study%20whether%20fine-tuning%0Aalone%20is%20a%20viable%20approach%20for%20building%20real-world%20computer%20control%20agents.%20In%0Aparticularly%2C%20we%20investigate%20how%20performance%20measured%20on%20both%20high%20and%0Alow-level%20tasks%20in%20domain%20and%20out%20of%20domain%20scales%20as%20more%20training%20data%20is%0Acollected.%20To%20this%20end%20we%20collect%20and%20release%20a%20new%20dataset%2C%20AndroidControl%2C%0Aconsisting%20of%2015%2C283%20demonstrations%20of%20everyday%20tasks%20with%20Android%20apps.%0ACompared%20to%20existing%20datasets%2C%20each%20AndroidControl%20task%20instance%20includes%20both%0Ahigh%20and%20low-level%20human-generated%20instructions%2C%20allowing%20us%20to%20explore%20the%0Alevel%20of%20task%20complexity%20an%20agent%20can%20handle.%20Moreover%2C%20AndroidControl%20is%20the%0Amost%20diverse%20computer%20control%20dataset%20to%20date%2C%20including%2014%2C548%20unique%20tasks%0Aover%20833%20Android%20apps%2C%20thus%20allowing%20us%20to%20conduct%20in-depth%20analysis%20of%20the%0Amodel%20performance%20in%20and%20out%20of%20the%20domain%20of%20the%20training%20data.%20Using%20the%0Adataset%2C%20we%20find%20that%20when%20tested%20in%20domain%20fine-tuned%20models%20outperform%20zero%0Aand%20few-shot%20baselines%20and%20scale%20in%20such%20a%20way%20that%20robust%20performance%20might%0Afeasibly%20be%20obtained%20simply%20by%20collecting%20more%20data.%20Out%20of%20domain%2C%20performance%0Ascales%20significantly%20more%20slowly%20and%20suggests%20that%20in%20particular%20for%20high-level%0Atasks%2C%20fine-tuning%20on%20more%20data%20alone%20may%20be%20insufficient%20for%20achieving%20robust%0Aout-of-domain%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03679v6&entry.124074799=Read"},
{"title": "Extracting polygonal footprints in off-nadir images with Segment\n  Anything Model", "author": "Kai Li and Yupeng Deng and Jingbo Chen and Yu Meng and Zhihao Xi and Junxian Ma and Chenhao Wang and Xiangyu Zhao", "abstract": "  Building Footprint Extraction (BFE) from off-nadir aerial images often\ninvolves roof segmentation and offset prediction to adjust roof boundaries to\nthe building footprint. However, this multi-stage approach typically produces\nlow-quality results, limiting its applicability in real-world data production.\nTo address this issue, we present OBMv2, an end-to-end and promptable model for\npolygonal footprint prediction. Unlike its predecessor OBM, OBMv2 introduces a\nnovel Self Offset Attention (SOFA) mechanism that improves performance across\ndiverse building types, from bungalows to skyscrapers, enabling end-to-end\nfootprint prediction without post-processing. Additionally, we propose a\nMulti-level Information System (MISS) to effectively leverage roof masks,\nbuilding masks, and offsets for accurate footprint prediction. We evaluate\nOBMv2 on the BONAI and OmniCity-view3 datasets and demonstrate its\ngeneralization on the Huizhou test set. The code will be available at\nhttps://github.com/likaiucas/OBMv2.\n", "link": "http://arxiv.org/abs/2408.08645v3", "date": "2024-11-13", "relevancy": 2.0344, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5156}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.51}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5045}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Extracting%20polygonal%20footprints%20in%20off-nadir%20images%20with%20Segment%0A%20%20Anything%20Model&body=Title%3A%20Extracting%20polygonal%20footprints%20in%20off-nadir%20images%20with%20Segment%0A%20%20Anything%20Model%0AAuthor%3A%20Kai%20Li%20and%20Yupeng%20Deng%20and%20Jingbo%20Chen%20and%20Yu%20Meng%20and%20Zhihao%20Xi%20and%20Junxian%20Ma%20and%20Chenhao%20Wang%20and%20Xiangyu%20Zhao%0AAbstract%3A%20%20%20Building%20Footprint%20Extraction%20%28BFE%29%20from%20off-nadir%20aerial%20images%20often%0Ainvolves%20roof%20segmentation%20and%20offset%20prediction%20to%20adjust%20roof%20boundaries%20to%0Athe%20building%20footprint.%20However%2C%20this%20multi-stage%20approach%20typically%20produces%0Alow-quality%20results%2C%20limiting%20its%20applicability%20in%20real-world%20data%20production.%0ATo%20address%20this%20issue%2C%20we%20present%20OBMv2%2C%20an%20end-to-end%20and%20promptable%20model%20for%0Apolygonal%20footprint%20prediction.%20Unlike%20its%20predecessor%20OBM%2C%20OBMv2%20introduces%20a%0Anovel%20Self%20Offset%20Attention%20%28SOFA%29%20mechanism%20that%20improves%20performance%20across%0Adiverse%20building%20types%2C%20from%20bungalows%20to%20skyscrapers%2C%20enabling%20end-to-end%0Afootprint%20prediction%20without%20post-processing.%20Additionally%2C%20we%20propose%20a%0AMulti-level%20Information%20System%20%28MISS%29%20to%20effectively%20leverage%20roof%20masks%2C%0Abuilding%20masks%2C%20and%20offsets%20for%20accurate%20footprint%20prediction.%20We%20evaluate%0AOBMv2%20on%20the%20BONAI%20and%20OmniCity-view3%20datasets%20and%20demonstrate%20its%0Ageneralization%20on%20the%20Huizhou%20test%20set.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/likaiucas/OBMv2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.08645v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExtracting%2520polygonal%2520footprints%2520in%2520off-nadir%2520images%2520with%2520Segment%250A%2520%2520Anything%2520Model%26entry.906535625%3DKai%2520Li%2520and%2520Yupeng%2520Deng%2520and%2520Jingbo%2520Chen%2520and%2520Yu%2520Meng%2520and%2520Zhihao%2520Xi%2520and%2520Junxian%2520Ma%2520and%2520Chenhao%2520Wang%2520and%2520Xiangyu%2520Zhao%26entry.1292438233%3D%2520%2520Building%2520Footprint%2520Extraction%2520%2528BFE%2529%2520from%2520off-nadir%2520aerial%2520images%2520often%250Ainvolves%2520roof%2520segmentation%2520and%2520offset%2520prediction%2520to%2520adjust%2520roof%2520boundaries%2520to%250Athe%2520building%2520footprint.%2520However%252C%2520this%2520multi-stage%2520approach%2520typically%2520produces%250Alow-quality%2520results%252C%2520limiting%2520its%2520applicability%2520in%2520real-world%2520data%2520production.%250ATo%2520address%2520this%2520issue%252C%2520we%2520present%2520OBMv2%252C%2520an%2520end-to-end%2520and%2520promptable%2520model%2520for%250Apolygonal%2520footprint%2520prediction.%2520Unlike%2520its%2520predecessor%2520OBM%252C%2520OBMv2%2520introduces%2520a%250Anovel%2520Self%2520Offset%2520Attention%2520%2528SOFA%2529%2520mechanism%2520that%2520improves%2520performance%2520across%250Adiverse%2520building%2520types%252C%2520from%2520bungalows%2520to%2520skyscrapers%252C%2520enabling%2520end-to-end%250Afootprint%2520prediction%2520without%2520post-processing.%2520Additionally%252C%2520we%2520propose%2520a%250AMulti-level%2520Information%2520System%2520%2528MISS%2529%2520to%2520effectively%2520leverage%2520roof%2520masks%252C%250Abuilding%2520masks%252C%2520and%2520offsets%2520for%2520accurate%2520footprint%2520prediction.%2520We%2520evaluate%250AOBMv2%2520on%2520the%2520BONAI%2520and%2520OmniCity-view3%2520datasets%2520and%2520demonstrate%2520its%250Ageneralization%2520on%2520the%2520Huizhou%2520test%2520set.%2520The%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/likaiucas/OBMv2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.08645v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Extracting%20polygonal%20footprints%20in%20off-nadir%20images%20with%20Segment%0A%20%20Anything%20Model&entry.906535625=Kai%20Li%20and%20Yupeng%20Deng%20and%20Jingbo%20Chen%20and%20Yu%20Meng%20and%20Zhihao%20Xi%20and%20Junxian%20Ma%20and%20Chenhao%20Wang%20and%20Xiangyu%20Zhao&entry.1292438233=%20%20Building%20Footprint%20Extraction%20%28BFE%29%20from%20off-nadir%20aerial%20images%20often%0Ainvolves%20roof%20segmentation%20and%20offset%20prediction%20to%20adjust%20roof%20boundaries%20to%0Athe%20building%20footprint.%20However%2C%20this%20multi-stage%20approach%20typically%20produces%0Alow-quality%20results%2C%20limiting%20its%20applicability%20in%20real-world%20data%20production.%0ATo%20address%20this%20issue%2C%20we%20present%20OBMv2%2C%20an%20end-to-end%20and%20promptable%20model%20for%0Apolygonal%20footprint%20prediction.%20Unlike%20its%20predecessor%20OBM%2C%20OBMv2%20introduces%20a%0Anovel%20Self%20Offset%20Attention%20%28SOFA%29%20mechanism%20that%20improves%20performance%20across%0Adiverse%20building%20types%2C%20from%20bungalows%20to%20skyscrapers%2C%20enabling%20end-to-end%0Afootprint%20prediction%20without%20post-processing.%20Additionally%2C%20we%20propose%20a%0AMulti-level%20Information%20System%20%28MISS%29%20to%20effectively%20leverage%20roof%20masks%2C%0Abuilding%20masks%2C%20and%20offsets%20for%20accurate%20footprint%20prediction.%20We%20evaluate%0AOBMv2%20on%20the%20BONAI%20and%20OmniCity-view3%20datasets%20and%20demonstrate%20its%0Ageneralization%20on%20the%20Huizhou%20test%20set.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/likaiucas/OBMv2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.08645v3&entry.124074799=Read"},
{"title": "SAD-TIME: a Spatiotemporal-fused network for depression detection with\n  Automated multi-scale Depth-wise and TIME-interval-related common feature\n  extractor", "author": "Han-Guang Wang and Hui-Rang Hou and Li-Cheng Jin and Chen-Yang Xu and Zhong-Yi Zhang and Qing-Hao Meng", "abstract": "  Background and Objective: Depression is a severe mental disorder, and\naccurate diagnosis is pivotal to the cure and rehabilitation of people with\ndepression. However, the current questionnaire-based diagnostic methods could\nbring subjective biases and may be denied by subjects. In search of a more\nobjective means of diagnosis, researchers have begun to experiment with deep\nlearning-based methods for identifying depressive disorders in recent years.\nMethods: In this study, a novel Spatiotemporal-fused network with Automated\nmulti-scale Depth-wise and TIME-interval-related common feature extractor\n(SAD-TIME) is proposed. SAD-TIME incorporates an automated nodes' common\nfeatures extractor (CFE), a spatial sector (SpS), a modified temporal sector\n(TeS), and a domain adversarial learner (DAL). The CFE includes a multi-scale\ndepth-wise 1D-convolutional neural network and a time-interval embedding\ngenerator, where the unique information of each channel is preserved. The SpS\nfuses the functional connectivity with the distance-based connectivity\ncontaining spatial position of EEG electrodes. A multi-head-attention graph\nconvolutional network is also applied in the SpS to fuse the features from\ndifferent EEG channels. The TeS is based on long short-term memory and graph\ntransformer networks, where the temporal information of different time-windows\nis fused. Moreover, the DAL is used after the SpS to obtain the\ndomain-invariant feature. Results: Experimental results under tenfold\ncross-validation show that the proposed SAD-TIME method achieves 92.00% and\n94.00% depression classification accuracies on two datasets, respectively, in\ncross-subject mode. Conclusion: SAD-TIME is a robust depression detection\nmodel, where the automatedly-generated features, the SpS and the TeS assist the\nclassification performance with the fusion of the innate spatiotemporal\ninformation in the EEG signals.\n", "link": "http://arxiv.org/abs/2411.08521v1", "date": "2024-11-13", "relevancy": 2.02, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5159}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5064}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4935}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAD-TIME%3A%20a%20Spatiotemporal-fused%20network%20for%20depression%20detection%20with%0A%20%20Automated%20multi-scale%20Depth-wise%20and%20TIME-interval-related%20common%20feature%0A%20%20extractor&body=Title%3A%20SAD-TIME%3A%20a%20Spatiotemporal-fused%20network%20for%20depression%20detection%20with%0A%20%20Automated%20multi-scale%20Depth-wise%20and%20TIME-interval-related%20common%20feature%0A%20%20extractor%0AAuthor%3A%20Han-Guang%20Wang%20and%20Hui-Rang%20Hou%20and%20Li-Cheng%20Jin%20and%20Chen-Yang%20Xu%20and%20Zhong-Yi%20Zhang%20and%20Qing-Hao%20Meng%0AAbstract%3A%20%20%20Background%20and%20Objective%3A%20Depression%20is%20a%20severe%20mental%20disorder%2C%20and%0Aaccurate%20diagnosis%20is%20pivotal%20to%20the%20cure%20and%20rehabilitation%20of%20people%20with%0Adepression.%20However%2C%20the%20current%20questionnaire-based%20diagnostic%20methods%20could%0Abring%20subjective%20biases%20and%20may%20be%20denied%20by%20subjects.%20In%20search%20of%20a%20more%0Aobjective%20means%20of%20diagnosis%2C%20researchers%20have%20begun%20to%20experiment%20with%20deep%0Alearning-based%20methods%20for%20identifying%20depressive%20disorders%20in%20recent%20years.%0AMethods%3A%20In%20this%20study%2C%20a%20novel%20Spatiotemporal-fused%20network%20with%20Automated%0Amulti-scale%20Depth-wise%20and%20TIME-interval-related%20common%20feature%20extractor%0A%28SAD-TIME%29%20is%20proposed.%20SAD-TIME%20incorporates%20an%20automated%20nodes%27%20common%0Afeatures%20extractor%20%28CFE%29%2C%20a%20spatial%20sector%20%28SpS%29%2C%20a%20modified%20temporal%20sector%0A%28TeS%29%2C%20and%20a%20domain%20adversarial%20learner%20%28DAL%29.%20The%20CFE%20includes%20a%20multi-scale%0Adepth-wise%201D-convolutional%20neural%20network%20and%20a%20time-interval%20embedding%0Agenerator%2C%20where%20the%20unique%20information%20of%20each%20channel%20is%20preserved.%20The%20SpS%0Afuses%20the%20functional%20connectivity%20with%20the%20distance-based%20connectivity%0Acontaining%20spatial%20position%20of%20EEG%20electrodes.%20A%20multi-head-attention%20graph%0Aconvolutional%20network%20is%20also%20applied%20in%20the%20SpS%20to%20fuse%20the%20features%20from%0Adifferent%20EEG%20channels.%20The%20TeS%20is%20based%20on%20long%20short-term%20memory%20and%20graph%0Atransformer%20networks%2C%20where%20the%20temporal%20information%20of%20different%20time-windows%0Ais%20fused.%20Moreover%2C%20the%20DAL%20is%20used%20after%20the%20SpS%20to%20obtain%20the%0Adomain-invariant%20feature.%20Results%3A%20Experimental%20results%20under%20tenfold%0Across-validation%20show%20that%20the%20proposed%20SAD-TIME%20method%20achieves%2092.00%25%20and%0A94.00%25%20depression%20classification%20accuracies%20on%20two%20datasets%2C%20respectively%2C%20in%0Across-subject%20mode.%20Conclusion%3A%20SAD-TIME%20is%20a%20robust%20depression%20detection%0Amodel%2C%20where%20the%20automatedly-generated%20features%2C%20the%20SpS%20and%20the%20TeS%20assist%20the%0Aclassification%20performance%20with%20the%20fusion%20of%20the%20innate%20spatiotemporal%0Ainformation%20in%20the%20EEG%20signals.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08521v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAD-TIME%253A%2520a%2520Spatiotemporal-fused%2520network%2520for%2520depression%2520detection%2520with%250A%2520%2520Automated%2520multi-scale%2520Depth-wise%2520and%2520TIME-interval-related%2520common%2520feature%250A%2520%2520extractor%26entry.906535625%3DHan-Guang%2520Wang%2520and%2520Hui-Rang%2520Hou%2520and%2520Li-Cheng%2520Jin%2520and%2520Chen-Yang%2520Xu%2520and%2520Zhong-Yi%2520Zhang%2520and%2520Qing-Hao%2520Meng%26entry.1292438233%3D%2520%2520Background%2520and%2520Objective%253A%2520Depression%2520is%2520a%2520severe%2520mental%2520disorder%252C%2520and%250Aaccurate%2520diagnosis%2520is%2520pivotal%2520to%2520the%2520cure%2520and%2520rehabilitation%2520of%2520people%2520with%250Adepression.%2520However%252C%2520the%2520current%2520questionnaire-based%2520diagnostic%2520methods%2520could%250Abring%2520subjective%2520biases%2520and%2520may%2520be%2520denied%2520by%2520subjects.%2520In%2520search%2520of%2520a%2520more%250Aobjective%2520means%2520of%2520diagnosis%252C%2520researchers%2520have%2520begun%2520to%2520experiment%2520with%2520deep%250Alearning-based%2520methods%2520for%2520identifying%2520depressive%2520disorders%2520in%2520recent%2520years.%250AMethods%253A%2520In%2520this%2520study%252C%2520a%2520novel%2520Spatiotemporal-fused%2520network%2520with%2520Automated%250Amulti-scale%2520Depth-wise%2520and%2520TIME-interval-related%2520common%2520feature%2520extractor%250A%2528SAD-TIME%2529%2520is%2520proposed.%2520SAD-TIME%2520incorporates%2520an%2520automated%2520nodes%2527%2520common%250Afeatures%2520extractor%2520%2528CFE%2529%252C%2520a%2520spatial%2520sector%2520%2528SpS%2529%252C%2520a%2520modified%2520temporal%2520sector%250A%2528TeS%2529%252C%2520and%2520a%2520domain%2520adversarial%2520learner%2520%2528DAL%2529.%2520The%2520CFE%2520includes%2520a%2520multi-scale%250Adepth-wise%25201D-convolutional%2520neural%2520network%2520and%2520a%2520time-interval%2520embedding%250Agenerator%252C%2520where%2520the%2520unique%2520information%2520of%2520each%2520channel%2520is%2520preserved.%2520The%2520SpS%250Afuses%2520the%2520functional%2520connectivity%2520with%2520the%2520distance-based%2520connectivity%250Acontaining%2520spatial%2520position%2520of%2520EEG%2520electrodes.%2520A%2520multi-head-attention%2520graph%250Aconvolutional%2520network%2520is%2520also%2520applied%2520in%2520the%2520SpS%2520to%2520fuse%2520the%2520features%2520from%250Adifferent%2520EEG%2520channels.%2520The%2520TeS%2520is%2520based%2520on%2520long%2520short-term%2520memory%2520and%2520graph%250Atransformer%2520networks%252C%2520where%2520the%2520temporal%2520information%2520of%2520different%2520time-windows%250Ais%2520fused.%2520Moreover%252C%2520the%2520DAL%2520is%2520used%2520after%2520the%2520SpS%2520to%2520obtain%2520the%250Adomain-invariant%2520feature.%2520Results%253A%2520Experimental%2520results%2520under%2520tenfold%250Across-validation%2520show%2520that%2520the%2520proposed%2520SAD-TIME%2520method%2520achieves%252092.00%2525%2520and%250A94.00%2525%2520depression%2520classification%2520accuracies%2520on%2520two%2520datasets%252C%2520respectively%252C%2520in%250Across-subject%2520mode.%2520Conclusion%253A%2520SAD-TIME%2520is%2520a%2520robust%2520depression%2520detection%250Amodel%252C%2520where%2520the%2520automatedly-generated%2520features%252C%2520the%2520SpS%2520and%2520the%2520TeS%2520assist%2520the%250Aclassification%2520performance%2520with%2520the%2520fusion%2520of%2520the%2520innate%2520spatiotemporal%250Ainformation%2520in%2520the%2520EEG%2520signals.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08521v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAD-TIME%3A%20a%20Spatiotemporal-fused%20network%20for%20depression%20detection%20with%0A%20%20Automated%20multi-scale%20Depth-wise%20and%20TIME-interval-related%20common%20feature%0A%20%20extractor&entry.906535625=Han-Guang%20Wang%20and%20Hui-Rang%20Hou%20and%20Li-Cheng%20Jin%20and%20Chen-Yang%20Xu%20and%20Zhong-Yi%20Zhang%20and%20Qing-Hao%20Meng&entry.1292438233=%20%20Background%20and%20Objective%3A%20Depression%20is%20a%20severe%20mental%20disorder%2C%20and%0Aaccurate%20diagnosis%20is%20pivotal%20to%20the%20cure%20and%20rehabilitation%20of%20people%20with%0Adepression.%20However%2C%20the%20current%20questionnaire-based%20diagnostic%20methods%20could%0Abring%20subjective%20biases%20and%20may%20be%20denied%20by%20subjects.%20In%20search%20of%20a%20more%0Aobjective%20means%20of%20diagnosis%2C%20researchers%20have%20begun%20to%20experiment%20with%20deep%0Alearning-based%20methods%20for%20identifying%20depressive%20disorders%20in%20recent%20years.%0AMethods%3A%20In%20this%20study%2C%20a%20novel%20Spatiotemporal-fused%20network%20with%20Automated%0Amulti-scale%20Depth-wise%20and%20TIME-interval-related%20common%20feature%20extractor%0A%28SAD-TIME%29%20is%20proposed.%20SAD-TIME%20incorporates%20an%20automated%20nodes%27%20common%0Afeatures%20extractor%20%28CFE%29%2C%20a%20spatial%20sector%20%28SpS%29%2C%20a%20modified%20temporal%20sector%0A%28TeS%29%2C%20and%20a%20domain%20adversarial%20learner%20%28DAL%29.%20The%20CFE%20includes%20a%20multi-scale%0Adepth-wise%201D-convolutional%20neural%20network%20and%20a%20time-interval%20embedding%0Agenerator%2C%20where%20the%20unique%20information%20of%20each%20channel%20is%20preserved.%20The%20SpS%0Afuses%20the%20functional%20connectivity%20with%20the%20distance-based%20connectivity%0Acontaining%20spatial%20position%20of%20EEG%20electrodes.%20A%20multi-head-attention%20graph%0Aconvolutional%20network%20is%20also%20applied%20in%20the%20SpS%20to%20fuse%20the%20features%20from%0Adifferent%20EEG%20channels.%20The%20TeS%20is%20based%20on%20long%20short-term%20memory%20and%20graph%0Atransformer%20networks%2C%20where%20the%20temporal%20information%20of%20different%20time-windows%0Ais%20fused.%20Moreover%2C%20the%20DAL%20is%20used%20after%20the%20SpS%20to%20obtain%20the%0Adomain-invariant%20feature.%20Results%3A%20Experimental%20results%20under%20tenfold%0Across-validation%20show%20that%20the%20proposed%20SAD-TIME%20method%20achieves%2092.00%25%20and%0A94.00%25%20depression%20classification%20accuracies%20on%20two%20datasets%2C%20respectively%2C%20in%0Across-subject%20mode.%20Conclusion%3A%20SAD-TIME%20is%20a%20robust%20depression%20detection%0Amodel%2C%20where%20the%20automatedly-generated%20features%2C%20the%20SpS%20and%20the%20TeS%20assist%20the%0Aclassification%20performance%20with%20the%20fusion%20of%20the%20innate%20spatiotemporal%0Ainformation%20in%20the%20EEG%20signals.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08521v1&entry.124074799=Read"},
{"title": "Zero-shot Cross-lingual Transfer Learning with Multiple Source and\n  Target Languages for Information Extraction: Language Selection and\n  Adversarial Training", "author": "Nghia Trung Ngo and Thien Huu Nguyen", "abstract": "  The majority of previous researches addressing multi-lingual IE are limited\nto zero-shot cross-lingual single-transfer (one-to-one) setting, with\nhigh-resource languages predominantly as source training data. As a result,\nthese works provide little understanding and benefit for the realistic goal of\ndeveloping a multi-lingual IE system that can generalize to as many languages\nas possible. Our study aims to fill this gap by providing a detailed analysis\non Cross-Lingual Multi-Transferability (many-to-many transfer learning), for\nthe recent IE corpora that cover a diverse set of languages. Specifically, we\nfirst determine the correlation between single-transfer performance and a wide\nrange of linguistic-based distances. From the obtained insights, a combined\nlanguage distance metric can be developed that is not only highly correlated\nbut also robust across different tasks and model scales. Next, we investigate\nthe more general zero-shot multi-lingual transfer settings where multiple\nlanguages are involved in the training and evaluation processes. Language\nclustering based on the newly defined distance can provide directions for\nachieving the optimal cost-performance trade-off in data (languages) selection\nproblem. Finally, a relational-transfer setting is proposed to further\nincorporate multi-lingual unlabeled data based on adversarial training using\nthe relation induced from the above linguistic distance.\n", "link": "http://arxiv.org/abs/2411.08785v1", "date": "2024-11-13", "relevancy": 2.011, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5209}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4908}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4894}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-shot%20Cross-lingual%20Transfer%20Learning%20with%20Multiple%20Source%20and%0A%20%20Target%20Languages%20for%20Information%20Extraction%3A%20Language%20Selection%20and%0A%20%20Adversarial%20Training&body=Title%3A%20Zero-shot%20Cross-lingual%20Transfer%20Learning%20with%20Multiple%20Source%20and%0A%20%20Target%20Languages%20for%20Information%20Extraction%3A%20Language%20Selection%20and%0A%20%20Adversarial%20Training%0AAuthor%3A%20Nghia%20Trung%20Ngo%20and%20Thien%20Huu%20Nguyen%0AAbstract%3A%20%20%20The%20majority%20of%20previous%20researches%20addressing%20multi-lingual%20IE%20are%20limited%0Ato%20zero-shot%20cross-lingual%20single-transfer%20%28one-to-one%29%20setting%2C%20with%0Ahigh-resource%20languages%20predominantly%20as%20source%20training%20data.%20As%20a%20result%2C%0Athese%20works%20provide%20little%20understanding%20and%20benefit%20for%20the%20realistic%20goal%20of%0Adeveloping%20a%20multi-lingual%20IE%20system%20that%20can%20generalize%20to%20as%20many%20languages%0Aas%20possible.%20Our%20study%20aims%20to%20fill%20this%20gap%20by%20providing%20a%20detailed%20analysis%0Aon%20Cross-Lingual%20Multi-Transferability%20%28many-to-many%20transfer%20learning%29%2C%20for%0Athe%20recent%20IE%20corpora%20that%20cover%20a%20diverse%20set%20of%20languages.%20Specifically%2C%20we%0Afirst%20determine%20the%20correlation%20between%20single-transfer%20performance%20and%20a%20wide%0Arange%20of%20linguistic-based%20distances.%20From%20the%20obtained%20insights%2C%20a%20combined%0Alanguage%20distance%20metric%20can%20be%20developed%20that%20is%20not%20only%20highly%20correlated%0Abut%20also%20robust%20across%20different%20tasks%20and%20model%20scales.%20Next%2C%20we%20investigate%0Athe%20more%20general%20zero-shot%20multi-lingual%20transfer%20settings%20where%20multiple%0Alanguages%20are%20involved%20in%20the%20training%20and%20evaluation%20processes.%20Language%0Aclustering%20based%20on%20the%20newly%20defined%20distance%20can%20provide%20directions%20for%0Aachieving%20the%20optimal%20cost-performance%20trade-off%20in%20data%20%28languages%29%20selection%0Aproblem.%20Finally%2C%20a%20relational-transfer%20setting%20is%20proposed%20to%20further%0Aincorporate%20multi-lingual%20unlabeled%20data%20based%20on%20adversarial%20training%20using%0Athe%20relation%20induced%20from%20the%20above%20linguistic%20distance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08785v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-shot%2520Cross-lingual%2520Transfer%2520Learning%2520with%2520Multiple%2520Source%2520and%250A%2520%2520Target%2520Languages%2520for%2520Information%2520Extraction%253A%2520Language%2520Selection%2520and%250A%2520%2520Adversarial%2520Training%26entry.906535625%3DNghia%2520Trung%2520Ngo%2520and%2520Thien%2520Huu%2520Nguyen%26entry.1292438233%3D%2520%2520The%2520majority%2520of%2520previous%2520researches%2520addressing%2520multi-lingual%2520IE%2520are%2520limited%250Ato%2520zero-shot%2520cross-lingual%2520single-transfer%2520%2528one-to-one%2529%2520setting%252C%2520with%250Ahigh-resource%2520languages%2520predominantly%2520as%2520source%2520training%2520data.%2520As%2520a%2520result%252C%250Athese%2520works%2520provide%2520little%2520understanding%2520and%2520benefit%2520for%2520the%2520realistic%2520goal%2520of%250Adeveloping%2520a%2520multi-lingual%2520IE%2520system%2520that%2520can%2520generalize%2520to%2520as%2520many%2520languages%250Aas%2520possible.%2520Our%2520study%2520aims%2520to%2520fill%2520this%2520gap%2520by%2520providing%2520a%2520detailed%2520analysis%250Aon%2520Cross-Lingual%2520Multi-Transferability%2520%2528many-to-many%2520transfer%2520learning%2529%252C%2520for%250Athe%2520recent%2520IE%2520corpora%2520that%2520cover%2520a%2520diverse%2520set%2520of%2520languages.%2520Specifically%252C%2520we%250Afirst%2520determine%2520the%2520correlation%2520between%2520single-transfer%2520performance%2520and%2520a%2520wide%250Arange%2520of%2520linguistic-based%2520distances.%2520From%2520the%2520obtained%2520insights%252C%2520a%2520combined%250Alanguage%2520distance%2520metric%2520can%2520be%2520developed%2520that%2520is%2520not%2520only%2520highly%2520correlated%250Abut%2520also%2520robust%2520across%2520different%2520tasks%2520and%2520model%2520scales.%2520Next%252C%2520we%2520investigate%250Athe%2520more%2520general%2520zero-shot%2520multi-lingual%2520transfer%2520settings%2520where%2520multiple%250Alanguages%2520are%2520involved%2520in%2520the%2520training%2520and%2520evaluation%2520processes.%2520Language%250Aclustering%2520based%2520on%2520the%2520newly%2520defined%2520distance%2520can%2520provide%2520directions%2520for%250Aachieving%2520the%2520optimal%2520cost-performance%2520trade-off%2520in%2520data%2520%2528languages%2529%2520selection%250Aproblem.%2520Finally%252C%2520a%2520relational-transfer%2520setting%2520is%2520proposed%2520to%2520further%250Aincorporate%2520multi-lingual%2520unlabeled%2520data%2520based%2520on%2520adversarial%2520training%2520using%250Athe%2520relation%2520induced%2520from%2520the%2520above%2520linguistic%2520distance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08785v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-shot%20Cross-lingual%20Transfer%20Learning%20with%20Multiple%20Source%20and%0A%20%20Target%20Languages%20for%20Information%20Extraction%3A%20Language%20Selection%20and%0A%20%20Adversarial%20Training&entry.906535625=Nghia%20Trung%20Ngo%20and%20Thien%20Huu%20Nguyen&entry.1292438233=%20%20The%20majority%20of%20previous%20researches%20addressing%20multi-lingual%20IE%20are%20limited%0Ato%20zero-shot%20cross-lingual%20single-transfer%20%28one-to-one%29%20setting%2C%20with%0Ahigh-resource%20languages%20predominantly%20as%20source%20training%20data.%20As%20a%20result%2C%0Athese%20works%20provide%20little%20understanding%20and%20benefit%20for%20the%20realistic%20goal%20of%0Adeveloping%20a%20multi-lingual%20IE%20system%20that%20can%20generalize%20to%20as%20many%20languages%0Aas%20possible.%20Our%20study%20aims%20to%20fill%20this%20gap%20by%20providing%20a%20detailed%20analysis%0Aon%20Cross-Lingual%20Multi-Transferability%20%28many-to-many%20transfer%20learning%29%2C%20for%0Athe%20recent%20IE%20corpora%20that%20cover%20a%20diverse%20set%20of%20languages.%20Specifically%2C%20we%0Afirst%20determine%20the%20correlation%20between%20single-transfer%20performance%20and%20a%20wide%0Arange%20of%20linguistic-based%20distances.%20From%20the%20obtained%20insights%2C%20a%20combined%0Alanguage%20distance%20metric%20can%20be%20developed%20that%20is%20not%20only%20highly%20correlated%0Abut%20also%20robust%20across%20different%20tasks%20and%20model%20scales.%20Next%2C%20we%20investigate%0Athe%20more%20general%20zero-shot%20multi-lingual%20transfer%20settings%20where%20multiple%0Alanguages%20are%20involved%20in%20the%20training%20and%20evaluation%20processes.%20Language%0Aclustering%20based%20on%20the%20newly%20defined%20distance%20can%20provide%20directions%20for%0Aachieving%20the%20optimal%20cost-performance%20trade-off%20in%20data%20%28languages%29%20selection%0Aproblem.%20Finally%2C%20a%20relational-transfer%20setting%20is%20proposed%20to%20further%0Aincorporate%20multi-lingual%20unlabeled%20data%20based%20on%20adversarial%20training%20using%0Athe%20relation%20induced%20from%20the%20above%20linguistic%20distance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08785v1&entry.124074799=Read"},
{"title": "Polymetis:Large Language Modeling for Multiple Material Domains", "author": "Chao Huang and Huichen Xiao and Chen Chen and Chunyan Chen and Yi Zhao and Shiyu Du and Yiming Zhang and He Sha and Ruixin Gu", "abstract": "  As the application of large language models in various fields continues to\nexpand, materials science also ushers in opportunities for AI-driven\ninnovation. The traditional way of relying on manual search for materials\nscience-related information is now using artificial intelligence technology as\nan auxiliary tool to improve the efficiency of materials science research. To\naccelerate researchers' knowledge acquisition and intelligent decision-making\nsupport in materials science research, this paper proposes a large language\nmodel Polymetis model for a variety of materials fields, aiming to provide\nhighly professional knowledge answers in the field of materials, covering\nenergy materials, functional materials, alloy materials, physical chemistry,\nbiology, and other material directions. The model uses a dataset of about 2\nmillion material knowledge instructions, and in the process of building the\ndataset, we developed the Intelligent Extraction Large Model (IELM), which is\nspecially used to extract and form structured knowledge from scientific texts,\navoiding a large number of costs that need to be manually annotated, and\nimproving efficiency. We inject this data into the GLM4-9B model for learning\nto enhance its inference capabilities in a variety of material domains. In\naddition, we have introduced enhanced prompt strategies to ensure that the\nanswers to the model are more organized and comprehensive, providing efficient\nand comprehensive intelligent support for the diverse needs of materials\nscience exploration, and promoting the development of material science.\n", "link": "http://arxiv.org/abs/2411.08728v1", "date": "2024-11-13", "relevancy": 1.9954, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5008}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4985}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Polymetis%3ALarge%20Language%20Modeling%20for%20Multiple%20Material%20Domains&body=Title%3A%20Polymetis%3ALarge%20Language%20Modeling%20for%20Multiple%20Material%20Domains%0AAuthor%3A%20Chao%20Huang%20and%20Huichen%20Xiao%20and%20Chen%20Chen%20and%20Chunyan%20Chen%20and%20Yi%20Zhao%20and%20Shiyu%20Du%20and%20Yiming%20Zhang%20and%20He%20Sha%20and%20Ruixin%20Gu%0AAbstract%3A%20%20%20As%20the%20application%20of%20large%20language%20models%20in%20various%20fields%20continues%20to%0Aexpand%2C%20materials%20science%20also%20ushers%20in%20opportunities%20for%20AI-driven%0Ainnovation.%20The%20traditional%20way%20of%20relying%20on%20manual%20search%20for%20materials%0Ascience-related%20information%20is%20now%20using%20artificial%20intelligence%20technology%20as%0Aan%20auxiliary%20tool%20to%20improve%20the%20efficiency%20of%20materials%20science%20research.%20To%0Aaccelerate%20researchers%27%20knowledge%20acquisition%20and%20intelligent%20decision-making%0Asupport%20in%20materials%20science%20research%2C%20this%20paper%20proposes%20a%20large%20language%0Amodel%20Polymetis%20model%20for%20a%20variety%20of%20materials%20fields%2C%20aiming%20to%20provide%0Ahighly%20professional%20knowledge%20answers%20in%20the%20field%20of%20materials%2C%20covering%0Aenergy%20materials%2C%20functional%20materials%2C%20alloy%20materials%2C%20physical%20chemistry%2C%0Abiology%2C%20and%20other%20material%20directions.%20The%20model%20uses%20a%20dataset%20of%20about%202%0Amillion%20material%20knowledge%20instructions%2C%20and%20in%20the%20process%20of%20building%20the%0Adataset%2C%20we%20developed%20the%20Intelligent%20Extraction%20Large%20Model%20%28IELM%29%2C%20which%20is%0Aspecially%20used%20to%20extract%20and%20form%20structured%20knowledge%20from%20scientific%20texts%2C%0Aavoiding%20a%20large%20number%20of%20costs%20that%20need%20to%20be%20manually%20annotated%2C%20and%0Aimproving%20efficiency.%20We%20inject%20this%20data%20into%20the%20GLM4-9B%20model%20for%20learning%0Ato%20enhance%20its%20inference%20capabilities%20in%20a%20variety%20of%20material%20domains.%20In%0Aaddition%2C%20we%20have%20introduced%20enhanced%20prompt%20strategies%20to%20ensure%20that%20the%0Aanswers%20to%20the%20model%20are%20more%20organized%20and%20comprehensive%2C%20providing%20efficient%0Aand%20comprehensive%20intelligent%20support%20for%20the%20diverse%20needs%20of%20materials%0Ascience%20exploration%2C%20and%20promoting%20the%20development%20of%20material%20science.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08728v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPolymetis%253ALarge%2520Language%2520Modeling%2520for%2520Multiple%2520Material%2520Domains%26entry.906535625%3DChao%2520Huang%2520and%2520Huichen%2520Xiao%2520and%2520Chen%2520Chen%2520and%2520Chunyan%2520Chen%2520and%2520Yi%2520Zhao%2520and%2520Shiyu%2520Du%2520and%2520Yiming%2520Zhang%2520and%2520He%2520Sha%2520and%2520Ruixin%2520Gu%26entry.1292438233%3D%2520%2520As%2520the%2520application%2520of%2520large%2520language%2520models%2520in%2520various%2520fields%2520continues%2520to%250Aexpand%252C%2520materials%2520science%2520also%2520ushers%2520in%2520opportunities%2520for%2520AI-driven%250Ainnovation.%2520The%2520traditional%2520way%2520of%2520relying%2520on%2520manual%2520search%2520for%2520materials%250Ascience-related%2520information%2520is%2520now%2520using%2520artificial%2520intelligence%2520technology%2520as%250Aan%2520auxiliary%2520tool%2520to%2520improve%2520the%2520efficiency%2520of%2520materials%2520science%2520research.%2520To%250Aaccelerate%2520researchers%2527%2520knowledge%2520acquisition%2520and%2520intelligent%2520decision-making%250Asupport%2520in%2520materials%2520science%2520research%252C%2520this%2520paper%2520proposes%2520a%2520large%2520language%250Amodel%2520Polymetis%2520model%2520for%2520a%2520variety%2520of%2520materials%2520fields%252C%2520aiming%2520to%2520provide%250Ahighly%2520professional%2520knowledge%2520answers%2520in%2520the%2520field%2520of%2520materials%252C%2520covering%250Aenergy%2520materials%252C%2520functional%2520materials%252C%2520alloy%2520materials%252C%2520physical%2520chemistry%252C%250Abiology%252C%2520and%2520other%2520material%2520directions.%2520The%2520model%2520uses%2520a%2520dataset%2520of%2520about%25202%250Amillion%2520material%2520knowledge%2520instructions%252C%2520and%2520in%2520the%2520process%2520of%2520building%2520the%250Adataset%252C%2520we%2520developed%2520the%2520Intelligent%2520Extraction%2520Large%2520Model%2520%2528IELM%2529%252C%2520which%2520is%250Aspecially%2520used%2520to%2520extract%2520and%2520form%2520structured%2520knowledge%2520from%2520scientific%2520texts%252C%250Aavoiding%2520a%2520large%2520number%2520of%2520costs%2520that%2520need%2520to%2520be%2520manually%2520annotated%252C%2520and%250Aimproving%2520efficiency.%2520We%2520inject%2520this%2520data%2520into%2520the%2520GLM4-9B%2520model%2520for%2520learning%250Ato%2520enhance%2520its%2520inference%2520capabilities%2520in%2520a%2520variety%2520of%2520material%2520domains.%2520In%250Aaddition%252C%2520we%2520have%2520introduced%2520enhanced%2520prompt%2520strategies%2520to%2520ensure%2520that%2520the%250Aanswers%2520to%2520the%2520model%2520are%2520more%2520organized%2520and%2520comprehensive%252C%2520providing%2520efficient%250Aand%2520comprehensive%2520intelligent%2520support%2520for%2520the%2520diverse%2520needs%2520of%2520materials%250Ascience%2520exploration%252C%2520and%2520promoting%2520the%2520development%2520of%2520material%2520science.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08728v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Polymetis%3ALarge%20Language%20Modeling%20for%20Multiple%20Material%20Domains&entry.906535625=Chao%20Huang%20and%20Huichen%20Xiao%20and%20Chen%20Chen%20and%20Chunyan%20Chen%20and%20Yi%20Zhao%20and%20Shiyu%20Du%20and%20Yiming%20Zhang%20and%20He%20Sha%20and%20Ruixin%20Gu&entry.1292438233=%20%20As%20the%20application%20of%20large%20language%20models%20in%20various%20fields%20continues%20to%0Aexpand%2C%20materials%20science%20also%20ushers%20in%20opportunities%20for%20AI-driven%0Ainnovation.%20The%20traditional%20way%20of%20relying%20on%20manual%20search%20for%20materials%0Ascience-related%20information%20is%20now%20using%20artificial%20intelligence%20technology%20as%0Aan%20auxiliary%20tool%20to%20improve%20the%20efficiency%20of%20materials%20science%20research.%20To%0Aaccelerate%20researchers%27%20knowledge%20acquisition%20and%20intelligent%20decision-making%0Asupport%20in%20materials%20science%20research%2C%20this%20paper%20proposes%20a%20large%20language%0Amodel%20Polymetis%20model%20for%20a%20variety%20of%20materials%20fields%2C%20aiming%20to%20provide%0Ahighly%20professional%20knowledge%20answers%20in%20the%20field%20of%20materials%2C%20covering%0Aenergy%20materials%2C%20functional%20materials%2C%20alloy%20materials%2C%20physical%20chemistry%2C%0Abiology%2C%20and%20other%20material%20directions.%20The%20model%20uses%20a%20dataset%20of%20about%202%0Amillion%20material%20knowledge%20instructions%2C%20and%20in%20the%20process%20of%20building%20the%0Adataset%2C%20we%20developed%20the%20Intelligent%20Extraction%20Large%20Model%20%28IELM%29%2C%20which%20is%0Aspecially%20used%20to%20extract%20and%20form%20structured%20knowledge%20from%20scientific%20texts%2C%0Aavoiding%20a%20large%20number%20of%20costs%20that%20need%20to%20be%20manually%20annotated%2C%20and%0Aimproving%20efficiency.%20We%20inject%20this%20data%20into%20the%20GLM4-9B%20model%20for%20learning%0Ato%20enhance%20its%20inference%20capabilities%20in%20a%20variety%20of%20material%20domains.%20In%0Aaddition%2C%20we%20have%20introduced%20enhanced%20prompt%20strategies%20to%20ensure%20that%20the%0Aanswers%20to%20the%20model%20are%20more%20organized%20and%20comprehensive%2C%20providing%20efficient%0Aand%20comprehensive%20intelligent%20support%20for%20the%20diverse%20needs%20of%20materials%0Ascience%20exploration%2C%20and%20promoting%20the%20development%20of%20material%20science.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08728v1&entry.124074799=Read"},
{"title": "Optimal Transport on the Lie Group of Roto-translations", "author": "Daan Bon and Gautam Pai and Gijs Bellaard and Olga Mula and Remco Duits", "abstract": "  The roto-translation group SE2 has been of active interest in image analysis\ndue to methods that lift the image data to multi-orientation representations\ndefined on this Lie group. This has led to impactful applications of\ncrossing-preserving flows for image de-noising, geodesic tracking, and\nroto-translation equivariant deep learning. In this paper, we develop a\ncomputational framework for optimal transportation over Lie groups, with a\nspecial focus on SE2. We make several theoretical contributions (generalizable\nto matrix Lie groups) such as the non-optimality of group actions as transport\nmaps, invariance and equivariance of optimal transport, and the quality of the\nentropic-regularized optimal transport plan using geodesic distance\napproximations. We develop a Sinkhorn like algorithm that can be efficiently\nimplemented using fast and accurate distance approximations of the Lie group\nand GPU-friendly group convolutions. We report valuable advancements in the\nexperiments on 1) image barycentric interpolation, 2) interpolation of planar\norientation fields, and 3) Wasserstein gradient flows on SE2. We observe that\nour framework of lifting images to SE2 and optimal transport with\nleft-invariant anisotropic metrics leads to equivariant transport along\ndominant contours and salient line structures in the image. This yields sharper\nand more meaningful interpolations compared to their counterparts on R^2\n", "link": "http://arxiv.org/abs/2402.15322v3", "date": "2024-11-13", "relevancy": 1.9936, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5243}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.505}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4814}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Transport%20on%20the%20Lie%20Group%20of%20Roto-translations&body=Title%3A%20Optimal%20Transport%20on%20the%20Lie%20Group%20of%20Roto-translations%0AAuthor%3A%20Daan%20Bon%20and%20Gautam%20Pai%20and%20Gijs%20Bellaard%20and%20Olga%20Mula%20and%20Remco%20Duits%0AAbstract%3A%20%20%20The%20roto-translation%20group%20SE2%20has%20been%20of%20active%20interest%20in%20image%20analysis%0Adue%20to%20methods%20that%20lift%20the%20image%20data%20to%20multi-orientation%20representations%0Adefined%20on%20this%20Lie%20group.%20This%20has%20led%20to%20impactful%20applications%20of%0Acrossing-preserving%20flows%20for%20image%20de-noising%2C%20geodesic%20tracking%2C%20and%0Aroto-translation%20equivariant%20deep%20learning.%20In%20this%20paper%2C%20we%20develop%20a%0Acomputational%20framework%20for%20optimal%20transportation%20over%20Lie%20groups%2C%20with%20a%0Aspecial%20focus%20on%20SE2.%20We%20make%20several%20theoretical%20contributions%20%28generalizable%0Ato%20matrix%20Lie%20groups%29%20such%20as%20the%20non-optimality%20of%20group%20actions%20as%20transport%0Amaps%2C%20invariance%20and%20equivariance%20of%20optimal%20transport%2C%20and%20the%20quality%20of%20the%0Aentropic-regularized%20optimal%20transport%20plan%20using%20geodesic%20distance%0Aapproximations.%20We%20develop%20a%20Sinkhorn%20like%20algorithm%20that%20can%20be%20efficiently%0Aimplemented%20using%20fast%20and%20accurate%20distance%20approximations%20of%20the%20Lie%20group%0Aand%20GPU-friendly%20group%20convolutions.%20We%20report%20valuable%20advancements%20in%20the%0Aexperiments%20on%201%29%20image%20barycentric%20interpolation%2C%202%29%20interpolation%20of%20planar%0Aorientation%20fields%2C%20and%203%29%20Wasserstein%20gradient%20flows%20on%20SE2.%20We%20observe%20that%0Aour%20framework%20of%20lifting%20images%20to%20SE2%20and%20optimal%20transport%20with%0Aleft-invariant%20anisotropic%20metrics%20leads%20to%20equivariant%20transport%20along%0Adominant%20contours%20and%20salient%20line%20structures%20in%20the%20image.%20This%20yields%20sharper%0Aand%20more%20meaningful%20interpolations%20compared%20to%20their%20counterparts%20on%20R%5E2%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15322v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Transport%2520on%2520the%2520Lie%2520Group%2520of%2520Roto-translations%26entry.906535625%3DDaan%2520Bon%2520and%2520Gautam%2520Pai%2520and%2520Gijs%2520Bellaard%2520and%2520Olga%2520Mula%2520and%2520Remco%2520Duits%26entry.1292438233%3D%2520%2520The%2520roto-translation%2520group%2520SE2%2520has%2520been%2520of%2520active%2520interest%2520in%2520image%2520analysis%250Adue%2520to%2520methods%2520that%2520lift%2520the%2520image%2520data%2520to%2520multi-orientation%2520representations%250Adefined%2520on%2520this%2520Lie%2520group.%2520This%2520has%2520led%2520to%2520impactful%2520applications%2520of%250Acrossing-preserving%2520flows%2520for%2520image%2520de-noising%252C%2520geodesic%2520tracking%252C%2520and%250Aroto-translation%2520equivariant%2520deep%2520learning.%2520In%2520this%2520paper%252C%2520we%2520develop%2520a%250Acomputational%2520framework%2520for%2520optimal%2520transportation%2520over%2520Lie%2520groups%252C%2520with%2520a%250Aspecial%2520focus%2520on%2520SE2.%2520We%2520make%2520several%2520theoretical%2520contributions%2520%2528generalizable%250Ato%2520matrix%2520Lie%2520groups%2529%2520such%2520as%2520the%2520non-optimality%2520of%2520group%2520actions%2520as%2520transport%250Amaps%252C%2520invariance%2520and%2520equivariance%2520of%2520optimal%2520transport%252C%2520and%2520the%2520quality%2520of%2520the%250Aentropic-regularized%2520optimal%2520transport%2520plan%2520using%2520geodesic%2520distance%250Aapproximations.%2520We%2520develop%2520a%2520Sinkhorn%2520like%2520algorithm%2520that%2520can%2520be%2520efficiently%250Aimplemented%2520using%2520fast%2520and%2520accurate%2520distance%2520approximations%2520of%2520the%2520Lie%2520group%250Aand%2520GPU-friendly%2520group%2520convolutions.%2520We%2520report%2520valuable%2520advancements%2520in%2520the%250Aexperiments%2520on%25201%2529%2520image%2520barycentric%2520interpolation%252C%25202%2529%2520interpolation%2520of%2520planar%250Aorientation%2520fields%252C%2520and%25203%2529%2520Wasserstein%2520gradient%2520flows%2520on%2520SE2.%2520We%2520observe%2520that%250Aour%2520framework%2520of%2520lifting%2520images%2520to%2520SE2%2520and%2520optimal%2520transport%2520with%250Aleft-invariant%2520anisotropic%2520metrics%2520leads%2520to%2520equivariant%2520transport%2520along%250Adominant%2520contours%2520and%2520salient%2520line%2520structures%2520in%2520the%2520image.%2520This%2520yields%2520sharper%250Aand%2520more%2520meaningful%2520interpolations%2520compared%2520to%2520their%2520counterparts%2520on%2520R%255E2%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.15322v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Transport%20on%20the%20Lie%20Group%20of%20Roto-translations&entry.906535625=Daan%20Bon%20and%20Gautam%20Pai%20and%20Gijs%20Bellaard%20and%20Olga%20Mula%20and%20Remco%20Duits&entry.1292438233=%20%20The%20roto-translation%20group%20SE2%20has%20been%20of%20active%20interest%20in%20image%20analysis%0Adue%20to%20methods%20that%20lift%20the%20image%20data%20to%20multi-orientation%20representations%0Adefined%20on%20this%20Lie%20group.%20This%20has%20led%20to%20impactful%20applications%20of%0Acrossing-preserving%20flows%20for%20image%20de-noising%2C%20geodesic%20tracking%2C%20and%0Aroto-translation%20equivariant%20deep%20learning.%20In%20this%20paper%2C%20we%20develop%20a%0Acomputational%20framework%20for%20optimal%20transportation%20over%20Lie%20groups%2C%20with%20a%0Aspecial%20focus%20on%20SE2.%20We%20make%20several%20theoretical%20contributions%20%28generalizable%0Ato%20matrix%20Lie%20groups%29%20such%20as%20the%20non-optimality%20of%20group%20actions%20as%20transport%0Amaps%2C%20invariance%20and%20equivariance%20of%20optimal%20transport%2C%20and%20the%20quality%20of%20the%0Aentropic-regularized%20optimal%20transport%20plan%20using%20geodesic%20distance%0Aapproximations.%20We%20develop%20a%20Sinkhorn%20like%20algorithm%20that%20can%20be%20efficiently%0Aimplemented%20using%20fast%20and%20accurate%20distance%20approximations%20of%20the%20Lie%20group%0Aand%20GPU-friendly%20group%20convolutions.%20We%20report%20valuable%20advancements%20in%20the%0Aexperiments%20on%201%29%20image%20barycentric%20interpolation%2C%202%29%20interpolation%20of%20planar%0Aorientation%20fields%2C%20and%203%29%20Wasserstein%20gradient%20flows%20on%20SE2.%20We%20observe%20that%0Aour%20framework%20of%20lifting%20images%20to%20SE2%20and%20optimal%20transport%20with%0Aleft-invariant%20anisotropic%20metrics%20leads%20to%20equivariant%20transport%20along%0Adominant%20contours%20and%20salient%20line%20structures%20in%20the%20image.%20This%20yields%20sharper%0Aand%20more%20meaningful%20interpolations%20compared%20to%20their%20counterparts%20on%20R%5E2%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15322v3&entry.124074799=Read"},
{"title": "LogLLM: Log-based Anomaly Detection Using Large Language Models", "author": "Wei Guan and Jian Cao and Shiyou Qian and Jianqi Gao", "abstract": "  Software systems often record important runtime information in logs to help\nwith troubleshooting. Log-based anomaly detection has become a key research\narea that aims to identify system issues through log data, ultimately enhancing\nthe reliability of software systems. Traditional deep learning methods often\nstruggle to capture the semantic information embedded in log data, which is\ntypically organized in natural language. In this paper, we propose LogLLM, a\nlog-based anomaly detection framework that leverages large language models\n(LLMs). LogLLM employs BERT for extracting semantic vectors from log messages,\nwhile utilizing Llama, a transformer decoder-based model, for classifying log\nsequences. Additionally, we introduce a projector to align the vector\nrepresentation spaces of BERT and Llama, ensuring a cohesive understanding of\nlog semantics. Unlike conventional methods that require log parsers to extract\ntemplates, LogLLM preprocesses log messages with regular expressions,\nstreamlining the entire process. Our framework is trained through a novel\nthree-stage procedure designed to enhance performance and adaptability.\nExperimental results across four public datasets demonstrate that LogLLM\noutperforms state-of-the-art methods. Even when handling unstable logs, it\neffectively captures the semantic meaning of log messages and detects anomalies\naccurately.\n", "link": "http://arxiv.org/abs/2411.08561v1", "date": "2024-11-13", "relevancy": 1.9881, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5166}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5062}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4799}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LogLLM%3A%20Log-based%20Anomaly%20Detection%20Using%20Large%20Language%20Models&body=Title%3A%20LogLLM%3A%20Log-based%20Anomaly%20Detection%20Using%20Large%20Language%20Models%0AAuthor%3A%20Wei%20Guan%20and%20Jian%20Cao%20and%20Shiyou%20Qian%20and%20Jianqi%20Gao%0AAbstract%3A%20%20%20Software%20systems%20often%20record%20important%20runtime%20information%20in%20logs%20to%20help%0Awith%20troubleshooting.%20Log-based%20anomaly%20detection%20has%20become%20a%20key%20research%0Aarea%20that%20aims%20to%20identify%20system%20issues%20through%20log%20data%2C%20ultimately%20enhancing%0Athe%20reliability%20of%20software%20systems.%20Traditional%20deep%20learning%20methods%20often%0Astruggle%20to%20capture%20the%20semantic%20information%20embedded%20in%20log%20data%2C%20which%20is%0Atypically%20organized%20in%20natural%20language.%20In%20this%20paper%2C%20we%20propose%20LogLLM%2C%20a%0Alog-based%20anomaly%20detection%20framework%20that%20leverages%20large%20language%20models%0A%28LLMs%29.%20LogLLM%20employs%20BERT%20for%20extracting%20semantic%20vectors%20from%20log%20messages%2C%0Awhile%20utilizing%20Llama%2C%20a%20transformer%20decoder-based%20model%2C%20for%20classifying%20log%0Asequences.%20Additionally%2C%20we%20introduce%20a%20projector%20to%20align%20the%20vector%0Arepresentation%20spaces%20of%20BERT%20and%20Llama%2C%20ensuring%20a%20cohesive%20understanding%20of%0Alog%20semantics.%20Unlike%20conventional%20methods%20that%20require%20log%20parsers%20to%20extract%0Atemplates%2C%20LogLLM%20preprocesses%20log%20messages%20with%20regular%20expressions%2C%0Astreamlining%20the%20entire%20process.%20Our%20framework%20is%20trained%20through%20a%20novel%0Athree-stage%20procedure%20designed%20to%20enhance%20performance%20and%20adaptability.%0AExperimental%20results%20across%20four%20public%20datasets%20demonstrate%20that%20LogLLM%0Aoutperforms%20state-of-the-art%20methods.%20Even%20when%20handling%20unstable%20logs%2C%20it%0Aeffectively%20captures%20the%20semantic%20meaning%20of%20log%20messages%20and%20detects%20anomalies%0Aaccurately.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08561v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLogLLM%253A%2520Log-based%2520Anomaly%2520Detection%2520Using%2520Large%2520Language%2520Models%26entry.906535625%3DWei%2520Guan%2520and%2520Jian%2520Cao%2520and%2520Shiyou%2520Qian%2520and%2520Jianqi%2520Gao%26entry.1292438233%3D%2520%2520Software%2520systems%2520often%2520record%2520important%2520runtime%2520information%2520in%2520logs%2520to%2520help%250Awith%2520troubleshooting.%2520Log-based%2520anomaly%2520detection%2520has%2520become%2520a%2520key%2520research%250Aarea%2520that%2520aims%2520to%2520identify%2520system%2520issues%2520through%2520log%2520data%252C%2520ultimately%2520enhancing%250Athe%2520reliability%2520of%2520software%2520systems.%2520Traditional%2520deep%2520learning%2520methods%2520often%250Astruggle%2520to%2520capture%2520the%2520semantic%2520information%2520embedded%2520in%2520log%2520data%252C%2520which%2520is%250Atypically%2520organized%2520in%2520natural%2520language.%2520In%2520this%2520paper%252C%2520we%2520propose%2520LogLLM%252C%2520a%250Alog-based%2520anomaly%2520detection%2520framework%2520that%2520leverages%2520large%2520language%2520models%250A%2528LLMs%2529.%2520LogLLM%2520employs%2520BERT%2520for%2520extracting%2520semantic%2520vectors%2520from%2520log%2520messages%252C%250Awhile%2520utilizing%2520Llama%252C%2520a%2520transformer%2520decoder-based%2520model%252C%2520for%2520classifying%2520log%250Asequences.%2520Additionally%252C%2520we%2520introduce%2520a%2520projector%2520to%2520align%2520the%2520vector%250Arepresentation%2520spaces%2520of%2520BERT%2520and%2520Llama%252C%2520ensuring%2520a%2520cohesive%2520understanding%2520of%250Alog%2520semantics.%2520Unlike%2520conventional%2520methods%2520that%2520require%2520log%2520parsers%2520to%2520extract%250Atemplates%252C%2520LogLLM%2520preprocesses%2520log%2520messages%2520with%2520regular%2520expressions%252C%250Astreamlining%2520the%2520entire%2520process.%2520Our%2520framework%2520is%2520trained%2520through%2520a%2520novel%250Athree-stage%2520procedure%2520designed%2520to%2520enhance%2520performance%2520and%2520adaptability.%250AExperimental%2520results%2520across%2520four%2520public%2520datasets%2520demonstrate%2520that%2520LogLLM%250Aoutperforms%2520state-of-the-art%2520methods.%2520Even%2520when%2520handling%2520unstable%2520logs%252C%2520it%250Aeffectively%2520captures%2520the%2520semantic%2520meaning%2520of%2520log%2520messages%2520and%2520detects%2520anomalies%250Aaccurately.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08561v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LogLLM%3A%20Log-based%20Anomaly%20Detection%20Using%20Large%20Language%20Models&entry.906535625=Wei%20Guan%20and%20Jian%20Cao%20and%20Shiyou%20Qian%20and%20Jianqi%20Gao&entry.1292438233=%20%20Software%20systems%20often%20record%20important%20runtime%20information%20in%20logs%20to%20help%0Awith%20troubleshooting.%20Log-based%20anomaly%20detection%20has%20become%20a%20key%20research%0Aarea%20that%20aims%20to%20identify%20system%20issues%20through%20log%20data%2C%20ultimately%20enhancing%0Athe%20reliability%20of%20software%20systems.%20Traditional%20deep%20learning%20methods%20often%0Astruggle%20to%20capture%20the%20semantic%20information%20embedded%20in%20log%20data%2C%20which%20is%0Atypically%20organized%20in%20natural%20language.%20In%20this%20paper%2C%20we%20propose%20LogLLM%2C%20a%0Alog-based%20anomaly%20detection%20framework%20that%20leverages%20large%20language%20models%0A%28LLMs%29.%20LogLLM%20employs%20BERT%20for%20extracting%20semantic%20vectors%20from%20log%20messages%2C%0Awhile%20utilizing%20Llama%2C%20a%20transformer%20decoder-based%20model%2C%20for%20classifying%20log%0Asequences.%20Additionally%2C%20we%20introduce%20a%20projector%20to%20align%20the%20vector%0Arepresentation%20spaces%20of%20BERT%20and%20Llama%2C%20ensuring%20a%20cohesive%20understanding%20of%0Alog%20semantics.%20Unlike%20conventional%20methods%20that%20require%20log%20parsers%20to%20extract%0Atemplates%2C%20LogLLM%20preprocesses%20log%20messages%20with%20regular%20expressions%2C%0Astreamlining%20the%20entire%20process.%20Our%20framework%20is%20trained%20through%20a%20novel%0Athree-stage%20procedure%20designed%20to%20enhance%20performance%20and%20adaptability.%0AExperimental%20results%20across%20four%20public%20datasets%20demonstrate%20that%20LogLLM%0Aoutperforms%20state-of-the-art%20methods.%20Even%20when%20handling%20unstable%20logs%2C%20it%0Aeffectively%20captures%20the%20semantic%20meaning%20of%20log%20messages%20and%20detects%20anomalies%0Aaccurately.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08561v1&entry.124074799=Read"},
{"title": "SynthesizRR: Generating Diverse Datasets with Retrieval Augmentation", "author": "Abhishek Divekar and Greg Durrett", "abstract": "  It is often desirable to distill the capabilities of large language models\n(LLMs) into smaller student models due to compute and memory constraints. One\nway to do this for classification tasks is via dataset synthesis, which can be\naccomplished by generating examples of each label from the LLM. Prior\napproaches to synthesis use few-shot prompting, which relies on the LLM's\nparametric knowledge to generate usable examples. However, this leads to issues\nof repetition, bias towards popular entities, and stylistic differences from\nhuman text. In this work, we propose Synthesize by Retrieval and Refinement\n(SynthesizRR), which uses retrieval augmentation to introduce variety into the\ndataset synthesis process: as retrieved passages vary, the LLM is seeded with\ndifferent content to generate its examples. We empirically study the synthesis\nof six datasets, covering topic classification, sentiment analysis, tone\ndetection, and humor, requiring complex synthesis strategies. We find that\nSynthesizRR greatly improves lexical and semantic diversity, similarity to\nhuman-written text, and distillation performance, when compared to 32-shot\nprompting and four prior approaches. We release our code to perform all steps\nat https://github.com/amazon-science/synthesizrr\n", "link": "http://arxiv.org/abs/2405.10040v3", "date": "2024-11-13", "relevancy": 1.9779, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5041}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4933}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4853}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SynthesizRR%3A%20Generating%20Diverse%20Datasets%20with%20Retrieval%20Augmentation&body=Title%3A%20SynthesizRR%3A%20Generating%20Diverse%20Datasets%20with%20Retrieval%20Augmentation%0AAuthor%3A%20Abhishek%20Divekar%20and%20Greg%20Durrett%0AAbstract%3A%20%20%20It%20is%20often%20desirable%20to%20distill%20the%20capabilities%20of%20large%20language%20models%0A%28LLMs%29%20into%20smaller%20student%20models%20due%20to%20compute%20and%20memory%20constraints.%20One%0Away%20to%20do%20this%20for%20classification%20tasks%20is%20via%20dataset%20synthesis%2C%20which%20can%20be%0Aaccomplished%20by%20generating%20examples%20of%20each%20label%20from%20the%20LLM.%20Prior%0Aapproaches%20to%20synthesis%20use%20few-shot%20prompting%2C%20which%20relies%20on%20the%20LLM%27s%0Aparametric%20knowledge%20to%20generate%20usable%20examples.%20However%2C%20this%20leads%20to%20issues%0Aof%20repetition%2C%20bias%20towards%20popular%20entities%2C%20and%20stylistic%20differences%20from%0Ahuman%20text.%20In%20this%20work%2C%20we%20propose%20Synthesize%20by%20Retrieval%20and%20Refinement%0A%28SynthesizRR%29%2C%20which%20uses%20retrieval%20augmentation%20to%20introduce%20variety%20into%20the%0Adataset%20synthesis%20process%3A%20as%20retrieved%20passages%20vary%2C%20the%20LLM%20is%20seeded%20with%0Adifferent%20content%20to%20generate%20its%20examples.%20We%20empirically%20study%20the%20synthesis%0Aof%20six%20datasets%2C%20covering%20topic%20classification%2C%20sentiment%20analysis%2C%20tone%0Adetection%2C%20and%20humor%2C%20requiring%20complex%20synthesis%20strategies.%20We%20find%20that%0ASynthesizRR%20greatly%20improves%20lexical%20and%20semantic%20diversity%2C%20similarity%20to%0Ahuman-written%20text%2C%20and%20distillation%20performance%2C%20when%20compared%20to%2032-shot%0Aprompting%20and%20four%20prior%20approaches.%20We%20release%20our%20code%20to%20perform%20all%20steps%0Aat%20https%3A//github.com/amazon-science/synthesizrr%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.10040v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSynthesizRR%253A%2520Generating%2520Diverse%2520Datasets%2520with%2520Retrieval%2520Augmentation%26entry.906535625%3DAbhishek%2520Divekar%2520and%2520Greg%2520Durrett%26entry.1292438233%3D%2520%2520It%2520is%2520often%2520desirable%2520to%2520distill%2520the%2520capabilities%2520of%2520large%2520language%2520models%250A%2528LLMs%2529%2520into%2520smaller%2520student%2520models%2520due%2520to%2520compute%2520and%2520memory%2520constraints.%2520One%250Away%2520to%2520do%2520this%2520for%2520classification%2520tasks%2520is%2520via%2520dataset%2520synthesis%252C%2520which%2520can%2520be%250Aaccomplished%2520by%2520generating%2520examples%2520of%2520each%2520label%2520from%2520the%2520LLM.%2520Prior%250Aapproaches%2520to%2520synthesis%2520use%2520few-shot%2520prompting%252C%2520which%2520relies%2520on%2520the%2520LLM%2527s%250Aparametric%2520knowledge%2520to%2520generate%2520usable%2520examples.%2520However%252C%2520this%2520leads%2520to%2520issues%250Aof%2520repetition%252C%2520bias%2520towards%2520popular%2520entities%252C%2520and%2520stylistic%2520differences%2520from%250Ahuman%2520text.%2520In%2520this%2520work%252C%2520we%2520propose%2520Synthesize%2520by%2520Retrieval%2520and%2520Refinement%250A%2528SynthesizRR%2529%252C%2520which%2520uses%2520retrieval%2520augmentation%2520to%2520introduce%2520variety%2520into%2520the%250Adataset%2520synthesis%2520process%253A%2520as%2520retrieved%2520passages%2520vary%252C%2520the%2520LLM%2520is%2520seeded%2520with%250Adifferent%2520content%2520to%2520generate%2520its%2520examples.%2520We%2520empirically%2520study%2520the%2520synthesis%250Aof%2520six%2520datasets%252C%2520covering%2520topic%2520classification%252C%2520sentiment%2520analysis%252C%2520tone%250Adetection%252C%2520and%2520humor%252C%2520requiring%2520complex%2520synthesis%2520strategies.%2520We%2520find%2520that%250ASynthesizRR%2520greatly%2520improves%2520lexical%2520and%2520semantic%2520diversity%252C%2520similarity%2520to%250Ahuman-written%2520text%252C%2520and%2520distillation%2520performance%252C%2520when%2520compared%2520to%252032-shot%250Aprompting%2520and%2520four%2520prior%2520approaches.%2520We%2520release%2520our%2520code%2520to%2520perform%2520all%2520steps%250Aat%2520https%253A//github.com/amazon-science/synthesizrr%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.10040v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SynthesizRR%3A%20Generating%20Diverse%20Datasets%20with%20Retrieval%20Augmentation&entry.906535625=Abhishek%20Divekar%20and%20Greg%20Durrett&entry.1292438233=%20%20It%20is%20often%20desirable%20to%20distill%20the%20capabilities%20of%20large%20language%20models%0A%28LLMs%29%20into%20smaller%20student%20models%20due%20to%20compute%20and%20memory%20constraints.%20One%0Away%20to%20do%20this%20for%20classification%20tasks%20is%20via%20dataset%20synthesis%2C%20which%20can%20be%0Aaccomplished%20by%20generating%20examples%20of%20each%20label%20from%20the%20LLM.%20Prior%0Aapproaches%20to%20synthesis%20use%20few-shot%20prompting%2C%20which%20relies%20on%20the%20LLM%27s%0Aparametric%20knowledge%20to%20generate%20usable%20examples.%20However%2C%20this%20leads%20to%20issues%0Aof%20repetition%2C%20bias%20towards%20popular%20entities%2C%20and%20stylistic%20differences%20from%0Ahuman%20text.%20In%20this%20work%2C%20we%20propose%20Synthesize%20by%20Retrieval%20and%20Refinement%0A%28SynthesizRR%29%2C%20which%20uses%20retrieval%20augmentation%20to%20introduce%20variety%20into%20the%0Adataset%20synthesis%20process%3A%20as%20retrieved%20passages%20vary%2C%20the%20LLM%20is%20seeded%20with%0Adifferent%20content%20to%20generate%20its%20examples.%20We%20empirically%20study%20the%20synthesis%0Aof%20six%20datasets%2C%20covering%20topic%20classification%2C%20sentiment%20analysis%2C%20tone%0Adetection%2C%20and%20humor%2C%20requiring%20complex%20synthesis%20strategies.%20We%20find%20that%0ASynthesizRR%20greatly%20improves%20lexical%20and%20semantic%20diversity%2C%20similarity%20to%0Ahuman-written%20text%2C%20and%20distillation%20performance%2C%20when%20compared%20to%2032-shot%0Aprompting%20and%20four%20prior%20approaches.%20We%20release%20our%20code%20to%20perform%20all%20steps%0Aat%20https%3A//github.com/amazon-science/synthesizrr%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.10040v3&entry.124074799=Read"},
{"title": "Predictive Inference in Multi-environment Scenarios", "author": "John C. Duchi and Suyash Gupta and Kuanhao Jiang and Pragya Sur", "abstract": "  We address the challenge of constructing valid confidence intervals and sets\nin problems of prediction across multiple environments. We investigate two\ntypes of coverage suitable for these problems, extending the jackknife and\nsplit-conformal methods to show how to obtain distribution-free coverage in\nsuch non-traditional, potentially hierarchical data-generating scenarios. We\ndemonstrate a novel resizing method to adapt to problem difficulty, which\napplies both to existing approaches for predictive inference and the methods we\ndevelop; this reduces prediction set sizes using limited information from the\ntest environment, a key to the methods' practical performance, which we\nevaluate through neurochemical sensing and species classification datasets. Our\ncontributions also include extensions for settings with non-real-valued\nresponses, a theory of consistency for predictive inference in these general\nproblems, and insights on the limits of conditional coverage.\n", "link": "http://arxiv.org/abs/2403.16336v2", "date": "2024-11-13", "relevancy": 1.9671, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5728}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4792}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4719}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predictive%20Inference%20in%20Multi-environment%20Scenarios&body=Title%3A%20Predictive%20Inference%20in%20Multi-environment%20Scenarios%0AAuthor%3A%20John%20C.%20Duchi%20and%20Suyash%20Gupta%20and%20Kuanhao%20Jiang%20and%20Pragya%20Sur%0AAbstract%3A%20%20%20We%20address%20the%20challenge%20of%20constructing%20valid%20confidence%20intervals%20and%20sets%0Ain%20problems%20of%20prediction%20across%20multiple%20environments.%20We%20investigate%20two%0Atypes%20of%20coverage%20suitable%20for%20these%20problems%2C%20extending%20the%20jackknife%20and%0Asplit-conformal%20methods%20to%20show%20how%20to%20obtain%20distribution-free%20coverage%20in%0Asuch%20non-traditional%2C%20potentially%20hierarchical%20data-generating%20scenarios.%20We%0Ademonstrate%20a%20novel%20resizing%20method%20to%20adapt%20to%20problem%20difficulty%2C%20which%0Aapplies%20both%20to%20existing%20approaches%20for%20predictive%20inference%20and%20the%20methods%20we%0Adevelop%3B%20this%20reduces%20prediction%20set%20sizes%20using%20limited%20information%20from%20the%0Atest%20environment%2C%20a%20key%20to%20the%20methods%27%20practical%20performance%2C%20which%20we%0Aevaluate%20through%20neurochemical%20sensing%20and%20species%20classification%20datasets.%20Our%0Acontributions%20also%20include%20extensions%20for%20settings%20with%20non-real-valued%0Aresponses%2C%20a%20theory%20of%20consistency%20for%20predictive%20inference%20in%20these%20general%0Aproblems%2C%20and%20insights%20on%20the%20limits%20of%20conditional%20coverage.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16336v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredictive%2520Inference%2520in%2520Multi-environment%2520Scenarios%26entry.906535625%3DJohn%2520C.%2520Duchi%2520and%2520Suyash%2520Gupta%2520and%2520Kuanhao%2520Jiang%2520and%2520Pragya%2520Sur%26entry.1292438233%3D%2520%2520We%2520address%2520the%2520challenge%2520of%2520constructing%2520valid%2520confidence%2520intervals%2520and%2520sets%250Ain%2520problems%2520of%2520prediction%2520across%2520multiple%2520environments.%2520We%2520investigate%2520two%250Atypes%2520of%2520coverage%2520suitable%2520for%2520these%2520problems%252C%2520extending%2520the%2520jackknife%2520and%250Asplit-conformal%2520methods%2520to%2520show%2520how%2520to%2520obtain%2520distribution-free%2520coverage%2520in%250Asuch%2520non-traditional%252C%2520potentially%2520hierarchical%2520data-generating%2520scenarios.%2520We%250Ademonstrate%2520a%2520novel%2520resizing%2520method%2520to%2520adapt%2520to%2520problem%2520difficulty%252C%2520which%250Aapplies%2520both%2520to%2520existing%2520approaches%2520for%2520predictive%2520inference%2520and%2520the%2520methods%2520we%250Adevelop%253B%2520this%2520reduces%2520prediction%2520set%2520sizes%2520using%2520limited%2520information%2520from%2520the%250Atest%2520environment%252C%2520a%2520key%2520to%2520the%2520methods%2527%2520practical%2520performance%252C%2520which%2520we%250Aevaluate%2520through%2520neurochemical%2520sensing%2520and%2520species%2520classification%2520datasets.%2520Our%250Acontributions%2520also%2520include%2520extensions%2520for%2520settings%2520with%2520non-real-valued%250Aresponses%252C%2520a%2520theory%2520of%2520consistency%2520for%2520predictive%2520inference%2520in%2520these%2520general%250Aproblems%252C%2520and%2520insights%2520on%2520the%2520limits%2520of%2520conditional%2520coverage.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16336v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predictive%20Inference%20in%20Multi-environment%20Scenarios&entry.906535625=John%20C.%20Duchi%20and%20Suyash%20Gupta%20and%20Kuanhao%20Jiang%20and%20Pragya%20Sur&entry.1292438233=%20%20We%20address%20the%20challenge%20of%20constructing%20valid%20confidence%20intervals%20and%20sets%0Ain%20problems%20of%20prediction%20across%20multiple%20environments.%20We%20investigate%20two%0Atypes%20of%20coverage%20suitable%20for%20these%20problems%2C%20extending%20the%20jackknife%20and%0Asplit-conformal%20methods%20to%20show%20how%20to%20obtain%20distribution-free%20coverage%20in%0Asuch%20non-traditional%2C%20potentially%20hierarchical%20data-generating%20scenarios.%20We%0Ademonstrate%20a%20novel%20resizing%20method%20to%20adapt%20to%20problem%20difficulty%2C%20which%0Aapplies%20both%20to%20existing%20approaches%20for%20predictive%20inference%20and%20the%20methods%20we%0Adevelop%3B%20this%20reduces%20prediction%20set%20sizes%20using%20limited%20information%20from%20the%0Atest%20environment%2C%20a%20key%20to%20the%20methods%27%20practical%20performance%2C%20which%20we%0Aevaluate%20through%20neurochemical%20sensing%20and%20species%20classification%20datasets.%20Our%0Acontributions%20also%20include%20extensions%20for%20settings%20with%20non-real-valued%0Aresponses%2C%20a%20theory%20of%20consistency%20for%20predictive%20inference%20in%20these%20general%0Aproblems%2C%20and%20insights%20on%20the%20limits%20of%20conditional%20coverage.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16336v2&entry.124074799=Read"},
{"title": "Investigating the Effectiveness of Explainability Methods in Parkinson's\n  Detection from Speech", "author": "Eleonora Mancini and Francesco Paissan and Paolo Torroni and Mirco Ravanelli and Cem Subakan", "abstract": "  Speech impairments in Parkinson's disease (PD) provide significant early\nindicators for diagnosis. While models for speech-based PD detection have shown\nstrong performance, their interpretability remains underexplored. This study\nsystematically evaluates several explainability methods to identify PD-specific\nspeech features, aiming to support the development of accurate, interpretable\nmodels for clinical decision-making in PD diagnosis and monitoring. Our\nmethodology involves (i) obtaining attributions and saliency maps using\nmainstream interpretability techniques, (ii) quantitatively evaluating the\nfaithfulness of these maps and their combinations obtained via union and\nintersection through a range of established metrics, and (iii) assessing the\ninformation conveyed by the saliency maps for PD detection from an auxiliary\nclassifier. Our results reveal that, while explanations are aligned with the\nclassifier, they often fail to provide valuable information for domain experts.\n", "link": "http://arxiv.org/abs/2411.08013v2", "date": "2024-11-13", "relevancy": 1.9611, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4961}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4891}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Investigating%20the%20Effectiveness%20of%20Explainability%20Methods%20in%20Parkinson%27s%0A%20%20Detection%20from%20Speech&body=Title%3A%20Investigating%20the%20Effectiveness%20of%20Explainability%20Methods%20in%20Parkinson%27s%0A%20%20Detection%20from%20Speech%0AAuthor%3A%20Eleonora%20Mancini%20and%20Francesco%20Paissan%20and%20Paolo%20Torroni%20and%20Mirco%20Ravanelli%20and%20Cem%20Subakan%0AAbstract%3A%20%20%20Speech%20impairments%20in%20Parkinson%27s%20disease%20%28PD%29%20provide%20significant%20early%0Aindicators%20for%20diagnosis.%20While%20models%20for%20speech-based%20PD%20detection%20have%20shown%0Astrong%20performance%2C%20their%20interpretability%20remains%20underexplored.%20This%20study%0Asystematically%20evaluates%20several%20explainability%20methods%20to%20identify%20PD-specific%0Aspeech%20features%2C%20aiming%20to%20support%20the%20development%20of%20accurate%2C%20interpretable%0Amodels%20for%20clinical%20decision-making%20in%20PD%20diagnosis%20and%20monitoring.%20Our%0Amethodology%20involves%20%28i%29%20obtaining%20attributions%20and%20saliency%20maps%20using%0Amainstream%20interpretability%20techniques%2C%20%28ii%29%20quantitatively%20evaluating%20the%0Afaithfulness%20of%20these%20maps%20and%20their%20combinations%20obtained%20via%20union%20and%0Aintersection%20through%20a%20range%20of%20established%20metrics%2C%20and%20%28iii%29%20assessing%20the%0Ainformation%20conveyed%20by%20the%20saliency%20maps%20for%20PD%20detection%20from%20an%20auxiliary%0Aclassifier.%20Our%20results%20reveal%20that%2C%20while%20explanations%20are%20aligned%20with%20the%0Aclassifier%2C%20they%20often%20fail%20to%20provide%20valuable%20information%20for%20domain%20experts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08013v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInvestigating%2520the%2520Effectiveness%2520of%2520Explainability%2520Methods%2520in%2520Parkinson%2527s%250A%2520%2520Detection%2520from%2520Speech%26entry.906535625%3DEleonora%2520Mancini%2520and%2520Francesco%2520Paissan%2520and%2520Paolo%2520Torroni%2520and%2520Mirco%2520Ravanelli%2520and%2520Cem%2520Subakan%26entry.1292438233%3D%2520%2520Speech%2520impairments%2520in%2520Parkinson%2527s%2520disease%2520%2528PD%2529%2520provide%2520significant%2520early%250Aindicators%2520for%2520diagnosis.%2520While%2520models%2520for%2520speech-based%2520PD%2520detection%2520have%2520shown%250Astrong%2520performance%252C%2520their%2520interpretability%2520remains%2520underexplored.%2520This%2520study%250Asystematically%2520evaluates%2520several%2520explainability%2520methods%2520to%2520identify%2520PD-specific%250Aspeech%2520features%252C%2520aiming%2520to%2520support%2520the%2520development%2520of%2520accurate%252C%2520interpretable%250Amodels%2520for%2520clinical%2520decision-making%2520in%2520PD%2520diagnosis%2520and%2520monitoring.%2520Our%250Amethodology%2520involves%2520%2528i%2529%2520obtaining%2520attributions%2520and%2520saliency%2520maps%2520using%250Amainstream%2520interpretability%2520techniques%252C%2520%2528ii%2529%2520quantitatively%2520evaluating%2520the%250Afaithfulness%2520of%2520these%2520maps%2520and%2520their%2520combinations%2520obtained%2520via%2520union%2520and%250Aintersection%2520through%2520a%2520range%2520of%2520established%2520metrics%252C%2520and%2520%2528iii%2529%2520assessing%2520the%250Ainformation%2520conveyed%2520by%2520the%2520saliency%2520maps%2520for%2520PD%2520detection%2520from%2520an%2520auxiliary%250Aclassifier.%2520Our%2520results%2520reveal%2520that%252C%2520while%2520explanations%2520are%2520aligned%2520with%2520the%250Aclassifier%252C%2520they%2520often%2520fail%2520to%2520provide%2520valuable%2520information%2520for%2520domain%2520experts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08013v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20the%20Effectiveness%20of%20Explainability%20Methods%20in%20Parkinson%27s%0A%20%20Detection%20from%20Speech&entry.906535625=Eleonora%20Mancini%20and%20Francesco%20Paissan%20and%20Paolo%20Torroni%20and%20Mirco%20Ravanelli%20and%20Cem%20Subakan&entry.1292438233=%20%20Speech%20impairments%20in%20Parkinson%27s%20disease%20%28PD%29%20provide%20significant%20early%0Aindicators%20for%20diagnosis.%20While%20models%20for%20speech-based%20PD%20detection%20have%20shown%0Astrong%20performance%2C%20their%20interpretability%20remains%20underexplored.%20This%20study%0Asystematically%20evaluates%20several%20explainability%20methods%20to%20identify%20PD-specific%0Aspeech%20features%2C%20aiming%20to%20support%20the%20development%20of%20accurate%2C%20interpretable%0Amodels%20for%20clinical%20decision-making%20in%20PD%20diagnosis%20and%20monitoring.%20Our%0Amethodology%20involves%20%28i%29%20obtaining%20attributions%20and%20saliency%20maps%20using%0Amainstream%20interpretability%20techniques%2C%20%28ii%29%20quantitatively%20evaluating%20the%0Afaithfulness%20of%20these%20maps%20and%20their%20combinations%20obtained%20via%20union%20and%0Aintersection%20through%20a%20range%20of%20established%20metrics%2C%20and%20%28iii%29%20assessing%20the%0Ainformation%20conveyed%20by%20the%20saliency%20maps%20for%20PD%20detection%20from%20an%20auxiliary%0Aclassifier.%20Our%20results%20reveal%20that%2C%20while%20explanations%20are%20aligned%20with%20the%0Aclassifier%2C%20they%20often%20fail%20to%20provide%20valuable%20information%20for%20domain%20experts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08013v2&entry.124074799=Read"},
{"title": "Process-aware Human Activity Recognition", "author": "Jiawei Zheng and Petros Papapanagiotou and Jacques D. Fleuriot and Jane Hillston", "abstract": "  Humans naturally follow distinct patterns when conducting their daily\nactivities, which are driven by established practices and processes, such as\nproduction workflows, social norms and daily routines. Human activity\nrecognition (HAR) algorithms usually use neural networks or machine learning\ntechniques to analyse inherent relationships within the data. However, these\napproaches often overlook the contextual information in which the data are\ngenerated, potentially limiting their effectiveness. We propose a novel\napproach that incorporates process information from context to enhance the HAR\nperformance. Specifically, we align probabilistic events generated by machine\nlearning models with process models derived from contextual information. This\nalignment adaptively weighs these two sources of information to optimise HAR\naccuracy. Our experiments demonstrate that our approach achieves better\naccuracy and Macro F1-score compared to baseline models.\n", "link": "http://arxiv.org/abs/2411.08814v1", "date": "2024-11-13", "relevancy": 1.9597, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5222}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4876}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Process-aware%20Human%20Activity%20Recognition&body=Title%3A%20Process-aware%20Human%20Activity%20Recognition%0AAuthor%3A%20Jiawei%20Zheng%20and%20Petros%20Papapanagiotou%20and%20Jacques%20D.%20Fleuriot%20and%20Jane%20Hillston%0AAbstract%3A%20%20%20Humans%20naturally%20follow%20distinct%20patterns%20when%20conducting%20their%20daily%0Aactivities%2C%20which%20are%20driven%20by%20established%20practices%20and%20processes%2C%20such%20as%0Aproduction%20workflows%2C%20social%20norms%20and%20daily%20routines.%20Human%20activity%0Arecognition%20%28HAR%29%20algorithms%20usually%20use%20neural%20networks%20or%20machine%20learning%0Atechniques%20to%20analyse%20inherent%20relationships%20within%20the%20data.%20However%2C%20these%0Aapproaches%20often%20overlook%20the%20contextual%20information%20in%20which%20the%20data%20are%0Agenerated%2C%20potentially%20limiting%20their%20effectiveness.%20We%20propose%20a%20novel%0Aapproach%20that%20incorporates%20process%20information%20from%20context%20to%20enhance%20the%20HAR%0Aperformance.%20Specifically%2C%20we%20align%20probabilistic%20events%20generated%20by%20machine%0Alearning%20models%20with%20process%20models%20derived%20from%20contextual%20information.%20This%0Aalignment%20adaptively%20weighs%20these%20two%20sources%20of%20information%20to%20optimise%20HAR%0Aaccuracy.%20Our%20experiments%20demonstrate%20that%20our%20approach%20achieves%20better%0Aaccuracy%20and%20Macro%20F1-score%20compared%20to%20baseline%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProcess-aware%2520Human%2520Activity%2520Recognition%26entry.906535625%3DJiawei%2520Zheng%2520and%2520Petros%2520Papapanagiotou%2520and%2520Jacques%2520D.%2520Fleuriot%2520and%2520Jane%2520Hillston%26entry.1292438233%3D%2520%2520Humans%2520naturally%2520follow%2520distinct%2520patterns%2520when%2520conducting%2520their%2520daily%250Aactivities%252C%2520which%2520are%2520driven%2520by%2520established%2520practices%2520and%2520processes%252C%2520such%2520as%250Aproduction%2520workflows%252C%2520social%2520norms%2520and%2520daily%2520routines.%2520Human%2520activity%250Arecognition%2520%2528HAR%2529%2520algorithms%2520usually%2520use%2520neural%2520networks%2520or%2520machine%2520learning%250Atechniques%2520to%2520analyse%2520inherent%2520relationships%2520within%2520the%2520data.%2520However%252C%2520these%250Aapproaches%2520often%2520overlook%2520the%2520contextual%2520information%2520in%2520which%2520the%2520data%2520are%250Agenerated%252C%2520potentially%2520limiting%2520their%2520effectiveness.%2520We%2520propose%2520a%2520novel%250Aapproach%2520that%2520incorporates%2520process%2520information%2520from%2520context%2520to%2520enhance%2520the%2520HAR%250Aperformance.%2520Specifically%252C%2520we%2520align%2520probabilistic%2520events%2520generated%2520by%2520machine%250Alearning%2520models%2520with%2520process%2520models%2520derived%2520from%2520contextual%2520information.%2520This%250Aalignment%2520adaptively%2520weighs%2520these%2520two%2520sources%2520of%2520information%2520to%2520optimise%2520HAR%250Aaccuracy.%2520Our%2520experiments%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520better%250Aaccuracy%2520and%2520Macro%2520F1-score%2520compared%2520to%2520baseline%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Process-aware%20Human%20Activity%20Recognition&entry.906535625=Jiawei%20Zheng%20and%20Petros%20Papapanagiotou%20and%20Jacques%20D.%20Fleuriot%20and%20Jane%20Hillston&entry.1292438233=%20%20Humans%20naturally%20follow%20distinct%20patterns%20when%20conducting%20their%20daily%0Aactivities%2C%20which%20are%20driven%20by%20established%20practices%20and%20processes%2C%20such%20as%0Aproduction%20workflows%2C%20social%20norms%20and%20daily%20routines.%20Human%20activity%0Arecognition%20%28HAR%29%20algorithms%20usually%20use%20neural%20networks%20or%20machine%20learning%0Atechniques%20to%20analyse%20inherent%20relationships%20within%20the%20data.%20However%2C%20these%0Aapproaches%20often%20overlook%20the%20contextual%20information%20in%20which%20the%20data%20are%0Agenerated%2C%20potentially%20limiting%20their%20effectiveness.%20We%20propose%20a%20novel%0Aapproach%20that%20incorporates%20process%20information%20from%20context%20to%20enhance%20the%20HAR%0Aperformance.%20Specifically%2C%20we%20align%20probabilistic%20events%20generated%20by%20machine%0Alearning%20models%20with%20process%20models%20derived%20from%20contextual%20information.%20This%0Aalignment%20adaptively%20weighs%20these%20two%20sources%20of%20information%20to%20optimise%20HAR%0Aaccuracy.%20Our%20experiments%20demonstrate%20that%20our%20approach%20achieves%20better%0Aaccuracy%20and%20Macro%20F1-score%20compared%20to%20baseline%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08814v1&entry.124074799=Read"},
{"title": "APDDv2: Aesthetics of Paintings and Drawings Dataset with Artist Labeled\n  Scores and Comments", "author": "Xin Jin and Qianqian Qiao and Yi Lu and Huaye Wang and Heng Huang and Shan Gao and Jianfei Liu and Rui Li", "abstract": "  Datasets play a pivotal role in training visual models, facilitating the\ndevelopment of abstract understandings of visual features through diverse image\nsamples and multidimensional attributes. However, in the realm of aesthetic\nevaluation of artistic images, datasets remain relatively scarce. Existing\npainting datasets are often characterized by limited scoring dimensions and\ninsufficient annotations, thereby constraining the advancement and application\nof automatic aesthetic evaluation methods in the domain of painting. To bridge\nthis gap, we introduce the Aesthetics Paintings and Drawings Dataset (APDD),\nthe first comprehensive collection of paintings encompassing 24 distinct\nartistic categories and 10 aesthetic attributes. Building upon the initial\nrelease of APDDv1, our ongoing research has identified opportunities for\nenhancement in data scale and annotation precision. Consequently, APDDv2 boasts\nan expanded image corpus and improved annotation quality, featuring detailed\nlanguage comments to better cater to the needs of both researchers and\npractitioners seeking high-quality painting datasets. Furthermore, we present\nan updated version of the Art Assessment Network for Specific Painting Styles,\ndenoted as ArtCLIP. Experimental validation demonstrates the superior\nperformance of this revised model in the realm of aesthetic evaluation,\nsurpassing its predecessor in accuracy and efficacy. The dataset and model are\navailable at https://github.com/BestiVictory/APDDv2.git.\n", "link": "http://arxiv.org/abs/2411.08545v1", "date": "2024-11-13", "relevancy": 1.955, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.499}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4918}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20APDDv2%3A%20Aesthetics%20of%20Paintings%20and%20Drawings%20Dataset%20with%20Artist%20Labeled%0A%20%20Scores%20and%20Comments&body=Title%3A%20APDDv2%3A%20Aesthetics%20of%20Paintings%20and%20Drawings%20Dataset%20with%20Artist%20Labeled%0A%20%20Scores%20and%20Comments%0AAuthor%3A%20Xin%20Jin%20and%20Qianqian%20Qiao%20and%20Yi%20Lu%20and%20Huaye%20Wang%20and%20Heng%20Huang%20and%20Shan%20Gao%20and%20Jianfei%20Liu%20and%20Rui%20Li%0AAbstract%3A%20%20%20Datasets%20play%20a%20pivotal%20role%20in%20training%20visual%20models%2C%20facilitating%20the%0Adevelopment%20of%20abstract%20understandings%20of%20visual%20features%20through%20diverse%20image%0Asamples%20and%20multidimensional%20attributes.%20However%2C%20in%20the%20realm%20of%20aesthetic%0Aevaluation%20of%20artistic%20images%2C%20datasets%20remain%20relatively%20scarce.%20Existing%0Apainting%20datasets%20are%20often%20characterized%20by%20limited%20scoring%20dimensions%20and%0Ainsufficient%20annotations%2C%20thereby%20constraining%20the%20advancement%20and%20application%0Aof%20automatic%20aesthetic%20evaluation%20methods%20in%20the%20domain%20of%20painting.%20To%20bridge%0Athis%20gap%2C%20we%20introduce%20the%20Aesthetics%20Paintings%20and%20Drawings%20Dataset%20%28APDD%29%2C%0Athe%20first%20comprehensive%20collection%20of%20paintings%20encompassing%2024%20distinct%0Aartistic%20categories%20and%2010%20aesthetic%20attributes.%20Building%20upon%20the%20initial%0Arelease%20of%20APDDv1%2C%20our%20ongoing%20research%20has%20identified%20opportunities%20for%0Aenhancement%20in%20data%20scale%20and%20annotation%20precision.%20Consequently%2C%20APDDv2%20boasts%0Aan%20expanded%20image%20corpus%20and%20improved%20annotation%20quality%2C%20featuring%20detailed%0Alanguage%20comments%20to%20better%20cater%20to%20the%20needs%20of%20both%20researchers%20and%0Apractitioners%20seeking%20high-quality%20painting%20datasets.%20Furthermore%2C%20we%20present%0Aan%20updated%20version%20of%20the%20Art%20Assessment%20Network%20for%20Specific%20Painting%20Styles%2C%0Adenoted%20as%20ArtCLIP.%20Experimental%20validation%20demonstrates%20the%20superior%0Aperformance%20of%20this%20revised%20model%20in%20the%20realm%20of%20aesthetic%20evaluation%2C%0Asurpassing%20its%20predecessor%20in%20accuracy%20and%20efficacy.%20The%20dataset%20and%20model%20are%0Aavailable%20at%20https%3A//github.com/BestiVictory/APDDv2.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08545v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAPDDv2%253A%2520Aesthetics%2520of%2520Paintings%2520and%2520Drawings%2520Dataset%2520with%2520Artist%2520Labeled%250A%2520%2520Scores%2520and%2520Comments%26entry.906535625%3DXin%2520Jin%2520and%2520Qianqian%2520Qiao%2520and%2520Yi%2520Lu%2520and%2520Huaye%2520Wang%2520and%2520Heng%2520Huang%2520and%2520Shan%2520Gao%2520and%2520Jianfei%2520Liu%2520and%2520Rui%2520Li%26entry.1292438233%3D%2520%2520Datasets%2520play%2520a%2520pivotal%2520role%2520in%2520training%2520visual%2520models%252C%2520facilitating%2520the%250Adevelopment%2520of%2520abstract%2520understandings%2520of%2520visual%2520features%2520through%2520diverse%2520image%250Asamples%2520and%2520multidimensional%2520attributes.%2520However%252C%2520in%2520the%2520realm%2520of%2520aesthetic%250Aevaluation%2520of%2520artistic%2520images%252C%2520datasets%2520remain%2520relatively%2520scarce.%2520Existing%250Apainting%2520datasets%2520are%2520often%2520characterized%2520by%2520limited%2520scoring%2520dimensions%2520and%250Ainsufficient%2520annotations%252C%2520thereby%2520constraining%2520the%2520advancement%2520and%2520application%250Aof%2520automatic%2520aesthetic%2520evaluation%2520methods%2520in%2520the%2520domain%2520of%2520painting.%2520To%2520bridge%250Athis%2520gap%252C%2520we%2520introduce%2520the%2520Aesthetics%2520Paintings%2520and%2520Drawings%2520Dataset%2520%2528APDD%2529%252C%250Athe%2520first%2520comprehensive%2520collection%2520of%2520paintings%2520encompassing%252024%2520distinct%250Aartistic%2520categories%2520and%252010%2520aesthetic%2520attributes.%2520Building%2520upon%2520the%2520initial%250Arelease%2520of%2520APDDv1%252C%2520our%2520ongoing%2520research%2520has%2520identified%2520opportunities%2520for%250Aenhancement%2520in%2520data%2520scale%2520and%2520annotation%2520precision.%2520Consequently%252C%2520APDDv2%2520boasts%250Aan%2520expanded%2520image%2520corpus%2520and%2520improved%2520annotation%2520quality%252C%2520featuring%2520detailed%250Alanguage%2520comments%2520to%2520better%2520cater%2520to%2520the%2520needs%2520of%2520both%2520researchers%2520and%250Apractitioners%2520seeking%2520high-quality%2520painting%2520datasets.%2520Furthermore%252C%2520we%2520present%250Aan%2520updated%2520version%2520of%2520the%2520Art%2520Assessment%2520Network%2520for%2520Specific%2520Painting%2520Styles%252C%250Adenoted%2520as%2520ArtCLIP.%2520Experimental%2520validation%2520demonstrates%2520the%2520superior%250Aperformance%2520of%2520this%2520revised%2520model%2520in%2520the%2520realm%2520of%2520aesthetic%2520evaluation%252C%250Asurpassing%2520its%2520predecessor%2520in%2520accuracy%2520and%2520efficacy.%2520The%2520dataset%2520and%2520model%2520are%250Aavailable%2520at%2520https%253A//github.com/BestiVictory/APDDv2.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08545v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=APDDv2%3A%20Aesthetics%20of%20Paintings%20and%20Drawings%20Dataset%20with%20Artist%20Labeled%0A%20%20Scores%20and%20Comments&entry.906535625=Xin%20Jin%20and%20Qianqian%20Qiao%20and%20Yi%20Lu%20and%20Huaye%20Wang%20and%20Heng%20Huang%20and%20Shan%20Gao%20and%20Jianfei%20Liu%20and%20Rui%20Li&entry.1292438233=%20%20Datasets%20play%20a%20pivotal%20role%20in%20training%20visual%20models%2C%20facilitating%20the%0Adevelopment%20of%20abstract%20understandings%20of%20visual%20features%20through%20diverse%20image%0Asamples%20and%20multidimensional%20attributes.%20However%2C%20in%20the%20realm%20of%20aesthetic%0Aevaluation%20of%20artistic%20images%2C%20datasets%20remain%20relatively%20scarce.%20Existing%0Apainting%20datasets%20are%20often%20characterized%20by%20limited%20scoring%20dimensions%20and%0Ainsufficient%20annotations%2C%20thereby%20constraining%20the%20advancement%20and%20application%0Aof%20automatic%20aesthetic%20evaluation%20methods%20in%20the%20domain%20of%20painting.%20To%20bridge%0Athis%20gap%2C%20we%20introduce%20the%20Aesthetics%20Paintings%20and%20Drawings%20Dataset%20%28APDD%29%2C%0Athe%20first%20comprehensive%20collection%20of%20paintings%20encompassing%2024%20distinct%0Aartistic%20categories%20and%2010%20aesthetic%20attributes.%20Building%20upon%20the%20initial%0Arelease%20of%20APDDv1%2C%20our%20ongoing%20research%20has%20identified%20opportunities%20for%0Aenhancement%20in%20data%20scale%20and%20annotation%20precision.%20Consequently%2C%20APDDv2%20boasts%0Aan%20expanded%20image%20corpus%20and%20improved%20annotation%20quality%2C%20featuring%20detailed%0Alanguage%20comments%20to%20better%20cater%20to%20the%20needs%20of%20both%20researchers%20and%0Apractitioners%20seeking%20high-quality%20painting%20datasets.%20Furthermore%2C%20we%20present%0Aan%20updated%20version%20of%20the%20Art%20Assessment%20Network%20for%20Specific%20Painting%20Styles%2C%0Adenoted%20as%20ArtCLIP.%20Experimental%20validation%20demonstrates%20the%20superior%0Aperformance%20of%20this%20revised%20model%20in%20the%20realm%20of%20aesthetic%20evaluation%2C%0Asurpassing%20its%20predecessor%20in%20accuracy%20and%20efficacy.%20The%20dataset%20and%20model%20are%0Aavailable%20at%20https%3A//github.com/BestiVictory/APDDv2.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08545v1&entry.124074799=Read"},
{"title": "Optimizing Automatic Summarization of Long Clinical Records Using\n  Dynamic Context Extension:Testing and Evaluation of the NBCE Method", "author": "Guoqing Zhang and Keita Fukuyama and Kazumasa Kishimoto and Tomohiro Kuroda", "abstract": "  Summarizing patient clinical notes is vital for reducing documentation\nburdens. Current manual summarization makes medical staff struggle. We propose\nan automatic method using LLMs, but long inputs cause LLMs to lose context,\nreducing output quality especially in small size model. We used a 7B model,\nopen-calm-7b, enhanced with Native Bayes Context Extend and a redesigned\ndecoding mechanism to reference one sentence at a time, keeping inputs within\ncontext windows, 2048 tokens. Our improved model achieved near parity with\nGoogle's over 175B Gemini on ROUGE-L metrics with 200 samples, indicating\nstrong performance using less resources, enhancing automated EMR summarization\nfeasibility.\n", "link": "http://arxiv.org/abs/2411.08586v1", "date": "2024-11-13", "relevancy": 1.9426, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.492}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.492}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Automatic%20Summarization%20of%20Long%20Clinical%20Records%20Using%0A%20%20Dynamic%20Context%20Extension%3ATesting%20and%20Evaluation%20of%20the%20NBCE%20Method&body=Title%3A%20Optimizing%20Automatic%20Summarization%20of%20Long%20Clinical%20Records%20Using%0A%20%20Dynamic%20Context%20Extension%3ATesting%20and%20Evaluation%20of%20the%20NBCE%20Method%0AAuthor%3A%20Guoqing%20Zhang%20and%20Keita%20Fukuyama%20and%20Kazumasa%20Kishimoto%20and%20Tomohiro%20Kuroda%0AAbstract%3A%20%20%20Summarizing%20patient%20clinical%20notes%20is%20vital%20for%20reducing%20documentation%0Aburdens.%20Current%20manual%20summarization%20makes%20medical%20staff%20struggle.%20We%20propose%0Aan%20automatic%20method%20using%20LLMs%2C%20but%20long%20inputs%20cause%20LLMs%20to%20lose%20context%2C%0Areducing%20output%20quality%20especially%20in%20small%20size%20model.%20We%20used%20a%207B%20model%2C%0Aopen-calm-7b%2C%20enhanced%20with%20Native%20Bayes%20Context%20Extend%20and%20a%20redesigned%0Adecoding%20mechanism%20to%20reference%20one%20sentence%20at%20a%20time%2C%20keeping%20inputs%20within%0Acontext%20windows%2C%202048%20tokens.%20Our%20improved%20model%20achieved%20near%20parity%20with%0AGoogle%27s%20over%20175B%20Gemini%20on%20ROUGE-L%20metrics%20with%20200%20samples%2C%20indicating%0Astrong%20performance%20using%20less%20resources%2C%20enhancing%20automated%20EMR%20summarization%0Afeasibility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08586v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Automatic%2520Summarization%2520of%2520Long%2520Clinical%2520Records%2520Using%250A%2520%2520Dynamic%2520Context%2520Extension%253ATesting%2520and%2520Evaluation%2520of%2520the%2520NBCE%2520Method%26entry.906535625%3DGuoqing%2520Zhang%2520and%2520Keita%2520Fukuyama%2520and%2520Kazumasa%2520Kishimoto%2520and%2520Tomohiro%2520Kuroda%26entry.1292438233%3D%2520%2520Summarizing%2520patient%2520clinical%2520notes%2520is%2520vital%2520for%2520reducing%2520documentation%250Aburdens.%2520Current%2520manual%2520summarization%2520makes%2520medical%2520staff%2520struggle.%2520We%2520propose%250Aan%2520automatic%2520method%2520using%2520LLMs%252C%2520but%2520long%2520inputs%2520cause%2520LLMs%2520to%2520lose%2520context%252C%250Areducing%2520output%2520quality%2520especially%2520in%2520small%2520size%2520model.%2520We%2520used%2520a%25207B%2520model%252C%250Aopen-calm-7b%252C%2520enhanced%2520with%2520Native%2520Bayes%2520Context%2520Extend%2520and%2520a%2520redesigned%250Adecoding%2520mechanism%2520to%2520reference%2520one%2520sentence%2520at%2520a%2520time%252C%2520keeping%2520inputs%2520within%250Acontext%2520windows%252C%25202048%2520tokens.%2520Our%2520improved%2520model%2520achieved%2520near%2520parity%2520with%250AGoogle%2527s%2520over%2520175B%2520Gemini%2520on%2520ROUGE-L%2520metrics%2520with%2520200%2520samples%252C%2520indicating%250Astrong%2520performance%2520using%2520less%2520resources%252C%2520enhancing%2520automated%2520EMR%2520summarization%250Afeasibility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08586v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Automatic%20Summarization%20of%20Long%20Clinical%20Records%20Using%0A%20%20Dynamic%20Context%20Extension%3ATesting%20and%20Evaluation%20of%20the%20NBCE%20Method&entry.906535625=Guoqing%20Zhang%20and%20Keita%20Fukuyama%20and%20Kazumasa%20Kishimoto%20and%20Tomohiro%20Kuroda&entry.1292438233=%20%20Summarizing%20patient%20clinical%20notes%20is%20vital%20for%20reducing%20documentation%0Aburdens.%20Current%20manual%20summarization%20makes%20medical%20staff%20struggle.%20We%20propose%0Aan%20automatic%20method%20using%20LLMs%2C%20but%20long%20inputs%20cause%20LLMs%20to%20lose%20context%2C%0Areducing%20output%20quality%20especially%20in%20small%20size%20model.%20We%20used%20a%207B%20model%2C%0Aopen-calm-7b%2C%20enhanced%20with%20Native%20Bayes%20Context%20Extend%20and%20a%20redesigned%0Adecoding%20mechanism%20to%20reference%20one%20sentence%20at%20a%20time%2C%20keeping%20inputs%20within%0Acontext%20windows%2C%202048%20tokens.%20Our%20improved%20model%20achieved%20near%20parity%20with%0AGoogle%27s%20over%20175B%20Gemini%20on%20ROUGE-L%20metrics%20with%20200%20samples%2C%20indicating%0Astrong%20performance%20using%20less%20resources%2C%20enhancing%20automated%20EMR%20summarization%0Afeasibility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08586v1&entry.124074799=Read"},
{"title": "DipMe: Haptic Recognition of Granular Media for Tangible Interactive\n  Applications", "author": "Xinkai Wang and Shuo Zhang and Ziyi Zhao and Lifeng Zhu and Aiguo Song", "abstract": "  While tangible user interface has shown its power in naturally interacting\nwith rigid or soft objects, users cannot conveniently use different types of\ngranular materials as the interaction media. We introduce DipMe as a smart\ndevice to recognize the types of granular media in real time, which can be used\nto connect the granular materials in the physical world with various virtual\ncontent. Other than vision-based solutions, we propose a dip operation of our\ndevice and exploit the haptic signals to recognize different types of granular\nmaterials. With modern machine learning tools, we find the haptic signals from\ndifferent granular media are distinguishable by DipMe. With the online granular\nobject recognition, we build several tangible interactive applications,\ndemonstrating the effects of DipMe in perceiving granular materials and its\npotential in developing a tangible user interface with granular objects as the\nnew media.\n", "link": "http://arxiv.org/abs/2411.08641v1", "date": "2024-11-13", "relevancy": 1.9334, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5016}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4836}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4758}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DipMe%3A%20Haptic%20Recognition%20of%20Granular%20Media%20for%20Tangible%20Interactive%0A%20%20Applications&body=Title%3A%20DipMe%3A%20Haptic%20Recognition%20of%20Granular%20Media%20for%20Tangible%20Interactive%0A%20%20Applications%0AAuthor%3A%20Xinkai%20Wang%20and%20Shuo%20Zhang%20and%20Ziyi%20Zhao%20and%20Lifeng%20Zhu%20and%20Aiguo%20Song%0AAbstract%3A%20%20%20While%20tangible%20user%20interface%20has%20shown%20its%20power%20in%20naturally%20interacting%0Awith%20rigid%20or%20soft%20objects%2C%20users%20cannot%20conveniently%20use%20different%20types%20of%0Agranular%20materials%20as%20the%20interaction%20media.%20We%20introduce%20DipMe%20as%20a%20smart%0Adevice%20to%20recognize%20the%20types%20of%20granular%20media%20in%20real%20time%2C%20which%20can%20be%20used%0Ato%20connect%20the%20granular%20materials%20in%20the%20physical%20world%20with%20various%20virtual%0Acontent.%20Other%20than%20vision-based%20solutions%2C%20we%20propose%20a%20dip%20operation%20of%20our%0Adevice%20and%20exploit%20the%20haptic%20signals%20to%20recognize%20different%20types%20of%20granular%0Amaterials.%20With%20modern%20machine%20learning%20tools%2C%20we%20find%20the%20haptic%20signals%20from%0Adifferent%20granular%20media%20are%20distinguishable%20by%20DipMe.%20With%20the%20online%20granular%0Aobject%20recognition%2C%20we%20build%20several%20tangible%20interactive%20applications%2C%0Ademonstrating%20the%20effects%20of%20DipMe%20in%20perceiving%20granular%20materials%20and%20its%0Apotential%20in%20developing%20a%20tangible%20user%20interface%20with%20granular%20objects%20as%20the%0Anew%20media.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08641v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDipMe%253A%2520Haptic%2520Recognition%2520of%2520Granular%2520Media%2520for%2520Tangible%2520Interactive%250A%2520%2520Applications%26entry.906535625%3DXinkai%2520Wang%2520and%2520Shuo%2520Zhang%2520and%2520Ziyi%2520Zhao%2520and%2520Lifeng%2520Zhu%2520and%2520Aiguo%2520Song%26entry.1292438233%3D%2520%2520While%2520tangible%2520user%2520interface%2520has%2520shown%2520its%2520power%2520in%2520naturally%2520interacting%250Awith%2520rigid%2520or%2520soft%2520objects%252C%2520users%2520cannot%2520conveniently%2520use%2520different%2520types%2520of%250Agranular%2520materials%2520as%2520the%2520interaction%2520media.%2520We%2520introduce%2520DipMe%2520as%2520a%2520smart%250Adevice%2520to%2520recognize%2520the%2520types%2520of%2520granular%2520media%2520in%2520real%2520time%252C%2520which%2520can%2520be%2520used%250Ato%2520connect%2520the%2520granular%2520materials%2520in%2520the%2520physical%2520world%2520with%2520various%2520virtual%250Acontent.%2520Other%2520than%2520vision-based%2520solutions%252C%2520we%2520propose%2520a%2520dip%2520operation%2520of%2520our%250Adevice%2520and%2520exploit%2520the%2520haptic%2520signals%2520to%2520recognize%2520different%2520types%2520of%2520granular%250Amaterials.%2520With%2520modern%2520machine%2520learning%2520tools%252C%2520we%2520find%2520the%2520haptic%2520signals%2520from%250Adifferent%2520granular%2520media%2520are%2520distinguishable%2520by%2520DipMe.%2520With%2520the%2520online%2520granular%250Aobject%2520recognition%252C%2520we%2520build%2520several%2520tangible%2520interactive%2520applications%252C%250Ademonstrating%2520the%2520effects%2520of%2520DipMe%2520in%2520perceiving%2520granular%2520materials%2520and%2520its%250Apotential%2520in%2520developing%2520a%2520tangible%2520user%2520interface%2520with%2520granular%2520objects%2520as%2520the%250Anew%2520media.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08641v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DipMe%3A%20Haptic%20Recognition%20of%20Granular%20Media%20for%20Tangible%20Interactive%0A%20%20Applications&entry.906535625=Xinkai%20Wang%20and%20Shuo%20Zhang%20and%20Ziyi%20Zhao%20and%20Lifeng%20Zhu%20and%20Aiguo%20Song&entry.1292438233=%20%20While%20tangible%20user%20interface%20has%20shown%20its%20power%20in%20naturally%20interacting%0Awith%20rigid%20or%20soft%20objects%2C%20users%20cannot%20conveniently%20use%20different%20types%20of%0Agranular%20materials%20as%20the%20interaction%20media.%20We%20introduce%20DipMe%20as%20a%20smart%0Adevice%20to%20recognize%20the%20types%20of%20granular%20media%20in%20real%20time%2C%20which%20can%20be%20used%0Ato%20connect%20the%20granular%20materials%20in%20the%20physical%20world%20with%20various%20virtual%0Acontent.%20Other%20than%20vision-based%20solutions%2C%20we%20propose%20a%20dip%20operation%20of%20our%0Adevice%20and%20exploit%20the%20haptic%20signals%20to%20recognize%20different%20types%20of%20granular%0Amaterials.%20With%20modern%20machine%20learning%20tools%2C%20we%20find%20the%20haptic%20signals%20from%0Adifferent%20granular%20media%20are%20distinguishable%20by%20DipMe.%20With%20the%20online%20granular%0Aobject%20recognition%2C%20we%20build%20several%20tangible%20interactive%20applications%2C%0Ademonstrating%20the%20effects%20of%20DipMe%20in%20perceiving%20granular%20materials%20and%20its%0Apotential%20in%20developing%20a%20tangible%20user%20interface%20with%20granular%20objects%20as%20the%0Anew%20media.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08641v1&entry.124074799=Read"},
{"title": "Sharingan: Extract User Action Sequence from Desktop Recordings", "author": "Yanting Chen and Yi Ren and Xiaoting Qin and Jue Zhang and Kehong Yuan and Lu Han and Qingwei Lin and Dongmei Zhang and Saravan Rajmohan and Qi Zhang", "abstract": "  Video recordings of user activities, particularly desktop recordings, offer a\nrich source of data for understanding user behaviors and automating processes.\nHowever, despite advancements in Vision-Language Models (VLMs) and their\nincreasing use in video analysis, extracting user actions from desktop\nrecordings remains an underexplored area. This paper addresses this gap by\nproposing two novel VLM-based methods for user action extraction: the Direct\nFrame-Based Approach (DF), which inputs sampled frames directly into VLMs, and\nthe Differential Frame-Based Approach (DiffF), which incorporates explicit\nframe differences detected via computer vision techniques. We evaluate these\nmethods using a basic self-curated dataset and an advanced benchmark adapted\nfrom prior work. Our results show that the DF approach achieves an accuracy of\n70% to 80% in identifying user actions, with the extracted action sequences\nbeing re-playable though Robotic Process Automation. We find that while VLMs\nshow potential, incorporating explicit UI changes can degrade performance,\nmaking the DF approach more reliable. This work represents the first\napplication of VLMs for extracting user action sequences from desktop\nrecordings, contributing new methods, benchmarks, and insights for future\nresearch.\n", "link": "http://arxiv.org/abs/2411.08768v1", "date": "2024-11-13", "relevancy": 1.9321, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5082}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4804}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sharingan%3A%20Extract%20User%20Action%20Sequence%20from%20Desktop%20Recordings&body=Title%3A%20Sharingan%3A%20Extract%20User%20Action%20Sequence%20from%20Desktop%20Recordings%0AAuthor%3A%20Yanting%20Chen%20and%20Yi%20Ren%20and%20Xiaoting%20Qin%20and%20Jue%20Zhang%20and%20Kehong%20Yuan%20and%20Lu%20Han%20and%20Qingwei%20Lin%20and%20Dongmei%20Zhang%20and%20Saravan%20Rajmohan%20and%20Qi%20Zhang%0AAbstract%3A%20%20%20Video%20recordings%20of%20user%20activities%2C%20particularly%20desktop%20recordings%2C%20offer%20a%0Arich%20source%20of%20data%20for%20understanding%20user%20behaviors%20and%20automating%20processes.%0AHowever%2C%20despite%20advancements%20in%20Vision-Language%20Models%20%28VLMs%29%20and%20their%0Aincreasing%20use%20in%20video%20analysis%2C%20extracting%20user%20actions%20from%20desktop%0Arecordings%20remains%20an%20underexplored%20area.%20This%20paper%20addresses%20this%20gap%20by%0Aproposing%20two%20novel%20VLM-based%20methods%20for%20user%20action%20extraction%3A%20the%20Direct%0AFrame-Based%20Approach%20%28DF%29%2C%20which%20inputs%20sampled%20frames%20directly%20into%20VLMs%2C%20and%0Athe%20Differential%20Frame-Based%20Approach%20%28DiffF%29%2C%20which%20incorporates%20explicit%0Aframe%20differences%20detected%20via%20computer%20vision%20techniques.%20We%20evaluate%20these%0Amethods%20using%20a%20basic%20self-curated%20dataset%20and%20an%20advanced%20benchmark%20adapted%0Afrom%20prior%20work.%20Our%20results%20show%20that%20the%20DF%20approach%20achieves%20an%20accuracy%20of%0A70%25%20to%2080%25%20in%20identifying%20user%20actions%2C%20with%20the%20extracted%20action%20sequences%0Abeing%20re-playable%20though%20Robotic%20Process%20Automation.%20We%20find%20that%20while%20VLMs%0Ashow%20potential%2C%20incorporating%20explicit%20UI%20changes%20can%20degrade%20performance%2C%0Amaking%20the%20DF%20approach%20more%20reliable.%20This%20work%20represents%20the%20first%0Aapplication%20of%20VLMs%20for%20extracting%20user%20action%20sequences%20from%20desktop%0Arecordings%2C%20contributing%20new%20methods%2C%20benchmarks%2C%20and%20insights%20for%20future%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08768v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSharingan%253A%2520Extract%2520User%2520Action%2520Sequence%2520from%2520Desktop%2520Recordings%26entry.906535625%3DYanting%2520Chen%2520and%2520Yi%2520Ren%2520and%2520Xiaoting%2520Qin%2520and%2520Jue%2520Zhang%2520and%2520Kehong%2520Yuan%2520and%2520Lu%2520Han%2520and%2520Qingwei%2520Lin%2520and%2520Dongmei%2520Zhang%2520and%2520Saravan%2520Rajmohan%2520and%2520Qi%2520Zhang%26entry.1292438233%3D%2520%2520Video%2520recordings%2520of%2520user%2520activities%252C%2520particularly%2520desktop%2520recordings%252C%2520offer%2520a%250Arich%2520source%2520of%2520data%2520for%2520understanding%2520user%2520behaviors%2520and%2520automating%2520processes.%250AHowever%252C%2520despite%2520advancements%2520in%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520and%2520their%250Aincreasing%2520use%2520in%2520video%2520analysis%252C%2520extracting%2520user%2520actions%2520from%2520desktop%250Arecordings%2520remains%2520an%2520underexplored%2520area.%2520This%2520paper%2520addresses%2520this%2520gap%2520by%250Aproposing%2520two%2520novel%2520VLM-based%2520methods%2520for%2520user%2520action%2520extraction%253A%2520the%2520Direct%250AFrame-Based%2520Approach%2520%2528DF%2529%252C%2520which%2520inputs%2520sampled%2520frames%2520directly%2520into%2520VLMs%252C%2520and%250Athe%2520Differential%2520Frame-Based%2520Approach%2520%2528DiffF%2529%252C%2520which%2520incorporates%2520explicit%250Aframe%2520differences%2520detected%2520via%2520computer%2520vision%2520techniques.%2520We%2520evaluate%2520these%250Amethods%2520using%2520a%2520basic%2520self-curated%2520dataset%2520and%2520an%2520advanced%2520benchmark%2520adapted%250Afrom%2520prior%2520work.%2520Our%2520results%2520show%2520that%2520the%2520DF%2520approach%2520achieves%2520an%2520accuracy%2520of%250A70%2525%2520to%252080%2525%2520in%2520identifying%2520user%2520actions%252C%2520with%2520the%2520extracted%2520action%2520sequences%250Abeing%2520re-playable%2520though%2520Robotic%2520Process%2520Automation.%2520We%2520find%2520that%2520while%2520VLMs%250Ashow%2520potential%252C%2520incorporating%2520explicit%2520UI%2520changes%2520can%2520degrade%2520performance%252C%250Amaking%2520the%2520DF%2520approach%2520more%2520reliable.%2520This%2520work%2520represents%2520the%2520first%250Aapplication%2520of%2520VLMs%2520for%2520extracting%2520user%2520action%2520sequences%2520from%2520desktop%250Arecordings%252C%2520contributing%2520new%2520methods%252C%2520benchmarks%252C%2520and%2520insights%2520for%2520future%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08768v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sharingan%3A%20Extract%20User%20Action%20Sequence%20from%20Desktop%20Recordings&entry.906535625=Yanting%20Chen%20and%20Yi%20Ren%20and%20Xiaoting%20Qin%20and%20Jue%20Zhang%20and%20Kehong%20Yuan%20and%20Lu%20Han%20and%20Qingwei%20Lin%20and%20Dongmei%20Zhang%20and%20Saravan%20Rajmohan%20and%20Qi%20Zhang&entry.1292438233=%20%20Video%20recordings%20of%20user%20activities%2C%20particularly%20desktop%20recordings%2C%20offer%20a%0Arich%20source%20of%20data%20for%20understanding%20user%20behaviors%20and%20automating%20processes.%0AHowever%2C%20despite%20advancements%20in%20Vision-Language%20Models%20%28VLMs%29%20and%20their%0Aincreasing%20use%20in%20video%20analysis%2C%20extracting%20user%20actions%20from%20desktop%0Arecordings%20remains%20an%20underexplored%20area.%20This%20paper%20addresses%20this%20gap%20by%0Aproposing%20two%20novel%20VLM-based%20methods%20for%20user%20action%20extraction%3A%20the%20Direct%0AFrame-Based%20Approach%20%28DF%29%2C%20which%20inputs%20sampled%20frames%20directly%20into%20VLMs%2C%20and%0Athe%20Differential%20Frame-Based%20Approach%20%28DiffF%29%2C%20which%20incorporates%20explicit%0Aframe%20differences%20detected%20via%20computer%20vision%20techniques.%20We%20evaluate%20these%0Amethods%20using%20a%20basic%20self-curated%20dataset%20and%20an%20advanced%20benchmark%20adapted%0Afrom%20prior%20work.%20Our%20results%20show%20that%20the%20DF%20approach%20achieves%20an%20accuracy%20of%0A70%25%20to%2080%25%20in%20identifying%20user%20actions%2C%20with%20the%20extracted%20action%20sequences%0Abeing%20re-playable%20though%20Robotic%20Process%20Automation.%20We%20find%20that%20while%20VLMs%0Ashow%20potential%2C%20incorporating%20explicit%20UI%20changes%20can%20degrade%20performance%2C%0Amaking%20the%20DF%20approach%20more%20reliable.%20This%20work%20represents%20the%20first%0Aapplication%20of%20VLMs%20for%20extracting%20user%20action%20sequences%20from%20desktop%0Arecordings%2C%20contributing%20new%20methods%2C%20benchmarks%2C%20and%20insights%20for%20future%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08768v1&entry.124074799=Read"},
{"title": "Harnessing Smartphone Sensors for Enhanced Road Safety: A Comprehensive\n  Dataset and Review", "author": "Amith Khandakar and David G. Michelson and Mansura Naznine and Abdus Salam and Md. Nahiduzzaman and Khaled M. Khan and Ponnuthurai Nagaratnam Suganthan and Mohamed Arselene Ayari and Hamid Menouar and Julfikar Haider", "abstract": "  Severe collisions can result from aggressive driving and poor road\nconditions, emphasizing the need for effective monitoring to ensure safety.\nSmartphones, with their array of built-in sensors, offer a practical and\naffordable solution for road-sensing. However, the lack of reliable,\nstandardized datasets has hindered progress in assessing road conditions and\ndriving patterns. This study addresses this gap by introducing a comprehensive\ndataset derived from smartphone sensors, which surpasses existing datasets by\nincorporating a diverse range of sensors including accelerometer, gyroscope,\nmagnetometer, GPS, gravity, orientation, and uncalibrated sensors. These\nsensors capture extensive parameters such as acceleration force, gravitation,\nrotation rate, magnetic field strength, and vehicle speed, providing a detailed\nunderstanding of road conditions and driving behaviors. The dataset is designed\nto enhance road safety, infrastructure maintenance, traffic management, and\nurban planning. By making this dataset available to the community, the study\naims to foster collaboration, inspire further research, and facilitate the\ndevelopment of innovative solutions in intelligent transportation systems.\n", "link": "http://arxiv.org/abs/2411.07315v2", "date": "2024-11-13", "relevancy": 1.9206, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4973}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4893}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harnessing%20Smartphone%20Sensors%20for%20Enhanced%20Road%20Safety%3A%20A%20Comprehensive%0A%20%20Dataset%20and%20Review&body=Title%3A%20Harnessing%20Smartphone%20Sensors%20for%20Enhanced%20Road%20Safety%3A%20A%20Comprehensive%0A%20%20Dataset%20and%20Review%0AAuthor%3A%20Amith%20Khandakar%20and%20David%20G.%20Michelson%20and%20Mansura%20Naznine%20and%20Abdus%20Salam%20and%20Md.%20Nahiduzzaman%20and%20Khaled%20M.%20Khan%20and%20Ponnuthurai%20Nagaratnam%20Suganthan%20and%20Mohamed%20Arselene%20Ayari%20and%20Hamid%20Menouar%20and%20Julfikar%20Haider%0AAbstract%3A%20%20%20Severe%20collisions%20can%20result%20from%20aggressive%20driving%20and%20poor%20road%0Aconditions%2C%20emphasizing%20the%20need%20for%20effective%20monitoring%20to%20ensure%20safety.%0ASmartphones%2C%20with%20their%20array%20of%20built-in%20sensors%2C%20offer%20a%20practical%20and%0Aaffordable%20solution%20for%20road-sensing.%20However%2C%20the%20lack%20of%20reliable%2C%0Astandardized%20datasets%20has%20hindered%20progress%20in%20assessing%20road%20conditions%20and%0Adriving%20patterns.%20This%20study%20addresses%20this%20gap%20by%20introducing%20a%20comprehensive%0Adataset%20derived%20from%20smartphone%20sensors%2C%20which%20surpasses%20existing%20datasets%20by%0Aincorporating%20a%20diverse%20range%20of%20sensors%20including%20accelerometer%2C%20gyroscope%2C%0Amagnetometer%2C%20GPS%2C%20gravity%2C%20orientation%2C%20and%20uncalibrated%20sensors.%20These%0Asensors%20capture%20extensive%20parameters%20such%20as%20acceleration%20force%2C%20gravitation%2C%0Arotation%20rate%2C%20magnetic%20field%20strength%2C%20and%20vehicle%20speed%2C%20providing%20a%20detailed%0Aunderstanding%20of%20road%20conditions%20and%20driving%20behaviors.%20The%20dataset%20is%20designed%0Ato%20enhance%20road%20safety%2C%20infrastructure%20maintenance%2C%20traffic%20management%2C%20and%0Aurban%20planning.%20By%20making%20this%20dataset%20available%20to%20the%20community%2C%20the%20study%0Aaims%20to%20foster%20collaboration%2C%20inspire%20further%20research%2C%20and%20facilitate%20the%0Adevelopment%20of%20innovative%20solutions%20in%20intelligent%20transportation%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07315v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarnessing%2520Smartphone%2520Sensors%2520for%2520Enhanced%2520Road%2520Safety%253A%2520A%2520Comprehensive%250A%2520%2520Dataset%2520and%2520Review%26entry.906535625%3DAmith%2520Khandakar%2520and%2520David%2520G.%2520Michelson%2520and%2520Mansura%2520Naznine%2520and%2520Abdus%2520Salam%2520and%2520Md.%2520Nahiduzzaman%2520and%2520Khaled%2520M.%2520Khan%2520and%2520Ponnuthurai%2520Nagaratnam%2520Suganthan%2520and%2520Mohamed%2520Arselene%2520Ayari%2520and%2520Hamid%2520Menouar%2520and%2520Julfikar%2520Haider%26entry.1292438233%3D%2520%2520Severe%2520collisions%2520can%2520result%2520from%2520aggressive%2520driving%2520and%2520poor%2520road%250Aconditions%252C%2520emphasizing%2520the%2520need%2520for%2520effective%2520monitoring%2520to%2520ensure%2520safety.%250ASmartphones%252C%2520with%2520their%2520array%2520of%2520built-in%2520sensors%252C%2520offer%2520a%2520practical%2520and%250Aaffordable%2520solution%2520for%2520road-sensing.%2520However%252C%2520the%2520lack%2520of%2520reliable%252C%250Astandardized%2520datasets%2520has%2520hindered%2520progress%2520in%2520assessing%2520road%2520conditions%2520and%250Adriving%2520patterns.%2520This%2520study%2520addresses%2520this%2520gap%2520by%2520introducing%2520a%2520comprehensive%250Adataset%2520derived%2520from%2520smartphone%2520sensors%252C%2520which%2520surpasses%2520existing%2520datasets%2520by%250Aincorporating%2520a%2520diverse%2520range%2520of%2520sensors%2520including%2520accelerometer%252C%2520gyroscope%252C%250Amagnetometer%252C%2520GPS%252C%2520gravity%252C%2520orientation%252C%2520and%2520uncalibrated%2520sensors.%2520These%250Asensors%2520capture%2520extensive%2520parameters%2520such%2520as%2520acceleration%2520force%252C%2520gravitation%252C%250Arotation%2520rate%252C%2520magnetic%2520field%2520strength%252C%2520and%2520vehicle%2520speed%252C%2520providing%2520a%2520detailed%250Aunderstanding%2520of%2520road%2520conditions%2520and%2520driving%2520behaviors.%2520The%2520dataset%2520is%2520designed%250Ato%2520enhance%2520road%2520safety%252C%2520infrastructure%2520maintenance%252C%2520traffic%2520management%252C%2520and%250Aurban%2520planning.%2520By%2520making%2520this%2520dataset%2520available%2520to%2520the%2520community%252C%2520the%2520study%250Aaims%2520to%2520foster%2520collaboration%252C%2520inspire%2520further%2520research%252C%2520and%2520facilitate%2520the%250Adevelopment%2520of%2520innovative%2520solutions%2520in%2520intelligent%2520transportation%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07315v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harnessing%20Smartphone%20Sensors%20for%20Enhanced%20Road%20Safety%3A%20A%20Comprehensive%0A%20%20Dataset%20and%20Review&entry.906535625=Amith%20Khandakar%20and%20David%20G.%20Michelson%20and%20Mansura%20Naznine%20and%20Abdus%20Salam%20and%20Md.%20Nahiduzzaman%20and%20Khaled%20M.%20Khan%20and%20Ponnuthurai%20Nagaratnam%20Suganthan%20and%20Mohamed%20Arselene%20Ayari%20and%20Hamid%20Menouar%20and%20Julfikar%20Haider&entry.1292438233=%20%20Severe%20collisions%20can%20result%20from%20aggressive%20driving%20and%20poor%20road%0Aconditions%2C%20emphasizing%20the%20need%20for%20effective%20monitoring%20to%20ensure%20safety.%0ASmartphones%2C%20with%20their%20array%20of%20built-in%20sensors%2C%20offer%20a%20practical%20and%0Aaffordable%20solution%20for%20road-sensing.%20However%2C%20the%20lack%20of%20reliable%2C%0Astandardized%20datasets%20has%20hindered%20progress%20in%20assessing%20road%20conditions%20and%0Adriving%20patterns.%20This%20study%20addresses%20this%20gap%20by%20introducing%20a%20comprehensive%0Adataset%20derived%20from%20smartphone%20sensors%2C%20which%20surpasses%20existing%20datasets%20by%0Aincorporating%20a%20diverse%20range%20of%20sensors%20including%20accelerometer%2C%20gyroscope%2C%0Amagnetometer%2C%20GPS%2C%20gravity%2C%20orientation%2C%20and%20uncalibrated%20sensors.%20These%0Asensors%20capture%20extensive%20parameters%20such%20as%20acceleration%20force%2C%20gravitation%2C%0Arotation%20rate%2C%20magnetic%20field%20strength%2C%20and%20vehicle%20speed%2C%20providing%20a%20detailed%0Aunderstanding%20of%20road%20conditions%20and%20driving%20behaviors.%20The%20dataset%20is%20designed%0Ato%20enhance%20road%20safety%2C%20infrastructure%20maintenance%2C%20traffic%20management%2C%20and%0Aurban%20planning.%20By%20making%20this%20dataset%20available%20to%20the%20community%2C%20the%20study%0Aaims%20to%20foster%20collaboration%2C%20inspire%20further%20research%2C%20and%20facilitate%20the%0Adevelopment%20of%20innovative%20solutions%20in%20intelligent%20transportation%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07315v2&entry.124074799=Read"},
{"title": "On the Application of Model Predictive Control to a Weighted Coverage\n  Path Planning Problem", "author": "Kilian Schweppe and Ludmila Moshagen and Georg Schildbach", "abstract": "  This paper considers the application of Model Predictive Control (MPC) to a\nweighted coverage path planning (WCPP) problem. The problem appears in a wide\nrange of practical applications, such as search and rescue (SAR) missions. The\nbasic setup is that one (or multiple) agents can move around a given search\nspace and collect rewards from a given spatial distribution. Unlike an\nartificial potential field, each reward can only be collected once. In contrast\nto a Traveling Salesman Problem (TSP), the agent moves in a continuous space.\nMoreover, he is not obliged to cover all locations and/or may return to\npreviously visited locations. The WCPP problem is tackled by a new Model\nPredictive Control (MPC) formulation with so-called Coverage Constraints (CCs).\nIt is shown that the solution becomes more effective if the solver is\ninitialized with a TSP-based heuristic. With and without this initialization,\nthe proposed MPC approach clearly outperforms a naive MPC formulation, as\ndemonstrated in a small simulation study.\n", "link": "http://arxiv.org/abs/2411.08634v1", "date": "2024-11-13", "relevancy": 1.9088, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5031}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4789}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4651}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Application%20of%20Model%20Predictive%20Control%20to%20a%20Weighted%20Coverage%0A%20%20Path%20Planning%20Problem&body=Title%3A%20On%20the%20Application%20of%20Model%20Predictive%20Control%20to%20a%20Weighted%20Coverage%0A%20%20Path%20Planning%20Problem%0AAuthor%3A%20Kilian%20Schweppe%20and%20Ludmila%20Moshagen%20and%20Georg%20Schildbach%0AAbstract%3A%20%20%20This%20paper%20considers%20the%20application%20of%20Model%20Predictive%20Control%20%28MPC%29%20to%20a%0Aweighted%20coverage%20path%20planning%20%28WCPP%29%20problem.%20The%20problem%20appears%20in%20a%20wide%0Arange%20of%20practical%20applications%2C%20such%20as%20search%20and%20rescue%20%28SAR%29%20missions.%20The%0Abasic%20setup%20is%20that%20one%20%28or%20multiple%29%20agents%20can%20move%20around%20a%20given%20search%0Aspace%20and%20collect%20rewards%20from%20a%20given%20spatial%20distribution.%20Unlike%20an%0Aartificial%20potential%20field%2C%20each%20reward%20can%20only%20be%20collected%20once.%20In%20contrast%0Ato%20a%20Traveling%20Salesman%20Problem%20%28TSP%29%2C%20the%20agent%20moves%20in%20a%20continuous%20space.%0AMoreover%2C%20he%20is%20not%20obliged%20to%20cover%20all%20locations%20and/or%20may%20return%20to%0Apreviously%20visited%20locations.%20The%20WCPP%20problem%20is%20tackled%20by%20a%20new%20Model%0APredictive%20Control%20%28MPC%29%20formulation%20with%20so-called%20Coverage%20Constraints%20%28CCs%29.%0AIt%20is%20shown%20that%20the%20solution%20becomes%20more%20effective%20if%20the%20solver%20is%0Ainitialized%20with%20a%20TSP-based%20heuristic.%20With%20and%20without%20this%20initialization%2C%0Athe%20proposed%20MPC%20approach%20clearly%20outperforms%20a%20naive%20MPC%20formulation%2C%20as%0Ademonstrated%20in%20a%20small%20simulation%20study.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08634v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Application%2520of%2520Model%2520Predictive%2520Control%2520to%2520a%2520Weighted%2520Coverage%250A%2520%2520Path%2520Planning%2520Problem%26entry.906535625%3DKilian%2520Schweppe%2520and%2520Ludmila%2520Moshagen%2520and%2520Georg%2520Schildbach%26entry.1292438233%3D%2520%2520This%2520paper%2520considers%2520the%2520application%2520of%2520Model%2520Predictive%2520Control%2520%2528MPC%2529%2520to%2520a%250Aweighted%2520coverage%2520path%2520planning%2520%2528WCPP%2529%2520problem.%2520The%2520problem%2520appears%2520in%2520a%2520wide%250Arange%2520of%2520practical%2520applications%252C%2520such%2520as%2520search%2520and%2520rescue%2520%2528SAR%2529%2520missions.%2520The%250Abasic%2520setup%2520is%2520that%2520one%2520%2528or%2520multiple%2529%2520agents%2520can%2520move%2520around%2520a%2520given%2520search%250Aspace%2520and%2520collect%2520rewards%2520from%2520a%2520given%2520spatial%2520distribution.%2520Unlike%2520an%250Aartificial%2520potential%2520field%252C%2520each%2520reward%2520can%2520only%2520be%2520collected%2520once.%2520In%2520contrast%250Ato%2520a%2520Traveling%2520Salesman%2520Problem%2520%2528TSP%2529%252C%2520the%2520agent%2520moves%2520in%2520a%2520continuous%2520space.%250AMoreover%252C%2520he%2520is%2520not%2520obliged%2520to%2520cover%2520all%2520locations%2520and/or%2520may%2520return%2520to%250Apreviously%2520visited%2520locations.%2520The%2520WCPP%2520problem%2520is%2520tackled%2520by%2520a%2520new%2520Model%250APredictive%2520Control%2520%2528MPC%2529%2520formulation%2520with%2520so-called%2520Coverage%2520Constraints%2520%2528CCs%2529.%250AIt%2520is%2520shown%2520that%2520the%2520solution%2520becomes%2520more%2520effective%2520if%2520the%2520solver%2520is%250Ainitialized%2520with%2520a%2520TSP-based%2520heuristic.%2520With%2520and%2520without%2520this%2520initialization%252C%250Athe%2520proposed%2520MPC%2520approach%2520clearly%2520outperforms%2520a%2520naive%2520MPC%2520formulation%252C%2520as%250Ademonstrated%2520in%2520a%2520small%2520simulation%2520study.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08634v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Application%20of%20Model%20Predictive%20Control%20to%20a%20Weighted%20Coverage%0A%20%20Path%20Planning%20Problem&entry.906535625=Kilian%20Schweppe%20and%20Ludmila%20Moshagen%20and%20Georg%20Schildbach&entry.1292438233=%20%20This%20paper%20considers%20the%20application%20of%20Model%20Predictive%20Control%20%28MPC%29%20to%20a%0Aweighted%20coverage%20path%20planning%20%28WCPP%29%20problem.%20The%20problem%20appears%20in%20a%20wide%0Arange%20of%20practical%20applications%2C%20such%20as%20search%20and%20rescue%20%28SAR%29%20missions.%20The%0Abasic%20setup%20is%20that%20one%20%28or%20multiple%29%20agents%20can%20move%20around%20a%20given%20search%0Aspace%20and%20collect%20rewards%20from%20a%20given%20spatial%20distribution.%20Unlike%20an%0Aartificial%20potential%20field%2C%20each%20reward%20can%20only%20be%20collected%20once.%20In%20contrast%0Ato%20a%20Traveling%20Salesman%20Problem%20%28TSP%29%2C%20the%20agent%20moves%20in%20a%20continuous%20space.%0AMoreover%2C%20he%20is%20not%20obliged%20to%20cover%20all%20locations%20and/or%20may%20return%20to%0Apreviously%20visited%20locations.%20The%20WCPP%20problem%20is%20tackled%20by%20a%20new%20Model%0APredictive%20Control%20%28MPC%29%20formulation%20with%20so-called%20Coverage%20Constraints%20%28CCs%29.%0AIt%20is%20shown%20that%20the%20solution%20becomes%20more%20effective%20if%20the%20solver%20is%0Ainitialized%20with%20a%20TSP-based%20heuristic.%20With%20and%20without%20this%20initialization%2C%0Athe%20proposed%20MPC%20approach%20clearly%20outperforms%20a%20naive%20MPC%20formulation%2C%20as%0Ademonstrated%20in%20a%20small%20simulation%20study.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08634v1&entry.124074799=Read"},
{"title": "Insights and Current Gaps in Open-Source LLM Vulnerability Scanners: A\n  Comparative Analysis", "author": "Jonathan Brokman and Omer Hofman and Oren Rachmil and Inderjeet Singh and Rathina Sabapathy Aishvariya Priya and Vikas Pahuja and Amit Giloni and Roman Vainshtein and Hisashi Kojima", "abstract": "  This report presents a comparative analysis of open-source vulnerability\nscanners for conversational large language models (LLMs). As LLMs become\nintegral to various applications, they also present potential attack surfaces,\nexposed to security risks such as information leakage and jailbreak attacks.\nOur study evaluates prominent scanners - Garak, Giskard, PyRIT, and\nCyberSecEval - that adapt red-teaming practices to expose these\nvulnerabilities. We detail the distinctive features and practical use of these\nscanners, outline unifying principles of their design and perform quantitative\nevaluations to compare them. These evaluations uncover significant reliability\nissues in detecting successful attacks, highlighting a fundamental gap for\nfuture development. Additionally, we contribute a preliminary labelled dataset,\nwhich serves as an initial step to bridge this gap. Based on the above, we\nprovide strategic recommendations to assist organizations choose the most\nsuitable scanner for their red-teaming needs, accounting for customizability,\ntest suite comprehensiveness, and industry-specific use cases.\n", "link": "http://arxiv.org/abs/2410.16527v2", "date": "2024-11-13", "relevancy": 1.9048, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4849}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4849}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4329}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Insights%20and%20Current%20Gaps%20in%20Open-Source%20LLM%20Vulnerability%20Scanners%3A%20A%0A%20%20Comparative%20Analysis&body=Title%3A%20Insights%20and%20Current%20Gaps%20in%20Open-Source%20LLM%20Vulnerability%20Scanners%3A%20A%0A%20%20Comparative%20Analysis%0AAuthor%3A%20Jonathan%20Brokman%20and%20Omer%20Hofman%20and%20Oren%20Rachmil%20and%20Inderjeet%20Singh%20and%20Rathina%20Sabapathy%20Aishvariya%20Priya%20and%20Vikas%20Pahuja%20and%20Amit%20Giloni%20and%20Roman%20Vainshtein%20and%20Hisashi%20Kojima%0AAbstract%3A%20%20%20This%20report%20presents%20a%20comparative%20analysis%20of%20open-source%20vulnerability%0Ascanners%20for%20conversational%20large%20language%20models%20%28LLMs%29.%20As%20LLMs%20become%0Aintegral%20to%20various%20applications%2C%20they%20also%20present%20potential%20attack%20surfaces%2C%0Aexposed%20to%20security%20risks%20such%20as%20information%20leakage%20and%20jailbreak%20attacks.%0AOur%20study%20evaluates%20prominent%20scanners%20-%20Garak%2C%20Giskard%2C%20PyRIT%2C%20and%0ACyberSecEval%20-%20that%20adapt%20red-teaming%20practices%20to%20expose%20these%0Avulnerabilities.%20We%20detail%20the%20distinctive%20features%20and%20practical%20use%20of%20these%0Ascanners%2C%20outline%20unifying%20principles%20of%20their%20design%20and%20perform%20quantitative%0Aevaluations%20to%20compare%20them.%20These%20evaluations%20uncover%20significant%20reliability%0Aissues%20in%20detecting%20successful%20attacks%2C%20highlighting%20a%20fundamental%20gap%20for%0Afuture%20development.%20Additionally%2C%20we%20contribute%20a%20preliminary%20labelled%20dataset%2C%0Awhich%20serves%20as%20an%20initial%20step%20to%20bridge%20this%20gap.%20Based%20on%20the%20above%2C%20we%0Aprovide%20strategic%20recommendations%20to%20assist%20organizations%20choose%20the%20most%0Asuitable%20scanner%20for%20their%20red-teaming%20needs%2C%20accounting%20for%20customizability%2C%0Atest%20suite%20comprehensiveness%2C%20and%20industry-specific%20use%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16527v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInsights%2520and%2520Current%2520Gaps%2520in%2520Open-Source%2520LLM%2520Vulnerability%2520Scanners%253A%2520A%250A%2520%2520Comparative%2520Analysis%26entry.906535625%3DJonathan%2520Brokman%2520and%2520Omer%2520Hofman%2520and%2520Oren%2520Rachmil%2520and%2520Inderjeet%2520Singh%2520and%2520Rathina%2520Sabapathy%2520Aishvariya%2520Priya%2520and%2520Vikas%2520Pahuja%2520and%2520Amit%2520Giloni%2520and%2520Roman%2520Vainshtein%2520and%2520Hisashi%2520Kojima%26entry.1292438233%3D%2520%2520This%2520report%2520presents%2520a%2520comparative%2520analysis%2520of%2520open-source%2520vulnerability%250Ascanners%2520for%2520conversational%2520large%2520language%2520models%2520%2528LLMs%2529.%2520As%2520LLMs%2520become%250Aintegral%2520to%2520various%2520applications%252C%2520they%2520also%2520present%2520potential%2520attack%2520surfaces%252C%250Aexposed%2520to%2520security%2520risks%2520such%2520as%2520information%2520leakage%2520and%2520jailbreak%2520attacks.%250AOur%2520study%2520evaluates%2520prominent%2520scanners%2520-%2520Garak%252C%2520Giskard%252C%2520PyRIT%252C%2520and%250ACyberSecEval%2520-%2520that%2520adapt%2520red-teaming%2520practices%2520to%2520expose%2520these%250Avulnerabilities.%2520We%2520detail%2520the%2520distinctive%2520features%2520and%2520practical%2520use%2520of%2520these%250Ascanners%252C%2520outline%2520unifying%2520principles%2520of%2520their%2520design%2520and%2520perform%2520quantitative%250Aevaluations%2520to%2520compare%2520them.%2520These%2520evaluations%2520uncover%2520significant%2520reliability%250Aissues%2520in%2520detecting%2520successful%2520attacks%252C%2520highlighting%2520a%2520fundamental%2520gap%2520for%250Afuture%2520development.%2520Additionally%252C%2520we%2520contribute%2520a%2520preliminary%2520labelled%2520dataset%252C%250Awhich%2520serves%2520as%2520an%2520initial%2520step%2520to%2520bridge%2520this%2520gap.%2520Based%2520on%2520the%2520above%252C%2520we%250Aprovide%2520strategic%2520recommendations%2520to%2520assist%2520organizations%2520choose%2520the%2520most%250Asuitable%2520scanner%2520for%2520their%2520red-teaming%2520needs%252C%2520accounting%2520for%2520customizability%252C%250Atest%2520suite%2520comprehensiveness%252C%2520and%2520industry-specific%2520use%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16527v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Insights%20and%20Current%20Gaps%20in%20Open-Source%20LLM%20Vulnerability%20Scanners%3A%20A%0A%20%20Comparative%20Analysis&entry.906535625=Jonathan%20Brokman%20and%20Omer%20Hofman%20and%20Oren%20Rachmil%20and%20Inderjeet%20Singh%20and%20Rathina%20Sabapathy%20Aishvariya%20Priya%20and%20Vikas%20Pahuja%20and%20Amit%20Giloni%20and%20Roman%20Vainshtein%20and%20Hisashi%20Kojima&entry.1292438233=%20%20This%20report%20presents%20a%20comparative%20analysis%20of%20open-source%20vulnerability%0Ascanners%20for%20conversational%20large%20language%20models%20%28LLMs%29.%20As%20LLMs%20become%0Aintegral%20to%20various%20applications%2C%20they%20also%20present%20potential%20attack%20surfaces%2C%0Aexposed%20to%20security%20risks%20such%20as%20information%20leakage%20and%20jailbreak%20attacks.%0AOur%20study%20evaluates%20prominent%20scanners%20-%20Garak%2C%20Giskard%2C%20PyRIT%2C%20and%0ACyberSecEval%20-%20that%20adapt%20red-teaming%20practices%20to%20expose%20these%0Avulnerabilities.%20We%20detail%20the%20distinctive%20features%20and%20practical%20use%20of%20these%0Ascanners%2C%20outline%20unifying%20principles%20of%20their%20design%20and%20perform%20quantitative%0Aevaluations%20to%20compare%20them.%20These%20evaluations%20uncover%20significant%20reliability%0Aissues%20in%20detecting%20successful%20attacks%2C%20highlighting%20a%20fundamental%20gap%20for%0Afuture%20development.%20Additionally%2C%20we%20contribute%20a%20preliminary%20labelled%20dataset%2C%0Awhich%20serves%20as%20an%20initial%20step%20to%20bridge%20this%20gap.%20Based%20on%20the%20above%2C%20we%0Aprovide%20strategic%20recommendations%20to%20assist%20organizations%20choose%20the%20most%0Asuitable%20scanner%20for%20their%20red-teaming%20needs%2C%20accounting%20for%20customizability%2C%0Atest%20suite%20comprehensiveness%2C%20and%20industry-specific%20use%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16527v2&entry.124074799=Read"},
{"title": "Vikhr: Constructing a State-of-the-art Bilingual Open-Source\n  Instruction-Following Large Language Model for Russian", "author": "Aleksandr Nikolich and Konstantin Korolev and Sergei Bratchikov and Igor Kiselev and Artem Shelmanov", "abstract": "  There has been a surge in developing various Large Language Models (LLMs).\nHowever, text generation for languages other than English often faces\nsignificant challenges, including poor generation quality and reduced\ncomputational performance due to the disproportionate representation of tokens\nin the model's vocabulary. In this work, we address these issues by developing\na pipeline for adapting English-oriented pre-trained models to other languages\nand constructing efficient bilingual LLMs. Using this pipeline, we construct\nVikhr, a state-of-the-art bilingual open-source instruction-following LLM\ndesigned specifically for the Russian language. \"Vikhr\" refers to the name of\nthe Mistral LLM series and means a \"strong gust of wind.\" Unlike previous\nRussian-language models that typically rely on LoRA adapters on top of\nEnglish-oriented models, sacrificing performance for lower training costs,\nVikhr features an adapted tokenizer vocabulary and undergoes continued\npre-training and instruction tuning of all weights. This not only enhances the\nmodel's performance but also significantly improves its computational and\ncontextual efficiency. The remarkable performance of Vikhr across various\nRussian-language benchmarks can also be attributed to our efforts in expanding\ninstruction datasets and corpora for continued pre-training. Vikhr not only\nsets a new state of the art among open-source LLMs for Russian but even\noutperforms some proprietary closed-source models on certain benchmarks. The\nmodel weights, instruction sets, and code are publicly available.\n", "link": "http://arxiv.org/abs/2405.13929v4", "date": "2024-11-13", "relevancy": 1.8948, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4756}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4756}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4641}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vikhr%3A%20Constructing%20a%20State-of-the-art%20Bilingual%20Open-Source%0A%20%20Instruction-Following%20Large%20Language%20Model%20for%20Russian&body=Title%3A%20Vikhr%3A%20Constructing%20a%20State-of-the-art%20Bilingual%20Open-Source%0A%20%20Instruction-Following%20Large%20Language%20Model%20for%20Russian%0AAuthor%3A%20Aleksandr%20Nikolich%20and%20Konstantin%20Korolev%20and%20Sergei%20Bratchikov%20and%20Igor%20Kiselev%20and%20Artem%20Shelmanov%0AAbstract%3A%20%20%20There%20has%20been%20a%20surge%20in%20developing%20various%20Large%20Language%20Models%20%28LLMs%29.%0AHowever%2C%20text%20generation%20for%20languages%20other%20than%20English%20often%20faces%0Asignificant%20challenges%2C%20including%20poor%20generation%20quality%20and%20reduced%0Acomputational%20performance%20due%20to%20the%20disproportionate%20representation%20of%20tokens%0Ain%20the%20model%27s%20vocabulary.%20In%20this%20work%2C%20we%20address%20these%20issues%20by%20developing%0Aa%20pipeline%20for%20adapting%20English-oriented%20pre-trained%20models%20to%20other%20languages%0Aand%20constructing%20efficient%20bilingual%20LLMs.%20Using%20this%20pipeline%2C%20we%20construct%0AVikhr%2C%20a%20state-of-the-art%20bilingual%20open-source%20instruction-following%20LLM%0Adesigned%20specifically%20for%20the%20Russian%20language.%20%22Vikhr%22%20refers%20to%20the%20name%20of%0Athe%20Mistral%20LLM%20series%20and%20means%20a%20%22strong%20gust%20of%20wind.%22%20Unlike%20previous%0ARussian-language%20models%20that%20typically%20rely%20on%20LoRA%20adapters%20on%20top%20of%0AEnglish-oriented%20models%2C%20sacrificing%20performance%20for%20lower%20training%20costs%2C%0AVikhr%20features%20an%20adapted%20tokenizer%20vocabulary%20and%20undergoes%20continued%0Apre-training%20and%20instruction%20tuning%20of%20all%20weights.%20This%20not%20only%20enhances%20the%0Amodel%27s%20performance%20but%20also%20significantly%20improves%20its%20computational%20and%0Acontextual%20efficiency.%20The%20remarkable%20performance%20of%20Vikhr%20across%20various%0ARussian-language%20benchmarks%20can%20also%20be%20attributed%20to%20our%20efforts%20in%20expanding%0Ainstruction%20datasets%20and%20corpora%20for%20continued%20pre-training.%20Vikhr%20not%20only%0Asets%20a%20new%20state%20of%20the%20art%20among%20open-source%20LLMs%20for%20Russian%20but%20even%0Aoutperforms%20some%20proprietary%20closed-source%20models%20on%20certain%20benchmarks.%20The%0Amodel%20weights%2C%20instruction%20sets%2C%20and%20code%20are%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.13929v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVikhr%253A%2520Constructing%2520a%2520State-of-the-art%2520Bilingual%2520Open-Source%250A%2520%2520Instruction-Following%2520Large%2520Language%2520Model%2520for%2520Russian%26entry.906535625%3DAleksandr%2520Nikolich%2520and%2520Konstantin%2520Korolev%2520and%2520Sergei%2520Bratchikov%2520and%2520Igor%2520Kiselev%2520and%2520Artem%2520Shelmanov%26entry.1292438233%3D%2520%2520There%2520has%2520been%2520a%2520surge%2520in%2520developing%2520various%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%250AHowever%252C%2520text%2520generation%2520for%2520languages%2520other%2520than%2520English%2520often%2520faces%250Asignificant%2520challenges%252C%2520including%2520poor%2520generation%2520quality%2520and%2520reduced%250Acomputational%2520performance%2520due%2520to%2520the%2520disproportionate%2520representation%2520of%2520tokens%250Ain%2520the%2520model%2527s%2520vocabulary.%2520In%2520this%2520work%252C%2520we%2520address%2520these%2520issues%2520by%2520developing%250Aa%2520pipeline%2520for%2520adapting%2520English-oriented%2520pre-trained%2520models%2520to%2520other%2520languages%250Aand%2520constructing%2520efficient%2520bilingual%2520LLMs.%2520Using%2520this%2520pipeline%252C%2520we%2520construct%250AVikhr%252C%2520a%2520state-of-the-art%2520bilingual%2520open-source%2520instruction-following%2520LLM%250Adesigned%2520specifically%2520for%2520the%2520Russian%2520language.%2520%2522Vikhr%2522%2520refers%2520to%2520the%2520name%2520of%250Athe%2520Mistral%2520LLM%2520series%2520and%2520means%2520a%2520%2522strong%2520gust%2520of%2520wind.%2522%2520Unlike%2520previous%250ARussian-language%2520models%2520that%2520typically%2520rely%2520on%2520LoRA%2520adapters%2520on%2520top%2520of%250AEnglish-oriented%2520models%252C%2520sacrificing%2520performance%2520for%2520lower%2520training%2520costs%252C%250AVikhr%2520features%2520an%2520adapted%2520tokenizer%2520vocabulary%2520and%2520undergoes%2520continued%250Apre-training%2520and%2520instruction%2520tuning%2520of%2520all%2520weights.%2520This%2520not%2520only%2520enhances%2520the%250Amodel%2527s%2520performance%2520but%2520also%2520significantly%2520improves%2520its%2520computational%2520and%250Acontextual%2520efficiency.%2520The%2520remarkable%2520performance%2520of%2520Vikhr%2520across%2520various%250ARussian-language%2520benchmarks%2520can%2520also%2520be%2520attributed%2520to%2520our%2520efforts%2520in%2520expanding%250Ainstruction%2520datasets%2520and%2520corpora%2520for%2520continued%2520pre-training.%2520Vikhr%2520not%2520only%250Asets%2520a%2520new%2520state%2520of%2520the%2520art%2520among%2520open-source%2520LLMs%2520for%2520Russian%2520but%2520even%250Aoutperforms%2520some%2520proprietary%2520closed-source%2520models%2520on%2520certain%2520benchmarks.%2520The%250Amodel%2520weights%252C%2520instruction%2520sets%252C%2520and%2520code%2520are%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.13929v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vikhr%3A%20Constructing%20a%20State-of-the-art%20Bilingual%20Open-Source%0A%20%20Instruction-Following%20Large%20Language%20Model%20for%20Russian&entry.906535625=Aleksandr%20Nikolich%20and%20Konstantin%20Korolev%20and%20Sergei%20Bratchikov%20and%20Igor%20Kiselev%20and%20Artem%20Shelmanov&entry.1292438233=%20%20There%20has%20been%20a%20surge%20in%20developing%20various%20Large%20Language%20Models%20%28LLMs%29.%0AHowever%2C%20text%20generation%20for%20languages%20other%20than%20English%20often%20faces%0Asignificant%20challenges%2C%20including%20poor%20generation%20quality%20and%20reduced%0Acomputational%20performance%20due%20to%20the%20disproportionate%20representation%20of%20tokens%0Ain%20the%20model%27s%20vocabulary.%20In%20this%20work%2C%20we%20address%20these%20issues%20by%20developing%0Aa%20pipeline%20for%20adapting%20English-oriented%20pre-trained%20models%20to%20other%20languages%0Aand%20constructing%20efficient%20bilingual%20LLMs.%20Using%20this%20pipeline%2C%20we%20construct%0AVikhr%2C%20a%20state-of-the-art%20bilingual%20open-source%20instruction-following%20LLM%0Adesigned%20specifically%20for%20the%20Russian%20language.%20%22Vikhr%22%20refers%20to%20the%20name%20of%0Athe%20Mistral%20LLM%20series%20and%20means%20a%20%22strong%20gust%20of%20wind.%22%20Unlike%20previous%0ARussian-language%20models%20that%20typically%20rely%20on%20LoRA%20adapters%20on%20top%20of%0AEnglish-oriented%20models%2C%20sacrificing%20performance%20for%20lower%20training%20costs%2C%0AVikhr%20features%20an%20adapted%20tokenizer%20vocabulary%20and%20undergoes%20continued%0Apre-training%20and%20instruction%20tuning%20of%20all%20weights.%20This%20not%20only%20enhances%20the%0Amodel%27s%20performance%20but%20also%20significantly%20improves%20its%20computational%20and%0Acontextual%20efficiency.%20The%20remarkable%20performance%20of%20Vikhr%20across%20various%0ARussian-language%20benchmarks%20can%20also%20be%20attributed%20to%20our%20efforts%20in%20expanding%0Ainstruction%20datasets%20and%20corpora%20for%20continued%20pre-training.%20Vikhr%20not%20only%0Asets%20a%20new%20state%20of%20the%20art%20among%20open-source%20LLMs%20for%20Russian%20but%20even%0Aoutperforms%20some%20proprietary%20closed-source%20models%20on%20certain%20benchmarks.%20The%0Amodel%20weights%2C%20instruction%20sets%2C%20and%20code%20are%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.13929v4&entry.124074799=Read"},
{"title": "Impact of Iris Pigmentation on Performance Bias in Visible Iris\n  Verification Systems: A Comparative Study", "author": "Geetanjali Sharma and Abhishek Tandon and Gaurav Jaswal and Aditya Nigam and Raghavendra Ramachandra", "abstract": "  Iris recognition technology plays a critical role in biometric identification\nsystems, but their performance can be affected by variations in iris\npigmentation. In this work, we investigate the impact of iris pigmentation on\nthe efficacy of biometric recognition systems, focusing on a comparative\nanalysis of blue and dark irises. Data sets were collected using multiple\ndevices, including P1, P2, and P3 smartphones [4], to assess the robustness of\nthe systems in different capture environments [19]. Both traditional machine\nlearning techniques and deep learning models were used, namely Open-Iris,\nViT-b, and ResNet50, to evaluate performance metrics such as Equal Error Rate\n(EER) and True Match Rate (TMR). Our results indicate that iris recognition\nsystems generally exhibit higher accuracy for blue irises compared to dark\nirises. Furthermore, we examined the generalization capabilities of these\nsystems across different iris colors and devices, finding that while training\non diverse datasets enhances recognition performance, the degree of improvement\nis contingent on the specific model and device used. Our analysis also\nidentifies inherent biases in recognition performance related to iris color and\ncross-device variability. These findings underscore the need for more inclusive\ndataset collection and model refinement to reduce bias and promote equitable\nbiometric recognition across varying iris pigmentation and device\nconfigurations.\n", "link": "http://arxiv.org/abs/2411.08490v1", "date": "2024-11-13", "relevancy": 1.8925, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4906}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4668}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4582}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Impact%20of%20Iris%20Pigmentation%20on%20Performance%20Bias%20in%20Visible%20Iris%0A%20%20Verification%20Systems%3A%20A%20Comparative%20Study&body=Title%3A%20Impact%20of%20Iris%20Pigmentation%20on%20Performance%20Bias%20in%20Visible%20Iris%0A%20%20Verification%20Systems%3A%20A%20Comparative%20Study%0AAuthor%3A%20Geetanjali%20Sharma%20and%20Abhishek%20Tandon%20and%20Gaurav%20Jaswal%20and%20Aditya%20Nigam%20and%20Raghavendra%20Ramachandra%0AAbstract%3A%20%20%20Iris%20recognition%20technology%20plays%20a%20critical%20role%20in%20biometric%20identification%0Asystems%2C%20but%20their%20performance%20can%20be%20affected%20by%20variations%20in%20iris%0Apigmentation.%20In%20this%20work%2C%20we%20investigate%20the%20impact%20of%20iris%20pigmentation%20on%0Athe%20efficacy%20of%20biometric%20recognition%20systems%2C%20focusing%20on%20a%20comparative%0Aanalysis%20of%20blue%20and%20dark%20irises.%20Data%20sets%20were%20collected%20using%20multiple%0Adevices%2C%20including%20P1%2C%20P2%2C%20and%20P3%20smartphones%20%5B4%5D%2C%20to%20assess%20the%20robustness%20of%0Athe%20systems%20in%20different%20capture%20environments%20%5B19%5D.%20Both%20traditional%20machine%0Alearning%20techniques%20and%20deep%20learning%20models%20were%20used%2C%20namely%20Open-Iris%2C%0AViT-b%2C%20and%20ResNet50%2C%20to%20evaluate%20performance%20metrics%20such%20as%20Equal%20Error%20Rate%0A%28EER%29%20and%20True%20Match%20Rate%20%28TMR%29.%20Our%20results%20indicate%20that%20iris%20recognition%0Asystems%20generally%20exhibit%20higher%20accuracy%20for%20blue%20irises%20compared%20to%20dark%0Airises.%20Furthermore%2C%20we%20examined%20the%20generalization%20capabilities%20of%20these%0Asystems%20across%20different%20iris%20colors%20and%20devices%2C%20finding%20that%20while%20training%0Aon%20diverse%20datasets%20enhances%20recognition%20performance%2C%20the%20degree%20of%20improvement%0Ais%20contingent%20on%20the%20specific%20model%20and%20device%20used.%20Our%20analysis%20also%0Aidentifies%20inherent%20biases%20in%20recognition%20performance%20related%20to%20iris%20color%20and%0Across-device%20variability.%20These%20findings%20underscore%20the%20need%20for%20more%20inclusive%0Adataset%20collection%20and%20model%20refinement%20to%20reduce%20bias%20and%20promote%20equitable%0Abiometric%20recognition%20across%20varying%20iris%20pigmentation%20and%20device%0Aconfigurations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08490v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImpact%2520of%2520Iris%2520Pigmentation%2520on%2520Performance%2520Bias%2520in%2520Visible%2520Iris%250A%2520%2520Verification%2520Systems%253A%2520A%2520Comparative%2520Study%26entry.906535625%3DGeetanjali%2520Sharma%2520and%2520Abhishek%2520Tandon%2520and%2520Gaurav%2520Jaswal%2520and%2520Aditya%2520Nigam%2520and%2520Raghavendra%2520Ramachandra%26entry.1292438233%3D%2520%2520Iris%2520recognition%2520technology%2520plays%2520a%2520critical%2520role%2520in%2520biometric%2520identification%250Asystems%252C%2520but%2520their%2520performance%2520can%2520be%2520affected%2520by%2520variations%2520in%2520iris%250Apigmentation.%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%2520impact%2520of%2520iris%2520pigmentation%2520on%250Athe%2520efficacy%2520of%2520biometric%2520recognition%2520systems%252C%2520focusing%2520on%2520a%2520comparative%250Aanalysis%2520of%2520blue%2520and%2520dark%2520irises.%2520Data%2520sets%2520were%2520collected%2520using%2520multiple%250Adevices%252C%2520including%2520P1%252C%2520P2%252C%2520and%2520P3%2520smartphones%2520%255B4%255D%252C%2520to%2520assess%2520the%2520robustness%2520of%250Athe%2520systems%2520in%2520different%2520capture%2520environments%2520%255B19%255D.%2520Both%2520traditional%2520machine%250Alearning%2520techniques%2520and%2520deep%2520learning%2520models%2520were%2520used%252C%2520namely%2520Open-Iris%252C%250AViT-b%252C%2520and%2520ResNet50%252C%2520to%2520evaluate%2520performance%2520metrics%2520such%2520as%2520Equal%2520Error%2520Rate%250A%2528EER%2529%2520and%2520True%2520Match%2520Rate%2520%2528TMR%2529.%2520Our%2520results%2520indicate%2520that%2520iris%2520recognition%250Asystems%2520generally%2520exhibit%2520higher%2520accuracy%2520for%2520blue%2520irises%2520compared%2520to%2520dark%250Airises.%2520Furthermore%252C%2520we%2520examined%2520the%2520generalization%2520capabilities%2520of%2520these%250Asystems%2520across%2520different%2520iris%2520colors%2520and%2520devices%252C%2520finding%2520that%2520while%2520training%250Aon%2520diverse%2520datasets%2520enhances%2520recognition%2520performance%252C%2520the%2520degree%2520of%2520improvement%250Ais%2520contingent%2520on%2520the%2520specific%2520model%2520and%2520device%2520used.%2520Our%2520analysis%2520also%250Aidentifies%2520inherent%2520biases%2520in%2520recognition%2520performance%2520related%2520to%2520iris%2520color%2520and%250Across-device%2520variability.%2520These%2520findings%2520underscore%2520the%2520need%2520for%2520more%2520inclusive%250Adataset%2520collection%2520and%2520model%2520refinement%2520to%2520reduce%2520bias%2520and%2520promote%2520equitable%250Abiometric%2520recognition%2520across%2520varying%2520iris%2520pigmentation%2520and%2520device%250Aconfigurations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08490v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Impact%20of%20Iris%20Pigmentation%20on%20Performance%20Bias%20in%20Visible%20Iris%0A%20%20Verification%20Systems%3A%20A%20Comparative%20Study&entry.906535625=Geetanjali%20Sharma%20and%20Abhishek%20Tandon%20and%20Gaurav%20Jaswal%20and%20Aditya%20Nigam%20and%20Raghavendra%20Ramachandra&entry.1292438233=%20%20Iris%20recognition%20technology%20plays%20a%20critical%20role%20in%20biometric%20identification%0Asystems%2C%20but%20their%20performance%20can%20be%20affected%20by%20variations%20in%20iris%0Apigmentation.%20In%20this%20work%2C%20we%20investigate%20the%20impact%20of%20iris%20pigmentation%20on%0Athe%20efficacy%20of%20biometric%20recognition%20systems%2C%20focusing%20on%20a%20comparative%0Aanalysis%20of%20blue%20and%20dark%20irises.%20Data%20sets%20were%20collected%20using%20multiple%0Adevices%2C%20including%20P1%2C%20P2%2C%20and%20P3%20smartphones%20%5B4%5D%2C%20to%20assess%20the%20robustness%20of%0Athe%20systems%20in%20different%20capture%20environments%20%5B19%5D.%20Both%20traditional%20machine%0Alearning%20techniques%20and%20deep%20learning%20models%20were%20used%2C%20namely%20Open-Iris%2C%0AViT-b%2C%20and%20ResNet50%2C%20to%20evaluate%20performance%20metrics%20such%20as%20Equal%20Error%20Rate%0A%28EER%29%20and%20True%20Match%20Rate%20%28TMR%29.%20Our%20results%20indicate%20that%20iris%20recognition%0Asystems%20generally%20exhibit%20higher%20accuracy%20for%20blue%20irises%20compared%20to%20dark%0Airises.%20Furthermore%2C%20we%20examined%20the%20generalization%20capabilities%20of%20these%0Asystems%20across%20different%20iris%20colors%20and%20devices%2C%20finding%20that%20while%20training%0Aon%20diverse%20datasets%20enhances%20recognition%20performance%2C%20the%20degree%20of%20improvement%0Ais%20contingent%20on%20the%20specific%20model%20and%20device%20used.%20Our%20analysis%20also%0Aidentifies%20inherent%20biases%20in%20recognition%20performance%20related%20to%20iris%20color%20and%0Across-device%20variability.%20These%20findings%20underscore%20the%20need%20for%20more%20inclusive%0Adataset%20collection%20and%20model%20refinement%20to%20reduce%20bias%20and%20promote%20equitable%0Abiometric%20recognition%20across%20varying%20iris%20pigmentation%20and%20device%0Aconfigurations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08490v1&entry.124074799=Read"},
{"title": "Target-driven Attack for Large Language Models", "author": "Chong Zhang and Mingyu Jin and Dong Shu and Taowen Wang and Dongfang Liu and Xiaobo Jin", "abstract": "  Current large language models (LLM) provide a strong foundation for\nlarge-scale user-oriented natural language tasks. Many users can easily inject\nadversarial text or instructions through the user interface, thus causing LLM\nmodel security challenges like the language model not giving the correct\nanswer. Although there is currently a large amount of research on black-box\nattacks, most of these black-box attacks use random and heuristic strategies.\nIt is unclear how these strategies relate to the success rate of attacks and\nthus effectively improve model robustness. To solve this problem, we propose\nour target-driven black-box attack method to maximize the KL divergence between\nthe conditional probabilities of the clean text and the attack text to redefine\nthe attack's goal. We transform the distance maximization problem into two\nconvex optimization problems based on the attack goal to solve the attack text\nand estimate the covariance. Furthermore, the projected gradient descent\nalgorithm solves the vector corresponding to the attack text. Our target-driven\nblack-box attack approach includes two attack strategies: token manipulation\nand misinformation attack. Experimental results on multiple Large Language\nModels and datasets demonstrate the effectiveness of our attack method.\n", "link": "http://arxiv.org/abs/2411.07268v2", "date": "2024-11-13", "relevancy": 1.8917, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4734}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4734}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Target-driven%20Attack%20for%20Large%20Language%20Models&body=Title%3A%20Target-driven%20Attack%20for%20Large%20Language%20Models%0AAuthor%3A%20Chong%20Zhang%20and%20Mingyu%20Jin%20and%20Dong%20Shu%20and%20Taowen%20Wang%20and%20Dongfang%20Liu%20and%20Xiaobo%20Jin%0AAbstract%3A%20%20%20Current%20large%20language%20models%20%28LLM%29%20provide%20a%20strong%20foundation%20for%0Alarge-scale%20user-oriented%20natural%20language%20tasks.%20Many%20users%20can%20easily%20inject%0Aadversarial%20text%20or%20instructions%20through%20the%20user%20interface%2C%20thus%20causing%20LLM%0Amodel%20security%20challenges%20like%20the%20language%20model%20not%20giving%20the%20correct%0Aanswer.%20Although%20there%20is%20currently%20a%20large%20amount%20of%20research%20on%20black-box%0Aattacks%2C%20most%20of%20these%20black-box%20attacks%20use%20random%20and%20heuristic%20strategies.%0AIt%20is%20unclear%20how%20these%20strategies%20relate%20to%20the%20success%20rate%20of%20attacks%20and%0Athus%20effectively%20improve%20model%20robustness.%20To%20solve%20this%20problem%2C%20we%20propose%0Aour%20target-driven%20black-box%20attack%20method%20to%20maximize%20the%20KL%20divergence%20between%0Athe%20conditional%20probabilities%20of%20the%20clean%20text%20and%20the%20attack%20text%20to%20redefine%0Athe%20attack%27s%20goal.%20We%20transform%20the%20distance%20maximization%20problem%20into%20two%0Aconvex%20optimization%20problems%20based%20on%20the%20attack%20goal%20to%20solve%20the%20attack%20text%0Aand%20estimate%20the%20covariance.%20Furthermore%2C%20the%20projected%20gradient%20descent%0Aalgorithm%20solves%20the%20vector%20corresponding%20to%20the%20attack%20text.%20Our%20target-driven%0Ablack-box%20attack%20approach%20includes%20two%20attack%20strategies%3A%20token%20manipulation%0Aand%20misinformation%20attack.%20Experimental%20results%20on%20multiple%20Large%20Language%0AModels%20and%20datasets%20demonstrate%20the%20effectiveness%20of%20our%20attack%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.07268v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTarget-driven%2520Attack%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DChong%2520Zhang%2520and%2520Mingyu%2520Jin%2520and%2520Dong%2520Shu%2520and%2520Taowen%2520Wang%2520and%2520Dongfang%2520Liu%2520and%2520Xiaobo%2520Jin%26entry.1292438233%3D%2520%2520Current%2520large%2520language%2520models%2520%2528LLM%2529%2520provide%2520a%2520strong%2520foundation%2520for%250Alarge-scale%2520user-oriented%2520natural%2520language%2520tasks.%2520Many%2520users%2520can%2520easily%2520inject%250Aadversarial%2520text%2520or%2520instructions%2520through%2520the%2520user%2520interface%252C%2520thus%2520causing%2520LLM%250Amodel%2520security%2520challenges%2520like%2520the%2520language%2520model%2520not%2520giving%2520the%2520correct%250Aanswer.%2520Although%2520there%2520is%2520currently%2520a%2520large%2520amount%2520of%2520research%2520on%2520black-box%250Aattacks%252C%2520most%2520of%2520these%2520black-box%2520attacks%2520use%2520random%2520and%2520heuristic%2520strategies.%250AIt%2520is%2520unclear%2520how%2520these%2520strategies%2520relate%2520to%2520the%2520success%2520rate%2520of%2520attacks%2520and%250Athus%2520effectively%2520improve%2520model%2520robustness.%2520To%2520solve%2520this%2520problem%252C%2520we%2520propose%250Aour%2520target-driven%2520black-box%2520attack%2520method%2520to%2520maximize%2520the%2520KL%2520divergence%2520between%250Athe%2520conditional%2520probabilities%2520of%2520the%2520clean%2520text%2520and%2520the%2520attack%2520text%2520to%2520redefine%250Athe%2520attack%2527s%2520goal.%2520We%2520transform%2520the%2520distance%2520maximization%2520problem%2520into%2520two%250Aconvex%2520optimization%2520problems%2520based%2520on%2520the%2520attack%2520goal%2520to%2520solve%2520the%2520attack%2520text%250Aand%2520estimate%2520the%2520covariance.%2520Furthermore%252C%2520the%2520projected%2520gradient%2520descent%250Aalgorithm%2520solves%2520the%2520vector%2520corresponding%2520to%2520the%2520attack%2520text.%2520Our%2520target-driven%250Ablack-box%2520attack%2520approach%2520includes%2520two%2520attack%2520strategies%253A%2520token%2520manipulation%250Aand%2520misinformation%2520attack.%2520Experimental%2520results%2520on%2520multiple%2520Large%2520Language%250AModels%2520and%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520attack%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.07268v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Target-driven%20Attack%20for%20Large%20Language%20Models&entry.906535625=Chong%20Zhang%20and%20Mingyu%20Jin%20and%20Dong%20Shu%20and%20Taowen%20Wang%20and%20Dongfang%20Liu%20and%20Xiaobo%20Jin&entry.1292438233=%20%20Current%20large%20language%20models%20%28LLM%29%20provide%20a%20strong%20foundation%20for%0Alarge-scale%20user-oriented%20natural%20language%20tasks.%20Many%20users%20can%20easily%20inject%0Aadversarial%20text%20or%20instructions%20through%20the%20user%20interface%2C%20thus%20causing%20LLM%0Amodel%20security%20challenges%20like%20the%20language%20model%20not%20giving%20the%20correct%0Aanswer.%20Although%20there%20is%20currently%20a%20large%20amount%20of%20research%20on%20black-box%0Aattacks%2C%20most%20of%20these%20black-box%20attacks%20use%20random%20and%20heuristic%20strategies.%0AIt%20is%20unclear%20how%20these%20strategies%20relate%20to%20the%20success%20rate%20of%20attacks%20and%0Athus%20effectively%20improve%20model%20robustness.%20To%20solve%20this%20problem%2C%20we%20propose%0Aour%20target-driven%20black-box%20attack%20method%20to%20maximize%20the%20KL%20divergence%20between%0Athe%20conditional%20probabilities%20of%20the%20clean%20text%20and%20the%20attack%20text%20to%20redefine%0Athe%20attack%27s%20goal.%20We%20transform%20the%20distance%20maximization%20problem%20into%20two%0Aconvex%20optimization%20problems%20based%20on%20the%20attack%20goal%20to%20solve%20the%20attack%20text%0Aand%20estimate%20the%20covariance.%20Furthermore%2C%20the%20projected%20gradient%20descent%0Aalgorithm%20solves%20the%20vector%20corresponding%20to%20the%20attack%20text.%20Our%20target-driven%0Ablack-box%20attack%20approach%20includes%20two%20attack%20strategies%3A%20token%20manipulation%0Aand%20misinformation%20attack.%20Experimental%20results%20on%20multiple%20Large%20Language%0AModels%20and%20datasets%20demonstrate%20the%20effectiveness%20of%20our%20attack%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.07268v2&entry.124074799=Read"},
{"title": "AutoSAT: Automatically Optimize SAT Solvers via Large Language Models", "author": "Yiwen Sun and Furong Ye and Xianyin Zhang and Shiyu Huang and Bingzhen Zhang and Ke Wei and Shaowei Cai", "abstract": "  Conflict-Driven Clause Learning (CDCL) is the mainstream framework for\nsolving the Satisfiability problem (SAT), and CDCL solvers typically rely on\nvarious heuristics, which have a significant impact on their performance.\nModern CDCL solvers, such as MiniSat and Kissat, commonly incorporate several\nheuristics and select one to use according to simple rules, requiring\nsignificant time and expert effort to fine-tune in practice. The pervasion of\nLarge Language Models (LLMs) provides a potential solution to address this\nissue. However, generating a CDCL solver from scratch is not effective due to\nthe complexity and context volume of SAT solvers. Instead, we propose AutoSAT,\na framework that automatically optimizes heuristics in a pre-defined modular\nsearch space based on existing CDCL solvers. Unlike existing automated\nalgorithm design approaches focusing on hyperparameter tuning and operator\nselection, AutoSAT can generate new efficient heuristics. In this first attempt\nat optimizing SAT solvers using LLMs, several strategies including the greedy\nhill climber and (1+1) Evolutionary Algorithm are employed to guide LLMs to\nsearch for better heuristics. Experimental results demonstrate that LLMs can\ngenerally enhance the performance of CDCL solvers. A realization of AutoSAT\noutperforms MiniSat on 9 out of 12 datasets and even surpasses the\nstate-of-the-art hybrid solver Kissat on 4 datasets.\n", "link": "http://arxiv.org/abs/2402.10705v3", "date": "2024-11-13", "relevancy": 1.8901, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4739}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4732}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoSAT%3A%20Automatically%20Optimize%20SAT%20Solvers%20via%20Large%20Language%20Models&body=Title%3A%20AutoSAT%3A%20Automatically%20Optimize%20SAT%20Solvers%20via%20Large%20Language%20Models%0AAuthor%3A%20Yiwen%20Sun%20and%20Furong%20Ye%20and%20Xianyin%20Zhang%20and%20Shiyu%20Huang%20and%20Bingzhen%20Zhang%20and%20Ke%20Wei%20and%20Shaowei%20Cai%0AAbstract%3A%20%20%20Conflict-Driven%20Clause%20Learning%20%28CDCL%29%20is%20the%20mainstream%20framework%20for%0Asolving%20the%20Satisfiability%20problem%20%28SAT%29%2C%20and%20CDCL%20solvers%20typically%20rely%20on%0Avarious%20heuristics%2C%20which%20have%20a%20significant%20impact%20on%20their%20performance.%0AModern%20CDCL%20solvers%2C%20such%20as%20MiniSat%20and%20Kissat%2C%20commonly%20incorporate%20several%0Aheuristics%20and%20select%20one%20to%20use%20according%20to%20simple%20rules%2C%20requiring%0Asignificant%20time%20and%20expert%20effort%20to%20fine-tune%20in%20practice.%20The%20pervasion%20of%0ALarge%20Language%20Models%20%28LLMs%29%20provides%20a%20potential%20solution%20to%20address%20this%0Aissue.%20However%2C%20generating%20a%20CDCL%20solver%20from%20scratch%20is%20not%20effective%20due%20to%0Athe%20complexity%20and%20context%20volume%20of%20SAT%20solvers.%20Instead%2C%20we%20propose%20AutoSAT%2C%0Aa%20framework%20that%20automatically%20optimizes%20heuristics%20in%20a%20pre-defined%20modular%0Asearch%20space%20based%20on%20existing%20CDCL%20solvers.%20Unlike%20existing%20automated%0Aalgorithm%20design%20approaches%20focusing%20on%20hyperparameter%20tuning%20and%20operator%0Aselection%2C%20AutoSAT%20can%20generate%20new%20efficient%20heuristics.%20In%20this%20first%20attempt%0Aat%20optimizing%20SAT%20solvers%20using%20LLMs%2C%20several%20strategies%20including%20the%20greedy%0Ahill%20climber%20and%20%281%2B1%29%20Evolutionary%20Algorithm%20are%20employed%20to%20guide%20LLMs%20to%0Asearch%20for%20better%20heuristics.%20Experimental%20results%20demonstrate%20that%20LLMs%20can%0Agenerally%20enhance%20the%20performance%20of%20CDCL%20solvers.%20A%20realization%20of%20AutoSAT%0Aoutperforms%20MiniSat%20on%209%20out%20of%2012%20datasets%20and%20even%20surpasses%20the%0Astate-of-the-art%20hybrid%20solver%20Kissat%20on%204%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.10705v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoSAT%253A%2520Automatically%2520Optimize%2520SAT%2520Solvers%2520via%2520Large%2520Language%2520Models%26entry.906535625%3DYiwen%2520Sun%2520and%2520Furong%2520Ye%2520and%2520Xianyin%2520Zhang%2520and%2520Shiyu%2520Huang%2520and%2520Bingzhen%2520Zhang%2520and%2520Ke%2520Wei%2520and%2520Shaowei%2520Cai%26entry.1292438233%3D%2520%2520Conflict-Driven%2520Clause%2520Learning%2520%2528CDCL%2529%2520is%2520the%2520mainstream%2520framework%2520for%250Asolving%2520the%2520Satisfiability%2520problem%2520%2528SAT%2529%252C%2520and%2520CDCL%2520solvers%2520typically%2520rely%2520on%250Avarious%2520heuristics%252C%2520which%2520have%2520a%2520significant%2520impact%2520on%2520their%2520performance.%250AModern%2520CDCL%2520solvers%252C%2520such%2520as%2520MiniSat%2520and%2520Kissat%252C%2520commonly%2520incorporate%2520several%250Aheuristics%2520and%2520select%2520one%2520to%2520use%2520according%2520to%2520simple%2520rules%252C%2520requiring%250Asignificant%2520time%2520and%2520expert%2520effort%2520to%2520fine-tune%2520in%2520practice.%2520The%2520pervasion%2520of%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%2520provides%2520a%2520potential%2520solution%2520to%2520address%2520this%250Aissue.%2520However%252C%2520generating%2520a%2520CDCL%2520solver%2520from%2520scratch%2520is%2520not%2520effective%2520due%2520to%250Athe%2520complexity%2520and%2520context%2520volume%2520of%2520SAT%2520solvers.%2520Instead%252C%2520we%2520propose%2520AutoSAT%252C%250Aa%2520framework%2520that%2520automatically%2520optimizes%2520heuristics%2520in%2520a%2520pre-defined%2520modular%250Asearch%2520space%2520based%2520on%2520existing%2520CDCL%2520solvers.%2520Unlike%2520existing%2520automated%250Aalgorithm%2520design%2520approaches%2520focusing%2520on%2520hyperparameter%2520tuning%2520and%2520operator%250Aselection%252C%2520AutoSAT%2520can%2520generate%2520new%2520efficient%2520heuristics.%2520In%2520this%2520first%2520attempt%250Aat%2520optimizing%2520SAT%2520solvers%2520using%2520LLMs%252C%2520several%2520strategies%2520including%2520the%2520greedy%250Ahill%2520climber%2520and%2520%25281%252B1%2529%2520Evolutionary%2520Algorithm%2520are%2520employed%2520to%2520guide%2520LLMs%2520to%250Asearch%2520for%2520better%2520heuristics.%2520Experimental%2520results%2520demonstrate%2520that%2520LLMs%2520can%250Agenerally%2520enhance%2520the%2520performance%2520of%2520CDCL%2520solvers.%2520A%2520realization%2520of%2520AutoSAT%250Aoutperforms%2520MiniSat%2520on%25209%2520out%2520of%252012%2520datasets%2520and%2520even%2520surpasses%2520the%250Astate-of-the-art%2520hybrid%2520solver%2520Kissat%2520on%25204%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10705v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoSAT%3A%20Automatically%20Optimize%20SAT%20Solvers%20via%20Large%20Language%20Models&entry.906535625=Yiwen%20Sun%20and%20Furong%20Ye%20and%20Xianyin%20Zhang%20and%20Shiyu%20Huang%20and%20Bingzhen%20Zhang%20and%20Ke%20Wei%20and%20Shaowei%20Cai&entry.1292438233=%20%20Conflict-Driven%20Clause%20Learning%20%28CDCL%29%20is%20the%20mainstream%20framework%20for%0Asolving%20the%20Satisfiability%20problem%20%28SAT%29%2C%20and%20CDCL%20solvers%20typically%20rely%20on%0Avarious%20heuristics%2C%20which%20have%20a%20significant%20impact%20on%20their%20performance.%0AModern%20CDCL%20solvers%2C%20such%20as%20MiniSat%20and%20Kissat%2C%20commonly%20incorporate%20several%0Aheuristics%20and%20select%20one%20to%20use%20according%20to%20simple%20rules%2C%20requiring%0Asignificant%20time%20and%20expert%20effort%20to%20fine-tune%20in%20practice.%20The%20pervasion%20of%0ALarge%20Language%20Models%20%28LLMs%29%20provides%20a%20potential%20solution%20to%20address%20this%0Aissue.%20However%2C%20generating%20a%20CDCL%20solver%20from%20scratch%20is%20not%20effective%20due%20to%0Athe%20complexity%20and%20context%20volume%20of%20SAT%20solvers.%20Instead%2C%20we%20propose%20AutoSAT%2C%0Aa%20framework%20that%20automatically%20optimizes%20heuristics%20in%20a%20pre-defined%20modular%0Asearch%20space%20based%20on%20existing%20CDCL%20solvers.%20Unlike%20existing%20automated%0Aalgorithm%20design%20approaches%20focusing%20on%20hyperparameter%20tuning%20and%20operator%0Aselection%2C%20AutoSAT%20can%20generate%20new%20efficient%20heuristics.%20In%20this%20first%20attempt%0Aat%20optimizing%20SAT%20solvers%20using%20LLMs%2C%20several%20strategies%20including%20the%20greedy%0Ahill%20climber%20and%20%281%2B1%29%20Evolutionary%20Algorithm%20are%20employed%20to%20guide%20LLMs%20to%0Asearch%20for%20better%20heuristics.%20Experimental%20results%20demonstrate%20that%20LLMs%20can%0Agenerally%20enhance%20the%20performance%20of%20CDCL%20solvers.%20A%20realization%20of%20AutoSAT%0Aoutperforms%20MiniSat%20on%209%20out%20of%2012%20datasets%20and%20even%20surpasses%20the%0Astate-of-the-art%20hybrid%20solver%20Kissat%20on%204%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.10705v3&entry.124074799=Read"},
{"title": "Gradient Normalization Provably Benefits Nonconvex SGD under\n  Heavy-Tailed Noise", "author": "Tao Sun and Xinwang Liu and Kun Yuan", "abstract": "  This paper investigates the roles of gradient normalization and clipping in\nensuring the convergence of Stochastic Gradient Descent (SGD) under\nheavy-tailed noise. While existing approaches consider gradient clipping\nindispensable for SGD convergence, we theoretically demonstrate that gradient\nnormalization alone without clipping is sufficient to ensure convergence.\nFurthermore, we establish that combining gradient normalization with clipping\noffers significantly improved convergence rates compared to using either\ntechnique in isolation, particularly as gradient noise diminishes. With these\nresults, our work provides the first theoretical evidence demonstrating the\nbenefits of gradient normalization in SGD under heavy-tailed noise. Finally, we\nintroduce an accelerated SGD variant that incorporates both gradient\nnormalization and clipping, further enhancing convergence rates under\nheavy-tailed noise.\n", "link": "http://arxiv.org/abs/2410.16561v2", "date": "2024-11-13", "relevancy": 1.8849, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4744}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4693}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gradient%20Normalization%20Provably%20Benefits%20Nonconvex%20SGD%20under%0A%20%20Heavy-Tailed%20Noise&body=Title%3A%20Gradient%20Normalization%20Provably%20Benefits%20Nonconvex%20SGD%20under%0A%20%20Heavy-Tailed%20Noise%0AAuthor%3A%20Tao%20Sun%20and%20Xinwang%20Liu%20and%20Kun%20Yuan%0AAbstract%3A%20%20%20This%20paper%20investigates%20the%20roles%20of%20gradient%20normalization%20and%20clipping%20in%0Aensuring%20the%20convergence%20of%20Stochastic%20Gradient%20Descent%20%28SGD%29%20under%0Aheavy-tailed%20noise.%20While%20existing%20approaches%20consider%20gradient%20clipping%0Aindispensable%20for%20SGD%20convergence%2C%20we%20theoretically%20demonstrate%20that%20gradient%0Anormalization%20alone%20without%20clipping%20is%20sufficient%20to%20ensure%20convergence.%0AFurthermore%2C%20we%20establish%20that%20combining%20gradient%20normalization%20with%20clipping%0Aoffers%20significantly%20improved%20convergence%20rates%20compared%20to%20using%20either%0Atechnique%20in%20isolation%2C%20particularly%20as%20gradient%20noise%20diminishes.%20With%20these%0Aresults%2C%20our%20work%20provides%20the%20first%20theoretical%20evidence%20demonstrating%20the%0Abenefits%20of%20gradient%20normalization%20in%20SGD%20under%20heavy-tailed%20noise.%20Finally%2C%20we%0Aintroduce%20an%20accelerated%20SGD%20variant%20that%20incorporates%20both%20gradient%0Anormalization%20and%20clipping%2C%20further%20enhancing%20convergence%20rates%20under%0Aheavy-tailed%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.16561v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGradient%2520Normalization%2520Provably%2520Benefits%2520Nonconvex%2520SGD%2520under%250A%2520%2520Heavy-Tailed%2520Noise%26entry.906535625%3DTao%2520Sun%2520and%2520Xinwang%2520Liu%2520and%2520Kun%2520Yuan%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520the%2520roles%2520of%2520gradient%2520normalization%2520and%2520clipping%2520in%250Aensuring%2520the%2520convergence%2520of%2520Stochastic%2520Gradient%2520Descent%2520%2528SGD%2529%2520under%250Aheavy-tailed%2520noise.%2520While%2520existing%2520approaches%2520consider%2520gradient%2520clipping%250Aindispensable%2520for%2520SGD%2520convergence%252C%2520we%2520theoretically%2520demonstrate%2520that%2520gradient%250Anormalization%2520alone%2520without%2520clipping%2520is%2520sufficient%2520to%2520ensure%2520convergence.%250AFurthermore%252C%2520we%2520establish%2520that%2520combining%2520gradient%2520normalization%2520with%2520clipping%250Aoffers%2520significantly%2520improved%2520convergence%2520rates%2520compared%2520to%2520using%2520either%250Atechnique%2520in%2520isolation%252C%2520particularly%2520as%2520gradient%2520noise%2520diminishes.%2520With%2520these%250Aresults%252C%2520our%2520work%2520provides%2520the%2520first%2520theoretical%2520evidence%2520demonstrating%2520the%250Abenefits%2520of%2520gradient%2520normalization%2520in%2520SGD%2520under%2520heavy-tailed%2520noise.%2520Finally%252C%2520we%250Aintroduce%2520an%2520accelerated%2520SGD%2520variant%2520that%2520incorporates%2520both%2520gradient%250Anormalization%2520and%2520clipping%252C%2520further%2520enhancing%2520convergence%2520rates%2520under%250Aheavy-tailed%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.16561v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient%20Normalization%20Provably%20Benefits%20Nonconvex%20SGD%20under%0A%20%20Heavy-Tailed%20Noise&entry.906535625=Tao%20Sun%20and%20Xinwang%20Liu%20and%20Kun%20Yuan&entry.1292438233=%20%20This%20paper%20investigates%20the%20roles%20of%20gradient%20normalization%20and%20clipping%20in%0Aensuring%20the%20convergence%20of%20Stochastic%20Gradient%20Descent%20%28SGD%29%20under%0Aheavy-tailed%20noise.%20While%20existing%20approaches%20consider%20gradient%20clipping%0Aindispensable%20for%20SGD%20convergence%2C%20we%20theoretically%20demonstrate%20that%20gradient%0Anormalization%20alone%20without%20clipping%20is%20sufficient%20to%20ensure%20convergence.%0AFurthermore%2C%20we%20establish%20that%20combining%20gradient%20normalization%20with%20clipping%0Aoffers%20significantly%20improved%20convergence%20rates%20compared%20to%20using%20either%0Atechnique%20in%20isolation%2C%20particularly%20as%20gradient%20noise%20diminishes.%20With%20these%0Aresults%2C%20our%20work%20provides%20the%20first%20theoretical%20evidence%20demonstrating%20the%0Abenefits%20of%20gradient%20normalization%20in%20SGD%20under%20heavy-tailed%20noise.%20Finally%2C%20we%0Aintroduce%20an%20accelerated%20SGD%20variant%20that%20incorporates%20both%20gradient%0Anormalization%20and%20clipping%2C%20further%20enhancing%20convergence%20rates%20under%0Aheavy-tailed%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.16561v2&entry.124074799=Read"},
{"title": "Explainers' Mental Representations of Explainees' Needs in Everyday\n  Explanations", "author": "Michael Erol Schaffer and Lutz Terfloth and Carsten Schulte and Heike M. Buhl", "abstract": "  In explanations, explainers have mental representations of explainees'\ndeveloping knowledge and shifting interests regarding the explanandum. These\nmental representations are dynamic in nature and develop over time, thereby\nenabling explainers to react to explainees' needs by adapting and customizing\nthe explanation. XAI should be able to react to explainees' needs in a similar\nmanner. Therefore, a component that incorporates aspects of explainers' mental\nrepresentations of explainees is required. In this study, we took first steps\nby investigating explainers' mental representations in everyday explanations of\ntechnological artifacts. According to the dual nature theory, technological\nartifacts require explanations with two distinct perspectives, namely\nobservable and measurable features addressing \"Architecture\" or interpretable\naspects addressing \"Relevance\". We conducted extended semi structured pre-,\npost- and video recall-interviews with explainers (N=9) in the context of an\nexplanation. The transcribed interviews were analyzed utilizing qualitative\ncontent analysis. The explainers' answers regarding the explainees' knowledge\nand interests with regard to the technological artifact emphasized the\nvagueness of early assumptions of explainers toward strong beliefs in the\ncourse of explanations. The assumed knowledge of explainees in the beginning is\ncentered around Architecture and develops toward knowledge with regard to both\nArchitecture and Relevance. In contrast, explainers assumed higher interests in\nRelevance in the beginning to interests regarding both Architecture and\nRelevance in the further course of explanations. Further, explainers often\nfinished the explanation despite their perception that explainees still had\ngaps in knowledge. These findings are transferred into practical implications\nrelevant for user models for adaptive explainable systems.\n", "link": "http://arxiv.org/abs/2411.08514v1", "date": "2024-11-13", "relevancy": 1.8806, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5058}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.463}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explainers%27%20Mental%20Representations%20of%20Explainees%27%20Needs%20in%20Everyday%0A%20%20Explanations&body=Title%3A%20Explainers%27%20Mental%20Representations%20of%20Explainees%27%20Needs%20in%20Everyday%0A%20%20Explanations%0AAuthor%3A%20Michael%20Erol%20Schaffer%20and%20Lutz%20Terfloth%20and%20Carsten%20Schulte%20and%20Heike%20M.%20Buhl%0AAbstract%3A%20%20%20In%20explanations%2C%20explainers%20have%20mental%20representations%20of%20explainees%27%0Adeveloping%20knowledge%20and%20shifting%20interests%20regarding%20the%20explanandum.%20These%0Amental%20representations%20are%20dynamic%20in%20nature%20and%20develop%20over%20time%2C%20thereby%0Aenabling%20explainers%20to%20react%20to%20explainees%27%20needs%20by%20adapting%20and%20customizing%0Athe%20explanation.%20XAI%20should%20be%20able%20to%20react%20to%20explainees%27%20needs%20in%20a%20similar%0Amanner.%20Therefore%2C%20a%20component%20that%20incorporates%20aspects%20of%20explainers%27%20mental%0Arepresentations%20of%20explainees%20is%20required.%20In%20this%20study%2C%20we%20took%20first%20steps%0Aby%20investigating%20explainers%27%20mental%20representations%20in%20everyday%20explanations%20of%0Atechnological%20artifacts.%20According%20to%20the%20dual%20nature%20theory%2C%20technological%0Aartifacts%20require%20explanations%20with%20two%20distinct%20perspectives%2C%20namely%0Aobservable%20and%20measurable%20features%20addressing%20%22Architecture%22%20or%20interpretable%0Aaspects%20addressing%20%22Relevance%22.%20We%20conducted%20extended%20semi%20structured%20pre-%2C%0Apost-%20and%20video%20recall-interviews%20with%20explainers%20%28N%3D9%29%20in%20the%20context%20of%20an%0Aexplanation.%20The%20transcribed%20interviews%20were%20analyzed%20utilizing%20qualitative%0Acontent%20analysis.%20The%20explainers%27%20answers%20regarding%20the%20explainees%27%20knowledge%0Aand%20interests%20with%20regard%20to%20the%20technological%20artifact%20emphasized%20the%0Avagueness%20of%20early%20assumptions%20of%20explainers%20toward%20strong%20beliefs%20in%20the%0Acourse%20of%20explanations.%20The%20assumed%20knowledge%20of%20explainees%20in%20the%20beginning%20is%0Acentered%20around%20Architecture%20and%20develops%20toward%20knowledge%20with%20regard%20to%20both%0AArchitecture%20and%20Relevance.%20In%20contrast%2C%20explainers%20assumed%20higher%20interests%20in%0ARelevance%20in%20the%20beginning%20to%20interests%20regarding%20both%20Architecture%20and%0ARelevance%20in%20the%20further%20course%20of%20explanations.%20Further%2C%20explainers%20often%0Afinished%20the%20explanation%20despite%20their%20perception%20that%20explainees%20still%20had%0Agaps%20in%20knowledge.%20These%20findings%20are%20transferred%20into%20practical%20implications%0Arelevant%20for%20user%20models%20for%20adaptive%20explainable%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08514v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplainers%2527%2520Mental%2520Representations%2520of%2520Explainees%2527%2520Needs%2520in%2520Everyday%250A%2520%2520Explanations%26entry.906535625%3DMichael%2520Erol%2520Schaffer%2520and%2520Lutz%2520Terfloth%2520and%2520Carsten%2520Schulte%2520and%2520Heike%2520M.%2520Buhl%26entry.1292438233%3D%2520%2520In%2520explanations%252C%2520explainers%2520have%2520mental%2520representations%2520of%2520explainees%2527%250Adeveloping%2520knowledge%2520and%2520shifting%2520interests%2520regarding%2520the%2520explanandum.%2520These%250Amental%2520representations%2520are%2520dynamic%2520in%2520nature%2520and%2520develop%2520over%2520time%252C%2520thereby%250Aenabling%2520explainers%2520to%2520react%2520to%2520explainees%2527%2520needs%2520by%2520adapting%2520and%2520customizing%250Athe%2520explanation.%2520XAI%2520should%2520be%2520able%2520to%2520react%2520to%2520explainees%2527%2520needs%2520in%2520a%2520similar%250Amanner.%2520Therefore%252C%2520a%2520component%2520that%2520incorporates%2520aspects%2520of%2520explainers%2527%2520mental%250Arepresentations%2520of%2520explainees%2520is%2520required.%2520In%2520this%2520study%252C%2520we%2520took%2520first%2520steps%250Aby%2520investigating%2520explainers%2527%2520mental%2520representations%2520in%2520everyday%2520explanations%2520of%250Atechnological%2520artifacts.%2520According%2520to%2520the%2520dual%2520nature%2520theory%252C%2520technological%250Aartifacts%2520require%2520explanations%2520with%2520two%2520distinct%2520perspectives%252C%2520namely%250Aobservable%2520and%2520measurable%2520features%2520addressing%2520%2522Architecture%2522%2520or%2520interpretable%250Aaspects%2520addressing%2520%2522Relevance%2522.%2520We%2520conducted%2520extended%2520semi%2520structured%2520pre-%252C%250Apost-%2520and%2520video%2520recall-interviews%2520with%2520explainers%2520%2528N%253D9%2529%2520in%2520the%2520context%2520of%2520an%250Aexplanation.%2520The%2520transcribed%2520interviews%2520were%2520analyzed%2520utilizing%2520qualitative%250Acontent%2520analysis.%2520The%2520explainers%2527%2520answers%2520regarding%2520the%2520explainees%2527%2520knowledge%250Aand%2520interests%2520with%2520regard%2520to%2520the%2520technological%2520artifact%2520emphasized%2520the%250Avagueness%2520of%2520early%2520assumptions%2520of%2520explainers%2520toward%2520strong%2520beliefs%2520in%2520the%250Acourse%2520of%2520explanations.%2520The%2520assumed%2520knowledge%2520of%2520explainees%2520in%2520the%2520beginning%2520is%250Acentered%2520around%2520Architecture%2520and%2520develops%2520toward%2520knowledge%2520with%2520regard%2520to%2520both%250AArchitecture%2520and%2520Relevance.%2520In%2520contrast%252C%2520explainers%2520assumed%2520higher%2520interests%2520in%250ARelevance%2520in%2520the%2520beginning%2520to%2520interests%2520regarding%2520both%2520Architecture%2520and%250ARelevance%2520in%2520the%2520further%2520course%2520of%2520explanations.%2520Further%252C%2520explainers%2520often%250Afinished%2520the%2520explanation%2520despite%2520their%2520perception%2520that%2520explainees%2520still%2520had%250Agaps%2520in%2520knowledge.%2520These%2520findings%2520are%2520transferred%2520into%2520practical%2520implications%250Arelevant%2520for%2520user%2520models%2520for%2520adaptive%2520explainable%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08514v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explainers%27%20Mental%20Representations%20of%20Explainees%27%20Needs%20in%20Everyday%0A%20%20Explanations&entry.906535625=Michael%20Erol%20Schaffer%20and%20Lutz%20Terfloth%20and%20Carsten%20Schulte%20and%20Heike%20M.%20Buhl&entry.1292438233=%20%20In%20explanations%2C%20explainers%20have%20mental%20representations%20of%20explainees%27%0Adeveloping%20knowledge%20and%20shifting%20interests%20regarding%20the%20explanandum.%20These%0Amental%20representations%20are%20dynamic%20in%20nature%20and%20develop%20over%20time%2C%20thereby%0Aenabling%20explainers%20to%20react%20to%20explainees%27%20needs%20by%20adapting%20and%20customizing%0Athe%20explanation.%20XAI%20should%20be%20able%20to%20react%20to%20explainees%27%20needs%20in%20a%20similar%0Amanner.%20Therefore%2C%20a%20component%20that%20incorporates%20aspects%20of%20explainers%27%20mental%0Arepresentations%20of%20explainees%20is%20required.%20In%20this%20study%2C%20we%20took%20first%20steps%0Aby%20investigating%20explainers%27%20mental%20representations%20in%20everyday%20explanations%20of%0Atechnological%20artifacts.%20According%20to%20the%20dual%20nature%20theory%2C%20technological%0Aartifacts%20require%20explanations%20with%20two%20distinct%20perspectives%2C%20namely%0Aobservable%20and%20measurable%20features%20addressing%20%22Architecture%22%20or%20interpretable%0Aaspects%20addressing%20%22Relevance%22.%20We%20conducted%20extended%20semi%20structured%20pre-%2C%0Apost-%20and%20video%20recall-interviews%20with%20explainers%20%28N%3D9%29%20in%20the%20context%20of%20an%0Aexplanation.%20The%20transcribed%20interviews%20were%20analyzed%20utilizing%20qualitative%0Acontent%20analysis.%20The%20explainers%27%20answers%20regarding%20the%20explainees%27%20knowledge%0Aand%20interests%20with%20regard%20to%20the%20technological%20artifact%20emphasized%20the%0Avagueness%20of%20early%20assumptions%20of%20explainers%20toward%20strong%20beliefs%20in%20the%0Acourse%20of%20explanations.%20The%20assumed%20knowledge%20of%20explainees%20in%20the%20beginning%20is%0Acentered%20around%20Architecture%20and%20develops%20toward%20knowledge%20with%20regard%20to%20both%0AArchitecture%20and%20Relevance.%20In%20contrast%2C%20explainers%20assumed%20higher%20interests%20in%0ARelevance%20in%20the%20beginning%20to%20interests%20regarding%20both%20Architecture%20and%0ARelevance%20in%20the%20further%20course%20of%20explanations.%20Further%2C%20explainers%20often%0Afinished%20the%20explanation%20despite%20their%20perception%20that%20explainees%20still%20had%0Agaps%20in%20knowledge.%20These%20findings%20are%20transferred%20into%20practical%20implications%0Arelevant%20for%20user%20models%20for%20adaptive%20explainable%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08514v1&entry.124074799=Read"},
{"title": "Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting\n  Volunteer Content Moderators", "author": "Yang Trista Cao and Lovely-Frances Domingo and Sarah Ann Gilbert and Michelle Mazurek and Katie Shilton and Hal Daum\u00e9 III", "abstract": "  Extensive efforts in automated approaches for content moderation have been\nfocused on developing models to identify toxic, offensive, and hateful content\nwith the aim of lightening the load for moderators. Yet, it remains uncertain\nwhether improvements on those tasks have truly addressed moderators' needs in\naccomplishing their work. In this paper, we surface gaps between past research\nefforts that have aimed to provide automation for aspects of content moderation\nand the needs of volunteer content moderators, regarding identifying violations\nof various moderation rules. To do so, we conduct a model review on Hugging\nFace to reveal the availability of models to cover various moderation rules and\nguidelines from three exemplar forums. We further put state-of-the-art LLMs to\nthe test, evaluating how well these models perform in flagging violations of\nplatform rules from one particular forum. Finally, we conduct a user survey\nstudy with volunteer moderators to gain insight into their perspectives on\nuseful moderation models. Overall, we observe a non-trivial gap, as missing\ndeveloped models and LLMs exhibit moderate to low performance on a significant\nportion of the rules. Moderators' reports provide guides for future work on\ndeveloping moderation assistant models.\n", "link": "http://arxiv.org/abs/2311.07879v4", "date": "2024-11-13", "relevancy": 1.8789, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4735}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4735}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toxicity%20Detection%20is%20NOT%20all%20you%20Need%3A%20Measuring%20the%20Gaps%20to%20Supporting%0A%20%20Volunteer%20Content%20Moderators&body=Title%3A%20Toxicity%20Detection%20is%20NOT%20all%20you%20Need%3A%20Measuring%20the%20Gaps%20to%20Supporting%0A%20%20Volunteer%20Content%20Moderators%0AAuthor%3A%20Yang%20Trista%20Cao%20and%20Lovely-Frances%20Domingo%20and%20Sarah%20Ann%20Gilbert%20and%20Michelle%20Mazurek%20and%20Katie%20Shilton%20and%20Hal%20Daum%C3%A9%20III%0AAbstract%3A%20%20%20Extensive%20efforts%20in%20automated%20approaches%20for%20content%20moderation%20have%20been%0Afocused%20on%20developing%20models%20to%20identify%20toxic%2C%20offensive%2C%20and%20hateful%20content%0Awith%20the%20aim%20of%20lightening%20the%20load%20for%20moderators.%20Yet%2C%20it%20remains%20uncertain%0Awhether%20improvements%20on%20those%20tasks%20have%20truly%20addressed%20moderators%27%20needs%20in%0Aaccomplishing%20their%20work.%20In%20this%20paper%2C%20we%20surface%20gaps%20between%20past%20research%0Aefforts%20that%20have%20aimed%20to%20provide%20automation%20for%20aspects%20of%20content%20moderation%0Aand%20the%20needs%20of%20volunteer%20content%20moderators%2C%20regarding%20identifying%20violations%0Aof%20various%20moderation%20rules.%20To%20do%20so%2C%20we%20conduct%20a%20model%20review%20on%20Hugging%0AFace%20to%20reveal%20the%20availability%20of%20models%20to%20cover%20various%20moderation%20rules%20and%0Aguidelines%20from%20three%20exemplar%20forums.%20We%20further%20put%20state-of-the-art%20LLMs%20to%0Athe%20test%2C%20evaluating%20how%20well%20these%20models%20perform%20in%20flagging%20violations%20of%0Aplatform%20rules%20from%20one%20particular%20forum.%20Finally%2C%20we%20conduct%20a%20user%20survey%0Astudy%20with%20volunteer%20moderators%20to%20gain%20insight%20into%20their%20perspectives%20on%0Auseful%20moderation%20models.%20Overall%2C%20we%20observe%20a%20non-trivial%20gap%2C%20as%20missing%0Adeveloped%20models%20and%20LLMs%20exhibit%20moderate%20to%20low%20performance%20on%20a%20significant%0Aportion%20of%20the%20rules.%20Moderators%27%20reports%20provide%20guides%20for%20future%20work%20on%0Adeveloping%20moderation%20assistant%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.07879v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToxicity%2520Detection%2520is%2520NOT%2520all%2520you%2520Need%253A%2520Measuring%2520the%2520Gaps%2520to%2520Supporting%250A%2520%2520Volunteer%2520Content%2520Moderators%26entry.906535625%3DYang%2520Trista%2520Cao%2520and%2520Lovely-Frances%2520Domingo%2520and%2520Sarah%2520Ann%2520Gilbert%2520and%2520Michelle%2520Mazurek%2520and%2520Katie%2520Shilton%2520and%2520Hal%2520Daum%25C3%25A9%2520III%26entry.1292438233%3D%2520%2520Extensive%2520efforts%2520in%2520automated%2520approaches%2520for%2520content%2520moderation%2520have%2520been%250Afocused%2520on%2520developing%2520models%2520to%2520identify%2520toxic%252C%2520offensive%252C%2520and%2520hateful%2520content%250Awith%2520the%2520aim%2520of%2520lightening%2520the%2520load%2520for%2520moderators.%2520Yet%252C%2520it%2520remains%2520uncertain%250Awhether%2520improvements%2520on%2520those%2520tasks%2520have%2520truly%2520addressed%2520moderators%2527%2520needs%2520in%250Aaccomplishing%2520their%2520work.%2520In%2520this%2520paper%252C%2520we%2520surface%2520gaps%2520between%2520past%2520research%250Aefforts%2520that%2520have%2520aimed%2520to%2520provide%2520automation%2520for%2520aspects%2520of%2520content%2520moderation%250Aand%2520the%2520needs%2520of%2520volunteer%2520content%2520moderators%252C%2520regarding%2520identifying%2520violations%250Aof%2520various%2520moderation%2520rules.%2520To%2520do%2520so%252C%2520we%2520conduct%2520a%2520model%2520review%2520on%2520Hugging%250AFace%2520to%2520reveal%2520the%2520availability%2520of%2520models%2520to%2520cover%2520various%2520moderation%2520rules%2520and%250Aguidelines%2520from%2520three%2520exemplar%2520forums.%2520We%2520further%2520put%2520state-of-the-art%2520LLMs%2520to%250Athe%2520test%252C%2520evaluating%2520how%2520well%2520these%2520models%2520perform%2520in%2520flagging%2520violations%2520of%250Aplatform%2520rules%2520from%2520one%2520particular%2520forum.%2520Finally%252C%2520we%2520conduct%2520a%2520user%2520survey%250Astudy%2520with%2520volunteer%2520moderators%2520to%2520gain%2520insight%2520into%2520their%2520perspectives%2520on%250Auseful%2520moderation%2520models.%2520Overall%252C%2520we%2520observe%2520a%2520non-trivial%2520gap%252C%2520as%2520missing%250Adeveloped%2520models%2520and%2520LLMs%2520exhibit%2520moderate%2520to%2520low%2520performance%2520on%2520a%2520significant%250Aportion%2520of%2520the%2520rules.%2520Moderators%2527%2520reports%2520provide%2520guides%2520for%2520future%2520work%2520on%250Adeveloping%2520moderation%2520assistant%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.07879v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toxicity%20Detection%20is%20NOT%20all%20you%20Need%3A%20Measuring%20the%20Gaps%20to%20Supporting%0A%20%20Volunteer%20Content%20Moderators&entry.906535625=Yang%20Trista%20Cao%20and%20Lovely-Frances%20Domingo%20and%20Sarah%20Ann%20Gilbert%20and%20Michelle%20Mazurek%20and%20Katie%20Shilton%20and%20Hal%20Daum%C3%A9%20III&entry.1292438233=%20%20Extensive%20efforts%20in%20automated%20approaches%20for%20content%20moderation%20have%20been%0Afocused%20on%20developing%20models%20to%20identify%20toxic%2C%20offensive%2C%20and%20hateful%20content%0Awith%20the%20aim%20of%20lightening%20the%20load%20for%20moderators.%20Yet%2C%20it%20remains%20uncertain%0Awhether%20improvements%20on%20those%20tasks%20have%20truly%20addressed%20moderators%27%20needs%20in%0Aaccomplishing%20their%20work.%20In%20this%20paper%2C%20we%20surface%20gaps%20between%20past%20research%0Aefforts%20that%20have%20aimed%20to%20provide%20automation%20for%20aspects%20of%20content%20moderation%0Aand%20the%20needs%20of%20volunteer%20content%20moderators%2C%20regarding%20identifying%20violations%0Aof%20various%20moderation%20rules.%20To%20do%20so%2C%20we%20conduct%20a%20model%20review%20on%20Hugging%0AFace%20to%20reveal%20the%20availability%20of%20models%20to%20cover%20various%20moderation%20rules%20and%0Aguidelines%20from%20three%20exemplar%20forums.%20We%20further%20put%20state-of-the-art%20LLMs%20to%0Athe%20test%2C%20evaluating%20how%20well%20these%20models%20perform%20in%20flagging%20violations%20of%0Aplatform%20rules%20from%20one%20particular%20forum.%20Finally%2C%20we%20conduct%20a%20user%20survey%0Astudy%20with%20volunteer%20moderators%20to%20gain%20insight%20into%20their%20perspectives%20on%0Auseful%20moderation%20models.%20Overall%2C%20we%20observe%20a%20non-trivial%20gap%2C%20as%20missing%0Adeveloped%20models%20and%20LLMs%20exhibit%20moderate%20to%20low%20performance%20on%20a%20significant%0Aportion%20of%20the%20rules.%20Moderators%27%20reports%20provide%20guides%20for%20future%20work%20on%0Adeveloping%20moderation%20assistant%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.07879v4&entry.124074799=Read"},
{"title": "Bayesian Comparisons Between Representations", "author": "Heiko H. Sch\u00fctt", "abstract": "  Which neural networks are similar is a fundamental question for both machine\nlearning and neuroscience. Our novel method compares representations based on\nBayesian statistics about linear readouts from the representations. Concretely,\nwe suggest to use the total variation distance or Jensen-Shannon distance\nbetween prior predictive distributions to compare representations. The prior\npredictive distribution is a full description of the inductive bias and\ngeneralization of a model in Bayesian statistics, making it a great basis for\ncomparisons. As Jensen-Shannon distance and total variation distance are\nmetrics our dissimilarity measures are pseudo-metrics for representations. For\na linear readout, our metrics just depend on the linear kernel matrix of the\nrepresentations. Thus, our metrics connects linear read-out based comparisons\nto kernel based metrics like centered kernel alignment and representational\nsimilarity analysis. We apply our new metrics to deep neural networks trained\non ImageNet-1k. Our new metrics can be computed efficiently including a\nstochastic gradient without dimensionality reductions of the representations.\nIt broadly agrees with existing metrics, but is more stringent. It varies less\nacross different random image samples, and it measures how well two\nrepresentations could be distinguished based on a linear read out. Thus our\nmetric nicely extends our toolkit for comparing representations.\n", "link": "http://arxiv.org/abs/2411.08739v1", "date": "2024-11-13", "relevancy": 1.8651, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5232}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.456}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4538}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Comparisons%20Between%20Representations&body=Title%3A%20Bayesian%20Comparisons%20Between%20Representations%0AAuthor%3A%20Heiko%20H.%20Sch%C3%BCtt%0AAbstract%3A%20%20%20Which%20neural%20networks%20are%20similar%20is%20a%20fundamental%20question%20for%20both%20machine%0Alearning%20and%20neuroscience.%20Our%20novel%20method%20compares%20representations%20based%20on%0ABayesian%20statistics%20about%20linear%20readouts%20from%20the%20representations.%20Concretely%2C%0Awe%20suggest%20to%20use%20the%20total%20variation%20distance%20or%20Jensen-Shannon%20distance%0Abetween%20prior%20predictive%20distributions%20to%20compare%20representations.%20The%20prior%0Apredictive%20distribution%20is%20a%20full%20description%20of%20the%20inductive%20bias%20and%0Ageneralization%20of%20a%20model%20in%20Bayesian%20statistics%2C%20making%20it%20a%20great%20basis%20for%0Acomparisons.%20As%20Jensen-Shannon%20distance%20and%20total%20variation%20distance%20are%0Ametrics%20our%20dissimilarity%20measures%20are%20pseudo-metrics%20for%20representations.%20For%0Aa%20linear%20readout%2C%20our%20metrics%20just%20depend%20on%20the%20linear%20kernel%20matrix%20of%20the%0Arepresentations.%20Thus%2C%20our%20metrics%20connects%20linear%20read-out%20based%20comparisons%0Ato%20kernel%20based%20metrics%20like%20centered%20kernel%20alignment%20and%20representational%0Asimilarity%20analysis.%20We%20apply%20our%20new%20metrics%20to%20deep%20neural%20networks%20trained%0Aon%20ImageNet-1k.%20Our%20new%20metrics%20can%20be%20computed%20efficiently%20including%20a%0Astochastic%20gradient%20without%20dimensionality%20reductions%20of%20the%20representations.%0AIt%20broadly%20agrees%20with%20existing%20metrics%2C%20but%20is%20more%20stringent.%20It%20varies%20less%0Aacross%20different%20random%20image%20samples%2C%20and%20it%20measures%20how%20well%20two%0Arepresentations%20could%20be%20distinguished%20based%20on%20a%20linear%20read%20out.%20Thus%20our%0Ametric%20nicely%20extends%20our%20toolkit%20for%20comparing%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08739v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Comparisons%2520Between%2520Representations%26entry.906535625%3DHeiko%2520H.%2520Sch%25C3%25BCtt%26entry.1292438233%3D%2520%2520Which%2520neural%2520networks%2520are%2520similar%2520is%2520a%2520fundamental%2520question%2520for%2520both%2520machine%250Alearning%2520and%2520neuroscience.%2520Our%2520novel%2520method%2520compares%2520representations%2520based%2520on%250ABayesian%2520statistics%2520about%2520linear%2520readouts%2520from%2520the%2520representations.%2520Concretely%252C%250Awe%2520suggest%2520to%2520use%2520the%2520total%2520variation%2520distance%2520or%2520Jensen-Shannon%2520distance%250Abetween%2520prior%2520predictive%2520distributions%2520to%2520compare%2520representations.%2520The%2520prior%250Apredictive%2520distribution%2520is%2520a%2520full%2520description%2520of%2520the%2520inductive%2520bias%2520and%250Ageneralization%2520of%2520a%2520model%2520in%2520Bayesian%2520statistics%252C%2520making%2520it%2520a%2520great%2520basis%2520for%250Acomparisons.%2520As%2520Jensen-Shannon%2520distance%2520and%2520total%2520variation%2520distance%2520are%250Ametrics%2520our%2520dissimilarity%2520measures%2520are%2520pseudo-metrics%2520for%2520representations.%2520For%250Aa%2520linear%2520readout%252C%2520our%2520metrics%2520just%2520depend%2520on%2520the%2520linear%2520kernel%2520matrix%2520of%2520the%250Arepresentations.%2520Thus%252C%2520our%2520metrics%2520connects%2520linear%2520read-out%2520based%2520comparisons%250Ato%2520kernel%2520based%2520metrics%2520like%2520centered%2520kernel%2520alignment%2520and%2520representational%250Asimilarity%2520analysis.%2520We%2520apply%2520our%2520new%2520metrics%2520to%2520deep%2520neural%2520networks%2520trained%250Aon%2520ImageNet-1k.%2520Our%2520new%2520metrics%2520can%2520be%2520computed%2520efficiently%2520including%2520a%250Astochastic%2520gradient%2520without%2520dimensionality%2520reductions%2520of%2520the%2520representations.%250AIt%2520broadly%2520agrees%2520with%2520existing%2520metrics%252C%2520but%2520is%2520more%2520stringent.%2520It%2520varies%2520less%250Aacross%2520different%2520random%2520image%2520samples%252C%2520and%2520it%2520measures%2520how%2520well%2520two%250Arepresentations%2520could%2520be%2520distinguished%2520based%2520on%2520a%2520linear%2520read%2520out.%2520Thus%2520our%250Ametric%2520nicely%2520extends%2520our%2520toolkit%2520for%2520comparing%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08739v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Comparisons%20Between%20Representations&entry.906535625=Heiko%20H.%20Sch%C3%BCtt&entry.1292438233=%20%20Which%20neural%20networks%20are%20similar%20is%20a%20fundamental%20question%20for%20both%20machine%0Alearning%20and%20neuroscience.%20Our%20novel%20method%20compares%20representations%20based%20on%0ABayesian%20statistics%20about%20linear%20readouts%20from%20the%20representations.%20Concretely%2C%0Awe%20suggest%20to%20use%20the%20total%20variation%20distance%20or%20Jensen-Shannon%20distance%0Abetween%20prior%20predictive%20distributions%20to%20compare%20representations.%20The%20prior%0Apredictive%20distribution%20is%20a%20full%20description%20of%20the%20inductive%20bias%20and%0Ageneralization%20of%20a%20model%20in%20Bayesian%20statistics%2C%20making%20it%20a%20great%20basis%20for%0Acomparisons.%20As%20Jensen-Shannon%20distance%20and%20total%20variation%20distance%20are%0Ametrics%20our%20dissimilarity%20measures%20are%20pseudo-metrics%20for%20representations.%20For%0Aa%20linear%20readout%2C%20our%20metrics%20just%20depend%20on%20the%20linear%20kernel%20matrix%20of%20the%0Arepresentations.%20Thus%2C%20our%20metrics%20connects%20linear%20read-out%20based%20comparisons%0Ato%20kernel%20based%20metrics%20like%20centered%20kernel%20alignment%20and%20representational%0Asimilarity%20analysis.%20We%20apply%20our%20new%20metrics%20to%20deep%20neural%20networks%20trained%0Aon%20ImageNet-1k.%20Our%20new%20metrics%20can%20be%20computed%20efficiently%20including%20a%0Astochastic%20gradient%20without%20dimensionality%20reductions%20of%20the%20representations.%0AIt%20broadly%20agrees%20with%20existing%20metrics%2C%20but%20is%20more%20stringent.%20It%20varies%20less%0Aacross%20different%20random%20image%20samples%2C%20and%20it%20measures%20how%20well%20two%0Arepresentations%20could%20be%20distinguished%20based%20on%20a%20linear%20read%20out.%20Thus%20our%0Ametric%20nicely%20extends%20our%20toolkit%20for%20comparing%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08739v1&entry.124074799=Read"},
{"title": "On the Robustness of Neural Collapse and the Neural Collapse of\n  Robustness", "author": "Jingtong Su and Ya Shi Zhang and Nikolaos Tsilivis and Julia Kempe", "abstract": "  Neural Collapse refers to the curious phenomenon in the end of training of a\nneural network, where feature vectors and classification weights converge to a\nvery simple geometrical arrangement (a simplex). While it has been observed\nempirically in various cases and has been theoretically motivated, its\nconnection with crucial properties of neural networks, like their\ngeneralization and robustness, remains unclear. In this work, we study the\nstability properties of these simplices. We find that the simplex structure\ndisappears under small adversarial attacks, and that perturbed examples \"leap\"\nbetween simplex vertices. We further analyze the geometry of networks that are\noptimized to be robust against adversarial perturbations of the input, and find\nthat Neural Collapse is a pervasive phenomenon in these cases as well, with\nclean and perturbed representations forming aligned simplices, and giving rise\nto a robust simple nearest-neighbor classifier. By studying the propagation of\nthe amount of collapse inside the network, we identify novel properties of both\nrobust and non-robust machine learning models, and show that earlier, unlike\nlater layers maintain reliable simplices on perturbed data. Our code is\navailable at https://github.com/JingtongSu/robust_neural_collapse .\n", "link": "http://arxiv.org/abs/2311.07444v2", "date": "2024-11-13", "relevancy": 1.8651, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4797}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4604}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Robustness%20of%20Neural%20Collapse%20and%20the%20Neural%20Collapse%20of%0A%20%20Robustness&body=Title%3A%20On%20the%20Robustness%20of%20Neural%20Collapse%20and%20the%20Neural%20Collapse%20of%0A%20%20Robustness%0AAuthor%3A%20Jingtong%20Su%20and%20Ya%20Shi%20Zhang%20and%20Nikolaos%20Tsilivis%20and%20Julia%20Kempe%0AAbstract%3A%20%20%20Neural%20Collapse%20refers%20to%20the%20curious%20phenomenon%20in%20the%20end%20of%20training%20of%20a%0Aneural%20network%2C%20where%20feature%20vectors%20and%20classification%20weights%20converge%20to%20a%0Avery%20simple%20geometrical%20arrangement%20%28a%20simplex%29.%20While%20it%20has%20been%20observed%0Aempirically%20in%20various%20cases%20and%20has%20been%20theoretically%20motivated%2C%20its%0Aconnection%20with%20crucial%20properties%20of%20neural%20networks%2C%20like%20their%0Ageneralization%20and%20robustness%2C%20remains%20unclear.%20In%20this%20work%2C%20we%20study%20the%0Astability%20properties%20of%20these%20simplices.%20We%20find%20that%20the%20simplex%20structure%0Adisappears%20under%20small%20adversarial%20attacks%2C%20and%20that%20perturbed%20examples%20%22leap%22%0Abetween%20simplex%20vertices.%20We%20further%20analyze%20the%20geometry%20of%20networks%20that%20are%0Aoptimized%20to%20be%20robust%20against%20adversarial%20perturbations%20of%20the%20input%2C%20and%20find%0Athat%20Neural%20Collapse%20is%20a%20pervasive%20phenomenon%20in%20these%20cases%20as%20well%2C%20with%0Aclean%20and%20perturbed%20representations%20forming%20aligned%20simplices%2C%20and%20giving%20rise%0Ato%20a%20robust%20simple%20nearest-neighbor%20classifier.%20By%20studying%20the%20propagation%20of%0Athe%20amount%20of%20collapse%20inside%20the%20network%2C%20we%20identify%20novel%20properties%20of%20both%0Arobust%20and%20non-robust%20machine%20learning%20models%2C%20and%20show%20that%20earlier%2C%20unlike%0Alater%20layers%20maintain%20reliable%20simplices%20on%20perturbed%20data.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/JingtongSu/robust_neural_collapse%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.07444v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Robustness%2520of%2520Neural%2520Collapse%2520and%2520the%2520Neural%2520Collapse%2520of%250A%2520%2520Robustness%26entry.906535625%3DJingtong%2520Su%2520and%2520Ya%2520Shi%2520Zhang%2520and%2520Nikolaos%2520Tsilivis%2520and%2520Julia%2520Kempe%26entry.1292438233%3D%2520%2520Neural%2520Collapse%2520refers%2520to%2520the%2520curious%2520phenomenon%2520in%2520the%2520end%2520of%2520training%2520of%2520a%250Aneural%2520network%252C%2520where%2520feature%2520vectors%2520and%2520classification%2520weights%2520converge%2520to%2520a%250Avery%2520simple%2520geometrical%2520arrangement%2520%2528a%2520simplex%2529.%2520While%2520it%2520has%2520been%2520observed%250Aempirically%2520in%2520various%2520cases%2520and%2520has%2520been%2520theoretically%2520motivated%252C%2520its%250Aconnection%2520with%2520crucial%2520properties%2520of%2520neural%2520networks%252C%2520like%2520their%250Ageneralization%2520and%2520robustness%252C%2520remains%2520unclear.%2520In%2520this%2520work%252C%2520we%2520study%2520the%250Astability%2520properties%2520of%2520these%2520simplices.%2520We%2520find%2520that%2520the%2520simplex%2520structure%250Adisappears%2520under%2520small%2520adversarial%2520attacks%252C%2520and%2520that%2520perturbed%2520examples%2520%2522leap%2522%250Abetween%2520simplex%2520vertices.%2520We%2520further%2520analyze%2520the%2520geometry%2520of%2520networks%2520that%2520are%250Aoptimized%2520to%2520be%2520robust%2520against%2520adversarial%2520perturbations%2520of%2520the%2520input%252C%2520and%2520find%250Athat%2520Neural%2520Collapse%2520is%2520a%2520pervasive%2520phenomenon%2520in%2520these%2520cases%2520as%2520well%252C%2520with%250Aclean%2520and%2520perturbed%2520representations%2520forming%2520aligned%2520simplices%252C%2520and%2520giving%2520rise%250Ato%2520a%2520robust%2520simple%2520nearest-neighbor%2520classifier.%2520By%2520studying%2520the%2520propagation%2520of%250Athe%2520amount%2520of%2520collapse%2520inside%2520the%2520network%252C%2520we%2520identify%2520novel%2520properties%2520of%2520both%250Arobust%2520and%2520non-robust%2520machine%2520learning%2520models%252C%2520and%2520show%2520that%2520earlier%252C%2520unlike%250Alater%2520layers%2520maintain%2520reliable%2520simplices%2520on%2520perturbed%2520data.%2520Our%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/JingtongSu/robust_neural_collapse%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.07444v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Robustness%20of%20Neural%20Collapse%20and%20the%20Neural%20Collapse%20of%0A%20%20Robustness&entry.906535625=Jingtong%20Su%20and%20Ya%20Shi%20Zhang%20and%20Nikolaos%20Tsilivis%20and%20Julia%20Kempe&entry.1292438233=%20%20Neural%20Collapse%20refers%20to%20the%20curious%20phenomenon%20in%20the%20end%20of%20training%20of%20a%0Aneural%20network%2C%20where%20feature%20vectors%20and%20classification%20weights%20converge%20to%20a%0Avery%20simple%20geometrical%20arrangement%20%28a%20simplex%29.%20While%20it%20has%20been%20observed%0Aempirically%20in%20various%20cases%20and%20has%20been%20theoretically%20motivated%2C%20its%0Aconnection%20with%20crucial%20properties%20of%20neural%20networks%2C%20like%20their%0Ageneralization%20and%20robustness%2C%20remains%20unclear.%20In%20this%20work%2C%20we%20study%20the%0Astability%20properties%20of%20these%20simplices.%20We%20find%20that%20the%20simplex%20structure%0Adisappears%20under%20small%20adversarial%20attacks%2C%20and%20that%20perturbed%20examples%20%22leap%22%0Abetween%20simplex%20vertices.%20We%20further%20analyze%20the%20geometry%20of%20networks%20that%20are%0Aoptimized%20to%20be%20robust%20against%20adversarial%20perturbations%20of%20the%20input%2C%20and%20find%0Athat%20Neural%20Collapse%20is%20a%20pervasive%20phenomenon%20in%20these%20cases%20as%20well%2C%20with%0Aclean%20and%20perturbed%20representations%20forming%20aligned%20simplices%2C%20and%20giving%20rise%0Ato%20a%20robust%20simple%20nearest-neighbor%20classifier.%20By%20studying%20the%20propagation%20of%0Athe%20amount%20of%20collapse%20inside%20the%20network%2C%20we%20identify%20novel%20properties%20of%20both%0Arobust%20and%20non-robust%20machine%20learning%20models%2C%20and%20show%20that%20earlier%2C%20unlike%0Alater%20layers%20maintain%20reliable%20simplices%20on%20perturbed%20data.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/JingtongSu/robust_neural_collapse%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.07444v2&entry.124074799=Read"},
{"title": "An Information Theoretic Approach to Operationalize Right to Data\n  Protection", "author": "Abhinav Java and Simra Shahid and Chirag Agarwal", "abstract": "  The widespread practice of indiscriminate data scraping to fine-tune language\nmodels (LMs) raises significant legal and ethical concerns, particularly\nregarding compliance with data protection laws such as the General Data\nProtection Regulation (GDPR). This practice often results in the unauthorized\nuse of personal information, prompting growing debate within the academic and\nregulatory communities. Recent works have introduced the concept of generating\nunlearnable datasets (by adding imperceptible noise to the clean data), such\nthat the underlying model achieves lower loss during training but fails to\ngeneralize to the unseen test setting. Though somewhat effective, these\napproaches are predominantly designed for images and are limited by several\npractical constraints like requiring knowledge of the target model. To this\nend, we introduce RegText, a framework that injects imperceptible spurious\ncorrelations into natural language datasets, effectively rendering them\nunlearnable without affecting semantic content. We demonstrate RegText's\nutility through rigorous empirical analysis of small and large LMs. Notably,\nRegText can restrict newer models like GPT-4o and Llama from learning on our\ngenerated data, resulting in a drop in their test accuracy compared to their\nzero-shot performance and paving the way for generating unlearnable text to\nprotect public data.\n", "link": "http://arxiv.org/abs/2411.08506v1", "date": "2024-11-13", "relevancy": 1.5045, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5265}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.503}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Information%20Theoretic%20Approach%20to%20Operationalize%20Right%20to%20Data%0A%20%20Protection&body=Title%3A%20An%20Information%20Theoretic%20Approach%20to%20Operationalize%20Right%20to%20Data%0A%20%20Protection%0AAuthor%3A%20Abhinav%20Java%20and%20Simra%20Shahid%20and%20Chirag%20Agarwal%0AAbstract%3A%20%20%20The%20widespread%20practice%20of%20indiscriminate%20data%20scraping%20to%20fine-tune%20language%0Amodels%20%28LMs%29%20raises%20significant%20legal%20and%20ethical%20concerns%2C%20particularly%0Aregarding%20compliance%20with%20data%20protection%20laws%20such%20as%20the%20General%20Data%0AProtection%20Regulation%20%28GDPR%29.%20This%20practice%20often%20results%20in%20the%20unauthorized%0Ause%20of%20personal%20information%2C%20prompting%20growing%20debate%20within%20the%20academic%20and%0Aregulatory%20communities.%20Recent%20works%20have%20introduced%20the%20concept%20of%20generating%0Aunlearnable%20datasets%20%28by%20adding%20imperceptible%20noise%20to%20the%20clean%20data%29%2C%20such%0Athat%20the%20underlying%20model%20achieves%20lower%20loss%20during%20training%20but%20fails%20to%0Ageneralize%20to%20the%20unseen%20test%20setting.%20Though%20somewhat%20effective%2C%20these%0Aapproaches%20are%20predominantly%20designed%20for%20images%20and%20are%20limited%20by%20several%0Apractical%20constraints%20like%20requiring%20knowledge%20of%20the%20target%20model.%20To%20this%0Aend%2C%20we%20introduce%20RegText%2C%20a%20framework%20that%20injects%20imperceptible%20spurious%0Acorrelations%20into%20natural%20language%20datasets%2C%20effectively%20rendering%20them%0Aunlearnable%20without%20affecting%20semantic%20content.%20We%20demonstrate%20RegText%27s%0Autility%20through%20rigorous%20empirical%20analysis%20of%20small%20and%20large%20LMs.%20Notably%2C%0ARegText%20can%20restrict%20newer%20models%20like%20GPT-4o%20and%20Llama%20from%20learning%20on%20our%0Agenerated%20data%2C%20resulting%20in%20a%20drop%20in%20their%20test%20accuracy%20compared%20to%20their%0Azero-shot%20performance%20and%20paving%20the%20way%20for%20generating%20unlearnable%20text%20to%0Aprotect%20public%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08506v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Information%2520Theoretic%2520Approach%2520to%2520Operationalize%2520Right%2520to%2520Data%250A%2520%2520Protection%26entry.906535625%3DAbhinav%2520Java%2520and%2520Simra%2520Shahid%2520and%2520Chirag%2520Agarwal%26entry.1292438233%3D%2520%2520The%2520widespread%2520practice%2520of%2520indiscriminate%2520data%2520scraping%2520to%2520fine-tune%2520language%250Amodels%2520%2528LMs%2529%2520raises%2520significant%2520legal%2520and%2520ethical%2520concerns%252C%2520particularly%250Aregarding%2520compliance%2520with%2520data%2520protection%2520laws%2520such%2520as%2520the%2520General%2520Data%250AProtection%2520Regulation%2520%2528GDPR%2529.%2520This%2520practice%2520often%2520results%2520in%2520the%2520unauthorized%250Ause%2520of%2520personal%2520information%252C%2520prompting%2520growing%2520debate%2520within%2520the%2520academic%2520and%250Aregulatory%2520communities.%2520Recent%2520works%2520have%2520introduced%2520the%2520concept%2520of%2520generating%250Aunlearnable%2520datasets%2520%2528by%2520adding%2520imperceptible%2520noise%2520to%2520the%2520clean%2520data%2529%252C%2520such%250Athat%2520the%2520underlying%2520model%2520achieves%2520lower%2520loss%2520during%2520training%2520but%2520fails%2520to%250Ageneralize%2520to%2520the%2520unseen%2520test%2520setting.%2520Though%2520somewhat%2520effective%252C%2520these%250Aapproaches%2520are%2520predominantly%2520designed%2520for%2520images%2520and%2520are%2520limited%2520by%2520several%250Apractical%2520constraints%2520like%2520requiring%2520knowledge%2520of%2520the%2520target%2520model.%2520To%2520this%250Aend%252C%2520we%2520introduce%2520RegText%252C%2520a%2520framework%2520that%2520injects%2520imperceptible%2520spurious%250Acorrelations%2520into%2520natural%2520language%2520datasets%252C%2520effectively%2520rendering%2520them%250Aunlearnable%2520without%2520affecting%2520semantic%2520content.%2520We%2520demonstrate%2520RegText%2527s%250Autility%2520through%2520rigorous%2520empirical%2520analysis%2520of%2520small%2520and%2520large%2520LMs.%2520Notably%252C%250ARegText%2520can%2520restrict%2520newer%2520models%2520like%2520GPT-4o%2520and%2520Llama%2520from%2520learning%2520on%2520our%250Agenerated%2520data%252C%2520resulting%2520in%2520a%2520drop%2520in%2520their%2520test%2520accuracy%2520compared%2520to%2520their%250Azero-shot%2520performance%2520and%2520paving%2520the%2520way%2520for%2520generating%2520unlearnable%2520text%2520to%250Aprotect%2520public%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08506v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Information%20Theoretic%20Approach%20to%20Operationalize%20Right%20to%20Data%0A%20%20Protection&entry.906535625=Abhinav%20Java%20and%20Simra%20Shahid%20and%20Chirag%20Agarwal&entry.1292438233=%20%20The%20widespread%20practice%20of%20indiscriminate%20data%20scraping%20to%20fine-tune%20language%0Amodels%20%28LMs%29%20raises%20significant%20legal%20and%20ethical%20concerns%2C%20particularly%0Aregarding%20compliance%20with%20data%20protection%20laws%20such%20as%20the%20General%20Data%0AProtection%20Regulation%20%28GDPR%29.%20This%20practice%20often%20results%20in%20the%20unauthorized%0Ause%20of%20personal%20information%2C%20prompting%20growing%20debate%20within%20the%20academic%20and%0Aregulatory%20communities.%20Recent%20works%20have%20introduced%20the%20concept%20of%20generating%0Aunlearnable%20datasets%20%28by%20adding%20imperceptible%20noise%20to%20the%20clean%20data%29%2C%20such%0Athat%20the%20underlying%20model%20achieves%20lower%20loss%20during%20training%20but%20fails%20to%0Ageneralize%20to%20the%20unseen%20test%20setting.%20Though%20somewhat%20effective%2C%20these%0Aapproaches%20are%20predominantly%20designed%20for%20images%20and%20are%20limited%20by%20several%0Apractical%20constraints%20like%20requiring%20knowledge%20of%20the%20target%20model.%20To%20this%0Aend%2C%20we%20introduce%20RegText%2C%20a%20framework%20that%20injects%20imperceptible%20spurious%0Acorrelations%20into%20natural%20language%20datasets%2C%20effectively%20rendering%20them%0Aunlearnable%20without%20affecting%20semantic%20content.%20We%20demonstrate%20RegText%27s%0Autility%20through%20rigorous%20empirical%20analysis%20of%20small%20and%20large%20LMs.%20Notably%2C%0ARegText%20can%20restrict%20newer%20models%20like%20GPT-4o%20and%20Llama%20from%20learning%20on%20our%0Agenerated%20data%2C%20resulting%20in%20a%20drop%20in%20their%20test%20accuracy%20compared%20to%20their%0Azero-shot%20performance%20and%20paving%20the%20way%20for%20generating%20unlearnable%20text%20to%0Aprotect%20public%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08506v1&entry.124074799=Read"},
{"title": "MVKTrans: Multi-View Knowledge Transfer for Robust Multiomics\n  Classification", "author": "Shan Cong and Zhiling Sang and Hongwei Liu and Haoran Luo and Xin Wang and Hong Liang and Jie Hao and Xiaohui Yao", "abstract": "  The distinct characteristics of multiomics data, including complex\ninteractions within and across biological layers and disease heterogeneity\n(e.g., heterogeneity in etiology and clinical symptoms), drive us to develop\nnovel designs to address unique challenges in multiomics prediction. In this\npaper, we propose the multi-view knowledge transfer learning (MVKTrans)\nframework, which transfers intra- and inter-omics knowledge in an adaptive\nmanner by reviewing data heterogeneity and suppressing bias transfer, thereby\nenhancing classification performance. Specifically, we design a graph\ncontrastive module that is trained on unlabeled data to effectively learn and\ntransfer the underlying intra-omics patterns to the supervised task. This\nunsupervised pretraining promotes learning general and unbiased representations\nfor each modality, regardless of the downstream tasks. In light of the varying\ndiscriminative capacities of modalities across different diseases and/or\nsamples, we introduce an adaptive and bi-directional cross-omics distillation\nmodule. This module automatically identifies richer modalities and facilitates\ndynamic knowledge transfer from more informative to less informative omics,\nthereby enabling a more robust and generalized integration. Extensive\nexperiments on four real biomedical datasets demonstrate the superior\nperformance and robustness of MVKTrans compared to the state-of-the-art. Code\nand data are available at https://github.com/Yaolab-fantastic/MVKTrans.\n", "link": "http://arxiv.org/abs/2411.08703v1", "date": "2024-11-13", "relevancy": 1.6436, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5732}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5335}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVKTrans%3A%20Multi-View%20Knowledge%20Transfer%20for%20Robust%20Multiomics%0A%20%20Classification&body=Title%3A%20MVKTrans%3A%20Multi-View%20Knowledge%20Transfer%20for%20Robust%20Multiomics%0A%20%20Classification%0AAuthor%3A%20Shan%20Cong%20and%20Zhiling%20Sang%20and%20Hongwei%20Liu%20and%20Haoran%20Luo%20and%20Xin%20Wang%20and%20Hong%20Liang%20and%20Jie%20Hao%20and%20Xiaohui%20Yao%0AAbstract%3A%20%20%20The%20distinct%20characteristics%20of%20multiomics%20data%2C%20including%20complex%0Ainteractions%20within%20and%20across%20biological%20layers%20and%20disease%20heterogeneity%0A%28e.g.%2C%20heterogeneity%20in%20etiology%20and%20clinical%20symptoms%29%2C%20drive%20us%20to%20develop%0Anovel%20designs%20to%20address%20unique%20challenges%20in%20multiomics%20prediction.%20In%20this%0Apaper%2C%20we%20propose%20the%20multi-view%20knowledge%20transfer%20learning%20%28MVKTrans%29%0Aframework%2C%20which%20transfers%20intra-%20and%20inter-omics%20knowledge%20in%20an%20adaptive%0Amanner%20by%20reviewing%20data%20heterogeneity%20and%20suppressing%20bias%20transfer%2C%20thereby%0Aenhancing%20classification%20performance.%20Specifically%2C%20we%20design%20a%20graph%0Acontrastive%20module%20that%20is%20trained%20on%20unlabeled%20data%20to%20effectively%20learn%20and%0Atransfer%20the%20underlying%20intra-omics%20patterns%20to%20the%20supervised%20task.%20This%0Aunsupervised%20pretraining%20promotes%20learning%20general%20and%20unbiased%20representations%0Afor%20each%20modality%2C%20regardless%20of%20the%20downstream%20tasks.%20In%20light%20of%20the%20varying%0Adiscriminative%20capacities%20of%20modalities%20across%20different%20diseases%20and/or%0Asamples%2C%20we%20introduce%20an%20adaptive%20and%20bi-directional%20cross-omics%20distillation%0Amodule.%20This%20module%20automatically%20identifies%20richer%20modalities%20and%20facilitates%0Adynamic%20knowledge%20transfer%20from%20more%20informative%20to%20less%20informative%20omics%2C%0Athereby%20enabling%20a%20more%20robust%20and%20generalized%20integration.%20Extensive%0Aexperiments%20on%20four%20real%20biomedical%20datasets%20demonstrate%20the%20superior%0Aperformance%20and%20robustness%20of%20MVKTrans%20compared%20to%20the%20state-of-the-art.%20Code%0Aand%20data%20are%20available%20at%20https%3A//github.com/Yaolab-fantastic/MVKTrans.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08703v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVKTrans%253A%2520Multi-View%2520Knowledge%2520Transfer%2520for%2520Robust%2520Multiomics%250A%2520%2520Classification%26entry.906535625%3DShan%2520Cong%2520and%2520Zhiling%2520Sang%2520and%2520Hongwei%2520Liu%2520and%2520Haoran%2520Luo%2520and%2520Xin%2520Wang%2520and%2520Hong%2520Liang%2520and%2520Jie%2520Hao%2520and%2520Xiaohui%2520Yao%26entry.1292438233%3D%2520%2520The%2520distinct%2520characteristics%2520of%2520multiomics%2520data%252C%2520including%2520complex%250Ainteractions%2520within%2520and%2520across%2520biological%2520layers%2520and%2520disease%2520heterogeneity%250A%2528e.g.%252C%2520heterogeneity%2520in%2520etiology%2520and%2520clinical%2520symptoms%2529%252C%2520drive%2520us%2520to%2520develop%250Anovel%2520designs%2520to%2520address%2520unique%2520challenges%2520in%2520multiomics%2520prediction.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520the%2520multi-view%2520knowledge%2520transfer%2520learning%2520%2528MVKTrans%2529%250Aframework%252C%2520which%2520transfers%2520intra-%2520and%2520inter-omics%2520knowledge%2520in%2520an%2520adaptive%250Amanner%2520by%2520reviewing%2520data%2520heterogeneity%2520and%2520suppressing%2520bias%2520transfer%252C%2520thereby%250Aenhancing%2520classification%2520performance.%2520Specifically%252C%2520we%2520design%2520a%2520graph%250Acontrastive%2520module%2520that%2520is%2520trained%2520on%2520unlabeled%2520data%2520to%2520effectively%2520learn%2520and%250Atransfer%2520the%2520underlying%2520intra-omics%2520patterns%2520to%2520the%2520supervised%2520task.%2520This%250Aunsupervised%2520pretraining%2520promotes%2520learning%2520general%2520and%2520unbiased%2520representations%250Afor%2520each%2520modality%252C%2520regardless%2520of%2520the%2520downstream%2520tasks.%2520In%2520light%2520of%2520the%2520varying%250Adiscriminative%2520capacities%2520of%2520modalities%2520across%2520different%2520diseases%2520and/or%250Asamples%252C%2520we%2520introduce%2520an%2520adaptive%2520and%2520bi-directional%2520cross-omics%2520distillation%250Amodule.%2520This%2520module%2520automatically%2520identifies%2520richer%2520modalities%2520and%2520facilitates%250Adynamic%2520knowledge%2520transfer%2520from%2520more%2520informative%2520to%2520less%2520informative%2520omics%252C%250Athereby%2520enabling%2520a%2520more%2520robust%2520and%2520generalized%2520integration.%2520Extensive%250Aexperiments%2520on%2520four%2520real%2520biomedical%2520datasets%2520demonstrate%2520the%2520superior%250Aperformance%2520and%2520robustness%2520of%2520MVKTrans%2520compared%2520to%2520the%2520state-of-the-art.%2520Code%250Aand%2520data%2520are%2520available%2520at%2520https%253A//github.com/Yaolab-fantastic/MVKTrans.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08703v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVKTrans%3A%20Multi-View%20Knowledge%20Transfer%20for%20Robust%20Multiomics%0A%20%20Classification&entry.906535625=Shan%20Cong%20and%20Zhiling%20Sang%20and%20Hongwei%20Liu%20and%20Haoran%20Luo%20and%20Xin%20Wang%20and%20Hong%20Liang%20and%20Jie%20Hao%20and%20Xiaohui%20Yao&entry.1292438233=%20%20The%20distinct%20characteristics%20of%20multiomics%20data%2C%20including%20complex%0Ainteractions%20within%20and%20across%20biological%20layers%20and%20disease%20heterogeneity%0A%28e.g.%2C%20heterogeneity%20in%20etiology%20and%20clinical%20symptoms%29%2C%20drive%20us%20to%20develop%0Anovel%20designs%20to%20address%20unique%20challenges%20in%20multiomics%20prediction.%20In%20this%0Apaper%2C%20we%20propose%20the%20multi-view%20knowledge%20transfer%20learning%20%28MVKTrans%29%0Aframework%2C%20which%20transfers%20intra-%20and%20inter-omics%20knowledge%20in%20an%20adaptive%0Amanner%20by%20reviewing%20data%20heterogeneity%20and%20suppressing%20bias%20transfer%2C%20thereby%0Aenhancing%20classification%20performance.%20Specifically%2C%20we%20design%20a%20graph%0Acontrastive%20module%20that%20is%20trained%20on%20unlabeled%20data%20to%20effectively%20learn%20and%0Atransfer%20the%20underlying%20intra-omics%20patterns%20to%20the%20supervised%20task.%20This%0Aunsupervised%20pretraining%20promotes%20learning%20general%20and%20unbiased%20representations%0Afor%20each%20modality%2C%20regardless%20of%20the%20downstream%20tasks.%20In%20light%20of%20the%20varying%0Adiscriminative%20capacities%20of%20modalities%20across%20different%20diseases%20and/or%0Asamples%2C%20we%20introduce%20an%20adaptive%20and%20bi-directional%20cross-omics%20distillation%0Amodule.%20This%20module%20automatically%20identifies%20richer%20modalities%20and%20facilitates%0Adynamic%20knowledge%20transfer%20from%20more%20informative%20to%20less%20informative%20omics%2C%0Athereby%20enabling%20a%20more%20robust%20and%20generalized%20integration.%20Extensive%0Aexperiments%20on%20four%20real%20biomedical%20datasets%20demonstrate%20the%20superior%0Aperformance%20and%20robustness%20of%20MVKTrans%20compared%20to%20the%20state-of-the-art.%20Code%0Aand%20data%20are%20available%20at%20https%3A//github.com/Yaolab-fantastic/MVKTrans.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08703v1&entry.124074799=Read"},
{"title": "Deep Learning Accelerated Quantum Transport Simulations in\n  Nanoelectronics: From Break Junctions to Field-Effect Transistors", "author": "Jijie Zou and Zhanghao Zhouyin and Dongying Lin and Linfeng Zhang and Shimin Hou and Qiangqiang Gu", "abstract": "  Quantum transport calculations are essential for understanding and designing\nnanoelectronic devices, yet the trade-off between accuracy and computational\nefficiency has long limited their practical applications. We present a general\nframework that combines the deep learning tight-binding Hamiltonian (DeePTB)\napproach with the non-equilibrium Green's Function (NEGF) method, enabling\nefficient quantum transport calculations while maintaining first-principles\naccuracy. We demonstrate the capabilities of the DeePTB-NEGF framework through\ntwo representative applications: comprehensive simulation of break junction\nsystems, where conductance histograms show good agreement with experimental\nmeasurements in both metallic contact and single-molecule junction cases; and\nsimulation of carbon nanotube field effect transistors through self-consistent\nNEGF-Poisson calculations, capturing essential physics including the\nelectrostatic potential and transfer characteristic curves under finite bias\nconditions. This framework bridges the gap between first-principles accuracy\nand computational efficiency, providing a powerful tool for high-throughput\nquantum transport simulations across different scales in nanoelectronics.\n", "link": "http://arxiv.org/abs/2411.08800v1", "date": "2024-11-13", "relevancy": 1.3047, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4455}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4335}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20Accelerated%20Quantum%20Transport%20Simulations%20in%0A%20%20Nanoelectronics%3A%20From%20Break%20Junctions%20to%20Field-Effect%20Transistors&body=Title%3A%20Deep%20Learning%20Accelerated%20Quantum%20Transport%20Simulations%20in%0A%20%20Nanoelectronics%3A%20From%20Break%20Junctions%20to%20Field-Effect%20Transistors%0AAuthor%3A%20Jijie%20Zou%20and%20Zhanghao%20Zhouyin%20and%20Dongying%20Lin%20and%20Linfeng%20Zhang%20and%20Shimin%20Hou%20and%20Qiangqiang%20Gu%0AAbstract%3A%20%20%20Quantum%20transport%20calculations%20are%20essential%20for%20understanding%20and%20designing%0Ananoelectronic%20devices%2C%20yet%20the%20trade-off%20between%20accuracy%20and%20computational%0Aefficiency%20has%20long%20limited%20their%20practical%20applications.%20We%20present%20a%20general%0Aframework%20that%20combines%20the%20deep%20learning%20tight-binding%20Hamiltonian%20%28DeePTB%29%0Aapproach%20with%20the%20non-equilibrium%20Green%27s%20Function%20%28NEGF%29%20method%2C%20enabling%0Aefficient%20quantum%20transport%20calculations%20while%20maintaining%20first-principles%0Aaccuracy.%20We%20demonstrate%20the%20capabilities%20of%20the%20DeePTB-NEGF%20framework%20through%0Atwo%20representative%20applications%3A%20comprehensive%20simulation%20of%20break%20junction%0Asystems%2C%20where%20conductance%20histograms%20show%20good%20agreement%20with%20experimental%0Ameasurements%20in%20both%20metallic%20contact%20and%20single-molecule%20junction%20cases%3B%20and%0Asimulation%20of%20carbon%20nanotube%20field%20effect%20transistors%20through%20self-consistent%0ANEGF-Poisson%20calculations%2C%20capturing%20essential%20physics%20including%20the%0Aelectrostatic%20potential%20and%20transfer%20characteristic%20curves%20under%20finite%20bias%0Aconditions.%20This%20framework%20bridges%20the%20gap%20between%20first-principles%20accuracy%0Aand%20computational%20efficiency%2C%20providing%20a%20powerful%20tool%20for%20high-throughput%0Aquantum%20transport%20simulations%20across%20different%20scales%20in%20nanoelectronics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08800v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520Accelerated%2520Quantum%2520Transport%2520Simulations%2520in%250A%2520%2520Nanoelectronics%253A%2520From%2520Break%2520Junctions%2520to%2520Field-Effect%2520Transistors%26entry.906535625%3DJijie%2520Zou%2520and%2520Zhanghao%2520Zhouyin%2520and%2520Dongying%2520Lin%2520and%2520Linfeng%2520Zhang%2520and%2520Shimin%2520Hou%2520and%2520Qiangqiang%2520Gu%26entry.1292438233%3D%2520%2520Quantum%2520transport%2520calculations%2520are%2520essential%2520for%2520understanding%2520and%2520designing%250Ananoelectronic%2520devices%252C%2520yet%2520the%2520trade-off%2520between%2520accuracy%2520and%2520computational%250Aefficiency%2520has%2520long%2520limited%2520their%2520practical%2520applications.%2520We%2520present%2520a%2520general%250Aframework%2520that%2520combines%2520the%2520deep%2520learning%2520tight-binding%2520Hamiltonian%2520%2528DeePTB%2529%250Aapproach%2520with%2520the%2520non-equilibrium%2520Green%2527s%2520Function%2520%2528NEGF%2529%2520method%252C%2520enabling%250Aefficient%2520quantum%2520transport%2520calculations%2520while%2520maintaining%2520first-principles%250Aaccuracy.%2520We%2520demonstrate%2520the%2520capabilities%2520of%2520the%2520DeePTB-NEGF%2520framework%2520through%250Atwo%2520representative%2520applications%253A%2520comprehensive%2520simulation%2520of%2520break%2520junction%250Asystems%252C%2520where%2520conductance%2520histograms%2520show%2520good%2520agreement%2520with%2520experimental%250Ameasurements%2520in%2520both%2520metallic%2520contact%2520and%2520single-molecule%2520junction%2520cases%253B%2520and%250Asimulation%2520of%2520carbon%2520nanotube%2520field%2520effect%2520transistors%2520through%2520self-consistent%250ANEGF-Poisson%2520calculations%252C%2520capturing%2520essential%2520physics%2520including%2520the%250Aelectrostatic%2520potential%2520and%2520transfer%2520characteristic%2520curves%2520under%2520finite%2520bias%250Aconditions.%2520This%2520framework%2520bridges%2520the%2520gap%2520between%2520first-principles%2520accuracy%250Aand%2520computational%2520efficiency%252C%2520providing%2520a%2520powerful%2520tool%2520for%2520high-throughput%250Aquantum%2520transport%2520simulations%2520across%2520different%2520scales%2520in%2520nanoelectronics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08800v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20Accelerated%20Quantum%20Transport%20Simulations%20in%0A%20%20Nanoelectronics%3A%20From%20Break%20Junctions%20to%20Field-Effect%20Transistors&entry.906535625=Jijie%20Zou%20and%20Zhanghao%20Zhouyin%20and%20Dongying%20Lin%20and%20Linfeng%20Zhang%20and%20Shimin%20Hou%20and%20Qiangqiang%20Gu&entry.1292438233=%20%20Quantum%20transport%20calculations%20are%20essential%20for%20understanding%20and%20designing%0Ananoelectronic%20devices%2C%20yet%20the%20trade-off%20between%20accuracy%20and%20computational%0Aefficiency%20has%20long%20limited%20their%20practical%20applications.%20We%20present%20a%20general%0Aframework%20that%20combines%20the%20deep%20learning%20tight-binding%20Hamiltonian%20%28DeePTB%29%0Aapproach%20with%20the%20non-equilibrium%20Green%27s%20Function%20%28NEGF%29%20method%2C%20enabling%0Aefficient%20quantum%20transport%20calculations%20while%20maintaining%20first-principles%0Aaccuracy.%20We%20demonstrate%20the%20capabilities%20of%20the%20DeePTB-NEGF%20framework%20through%0Atwo%20representative%20applications%3A%20comprehensive%20simulation%20of%20break%20junction%0Asystems%2C%20where%20conductance%20histograms%20show%20good%20agreement%20with%20experimental%0Ameasurements%20in%20both%20metallic%20contact%20and%20single-molecule%20junction%20cases%3B%20and%0Asimulation%20of%20carbon%20nanotube%20field%20effect%20transistors%20through%20self-consistent%0ANEGF-Poisson%20calculations%2C%20capturing%20essential%20physics%20including%20the%0Aelectrostatic%20potential%20and%20transfer%20characteristic%20curves%20under%20finite%20bias%0Aconditions.%20This%20framework%20bridges%20the%20gap%20between%20first-principles%20accuracy%0Aand%20computational%20efficiency%2C%20providing%20a%20powerful%20tool%20for%20high-throughput%0Aquantum%20transport%20simulations%20across%20different%20scales%20in%20nanoelectronics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08800v1&entry.124074799=Read"},
{"title": "Learning Robust Grasping Strategy Through Tactile Sensing and Adaption\n  Skill", "author": "Yueming Hu and Mengde Li and Songhua Yang and Xuetao Li and Sheng Liu and Miao Li", "abstract": "  Robust grasping represents an essential task in robotics, necessitating\ntactile feedback and reactive grasping adjustments for robust grasping of\nobjects. Previous research has extensively combined tactile sensing with\ngrasping, primarily relying on rule-based approaches, frequently neglecting\npost-grasping difficulties such as external disruptions or inherent\nuncertainties of the object's physics and geometry. To address these\nlimitations, this paper introduces an human-demonstration-based adaptive\ngrasping policy base on tactile, which aims to achieve robust gripping while\nresisting disturbances to maintain grasp stability. Our trained model\ngeneralizes to daily objects with seven different sizes, shapes, and textures.\nExperimental results demonstrate that our method performs well in dynamic and\nforce interaction tasks and exhibits excellent generalization ability.\n", "link": "http://arxiv.org/abs/2411.08499v1", "date": "2024-11-13", "relevancy": 1.8458, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6297}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6059}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5885}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Robust%20Grasping%20Strategy%20Through%20Tactile%20Sensing%20and%20Adaption%0A%20%20Skill&body=Title%3A%20Learning%20Robust%20Grasping%20Strategy%20Through%20Tactile%20Sensing%20and%20Adaption%0A%20%20Skill%0AAuthor%3A%20Yueming%20Hu%20and%20Mengde%20Li%20and%20Songhua%20Yang%20and%20Xuetao%20Li%20and%20Sheng%20Liu%20and%20Miao%20Li%0AAbstract%3A%20%20%20Robust%20grasping%20represents%20an%20essential%20task%20in%20robotics%2C%20necessitating%0Atactile%20feedback%20and%20reactive%20grasping%20adjustments%20for%20robust%20grasping%20of%0Aobjects.%20Previous%20research%20has%20extensively%20combined%20tactile%20sensing%20with%0Agrasping%2C%20primarily%20relying%20on%20rule-based%20approaches%2C%20frequently%20neglecting%0Apost-grasping%20difficulties%20such%20as%20external%20disruptions%20or%20inherent%0Auncertainties%20of%20the%20object%27s%20physics%20and%20geometry.%20To%20address%20these%0Alimitations%2C%20this%20paper%20introduces%20an%20human-demonstration-based%20adaptive%0Agrasping%20policy%20base%20on%20tactile%2C%20which%20aims%20to%20achieve%20robust%20gripping%20while%0Aresisting%20disturbances%20to%20maintain%20grasp%20stability.%20Our%20trained%20model%0Ageneralizes%20to%20daily%20objects%20with%20seven%20different%20sizes%2C%20shapes%2C%20and%20textures.%0AExperimental%20results%20demonstrate%20that%20our%20method%20performs%20well%20in%20dynamic%20and%0Aforce%20interaction%20tasks%20and%20exhibits%20excellent%20generalization%20ability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08499v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Robust%2520Grasping%2520Strategy%2520Through%2520Tactile%2520Sensing%2520and%2520Adaption%250A%2520%2520Skill%26entry.906535625%3DYueming%2520Hu%2520and%2520Mengde%2520Li%2520and%2520Songhua%2520Yang%2520and%2520Xuetao%2520Li%2520and%2520Sheng%2520Liu%2520and%2520Miao%2520Li%26entry.1292438233%3D%2520%2520Robust%2520grasping%2520represents%2520an%2520essential%2520task%2520in%2520robotics%252C%2520necessitating%250Atactile%2520feedback%2520and%2520reactive%2520grasping%2520adjustments%2520for%2520robust%2520grasping%2520of%250Aobjects.%2520Previous%2520research%2520has%2520extensively%2520combined%2520tactile%2520sensing%2520with%250Agrasping%252C%2520primarily%2520relying%2520on%2520rule-based%2520approaches%252C%2520frequently%2520neglecting%250Apost-grasping%2520difficulties%2520such%2520as%2520external%2520disruptions%2520or%2520inherent%250Auncertainties%2520of%2520the%2520object%2527s%2520physics%2520and%2520geometry.%2520To%2520address%2520these%250Alimitations%252C%2520this%2520paper%2520introduces%2520an%2520human-demonstration-based%2520adaptive%250Agrasping%2520policy%2520base%2520on%2520tactile%252C%2520which%2520aims%2520to%2520achieve%2520robust%2520gripping%2520while%250Aresisting%2520disturbances%2520to%2520maintain%2520grasp%2520stability.%2520Our%2520trained%2520model%250Ageneralizes%2520to%2520daily%2520objects%2520with%2520seven%2520different%2520sizes%252C%2520shapes%252C%2520and%2520textures.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520method%2520performs%2520well%2520in%2520dynamic%2520and%250Aforce%2520interaction%2520tasks%2520and%2520exhibits%2520excellent%2520generalization%2520ability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08499v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Robust%20Grasping%20Strategy%20Through%20Tactile%20Sensing%20and%20Adaption%0A%20%20Skill&entry.906535625=Yueming%20Hu%20and%20Mengde%20Li%20and%20Songhua%20Yang%20and%20Xuetao%20Li%20and%20Sheng%20Liu%20and%20Miao%20Li&entry.1292438233=%20%20Robust%20grasping%20represents%20an%20essential%20task%20in%20robotics%2C%20necessitating%0Atactile%20feedback%20and%20reactive%20grasping%20adjustments%20for%20robust%20grasping%20of%0Aobjects.%20Previous%20research%20has%20extensively%20combined%20tactile%20sensing%20with%0Agrasping%2C%20primarily%20relying%20on%20rule-based%20approaches%2C%20frequently%20neglecting%0Apost-grasping%20difficulties%20such%20as%20external%20disruptions%20or%20inherent%0Auncertainties%20of%20the%20object%27s%20physics%20and%20geometry.%20To%20address%20these%0Alimitations%2C%20this%20paper%20introduces%20an%20human-demonstration-based%20adaptive%0Agrasping%20policy%20base%20on%20tactile%2C%20which%20aims%20to%20achieve%20robust%20gripping%20while%0Aresisting%20disturbances%20to%20maintain%20grasp%20stability.%20Our%20trained%20model%0Ageneralizes%20to%20daily%20objects%20with%20seven%20different%20sizes%2C%20shapes%2C%20and%20textures.%0AExperimental%20results%20demonstrate%20that%20our%20method%20performs%20well%20in%20dynamic%20and%0Aforce%20interaction%20tasks%20and%20exhibits%20excellent%20generalization%20ability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08499v1&entry.124074799=Read"},
{"title": "OML: Open, Monetizable, and Loyal AI", "author": "Zerui Cheng and Edoardo Contente and Ben Finch and Oleg Golev and Jonathan Hayase and Andrew Miller and Niusha Moshrefi and Anshul Nasery and Sandeep Nailwal and Sewoong Oh and Himanshu Tyagi and Pramod Viswanath", "abstract": "  Artificial Intelligence (AI) has steadily improved across a wide range of\ntasks. However, the development and deployment of AI are almost entirely\ncontrolled by a few powerful organizations that are racing to create Artificial\nGeneral Intelligence (AGI). The centralized entities make decisions with little\npublic oversight, shaping the future of humanity, often with unforeseen\nconsequences. In this paper, we propose OML, which stands for Open,\nMonetizable, and Loyal AI, an approach designed to democratize AI development.\nOML is realized through an interdisciplinary framework spanning AI, blockchain,\nand cryptography. We present several ideas for constructing OML using\ntechnologies such as Trusted Execution Environments (TEE), traditional\ncryptographic primitives like fully homomorphic encryption and functional\nencryption, obfuscation, and AI-native solutions rooted in the sample\ncomplexity and intrinsic hardness of AI tasks. A key innovation of our work is\nintroducing a new scientific field: AI-native cryptography. Unlike conventional\ncryptography, which focuses on discrete data and binary security guarantees,\nAI-native cryptography exploits the continuous nature of AI data\nrepresentations and their low-dimensional manifolds, focusing on improving\napproximate performance. One core idea is to transform AI attack methods, such\nas data poisoning, into security tools. This novel approach serves as a\nfoundation for OML 1.0 which uses model fingerprinting to protect the integrity\nand ownership of AI models. The spirit of OML is to establish a decentralized,\nopen, and transparent platform for AI development, enabling the community to\ncontribute, monetize, and take ownership of AI models. By decentralizing\ncontrol and ensuring transparency through blockchain technology, OML prevents\nthe concentration of power and provides accountability in AI development that\nhas not been possible before.\n", "link": "http://arxiv.org/abs/2411.03887v2", "date": "2024-11-13", "relevancy": 1.8241, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4637}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4578}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OML%3A%20Open%2C%20Monetizable%2C%20and%20Loyal%20AI&body=Title%3A%20OML%3A%20Open%2C%20Monetizable%2C%20and%20Loyal%20AI%0AAuthor%3A%20Zerui%20Cheng%20and%20Edoardo%20Contente%20and%20Ben%20Finch%20and%20Oleg%20Golev%20and%20Jonathan%20Hayase%20and%20Andrew%20Miller%20and%20Niusha%20Moshrefi%20and%20Anshul%20Nasery%20and%20Sandeep%20Nailwal%20and%20Sewoong%20Oh%20and%20Himanshu%20Tyagi%20and%20Pramod%20Viswanath%0AAbstract%3A%20%20%20Artificial%20Intelligence%20%28AI%29%20has%20steadily%20improved%20across%20a%20wide%20range%20of%0Atasks.%20However%2C%20the%20development%20and%20deployment%20of%20AI%20are%20almost%20entirely%0Acontrolled%20by%20a%20few%20powerful%20organizations%20that%20are%20racing%20to%20create%20Artificial%0AGeneral%20Intelligence%20%28AGI%29.%20The%20centralized%20entities%20make%20decisions%20with%20little%0Apublic%20oversight%2C%20shaping%20the%20future%20of%20humanity%2C%20often%20with%20unforeseen%0Aconsequences.%20In%20this%20paper%2C%20we%20propose%20OML%2C%20which%20stands%20for%20Open%2C%0AMonetizable%2C%20and%20Loyal%20AI%2C%20an%20approach%20designed%20to%20democratize%20AI%20development.%0AOML%20is%20realized%20through%20an%20interdisciplinary%20framework%20spanning%20AI%2C%20blockchain%2C%0Aand%20cryptography.%20We%20present%20several%20ideas%20for%20constructing%20OML%20using%0Atechnologies%20such%20as%20Trusted%20Execution%20Environments%20%28TEE%29%2C%20traditional%0Acryptographic%20primitives%20like%20fully%20homomorphic%20encryption%20and%20functional%0Aencryption%2C%20obfuscation%2C%20and%20AI-native%20solutions%20rooted%20in%20the%20sample%0Acomplexity%20and%20intrinsic%20hardness%20of%20AI%20tasks.%20A%20key%20innovation%20of%20our%20work%20is%0Aintroducing%20a%20new%20scientific%20field%3A%20AI-native%20cryptography.%20Unlike%20conventional%0Acryptography%2C%20which%20focuses%20on%20discrete%20data%20and%20binary%20security%20guarantees%2C%0AAI-native%20cryptography%20exploits%20the%20continuous%20nature%20of%20AI%20data%0Arepresentations%20and%20their%20low-dimensional%20manifolds%2C%20focusing%20on%20improving%0Aapproximate%20performance.%20One%20core%20idea%20is%20to%20transform%20AI%20attack%20methods%2C%20such%0Aas%20data%20poisoning%2C%20into%20security%20tools.%20This%20novel%20approach%20serves%20as%20a%0Afoundation%20for%20OML%201.0%20which%20uses%20model%20fingerprinting%20to%20protect%20the%20integrity%0Aand%20ownership%20of%20AI%20models.%20The%20spirit%20of%20OML%20is%20to%20establish%20a%20decentralized%2C%0Aopen%2C%20and%20transparent%20platform%20for%20AI%20development%2C%20enabling%20the%20community%20to%0Acontribute%2C%20monetize%2C%20and%20take%20ownership%20of%20AI%20models.%20By%20decentralizing%0Acontrol%20and%20ensuring%20transparency%20through%20blockchain%20technology%2C%20OML%20prevents%0Athe%20concentration%20of%20power%20and%20provides%20accountability%20in%20AI%20development%20that%0Ahas%20not%20been%20possible%20before.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.03887v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOML%253A%2520Open%252C%2520Monetizable%252C%2520and%2520Loyal%2520AI%26entry.906535625%3DZerui%2520Cheng%2520and%2520Edoardo%2520Contente%2520and%2520Ben%2520Finch%2520and%2520Oleg%2520Golev%2520and%2520Jonathan%2520Hayase%2520and%2520Andrew%2520Miller%2520and%2520Niusha%2520Moshrefi%2520and%2520Anshul%2520Nasery%2520and%2520Sandeep%2520Nailwal%2520and%2520Sewoong%2520Oh%2520and%2520Himanshu%2520Tyagi%2520and%2520Pramod%2520Viswanath%26entry.1292438233%3D%2520%2520Artificial%2520Intelligence%2520%2528AI%2529%2520has%2520steadily%2520improved%2520across%2520a%2520wide%2520range%2520of%250Atasks.%2520However%252C%2520the%2520development%2520and%2520deployment%2520of%2520AI%2520are%2520almost%2520entirely%250Acontrolled%2520by%2520a%2520few%2520powerful%2520organizations%2520that%2520are%2520racing%2520to%2520create%2520Artificial%250AGeneral%2520Intelligence%2520%2528AGI%2529.%2520The%2520centralized%2520entities%2520make%2520decisions%2520with%2520little%250Apublic%2520oversight%252C%2520shaping%2520the%2520future%2520of%2520humanity%252C%2520often%2520with%2520unforeseen%250Aconsequences.%2520In%2520this%2520paper%252C%2520we%2520propose%2520OML%252C%2520which%2520stands%2520for%2520Open%252C%250AMonetizable%252C%2520and%2520Loyal%2520AI%252C%2520an%2520approach%2520designed%2520to%2520democratize%2520AI%2520development.%250AOML%2520is%2520realized%2520through%2520an%2520interdisciplinary%2520framework%2520spanning%2520AI%252C%2520blockchain%252C%250Aand%2520cryptography.%2520We%2520present%2520several%2520ideas%2520for%2520constructing%2520OML%2520using%250Atechnologies%2520such%2520as%2520Trusted%2520Execution%2520Environments%2520%2528TEE%2529%252C%2520traditional%250Acryptographic%2520primitives%2520like%2520fully%2520homomorphic%2520encryption%2520and%2520functional%250Aencryption%252C%2520obfuscation%252C%2520and%2520AI-native%2520solutions%2520rooted%2520in%2520the%2520sample%250Acomplexity%2520and%2520intrinsic%2520hardness%2520of%2520AI%2520tasks.%2520A%2520key%2520innovation%2520of%2520our%2520work%2520is%250Aintroducing%2520a%2520new%2520scientific%2520field%253A%2520AI-native%2520cryptography.%2520Unlike%2520conventional%250Acryptography%252C%2520which%2520focuses%2520on%2520discrete%2520data%2520and%2520binary%2520security%2520guarantees%252C%250AAI-native%2520cryptography%2520exploits%2520the%2520continuous%2520nature%2520of%2520AI%2520data%250Arepresentations%2520and%2520their%2520low-dimensional%2520manifolds%252C%2520focusing%2520on%2520improving%250Aapproximate%2520performance.%2520One%2520core%2520idea%2520is%2520to%2520transform%2520AI%2520attack%2520methods%252C%2520such%250Aas%2520data%2520poisoning%252C%2520into%2520security%2520tools.%2520This%2520novel%2520approach%2520serves%2520as%2520a%250Afoundation%2520for%2520OML%25201.0%2520which%2520uses%2520model%2520fingerprinting%2520to%2520protect%2520the%2520integrity%250Aand%2520ownership%2520of%2520AI%2520models.%2520The%2520spirit%2520of%2520OML%2520is%2520to%2520establish%2520a%2520decentralized%252C%250Aopen%252C%2520and%2520transparent%2520platform%2520for%2520AI%2520development%252C%2520enabling%2520the%2520community%2520to%250Acontribute%252C%2520monetize%252C%2520and%2520take%2520ownership%2520of%2520AI%2520models.%2520By%2520decentralizing%250Acontrol%2520and%2520ensuring%2520transparency%2520through%2520blockchain%2520technology%252C%2520OML%2520prevents%250Athe%2520concentration%2520of%2520power%2520and%2520provides%2520accountability%2520in%2520AI%2520development%2520that%250Ahas%2520not%2520been%2520possible%2520before.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.03887v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OML%3A%20Open%2C%20Monetizable%2C%20and%20Loyal%20AI&entry.906535625=Zerui%20Cheng%20and%20Edoardo%20Contente%20and%20Ben%20Finch%20and%20Oleg%20Golev%20and%20Jonathan%20Hayase%20and%20Andrew%20Miller%20and%20Niusha%20Moshrefi%20and%20Anshul%20Nasery%20and%20Sandeep%20Nailwal%20and%20Sewoong%20Oh%20and%20Himanshu%20Tyagi%20and%20Pramod%20Viswanath&entry.1292438233=%20%20Artificial%20Intelligence%20%28AI%29%20has%20steadily%20improved%20across%20a%20wide%20range%20of%0Atasks.%20However%2C%20the%20development%20and%20deployment%20of%20AI%20are%20almost%20entirely%0Acontrolled%20by%20a%20few%20powerful%20organizations%20that%20are%20racing%20to%20create%20Artificial%0AGeneral%20Intelligence%20%28AGI%29.%20The%20centralized%20entities%20make%20decisions%20with%20little%0Apublic%20oversight%2C%20shaping%20the%20future%20of%20humanity%2C%20often%20with%20unforeseen%0Aconsequences.%20In%20this%20paper%2C%20we%20propose%20OML%2C%20which%20stands%20for%20Open%2C%0AMonetizable%2C%20and%20Loyal%20AI%2C%20an%20approach%20designed%20to%20democratize%20AI%20development.%0AOML%20is%20realized%20through%20an%20interdisciplinary%20framework%20spanning%20AI%2C%20blockchain%2C%0Aand%20cryptography.%20We%20present%20several%20ideas%20for%20constructing%20OML%20using%0Atechnologies%20such%20as%20Trusted%20Execution%20Environments%20%28TEE%29%2C%20traditional%0Acryptographic%20primitives%20like%20fully%20homomorphic%20encryption%20and%20functional%0Aencryption%2C%20obfuscation%2C%20and%20AI-native%20solutions%20rooted%20in%20the%20sample%0Acomplexity%20and%20intrinsic%20hardness%20of%20AI%20tasks.%20A%20key%20innovation%20of%20our%20work%20is%0Aintroducing%20a%20new%20scientific%20field%3A%20AI-native%20cryptography.%20Unlike%20conventional%0Acryptography%2C%20which%20focuses%20on%20discrete%20data%20and%20binary%20security%20guarantees%2C%0AAI-native%20cryptography%20exploits%20the%20continuous%20nature%20of%20AI%20data%0Arepresentations%20and%20their%20low-dimensional%20manifolds%2C%20focusing%20on%20improving%0Aapproximate%20performance.%20One%20core%20idea%20is%20to%20transform%20AI%20attack%20methods%2C%20such%0Aas%20data%20poisoning%2C%20into%20security%20tools.%20This%20novel%20approach%20serves%20as%20a%0Afoundation%20for%20OML%201.0%20which%20uses%20model%20fingerprinting%20to%20protect%20the%20integrity%0Aand%20ownership%20of%20AI%20models.%20The%20spirit%20of%20OML%20is%20to%20establish%20a%20decentralized%2C%0Aopen%2C%20and%20transparent%20platform%20for%20AI%20development%2C%20enabling%20the%20community%20to%0Acontribute%2C%20monetize%2C%20and%20take%20ownership%20of%20AI%20models.%20By%20decentralizing%0Acontrol%20and%20ensuring%20transparency%20through%20blockchain%20technology%2C%20OML%20prevents%0Athe%20concentration%20of%20power%20and%20provides%20accountability%20in%20AI%20development%20that%0Ahas%20not%20been%20possible%20before.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.03887v2&entry.124074799=Read"},
{"title": "Data-driven Surface Solar Irradiance Estimation using Neural Operators\n  at Global Scale", "author": "Alberto Carpentieri and Jussi Leinonen and Jeff Adie and Boris Bonev and Doris Folini and Farah Hariri", "abstract": "  Accurate surface solar irradiance (SSI) forecasting is essential for\noptimizing renewable energy systems, particularly in the context of long-term\nenergy planning on a global scale. This paper presents a pioneering approach to\nsolar radiation forecasting that leverages recent advancements in numerical\nweather prediction (NWP) and data-driven machine learning weather models. These\nadvances facilitate long, stable rollouts and enable large ensemble forecasts,\nenhancing the reliability of predictions. Our flexible model utilizes variables\nforecast by these NWP and AI weather models to estimate 6-hourly SSI at global\nscale. Developed using NVIDIA Modulus, our model represents the first adaptive\nglobal framework capable of providing long-term SSI forecasts. Furthermore, it\ncan be fine-tuned using satellite data, which significantly enhances its\nperformance in the fine-tuned regions, while maintaining accuracy elsewhere.\nThe improved accuracy of these forecasts has substantial implications for the\nintegration of solar energy into power grids, enabling more efficient energy\nmanagement and contributing to the global transition to renewable energy\nsources.\n", "link": "http://arxiv.org/abs/2411.08843v1", "date": "2024-11-13", "relevancy": 0.9498, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4937}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4698}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-driven%20Surface%20Solar%20Irradiance%20Estimation%20using%20Neural%20Operators%0A%20%20at%20Global%20Scale&body=Title%3A%20Data-driven%20Surface%20Solar%20Irradiance%20Estimation%20using%20Neural%20Operators%0A%20%20at%20Global%20Scale%0AAuthor%3A%20Alberto%20Carpentieri%20and%20Jussi%20Leinonen%20and%20Jeff%20Adie%20and%20Boris%20Bonev%20and%20Doris%20Folini%20and%20Farah%20Hariri%0AAbstract%3A%20%20%20Accurate%20surface%20solar%20irradiance%20%28SSI%29%20forecasting%20is%20essential%20for%0Aoptimizing%20renewable%20energy%20systems%2C%20particularly%20in%20the%20context%20of%20long-term%0Aenergy%20planning%20on%20a%20global%20scale.%20This%20paper%20presents%20a%20pioneering%20approach%20to%0Asolar%20radiation%20forecasting%20that%20leverages%20recent%20advancements%20in%20numerical%0Aweather%20prediction%20%28NWP%29%20and%20data-driven%20machine%20learning%20weather%20models.%20These%0Aadvances%20facilitate%20long%2C%20stable%20rollouts%20and%20enable%20large%20ensemble%20forecasts%2C%0Aenhancing%20the%20reliability%20of%20predictions.%20Our%20flexible%20model%20utilizes%20variables%0Aforecast%20by%20these%20NWP%20and%20AI%20weather%20models%20to%20estimate%206-hourly%20SSI%20at%20global%0Ascale.%20Developed%20using%20NVIDIA%20Modulus%2C%20our%20model%20represents%20the%20first%20adaptive%0Aglobal%20framework%20capable%20of%20providing%20long-term%20SSI%20forecasts.%20Furthermore%2C%20it%0Acan%20be%20fine-tuned%20using%20satellite%20data%2C%20which%20significantly%20enhances%20its%0Aperformance%20in%20the%20fine-tuned%20regions%2C%20while%20maintaining%20accuracy%20elsewhere.%0AThe%20improved%20accuracy%20of%20these%20forecasts%20has%20substantial%20implications%20for%20the%0Aintegration%20of%20solar%20energy%20into%20power%20grids%2C%20enabling%20more%20efficient%20energy%0Amanagement%20and%20contributing%20to%20the%20global%20transition%20to%20renewable%20energy%0Asources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08843v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-driven%2520Surface%2520Solar%2520Irradiance%2520Estimation%2520using%2520Neural%2520Operators%250A%2520%2520at%2520Global%2520Scale%26entry.906535625%3DAlberto%2520Carpentieri%2520and%2520Jussi%2520Leinonen%2520and%2520Jeff%2520Adie%2520and%2520Boris%2520Bonev%2520and%2520Doris%2520Folini%2520and%2520Farah%2520Hariri%26entry.1292438233%3D%2520%2520Accurate%2520surface%2520solar%2520irradiance%2520%2528SSI%2529%2520forecasting%2520is%2520essential%2520for%250Aoptimizing%2520renewable%2520energy%2520systems%252C%2520particularly%2520in%2520the%2520context%2520of%2520long-term%250Aenergy%2520planning%2520on%2520a%2520global%2520scale.%2520This%2520paper%2520presents%2520a%2520pioneering%2520approach%2520to%250Asolar%2520radiation%2520forecasting%2520that%2520leverages%2520recent%2520advancements%2520in%2520numerical%250Aweather%2520prediction%2520%2528NWP%2529%2520and%2520data-driven%2520machine%2520learning%2520weather%2520models.%2520These%250Aadvances%2520facilitate%2520long%252C%2520stable%2520rollouts%2520and%2520enable%2520large%2520ensemble%2520forecasts%252C%250Aenhancing%2520the%2520reliability%2520of%2520predictions.%2520Our%2520flexible%2520model%2520utilizes%2520variables%250Aforecast%2520by%2520these%2520NWP%2520and%2520AI%2520weather%2520models%2520to%2520estimate%25206-hourly%2520SSI%2520at%2520global%250Ascale.%2520Developed%2520using%2520NVIDIA%2520Modulus%252C%2520our%2520model%2520represents%2520the%2520first%2520adaptive%250Aglobal%2520framework%2520capable%2520of%2520providing%2520long-term%2520SSI%2520forecasts.%2520Furthermore%252C%2520it%250Acan%2520be%2520fine-tuned%2520using%2520satellite%2520data%252C%2520which%2520significantly%2520enhances%2520its%250Aperformance%2520in%2520the%2520fine-tuned%2520regions%252C%2520while%2520maintaining%2520accuracy%2520elsewhere.%250AThe%2520improved%2520accuracy%2520of%2520these%2520forecasts%2520has%2520substantial%2520implications%2520for%2520the%250Aintegration%2520of%2520solar%2520energy%2520into%2520power%2520grids%252C%2520enabling%2520more%2520efficient%2520energy%250Amanagement%2520and%2520contributing%2520to%2520the%2520global%2520transition%2520to%2520renewable%2520energy%250Asources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08843v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-driven%20Surface%20Solar%20Irradiance%20Estimation%20using%20Neural%20Operators%0A%20%20at%20Global%20Scale&entry.906535625=Alberto%20Carpentieri%20and%20Jussi%20Leinonen%20and%20Jeff%20Adie%20and%20Boris%20Bonev%20and%20Doris%20Folini%20and%20Farah%20Hariri&entry.1292438233=%20%20Accurate%20surface%20solar%20irradiance%20%28SSI%29%20forecasting%20is%20essential%20for%0Aoptimizing%20renewable%20energy%20systems%2C%20particularly%20in%20the%20context%20of%20long-term%0Aenergy%20planning%20on%20a%20global%20scale.%20This%20paper%20presents%20a%20pioneering%20approach%20to%0Asolar%20radiation%20forecasting%20that%20leverages%20recent%20advancements%20in%20numerical%0Aweather%20prediction%20%28NWP%29%20and%20data-driven%20machine%20learning%20weather%20models.%20These%0Aadvances%20facilitate%20long%2C%20stable%20rollouts%20and%20enable%20large%20ensemble%20forecasts%2C%0Aenhancing%20the%20reliability%20of%20predictions.%20Our%20flexible%20model%20utilizes%20variables%0Aforecast%20by%20these%20NWP%20and%20AI%20weather%20models%20to%20estimate%206-hourly%20SSI%20at%20global%0Ascale.%20Developed%20using%20NVIDIA%20Modulus%2C%20our%20model%20represents%20the%20first%20adaptive%0Aglobal%20framework%20capable%20of%20providing%20long-term%20SSI%20forecasts.%20Furthermore%2C%20it%0Acan%20be%20fine-tuned%20using%20satellite%20data%2C%20which%20significantly%20enhances%20its%0Aperformance%20in%20the%20fine-tuned%20regions%2C%20while%20maintaining%20accuracy%20elsewhere.%0AThe%20improved%20accuracy%20of%20these%20forecasts%20has%20substantial%20implications%20for%20the%0Aintegration%20of%20solar%20energy%20into%20power%20grids%2C%20enabling%20more%20efficient%20energy%0Amanagement%20and%20contributing%20to%20the%20global%20transition%20to%20renewable%20energy%0Asources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08843v1&entry.124074799=Read"},
{"title": "Goal-oriented Semantic Communication for Robot Arm Reconstruction in\n  Digital Twin: Feature and Temporal Selections", "author": "Shutong Chen and Emmanouil Spyrakos-Papastavridis and Yichao Jin and Yansha Deng", "abstract": "  As one of the most promising technologies in industry, the Digital Twin (DT)\nfacilitates real-time monitoring and predictive analysis for real-world systems\nby precisely reconstructing virtual replicas of physical entities. However,\nthis reconstruction faces unprecedented challenges due to the everincreasing\ncommunication overhead, especially for digital robot arm reconstruction. To\nthis end, we propose a novel goal-oriented semantic communication (GSC)\nframework to extract the GSC information for the robot arm reconstruction task\nin the DT, with the aim of minimising the communication load under the strict\nand relaxed reconstruction error constraints. Unlike the traditional\nreconstruction framework that periodically transmits a reconstruction message\nfor real-time DT reconstruction, our framework implements a feature selection\n(FS) algorithm to extract the semantic information from the reconstruction\nmessage, and a deep reinforcement learning-based temporal selection algorithm\nto selectively transmit the semantic information over time. We validate our\nproposed GSC framework through both Pybullet simulations and lab experiments\nbased on the Franka Research 3 robot arm. For a range of distinct robotic\ntasks, simulation results show that our framework can reduce the communication\nload by at least 59.5% under strict reconstruction error constraints and 80%\nunder relaxed reconstruction error constraints, compared with traditional\ncommunication framework. Also, experimental results confirm the effectiveness\nof our framework, where the communication load is reduced by 53% in strict\nconstraint case and 74% in relaxed constraint case. The demo is available at:\nhttps://youtu.be/2OdeHKxcgnk.\n", "link": "http://arxiv.org/abs/2411.08835v1", "date": "2024-11-13", "relevancy": 1.7097, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5768}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5626}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.56}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Goal-oriented%20Semantic%20Communication%20for%20Robot%20Arm%20Reconstruction%20in%0A%20%20Digital%20Twin%3A%20Feature%20and%20Temporal%20Selections&body=Title%3A%20Goal-oriented%20Semantic%20Communication%20for%20Robot%20Arm%20Reconstruction%20in%0A%20%20Digital%20Twin%3A%20Feature%20and%20Temporal%20Selections%0AAuthor%3A%20Shutong%20Chen%20and%20Emmanouil%20Spyrakos-Papastavridis%20and%20Yichao%20Jin%20and%20Yansha%20Deng%0AAbstract%3A%20%20%20As%20one%20of%20the%20most%20promising%20technologies%20in%20industry%2C%20the%20Digital%20Twin%20%28DT%29%0Afacilitates%20real-time%20monitoring%20and%20predictive%20analysis%20for%20real-world%20systems%0Aby%20precisely%20reconstructing%20virtual%20replicas%20of%20physical%20entities.%20However%2C%0Athis%20reconstruction%20faces%20unprecedented%20challenges%20due%20to%20the%20everincreasing%0Acommunication%20overhead%2C%20especially%20for%20digital%20robot%20arm%20reconstruction.%20To%0Athis%20end%2C%20we%20propose%20a%20novel%20goal-oriented%20semantic%20communication%20%28GSC%29%0Aframework%20to%20extract%20the%20GSC%20information%20for%20the%20robot%20arm%20reconstruction%20task%0Ain%20the%20DT%2C%20with%20the%20aim%20of%20minimising%20the%20communication%20load%20under%20the%20strict%0Aand%20relaxed%20reconstruction%20error%20constraints.%20Unlike%20the%20traditional%0Areconstruction%20framework%20that%20periodically%20transmits%20a%20reconstruction%20message%0Afor%20real-time%20DT%20reconstruction%2C%20our%20framework%20implements%20a%20feature%20selection%0A%28FS%29%20algorithm%20to%20extract%20the%20semantic%20information%20from%20the%20reconstruction%0Amessage%2C%20and%20a%20deep%20reinforcement%20learning-based%20temporal%20selection%20algorithm%0Ato%20selectively%20transmit%20the%20semantic%20information%20over%20time.%20We%20validate%20our%0Aproposed%20GSC%20framework%20through%20both%20Pybullet%20simulations%20and%20lab%20experiments%0Abased%20on%20the%20Franka%20Research%203%20robot%20arm.%20For%20a%20range%20of%20distinct%20robotic%0Atasks%2C%20simulation%20results%20show%20that%20our%20framework%20can%20reduce%20the%20communication%0Aload%20by%20at%20least%2059.5%25%20under%20strict%20reconstruction%20error%20constraints%20and%2080%25%0Aunder%20relaxed%20reconstruction%20error%20constraints%2C%20compared%20with%20traditional%0Acommunication%20framework.%20Also%2C%20experimental%20results%20confirm%20the%20effectiveness%0Aof%20our%20framework%2C%20where%20the%20communication%20load%20is%20reduced%20by%2053%25%20in%20strict%0Aconstraint%20case%20and%2074%25%20in%20relaxed%20constraint%20case.%20The%20demo%20is%20available%20at%3A%0Ahttps%3A//youtu.be/2OdeHKxcgnk.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08835v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGoal-oriented%2520Semantic%2520Communication%2520for%2520Robot%2520Arm%2520Reconstruction%2520in%250A%2520%2520Digital%2520Twin%253A%2520Feature%2520and%2520Temporal%2520Selections%26entry.906535625%3DShutong%2520Chen%2520and%2520Emmanouil%2520Spyrakos-Papastavridis%2520and%2520Yichao%2520Jin%2520and%2520Yansha%2520Deng%26entry.1292438233%3D%2520%2520As%2520one%2520of%2520the%2520most%2520promising%2520technologies%2520in%2520industry%252C%2520the%2520Digital%2520Twin%2520%2528DT%2529%250Afacilitates%2520real-time%2520monitoring%2520and%2520predictive%2520analysis%2520for%2520real-world%2520systems%250Aby%2520precisely%2520reconstructing%2520virtual%2520replicas%2520of%2520physical%2520entities.%2520However%252C%250Athis%2520reconstruction%2520faces%2520unprecedented%2520challenges%2520due%2520to%2520the%2520everincreasing%250Acommunication%2520overhead%252C%2520especially%2520for%2520digital%2520robot%2520arm%2520reconstruction.%2520To%250Athis%2520end%252C%2520we%2520propose%2520a%2520novel%2520goal-oriented%2520semantic%2520communication%2520%2528GSC%2529%250Aframework%2520to%2520extract%2520the%2520GSC%2520information%2520for%2520the%2520robot%2520arm%2520reconstruction%2520task%250Ain%2520the%2520DT%252C%2520with%2520the%2520aim%2520of%2520minimising%2520the%2520communication%2520load%2520under%2520the%2520strict%250Aand%2520relaxed%2520reconstruction%2520error%2520constraints.%2520Unlike%2520the%2520traditional%250Areconstruction%2520framework%2520that%2520periodically%2520transmits%2520a%2520reconstruction%2520message%250Afor%2520real-time%2520DT%2520reconstruction%252C%2520our%2520framework%2520implements%2520a%2520feature%2520selection%250A%2528FS%2529%2520algorithm%2520to%2520extract%2520the%2520semantic%2520information%2520from%2520the%2520reconstruction%250Amessage%252C%2520and%2520a%2520deep%2520reinforcement%2520learning-based%2520temporal%2520selection%2520algorithm%250Ato%2520selectively%2520transmit%2520the%2520semantic%2520information%2520over%2520time.%2520We%2520validate%2520our%250Aproposed%2520GSC%2520framework%2520through%2520both%2520Pybullet%2520simulations%2520and%2520lab%2520experiments%250Abased%2520on%2520the%2520Franka%2520Research%25203%2520robot%2520arm.%2520For%2520a%2520range%2520of%2520distinct%2520robotic%250Atasks%252C%2520simulation%2520results%2520show%2520that%2520our%2520framework%2520can%2520reduce%2520the%2520communication%250Aload%2520by%2520at%2520least%252059.5%2525%2520under%2520strict%2520reconstruction%2520error%2520constraints%2520and%252080%2525%250Aunder%2520relaxed%2520reconstruction%2520error%2520constraints%252C%2520compared%2520with%2520traditional%250Acommunication%2520framework.%2520Also%252C%2520experimental%2520results%2520confirm%2520the%2520effectiveness%250Aof%2520our%2520framework%252C%2520where%2520the%2520communication%2520load%2520is%2520reduced%2520by%252053%2525%2520in%2520strict%250Aconstraint%2520case%2520and%252074%2525%2520in%2520relaxed%2520constraint%2520case.%2520The%2520demo%2520is%2520available%2520at%253A%250Ahttps%253A//youtu.be/2OdeHKxcgnk.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08835v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Goal-oriented%20Semantic%20Communication%20for%20Robot%20Arm%20Reconstruction%20in%0A%20%20Digital%20Twin%3A%20Feature%20and%20Temporal%20Selections&entry.906535625=Shutong%20Chen%20and%20Emmanouil%20Spyrakos-Papastavridis%20and%20Yichao%20Jin%20and%20Yansha%20Deng&entry.1292438233=%20%20As%20one%20of%20the%20most%20promising%20technologies%20in%20industry%2C%20the%20Digital%20Twin%20%28DT%29%0Afacilitates%20real-time%20monitoring%20and%20predictive%20analysis%20for%20real-world%20systems%0Aby%20precisely%20reconstructing%20virtual%20replicas%20of%20physical%20entities.%20However%2C%0Athis%20reconstruction%20faces%20unprecedented%20challenges%20due%20to%20the%20everincreasing%0Acommunication%20overhead%2C%20especially%20for%20digital%20robot%20arm%20reconstruction.%20To%0Athis%20end%2C%20we%20propose%20a%20novel%20goal-oriented%20semantic%20communication%20%28GSC%29%0Aframework%20to%20extract%20the%20GSC%20information%20for%20the%20robot%20arm%20reconstruction%20task%0Ain%20the%20DT%2C%20with%20the%20aim%20of%20minimising%20the%20communication%20load%20under%20the%20strict%0Aand%20relaxed%20reconstruction%20error%20constraints.%20Unlike%20the%20traditional%0Areconstruction%20framework%20that%20periodically%20transmits%20a%20reconstruction%20message%0Afor%20real-time%20DT%20reconstruction%2C%20our%20framework%20implements%20a%20feature%20selection%0A%28FS%29%20algorithm%20to%20extract%20the%20semantic%20information%20from%20the%20reconstruction%0Amessage%2C%20and%20a%20deep%20reinforcement%20learning-based%20temporal%20selection%20algorithm%0Ato%20selectively%20transmit%20the%20semantic%20information%20over%20time.%20We%20validate%20our%0Aproposed%20GSC%20framework%20through%20both%20Pybullet%20simulations%20and%20lab%20experiments%0Abased%20on%20the%20Franka%20Research%203%20robot%20arm.%20For%20a%20range%20of%20distinct%20robotic%0Atasks%2C%20simulation%20results%20show%20that%20our%20framework%20can%20reduce%20the%20communication%0Aload%20by%20at%20least%2059.5%25%20under%20strict%20reconstruction%20error%20constraints%20and%2080%25%0Aunder%20relaxed%20reconstruction%20error%20constraints%2C%20compared%20with%20traditional%0Acommunication%20framework.%20Also%2C%20experimental%20results%20confirm%20the%20effectiveness%0Aof%20our%20framework%2C%20where%20the%20communication%20load%20is%20reduced%20by%2053%25%20in%20strict%0Aconstraint%20case%20and%2074%25%20in%20relaxed%20constraint%20case.%20The%20demo%20is%20available%20at%3A%0Ahttps%3A//youtu.be/2OdeHKxcgnk.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08835v1&entry.124074799=Read"},
{"title": "A Review of Electromagnetic Elimination Methods for low-field portable\n  MRI scanner", "author": "Wanyu Bian and Panfeng Li and Mengyao Zheng and Chihang Wang and Anying Li and Ying Li and Haowei Ni and Zixuan Zeng", "abstract": "  This paper analyzes conventional and deep learning methods for eliminating\nelectromagnetic interference (EMI) in MRI systems. We compare traditional\nanalytical and adaptive techniques with advanced deep learning approaches. Key\nstrengths and limitations of each method are highlighted. Recent advancements\nin active EMI elimination, such as external EMI receiver coils, are discussed\nalongside deep learning methods, which show superior EMI suppression by\nleveraging neural networks trained on MRI data. While deep learning improves\nEMI elimination and diagnostic capabilities, it introduces security and safety\nconcerns, particularly in commercial applications. A balanced approach,\nintegrating conventional reliability with deep learning's advanced\ncapabilities, is proposed for more effective EMI suppression in MRI systems.\n", "link": "http://arxiv.org/abs/2406.17804v3", "date": "2024-11-13", "relevancy": 1.7031, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4653}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4323}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4035}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Review%20of%20Electromagnetic%20Elimination%20Methods%20for%20low-field%20portable%0A%20%20MRI%20scanner&body=Title%3A%20A%20Review%20of%20Electromagnetic%20Elimination%20Methods%20for%20low-field%20portable%0A%20%20MRI%20scanner%0AAuthor%3A%20Wanyu%20Bian%20and%20Panfeng%20Li%20and%20Mengyao%20Zheng%20and%20Chihang%20Wang%20and%20Anying%20Li%20and%20Ying%20Li%20and%20Haowei%20Ni%20and%20Zixuan%20Zeng%0AAbstract%3A%20%20%20This%20paper%20analyzes%20conventional%20and%20deep%20learning%20methods%20for%20eliminating%0Aelectromagnetic%20interference%20%28EMI%29%20in%20MRI%20systems.%20We%20compare%20traditional%0Aanalytical%20and%20adaptive%20techniques%20with%20advanced%20deep%20learning%20approaches.%20Key%0Astrengths%20and%20limitations%20of%20each%20method%20are%20highlighted.%20Recent%20advancements%0Ain%20active%20EMI%20elimination%2C%20such%20as%20external%20EMI%20receiver%20coils%2C%20are%20discussed%0Aalongside%20deep%20learning%20methods%2C%20which%20show%20superior%20EMI%20suppression%20by%0Aleveraging%20neural%20networks%20trained%20on%20MRI%20data.%20While%20deep%20learning%20improves%0AEMI%20elimination%20and%20diagnostic%20capabilities%2C%20it%20introduces%20security%20and%20safety%0Aconcerns%2C%20particularly%20in%20commercial%20applications.%20A%20balanced%20approach%2C%0Aintegrating%20conventional%20reliability%20with%20deep%20learning%27s%20advanced%0Acapabilities%2C%20is%20proposed%20for%20more%20effective%20EMI%20suppression%20in%20MRI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17804v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Review%2520of%2520Electromagnetic%2520Elimination%2520Methods%2520for%2520low-field%2520portable%250A%2520%2520MRI%2520scanner%26entry.906535625%3DWanyu%2520Bian%2520and%2520Panfeng%2520Li%2520and%2520Mengyao%2520Zheng%2520and%2520Chihang%2520Wang%2520and%2520Anying%2520Li%2520and%2520Ying%2520Li%2520and%2520Haowei%2520Ni%2520and%2520Zixuan%2520Zeng%26entry.1292438233%3D%2520%2520This%2520paper%2520analyzes%2520conventional%2520and%2520deep%2520learning%2520methods%2520for%2520eliminating%250Aelectromagnetic%2520interference%2520%2528EMI%2529%2520in%2520MRI%2520systems.%2520We%2520compare%2520traditional%250Aanalytical%2520and%2520adaptive%2520techniques%2520with%2520advanced%2520deep%2520learning%2520approaches.%2520Key%250Astrengths%2520and%2520limitations%2520of%2520each%2520method%2520are%2520highlighted.%2520Recent%2520advancements%250Ain%2520active%2520EMI%2520elimination%252C%2520such%2520as%2520external%2520EMI%2520receiver%2520coils%252C%2520are%2520discussed%250Aalongside%2520deep%2520learning%2520methods%252C%2520which%2520show%2520superior%2520EMI%2520suppression%2520by%250Aleveraging%2520neural%2520networks%2520trained%2520on%2520MRI%2520data.%2520While%2520deep%2520learning%2520improves%250AEMI%2520elimination%2520and%2520diagnostic%2520capabilities%252C%2520it%2520introduces%2520security%2520and%2520safety%250Aconcerns%252C%2520particularly%2520in%2520commercial%2520applications.%2520A%2520balanced%2520approach%252C%250Aintegrating%2520conventional%2520reliability%2520with%2520deep%2520learning%2527s%2520advanced%250Acapabilities%252C%2520is%2520proposed%2520for%2520more%2520effective%2520EMI%2520suppression%2520in%2520MRI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17804v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Review%20of%20Electromagnetic%20Elimination%20Methods%20for%20low-field%20portable%0A%20%20MRI%20scanner&entry.906535625=Wanyu%20Bian%20and%20Panfeng%20Li%20and%20Mengyao%20Zheng%20and%20Chihang%20Wang%20and%20Anying%20Li%20and%20Ying%20Li%20and%20Haowei%20Ni%20and%20Zixuan%20Zeng&entry.1292438233=%20%20This%20paper%20analyzes%20conventional%20and%20deep%20learning%20methods%20for%20eliminating%0Aelectromagnetic%20interference%20%28EMI%29%20in%20MRI%20systems.%20We%20compare%20traditional%0Aanalytical%20and%20adaptive%20techniques%20with%20advanced%20deep%20learning%20approaches.%20Key%0Astrengths%20and%20limitations%20of%20each%20method%20are%20highlighted.%20Recent%20advancements%0Ain%20active%20EMI%20elimination%2C%20such%20as%20external%20EMI%20receiver%20coils%2C%20are%20discussed%0Aalongside%20deep%20learning%20methods%2C%20which%20show%20superior%20EMI%20suppression%20by%0Aleveraging%20neural%20networks%20trained%20on%20MRI%20data.%20While%20deep%20learning%20improves%0AEMI%20elimination%20and%20diagnostic%20capabilities%2C%20it%20introduces%20security%20and%20safety%0Aconcerns%2C%20particularly%20in%20commercial%20applications.%20A%20balanced%20approach%2C%0Aintegrating%20conventional%20reliability%20with%20deep%20learning%27s%20advanced%0Acapabilities%2C%20is%20proposed%20for%20more%20effective%20EMI%20suppression%20in%20MRI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17804v3&entry.124074799=Read"},
{"title": "An Empirical Examination of the Evaluative AI Framework", "author": "Jaroslaw Kornowicz", "abstract": "  This study empirically examines the \"Evaluative AI\" framework, which aims to\nenhance the decision-making process for AI users by transitioning from a\nrecommendation-based approach to a hypothesis-driven one. Rather than offering\ndirect recommendations, this framework presents users pro and con evidence for\nhypotheses to support more informed decisions. However, findings from the\ncurrent behavioral experiment reveal no significant improvement in\ndecision-making performance and limited user engagement with the evidence\nprovided, resulting in cognitive processes similar to those observed in\ntraditional AI systems. Despite these results, the framework still holds\npromise for further exploration in future research.\n", "link": "http://arxiv.org/abs/2411.08583v1", "date": "2024-11-13", "relevancy": 1.3645, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4711}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4567}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Empirical%20Examination%20of%20the%20Evaluative%20AI%20Framework&body=Title%3A%20An%20Empirical%20Examination%20of%20the%20Evaluative%20AI%20Framework%0AAuthor%3A%20Jaroslaw%20Kornowicz%0AAbstract%3A%20%20%20This%20study%20empirically%20examines%20the%20%22Evaluative%20AI%22%20framework%2C%20which%20aims%20to%0Aenhance%20the%20decision-making%20process%20for%20AI%20users%20by%20transitioning%20from%20a%0Arecommendation-based%20approach%20to%20a%20hypothesis-driven%20one.%20Rather%20than%20offering%0Adirect%20recommendations%2C%20this%20framework%20presents%20users%20pro%20and%20con%20evidence%20for%0Ahypotheses%20to%20support%20more%20informed%20decisions.%20However%2C%20findings%20from%20the%0Acurrent%20behavioral%20experiment%20reveal%20no%20significant%20improvement%20in%0Adecision-making%20performance%20and%20limited%20user%20engagement%20with%20the%20evidence%0Aprovided%2C%20resulting%20in%20cognitive%20processes%20similar%20to%20those%20observed%20in%0Atraditional%20AI%20systems.%20Despite%20these%20results%2C%20the%20framework%20still%20holds%0Apromise%20for%20further%20exploration%20in%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.08583v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Empirical%2520Examination%2520of%2520the%2520Evaluative%2520AI%2520Framework%26entry.906535625%3DJaroslaw%2520Kornowicz%26entry.1292438233%3D%2520%2520This%2520study%2520empirically%2520examines%2520the%2520%2522Evaluative%2520AI%2522%2520framework%252C%2520which%2520aims%2520to%250Aenhance%2520the%2520decision-making%2520process%2520for%2520AI%2520users%2520by%2520transitioning%2520from%2520a%250Arecommendation-based%2520approach%2520to%2520a%2520hypothesis-driven%2520one.%2520Rather%2520than%2520offering%250Adirect%2520recommendations%252C%2520this%2520framework%2520presents%2520users%2520pro%2520and%2520con%2520evidence%2520for%250Ahypotheses%2520to%2520support%2520more%2520informed%2520decisions.%2520However%252C%2520findings%2520from%2520the%250Acurrent%2520behavioral%2520experiment%2520reveal%2520no%2520significant%2520improvement%2520in%250Adecision-making%2520performance%2520and%2520limited%2520user%2520engagement%2520with%2520the%2520evidence%250Aprovided%252C%2520resulting%2520in%2520cognitive%2520processes%2520similar%2520to%2520those%2520observed%2520in%250Atraditional%2520AI%2520systems.%2520Despite%2520these%2520results%252C%2520the%2520framework%2520still%2520holds%250Apromise%2520for%2520further%2520exploration%2520in%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.08583v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Empirical%20Examination%20of%20the%20Evaluative%20AI%20Framework&entry.906535625=Jaroslaw%20Kornowicz&entry.1292438233=%20%20This%20study%20empirically%20examines%20the%20%22Evaluative%20AI%22%20framework%2C%20which%20aims%20to%0Aenhance%20the%20decision-making%20process%20for%20AI%20users%20by%20transitioning%20from%20a%0Arecommendation-based%20approach%20to%20a%20hypothesis-driven%20one.%20Rather%20than%20offering%0Adirect%20recommendations%2C%20this%20framework%20presents%20users%20pro%20and%20con%20evidence%20for%0Ahypotheses%20to%20support%20more%20informed%20decisions.%20However%2C%20findings%20from%20the%0Acurrent%20behavioral%20experiment%20reveal%20no%20significant%20improvement%20in%0Adecision-making%20performance%20and%20limited%20user%20engagement%20with%20the%20evidence%0Aprovided%2C%20resulting%20in%20cognitive%20processes%20similar%20to%20those%20observed%20in%0Atraditional%20AI%20systems.%20Despite%20these%20results%2C%20the%20framework%20still%20holds%0Apromise%20for%20further%20exploration%20in%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.08583v1&entry.124074799=Read"},
{"title": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System", "author": "Youngsuk Kim and Junghwan Lim and Hyuk-Jae Lee and Chae Eun Rhee", "abstract": "  The personalized recommendation system's continuous size growth poses new\nchallenges for model inference. Although weight-sharing algorithms have been\nproposed to reduce embedding table capacity, they increase memory access.\nRecent advancements in processing-in-memory (PIM) successfully enhance the\nrecommendation system's throughput by exploiting memory parallelism, but our\nanalysis shows that those algorithms introduce CPU-PIM communication overhead\ninto prior PIM systems, compromising the PIM throughput. We propose\nProactivePIM, a specialized memory architecture integrated with PIM technology\ntailored to accelerate the weight-sharing algorithms. ProacitvePIM integrates\nan SRAM cache within the PIM with an efficient prefetching scheme to leverage a\nunique locality of the algorithm and eliminate CPU-PIM communication.\n", "link": "http://arxiv.org/abs/2402.04032v4", "date": "2024-11-13", "relevancy": 1.7466, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4447}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.431}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4305}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProactivePIM%3A%20Accelerating%20Weight-Sharing%20Embedding%20Layer%20with%20PIM%20for%0A%20%20Scalable%20Recommendation%20System&body=Title%3A%20ProactivePIM%3A%20Accelerating%20Weight-Sharing%20Embedding%20Layer%20with%20PIM%20for%0A%20%20Scalable%20Recommendation%20System%0AAuthor%3A%20Youngsuk%20Kim%20and%20Junghwan%20Lim%20and%20Hyuk-Jae%20Lee%20and%20Chae%20Eun%20Rhee%0AAbstract%3A%20%20%20The%20personalized%20recommendation%20system%27s%20continuous%20size%20growth%20poses%20new%0Achallenges%20for%20model%20inference.%20Although%20weight-sharing%20algorithms%20have%20been%0Aproposed%20to%20reduce%20embedding%20table%20capacity%2C%20they%20increase%20memory%20access.%0ARecent%20advancements%20in%20processing-in-memory%20%28PIM%29%20successfully%20enhance%20the%0Arecommendation%20system%27s%20throughput%20by%20exploiting%20memory%20parallelism%2C%20but%20our%0Aanalysis%20shows%20that%20those%20algorithms%20introduce%20CPU-PIM%20communication%20overhead%0Ainto%20prior%20PIM%20systems%2C%20compromising%20the%20PIM%20throughput.%20We%20propose%0AProactivePIM%2C%20a%20specialized%20memory%20architecture%20integrated%20with%20PIM%20technology%0Atailored%20to%20accelerate%20the%20weight-sharing%20algorithms.%20ProacitvePIM%20integrates%0Aan%20SRAM%20cache%20within%20the%20PIM%20with%20an%20efficient%20prefetching%20scheme%20to%20leverage%20a%0Aunique%20locality%20of%20the%20algorithm%20and%20eliminate%20CPU-PIM%20communication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.04032v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProactivePIM%253A%2520Accelerating%2520Weight-Sharing%2520Embedding%2520Layer%2520with%2520PIM%2520for%250A%2520%2520Scalable%2520Recommendation%2520System%26entry.906535625%3DYoungsuk%2520Kim%2520and%2520Junghwan%2520Lim%2520and%2520Hyuk-Jae%2520Lee%2520and%2520Chae%2520Eun%2520Rhee%26entry.1292438233%3D%2520%2520The%2520personalized%2520recommendation%2520system%2527s%2520continuous%2520size%2520growth%2520poses%2520new%250Achallenges%2520for%2520model%2520inference.%2520Although%2520weight-sharing%2520algorithms%2520have%2520been%250Aproposed%2520to%2520reduce%2520embedding%2520table%2520capacity%252C%2520they%2520increase%2520memory%2520access.%250ARecent%2520advancements%2520in%2520processing-in-memory%2520%2528PIM%2529%2520successfully%2520enhance%2520the%250Arecommendation%2520system%2527s%2520throughput%2520by%2520exploiting%2520memory%2520parallelism%252C%2520but%2520our%250Aanalysis%2520shows%2520that%2520those%2520algorithms%2520introduce%2520CPU-PIM%2520communication%2520overhead%250Ainto%2520prior%2520PIM%2520systems%252C%2520compromising%2520the%2520PIM%2520throughput.%2520We%2520propose%250AProactivePIM%252C%2520a%2520specialized%2520memory%2520architecture%2520integrated%2520with%2520PIM%2520technology%250Atailored%2520to%2520accelerate%2520the%2520weight-sharing%2520algorithms.%2520ProacitvePIM%2520integrates%250Aan%2520SRAM%2520cache%2520within%2520the%2520PIM%2520with%2520an%2520efficient%2520prefetching%2520scheme%2520to%2520leverage%2520a%250Aunique%2520locality%2520of%2520the%2520algorithm%2520and%2520eliminate%2520CPU-PIM%2520communication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.04032v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProactivePIM%3A%20Accelerating%20Weight-Sharing%20Embedding%20Layer%20with%20PIM%20for%0A%20%20Scalable%20Recommendation%20System&entry.906535625=Youngsuk%20Kim%20and%20Junghwan%20Lim%20and%20Hyuk-Jae%20Lee%20and%20Chae%20Eun%20Rhee&entry.1292438233=%20%20The%20personalized%20recommendation%20system%27s%20continuous%20size%20growth%20poses%20new%0Achallenges%20for%20model%20inference.%20Although%20weight-sharing%20algorithms%20have%20been%0Aproposed%20to%20reduce%20embedding%20table%20capacity%2C%20they%20increase%20memory%20access.%0ARecent%20advancements%20in%20processing-in-memory%20%28PIM%29%20successfully%20enhance%20the%0Arecommendation%20system%27s%20throughput%20by%20exploiting%20memory%20parallelism%2C%20but%20our%0Aanalysis%20shows%20that%20those%20algorithms%20introduce%20CPU-PIM%20communication%20overhead%0Ainto%20prior%20PIM%20systems%2C%20compromising%20the%20PIM%20throughput.%20We%20propose%0AProactivePIM%2C%20a%20specialized%20memory%20architecture%20integrated%20with%20PIM%20technology%0Atailored%20to%20accelerate%20the%20weight-sharing%20algorithms.%20ProacitvePIM%20integrates%0Aan%20SRAM%20cache%20within%20the%20PIM%20with%20an%20efficient%20prefetching%20scheme%20to%20leverage%20a%0Aunique%20locality%20of%20the%20algorithm%20and%20eliminate%20CPU-PIM%20communication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.04032v4&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


