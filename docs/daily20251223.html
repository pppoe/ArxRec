<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251222.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "GSRender: Deduplicated Occupancy Prediction via Weakly Supervised 3D Gaussian Splatting", "author": "Qianpu Sun and Changyong Shu and Sifan Zhou and Runxi Cheng and Yongxian Wei and Zichen Yu and Dawei Yang and Sirui Han and Yuan Chun", "abstract": "Weakly-supervised 3D occupancy perception is crucial for vision-based autonomous driving in outdoor environments. Previous methods based on NeRF often face a challenge in balancing the number of samples used. Too many samples can decrease efficiency, while too few can compromise accuracy, leading to variations in the mean Intersection over Union (mIoU) by 5-10 points. Furthermore, even with surrounding-view image inputs, only a single image is rendered from each viewpoint at any given moment. This limitation leads to duplicated predictions, which significantly impacts the practicality of the approach. However, this issue has largely been overlooked in existing research. To address this, we propose GSRender, which uses 3D Gaussian Splatting for weakly-supervised occupancy estimation, simplifying the sampling process. Additionally, we introduce the Ray Compensation module, which reduces duplicated predictions by compensating for features from adjacent frames. Finally, we redesign the dynamic loss to remove the influence of dynamic objects from adjacent frames. Extensive experiments show that our approach achieves SOTA results in RayIoU (+6.0), while also narrowing the gap with 3D- supervised methods. This work lays a solid foundation for weakly-supervised occupancy perception. The code is available at https://github.com/Jasper-sudo-Sun/GSRender.", "link": "http://arxiv.org/abs/2412.14579v2", "date": "2025-12-22", "relevancy": 3.3086, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7039}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6521}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GSRender%3A%20Deduplicated%20Occupancy%20Prediction%20via%20Weakly%20Supervised%203D%20Gaussian%20Splatting&body=Title%3A%20GSRender%3A%20Deduplicated%20Occupancy%20Prediction%20via%20Weakly%20Supervised%203D%20Gaussian%20Splatting%0AAuthor%3A%20Qianpu%20Sun%20and%20Changyong%20Shu%20and%20Sifan%20Zhou%20and%20Runxi%20Cheng%20and%20Yongxian%20Wei%20and%20Zichen%20Yu%20and%20Dawei%20Yang%20and%20Sirui%20Han%20and%20Yuan%20Chun%0AAbstract%3A%20Weakly-supervised%203D%20occupancy%20perception%20is%20crucial%20for%20vision-based%20autonomous%20driving%20in%20outdoor%20environments.%20Previous%20methods%20based%20on%20NeRF%20often%20face%20a%20challenge%20in%20balancing%20the%20number%20of%20samples%20used.%20Too%20many%20samples%20can%20decrease%20efficiency%2C%20while%20too%20few%20can%20compromise%20accuracy%2C%20leading%20to%20variations%20in%20the%20mean%20Intersection%20over%20Union%20%28mIoU%29%20by%205-10%20points.%20Furthermore%2C%20even%20with%20surrounding-view%20image%20inputs%2C%20only%20a%20single%20image%20is%20rendered%20from%20each%20viewpoint%20at%20any%20given%20moment.%20This%20limitation%20leads%20to%20duplicated%20predictions%2C%20which%20significantly%20impacts%20the%20practicality%20of%20the%20approach.%20However%2C%20this%20issue%20has%20largely%20been%20overlooked%20in%20existing%20research.%20To%20address%20this%2C%20we%20propose%20GSRender%2C%20which%20uses%203D%20Gaussian%20Splatting%20for%20weakly-supervised%20occupancy%20estimation%2C%20simplifying%20the%20sampling%20process.%20Additionally%2C%20we%20introduce%20the%20Ray%20Compensation%20module%2C%20which%20reduces%20duplicated%20predictions%20by%20compensating%20for%20features%20from%20adjacent%20frames.%20Finally%2C%20we%20redesign%20the%20dynamic%20loss%20to%20remove%20the%20influence%20of%20dynamic%20objects%20from%20adjacent%20frames.%20Extensive%20experiments%20show%20that%20our%20approach%20achieves%20SOTA%20results%20in%20RayIoU%20%28%2B6.0%29%2C%20while%20also%20narrowing%20the%20gap%20with%203D-%20supervised%20methods.%20This%20work%20lays%20a%20solid%20foundation%20for%20weakly-supervised%20occupancy%20perception.%20The%20code%20is%20available%20at%20https%3A//github.com/Jasper-sudo-Sun/GSRender.%0ALink%3A%20http%3A//arxiv.org/abs/2412.14579v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGSRender%253A%2520Deduplicated%2520Occupancy%2520Prediction%2520via%2520Weakly%2520Supervised%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DQianpu%2520Sun%2520and%2520Changyong%2520Shu%2520and%2520Sifan%2520Zhou%2520and%2520Runxi%2520Cheng%2520and%2520Yongxian%2520Wei%2520and%2520Zichen%2520Yu%2520and%2520Dawei%2520Yang%2520and%2520Sirui%2520Han%2520and%2520Yuan%2520Chun%26entry.1292438233%3DWeakly-supervised%25203D%2520occupancy%2520perception%2520is%2520crucial%2520for%2520vision-based%2520autonomous%2520driving%2520in%2520outdoor%2520environments.%2520Previous%2520methods%2520based%2520on%2520NeRF%2520often%2520face%2520a%2520challenge%2520in%2520balancing%2520the%2520number%2520of%2520samples%2520used.%2520Too%2520many%2520samples%2520can%2520decrease%2520efficiency%252C%2520while%2520too%2520few%2520can%2520compromise%2520accuracy%252C%2520leading%2520to%2520variations%2520in%2520the%2520mean%2520Intersection%2520over%2520Union%2520%2528mIoU%2529%2520by%25205-10%2520points.%2520Furthermore%252C%2520even%2520with%2520surrounding-view%2520image%2520inputs%252C%2520only%2520a%2520single%2520image%2520is%2520rendered%2520from%2520each%2520viewpoint%2520at%2520any%2520given%2520moment.%2520This%2520limitation%2520leads%2520to%2520duplicated%2520predictions%252C%2520which%2520significantly%2520impacts%2520the%2520practicality%2520of%2520the%2520approach.%2520However%252C%2520this%2520issue%2520has%2520largely%2520been%2520overlooked%2520in%2520existing%2520research.%2520To%2520address%2520this%252C%2520we%2520propose%2520GSRender%252C%2520which%2520uses%25203D%2520Gaussian%2520Splatting%2520for%2520weakly-supervised%2520occupancy%2520estimation%252C%2520simplifying%2520the%2520sampling%2520process.%2520Additionally%252C%2520we%2520introduce%2520the%2520Ray%2520Compensation%2520module%252C%2520which%2520reduces%2520duplicated%2520predictions%2520by%2520compensating%2520for%2520features%2520from%2520adjacent%2520frames.%2520Finally%252C%2520we%2520redesign%2520the%2520dynamic%2520loss%2520to%2520remove%2520the%2520influence%2520of%2520dynamic%2520objects%2520from%2520adjacent%2520frames.%2520Extensive%2520experiments%2520show%2520that%2520our%2520approach%2520achieves%2520SOTA%2520results%2520in%2520RayIoU%2520%2528%252B6.0%2529%252C%2520while%2520also%2520narrowing%2520the%2520gap%2520with%25203D-%2520supervised%2520methods.%2520This%2520work%2520lays%2520a%2520solid%2520foundation%2520for%2520weakly-supervised%2520occupancy%2520perception.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/Jasper-sudo-Sun/GSRender.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.14579v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GSRender%3A%20Deduplicated%20Occupancy%20Prediction%20via%20Weakly%20Supervised%203D%20Gaussian%20Splatting&entry.906535625=Qianpu%20Sun%20and%20Changyong%20Shu%20and%20Sifan%20Zhou%20and%20Runxi%20Cheng%20and%20Yongxian%20Wei%20and%20Zichen%20Yu%20and%20Dawei%20Yang%20and%20Sirui%20Han%20and%20Yuan%20Chun&entry.1292438233=Weakly-supervised%203D%20occupancy%20perception%20is%20crucial%20for%20vision-based%20autonomous%20driving%20in%20outdoor%20environments.%20Previous%20methods%20based%20on%20NeRF%20often%20face%20a%20challenge%20in%20balancing%20the%20number%20of%20samples%20used.%20Too%20many%20samples%20can%20decrease%20efficiency%2C%20while%20too%20few%20can%20compromise%20accuracy%2C%20leading%20to%20variations%20in%20the%20mean%20Intersection%20over%20Union%20%28mIoU%29%20by%205-10%20points.%20Furthermore%2C%20even%20with%20surrounding-view%20image%20inputs%2C%20only%20a%20single%20image%20is%20rendered%20from%20each%20viewpoint%20at%20any%20given%20moment.%20This%20limitation%20leads%20to%20duplicated%20predictions%2C%20which%20significantly%20impacts%20the%20practicality%20of%20the%20approach.%20However%2C%20this%20issue%20has%20largely%20been%20overlooked%20in%20existing%20research.%20To%20address%20this%2C%20we%20propose%20GSRender%2C%20which%20uses%203D%20Gaussian%20Splatting%20for%20weakly-supervised%20occupancy%20estimation%2C%20simplifying%20the%20sampling%20process.%20Additionally%2C%20we%20introduce%20the%20Ray%20Compensation%20module%2C%20which%20reduces%20duplicated%20predictions%20by%20compensating%20for%20features%20from%20adjacent%20frames.%20Finally%2C%20we%20redesign%20the%20dynamic%20loss%20to%20remove%20the%20influence%20of%20dynamic%20objects%20from%20adjacent%20frames.%20Extensive%20experiments%20show%20that%20our%20approach%20achieves%20SOTA%20results%20in%20RayIoU%20%28%2B6.0%29%2C%20while%20also%20narrowing%20the%20gap%20with%203D-%20supervised%20methods.%20This%20work%20lays%20a%20solid%20foundation%20for%20weakly-supervised%20occupancy%20perception.%20The%20code%20is%20available%20at%20https%3A//github.com/Jasper-sudo-Sun/GSRender.&entry.1838667208=http%3A//arxiv.org/abs/2412.14579v2&entry.124074799=Read"},
{"title": "InterPose: Learning to Generate Human-Object Interactions from Large-Scale Web Videos", "author": "Yangsong Zhang and Abdul Ahad Butt and G\u00fcl Varol and Ivan Laptev", "abstract": "Human motion generation has shown great advances thanks to the recent diffusion models trained on large-scale motion capture data. Most of existing works, however, currently target animation of isolated people in empty scenes. Meanwhile, synthesizing realistic human-object interactions in complex 3D scenes remains a critical challenge in computer graphics and robotics. One obstacle towards generating versatile high-fidelity human-object interactions is the lack of large-scale datasets with diverse object manipulations. Indeed, existing motion capture data is typically restricted to single people and manipulations of limited sets of objects. To address this issue, we propose an automatic motion extraction pipeline and use it to collect interaction-rich human motions. Our new dataset InterPose contains 73.8K sequences of 3D human motions and corresponding text captions automatically obtained from 45.8K videos with human-object interactions. We perform extensive experiments and demonstrate InterPose to bring significant improvements to state-of-the-art methods for human motion generation. Moreover, using InterPose we develop an LLM-based agent enabling zero-shot animation of people interacting with diverse objects and scenes.", "link": "http://arxiv.org/abs/2509.00767v2", "date": "2025-12-22", "relevancy": 3.2788, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7211}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6232}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InterPose%3A%20Learning%20to%20Generate%20Human-Object%20Interactions%20from%20Large-Scale%20Web%20Videos&body=Title%3A%20InterPose%3A%20Learning%20to%20Generate%20Human-Object%20Interactions%20from%20Large-Scale%20Web%20Videos%0AAuthor%3A%20Yangsong%20Zhang%20and%20Abdul%20Ahad%20Butt%20and%20G%C3%BCl%20Varol%20and%20Ivan%20Laptev%0AAbstract%3A%20Human%20motion%20generation%20has%20shown%20great%20advances%20thanks%20to%20the%20recent%20diffusion%20models%20trained%20on%20large-scale%20motion%20capture%20data.%20Most%20of%20existing%20works%2C%20however%2C%20currently%20target%20animation%20of%20isolated%20people%20in%20empty%20scenes.%20Meanwhile%2C%20synthesizing%20realistic%20human-object%20interactions%20in%20complex%203D%20scenes%20remains%20a%20critical%20challenge%20in%20computer%20graphics%20and%20robotics.%20One%20obstacle%20towards%20generating%20versatile%20high-fidelity%20human-object%20interactions%20is%20the%20lack%20of%20large-scale%20datasets%20with%20diverse%20object%20manipulations.%20Indeed%2C%20existing%20motion%20capture%20data%20is%20typically%20restricted%20to%20single%20people%20and%20manipulations%20of%20limited%20sets%20of%20objects.%20To%20address%20this%20issue%2C%20we%20propose%20an%20automatic%20motion%20extraction%20pipeline%20and%20use%20it%20to%20collect%20interaction-rich%20human%20motions.%20Our%20new%20dataset%20InterPose%20contains%2073.8K%20sequences%20of%203D%20human%20motions%20and%20corresponding%20text%20captions%20automatically%20obtained%20from%2045.8K%20videos%20with%20human-object%20interactions.%20We%20perform%20extensive%20experiments%20and%20demonstrate%20InterPose%20to%20bring%20significant%20improvements%20to%20state-of-the-art%20methods%20for%20human%20motion%20generation.%20Moreover%2C%20using%20InterPose%20we%20develop%20an%20LLM-based%20agent%20enabling%20zero-shot%20animation%20of%20people%20interacting%20with%20diverse%20objects%20and%20scenes.%0ALink%3A%20http%3A//arxiv.org/abs/2509.00767v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterPose%253A%2520Learning%2520to%2520Generate%2520Human-Object%2520Interactions%2520from%2520Large-Scale%2520Web%2520Videos%26entry.906535625%3DYangsong%2520Zhang%2520and%2520Abdul%2520Ahad%2520Butt%2520and%2520G%25C3%25BCl%2520Varol%2520and%2520Ivan%2520Laptev%26entry.1292438233%3DHuman%2520motion%2520generation%2520has%2520shown%2520great%2520advances%2520thanks%2520to%2520the%2520recent%2520diffusion%2520models%2520trained%2520on%2520large-scale%2520motion%2520capture%2520data.%2520Most%2520of%2520existing%2520works%252C%2520however%252C%2520currently%2520target%2520animation%2520of%2520isolated%2520people%2520in%2520empty%2520scenes.%2520Meanwhile%252C%2520synthesizing%2520realistic%2520human-object%2520interactions%2520in%2520complex%25203D%2520scenes%2520remains%2520a%2520critical%2520challenge%2520in%2520computer%2520graphics%2520and%2520robotics.%2520One%2520obstacle%2520towards%2520generating%2520versatile%2520high-fidelity%2520human-object%2520interactions%2520is%2520the%2520lack%2520of%2520large-scale%2520datasets%2520with%2520diverse%2520object%2520manipulations.%2520Indeed%252C%2520existing%2520motion%2520capture%2520data%2520is%2520typically%2520restricted%2520to%2520single%2520people%2520and%2520manipulations%2520of%2520limited%2520sets%2520of%2520objects.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520an%2520automatic%2520motion%2520extraction%2520pipeline%2520and%2520use%2520it%2520to%2520collect%2520interaction-rich%2520human%2520motions.%2520Our%2520new%2520dataset%2520InterPose%2520contains%252073.8K%2520sequences%2520of%25203D%2520human%2520motions%2520and%2520corresponding%2520text%2520captions%2520automatically%2520obtained%2520from%252045.8K%2520videos%2520with%2520human-object%2520interactions.%2520We%2520perform%2520extensive%2520experiments%2520and%2520demonstrate%2520InterPose%2520to%2520bring%2520significant%2520improvements%2520to%2520state-of-the-art%2520methods%2520for%2520human%2520motion%2520generation.%2520Moreover%252C%2520using%2520InterPose%2520we%2520develop%2520an%2520LLM-based%2520agent%2520enabling%2520zero-shot%2520animation%2520of%2520people%2520interacting%2520with%2520diverse%2520objects%2520and%2520scenes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.00767v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InterPose%3A%20Learning%20to%20Generate%20Human-Object%20Interactions%20from%20Large-Scale%20Web%20Videos&entry.906535625=Yangsong%20Zhang%20and%20Abdul%20Ahad%20Butt%20and%20G%C3%BCl%20Varol%20and%20Ivan%20Laptev&entry.1292438233=Human%20motion%20generation%20has%20shown%20great%20advances%20thanks%20to%20the%20recent%20diffusion%20models%20trained%20on%20large-scale%20motion%20capture%20data.%20Most%20of%20existing%20works%2C%20however%2C%20currently%20target%20animation%20of%20isolated%20people%20in%20empty%20scenes.%20Meanwhile%2C%20synthesizing%20realistic%20human-object%20interactions%20in%20complex%203D%20scenes%20remains%20a%20critical%20challenge%20in%20computer%20graphics%20and%20robotics.%20One%20obstacle%20towards%20generating%20versatile%20high-fidelity%20human-object%20interactions%20is%20the%20lack%20of%20large-scale%20datasets%20with%20diverse%20object%20manipulations.%20Indeed%2C%20existing%20motion%20capture%20data%20is%20typically%20restricted%20to%20single%20people%20and%20manipulations%20of%20limited%20sets%20of%20objects.%20To%20address%20this%20issue%2C%20we%20propose%20an%20automatic%20motion%20extraction%20pipeline%20and%20use%20it%20to%20collect%20interaction-rich%20human%20motions.%20Our%20new%20dataset%20InterPose%20contains%2073.8K%20sequences%20of%203D%20human%20motions%20and%20corresponding%20text%20captions%20automatically%20obtained%20from%2045.8K%20videos%20with%20human-object%20interactions.%20We%20perform%20extensive%20experiments%20and%20demonstrate%20InterPose%20to%20bring%20significant%20improvements%20to%20state-of-the-art%20methods%20for%20human%20motion%20generation.%20Moreover%2C%20using%20InterPose%20we%20develop%20an%20LLM-based%20agent%20enabling%20zero-shot%20animation%20of%20people%20interacting%20with%20diverse%20objects%20and%20scenes.&entry.1838667208=http%3A//arxiv.org/abs/2509.00767v2&entry.124074799=Read"},
{"title": "4D Gaussian Splatting as a Learned Dynamical System", "author": "Arnold Caleb Asiimwe and Carl Vondrick", "abstract": "We reinterpret 4D Gaussian Splatting as a continuous-time dynamical system, where scene motion arises from integrating a learned neural dynamical field rather than applying per-frame deformations. This formulation, which we call EvoGS, treats the Gaussian representation as an evolving physical system whose state evolves continuously under a learned motion law. This unlocks capabilities absent in deformation-based approaches:(1) sample-efficient learning from sparse temporal supervision by modeling the underlying motion law; (2) temporal extrapolation enabling forward and backward prediction beyond observed time ranges; and (3) compositional dynamics that allow localized dynamics injection for controllable scene synthesis. Experiments on dynamic scene benchmarks show that EvoGS achieves better motion coherence and temporal consistency compared to deformation-field baselines while maintaining real-time rendering", "link": "http://arxiv.org/abs/2512.19648v1", "date": "2025-12-22", "relevancy": 3.131, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6359}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6287}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204D%20Gaussian%20Splatting%20as%20a%20Learned%20Dynamical%20System&body=Title%3A%204D%20Gaussian%20Splatting%20as%20a%20Learned%20Dynamical%20System%0AAuthor%3A%20Arnold%20Caleb%20Asiimwe%20and%20Carl%20Vondrick%0AAbstract%3A%20We%20reinterpret%204D%20Gaussian%20Splatting%20as%20a%20continuous-time%20dynamical%20system%2C%20where%20scene%20motion%20arises%20from%20integrating%20a%20learned%20neural%20dynamical%20field%20rather%20than%20applying%20per-frame%20deformations.%20This%20formulation%2C%20which%20we%20call%20EvoGS%2C%20treats%20the%20Gaussian%20representation%20as%20an%20evolving%20physical%20system%20whose%20state%20evolves%20continuously%20under%20a%20learned%20motion%20law.%20This%20unlocks%20capabilities%20absent%20in%20deformation-based%20approaches%3A%281%29%20sample-efficient%20learning%20from%20sparse%20temporal%20supervision%20by%20modeling%20the%20underlying%20motion%20law%3B%20%282%29%20temporal%20extrapolation%20enabling%20forward%20and%20backward%20prediction%20beyond%20observed%20time%20ranges%3B%20and%20%283%29%20compositional%20dynamics%20that%20allow%20localized%20dynamics%20injection%20for%20controllable%20scene%20synthesis.%20Experiments%20on%20dynamic%20scene%20benchmarks%20show%20that%20EvoGS%20achieves%20better%20motion%20coherence%20and%20temporal%20consistency%20compared%20to%20deformation-field%20baselines%20while%20maintaining%20real-time%20rendering%0ALink%3A%20http%3A//arxiv.org/abs/2512.19648v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4D%2520Gaussian%2520Splatting%2520as%2520a%2520Learned%2520Dynamical%2520System%26entry.906535625%3DArnold%2520Caleb%2520Asiimwe%2520and%2520Carl%2520Vondrick%26entry.1292438233%3DWe%2520reinterpret%25204D%2520Gaussian%2520Splatting%2520as%2520a%2520continuous-time%2520dynamical%2520system%252C%2520where%2520scene%2520motion%2520arises%2520from%2520integrating%2520a%2520learned%2520neural%2520dynamical%2520field%2520rather%2520than%2520applying%2520per-frame%2520deformations.%2520This%2520formulation%252C%2520which%2520we%2520call%2520EvoGS%252C%2520treats%2520the%2520Gaussian%2520representation%2520as%2520an%2520evolving%2520physical%2520system%2520whose%2520state%2520evolves%2520continuously%2520under%2520a%2520learned%2520motion%2520law.%2520This%2520unlocks%2520capabilities%2520absent%2520in%2520deformation-based%2520approaches%253A%25281%2529%2520sample-efficient%2520learning%2520from%2520sparse%2520temporal%2520supervision%2520by%2520modeling%2520the%2520underlying%2520motion%2520law%253B%2520%25282%2529%2520temporal%2520extrapolation%2520enabling%2520forward%2520and%2520backward%2520prediction%2520beyond%2520observed%2520time%2520ranges%253B%2520and%2520%25283%2529%2520compositional%2520dynamics%2520that%2520allow%2520localized%2520dynamics%2520injection%2520for%2520controllable%2520scene%2520synthesis.%2520Experiments%2520on%2520dynamic%2520scene%2520benchmarks%2520show%2520that%2520EvoGS%2520achieves%2520better%2520motion%2520coherence%2520and%2520temporal%2520consistency%2520compared%2520to%2520deformation-field%2520baselines%2520while%2520maintaining%2520real-time%2520rendering%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19648v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4D%20Gaussian%20Splatting%20as%20a%20Learned%20Dynamical%20System&entry.906535625=Arnold%20Caleb%20Asiimwe%20and%20Carl%20Vondrick&entry.1292438233=We%20reinterpret%204D%20Gaussian%20Splatting%20as%20a%20continuous-time%20dynamical%20system%2C%20where%20scene%20motion%20arises%20from%20integrating%20a%20learned%20neural%20dynamical%20field%20rather%20than%20applying%20per-frame%20deformations.%20This%20formulation%2C%20which%20we%20call%20EvoGS%2C%20treats%20the%20Gaussian%20representation%20as%20an%20evolving%20physical%20system%20whose%20state%20evolves%20continuously%20under%20a%20learned%20motion%20law.%20This%20unlocks%20capabilities%20absent%20in%20deformation-based%20approaches%3A%281%29%20sample-efficient%20learning%20from%20sparse%20temporal%20supervision%20by%20modeling%20the%20underlying%20motion%20law%3B%20%282%29%20temporal%20extrapolation%20enabling%20forward%20and%20backward%20prediction%20beyond%20observed%20time%20ranges%3B%20and%20%283%29%20compositional%20dynamics%20that%20allow%20localized%20dynamics%20injection%20for%20controllable%20scene%20synthesis.%20Experiments%20on%20dynamic%20scene%20benchmarks%20show%20that%20EvoGS%20achieves%20better%20motion%20coherence%20and%20temporal%20consistency%20compared%20to%20deformation-field%20baselines%20while%20maintaining%20real-time%20rendering&entry.1838667208=http%3A//arxiv.org/abs/2512.19648v1&entry.124074799=Read"},
{"title": "BabyFlow: 3D modeling of realistic and expressive infant faces", "author": "Antonia Alomar and Mireia Masias and Marius George Linguraru and Federico M. Sukno and Gemma Piella", "abstract": "Early detection of developmental disorders can be aided by analyzing infant craniofacial morphology, but modeling infant faces is challenging due to limited data and frequent spontaneous expressions. We introduce BabyFlow, a generative AI model that disentangles facial identity and expression, enabling independent control over both. Using normalizing flows, BabyFlow learns flexible, probabilistic representations that capture the complex, non-linear variability of expressive infant faces without restrictive linear assumptions. To address scarce and uncontrolled expressive data, we perform cross-age expression transfer, adapting expressions from adult 3D scans to enrich infant datasets with realistic and systematic expressive variants. As a result, BabyFlow improves 3D reconstruction accuracy, particularly in highly expressive regions such as the mouth, eyes, and nose, and supports synthesis and modification of infant expressions while preserving identity. Additionally, by integrating with diffusion models, BabyFlow generates high-fidelity 2D infant images with consistent 3D geometry, providing powerful tools for data augmentation and early facial analysis.", "link": "http://arxiv.org/abs/2512.19560v1", "date": "2025-12-22", "relevancy": 3.0581, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6128}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6128}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6093}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BabyFlow%3A%203D%20modeling%20of%20realistic%20and%20expressive%20infant%20faces&body=Title%3A%20BabyFlow%3A%203D%20modeling%20of%20realistic%20and%20expressive%20infant%20faces%0AAuthor%3A%20Antonia%20Alomar%20and%20Mireia%20Masias%20and%20Marius%20George%20Linguraru%20and%20Federico%20M.%20Sukno%20and%20Gemma%20Piella%0AAbstract%3A%20Early%20detection%20of%20developmental%20disorders%20can%20be%20aided%20by%20analyzing%20infant%20craniofacial%20morphology%2C%20but%20modeling%20infant%20faces%20is%20challenging%20due%20to%20limited%20data%20and%20frequent%20spontaneous%20expressions.%20We%20introduce%20BabyFlow%2C%20a%20generative%20AI%20model%20that%20disentangles%20facial%20identity%20and%20expression%2C%20enabling%20independent%20control%20over%20both.%20Using%20normalizing%20flows%2C%20BabyFlow%20learns%20flexible%2C%20probabilistic%20representations%20that%20capture%20the%20complex%2C%20non-linear%20variability%20of%20expressive%20infant%20faces%20without%20restrictive%20linear%20assumptions.%20To%20address%20scarce%20and%20uncontrolled%20expressive%20data%2C%20we%20perform%20cross-age%20expression%20transfer%2C%20adapting%20expressions%20from%20adult%203D%20scans%20to%20enrich%20infant%20datasets%20with%20realistic%20and%20systematic%20expressive%20variants.%20As%20a%20result%2C%20BabyFlow%20improves%203D%20reconstruction%20accuracy%2C%20particularly%20in%20highly%20expressive%20regions%20such%20as%20the%20mouth%2C%20eyes%2C%20and%20nose%2C%20and%20supports%20synthesis%20and%20modification%20of%20infant%20expressions%20while%20preserving%20identity.%20Additionally%2C%20by%20integrating%20with%20diffusion%20models%2C%20BabyFlow%20generates%20high-fidelity%202D%20infant%20images%20with%20consistent%203D%20geometry%2C%20providing%20powerful%20tools%20for%20data%20augmentation%20and%20early%20facial%20analysis.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19560v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBabyFlow%253A%25203D%2520modeling%2520of%2520realistic%2520and%2520expressive%2520infant%2520faces%26entry.906535625%3DAntonia%2520Alomar%2520and%2520Mireia%2520Masias%2520and%2520Marius%2520George%2520Linguraru%2520and%2520Federico%2520M.%2520Sukno%2520and%2520Gemma%2520Piella%26entry.1292438233%3DEarly%2520detection%2520of%2520developmental%2520disorders%2520can%2520be%2520aided%2520by%2520analyzing%2520infant%2520craniofacial%2520morphology%252C%2520but%2520modeling%2520infant%2520faces%2520is%2520challenging%2520due%2520to%2520limited%2520data%2520and%2520frequent%2520spontaneous%2520expressions.%2520We%2520introduce%2520BabyFlow%252C%2520a%2520generative%2520AI%2520model%2520that%2520disentangles%2520facial%2520identity%2520and%2520expression%252C%2520enabling%2520independent%2520control%2520over%2520both.%2520Using%2520normalizing%2520flows%252C%2520BabyFlow%2520learns%2520flexible%252C%2520probabilistic%2520representations%2520that%2520capture%2520the%2520complex%252C%2520non-linear%2520variability%2520of%2520expressive%2520infant%2520faces%2520without%2520restrictive%2520linear%2520assumptions.%2520To%2520address%2520scarce%2520and%2520uncontrolled%2520expressive%2520data%252C%2520we%2520perform%2520cross-age%2520expression%2520transfer%252C%2520adapting%2520expressions%2520from%2520adult%25203D%2520scans%2520to%2520enrich%2520infant%2520datasets%2520with%2520realistic%2520and%2520systematic%2520expressive%2520variants.%2520As%2520a%2520result%252C%2520BabyFlow%2520improves%25203D%2520reconstruction%2520accuracy%252C%2520particularly%2520in%2520highly%2520expressive%2520regions%2520such%2520as%2520the%2520mouth%252C%2520eyes%252C%2520and%2520nose%252C%2520and%2520supports%2520synthesis%2520and%2520modification%2520of%2520infant%2520expressions%2520while%2520preserving%2520identity.%2520Additionally%252C%2520by%2520integrating%2520with%2520diffusion%2520models%252C%2520BabyFlow%2520generates%2520high-fidelity%25202D%2520infant%2520images%2520with%2520consistent%25203D%2520geometry%252C%2520providing%2520powerful%2520tools%2520for%2520data%2520augmentation%2520and%2520early%2520facial%2520analysis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19560v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BabyFlow%3A%203D%20modeling%20of%20realistic%20and%20expressive%20infant%20faces&entry.906535625=Antonia%20Alomar%20and%20Mireia%20Masias%20and%20Marius%20George%20Linguraru%20and%20Federico%20M.%20Sukno%20and%20Gemma%20Piella&entry.1292438233=Early%20detection%20of%20developmental%20disorders%20can%20be%20aided%20by%20analyzing%20infant%20craniofacial%20morphology%2C%20but%20modeling%20infant%20faces%20is%20challenging%20due%20to%20limited%20data%20and%20frequent%20spontaneous%20expressions.%20We%20introduce%20BabyFlow%2C%20a%20generative%20AI%20model%20that%20disentangles%20facial%20identity%20and%20expression%2C%20enabling%20independent%20control%20over%20both.%20Using%20normalizing%20flows%2C%20BabyFlow%20learns%20flexible%2C%20probabilistic%20representations%20that%20capture%20the%20complex%2C%20non-linear%20variability%20of%20expressive%20infant%20faces%20without%20restrictive%20linear%20assumptions.%20To%20address%20scarce%20and%20uncontrolled%20expressive%20data%2C%20we%20perform%20cross-age%20expression%20transfer%2C%20adapting%20expressions%20from%20adult%203D%20scans%20to%20enrich%20infant%20datasets%20with%20realistic%20and%20systematic%20expressive%20variants.%20As%20a%20result%2C%20BabyFlow%20improves%203D%20reconstruction%20accuracy%2C%20particularly%20in%20highly%20expressive%20regions%20such%20as%20the%20mouth%2C%20eyes%2C%20and%20nose%2C%20and%20supports%20synthesis%20and%20modification%20of%20infant%20expressions%20while%20preserving%20identity.%20Additionally%2C%20by%20integrating%20with%20diffusion%20models%2C%20BabyFlow%20generates%20high-fidelity%202D%20infant%20images%20with%20consistent%203D%20geometry%2C%20providing%20powerful%20tools%20for%20data%20augmentation%20and%20early%20facial%20analysis.&entry.1838667208=http%3A//arxiv.org/abs/2512.19560v1&entry.124074799=Read"},
{"title": "Towards 3D Object-Centric Feature Learning for Semantic Scene Completion", "author": "Weihua Wang and Yubo Cui and Xiangru Lin and Zhiheng Li and Zheng Fang", "abstract": "Vision-based 3D Semantic Scene Completion (SSC) has received growing attention due to its potential in autonomous driving. While most existing approaches follow an ego-centric paradigm by aggregating and diffusing features over the entire scene, they often overlook fine-grained object-level details, leading to semantic and geometric ambiguities, especially in complex environments. To address this limitation, we propose Ocean, an object-centric prediction framework that decomposes the scene into individual object instances to enable more accurate semantic occupancy prediction. Specifically, we first employ a lightweight segmentation model, MobileSAM, to extract instance masks from the input image. Then, we introduce a 3D Semantic Group Attention module that leverages linear attention to aggregate object-centric features in 3D space. To handle segmentation errors and missing instances, we further design a Global Similarity-Guided Attention module that leverages segmentation features for global interaction. Finally, we propose an Instance-aware Local Diffusion module that improves instance features through a generative process and subsequently refines the scene representation in the BEV space. Extensive experiments on the SemanticKITTI and SSCBench-KITTI360 benchmarks demonstrate that Ocean achieves state-of-the-art performance, with mIoU scores of 17.40 and 20.28, respectively.", "link": "http://arxiv.org/abs/2511.13031v3", "date": "2025-12-22", "relevancy": 3.0412, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6142}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6142}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5963}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%203D%20Object-Centric%20Feature%20Learning%20for%20Semantic%20Scene%20Completion&body=Title%3A%20Towards%203D%20Object-Centric%20Feature%20Learning%20for%20Semantic%20Scene%20Completion%0AAuthor%3A%20Weihua%20Wang%20and%20Yubo%20Cui%20and%20Xiangru%20Lin%20and%20Zhiheng%20Li%20and%20Zheng%20Fang%0AAbstract%3A%20Vision-based%203D%20Semantic%20Scene%20Completion%20%28SSC%29%20has%20received%20growing%20attention%20due%20to%20its%20potential%20in%20autonomous%20driving.%20While%20most%20existing%20approaches%20follow%20an%20ego-centric%20paradigm%20by%20aggregating%20and%20diffusing%20features%20over%20the%20entire%20scene%2C%20they%20often%20overlook%20fine-grained%20object-level%20details%2C%20leading%20to%20semantic%20and%20geometric%20ambiguities%2C%20especially%20in%20complex%20environments.%20To%20address%20this%20limitation%2C%20we%20propose%20Ocean%2C%20an%20object-centric%20prediction%20framework%20that%20decomposes%20the%20scene%20into%20individual%20object%20instances%20to%20enable%20more%20accurate%20semantic%20occupancy%20prediction.%20Specifically%2C%20we%20first%20employ%20a%20lightweight%20segmentation%20model%2C%20MobileSAM%2C%20to%20extract%20instance%20masks%20from%20the%20input%20image.%20Then%2C%20we%20introduce%20a%203D%20Semantic%20Group%20Attention%20module%20that%20leverages%20linear%20attention%20to%20aggregate%20object-centric%20features%20in%203D%20space.%20To%20handle%20segmentation%20errors%20and%20missing%20instances%2C%20we%20further%20design%20a%20Global%20Similarity-Guided%20Attention%20module%20that%20leverages%20segmentation%20features%20for%20global%20interaction.%20Finally%2C%20we%20propose%20an%20Instance-aware%20Local%20Diffusion%20module%20that%20improves%20instance%20features%20through%20a%20generative%20process%20and%20subsequently%20refines%20the%20scene%20representation%20in%20the%20BEV%20space.%20Extensive%20experiments%20on%20the%20SemanticKITTI%20and%20SSCBench-KITTI360%20benchmarks%20demonstrate%20that%20Ocean%20achieves%20state-of-the-art%20performance%2C%20with%20mIoU%20scores%20of%2017.40%20and%2020.28%2C%20respectively.%0ALink%3A%20http%3A//arxiv.org/abs/2511.13031v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%25203D%2520Object-Centric%2520Feature%2520Learning%2520for%2520Semantic%2520Scene%2520Completion%26entry.906535625%3DWeihua%2520Wang%2520and%2520Yubo%2520Cui%2520and%2520Xiangru%2520Lin%2520and%2520Zhiheng%2520Li%2520and%2520Zheng%2520Fang%26entry.1292438233%3DVision-based%25203D%2520Semantic%2520Scene%2520Completion%2520%2528SSC%2529%2520has%2520received%2520growing%2520attention%2520due%2520to%2520its%2520potential%2520in%2520autonomous%2520driving.%2520While%2520most%2520existing%2520approaches%2520follow%2520an%2520ego-centric%2520paradigm%2520by%2520aggregating%2520and%2520diffusing%2520features%2520over%2520the%2520entire%2520scene%252C%2520they%2520often%2520overlook%2520fine-grained%2520object-level%2520details%252C%2520leading%2520to%2520semantic%2520and%2520geometric%2520ambiguities%252C%2520especially%2520in%2520complex%2520environments.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520Ocean%252C%2520an%2520object-centric%2520prediction%2520framework%2520that%2520decomposes%2520the%2520scene%2520into%2520individual%2520object%2520instances%2520to%2520enable%2520more%2520accurate%2520semantic%2520occupancy%2520prediction.%2520Specifically%252C%2520we%2520first%2520employ%2520a%2520lightweight%2520segmentation%2520model%252C%2520MobileSAM%252C%2520to%2520extract%2520instance%2520masks%2520from%2520the%2520input%2520image.%2520Then%252C%2520we%2520introduce%2520a%25203D%2520Semantic%2520Group%2520Attention%2520module%2520that%2520leverages%2520linear%2520attention%2520to%2520aggregate%2520object-centric%2520features%2520in%25203D%2520space.%2520To%2520handle%2520segmentation%2520errors%2520and%2520missing%2520instances%252C%2520we%2520further%2520design%2520a%2520Global%2520Similarity-Guided%2520Attention%2520module%2520that%2520leverages%2520segmentation%2520features%2520for%2520global%2520interaction.%2520Finally%252C%2520we%2520propose%2520an%2520Instance-aware%2520Local%2520Diffusion%2520module%2520that%2520improves%2520instance%2520features%2520through%2520a%2520generative%2520process%2520and%2520subsequently%2520refines%2520the%2520scene%2520representation%2520in%2520the%2520BEV%2520space.%2520Extensive%2520experiments%2520on%2520the%2520SemanticKITTI%2520and%2520SSCBench-KITTI360%2520benchmarks%2520demonstrate%2520that%2520Ocean%2520achieves%2520state-of-the-art%2520performance%252C%2520with%2520mIoU%2520scores%2520of%252017.40%2520and%252020.28%252C%2520respectively.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.13031v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%203D%20Object-Centric%20Feature%20Learning%20for%20Semantic%20Scene%20Completion&entry.906535625=Weihua%20Wang%20and%20Yubo%20Cui%20and%20Xiangru%20Lin%20and%20Zhiheng%20Li%20and%20Zheng%20Fang&entry.1292438233=Vision-based%203D%20Semantic%20Scene%20Completion%20%28SSC%29%20has%20received%20growing%20attention%20due%20to%20its%20potential%20in%20autonomous%20driving.%20While%20most%20existing%20approaches%20follow%20an%20ego-centric%20paradigm%20by%20aggregating%20and%20diffusing%20features%20over%20the%20entire%20scene%2C%20they%20often%20overlook%20fine-grained%20object-level%20details%2C%20leading%20to%20semantic%20and%20geometric%20ambiguities%2C%20especially%20in%20complex%20environments.%20To%20address%20this%20limitation%2C%20we%20propose%20Ocean%2C%20an%20object-centric%20prediction%20framework%20that%20decomposes%20the%20scene%20into%20individual%20object%20instances%20to%20enable%20more%20accurate%20semantic%20occupancy%20prediction.%20Specifically%2C%20we%20first%20employ%20a%20lightweight%20segmentation%20model%2C%20MobileSAM%2C%20to%20extract%20instance%20masks%20from%20the%20input%20image.%20Then%2C%20we%20introduce%20a%203D%20Semantic%20Group%20Attention%20module%20that%20leverages%20linear%20attention%20to%20aggregate%20object-centric%20features%20in%203D%20space.%20To%20handle%20segmentation%20errors%20and%20missing%20instances%2C%20we%20further%20design%20a%20Global%20Similarity-Guided%20Attention%20module%20that%20leverages%20segmentation%20features%20for%20global%20interaction.%20Finally%2C%20we%20propose%20an%20Instance-aware%20Local%20Diffusion%20module%20that%20improves%20instance%20features%20through%20a%20generative%20process%20and%20subsequently%20refines%20the%20scene%20representation%20in%20the%20BEV%20space.%20Extensive%20experiments%20on%20the%20SemanticKITTI%20and%20SSCBench-KITTI360%20benchmarks%20demonstrate%20that%20Ocean%20achieves%20state-of-the-art%20performance%2C%20with%20mIoU%20scores%20of%2017.40%20and%2020.28%2C%20respectively.&entry.1838667208=http%3A//arxiv.org/abs/2511.13031v3&entry.124074799=Read"},
{"title": "Is Visual Realism Enough? Evaluating Gait Biometric Fidelity in Generative AI Human Animation", "author": "Ivan DeAndres-Tame and Chengwei Ye and Ruben Tolosana and Ruben Vera-Rodriguez and Shiqi Yu", "abstract": "Generative AI (GenAI) models have revolutionized animation, enabling the synthesis of humans and motion patterns with remarkable visual fidelity. However, generating truly realistic human animation remains a formidable challenge, where even minor inconsistencies can make a subject appear unnatural. This limitation is particularly critical when AI-generated videos are evaluated for behavioral biometrics, where subtle motion cues that define identity are easily lost or distorted. The present study investigates whether state-of-the-art GenAI human animation models can preserve the subtle spatio-temporal details needed for person identification through gait biometrics. Specifically, we evaluate four different GenAI models across two primary evaluation tasks to assess their ability to i) restore gait patterns from reference videos under varying conditions of complexity, and ii) transfer these gait patterns to different visual identities. Our results show that while visual quality is mostly high, biometric fidelity remains low in tasks focusing on identification, suggesting that current GenAI models struggle to disentangle identity from motion. Furthermore, through an identity transfer task, we expose a fundamental flaw in appearance-based gait recognition: when texture is disentangled from motion, identification collapses, proving current GenAI models rely on visual attributes rather than temporal dynamics.", "link": "http://arxiv.org/abs/2512.19275v1", "date": "2025-12-22", "relevancy": 2.9826, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5994}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5973}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20Visual%20Realism%20Enough%3F%20Evaluating%20Gait%20Biometric%20Fidelity%20in%20Generative%20AI%20Human%20Animation&body=Title%3A%20Is%20Visual%20Realism%20Enough%3F%20Evaluating%20Gait%20Biometric%20Fidelity%20in%20Generative%20AI%20Human%20Animation%0AAuthor%3A%20Ivan%20DeAndres-Tame%20and%20Chengwei%20Ye%20and%20Ruben%20Tolosana%20and%20Ruben%20Vera-Rodriguez%20and%20Shiqi%20Yu%0AAbstract%3A%20Generative%20AI%20%28GenAI%29%20models%20have%20revolutionized%20animation%2C%20enabling%20the%20synthesis%20of%20humans%20and%20motion%20patterns%20with%20remarkable%20visual%20fidelity.%20However%2C%20generating%20truly%20realistic%20human%20animation%20remains%20a%20formidable%20challenge%2C%20where%20even%20minor%20inconsistencies%20can%20make%20a%20subject%20appear%20unnatural.%20This%20limitation%20is%20particularly%20critical%20when%20AI-generated%20videos%20are%20evaluated%20for%20behavioral%20biometrics%2C%20where%20subtle%20motion%20cues%20that%20define%20identity%20are%20easily%20lost%20or%20distorted.%20The%20present%20study%20investigates%20whether%20state-of-the-art%20GenAI%20human%20animation%20models%20can%20preserve%20the%20subtle%20spatio-temporal%20details%20needed%20for%20person%20identification%20through%20gait%20biometrics.%20Specifically%2C%20we%20evaluate%20four%20different%20GenAI%20models%20across%20two%20primary%20evaluation%20tasks%20to%20assess%20their%20ability%20to%20i%29%20restore%20gait%20patterns%20from%20reference%20videos%20under%20varying%20conditions%20of%20complexity%2C%20and%20ii%29%20transfer%20these%20gait%20patterns%20to%20different%20visual%20identities.%20Our%20results%20show%20that%20while%20visual%20quality%20is%20mostly%20high%2C%20biometric%20fidelity%20remains%20low%20in%20tasks%20focusing%20on%20identification%2C%20suggesting%20that%20current%20GenAI%20models%20struggle%20to%20disentangle%20identity%20from%20motion.%20Furthermore%2C%20through%20an%20identity%20transfer%20task%2C%20we%20expose%20a%20fundamental%20flaw%20in%20appearance-based%20gait%20recognition%3A%20when%20texture%20is%20disentangled%20from%20motion%2C%20identification%20collapses%2C%20proving%20current%20GenAI%20models%20rely%20on%20visual%20attributes%20rather%20than%20temporal%20dynamics.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19275v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520Visual%2520Realism%2520Enough%253F%2520Evaluating%2520Gait%2520Biometric%2520Fidelity%2520in%2520Generative%2520AI%2520Human%2520Animation%26entry.906535625%3DIvan%2520DeAndres-Tame%2520and%2520Chengwei%2520Ye%2520and%2520Ruben%2520Tolosana%2520and%2520Ruben%2520Vera-Rodriguez%2520and%2520Shiqi%2520Yu%26entry.1292438233%3DGenerative%2520AI%2520%2528GenAI%2529%2520models%2520have%2520revolutionized%2520animation%252C%2520enabling%2520the%2520synthesis%2520of%2520humans%2520and%2520motion%2520patterns%2520with%2520remarkable%2520visual%2520fidelity.%2520However%252C%2520generating%2520truly%2520realistic%2520human%2520animation%2520remains%2520a%2520formidable%2520challenge%252C%2520where%2520even%2520minor%2520inconsistencies%2520can%2520make%2520a%2520subject%2520appear%2520unnatural.%2520This%2520limitation%2520is%2520particularly%2520critical%2520when%2520AI-generated%2520videos%2520are%2520evaluated%2520for%2520behavioral%2520biometrics%252C%2520where%2520subtle%2520motion%2520cues%2520that%2520define%2520identity%2520are%2520easily%2520lost%2520or%2520distorted.%2520The%2520present%2520study%2520investigates%2520whether%2520state-of-the-art%2520GenAI%2520human%2520animation%2520models%2520can%2520preserve%2520the%2520subtle%2520spatio-temporal%2520details%2520needed%2520for%2520person%2520identification%2520through%2520gait%2520biometrics.%2520Specifically%252C%2520we%2520evaluate%2520four%2520different%2520GenAI%2520models%2520across%2520two%2520primary%2520evaluation%2520tasks%2520to%2520assess%2520their%2520ability%2520to%2520i%2529%2520restore%2520gait%2520patterns%2520from%2520reference%2520videos%2520under%2520varying%2520conditions%2520of%2520complexity%252C%2520and%2520ii%2529%2520transfer%2520these%2520gait%2520patterns%2520to%2520different%2520visual%2520identities.%2520Our%2520results%2520show%2520that%2520while%2520visual%2520quality%2520is%2520mostly%2520high%252C%2520biometric%2520fidelity%2520remains%2520low%2520in%2520tasks%2520focusing%2520on%2520identification%252C%2520suggesting%2520that%2520current%2520GenAI%2520models%2520struggle%2520to%2520disentangle%2520identity%2520from%2520motion.%2520Furthermore%252C%2520through%2520an%2520identity%2520transfer%2520task%252C%2520we%2520expose%2520a%2520fundamental%2520flaw%2520in%2520appearance-based%2520gait%2520recognition%253A%2520when%2520texture%2520is%2520disentangled%2520from%2520motion%252C%2520identification%2520collapses%252C%2520proving%2520current%2520GenAI%2520models%2520rely%2520on%2520visual%2520attributes%2520rather%2520than%2520temporal%2520dynamics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19275v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20Visual%20Realism%20Enough%3F%20Evaluating%20Gait%20Biometric%20Fidelity%20in%20Generative%20AI%20Human%20Animation&entry.906535625=Ivan%20DeAndres-Tame%20and%20Chengwei%20Ye%20and%20Ruben%20Tolosana%20and%20Ruben%20Vera-Rodriguez%20and%20Shiqi%20Yu&entry.1292438233=Generative%20AI%20%28GenAI%29%20models%20have%20revolutionized%20animation%2C%20enabling%20the%20synthesis%20of%20humans%20and%20motion%20patterns%20with%20remarkable%20visual%20fidelity.%20However%2C%20generating%20truly%20realistic%20human%20animation%20remains%20a%20formidable%20challenge%2C%20where%20even%20minor%20inconsistencies%20can%20make%20a%20subject%20appear%20unnatural.%20This%20limitation%20is%20particularly%20critical%20when%20AI-generated%20videos%20are%20evaluated%20for%20behavioral%20biometrics%2C%20where%20subtle%20motion%20cues%20that%20define%20identity%20are%20easily%20lost%20or%20distorted.%20The%20present%20study%20investigates%20whether%20state-of-the-art%20GenAI%20human%20animation%20models%20can%20preserve%20the%20subtle%20spatio-temporal%20details%20needed%20for%20person%20identification%20through%20gait%20biometrics.%20Specifically%2C%20we%20evaluate%20four%20different%20GenAI%20models%20across%20two%20primary%20evaluation%20tasks%20to%20assess%20their%20ability%20to%20i%29%20restore%20gait%20patterns%20from%20reference%20videos%20under%20varying%20conditions%20of%20complexity%2C%20and%20ii%29%20transfer%20these%20gait%20patterns%20to%20different%20visual%20identities.%20Our%20results%20show%20that%20while%20visual%20quality%20is%20mostly%20high%2C%20biometric%20fidelity%20remains%20low%20in%20tasks%20focusing%20on%20identification%2C%20suggesting%20that%20current%20GenAI%20models%20struggle%20to%20disentangle%20identity%20from%20motion.%20Furthermore%2C%20through%20an%20identity%20transfer%20task%2C%20we%20expose%20a%20fundamental%20flaw%20in%20appearance-based%20gait%20recognition%3A%20when%20texture%20is%20disentangled%20from%20motion%2C%20identification%20collapses%2C%20proving%20current%20GenAI%20models%20rely%20on%20visual%20attributes%20rather%20than%20temporal%20dynamics.&entry.1838667208=http%3A//arxiv.org/abs/2512.19275v1&entry.124074799=Read"},
{"title": "Learning Generalizable Hand-Object Tracking from Synthetic Demonstrations", "author": "Yinhuai Wang and Runyi Yu and Hok Wai Tsui and Xiaoyi Lin and Hui Zhang and Qihan Zhao and Ke Fan and Miao Li and Jie Song and Jingbo Wang and Qifeng Chen and Ping Tan", "abstract": "We present a system for learning generalizable hand-object tracking controllers purely from synthetic data, without requiring any human demonstrations. Our approach makes two key contributions: (1) HOP, a Hand-Object Planner, which can synthesize diverse hand-object trajectories; and (2) HOT, a Hand-Object Tracker that bridges synthetic-to-physical transfer through reinforcement learning and interaction imitation learning, delivering a generalizable controller conditioned on target hand-object states. Our method extends to diverse object shapes and hand morphologies. Through extensive evaluations, we show that our approach enables dexterous hands to track challenging, long-horizon sequences including object re-arrangement and agile in-hand reorientation. These results represent a significant step toward scalable foundation controllers for manipulation that can learn entirely from synthetic data, breaking the data bottleneck that has long constrained progress in dexterous manipulation.", "link": "http://arxiv.org/abs/2512.19583v1", "date": "2025-12-22", "relevancy": 2.9784, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6207}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6042}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Generalizable%20Hand-Object%20Tracking%20from%20Synthetic%20Demonstrations&body=Title%3A%20Learning%20Generalizable%20Hand-Object%20Tracking%20from%20Synthetic%20Demonstrations%0AAuthor%3A%20Yinhuai%20Wang%20and%20Runyi%20Yu%20and%20Hok%20Wai%20Tsui%20and%20Xiaoyi%20Lin%20and%20Hui%20Zhang%20and%20Qihan%20Zhao%20and%20Ke%20Fan%20and%20Miao%20Li%20and%20Jie%20Song%20and%20Jingbo%20Wang%20and%20Qifeng%20Chen%20and%20Ping%20Tan%0AAbstract%3A%20We%20present%20a%20system%20for%20learning%20generalizable%20hand-object%20tracking%20controllers%20purely%20from%20synthetic%20data%2C%20without%20requiring%20any%20human%20demonstrations.%20Our%20approach%20makes%20two%20key%20contributions%3A%20%281%29%20HOP%2C%20a%20Hand-Object%20Planner%2C%20which%20can%20synthesize%20diverse%20hand-object%20trajectories%3B%20and%20%282%29%20HOT%2C%20a%20Hand-Object%20Tracker%20that%20bridges%20synthetic-to-physical%20transfer%20through%20reinforcement%20learning%20and%20interaction%20imitation%20learning%2C%20delivering%20a%20generalizable%20controller%20conditioned%20on%20target%20hand-object%20states.%20Our%20method%20extends%20to%20diverse%20object%20shapes%20and%20hand%20morphologies.%20Through%20extensive%20evaluations%2C%20we%20show%20that%20our%20approach%20enables%20dexterous%20hands%20to%20track%20challenging%2C%20long-horizon%20sequences%20including%20object%20re-arrangement%20and%20agile%20in-hand%20reorientation.%20These%20results%20represent%20a%20significant%20step%20toward%20scalable%20foundation%20controllers%20for%20manipulation%20that%20can%20learn%20entirely%20from%20synthetic%20data%2C%20breaking%20the%20data%20bottleneck%20that%20has%20long%20constrained%20progress%20in%20dexterous%20manipulation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19583v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Generalizable%2520Hand-Object%2520Tracking%2520from%2520Synthetic%2520Demonstrations%26entry.906535625%3DYinhuai%2520Wang%2520and%2520Runyi%2520Yu%2520and%2520Hok%2520Wai%2520Tsui%2520and%2520Xiaoyi%2520Lin%2520and%2520Hui%2520Zhang%2520and%2520Qihan%2520Zhao%2520and%2520Ke%2520Fan%2520and%2520Miao%2520Li%2520and%2520Jie%2520Song%2520and%2520Jingbo%2520Wang%2520and%2520Qifeng%2520Chen%2520and%2520Ping%2520Tan%26entry.1292438233%3DWe%2520present%2520a%2520system%2520for%2520learning%2520generalizable%2520hand-object%2520tracking%2520controllers%2520purely%2520from%2520synthetic%2520data%252C%2520without%2520requiring%2520any%2520human%2520demonstrations.%2520Our%2520approach%2520makes%2520two%2520key%2520contributions%253A%2520%25281%2529%2520HOP%252C%2520a%2520Hand-Object%2520Planner%252C%2520which%2520can%2520synthesize%2520diverse%2520hand-object%2520trajectories%253B%2520and%2520%25282%2529%2520HOT%252C%2520a%2520Hand-Object%2520Tracker%2520that%2520bridges%2520synthetic-to-physical%2520transfer%2520through%2520reinforcement%2520learning%2520and%2520interaction%2520imitation%2520learning%252C%2520delivering%2520a%2520generalizable%2520controller%2520conditioned%2520on%2520target%2520hand-object%2520states.%2520Our%2520method%2520extends%2520to%2520diverse%2520object%2520shapes%2520and%2520hand%2520morphologies.%2520Through%2520extensive%2520evaluations%252C%2520we%2520show%2520that%2520our%2520approach%2520enables%2520dexterous%2520hands%2520to%2520track%2520challenging%252C%2520long-horizon%2520sequences%2520including%2520object%2520re-arrangement%2520and%2520agile%2520in-hand%2520reorientation.%2520These%2520results%2520represent%2520a%2520significant%2520step%2520toward%2520scalable%2520foundation%2520controllers%2520for%2520manipulation%2520that%2520can%2520learn%2520entirely%2520from%2520synthetic%2520data%252C%2520breaking%2520the%2520data%2520bottleneck%2520that%2520has%2520long%2520constrained%2520progress%2520in%2520dexterous%2520manipulation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19583v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Generalizable%20Hand-Object%20Tracking%20from%20Synthetic%20Demonstrations&entry.906535625=Yinhuai%20Wang%20and%20Runyi%20Yu%20and%20Hok%20Wai%20Tsui%20and%20Xiaoyi%20Lin%20and%20Hui%20Zhang%20and%20Qihan%20Zhao%20and%20Ke%20Fan%20and%20Miao%20Li%20and%20Jie%20Song%20and%20Jingbo%20Wang%20and%20Qifeng%20Chen%20and%20Ping%20Tan&entry.1292438233=We%20present%20a%20system%20for%20learning%20generalizable%20hand-object%20tracking%20controllers%20purely%20from%20synthetic%20data%2C%20without%20requiring%20any%20human%20demonstrations.%20Our%20approach%20makes%20two%20key%20contributions%3A%20%281%29%20HOP%2C%20a%20Hand-Object%20Planner%2C%20which%20can%20synthesize%20diverse%20hand-object%20trajectories%3B%20and%20%282%29%20HOT%2C%20a%20Hand-Object%20Tracker%20that%20bridges%20synthetic-to-physical%20transfer%20through%20reinforcement%20learning%20and%20interaction%20imitation%20learning%2C%20delivering%20a%20generalizable%20controller%20conditioned%20on%20target%20hand-object%20states.%20Our%20method%20extends%20to%20diverse%20object%20shapes%20and%20hand%20morphologies.%20Through%20extensive%20evaluations%2C%20we%20show%20that%20our%20approach%20enables%20dexterous%20hands%20to%20track%20challenging%2C%20long-horizon%20sequences%20including%20object%20re-arrangement%20and%20agile%20in-hand%20reorientation.%20These%20results%20represent%20a%20significant%20step%20toward%20scalable%20foundation%20controllers%20for%20manipulation%20that%20can%20learn%20entirely%20from%20synthetic%20data%2C%20breaking%20the%20data%20bottleneck%20that%20has%20long%20constrained%20progress%20in%20dexterous%20manipulation.&entry.1838667208=http%3A//arxiv.org/abs/2512.19583v1&entry.124074799=Read"},
{"title": "Skeleton-Snippet Contrastive Learning with Multiscale Feature Fusion for Action Localization", "author": "Qiushuo Cheng and Jingjing Liu and Catherine Morgan and Alan Whone and Majid Mirmehdi", "abstract": "The self-supervised pretraining paradigm has achieved great success in learning 3D action representations for skeleton-based action recognition using contrastive learning. However, learning effective representations for skeleton-based temporal action localization remains challenging and underexplored. Unlike video-level {action} recognition, detecting action boundaries requires temporally sensitive features that capture subtle differences between adjacent frames where labels change. To this end, we formulate a snippet discrimination pretext task for self-supervised pretraining, which densely projects skeleton sequences into non-overlapping segments and promotes features that distinguish them across videos via contrastive learning. Additionally, we build on strong backbones of skeleton-based action recognition models by fusing intermediate features with a U-shaped module to enhance feature resolution for frame-level localization. Our approach consistently improves existing skeleton-based contrastive learning methods for action localization on BABEL across diverse subsets and evaluation protocols. We also achieve state-of-the-art transfer learning performance on PKUMMD with pretraining on NTU RGB+D and BABEL.", "link": "http://arxiv.org/abs/2512.16504v2", "date": "2025-12-22", "relevancy": 2.9498, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6299}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5889}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.551}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Skeleton-Snippet%20Contrastive%20Learning%20with%20Multiscale%20Feature%20Fusion%20for%20Action%20Localization&body=Title%3A%20Skeleton-Snippet%20Contrastive%20Learning%20with%20Multiscale%20Feature%20Fusion%20for%20Action%20Localization%0AAuthor%3A%20Qiushuo%20Cheng%20and%20Jingjing%20Liu%20and%20Catherine%20Morgan%20and%20Alan%20Whone%20and%20Majid%20Mirmehdi%0AAbstract%3A%20The%20self-supervised%20pretraining%20paradigm%20has%20achieved%20great%20success%20in%20learning%203D%20action%20representations%20for%20skeleton-based%20action%20recognition%20using%20contrastive%20learning.%20However%2C%20learning%20effective%20representations%20for%20skeleton-based%20temporal%20action%20localization%20remains%20challenging%20and%20underexplored.%20Unlike%20video-level%20%7Baction%7D%20recognition%2C%20detecting%20action%20boundaries%20requires%20temporally%20sensitive%20features%20that%20capture%20subtle%20differences%20between%20adjacent%20frames%20where%20labels%20change.%20To%20this%20end%2C%20we%20formulate%20a%20snippet%20discrimination%20pretext%20task%20for%20self-supervised%20pretraining%2C%20which%20densely%20projects%20skeleton%20sequences%20into%20non-overlapping%20segments%20and%20promotes%20features%20that%20distinguish%20them%20across%20videos%20via%20contrastive%20learning.%20Additionally%2C%20we%20build%20on%20strong%20backbones%20of%20skeleton-based%20action%20recognition%20models%20by%20fusing%20intermediate%20features%20with%20a%20U-shaped%20module%20to%20enhance%20feature%20resolution%20for%20frame-level%20localization.%20Our%20approach%20consistently%20improves%20existing%20skeleton-based%20contrastive%20learning%20methods%20for%20action%20localization%20on%20BABEL%20across%20diverse%20subsets%20and%20evaluation%20protocols.%20We%20also%20achieve%20state-of-the-art%20transfer%20learning%20performance%20on%20PKUMMD%20with%20pretraining%20on%20NTU%20RGB%2BD%20and%20BABEL.%0ALink%3A%20http%3A//arxiv.org/abs/2512.16504v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSkeleton-Snippet%2520Contrastive%2520Learning%2520with%2520Multiscale%2520Feature%2520Fusion%2520for%2520Action%2520Localization%26entry.906535625%3DQiushuo%2520Cheng%2520and%2520Jingjing%2520Liu%2520and%2520Catherine%2520Morgan%2520and%2520Alan%2520Whone%2520and%2520Majid%2520Mirmehdi%26entry.1292438233%3DThe%2520self-supervised%2520pretraining%2520paradigm%2520has%2520achieved%2520great%2520success%2520in%2520learning%25203D%2520action%2520representations%2520for%2520skeleton-based%2520action%2520recognition%2520using%2520contrastive%2520learning.%2520However%252C%2520learning%2520effective%2520representations%2520for%2520skeleton-based%2520temporal%2520action%2520localization%2520remains%2520challenging%2520and%2520underexplored.%2520Unlike%2520video-level%2520%257Baction%257D%2520recognition%252C%2520detecting%2520action%2520boundaries%2520requires%2520temporally%2520sensitive%2520features%2520that%2520capture%2520subtle%2520differences%2520between%2520adjacent%2520frames%2520where%2520labels%2520change.%2520To%2520this%2520end%252C%2520we%2520formulate%2520a%2520snippet%2520discrimination%2520pretext%2520task%2520for%2520self-supervised%2520pretraining%252C%2520which%2520densely%2520projects%2520skeleton%2520sequences%2520into%2520non-overlapping%2520segments%2520and%2520promotes%2520features%2520that%2520distinguish%2520them%2520across%2520videos%2520via%2520contrastive%2520learning.%2520Additionally%252C%2520we%2520build%2520on%2520strong%2520backbones%2520of%2520skeleton-based%2520action%2520recognition%2520models%2520by%2520fusing%2520intermediate%2520features%2520with%2520a%2520U-shaped%2520module%2520to%2520enhance%2520feature%2520resolution%2520for%2520frame-level%2520localization.%2520Our%2520approach%2520consistently%2520improves%2520existing%2520skeleton-based%2520contrastive%2520learning%2520methods%2520for%2520action%2520localization%2520on%2520BABEL%2520across%2520diverse%2520subsets%2520and%2520evaluation%2520protocols.%2520We%2520also%2520achieve%2520state-of-the-art%2520transfer%2520learning%2520performance%2520on%2520PKUMMD%2520with%2520pretraining%2520on%2520NTU%2520RGB%252BD%2520and%2520BABEL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.16504v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Skeleton-Snippet%20Contrastive%20Learning%20with%20Multiscale%20Feature%20Fusion%20for%20Action%20Localization&entry.906535625=Qiushuo%20Cheng%20and%20Jingjing%20Liu%20and%20Catherine%20Morgan%20and%20Alan%20Whone%20and%20Majid%20Mirmehdi&entry.1292438233=The%20self-supervised%20pretraining%20paradigm%20has%20achieved%20great%20success%20in%20learning%203D%20action%20representations%20for%20skeleton-based%20action%20recognition%20using%20contrastive%20learning.%20However%2C%20learning%20effective%20representations%20for%20skeleton-based%20temporal%20action%20localization%20remains%20challenging%20and%20underexplored.%20Unlike%20video-level%20%7Baction%7D%20recognition%2C%20detecting%20action%20boundaries%20requires%20temporally%20sensitive%20features%20that%20capture%20subtle%20differences%20between%20adjacent%20frames%20where%20labels%20change.%20To%20this%20end%2C%20we%20formulate%20a%20snippet%20discrimination%20pretext%20task%20for%20self-supervised%20pretraining%2C%20which%20densely%20projects%20skeleton%20sequences%20into%20non-overlapping%20segments%20and%20promotes%20features%20that%20distinguish%20them%20across%20videos%20via%20contrastive%20learning.%20Additionally%2C%20we%20build%20on%20strong%20backbones%20of%20skeleton-based%20action%20recognition%20models%20by%20fusing%20intermediate%20features%20with%20a%20U-shaped%20module%20to%20enhance%20feature%20resolution%20for%20frame-level%20localization.%20Our%20approach%20consistently%20improves%20existing%20skeleton-based%20contrastive%20learning%20methods%20for%20action%20localization%20on%20BABEL%20across%20diverse%20subsets%20and%20evaluation%20protocols.%20We%20also%20achieve%20state-of-the-art%20transfer%20learning%20performance%20on%20PKUMMD%20with%20pretraining%20on%20NTU%20RGB%2BD%20and%20BABEL.&entry.1838667208=http%3A//arxiv.org/abs/2512.16504v2&entry.124074799=Read"},
{"title": "PRISM-Loc: a Lightweight Long-range LiDAR Localization in Urban Environments with Topological Maps", "author": "Kirill Muravyev and Artem Kobozev and Vasily Yuryev and Alexander Melekhin and Oleg Bulichev and Dmitry Yudin and Konstantin Yakovlev", "abstract": "We propose PRISM-Loc - a lightweight and robust approach for localization in large outdoor environments that combines a compact topological representation with a novel scan-matching and curb-detection module operating on raw LiDAR scans. The method is designed for resource-constrained platforms and emphasizes real-time performance and resilience to common urban sensing challenges. It provides accurate localization in compact topological maps using global place recognition and an original scan matching technique. Experiments on standard benchmarks and on an embedded platform demonstrate the effectiveness of our approach. Our method achieves a 99\\% success rate on the large-scale ITLP-Campus dataset while running at 150 ms per localization and using a 20 MB map for localization. We highlight three main contributions: (1) a compact representation for city-scale localization; (2) a novel curb detection and scan matching pipeline operating directly on raw LiDAR points; (3) a thorough evaluation of our method with performance analysis.", "link": "http://arxiv.org/abs/2506.15849v2", "date": "2025-12-22", "relevancy": 2.9101, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6173}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5686}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PRISM-Loc%3A%20a%20Lightweight%20Long-range%20LiDAR%20Localization%20in%20Urban%20Environments%20with%20Topological%20Maps&body=Title%3A%20PRISM-Loc%3A%20a%20Lightweight%20Long-range%20LiDAR%20Localization%20in%20Urban%20Environments%20with%20Topological%20Maps%0AAuthor%3A%20Kirill%20Muravyev%20and%20Artem%20Kobozev%20and%20Vasily%20Yuryev%20and%20Alexander%20Melekhin%20and%20Oleg%20Bulichev%20and%20Dmitry%20Yudin%20and%20Konstantin%20Yakovlev%0AAbstract%3A%20We%20propose%20PRISM-Loc%20-%20a%20lightweight%20and%20robust%20approach%20for%20localization%20in%20large%20outdoor%20environments%20that%20combines%20a%20compact%20topological%20representation%20with%20a%20novel%20scan-matching%20and%20curb-detection%20module%20operating%20on%20raw%20LiDAR%20scans.%20The%20method%20is%20designed%20for%20resource-constrained%20platforms%20and%20emphasizes%20real-time%20performance%20and%20resilience%20to%20common%20urban%20sensing%20challenges.%20It%20provides%20accurate%20localization%20in%20compact%20topological%20maps%20using%20global%20place%20recognition%20and%20an%20original%20scan%20matching%20technique.%20Experiments%20on%20standard%20benchmarks%20and%20on%20an%20embedded%20platform%20demonstrate%20the%20effectiveness%20of%20our%20approach.%20Our%20method%20achieves%20a%2099%5C%25%20success%20rate%20on%20the%20large-scale%20ITLP-Campus%20dataset%20while%20running%20at%20150%20ms%20per%20localization%20and%20using%20a%2020%20MB%20map%20for%20localization.%20We%20highlight%20three%20main%20contributions%3A%20%281%29%20a%20compact%20representation%20for%20city-scale%20localization%3B%20%282%29%20a%20novel%20curb%20detection%20and%20scan%20matching%20pipeline%20operating%20directly%20on%20raw%20LiDAR%20points%3B%20%283%29%20a%20thorough%20evaluation%20of%20our%20method%20with%20performance%20analysis.%0ALink%3A%20http%3A//arxiv.org/abs/2506.15849v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPRISM-Loc%253A%2520a%2520Lightweight%2520Long-range%2520LiDAR%2520Localization%2520in%2520Urban%2520Environments%2520with%2520Topological%2520Maps%26entry.906535625%3DKirill%2520Muravyev%2520and%2520Artem%2520Kobozev%2520and%2520Vasily%2520Yuryev%2520and%2520Alexander%2520Melekhin%2520and%2520Oleg%2520Bulichev%2520and%2520Dmitry%2520Yudin%2520and%2520Konstantin%2520Yakovlev%26entry.1292438233%3DWe%2520propose%2520PRISM-Loc%2520-%2520a%2520lightweight%2520and%2520robust%2520approach%2520for%2520localization%2520in%2520large%2520outdoor%2520environments%2520that%2520combines%2520a%2520compact%2520topological%2520representation%2520with%2520a%2520novel%2520scan-matching%2520and%2520curb-detection%2520module%2520operating%2520on%2520raw%2520LiDAR%2520scans.%2520The%2520method%2520is%2520designed%2520for%2520resource-constrained%2520platforms%2520and%2520emphasizes%2520real-time%2520performance%2520and%2520resilience%2520to%2520common%2520urban%2520sensing%2520challenges.%2520It%2520provides%2520accurate%2520localization%2520in%2520compact%2520topological%2520maps%2520using%2520global%2520place%2520recognition%2520and%2520an%2520original%2520scan%2520matching%2520technique.%2520Experiments%2520on%2520standard%2520benchmarks%2520and%2520on%2520an%2520embedded%2520platform%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach.%2520Our%2520method%2520achieves%2520a%252099%255C%2525%2520success%2520rate%2520on%2520the%2520large-scale%2520ITLP-Campus%2520dataset%2520while%2520running%2520at%2520150%2520ms%2520per%2520localization%2520and%2520using%2520a%252020%2520MB%2520map%2520for%2520localization.%2520We%2520highlight%2520three%2520main%2520contributions%253A%2520%25281%2529%2520a%2520compact%2520representation%2520for%2520city-scale%2520localization%253B%2520%25282%2529%2520a%2520novel%2520curb%2520detection%2520and%2520scan%2520matching%2520pipeline%2520operating%2520directly%2520on%2520raw%2520LiDAR%2520points%253B%2520%25283%2529%2520a%2520thorough%2520evaluation%2520of%2520our%2520method%2520with%2520performance%2520analysis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.15849v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PRISM-Loc%3A%20a%20Lightweight%20Long-range%20LiDAR%20Localization%20in%20Urban%20Environments%20with%20Topological%20Maps&entry.906535625=Kirill%20Muravyev%20and%20Artem%20Kobozev%20and%20Vasily%20Yuryev%20and%20Alexander%20Melekhin%20and%20Oleg%20Bulichev%20and%20Dmitry%20Yudin%20and%20Konstantin%20Yakovlev&entry.1292438233=We%20propose%20PRISM-Loc%20-%20a%20lightweight%20and%20robust%20approach%20for%20localization%20in%20large%20outdoor%20environments%20that%20combines%20a%20compact%20topological%20representation%20with%20a%20novel%20scan-matching%20and%20curb-detection%20module%20operating%20on%20raw%20LiDAR%20scans.%20The%20method%20is%20designed%20for%20resource-constrained%20platforms%20and%20emphasizes%20real-time%20performance%20and%20resilience%20to%20common%20urban%20sensing%20challenges.%20It%20provides%20accurate%20localization%20in%20compact%20topological%20maps%20using%20global%20place%20recognition%20and%20an%20original%20scan%20matching%20technique.%20Experiments%20on%20standard%20benchmarks%20and%20on%20an%20embedded%20platform%20demonstrate%20the%20effectiveness%20of%20our%20approach.%20Our%20method%20achieves%20a%2099%5C%25%20success%20rate%20on%20the%20large-scale%20ITLP-Campus%20dataset%20while%20running%20at%20150%20ms%20per%20localization%20and%20using%20a%2020%20MB%20map%20for%20localization.%20We%20highlight%20three%20main%20contributions%3A%20%281%29%20a%20compact%20representation%20for%20city-scale%20localization%3B%20%282%29%20a%20novel%20curb%20detection%20and%20scan%20matching%20pipeline%20operating%20directly%20on%20raw%20LiDAR%20points%3B%20%283%29%20a%20thorough%20evaluation%20of%20our%20method%20with%20performance%20analysis.&entry.1838667208=http%3A//arxiv.org/abs/2506.15849v2&entry.124074799=Read"},
{"title": "A Comprehensive Survey on Generative AI for Video-to-Music Generation", "author": "Shulei Ji and Songruoyao Wu and Zihao Wang and Shuyu Li and Kejun Zhang", "abstract": "The burgeoning growth of video-to-music generation can be attributed to the ascendancy of multimodal generative models. However, there is a lack of literature that comprehensively combs through the work in this field. To fill this gap, this paper presents a comprehensive review of video-to-music generation using deep generative AI techniques, focusing on three key components: conditioning input construction, conditioning mechanism, and music generation frameworks. We categorize existing approaches based on their designs for each component, clarifying the roles of different strategies. Preceding this, we provide a fine-grained categorization of video and music modalities, illustrating how different categories influence the design of components within the generation pipelines. Furthermore, we summarize available multimodal datasets and evaluation metrics while highlighting ongoing challenges in the field.", "link": "http://arxiv.org/abs/2502.12489v2", "date": "2025-12-22", "relevancy": 2.9023, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.605}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5741}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5623}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Survey%20on%20Generative%20AI%20for%20Video-to-Music%20Generation&body=Title%3A%20A%20Comprehensive%20Survey%20on%20Generative%20AI%20for%20Video-to-Music%20Generation%0AAuthor%3A%20Shulei%20Ji%20and%20Songruoyao%20Wu%20and%20Zihao%20Wang%20and%20Shuyu%20Li%20and%20Kejun%20Zhang%0AAbstract%3A%20The%20burgeoning%20growth%20of%20video-to-music%20generation%20can%20be%20attributed%20to%20the%20ascendancy%20of%20multimodal%20generative%20models.%20However%2C%20there%20is%20a%20lack%20of%20literature%20that%20comprehensively%20combs%20through%20the%20work%20in%20this%20field.%20To%20fill%20this%20gap%2C%20this%20paper%20presents%20a%20comprehensive%20review%20of%20video-to-music%20generation%20using%20deep%20generative%20AI%20techniques%2C%20focusing%20on%20three%20key%20components%3A%20conditioning%20input%20construction%2C%20conditioning%20mechanism%2C%20and%20music%20generation%20frameworks.%20We%20categorize%20existing%20approaches%20based%20on%20their%20designs%20for%20each%20component%2C%20clarifying%20the%20roles%20of%20different%20strategies.%20Preceding%20this%2C%20we%20provide%20a%20fine-grained%20categorization%20of%20video%20and%20music%20modalities%2C%20illustrating%20how%20different%20categories%20influence%20the%20design%20of%20components%20within%20the%20generation%20pipelines.%20Furthermore%2C%20we%20summarize%20available%20multimodal%20datasets%20and%20evaluation%20metrics%20while%20highlighting%20ongoing%20challenges%20in%20the%20field.%0ALink%3A%20http%3A//arxiv.org/abs/2502.12489v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Survey%2520on%2520Generative%2520AI%2520for%2520Video-to-Music%2520Generation%26entry.906535625%3DShulei%2520Ji%2520and%2520Songruoyao%2520Wu%2520and%2520Zihao%2520Wang%2520and%2520Shuyu%2520Li%2520and%2520Kejun%2520Zhang%26entry.1292438233%3DThe%2520burgeoning%2520growth%2520of%2520video-to-music%2520generation%2520can%2520be%2520attributed%2520to%2520the%2520ascendancy%2520of%2520multimodal%2520generative%2520models.%2520However%252C%2520there%2520is%2520a%2520lack%2520of%2520literature%2520that%2520comprehensively%2520combs%2520through%2520the%2520work%2520in%2520this%2520field.%2520To%2520fill%2520this%2520gap%252C%2520this%2520paper%2520presents%2520a%2520comprehensive%2520review%2520of%2520video-to-music%2520generation%2520using%2520deep%2520generative%2520AI%2520techniques%252C%2520focusing%2520on%2520three%2520key%2520components%253A%2520conditioning%2520input%2520construction%252C%2520conditioning%2520mechanism%252C%2520and%2520music%2520generation%2520frameworks.%2520We%2520categorize%2520existing%2520approaches%2520based%2520on%2520their%2520designs%2520for%2520each%2520component%252C%2520clarifying%2520the%2520roles%2520of%2520different%2520strategies.%2520Preceding%2520this%252C%2520we%2520provide%2520a%2520fine-grained%2520categorization%2520of%2520video%2520and%2520music%2520modalities%252C%2520illustrating%2520how%2520different%2520categories%2520influence%2520the%2520design%2520of%2520components%2520within%2520the%2520generation%2520pipelines.%2520Furthermore%252C%2520we%2520summarize%2520available%2520multimodal%2520datasets%2520and%2520evaluation%2520metrics%2520while%2520highlighting%2520ongoing%2520challenges%2520in%2520the%2520field.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12489v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Survey%20on%20Generative%20AI%20for%20Video-to-Music%20Generation&entry.906535625=Shulei%20Ji%20and%20Songruoyao%20Wu%20and%20Zihao%20Wang%20and%20Shuyu%20Li%20and%20Kejun%20Zhang&entry.1292438233=The%20burgeoning%20growth%20of%20video-to-music%20generation%20can%20be%20attributed%20to%20the%20ascendancy%20of%20multimodal%20generative%20models.%20However%2C%20there%20is%20a%20lack%20of%20literature%20that%20comprehensively%20combs%20through%20the%20work%20in%20this%20field.%20To%20fill%20this%20gap%2C%20this%20paper%20presents%20a%20comprehensive%20review%20of%20video-to-music%20generation%20using%20deep%20generative%20AI%20techniques%2C%20focusing%20on%20three%20key%20components%3A%20conditioning%20input%20construction%2C%20conditioning%20mechanism%2C%20and%20music%20generation%20frameworks.%20We%20categorize%20existing%20approaches%20based%20on%20their%20designs%20for%20each%20component%2C%20clarifying%20the%20roles%20of%20different%20strategies.%20Preceding%20this%2C%20we%20provide%20a%20fine-grained%20categorization%20of%20video%20and%20music%20modalities%2C%20illustrating%20how%20different%20categories%20influence%20the%20design%20of%20components%20within%20the%20generation%20pipelines.%20Furthermore%2C%20we%20summarize%20available%20multimodal%20datasets%20and%20evaluation%20metrics%20while%20highlighting%20ongoing%20challenges%20in%20the%20field.&entry.1838667208=http%3A//arxiv.org/abs/2502.12489v2&entry.124074799=Read"},
{"title": "Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning", "author": "Apoorv Vyas and Heng-Jui Chang and Cheng-Fu Yang and Po-Yao Huang and Luya Gao and Julius Richter and Sanyuan Chen and Matt Le and Piotr Doll\u00e1r and Christoph Feichtenhofer and Ann Lee and Wei-Ning Hsu", "abstract": "We introduce Perception Encoder Audiovisual, PE-AV, a new family of encoders for audio and video understanding trained with scaled contrastive learning. Built on PE, PE-AV makes several key contributions to extend representations to audio, and natively support joint embeddings across audio-video, audio-text, and video-text modalities. PE-AV's unified cross-modal embeddings enable novel tasks such as speech retrieval, and set a new state of the art across standard audio and video benchmarks. We unlock this by building a strong audiovisual data engine that synthesizes high-quality captions for O(100M) audio-video pairs, enabling large-scale supervision consistent across modalities. Our audio data includes speech, music, and general sound effects-avoiding single-domain limitations common in prior work. We exploit ten pairwise contrastive objectives, showing that scaling cross-modality and caption-type pairs strengthens alignment and improves zero-shot performance. We further develop PE-A-Frame by fine-tuning PE-AV with frame-level contrastive objectives, enabling fine-grained audio-frame-to-text alignment for tasks such as sound event detection.", "link": "http://arxiv.org/abs/2512.19687v1", "date": "2025-12-22", "relevancy": 2.9008, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5888}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5888}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pushing%20the%20Frontier%20of%20Audiovisual%20Perception%20with%20Large-Scale%20Multimodal%20Correspondence%20Learning&body=Title%3A%20Pushing%20the%20Frontier%20of%20Audiovisual%20Perception%20with%20Large-Scale%20Multimodal%20Correspondence%20Learning%0AAuthor%3A%20Apoorv%20Vyas%20and%20Heng-Jui%20Chang%20and%20Cheng-Fu%20Yang%20and%20Po-Yao%20Huang%20and%20Luya%20Gao%20and%20Julius%20Richter%20and%20Sanyuan%20Chen%20and%20Matt%20Le%20and%20Piotr%20Doll%C3%A1r%20and%20Christoph%20Feichtenhofer%20and%20Ann%20Lee%20and%20Wei-Ning%20Hsu%0AAbstract%3A%20We%20introduce%20Perception%20Encoder%20Audiovisual%2C%20PE-AV%2C%20a%20new%20family%20of%20encoders%20for%20audio%20and%20video%20understanding%20trained%20with%20scaled%20contrastive%20learning.%20Built%20on%20PE%2C%20PE-AV%20makes%20several%20key%20contributions%20to%20extend%20representations%20to%20audio%2C%20and%20natively%20support%20joint%20embeddings%20across%20audio-video%2C%20audio-text%2C%20and%20video-text%20modalities.%20PE-AV%27s%20unified%20cross-modal%20embeddings%20enable%20novel%20tasks%20such%20as%20speech%20retrieval%2C%20and%20set%20a%20new%20state%20of%20the%20art%20across%20standard%20audio%20and%20video%20benchmarks.%20We%20unlock%20this%20by%20building%20a%20strong%20audiovisual%20data%20engine%20that%20synthesizes%20high-quality%20captions%20for%20O%28100M%29%20audio-video%20pairs%2C%20enabling%20large-scale%20supervision%20consistent%20across%20modalities.%20Our%20audio%20data%20includes%20speech%2C%20music%2C%20and%20general%20sound%20effects-avoiding%20single-domain%20limitations%20common%20in%20prior%20work.%20We%20exploit%20ten%20pairwise%20contrastive%20objectives%2C%20showing%20that%20scaling%20cross-modality%20and%20caption-type%20pairs%20strengthens%20alignment%20and%20improves%20zero-shot%20performance.%20We%20further%20develop%20PE-A-Frame%20by%20fine-tuning%20PE-AV%20with%20frame-level%20contrastive%20objectives%2C%20enabling%20fine-grained%20audio-frame-to-text%20alignment%20for%20tasks%20such%20as%20sound%20event%20detection.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19687v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPushing%2520the%2520Frontier%2520of%2520Audiovisual%2520Perception%2520with%2520Large-Scale%2520Multimodal%2520Correspondence%2520Learning%26entry.906535625%3DApoorv%2520Vyas%2520and%2520Heng-Jui%2520Chang%2520and%2520Cheng-Fu%2520Yang%2520and%2520Po-Yao%2520Huang%2520and%2520Luya%2520Gao%2520and%2520Julius%2520Richter%2520and%2520Sanyuan%2520Chen%2520and%2520Matt%2520Le%2520and%2520Piotr%2520Doll%25C3%25A1r%2520and%2520Christoph%2520Feichtenhofer%2520and%2520Ann%2520Lee%2520and%2520Wei-Ning%2520Hsu%26entry.1292438233%3DWe%2520introduce%2520Perception%2520Encoder%2520Audiovisual%252C%2520PE-AV%252C%2520a%2520new%2520family%2520of%2520encoders%2520for%2520audio%2520and%2520video%2520understanding%2520trained%2520with%2520scaled%2520contrastive%2520learning.%2520Built%2520on%2520PE%252C%2520PE-AV%2520makes%2520several%2520key%2520contributions%2520to%2520extend%2520representations%2520to%2520audio%252C%2520and%2520natively%2520support%2520joint%2520embeddings%2520across%2520audio-video%252C%2520audio-text%252C%2520and%2520video-text%2520modalities.%2520PE-AV%2527s%2520unified%2520cross-modal%2520embeddings%2520enable%2520novel%2520tasks%2520such%2520as%2520speech%2520retrieval%252C%2520and%2520set%2520a%2520new%2520state%2520of%2520the%2520art%2520across%2520standard%2520audio%2520and%2520video%2520benchmarks.%2520We%2520unlock%2520this%2520by%2520building%2520a%2520strong%2520audiovisual%2520data%2520engine%2520that%2520synthesizes%2520high-quality%2520captions%2520for%2520O%2528100M%2529%2520audio-video%2520pairs%252C%2520enabling%2520large-scale%2520supervision%2520consistent%2520across%2520modalities.%2520Our%2520audio%2520data%2520includes%2520speech%252C%2520music%252C%2520and%2520general%2520sound%2520effects-avoiding%2520single-domain%2520limitations%2520common%2520in%2520prior%2520work.%2520We%2520exploit%2520ten%2520pairwise%2520contrastive%2520objectives%252C%2520showing%2520that%2520scaling%2520cross-modality%2520and%2520caption-type%2520pairs%2520strengthens%2520alignment%2520and%2520improves%2520zero-shot%2520performance.%2520We%2520further%2520develop%2520PE-A-Frame%2520by%2520fine-tuning%2520PE-AV%2520with%2520frame-level%2520contrastive%2520objectives%252C%2520enabling%2520fine-grained%2520audio-frame-to-text%2520alignment%2520for%2520tasks%2520such%2520as%2520sound%2520event%2520detection.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19687v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pushing%20the%20Frontier%20of%20Audiovisual%20Perception%20with%20Large-Scale%20Multimodal%20Correspondence%20Learning&entry.906535625=Apoorv%20Vyas%20and%20Heng-Jui%20Chang%20and%20Cheng-Fu%20Yang%20and%20Po-Yao%20Huang%20and%20Luya%20Gao%20and%20Julius%20Richter%20and%20Sanyuan%20Chen%20and%20Matt%20Le%20and%20Piotr%20Doll%C3%A1r%20and%20Christoph%20Feichtenhofer%20and%20Ann%20Lee%20and%20Wei-Ning%20Hsu&entry.1292438233=We%20introduce%20Perception%20Encoder%20Audiovisual%2C%20PE-AV%2C%20a%20new%20family%20of%20encoders%20for%20audio%20and%20video%20understanding%20trained%20with%20scaled%20contrastive%20learning.%20Built%20on%20PE%2C%20PE-AV%20makes%20several%20key%20contributions%20to%20extend%20representations%20to%20audio%2C%20and%20natively%20support%20joint%20embeddings%20across%20audio-video%2C%20audio-text%2C%20and%20video-text%20modalities.%20PE-AV%27s%20unified%20cross-modal%20embeddings%20enable%20novel%20tasks%20such%20as%20speech%20retrieval%2C%20and%20set%20a%20new%20state%20of%20the%20art%20across%20standard%20audio%20and%20video%20benchmarks.%20We%20unlock%20this%20by%20building%20a%20strong%20audiovisual%20data%20engine%20that%20synthesizes%20high-quality%20captions%20for%20O%28100M%29%20audio-video%20pairs%2C%20enabling%20large-scale%20supervision%20consistent%20across%20modalities.%20Our%20audio%20data%20includes%20speech%2C%20music%2C%20and%20general%20sound%20effects-avoiding%20single-domain%20limitations%20common%20in%20prior%20work.%20We%20exploit%20ten%20pairwise%20contrastive%20objectives%2C%20showing%20that%20scaling%20cross-modality%20and%20caption-type%20pairs%20strengthens%20alignment%20and%20improves%20zero-shot%20performance.%20We%20further%20develop%20PE-A-Frame%20by%20fine-tuning%20PE-AV%20with%20frame-level%20contrastive%20objectives%2C%20enabling%20fine-grained%20audio-frame-to-text%20alignment%20for%20tasks%20such%20as%20sound%20event%20detection.&entry.1838667208=http%3A//arxiv.org/abs/2512.19687v1&entry.124074799=Read"},
{"title": "From Easy to Hard: Progressive Active Learning Framework for Infrared Small Target Detection with Single Point Supervision", "author": "Chuang Yu and Jinmiao Zhao and Yunpeng Liu and Sicheng Zhao and Yimian Dai and Xiangyu Yue", "abstract": "Recently, single-frame infrared small target (SIRST) detection with single point supervision has drawn wide-spread attention. However, the latest label evolution with single point supervision (LESPS) framework suffers from instability, excessive label evolution, and difficulty in exerting embedded network performance. Inspired by organisms gradually adapting to their environment and continuously accumulating knowledge, we construct an innovative Progressive Active Learning (PAL) framework, which drives the existing SIRST detection networks progressively and actively recognizes and learns harder samples. Specifically, to avoid the early low-performance model leading to the wrong selection of hard samples, we propose a model pre-start concept, which focuses on automatically selecting a portion of easy samples and helping the model have basic task-specific learning capabilities. Meanwhile, we propose a refined dual-update strategy, which can promote reasonable learning of harder samples and continuous refinement of pseudo-labels. In addition, to alleviate the risk of excessive label evolution, a decay factor is reasonably introduced, which helps to achieve a dynamic balance between the expansion and contraction of target annotations. Extensive experiments show that existing SIRST detection networks equipped with our PAL framework have achieved state-of-the-art (SOTA) results on multiple public datasets. Furthermore, our PAL framework can build an efficient and stable bridge between full supervision and single point supervision tasks. Our code is available at https://github.com/YuChuang1205/PAL", "link": "http://arxiv.org/abs/2412.11154v3", "date": "2025-12-22", "relevancy": 2.8062, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6039}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5532}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Easy%20to%20Hard%3A%20Progressive%20Active%20Learning%20Framework%20for%20Infrared%20Small%20Target%20Detection%20with%20Single%20Point%20Supervision&body=Title%3A%20From%20Easy%20to%20Hard%3A%20Progressive%20Active%20Learning%20Framework%20for%20Infrared%20Small%20Target%20Detection%20with%20Single%20Point%20Supervision%0AAuthor%3A%20Chuang%20Yu%20and%20Jinmiao%20Zhao%20and%20Yunpeng%20Liu%20and%20Sicheng%20Zhao%20and%20Yimian%20Dai%20and%20Xiangyu%20Yue%0AAbstract%3A%20Recently%2C%20single-frame%20infrared%20small%20target%20%28SIRST%29%20detection%20with%20single%20point%20supervision%20has%20drawn%20wide-spread%20attention.%20However%2C%20the%20latest%20label%20evolution%20with%20single%20point%20supervision%20%28LESPS%29%20framework%20suffers%20from%20instability%2C%20excessive%20label%20evolution%2C%20and%20difficulty%20in%20exerting%20embedded%20network%20performance.%20Inspired%20by%20organisms%20gradually%20adapting%20to%20their%20environment%20and%20continuously%20accumulating%20knowledge%2C%20we%20construct%20an%20innovative%20Progressive%20Active%20Learning%20%28PAL%29%20framework%2C%20which%20drives%20the%20existing%20SIRST%20detection%20networks%20progressively%20and%20actively%20recognizes%20and%20learns%20harder%20samples.%20Specifically%2C%20to%20avoid%20the%20early%20low-performance%20model%20leading%20to%20the%20wrong%20selection%20of%20hard%20samples%2C%20we%20propose%20a%20model%20pre-start%20concept%2C%20which%20focuses%20on%20automatically%20selecting%20a%20portion%20of%20easy%20samples%20and%20helping%20the%20model%20have%20basic%20task-specific%20learning%20capabilities.%20Meanwhile%2C%20we%20propose%20a%20refined%20dual-update%20strategy%2C%20which%20can%20promote%20reasonable%20learning%20of%20harder%20samples%20and%20continuous%20refinement%20of%20pseudo-labels.%20In%20addition%2C%20to%20alleviate%20the%20risk%20of%20excessive%20label%20evolution%2C%20a%20decay%20factor%20is%20reasonably%20introduced%2C%20which%20helps%20to%20achieve%20a%20dynamic%20balance%20between%20the%20expansion%20and%20contraction%20of%20target%20annotations.%20Extensive%20experiments%20show%20that%20existing%20SIRST%20detection%20networks%20equipped%20with%20our%20PAL%20framework%20have%20achieved%20state-of-the-art%20%28SOTA%29%20results%20on%20multiple%20public%20datasets.%20Furthermore%2C%20our%20PAL%20framework%20can%20build%20an%20efficient%20and%20stable%20bridge%20between%20full%20supervision%20and%20single%20point%20supervision%20tasks.%20Our%20code%20is%20available%20at%20https%3A//github.com/YuChuang1205/PAL%0ALink%3A%20http%3A//arxiv.org/abs/2412.11154v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Easy%2520to%2520Hard%253A%2520Progressive%2520Active%2520Learning%2520Framework%2520for%2520Infrared%2520Small%2520Target%2520Detection%2520with%2520Single%2520Point%2520Supervision%26entry.906535625%3DChuang%2520Yu%2520and%2520Jinmiao%2520Zhao%2520and%2520Yunpeng%2520Liu%2520and%2520Sicheng%2520Zhao%2520and%2520Yimian%2520Dai%2520and%2520Xiangyu%2520Yue%26entry.1292438233%3DRecently%252C%2520single-frame%2520infrared%2520small%2520target%2520%2528SIRST%2529%2520detection%2520with%2520single%2520point%2520supervision%2520has%2520drawn%2520wide-spread%2520attention.%2520However%252C%2520the%2520latest%2520label%2520evolution%2520with%2520single%2520point%2520supervision%2520%2528LESPS%2529%2520framework%2520suffers%2520from%2520instability%252C%2520excessive%2520label%2520evolution%252C%2520and%2520difficulty%2520in%2520exerting%2520embedded%2520network%2520performance.%2520Inspired%2520by%2520organisms%2520gradually%2520adapting%2520to%2520their%2520environment%2520and%2520continuously%2520accumulating%2520knowledge%252C%2520we%2520construct%2520an%2520innovative%2520Progressive%2520Active%2520Learning%2520%2528PAL%2529%2520framework%252C%2520which%2520drives%2520the%2520existing%2520SIRST%2520detection%2520networks%2520progressively%2520and%2520actively%2520recognizes%2520and%2520learns%2520harder%2520samples.%2520Specifically%252C%2520to%2520avoid%2520the%2520early%2520low-performance%2520model%2520leading%2520to%2520the%2520wrong%2520selection%2520of%2520hard%2520samples%252C%2520we%2520propose%2520a%2520model%2520pre-start%2520concept%252C%2520which%2520focuses%2520on%2520automatically%2520selecting%2520a%2520portion%2520of%2520easy%2520samples%2520and%2520helping%2520the%2520model%2520have%2520basic%2520task-specific%2520learning%2520capabilities.%2520Meanwhile%252C%2520we%2520propose%2520a%2520refined%2520dual-update%2520strategy%252C%2520which%2520can%2520promote%2520reasonable%2520learning%2520of%2520harder%2520samples%2520and%2520continuous%2520refinement%2520of%2520pseudo-labels.%2520In%2520addition%252C%2520to%2520alleviate%2520the%2520risk%2520of%2520excessive%2520label%2520evolution%252C%2520a%2520decay%2520factor%2520is%2520reasonably%2520introduced%252C%2520which%2520helps%2520to%2520achieve%2520a%2520dynamic%2520balance%2520between%2520the%2520expansion%2520and%2520contraction%2520of%2520target%2520annotations.%2520Extensive%2520experiments%2520show%2520that%2520existing%2520SIRST%2520detection%2520networks%2520equipped%2520with%2520our%2520PAL%2520framework%2520have%2520achieved%2520state-of-the-art%2520%2528SOTA%2529%2520results%2520on%2520multiple%2520public%2520datasets.%2520Furthermore%252C%2520our%2520PAL%2520framework%2520can%2520build%2520an%2520efficient%2520and%2520stable%2520bridge%2520between%2520full%2520supervision%2520and%2520single%2520point%2520supervision%2520tasks.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/YuChuang1205/PAL%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.11154v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Easy%20to%20Hard%3A%20Progressive%20Active%20Learning%20Framework%20for%20Infrared%20Small%20Target%20Detection%20with%20Single%20Point%20Supervision&entry.906535625=Chuang%20Yu%20and%20Jinmiao%20Zhao%20and%20Yunpeng%20Liu%20and%20Sicheng%20Zhao%20and%20Yimian%20Dai%20and%20Xiangyu%20Yue&entry.1292438233=Recently%2C%20single-frame%20infrared%20small%20target%20%28SIRST%29%20detection%20with%20single%20point%20supervision%20has%20drawn%20wide-spread%20attention.%20However%2C%20the%20latest%20label%20evolution%20with%20single%20point%20supervision%20%28LESPS%29%20framework%20suffers%20from%20instability%2C%20excessive%20label%20evolution%2C%20and%20difficulty%20in%20exerting%20embedded%20network%20performance.%20Inspired%20by%20organisms%20gradually%20adapting%20to%20their%20environment%20and%20continuously%20accumulating%20knowledge%2C%20we%20construct%20an%20innovative%20Progressive%20Active%20Learning%20%28PAL%29%20framework%2C%20which%20drives%20the%20existing%20SIRST%20detection%20networks%20progressively%20and%20actively%20recognizes%20and%20learns%20harder%20samples.%20Specifically%2C%20to%20avoid%20the%20early%20low-performance%20model%20leading%20to%20the%20wrong%20selection%20of%20hard%20samples%2C%20we%20propose%20a%20model%20pre-start%20concept%2C%20which%20focuses%20on%20automatically%20selecting%20a%20portion%20of%20easy%20samples%20and%20helping%20the%20model%20have%20basic%20task-specific%20learning%20capabilities.%20Meanwhile%2C%20we%20propose%20a%20refined%20dual-update%20strategy%2C%20which%20can%20promote%20reasonable%20learning%20of%20harder%20samples%20and%20continuous%20refinement%20of%20pseudo-labels.%20In%20addition%2C%20to%20alleviate%20the%20risk%20of%20excessive%20label%20evolution%2C%20a%20decay%20factor%20is%20reasonably%20introduced%2C%20which%20helps%20to%20achieve%20a%20dynamic%20balance%20between%20the%20expansion%20and%20contraction%20of%20target%20annotations.%20Extensive%20experiments%20show%20that%20existing%20SIRST%20detection%20networks%20equipped%20with%20our%20PAL%20framework%20have%20achieved%20state-of-the-art%20%28SOTA%29%20results%20on%20multiple%20public%20datasets.%20Furthermore%2C%20our%20PAL%20framework%20can%20build%20an%20efficient%20and%20stable%20bridge%20between%20full%20supervision%20and%20single%20point%20supervision%20tasks.%20Our%20code%20is%20available%20at%20https%3A//github.com/YuChuang1205/PAL&entry.1838667208=http%3A//arxiv.org/abs/2412.11154v3&entry.124074799=Read"},
{"title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding", "author": "Weichen Fan and Haiwen Diao and Quan Wang and Dahua Lin and Ziwei Liu", "abstract": "Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.", "link": "http://arxiv.org/abs/2512.19693v1", "date": "2025-12-22", "relevancy": 2.7934, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5901}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5429}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5429}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Prism%20Hypothesis%3A%20Harmonizing%20Semantic%20and%20Pixel%20Representations%20via%20Unified%20Autoencoding&body=Title%3A%20The%20Prism%20Hypothesis%3A%20Harmonizing%20Semantic%20and%20Pixel%20Representations%20via%20Unified%20Autoencoding%0AAuthor%3A%20Weichen%20Fan%20and%20Haiwen%20Diao%20and%20Quan%20Wang%20and%20Dahua%20Lin%20and%20Ziwei%20Liu%0AAbstract%3A%20Deep%20representations%20across%20modalities%20are%20inherently%20intertwined.%20In%20this%20paper%2C%20we%20systematically%20analyze%20the%20spectral%20characteristics%20of%20various%20semantic%20and%20pixel%20encoders.%20Interestingly%2C%20our%20study%20uncovers%20a%20highly%20inspiring%20and%20rarely%20explored%20correspondence%20between%20an%20encoder%27s%20feature%20spectrum%20and%20its%20functional%20role%3A%20semantic%20encoders%20primarily%20capture%20low-frequency%20components%20that%20encode%20abstract%20meaning%2C%20whereas%20pixel%20encoders%20additionally%20retain%20high-frequency%20information%20that%20conveys%20fine-grained%20detail.%20This%20heuristic%20finding%20offers%20a%20unifying%20perspective%20that%20ties%20encoder%20behavior%20to%20its%20underlying%20spectral%20structure.%20We%20define%20it%20as%20the%20Prism%20Hypothesis%2C%20where%20each%20data%20modality%20can%20be%20viewed%20as%20a%20projection%20of%20the%20natural%20world%20onto%20a%20shared%20feature%20spectrum%2C%20just%20like%20the%20prism.%20Building%20on%20this%20insight%2C%20we%20propose%20Unified%20Autoencoding%20%28UAE%29%2C%20a%20model%20that%20harmonizes%20semantic%20structure%20and%20pixel%20details%20via%20an%20innovative%20frequency-band%20modulator%2C%20enabling%20their%20seamless%20coexistence.%20Extensive%20experiments%20on%20ImageNet%20and%20MS-COCO%20benchmarks%20validate%20that%20our%20UAE%20effectively%20unifies%20semantic%20abstraction%20and%20pixel-level%20fidelity%20into%20a%20single%20latent%20space%20with%20state-of-the-art%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19693v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Prism%2520Hypothesis%253A%2520Harmonizing%2520Semantic%2520and%2520Pixel%2520Representations%2520via%2520Unified%2520Autoencoding%26entry.906535625%3DWeichen%2520Fan%2520and%2520Haiwen%2520Diao%2520and%2520Quan%2520Wang%2520and%2520Dahua%2520Lin%2520and%2520Ziwei%2520Liu%26entry.1292438233%3DDeep%2520representations%2520across%2520modalities%2520are%2520inherently%2520intertwined.%2520In%2520this%2520paper%252C%2520we%2520systematically%2520analyze%2520the%2520spectral%2520characteristics%2520of%2520various%2520semantic%2520and%2520pixel%2520encoders.%2520Interestingly%252C%2520our%2520study%2520uncovers%2520a%2520highly%2520inspiring%2520and%2520rarely%2520explored%2520correspondence%2520between%2520an%2520encoder%2527s%2520feature%2520spectrum%2520and%2520its%2520functional%2520role%253A%2520semantic%2520encoders%2520primarily%2520capture%2520low-frequency%2520components%2520that%2520encode%2520abstract%2520meaning%252C%2520whereas%2520pixel%2520encoders%2520additionally%2520retain%2520high-frequency%2520information%2520that%2520conveys%2520fine-grained%2520detail.%2520This%2520heuristic%2520finding%2520offers%2520a%2520unifying%2520perspective%2520that%2520ties%2520encoder%2520behavior%2520to%2520its%2520underlying%2520spectral%2520structure.%2520We%2520define%2520it%2520as%2520the%2520Prism%2520Hypothesis%252C%2520where%2520each%2520data%2520modality%2520can%2520be%2520viewed%2520as%2520a%2520projection%2520of%2520the%2520natural%2520world%2520onto%2520a%2520shared%2520feature%2520spectrum%252C%2520just%2520like%2520the%2520prism.%2520Building%2520on%2520this%2520insight%252C%2520we%2520propose%2520Unified%2520Autoencoding%2520%2528UAE%2529%252C%2520a%2520model%2520that%2520harmonizes%2520semantic%2520structure%2520and%2520pixel%2520details%2520via%2520an%2520innovative%2520frequency-band%2520modulator%252C%2520enabling%2520their%2520seamless%2520coexistence.%2520Extensive%2520experiments%2520on%2520ImageNet%2520and%2520MS-COCO%2520benchmarks%2520validate%2520that%2520our%2520UAE%2520effectively%2520unifies%2520semantic%2520abstraction%2520and%2520pixel-level%2520fidelity%2520into%2520a%2520single%2520latent%2520space%2520with%2520state-of-the-art%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19693v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Prism%20Hypothesis%3A%20Harmonizing%20Semantic%20and%20Pixel%20Representations%20via%20Unified%20Autoencoding&entry.906535625=Weichen%20Fan%20and%20Haiwen%20Diao%20and%20Quan%20Wang%20and%20Dahua%20Lin%20and%20Ziwei%20Liu&entry.1292438233=Deep%20representations%20across%20modalities%20are%20inherently%20intertwined.%20In%20this%20paper%2C%20we%20systematically%20analyze%20the%20spectral%20characteristics%20of%20various%20semantic%20and%20pixel%20encoders.%20Interestingly%2C%20our%20study%20uncovers%20a%20highly%20inspiring%20and%20rarely%20explored%20correspondence%20between%20an%20encoder%27s%20feature%20spectrum%20and%20its%20functional%20role%3A%20semantic%20encoders%20primarily%20capture%20low-frequency%20components%20that%20encode%20abstract%20meaning%2C%20whereas%20pixel%20encoders%20additionally%20retain%20high-frequency%20information%20that%20conveys%20fine-grained%20detail.%20This%20heuristic%20finding%20offers%20a%20unifying%20perspective%20that%20ties%20encoder%20behavior%20to%20its%20underlying%20spectral%20structure.%20We%20define%20it%20as%20the%20Prism%20Hypothesis%2C%20where%20each%20data%20modality%20can%20be%20viewed%20as%20a%20projection%20of%20the%20natural%20world%20onto%20a%20shared%20feature%20spectrum%2C%20just%20like%20the%20prism.%20Building%20on%20this%20insight%2C%20we%20propose%20Unified%20Autoencoding%20%28UAE%29%2C%20a%20model%20that%20harmonizes%20semantic%20structure%20and%20pixel%20details%20via%20an%20innovative%20frequency-band%20modulator%2C%20enabling%20their%20seamless%20coexistence.%20Extensive%20experiments%20on%20ImageNet%20and%20MS-COCO%20benchmarks%20validate%20that%20our%20UAE%20effectively%20unifies%20semantic%20abstraction%20and%20pixel-level%20fidelity%20into%20a%20single%20latent%20space%20with%20state-of-the-art%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2512.19693v1&entry.124074799=Read"},
{"title": "ActAvatar: Temporally-Aware Precise Action Control for Talking Avatars", "author": "Ziqiao Peng and Yi Chen and Yifeng Ma and Guozhen Zhang and Zhiyao Sun and Zixiang Zhou and Youliang Zhang and Zhengguang Zhou and Zhaoxin Fan and Hongyan Liu and Yuan Zhou and Qinglin Lu and Jun He", "abstract": "Despite significant advances in talking avatar generation, existing methods face critical challenges: insufficient text-following capability for diverse actions, lack of temporal alignment between actions and audio content, and dependency on additional control signals such as pose skeletons. We present ActAvatar, a framework that achieves phase-level precision in action control through textual guidance by capturing both action semantics and temporal context. Our approach introduces three core innovations: (1) Phase-Aware Cross-Attention (PACA), which decomposes prompts into a global base block and temporally-anchored phase blocks, enabling the model to concentrate on phase-relevant tokens for precise temporal-semantic alignment; (2) Progressive Audio-Visual Alignment, which aligns modality influence with the hierarchical feature learning process-early layers prioritize text for establishing action structure while deeper layers emphasize audio for refining lip movements, preventing modality interference; (3) A two-stage training strategy that first establishes robust audio-visual correspondence on diverse data, then injects action control through fine-tuning on structured annotations, maintaining both audio-visual alignment and the model's text-following capabilities. Extensive experiments demonstrate that ActAvatar significantly outperforms state-of-the-art methods in both action control and visual quality.", "link": "http://arxiv.org/abs/2512.19546v1", "date": "2025-12-22", "relevancy": 2.7733, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5908}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5366}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5366}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ActAvatar%3A%20Temporally-Aware%20Precise%20Action%20Control%20for%20Talking%20Avatars&body=Title%3A%20ActAvatar%3A%20Temporally-Aware%20Precise%20Action%20Control%20for%20Talking%20Avatars%0AAuthor%3A%20Ziqiao%20Peng%20and%20Yi%20Chen%20and%20Yifeng%20Ma%20and%20Guozhen%20Zhang%20and%20Zhiyao%20Sun%20and%20Zixiang%20Zhou%20and%20Youliang%20Zhang%20and%20Zhengguang%20Zhou%20and%20Zhaoxin%20Fan%20and%20Hongyan%20Liu%20and%20Yuan%20Zhou%20and%20Qinglin%20Lu%20and%20Jun%20He%0AAbstract%3A%20Despite%20significant%20advances%20in%20talking%20avatar%20generation%2C%20existing%20methods%20face%20critical%20challenges%3A%20insufficient%20text-following%20capability%20for%20diverse%20actions%2C%20lack%20of%20temporal%20alignment%20between%20actions%20and%20audio%20content%2C%20and%20dependency%20on%20additional%20control%20signals%20such%20as%20pose%20skeletons.%20We%20present%20ActAvatar%2C%20a%20framework%20that%20achieves%20phase-level%20precision%20in%20action%20control%20through%20textual%20guidance%20by%20capturing%20both%20action%20semantics%20and%20temporal%20context.%20Our%20approach%20introduces%20three%20core%20innovations%3A%20%281%29%20Phase-Aware%20Cross-Attention%20%28PACA%29%2C%20which%20decomposes%20prompts%20into%20a%20global%20base%20block%20and%20temporally-anchored%20phase%20blocks%2C%20enabling%20the%20model%20to%20concentrate%20on%20phase-relevant%20tokens%20for%20precise%20temporal-semantic%20alignment%3B%20%282%29%20Progressive%20Audio-Visual%20Alignment%2C%20which%20aligns%20modality%20influence%20with%20the%20hierarchical%20feature%20learning%20process-early%20layers%20prioritize%20text%20for%20establishing%20action%20structure%20while%20deeper%20layers%20emphasize%20audio%20for%20refining%20lip%20movements%2C%20preventing%20modality%20interference%3B%20%283%29%20A%20two-stage%20training%20strategy%20that%20first%20establishes%20robust%20audio-visual%20correspondence%20on%20diverse%20data%2C%20then%20injects%20action%20control%20through%20fine-tuning%20on%20structured%20annotations%2C%20maintaining%20both%20audio-visual%20alignment%20and%20the%20model%27s%20text-following%20capabilities.%20Extensive%20experiments%20demonstrate%20that%20ActAvatar%20significantly%20outperforms%20state-of-the-art%20methods%20in%20both%20action%20control%20and%20visual%20quality.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19546v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActAvatar%253A%2520Temporally-Aware%2520Precise%2520Action%2520Control%2520for%2520Talking%2520Avatars%26entry.906535625%3DZiqiao%2520Peng%2520and%2520Yi%2520Chen%2520and%2520Yifeng%2520Ma%2520and%2520Guozhen%2520Zhang%2520and%2520Zhiyao%2520Sun%2520and%2520Zixiang%2520Zhou%2520and%2520Youliang%2520Zhang%2520and%2520Zhengguang%2520Zhou%2520and%2520Zhaoxin%2520Fan%2520and%2520Hongyan%2520Liu%2520and%2520Yuan%2520Zhou%2520and%2520Qinglin%2520Lu%2520and%2520Jun%2520He%26entry.1292438233%3DDespite%2520significant%2520advances%2520in%2520talking%2520avatar%2520generation%252C%2520existing%2520methods%2520face%2520critical%2520challenges%253A%2520insufficient%2520text-following%2520capability%2520for%2520diverse%2520actions%252C%2520lack%2520of%2520temporal%2520alignment%2520between%2520actions%2520and%2520audio%2520content%252C%2520and%2520dependency%2520on%2520additional%2520control%2520signals%2520such%2520as%2520pose%2520skeletons.%2520We%2520present%2520ActAvatar%252C%2520a%2520framework%2520that%2520achieves%2520phase-level%2520precision%2520in%2520action%2520control%2520through%2520textual%2520guidance%2520by%2520capturing%2520both%2520action%2520semantics%2520and%2520temporal%2520context.%2520Our%2520approach%2520introduces%2520three%2520core%2520innovations%253A%2520%25281%2529%2520Phase-Aware%2520Cross-Attention%2520%2528PACA%2529%252C%2520which%2520decomposes%2520prompts%2520into%2520a%2520global%2520base%2520block%2520and%2520temporally-anchored%2520phase%2520blocks%252C%2520enabling%2520the%2520model%2520to%2520concentrate%2520on%2520phase-relevant%2520tokens%2520for%2520precise%2520temporal-semantic%2520alignment%253B%2520%25282%2529%2520Progressive%2520Audio-Visual%2520Alignment%252C%2520which%2520aligns%2520modality%2520influence%2520with%2520the%2520hierarchical%2520feature%2520learning%2520process-early%2520layers%2520prioritize%2520text%2520for%2520establishing%2520action%2520structure%2520while%2520deeper%2520layers%2520emphasize%2520audio%2520for%2520refining%2520lip%2520movements%252C%2520preventing%2520modality%2520interference%253B%2520%25283%2529%2520A%2520two-stage%2520training%2520strategy%2520that%2520first%2520establishes%2520robust%2520audio-visual%2520correspondence%2520on%2520diverse%2520data%252C%2520then%2520injects%2520action%2520control%2520through%2520fine-tuning%2520on%2520structured%2520annotations%252C%2520maintaining%2520both%2520audio-visual%2520alignment%2520and%2520the%2520model%2527s%2520text-following%2520capabilities.%2520Extensive%2520experiments%2520demonstrate%2520that%2520ActAvatar%2520significantly%2520outperforms%2520state-of-the-art%2520methods%2520in%2520both%2520action%2520control%2520and%2520visual%2520quality.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19546v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ActAvatar%3A%20Temporally-Aware%20Precise%20Action%20Control%20for%20Talking%20Avatars&entry.906535625=Ziqiao%20Peng%20and%20Yi%20Chen%20and%20Yifeng%20Ma%20and%20Guozhen%20Zhang%20and%20Zhiyao%20Sun%20and%20Zixiang%20Zhou%20and%20Youliang%20Zhang%20and%20Zhengguang%20Zhou%20and%20Zhaoxin%20Fan%20and%20Hongyan%20Liu%20and%20Yuan%20Zhou%20and%20Qinglin%20Lu%20and%20Jun%20He&entry.1292438233=Despite%20significant%20advances%20in%20talking%20avatar%20generation%2C%20existing%20methods%20face%20critical%20challenges%3A%20insufficient%20text-following%20capability%20for%20diverse%20actions%2C%20lack%20of%20temporal%20alignment%20between%20actions%20and%20audio%20content%2C%20and%20dependency%20on%20additional%20control%20signals%20such%20as%20pose%20skeletons.%20We%20present%20ActAvatar%2C%20a%20framework%20that%20achieves%20phase-level%20precision%20in%20action%20control%20through%20textual%20guidance%20by%20capturing%20both%20action%20semantics%20and%20temporal%20context.%20Our%20approach%20introduces%20three%20core%20innovations%3A%20%281%29%20Phase-Aware%20Cross-Attention%20%28PACA%29%2C%20which%20decomposes%20prompts%20into%20a%20global%20base%20block%20and%20temporally-anchored%20phase%20blocks%2C%20enabling%20the%20model%20to%20concentrate%20on%20phase-relevant%20tokens%20for%20precise%20temporal-semantic%20alignment%3B%20%282%29%20Progressive%20Audio-Visual%20Alignment%2C%20which%20aligns%20modality%20influence%20with%20the%20hierarchical%20feature%20learning%20process-early%20layers%20prioritize%20text%20for%20establishing%20action%20structure%20while%20deeper%20layers%20emphasize%20audio%20for%20refining%20lip%20movements%2C%20preventing%20modality%20interference%3B%20%283%29%20A%20two-stage%20training%20strategy%20that%20first%20establishes%20robust%20audio-visual%20correspondence%20on%20diverse%20data%2C%20then%20injects%20action%20control%20through%20fine-tuning%20on%20structured%20annotations%2C%20maintaining%20both%20audio-visual%20alignment%20and%20the%20model%27s%20text-following%20capabilities.%20Extensive%20experiments%20demonstrate%20that%20ActAvatar%20significantly%20outperforms%20state-of-the-art%20methods%20in%20both%20action%20control%20and%20visual%20quality.&entry.1838667208=http%3A//arxiv.org/abs/2512.19546v1&entry.124074799=Read"},
{"title": "AsyMoE: Leveraging Modal Asymmetry for Enhanced Expert Specialization in Large Vision-Language Models", "author": "Heng Zhang and Haichuan Hu and Yaomin Shen and Weihao Yu and Yilei Yuan and Haochen You and Guo Cheng and Zijian Zhang and Lubin Gan and Huihui Wei and Hao Zhang and Jin Huang", "abstract": "Large Vision-Language Models (LVLMs) have demonstrated impressive performance on multimodal tasks through scaled architectures and extensive training. However, existing Mixture of Experts (MoE) approaches face challenges due to the asymmetry between visual and linguistic processing. Visual information is spatially complete, while language requires maintaining sequential context. As a result, MoE models struggle to balance modality-specific features and cross-modal interactions. Through systematic analysis, we observe that language experts in deeper layers progressively lose contextual grounding and rely more on parametric knowledge rather than utilizing the provided visual and linguistic information. To address this, we propose AsyMoE, a novel architecture that models this asymmetry using three specialized expert groups. We design intra-modality experts for modality-specific processing, hyperbolic inter-modality experts for hierarchical cross-modal interactions, and evidence-priority language experts to suppress parametric biases and maintain contextual grounding. Extensive experiments demonstrate that AsyMoE achieves 26.58% and 15.45% accuracy improvements over vanilla MoE and modality-specific MoE respectively, with 25.45% fewer activated parameters than dense models.", "link": "http://arxiv.org/abs/2509.12715v2", "date": "2025-12-22", "relevancy": 2.7599, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5569}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5569}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AsyMoE%3A%20Leveraging%20Modal%20Asymmetry%20for%20Enhanced%20Expert%20Specialization%20in%20Large%20Vision-Language%20Models&body=Title%3A%20AsyMoE%3A%20Leveraging%20Modal%20Asymmetry%20for%20Enhanced%20Expert%20Specialization%20in%20Large%20Vision-Language%20Models%0AAuthor%3A%20Heng%20Zhang%20and%20Haichuan%20Hu%20and%20Yaomin%20Shen%20and%20Weihao%20Yu%20and%20Yilei%20Yuan%20and%20Haochen%20You%20and%20Guo%20Cheng%20and%20Zijian%20Zhang%20and%20Lubin%20Gan%20and%20Huihui%20Wei%20and%20Hao%20Zhang%20and%20Jin%20Huang%0AAbstract%3A%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20demonstrated%20impressive%20performance%20on%20multimodal%20tasks%20through%20scaled%20architectures%20and%20extensive%20training.%20However%2C%20existing%20Mixture%20of%20Experts%20%28MoE%29%20approaches%20face%20challenges%20due%20to%20the%20asymmetry%20between%20visual%20and%20linguistic%20processing.%20Visual%20information%20is%20spatially%20complete%2C%20while%20language%20requires%20maintaining%20sequential%20context.%20As%20a%20result%2C%20MoE%20models%20struggle%20to%20balance%20modality-specific%20features%20and%20cross-modal%20interactions.%20Through%20systematic%20analysis%2C%20we%20observe%20that%20language%20experts%20in%20deeper%20layers%20progressively%20lose%20contextual%20grounding%20and%20rely%20more%20on%20parametric%20knowledge%20rather%20than%20utilizing%20the%20provided%20visual%20and%20linguistic%20information.%20To%20address%20this%2C%20we%20propose%20AsyMoE%2C%20a%20novel%20architecture%20that%20models%20this%20asymmetry%20using%20three%20specialized%20expert%20groups.%20We%20design%20intra-modality%20experts%20for%20modality-specific%20processing%2C%20hyperbolic%20inter-modality%20experts%20for%20hierarchical%20cross-modal%20interactions%2C%20and%20evidence-priority%20language%20experts%20to%20suppress%20parametric%20biases%20and%20maintain%20contextual%20grounding.%20Extensive%20experiments%20demonstrate%20that%20AsyMoE%20achieves%2026.58%25%20and%2015.45%25%20accuracy%20improvements%20over%20vanilla%20MoE%20and%20modality-specific%20MoE%20respectively%2C%20with%2025.45%25%20fewer%20activated%20parameters%20than%20dense%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2509.12715v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAsyMoE%253A%2520Leveraging%2520Modal%2520Asymmetry%2520for%2520Enhanced%2520Expert%2520Specialization%2520in%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DHeng%2520Zhang%2520and%2520Haichuan%2520Hu%2520and%2520Yaomin%2520Shen%2520and%2520Weihao%2520Yu%2520and%2520Yilei%2520Yuan%2520and%2520Haochen%2520You%2520and%2520Guo%2520Cheng%2520and%2520Zijian%2520Zhang%2520and%2520Lubin%2520Gan%2520and%2520Huihui%2520Wei%2520and%2520Hao%2520Zhang%2520and%2520Jin%2520Huang%26entry.1292438233%3DLarge%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520demonstrated%2520impressive%2520performance%2520on%2520multimodal%2520tasks%2520through%2520scaled%2520architectures%2520and%2520extensive%2520training.%2520However%252C%2520existing%2520Mixture%2520of%2520Experts%2520%2528MoE%2529%2520approaches%2520face%2520challenges%2520due%2520to%2520the%2520asymmetry%2520between%2520visual%2520and%2520linguistic%2520processing.%2520Visual%2520information%2520is%2520spatially%2520complete%252C%2520while%2520language%2520requires%2520maintaining%2520sequential%2520context.%2520As%2520a%2520result%252C%2520MoE%2520models%2520struggle%2520to%2520balance%2520modality-specific%2520features%2520and%2520cross-modal%2520interactions.%2520Through%2520systematic%2520analysis%252C%2520we%2520observe%2520that%2520language%2520experts%2520in%2520deeper%2520layers%2520progressively%2520lose%2520contextual%2520grounding%2520and%2520rely%2520more%2520on%2520parametric%2520knowledge%2520rather%2520than%2520utilizing%2520the%2520provided%2520visual%2520and%2520linguistic%2520information.%2520To%2520address%2520this%252C%2520we%2520propose%2520AsyMoE%252C%2520a%2520novel%2520architecture%2520that%2520models%2520this%2520asymmetry%2520using%2520three%2520specialized%2520expert%2520groups.%2520We%2520design%2520intra-modality%2520experts%2520for%2520modality-specific%2520processing%252C%2520hyperbolic%2520inter-modality%2520experts%2520for%2520hierarchical%2520cross-modal%2520interactions%252C%2520and%2520evidence-priority%2520language%2520experts%2520to%2520suppress%2520parametric%2520biases%2520and%2520maintain%2520contextual%2520grounding.%2520Extensive%2520experiments%2520demonstrate%2520that%2520AsyMoE%2520achieves%252026.58%2525%2520and%252015.45%2525%2520accuracy%2520improvements%2520over%2520vanilla%2520MoE%2520and%2520modality-specific%2520MoE%2520respectively%252C%2520with%252025.45%2525%2520fewer%2520activated%2520parameters%2520than%2520dense%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12715v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AsyMoE%3A%20Leveraging%20Modal%20Asymmetry%20for%20Enhanced%20Expert%20Specialization%20in%20Large%20Vision-Language%20Models&entry.906535625=Heng%20Zhang%20and%20Haichuan%20Hu%20and%20Yaomin%20Shen%20and%20Weihao%20Yu%20and%20Yilei%20Yuan%20and%20Haochen%20You%20and%20Guo%20Cheng%20and%20Zijian%20Zhang%20and%20Lubin%20Gan%20and%20Huihui%20Wei%20and%20Hao%20Zhang%20and%20Jin%20Huang&entry.1292438233=Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20demonstrated%20impressive%20performance%20on%20multimodal%20tasks%20through%20scaled%20architectures%20and%20extensive%20training.%20However%2C%20existing%20Mixture%20of%20Experts%20%28MoE%29%20approaches%20face%20challenges%20due%20to%20the%20asymmetry%20between%20visual%20and%20linguistic%20processing.%20Visual%20information%20is%20spatially%20complete%2C%20while%20language%20requires%20maintaining%20sequential%20context.%20As%20a%20result%2C%20MoE%20models%20struggle%20to%20balance%20modality-specific%20features%20and%20cross-modal%20interactions.%20Through%20systematic%20analysis%2C%20we%20observe%20that%20language%20experts%20in%20deeper%20layers%20progressively%20lose%20contextual%20grounding%20and%20rely%20more%20on%20parametric%20knowledge%20rather%20than%20utilizing%20the%20provided%20visual%20and%20linguistic%20information.%20To%20address%20this%2C%20we%20propose%20AsyMoE%2C%20a%20novel%20architecture%20that%20models%20this%20asymmetry%20using%20three%20specialized%20expert%20groups.%20We%20design%20intra-modality%20experts%20for%20modality-specific%20processing%2C%20hyperbolic%20inter-modality%20experts%20for%20hierarchical%20cross-modal%20interactions%2C%20and%20evidence-priority%20language%20experts%20to%20suppress%20parametric%20biases%20and%20maintain%20contextual%20grounding.%20Extensive%20experiments%20demonstrate%20that%20AsyMoE%20achieves%2026.58%25%20and%2015.45%25%20accuracy%20improvements%20over%20vanilla%20MoE%20and%20modality-specific%20MoE%20respectively%2C%20with%2025.45%25%20fewer%20activated%20parameters%20than%20dense%20models.&entry.1838667208=http%3A//arxiv.org/abs/2509.12715v2&entry.124074799=Read"},
{"title": "VA-$\u03c0$: Variational Policy Alignment for Pixel-Aware Autoregressive Generation", "author": "Xinyao Liao and Qiyuan He and Kai Xu and Xiaoye Qu and Yicong Li and Wei Wei and Angela Yao", "abstract": "Autoregressive (AR) visual generation relies on tokenizers to map images to and from discrete sequences. However, tokenizers are trained to reconstruct clean images from ground-truth tokens, while AR generators are optimized only for token likelihood. This misalignment leads to generated token sequences that may decode into low-quality images, without direct supervision from the pixel space. We propose VA-$\u03c0$, a lightweight post-training framework that directly optimizes AR models with a principled pixel-space objective. VA-$\u03c0$ formulates the generator-tokenizer alignment as a variational optimization, deriving an evidence lower bound (ELBO) that unifies pixel reconstruction and autoregressive modeling. To optimize under the discrete token space, VA-$\u03c0$ introduces a reinforcement-based alignment strategy that treats the AR generator as a policy, uses pixel-space reconstruction quality as its intrinsic reward. The reward is measured by how well the predicted token sequences can reconstruct the original image under teacher forcing, giving the model direct pixel-level guidance without expensive free-running sampling. The regularization term of the ELBO serves as a natural regularizer, maintaining distributional consistency of tokens. VA-$\u03c0$ enables rapid adaptation of existing AR generators, without neither tokenizer retraining nor external reward models. With only 1% ImageNet-1K data and 25 minutes of tuning, it reduces FID from 14.36 to 7.65 and improves IS from 86.55 to 116.70 on LlamaGen-XXL, while also yielding notable gains in the text-to-image task on GenEval for both visual generation model (LlamaGen: from 0.306 to 0.339) and unified multi-modal model (Janus-Pro: from 0.725 to 0.744). Code is available at https://github.com/Lil-Shake/VA-Pi.", "link": "http://arxiv.org/abs/2512.19680v1", "date": "2025-12-22", "relevancy": 2.7554, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5915}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5326}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VA-%24%CF%80%24%3A%20Variational%20Policy%20Alignment%20for%20Pixel-Aware%20Autoregressive%20Generation&body=Title%3A%20VA-%24%CF%80%24%3A%20Variational%20Policy%20Alignment%20for%20Pixel-Aware%20Autoregressive%20Generation%0AAuthor%3A%20Xinyao%20Liao%20and%20Qiyuan%20He%20and%20Kai%20Xu%20and%20Xiaoye%20Qu%20and%20Yicong%20Li%20and%20Wei%20Wei%20and%20Angela%20Yao%0AAbstract%3A%20Autoregressive%20%28AR%29%20visual%20generation%20relies%20on%20tokenizers%20to%20map%20images%20to%20and%20from%20discrete%20sequences.%20However%2C%20tokenizers%20are%20trained%20to%20reconstruct%20clean%20images%20from%20ground-truth%20tokens%2C%20while%20AR%20generators%20are%20optimized%20only%20for%20token%20likelihood.%20This%20misalignment%20leads%20to%20generated%20token%20sequences%20that%20may%20decode%20into%20low-quality%20images%2C%20without%20direct%20supervision%20from%20the%20pixel%20space.%20We%20propose%20VA-%24%CF%80%24%2C%20a%20lightweight%20post-training%20framework%20that%20directly%20optimizes%20AR%20models%20with%20a%20principled%20pixel-space%20objective.%20VA-%24%CF%80%24%20formulates%20the%20generator-tokenizer%20alignment%20as%20a%20variational%20optimization%2C%20deriving%20an%20evidence%20lower%20bound%20%28ELBO%29%20that%20unifies%20pixel%20reconstruction%20and%20autoregressive%20modeling.%20To%20optimize%20under%20the%20discrete%20token%20space%2C%20VA-%24%CF%80%24%20introduces%20a%20reinforcement-based%20alignment%20strategy%20that%20treats%20the%20AR%20generator%20as%20a%20policy%2C%20uses%20pixel-space%20reconstruction%20quality%20as%20its%20intrinsic%20reward.%20The%20reward%20is%20measured%20by%20how%20well%20the%20predicted%20token%20sequences%20can%20reconstruct%20the%20original%20image%20under%20teacher%20forcing%2C%20giving%20the%20model%20direct%20pixel-level%20guidance%20without%20expensive%20free-running%20sampling.%20The%20regularization%20term%20of%20the%20ELBO%20serves%20as%20a%20natural%20regularizer%2C%20maintaining%20distributional%20consistency%20of%20tokens.%20VA-%24%CF%80%24%20enables%20rapid%20adaptation%20of%20existing%20AR%20generators%2C%20without%20neither%20tokenizer%20retraining%20nor%20external%20reward%20models.%20With%20only%201%25%20ImageNet-1K%20data%20and%2025%20minutes%20of%20tuning%2C%20it%20reduces%20FID%20from%2014.36%20to%207.65%20and%20improves%20IS%20from%2086.55%20to%20116.70%20on%20LlamaGen-XXL%2C%20while%20also%20yielding%20notable%20gains%20in%20the%20text-to-image%20task%20on%20GenEval%20for%20both%20visual%20generation%20model%20%28LlamaGen%3A%20from%200.306%20to%200.339%29%20and%20unified%20multi-modal%20model%20%28Janus-Pro%3A%20from%200.725%20to%200.744%29.%20Code%20is%20available%20at%20https%3A//github.com/Lil-Shake/VA-Pi.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19680v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVA-%2524%25CF%2580%2524%253A%2520Variational%2520Policy%2520Alignment%2520for%2520Pixel-Aware%2520Autoregressive%2520Generation%26entry.906535625%3DXinyao%2520Liao%2520and%2520Qiyuan%2520He%2520and%2520Kai%2520Xu%2520and%2520Xiaoye%2520Qu%2520and%2520Yicong%2520Li%2520and%2520Wei%2520Wei%2520and%2520Angela%2520Yao%26entry.1292438233%3DAutoregressive%2520%2528AR%2529%2520visual%2520generation%2520relies%2520on%2520tokenizers%2520to%2520map%2520images%2520to%2520and%2520from%2520discrete%2520sequences.%2520However%252C%2520tokenizers%2520are%2520trained%2520to%2520reconstruct%2520clean%2520images%2520from%2520ground-truth%2520tokens%252C%2520while%2520AR%2520generators%2520are%2520optimized%2520only%2520for%2520token%2520likelihood.%2520This%2520misalignment%2520leads%2520to%2520generated%2520token%2520sequences%2520that%2520may%2520decode%2520into%2520low-quality%2520images%252C%2520without%2520direct%2520supervision%2520from%2520the%2520pixel%2520space.%2520We%2520propose%2520VA-%2524%25CF%2580%2524%252C%2520a%2520lightweight%2520post-training%2520framework%2520that%2520directly%2520optimizes%2520AR%2520models%2520with%2520a%2520principled%2520pixel-space%2520objective.%2520VA-%2524%25CF%2580%2524%2520formulates%2520the%2520generator-tokenizer%2520alignment%2520as%2520a%2520variational%2520optimization%252C%2520deriving%2520an%2520evidence%2520lower%2520bound%2520%2528ELBO%2529%2520that%2520unifies%2520pixel%2520reconstruction%2520and%2520autoregressive%2520modeling.%2520To%2520optimize%2520under%2520the%2520discrete%2520token%2520space%252C%2520VA-%2524%25CF%2580%2524%2520introduces%2520a%2520reinforcement-based%2520alignment%2520strategy%2520that%2520treats%2520the%2520AR%2520generator%2520as%2520a%2520policy%252C%2520uses%2520pixel-space%2520reconstruction%2520quality%2520as%2520its%2520intrinsic%2520reward.%2520The%2520reward%2520is%2520measured%2520by%2520how%2520well%2520the%2520predicted%2520token%2520sequences%2520can%2520reconstruct%2520the%2520original%2520image%2520under%2520teacher%2520forcing%252C%2520giving%2520the%2520model%2520direct%2520pixel-level%2520guidance%2520without%2520expensive%2520free-running%2520sampling.%2520The%2520regularization%2520term%2520of%2520the%2520ELBO%2520serves%2520as%2520a%2520natural%2520regularizer%252C%2520maintaining%2520distributional%2520consistency%2520of%2520tokens.%2520VA-%2524%25CF%2580%2524%2520enables%2520rapid%2520adaptation%2520of%2520existing%2520AR%2520generators%252C%2520without%2520neither%2520tokenizer%2520retraining%2520nor%2520external%2520reward%2520models.%2520With%2520only%25201%2525%2520ImageNet-1K%2520data%2520and%252025%2520minutes%2520of%2520tuning%252C%2520it%2520reduces%2520FID%2520from%252014.36%2520to%25207.65%2520and%2520improves%2520IS%2520from%252086.55%2520to%2520116.70%2520on%2520LlamaGen-XXL%252C%2520while%2520also%2520yielding%2520notable%2520gains%2520in%2520the%2520text-to-image%2520task%2520on%2520GenEval%2520for%2520both%2520visual%2520generation%2520model%2520%2528LlamaGen%253A%2520from%25200.306%2520to%25200.339%2529%2520and%2520unified%2520multi-modal%2520model%2520%2528Janus-Pro%253A%2520from%25200.725%2520to%25200.744%2529.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/Lil-Shake/VA-Pi.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19680v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VA-%24%CF%80%24%3A%20Variational%20Policy%20Alignment%20for%20Pixel-Aware%20Autoregressive%20Generation&entry.906535625=Xinyao%20Liao%20and%20Qiyuan%20He%20and%20Kai%20Xu%20and%20Xiaoye%20Qu%20and%20Yicong%20Li%20and%20Wei%20Wei%20and%20Angela%20Yao&entry.1292438233=Autoregressive%20%28AR%29%20visual%20generation%20relies%20on%20tokenizers%20to%20map%20images%20to%20and%20from%20discrete%20sequences.%20However%2C%20tokenizers%20are%20trained%20to%20reconstruct%20clean%20images%20from%20ground-truth%20tokens%2C%20while%20AR%20generators%20are%20optimized%20only%20for%20token%20likelihood.%20This%20misalignment%20leads%20to%20generated%20token%20sequences%20that%20may%20decode%20into%20low-quality%20images%2C%20without%20direct%20supervision%20from%20the%20pixel%20space.%20We%20propose%20VA-%24%CF%80%24%2C%20a%20lightweight%20post-training%20framework%20that%20directly%20optimizes%20AR%20models%20with%20a%20principled%20pixel-space%20objective.%20VA-%24%CF%80%24%20formulates%20the%20generator-tokenizer%20alignment%20as%20a%20variational%20optimization%2C%20deriving%20an%20evidence%20lower%20bound%20%28ELBO%29%20that%20unifies%20pixel%20reconstruction%20and%20autoregressive%20modeling.%20To%20optimize%20under%20the%20discrete%20token%20space%2C%20VA-%24%CF%80%24%20introduces%20a%20reinforcement-based%20alignment%20strategy%20that%20treats%20the%20AR%20generator%20as%20a%20policy%2C%20uses%20pixel-space%20reconstruction%20quality%20as%20its%20intrinsic%20reward.%20The%20reward%20is%20measured%20by%20how%20well%20the%20predicted%20token%20sequences%20can%20reconstruct%20the%20original%20image%20under%20teacher%20forcing%2C%20giving%20the%20model%20direct%20pixel-level%20guidance%20without%20expensive%20free-running%20sampling.%20The%20regularization%20term%20of%20the%20ELBO%20serves%20as%20a%20natural%20regularizer%2C%20maintaining%20distributional%20consistency%20of%20tokens.%20VA-%24%CF%80%24%20enables%20rapid%20adaptation%20of%20existing%20AR%20generators%2C%20without%20neither%20tokenizer%20retraining%20nor%20external%20reward%20models.%20With%20only%201%25%20ImageNet-1K%20data%20and%2025%20minutes%20of%20tuning%2C%20it%20reduces%20FID%20from%2014.36%20to%207.65%20and%20improves%20IS%20from%2086.55%20to%20116.70%20on%20LlamaGen-XXL%2C%20while%20also%20yielding%20notable%20gains%20in%20the%20text-to-image%20task%20on%20GenEval%20for%20both%20visual%20generation%20model%20%28LlamaGen%3A%20from%200.306%20to%200.339%29%20and%20unified%20multi-modal%20model%20%28Janus-Pro%3A%20from%200.725%20to%200.744%29.%20Code%20is%20available%20at%20https%3A//github.com/Lil-Shake/VA-Pi.&entry.1838667208=http%3A//arxiv.org/abs/2512.19680v1&entry.124074799=Read"},
{"title": "SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning", "author": "Xiaojun Guo and Runyu Zhou and Yifei Wang and Qi Zhang and Chenheng Zhang and Stefanie Jegelka and Xiaohan Wang and Jiajun Chai and Guojun Yin and Wei Lin and Yisen Wang", "abstract": "Vision-language models (VLMs) have shown remarkable abilities by integrating large language models with visual inputs. However, they often fail to utilize visual evidence adequately, either depending on linguistic priors in vision-centric tasks or resorting to textual shortcuts during reasoning. Although reinforcement learning (RL) can align models with desired behaviors, its application to VLMs has been hindered by the lack of scalable and reliable reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel framework that leverages self-supervised learning (SSL) tasks as a source of verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL objectives-such as predicting image rotation or reconstructing masked patches-into dense, automatic reward signals, eliminating the need for human preference data or unreliable AI evaluators. Experiments show that SSL4RL substantially improves performance on both vision-centric and vision-language reasoning benchmarks. Furthermore, through systematic ablations, we identify key factors-such as task difficulty, model scale, and semantic alignment with the target domain-that influence the effectiveness of SSL4RL tasks, offering new design principles for future work. We also demonstrate the framework's generality by applying it to graph learning, where it yields significant gains. SSL4RL establishes a versatile and effective paradigm for aligning multimodal models using verifiable, self-supervised objectives.", "link": "http://arxiv.org/abs/2510.16416v2", "date": "2025-12-22", "relevancy": 2.7256, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5546}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5546}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5262}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSL4RL%3A%20Revisiting%20Self-supervised%20Learning%20as%20Intrinsic%20Reward%20for%20Visual-Language%20Reasoning&body=Title%3A%20SSL4RL%3A%20Revisiting%20Self-supervised%20Learning%20as%20Intrinsic%20Reward%20for%20Visual-Language%20Reasoning%0AAuthor%3A%20Xiaojun%20Guo%20and%20Runyu%20Zhou%20and%20Yifei%20Wang%20and%20Qi%20Zhang%20and%20Chenheng%20Zhang%20and%20Stefanie%20Jegelka%20and%20Xiaohan%20Wang%20and%20Jiajun%20Chai%20and%20Guojun%20Yin%20and%20Wei%20Lin%20and%20Yisen%20Wang%0AAbstract%3A%20Vision-language%20models%20%28VLMs%29%20have%20shown%20remarkable%20abilities%20by%20integrating%20large%20language%20models%20with%20visual%20inputs.%20However%2C%20they%20often%20fail%20to%20utilize%20visual%20evidence%20adequately%2C%20either%20depending%20on%20linguistic%20priors%20in%20vision-centric%20tasks%20or%20resorting%20to%20textual%20shortcuts%20during%20reasoning.%20Although%20reinforcement%20learning%20%28RL%29%20can%20align%20models%20with%20desired%20behaviors%2C%20its%20application%20to%20VLMs%20has%20been%20hindered%20by%20the%20lack%20of%20scalable%20and%20reliable%20reward%20mechanisms.%20To%20overcome%20this%20challenge%2C%20we%20propose%20SSL4RL%2C%20a%20novel%20framework%20that%20leverages%20self-supervised%20learning%20%28SSL%29%20tasks%20as%20a%20source%20of%20verifiable%20rewards%20for%20RL-based%20fine-tuning.%20Our%20approach%20reformulates%20SSL%20objectives-such%20as%20predicting%20image%20rotation%20or%20reconstructing%20masked%20patches-into%20dense%2C%20automatic%20reward%20signals%2C%20eliminating%20the%20need%20for%20human%20preference%20data%20or%20unreliable%20AI%20evaluators.%20Experiments%20show%20that%20SSL4RL%20substantially%20improves%20performance%20on%20both%20vision-centric%20and%20vision-language%20reasoning%20benchmarks.%20Furthermore%2C%20through%20systematic%20ablations%2C%20we%20identify%20key%20factors-such%20as%20task%20difficulty%2C%20model%20scale%2C%20and%20semantic%20alignment%20with%20the%20target%20domain-that%20influence%20the%20effectiveness%20of%20SSL4RL%20tasks%2C%20offering%20new%20design%20principles%20for%20future%20work.%20We%20also%20demonstrate%20the%20framework%27s%20generality%20by%20applying%20it%20to%20graph%20learning%2C%20where%20it%20yields%20significant%20gains.%20SSL4RL%20establishes%20a%20versatile%20and%20effective%20paradigm%20for%20aligning%20multimodal%20models%20using%20verifiable%2C%20self-supervised%20objectives.%0ALink%3A%20http%3A//arxiv.org/abs/2510.16416v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSL4RL%253A%2520Revisiting%2520Self-supervised%2520Learning%2520as%2520Intrinsic%2520Reward%2520for%2520Visual-Language%2520Reasoning%26entry.906535625%3DXiaojun%2520Guo%2520and%2520Runyu%2520Zhou%2520and%2520Yifei%2520Wang%2520and%2520Qi%2520Zhang%2520and%2520Chenheng%2520Zhang%2520and%2520Stefanie%2520Jegelka%2520and%2520Xiaohan%2520Wang%2520and%2520Jiajun%2520Chai%2520and%2520Guojun%2520Yin%2520and%2520Wei%2520Lin%2520and%2520Yisen%2520Wang%26entry.1292438233%3DVision-language%2520models%2520%2528VLMs%2529%2520have%2520shown%2520remarkable%2520abilities%2520by%2520integrating%2520large%2520language%2520models%2520with%2520visual%2520inputs.%2520However%252C%2520they%2520often%2520fail%2520to%2520utilize%2520visual%2520evidence%2520adequately%252C%2520either%2520depending%2520on%2520linguistic%2520priors%2520in%2520vision-centric%2520tasks%2520or%2520resorting%2520to%2520textual%2520shortcuts%2520during%2520reasoning.%2520Although%2520reinforcement%2520learning%2520%2528RL%2529%2520can%2520align%2520models%2520with%2520desired%2520behaviors%252C%2520its%2520application%2520to%2520VLMs%2520has%2520been%2520hindered%2520by%2520the%2520lack%2520of%2520scalable%2520and%2520reliable%2520reward%2520mechanisms.%2520To%2520overcome%2520this%2520challenge%252C%2520we%2520propose%2520SSL4RL%252C%2520a%2520novel%2520framework%2520that%2520leverages%2520self-supervised%2520learning%2520%2528SSL%2529%2520tasks%2520as%2520a%2520source%2520of%2520verifiable%2520rewards%2520for%2520RL-based%2520fine-tuning.%2520Our%2520approach%2520reformulates%2520SSL%2520objectives-such%2520as%2520predicting%2520image%2520rotation%2520or%2520reconstructing%2520masked%2520patches-into%2520dense%252C%2520automatic%2520reward%2520signals%252C%2520eliminating%2520the%2520need%2520for%2520human%2520preference%2520data%2520or%2520unreliable%2520AI%2520evaluators.%2520Experiments%2520show%2520that%2520SSL4RL%2520substantially%2520improves%2520performance%2520on%2520both%2520vision-centric%2520and%2520vision-language%2520reasoning%2520benchmarks.%2520Furthermore%252C%2520through%2520systematic%2520ablations%252C%2520we%2520identify%2520key%2520factors-such%2520as%2520task%2520difficulty%252C%2520model%2520scale%252C%2520and%2520semantic%2520alignment%2520with%2520the%2520target%2520domain-that%2520influence%2520the%2520effectiveness%2520of%2520SSL4RL%2520tasks%252C%2520offering%2520new%2520design%2520principles%2520for%2520future%2520work.%2520We%2520also%2520demonstrate%2520the%2520framework%2527s%2520generality%2520by%2520applying%2520it%2520to%2520graph%2520learning%252C%2520where%2520it%2520yields%2520significant%2520gains.%2520SSL4RL%2520establishes%2520a%2520versatile%2520and%2520effective%2520paradigm%2520for%2520aligning%2520multimodal%2520models%2520using%2520verifiable%252C%2520self-supervised%2520objectives.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.16416v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSL4RL%3A%20Revisiting%20Self-supervised%20Learning%20as%20Intrinsic%20Reward%20for%20Visual-Language%20Reasoning&entry.906535625=Xiaojun%20Guo%20and%20Runyu%20Zhou%20and%20Yifei%20Wang%20and%20Qi%20Zhang%20and%20Chenheng%20Zhang%20and%20Stefanie%20Jegelka%20and%20Xiaohan%20Wang%20and%20Jiajun%20Chai%20and%20Guojun%20Yin%20and%20Wei%20Lin%20and%20Yisen%20Wang&entry.1292438233=Vision-language%20models%20%28VLMs%29%20have%20shown%20remarkable%20abilities%20by%20integrating%20large%20language%20models%20with%20visual%20inputs.%20However%2C%20they%20often%20fail%20to%20utilize%20visual%20evidence%20adequately%2C%20either%20depending%20on%20linguistic%20priors%20in%20vision-centric%20tasks%20or%20resorting%20to%20textual%20shortcuts%20during%20reasoning.%20Although%20reinforcement%20learning%20%28RL%29%20can%20align%20models%20with%20desired%20behaviors%2C%20its%20application%20to%20VLMs%20has%20been%20hindered%20by%20the%20lack%20of%20scalable%20and%20reliable%20reward%20mechanisms.%20To%20overcome%20this%20challenge%2C%20we%20propose%20SSL4RL%2C%20a%20novel%20framework%20that%20leverages%20self-supervised%20learning%20%28SSL%29%20tasks%20as%20a%20source%20of%20verifiable%20rewards%20for%20RL-based%20fine-tuning.%20Our%20approach%20reformulates%20SSL%20objectives-such%20as%20predicting%20image%20rotation%20or%20reconstructing%20masked%20patches-into%20dense%2C%20automatic%20reward%20signals%2C%20eliminating%20the%20need%20for%20human%20preference%20data%20or%20unreliable%20AI%20evaluators.%20Experiments%20show%20that%20SSL4RL%20substantially%20improves%20performance%20on%20both%20vision-centric%20and%20vision-language%20reasoning%20benchmarks.%20Furthermore%2C%20through%20systematic%20ablations%2C%20we%20identify%20key%20factors-such%20as%20task%20difficulty%2C%20model%20scale%2C%20and%20semantic%20alignment%20with%20the%20target%20domain-that%20influence%20the%20effectiveness%20of%20SSL4RL%20tasks%2C%20offering%20new%20design%20principles%20for%20future%20work.%20We%20also%20demonstrate%20the%20framework%27s%20generality%20by%20applying%20it%20to%20graph%20learning%2C%20where%20it%20yields%20significant%20gains.%20SSL4RL%20establishes%20a%20versatile%20and%20effective%20paradigm%20for%20aligning%20multimodal%20models%20using%20verifiable%2C%20self-supervised%20objectives.&entry.1838667208=http%3A//arxiv.org/abs/2510.16416v2&entry.124074799=Read"},
{"title": "Brain-language fusion enables interactive neural readout and in-silico experimentation", "author": "Victoria Bosch and Daniel Anthes and Adrien Doerig and Sushrut Thorat and Peter K\u00f6nig and Tim Christian Kietzmann", "abstract": "Large language models (LLMs) have revolutionized human-machine interaction, and have been extended by embedding diverse modalities such as images into a shared language space. Yet, neural decoding has remained constrained by static, non-interactive methods. We introduce CorText, a framework that integrates neural activity directly into the latent space of an LLM, enabling open-ended, natural language interaction with brain data. Trained on fMRI data recorded during viewing of natural scenes, CorText generates accurate image captions and can answer more detailed questions better than controls, while having access to neural data only. We showcase that CorText achieves zero-shot generalization beyond semantic categories seen during training. In-silico microstimulation experiments, which enable counterfactual prompts on brain activity, reveal a consistent, and graded mapping between brain-state and language output. These advances mark a shift from passive decoding toward generative, flexible interfaces between brain activity and language.", "link": "http://arxiv.org/abs/2509.23941v2", "date": "2025-12-22", "relevancy": 2.7244, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5606}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5606}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brain-language%20fusion%20enables%20interactive%20neural%20readout%20and%20in-silico%20experimentation&body=Title%3A%20Brain-language%20fusion%20enables%20interactive%20neural%20readout%20and%20in-silico%20experimentation%0AAuthor%3A%20Victoria%20Bosch%20and%20Daniel%20Anthes%20and%20Adrien%20Doerig%20and%20Sushrut%20Thorat%20and%20Peter%20K%C3%B6nig%20and%20Tim%20Christian%20Kietzmann%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20revolutionized%20human-machine%20interaction%2C%20and%20have%20been%20extended%20by%20embedding%20diverse%20modalities%20such%20as%20images%20into%20a%20shared%20language%20space.%20Yet%2C%20neural%20decoding%20has%20remained%20constrained%20by%20static%2C%20non-interactive%20methods.%20We%20introduce%20CorText%2C%20a%20framework%20that%20integrates%20neural%20activity%20directly%20into%20the%20latent%20space%20of%20an%20LLM%2C%20enabling%20open-ended%2C%20natural%20language%20interaction%20with%20brain%20data.%20Trained%20on%20fMRI%20data%20recorded%20during%20viewing%20of%20natural%20scenes%2C%20CorText%20generates%20accurate%20image%20captions%20and%20can%20answer%20more%20detailed%20questions%20better%20than%20controls%2C%20while%20having%20access%20to%20neural%20data%20only.%20We%20showcase%20that%20CorText%20achieves%20zero-shot%20generalization%20beyond%20semantic%20categories%20seen%20during%20training.%20In-silico%20microstimulation%20experiments%2C%20which%20enable%20counterfactual%20prompts%20on%20brain%20activity%2C%20reveal%20a%20consistent%2C%20and%20graded%20mapping%20between%20brain-state%20and%20language%20output.%20These%20advances%20mark%20a%20shift%20from%20passive%20decoding%20toward%20generative%2C%20flexible%20interfaces%20between%20brain%20activity%20and%20language.%0ALink%3A%20http%3A//arxiv.org/abs/2509.23941v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrain-language%2520fusion%2520enables%2520interactive%2520neural%2520readout%2520and%2520in-silico%2520experimentation%26entry.906535625%3DVictoria%2520Bosch%2520and%2520Daniel%2520Anthes%2520and%2520Adrien%2520Doerig%2520and%2520Sushrut%2520Thorat%2520and%2520Peter%2520K%25C3%25B6nig%2520and%2520Tim%2520Christian%2520Kietzmann%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520revolutionized%2520human-machine%2520interaction%252C%2520and%2520have%2520been%2520extended%2520by%2520embedding%2520diverse%2520modalities%2520such%2520as%2520images%2520into%2520a%2520shared%2520language%2520space.%2520Yet%252C%2520neural%2520decoding%2520has%2520remained%2520constrained%2520by%2520static%252C%2520non-interactive%2520methods.%2520We%2520introduce%2520CorText%252C%2520a%2520framework%2520that%2520integrates%2520neural%2520activity%2520directly%2520into%2520the%2520latent%2520space%2520of%2520an%2520LLM%252C%2520enabling%2520open-ended%252C%2520natural%2520language%2520interaction%2520with%2520brain%2520data.%2520Trained%2520on%2520fMRI%2520data%2520recorded%2520during%2520viewing%2520of%2520natural%2520scenes%252C%2520CorText%2520generates%2520accurate%2520image%2520captions%2520and%2520can%2520answer%2520more%2520detailed%2520questions%2520better%2520than%2520controls%252C%2520while%2520having%2520access%2520to%2520neural%2520data%2520only.%2520We%2520showcase%2520that%2520CorText%2520achieves%2520zero-shot%2520generalization%2520beyond%2520semantic%2520categories%2520seen%2520during%2520training.%2520In-silico%2520microstimulation%2520experiments%252C%2520which%2520enable%2520counterfactual%2520prompts%2520on%2520brain%2520activity%252C%2520reveal%2520a%2520consistent%252C%2520and%2520graded%2520mapping%2520between%2520brain-state%2520and%2520language%2520output.%2520These%2520advances%2520mark%2520a%2520shift%2520from%2520passive%2520decoding%2520toward%2520generative%252C%2520flexible%2520interfaces%2520between%2520brain%2520activity%2520and%2520language.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.23941v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brain-language%20fusion%20enables%20interactive%20neural%20readout%20and%20in-silico%20experimentation&entry.906535625=Victoria%20Bosch%20and%20Daniel%20Anthes%20and%20Adrien%20Doerig%20and%20Sushrut%20Thorat%20and%20Peter%20K%C3%B6nig%20and%20Tim%20Christian%20Kietzmann&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20revolutionized%20human-machine%20interaction%2C%20and%20have%20been%20extended%20by%20embedding%20diverse%20modalities%20such%20as%20images%20into%20a%20shared%20language%20space.%20Yet%2C%20neural%20decoding%20has%20remained%20constrained%20by%20static%2C%20non-interactive%20methods.%20We%20introduce%20CorText%2C%20a%20framework%20that%20integrates%20neural%20activity%20directly%20into%20the%20latent%20space%20of%20an%20LLM%2C%20enabling%20open-ended%2C%20natural%20language%20interaction%20with%20brain%20data.%20Trained%20on%20fMRI%20data%20recorded%20during%20viewing%20of%20natural%20scenes%2C%20CorText%20generates%20accurate%20image%20captions%20and%20can%20answer%20more%20detailed%20questions%20better%20than%20controls%2C%20while%20having%20access%20to%20neural%20data%20only.%20We%20showcase%20that%20CorText%20achieves%20zero-shot%20generalization%20beyond%20semantic%20categories%20seen%20during%20training.%20In-silico%20microstimulation%20experiments%2C%20which%20enable%20counterfactual%20prompts%20on%20brain%20activity%2C%20reveal%20a%20consistent%2C%20and%20graded%20mapping%20between%20brain-state%20and%20language%20output.%20These%20advances%20mark%20a%20shift%20from%20passive%20decoding%20toward%20generative%2C%20flexible%20interfaces%20between%20brain%20activity%20and%20language.&entry.1838667208=http%3A//arxiv.org/abs/2509.23941v2&entry.124074799=Read"},
{"title": "Efficient Spike-driven Transformer for High-performance Drone-View Geo-Localization", "author": "Zhongwei Chen and Hai-Jun Rong and Zhao-Xu Yang and Guoqi Li", "abstract": "Traditional drone-view geo-localization (DVGL) methods based on artificial neural networks (ANNs) have achieved remarkable performance. However, ANNs rely on dense computation, which results in high power consumption. In contrast, spiking neural networks (SNNs), which benefit from spike-driven computation, inherently provide low power consumption. Regrettably, the potential of SNNs for DVGL has yet to be thoroughly investigated. Meanwhile, the inherent sparsity of spike-driven computation for representation learning scenarios also results in loss of critical information and difficulties in learning long-range dependencies when aligning heterogeneous visual data sources. To address these, we propose SpikeViMFormer, the first SNN framework designed for DVGL. In this framework, a lightweight spike-driven transformer backbone is adopted to extract coarse-grained features. To mitigate the loss of critical information, the spike-driven selective attention (SSA) block is designed, which uses a spike-driven gating mechanism to achieve selective feature enhancement and highlight discriminative regions. Furthermore, a spike-driven hybrid state space (SHS) block is introduced to learn long-range dependencies using a hybrid state space. Moreover, only the backbone is utilized during the inference stage to reduce computational cost. To ensure backbone effectiveness, a novel hierarchical re-ranking alignment learning (HRAL) strategy is proposed. It refines features via neighborhood re-ranking and maintains cross-batch consistency to directly optimize the backbone. Experimental results demonstrate that SpikeViMFormer outperforms state-of-the-art SNNs. Compared with advanced ANNs, it also achieves competitive performance.Our code is available at https://github.com/ISChenawei/SpikeViMFormer", "link": "http://arxiv.org/abs/2512.19365v1", "date": "2025-12-22", "relevancy": 2.6988, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5424}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5387}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Spike-driven%20Transformer%20for%20High-performance%20Drone-View%20Geo-Localization&body=Title%3A%20Efficient%20Spike-driven%20Transformer%20for%20High-performance%20Drone-View%20Geo-Localization%0AAuthor%3A%20Zhongwei%20Chen%20and%20Hai-Jun%20Rong%20and%20Zhao-Xu%20Yang%20and%20Guoqi%20Li%0AAbstract%3A%20Traditional%20drone-view%20geo-localization%20%28DVGL%29%20methods%20based%20on%20artificial%20neural%20networks%20%28ANNs%29%20have%20achieved%20remarkable%20performance.%20However%2C%20ANNs%20rely%20on%20dense%20computation%2C%20which%20results%20in%20high%20power%20consumption.%20In%20contrast%2C%20spiking%20neural%20networks%20%28SNNs%29%2C%20which%20benefit%20from%20spike-driven%20computation%2C%20inherently%20provide%20low%20power%20consumption.%20Regrettably%2C%20the%20potential%20of%20SNNs%20for%20DVGL%20has%20yet%20to%20be%20thoroughly%20investigated.%20Meanwhile%2C%20the%20inherent%20sparsity%20of%20spike-driven%20computation%20for%20representation%20learning%20scenarios%20also%20results%20in%20loss%20of%20critical%20information%20and%20difficulties%20in%20learning%20long-range%20dependencies%20when%20aligning%20heterogeneous%20visual%20data%20sources.%20To%20address%20these%2C%20we%20propose%20SpikeViMFormer%2C%20the%20first%20SNN%20framework%20designed%20for%20DVGL.%20In%20this%20framework%2C%20a%20lightweight%20spike-driven%20transformer%20backbone%20is%20adopted%20to%20extract%20coarse-grained%20features.%20To%20mitigate%20the%20loss%20of%20critical%20information%2C%20the%20spike-driven%20selective%20attention%20%28SSA%29%20block%20is%20designed%2C%20which%20uses%20a%20spike-driven%20gating%20mechanism%20to%20achieve%20selective%20feature%20enhancement%20and%20highlight%20discriminative%20regions.%20Furthermore%2C%20a%20spike-driven%20hybrid%20state%20space%20%28SHS%29%20block%20is%20introduced%20to%20learn%20long-range%20dependencies%20using%20a%20hybrid%20state%20space.%20Moreover%2C%20only%20the%20backbone%20is%20utilized%20during%20the%20inference%20stage%20to%20reduce%20computational%20cost.%20To%20ensure%20backbone%20effectiveness%2C%20a%20novel%20hierarchical%20re-ranking%20alignment%20learning%20%28HRAL%29%20strategy%20is%20proposed.%20It%20refines%20features%20via%20neighborhood%20re-ranking%20and%20maintains%20cross-batch%20consistency%20to%20directly%20optimize%20the%20backbone.%20Experimental%20results%20demonstrate%20that%20SpikeViMFormer%20outperforms%20state-of-the-art%20SNNs.%20Compared%20with%20advanced%20ANNs%2C%20it%20also%20achieves%20competitive%20performance.Our%20code%20is%20available%20at%20https%3A//github.com/ISChenawei/SpikeViMFormer%0ALink%3A%20http%3A//arxiv.org/abs/2512.19365v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Spike-driven%2520Transformer%2520for%2520High-performance%2520Drone-View%2520Geo-Localization%26entry.906535625%3DZhongwei%2520Chen%2520and%2520Hai-Jun%2520Rong%2520and%2520Zhao-Xu%2520Yang%2520and%2520Guoqi%2520Li%26entry.1292438233%3DTraditional%2520drone-view%2520geo-localization%2520%2528DVGL%2529%2520methods%2520based%2520on%2520artificial%2520neural%2520networks%2520%2528ANNs%2529%2520have%2520achieved%2520remarkable%2520performance.%2520However%252C%2520ANNs%2520rely%2520on%2520dense%2520computation%252C%2520which%2520results%2520in%2520high%2520power%2520consumption.%2520In%2520contrast%252C%2520spiking%2520neural%2520networks%2520%2528SNNs%2529%252C%2520which%2520benefit%2520from%2520spike-driven%2520computation%252C%2520inherently%2520provide%2520low%2520power%2520consumption.%2520Regrettably%252C%2520the%2520potential%2520of%2520SNNs%2520for%2520DVGL%2520has%2520yet%2520to%2520be%2520thoroughly%2520investigated.%2520Meanwhile%252C%2520the%2520inherent%2520sparsity%2520of%2520spike-driven%2520computation%2520for%2520representation%2520learning%2520scenarios%2520also%2520results%2520in%2520loss%2520of%2520critical%2520information%2520and%2520difficulties%2520in%2520learning%2520long-range%2520dependencies%2520when%2520aligning%2520heterogeneous%2520visual%2520data%2520sources.%2520To%2520address%2520these%252C%2520we%2520propose%2520SpikeViMFormer%252C%2520the%2520first%2520SNN%2520framework%2520designed%2520for%2520DVGL.%2520In%2520this%2520framework%252C%2520a%2520lightweight%2520spike-driven%2520transformer%2520backbone%2520is%2520adopted%2520to%2520extract%2520coarse-grained%2520features.%2520To%2520mitigate%2520the%2520loss%2520of%2520critical%2520information%252C%2520the%2520spike-driven%2520selective%2520attention%2520%2528SSA%2529%2520block%2520is%2520designed%252C%2520which%2520uses%2520a%2520spike-driven%2520gating%2520mechanism%2520to%2520achieve%2520selective%2520feature%2520enhancement%2520and%2520highlight%2520discriminative%2520regions.%2520Furthermore%252C%2520a%2520spike-driven%2520hybrid%2520state%2520space%2520%2528SHS%2529%2520block%2520is%2520introduced%2520to%2520learn%2520long-range%2520dependencies%2520using%2520a%2520hybrid%2520state%2520space.%2520Moreover%252C%2520only%2520the%2520backbone%2520is%2520utilized%2520during%2520the%2520inference%2520stage%2520to%2520reduce%2520computational%2520cost.%2520To%2520ensure%2520backbone%2520effectiveness%252C%2520a%2520novel%2520hierarchical%2520re-ranking%2520alignment%2520learning%2520%2528HRAL%2529%2520strategy%2520is%2520proposed.%2520It%2520refines%2520features%2520via%2520neighborhood%2520re-ranking%2520and%2520maintains%2520cross-batch%2520consistency%2520to%2520directly%2520optimize%2520the%2520backbone.%2520Experimental%2520results%2520demonstrate%2520that%2520SpikeViMFormer%2520outperforms%2520state-of-the-art%2520SNNs.%2520Compared%2520with%2520advanced%2520ANNs%252C%2520it%2520also%2520achieves%2520competitive%2520performance.Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/ISChenawei/SpikeViMFormer%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19365v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Spike-driven%20Transformer%20for%20High-performance%20Drone-View%20Geo-Localization&entry.906535625=Zhongwei%20Chen%20and%20Hai-Jun%20Rong%20and%20Zhao-Xu%20Yang%20and%20Guoqi%20Li&entry.1292438233=Traditional%20drone-view%20geo-localization%20%28DVGL%29%20methods%20based%20on%20artificial%20neural%20networks%20%28ANNs%29%20have%20achieved%20remarkable%20performance.%20However%2C%20ANNs%20rely%20on%20dense%20computation%2C%20which%20results%20in%20high%20power%20consumption.%20In%20contrast%2C%20spiking%20neural%20networks%20%28SNNs%29%2C%20which%20benefit%20from%20spike-driven%20computation%2C%20inherently%20provide%20low%20power%20consumption.%20Regrettably%2C%20the%20potential%20of%20SNNs%20for%20DVGL%20has%20yet%20to%20be%20thoroughly%20investigated.%20Meanwhile%2C%20the%20inherent%20sparsity%20of%20spike-driven%20computation%20for%20representation%20learning%20scenarios%20also%20results%20in%20loss%20of%20critical%20information%20and%20difficulties%20in%20learning%20long-range%20dependencies%20when%20aligning%20heterogeneous%20visual%20data%20sources.%20To%20address%20these%2C%20we%20propose%20SpikeViMFormer%2C%20the%20first%20SNN%20framework%20designed%20for%20DVGL.%20In%20this%20framework%2C%20a%20lightweight%20spike-driven%20transformer%20backbone%20is%20adopted%20to%20extract%20coarse-grained%20features.%20To%20mitigate%20the%20loss%20of%20critical%20information%2C%20the%20spike-driven%20selective%20attention%20%28SSA%29%20block%20is%20designed%2C%20which%20uses%20a%20spike-driven%20gating%20mechanism%20to%20achieve%20selective%20feature%20enhancement%20and%20highlight%20discriminative%20regions.%20Furthermore%2C%20a%20spike-driven%20hybrid%20state%20space%20%28SHS%29%20block%20is%20introduced%20to%20learn%20long-range%20dependencies%20using%20a%20hybrid%20state%20space.%20Moreover%2C%20only%20the%20backbone%20is%20utilized%20during%20the%20inference%20stage%20to%20reduce%20computational%20cost.%20To%20ensure%20backbone%20effectiveness%2C%20a%20novel%20hierarchical%20re-ranking%20alignment%20learning%20%28HRAL%29%20strategy%20is%20proposed.%20It%20refines%20features%20via%20neighborhood%20re-ranking%20and%20maintains%20cross-batch%20consistency%20to%20directly%20optimize%20the%20backbone.%20Experimental%20results%20demonstrate%20that%20SpikeViMFormer%20outperforms%20state-of-the-art%20SNNs.%20Compared%20with%20advanced%20ANNs%2C%20it%20also%20achieves%20competitive%20performance.Our%20code%20is%20available%20at%20https%3A//github.com/ISChenawei/SpikeViMFormer&entry.1838667208=http%3A//arxiv.org/abs/2512.19365v1&entry.124074799=Read"},
{"title": "D2Pruner: Debiased Importance and Structural Diversity for MLLM Token Pruning", "author": "Evelyn Zhang and Fufu Yu and Aoqi Wu and Zichen Wen and Ke Yan and Shouhong Ding and Biqing Qi and Linfeng Zhang", "abstract": "Processing long visual token sequences poses a significant computational burden on Multimodal Large Language Models (MLLMs). While token pruning offers a path to acceleration, we find that current methods, while adequate for general understanding, catastrophically fail on fine-grained localization tasks. We attribute this failure to the inherent flaws of the two prevailing strategies: importance-based methods suffer from a strong positional bias, an inherent model artifact that distracts from semantic content, while diversity-based methods exhibit structural blindness, disregarding the user's prompt and spatial redundancy. To address this, we introduce D2Pruner, a framework that rectifies these issues by uniquely combining debiased importance with a structural pruning mechanism. Our method first secures a core set of the most critical tokens as pivots based on a debiased attention score. It then performs a Maximal Independent Set (MIS) selection on the remaining tokens, which are modeled on a hybrid graph where edges signify spatial proximity and semantic similarity. This process iteratively preserves the most important and available token while removing its neighbors, ensuring that the supplementary tokens are chosen to maximize importance and diversity. Extensive experiments demonstrate that D2Pruner has exceptional efficiency and fidelity. Applied to LLaVA-1.5-7B for general understanding tasks, it reduces FLOPs by 74.2\\% while retaining 99.2\\% of its original performance. Furthermore, in challenging localization benchmarks with InternVL-2.5-8B, it maintains 85.7\\% performance at a 90\\% token reduction rate, marking a significant advancement with up to 63. 53\\% improvement over existing methods.", "link": "http://arxiv.org/abs/2512.19443v1", "date": "2025-12-22", "relevancy": 2.676, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5352}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5352}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5352}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20D2Pruner%3A%20Debiased%20Importance%20and%20Structural%20Diversity%20for%20MLLM%20Token%20Pruning&body=Title%3A%20D2Pruner%3A%20Debiased%20Importance%20and%20Structural%20Diversity%20for%20MLLM%20Token%20Pruning%0AAuthor%3A%20Evelyn%20Zhang%20and%20Fufu%20Yu%20and%20Aoqi%20Wu%20and%20Zichen%20Wen%20and%20Ke%20Yan%20and%20Shouhong%20Ding%20and%20Biqing%20Qi%20and%20Linfeng%20Zhang%0AAbstract%3A%20Processing%20long%20visual%20token%20sequences%20poses%20a%20significant%20computational%20burden%20on%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20While%20token%20pruning%20offers%20a%20path%20to%20acceleration%2C%20we%20find%20that%20current%20methods%2C%20while%20adequate%20for%20general%20understanding%2C%20catastrophically%20fail%20on%20fine-grained%20localization%20tasks.%20We%20attribute%20this%20failure%20to%20the%20inherent%20flaws%20of%20the%20two%20prevailing%20strategies%3A%20importance-based%20methods%20suffer%20from%20a%20strong%20positional%20bias%2C%20an%20inherent%20model%20artifact%20that%20distracts%20from%20semantic%20content%2C%20while%20diversity-based%20methods%20exhibit%20structural%20blindness%2C%20disregarding%20the%20user%27s%20prompt%20and%20spatial%20redundancy.%20To%20address%20this%2C%20we%20introduce%20D2Pruner%2C%20a%20framework%20that%20rectifies%20these%20issues%20by%20uniquely%20combining%20debiased%20importance%20with%20a%20structural%20pruning%20mechanism.%20Our%20method%20first%20secures%20a%20core%20set%20of%20the%20most%20critical%20tokens%20as%20pivots%20based%20on%20a%20debiased%20attention%20score.%20It%20then%20performs%20a%20Maximal%20Independent%20Set%20%28MIS%29%20selection%20on%20the%20remaining%20tokens%2C%20which%20are%20modeled%20on%20a%20hybrid%20graph%20where%20edges%20signify%20spatial%20proximity%20and%20semantic%20similarity.%20This%20process%20iteratively%20preserves%20the%20most%20important%20and%20available%20token%20while%20removing%20its%20neighbors%2C%20ensuring%20that%20the%20supplementary%20tokens%20are%20chosen%20to%20maximize%20importance%20and%20diversity.%20Extensive%20experiments%20demonstrate%20that%20D2Pruner%20has%20exceptional%20efficiency%20and%20fidelity.%20Applied%20to%20LLaVA-1.5-7B%20for%20general%20understanding%20tasks%2C%20it%20reduces%20FLOPs%20by%2074.2%5C%25%20while%20retaining%2099.2%5C%25%20of%20its%20original%20performance.%20Furthermore%2C%20in%20challenging%20localization%20benchmarks%20with%20InternVL-2.5-8B%2C%20it%20maintains%2085.7%5C%25%20performance%20at%20a%2090%5C%25%20token%20reduction%20rate%2C%20marking%20a%20significant%20advancement%20with%20up%20to%2063.%2053%5C%25%20improvement%20over%20existing%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19443v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DD2Pruner%253A%2520Debiased%2520Importance%2520and%2520Structural%2520Diversity%2520for%2520MLLM%2520Token%2520Pruning%26entry.906535625%3DEvelyn%2520Zhang%2520and%2520Fufu%2520Yu%2520and%2520Aoqi%2520Wu%2520and%2520Zichen%2520Wen%2520and%2520Ke%2520Yan%2520and%2520Shouhong%2520Ding%2520and%2520Biqing%2520Qi%2520and%2520Linfeng%2520Zhang%26entry.1292438233%3DProcessing%2520long%2520visual%2520token%2520sequences%2520poses%2520a%2520significant%2520computational%2520burden%2520on%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529.%2520While%2520token%2520pruning%2520offers%2520a%2520path%2520to%2520acceleration%252C%2520we%2520find%2520that%2520current%2520methods%252C%2520while%2520adequate%2520for%2520general%2520understanding%252C%2520catastrophically%2520fail%2520on%2520fine-grained%2520localization%2520tasks.%2520We%2520attribute%2520this%2520failure%2520to%2520the%2520inherent%2520flaws%2520of%2520the%2520two%2520prevailing%2520strategies%253A%2520importance-based%2520methods%2520suffer%2520from%2520a%2520strong%2520positional%2520bias%252C%2520an%2520inherent%2520model%2520artifact%2520that%2520distracts%2520from%2520semantic%2520content%252C%2520while%2520diversity-based%2520methods%2520exhibit%2520structural%2520blindness%252C%2520disregarding%2520the%2520user%2527s%2520prompt%2520and%2520spatial%2520redundancy.%2520To%2520address%2520this%252C%2520we%2520introduce%2520D2Pruner%252C%2520a%2520framework%2520that%2520rectifies%2520these%2520issues%2520by%2520uniquely%2520combining%2520debiased%2520importance%2520with%2520a%2520structural%2520pruning%2520mechanism.%2520Our%2520method%2520first%2520secures%2520a%2520core%2520set%2520of%2520the%2520most%2520critical%2520tokens%2520as%2520pivots%2520based%2520on%2520a%2520debiased%2520attention%2520score.%2520It%2520then%2520performs%2520a%2520Maximal%2520Independent%2520Set%2520%2528MIS%2529%2520selection%2520on%2520the%2520remaining%2520tokens%252C%2520which%2520are%2520modeled%2520on%2520a%2520hybrid%2520graph%2520where%2520edges%2520signify%2520spatial%2520proximity%2520and%2520semantic%2520similarity.%2520This%2520process%2520iteratively%2520preserves%2520the%2520most%2520important%2520and%2520available%2520token%2520while%2520removing%2520its%2520neighbors%252C%2520ensuring%2520that%2520the%2520supplementary%2520tokens%2520are%2520chosen%2520to%2520maximize%2520importance%2520and%2520diversity.%2520Extensive%2520experiments%2520demonstrate%2520that%2520D2Pruner%2520has%2520exceptional%2520efficiency%2520and%2520fidelity.%2520Applied%2520to%2520LLaVA-1.5-7B%2520for%2520general%2520understanding%2520tasks%252C%2520it%2520reduces%2520FLOPs%2520by%252074.2%255C%2525%2520while%2520retaining%252099.2%255C%2525%2520of%2520its%2520original%2520performance.%2520Furthermore%252C%2520in%2520challenging%2520localization%2520benchmarks%2520with%2520InternVL-2.5-8B%252C%2520it%2520maintains%252085.7%255C%2525%2520performance%2520at%2520a%252090%255C%2525%2520token%2520reduction%2520rate%252C%2520marking%2520a%2520significant%2520advancement%2520with%2520up%2520to%252063.%252053%255C%2525%2520improvement%2520over%2520existing%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19443v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=D2Pruner%3A%20Debiased%20Importance%20and%20Structural%20Diversity%20for%20MLLM%20Token%20Pruning&entry.906535625=Evelyn%20Zhang%20and%20Fufu%20Yu%20and%20Aoqi%20Wu%20and%20Zichen%20Wen%20and%20Ke%20Yan%20and%20Shouhong%20Ding%20and%20Biqing%20Qi%20and%20Linfeng%20Zhang&entry.1292438233=Processing%20long%20visual%20token%20sequences%20poses%20a%20significant%20computational%20burden%20on%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20While%20token%20pruning%20offers%20a%20path%20to%20acceleration%2C%20we%20find%20that%20current%20methods%2C%20while%20adequate%20for%20general%20understanding%2C%20catastrophically%20fail%20on%20fine-grained%20localization%20tasks.%20We%20attribute%20this%20failure%20to%20the%20inherent%20flaws%20of%20the%20two%20prevailing%20strategies%3A%20importance-based%20methods%20suffer%20from%20a%20strong%20positional%20bias%2C%20an%20inherent%20model%20artifact%20that%20distracts%20from%20semantic%20content%2C%20while%20diversity-based%20methods%20exhibit%20structural%20blindness%2C%20disregarding%20the%20user%27s%20prompt%20and%20spatial%20redundancy.%20To%20address%20this%2C%20we%20introduce%20D2Pruner%2C%20a%20framework%20that%20rectifies%20these%20issues%20by%20uniquely%20combining%20debiased%20importance%20with%20a%20structural%20pruning%20mechanism.%20Our%20method%20first%20secures%20a%20core%20set%20of%20the%20most%20critical%20tokens%20as%20pivots%20based%20on%20a%20debiased%20attention%20score.%20It%20then%20performs%20a%20Maximal%20Independent%20Set%20%28MIS%29%20selection%20on%20the%20remaining%20tokens%2C%20which%20are%20modeled%20on%20a%20hybrid%20graph%20where%20edges%20signify%20spatial%20proximity%20and%20semantic%20similarity.%20This%20process%20iteratively%20preserves%20the%20most%20important%20and%20available%20token%20while%20removing%20its%20neighbors%2C%20ensuring%20that%20the%20supplementary%20tokens%20are%20chosen%20to%20maximize%20importance%20and%20diversity.%20Extensive%20experiments%20demonstrate%20that%20D2Pruner%20has%20exceptional%20efficiency%20and%20fidelity.%20Applied%20to%20LLaVA-1.5-7B%20for%20general%20understanding%20tasks%2C%20it%20reduces%20FLOPs%20by%2074.2%5C%25%20while%20retaining%2099.2%5C%25%20of%20its%20original%20performance.%20Furthermore%2C%20in%20challenging%20localization%20benchmarks%20with%20InternVL-2.5-8B%2C%20it%20maintains%2085.7%5C%25%20performance%20at%20a%2090%5C%25%20token%20reduction%20rate%2C%20marking%20a%20significant%20advancement%20with%20up%20to%2063.%2053%5C%25%20improvement%20over%20existing%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2512.19443v1&entry.124074799=Read"},
{"title": "MT-Mark: Rethinking Image Watermarking via Mutual-Teacher Collaboration with Adaptive Feature Modulation", "author": "Fei Ge and Ying Huang and Jie Liu and Guixuan Zhang and Zhi Zeng and Shuwu Zhang and Hu Guan", "abstract": "Existing deep image watermarking methods follow a fixed embedding-distortion-extraction pipeline, where the embedder and extractor are weakly coupled through a final loss and optimized in isolation. This design lacks explicit collaboration, leaving no structured mechanism for the embedder to incorporate decoding-aware cues or for the extractor to guide embedding during training. To address this architectural limitation, we rethink deep image watermarking by reformulating embedding and extraction as explicitly collaborative components. To realize this reformulation, we introduce a Collaborative Interaction Mechanism (CIM) that establishes direct, bidirectional communication between the embedder and extractor, enabling a mutual-teacher training paradigm and coordinated optimization. Built upon this explicitly collaborative architecture, we further propose an Adaptive Feature Modulation Module (AFMM) to support effective interaction. AFMM enables content-aware feature regulation by decoupling modulation structure and strength, guiding watermark embedding toward stable image features while suppressing host interference during extraction. Under CIM, the AFMMs on both sides form a closed-loop collaboration that aligns embedding behavior with extraction objectives. This architecture-level redesign changes how robustness is learned in watermarking systems. Rather than relying on exhaustive distortion simulation, robustness emerges from coordinated representation learning between embedding and extraction. Experiments on real-world and AI-generated datasets demonstrate that the proposed method consistently outperforms state-of-the-art approaches in watermark extraction accuracy while maintaining high perceptual quality, showing strong robustness and generalization.", "link": "http://arxiv.org/abs/2512.19438v1", "date": "2025-12-22", "relevancy": 2.6422, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5422}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.534}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.509}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MT-Mark%3A%20Rethinking%20Image%20Watermarking%20via%20Mutual-Teacher%20Collaboration%20with%20Adaptive%20Feature%20Modulation&body=Title%3A%20MT-Mark%3A%20Rethinking%20Image%20Watermarking%20via%20Mutual-Teacher%20Collaboration%20with%20Adaptive%20Feature%20Modulation%0AAuthor%3A%20Fei%20Ge%20and%20Ying%20Huang%20and%20Jie%20Liu%20and%20Guixuan%20Zhang%20and%20Zhi%20Zeng%20and%20Shuwu%20Zhang%20and%20Hu%20Guan%0AAbstract%3A%20Existing%20deep%20image%20watermarking%20methods%20follow%20a%20fixed%20embedding-distortion-extraction%20pipeline%2C%20where%20the%20embedder%20and%20extractor%20are%20weakly%20coupled%20through%20a%20final%20loss%20and%20optimized%20in%20isolation.%20This%20design%20lacks%20explicit%20collaboration%2C%20leaving%20no%20structured%20mechanism%20for%20the%20embedder%20to%20incorporate%20decoding-aware%20cues%20or%20for%20the%20extractor%20to%20guide%20embedding%20during%20training.%20To%20address%20this%20architectural%20limitation%2C%20we%20rethink%20deep%20image%20watermarking%20by%20reformulating%20embedding%20and%20extraction%20as%20explicitly%20collaborative%20components.%20To%20realize%20this%20reformulation%2C%20we%20introduce%20a%20Collaborative%20Interaction%20Mechanism%20%28CIM%29%20that%20establishes%20direct%2C%20bidirectional%20communication%20between%20the%20embedder%20and%20extractor%2C%20enabling%20a%20mutual-teacher%20training%20paradigm%20and%20coordinated%20optimization.%20Built%20upon%20this%20explicitly%20collaborative%20architecture%2C%20we%20further%20propose%20an%20Adaptive%20Feature%20Modulation%20Module%20%28AFMM%29%20to%20support%20effective%20interaction.%20AFMM%20enables%20content-aware%20feature%20regulation%20by%20decoupling%20modulation%20structure%20and%20strength%2C%20guiding%20watermark%20embedding%20toward%20stable%20image%20features%20while%20suppressing%20host%20interference%20during%20extraction.%20Under%20CIM%2C%20the%20AFMMs%20on%20both%20sides%20form%20a%20closed-loop%20collaboration%20that%20aligns%20embedding%20behavior%20with%20extraction%20objectives.%20This%20architecture-level%20redesign%20changes%20how%20robustness%20is%20learned%20in%20watermarking%20systems.%20Rather%20than%20relying%20on%20exhaustive%20distortion%20simulation%2C%20robustness%20emerges%20from%20coordinated%20representation%20learning%20between%20embedding%20and%20extraction.%20Experiments%20on%20real-world%20and%20AI-generated%20datasets%20demonstrate%20that%20the%20proposed%20method%20consistently%20outperforms%20state-of-the-art%20approaches%20in%20watermark%20extraction%20accuracy%20while%20maintaining%20high%20perceptual%20quality%2C%20showing%20strong%20robustness%20and%20generalization.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19438v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMT-Mark%253A%2520Rethinking%2520Image%2520Watermarking%2520via%2520Mutual-Teacher%2520Collaboration%2520with%2520Adaptive%2520Feature%2520Modulation%26entry.906535625%3DFei%2520Ge%2520and%2520Ying%2520Huang%2520and%2520Jie%2520Liu%2520and%2520Guixuan%2520Zhang%2520and%2520Zhi%2520Zeng%2520and%2520Shuwu%2520Zhang%2520and%2520Hu%2520Guan%26entry.1292438233%3DExisting%2520deep%2520image%2520watermarking%2520methods%2520follow%2520a%2520fixed%2520embedding-distortion-extraction%2520pipeline%252C%2520where%2520the%2520embedder%2520and%2520extractor%2520are%2520weakly%2520coupled%2520through%2520a%2520final%2520loss%2520and%2520optimized%2520in%2520isolation.%2520This%2520design%2520lacks%2520explicit%2520collaboration%252C%2520leaving%2520no%2520structured%2520mechanism%2520for%2520the%2520embedder%2520to%2520incorporate%2520decoding-aware%2520cues%2520or%2520for%2520the%2520extractor%2520to%2520guide%2520embedding%2520during%2520training.%2520To%2520address%2520this%2520architectural%2520limitation%252C%2520we%2520rethink%2520deep%2520image%2520watermarking%2520by%2520reformulating%2520embedding%2520and%2520extraction%2520as%2520explicitly%2520collaborative%2520components.%2520To%2520realize%2520this%2520reformulation%252C%2520we%2520introduce%2520a%2520Collaborative%2520Interaction%2520Mechanism%2520%2528CIM%2529%2520that%2520establishes%2520direct%252C%2520bidirectional%2520communication%2520between%2520the%2520embedder%2520and%2520extractor%252C%2520enabling%2520a%2520mutual-teacher%2520training%2520paradigm%2520and%2520coordinated%2520optimization.%2520Built%2520upon%2520this%2520explicitly%2520collaborative%2520architecture%252C%2520we%2520further%2520propose%2520an%2520Adaptive%2520Feature%2520Modulation%2520Module%2520%2528AFMM%2529%2520to%2520support%2520effective%2520interaction.%2520AFMM%2520enables%2520content-aware%2520feature%2520regulation%2520by%2520decoupling%2520modulation%2520structure%2520and%2520strength%252C%2520guiding%2520watermark%2520embedding%2520toward%2520stable%2520image%2520features%2520while%2520suppressing%2520host%2520interference%2520during%2520extraction.%2520Under%2520CIM%252C%2520the%2520AFMMs%2520on%2520both%2520sides%2520form%2520a%2520closed-loop%2520collaboration%2520that%2520aligns%2520embedding%2520behavior%2520with%2520extraction%2520objectives.%2520This%2520architecture-level%2520redesign%2520changes%2520how%2520robustness%2520is%2520learned%2520in%2520watermarking%2520systems.%2520Rather%2520than%2520relying%2520on%2520exhaustive%2520distortion%2520simulation%252C%2520robustness%2520emerges%2520from%2520coordinated%2520representation%2520learning%2520between%2520embedding%2520and%2520extraction.%2520Experiments%2520on%2520real-world%2520and%2520AI-generated%2520datasets%2520demonstrate%2520that%2520the%2520proposed%2520method%2520consistently%2520outperforms%2520state-of-the-art%2520approaches%2520in%2520watermark%2520extraction%2520accuracy%2520while%2520maintaining%2520high%2520perceptual%2520quality%252C%2520showing%2520strong%2520robustness%2520and%2520generalization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19438v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MT-Mark%3A%20Rethinking%20Image%20Watermarking%20via%20Mutual-Teacher%20Collaboration%20with%20Adaptive%20Feature%20Modulation&entry.906535625=Fei%20Ge%20and%20Ying%20Huang%20and%20Jie%20Liu%20and%20Guixuan%20Zhang%20and%20Zhi%20Zeng%20and%20Shuwu%20Zhang%20and%20Hu%20Guan&entry.1292438233=Existing%20deep%20image%20watermarking%20methods%20follow%20a%20fixed%20embedding-distortion-extraction%20pipeline%2C%20where%20the%20embedder%20and%20extractor%20are%20weakly%20coupled%20through%20a%20final%20loss%20and%20optimized%20in%20isolation.%20This%20design%20lacks%20explicit%20collaboration%2C%20leaving%20no%20structured%20mechanism%20for%20the%20embedder%20to%20incorporate%20decoding-aware%20cues%20or%20for%20the%20extractor%20to%20guide%20embedding%20during%20training.%20To%20address%20this%20architectural%20limitation%2C%20we%20rethink%20deep%20image%20watermarking%20by%20reformulating%20embedding%20and%20extraction%20as%20explicitly%20collaborative%20components.%20To%20realize%20this%20reformulation%2C%20we%20introduce%20a%20Collaborative%20Interaction%20Mechanism%20%28CIM%29%20that%20establishes%20direct%2C%20bidirectional%20communication%20between%20the%20embedder%20and%20extractor%2C%20enabling%20a%20mutual-teacher%20training%20paradigm%20and%20coordinated%20optimization.%20Built%20upon%20this%20explicitly%20collaborative%20architecture%2C%20we%20further%20propose%20an%20Adaptive%20Feature%20Modulation%20Module%20%28AFMM%29%20to%20support%20effective%20interaction.%20AFMM%20enables%20content-aware%20feature%20regulation%20by%20decoupling%20modulation%20structure%20and%20strength%2C%20guiding%20watermark%20embedding%20toward%20stable%20image%20features%20while%20suppressing%20host%20interference%20during%20extraction.%20Under%20CIM%2C%20the%20AFMMs%20on%20both%20sides%20form%20a%20closed-loop%20collaboration%20that%20aligns%20embedding%20behavior%20with%20extraction%20objectives.%20This%20architecture-level%20redesign%20changes%20how%20robustness%20is%20learned%20in%20watermarking%20systems.%20Rather%20than%20relying%20on%20exhaustive%20distortion%20simulation%2C%20robustness%20emerges%20from%20coordinated%20representation%20learning%20between%20embedding%20and%20extraction.%20Experiments%20on%20real-world%20and%20AI-generated%20datasets%20demonstrate%20that%20the%20proposed%20method%20consistently%20outperforms%20state-of-the-art%20approaches%20in%20watermark%20extraction%20accuracy%20while%20maintaining%20high%20perceptual%20quality%2C%20showing%20strong%20robustness%20and%20generalization.&entry.1838667208=http%3A//arxiv.org/abs/2512.19438v1&entry.124074799=Read"},
{"title": "Neural Decoding of Overt Speech from ECoG Using Vision Transformers and Contrastive Representation Learning", "author": "Mohamed Baha Ben Ticha and Xingchen Ran and Guillaume Saldanha and Ga\u00ebl Le Godais and Phil\u00e9mon Roussel and Marc Aubert and Amina Fontanell and Thomas Costecalde and Lucas Struber and Serpil Karakas and Shaomin Zhang and Philippe Kahane and Guillaume Charvet and St\u00e9phan Chabard\u00e8s and Blaise Yvert", "abstract": "Speech Brain Computer Interfaces (BCIs) offer promising solutions to people with severe paralysis unable to communicate. A number of recent studies have demonstrated convincing reconstruction of intelligible speech from surface electrocorticographic (ECoG) or intracortical recordings by predicting a series of phonemes or words and using downstream language models to obtain meaningful sentences. A current challenge is to reconstruct speech in a streaming mode by directly regressing cortical signals into acoustic speech. While this has been achieved recently using intracortical data, further work is needed to obtain comparable results with surface ECoG recordings. In particular, optimizing neural decoders becomes critical in this case. Here we present an offline speech decoding pipeline based on an encoder-decoder deep neural architecture, integrating Vision Transformers and contrastive learning to enhance the direct regression of speech from ECoG signals. The approach is evaluated on two datasets, one obtained with clinical subdural electrodes in an epileptic patient, and another obtained with the fully implantable WIMAGINE epidural system in a participant of a motor BCI trial. To our knowledge this presents a first attempt to decode speech from a fully implantable and wireless epidural recording system offering perspectives for long-term use.", "link": "http://arxiv.org/abs/2512.04618v2", "date": "2025-12-22", "relevancy": 2.641, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5445}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5445}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4957}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Decoding%20of%20Overt%20Speech%20from%20ECoG%20Using%20Vision%20Transformers%20and%20Contrastive%20Representation%20Learning&body=Title%3A%20Neural%20Decoding%20of%20Overt%20Speech%20from%20ECoG%20Using%20Vision%20Transformers%20and%20Contrastive%20Representation%20Learning%0AAuthor%3A%20Mohamed%20Baha%20Ben%20Ticha%20and%20Xingchen%20Ran%20and%20Guillaume%20Saldanha%20and%20Ga%C3%ABl%20Le%20Godais%20and%20Phil%C3%A9mon%20Roussel%20and%20Marc%20Aubert%20and%20Amina%20Fontanell%20and%20Thomas%20Costecalde%20and%20Lucas%20Struber%20and%20Serpil%20Karakas%20and%20Shaomin%20Zhang%20and%20Philippe%20Kahane%20and%20Guillaume%20Charvet%20and%20St%C3%A9phan%20Chabard%C3%A8s%20and%20Blaise%20Yvert%0AAbstract%3A%20Speech%20Brain%20Computer%20Interfaces%20%28BCIs%29%20offer%20promising%20solutions%20to%20people%20with%20severe%20paralysis%20unable%20to%20communicate.%20A%20number%20of%20recent%20studies%20have%20demonstrated%20convincing%20reconstruction%20of%20intelligible%20speech%20from%20surface%20electrocorticographic%20%28ECoG%29%20or%20intracortical%20recordings%20by%20predicting%20a%20series%20of%20phonemes%20or%20words%20and%20using%20downstream%20language%20models%20to%20obtain%20meaningful%20sentences.%20A%20current%20challenge%20is%20to%20reconstruct%20speech%20in%20a%20streaming%20mode%20by%20directly%20regressing%20cortical%20signals%20into%20acoustic%20speech.%20While%20this%20has%20been%20achieved%20recently%20using%20intracortical%20data%2C%20further%20work%20is%20needed%20to%20obtain%20comparable%20results%20with%20surface%20ECoG%20recordings.%20In%20particular%2C%20optimizing%20neural%20decoders%20becomes%20critical%20in%20this%20case.%20Here%20we%20present%20an%20offline%20speech%20decoding%20pipeline%20based%20on%20an%20encoder-decoder%20deep%20neural%20architecture%2C%20integrating%20Vision%20Transformers%20and%20contrastive%20learning%20to%20enhance%20the%20direct%20regression%20of%20speech%20from%20ECoG%20signals.%20The%20approach%20is%20evaluated%20on%20two%20datasets%2C%20one%20obtained%20with%20clinical%20subdural%20electrodes%20in%20an%20epileptic%20patient%2C%20and%20another%20obtained%20with%20the%20fully%20implantable%20WIMAGINE%20epidural%20system%20in%20a%20participant%20of%20a%20motor%20BCI%20trial.%20To%20our%20knowledge%20this%20presents%20a%20first%20attempt%20to%20decode%20speech%20from%20a%20fully%20implantable%20and%20wireless%20epidural%20recording%20system%20offering%20perspectives%20for%20long-term%20use.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04618v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Decoding%2520of%2520Overt%2520Speech%2520from%2520ECoG%2520Using%2520Vision%2520Transformers%2520and%2520Contrastive%2520Representation%2520Learning%26entry.906535625%3DMohamed%2520Baha%2520Ben%2520Ticha%2520and%2520Xingchen%2520Ran%2520and%2520Guillaume%2520Saldanha%2520and%2520Ga%25C3%25ABl%2520Le%2520Godais%2520and%2520Phil%25C3%25A9mon%2520Roussel%2520and%2520Marc%2520Aubert%2520and%2520Amina%2520Fontanell%2520and%2520Thomas%2520Costecalde%2520and%2520Lucas%2520Struber%2520and%2520Serpil%2520Karakas%2520and%2520Shaomin%2520Zhang%2520and%2520Philippe%2520Kahane%2520and%2520Guillaume%2520Charvet%2520and%2520St%25C3%25A9phan%2520Chabard%25C3%25A8s%2520and%2520Blaise%2520Yvert%26entry.1292438233%3DSpeech%2520Brain%2520Computer%2520Interfaces%2520%2528BCIs%2529%2520offer%2520promising%2520solutions%2520to%2520people%2520with%2520severe%2520paralysis%2520unable%2520to%2520communicate.%2520A%2520number%2520of%2520recent%2520studies%2520have%2520demonstrated%2520convincing%2520reconstruction%2520of%2520intelligible%2520speech%2520from%2520surface%2520electrocorticographic%2520%2528ECoG%2529%2520or%2520intracortical%2520recordings%2520by%2520predicting%2520a%2520series%2520of%2520phonemes%2520or%2520words%2520and%2520using%2520downstream%2520language%2520models%2520to%2520obtain%2520meaningful%2520sentences.%2520A%2520current%2520challenge%2520is%2520to%2520reconstruct%2520speech%2520in%2520a%2520streaming%2520mode%2520by%2520directly%2520regressing%2520cortical%2520signals%2520into%2520acoustic%2520speech.%2520While%2520this%2520has%2520been%2520achieved%2520recently%2520using%2520intracortical%2520data%252C%2520further%2520work%2520is%2520needed%2520to%2520obtain%2520comparable%2520results%2520with%2520surface%2520ECoG%2520recordings.%2520In%2520particular%252C%2520optimizing%2520neural%2520decoders%2520becomes%2520critical%2520in%2520this%2520case.%2520Here%2520we%2520present%2520an%2520offline%2520speech%2520decoding%2520pipeline%2520based%2520on%2520an%2520encoder-decoder%2520deep%2520neural%2520architecture%252C%2520integrating%2520Vision%2520Transformers%2520and%2520contrastive%2520learning%2520to%2520enhance%2520the%2520direct%2520regression%2520of%2520speech%2520from%2520ECoG%2520signals.%2520The%2520approach%2520is%2520evaluated%2520on%2520two%2520datasets%252C%2520one%2520obtained%2520with%2520clinical%2520subdural%2520electrodes%2520in%2520an%2520epileptic%2520patient%252C%2520and%2520another%2520obtained%2520with%2520the%2520fully%2520implantable%2520WIMAGINE%2520epidural%2520system%2520in%2520a%2520participant%2520of%2520a%2520motor%2520BCI%2520trial.%2520To%2520our%2520knowledge%2520this%2520presents%2520a%2520first%2520attempt%2520to%2520decode%2520speech%2520from%2520a%2520fully%2520implantable%2520and%2520wireless%2520epidural%2520recording%2520system%2520offering%2520perspectives%2520for%2520long-term%2520use.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04618v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Decoding%20of%20Overt%20Speech%20from%20ECoG%20Using%20Vision%20Transformers%20and%20Contrastive%20Representation%20Learning&entry.906535625=Mohamed%20Baha%20Ben%20Ticha%20and%20Xingchen%20Ran%20and%20Guillaume%20Saldanha%20and%20Ga%C3%ABl%20Le%20Godais%20and%20Phil%C3%A9mon%20Roussel%20and%20Marc%20Aubert%20and%20Amina%20Fontanell%20and%20Thomas%20Costecalde%20and%20Lucas%20Struber%20and%20Serpil%20Karakas%20and%20Shaomin%20Zhang%20and%20Philippe%20Kahane%20and%20Guillaume%20Charvet%20and%20St%C3%A9phan%20Chabard%C3%A8s%20and%20Blaise%20Yvert&entry.1292438233=Speech%20Brain%20Computer%20Interfaces%20%28BCIs%29%20offer%20promising%20solutions%20to%20people%20with%20severe%20paralysis%20unable%20to%20communicate.%20A%20number%20of%20recent%20studies%20have%20demonstrated%20convincing%20reconstruction%20of%20intelligible%20speech%20from%20surface%20electrocorticographic%20%28ECoG%29%20or%20intracortical%20recordings%20by%20predicting%20a%20series%20of%20phonemes%20or%20words%20and%20using%20downstream%20language%20models%20to%20obtain%20meaningful%20sentences.%20A%20current%20challenge%20is%20to%20reconstruct%20speech%20in%20a%20streaming%20mode%20by%20directly%20regressing%20cortical%20signals%20into%20acoustic%20speech.%20While%20this%20has%20been%20achieved%20recently%20using%20intracortical%20data%2C%20further%20work%20is%20needed%20to%20obtain%20comparable%20results%20with%20surface%20ECoG%20recordings.%20In%20particular%2C%20optimizing%20neural%20decoders%20becomes%20critical%20in%20this%20case.%20Here%20we%20present%20an%20offline%20speech%20decoding%20pipeline%20based%20on%20an%20encoder-decoder%20deep%20neural%20architecture%2C%20integrating%20Vision%20Transformers%20and%20contrastive%20learning%20to%20enhance%20the%20direct%20regression%20of%20speech%20from%20ECoG%20signals.%20The%20approach%20is%20evaluated%20on%20two%20datasets%2C%20one%20obtained%20with%20clinical%20subdural%20electrodes%20in%20an%20epileptic%20patient%2C%20and%20another%20obtained%20with%20the%20fully%20implantable%20WIMAGINE%20epidural%20system%20in%20a%20participant%20of%20a%20motor%20BCI%20trial.%20To%20our%20knowledge%20this%20presents%20a%20first%20attempt%20to%20decode%20speech%20from%20a%20fully%20implantable%20and%20wireless%20epidural%20recording%20system%20offering%20perspectives%20for%20long-term%20use.&entry.1838667208=http%3A//arxiv.org/abs/2512.04618v2&entry.124074799=Read"},
{"title": "MAGIC: Achieving Superior Model Merging via Magnitude Calibration", "author": "Yayuan Li and Jian Zhang and Jintao Guo and Zihan Cheng and Lei Qi and Yinghuan Shi and Yang Gao", "abstract": "The proliferation of pre-trained models has given rise to a wide array of specialised, fine-tuned models. Model merging aims to merge the distinct capabilities of these specialised models into a unified model, requiring minimal or even no additional training. A core objective of model merging is to ensure the merged model retains the behavioural characteristics of the specialised models, typically achieved through feature alignment. We identify that features consist of two critical components: direction and magnitude. Prior research has predominantly focused on directional alignment, while the influence of magnitude remains largely neglected, despite its pronounced vulnerability to perturbations introduced by common merging operations (e.g., parameter fusion and sparsification). Such perturbations to magnitude inevitably lead to feature deviations in the merged model from the specialised models, resulting in subsequent performance degradation. To address this, we propose MAGnItude Calibration (MAGIC), a plug-and-play framework that rectifies layer-wise magnitudes in feature and weight spaces, with three variants. Specifically, our Feature Space Calibration (FSC) realigns the merged model's features using a small set of unlabelled data, while Weight Space Calibration (WSC) extends this calibration to the weight space without requiring additional data. Combining these yields Dual Space Calibration (DSC). Comprehensive experiments demonstrate that MAGIC consistently boosts performance across diverse Computer Vision tasks (+4.3% on eight datasets) and NLP tasks (+8.0% on Llama) without additional training. Our code is available at: https://github.com/lyymuwu/MAGIC", "link": "http://arxiv.org/abs/2512.19320v1", "date": "2025-12-22", "relevancy": 2.6166, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5277}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5263}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MAGIC%3A%20Achieving%20Superior%20Model%20Merging%20via%20Magnitude%20Calibration&body=Title%3A%20MAGIC%3A%20Achieving%20Superior%20Model%20Merging%20via%20Magnitude%20Calibration%0AAuthor%3A%20Yayuan%20Li%20and%20Jian%20Zhang%20and%20Jintao%20Guo%20and%20Zihan%20Cheng%20and%20Lei%20Qi%20and%20Yinghuan%20Shi%20and%20Yang%20Gao%0AAbstract%3A%20The%20proliferation%20of%20pre-trained%20models%20has%20given%20rise%20to%20a%20wide%20array%20of%20specialised%2C%20fine-tuned%20models.%20Model%20merging%20aims%20to%20merge%20the%20distinct%20capabilities%20of%20these%20specialised%20models%20into%20a%20unified%20model%2C%20requiring%20minimal%20or%20even%20no%20additional%20training.%20A%20core%20objective%20of%20model%20merging%20is%20to%20ensure%20the%20merged%20model%20retains%20the%20behavioural%20characteristics%20of%20the%20specialised%20models%2C%20typically%20achieved%20through%20feature%20alignment.%20We%20identify%20that%20features%20consist%20of%20two%20critical%20components%3A%20direction%20and%20magnitude.%20Prior%20research%20has%20predominantly%20focused%20on%20directional%20alignment%2C%20while%20the%20influence%20of%20magnitude%20remains%20largely%20neglected%2C%20despite%20its%20pronounced%20vulnerability%20to%20perturbations%20introduced%20by%20common%20merging%20operations%20%28e.g.%2C%20parameter%20fusion%20and%20sparsification%29.%20Such%20perturbations%20to%20magnitude%20inevitably%20lead%20to%20feature%20deviations%20in%20the%20merged%20model%20from%20the%20specialised%20models%2C%20resulting%20in%20subsequent%20performance%20degradation.%20To%20address%20this%2C%20we%20propose%20MAGnItude%20Calibration%20%28MAGIC%29%2C%20a%20plug-and-play%20framework%20that%20rectifies%20layer-wise%20magnitudes%20in%20feature%20and%20weight%20spaces%2C%20with%20three%20variants.%20Specifically%2C%20our%20Feature%20Space%20Calibration%20%28FSC%29%20realigns%20the%20merged%20model%27s%20features%20using%20a%20small%20set%20of%20unlabelled%20data%2C%20while%20Weight%20Space%20Calibration%20%28WSC%29%20extends%20this%20calibration%20to%20the%20weight%20space%20without%20requiring%20additional%20data.%20Combining%20these%20yields%20Dual%20Space%20Calibration%20%28DSC%29.%20Comprehensive%20experiments%20demonstrate%20that%20MAGIC%20consistently%20boosts%20performance%20across%20diverse%20Computer%20Vision%20tasks%20%28%2B4.3%25%20on%20eight%20datasets%29%20and%20NLP%20tasks%20%28%2B8.0%25%20on%20Llama%29%20without%20additional%20training.%20Our%20code%20is%20available%20at%3A%20https%3A//github.com/lyymuwu/MAGIC%0ALink%3A%20http%3A//arxiv.org/abs/2512.19320v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMAGIC%253A%2520Achieving%2520Superior%2520Model%2520Merging%2520via%2520Magnitude%2520Calibration%26entry.906535625%3DYayuan%2520Li%2520and%2520Jian%2520Zhang%2520and%2520Jintao%2520Guo%2520and%2520Zihan%2520Cheng%2520and%2520Lei%2520Qi%2520and%2520Yinghuan%2520Shi%2520and%2520Yang%2520Gao%26entry.1292438233%3DThe%2520proliferation%2520of%2520pre-trained%2520models%2520has%2520given%2520rise%2520to%2520a%2520wide%2520array%2520of%2520specialised%252C%2520fine-tuned%2520models.%2520Model%2520merging%2520aims%2520to%2520merge%2520the%2520distinct%2520capabilities%2520of%2520these%2520specialised%2520models%2520into%2520a%2520unified%2520model%252C%2520requiring%2520minimal%2520or%2520even%2520no%2520additional%2520training.%2520A%2520core%2520objective%2520of%2520model%2520merging%2520is%2520to%2520ensure%2520the%2520merged%2520model%2520retains%2520the%2520behavioural%2520characteristics%2520of%2520the%2520specialised%2520models%252C%2520typically%2520achieved%2520through%2520feature%2520alignment.%2520We%2520identify%2520that%2520features%2520consist%2520of%2520two%2520critical%2520components%253A%2520direction%2520and%2520magnitude.%2520Prior%2520research%2520has%2520predominantly%2520focused%2520on%2520directional%2520alignment%252C%2520while%2520the%2520influence%2520of%2520magnitude%2520remains%2520largely%2520neglected%252C%2520despite%2520its%2520pronounced%2520vulnerability%2520to%2520perturbations%2520introduced%2520by%2520common%2520merging%2520operations%2520%2528e.g.%252C%2520parameter%2520fusion%2520and%2520sparsification%2529.%2520Such%2520perturbations%2520to%2520magnitude%2520inevitably%2520lead%2520to%2520feature%2520deviations%2520in%2520the%2520merged%2520model%2520from%2520the%2520specialised%2520models%252C%2520resulting%2520in%2520subsequent%2520performance%2520degradation.%2520To%2520address%2520this%252C%2520we%2520propose%2520MAGnItude%2520Calibration%2520%2528MAGIC%2529%252C%2520a%2520plug-and-play%2520framework%2520that%2520rectifies%2520layer-wise%2520magnitudes%2520in%2520feature%2520and%2520weight%2520spaces%252C%2520with%2520three%2520variants.%2520Specifically%252C%2520our%2520Feature%2520Space%2520Calibration%2520%2528FSC%2529%2520realigns%2520the%2520merged%2520model%2527s%2520features%2520using%2520a%2520small%2520set%2520of%2520unlabelled%2520data%252C%2520while%2520Weight%2520Space%2520Calibration%2520%2528WSC%2529%2520extends%2520this%2520calibration%2520to%2520the%2520weight%2520space%2520without%2520requiring%2520additional%2520data.%2520Combining%2520these%2520yields%2520Dual%2520Space%2520Calibration%2520%2528DSC%2529.%2520Comprehensive%2520experiments%2520demonstrate%2520that%2520MAGIC%2520consistently%2520boosts%2520performance%2520across%2520diverse%2520Computer%2520Vision%2520tasks%2520%2528%252B4.3%2525%2520on%2520eight%2520datasets%2529%2520and%2520NLP%2520tasks%2520%2528%252B8.0%2525%2520on%2520Llama%2529%2520without%2520additional%2520training.%2520Our%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/lyymuwu/MAGIC%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19320v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MAGIC%3A%20Achieving%20Superior%20Model%20Merging%20via%20Magnitude%20Calibration&entry.906535625=Yayuan%20Li%20and%20Jian%20Zhang%20and%20Jintao%20Guo%20and%20Zihan%20Cheng%20and%20Lei%20Qi%20and%20Yinghuan%20Shi%20and%20Yang%20Gao&entry.1292438233=The%20proliferation%20of%20pre-trained%20models%20has%20given%20rise%20to%20a%20wide%20array%20of%20specialised%2C%20fine-tuned%20models.%20Model%20merging%20aims%20to%20merge%20the%20distinct%20capabilities%20of%20these%20specialised%20models%20into%20a%20unified%20model%2C%20requiring%20minimal%20or%20even%20no%20additional%20training.%20A%20core%20objective%20of%20model%20merging%20is%20to%20ensure%20the%20merged%20model%20retains%20the%20behavioural%20characteristics%20of%20the%20specialised%20models%2C%20typically%20achieved%20through%20feature%20alignment.%20We%20identify%20that%20features%20consist%20of%20two%20critical%20components%3A%20direction%20and%20magnitude.%20Prior%20research%20has%20predominantly%20focused%20on%20directional%20alignment%2C%20while%20the%20influence%20of%20magnitude%20remains%20largely%20neglected%2C%20despite%20its%20pronounced%20vulnerability%20to%20perturbations%20introduced%20by%20common%20merging%20operations%20%28e.g.%2C%20parameter%20fusion%20and%20sparsification%29.%20Such%20perturbations%20to%20magnitude%20inevitably%20lead%20to%20feature%20deviations%20in%20the%20merged%20model%20from%20the%20specialised%20models%2C%20resulting%20in%20subsequent%20performance%20degradation.%20To%20address%20this%2C%20we%20propose%20MAGnItude%20Calibration%20%28MAGIC%29%2C%20a%20plug-and-play%20framework%20that%20rectifies%20layer-wise%20magnitudes%20in%20feature%20and%20weight%20spaces%2C%20with%20three%20variants.%20Specifically%2C%20our%20Feature%20Space%20Calibration%20%28FSC%29%20realigns%20the%20merged%20model%27s%20features%20using%20a%20small%20set%20of%20unlabelled%20data%2C%20while%20Weight%20Space%20Calibration%20%28WSC%29%20extends%20this%20calibration%20to%20the%20weight%20space%20without%20requiring%20additional%20data.%20Combining%20these%20yields%20Dual%20Space%20Calibration%20%28DSC%29.%20Comprehensive%20experiments%20demonstrate%20that%20MAGIC%20consistently%20boosts%20performance%20across%20diverse%20Computer%20Vision%20tasks%20%28%2B4.3%25%20on%20eight%20datasets%29%20and%20NLP%20tasks%20%28%2B8.0%25%20on%20Llama%29%20without%20additional%20training.%20Our%20code%20is%20available%20at%3A%20https%3A//github.com/lyymuwu/MAGIC&entry.1838667208=http%3A//arxiv.org/abs/2512.19320v1&entry.124074799=Read"},
{"title": "LIMOncello: Revisited IKFoM on the SGal(3) Manifold for Fast LiDAR-Inertial Odometry", "author": "Carlos P\u00e9rez-Ruiz and Joan Sol\u00e0", "abstract": "This work introduces LIMOncello, a tightly coupled LiDAR-Inertial Odometry system that models 6-DoF motion on the $\\mathrm{SGal}(3)$ manifold within an iterated error-state Kalman filter backend. Compared to state representations defined on $\\mathrm{SO}(3)\\times\\mathbb{R}^6$, the use of $\\mathrm{SGal}(3)$ provides a coherent and numerically stable discrete-time propagation model that helps limit drift in low-observability conditions.\n  LIMOncello also includes a lightweight incremental i-Octree mapping backend that enables faster updates and substantially lower memory usage than incremental kd-tree style map structures, without relying on locality-restricted search heuristics. Experiments on multiple real-world datasets show that LIMOncello achieves competitive accuracy while improving robustness in geometrically sparse environments. The system maintains real-time performance with stable memory growth and is released as an extensible open-source implementation at https://github.com/CPerezRuiz335/LIMOncello.", "link": "http://arxiv.org/abs/2512.19567v1", "date": "2025-12-22", "relevancy": 2.5868, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5343}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5214}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4964}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LIMOncello%3A%20Revisited%20IKFoM%20on%20the%20SGal%283%29%20Manifold%20for%20Fast%20LiDAR-Inertial%20Odometry&body=Title%3A%20LIMOncello%3A%20Revisited%20IKFoM%20on%20the%20SGal%283%29%20Manifold%20for%20Fast%20LiDAR-Inertial%20Odometry%0AAuthor%3A%20Carlos%20P%C3%A9rez-Ruiz%20and%20Joan%20Sol%C3%A0%0AAbstract%3A%20This%20work%20introduces%20LIMOncello%2C%20a%20tightly%20coupled%20LiDAR-Inertial%20Odometry%20system%20that%20models%206-DoF%20motion%20on%20the%20%24%5Cmathrm%7BSGal%7D%283%29%24%20manifold%20within%20an%20iterated%20error-state%20Kalman%20filter%20backend.%20Compared%20to%20state%20representations%20defined%20on%20%24%5Cmathrm%7BSO%7D%283%29%5Ctimes%5Cmathbb%7BR%7D%5E6%24%2C%20the%20use%20of%20%24%5Cmathrm%7BSGal%7D%283%29%24%20provides%20a%20coherent%20and%20numerically%20stable%20discrete-time%20propagation%20model%20that%20helps%20limit%20drift%20in%20low-observability%20conditions.%0A%20%20LIMOncello%20also%20includes%20a%20lightweight%20incremental%20i-Octree%20mapping%20backend%20that%20enables%20faster%20updates%20and%20substantially%20lower%20memory%20usage%20than%20incremental%20kd-tree%20style%20map%20structures%2C%20without%20relying%20on%20locality-restricted%20search%20heuristics.%20Experiments%20on%20multiple%20real-world%20datasets%20show%20that%20LIMOncello%20achieves%20competitive%20accuracy%20while%20improving%20robustness%20in%20geometrically%20sparse%20environments.%20The%20system%20maintains%20real-time%20performance%20with%20stable%20memory%20growth%20and%20is%20released%20as%20an%20extensible%20open-source%20implementation%20at%20https%3A//github.com/CPerezRuiz335/LIMOncello.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19567v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLIMOncello%253A%2520Revisited%2520IKFoM%2520on%2520the%2520SGal%25283%2529%2520Manifold%2520for%2520Fast%2520LiDAR-Inertial%2520Odometry%26entry.906535625%3DCarlos%2520P%25C3%25A9rez-Ruiz%2520and%2520Joan%2520Sol%25C3%25A0%26entry.1292438233%3DThis%2520work%2520introduces%2520LIMOncello%252C%2520a%2520tightly%2520coupled%2520LiDAR-Inertial%2520Odometry%2520system%2520that%2520models%25206-DoF%2520motion%2520on%2520the%2520%2524%255Cmathrm%257BSGal%257D%25283%2529%2524%2520manifold%2520within%2520an%2520iterated%2520error-state%2520Kalman%2520filter%2520backend.%2520Compared%2520to%2520state%2520representations%2520defined%2520on%2520%2524%255Cmathrm%257BSO%257D%25283%2529%255Ctimes%255Cmathbb%257BR%257D%255E6%2524%252C%2520the%2520use%2520of%2520%2524%255Cmathrm%257BSGal%257D%25283%2529%2524%2520provides%2520a%2520coherent%2520and%2520numerically%2520stable%2520discrete-time%2520propagation%2520model%2520that%2520helps%2520limit%2520drift%2520in%2520low-observability%2520conditions.%250A%2520%2520LIMOncello%2520also%2520includes%2520a%2520lightweight%2520incremental%2520i-Octree%2520mapping%2520backend%2520that%2520enables%2520faster%2520updates%2520and%2520substantially%2520lower%2520memory%2520usage%2520than%2520incremental%2520kd-tree%2520style%2520map%2520structures%252C%2520without%2520relying%2520on%2520locality-restricted%2520search%2520heuristics.%2520Experiments%2520on%2520multiple%2520real-world%2520datasets%2520show%2520that%2520LIMOncello%2520achieves%2520competitive%2520accuracy%2520while%2520improving%2520robustness%2520in%2520geometrically%2520sparse%2520environments.%2520The%2520system%2520maintains%2520real-time%2520performance%2520with%2520stable%2520memory%2520growth%2520and%2520is%2520released%2520as%2520an%2520extensible%2520open-source%2520implementation%2520at%2520https%253A//github.com/CPerezRuiz335/LIMOncello.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19567v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LIMOncello%3A%20Revisited%20IKFoM%20on%20the%20SGal%283%29%20Manifold%20for%20Fast%20LiDAR-Inertial%20Odometry&entry.906535625=Carlos%20P%C3%A9rez-Ruiz%20and%20Joan%20Sol%C3%A0&entry.1292438233=This%20work%20introduces%20LIMOncello%2C%20a%20tightly%20coupled%20LiDAR-Inertial%20Odometry%20system%20that%20models%206-DoF%20motion%20on%20the%20%24%5Cmathrm%7BSGal%7D%283%29%24%20manifold%20within%20an%20iterated%20error-state%20Kalman%20filter%20backend.%20Compared%20to%20state%20representations%20defined%20on%20%24%5Cmathrm%7BSO%7D%283%29%5Ctimes%5Cmathbb%7BR%7D%5E6%24%2C%20the%20use%20of%20%24%5Cmathrm%7BSGal%7D%283%29%24%20provides%20a%20coherent%20and%20numerically%20stable%20discrete-time%20propagation%20model%20that%20helps%20limit%20drift%20in%20low-observability%20conditions.%0A%20%20LIMOncello%20also%20includes%20a%20lightweight%20incremental%20i-Octree%20mapping%20backend%20that%20enables%20faster%20updates%20and%20substantially%20lower%20memory%20usage%20than%20incremental%20kd-tree%20style%20map%20structures%2C%20without%20relying%20on%20locality-restricted%20search%20heuristics.%20Experiments%20on%20multiple%20real-world%20datasets%20show%20that%20LIMOncello%20achieves%20competitive%20accuracy%20while%20improving%20robustness%20in%20geometrically%20sparse%20environments.%20The%20system%20maintains%20real-time%20performance%20with%20stable%20memory%20growth%20and%20is%20released%20as%20an%20extensible%20open-source%20implementation%20at%20https%3A//github.com/CPerezRuiz335/LIMOncello.&entry.1838667208=http%3A//arxiv.org/abs/2512.19567v1&entry.124074799=Read"},
{"title": "Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context", "author": "Kyungwon Cho and Hanbyul Joo", "abstract": "Egocentric vision systems are becoming widely available, creating new opportunities for human-computer interaction. A core challenge is estimating the wearer's full-body motion from first-person videos, which is crucial for understanding human behavior. However, this task is difficult since most body parts are invisible from the egocentric view. Prior approaches mainly rely on head trajectories, leading to ambiguity, or assume continuously tracked hands, which is unrealistic for lightweight egocentric devices. In this work, we present HaMoS, the first hand-aware, sequence-level diffusion framework that directly conditions on both head trajectory and intermittently visible hand cues caused by field-of-view limitations and occlusions, as in real-world egocentric devices. To overcome the lack of datasets pairing diverse camera views with human motion, we introduce a novel augmentation method that models such real-world conditions. We also demonstrate that sequence-level contexts such as body shape and field-of-view are crucial for accurate motion reconstruction, and thus employ local attention to infer long sequences efficiently. Experiments on public benchmarks show that our method achieves state-of-the-art accuracy and temporal smoothness, demonstrating a practical step toward reliable in-the-wild egocentric 3D motion understanding.", "link": "http://arxiv.org/abs/2512.19283v1", "date": "2025-12-22", "relevancy": 2.5764, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6596}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6547}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.579}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hand-Aware%20Egocentric%20Motion%20Reconstruction%20with%20Sequence-Level%20Context&body=Title%3A%20Hand-Aware%20Egocentric%20Motion%20Reconstruction%20with%20Sequence-Level%20Context%0AAuthor%3A%20Kyungwon%20Cho%20and%20Hanbyul%20Joo%0AAbstract%3A%20Egocentric%20vision%20systems%20are%20becoming%20widely%20available%2C%20creating%20new%20opportunities%20for%20human-computer%20interaction.%20A%20core%20challenge%20is%20estimating%20the%20wearer%27s%20full-body%20motion%20from%20first-person%20videos%2C%20which%20is%20crucial%20for%20understanding%20human%20behavior.%20However%2C%20this%20task%20is%20difficult%20since%20most%20body%20parts%20are%20invisible%20from%20the%20egocentric%20view.%20Prior%20approaches%20mainly%20rely%20on%20head%20trajectories%2C%20leading%20to%20ambiguity%2C%20or%20assume%20continuously%20tracked%20hands%2C%20which%20is%20unrealistic%20for%20lightweight%20egocentric%20devices.%20In%20this%20work%2C%20we%20present%20HaMoS%2C%20the%20first%20hand-aware%2C%20sequence-level%20diffusion%20framework%20that%20directly%20conditions%20on%20both%20head%20trajectory%20and%20intermittently%20visible%20hand%20cues%20caused%20by%20field-of-view%20limitations%20and%20occlusions%2C%20as%20in%20real-world%20egocentric%20devices.%20To%20overcome%20the%20lack%20of%20datasets%20pairing%20diverse%20camera%20views%20with%20human%20motion%2C%20we%20introduce%20a%20novel%20augmentation%20method%20that%20models%20such%20real-world%20conditions.%20We%20also%20demonstrate%20that%20sequence-level%20contexts%20such%20as%20body%20shape%20and%20field-of-view%20are%20crucial%20for%20accurate%20motion%20reconstruction%2C%20and%20thus%20employ%20local%20attention%20to%20infer%20long%20sequences%20efficiently.%20Experiments%20on%20public%20benchmarks%20show%20that%20our%20method%20achieves%20state-of-the-art%20accuracy%20and%20temporal%20smoothness%2C%20demonstrating%20a%20practical%20step%20toward%20reliable%20in-the-wild%20egocentric%203D%20motion%20understanding.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19283v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHand-Aware%2520Egocentric%2520Motion%2520Reconstruction%2520with%2520Sequence-Level%2520Context%26entry.906535625%3DKyungwon%2520Cho%2520and%2520Hanbyul%2520Joo%26entry.1292438233%3DEgocentric%2520vision%2520systems%2520are%2520becoming%2520widely%2520available%252C%2520creating%2520new%2520opportunities%2520for%2520human-computer%2520interaction.%2520A%2520core%2520challenge%2520is%2520estimating%2520the%2520wearer%2527s%2520full-body%2520motion%2520from%2520first-person%2520videos%252C%2520which%2520is%2520crucial%2520for%2520understanding%2520human%2520behavior.%2520However%252C%2520this%2520task%2520is%2520difficult%2520since%2520most%2520body%2520parts%2520are%2520invisible%2520from%2520the%2520egocentric%2520view.%2520Prior%2520approaches%2520mainly%2520rely%2520on%2520head%2520trajectories%252C%2520leading%2520to%2520ambiguity%252C%2520or%2520assume%2520continuously%2520tracked%2520hands%252C%2520which%2520is%2520unrealistic%2520for%2520lightweight%2520egocentric%2520devices.%2520In%2520this%2520work%252C%2520we%2520present%2520HaMoS%252C%2520the%2520first%2520hand-aware%252C%2520sequence-level%2520diffusion%2520framework%2520that%2520directly%2520conditions%2520on%2520both%2520head%2520trajectory%2520and%2520intermittently%2520visible%2520hand%2520cues%2520caused%2520by%2520field-of-view%2520limitations%2520and%2520occlusions%252C%2520as%2520in%2520real-world%2520egocentric%2520devices.%2520To%2520overcome%2520the%2520lack%2520of%2520datasets%2520pairing%2520diverse%2520camera%2520views%2520with%2520human%2520motion%252C%2520we%2520introduce%2520a%2520novel%2520augmentation%2520method%2520that%2520models%2520such%2520real-world%2520conditions.%2520We%2520also%2520demonstrate%2520that%2520sequence-level%2520contexts%2520such%2520as%2520body%2520shape%2520and%2520field-of-view%2520are%2520crucial%2520for%2520accurate%2520motion%2520reconstruction%252C%2520and%2520thus%2520employ%2520local%2520attention%2520to%2520infer%2520long%2520sequences%2520efficiently.%2520Experiments%2520on%2520public%2520benchmarks%2520show%2520that%2520our%2520method%2520achieves%2520state-of-the-art%2520accuracy%2520and%2520temporal%2520smoothness%252C%2520demonstrating%2520a%2520practical%2520step%2520toward%2520reliable%2520in-the-wild%2520egocentric%25203D%2520motion%2520understanding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19283v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hand-Aware%20Egocentric%20Motion%20Reconstruction%20with%20Sequence-Level%20Context&entry.906535625=Kyungwon%20Cho%20and%20Hanbyul%20Joo&entry.1292438233=Egocentric%20vision%20systems%20are%20becoming%20widely%20available%2C%20creating%20new%20opportunities%20for%20human-computer%20interaction.%20A%20core%20challenge%20is%20estimating%20the%20wearer%27s%20full-body%20motion%20from%20first-person%20videos%2C%20which%20is%20crucial%20for%20understanding%20human%20behavior.%20However%2C%20this%20task%20is%20difficult%20since%20most%20body%20parts%20are%20invisible%20from%20the%20egocentric%20view.%20Prior%20approaches%20mainly%20rely%20on%20head%20trajectories%2C%20leading%20to%20ambiguity%2C%20or%20assume%20continuously%20tracked%20hands%2C%20which%20is%20unrealistic%20for%20lightweight%20egocentric%20devices.%20In%20this%20work%2C%20we%20present%20HaMoS%2C%20the%20first%20hand-aware%2C%20sequence-level%20diffusion%20framework%20that%20directly%20conditions%20on%20both%20head%20trajectory%20and%20intermittently%20visible%20hand%20cues%20caused%20by%20field-of-view%20limitations%20and%20occlusions%2C%20as%20in%20real-world%20egocentric%20devices.%20To%20overcome%20the%20lack%20of%20datasets%20pairing%20diverse%20camera%20views%20with%20human%20motion%2C%20we%20introduce%20a%20novel%20augmentation%20method%20that%20models%20such%20real-world%20conditions.%20We%20also%20demonstrate%20that%20sequence-level%20contexts%20such%20as%20body%20shape%20and%20field-of-view%20are%20crucial%20for%20accurate%20motion%20reconstruction%2C%20and%20thus%20employ%20local%20attention%20to%20infer%20long%20sequences%20efficiently.%20Experiments%20on%20public%20benchmarks%20show%20that%20our%20method%20achieves%20state-of-the-art%20accuracy%20and%20temporal%20smoothness%2C%20demonstrating%20a%20practical%20step%20toward%20reliable%20in-the-wild%20egocentric%203D%20motion%20understanding.&entry.1838667208=http%3A//arxiv.org/abs/2512.19283v1&entry.124074799=Read"},
{"title": "Neural Implicit Heart Coordinates: 3D cardiac shape reconstruction from sparse segmentations", "author": "Marica Muffoletto and Uxio Hermida and Charl\u00e8ne Mauger and Avan Suinesiaputra and Yiyang Xu and Richard Burns and Lisa Pankewitz and Andrew D McCulloch and Steffen E Petersen and Daniel Rueckert and Alistair A Young", "abstract": "Accurate reconstruction of cardiac anatomy from sparse clinical images remains a major challenge in patient-specific modeling. While neural implicit functions have previously been applied to this task, their application to mapping anatomical consistency across subjects has been limited. In this work, we introduce Neural Implicit Heart Coordinates (NIHCs), a standardized implicit coordinate system, based on universal ventricular coordinates, that provides a common anatomical reference frame for the human heart. Our method predicts NIHCs directly from a limited number of 2D segmentations (sparse acquisition) and subsequently decodes them into dense 3D segmentations and high-resolution meshes at arbitrary output resolution. Trained on a large dataset of 5,000 cardiac meshes, the model achieves high reconstruction accuracy on clinical contours, with mean Euclidean surface errors of 2.51$\\pm$0.33 mm in a diseased cohort (n=4549) and 2.3$\\pm$0.36 mm in a healthy cohort (n=5576). The NIHC representation enables anatomically coherent reconstruction even under severe slice sparsity and segmentation noise, faithfully recovering complex structures such as the valve planes. Compared with traditional pipelines, inference time is reduced from over 60 s to 5-15 s. These results demonstrate that NIHCs constitute a robust and efficient anatomical representation for patient-specific 3D cardiac reconstruction from minimal input data.", "link": "http://arxiv.org/abs/2512.19316v1", "date": "2025-12-22", "relevancy": 2.5687, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5227}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.51}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Neural%20Implicit%20Heart%20Coordinates%3A%203D%20cardiac%20shape%20reconstruction%20from%20sparse%20segmentations&body=Title%3A%20Neural%20Implicit%20Heart%20Coordinates%3A%203D%20cardiac%20shape%20reconstruction%20from%20sparse%20segmentations%0AAuthor%3A%20Marica%20Muffoletto%20and%20Uxio%20Hermida%20and%20Charl%C3%A8ne%20Mauger%20and%20Avan%20Suinesiaputra%20and%20Yiyang%20Xu%20and%20Richard%20Burns%20and%20Lisa%20Pankewitz%20and%20Andrew%20D%20McCulloch%20and%20Steffen%20E%20Petersen%20and%20Daniel%20Rueckert%20and%20Alistair%20A%20Young%0AAbstract%3A%20Accurate%20reconstruction%20of%20cardiac%20anatomy%20from%20sparse%20clinical%20images%20remains%20a%20major%20challenge%20in%20patient-specific%20modeling.%20While%20neural%20implicit%20functions%20have%20previously%20been%20applied%20to%20this%20task%2C%20their%20application%20to%20mapping%20anatomical%20consistency%20across%20subjects%20has%20been%20limited.%20In%20this%20work%2C%20we%20introduce%20Neural%20Implicit%20Heart%20Coordinates%20%28NIHCs%29%2C%20a%20standardized%20implicit%20coordinate%20system%2C%20based%20on%20universal%20ventricular%20coordinates%2C%20that%20provides%20a%20common%20anatomical%20reference%20frame%20for%20the%20human%20heart.%20Our%20method%20predicts%20NIHCs%20directly%20from%20a%20limited%20number%20of%202D%20segmentations%20%28sparse%20acquisition%29%20and%20subsequently%20decodes%20them%20into%20dense%203D%20segmentations%20and%20high-resolution%20meshes%20at%20arbitrary%20output%20resolution.%20Trained%20on%20a%20large%20dataset%20of%205%2C000%20cardiac%20meshes%2C%20the%20model%20achieves%20high%20reconstruction%20accuracy%20on%20clinical%20contours%2C%20with%20mean%20Euclidean%20surface%20errors%20of%202.51%24%5Cpm%240.33%20mm%20in%20a%20diseased%20cohort%20%28n%3D4549%29%20and%202.3%24%5Cpm%240.36%20mm%20in%20a%20healthy%20cohort%20%28n%3D5576%29.%20The%20NIHC%20representation%20enables%20anatomically%20coherent%20reconstruction%20even%20under%20severe%20slice%20sparsity%20and%20segmentation%20noise%2C%20faithfully%20recovering%20complex%20structures%20such%20as%20the%20valve%20planes.%20Compared%20with%20traditional%20pipelines%2C%20inference%20time%20is%20reduced%20from%20over%2060%20s%20to%205-15%20s.%20These%20results%20demonstrate%20that%20NIHCs%20constitute%20a%20robust%20and%20efficient%20anatomical%20representation%20for%20patient-specific%203D%20cardiac%20reconstruction%20from%20minimal%20input%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19316v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeural%2520Implicit%2520Heart%2520Coordinates%253A%25203D%2520cardiac%2520shape%2520reconstruction%2520from%2520sparse%2520segmentations%26entry.906535625%3DMarica%2520Muffoletto%2520and%2520Uxio%2520Hermida%2520and%2520Charl%25C3%25A8ne%2520Mauger%2520and%2520Avan%2520Suinesiaputra%2520and%2520Yiyang%2520Xu%2520and%2520Richard%2520Burns%2520and%2520Lisa%2520Pankewitz%2520and%2520Andrew%2520D%2520McCulloch%2520and%2520Steffen%2520E%2520Petersen%2520and%2520Daniel%2520Rueckert%2520and%2520Alistair%2520A%2520Young%26entry.1292438233%3DAccurate%2520reconstruction%2520of%2520cardiac%2520anatomy%2520from%2520sparse%2520clinical%2520images%2520remains%2520a%2520major%2520challenge%2520in%2520patient-specific%2520modeling.%2520While%2520neural%2520implicit%2520functions%2520have%2520previously%2520been%2520applied%2520to%2520this%2520task%252C%2520their%2520application%2520to%2520mapping%2520anatomical%2520consistency%2520across%2520subjects%2520has%2520been%2520limited.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Neural%2520Implicit%2520Heart%2520Coordinates%2520%2528NIHCs%2529%252C%2520a%2520standardized%2520implicit%2520coordinate%2520system%252C%2520based%2520on%2520universal%2520ventricular%2520coordinates%252C%2520that%2520provides%2520a%2520common%2520anatomical%2520reference%2520frame%2520for%2520the%2520human%2520heart.%2520Our%2520method%2520predicts%2520NIHCs%2520directly%2520from%2520a%2520limited%2520number%2520of%25202D%2520segmentations%2520%2528sparse%2520acquisition%2529%2520and%2520subsequently%2520decodes%2520them%2520into%2520dense%25203D%2520segmentations%2520and%2520high-resolution%2520meshes%2520at%2520arbitrary%2520output%2520resolution.%2520Trained%2520on%2520a%2520large%2520dataset%2520of%25205%252C000%2520cardiac%2520meshes%252C%2520the%2520model%2520achieves%2520high%2520reconstruction%2520accuracy%2520on%2520clinical%2520contours%252C%2520with%2520mean%2520Euclidean%2520surface%2520errors%2520of%25202.51%2524%255Cpm%25240.33%2520mm%2520in%2520a%2520diseased%2520cohort%2520%2528n%253D4549%2529%2520and%25202.3%2524%255Cpm%25240.36%2520mm%2520in%2520a%2520healthy%2520cohort%2520%2528n%253D5576%2529.%2520The%2520NIHC%2520representation%2520enables%2520anatomically%2520coherent%2520reconstruction%2520even%2520under%2520severe%2520slice%2520sparsity%2520and%2520segmentation%2520noise%252C%2520faithfully%2520recovering%2520complex%2520structures%2520such%2520as%2520the%2520valve%2520planes.%2520Compared%2520with%2520traditional%2520pipelines%252C%2520inference%2520time%2520is%2520reduced%2520from%2520over%252060%2520s%2520to%25205-15%2520s.%2520These%2520results%2520demonstrate%2520that%2520NIHCs%2520constitute%2520a%2520robust%2520and%2520efficient%2520anatomical%2520representation%2520for%2520patient-specific%25203D%2520cardiac%2520reconstruction%2520from%2520minimal%2520input%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19316v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Implicit%20Heart%20Coordinates%3A%203D%20cardiac%20shape%20reconstruction%20from%20sparse%20segmentations&entry.906535625=Marica%20Muffoletto%20and%20Uxio%20Hermida%20and%20Charl%C3%A8ne%20Mauger%20and%20Avan%20Suinesiaputra%20and%20Yiyang%20Xu%20and%20Richard%20Burns%20and%20Lisa%20Pankewitz%20and%20Andrew%20D%20McCulloch%20and%20Steffen%20E%20Petersen%20and%20Daniel%20Rueckert%20and%20Alistair%20A%20Young&entry.1292438233=Accurate%20reconstruction%20of%20cardiac%20anatomy%20from%20sparse%20clinical%20images%20remains%20a%20major%20challenge%20in%20patient-specific%20modeling.%20While%20neural%20implicit%20functions%20have%20previously%20been%20applied%20to%20this%20task%2C%20their%20application%20to%20mapping%20anatomical%20consistency%20across%20subjects%20has%20been%20limited.%20In%20this%20work%2C%20we%20introduce%20Neural%20Implicit%20Heart%20Coordinates%20%28NIHCs%29%2C%20a%20standardized%20implicit%20coordinate%20system%2C%20based%20on%20universal%20ventricular%20coordinates%2C%20that%20provides%20a%20common%20anatomical%20reference%20frame%20for%20the%20human%20heart.%20Our%20method%20predicts%20NIHCs%20directly%20from%20a%20limited%20number%20of%202D%20segmentations%20%28sparse%20acquisition%29%20and%20subsequently%20decodes%20them%20into%20dense%203D%20segmentations%20and%20high-resolution%20meshes%20at%20arbitrary%20output%20resolution.%20Trained%20on%20a%20large%20dataset%20of%205%2C000%20cardiac%20meshes%2C%20the%20model%20achieves%20high%20reconstruction%20accuracy%20on%20clinical%20contours%2C%20with%20mean%20Euclidean%20surface%20errors%20of%202.51%24%5Cpm%240.33%20mm%20in%20a%20diseased%20cohort%20%28n%3D4549%29%20and%202.3%24%5Cpm%240.36%20mm%20in%20a%20healthy%20cohort%20%28n%3D5576%29.%20The%20NIHC%20representation%20enables%20anatomically%20coherent%20reconstruction%20even%20under%20severe%20slice%20sparsity%20and%20segmentation%20noise%2C%20faithfully%20recovering%20complex%20structures%20such%20as%20the%20valve%20planes.%20Compared%20with%20traditional%20pipelines%2C%20inference%20time%20is%20reduced%20from%20over%2060%20s%20to%205-15%20s.%20These%20results%20demonstrate%20that%20NIHCs%20constitute%20a%20robust%20and%20efficient%20anatomical%20representation%20for%20patient-specific%203D%20cardiac%20reconstruction%20from%20minimal%20input%20data.&entry.1838667208=http%3A//arxiv.org/abs/2512.19316v1&entry.124074799=Read"},
{"title": "Personalized and Resilient Distributed Learning Through Opinion Dynamics", "author": "Luca Ballotta and Nicola Bastianello and Riccardo M. G. Ferrari and Karl H. Johansson", "abstract": "In this paper, we address two practical challenges of distributed learning in multi-agent network systems, namely personalization and resilience. Personalization is the need of heterogeneous agents to learn local models tailored to their own data and tasks, while still generalizing well; on the other hand, the learning process must be resilient to cyberattacks or anomalous training data to avoid disruption. Motivated by a conceptual affinity between these two requirements, we devise a distributed learning algorithm that combines distributed gradient descent and the Friedkin-Johnsen model of opinion dynamics to fulfill both of them. We quantify its convergence speed and the neighborhood that contains the final learned models, which can be easily controlled by tuning the algorithm parameters to enforce a more personalized/resilient behavior. We numerically showcase the effectiveness of our algorithm on synthetic and real-world distributed learning tasks, where it achieves high global accuracy both for personalized models and with malicious agents compared to standard strategies.", "link": "http://arxiv.org/abs/2505.14081v2", "date": "2025-12-22", "relevancy": 2.5328, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.525}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5145}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Personalized%20and%20Resilient%20Distributed%20Learning%20Through%20Opinion%20Dynamics&body=Title%3A%20Personalized%20and%20Resilient%20Distributed%20Learning%20Through%20Opinion%20Dynamics%0AAuthor%3A%20Luca%20Ballotta%20and%20Nicola%20Bastianello%20and%20Riccardo%20M.%20G.%20Ferrari%20and%20Karl%20H.%20Johansson%0AAbstract%3A%20In%20this%20paper%2C%20we%20address%20two%20practical%20challenges%20of%20distributed%20learning%20in%20multi-agent%20network%20systems%2C%20namely%20personalization%20and%20resilience.%20Personalization%20is%20the%20need%20of%20heterogeneous%20agents%20to%20learn%20local%20models%20tailored%20to%20their%20own%20data%20and%20tasks%2C%20while%20still%20generalizing%20well%3B%20on%20the%20other%20hand%2C%20the%20learning%20process%20must%20be%20resilient%20to%20cyberattacks%20or%20anomalous%20training%20data%20to%20avoid%20disruption.%20Motivated%20by%20a%20conceptual%20affinity%20between%20these%20two%20requirements%2C%20we%20devise%20a%20distributed%20learning%20algorithm%20that%20combines%20distributed%20gradient%20descent%20and%20the%20Friedkin-Johnsen%20model%20of%20opinion%20dynamics%20to%20fulfill%20both%20of%20them.%20We%20quantify%20its%20convergence%20speed%20and%20the%20neighborhood%20that%20contains%20the%20final%20learned%20models%2C%20which%20can%20be%20easily%20controlled%20by%20tuning%20the%20algorithm%20parameters%20to%20enforce%20a%20more%20personalized/resilient%20behavior.%20We%20numerically%20showcase%20the%20effectiveness%20of%20our%20algorithm%20on%20synthetic%20and%20real-world%20distributed%20learning%20tasks%2C%20where%20it%20achieves%20high%20global%20accuracy%20both%20for%20personalized%20models%20and%20with%20malicious%20agents%20compared%20to%20standard%20strategies.%0ALink%3A%20http%3A//arxiv.org/abs/2505.14081v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPersonalized%2520and%2520Resilient%2520Distributed%2520Learning%2520Through%2520Opinion%2520Dynamics%26entry.906535625%3DLuca%2520Ballotta%2520and%2520Nicola%2520Bastianello%2520and%2520Riccardo%2520M.%2520G.%2520Ferrari%2520and%2520Karl%2520H.%2520Johansson%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520address%2520two%2520practical%2520challenges%2520of%2520distributed%2520learning%2520in%2520multi-agent%2520network%2520systems%252C%2520namely%2520personalization%2520and%2520resilience.%2520Personalization%2520is%2520the%2520need%2520of%2520heterogeneous%2520agents%2520to%2520learn%2520local%2520models%2520tailored%2520to%2520their%2520own%2520data%2520and%2520tasks%252C%2520while%2520still%2520generalizing%2520well%253B%2520on%2520the%2520other%2520hand%252C%2520the%2520learning%2520process%2520must%2520be%2520resilient%2520to%2520cyberattacks%2520or%2520anomalous%2520training%2520data%2520to%2520avoid%2520disruption.%2520Motivated%2520by%2520a%2520conceptual%2520affinity%2520between%2520these%2520two%2520requirements%252C%2520we%2520devise%2520a%2520distributed%2520learning%2520algorithm%2520that%2520combines%2520distributed%2520gradient%2520descent%2520and%2520the%2520Friedkin-Johnsen%2520model%2520of%2520opinion%2520dynamics%2520to%2520fulfill%2520both%2520of%2520them.%2520We%2520quantify%2520its%2520convergence%2520speed%2520and%2520the%2520neighborhood%2520that%2520contains%2520the%2520final%2520learned%2520models%252C%2520which%2520can%2520be%2520easily%2520controlled%2520by%2520tuning%2520the%2520algorithm%2520parameters%2520to%2520enforce%2520a%2520more%2520personalized/resilient%2520behavior.%2520We%2520numerically%2520showcase%2520the%2520effectiveness%2520of%2520our%2520algorithm%2520on%2520synthetic%2520and%2520real-world%2520distributed%2520learning%2520tasks%252C%2520where%2520it%2520achieves%2520high%2520global%2520accuracy%2520both%2520for%2520personalized%2520models%2520and%2520with%2520malicious%2520agents%2520compared%2520to%2520standard%2520strategies.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.14081v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%20and%20Resilient%20Distributed%20Learning%20Through%20Opinion%20Dynamics&entry.906535625=Luca%20Ballotta%20and%20Nicola%20Bastianello%20and%20Riccardo%20M.%20G.%20Ferrari%20and%20Karl%20H.%20Johansson&entry.1292438233=In%20this%20paper%2C%20we%20address%20two%20practical%20challenges%20of%20distributed%20learning%20in%20multi-agent%20network%20systems%2C%20namely%20personalization%20and%20resilience.%20Personalization%20is%20the%20need%20of%20heterogeneous%20agents%20to%20learn%20local%20models%20tailored%20to%20their%20own%20data%20and%20tasks%2C%20while%20still%20generalizing%20well%3B%20on%20the%20other%20hand%2C%20the%20learning%20process%20must%20be%20resilient%20to%20cyberattacks%20or%20anomalous%20training%20data%20to%20avoid%20disruption.%20Motivated%20by%20a%20conceptual%20affinity%20between%20these%20two%20requirements%2C%20we%20devise%20a%20distributed%20learning%20algorithm%20that%20combines%20distributed%20gradient%20descent%20and%20the%20Friedkin-Johnsen%20model%20of%20opinion%20dynamics%20to%20fulfill%20both%20of%20them.%20We%20quantify%20its%20convergence%20speed%20and%20the%20neighborhood%20that%20contains%20the%20final%20learned%20models%2C%20which%20can%20be%20easily%20controlled%20by%20tuning%20the%20algorithm%20parameters%20to%20enforce%20a%20more%20personalized/resilient%20behavior.%20We%20numerically%20showcase%20the%20effectiveness%20of%20our%20algorithm%20on%20synthetic%20and%20real-world%20distributed%20learning%20tasks%2C%20where%20it%20achieves%20high%20global%20accuracy%20both%20for%20personalized%20models%20and%20with%20malicious%20agents%20compared%20to%20standard%20strategies.&entry.1838667208=http%3A//arxiv.org/abs/2505.14081v2&entry.124074799=Read"},
{"title": "Real2Edit2Real: Generating Robotic Demonstrations via a 3D Control Interface", "author": "Yujie Zhao and Hongwei Fan and Di Chen and Shengcong Chen and Liliang Chen and Xiaoqi Li and Guanghui Ren and Hao Dong", "abstract": "Recent progress in robot learning has been driven by large-scale datasets and powerful visuomotor policy architectures, yet policy robustness remains limited by the substantial cost of collecting diverse demonstrations, particularly for spatial generalization in manipulation tasks. To reduce repetitive data collection, we present Real2Edit2Real, a framework that generates new demonstrations by bridging 3D editability with 2D visual data through a 3D control interface. Our approach first reconstructs scene geometry from multi-view RGB observations with a metric-scale 3D reconstruction model. Based on the reconstructed geometry, we perform depth-reliable 3D editing on point clouds to generate new manipulation trajectories while geometrically correcting the robot poses to recover physically consistent depth, which serves as a reliable condition for synthesizing new demonstrations. Finally, we propose a multi-conditional video generation model guided by depth as the primary control signal, together with action, edge, and ray maps, to synthesize spatially augmented multi-view manipulation videos. Experiments on four real-world manipulation tasks demonstrate that policies trained on data generated from only 1-5 source demonstrations can match or outperform those trained on 50 real-world demonstrations, improving data efficiency by up to 10-50x. Moreover, experimental results on height and texture editing demonstrate the framework's flexibility and extensibility, indicating its potential to serve as a unified data generation framework.", "link": "http://arxiv.org/abs/2512.19402v1", "date": "2025-12-22", "relevancy": 2.5318, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6598}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6332}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Real2Edit2Real%3A%20Generating%20Robotic%20Demonstrations%20via%20a%203D%20Control%20Interface&body=Title%3A%20Real2Edit2Real%3A%20Generating%20Robotic%20Demonstrations%20via%20a%203D%20Control%20Interface%0AAuthor%3A%20Yujie%20Zhao%20and%20Hongwei%20Fan%20and%20Di%20Chen%20and%20Shengcong%20Chen%20and%20Liliang%20Chen%20and%20Xiaoqi%20Li%20and%20Guanghui%20Ren%20and%20Hao%20Dong%0AAbstract%3A%20Recent%20progress%20in%20robot%20learning%20has%20been%20driven%20by%20large-scale%20datasets%20and%20powerful%20visuomotor%20policy%20architectures%2C%20yet%20policy%20robustness%20remains%20limited%20by%20the%20substantial%20cost%20of%20collecting%20diverse%20demonstrations%2C%20particularly%20for%20spatial%20generalization%20in%20manipulation%20tasks.%20To%20reduce%20repetitive%20data%20collection%2C%20we%20present%20Real2Edit2Real%2C%20a%20framework%20that%20generates%20new%20demonstrations%20by%20bridging%203D%20editability%20with%202D%20visual%20data%20through%20a%203D%20control%20interface.%20Our%20approach%20first%20reconstructs%20scene%20geometry%20from%20multi-view%20RGB%20observations%20with%20a%20metric-scale%203D%20reconstruction%20model.%20Based%20on%20the%20reconstructed%20geometry%2C%20we%20perform%20depth-reliable%203D%20editing%20on%20point%20clouds%20to%20generate%20new%20manipulation%20trajectories%20while%20geometrically%20correcting%20the%20robot%20poses%20to%20recover%20physically%20consistent%20depth%2C%20which%20serves%20as%20a%20reliable%20condition%20for%20synthesizing%20new%20demonstrations.%20Finally%2C%20we%20propose%20a%20multi-conditional%20video%20generation%20model%20guided%20by%20depth%20as%20the%20primary%20control%20signal%2C%20together%20with%20action%2C%20edge%2C%20and%20ray%20maps%2C%20to%20synthesize%20spatially%20augmented%20multi-view%20manipulation%20videos.%20Experiments%20on%20four%20real-world%20manipulation%20tasks%20demonstrate%20that%20policies%20trained%20on%20data%20generated%20from%20only%201-5%20source%20demonstrations%20can%20match%20or%20outperform%20those%20trained%20on%2050%20real-world%20demonstrations%2C%20improving%20data%20efficiency%20by%20up%20to%2010-50x.%20Moreover%2C%20experimental%20results%20on%20height%20and%20texture%20editing%20demonstrate%20the%20framework%27s%20flexibility%20and%20extensibility%2C%20indicating%20its%20potential%20to%20serve%20as%20a%20unified%20data%20generation%20framework.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19402v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReal2Edit2Real%253A%2520Generating%2520Robotic%2520Demonstrations%2520via%2520a%25203D%2520Control%2520Interface%26entry.906535625%3DYujie%2520Zhao%2520and%2520Hongwei%2520Fan%2520and%2520Di%2520Chen%2520and%2520Shengcong%2520Chen%2520and%2520Liliang%2520Chen%2520and%2520Xiaoqi%2520Li%2520and%2520Guanghui%2520Ren%2520and%2520Hao%2520Dong%26entry.1292438233%3DRecent%2520progress%2520in%2520robot%2520learning%2520has%2520been%2520driven%2520by%2520large-scale%2520datasets%2520and%2520powerful%2520visuomotor%2520policy%2520architectures%252C%2520yet%2520policy%2520robustness%2520remains%2520limited%2520by%2520the%2520substantial%2520cost%2520of%2520collecting%2520diverse%2520demonstrations%252C%2520particularly%2520for%2520spatial%2520generalization%2520in%2520manipulation%2520tasks.%2520To%2520reduce%2520repetitive%2520data%2520collection%252C%2520we%2520present%2520Real2Edit2Real%252C%2520a%2520framework%2520that%2520generates%2520new%2520demonstrations%2520by%2520bridging%25203D%2520editability%2520with%25202D%2520visual%2520data%2520through%2520a%25203D%2520control%2520interface.%2520Our%2520approach%2520first%2520reconstructs%2520scene%2520geometry%2520from%2520multi-view%2520RGB%2520observations%2520with%2520a%2520metric-scale%25203D%2520reconstruction%2520model.%2520Based%2520on%2520the%2520reconstructed%2520geometry%252C%2520we%2520perform%2520depth-reliable%25203D%2520editing%2520on%2520point%2520clouds%2520to%2520generate%2520new%2520manipulation%2520trajectories%2520while%2520geometrically%2520correcting%2520the%2520robot%2520poses%2520to%2520recover%2520physically%2520consistent%2520depth%252C%2520which%2520serves%2520as%2520a%2520reliable%2520condition%2520for%2520synthesizing%2520new%2520demonstrations.%2520Finally%252C%2520we%2520propose%2520a%2520multi-conditional%2520video%2520generation%2520model%2520guided%2520by%2520depth%2520as%2520the%2520primary%2520control%2520signal%252C%2520together%2520with%2520action%252C%2520edge%252C%2520and%2520ray%2520maps%252C%2520to%2520synthesize%2520spatially%2520augmented%2520multi-view%2520manipulation%2520videos.%2520Experiments%2520on%2520four%2520real-world%2520manipulation%2520tasks%2520demonstrate%2520that%2520policies%2520trained%2520on%2520data%2520generated%2520from%2520only%25201-5%2520source%2520demonstrations%2520can%2520match%2520or%2520outperform%2520those%2520trained%2520on%252050%2520real-world%2520demonstrations%252C%2520improving%2520data%2520efficiency%2520by%2520up%2520to%252010-50x.%2520Moreover%252C%2520experimental%2520results%2520on%2520height%2520and%2520texture%2520editing%2520demonstrate%2520the%2520framework%2527s%2520flexibility%2520and%2520extensibility%252C%2520indicating%2520its%2520potential%2520to%2520serve%2520as%2520a%2520unified%2520data%2520generation%2520framework.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19402v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Real2Edit2Real%3A%20Generating%20Robotic%20Demonstrations%20via%20a%203D%20Control%20Interface&entry.906535625=Yujie%20Zhao%20and%20Hongwei%20Fan%20and%20Di%20Chen%20and%20Shengcong%20Chen%20and%20Liliang%20Chen%20and%20Xiaoqi%20Li%20and%20Guanghui%20Ren%20and%20Hao%20Dong&entry.1292438233=Recent%20progress%20in%20robot%20learning%20has%20been%20driven%20by%20large-scale%20datasets%20and%20powerful%20visuomotor%20policy%20architectures%2C%20yet%20policy%20robustness%20remains%20limited%20by%20the%20substantial%20cost%20of%20collecting%20diverse%20demonstrations%2C%20particularly%20for%20spatial%20generalization%20in%20manipulation%20tasks.%20To%20reduce%20repetitive%20data%20collection%2C%20we%20present%20Real2Edit2Real%2C%20a%20framework%20that%20generates%20new%20demonstrations%20by%20bridging%203D%20editability%20with%202D%20visual%20data%20through%20a%203D%20control%20interface.%20Our%20approach%20first%20reconstructs%20scene%20geometry%20from%20multi-view%20RGB%20observations%20with%20a%20metric-scale%203D%20reconstruction%20model.%20Based%20on%20the%20reconstructed%20geometry%2C%20we%20perform%20depth-reliable%203D%20editing%20on%20point%20clouds%20to%20generate%20new%20manipulation%20trajectories%20while%20geometrically%20correcting%20the%20robot%20poses%20to%20recover%20physically%20consistent%20depth%2C%20which%20serves%20as%20a%20reliable%20condition%20for%20synthesizing%20new%20demonstrations.%20Finally%2C%20we%20propose%20a%20multi-conditional%20video%20generation%20model%20guided%20by%20depth%20as%20the%20primary%20control%20signal%2C%20together%20with%20action%2C%20edge%2C%20and%20ray%20maps%2C%20to%20synthesize%20spatially%20augmented%20multi-view%20manipulation%20videos.%20Experiments%20on%20four%20real-world%20manipulation%20tasks%20demonstrate%20that%20policies%20trained%20on%20data%20generated%20from%20only%201-5%20source%20demonstrations%20can%20match%20or%20outperform%20those%20trained%20on%2050%20real-world%20demonstrations%2C%20improving%20data%20efficiency%20by%20up%20to%2010-50x.%20Moreover%2C%20experimental%20results%20on%20height%20and%20texture%20editing%20demonstrate%20the%20framework%27s%20flexibility%20and%20extensibility%2C%20indicating%20its%20potential%20to%20serve%20as%20a%20unified%20data%20generation%20framework.&entry.1838667208=http%3A//arxiv.org/abs/2512.19402v1&entry.124074799=Read"},
{"title": "One Perturbation is Enough: On Generating Universal Adversarial Perturbations against Vision-Language Pre-training Models", "author": "Hao Fang and Jiawei Kong and Wenbo Yu and Bin Chen and Jiawei Li and Hao Wu and Shutao Xia and Ke Xu", "abstract": "Vision-Language Pre-training (VLP) models have exhibited unprecedented capability in many applications by taking full advantage of the multimodal alignment. However, previous studies have shown they are vulnerable to maliciously crafted adversarial samples. Despite recent success, these methods are generally instance-specific and require generating perturbations for each input sample. In this paper, we reveal that VLP models are also vulnerable to the instance-agnostic universal adversarial perturbation (UAP). Specifically, we design a novel Contrastive-training Perturbation Generator with Cross-modal conditions (C-PGC) to achieve the attack. In light that the pivotal multimodal alignment is achieved through the advanced contrastive learning technique, we devise to turn this powerful weapon against themselves, i.e., employ a malicious version of contrastive learning to train the C-PGC based on our carefully crafted positive and negative image-text pairs for essentially destroying the alignment relationship learned by VLP models. Besides, C-PGC fully utilizes the characteristics of Vision-and-Language (V+L) scenarios by incorporating both unimodal and cross-modal information as effective guidance. Extensive experiments show that C-PGC successfully forces adversarial samples to move away from their original area in the VLP model's feature space, thus essentially enhancing attacks across various victim models and V+L tasks. The GitHub repository is available at https://github.com/ffhibnese/CPGC_VLP_Universal_Attacks.", "link": "http://arxiv.org/abs/2406.05491v4", "date": "2025-12-22", "relevancy": 2.5297, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5167}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5095}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4916}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20Perturbation%20is%20Enough%3A%20On%20Generating%20Universal%20Adversarial%20Perturbations%20against%20Vision-Language%20Pre-training%20Models&body=Title%3A%20One%20Perturbation%20is%20Enough%3A%20On%20Generating%20Universal%20Adversarial%20Perturbations%20against%20Vision-Language%20Pre-training%20Models%0AAuthor%3A%20Hao%20Fang%20and%20Jiawei%20Kong%20and%20Wenbo%20Yu%20and%20Bin%20Chen%20and%20Jiawei%20Li%20and%20Hao%20Wu%20and%20Shutao%20Xia%20and%20Ke%20Xu%0AAbstract%3A%20Vision-Language%20Pre-training%20%28VLP%29%20models%20have%20exhibited%20unprecedented%20capability%20in%20many%20applications%20by%20taking%20full%20advantage%20of%20the%20multimodal%20alignment.%20However%2C%20previous%20studies%20have%20shown%20they%20are%20vulnerable%20to%20maliciously%20crafted%20adversarial%20samples.%20Despite%20recent%20success%2C%20these%20methods%20are%20generally%20instance-specific%20and%20require%20generating%20perturbations%20for%20each%20input%20sample.%20In%20this%20paper%2C%20we%20reveal%20that%20VLP%20models%20are%20also%20vulnerable%20to%20the%20instance-agnostic%20universal%20adversarial%20perturbation%20%28UAP%29.%20Specifically%2C%20we%20design%20a%20novel%20Contrastive-training%20Perturbation%20Generator%20with%20Cross-modal%20conditions%20%28C-PGC%29%20to%20achieve%20the%20attack.%20In%20light%20that%20the%20pivotal%20multimodal%20alignment%20is%20achieved%20through%20the%20advanced%20contrastive%20learning%20technique%2C%20we%20devise%20to%20turn%20this%20powerful%20weapon%20against%20themselves%2C%20i.e.%2C%20employ%20a%20malicious%20version%20of%20contrastive%20learning%20to%20train%20the%20C-PGC%20based%20on%20our%20carefully%20crafted%20positive%20and%20negative%20image-text%20pairs%20for%20essentially%20destroying%20the%20alignment%20relationship%20learned%20by%20VLP%20models.%20Besides%2C%20C-PGC%20fully%20utilizes%20the%20characteristics%20of%20Vision-and-Language%20%28V%2BL%29%20scenarios%20by%20incorporating%20both%20unimodal%20and%20cross-modal%20information%20as%20effective%20guidance.%20Extensive%20experiments%20show%20that%20C-PGC%20successfully%20forces%20adversarial%20samples%20to%20move%20away%20from%20their%20original%20area%20in%20the%20VLP%20model%27s%20feature%20space%2C%20thus%20essentially%20enhancing%20attacks%20across%20various%20victim%20models%20and%20V%2BL%20tasks.%20The%20GitHub%20repository%20is%20available%20at%20https%3A//github.com/ffhibnese/CPGC_VLP_Universal_Attacks.%0ALink%3A%20http%3A//arxiv.org/abs/2406.05491v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520Perturbation%2520is%2520Enough%253A%2520On%2520Generating%2520Universal%2520Adversarial%2520Perturbations%2520against%2520Vision-Language%2520Pre-training%2520Models%26entry.906535625%3DHao%2520Fang%2520and%2520Jiawei%2520Kong%2520and%2520Wenbo%2520Yu%2520and%2520Bin%2520Chen%2520and%2520Jiawei%2520Li%2520and%2520Hao%2520Wu%2520and%2520Shutao%2520Xia%2520and%2520Ke%2520Xu%26entry.1292438233%3DVision-Language%2520Pre-training%2520%2528VLP%2529%2520models%2520have%2520exhibited%2520unprecedented%2520capability%2520in%2520many%2520applications%2520by%2520taking%2520full%2520advantage%2520of%2520the%2520multimodal%2520alignment.%2520However%252C%2520previous%2520studies%2520have%2520shown%2520they%2520are%2520vulnerable%2520to%2520maliciously%2520crafted%2520adversarial%2520samples.%2520Despite%2520recent%2520success%252C%2520these%2520methods%2520are%2520generally%2520instance-specific%2520and%2520require%2520generating%2520perturbations%2520for%2520each%2520input%2520sample.%2520In%2520this%2520paper%252C%2520we%2520reveal%2520that%2520VLP%2520models%2520are%2520also%2520vulnerable%2520to%2520the%2520instance-agnostic%2520universal%2520adversarial%2520perturbation%2520%2528UAP%2529.%2520Specifically%252C%2520we%2520design%2520a%2520novel%2520Contrastive-training%2520Perturbation%2520Generator%2520with%2520Cross-modal%2520conditions%2520%2528C-PGC%2529%2520to%2520achieve%2520the%2520attack.%2520In%2520light%2520that%2520the%2520pivotal%2520multimodal%2520alignment%2520is%2520achieved%2520through%2520the%2520advanced%2520contrastive%2520learning%2520technique%252C%2520we%2520devise%2520to%2520turn%2520this%2520powerful%2520weapon%2520against%2520themselves%252C%2520i.e.%252C%2520employ%2520a%2520malicious%2520version%2520of%2520contrastive%2520learning%2520to%2520train%2520the%2520C-PGC%2520based%2520on%2520our%2520carefully%2520crafted%2520positive%2520and%2520negative%2520image-text%2520pairs%2520for%2520essentially%2520destroying%2520the%2520alignment%2520relationship%2520learned%2520by%2520VLP%2520models.%2520Besides%252C%2520C-PGC%2520fully%2520utilizes%2520the%2520characteristics%2520of%2520Vision-and-Language%2520%2528V%252BL%2529%2520scenarios%2520by%2520incorporating%2520both%2520unimodal%2520and%2520cross-modal%2520information%2520as%2520effective%2520guidance.%2520Extensive%2520experiments%2520show%2520that%2520C-PGC%2520successfully%2520forces%2520adversarial%2520samples%2520to%2520move%2520away%2520from%2520their%2520original%2520area%2520in%2520the%2520VLP%2520model%2527s%2520feature%2520space%252C%2520thus%2520essentially%2520enhancing%2520attacks%2520across%2520various%2520victim%2520models%2520and%2520V%252BL%2520tasks.%2520The%2520GitHub%2520repository%2520is%2520available%2520at%2520https%253A//github.com/ffhibnese/CPGC_VLP_Universal_Attacks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05491v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20Perturbation%20is%20Enough%3A%20On%20Generating%20Universal%20Adversarial%20Perturbations%20against%20Vision-Language%20Pre-training%20Models&entry.906535625=Hao%20Fang%20and%20Jiawei%20Kong%20and%20Wenbo%20Yu%20and%20Bin%20Chen%20and%20Jiawei%20Li%20and%20Hao%20Wu%20and%20Shutao%20Xia%20and%20Ke%20Xu&entry.1292438233=Vision-Language%20Pre-training%20%28VLP%29%20models%20have%20exhibited%20unprecedented%20capability%20in%20many%20applications%20by%20taking%20full%20advantage%20of%20the%20multimodal%20alignment.%20However%2C%20previous%20studies%20have%20shown%20they%20are%20vulnerable%20to%20maliciously%20crafted%20adversarial%20samples.%20Despite%20recent%20success%2C%20these%20methods%20are%20generally%20instance-specific%20and%20require%20generating%20perturbations%20for%20each%20input%20sample.%20In%20this%20paper%2C%20we%20reveal%20that%20VLP%20models%20are%20also%20vulnerable%20to%20the%20instance-agnostic%20universal%20adversarial%20perturbation%20%28UAP%29.%20Specifically%2C%20we%20design%20a%20novel%20Contrastive-training%20Perturbation%20Generator%20with%20Cross-modal%20conditions%20%28C-PGC%29%20to%20achieve%20the%20attack.%20In%20light%20that%20the%20pivotal%20multimodal%20alignment%20is%20achieved%20through%20the%20advanced%20contrastive%20learning%20technique%2C%20we%20devise%20to%20turn%20this%20powerful%20weapon%20against%20themselves%2C%20i.e.%2C%20employ%20a%20malicious%20version%20of%20contrastive%20learning%20to%20train%20the%20C-PGC%20based%20on%20our%20carefully%20crafted%20positive%20and%20negative%20image-text%20pairs%20for%20essentially%20destroying%20the%20alignment%20relationship%20learned%20by%20VLP%20models.%20Besides%2C%20C-PGC%20fully%20utilizes%20the%20characteristics%20of%20Vision-and-Language%20%28V%2BL%29%20scenarios%20by%20incorporating%20both%20unimodal%20and%20cross-modal%20information%20as%20effective%20guidance.%20Extensive%20experiments%20show%20that%20C-PGC%20successfully%20forces%20adversarial%20samples%20to%20move%20away%20from%20their%20original%20area%20in%20the%20VLP%20model%27s%20feature%20space%2C%20thus%20essentially%20enhancing%20attacks%20across%20various%20victim%20models%20and%20V%2BL%20tasks.%20The%20GitHub%20repository%20is%20available%20at%20https%3A//github.com/ffhibnese/CPGC_VLP_Universal_Attacks.&entry.1838667208=http%3A//arxiv.org/abs/2406.05491v4&entry.124074799=Read"},
{"title": "Interact2Ar: Full-Body Human-Human Interaction Generation via Autoregressive Diffusion Models", "author": "Pablo Ruiz-Ponce and Sergio Escalera and Jos\u00e9 Garc\u00eda-Rodr\u00edguez and Jiankang Deng and Rolandos Alexandros Potamias", "abstract": "Generating realistic human-human interactions is a challenging task that requires not only high-quality individual body and hand motions, but also coherent coordination among all interactants. Due to limitations in available data and increased learning complexity, previous methods tend to ignore hand motions, limiting the realism and expressivity of the interactions. Additionally, current diffusion-based approaches generate entire motion sequences simultaneously, limiting their ability to capture the reactive and adaptive nature of human interactions. To address these limitations, we introduce Interact2Ar, the first end-to-end text-conditioned autoregressive diffusion model for generating full-body, human-human interactions. Interact2Ar incorporates detailed hand kinematics through dedicated parallel branches, enabling high-fidelity full-body generation. Furthermore, we introduce an autoregressive pipeline coupled with a novel memory technique that facilitates adaptation to the inherent variability of human interactions using efficient large context windows. The adaptability of our model enables a series of downstream applications, including temporal motion composition, real-time adaptation to disturbances, and extension beyond dyadic to multi-person scenarios. To validate the generated motions, we introduce a set of robust evaluators and extended metrics designed specifically for assessing full-body interactions. Through quantitative and qualitative experiments, we demonstrate the state-of-the-art performance of Interact2Ar.", "link": "http://arxiv.org/abs/2512.19692v1", "date": "2025-12-22", "relevancy": 2.5273, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6947}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.591}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interact2Ar%3A%20Full-Body%20Human-Human%20Interaction%20Generation%20via%20Autoregressive%20Diffusion%20Models&body=Title%3A%20Interact2Ar%3A%20Full-Body%20Human-Human%20Interaction%20Generation%20via%20Autoregressive%20Diffusion%20Models%0AAuthor%3A%20Pablo%20Ruiz-Ponce%20and%20Sergio%20Escalera%20and%20Jos%C3%A9%20Garc%C3%ADa-Rodr%C3%ADguez%20and%20Jiankang%20Deng%20and%20Rolandos%20Alexandros%20Potamias%0AAbstract%3A%20Generating%20realistic%20human-human%20interactions%20is%20a%20challenging%20task%20that%20requires%20not%20only%20high-quality%20individual%20body%20and%20hand%20motions%2C%20but%20also%20coherent%20coordination%20among%20all%20interactants.%20Due%20to%20limitations%20in%20available%20data%20and%20increased%20learning%20complexity%2C%20previous%20methods%20tend%20to%20ignore%20hand%20motions%2C%20limiting%20the%20realism%20and%20expressivity%20of%20the%20interactions.%20Additionally%2C%20current%20diffusion-based%20approaches%20generate%20entire%20motion%20sequences%20simultaneously%2C%20limiting%20their%20ability%20to%20capture%20the%20reactive%20and%20adaptive%20nature%20of%20human%20interactions.%20To%20address%20these%20limitations%2C%20we%20introduce%20Interact2Ar%2C%20the%20first%20end-to-end%20text-conditioned%20autoregressive%20diffusion%20model%20for%20generating%20full-body%2C%20human-human%20interactions.%20Interact2Ar%20incorporates%20detailed%20hand%20kinematics%20through%20dedicated%20parallel%20branches%2C%20enabling%20high-fidelity%20full-body%20generation.%20Furthermore%2C%20we%20introduce%20an%20autoregressive%20pipeline%20coupled%20with%20a%20novel%20memory%20technique%20that%20facilitates%20adaptation%20to%20the%20inherent%20variability%20of%20human%20interactions%20using%20efficient%20large%20context%20windows.%20The%20adaptability%20of%20our%20model%20enables%20a%20series%20of%20downstream%20applications%2C%20including%20temporal%20motion%20composition%2C%20real-time%20adaptation%20to%20disturbances%2C%20and%20extension%20beyond%20dyadic%20to%20multi-person%20scenarios.%20To%20validate%20the%20generated%20motions%2C%20we%20introduce%20a%20set%20of%20robust%20evaluators%20and%20extended%20metrics%20designed%20specifically%20for%20assessing%20full-body%20interactions.%20Through%20quantitative%20and%20qualitative%20experiments%2C%20we%20demonstrate%20the%20state-of-the-art%20performance%20of%20Interact2Ar.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19692v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInteract2Ar%253A%2520Full-Body%2520Human-Human%2520Interaction%2520Generation%2520via%2520Autoregressive%2520Diffusion%2520Models%26entry.906535625%3DPablo%2520Ruiz-Ponce%2520and%2520Sergio%2520Escalera%2520and%2520Jos%25C3%25A9%2520Garc%25C3%25ADa-Rodr%25C3%25ADguez%2520and%2520Jiankang%2520Deng%2520and%2520Rolandos%2520Alexandros%2520Potamias%26entry.1292438233%3DGenerating%2520realistic%2520human-human%2520interactions%2520is%2520a%2520challenging%2520task%2520that%2520requires%2520not%2520only%2520high-quality%2520individual%2520body%2520and%2520hand%2520motions%252C%2520but%2520also%2520coherent%2520coordination%2520among%2520all%2520interactants.%2520Due%2520to%2520limitations%2520in%2520available%2520data%2520and%2520increased%2520learning%2520complexity%252C%2520previous%2520methods%2520tend%2520to%2520ignore%2520hand%2520motions%252C%2520limiting%2520the%2520realism%2520and%2520expressivity%2520of%2520the%2520interactions.%2520Additionally%252C%2520current%2520diffusion-based%2520approaches%2520generate%2520entire%2520motion%2520sequences%2520simultaneously%252C%2520limiting%2520their%2520ability%2520to%2520capture%2520the%2520reactive%2520and%2520adaptive%2520nature%2520of%2520human%2520interactions.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520Interact2Ar%252C%2520the%2520first%2520end-to-end%2520text-conditioned%2520autoregressive%2520diffusion%2520model%2520for%2520generating%2520full-body%252C%2520human-human%2520interactions.%2520Interact2Ar%2520incorporates%2520detailed%2520hand%2520kinematics%2520through%2520dedicated%2520parallel%2520branches%252C%2520enabling%2520high-fidelity%2520full-body%2520generation.%2520Furthermore%252C%2520we%2520introduce%2520an%2520autoregressive%2520pipeline%2520coupled%2520with%2520a%2520novel%2520memory%2520technique%2520that%2520facilitates%2520adaptation%2520to%2520the%2520inherent%2520variability%2520of%2520human%2520interactions%2520using%2520efficient%2520large%2520context%2520windows.%2520The%2520adaptability%2520of%2520our%2520model%2520enables%2520a%2520series%2520of%2520downstream%2520applications%252C%2520including%2520temporal%2520motion%2520composition%252C%2520real-time%2520adaptation%2520to%2520disturbances%252C%2520and%2520extension%2520beyond%2520dyadic%2520to%2520multi-person%2520scenarios.%2520To%2520validate%2520the%2520generated%2520motions%252C%2520we%2520introduce%2520a%2520set%2520of%2520robust%2520evaluators%2520and%2520extended%2520metrics%2520designed%2520specifically%2520for%2520assessing%2520full-body%2520interactions.%2520Through%2520quantitative%2520and%2520qualitative%2520experiments%252C%2520we%2520demonstrate%2520the%2520state-of-the-art%2520performance%2520of%2520Interact2Ar.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19692v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interact2Ar%3A%20Full-Body%20Human-Human%20Interaction%20Generation%20via%20Autoregressive%20Diffusion%20Models&entry.906535625=Pablo%20Ruiz-Ponce%20and%20Sergio%20Escalera%20and%20Jos%C3%A9%20Garc%C3%ADa-Rodr%C3%ADguez%20and%20Jiankang%20Deng%20and%20Rolandos%20Alexandros%20Potamias&entry.1292438233=Generating%20realistic%20human-human%20interactions%20is%20a%20challenging%20task%20that%20requires%20not%20only%20high-quality%20individual%20body%20and%20hand%20motions%2C%20but%20also%20coherent%20coordination%20among%20all%20interactants.%20Due%20to%20limitations%20in%20available%20data%20and%20increased%20learning%20complexity%2C%20previous%20methods%20tend%20to%20ignore%20hand%20motions%2C%20limiting%20the%20realism%20and%20expressivity%20of%20the%20interactions.%20Additionally%2C%20current%20diffusion-based%20approaches%20generate%20entire%20motion%20sequences%20simultaneously%2C%20limiting%20their%20ability%20to%20capture%20the%20reactive%20and%20adaptive%20nature%20of%20human%20interactions.%20To%20address%20these%20limitations%2C%20we%20introduce%20Interact2Ar%2C%20the%20first%20end-to-end%20text-conditioned%20autoregressive%20diffusion%20model%20for%20generating%20full-body%2C%20human-human%20interactions.%20Interact2Ar%20incorporates%20detailed%20hand%20kinematics%20through%20dedicated%20parallel%20branches%2C%20enabling%20high-fidelity%20full-body%20generation.%20Furthermore%2C%20we%20introduce%20an%20autoregressive%20pipeline%20coupled%20with%20a%20novel%20memory%20technique%20that%20facilitates%20adaptation%20to%20the%20inherent%20variability%20of%20human%20interactions%20using%20efficient%20large%20context%20windows.%20The%20adaptability%20of%20our%20model%20enables%20a%20series%20of%20downstream%20applications%2C%20including%20temporal%20motion%20composition%2C%20real-time%20adaptation%20to%20disturbances%2C%20and%20extension%20beyond%20dyadic%20to%20multi-person%20scenarios.%20To%20validate%20the%20generated%20motions%2C%20we%20introduce%20a%20set%20of%20robust%20evaluators%20and%20extended%20metrics%20designed%20specifically%20for%20assessing%20full-body%20interactions.%20Through%20quantitative%20and%20qualitative%20experiments%2C%20we%20demonstrate%20the%20state-of-the-art%20performance%20of%20Interact2Ar.&entry.1838667208=http%3A//arxiv.org/abs/2512.19692v1&entry.124074799=Read"},
{"title": "Brain-Grounded Axes for Reading and Steering LLM States", "author": "Sandro Andric", "abstract": "Interpretability methods for large language models (LLMs) typically derive directions from textual supervision, which can lack external grounding. We propose using human brain activity not as a training signal but as a coordinate system for reading and steering LLM states. Using the SMN4Lang MEG dataset, we construct a word-level brain atlas of phase-locking value (PLV) patterns and extract latent axes via ICA. We validate axes with independent lexica and NER-based labels (POS/log-frequency used as sanity checks), then train lightweight adapters that map LLM hidden states to these brain axes without fine-tuning the LLM. Steering along the resulting brain-derived directions yields a robust lexical (frequency-linked) axis in a mid TinyLlama layer, surviving perplexity-matched controls, and a brain-vs-text probe comparison shows larger log-frequency shifts (relative to the text probe) with lower perplexity for the brain axis. A function/content axis (axis 13) shows consistent steering in TinyLlama, Qwen2-0.5B, and GPT-2, with PPL-matched text-level corroboration. Layer-4 effects in TinyLlama are large but inconsistent, so we treat them as secondary (Appendix). Axis structure is stable when the atlas is rebuilt without GPT embedding-change features or with word2vec embeddings (|r|=0.64-0.95 across matched axes), reducing circularity concerns. Exploratory fMRI anchoring suggests potential alignment for embedding change and log frequency, but effects are sensitive to hemodynamic modeling assumptions and are treated as population-level evidence only. These results support a new interface: neurophysiology-grounded axes provide interpretable and controllable handles for LLM behavior.", "link": "http://arxiv.org/abs/2512.19399v1", "date": "2025-12-22", "relevancy": 2.5251, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5097}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5097}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4957}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Brain-Grounded%20Axes%20for%20Reading%20and%20Steering%20LLM%20States&body=Title%3A%20Brain-Grounded%20Axes%20for%20Reading%20and%20Steering%20LLM%20States%0AAuthor%3A%20Sandro%20Andric%0AAbstract%3A%20Interpretability%20methods%20for%20large%20language%20models%20%28LLMs%29%20typically%20derive%20directions%20from%20textual%20supervision%2C%20which%20can%20lack%20external%20grounding.%20We%20propose%20using%20human%20brain%20activity%20not%20as%20a%20training%20signal%20but%20as%20a%20coordinate%20system%20for%20reading%20and%20steering%20LLM%20states.%20Using%20the%20SMN4Lang%20MEG%20dataset%2C%20we%20construct%20a%20word-level%20brain%20atlas%20of%20phase-locking%20value%20%28PLV%29%20patterns%20and%20extract%20latent%20axes%20via%20ICA.%20We%20validate%20axes%20with%20independent%20lexica%20and%20NER-based%20labels%20%28POS/log-frequency%20used%20as%20sanity%20checks%29%2C%20then%20train%20lightweight%20adapters%20that%20map%20LLM%20hidden%20states%20to%20these%20brain%20axes%20without%20fine-tuning%20the%20LLM.%20Steering%20along%20the%20resulting%20brain-derived%20directions%20yields%20a%20robust%20lexical%20%28frequency-linked%29%20axis%20in%20a%20mid%20TinyLlama%20layer%2C%20surviving%20perplexity-matched%20controls%2C%20and%20a%20brain-vs-text%20probe%20comparison%20shows%20larger%20log-frequency%20shifts%20%28relative%20to%20the%20text%20probe%29%20with%20lower%20perplexity%20for%20the%20brain%20axis.%20A%20function/content%20axis%20%28axis%2013%29%20shows%20consistent%20steering%20in%20TinyLlama%2C%20Qwen2-0.5B%2C%20and%20GPT-2%2C%20with%20PPL-matched%20text-level%20corroboration.%20Layer-4%20effects%20in%20TinyLlama%20are%20large%20but%20inconsistent%2C%20so%20we%20treat%20them%20as%20secondary%20%28Appendix%29.%20Axis%20structure%20is%20stable%20when%20the%20atlas%20is%20rebuilt%20without%20GPT%20embedding-change%20features%20or%20with%20word2vec%20embeddings%20%28%7Cr%7C%3D0.64-0.95%20across%20matched%20axes%29%2C%20reducing%20circularity%20concerns.%20Exploratory%20fMRI%20anchoring%20suggests%20potential%20alignment%20for%20embedding%20change%20and%20log%20frequency%2C%20but%20effects%20are%20sensitive%20to%20hemodynamic%20modeling%20assumptions%20and%20are%20treated%20as%20population-level%20evidence%20only.%20These%20results%20support%20a%20new%20interface%3A%20neurophysiology-grounded%20axes%20provide%20interpretable%20and%20controllable%20handles%20for%20LLM%20behavior.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19399v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrain-Grounded%2520Axes%2520for%2520Reading%2520and%2520Steering%2520LLM%2520States%26entry.906535625%3DSandro%2520Andric%26entry.1292438233%3DInterpretability%2520methods%2520for%2520large%2520language%2520models%2520%2528LLMs%2529%2520typically%2520derive%2520directions%2520from%2520textual%2520supervision%252C%2520which%2520can%2520lack%2520external%2520grounding.%2520We%2520propose%2520using%2520human%2520brain%2520activity%2520not%2520as%2520a%2520training%2520signal%2520but%2520as%2520a%2520coordinate%2520system%2520for%2520reading%2520and%2520steering%2520LLM%2520states.%2520Using%2520the%2520SMN4Lang%2520MEG%2520dataset%252C%2520we%2520construct%2520a%2520word-level%2520brain%2520atlas%2520of%2520phase-locking%2520value%2520%2528PLV%2529%2520patterns%2520and%2520extract%2520latent%2520axes%2520via%2520ICA.%2520We%2520validate%2520axes%2520with%2520independent%2520lexica%2520and%2520NER-based%2520labels%2520%2528POS/log-frequency%2520used%2520as%2520sanity%2520checks%2529%252C%2520then%2520train%2520lightweight%2520adapters%2520that%2520map%2520LLM%2520hidden%2520states%2520to%2520these%2520brain%2520axes%2520without%2520fine-tuning%2520the%2520LLM.%2520Steering%2520along%2520the%2520resulting%2520brain-derived%2520directions%2520yields%2520a%2520robust%2520lexical%2520%2528frequency-linked%2529%2520axis%2520in%2520a%2520mid%2520TinyLlama%2520layer%252C%2520surviving%2520perplexity-matched%2520controls%252C%2520and%2520a%2520brain-vs-text%2520probe%2520comparison%2520shows%2520larger%2520log-frequency%2520shifts%2520%2528relative%2520to%2520the%2520text%2520probe%2529%2520with%2520lower%2520perplexity%2520for%2520the%2520brain%2520axis.%2520A%2520function/content%2520axis%2520%2528axis%252013%2529%2520shows%2520consistent%2520steering%2520in%2520TinyLlama%252C%2520Qwen2-0.5B%252C%2520and%2520GPT-2%252C%2520with%2520PPL-matched%2520text-level%2520corroboration.%2520Layer-4%2520effects%2520in%2520TinyLlama%2520are%2520large%2520but%2520inconsistent%252C%2520so%2520we%2520treat%2520them%2520as%2520secondary%2520%2528Appendix%2529.%2520Axis%2520structure%2520is%2520stable%2520when%2520the%2520atlas%2520is%2520rebuilt%2520without%2520GPT%2520embedding-change%2520features%2520or%2520with%2520word2vec%2520embeddings%2520%2528%257Cr%257C%253D0.64-0.95%2520across%2520matched%2520axes%2529%252C%2520reducing%2520circularity%2520concerns.%2520Exploratory%2520fMRI%2520anchoring%2520suggests%2520potential%2520alignment%2520for%2520embedding%2520change%2520and%2520log%2520frequency%252C%2520but%2520effects%2520are%2520sensitive%2520to%2520hemodynamic%2520modeling%2520assumptions%2520and%2520are%2520treated%2520as%2520population-level%2520evidence%2520only.%2520These%2520results%2520support%2520a%2520new%2520interface%253A%2520neurophysiology-grounded%2520axes%2520provide%2520interpretable%2520and%2520controllable%2520handles%2520for%2520LLM%2520behavior.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19399v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brain-Grounded%20Axes%20for%20Reading%20and%20Steering%20LLM%20States&entry.906535625=Sandro%20Andric&entry.1292438233=Interpretability%20methods%20for%20large%20language%20models%20%28LLMs%29%20typically%20derive%20directions%20from%20textual%20supervision%2C%20which%20can%20lack%20external%20grounding.%20We%20propose%20using%20human%20brain%20activity%20not%20as%20a%20training%20signal%20but%20as%20a%20coordinate%20system%20for%20reading%20and%20steering%20LLM%20states.%20Using%20the%20SMN4Lang%20MEG%20dataset%2C%20we%20construct%20a%20word-level%20brain%20atlas%20of%20phase-locking%20value%20%28PLV%29%20patterns%20and%20extract%20latent%20axes%20via%20ICA.%20We%20validate%20axes%20with%20independent%20lexica%20and%20NER-based%20labels%20%28POS/log-frequency%20used%20as%20sanity%20checks%29%2C%20then%20train%20lightweight%20adapters%20that%20map%20LLM%20hidden%20states%20to%20these%20brain%20axes%20without%20fine-tuning%20the%20LLM.%20Steering%20along%20the%20resulting%20brain-derived%20directions%20yields%20a%20robust%20lexical%20%28frequency-linked%29%20axis%20in%20a%20mid%20TinyLlama%20layer%2C%20surviving%20perplexity-matched%20controls%2C%20and%20a%20brain-vs-text%20probe%20comparison%20shows%20larger%20log-frequency%20shifts%20%28relative%20to%20the%20text%20probe%29%20with%20lower%20perplexity%20for%20the%20brain%20axis.%20A%20function/content%20axis%20%28axis%2013%29%20shows%20consistent%20steering%20in%20TinyLlama%2C%20Qwen2-0.5B%2C%20and%20GPT-2%2C%20with%20PPL-matched%20text-level%20corroboration.%20Layer-4%20effects%20in%20TinyLlama%20are%20large%20but%20inconsistent%2C%20so%20we%20treat%20them%20as%20secondary%20%28Appendix%29.%20Axis%20structure%20is%20stable%20when%20the%20atlas%20is%20rebuilt%20without%20GPT%20embedding-change%20features%20or%20with%20word2vec%20embeddings%20%28%7Cr%7C%3D0.64-0.95%20across%20matched%20axes%29%2C%20reducing%20circularity%20concerns.%20Exploratory%20fMRI%20anchoring%20suggests%20potential%20alignment%20for%20embedding%20change%20and%20log%20frequency%2C%20but%20effects%20are%20sensitive%20to%20hemodynamic%20modeling%20assumptions%20and%20are%20treated%20as%20population-level%20evidence%20only.%20These%20results%20support%20a%20new%20interface%3A%20neurophysiology-grounded%20axes%20provide%20interpretable%20and%20controllable%20handles%20for%20LLM%20behavior.&entry.1838667208=http%3A//arxiv.org/abs/2512.19399v1&entry.124074799=Read"},
{"title": "Restrictive Hierarchical Semantic Segmentation for Stratified Tooth Layer Detection", "author": "Ryan Banks and Camila Lindoni Azevedo and Hongying Tang and Yunpeng Li", "abstract": "Accurate understanding of anatomical structures is essential for reliably staging certain dental diseases. A way of introducing this within semantic segmentation models is by utilising hierarchy-aware methodologies. However, existing hierarchy-aware segmentation methods largely encode anatomical structure through the loss functions, providing weak and indirect supervision. We introduce a general framework that embeds an explicit anatomical hierarchy into semantic segmentation by coupling a recurrent, level-wise prediction scheme with restrictive output heads and top-down feature conditioning. At each depth of the class tree, the backbone is re-run on the original image concatenated with logits from the previous level. Child class features are conditioned using Feature-wise Linear Modulation of their parent class probabilities, to modulate child feature spaces for fine grained detection. A probabilistic composition rule enforces consistency between parent and descendant classes. Hierarchical loss combines per-level class weighted Dice and cross entropy loss and a consistency term loss, ensuring parent predictions are the sum of their children. We validate our approach on our proposed dataset, TL-pano, containing 194 panoramic radiographs with dense instance and semantic segmentation annotations, of tooth layers and alveolar bone. Utilising UNet and HRNet as donor models across a 5-fold cross validation scheme, the hierarchical variants consistently increase IoU, Dice, and recall, particularly for fine-grained anatomies, and produce more anatomically coherent masks. However, hierarchical variants also demonstrated increased recall over precision, implying increased false positives. The results demonstrate that explicit hierarchical structuring improves both performance and clinical plausibility, especially in low data dental imaging regimes.", "link": "http://arxiv.org/abs/2512.07984v3", "date": "2025-12-22", "relevancy": 2.5142, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.505}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.505}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Restrictive%20Hierarchical%20Semantic%20Segmentation%20for%20Stratified%20Tooth%20Layer%20Detection&body=Title%3A%20Restrictive%20Hierarchical%20Semantic%20Segmentation%20for%20Stratified%20Tooth%20Layer%20Detection%0AAuthor%3A%20Ryan%20Banks%20and%20Camila%20Lindoni%20Azevedo%20and%20Hongying%20Tang%20and%20Yunpeng%20Li%0AAbstract%3A%20Accurate%20understanding%20of%20anatomical%20structures%20is%20essential%20for%20reliably%20staging%20certain%20dental%20diseases.%20A%20way%20of%20introducing%20this%20within%20semantic%20segmentation%20models%20is%20by%20utilising%20hierarchy-aware%20methodologies.%20However%2C%20existing%20hierarchy-aware%20segmentation%20methods%20largely%20encode%20anatomical%20structure%20through%20the%20loss%20functions%2C%20providing%20weak%20and%20indirect%20supervision.%20We%20introduce%20a%20general%20framework%20that%20embeds%20an%20explicit%20anatomical%20hierarchy%20into%20semantic%20segmentation%20by%20coupling%20a%20recurrent%2C%20level-wise%20prediction%20scheme%20with%20restrictive%20output%20heads%20and%20top-down%20feature%20conditioning.%20At%20each%20depth%20of%20the%20class%20tree%2C%20the%20backbone%20is%20re-run%20on%20the%20original%20image%20concatenated%20with%20logits%20from%20the%20previous%20level.%20Child%20class%20features%20are%20conditioned%20using%20Feature-wise%20Linear%20Modulation%20of%20their%20parent%20class%20probabilities%2C%20to%20modulate%20child%20feature%20spaces%20for%20fine%20grained%20detection.%20A%20probabilistic%20composition%20rule%20enforces%20consistency%20between%20parent%20and%20descendant%20classes.%20Hierarchical%20loss%20combines%20per-level%20class%20weighted%20Dice%20and%20cross%20entropy%20loss%20and%20a%20consistency%20term%20loss%2C%20ensuring%20parent%20predictions%20are%20the%20sum%20of%20their%20children.%20We%20validate%20our%20approach%20on%20our%20proposed%20dataset%2C%20TL-pano%2C%20containing%20194%20panoramic%20radiographs%20with%20dense%20instance%20and%20semantic%20segmentation%20annotations%2C%20of%20tooth%20layers%20and%20alveolar%20bone.%20Utilising%20UNet%20and%20HRNet%20as%20donor%20models%20across%20a%205-fold%20cross%20validation%20scheme%2C%20the%20hierarchical%20variants%20consistently%20increase%20IoU%2C%20Dice%2C%20and%20recall%2C%20particularly%20for%20fine-grained%20anatomies%2C%20and%20produce%20more%20anatomically%20coherent%20masks.%20However%2C%20hierarchical%20variants%20also%20demonstrated%20increased%20recall%20over%20precision%2C%20implying%20increased%20false%20positives.%20The%20results%20demonstrate%20that%20explicit%20hierarchical%20structuring%20improves%20both%20performance%20and%20clinical%20plausibility%2C%20especially%20in%20low%20data%20dental%20imaging%20regimes.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07984v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRestrictive%2520Hierarchical%2520Semantic%2520Segmentation%2520for%2520Stratified%2520Tooth%2520Layer%2520Detection%26entry.906535625%3DRyan%2520Banks%2520and%2520Camila%2520Lindoni%2520Azevedo%2520and%2520Hongying%2520Tang%2520and%2520Yunpeng%2520Li%26entry.1292438233%3DAccurate%2520understanding%2520of%2520anatomical%2520structures%2520is%2520essential%2520for%2520reliably%2520staging%2520certain%2520dental%2520diseases.%2520A%2520way%2520of%2520introducing%2520this%2520within%2520semantic%2520segmentation%2520models%2520is%2520by%2520utilising%2520hierarchy-aware%2520methodologies.%2520However%252C%2520existing%2520hierarchy-aware%2520segmentation%2520methods%2520largely%2520encode%2520anatomical%2520structure%2520through%2520the%2520loss%2520functions%252C%2520providing%2520weak%2520and%2520indirect%2520supervision.%2520We%2520introduce%2520a%2520general%2520framework%2520that%2520embeds%2520an%2520explicit%2520anatomical%2520hierarchy%2520into%2520semantic%2520segmentation%2520by%2520coupling%2520a%2520recurrent%252C%2520level-wise%2520prediction%2520scheme%2520with%2520restrictive%2520output%2520heads%2520and%2520top-down%2520feature%2520conditioning.%2520At%2520each%2520depth%2520of%2520the%2520class%2520tree%252C%2520the%2520backbone%2520is%2520re-run%2520on%2520the%2520original%2520image%2520concatenated%2520with%2520logits%2520from%2520the%2520previous%2520level.%2520Child%2520class%2520features%2520are%2520conditioned%2520using%2520Feature-wise%2520Linear%2520Modulation%2520of%2520their%2520parent%2520class%2520probabilities%252C%2520to%2520modulate%2520child%2520feature%2520spaces%2520for%2520fine%2520grained%2520detection.%2520A%2520probabilistic%2520composition%2520rule%2520enforces%2520consistency%2520between%2520parent%2520and%2520descendant%2520classes.%2520Hierarchical%2520loss%2520combines%2520per-level%2520class%2520weighted%2520Dice%2520and%2520cross%2520entropy%2520loss%2520and%2520a%2520consistency%2520term%2520loss%252C%2520ensuring%2520parent%2520predictions%2520are%2520the%2520sum%2520of%2520their%2520children.%2520We%2520validate%2520our%2520approach%2520on%2520our%2520proposed%2520dataset%252C%2520TL-pano%252C%2520containing%2520194%2520panoramic%2520radiographs%2520with%2520dense%2520instance%2520and%2520semantic%2520segmentation%2520annotations%252C%2520of%2520tooth%2520layers%2520and%2520alveolar%2520bone.%2520Utilising%2520UNet%2520and%2520HRNet%2520as%2520donor%2520models%2520across%2520a%25205-fold%2520cross%2520validation%2520scheme%252C%2520the%2520hierarchical%2520variants%2520consistently%2520increase%2520IoU%252C%2520Dice%252C%2520and%2520recall%252C%2520particularly%2520for%2520fine-grained%2520anatomies%252C%2520and%2520produce%2520more%2520anatomically%2520coherent%2520masks.%2520However%252C%2520hierarchical%2520variants%2520also%2520demonstrated%2520increased%2520recall%2520over%2520precision%252C%2520implying%2520increased%2520false%2520positives.%2520The%2520results%2520demonstrate%2520that%2520explicit%2520hierarchical%2520structuring%2520improves%2520both%2520performance%2520and%2520clinical%2520plausibility%252C%2520especially%2520in%2520low%2520data%2520dental%2520imaging%2520regimes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07984v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Restrictive%20Hierarchical%20Semantic%20Segmentation%20for%20Stratified%20Tooth%20Layer%20Detection&entry.906535625=Ryan%20Banks%20and%20Camila%20Lindoni%20Azevedo%20and%20Hongying%20Tang%20and%20Yunpeng%20Li&entry.1292438233=Accurate%20understanding%20of%20anatomical%20structures%20is%20essential%20for%20reliably%20staging%20certain%20dental%20diseases.%20A%20way%20of%20introducing%20this%20within%20semantic%20segmentation%20models%20is%20by%20utilising%20hierarchy-aware%20methodologies.%20However%2C%20existing%20hierarchy-aware%20segmentation%20methods%20largely%20encode%20anatomical%20structure%20through%20the%20loss%20functions%2C%20providing%20weak%20and%20indirect%20supervision.%20We%20introduce%20a%20general%20framework%20that%20embeds%20an%20explicit%20anatomical%20hierarchy%20into%20semantic%20segmentation%20by%20coupling%20a%20recurrent%2C%20level-wise%20prediction%20scheme%20with%20restrictive%20output%20heads%20and%20top-down%20feature%20conditioning.%20At%20each%20depth%20of%20the%20class%20tree%2C%20the%20backbone%20is%20re-run%20on%20the%20original%20image%20concatenated%20with%20logits%20from%20the%20previous%20level.%20Child%20class%20features%20are%20conditioned%20using%20Feature-wise%20Linear%20Modulation%20of%20their%20parent%20class%20probabilities%2C%20to%20modulate%20child%20feature%20spaces%20for%20fine%20grained%20detection.%20A%20probabilistic%20composition%20rule%20enforces%20consistency%20between%20parent%20and%20descendant%20classes.%20Hierarchical%20loss%20combines%20per-level%20class%20weighted%20Dice%20and%20cross%20entropy%20loss%20and%20a%20consistency%20term%20loss%2C%20ensuring%20parent%20predictions%20are%20the%20sum%20of%20their%20children.%20We%20validate%20our%20approach%20on%20our%20proposed%20dataset%2C%20TL-pano%2C%20containing%20194%20panoramic%20radiographs%20with%20dense%20instance%20and%20semantic%20segmentation%20annotations%2C%20of%20tooth%20layers%20and%20alveolar%20bone.%20Utilising%20UNet%20and%20HRNet%20as%20donor%20models%20across%20a%205-fold%20cross%20validation%20scheme%2C%20the%20hierarchical%20variants%20consistently%20increase%20IoU%2C%20Dice%2C%20and%20recall%2C%20particularly%20for%20fine-grained%20anatomies%2C%20and%20produce%20more%20anatomically%20coherent%20masks.%20However%2C%20hierarchical%20variants%20also%20demonstrated%20increased%20recall%20over%20precision%2C%20implying%20increased%20false%20positives.%20The%20results%20demonstrate%20that%20explicit%20hierarchical%20structuring%20improves%20both%20performance%20and%20clinical%20plausibility%2C%20especially%20in%20low%20data%20dental%20imaging%20regimes.&entry.1838667208=http%3A//arxiv.org/abs/2512.07984v3&entry.124074799=Read"},
{"title": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion", "author": "Hanyang Kong and Xingyi Yang and Xiaoxu Zheng and Xinchao Wang", "abstract": "Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a \"fill-and-revise\" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: \\href{https://hyokong.github.io/worldwarp-page/}{https://hyokong.github.io/worldwarp-page/}.", "link": "http://arxiv.org/abs/2512.19678v1", "date": "2025-12-22", "relevancy": 2.4952, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6917}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6158}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6047}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WorldWarp%3A%20Propagating%203D%20Geometry%20with%20Asynchronous%20Video%20Diffusion&body=Title%3A%20WorldWarp%3A%20Propagating%203D%20Geometry%20with%20Asynchronous%20Video%20Diffusion%0AAuthor%3A%20Hanyang%20Kong%20and%20Xingyi%20Yang%20and%20Xiaoxu%20Zheng%20and%20Xinchao%20Wang%0AAbstract%3A%20Generating%20long-range%2C%20geometrically%20consistent%20video%20presents%20a%20fundamental%20dilemma%3A%20while%20consistency%20demands%20strict%20adherence%20to%203D%20geometry%20in%20pixel%20space%2C%20state-of-the-art%20generative%20models%20operate%20most%20effectively%20in%20a%20camera-conditioned%20latent%20space.%20This%20disconnect%20causes%20current%20methods%20to%20struggle%20with%20occluded%20areas%20and%20complex%20camera%20trajectories.%20To%20bridge%20this%20gap%2C%20we%20propose%20WorldWarp%2C%20a%20framework%20that%20couples%20a%203D%20structural%20anchor%20with%20a%202D%20generative%20refiner.%20To%20establish%20geometric%20grounding%2C%20WorldWarp%20maintains%20an%20online%203D%20geometric%20cache%20built%20via%20Gaussian%20Splatting%20%283DGS%29.%20By%20explicitly%20warping%20historical%20content%20into%20novel%20views%2C%20this%20cache%20acts%20as%20a%20structural%20scaffold%2C%20ensuring%20each%20new%20frame%20respects%20prior%20geometry.%20However%2C%20static%20warping%20inevitably%20leaves%20holes%20and%20artifacts%20due%20to%20occlusions.%20We%20address%20this%20using%20a%20Spatio-Temporal%20Diffusion%20%28ST-Diff%29%20model%20designed%20for%20a%20%22fill-and-revise%22%20objective.%20Our%20key%20innovation%20is%20a%20spatio-temporal%20varying%20noise%20schedule%3A%20blank%20regions%20receive%20full%20noise%20to%20trigger%20generation%2C%20while%20warped%20regions%20receive%20partial%20noise%20to%20enable%20refinement.%20By%20dynamically%20updating%20the%203D%20cache%20at%20every%20step%2C%20WorldWarp%20maintains%20consistency%20across%20video%20chunks.%20Consequently%2C%20it%20achieves%20state-of-the-art%20fidelity%20by%20ensuring%20that%203D%20logic%20guides%20structure%20while%20diffusion%20logic%20perfects%20texture.%20Project%20page%3A%20%5Chref%7Bhttps%3A//hyokong.github.io/worldwarp-page/%7D%7Bhttps%3A//hyokong.github.io/worldwarp-page/%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19678v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWorldWarp%253A%2520Propagating%25203D%2520Geometry%2520with%2520Asynchronous%2520Video%2520Diffusion%26entry.906535625%3DHanyang%2520Kong%2520and%2520Xingyi%2520Yang%2520and%2520Xiaoxu%2520Zheng%2520and%2520Xinchao%2520Wang%26entry.1292438233%3DGenerating%2520long-range%252C%2520geometrically%2520consistent%2520video%2520presents%2520a%2520fundamental%2520dilemma%253A%2520while%2520consistency%2520demands%2520strict%2520adherence%2520to%25203D%2520geometry%2520in%2520pixel%2520space%252C%2520state-of-the-art%2520generative%2520models%2520operate%2520most%2520effectively%2520in%2520a%2520camera-conditioned%2520latent%2520space.%2520This%2520disconnect%2520causes%2520current%2520methods%2520to%2520struggle%2520with%2520occluded%2520areas%2520and%2520complex%2520camera%2520trajectories.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520WorldWarp%252C%2520a%2520framework%2520that%2520couples%2520a%25203D%2520structural%2520anchor%2520with%2520a%25202D%2520generative%2520refiner.%2520To%2520establish%2520geometric%2520grounding%252C%2520WorldWarp%2520maintains%2520an%2520online%25203D%2520geometric%2520cache%2520built%2520via%2520Gaussian%2520Splatting%2520%25283DGS%2529.%2520By%2520explicitly%2520warping%2520historical%2520content%2520into%2520novel%2520views%252C%2520this%2520cache%2520acts%2520as%2520a%2520structural%2520scaffold%252C%2520ensuring%2520each%2520new%2520frame%2520respects%2520prior%2520geometry.%2520However%252C%2520static%2520warping%2520inevitably%2520leaves%2520holes%2520and%2520artifacts%2520due%2520to%2520occlusions.%2520We%2520address%2520this%2520using%2520a%2520Spatio-Temporal%2520Diffusion%2520%2528ST-Diff%2529%2520model%2520designed%2520for%2520a%2520%2522fill-and-revise%2522%2520objective.%2520Our%2520key%2520innovation%2520is%2520a%2520spatio-temporal%2520varying%2520noise%2520schedule%253A%2520blank%2520regions%2520receive%2520full%2520noise%2520to%2520trigger%2520generation%252C%2520while%2520warped%2520regions%2520receive%2520partial%2520noise%2520to%2520enable%2520refinement.%2520By%2520dynamically%2520updating%2520the%25203D%2520cache%2520at%2520every%2520step%252C%2520WorldWarp%2520maintains%2520consistency%2520across%2520video%2520chunks.%2520Consequently%252C%2520it%2520achieves%2520state-of-the-art%2520fidelity%2520by%2520ensuring%2520that%25203D%2520logic%2520guides%2520structure%2520while%2520diffusion%2520logic%2520perfects%2520texture.%2520Project%2520page%253A%2520%255Chref%257Bhttps%253A//hyokong.github.io/worldwarp-page/%257D%257Bhttps%253A//hyokong.github.io/worldwarp-page/%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19678v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WorldWarp%3A%20Propagating%203D%20Geometry%20with%20Asynchronous%20Video%20Diffusion&entry.906535625=Hanyang%20Kong%20and%20Xingyi%20Yang%20and%20Xiaoxu%20Zheng%20and%20Xinchao%20Wang&entry.1292438233=Generating%20long-range%2C%20geometrically%20consistent%20video%20presents%20a%20fundamental%20dilemma%3A%20while%20consistency%20demands%20strict%20adherence%20to%203D%20geometry%20in%20pixel%20space%2C%20state-of-the-art%20generative%20models%20operate%20most%20effectively%20in%20a%20camera-conditioned%20latent%20space.%20This%20disconnect%20causes%20current%20methods%20to%20struggle%20with%20occluded%20areas%20and%20complex%20camera%20trajectories.%20To%20bridge%20this%20gap%2C%20we%20propose%20WorldWarp%2C%20a%20framework%20that%20couples%20a%203D%20structural%20anchor%20with%20a%202D%20generative%20refiner.%20To%20establish%20geometric%20grounding%2C%20WorldWarp%20maintains%20an%20online%203D%20geometric%20cache%20built%20via%20Gaussian%20Splatting%20%283DGS%29.%20By%20explicitly%20warping%20historical%20content%20into%20novel%20views%2C%20this%20cache%20acts%20as%20a%20structural%20scaffold%2C%20ensuring%20each%20new%20frame%20respects%20prior%20geometry.%20However%2C%20static%20warping%20inevitably%20leaves%20holes%20and%20artifacts%20due%20to%20occlusions.%20We%20address%20this%20using%20a%20Spatio-Temporal%20Diffusion%20%28ST-Diff%29%20model%20designed%20for%20a%20%22fill-and-revise%22%20objective.%20Our%20key%20innovation%20is%20a%20spatio-temporal%20varying%20noise%20schedule%3A%20blank%20regions%20receive%20full%20noise%20to%20trigger%20generation%2C%20while%20warped%20regions%20receive%20partial%20noise%20to%20enable%20refinement.%20By%20dynamically%20updating%20the%203D%20cache%20at%20every%20step%2C%20WorldWarp%20maintains%20consistency%20across%20video%20chunks.%20Consequently%2C%20it%20achieves%20state-of-the-art%20fidelity%20by%20ensuring%20that%203D%20logic%20guides%20structure%20while%20diffusion%20logic%20perfects%20texture.%20Project%20page%3A%20%5Chref%7Bhttps%3A//hyokong.github.io/worldwarp-page/%7D%7Bhttps%3A//hyokong.github.io/worldwarp-page/%7D.&entry.1838667208=http%3A//arxiv.org/abs/2512.19678v1&entry.124074799=Read"},
{"title": "HippMetric: A skeletal-representation-based framework for cross-sectional and longitudinal hippocampal substructural morphometry", "author": "Na Gao and Chenfei Ye and Yanwu Yang and Anqi Li and Zhengbo He and Li Liang and Zhiyuan Liu and Xingyu Hao and Ting Ma and Tengfei Guo", "abstract": "Accurate characterization of hippocampal substructure is crucial for detecting subtle structural changes and identifying early neurodegenerative biomarkers. However, high inter-subject variability and complex folding pattern of human hippocampus hinder consistent cross-subject and longitudinal analysis. Most existing approaches rely on subject-specific modelling and lack a stable intrinsic coordinate system to accommodate anatomical variability, which limits their ability to establish reliable inter- and intra-individual correspondence. To address this, we propose HippMetric, a skeletal representation (s-rep)-based framework for hippocampal substructural morphometry and point-wise correspondence across individuals and scans. HippMetric builds on the Axis-Referenced Morphometric Model (ARMM) and employs a deformable skeletal coordinate system aligned with hippocampal anatomy and function, providing a biologically grounded reference for correspondence. Our framework comprises two core modules: a skeletal-based coordinate system that respects the hippocampus' conserved longitudinal lamellar architecture, in which functional units (lamellae) are stacked perpendicular to the long-axis, enabling anatomically consistent localization across subjects and time; and individualized s-reps generated through surface reconstruction, deformation, and geometrically constrained spoke refinement, enforcing boundary adherence, orthogonality and non-intersection to produce mathematically valid skeletal geometry. Extensive experiments on two international cohorts demonstrate that HippMetric achieves higher accuracy, reliability, and correspondence stability compared to existing shape models.", "link": "http://arxiv.org/abs/2512.19214v1", "date": "2025-12-22", "relevancy": 2.4898, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.51}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4951}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HippMetric%3A%20A%20skeletal-representation-based%20framework%20for%20cross-sectional%20and%20longitudinal%20hippocampal%20substructural%20morphometry&body=Title%3A%20HippMetric%3A%20A%20skeletal-representation-based%20framework%20for%20cross-sectional%20and%20longitudinal%20hippocampal%20substructural%20morphometry%0AAuthor%3A%20Na%20Gao%20and%20Chenfei%20Ye%20and%20Yanwu%20Yang%20and%20Anqi%20Li%20and%20Zhengbo%20He%20and%20Li%20Liang%20and%20Zhiyuan%20Liu%20and%20Xingyu%20Hao%20and%20Ting%20Ma%20and%20Tengfei%20Guo%0AAbstract%3A%20Accurate%20characterization%20of%20hippocampal%20substructure%20is%20crucial%20for%20detecting%20subtle%20structural%20changes%20and%20identifying%20early%20neurodegenerative%20biomarkers.%20However%2C%20high%20inter-subject%20variability%20and%20complex%20folding%20pattern%20of%20human%20hippocampus%20hinder%20consistent%20cross-subject%20and%20longitudinal%20analysis.%20Most%20existing%20approaches%20rely%20on%20subject-specific%20modelling%20and%20lack%20a%20stable%20intrinsic%20coordinate%20system%20to%20accommodate%20anatomical%20variability%2C%20which%20limits%20their%20ability%20to%20establish%20reliable%20inter-%20and%20intra-individual%20correspondence.%20To%20address%20this%2C%20we%20propose%20HippMetric%2C%20a%20skeletal%20representation%20%28s-rep%29-based%20framework%20for%20hippocampal%20substructural%20morphometry%20and%20point-wise%20correspondence%20across%20individuals%20and%20scans.%20HippMetric%20builds%20on%20the%20Axis-Referenced%20Morphometric%20Model%20%28ARMM%29%20and%20employs%20a%20deformable%20skeletal%20coordinate%20system%20aligned%20with%20hippocampal%20anatomy%20and%20function%2C%20providing%20a%20biologically%20grounded%20reference%20for%20correspondence.%20Our%20framework%20comprises%20two%20core%20modules%3A%20a%20skeletal-based%20coordinate%20system%20that%20respects%20the%20hippocampus%27%20conserved%20longitudinal%20lamellar%20architecture%2C%20in%20which%20functional%20units%20%28lamellae%29%20are%20stacked%20perpendicular%20to%20the%20long-axis%2C%20enabling%20anatomically%20consistent%20localization%20across%20subjects%20and%20time%3B%20and%20individualized%20s-reps%20generated%20through%20surface%20reconstruction%2C%20deformation%2C%20and%20geometrically%20constrained%20spoke%20refinement%2C%20enforcing%20boundary%20adherence%2C%20orthogonality%20and%20non-intersection%20to%20produce%20mathematically%20valid%20skeletal%20geometry.%20Extensive%20experiments%20on%20two%20international%20cohorts%20demonstrate%20that%20HippMetric%20achieves%20higher%20accuracy%2C%20reliability%2C%20and%20correspondence%20stability%20compared%20to%20existing%20shape%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19214v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHippMetric%253A%2520A%2520skeletal-representation-based%2520framework%2520for%2520cross-sectional%2520and%2520longitudinal%2520hippocampal%2520substructural%2520morphometry%26entry.906535625%3DNa%2520Gao%2520and%2520Chenfei%2520Ye%2520and%2520Yanwu%2520Yang%2520and%2520Anqi%2520Li%2520and%2520Zhengbo%2520He%2520and%2520Li%2520Liang%2520and%2520Zhiyuan%2520Liu%2520and%2520Xingyu%2520Hao%2520and%2520Ting%2520Ma%2520and%2520Tengfei%2520Guo%26entry.1292438233%3DAccurate%2520characterization%2520of%2520hippocampal%2520substructure%2520is%2520crucial%2520for%2520detecting%2520subtle%2520structural%2520changes%2520and%2520identifying%2520early%2520neurodegenerative%2520biomarkers.%2520However%252C%2520high%2520inter-subject%2520variability%2520and%2520complex%2520folding%2520pattern%2520of%2520human%2520hippocampus%2520hinder%2520consistent%2520cross-subject%2520and%2520longitudinal%2520analysis.%2520Most%2520existing%2520approaches%2520rely%2520on%2520subject-specific%2520modelling%2520and%2520lack%2520a%2520stable%2520intrinsic%2520coordinate%2520system%2520to%2520accommodate%2520anatomical%2520variability%252C%2520which%2520limits%2520their%2520ability%2520to%2520establish%2520reliable%2520inter-%2520and%2520intra-individual%2520correspondence.%2520To%2520address%2520this%252C%2520we%2520propose%2520HippMetric%252C%2520a%2520skeletal%2520representation%2520%2528s-rep%2529-based%2520framework%2520for%2520hippocampal%2520substructural%2520morphometry%2520and%2520point-wise%2520correspondence%2520across%2520individuals%2520and%2520scans.%2520HippMetric%2520builds%2520on%2520the%2520Axis-Referenced%2520Morphometric%2520Model%2520%2528ARMM%2529%2520and%2520employs%2520a%2520deformable%2520skeletal%2520coordinate%2520system%2520aligned%2520with%2520hippocampal%2520anatomy%2520and%2520function%252C%2520providing%2520a%2520biologically%2520grounded%2520reference%2520for%2520correspondence.%2520Our%2520framework%2520comprises%2520two%2520core%2520modules%253A%2520a%2520skeletal-based%2520coordinate%2520system%2520that%2520respects%2520the%2520hippocampus%2527%2520conserved%2520longitudinal%2520lamellar%2520architecture%252C%2520in%2520which%2520functional%2520units%2520%2528lamellae%2529%2520are%2520stacked%2520perpendicular%2520to%2520the%2520long-axis%252C%2520enabling%2520anatomically%2520consistent%2520localization%2520across%2520subjects%2520and%2520time%253B%2520and%2520individualized%2520s-reps%2520generated%2520through%2520surface%2520reconstruction%252C%2520deformation%252C%2520and%2520geometrically%2520constrained%2520spoke%2520refinement%252C%2520enforcing%2520boundary%2520adherence%252C%2520orthogonality%2520and%2520non-intersection%2520to%2520produce%2520mathematically%2520valid%2520skeletal%2520geometry.%2520Extensive%2520experiments%2520on%2520two%2520international%2520cohorts%2520demonstrate%2520that%2520HippMetric%2520achieves%2520higher%2520accuracy%252C%2520reliability%252C%2520and%2520correspondence%2520stability%2520compared%2520to%2520existing%2520shape%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19214v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HippMetric%3A%20A%20skeletal-representation-based%20framework%20for%20cross-sectional%20and%20longitudinal%20hippocampal%20substructural%20morphometry&entry.906535625=Na%20Gao%20and%20Chenfei%20Ye%20and%20Yanwu%20Yang%20and%20Anqi%20Li%20and%20Zhengbo%20He%20and%20Li%20Liang%20and%20Zhiyuan%20Liu%20and%20Xingyu%20Hao%20and%20Ting%20Ma%20and%20Tengfei%20Guo&entry.1292438233=Accurate%20characterization%20of%20hippocampal%20substructure%20is%20crucial%20for%20detecting%20subtle%20structural%20changes%20and%20identifying%20early%20neurodegenerative%20biomarkers.%20However%2C%20high%20inter-subject%20variability%20and%20complex%20folding%20pattern%20of%20human%20hippocampus%20hinder%20consistent%20cross-subject%20and%20longitudinal%20analysis.%20Most%20existing%20approaches%20rely%20on%20subject-specific%20modelling%20and%20lack%20a%20stable%20intrinsic%20coordinate%20system%20to%20accommodate%20anatomical%20variability%2C%20which%20limits%20their%20ability%20to%20establish%20reliable%20inter-%20and%20intra-individual%20correspondence.%20To%20address%20this%2C%20we%20propose%20HippMetric%2C%20a%20skeletal%20representation%20%28s-rep%29-based%20framework%20for%20hippocampal%20substructural%20morphometry%20and%20point-wise%20correspondence%20across%20individuals%20and%20scans.%20HippMetric%20builds%20on%20the%20Axis-Referenced%20Morphometric%20Model%20%28ARMM%29%20and%20employs%20a%20deformable%20skeletal%20coordinate%20system%20aligned%20with%20hippocampal%20anatomy%20and%20function%2C%20providing%20a%20biologically%20grounded%20reference%20for%20correspondence.%20Our%20framework%20comprises%20two%20core%20modules%3A%20a%20skeletal-based%20coordinate%20system%20that%20respects%20the%20hippocampus%27%20conserved%20longitudinal%20lamellar%20architecture%2C%20in%20which%20functional%20units%20%28lamellae%29%20are%20stacked%20perpendicular%20to%20the%20long-axis%2C%20enabling%20anatomically%20consistent%20localization%20across%20subjects%20and%20time%3B%20and%20individualized%20s-reps%20generated%20through%20surface%20reconstruction%2C%20deformation%2C%20and%20geometrically%20constrained%20spoke%20refinement%2C%20enforcing%20boundary%20adherence%2C%20orthogonality%20and%20non-intersection%20to%20produce%20mathematically%20valid%20skeletal%20geometry.%20Extensive%20experiments%20on%20two%20international%20cohorts%20demonstrate%20that%20HippMetric%20achieves%20higher%20accuracy%2C%20reliability%2C%20and%20correspondence%20stability%20compared%20to%20existing%20shape%20models.&entry.1838667208=http%3A//arxiv.org/abs/2512.19214v1&entry.124074799=Read"},
{"title": "VisionDirector: Vision-Language Guided Closed-Loop Refinement for Generative Image Synthesis", "author": "Meng Chu and Senqiao Yang and Haoxuan Che and Suiyun Zhang and Xichen Zhang and Shaozuo Yu and Haokun Gui and Zhefan Rao and Dandan Tu and Rui Liu and Jiaya Jia", "abstract": "Generative models can now produce photorealistic imagery, yet they still struggle with the long, multi-goal prompts that professional designers issue. To expose this gap and better evaluate models' performance in real-world settings, we introduce Long Goal Bench (LGBench), a 2,000-task suite (1,000 T2I and 1,000 I2I) whose average instruction contains 18 to 22 tightly coupled goals spanning global layout, local object placement, typography, and logo fidelity. We find that even state-of-the-art models satisfy fewer than 72 percent of the goals and routinely miss localized edits, confirming the brittleness of current pipelines. To address this, we present VisionDirector, a training-free vision-language supervisor that (i) extracts structured goals from long instructions, (ii) dynamically decides between one-shot generation and staged edits, (iii) runs micro-grid sampling with semantic verification and rollback after every edit, and (iv) logs goal-level rewards. We further fine-tune the planner with Group Relative Policy Optimization, yielding shorter edit trajectories (3.1 versus 4.2 steps) and stronger alignment. VisionDirector achieves new state of the art on GenEval (plus 7 percent overall) and ImgEdit (plus 0.07 absolute) while producing consistent qualitative improvements on typography, multi-object scenes, and pose editing.", "link": "http://arxiv.org/abs/2512.19243v1", "date": "2025-12-22", "relevancy": 2.4465, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6166}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6159}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VisionDirector%3A%20Vision-Language%20Guided%20Closed-Loop%20Refinement%20for%20Generative%20Image%20Synthesis&body=Title%3A%20VisionDirector%3A%20Vision-Language%20Guided%20Closed-Loop%20Refinement%20for%20Generative%20Image%20Synthesis%0AAuthor%3A%20Meng%20Chu%20and%20Senqiao%20Yang%20and%20Haoxuan%20Che%20and%20Suiyun%20Zhang%20and%20Xichen%20Zhang%20and%20Shaozuo%20Yu%20and%20Haokun%20Gui%20and%20Zhefan%20Rao%20and%20Dandan%20Tu%20and%20Rui%20Liu%20and%20Jiaya%20Jia%0AAbstract%3A%20Generative%20models%20can%20now%20produce%20photorealistic%20imagery%2C%20yet%20they%20still%20struggle%20with%20the%20long%2C%20multi-goal%20prompts%20that%20professional%20designers%20issue.%20To%20expose%20this%20gap%20and%20better%20evaluate%20models%27%20performance%20in%20real-world%20settings%2C%20we%20introduce%20Long%20Goal%20Bench%20%28LGBench%29%2C%20a%202%2C000-task%20suite%20%281%2C000%20T2I%20and%201%2C000%20I2I%29%20whose%20average%20instruction%20contains%2018%20to%2022%20tightly%20coupled%20goals%20spanning%20global%20layout%2C%20local%20object%20placement%2C%20typography%2C%20and%20logo%20fidelity.%20We%20find%20that%20even%20state-of-the-art%20models%20satisfy%20fewer%20than%2072%20percent%20of%20the%20goals%20and%20routinely%20miss%20localized%20edits%2C%20confirming%20the%20brittleness%20of%20current%20pipelines.%20To%20address%20this%2C%20we%20present%20VisionDirector%2C%20a%20training-free%20vision-language%20supervisor%20that%20%28i%29%20extracts%20structured%20goals%20from%20long%20instructions%2C%20%28ii%29%20dynamically%20decides%20between%20one-shot%20generation%20and%20staged%20edits%2C%20%28iii%29%20runs%20micro-grid%20sampling%20with%20semantic%20verification%20and%20rollback%20after%20every%20edit%2C%20and%20%28iv%29%20logs%20goal-level%20rewards.%20We%20further%20fine-tune%20the%20planner%20with%20Group%20Relative%20Policy%20Optimization%2C%20yielding%20shorter%20edit%20trajectories%20%283.1%20versus%204.2%20steps%29%20and%20stronger%20alignment.%20VisionDirector%20achieves%20new%20state%20of%20the%20art%20on%20GenEval%20%28plus%207%20percent%20overall%29%20and%20ImgEdit%20%28plus%200.07%20absolute%29%20while%20producing%20consistent%20qualitative%20improvements%20on%20typography%2C%20multi-object%20scenes%2C%20and%20pose%20editing.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19243v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisionDirector%253A%2520Vision-Language%2520Guided%2520Closed-Loop%2520Refinement%2520for%2520Generative%2520Image%2520Synthesis%26entry.906535625%3DMeng%2520Chu%2520and%2520Senqiao%2520Yang%2520and%2520Haoxuan%2520Che%2520and%2520Suiyun%2520Zhang%2520and%2520Xichen%2520Zhang%2520and%2520Shaozuo%2520Yu%2520and%2520Haokun%2520Gui%2520and%2520Zhefan%2520Rao%2520and%2520Dandan%2520Tu%2520and%2520Rui%2520Liu%2520and%2520Jiaya%2520Jia%26entry.1292438233%3DGenerative%2520models%2520can%2520now%2520produce%2520photorealistic%2520imagery%252C%2520yet%2520they%2520still%2520struggle%2520with%2520the%2520long%252C%2520multi-goal%2520prompts%2520that%2520professional%2520designers%2520issue.%2520To%2520expose%2520this%2520gap%2520and%2520better%2520evaluate%2520models%2527%2520performance%2520in%2520real-world%2520settings%252C%2520we%2520introduce%2520Long%2520Goal%2520Bench%2520%2528LGBench%2529%252C%2520a%25202%252C000-task%2520suite%2520%25281%252C000%2520T2I%2520and%25201%252C000%2520I2I%2529%2520whose%2520average%2520instruction%2520contains%252018%2520to%252022%2520tightly%2520coupled%2520goals%2520spanning%2520global%2520layout%252C%2520local%2520object%2520placement%252C%2520typography%252C%2520and%2520logo%2520fidelity.%2520We%2520find%2520that%2520even%2520state-of-the-art%2520models%2520satisfy%2520fewer%2520than%252072%2520percent%2520of%2520the%2520goals%2520and%2520routinely%2520miss%2520localized%2520edits%252C%2520confirming%2520the%2520brittleness%2520of%2520current%2520pipelines.%2520To%2520address%2520this%252C%2520we%2520present%2520VisionDirector%252C%2520a%2520training-free%2520vision-language%2520supervisor%2520that%2520%2528i%2529%2520extracts%2520structured%2520goals%2520from%2520long%2520instructions%252C%2520%2528ii%2529%2520dynamically%2520decides%2520between%2520one-shot%2520generation%2520and%2520staged%2520edits%252C%2520%2528iii%2529%2520runs%2520micro-grid%2520sampling%2520with%2520semantic%2520verification%2520and%2520rollback%2520after%2520every%2520edit%252C%2520and%2520%2528iv%2529%2520logs%2520goal-level%2520rewards.%2520We%2520further%2520fine-tune%2520the%2520planner%2520with%2520Group%2520Relative%2520Policy%2520Optimization%252C%2520yielding%2520shorter%2520edit%2520trajectories%2520%25283.1%2520versus%25204.2%2520steps%2529%2520and%2520stronger%2520alignment.%2520VisionDirector%2520achieves%2520new%2520state%2520of%2520the%2520art%2520on%2520GenEval%2520%2528plus%25207%2520percent%2520overall%2529%2520and%2520ImgEdit%2520%2528plus%25200.07%2520absolute%2529%2520while%2520producing%2520consistent%2520qualitative%2520improvements%2520on%2520typography%252C%2520multi-object%2520scenes%252C%2520and%2520pose%2520editing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19243v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisionDirector%3A%20Vision-Language%20Guided%20Closed-Loop%20Refinement%20for%20Generative%20Image%20Synthesis&entry.906535625=Meng%20Chu%20and%20Senqiao%20Yang%20and%20Haoxuan%20Che%20and%20Suiyun%20Zhang%20and%20Xichen%20Zhang%20and%20Shaozuo%20Yu%20and%20Haokun%20Gui%20and%20Zhefan%20Rao%20and%20Dandan%20Tu%20and%20Rui%20Liu%20and%20Jiaya%20Jia&entry.1292438233=Generative%20models%20can%20now%20produce%20photorealistic%20imagery%2C%20yet%20they%20still%20struggle%20with%20the%20long%2C%20multi-goal%20prompts%20that%20professional%20designers%20issue.%20To%20expose%20this%20gap%20and%20better%20evaluate%20models%27%20performance%20in%20real-world%20settings%2C%20we%20introduce%20Long%20Goal%20Bench%20%28LGBench%29%2C%20a%202%2C000-task%20suite%20%281%2C000%20T2I%20and%201%2C000%20I2I%29%20whose%20average%20instruction%20contains%2018%20to%2022%20tightly%20coupled%20goals%20spanning%20global%20layout%2C%20local%20object%20placement%2C%20typography%2C%20and%20logo%20fidelity.%20We%20find%20that%20even%20state-of-the-art%20models%20satisfy%20fewer%20than%2072%20percent%20of%20the%20goals%20and%20routinely%20miss%20localized%20edits%2C%20confirming%20the%20brittleness%20of%20current%20pipelines.%20To%20address%20this%2C%20we%20present%20VisionDirector%2C%20a%20training-free%20vision-language%20supervisor%20that%20%28i%29%20extracts%20structured%20goals%20from%20long%20instructions%2C%20%28ii%29%20dynamically%20decides%20between%20one-shot%20generation%20and%20staged%20edits%2C%20%28iii%29%20runs%20micro-grid%20sampling%20with%20semantic%20verification%20and%20rollback%20after%20every%20edit%2C%20and%20%28iv%29%20logs%20goal-level%20rewards.%20We%20further%20fine-tune%20the%20planner%20with%20Group%20Relative%20Policy%20Optimization%2C%20yielding%20shorter%20edit%20trajectories%20%283.1%20versus%204.2%20steps%29%20and%20stronger%20alignment.%20VisionDirector%20achieves%20new%20state%20of%20the%20art%20on%20GenEval%20%28plus%207%20percent%20overall%29%20and%20ImgEdit%20%28plus%200.07%20absolute%29%20while%20producing%20consistent%20qualitative%20improvements%20on%20typography%2C%20multi-object%20scenes%2C%20and%20pose%20editing.&entry.1838667208=http%3A//arxiv.org/abs/2512.19243v1&entry.124074799=Read"},
{"title": "From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs", "author": "Mingrui Wu and Zhaozhi Wang and Fangjinhua Wang and Jiaolong Yang and Marc Pollefeys and Tong Zhang", "abstract": "While Multimodal Large Language Models (MLLMs) have achieved impressive performance on semantic tasks, their spatial intelligence--crucial for robust and grounded AI systems--remains underdeveloped. Existing benchmarks fall short of diagnosing this limitation: they either focus on overly simplified qualitative reasoning or rely on domain-specific indoor data, constrained by the lack of outdoor datasets with verifiable metric ground truth. To bridge this gap, we introduce a large-scale benchmark built from pedestrian-perspective videos captured with synchronized stereo cameras, LiDAR, and IMU/GPS sensors. This dataset provides metrically precise 3D information, enabling the automatic generation of spatial reasoning questions that span a hierarchical spectrum--from qualitative relational reasoning to quantitative metric and kinematic understanding. Evaluations reveal that the performance gains observed in structured indoor benchmarks vanish in open-world settings. Further analysis using synthetic abnormal scenes and blinding tests confirms that current MLLMs depend heavily on linguistic priors instead of grounded visual reasoning. Our benchmark thus provides a principled platform for diagnosing these limitations and advancing physically grounded spatial intelligence.", "link": "http://arxiv.org/abs/2512.19683v1", "date": "2025-12-22", "relevancy": 2.4458, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6158}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6158}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Indoor%20to%20Open%20World%3A%20Revealing%20the%20Spatial%20Reasoning%20Gap%20in%20MLLMs&body=Title%3A%20From%20Indoor%20to%20Open%20World%3A%20Revealing%20the%20Spatial%20Reasoning%20Gap%20in%20MLLMs%0AAuthor%3A%20Mingrui%20Wu%20and%20Zhaozhi%20Wang%20and%20Fangjinhua%20Wang%20and%20Jiaolong%20Yang%20and%20Marc%20Pollefeys%20and%20Tong%20Zhang%0AAbstract%3A%20While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20impressive%20performance%20on%20semantic%20tasks%2C%20their%20spatial%20intelligence--crucial%20for%20robust%20and%20grounded%20AI%20systems--remains%20underdeveloped.%20Existing%20benchmarks%20fall%20short%20of%20diagnosing%20this%20limitation%3A%20they%20either%20focus%20on%20overly%20simplified%20qualitative%20reasoning%20or%20rely%20on%20domain-specific%20indoor%20data%2C%20constrained%20by%20the%20lack%20of%20outdoor%20datasets%20with%20verifiable%20metric%20ground%20truth.%20To%20bridge%20this%20gap%2C%20we%20introduce%20a%20large-scale%20benchmark%20built%20from%20pedestrian-perspective%20videos%20captured%20with%20synchronized%20stereo%20cameras%2C%20LiDAR%2C%20and%20IMU/GPS%20sensors.%20This%20dataset%20provides%20metrically%20precise%203D%20information%2C%20enabling%20the%20automatic%20generation%20of%20spatial%20reasoning%20questions%20that%20span%20a%20hierarchical%20spectrum--from%20qualitative%20relational%20reasoning%20to%20quantitative%20metric%20and%20kinematic%20understanding.%20Evaluations%20reveal%20that%20the%20performance%20gains%20observed%20in%20structured%20indoor%20benchmarks%20vanish%20in%20open-world%20settings.%20Further%20analysis%20using%20synthetic%20abnormal%20scenes%20and%20blinding%20tests%20confirms%20that%20current%20MLLMs%20depend%20heavily%20on%20linguistic%20priors%20instead%20of%20grounded%20visual%20reasoning.%20Our%20benchmark%20thus%20provides%20a%20principled%20platform%20for%20diagnosing%20these%20limitations%20and%20advancing%20physically%20grounded%20spatial%20intelligence.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19683v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Indoor%2520to%2520Open%2520World%253A%2520Revealing%2520the%2520Spatial%2520Reasoning%2520Gap%2520in%2520MLLMs%26entry.906535625%3DMingrui%2520Wu%2520and%2520Zhaozhi%2520Wang%2520and%2520Fangjinhua%2520Wang%2520and%2520Jiaolong%2520Yang%2520and%2520Marc%2520Pollefeys%2520and%2520Tong%2520Zhang%26entry.1292438233%3DWhile%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520achieved%2520impressive%2520performance%2520on%2520semantic%2520tasks%252C%2520their%2520spatial%2520intelligence--crucial%2520for%2520robust%2520and%2520grounded%2520AI%2520systems--remains%2520underdeveloped.%2520Existing%2520benchmarks%2520fall%2520short%2520of%2520diagnosing%2520this%2520limitation%253A%2520they%2520either%2520focus%2520on%2520overly%2520simplified%2520qualitative%2520reasoning%2520or%2520rely%2520on%2520domain-specific%2520indoor%2520data%252C%2520constrained%2520by%2520the%2520lack%2520of%2520outdoor%2520datasets%2520with%2520verifiable%2520metric%2520ground%2520truth.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520a%2520large-scale%2520benchmark%2520built%2520from%2520pedestrian-perspective%2520videos%2520captured%2520with%2520synchronized%2520stereo%2520cameras%252C%2520LiDAR%252C%2520and%2520IMU/GPS%2520sensors.%2520This%2520dataset%2520provides%2520metrically%2520precise%25203D%2520information%252C%2520enabling%2520the%2520automatic%2520generation%2520of%2520spatial%2520reasoning%2520questions%2520that%2520span%2520a%2520hierarchical%2520spectrum--from%2520qualitative%2520relational%2520reasoning%2520to%2520quantitative%2520metric%2520and%2520kinematic%2520understanding.%2520Evaluations%2520reveal%2520that%2520the%2520performance%2520gains%2520observed%2520in%2520structured%2520indoor%2520benchmarks%2520vanish%2520in%2520open-world%2520settings.%2520Further%2520analysis%2520using%2520synthetic%2520abnormal%2520scenes%2520and%2520blinding%2520tests%2520confirms%2520that%2520current%2520MLLMs%2520depend%2520heavily%2520on%2520linguistic%2520priors%2520instead%2520of%2520grounded%2520visual%2520reasoning.%2520Our%2520benchmark%2520thus%2520provides%2520a%2520principled%2520platform%2520for%2520diagnosing%2520these%2520limitations%2520and%2520advancing%2520physically%2520grounded%2520spatial%2520intelligence.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19683v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Indoor%20to%20Open%20World%3A%20Revealing%20the%20Spatial%20Reasoning%20Gap%20in%20MLLMs&entry.906535625=Mingrui%20Wu%20and%20Zhaozhi%20Wang%20and%20Fangjinhua%20Wang%20and%20Jiaolong%20Yang%20and%20Marc%20Pollefeys%20and%20Tong%20Zhang&entry.1292438233=While%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20impressive%20performance%20on%20semantic%20tasks%2C%20their%20spatial%20intelligence--crucial%20for%20robust%20and%20grounded%20AI%20systems--remains%20underdeveloped.%20Existing%20benchmarks%20fall%20short%20of%20diagnosing%20this%20limitation%3A%20they%20either%20focus%20on%20overly%20simplified%20qualitative%20reasoning%20or%20rely%20on%20domain-specific%20indoor%20data%2C%20constrained%20by%20the%20lack%20of%20outdoor%20datasets%20with%20verifiable%20metric%20ground%20truth.%20To%20bridge%20this%20gap%2C%20we%20introduce%20a%20large-scale%20benchmark%20built%20from%20pedestrian-perspective%20videos%20captured%20with%20synchronized%20stereo%20cameras%2C%20LiDAR%2C%20and%20IMU/GPS%20sensors.%20This%20dataset%20provides%20metrically%20precise%203D%20information%2C%20enabling%20the%20automatic%20generation%20of%20spatial%20reasoning%20questions%20that%20span%20a%20hierarchical%20spectrum--from%20qualitative%20relational%20reasoning%20to%20quantitative%20metric%20and%20kinematic%20understanding.%20Evaluations%20reveal%20that%20the%20performance%20gains%20observed%20in%20structured%20indoor%20benchmarks%20vanish%20in%20open-world%20settings.%20Further%20analysis%20using%20synthetic%20abnormal%20scenes%20and%20blinding%20tests%20confirms%20that%20current%20MLLMs%20depend%20heavily%20on%20linguistic%20priors%20instead%20of%20grounded%20visual%20reasoning.%20Our%20benchmark%20thus%20provides%20a%20principled%20platform%20for%20diagnosing%20these%20limitations%20and%20advancing%20physically%20grounded%20spatial%20intelligence.&entry.1838667208=http%3A//arxiv.org/abs/2512.19683v1&entry.124074799=Read"},
{"title": "Exploring the features used for summary evaluation by Human and GPT", "author": "Zahra Sadeghi and Evangelos Milios and Frank Rudzicz", "abstract": "Summary assessment involves evaluating how well a generated summary reflects the key ideas and meaning of the source text, requiring a deep understanding of the content. Large Language Models (LLMs) have been used to automate this process, acting as judges to evaluate summaries with respect to the original text. While previous research investigated the alignment between LLMs and Human responses, it is not yet well understood what properties or features are exploited by them when asked to evaluate based on a particular quality dimension, and there has not been much attention towards mapping between evaluation scores and metrics. In this paper, we address this issue and discover features aligned with Human and Generative Pre-trained Transformers (GPTs) responses by studying statistical and machine learning metrics. Furthermore, we show that instructing GPTs to employ metrics used by Human can improve their judgment and conforming them better with human responses.", "link": "http://arxiv.org/abs/2512.19620v1", "date": "2025-12-22", "relevancy": 2.4403, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.49}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.49}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20features%20used%20for%20summary%20evaluation%20by%20Human%20and%20GPT&body=Title%3A%20Exploring%20the%20features%20used%20for%20summary%20evaluation%20by%20Human%20and%20GPT%0AAuthor%3A%20Zahra%20Sadeghi%20and%20Evangelos%20Milios%20and%20Frank%20Rudzicz%0AAbstract%3A%20Summary%20assessment%20involves%20evaluating%20how%20well%20a%20generated%20summary%20reflects%20the%20key%20ideas%20and%20meaning%20of%20the%20source%20text%2C%20requiring%20a%20deep%20understanding%20of%20the%20content.%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20used%20to%20automate%20this%20process%2C%20acting%20as%20judges%20to%20evaluate%20summaries%20with%20respect%20to%20the%20original%20text.%20While%20previous%20research%20investigated%20the%20alignment%20between%20LLMs%20and%20Human%20responses%2C%20it%20is%20not%20yet%20well%20understood%20what%20properties%20or%20features%20are%20exploited%20by%20them%20when%20asked%20to%20evaluate%20based%20on%20a%20particular%20quality%20dimension%2C%20and%20there%20has%20not%20been%20much%20attention%20towards%20mapping%20between%20evaluation%20scores%20and%20metrics.%20In%20this%20paper%2C%20we%20address%20this%20issue%20and%20discover%20features%20aligned%20with%20Human%20and%20Generative%20Pre-trained%20Transformers%20%28GPTs%29%20responses%20by%20studying%20statistical%20and%20machine%20learning%20metrics.%20Furthermore%2C%20we%20show%20that%20instructing%20GPTs%20to%20employ%20metrics%20used%20by%20Human%20can%20improve%20their%20judgment%20and%20conforming%20them%20better%20with%20human%20responses.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19620v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520features%2520used%2520for%2520summary%2520evaluation%2520by%2520Human%2520and%2520GPT%26entry.906535625%3DZahra%2520Sadeghi%2520and%2520Evangelos%2520Milios%2520and%2520Frank%2520Rudzicz%26entry.1292438233%3DSummary%2520assessment%2520involves%2520evaluating%2520how%2520well%2520a%2520generated%2520summary%2520reflects%2520the%2520key%2520ideas%2520and%2520meaning%2520of%2520the%2520source%2520text%252C%2520requiring%2520a%2520deep%2520understanding%2520of%2520the%2520content.%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520been%2520used%2520to%2520automate%2520this%2520process%252C%2520acting%2520as%2520judges%2520to%2520evaluate%2520summaries%2520with%2520respect%2520to%2520the%2520original%2520text.%2520While%2520previous%2520research%2520investigated%2520the%2520alignment%2520between%2520LLMs%2520and%2520Human%2520responses%252C%2520it%2520is%2520not%2520yet%2520well%2520understood%2520what%2520properties%2520or%2520features%2520are%2520exploited%2520by%2520them%2520when%2520asked%2520to%2520evaluate%2520based%2520on%2520a%2520particular%2520quality%2520dimension%252C%2520and%2520there%2520has%2520not%2520been%2520much%2520attention%2520towards%2520mapping%2520between%2520evaluation%2520scores%2520and%2520metrics.%2520In%2520this%2520paper%252C%2520we%2520address%2520this%2520issue%2520and%2520discover%2520features%2520aligned%2520with%2520Human%2520and%2520Generative%2520Pre-trained%2520Transformers%2520%2528GPTs%2529%2520responses%2520by%2520studying%2520statistical%2520and%2520machine%2520learning%2520metrics.%2520Furthermore%252C%2520we%2520show%2520that%2520instructing%2520GPTs%2520to%2520employ%2520metrics%2520used%2520by%2520Human%2520can%2520improve%2520their%2520judgment%2520and%2520conforming%2520them%2520better%2520with%2520human%2520responses.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19620v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20features%20used%20for%20summary%20evaluation%20by%20Human%20and%20GPT&entry.906535625=Zahra%20Sadeghi%20and%20Evangelos%20Milios%20and%20Frank%20Rudzicz&entry.1292438233=Summary%20assessment%20involves%20evaluating%20how%20well%20a%20generated%20summary%20reflects%20the%20key%20ideas%20and%20meaning%20of%20the%20source%20text%2C%20requiring%20a%20deep%20understanding%20of%20the%20content.%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20used%20to%20automate%20this%20process%2C%20acting%20as%20judges%20to%20evaluate%20summaries%20with%20respect%20to%20the%20original%20text.%20While%20previous%20research%20investigated%20the%20alignment%20between%20LLMs%20and%20Human%20responses%2C%20it%20is%20not%20yet%20well%20understood%20what%20properties%20or%20features%20are%20exploited%20by%20them%20when%20asked%20to%20evaluate%20based%20on%20a%20particular%20quality%20dimension%2C%20and%20there%20has%20not%20been%20much%20attention%20towards%20mapping%20between%20evaluation%20scores%20and%20metrics.%20In%20this%20paper%2C%20we%20address%20this%20issue%20and%20discover%20features%20aligned%20with%20Human%20and%20Generative%20Pre-trained%20Transformers%20%28GPTs%29%20responses%20by%20studying%20statistical%20and%20machine%20learning%20metrics.%20Furthermore%2C%20we%20show%20that%20instructing%20GPTs%20to%20employ%20metrics%20used%20by%20Human%20can%20improve%20their%20judgment%20and%20conforming%20them%20better%20with%20human%20responses.&entry.1838667208=http%3A//arxiv.org/abs/2512.19620v1&entry.124074799=Read"},
{"title": "Over++: Generative Video Compositing for Layer Interaction Effects", "author": "Luchao Qi and Jiaye Wu and Jun Myeong Choi and Cary Phillips and Roni Sengupta and Dan B Goldman", "abstract": "In professional video compositing workflows, artists must manually create environmental interactions-such as shadows, reflections, dust, and splashes-between foreground subjects and background layers. Existing video generative models struggle to preserve the input video while adding such effects, and current video inpainting methods either require costly per-frame masks or yield implausible results. We introduce augmented compositing, a new task that synthesizes realistic, semi-transparent environmental effects conditioned on text prompts and input video layers, while preserving the original scene. To address this task, we present Over++, a video effect generation framework that makes no assumptions about camera pose, scene stationarity, or depth supervision. We construct a paired effect dataset tailored for this task and introduce an unpaired augmentation strategy that preserves text-driven editability. Our method also supports optional mask control and keyframe guidance without requiring dense annotations. Despite training on limited data, Over++ produces diverse and realistic environmental effects and outperforms existing baselines in both effect generation and scene preservation.", "link": "http://arxiv.org/abs/2512.19661v1", "date": "2025-12-22", "relevancy": 2.4336, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.7003}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.592}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Over%2B%2B%3A%20Generative%20Video%20Compositing%20for%20Layer%20Interaction%20Effects&body=Title%3A%20Over%2B%2B%3A%20Generative%20Video%20Compositing%20for%20Layer%20Interaction%20Effects%0AAuthor%3A%20Luchao%20Qi%20and%20Jiaye%20Wu%20and%20Jun%20Myeong%20Choi%20and%20Cary%20Phillips%20and%20Roni%20Sengupta%20and%20Dan%20B%20Goldman%0AAbstract%3A%20In%20professional%20video%20compositing%20workflows%2C%20artists%20must%20manually%20create%20environmental%20interactions-such%20as%20shadows%2C%20reflections%2C%20dust%2C%20and%20splashes-between%20foreground%20subjects%20and%20background%20layers.%20Existing%20video%20generative%20models%20struggle%20to%20preserve%20the%20input%20video%20while%20adding%20such%20effects%2C%20and%20current%20video%20inpainting%20methods%20either%20require%20costly%20per-frame%20masks%20or%20yield%20implausible%20results.%20We%20introduce%20augmented%20compositing%2C%20a%20new%20task%20that%20synthesizes%20realistic%2C%20semi-transparent%20environmental%20effects%20conditioned%20on%20text%20prompts%20and%20input%20video%20layers%2C%20while%20preserving%20the%20original%20scene.%20To%20address%20this%20task%2C%20we%20present%20Over%2B%2B%2C%20a%20video%20effect%20generation%20framework%20that%20makes%20no%20assumptions%20about%20camera%20pose%2C%20scene%20stationarity%2C%20or%20depth%20supervision.%20We%20construct%20a%20paired%20effect%20dataset%20tailored%20for%20this%20task%20and%20introduce%20an%20unpaired%20augmentation%20strategy%20that%20preserves%20text-driven%20editability.%20Our%20method%20also%20supports%20optional%20mask%20control%20and%20keyframe%20guidance%20without%20requiring%20dense%20annotations.%20Despite%20training%20on%20limited%20data%2C%20Over%2B%2B%20produces%20diverse%20and%20realistic%20environmental%20effects%20and%20outperforms%20existing%20baselines%20in%20both%20effect%20generation%20and%20scene%20preservation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19661v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOver%252B%252B%253A%2520Generative%2520Video%2520Compositing%2520for%2520Layer%2520Interaction%2520Effects%26entry.906535625%3DLuchao%2520Qi%2520and%2520Jiaye%2520Wu%2520and%2520Jun%2520Myeong%2520Choi%2520and%2520Cary%2520Phillips%2520and%2520Roni%2520Sengupta%2520and%2520Dan%2520B%2520Goldman%26entry.1292438233%3DIn%2520professional%2520video%2520compositing%2520workflows%252C%2520artists%2520must%2520manually%2520create%2520environmental%2520interactions-such%2520as%2520shadows%252C%2520reflections%252C%2520dust%252C%2520and%2520splashes-between%2520foreground%2520subjects%2520and%2520background%2520layers.%2520Existing%2520video%2520generative%2520models%2520struggle%2520to%2520preserve%2520the%2520input%2520video%2520while%2520adding%2520such%2520effects%252C%2520and%2520current%2520video%2520inpainting%2520methods%2520either%2520require%2520costly%2520per-frame%2520masks%2520or%2520yield%2520implausible%2520results.%2520We%2520introduce%2520augmented%2520compositing%252C%2520a%2520new%2520task%2520that%2520synthesizes%2520realistic%252C%2520semi-transparent%2520environmental%2520effects%2520conditioned%2520on%2520text%2520prompts%2520and%2520input%2520video%2520layers%252C%2520while%2520preserving%2520the%2520original%2520scene.%2520To%2520address%2520this%2520task%252C%2520we%2520present%2520Over%252B%252B%252C%2520a%2520video%2520effect%2520generation%2520framework%2520that%2520makes%2520no%2520assumptions%2520about%2520camera%2520pose%252C%2520scene%2520stationarity%252C%2520or%2520depth%2520supervision.%2520We%2520construct%2520a%2520paired%2520effect%2520dataset%2520tailored%2520for%2520this%2520task%2520and%2520introduce%2520an%2520unpaired%2520augmentation%2520strategy%2520that%2520preserves%2520text-driven%2520editability.%2520Our%2520method%2520also%2520supports%2520optional%2520mask%2520control%2520and%2520keyframe%2520guidance%2520without%2520requiring%2520dense%2520annotations.%2520Despite%2520training%2520on%2520limited%2520data%252C%2520Over%252B%252B%2520produces%2520diverse%2520and%2520realistic%2520environmental%2520effects%2520and%2520outperforms%2520existing%2520baselines%2520in%2520both%2520effect%2520generation%2520and%2520scene%2520preservation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19661v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Over%2B%2B%3A%20Generative%20Video%20Compositing%20for%20Layer%20Interaction%20Effects&entry.906535625=Luchao%20Qi%20and%20Jiaye%20Wu%20and%20Jun%20Myeong%20Choi%20and%20Cary%20Phillips%20and%20Roni%20Sengupta%20and%20Dan%20B%20Goldman&entry.1292438233=In%20professional%20video%20compositing%20workflows%2C%20artists%20must%20manually%20create%20environmental%20interactions-such%20as%20shadows%2C%20reflections%2C%20dust%2C%20and%20splashes-between%20foreground%20subjects%20and%20background%20layers.%20Existing%20video%20generative%20models%20struggle%20to%20preserve%20the%20input%20video%20while%20adding%20such%20effects%2C%20and%20current%20video%20inpainting%20methods%20either%20require%20costly%20per-frame%20masks%20or%20yield%20implausible%20results.%20We%20introduce%20augmented%20compositing%2C%20a%20new%20task%20that%20synthesizes%20realistic%2C%20semi-transparent%20environmental%20effects%20conditioned%20on%20text%20prompts%20and%20input%20video%20layers%2C%20while%20preserving%20the%20original%20scene.%20To%20address%20this%20task%2C%20we%20present%20Over%2B%2B%2C%20a%20video%20effect%20generation%20framework%20that%20makes%20no%20assumptions%20about%20camera%20pose%2C%20scene%20stationarity%2C%20or%20depth%20supervision.%20We%20construct%20a%20paired%20effect%20dataset%20tailored%20for%20this%20task%20and%20introduce%20an%20unpaired%20augmentation%20strategy%20that%20preserves%20text-driven%20editability.%20Our%20method%20also%20supports%20optional%20mask%20control%20and%20keyframe%20guidance%20without%20requiring%20dense%20annotations.%20Despite%20training%20on%20limited%20data%2C%20Over%2B%2B%20produces%20diverse%20and%20realistic%20environmental%20effects%20and%20outperforms%20existing%20baselines%20in%20both%20effect%20generation%20and%20scene%20preservation.&entry.1838667208=http%3A//arxiv.org/abs/2512.19661v1&entry.124074799=Read"},
{"title": "3SGen: Unified Subject, Style, and Structure-Driven Image Generation with Adaptive Task-specific Memory", "author": "Xinyang Song and Libin Wang and Weining Wang and Zhiwei Li and Jianxin Sun and Dandan Zheng and Jingdong Chen and Qi Li and Zhenan Sun", "abstract": "Recent image generation approaches often address subject, style, and structure-driven conditioning in isolation, leading to feature entanglement and limited task transferability. In this paper, we introduce 3SGen, a task-aware unified framework that performs all three conditioning modes within a single model. 3SGen employs an MLLM equipped with learnable semantic queries to align text-image semantics, complemented by a VAE branch that preserves fine-grained visual details. At its core, an Adaptive Task-specific Memory (ATM) module dynamically disentangles, stores, and retrieves condition-specific priors, such as identity for subjects, textures for styles, and spatial layouts for structures, via a lightweight gating mechanism along with several scalable memory items. This design mitigates inter-task interference and naturally scales to compositional inputs. In addition, we propose 3SGen-Bench, a unified image-driven generation benchmark with standardized metrics for evaluating cross-task fidelity and controllability. Extensive experiments on our proposed 3SGen-Bench and other public benchmarks demonstrate our superior performance across diverse image-driven generation tasks.", "link": "http://arxiv.org/abs/2512.19271v1", "date": "2025-12-22", "relevancy": 2.4252, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6091}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6046}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203SGen%3A%20Unified%20Subject%2C%20Style%2C%20and%20Structure-Driven%20Image%20Generation%20with%20Adaptive%20Task-specific%20Memory&body=Title%3A%203SGen%3A%20Unified%20Subject%2C%20Style%2C%20and%20Structure-Driven%20Image%20Generation%20with%20Adaptive%20Task-specific%20Memory%0AAuthor%3A%20Xinyang%20Song%20and%20Libin%20Wang%20and%20Weining%20Wang%20and%20Zhiwei%20Li%20and%20Jianxin%20Sun%20and%20Dandan%20Zheng%20and%20Jingdong%20Chen%20and%20Qi%20Li%20and%20Zhenan%20Sun%0AAbstract%3A%20Recent%20image%20generation%20approaches%20often%20address%20subject%2C%20style%2C%20and%20structure-driven%20conditioning%20in%20isolation%2C%20leading%20to%20feature%20entanglement%20and%20limited%20task%20transferability.%20In%20this%20paper%2C%20we%20introduce%203SGen%2C%20a%20task-aware%20unified%20framework%20that%20performs%20all%20three%20conditioning%20modes%20within%20a%20single%20model.%203SGen%20employs%20an%20MLLM%20equipped%20with%20learnable%20semantic%20queries%20to%20align%20text-image%20semantics%2C%20complemented%20by%20a%20VAE%20branch%20that%20preserves%20fine-grained%20visual%20details.%20At%20its%20core%2C%20an%20Adaptive%20Task-specific%20Memory%20%28ATM%29%20module%20dynamically%20disentangles%2C%20stores%2C%20and%20retrieves%20condition-specific%20priors%2C%20such%20as%20identity%20for%20subjects%2C%20textures%20for%20styles%2C%20and%20spatial%20layouts%20for%20structures%2C%20via%20a%20lightweight%20gating%20mechanism%20along%20with%20several%20scalable%20memory%20items.%20This%20design%20mitigates%20inter-task%20interference%20and%20naturally%20scales%20to%20compositional%20inputs.%20In%20addition%2C%20we%20propose%203SGen-Bench%2C%20a%20unified%20image-driven%20generation%20benchmark%20with%20standardized%20metrics%20for%20evaluating%20cross-task%20fidelity%20and%20controllability.%20Extensive%20experiments%20on%20our%20proposed%203SGen-Bench%20and%20other%20public%20benchmarks%20demonstrate%20our%20superior%20performance%20across%20diverse%20image-driven%20generation%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19271v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3SGen%253A%2520Unified%2520Subject%252C%2520Style%252C%2520and%2520Structure-Driven%2520Image%2520Generation%2520with%2520Adaptive%2520Task-specific%2520Memory%26entry.906535625%3DXinyang%2520Song%2520and%2520Libin%2520Wang%2520and%2520Weining%2520Wang%2520and%2520Zhiwei%2520Li%2520and%2520Jianxin%2520Sun%2520and%2520Dandan%2520Zheng%2520and%2520Jingdong%2520Chen%2520and%2520Qi%2520Li%2520and%2520Zhenan%2520Sun%26entry.1292438233%3DRecent%2520image%2520generation%2520approaches%2520often%2520address%2520subject%252C%2520style%252C%2520and%2520structure-driven%2520conditioning%2520in%2520isolation%252C%2520leading%2520to%2520feature%2520entanglement%2520and%2520limited%2520task%2520transferability.%2520In%2520this%2520paper%252C%2520we%2520introduce%25203SGen%252C%2520a%2520task-aware%2520unified%2520framework%2520that%2520performs%2520all%2520three%2520conditioning%2520modes%2520within%2520a%2520single%2520model.%25203SGen%2520employs%2520an%2520MLLM%2520equipped%2520with%2520learnable%2520semantic%2520queries%2520to%2520align%2520text-image%2520semantics%252C%2520complemented%2520by%2520a%2520VAE%2520branch%2520that%2520preserves%2520fine-grained%2520visual%2520details.%2520At%2520its%2520core%252C%2520an%2520Adaptive%2520Task-specific%2520Memory%2520%2528ATM%2529%2520module%2520dynamically%2520disentangles%252C%2520stores%252C%2520and%2520retrieves%2520condition-specific%2520priors%252C%2520such%2520as%2520identity%2520for%2520subjects%252C%2520textures%2520for%2520styles%252C%2520and%2520spatial%2520layouts%2520for%2520structures%252C%2520via%2520a%2520lightweight%2520gating%2520mechanism%2520along%2520with%2520several%2520scalable%2520memory%2520items.%2520This%2520design%2520mitigates%2520inter-task%2520interference%2520and%2520naturally%2520scales%2520to%2520compositional%2520inputs.%2520In%2520addition%252C%2520we%2520propose%25203SGen-Bench%252C%2520a%2520unified%2520image-driven%2520generation%2520benchmark%2520with%2520standardized%2520metrics%2520for%2520evaluating%2520cross-task%2520fidelity%2520and%2520controllability.%2520Extensive%2520experiments%2520on%2520our%2520proposed%25203SGen-Bench%2520and%2520other%2520public%2520benchmarks%2520demonstrate%2520our%2520superior%2520performance%2520across%2520diverse%2520image-driven%2520generation%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19271v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3SGen%3A%20Unified%20Subject%2C%20Style%2C%20and%20Structure-Driven%20Image%20Generation%20with%20Adaptive%20Task-specific%20Memory&entry.906535625=Xinyang%20Song%20and%20Libin%20Wang%20and%20Weining%20Wang%20and%20Zhiwei%20Li%20and%20Jianxin%20Sun%20and%20Dandan%20Zheng%20and%20Jingdong%20Chen%20and%20Qi%20Li%20and%20Zhenan%20Sun&entry.1292438233=Recent%20image%20generation%20approaches%20often%20address%20subject%2C%20style%2C%20and%20structure-driven%20conditioning%20in%20isolation%2C%20leading%20to%20feature%20entanglement%20and%20limited%20task%20transferability.%20In%20this%20paper%2C%20we%20introduce%203SGen%2C%20a%20task-aware%20unified%20framework%20that%20performs%20all%20three%20conditioning%20modes%20within%20a%20single%20model.%203SGen%20employs%20an%20MLLM%20equipped%20with%20learnable%20semantic%20queries%20to%20align%20text-image%20semantics%2C%20complemented%20by%20a%20VAE%20branch%20that%20preserves%20fine-grained%20visual%20details.%20At%20its%20core%2C%20an%20Adaptive%20Task-specific%20Memory%20%28ATM%29%20module%20dynamically%20disentangles%2C%20stores%2C%20and%20retrieves%20condition-specific%20priors%2C%20such%20as%20identity%20for%20subjects%2C%20textures%20for%20styles%2C%20and%20spatial%20layouts%20for%20structures%2C%20via%20a%20lightweight%20gating%20mechanism%20along%20with%20several%20scalable%20memory%20items.%20This%20design%20mitigates%20inter-task%20interference%20and%20naturally%20scales%20to%20compositional%20inputs.%20In%20addition%2C%20we%20propose%203SGen-Bench%2C%20a%20unified%20image-driven%20generation%20benchmark%20with%20standardized%20metrics%20for%20evaluating%20cross-task%20fidelity%20and%20controllability.%20Extensive%20experiments%20on%20our%20proposed%203SGen-Bench%20and%20other%20public%20benchmarks%20demonstrate%20our%20superior%20performance%20across%20diverse%20image-driven%20generation%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2512.19271v1&entry.124074799=Read"},
{"title": "The Reasoning Lingua Franca: A Double-Edged Sword for Multilingual AI", "author": "Alan Saji and Raj Dabre and Anoop Kunchukuttan and Ratish Puduppully", "abstract": "Large Reasoning Models (LRMs) achieve strong performance on mathematical, scientific, and other question-answering tasks, but their multilingual reasoning abilities remain underexplored. When presented with non-English questions, LRMs often default to reasoning in English, raising concerns about interpretability and the handling of linguistic and cultural nuances. We systematically compare an LRM's reasoning in English versus the language of the question. Our evaluation spans two tasks: MGSM and GPQA Diamond. Beyond measuring answer accuracy, we also analyze cognitive attributes in the reasoning traces. We find that English reasoning traces exhibit a substantially higher presence of these cognitive behaviors, and that reasoning in English generally yields higher final-answer accuracy, with the performance gap increasing as tasks become more complex. However, this English-centric strategy is susceptible to a key failure mode - getting \"Lost in Translation,\" where translation steps lead to errors that would have been avoided by question's language reasoning.", "link": "http://arxiv.org/abs/2510.20647v2", "date": "2025-12-22", "relevancy": 2.4215, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5104}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5104}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4322}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Reasoning%20Lingua%20Franca%3A%20A%20Double-Edged%20Sword%20for%20Multilingual%20AI&body=Title%3A%20The%20Reasoning%20Lingua%20Franca%3A%20A%20Double-Edged%20Sword%20for%20Multilingual%20AI%0AAuthor%3A%20Alan%20Saji%20and%20Raj%20Dabre%20and%20Anoop%20Kunchukuttan%20and%20Ratish%20Puduppully%0AAbstract%3A%20Large%20Reasoning%20Models%20%28LRMs%29%20achieve%20strong%20performance%20on%20mathematical%2C%20scientific%2C%20and%20other%20question-answering%20tasks%2C%20but%20their%20multilingual%20reasoning%20abilities%20remain%20underexplored.%20When%20presented%20with%20non-English%20questions%2C%20LRMs%20often%20default%20to%20reasoning%20in%20English%2C%20raising%20concerns%20about%20interpretability%20and%20the%20handling%20of%20linguistic%20and%20cultural%20nuances.%20We%20systematically%20compare%20an%20LRM%27s%20reasoning%20in%20English%20versus%20the%20language%20of%20the%20question.%20Our%20evaluation%20spans%20two%20tasks%3A%20MGSM%20and%20GPQA%20Diamond.%20Beyond%20measuring%20answer%20accuracy%2C%20we%20also%20analyze%20cognitive%20attributes%20in%20the%20reasoning%20traces.%20We%20find%20that%20English%20reasoning%20traces%20exhibit%20a%20substantially%20higher%20presence%20of%20these%20cognitive%20behaviors%2C%20and%20that%20reasoning%20in%20English%20generally%20yields%20higher%20final-answer%20accuracy%2C%20with%20the%20performance%20gap%20increasing%20as%20tasks%20become%20more%20complex.%20However%2C%20this%20English-centric%20strategy%20is%20susceptible%20to%20a%20key%20failure%20mode%20-%20getting%20%22Lost%20in%20Translation%2C%22%20where%20translation%20steps%20lead%20to%20errors%20that%20would%20have%20been%20avoided%20by%20question%27s%20language%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2510.20647v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Reasoning%2520Lingua%2520Franca%253A%2520A%2520Double-Edged%2520Sword%2520for%2520Multilingual%2520AI%26entry.906535625%3DAlan%2520Saji%2520and%2520Raj%2520Dabre%2520and%2520Anoop%2520Kunchukuttan%2520and%2520Ratish%2520Puduppully%26entry.1292438233%3DLarge%2520Reasoning%2520Models%2520%2528LRMs%2529%2520achieve%2520strong%2520performance%2520on%2520mathematical%252C%2520scientific%252C%2520and%2520other%2520question-answering%2520tasks%252C%2520but%2520their%2520multilingual%2520reasoning%2520abilities%2520remain%2520underexplored.%2520When%2520presented%2520with%2520non-English%2520questions%252C%2520LRMs%2520often%2520default%2520to%2520reasoning%2520in%2520English%252C%2520raising%2520concerns%2520about%2520interpretability%2520and%2520the%2520handling%2520of%2520linguistic%2520and%2520cultural%2520nuances.%2520We%2520systematically%2520compare%2520an%2520LRM%2527s%2520reasoning%2520in%2520English%2520versus%2520the%2520language%2520of%2520the%2520question.%2520Our%2520evaluation%2520spans%2520two%2520tasks%253A%2520MGSM%2520and%2520GPQA%2520Diamond.%2520Beyond%2520measuring%2520answer%2520accuracy%252C%2520we%2520also%2520analyze%2520cognitive%2520attributes%2520in%2520the%2520reasoning%2520traces.%2520We%2520find%2520that%2520English%2520reasoning%2520traces%2520exhibit%2520a%2520substantially%2520higher%2520presence%2520of%2520these%2520cognitive%2520behaviors%252C%2520and%2520that%2520reasoning%2520in%2520English%2520generally%2520yields%2520higher%2520final-answer%2520accuracy%252C%2520with%2520the%2520performance%2520gap%2520increasing%2520as%2520tasks%2520become%2520more%2520complex.%2520However%252C%2520this%2520English-centric%2520strategy%2520is%2520susceptible%2520to%2520a%2520key%2520failure%2520mode%2520-%2520getting%2520%2522Lost%2520in%2520Translation%252C%2522%2520where%2520translation%2520steps%2520lead%2520to%2520errors%2520that%2520would%2520have%2520been%2520avoided%2520by%2520question%2527s%2520language%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.20647v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Reasoning%20Lingua%20Franca%3A%20A%20Double-Edged%20Sword%20for%20Multilingual%20AI&entry.906535625=Alan%20Saji%20and%20Raj%20Dabre%20and%20Anoop%20Kunchukuttan%20and%20Ratish%20Puduppully&entry.1292438233=Large%20Reasoning%20Models%20%28LRMs%29%20achieve%20strong%20performance%20on%20mathematical%2C%20scientific%2C%20and%20other%20question-answering%20tasks%2C%20but%20their%20multilingual%20reasoning%20abilities%20remain%20underexplored.%20When%20presented%20with%20non-English%20questions%2C%20LRMs%20often%20default%20to%20reasoning%20in%20English%2C%20raising%20concerns%20about%20interpretability%20and%20the%20handling%20of%20linguistic%20and%20cultural%20nuances.%20We%20systematically%20compare%20an%20LRM%27s%20reasoning%20in%20English%20versus%20the%20language%20of%20the%20question.%20Our%20evaluation%20spans%20two%20tasks%3A%20MGSM%20and%20GPQA%20Diamond.%20Beyond%20measuring%20answer%20accuracy%2C%20we%20also%20analyze%20cognitive%20attributes%20in%20the%20reasoning%20traces.%20We%20find%20that%20English%20reasoning%20traces%20exhibit%20a%20substantially%20higher%20presence%20of%20these%20cognitive%20behaviors%2C%20and%20that%20reasoning%20in%20English%20generally%20yields%20higher%20final-answer%20accuracy%2C%20with%20the%20performance%20gap%20increasing%20as%20tasks%20become%20more%20complex.%20However%2C%20this%20English-centric%20strategy%20is%20susceptible%20to%20a%20key%20failure%20mode%20-%20getting%20%22Lost%20in%20Translation%2C%22%20where%20translation%20steps%20lead%20to%20errors%20that%20would%20have%20been%20avoided%20by%20question%27s%20language%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2510.20647v2&entry.124074799=Read"},
{"title": "Overcoming Growth-Induced Forgetting in Task-Agnostic Continual Learning", "author": "Yuqing Zhao and Jiannong Cao and Divya Saxena and Xiaoyun Liu and Changlin Song and Bo Yuan and Julie McCann", "abstract": "In continual learning (CL), model growth enhances adaptability to new data. However, when model growth is applied improperly, especially in task-agnostic CL, where the entire grown model is used for inference, it can lead to severe degradation of learned knowledge, a problem we term growth-induced forgetting. Most existing methods that adopt model growth to improve adaptability often overlook the forgetting issue, resulting in compromised knowledge retention, making them unsuitable for task-agnostic settings. To promote both adaptability and knowledge retention with model growth, we identify the key: gradient and parameter sparsity. Introducing SparseGrow, which increases gradient sparsity through layer expansion and gradient gating to enable focused updates on parameters while preserving critical parameters, thus inhibiting forgetting. Moreover, it promotes parameter sparsity with sparse initialization and training, aiming at better control of model plasticity, improving adaptability over new data. Extensive experiments across diverse datasets, task-agnostic settings, and a large number of tasks demonstrate the necessity of controlled layer expansion and validate the effectiveness of SparseGrow in achieving high adaptability while minimizing forgetting in continual learning. By enabling model growth with sparsified gradients and parameters, SparseGrow paves the way for building scalable lifelong learning systems capable of continual adaptation with better knowledge retention.", "link": "http://arxiv.org/abs/2408.10566v5", "date": "2025-12-22", "relevancy": 2.4128, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.487}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4851}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Overcoming%20Growth-Induced%20Forgetting%20in%20Task-Agnostic%20Continual%20Learning&body=Title%3A%20Overcoming%20Growth-Induced%20Forgetting%20in%20Task-Agnostic%20Continual%20Learning%0AAuthor%3A%20Yuqing%20Zhao%20and%20Jiannong%20Cao%20and%20Divya%20Saxena%20and%20Xiaoyun%20Liu%20and%20Changlin%20Song%20and%20Bo%20Yuan%20and%20Julie%20McCann%0AAbstract%3A%20In%20continual%20learning%20%28CL%29%2C%20model%20growth%20enhances%20adaptability%20to%20new%20data.%20However%2C%20when%20model%20growth%20is%20applied%20improperly%2C%20especially%20in%20task-agnostic%20CL%2C%20where%20the%20entire%20grown%20model%20is%20used%20for%20inference%2C%20it%20can%20lead%20to%20severe%20degradation%20of%20learned%20knowledge%2C%20a%20problem%20we%20term%20growth-induced%20forgetting.%20Most%20existing%20methods%20that%20adopt%20model%20growth%20to%20improve%20adaptability%20often%20overlook%20the%20forgetting%20issue%2C%20resulting%20in%20compromised%20knowledge%20retention%2C%20making%20them%20unsuitable%20for%20task-agnostic%20settings.%20To%20promote%20both%20adaptability%20and%20knowledge%20retention%20with%20model%20growth%2C%20we%20identify%20the%20key%3A%20gradient%20and%20parameter%20sparsity.%20Introducing%20SparseGrow%2C%20which%20increases%20gradient%20sparsity%20through%20layer%20expansion%20and%20gradient%20gating%20to%20enable%20focused%20updates%20on%20parameters%20while%20preserving%20critical%20parameters%2C%20thus%20inhibiting%20forgetting.%20Moreover%2C%20it%20promotes%20parameter%20sparsity%20with%20sparse%20initialization%20and%20training%2C%20aiming%20at%20better%20control%20of%20model%20plasticity%2C%20improving%20adaptability%20over%20new%20data.%20Extensive%20experiments%20across%20diverse%20datasets%2C%20task-agnostic%20settings%2C%20and%20a%20large%20number%20of%20tasks%20demonstrate%20the%20necessity%20of%20controlled%20layer%20expansion%20and%20validate%20the%20effectiveness%20of%20SparseGrow%20in%20achieving%20high%20adaptability%20while%20minimizing%20forgetting%20in%20continual%20learning.%20By%20enabling%20model%20growth%20with%20sparsified%20gradients%20and%20parameters%2C%20SparseGrow%20paves%20the%20way%20for%20building%20scalable%20lifelong%20learning%20systems%20capable%20of%20continual%20adaptation%20with%20better%20knowledge%20retention.%0ALink%3A%20http%3A//arxiv.org/abs/2408.10566v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOvercoming%2520Growth-Induced%2520Forgetting%2520in%2520Task-Agnostic%2520Continual%2520Learning%26entry.906535625%3DYuqing%2520Zhao%2520and%2520Jiannong%2520Cao%2520and%2520Divya%2520Saxena%2520and%2520Xiaoyun%2520Liu%2520and%2520Changlin%2520Song%2520and%2520Bo%2520Yuan%2520and%2520Julie%2520McCann%26entry.1292438233%3DIn%2520continual%2520learning%2520%2528CL%2529%252C%2520model%2520growth%2520enhances%2520adaptability%2520to%2520new%2520data.%2520However%252C%2520when%2520model%2520growth%2520is%2520applied%2520improperly%252C%2520especially%2520in%2520task-agnostic%2520CL%252C%2520where%2520the%2520entire%2520grown%2520model%2520is%2520used%2520for%2520inference%252C%2520it%2520can%2520lead%2520to%2520severe%2520degradation%2520of%2520learned%2520knowledge%252C%2520a%2520problem%2520we%2520term%2520growth-induced%2520forgetting.%2520Most%2520existing%2520methods%2520that%2520adopt%2520model%2520growth%2520to%2520improve%2520adaptability%2520often%2520overlook%2520the%2520forgetting%2520issue%252C%2520resulting%2520in%2520compromised%2520knowledge%2520retention%252C%2520making%2520them%2520unsuitable%2520for%2520task-agnostic%2520settings.%2520To%2520promote%2520both%2520adaptability%2520and%2520knowledge%2520retention%2520with%2520model%2520growth%252C%2520we%2520identify%2520the%2520key%253A%2520gradient%2520and%2520parameter%2520sparsity.%2520Introducing%2520SparseGrow%252C%2520which%2520increases%2520gradient%2520sparsity%2520through%2520layer%2520expansion%2520and%2520gradient%2520gating%2520to%2520enable%2520focused%2520updates%2520on%2520parameters%2520while%2520preserving%2520critical%2520parameters%252C%2520thus%2520inhibiting%2520forgetting.%2520Moreover%252C%2520it%2520promotes%2520parameter%2520sparsity%2520with%2520sparse%2520initialization%2520and%2520training%252C%2520aiming%2520at%2520better%2520control%2520of%2520model%2520plasticity%252C%2520improving%2520adaptability%2520over%2520new%2520data.%2520Extensive%2520experiments%2520across%2520diverse%2520datasets%252C%2520task-agnostic%2520settings%252C%2520and%2520a%2520large%2520number%2520of%2520tasks%2520demonstrate%2520the%2520necessity%2520of%2520controlled%2520layer%2520expansion%2520and%2520validate%2520the%2520effectiveness%2520of%2520SparseGrow%2520in%2520achieving%2520high%2520adaptability%2520while%2520minimizing%2520forgetting%2520in%2520continual%2520learning.%2520By%2520enabling%2520model%2520growth%2520with%2520sparsified%2520gradients%2520and%2520parameters%252C%2520SparseGrow%2520paves%2520the%2520way%2520for%2520building%2520scalable%2520lifelong%2520learning%2520systems%2520capable%2520of%2520continual%2520adaptation%2520with%2520better%2520knowledge%2520retention.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.10566v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Overcoming%20Growth-Induced%20Forgetting%20in%20Task-Agnostic%20Continual%20Learning&entry.906535625=Yuqing%20Zhao%20and%20Jiannong%20Cao%20and%20Divya%20Saxena%20and%20Xiaoyun%20Liu%20and%20Changlin%20Song%20and%20Bo%20Yuan%20and%20Julie%20McCann&entry.1292438233=In%20continual%20learning%20%28CL%29%2C%20model%20growth%20enhances%20adaptability%20to%20new%20data.%20However%2C%20when%20model%20growth%20is%20applied%20improperly%2C%20especially%20in%20task-agnostic%20CL%2C%20where%20the%20entire%20grown%20model%20is%20used%20for%20inference%2C%20it%20can%20lead%20to%20severe%20degradation%20of%20learned%20knowledge%2C%20a%20problem%20we%20term%20growth-induced%20forgetting.%20Most%20existing%20methods%20that%20adopt%20model%20growth%20to%20improve%20adaptability%20often%20overlook%20the%20forgetting%20issue%2C%20resulting%20in%20compromised%20knowledge%20retention%2C%20making%20them%20unsuitable%20for%20task-agnostic%20settings.%20To%20promote%20both%20adaptability%20and%20knowledge%20retention%20with%20model%20growth%2C%20we%20identify%20the%20key%3A%20gradient%20and%20parameter%20sparsity.%20Introducing%20SparseGrow%2C%20which%20increases%20gradient%20sparsity%20through%20layer%20expansion%20and%20gradient%20gating%20to%20enable%20focused%20updates%20on%20parameters%20while%20preserving%20critical%20parameters%2C%20thus%20inhibiting%20forgetting.%20Moreover%2C%20it%20promotes%20parameter%20sparsity%20with%20sparse%20initialization%20and%20training%2C%20aiming%20at%20better%20control%20of%20model%20plasticity%2C%20improving%20adaptability%20over%20new%20data.%20Extensive%20experiments%20across%20diverse%20datasets%2C%20task-agnostic%20settings%2C%20and%20a%20large%20number%20of%20tasks%20demonstrate%20the%20necessity%20of%20controlled%20layer%20expansion%20and%20validate%20the%20effectiveness%20of%20SparseGrow%20in%20achieving%20high%20adaptability%20while%20minimizing%20forgetting%20in%20continual%20learning.%20By%20enabling%20model%20growth%20with%20sparsified%20gradients%20and%20parameters%2C%20SparseGrow%20paves%20the%20way%20for%20building%20scalable%20lifelong%20learning%20systems%20capable%20of%20continual%20adaptation%20with%20better%20knowledge%20retention.&entry.1838667208=http%3A//arxiv.org/abs/2408.10566v5&entry.124074799=Read"},
{"title": "Activations as Features: Probing LLMs for Generalizable Essay Scoring Representations", "author": "Jinwei Chi and Ke Wang and Yu Chen and Xuanye Lin and Qiang Xu", "abstract": "Automated essay scoring (AES) is a challenging task in cross-prompt settings due to the diversity of scoring criteria. While previous studies have focused on the output of large language models (LLMs) to improve scoring accuracy, we believe activations from intermediate layers may also provide valuable information. To explore this possibility, we evaluated the discriminative power of LLMs' activations in cross-prompt essay scoring task. Specifically, we used activations to fit probes and further analyzed the effects of different models and input content of LLMs on this discriminative power. By computing the directions of essays across various trait dimensions under different prompts, we analyzed the variation in evaluation perspectives of large language models concerning essay types and traits. Results show that the activations possess strong discriminative power in evaluating essay quality and that LLMs can adapt their evaluation perspectives to different traits and essay types, effectively handling the diversity of scoring criteria in cross-prompt settings.", "link": "http://arxiv.org/abs/2512.19456v1", "date": "2025-12-22", "relevancy": 2.3963, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5034}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5034}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Activations%20as%20Features%3A%20Probing%20LLMs%20for%20Generalizable%20Essay%20Scoring%20Representations&body=Title%3A%20Activations%20as%20Features%3A%20Probing%20LLMs%20for%20Generalizable%20Essay%20Scoring%20Representations%0AAuthor%3A%20Jinwei%20Chi%20and%20Ke%20Wang%20and%20Yu%20Chen%20and%20Xuanye%20Lin%20and%20Qiang%20Xu%0AAbstract%3A%20Automated%20essay%20scoring%20%28AES%29%20is%20a%20challenging%20task%20in%20cross-prompt%20settings%20due%20to%20the%20diversity%20of%20scoring%20criteria.%20While%20previous%20studies%20have%20focused%20on%20the%20output%20of%20large%20language%20models%20%28LLMs%29%20to%20improve%20scoring%20accuracy%2C%20we%20believe%20activations%20from%20intermediate%20layers%20may%20also%20provide%20valuable%20information.%20To%20explore%20this%20possibility%2C%20we%20evaluated%20the%20discriminative%20power%20of%20LLMs%27%20activations%20in%20cross-prompt%20essay%20scoring%20task.%20Specifically%2C%20we%20used%20activations%20to%20fit%20probes%20and%20further%20analyzed%20the%20effects%20of%20different%20models%20and%20input%20content%20of%20LLMs%20on%20this%20discriminative%20power.%20By%20computing%20the%20directions%20of%20essays%20across%20various%20trait%20dimensions%20under%20different%20prompts%2C%20we%20analyzed%20the%20variation%20in%20evaluation%20perspectives%20of%20large%20language%20models%20concerning%20essay%20types%20and%20traits.%20Results%20show%20that%20the%20activations%20possess%20strong%20discriminative%20power%20in%20evaluating%20essay%20quality%20and%20that%20LLMs%20can%20adapt%20their%20evaluation%20perspectives%20to%20different%20traits%20and%20essay%20types%2C%20effectively%20handling%20the%20diversity%20of%20scoring%20criteria%20in%20cross-prompt%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19456v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActivations%2520as%2520Features%253A%2520Probing%2520LLMs%2520for%2520Generalizable%2520Essay%2520Scoring%2520Representations%26entry.906535625%3DJinwei%2520Chi%2520and%2520Ke%2520Wang%2520and%2520Yu%2520Chen%2520and%2520Xuanye%2520Lin%2520and%2520Qiang%2520Xu%26entry.1292438233%3DAutomated%2520essay%2520scoring%2520%2528AES%2529%2520is%2520a%2520challenging%2520task%2520in%2520cross-prompt%2520settings%2520due%2520to%2520the%2520diversity%2520of%2520scoring%2520criteria.%2520While%2520previous%2520studies%2520have%2520focused%2520on%2520the%2520output%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520improve%2520scoring%2520accuracy%252C%2520we%2520believe%2520activations%2520from%2520intermediate%2520layers%2520may%2520also%2520provide%2520valuable%2520information.%2520To%2520explore%2520this%2520possibility%252C%2520we%2520evaluated%2520the%2520discriminative%2520power%2520of%2520LLMs%2527%2520activations%2520in%2520cross-prompt%2520essay%2520scoring%2520task.%2520Specifically%252C%2520we%2520used%2520activations%2520to%2520fit%2520probes%2520and%2520further%2520analyzed%2520the%2520effects%2520of%2520different%2520models%2520and%2520input%2520content%2520of%2520LLMs%2520on%2520this%2520discriminative%2520power.%2520By%2520computing%2520the%2520directions%2520of%2520essays%2520across%2520various%2520trait%2520dimensions%2520under%2520different%2520prompts%252C%2520we%2520analyzed%2520the%2520variation%2520in%2520evaluation%2520perspectives%2520of%2520large%2520language%2520models%2520concerning%2520essay%2520types%2520and%2520traits.%2520Results%2520show%2520that%2520the%2520activations%2520possess%2520strong%2520discriminative%2520power%2520in%2520evaluating%2520essay%2520quality%2520and%2520that%2520LLMs%2520can%2520adapt%2520their%2520evaluation%2520perspectives%2520to%2520different%2520traits%2520and%2520essay%2520types%252C%2520effectively%2520handling%2520the%2520diversity%2520of%2520scoring%2520criteria%2520in%2520cross-prompt%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19456v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Activations%20as%20Features%3A%20Probing%20LLMs%20for%20Generalizable%20Essay%20Scoring%20Representations&entry.906535625=Jinwei%20Chi%20and%20Ke%20Wang%20and%20Yu%20Chen%20and%20Xuanye%20Lin%20and%20Qiang%20Xu&entry.1292438233=Automated%20essay%20scoring%20%28AES%29%20is%20a%20challenging%20task%20in%20cross-prompt%20settings%20due%20to%20the%20diversity%20of%20scoring%20criteria.%20While%20previous%20studies%20have%20focused%20on%20the%20output%20of%20large%20language%20models%20%28LLMs%29%20to%20improve%20scoring%20accuracy%2C%20we%20believe%20activations%20from%20intermediate%20layers%20may%20also%20provide%20valuable%20information.%20To%20explore%20this%20possibility%2C%20we%20evaluated%20the%20discriminative%20power%20of%20LLMs%27%20activations%20in%20cross-prompt%20essay%20scoring%20task.%20Specifically%2C%20we%20used%20activations%20to%20fit%20probes%20and%20further%20analyzed%20the%20effects%20of%20different%20models%20and%20input%20content%20of%20LLMs%20on%20this%20discriminative%20power.%20By%20computing%20the%20directions%20of%20essays%20across%20various%20trait%20dimensions%20under%20different%20prompts%2C%20we%20analyzed%20the%20variation%20in%20evaluation%20perspectives%20of%20large%20language%20models%20concerning%20essay%20types%20and%20traits.%20Results%20show%20that%20the%20activations%20possess%20strong%20discriminative%20power%20in%20evaluating%20essay%20quality%20and%20that%20LLMs%20can%20adapt%20their%20evaluation%20perspectives%20to%20different%20traits%20and%20essay%20types%2C%20effectively%20handling%20the%20diversity%20of%20scoring%20criteria%20in%20cross-prompt%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2512.19456v1&entry.124074799=Read"},
{"title": "LoGoPlanner: Localization Grounded Navigation Policy with Metric-aware Visual Geometry", "author": "Jiaqi Peng and Wenzhe Cai and Yuqiang Yang and Tai Wang and Yuan Shen and Jiangmiao Pang", "abstract": "Trajectory planning in unstructured environments is a fundamental and challenging capability for mobile robots. Traditional modular pipelines suffer from latency and cascading errors across perception, localization, mapping, and planning modules. Recent end-to-end learning methods map raw visual observations directly to control signals or trajectories, promising greater performance and efficiency in open-world settings. However, most prior end-to-end approaches still rely on separate localization modules that depend on accurate sensor extrinsic calibration for self-state estimation, thereby limiting generalization across embodiments and environments. We introduce LoGoPlanner, a localization-grounded, end-to-end navigation framework that addresses these limitations by: (1) finetuning a long-horizon visual-geometry backbone to ground predictions with absolute metric scale, thereby providing implicit state estimation for accurate localization; (2) reconstructing surrounding scene geometry from historical observations to supply dense, fine-grained environmental awareness for reliable obstacle avoidance; and (3) conditioning the policy on implicit geometry bootstrapped by the aforementioned auxiliary tasks, thereby reducing error propagation.We evaluate LoGoPlanner in both simulation and real-world settings, where its fully end-to-end design reduces cumulative error while metric-aware geometry memory enhances planning consistency and obstacle avoidance, leading to more than a 27.3\\% improvement over oracle-localization baselines and strong generalization across embodiments and environments. The code and models have been made publicly available on the \\href{https://steinate.github.io/logoplanner.github.io/}{project page}.", "link": "http://arxiv.org/abs/2512.19629v1", "date": "2025-12-22", "relevancy": 2.3843, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6218}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5805}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LoGoPlanner%3A%20Localization%20Grounded%20Navigation%20Policy%20with%20Metric-aware%20Visual%20Geometry&body=Title%3A%20LoGoPlanner%3A%20Localization%20Grounded%20Navigation%20Policy%20with%20Metric-aware%20Visual%20Geometry%0AAuthor%3A%20Jiaqi%20Peng%20and%20Wenzhe%20Cai%20and%20Yuqiang%20Yang%20and%20Tai%20Wang%20and%20Yuan%20Shen%20and%20Jiangmiao%20Pang%0AAbstract%3A%20Trajectory%20planning%20in%20unstructured%20environments%20is%20a%20fundamental%20and%20challenging%20capability%20for%20mobile%20robots.%20Traditional%20modular%20pipelines%20suffer%20from%20latency%20and%20cascading%20errors%20across%20perception%2C%20localization%2C%20mapping%2C%20and%20planning%20modules.%20Recent%20end-to-end%20learning%20methods%20map%20raw%20visual%20observations%20directly%20to%20control%20signals%20or%20trajectories%2C%20promising%20greater%20performance%20and%20efficiency%20in%20open-world%20settings.%20However%2C%20most%20prior%20end-to-end%20approaches%20still%20rely%20on%20separate%20localization%20modules%20that%20depend%20on%20accurate%20sensor%20extrinsic%20calibration%20for%20self-state%20estimation%2C%20thereby%20limiting%20generalization%20across%20embodiments%20and%20environments.%20We%20introduce%20LoGoPlanner%2C%20a%20localization-grounded%2C%20end-to-end%20navigation%20framework%20that%20addresses%20these%20limitations%20by%3A%20%281%29%20finetuning%20a%20long-horizon%20visual-geometry%20backbone%20to%20ground%20predictions%20with%20absolute%20metric%20scale%2C%20thereby%20providing%20implicit%20state%20estimation%20for%20accurate%20localization%3B%20%282%29%20reconstructing%20surrounding%20scene%20geometry%20from%20historical%20observations%20to%20supply%20dense%2C%20fine-grained%20environmental%20awareness%20for%20reliable%20obstacle%20avoidance%3B%20and%20%283%29%20conditioning%20the%20policy%20on%20implicit%20geometry%20bootstrapped%20by%20the%20aforementioned%20auxiliary%20tasks%2C%20thereby%20reducing%20error%20propagation.We%20evaluate%20LoGoPlanner%20in%20both%20simulation%20and%20real-world%20settings%2C%20where%20its%20fully%20end-to-end%20design%20reduces%20cumulative%20error%20while%20metric-aware%20geometry%20memory%20enhances%20planning%20consistency%20and%20obstacle%20avoidance%2C%20leading%20to%20more%20than%20a%2027.3%5C%25%20improvement%20over%20oracle-localization%20baselines%20and%20strong%20generalization%20across%20embodiments%20and%20environments.%20The%20code%20and%20models%20have%20been%20made%20publicly%20available%20on%20the%20%5Chref%7Bhttps%3A//steinate.github.io/logoplanner.github.io/%7D%7Bproject%20page%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19629v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoGoPlanner%253A%2520Localization%2520Grounded%2520Navigation%2520Policy%2520with%2520Metric-aware%2520Visual%2520Geometry%26entry.906535625%3DJiaqi%2520Peng%2520and%2520Wenzhe%2520Cai%2520and%2520Yuqiang%2520Yang%2520and%2520Tai%2520Wang%2520and%2520Yuan%2520Shen%2520and%2520Jiangmiao%2520Pang%26entry.1292438233%3DTrajectory%2520planning%2520in%2520unstructured%2520environments%2520is%2520a%2520fundamental%2520and%2520challenging%2520capability%2520for%2520mobile%2520robots.%2520Traditional%2520modular%2520pipelines%2520suffer%2520from%2520latency%2520and%2520cascading%2520errors%2520across%2520perception%252C%2520localization%252C%2520mapping%252C%2520and%2520planning%2520modules.%2520Recent%2520end-to-end%2520learning%2520methods%2520map%2520raw%2520visual%2520observations%2520directly%2520to%2520control%2520signals%2520or%2520trajectories%252C%2520promising%2520greater%2520performance%2520and%2520efficiency%2520in%2520open-world%2520settings.%2520However%252C%2520most%2520prior%2520end-to-end%2520approaches%2520still%2520rely%2520on%2520separate%2520localization%2520modules%2520that%2520depend%2520on%2520accurate%2520sensor%2520extrinsic%2520calibration%2520for%2520self-state%2520estimation%252C%2520thereby%2520limiting%2520generalization%2520across%2520embodiments%2520and%2520environments.%2520We%2520introduce%2520LoGoPlanner%252C%2520a%2520localization-grounded%252C%2520end-to-end%2520navigation%2520framework%2520that%2520addresses%2520these%2520limitations%2520by%253A%2520%25281%2529%2520finetuning%2520a%2520long-horizon%2520visual-geometry%2520backbone%2520to%2520ground%2520predictions%2520with%2520absolute%2520metric%2520scale%252C%2520thereby%2520providing%2520implicit%2520state%2520estimation%2520for%2520accurate%2520localization%253B%2520%25282%2529%2520reconstructing%2520surrounding%2520scene%2520geometry%2520from%2520historical%2520observations%2520to%2520supply%2520dense%252C%2520fine-grained%2520environmental%2520awareness%2520for%2520reliable%2520obstacle%2520avoidance%253B%2520and%2520%25283%2529%2520conditioning%2520the%2520policy%2520on%2520implicit%2520geometry%2520bootstrapped%2520by%2520the%2520aforementioned%2520auxiliary%2520tasks%252C%2520thereby%2520reducing%2520error%2520propagation.We%2520evaluate%2520LoGoPlanner%2520in%2520both%2520simulation%2520and%2520real-world%2520settings%252C%2520where%2520its%2520fully%2520end-to-end%2520design%2520reduces%2520cumulative%2520error%2520while%2520metric-aware%2520geometry%2520memory%2520enhances%2520planning%2520consistency%2520and%2520obstacle%2520avoidance%252C%2520leading%2520to%2520more%2520than%2520a%252027.3%255C%2525%2520improvement%2520over%2520oracle-localization%2520baselines%2520and%2520strong%2520generalization%2520across%2520embodiments%2520and%2520environments.%2520The%2520code%2520and%2520models%2520have%2520been%2520made%2520publicly%2520available%2520on%2520the%2520%255Chref%257Bhttps%253A//steinate.github.io/logoplanner.github.io/%257D%257Bproject%2520page%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19629v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoGoPlanner%3A%20Localization%20Grounded%20Navigation%20Policy%20with%20Metric-aware%20Visual%20Geometry&entry.906535625=Jiaqi%20Peng%20and%20Wenzhe%20Cai%20and%20Yuqiang%20Yang%20and%20Tai%20Wang%20and%20Yuan%20Shen%20and%20Jiangmiao%20Pang&entry.1292438233=Trajectory%20planning%20in%20unstructured%20environments%20is%20a%20fundamental%20and%20challenging%20capability%20for%20mobile%20robots.%20Traditional%20modular%20pipelines%20suffer%20from%20latency%20and%20cascading%20errors%20across%20perception%2C%20localization%2C%20mapping%2C%20and%20planning%20modules.%20Recent%20end-to-end%20learning%20methods%20map%20raw%20visual%20observations%20directly%20to%20control%20signals%20or%20trajectories%2C%20promising%20greater%20performance%20and%20efficiency%20in%20open-world%20settings.%20However%2C%20most%20prior%20end-to-end%20approaches%20still%20rely%20on%20separate%20localization%20modules%20that%20depend%20on%20accurate%20sensor%20extrinsic%20calibration%20for%20self-state%20estimation%2C%20thereby%20limiting%20generalization%20across%20embodiments%20and%20environments.%20We%20introduce%20LoGoPlanner%2C%20a%20localization-grounded%2C%20end-to-end%20navigation%20framework%20that%20addresses%20these%20limitations%20by%3A%20%281%29%20finetuning%20a%20long-horizon%20visual-geometry%20backbone%20to%20ground%20predictions%20with%20absolute%20metric%20scale%2C%20thereby%20providing%20implicit%20state%20estimation%20for%20accurate%20localization%3B%20%282%29%20reconstructing%20surrounding%20scene%20geometry%20from%20historical%20observations%20to%20supply%20dense%2C%20fine-grained%20environmental%20awareness%20for%20reliable%20obstacle%20avoidance%3B%20and%20%283%29%20conditioning%20the%20policy%20on%20implicit%20geometry%20bootstrapped%20by%20the%20aforementioned%20auxiliary%20tasks%2C%20thereby%20reducing%20error%20propagation.We%20evaluate%20LoGoPlanner%20in%20both%20simulation%20and%20real-world%20settings%2C%20where%20its%20fully%20end-to-end%20design%20reduces%20cumulative%20error%20while%20metric-aware%20geometry%20memory%20enhances%20planning%20consistency%20and%20obstacle%20avoidance%2C%20leading%20to%20more%20than%20a%2027.3%5C%25%20improvement%20over%20oracle-localization%20baselines%20and%20strong%20generalization%20across%20embodiments%20and%20environments.%20The%20code%20and%20models%20have%20been%20made%20publicly%20available%20on%20the%20%5Chref%7Bhttps%3A//steinate.github.io/logoplanner.github.io/%7D%7Bproject%20page%7D.&entry.1838667208=http%3A//arxiv.org/abs/2512.19629v1&entry.124074799=Read"},
{"title": "Helios: A Foundational Language Model for Smart Energy Knowledge Reasoning and Application", "author": "Haoyu Jiang and Fanjie Zeng and Boan Qu and Xiaojie Lin and Wei Zhong", "abstract": "In the global drive toward carbon neutrality, deeply coordinated smart energy systems underpin industrial transformation. However, the interdisciplinary, fragmented, and fast-evolving expertise in this domain prevents general-purpose LLMs, which lack domain knowledge and physical-constraint awareness, from delivering precise engineering-aligned inference and generation. To address these challenges, we introduce Helios, a large language model tailored to the smart energy domain, together with a comprehensive suite of resources to advance LLM research in this field. Specifically, we develop Enersys, a multi-agent collaborative framework for end-to-end dataset construction, through which we produce: (1) a smart energy knowledge base, EnerBase, to enrich the model's foundational expertise; (2) an instruction fine-tuning dataset, EnerInstruct, to strengthen performance on domain-specific downstream tasks; and (3) an RLHF dataset, EnerReinforce, to align the model with human preferences and industry standards. Leveraging these resources, Helios undergoes large-scale pretraining, SFT, and RLHF. We also release EnerBench, a benchmark for evaluating LLMs in smart energy scenarios, and demonstrate that our approach significantly enhances domain knowledge mastery, task execution accuracy, and alignment with human preferences.", "link": "http://arxiv.org/abs/2512.19299v1", "date": "2025-12-22", "relevancy": 2.3614, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4871}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4871}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Helios%3A%20A%20Foundational%20Language%20Model%20for%20Smart%20Energy%20Knowledge%20Reasoning%20and%20Application&body=Title%3A%20Helios%3A%20A%20Foundational%20Language%20Model%20for%20Smart%20Energy%20Knowledge%20Reasoning%20and%20Application%0AAuthor%3A%20Haoyu%20Jiang%20and%20Fanjie%20Zeng%20and%20Boan%20Qu%20and%20Xiaojie%20Lin%20and%20Wei%20Zhong%0AAbstract%3A%20In%20the%20global%20drive%20toward%20carbon%20neutrality%2C%20deeply%20coordinated%20smart%20energy%20systems%20underpin%20industrial%20transformation.%20However%2C%20the%20interdisciplinary%2C%20fragmented%2C%20and%20fast-evolving%20expertise%20in%20this%20domain%20prevents%20general-purpose%20LLMs%2C%20which%20lack%20domain%20knowledge%20and%20physical-constraint%20awareness%2C%20from%20delivering%20precise%20engineering-aligned%20inference%20and%20generation.%20To%20address%20these%20challenges%2C%20we%20introduce%20Helios%2C%20a%20large%20language%20model%20tailored%20to%20the%20smart%20energy%20domain%2C%20together%20with%20a%20comprehensive%20suite%20of%20resources%20to%20advance%20LLM%20research%20in%20this%20field.%20Specifically%2C%20we%20develop%20Enersys%2C%20a%20multi-agent%20collaborative%20framework%20for%20end-to-end%20dataset%20construction%2C%20through%20which%20we%20produce%3A%20%281%29%20a%20smart%20energy%20knowledge%20base%2C%20EnerBase%2C%20to%20enrich%20the%20model%27s%20foundational%20expertise%3B%20%282%29%20an%20instruction%20fine-tuning%20dataset%2C%20EnerInstruct%2C%20to%20strengthen%20performance%20on%20domain-specific%20downstream%20tasks%3B%20and%20%283%29%20an%20RLHF%20dataset%2C%20EnerReinforce%2C%20to%20align%20the%20model%20with%20human%20preferences%20and%20industry%20standards.%20Leveraging%20these%20resources%2C%20Helios%20undergoes%20large-scale%20pretraining%2C%20SFT%2C%20and%20RLHF.%20We%20also%20release%20EnerBench%2C%20a%20benchmark%20for%20evaluating%20LLMs%20in%20smart%20energy%20scenarios%2C%20and%20demonstrate%20that%20our%20approach%20significantly%20enhances%20domain%20knowledge%20mastery%2C%20task%20execution%20accuracy%2C%20and%20alignment%20with%20human%20preferences.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19299v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHelios%253A%2520A%2520Foundational%2520Language%2520Model%2520for%2520Smart%2520Energy%2520Knowledge%2520Reasoning%2520and%2520Application%26entry.906535625%3DHaoyu%2520Jiang%2520and%2520Fanjie%2520Zeng%2520and%2520Boan%2520Qu%2520and%2520Xiaojie%2520Lin%2520and%2520Wei%2520Zhong%26entry.1292438233%3DIn%2520the%2520global%2520drive%2520toward%2520carbon%2520neutrality%252C%2520deeply%2520coordinated%2520smart%2520energy%2520systems%2520underpin%2520industrial%2520transformation.%2520However%252C%2520the%2520interdisciplinary%252C%2520fragmented%252C%2520and%2520fast-evolving%2520expertise%2520in%2520this%2520domain%2520prevents%2520general-purpose%2520LLMs%252C%2520which%2520lack%2520domain%2520knowledge%2520and%2520physical-constraint%2520awareness%252C%2520from%2520delivering%2520precise%2520engineering-aligned%2520inference%2520and%2520generation.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%2520Helios%252C%2520a%2520large%2520language%2520model%2520tailored%2520to%2520the%2520smart%2520energy%2520domain%252C%2520together%2520with%2520a%2520comprehensive%2520suite%2520of%2520resources%2520to%2520advance%2520LLM%2520research%2520in%2520this%2520field.%2520Specifically%252C%2520we%2520develop%2520Enersys%252C%2520a%2520multi-agent%2520collaborative%2520framework%2520for%2520end-to-end%2520dataset%2520construction%252C%2520through%2520which%2520we%2520produce%253A%2520%25281%2529%2520a%2520smart%2520energy%2520knowledge%2520base%252C%2520EnerBase%252C%2520to%2520enrich%2520the%2520model%2527s%2520foundational%2520expertise%253B%2520%25282%2529%2520an%2520instruction%2520fine-tuning%2520dataset%252C%2520EnerInstruct%252C%2520to%2520strengthen%2520performance%2520on%2520domain-specific%2520downstream%2520tasks%253B%2520and%2520%25283%2529%2520an%2520RLHF%2520dataset%252C%2520EnerReinforce%252C%2520to%2520align%2520the%2520model%2520with%2520human%2520preferences%2520and%2520industry%2520standards.%2520Leveraging%2520these%2520resources%252C%2520Helios%2520undergoes%2520large-scale%2520pretraining%252C%2520SFT%252C%2520and%2520RLHF.%2520We%2520also%2520release%2520EnerBench%252C%2520a%2520benchmark%2520for%2520evaluating%2520LLMs%2520in%2520smart%2520energy%2520scenarios%252C%2520and%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520enhances%2520domain%2520knowledge%2520mastery%252C%2520task%2520execution%2520accuracy%252C%2520and%2520alignment%2520with%2520human%2520preferences.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19299v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Helios%3A%20A%20Foundational%20Language%20Model%20for%20Smart%20Energy%20Knowledge%20Reasoning%20and%20Application&entry.906535625=Haoyu%20Jiang%20and%20Fanjie%20Zeng%20and%20Boan%20Qu%20and%20Xiaojie%20Lin%20and%20Wei%20Zhong&entry.1292438233=In%20the%20global%20drive%20toward%20carbon%20neutrality%2C%20deeply%20coordinated%20smart%20energy%20systems%20underpin%20industrial%20transformation.%20However%2C%20the%20interdisciplinary%2C%20fragmented%2C%20and%20fast-evolving%20expertise%20in%20this%20domain%20prevents%20general-purpose%20LLMs%2C%20which%20lack%20domain%20knowledge%20and%20physical-constraint%20awareness%2C%20from%20delivering%20precise%20engineering-aligned%20inference%20and%20generation.%20To%20address%20these%20challenges%2C%20we%20introduce%20Helios%2C%20a%20large%20language%20model%20tailored%20to%20the%20smart%20energy%20domain%2C%20together%20with%20a%20comprehensive%20suite%20of%20resources%20to%20advance%20LLM%20research%20in%20this%20field.%20Specifically%2C%20we%20develop%20Enersys%2C%20a%20multi-agent%20collaborative%20framework%20for%20end-to-end%20dataset%20construction%2C%20through%20which%20we%20produce%3A%20%281%29%20a%20smart%20energy%20knowledge%20base%2C%20EnerBase%2C%20to%20enrich%20the%20model%27s%20foundational%20expertise%3B%20%282%29%20an%20instruction%20fine-tuning%20dataset%2C%20EnerInstruct%2C%20to%20strengthen%20performance%20on%20domain-specific%20downstream%20tasks%3B%20and%20%283%29%20an%20RLHF%20dataset%2C%20EnerReinforce%2C%20to%20align%20the%20model%20with%20human%20preferences%20and%20industry%20standards.%20Leveraging%20these%20resources%2C%20Helios%20undergoes%20large-scale%20pretraining%2C%20SFT%2C%20and%20RLHF.%20We%20also%20release%20EnerBench%2C%20a%20benchmark%20for%20evaluating%20LLMs%20in%20smart%20energy%20scenarios%2C%20and%20demonstrate%20that%20our%20approach%20significantly%20enhances%20domain%20knowledge%20mastery%2C%20task%20execution%20accuracy%2C%20and%20alignment%20with%20human%20preferences.&entry.1838667208=http%3A//arxiv.org/abs/2512.19299v1&entry.124074799=Read"},
{"title": "GLUE: Generative Latent Unification of Expertise-Informed Engineering Models", "author": "Tim Aebersold and Soheyl Massoudi and Mark D. Fuge", "abstract": "Engineering complex systems (aircraft, buildings, vehicles) requires accounting for geometric and performance couplings across subsystems. As generative models proliferate for specialized domains (wings, structures, engines), a key research gap is how to coordinate frozen, pre-trained submodels to generate full-system designs that are feasible, diverse, and high-performing. We introduce Generative Latent Unification of Expertise-Informed Engineering Models (GLUE), which orchestrates pre-trained, frozen subsystem generators while enforcing system-level feasibility, optimality, and diversity. We propose and benchmark (i) data-driven GLUE models trained on pre-generated system-level designs and (ii) a data-free GLUE model trained online on a differentiable geometry layer. On a UAV design problem with five coupling constraints, we find that data-driven approaches yield diverse, high-performing designs but require large datasets to satisfy constraints reliably. The data-free approach is competitive with Bayesian optimization and gradient-based optimization in performance and feasibility while training a full generative model in only 10 min on a RTX 4090 GPU, requiring more than two orders of magnitude fewer geometry evaluations and FLOPs than the data-driven method. Ablations focused on data-free training show that subsystem output continuity affects coordination, and equality constraints can trigger mode collapse unless mitigated. By integrating unmodified, domain-informed submodels into a modular generative workflow, this work provides a viable path for scaling generative design to complex, real-world engineering systems.", "link": "http://arxiv.org/abs/2512.19469v1", "date": "2025-12-22", "relevancy": 2.3544, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6191}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5792}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GLUE%3A%20Generative%20Latent%20Unification%20of%20Expertise-Informed%20Engineering%20Models&body=Title%3A%20GLUE%3A%20Generative%20Latent%20Unification%20of%20Expertise-Informed%20Engineering%20Models%0AAuthor%3A%20Tim%20Aebersold%20and%20Soheyl%20Massoudi%20and%20Mark%20D.%20Fuge%0AAbstract%3A%20Engineering%20complex%20systems%20%28aircraft%2C%20buildings%2C%20vehicles%29%20requires%20accounting%20for%20geometric%20and%20performance%20couplings%20across%20subsystems.%20As%20generative%20models%20proliferate%20for%20specialized%20domains%20%28wings%2C%20structures%2C%20engines%29%2C%20a%20key%20research%20gap%20is%20how%20to%20coordinate%20frozen%2C%20pre-trained%20submodels%20to%20generate%20full-system%20designs%20that%20are%20feasible%2C%20diverse%2C%20and%20high-performing.%20We%20introduce%20Generative%20Latent%20Unification%20of%20Expertise-Informed%20Engineering%20Models%20%28GLUE%29%2C%20which%20orchestrates%20pre-trained%2C%20frozen%20subsystem%20generators%20while%20enforcing%20system-level%20feasibility%2C%20optimality%2C%20and%20diversity.%20We%20propose%20and%20benchmark%20%28i%29%20data-driven%20GLUE%20models%20trained%20on%20pre-generated%20system-level%20designs%20and%20%28ii%29%20a%20data-free%20GLUE%20model%20trained%20online%20on%20a%20differentiable%20geometry%20layer.%20On%20a%20UAV%20design%20problem%20with%20five%20coupling%20constraints%2C%20we%20find%20that%20data-driven%20approaches%20yield%20diverse%2C%20high-performing%20designs%20but%20require%20large%20datasets%20to%20satisfy%20constraints%20reliably.%20The%20data-free%20approach%20is%20competitive%20with%20Bayesian%20optimization%20and%20gradient-based%20optimization%20in%20performance%20and%20feasibility%20while%20training%20a%20full%20generative%20model%20in%20only%2010%20min%20on%20a%20RTX%204090%20GPU%2C%20requiring%20more%20than%20two%20orders%20of%20magnitude%20fewer%20geometry%20evaluations%20and%20FLOPs%20than%20the%20data-driven%20method.%20Ablations%20focused%20on%20data-free%20training%20show%20that%20subsystem%20output%20continuity%20affects%20coordination%2C%20and%20equality%20constraints%20can%20trigger%20mode%20collapse%20unless%20mitigated.%20By%20integrating%20unmodified%2C%20domain-informed%20submodels%20into%20a%20modular%20generative%20workflow%2C%20this%20work%20provides%20a%20viable%20path%20for%20scaling%20generative%20design%20to%20complex%2C%20real-world%20engineering%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19469v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGLUE%253A%2520Generative%2520Latent%2520Unification%2520of%2520Expertise-Informed%2520Engineering%2520Models%26entry.906535625%3DTim%2520Aebersold%2520and%2520Soheyl%2520Massoudi%2520and%2520Mark%2520D.%2520Fuge%26entry.1292438233%3DEngineering%2520complex%2520systems%2520%2528aircraft%252C%2520buildings%252C%2520vehicles%2529%2520requires%2520accounting%2520for%2520geometric%2520and%2520performance%2520couplings%2520across%2520subsystems.%2520As%2520generative%2520models%2520proliferate%2520for%2520specialized%2520domains%2520%2528wings%252C%2520structures%252C%2520engines%2529%252C%2520a%2520key%2520research%2520gap%2520is%2520how%2520to%2520coordinate%2520frozen%252C%2520pre-trained%2520submodels%2520to%2520generate%2520full-system%2520designs%2520that%2520are%2520feasible%252C%2520diverse%252C%2520and%2520high-performing.%2520We%2520introduce%2520Generative%2520Latent%2520Unification%2520of%2520Expertise-Informed%2520Engineering%2520Models%2520%2528GLUE%2529%252C%2520which%2520orchestrates%2520pre-trained%252C%2520frozen%2520subsystem%2520generators%2520while%2520enforcing%2520system-level%2520feasibility%252C%2520optimality%252C%2520and%2520diversity.%2520We%2520propose%2520and%2520benchmark%2520%2528i%2529%2520data-driven%2520GLUE%2520models%2520trained%2520on%2520pre-generated%2520system-level%2520designs%2520and%2520%2528ii%2529%2520a%2520data-free%2520GLUE%2520model%2520trained%2520online%2520on%2520a%2520differentiable%2520geometry%2520layer.%2520On%2520a%2520UAV%2520design%2520problem%2520with%2520five%2520coupling%2520constraints%252C%2520we%2520find%2520that%2520data-driven%2520approaches%2520yield%2520diverse%252C%2520high-performing%2520designs%2520but%2520require%2520large%2520datasets%2520to%2520satisfy%2520constraints%2520reliably.%2520The%2520data-free%2520approach%2520is%2520competitive%2520with%2520Bayesian%2520optimization%2520and%2520gradient-based%2520optimization%2520in%2520performance%2520and%2520feasibility%2520while%2520training%2520a%2520full%2520generative%2520model%2520in%2520only%252010%2520min%2520on%2520a%2520RTX%25204090%2520GPU%252C%2520requiring%2520more%2520than%2520two%2520orders%2520of%2520magnitude%2520fewer%2520geometry%2520evaluations%2520and%2520FLOPs%2520than%2520the%2520data-driven%2520method.%2520Ablations%2520focused%2520on%2520data-free%2520training%2520show%2520that%2520subsystem%2520output%2520continuity%2520affects%2520coordination%252C%2520and%2520equality%2520constraints%2520can%2520trigger%2520mode%2520collapse%2520unless%2520mitigated.%2520By%2520integrating%2520unmodified%252C%2520domain-informed%2520submodels%2520into%2520a%2520modular%2520generative%2520workflow%252C%2520this%2520work%2520provides%2520a%2520viable%2520path%2520for%2520scaling%2520generative%2520design%2520to%2520complex%252C%2520real-world%2520engineering%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19469v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GLUE%3A%20Generative%20Latent%20Unification%20of%20Expertise-Informed%20Engineering%20Models&entry.906535625=Tim%20Aebersold%20and%20Soheyl%20Massoudi%20and%20Mark%20D.%20Fuge&entry.1292438233=Engineering%20complex%20systems%20%28aircraft%2C%20buildings%2C%20vehicles%29%20requires%20accounting%20for%20geometric%20and%20performance%20couplings%20across%20subsystems.%20As%20generative%20models%20proliferate%20for%20specialized%20domains%20%28wings%2C%20structures%2C%20engines%29%2C%20a%20key%20research%20gap%20is%20how%20to%20coordinate%20frozen%2C%20pre-trained%20submodels%20to%20generate%20full-system%20designs%20that%20are%20feasible%2C%20diverse%2C%20and%20high-performing.%20We%20introduce%20Generative%20Latent%20Unification%20of%20Expertise-Informed%20Engineering%20Models%20%28GLUE%29%2C%20which%20orchestrates%20pre-trained%2C%20frozen%20subsystem%20generators%20while%20enforcing%20system-level%20feasibility%2C%20optimality%2C%20and%20diversity.%20We%20propose%20and%20benchmark%20%28i%29%20data-driven%20GLUE%20models%20trained%20on%20pre-generated%20system-level%20designs%20and%20%28ii%29%20a%20data-free%20GLUE%20model%20trained%20online%20on%20a%20differentiable%20geometry%20layer.%20On%20a%20UAV%20design%20problem%20with%20five%20coupling%20constraints%2C%20we%20find%20that%20data-driven%20approaches%20yield%20diverse%2C%20high-performing%20designs%20but%20require%20large%20datasets%20to%20satisfy%20constraints%20reliably.%20The%20data-free%20approach%20is%20competitive%20with%20Bayesian%20optimization%20and%20gradient-based%20optimization%20in%20performance%20and%20feasibility%20while%20training%20a%20full%20generative%20model%20in%20only%2010%20min%20on%20a%20RTX%204090%20GPU%2C%20requiring%20more%20than%20two%20orders%20of%20magnitude%20fewer%20geometry%20evaluations%20and%20FLOPs%20than%20the%20data-driven%20method.%20Ablations%20focused%20on%20data-free%20training%20show%20that%20subsystem%20output%20continuity%20affects%20coordination%2C%20and%20equality%20constraints%20can%20trigger%20mode%20collapse%20unless%20mitigated.%20By%20integrating%20unmodified%2C%20domain-informed%20submodels%20into%20a%20modular%20generative%20workflow%2C%20this%20work%20provides%20a%20viable%20path%20for%20scaling%20generative%20design%20to%20complex%2C%20real-world%20engineering%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2512.19469v1&entry.124074799=Read"},
{"title": "From Pixels to Predicates Structuring urban perception with scene graphs", "author": "Yunlong Liu and Shuyang Li and Pengyuan Liu and Yu Zhang and Rudi Stouffs", "abstract": "Perception research is increasingly modelled using streetscapes, yet many approaches still rely on pixel features or object co-occurrence statistics, overlooking the explicit relations that shape human perception. This study proposes a three stage pipeline that transforms street view imagery (SVI) into structured representations for predicting six perceptual indicators. In the first stage, each image is parsed using an open-set Panoptic Scene Graph model (OpenPSG) to extract object predicate object triplets. In the second stage, compact scene-level embeddings are learned through a heterogeneous graph autoencoder (GraphMAE). In the third stage, a neural network predicts perception scores from these embeddings. We evaluate the proposed approach against image-only baselines in terms of accuracy, precision, and cross-city generalization. Results indicate that (i) our approach improves perception prediction accuracy by an average of 26% over baseline models, and (ii) maintains strong generalization performance in cross-city prediction tasks. Additionally, the structured representation clarifies which relational patterns contribute to lower perception scores in urban scenes, such as graffiti on wall and car parked on sidewalk. Overall, this study demonstrates that graph-based structure provides expressive, generalizable, and interpretable signals for modelling urban perception, advancing human-centric and context-aware urban analytics.", "link": "http://arxiv.org/abs/2512.19221v1", "date": "2025-12-22", "relevancy": 2.352, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.593}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.593}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5631}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Pixels%20to%20Predicates%20Structuring%20urban%20perception%20with%20scene%20graphs&body=Title%3A%20From%20Pixels%20to%20Predicates%20Structuring%20urban%20perception%20with%20scene%20graphs%0AAuthor%3A%20Yunlong%20Liu%20and%20Shuyang%20Li%20and%20Pengyuan%20Liu%20and%20Yu%20Zhang%20and%20Rudi%20Stouffs%0AAbstract%3A%20Perception%20research%20is%20increasingly%20modelled%20using%20streetscapes%2C%20yet%20many%20approaches%20still%20rely%20on%20pixel%20features%20or%20object%20co-occurrence%20statistics%2C%20overlooking%20the%20explicit%20relations%20that%20shape%20human%20perception.%20This%20study%20proposes%20a%20three%20stage%20pipeline%20that%20transforms%20street%20view%20imagery%20%28SVI%29%20into%20structured%20representations%20for%20predicting%20six%20perceptual%20indicators.%20In%20the%20first%20stage%2C%20each%20image%20is%20parsed%20using%20an%20open-set%20Panoptic%20Scene%20Graph%20model%20%28OpenPSG%29%20to%20extract%20object%20predicate%20object%20triplets.%20In%20the%20second%20stage%2C%20compact%20scene-level%20embeddings%20are%20learned%20through%20a%20heterogeneous%20graph%20autoencoder%20%28GraphMAE%29.%20In%20the%20third%20stage%2C%20a%20neural%20network%20predicts%20perception%20scores%20from%20these%20embeddings.%20We%20evaluate%20the%20proposed%20approach%20against%20image-only%20baselines%20in%20terms%20of%20accuracy%2C%20precision%2C%20and%20cross-city%20generalization.%20Results%20indicate%20that%20%28i%29%20our%20approach%20improves%20perception%20prediction%20accuracy%20by%20an%20average%20of%2026%25%20over%20baseline%20models%2C%20and%20%28ii%29%20maintains%20strong%20generalization%20performance%20in%20cross-city%20prediction%20tasks.%20Additionally%2C%20the%20structured%20representation%20clarifies%20which%20relational%20patterns%20contribute%20to%20lower%20perception%20scores%20in%20urban%20scenes%2C%20such%20as%20graffiti%20on%20wall%20and%20car%20parked%20on%20sidewalk.%20Overall%2C%20this%20study%20demonstrates%20that%20graph-based%20structure%20provides%20expressive%2C%20generalizable%2C%20and%20interpretable%20signals%20for%20modelling%20urban%20perception%2C%20advancing%20human-centric%20and%20context-aware%20urban%20analytics.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19221v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Pixels%2520to%2520Predicates%2520Structuring%2520urban%2520perception%2520with%2520scene%2520graphs%26entry.906535625%3DYunlong%2520Liu%2520and%2520Shuyang%2520Li%2520and%2520Pengyuan%2520Liu%2520and%2520Yu%2520Zhang%2520and%2520Rudi%2520Stouffs%26entry.1292438233%3DPerception%2520research%2520is%2520increasingly%2520modelled%2520using%2520streetscapes%252C%2520yet%2520many%2520approaches%2520still%2520rely%2520on%2520pixel%2520features%2520or%2520object%2520co-occurrence%2520statistics%252C%2520overlooking%2520the%2520explicit%2520relations%2520that%2520shape%2520human%2520perception.%2520This%2520study%2520proposes%2520a%2520three%2520stage%2520pipeline%2520that%2520transforms%2520street%2520view%2520imagery%2520%2528SVI%2529%2520into%2520structured%2520representations%2520for%2520predicting%2520six%2520perceptual%2520indicators.%2520In%2520the%2520first%2520stage%252C%2520each%2520image%2520is%2520parsed%2520using%2520an%2520open-set%2520Panoptic%2520Scene%2520Graph%2520model%2520%2528OpenPSG%2529%2520to%2520extract%2520object%2520predicate%2520object%2520triplets.%2520In%2520the%2520second%2520stage%252C%2520compact%2520scene-level%2520embeddings%2520are%2520learned%2520through%2520a%2520heterogeneous%2520graph%2520autoencoder%2520%2528GraphMAE%2529.%2520In%2520the%2520third%2520stage%252C%2520a%2520neural%2520network%2520predicts%2520perception%2520scores%2520from%2520these%2520embeddings.%2520We%2520evaluate%2520the%2520proposed%2520approach%2520against%2520image-only%2520baselines%2520in%2520terms%2520of%2520accuracy%252C%2520precision%252C%2520and%2520cross-city%2520generalization.%2520Results%2520indicate%2520that%2520%2528i%2529%2520our%2520approach%2520improves%2520perception%2520prediction%2520accuracy%2520by%2520an%2520average%2520of%252026%2525%2520over%2520baseline%2520models%252C%2520and%2520%2528ii%2529%2520maintains%2520strong%2520generalization%2520performance%2520in%2520cross-city%2520prediction%2520tasks.%2520Additionally%252C%2520the%2520structured%2520representation%2520clarifies%2520which%2520relational%2520patterns%2520contribute%2520to%2520lower%2520perception%2520scores%2520in%2520urban%2520scenes%252C%2520such%2520as%2520graffiti%2520on%2520wall%2520and%2520car%2520parked%2520on%2520sidewalk.%2520Overall%252C%2520this%2520study%2520demonstrates%2520that%2520graph-based%2520structure%2520provides%2520expressive%252C%2520generalizable%252C%2520and%2520interpretable%2520signals%2520for%2520modelling%2520urban%2520perception%252C%2520advancing%2520human-centric%2520and%2520context-aware%2520urban%2520analytics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19221v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Pixels%20to%20Predicates%20Structuring%20urban%20perception%20with%20scene%20graphs&entry.906535625=Yunlong%20Liu%20and%20Shuyang%20Li%20and%20Pengyuan%20Liu%20and%20Yu%20Zhang%20and%20Rudi%20Stouffs&entry.1292438233=Perception%20research%20is%20increasingly%20modelled%20using%20streetscapes%2C%20yet%20many%20approaches%20still%20rely%20on%20pixel%20features%20or%20object%20co-occurrence%20statistics%2C%20overlooking%20the%20explicit%20relations%20that%20shape%20human%20perception.%20This%20study%20proposes%20a%20three%20stage%20pipeline%20that%20transforms%20street%20view%20imagery%20%28SVI%29%20into%20structured%20representations%20for%20predicting%20six%20perceptual%20indicators.%20In%20the%20first%20stage%2C%20each%20image%20is%20parsed%20using%20an%20open-set%20Panoptic%20Scene%20Graph%20model%20%28OpenPSG%29%20to%20extract%20object%20predicate%20object%20triplets.%20In%20the%20second%20stage%2C%20compact%20scene-level%20embeddings%20are%20learned%20through%20a%20heterogeneous%20graph%20autoencoder%20%28GraphMAE%29.%20In%20the%20third%20stage%2C%20a%20neural%20network%20predicts%20perception%20scores%20from%20these%20embeddings.%20We%20evaluate%20the%20proposed%20approach%20against%20image-only%20baselines%20in%20terms%20of%20accuracy%2C%20precision%2C%20and%20cross-city%20generalization.%20Results%20indicate%20that%20%28i%29%20our%20approach%20improves%20perception%20prediction%20accuracy%20by%20an%20average%20of%2026%25%20over%20baseline%20models%2C%20and%20%28ii%29%20maintains%20strong%20generalization%20performance%20in%20cross-city%20prediction%20tasks.%20Additionally%2C%20the%20structured%20representation%20clarifies%20which%20relational%20patterns%20contribute%20to%20lower%20perception%20scores%20in%20urban%20scenes%2C%20such%20as%20graffiti%20on%20wall%20and%20car%20parked%20on%20sidewalk.%20Overall%2C%20this%20study%20demonstrates%20that%20graph-based%20structure%20provides%20expressive%2C%20generalizable%2C%20and%20interpretable%20signals%20for%20modelling%20urban%20perception%2C%20advancing%20human-centric%20and%20context-aware%20urban%20analytics.&entry.1838667208=http%3A//arxiv.org/abs/2512.19221v1&entry.124074799=Read"},
{"title": "Bridging Semantics and Geometry: A Decoupled LVLM-SAM Framework for Reasoning Segmentation in Remote Sensing", "author": "Xu Zhang and Junyao Ge and Yang Zheng and Kaitai Guo and Jimin Liang", "abstract": "Large Vision-Language Models (LVLMs) hold great promise for advancing remote sensing (RS) analysis, yet existing reasoning segmentation frameworks couple linguistic reasoning and pixel prediction through end-to-end supervised fine-tuning, leading to weak geometric grounding and limited generalization across tasks. To address this, we developed Think2Seg-RS, a decoupled framework that trains an LVLM prompter to control a frozen Segment Anything Model (SAM) via structured geometric prompts. Through a mask-only reinforcement learning objective, the LVLM learns to translate abstract semantic reasoning into spatially grounded actions, achieving state-of-the-art performance on the EarthReason dataset. Remarkably, the learned prompting policy generalizes zero-shot to multiple referring segmentation benchmarks, exposing a distinct divide between semantic-level and instance-level grounding. We further found that compact segmenters outperform larger ones under semantic-level supervision, and that negative prompts are ineffective in heterogeneous aerial backgrounds. Together, these findings establish semantic-level reasoning segmentation as a new paradigm for geospatial understanding, opening the way toward unified, interpretable LVLM-driven Earth observation. Our code and model are available at https://github.com/Ricardo-XZ/Think2Seg-RS.", "link": "http://arxiv.org/abs/2512.19302v1", "date": "2025-12-22", "relevancy": 2.3519, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5931}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5931}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5626}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Semantics%20and%20Geometry%3A%20A%20Decoupled%20LVLM-SAM%20Framework%20for%20Reasoning%20Segmentation%20in%20Remote%20Sensing&body=Title%3A%20Bridging%20Semantics%20and%20Geometry%3A%20A%20Decoupled%20LVLM-SAM%20Framework%20for%20Reasoning%20Segmentation%20in%20Remote%20Sensing%0AAuthor%3A%20Xu%20Zhang%20and%20Junyao%20Ge%20and%20Yang%20Zheng%20and%20Kaitai%20Guo%20and%20Jimin%20Liang%0AAbstract%3A%20Large%20Vision-Language%20Models%20%28LVLMs%29%20hold%20great%20promise%20for%20advancing%20remote%20sensing%20%28RS%29%20analysis%2C%20yet%20existing%20reasoning%20segmentation%20frameworks%20couple%20linguistic%20reasoning%20and%20pixel%20prediction%20through%20end-to-end%20supervised%20fine-tuning%2C%20leading%20to%20weak%20geometric%20grounding%20and%20limited%20generalization%20across%20tasks.%20To%20address%20this%2C%20we%20developed%20Think2Seg-RS%2C%20a%20decoupled%20framework%20that%20trains%20an%20LVLM%20prompter%20to%20control%20a%20frozen%20Segment%20Anything%20Model%20%28SAM%29%20via%20structured%20geometric%20prompts.%20Through%20a%20mask-only%20reinforcement%20learning%20objective%2C%20the%20LVLM%20learns%20to%20translate%20abstract%20semantic%20reasoning%20into%20spatially%20grounded%20actions%2C%20achieving%20state-of-the-art%20performance%20on%20the%20EarthReason%20dataset.%20Remarkably%2C%20the%20learned%20prompting%20policy%20generalizes%20zero-shot%20to%20multiple%20referring%20segmentation%20benchmarks%2C%20exposing%20a%20distinct%20divide%20between%20semantic-level%20and%20instance-level%20grounding.%20We%20further%20found%20that%20compact%20segmenters%20outperform%20larger%20ones%20under%20semantic-level%20supervision%2C%20and%20that%20negative%20prompts%20are%20ineffective%20in%20heterogeneous%20aerial%20backgrounds.%20Together%2C%20these%20findings%20establish%20semantic-level%20reasoning%20segmentation%20as%20a%20new%20paradigm%20for%20geospatial%20understanding%2C%20opening%20the%20way%20toward%20unified%2C%20interpretable%20LVLM-driven%20Earth%20observation.%20Our%20code%20and%20model%20are%20available%20at%20https%3A//github.com/Ricardo-XZ/Think2Seg-RS.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19302v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Semantics%2520and%2520Geometry%253A%2520A%2520Decoupled%2520LVLM-SAM%2520Framework%2520for%2520Reasoning%2520Segmentation%2520in%2520Remote%2520Sensing%26entry.906535625%3DXu%2520Zhang%2520and%2520Junyao%2520Ge%2520and%2520Yang%2520Zheng%2520and%2520Kaitai%2520Guo%2520and%2520Jimin%2520Liang%26entry.1292438233%3DLarge%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520hold%2520great%2520promise%2520for%2520advancing%2520remote%2520sensing%2520%2528RS%2529%2520analysis%252C%2520yet%2520existing%2520reasoning%2520segmentation%2520frameworks%2520couple%2520linguistic%2520reasoning%2520and%2520pixel%2520prediction%2520through%2520end-to-end%2520supervised%2520fine-tuning%252C%2520leading%2520to%2520weak%2520geometric%2520grounding%2520and%2520limited%2520generalization%2520across%2520tasks.%2520To%2520address%2520this%252C%2520we%2520developed%2520Think2Seg-RS%252C%2520a%2520decoupled%2520framework%2520that%2520trains%2520an%2520LVLM%2520prompter%2520to%2520control%2520a%2520frozen%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520via%2520structured%2520geometric%2520prompts.%2520Through%2520a%2520mask-only%2520reinforcement%2520learning%2520objective%252C%2520the%2520LVLM%2520learns%2520to%2520translate%2520abstract%2520semantic%2520reasoning%2520into%2520spatially%2520grounded%2520actions%252C%2520achieving%2520state-of-the-art%2520performance%2520on%2520the%2520EarthReason%2520dataset.%2520Remarkably%252C%2520the%2520learned%2520prompting%2520policy%2520generalizes%2520zero-shot%2520to%2520multiple%2520referring%2520segmentation%2520benchmarks%252C%2520exposing%2520a%2520distinct%2520divide%2520between%2520semantic-level%2520and%2520instance-level%2520grounding.%2520We%2520further%2520found%2520that%2520compact%2520segmenters%2520outperform%2520larger%2520ones%2520under%2520semantic-level%2520supervision%252C%2520and%2520that%2520negative%2520prompts%2520are%2520ineffective%2520in%2520heterogeneous%2520aerial%2520backgrounds.%2520Together%252C%2520these%2520findings%2520establish%2520semantic-level%2520reasoning%2520segmentation%2520as%2520a%2520new%2520paradigm%2520for%2520geospatial%2520understanding%252C%2520opening%2520the%2520way%2520toward%2520unified%252C%2520interpretable%2520LVLM-driven%2520Earth%2520observation.%2520Our%2520code%2520and%2520model%2520are%2520available%2520at%2520https%253A//github.com/Ricardo-XZ/Think2Seg-RS.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19302v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Semantics%20and%20Geometry%3A%20A%20Decoupled%20LVLM-SAM%20Framework%20for%20Reasoning%20Segmentation%20in%20Remote%20Sensing&entry.906535625=Xu%20Zhang%20and%20Junyao%20Ge%20and%20Yang%20Zheng%20and%20Kaitai%20Guo%20and%20Jimin%20Liang&entry.1292438233=Large%20Vision-Language%20Models%20%28LVLMs%29%20hold%20great%20promise%20for%20advancing%20remote%20sensing%20%28RS%29%20analysis%2C%20yet%20existing%20reasoning%20segmentation%20frameworks%20couple%20linguistic%20reasoning%20and%20pixel%20prediction%20through%20end-to-end%20supervised%20fine-tuning%2C%20leading%20to%20weak%20geometric%20grounding%20and%20limited%20generalization%20across%20tasks.%20To%20address%20this%2C%20we%20developed%20Think2Seg-RS%2C%20a%20decoupled%20framework%20that%20trains%20an%20LVLM%20prompter%20to%20control%20a%20frozen%20Segment%20Anything%20Model%20%28SAM%29%20via%20structured%20geometric%20prompts.%20Through%20a%20mask-only%20reinforcement%20learning%20objective%2C%20the%20LVLM%20learns%20to%20translate%20abstract%20semantic%20reasoning%20into%20spatially%20grounded%20actions%2C%20achieving%20state-of-the-art%20performance%20on%20the%20EarthReason%20dataset.%20Remarkably%2C%20the%20learned%20prompting%20policy%20generalizes%20zero-shot%20to%20multiple%20referring%20segmentation%20benchmarks%2C%20exposing%20a%20distinct%20divide%20between%20semantic-level%20and%20instance-level%20grounding.%20We%20further%20found%20that%20compact%20segmenters%20outperform%20larger%20ones%20under%20semantic-level%20supervision%2C%20and%20that%20negative%20prompts%20are%20ineffective%20in%20heterogeneous%20aerial%20backgrounds.%20Together%2C%20these%20findings%20establish%20semantic-level%20reasoning%20segmentation%20as%20a%20new%20paradigm%20for%20geospatial%20understanding%2C%20opening%20the%20way%20toward%20unified%2C%20interpretable%20LVLM-driven%20Earth%20observation.%20Our%20code%20and%20model%20are%20available%20at%20https%3A//github.com/Ricardo-XZ/Think2Seg-RS.&entry.1838667208=http%3A//arxiv.org/abs/2512.19302v1&entry.124074799=Read"},
{"title": "QuantiPhy: A Quantitative Benchmark Evaluating Physical Reasoning Abilities of Vision-Language Models", "author": "Li Puyin and Tiange Xiang and Ella Mao and Shirley Wei and Xinye Chen and Adnan Masood and Li Fei-fei and Ehsan Adeli", "abstract": "Understanding the physical world is essential for generalist AI agents. However, it remains unclear whether state-of-the-art vision perception models (e.g., large VLMs) can reason physical properties quantitatively. Existing evaluations are predominantly VQA-based and qualitative, offering limited insight into whether these models can infer the kinematic quantities of moving objects from video observations. To address this, we present QuantiPhy, the first benchmark designed to quantitatively measure a VLM's physical reasoning ability. Comprising more than 3.3K video-text instances with numerical ground truth, QuantiPhy evaluates a VLM's performance on estimating an object's size, velocity, and acceleration at a given timestamp, using one of these properties as an input prior. The benchmark standardizes prompts and scoring to assess numerical accuracy, enabling fair comparisons across models. Our experiments on state-of-the-art VLMs reveal a consistent gap between their qualitative plausibility and actual numerical correctness. We further provide an in-depth analysis of key factors like background noise, counterfactual priors, and strategic prompting and find that state-of-the-art VLMs lean heavily on pre-trained world knowledge rather than faithfully using the provided visual and textual inputs as references when reasoning kinematic properties quantitatively. QuantiPhy offers the first rigorous, scalable testbed to move VLMs beyond mere verbal plausibility toward a numerically grounded physical understanding.", "link": "http://arxiv.org/abs/2512.19526v1", "date": "2025-12-22", "relevancy": 2.3325, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.589}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.582}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.582}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QuantiPhy%3A%20A%20Quantitative%20Benchmark%20Evaluating%20Physical%20Reasoning%20Abilities%20of%20Vision-Language%20Models&body=Title%3A%20QuantiPhy%3A%20A%20Quantitative%20Benchmark%20Evaluating%20Physical%20Reasoning%20Abilities%20of%20Vision-Language%20Models%0AAuthor%3A%20Li%20Puyin%20and%20Tiange%20Xiang%20and%20Ella%20Mao%20and%20Shirley%20Wei%20and%20Xinye%20Chen%20and%20Adnan%20Masood%20and%20Li%20Fei-fei%20and%20Ehsan%20Adeli%0AAbstract%3A%20Understanding%20the%20physical%20world%20is%20essential%20for%20generalist%20AI%20agents.%20However%2C%20it%20remains%20unclear%20whether%20state-of-the-art%20vision%20perception%20models%20%28e.g.%2C%20large%20VLMs%29%20can%20reason%20physical%20properties%20quantitatively.%20Existing%20evaluations%20are%20predominantly%20VQA-based%20and%20qualitative%2C%20offering%20limited%20insight%20into%20whether%20these%20models%20can%20infer%20the%20kinematic%20quantities%20of%20moving%20objects%20from%20video%20observations.%20To%20address%20this%2C%20we%20present%20QuantiPhy%2C%20the%20first%20benchmark%20designed%20to%20quantitatively%20measure%20a%20VLM%27s%20physical%20reasoning%20ability.%20Comprising%20more%20than%203.3K%20video-text%20instances%20with%20numerical%20ground%20truth%2C%20QuantiPhy%20evaluates%20a%20VLM%27s%20performance%20on%20estimating%20an%20object%27s%20size%2C%20velocity%2C%20and%20acceleration%20at%20a%20given%20timestamp%2C%20using%20one%20of%20these%20properties%20as%20an%20input%20prior.%20The%20benchmark%20standardizes%20prompts%20and%20scoring%20to%20assess%20numerical%20accuracy%2C%20enabling%20fair%20comparisons%20across%20models.%20Our%20experiments%20on%20state-of-the-art%20VLMs%20reveal%20a%20consistent%20gap%20between%20their%20qualitative%20plausibility%20and%20actual%20numerical%20correctness.%20We%20further%20provide%20an%20in-depth%20analysis%20of%20key%20factors%20like%20background%20noise%2C%20counterfactual%20priors%2C%20and%20strategic%20prompting%20and%20find%20that%20state-of-the-art%20VLMs%20lean%20heavily%20on%20pre-trained%20world%20knowledge%20rather%20than%20faithfully%20using%20the%20provided%20visual%20and%20textual%20inputs%20as%20references%20when%20reasoning%20kinematic%20properties%20quantitatively.%20QuantiPhy%20offers%20the%20first%20rigorous%2C%20scalable%20testbed%20to%20move%20VLMs%20beyond%20mere%20verbal%20plausibility%20toward%20a%20numerically%20grounded%20physical%20understanding.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19526v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantiPhy%253A%2520A%2520Quantitative%2520Benchmark%2520Evaluating%2520Physical%2520Reasoning%2520Abilities%2520of%2520Vision-Language%2520Models%26entry.906535625%3DLi%2520Puyin%2520and%2520Tiange%2520Xiang%2520and%2520Ella%2520Mao%2520and%2520Shirley%2520Wei%2520and%2520Xinye%2520Chen%2520and%2520Adnan%2520Masood%2520and%2520Li%2520Fei-fei%2520and%2520Ehsan%2520Adeli%26entry.1292438233%3DUnderstanding%2520the%2520physical%2520world%2520is%2520essential%2520for%2520generalist%2520AI%2520agents.%2520However%252C%2520it%2520remains%2520unclear%2520whether%2520state-of-the-art%2520vision%2520perception%2520models%2520%2528e.g.%252C%2520large%2520VLMs%2529%2520can%2520reason%2520physical%2520properties%2520quantitatively.%2520Existing%2520evaluations%2520are%2520predominantly%2520VQA-based%2520and%2520qualitative%252C%2520offering%2520limited%2520insight%2520into%2520whether%2520these%2520models%2520can%2520infer%2520the%2520kinematic%2520quantities%2520of%2520moving%2520objects%2520from%2520video%2520observations.%2520To%2520address%2520this%252C%2520we%2520present%2520QuantiPhy%252C%2520the%2520first%2520benchmark%2520designed%2520to%2520quantitatively%2520measure%2520a%2520VLM%2527s%2520physical%2520reasoning%2520ability.%2520Comprising%2520more%2520than%25203.3K%2520video-text%2520instances%2520with%2520numerical%2520ground%2520truth%252C%2520QuantiPhy%2520evaluates%2520a%2520VLM%2527s%2520performance%2520on%2520estimating%2520an%2520object%2527s%2520size%252C%2520velocity%252C%2520and%2520acceleration%2520at%2520a%2520given%2520timestamp%252C%2520using%2520one%2520of%2520these%2520properties%2520as%2520an%2520input%2520prior.%2520The%2520benchmark%2520standardizes%2520prompts%2520and%2520scoring%2520to%2520assess%2520numerical%2520accuracy%252C%2520enabling%2520fair%2520comparisons%2520across%2520models.%2520Our%2520experiments%2520on%2520state-of-the-art%2520VLMs%2520reveal%2520a%2520consistent%2520gap%2520between%2520their%2520qualitative%2520plausibility%2520and%2520actual%2520numerical%2520correctness.%2520We%2520further%2520provide%2520an%2520in-depth%2520analysis%2520of%2520key%2520factors%2520like%2520background%2520noise%252C%2520counterfactual%2520priors%252C%2520and%2520strategic%2520prompting%2520and%2520find%2520that%2520state-of-the-art%2520VLMs%2520lean%2520heavily%2520on%2520pre-trained%2520world%2520knowledge%2520rather%2520than%2520faithfully%2520using%2520the%2520provided%2520visual%2520and%2520textual%2520inputs%2520as%2520references%2520when%2520reasoning%2520kinematic%2520properties%2520quantitatively.%2520QuantiPhy%2520offers%2520the%2520first%2520rigorous%252C%2520scalable%2520testbed%2520to%2520move%2520VLMs%2520beyond%2520mere%2520verbal%2520plausibility%2520toward%2520a%2520numerically%2520grounded%2520physical%2520understanding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19526v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QuantiPhy%3A%20A%20Quantitative%20Benchmark%20Evaluating%20Physical%20Reasoning%20Abilities%20of%20Vision-Language%20Models&entry.906535625=Li%20Puyin%20and%20Tiange%20Xiang%20and%20Ella%20Mao%20and%20Shirley%20Wei%20and%20Xinye%20Chen%20and%20Adnan%20Masood%20and%20Li%20Fei-fei%20and%20Ehsan%20Adeli&entry.1292438233=Understanding%20the%20physical%20world%20is%20essential%20for%20generalist%20AI%20agents.%20However%2C%20it%20remains%20unclear%20whether%20state-of-the-art%20vision%20perception%20models%20%28e.g.%2C%20large%20VLMs%29%20can%20reason%20physical%20properties%20quantitatively.%20Existing%20evaluations%20are%20predominantly%20VQA-based%20and%20qualitative%2C%20offering%20limited%20insight%20into%20whether%20these%20models%20can%20infer%20the%20kinematic%20quantities%20of%20moving%20objects%20from%20video%20observations.%20To%20address%20this%2C%20we%20present%20QuantiPhy%2C%20the%20first%20benchmark%20designed%20to%20quantitatively%20measure%20a%20VLM%27s%20physical%20reasoning%20ability.%20Comprising%20more%20than%203.3K%20video-text%20instances%20with%20numerical%20ground%20truth%2C%20QuantiPhy%20evaluates%20a%20VLM%27s%20performance%20on%20estimating%20an%20object%27s%20size%2C%20velocity%2C%20and%20acceleration%20at%20a%20given%20timestamp%2C%20using%20one%20of%20these%20properties%20as%20an%20input%20prior.%20The%20benchmark%20standardizes%20prompts%20and%20scoring%20to%20assess%20numerical%20accuracy%2C%20enabling%20fair%20comparisons%20across%20models.%20Our%20experiments%20on%20state-of-the-art%20VLMs%20reveal%20a%20consistent%20gap%20between%20their%20qualitative%20plausibility%20and%20actual%20numerical%20correctness.%20We%20further%20provide%20an%20in-depth%20analysis%20of%20key%20factors%20like%20background%20noise%2C%20counterfactual%20priors%2C%20and%20strategic%20prompting%20and%20find%20that%20state-of-the-art%20VLMs%20lean%20heavily%20on%20pre-trained%20world%20knowledge%20rather%20than%20faithfully%20using%20the%20provided%20visual%20and%20textual%20inputs%20as%20references%20when%20reasoning%20kinematic%20properties%20quantitatively.%20QuantiPhy%20offers%20the%20first%20rigorous%2C%20scalable%20testbed%20to%20move%20VLMs%20beyond%20mere%20verbal%20plausibility%20toward%20a%20numerically%20grounded%20physical%20understanding.&entry.1838667208=http%3A//arxiv.org/abs/2512.19526v1&entry.124074799=Read"},
{"title": "Mixed formulation and structure-preserving discretization of Cosserat rod dynamics in a port-Hamiltonian framework", "author": "Philipp L. Kinon and Simon R. Eugster and Peter Betsch", "abstract": "An energy-based modeling framework for the nonlinear dynamics of spatial Cosserat rods undergoing large displacements and rotations is proposed. The mixed formulation features independent displacement, velocity and stress variables and is further objective and locking-free. Finite rotations are represented using a director formulation that avoids singularities and yields a constant mass matrix. This results in an infinite-dimensional nonlinear port-Hamiltonian (PH) system governed by partial differential-algebraic equations with a quadratic energy functional. Using a time-differentiated compliance form of the stress-strain relations allows for the imposition of kinematic constraints, such as inextensibility or shear-rigidity. A structure-preserving finite element discretization leads to a finite-dimensional system with PH structure, thus facilitating the design of an energy-momentum consistent integration scheme. Dissipative material behavior (via the generalized-Maxwell model) and non-standard actuation approaches (via pneumatic chambers or tendons) integrate naturally into the framework. As illustrated by selected numerical examples, the present framework establishes a new approach to energy-momentum consistent formulations in computational mechanics involving finite rotations.", "link": "http://arxiv.org/abs/2512.19408v1", "date": "2025-12-22", "relevancy": 2.3283, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.4788}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.461}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4572}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixed%20formulation%20and%20structure-preserving%20discretization%20of%20Cosserat%20rod%20dynamics%20in%20a%20port-Hamiltonian%20framework&body=Title%3A%20Mixed%20formulation%20and%20structure-preserving%20discretization%20of%20Cosserat%20rod%20dynamics%20in%20a%20port-Hamiltonian%20framework%0AAuthor%3A%20Philipp%20L.%20Kinon%20and%20Simon%20R.%20Eugster%20and%20Peter%20Betsch%0AAbstract%3A%20An%20energy-based%20modeling%20framework%20for%20the%20nonlinear%20dynamics%20of%20spatial%20Cosserat%20rods%20undergoing%20large%20displacements%20and%20rotations%20is%20proposed.%20The%20mixed%20formulation%20features%20independent%20displacement%2C%20velocity%20and%20stress%20variables%20and%20is%20further%20objective%20and%20locking-free.%20Finite%20rotations%20are%20represented%20using%20a%20director%20formulation%20that%20avoids%20singularities%20and%20yields%20a%20constant%20mass%20matrix.%20This%20results%20in%20an%20infinite-dimensional%20nonlinear%20port-Hamiltonian%20%28PH%29%20system%20governed%20by%20partial%20differential-algebraic%20equations%20with%20a%20quadratic%20energy%20functional.%20Using%20a%20time-differentiated%20compliance%20form%20of%20the%20stress-strain%20relations%20allows%20for%20the%20imposition%20of%20kinematic%20constraints%2C%20such%20as%20inextensibility%20or%20shear-rigidity.%20A%20structure-preserving%20finite%20element%20discretization%20leads%20to%20a%20finite-dimensional%20system%20with%20PH%20structure%2C%20thus%20facilitating%20the%20design%20of%20an%20energy-momentum%20consistent%20integration%20scheme.%20Dissipative%20material%20behavior%20%28via%20the%20generalized-Maxwell%20model%29%20and%20non-standard%20actuation%20approaches%20%28via%20pneumatic%20chambers%20or%20tendons%29%20integrate%20naturally%20into%20the%20framework.%20As%20illustrated%20by%20selected%20numerical%20examples%2C%20the%20present%20framework%20establishes%20a%20new%20approach%20to%20energy-momentum%20consistent%20formulations%20in%20computational%20mechanics%20involving%20finite%20rotations.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19408v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixed%2520formulation%2520and%2520structure-preserving%2520discretization%2520of%2520Cosserat%2520rod%2520dynamics%2520in%2520a%2520port-Hamiltonian%2520framework%26entry.906535625%3DPhilipp%2520L.%2520Kinon%2520and%2520Simon%2520R.%2520Eugster%2520and%2520Peter%2520Betsch%26entry.1292438233%3DAn%2520energy-based%2520modeling%2520framework%2520for%2520the%2520nonlinear%2520dynamics%2520of%2520spatial%2520Cosserat%2520rods%2520undergoing%2520large%2520displacements%2520and%2520rotations%2520is%2520proposed.%2520The%2520mixed%2520formulation%2520features%2520independent%2520displacement%252C%2520velocity%2520and%2520stress%2520variables%2520and%2520is%2520further%2520objective%2520and%2520locking-free.%2520Finite%2520rotations%2520are%2520represented%2520using%2520a%2520director%2520formulation%2520that%2520avoids%2520singularities%2520and%2520yields%2520a%2520constant%2520mass%2520matrix.%2520This%2520results%2520in%2520an%2520infinite-dimensional%2520nonlinear%2520port-Hamiltonian%2520%2528PH%2529%2520system%2520governed%2520by%2520partial%2520differential-algebraic%2520equations%2520with%2520a%2520quadratic%2520energy%2520functional.%2520Using%2520a%2520time-differentiated%2520compliance%2520form%2520of%2520the%2520stress-strain%2520relations%2520allows%2520for%2520the%2520imposition%2520of%2520kinematic%2520constraints%252C%2520such%2520as%2520inextensibility%2520or%2520shear-rigidity.%2520A%2520structure-preserving%2520finite%2520element%2520discretization%2520leads%2520to%2520a%2520finite-dimensional%2520system%2520with%2520PH%2520structure%252C%2520thus%2520facilitating%2520the%2520design%2520of%2520an%2520energy-momentum%2520consistent%2520integration%2520scheme.%2520Dissipative%2520material%2520behavior%2520%2528via%2520the%2520generalized-Maxwell%2520model%2529%2520and%2520non-standard%2520actuation%2520approaches%2520%2528via%2520pneumatic%2520chambers%2520or%2520tendons%2529%2520integrate%2520naturally%2520into%2520the%2520framework.%2520As%2520illustrated%2520by%2520selected%2520numerical%2520examples%252C%2520the%2520present%2520framework%2520establishes%2520a%2520new%2520approach%2520to%2520energy-momentum%2520consistent%2520formulations%2520in%2520computational%2520mechanics%2520involving%2520finite%2520rotations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19408v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixed%20formulation%20and%20structure-preserving%20discretization%20of%20Cosserat%20rod%20dynamics%20in%20a%20port-Hamiltonian%20framework&entry.906535625=Philipp%20L.%20Kinon%20and%20Simon%20R.%20Eugster%20and%20Peter%20Betsch&entry.1292438233=An%20energy-based%20modeling%20framework%20for%20the%20nonlinear%20dynamics%20of%20spatial%20Cosserat%20rods%20undergoing%20large%20displacements%20and%20rotations%20is%20proposed.%20The%20mixed%20formulation%20features%20independent%20displacement%2C%20velocity%20and%20stress%20variables%20and%20is%20further%20objective%20and%20locking-free.%20Finite%20rotations%20are%20represented%20using%20a%20director%20formulation%20that%20avoids%20singularities%20and%20yields%20a%20constant%20mass%20matrix.%20This%20results%20in%20an%20infinite-dimensional%20nonlinear%20port-Hamiltonian%20%28PH%29%20system%20governed%20by%20partial%20differential-algebraic%20equations%20with%20a%20quadratic%20energy%20functional.%20Using%20a%20time-differentiated%20compliance%20form%20of%20the%20stress-strain%20relations%20allows%20for%20the%20imposition%20of%20kinematic%20constraints%2C%20such%20as%20inextensibility%20or%20shear-rigidity.%20A%20structure-preserving%20finite%20element%20discretization%20leads%20to%20a%20finite-dimensional%20system%20with%20PH%20structure%2C%20thus%20facilitating%20the%20design%20of%20an%20energy-momentum%20consistent%20integration%20scheme.%20Dissipative%20material%20behavior%20%28via%20the%20generalized-Maxwell%20model%29%20and%20non-standard%20actuation%20approaches%20%28via%20pneumatic%20chambers%20or%20tendons%29%20integrate%20naturally%20into%20the%20framework.%20As%20illustrated%20by%20selected%20numerical%20examples%2C%20the%20present%20framework%20establishes%20a%20new%20approach%20to%20energy-momentum%20consistent%20formulations%20in%20computational%20mechanics%20involving%20finite%20rotations.&entry.1838667208=http%3A//arxiv.org/abs/2512.19408v1&entry.124074799=Read"},
{"title": "Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis", "author": "Argha Kamal Samanta and Harshika Goyal and Vasudha Joshi and Tushar Mungle and Pabitra Mitra", "abstract": "Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems. While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images. We propose a novel knowledge-enhanced joint embedding framework that integrates retinal fundus images, clinical text, and structured patient data through a multimodal transformer architecture to address the critical gap in medical image-text alignment. Our approach employs separate encoders for each modality: a Vision Transformer (ViT-B/16) for retinal images, Bio-ClinicalBERT for clinical narratives, and a multilayer perceptron for structured demographic and clinical features. These modalities are fused through a joint transformer with modality-specific embeddings, trained using multiple objectives including contrastive losses between modality pairs, reconstruction losses for images and text, and classification losses for DR severity grading according to ICDR and SDRG schemes. Experimental results on the Brazilian Multilabel Ophthalmological Dataset (BRSET) demonstrate significant improvements over baseline models. Our framework achieves near-perfect text-to-image retrieval performance with Recall@1 of 99.94% compared to fine-tuned CLIP's 1.29%, while maintaining state-of-the-art classification accuracy of 97.05% for SDRG and 97.97% for ICDR. Furthermore, zero-shot evaluation on the unseen DeepEyeNet dataset validates strong generalizability with 93.95% Recall@1 versus 0.22% for fine-tuned CLIP. These results demonstrate that our multimodal training approach effectively captures cross-modal relationships in the medical domain, establishing both superior retrieval capabilities and robust diagnostic performance.", "link": "http://arxiv.org/abs/2512.19663v1", "date": "2025-12-22", "relevancy": 2.3142, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6383}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5383}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5298}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20CLIP%3A%20Knowledge-Enhanced%20Multimodal%20Transformers%20for%20Cross-Modal%20Alignment%20in%20Diabetic%20Retinopathy%20Diagnosis&body=Title%3A%20Beyond%20CLIP%3A%20Knowledge-Enhanced%20Multimodal%20Transformers%20for%20Cross-Modal%20Alignment%20in%20Diabetic%20Retinopathy%20Diagnosis%0AAuthor%3A%20Argha%20Kamal%20Samanta%20and%20Harshika%20Goyal%20and%20Vasudha%20Joshi%20and%20Tushar%20Mungle%20and%20Pabitra%20Mitra%0AAbstract%3A%20Diabetic%20retinopathy%20%28DR%29%20is%20a%20leading%20cause%20of%20preventable%20blindness%20worldwide%2C%20demanding%20accurate%20automated%20diagnostic%20systems.%20While%20general-domain%20vision-language%20models%20like%20Contrastive%20Language-Image%20Pre-Training%20%28CLIP%29%20perform%20well%20on%20natural%20image%20tasks%2C%20they%20struggle%20in%20medical%20domain%20applications%2C%20particularly%20in%20cross-modal%20retrieval%20for%20ophthalmological%20images.%20We%20propose%20a%20novel%20knowledge-enhanced%20joint%20embedding%20framework%20that%20integrates%20retinal%20fundus%20images%2C%20clinical%20text%2C%20and%20structured%20patient%20data%20through%20a%20multimodal%20transformer%20architecture%20to%20address%20the%20critical%20gap%20in%20medical%20image-text%20alignment.%20Our%20approach%20employs%20separate%20encoders%20for%20each%20modality%3A%20a%20Vision%20Transformer%20%28ViT-B/16%29%20for%20retinal%20images%2C%20Bio-ClinicalBERT%20for%20clinical%20narratives%2C%20and%20a%20multilayer%20perceptron%20for%20structured%20demographic%20and%20clinical%20features.%20These%20modalities%20are%20fused%20through%20a%20joint%20transformer%20with%20modality-specific%20embeddings%2C%20trained%20using%20multiple%20objectives%20including%20contrastive%20losses%20between%20modality%20pairs%2C%20reconstruction%20losses%20for%20images%20and%20text%2C%20and%20classification%20losses%20for%20DR%20severity%20grading%20according%20to%20ICDR%20and%20SDRG%20schemes.%20Experimental%20results%20on%20the%20Brazilian%20Multilabel%20Ophthalmological%20Dataset%20%28BRSET%29%20demonstrate%20significant%20improvements%20over%20baseline%20models.%20Our%20framework%20achieves%20near-perfect%20text-to-image%20retrieval%20performance%20with%20Recall%401%20of%2099.94%25%20compared%20to%20fine-tuned%20CLIP%27s%201.29%25%2C%20while%20maintaining%20state-of-the-art%20classification%20accuracy%20of%2097.05%25%20for%20SDRG%20and%2097.97%25%20for%20ICDR.%20Furthermore%2C%20zero-shot%20evaluation%20on%20the%20unseen%20DeepEyeNet%20dataset%20validates%20strong%20generalizability%20with%2093.95%25%20Recall%401%20versus%200.22%25%20for%20fine-tuned%20CLIP.%20These%20results%20demonstrate%20that%20our%20multimodal%20training%20approach%20effectively%20captures%20cross-modal%20relationships%20in%20the%20medical%20domain%2C%20establishing%20both%20superior%20retrieval%20capabilities%20and%20robust%20diagnostic%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19663v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520CLIP%253A%2520Knowledge-Enhanced%2520Multimodal%2520Transformers%2520for%2520Cross-Modal%2520Alignment%2520in%2520Diabetic%2520Retinopathy%2520Diagnosis%26entry.906535625%3DArgha%2520Kamal%2520Samanta%2520and%2520Harshika%2520Goyal%2520and%2520Vasudha%2520Joshi%2520and%2520Tushar%2520Mungle%2520and%2520Pabitra%2520Mitra%26entry.1292438233%3DDiabetic%2520retinopathy%2520%2528DR%2529%2520is%2520a%2520leading%2520cause%2520of%2520preventable%2520blindness%2520worldwide%252C%2520demanding%2520accurate%2520automated%2520diagnostic%2520systems.%2520While%2520general-domain%2520vision-language%2520models%2520like%2520Contrastive%2520Language-Image%2520Pre-Training%2520%2528CLIP%2529%2520perform%2520well%2520on%2520natural%2520image%2520tasks%252C%2520they%2520struggle%2520in%2520medical%2520domain%2520applications%252C%2520particularly%2520in%2520cross-modal%2520retrieval%2520for%2520ophthalmological%2520images.%2520We%2520propose%2520a%2520novel%2520knowledge-enhanced%2520joint%2520embedding%2520framework%2520that%2520integrates%2520retinal%2520fundus%2520images%252C%2520clinical%2520text%252C%2520and%2520structured%2520patient%2520data%2520through%2520a%2520multimodal%2520transformer%2520architecture%2520to%2520address%2520the%2520critical%2520gap%2520in%2520medical%2520image-text%2520alignment.%2520Our%2520approach%2520employs%2520separate%2520encoders%2520for%2520each%2520modality%253A%2520a%2520Vision%2520Transformer%2520%2528ViT-B/16%2529%2520for%2520retinal%2520images%252C%2520Bio-ClinicalBERT%2520for%2520clinical%2520narratives%252C%2520and%2520a%2520multilayer%2520perceptron%2520for%2520structured%2520demographic%2520and%2520clinical%2520features.%2520These%2520modalities%2520are%2520fused%2520through%2520a%2520joint%2520transformer%2520with%2520modality-specific%2520embeddings%252C%2520trained%2520using%2520multiple%2520objectives%2520including%2520contrastive%2520losses%2520between%2520modality%2520pairs%252C%2520reconstruction%2520losses%2520for%2520images%2520and%2520text%252C%2520and%2520classification%2520losses%2520for%2520DR%2520severity%2520grading%2520according%2520to%2520ICDR%2520and%2520SDRG%2520schemes.%2520Experimental%2520results%2520on%2520the%2520Brazilian%2520Multilabel%2520Ophthalmological%2520Dataset%2520%2528BRSET%2529%2520demonstrate%2520significant%2520improvements%2520over%2520baseline%2520models.%2520Our%2520framework%2520achieves%2520near-perfect%2520text-to-image%2520retrieval%2520performance%2520with%2520Recall%25401%2520of%252099.94%2525%2520compared%2520to%2520fine-tuned%2520CLIP%2527s%25201.29%2525%252C%2520while%2520maintaining%2520state-of-the-art%2520classification%2520accuracy%2520of%252097.05%2525%2520for%2520SDRG%2520and%252097.97%2525%2520for%2520ICDR.%2520Furthermore%252C%2520zero-shot%2520evaluation%2520on%2520the%2520unseen%2520DeepEyeNet%2520dataset%2520validates%2520strong%2520generalizability%2520with%252093.95%2525%2520Recall%25401%2520versus%25200.22%2525%2520for%2520fine-tuned%2520CLIP.%2520These%2520results%2520demonstrate%2520that%2520our%2520multimodal%2520training%2520approach%2520effectively%2520captures%2520cross-modal%2520relationships%2520in%2520the%2520medical%2520domain%252C%2520establishing%2520both%2520superior%2520retrieval%2520capabilities%2520and%2520robust%2520diagnostic%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19663v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20CLIP%3A%20Knowledge-Enhanced%20Multimodal%20Transformers%20for%20Cross-Modal%20Alignment%20in%20Diabetic%20Retinopathy%20Diagnosis&entry.906535625=Argha%20Kamal%20Samanta%20and%20Harshika%20Goyal%20and%20Vasudha%20Joshi%20and%20Tushar%20Mungle%20and%20Pabitra%20Mitra&entry.1292438233=Diabetic%20retinopathy%20%28DR%29%20is%20a%20leading%20cause%20of%20preventable%20blindness%20worldwide%2C%20demanding%20accurate%20automated%20diagnostic%20systems.%20While%20general-domain%20vision-language%20models%20like%20Contrastive%20Language-Image%20Pre-Training%20%28CLIP%29%20perform%20well%20on%20natural%20image%20tasks%2C%20they%20struggle%20in%20medical%20domain%20applications%2C%20particularly%20in%20cross-modal%20retrieval%20for%20ophthalmological%20images.%20We%20propose%20a%20novel%20knowledge-enhanced%20joint%20embedding%20framework%20that%20integrates%20retinal%20fundus%20images%2C%20clinical%20text%2C%20and%20structured%20patient%20data%20through%20a%20multimodal%20transformer%20architecture%20to%20address%20the%20critical%20gap%20in%20medical%20image-text%20alignment.%20Our%20approach%20employs%20separate%20encoders%20for%20each%20modality%3A%20a%20Vision%20Transformer%20%28ViT-B/16%29%20for%20retinal%20images%2C%20Bio-ClinicalBERT%20for%20clinical%20narratives%2C%20and%20a%20multilayer%20perceptron%20for%20structured%20demographic%20and%20clinical%20features.%20These%20modalities%20are%20fused%20through%20a%20joint%20transformer%20with%20modality-specific%20embeddings%2C%20trained%20using%20multiple%20objectives%20including%20contrastive%20losses%20between%20modality%20pairs%2C%20reconstruction%20losses%20for%20images%20and%20text%2C%20and%20classification%20losses%20for%20DR%20severity%20grading%20according%20to%20ICDR%20and%20SDRG%20schemes.%20Experimental%20results%20on%20the%20Brazilian%20Multilabel%20Ophthalmological%20Dataset%20%28BRSET%29%20demonstrate%20significant%20improvements%20over%20baseline%20models.%20Our%20framework%20achieves%20near-perfect%20text-to-image%20retrieval%20performance%20with%20Recall%401%20of%2099.94%25%20compared%20to%20fine-tuned%20CLIP%27s%201.29%25%2C%20while%20maintaining%20state-of-the-art%20classification%20accuracy%20of%2097.05%25%20for%20SDRG%20and%2097.97%25%20for%20ICDR.%20Furthermore%2C%20zero-shot%20evaluation%20on%20the%20unseen%20DeepEyeNet%20dataset%20validates%20strong%20generalizability%20with%2093.95%25%20Recall%401%20versus%200.22%25%20for%20fine-tuned%20CLIP.%20These%20results%20demonstrate%20that%20our%20multimodal%20training%20approach%20effectively%20captures%20cross-modal%20relationships%20in%20the%20medical%20domain%2C%20establishing%20both%20superior%20retrieval%20capabilities%20and%20robust%20diagnostic%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2512.19663v1&entry.124074799=Read"},
{"title": "Sprecher Networks: A Parameter-Efficient Kolmogorov-Arnold Architecture", "author": "Christian H\u00e4gg and Kathl\u00e9n Kohn and Giovanni Luca Marchetti and Boris Shapiro", "abstract": "We present Sprecher Networks (SNs), a family of trainable neural architectures inspired by the classical Kolmogorov-Arnold-Sprecher (KAS) construction for approximating multivariate continuous functions. Distinct from Multi-Layer Perceptrons (MLPs) with fixed node activations and Kolmogorov-Arnold Networks (KANs) featuring learnable edge activations, SNs utilize shared, learnable splines (monotonic and general) within structured blocks incorporating explicit shift parameters and mixing weights. Our approach directly realizes Sprecher's specific 1965 sum of shifted splines formula in its single-layer variant and extends it to deeper, multi-layer compositions. We further enhance the architecture with optional lateral mixing connections that enable intra-block communication between output dimensions, providing a parameter-efficient alternative to full attention mechanisms. Beyond parameter efficiency with $O(LN + LG)$ scaling (where $G$ is the knot count of the shared splines) versus MLPs' $O(LN^2)$, SNs admit a sequential evaluation strategy that reduces peak forward-intermediate memory from $O(N^2)$ to $O(N)$ (treating batch size as constant), making much wider architectures feasible under memory constraints. We demonstrate empirically that composing these blocks into deep networks leads to highly parameter and memory-efficient models, discuss theoretical motivations, and compare SNs with related architectures (MLPs, KANs, and networks with learnable node activations).", "link": "http://arxiv.org/abs/2512.19367v1", "date": "2025-12-22", "relevancy": 2.3092, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.484}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4567}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4448}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sprecher%20Networks%3A%20A%20Parameter-Efficient%20Kolmogorov-Arnold%20Architecture&body=Title%3A%20Sprecher%20Networks%3A%20A%20Parameter-Efficient%20Kolmogorov-Arnold%20Architecture%0AAuthor%3A%20Christian%20H%C3%A4gg%20and%20Kathl%C3%A9n%20Kohn%20and%20Giovanni%20Luca%20Marchetti%20and%20Boris%20Shapiro%0AAbstract%3A%20We%20present%20Sprecher%20Networks%20%28SNs%29%2C%20a%20family%20of%20trainable%20neural%20architectures%20inspired%20by%20the%20classical%20Kolmogorov-Arnold-Sprecher%20%28KAS%29%20construction%20for%20approximating%20multivariate%20continuous%20functions.%20Distinct%20from%20Multi-Layer%20Perceptrons%20%28MLPs%29%20with%20fixed%20node%20activations%20and%20Kolmogorov-Arnold%20Networks%20%28KANs%29%20featuring%20learnable%20edge%20activations%2C%20SNs%20utilize%20shared%2C%20learnable%20splines%20%28monotonic%20and%20general%29%20within%20structured%20blocks%20incorporating%20explicit%20shift%20parameters%20and%20mixing%20weights.%20Our%20approach%20directly%20realizes%20Sprecher%27s%20specific%201965%20sum%20of%20shifted%20splines%20formula%20in%20its%20single-layer%20variant%20and%20extends%20it%20to%20deeper%2C%20multi-layer%20compositions.%20We%20further%20enhance%20the%20architecture%20with%20optional%20lateral%20mixing%20connections%20that%20enable%20intra-block%20communication%20between%20output%20dimensions%2C%20providing%20a%20parameter-efficient%20alternative%20to%20full%20attention%20mechanisms.%20Beyond%20parameter%20efficiency%20with%20%24O%28LN%20%2B%20LG%29%24%20scaling%20%28where%20%24G%24%20is%20the%20knot%20count%20of%20the%20shared%20splines%29%20versus%20MLPs%27%20%24O%28LN%5E2%29%24%2C%20SNs%20admit%20a%20sequential%20evaluation%20strategy%20that%20reduces%20peak%20forward-intermediate%20memory%20from%20%24O%28N%5E2%29%24%20to%20%24O%28N%29%24%20%28treating%20batch%20size%20as%20constant%29%2C%20making%20much%20wider%20architectures%20feasible%20under%20memory%20constraints.%20We%20demonstrate%20empirically%20that%20composing%20these%20blocks%20into%20deep%20networks%20leads%20to%20highly%20parameter%20and%20memory-efficient%20models%2C%20discuss%20theoretical%20motivations%2C%20and%20compare%20SNs%20with%20related%20architectures%20%28MLPs%2C%20KANs%2C%20and%20networks%20with%20learnable%20node%20activations%29.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19367v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSprecher%2520Networks%253A%2520A%2520Parameter-Efficient%2520Kolmogorov-Arnold%2520Architecture%26entry.906535625%3DChristian%2520H%25C3%25A4gg%2520and%2520Kathl%25C3%25A9n%2520Kohn%2520and%2520Giovanni%2520Luca%2520Marchetti%2520and%2520Boris%2520Shapiro%26entry.1292438233%3DWe%2520present%2520Sprecher%2520Networks%2520%2528SNs%2529%252C%2520a%2520family%2520of%2520trainable%2520neural%2520architectures%2520inspired%2520by%2520the%2520classical%2520Kolmogorov-Arnold-Sprecher%2520%2528KAS%2529%2520construction%2520for%2520approximating%2520multivariate%2520continuous%2520functions.%2520Distinct%2520from%2520Multi-Layer%2520Perceptrons%2520%2528MLPs%2529%2520with%2520fixed%2520node%2520activations%2520and%2520Kolmogorov-Arnold%2520Networks%2520%2528KANs%2529%2520featuring%2520learnable%2520edge%2520activations%252C%2520SNs%2520utilize%2520shared%252C%2520learnable%2520splines%2520%2528monotonic%2520and%2520general%2529%2520within%2520structured%2520blocks%2520incorporating%2520explicit%2520shift%2520parameters%2520and%2520mixing%2520weights.%2520Our%2520approach%2520directly%2520realizes%2520Sprecher%2527s%2520specific%25201965%2520sum%2520of%2520shifted%2520splines%2520formula%2520in%2520its%2520single-layer%2520variant%2520and%2520extends%2520it%2520to%2520deeper%252C%2520multi-layer%2520compositions.%2520We%2520further%2520enhance%2520the%2520architecture%2520with%2520optional%2520lateral%2520mixing%2520connections%2520that%2520enable%2520intra-block%2520communication%2520between%2520output%2520dimensions%252C%2520providing%2520a%2520parameter-efficient%2520alternative%2520to%2520full%2520attention%2520mechanisms.%2520Beyond%2520parameter%2520efficiency%2520with%2520%2524O%2528LN%2520%252B%2520LG%2529%2524%2520scaling%2520%2528where%2520%2524G%2524%2520is%2520the%2520knot%2520count%2520of%2520the%2520shared%2520splines%2529%2520versus%2520MLPs%2527%2520%2524O%2528LN%255E2%2529%2524%252C%2520SNs%2520admit%2520a%2520sequential%2520evaluation%2520strategy%2520that%2520reduces%2520peak%2520forward-intermediate%2520memory%2520from%2520%2524O%2528N%255E2%2529%2524%2520to%2520%2524O%2528N%2529%2524%2520%2528treating%2520batch%2520size%2520as%2520constant%2529%252C%2520making%2520much%2520wider%2520architectures%2520feasible%2520under%2520memory%2520constraints.%2520We%2520demonstrate%2520empirically%2520that%2520composing%2520these%2520blocks%2520into%2520deep%2520networks%2520leads%2520to%2520highly%2520parameter%2520and%2520memory-efficient%2520models%252C%2520discuss%2520theoretical%2520motivations%252C%2520and%2520compare%2520SNs%2520with%2520related%2520architectures%2520%2528MLPs%252C%2520KANs%252C%2520and%2520networks%2520with%2520learnable%2520node%2520activations%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19367v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sprecher%20Networks%3A%20A%20Parameter-Efficient%20Kolmogorov-Arnold%20Architecture&entry.906535625=Christian%20H%C3%A4gg%20and%20Kathl%C3%A9n%20Kohn%20and%20Giovanni%20Luca%20Marchetti%20and%20Boris%20Shapiro&entry.1292438233=We%20present%20Sprecher%20Networks%20%28SNs%29%2C%20a%20family%20of%20trainable%20neural%20architectures%20inspired%20by%20the%20classical%20Kolmogorov-Arnold-Sprecher%20%28KAS%29%20construction%20for%20approximating%20multivariate%20continuous%20functions.%20Distinct%20from%20Multi-Layer%20Perceptrons%20%28MLPs%29%20with%20fixed%20node%20activations%20and%20Kolmogorov-Arnold%20Networks%20%28KANs%29%20featuring%20learnable%20edge%20activations%2C%20SNs%20utilize%20shared%2C%20learnable%20splines%20%28monotonic%20and%20general%29%20within%20structured%20blocks%20incorporating%20explicit%20shift%20parameters%20and%20mixing%20weights.%20Our%20approach%20directly%20realizes%20Sprecher%27s%20specific%201965%20sum%20of%20shifted%20splines%20formula%20in%20its%20single-layer%20variant%20and%20extends%20it%20to%20deeper%2C%20multi-layer%20compositions.%20We%20further%20enhance%20the%20architecture%20with%20optional%20lateral%20mixing%20connections%20that%20enable%20intra-block%20communication%20between%20output%20dimensions%2C%20providing%20a%20parameter-efficient%20alternative%20to%20full%20attention%20mechanisms.%20Beyond%20parameter%20efficiency%20with%20%24O%28LN%20%2B%20LG%29%24%20scaling%20%28where%20%24G%24%20is%20the%20knot%20count%20of%20the%20shared%20splines%29%20versus%20MLPs%27%20%24O%28LN%5E2%29%24%2C%20SNs%20admit%20a%20sequential%20evaluation%20strategy%20that%20reduces%20peak%20forward-intermediate%20memory%20from%20%24O%28N%5E2%29%24%20to%20%24O%28N%29%24%20%28treating%20batch%20size%20as%20constant%29%2C%20making%20much%20wider%20architectures%20feasible%20under%20memory%20constraints.%20We%20demonstrate%20empirically%20that%20composing%20these%20blocks%20into%20deep%20networks%20leads%20to%20highly%20parameter%20and%20memory-efficient%20models%2C%20discuss%20theoretical%20motivations%2C%20and%20compare%20SNs%20with%20related%20architectures%20%28MLPs%2C%20KANs%2C%20and%20networks%20with%20learnable%20node%20activations%29.&entry.1838667208=http%3A//arxiv.org/abs/2512.19367v1&entry.124074799=Read"},
{"title": "Zero-shot Reconstruction of In-Scene Object Manipulation from Video", "author": "Dixuan Lin and Tianyou Wang and Zhuoyang Pan and Yufu Wang and Lingjie Liu and Kostas Daniilidis", "abstract": "We build the first system to address the problem of reconstructing in-scene object manipulation from a monocular RGB video. It is challenging due to ill-posed scene reconstruction, ambiguous hand-object depth, and the need for physically plausible interactions. Existing methods operate in hand centric coordinates and ignore the scene, hindering metric accuracy and practical use. In our method, we first use data-driven foundation models to initialize the core components, including the object mesh and poses, the scene point cloud, and the hand poses. We then apply a two-stage optimization that recovers a complete hand-object motion from grasping to interaction, which remains consistent with the scene information observed in the input video.", "link": "http://arxiv.org/abs/2512.19684v1", "date": "2025-12-22", "relevancy": 2.3074, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5823}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5752}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-shot%20Reconstruction%20of%20In-Scene%20Object%20Manipulation%20from%20Video&body=Title%3A%20Zero-shot%20Reconstruction%20of%20In-Scene%20Object%20Manipulation%20from%20Video%0AAuthor%3A%20Dixuan%20Lin%20and%20Tianyou%20Wang%20and%20Zhuoyang%20Pan%20and%20Yufu%20Wang%20and%20Lingjie%20Liu%20and%20Kostas%20Daniilidis%0AAbstract%3A%20We%20build%20the%20first%20system%20to%20address%20the%20problem%20of%20reconstructing%20in-scene%20object%20manipulation%20from%20a%20monocular%20RGB%20video.%20It%20is%20challenging%20due%20to%20ill-posed%20scene%20reconstruction%2C%20ambiguous%20hand-object%20depth%2C%20and%20the%20need%20for%20physically%20plausible%20interactions.%20Existing%20methods%20operate%20in%20hand%20centric%20coordinates%20and%20ignore%20the%20scene%2C%20hindering%20metric%20accuracy%20and%20practical%20use.%20In%20our%20method%2C%20we%20first%20use%20data-driven%20foundation%20models%20to%20initialize%20the%20core%20components%2C%20including%20the%20object%20mesh%20and%20poses%2C%20the%20scene%20point%20cloud%2C%20and%20the%20hand%20poses.%20We%20then%20apply%20a%20two-stage%20optimization%20that%20recovers%20a%20complete%20hand-object%20motion%20from%20grasping%20to%20interaction%2C%20which%20remains%20consistent%20with%20the%20scene%20information%20observed%20in%20the%20input%20video.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19684v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-shot%2520Reconstruction%2520of%2520In-Scene%2520Object%2520Manipulation%2520from%2520Video%26entry.906535625%3DDixuan%2520Lin%2520and%2520Tianyou%2520Wang%2520and%2520Zhuoyang%2520Pan%2520and%2520Yufu%2520Wang%2520and%2520Lingjie%2520Liu%2520and%2520Kostas%2520Daniilidis%26entry.1292438233%3DWe%2520build%2520the%2520first%2520system%2520to%2520address%2520the%2520problem%2520of%2520reconstructing%2520in-scene%2520object%2520manipulation%2520from%2520a%2520monocular%2520RGB%2520video.%2520It%2520is%2520challenging%2520due%2520to%2520ill-posed%2520scene%2520reconstruction%252C%2520ambiguous%2520hand-object%2520depth%252C%2520and%2520the%2520need%2520for%2520physically%2520plausible%2520interactions.%2520Existing%2520methods%2520operate%2520in%2520hand%2520centric%2520coordinates%2520and%2520ignore%2520the%2520scene%252C%2520hindering%2520metric%2520accuracy%2520and%2520practical%2520use.%2520In%2520our%2520method%252C%2520we%2520first%2520use%2520data-driven%2520foundation%2520models%2520to%2520initialize%2520the%2520core%2520components%252C%2520including%2520the%2520object%2520mesh%2520and%2520poses%252C%2520the%2520scene%2520point%2520cloud%252C%2520and%2520the%2520hand%2520poses.%2520We%2520then%2520apply%2520a%2520two-stage%2520optimization%2520that%2520recovers%2520a%2520complete%2520hand-object%2520motion%2520from%2520grasping%2520to%2520interaction%252C%2520which%2520remains%2520consistent%2520with%2520the%2520scene%2520information%2520observed%2520in%2520the%2520input%2520video.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19684v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-shot%20Reconstruction%20of%20In-Scene%20Object%20Manipulation%20from%20Video&entry.906535625=Dixuan%20Lin%20and%20Tianyou%20Wang%20and%20Zhuoyang%20Pan%20and%20Yufu%20Wang%20and%20Lingjie%20Liu%20and%20Kostas%20Daniilidis&entry.1292438233=We%20build%20the%20first%20system%20to%20address%20the%20problem%20of%20reconstructing%20in-scene%20object%20manipulation%20from%20a%20monocular%20RGB%20video.%20It%20is%20challenging%20due%20to%20ill-posed%20scene%20reconstruction%2C%20ambiguous%20hand-object%20depth%2C%20and%20the%20need%20for%20physically%20plausible%20interactions.%20Existing%20methods%20operate%20in%20hand%20centric%20coordinates%20and%20ignore%20the%20scene%2C%20hindering%20metric%20accuracy%20and%20practical%20use.%20In%20our%20method%2C%20we%20first%20use%20data-driven%20foundation%20models%20to%20initialize%20the%20core%20components%2C%20including%20the%20object%20mesh%20and%20poses%2C%20the%20scene%20point%20cloud%2C%20and%20the%20hand%20poses.%20We%20then%20apply%20a%20two-stage%20optimization%20that%20recovers%20a%20complete%20hand-object%20motion%20from%20grasping%20to%20interaction%2C%20which%20remains%20consistent%20with%20the%20scene%20information%20observed%20in%20the%20input%20video.&entry.1838667208=http%3A//arxiv.org/abs/2512.19684v1&entry.124074799=Read"},
{"title": "Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies", "author": "Zhixuan Liang and Yizhuo Li and Tianshuo Yang and Chengyue Wu and Sitong Mao and Tian Nian and Liuao Pei and Shunbo Zhou and Xiaokang Yang and Jiangmiao Pang and Yao Mu and Ping Luo", "abstract": "Vision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions into robot actions. However, prevailing VLAs either generate actions auto-regressively in a fixed left-to-right order or attach separate MLP or diffusion heads outside the backbone, leading to fragmented information pathways and specialized training requirements that hinder a unified, scalable architecture. We present Discrete Diffusion VLA, a unified-transformer policy that models discretized action chunks with discrete diffusion. The design retains diffusion's progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive decoding order that resolves easy action elements before harder ones and uses secondary re-masking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified decoder preserves pre-trained vision-language priors, supports parallel decoding, breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3% avg. success rates on LIBERO, 71.2% visual matching on SimplerEnv-Fractal and 54.2% overall on SimplerEnv-Bridge. We also provide ablation study on vision-language ability retention on LIBERO-OOD (Out-of-Distribution) benchmark, with our method improving over autoregressive, MLP decoder and continuous diffusion baselines. These findings indicate that discrete-diffusion VLA supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets. Our code is available at https://github.com/Liang-ZX/DiscreteDiffusionVLA/tree/libero.", "link": "http://arxiv.org/abs/2508.20072v3", "date": "2025-12-22", "relevancy": 2.3014, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6134}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5677}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discrete%20Diffusion%20VLA%3A%20Bringing%20Discrete%20Diffusion%20to%20Action%20Decoding%20in%20Vision-Language-Action%20Policies&body=Title%3A%20Discrete%20Diffusion%20VLA%3A%20Bringing%20Discrete%20Diffusion%20to%20Action%20Decoding%20in%20Vision-Language-Action%20Policies%0AAuthor%3A%20Zhixuan%20Liang%20and%20Yizhuo%20Li%20and%20Tianshuo%20Yang%20and%20Chengyue%20Wu%20and%20Sitong%20Mao%20and%20Tian%20Nian%20and%20Liuao%20Pei%20and%20Shunbo%20Zhou%20and%20Xiaokang%20Yang%20and%20Jiangmiao%20Pang%20and%20Yao%20Mu%20and%20Ping%20Luo%0AAbstract%3A%20Vision-Language-Action%20%28VLA%29%20models%20adapt%20large%20vision-language%20backbones%20to%20map%20images%20and%20instructions%20into%20robot%20actions.%20However%2C%20prevailing%20VLAs%20either%20generate%20actions%20auto-regressively%20in%20a%20fixed%20left-to-right%20order%20or%20attach%20separate%20MLP%20or%20diffusion%20heads%20outside%20the%20backbone%2C%20leading%20to%20fragmented%20information%20pathways%20and%20specialized%20training%20requirements%20that%20hinder%20a%20unified%2C%20scalable%20architecture.%20We%20present%20Discrete%20Diffusion%20VLA%2C%20a%20unified-transformer%20policy%20that%20models%20discretized%20action%20chunks%20with%20discrete%20diffusion.%20The%20design%20retains%20diffusion%27s%20progressive%20refinement%20paradigm%20while%20remaining%20natively%20compatible%20with%20the%20discrete%20token%20interface%20of%20VLMs.%20Our%20method%20achieves%20an%20adaptive%20decoding%20order%20that%20resolves%20easy%20action%20elements%20before%20harder%20ones%20and%20uses%20secondary%20re-masking%20to%20revisit%20uncertain%20predictions%20across%20refinement%20rounds%2C%20which%20improves%20consistency%20and%20enables%20robust%20error%20correction.%20This%20unified%20decoder%20preserves%20pre-trained%20vision-language%20priors%2C%20supports%20parallel%20decoding%2C%20breaks%20the%20autoregressive%20bottleneck%2C%20and%20reduces%20the%20number%20of%20function%20evaluations.%20Discrete%20Diffusion%20VLA%20achieves%2096.3%25%20avg.%20success%20rates%20on%20LIBERO%2C%2071.2%25%20visual%20matching%20on%20SimplerEnv-Fractal%20and%2054.2%25%20overall%20on%20SimplerEnv-Bridge.%20We%20also%20provide%20ablation%20study%20on%20vision-language%20ability%20retention%20on%20LIBERO-OOD%20%28Out-of-Distribution%29%20benchmark%2C%20with%20our%20method%20improving%20over%20autoregressive%2C%20MLP%20decoder%20and%20continuous%20diffusion%20baselines.%20These%20findings%20indicate%20that%20discrete-diffusion%20VLA%20supports%20precise%20action%20modeling%20and%20consistent%20training%2C%20laying%20groundwork%20for%20scaling%20VLA%20to%20larger%20models%20and%20datasets.%20Our%20code%20is%20available%20at%20https%3A//github.com/Liang-ZX/DiscreteDiffusionVLA/tree/libero.%0ALink%3A%20http%3A//arxiv.org/abs/2508.20072v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscrete%2520Diffusion%2520VLA%253A%2520Bringing%2520Discrete%2520Diffusion%2520to%2520Action%2520Decoding%2520in%2520Vision-Language-Action%2520Policies%26entry.906535625%3DZhixuan%2520Liang%2520and%2520Yizhuo%2520Li%2520and%2520Tianshuo%2520Yang%2520and%2520Chengyue%2520Wu%2520and%2520Sitong%2520Mao%2520and%2520Tian%2520Nian%2520and%2520Liuao%2520Pei%2520and%2520Shunbo%2520Zhou%2520and%2520Xiaokang%2520Yang%2520and%2520Jiangmiao%2520Pang%2520and%2520Yao%2520Mu%2520and%2520Ping%2520Luo%26entry.1292438233%3DVision-Language-Action%2520%2528VLA%2529%2520models%2520adapt%2520large%2520vision-language%2520backbones%2520to%2520map%2520images%2520and%2520instructions%2520into%2520robot%2520actions.%2520However%252C%2520prevailing%2520VLAs%2520either%2520generate%2520actions%2520auto-regressively%2520in%2520a%2520fixed%2520left-to-right%2520order%2520or%2520attach%2520separate%2520MLP%2520or%2520diffusion%2520heads%2520outside%2520the%2520backbone%252C%2520leading%2520to%2520fragmented%2520information%2520pathways%2520and%2520specialized%2520training%2520requirements%2520that%2520hinder%2520a%2520unified%252C%2520scalable%2520architecture.%2520We%2520present%2520Discrete%2520Diffusion%2520VLA%252C%2520a%2520unified-transformer%2520policy%2520that%2520models%2520discretized%2520action%2520chunks%2520with%2520discrete%2520diffusion.%2520The%2520design%2520retains%2520diffusion%2527s%2520progressive%2520refinement%2520paradigm%2520while%2520remaining%2520natively%2520compatible%2520with%2520the%2520discrete%2520token%2520interface%2520of%2520VLMs.%2520Our%2520method%2520achieves%2520an%2520adaptive%2520decoding%2520order%2520that%2520resolves%2520easy%2520action%2520elements%2520before%2520harder%2520ones%2520and%2520uses%2520secondary%2520re-masking%2520to%2520revisit%2520uncertain%2520predictions%2520across%2520refinement%2520rounds%252C%2520which%2520improves%2520consistency%2520and%2520enables%2520robust%2520error%2520correction.%2520This%2520unified%2520decoder%2520preserves%2520pre-trained%2520vision-language%2520priors%252C%2520supports%2520parallel%2520decoding%252C%2520breaks%2520the%2520autoregressive%2520bottleneck%252C%2520and%2520reduces%2520the%2520number%2520of%2520function%2520evaluations.%2520Discrete%2520Diffusion%2520VLA%2520achieves%252096.3%2525%2520avg.%2520success%2520rates%2520on%2520LIBERO%252C%252071.2%2525%2520visual%2520matching%2520on%2520SimplerEnv-Fractal%2520and%252054.2%2525%2520overall%2520on%2520SimplerEnv-Bridge.%2520We%2520also%2520provide%2520ablation%2520study%2520on%2520vision-language%2520ability%2520retention%2520on%2520LIBERO-OOD%2520%2528Out-of-Distribution%2529%2520benchmark%252C%2520with%2520our%2520method%2520improving%2520over%2520autoregressive%252C%2520MLP%2520decoder%2520and%2520continuous%2520diffusion%2520baselines.%2520These%2520findings%2520indicate%2520that%2520discrete-diffusion%2520VLA%2520supports%2520precise%2520action%2520modeling%2520and%2520consistent%2520training%252C%2520laying%2520groundwork%2520for%2520scaling%2520VLA%2520to%2520larger%2520models%2520and%2520datasets.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/Liang-ZX/DiscreteDiffusionVLA/tree/libero.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.20072v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discrete%20Diffusion%20VLA%3A%20Bringing%20Discrete%20Diffusion%20to%20Action%20Decoding%20in%20Vision-Language-Action%20Policies&entry.906535625=Zhixuan%20Liang%20and%20Yizhuo%20Li%20and%20Tianshuo%20Yang%20and%20Chengyue%20Wu%20and%20Sitong%20Mao%20and%20Tian%20Nian%20and%20Liuao%20Pei%20and%20Shunbo%20Zhou%20and%20Xiaokang%20Yang%20and%20Jiangmiao%20Pang%20and%20Yao%20Mu%20and%20Ping%20Luo&entry.1292438233=Vision-Language-Action%20%28VLA%29%20models%20adapt%20large%20vision-language%20backbones%20to%20map%20images%20and%20instructions%20into%20robot%20actions.%20However%2C%20prevailing%20VLAs%20either%20generate%20actions%20auto-regressively%20in%20a%20fixed%20left-to-right%20order%20or%20attach%20separate%20MLP%20or%20diffusion%20heads%20outside%20the%20backbone%2C%20leading%20to%20fragmented%20information%20pathways%20and%20specialized%20training%20requirements%20that%20hinder%20a%20unified%2C%20scalable%20architecture.%20We%20present%20Discrete%20Diffusion%20VLA%2C%20a%20unified-transformer%20policy%20that%20models%20discretized%20action%20chunks%20with%20discrete%20diffusion.%20The%20design%20retains%20diffusion%27s%20progressive%20refinement%20paradigm%20while%20remaining%20natively%20compatible%20with%20the%20discrete%20token%20interface%20of%20VLMs.%20Our%20method%20achieves%20an%20adaptive%20decoding%20order%20that%20resolves%20easy%20action%20elements%20before%20harder%20ones%20and%20uses%20secondary%20re-masking%20to%20revisit%20uncertain%20predictions%20across%20refinement%20rounds%2C%20which%20improves%20consistency%20and%20enables%20robust%20error%20correction.%20This%20unified%20decoder%20preserves%20pre-trained%20vision-language%20priors%2C%20supports%20parallel%20decoding%2C%20breaks%20the%20autoregressive%20bottleneck%2C%20and%20reduces%20the%20number%20of%20function%20evaluations.%20Discrete%20Diffusion%20VLA%20achieves%2096.3%25%20avg.%20success%20rates%20on%20LIBERO%2C%2071.2%25%20visual%20matching%20on%20SimplerEnv-Fractal%20and%2054.2%25%20overall%20on%20SimplerEnv-Bridge.%20We%20also%20provide%20ablation%20study%20on%20vision-language%20ability%20retention%20on%20LIBERO-OOD%20%28Out-of-Distribution%29%20benchmark%2C%20with%20our%20method%20improving%20over%20autoregressive%2C%20MLP%20decoder%20and%20continuous%20diffusion%20baselines.%20These%20findings%20indicate%20that%20discrete-diffusion%20VLA%20supports%20precise%20action%20modeling%20and%20consistent%20training%2C%20laying%20groundwork%20for%20scaling%20VLA%20to%20larger%20models%20and%20datasets.%20Our%20code%20is%20available%20at%20https%3A//github.com/Liang-ZX/DiscreteDiffusionVLA/tree/libero.&entry.1838667208=http%3A//arxiv.org/abs/2508.20072v3&entry.124074799=Read"},
{"title": "OM4OV: Leveraging Ontology Matching for Ontology Versioning", "author": "Zhangcheng Qiang and Kerry Taylor and Weiqing Wang", "abstract": "Due to the dynamic nature of the Semantic Web, version control is necessary to capture time-varying information for widely used ontologies. Despite the long-standing recognition of ontology versioning (OV) as a crucial component of efficient ontology management, many approaches treat OV as similar to ontology matching (OM) and directly reuse OM systems for OV tasks. In this study, we systematically analyse the similarities and differences between OM and OV and formalise the OM4OV pipeline. The pipeline is implemented and evaluated in the state-of-the-art OM system Agent-OM. The experimental results indicate that OM systems can be reused for OV tasks, but without necessary extensions, the current OM4OV pipeline can produce skewed measurements, poor performance in detecting update entities, and limited explainability for false mappings. To tackle these issues, we propose an optimisation method called the cross-reference (CR) mechanism, building upon the existing alignments from OM to reduce the number of matching candidates and improve overall OV performance.", "link": "http://arxiv.org/abs/2409.20302v6", "date": "2025-12-22", "relevancy": 2.2991, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4636}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4636}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OM4OV%3A%20Leveraging%20Ontology%20Matching%20for%20Ontology%20Versioning&body=Title%3A%20OM4OV%3A%20Leveraging%20Ontology%20Matching%20for%20Ontology%20Versioning%0AAuthor%3A%20Zhangcheng%20Qiang%20and%20Kerry%20Taylor%20and%20Weiqing%20Wang%0AAbstract%3A%20Due%20to%20the%20dynamic%20nature%20of%20the%20Semantic%20Web%2C%20version%20control%20is%20necessary%20to%20capture%20time-varying%20information%20for%20widely%20used%20ontologies.%20Despite%20the%20long-standing%20recognition%20of%20ontology%20versioning%20%28OV%29%20as%20a%20crucial%20component%20of%20efficient%20ontology%20management%2C%20many%20approaches%20treat%20OV%20as%20similar%20to%20ontology%20matching%20%28OM%29%20and%20directly%20reuse%20OM%20systems%20for%20OV%20tasks.%20In%20this%20study%2C%20we%20systematically%20analyse%20the%20similarities%20and%20differences%20between%20OM%20and%20OV%20and%20formalise%20the%20OM4OV%20pipeline.%20The%20pipeline%20is%20implemented%20and%20evaluated%20in%20the%20state-of-the-art%20OM%20system%20Agent-OM.%20The%20experimental%20results%20indicate%20that%20OM%20systems%20can%20be%20reused%20for%20OV%20tasks%2C%20but%20without%20necessary%20extensions%2C%20the%20current%20OM4OV%20pipeline%20can%20produce%20skewed%20measurements%2C%20poor%20performance%20in%20detecting%20update%20entities%2C%20and%20limited%20explainability%20for%20false%20mappings.%20To%20tackle%20these%20issues%2C%20we%20propose%20an%20optimisation%20method%20called%20the%20cross-reference%20%28CR%29%20mechanism%2C%20building%20upon%20the%20existing%20alignments%20from%20OM%20to%20reduce%20the%20number%20of%20matching%20candidates%20and%20improve%20overall%20OV%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2409.20302v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOM4OV%253A%2520Leveraging%2520Ontology%2520Matching%2520for%2520Ontology%2520Versioning%26entry.906535625%3DZhangcheng%2520Qiang%2520and%2520Kerry%2520Taylor%2520and%2520Weiqing%2520Wang%26entry.1292438233%3DDue%2520to%2520the%2520dynamic%2520nature%2520of%2520the%2520Semantic%2520Web%252C%2520version%2520control%2520is%2520necessary%2520to%2520capture%2520time-varying%2520information%2520for%2520widely%2520used%2520ontologies.%2520Despite%2520the%2520long-standing%2520recognition%2520of%2520ontology%2520versioning%2520%2528OV%2529%2520as%2520a%2520crucial%2520component%2520of%2520efficient%2520ontology%2520management%252C%2520many%2520approaches%2520treat%2520OV%2520as%2520similar%2520to%2520ontology%2520matching%2520%2528OM%2529%2520and%2520directly%2520reuse%2520OM%2520systems%2520for%2520OV%2520tasks.%2520In%2520this%2520study%252C%2520we%2520systematically%2520analyse%2520the%2520similarities%2520and%2520differences%2520between%2520OM%2520and%2520OV%2520and%2520formalise%2520the%2520OM4OV%2520pipeline.%2520The%2520pipeline%2520is%2520implemented%2520and%2520evaluated%2520in%2520the%2520state-of-the-art%2520OM%2520system%2520Agent-OM.%2520The%2520experimental%2520results%2520indicate%2520that%2520OM%2520systems%2520can%2520be%2520reused%2520for%2520OV%2520tasks%252C%2520but%2520without%2520necessary%2520extensions%252C%2520the%2520current%2520OM4OV%2520pipeline%2520can%2520produce%2520skewed%2520measurements%252C%2520poor%2520performance%2520in%2520detecting%2520update%2520entities%252C%2520and%2520limited%2520explainability%2520for%2520false%2520mappings.%2520To%2520tackle%2520these%2520issues%252C%2520we%2520propose%2520an%2520optimisation%2520method%2520called%2520the%2520cross-reference%2520%2528CR%2529%2520mechanism%252C%2520building%2520upon%2520the%2520existing%2520alignments%2520from%2520OM%2520to%2520reduce%2520the%2520number%2520of%2520matching%2520candidates%2520and%2520improve%2520overall%2520OV%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.20302v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OM4OV%3A%20Leveraging%20Ontology%20Matching%20for%20Ontology%20Versioning&entry.906535625=Zhangcheng%20Qiang%20and%20Kerry%20Taylor%20and%20Weiqing%20Wang&entry.1292438233=Due%20to%20the%20dynamic%20nature%20of%20the%20Semantic%20Web%2C%20version%20control%20is%20necessary%20to%20capture%20time-varying%20information%20for%20widely%20used%20ontologies.%20Despite%20the%20long-standing%20recognition%20of%20ontology%20versioning%20%28OV%29%20as%20a%20crucial%20component%20of%20efficient%20ontology%20management%2C%20many%20approaches%20treat%20OV%20as%20similar%20to%20ontology%20matching%20%28OM%29%20and%20directly%20reuse%20OM%20systems%20for%20OV%20tasks.%20In%20this%20study%2C%20we%20systematically%20analyse%20the%20similarities%20and%20differences%20between%20OM%20and%20OV%20and%20formalise%20the%20OM4OV%20pipeline.%20The%20pipeline%20is%20implemented%20and%20evaluated%20in%20the%20state-of-the-art%20OM%20system%20Agent-OM.%20The%20experimental%20results%20indicate%20that%20OM%20systems%20can%20be%20reused%20for%20OV%20tasks%2C%20but%20without%20necessary%20extensions%2C%20the%20current%20OM4OV%20pipeline%20can%20produce%20skewed%20measurements%2C%20poor%20performance%20in%20detecting%20update%20entities%2C%20and%20limited%20explainability%20for%20false%20mappings.%20To%20tackle%20these%20issues%2C%20we%20propose%20an%20optimisation%20method%20called%20the%20cross-reference%20%28CR%29%20mechanism%2C%20building%20upon%20the%20existing%20alignments%20from%20OM%20to%20reduce%20the%20number%20of%20matching%20candidates%20and%20improve%20overall%20OV%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2409.20302v6&entry.124074799=Read"},
{"title": "PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code", "author": "Itay Dreyfuss and Antonio Abu Nassar and Samuel Ackerman and Axel Ben David and Eitan Farchi and Rami Katan and Orna Raz and Marcel Zalmanovici", "abstract": "Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks.", "link": "http://arxiv.org/abs/2512.10713v2", "date": "2025-12-22", "relevancy": 2.2864, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4693}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4513}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PACIFIC%3A%20a%20framework%20for%20generating%20benchmarks%20to%20check%20Precise%20Automatically%20Checked%20Instruction%20Following%20In%20Code&body=Title%3A%20PACIFIC%3A%20a%20framework%20for%20generating%20benchmarks%20to%20check%20Precise%20Automatically%20Checked%20Instruction%20Following%20In%20Code%0AAuthor%3A%20Itay%20Dreyfuss%20and%20Antonio%20Abu%20Nassar%20and%20Samuel%20Ackerman%20and%20Axel%20Ben%20David%20and%20Eitan%20Farchi%20and%20Rami%20Katan%20and%20Orna%20Raz%20and%20Marcel%20Zalmanovici%0AAbstract%3A%20Large%20Language%20Model%20%28LLM%29-based%20code%20assistants%20have%20emerged%20as%20a%20powerful%20application%20of%20generative%20AI%2C%20demonstrating%20impressive%20capabilities%20in%20code%20generation%20and%20comprehension.%20A%20key%20requirement%20for%20these%20systems%20is%20their%20ability%20to%20accurately%20follow%20user%20instructions.%20We%20present%20Precise%20Automatically%20Checked%20Instruction%20Following%20In%20Code%20%28PACIFIC%29%2C%20a%20novel%20framework%20designed%20to%20automatically%20generate%20benchmarks%20that%20rigorously%20assess%20sequential%20instruction-following%20and%20code%20dry-running%20capabilities%20in%20LLMs%2C%20while%20allowing%20control%20over%20benchmark%20difficulty.%20PACIFIC%20produces%20benchmark%20variants%20with%20clearly%20defined%20expected%20outputs%2C%20enabling%20straightforward%20and%20reliable%20evaluation%20through%20simple%20output%20comparisons.%20In%20contrast%20to%20existing%20approaches%20that%20often%20rely%20on%20tool%20usage%20or%20agentic%20behavior%2C%20our%20work%20isolates%20and%20evaluates%20the%20LLM%27s%20intrinsic%20ability%20to%20reason%20through%20code%20behavior%20step-by-step%20without%20execution%20%28dry%20running%29%20and%20to%20follow%20instructions.%20Furthermore%2C%20our%20framework%20mitigates%20training%20data%20contamination%20by%20facilitating%20effortless%20generation%20of%20novel%20benchmark%20variations.%20We%20validate%20our%20framework%20by%20generating%20a%20suite%20of%20benchmarks%20spanning%20a%20range%20of%20difficulty%20levels%20and%20evaluating%20multiple%20state-of-the-art%20LLMs.%20Our%20results%20demonstrate%20that%20PACIFIC%20can%20produce%20increasingly%20challenging%20benchmarks%20that%20effectively%20differentiate%20instruction-following%20and%20dry%20running%20capabilities%2C%20even%20among%20advanced%20models.%20Overall%2C%20our%20framework%20offers%20a%20scalable%2C%20contamination-resilient%20methodology%20for%20assessing%20core%20competencies%20of%20LLMs%20in%20code-related%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.10713v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPACIFIC%253A%2520a%2520framework%2520for%2520generating%2520benchmarks%2520to%2520check%2520Precise%2520Automatically%2520Checked%2520Instruction%2520Following%2520In%2520Code%26entry.906535625%3DItay%2520Dreyfuss%2520and%2520Antonio%2520Abu%2520Nassar%2520and%2520Samuel%2520Ackerman%2520and%2520Axel%2520Ben%2520David%2520and%2520Eitan%2520Farchi%2520and%2520Rami%2520Katan%2520and%2520Orna%2520Raz%2520and%2520Marcel%2520Zalmanovici%26entry.1292438233%3DLarge%2520Language%2520Model%2520%2528LLM%2529-based%2520code%2520assistants%2520have%2520emerged%2520as%2520a%2520powerful%2520application%2520of%2520generative%2520AI%252C%2520demonstrating%2520impressive%2520capabilities%2520in%2520code%2520generation%2520and%2520comprehension.%2520A%2520key%2520requirement%2520for%2520these%2520systems%2520is%2520their%2520ability%2520to%2520accurately%2520follow%2520user%2520instructions.%2520We%2520present%2520Precise%2520Automatically%2520Checked%2520Instruction%2520Following%2520In%2520Code%2520%2528PACIFIC%2529%252C%2520a%2520novel%2520framework%2520designed%2520to%2520automatically%2520generate%2520benchmarks%2520that%2520rigorously%2520assess%2520sequential%2520instruction-following%2520and%2520code%2520dry-running%2520capabilities%2520in%2520LLMs%252C%2520while%2520allowing%2520control%2520over%2520benchmark%2520difficulty.%2520PACIFIC%2520produces%2520benchmark%2520variants%2520with%2520clearly%2520defined%2520expected%2520outputs%252C%2520enabling%2520straightforward%2520and%2520reliable%2520evaluation%2520through%2520simple%2520output%2520comparisons.%2520In%2520contrast%2520to%2520existing%2520approaches%2520that%2520often%2520rely%2520on%2520tool%2520usage%2520or%2520agentic%2520behavior%252C%2520our%2520work%2520isolates%2520and%2520evaluates%2520the%2520LLM%2527s%2520intrinsic%2520ability%2520to%2520reason%2520through%2520code%2520behavior%2520step-by-step%2520without%2520execution%2520%2528dry%2520running%2529%2520and%2520to%2520follow%2520instructions.%2520Furthermore%252C%2520our%2520framework%2520mitigates%2520training%2520data%2520contamination%2520by%2520facilitating%2520effortless%2520generation%2520of%2520novel%2520benchmark%2520variations.%2520We%2520validate%2520our%2520framework%2520by%2520generating%2520a%2520suite%2520of%2520benchmarks%2520spanning%2520a%2520range%2520of%2520difficulty%2520levels%2520and%2520evaluating%2520multiple%2520state-of-the-art%2520LLMs.%2520Our%2520results%2520demonstrate%2520that%2520PACIFIC%2520can%2520produce%2520increasingly%2520challenging%2520benchmarks%2520that%2520effectively%2520differentiate%2520instruction-following%2520and%2520dry%2520running%2520capabilities%252C%2520even%2520among%2520advanced%2520models.%2520Overall%252C%2520our%2520framework%2520offers%2520a%2520scalable%252C%2520contamination-resilient%2520methodology%2520for%2520assessing%2520core%2520competencies%2520of%2520LLMs%2520in%2520code-related%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.10713v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PACIFIC%3A%20a%20framework%20for%20generating%20benchmarks%20to%20check%20Precise%20Automatically%20Checked%20Instruction%20Following%20In%20Code&entry.906535625=Itay%20Dreyfuss%20and%20Antonio%20Abu%20Nassar%20and%20Samuel%20Ackerman%20and%20Axel%20Ben%20David%20and%20Eitan%20Farchi%20and%20Rami%20Katan%20and%20Orna%20Raz%20and%20Marcel%20Zalmanovici&entry.1292438233=Large%20Language%20Model%20%28LLM%29-based%20code%20assistants%20have%20emerged%20as%20a%20powerful%20application%20of%20generative%20AI%2C%20demonstrating%20impressive%20capabilities%20in%20code%20generation%20and%20comprehension.%20A%20key%20requirement%20for%20these%20systems%20is%20their%20ability%20to%20accurately%20follow%20user%20instructions.%20We%20present%20Precise%20Automatically%20Checked%20Instruction%20Following%20In%20Code%20%28PACIFIC%29%2C%20a%20novel%20framework%20designed%20to%20automatically%20generate%20benchmarks%20that%20rigorously%20assess%20sequential%20instruction-following%20and%20code%20dry-running%20capabilities%20in%20LLMs%2C%20while%20allowing%20control%20over%20benchmark%20difficulty.%20PACIFIC%20produces%20benchmark%20variants%20with%20clearly%20defined%20expected%20outputs%2C%20enabling%20straightforward%20and%20reliable%20evaluation%20through%20simple%20output%20comparisons.%20In%20contrast%20to%20existing%20approaches%20that%20often%20rely%20on%20tool%20usage%20or%20agentic%20behavior%2C%20our%20work%20isolates%20and%20evaluates%20the%20LLM%27s%20intrinsic%20ability%20to%20reason%20through%20code%20behavior%20step-by-step%20without%20execution%20%28dry%20running%29%20and%20to%20follow%20instructions.%20Furthermore%2C%20our%20framework%20mitigates%20training%20data%20contamination%20by%20facilitating%20effortless%20generation%20of%20novel%20benchmark%20variations.%20We%20validate%20our%20framework%20by%20generating%20a%20suite%20of%20benchmarks%20spanning%20a%20range%20of%20difficulty%20levels%20and%20evaluating%20multiple%20state-of-the-art%20LLMs.%20Our%20results%20demonstrate%20that%20PACIFIC%20can%20produce%20increasingly%20challenging%20benchmarks%20that%20effectively%20differentiate%20instruction-following%20and%20dry%20running%20capabilities%2C%20even%20among%20advanced%20models.%20Overall%2C%20our%20framework%20offers%20a%20scalable%2C%20contamination-resilient%20methodology%20for%20assessing%20core%20competencies%20of%20LLMs%20in%20code-related%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2512.10713v2&entry.124074799=Read"},
{"title": "BRIC: Bridging Kinematic Plans and Physical Control at Test Time", "author": "Dohun Lim and Minji Kim and Jaewoon Lim and Sungchan Kim", "abstract": "We propose BRIC, a novel test-time adaptation (TTA) framework that enables long-term human motion generation by resolving execution discrepancies between diffusion-based kinematic motion planners and reinforcement learning-based physics controllers. While diffusion models can generate diverse and expressive motions conditioned on text and scene context, they often produce physically implausible outputs, leading to execution drift during simulation. To address this, BRIC dynamically adapts the physics controller to noisy motion plans at test time, while preserving pre-trained skills via a loss function that mitigates catastrophic forgetting. In addition, BRIC introduces a lightweight test-time guidance mechanism that steers the diffusion model in the signal space without updating its parameters. By combining both adaptation strategies, BRIC ensures consistent and physically plausible long-term executions across diverse environments in an effective and efficient manner. We validate the effectiveness of BRIC on a variety of long-term tasks, including motion composition, obstacle avoidance, and human-scene interaction, achieving state-of-the-art performance across all tasks.", "link": "http://arxiv.org/abs/2511.20431v3", "date": "2025-12-22", "relevancy": 2.2806, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5875}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5846}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BRIC%3A%20Bridging%20Kinematic%20Plans%20and%20Physical%20Control%20at%20Test%20Time&body=Title%3A%20BRIC%3A%20Bridging%20Kinematic%20Plans%20and%20Physical%20Control%20at%20Test%20Time%0AAuthor%3A%20Dohun%20Lim%20and%20Minji%20Kim%20and%20Jaewoon%20Lim%20and%20Sungchan%20Kim%0AAbstract%3A%20We%20propose%20BRIC%2C%20a%20novel%20test-time%20adaptation%20%28TTA%29%20framework%20that%20enables%20long-term%20human%20motion%20generation%20by%20resolving%20execution%20discrepancies%20between%20diffusion-based%20kinematic%20motion%20planners%20and%20reinforcement%20learning-based%20physics%20controllers.%20While%20diffusion%20models%20can%20generate%20diverse%20and%20expressive%20motions%20conditioned%20on%20text%20and%20scene%20context%2C%20they%20often%20produce%20physically%20implausible%20outputs%2C%20leading%20to%20execution%20drift%20during%20simulation.%20To%20address%20this%2C%20BRIC%20dynamically%20adapts%20the%20physics%20controller%20to%20noisy%20motion%20plans%20at%20test%20time%2C%20while%20preserving%20pre-trained%20skills%20via%20a%20loss%20function%20that%20mitigates%20catastrophic%20forgetting.%20In%20addition%2C%20BRIC%20introduces%20a%20lightweight%20test-time%20guidance%20mechanism%20that%20steers%20the%20diffusion%20model%20in%20the%20signal%20space%20without%20updating%20its%20parameters.%20By%20combining%20both%20adaptation%20strategies%2C%20BRIC%20ensures%20consistent%20and%20physically%20plausible%20long-term%20executions%20across%20diverse%20environments%20in%20an%20effective%20and%20efficient%20manner.%20We%20validate%20the%20effectiveness%20of%20BRIC%20on%20a%20variety%20of%20long-term%20tasks%2C%20including%20motion%20composition%2C%20obstacle%20avoidance%2C%20and%20human-scene%20interaction%2C%20achieving%20state-of-the-art%20performance%20across%20all%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2511.20431v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBRIC%253A%2520Bridging%2520Kinematic%2520Plans%2520and%2520Physical%2520Control%2520at%2520Test%2520Time%26entry.906535625%3DDohun%2520Lim%2520and%2520Minji%2520Kim%2520and%2520Jaewoon%2520Lim%2520and%2520Sungchan%2520Kim%26entry.1292438233%3DWe%2520propose%2520BRIC%252C%2520a%2520novel%2520test-time%2520adaptation%2520%2528TTA%2529%2520framework%2520that%2520enables%2520long-term%2520human%2520motion%2520generation%2520by%2520resolving%2520execution%2520discrepancies%2520between%2520diffusion-based%2520kinematic%2520motion%2520planners%2520and%2520reinforcement%2520learning-based%2520physics%2520controllers.%2520While%2520diffusion%2520models%2520can%2520generate%2520diverse%2520and%2520expressive%2520motions%2520conditioned%2520on%2520text%2520and%2520scene%2520context%252C%2520they%2520often%2520produce%2520physically%2520implausible%2520outputs%252C%2520leading%2520to%2520execution%2520drift%2520during%2520simulation.%2520To%2520address%2520this%252C%2520BRIC%2520dynamically%2520adapts%2520the%2520physics%2520controller%2520to%2520noisy%2520motion%2520plans%2520at%2520test%2520time%252C%2520while%2520preserving%2520pre-trained%2520skills%2520via%2520a%2520loss%2520function%2520that%2520mitigates%2520catastrophic%2520forgetting.%2520In%2520addition%252C%2520BRIC%2520introduces%2520a%2520lightweight%2520test-time%2520guidance%2520mechanism%2520that%2520steers%2520the%2520diffusion%2520model%2520in%2520the%2520signal%2520space%2520without%2520updating%2520its%2520parameters.%2520By%2520combining%2520both%2520adaptation%2520strategies%252C%2520BRIC%2520ensures%2520consistent%2520and%2520physically%2520plausible%2520long-term%2520executions%2520across%2520diverse%2520environments%2520in%2520an%2520effective%2520and%2520efficient%2520manner.%2520We%2520validate%2520the%2520effectiveness%2520of%2520BRIC%2520on%2520a%2520variety%2520of%2520long-term%2520tasks%252C%2520including%2520motion%2520composition%252C%2520obstacle%2520avoidance%252C%2520and%2520human-scene%2520interaction%252C%2520achieving%2520state-of-the-art%2520performance%2520across%2520all%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.20431v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BRIC%3A%20Bridging%20Kinematic%20Plans%20and%20Physical%20Control%20at%20Test%20Time&entry.906535625=Dohun%20Lim%20and%20Minji%20Kim%20and%20Jaewoon%20Lim%20and%20Sungchan%20Kim&entry.1292438233=We%20propose%20BRIC%2C%20a%20novel%20test-time%20adaptation%20%28TTA%29%20framework%20that%20enables%20long-term%20human%20motion%20generation%20by%20resolving%20execution%20discrepancies%20between%20diffusion-based%20kinematic%20motion%20planners%20and%20reinforcement%20learning-based%20physics%20controllers.%20While%20diffusion%20models%20can%20generate%20diverse%20and%20expressive%20motions%20conditioned%20on%20text%20and%20scene%20context%2C%20they%20often%20produce%20physically%20implausible%20outputs%2C%20leading%20to%20execution%20drift%20during%20simulation.%20To%20address%20this%2C%20BRIC%20dynamically%20adapts%20the%20physics%20controller%20to%20noisy%20motion%20plans%20at%20test%20time%2C%20while%20preserving%20pre-trained%20skills%20via%20a%20loss%20function%20that%20mitigates%20catastrophic%20forgetting.%20In%20addition%2C%20BRIC%20introduces%20a%20lightweight%20test-time%20guidance%20mechanism%20that%20steers%20the%20diffusion%20model%20in%20the%20signal%20space%20without%20updating%20its%20parameters.%20By%20combining%20both%20adaptation%20strategies%2C%20BRIC%20ensures%20consistent%20and%20physically%20plausible%20long-term%20executions%20across%20diverse%20environments%20in%20an%20effective%20and%20efficient%20manner.%20We%20validate%20the%20effectiveness%20of%20BRIC%20on%20a%20variety%20of%20long-term%20tasks%2C%20including%20motion%20composition%2C%20obstacle%20avoidance%2C%20and%20human-scene%20interaction%2C%20achieving%20state-of-the-art%20performance%20across%20all%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2511.20431v3&entry.124074799=Read"},
{"title": "StoryMem: Multi-shot Long Video Storytelling with Memory", "author": "Kaiwen Zhang and Liming Jiang and Angtian Wang and Jacob Zhiyuan Fang and Tiancheng Zhi and Qing Yan and Hao Kang and Xin Lu and Xingang Pan", "abstract": "Visual storytelling requires generating multi-shot videos with cinematic quality and long-range consistency. Inspired by human memory, we propose StoryMem, a paradigm that reformulates long-form video storytelling as iterative shot synthesis conditioned on explicit visual memory, transforming pre-trained single-shot video diffusion models into multi-shot storytellers. This is achieved by a novel Memory-to-Video (M2V) design, which maintains a compact and dynamically updated memory bank of keyframes from historical generated shots. The stored memory is then injected into single-shot video diffusion models via latent concatenation and negative RoPE shifts with only LoRA fine-tuning. A semantic keyframe selection strategy, together with aesthetic preference filtering, further ensures informative and stable memory throughout generation. Moreover, the proposed framework naturally accommodates smooth shot transitions and customized story generation applications. To facilitate evaluation, we introduce ST-Bench, a diverse benchmark for multi-shot video storytelling. Extensive experiments demonstrate that StoryMem achieves superior cross-shot consistency over previous methods while preserving high aesthetic quality and prompt adherence, marking a significant step toward coherent minute-long video storytelling.", "link": "http://arxiv.org/abs/2512.19539v1", "date": "2025-12-22", "relevancy": 2.2775, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6078}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5828}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StoryMem%3A%20Multi-shot%20Long%20Video%20Storytelling%20with%20Memory&body=Title%3A%20StoryMem%3A%20Multi-shot%20Long%20Video%20Storytelling%20with%20Memory%0AAuthor%3A%20Kaiwen%20Zhang%20and%20Liming%20Jiang%20and%20Angtian%20Wang%20and%20Jacob%20Zhiyuan%20Fang%20and%20Tiancheng%20Zhi%20and%20Qing%20Yan%20and%20Hao%20Kang%20and%20Xin%20Lu%20and%20Xingang%20Pan%0AAbstract%3A%20Visual%20storytelling%20requires%20generating%20multi-shot%20videos%20with%20cinematic%20quality%20and%20long-range%20consistency.%20Inspired%20by%20human%20memory%2C%20we%20propose%20StoryMem%2C%20a%20paradigm%20that%20reformulates%20long-form%20video%20storytelling%20as%20iterative%20shot%20synthesis%20conditioned%20on%20explicit%20visual%20memory%2C%20transforming%20pre-trained%20single-shot%20video%20diffusion%20models%20into%20multi-shot%20storytellers.%20This%20is%20achieved%20by%20a%20novel%20Memory-to-Video%20%28M2V%29%20design%2C%20which%20maintains%20a%20compact%20and%20dynamically%20updated%20memory%20bank%20of%20keyframes%20from%20historical%20generated%20shots.%20The%20stored%20memory%20is%20then%20injected%20into%20single-shot%20video%20diffusion%20models%20via%20latent%20concatenation%20and%20negative%20RoPE%20shifts%20with%20only%20LoRA%20fine-tuning.%20A%20semantic%20keyframe%20selection%20strategy%2C%20together%20with%20aesthetic%20preference%20filtering%2C%20further%20ensures%20informative%20and%20stable%20memory%20throughout%20generation.%20Moreover%2C%20the%20proposed%20framework%20naturally%20accommodates%20smooth%20shot%20transitions%20and%20customized%20story%20generation%20applications.%20To%20facilitate%20evaluation%2C%20we%20introduce%20ST-Bench%2C%20a%20diverse%20benchmark%20for%20multi-shot%20video%20storytelling.%20Extensive%20experiments%20demonstrate%20that%20StoryMem%20achieves%20superior%20cross-shot%20consistency%20over%20previous%20methods%20while%20preserving%20high%20aesthetic%20quality%20and%20prompt%20adherence%2C%20marking%20a%20significant%20step%20toward%20coherent%20minute-long%20video%20storytelling.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19539v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStoryMem%253A%2520Multi-shot%2520Long%2520Video%2520Storytelling%2520with%2520Memory%26entry.906535625%3DKaiwen%2520Zhang%2520and%2520Liming%2520Jiang%2520and%2520Angtian%2520Wang%2520and%2520Jacob%2520Zhiyuan%2520Fang%2520and%2520Tiancheng%2520Zhi%2520and%2520Qing%2520Yan%2520and%2520Hao%2520Kang%2520and%2520Xin%2520Lu%2520and%2520Xingang%2520Pan%26entry.1292438233%3DVisual%2520storytelling%2520requires%2520generating%2520multi-shot%2520videos%2520with%2520cinematic%2520quality%2520and%2520long-range%2520consistency.%2520Inspired%2520by%2520human%2520memory%252C%2520we%2520propose%2520StoryMem%252C%2520a%2520paradigm%2520that%2520reformulates%2520long-form%2520video%2520storytelling%2520as%2520iterative%2520shot%2520synthesis%2520conditioned%2520on%2520explicit%2520visual%2520memory%252C%2520transforming%2520pre-trained%2520single-shot%2520video%2520diffusion%2520models%2520into%2520multi-shot%2520storytellers.%2520This%2520is%2520achieved%2520by%2520a%2520novel%2520Memory-to-Video%2520%2528M2V%2529%2520design%252C%2520which%2520maintains%2520a%2520compact%2520and%2520dynamically%2520updated%2520memory%2520bank%2520of%2520keyframes%2520from%2520historical%2520generated%2520shots.%2520The%2520stored%2520memory%2520is%2520then%2520injected%2520into%2520single-shot%2520video%2520diffusion%2520models%2520via%2520latent%2520concatenation%2520and%2520negative%2520RoPE%2520shifts%2520with%2520only%2520LoRA%2520fine-tuning.%2520A%2520semantic%2520keyframe%2520selection%2520strategy%252C%2520together%2520with%2520aesthetic%2520preference%2520filtering%252C%2520further%2520ensures%2520informative%2520and%2520stable%2520memory%2520throughout%2520generation.%2520Moreover%252C%2520the%2520proposed%2520framework%2520naturally%2520accommodates%2520smooth%2520shot%2520transitions%2520and%2520customized%2520story%2520generation%2520applications.%2520To%2520facilitate%2520evaluation%252C%2520we%2520introduce%2520ST-Bench%252C%2520a%2520diverse%2520benchmark%2520for%2520multi-shot%2520video%2520storytelling.%2520Extensive%2520experiments%2520demonstrate%2520that%2520StoryMem%2520achieves%2520superior%2520cross-shot%2520consistency%2520over%2520previous%2520methods%2520while%2520preserving%2520high%2520aesthetic%2520quality%2520and%2520prompt%2520adherence%252C%2520marking%2520a%2520significant%2520step%2520toward%2520coherent%2520minute-long%2520video%2520storytelling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19539v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StoryMem%3A%20Multi-shot%20Long%20Video%20Storytelling%20with%20Memory&entry.906535625=Kaiwen%20Zhang%20and%20Liming%20Jiang%20and%20Angtian%20Wang%20and%20Jacob%20Zhiyuan%20Fang%20and%20Tiancheng%20Zhi%20and%20Qing%20Yan%20and%20Hao%20Kang%20and%20Xin%20Lu%20and%20Xingang%20Pan&entry.1292438233=Visual%20storytelling%20requires%20generating%20multi-shot%20videos%20with%20cinematic%20quality%20and%20long-range%20consistency.%20Inspired%20by%20human%20memory%2C%20we%20propose%20StoryMem%2C%20a%20paradigm%20that%20reformulates%20long-form%20video%20storytelling%20as%20iterative%20shot%20synthesis%20conditioned%20on%20explicit%20visual%20memory%2C%20transforming%20pre-trained%20single-shot%20video%20diffusion%20models%20into%20multi-shot%20storytellers.%20This%20is%20achieved%20by%20a%20novel%20Memory-to-Video%20%28M2V%29%20design%2C%20which%20maintains%20a%20compact%20and%20dynamically%20updated%20memory%20bank%20of%20keyframes%20from%20historical%20generated%20shots.%20The%20stored%20memory%20is%20then%20injected%20into%20single-shot%20video%20diffusion%20models%20via%20latent%20concatenation%20and%20negative%20RoPE%20shifts%20with%20only%20LoRA%20fine-tuning.%20A%20semantic%20keyframe%20selection%20strategy%2C%20together%20with%20aesthetic%20preference%20filtering%2C%20further%20ensures%20informative%20and%20stable%20memory%20throughout%20generation.%20Moreover%2C%20the%20proposed%20framework%20naturally%20accommodates%20smooth%20shot%20transitions%20and%20customized%20story%20generation%20applications.%20To%20facilitate%20evaluation%2C%20we%20introduce%20ST-Bench%2C%20a%20diverse%20benchmark%20for%20multi-shot%20video%20storytelling.%20Extensive%20experiments%20demonstrate%20that%20StoryMem%20achieves%20superior%20cross-shot%20consistency%20over%20previous%20methods%20while%20preserving%20high%20aesthetic%20quality%20and%20prompt%20adherence%2C%20marking%20a%20significant%20step%20toward%20coherent%20minute-long%20video%20storytelling.&entry.1838667208=http%3A//arxiv.org/abs/2512.19539v1&entry.124074799=Read"},
{"title": "Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation", "author": "Ziyang Song and Zelin Zang and Zuyao Chen and Xusheng Liang and Dong Yi and Jinlin Wu and Hongbin Liu and Jiebo Luo", "abstract": "Multimodal Large Language Models (MLLMs) have achieved impressive progress in natural image reasoning, yet their potential in medical imaging remains underexplored, especially in clinical anatomical surgical images. Anatomy understanding tasks demand precise understanding and clinically coherent answers, which are difficult to achieve due to the complexity of medical data and the scarcity of high-quality expert annotations. These challenges limit the effectiveness of conventional Supervised Fine-Tuning (SFT) strategies. While recent work has demonstrated that Group Relative Policy Optimization (GRPO) can enhance reasoning in MLLMs without relying on large amounts of data, we find two weaknesses that hinder GRPO's reasoning performance in anatomy recognition: 1) knowledge cannot be effectively shared between different anatomical structures, resulting in uneven information gain and preventing the model from converging, and 2) the model quickly converges to a single reasoning path, suppressing the exploration of diverse strategies. To overcome these challenges, we propose two novel methods. First, we implement a progressive learning strategy called Anatomical Similarity Curriculum Learning by controlling question difficulty via the similarity of answer choices, enabling the model to master complex problems incrementally. Second, we utilize question augmentation referred to as Group Diversity Question Augmentation to expand the model's search space for difficult queries, mitigating the tendency to produce uniform responses. Comprehensive experiments on the SGG-VQA and OmniMedVQA benchmarks show our method achieves a significant improvement across the two benchmarks, demonstrating its effectiveness in enhancing the medical reasoning capabilities of MLLMs. The code can be found in https://github.com/tomato996/Anatomy-R1", "link": "http://arxiv.org/abs/2512.19512v1", "date": "2025-12-22", "relevancy": 2.2714, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5876}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5564}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Anatomy-R1%3A%20Enhancing%20Anatomy%20Reasoning%20in%20Multimodal%20Large%20Language%20Models%20via%20Anatomical%20Similarity%20Curriculum%20and%20Group%20Diversity%20Augmentation&body=Title%3A%20Anatomy-R1%3A%20Enhancing%20Anatomy%20Reasoning%20in%20Multimodal%20Large%20Language%20Models%20via%20Anatomical%20Similarity%20Curriculum%20and%20Group%20Diversity%20Augmentation%0AAuthor%3A%20Ziyang%20Song%20and%20Zelin%20Zang%20and%20Zuyao%20Chen%20and%20Xusheng%20Liang%20and%20Dong%20Yi%20and%20Jinlin%20Wu%20and%20Hongbin%20Liu%20and%20Jiebo%20Luo%0AAbstract%3A%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20impressive%20progress%20in%20natural%20image%20reasoning%2C%20yet%20their%20potential%20in%20medical%20imaging%20remains%20underexplored%2C%20especially%20in%20clinical%20anatomical%20surgical%20images.%20Anatomy%20understanding%20tasks%20demand%20precise%20understanding%20and%20clinically%20coherent%20answers%2C%20which%20are%20difficult%20to%20achieve%20due%20to%20the%20complexity%20of%20medical%20data%20and%20the%20scarcity%20of%20high-quality%20expert%20annotations.%20These%20challenges%20limit%20the%20effectiveness%20of%20conventional%20Supervised%20Fine-Tuning%20%28SFT%29%20strategies.%20While%20recent%20work%20has%20demonstrated%20that%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20can%20enhance%20reasoning%20in%20MLLMs%20without%20relying%20on%20large%20amounts%20of%20data%2C%20we%20find%20two%20weaknesses%20that%20hinder%20GRPO%27s%20reasoning%20performance%20in%20anatomy%20recognition%3A%201%29%20knowledge%20cannot%20be%20effectively%20shared%20between%20different%20anatomical%20structures%2C%20resulting%20in%20uneven%20information%20gain%20and%20preventing%20the%20model%20from%20converging%2C%20and%202%29%20the%20model%20quickly%20converges%20to%20a%20single%20reasoning%20path%2C%20suppressing%20the%20exploration%20of%20diverse%20strategies.%20To%20overcome%20these%20challenges%2C%20we%20propose%20two%20novel%20methods.%20First%2C%20we%20implement%20a%20progressive%20learning%20strategy%20called%20Anatomical%20Similarity%20Curriculum%20Learning%20by%20controlling%20question%20difficulty%20via%20the%20similarity%20of%20answer%20choices%2C%20enabling%20the%20model%20to%20master%20complex%20problems%20incrementally.%20Second%2C%20we%20utilize%20question%20augmentation%20referred%20to%20as%20Group%20Diversity%20Question%20Augmentation%20to%20expand%20the%20model%27s%20search%20space%20for%20difficult%20queries%2C%20mitigating%20the%20tendency%20to%20produce%20uniform%20responses.%20Comprehensive%20experiments%20on%20the%20SGG-VQA%20and%20OmniMedVQA%20benchmarks%20show%20our%20method%20achieves%20a%20significant%20improvement%20across%20the%20two%20benchmarks%2C%20demonstrating%20its%20effectiveness%20in%20enhancing%20the%20medical%20reasoning%20capabilities%20of%20MLLMs.%20The%20code%20can%20be%20found%20in%20https%3A//github.com/tomato996/Anatomy-R1%0ALink%3A%20http%3A//arxiv.org/abs/2512.19512v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnatomy-R1%253A%2520Enhancing%2520Anatomy%2520Reasoning%2520in%2520Multimodal%2520Large%2520Language%2520Models%2520via%2520Anatomical%2520Similarity%2520Curriculum%2520and%2520Group%2520Diversity%2520Augmentation%26entry.906535625%3DZiyang%2520Song%2520and%2520Zelin%2520Zang%2520and%2520Zuyao%2520Chen%2520and%2520Xusheng%2520Liang%2520and%2520Dong%2520Yi%2520and%2520Jinlin%2520Wu%2520and%2520Hongbin%2520Liu%2520and%2520Jiebo%2520Luo%26entry.1292438233%3DMultimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520achieved%2520impressive%2520progress%2520in%2520natural%2520image%2520reasoning%252C%2520yet%2520their%2520potential%2520in%2520medical%2520imaging%2520remains%2520underexplored%252C%2520especially%2520in%2520clinical%2520anatomical%2520surgical%2520images.%2520Anatomy%2520understanding%2520tasks%2520demand%2520precise%2520understanding%2520and%2520clinically%2520coherent%2520answers%252C%2520which%2520are%2520difficult%2520to%2520achieve%2520due%2520to%2520the%2520complexity%2520of%2520medical%2520data%2520and%2520the%2520scarcity%2520of%2520high-quality%2520expert%2520annotations.%2520These%2520challenges%2520limit%2520the%2520effectiveness%2520of%2520conventional%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520strategies.%2520While%2520recent%2520work%2520has%2520demonstrated%2520that%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520can%2520enhance%2520reasoning%2520in%2520MLLMs%2520without%2520relying%2520on%2520large%2520amounts%2520of%2520data%252C%2520we%2520find%2520two%2520weaknesses%2520that%2520hinder%2520GRPO%2527s%2520reasoning%2520performance%2520in%2520anatomy%2520recognition%253A%25201%2529%2520knowledge%2520cannot%2520be%2520effectively%2520shared%2520between%2520different%2520anatomical%2520structures%252C%2520resulting%2520in%2520uneven%2520information%2520gain%2520and%2520preventing%2520the%2520model%2520from%2520converging%252C%2520and%25202%2529%2520the%2520model%2520quickly%2520converges%2520to%2520a%2520single%2520reasoning%2520path%252C%2520suppressing%2520the%2520exploration%2520of%2520diverse%2520strategies.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520propose%2520two%2520novel%2520methods.%2520First%252C%2520we%2520implement%2520a%2520progressive%2520learning%2520strategy%2520called%2520Anatomical%2520Similarity%2520Curriculum%2520Learning%2520by%2520controlling%2520question%2520difficulty%2520via%2520the%2520similarity%2520of%2520answer%2520choices%252C%2520enabling%2520the%2520model%2520to%2520master%2520complex%2520problems%2520incrementally.%2520Second%252C%2520we%2520utilize%2520question%2520augmentation%2520referred%2520to%2520as%2520Group%2520Diversity%2520Question%2520Augmentation%2520to%2520expand%2520the%2520model%2527s%2520search%2520space%2520for%2520difficult%2520queries%252C%2520mitigating%2520the%2520tendency%2520to%2520produce%2520uniform%2520responses.%2520Comprehensive%2520experiments%2520on%2520the%2520SGG-VQA%2520and%2520OmniMedVQA%2520benchmarks%2520show%2520our%2520method%2520achieves%2520a%2520significant%2520improvement%2520across%2520the%2520two%2520benchmarks%252C%2520demonstrating%2520its%2520effectiveness%2520in%2520enhancing%2520the%2520medical%2520reasoning%2520capabilities%2520of%2520MLLMs.%2520The%2520code%2520can%2520be%2520found%2520in%2520https%253A//github.com/tomato996/Anatomy-R1%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19512v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anatomy-R1%3A%20Enhancing%20Anatomy%20Reasoning%20in%20Multimodal%20Large%20Language%20Models%20via%20Anatomical%20Similarity%20Curriculum%20and%20Group%20Diversity%20Augmentation&entry.906535625=Ziyang%20Song%20and%20Zelin%20Zang%20and%20Zuyao%20Chen%20and%20Xusheng%20Liang%20and%20Dong%20Yi%20and%20Jinlin%20Wu%20and%20Hongbin%20Liu%20and%20Jiebo%20Luo&entry.1292438233=Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20achieved%20impressive%20progress%20in%20natural%20image%20reasoning%2C%20yet%20their%20potential%20in%20medical%20imaging%20remains%20underexplored%2C%20especially%20in%20clinical%20anatomical%20surgical%20images.%20Anatomy%20understanding%20tasks%20demand%20precise%20understanding%20and%20clinically%20coherent%20answers%2C%20which%20are%20difficult%20to%20achieve%20due%20to%20the%20complexity%20of%20medical%20data%20and%20the%20scarcity%20of%20high-quality%20expert%20annotations.%20These%20challenges%20limit%20the%20effectiveness%20of%20conventional%20Supervised%20Fine-Tuning%20%28SFT%29%20strategies.%20While%20recent%20work%20has%20demonstrated%20that%20Group%20Relative%20Policy%20Optimization%20%28GRPO%29%20can%20enhance%20reasoning%20in%20MLLMs%20without%20relying%20on%20large%20amounts%20of%20data%2C%20we%20find%20two%20weaknesses%20that%20hinder%20GRPO%27s%20reasoning%20performance%20in%20anatomy%20recognition%3A%201%29%20knowledge%20cannot%20be%20effectively%20shared%20between%20different%20anatomical%20structures%2C%20resulting%20in%20uneven%20information%20gain%20and%20preventing%20the%20model%20from%20converging%2C%20and%202%29%20the%20model%20quickly%20converges%20to%20a%20single%20reasoning%20path%2C%20suppressing%20the%20exploration%20of%20diverse%20strategies.%20To%20overcome%20these%20challenges%2C%20we%20propose%20two%20novel%20methods.%20First%2C%20we%20implement%20a%20progressive%20learning%20strategy%20called%20Anatomical%20Similarity%20Curriculum%20Learning%20by%20controlling%20question%20difficulty%20via%20the%20similarity%20of%20answer%20choices%2C%20enabling%20the%20model%20to%20master%20complex%20problems%20incrementally.%20Second%2C%20we%20utilize%20question%20augmentation%20referred%20to%20as%20Group%20Diversity%20Question%20Augmentation%20to%20expand%20the%20model%27s%20search%20space%20for%20difficult%20queries%2C%20mitigating%20the%20tendency%20to%20produce%20uniform%20responses.%20Comprehensive%20experiments%20on%20the%20SGG-VQA%20and%20OmniMedVQA%20benchmarks%20show%20our%20method%20achieves%20a%20significant%20improvement%20across%20the%20two%20benchmarks%2C%20demonstrating%20its%20effectiveness%20in%20enhancing%20the%20medical%20reasoning%20capabilities%20of%20MLLMs.%20The%20code%20can%20be%20found%20in%20https%3A//github.com/tomato996/Anatomy-R1&entry.1838667208=http%3A//arxiv.org/abs/2512.19512v1&entry.124074799=Read"},
{"title": "A Unified Representation of Neural Networks Architectures", "author": "Christophe Prieur and Mircea Lazar and Bogdan Robu", "abstract": "In this paper we consider the limiting case of neural networks (NNs) architectures when the number of neurons in each hidden layer and the number of hidden layers tend to infinity thus forming a continuum, and we derive approximation errors as a function of the number of neurons and/or hidden layers. Firstly, we consider the case of neural networks with a single hidden layer and we derive an integral infinite width neural representation that generalizes existing continuous neural networks (CNNs) representations. Then we extend this to deep residual CNNs that have a finite number of integral hidden layers and residual connections. Secondly, we revisit the relation between neural ODEs and deep residual NNs and we formalize approximation errors via discretization techniques. Then, we merge these two approaches into a unified homogeneous representation of NNs as a Distributed Parameter neural Network (DiPaNet) and we show that most of the existing finite and infinite-dimensional NNs architectures are related via homogenization/discretization with the DiPaNet representation. Our approach is purely deterministic and applies to general, uniformly continuous matrix weight functions. Relations with neural fields and other neural integro-differential equations are discussed along with further possible generalizations and applications of the DiPaNet framework.", "link": "http://arxiv.org/abs/2512.17593v2", "date": "2025-12-22", "relevancy": 2.2713, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4875}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4403}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4349}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Representation%20of%20Neural%20Networks%20Architectures&body=Title%3A%20A%20Unified%20Representation%20of%20Neural%20Networks%20Architectures%0AAuthor%3A%20Christophe%20Prieur%20and%20Mircea%20Lazar%20and%20Bogdan%20Robu%0AAbstract%3A%20In%20this%20paper%20we%20consider%20the%20limiting%20case%20of%20neural%20networks%20%28NNs%29%20architectures%20when%20the%20number%20of%20neurons%20in%20each%20hidden%20layer%20and%20the%20number%20of%20hidden%20layers%20tend%20to%20infinity%20thus%20forming%20a%20continuum%2C%20and%20we%20derive%20approximation%20errors%20as%20a%20function%20of%20the%20number%20of%20neurons%20and/or%20hidden%20layers.%20Firstly%2C%20we%20consider%20the%20case%20of%20neural%20networks%20with%20a%20single%20hidden%20layer%20and%20we%20derive%20an%20integral%20infinite%20width%20neural%20representation%20that%20generalizes%20existing%20continuous%20neural%20networks%20%28CNNs%29%20representations.%20Then%20we%20extend%20this%20to%20deep%20residual%20CNNs%20that%20have%20a%20finite%20number%20of%20integral%20hidden%20layers%20and%20residual%20connections.%20Secondly%2C%20we%20revisit%20the%20relation%20between%20neural%20ODEs%20and%20deep%20residual%20NNs%20and%20we%20formalize%20approximation%20errors%20via%20discretization%20techniques.%20Then%2C%20we%20merge%20these%20two%20approaches%20into%20a%20unified%20homogeneous%20representation%20of%20NNs%20as%20a%20Distributed%20Parameter%20neural%20Network%20%28DiPaNet%29%20and%20we%20show%20that%20most%20of%20the%20existing%20finite%20and%20infinite-dimensional%20NNs%20architectures%20are%20related%20via%20homogenization/discretization%20with%20the%20DiPaNet%20representation.%20Our%20approach%20is%20purely%20deterministic%20and%20applies%20to%20general%2C%20uniformly%20continuous%20matrix%20weight%20functions.%20Relations%20with%20neural%20fields%20and%20other%20neural%20integro-differential%20equations%20are%20discussed%20along%20with%20further%20possible%20generalizations%20and%20applications%20of%20the%20DiPaNet%20framework.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17593v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Representation%2520of%2520Neural%2520Networks%2520Architectures%26entry.906535625%3DChristophe%2520Prieur%2520and%2520Mircea%2520Lazar%2520and%2520Bogdan%2520Robu%26entry.1292438233%3DIn%2520this%2520paper%2520we%2520consider%2520the%2520limiting%2520case%2520of%2520neural%2520networks%2520%2528NNs%2529%2520architectures%2520when%2520the%2520number%2520of%2520neurons%2520in%2520each%2520hidden%2520layer%2520and%2520the%2520number%2520of%2520hidden%2520layers%2520tend%2520to%2520infinity%2520thus%2520forming%2520a%2520continuum%252C%2520and%2520we%2520derive%2520approximation%2520errors%2520as%2520a%2520function%2520of%2520the%2520number%2520of%2520neurons%2520and/or%2520hidden%2520layers.%2520Firstly%252C%2520we%2520consider%2520the%2520case%2520of%2520neural%2520networks%2520with%2520a%2520single%2520hidden%2520layer%2520and%2520we%2520derive%2520an%2520integral%2520infinite%2520width%2520neural%2520representation%2520that%2520generalizes%2520existing%2520continuous%2520neural%2520networks%2520%2528CNNs%2529%2520representations.%2520Then%2520we%2520extend%2520this%2520to%2520deep%2520residual%2520CNNs%2520that%2520have%2520a%2520finite%2520number%2520of%2520integral%2520hidden%2520layers%2520and%2520residual%2520connections.%2520Secondly%252C%2520we%2520revisit%2520the%2520relation%2520between%2520neural%2520ODEs%2520and%2520deep%2520residual%2520NNs%2520and%2520we%2520formalize%2520approximation%2520errors%2520via%2520discretization%2520techniques.%2520Then%252C%2520we%2520merge%2520these%2520two%2520approaches%2520into%2520a%2520unified%2520homogeneous%2520representation%2520of%2520NNs%2520as%2520a%2520Distributed%2520Parameter%2520neural%2520Network%2520%2528DiPaNet%2529%2520and%2520we%2520show%2520that%2520most%2520of%2520the%2520existing%2520finite%2520and%2520infinite-dimensional%2520NNs%2520architectures%2520are%2520related%2520via%2520homogenization/discretization%2520with%2520the%2520DiPaNet%2520representation.%2520Our%2520approach%2520is%2520purely%2520deterministic%2520and%2520applies%2520to%2520general%252C%2520uniformly%2520continuous%2520matrix%2520weight%2520functions.%2520Relations%2520with%2520neural%2520fields%2520and%2520other%2520neural%2520integro-differential%2520equations%2520are%2520discussed%2520along%2520with%2520further%2520possible%2520generalizations%2520and%2520applications%2520of%2520the%2520DiPaNet%2520framework.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17593v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Representation%20of%20Neural%20Networks%20Architectures&entry.906535625=Christophe%20Prieur%20and%20Mircea%20Lazar%20and%20Bogdan%20Robu&entry.1292438233=In%20this%20paper%20we%20consider%20the%20limiting%20case%20of%20neural%20networks%20%28NNs%29%20architectures%20when%20the%20number%20of%20neurons%20in%20each%20hidden%20layer%20and%20the%20number%20of%20hidden%20layers%20tend%20to%20infinity%20thus%20forming%20a%20continuum%2C%20and%20we%20derive%20approximation%20errors%20as%20a%20function%20of%20the%20number%20of%20neurons%20and/or%20hidden%20layers.%20Firstly%2C%20we%20consider%20the%20case%20of%20neural%20networks%20with%20a%20single%20hidden%20layer%20and%20we%20derive%20an%20integral%20infinite%20width%20neural%20representation%20that%20generalizes%20existing%20continuous%20neural%20networks%20%28CNNs%29%20representations.%20Then%20we%20extend%20this%20to%20deep%20residual%20CNNs%20that%20have%20a%20finite%20number%20of%20integral%20hidden%20layers%20and%20residual%20connections.%20Secondly%2C%20we%20revisit%20the%20relation%20between%20neural%20ODEs%20and%20deep%20residual%20NNs%20and%20we%20formalize%20approximation%20errors%20via%20discretization%20techniques.%20Then%2C%20we%20merge%20these%20two%20approaches%20into%20a%20unified%20homogeneous%20representation%20of%20NNs%20as%20a%20Distributed%20Parameter%20neural%20Network%20%28DiPaNet%29%20and%20we%20show%20that%20most%20of%20the%20existing%20finite%20and%20infinite-dimensional%20NNs%20architectures%20are%20related%20via%20homogenization/discretization%20with%20the%20DiPaNet%20representation.%20Our%20approach%20is%20purely%20deterministic%20and%20applies%20to%20general%2C%20uniformly%20continuous%20matrix%20weight%20functions.%20Relations%20with%20neural%20fields%20and%20other%20neural%20integro-differential%20equations%20are%20discussed%20along%20with%20further%20possible%20generalizations%20and%20applications%20of%20the%20DiPaNet%20framework.&entry.1838667208=http%3A//arxiv.org/abs/2512.17593v2&entry.124074799=Read"},
{"title": "Dynamic Stream Network for Combinatorial Explosion Problem in Deformable Medical Image Registration", "author": "Shaochen Bi and Yuting He and Weiming Wang and Hao Chen", "abstract": "Combinatorial explosion problem caused by dual inputs presents a critical challenge in Deformable Medical Image Registration (DMIR). Since DMIR processes two images simultaneously as input, the combination relationships between features has grown exponentially, ultimately the model considers more interfering features during the feature modeling process. Introducing dynamics in the receptive fields and weights of the network enable the model to eliminate the interfering features combination and model the potential feature combination relationships. In this paper, we propose the Dynamic Stream Network (DySNet), which enables the receptive fields and weights to be dynamically adjusted. This ultimately enables the model to ignore interfering feature combinations and model the potential feature relationships. With two key innovations: 1) Adaptive Stream Basin (AdSB) module dynamically adjusts the shape of the receptive field, thereby enabling the model to focus on the feature relationships with greater correlation. 2) Dynamic Stream Attention (DySA) mechanism generates dynamic weights to search for more valuable feature relationships. Extensive experiments have shown that DySNet consistently outperforms the most advanced DMIR methods, highlighting its outstanding generalization ability. Our code will be released on the website: https://github.com/ShaochenBi/DySNet.", "link": "http://arxiv.org/abs/2512.19486v1", "date": "2025-12-22", "relevancy": 2.2619, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5853}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5583}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Stream%20Network%20for%20Combinatorial%20Explosion%20Problem%20in%20Deformable%20Medical%20Image%20Registration&body=Title%3A%20Dynamic%20Stream%20Network%20for%20Combinatorial%20Explosion%20Problem%20in%20Deformable%20Medical%20Image%20Registration%0AAuthor%3A%20Shaochen%20Bi%20and%20Yuting%20He%20and%20Weiming%20Wang%20and%20Hao%20Chen%0AAbstract%3A%20Combinatorial%20explosion%20problem%20caused%20by%20dual%20inputs%20presents%20a%20critical%20challenge%20in%20Deformable%20Medical%20Image%20Registration%20%28DMIR%29.%20Since%20DMIR%20processes%20two%20images%20simultaneously%20as%20input%2C%20the%20combination%20relationships%20between%20features%20has%20grown%20exponentially%2C%20ultimately%20the%20model%20considers%20more%20interfering%20features%20during%20the%20feature%20modeling%20process.%20Introducing%20dynamics%20in%20the%20receptive%20fields%20and%20weights%20of%20the%20network%20enable%20the%20model%20to%20eliminate%20the%20interfering%20features%20combination%20and%20model%20the%20potential%20feature%20combination%20relationships.%20In%20this%20paper%2C%20we%20propose%20the%20Dynamic%20Stream%20Network%20%28DySNet%29%2C%20which%20enables%20the%20receptive%20fields%20and%20weights%20to%20be%20dynamically%20adjusted.%20This%20ultimately%20enables%20the%20model%20to%20ignore%20interfering%20feature%20combinations%20and%20model%20the%20potential%20feature%20relationships.%20With%20two%20key%20innovations%3A%201%29%20Adaptive%20Stream%20Basin%20%28AdSB%29%20module%20dynamically%20adjusts%20the%20shape%20of%20the%20receptive%20field%2C%20thereby%20enabling%20the%20model%20to%20focus%20on%20the%20feature%20relationships%20with%20greater%20correlation.%202%29%20Dynamic%20Stream%20Attention%20%28DySA%29%20mechanism%20generates%20dynamic%20weights%20to%20search%20for%20more%20valuable%20feature%20relationships.%20Extensive%20experiments%20have%20shown%20that%20DySNet%20consistently%20outperforms%20the%20most%20advanced%20DMIR%20methods%2C%20highlighting%20its%20outstanding%20generalization%20ability.%20Our%20code%20will%20be%20released%20on%20the%20website%3A%20https%3A//github.com/ShaochenBi/DySNet.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19486v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Stream%2520Network%2520for%2520Combinatorial%2520Explosion%2520Problem%2520in%2520Deformable%2520Medical%2520Image%2520Registration%26entry.906535625%3DShaochen%2520Bi%2520and%2520Yuting%2520He%2520and%2520Weiming%2520Wang%2520and%2520Hao%2520Chen%26entry.1292438233%3DCombinatorial%2520explosion%2520problem%2520caused%2520by%2520dual%2520inputs%2520presents%2520a%2520critical%2520challenge%2520in%2520Deformable%2520Medical%2520Image%2520Registration%2520%2528DMIR%2529.%2520Since%2520DMIR%2520processes%2520two%2520images%2520simultaneously%2520as%2520input%252C%2520the%2520combination%2520relationships%2520between%2520features%2520has%2520grown%2520exponentially%252C%2520ultimately%2520the%2520model%2520considers%2520more%2520interfering%2520features%2520during%2520the%2520feature%2520modeling%2520process.%2520Introducing%2520dynamics%2520in%2520the%2520receptive%2520fields%2520and%2520weights%2520of%2520the%2520network%2520enable%2520the%2520model%2520to%2520eliminate%2520the%2520interfering%2520features%2520combination%2520and%2520model%2520the%2520potential%2520feature%2520combination%2520relationships.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520Dynamic%2520Stream%2520Network%2520%2528DySNet%2529%252C%2520which%2520enables%2520the%2520receptive%2520fields%2520and%2520weights%2520to%2520be%2520dynamically%2520adjusted.%2520This%2520ultimately%2520enables%2520the%2520model%2520to%2520ignore%2520interfering%2520feature%2520combinations%2520and%2520model%2520the%2520potential%2520feature%2520relationships.%2520With%2520two%2520key%2520innovations%253A%25201%2529%2520Adaptive%2520Stream%2520Basin%2520%2528AdSB%2529%2520module%2520dynamically%2520adjusts%2520the%2520shape%2520of%2520the%2520receptive%2520field%252C%2520thereby%2520enabling%2520the%2520model%2520to%2520focus%2520on%2520the%2520feature%2520relationships%2520with%2520greater%2520correlation.%25202%2529%2520Dynamic%2520Stream%2520Attention%2520%2528DySA%2529%2520mechanism%2520generates%2520dynamic%2520weights%2520to%2520search%2520for%2520more%2520valuable%2520feature%2520relationships.%2520Extensive%2520experiments%2520have%2520shown%2520that%2520DySNet%2520consistently%2520outperforms%2520the%2520most%2520advanced%2520DMIR%2520methods%252C%2520highlighting%2520its%2520outstanding%2520generalization%2520ability.%2520Our%2520code%2520will%2520be%2520released%2520on%2520the%2520website%253A%2520https%253A//github.com/ShaochenBi/DySNet.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19486v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Stream%20Network%20for%20Combinatorial%20Explosion%20Problem%20in%20Deformable%20Medical%20Image%20Registration&entry.906535625=Shaochen%20Bi%20and%20Yuting%20He%20and%20Weiming%20Wang%20and%20Hao%20Chen&entry.1292438233=Combinatorial%20explosion%20problem%20caused%20by%20dual%20inputs%20presents%20a%20critical%20challenge%20in%20Deformable%20Medical%20Image%20Registration%20%28DMIR%29.%20Since%20DMIR%20processes%20two%20images%20simultaneously%20as%20input%2C%20the%20combination%20relationships%20between%20features%20has%20grown%20exponentially%2C%20ultimately%20the%20model%20considers%20more%20interfering%20features%20during%20the%20feature%20modeling%20process.%20Introducing%20dynamics%20in%20the%20receptive%20fields%20and%20weights%20of%20the%20network%20enable%20the%20model%20to%20eliminate%20the%20interfering%20features%20combination%20and%20model%20the%20potential%20feature%20combination%20relationships.%20In%20this%20paper%2C%20we%20propose%20the%20Dynamic%20Stream%20Network%20%28DySNet%29%2C%20which%20enables%20the%20receptive%20fields%20and%20weights%20to%20be%20dynamically%20adjusted.%20This%20ultimately%20enables%20the%20model%20to%20ignore%20interfering%20feature%20combinations%20and%20model%20the%20potential%20feature%20relationships.%20With%20two%20key%20innovations%3A%201%29%20Adaptive%20Stream%20Basin%20%28AdSB%29%20module%20dynamically%20adjusts%20the%20shape%20of%20the%20receptive%20field%2C%20thereby%20enabling%20the%20model%20to%20focus%20on%20the%20feature%20relationships%20with%20greater%20correlation.%202%29%20Dynamic%20Stream%20Attention%20%28DySA%29%20mechanism%20generates%20dynamic%20weights%20to%20search%20for%20more%20valuable%20feature%20relationships.%20Extensive%20experiments%20have%20shown%20that%20DySNet%20consistently%20outperforms%20the%20most%20advanced%20DMIR%20methods%2C%20highlighting%20its%20outstanding%20generalization%20ability.%20Our%20code%20will%20be%20released%20on%20the%20website%3A%20https%3A//github.com/ShaochenBi/DySNet.&entry.1838667208=http%3A//arxiv.org/abs/2512.19486v1&entry.124074799=Read"},
{"title": "From Points to Coalitions: Hierarchical Contrastive Shapley Values for Prioritizing Data Samples", "author": "Canran Xiao and Jiabao Dou and Zhiming Lin and Zong Ke and Liwei Hou", "abstract": "How should we quantify the value of each training example when datasets are large, heterogeneous, and geometrically structured? Classical Data-Shapley answers in principle, but its O(n!) complexity and point-wise perspective are ill-suited to modern scales. We propose Hierarchical Contrastive Data Valuation (HCDV), a three-stage framework that (i) learns a contrastive, geometry-preserving representation, (ii) organizes the data into a balanced coarse-to-fine hierarchy of clusters, and (iii) assigns Shapley-style payoffs to coalitions via local Monte-Carlo games whose budgets are propagated downward. HCDV collapses the factorial burden to O(T sum_{l} K_{l}) = O(T K_max log n), rewards examples that sharpen decision boundaries, and regularizes outliers through curvature-based smoothness. We prove that HCDV approximately satisfies the four Shapley axioms with surplus loss O(eta log n), enjoys sub-Gaussian coalition deviation tilde O(1/sqrt{T}), and incurs at most k epsilon_infty regret for top-k selection. Experiments on four benchmarks--tabular, vision, streaming, and a 45M-sample CTR task--plus the OpenDataVal suite show that HCDV lifts accuracy by up to +5 pp, slashes valuation time by up to 100x, and directly supports tasks such as augmentation filtering, low-latency streaming updates, and fair marketplace payouts.", "link": "http://arxiv.org/abs/2512.19363v1", "date": "2025-12-22", "relevancy": 2.2515, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4602}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4508}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Points%20to%20Coalitions%3A%20Hierarchical%20Contrastive%20Shapley%20Values%20for%20Prioritizing%20Data%20Samples&body=Title%3A%20From%20Points%20to%20Coalitions%3A%20Hierarchical%20Contrastive%20Shapley%20Values%20for%20Prioritizing%20Data%20Samples%0AAuthor%3A%20Canran%20Xiao%20and%20Jiabao%20Dou%20and%20Zhiming%20Lin%20and%20Zong%20Ke%20and%20Liwei%20Hou%0AAbstract%3A%20How%20should%20we%20quantify%20the%20value%20of%20each%20training%20example%20when%20datasets%20are%20large%2C%20heterogeneous%2C%20and%20geometrically%20structured%3F%20Classical%20Data-Shapley%20answers%20in%20principle%2C%20but%20its%20O%28n%21%29%20complexity%20and%20point-wise%20perspective%20are%20ill-suited%20to%20modern%20scales.%20We%20propose%20Hierarchical%20Contrastive%20Data%20Valuation%20%28HCDV%29%2C%20a%20three-stage%20framework%20that%20%28i%29%20learns%20a%20contrastive%2C%20geometry-preserving%20representation%2C%20%28ii%29%20organizes%20the%20data%20into%20a%20balanced%20coarse-to-fine%20hierarchy%20of%20clusters%2C%20and%20%28iii%29%20assigns%20Shapley-style%20payoffs%20to%20coalitions%20via%20local%20Monte-Carlo%20games%20whose%20budgets%20are%20propagated%20downward.%20HCDV%20collapses%20the%20factorial%20burden%20to%20O%28T%20sum_%7Bl%7D%20K_%7Bl%7D%29%20%3D%20O%28T%20K_max%20log%20n%29%2C%20rewards%20examples%20that%20sharpen%20decision%20boundaries%2C%20and%20regularizes%20outliers%20through%20curvature-based%20smoothness.%20We%20prove%20that%20HCDV%20approximately%20satisfies%20the%20four%20Shapley%20axioms%20with%20surplus%20loss%20O%28eta%20log%20n%29%2C%20enjoys%20sub-Gaussian%20coalition%20deviation%20tilde%20O%281/sqrt%7BT%7D%29%2C%20and%20incurs%20at%20most%20k%20epsilon_infty%20regret%20for%20top-k%20selection.%20Experiments%20on%20four%20benchmarks--tabular%2C%20vision%2C%20streaming%2C%20and%20a%2045M-sample%20CTR%20task--plus%20the%20OpenDataVal%20suite%20show%20that%20HCDV%20lifts%20accuracy%20by%20up%20to%20%2B5%20pp%2C%20slashes%20valuation%20time%20by%20up%20to%20100x%2C%20and%20directly%20supports%20tasks%20such%20as%20augmentation%20filtering%2C%20low-latency%20streaming%20updates%2C%20and%20fair%20marketplace%20payouts.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19363v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Points%2520to%2520Coalitions%253A%2520Hierarchical%2520Contrastive%2520Shapley%2520Values%2520for%2520Prioritizing%2520Data%2520Samples%26entry.906535625%3DCanran%2520Xiao%2520and%2520Jiabao%2520Dou%2520and%2520Zhiming%2520Lin%2520and%2520Zong%2520Ke%2520and%2520Liwei%2520Hou%26entry.1292438233%3DHow%2520should%2520we%2520quantify%2520the%2520value%2520of%2520each%2520training%2520example%2520when%2520datasets%2520are%2520large%252C%2520heterogeneous%252C%2520and%2520geometrically%2520structured%253F%2520Classical%2520Data-Shapley%2520answers%2520in%2520principle%252C%2520but%2520its%2520O%2528n%2521%2529%2520complexity%2520and%2520point-wise%2520perspective%2520are%2520ill-suited%2520to%2520modern%2520scales.%2520We%2520propose%2520Hierarchical%2520Contrastive%2520Data%2520Valuation%2520%2528HCDV%2529%252C%2520a%2520three-stage%2520framework%2520that%2520%2528i%2529%2520learns%2520a%2520contrastive%252C%2520geometry-preserving%2520representation%252C%2520%2528ii%2529%2520organizes%2520the%2520data%2520into%2520a%2520balanced%2520coarse-to-fine%2520hierarchy%2520of%2520clusters%252C%2520and%2520%2528iii%2529%2520assigns%2520Shapley-style%2520payoffs%2520to%2520coalitions%2520via%2520local%2520Monte-Carlo%2520games%2520whose%2520budgets%2520are%2520propagated%2520downward.%2520HCDV%2520collapses%2520the%2520factorial%2520burden%2520to%2520O%2528T%2520sum_%257Bl%257D%2520K_%257Bl%257D%2529%2520%253D%2520O%2528T%2520K_max%2520log%2520n%2529%252C%2520rewards%2520examples%2520that%2520sharpen%2520decision%2520boundaries%252C%2520and%2520regularizes%2520outliers%2520through%2520curvature-based%2520smoothness.%2520We%2520prove%2520that%2520HCDV%2520approximately%2520satisfies%2520the%2520four%2520Shapley%2520axioms%2520with%2520surplus%2520loss%2520O%2528eta%2520log%2520n%2529%252C%2520enjoys%2520sub-Gaussian%2520coalition%2520deviation%2520tilde%2520O%25281/sqrt%257BT%257D%2529%252C%2520and%2520incurs%2520at%2520most%2520k%2520epsilon_infty%2520regret%2520for%2520top-k%2520selection.%2520Experiments%2520on%2520four%2520benchmarks--tabular%252C%2520vision%252C%2520streaming%252C%2520and%2520a%252045M-sample%2520CTR%2520task--plus%2520the%2520OpenDataVal%2520suite%2520show%2520that%2520HCDV%2520lifts%2520accuracy%2520by%2520up%2520to%2520%252B5%2520pp%252C%2520slashes%2520valuation%2520time%2520by%2520up%2520to%2520100x%252C%2520and%2520directly%2520supports%2520tasks%2520such%2520as%2520augmentation%2520filtering%252C%2520low-latency%2520streaming%2520updates%252C%2520and%2520fair%2520marketplace%2520payouts.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19363v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Points%20to%20Coalitions%3A%20Hierarchical%20Contrastive%20Shapley%20Values%20for%20Prioritizing%20Data%20Samples&entry.906535625=Canran%20Xiao%20and%20Jiabao%20Dou%20and%20Zhiming%20Lin%20and%20Zong%20Ke%20and%20Liwei%20Hou&entry.1292438233=How%20should%20we%20quantify%20the%20value%20of%20each%20training%20example%20when%20datasets%20are%20large%2C%20heterogeneous%2C%20and%20geometrically%20structured%3F%20Classical%20Data-Shapley%20answers%20in%20principle%2C%20but%20its%20O%28n%21%29%20complexity%20and%20point-wise%20perspective%20are%20ill-suited%20to%20modern%20scales.%20We%20propose%20Hierarchical%20Contrastive%20Data%20Valuation%20%28HCDV%29%2C%20a%20three-stage%20framework%20that%20%28i%29%20learns%20a%20contrastive%2C%20geometry-preserving%20representation%2C%20%28ii%29%20organizes%20the%20data%20into%20a%20balanced%20coarse-to-fine%20hierarchy%20of%20clusters%2C%20and%20%28iii%29%20assigns%20Shapley-style%20payoffs%20to%20coalitions%20via%20local%20Monte-Carlo%20games%20whose%20budgets%20are%20propagated%20downward.%20HCDV%20collapses%20the%20factorial%20burden%20to%20O%28T%20sum_%7Bl%7D%20K_%7Bl%7D%29%20%3D%20O%28T%20K_max%20log%20n%29%2C%20rewards%20examples%20that%20sharpen%20decision%20boundaries%2C%20and%20regularizes%20outliers%20through%20curvature-based%20smoothness.%20We%20prove%20that%20HCDV%20approximately%20satisfies%20the%20four%20Shapley%20axioms%20with%20surplus%20loss%20O%28eta%20log%20n%29%2C%20enjoys%20sub-Gaussian%20coalition%20deviation%20tilde%20O%281/sqrt%7BT%7D%29%2C%20and%20incurs%20at%20most%20k%20epsilon_infty%20regret%20for%20top-k%20selection.%20Experiments%20on%20four%20benchmarks--tabular%2C%20vision%2C%20streaming%2C%20and%20a%2045M-sample%20CTR%20task--plus%20the%20OpenDataVal%20suite%20show%20that%20HCDV%20lifts%20accuracy%20by%20up%20to%20%2B5%20pp%2C%20slashes%20valuation%20time%20by%20up%20to%20100x%2C%20and%20directly%20supports%20tasks%20such%20as%20augmentation%20filtering%2C%20low-latency%20streaming%20updates%2C%20and%20fair%20marketplace%20payouts.&entry.1838667208=http%3A//arxiv.org/abs/2512.19363v1&entry.124074799=Read"},
{"title": "VERDI: VLM-Embedded Reasoning for Autonomous Driving", "author": "Bowen Feng and Zhiting Mei and Baiang Li and Julian Ost and Filippo Ghilotti and Roger Girgis and Anirudha Majumdar and Felix Heide", "abstract": "While autonomous driving (AD) stacks struggle with decision making under partial observability and real-world complexity, human drivers are capable of commonsense reasoning to make near-optimal decisions with limited information. Recent work has attempted to leverage finetuned Vision-Language Models (VLMs) for trajectory planning at inference time to emulate human behavior. Despite their success in benchmark evaluations, these methods are often impractical to deploy (a 70B parameter VLM inference at merely 8 tokens per second requires more than 160G of memory), and their monolithic network structure prohibits safety decomposition. To bridge this gap, we propose VLM-Embedded Reasoning for autonomous Driving (VERDI), a training-time framework that distills the reasoning process and commonsense knowledge of VLMs into the AD stack. VERDI augments modular differentiable end-to-end (e2e) AD models by aligning intermediate module outputs at the perception, prediction, and planning stages with text features explaining the driving reasoning process produced by VLMs. By encouraging alignment in latent space, VERDI enables the modular AD stack to internalize structured reasoning, without incurring the inference-time costs of large VLMs. We validate VERDI in both open-loop (NuScenes and Bench2Drive benchmarks) and closed-loop (HugSim Simulator) settings. We find that VERDI outperforms existing e2e methods that do not embed reasoning by up to 11% in $\\ell_{2}$ distance and 11% in driving performance, while maintaining real-time inference speed.", "link": "http://arxiv.org/abs/2505.15925v3", "date": "2025-12-22", "relevancy": 2.2327, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5618}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5618}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5399}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VERDI%3A%20VLM-Embedded%20Reasoning%20for%20Autonomous%20Driving&body=Title%3A%20VERDI%3A%20VLM-Embedded%20Reasoning%20for%20Autonomous%20Driving%0AAuthor%3A%20Bowen%20Feng%20and%20Zhiting%20Mei%20and%20Baiang%20Li%20and%20Julian%20Ost%20and%20Filippo%20Ghilotti%20and%20Roger%20Girgis%20and%20Anirudha%20Majumdar%20and%20Felix%20Heide%0AAbstract%3A%20While%20autonomous%20driving%20%28AD%29%20stacks%20struggle%20with%20decision%20making%20under%20partial%20observability%20and%20real-world%20complexity%2C%20human%20drivers%20are%20capable%20of%20commonsense%20reasoning%20to%20make%20near-optimal%20decisions%20with%20limited%20information.%20Recent%20work%20has%20attempted%20to%20leverage%20finetuned%20Vision-Language%20Models%20%28VLMs%29%20for%20trajectory%20planning%20at%20inference%20time%20to%20emulate%20human%20behavior.%20Despite%20their%20success%20in%20benchmark%20evaluations%2C%20these%20methods%20are%20often%20impractical%20to%20deploy%20%28a%2070B%20parameter%20VLM%20inference%20at%20merely%208%20tokens%20per%20second%20requires%20more%20than%20160G%20of%20memory%29%2C%20and%20their%20monolithic%20network%20structure%20prohibits%20safety%20decomposition.%20To%20bridge%20this%20gap%2C%20we%20propose%20VLM-Embedded%20Reasoning%20for%20autonomous%20Driving%20%28VERDI%29%2C%20a%20training-time%20framework%20that%20distills%20the%20reasoning%20process%20and%20commonsense%20knowledge%20of%20VLMs%20into%20the%20AD%20stack.%20VERDI%20augments%20modular%20differentiable%20end-to-end%20%28e2e%29%20AD%20models%20by%20aligning%20intermediate%20module%20outputs%20at%20the%20perception%2C%20prediction%2C%20and%20planning%20stages%20with%20text%20features%20explaining%20the%20driving%20reasoning%20process%20produced%20by%20VLMs.%20By%20encouraging%20alignment%20in%20latent%20space%2C%20VERDI%20enables%20the%20modular%20AD%20stack%20to%20internalize%20structured%20reasoning%2C%20without%20incurring%20the%20inference-time%20costs%20of%20large%20VLMs.%20We%20validate%20VERDI%20in%20both%20open-loop%20%28NuScenes%20and%20Bench2Drive%20benchmarks%29%20and%20closed-loop%20%28HugSim%20Simulator%29%20settings.%20We%20find%20that%20VERDI%20outperforms%20existing%20e2e%20methods%20that%20do%20not%20embed%20reasoning%20by%20up%20to%2011%25%20in%20%24%5Cell_%7B2%7D%24%20distance%20and%2011%25%20in%20driving%20performance%2C%20while%20maintaining%20real-time%20inference%20speed.%0ALink%3A%20http%3A//arxiv.org/abs/2505.15925v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVERDI%253A%2520VLM-Embedded%2520Reasoning%2520for%2520Autonomous%2520Driving%26entry.906535625%3DBowen%2520Feng%2520and%2520Zhiting%2520Mei%2520and%2520Baiang%2520Li%2520and%2520Julian%2520Ost%2520and%2520Filippo%2520Ghilotti%2520and%2520Roger%2520Girgis%2520and%2520Anirudha%2520Majumdar%2520and%2520Felix%2520Heide%26entry.1292438233%3DWhile%2520autonomous%2520driving%2520%2528AD%2529%2520stacks%2520struggle%2520with%2520decision%2520making%2520under%2520partial%2520observability%2520and%2520real-world%2520complexity%252C%2520human%2520drivers%2520are%2520capable%2520of%2520commonsense%2520reasoning%2520to%2520make%2520near-optimal%2520decisions%2520with%2520limited%2520information.%2520Recent%2520work%2520has%2520attempted%2520to%2520leverage%2520finetuned%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520for%2520trajectory%2520planning%2520at%2520inference%2520time%2520to%2520emulate%2520human%2520behavior.%2520Despite%2520their%2520success%2520in%2520benchmark%2520evaluations%252C%2520these%2520methods%2520are%2520often%2520impractical%2520to%2520deploy%2520%2528a%252070B%2520parameter%2520VLM%2520inference%2520at%2520merely%25208%2520tokens%2520per%2520second%2520requires%2520more%2520than%2520160G%2520of%2520memory%2529%252C%2520and%2520their%2520monolithic%2520network%2520structure%2520prohibits%2520safety%2520decomposition.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520VLM-Embedded%2520Reasoning%2520for%2520autonomous%2520Driving%2520%2528VERDI%2529%252C%2520a%2520training-time%2520framework%2520that%2520distills%2520the%2520reasoning%2520process%2520and%2520commonsense%2520knowledge%2520of%2520VLMs%2520into%2520the%2520AD%2520stack.%2520VERDI%2520augments%2520modular%2520differentiable%2520end-to-end%2520%2528e2e%2529%2520AD%2520models%2520by%2520aligning%2520intermediate%2520module%2520outputs%2520at%2520the%2520perception%252C%2520prediction%252C%2520and%2520planning%2520stages%2520with%2520text%2520features%2520explaining%2520the%2520driving%2520reasoning%2520process%2520produced%2520by%2520VLMs.%2520By%2520encouraging%2520alignment%2520in%2520latent%2520space%252C%2520VERDI%2520enables%2520the%2520modular%2520AD%2520stack%2520to%2520internalize%2520structured%2520reasoning%252C%2520without%2520incurring%2520the%2520inference-time%2520costs%2520of%2520large%2520VLMs.%2520We%2520validate%2520VERDI%2520in%2520both%2520open-loop%2520%2528NuScenes%2520and%2520Bench2Drive%2520benchmarks%2529%2520and%2520closed-loop%2520%2528HugSim%2520Simulator%2529%2520settings.%2520We%2520find%2520that%2520VERDI%2520outperforms%2520existing%2520e2e%2520methods%2520that%2520do%2520not%2520embed%2520reasoning%2520by%2520up%2520to%252011%2525%2520in%2520%2524%255Cell_%257B2%257D%2524%2520distance%2520and%252011%2525%2520in%2520driving%2520performance%252C%2520while%2520maintaining%2520real-time%2520inference%2520speed.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.15925v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VERDI%3A%20VLM-Embedded%20Reasoning%20for%20Autonomous%20Driving&entry.906535625=Bowen%20Feng%20and%20Zhiting%20Mei%20and%20Baiang%20Li%20and%20Julian%20Ost%20and%20Filippo%20Ghilotti%20and%20Roger%20Girgis%20and%20Anirudha%20Majumdar%20and%20Felix%20Heide&entry.1292438233=While%20autonomous%20driving%20%28AD%29%20stacks%20struggle%20with%20decision%20making%20under%20partial%20observability%20and%20real-world%20complexity%2C%20human%20drivers%20are%20capable%20of%20commonsense%20reasoning%20to%20make%20near-optimal%20decisions%20with%20limited%20information.%20Recent%20work%20has%20attempted%20to%20leverage%20finetuned%20Vision-Language%20Models%20%28VLMs%29%20for%20trajectory%20planning%20at%20inference%20time%20to%20emulate%20human%20behavior.%20Despite%20their%20success%20in%20benchmark%20evaluations%2C%20these%20methods%20are%20often%20impractical%20to%20deploy%20%28a%2070B%20parameter%20VLM%20inference%20at%20merely%208%20tokens%20per%20second%20requires%20more%20than%20160G%20of%20memory%29%2C%20and%20their%20monolithic%20network%20structure%20prohibits%20safety%20decomposition.%20To%20bridge%20this%20gap%2C%20we%20propose%20VLM-Embedded%20Reasoning%20for%20autonomous%20Driving%20%28VERDI%29%2C%20a%20training-time%20framework%20that%20distills%20the%20reasoning%20process%20and%20commonsense%20knowledge%20of%20VLMs%20into%20the%20AD%20stack.%20VERDI%20augments%20modular%20differentiable%20end-to-end%20%28e2e%29%20AD%20models%20by%20aligning%20intermediate%20module%20outputs%20at%20the%20perception%2C%20prediction%2C%20and%20planning%20stages%20with%20text%20features%20explaining%20the%20driving%20reasoning%20process%20produced%20by%20VLMs.%20By%20encouraging%20alignment%20in%20latent%20space%2C%20VERDI%20enables%20the%20modular%20AD%20stack%20to%20internalize%20structured%20reasoning%2C%20without%20incurring%20the%20inference-time%20costs%20of%20large%20VLMs.%20We%20validate%20VERDI%20in%20both%20open-loop%20%28NuScenes%20and%20Bench2Drive%20benchmarks%29%20and%20closed-loop%20%28HugSim%20Simulator%29%20settings.%20We%20find%20that%20VERDI%20outperforms%20existing%20e2e%20methods%20that%20do%20not%20embed%20reasoning%20by%20up%20to%2011%25%20in%20%24%5Cell_%7B2%7D%24%20distance%20and%2011%25%20in%20driving%20performance%2C%20while%20maintaining%20real-time%20inference%20speed.&entry.1838667208=http%3A//arxiv.org/abs/2505.15925v3&entry.124074799=Read"},
{"title": "3DFETUS: Deep Learning-Based Standardization of Facial Planes in 3D Ultrasound", "author": "Alomar Antonia and Rubio Ricardo and Albaiges Gerard and Salort-Benejam Laura and Caminal Julia and Prat Maria and Rueda Carolina and Cortes Berta and Piella Gemma and Sukno Federico", "abstract": "The automatic localization and standardization of anatomical planes in 3D medical imaging remains a challenging problem due to variability in object pose, appearance, and image quality. In 3D ultrasound, these challenges are exacerbated by speckle noise and limited contrast, particularly in fetal imaging.\n  To address these challenges in the context of facial assessment, we present: 1) GT++, a robust algorithm that estimates standard facial planes from 3D US volumes using annotated anatomical landmarks; and 2) 3DFETUS, a deep learning model that automates and standardizes their localization in 3D fetal US volumes.\n  We evaluated our methods both qualitatively, through expert clinical review, and quantitatively. The proposed approach achieved a mean translation error of 3.21 $\\pm$ 1.98mm and a mean rotation error of 5.31 $\\pm$ 3.945$^\\circ$ per plane, outperforming other state-of-the-art methods on 3D US volumes. Clinical assessments further confirmed the effectiveness of both GT++ and 3DFETUS, demonstrating statistically significant improvements in plane estimation accuracy.", "link": "http://arxiv.org/abs/2511.10412v2", "date": "2025-12-22", "relevancy": 2.2226, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5877}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5411}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203DFETUS%3A%20Deep%20Learning-Based%20Standardization%20of%20Facial%20Planes%20in%203D%20Ultrasound&body=Title%3A%203DFETUS%3A%20Deep%20Learning-Based%20Standardization%20of%20Facial%20Planes%20in%203D%20Ultrasound%0AAuthor%3A%20Alomar%20Antonia%20and%20Rubio%20Ricardo%20and%20Albaiges%20Gerard%20and%20Salort-Benejam%20Laura%20and%20Caminal%20Julia%20and%20Prat%20Maria%20and%20Rueda%20Carolina%20and%20Cortes%20Berta%20and%20Piella%20Gemma%20and%20Sukno%20Federico%0AAbstract%3A%20The%20automatic%20localization%20and%20standardization%20of%20anatomical%20planes%20in%203D%20medical%20imaging%20remains%20a%20challenging%20problem%20due%20to%20variability%20in%20object%20pose%2C%20appearance%2C%20and%20image%20quality.%20In%203D%20ultrasound%2C%20these%20challenges%20are%20exacerbated%20by%20speckle%20noise%20and%20limited%20contrast%2C%20particularly%20in%20fetal%20imaging.%0A%20%20To%20address%20these%20challenges%20in%20the%20context%20of%20facial%20assessment%2C%20we%20present%3A%201%29%20GT%2B%2B%2C%20a%20robust%20algorithm%20that%20estimates%20standard%20facial%20planes%20from%203D%20US%20volumes%20using%20annotated%20anatomical%20landmarks%3B%20and%202%29%203DFETUS%2C%20a%20deep%20learning%20model%20that%20automates%20and%20standardizes%20their%20localization%20in%203D%20fetal%20US%20volumes.%0A%20%20We%20evaluated%20our%20methods%20both%20qualitatively%2C%20through%20expert%20clinical%20review%2C%20and%20quantitatively.%20The%20proposed%20approach%20achieved%20a%20mean%20translation%20error%20of%203.21%20%24%5Cpm%24%201.98mm%20and%20a%20mean%20rotation%20error%20of%205.31%20%24%5Cpm%24%203.945%24%5E%5Ccirc%24%20per%20plane%2C%20outperforming%20other%20state-of-the-art%20methods%20on%203D%20US%20volumes.%20Clinical%20assessments%20further%20confirmed%20the%20effectiveness%20of%20both%20GT%2B%2B%20and%203DFETUS%2C%20demonstrating%20statistically%20significant%20improvements%20in%20plane%20estimation%20accuracy.%0ALink%3A%20http%3A//arxiv.org/abs/2511.10412v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3DFETUS%253A%2520Deep%2520Learning-Based%2520Standardization%2520of%2520Facial%2520Planes%2520in%25203D%2520Ultrasound%26entry.906535625%3DAlomar%2520Antonia%2520and%2520Rubio%2520Ricardo%2520and%2520Albaiges%2520Gerard%2520and%2520Salort-Benejam%2520Laura%2520and%2520Caminal%2520Julia%2520and%2520Prat%2520Maria%2520and%2520Rueda%2520Carolina%2520and%2520Cortes%2520Berta%2520and%2520Piella%2520Gemma%2520and%2520Sukno%2520Federico%26entry.1292438233%3DThe%2520automatic%2520localization%2520and%2520standardization%2520of%2520anatomical%2520planes%2520in%25203D%2520medical%2520imaging%2520remains%2520a%2520challenging%2520problem%2520due%2520to%2520variability%2520in%2520object%2520pose%252C%2520appearance%252C%2520and%2520image%2520quality.%2520In%25203D%2520ultrasound%252C%2520these%2520challenges%2520are%2520exacerbated%2520by%2520speckle%2520noise%2520and%2520limited%2520contrast%252C%2520particularly%2520in%2520fetal%2520imaging.%250A%2520%2520To%2520address%2520these%2520challenges%2520in%2520the%2520context%2520of%2520facial%2520assessment%252C%2520we%2520present%253A%25201%2529%2520GT%252B%252B%252C%2520a%2520robust%2520algorithm%2520that%2520estimates%2520standard%2520facial%2520planes%2520from%25203D%2520US%2520volumes%2520using%2520annotated%2520anatomical%2520landmarks%253B%2520and%25202%2529%25203DFETUS%252C%2520a%2520deep%2520learning%2520model%2520that%2520automates%2520and%2520standardizes%2520their%2520localization%2520in%25203D%2520fetal%2520US%2520volumes.%250A%2520%2520We%2520evaluated%2520our%2520methods%2520both%2520qualitatively%252C%2520through%2520expert%2520clinical%2520review%252C%2520and%2520quantitatively.%2520The%2520proposed%2520approach%2520achieved%2520a%2520mean%2520translation%2520error%2520of%25203.21%2520%2524%255Cpm%2524%25201.98mm%2520and%2520a%2520mean%2520rotation%2520error%2520of%25205.31%2520%2524%255Cpm%2524%25203.945%2524%255E%255Ccirc%2524%2520per%2520plane%252C%2520outperforming%2520other%2520state-of-the-art%2520methods%2520on%25203D%2520US%2520volumes.%2520Clinical%2520assessments%2520further%2520confirmed%2520the%2520effectiveness%2520of%2520both%2520GT%252B%252B%2520and%25203DFETUS%252C%2520demonstrating%2520statistically%2520significant%2520improvements%2520in%2520plane%2520estimation%2520accuracy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.10412v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3DFETUS%3A%20Deep%20Learning-Based%20Standardization%20of%20Facial%20Planes%20in%203D%20Ultrasound&entry.906535625=Alomar%20Antonia%20and%20Rubio%20Ricardo%20and%20Albaiges%20Gerard%20and%20Salort-Benejam%20Laura%20and%20Caminal%20Julia%20and%20Prat%20Maria%20and%20Rueda%20Carolina%20and%20Cortes%20Berta%20and%20Piella%20Gemma%20and%20Sukno%20Federico&entry.1292438233=The%20automatic%20localization%20and%20standardization%20of%20anatomical%20planes%20in%203D%20medical%20imaging%20remains%20a%20challenging%20problem%20due%20to%20variability%20in%20object%20pose%2C%20appearance%2C%20and%20image%20quality.%20In%203D%20ultrasound%2C%20these%20challenges%20are%20exacerbated%20by%20speckle%20noise%20and%20limited%20contrast%2C%20particularly%20in%20fetal%20imaging.%0A%20%20To%20address%20these%20challenges%20in%20the%20context%20of%20facial%20assessment%2C%20we%20present%3A%201%29%20GT%2B%2B%2C%20a%20robust%20algorithm%20that%20estimates%20standard%20facial%20planes%20from%203D%20US%20volumes%20using%20annotated%20anatomical%20landmarks%3B%20and%202%29%203DFETUS%2C%20a%20deep%20learning%20model%20that%20automates%20and%20standardizes%20their%20localization%20in%203D%20fetal%20US%20volumes.%0A%20%20We%20evaluated%20our%20methods%20both%20qualitatively%2C%20through%20expert%20clinical%20review%2C%20and%20quantitatively.%20The%20proposed%20approach%20achieved%20a%20mean%20translation%20error%20of%203.21%20%24%5Cpm%24%201.98mm%20and%20a%20mean%20rotation%20error%20of%205.31%20%24%5Cpm%24%203.945%24%5E%5Ccirc%24%20per%20plane%2C%20outperforming%20other%20state-of-the-art%20methods%20on%203D%20US%20volumes.%20Clinical%20assessments%20further%20confirmed%20the%20effectiveness%20of%20both%20GT%2B%2B%20and%203DFETUS%2C%20demonstrating%20statistically%20significant%20improvements%20in%20plane%20estimation%20accuracy.&entry.1838667208=http%3A//arxiv.org/abs/2511.10412v2&entry.124074799=Read"},
{"title": "CASA: Cross-Attention via Self-Attention for Efficient Vision-Language Fusion", "author": "Moritz B\u00f6hle and Am\u00e9lie Royer and Juliette Marrie and Edouard Grave and Patrick P\u00e9rez", "abstract": "Vision-language models (VLMs) are commonly trained by inserting image tokens from a pretrained vision encoder into the textual stream of a language model. This allows text and image information to fully attend to one another within the model, but becomes extremely costly for high-resolution images, long conversations, or streaming videos, both in memory and compute. VLMs leveraging cross-attention are an efficient alternative to token insertion but exhibit a clear performance gap, in particular on tasks involving fine-grained visual details. We find that a key to improving such models is to also enable local text-to-text interaction in the dedicated cross-attention layers. Building on this, we propose CASA, Cross-Attention via Self-Attention, a simple and efficient paradigm which substantially reduces the gap with full token insertion on common image understanding benchmarks, while enjoying the same scalability as cross-attention models when applied to long-context multimodal tasks such as streaming video captioning. For samples and code, please see our project page at https://kyutai.org/casa .", "link": "http://arxiv.org/abs/2512.19535v1", "date": "2025-12-22", "relevancy": 2.2088, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5626}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5509}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CASA%3A%20Cross-Attention%20via%20Self-Attention%20for%20Efficient%20Vision-Language%20Fusion&body=Title%3A%20CASA%3A%20Cross-Attention%20via%20Self-Attention%20for%20Efficient%20Vision-Language%20Fusion%0AAuthor%3A%20Moritz%20B%C3%B6hle%20and%20Am%C3%A9lie%20Royer%20and%20Juliette%20Marrie%20and%20Edouard%20Grave%20and%20Patrick%20P%C3%A9rez%0AAbstract%3A%20Vision-language%20models%20%28VLMs%29%20are%20commonly%20trained%20by%20inserting%20image%20tokens%20from%20a%20pretrained%20vision%20encoder%20into%20the%20textual%20stream%20of%20a%20language%20model.%20This%20allows%20text%20and%20image%20information%20to%20fully%20attend%20to%20one%20another%20within%20the%20model%2C%20but%20becomes%20extremely%20costly%20for%20high-resolution%20images%2C%20long%20conversations%2C%20or%20streaming%20videos%2C%20both%20in%20memory%20and%20compute.%20VLMs%20leveraging%20cross-attention%20are%20an%20efficient%20alternative%20to%20token%20insertion%20but%20exhibit%20a%20clear%20performance%20gap%2C%20in%20particular%20on%20tasks%20involving%20fine-grained%20visual%20details.%20We%20find%20that%20a%20key%20to%20improving%20such%20models%20is%20to%20also%20enable%20local%20text-to-text%20interaction%20in%20the%20dedicated%20cross-attention%20layers.%20Building%20on%20this%2C%20we%20propose%20CASA%2C%20Cross-Attention%20via%20Self-Attention%2C%20a%20simple%20and%20efficient%20paradigm%20which%20substantially%20reduces%20the%20gap%20with%20full%20token%20insertion%20on%20common%20image%20understanding%20benchmarks%2C%20while%20enjoying%20the%20same%20scalability%20as%20cross-attention%20models%20when%20applied%20to%20long-context%20multimodal%20tasks%20such%20as%20streaming%20video%20captioning.%20For%20samples%20and%20code%2C%20please%20see%20our%20project%20page%20at%20https%3A//kyutai.org/casa%20.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCASA%253A%2520Cross-Attention%2520via%2520Self-Attention%2520for%2520Efficient%2520Vision-Language%2520Fusion%26entry.906535625%3DMoritz%2520B%25C3%25B6hle%2520and%2520Am%25C3%25A9lie%2520Royer%2520and%2520Juliette%2520Marrie%2520and%2520Edouard%2520Grave%2520and%2520Patrick%2520P%25C3%25A9rez%26entry.1292438233%3DVision-language%2520models%2520%2528VLMs%2529%2520are%2520commonly%2520trained%2520by%2520inserting%2520image%2520tokens%2520from%2520a%2520pretrained%2520vision%2520encoder%2520into%2520the%2520textual%2520stream%2520of%2520a%2520language%2520model.%2520This%2520allows%2520text%2520and%2520image%2520information%2520to%2520fully%2520attend%2520to%2520one%2520another%2520within%2520the%2520model%252C%2520but%2520becomes%2520extremely%2520costly%2520for%2520high-resolution%2520images%252C%2520long%2520conversations%252C%2520or%2520streaming%2520videos%252C%2520both%2520in%2520memory%2520and%2520compute.%2520VLMs%2520leveraging%2520cross-attention%2520are%2520an%2520efficient%2520alternative%2520to%2520token%2520insertion%2520but%2520exhibit%2520a%2520clear%2520performance%2520gap%252C%2520in%2520particular%2520on%2520tasks%2520involving%2520fine-grained%2520visual%2520details.%2520We%2520find%2520that%2520a%2520key%2520to%2520improving%2520such%2520models%2520is%2520to%2520also%2520enable%2520local%2520text-to-text%2520interaction%2520in%2520the%2520dedicated%2520cross-attention%2520layers.%2520Building%2520on%2520this%252C%2520we%2520propose%2520CASA%252C%2520Cross-Attention%2520via%2520Self-Attention%252C%2520a%2520simple%2520and%2520efficient%2520paradigm%2520which%2520substantially%2520reduces%2520the%2520gap%2520with%2520full%2520token%2520insertion%2520on%2520common%2520image%2520understanding%2520benchmarks%252C%2520while%2520enjoying%2520the%2520same%2520scalability%2520as%2520cross-attention%2520models%2520when%2520applied%2520to%2520long-context%2520multimodal%2520tasks%2520such%2520as%2520streaming%2520video%2520captioning.%2520For%2520samples%2520and%2520code%252C%2520please%2520see%2520our%2520project%2520page%2520at%2520https%253A//kyutai.org/casa%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CASA%3A%20Cross-Attention%20via%20Self-Attention%20for%20Efficient%20Vision-Language%20Fusion&entry.906535625=Moritz%20B%C3%B6hle%20and%20Am%C3%A9lie%20Royer%20and%20Juliette%20Marrie%20and%20Edouard%20Grave%20and%20Patrick%20P%C3%A9rez&entry.1292438233=Vision-language%20models%20%28VLMs%29%20are%20commonly%20trained%20by%20inserting%20image%20tokens%20from%20a%20pretrained%20vision%20encoder%20into%20the%20textual%20stream%20of%20a%20language%20model.%20This%20allows%20text%20and%20image%20information%20to%20fully%20attend%20to%20one%20another%20within%20the%20model%2C%20but%20becomes%20extremely%20costly%20for%20high-resolution%20images%2C%20long%20conversations%2C%20or%20streaming%20videos%2C%20both%20in%20memory%20and%20compute.%20VLMs%20leveraging%20cross-attention%20are%20an%20efficient%20alternative%20to%20token%20insertion%20but%20exhibit%20a%20clear%20performance%20gap%2C%20in%20particular%20on%20tasks%20involving%20fine-grained%20visual%20details.%20We%20find%20that%20a%20key%20to%20improving%20such%20models%20is%20to%20also%20enable%20local%20text-to-text%20interaction%20in%20the%20dedicated%20cross-attention%20layers.%20Building%20on%20this%2C%20we%20propose%20CASA%2C%20Cross-Attention%20via%20Self-Attention%2C%20a%20simple%20and%20efficient%20paradigm%20which%20substantially%20reduces%20the%20gap%20with%20full%20token%20insertion%20on%20common%20image%20understanding%20benchmarks%2C%20while%20enjoying%20the%20same%20scalability%20as%20cross-attention%20models%20when%20applied%20to%20long-context%20multimodal%20tasks%20such%20as%20streaming%20video%20captioning.%20For%20samples%20and%20code%2C%20please%20see%20our%20project%20page%20at%20https%3A//kyutai.org/casa%20.&entry.1838667208=http%3A//arxiv.org/abs/2512.19535v1&entry.124074799=Read"},
{"title": "ReasonCD: A Multimodal Reasoning Large Model for Implicit Change-of-Interest Semantic Mining", "author": "Zhenyang Huang and Xiao Yu and Yi Zhang and Decheng Wang and Hang Ruan", "abstract": "Remote sensing image change detection is one of the fundamental tasks in remote sensing intelligent interpretation. Its core objective is to identify changes within change regions of interest (CRoI). Current multimodal large models encode rich human semantic knowledge, which is utilized for guidance in tasks such as remote sensing change detection. However, existing methods that use semantic guidance for detecting users' CRoI overly rely on explicit textual descriptions of CRoI, leading to the problem of near-complete performance failure when presented with implicit CRoI textual descriptions. This paper proposes a multimodal reasoning change detection model named ReasonCD, capable of mining users' implicit task intent. The model leverages the powerful reasoning capabilities of pre-trained large language models to mine users' implicit task intents and subsequently obtains different change detection results based on these intents. Experiments on public datasets demonstrate that the model achieves excellent change detection performance, with an F1 score of 92.1\\% on the BCDD dataset. Furthermore, to validate its superior reasoning functionality, this paper annotates a subset of reasoning data based on the SECOND dataset. Experimental results show that the model not only excels at basic reasoning-based change detection tasks but can also explain the reasoning process to aid human decision-making.", "link": "http://arxiv.org/abs/2512.19354v1", "date": "2025-12-22", "relevancy": 2.2013, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5704}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5463}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5463}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReasonCD%3A%20A%20Multimodal%20Reasoning%20Large%20Model%20for%20Implicit%20Change-of-Interest%20Semantic%20Mining&body=Title%3A%20ReasonCD%3A%20A%20Multimodal%20Reasoning%20Large%20Model%20for%20Implicit%20Change-of-Interest%20Semantic%20Mining%0AAuthor%3A%20Zhenyang%20Huang%20and%20Xiao%20Yu%20and%20Yi%20Zhang%20and%20Decheng%20Wang%20and%20Hang%20Ruan%0AAbstract%3A%20Remote%20sensing%20image%20change%20detection%20is%20one%20of%20the%20fundamental%20tasks%20in%20remote%20sensing%20intelligent%20interpretation.%20Its%20core%20objective%20is%20to%20identify%20changes%20within%20change%20regions%20of%20interest%20%28CRoI%29.%20Current%20multimodal%20large%20models%20encode%20rich%20human%20semantic%20knowledge%2C%20which%20is%20utilized%20for%20guidance%20in%20tasks%20such%20as%20remote%20sensing%20change%20detection.%20However%2C%20existing%20methods%20that%20use%20semantic%20guidance%20for%20detecting%20users%27%20CRoI%20overly%20rely%20on%20explicit%20textual%20descriptions%20of%20CRoI%2C%20leading%20to%20the%20problem%20of%20near-complete%20performance%20failure%20when%20presented%20with%20implicit%20CRoI%20textual%20descriptions.%20This%20paper%20proposes%20a%20multimodal%20reasoning%20change%20detection%20model%20named%20ReasonCD%2C%20capable%20of%20mining%20users%27%20implicit%20task%20intent.%20The%20model%20leverages%20the%20powerful%20reasoning%20capabilities%20of%20pre-trained%20large%20language%20models%20to%20mine%20users%27%20implicit%20task%20intents%20and%20subsequently%20obtains%20different%20change%20detection%20results%20based%20on%20these%20intents.%20Experiments%20on%20public%20datasets%20demonstrate%20that%20the%20model%20achieves%20excellent%20change%20detection%20performance%2C%20with%20an%20F1%20score%20of%2092.1%5C%25%20on%20the%20BCDD%20dataset.%20Furthermore%2C%20to%20validate%20its%20superior%20reasoning%20functionality%2C%20this%20paper%20annotates%20a%20subset%20of%20reasoning%20data%20based%20on%20the%20SECOND%20dataset.%20Experimental%20results%20show%20that%20the%20model%20not%20only%20excels%20at%20basic%20reasoning-based%20change%20detection%20tasks%20but%20can%20also%20explain%20the%20reasoning%20process%20to%20aid%20human%20decision-making.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19354v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasonCD%253A%2520A%2520Multimodal%2520Reasoning%2520Large%2520Model%2520for%2520Implicit%2520Change-of-Interest%2520Semantic%2520Mining%26entry.906535625%3DZhenyang%2520Huang%2520and%2520Xiao%2520Yu%2520and%2520Yi%2520Zhang%2520and%2520Decheng%2520Wang%2520and%2520Hang%2520Ruan%26entry.1292438233%3DRemote%2520sensing%2520image%2520change%2520detection%2520is%2520one%2520of%2520the%2520fundamental%2520tasks%2520in%2520remote%2520sensing%2520intelligent%2520interpretation.%2520Its%2520core%2520objective%2520is%2520to%2520identify%2520changes%2520within%2520change%2520regions%2520of%2520interest%2520%2528CRoI%2529.%2520Current%2520multimodal%2520large%2520models%2520encode%2520rich%2520human%2520semantic%2520knowledge%252C%2520which%2520is%2520utilized%2520for%2520guidance%2520in%2520tasks%2520such%2520as%2520remote%2520sensing%2520change%2520detection.%2520However%252C%2520existing%2520methods%2520that%2520use%2520semantic%2520guidance%2520for%2520detecting%2520users%2527%2520CRoI%2520overly%2520rely%2520on%2520explicit%2520textual%2520descriptions%2520of%2520CRoI%252C%2520leading%2520to%2520the%2520problem%2520of%2520near-complete%2520performance%2520failure%2520when%2520presented%2520with%2520implicit%2520CRoI%2520textual%2520descriptions.%2520This%2520paper%2520proposes%2520a%2520multimodal%2520reasoning%2520change%2520detection%2520model%2520named%2520ReasonCD%252C%2520capable%2520of%2520mining%2520users%2527%2520implicit%2520task%2520intent.%2520The%2520model%2520leverages%2520the%2520powerful%2520reasoning%2520capabilities%2520of%2520pre-trained%2520large%2520language%2520models%2520to%2520mine%2520users%2527%2520implicit%2520task%2520intents%2520and%2520subsequently%2520obtains%2520different%2520change%2520detection%2520results%2520based%2520on%2520these%2520intents.%2520Experiments%2520on%2520public%2520datasets%2520demonstrate%2520that%2520the%2520model%2520achieves%2520excellent%2520change%2520detection%2520performance%252C%2520with%2520an%2520F1%2520score%2520of%252092.1%255C%2525%2520on%2520the%2520BCDD%2520dataset.%2520Furthermore%252C%2520to%2520validate%2520its%2520superior%2520reasoning%2520functionality%252C%2520this%2520paper%2520annotates%2520a%2520subset%2520of%2520reasoning%2520data%2520based%2520on%2520the%2520SECOND%2520dataset.%2520Experimental%2520results%2520show%2520that%2520the%2520model%2520not%2520only%2520excels%2520at%2520basic%2520reasoning-based%2520change%2520detection%2520tasks%2520but%2520can%2520also%2520explain%2520the%2520reasoning%2520process%2520to%2520aid%2520human%2520decision-making.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19354v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReasonCD%3A%20A%20Multimodal%20Reasoning%20Large%20Model%20for%20Implicit%20Change-of-Interest%20Semantic%20Mining&entry.906535625=Zhenyang%20Huang%20and%20Xiao%20Yu%20and%20Yi%20Zhang%20and%20Decheng%20Wang%20and%20Hang%20Ruan&entry.1292438233=Remote%20sensing%20image%20change%20detection%20is%20one%20of%20the%20fundamental%20tasks%20in%20remote%20sensing%20intelligent%20interpretation.%20Its%20core%20objective%20is%20to%20identify%20changes%20within%20change%20regions%20of%20interest%20%28CRoI%29.%20Current%20multimodal%20large%20models%20encode%20rich%20human%20semantic%20knowledge%2C%20which%20is%20utilized%20for%20guidance%20in%20tasks%20such%20as%20remote%20sensing%20change%20detection.%20However%2C%20existing%20methods%20that%20use%20semantic%20guidance%20for%20detecting%20users%27%20CRoI%20overly%20rely%20on%20explicit%20textual%20descriptions%20of%20CRoI%2C%20leading%20to%20the%20problem%20of%20near-complete%20performance%20failure%20when%20presented%20with%20implicit%20CRoI%20textual%20descriptions.%20This%20paper%20proposes%20a%20multimodal%20reasoning%20change%20detection%20model%20named%20ReasonCD%2C%20capable%20of%20mining%20users%27%20implicit%20task%20intent.%20The%20model%20leverages%20the%20powerful%20reasoning%20capabilities%20of%20pre-trained%20large%20language%20models%20to%20mine%20users%27%20implicit%20task%20intents%20and%20subsequently%20obtains%20different%20change%20detection%20results%20based%20on%20these%20intents.%20Experiments%20on%20public%20datasets%20demonstrate%20that%20the%20model%20achieves%20excellent%20change%20detection%20performance%2C%20with%20an%20F1%20score%20of%2092.1%5C%25%20on%20the%20BCDD%20dataset.%20Furthermore%2C%20to%20validate%20its%20superior%20reasoning%20functionality%2C%20this%20paper%20annotates%20a%20subset%20of%20reasoning%20data%20based%20on%20the%20SECOND%20dataset.%20Experimental%20results%20show%20that%20the%20model%20not%20only%20excels%20at%20basic%20reasoning-based%20change%20detection%20tasks%20but%20can%20also%20explain%20the%20reasoning%20process%20to%20aid%20human%20decision-making.&entry.1838667208=http%3A//arxiv.org/abs/2512.19354v1&entry.124074799=Read"},
{"title": "Efficient Vision Mamba for MRI Super-Resolution via Hybrid Selective Scanning", "author": "Mojtaba Safari and Shansong Wang and Vanessa L Wildman and Mingzhe Hu and Zach Eidex and Chih-Wei Chang and Erik H Middlebrooks and Richard L. J Qiu and Pretesh Patel and Ashesh B. Jania and Hui Mao and Zhen Tian and Xiaofeng Yang", "abstract": "Background: High-resolution MRI is critical for diagnosis, but long acquisition times limit clinical use. Super-resolution (SR) can enhance resolution post-scan, yet existing deep learning methods face fidelity-efficiency trade-offs. Purpose: To develop a computationally efficient and accurate deep learning framework for MRI SR that preserves anatomical detail for clinical integration. Materials and Methods: We propose a novel SR framework combining multi-head selective state-space models (MHSSM) with a lightweight channel MLP. The model uses 2D patch extraction with hybrid scanning to capture long-range dependencies. Each MambaFormer block integrates MHSSM, depthwise convolutions, and gated channel mixing. Evaluation used 7T brain T1 MP2RAGE maps (n=142) and 1.5T prostate T2w MRI (n=334). Comparisons included Bicubic interpolation, GANs (CycleGAN, Pix2pix, SPSR), transformers (SwinIR), Mamba (MambaIR), and diffusion models (I2SB, Res-SRDiff). Results: Our model achieved superior performance with exceptional efficiency. For 7T brain data: SSIM=0.951+-0.021, PSNR=26.90+-1.41 dB, LPIPS=0.076+-0.022, GMSD=0.083+-0.017, significantly outperforming all baselines (p<0.001). For prostate data: SSIM=0.770+-0.049, PSNR=27.15+-2.19 dB, LPIPS=0.190+-0.095, GMSD=0.087+-0.013. The framework used only 0.9M parameters and 57 GFLOPs, reducing parameters by 99.8% and computation by 97.5% versus Res-SRDiff, while outperforming SwinIR and MambaIR in accuracy and efficiency. Conclusion: The proposed framework provides an efficient, accurate MRI SR solution, delivering enhanced anatomical detail across datasets. Its low computational demand and state-of-the-art performance show strong potential for clinical translation.", "link": "http://arxiv.org/abs/2512.19676v1", "date": "2025-12-22", "relevancy": 2.2007, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5594}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5481}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Vision%20Mamba%20for%20MRI%20Super-Resolution%20via%20Hybrid%20Selective%20Scanning&body=Title%3A%20Efficient%20Vision%20Mamba%20for%20MRI%20Super-Resolution%20via%20Hybrid%20Selective%20Scanning%0AAuthor%3A%20Mojtaba%20Safari%20and%20Shansong%20Wang%20and%20Vanessa%20L%20Wildman%20and%20Mingzhe%20Hu%20and%20Zach%20Eidex%20and%20Chih-Wei%20Chang%20and%20Erik%20H%20Middlebrooks%20and%20Richard%20L.%20J%20Qiu%20and%20Pretesh%20Patel%20and%20Ashesh%20B.%20Jania%20and%20Hui%20Mao%20and%20Zhen%20Tian%20and%20Xiaofeng%20Yang%0AAbstract%3A%20Background%3A%20High-resolution%20MRI%20is%20critical%20for%20diagnosis%2C%20but%20long%20acquisition%20times%20limit%20clinical%20use.%20Super-resolution%20%28SR%29%20can%20enhance%20resolution%20post-scan%2C%20yet%20existing%20deep%20learning%20methods%20face%20fidelity-efficiency%20trade-offs.%20Purpose%3A%20To%20develop%20a%20computationally%20efficient%20and%20accurate%20deep%20learning%20framework%20for%20MRI%20SR%20that%20preserves%20anatomical%20detail%20for%20clinical%20integration.%20Materials%20and%20Methods%3A%20We%20propose%20a%20novel%20SR%20framework%20combining%20multi-head%20selective%20state-space%20models%20%28MHSSM%29%20with%20a%20lightweight%20channel%20MLP.%20The%20model%20uses%202D%20patch%20extraction%20with%20hybrid%20scanning%20to%20capture%20long-range%20dependencies.%20Each%20MambaFormer%20block%20integrates%20MHSSM%2C%20depthwise%20convolutions%2C%20and%20gated%20channel%20mixing.%20Evaluation%20used%207T%20brain%20T1%20MP2RAGE%20maps%20%28n%3D142%29%20and%201.5T%20prostate%20T2w%20MRI%20%28n%3D334%29.%20Comparisons%20included%20Bicubic%20interpolation%2C%20GANs%20%28CycleGAN%2C%20Pix2pix%2C%20SPSR%29%2C%20transformers%20%28SwinIR%29%2C%20Mamba%20%28MambaIR%29%2C%20and%20diffusion%20models%20%28I2SB%2C%20Res-SRDiff%29.%20Results%3A%20Our%20model%20achieved%20superior%20performance%20with%20exceptional%20efficiency.%20For%207T%20brain%20data%3A%20SSIM%3D0.951%2B-0.021%2C%20PSNR%3D26.90%2B-1.41%20dB%2C%20LPIPS%3D0.076%2B-0.022%2C%20GMSD%3D0.083%2B-0.017%2C%20significantly%20outperforming%20all%20baselines%20%28p%3C0.001%29.%20For%20prostate%20data%3A%20SSIM%3D0.770%2B-0.049%2C%20PSNR%3D27.15%2B-2.19%20dB%2C%20LPIPS%3D0.190%2B-0.095%2C%20GMSD%3D0.087%2B-0.013.%20The%20framework%20used%20only%200.9M%20parameters%20and%2057%20GFLOPs%2C%20reducing%20parameters%20by%2099.8%25%20and%20computation%20by%2097.5%25%20versus%20Res-SRDiff%2C%20while%20outperforming%20SwinIR%20and%20MambaIR%20in%20accuracy%20and%20efficiency.%20Conclusion%3A%20The%20proposed%20framework%20provides%20an%20efficient%2C%20accurate%20MRI%20SR%20solution%2C%20delivering%20enhanced%20anatomical%20detail%20across%20datasets.%20Its%20low%20computational%20demand%20and%20state-of-the-art%20performance%20show%20strong%20potential%20for%20clinical%20translation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19676v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Vision%2520Mamba%2520for%2520MRI%2520Super-Resolution%2520via%2520Hybrid%2520Selective%2520Scanning%26entry.906535625%3DMojtaba%2520Safari%2520and%2520Shansong%2520Wang%2520and%2520Vanessa%2520L%2520Wildman%2520and%2520Mingzhe%2520Hu%2520and%2520Zach%2520Eidex%2520and%2520Chih-Wei%2520Chang%2520and%2520Erik%2520H%2520Middlebrooks%2520and%2520Richard%2520L.%2520J%2520Qiu%2520and%2520Pretesh%2520Patel%2520and%2520Ashesh%2520B.%2520Jania%2520and%2520Hui%2520Mao%2520and%2520Zhen%2520Tian%2520and%2520Xiaofeng%2520Yang%26entry.1292438233%3DBackground%253A%2520High-resolution%2520MRI%2520is%2520critical%2520for%2520diagnosis%252C%2520but%2520long%2520acquisition%2520times%2520limit%2520clinical%2520use.%2520Super-resolution%2520%2528SR%2529%2520can%2520enhance%2520resolution%2520post-scan%252C%2520yet%2520existing%2520deep%2520learning%2520methods%2520face%2520fidelity-efficiency%2520trade-offs.%2520Purpose%253A%2520To%2520develop%2520a%2520computationally%2520efficient%2520and%2520accurate%2520deep%2520learning%2520framework%2520for%2520MRI%2520SR%2520that%2520preserves%2520anatomical%2520detail%2520for%2520clinical%2520integration.%2520Materials%2520and%2520Methods%253A%2520We%2520propose%2520a%2520novel%2520SR%2520framework%2520combining%2520multi-head%2520selective%2520state-space%2520models%2520%2528MHSSM%2529%2520with%2520a%2520lightweight%2520channel%2520MLP.%2520The%2520model%2520uses%25202D%2520patch%2520extraction%2520with%2520hybrid%2520scanning%2520to%2520capture%2520long-range%2520dependencies.%2520Each%2520MambaFormer%2520block%2520integrates%2520MHSSM%252C%2520depthwise%2520convolutions%252C%2520and%2520gated%2520channel%2520mixing.%2520Evaluation%2520used%25207T%2520brain%2520T1%2520MP2RAGE%2520maps%2520%2528n%253D142%2529%2520and%25201.5T%2520prostate%2520T2w%2520MRI%2520%2528n%253D334%2529.%2520Comparisons%2520included%2520Bicubic%2520interpolation%252C%2520GANs%2520%2528CycleGAN%252C%2520Pix2pix%252C%2520SPSR%2529%252C%2520transformers%2520%2528SwinIR%2529%252C%2520Mamba%2520%2528MambaIR%2529%252C%2520and%2520diffusion%2520models%2520%2528I2SB%252C%2520Res-SRDiff%2529.%2520Results%253A%2520Our%2520model%2520achieved%2520superior%2520performance%2520with%2520exceptional%2520efficiency.%2520For%25207T%2520brain%2520data%253A%2520SSIM%253D0.951%252B-0.021%252C%2520PSNR%253D26.90%252B-1.41%2520dB%252C%2520LPIPS%253D0.076%252B-0.022%252C%2520GMSD%253D0.083%252B-0.017%252C%2520significantly%2520outperforming%2520all%2520baselines%2520%2528p%253C0.001%2529.%2520For%2520prostate%2520data%253A%2520SSIM%253D0.770%252B-0.049%252C%2520PSNR%253D27.15%252B-2.19%2520dB%252C%2520LPIPS%253D0.190%252B-0.095%252C%2520GMSD%253D0.087%252B-0.013.%2520The%2520framework%2520used%2520only%25200.9M%2520parameters%2520and%252057%2520GFLOPs%252C%2520reducing%2520parameters%2520by%252099.8%2525%2520and%2520computation%2520by%252097.5%2525%2520versus%2520Res-SRDiff%252C%2520while%2520outperforming%2520SwinIR%2520and%2520MambaIR%2520in%2520accuracy%2520and%2520efficiency.%2520Conclusion%253A%2520The%2520proposed%2520framework%2520provides%2520an%2520efficient%252C%2520accurate%2520MRI%2520SR%2520solution%252C%2520delivering%2520enhanced%2520anatomical%2520detail%2520across%2520datasets.%2520Its%2520low%2520computational%2520demand%2520and%2520state-of-the-art%2520performance%2520show%2520strong%2520potential%2520for%2520clinical%2520translation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19676v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Vision%20Mamba%20for%20MRI%20Super-Resolution%20via%20Hybrid%20Selective%20Scanning&entry.906535625=Mojtaba%20Safari%20and%20Shansong%20Wang%20and%20Vanessa%20L%20Wildman%20and%20Mingzhe%20Hu%20and%20Zach%20Eidex%20and%20Chih-Wei%20Chang%20and%20Erik%20H%20Middlebrooks%20and%20Richard%20L.%20J%20Qiu%20and%20Pretesh%20Patel%20and%20Ashesh%20B.%20Jania%20and%20Hui%20Mao%20and%20Zhen%20Tian%20and%20Xiaofeng%20Yang&entry.1292438233=Background%3A%20High-resolution%20MRI%20is%20critical%20for%20diagnosis%2C%20but%20long%20acquisition%20times%20limit%20clinical%20use.%20Super-resolution%20%28SR%29%20can%20enhance%20resolution%20post-scan%2C%20yet%20existing%20deep%20learning%20methods%20face%20fidelity-efficiency%20trade-offs.%20Purpose%3A%20To%20develop%20a%20computationally%20efficient%20and%20accurate%20deep%20learning%20framework%20for%20MRI%20SR%20that%20preserves%20anatomical%20detail%20for%20clinical%20integration.%20Materials%20and%20Methods%3A%20We%20propose%20a%20novel%20SR%20framework%20combining%20multi-head%20selective%20state-space%20models%20%28MHSSM%29%20with%20a%20lightweight%20channel%20MLP.%20The%20model%20uses%202D%20patch%20extraction%20with%20hybrid%20scanning%20to%20capture%20long-range%20dependencies.%20Each%20MambaFormer%20block%20integrates%20MHSSM%2C%20depthwise%20convolutions%2C%20and%20gated%20channel%20mixing.%20Evaluation%20used%207T%20brain%20T1%20MP2RAGE%20maps%20%28n%3D142%29%20and%201.5T%20prostate%20T2w%20MRI%20%28n%3D334%29.%20Comparisons%20included%20Bicubic%20interpolation%2C%20GANs%20%28CycleGAN%2C%20Pix2pix%2C%20SPSR%29%2C%20transformers%20%28SwinIR%29%2C%20Mamba%20%28MambaIR%29%2C%20and%20diffusion%20models%20%28I2SB%2C%20Res-SRDiff%29.%20Results%3A%20Our%20model%20achieved%20superior%20performance%20with%20exceptional%20efficiency.%20For%207T%20brain%20data%3A%20SSIM%3D0.951%2B-0.021%2C%20PSNR%3D26.90%2B-1.41%20dB%2C%20LPIPS%3D0.076%2B-0.022%2C%20GMSD%3D0.083%2B-0.017%2C%20significantly%20outperforming%20all%20baselines%20%28p%3C0.001%29.%20For%20prostate%20data%3A%20SSIM%3D0.770%2B-0.049%2C%20PSNR%3D27.15%2B-2.19%20dB%2C%20LPIPS%3D0.190%2B-0.095%2C%20GMSD%3D0.087%2B-0.013.%20The%20framework%20used%20only%200.9M%20parameters%20and%2057%20GFLOPs%2C%20reducing%20parameters%20by%2099.8%25%20and%20computation%20by%2097.5%25%20versus%20Res-SRDiff%2C%20while%20outperforming%20SwinIR%20and%20MambaIR%20in%20accuracy%20and%20efficiency.%20Conclusion%3A%20The%20proposed%20framework%20provides%20an%20efficient%2C%20accurate%20MRI%20SR%20solution%2C%20delivering%20enhanced%20anatomical%20detail%20across%20datasets.%20Its%20low%20computational%20demand%20and%20state-of-the-art%20performance%20show%20strong%20potential%20for%20clinical%20translation.&entry.1838667208=http%3A//arxiv.org/abs/2512.19676v1&entry.124074799=Read"},
{"title": "TwinAligner: Visual-Dynamic Alignment Empowers Physics-aware Real2Sim2Real for Robotic Manipulation", "author": "Hongwei Fan and Hang Dai and Jiyao Zhang and Jinzhou Li and Qiyang Yan and Yujie Zhao and Mingju Gao and Jinghang Wu and Hao Tang and Hao Dong", "abstract": "The robotics field is evolving towards data-driven, end-to-end learning, inspired by multimodal large models. However, reliance on expensive real-world data limits progress. Simulators offer cost-effective alternatives, but the gap between simulation and reality challenges effective policy transfer. This paper introduces TwinAligner, a novel Real2Sim2Real system that addresses both visual and dynamic gaps. The visual alignment module achieves pixel-level alignment through SDF reconstruction and editable 3DGS rendering, while the dynamic alignment module ensures dynamic consistency by identifying rigid physics from robot-object interaction. TwinAligner improves robot learning by providing scalable data collection and establishing a trustworthy iterative cycle, accelerating algorithm development. Quantitative evaluations highlight TwinAligner's strong capabilities in visual and dynamic real-to-sim alignment. This system enables policies trained in simulation to achieve strong zero-shot generalization to the real world. The high consistency between real-world and simulated policy performance underscores TwinAligner's potential to advance scalable robot learning. Code and data will be released on https://twin-aligner.github.io", "link": "http://arxiv.org/abs/2512.19390v1", "date": "2025-12-22", "relevancy": 2.1892, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5823}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5472}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TwinAligner%3A%20Visual-Dynamic%20Alignment%20Empowers%20Physics-aware%20Real2Sim2Real%20for%20Robotic%20Manipulation&body=Title%3A%20TwinAligner%3A%20Visual-Dynamic%20Alignment%20Empowers%20Physics-aware%20Real2Sim2Real%20for%20Robotic%20Manipulation%0AAuthor%3A%20Hongwei%20Fan%20and%20Hang%20Dai%20and%20Jiyao%20Zhang%20and%20Jinzhou%20Li%20and%20Qiyang%20Yan%20and%20Yujie%20Zhao%20and%20Mingju%20Gao%20and%20Jinghang%20Wu%20and%20Hao%20Tang%20and%20Hao%20Dong%0AAbstract%3A%20The%20robotics%20field%20is%20evolving%20towards%20data-driven%2C%20end-to-end%20learning%2C%20inspired%20by%20multimodal%20large%20models.%20However%2C%20reliance%20on%20expensive%20real-world%20data%20limits%20progress.%20Simulators%20offer%20cost-effective%20alternatives%2C%20but%20the%20gap%20between%20simulation%20and%20reality%20challenges%20effective%20policy%20transfer.%20This%20paper%20introduces%20TwinAligner%2C%20a%20novel%20Real2Sim2Real%20system%20that%20addresses%20both%20visual%20and%20dynamic%20gaps.%20The%20visual%20alignment%20module%20achieves%20pixel-level%20alignment%20through%20SDF%20reconstruction%20and%20editable%203DGS%20rendering%2C%20while%20the%20dynamic%20alignment%20module%20ensures%20dynamic%20consistency%20by%20identifying%20rigid%20physics%20from%20robot-object%20interaction.%20TwinAligner%20improves%20robot%20learning%20by%20providing%20scalable%20data%20collection%20and%20establishing%20a%20trustworthy%20iterative%20cycle%2C%20accelerating%20algorithm%20development.%20Quantitative%20evaluations%20highlight%20TwinAligner%27s%20strong%20capabilities%20in%20visual%20and%20dynamic%20real-to-sim%20alignment.%20This%20system%20enables%20policies%20trained%20in%20simulation%20to%20achieve%20strong%20zero-shot%20generalization%20to%20the%20real%20world.%20The%20high%20consistency%20between%20real-world%20and%20simulated%20policy%20performance%20underscores%20TwinAligner%27s%20potential%20to%20advance%20scalable%20robot%20learning.%20Code%20and%20data%20will%20be%20released%20on%20https%3A//twin-aligner.github.io%0ALink%3A%20http%3A//arxiv.org/abs/2512.19390v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTwinAligner%253A%2520Visual-Dynamic%2520Alignment%2520Empowers%2520Physics-aware%2520Real2Sim2Real%2520for%2520Robotic%2520Manipulation%26entry.906535625%3DHongwei%2520Fan%2520and%2520Hang%2520Dai%2520and%2520Jiyao%2520Zhang%2520and%2520Jinzhou%2520Li%2520and%2520Qiyang%2520Yan%2520and%2520Yujie%2520Zhao%2520and%2520Mingju%2520Gao%2520and%2520Jinghang%2520Wu%2520and%2520Hao%2520Tang%2520and%2520Hao%2520Dong%26entry.1292438233%3DThe%2520robotics%2520field%2520is%2520evolving%2520towards%2520data-driven%252C%2520end-to-end%2520learning%252C%2520inspired%2520by%2520multimodal%2520large%2520models.%2520However%252C%2520reliance%2520on%2520expensive%2520real-world%2520data%2520limits%2520progress.%2520Simulators%2520offer%2520cost-effective%2520alternatives%252C%2520but%2520the%2520gap%2520between%2520simulation%2520and%2520reality%2520challenges%2520effective%2520policy%2520transfer.%2520This%2520paper%2520introduces%2520TwinAligner%252C%2520a%2520novel%2520Real2Sim2Real%2520system%2520that%2520addresses%2520both%2520visual%2520and%2520dynamic%2520gaps.%2520The%2520visual%2520alignment%2520module%2520achieves%2520pixel-level%2520alignment%2520through%2520SDF%2520reconstruction%2520and%2520editable%25203DGS%2520rendering%252C%2520while%2520the%2520dynamic%2520alignment%2520module%2520ensures%2520dynamic%2520consistency%2520by%2520identifying%2520rigid%2520physics%2520from%2520robot-object%2520interaction.%2520TwinAligner%2520improves%2520robot%2520learning%2520by%2520providing%2520scalable%2520data%2520collection%2520and%2520establishing%2520a%2520trustworthy%2520iterative%2520cycle%252C%2520accelerating%2520algorithm%2520development.%2520Quantitative%2520evaluations%2520highlight%2520TwinAligner%2527s%2520strong%2520capabilities%2520in%2520visual%2520and%2520dynamic%2520real-to-sim%2520alignment.%2520This%2520system%2520enables%2520policies%2520trained%2520in%2520simulation%2520to%2520achieve%2520strong%2520zero-shot%2520generalization%2520to%2520the%2520real%2520world.%2520The%2520high%2520consistency%2520between%2520real-world%2520and%2520simulated%2520policy%2520performance%2520underscores%2520TwinAligner%2527s%2520potential%2520to%2520advance%2520scalable%2520robot%2520learning.%2520Code%2520and%2520data%2520will%2520be%2520released%2520on%2520https%253A//twin-aligner.github.io%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19390v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TwinAligner%3A%20Visual-Dynamic%20Alignment%20Empowers%20Physics-aware%20Real2Sim2Real%20for%20Robotic%20Manipulation&entry.906535625=Hongwei%20Fan%20and%20Hang%20Dai%20and%20Jiyao%20Zhang%20and%20Jinzhou%20Li%20and%20Qiyang%20Yan%20and%20Yujie%20Zhao%20and%20Mingju%20Gao%20and%20Jinghang%20Wu%20and%20Hao%20Tang%20and%20Hao%20Dong&entry.1292438233=The%20robotics%20field%20is%20evolving%20towards%20data-driven%2C%20end-to-end%20learning%2C%20inspired%20by%20multimodal%20large%20models.%20However%2C%20reliance%20on%20expensive%20real-world%20data%20limits%20progress.%20Simulators%20offer%20cost-effective%20alternatives%2C%20but%20the%20gap%20between%20simulation%20and%20reality%20challenges%20effective%20policy%20transfer.%20This%20paper%20introduces%20TwinAligner%2C%20a%20novel%20Real2Sim2Real%20system%20that%20addresses%20both%20visual%20and%20dynamic%20gaps.%20The%20visual%20alignment%20module%20achieves%20pixel-level%20alignment%20through%20SDF%20reconstruction%20and%20editable%203DGS%20rendering%2C%20while%20the%20dynamic%20alignment%20module%20ensures%20dynamic%20consistency%20by%20identifying%20rigid%20physics%20from%20robot-object%20interaction.%20TwinAligner%20improves%20robot%20learning%20by%20providing%20scalable%20data%20collection%20and%20establishing%20a%20trustworthy%20iterative%20cycle%2C%20accelerating%20algorithm%20development.%20Quantitative%20evaluations%20highlight%20TwinAligner%27s%20strong%20capabilities%20in%20visual%20and%20dynamic%20real-to-sim%20alignment.%20This%20system%20enables%20policies%20trained%20in%20simulation%20to%20achieve%20strong%20zero-shot%20generalization%20to%20the%20real%20world.%20The%20high%20consistency%20between%20real-world%20and%20simulated%20policy%20performance%20underscores%20TwinAligner%27s%20potential%20to%20advance%20scalable%20robot%20learning.%20Code%20and%20data%20will%20be%20released%20on%20https%3A//twin-aligner.github.io&entry.1838667208=http%3A//arxiv.org/abs/2512.19390v1&entry.124074799=Read"},
{"title": "Multi-Layer Confidence Scoring for Detection of Out-of-Distribution Samples, Adversarial Attacks, and In-Distribution Misclassifications", "author": "Lorenzo Capelli and Leandro de Souza Rosa and Gianluca Setti and Mauro Mangia and Riccardo Rovatti", "abstract": "The recent explosive growth in Deep Neural Networks applications raises concerns about the black-box usage of such models, with limited trasparency and trustworthiness in high-stakes domains, which have been crystallized as regulatory requirements such as the European Union Artificial Intelligence Act. While models with embedded confidence metrics have been proposed, such approaches cannot be applied to already existing models without retraining, limiting their broad application. On the other hand, post-hoc methods, which evaluate pre-trained models, focus on solving problems related to improving the confidence in the model's predictions, and detecting Out-Of-Distribution or Adversarial Attacks samples as independent applications. To tackle the limited applicability of already existing methods, we introduce Multi-Layer Analysis for Confidence Scoring (MACS), a unified post-hoc framework that analyzes intermediate activations to produce classification-maps. From the classification-maps, we derive a score applicable for confidence estimation, detecting distributional shifts and adversarial attacks, unifying the three problems in a common framework, and achieving performances that surpass the state-of-the-art approaches in our experiments with the VGG16 and ViTb16 models with a fraction of their computational overhead.", "link": "http://arxiv.org/abs/2512.19472v1", "date": "2025-12-22", "relevancy": 2.1874, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5478}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5473}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.546}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Layer%20Confidence%20Scoring%20for%20Detection%20of%20Out-of-Distribution%20Samples%2C%20Adversarial%20Attacks%2C%20and%20In-Distribution%20Misclassifications&body=Title%3A%20Multi-Layer%20Confidence%20Scoring%20for%20Detection%20of%20Out-of-Distribution%20Samples%2C%20Adversarial%20Attacks%2C%20and%20In-Distribution%20Misclassifications%0AAuthor%3A%20Lorenzo%20Capelli%20and%20Leandro%20de%20Souza%20Rosa%20and%20Gianluca%20Setti%20and%20Mauro%20Mangia%20and%20Riccardo%20Rovatti%0AAbstract%3A%20The%20recent%20explosive%20growth%20in%20Deep%20Neural%20Networks%20applications%20raises%20concerns%20about%20the%20black-box%20usage%20of%20such%20models%2C%20with%20limited%20trasparency%20and%20trustworthiness%20in%20high-stakes%20domains%2C%20which%20have%20been%20crystallized%20as%20regulatory%20requirements%20such%20as%20the%20European%20Union%20Artificial%20Intelligence%20Act.%20While%20models%20with%20embedded%20confidence%20metrics%20have%20been%20proposed%2C%20such%20approaches%20cannot%20be%20applied%20to%20already%20existing%20models%20without%20retraining%2C%20limiting%20their%20broad%20application.%20On%20the%20other%20hand%2C%20post-hoc%20methods%2C%20which%20evaluate%20pre-trained%20models%2C%20focus%20on%20solving%20problems%20related%20to%20improving%20the%20confidence%20in%20the%20model%27s%20predictions%2C%20and%20detecting%20Out-Of-Distribution%20or%20Adversarial%20Attacks%20samples%20as%20independent%20applications.%20To%20tackle%20the%20limited%20applicability%20of%20already%20existing%20methods%2C%20we%20introduce%20Multi-Layer%20Analysis%20for%20Confidence%20Scoring%20%28MACS%29%2C%20a%20unified%20post-hoc%20framework%20that%20analyzes%20intermediate%20activations%20to%20produce%20classification-maps.%20From%20the%20classification-maps%2C%20we%20derive%20a%20score%20applicable%20for%20confidence%20estimation%2C%20detecting%20distributional%20shifts%20and%20adversarial%20attacks%2C%20unifying%20the%20three%20problems%20in%20a%20common%20framework%2C%20and%20achieving%20performances%20that%20surpass%20the%20state-of-the-art%20approaches%20in%20our%20experiments%20with%20the%20VGG16%20and%20ViTb16%20models%20with%20a%20fraction%20of%20their%20computational%20overhead.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19472v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Layer%2520Confidence%2520Scoring%2520for%2520Detection%2520of%2520Out-of-Distribution%2520Samples%252C%2520Adversarial%2520Attacks%252C%2520and%2520In-Distribution%2520Misclassifications%26entry.906535625%3DLorenzo%2520Capelli%2520and%2520Leandro%2520de%2520Souza%2520Rosa%2520and%2520Gianluca%2520Setti%2520and%2520Mauro%2520Mangia%2520and%2520Riccardo%2520Rovatti%26entry.1292438233%3DThe%2520recent%2520explosive%2520growth%2520in%2520Deep%2520Neural%2520Networks%2520applications%2520raises%2520concerns%2520about%2520the%2520black-box%2520usage%2520of%2520such%2520models%252C%2520with%2520limited%2520trasparency%2520and%2520trustworthiness%2520in%2520high-stakes%2520domains%252C%2520which%2520have%2520been%2520crystallized%2520as%2520regulatory%2520requirements%2520such%2520as%2520the%2520European%2520Union%2520Artificial%2520Intelligence%2520Act.%2520While%2520models%2520with%2520embedded%2520confidence%2520metrics%2520have%2520been%2520proposed%252C%2520such%2520approaches%2520cannot%2520be%2520applied%2520to%2520already%2520existing%2520models%2520without%2520retraining%252C%2520limiting%2520their%2520broad%2520application.%2520On%2520the%2520other%2520hand%252C%2520post-hoc%2520methods%252C%2520which%2520evaluate%2520pre-trained%2520models%252C%2520focus%2520on%2520solving%2520problems%2520related%2520to%2520improving%2520the%2520confidence%2520in%2520the%2520model%2527s%2520predictions%252C%2520and%2520detecting%2520Out-Of-Distribution%2520or%2520Adversarial%2520Attacks%2520samples%2520as%2520independent%2520applications.%2520To%2520tackle%2520the%2520limited%2520applicability%2520of%2520already%2520existing%2520methods%252C%2520we%2520introduce%2520Multi-Layer%2520Analysis%2520for%2520Confidence%2520Scoring%2520%2528MACS%2529%252C%2520a%2520unified%2520post-hoc%2520framework%2520that%2520analyzes%2520intermediate%2520activations%2520to%2520produce%2520classification-maps.%2520From%2520the%2520classification-maps%252C%2520we%2520derive%2520a%2520score%2520applicable%2520for%2520confidence%2520estimation%252C%2520detecting%2520distributional%2520shifts%2520and%2520adversarial%2520attacks%252C%2520unifying%2520the%2520three%2520problems%2520in%2520a%2520common%2520framework%252C%2520and%2520achieving%2520performances%2520that%2520surpass%2520the%2520state-of-the-art%2520approaches%2520in%2520our%2520experiments%2520with%2520the%2520VGG16%2520and%2520ViTb16%2520models%2520with%2520a%2520fraction%2520of%2520their%2520computational%2520overhead.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19472v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Layer%20Confidence%20Scoring%20for%20Detection%20of%20Out-of-Distribution%20Samples%2C%20Adversarial%20Attacks%2C%20and%20In-Distribution%20Misclassifications&entry.906535625=Lorenzo%20Capelli%20and%20Leandro%20de%20Souza%20Rosa%20and%20Gianluca%20Setti%20and%20Mauro%20Mangia%20and%20Riccardo%20Rovatti&entry.1292438233=The%20recent%20explosive%20growth%20in%20Deep%20Neural%20Networks%20applications%20raises%20concerns%20about%20the%20black-box%20usage%20of%20such%20models%2C%20with%20limited%20trasparency%20and%20trustworthiness%20in%20high-stakes%20domains%2C%20which%20have%20been%20crystallized%20as%20regulatory%20requirements%20such%20as%20the%20European%20Union%20Artificial%20Intelligence%20Act.%20While%20models%20with%20embedded%20confidence%20metrics%20have%20been%20proposed%2C%20such%20approaches%20cannot%20be%20applied%20to%20already%20existing%20models%20without%20retraining%2C%20limiting%20their%20broad%20application.%20On%20the%20other%20hand%2C%20post-hoc%20methods%2C%20which%20evaluate%20pre-trained%20models%2C%20focus%20on%20solving%20problems%20related%20to%20improving%20the%20confidence%20in%20the%20model%27s%20predictions%2C%20and%20detecting%20Out-Of-Distribution%20or%20Adversarial%20Attacks%20samples%20as%20independent%20applications.%20To%20tackle%20the%20limited%20applicability%20of%20already%20existing%20methods%2C%20we%20introduce%20Multi-Layer%20Analysis%20for%20Confidence%20Scoring%20%28MACS%29%2C%20a%20unified%20post-hoc%20framework%20that%20analyzes%20intermediate%20activations%20to%20produce%20classification-maps.%20From%20the%20classification-maps%2C%20we%20derive%20a%20score%20applicable%20for%20confidence%20estimation%2C%20detecting%20distributional%20shifts%20and%20adversarial%20attacks%2C%20unifying%20the%20three%20problems%20in%20a%20common%20framework%2C%20and%20achieving%20performances%20that%20surpass%20the%20state-of-the-art%20approaches%20in%20our%20experiments%20with%20the%20VGG16%20and%20ViTb16%20models%20with%20a%20fraction%20of%20their%20computational%20overhead.&entry.1838667208=http%3A//arxiv.org/abs/2512.19472v1&entry.124074799=Read"},
{"title": "Orthogonal Approximate Message Passing with Optimal Spectral Initializations for Rectangular Spiked Matrix Models", "author": "Haohua Chen and Songbin Liu and Junjie Ma", "abstract": "We propose an orthogonal approximate message passing (OAMP) algorithm for signal estimation in the rectangular spiked matrix model with general rotationally invariant (RI) noise. We establish a rigorous state evolution that precisely characterizes the algorithm's high-dimensional dynamics and enables the construction of iteration-wise optimal denoisers. Within this framework, we accommodate spectral initializations under minimal assumptions on the empirical noise spectrum. In the rectangular setting, where a single rank-one component typically generates multiple informative outliers, we further propose a procedure for combining these outliers under mild non-Gaussian signal assumptions. For general RI noise models, the predicted performance of the proposed optimal OAMP algorithm agrees with replica-symmetric predictions for the associated Bayes-optimal estimator, and we conjecture that it is statistically optimal within a broad class of iterative estimation methods.", "link": "http://arxiv.org/abs/2512.19334v1", "date": "2025-12-22", "relevancy": 2.1837, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4496}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4377}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4229}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Orthogonal%20Approximate%20Message%20Passing%20with%20Optimal%20Spectral%20Initializations%20for%20Rectangular%20Spiked%20Matrix%20Models&body=Title%3A%20Orthogonal%20Approximate%20Message%20Passing%20with%20Optimal%20Spectral%20Initializations%20for%20Rectangular%20Spiked%20Matrix%20Models%0AAuthor%3A%20Haohua%20Chen%20and%20Songbin%20Liu%20and%20Junjie%20Ma%0AAbstract%3A%20We%20propose%20an%20orthogonal%20approximate%20message%20passing%20%28OAMP%29%20algorithm%20for%20signal%20estimation%20in%20the%20rectangular%20spiked%20matrix%20model%20with%20general%20rotationally%20invariant%20%28RI%29%20noise.%20We%20establish%20a%20rigorous%20state%20evolution%20that%20precisely%20characterizes%20the%20algorithm%27s%20high-dimensional%20dynamics%20and%20enables%20the%20construction%20of%20iteration-wise%20optimal%20denoisers.%20Within%20this%20framework%2C%20we%20accommodate%20spectral%20initializations%20under%20minimal%20assumptions%20on%20the%20empirical%20noise%20spectrum.%20In%20the%20rectangular%20setting%2C%20where%20a%20single%20rank-one%20component%20typically%20generates%20multiple%20informative%20outliers%2C%20we%20further%20propose%20a%20procedure%20for%20combining%20these%20outliers%20under%20mild%20non-Gaussian%20signal%20assumptions.%20For%20general%20RI%20noise%20models%2C%20the%20predicted%20performance%20of%20the%20proposed%20optimal%20OAMP%20algorithm%20agrees%20with%20replica-symmetric%20predictions%20for%20the%20associated%20Bayes-optimal%20estimator%2C%20and%20we%20conjecture%20that%20it%20is%20statistically%20optimal%20within%20a%20broad%20class%20of%20iterative%20estimation%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19334v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOrthogonal%2520Approximate%2520Message%2520Passing%2520with%2520Optimal%2520Spectral%2520Initializations%2520for%2520Rectangular%2520Spiked%2520Matrix%2520Models%26entry.906535625%3DHaohua%2520Chen%2520and%2520Songbin%2520Liu%2520and%2520Junjie%2520Ma%26entry.1292438233%3DWe%2520propose%2520an%2520orthogonal%2520approximate%2520message%2520passing%2520%2528OAMP%2529%2520algorithm%2520for%2520signal%2520estimation%2520in%2520the%2520rectangular%2520spiked%2520matrix%2520model%2520with%2520general%2520rotationally%2520invariant%2520%2528RI%2529%2520noise.%2520We%2520establish%2520a%2520rigorous%2520state%2520evolution%2520that%2520precisely%2520characterizes%2520the%2520algorithm%2527s%2520high-dimensional%2520dynamics%2520and%2520enables%2520the%2520construction%2520of%2520iteration-wise%2520optimal%2520denoisers.%2520Within%2520this%2520framework%252C%2520we%2520accommodate%2520spectral%2520initializations%2520under%2520minimal%2520assumptions%2520on%2520the%2520empirical%2520noise%2520spectrum.%2520In%2520the%2520rectangular%2520setting%252C%2520where%2520a%2520single%2520rank-one%2520component%2520typically%2520generates%2520multiple%2520informative%2520outliers%252C%2520we%2520further%2520propose%2520a%2520procedure%2520for%2520combining%2520these%2520outliers%2520under%2520mild%2520non-Gaussian%2520signal%2520assumptions.%2520For%2520general%2520RI%2520noise%2520models%252C%2520the%2520predicted%2520performance%2520of%2520the%2520proposed%2520optimal%2520OAMP%2520algorithm%2520agrees%2520with%2520replica-symmetric%2520predictions%2520for%2520the%2520associated%2520Bayes-optimal%2520estimator%252C%2520and%2520we%2520conjecture%2520that%2520it%2520is%2520statistically%2520optimal%2520within%2520a%2520broad%2520class%2520of%2520iterative%2520estimation%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19334v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Orthogonal%20Approximate%20Message%20Passing%20with%20Optimal%20Spectral%20Initializations%20for%20Rectangular%20Spiked%20Matrix%20Models&entry.906535625=Haohua%20Chen%20and%20Songbin%20Liu%20and%20Junjie%20Ma&entry.1292438233=We%20propose%20an%20orthogonal%20approximate%20message%20passing%20%28OAMP%29%20algorithm%20for%20signal%20estimation%20in%20the%20rectangular%20spiked%20matrix%20model%20with%20general%20rotationally%20invariant%20%28RI%29%20noise.%20We%20establish%20a%20rigorous%20state%20evolution%20that%20precisely%20characterizes%20the%20algorithm%27s%20high-dimensional%20dynamics%20and%20enables%20the%20construction%20of%20iteration-wise%20optimal%20denoisers.%20Within%20this%20framework%2C%20we%20accommodate%20spectral%20initializations%20under%20minimal%20assumptions%20on%20the%20empirical%20noise%20spectrum.%20In%20the%20rectangular%20setting%2C%20where%20a%20single%20rank-one%20component%20typically%20generates%20multiple%20informative%20outliers%2C%20we%20further%20propose%20a%20procedure%20for%20combining%20these%20outliers%20under%20mild%20non-Gaussian%20signal%20assumptions.%20For%20general%20RI%20noise%20models%2C%20the%20predicted%20performance%20of%20the%20proposed%20optimal%20OAMP%20algorithm%20agrees%20with%20replica-symmetric%20predictions%20for%20the%20associated%20Bayes-optimal%20estimator%2C%20and%20we%20conjecture%20that%20it%20is%20statistically%20optimal%20within%20a%20broad%20class%20of%20iterative%20estimation%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2512.19334v1&entry.124074799=Read"},
{"title": "TrackNetV5: Residual-Driven Spatio-Temporal Refinement and Motion Direction Decoupling for Fast Object Tracking", "author": "Tang Haonan and Chen Yanjun and Jiang Lezhi", "abstract": "The TrackNet series has established a strong baseline for fast-moving small object tracking in sports. However, existing iterations face significant limitations: V1-V3 struggle with occlusions due to a reliance on purely visual cues, while TrackNetV4, despite introducing motion inputs, suffers from directional ambiguity as its absolute difference method discards motion polarity. To overcome these bottlenecks, we propose TrackNetV5, a robust architecture integrating two novel mechanisms. First, to recover lost directional priors, we introduce the Motion Direction Decoupling (MDD) module. Unlike V4, MDD decomposes temporal dynamics into signed polarity fields, explicitly encoding both movement occurrence and trajectory direction. Second, we propose the Residual-Driven Spatio-Temporal Refinement (R-STR) head. Operating on a coarse-to-fine paradigm, this Transformer-based module leverages factorized spatio-temporal contexts to estimate a corrective residual, effectively recovering occluded targets. Extensive experiments on the TrackNetV2 dataset demonstrate that TrackNetV5 achieves a new state-of-the-art F1-score of 0.9859 and an accuracy of 0.9733, significantly outperforming previous versions. Notably, this performance leap is achieved with a marginal 3.7% increase in FLOPs compared to V4, maintaining real-time inference capabilities while delivering superior tracking precision.", "link": "http://arxiv.org/abs/2512.02789v2", "date": "2025-12-22", "relevancy": 2.1836, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5629}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5383}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.532}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TrackNetV5%3A%20Residual-Driven%20Spatio-Temporal%20Refinement%20and%20Motion%20Direction%20Decoupling%20for%20Fast%20Object%20Tracking&body=Title%3A%20TrackNetV5%3A%20Residual-Driven%20Spatio-Temporal%20Refinement%20and%20Motion%20Direction%20Decoupling%20for%20Fast%20Object%20Tracking%0AAuthor%3A%20Tang%20Haonan%20and%20Chen%20Yanjun%20and%20Jiang%20Lezhi%0AAbstract%3A%20The%20TrackNet%20series%20has%20established%20a%20strong%20baseline%20for%20fast-moving%20small%20object%20tracking%20in%20sports.%20However%2C%20existing%20iterations%20face%20significant%20limitations%3A%20V1-V3%20struggle%20with%20occlusions%20due%20to%20a%20reliance%20on%20purely%20visual%20cues%2C%20while%20TrackNetV4%2C%20despite%20introducing%20motion%20inputs%2C%20suffers%20from%20directional%20ambiguity%20as%20its%20absolute%20difference%20method%20discards%20motion%20polarity.%20To%20overcome%20these%20bottlenecks%2C%20we%20propose%20TrackNetV5%2C%20a%20robust%20architecture%20integrating%20two%20novel%20mechanisms.%20First%2C%20to%20recover%20lost%20directional%20priors%2C%20we%20introduce%20the%20Motion%20Direction%20Decoupling%20%28MDD%29%20module.%20Unlike%20V4%2C%20MDD%20decomposes%20temporal%20dynamics%20into%20signed%20polarity%20fields%2C%20explicitly%20encoding%20both%20movement%20occurrence%20and%20trajectory%20direction.%20Second%2C%20we%20propose%20the%20Residual-Driven%20Spatio-Temporal%20Refinement%20%28R-STR%29%20head.%20Operating%20on%20a%20coarse-to-fine%20paradigm%2C%20this%20Transformer-based%20module%20leverages%20factorized%20spatio-temporal%20contexts%20to%20estimate%20a%20corrective%20residual%2C%20effectively%20recovering%20occluded%20targets.%20Extensive%20experiments%20on%20the%20TrackNetV2%20dataset%20demonstrate%20that%20TrackNetV5%20achieves%20a%20new%20state-of-the-art%20F1-score%20of%200.9859%20and%20an%20accuracy%20of%200.9733%2C%20significantly%20outperforming%20previous%20versions.%20Notably%2C%20this%20performance%20leap%20is%20achieved%20with%20a%20marginal%203.7%25%20increase%20in%20FLOPs%20compared%20to%20V4%2C%20maintaining%20real-time%20inference%20capabilities%20while%20delivering%20superior%20tracking%20precision.%0ALink%3A%20http%3A//arxiv.org/abs/2512.02789v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrackNetV5%253A%2520Residual-Driven%2520Spatio-Temporal%2520Refinement%2520and%2520Motion%2520Direction%2520Decoupling%2520for%2520Fast%2520Object%2520Tracking%26entry.906535625%3DTang%2520Haonan%2520and%2520Chen%2520Yanjun%2520and%2520Jiang%2520Lezhi%26entry.1292438233%3DThe%2520TrackNet%2520series%2520has%2520established%2520a%2520strong%2520baseline%2520for%2520fast-moving%2520small%2520object%2520tracking%2520in%2520sports.%2520However%252C%2520existing%2520iterations%2520face%2520significant%2520limitations%253A%2520V1-V3%2520struggle%2520with%2520occlusions%2520due%2520to%2520a%2520reliance%2520on%2520purely%2520visual%2520cues%252C%2520while%2520TrackNetV4%252C%2520despite%2520introducing%2520motion%2520inputs%252C%2520suffers%2520from%2520directional%2520ambiguity%2520as%2520its%2520absolute%2520difference%2520method%2520discards%2520motion%2520polarity.%2520To%2520overcome%2520these%2520bottlenecks%252C%2520we%2520propose%2520TrackNetV5%252C%2520a%2520robust%2520architecture%2520integrating%2520two%2520novel%2520mechanisms.%2520First%252C%2520to%2520recover%2520lost%2520directional%2520priors%252C%2520we%2520introduce%2520the%2520Motion%2520Direction%2520Decoupling%2520%2528MDD%2529%2520module.%2520Unlike%2520V4%252C%2520MDD%2520decomposes%2520temporal%2520dynamics%2520into%2520signed%2520polarity%2520fields%252C%2520explicitly%2520encoding%2520both%2520movement%2520occurrence%2520and%2520trajectory%2520direction.%2520Second%252C%2520we%2520propose%2520the%2520Residual-Driven%2520Spatio-Temporal%2520Refinement%2520%2528R-STR%2529%2520head.%2520Operating%2520on%2520a%2520coarse-to-fine%2520paradigm%252C%2520this%2520Transformer-based%2520module%2520leverages%2520factorized%2520spatio-temporal%2520contexts%2520to%2520estimate%2520a%2520corrective%2520residual%252C%2520effectively%2520recovering%2520occluded%2520targets.%2520Extensive%2520experiments%2520on%2520the%2520TrackNetV2%2520dataset%2520demonstrate%2520that%2520TrackNetV5%2520achieves%2520a%2520new%2520state-of-the-art%2520F1-score%2520of%25200.9859%2520and%2520an%2520accuracy%2520of%25200.9733%252C%2520significantly%2520outperforming%2520previous%2520versions.%2520Notably%252C%2520this%2520performance%2520leap%2520is%2520achieved%2520with%2520a%2520marginal%25203.7%2525%2520increase%2520in%2520FLOPs%2520compared%2520to%2520V4%252C%2520maintaining%2520real-time%2520inference%2520capabilities%2520while%2520delivering%2520superior%2520tracking%2520precision.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.02789v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TrackNetV5%3A%20Residual-Driven%20Spatio-Temporal%20Refinement%20and%20Motion%20Direction%20Decoupling%20for%20Fast%20Object%20Tracking&entry.906535625=Tang%20Haonan%20and%20Chen%20Yanjun%20and%20Jiang%20Lezhi&entry.1292438233=The%20TrackNet%20series%20has%20established%20a%20strong%20baseline%20for%20fast-moving%20small%20object%20tracking%20in%20sports.%20However%2C%20existing%20iterations%20face%20significant%20limitations%3A%20V1-V3%20struggle%20with%20occlusions%20due%20to%20a%20reliance%20on%20purely%20visual%20cues%2C%20while%20TrackNetV4%2C%20despite%20introducing%20motion%20inputs%2C%20suffers%20from%20directional%20ambiguity%20as%20its%20absolute%20difference%20method%20discards%20motion%20polarity.%20To%20overcome%20these%20bottlenecks%2C%20we%20propose%20TrackNetV5%2C%20a%20robust%20architecture%20integrating%20two%20novel%20mechanisms.%20First%2C%20to%20recover%20lost%20directional%20priors%2C%20we%20introduce%20the%20Motion%20Direction%20Decoupling%20%28MDD%29%20module.%20Unlike%20V4%2C%20MDD%20decomposes%20temporal%20dynamics%20into%20signed%20polarity%20fields%2C%20explicitly%20encoding%20both%20movement%20occurrence%20and%20trajectory%20direction.%20Second%2C%20we%20propose%20the%20Residual-Driven%20Spatio-Temporal%20Refinement%20%28R-STR%29%20head.%20Operating%20on%20a%20coarse-to-fine%20paradigm%2C%20this%20Transformer-based%20module%20leverages%20factorized%20spatio-temporal%20contexts%20to%20estimate%20a%20corrective%20residual%2C%20effectively%20recovering%20occluded%20targets.%20Extensive%20experiments%20on%20the%20TrackNetV2%20dataset%20demonstrate%20that%20TrackNetV5%20achieves%20a%20new%20state-of-the-art%20F1-score%20of%200.9859%20and%20an%20accuracy%20of%200.9733%2C%20significantly%20outperforming%20previous%20versions.%20Notably%2C%20this%20performance%20leap%20is%20achieved%20with%20a%20marginal%203.7%25%20increase%20in%20FLOPs%20compared%20to%20V4%2C%20maintaining%20real-time%20inference%20capabilities%20while%20delivering%20superior%20tracking%20precision.&entry.1838667208=http%3A//arxiv.org/abs/2512.02789v2&entry.124074799=Read"},
{"title": "GraphShaper: Geometry-aware Alignment for Improving Transfer Learning in Text-Attributed Graphs", "author": "Heng Zhang and Tianyi Zhang and Yuling Shi and Xiaodong Gu and Yaomin Shen and Haochen You and Zijian Zhang and Yilei Yuan and Jin Huang", "abstract": "Graph foundation models represent a transformative paradigm for learning transferable representations across diverse graph domains. Recent methods leverage large language models to unify graph and text modalities into a shared representation space using contrastive learning. However, systematic evaluations reveal significant performance degradation at structural boundaries where distinct topological patterns converge, with accuracy losses exceeding 20 percentage points. This issue arises from a key limitation: current methods assume all graph structures can be encoded within a single Euclidean space. In reality, tree structures require hyperbolic geometry to preserve hierarchical branching, while cyclic patterns depend on spherical geometry for closure properties. At structural boundaries, nodes experience conflicting geometric constraints that uniform encoding spaces cannot resolve. This raises a crucial challenge: \\textbf{Can alignment frameworks be designed to respect the intrinsic geometric diversity of graph structures?} We introduce \\textbf{GraphShaper}, a geometry-aware framework that enhances graph encoding through multi-geometric specialization. Our approach employs expert networks tailored to different geometric spaces, dynamically computing fusion weights to adaptively integrate geometric properties based on local structural characteristics. This adaptive fusion preserves structural integrity before alignment with text embeddings. Extensive experiments demonstrate that GraphShaper achieves 9.47\\% accuracy improvements on citation networks and 7.63\\% on social networks in zero-shot settings.", "link": "http://arxiv.org/abs/2510.12085v2", "date": "2025-12-22", "relevancy": 2.1679, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5571}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5335}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5251}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphShaper%3A%20Geometry-aware%20Alignment%20for%20Improving%20Transfer%20Learning%20in%20Text-Attributed%20Graphs&body=Title%3A%20GraphShaper%3A%20Geometry-aware%20Alignment%20for%20Improving%20Transfer%20Learning%20in%20Text-Attributed%20Graphs%0AAuthor%3A%20Heng%20Zhang%20and%20Tianyi%20Zhang%20and%20Yuling%20Shi%20and%20Xiaodong%20Gu%20and%20Yaomin%20Shen%20and%20Haochen%20You%20and%20Zijian%20Zhang%20and%20Yilei%20Yuan%20and%20Jin%20Huang%0AAbstract%3A%20Graph%20foundation%20models%20represent%20a%20transformative%20paradigm%20for%20learning%20transferable%20representations%20across%20diverse%20graph%20domains.%20Recent%20methods%20leverage%20large%20language%20models%20to%20unify%20graph%20and%20text%20modalities%20into%20a%20shared%20representation%20space%20using%20contrastive%20learning.%20However%2C%20systematic%20evaluations%20reveal%20significant%20performance%20degradation%20at%20structural%20boundaries%20where%20distinct%20topological%20patterns%20converge%2C%20with%20accuracy%20losses%20exceeding%2020%20percentage%20points.%20This%20issue%20arises%20from%20a%20key%20limitation%3A%20current%20methods%20assume%20all%20graph%20structures%20can%20be%20encoded%20within%20a%20single%20Euclidean%20space.%20In%20reality%2C%20tree%20structures%20require%20hyperbolic%20geometry%20to%20preserve%20hierarchical%20branching%2C%20while%20cyclic%20patterns%20depend%20on%20spherical%20geometry%20for%20closure%20properties.%20At%20structural%20boundaries%2C%20nodes%20experience%20conflicting%20geometric%20constraints%20that%20uniform%20encoding%20spaces%20cannot%20resolve.%20This%20raises%20a%20crucial%20challenge%3A%20%5Ctextbf%7BCan%20alignment%20frameworks%20be%20designed%20to%20respect%20the%20intrinsic%20geometric%20diversity%20of%20graph%20structures%3F%7D%20We%20introduce%20%5Ctextbf%7BGraphShaper%7D%2C%20a%20geometry-aware%20framework%20that%20enhances%20graph%20encoding%20through%20multi-geometric%20specialization.%20Our%20approach%20employs%20expert%20networks%20tailored%20to%20different%20geometric%20spaces%2C%20dynamically%20computing%20fusion%20weights%20to%20adaptively%20integrate%20geometric%20properties%20based%20on%20local%20structural%20characteristics.%20This%20adaptive%20fusion%20preserves%20structural%20integrity%20before%20alignment%20with%20text%20embeddings.%20Extensive%20experiments%20demonstrate%20that%20GraphShaper%20achieves%209.47%5C%25%20accuracy%20improvements%20on%20citation%20networks%20and%207.63%5C%25%20on%20social%20networks%20in%20zero-shot%20settings.%0ALink%3A%20http%3A//arxiv.org/abs/2510.12085v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphShaper%253A%2520Geometry-aware%2520Alignment%2520for%2520Improving%2520Transfer%2520Learning%2520in%2520Text-Attributed%2520Graphs%26entry.906535625%3DHeng%2520Zhang%2520and%2520Tianyi%2520Zhang%2520and%2520Yuling%2520Shi%2520and%2520Xiaodong%2520Gu%2520and%2520Yaomin%2520Shen%2520and%2520Haochen%2520You%2520and%2520Zijian%2520Zhang%2520and%2520Yilei%2520Yuan%2520and%2520Jin%2520Huang%26entry.1292438233%3DGraph%2520foundation%2520models%2520represent%2520a%2520transformative%2520paradigm%2520for%2520learning%2520transferable%2520representations%2520across%2520diverse%2520graph%2520domains.%2520Recent%2520methods%2520leverage%2520large%2520language%2520models%2520to%2520unify%2520graph%2520and%2520text%2520modalities%2520into%2520a%2520shared%2520representation%2520space%2520using%2520contrastive%2520learning.%2520However%252C%2520systematic%2520evaluations%2520reveal%2520significant%2520performance%2520degradation%2520at%2520structural%2520boundaries%2520where%2520distinct%2520topological%2520patterns%2520converge%252C%2520with%2520accuracy%2520losses%2520exceeding%252020%2520percentage%2520points.%2520This%2520issue%2520arises%2520from%2520a%2520key%2520limitation%253A%2520current%2520methods%2520assume%2520all%2520graph%2520structures%2520can%2520be%2520encoded%2520within%2520a%2520single%2520Euclidean%2520space.%2520In%2520reality%252C%2520tree%2520structures%2520require%2520hyperbolic%2520geometry%2520to%2520preserve%2520hierarchical%2520branching%252C%2520while%2520cyclic%2520patterns%2520depend%2520on%2520spherical%2520geometry%2520for%2520closure%2520properties.%2520At%2520structural%2520boundaries%252C%2520nodes%2520experience%2520conflicting%2520geometric%2520constraints%2520that%2520uniform%2520encoding%2520spaces%2520cannot%2520resolve.%2520This%2520raises%2520a%2520crucial%2520challenge%253A%2520%255Ctextbf%257BCan%2520alignment%2520frameworks%2520be%2520designed%2520to%2520respect%2520the%2520intrinsic%2520geometric%2520diversity%2520of%2520graph%2520structures%253F%257D%2520We%2520introduce%2520%255Ctextbf%257BGraphShaper%257D%252C%2520a%2520geometry-aware%2520framework%2520that%2520enhances%2520graph%2520encoding%2520through%2520multi-geometric%2520specialization.%2520Our%2520approach%2520employs%2520expert%2520networks%2520tailored%2520to%2520different%2520geometric%2520spaces%252C%2520dynamically%2520computing%2520fusion%2520weights%2520to%2520adaptively%2520integrate%2520geometric%2520properties%2520based%2520on%2520local%2520structural%2520characteristics.%2520This%2520adaptive%2520fusion%2520preserves%2520structural%2520integrity%2520before%2520alignment%2520with%2520text%2520embeddings.%2520Extensive%2520experiments%2520demonstrate%2520that%2520GraphShaper%2520achieves%25209.47%255C%2525%2520accuracy%2520improvements%2520on%2520citation%2520networks%2520and%25207.63%255C%2525%2520on%2520social%2520networks%2520in%2520zero-shot%2520settings.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12085v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphShaper%3A%20Geometry-aware%20Alignment%20for%20Improving%20Transfer%20Learning%20in%20Text-Attributed%20Graphs&entry.906535625=Heng%20Zhang%20and%20Tianyi%20Zhang%20and%20Yuling%20Shi%20and%20Xiaodong%20Gu%20and%20Yaomin%20Shen%20and%20Haochen%20You%20and%20Zijian%20Zhang%20and%20Yilei%20Yuan%20and%20Jin%20Huang&entry.1292438233=Graph%20foundation%20models%20represent%20a%20transformative%20paradigm%20for%20learning%20transferable%20representations%20across%20diverse%20graph%20domains.%20Recent%20methods%20leverage%20large%20language%20models%20to%20unify%20graph%20and%20text%20modalities%20into%20a%20shared%20representation%20space%20using%20contrastive%20learning.%20However%2C%20systematic%20evaluations%20reveal%20significant%20performance%20degradation%20at%20structural%20boundaries%20where%20distinct%20topological%20patterns%20converge%2C%20with%20accuracy%20losses%20exceeding%2020%20percentage%20points.%20This%20issue%20arises%20from%20a%20key%20limitation%3A%20current%20methods%20assume%20all%20graph%20structures%20can%20be%20encoded%20within%20a%20single%20Euclidean%20space.%20In%20reality%2C%20tree%20structures%20require%20hyperbolic%20geometry%20to%20preserve%20hierarchical%20branching%2C%20while%20cyclic%20patterns%20depend%20on%20spherical%20geometry%20for%20closure%20properties.%20At%20structural%20boundaries%2C%20nodes%20experience%20conflicting%20geometric%20constraints%20that%20uniform%20encoding%20spaces%20cannot%20resolve.%20This%20raises%20a%20crucial%20challenge%3A%20%5Ctextbf%7BCan%20alignment%20frameworks%20be%20designed%20to%20respect%20the%20intrinsic%20geometric%20diversity%20of%20graph%20structures%3F%7D%20We%20introduce%20%5Ctextbf%7BGraphShaper%7D%2C%20a%20geometry-aware%20framework%20that%20enhances%20graph%20encoding%20through%20multi-geometric%20specialization.%20Our%20approach%20employs%20expert%20networks%20tailored%20to%20different%20geometric%20spaces%2C%20dynamically%20computing%20fusion%20weights%20to%20adaptively%20integrate%20geometric%20properties%20based%20on%20local%20structural%20characteristics.%20This%20adaptive%20fusion%20preserves%20structural%20integrity%20before%20alignment%20with%20text%20embeddings.%20Extensive%20experiments%20demonstrate%20that%20GraphShaper%20achieves%209.47%5C%25%20accuracy%20improvements%20on%20citation%20networks%20and%207.63%5C%25%20on%20social%20networks%20in%20zero-shot%20settings.&entry.1838667208=http%3A//arxiv.org/abs/2510.12085v2&entry.124074799=Read"},
{"title": "DSTED: Decoupling Temporal Stabilization and Discriminative Enhancement for Surgical Workflow Recognition", "author": "Yueyao Chen and Kai-Ni Wang and Dario Tayupo and Arnaud Huaulm'e and Krystel Nyangoh Timoh and Pierre Jannin and Qi Dou", "abstract": "Purpose: Surgical workflow recognition enables context-aware assistance and skill assessment in computer-assisted interventions. Despite recent advances, current methods suffer from two critical challenges: prediction jitter across consecutive frames and poor discrimination of ambiguous phases. This paper aims to develop a stable framework by selectively propagating reliable historical information and explicitly modeling uncertainty for hard sample enhancement.\n  Methods: We propose a dual-pathway framework DSTED with Reliable Memory Propagation (RMP) and Uncertainty-Aware Prototype Retrieval (UPR). RMP maintains temporal coherence by filtering and fusing high-confidence historical features through multi-criteria reliability assessment. UPR constructs learnable class-specific prototypes from high-uncertainty samples and performs adaptive prototype matching to refine ambiguous frame representations. Finally, a confidence-driven gate dynamically balances both pathways based on prediction certainty.\n  Results: Our method achieves state-of-the-art performance on AutoLaparo-hysterectomy with 84.36% accuracy and 65.51% F1-score, surpassing the second-best method by 3.51% and 4.88% respectively. Ablations reveal complementary gains from RMP (2.19%) and UPR (1.93%), with synergistic effects when combined. Extensive analysis confirms substantial reduction in temporal jitter and marked improvement on challenging phase transitions.\n  Conclusion: Our dual-pathway design introduces a novel paradigm for stable workflow recognition, demonstrating that decoupling the modeling of temporal consistency and phase ambiguity yields superior performance and clinical applicability.", "link": "http://arxiv.org/abs/2512.19387v1", "date": "2025-12-22", "relevancy": 2.1673, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5691}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.559}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DSTED%3A%20Decoupling%20Temporal%20Stabilization%20and%20Discriminative%20Enhancement%20for%20Surgical%20Workflow%20Recognition&body=Title%3A%20DSTED%3A%20Decoupling%20Temporal%20Stabilization%20and%20Discriminative%20Enhancement%20for%20Surgical%20Workflow%20Recognition%0AAuthor%3A%20Yueyao%20Chen%20and%20Kai-Ni%20Wang%20and%20Dario%20Tayupo%20and%20Arnaud%20Huaulm%27e%20and%20Krystel%20Nyangoh%20Timoh%20and%20Pierre%20Jannin%20and%20Qi%20Dou%0AAbstract%3A%20Purpose%3A%20Surgical%20workflow%20recognition%20enables%20context-aware%20assistance%20and%20skill%20assessment%20in%20computer-assisted%20interventions.%20Despite%20recent%20advances%2C%20current%20methods%20suffer%20from%20two%20critical%20challenges%3A%20prediction%20jitter%20across%20consecutive%20frames%20and%20poor%20discrimination%20of%20ambiguous%20phases.%20This%20paper%20aims%20to%20develop%20a%20stable%20framework%20by%20selectively%20propagating%20reliable%20historical%20information%20and%20explicitly%20modeling%20uncertainty%20for%20hard%20sample%20enhancement.%0A%20%20Methods%3A%20We%20propose%20a%20dual-pathway%20framework%20DSTED%20with%20Reliable%20Memory%20Propagation%20%28RMP%29%20and%20Uncertainty-Aware%20Prototype%20Retrieval%20%28UPR%29.%20RMP%20maintains%20temporal%20coherence%20by%20filtering%20and%20fusing%20high-confidence%20historical%20features%20through%20multi-criteria%20reliability%20assessment.%20UPR%20constructs%20learnable%20class-specific%20prototypes%20from%20high-uncertainty%20samples%20and%20performs%20adaptive%20prototype%20matching%20to%20refine%20ambiguous%20frame%20representations.%20Finally%2C%20a%20confidence-driven%20gate%20dynamically%20balances%20both%20pathways%20based%20on%20prediction%20certainty.%0A%20%20Results%3A%20Our%20method%20achieves%20state-of-the-art%20performance%20on%20AutoLaparo-hysterectomy%20with%2084.36%25%20accuracy%20and%2065.51%25%20F1-score%2C%20surpassing%20the%20second-best%20method%20by%203.51%25%20and%204.88%25%20respectively.%20Ablations%20reveal%20complementary%20gains%20from%20RMP%20%282.19%25%29%20and%20UPR%20%281.93%25%29%2C%20with%20synergistic%20effects%20when%20combined.%20Extensive%20analysis%20confirms%20substantial%20reduction%20in%20temporal%20jitter%20and%20marked%20improvement%20on%20challenging%20phase%20transitions.%0A%20%20Conclusion%3A%20Our%20dual-pathway%20design%20introduces%20a%20novel%20paradigm%20for%20stable%20workflow%20recognition%2C%20demonstrating%20that%20decoupling%20the%20modeling%20of%20temporal%20consistency%20and%20phase%20ambiguity%20yields%20superior%20performance%20and%20clinical%20applicability.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19387v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDSTED%253A%2520Decoupling%2520Temporal%2520Stabilization%2520and%2520Discriminative%2520Enhancement%2520for%2520Surgical%2520Workflow%2520Recognition%26entry.906535625%3DYueyao%2520Chen%2520and%2520Kai-Ni%2520Wang%2520and%2520Dario%2520Tayupo%2520and%2520Arnaud%2520Huaulm%2527e%2520and%2520Krystel%2520Nyangoh%2520Timoh%2520and%2520Pierre%2520Jannin%2520and%2520Qi%2520Dou%26entry.1292438233%3DPurpose%253A%2520Surgical%2520workflow%2520recognition%2520enables%2520context-aware%2520assistance%2520and%2520skill%2520assessment%2520in%2520computer-assisted%2520interventions.%2520Despite%2520recent%2520advances%252C%2520current%2520methods%2520suffer%2520from%2520two%2520critical%2520challenges%253A%2520prediction%2520jitter%2520across%2520consecutive%2520frames%2520and%2520poor%2520discrimination%2520of%2520ambiguous%2520phases.%2520This%2520paper%2520aims%2520to%2520develop%2520a%2520stable%2520framework%2520by%2520selectively%2520propagating%2520reliable%2520historical%2520information%2520and%2520explicitly%2520modeling%2520uncertainty%2520for%2520hard%2520sample%2520enhancement.%250A%2520%2520Methods%253A%2520We%2520propose%2520a%2520dual-pathway%2520framework%2520DSTED%2520with%2520Reliable%2520Memory%2520Propagation%2520%2528RMP%2529%2520and%2520Uncertainty-Aware%2520Prototype%2520Retrieval%2520%2528UPR%2529.%2520RMP%2520maintains%2520temporal%2520coherence%2520by%2520filtering%2520and%2520fusing%2520high-confidence%2520historical%2520features%2520through%2520multi-criteria%2520reliability%2520assessment.%2520UPR%2520constructs%2520learnable%2520class-specific%2520prototypes%2520from%2520high-uncertainty%2520samples%2520and%2520performs%2520adaptive%2520prototype%2520matching%2520to%2520refine%2520ambiguous%2520frame%2520representations.%2520Finally%252C%2520a%2520confidence-driven%2520gate%2520dynamically%2520balances%2520both%2520pathways%2520based%2520on%2520prediction%2520certainty.%250A%2520%2520Results%253A%2520Our%2520method%2520achieves%2520state-of-the-art%2520performance%2520on%2520AutoLaparo-hysterectomy%2520with%252084.36%2525%2520accuracy%2520and%252065.51%2525%2520F1-score%252C%2520surpassing%2520the%2520second-best%2520method%2520by%25203.51%2525%2520and%25204.88%2525%2520respectively.%2520Ablations%2520reveal%2520complementary%2520gains%2520from%2520RMP%2520%25282.19%2525%2529%2520and%2520UPR%2520%25281.93%2525%2529%252C%2520with%2520synergistic%2520effects%2520when%2520combined.%2520Extensive%2520analysis%2520confirms%2520substantial%2520reduction%2520in%2520temporal%2520jitter%2520and%2520marked%2520improvement%2520on%2520challenging%2520phase%2520transitions.%250A%2520%2520Conclusion%253A%2520Our%2520dual-pathway%2520design%2520introduces%2520a%2520novel%2520paradigm%2520for%2520stable%2520workflow%2520recognition%252C%2520demonstrating%2520that%2520decoupling%2520the%2520modeling%2520of%2520temporal%2520consistency%2520and%2520phase%2520ambiguity%2520yields%2520superior%2520performance%2520and%2520clinical%2520applicability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19387v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DSTED%3A%20Decoupling%20Temporal%20Stabilization%20and%20Discriminative%20Enhancement%20for%20Surgical%20Workflow%20Recognition&entry.906535625=Yueyao%20Chen%20and%20Kai-Ni%20Wang%20and%20Dario%20Tayupo%20and%20Arnaud%20Huaulm%27e%20and%20Krystel%20Nyangoh%20Timoh%20and%20Pierre%20Jannin%20and%20Qi%20Dou&entry.1292438233=Purpose%3A%20Surgical%20workflow%20recognition%20enables%20context-aware%20assistance%20and%20skill%20assessment%20in%20computer-assisted%20interventions.%20Despite%20recent%20advances%2C%20current%20methods%20suffer%20from%20two%20critical%20challenges%3A%20prediction%20jitter%20across%20consecutive%20frames%20and%20poor%20discrimination%20of%20ambiguous%20phases.%20This%20paper%20aims%20to%20develop%20a%20stable%20framework%20by%20selectively%20propagating%20reliable%20historical%20information%20and%20explicitly%20modeling%20uncertainty%20for%20hard%20sample%20enhancement.%0A%20%20Methods%3A%20We%20propose%20a%20dual-pathway%20framework%20DSTED%20with%20Reliable%20Memory%20Propagation%20%28RMP%29%20and%20Uncertainty-Aware%20Prototype%20Retrieval%20%28UPR%29.%20RMP%20maintains%20temporal%20coherence%20by%20filtering%20and%20fusing%20high-confidence%20historical%20features%20through%20multi-criteria%20reliability%20assessment.%20UPR%20constructs%20learnable%20class-specific%20prototypes%20from%20high-uncertainty%20samples%20and%20performs%20adaptive%20prototype%20matching%20to%20refine%20ambiguous%20frame%20representations.%20Finally%2C%20a%20confidence-driven%20gate%20dynamically%20balances%20both%20pathways%20based%20on%20prediction%20certainty.%0A%20%20Results%3A%20Our%20method%20achieves%20state-of-the-art%20performance%20on%20AutoLaparo-hysterectomy%20with%2084.36%25%20accuracy%20and%2065.51%25%20F1-score%2C%20surpassing%20the%20second-best%20method%20by%203.51%25%20and%204.88%25%20respectively.%20Ablations%20reveal%20complementary%20gains%20from%20RMP%20%282.19%25%29%20and%20UPR%20%281.93%25%29%2C%20with%20synergistic%20effects%20when%20combined.%20Extensive%20analysis%20confirms%20substantial%20reduction%20in%20temporal%20jitter%20and%20marked%20improvement%20on%20challenging%20phase%20transitions.%0A%20%20Conclusion%3A%20Our%20dual-pathway%20design%20introduces%20a%20novel%20paradigm%20for%20stable%20workflow%20recognition%2C%20demonstrating%20that%20decoupling%20the%20modeling%20of%20temporal%20consistency%20and%20phase%20ambiguity%20yields%20superior%20performance%20and%20clinical%20applicability.&entry.1838667208=http%3A//arxiv.org/abs/2512.19387v1&entry.124074799=Read"},
{"title": "GANeXt: A Fully ConvNeXt-Enhanced Generative Adversarial Network for MRI- and CBCT-to-CT Synthesis", "author": "Siyuan Mei and Yan Xia and Fuxin Fan", "abstract": "The synthesis of computed tomography (CT) from magnetic resonance imaging (MRI) and cone-beam CT (CBCT) plays a critical role in clinical treatment planning by enabling accurate anatomical representation in adaptive radiotherapy. In this work, we propose GANeXt, a 3D patch-based, fully ConvNeXt-powered generative adversarial network for unified CT synthesis across different modalities and anatomical regions. Specifically, GANeXt employs an efficient U-shaped generator constructed from stacked 3D ConvNeXt blocks with compact convolution kernels, while the discriminator adopts a conditional PatchGAN. To improve synthesis quality, we incorporate a combination of loss functions, including mean absolute error (MAE), perceptual loss, segmentation-based masked MAE, and adversarial loss and a combination of Dice loss and cross-entropy for multi-head segmentation discriminator. For both tasks, training is performed with a batch size of 8 using two separate AdamW optimizers for the generator and discriminator, each equipped with a warmup and cosine decay scheduler, with learning rates of $5\\times10^{-4}$ and $1\\times10^{-3}$, respectively. Data preprocessing includes deformable registration, foreground cropping, percentile normalization for the input modality, and linear normalization of the CT to the range $[-1024, 1000]$. Data augmentation involves random zooming within $(0.8, 1.3)$ (for MRI-to-CT only), fixed-size cropping to $32\\times160\\times192$ for MRI-to-CT and $32\\times128\\times128$ for CBCT-to-CT, and random flipping. During inference, we apply a sliding-window approach with $0.8$ overlap and average folding to reconstruct the full-size sCT, followed by inversion of the CT normalization. After joint training on all regions without any fine-tuning, the final models are selected at the end of 3000 epochs for MRI-to-CT and 1000 epochs for CBCT-to-CT using the full training dataset.", "link": "http://arxiv.org/abs/2512.19336v1", "date": "2025-12-22", "relevancy": 2.1673, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5463}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5426}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5393}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GANeXt%3A%20A%20Fully%20ConvNeXt-Enhanced%20Generative%20Adversarial%20Network%20for%20MRI-%20and%20CBCT-to-CT%20Synthesis&body=Title%3A%20GANeXt%3A%20A%20Fully%20ConvNeXt-Enhanced%20Generative%20Adversarial%20Network%20for%20MRI-%20and%20CBCT-to-CT%20Synthesis%0AAuthor%3A%20Siyuan%20Mei%20and%20Yan%20Xia%20and%20Fuxin%20Fan%0AAbstract%3A%20The%20synthesis%20of%20computed%20tomography%20%28CT%29%20from%20magnetic%20resonance%20imaging%20%28MRI%29%20and%20cone-beam%20CT%20%28CBCT%29%20plays%20a%20critical%20role%20in%20clinical%20treatment%20planning%20by%20enabling%20accurate%20anatomical%20representation%20in%20adaptive%20radiotherapy.%20In%20this%20work%2C%20we%20propose%20GANeXt%2C%20a%203D%20patch-based%2C%20fully%20ConvNeXt-powered%20generative%20adversarial%20network%20for%20unified%20CT%20synthesis%20across%20different%20modalities%20and%20anatomical%20regions.%20Specifically%2C%20GANeXt%20employs%20an%20efficient%20U-shaped%20generator%20constructed%20from%20stacked%203D%20ConvNeXt%20blocks%20with%20compact%20convolution%20kernels%2C%20while%20the%20discriminator%20adopts%20a%20conditional%20PatchGAN.%20To%20improve%20synthesis%20quality%2C%20we%20incorporate%20a%20combination%20of%20loss%20functions%2C%20including%20mean%20absolute%20error%20%28MAE%29%2C%20perceptual%20loss%2C%20segmentation-based%20masked%20MAE%2C%20and%20adversarial%20loss%20and%20a%20combination%20of%20Dice%20loss%20and%20cross-entropy%20for%20multi-head%20segmentation%20discriminator.%20For%20both%20tasks%2C%20training%20is%20performed%20with%20a%20batch%20size%20of%208%20using%20two%20separate%20AdamW%20optimizers%20for%20the%20generator%20and%20discriminator%2C%20each%20equipped%20with%20a%20warmup%20and%20cosine%20decay%20scheduler%2C%20with%20learning%20rates%20of%20%245%5Ctimes10%5E%7B-4%7D%24%20and%20%241%5Ctimes10%5E%7B-3%7D%24%2C%20respectively.%20Data%20preprocessing%20includes%20deformable%20registration%2C%20foreground%20cropping%2C%20percentile%20normalization%20for%20the%20input%20modality%2C%20and%20linear%20normalization%20of%20the%20CT%20to%20the%20range%20%24%5B-1024%2C%201000%5D%24.%20Data%20augmentation%20involves%20random%20zooming%20within%20%24%280.8%2C%201.3%29%24%20%28for%20MRI-to-CT%20only%29%2C%20fixed-size%20cropping%20to%20%2432%5Ctimes160%5Ctimes192%24%20for%20MRI-to-CT%20and%20%2432%5Ctimes128%5Ctimes128%24%20for%20CBCT-to-CT%2C%20and%20random%20flipping.%20During%20inference%2C%20we%20apply%20a%20sliding-window%20approach%20with%20%240.8%24%20overlap%20and%20average%20folding%20to%20reconstruct%20the%20full-size%20sCT%2C%20followed%20by%20inversion%20of%20the%20CT%20normalization.%20After%20joint%20training%20on%20all%20regions%20without%20any%20fine-tuning%2C%20the%20final%20models%20are%20selected%20at%20the%20end%20of%203000%20epochs%20for%20MRI-to-CT%20and%201000%20epochs%20for%20CBCT-to-CT%20using%20the%20full%20training%20dataset.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19336v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGANeXt%253A%2520A%2520Fully%2520ConvNeXt-Enhanced%2520Generative%2520Adversarial%2520Network%2520for%2520MRI-%2520and%2520CBCT-to-CT%2520Synthesis%26entry.906535625%3DSiyuan%2520Mei%2520and%2520Yan%2520Xia%2520and%2520Fuxin%2520Fan%26entry.1292438233%3DThe%2520synthesis%2520of%2520computed%2520tomography%2520%2528CT%2529%2520from%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520and%2520cone-beam%2520CT%2520%2528CBCT%2529%2520plays%2520a%2520critical%2520role%2520in%2520clinical%2520treatment%2520planning%2520by%2520enabling%2520accurate%2520anatomical%2520representation%2520in%2520adaptive%2520radiotherapy.%2520In%2520this%2520work%252C%2520we%2520propose%2520GANeXt%252C%2520a%25203D%2520patch-based%252C%2520fully%2520ConvNeXt-powered%2520generative%2520adversarial%2520network%2520for%2520unified%2520CT%2520synthesis%2520across%2520different%2520modalities%2520and%2520anatomical%2520regions.%2520Specifically%252C%2520GANeXt%2520employs%2520an%2520efficient%2520U-shaped%2520generator%2520constructed%2520from%2520stacked%25203D%2520ConvNeXt%2520blocks%2520with%2520compact%2520convolution%2520kernels%252C%2520while%2520the%2520discriminator%2520adopts%2520a%2520conditional%2520PatchGAN.%2520To%2520improve%2520synthesis%2520quality%252C%2520we%2520incorporate%2520a%2520combination%2520of%2520loss%2520functions%252C%2520including%2520mean%2520absolute%2520error%2520%2528MAE%2529%252C%2520perceptual%2520loss%252C%2520segmentation-based%2520masked%2520MAE%252C%2520and%2520adversarial%2520loss%2520and%2520a%2520combination%2520of%2520Dice%2520loss%2520and%2520cross-entropy%2520for%2520multi-head%2520segmentation%2520discriminator.%2520For%2520both%2520tasks%252C%2520training%2520is%2520performed%2520with%2520a%2520batch%2520size%2520of%25208%2520using%2520two%2520separate%2520AdamW%2520optimizers%2520for%2520the%2520generator%2520and%2520discriminator%252C%2520each%2520equipped%2520with%2520a%2520warmup%2520and%2520cosine%2520decay%2520scheduler%252C%2520with%2520learning%2520rates%2520of%2520%25245%255Ctimes10%255E%257B-4%257D%2524%2520and%2520%25241%255Ctimes10%255E%257B-3%257D%2524%252C%2520respectively.%2520Data%2520preprocessing%2520includes%2520deformable%2520registration%252C%2520foreground%2520cropping%252C%2520percentile%2520normalization%2520for%2520the%2520input%2520modality%252C%2520and%2520linear%2520normalization%2520of%2520the%2520CT%2520to%2520the%2520range%2520%2524%255B-1024%252C%25201000%255D%2524.%2520Data%2520augmentation%2520involves%2520random%2520zooming%2520within%2520%2524%25280.8%252C%25201.3%2529%2524%2520%2528for%2520MRI-to-CT%2520only%2529%252C%2520fixed-size%2520cropping%2520to%2520%252432%255Ctimes160%255Ctimes192%2524%2520for%2520MRI-to-CT%2520and%2520%252432%255Ctimes128%255Ctimes128%2524%2520for%2520CBCT-to-CT%252C%2520and%2520random%2520flipping.%2520During%2520inference%252C%2520we%2520apply%2520a%2520sliding-window%2520approach%2520with%2520%25240.8%2524%2520overlap%2520and%2520average%2520folding%2520to%2520reconstruct%2520the%2520full-size%2520sCT%252C%2520followed%2520by%2520inversion%2520of%2520the%2520CT%2520normalization.%2520After%2520joint%2520training%2520on%2520all%2520regions%2520without%2520any%2520fine-tuning%252C%2520the%2520final%2520models%2520are%2520selected%2520at%2520the%2520end%2520of%25203000%2520epochs%2520for%2520MRI-to-CT%2520and%25201000%2520epochs%2520for%2520CBCT-to-CT%2520using%2520the%2520full%2520training%2520dataset.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19336v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GANeXt%3A%20A%20Fully%20ConvNeXt-Enhanced%20Generative%20Adversarial%20Network%20for%20MRI-%20and%20CBCT-to-CT%20Synthesis&entry.906535625=Siyuan%20Mei%20and%20Yan%20Xia%20and%20Fuxin%20Fan&entry.1292438233=The%20synthesis%20of%20computed%20tomography%20%28CT%29%20from%20magnetic%20resonance%20imaging%20%28MRI%29%20and%20cone-beam%20CT%20%28CBCT%29%20plays%20a%20critical%20role%20in%20clinical%20treatment%20planning%20by%20enabling%20accurate%20anatomical%20representation%20in%20adaptive%20radiotherapy.%20In%20this%20work%2C%20we%20propose%20GANeXt%2C%20a%203D%20patch-based%2C%20fully%20ConvNeXt-powered%20generative%20adversarial%20network%20for%20unified%20CT%20synthesis%20across%20different%20modalities%20and%20anatomical%20regions.%20Specifically%2C%20GANeXt%20employs%20an%20efficient%20U-shaped%20generator%20constructed%20from%20stacked%203D%20ConvNeXt%20blocks%20with%20compact%20convolution%20kernels%2C%20while%20the%20discriminator%20adopts%20a%20conditional%20PatchGAN.%20To%20improve%20synthesis%20quality%2C%20we%20incorporate%20a%20combination%20of%20loss%20functions%2C%20including%20mean%20absolute%20error%20%28MAE%29%2C%20perceptual%20loss%2C%20segmentation-based%20masked%20MAE%2C%20and%20adversarial%20loss%20and%20a%20combination%20of%20Dice%20loss%20and%20cross-entropy%20for%20multi-head%20segmentation%20discriminator.%20For%20both%20tasks%2C%20training%20is%20performed%20with%20a%20batch%20size%20of%208%20using%20two%20separate%20AdamW%20optimizers%20for%20the%20generator%20and%20discriminator%2C%20each%20equipped%20with%20a%20warmup%20and%20cosine%20decay%20scheduler%2C%20with%20learning%20rates%20of%20%245%5Ctimes10%5E%7B-4%7D%24%20and%20%241%5Ctimes10%5E%7B-3%7D%24%2C%20respectively.%20Data%20preprocessing%20includes%20deformable%20registration%2C%20foreground%20cropping%2C%20percentile%20normalization%20for%20the%20input%20modality%2C%20and%20linear%20normalization%20of%20the%20CT%20to%20the%20range%20%24%5B-1024%2C%201000%5D%24.%20Data%20augmentation%20involves%20random%20zooming%20within%20%24%280.8%2C%201.3%29%24%20%28for%20MRI-to-CT%20only%29%2C%20fixed-size%20cropping%20to%20%2432%5Ctimes160%5Ctimes192%24%20for%20MRI-to-CT%20and%20%2432%5Ctimes128%5Ctimes128%24%20for%20CBCT-to-CT%2C%20and%20random%20flipping.%20During%20inference%2C%20we%20apply%20a%20sliding-window%20approach%20with%20%240.8%24%20overlap%20and%20average%20folding%20to%20reconstruct%20the%20full-size%20sCT%2C%20followed%20by%20inversion%20of%20the%20CT%20normalization.%20After%20joint%20training%20on%20all%20regions%20without%20any%20fine-tuning%2C%20the%20final%20models%20are%20selected%20at%20the%20end%20of%203000%20epochs%20for%20MRI-to-CT%20and%201000%20epochs%20for%20CBCT-to-CT%20using%20the%20full%20training%20dataset.&entry.1838667208=http%3A//arxiv.org/abs/2512.19336v1&entry.124074799=Read"},
{"title": "Xiaomi MiMo-VL-Miloco Technical Report", "author": "Jiaze Li and Jingyang Chen and Yuxun Qu and Shijie Xu and Zhenru Lin and Junyou Zhu and Boshen Xu and Wenhui Tan and Pei Fu and Jianzhong Ju and Zhenbo Luo and Jian Luan", "abstract": "We open-source MiMo-VL-Miloco-7B and its quantized variant MiMo-VL-Miloco-7B-GGUF, a pair of home-centric vision-language models that achieve strong performance on both home-scenario understanding and general multimodal reasoning. Built on the MiMo-VL-7B backbone, MiMo-VL-Miloco-7B is specialized for smart-home environments, attaining leading F1 scores on gesture recognition and common home-scenario understanding, while also delivering consistent gains across video benchmarks such as Video-MME, Video-MMMU, and Charades-STA, as well as language understanding benchmarks including MMMU-Pro and MMLU-Pro. In our experiments, MiMo-VL-Miloco-7B outperforms strong closed-source and open-source baselines on home-scenario understanding and several multimodal reasoning benchmarks. To balance specialization and generality, we design a two-stage training pipeline that combines supervised fine-tuning with reinforcement learning based on Group Relative Policy Optimization, leveraging efficient multi-domain data. We further incorporate chain-of-thought supervision and token-budget-aware reasoning, enabling the model to learn knowledge in a data-efficient manner while also performing reasoning efficiently. Our analysis shows that targeted home-scenario training not only enhances activity and gesture understanding, but also improves text-only reasoning with only modest trade-offs on document-centric tasks. Model checkpoints, quantized GGUF weights, and our home-scenario evaluation toolkit are publicly available at https://github.com/XiaoMi/xiaomi-mimo-vl-miloco to support research and deployment in real-world smart-home applications.", "link": "http://arxiv.org/abs/2512.17436v2", "date": "2025-12-22", "relevancy": 2.1545, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.565}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5334}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Xiaomi%20MiMo-VL-Miloco%20Technical%20Report&body=Title%3A%20Xiaomi%20MiMo-VL-Miloco%20Technical%20Report%0AAuthor%3A%20Jiaze%20Li%20and%20Jingyang%20Chen%20and%20Yuxun%20Qu%20and%20Shijie%20Xu%20and%20Zhenru%20Lin%20and%20Junyou%20Zhu%20and%20Boshen%20Xu%20and%20Wenhui%20Tan%20and%20Pei%20Fu%20and%20Jianzhong%20Ju%20and%20Zhenbo%20Luo%20and%20Jian%20Luan%0AAbstract%3A%20We%20open-source%20MiMo-VL-Miloco-7B%20and%20its%20quantized%20variant%20MiMo-VL-Miloco-7B-GGUF%2C%20a%20pair%20of%20home-centric%20vision-language%20models%20that%20achieve%20strong%20performance%20on%20both%20home-scenario%20understanding%20and%20general%20multimodal%20reasoning.%20Built%20on%20the%20MiMo-VL-7B%20backbone%2C%20MiMo-VL-Miloco-7B%20is%20specialized%20for%20smart-home%20environments%2C%20attaining%20leading%20F1%20scores%20on%20gesture%20recognition%20and%20common%20home-scenario%20understanding%2C%20while%20also%20delivering%20consistent%20gains%20across%20video%20benchmarks%20such%20as%20Video-MME%2C%20Video-MMMU%2C%20and%20Charades-STA%2C%20as%20well%20as%20language%20understanding%20benchmarks%20including%20MMMU-Pro%20and%20MMLU-Pro.%20In%20our%20experiments%2C%20MiMo-VL-Miloco-7B%20outperforms%20strong%20closed-source%20and%20open-source%20baselines%20on%20home-scenario%20understanding%20and%20several%20multimodal%20reasoning%20benchmarks.%20To%20balance%20specialization%20and%20generality%2C%20we%20design%20a%20two-stage%20training%20pipeline%20that%20combines%20supervised%20fine-tuning%20with%20reinforcement%20learning%20based%20on%20Group%20Relative%20Policy%20Optimization%2C%20leveraging%20efficient%20multi-domain%20data.%20We%20further%20incorporate%20chain-of-thought%20supervision%20and%20token-budget-aware%20reasoning%2C%20enabling%20the%20model%20to%20learn%20knowledge%20in%20a%20data-efficient%20manner%20while%20also%20performing%20reasoning%20efficiently.%20Our%20analysis%20shows%20that%20targeted%20home-scenario%20training%20not%20only%20enhances%20activity%20and%20gesture%20understanding%2C%20but%20also%20improves%20text-only%20reasoning%20with%20only%20modest%20trade-offs%20on%20document-centric%20tasks.%20Model%20checkpoints%2C%20quantized%20GGUF%20weights%2C%20and%20our%20home-scenario%20evaluation%20toolkit%20are%20publicly%20available%20at%20https%3A//github.com/XiaoMi/xiaomi-mimo-vl-miloco%20to%20support%20research%20and%20deployment%20in%20real-world%20smart-home%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17436v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DXiaomi%2520MiMo-VL-Miloco%2520Technical%2520Report%26entry.906535625%3DJiaze%2520Li%2520and%2520Jingyang%2520Chen%2520and%2520Yuxun%2520Qu%2520and%2520Shijie%2520Xu%2520and%2520Zhenru%2520Lin%2520and%2520Junyou%2520Zhu%2520and%2520Boshen%2520Xu%2520and%2520Wenhui%2520Tan%2520and%2520Pei%2520Fu%2520and%2520Jianzhong%2520Ju%2520and%2520Zhenbo%2520Luo%2520and%2520Jian%2520Luan%26entry.1292438233%3DWe%2520open-source%2520MiMo-VL-Miloco-7B%2520and%2520its%2520quantized%2520variant%2520MiMo-VL-Miloco-7B-GGUF%252C%2520a%2520pair%2520of%2520home-centric%2520vision-language%2520models%2520that%2520achieve%2520strong%2520performance%2520on%2520both%2520home-scenario%2520understanding%2520and%2520general%2520multimodal%2520reasoning.%2520Built%2520on%2520the%2520MiMo-VL-7B%2520backbone%252C%2520MiMo-VL-Miloco-7B%2520is%2520specialized%2520for%2520smart-home%2520environments%252C%2520attaining%2520leading%2520F1%2520scores%2520on%2520gesture%2520recognition%2520and%2520common%2520home-scenario%2520understanding%252C%2520while%2520also%2520delivering%2520consistent%2520gains%2520across%2520video%2520benchmarks%2520such%2520as%2520Video-MME%252C%2520Video-MMMU%252C%2520and%2520Charades-STA%252C%2520as%2520well%2520as%2520language%2520understanding%2520benchmarks%2520including%2520MMMU-Pro%2520and%2520MMLU-Pro.%2520In%2520our%2520experiments%252C%2520MiMo-VL-Miloco-7B%2520outperforms%2520strong%2520closed-source%2520and%2520open-source%2520baselines%2520on%2520home-scenario%2520understanding%2520and%2520several%2520multimodal%2520reasoning%2520benchmarks.%2520To%2520balance%2520specialization%2520and%2520generality%252C%2520we%2520design%2520a%2520two-stage%2520training%2520pipeline%2520that%2520combines%2520supervised%2520fine-tuning%2520with%2520reinforcement%2520learning%2520based%2520on%2520Group%2520Relative%2520Policy%2520Optimization%252C%2520leveraging%2520efficient%2520multi-domain%2520data.%2520We%2520further%2520incorporate%2520chain-of-thought%2520supervision%2520and%2520token-budget-aware%2520reasoning%252C%2520enabling%2520the%2520model%2520to%2520learn%2520knowledge%2520in%2520a%2520data-efficient%2520manner%2520while%2520also%2520performing%2520reasoning%2520efficiently.%2520Our%2520analysis%2520shows%2520that%2520targeted%2520home-scenario%2520training%2520not%2520only%2520enhances%2520activity%2520and%2520gesture%2520understanding%252C%2520but%2520also%2520improves%2520text-only%2520reasoning%2520with%2520only%2520modest%2520trade-offs%2520on%2520document-centric%2520tasks.%2520Model%2520checkpoints%252C%2520quantized%2520GGUF%2520weights%252C%2520and%2520our%2520home-scenario%2520evaluation%2520toolkit%2520are%2520publicly%2520available%2520at%2520https%253A//github.com/XiaoMi/xiaomi-mimo-vl-miloco%2520to%2520support%2520research%2520and%2520deployment%2520in%2520real-world%2520smart-home%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17436v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Xiaomi%20MiMo-VL-Miloco%20Technical%20Report&entry.906535625=Jiaze%20Li%20and%20Jingyang%20Chen%20and%20Yuxun%20Qu%20and%20Shijie%20Xu%20and%20Zhenru%20Lin%20and%20Junyou%20Zhu%20and%20Boshen%20Xu%20and%20Wenhui%20Tan%20and%20Pei%20Fu%20and%20Jianzhong%20Ju%20and%20Zhenbo%20Luo%20and%20Jian%20Luan&entry.1292438233=We%20open-source%20MiMo-VL-Miloco-7B%20and%20its%20quantized%20variant%20MiMo-VL-Miloco-7B-GGUF%2C%20a%20pair%20of%20home-centric%20vision-language%20models%20that%20achieve%20strong%20performance%20on%20both%20home-scenario%20understanding%20and%20general%20multimodal%20reasoning.%20Built%20on%20the%20MiMo-VL-7B%20backbone%2C%20MiMo-VL-Miloco-7B%20is%20specialized%20for%20smart-home%20environments%2C%20attaining%20leading%20F1%20scores%20on%20gesture%20recognition%20and%20common%20home-scenario%20understanding%2C%20while%20also%20delivering%20consistent%20gains%20across%20video%20benchmarks%20such%20as%20Video-MME%2C%20Video-MMMU%2C%20and%20Charades-STA%2C%20as%20well%20as%20language%20understanding%20benchmarks%20including%20MMMU-Pro%20and%20MMLU-Pro.%20In%20our%20experiments%2C%20MiMo-VL-Miloco-7B%20outperforms%20strong%20closed-source%20and%20open-source%20baselines%20on%20home-scenario%20understanding%20and%20several%20multimodal%20reasoning%20benchmarks.%20To%20balance%20specialization%20and%20generality%2C%20we%20design%20a%20two-stage%20training%20pipeline%20that%20combines%20supervised%20fine-tuning%20with%20reinforcement%20learning%20based%20on%20Group%20Relative%20Policy%20Optimization%2C%20leveraging%20efficient%20multi-domain%20data.%20We%20further%20incorporate%20chain-of-thought%20supervision%20and%20token-budget-aware%20reasoning%2C%20enabling%20the%20model%20to%20learn%20knowledge%20in%20a%20data-efficient%20manner%20while%20also%20performing%20reasoning%20efficiently.%20Our%20analysis%20shows%20that%20targeted%20home-scenario%20training%20not%20only%20enhances%20activity%20and%20gesture%20understanding%2C%20but%20also%20improves%20text-only%20reasoning%20with%20only%20modest%20trade-offs%20on%20document-centric%20tasks.%20Model%20checkpoints%2C%20quantized%20GGUF%20weights%2C%20and%20our%20home-scenario%20evaluation%20toolkit%20are%20publicly%20available%20at%20https%3A//github.com/XiaoMi/xiaomi-mimo-vl-miloco%20to%20support%20research%20and%20deployment%20in%20real-world%20smart-home%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2512.17436v2&entry.124074799=Read"},
{"title": "AIDOVECL: AI-generated Dataset of Outpainted Vehicles for Eye-level Classification and Localization", "author": "Amir Kazemi and Qurat ul ain Fatima and Volodymyr Kindratenko and Christopher Tessum", "abstract": "Image labeling is a critical bottleneck in the development of computer vision technologies, often constraining the potential of machine learning models due to the time-intensive nature of manual annotations. This work introduces a novel approach that leverages outpainting to mitigate the problem of annotated data scarcity by generating artificial contexts and annotations, significantly reducing manual labeling efforts. We apply this technique to a particularly acute challenge in autonomous driving, urban planning, and environmental monitoring: the lack of diverse, eye-level vehicle images in desired classes. Our dataset comprises AI-generated vehicle images obtained by detecting and cropping vehicles from manually selected seed images, which are then outpainted onto larger canvases to simulate varied real-world conditions. The outpainted images include detailed annotations, providing high-quality ground truth data. Advanced outpainting techniques and image quality assessments ensure visual fidelity and contextual relevance. Ablation results show that incorporating AIDOVECL improves overall detection performance by up to 10%, and delivers gains of up to 40% in settings with greater diversity of context, object scale, and placement, with underrepresented classes achieving up to 50% higher true positives. AIDOVECL enhances vehicle detection by augmenting real training data and supporting evaluation across diverse scenarios. By demonstrating outpainting as an automatic annotation paradigm, it offers a practical and versatile solution for building fine-grained datasets with reduced labeling effort across multiple machine learning domains. The code and links to datasets used in this study are available for further research and replication at https://github.com/amir-kazemi/aidovecl .", "link": "http://arxiv.org/abs/2410.24116v2", "date": "2025-12-22", "relevancy": 2.1544, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5456}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5402}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5309}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AIDOVECL%3A%20AI-generated%20Dataset%20of%20Outpainted%20Vehicles%20for%20Eye-level%20Classification%20and%20Localization&body=Title%3A%20AIDOVECL%3A%20AI-generated%20Dataset%20of%20Outpainted%20Vehicles%20for%20Eye-level%20Classification%20and%20Localization%0AAuthor%3A%20Amir%20Kazemi%20and%20Qurat%20ul%20ain%20Fatima%20and%20Volodymyr%20Kindratenko%20and%20Christopher%20Tessum%0AAbstract%3A%20Image%20labeling%20is%20a%20critical%20bottleneck%20in%20the%20development%20of%20computer%20vision%20technologies%2C%20often%20constraining%20the%20potential%20of%20machine%20learning%20models%20due%20to%20the%20time-intensive%20nature%20of%20manual%20annotations.%20This%20work%20introduces%20a%20novel%20approach%20that%20leverages%20outpainting%20to%20mitigate%20the%20problem%20of%20annotated%20data%20scarcity%20by%20generating%20artificial%20contexts%20and%20annotations%2C%20significantly%20reducing%20manual%20labeling%20efforts.%20We%20apply%20this%20technique%20to%20a%20particularly%20acute%20challenge%20in%20autonomous%20driving%2C%20urban%20planning%2C%20and%20environmental%20monitoring%3A%20the%20lack%20of%20diverse%2C%20eye-level%20vehicle%20images%20in%20desired%20classes.%20Our%20dataset%20comprises%20AI-generated%20vehicle%20images%20obtained%20by%20detecting%20and%20cropping%20vehicles%20from%20manually%20selected%20seed%20images%2C%20which%20are%20then%20outpainted%20onto%20larger%20canvases%20to%20simulate%20varied%20real-world%20conditions.%20The%20outpainted%20images%20include%20detailed%20annotations%2C%20providing%20high-quality%20ground%20truth%20data.%20Advanced%20outpainting%20techniques%20and%20image%20quality%20assessments%20ensure%20visual%20fidelity%20and%20contextual%20relevance.%20Ablation%20results%20show%20that%20incorporating%20AIDOVECL%20improves%20overall%20detection%20performance%20by%20up%20to%2010%25%2C%20and%20delivers%20gains%20of%20up%20to%2040%25%20in%20settings%20with%20greater%20diversity%20of%20context%2C%20object%20scale%2C%20and%20placement%2C%20with%20underrepresented%20classes%20achieving%20up%20to%2050%25%20higher%20true%20positives.%20AIDOVECL%20enhances%20vehicle%20detection%20by%20augmenting%20real%20training%20data%20and%20supporting%20evaluation%20across%20diverse%20scenarios.%20By%20demonstrating%20outpainting%20as%20an%20automatic%20annotation%20paradigm%2C%20it%20offers%20a%20practical%20and%20versatile%20solution%20for%20building%20fine-grained%20datasets%20with%20reduced%20labeling%20effort%20across%20multiple%20machine%20learning%20domains.%20The%20code%20and%20links%20to%20datasets%20used%20in%20this%20study%20are%20available%20for%20further%20research%20and%20replication%20at%20https%3A//github.com/amir-kazemi/aidovecl%20.%0ALink%3A%20http%3A//arxiv.org/abs/2410.24116v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAIDOVECL%253A%2520AI-generated%2520Dataset%2520of%2520Outpainted%2520Vehicles%2520for%2520Eye-level%2520Classification%2520and%2520Localization%26entry.906535625%3DAmir%2520Kazemi%2520and%2520Qurat%2520ul%2520ain%2520Fatima%2520and%2520Volodymyr%2520Kindratenko%2520and%2520Christopher%2520Tessum%26entry.1292438233%3DImage%2520labeling%2520is%2520a%2520critical%2520bottleneck%2520in%2520the%2520development%2520of%2520computer%2520vision%2520technologies%252C%2520often%2520constraining%2520the%2520potential%2520of%2520machine%2520learning%2520models%2520due%2520to%2520the%2520time-intensive%2520nature%2520of%2520manual%2520annotations.%2520This%2520work%2520introduces%2520a%2520novel%2520approach%2520that%2520leverages%2520outpainting%2520to%2520mitigate%2520the%2520problem%2520of%2520annotated%2520data%2520scarcity%2520by%2520generating%2520artificial%2520contexts%2520and%2520annotations%252C%2520significantly%2520reducing%2520manual%2520labeling%2520efforts.%2520We%2520apply%2520this%2520technique%2520to%2520a%2520particularly%2520acute%2520challenge%2520in%2520autonomous%2520driving%252C%2520urban%2520planning%252C%2520and%2520environmental%2520monitoring%253A%2520the%2520lack%2520of%2520diverse%252C%2520eye-level%2520vehicle%2520images%2520in%2520desired%2520classes.%2520Our%2520dataset%2520comprises%2520AI-generated%2520vehicle%2520images%2520obtained%2520by%2520detecting%2520and%2520cropping%2520vehicles%2520from%2520manually%2520selected%2520seed%2520images%252C%2520which%2520are%2520then%2520outpainted%2520onto%2520larger%2520canvases%2520to%2520simulate%2520varied%2520real-world%2520conditions.%2520The%2520outpainted%2520images%2520include%2520detailed%2520annotations%252C%2520providing%2520high-quality%2520ground%2520truth%2520data.%2520Advanced%2520outpainting%2520techniques%2520and%2520image%2520quality%2520assessments%2520ensure%2520visual%2520fidelity%2520and%2520contextual%2520relevance.%2520Ablation%2520results%2520show%2520that%2520incorporating%2520AIDOVECL%2520improves%2520overall%2520detection%2520performance%2520by%2520up%2520to%252010%2525%252C%2520and%2520delivers%2520gains%2520of%2520up%2520to%252040%2525%2520in%2520settings%2520with%2520greater%2520diversity%2520of%2520context%252C%2520object%2520scale%252C%2520and%2520placement%252C%2520with%2520underrepresented%2520classes%2520achieving%2520up%2520to%252050%2525%2520higher%2520true%2520positives.%2520AIDOVECL%2520enhances%2520vehicle%2520detection%2520by%2520augmenting%2520real%2520training%2520data%2520and%2520supporting%2520evaluation%2520across%2520diverse%2520scenarios.%2520By%2520demonstrating%2520outpainting%2520as%2520an%2520automatic%2520annotation%2520paradigm%252C%2520it%2520offers%2520a%2520practical%2520and%2520versatile%2520solution%2520for%2520building%2520fine-grained%2520datasets%2520with%2520reduced%2520labeling%2520effort%2520across%2520multiple%2520machine%2520learning%2520domains.%2520The%2520code%2520and%2520links%2520to%2520datasets%2520used%2520in%2520this%2520study%2520are%2520available%2520for%2520further%2520research%2520and%2520replication%2520at%2520https%253A//github.com/amir-kazemi/aidovecl%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.24116v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AIDOVECL%3A%20AI-generated%20Dataset%20of%20Outpainted%20Vehicles%20for%20Eye-level%20Classification%20and%20Localization&entry.906535625=Amir%20Kazemi%20and%20Qurat%20ul%20ain%20Fatima%20and%20Volodymyr%20Kindratenko%20and%20Christopher%20Tessum&entry.1292438233=Image%20labeling%20is%20a%20critical%20bottleneck%20in%20the%20development%20of%20computer%20vision%20technologies%2C%20often%20constraining%20the%20potential%20of%20machine%20learning%20models%20due%20to%20the%20time-intensive%20nature%20of%20manual%20annotations.%20This%20work%20introduces%20a%20novel%20approach%20that%20leverages%20outpainting%20to%20mitigate%20the%20problem%20of%20annotated%20data%20scarcity%20by%20generating%20artificial%20contexts%20and%20annotations%2C%20significantly%20reducing%20manual%20labeling%20efforts.%20We%20apply%20this%20technique%20to%20a%20particularly%20acute%20challenge%20in%20autonomous%20driving%2C%20urban%20planning%2C%20and%20environmental%20monitoring%3A%20the%20lack%20of%20diverse%2C%20eye-level%20vehicle%20images%20in%20desired%20classes.%20Our%20dataset%20comprises%20AI-generated%20vehicle%20images%20obtained%20by%20detecting%20and%20cropping%20vehicles%20from%20manually%20selected%20seed%20images%2C%20which%20are%20then%20outpainted%20onto%20larger%20canvases%20to%20simulate%20varied%20real-world%20conditions.%20The%20outpainted%20images%20include%20detailed%20annotations%2C%20providing%20high-quality%20ground%20truth%20data.%20Advanced%20outpainting%20techniques%20and%20image%20quality%20assessments%20ensure%20visual%20fidelity%20and%20contextual%20relevance.%20Ablation%20results%20show%20that%20incorporating%20AIDOVECL%20improves%20overall%20detection%20performance%20by%20up%20to%2010%25%2C%20and%20delivers%20gains%20of%20up%20to%2040%25%20in%20settings%20with%20greater%20diversity%20of%20context%2C%20object%20scale%2C%20and%20placement%2C%20with%20underrepresented%20classes%20achieving%20up%20to%2050%25%20higher%20true%20positives.%20AIDOVECL%20enhances%20vehicle%20detection%20by%20augmenting%20real%20training%20data%20and%20supporting%20evaluation%20across%20diverse%20scenarios.%20By%20demonstrating%20outpainting%20as%20an%20automatic%20annotation%20paradigm%2C%20it%20offers%20a%20practical%20and%20versatile%20solution%20for%20building%20fine-grained%20datasets%20with%20reduced%20labeling%20effort%20across%20multiple%20machine%20learning%20domains.%20The%20code%20and%20links%20to%20datasets%20used%20in%20this%20study%20are%20available%20for%20further%20research%20and%20replication%20at%20https%3A//github.com/amir-kazemi/aidovecl%20.&entry.1838667208=http%3A//arxiv.org/abs/2410.24116v2&entry.124074799=Read"},
{"title": "Source-Optimal Training is Transfer-Suboptimal", "author": "C. Evans Hedges", "abstract": "We prove that training a source model optimally for its own task is generically suboptimal when the objective is downstream transfer. We study the source-side optimization problem in L2-SP ridge regression and show a fundamental mismatch between the source-optimal and transfer-optimal source regularization: outside of a measure-zero set, $\u03c4_0^* \\neq \u03c4_S^*$. We characterize the transfer-optimal source penalty $\u03c4_0^*$ as a function of task alignment and identify an alignment-dependent reversal: with imperfect alignment ($0<\u03c1<1$), transfer benefits from stronger source regularization, while in super-aligned regimes ($\u03c1>1$), transfer benefits from weaker regularization. In isotropic settings, the decision of whether transfer helps is independent of the target sample size and noise, depending only on task alignment and source characteristics. We verify the linear predictions in a synthetic ridge regression experiment, and we present CIFAR-10 experiments as evidence that the source-optimal versus transfer-optimal mismatch can persist in nonlinear networks.", "link": "http://arxiv.org/abs/2511.08401v3", "date": "2025-12-22", "relevancy": 2.1537, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4341}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4323}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4258}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Source-Optimal%20Training%20is%20Transfer-Suboptimal&body=Title%3A%20Source-Optimal%20Training%20is%20Transfer-Suboptimal%0AAuthor%3A%20C.%20Evans%20Hedges%0AAbstract%3A%20We%20prove%20that%20training%20a%20source%20model%20optimally%20for%20its%20own%20task%20is%20generically%20suboptimal%20when%20the%20objective%20is%20downstream%20transfer.%20We%20study%20the%20source-side%20optimization%20problem%20in%20L2-SP%20ridge%20regression%20and%20show%20a%20fundamental%20mismatch%20between%20the%20source-optimal%20and%20transfer-optimal%20source%20regularization%3A%20outside%20of%20a%20measure-zero%20set%2C%20%24%CF%84_0%5E%2A%20%5Cneq%20%CF%84_S%5E%2A%24.%20We%20characterize%20the%20transfer-optimal%20source%20penalty%20%24%CF%84_0%5E%2A%24%20as%20a%20function%20of%20task%20alignment%20and%20identify%20an%20alignment-dependent%20reversal%3A%20with%20imperfect%20alignment%20%28%240%3C%CF%81%3C1%24%29%2C%20transfer%20benefits%20from%20stronger%20source%20regularization%2C%20while%20in%20super-aligned%20regimes%20%28%24%CF%81%3E1%24%29%2C%20transfer%20benefits%20from%20weaker%20regularization.%20In%20isotropic%20settings%2C%20the%20decision%20of%20whether%20transfer%20helps%20is%20independent%20of%20the%20target%20sample%20size%20and%20noise%2C%20depending%20only%20on%20task%20alignment%20and%20source%20characteristics.%20We%20verify%20the%20linear%20predictions%20in%20a%20synthetic%20ridge%20regression%20experiment%2C%20and%20we%20present%20CIFAR-10%20experiments%20as%20evidence%20that%20the%20source-optimal%20versus%20transfer-optimal%20mismatch%20can%20persist%20in%20nonlinear%20networks.%0ALink%3A%20http%3A//arxiv.org/abs/2511.08401v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSource-Optimal%2520Training%2520is%2520Transfer-Suboptimal%26entry.906535625%3DC.%2520Evans%2520Hedges%26entry.1292438233%3DWe%2520prove%2520that%2520training%2520a%2520source%2520model%2520optimally%2520for%2520its%2520own%2520task%2520is%2520generically%2520suboptimal%2520when%2520the%2520objective%2520is%2520downstream%2520transfer.%2520We%2520study%2520the%2520source-side%2520optimization%2520problem%2520in%2520L2-SP%2520ridge%2520regression%2520and%2520show%2520a%2520fundamental%2520mismatch%2520between%2520the%2520source-optimal%2520and%2520transfer-optimal%2520source%2520regularization%253A%2520outside%2520of%2520a%2520measure-zero%2520set%252C%2520%2524%25CF%2584_0%255E%252A%2520%255Cneq%2520%25CF%2584_S%255E%252A%2524.%2520We%2520characterize%2520the%2520transfer-optimal%2520source%2520penalty%2520%2524%25CF%2584_0%255E%252A%2524%2520as%2520a%2520function%2520of%2520task%2520alignment%2520and%2520identify%2520an%2520alignment-dependent%2520reversal%253A%2520with%2520imperfect%2520alignment%2520%2528%25240%253C%25CF%2581%253C1%2524%2529%252C%2520transfer%2520benefits%2520from%2520stronger%2520source%2520regularization%252C%2520while%2520in%2520super-aligned%2520regimes%2520%2528%2524%25CF%2581%253E1%2524%2529%252C%2520transfer%2520benefits%2520from%2520weaker%2520regularization.%2520In%2520isotropic%2520settings%252C%2520the%2520decision%2520of%2520whether%2520transfer%2520helps%2520is%2520independent%2520of%2520the%2520target%2520sample%2520size%2520and%2520noise%252C%2520depending%2520only%2520on%2520task%2520alignment%2520and%2520source%2520characteristics.%2520We%2520verify%2520the%2520linear%2520predictions%2520in%2520a%2520synthetic%2520ridge%2520regression%2520experiment%252C%2520and%2520we%2520present%2520CIFAR-10%2520experiments%2520as%2520evidence%2520that%2520the%2520source-optimal%2520versus%2520transfer-optimal%2520mismatch%2520can%2520persist%2520in%2520nonlinear%2520networks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.08401v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Source-Optimal%20Training%20is%20Transfer-Suboptimal&entry.906535625=C.%20Evans%20Hedges&entry.1292438233=We%20prove%20that%20training%20a%20source%20model%20optimally%20for%20its%20own%20task%20is%20generically%20suboptimal%20when%20the%20objective%20is%20downstream%20transfer.%20We%20study%20the%20source-side%20optimization%20problem%20in%20L2-SP%20ridge%20regression%20and%20show%20a%20fundamental%20mismatch%20between%20the%20source-optimal%20and%20transfer-optimal%20source%20regularization%3A%20outside%20of%20a%20measure-zero%20set%2C%20%24%CF%84_0%5E%2A%20%5Cneq%20%CF%84_S%5E%2A%24.%20We%20characterize%20the%20transfer-optimal%20source%20penalty%20%24%CF%84_0%5E%2A%24%20as%20a%20function%20of%20task%20alignment%20and%20identify%20an%20alignment-dependent%20reversal%3A%20with%20imperfect%20alignment%20%28%240%3C%CF%81%3C1%24%29%2C%20transfer%20benefits%20from%20stronger%20source%20regularization%2C%20while%20in%20super-aligned%20regimes%20%28%24%CF%81%3E1%24%29%2C%20transfer%20benefits%20from%20weaker%20regularization.%20In%20isotropic%20settings%2C%20the%20decision%20of%20whether%20transfer%20helps%20is%20independent%20of%20the%20target%20sample%20size%20and%20noise%2C%20depending%20only%20on%20task%20alignment%20and%20source%20characteristics.%20We%20verify%20the%20linear%20predictions%20in%20a%20synthetic%20ridge%20regression%20experiment%2C%20and%20we%20present%20CIFAR-10%20experiments%20as%20evidence%20that%20the%20source-optimal%20versus%20transfer-optimal%20mismatch%20can%20persist%20in%20nonlinear%20networks.&entry.1838667208=http%3A//arxiv.org/abs/2511.08401v3&entry.124074799=Read"},
{"title": "SlicerOrbitSurgerySim: An Open-Source Platform for Virtual Registration and Quantitative Comparison of Preformed Orbital Plates", "author": "Chi Zhang and Braedon Gunn and Andrew M. Read-Fuller", "abstract": "Poor adaptation of orbital implants remains a major contributor to postoperative complications and revision surgery. Although preformed orbital plates are widely used to reduce cost and operative time compared with customized implants, surgeons currently lack publicly available tools and standardized metrics to quantitatively compare plate fit across vendors, sizes, and patient anatomy. We developed SlicerOrbitSurgerySim, an open-source extension for the 3D Slicer platform that enables interactive virtual registration, evaluation, and comparison of multiple preformed orbital plates in a patient-specific virtual planning environment. The software generates reproducible quantitative plate-to-orbit distance metrics and visualization tools that support both patient-specific planning and population-level statistical analysis of plate adaptability. By facilitating objective comparison of implant designs and placement strategies, this tool aims to improve preoperative decision-making, reduce intraoperative plate modification, and promote collaborative research and surgical education. Pilot studies, sample datasets, and detailed tutorials are provided to support testing, transparency, and reproducibility.", "link": "http://arxiv.org/abs/2512.19534v1", "date": "2025-12-22", "relevancy": 2.1493, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4333}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4333}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SlicerOrbitSurgerySim%3A%20An%20Open-Source%20Platform%20for%20Virtual%20Registration%20and%20Quantitative%20Comparison%20of%20Preformed%20Orbital%20Plates&body=Title%3A%20SlicerOrbitSurgerySim%3A%20An%20Open-Source%20Platform%20for%20Virtual%20Registration%20and%20Quantitative%20Comparison%20of%20Preformed%20Orbital%20Plates%0AAuthor%3A%20Chi%20Zhang%20and%20Braedon%20Gunn%20and%20Andrew%20M.%20Read-Fuller%0AAbstract%3A%20Poor%20adaptation%20of%20orbital%20implants%20remains%20a%20major%20contributor%20to%20postoperative%20complications%20and%20revision%20surgery.%20Although%20preformed%20orbital%20plates%20are%20widely%20used%20to%20reduce%20cost%20and%20operative%20time%20compared%20with%20customized%20implants%2C%20surgeons%20currently%20lack%20publicly%20available%20tools%20and%20standardized%20metrics%20to%20quantitatively%20compare%20plate%20fit%20across%20vendors%2C%20sizes%2C%20and%20patient%20anatomy.%20We%20developed%20SlicerOrbitSurgerySim%2C%20an%20open-source%20extension%20for%20the%203D%20Slicer%20platform%20that%20enables%20interactive%20virtual%20registration%2C%20evaluation%2C%20and%20comparison%20of%20multiple%20preformed%20orbital%20plates%20in%20a%20patient-specific%20virtual%20planning%20environment.%20The%20software%20generates%20reproducible%20quantitative%20plate-to-orbit%20distance%20metrics%20and%20visualization%20tools%20that%20support%20both%20patient-specific%20planning%20and%20population-level%20statistical%20analysis%20of%20plate%20adaptability.%20By%20facilitating%20objective%20comparison%20of%20implant%20designs%20and%20placement%20strategies%2C%20this%20tool%20aims%20to%20improve%20preoperative%20decision-making%2C%20reduce%20intraoperative%20plate%20modification%2C%20and%20promote%20collaborative%20research%20and%20surgical%20education.%20Pilot%20studies%2C%20sample%20datasets%2C%20and%20detailed%20tutorials%20are%20provided%20to%20support%20testing%2C%20transparency%2C%20and%20reproducibility.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19534v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSlicerOrbitSurgerySim%253A%2520An%2520Open-Source%2520Platform%2520for%2520Virtual%2520Registration%2520and%2520Quantitative%2520Comparison%2520of%2520Preformed%2520Orbital%2520Plates%26entry.906535625%3DChi%2520Zhang%2520and%2520Braedon%2520Gunn%2520and%2520Andrew%2520M.%2520Read-Fuller%26entry.1292438233%3DPoor%2520adaptation%2520of%2520orbital%2520implants%2520remains%2520a%2520major%2520contributor%2520to%2520postoperative%2520complications%2520and%2520revision%2520surgery.%2520Although%2520preformed%2520orbital%2520plates%2520are%2520widely%2520used%2520to%2520reduce%2520cost%2520and%2520operative%2520time%2520compared%2520with%2520customized%2520implants%252C%2520surgeons%2520currently%2520lack%2520publicly%2520available%2520tools%2520and%2520standardized%2520metrics%2520to%2520quantitatively%2520compare%2520plate%2520fit%2520across%2520vendors%252C%2520sizes%252C%2520and%2520patient%2520anatomy.%2520We%2520developed%2520SlicerOrbitSurgerySim%252C%2520an%2520open-source%2520extension%2520for%2520the%25203D%2520Slicer%2520platform%2520that%2520enables%2520interactive%2520virtual%2520registration%252C%2520evaluation%252C%2520and%2520comparison%2520of%2520multiple%2520preformed%2520orbital%2520plates%2520in%2520a%2520patient-specific%2520virtual%2520planning%2520environment.%2520The%2520software%2520generates%2520reproducible%2520quantitative%2520plate-to-orbit%2520distance%2520metrics%2520and%2520visualization%2520tools%2520that%2520support%2520both%2520patient-specific%2520planning%2520and%2520population-level%2520statistical%2520analysis%2520of%2520plate%2520adaptability.%2520By%2520facilitating%2520objective%2520comparison%2520of%2520implant%2520designs%2520and%2520placement%2520strategies%252C%2520this%2520tool%2520aims%2520to%2520improve%2520preoperative%2520decision-making%252C%2520reduce%2520intraoperative%2520plate%2520modification%252C%2520and%2520promote%2520collaborative%2520research%2520and%2520surgical%2520education.%2520Pilot%2520studies%252C%2520sample%2520datasets%252C%2520and%2520detailed%2520tutorials%2520are%2520provided%2520to%2520support%2520testing%252C%2520transparency%252C%2520and%2520reproducibility.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19534v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SlicerOrbitSurgerySim%3A%20An%20Open-Source%20Platform%20for%20Virtual%20Registration%20and%20Quantitative%20Comparison%20of%20Preformed%20Orbital%20Plates&entry.906535625=Chi%20Zhang%20and%20Braedon%20Gunn%20and%20Andrew%20M.%20Read-Fuller&entry.1292438233=Poor%20adaptation%20of%20orbital%20implants%20remains%20a%20major%20contributor%20to%20postoperative%20complications%20and%20revision%20surgery.%20Although%20preformed%20orbital%20plates%20are%20widely%20used%20to%20reduce%20cost%20and%20operative%20time%20compared%20with%20customized%20implants%2C%20surgeons%20currently%20lack%20publicly%20available%20tools%20and%20standardized%20metrics%20to%20quantitatively%20compare%20plate%20fit%20across%20vendors%2C%20sizes%2C%20and%20patient%20anatomy.%20We%20developed%20SlicerOrbitSurgerySim%2C%20an%20open-source%20extension%20for%20the%203D%20Slicer%20platform%20that%20enables%20interactive%20virtual%20registration%2C%20evaluation%2C%20and%20comparison%20of%20multiple%20preformed%20orbital%20plates%20in%20a%20patient-specific%20virtual%20planning%20environment.%20The%20software%20generates%20reproducible%20quantitative%20plate-to-orbit%20distance%20metrics%20and%20visualization%20tools%20that%20support%20both%20patient-specific%20planning%20and%20population-level%20statistical%20analysis%20of%20plate%20adaptability.%20By%20facilitating%20objective%20comparison%20of%20implant%20designs%20and%20placement%20strategies%2C%20this%20tool%20aims%20to%20improve%20preoperative%20decision-making%2C%20reduce%20intraoperative%20plate%20modification%2C%20and%20promote%20collaborative%20research%20and%20surgical%20education.%20Pilot%20studies%2C%20sample%20datasets%2C%20and%20detailed%20tutorials%20are%20provided%20to%20support%20testing%2C%20transparency%2C%20and%20reproducibility.&entry.1838667208=http%3A//arxiv.org/abs/2512.19534v1&entry.124074799=Read"},
{"title": "Vision-Aided Relative State Estimation for Approach and Landing on a Moving Platform with Inertial Measurements", "author": "Tarek Bouazza and Alessandro Melis and Soulaimane Berkane and Robert Mahony and Tarek Hamel", "abstract": "This paper tackles the problem of estimating the relative position, orientation, and velocity between a UAV and a planar platform undergoing arbitrary 3D motion during approach and landing. The estimation relies on measurements from Inertial Measurement Units (IMUs) mounted on both systems, assuming there is a suitable communication channel to exchange data, together with visual information provided by an onboard monocular camera, from which the bearing (line-of-sight direction) to the platform's center and the normal vector of its planar surface are extracted. We propose a cascade observer with a complementary filter on SO(3) to reconstruct the relative attitude, followed by a linear Riccati observer for relative position and velocity estimation. Convergence of both observers is established under persistently exciting conditions, and the cascade is shown to be almost globally asymptotically and locally exponentially stable. We further extend the design to the case where the platform's rotation is restricted to its normal axis and show that its measured linear acceleration can be exploited to recover the remaining unobservable rotation angle. A sufficient condition to ensure local exponential convergence in this setting is provided. The performance of the proposed observers is validated through extensive simulations.", "link": "http://arxiv.org/abs/2512.19245v1", "date": "2025-12-22", "relevancy": 2.1477, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5719}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5329}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vision-Aided%20Relative%20State%20Estimation%20for%20Approach%20and%20Landing%20on%20a%20Moving%20Platform%20with%20Inertial%20Measurements&body=Title%3A%20Vision-Aided%20Relative%20State%20Estimation%20for%20Approach%20and%20Landing%20on%20a%20Moving%20Platform%20with%20Inertial%20Measurements%0AAuthor%3A%20Tarek%20Bouazza%20and%20Alessandro%20Melis%20and%20Soulaimane%20Berkane%20and%20Robert%20Mahony%20and%20Tarek%20Hamel%0AAbstract%3A%20This%20paper%20tackles%20the%20problem%20of%20estimating%20the%20relative%20position%2C%20orientation%2C%20and%20velocity%20between%20a%20UAV%20and%20a%20planar%20platform%20undergoing%20arbitrary%203D%20motion%20during%20approach%20and%20landing.%20The%20estimation%20relies%20on%20measurements%20from%20Inertial%20Measurement%20Units%20%28IMUs%29%20mounted%20on%20both%20systems%2C%20assuming%20there%20is%20a%20suitable%20communication%20channel%20to%20exchange%20data%2C%20together%20with%20visual%20information%20provided%20by%20an%20onboard%20monocular%20camera%2C%20from%20which%20the%20bearing%20%28line-of-sight%20direction%29%20to%20the%20platform%27s%20center%20and%20the%20normal%20vector%20of%20its%20planar%20surface%20are%20extracted.%20We%20propose%20a%20cascade%20observer%20with%20a%20complementary%20filter%20on%20SO%283%29%20to%20reconstruct%20the%20relative%20attitude%2C%20followed%20by%20a%20linear%20Riccati%20observer%20for%20relative%20position%20and%20velocity%20estimation.%20Convergence%20of%20both%20observers%20is%20established%20under%20persistently%20exciting%20conditions%2C%20and%20the%20cascade%20is%20shown%20to%20be%20almost%20globally%20asymptotically%20and%20locally%20exponentially%20stable.%20We%20further%20extend%20the%20design%20to%20the%20case%20where%20the%20platform%27s%20rotation%20is%20restricted%20to%20its%20normal%20axis%20and%20show%20that%20its%20measured%20linear%20acceleration%20can%20be%20exploited%20to%20recover%20the%20remaining%20unobservable%20rotation%20angle.%20A%20sufficient%20condition%20to%20ensure%20local%20exponential%20convergence%20in%20this%20setting%20is%20provided.%20The%20performance%20of%20the%20proposed%20observers%20is%20validated%20through%20extensive%20simulations.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19245v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVision-Aided%2520Relative%2520State%2520Estimation%2520for%2520Approach%2520and%2520Landing%2520on%2520a%2520Moving%2520Platform%2520with%2520Inertial%2520Measurements%26entry.906535625%3DTarek%2520Bouazza%2520and%2520Alessandro%2520Melis%2520and%2520Soulaimane%2520Berkane%2520and%2520Robert%2520Mahony%2520and%2520Tarek%2520Hamel%26entry.1292438233%3DThis%2520paper%2520tackles%2520the%2520problem%2520of%2520estimating%2520the%2520relative%2520position%252C%2520orientation%252C%2520and%2520velocity%2520between%2520a%2520UAV%2520and%2520a%2520planar%2520platform%2520undergoing%2520arbitrary%25203D%2520motion%2520during%2520approach%2520and%2520landing.%2520The%2520estimation%2520relies%2520on%2520measurements%2520from%2520Inertial%2520Measurement%2520Units%2520%2528IMUs%2529%2520mounted%2520on%2520both%2520systems%252C%2520assuming%2520there%2520is%2520a%2520suitable%2520communication%2520channel%2520to%2520exchange%2520data%252C%2520together%2520with%2520visual%2520information%2520provided%2520by%2520an%2520onboard%2520monocular%2520camera%252C%2520from%2520which%2520the%2520bearing%2520%2528line-of-sight%2520direction%2529%2520to%2520the%2520platform%2527s%2520center%2520and%2520the%2520normal%2520vector%2520of%2520its%2520planar%2520surface%2520are%2520extracted.%2520We%2520propose%2520a%2520cascade%2520observer%2520with%2520a%2520complementary%2520filter%2520on%2520SO%25283%2529%2520to%2520reconstruct%2520the%2520relative%2520attitude%252C%2520followed%2520by%2520a%2520linear%2520Riccati%2520observer%2520for%2520relative%2520position%2520and%2520velocity%2520estimation.%2520Convergence%2520of%2520both%2520observers%2520is%2520established%2520under%2520persistently%2520exciting%2520conditions%252C%2520and%2520the%2520cascade%2520is%2520shown%2520to%2520be%2520almost%2520globally%2520asymptotically%2520and%2520locally%2520exponentially%2520stable.%2520We%2520further%2520extend%2520the%2520design%2520to%2520the%2520case%2520where%2520the%2520platform%2527s%2520rotation%2520is%2520restricted%2520to%2520its%2520normal%2520axis%2520and%2520show%2520that%2520its%2520measured%2520linear%2520acceleration%2520can%2520be%2520exploited%2520to%2520recover%2520the%2520remaining%2520unobservable%2520rotation%2520angle.%2520A%2520sufficient%2520condition%2520to%2520ensure%2520local%2520exponential%2520convergence%2520in%2520this%2520setting%2520is%2520provided.%2520The%2520performance%2520of%2520the%2520proposed%2520observers%2520is%2520validated%2520through%2520extensive%2520simulations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19245v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vision-Aided%20Relative%20State%20Estimation%20for%20Approach%20and%20Landing%20on%20a%20Moving%20Platform%20with%20Inertial%20Measurements&entry.906535625=Tarek%20Bouazza%20and%20Alessandro%20Melis%20and%20Soulaimane%20Berkane%20and%20Robert%20Mahony%20and%20Tarek%20Hamel&entry.1292438233=This%20paper%20tackles%20the%20problem%20of%20estimating%20the%20relative%20position%2C%20orientation%2C%20and%20velocity%20between%20a%20UAV%20and%20a%20planar%20platform%20undergoing%20arbitrary%203D%20motion%20during%20approach%20and%20landing.%20The%20estimation%20relies%20on%20measurements%20from%20Inertial%20Measurement%20Units%20%28IMUs%29%20mounted%20on%20both%20systems%2C%20assuming%20there%20is%20a%20suitable%20communication%20channel%20to%20exchange%20data%2C%20together%20with%20visual%20information%20provided%20by%20an%20onboard%20monocular%20camera%2C%20from%20which%20the%20bearing%20%28line-of-sight%20direction%29%20to%20the%20platform%27s%20center%20and%20the%20normal%20vector%20of%20its%20planar%20surface%20are%20extracted.%20We%20propose%20a%20cascade%20observer%20with%20a%20complementary%20filter%20on%20SO%283%29%20to%20reconstruct%20the%20relative%20attitude%2C%20followed%20by%20a%20linear%20Riccati%20observer%20for%20relative%20position%20and%20velocity%20estimation.%20Convergence%20of%20both%20observers%20is%20established%20under%20persistently%20exciting%20conditions%2C%20and%20the%20cascade%20is%20shown%20to%20be%20almost%20globally%20asymptotically%20and%20locally%20exponentially%20stable.%20We%20further%20extend%20the%20design%20to%20the%20case%20where%20the%20platform%27s%20rotation%20is%20restricted%20to%20its%20normal%20axis%20and%20show%20that%20its%20measured%20linear%20acceleration%20can%20be%20exploited%20to%20recover%20the%20remaining%20unobservable%20rotation%20angle.%20A%20sufficient%20condition%20to%20ensure%20local%20exponential%20convergence%20in%20this%20setting%20is%20provided.%20The%20performance%20of%20the%20proposed%20observers%20is%20validated%20through%20extensive%20simulations.&entry.1838667208=http%3A//arxiv.org/abs/2512.19245v1&entry.124074799=Read"},
{"title": "Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models", "author": "Zixuan Ye and Quande Liu and Cong Wei and Yuanxing Zhang and Xintao Wang and Pengfei Wan and Kun Gai and Wenhan Luo", "abstract": "Recently, the introduction of Chain-of-Thought (CoT) has largely improved the generation ability of unified models. However, it is observed that the current thinking process during generation mainly focuses on the text consistency with the text prompt, ignoring the \\textbf{visual context consistency} with the visual reference images during the multi-modal generation, e.g., multi-reference generation. The lack of such consistency results in the failure in maintaining key visual features (like human ID, object attribute, style). To this end, we integrate the visual context consistency into the reasoning of unified models, explicitly motivating the model to sustain such consistency by 1) Adaptive Visual Planning: generating structured visual check list to figure out the visual element of needed consistency keeping, and 2) Iterative Visual Correction: performing self-reflection with the guidance of check lists and refining the generated result in an iterative manner. To achieve this, we use supervised finetuning to teach the model how to plan the visual checking, conduct self-reflection and self-refinement, and use flow-GRPO to further enhance the visual consistency through a customized visual checking reward. The experiments show that our method outperforms both zero-shot unified models and those with text CoTs in multi-modal generation, demonstrating higher visual context consistency.", "link": "http://arxiv.org/abs/2512.19686v1", "date": "2025-12-22", "relevancy": 2.1472, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5506}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.534}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual-Aware%20CoT%3A%20Achieving%20High-Fidelity%20Visual%20Consistency%20in%20Unified%20Models&body=Title%3A%20Visual-Aware%20CoT%3A%20Achieving%20High-Fidelity%20Visual%20Consistency%20in%20Unified%20Models%0AAuthor%3A%20Zixuan%20Ye%20and%20Quande%20Liu%20and%20Cong%20Wei%20and%20Yuanxing%20Zhang%20and%20Xintao%20Wang%20and%20Pengfei%20Wan%20and%20Kun%20Gai%20and%20Wenhan%20Luo%0AAbstract%3A%20Recently%2C%20the%20introduction%20of%20Chain-of-Thought%20%28CoT%29%20has%20largely%20improved%20the%20generation%20ability%20of%20unified%20models.%20However%2C%20it%20is%20observed%20that%20the%20current%20thinking%20process%20during%20generation%20mainly%20focuses%20on%20the%20text%20consistency%20with%20the%20text%20prompt%2C%20ignoring%20the%20%5Ctextbf%7Bvisual%20context%20consistency%7D%20with%20the%20visual%20reference%20images%20during%20the%20multi-modal%20generation%2C%20e.g.%2C%20multi-reference%20generation.%20The%20lack%20of%20such%20consistency%20results%20in%20the%20failure%20in%20maintaining%20key%20visual%20features%20%28like%20human%20ID%2C%20object%20attribute%2C%20style%29.%20To%20this%20end%2C%20we%20integrate%20the%20visual%20context%20consistency%20into%20the%20reasoning%20of%20unified%20models%2C%20explicitly%20motivating%20the%20model%20to%20sustain%20such%20consistency%20by%201%29%20Adaptive%20Visual%20Planning%3A%20generating%20structured%20visual%20check%20list%20to%20figure%20out%20the%20visual%20element%20of%20needed%20consistency%20keeping%2C%20and%202%29%20Iterative%20Visual%20Correction%3A%20performing%20self-reflection%20with%20the%20guidance%20of%20check%20lists%20and%20refining%20the%20generated%20result%20in%20an%20iterative%20manner.%20To%20achieve%20this%2C%20we%20use%20supervised%20finetuning%20to%20teach%20the%20model%20how%20to%20plan%20the%20visual%20checking%2C%20conduct%20self-reflection%20and%20self-refinement%2C%20and%20use%20flow-GRPO%20to%20further%20enhance%20the%20visual%20consistency%20through%20a%20customized%20visual%20checking%20reward.%20The%20experiments%20show%20that%20our%20method%20outperforms%20both%20zero-shot%20unified%20models%20and%20those%20with%20text%20CoTs%20in%20multi-modal%20generation%2C%20demonstrating%20higher%20visual%20context%20consistency.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19686v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual-Aware%2520CoT%253A%2520Achieving%2520High-Fidelity%2520Visual%2520Consistency%2520in%2520Unified%2520Models%26entry.906535625%3DZixuan%2520Ye%2520and%2520Quande%2520Liu%2520and%2520Cong%2520Wei%2520and%2520Yuanxing%2520Zhang%2520and%2520Xintao%2520Wang%2520and%2520Pengfei%2520Wan%2520and%2520Kun%2520Gai%2520and%2520Wenhan%2520Luo%26entry.1292438233%3DRecently%252C%2520the%2520introduction%2520of%2520Chain-of-Thought%2520%2528CoT%2529%2520has%2520largely%2520improved%2520the%2520generation%2520ability%2520of%2520unified%2520models.%2520However%252C%2520it%2520is%2520observed%2520that%2520the%2520current%2520thinking%2520process%2520during%2520generation%2520mainly%2520focuses%2520on%2520the%2520text%2520consistency%2520with%2520the%2520text%2520prompt%252C%2520ignoring%2520the%2520%255Ctextbf%257Bvisual%2520context%2520consistency%257D%2520with%2520the%2520visual%2520reference%2520images%2520during%2520the%2520multi-modal%2520generation%252C%2520e.g.%252C%2520multi-reference%2520generation.%2520The%2520lack%2520of%2520such%2520consistency%2520results%2520in%2520the%2520failure%2520in%2520maintaining%2520key%2520visual%2520features%2520%2528like%2520human%2520ID%252C%2520object%2520attribute%252C%2520style%2529.%2520To%2520this%2520end%252C%2520we%2520integrate%2520the%2520visual%2520context%2520consistency%2520into%2520the%2520reasoning%2520of%2520unified%2520models%252C%2520explicitly%2520motivating%2520the%2520model%2520to%2520sustain%2520such%2520consistency%2520by%25201%2529%2520Adaptive%2520Visual%2520Planning%253A%2520generating%2520structured%2520visual%2520check%2520list%2520to%2520figure%2520out%2520the%2520visual%2520element%2520of%2520needed%2520consistency%2520keeping%252C%2520and%25202%2529%2520Iterative%2520Visual%2520Correction%253A%2520performing%2520self-reflection%2520with%2520the%2520guidance%2520of%2520check%2520lists%2520and%2520refining%2520the%2520generated%2520result%2520in%2520an%2520iterative%2520manner.%2520To%2520achieve%2520this%252C%2520we%2520use%2520supervised%2520finetuning%2520to%2520teach%2520the%2520model%2520how%2520to%2520plan%2520the%2520visual%2520checking%252C%2520conduct%2520self-reflection%2520and%2520self-refinement%252C%2520and%2520use%2520flow-GRPO%2520to%2520further%2520enhance%2520the%2520visual%2520consistency%2520through%2520a%2520customized%2520visual%2520checking%2520reward.%2520The%2520experiments%2520show%2520that%2520our%2520method%2520outperforms%2520both%2520zero-shot%2520unified%2520models%2520and%2520those%2520with%2520text%2520CoTs%2520in%2520multi-modal%2520generation%252C%2520demonstrating%2520higher%2520visual%2520context%2520consistency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19686v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual-Aware%20CoT%3A%20Achieving%20High-Fidelity%20Visual%20Consistency%20in%20Unified%20Models&entry.906535625=Zixuan%20Ye%20and%20Quande%20Liu%20and%20Cong%20Wei%20and%20Yuanxing%20Zhang%20and%20Xintao%20Wang%20and%20Pengfei%20Wan%20and%20Kun%20Gai%20and%20Wenhan%20Luo&entry.1292438233=Recently%2C%20the%20introduction%20of%20Chain-of-Thought%20%28CoT%29%20has%20largely%20improved%20the%20generation%20ability%20of%20unified%20models.%20However%2C%20it%20is%20observed%20that%20the%20current%20thinking%20process%20during%20generation%20mainly%20focuses%20on%20the%20text%20consistency%20with%20the%20text%20prompt%2C%20ignoring%20the%20%5Ctextbf%7Bvisual%20context%20consistency%7D%20with%20the%20visual%20reference%20images%20during%20the%20multi-modal%20generation%2C%20e.g.%2C%20multi-reference%20generation.%20The%20lack%20of%20such%20consistency%20results%20in%20the%20failure%20in%20maintaining%20key%20visual%20features%20%28like%20human%20ID%2C%20object%20attribute%2C%20style%29.%20To%20this%20end%2C%20we%20integrate%20the%20visual%20context%20consistency%20into%20the%20reasoning%20of%20unified%20models%2C%20explicitly%20motivating%20the%20model%20to%20sustain%20such%20consistency%20by%201%29%20Adaptive%20Visual%20Planning%3A%20generating%20structured%20visual%20check%20list%20to%20figure%20out%20the%20visual%20element%20of%20needed%20consistency%20keeping%2C%20and%202%29%20Iterative%20Visual%20Correction%3A%20performing%20self-reflection%20with%20the%20guidance%20of%20check%20lists%20and%20refining%20the%20generated%20result%20in%20an%20iterative%20manner.%20To%20achieve%20this%2C%20we%20use%20supervised%20finetuning%20to%20teach%20the%20model%20how%20to%20plan%20the%20visual%20checking%2C%20conduct%20self-reflection%20and%20self-refinement%2C%20and%20use%20flow-GRPO%20to%20further%20enhance%20the%20visual%20consistency%20through%20a%20customized%20visual%20checking%20reward.%20The%20experiments%20show%20that%20our%20method%20outperforms%20both%20zero-shot%20unified%20models%20and%20those%20with%20text%20CoTs%20in%20multi-modal%20generation%2C%20demonstrating%20higher%20visual%20context%20consistency.&entry.1838667208=http%3A//arxiv.org/abs/2512.19686v1&entry.124074799=Read"},
{"title": "Confidence Calibration in Vision-Language-Action Models", "author": "Thomas P Zollo and Richard Zemel", "abstract": "Trustworthy robot behavior requires not only high levels of task success but also that the robot can reliably quantify how likely it is to succeed. To this end, we present a first-of-its-kind study of confidence calibration in vision-language-action (VLA) foundation models, which map visual observations and natural language instructions to low-level robot motor commands. We establish a confidence baseline for VLAs, examine how task success relates to calibration error and how calibration evolves over time, and introduce two lightweight techniques to remedy the miscalibration we observe: prompt ensembles and action-wise Platt scaling. Our aim in this study is to begin to develop the tools and conceptual understanding necessary to render VLAs both highly performant and highly trustworthy via reliable uncertainty quantification.", "link": "http://arxiv.org/abs/2507.17383v2", "date": "2025-12-22", "relevancy": 2.1438, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5589}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5525}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5102}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Confidence%20Calibration%20in%20Vision-Language-Action%20Models&body=Title%3A%20Confidence%20Calibration%20in%20Vision-Language-Action%20Models%0AAuthor%3A%20Thomas%20P%20Zollo%20and%20Richard%20Zemel%0AAbstract%3A%20Trustworthy%20robot%20behavior%20requires%20not%20only%20high%20levels%20of%20task%20success%20but%20also%20that%20the%20robot%20can%20reliably%20quantify%20how%20likely%20it%20is%20to%20succeed.%20To%20this%20end%2C%20we%20present%20a%20first-of-its-kind%20study%20of%20confidence%20calibration%20in%20vision-language-action%20%28VLA%29%20foundation%20models%2C%20which%20map%20visual%20observations%20and%20natural%20language%20instructions%20to%20low-level%20robot%20motor%20commands.%20We%20establish%20a%20confidence%20baseline%20for%20VLAs%2C%20examine%20how%20task%20success%20relates%20to%20calibration%20error%20and%20how%20calibration%20evolves%20over%20time%2C%20and%20introduce%20two%20lightweight%20techniques%20to%20remedy%20the%20miscalibration%20we%20observe%3A%20prompt%20ensembles%20and%20action-wise%20Platt%20scaling.%20Our%20aim%20in%20this%20study%20is%20to%20begin%20to%20develop%20the%20tools%20and%20conceptual%20understanding%20necessary%20to%20render%20VLAs%20both%20highly%20performant%20and%20highly%20trustworthy%20via%20reliable%20uncertainty%20quantification.%0ALink%3A%20http%3A//arxiv.org/abs/2507.17383v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConfidence%2520Calibration%2520in%2520Vision-Language-Action%2520Models%26entry.906535625%3DThomas%2520P%2520Zollo%2520and%2520Richard%2520Zemel%26entry.1292438233%3DTrustworthy%2520robot%2520behavior%2520requires%2520not%2520only%2520high%2520levels%2520of%2520task%2520success%2520but%2520also%2520that%2520the%2520robot%2520can%2520reliably%2520quantify%2520how%2520likely%2520it%2520is%2520to%2520succeed.%2520To%2520this%2520end%252C%2520we%2520present%2520a%2520first-of-its-kind%2520study%2520of%2520confidence%2520calibration%2520in%2520vision-language-action%2520%2528VLA%2529%2520foundation%2520models%252C%2520which%2520map%2520visual%2520observations%2520and%2520natural%2520language%2520instructions%2520to%2520low-level%2520robot%2520motor%2520commands.%2520We%2520establish%2520a%2520confidence%2520baseline%2520for%2520VLAs%252C%2520examine%2520how%2520task%2520success%2520relates%2520to%2520calibration%2520error%2520and%2520how%2520calibration%2520evolves%2520over%2520time%252C%2520and%2520introduce%2520two%2520lightweight%2520techniques%2520to%2520remedy%2520the%2520miscalibration%2520we%2520observe%253A%2520prompt%2520ensembles%2520and%2520action-wise%2520Platt%2520scaling.%2520Our%2520aim%2520in%2520this%2520study%2520is%2520to%2520begin%2520to%2520develop%2520the%2520tools%2520and%2520conceptual%2520understanding%2520necessary%2520to%2520render%2520VLAs%2520both%2520highly%2520performant%2520and%2520highly%2520trustworthy%2520via%2520reliable%2520uncertainty%2520quantification.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17383v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Confidence%20Calibration%20in%20Vision-Language-Action%20Models&entry.906535625=Thomas%20P%20Zollo%20and%20Richard%20Zemel&entry.1292438233=Trustworthy%20robot%20behavior%20requires%20not%20only%20high%20levels%20of%20task%20success%20but%20also%20that%20the%20robot%20can%20reliably%20quantify%20how%20likely%20it%20is%20to%20succeed.%20To%20this%20end%2C%20we%20present%20a%20first-of-its-kind%20study%20of%20confidence%20calibration%20in%20vision-language-action%20%28VLA%29%20foundation%20models%2C%20which%20map%20visual%20observations%20and%20natural%20language%20instructions%20to%20low-level%20robot%20motor%20commands.%20We%20establish%20a%20confidence%20baseline%20for%20VLAs%2C%20examine%20how%20task%20success%20relates%20to%20calibration%20error%20and%20how%20calibration%20evolves%20over%20time%2C%20and%20introduce%20two%20lightweight%20techniques%20to%20remedy%20the%20miscalibration%20we%20observe%3A%20prompt%20ensembles%20and%20action-wise%20Platt%20scaling.%20Our%20aim%20in%20this%20study%20is%20to%20begin%20to%20develop%20the%20tools%20and%20conceptual%20understanding%20necessary%20to%20render%20VLAs%20both%20highly%20performant%20and%20highly%20trustworthy%20via%20reliable%20uncertainty%20quantification.&entry.1838667208=http%3A//arxiv.org/abs/2507.17383v2&entry.124074799=Read"},
{"title": "Patlak Parametric Image Estimation from Dynamic PET Using Diffusion Model Prior", "author": "Ziqian Huang and Boxiao Yu and Siqi Li and Savas Ozdemir and Sangjin Bae and Jae Sung Lee and Guobao Wang and Kuang Gong", "abstract": "Dynamic PET enables the quantitative estimation of physiology-related parameters and is widely utilized in research and increasingly adopted in clinical settings. Parametric imaging in dynamic PET requires kinetic modeling to estimate voxel-wise physiological parameters based on specific kinetic models. However, parametric images estimated through kinetic model fitting often suffer from low image quality due to the inherently ill-posed nature of the fitting process and the limited counts resulting from non-continuous data acquisition across multiple bed positions in whole-body PET. In this work, we proposed a diffusion model-based kinetic modeling framework for parametric image estimation, using the Patlak model as an example. The score function of the diffusion model was pre-trained on static total-body PET images and served as a prior for both Patlak slope and intercept images by leveraging their patch-wise similarity. During inference, the kinetic model was incorporated as a data-consistency constraint to guide the parametric image estimation. The proposed framework was evaluated on total-body dynamic PET datasets with different dose levels, demonstrating the feasibility and promising performance of the proposed framework in improving parametric image quality.", "link": "http://arxiv.org/abs/2512.19584v1", "date": "2025-12-22", "relevancy": 2.1398, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5634}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5366}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5219}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Patlak%20Parametric%20Image%20Estimation%20from%20Dynamic%20PET%20Using%20Diffusion%20Model%20Prior&body=Title%3A%20Patlak%20Parametric%20Image%20Estimation%20from%20Dynamic%20PET%20Using%20Diffusion%20Model%20Prior%0AAuthor%3A%20Ziqian%20Huang%20and%20Boxiao%20Yu%20and%20Siqi%20Li%20and%20Savas%20Ozdemir%20and%20Sangjin%20Bae%20and%20Jae%20Sung%20Lee%20and%20Guobao%20Wang%20and%20Kuang%20Gong%0AAbstract%3A%20Dynamic%20PET%20enables%20the%20quantitative%20estimation%20of%20physiology-related%20parameters%20and%20is%20widely%20utilized%20in%20research%20and%20increasingly%20adopted%20in%20clinical%20settings.%20Parametric%20imaging%20in%20dynamic%20PET%20requires%20kinetic%20modeling%20to%20estimate%20voxel-wise%20physiological%20parameters%20based%20on%20specific%20kinetic%20models.%20However%2C%20parametric%20images%20estimated%20through%20kinetic%20model%20fitting%20often%20suffer%20from%20low%20image%20quality%20due%20to%20the%20inherently%20ill-posed%20nature%20of%20the%20fitting%20process%20and%20the%20limited%20counts%20resulting%20from%20non-continuous%20data%20acquisition%20across%20multiple%20bed%20positions%20in%20whole-body%20PET.%20In%20this%20work%2C%20we%20proposed%20a%20diffusion%20model-based%20kinetic%20modeling%20framework%20for%20parametric%20image%20estimation%2C%20using%20the%20Patlak%20model%20as%20an%20example.%20The%20score%20function%20of%20the%20diffusion%20model%20was%20pre-trained%20on%20static%20total-body%20PET%20images%20and%20served%20as%20a%20prior%20for%20both%20Patlak%20slope%20and%20intercept%20images%20by%20leveraging%20their%20patch-wise%20similarity.%20During%20inference%2C%20the%20kinetic%20model%20was%20incorporated%20as%20a%20data-consistency%20constraint%20to%20guide%20the%20parametric%20image%20estimation.%20The%20proposed%20framework%20was%20evaluated%20on%20total-body%20dynamic%20PET%20datasets%20with%20different%20dose%20levels%2C%20demonstrating%20the%20feasibility%20and%20promising%20performance%20of%20the%20proposed%20framework%20in%20improving%20parametric%20image%20quality.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19584v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPatlak%2520Parametric%2520Image%2520Estimation%2520from%2520Dynamic%2520PET%2520Using%2520Diffusion%2520Model%2520Prior%26entry.906535625%3DZiqian%2520Huang%2520and%2520Boxiao%2520Yu%2520and%2520Siqi%2520Li%2520and%2520Savas%2520Ozdemir%2520and%2520Sangjin%2520Bae%2520and%2520Jae%2520Sung%2520Lee%2520and%2520Guobao%2520Wang%2520and%2520Kuang%2520Gong%26entry.1292438233%3DDynamic%2520PET%2520enables%2520the%2520quantitative%2520estimation%2520of%2520physiology-related%2520parameters%2520and%2520is%2520widely%2520utilized%2520in%2520research%2520and%2520increasingly%2520adopted%2520in%2520clinical%2520settings.%2520Parametric%2520imaging%2520in%2520dynamic%2520PET%2520requires%2520kinetic%2520modeling%2520to%2520estimate%2520voxel-wise%2520physiological%2520parameters%2520based%2520on%2520specific%2520kinetic%2520models.%2520However%252C%2520parametric%2520images%2520estimated%2520through%2520kinetic%2520model%2520fitting%2520often%2520suffer%2520from%2520low%2520image%2520quality%2520due%2520to%2520the%2520inherently%2520ill-posed%2520nature%2520of%2520the%2520fitting%2520process%2520and%2520the%2520limited%2520counts%2520resulting%2520from%2520non-continuous%2520data%2520acquisition%2520across%2520multiple%2520bed%2520positions%2520in%2520whole-body%2520PET.%2520In%2520this%2520work%252C%2520we%2520proposed%2520a%2520diffusion%2520model-based%2520kinetic%2520modeling%2520framework%2520for%2520parametric%2520image%2520estimation%252C%2520using%2520the%2520Patlak%2520model%2520as%2520an%2520example.%2520The%2520score%2520function%2520of%2520the%2520diffusion%2520model%2520was%2520pre-trained%2520on%2520static%2520total-body%2520PET%2520images%2520and%2520served%2520as%2520a%2520prior%2520for%2520both%2520Patlak%2520slope%2520and%2520intercept%2520images%2520by%2520leveraging%2520their%2520patch-wise%2520similarity.%2520During%2520inference%252C%2520the%2520kinetic%2520model%2520was%2520incorporated%2520as%2520a%2520data-consistency%2520constraint%2520to%2520guide%2520the%2520parametric%2520image%2520estimation.%2520The%2520proposed%2520framework%2520was%2520evaluated%2520on%2520total-body%2520dynamic%2520PET%2520datasets%2520with%2520different%2520dose%2520levels%252C%2520demonstrating%2520the%2520feasibility%2520and%2520promising%2520performance%2520of%2520the%2520proposed%2520framework%2520in%2520improving%2520parametric%2520image%2520quality.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19584v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Patlak%20Parametric%20Image%20Estimation%20from%20Dynamic%20PET%20Using%20Diffusion%20Model%20Prior&entry.906535625=Ziqian%20Huang%20and%20Boxiao%20Yu%20and%20Siqi%20Li%20and%20Savas%20Ozdemir%20and%20Sangjin%20Bae%20and%20Jae%20Sung%20Lee%20and%20Guobao%20Wang%20and%20Kuang%20Gong&entry.1292438233=Dynamic%20PET%20enables%20the%20quantitative%20estimation%20of%20physiology-related%20parameters%20and%20is%20widely%20utilized%20in%20research%20and%20increasingly%20adopted%20in%20clinical%20settings.%20Parametric%20imaging%20in%20dynamic%20PET%20requires%20kinetic%20modeling%20to%20estimate%20voxel-wise%20physiological%20parameters%20based%20on%20specific%20kinetic%20models.%20However%2C%20parametric%20images%20estimated%20through%20kinetic%20model%20fitting%20often%20suffer%20from%20low%20image%20quality%20due%20to%20the%20inherently%20ill-posed%20nature%20of%20the%20fitting%20process%20and%20the%20limited%20counts%20resulting%20from%20non-continuous%20data%20acquisition%20across%20multiple%20bed%20positions%20in%20whole-body%20PET.%20In%20this%20work%2C%20we%20proposed%20a%20diffusion%20model-based%20kinetic%20modeling%20framework%20for%20parametric%20image%20estimation%2C%20using%20the%20Patlak%20model%20as%20an%20example.%20The%20score%20function%20of%20the%20diffusion%20model%20was%20pre-trained%20on%20static%20total-body%20PET%20images%20and%20served%20as%20a%20prior%20for%20both%20Patlak%20slope%20and%20intercept%20images%20by%20leveraging%20their%20patch-wise%20similarity.%20During%20inference%2C%20the%20kinetic%20model%20was%20incorporated%20as%20a%20data-consistency%20constraint%20to%20guide%20the%20parametric%20image%20estimation.%20The%20proposed%20framework%20was%20evaluated%20on%20total-body%20dynamic%20PET%20datasets%20with%20different%20dose%20levels%2C%20demonstrating%20the%20feasibility%20and%20promising%20performance%20of%20the%20proposed%20framework%20in%20improving%20parametric%20image%20quality.&entry.1838667208=http%3A//arxiv.org/abs/2512.19584v1&entry.124074799=Read"},
{"title": "GShield: Mitigating Poisoning Attacks in Federated Learning", "author": "Sameera K. M. and Serena Nicolazzo and Antonino Nocera and Vinod P. and Rafidha Rehiman K. A", "abstract": "Federated Learning (FL) has recently emerged as a revolutionary approach to collaborative training Machine Learning models. In particular, it enables decentralized model training while preserving data privacy, but its distributed nature makes it highly vulnerable to a severe attack known as Data Poisoning. In such scenarios, malicious clients inject manipulated data into the training process, thereby degrading global model performance or causing targeted misclassification. In this paper, we present a novel defense mechanism called GShield, designed to detect and mitigate malicious and low-quality updates, especially under non-independent and identically distributed (non-IID) data scenarios. GShield operates by learning the distribution of benign gradients through clustering and Gaussian modeling during an initial round, enabling it to establish a reliable baseline of trusted client behavior. With this benign profile, GShield selectively aggregates only those updates that align with the expected gradient patterns, effectively isolating adversarial clients and preserving the integrity of the global model. An extensive experimental campaign demonstrates that our proposed defense significantly improves model robustness compared to the state-of-the-art methods while maintaining a high accuracy of performance across both tabular and image datasets. Furthermore, GShield improves the accuracy of the targeted class by 43\\% to 65\\% after detecting malicious and low-quality clients.", "link": "http://arxiv.org/abs/2512.19286v1", "date": "2025-12-22", "relevancy": 2.1396, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.434}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4291}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GShield%3A%20Mitigating%20Poisoning%20Attacks%20in%20Federated%20Learning&body=Title%3A%20GShield%3A%20Mitigating%20Poisoning%20Attacks%20in%20Federated%20Learning%0AAuthor%3A%20Sameera%20K.%20M.%20and%20Serena%20Nicolazzo%20and%20Antonino%20Nocera%20and%20Vinod%20P.%20and%20Rafidha%20Rehiman%20K.%20A%0AAbstract%3A%20Federated%20Learning%20%28FL%29%20has%20recently%20emerged%20as%20a%20revolutionary%20approach%20to%20collaborative%20training%20Machine%20Learning%20models.%20In%20particular%2C%20it%20enables%20decentralized%20model%20training%20while%20preserving%20data%20privacy%2C%20but%20its%20distributed%20nature%20makes%20it%20highly%20vulnerable%20to%20a%20severe%20attack%20known%20as%20Data%20Poisoning.%20In%20such%20scenarios%2C%20malicious%20clients%20inject%20manipulated%20data%20into%20the%20training%20process%2C%20thereby%20degrading%20global%20model%20performance%20or%20causing%20targeted%20misclassification.%20In%20this%20paper%2C%20we%20present%20a%20novel%20defense%20mechanism%20called%20GShield%2C%20designed%20to%20detect%20and%20mitigate%20malicious%20and%20low-quality%20updates%2C%20especially%20under%20non-independent%20and%20identically%20distributed%20%28non-IID%29%20data%20scenarios.%20GShield%20operates%20by%20learning%20the%20distribution%20of%20benign%20gradients%20through%20clustering%20and%20Gaussian%20modeling%20during%20an%20initial%20round%2C%20enabling%20it%20to%20establish%20a%20reliable%20baseline%20of%20trusted%20client%20behavior.%20With%20this%20benign%20profile%2C%20GShield%20selectively%20aggregates%20only%20those%20updates%20that%20align%20with%20the%20expected%20gradient%20patterns%2C%20effectively%20isolating%20adversarial%20clients%20and%20preserving%20the%20integrity%20of%20the%20global%20model.%20An%20extensive%20experimental%20campaign%20demonstrates%20that%20our%20proposed%20defense%20significantly%20improves%20model%20robustness%20compared%20to%20the%20state-of-the-art%20methods%20while%20maintaining%20a%20high%20accuracy%20of%20performance%20across%20both%20tabular%20and%20image%20datasets.%20Furthermore%2C%20GShield%20improves%20the%20accuracy%20of%20the%20targeted%20class%20by%2043%5C%25%20to%2065%5C%25%20after%20detecting%20malicious%20and%20low-quality%20clients.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19286v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGShield%253A%2520Mitigating%2520Poisoning%2520Attacks%2520in%2520Federated%2520Learning%26entry.906535625%3DSameera%2520K.%2520M.%2520and%2520Serena%2520Nicolazzo%2520and%2520Antonino%2520Nocera%2520and%2520Vinod%2520P.%2520and%2520Rafidha%2520Rehiman%2520K.%2520A%26entry.1292438233%3DFederated%2520Learning%2520%2528FL%2529%2520has%2520recently%2520emerged%2520as%2520a%2520revolutionary%2520approach%2520to%2520collaborative%2520training%2520Machine%2520Learning%2520models.%2520In%2520particular%252C%2520it%2520enables%2520decentralized%2520model%2520training%2520while%2520preserving%2520data%2520privacy%252C%2520but%2520its%2520distributed%2520nature%2520makes%2520it%2520highly%2520vulnerable%2520to%2520a%2520severe%2520attack%2520known%2520as%2520Data%2520Poisoning.%2520In%2520such%2520scenarios%252C%2520malicious%2520clients%2520inject%2520manipulated%2520data%2520into%2520the%2520training%2520process%252C%2520thereby%2520degrading%2520global%2520model%2520performance%2520or%2520causing%2520targeted%2520misclassification.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520defense%2520mechanism%2520called%2520GShield%252C%2520designed%2520to%2520detect%2520and%2520mitigate%2520malicious%2520and%2520low-quality%2520updates%252C%2520especially%2520under%2520non-independent%2520and%2520identically%2520distributed%2520%2528non-IID%2529%2520data%2520scenarios.%2520GShield%2520operates%2520by%2520learning%2520the%2520distribution%2520of%2520benign%2520gradients%2520through%2520clustering%2520and%2520Gaussian%2520modeling%2520during%2520an%2520initial%2520round%252C%2520enabling%2520it%2520to%2520establish%2520a%2520reliable%2520baseline%2520of%2520trusted%2520client%2520behavior.%2520With%2520this%2520benign%2520profile%252C%2520GShield%2520selectively%2520aggregates%2520only%2520those%2520updates%2520that%2520align%2520with%2520the%2520expected%2520gradient%2520patterns%252C%2520effectively%2520isolating%2520adversarial%2520clients%2520and%2520preserving%2520the%2520integrity%2520of%2520the%2520global%2520model.%2520An%2520extensive%2520experimental%2520campaign%2520demonstrates%2520that%2520our%2520proposed%2520defense%2520significantly%2520improves%2520model%2520robustness%2520compared%2520to%2520the%2520state-of-the-art%2520methods%2520while%2520maintaining%2520a%2520high%2520accuracy%2520of%2520performance%2520across%2520both%2520tabular%2520and%2520image%2520datasets.%2520Furthermore%252C%2520GShield%2520improves%2520the%2520accuracy%2520of%2520the%2520targeted%2520class%2520by%252043%255C%2525%2520to%252065%255C%2525%2520after%2520detecting%2520malicious%2520and%2520low-quality%2520clients.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19286v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GShield%3A%20Mitigating%20Poisoning%20Attacks%20in%20Federated%20Learning&entry.906535625=Sameera%20K.%20M.%20and%20Serena%20Nicolazzo%20and%20Antonino%20Nocera%20and%20Vinod%20P.%20and%20Rafidha%20Rehiman%20K.%20A&entry.1292438233=Federated%20Learning%20%28FL%29%20has%20recently%20emerged%20as%20a%20revolutionary%20approach%20to%20collaborative%20training%20Machine%20Learning%20models.%20In%20particular%2C%20it%20enables%20decentralized%20model%20training%20while%20preserving%20data%20privacy%2C%20but%20its%20distributed%20nature%20makes%20it%20highly%20vulnerable%20to%20a%20severe%20attack%20known%20as%20Data%20Poisoning.%20In%20such%20scenarios%2C%20malicious%20clients%20inject%20manipulated%20data%20into%20the%20training%20process%2C%20thereby%20degrading%20global%20model%20performance%20or%20causing%20targeted%20misclassification.%20In%20this%20paper%2C%20we%20present%20a%20novel%20defense%20mechanism%20called%20GShield%2C%20designed%20to%20detect%20and%20mitigate%20malicious%20and%20low-quality%20updates%2C%20especially%20under%20non-independent%20and%20identically%20distributed%20%28non-IID%29%20data%20scenarios.%20GShield%20operates%20by%20learning%20the%20distribution%20of%20benign%20gradients%20through%20clustering%20and%20Gaussian%20modeling%20during%20an%20initial%20round%2C%20enabling%20it%20to%20establish%20a%20reliable%20baseline%20of%20trusted%20client%20behavior.%20With%20this%20benign%20profile%2C%20GShield%20selectively%20aggregates%20only%20those%20updates%20that%20align%20with%20the%20expected%20gradient%20patterns%2C%20effectively%20isolating%20adversarial%20clients%20and%20preserving%20the%20integrity%20of%20the%20global%20model.%20An%20extensive%20experimental%20campaign%20demonstrates%20that%20our%20proposed%20defense%20significantly%20improves%20model%20robustness%20compared%20to%20the%20state-of-the-art%20methods%20while%20maintaining%20a%20high%20accuracy%20of%20performance%20across%20both%20tabular%20and%20image%20datasets.%20Furthermore%2C%20GShield%20improves%20the%20accuracy%20of%20the%20targeted%20class%20by%2043%5C%25%20to%2065%5C%25%20after%20detecting%20malicious%20and%20low-quality%20clients.&entry.1838667208=http%3A//arxiv.org/abs/2512.19286v1&entry.124074799=Read"},
{"title": "MapTrace: Scalable Data Generation for Route Tracing on Maps", "author": "Artemis Panagopoulou and Aveek Purohit and Achin Kulshrestha and Soroosh Yazdani and Mohit Goyal", "abstract": "While Multimodal Large Language Models have achieved human-like performance on many visual and textual reasoning tasks, their proficiency in fine-grained spatial understanding, such as route tracing on maps remains limited. Unlike humans, who can quickly learn to parse and navigate maps, current models often fail to respect fundamental path constraints, in part due to the prohibitive cost and difficulty of collecting large-scale, pixel-accurate path annotations. To address this, we introduce a scalable synthetic data generation pipeline that leverages synthetic map images and pixel-level parsing to automatically produce precise annotations for this challenging task. Using this pipeline, we construct a fine-tuning dataset of 23k path samples across 4k maps, enabling models to acquire more human-like spatial capabilities. Using this dataset, we fine-tune both open-source and proprietary MLLMs. Results on MapBench show that finetuning substantially improves robustness, raising success rates by up to 6.4 points, while also reducing path-tracing error (NDTW). These gains highlight that fine-grained spatial reasoning, absent in pretrained models, can be explicitly taught with synthetic supervision.", "link": "http://arxiv.org/abs/2512.19609v1", "date": "2025-12-22", "relevancy": 2.1385, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5391}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5323}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MapTrace%3A%20Scalable%20Data%20Generation%20for%20Route%20Tracing%20on%20Maps&body=Title%3A%20MapTrace%3A%20Scalable%20Data%20Generation%20for%20Route%20Tracing%20on%20Maps%0AAuthor%3A%20Artemis%20Panagopoulou%20and%20Aveek%20Purohit%20and%20Achin%20Kulshrestha%20and%20Soroosh%20Yazdani%20and%20Mohit%20Goyal%0AAbstract%3A%20While%20Multimodal%20Large%20Language%20Models%20have%20achieved%20human-like%20performance%20on%20many%20visual%20and%20textual%20reasoning%20tasks%2C%20their%20proficiency%20in%20fine-grained%20spatial%20understanding%2C%20such%20as%20route%20tracing%20on%20maps%20remains%20limited.%20Unlike%20humans%2C%20who%20can%20quickly%20learn%20to%20parse%20and%20navigate%20maps%2C%20current%20models%20often%20fail%20to%20respect%20fundamental%20path%20constraints%2C%20in%20part%20due%20to%20the%20prohibitive%20cost%20and%20difficulty%20of%20collecting%20large-scale%2C%20pixel-accurate%20path%20annotations.%20To%20address%20this%2C%20we%20introduce%20a%20scalable%20synthetic%20data%20generation%20pipeline%20that%20leverages%20synthetic%20map%20images%20and%20pixel-level%20parsing%20to%20automatically%20produce%20precise%20annotations%20for%20this%20challenging%20task.%20Using%20this%20pipeline%2C%20we%20construct%20a%20fine-tuning%20dataset%20of%2023k%20path%20samples%20across%204k%20maps%2C%20enabling%20models%20to%20acquire%20more%20human-like%20spatial%20capabilities.%20Using%20this%20dataset%2C%20we%20fine-tune%20both%20open-source%20and%20proprietary%20MLLMs.%20Results%20on%20MapBench%20show%20that%20finetuning%20substantially%20improves%20robustness%2C%20raising%20success%20rates%20by%20up%20to%206.4%20points%2C%20while%20also%20reducing%20path-tracing%20error%20%28NDTW%29.%20These%20gains%20highlight%20that%20fine-grained%20spatial%20reasoning%2C%20absent%20in%20pretrained%20models%2C%20can%20be%20explicitly%20taught%20with%20synthetic%20supervision.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19609v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMapTrace%253A%2520Scalable%2520Data%2520Generation%2520for%2520Route%2520Tracing%2520on%2520Maps%26entry.906535625%3DArtemis%2520Panagopoulou%2520and%2520Aveek%2520Purohit%2520and%2520Achin%2520Kulshrestha%2520and%2520Soroosh%2520Yazdani%2520and%2520Mohit%2520Goyal%26entry.1292438233%3DWhile%2520Multimodal%2520Large%2520Language%2520Models%2520have%2520achieved%2520human-like%2520performance%2520on%2520many%2520visual%2520and%2520textual%2520reasoning%2520tasks%252C%2520their%2520proficiency%2520in%2520fine-grained%2520spatial%2520understanding%252C%2520such%2520as%2520route%2520tracing%2520on%2520maps%2520remains%2520limited.%2520Unlike%2520humans%252C%2520who%2520can%2520quickly%2520learn%2520to%2520parse%2520and%2520navigate%2520maps%252C%2520current%2520models%2520often%2520fail%2520to%2520respect%2520fundamental%2520path%2520constraints%252C%2520in%2520part%2520due%2520to%2520the%2520prohibitive%2520cost%2520and%2520difficulty%2520of%2520collecting%2520large-scale%252C%2520pixel-accurate%2520path%2520annotations.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520scalable%2520synthetic%2520data%2520generation%2520pipeline%2520that%2520leverages%2520synthetic%2520map%2520images%2520and%2520pixel-level%2520parsing%2520to%2520automatically%2520produce%2520precise%2520annotations%2520for%2520this%2520challenging%2520task.%2520Using%2520this%2520pipeline%252C%2520we%2520construct%2520a%2520fine-tuning%2520dataset%2520of%252023k%2520path%2520samples%2520across%25204k%2520maps%252C%2520enabling%2520models%2520to%2520acquire%2520more%2520human-like%2520spatial%2520capabilities.%2520Using%2520this%2520dataset%252C%2520we%2520fine-tune%2520both%2520open-source%2520and%2520proprietary%2520MLLMs.%2520Results%2520on%2520MapBench%2520show%2520that%2520finetuning%2520substantially%2520improves%2520robustness%252C%2520raising%2520success%2520rates%2520by%2520up%2520to%25206.4%2520points%252C%2520while%2520also%2520reducing%2520path-tracing%2520error%2520%2528NDTW%2529.%2520These%2520gains%2520highlight%2520that%2520fine-grained%2520spatial%2520reasoning%252C%2520absent%2520in%2520pretrained%2520models%252C%2520can%2520be%2520explicitly%2520taught%2520with%2520synthetic%2520supervision.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19609v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MapTrace%3A%20Scalable%20Data%20Generation%20for%20Route%20Tracing%20on%20Maps&entry.906535625=Artemis%20Panagopoulou%20and%20Aveek%20Purohit%20and%20Achin%20Kulshrestha%20and%20Soroosh%20Yazdani%20and%20Mohit%20Goyal&entry.1292438233=While%20Multimodal%20Large%20Language%20Models%20have%20achieved%20human-like%20performance%20on%20many%20visual%20and%20textual%20reasoning%20tasks%2C%20their%20proficiency%20in%20fine-grained%20spatial%20understanding%2C%20such%20as%20route%20tracing%20on%20maps%20remains%20limited.%20Unlike%20humans%2C%20who%20can%20quickly%20learn%20to%20parse%20and%20navigate%20maps%2C%20current%20models%20often%20fail%20to%20respect%20fundamental%20path%20constraints%2C%20in%20part%20due%20to%20the%20prohibitive%20cost%20and%20difficulty%20of%20collecting%20large-scale%2C%20pixel-accurate%20path%20annotations.%20To%20address%20this%2C%20we%20introduce%20a%20scalable%20synthetic%20data%20generation%20pipeline%20that%20leverages%20synthetic%20map%20images%20and%20pixel-level%20parsing%20to%20automatically%20produce%20precise%20annotations%20for%20this%20challenging%20task.%20Using%20this%20pipeline%2C%20we%20construct%20a%20fine-tuning%20dataset%20of%2023k%20path%20samples%20across%204k%20maps%2C%20enabling%20models%20to%20acquire%20more%20human-like%20spatial%20capabilities.%20Using%20this%20dataset%2C%20we%20fine-tune%20both%20open-source%20and%20proprietary%20MLLMs.%20Results%20on%20MapBench%20show%20that%20finetuning%20substantially%20improves%20robustness%2C%20raising%20success%20rates%20by%20up%20to%206.4%20points%2C%20while%20also%20reducing%20path-tracing%20error%20%28NDTW%29.%20These%20gains%20highlight%20that%20fine-grained%20spatial%20reasoning%2C%20absent%20in%20pretrained%20models%2C%20can%20be%20explicitly%20taught%20with%20synthetic%20supervision.&entry.1838667208=http%3A//arxiv.org/abs/2512.19609v1&entry.124074799=Read"},
{"title": "EchoTrail-GUI: Building Actionable Memory for GUI Agents via Critic-Guided Self-Exploration", "author": "Runze Li and Yuwen Zhai and Bo Xu and LiWu Xu and Nian Shi and Wei Zhang and Ran Lin and Liang Wang", "abstract": "Contemporary GUI agents, while increasingly capable due to advances in Large Vision-Language Models (VLMs), often operate with a critical limitation: they treat each task in isolation, lacking a mechanism to systematically learn from past successes. This digital ''amnesia'' results in sub-optimal performance, repeated errors, and poor generalization to novel challenges. To bridge this gap, we introduce EchoTrail-GUI, a novel framework designed to mimic human-like experiential learning by equipping agents with a dynamic, accessible memory. Our framework operates in three distinct stages. First, during Experience Exploration, an agent autonomously interacts with GUI environments to build a curated database of successful task trajectories, validated by a reward model. Crucially, the entire knowledge base construction is thus fully automated, requiring no human supervision. Second, in the Memory Injection stage, upon receiving a new task, our system efficiently retrieves the most relevant past trajectories to serve as actionable ''memories''. Finally, during GUI Task Inference, these memories are injected as in-context guidance to inform the agent's reasoning and decision-making process. We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab. The results show that EchoTrail-GUI significantly improves the task success rate and operational efficiency of baseline agents, validating the power of structured memory in creating more robust and intelligent GUI automation.", "link": "http://arxiv.org/abs/2512.19396v1", "date": "2025-12-22", "relevancy": 2.1356, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.574}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5311}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EchoTrail-GUI%3A%20Building%20Actionable%20Memory%20for%20GUI%20Agents%20via%20Critic-Guided%20Self-Exploration&body=Title%3A%20EchoTrail-GUI%3A%20Building%20Actionable%20Memory%20for%20GUI%20Agents%20via%20Critic-Guided%20Self-Exploration%0AAuthor%3A%20Runze%20Li%20and%20Yuwen%20Zhai%20and%20Bo%20Xu%20and%20LiWu%20Xu%20and%20Nian%20Shi%20and%20Wei%20Zhang%20and%20Ran%20Lin%20and%20Liang%20Wang%0AAbstract%3A%20Contemporary%20GUI%20agents%2C%20while%20increasingly%20capable%20due%20to%20advances%20in%20Large%20Vision-Language%20Models%20%28VLMs%29%2C%20often%20operate%20with%20a%20critical%20limitation%3A%20they%20treat%20each%20task%20in%20isolation%2C%20lacking%20a%20mechanism%20to%20systematically%20learn%20from%20past%20successes.%20This%20digital%20%27%27amnesia%27%27%20results%20in%20sub-optimal%20performance%2C%20repeated%20errors%2C%20and%20poor%20generalization%20to%20novel%20challenges.%20To%20bridge%20this%20gap%2C%20we%20introduce%20EchoTrail-GUI%2C%20a%20novel%20framework%20designed%20to%20mimic%20human-like%20experiential%20learning%20by%20equipping%20agents%20with%20a%20dynamic%2C%20accessible%20memory.%20Our%20framework%20operates%20in%20three%20distinct%20stages.%20First%2C%20during%20Experience%20Exploration%2C%20an%20agent%20autonomously%20interacts%20with%20GUI%20environments%20to%20build%20a%20curated%20database%20of%20successful%20task%20trajectories%2C%20validated%20by%20a%20reward%20model.%20Crucially%2C%20the%20entire%20knowledge%20base%20construction%20is%20thus%20fully%20automated%2C%20requiring%20no%20human%20supervision.%20Second%2C%20in%20the%20Memory%20Injection%20stage%2C%20upon%20receiving%20a%20new%20task%2C%20our%20system%20efficiently%20retrieves%20the%20most%20relevant%20past%20trajectories%20to%20serve%20as%20actionable%20%27%27memories%27%27.%20Finally%2C%20during%20GUI%20Task%20Inference%2C%20these%20memories%20are%20injected%20as%20in-context%20guidance%20to%20inform%20the%20agent%27s%20reasoning%20and%20decision-making%20process.%20We%20demonstrate%20the%20efficacy%20of%20our%20approach%20on%20benchmarks%20including%20Android%20World%20and%20AndroidLab.%20The%20results%20show%20that%20EchoTrail-GUI%20significantly%20improves%20the%20task%20success%20rate%20and%20operational%20efficiency%20of%20baseline%20agents%2C%20validating%20the%20power%20of%20structured%20memory%20in%20creating%20more%20robust%20and%20intelligent%20GUI%20automation.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19396v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEchoTrail-GUI%253A%2520Building%2520Actionable%2520Memory%2520for%2520GUI%2520Agents%2520via%2520Critic-Guided%2520Self-Exploration%26entry.906535625%3DRunze%2520Li%2520and%2520Yuwen%2520Zhai%2520and%2520Bo%2520Xu%2520and%2520LiWu%2520Xu%2520and%2520Nian%2520Shi%2520and%2520Wei%2520Zhang%2520and%2520Ran%2520Lin%2520and%2520Liang%2520Wang%26entry.1292438233%3DContemporary%2520GUI%2520agents%252C%2520while%2520increasingly%2520capable%2520due%2520to%2520advances%2520in%2520Large%2520Vision-Language%2520Models%2520%2528VLMs%2529%252C%2520often%2520operate%2520with%2520a%2520critical%2520limitation%253A%2520they%2520treat%2520each%2520task%2520in%2520isolation%252C%2520lacking%2520a%2520mechanism%2520to%2520systematically%2520learn%2520from%2520past%2520successes.%2520This%2520digital%2520%2527%2527amnesia%2527%2527%2520results%2520in%2520sub-optimal%2520performance%252C%2520repeated%2520errors%252C%2520and%2520poor%2520generalization%2520to%2520novel%2520challenges.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520EchoTrail-GUI%252C%2520a%2520novel%2520framework%2520designed%2520to%2520mimic%2520human-like%2520experiential%2520learning%2520by%2520equipping%2520agents%2520with%2520a%2520dynamic%252C%2520accessible%2520memory.%2520Our%2520framework%2520operates%2520in%2520three%2520distinct%2520stages.%2520First%252C%2520during%2520Experience%2520Exploration%252C%2520an%2520agent%2520autonomously%2520interacts%2520with%2520GUI%2520environments%2520to%2520build%2520a%2520curated%2520database%2520of%2520successful%2520task%2520trajectories%252C%2520validated%2520by%2520a%2520reward%2520model.%2520Crucially%252C%2520the%2520entire%2520knowledge%2520base%2520construction%2520is%2520thus%2520fully%2520automated%252C%2520requiring%2520no%2520human%2520supervision.%2520Second%252C%2520in%2520the%2520Memory%2520Injection%2520stage%252C%2520upon%2520receiving%2520a%2520new%2520task%252C%2520our%2520system%2520efficiently%2520retrieves%2520the%2520most%2520relevant%2520past%2520trajectories%2520to%2520serve%2520as%2520actionable%2520%2527%2527memories%2527%2527.%2520Finally%252C%2520during%2520GUI%2520Task%2520Inference%252C%2520these%2520memories%2520are%2520injected%2520as%2520in-context%2520guidance%2520to%2520inform%2520the%2520agent%2527s%2520reasoning%2520and%2520decision-making%2520process.%2520We%2520demonstrate%2520the%2520efficacy%2520of%2520our%2520approach%2520on%2520benchmarks%2520including%2520Android%2520World%2520and%2520AndroidLab.%2520The%2520results%2520show%2520that%2520EchoTrail-GUI%2520significantly%2520improves%2520the%2520task%2520success%2520rate%2520and%2520operational%2520efficiency%2520of%2520baseline%2520agents%252C%2520validating%2520the%2520power%2520of%2520structured%2520memory%2520in%2520creating%2520more%2520robust%2520and%2520intelligent%2520GUI%2520automation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19396v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EchoTrail-GUI%3A%20Building%20Actionable%20Memory%20for%20GUI%20Agents%20via%20Critic-Guided%20Self-Exploration&entry.906535625=Runze%20Li%20and%20Yuwen%20Zhai%20and%20Bo%20Xu%20and%20LiWu%20Xu%20and%20Nian%20Shi%20and%20Wei%20Zhang%20and%20Ran%20Lin%20and%20Liang%20Wang&entry.1292438233=Contemporary%20GUI%20agents%2C%20while%20increasingly%20capable%20due%20to%20advances%20in%20Large%20Vision-Language%20Models%20%28VLMs%29%2C%20often%20operate%20with%20a%20critical%20limitation%3A%20they%20treat%20each%20task%20in%20isolation%2C%20lacking%20a%20mechanism%20to%20systematically%20learn%20from%20past%20successes.%20This%20digital%20%27%27amnesia%27%27%20results%20in%20sub-optimal%20performance%2C%20repeated%20errors%2C%20and%20poor%20generalization%20to%20novel%20challenges.%20To%20bridge%20this%20gap%2C%20we%20introduce%20EchoTrail-GUI%2C%20a%20novel%20framework%20designed%20to%20mimic%20human-like%20experiential%20learning%20by%20equipping%20agents%20with%20a%20dynamic%2C%20accessible%20memory.%20Our%20framework%20operates%20in%20three%20distinct%20stages.%20First%2C%20during%20Experience%20Exploration%2C%20an%20agent%20autonomously%20interacts%20with%20GUI%20environments%20to%20build%20a%20curated%20database%20of%20successful%20task%20trajectories%2C%20validated%20by%20a%20reward%20model.%20Crucially%2C%20the%20entire%20knowledge%20base%20construction%20is%20thus%20fully%20automated%2C%20requiring%20no%20human%20supervision.%20Second%2C%20in%20the%20Memory%20Injection%20stage%2C%20upon%20receiving%20a%20new%20task%2C%20our%20system%20efficiently%20retrieves%20the%20most%20relevant%20past%20trajectories%20to%20serve%20as%20actionable%20%27%27memories%27%27.%20Finally%2C%20during%20GUI%20Task%20Inference%2C%20these%20memories%20are%20injected%20as%20in-context%20guidance%20to%20inform%20the%20agent%27s%20reasoning%20and%20decision-making%20process.%20We%20demonstrate%20the%20efficacy%20of%20our%20approach%20on%20benchmarks%20including%20Android%20World%20and%20AndroidLab.%20The%20results%20show%20that%20EchoTrail-GUI%20significantly%20improves%20the%20task%20success%20rate%20and%20operational%20efficiency%20of%20baseline%20agents%2C%20validating%20the%20power%20of%20structured%20memory%20in%20creating%20more%20robust%20and%20intelligent%20GUI%20automation.&entry.1838667208=http%3A//arxiv.org/abs/2512.19396v1&entry.124074799=Read"},
{"title": "DeltaMIL: Gated Memory Integration for Efficient and Discriminative Whole Slide Image Analysis", "author": "Yueting Zhu and Yuehao Song and Shuai Zhang and Wenyu Liu and Xinggang Wang", "abstract": "Whole Slide Images (WSIs) are typically analyzed using multiple instance learning (MIL) methods. However, the scale and heterogeneity of WSIs generate highly redundant and dispersed information, making it difficult to identify and integrate discriminative signals. Existing MIL methods either fail to discard uninformative cues effectively or have limited ability to consolidate relevant features from multiple patches, which restricts their performance on large and heterogeneous WSIs. To address this issue, we propose DeltaMIL, a novel MIL framework that explicitly selects semantically relevant regions and integrates the discriminative information from WSIs. Our method leverages the gated delta rule to efficiently filter and integrate information through a block combining forgetting and memory mechanisms. The delta mechanism dynamically updates the memory by removing old values and inserting new ones according to their correlation with the current patch. The gating mechanism further enables rapid forgetting of irrelevant signals. Additionally, DeltaMIL integrates a complementary local pattern mixing mechanism to retain fine-grained pathological locality. Our design enhances the extraction of meaningful cues and suppresses redundant or noisy information, which improves the model's robustness and discriminative power. Experiments demonstrate that DeltaMIL achieves state-of-the-art performance. Specifically, for survival prediction, DeltaMIL improves performance by 3.69\\% using ResNet-50 features and 2.36\\% using UNI features. For slide-level classification, it increases accuracy by 3.09\\% with ResNet-50 features and 3.75\\% with UNI features. These results demonstrate the strong and consistent performance of DeltaMIL across diverse WSI tasks.", "link": "http://arxiv.org/abs/2512.19331v1", "date": "2025-12-22", "relevancy": 2.123, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5555}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.533}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeltaMIL%3A%20Gated%20Memory%20Integration%20for%20Efficient%20and%20Discriminative%20Whole%20Slide%20Image%20Analysis&body=Title%3A%20DeltaMIL%3A%20Gated%20Memory%20Integration%20for%20Efficient%20and%20Discriminative%20Whole%20Slide%20Image%20Analysis%0AAuthor%3A%20Yueting%20Zhu%20and%20Yuehao%20Song%20and%20Shuai%20Zhang%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang%0AAbstract%3A%20Whole%20Slide%20Images%20%28WSIs%29%20are%20typically%20analyzed%20using%20multiple%20instance%20learning%20%28MIL%29%20methods.%20However%2C%20the%20scale%20and%20heterogeneity%20of%20WSIs%20generate%20highly%20redundant%20and%20dispersed%20information%2C%20making%20it%20difficult%20to%20identify%20and%20integrate%20discriminative%20signals.%20Existing%20MIL%20methods%20either%20fail%20to%20discard%20uninformative%20cues%20effectively%20or%20have%20limited%20ability%20to%20consolidate%20relevant%20features%20from%20multiple%20patches%2C%20which%20restricts%20their%20performance%20on%20large%20and%20heterogeneous%20WSIs.%20To%20address%20this%20issue%2C%20we%20propose%20DeltaMIL%2C%20a%20novel%20MIL%20framework%20that%20explicitly%20selects%20semantically%20relevant%20regions%20and%20integrates%20the%20discriminative%20information%20from%20WSIs.%20Our%20method%20leverages%20the%20gated%20delta%20rule%20to%20efficiently%20filter%20and%20integrate%20information%20through%20a%20block%20combining%20forgetting%20and%20memory%20mechanisms.%20The%20delta%20mechanism%20dynamically%20updates%20the%20memory%20by%20removing%20old%20values%20and%20inserting%20new%20ones%20according%20to%20their%20correlation%20with%20the%20current%20patch.%20The%20gating%20mechanism%20further%20enables%20rapid%20forgetting%20of%20irrelevant%20signals.%20Additionally%2C%20DeltaMIL%20integrates%20a%20complementary%20local%20pattern%20mixing%20mechanism%20to%20retain%20fine-grained%20pathological%20locality.%20Our%20design%20enhances%20the%20extraction%20of%20meaningful%20cues%20and%20suppresses%20redundant%20or%20noisy%20information%2C%20which%20improves%20the%20model%27s%20robustness%20and%20discriminative%20power.%20Experiments%20demonstrate%20that%20DeltaMIL%20achieves%20state-of-the-art%20performance.%20Specifically%2C%20for%20survival%20prediction%2C%20DeltaMIL%20improves%20performance%20by%203.69%5C%25%20using%20ResNet-50%20features%20and%202.36%5C%25%20using%20UNI%20features.%20For%20slide-level%20classification%2C%20it%20increases%20accuracy%20by%203.09%5C%25%20with%20ResNet-50%20features%20and%203.75%5C%25%20with%20UNI%20features.%20These%20results%20demonstrate%20the%20strong%20and%20consistent%20performance%20of%20DeltaMIL%20across%20diverse%20WSI%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19331v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeltaMIL%253A%2520Gated%2520Memory%2520Integration%2520for%2520Efficient%2520and%2520Discriminative%2520Whole%2520Slide%2520Image%2520Analysis%26entry.906535625%3DYueting%2520Zhu%2520and%2520Yuehao%2520Song%2520and%2520Shuai%2520Zhang%2520and%2520Wenyu%2520Liu%2520and%2520Xinggang%2520Wang%26entry.1292438233%3DWhole%2520Slide%2520Images%2520%2528WSIs%2529%2520are%2520typically%2520analyzed%2520using%2520multiple%2520instance%2520learning%2520%2528MIL%2529%2520methods.%2520However%252C%2520the%2520scale%2520and%2520heterogeneity%2520of%2520WSIs%2520generate%2520highly%2520redundant%2520and%2520dispersed%2520information%252C%2520making%2520it%2520difficult%2520to%2520identify%2520and%2520integrate%2520discriminative%2520signals.%2520Existing%2520MIL%2520methods%2520either%2520fail%2520to%2520discard%2520uninformative%2520cues%2520effectively%2520or%2520have%2520limited%2520ability%2520to%2520consolidate%2520relevant%2520features%2520from%2520multiple%2520patches%252C%2520which%2520restricts%2520their%2520performance%2520on%2520large%2520and%2520heterogeneous%2520WSIs.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520DeltaMIL%252C%2520a%2520novel%2520MIL%2520framework%2520that%2520explicitly%2520selects%2520semantically%2520relevant%2520regions%2520and%2520integrates%2520the%2520discriminative%2520information%2520from%2520WSIs.%2520Our%2520method%2520leverages%2520the%2520gated%2520delta%2520rule%2520to%2520efficiently%2520filter%2520and%2520integrate%2520information%2520through%2520a%2520block%2520combining%2520forgetting%2520and%2520memory%2520mechanisms.%2520The%2520delta%2520mechanism%2520dynamically%2520updates%2520the%2520memory%2520by%2520removing%2520old%2520values%2520and%2520inserting%2520new%2520ones%2520according%2520to%2520their%2520correlation%2520with%2520the%2520current%2520patch.%2520The%2520gating%2520mechanism%2520further%2520enables%2520rapid%2520forgetting%2520of%2520irrelevant%2520signals.%2520Additionally%252C%2520DeltaMIL%2520integrates%2520a%2520complementary%2520local%2520pattern%2520mixing%2520mechanism%2520to%2520retain%2520fine-grained%2520pathological%2520locality.%2520Our%2520design%2520enhances%2520the%2520extraction%2520of%2520meaningful%2520cues%2520and%2520suppresses%2520redundant%2520or%2520noisy%2520information%252C%2520which%2520improves%2520the%2520model%2527s%2520robustness%2520and%2520discriminative%2520power.%2520Experiments%2520demonstrate%2520that%2520DeltaMIL%2520achieves%2520state-of-the-art%2520performance.%2520Specifically%252C%2520for%2520survival%2520prediction%252C%2520DeltaMIL%2520improves%2520performance%2520by%25203.69%255C%2525%2520using%2520ResNet-50%2520features%2520and%25202.36%255C%2525%2520using%2520UNI%2520features.%2520For%2520slide-level%2520classification%252C%2520it%2520increases%2520accuracy%2520by%25203.09%255C%2525%2520with%2520ResNet-50%2520features%2520and%25203.75%255C%2525%2520with%2520UNI%2520features.%2520These%2520results%2520demonstrate%2520the%2520strong%2520and%2520consistent%2520performance%2520of%2520DeltaMIL%2520across%2520diverse%2520WSI%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19331v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeltaMIL%3A%20Gated%20Memory%20Integration%20for%20Efficient%20and%20Discriminative%20Whole%20Slide%20Image%20Analysis&entry.906535625=Yueting%20Zhu%20and%20Yuehao%20Song%20and%20Shuai%20Zhang%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang&entry.1292438233=Whole%20Slide%20Images%20%28WSIs%29%20are%20typically%20analyzed%20using%20multiple%20instance%20learning%20%28MIL%29%20methods.%20However%2C%20the%20scale%20and%20heterogeneity%20of%20WSIs%20generate%20highly%20redundant%20and%20dispersed%20information%2C%20making%20it%20difficult%20to%20identify%20and%20integrate%20discriminative%20signals.%20Existing%20MIL%20methods%20either%20fail%20to%20discard%20uninformative%20cues%20effectively%20or%20have%20limited%20ability%20to%20consolidate%20relevant%20features%20from%20multiple%20patches%2C%20which%20restricts%20their%20performance%20on%20large%20and%20heterogeneous%20WSIs.%20To%20address%20this%20issue%2C%20we%20propose%20DeltaMIL%2C%20a%20novel%20MIL%20framework%20that%20explicitly%20selects%20semantically%20relevant%20regions%20and%20integrates%20the%20discriminative%20information%20from%20WSIs.%20Our%20method%20leverages%20the%20gated%20delta%20rule%20to%20efficiently%20filter%20and%20integrate%20information%20through%20a%20block%20combining%20forgetting%20and%20memory%20mechanisms.%20The%20delta%20mechanism%20dynamically%20updates%20the%20memory%20by%20removing%20old%20values%20and%20inserting%20new%20ones%20according%20to%20their%20correlation%20with%20the%20current%20patch.%20The%20gating%20mechanism%20further%20enables%20rapid%20forgetting%20of%20irrelevant%20signals.%20Additionally%2C%20DeltaMIL%20integrates%20a%20complementary%20local%20pattern%20mixing%20mechanism%20to%20retain%20fine-grained%20pathological%20locality.%20Our%20design%20enhances%20the%20extraction%20of%20meaningful%20cues%20and%20suppresses%20redundant%20or%20noisy%20information%2C%20which%20improves%20the%20model%27s%20robustness%20and%20discriminative%20power.%20Experiments%20demonstrate%20that%20DeltaMIL%20achieves%20state-of-the-art%20performance.%20Specifically%2C%20for%20survival%20prediction%2C%20DeltaMIL%20improves%20performance%20by%203.69%5C%25%20using%20ResNet-50%20features%20and%202.36%5C%25%20using%20UNI%20features.%20For%20slide-level%20classification%2C%20it%20increases%20accuracy%20by%203.09%5C%25%20with%20ResNet-50%20features%20and%203.75%5C%25%20with%20UNI%20features.%20These%20results%20demonstrate%20the%20strong%20and%20consistent%20performance%20of%20DeltaMIL%20across%20diverse%20WSI%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2512.19331v1&entry.124074799=Read"},
{"title": "SafeMed-R1: Adversarial Reinforcement Learning for Generalizable and Robust Medical Reasoning in Vision-Language Models", "author": "A. A. Gde Yogi Pramana and Jason Ray and Anthony Jaya and Michael Wijaya", "abstract": "Vision--Language Models (VLMs) show significant promise for Medical Visual Question Answering (VQA), yet their deployment in clinical settings is hindered by severe vulnerability to adversarial attacks. Standard adversarial training, while effective for simpler tasks, often degrades both generalization performance and the quality of generated clinical reasoning. We introduce SafeMed-R1, a hybrid defense framework that ensures robust performance while preserving high-quality, interpretable medical reasoning. SafeMed-R1 employs a two-stage approach: at training time, we integrate Adversarial Training with Group Relative Policy Optimization (AT-GRPO) to explicitly robustify the reasoning process against worst-case perturbations; at inference time, we augment the model with Randomized Smoothing to provide certified $L_2$-norm robustness guarantees. We evaluate SafeMed-R1 on the OmniMedVQA benchmark across eight medical imaging modalities comprising over 88,000 samples. Our experiments reveal that standard fine-tuned VLMs, despite achieving 95\\% accuracy on clean inputs, collapse to approximately 25\\% under PGD attacks. In contrast, SafeMed-R1 maintains 84.45\\% accuracy under the same adversarial conditions, representing a 59 percentage point improvement in robustness. Furthermore, we demonstrate that models trained with explicit chain-of-thought reasoning exhibit superior adversarial robustness compared to instruction-only variants, suggesting a synergy between interpretability and security in medical AI systems.", "link": "http://arxiv.org/abs/2512.19317v1", "date": "2025-12-22", "relevancy": 2.085, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5534}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5148}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SafeMed-R1%3A%20Adversarial%20Reinforcement%20Learning%20for%20Generalizable%20and%20Robust%20Medical%20Reasoning%20in%20Vision-Language%20Models&body=Title%3A%20SafeMed-R1%3A%20Adversarial%20Reinforcement%20Learning%20for%20Generalizable%20and%20Robust%20Medical%20Reasoning%20in%20Vision-Language%20Models%0AAuthor%3A%20A.%20A.%20Gde%20Yogi%20Pramana%20and%20Jason%20Ray%20and%20Anthony%20Jaya%20and%20Michael%20Wijaya%0AAbstract%3A%20Vision--Language%20Models%20%28VLMs%29%20show%20significant%20promise%20for%20Medical%20Visual%20Question%20Answering%20%28VQA%29%2C%20yet%20their%20deployment%20in%20clinical%20settings%20is%20hindered%20by%20severe%20vulnerability%20to%20adversarial%20attacks.%20Standard%20adversarial%20training%2C%20while%20effective%20for%20simpler%20tasks%2C%20often%20degrades%20both%20generalization%20performance%20and%20the%20quality%20of%20generated%20clinical%20reasoning.%20We%20introduce%20SafeMed-R1%2C%20a%20hybrid%20defense%20framework%20that%20ensures%20robust%20performance%20while%20preserving%20high-quality%2C%20interpretable%20medical%20reasoning.%20SafeMed-R1%20employs%20a%20two-stage%20approach%3A%20at%20training%20time%2C%20we%20integrate%20Adversarial%20Training%20with%20Group%20Relative%20Policy%20Optimization%20%28AT-GRPO%29%20to%20explicitly%20robustify%20the%20reasoning%20process%20against%20worst-case%20perturbations%3B%20at%20inference%20time%2C%20we%20augment%20the%20model%20with%20Randomized%20Smoothing%20to%20provide%20certified%20%24L_2%24-norm%20robustness%20guarantees.%20We%20evaluate%20SafeMed-R1%20on%20the%20OmniMedVQA%20benchmark%20across%20eight%20medical%20imaging%20modalities%20comprising%20over%2088%2C000%20samples.%20Our%20experiments%20reveal%20that%20standard%20fine-tuned%20VLMs%2C%20despite%20achieving%2095%5C%25%20accuracy%20on%20clean%20inputs%2C%20collapse%20to%20approximately%2025%5C%25%20under%20PGD%20attacks.%20In%20contrast%2C%20SafeMed-R1%20maintains%2084.45%5C%25%20accuracy%20under%20the%20same%20adversarial%20conditions%2C%20representing%20a%2059%20percentage%20point%20improvement%20in%20robustness.%20Furthermore%2C%20we%20demonstrate%20that%20models%20trained%20with%20explicit%20chain-of-thought%20reasoning%20exhibit%20superior%20adversarial%20robustness%20compared%20to%20instruction-only%20variants%2C%20suggesting%20a%20synergy%20between%20interpretability%20and%20security%20in%20medical%20AI%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19317v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafeMed-R1%253A%2520Adversarial%2520Reinforcement%2520Learning%2520for%2520Generalizable%2520and%2520Robust%2520Medical%2520Reasoning%2520in%2520Vision-Language%2520Models%26entry.906535625%3DA.%2520A.%2520Gde%2520Yogi%2520Pramana%2520and%2520Jason%2520Ray%2520and%2520Anthony%2520Jaya%2520and%2520Michael%2520Wijaya%26entry.1292438233%3DVision--Language%2520Models%2520%2528VLMs%2529%2520show%2520significant%2520promise%2520for%2520Medical%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%252C%2520yet%2520their%2520deployment%2520in%2520clinical%2520settings%2520is%2520hindered%2520by%2520severe%2520vulnerability%2520to%2520adversarial%2520attacks.%2520Standard%2520adversarial%2520training%252C%2520while%2520effective%2520for%2520simpler%2520tasks%252C%2520often%2520degrades%2520both%2520generalization%2520performance%2520and%2520the%2520quality%2520of%2520generated%2520clinical%2520reasoning.%2520We%2520introduce%2520SafeMed-R1%252C%2520a%2520hybrid%2520defense%2520framework%2520that%2520ensures%2520robust%2520performance%2520while%2520preserving%2520high-quality%252C%2520interpretable%2520medical%2520reasoning.%2520SafeMed-R1%2520employs%2520a%2520two-stage%2520approach%253A%2520at%2520training%2520time%252C%2520we%2520integrate%2520Adversarial%2520Training%2520with%2520Group%2520Relative%2520Policy%2520Optimization%2520%2528AT-GRPO%2529%2520to%2520explicitly%2520robustify%2520the%2520reasoning%2520process%2520against%2520worst-case%2520perturbations%253B%2520at%2520inference%2520time%252C%2520we%2520augment%2520the%2520model%2520with%2520Randomized%2520Smoothing%2520to%2520provide%2520certified%2520%2524L_2%2524-norm%2520robustness%2520guarantees.%2520We%2520evaluate%2520SafeMed-R1%2520on%2520the%2520OmniMedVQA%2520benchmark%2520across%2520eight%2520medical%2520imaging%2520modalities%2520comprising%2520over%252088%252C000%2520samples.%2520Our%2520experiments%2520reveal%2520that%2520standard%2520fine-tuned%2520VLMs%252C%2520despite%2520achieving%252095%255C%2525%2520accuracy%2520on%2520clean%2520inputs%252C%2520collapse%2520to%2520approximately%252025%255C%2525%2520under%2520PGD%2520attacks.%2520In%2520contrast%252C%2520SafeMed-R1%2520maintains%252084.45%255C%2525%2520accuracy%2520under%2520the%2520same%2520adversarial%2520conditions%252C%2520representing%2520a%252059%2520percentage%2520point%2520improvement%2520in%2520robustness.%2520Furthermore%252C%2520we%2520demonstrate%2520that%2520models%2520trained%2520with%2520explicit%2520chain-of-thought%2520reasoning%2520exhibit%2520superior%2520adversarial%2520robustness%2520compared%2520to%2520instruction-only%2520variants%252C%2520suggesting%2520a%2520synergy%2520between%2520interpretability%2520and%2520security%2520in%2520medical%2520AI%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19317v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SafeMed-R1%3A%20Adversarial%20Reinforcement%20Learning%20for%20Generalizable%20and%20Robust%20Medical%20Reasoning%20in%20Vision-Language%20Models&entry.906535625=A.%20A.%20Gde%20Yogi%20Pramana%20and%20Jason%20Ray%20and%20Anthony%20Jaya%20and%20Michael%20Wijaya&entry.1292438233=Vision--Language%20Models%20%28VLMs%29%20show%20significant%20promise%20for%20Medical%20Visual%20Question%20Answering%20%28VQA%29%2C%20yet%20their%20deployment%20in%20clinical%20settings%20is%20hindered%20by%20severe%20vulnerability%20to%20adversarial%20attacks.%20Standard%20adversarial%20training%2C%20while%20effective%20for%20simpler%20tasks%2C%20often%20degrades%20both%20generalization%20performance%20and%20the%20quality%20of%20generated%20clinical%20reasoning.%20We%20introduce%20SafeMed-R1%2C%20a%20hybrid%20defense%20framework%20that%20ensures%20robust%20performance%20while%20preserving%20high-quality%2C%20interpretable%20medical%20reasoning.%20SafeMed-R1%20employs%20a%20two-stage%20approach%3A%20at%20training%20time%2C%20we%20integrate%20Adversarial%20Training%20with%20Group%20Relative%20Policy%20Optimization%20%28AT-GRPO%29%20to%20explicitly%20robustify%20the%20reasoning%20process%20against%20worst-case%20perturbations%3B%20at%20inference%20time%2C%20we%20augment%20the%20model%20with%20Randomized%20Smoothing%20to%20provide%20certified%20%24L_2%24-norm%20robustness%20guarantees.%20We%20evaluate%20SafeMed-R1%20on%20the%20OmniMedVQA%20benchmark%20across%20eight%20medical%20imaging%20modalities%20comprising%20over%2088%2C000%20samples.%20Our%20experiments%20reveal%20that%20standard%20fine-tuned%20VLMs%2C%20despite%20achieving%2095%5C%25%20accuracy%20on%20clean%20inputs%2C%20collapse%20to%20approximately%2025%5C%25%20under%20PGD%20attacks.%20In%20contrast%2C%20SafeMed-R1%20maintains%2084.45%5C%25%20accuracy%20under%20the%20same%20adversarial%20conditions%2C%20representing%20a%2059%20percentage%20point%20improvement%20in%20robustness.%20Furthermore%2C%20we%20demonstrate%20that%20models%20trained%20with%20explicit%20chain-of-thought%20reasoning%20exhibit%20superior%20adversarial%20robustness%20compared%20to%20instruction-only%20variants%2C%20suggesting%20a%20synergy%20between%20interpretability%20and%20security%20in%20medical%20AI%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2512.19317v1&entry.124074799=Read"},
{"title": "Geometric Learning of Canonical Parameterizations of $2D$-curves", "author": "Ioana Ciuclea and Giorgio Longari and Alice Barbara Tumpach", "abstract": "Most datasets encountered in computer vision and medical applications present symmetries that should be taken into account in classification tasks. A typical example is the symmetry by rotation and/or scaling in object detection. A common way to build neural networks that learn the symmetries is to use data augmentation. In order to avoid data augmentation and build more sustainable algorithms, we present an alternative method to mod out symmetries based on the notion of section of a principal fiber bundle. This framework allows the use of simple metrics on the space of objects in order to measure dissimilarities between orbits of objects under the symmetry group. Moreover, the section used can be optimized to maximize separation of classes. We illustrate this methodology on a dataset of contours of objects for the groups of translations, rotations, scalings and reparameterizations. In particular, we present a $2$-parameter family of canonical parameterizations of curves, containing the constant-speed parameterization as a special case, which we believe is interesting in its own right. We hope that this simple application will serve to convey the geometric concepts underlying this method, which have a wide range of possible applications. The code is available at the following link: $\\href{https://github.com/GiLonga/Geometric-Learning}{https://github.com/GiLonga/Geometric-Learning}$. A tutorial notebook showcasing an application of the code to a specific dataset is available at the following link: $\\href{https://github.com/ioanaciuclea/geometric-learning-notebook}{https://github.com/ioanaciuclea/geometric-learning-notebook}$", "link": "http://arxiv.org/abs/2509.26070v2", "date": "2025-12-22", "relevancy": 2.0739, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5225}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5184}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Geometric%20Learning%20of%20Canonical%20Parameterizations%20of%20%242D%24-curves&body=Title%3A%20Geometric%20Learning%20of%20Canonical%20Parameterizations%20of%20%242D%24-curves%0AAuthor%3A%20Ioana%20Ciuclea%20and%20Giorgio%20Longari%20and%20Alice%20Barbara%20Tumpach%0AAbstract%3A%20Most%20datasets%20encountered%20in%20computer%20vision%20and%20medical%20applications%20present%20symmetries%20that%20should%20be%20taken%20into%20account%20in%20classification%20tasks.%20A%20typical%20example%20is%20the%20symmetry%20by%20rotation%20and/or%20scaling%20in%20object%20detection.%20A%20common%20way%20to%20build%20neural%20networks%20that%20learn%20the%20symmetries%20is%20to%20use%20data%20augmentation.%20In%20order%20to%20avoid%20data%20augmentation%20and%20build%20more%20sustainable%20algorithms%2C%20we%20present%20an%20alternative%20method%20to%20mod%20out%20symmetries%20based%20on%20the%20notion%20of%20section%20of%20a%20principal%20fiber%20bundle.%20This%20framework%20allows%20the%20use%20of%20simple%20metrics%20on%20the%20space%20of%20objects%20in%20order%20to%20measure%20dissimilarities%20between%20orbits%20of%20objects%20under%20the%20symmetry%20group.%20Moreover%2C%20the%20section%20used%20can%20be%20optimized%20to%20maximize%20separation%20of%20classes.%20We%20illustrate%20this%20methodology%20on%20a%20dataset%20of%20contours%20of%20objects%20for%20the%20groups%20of%20translations%2C%20rotations%2C%20scalings%20and%20reparameterizations.%20In%20particular%2C%20we%20present%20a%20%242%24-parameter%20family%20of%20canonical%20parameterizations%20of%20curves%2C%20containing%20the%20constant-speed%20parameterization%20as%20a%20special%20case%2C%20which%20we%20believe%20is%20interesting%20in%20its%20own%20right.%20We%20hope%20that%20this%20simple%20application%20will%20serve%20to%20convey%20the%20geometric%20concepts%20underlying%20this%20method%2C%20which%20have%20a%20wide%20range%20of%20possible%20applications.%20The%20code%20is%20available%20at%20the%20following%20link%3A%20%24%5Chref%7Bhttps%3A//github.com/GiLonga/Geometric-Learning%7D%7Bhttps%3A//github.com/GiLonga/Geometric-Learning%7D%24.%20A%20tutorial%20notebook%20showcasing%20an%20application%20of%20the%20code%20to%20a%20specific%20dataset%20is%20available%20at%20the%20following%20link%3A%20%24%5Chref%7Bhttps%3A//github.com/ioanaciuclea/geometric-learning-notebook%7D%7Bhttps%3A//github.com/ioanaciuclea/geometric-learning-notebook%7D%24%0ALink%3A%20http%3A//arxiv.org/abs/2509.26070v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeometric%2520Learning%2520of%2520Canonical%2520Parameterizations%2520of%2520%25242D%2524-curves%26entry.906535625%3DIoana%2520Ciuclea%2520and%2520Giorgio%2520Longari%2520and%2520Alice%2520Barbara%2520Tumpach%26entry.1292438233%3DMost%2520datasets%2520encountered%2520in%2520computer%2520vision%2520and%2520medical%2520applications%2520present%2520symmetries%2520that%2520should%2520be%2520taken%2520into%2520account%2520in%2520classification%2520tasks.%2520A%2520typical%2520example%2520is%2520the%2520symmetry%2520by%2520rotation%2520and/or%2520scaling%2520in%2520object%2520detection.%2520A%2520common%2520way%2520to%2520build%2520neural%2520networks%2520that%2520learn%2520the%2520symmetries%2520is%2520to%2520use%2520data%2520augmentation.%2520In%2520order%2520to%2520avoid%2520data%2520augmentation%2520and%2520build%2520more%2520sustainable%2520algorithms%252C%2520we%2520present%2520an%2520alternative%2520method%2520to%2520mod%2520out%2520symmetries%2520based%2520on%2520the%2520notion%2520of%2520section%2520of%2520a%2520principal%2520fiber%2520bundle.%2520This%2520framework%2520allows%2520the%2520use%2520of%2520simple%2520metrics%2520on%2520the%2520space%2520of%2520objects%2520in%2520order%2520to%2520measure%2520dissimilarities%2520between%2520orbits%2520of%2520objects%2520under%2520the%2520symmetry%2520group.%2520Moreover%252C%2520the%2520section%2520used%2520can%2520be%2520optimized%2520to%2520maximize%2520separation%2520of%2520classes.%2520We%2520illustrate%2520this%2520methodology%2520on%2520a%2520dataset%2520of%2520contours%2520of%2520objects%2520for%2520the%2520groups%2520of%2520translations%252C%2520rotations%252C%2520scalings%2520and%2520reparameterizations.%2520In%2520particular%252C%2520we%2520present%2520a%2520%25242%2524-parameter%2520family%2520of%2520canonical%2520parameterizations%2520of%2520curves%252C%2520containing%2520the%2520constant-speed%2520parameterization%2520as%2520a%2520special%2520case%252C%2520which%2520we%2520believe%2520is%2520interesting%2520in%2520its%2520own%2520right.%2520We%2520hope%2520that%2520this%2520simple%2520application%2520will%2520serve%2520to%2520convey%2520the%2520geometric%2520concepts%2520underlying%2520this%2520method%252C%2520which%2520have%2520a%2520wide%2520range%2520of%2520possible%2520applications.%2520The%2520code%2520is%2520available%2520at%2520the%2520following%2520link%253A%2520%2524%255Chref%257Bhttps%253A//github.com/GiLonga/Geometric-Learning%257D%257Bhttps%253A//github.com/GiLonga/Geometric-Learning%257D%2524.%2520A%2520tutorial%2520notebook%2520showcasing%2520an%2520application%2520of%2520the%2520code%2520to%2520a%2520specific%2520dataset%2520is%2520available%2520at%2520the%2520following%2520link%253A%2520%2524%255Chref%257Bhttps%253A//github.com/ioanaciuclea/geometric-learning-notebook%257D%257Bhttps%253A//github.com/ioanaciuclea/geometric-learning-notebook%257D%2524%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.26070v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Geometric%20Learning%20of%20Canonical%20Parameterizations%20of%20%242D%24-curves&entry.906535625=Ioana%20Ciuclea%20and%20Giorgio%20Longari%20and%20Alice%20Barbara%20Tumpach&entry.1292438233=Most%20datasets%20encountered%20in%20computer%20vision%20and%20medical%20applications%20present%20symmetries%20that%20should%20be%20taken%20into%20account%20in%20classification%20tasks.%20A%20typical%20example%20is%20the%20symmetry%20by%20rotation%20and/or%20scaling%20in%20object%20detection.%20A%20common%20way%20to%20build%20neural%20networks%20that%20learn%20the%20symmetries%20is%20to%20use%20data%20augmentation.%20In%20order%20to%20avoid%20data%20augmentation%20and%20build%20more%20sustainable%20algorithms%2C%20we%20present%20an%20alternative%20method%20to%20mod%20out%20symmetries%20based%20on%20the%20notion%20of%20section%20of%20a%20principal%20fiber%20bundle.%20This%20framework%20allows%20the%20use%20of%20simple%20metrics%20on%20the%20space%20of%20objects%20in%20order%20to%20measure%20dissimilarities%20between%20orbits%20of%20objects%20under%20the%20symmetry%20group.%20Moreover%2C%20the%20section%20used%20can%20be%20optimized%20to%20maximize%20separation%20of%20classes.%20We%20illustrate%20this%20methodology%20on%20a%20dataset%20of%20contours%20of%20objects%20for%20the%20groups%20of%20translations%2C%20rotations%2C%20scalings%20and%20reparameterizations.%20In%20particular%2C%20we%20present%20a%20%242%24-parameter%20family%20of%20canonical%20parameterizations%20of%20curves%2C%20containing%20the%20constant-speed%20parameterization%20as%20a%20special%20case%2C%20which%20we%20believe%20is%20interesting%20in%20its%20own%20right.%20We%20hope%20that%20this%20simple%20application%20will%20serve%20to%20convey%20the%20geometric%20concepts%20underlying%20this%20method%2C%20which%20have%20a%20wide%20range%20of%20possible%20applications.%20The%20code%20is%20available%20at%20the%20following%20link%3A%20%24%5Chref%7Bhttps%3A//github.com/GiLonga/Geometric-Learning%7D%7Bhttps%3A//github.com/GiLonga/Geometric-Learning%7D%24.%20A%20tutorial%20notebook%20showcasing%20an%20application%20of%20the%20code%20to%20a%20specific%20dataset%20is%20available%20at%20the%20following%20link%3A%20%24%5Chref%7Bhttps%3A//github.com/ioanaciuclea/geometric-learning-notebook%7D%7Bhttps%3A//github.com/ioanaciuclea/geometric-learning-notebook%7D%24&entry.1838667208=http%3A//arxiv.org/abs/2509.26070v2&entry.124074799=Read"},
{"title": "GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks", "author": "Heng Zheng and Yuling Shi and Xiaodong Gu and Haochen You and Zijian Zhang and Lubin Gan and Hao Zhang and Wenjun Huang and Jin Huang", "abstract": "Visual geo-localization requires extensive geographic knowledge and sophisticated reasoning to determine image locations without GPS metadata. Traditional retrieval methods are constrained by database coverage and quality. Recent Large Vision-Language Models (LVLMs) enable direct location reasoning from image content, yet individual models struggle with diverse geographic regions and complex scenes. Existing multi-agent systems improve performance through model collaboration but treat all agent interactions uniformly. They lack mechanisms to handle conflicting predictions effectively. We propose \\textbf{GraphGeo}, a multi-agent debate framework using heterogeneous graph neural networks for visual geo-localization. Our approach models diverse debate relationships through typed edges, distinguishing supportive collaboration, competitive argumentation, and knowledge transfer. We introduce a dual-level debate mechanism combining node-level refinement and edge-level argumentation modeling. A cross-level topology refinement strategy enables co-evolution between graph structure and agent representations. Experiments on multiple benchmarks demonstrate GraphGeo significantly outperforms state-of-the-art methods. Our framework transforms cognitive conflicts between agents into enhanced geo-localization accuracy through structured debate.", "link": "http://arxiv.org/abs/2511.00908v2", "date": "2025-12-22", "relevancy": 2.0689, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5321}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5164}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GraphGeo%3A%20Multi-Agent%20Debate%20Framework%20for%20Visual%20Geo-localization%20with%20Heterogeneous%20Graph%20Neural%20Networks&body=Title%3A%20GraphGeo%3A%20Multi-Agent%20Debate%20Framework%20for%20Visual%20Geo-localization%20with%20Heterogeneous%20Graph%20Neural%20Networks%0AAuthor%3A%20Heng%20Zheng%20and%20Yuling%20Shi%20and%20Xiaodong%20Gu%20and%20Haochen%20You%20and%20Zijian%20Zhang%20and%20Lubin%20Gan%20and%20Hao%20Zhang%20and%20Wenjun%20Huang%20and%20Jin%20Huang%0AAbstract%3A%20Visual%20geo-localization%20requires%20extensive%20geographic%20knowledge%20and%20sophisticated%20reasoning%20to%20determine%20image%20locations%20without%20GPS%20metadata.%20Traditional%20retrieval%20methods%20are%20constrained%20by%20database%20coverage%20and%20quality.%20Recent%20Large%20Vision-Language%20Models%20%28LVLMs%29%20enable%20direct%20location%20reasoning%20from%20image%20content%2C%20yet%20individual%20models%20struggle%20with%20diverse%20geographic%20regions%20and%20complex%20scenes.%20Existing%20multi-agent%20systems%20improve%20performance%20through%20model%20collaboration%20but%20treat%20all%20agent%20interactions%20uniformly.%20They%20lack%20mechanisms%20to%20handle%20conflicting%20predictions%20effectively.%20We%20propose%20%5Ctextbf%7BGraphGeo%7D%2C%20a%20multi-agent%20debate%20framework%20using%20heterogeneous%20graph%20neural%20networks%20for%20visual%20geo-localization.%20Our%20approach%20models%20diverse%20debate%20relationships%20through%20typed%20edges%2C%20distinguishing%20supportive%20collaboration%2C%20competitive%20argumentation%2C%20and%20knowledge%20transfer.%20We%20introduce%20a%20dual-level%20debate%20mechanism%20combining%20node-level%20refinement%20and%20edge-level%20argumentation%20modeling.%20A%20cross-level%20topology%20refinement%20strategy%20enables%20co-evolution%20between%20graph%20structure%20and%20agent%20representations.%20Experiments%20on%20multiple%20benchmarks%20demonstrate%20GraphGeo%20significantly%20outperforms%20state-of-the-art%20methods.%20Our%20framework%20transforms%20cognitive%20conflicts%20between%20agents%20into%20enhanced%20geo-localization%20accuracy%20through%20structured%20debate.%0ALink%3A%20http%3A//arxiv.org/abs/2511.00908v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraphGeo%253A%2520Multi-Agent%2520Debate%2520Framework%2520for%2520Visual%2520Geo-localization%2520with%2520Heterogeneous%2520Graph%2520Neural%2520Networks%26entry.906535625%3DHeng%2520Zheng%2520and%2520Yuling%2520Shi%2520and%2520Xiaodong%2520Gu%2520and%2520Haochen%2520You%2520and%2520Zijian%2520Zhang%2520and%2520Lubin%2520Gan%2520and%2520Hao%2520Zhang%2520and%2520Wenjun%2520Huang%2520and%2520Jin%2520Huang%26entry.1292438233%3DVisual%2520geo-localization%2520requires%2520extensive%2520geographic%2520knowledge%2520and%2520sophisticated%2520reasoning%2520to%2520determine%2520image%2520locations%2520without%2520GPS%2520metadata.%2520Traditional%2520retrieval%2520methods%2520are%2520constrained%2520by%2520database%2520coverage%2520and%2520quality.%2520Recent%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520enable%2520direct%2520location%2520reasoning%2520from%2520image%2520content%252C%2520yet%2520individual%2520models%2520struggle%2520with%2520diverse%2520geographic%2520regions%2520and%2520complex%2520scenes.%2520Existing%2520multi-agent%2520systems%2520improve%2520performance%2520through%2520model%2520collaboration%2520but%2520treat%2520all%2520agent%2520interactions%2520uniformly.%2520They%2520lack%2520mechanisms%2520to%2520handle%2520conflicting%2520predictions%2520effectively.%2520We%2520propose%2520%255Ctextbf%257BGraphGeo%257D%252C%2520a%2520multi-agent%2520debate%2520framework%2520using%2520heterogeneous%2520graph%2520neural%2520networks%2520for%2520visual%2520geo-localization.%2520Our%2520approach%2520models%2520diverse%2520debate%2520relationships%2520through%2520typed%2520edges%252C%2520distinguishing%2520supportive%2520collaboration%252C%2520competitive%2520argumentation%252C%2520and%2520knowledge%2520transfer.%2520We%2520introduce%2520a%2520dual-level%2520debate%2520mechanism%2520combining%2520node-level%2520refinement%2520and%2520edge-level%2520argumentation%2520modeling.%2520A%2520cross-level%2520topology%2520refinement%2520strategy%2520enables%2520co-evolution%2520between%2520graph%2520structure%2520and%2520agent%2520representations.%2520Experiments%2520on%2520multiple%2520benchmarks%2520demonstrate%2520GraphGeo%2520significantly%2520outperforms%2520state-of-the-art%2520methods.%2520Our%2520framework%2520transforms%2520cognitive%2520conflicts%2520between%2520agents%2520into%2520enhanced%2520geo-localization%2520accuracy%2520through%2520structured%2520debate.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.00908v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GraphGeo%3A%20Multi-Agent%20Debate%20Framework%20for%20Visual%20Geo-localization%20with%20Heterogeneous%20Graph%20Neural%20Networks&entry.906535625=Heng%20Zheng%20and%20Yuling%20Shi%20and%20Xiaodong%20Gu%20and%20Haochen%20You%20and%20Zijian%20Zhang%20and%20Lubin%20Gan%20and%20Hao%20Zhang%20and%20Wenjun%20Huang%20and%20Jin%20Huang&entry.1292438233=Visual%20geo-localization%20requires%20extensive%20geographic%20knowledge%20and%20sophisticated%20reasoning%20to%20determine%20image%20locations%20without%20GPS%20metadata.%20Traditional%20retrieval%20methods%20are%20constrained%20by%20database%20coverage%20and%20quality.%20Recent%20Large%20Vision-Language%20Models%20%28LVLMs%29%20enable%20direct%20location%20reasoning%20from%20image%20content%2C%20yet%20individual%20models%20struggle%20with%20diverse%20geographic%20regions%20and%20complex%20scenes.%20Existing%20multi-agent%20systems%20improve%20performance%20through%20model%20collaboration%20but%20treat%20all%20agent%20interactions%20uniformly.%20They%20lack%20mechanisms%20to%20handle%20conflicting%20predictions%20effectively.%20We%20propose%20%5Ctextbf%7BGraphGeo%7D%2C%20a%20multi-agent%20debate%20framework%20using%20heterogeneous%20graph%20neural%20networks%20for%20visual%20geo-localization.%20Our%20approach%20models%20diverse%20debate%20relationships%20through%20typed%20edges%2C%20distinguishing%20supportive%20collaboration%2C%20competitive%20argumentation%2C%20and%20knowledge%20transfer.%20We%20introduce%20a%20dual-level%20debate%20mechanism%20combining%20node-level%20refinement%20and%20edge-level%20argumentation%20modeling.%20A%20cross-level%20topology%20refinement%20strategy%20enables%20co-evolution%20between%20graph%20structure%20and%20agent%20representations.%20Experiments%20on%20multiple%20benchmarks%20demonstrate%20GraphGeo%20significantly%20outperforms%20state-of-the-art%20methods.%20Our%20framework%20transforms%20cognitive%20conflicts%20between%20agents%20into%20enhanced%20geo-localization%20accuracy%20through%20structured%20debate.&entry.1838667208=http%3A//arxiv.org/abs/2511.00908v2&entry.124074799=Read"},
{"title": "Selective Phase-Aware Training of nnU-Net for Robust Breast Cancer Segmentation in Multi-Center DCE-MRI", "author": "Beyza Zayim and Aissiou Ikram and Boukhiar Naima", "abstract": "Breast cancer remains the most common cancer among women and is a leading cause of female mortality. Dynamic contrast-enhanced MRI (DCE-MRI) is a powerful imaging tool for evaluating breast tumors, yet the field lacks a standardized benchmark for analyzing treatment responses and guiding personalized care. We participated in the MAMA-MIA Challenge's Primary Tumor Segmentation task and this work presents a proposed selective, phase-aware training framework for the nnU-Net architecture, emphasizing quality-focused data selection to strengthen model robustness and generalization. We employed the No New Net (nnU-Net) framework with a selective training strategy that systematically analyzed the impact of image quality and center-specific variability on segmentation performance. Controlled experiments on the DUKE, NACT, ISPY1, and ISPY2 datasets revealed that including ISPY scans with motion artifacts and reduced contrast impaired segmentation performance, even with advanced preprocessing, such as contrast-limited adaptive histogram equalization (CLAHE). In contrast, training on DUKE and NACT data, which exhibited clearer contrast and fewer motion artifacts despite varying resolutions, with early phase images (0000-0002) provided more stable training conditions. Our results demonstrate the importance of phase-sensitive and quality-aware training strategies in achieving reliable segmentation performance in heterogeneous clinical datasets, highlighting the limitations of the expansion of naive datasets and motivating the need for future automation of quality-based data selection strategies.", "link": "http://arxiv.org/abs/2512.19225v1", "date": "2025-12-22", "relevancy": 2.0597, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.521}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5185}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Selective%20Phase-Aware%20Training%20of%20nnU-Net%20for%20Robust%20Breast%20Cancer%20Segmentation%20in%20Multi-Center%20DCE-MRI&body=Title%3A%20Selective%20Phase-Aware%20Training%20of%20nnU-Net%20for%20Robust%20Breast%20Cancer%20Segmentation%20in%20Multi-Center%20DCE-MRI%0AAuthor%3A%20Beyza%20Zayim%20and%20Aissiou%20Ikram%20and%20Boukhiar%20Naima%0AAbstract%3A%20Breast%20cancer%20remains%20the%20most%20common%20cancer%20among%20women%20and%20is%20a%20leading%20cause%20of%20female%20mortality.%20Dynamic%20contrast-enhanced%20MRI%20%28DCE-MRI%29%20is%20a%20powerful%20imaging%20tool%20for%20evaluating%20breast%20tumors%2C%20yet%20the%20field%20lacks%20a%20standardized%20benchmark%20for%20analyzing%20treatment%20responses%20and%20guiding%20personalized%20care.%20We%20participated%20in%20the%20MAMA-MIA%20Challenge%27s%20Primary%20Tumor%20Segmentation%20task%20and%20this%20work%20presents%20a%20proposed%20selective%2C%20phase-aware%20training%20framework%20for%20the%20nnU-Net%20architecture%2C%20emphasizing%20quality-focused%20data%20selection%20to%20strengthen%20model%20robustness%20and%20generalization.%20We%20employed%20the%20No%20New%20Net%20%28nnU-Net%29%20framework%20with%20a%20selective%20training%20strategy%20that%20systematically%20analyzed%20the%20impact%20of%20image%20quality%20and%20center-specific%20variability%20on%20segmentation%20performance.%20Controlled%20experiments%20on%20the%20DUKE%2C%20NACT%2C%20ISPY1%2C%20and%20ISPY2%20datasets%20revealed%20that%20including%20ISPY%20scans%20with%20motion%20artifacts%20and%20reduced%20contrast%20impaired%20segmentation%20performance%2C%20even%20with%20advanced%20preprocessing%2C%20such%20as%20contrast-limited%20adaptive%20histogram%20equalization%20%28CLAHE%29.%20In%20contrast%2C%20training%20on%20DUKE%20and%20NACT%20data%2C%20which%20exhibited%20clearer%20contrast%20and%20fewer%20motion%20artifacts%20despite%20varying%20resolutions%2C%20with%20early%20phase%20images%20%280000-0002%29%20provided%20more%20stable%20training%20conditions.%20Our%20results%20demonstrate%20the%20importance%20of%20phase-sensitive%20and%20quality-aware%20training%20strategies%20in%20achieving%20reliable%20segmentation%20performance%20in%20heterogeneous%20clinical%20datasets%2C%20highlighting%20the%20limitations%20of%20the%20expansion%20of%20naive%20datasets%20and%20motivating%20the%20need%20for%20future%20automation%20of%20quality-based%20data%20selection%20strategies.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19225v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelective%2520Phase-Aware%2520Training%2520of%2520nnU-Net%2520for%2520Robust%2520Breast%2520Cancer%2520Segmentation%2520in%2520Multi-Center%2520DCE-MRI%26entry.906535625%3DBeyza%2520Zayim%2520and%2520Aissiou%2520Ikram%2520and%2520Boukhiar%2520Naima%26entry.1292438233%3DBreast%2520cancer%2520remains%2520the%2520most%2520common%2520cancer%2520among%2520women%2520and%2520is%2520a%2520leading%2520cause%2520of%2520female%2520mortality.%2520Dynamic%2520contrast-enhanced%2520MRI%2520%2528DCE-MRI%2529%2520is%2520a%2520powerful%2520imaging%2520tool%2520for%2520evaluating%2520breast%2520tumors%252C%2520yet%2520the%2520field%2520lacks%2520a%2520standardized%2520benchmark%2520for%2520analyzing%2520treatment%2520responses%2520and%2520guiding%2520personalized%2520care.%2520We%2520participated%2520in%2520the%2520MAMA-MIA%2520Challenge%2527s%2520Primary%2520Tumor%2520Segmentation%2520task%2520and%2520this%2520work%2520presents%2520a%2520proposed%2520selective%252C%2520phase-aware%2520training%2520framework%2520for%2520the%2520nnU-Net%2520architecture%252C%2520emphasizing%2520quality-focused%2520data%2520selection%2520to%2520strengthen%2520model%2520robustness%2520and%2520generalization.%2520We%2520employed%2520the%2520No%2520New%2520Net%2520%2528nnU-Net%2529%2520framework%2520with%2520a%2520selective%2520training%2520strategy%2520that%2520systematically%2520analyzed%2520the%2520impact%2520of%2520image%2520quality%2520and%2520center-specific%2520variability%2520on%2520segmentation%2520performance.%2520Controlled%2520experiments%2520on%2520the%2520DUKE%252C%2520NACT%252C%2520ISPY1%252C%2520and%2520ISPY2%2520datasets%2520revealed%2520that%2520including%2520ISPY%2520scans%2520with%2520motion%2520artifacts%2520and%2520reduced%2520contrast%2520impaired%2520segmentation%2520performance%252C%2520even%2520with%2520advanced%2520preprocessing%252C%2520such%2520as%2520contrast-limited%2520adaptive%2520histogram%2520equalization%2520%2528CLAHE%2529.%2520In%2520contrast%252C%2520training%2520on%2520DUKE%2520and%2520NACT%2520data%252C%2520which%2520exhibited%2520clearer%2520contrast%2520and%2520fewer%2520motion%2520artifacts%2520despite%2520varying%2520resolutions%252C%2520with%2520early%2520phase%2520images%2520%25280000-0002%2529%2520provided%2520more%2520stable%2520training%2520conditions.%2520Our%2520results%2520demonstrate%2520the%2520importance%2520of%2520phase-sensitive%2520and%2520quality-aware%2520training%2520strategies%2520in%2520achieving%2520reliable%2520segmentation%2520performance%2520in%2520heterogeneous%2520clinical%2520datasets%252C%2520highlighting%2520the%2520limitations%2520of%2520the%2520expansion%2520of%2520naive%2520datasets%2520and%2520motivating%2520the%2520need%2520for%2520future%2520automation%2520of%2520quality-based%2520data%2520selection%2520strategies.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19225v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Selective%20Phase-Aware%20Training%20of%20nnU-Net%20for%20Robust%20Breast%20Cancer%20Segmentation%20in%20Multi-Center%20DCE-MRI&entry.906535625=Beyza%20Zayim%20and%20Aissiou%20Ikram%20and%20Boukhiar%20Naima&entry.1292438233=Breast%20cancer%20remains%20the%20most%20common%20cancer%20among%20women%20and%20is%20a%20leading%20cause%20of%20female%20mortality.%20Dynamic%20contrast-enhanced%20MRI%20%28DCE-MRI%29%20is%20a%20powerful%20imaging%20tool%20for%20evaluating%20breast%20tumors%2C%20yet%20the%20field%20lacks%20a%20standardized%20benchmark%20for%20analyzing%20treatment%20responses%20and%20guiding%20personalized%20care.%20We%20participated%20in%20the%20MAMA-MIA%20Challenge%27s%20Primary%20Tumor%20Segmentation%20task%20and%20this%20work%20presents%20a%20proposed%20selective%2C%20phase-aware%20training%20framework%20for%20the%20nnU-Net%20architecture%2C%20emphasizing%20quality-focused%20data%20selection%20to%20strengthen%20model%20robustness%20and%20generalization.%20We%20employed%20the%20No%20New%20Net%20%28nnU-Net%29%20framework%20with%20a%20selective%20training%20strategy%20that%20systematically%20analyzed%20the%20impact%20of%20image%20quality%20and%20center-specific%20variability%20on%20segmentation%20performance.%20Controlled%20experiments%20on%20the%20DUKE%2C%20NACT%2C%20ISPY1%2C%20and%20ISPY2%20datasets%20revealed%20that%20including%20ISPY%20scans%20with%20motion%20artifacts%20and%20reduced%20contrast%20impaired%20segmentation%20performance%2C%20even%20with%20advanced%20preprocessing%2C%20such%20as%20contrast-limited%20adaptive%20histogram%20equalization%20%28CLAHE%29.%20In%20contrast%2C%20training%20on%20DUKE%20and%20NACT%20data%2C%20which%20exhibited%20clearer%20contrast%20and%20fewer%20motion%20artifacts%20despite%20varying%20resolutions%2C%20with%20early%20phase%20images%20%280000-0002%29%20provided%20more%20stable%20training%20conditions.%20Our%20results%20demonstrate%20the%20importance%20of%20phase-sensitive%20and%20quality-aware%20training%20strategies%20in%20achieving%20reliable%20segmentation%20performance%20in%20heterogeneous%20clinical%20datasets%2C%20highlighting%20the%20limitations%20of%20the%20expansion%20of%20naive%20datasets%20and%20motivating%20the%20need%20for%20future%20automation%20of%20quality-based%20data%20selection%20strategies.&entry.1838667208=http%3A//arxiv.org/abs/2512.19225v1&entry.124074799=Read"},
{"title": "Deep Reinforcement Learning Optimization for Uncertain Nonlinear Systems via Event-Triggered Robust Adaptive Dynamic Programming", "author": "Ningwei Bai and Chi Pui Chan and Qichen Yin and Tengyang Gong and Yunda Yan and Zezhi Tang", "abstract": "This work proposes a unified control architecture that couples a Reinforcement Learning (RL)-driven controller with a disturbance-rejection Extended State Observer (ESO), complemented by an Event-Triggered Mechanism (ETM) to limit unnecessary computations. The ESO is utilized to estimate the system states and the lumped disturbance in real time, forming the foundation for effective disturbance compensation. To obtain near-optimal behavior without an accurate system description, a value-iteration-based Adaptive Dynamic Programming (ADP) method is adopted for policy approximation. The inclusion of the ETM ensures that parameter updates of the learning module are executed only when the state deviation surpasses a predefined bound, thereby preventing excessive learning activity and substantially reducing computational load. A Lyapunov-oriented analysis is used to characterize the stability properties of the resulting closed-loop system. Numerical experiments further confirm that the developed approach maintains strong control performance and disturbance tolerance, while achieving a significant reduction in sampling and processing effort compared with standard time-triggered ADP schemes.", "link": "http://arxiv.org/abs/2512.15735v2", "date": "2025-12-22", "relevancy": 2.0514, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5354}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5036}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.494}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Reinforcement%20Learning%20Optimization%20for%20Uncertain%20Nonlinear%20Systems%20via%20Event-Triggered%20Robust%20Adaptive%20Dynamic%20Programming&body=Title%3A%20Deep%20Reinforcement%20Learning%20Optimization%20for%20Uncertain%20Nonlinear%20Systems%20via%20Event-Triggered%20Robust%20Adaptive%20Dynamic%20Programming%0AAuthor%3A%20Ningwei%20Bai%20and%20Chi%20Pui%20Chan%20and%20Qichen%20Yin%20and%20Tengyang%20Gong%20and%20Yunda%20Yan%20and%20Zezhi%20Tang%0AAbstract%3A%20This%20work%20proposes%20a%20unified%20control%20architecture%20that%20couples%20a%20Reinforcement%20Learning%20%28RL%29-driven%20controller%20with%20a%20disturbance-rejection%20Extended%20State%20Observer%20%28ESO%29%2C%20complemented%20by%20an%20Event-Triggered%20Mechanism%20%28ETM%29%20to%20limit%20unnecessary%20computations.%20The%20ESO%20is%20utilized%20to%20estimate%20the%20system%20states%20and%20the%20lumped%20disturbance%20in%20real%20time%2C%20forming%20the%20foundation%20for%20effective%20disturbance%20compensation.%20To%20obtain%20near-optimal%20behavior%20without%20an%20accurate%20system%20description%2C%20a%20value-iteration-based%20Adaptive%20Dynamic%20Programming%20%28ADP%29%20method%20is%20adopted%20for%20policy%20approximation.%20The%20inclusion%20of%20the%20ETM%20ensures%20that%20parameter%20updates%20of%20the%20learning%20module%20are%20executed%20only%20when%20the%20state%20deviation%20surpasses%20a%20predefined%20bound%2C%20thereby%20preventing%20excessive%20learning%20activity%20and%20substantially%20reducing%20computational%20load.%20A%20Lyapunov-oriented%20analysis%20is%20used%20to%20characterize%20the%20stability%20properties%20of%20the%20resulting%20closed-loop%20system.%20Numerical%20experiments%20further%20confirm%20that%20the%20developed%20approach%20maintains%20strong%20control%20performance%20and%20disturbance%20tolerance%2C%20while%20achieving%20a%20significant%20reduction%20in%20sampling%20and%20processing%20effort%20compared%20with%20standard%20time-triggered%20ADP%20schemes.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15735v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Reinforcement%2520Learning%2520Optimization%2520for%2520Uncertain%2520Nonlinear%2520Systems%2520via%2520Event-Triggered%2520Robust%2520Adaptive%2520Dynamic%2520Programming%26entry.906535625%3DNingwei%2520Bai%2520and%2520Chi%2520Pui%2520Chan%2520and%2520Qichen%2520Yin%2520and%2520Tengyang%2520Gong%2520and%2520Yunda%2520Yan%2520and%2520Zezhi%2520Tang%26entry.1292438233%3DThis%2520work%2520proposes%2520a%2520unified%2520control%2520architecture%2520that%2520couples%2520a%2520Reinforcement%2520Learning%2520%2528RL%2529-driven%2520controller%2520with%2520a%2520disturbance-rejection%2520Extended%2520State%2520Observer%2520%2528ESO%2529%252C%2520complemented%2520by%2520an%2520Event-Triggered%2520Mechanism%2520%2528ETM%2529%2520to%2520limit%2520unnecessary%2520computations.%2520The%2520ESO%2520is%2520utilized%2520to%2520estimate%2520the%2520system%2520states%2520and%2520the%2520lumped%2520disturbance%2520in%2520real%2520time%252C%2520forming%2520the%2520foundation%2520for%2520effective%2520disturbance%2520compensation.%2520To%2520obtain%2520near-optimal%2520behavior%2520without%2520an%2520accurate%2520system%2520description%252C%2520a%2520value-iteration-based%2520Adaptive%2520Dynamic%2520Programming%2520%2528ADP%2529%2520method%2520is%2520adopted%2520for%2520policy%2520approximation.%2520The%2520inclusion%2520of%2520the%2520ETM%2520ensures%2520that%2520parameter%2520updates%2520of%2520the%2520learning%2520module%2520are%2520executed%2520only%2520when%2520the%2520state%2520deviation%2520surpasses%2520a%2520predefined%2520bound%252C%2520thereby%2520preventing%2520excessive%2520learning%2520activity%2520and%2520substantially%2520reducing%2520computational%2520load.%2520A%2520Lyapunov-oriented%2520analysis%2520is%2520used%2520to%2520characterize%2520the%2520stability%2520properties%2520of%2520the%2520resulting%2520closed-loop%2520system.%2520Numerical%2520experiments%2520further%2520confirm%2520that%2520the%2520developed%2520approach%2520maintains%2520strong%2520control%2520performance%2520and%2520disturbance%2520tolerance%252C%2520while%2520achieving%2520a%2520significant%2520reduction%2520in%2520sampling%2520and%2520processing%2520effort%2520compared%2520with%2520standard%2520time-triggered%2520ADP%2520schemes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15735v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Reinforcement%20Learning%20Optimization%20for%20Uncertain%20Nonlinear%20Systems%20via%20Event-Triggered%20Robust%20Adaptive%20Dynamic%20Programming&entry.906535625=Ningwei%20Bai%20and%20Chi%20Pui%20Chan%20and%20Qichen%20Yin%20and%20Tengyang%20Gong%20and%20Yunda%20Yan%20and%20Zezhi%20Tang&entry.1292438233=This%20work%20proposes%20a%20unified%20control%20architecture%20that%20couples%20a%20Reinforcement%20Learning%20%28RL%29-driven%20controller%20with%20a%20disturbance-rejection%20Extended%20State%20Observer%20%28ESO%29%2C%20complemented%20by%20an%20Event-Triggered%20Mechanism%20%28ETM%29%20to%20limit%20unnecessary%20computations.%20The%20ESO%20is%20utilized%20to%20estimate%20the%20system%20states%20and%20the%20lumped%20disturbance%20in%20real%20time%2C%20forming%20the%20foundation%20for%20effective%20disturbance%20compensation.%20To%20obtain%20near-optimal%20behavior%20without%20an%20accurate%20system%20description%2C%20a%20value-iteration-based%20Adaptive%20Dynamic%20Programming%20%28ADP%29%20method%20is%20adopted%20for%20policy%20approximation.%20The%20inclusion%20of%20the%20ETM%20ensures%20that%20parameter%20updates%20of%20the%20learning%20module%20are%20executed%20only%20when%20the%20state%20deviation%20surpasses%20a%20predefined%20bound%2C%20thereby%20preventing%20excessive%20learning%20activity%20and%20substantially%20reducing%20computational%20load.%20A%20Lyapunov-oriented%20analysis%20is%20used%20to%20characterize%20the%20stability%20properties%20of%20the%20resulting%20closed-loop%20system.%20Numerical%20experiments%20further%20confirm%20that%20the%20developed%20approach%20maintains%20strong%20control%20performance%20and%20disturbance%20tolerance%2C%20while%20achieving%20a%20significant%20reduction%20in%20sampling%20and%20processing%20effort%20compared%20with%20standard%20time-triggered%20ADP%20schemes.&entry.1838667208=http%3A//arxiv.org/abs/2512.15735v2&entry.124074799=Read"},
{"title": "MaP-AVR: A Meta-Action Planner for Agents Leveraging Vision Language Models and Retrieval-Augmented Generation", "author": "Zhenglong Guo and Yiming Zhao and Feng Jiang and Heng Jin and Zongbao Feng and Jianbin Zhou and Siyuan Xu", "abstract": "Embodied robotic AI systems designed to manage complex daily tasks rely on a task planner to understand and decompose high-level tasks. While most research focuses on enhancing the task-understanding abilities of LLMs/VLMs through fine-tuning or chain-of-thought prompting, this paper argues that defining the planned skill set is equally crucial. To handle the complexity of daily environments, the skill set should possess a high degree of generalization ability. Empirically, more abstract expressions tend to be more generalizable. Therefore, we propose to abstract the planned result as a set of meta-actions. Each meta-action comprises three components: {move/rotate, end-effector status change, relationship with the environment}. This abstraction replaces human-centric concepts, such as grasping or pushing, with the robot's intrinsic functionalities. As a result, the planned outcomes align seamlessly with the complete range of actions that the robot is capable of performing. Furthermore, to ensure that the LLM/VLM accurately produces the desired meta-action format, we employ the Retrieval-Augmented Generation (RAG) technique, which leverages a database of human-annotated planning demonstrations to facilitate in-context learning. As the system successfully completes more tasks, the database will self-augment to continue supporting diversity. The meta-action set and its integration with RAG are two novel contributions of our planner, denoted as MaP-AVR, the meta-action planner for agents composed of VLM and RAG. To validate its efficacy, we design experiments using GPT-4o as the pre-trained LLM/VLM model and OmniGibson as our robotic platform. Our approach demonstrates promising performance compared to the current state-of-the-art method. Project page: https://map-avr.github.io/.", "link": "http://arxiv.org/abs/2512.19453v1", "date": "2025-12-22", "relevancy": 1.7307, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6419}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5838}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5481}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MaP-AVR%3A%20A%20Meta-Action%20Planner%20for%20Agents%20Leveraging%20Vision%20Language%20Models%20and%20Retrieval-Augmented%20Generation&body=Title%3A%20MaP-AVR%3A%20A%20Meta-Action%20Planner%20for%20Agents%20Leveraging%20Vision%20Language%20Models%20and%20Retrieval-Augmented%20Generation%0AAuthor%3A%20Zhenglong%20Guo%20and%20Yiming%20Zhao%20and%20Feng%20Jiang%20and%20Heng%20Jin%20and%20Zongbao%20Feng%20and%20Jianbin%20Zhou%20and%20Siyuan%20Xu%0AAbstract%3A%20Embodied%20robotic%20AI%20systems%20designed%20to%20manage%20complex%20daily%20tasks%20rely%20on%20a%20task%20planner%20to%20understand%20and%20decompose%20high-level%20tasks.%20While%20most%20research%20focuses%20on%20enhancing%20the%20task-understanding%20abilities%20of%20LLMs/VLMs%20through%20fine-tuning%20or%20chain-of-thought%20prompting%2C%20this%20paper%20argues%20that%20defining%20the%20planned%20skill%20set%20is%20equally%20crucial.%20To%20handle%20the%20complexity%20of%20daily%20environments%2C%20the%20skill%20set%20should%20possess%20a%20high%20degree%20of%20generalization%20ability.%20Empirically%2C%20more%20abstract%20expressions%20tend%20to%20be%20more%20generalizable.%20Therefore%2C%20we%20propose%20to%20abstract%20the%20planned%20result%20as%20a%20set%20of%20meta-actions.%20Each%20meta-action%20comprises%20three%20components%3A%20%7Bmove/rotate%2C%20end-effector%20status%20change%2C%20relationship%20with%20the%20environment%7D.%20This%20abstraction%20replaces%20human-centric%20concepts%2C%20such%20as%20grasping%20or%20pushing%2C%20with%20the%20robot%27s%20intrinsic%20functionalities.%20As%20a%20result%2C%20the%20planned%20outcomes%20align%20seamlessly%20with%20the%20complete%20range%20of%20actions%20that%20the%20robot%20is%20capable%20of%20performing.%20Furthermore%2C%20to%20ensure%20that%20the%20LLM/VLM%20accurately%20produces%20the%20desired%20meta-action%20format%2C%20we%20employ%20the%20Retrieval-Augmented%20Generation%20%28RAG%29%20technique%2C%20which%20leverages%20a%20database%20of%20human-annotated%20planning%20demonstrations%20to%20facilitate%20in-context%20learning.%20As%20the%20system%20successfully%20completes%20more%20tasks%2C%20the%20database%20will%20self-augment%20to%20continue%20supporting%20diversity.%20The%20meta-action%20set%20and%20its%20integration%20with%20RAG%20are%20two%20novel%20contributions%20of%20our%20planner%2C%20denoted%20as%20MaP-AVR%2C%20the%20meta-action%20planner%20for%20agents%20composed%20of%20VLM%20and%20RAG.%20To%20validate%20its%20efficacy%2C%20we%20design%20experiments%20using%20GPT-4o%20as%20the%20pre-trained%20LLM/VLM%20model%20and%20OmniGibson%20as%20our%20robotic%20platform.%20Our%20approach%20demonstrates%20promising%20performance%20compared%20to%20the%20current%20state-of-the-art%20method.%20Project%20page%3A%20https%3A//map-avr.github.io/.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19453v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMaP-AVR%253A%2520A%2520Meta-Action%2520Planner%2520for%2520Agents%2520Leveraging%2520Vision%2520Language%2520Models%2520and%2520Retrieval-Augmented%2520Generation%26entry.906535625%3DZhenglong%2520Guo%2520and%2520Yiming%2520Zhao%2520and%2520Feng%2520Jiang%2520and%2520Heng%2520Jin%2520and%2520Zongbao%2520Feng%2520and%2520Jianbin%2520Zhou%2520and%2520Siyuan%2520Xu%26entry.1292438233%3DEmbodied%2520robotic%2520AI%2520systems%2520designed%2520to%2520manage%2520complex%2520daily%2520tasks%2520rely%2520on%2520a%2520task%2520planner%2520to%2520understand%2520and%2520decompose%2520high-level%2520tasks.%2520While%2520most%2520research%2520focuses%2520on%2520enhancing%2520the%2520task-understanding%2520abilities%2520of%2520LLMs/VLMs%2520through%2520fine-tuning%2520or%2520chain-of-thought%2520prompting%252C%2520this%2520paper%2520argues%2520that%2520defining%2520the%2520planned%2520skill%2520set%2520is%2520equally%2520crucial.%2520To%2520handle%2520the%2520complexity%2520of%2520daily%2520environments%252C%2520the%2520skill%2520set%2520should%2520possess%2520a%2520high%2520degree%2520of%2520generalization%2520ability.%2520Empirically%252C%2520more%2520abstract%2520expressions%2520tend%2520to%2520be%2520more%2520generalizable.%2520Therefore%252C%2520we%2520propose%2520to%2520abstract%2520the%2520planned%2520result%2520as%2520a%2520set%2520of%2520meta-actions.%2520Each%2520meta-action%2520comprises%2520three%2520components%253A%2520%257Bmove/rotate%252C%2520end-effector%2520status%2520change%252C%2520relationship%2520with%2520the%2520environment%257D.%2520This%2520abstraction%2520replaces%2520human-centric%2520concepts%252C%2520such%2520as%2520grasping%2520or%2520pushing%252C%2520with%2520the%2520robot%2527s%2520intrinsic%2520functionalities.%2520As%2520a%2520result%252C%2520the%2520planned%2520outcomes%2520align%2520seamlessly%2520with%2520the%2520complete%2520range%2520of%2520actions%2520that%2520the%2520robot%2520is%2520capable%2520of%2520performing.%2520Furthermore%252C%2520to%2520ensure%2520that%2520the%2520LLM/VLM%2520accurately%2520produces%2520the%2520desired%2520meta-action%2520format%252C%2520we%2520employ%2520the%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520technique%252C%2520which%2520leverages%2520a%2520database%2520of%2520human-annotated%2520planning%2520demonstrations%2520to%2520facilitate%2520in-context%2520learning.%2520As%2520the%2520system%2520successfully%2520completes%2520more%2520tasks%252C%2520the%2520database%2520will%2520self-augment%2520to%2520continue%2520supporting%2520diversity.%2520The%2520meta-action%2520set%2520and%2520its%2520integration%2520with%2520RAG%2520are%2520two%2520novel%2520contributions%2520of%2520our%2520planner%252C%2520denoted%2520as%2520MaP-AVR%252C%2520the%2520meta-action%2520planner%2520for%2520agents%2520composed%2520of%2520VLM%2520and%2520RAG.%2520To%2520validate%2520its%2520efficacy%252C%2520we%2520design%2520experiments%2520using%2520GPT-4o%2520as%2520the%2520pre-trained%2520LLM/VLM%2520model%2520and%2520OmniGibson%2520as%2520our%2520robotic%2520platform.%2520Our%2520approach%2520demonstrates%2520promising%2520performance%2520compared%2520to%2520the%2520current%2520state-of-the-art%2520method.%2520Project%2520page%253A%2520https%253A//map-avr.github.io/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19453v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MaP-AVR%3A%20A%20Meta-Action%20Planner%20for%20Agents%20Leveraging%20Vision%20Language%20Models%20and%20Retrieval-Augmented%20Generation&entry.906535625=Zhenglong%20Guo%20and%20Yiming%20Zhao%20and%20Feng%20Jiang%20and%20Heng%20Jin%20and%20Zongbao%20Feng%20and%20Jianbin%20Zhou%20and%20Siyuan%20Xu&entry.1292438233=Embodied%20robotic%20AI%20systems%20designed%20to%20manage%20complex%20daily%20tasks%20rely%20on%20a%20task%20planner%20to%20understand%20and%20decompose%20high-level%20tasks.%20While%20most%20research%20focuses%20on%20enhancing%20the%20task-understanding%20abilities%20of%20LLMs/VLMs%20through%20fine-tuning%20or%20chain-of-thought%20prompting%2C%20this%20paper%20argues%20that%20defining%20the%20planned%20skill%20set%20is%20equally%20crucial.%20To%20handle%20the%20complexity%20of%20daily%20environments%2C%20the%20skill%20set%20should%20possess%20a%20high%20degree%20of%20generalization%20ability.%20Empirically%2C%20more%20abstract%20expressions%20tend%20to%20be%20more%20generalizable.%20Therefore%2C%20we%20propose%20to%20abstract%20the%20planned%20result%20as%20a%20set%20of%20meta-actions.%20Each%20meta-action%20comprises%20three%20components%3A%20%7Bmove/rotate%2C%20end-effector%20status%20change%2C%20relationship%20with%20the%20environment%7D.%20This%20abstraction%20replaces%20human-centric%20concepts%2C%20such%20as%20grasping%20or%20pushing%2C%20with%20the%20robot%27s%20intrinsic%20functionalities.%20As%20a%20result%2C%20the%20planned%20outcomes%20align%20seamlessly%20with%20the%20complete%20range%20of%20actions%20that%20the%20robot%20is%20capable%20of%20performing.%20Furthermore%2C%20to%20ensure%20that%20the%20LLM/VLM%20accurately%20produces%20the%20desired%20meta-action%20format%2C%20we%20employ%20the%20Retrieval-Augmented%20Generation%20%28RAG%29%20technique%2C%20which%20leverages%20a%20database%20of%20human-annotated%20planning%20demonstrations%20to%20facilitate%20in-context%20learning.%20As%20the%20system%20successfully%20completes%20more%20tasks%2C%20the%20database%20will%20self-augment%20to%20continue%20supporting%20diversity.%20The%20meta-action%20set%20and%20its%20integration%20with%20RAG%20are%20two%20novel%20contributions%20of%20our%20planner%2C%20denoted%20as%20MaP-AVR%2C%20the%20meta-action%20planner%20for%20agents%20composed%20of%20VLM%20and%20RAG.%20To%20validate%20its%20efficacy%2C%20we%20design%20experiments%20using%20GPT-4o%20as%20the%20pre-trained%20LLM/VLM%20model%20and%20OmniGibson%20as%20our%20robotic%20platform.%20Our%20approach%20demonstrates%20promising%20performance%20compared%20to%20the%20current%20state-of-the-art%20method.%20Project%20page%3A%20https%3A//map-avr.github.io/.&entry.1838667208=http%3A//arxiv.org/abs/2512.19453v1&entry.124074799=Read"},
{"title": "Bridging the Gap Between Scientific Laws Derived by AI Systems and Canonical Knowledge via Abductive Inference with AI-Noether", "author": "Karan Srivastava and Sanjeeb Dash and Ryan Cory-Wright and Barry Trager and Cristina Cornelio and Lior Horesh", "abstract": "Advances in AI have shown great potential in contributing to the acceleration of scientific discovery. Symbolic regression can fit interpretable models to data, but these models are not necessarily derivable from established theory. Recent systems (e.g., AI-Descartes, AI-Hilbert) enforce derivability from prior knowledge. However, when existing theories are incomplete or incorrect, these machine-generated hypotheses may fall outside the theoretical scope. Automatically finding corrections to axiom systems to close this gap remains a central challenge in scientific discovery. We propose a solution: an open-source algebraic geometry-based system that, given an incomplete axiom system expressible as polynomials and a hypothesis that the axioms cannot derive, generates a minimal set of candidate axioms that, when added to the theory, provably derive the (possibly noisy) hypothesis. We illustrate the efficacy of our approach by showing that it can reconstruct key axioms required to derive the carrier-resolved photo-Hall effect, Einstein's relativistic laws, and several other laws.", "link": "http://arxiv.org/abs/2509.23004v2", "date": "2025-12-22", "relevancy": 1.7062, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.457}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4287}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4123}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20the%20Gap%20Between%20Scientific%20Laws%20Derived%20by%20AI%20Systems%20and%20Canonical%20Knowledge%20via%20Abductive%20Inference%20with%20AI-Noether&body=Title%3A%20Bridging%20the%20Gap%20Between%20Scientific%20Laws%20Derived%20by%20AI%20Systems%20and%20Canonical%20Knowledge%20via%20Abductive%20Inference%20with%20AI-Noether%0AAuthor%3A%20Karan%20Srivastava%20and%20Sanjeeb%20Dash%20and%20Ryan%20Cory-Wright%20and%20Barry%20Trager%20and%20Cristina%20Cornelio%20and%20Lior%20Horesh%0AAbstract%3A%20Advances%20in%20AI%20have%20shown%20great%20potential%20in%20contributing%20to%20the%20acceleration%20of%20scientific%20discovery.%20Symbolic%20regression%20can%20fit%20interpretable%20models%20to%20data%2C%20but%20these%20models%20are%20not%20necessarily%20derivable%20from%20established%20theory.%20Recent%20systems%20%28e.g.%2C%20AI-Descartes%2C%20AI-Hilbert%29%20enforce%20derivability%20from%20prior%20knowledge.%20However%2C%20when%20existing%20theories%20are%20incomplete%20or%20incorrect%2C%20these%20machine-generated%20hypotheses%20may%20fall%20outside%20the%20theoretical%20scope.%20Automatically%20finding%20corrections%20to%20axiom%20systems%20to%20close%20this%20gap%20remains%20a%20central%20challenge%20in%20scientific%20discovery.%20We%20propose%20a%20solution%3A%20an%20open-source%20algebraic%20geometry-based%20system%20that%2C%20given%20an%20incomplete%20axiom%20system%20expressible%20as%20polynomials%20and%20a%20hypothesis%20that%20the%20axioms%20cannot%20derive%2C%20generates%20a%20minimal%20set%20of%20candidate%20axioms%20that%2C%20when%20added%20to%20the%20theory%2C%20provably%20derive%20the%20%28possibly%20noisy%29%20hypothesis.%20We%20illustrate%20the%20efficacy%20of%20our%20approach%20by%20showing%20that%20it%20can%20reconstruct%20key%20axioms%20required%20to%20derive%20the%20carrier-resolved%20photo-Hall%20effect%2C%20Einstein%27s%20relativistic%20laws%2C%20and%20several%20other%20laws.%0ALink%3A%20http%3A//arxiv.org/abs/2509.23004v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520the%2520Gap%2520Between%2520Scientific%2520Laws%2520Derived%2520by%2520AI%2520Systems%2520and%2520Canonical%2520Knowledge%2520via%2520Abductive%2520Inference%2520with%2520AI-Noether%26entry.906535625%3DKaran%2520Srivastava%2520and%2520Sanjeeb%2520Dash%2520and%2520Ryan%2520Cory-Wright%2520and%2520Barry%2520Trager%2520and%2520Cristina%2520Cornelio%2520and%2520Lior%2520Horesh%26entry.1292438233%3DAdvances%2520in%2520AI%2520have%2520shown%2520great%2520potential%2520in%2520contributing%2520to%2520the%2520acceleration%2520of%2520scientific%2520discovery.%2520Symbolic%2520regression%2520can%2520fit%2520interpretable%2520models%2520to%2520data%252C%2520but%2520these%2520models%2520are%2520not%2520necessarily%2520derivable%2520from%2520established%2520theory.%2520Recent%2520systems%2520%2528e.g.%252C%2520AI-Descartes%252C%2520AI-Hilbert%2529%2520enforce%2520derivability%2520from%2520prior%2520knowledge.%2520However%252C%2520when%2520existing%2520theories%2520are%2520incomplete%2520or%2520incorrect%252C%2520these%2520machine-generated%2520hypotheses%2520may%2520fall%2520outside%2520the%2520theoretical%2520scope.%2520Automatically%2520finding%2520corrections%2520to%2520axiom%2520systems%2520to%2520close%2520this%2520gap%2520remains%2520a%2520central%2520challenge%2520in%2520scientific%2520discovery.%2520We%2520propose%2520a%2520solution%253A%2520an%2520open-source%2520algebraic%2520geometry-based%2520system%2520that%252C%2520given%2520an%2520incomplete%2520axiom%2520system%2520expressible%2520as%2520polynomials%2520and%2520a%2520hypothesis%2520that%2520the%2520axioms%2520cannot%2520derive%252C%2520generates%2520a%2520minimal%2520set%2520of%2520candidate%2520axioms%2520that%252C%2520when%2520added%2520to%2520the%2520theory%252C%2520provably%2520derive%2520the%2520%2528possibly%2520noisy%2529%2520hypothesis.%2520We%2520illustrate%2520the%2520efficacy%2520of%2520our%2520approach%2520by%2520showing%2520that%2520it%2520can%2520reconstruct%2520key%2520axioms%2520required%2520to%2520derive%2520the%2520carrier-resolved%2520photo-Hall%2520effect%252C%2520Einstein%2527s%2520relativistic%2520laws%252C%2520and%2520several%2520other%2520laws.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.23004v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20the%20Gap%20Between%20Scientific%20Laws%20Derived%20by%20AI%20Systems%20and%20Canonical%20Knowledge%20via%20Abductive%20Inference%20with%20AI-Noether&entry.906535625=Karan%20Srivastava%20and%20Sanjeeb%20Dash%20and%20Ryan%20Cory-Wright%20and%20Barry%20Trager%20and%20Cristina%20Cornelio%20and%20Lior%20Horesh&entry.1292438233=Advances%20in%20AI%20have%20shown%20great%20potential%20in%20contributing%20to%20the%20acceleration%20of%20scientific%20discovery.%20Symbolic%20regression%20can%20fit%20interpretable%20models%20to%20data%2C%20but%20these%20models%20are%20not%20necessarily%20derivable%20from%20established%20theory.%20Recent%20systems%20%28e.g.%2C%20AI-Descartes%2C%20AI-Hilbert%29%20enforce%20derivability%20from%20prior%20knowledge.%20However%2C%20when%20existing%20theories%20are%20incomplete%20or%20incorrect%2C%20these%20machine-generated%20hypotheses%20may%20fall%20outside%20the%20theoretical%20scope.%20Automatically%20finding%20corrections%20to%20axiom%20systems%20to%20close%20this%20gap%20remains%20a%20central%20challenge%20in%20scientific%20discovery.%20We%20propose%20a%20solution%3A%20an%20open-source%20algebraic%20geometry-based%20system%20that%2C%20given%20an%20incomplete%20axiom%20system%20expressible%20as%20polynomials%20and%20a%20hypothesis%20that%20the%20axioms%20cannot%20derive%2C%20generates%20a%20minimal%20set%20of%20candidate%20axioms%20that%2C%20when%20added%20to%20the%20theory%2C%20provably%20derive%20the%20%28possibly%20noisy%29%20hypothesis.%20We%20illustrate%20the%20efficacy%20of%20our%20approach%20by%20showing%20that%20it%20can%20reconstruct%20key%20axioms%20required%20to%20derive%20the%20carrier-resolved%20photo-Hall%20effect%2C%20Einstein%27s%20relativistic%20laws%2C%20and%20several%20other%20laws.&entry.1838667208=http%3A//arxiv.org/abs/2509.23004v2&entry.124074799=Read"},
{"title": "Learning General Policies with Policy Gradient Methods", "author": "Simon St\u00e5hlberg and Blai Bonet and Hector Geffner", "abstract": "While reinforcement learning methods have delivered remarkable results in a number of settings, generalization, i.e., the ability to produce policies that generalize in a reliable and systematic way, has remained a challenge. The problem of generalization has been addressed formally in classical planning where provable correct policies that generalize over all instances of a given domain have been learned using combinatorial methods. The aim of this work is to bring these two research threads together to illuminate the conditions under which (deep) reinforcement learning approaches, and in particular, policy optimization methods, can be used to learn policies that generalize like combinatorial methods do. We draw on lessons learned from previous combinatorial and deep learning approaches, and extend them in a convenient way. From the former, we model policies as state transition classifiers, as (ground) actions are not general and change from instance to instance. From the latter, we use graph neural networks (GNNs) adapted to deal with relational structures for representing value functions over planning states, and in our case, policies. With these ingredients in place, we find that actor-critic methods can be used to learn policies that generalize almost as well as those obtained using combinatorial approaches while avoiding the scalability bottleneck and the use of feature pools. Moreover, the limitations of the DRL methods on the benchmarks considered have little to do with deep learning or reinforcement learning algorithms, and result from the well-understood expressive limitations of GNNs, and the tradeoff between optimality and generalization (general policies cannot be optimal in some domains). Both of these limitations are addressed without changing the basic DRL methods by adding derived predicates and an alternative cost structure to optimize.", "link": "http://arxiv.org/abs/2512.19366v1", "date": "2025-12-22", "relevancy": 1.4307, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4831}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.47}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20General%20Policies%20with%20Policy%20Gradient%20Methods&body=Title%3A%20Learning%20General%20Policies%20with%20Policy%20Gradient%20Methods%0AAuthor%3A%20Simon%20St%C3%A5hlberg%20and%20Blai%20Bonet%20and%20Hector%20Geffner%0AAbstract%3A%20While%20reinforcement%20learning%20methods%20have%20delivered%20remarkable%20results%20in%20a%20number%20of%20settings%2C%20generalization%2C%20i.e.%2C%20the%20ability%20to%20produce%20policies%20that%20generalize%20in%20a%20reliable%20and%20systematic%20way%2C%20has%20remained%20a%20challenge.%20The%20problem%20of%20generalization%20has%20been%20addressed%20formally%20in%20classical%20planning%20where%20provable%20correct%20policies%20that%20generalize%20over%20all%20instances%20of%20a%20given%20domain%20have%20been%20learned%20using%20combinatorial%20methods.%20The%20aim%20of%20this%20work%20is%20to%20bring%20these%20two%20research%20threads%20together%20to%20illuminate%20the%20conditions%20under%20which%20%28deep%29%20reinforcement%20learning%20approaches%2C%20and%20in%20particular%2C%20policy%20optimization%20methods%2C%20can%20be%20used%20to%20learn%20policies%20that%20generalize%20like%20combinatorial%20methods%20do.%20We%20draw%20on%20lessons%20learned%20from%20previous%20combinatorial%20and%20deep%20learning%20approaches%2C%20and%20extend%20them%20in%20a%20convenient%20way.%20From%20the%20former%2C%20we%20model%20policies%20as%20state%20transition%20classifiers%2C%20as%20%28ground%29%20actions%20are%20not%20general%20and%20change%20from%20instance%20to%20instance.%20From%20the%20latter%2C%20we%20use%20graph%20neural%20networks%20%28GNNs%29%20adapted%20to%20deal%20with%20relational%20structures%20for%20representing%20value%20functions%20over%20planning%20states%2C%20and%20in%20our%20case%2C%20policies.%20With%20these%20ingredients%20in%20place%2C%20we%20find%20that%20actor-critic%20methods%20can%20be%20used%20to%20learn%20policies%20that%20generalize%20almost%20as%20well%20as%20those%20obtained%20using%20combinatorial%20approaches%20while%20avoiding%20the%20scalability%20bottleneck%20and%20the%20use%20of%20feature%20pools.%20Moreover%2C%20the%20limitations%20of%20the%20DRL%20methods%20on%20the%20benchmarks%20considered%20have%20little%20to%20do%20with%20deep%20learning%20or%20reinforcement%20learning%20algorithms%2C%20and%20result%20from%20the%20well-understood%20expressive%20limitations%20of%20GNNs%2C%20and%20the%20tradeoff%20between%20optimality%20and%20generalization%20%28general%20policies%20cannot%20be%20optimal%20in%20some%20domains%29.%20Both%20of%20these%20limitations%20are%20addressed%20without%20changing%20the%20basic%20DRL%20methods%20by%20adding%20derived%20predicates%20and%20an%20alternative%20cost%20structure%20to%20optimize.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19366v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520General%2520Policies%2520with%2520Policy%2520Gradient%2520Methods%26entry.906535625%3DSimon%2520St%25C3%25A5hlberg%2520and%2520Blai%2520Bonet%2520and%2520Hector%2520Geffner%26entry.1292438233%3DWhile%2520reinforcement%2520learning%2520methods%2520have%2520delivered%2520remarkable%2520results%2520in%2520a%2520number%2520of%2520settings%252C%2520generalization%252C%2520i.e.%252C%2520the%2520ability%2520to%2520produce%2520policies%2520that%2520generalize%2520in%2520a%2520reliable%2520and%2520systematic%2520way%252C%2520has%2520remained%2520a%2520challenge.%2520The%2520problem%2520of%2520generalization%2520has%2520been%2520addressed%2520formally%2520in%2520classical%2520planning%2520where%2520provable%2520correct%2520policies%2520that%2520generalize%2520over%2520all%2520instances%2520of%2520a%2520given%2520domain%2520have%2520been%2520learned%2520using%2520combinatorial%2520methods.%2520The%2520aim%2520of%2520this%2520work%2520is%2520to%2520bring%2520these%2520two%2520research%2520threads%2520together%2520to%2520illuminate%2520the%2520conditions%2520under%2520which%2520%2528deep%2529%2520reinforcement%2520learning%2520approaches%252C%2520and%2520in%2520particular%252C%2520policy%2520optimization%2520methods%252C%2520can%2520be%2520used%2520to%2520learn%2520policies%2520that%2520generalize%2520like%2520combinatorial%2520methods%2520do.%2520We%2520draw%2520on%2520lessons%2520learned%2520from%2520previous%2520combinatorial%2520and%2520deep%2520learning%2520approaches%252C%2520and%2520extend%2520them%2520in%2520a%2520convenient%2520way.%2520From%2520the%2520former%252C%2520we%2520model%2520policies%2520as%2520state%2520transition%2520classifiers%252C%2520as%2520%2528ground%2529%2520actions%2520are%2520not%2520general%2520and%2520change%2520from%2520instance%2520to%2520instance.%2520From%2520the%2520latter%252C%2520we%2520use%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520adapted%2520to%2520deal%2520with%2520relational%2520structures%2520for%2520representing%2520value%2520functions%2520over%2520planning%2520states%252C%2520and%2520in%2520our%2520case%252C%2520policies.%2520With%2520these%2520ingredients%2520in%2520place%252C%2520we%2520find%2520that%2520actor-critic%2520methods%2520can%2520be%2520used%2520to%2520learn%2520policies%2520that%2520generalize%2520almost%2520as%2520well%2520as%2520those%2520obtained%2520using%2520combinatorial%2520approaches%2520while%2520avoiding%2520the%2520scalability%2520bottleneck%2520and%2520the%2520use%2520of%2520feature%2520pools.%2520Moreover%252C%2520the%2520limitations%2520of%2520the%2520DRL%2520methods%2520on%2520the%2520benchmarks%2520considered%2520have%2520little%2520to%2520do%2520with%2520deep%2520learning%2520or%2520reinforcement%2520learning%2520algorithms%252C%2520and%2520result%2520from%2520the%2520well-understood%2520expressive%2520limitations%2520of%2520GNNs%252C%2520and%2520the%2520tradeoff%2520between%2520optimality%2520and%2520generalization%2520%2528general%2520policies%2520cannot%2520be%2520optimal%2520in%2520some%2520domains%2529.%2520Both%2520of%2520these%2520limitations%2520are%2520addressed%2520without%2520changing%2520the%2520basic%2520DRL%2520methods%2520by%2520adding%2520derived%2520predicates%2520and%2520an%2520alternative%2520cost%2520structure%2520to%2520optimize.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19366v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20General%20Policies%20with%20Policy%20Gradient%20Methods&entry.906535625=Simon%20St%C3%A5hlberg%20and%20Blai%20Bonet%20and%20Hector%20Geffner&entry.1292438233=While%20reinforcement%20learning%20methods%20have%20delivered%20remarkable%20results%20in%20a%20number%20of%20settings%2C%20generalization%2C%20i.e.%2C%20the%20ability%20to%20produce%20policies%20that%20generalize%20in%20a%20reliable%20and%20systematic%20way%2C%20has%20remained%20a%20challenge.%20The%20problem%20of%20generalization%20has%20been%20addressed%20formally%20in%20classical%20planning%20where%20provable%20correct%20policies%20that%20generalize%20over%20all%20instances%20of%20a%20given%20domain%20have%20been%20learned%20using%20combinatorial%20methods.%20The%20aim%20of%20this%20work%20is%20to%20bring%20these%20two%20research%20threads%20together%20to%20illuminate%20the%20conditions%20under%20which%20%28deep%29%20reinforcement%20learning%20approaches%2C%20and%20in%20particular%2C%20policy%20optimization%20methods%2C%20can%20be%20used%20to%20learn%20policies%20that%20generalize%20like%20combinatorial%20methods%20do.%20We%20draw%20on%20lessons%20learned%20from%20previous%20combinatorial%20and%20deep%20learning%20approaches%2C%20and%20extend%20them%20in%20a%20convenient%20way.%20From%20the%20former%2C%20we%20model%20policies%20as%20state%20transition%20classifiers%2C%20as%20%28ground%29%20actions%20are%20not%20general%20and%20change%20from%20instance%20to%20instance.%20From%20the%20latter%2C%20we%20use%20graph%20neural%20networks%20%28GNNs%29%20adapted%20to%20deal%20with%20relational%20structures%20for%20representing%20value%20functions%20over%20planning%20states%2C%20and%20in%20our%20case%2C%20policies.%20With%20these%20ingredients%20in%20place%2C%20we%20find%20that%20actor-critic%20methods%20can%20be%20used%20to%20learn%20policies%20that%20generalize%20almost%20as%20well%20as%20those%20obtained%20using%20combinatorial%20approaches%20while%20avoiding%20the%20scalability%20bottleneck%20and%20the%20use%20of%20feature%20pools.%20Moreover%2C%20the%20limitations%20of%20the%20DRL%20methods%20on%20the%20benchmarks%20considered%20have%20little%20to%20do%20with%20deep%20learning%20or%20reinforcement%20learning%20algorithms%2C%20and%20result%20from%20the%20well-understood%20expressive%20limitations%20of%20GNNs%2C%20and%20the%20tradeoff%20between%20optimality%20and%20generalization%20%28general%20policies%20cannot%20be%20optimal%20in%20some%20domains%29.%20Both%20of%20these%20limitations%20are%20addressed%20without%20changing%20the%20basic%20DRL%20methods%20by%20adding%20derived%20predicates%20and%20an%20alternative%20cost%20structure%20to%20optimize.&entry.1838667208=http%3A//arxiv.org/abs/2512.19366v1&entry.124074799=Read"},
{"title": "Alternating Direction Method of Multipliers for Nonlinear Matrix Decompositions", "author": "Atharva Awari and Nicolas Gillis and Arnaud Vandaele", "abstract": "We present an algorithm based on the alternating direction method of multipliers (ADMM) for solving nonlinear matrix decompositions (NMD). Given an input matrix $X \\in \\mathbb{R}^{m \\times n}$ and a factorization rank $r \\ll \\min(m, n)$, NMD seeks matrices $W \\in \\mathbb{R}^{m \\times r}$ and $H \\in \\mathbb{R}^{r \\times n}$ such that $X \\approx f(WH)$, where $f$ is an element-wise nonlinear function. We evaluate our method on several representative nonlinear models: the rectified linear unit activation $f(x) = \\max(0, x)$, suitable for nonnegative sparse data approximation, the component-wise square $f(x) = x^2$, applicable to probabilistic circuit representation, and the MinMax transform $f(x) = \\min(b, \\max(a, x))$, relevant for recommender systems. The proposed framework flexibly supports diverse loss functions, including least squares, $\\ell_1$ norm, and the Kullback-Leibler divergence, and can be readily extended to other nonlinearities and metrics. We illustrate the applicability, efficiency, and adaptability of the approach on real-world datasets, highlighting its potential for a broad range of applications.", "link": "http://arxiv.org/abs/2512.17473v2", "date": "2025-12-22", "relevancy": 1.7541, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4424}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4382}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4373}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Alternating%20Direction%20Method%20of%20Multipliers%20for%20Nonlinear%20Matrix%20Decompositions&body=Title%3A%20Alternating%20Direction%20Method%20of%20Multipliers%20for%20Nonlinear%20Matrix%20Decompositions%0AAuthor%3A%20Atharva%20Awari%20and%20Nicolas%20Gillis%20and%20Arnaud%20Vandaele%0AAbstract%3A%20We%20present%20an%20algorithm%20based%20on%20the%20alternating%20direction%20method%20of%20multipliers%20%28ADMM%29%20for%20solving%20nonlinear%20matrix%20decompositions%20%28NMD%29.%20Given%20an%20input%20matrix%20%24X%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20n%7D%24%20and%20a%20factorization%20rank%20%24r%20%5Cll%20%5Cmin%28m%2C%20n%29%24%2C%20NMD%20seeks%20matrices%20%24W%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20r%7D%24%20and%20%24H%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Br%20%5Ctimes%20n%7D%24%20such%20that%20%24X%20%5Capprox%20f%28WH%29%24%2C%20where%20%24f%24%20is%20an%20element-wise%20nonlinear%20function.%20We%20evaluate%20our%20method%20on%20several%20representative%20nonlinear%20models%3A%20the%20rectified%20linear%20unit%20activation%20%24f%28x%29%20%3D%20%5Cmax%280%2C%20x%29%24%2C%20suitable%20for%20nonnegative%20sparse%20data%20approximation%2C%20the%20component-wise%20square%20%24f%28x%29%20%3D%20x%5E2%24%2C%20applicable%20to%20probabilistic%20circuit%20representation%2C%20and%20the%20MinMax%20transform%20%24f%28x%29%20%3D%20%5Cmin%28b%2C%20%5Cmax%28a%2C%20x%29%29%24%2C%20relevant%20for%20recommender%20systems.%20The%20proposed%20framework%20flexibly%20supports%20diverse%20loss%20functions%2C%20including%20least%20squares%2C%20%24%5Cell_1%24%20norm%2C%20and%20the%20Kullback-Leibler%20divergence%2C%20and%20can%20be%20readily%20extended%20to%20other%20nonlinearities%20and%20metrics.%20We%20illustrate%20the%20applicability%2C%20efficiency%2C%20and%20adaptability%20of%20the%20approach%20on%20real-world%20datasets%2C%20highlighting%20its%20potential%20for%20a%20broad%20range%20of%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17473v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlternating%2520Direction%2520Method%2520of%2520Multipliers%2520for%2520Nonlinear%2520Matrix%2520Decompositions%26entry.906535625%3DAtharva%2520Awari%2520and%2520Nicolas%2520Gillis%2520and%2520Arnaud%2520Vandaele%26entry.1292438233%3DWe%2520present%2520an%2520algorithm%2520based%2520on%2520the%2520alternating%2520direction%2520method%2520of%2520multipliers%2520%2528ADMM%2529%2520for%2520solving%2520nonlinear%2520matrix%2520decompositions%2520%2528NMD%2529.%2520Given%2520an%2520input%2520matrix%2520%2524X%2520%255Cin%2520%255Cmathbb%257BR%257D%255E%257Bm%2520%255Ctimes%2520n%257D%2524%2520and%2520a%2520factorization%2520rank%2520%2524r%2520%255Cll%2520%255Cmin%2528m%252C%2520n%2529%2524%252C%2520NMD%2520seeks%2520matrices%2520%2524W%2520%255Cin%2520%255Cmathbb%257BR%257D%255E%257Bm%2520%255Ctimes%2520r%257D%2524%2520and%2520%2524H%2520%255Cin%2520%255Cmathbb%257BR%257D%255E%257Br%2520%255Ctimes%2520n%257D%2524%2520such%2520that%2520%2524X%2520%255Capprox%2520f%2528WH%2529%2524%252C%2520where%2520%2524f%2524%2520is%2520an%2520element-wise%2520nonlinear%2520function.%2520We%2520evaluate%2520our%2520method%2520on%2520several%2520representative%2520nonlinear%2520models%253A%2520the%2520rectified%2520linear%2520unit%2520activation%2520%2524f%2528x%2529%2520%253D%2520%255Cmax%25280%252C%2520x%2529%2524%252C%2520suitable%2520for%2520nonnegative%2520sparse%2520data%2520approximation%252C%2520the%2520component-wise%2520square%2520%2524f%2528x%2529%2520%253D%2520x%255E2%2524%252C%2520applicable%2520to%2520probabilistic%2520circuit%2520representation%252C%2520and%2520the%2520MinMax%2520transform%2520%2524f%2528x%2529%2520%253D%2520%255Cmin%2528b%252C%2520%255Cmax%2528a%252C%2520x%2529%2529%2524%252C%2520relevant%2520for%2520recommender%2520systems.%2520The%2520proposed%2520framework%2520flexibly%2520supports%2520diverse%2520loss%2520functions%252C%2520including%2520least%2520squares%252C%2520%2524%255Cell_1%2524%2520norm%252C%2520and%2520the%2520Kullback-Leibler%2520divergence%252C%2520and%2520can%2520be%2520readily%2520extended%2520to%2520other%2520nonlinearities%2520and%2520metrics.%2520We%2520illustrate%2520the%2520applicability%252C%2520efficiency%252C%2520and%2520adaptability%2520of%2520the%2520approach%2520on%2520real-world%2520datasets%252C%2520highlighting%2520its%2520potential%2520for%2520a%2520broad%2520range%2520of%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17473v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alternating%20Direction%20Method%20of%20Multipliers%20for%20Nonlinear%20Matrix%20Decompositions&entry.906535625=Atharva%20Awari%20and%20Nicolas%20Gillis%20and%20Arnaud%20Vandaele&entry.1292438233=We%20present%20an%20algorithm%20based%20on%20the%20alternating%20direction%20method%20of%20multipliers%20%28ADMM%29%20for%20solving%20nonlinear%20matrix%20decompositions%20%28NMD%29.%20Given%20an%20input%20matrix%20%24X%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20n%7D%24%20and%20a%20factorization%20rank%20%24r%20%5Cll%20%5Cmin%28m%2C%20n%29%24%2C%20NMD%20seeks%20matrices%20%24W%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bm%20%5Ctimes%20r%7D%24%20and%20%24H%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Br%20%5Ctimes%20n%7D%24%20such%20that%20%24X%20%5Capprox%20f%28WH%29%24%2C%20where%20%24f%24%20is%20an%20element-wise%20nonlinear%20function.%20We%20evaluate%20our%20method%20on%20several%20representative%20nonlinear%20models%3A%20the%20rectified%20linear%20unit%20activation%20%24f%28x%29%20%3D%20%5Cmax%280%2C%20x%29%24%2C%20suitable%20for%20nonnegative%20sparse%20data%20approximation%2C%20the%20component-wise%20square%20%24f%28x%29%20%3D%20x%5E2%24%2C%20applicable%20to%20probabilistic%20circuit%20representation%2C%20and%20the%20MinMax%20transform%20%24f%28x%29%20%3D%20%5Cmin%28b%2C%20%5Cmax%28a%2C%20x%29%29%24%2C%20relevant%20for%20recommender%20systems.%20The%20proposed%20framework%20flexibly%20supports%20diverse%20loss%20functions%2C%20including%20least%20squares%2C%20%24%5Cell_1%24%20norm%2C%20and%20the%20Kullback-Leibler%20divergence%2C%20and%20can%20be%20readily%20extended%20to%20other%20nonlinearities%20and%20metrics.%20We%20illustrate%20the%20applicability%2C%20efficiency%2C%20and%20adaptability%20of%20the%20approach%20on%20real-world%20datasets%2C%20highlighting%20its%20potential%20for%20a%20broad%20range%20of%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2512.17473v2&entry.124074799=Read"},
{"title": "Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis", "author": "Ko Watanabe and Stanislav Frolov and Aya Hassan and David Dembinsky and Adriano Lucieri and Andreas Dengel", "abstract": "Recent advances in deep learning and on-device inference could transform routine screening for skin cancers. Along with the anticipated benefits of this technology, potential dangers arise from unforeseen and inherent biases. A significant obstacle is building evaluation datasets that accurately reflect key demographics, including sex, age, and race, as well as other underrepresented groups. To address this, we train a state-of-the-art generative model to generate synthetic data in a controllable manner to assess the fairness of publicly available skin cancer classifiers. To evaluate whether synthetic images can be used as a fairness testing dataset, we prepare a real-image dataset (MILK10K) as a benchmark and compare the True Positive Rate result of three models (DeepGuide, MelaNet, and SkinLesionDensnet). As a result, the classification tendencies observed in each model when tested on real and generated images showed similar patterns across different attribute data sets. We confirm that highly realistic synthetic images facilitate model fairness verification.", "link": "http://arxiv.org/abs/2507.17860v4", "date": "2025-12-22", "relevancy": 1.5611, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5215}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5214}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5165}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Facilitated%20Fairness%20Assessment%20of%20AI-based%20Skin%20Lesion%20Classifiers%20Through%20GenAI-based%20Image%20Synthesis&body=Title%3A%20Towards%20Facilitated%20Fairness%20Assessment%20of%20AI-based%20Skin%20Lesion%20Classifiers%20Through%20GenAI-based%20Image%20Synthesis%0AAuthor%3A%20Ko%20Watanabe%20and%20Stanislav%20Frolov%20and%20Aya%20Hassan%20and%20David%20Dembinsky%20and%20Adriano%20Lucieri%20and%20Andreas%20Dengel%0AAbstract%3A%20Recent%20advances%20in%20deep%20learning%20and%20on-device%20inference%20could%20transform%20routine%20screening%20for%20skin%20cancers.%20Along%20with%20the%20anticipated%20benefits%20of%20this%20technology%2C%20potential%20dangers%20arise%20from%20unforeseen%20and%20inherent%20biases.%20A%20significant%20obstacle%20is%20building%20evaluation%20datasets%20that%20accurately%20reflect%20key%20demographics%2C%20including%20sex%2C%20age%2C%20and%20race%2C%20as%20well%20as%20other%20underrepresented%20groups.%20To%20address%20this%2C%20we%20train%20a%20state-of-the-art%20generative%20model%20to%20generate%20synthetic%20data%20in%20a%20controllable%20manner%20to%20assess%20the%20fairness%20of%20publicly%20available%20skin%20cancer%20classifiers.%20To%20evaluate%20whether%20synthetic%20images%20can%20be%20used%20as%20a%20fairness%20testing%20dataset%2C%20we%20prepare%20a%20real-image%20dataset%20%28MILK10K%29%20as%20a%20benchmark%20and%20compare%20the%20True%20Positive%20Rate%20result%20of%20three%20models%20%28DeepGuide%2C%20MelaNet%2C%20and%20SkinLesionDensnet%29.%20As%20a%20result%2C%20the%20classification%20tendencies%20observed%20in%20each%20model%20when%20tested%20on%20real%20and%20generated%20images%20showed%20similar%20patterns%20across%20different%20attribute%20data%20sets.%20We%20confirm%20that%20highly%20realistic%20synthetic%20images%20facilitate%20model%20fairness%20verification.%0ALink%3A%20http%3A//arxiv.org/abs/2507.17860v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Facilitated%2520Fairness%2520Assessment%2520of%2520AI-based%2520Skin%2520Lesion%2520Classifiers%2520Through%2520GenAI-based%2520Image%2520Synthesis%26entry.906535625%3DKo%2520Watanabe%2520and%2520Stanislav%2520Frolov%2520and%2520Aya%2520Hassan%2520and%2520David%2520Dembinsky%2520and%2520Adriano%2520Lucieri%2520and%2520Andreas%2520Dengel%26entry.1292438233%3DRecent%2520advances%2520in%2520deep%2520learning%2520and%2520on-device%2520inference%2520could%2520transform%2520routine%2520screening%2520for%2520skin%2520cancers.%2520Along%2520with%2520the%2520anticipated%2520benefits%2520of%2520this%2520technology%252C%2520potential%2520dangers%2520arise%2520from%2520unforeseen%2520and%2520inherent%2520biases.%2520A%2520significant%2520obstacle%2520is%2520building%2520evaluation%2520datasets%2520that%2520accurately%2520reflect%2520key%2520demographics%252C%2520including%2520sex%252C%2520age%252C%2520and%2520race%252C%2520as%2520well%2520as%2520other%2520underrepresented%2520groups.%2520To%2520address%2520this%252C%2520we%2520train%2520a%2520state-of-the-art%2520generative%2520model%2520to%2520generate%2520synthetic%2520data%2520in%2520a%2520controllable%2520manner%2520to%2520assess%2520the%2520fairness%2520of%2520publicly%2520available%2520skin%2520cancer%2520classifiers.%2520To%2520evaluate%2520whether%2520synthetic%2520images%2520can%2520be%2520used%2520as%2520a%2520fairness%2520testing%2520dataset%252C%2520we%2520prepare%2520a%2520real-image%2520dataset%2520%2528MILK10K%2529%2520as%2520a%2520benchmark%2520and%2520compare%2520the%2520True%2520Positive%2520Rate%2520result%2520of%2520three%2520models%2520%2528DeepGuide%252C%2520MelaNet%252C%2520and%2520SkinLesionDensnet%2529.%2520As%2520a%2520result%252C%2520the%2520classification%2520tendencies%2520observed%2520in%2520each%2520model%2520when%2520tested%2520on%2520real%2520and%2520generated%2520images%2520showed%2520similar%2520patterns%2520across%2520different%2520attribute%2520data%2520sets.%2520We%2520confirm%2520that%2520highly%2520realistic%2520synthetic%2520images%2520facilitate%2520model%2520fairness%2520verification.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.17860v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Facilitated%20Fairness%20Assessment%20of%20AI-based%20Skin%20Lesion%20Classifiers%20Through%20GenAI-based%20Image%20Synthesis&entry.906535625=Ko%20Watanabe%20and%20Stanislav%20Frolov%20and%20Aya%20Hassan%20and%20David%20Dembinsky%20and%20Adriano%20Lucieri%20and%20Andreas%20Dengel&entry.1292438233=Recent%20advances%20in%20deep%20learning%20and%20on-device%20inference%20could%20transform%20routine%20screening%20for%20skin%20cancers.%20Along%20with%20the%20anticipated%20benefits%20of%20this%20technology%2C%20potential%20dangers%20arise%20from%20unforeseen%20and%20inherent%20biases.%20A%20significant%20obstacle%20is%20building%20evaluation%20datasets%20that%20accurately%20reflect%20key%20demographics%2C%20including%20sex%2C%20age%2C%20and%20race%2C%20as%20well%20as%20other%20underrepresented%20groups.%20To%20address%20this%2C%20we%20train%20a%20state-of-the-art%20generative%20model%20to%20generate%20synthetic%20data%20in%20a%20controllable%20manner%20to%20assess%20the%20fairness%20of%20publicly%20available%20skin%20cancer%20classifiers.%20To%20evaluate%20whether%20synthetic%20images%20can%20be%20used%20as%20a%20fairness%20testing%20dataset%2C%20we%20prepare%20a%20real-image%20dataset%20%28MILK10K%29%20as%20a%20benchmark%20and%20compare%20the%20True%20Positive%20Rate%20result%20of%20three%20models%20%28DeepGuide%2C%20MelaNet%2C%20and%20SkinLesionDensnet%29.%20As%20a%20result%2C%20the%20classification%20tendencies%20observed%20in%20each%20model%20when%20tested%20on%20real%20and%20generated%20images%20showed%20similar%20patterns%20across%20different%20attribute%20data%20sets.%20We%20confirm%20that%20highly%20realistic%20synthetic%20images%20facilitate%20model%20fairness%20verification.&entry.1838667208=http%3A//arxiv.org/abs/2507.17860v4&entry.124074799=Read"},
{"title": "Towards Minimal Fine-Tuning of VLMs", "author": "Tiange Luo and Lajanugen Logeswaran and Jaekyeom Kim and Justin Johnson and Honglak Lee", "abstract": "We introduce Image-LoRA, a lightweight parameter efficient fine-tuning (PEFT) recipe for transformer-based vision-language models (VLMs). Image-LoRA applies low-rank adaptation only to the value path of attention layers within the visual-token span, reducing adapter-only training FLOPs roughly in proportion to the visual-token fraction. We further adapt only a subset of attention heads, selected using head influence scores estimated with a rank-1 Image-LoRA, and stabilize per-layer updates via selection-size normalization. Across screen-centric grounding and referring benchmarks spanning text-heavy to image-heavy regimes, Image-LoRA matches or closely approaches standard LoRA accuracy while using fewer trainable parameters and lower adapter-only training FLOPs. The method also preserves the pure-text reasoning performance of VLMs before and after fine-tuning, as further shown on GSM8K.", "link": "http://arxiv.org/abs/2512.19219v1", "date": "2025-12-22", "relevancy": 2.0348, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5136}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5089}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5066}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Minimal%20Fine-Tuning%20of%20VLMs&body=Title%3A%20Towards%20Minimal%20Fine-Tuning%20of%20VLMs%0AAuthor%3A%20Tiange%20Luo%20and%20Lajanugen%20Logeswaran%20and%20Jaekyeom%20Kim%20and%20Justin%20Johnson%20and%20Honglak%20Lee%0AAbstract%3A%20We%20introduce%20Image-LoRA%2C%20a%20lightweight%20parameter%20efficient%20fine-tuning%20%28PEFT%29%20recipe%20for%20transformer-based%20vision-language%20models%20%28VLMs%29.%20Image-LoRA%20applies%20low-rank%20adaptation%20only%20to%20the%20value%20path%20of%20attention%20layers%20within%20the%20visual-token%20span%2C%20reducing%20adapter-only%20training%20FLOPs%20roughly%20in%20proportion%20to%20the%20visual-token%20fraction.%20We%20further%20adapt%20only%20a%20subset%20of%20attention%20heads%2C%20selected%20using%20head%20influence%20scores%20estimated%20with%20a%20rank-1%20Image-LoRA%2C%20and%20stabilize%20per-layer%20updates%20via%20selection-size%20normalization.%20Across%20screen-centric%20grounding%20and%20referring%20benchmarks%20spanning%20text-heavy%20to%20image-heavy%20regimes%2C%20Image-LoRA%20matches%20or%20closely%20approaches%20standard%20LoRA%20accuracy%20while%20using%20fewer%20trainable%20parameters%20and%20lower%20adapter-only%20training%20FLOPs.%20The%20method%20also%20preserves%20the%20pure-text%20reasoning%20performance%20of%20VLMs%20before%20and%20after%20fine-tuning%2C%20as%20further%20shown%20on%20GSM8K.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19219v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Minimal%2520Fine-Tuning%2520of%2520VLMs%26entry.906535625%3DTiange%2520Luo%2520and%2520Lajanugen%2520Logeswaran%2520and%2520Jaekyeom%2520Kim%2520and%2520Justin%2520Johnson%2520and%2520Honglak%2520Lee%26entry.1292438233%3DWe%2520introduce%2520Image-LoRA%252C%2520a%2520lightweight%2520parameter%2520efficient%2520fine-tuning%2520%2528PEFT%2529%2520recipe%2520for%2520transformer-based%2520vision-language%2520models%2520%2528VLMs%2529.%2520Image-LoRA%2520applies%2520low-rank%2520adaptation%2520only%2520to%2520the%2520value%2520path%2520of%2520attention%2520layers%2520within%2520the%2520visual-token%2520span%252C%2520reducing%2520adapter-only%2520training%2520FLOPs%2520roughly%2520in%2520proportion%2520to%2520the%2520visual-token%2520fraction.%2520We%2520further%2520adapt%2520only%2520a%2520subset%2520of%2520attention%2520heads%252C%2520selected%2520using%2520head%2520influence%2520scores%2520estimated%2520with%2520a%2520rank-1%2520Image-LoRA%252C%2520and%2520stabilize%2520per-layer%2520updates%2520via%2520selection-size%2520normalization.%2520Across%2520screen-centric%2520grounding%2520and%2520referring%2520benchmarks%2520spanning%2520text-heavy%2520to%2520image-heavy%2520regimes%252C%2520Image-LoRA%2520matches%2520or%2520closely%2520approaches%2520standard%2520LoRA%2520accuracy%2520while%2520using%2520fewer%2520trainable%2520parameters%2520and%2520lower%2520adapter-only%2520training%2520FLOPs.%2520The%2520method%2520also%2520preserves%2520the%2520pure-text%2520reasoning%2520performance%2520of%2520VLMs%2520before%2520and%2520after%2520fine-tuning%252C%2520as%2520further%2520shown%2520on%2520GSM8K.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19219v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Minimal%20Fine-Tuning%20of%20VLMs&entry.906535625=Tiange%20Luo%20and%20Lajanugen%20Logeswaran%20and%20Jaekyeom%20Kim%20and%20Justin%20Johnson%20and%20Honglak%20Lee&entry.1292438233=We%20introduce%20Image-LoRA%2C%20a%20lightweight%20parameter%20efficient%20fine-tuning%20%28PEFT%29%20recipe%20for%20transformer-based%20vision-language%20models%20%28VLMs%29.%20Image-LoRA%20applies%20low-rank%20adaptation%20only%20to%20the%20value%20path%20of%20attention%20layers%20within%20the%20visual-token%20span%2C%20reducing%20adapter-only%20training%20FLOPs%20roughly%20in%20proportion%20to%20the%20visual-token%20fraction.%20We%20further%20adapt%20only%20a%20subset%20of%20attention%20heads%2C%20selected%20using%20head%20influence%20scores%20estimated%20with%20a%20rank-1%20Image-LoRA%2C%20and%20stabilize%20per-layer%20updates%20via%20selection-size%20normalization.%20Across%20screen-centric%20grounding%20and%20referring%20benchmarks%20spanning%20text-heavy%20to%20image-heavy%20regimes%2C%20Image-LoRA%20matches%20or%20closely%20approaches%20standard%20LoRA%20accuracy%20while%20using%20fewer%20trainable%20parameters%20and%20lower%20adapter-only%20training%20FLOPs.%20The%20method%20also%20preserves%20the%20pure-text%20reasoning%20performance%20of%20VLMs%20before%20and%20after%20fine-tuning%2C%20as%20further%20shown%20on%20GSM8K.&entry.1838667208=http%3A//arxiv.org/abs/2512.19219v1&entry.124074799=Read"},
{"title": "From Black-Box Tuning to Guided Optimization via Hyperparameters Interaction Analysis", "author": "Moncef Garouani and Ayah Barhrhouj", "abstract": "Hyperparameters tuning is a fundamental, yet computationally expensive, step in optimizing machine learning models. Beyond optimization, understanding the relative importance and interaction of hyperparameters is critical to efficient model development. In this paper, we introduce MetaSHAP, a scalable semi-automated eXplainable AI (XAI) method, that uses meta-learning and Shapley values analysis to provide actionable and dataset-aware tuning insights. MetaSHAP operates over a vast benchmark of over 09 millions evaluated machine learning pipelines, allowing it to produce interpretable importance scores and actionable tuning insights that reveal how much each hyperparameter matters, how it interacts with others and in which value ranges its influence is concentrated. For a given algorithm and dataset, MetaSHAP learns a surrogate performance model from historical configurations, computes hyperparameters interactions using SHAP-based analysis, and derives interpretable tuning ranges from the most influential hyperparameters. This allows practitioners not only to prioritize which hyperparameters to tune, but also to understand their directionality and interactions. We empirically validate MetaSHAP on a diverse benchmark of 164 classification datasets and 14 classifiers, demonstrating that it produces reliable importance rankings and competitive performance when used to guide Bayesian optimization.", "link": "http://arxiv.org/abs/2512.19246v1", "date": "2025-12-22", "relevancy": 1.7533, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4445}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4354}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Black-Box%20Tuning%20to%20Guided%20Optimization%20via%20Hyperparameters%20Interaction%20Analysis&body=Title%3A%20From%20Black-Box%20Tuning%20to%20Guided%20Optimization%20via%20Hyperparameters%20Interaction%20Analysis%0AAuthor%3A%20Moncef%20Garouani%20and%20Ayah%20Barhrhouj%0AAbstract%3A%20Hyperparameters%20tuning%20is%20a%20fundamental%2C%20yet%20computationally%20expensive%2C%20step%20in%20optimizing%20machine%20learning%20models.%20Beyond%20optimization%2C%20understanding%20the%20relative%20importance%20and%20interaction%20of%20hyperparameters%20is%20critical%20to%20efficient%20model%20development.%20In%20this%20paper%2C%20we%20introduce%20MetaSHAP%2C%20a%20scalable%20semi-automated%20eXplainable%20AI%20%28XAI%29%20method%2C%20that%20uses%20meta-learning%20and%20Shapley%20values%20analysis%20to%20provide%20actionable%20and%20dataset-aware%20tuning%20insights.%20MetaSHAP%20operates%20over%20a%20vast%20benchmark%20of%20over%2009%20millions%20evaluated%20machine%20learning%20pipelines%2C%20allowing%20it%20to%20produce%20interpretable%20importance%20scores%20and%20actionable%20tuning%20insights%20that%20reveal%20how%20much%20each%20hyperparameter%20matters%2C%20how%20it%20interacts%20with%20others%20and%20in%20which%20value%20ranges%20its%20influence%20is%20concentrated.%20For%20a%20given%20algorithm%20and%20dataset%2C%20MetaSHAP%20learns%20a%20surrogate%20performance%20model%20from%20historical%20configurations%2C%20computes%20hyperparameters%20interactions%20using%20SHAP-based%20analysis%2C%20and%20derives%20interpretable%20tuning%20ranges%20from%20the%20most%20influential%20hyperparameters.%20This%20allows%20practitioners%20not%20only%20to%20prioritize%20which%20hyperparameters%20to%20tune%2C%20but%20also%20to%20understand%20their%20directionality%20and%20interactions.%20We%20empirically%20validate%20MetaSHAP%20on%20a%20diverse%20benchmark%20of%20164%20classification%20datasets%20and%2014%20classifiers%2C%20demonstrating%20that%20it%20produces%20reliable%20importance%20rankings%20and%20competitive%20performance%20when%20used%20to%20guide%20Bayesian%20optimization.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19246v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Black-Box%2520Tuning%2520to%2520Guided%2520Optimization%2520via%2520Hyperparameters%2520Interaction%2520Analysis%26entry.906535625%3DMoncef%2520Garouani%2520and%2520Ayah%2520Barhrhouj%26entry.1292438233%3DHyperparameters%2520tuning%2520is%2520a%2520fundamental%252C%2520yet%2520computationally%2520expensive%252C%2520step%2520in%2520optimizing%2520machine%2520learning%2520models.%2520Beyond%2520optimization%252C%2520understanding%2520the%2520relative%2520importance%2520and%2520interaction%2520of%2520hyperparameters%2520is%2520critical%2520to%2520efficient%2520model%2520development.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520MetaSHAP%252C%2520a%2520scalable%2520semi-automated%2520eXplainable%2520AI%2520%2528XAI%2529%2520method%252C%2520that%2520uses%2520meta-learning%2520and%2520Shapley%2520values%2520analysis%2520to%2520provide%2520actionable%2520and%2520dataset-aware%2520tuning%2520insights.%2520MetaSHAP%2520operates%2520over%2520a%2520vast%2520benchmark%2520of%2520over%252009%2520millions%2520evaluated%2520machine%2520learning%2520pipelines%252C%2520allowing%2520it%2520to%2520produce%2520interpretable%2520importance%2520scores%2520and%2520actionable%2520tuning%2520insights%2520that%2520reveal%2520how%2520much%2520each%2520hyperparameter%2520matters%252C%2520how%2520it%2520interacts%2520with%2520others%2520and%2520in%2520which%2520value%2520ranges%2520its%2520influence%2520is%2520concentrated.%2520For%2520a%2520given%2520algorithm%2520and%2520dataset%252C%2520MetaSHAP%2520learns%2520a%2520surrogate%2520performance%2520model%2520from%2520historical%2520configurations%252C%2520computes%2520hyperparameters%2520interactions%2520using%2520SHAP-based%2520analysis%252C%2520and%2520derives%2520interpretable%2520tuning%2520ranges%2520from%2520the%2520most%2520influential%2520hyperparameters.%2520This%2520allows%2520practitioners%2520not%2520only%2520to%2520prioritize%2520which%2520hyperparameters%2520to%2520tune%252C%2520but%2520also%2520to%2520understand%2520their%2520directionality%2520and%2520interactions.%2520We%2520empirically%2520validate%2520MetaSHAP%2520on%2520a%2520diverse%2520benchmark%2520of%2520164%2520classification%2520datasets%2520and%252014%2520classifiers%252C%2520demonstrating%2520that%2520it%2520produces%2520reliable%2520importance%2520rankings%2520and%2520competitive%2520performance%2520when%2520used%2520to%2520guide%2520Bayesian%2520optimization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19246v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Black-Box%20Tuning%20to%20Guided%20Optimization%20via%20Hyperparameters%20Interaction%20Analysis&entry.906535625=Moncef%20Garouani%20and%20Ayah%20Barhrhouj&entry.1292438233=Hyperparameters%20tuning%20is%20a%20fundamental%2C%20yet%20computationally%20expensive%2C%20step%20in%20optimizing%20machine%20learning%20models.%20Beyond%20optimization%2C%20understanding%20the%20relative%20importance%20and%20interaction%20of%20hyperparameters%20is%20critical%20to%20efficient%20model%20development.%20In%20this%20paper%2C%20we%20introduce%20MetaSHAP%2C%20a%20scalable%20semi-automated%20eXplainable%20AI%20%28XAI%29%20method%2C%20that%20uses%20meta-learning%20and%20Shapley%20values%20analysis%20to%20provide%20actionable%20and%20dataset-aware%20tuning%20insights.%20MetaSHAP%20operates%20over%20a%20vast%20benchmark%20of%20over%2009%20millions%20evaluated%20machine%20learning%20pipelines%2C%20allowing%20it%20to%20produce%20interpretable%20importance%20scores%20and%20actionable%20tuning%20insights%20that%20reveal%20how%20much%20each%20hyperparameter%20matters%2C%20how%20it%20interacts%20with%20others%20and%20in%20which%20value%20ranges%20its%20influence%20is%20concentrated.%20For%20a%20given%20algorithm%20and%20dataset%2C%20MetaSHAP%20learns%20a%20surrogate%20performance%20model%20from%20historical%20configurations%2C%20computes%20hyperparameters%20interactions%20using%20SHAP-based%20analysis%2C%20and%20derives%20interpretable%20tuning%20ranges%20from%20the%20most%20influential%20hyperparameters.%20This%20allows%20practitioners%20not%20only%20to%20prioritize%20which%20hyperparameters%20to%20tune%2C%20but%20also%20to%20understand%20their%20directionality%20and%20interactions.%20We%20empirically%20validate%20MetaSHAP%20on%20a%20diverse%20benchmark%20of%20164%20classification%20datasets%20and%2014%20classifiers%2C%20demonstrating%20that%20it%20produces%20reliable%20importance%20rankings%20and%20competitive%20performance%20when%20used%20to%20guide%20Bayesian%20optimization.&entry.1838667208=http%3A//arxiv.org/abs/2512.19246v1&entry.124074799=Read"},
{"title": "Attention Is Not What You Need", "author": "Zhang Chong", "abstract": "We revisit a basic question in sequence modeling: is explicit self-attention actually necessary for strong performance and reasoning? We argue that standard multi-head attention is best seen as a form of tensor lifting: hidden vectors are mapped into a high-dimensional space of pairwise interactions, and learning proceeds by constraining this lifted tensor through gradient descent. This mechanism is extremely expressive but mathematically opaque, because after many layers it becomes very hard to describe the model with a small family of explicit invariants.\n  To explore an alternative, we propose an attention-free architecture based on Grassmann flows. Instead of forming an L by L attention matrix, our Causal Grassmann layer (i) linearly reduces token states, (ii) encodes local token pairs as two-dimensional subspaces on a Grassmann manifold via Plucker coordinates, and (iii) fuses these geometric features back into the hidden states through gated mixing. Information therefore propagates by controlled deformations of low-rank subspaces over multi-scale local windows, so the core computation lives on a finite-dimensional manifold rather than in an unstructured tensor space.\n  On the Wikitext-2 language modeling benchmark, purely Grassmann-based models with 13 to 18 million parameters achieve validation perplexities within about 10 to 15 percent of size-matched Transformers. On the SNLI natural language inference task, a Grassmann-Plucker head on top of DistilBERT slightly outperforms a Transformer head, with best validation and test accuracies of 0.8550 and 0.8538 compared to 0.8545 and 0.8511. We analyze the complexity of Grassmann mixing, show linear scaling in sequence length for fixed rank, and argue that such manifold-based designs offer a more structured route toward geometric and invariant-based interpretations of neural reasoning.", "link": "http://arxiv.org/abs/2512.19428v1", "date": "2025-12-22", "relevancy": 2.0267, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5373}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5179}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention%20Is%20Not%20What%20You%20Need&body=Title%3A%20Attention%20Is%20Not%20What%20You%20Need%0AAuthor%3A%20Zhang%20Chong%0AAbstract%3A%20We%20revisit%20a%20basic%20question%20in%20sequence%20modeling%3A%20is%20explicit%20self-attention%20actually%20necessary%20for%20strong%20performance%20and%20reasoning%3F%20We%20argue%20that%20standard%20multi-head%20attention%20is%20best%20seen%20as%20a%20form%20of%20tensor%20lifting%3A%20hidden%20vectors%20are%20mapped%20into%20a%20high-dimensional%20space%20of%20pairwise%20interactions%2C%20and%20learning%20proceeds%20by%20constraining%20this%20lifted%20tensor%20through%20gradient%20descent.%20This%20mechanism%20is%20extremely%20expressive%20but%20mathematically%20opaque%2C%20because%20after%20many%20layers%20it%20becomes%20very%20hard%20to%20describe%20the%20model%20with%20a%20small%20family%20of%20explicit%20invariants.%0A%20%20To%20explore%20an%20alternative%2C%20we%20propose%20an%20attention-free%20architecture%20based%20on%20Grassmann%20flows.%20Instead%20of%20forming%20an%20L%20by%20L%20attention%20matrix%2C%20our%20Causal%20Grassmann%20layer%20%28i%29%20linearly%20reduces%20token%20states%2C%20%28ii%29%20encodes%20local%20token%20pairs%20as%20two-dimensional%20subspaces%20on%20a%20Grassmann%20manifold%20via%20Plucker%20coordinates%2C%20and%20%28iii%29%20fuses%20these%20geometric%20features%20back%20into%20the%20hidden%20states%20through%20gated%20mixing.%20Information%20therefore%20propagates%20by%20controlled%20deformations%20of%20low-rank%20subspaces%20over%20multi-scale%20local%20windows%2C%20so%20the%20core%20computation%20lives%20on%20a%20finite-dimensional%20manifold%20rather%20than%20in%20an%20unstructured%20tensor%20space.%0A%20%20On%20the%20Wikitext-2%20language%20modeling%20benchmark%2C%20purely%20Grassmann-based%20models%20with%2013%20to%2018%20million%20parameters%20achieve%20validation%20perplexities%20within%20about%2010%20to%2015%20percent%20of%20size-matched%20Transformers.%20On%20the%20SNLI%20natural%20language%20inference%20task%2C%20a%20Grassmann-Plucker%20head%20on%20top%20of%20DistilBERT%20slightly%20outperforms%20a%20Transformer%20head%2C%20with%20best%20validation%20and%20test%20accuracies%20of%200.8550%20and%200.8538%20compared%20to%200.8545%20and%200.8511.%20We%20analyze%20the%20complexity%20of%20Grassmann%20mixing%2C%20show%20linear%20scaling%20in%20sequence%20length%20for%20fixed%20rank%2C%20and%20argue%20that%20such%20manifold-based%20designs%20offer%20a%20more%20structured%20route%20toward%20geometric%20and%20invariant-based%20interpretations%20of%20neural%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19428v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention%2520Is%2520Not%2520What%2520You%2520Need%26entry.906535625%3DZhang%2520Chong%26entry.1292438233%3DWe%2520revisit%2520a%2520basic%2520question%2520in%2520sequence%2520modeling%253A%2520is%2520explicit%2520self-attention%2520actually%2520necessary%2520for%2520strong%2520performance%2520and%2520reasoning%253F%2520We%2520argue%2520that%2520standard%2520multi-head%2520attention%2520is%2520best%2520seen%2520as%2520a%2520form%2520of%2520tensor%2520lifting%253A%2520hidden%2520vectors%2520are%2520mapped%2520into%2520a%2520high-dimensional%2520space%2520of%2520pairwise%2520interactions%252C%2520and%2520learning%2520proceeds%2520by%2520constraining%2520this%2520lifted%2520tensor%2520through%2520gradient%2520descent.%2520This%2520mechanism%2520is%2520extremely%2520expressive%2520but%2520mathematically%2520opaque%252C%2520because%2520after%2520many%2520layers%2520it%2520becomes%2520very%2520hard%2520to%2520describe%2520the%2520model%2520with%2520a%2520small%2520family%2520of%2520explicit%2520invariants.%250A%2520%2520To%2520explore%2520an%2520alternative%252C%2520we%2520propose%2520an%2520attention-free%2520architecture%2520based%2520on%2520Grassmann%2520flows.%2520Instead%2520of%2520forming%2520an%2520L%2520by%2520L%2520attention%2520matrix%252C%2520our%2520Causal%2520Grassmann%2520layer%2520%2528i%2529%2520linearly%2520reduces%2520token%2520states%252C%2520%2528ii%2529%2520encodes%2520local%2520token%2520pairs%2520as%2520two-dimensional%2520subspaces%2520on%2520a%2520Grassmann%2520manifold%2520via%2520Plucker%2520coordinates%252C%2520and%2520%2528iii%2529%2520fuses%2520these%2520geometric%2520features%2520back%2520into%2520the%2520hidden%2520states%2520through%2520gated%2520mixing.%2520Information%2520therefore%2520propagates%2520by%2520controlled%2520deformations%2520of%2520low-rank%2520subspaces%2520over%2520multi-scale%2520local%2520windows%252C%2520so%2520the%2520core%2520computation%2520lives%2520on%2520a%2520finite-dimensional%2520manifold%2520rather%2520than%2520in%2520an%2520unstructured%2520tensor%2520space.%250A%2520%2520On%2520the%2520Wikitext-2%2520language%2520modeling%2520benchmark%252C%2520purely%2520Grassmann-based%2520models%2520with%252013%2520to%252018%2520million%2520parameters%2520achieve%2520validation%2520perplexities%2520within%2520about%252010%2520to%252015%2520percent%2520of%2520size-matched%2520Transformers.%2520On%2520the%2520SNLI%2520natural%2520language%2520inference%2520task%252C%2520a%2520Grassmann-Plucker%2520head%2520on%2520top%2520of%2520DistilBERT%2520slightly%2520outperforms%2520a%2520Transformer%2520head%252C%2520with%2520best%2520validation%2520and%2520test%2520accuracies%2520of%25200.8550%2520and%25200.8538%2520compared%2520to%25200.8545%2520and%25200.8511.%2520We%2520analyze%2520the%2520complexity%2520of%2520Grassmann%2520mixing%252C%2520show%2520linear%2520scaling%2520in%2520sequence%2520length%2520for%2520fixed%2520rank%252C%2520and%2520argue%2520that%2520such%2520manifold-based%2520designs%2520offer%2520a%2520more%2520structured%2520route%2520toward%2520geometric%2520and%2520invariant-based%2520interpretations%2520of%2520neural%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19428v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20Is%20Not%20What%20You%20Need&entry.906535625=Zhang%20Chong&entry.1292438233=We%20revisit%20a%20basic%20question%20in%20sequence%20modeling%3A%20is%20explicit%20self-attention%20actually%20necessary%20for%20strong%20performance%20and%20reasoning%3F%20We%20argue%20that%20standard%20multi-head%20attention%20is%20best%20seen%20as%20a%20form%20of%20tensor%20lifting%3A%20hidden%20vectors%20are%20mapped%20into%20a%20high-dimensional%20space%20of%20pairwise%20interactions%2C%20and%20learning%20proceeds%20by%20constraining%20this%20lifted%20tensor%20through%20gradient%20descent.%20This%20mechanism%20is%20extremely%20expressive%20but%20mathematically%20opaque%2C%20because%20after%20many%20layers%20it%20becomes%20very%20hard%20to%20describe%20the%20model%20with%20a%20small%20family%20of%20explicit%20invariants.%0A%20%20To%20explore%20an%20alternative%2C%20we%20propose%20an%20attention-free%20architecture%20based%20on%20Grassmann%20flows.%20Instead%20of%20forming%20an%20L%20by%20L%20attention%20matrix%2C%20our%20Causal%20Grassmann%20layer%20%28i%29%20linearly%20reduces%20token%20states%2C%20%28ii%29%20encodes%20local%20token%20pairs%20as%20two-dimensional%20subspaces%20on%20a%20Grassmann%20manifold%20via%20Plucker%20coordinates%2C%20and%20%28iii%29%20fuses%20these%20geometric%20features%20back%20into%20the%20hidden%20states%20through%20gated%20mixing.%20Information%20therefore%20propagates%20by%20controlled%20deformations%20of%20low-rank%20subspaces%20over%20multi-scale%20local%20windows%2C%20so%20the%20core%20computation%20lives%20on%20a%20finite-dimensional%20manifold%20rather%20than%20in%20an%20unstructured%20tensor%20space.%0A%20%20On%20the%20Wikitext-2%20language%20modeling%20benchmark%2C%20purely%20Grassmann-based%20models%20with%2013%20to%2018%20million%20parameters%20achieve%20validation%20perplexities%20within%20about%2010%20to%2015%20percent%20of%20size-matched%20Transformers.%20On%20the%20SNLI%20natural%20language%20inference%20task%2C%20a%20Grassmann-Plucker%20head%20on%20top%20of%20DistilBERT%20slightly%20outperforms%20a%20Transformer%20head%2C%20with%20best%20validation%20and%20test%20accuracies%20of%200.8550%20and%200.8538%20compared%20to%200.8545%20and%200.8511.%20We%20analyze%20the%20complexity%20of%20Grassmann%20mixing%2C%20show%20linear%20scaling%20in%20sequence%20length%20for%20fixed%20rank%2C%20and%20argue%20that%20such%20manifold-based%20designs%20offer%20a%20more%20structured%20route%20toward%20geometric%20and%20invariant-based%20interpretations%20of%20neural%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2512.19428v1&entry.124074799=Read"},
{"title": "LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller", "author": "Kirill Djebko and Tom Baumann and Erik Dilger and Frank Puppe and Sergio Montenegro", "abstract": "Attitude control is essential for many satellite missions. Classical controllers, however, are time-consuming to design and sensitive to model uncertainties and variations in operational boundary conditions. Deep Reinforcement Learning (DRL) offers a promising alternative by learning adaptive control strategies through autonomous interaction with a simulation environment. Overcoming the Sim2Real gap, which involves deploying an agent trained in simulation onto the real physical satellite, remains a significant challenge. In this work, we present the first successful in-orbit demonstration of an AI-based attitude controller for inertial pointing maneuvers. The controller was trained entirely in simulation and deployed to the InnoCube 3U nanosatellite, which was developed by the Julius-Maximilians-Universit\u00e4t W\u00fcrzburg in cooperation with the Technische Universit\u00e4t Berlin, and launched in January 2025. We present the AI agent design, the methodology of the training procedure, the discrepancies between the simulation and the observed behavior of the real satellite, and a comparison of the AI-based attitude controller with the classical PD controller of InnoCube. Steady-state metrics confirm the robust performance of the AI-based controller during repeated in-orbit maneuvers.", "link": "http://arxiv.org/abs/2512.19576v1", "date": "2025-12-22", "relevancy": 2.0052, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5418}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4892}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LeLaR%3A%20The%20First%20In-Orbit%20Demonstration%20of%20an%20AI-Based%20Satellite%20Attitude%20Controller&body=Title%3A%20LeLaR%3A%20The%20First%20In-Orbit%20Demonstration%20of%20an%20AI-Based%20Satellite%20Attitude%20Controller%0AAuthor%3A%20Kirill%20Djebko%20and%20Tom%20Baumann%20and%20Erik%20Dilger%20and%20Frank%20Puppe%20and%20Sergio%20Montenegro%0AAbstract%3A%20Attitude%20control%20is%20essential%20for%20many%20satellite%20missions.%20Classical%20controllers%2C%20however%2C%20are%20time-consuming%20to%20design%20and%20sensitive%20to%20model%20uncertainties%20and%20variations%20in%20operational%20boundary%20conditions.%20Deep%20Reinforcement%20Learning%20%28DRL%29%20offers%20a%20promising%20alternative%20by%20learning%20adaptive%20control%20strategies%20through%20autonomous%20interaction%20with%20a%20simulation%20environment.%20Overcoming%20the%20Sim2Real%20gap%2C%20which%20involves%20deploying%20an%20agent%20trained%20in%20simulation%20onto%20the%20real%20physical%20satellite%2C%20remains%20a%20significant%20challenge.%20In%20this%20work%2C%20we%20present%20the%20first%20successful%20in-orbit%20demonstration%20of%20an%20AI-based%20attitude%20controller%20for%20inertial%20pointing%20maneuvers.%20The%20controller%20was%20trained%20entirely%20in%20simulation%20and%20deployed%20to%20the%20InnoCube%203U%20nanosatellite%2C%20which%20was%20developed%20by%20the%20Julius-Maximilians-Universit%C3%A4t%20W%C3%BCrzburg%20in%20cooperation%20with%20the%20Technische%20Universit%C3%A4t%20Berlin%2C%20and%20launched%20in%20January%202025.%20We%20present%20the%20AI%20agent%20design%2C%20the%20methodology%20of%20the%20training%20procedure%2C%20the%20discrepancies%20between%20the%20simulation%20and%20the%20observed%20behavior%20of%20the%20real%20satellite%2C%20and%20a%20comparison%20of%20the%20AI-based%20attitude%20controller%20with%20the%20classical%20PD%20controller%20of%20InnoCube.%20Steady-state%20metrics%20confirm%20the%20robust%20performance%20of%20the%20AI-based%20controller%20during%20repeated%20in-orbit%20maneuvers.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19576v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeLaR%253A%2520The%2520First%2520In-Orbit%2520Demonstration%2520of%2520an%2520AI-Based%2520Satellite%2520Attitude%2520Controller%26entry.906535625%3DKirill%2520Djebko%2520and%2520Tom%2520Baumann%2520and%2520Erik%2520Dilger%2520and%2520Frank%2520Puppe%2520and%2520Sergio%2520Montenegro%26entry.1292438233%3DAttitude%2520control%2520is%2520essential%2520for%2520many%2520satellite%2520missions.%2520Classical%2520controllers%252C%2520however%252C%2520are%2520time-consuming%2520to%2520design%2520and%2520sensitive%2520to%2520model%2520uncertainties%2520and%2520variations%2520in%2520operational%2520boundary%2520conditions.%2520Deep%2520Reinforcement%2520Learning%2520%2528DRL%2529%2520offers%2520a%2520promising%2520alternative%2520by%2520learning%2520adaptive%2520control%2520strategies%2520through%2520autonomous%2520interaction%2520with%2520a%2520simulation%2520environment.%2520Overcoming%2520the%2520Sim2Real%2520gap%252C%2520which%2520involves%2520deploying%2520an%2520agent%2520trained%2520in%2520simulation%2520onto%2520the%2520real%2520physical%2520satellite%252C%2520remains%2520a%2520significant%2520challenge.%2520In%2520this%2520work%252C%2520we%2520present%2520the%2520first%2520successful%2520in-orbit%2520demonstration%2520of%2520an%2520AI-based%2520attitude%2520controller%2520for%2520inertial%2520pointing%2520maneuvers.%2520The%2520controller%2520was%2520trained%2520entirely%2520in%2520simulation%2520and%2520deployed%2520to%2520the%2520InnoCube%25203U%2520nanosatellite%252C%2520which%2520was%2520developed%2520by%2520the%2520Julius-Maximilians-Universit%25C3%25A4t%2520W%25C3%25BCrzburg%2520in%2520cooperation%2520with%2520the%2520Technische%2520Universit%25C3%25A4t%2520Berlin%252C%2520and%2520launched%2520in%2520January%25202025.%2520We%2520present%2520the%2520AI%2520agent%2520design%252C%2520the%2520methodology%2520of%2520the%2520training%2520procedure%252C%2520the%2520discrepancies%2520between%2520the%2520simulation%2520and%2520the%2520observed%2520behavior%2520of%2520the%2520real%2520satellite%252C%2520and%2520a%2520comparison%2520of%2520the%2520AI-based%2520attitude%2520controller%2520with%2520the%2520classical%2520PD%2520controller%2520of%2520InnoCube.%2520Steady-state%2520metrics%2520confirm%2520the%2520robust%2520performance%2520of%2520the%2520AI-based%2520controller%2520during%2520repeated%2520in-orbit%2520maneuvers.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19576v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LeLaR%3A%20The%20First%20In-Orbit%20Demonstration%20of%20an%20AI-Based%20Satellite%20Attitude%20Controller&entry.906535625=Kirill%20Djebko%20and%20Tom%20Baumann%20and%20Erik%20Dilger%20and%20Frank%20Puppe%20and%20Sergio%20Montenegro&entry.1292438233=Attitude%20control%20is%20essential%20for%20many%20satellite%20missions.%20Classical%20controllers%2C%20however%2C%20are%20time-consuming%20to%20design%20and%20sensitive%20to%20model%20uncertainties%20and%20variations%20in%20operational%20boundary%20conditions.%20Deep%20Reinforcement%20Learning%20%28DRL%29%20offers%20a%20promising%20alternative%20by%20learning%20adaptive%20control%20strategies%20through%20autonomous%20interaction%20with%20a%20simulation%20environment.%20Overcoming%20the%20Sim2Real%20gap%2C%20which%20involves%20deploying%20an%20agent%20trained%20in%20simulation%20onto%20the%20real%20physical%20satellite%2C%20remains%20a%20significant%20challenge.%20In%20this%20work%2C%20we%20present%20the%20first%20successful%20in-orbit%20demonstration%20of%20an%20AI-based%20attitude%20controller%20for%20inertial%20pointing%20maneuvers.%20The%20controller%20was%20trained%20entirely%20in%20simulation%20and%20deployed%20to%20the%20InnoCube%203U%20nanosatellite%2C%20which%20was%20developed%20by%20the%20Julius-Maximilians-Universit%C3%A4t%20W%C3%BCrzburg%20in%20cooperation%20with%20the%20Technische%20Universit%C3%A4t%20Berlin%2C%20and%20launched%20in%20January%202025.%20We%20present%20the%20AI%20agent%20design%2C%20the%20methodology%20of%20the%20training%20procedure%2C%20the%20discrepancies%20between%20the%20simulation%20and%20the%20observed%20behavior%20of%20the%20real%20satellite%2C%20and%20a%20comparison%20of%20the%20AI-based%20attitude%20controller%20with%20the%20classical%20PD%20controller%20of%20InnoCube.%20Steady-state%20metrics%20confirm%20the%20robust%20performance%20of%20the%20AI-based%20controller%20during%20repeated%20in-orbit%20maneuvers.&entry.1838667208=http%3A//arxiv.org/abs/2512.19576v1&entry.124074799=Read"},
{"title": "The Best of Both Worlds: Hybridizing Neural Operators and Solvers for Stable Long-Horizon Inference", "author": "Rajyasri Roy and Dibyajyoti Nayak and Somdatta Goswami", "abstract": "Numerical simulation of time-dependent partial differential equations (PDEs) is central to scientific and engineering applications, but high-fidelity solvers are often prohibitively expensive for long-horizon or time-critical settings. Neural operator (NO) surrogates offer fast inference across parametric and functional inputs; however, most autoregressive NO frameworks remain vulnerable to compounding errors, and ensemble-averaged metrics provide limited guarantees for individual inference trajectories. In practice, error accumulation can become unacceptable beyond the training horizon, and existing methods lack mechanisms for online monitoring or correction. To address this gap, we propose ANCHOR (Adaptive Numerical Correction for High-fidelity Operator Rollouts), an online, instance-aware hybrid inference framework for stable long-horizon prediction of nonlinear, time-dependent PDEs. ANCHOR treats a pretrained NO as the primary inference engine and adaptively couples it with a classical numerical solver using a physics-informed, residual-based error estimator. Inspired by adaptive time-stepping in numerical analysis, ANCHOR monitors an exponential moving average (EMA) of the normalized PDE residual to detect accumulating error and trigger corrective solver interventions without requiring access to ground-truth solutions. We show that the EMA-based estimator correlates strongly with the true relative L2 error, enabling data-free, instance-aware error control during inference. Evaluations on four canonical PDEs: 1D and 2D Burgers', 2D Allen-Cahn, and 3D heat conduction, demonstrate that ANCHOR reliably bounds long-horizon error growth, stabilizes extrapolative rollouts, and significantly improves robustness over standalone neural operators, while remaining substantially more efficient than high-fidelity numerical solvers.", "link": "http://arxiv.org/abs/2512.19643v1", "date": "2025-12-22", "relevancy": 2.0079, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5282}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4995}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Best%20of%20Both%20Worlds%3A%20Hybridizing%20Neural%20Operators%20and%20Solvers%20for%20Stable%20Long-Horizon%20Inference&body=Title%3A%20The%20Best%20of%20Both%20Worlds%3A%20Hybridizing%20Neural%20Operators%20and%20Solvers%20for%20Stable%20Long-Horizon%20Inference%0AAuthor%3A%20Rajyasri%20Roy%20and%20Dibyajyoti%20Nayak%20and%20Somdatta%20Goswami%0AAbstract%3A%20Numerical%20simulation%20of%20time-dependent%20partial%20differential%20equations%20%28PDEs%29%20is%20central%20to%20scientific%20and%20engineering%20applications%2C%20but%20high-fidelity%20solvers%20are%20often%20prohibitively%20expensive%20for%20long-horizon%20or%20time-critical%20settings.%20Neural%20operator%20%28NO%29%20surrogates%20offer%20fast%20inference%20across%20parametric%20and%20functional%20inputs%3B%20however%2C%20most%20autoregressive%20NO%20frameworks%20remain%20vulnerable%20to%20compounding%20errors%2C%20and%20ensemble-averaged%20metrics%20provide%20limited%20guarantees%20for%20individual%20inference%20trajectories.%20In%20practice%2C%20error%20accumulation%20can%20become%20unacceptable%20beyond%20the%20training%20horizon%2C%20and%20existing%20methods%20lack%20mechanisms%20for%20online%20monitoring%20or%20correction.%20To%20address%20this%20gap%2C%20we%20propose%20ANCHOR%20%28Adaptive%20Numerical%20Correction%20for%20High-fidelity%20Operator%20Rollouts%29%2C%20an%20online%2C%20instance-aware%20hybrid%20inference%20framework%20for%20stable%20long-horizon%20prediction%20of%20nonlinear%2C%20time-dependent%20PDEs.%20ANCHOR%20treats%20a%20pretrained%20NO%20as%20the%20primary%20inference%20engine%20and%20adaptively%20couples%20it%20with%20a%20classical%20numerical%20solver%20using%20a%20physics-informed%2C%20residual-based%20error%20estimator.%20Inspired%20by%20adaptive%20time-stepping%20in%20numerical%20analysis%2C%20ANCHOR%20monitors%20an%20exponential%20moving%20average%20%28EMA%29%20of%20the%20normalized%20PDE%20residual%20to%20detect%20accumulating%20error%20and%20trigger%20corrective%20solver%20interventions%20without%20requiring%20access%20to%20ground-truth%20solutions.%20We%20show%20that%20the%20EMA-based%20estimator%20correlates%20strongly%20with%20the%20true%20relative%20L2%20error%2C%20enabling%20data-free%2C%20instance-aware%20error%20control%20during%20inference.%20Evaluations%20on%20four%20canonical%20PDEs%3A%201D%20and%202D%20Burgers%27%2C%202D%20Allen-Cahn%2C%20and%203D%20heat%20conduction%2C%20demonstrate%20that%20ANCHOR%20reliably%20bounds%20long-horizon%20error%20growth%2C%20stabilizes%20extrapolative%20rollouts%2C%20and%20significantly%20improves%20robustness%20over%20standalone%20neural%20operators%2C%20while%20remaining%20substantially%20more%20efficient%20than%20high-fidelity%20numerical%20solvers.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19643v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Best%2520of%2520Both%2520Worlds%253A%2520Hybridizing%2520Neural%2520Operators%2520and%2520Solvers%2520for%2520Stable%2520Long-Horizon%2520Inference%26entry.906535625%3DRajyasri%2520Roy%2520and%2520Dibyajyoti%2520Nayak%2520and%2520Somdatta%2520Goswami%26entry.1292438233%3DNumerical%2520simulation%2520of%2520time-dependent%2520partial%2520differential%2520equations%2520%2528PDEs%2529%2520is%2520central%2520to%2520scientific%2520and%2520engineering%2520applications%252C%2520but%2520high-fidelity%2520solvers%2520are%2520often%2520prohibitively%2520expensive%2520for%2520long-horizon%2520or%2520time-critical%2520settings.%2520Neural%2520operator%2520%2528NO%2529%2520surrogates%2520offer%2520fast%2520inference%2520across%2520parametric%2520and%2520functional%2520inputs%253B%2520however%252C%2520most%2520autoregressive%2520NO%2520frameworks%2520remain%2520vulnerable%2520to%2520compounding%2520errors%252C%2520and%2520ensemble-averaged%2520metrics%2520provide%2520limited%2520guarantees%2520for%2520individual%2520inference%2520trajectories.%2520In%2520practice%252C%2520error%2520accumulation%2520can%2520become%2520unacceptable%2520beyond%2520the%2520training%2520horizon%252C%2520and%2520existing%2520methods%2520lack%2520mechanisms%2520for%2520online%2520monitoring%2520or%2520correction.%2520To%2520address%2520this%2520gap%252C%2520we%2520propose%2520ANCHOR%2520%2528Adaptive%2520Numerical%2520Correction%2520for%2520High-fidelity%2520Operator%2520Rollouts%2529%252C%2520an%2520online%252C%2520instance-aware%2520hybrid%2520inference%2520framework%2520for%2520stable%2520long-horizon%2520prediction%2520of%2520nonlinear%252C%2520time-dependent%2520PDEs.%2520ANCHOR%2520treats%2520a%2520pretrained%2520NO%2520as%2520the%2520primary%2520inference%2520engine%2520and%2520adaptively%2520couples%2520it%2520with%2520a%2520classical%2520numerical%2520solver%2520using%2520a%2520physics-informed%252C%2520residual-based%2520error%2520estimator.%2520Inspired%2520by%2520adaptive%2520time-stepping%2520in%2520numerical%2520analysis%252C%2520ANCHOR%2520monitors%2520an%2520exponential%2520moving%2520average%2520%2528EMA%2529%2520of%2520the%2520normalized%2520PDE%2520residual%2520to%2520detect%2520accumulating%2520error%2520and%2520trigger%2520corrective%2520solver%2520interventions%2520without%2520requiring%2520access%2520to%2520ground-truth%2520solutions.%2520We%2520show%2520that%2520the%2520EMA-based%2520estimator%2520correlates%2520strongly%2520with%2520the%2520true%2520relative%2520L2%2520error%252C%2520enabling%2520data-free%252C%2520instance-aware%2520error%2520control%2520during%2520inference.%2520Evaluations%2520on%2520four%2520canonical%2520PDEs%253A%25201D%2520and%25202D%2520Burgers%2527%252C%25202D%2520Allen-Cahn%252C%2520and%25203D%2520heat%2520conduction%252C%2520demonstrate%2520that%2520ANCHOR%2520reliably%2520bounds%2520long-horizon%2520error%2520growth%252C%2520stabilizes%2520extrapolative%2520rollouts%252C%2520and%2520significantly%2520improves%2520robustness%2520over%2520standalone%2520neural%2520operators%252C%2520while%2520remaining%2520substantially%2520more%2520efficient%2520than%2520high-fidelity%2520numerical%2520solvers.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19643v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Best%20of%20Both%20Worlds%3A%20Hybridizing%20Neural%20Operators%20and%20Solvers%20for%20Stable%20Long-Horizon%20Inference&entry.906535625=Rajyasri%20Roy%20and%20Dibyajyoti%20Nayak%20and%20Somdatta%20Goswami&entry.1292438233=Numerical%20simulation%20of%20time-dependent%20partial%20differential%20equations%20%28PDEs%29%20is%20central%20to%20scientific%20and%20engineering%20applications%2C%20but%20high-fidelity%20solvers%20are%20often%20prohibitively%20expensive%20for%20long-horizon%20or%20time-critical%20settings.%20Neural%20operator%20%28NO%29%20surrogates%20offer%20fast%20inference%20across%20parametric%20and%20functional%20inputs%3B%20however%2C%20most%20autoregressive%20NO%20frameworks%20remain%20vulnerable%20to%20compounding%20errors%2C%20and%20ensemble-averaged%20metrics%20provide%20limited%20guarantees%20for%20individual%20inference%20trajectories.%20In%20practice%2C%20error%20accumulation%20can%20become%20unacceptable%20beyond%20the%20training%20horizon%2C%20and%20existing%20methods%20lack%20mechanisms%20for%20online%20monitoring%20or%20correction.%20To%20address%20this%20gap%2C%20we%20propose%20ANCHOR%20%28Adaptive%20Numerical%20Correction%20for%20High-fidelity%20Operator%20Rollouts%29%2C%20an%20online%2C%20instance-aware%20hybrid%20inference%20framework%20for%20stable%20long-horizon%20prediction%20of%20nonlinear%2C%20time-dependent%20PDEs.%20ANCHOR%20treats%20a%20pretrained%20NO%20as%20the%20primary%20inference%20engine%20and%20adaptively%20couples%20it%20with%20a%20classical%20numerical%20solver%20using%20a%20physics-informed%2C%20residual-based%20error%20estimator.%20Inspired%20by%20adaptive%20time-stepping%20in%20numerical%20analysis%2C%20ANCHOR%20monitors%20an%20exponential%20moving%20average%20%28EMA%29%20of%20the%20normalized%20PDE%20residual%20to%20detect%20accumulating%20error%20and%20trigger%20corrective%20solver%20interventions%20without%20requiring%20access%20to%20ground-truth%20solutions.%20We%20show%20that%20the%20EMA-based%20estimator%20correlates%20strongly%20with%20the%20true%20relative%20L2%20error%2C%20enabling%20data-free%2C%20instance-aware%20error%20control%20during%20inference.%20Evaluations%20on%20four%20canonical%20PDEs%3A%201D%20and%202D%20Burgers%27%2C%202D%20Allen-Cahn%2C%20and%203D%20heat%20conduction%2C%20demonstrate%20that%20ANCHOR%20reliably%20bounds%20long-horizon%20error%20growth%2C%20stabilizes%20extrapolative%20rollouts%2C%20and%20significantly%20improves%20robustness%20over%20standalone%20neural%20operators%2C%20while%20remaining%20substantially%20more%20efficient%20than%20high-fidelity%20numerical%20solvers.&entry.1838667208=http%3A//arxiv.org/abs/2512.19643v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


