<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250603.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "VTGaussian-SLAM: RGBD SLAM for Large Scale Scenes with Splatting\n  View-Tied 3D Gaussians", "author": "Pengchong Hu and Zhizhong Han", "abstract": "  Jointly estimating camera poses and mapping scenes from RGBD images is a\nfundamental task in simultaneous localization and mapping (SLAM).\nState-of-the-art methods employ 3D Gaussians to represent a scene, and render\nthese Gaussians through splatting for higher efficiency and better rendering.\nHowever, these methods cannot scale up to extremely large scenes, due to the\ninefficient tracking and mapping strategies that need to optimize all 3D\nGaussians in the limited GPU memories throughout the training to maintain the\ngeometry and color consistency to previous RGBD observations. To resolve this\nissue, we propose novel tracking and mapping strategies to work with a novel 3D\nrepresentation, dubbed view-tied 3D Gaussians, for RGBD SLAM systems. View-tied\n3D Gaussians is a kind of simplified Gaussians, which is tied to depth pixels,\nwithout needing to learn locations, rotations, and multi-dimensional variances.\nTying Gaussians to views not only significantly saves storage but also allows\nus to employ many more Gaussians to represent local details in the limited GPU\nmemory. Moreover, our strategies remove the need of maintaining all Gaussians\nlearnable throughout the training, while improving rendering quality, and\ntracking accuracy. We justify the effectiveness of these designs, and report\nbetter performance over the latest methods on the widely used benchmarks in\nterms of rendering and tracking accuracy and scalability. Please see our\nproject page for code and videos at\nhttps://machineperceptionlab.github.io/VTGaussian-SLAM-Project .\n", "link": "http://arxiv.org/abs/2506.02741v1", "date": "2025-06-03", "relevancy": 3.6639, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.8242}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7134}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VTGaussian-SLAM%3A%20RGBD%20SLAM%20for%20Large%20Scale%20Scenes%20with%20Splatting%0A%20%20View-Tied%203D%20Gaussians&body=Title%3A%20VTGaussian-SLAM%3A%20RGBD%20SLAM%20for%20Large%20Scale%20Scenes%20with%20Splatting%0A%20%20View-Tied%203D%20Gaussians%0AAuthor%3A%20Pengchong%20Hu%20and%20Zhizhong%20Han%0AAbstract%3A%20%20%20Jointly%20estimating%20camera%20poses%20and%20mapping%20scenes%20from%20RGBD%20images%20is%20a%0Afundamental%20task%20in%20simultaneous%20localization%20and%20mapping%20%28SLAM%29.%0AState-of-the-art%20methods%20employ%203D%20Gaussians%20to%20represent%20a%20scene%2C%20and%20render%0Athese%20Gaussians%20through%20splatting%20for%20higher%20efficiency%20and%20better%20rendering.%0AHowever%2C%20these%20methods%20cannot%20scale%20up%20to%20extremely%20large%20scenes%2C%20due%20to%20the%0Ainefficient%20tracking%20and%20mapping%20strategies%20that%20need%20to%20optimize%20all%203D%0AGaussians%20in%20the%20limited%20GPU%20memories%20throughout%20the%20training%20to%20maintain%20the%0Ageometry%20and%20color%20consistency%20to%20previous%20RGBD%20observations.%20To%20resolve%20this%0Aissue%2C%20we%20propose%20novel%20tracking%20and%20mapping%20strategies%20to%20work%20with%20a%20novel%203D%0Arepresentation%2C%20dubbed%20view-tied%203D%20Gaussians%2C%20for%20RGBD%20SLAM%20systems.%20View-tied%0A3D%20Gaussians%20is%20a%20kind%20of%20simplified%20Gaussians%2C%20which%20is%20tied%20to%20depth%20pixels%2C%0Awithout%20needing%20to%20learn%20locations%2C%20rotations%2C%20and%20multi-dimensional%20variances.%0ATying%20Gaussians%20to%20views%20not%20only%20significantly%20saves%20storage%20but%20also%20allows%0Aus%20to%20employ%20many%20more%20Gaussians%20to%20represent%20local%20details%20in%20the%20limited%20GPU%0Amemory.%20Moreover%2C%20our%20strategies%20remove%20the%20need%20of%20maintaining%20all%20Gaussians%0Alearnable%20throughout%20the%20training%2C%20while%20improving%20rendering%20quality%2C%20and%0Atracking%20accuracy.%20We%20justify%20the%20effectiveness%20of%20these%20designs%2C%20and%20report%0Abetter%20performance%20over%20the%20latest%20methods%20on%20the%20widely%20used%20benchmarks%20in%0Aterms%20of%20rendering%20and%20tracking%20accuracy%20and%20scalability.%20Please%20see%20our%0Aproject%20page%20for%20code%20and%20videos%20at%0Ahttps%3A//machineperceptionlab.github.io/VTGaussian-SLAM-Project%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02741v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVTGaussian-SLAM%253A%2520RGBD%2520SLAM%2520for%2520Large%2520Scale%2520Scenes%2520with%2520Splatting%250A%2520%2520View-Tied%25203D%2520Gaussians%26entry.906535625%3DPengchong%2520Hu%2520and%2520Zhizhong%2520Han%26entry.1292438233%3D%2520%2520Jointly%2520estimating%2520camera%2520poses%2520and%2520mapping%2520scenes%2520from%2520RGBD%2520images%2520is%2520a%250Afundamental%2520task%2520in%2520simultaneous%2520localization%2520and%2520mapping%2520%2528SLAM%2529.%250AState-of-the-art%2520methods%2520employ%25203D%2520Gaussians%2520to%2520represent%2520a%2520scene%252C%2520and%2520render%250Athese%2520Gaussians%2520through%2520splatting%2520for%2520higher%2520efficiency%2520and%2520better%2520rendering.%250AHowever%252C%2520these%2520methods%2520cannot%2520scale%2520up%2520to%2520extremely%2520large%2520scenes%252C%2520due%2520to%2520the%250Ainefficient%2520tracking%2520and%2520mapping%2520strategies%2520that%2520need%2520to%2520optimize%2520all%25203D%250AGaussians%2520in%2520the%2520limited%2520GPU%2520memories%2520throughout%2520the%2520training%2520to%2520maintain%2520the%250Ageometry%2520and%2520color%2520consistency%2520to%2520previous%2520RGBD%2520observations.%2520To%2520resolve%2520this%250Aissue%252C%2520we%2520propose%2520novel%2520tracking%2520and%2520mapping%2520strategies%2520to%2520work%2520with%2520a%2520novel%25203D%250Arepresentation%252C%2520dubbed%2520view-tied%25203D%2520Gaussians%252C%2520for%2520RGBD%2520SLAM%2520systems.%2520View-tied%250A3D%2520Gaussians%2520is%2520a%2520kind%2520of%2520simplified%2520Gaussians%252C%2520which%2520is%2520tied%2520to%2520depth%2520pixels%252C%250Awithout%2520needing%2520to%2520learn%2520locations%252C%2520rotations%252C%2520and%2520multi-dimensional%2520variances.%250ATying%2520Gaussians%2520to%2520views%2520not%2520only%2520significantly%2520saves%2520storage%2520but%2520also%2520allows%250Aus%2520to%2520employ%2520many%2520more%2520Gaussians%2520to%2520represent%2520local%2520details%2520in%2520the%2520limited%2520GPU%250Amemory.%2520Moreover%252C%2520our%2520strategies%2520remove%2520the%2520need%2520of%2520maintaining%2520all%2520Gaussians%250Alearnable%2520throughout%2520the%2520training%252C%2520while%2520improving%2520rendering%2520quality%252C%2520and%250Atracking%2520accuracy.%2520We%2520justify%2520the%2520effectiveness%2520of%2520these%2520designs%252C%2520and%2520report%250Abetter%2520performance%2520over%2520the%2520latest%2520methods%2520on%2520the%2520widely%2520used%2520benchmarks%2520in%250Aterms%2520of%2520rendering%2520and%2520tracking%2520accuracy%2520and%2520scalability.%2520Please%2520see%2520our%250Aproject%2520page%2520for%2520code%2520and%2520videos%2520at%250Ahttps%253A//machineperceptionlab.github.io/VTGaussian-SLAM-Project%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02741v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VTGaussian-SLAM%3A%20RGBD%20SLAM%20for%20Large%20Scale%20Scenes%20with%20Splatting%0A%20%20View-Tied%203D%20Gaussians&entry.906535625=Pengchong%20Hu%20and%20Zhizhong%20Han&entry.1292438233=%20%20Jointly%20estimating%20camera%20poses%20and%20mapping%20scenes%20from%20RGBD%20images%20is%20a%0Afundamental%20task%20in%20simultaneous%20localization%20and%20mapping%20%28SLAM%29.%0AState-of-the-art%20methods%20employ%203D%20Gaussians%20to%20represent%20a%20scene%2C%20and%20render%0Athese%20Gaussians%20through%20splatting%20for%20higher%20efficiency%20and%20better%20rendering.%0AHowever%2C%20these%20methods%20cannot%20scale%20up%20to%20extremely%20large%20scenes%2C%20due%20to%20the%0Ainefficient%20tracking%20and%20mapping%20strategies%20that%20need%20to%20optimize%20all%203D%0AGaussians%20in%20the%20limited%20GPU%20memories%20throughout%20the%20training%20to%20maintain%20the%0Ageometry%20and%20color%20consistency%20to%20previous%20RGBD%20observations.%20To%20resolve%20this%0Aissue%2C%20we%20propose%20novel%20tracking%20and%20mapping%20strategies%20to%20work%20with%20a%20novel%203D%0Arepresentation%2C%20dubbed%20view-tied%203D%20Gaussians%2C%20for%20RGBD%20SLAM%20systems.%20View-tied%0A3D%20Gaussians%20is%20a%20kind%20of%20simplified%20Gaussians%2C%20which%20is%20tied%20to%20depth%20pixels%2C%0Awithout%20needing%20to%20learn%20locations%2C%20rotations%2C%20and%20multi-dimensional%20variances.%0ATying%20Gaussians%20to%20views%20not%20only%20significantly%20saves%20storage%20but%20also%20allows%0Aus%20to%20employ%20many%20more%20Gaussians%20to%20represent%20local%20details%20in%20the%20limited%20GPU%0Amemory.%20Moreover%2C%20our%20strategies%20remove%20the%20need%20of%20maintaining%20all%20Gaussians%0Alearnable%20throughout%20the%20training%2C%20while%20improving%20rendering%20quality%2C%20and%0Atracking%20accuracy.%20We%20justify%20the%20effectiveness%20of%20these%20designs%2C%20and%20report%0Abetter%20performance%20over%20the%20latest%20methods%20on%20the%20widely%20used%20benchmarks%20in%0Aterms%20of%20rendering%20and%20tracking%20accuracy%20and%20scalability.%20Please%20see%20our%0Aproject%20page%20for%20code%20and%20videos%20at%0Ahttps%3A//machineperceptionlab.github.io/VTGaussian-SLAM-Project%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02741v1&entry.124074799=Read"},
{"title": "RobustSplat: Decoupling Densification and Dynamics for Transient-Free\n  3DGS", "author": "Chuanyu Fu and Yuqi Zhang and Kunbin Yao and Guanying Chen and Yuan Xiong and Chuan Huang and Shuguang Cui and Xiaochun Cao", "abstract": "  3D Gaussian Splatting (3DGS) has gained significant attention for its\nreal-time, photo-realistic rendering in novel-view synthesis and 3D modeling.\nHowever, existing methods struggle with accurately modeling scenes affected by\ntransient objects, leading to artifacts in the rendered images. We identify\nthat the Gaussian densification process, while enhancing scene detail capture,\nunintentionally contributes to these artifacts by growing additional Gaussians\nthat model transient disturbances. To address this, we propose RobustSplat, a\nrobust solution based on two critical designs. First, we introduce a delayed\nGaussian growth strategy that prioritizes optimizing static scene structure\nbefore allowing Gaussian splitting/cloning, mitigating overfitting to transient\nobjects in early optimization. Second, we design a scale-cascaded mask\nbootstrapping approach that first leverages lower-resolution feature similarity\nsupervision for reliable initial transient mask estimation, taking advantage of\nits stronger semantic consistency and robustness to noise, and then progresses\nto high-resolution supervision to achieve more precise mask prediction.\nExtensive experiments on multiple challenging datasets show that our method\noutperforms existing methods, clearly demonstrating the robustness and\neffectiveness of our method. Our project page is\nhttps://fcyycf.github.io/RobustSplat/.\n", "link": "http://arxiv.org/abs/2506.02751v1", "date": "2025-06-03", "relevancy": 3.5334, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7298}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.7141}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RobustSplat%3A%20Decoupling%20Densification%20and%20Dynamics%20for%20Transient-Free%0A%20%203DGS&body=Title%3A%20RobustSplat%3A%20Decoupling%20Densification%20and%20Dynamics%20for%20Transient-Free%0A%20%203DGS%0AAuthor%3A%20Chuanyu%20Fu%20and%20Yuqi%20Zhang%20and%20Kunbin%20Yao%20and%20Guanying%20Chen%20and%20Yuan%20Xiong%20and%20Chuan%20Huang%20and%20Shuguang%20Cui%20and%20Xiaochun%20Cao%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20gained%20significant%20attention%20for%20its%0Areal-time%2C%20photo-realistic%20rendering%20in%20novel-view%20synthesis%20and%203D%20modeling.%0AHowever%2C%20existing%20methods%20struggle%20with%20accurately%20modeling%20scenes%20affected%20by%0Atransient%20objects%2C%20leading%20to%20artifacts%20in%20the%20rendered%20images.%20We%20identify%0Athat%20the%20Gaussian%20densification%20process%2C%20while%20enhancing%20scene%20detail%20capture%2C%0Aunintentionally%20contributes%20to%20these%20artifacts%20by%20growing%20additional%20Gaussians%0Athat%20model%20transient%20disturbances.%20To%20address%20this%2C%20we%20propose%20RobustSplat%2C%20a%0Arobust%20solution%20based%20on%20two%20critical%20designs.%20First%2C%20we%20introduce%20a%20delayed%0AGaussian%20growth%20strategy%20that%20prioritizes%20optimizing%20static%20scene%20structure%0Abefore%20allowing%20Gaussian%20splitting/cloning%2C%20mitigating%20overfitting%20to%20transient%0Aobjects%20in%20early%20optimization.%20Second%2C%20we%20design%20a%20scale-cascaded%20mask%0Abootstrapping%20approach%20that%20first%20leverages%20lower-resolution%20feature%20similarity%0Asupervision%20for%20reliable%20initial%20transient%20mask%20estimation%2C%20taking%20advantage%20of%0Aits%20stronger%20semantic%20consistency%20and%20robustness%20to%20noise%2C%20and%20then%20progresses%0Ato%20high-resolution%20supervision%20to%20achieve%20more%20precise%20mask%20prediction.%0AExtensive%20experiments%20on%20multiple%20challenging%20datasets%20show%20that%20our%20method%0Aoutperforms%20existing%20methods%2C%20clearly%20demonstrating%20the%20robustness%20and%0Aeffectiveness%20of%20our%20method.%20Our%20project%20page%20is%0Ahttps%3A//fcyycf.github.io/RobustSplat/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02751v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobustSplat%253A%2520Decoupling%2520Densification%2520and%2520Dynamics%2520for%2520Transient-Free%250A%2520%25203DGS%26entry.906535625%3DChuanyu%2520Fu%2520and%2520Yuqi%2520Zhang%2520and%2520Kunbin%2520Yao%2520and%2520Guanying%2520Chen%2520and%2520Yuan%2520Xiong%2520and%2520Chuan%2520Huang%2520and%2520Shuguang%2520Cui%2520and%2520Xiaochun%2520Cao%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520gained%2520significant%2520attention%2520for%2520its%250Areal-time%252C%2520photo-realistic%2520rendering%2520in%2520novel-view%2520synthesis%2520and%25203D%2520modeling.%250AHowever%252C%2520existing%2520methods%2520struggle%2520with%2520accurately%2520modeling%2520scenes%2520affected%2520by%250Atransient%2520objects%252C%2520leading%2520to%2520artifacts%2520in%2520the%2520rendered%2520images.%2520We%2520identify%250Athat%2520the%2520Gaussian%2520densification%2520process%252C%2520while%2520enhancing%2520scene%2520detail%2520capture%252C%250Aunintentionally%2520contributes%2520to%2520these%2520artifacts%2520by%2520growing%2520additional%2520Gaussians%250Athat%2520model%2520transient%2520disturbances.%2520To%2520address%2520this%252C%2520we%2520propose%2520RobustSplat%252C%2520a%250Arobust%2520solution%2520based%2520on%2520two%2520critical%2520designs.%2520First%252C%2520we%2520introduce%2520a%2520delayed%250AGaussian%2520growth%2520strategy%2520that%2520prioritizes%2520optimizing%2520static%2520scene%2520structure%250Abefore%2520allowing%2520Gaussian%2520splitting/cloning%252C%2520mitigating%2520overfitting%2520to%2520transient%250Aobjects%2520in%2520early%2520optimization.%2520Second%252C%2520we%2520design%2520a%2520scale-cascaded%2520mask%250Abootstrapping%2520approach%2520that%2520first%2520leverages%2520lower-resolution%2520feature%2520similarity%250Asupervision%2520for%2520reliable%2520initial%2520transient%2520mask%2520estimation%252C%2520taking%2520advantage%2520of%250Aits%2520stronger%2520semantic%2520consistency%2520and%2520robustness%2520to%2520noise%252C%2520and%2520then%2520progresses%250Ato%2520high-resolution%2520supervision%2520to%2520achieve%2520more%2520precise%2520mask%2520prediction.%250AExtensive%2520experiments%2520on%2520multiple%2520challenging%2520datasets%2520show%2520that%2520our%2520method%250Aoutperforms%2520existing%2520methods%252C%2520clearly%2520demonstrating%2520the%2520robustness%2520and%250Aeffectiveness%2520of%2520our%2520method.%2520Our%2520project%2520page%2520is%250Ahttps%253A//fcyycf.github.io/RobustSplat/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02751v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RobustSplat%3A%20Decoupling%20Densification%20and%20Dynamics%20for%20Transient-Free%0A%20%203DGS&entry.906535625=Chuanyu%20Fu%20and%20Yuqi%20Zhang%20and%20Kunbin%20Yao%20and%20Guanying%20Chen%20and%20Yuan%20Xiong%20and%20Chuan%20Huang%20and%20Shuguang%20Cui%20and%20Xiaochun%20Cao&entry.1292438233=%20%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20gained%20significant%20attention%20for%20its%0Areal-time%2C%20photo-realistic%20rendering%20in%20novel-view%20synthesis%20and%203D%20modeling.%0AHowever%2C%20existing%20methods%20struggle%20with%20accurately%20modeling%20scenes%20affected%20by%0Atransient%20objects%2C%20leading%20to%20artifacts%20in%20the%20rendered%20images.%20We%20identify%0Athat%20the%20Gaussian%20densification%20process%2C%20while%20enhancing%20scene%20detail%20capture%2C%0Aunintentionally%20contributes%20to%20these%20artifacts%20by%20growing%20additional%20Gaussians%0Athat%20model%20transient%20disturbances.%20To%20address%20this%2C%20we%20propose%20RobustSplat%2C%20a%0Arobust%20solution%20based%20on%20two%20critical%20designs.%20First%2C%20we%20introduce%20a%20delayed%0AGaussian%20growth%20strategy%20that%20prioritizes%20optimizing%20static%20scene%20structure%0Abefore%20allowing%20Gaussian%20splitting/cloning%2C%20mitigating%20overfitting%20to%20transient%0Aobjects%20in%20early%20optimization.%20Second%2C%20we%20design%20a%20scale-cascaded%20mask%0Abootstrapping%20approach%20that%20first%20leverages%20lower-resolution%20feature%20similarity%0Asupervision%20for%20reliable%20initial%20transient%20mask%20estimation%2C%20taking%20advantage%20of%0Aits%20stronger%20semantic%20consistency%20and%20robustness%20to%20noise%2C%20and%20then%20progresses%0Ato%20high-resolution%20supervision%20to%20achieve%20more%20precise%20mask%20prediction.%0AExtensive%20experiments%20on%20multiple%20challenging%20datasets%20show%20that%20our%20method%0Aoutperforms%20existing%20methods%2C%20clearly%20demonstrating%20the%20robustness%20and%0Aeffectiveness%20of%20our%20method.%20Our%20project%20page%20is%0Ahttps%3A//fcyycf.github.io/RobustSplat/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02751v1&entry.124074799=Read"},
{"title": "LEG-SLAM: Real-Time Language-Enhanced Gaussian Splatting for SLAM", "author": "Roman Titkov and Egor Zubkov and Dmitry Yudin and Jaafar Mahmoud and Malik Mohrat and Gennady Sidorov", "abstract": "  Modern Gaussian Splatting methods have proven highly effective for real-time\nphotorealistic rendering of 3D scenes. However, integrating semantic\ninformation into this representation remains a significant challenge,\nespecially in maintaining real-time performance for SLAM (Simultaneous\nLocalization and Mapping) applications. In this work, we introduce LEG-SLAM --\na novel approach that fuses an optimized Gaussian Splatting implementation with\nvisual-language feature extraction using DINOv2 followed by a learnable feature\ncompressor based on Principal Component Analysis, while enabling an online\ndense SLAM. Our method simultaneously generates high-quality photorealistic\nimages and semantically labeled scene maps, achieving real-time scene\nreconstruction with more than 10 fps on the Replica dataset and 18 fps on\nScanNet. Experimental results show that our approach significantly outperforms\nstate-of-the-art methods in reconstruction speed while achieving competitive\nrendering quality. The proposed system eliminates the need for prior data\npreparation such as camera's ego motion or pre-computed static semantic maps.\nWith its potential applications in autonomous robotics, augmented reality, and\nother interactive domains, LEG-SLAM represents a significant step forward in\nreal-time semantic 3D Gaussian-based SLAM. Project page:\nhttps://titrom025.github.io/LEG-SLAM/\n", "link": "http://arxiv.org/abs/2506.03073v1", "date": "2025-06-03", "relevancy": 3.4081, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7999}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6317}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LEG-SLAM%3A%20Real-Time%20Language-Enhanced%20Gaussian%20Splatting%20for%20SLAM&body=Title%3A%20LEG-SLAM%3A%20Real-Time%20Language-Enhanced%20Gaussian%20Splatting%20for%20SLAM%0AAuthor%3A%20Roman%20Titkov%20and%20Egor%20Zubkov%20and%20Dmitry%20Yudin%20and%20Jaafar%20Mahmoud%20and%20Malik%20Mohrat%20and%20Gennady%20Sidorov%0AAbstract%3A%20%20%20Modern%20Gaussian%20Splatting%20methods%20have%20proven%20highly%20effective%20for%20real-time%0Aphotorealistic%20rendering%20of%203D%20scenes.%20However%2C%20integrating%20semantic%0Ainformation%20into%20this%20representation%20remains%20a%20significant%20challenge%2C%0Aespecially%20in%20maintaining%20real-time%20performance%20for%20SLAM%20%28Simultaneous%0ALocalization%20and%20Mapping%29%20applications.%20In%20this%20work%2C%20we%20introduce%20LEG-SLAM%20--%0Aa%20novel%20approach%20that%20fuses%20an%20optimized%20Gaussian%20Splatting%20implementation%20with%0Avisual-language%20feature%20extraction%20using%20DINOv2%20followed%20by%20a%20learnable%20feature%0Acompressor%20based%20on%20Principal%20Component%20Analysis%2C%20while%20enabling%20an%20online%0Adense%20SLAM.%20Our%20method%20simultaneously%20generates%20high-quality%20photorealistic%0Aimages%20and%20semantically%20labeled%20scene%20maps%2C%20achieving%20real-time%20scene%0Areconstruction%20with%20more%20than%2010%20fps%20on%20the%20Replica%20dataset%20and%2018%20fps%20on%0AScanNet.%20Experimental%20results%20show%20that%20our%20approach%20significantly%20outperforms%0Astate-of-the-art%20methods%20in%20reconstruction%20speed%20while%20achieving%20competitive%0Arendering%20quality.%20The%20proposed%20system%20eliminates%20the%20need%20for%20prior%20data%0Apreparation%20such%20as%20camera%27s%20ego%20motion%20or%20pre-computed%20static%20semantic%20maps.%0AWith%20its%20potential%20applications%20in%20autonomous%20robotics%2C%20augmented%20reality%2C%20and%0Aother%20interactive%20domains%2C%20LEG-SLAM%20represents%20a%20significant%20step%20forward%20in%0Areal-time%20semantic%203D%20Gaussian-based%20SLAM.%20Project%20page%3A%0Ahttps%3A//titrom025.github.io/LEG-SLAM/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03073v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLEG-SLAM%253A%2520Real-Time%2520Language-Enhanced%2520Gaussian%2520Splatting%2520for%2520SLAM%26entry.906535625%3DRoman%2520Titkov%2520and%2520Egor%2520Zubkov%2520and%2520Dmitry%2520Yudin%2520and%2520Jaafar%2520Mahmoud%2520and%2520Malik%2520Mohrat%2520and%2520Gennady%2520Sidorov%26entry.1292438233%3D%2520%2520Modern%2520Gaussian%2520Splatting%2520methods%2520have%2520proven%2520highly%2520effective%2520for%2520real-time%250Aphotorealistic%2520rendering%2520of%25203D%2520scenes.%2520However%252C%2520integrating%2520semantic%250Ainformation%2520into%2520this%2520representation%2520remains%2520a%2520significant%2520challenge%252C%250Aespecially%2520in%2520maintaining%2520real-time%2520performance%2520for%2520SLAM%2520%2528Simultaneous%250ALocalization%2520and%2520Mapping%2529%2520applications.%2520In%2520this%2520work%252C%2520we%2520introduce%2520LEG-SLAM%2520--%250Aa%2520novel%2520approach%2520that%2520fuses%2520an%2520optimized%2520Gaussian%2520Splatting%2520implementation%2520with%250Avisual-language%2520feature%2520extraction%2520using%2520DINOv2%2520followed%2520by%2520a%2520learnable%2520feature%250Acompressor%2520based%2520on%2520Principal%2520Component%2520Analysis%252C%2520while%2520enabling%2520an%2520online%250Adense%2520SLAM.%2520Our%2520method%2520simultaneously%2520generates%2520high-quality%2520photorealistic%250Aimages%2520and%2520semantically%2520labeled%2520scene%2520maps%252C%2520achieving%2520real-time%2520scene%250Areconstruction%2520with%2520more%2520than%252010%2520fps%2520on%2520the%2520Replica%2520dataset%2520and%252018%2520fps%2520on%250AScanNet.%2520Experimental%2520results%2520show%2520that%2520our%2520approach%2520significantly%2520outperforms%250Astate-of-the-art%2520methods%2520in%2520reconstruction%2520speed%2520while%2520achieving%2520competitive%250Arendering%2520quality.%2520The%2520proposed%2520system%2520eliminates%2520the%2520need%2520for%2520prior%2520data%250Apreparation%2520such%2520as%2520camera%2527s%2520ego%2520motion%2520or%2520pre-computed%2520static%2520semantic%2520maps.%250AWith%2520its%2520potential%2520applications%2520in%2520autonomous%2520robotics%252C%2520augmented%2520reality%252C%2520and%250Aother%2520interactive%2520domains%252C%2520LEG-SLAM%2520represents%2520a%2520significant%2520step%2520forward%2520in%250Areal-time%2520semantic%25203D%2520Gaussian-based%2520SLAM.%2520Project%2520page%253A%250Ahttps%253A//titrom025.github.io/LEG-SLAM/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03073v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LEG-SLAM%3A%20Real-Time%20Language-Enhanced%20Gaussian%20Splatting%20for%20SLAM&entry.906535625=Roman%20Titkov%20and%20Egor%20Zubkov%20and%20Dmitry%20Yudin%20and%20Jaafar%20Mahmoud%20and%20Malik%20Mohrat%20and%20Gennady%20Sidorov&entry.1292438233=%20%20Modern%20Gaussian%20Splatting%20methods%20have%20proven%20highly%20effective%20for%20real-time%0Aphotorealistic%20rendering%20of%203D%20scenes.%20However%2C%20integrating%20semantic%0Ainformation%20into%20this%20representation%20remains%20a%20significant%20challenge%2C%0Aespecially%20in%20maintaining%20real-time%20performance%20for%20SLAM%20%28Simultaneous%0ALocalization%20and%20Mapping%29%20applications.%20In%20this%20work%2C%20we%20introduce%20LEG-SLAM%20--%0Aa%20novel%20approach%20that%20fuses%20an%20optimized%20Gaussian%20Splatting%20implementation%20with%0Avisual-language%20feature%20extraction%20using%20DINOv2%20followed%20by%20a%20learnable%20feature%0Acompressor%20based%20on%20Principal%20Component%20Analysis%2C%20while%20enabling%20an%20online%0Adense%20SLAM.%20Our%20method%20simultaneously%20generates%20high-quality%20photorealistic%0Aimages%20and%20semantically%20labeled%20scene%20maps%2C%20achieving%20real-time%20scene%0Areconstruction%20with%20more%20than%2010%20fps%20on%20the%20Replica%20dataset%20and%2018%20fps%20on%0AScanNet.%20Experimental%20results%20show%20that%20our%20approach%20significantly%20outperforms%0Astate-of-the-art%20methods%20in%20reconstruction%20speed%20while%20achieving%20competitive%0Arendering%20quality.%20The%20proposed%20system%20eliminates%20the%20need%20for%20prior%20data%0Apreparation%20such%20as%20camera%27s%20ego%20motion%20or%20pre-computed%20static%20semantic%20maps.%0AWith%20its%20potential%20applications%20in%20autonomous%20robotics%2C%20augmented%20reality%2C%20and%0Aother%20interactive%20domains%2C%20LEG-SLAM%20represents%20a%20significant%20step%20forward%20in%0Areal-time%20semantic%203D%20Gaussian-based%20SLAM.%20Project%20page%3A%0Ahttps%3A//titrom025.github.io/LEG-SLAM/%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03073v1&entry.124074799=Read"},
{"title": "SceneSplat: Gaussian Splatting-based Scene Understanding with\n  Vision-Language Pretraining", "author": "Yue Li and Qi Ma and Runyi Yang and Huapeng Li and Mengjiao Ma and Bin Ren and Nikola Popovic and Nicu Sebe and Ender Konukoglu and Theo Gevers and Luc Van Gool and Martin R. Oswald and Danda Pani Paudel", "abstract": "  Recognizing arbitrary or previously unseen categories is essential for\ncomprehensive real-world 3D scene understanding. Currently, all existing\nmethods rely on 2D or textual modalities during training or together at\ninference. This highlights the clear absence of a model capable of processing\n3D data alone for learning semantics end-to-end, along with the necessary data\nto train such a model. Meanwhile, 3D Gaussian Splatting (3DGS) has emerged as\nthe de facto standard for 3D scene representation across various vision tasks.\nHowever, effectively integrating semantic reasoning into 3DGS in a\ngeneralizable manner remains an open challenge. To address these limitations,\nwe introduce SceneSplat, to our knowledge the first large-scale 3D indoor scene\nunderstanding approach that operates natively on 3DGS. Furthermore, we propose\na self-supervised learning scheme that unlocks rich 3D feature learning from\nunlabeled scenes. To power the proposed methods, we introduce SceneSplat-7K,\nthe first large-scale 3DGS dataset for indoor scenes, comprising 7916 scenes\nderived from seven established datasets, such as ScanNet and Matterport3D.\nGenerating SceneSplat-7K required computational resources equivalent to 150 GPU\ndays on an L4 GPU, enabling standardized benchmarking for 3DGS-based reasoning\nfor indoor scenes. Our exhaustive experiments on SceneSplat-7K demonstrate the\nsignificant benefit of the proposed method over the established baselines.\n", "link": "http://arxiv.org/abs/2503.18052v2", "date": "2025-06-03", "relevancy": 3.3216, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6837}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6764}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6328}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SceneSplat%3A%20Gaussian%20Splatting-based%20Scene%20Understanding%20with%0A%20%20Vision-Language%20Pretraining&body=Title%3A%20SceneSplat%3A%20Gaussian%20Splatting-based%20Scene%20Understanding%20with%0A%20%20Vision-Language%20Pretraining%0AAuthor%3A%20Yue%20Li%20and%20Qi%20Ma%20and%20Runyi%20Yang%20and%20Huapeng%20Li%20and%20Mengjiao%20Ma%20and%20Bin%20Ren%20and%20Nikola%20Popovic%20and%20Nicu%20Sebe%20and%20Ender%20Konukoglu%20and%20Theo%20Gevers%20and%20Luc%20Van%20Gool%20and%20Martin%20R.%20Oswald%20and%20Danda%20Pani%20Paudel%0AAbstract%3A%20%20%20Recognizing%20arbitrary%20or%20previously%20unseen%20categories%20is%20essential%20for%0Acomprehensive%20real-world%203D%20scene%20understanding.%20Currently%2C%20all%20existing%0Amethods%20rely%20on%202D%20or%20textual%20modalities%20during%20training%20or%20together%20at%0Ainference.%20This%20highlights%20the%20clear%20absence%20of%20a%20model%20capable%20of%20processing%0A3D%20data%20alone%20for%20learning%20semantics%20end-to-end%2C%20along%20with%20the%20necessary%20data%0Ato%20train%20such%20a%20model.%20Meanwhile%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%0Athe%20de%20facto%20standard%20for%203D%20scene%20representation%20across%20various%20vision%20tasks.%0AHowever%2C%20effectively%20integrating%20semantic%20reasoning%20into%203DGS%20in%20a%0Ageneralizable%20manner%20remains%20an%20open%20challenge.%20To%20address%20these%20limitations%2C%0Awe%20introduce%20SceneSplat%2C%20to%20our%20knowledge%20the%20first%20large-scale%203D%20indoor%20scene%0Aunderstanding%20approach%20that%20operates%20natively%20on%203DGS.%20Furthermore%2C%20we%20propose%0Aa%20self-supervised%20learning%20scheme%20that%20unlocks%20rich%203D%20feature%20learning%20from%0Aunlabeled%20scenes.%20To%20power%20the%20proposed%20methods%2C%20we%20introduce%20SceneSplat-7K%2C%0Athe%20first%20large-scale%203DGS%20dataset%20for%20indoor%20scenes%2C%20comprising%207916%20scenes%0Aderived%20from%20seven%20established%20datasets%2C%20such%20as%20ScanNet%20and%20Matterport3D.%0AGenerating%20SceneSplat-7K%20required%20computational%20resources%20equivalent%20to%20150%20GPU%0Adays%20on%20an%20L4%20GPU%2C%20enabling%20standardized%20benchmarking%20for%203DGS-based%20reasoning%0Afor%20indoor%20scenes.%20Our%20exhaustive%20experiments%20on%20SceneSplat-7K%20demonstrate%20the%0Asignificant%20benefit%20of%20the%20proposed%20method%20over%20the%20established%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.18052v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSceneSplat%253A%2520Gaussian%2520Splatting-based%2520Scene%2520Understanding%2520with%250A%2520%2520Vision-Language%2520Pretraining%26entry.906535625%3DYue%2520Li%2520and%2520Qi%2520Ma%2520and%2520Runyi%2520Yang%2520and%2520Huapeng%2520Li%2520and%2520Mengjiao%2520Ma%2520and%2520Bin%2520Ren%2520and%2520Nikola%2520Popovic%2520and%2520Nicu%2520Sebe%2520and%2520Ender%2520Konukoglu%2520and%2520Theo%2520Gevers%2520and%2520Luc%2520Van%2520Gool%2520and%2520Martin%2520R.%2520Oswald%2520and%2520Danda%2520Pani%2520Paudel%26entry.1292438233%3D%2520%2520Recognizing%2520arbitrary%2520or%2520previously%2520unseen%2520categories%2520is%2520essential%2520for%250Acomprehensive%2520real-world%25203D%2520scene%2520understanding.%2520Currently%252C%2520all%2520existing%250Amethods%2520rely%2520on%25202D%2520or%2520textual%2520modalities%2520during%2520training%2520or%2520together%2520at%250Ainference.%2520This%2520highlights%2520the%2520clear%2520absence%2520of%2520a%2520model%2520capable%2520of%2520processing%250A3D%2520data%2520alone%2520for%2520learning%2520semantics%2520end-to-end%252C%2520along%2520with%2520the%2520necessary%2520data%250Ato%2520train%2520such%2520a%2520model.%2520Meanwhile%252C%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520has%2520emerged%2520as%250Athe%2520de%2520facto%2520standard%2520for%25203D%2520scene%2520representation%2520across%2520various%2520vision%2520tasks.%250AHowever%252C%2520effectively%2520integrating%2520semantic%2520reasoning%2520into%25203DGS%2520in%2520a%250Ageneralizable%2520manner%2520remains%2520an%2520open%2520challenge.%2520To%2520address%2520these%2520limitations%252C%250Awe%2520introduce%2520SceneSplat%252C%2520to%2520our%2520knowledge%2520the%2520first%2520large-scale%25203D%2520indoor%2520scene%250Aunderstanding%2520approach%2520that%2520operates%2520natively%2520on%25203DGS.%2520Furthermore%252C%2520we%2520propose%250Aa%2520self-supervised%2520learning%2520scheme%2520that%2520unlocks%2520rich%25203D%2520feature%2520learning%2520from%250Aunlabeled%2520scenes.%2520To%2520power%2520the%2520proposed%2520methods%252C%2520we%2520introduce%2520SceneSplat-7K%252C%250Athe%2520first%2520large-scale%25203DGS%2520dataset%2520for%2520indoor%2520scenes%252C%2520comprising%25207916%2520scenes%250Aderived%2520from%2520seven%2520established%2520datasets%252C%2520such%2520as%2520ScanNet%2520and%2520Matterport3D.%250AGenerating%2520SceneSplat-7K%2520required%2520computational%2520resources%2520equivalent%2520to%2520150%2520GPU%250Adays%2520on%2520an%2520L4%2520GPU%252C%2520enabling%2520standardized%2520benchmarking%2520for%25203DGS-based%2520reasoning%250Afor%2520indoor%2520scenes.%2520Our%2520exhaustive%2520experiments%2520on%2520SceneSplat-7K%2520demonstrate%2520the%250Asignificant%2520benefit%2520of%2520the%2520proposed%2520method%2520over%2520the%2520established%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.18052v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SceneSplat%3A%20Gaussian%20Splatting-based%20Scene%20Understanding%20with%0A%20%20Vision-Language%20Pretraining&entry.906535625=Yue%20Li%20and%20Qi%20Ma%20and%20Runyi%20Yang%20and%20Huapeng%20Li%20and%20Mengjiao%20Ma%20and%20Bin%20Ren%20and%20Nikola%20Popovic%20and%20Nicu%20Sebe%20and%20Ender%20Konukoglu%20and%20Theo%20Gevers%20and%20Luc%20Van%20Gool%20and%20Martin%20R.%20Oswald%20and%20Danda%20Pani%20Paudel&entry.1292438233=%20%20Recognizing%20arbitrary%20or%20previously%20unseen%20categories%20is%20essential%20for%0Acomprehensive%20real-world%203D%20scene%20understanding.%20Currently%2C%20all%20existing%0Amethods%20rely%20on%202D%20or%20textual%20modalities%20during%20training%20or%20together%20at%0Ainference.%20This%20highlights%20the%20clear%20absence%20of%20a%20model%20capable%20of%20processing%0A3D%20data%20alone%20for%20learning%20semantics%20end-to-end%2C%20along%20with%20the%20necessary%20data%0Ato%20train%20such%20a%20model.%20Meanwhile%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20has%20emerged%20as%0Athe%20de%20facto%20standard%20for%203D%20scene%20representation%20across%20various%20vision%20tasks.%0AHowever%2C%20effectively%20integrating%20semantic%20reasoning%20into%203DGS%20in%20a%0Ageneralizable%20manner%20remains%20an%20open%20challenge.%20To%20address%20these%20limitations%2C%0Awe%20introduce%20SceneSplat%2C%20to%20our%20knowledge%20the%20first%20large-scale%203D%20indoor%20scene%0Aunderstanding%20approach%20that%20operates%20natively%20on%203DGS.%20Furthermore%2C%20we%20propose%0Aa%20self-supervised%20learning%20scheme%20that%20unlocks%20rich%203D%20feature%20learning%20from%0Aunlabeled%20scenes.%20To%20power%20the%20proposed%20methods%2C%20we%20introduce%20SceneSplat-7K%2C%0Athe%20first%20large-scale%203DGS%20dataset%20for%20indoor%20scenes%2C%20comprising%207916%20scenes%0Aderived%20from%20seven%20established%20datasets%2C%20such%20as%20ScanNet%20and%20Matterport3D.%0AGenerating%20SceneSplat-7K%20required%20computational%20resources%20equivalent%20to%20150%20GPU%0Adays%20on%20an%20L4%20GPU%2C%20enabling%20standardized%20benchmarking%20for%203DGS-based%20reasoning%0Afor%20indoor%20scenes.%20Our%20exhaustive%20experiments%20on%20SceneSplat-7K%20demonstrate%20the%0Asignificant%20benefit%20of%20the%20proposed%20method%20over%20the%20established%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.18052v2&entry.124074799=Read"},
{"title": "HumanRAM: Feed-forward Human Reconstruction and Animation Model using\n  Transformers", "author": "Zhiyuan Yu and Zhe Li and Hujun Bao and Can Yang and Xiaowei Zhou", "abstract": "  3D human reconstruction and animation are long-standing topics in computer\ngraphics and vision. However, existing methods typically rely on sophisticated\ndense-view capture and/or time-consuming per-subject optimization procedures.\nTo address these limitations, we propose HumanRAM, a novel feed-forward\napproach for generalizable human reconstruction and animation from monocular or\nsparse human images. Our approach integrates human reconstruction and animation\ninto a unified framework by introducing explicit pose conditions, parameterized\nby a shared SMPL-X neural texture, into transformer-based large reconstruction\nmodels (LRM). Given monocular or sparse input images with associated camera\nparameters and SMPL-X poses, our model employs scalable transformers and a\nDPT-based decoder to synthesize realistic human renderings under novel\nviewpoints and novel poses. By leveraging the explicit pose conditions, our\nmodel simultaneously enables high-quality human reconstruction and\nhigh-fidelity pose-controlled animation. Experiments show that HumanRAM\nsignificantly surpasses previous methods in terms of reconstruction accuracy,\nanimation fidelity, and generalization performance on real-world datasets.\nVideo results are available at https://zju3dv.github.io/humanram/.\n", "link": "http://arxiv.org/abs/2506.03118v1", "date": "2025-06-03", "relevancy": 3.1762, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6568}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6371}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HumanRAM%3A%20Feed-forward%20Human%20Reconstruction%20and%20Animation%20Model%20using%0A%20%20Transformers&body=Title%3A%20HumanRAM%3A%20Feed-forward%20Human%20Reconstruction%20and%20Animation%20Model%20using%0A%20%20Transformers%0AAuthor%3A%20Zhiyuan%20Yu%20and%20Zhe%20Li%20and%20Hujun%20Bao%20and%20Can%20Yang%20and%20Xiaowei%20Zhou%0AAbstract%3A%20%20%203D%20human%20reconstruction%20and%20animation%20are%20long-standing%20topics%20in%20computer%0Agraphics%20and%20vision.%20However%2C%20existing%20methods%20typically%20rely%20on%20sophisticated%0Adense-view%20capture%20and/or%20time-consuming%20per-subject%20optimization%20procedures.%0ATo%20address%20these%20limitations%2C%20we%20propose%20HumanRAM%2C%20a%20novel%20feed-forward%0Aapproach%20for%20generalizable%20human%20reconstruction%20and%20animation%20from%20monocular%20or%0Asparse%20human%20images.%20Our%20approach%20integrates%20human%20reconstruction%20and%20animation%0Ainto%20a%20unified%20framework%20by%20introducing%20explicit%20pose%20conditions%2C%20parameterized%0Aby%20a%20shared%20SMPL-X%20neural%20texture%2C%20into%20transformer-based%20large%20reconstruction%0Amodels%20%28LRM%29.%20Given%20monocular%20or%20sparse%20input%20images%20with%20associated%20camera%0Aparameters%20and%20SMPL-X%20poses%2C%20our%20model%20employs%20scalable%20transformers%20and%20a%0ADPT-based%20decoder%20to%20synthesize%20realistic%20human%20renderings%20under%20novel%0Aviewpoints%20and%20novel%20poses.%20By%20leveraging%20the%20explicit%20pose%20conditions%2C%20our%0Amodel%20simultaneously%20enables%20high-quality%20human%20reconstruction%20and%0Ahigh-fidelity%20pose-controlled%20animation.%20Experiments%20show%20that%20HumanRAM%0Asignificantly%20surpasses%20previous%20methods%20in%20terms%20of%20reconstruction%20accuracy%2C%0Aanimation%20fidelity%2C%20and%20generalization%20performance%20on%20real-world%20datasets.%0AVideo%20results%20are%20available%20at%20https%3A//zju3dv.github.io/humanram/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03118v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHumanRAM%253A%2520Feed-forward%2520Human%2520Reconstruction%2520and%2520Animation%2520Model%2520using%250A%2520%2520Transformers%26entry.906535625%3DZhiyuan%2520Yu%2520and%2520Zhe%2520Li%2520and%2520Hujun%2520Bao%2520and%2520Can%2520Yang%2520and%2520Xiaowei%2520Zhou%26entry.1292438233%3D%2520%25203D%2520human%2520reconstruction%2520and%2520animation%2520are%2520long-standing%2520topics%2520in%2520computer%250Agraphics%2520and%2520vision.%2520However%252C%2520existing%2520methods%2520typically%2520rely%2520on%2520sophisticated%250Adense-view%2520capture%2520and/or%2520time-consuming%2520per-subject%2520optimization%2520procedures.%250ATo%2520address%2520these%2520limitations%252C%2520we%2520propose%2520HumanRAM%252C%2520a%2520novel%2520feed-forward%250Aapproach%2520for%2520generalizable%2520human%2520reconstruction%2520and%2520animation%2520from%2520monocular%2520or%250Asparse%2520human%2520images.%2520Our%2520approach%2520integrates%2520human%2520reconstruction%2520and%2520animation%250Ainto%2520a%2520unified%2520framework%2520by%2520introducing%2520explicit%2520pose%2520conditions%252C%2520parameterized%250Aby%2520a%2520shared%2520SMPL-X%2520neural%2520texture%252C%2520into%2520transformer-based%2520large%2520reconstruction%250Amodels%2520%2528LRM%2529.%2520Given%2520monocular%2520or%2520sparse%2520input%2520images%2520with%2520associated%2520camera%250Aparameters%2520and%2520SMPL-X%2520poses%252C%2520our%2520model%2520employs%2520scalable%2520transformers%2520and%2520a%250ADPT-based%2520decoder%2520to%2520synthesize%2520realistic%2520human%2520renderings%2520under%2520novel%250Aviewpoints%2520and%2520novel%2520poses.%2520By%2520leveraging%2520the%2520explicit%2520pose%2520conditions%252C%2520our%250Amodel%2520simultaneously%2520enables%2520high-quality%2520human%2520reconstruction%2520and%250Ahigh-fidelity%2520pose-controlled%2520animation.%2520Experiments%2520show%2520that%2520HumanRAM%250Asignificantly%2520surpasses%2520previous%2520methods%2520in%2520terms%2520of%2520reconstruction%2520accuracy%252C%250Aanimation%2520fidelity%252C%2520and%2520generalization%2520performance%2520on%2520real-world%2520datasets.%250AVideo%2520results%2520are%2520available%2520at%2520https%253A//zju3dv.github.io/humanram/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03118v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HumanRAM%3A%20Feed-forward%20Human%20Reconstruction%20and%20Animation%20Model%20using%0A%20%20Transformers&entry.906535625=Zhiyuan%20Yu%20and%20Zhe%20Li%20and%20Hujun%20Bao%20and%20Can%20Yang%20and%20Xiaowei%20Zhou&entry.1292438233=%20%203D%20human%20reconstruction%20and%20animation%20are%20long-standing%20topics%20in%20computer%0Agraphics%20and%20vision.%20However%2C%20existing%20methods%20typically%20rely%20on%20sophisticated%0Adense-view%20capture%20and/or%20time-consuming%20per-subject%20optimization%20procedures.%0ATo%20address%20these%20limitations%2C%20we%20propose%20HumanRAM%2C%20a%20novel%20feed-forward%0Aapproach%20for%20generalizable%20human%20reconstruction%20and%20animation%20from%20monocular%20or%0Asparse%20human%20images.%20Our%20approach%20integrates%20human%20reconstruction%20and%20animation%0Ainto%20a%20unified%20framework%20by%20introducing%20explicit%20pose%20conditions%2C%20parameterized%0Aby%20a%20shared%20SMPL-X%20neural%20texture%2C%20into%20transformer-based%20large%20reconstruction%0Amodels%20%28LRM%29.%20Given%20monocular%20or%20sparse%20input%20images%20with%20associated%20camera%0Aparameters%20and%20SMPL-X%20poses%2C%20our%20model%20employs%20scalable%20transformers%20and%20a%0ADPT-based%20decoder%20to%20synthesize%20realistic%20human%20renderings%20under%20novel%0Aviewpoints%20and%20novel%20poses.%20By%20leveraging%20the%20explicit%20pose%20conditions%2C%20our%0Amodel%20simultaneously%20enables%20high-quality%20human%20reconstruction%20and%0Ahigh-fidelity%20pose-controlled%20animation.%20Experiments%20show%20that%20HumanRAM%0Asignificantly%20surpasses%20previous%20methods%20in%20terms%20of%20reconstruction%20accuracy%2C%0Aanimation%20fidelity%2C%20and%20generalization%20performance%20on%20real-world%20datasets.%0AVideo%20results%20are%20available%20at%20https%3A//zju3dv.github.io/humanram/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03118v1&entry.124074799=Read"},
{"title": "IllumiCraft: Unified Geometry and Illumination Diffusion for\n  Controllable Video Generation", "author": "Yuanze Lin and Yi-Wen Chen and Yi-Hsuan Tsai and Ronald Clark and Ming-Hsuan Yang", "abstract": "  Although diffusion-based models can generate high-quality and high-resolution\nvideo sequences from textual or image inputs, they lack explicit integration of\ngeometric cues when controlling scene lighting and visual appearance across\nframes. To address this limitation, we propose IllumiCraft, an end-to-end\ndiffusion framework accepting three complementary inputs: (1)\nhigh-dynamic-range (HDR) video maps for detailed lighting control; (2)\nsynthetically relit frames with randomized illumination changes (optionally\npaired with a static background reference image) to provide appearance cues;\nand (3) 3D point tracks that capture precise 3D geometry information. By\nintegrating the lighting, appearance, and geometry cues within a unified\ndiffusion architecture, IllumiCraft generates temporally coherent videos\naligned with user-defined prompts. It supports background-conditioned and\ntext-conditioned video relighting and provides better fidelity than existing\ncontrollable video generation methods. Project Page:\nhttps://yuanze-lin.me/IllumiCraft_page\n", "link": "http://arxiv.org/abs/2506.03150v1", "date": "2025-06-03", "relevancy": 3.1326, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6521}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6138}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IllumiCraft%3A%20Unified%20Geometry%20and%20Illumination%20Diffusion%20for%0A%20%20Controllable%20Video%20Generation&body=Title%3A%20IllumiCraft%3A%20Unified%20Geometry%20and%20Illumination%20Diffusion%20for%0A%20%20Controllable%20Video%20Generation%0AAuthor%3A%20Yuanze%20Lin%20and%20Yi-Wen%20Chen%20and%20Yi-Hsuan%20Tsai%20and%20Ronald%20Clark%20and%20Ming-Hsuan%20Yang%0AAbstract%3A%20%20%20Although%20diffusion-based%20models%20can%20generate%20high-quality%20and%20high-resolution%0Avideo%20sequences%20from%20textual%20or%20image%20inputs%2C%20they%20lack%20explicit%20integration%20of%0Ageometric%20cues%20when%20controlling%20scene%20lighting%20and%20visual%20appearance%20across%0Aframes.%20To%20address%20this%20limitation%2C%20we%20propose%20IllumiCraft%2C%20an%20end-to-end%0Adiffusion%20framework%20accepting%20three%20complementary%20inputs%3A%20%281%29%0Ahigh-dynamic-range%20%28HDR%29%20video%20maps%20for%20detailed%20lighting%20control%3B%20%282%29%0Asynthetically%20relit%20frames%20with%20randomized%20illumination%20changes%20%28optionally%0Apaired%20with%20a%20static%20background%20reference%20image%29%20to%20provide%20appearance%20cues%3B%0Aand%20%283%29%203D%20point%20tracks%20that%20capture%20precise%203D%20geometry%20information.%20By%0Aintegrating%20the%20lighting%2C%20appearance%2C%20and%20geometry%20cues%20within%20a%20unified%0Adiffusion%20architecture%2C%20IllumiCraft%20generates%20temporally%20coherent%20videos%0Aaligned%20with%20user-defined%20prompts.%20It%20supports%20background-conditioned%20and%0Atext-conditioned%20video%20relighting%20and%20provides%20better%20fidelity%20than%20existing%0Acontrollable%20video%20generation%20methods.%20Project%20Page%3A%0Ahttps%3A//yuanze-lin.me/IllumiCraft_page%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03150v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIllumiCraft%253A%2520Unified%2520Geometry%2520and%2520Illumination%2520Diffusion%2520for%250A%2520%2520Controllable%2520Video%2520Generation%26entry.906535625%3DYuanze%2520Lin%2520and%2520Yi-Wen%2520Chen%2520and%2520Yi-Hsuan%2520Tsai%2520and%2520Ronald%2520Clark%2520and%2520Ming-Hsuan%2520Yang%26entry.1292438233%3D%2520%2520Although%2520diffusion-based%2520models%2520can%2520generate%2520high-quality%2520and%2520high-resolution%250Avideo%2520sequences%2520from%2520textual%2520or%2520image%2520inputs%252C%2520they%2520lack%2520explicit%2520integration%2520of%250Ageometric%2520cues%2520when%2520controlling%2520scene%2520lighting%2520and%2520visual%2520appearance%2520across%250Aframes.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%2520IllumiCraft%252C%2520an%2520end-to-end%250Adiffusion%2520framework%2520accepting%2520three%2520complementary%2520inputs%253A%2520%25281%2529%250Ahigh-dynamic-range%2520%2528HDR%2529%2520video%2520maps%2520for%2520detailed%2520lighting%2520control%253B%2520%25282%2529%250Asynthetically%2520relit%2520frames%2520with%2520randomized%2520illumination%2520changes%2520%2528optionally%250Apaired%2520with%2520a%2520static%2520background%2520reference%2520image%2529%2520to%2520provide%2520appearance%2520cues%253B%250Aand%2520%25283%2529%25203D%2520point%2520tracks%2520that%2520capture%2520precise%25203D%2520geometry%2520information.%2520By%250Aintegrating%2520the%2520lighting%252C%2520appearance%252C%2520and%2520geometry%2520cues%2520within%2520a%2520unified%250Adiffusion%2520architecture%252C%2520IllumiCraft%2520generates%2520temporally%2520coherent%2520videos%250Aaligned%2520with%2520user-defined%2520prompts.%2520It%2520supports%2520background-conditioned%2520and%250Atext-conditioned%2520video%2520relighting%2520and%2520provides%2520better%2520fidelity%2520than%2520existing%250Acontrollable%2520video%2520generation%2520methods.%2520Project%2520Page%253A%250Ahttps%253A//yuanze-lin.me/IllumiCraft_page%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03150v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IllumiCraft%3A%20Unified%20Geometry%20and%20Illumination%20Diffusion%20for%0A%20%20Controllable%20Video%20Generation&entry.906535625=Yuanze%20Lin%20and%20Yi-Wen%20Chen%20and%20Yi-Hsuan%20Tsai%20and%20Ronald%20Clark%20and%20Ming-Hsuan%20Yang&entry.1292438233=%20%20Although%20diffusion-based%20models%20can%20generate%20high-quality%20and%20high-resolution%0Avideo%20sequences%20from%20textual%20or%20image%20inputs%2C%20they%20lack%20explicit%20integration%20of%0Ageometric%20cues%20when%20controlling%20scene%20lighting%20and%20visual%20appearance%20across%0Aframes.%20To%20address%20this%20limitation%2C%20we%20propose%20IllumiCraft%2C%20an%20end-to-end%0Adiffusion%20framework%20accepting%20three%20complementary%20inputs%3A%20%281%29%0Ahigh-dynamic-range%20%28HDR%29%20video%20maps%20for%20detailed%20lighting%20control%3B%20%282%29%0Asynthetically%20relit%20frames%20with%20randomized%20illumination%20changes%20%28optionally%0Apaired%20with%20a%20static%20background%20reference%20image%29%20to%20provide%20appearance%20cues%3B%0Aand%20%283%29%203D%20point%20tracks%20that%20capture%20precise%203D%20geometry%20information.%20By%0Aintegrating%20the%20lighting%2C%20appearance%2C%20and%20geometry%20cues%20within%20a%20unified%0Adiffusion%20architecture%2C%20IllumiCraft%20generates%20temporally%20coherent%20videos%0Aaligned%20with%20user-defined%20prompts.%20It%20supports%20background-conditioned%20and%0Atext-conditioned%20video%20relighting%20and%20provides%20better%20fidelity%20than%20existing%0Acontrollable%20video%20generation%20methods.%20Project%20Page%3A%0Ahttps%3A//yuanze-lin.me/IllumiCraft_page%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03150v1&entry.124074799=Read"},
{"title": "HAMMER: Heterogeneous, Multi-Robot Semantic Gaussian Splatting", "author": "Javier Yu and Timothy Chen and Mac Schwager", "abstract": "  3D Gaussian Splatting offers expressive scene reconstruction, modeling a\nbroad range of visual, geometric, and semantic information. However, efficient\nreal-time map reconstruction with data streamed from multiple robots and\ndevices remains a challenge. To that end, we propose HAMMER, a server-based\ncollaborative Gaussian Splatting method that leverages widely available ROS\ncommunication infrastructure to generate 3D, metric-semantic maps from\nasynchronous robot data-streams with no prior knowledge of initial robot\npositions and varying on-device pose estimators. HAMMER consists of (i) a frame\nalignment module that transforms local SLAM poses and image data into a global\nframe and requires no prior relative pose knowledge, and (ii) an online module\nfor training semantic 3DGS maps from streaming data. HAMMER handles mixed\nperception modes, adjusts automatically for variations in image pre-processing\namong different devices, and distills CLIP semantic codes into the 3D scene for\nopen-vocabulary language queries. In our real-world experiments, HAMMER creates\nhigher-fidelity maps (2x) compared to competing baselines and is useful for\ndownstream tasks, such as semantic goal-conditioned navigation (e.g., \"go to\nthe couch\"). Accompanying content available at hammer-project.github.io.\n", "link": "http://arxiv.org/abs/2501.14147v2", "date": "2025-06-03", "relevancy": 3.1296, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6791}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6137}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5849}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HAMMER%3A%20Heterogeneous%2C%20Multi-Robot%20Semantic%20Gaussian%20Splatting&body=Title%3A%20HAMMER%3A%20Heterogeneous%2C%20Multi-Robot%20Semantic%20Gaussian%20Splatting%0AAuthor%3A%20Javier%20Yu%20and%20Timothy%20Chen%20and%20Mac%20Schwager%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20offers%20expressive%20scene%20reconstruction%2C%20modeling%20a%0Abroad%20range%20of%20visual%2C%20geometric%2C%20and%20semantic%20information.%20However%2C%20efficient%0Areal-time%20map%20reconstruction%20with%20data%20streamed%20from%20multiple%20robots%20and%0Adevices%20remains%20a%20challenge.%20To%20that%20end%2C%20we%20propose%20HAMMER%2C%20a%20server-based%0Acollaborative%20Gaussian%20Splatting%20method%20that%20leverages%20widely%20available%20ROS%0Acommunication%20infrastructure%20to%20generate%203D%2C%20metric-semantic%20maps%20from%0Aasynchronous%20robot%20data-streams%20with%20no%20prior%20knowledge%20of%20initial%20robot%0Apositions%20and%20varying%20on-device%20pose%20estimators.%20HAMMER%20consists%20of%20%28i%29%20a%20frame%0Aalignment%20module%20that%20transforms%20local%20SLAM%20poses%20and%20image%20data%20into%20a%20global%0Aframe%20and%20requires%20no%20prior%20relative%20pose%20knowledge%2C%20and%20%28ii%29%20an%20online%20module%0Afor%20training%20semantic%203DGS%20maps%20from%20streaming%20data.%20HAMMER%20handles%20mixed%0Aperception%20modes%2C%20adjusts%20automatically%20for%20variations%20in%20image%20pre-processing%0Aamong%20different%20devices%2C%20and%20distills%20CLIP%20semantic%20codes%20into%20the%203D%20scene%20for%0Aopen-vocabulary%20language%20queries.%20In%20our%20real-world%20experiments%2C%20HAMMER%20creates%0Ahigher-fidelity%20maps%20%282x%29%20compared%20to%20competing%20baselines%20and%20is%20useful%20for%0Adownstream%20tasks%2C%20such%20as%20semantic%20goal-conditioned%20navigation%20%28e.g.%2C%20%22go%20to%0Athe%20couch%22%29.%20Accompanying%20content%20available%20at%20hammer-project.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.14147v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHAMMER%253A%2520Heterogeneous%252C%2520Multi-Robot%2520Semantic%2520Gaussian%2520Splatting%26entry.906535625%3DJavier%2520Yu%2520and%2520Timothy%2520Chen%2520and%2520Mac%2520Schwager%26entry.1292438233%3D%2520%25203D%2520Gaussian%2520Splatting%2520offers%2520expressive%2520scene%2520reconstruction%252C%2520modeling%2520a%250Abroad%2520range%2520of%2520visual%252C%2520geometric%252C%2520and%2520semantic%2520information.%2520However%252C%2520efficient%250Areal-time%2520map%2520reconstruction%2520with%2520data%2520streamed%2520from%2520multiple%2520robots%2520and%250Adevices%2520remains%2520a%2520challenge.%2520To%2520that%2520end%252C%2520we%2520propose%2520HAMMER%252C%2520a%2520server-based%250Acollaborative%2520Gaussian%2520Splatting%2520method%2520that%2520leverages%2520widely%2520available%2520ROS%250Acommunication%2520infrastructure%2520to%2520generate%25203D%252C%2520metric-semantic%2520maps%2520from%250Aasynchronous%2520robot%2520data-streams%2520with%2520no%2520prior%2520knowledge%2520of%2520initial%2520robot%250Apositions%2520and%2520varying%2520on-device%2520pose%2520estimators.%2520HAMMER%2520consists%2520of%2520%2528i%2529%2520a%2520frame%250Aalignment%2520module%2520that%2520transforms%2520local%2520SLAM%2520poses%2520and%2520image%2520data%2520into%2520a%2520global%250Aframe%2520and%2520requires%2520no%2520prior%2520relative%2520pose%2520knowledge%252C%2520and%2520%2528ii%2529%2520an%2520online%2520module%250Afor%2520training%2520semantic%25203DGS%2520maps%2520from%2520streaming%2520data.%2520HAMMER%2520handles%2520mixed%250Aperception%2520modes%252C%2520adjusts%2520automatically%2520for%2520variations%2520in%2520image%2520pre-processing%250Aamong%2520different%2520devices%252C%2520and%2520distills%2520CLIP%2520semantic%2520codes%2520into%2520the%25203D%2520scene%2520for%250Aopen-vocabulary%2520language%2520queries.%2520In%2520our%2520real-world%2520experiments%252C%2520HAMMER%2520creates%250Ahigher-fidelity%2520maps%2520%25282x%2529%2520compared%2520to%2520competing%2520baselines%2520and%2520is%2520useful%2520for%250Adownstream%2520tasks%252C%2520such%2520as%2520semantic%2520goal-conditioned%2520navigation%2520%2528e.g.%252C%2520%2522go%2520to%250Athe%2520couch%2522%2529.%2520Accompanying%2520content%2520available%2520at%2520hammer-project.github.io.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.14147v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HAMMER%3A%20Heterogeneous%2C%20Multi-Robot%20Semantic%20Gaussian%20Splatting&entry.906535625=Javier%20Yu%20and%20Timothy%20Chen%20and%20Mac%20Schwager&entry.1292438233=%20%203D%20Gaussian%20Splatting%20offers%20expressive%20scene%20reconstruction%2C%20modeling%20a%0Abroad%20range%20of%20visual%2C%20geometric%2C%20and%20semantic%20information.%20However%2C%20efficient%0Areal-time%20map%20reconstruction%20with%20data%20streamed%20from%20multiple%20robots%20and%0Adevices%20remains%20a%20challenge.%20To%20that%20end%2C%20we%20propose%20HAMMER%2C%20a%20server-based%0Acollaborative%20Gaussian%20Splatting%20method%20that%20leverages%20widely%20available%20ROS%0Acommunication%20infrastructure%20to%20generate%203D%2C%20metric-semantic%20maps%20from%0Aasynchronous%20robot%20data-streams%20with%20no%20prior%20knowledge%20of%20initial%20robot%0Apositions%20and%20varying%20on-device%20pose%20estimators.%20HAMMER%20consists%20of%20%28i%29%20a%20frame%0Aalignment%20module%20that%20transforms%20local%20SLAM%20poses%20and%20image%20data%20into%20a%20global%0Aframe%20and%20requires%20no%20prior%20relative%20pose%20knowledge%2C%20and%20%28ii%29%20an%20online%20module%0Afor%20training%20semantic%203DGS%20maps%20from%20streaming%20data.%20HAMMER%20handles%20mixed%0Aperception%20modes%2C%20adjusts%20automatically%20for%20variations%20in%20image%20pre-processing%0Aamong%20different%20devices%2C%20and%20distills%20CLIP%20semantic%20codes%20into%20the%203D%20scene%20for%0Aopen-vocabulary%20language%20queries.%20In%20our%20real-world%20experiments%2C%20HAMMER%20creates%0Ahigher-fidelity%20maps%20%282x%29%20compared%20to%20competing%20baselines%20and%20is%20useful%20for%0Adownstream%20tasks%2C%20such%20as%20semantic%20goal-conditioned%20navigation%20%28e.g.%2C%20%22go%20to%0Athe%20couch%22%29.%20Accompanying%20content%20available%20at%20hammer-project.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.14147v2&entry.124074799=Read"},
{"title": "HunyuanVideo-Avatar: High-Fidelity Audio-Driven Human Animation for\n  Multiple Characters", "author": "Yi Chen and Sen Liang and Zixiang Zhou and Ziyao Huang and Yifeng Ma and Junshu Tang and Qin Lin and Yuan Zhou and Qinglin Lu", "abstract": "  Recent years have witnessed significant progress in audio-driven human\nanimation. However, critical challenges remain in (i) generating highly dynamic\nvideos while preserving character consistency, (ii) achieving precise emotion\nalignment between characters and audio, and (iii) enabling multi-character\naudio-driven animation. To address these challenges, we propose\nHunyuanVideo-Avatar, a multimodal diffusion transformer (MM-DiT)-based model\ncapable of simultaneously generating dynamic, emotion-controllable, and\nmulti-character dialogue videos. Concretely, HunyuanVideo-Avatar introduces\nthree key innovations: (i) A character image injection module is designed to\nreplace the conventional addition-based character conditioning scheme,\neliminating the inherent condition mismatch between training and inference.\nThis ensures the dynamic motion and strong character consistency; (ii) An Audio\nEmotion Module (AEM) is introduced to extract and transfer the emotional cues\nfrom an emotion reference image to the target generated video, enabling\nfine-grained and accurate emotion style control; (iii) A Face-Aware Audio\nAdapter (FAA) is proposed to isolate the audio-driven character with\nlatent-level face mask, enabling independent audio injection via\ncross-attention for multi-character scenarios. These innovations empower\nHunyuanVideo-Avatar to surpass state-of-the-art methods on benchmark datasets\nand a newly proposed wild dataset, generating realistic avatars in dynamic,\nimmersive scenarios.\n", "link": "http://arxiv.org/abs/2505.20156v2", "date": "2025-06-03", "relevancy": 3.1202, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6557}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6155}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HunyuanVideo-Avatar%3A%20High-Fidelity%20Audio-Driven%20Human%20Animation%20for%0A%20%20Multiple%20Characters&body=Title%3A%20HunyuanVideo-Avatar%3A%20High-Fidelity%20Audio-Driven%20Human%20Animation%20for%0A%20%20Multiple%20Characters%0AAuthor%3A%20Yi%20Chen%20and%20Sen%20Liang%20and%20Zixiang%20Zhou%20and%20Ziyao%20Huang%20and%20Yifeng%20Ma%20and%20Junshu%20Tang%20and%20Qin%20Lin%20and%20Yuan%20Zhou%20and%20Qinglin%20Lu%0AAbstract%3A%20%20%20Recent%20years%20have%20witnessed%20significant%20progress%20in%20audio-driven%20human%0Aanimation.%20However%2C%20critical%20challenges%20remain%20in%20%28i%29%20generating%20highly%20dynamic%0Avideos%20while%20preserving%20character%20consistency%2C%20%28ii%29%20achieving%20precise%20emotion%0Aalignment%20between%20characters%20and%20audio%2C%20and%20%28iii%29%20enabling%20multi-character%0Aaudio-driven%20animation.%20To%20address%20these%20challenges%2C%20we%20propose%0AHunyuanVideo-Avatar%2C%20a%20multimodal%20diffusion%20transformer%20%28MM-DiT%29-based%20model%0Acapable%20of%20simultaneously%20generating%20dynamic%2C%20emotion-controllable%2C%20and%0Amulti-character%20dialogue%20videos.%20Concretely%2C%20HunyuanVideo-Avatar%20introduces%0Athree%20key%20innovations%3A%20%28i%29%20A%20character%20image%20injection%20module%20is%20designed%20to%0Areplace%20the%20conventional%20addition-based%20character%20conditioning%20scheme%2C%0Aeliminating%20the%20inherent%20condition%20mismatch%20between%20training%20and%20inference.%0AThis%20ensures%20the%20dynamic%20motion%20and%20strong%20character%20consistency%3B%20%28ii%29%20An%20Audio%0AEmotion%20Module%20%28AEM%29%20is%20introduced%20to%20extract%20and%20transfer%20the%20emotional%20cues%0Afrom%20an%20emotion%20reference%20image%20to%20the%20target%20generated%20video%2C%20enabling%0Afine-grained%20and%20accurate%20emotion%20style%20control%3B%20%28iii%29%20A%20Face-Aware%20Audio%0AAdapter%20%28FAA%29%20is%20proposed%20to%20isolate%20the%20audio-driven%20character%20with%0Alatent-level%20face%20mask%2C%20enabling%20independent%20audio%20injection%20via%0Across-attention%20for%20multi-character%20scenarios.%20These%20innovations%20empower%0AHunyuanVideo-Avatar%20to%20surpass%20state-of-the-art%20methods%20on%20benchmark%20datasets%0Aand%20a%20newly%20proposed%20wild%20dataset%2C%20generating%20realistic%20avatars%20in%20dynamic%2C%0Aimmersive%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20156v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHunyuanVideo-Avatar%253A%2520High-Fidelity%2520Audio-Driven%2520Human%2520Animation%2520for%250A%2520%2520Multiple%2520Characters%26entry.906535625%3DYi%2520Chen%2520and%2520Sen%2520Liang%2520and%2520Zixiang%2520Zhou%2520and%2520Ziyao%2520Huang%2520and%2520Yifeng%2520Ma%2520and%2520Junshu%2520Tang%2520and%2520Qin%2520Lin%2520and%2520Yuan%2520Zhou%2520and%2520Qinglin%2520Lu%26entry.1292438233%3D%2520%2520Recent%2520years%2520have%2520witnessed%2520significant%2520progress%2520in%2520audio-driven%2520human%250Aanimation.%2520However%252C%2520critical%2520challenges%2520remain%2520in%2520%2528i%2529%2520generating%2520highly%2520dynamic%250Avideos%2520while%2520preserving%2520character%2520consistency%252C%2520%2528ii%2529%2520achieving%2520precise%2520emotion%250Aalignment%2520between%2520characters%2520and%2520audio%252C%2520and%2520%2528iii%2529%2520enabling%2520multi-character%250Aaudio-driven%2520animation.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%250AHunyuanVideo-Avatar%252C%2520a%2520multimodal%2520diffusion%2520transformer%2520%2528MM-DiT%2529-based%2520model%250Acapable%2520of%2520simultaneously%2520generating%2520dynamic%252C%2520emotion-controllable%252C%2520and%250Amulti-character%2520dialogue%2520videos.%2520Concretely%252C%2520HunyuanVideo-Avatar%2520introduces%250Athree%2520key%2520innovations%253A%2520%2528i%2529%2520A%2520character%2520image%2520injection%2520module%2520is%2520designed%2520to%250Areplace%2520the%2520conventional%2520addition-based%2520character%2520conditioning%2520scheme%252C%250Aeliminating%2520the%2520inherent%2520condition%2520mismatch%2520between%2520training%2520and%2520inference.%250AThis%2520ensures%2520the%2520dynamic%2520motion%2520and%2520strong%2520character%2520consistency%253B%2520%2528ii%2529%2520An%2520Audio%250AEmotion%2520Module%2520%2528AEM%2529%2520is%2520introduced%2520to%2520extract%2520and%2520transfer%2520the%2520emotional%2520cues%250Afrom%2520an%2520emotion%2520reference%2520image%2520to%2520the%2520target%2520generated%2520video%252C%2520enabling%250Afine-grained%2520and%2520accurate%2520emotion%2520style%2520control%253B%2520%2528iii%2529%2520A%2520Face-Aware%2520Audio%250AAdapter%2520%2528FAA%2529%2520is%2520proposed%2520to%2520isolate%2520the%2520audio-driven%2520character%2520with%250Alatent-level%2520face%2520mask%252C%2520enabling%2520independent%2520audio%2520injection%2520via%250Across-attention%2520for%2520multi-character%2520scenarios.%2520These%2520innovations%2520empower%250AHunyuanVideo-Avatar%2520to%2520surpass%2520state-of-the-art%2520methods%2520on%2520benchmark%2520datasets%250Aand%2520a%2520newly%2520proposed%2520wild%2520dataset%252C%2520generating%2520realistic%2520avatars%2520in%2520dynamic%252C%250Aimmersive%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20156v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HunyuanVideo-Avatar%3A%20High-Fidelity%20Audio-Driven%20Human%20Animation%20for%0A%20%20Multiple%20Characters&entry.906535625=Yi%20Chen%20and%20Sen%20Liang%20and%20Zixiang%20Zhou%20and%20Ziyao%20Huang%20and%20Yifeng%20Ma%20and%20Junshu%20Tang%20and%20Qin%20Lin%20and%20Yuan%20Zhou%20and%20Qinglin%20Lu&entry.1292438233=%20%20Recent%20years%20have%20witnessed%20significant%20progress%20in%20audio-driven%20human%0Aanimation.%20However%2C%20critical%20challenges%20remain%20in%20%28i%29%20generating%20highly%20dynamic%0Avideos%20while%20preserving%20character%20consistency%2C%20%28ii%29%20achieving%20precise%20emotion%0Aalignment%20between%20characters%20and%20audio%2C%20and%20%28iii%29%20enabling%20multi-character%0Aaudio-driven%20animation.%20To%20address%20these%20challenges%2C%20we%20propose%0AHunyuanVideo-Avatar%2C%20a%20multimodal%20diffusion%20transformer%20%28MM-DiT%29-based%20model%0Acapable%20of%20simultaneously%20generating%20dynamic%2C%20emotion-controllable%2C%20and%0Amulti-character%20dialogue%20videos.%20Concretely%2C%20HunyuanVideo-Avatar%20introduces%0Athree%20key%20innovations%3A%20%28i%29%20A%20character%20image%20injection%20module%20is%20designed%20to%0Areplace%20the%20conventional%20addition-based%20character%20conditioning%20scheme%2C%0Aeliminating%20the%20inherent%20condition%20mismatch%20between%20training%20and%20inference.%0AThis%20ensures%20the%20dynamic%20motion%20and%20strong%20character%20consistency%3B%20%28ii%29%20An%20Audio%0AEmotion%20Module%20%28AEM%29%20is%20introduced%20to%20extract%20and%20transfer%20the%20emotional%20cues%0Afrom%20an%20emotion%20reference%20image%20to%20the%20target%20generated%20video%2C%20enabling%0Afine-grained%20and%20accurate%20emotion%20style%20control%3B%20%28iii%29%20A%20Face-Aware%20Audio%0AAdapter%20%28FAA%29%20is%20proposed%20to%20isolate%20the%20audio-driven%20character%20with%0Alatent-level%20face%20mask%2C%20enabling%20independent%20audio%20injection%20via%0Across-attention%20for%20multi-character%20scenarios.%20These%20innovations%20empower%0AHunyuanVideo-Avatar%20to%20surpass%20state-of-the-art%20methods%20on%20benchmark%20datasets%0Aand%20a%20newly%20proposed%20wild%20dataset%2C%20generating%20realistic%20avatars%20in%20dynamic%2C%0Aimmersive%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20156v2&entry.124074799=Read"},
{"title": "UniWorld: High-Resolution Semantic Encoders for Unified Visual\n  Understanding and Generation", "author": "Bin Lin and Zongjian Li and Xinhua Cheng and Yuwei Niu and Yang Ye and Xianyi He and Shenghai Yuan and Wangbo Yu and Shaodong Wang and Yunyang Ge and Yatian Pang and Li Yuan", "abstract": "  Although existing unified models deliver strong performance on\nvision-language understanding and text-to-image generation, their models are\nlimited in exploring image perception and manipulation tasks, which are\nurgently desired by users for wide applications. Recently, OpenAI released\ntheir powerful GPT-4o-Image model for comprehensive image perception and\nmanipulation, achieving expressive capability and attracting community\ninterests. By observing the performance of GPT-4o-Image in our carefully\nconstructed experiments, we infer that GPT-4o-Image leverages features\nextracted by semantic encoders instead of VAE, while VAEs are considered\nessential components in many image manipulation models. Motivated by such\ninspiring observations, we present a unified generative framework named\nUniWorld based on semantic features provided by powerful visual-language models\nand contrastive semantic encoders. As a result, we build a strong unified model\nusing only 1% amount of BAGEL's data, which consistently outperforms BAGEL on\nimage editing benchmarks. UniWorld also maintains competitive image\nunderstanding and generation capabilities, achieving strong performance across\nmultiple image perception tasks. We fully open-source our models, including\nmodel weights, training and evaluation scripts, and datasets.\n", "link": "http://arxiv.org/abs/2506.03147v1", "date": "2025-06-03", "relevancy": 3.0767, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6162}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6162}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6137}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniWorld%3A%20High-Resolution%20Semantic%20Encoders%20for%20Unified%20Visual%0A%20%20Understanding%20and%20Generation&body=Title%3A%20UniWorld%3A%20High-Resolution%20Semantic%20Encoders%20for%20Unified%20Visual%0A%20%20Understanding%20and%20Generation%0AAuthor%3A%20Bin%20Lin%20and%20Zongjian%20Li%20and%20Xinhua%20Cheng%20and%20Yuwei%20Niu%20and%20Yang%20Ye%20and%20Xianyi%20He%20and%20Shenghai%20Yuan%20and%20Wangbo%20Yu%20and%20Shaodong%20Wang%20and%20Yunyang%20Ge%20and%20Yatian%20Pang%20and%20Li%20Yuan%0AAbstract%3A%20%20%20Although%20existing%20unified%20models%20deliver%20strong%20performance%20on%0Avision-language%20understanding%20and%20text-to-image%20generation%2C%20their%20models%20are%0Alimited%20in%20exploring%20image%20perception%20and%20manipulation%20tasks%2C%20which%20are%0Aurgently%20desired%20by%20users%20for%20wide%20applications.%20Recently%2C%20OpenAI%20released%0Atheir%20powerful%20GPT-4o-Image%20model%20for%20comprehensive%20image%20perception%20and%0Amanipulation%2C%20achieving%20expressive%20capability%20and%20attracting%20community%0Ainterests.%20By%20observing%20the%20performance%20of%20GPT-4o-Image%20in%20our%20carefully%0Aconstructed%20experiments%2C%20we%20infer%20that%20GPT-4o-Image%20leverages%20features%0Aextracted%20by%20semantic%20encoders%20instead%20of%20VAE%2C%20while%20VAEs%20are%20considered%0Aessential%20components%20in%20many%20image%20manipulation%20models.%20Motivated%20by%20such%0Ainspiring%20observations%2C%20we%20present%20a%20unified%20generative%20framework%20named%0AUniWorld%20based%20on%20semantic%20features%20provided%20by%20powerful%20visual-language%20models%0Aand%20contrastive%20semantic%20encoders.%20As%20a%20result%2C%20we%20build%20a%20strong%20unified%20model%0Ausing%20only%201%25%20amount%20of%20BAGEL%27s%20data%2C%20which%20consistently%20outperforms%20BAGEL%20on%0Aimage%20editing%20benchmarks.%20UniWorld%20also%20maintains%20competitive%20image%0Aunderstanding%20and%20generation%20capabilities%2C%20achieving%20strong%20performance%20across%0Amultiple%20image%20perception%20tasks.%20We%20fully%20open-source%20our%20models%2C%20including%0Amodel%20weights%2C%20training%20and%20evaluation%20scripts%2C%20and%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03147v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniWorld%253A%2520High-Resolution%2520Semantic%2520Encoders%2520for%2520Unified%2520Visual%250A%2520%2520Understanding%2520and%2520Generation%26entry.906535625%3DBin%2520Lin%2520and%2520Zongjian%2520Li%2520and%2520Xinhua%2520Cheng%2520and%2520Yuwei%2520Niu%2520and%2520Yang%2520Ye%2520and%2520Xianyi%2520He%2520and%2520Shenghai%2520Yuan%2520and%2520Wangbo%2520Yu%2520and%2520Shaodong%2520Wang%2520and%2520Yunyang%2520Ge%2520and%2520Yatian%2520Pang%2520and%2520Li%2520Yuan%26entry.1292438233%3D%2520%2520Although%2520existing%2520unified%2520models%2520deliver%2520strong%2520performance%2520on%250Avision-language%2520understanding%2520and%2520text-to-image%2520generation%252C%2520their%2520models%2520are%250Alimited%2520in%2520exploring%2520image%2520perception%2520and%2520manipulation%2520tasks%252C%2520which%2520are%250Aurgently%2520desired%2520by%2520users%2520for%2520wide%2520applications.%2520Recently%252C%2520OpenAI%2520released%250Atheir%2520powerful%2520GPT-4o-Image%2520model%2520for%2520comprehensive%2520image%2520perception%2520and%250Amanipulation%252C%2520achieving%2520expressive%2520capability%2520and%2520attracting%2520community%250Ainterests.%2520By%2520observing%2520the%2520performance%2520of%2520GPT-4o-Image%2520in%2520our%2520carefully%250Aconstructed%2520experiments%252C%2520we%2520infer%2520that%2520GPT-4o-Image%2520leverages%2520features%250Aextracted%2520by%2520semantic%2520encoders%2520instead%2520of%2520VAE%252C%2520while%2520VAEs%2520are%2520considered%250Aessential%2520components%2520in%2520many%2520image%2520manipulation%2520models.%2520Motivated%2520by%2520such%250Ainspiring%2520observations%252C%2520we%2520present%2520a%2520unified%2520generative%2520framework%2520named%250AUniWorld%2520based%2520on%2520semantic%2520features%2520provided%2520by%2520powerful%2520visual-language%2520models%250Aand%2520contrastive%2520semantic%2520encoders.%2520As%2520a%2520result%252C%2520we%2520build%2520a%2520strong%2520unified%2520model%250Ausing%2520only%25201%2525%2520amount%2520of%2520BAGEL%2527s%2520data%252C%2520which%2520consistently%2520outperforms%2520BAGEL%2520on%250Aimage%2520editing%2520benchmarks.%2520UniWorld%2520also%2520maintains%2520competitive%2520image%250Aunderstanding%2520and%2520generation%2520capabilities%252C%2520achieving%2520strong%2520performance%2520across%250Amultiple%2520image%2520perception%2520tasks.%2520We%2520fully%2520open-source%2520our%2520models%252C%2520including%250Amodel%2520weights%252C%2520training%2520and%2520evaluation%2520scripts%252C%2520and%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03147v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniWorld%3A%20High-Resolution%20Semantic%20Encoders%20for%20Unified%20Visual%0A%20%20Understanding%20and%20Generation&entry.906535625=Bin%20Lin%20and%20Zongjian%20Li%20and%20Xinhua%20Cheng%20and%20Yuwei%20Niu%20and%20Yang%20Ye%20and%20Xianyi%20He%20and%20Shenghai%20Yuan%20and%20Wangbo%20Yu%20and%20Shaodong%20Wang%20and%20Yunyang%20Ge%20and%20Yatian%20Pang%20and%20Li%20Yuan&entry.1292438233=%20%20Although%20existing%20unified%20models%20deliver%20strong%20performance%20on%0Avision-language%20understanding%20and%20text-to-image%20generation%2C%20their%20models%20are%0Alimited%20in%20exploring%20image%20perception%20and%20manipulation%20tasks%2C%20which%20are%0Aurgently%20desired%20by%20users%20for%20wide%20applications.%20Recently%2C%20OpenAI%20released%0Atheir%20powerful%20GPT-4o-Image%20model%20for%20comprehensive%20image%20perception%20and%0Amanipulation%2C%20achieving%20expressive%20capability%20and%20attracting%20community%0Ainterests.%20By%20observing%20the%20performance%20of%20GPT-4o-Image%20in%20our%20carefully%0Aconstructed%20experiments%2C%20we%20infer%20that%20GPT-4o-Image%20leverages%20features%0Aextracted%20by%20semantic%20encoders%20instead%20of%20VAE%2C%20while%20VAEs%20are%20considered%0Aessential%20components%20in%20many%20image%20manipulation%20models.%20Motivated%20by%20such%0Ainspiring%20observations%2C%20we%20present%20a%20unified%20generative%20framework%20named%0AUniWorld%20based%20on%20semantic%20features%20provided%20by%20powerful%20visual-language%20models%0Aand%20contrastive%20semantic%20encoders.%20As%20a%20result%2C%20we%20build%20a%20strong%20unified%20model%0Ausing%20only%201%25%20amount%20of%20BAGEL%27s%20data%2C%20which%20consistently%20outperforms%20BAGEL%20on%0Aimage%20editing%20benchmarks.%20UniWorld%20also%20maintains%20competitive%20image%0Aunderstanding%20and%20generation%20capabilities%2C%20achieving%20strong%20performance%20across%0Amultiple%20image%20perception%20tasks.%20We%20fully%20open-source%20our%20models%2C%20including%0Amodel%20weights%2C%20training%20and%20evaluation%20scripts%2C%20and%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03147v1&entry.124074799=Read"},
{"title": "Learning Pyramid-structured Long-range Dependencies for 3D Human Pose\n  Estimation", "author": "Mingjie Wei and Xuemei Xie and Yutong Zhong and Guangming Shi", "abstract": "  Action coordination in human structure is indispensable for the spatial\nconstraints of 2D joints to recover 3D pose. Usually, action coordination is\nrepresented as a long-range dependence among body parts. However, there are two\nmain challenges in modeling long-range dependencies. First, joints should not\nonly be constrained by other individual joints but also be modulated by the\nbody parts. Second, existing methods make networks deeper to learn dependencies\nbetween non-linked parts. They introduce uncorrelated noise and increase the\nmodel size. In this paper, we utilize a pyramid structure to better learn\npotential long-range dependencies. It can capture the correlation across joints\nand groups, which complements the context of the human sub-structure. In an\neffective cross-scale way, it captures the pyramid-structured long-range\ndependence. Specifically, we propose a novel Pyramid Graph Attention (PGA)\nmodule to capture long-range cross-scale dependencies. It concatenates\ninformation from various scales into a compact sequence, and then computes the\ncorrelation between scales in parallel. Combining PGA with graph convolution\nmodules, we develop a Pyramid Graph Transformer (PGFormer) for 3D human pose\nestimation, which is a lightweight multi-scale transformer architecture. It\nencapsulates human sub-structures into self-attention by pooling. Extensive\nexperiments show that our approach achieves lower error and smaller model size\nthan state-of-the-art methods on Human3.6M and MPI-INF-3DHP datasets. The code\nis available at https://github.com/MingjieWe/PGFormer.\n", "link": "http://arxiv.org/abs/2506.02853v1", "date": "2025-06-03", "relevancy": 3.0589, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.645}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6088}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5816}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Pyramid-structured%20Long-range%20Dependencies%20for%203D%20Human%20Pose%0A%20%20Estimation&body=Title%3A%20Learning%20Pyramid-structured%20Long-range%20Dependencies%20for%203D%20Human%20Pose%0A%20%20Estimation%0AAuthor%3A%20Mingjie%20Wei%20and%20Xuemei%20Xie%20and%20Yutong%20Zhong%20and%20Guangming%20Shi%0AAbstract%3A%20%20%20Action%20coordination%20in%20human%20structure%20is%20indispensable%20for%20the%20spatial%0Aconstraints%20of%202D%20joints%20to%20recover%203D%20pose.%20Usually%2C%20action%20coordination%20is%0Arepresented%20as%20a%20long-range%20dependence%20among%20body%20parts.%20However%2C%20there%20are%20two%0Amain%20challenges%20in%20modeling%20long-range%20dependencies.%20First%2C%20joints%20should%20not%0Aonly%20be%20constrained%20by%20other%20individual%20joints%20but%20also%20be%20modulated%20by%20the%0Abody%20parts.%20Second%2C%20existing%20methods%20make%20networks%20deeper%20to%20learn%20dependencies%0Abetween%20non-linked%20parts.%20They%20introduce%20uncorrelated%20noise%20and%20increase%20the%0Amodel%20size.%20In%20this%20paper%2C%20we%20utilize%20a%20pyramid%20structure%20to%20better%20learn%0Apotential%20long-range%20dependencies.%20It%20can%20capture%20the%20correlation%20across%20joints%0Aand%20groups%2C%20which%20complements%20the%20context%20of%20the%20human%20sub-structure.%20In%20an%0Aeffective%20cross-scale%20way%2C%20it%20captures%20the%20pyramid-structured%20long-range%0Adependence.%20Specifically%2C%20we%20propose%20a%20novel%20Pyramid%20Graph%20Attention%20%28PGA%29%0Amodule%20to%20capture%20long-range%20cross-scale%20dependencies.%20It%20concatenates%0Ainformation%20from%20various%20scales%20into%20a%20compact%20sequence%2C%20and%20then%20computes%20the%0Acorrelation%20between%20scales%20in%20parallel.%20Combining%20PGA%20with%20graph%20convolution%0Amodules%2C%20we%20develop%20a%20Pyramid%20Graph%20Transformer%20%28PGFormer%29%20for%203D%20human%20pose%0Aestimation%2C%20which%20is%20a%20lightweight%20multi-scale%20transformer%20architecture.%20It%0Aencapsulates%20human%20sub-structures%20into%20self-attention%20by%20pooling.%20Extensive%0Aexperiments%20show%20that%20our%20approach%20achieves%20lower%20error%20and%20smaller%20model%20size%0Athan%20state-of-the-art%20methods%20on%20Human3.6M%20and%20MPI-INF-3DHP%20datasets.%20The%20code%0Ais%20available%20at%20https%3A//github.com/MingjieWe/PGFormer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02853v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Pyramid-structured%2520Long-range%2520Dependencies%2520for%25203D%2520Human%2520Pose%250A%2520%2520Estimation%26entry.906535625%3DMingjie%2520Wei%2520and%2520Xuemei%2520Xie%2520and%2520Yutong%2520Zhong%2520and%2520Guangming%2520Shi%26entry.1292438233%3D%2520%2520Action%2520coordination%2520in%2520human%2520structure%2520is%2520indispensable%2520for%2520the%2520spatial%250Aconstraints%2520of%25202D%2520joints%2520to%2520recover%25203D%2520pose.%2520Usually%252C%2520action%2520coordination%2520is%250Arepresented%2520as%2520a%2520long-range%2520dependence%2520among%2520body%2520parts.%2520However%252C%2520there%2520are%2520two%250Amain%2520challenges%2520in%2520modeling%2520long-range%2520dependencies.%2520First%252C%2520joints%2520should%2520not%250Aonly%2520be%2520constrained%2520by%2520other%2520individual%2520joints%2520but%2520also%2520be%2520modulated%2520by%2520the%250Abody%2520parts.%2520Second%252C%2520existing%2520methods%2520make%2520networks%2520deeper%2520to%2520learn%2520dependencies%250Abetween%2520non-linked%2520parts.%2520They%2520introduce%2520uncorrelated%2520noise%2520and%2520increase%2520the%250Amodel%2520size.%2520In%2520this%2520paper%252C%2520we%2520utilize%2520a%2520pyramid%2520structure%2520to%2520better%2520learn%250Apotential%2520long-range%2520dependencies.%2520It%2520can%2520capture%2520the%2520correlation%2520across%2520joints%250Aand%2520groups%252C%2520which%2520complements%2520the%2520context%2520of%2520the%2520human%2520sub-structure.%2520In%2520an%250Aeffective%2520cross-scale%2520way%252C%2520it%2520captures%2520the%2520pyramid-structured%2520long-range%250Adependence.%2520Specifically%252C%2520we%2520propose%2520a%2520novel%2520Pyramid%2520Graph%2520Attention%2520%2528PGA%2529%250Amodule%2520to%2520capture%2520long-range%2520cross-scale%2520dependencies.%2520It%2520concatenates%250Ainformation%2520from%2520various%2520scales%2520into%2520a%2520compact%2520sequence%252C%2520and%2520then%2520computes%2520the%250Acorrelation%2520between%2520scales%2520in%2520parallel.%2520Combining%2520PGA%2520with%2520graph%2520convolution%250Amodules%252C%2520we%2520develop%2520a%2520Pyramid%2520Graph%2520Transformer%2520%2528PGFormer%2529%2520for%25203D%2520human%2520pose%250Aestimation%252C%2520which%2520is%2520a%2520lightweight%2520multi-scale%2520transformer%2520architecture.%2520It%250Aencapsulates%2520human%2520sub-structures%2520into%2520self-attention%2520by%2520pooling.%2520Extensive%250Aexperiments%2520show%2520that%2520our%2520approach%2520achieves%2520lower%2520error%2520and%2520smaller%2520model%2520size%250Athan%2520state-of-the-art%2520methods%2520on%2520Human3.6M%2520and%2520MPI-INF-3DHP%2520datasets.%2520The%2520code%250Ais%2520available%2520at%2520https%253A//github.com/MingjieWe/PGFormer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02853v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Pyramid-structured%20Long-range%20Dependencies%20for%203D%20Human%20Pose%0A%20%20Estimation&entry.906535625=Mingjie%20Wei%20and%20Xuemei%20Xie%20and%20Yutong%20Zhong%20and%20Guangming%20Shi&entry.1292438233=%20%20Action%20coordination%20in%20human%20structure%20is%20indispensable%20for%20the%20spatial%0Aconstraints%20of%202D%20joints%20to%20recover%203D%20pose.%20Usually%2C%20action%20coordination%20is%0Arepresented%20as%20a%20long-range%20dependence%20among%20body%20parts.%20However%2C%20there%20are%20two%0Amain%20challenges%20in%20modeling%20long-range%20dependencies.%20First%2C%20joints%20should%20not%0Aonly%20be%20constrained%20by%20other%20individual%20joints%20but%20also%20be%20modulated%20by%20the%0Abody%20parts.%20Second%2C%20existing%20methods%20make%20networks%20deeper%20to%20learn%20dependencies%0Abetween%20non-linked%20parts.%20They%20introduce%20uncorrelated%20noise%20and%20increase%20the%0Amodel%20size.%20In%20this%20paper%2C%20we%20utilize%20a%20pyramid%20structure%20to%20better%20learn%0Apotential%20long-range%20dependencies.%20It%20can%20capture%20the%20correlation%20across%20joints%0Aand%20groups%2C%20which%20complements%20the%20context%20of%20the%20human%20sub-structure.%20In%20an%0Aeffective%20cross-scale%20way%2C%20it%20captures%20the%20pyramid-structured%20long-range%0Adependence.%20Specifically%2C%20we%20propose%20a%20novel%20Pyramid%20Graph%20Attention%20%28PGA%29%0Amodule%20to%20capture%20long-range%20cross-scale%20dependencies.%20It%20concatenates%0Ainformation%20from%20various%20scales%20into%20a%20compact%20sequence%2C%20and%20then%20computes%20the%0Acorrelation%20between%20scales%20in%20parallel.%20Combining%20PGA%20with%20graph%20convolution%0Amodules%2C%20we%20develop%20a%20Pyramid%20Graph%20Transformer%20%28PGFormer%29%20for%203D%20human%20pose%0Aestimation%2C%20which%20is%20a%20lightweight%20multi-scale%20transformer%20architecture.%20It%0Aencapsulates%20human%20sub-structures%20into%20self-attention%20by%20pooling.%20Extensive%0Aexperiments%20show%20that%20our%20approach%20achieves%20lower%20error%20and%20smaller%20model%20size%0Athan%20state-of-the-art%20methods%20on%20Human3.6M%20and%20MPI-INF-3DHP%20datasets.%20The%20code%0Ais%20available%20at%20https%3A//github.com/MingjieWe/PGFormer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02853v1&entry.124074799=Read"},
{"title": "Pro3D-Editor : A Progressive-Views Perspective for Consistent and\n  Precise 3D Editing", "author": "Yang Zheng and Mengqi Huang and Nan Chen and Zhendong Mao", "abstract": "  Text-guided 3D editing aims to precisely edit semantically relevant local 3D\nregions, which has significant potential for various practical applications\nranging from 3D games to film production. Existing methods typically follow a\nview-indiscriminate paradigm: editing 2D views indiscriminately and projecting\nthem back into 3D space. However, they overlook the different cross-view\ninterdependencies, resulting in inconsistent multi-view editing. In this study,\nwe argue that ideal consistent 3D editing can be achieved through a\n\\textit{progressive-views paradigm}, which propagates editing semantics from\nthe editing-salient view to other editing-sparse views. Specifically, we\npropose \\textit{Pro3D-Editor}, a novel framework, which mainly includes\nPrimary-view Sampler, Key-view Render, and Full-view Refiner. Primary-view\nSampler dynamically samples and edits the most editing-salient view as the\nprimary view. Key-view Render accurately propagates editing semantics from the\nprimary view to other key views through its Mixture-of-View-Experts Low-Rank\nAdaption (MoVE-LoRA). Full-view Refiner edits and refines the 3D object based\non the edited multi-views. Extensive experiments demonstrate that our method\noutperforms existing methods in editing accuracy and spatial consistency.\n", "link": "http://arxiv.org/abs/2506.00512v2", "date": "2025-06-03", "relevancy": 3.0251, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6171}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6171}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5808}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pro3D-Editor%20%3A%20A%20Progressive-Views%20Perspective%20for%20Consistent%20and%0A%20%20Precise%203D%20Editing&body=Title%3A%20Pro3D-Editor%20%3A%20A%20Progressive-Views%20Perspective%20for%20Consistent%20and%0A%20%20Precise%203D%20Editing%0AAuthor%3A%20Yang%20Zheng%20and%20Mengqi%20Huang%20and%20Nan%20Chen%20and%20Zhendong%20Mao%0AAbstract%3A%20%20%20Text-guided%203D%20editing%20aims%20to%20precisely%20edit%20semantically%20relevant%20local%203D%0Aregions%2C%20which%20has%20significant%20potential%20for%20various%20practical%20applications%0Aranging%20from%203D%20games%20to%20film%20production.%20Existing%20methods%20typically%20follow%20a%0Aview-indiscriminate%20paradigm%3A%20editing%202D%20views%20indiscriminately%20and%20projecting%0Athem%20back%20into%203D%20space.%20However%2C%20they%20overlook%20the%20different%20cross-view%0Ainterdependencies%2C%20resulting%20in%20inconsistent%20multi-view%20editing.%20In%20this%20study%2C%0Awe%20argue%20that%20ideal%20consistent%203D%20editing%20can%20be%20achieved%20through%20a%0A%5Ctextit%7Bprogressive-views%20paradigm%7D%2C%20which%20propagates%20editing%20semantics%20from%0Athe%20editing-salient%20view%20to%20other%20editing-sparse%20views.%20Specifically%2C%20we%0Apropose%20%5Ctextit%7BPro3D-Editor%7D%2C%20a%20novel%20framework%2C%20which%20mainly%20includes%0APrimary-view%20Sampler%2C%20Key-view%20Render%2C%20and%20Full-view%20Refiner.%20Primary-view%0ASampler%20dynamically%20samples%20and%20edits%20the%20most%20editing-salient%20view%20as%20the%0Aprimary%20view.%20Key-view%20Render%20accurately%20propagates%20editing%20semantics%20from%20the%0Aprimary%20view%20to%20other%20key%20views%20through%20its%20Mixture-of-View-Experts%20Low-Rank%0AAdaption%20%28MoVE-LoRA%29.%20Full-view%20Refiner%20edits%20and%20refines%20the%203D%20object%20based%0Aon%20the%20edited%20multi-views.%20Extensive%20experiments%20demonstrate%20that%20our%20method%0Aoutperforms%20existing%20methods%20in%20editing%20accuracy%20and%20spatial%20consistency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.00512v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPro3D-Editor%2520%253A%2520A%2520Progressive-Views%2520Perspective%2520for%2520Consistent%2520and%250A%2520%2520Precise%25203D%2520Editing%26entry.906535625%3DYang%2520Zheng%2520and%2520Mengqi%2520Huang%2520and%2520Nan%2520Chen%2520and%2520Zhendong%2520Mao%26entry.1292438233%3D%2520%2520Text-guided%25203D%2520editing%2520aims%2520to%2520precisely%2520edit%2520semantically%2520relevant%2520local%25203D%250Aregions%252C%2520which%2520has%2520significant%2520potential%2520for%2520various%2520practical%2520applications%250Aranging%2520from%25203D%2520games%2520to%2520film%2520production.%2520Existing%2520methods%2520typically%2520follow%2520a%250Aview-indiscriminate%2520paradigm%253A%2520editing%25202D%2520views%2520indiscriminately%2520and%2520projecting%250Athem%2520back%2520into%25203D%2520space.%2520However%252C%2520they%2520overlook%2520the%2520different%2520cross-view%250Ainterdependencies%252C%2520resulting%2520in%2520inconsistent%2520multi-view%2520editing.%2520In%2520this%2520study%252C%250Awe%2520argue%2520that%2520ideal%2520consistent%25203D%2520editing%2520can%2520be%2520achieved%2520through%2520a%250A%255Ctextit%257Bprogressive-views%2520paradigm%257D%252C%2520which%2520propagates%2520editing%2520semantics%2520from%250Athe%2520editing-salient%2520view%2520to%2520other%2520editing-sparse%2520views.%2520Specifically%252C%2520we%250Apropose%2520%255Ctextit%257BPro3D-Editor%257D%252C%2520a%2520novel%2520framework%252C%2520which%2520mainly%2520includes%250APrimary-view%2520Sampler%252C%2520Key-view%2520Render%252C%2520and%2520Full-view%2520Refiner.%2520Primary-view%250ASampler%2520dynamically%2520samples%2520and%2520edits%2520the%2520most%2520editing-salient%2520view%2520as%2520the%250Aprimary%2520view.%2520Key-view%2520Render%2520accurately%2520propagates%2520editing%2520semantics%2520from%2520the%250Aprimary%2520view%2520to%2520other%2520key%2520views%2520through%2520its%2520Mixture-of-View-Experts%2520Low-Rank%250AAdaption%2520%2528MoVE-LoRA%2529.%2520Full-view%2520Refiner%2520edits%2520and%2520refines%2520the%25203D%2520object%2520based%250Aon%2520the%2520edited%2520multi-views.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%250Aoutperforms%2520existing%2520methods%2520in%2520editing%2520accuracy%2520and%2520spatial%2520consistency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.00512v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pro3D-Editor%20%3A%20A%20Progressive-Views%20Perspective%20for%20Consistent%20and%0A%20%20Precise%203D%20Editing&entry.906535625=Yang%20Zheng%20and%20Mengqi%20Huang%20and%20Nan%20Chen%20and%20Zhendong%20Mao&entry.1292438233=%20%20Text-guided%203D%20editing%20aims%20to%20precisely%20edit%20semantically%20relevant%20local%203D%0Aregions%2C%20which%20has%20significant%20potential%20for%20various%20practical%20applications%0Aranging%20from%203D%20games%20to%20film%20production.%20Existing%20methods%20typically%20follow%20a%0Aview-indiscriminate%20paradigm%3A%20editing%202D%20views%20indiscriminately%20and%20projecting%0Athem%20back%20into%203D%20space.%20However%2C%20they%20overlook%20the%20different%20cross-view%0Ainterdependencies%2C%20resulting%20in%20inconsistent%20multi-view%20editing.%20In%20this%20study%2C%0Awe%20argue%20that%20ideal%20consistent%203D%20editing%20can%20be%20achieved%20through%20a%0A%5Ctextit%7Bprogressive-views%20paradigm%7D%2C%20which%20propagates%20editing%20semantics%20from%0Athe%20editing-salient%20view%20to%20other%20editing-sparse%20views.%20Specifically%2C%20we%0Apropose%20%5Ctextit%7BPro3D-Editor%7D%2C%20a%20novel%20framework%2C%20which%20mainly%20includes%0APrimary-view%20Sampler%2C%20Key-view%20Render%2C%20and%20Full-view%20Refiner.%20Primary-view%0ASampler%20dynamically%20samples%20and%20edits%20the%20most%20editing-salient%20view%20as%20the%0Aprimary%20view.%20Key-view%20Render%20accurately%20propagates%20editing%20semantics%20from%20the%0Aprimary%20view%20to%20other%20key%20views%20through%20its%20Mixture-of-View-Experts%20Low-Rank%0AAdaption%20%28MoVE-LoRA%29.%20Full-view%20Refiner%20edits%20and%20refines%20the%203D%20object%20based%0Aon%20the%20edited%20multi-views.%20Extensive%20experiments%20demonstrate%20that%20our%20method%0Aoutperforms%20existing%20methods%20in%20editing%20accuracy%20and%20spatial%20consistency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.00512v2&entry.124074799=Read"},
{"title": "PhysGaia: A Physics-Aware Dataset of Multi-Body Interactions for Dynamic\n  Novel View Synthesis", "author": "Mijeong Kim and Gunhee Kim and Jungyoon Choi and Wonjae Roh and Bohyung Han", "abstract": "  We introduce PhysGaia, a novel physics-aware dataset specifically designed\nfor Dynamic Novel View Synthesis (DyNVS), encompassing both structured objects\nand unstructured physical phenomena. Unlike existing datasets that primarily\nfocus on photorealistic reconstruction, PhysGaia is created to actively support\nphysics-aware dynamic scene modeling. Our dataset provides complex dynamic\nscenarios with rich interactions among multiple objects, where they\nrealistically collide with each other and exchange forces. Furthermore, it\ncontains a diverse range of physical materials, such as liquid, gas,\nviscoelastic substance, and textile, which moves beyond the rigid bodies\nprevalent in existing datasets. All scenes in PhysGaia are faithfully generated\nto strictly adhere to physical laws, leveraging carefully selected\nmaterial-specific physics solvers. To enable quantitative evaluation of\nphysical modeling, our dataset provides essential ground-truth information,\nincluding 3D particle trajectories and physics parameters, e.g., viscosity. To\nfacilitate research adoption, we also provide essential integration pipelines\nfor using state-of-the-art DyNVS models with our dataset and report their\nresults. By addressing the critical lack of datasets for physics-aware\nmodeling, PhysGaia will significantly advance research in dynamic view\nsynthesis, physics-based scene understanding, and deep learning models\nintegrated with physical simulation -- ultimately enabling more faithful\nreconstruction and interpretation of complex dynamic scenes. Our datasets and\ncodes are available in the project website,\nhttp://cvlab.snu.ac.kr/research/PhysGaia.\n", "link": "http://arxiv.org/abs/2506.02794v1", "date": "2025-06-03", "relevancy": 3.0123, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6299}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5897}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PhysGaia%3A%20A%20Physics-Aware%20Dataset%20of%20Multi-Body%20Interactions%20for%20Dynamic%0A%20%20Novel%20View%20Synthesis&body=Title%3A%20PhysGaia%3A%20A%20Physics-Aware%20Dataset%20of%20Multi-Body%20Interactions%20for%20Dynamic%0A%20%20Novel%20View%20Synthesis%0AAuthor%3A%20Mijeong%20Kim%20and%20Gunhee%20Kim%20and%20Jungyoon%20Choi%20and%20Wonjae%20Roh%20and%20Bohyung%20Han%0AAbstract%3A%20%20%20We%20introduce%20PhysGaia%2C%20a%20novel%20physics-aware%20dataset%20specifically%20designed%0Afor%20Dynamic%20Novel%20View%20Synthesis%20%28DyNVS%29%2C%20encompassing%20both%20structured%20objects%0Aand%20unstructured%20physical%20phenomena.%20Unlike%20existing%20datasets%20that%20primarily%0Afocus%20on%20photorealistic%20reconstruction%2C%20PhysGaia%20is%20created%20to%20actively%20support%0Aphysics-aware%20dynamic%20scene%20modeling.%20Our%20dataset%20provides%20complex%20dynamic%0Ascenarios%20with%20rich%20interactions%20among%20multiple%20objects%2C%20where%20they%0Arealistically%20collide%20with%20each%20other%20and%20exchange%20forces.%20Furthermore%2C%20it%0Acontains%20a%20diverse%20range%20of%20physical%20materials%2C%20such%20as%20liquid%2C%20gas%2C%0Aviscoelastic%20substance%2C%20and%20textile%2C%20which%20moves%20beyond%20the%20rigid%20bodies%0Aprevalent%20in%20existing%20datasets.%20All%20scenes%20in%20PhysGaia%20are%20faithfully%20generated%0Ato%20strictly%20adhere%20to%20physical%20laws%2C%20leveraging%20carefully%20selected%0Amaterial-specific%20physics%20solvers.%20To%20enable%20quantitative%20evaluation%20of%0Aphysical%20modeling%2C%20our%20dataset%20provides%20essential%20ground-truth%20information%2C%0Aincluding%203D%20particle%20trajectories%20and%20physics%20parameters%2C%20e.g.%2C%20viscosity.%20To%0Afacilitate%20research%20adoption%2C%20we%20also%20provide%20essential%20integration%20pipelines%0Afor%20using%20state-of-the-art%20DyNVS%20models%20with%20our%20dataset%20and%20report%20their%0Aresults.%20By%20addressing%20the%20critical%20lack%20of%20datasets%20for%20physics-aware%0Amodeling%2C%20PhysGaia%20will%20significantly%20advance%20research%20in%20dynamic%20view%0Asynthesis%2C%20physics-based%20scene%20understanding%2C%20and%20deep%20learning%20models%0Aintegrated%20with%20physical%20simulation%20--%20ultimately%20enabling%20more%20faithful%0Areconstruction%20and%20interpretation%20of%20complex%20dynamic%20scenes.%20Our%20datasets%20and%0Acodes%20are%20available%20in%20the%20project%20website%2C%0Ahttp%3A//cvlab.snu.ac.kr/research/PhysGaia.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02794v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhysGaia%253A%2520A%2520Physics-Aware%2520Dataset%2520of%2520Multi-Body%2520Interactions%2520for%2520Dynamic%250A%2520%2520Novel%2520View%2520Synthesis%26entry.906535625%3DMijeong%2520Kim%2520and%2520Gunhee%2520Kim%2520and%2520Jungyoon%2520Choi%2520and%2520Wonjae%2520Roh%2520and%2520Bohyung%2520Han%26entry.1292438233%3D%2520%2520We%2520introduce%2520PhysGaia%252C%2520a%2520novel%2520physics-aware%2520dataset%2520specifically%2520designed%250Afor%2520Dynamic%2520Novel%2520View%2520Synthesis%2520%2528DyNVS%2529%252C%2520encompassing%2520both%2520structured%2520objects%250Aand%2520unstructured%2520physical%2520phenomena.%2520Unlike%2520existing%2520datasets%2520that%2520primarily%250Afocus%2520on%2520photorealistic%2520reconstruction%252C%2520PhysGaia%2520is%2520created%2520to%2520actively%2520support%250Aphysics-aware%2520dynamic%2520scene%2520modeling.%2520Our%2520dataset%2520provides%2520complex%2520dynamic%250Ascenarios%2520with%2520rich%2520interactions%2520among%2520multiple%2520objects%252C%2520where%2520they%250Arealistically%2520collide%2520with%2520each%2520other%2520and%2520exchange%2520forces.%2520Furthermore%252C%2520it%250Acontains%2520a%2520diverse%2520range%2520of%2520physical%2520materials%252C%2520such%2520as%2520liquid%252C%2520gas%252C%250Aviscoelastic%2520substance%252C%2520and%2520textile%252C%2520which%2520moves%2520beyond%2520the%2520rigid%2520bodies%250Aprevalent%2520in%2520existing%2520datasets.%2520All%2520scenes%2520in%2520PhysGaia%2520are%2520faithfully%2520generated%250Ato%2520strictly%2520adhere%2520to%2520physical%2520laws%252C%2520leveraging%2520carefully%2520selected%250Amaterial-specific%2520physics%2520solvers.%2520To%2520enable%2520quantitative%2520evaluation%2520of%250Aphysical%2520modeling%252C%2520our%2520dataset%2520provides%2520essential%2520ground-truth%2520information%252C%250Aincluding%25203D%2520particle%2520trajectories%2520and%2520physics%2520parameters%252C%2520e.g.%252C%2520viscosity.%2520To%250Afacilitate%2520research%2520adoption%252C%2520we%2520also%2520provide%2520essential%2520integration%2520pipelines%250Afor%2520using%2520state-of-the-art%2520DyNVS%2520models%2520with%2520our%2520dataset%2520and%2520report%2520their%250Aresults.%2520By%2520addressing%2520the%2520critical%2520lack%2520of%2520datasets%2520for%2520physics-aware%250Amodeling%252C%2520PhysGaia%2520will%2520significantly%2520advance%2520research%2520in%2520dynamic%2520view%250Asynthesis%252C%2520physics-based%2520scene%2520understanding%252C%2520and%2520deep%2520learning%2520models%250Aintegrated%2520with%2520physical%2520simulation%2520--%2520ultimately%2520enabling%2520more%2520faithful%250Areconstruction%2520and%2520interpretation%2520of%2520complex%2520dynamic%2520scenes.%2520Our%2520datasets%2520and%250Acodes%2520are%2520available%2520in%2520the%2520project%2520website%252C%250Ahttp%253A//cvlab.snu.ac.kr/research/PhysGaia.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02794v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PhysGaia%3A%20A%20Physics-Aware%20Dataset%20of%20Multi-Body%20Interactions%20for%20Dynamic%0A%20%20Novel%20View%20Synthesis&entry.906535625=Mijeong%20Kim%20and%20Gunhee%20Kim%20and%20Jungyoon%20Choi%20and%20Wonjae%20Roh%20and%20Bohyung%20Han&entry.1292438233=%20%20We%20introduce%20PhysGaia%2C%20a%20novel%20physics-aware%20dataset%20specifically%20designed%0Afor%20Dynamic%20Novel%20View%20Synthesis%20%28DyNVS%29%2C%20encompassing%20both%20structured%20objects%0Aand%20unstructured%20physical%20phenomena.%20Unlike%20existing%20datasets%20that%20primarily%0Afocus%20on%20photorealistic%20reconstruction%2C%20PhysGaia%20is%20created%20to%20actively%20support%0Aphysics-aware%20dynamic%20scene%20modeling.%20Our%20dataset%20provides%20complex%20dynamic%0Ascenarios%20with%20rich%20interactions%20among%20multiple%20objects%2C%20where%20they%0Arealistically%20collide%20with%20each%20other%20and%20exchange%20forces.%20Furthermore%2C%20it%0Acontains%20a%20diverse%20range%20of%20physical%20materials%2C%20such%20as%20liquid%2C%20gas%2C%0Aviscoelastic%20substance%2C%20and%20textile%2C%20which%20moves%20beyond%20the%20rigid%20bodies%0Aprevalent%20in%20existing%20datasets.%20All%20scenes%20in%20PhysGaia%20are%20faithfully%20generated%0Ato%20strictly%20adhere%20to%20physical%20laws%2C%20leveraging%20carefully%20selected%0Amaterial-specific%20physics%20solvers.%20To%20enable%20quantitative%20evaluation%20of%0Aphysical%20modeling%2C%20our%20dataset%20provides%20essential%20ground-truth%20information%2C%0Aincluding%203D%20particle%20trajectories%20and%20physics%20parameters%2C%20e.g.%2C%20viscosity.%20To%0Afacilitate%20research%20adoption%2C%20we%20also%20provide%20essential%20integration%20pipelines%0Afor%20using%20state-of-the-art%20DyNVS%20models%20with%20our%20dataset%20and%20report%20their%0Aresults.%20By%20addressing%20the%20critical%20lack%20of%20datasets%20for%20physics-aware%0Amodeling%2C%20PhysGaia%20will%20significantly%20advance%20research%20in%20dynamic%20view%0Asynthesis%2C%20physics-based%20scene%20understanding%2C%20and%20deep%20learning%20models%0Aintegrated%20with%20physical%20simulation%20--%20ultimately%20enabling%20more%20faithful%0Areconstruction%20and%20interpretation%20of%20complex%20dynamic%20scenes.%20Our%20datasets%20and%0Acodes%20are%20available%20in%20the%20project%20website%2C%0Ahttp%3A//cvlab.snu.ac.kr/research/PhysGaia.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02794v1&entry.124074799=Read"},
{"title": "OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for\n  Vision Language Models", "author": "Mengdi Jia and Zekun Qi and Shaochen Zhang and Wenyao Zhang and Xinqiang Yu and Jiawei He and He Wang and Li Yi", "abstract": "  Spatial reasoning is a key aspect of cognitive psychology and remains a major\nbottleneck for current vision-language models (VLMs). While extensive research\nhas aimed to evaluate or improve VLMs' understanding of basic spatial\nrelations, such as distinguishing left from right, near from far, and object\ncounting, these tasks represent only the most fundamental level of spatial\nreasoning. In this work, we introduce OmniSpatial, a comprehensive and\nchallenging benchmark for spatial reasoning, grounded in cognitive psychology.\nOmniSpatial covers four major categories: dynamic reasoning, complex spatial\nlogic, spatial interaction, and perspective-taking, with 50 fine-grained\nsubcategories. Through Internet data crawling and careful manual annotation, we\nconstruct over 1.5K question-answer pairs. Extensive experiments show that both\nopen- and closed-source VLMs, as well as existing reasoning and spatial\nunderstanding models, exhibit significant limitations in comprehensive spatial\nunderstanding. We further analyze failure cases and propose potential\ndirections for future research.\n", "link": "http://arxiv.org/abs/2506.03135v1", "date": "2025-06-03", "relevancy": 3.0122, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6303}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6303}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniSpatial%3A%20Towards%20Comprehensive%20Spatial%20Reasoning%20Benchmark%20for%0A%20%20Vision%20Language%20Models&body=Title%3A%20OmniSpatial%3A%20Towards%20Comprehensive%20Spatial%20Reasoning%20Benchmark%20for%0A%20%20Vision%20Language%20Models%0AAuthor%3A%20Mengdi%20Jia%20and%20Zekun%20Qi%20and%20Shaochen%20Zhang%20and%20Wenyao%20Zhang%20and%20Xinqiang%20Yu%20and%20Jiawei%20He%20and%20He%20Wang%20and%20Li%20Yi%0AAbstract%3A%20%20%20Spatial%20reasoning%20is%20a%20key%20aspect%20of%20cognitive%20psychology%20and%20remains%20a%20major%0Abottleneck%20for%20current%20vision-language%20models%20%28VLMs%29.%20While%20extensive%20research%0Ahas%20aimed%20to%20evaluate%20or%20improve%20VLMs%27%20understanding%20of%20basic%20spatial%0Arelations%2C%20such%20as%20distinguishing%20left%20from%20right%2C%20near%20from%20far%2C%20and%20object%0Acounting%2C%20these%20tasks%20represent%20only%20the%20most%20fundamental%20level%20of%20spatial%0Areasoning.%20In%20this%20work%2C%20we%20introduce%20OmniSpatial%2C%20a%20comprehensive%20and%0Achallenging%20benchmark%20for%20spatial%20reasoning%2C%20grounded%20in%20cognitive%20psychology.%0AOmniSpatial%20covers%20four%20major%20categories%3A%20dynamic%20reasoning%2C%20complex%20spatial%0Alogic%2C%20spatial%20interaction%2C%20and%20perspective-taking%2C%20with%2050%20fine-grained%0Asubcategories.%20Through%20Internet%20data%20crawling%20and%20careful%20manual%20annotation%2C%20we%0Aconstruct%20over%201.5K%20question-answer%20pairs.%20Extensive%20experiments%20show%20that%20both%0Aopen-%20and%20closed-source%20VLMs%2C%20as%20well%20as%20existing%20reasoning%20and%20spatial%0Aunderstanding%20models%2C%20exhibit%20significant%20limitations%20in%20comprehensive%20spatial%0Aunderstanding.%20We%20further%20analyze%20failure%20cases%20and%20propose%20potential%0Adirections%20for%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03135v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniSpatial%253A%2520Towards%2520Comprehensive%2520Spatial%2520Reasoning%2520Benchmark%2520for%250A%2520%2520Vision%2520Language%2520Models%26entry.906535625%3DMengdi%2520Jia%2520and%2520Zekun%2520Qi%2520and%2520Shaochen%2520Zhang%2520and%2520Wenyao%2520Zhang%2520and%2520Xinqiang%2520Yu%2520and%2520Jiawei%2520He%2520and%2520He%2520Wang%2520and%2520Li%2520Yi%26entry.1292438233%3D%2520%2520Spatial%2520reasoning%2520is%2520a%2520key%2520aspect%2520of%2520cognitive%2520psychology%2520and%2520remains%2520a%2520major%250Abottleneck%2520for%2520current%2520vision-language%2520models%2520%2528VLMs%2529.%2520While%2520extensive%2520research%250Ahas%2520aimed%2520to%2520evaluate%2520or%2520improve%2520VLMs%2527%2520understanding%2520of%2520basic%2520spatial%250Arelations%252C%2520such%2520as%2520distinguishing%2520left%2520from%2520right%252C%2520near%2520from%2520far%252C%2520and%2520object%250Acounting%252C%2520these%2520tasks%2520represent%2520only%2520the%2520most%2520fundamental%2520level%2520of%2520spatial%250Areasoning.%2520In%2520this%2520work%252C%2520we%2520introduce%2520OmniSpatial%252C%2520a%2520comprehensive%2520and%250Achallenging%2520benchmark%2520for%2520spatial%2520reasoning%252C%2520grounded%2520in%2520cognitive%2520psychology.%250AOmniSpatial%2520covers%2520four%2520major%2520categories%253A%2520dynamic%2520reasoning%252C%2520complex%2520spatial%250Alogic%252C%2520spatial%2520interaction%252C%2520and%2520perspective-taking%252C%2520with%252050%2520fine-grained%250Asubcategories.%2520Through%2520Internet%2520data%2520crawling%2520and%2520careful%2520manual%2520annotation%252C%2520we%250Aconstruct%2520over%25201.5K%2520question-answer%2520pairs.%2520Extensive%2520experiments%2520show%2520that%2520both%250Aopen-%2520and%2520closed-source%2520VLMs%252C%2520as%2520well%2520as%2520existing%2520reasoning%2520and%2520spatial%250Aunderstanding%2520models%252C%2520exhibit%2520significant%2520limitations%2520in%2520comprehensive%2520spatial%250Aunderstanding.%2520We%2520further%2520analyze%2520failure%2520cases%2520and%2520propose%2520potential%250Adirections%2520for%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03135v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniSpatial%3A%20Towards%20Comprehensive%20Spatial%20Reasoning%20Benchmark%20for%0A%20%20Vision%20Language%20Models&entry.906535625=Mengdi%20Jia%20and%20Zekun%20Qi%20and%20Shaochen%20Zhang%20and%20Wenyao%20Zhang%20and%20Xinqiang%20Yu%20and%20Jiawei%20He%20and%20He%20Wang%20and%20Li%20Yi&entry.1292438233=%20%20Spatial%20reasoning%20is%20a%20key%20aspect%20of%20cognitive%20psychology%20and%20remains%20a%20major%0Abottleneck%20for%20current%20vision-language%20models%20%28VLMs%29.%20While%20extensive%20research%0Ahas%20aimed%20to%20evaluate%20or%20improve%20VLMs%27%20understanding%20of%20basic%20spatial%0Arelations%2C%20such%20as%20distinguishing%20left%20from%20right%2C%20near%20from%20far%2C%20and%20object%0Acounting%2C%20these%20tasks%20represent%20only%20the%20most%20fundamental%20level%20of%20spatial%0Areasoning.%20In%20this%20work%2C%20we%20introduce%20OmniSpatial%2C%20a%20comprehensive%20and%0Achallenging%20benchmark%20for%20spatial%20reasoning%2C%20grounded%20in%20cognitive%20psychology.%0AOmniSpatial%20covers%20four%20major%20categories%3A%20dynamic%20reasoning%2C%20complex%20spatial%0Alogic%2C%20spatial%20interaction%2C%20and%20perspective-taking%2C%20with%2050%20fine-grained%0Asubcategories.%20Through%20Internet%20data%20crawling%20and%20careful%20manual%20annotation%2C%20we%0Aconstruct%20over%201.5K%20question-answer%20pairs.%20Extensive%20experiments%20show%20that%20both%0Aopen-%20and%20closed-source%20VLMs%2C%20as%20well%20as%20existing%20reasoning%20and%20spatial%0Aunderstanding%20models%2C%20exhibit%20significant%20limitations%20in%20comprehensive%20spatial%0Aunderstanding.%20We%20further%20analyze%20failure%20cases%20and%20propose%20potential%0Adirections%20for%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03135v1&entry.124074799=Read"},
{"title": "HaploOmni: Unified Single Transformer for Multimodal Video Understanding\n  and Generation", "author": "Yicheng Xiao and Lin Song and Rui Yang and Cheng Cheng and Zunnan Xu and Zhaoyang Zhang and Yixiao Ge and Xiu Li and Ying Shan", "abstract": "  With the advancement of language models, unified multimodal understanding and\ngeneration have made significant strides, with model architectures evolving\nfrom separated components to unified single-model frameworks. This paper\nexplores an efficient training paradigm to build a single transformer for\nunified multimodal understanding and generation. Specifically, we propose a\nmultimodal warmup strategy utilizing prior knowledge to extend capabilities. To\naddress cross-modal compatibility challenges, we introduce feature pre-scaling\nand multimodal AdaLN techniques. Integrating the proposed technologies, we\npresent the HaploOmni, a new single multimodal transformer. With limited\ntraining costs, HaploOmni achieves competitive performance across multiple\nimage and video understanding and generation benchmarks over advanced unified\nmodels. All codes will be made public at https://github.com/Tencent/HaploVLM.\n", "link": "http://arxiv.org/abs/2506.02975v1", "date": "2025-06-03", "relevancy": 2.9917, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6407}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5771}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5771}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HaploOmni%3A%20Unified%20Single%20Transformer%20for%20Multimodal%20Video%20Understanding%0A%20%20and%20Generation&body=Title%3A%20HaploOmni%3A%20Unified%20Single%20Transformer%20for%20Multimodal%20Video%20Understanding%0A%20%20and%20Generation%0AAuthor%3A%20Yicheng%20Xiao%20and%20Lin%20Song%20and%20Rui%20Yang%20and%20Cheng%20Cheng%20and%20Zunnan%20Xu%20and%20Zhaoyang%20Zhang%20and%20Yixiao%20Ge%20and%20Xiu%20Li%20and%20Ying%20Shan%0AAbstract%3A%20%20%20With%20the%20advancement%20of%20language%20models%2C%20unified%20multimodal%20understanding%20and%0Ageneration%20have%20made%20significant%20strides%2C%20with%20model%20architectures%20evolving%0Afrom%20separated%20components%20to%20unified%20single-model%20frameworks.%20This%20paper%0Aexplores%20an%20efficient%20training%20paradigm%20to%20build%20a%20single%20transformer%20for%0Aunified%20multimodal%20understanding%20and%20generation.%20Specifically%2C%20we%20propose%20a%0Amultimodal%20warmup%20strategy%20utilizing%20prior%20knowledge%20to%20extend%20capabilities.%20To%0Aaddress%20cross-modal%20compatibility%20challenges%2C%20we%20introduce%20feature%20pre-scaling%0Aand%20multimodal%20AdaLN%20techniques.%20Integrating%20the%20proposed%20technologies%2C%20we%0Apresent%20the%20HaploOmni%2C%20a%20new%20single%20multimodal%20transformer.%20With%20limited%0Atraining%20costs%2C%20HaploOmni%20achieves%20competitive%20performance%20across%20multiple%0Aimage%20and%20video%20understanding%20and%20generation%20benchmarks%20over%20advanced%20unified%0Amodels.%20All%20codes%20will%20be%20made%20public%20at%20https%3A//github.com/Tencent/HaploVLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02975v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHaploOmni%253A%2520Unified%2520Single%2520Transformer%2520for%2520Multimodal%2520Video%2520Understanding%250A%2520%2520and%2520Generation%26entry.906535625%3DYicheng%2520Xiao%2520and%2520Lin%2520Song%2520and%2520Rui%2520Yang%2520and%2520Cheng%2520Cheng%2520and%2520Zunnan%2520Xu%2520and%2520Zhaoyang%2520Zhang%2520and%2520Yixiao%2520Ge%2520and%2520Xiu%2520Li%2520and%2520Ying%2520Shan%26entry.1292438233%3D%2520%2520With%2520the%2520advancement%2520of%2520language%2520models%252C%2520unified%2520multimodal%2520understanding%2520and%250Ageneration%2520have%2520made%2520significant%2520strides%252C%2520with%2520model%2520architectures%2520evolving%250Afrom%2520separated%2520components%2520to%2520unified%2520single-model%2520frameworks.%2520This%2520paper%250Aexplores%2520an%2520efficient%2520training%2520paradigm%2520to%2520build%2520a%2520single%2520transformer%2520for%250Aunified%2520multimodal%2520understanding%2520and%2520generation.%2520Specifically%252C%2520we%2520propose%2520a%250Amultimodal%2520warmup%2520strategy%2520utilizing%2520prior%2520knowledge%2520to%2520extend%2520capabilities.%2520To%250Aaddress%2520cross-modal%2520compatibility%2520challenges%252C%2520we%2520introduce%2520feature%2520pre-scaling%250Aand%2520multimodal%2520AdaLN%2520techniques.%2520Integrating%2520the%2520proposed%2520technologies%252C%2520we%250Apresent%2520the%2520HaploOmni%252C%2520a%2520new%2520single%2520multimodal%2520transformer.%2520With%2520limited%250Atraining%2520costs%252C%2520HaploOmni%2520achieves%2520competitive%2520performance%2520across%2520multiple%250Aimage%2520and%2520video%2520understanding%2520and%2520generation%2520benchmarks%2520over%2520advanced%2520unified%250Amodels.%2520All%2520codes%2520will%2520be%2520made%2520public%2520at%2520https%253A//github.com/Tencent/HaploVLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02975v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HaploOmni%3A%20Unified%20Single%20Transformer%20for%20Multimodal%20Video%20Understanding%0A%20%20and%20Generation&entry.906535625=Yicheng%20Xiao%20and%20Lin%20Song%20and%20Rui%20Yang%20and%20Cheng%20Cheng%20and%20Zunnan%20Xu%20and%20Zhaoyang%20Zhang%20and%20Yixiao%20Ge%20and%20Xiu%20Li%20and%20Ying%20Shan&entry.1292438233=%20%20With%20the%20advancement%20of%20language%20models%2C%20unified%20multimodal%20understanding%20and%0Ageneration%20have%20made%20significant%20strides%2C%20with%20model%20architectures%20evolving%0Afrom%20separated%20components%20to%20unified%20single-model%20frameworks.%20This%20paper%0Aexplores%20an%20efficient%20training%20paradigm%20to%20build%20a%20single%20transformer%20for%0Aunified%20multimodal%20understanding%20and%20generation.%20Specifically%2C%20we%20propose%20a%0Amultimodal%20warmup%20strategy%20utilizing%20prior%20knowledge%20to%20extend%20capabilities.%20To%0Aaddress%20cross-modal%20compatibility%20challenges%2C%20we%20introduce%20feature%20pre-scaling%0Aand%20multimodal%20AdaLN%20techniques.%20Integrating%20the%20proposed%20technologies%2C%20we%0Apresent%20the%20HaploOmni%2C%20a%20new%20single%20multimodal%20transformer.%20With%20limited%0Atraining%20costs%2C%20HaploOmni%20achieves%20competitive%20performance%20across%20multiple%0Aimage%20and%20video%20understanding%20and%20generation%20benchmarks%20over%20advanced%20unified%0Amodels.%20All%20codes%20will%20be%20made%20public%20at%20https%3A//github.com/Tencent/HaploVLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02975v1&entry.124074799=Read"},
{"title": "SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via\n  Visual Global Thinking", "author": "Sifan Li and Yujun Cai and Yiwei Wang", "abstract": "  Vision-language models (VLMs) excel in semantic tasks but falter at a core\nhuman capability: detecting hidden content in optical illusions or AI-generated\nimages through perceptual adjustments like zooming. We introduce HC-Bench, a\nbenchmark of 112 images with hidden text, objects, and illusions, revealing\nthat leading VLMs achieve near-zero accuracy (0-5.36%)-even with explicit\nprompting. Humans resolve such ambiguities instinctively, yet VLMs fail due to\nan overreliance on high-level semantics. Strikingly, we propose SemVink\n(Semantic Visual Thinking) by simply scaling images to low resolutions (32-128\npixels), which unlocks >99% accuracy by eliminating redundant visual noise.\nThis exposes a critical architectural flaw: VLMs prioritize abstract reasoning\nover low-level visual operations crucial for real-world robustness. Our work\nurges a shift toward hybrid models integrating multi-scale processing, bridging\nthe gap between computational vision and human cognition for applications in\nmedical imaging, security, and beyond.\n", "link": "http://arxiv.org/abs/2506.02803v1", "date": "2025-06-03", "relevancy": 2.9871, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6235}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6235}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SemVink%3A%20Advancing%20VLMs%27%20Semantic%20Understanding%20of%20Optical%20Illusions%20via%0A%20%20Visual%20Global%20Thinking&body=Title%3A%20SemVink%3A%20Advancing%20VLMs%27%20Semantic%20Understanding%20of%20Optical%20Illusions%20via%0A%20%20Visual%20Global%20Thinking%0AAuthor%3A%20Sifan%20Li%20and%20Yujun%20Cai%20and%20Yiwei%20Wang%0AAbstract%3A%20%20%20Vision-language%20models%20%28VLMs%29%20excel%20in%20semantic%20tasks%20but%20falter%20at%20a%20core%0Ahuman%20capability%3A%20detecting%20hidden%20content%20in%20optical%20illusions%20or%20AI-generated%0Aimages%20through%20perceptual%20adjustments%20like%20zooming.%20We%20introduce%20HC-Bench%2C%20a%0Abenchmark%20of%20112%20images%20with%20hidden%20text%2C%20objects%2C%20and%20illusions%2C%20revealing%0Athat%20leading%20VLMs%20achieve%20near-zero%20accuracy%20%280-5.36%25%29-even%20with%20explicit%0Aprompting.%20Humans%20resolve%20such%20ambiguities%20instinctively%2C%20yet%20VLMs%20fail%20due%20to%0Aan%20overreliance%20on%20high-level%20semantics.%20Strikingly%2C%20we%20propose%20SemVink%0A%28Semantic%20Visual%20Thinking%29%20by%20simply%20scaling%20images%20to%20low%20resolutions%20%2832-128%0Apixels%29%2C%20which%20unlocks%20%3E99%25%20accuracy%20by%20eliminating%20redundant%20visual%20noise.%0AThis%20exposes%20a%20critical%20architectural%20flaw%3A%20VLMs%20prioritize%20abstract%20reasoning%0Aover%20low-level%20visual%20operations%20crucial%20for%20real-world%20robustness.%20Our%20work%0Aurges%20a%20shift%20toward%20hybrid%20models%20integrating%20multi-scale%20processing%2C%20bridging%0Athe%20gap%20between%20computational%20vision%20and%20human%20cognition%20for%20applications%20in%0Amedical%20imaging%2C%20security%2C%20and%20beyond.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02803v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemVink%253A%2520Advancing%2520VLMs%2527%2520Semantic%2520Understanding%2520of%2520Optical%2520Illusions%2520via%250A%2520%2520Visual%2520Global%2520Thinking%26entry.906535625%3DSifan%2520Li%2520and%2520Yujun%2520Cai%2520and%2520Yiwei%2520Wang%26entry.1292438233%3D%2520%2520Vision-language%2520models%2520%2528VLMs%2529%2520excel%2520in%2520semantic%2520tasks%2520but%2520falter%2520at%2520a%2520core%250Ahuman%2520capability%253A%2520detecting%2520hidden%2520content%2520in%2520optical%2520illusions%2520or%2520AI-generated%250Aimages%2520through%2520perceptual%2520adjustments%2520like%2520zooming.%2520We%2520introduce%2520HC-Bench%252C%2520a%250Abenchmark%2520of%2520112%2520images%2520with%2520hidden%2520text%252C%2520objects%252C%2520and%2520illusions%252C%2520revealing%250Athat%2520leading%2520VLMs%2520achieve%2520near-zero%2520accuracy%2520%25280-5.36%2525%2529-even%2520with%2520explicit%250Aprompting.%2520Humans%2520resolve%2520such%2520ambiguities%2520instinctively%252C%2520yet%2520VLMs%2520fail%2520due%2520to%250Aan%2520overreliance%2520on%2520high-level%2520semantics.%2520Strikingly%252C%2520we%2520propose%2520SemVink%250A%2528Semantic%2520Visual%2520Thinking%2529%2520by%2520simply%2520scaling%2520images%2520to%2520low%2520resolutions%2520%252832-128%250Apixels%2529%252C%2520which%2520unlocks%2520%253E99%2525%2520accuracy%2520by%2520eliminating%2520redundant%2520visual%2520noise.%250AThis%2520exposes%2520a%2520critical%2520architectural%2520flaw%253A%2520VLMs%2520prioritize%2520abstract%2520reasoning%250Aover%2520low-level%2520visual%2520operations%2520crucial%2520for%2520real-world%2520robustness.%2520Our%2520work%250Aurges%2520a%2520shift%2520toward%2520hybrid%2520models%2520integrating%2520multi-scale%2520processing%252C%2520bridging%250Athe%2520gap%2520between%2520computational%2520vision%2520and%2520human%2520cognition%2520for%2520applications%2520in%250Amedical%2520imaging%252C%2520security%252C%2520and%2520beyond.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02803v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SemVink%3A%20Advancing%20VLMs%27%20Semantic%20Understanding%20of%20Optical%20Illusions%20via%0A%20%20Visual%20Global%20Thinking&entry.906535625=Sifan%20Li%20and%20Yujun%20Cai%20and%20Yiwei%20Wang&entry.1292438233=%20%20Vision-language%20models%20%28VLMs%29%20excel%20in%20semantic%20tasks%20but%20falter%20at%20a%20core%0Ahuman%20capability%3A%20detecting%20hidden%20content%20in%20optical%20illusions%20or%20AI-generated%0Aimages%20through%20perceptual%20adjustments%20like%20zooming.%20We%20introduce%20HC-Bench%2C%20a%0Abenchmark%20of%20112%20images%20with%20hidden%20text%2C%20objects%2C%20and%20illusions%2C%20revealing%0Athat%20leading%20VLMs%20achieve%20near-zero%20accuracy%20%280-5.36%25%29-even%20with%20explicit%0Aprompting.%20Humans%20resolve%20such%20ambiguities%20instinctively%2C%20yet%20VLMs%20fail%20due%20to%0Aan%20overreliance%20on%20high-level%20semantics.%20Strikingly%2C%20we%20propose%20SemVink%0A%28Semantic%20Visual%20Thinking%29%20by%20simply%20scaling%20images%20to%20low%20resolutions%20%2832-128%0Apixels%29%2C%20which%20unlocks%20%3E99%25%20accuracy%20by%20eliminating%20redundant%20visual%20noise.%0AThis%20exposes%20a%20critical%20architectural%20flaw%3A%20VLMs%20prioritize%20abstract%20reasoning%0Aover%20low-level%20visual%20operations%20crucial%20for%20real-world%20robustness.%20Our%20work%0Aurges%20a%20shift%20toward%20hybrid%20models%20integrating%20multi-scale%20processing%2C%20bridging%0Athe%20gap%20between%20computational%20vision%20and%20human%20cognition%20for%20applications%20in%0Amedical%20imaging%2C%20security%2C%20and%20beyond.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02803v1&entry.124074799=Read"},
{"title": "Likelihood-Scheduled Score-Based Generative Modeling for Fully 3D PET\n  Image Reconstruction", "author": "George Webber and Yuya Mizuno and Oliver D. Howes and Alexander Hammers and Andrew P. King and Andrew J. Reader", "abstract": "  Medical image reconstruction with pre-trained score-based generative models\n(SGMs) has advantages over other existing state-of-the-art deep-learned\nreconstruction methods, including improved resilience to different scanner\nsetups and advanced image distribution modeling. SGM-based reconstruction has\nrecently been applied to simulated positron emission tomography (PET) datasets,\nshowing improved contrast recovery for out-of-distribution lesions relative to\nthe state-of-the-art. However, existing methods for SGM-based reconstruction\nfrom PET data suffer from slow reconstruction, burdensome hyperparameter tuning\nand slice inconsistency effects (in 3D). In this work, we propose a practical\nmethodology for fully 3D reconstruction that accelerates reconstruction and\nreduces the number of critical hyperparameters by matching the likelihood of an\nSGM's reverse diffusion process to a current iterate of the maximum-likelihood\nexpectation maximization algorithm. Using the example of low-count\nreconstruction from simulated [$^{18}$F]DPA-714 datasets, we show our\nmethodology can match or improve on the NRMSE and SSIM of existing\nstate-of-the-art SGM-based PET reconstruction while reducing reconstruction\ntime and the need for hyperparameter tuning. We evaluate our methodology\nagainst state-of-the-art supervised and conventional reconstruction algorithms.\nFinally, we demonstrate a first-ever implementation of SGM-based reconstruction\nfor real 3D PET data, specifically [$^{18}$F]DPA-714 data, where we integrate\nperpendicular pre-trained SGMs to eliminate slice inconsistency issues.\n", "link": "http://arxiv.org/abs/2412.04339v2", "date": "2025-06-03", "relevancy": 2.9727, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6149}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5848}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Likelihood-Scheduled%20Score-Based%20Generative%20Modeling%20for%20Fully%203D%20PET%0A%20%20Image%20Reconstruction&body=Title%3A%20Likelihood-Scheduled%20Score-Based%20Generative%20Modeling%20for%20Fully%203D%20PET%0A%20%20Image%20Reconstruction%0AAuthor%3A%20George%20Webber%20and%20Yuya%20Mizuno%20and%20Oliver%20D.%20Howes%20and%20Alexander%20Hammers%20and%20Andrew%20P.%20King%20and%20Andrew%20J.%20Reader%0AAbstract%3A%20%20%20Medical%20image%20reconstruction%20with%20pre-trained%20score-based%20generative%20models%0A%28SGMs%29%20has%20advantages%20over%20other%20existing%20state-of-the-art%20deep-learned%0Areconstruction%20methods%2C%20including%20improved%20resilience%20to%20different%20scanner%0Asetups%20and%20advanced%20image%20distribution%20modeling.%20SGM-based%20reconstruction%20has%0Arecently%20been%20applied%20to%20simulated%20positron%20emission%20tomography%20%28PET%29%20datasets%2C%0Ashowing%20improved%20contrast%20recovery%20for%20out-of-distribution%20lesions%20relative%20to%0Athe%20state-of-the-art.%20However%2C%20existing%20methods%20for%20SGM-based%20reconstruction%0Afrom%20PET%20data%20suffer%20from%20slow%20reconstruction%2C%20burdensome%20hyperparameter%20tuning%0Aand%20slice%20inconsistency%20effects%20%28in%203D%29.%20In%20this%20work%2C%20we%20propose%20a%20practical%0Amethodology%20for%20fully%203D%20reconstruction%20that%20accelerates%20reconstruction%20and%0Areduces%20the%20number%20of%20critical%20hyperparameters%20by%20matching%20the%20likelihood%20of%20an%0ASGM%27s%20reverse%20diffusion%20process%20to%20a%20current%20iterate%20of%20the%20maximum-likelihood%0Aexpectation%20maximization%20algorithm.%20Using%20the%20example%20of%20low-count%0Areconstruction%20from%20simulated%20%5B%24%5E%7B18%7D%24F%5DDPA-714%20datasets%2C%20we%20show%20our%0Amethodology%20can%20match%20or%20improve%20on%20the%20NRMSE%20and%20SSIM%20of%20existing%0Astate-of-the-art%20SGM-based%20PET%20reconstruction%20while%20reducing%20reconstruction%0Atime%20and%20the%20need%20for%20hyperparameter%20tuning.%20We%20evaluate%20our%20methodology%0Aagainst%20state-of-the-art%20supervised%20and%20conventional%20reconstruction%20algorithms.%0AFinally%2C%20we%20demonstrate%20a%20first-ever%20implementation%20of%20SGM-based%20reconstruction%0Afor%20real%203D%20PET%20data%2C%20specifically%20%5B%24%5E%7B18%7D%24F%5DDPA-714%20data%2C%20where%20we%20integrate%0Aperpendicular%20pre-trained%20SGMs%20to%20eliminate%20slice%20inconsistency%20issues.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.04339v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLikelihood-Scheduled%2520Score-Based%2520Generative%2520Modeling%2520for%2520Fully%25203D%2520PET%250A%2520%2520Image%2520Reconstruction%26entry.906535625%3DGeorge%2520Webber%2520and%2520Yuya%2520Mizuno%2520and%2520Oliver%2520D.%2520Howes%2520and%2520Alexander%2520Hammers%2520and%2520Andrew%2520P.%2520King%2520and%2520Andrew%2520J.%2520Reader%26entry.1292438233%3D%2520%2520Medical%2520image%2520reconstruction%2520with%2520pre-trained%2520score-based%2520generative%2520models%250A%2528SGMs%2529%2520has%2520advantages%2520over%2520other%2520existing%2520state-of-the-art%2520deep-learned%250Areconstruction%2520methods%252C%2520including%2520improved%2520resilience%2520to%2520different%2520scanner%250Asetups%2520and%2520advanced%2520image%2520distribution%2520modeling.%2520SGM-based%2520reconstruction%2520has%250Arecently%2520been%2520applied%2520to%2520simulated%2520positron%2520emission%2520tomography%2520%2528PET%2529%2520datasets%252C%250Ashowing%2520improved%2520contrast%2520recovery%2520for%2520out-of-distribution%2520lesions%2520relative%2520to%250Athe%2520state-of-the-art.%2520However%252C%2520existing%2520methods%2520for%2520SGM-based%2520reconstruction%250Afrom%2520PET%2520data%2520suffer%2520from%2520slow%2520reconstruction%252C%2520burdensome%2520hyperparameter%2520tuning%250Aand%2520slice%2520inconsistency%2520effects%2520%2528in%25203D%2529.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520practical%250Amethodology%2520for%2520fully%25203D%2520reconstruction%2520that%2520accelerates%2520reconstruction%2520and%250Areduces%2520the%2520number%2520of%2520critical%2520hyperparameters%2520by%2520matching%2520the%2520likelihood%2520of%2520an%250ASGM%2527s%2520reverse%2520diffusion%2520process%2520to%2520a%2520current%2520iterate%2520of%2520the%2520maximum-likelihood%250Aexpectation%2520maximization%2520algorithm.%2520Using%2520the%2520example%2520of%2520low-count%250Areconstruction%2520from%2520simulated%2520%255B%2524%255E%257B18%257D%2524F%255DDPA-714%2520datasets%252C%2520we%2520show%2520our%250Amethodology%2520can%2520match%2520or%2520improve%2520on%2520the%2520NRMSE%2520and%2520SSIM%2520of%2520existing%250Astate-of-the-art%2520SGM-based%2520PET%2520reconstruction%2520while%2520reducing%2520reconstruction%250Atime%2520and%2520the%2520need%2520for%2520hyperparameter%2520tuning.%2520We%2520evaluate%2520our%2520methodology%250Aagainst%2520state-of-the-art%2520supervised%2520and%2520conventional%2520reconstruction%2520algorithms.%250AFinally%252C%2520we%2520demonstrate%2520a%2520first-ever%2520implementation%2520of%2520SGM-based%2520reconstruction%250Afor%2520real%25203D%2520PET%2520data%252C%2520specifically%2520%255B%2524%255E%257B18%257D%2524F%255DDPA-714%2520data%252C%2520where%2520we%2520integrate%250Aperpendicular%2520pre-trained%2520SGMs%2520to%2520eliminate%2520slice%2520inconsistency%2520issues.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.04339v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Likelihood-Scheduled%20Score-Based%20Generative%20Modeling%20for%20Fully%203D%20PET%0A%20%20Image%20Reconstruction&entry.906535625=George%20Webber%20and%20Yuya%20Mizuno%20and%20Oliver%20D.%20Howes%20and%20Alexander%20Hammers%20and%20Andrew%20P.%20King%20and%20Andrew%20J.%20Reader&entry.1292438233=%20%20Medical%20image%20reconstruction%20with%20pre-trained%20score-based%20generative%20models%0A%28SGMs%29%20has%20advantages%20over%20other%20existing%20state-of-the-art%20deep-learned%0Areconstruction%20methods%2C%20including%20improved%20resilience%20to%20different%20scanner%0Asetups%20and%20advanced%20image%20distribution%20modeling.%20SGM-based%20reconstruction%20has%0Arecently%20been%20applied%20to%20simulated%20positron%20emission%20tomography%20%28PET%29%20datasets%2C%0Ashowing%20improved%20contrast%20recovery%20for%20out-of-distribution%20lesions%20relative%20to%0Athe%20state-of-the-art.%20However%2C%20existing%20methods%20for%20SGM-based%20reconstruction%0Afrom%20PET%20data%20suffer%20from%20slow%20reconstruction%2C%20burdensome%20hyperparameter%20tuning%0Aand%20slice%20inconsistency%20effects%20%28in%203D%29.%20In%20this%20work%2C%20we%20propose%20a%20practical%0Amethodology%20for%20fully%203D%20reconstruction%20that%20accelerates%20reconstruction%20and%0Areduces%20the%20number%20of%20critical%20hyperparameters%20by%20matching%20the%20likelihood%20of%20an%0ASGM%27s%20reverse%20diffusion%20process%20to%20a%20current%20iterate%20of%20the%20maximum-likelihood%0Aexpectation%20maximization%20algorithm.%20Using%20the%20example%20of%20low-count%0Areconstruction%20from%20simulated%20%5B%24%5E%7B18%7D%24F%5DDPA-714%20datasets%2C%20we%20show%20our%0Amethodology%20can%20match%20or%20improve%20on%20the%20NRMSE%20and%20SSIM%20of%20existing%0Astate-of-the-art%20SGM-based%20PET%20reconstruction%20while%20reducing%20reconstruction%0Atime%20and%20the%20need%20for%20hyperparameter%20tuning.%20We%20evaluate%20our%20methodology%0Aagainst%20state-of-the-art%20supervised%20and%20conventional%20reconstruction%20algorithms.%0AFinally%2C%20we%20demonstrate%20a%20first-ever%20implementation%20of%20SGM-based%20reconstruction%0Afor%20real%203D%20PET%20data%2C%20specifically%20%5B%24%5E%7B18%7D%24F%5DDPA-714%20data%2C%20where%20we%20integrate%0Aperpendicular%20pre-trained%20SGMs%20to%20eliminate%20slice%20inconsistency%20issues.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.04339v2&entry.124074799=Read"},
{"title": "Self-Supervised Spatial Correspondence Across Modalities", "author": "Ayush Shrivastava and Andrew Owens", "abstract": "  We present a method for finding cross-modal space-time correspondences. Given\ntwo images from different visual modalities, such as an RGB image and a depth\nmap, our model identifies which pairs of pixels correspond to the same physical\npoints in the scene. To solve this problem, we extend the contrastive random\nwalk framework to simultaneously learn cycle-consistent feature representations\nfor both cross-modal and intra-modal matching. The resulting model is simple\nand has no explicit photo-consistency assumptions. It can be trained entirely\nusing unlabeled data, without the need for any spatially aligned multimodal\nimage pairs. We evaluate our method on both geometric and semantic\ncorrespondence tasks. For geometric matching, we consider challenging tasks\nsuch as RGB-to-depth and RGB-to-thermal matching (and vice versa); for semantic\nmatching, we evaluate on photo-sketch and cross-style image alignment. Our\nmethod achieves strong performance across all benchmarks.\n", "link": "http://arxiv.org/abs/2506.03148v1", "date": "2025-06-03", "relevancy": 2.9594, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.595}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5911}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Spatial%20Correspondence%20Across%20Modalities&body=Title%3A%20Self-Supervised%20Spatial%20Correspondence%20Across%20Modalities%0AAuthor%3A%20Ayush%20Shrivastava%20and%20Andrew%20Owens%0AAbstract%3A%20%20%20We%20present%20a%20method%20for%20finding%20cross-modal%20space-time%20correspondences.%20Given%0Atwo%20images%20from%20different%20visual%20modalities%2C%20such%20as%20an%20RGB%20image%20and%20a%20depth%0Amap%2C%20our%20model%20identifies%20which%20pairs%20of%20pixels%20correspond%20to%20the%20same%20physical%0Apoints%20in%20the%20scene.%20To%20solve%20this%20problem%2C%20we%20extend%20the%20contrastive%20random%0Awalk%20framework%20to%20simultaneously%20learn%20cycle-consistent%20feature%20representations%0Afor%20both%20cross-modal%20and%20intra-modal%20matching.%20The%20resulting%20model%20is%20simple%0Aand%20has%20no%20explicit%20photo-consistency%20assumptions.%20It%20can%20be%20trained%20entirely%0Ausing%20unlabeled%20data%2C%20without%20the%20need%20for%20any%20spatially%20aligned%20multimodal%0Aimage%20pairs.%20We%20evaluate%20our%20method%20on%20both%20geometric%20and%20semantic%0Acorrespondence%20tasks.%20For%20geometric%20matching%2C%20we%20consider%20challenging%20tasks%0Asuch%20as%20RGB-to-depth%20and%20RGB-to-thermal%20matching%20%28and%20vice%20versa%29%3B%20for%20semantic%0Amatching%2C%20we%20evaluate%20on%20photo-sketch%20and%20cross-style%20image%20alignment.%20Our%0Amethod%20achieves%20strong%20performance%20across%20all%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03148v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Supervised%2520Spatial%2520Correspondence%2520Across%2520Modalities%26entry.906535625%3DAyush%2520Shrivastava%2520and%2520Andrew%2520Owens%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520method%2520for%2520finding%2520cross-modal%2520space-time%2520correspondences.%2520Given%250Atwo%2520images%2520from%2520different%2520visual%2520modalities%252C%2520such%2520as%2520an%2520RGB%2520image%2520and%2520a%2520depth%250Amap%252C%2520our%2520model%2520identifies%2520which%2520pairs%2520of%2520pixels%2520correspond%2520to%2520the%2520same%2520physical%250Apoints%2520in%2520the%2520scene.%2520To%2520solve%2520this%2520problem%252C%2520we%2520extend%2520the%2520contrastive%2520random%250Awalk%2520framework%2520to%2520simultaneously%2520learn%2520cycle-consistent%2520feature%2520representations%250Afor%2520both%2520cross-modal%2520and%2520intra-modal%2520matching.%2520The%2520resulting%2520model%2520is%2520simple%250Aand%2520has%2520no%2520explicit%2520photo-consistency%2520assumptions.%2520It%2520can%2520be%2520trained%2520entirely%250Ausing%2520unlabeled%2520data%252C%2520without%2520the%2520need%2520for%2520any%2520spatially%2520aligned%2520multimodal%250Aimage%2520pairs.%2520We%2520evaluate%2520our%2520method%2520on%2520both%2520geometric%2520and%2520semantic%250Acorrespondence%2520tasks.%2520For%2520geometric%2520matching%252C%2520we%2520consider%2520challenging%2520tasks%250Asuch%2520as%2520RGB-to-depth%2520and%2520RGB-to-thermal%2520matching%2520%2528and%2520vice%2520versa%2529%253B%2520for%2520semantic%250Amatching%252C%2520we%2520evaluate%2520on%2520photo-sketch%2520and%2520cross-style%2520image%2520alignment.%2520Our%250Amethod%2520achieves%2520strong%2520performance%2520across%2520all%2520benchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03148v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Spatial%20Correspondence%20Across%20Modalities&entry.906535625=Ayush%20Shrivastava%20and%20Andrew%20Owens&entry.1292438233=%20%20We%20present%20a%20method%20for%20finding%20cross-modal%20space-time%20correspondences.%20Given%0Atwo%20images%20from%20different%20visual%20modalities%2C%20such%20as%20an%20RGB%20image%20and%20a%20depth%0Amap%2C%20our%20model%20identifies%20which%20pairs%20of%20pixels%20correspond%20to%20the%20same%20physical%0Apoints%20in%20the%20scene.%20To%20solve%20this%20problem%2C%20we%20extend%20the%20contrastive%20random%0Awalk%20framework%20to%20simultaneously%20learn%20cycle-consistent%20feature%20representations%0Afor%20both%20cross-modal%20and%20intra-modal%20matching.%20The%20resulting%20model%20is%20simple%0Aand%20has%20no%20explicit%20photo-consistency%20assumptions.%20It%20can%20be%20trained%20entirely%0Ausing%20unlabeled%20data%2C%20without%20the%20need%20for%20any%20spatially%20aligned%20multimodal%0Aimage%20pairs.%20We%20evaluate%20our%20method%20on%20both%20geometric%20and%20semantic%0Acorrespondence%20tasks.%20For%20geometric%20matching%2C%20we%20consider%20challenging%20tasks%0Asuch%20as%20RGB-to-depth%20and%20RGB-to-thermal%20matching%20%28and%20vice%20versa%29%3B%20for%20semantic%0Amatching%2C%20we%20evaluate%20on%20photo-sketch%20and%20cross-style%20image%20alignment.%20Our%0Amethod%20achieves%20strong%20performance%20across%20all%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03148v1&entry.124074799=Read"},
{"title": "DyTact: Capturing Dynamic Contacts in Hand-Object Manipulation", "author": "Xiaoyan Cong and Angela Xing and Chandradeep Pokhariya and Rao Fu and Srinath Sridhar", "abstract": "  Reconstructing dynamic hand-object contacts is essential for realistic\nmanipulation in AI character animation, XR, and robotics, yet it remains\nchallenging due to heavy occlusions, complex surface details, and limitations\nin existing capture techniques. In this paper, we introduce DyTact, a\nmarkerless capture method for accurately capturing dynamic contact in\nhand-object manipulations in a non-intrusive manner. Our approach leverages a\ndynamic, articulated representation based on 2D Gaussian surfels to model\ncomplex manipulations. By binding these surfels to MANO meshes, DyTact\nharnesses the inductive bias of template models to stabilize and accelerate\noptimization. A refinement module addresses time-dependent high-frequency\ndeformations, while a contact-guided adaptive sampling strategy selectively\nincreases surfel density in contact regions to handle heavy occlusion.\nExtensive experiments demonstrate that DyTact not only achieves\nstate-of-the-art dynamic contact estimation accuracy but also significantly\nimproves novel view synthesis quality, all while operating with fast\noptimization and efficient memory usage. Project Page:\nhttps://oliver-cong02.github.io/DyTact.github.io/ .\n", "link": "http://arxiv.org/abs/2506.03103v1", "date": "2025-06-03", "relevancy": 2.923, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6171}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.58}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DyTact%3A%20Capturing%20Dynamic%20Contacts%20in%20Hand-Object%20Manipulation&body=Title%3A%20DyTact%3A%20Capturing%20Dynamic%20Contacts%20in%20Hand-Object%20Manipulation%0AAuthor%3A%20Xiaoyan%20Cong%20and%20Angela%20Xing%20and%20Chandradeep%20Pokhariya%20and%20Rao%20Fu%20and%20Srinath%20Sridhar%0AAbstract%3A%20%20%20Reconstructing%20dynamic%20hand-object%20contacts%20is%20essential%20for%20realistic%0Amanipulation%20in%20AI%20character%20animation%2C%20XR%2C%20and%20robotics%2C%20yet%20it%20remains%0Achallenging%20due%20to%20heavy%20occlusions%2C%20complex%20surface%20details%2C%20and%20limitations%0Ain%20existing%20capture%20techniques.%20In%20this%20paper%2C%20we%20introduce%20DyTact%2C%20a%0Amarkerless%20capture%20method%20for%20accurately%20capturing%20dynamic%20contact%20in%0Ahand-object%20manipulations%20in%20a%20non-intrusive%20manner.%20Our%20approach%20leverages%20a%0Adynamic%2C%20articulated%20representation%20based%20on%202D%20Gaussian%20surfels%20to%20model%0Acomplex%20manipulations.%20By%20binding%20these%20surfels%20to%20MANO%20meshes%2C%20DyTact%0Aharnesses%20the%20inductive%20bias%20of%20template%20models%20to%20stabilize%20and%20accelerate%0Aoptimization.%20A%20refinement%20module%20addresses%20time-dependent%20high-frequency%0Adeformations%2C%20while%20a%20contact-guided%20adaptive%20sampling%20strategy%20selectively%0Aincreases%20surfel%20density%20in%20contact%20regions%20to%20handle%20heavy%20occlusion.%0AExtensive%20experiments%20demonstrate%20that%20DyTact%20not%20only%20achieves%0Astate-of-the-art%20dynamic%20contact%20estimation%20accuracy%20but%20also%20significantly%0Aimproves%20novel%20view%20synthesis%20quality%2C%20all%20while%20operating%20with%20fast%0Aoptimization%20and%20efficient%20memory%20usage.%20Project%20Page%3A%0Ahttps%3A//oliver-cong02.github.io/DyTact.github.io/%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03103v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDyTact%253A%2520Capturing%2520Dynamic%2520Contacts%2520in%2520Hand-Object%2520Manipulation%26entry.906535625%3DXiaoyan%2520Cong%2520and%2520Angela%2520Xing%2520and%2520Chandradeep%2520Pokhariya%2520and%2520Rao%2520Fu%2520and%2520Srinath%2520Sridhar%26entry.1292438233%3D%2520%2520Reconstructing%2520dynamic%2520hand-object%2520contacts%2520is%2520essential%2520for%2520realistic%250Amanipulation%2520in%2520AI%2520character%2520animation%252C%2520XR%252C%2520and%2520robotics%252C%2520yet%2520it%2520remains%250Achallenging%2520due%2520to%2520heavy%2520occlusions%252C%2520complex%2520surface%2520details%252C%2520and%2520limitations%250Ain%2520existing%2520capture%2520techniques.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520DyTact%252C%2520a%250Amarkerless%2520capture%2520method%2520for%2520accurately%2520capturing%2520dynamic%2520contact%2520in%250Ahand-object%2520manipulations%2520in%2520a%2520non-intrusive%2520manner.%2520Our%2520approach%2520leverages%2520a%250Adynamic%252C%2520articulated%2520representation%2520based%2520on%25202D%2520Gaussian%2520surfels%2520to%2520model%250Acomplex%2520manipulations.%2520By%2520binding%2520these%2520surfels%2520to%2520MANO%2520meshes%252C%2520DyTact%250Aharnesses%2520the%2520inductive%2520bias%2520of%2520template%2520models%2520to%2520stabilize%2520and%2520accelerate%250Aoptimization.%2520A%2520refinement%2520module%2520addresses%2520time-dependent%2520high-frequency%250Adeformations%252C%2520while%2520a%2520contact-guided%2520adaptive%2520sampling%2520strategy%2520selectively%250Aincreases%2520surfel%2520density%2520in%2520contact%2520regions%2520to%2520handle%2520heavy%2520occlusion.%250AExtensive%2520experiments%2520demonstrate%2520that%2520DyTact%2520not%2520only%2520achieves%250Astate-of-the-art%2520dynamic%2520contact%2520estimation%2520accuracy%2520but%2520also%2520significantly%250Aimproves%2520novel%2520view%2520synthesis%2520quality%252C%2520all%2520while%2520operating%2520with%2520fast%250Aoptimization%2520and%2520efficient%2520memory%2520usage.%2520Project%2520Page%253A%250Ahttps%253A//oliver-cong02.github.io/DyTact.github.io/%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03103v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DyTact%3A%20Capturing%20Dynamic%20Contacts%20in%20Hand-Object%20Manipulation&entry.906535625=Xiaoyan%20Cong%20and%20Angela%20Xing%20and%20Chandradeep%20Pokhariya%20and%20Rao%20Fu%20and%20Srinath%20Sridhar&entry.1292438233=%20%20Reconstructing%20dynamic%20hand-object%20contacts%20is%20essential%20for%20realistic%0Amanipulation%20in%20AI%20character%20animation%2C%20XR%2C%20and%20robotics%2C%20yet%20it%20remains%0Achallenging%20due%20to%20heavy%20occlusions%2C%20complex%20surface%20details%2C%20and%20limitations%0Ain%20existing%20capture%20techniques.%20In%20this%20paper%2C%20we%20introduce%20DyTact%2C%20a%0Amarkerless%20capture%20method%20for%20accurately%20capturing%20dynamic%20contact%20in%0Ahand-object%20manipulations%20in%20a%20non-intrusive%20manner.%20Our%20approach%20leverages%20a%0Adynamic%2C%20articulated%20representation%20based%20on%202D%20Gaussian%20surfels%20to%20model%0Acomplex%20manipulations.%20By%20binding%20these%20surfels%20to%20MANO%20meshes%2C%20DyTact%0Aharnesses%20the%20inductive%20bias%20of%20template%20models%20to%20stabilize%20and%20accelerate%0Aoptimization.%20A%20refinement%20module%20addresses%20time-dependent%20high-frequency%0Adeformations%2C%20while%20a%20contact-guided%20adaptive%20sampling%20strategy%20selectively%0Aincreases%20surfel%20density%20in%20contact%20regions%20to%20handle%20heavy%20occlusion.%0AExtensive%20experiments%20demonstrate%20that%20DyTact%20not%20only%20achieves%0Astate-of-the-art%20dynamic%20contact%20estimation%20accuracy%20but%20also%20significantly%0Aimproves%20novel%20view%20synthesis%20quality%2C%20all%20while%20operating%20with%20fast%0Aoptimization%20and%20efficient%20memory%20usage.%20Project%20Page%3A%0Ahttps%3A//oliver-cong02.github.io/DyTact.github.io/%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03103v1&entry.124074799=Read"},
{"title": "S4-Driver: Scalable Self-Supervised Driving Multimodal Large Language\n  Modelwith Spatio-Temporal Visual Representation", "author": "Yichen Xie and Runsheng Xu and Tong He and Jyh-Jing Hwang and Katie Luo and Jingwei Ji and Hubert Lin and Letian Chen and Yiren Lu and Zhaoqi Leng and Dragomir Anguelov and Mingxing Tan", "abstract": "  The latest advancements in multi-modal large language models (MLLMs) have\nspurred a strong renewed interest in end-to-end motion planning approaches for\nautonomous driving. Many end-to-end approaches rely on human annotations to\nlearn intermediate perception and prediction tasks, while purely\nself-supervised approaches--which directly learn from sensor inputs to generate\nplanning trajectories without human annotations often underperform the state of\nthe art. We observe a key gap in the input representation space: end-to-end\napproaches built on MLLMs are often pretrained with reasoning tasks in 2D image\nspace rather than the native 3D space in which autonomous vehicles plan. To\nthis end, we propose S4-Driver, a scalable self-supervised motion planning\nalgorithm with spatio-temporal visual representation, based on the popular PaLI\nmultimodal large language model. S4-Driver uses a novel sparse volume strategy\nto seamlessly transform the strong visual representation of MLLMs from\nperspective view to 3D space without the need to finetune the vision encoder.\nThis representation aggregates multi-view and multi-frame visual inputs and\nenables better prediction of planning trajectories in 3D space. To validate our\nmethod, we run experiments on both nuScenes and Waymo Open Motion Dataset (with\nin-house camera data). Results show that S4-Driver performs favorably against\nexisting supervised multi-task approaches while requiring no human annotations.\nIt also demonstrates great scalability when pretrained on large volumes of\nunannotated driving logs.\n", "link": "http://arxiv.org/abs/2505.24139v2", "date": "2025-06-03", "relevancy": 2.9113, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5842}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5813}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5813}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20S4-Driver%3A%20Scalable%20Self-Supervised%20Driving%20Multimodal%20Large%20Language%0A%20%20Modelwith%20Spatio-Temporal%20Visual%20Representation&body=Title%3A%20S4-Driver%3A%20Scalable%20Self-Supervised%20Driving%20Multimodal%20Large%20Language%0A%20%20Modelwith%20Spatio-Temporal%20Visual%20Representation%0AAuthor%3A%20Yichen%20Xie%20and%20Runsheng%20Xu%20and%20Tong%20He%20and%20Jyh-Jing%20Hwang%20and%20Katie%20Luo%20and%20Jingwei%20Ji%20and%20Hubert%20Lin%20and%20Letian%20Chen%20and%20Yiren%20Lu%20and%20Zhaoqi%20Leng%20and%20Dragomir%20Anguelov%20and%20Mingxing%20Tan%0AAbstract%3A%20%20%20The%20latest%20advancements%20in%20multi-modal%20large%20language%20models%20%28MLLMs%29%20have%0Aspurred%20a%20strong%20renewed%20interest%20in%20end-to-end%20motion%20planning%20approaches%20for%0Aautonomous%20driving.%20Many%20end-to-end%20approaches%20rely%20on%20human%20annotations%20to%0Alearn%20intermediate%20perception%20and%20prediction%20tasks%2C%20while%20purely%0Aself-supervised%20approaches--which%20directly%20learn%20from%20sensor%20inputs%20to%20generate%0Aplanning%20trajectories%20without%20human%20annotations%20often%20underperform%20the%20state%20of%0Athe%20art.%20We%20observe%20a%20key%20gap%20in%20the%20input%20representation%20space%3A%20end-to-end%0Aapproaches%20built%20on%20MLLMs%20are%20often%20pretrained%20with%20reasoning%20tasks%20in%202D%20image%0Aspace%20rather%20than%20the%20native%203D%20space%20in%20which%20autonomous%20vehicles%20plan.%20To%0Athis%20end%2C%20we%20propose%20S4-Driver%2C%20a%20scalable%20self-supervised%20motion%20planning%0Aalgorithm%20with%20spatio-temporal%20visual%20representation%2C%20based%20on%20the%20popular%20PaLI%0Amultimodal%20large%20language%20model.%20S4-Driver%20uses%20a%20novel%20sparse%20volume%20strategy%0Ato%20seamlessly%20transform%20the%20strong%20visual%20representation%20of%20MLLMs%20from%0Aperspective%20view%20to%203D%20space%20without%20the%20need%20to%20finetune%20the%20vision%20encoder.%0AThis%20representation%20aggregates%20multi-view%20and%20multi-frame%20visual%20inputs%20and%0Aenables%20better%20prediction%20of%20planning%20trajectories%20in%203D%20space.%20To%20validate%20our%0Amethod%2C%20we%20run%20experiments%20on%20both%20nuScenes%20and%20Waymo%20Open%20Motion%20Dataset%20%28with%0Ain-house%20camera%20data%29.%20Results%20show%20that%20S4-Driver%20performs%20favorably%20against%0Aexisting%20supervised%20multi-task%20approaches%20while%20requiring%20no%20human%20annotations.%0AIt%20also%20demonstrates%20great%20scalability%20when%20pretrained%20on%20large%20volumes%20of%0Aunannotated%20driving%20logs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24139v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DS4-Driver%253A%2520Scalable%2520Self-Supervised%2520Driving%2520Multimodal%2520Large%2520Language%250A%2520%2520Modelwith%2520Spatio-Temporal%2520Visual%2520Representation%26entry.906535625%3DYichen%2520Xie%2520and%2520Runsheng%2520Xu%2520and%2520Tong%2520He%2520and%2520Jyh-Jing%2520Hwang%2520and%2520Katie%2520Luo%2520and%2520Jingwei%2520Ji%2520and%2520Hubert%2520Lin%2520and%2520Letian%2520Chen%2520and%2520Yiren%2520Lu%2520and%2520Zhaoqi%2520Leng%2520and%2520Dragomir%2520Anguelov%2520and%2520Mingxing%2520Tan%26entry.1292438233%3D%2520%2520The%2520latest%2520advancements%2520in%2520multi-modal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520have%250Aspurred%2520a%2520strong%2520renewed%2520interest%2520in%2520end-to-end%2520motion%2520planning%2520approaches%2520for%250Aautonomous%2520driving.%2520Many%2520end-to-end%2520approaches%2520rely%2520on%2520human%2520annotations%2520to%250Alearn%2520intermediate%2520perception%2520and%2520prediction%2520tasks%252C%2520while%2520purely%250Aself-supervised%2520approaches--which%2520directly%2520learn%2520from%2520sensor%2520inputs%2520to%2520generate%250Aplanning%2520trajectories%2520without%2520human%2520annotations%2520often%2520underperform%2520the%2520state%2520of%250Athe%2520art.%2520We%2520observe%2520a%2520key%2520gap%2520in%2520the%2520input%2520representation%2520space%253A%2520end-to-end%250Aapproaches%2520built%2520on%2520MLLMs%2520are%2520often%2520pretrained%2520with%2520reasoning%2520tasks%2520in%25202D%2520image%250Aspace%2520rather%2520than%2520the%2520native%25203D%2520space%2520in%2520which%2520autonomous%2520vehicles%2520plan.%2520To%250Athis%2520end%252C%2520we%2520propose%2520S4-Driver%252C%2520a%2520scalable%2520self-supervised%2520motion%2520planning%250Aalgorithm%2520with%2520spatio-temporal%2520visual%2520representation%252C%2520based%2520on%2520the%2520popular%2520PaLI%250Amultimodal%2520large%2520language%2520model.%2520S4-Driver%2520uses%2520a%2520novel%2520sparse%2520volume%2520strategy%250Ato%2520seamlessly%2520transform%2520the%2520strong%2520visual%2520representation%2520of%2520MLLMs%2520from%250Aperspective%2520view%2520to%25203D%2520space%2520without%2520the%2520need%2520to%2520finetune%2520the%2520vision%2520encoder.%250AThis%2520representation%2520aggregates%2520multi-view%2520and%2520multi-frame%2520visual%2520inputs%2520and%250Aenables%2520better%2520prediction%2520of%2520planning%2520trajectories%2520in%25203D%2520space.%2520To%2520validate%2520our%250Amethod%252C%2520we%2520run%2520experiments%2520on%2520both%2520nuScenes%2520and%2520Waymo%2520Open%2520Motion%2520Dataset%2520%2528with%250Ain-house%2520camera%2520data%2529.%2520Results%2520show%2520that%2520S4-Driver%2520performs%2520favorably%2520against%250Aexisting%2520supervised%2520multi-task%2520approaches%2520while%2520requiring%2520no%2520human%2520annotations.%250AIt%2520also%2520demonstrates%2520great%2520scalability%2520when%2520pretrained%2520on%2520large%2520volumes%2520of%250Aunannotated%2520driving%2520logs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24139v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=S4-Driver%3A%20Scalable%20Self-Supervised%20Driving%20Multimodal%20Large%20Language%0A%20%20Modelwith%20Spatio-Temporal%20Visual%20Representation&entry.906535625=Yichen%20Xie%20and%20Runsheng%20Xu%20and%20Tong%20He%20and%20Jyh-Jing%20Hwang%20and%20Katie%20Luo%20and%20Jingwei%20Ji%20and%20Hubert%20Lin%20and%20Letian%20Chen%20and%20Yiren%20Lu%20and%20Zhaoqi%20Leng%20and%20Dragomir%20Anguelov%20and%20Mingxing%20Tan&entry.1292438233=%20%20The%20latest%20advancements%20in%20multi-modal%20large%20language%20models%20%28MLLMs%29%20have%0Aspurred%20a%20strong%20renewed%20interest%20in%20end-to-end%20motion%20planning%20approaches%20for%0Aautonomous%20driving.%20Many%20end-to-end%20approaches%20rely%20on%20human%20annotations%20to%0Alearn%20intermediate%20perception%20and%20prediction%20tasks%2C%20while%20purely%0Aself-supervised%20approaches--which%20directly%20learn%20from%20sensor%20inputs%20to%20generate%0Aplanning%20trajectories%20without%20human%20annotations%20often%20underperform%20the%20state%20of%0Athe%20art.%20We%20observe%20a%20key%20gap%20in%20the%20input%20representation%20space%3A%20end-to-end%0Aapproaches%20built%20on%20MLLMs%20are%20often%20pretrained%20with%20reasoning%20tasks%20in%202D%20image%0Aspace%20rather%20than%20the%20native%203D%20space%20in%20which%20autonomous%20vehicles%20plan.%20To%0Athis%20end%2C%20we%20propose%20S4-Driver%2C%20a%20scalable%20self-supervised%20motion%20planning%0Aalgorithm%20with%20spatio-temporal%20visual%20representation%2C%20based%20on%20the%20popular%20PaLI%0Amultimodal%20large%20language%20model.%20S4-Driver%20uses%20a%20novel%20sparse%20volume%20strategy%0Ato%20seamlessly%20transform%20the%20strong%20visual%20representation%20of%20MLLMs%20from%0Aperspective%20view%20to%203D%20space%20without%20the%20need%20to%20finetune%20the%20vision%20encoder.%0AThis%20representation%20aggregates%20multi-view%20and%20multi-frame%20visual%20inputs%20and%0Aenables%20better%20prediction%20of%20planning%20trajectories%20in%203D%20space.%20To%20validate%20our%0Amethod%2C%20we%20run%20experiments%20on%20both%20nuScenes%20and%20Waymo%20Open%20Motion%20Dataset%20%28with%0Ain-house%20camera%20data%29.%20Results%20show%20that%20S4-Driver%20performs%20favorably%20against%0Aexisting%20supervised%20multi-task%20approaches%20while%20requiring%20no%20human%20annotations.%0AIt%20also%20demonstrates%20great%20scalability%20when%20pretrained%20on%20large%20volumes%20of%0Aunannotated%20driving%20logs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24139v2&entry.124074799=Read"},
{"title": "Iterative Self-Improvement of Vision Language Models for Image Scoring\n  and Self-Explanation", "author": "Naoto Tanji and Toshihiko Yamasaki", "abstract": "  Image scoring is a crucial task in numerous real-world applications. To trust\na model's judgment, understanding its rationale is essential. This paper\nproposes a novel training method for Vision Language Models (VLMs) to generate\nnot only image scores but also corresponding justifications in natural\nlanguage. Leveraging only an image scoring dataset and an instruction-tuned\nVLM, our method enables self-training, utilizing the VLM's generated text\nwithout relying on external data or models. In addition, we introduce a simple\nmethod for creating a dataset designed to improve alignment between predicted\nscores and their textual justifications. By iteratively training the model with\nDirect Preference Optimization on two distinct datasets and merging them, we\ncan improve both scoring accuracy and the coherence of generated explanations.\n", "link": "http://arxiv.org/abs/2506.02708v1", "date": "2025-06-03", "relevancy": 2.9033, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5963}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5963}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5493}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Iterative%20Self-Improvement%20of%20Vision%20Language%20Models%20for%20Image%20Scoring%0A%20%20and%20Self-Explanation&body=Title%3A%20Iterative%20Self-Improvement%20of%20Vision%20Language%20Models%20for%20Image%20Scoring%0A%20%20and%20Self-Explanation%0AAuthor%3A%20Naoto%20Tanji%20and%20Toshihiko%20Yamasaki%0AAbstract%3A%20%20%20Image%20scoring%20is%20a%20crucial%20task%20in%20numerous%20real-world%20applications.%20To%20trust%0Aa%20model%27s%20judgment%2C%20understanding%20its%20rationale%20is%20essential.%20This%20paper%0Aproposes%20a%20novel%20training%20method%20for%20Vision%20Language%20Models%20%28VLMs%29%20to%20generate%0Anot%20only%20image%20scores%20but%20also%20corresponding%20justifications%20in%20natural%0Alanguage.%20Leveraging%20only%20an%20image%20scoring%20dataset%20and%20an%20instruction-tuned%0AVLM%2C%20our%20method%20enables%20self-training%2C%20utilizing%20the%20VLM%27s%20generated%20text%0Awithout%20relying%20on%20external%20data%20or%20models.%20In%20addition%2C%20we%20introduce%20a%20simple%0Amethod%20for%20creating%20a%20dataset%20designed%20to%20improve%20alignment%20between%20predicted%0Ascores%20and%20their%20textual%20justifications.%20By%20iteratively%20training%20the%20model%20with%0ADirect%20Preference%20Optimization%20on%20two%20distinct%20datasets%20and%20merging%20them%2C%20we%0Acan%20improve%20both%20scoring%20accuracy%20and%20the%20coherence%20of%20generated%20explanations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02708v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIterative%2520Self-Improvement%2520of%2520Vision%2520Language%2520Models%2520for%2520Image%2520Scoring%250A%2520%2520and%2520Self-Explanation%26entry.906535625%3DNaoto%2520Tanji%2520and%2520Toshihiko%2520Yamasaki%26entry.1292438233%3D%2520%2520Image%2520scoring%2520is%2520a%2520crucial%2520task%2520in%2520numerous%2520real-world%2520applications.%2520To%2520trust%250Aa%2520model%2527s%2520judgment%252C%2520understanding%2520its%2520rationale%2520is%2520essential.%2520This%2520paper%250Aproposes%2520a%2520novel%2520training%2520method%2520for%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520to%2520generate%250Anot%2520only%2520image%2520scores%2520but%2520also%2520corresponding%2520justifications%2520in%2520natural%250Alanguage.%2520Leveraging%2520only%2520an%2520image%2520scoring%2520dataset%2520and%2520an%2520instruction-tuned%250AVLM%252C%2520our%2520method%2520enables%2520self-training%252C%2520utilizing%2520the%2520VLM%2527s%2520generated%2520text%250Awithout%2520relying%2520on%2520external%2520data%2520or%2520models.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520simple%250Amethod%2520for%2520creating%2520a%2520dataset%2520designed%2520to%2520improve%2520alignment%2520between%2520predicted%250Ascores%2520and%2520their%2520textual%2520justifications.%2520By%2520iteratively%2520training%2520the%2520model%2520with%250ADirect%2520Preference%2520Optimization%2520on%2520two%2520distinct%2520datasets%2520and%2520merging%2520them%252C%2520we%250Acan%2520improve%2520both%2520scoring%2520accuracy%2520and%2520the%2520coherence%2520of%2520generated%2520explanations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02708v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Iterative%20Self-Improvement%20of%20Vision%20Language%20Models%20for%20Image%20Scoring%0A%20%20and%20Self-Explanation&entry.906535625=Naoto%20Tanji%20and%20Toshihiko%20Yamasaki&entry.1292438233=%20%20Image%20scoring%20is%20a%20crucial%20task%20in%20numerous%20real-world%20applications.%20To%20trust%0Aa%20model%27s%20judgment%2C%20understanding%20its%20rationale%20is%20essential.%20This%20paper%0Aproposes%20a%20novel%20training%20method%20for%20Vision%20Language%20Models%20%28VLMs%29%20to%20generate%0Anot%20only%20image%20scores%20but%20also%20corresponding%20justifications%20in%20natural%0Alanguage.%20Leveraging%20only%20an%20image%20scoring%20dataset%20and%20an%20instruction-tuned%0AVLM%2C%20our%20method%20enables%20self-training%2C%20utilizing%20the%20VLM%27s%20generated%20text%0Awithout%20relying%20on%20external%20data%20or%20models.%20In%20addition%2C%20we%20introduce%20a%20simple%0Amethod%20for%20creating%20a%20dataset%20designed%20to%20improve%20alignment%20between%20predicted%0Ascores%20and%20their%20textual%20justifications.%20By%20iteratively%20training%20the%20model%20with%0ADirect%20Preference%20Optimization%20on%20two%20distinct%20datasets%20and%20merging%20them%2C%20we%0Acan%20improve%20both%20scoring%20accuracy%20and%20the%20coherence%20of%20generated%20explanations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02708v1&entry.124074799=Read"},
{"title": "From Flat to Hierarchical: Extracting Sparse Representations with\n  Matching Pursuit", "author": "Val\u00e9rie Costa and Thomas Fel and Ekdeep Singh Lubana and Bahareh Tolooshams and Demba Ba", "abstract": "  Motivated by the hypothesis that neural network representations encode\nabstract, interpretable features as linearly accessible, approximately\northogonal directions, sparse autoencoders (SAEs) have become a popular tool in\ninterpretability. However, recent work has demonstrated phenomenology of model\nrepresentations that lies outside the scope of this hypothesis, showing\nsignatures of hierarchical, nonlinear, and multi-dimensional features. This\nraises the question: do SAEs represent features that possess structure at odds\nwith their motivating hypothesis? If not, does avoiding this mismatch help\nidentify said features and gain further insights into neural network\nrepresentations? To answer these questions, we take a construction-based\napproach and re-contextualize the popular matching pursuits (MP) algorithm from\nsparse coding to design MP-SAE -- an SAE that unrolls its encoder into a\nsequence of residual-guided steps, allowing it to capture hierarchical and\nnonlinearly accessible features. Comparing this architecture with existing SAEs\non a mixture of synthetic and natural data settings, we show: (i) hierarchical\nconcepts induce conditionally orthogonal features, which existing SAEs are\nunable to faithfully capture, and (ii) the nonlinear encoding step of MP-SAE\nrecovers highly meaningful features, helping us unravel shared structure in the\nseemingly dichotomous representation spaces of different modalities in a\nvision-language model, hence demonstrating the assumption that useful features\nare solely linearly accessible is insufficient. We also show that the\nsequential encoder principle of MP-SAE affords an additional benefit of\nadaptive sparsity at inference time, which may be of independent interest.\nOverall, we argue our results provide credence to the idea that\ninterpretability should begin with the phenomenology of representations, with\nmethods emerging from assumptions that fit it.\n", "link": "http://arxiv.org/abs/2506.03093v1", "date": "2025-06-03", "relevancy": 2.8936, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5807}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5807}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Flat%20to%20Hierarchical%3A%20Extracting%20Sparse%20Representations%20with%0A%20%20Matching%20Pursuit&body=Title%3A%20From%20Flat%20to%20Hierarchical%3A%20Extracting%20Sparse%20Representations%20with%0A%20%20Matching%20Pursuit%0AAuthor%3A%20Val%C3%A9rie%20Costa%20and%20Thomas%20Fel%20and%20Ekdeep%20Singh%20Lubana%20and%20Bahareh%20Tolooshams%20and%20Demba%20Ba%0AAbstract%3A%20%20%20Motivated%20by%20the%20hypothesis%20that%20neural%20network%20representations%20encode%0Aabstract%2C%20interpretable%20features%20as%20linearly%20accessible%2C%20approximately%0Aorthogonal%20directions%2C%20sparse%20autoencoders%20%28SAEs%29%20have%20become%20a%20popular%20tool%20in%0Ainterpretability.%20However%2C%20recent%20work%20has%20demonstrated%20phenomenology%20of%20model%0Arepresentations%20that%20lies%20outside%20the%20scope%20of%20this%20hypothesis%2C%20showing%0Asignatures%20of%20hierarchical%2C%20nonlinear%2C%20and%20multi-dimensional%20features.%20This%0Araises%20the%20question%3A%20do%20SAEs%20represent%20features%20that%20possess%20structure%20at%20odds%0Awith%20their%20motivating%20hypothesis%3F%20If%20not%2C%20does%20avoiding%20this%20mismatch%20help%0Aidentify%20said%20features%20and%20gain%20further%20insights%20into%20neural%20network%0Arepresentations%3F%20To%20answer%20these%20questions%2C%20we%20take%20a%20construction-based%0Aapproach%20and%20re-contextualize%20the%20popular%20matching%20pursuits%20%28MP%29%20algorithm%20from%0Asparse%20coding%20to%20design%20MP-SAE%20--%20an%20SAE%20that%20unrolls%20its%20encoder%20into%20a%0Asequence%20of%20residual-guided%20steps%2C%20allowing%20it%20to%20capture%20hierarchical%20and%0Anonlinearly%20accessible%20features.%20Comparing%20this%20architecture%20with%20existing%20SAEs%0Aon%20a%20mixture%20of%20synthetic%20and%20natural%20data%20settings%2C%20we%20show%3A%20%28i%29%20hierarchical%0Aconcepts%20induce%20conditionally%20orthogonal%20features%2C%20which%20existing%20SAEs%20are%0Aunable%20to%20faithfully%20capture%2C%20and%20%28ii%29%20the%20nonlinear%20encoding%20step%20of%20MP-SAE%0Arecovers%20highly%20meaningful%20features%2C%20helping%20us%20unravel%20shared%20structure%20in%20the%0Aseemingly%20dichotomous%20representation%20spaces%20of%20different%20modalities%20in%20a%0Avision-language%20model%2C%20hence%20demonstrating%20the%20assumption%20that%20useful%20features%0Aare%20solely%20linearly%20accessible%20is%20insufficient.%20We%20also%20show%20that%20the%0Asequential%20encoder%20principle%20of%20MP-SAE%20affords%20an%20additional%20benefit%20of%0Aadaptive%20sparsity%20at%20inference%20time%2C%20which%20may%20be%20of%20independent%20interest.%0AOverall%2C%20we%20argue%20our%20results%20provide%20credence%20to%20the%20idea%20that%0Ainterpretability%20should%20begin%20with%20the%20phenomenology%20of%20representations%2C%20with%0Amethods%20emerging%20from%20assumptions%20that%20fit%20it.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03093v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Flat%2520to%2520Hierarchical%253A%2520Extracting%2520Sparse%2520Representations%2520with%250A%2520%2520Matching%2520Pursuit%26entry.906535625%3DVal%25C3%25A9rie%2520Costa%2520and%2520Thomas%2520Fel%2520and%2520Ekdeep%2520Singh%2520Lubana%2520and%2520Bahareh%2520Tolooshams%2520and%2520Demba%2520Ba%26entry.1292438233%3D%2520%2520Motivated%2520by%2520the%2520hypothesis%2520that%2520neural%2520network%2520representations%2520encode%250Aabstract%252C%2520interpretable%2520features%2520as%2520linearly%2520accessible%252C%2520approximately%250Aorthogonal%2520directions%252C%2520sparse%2520autoencoders%2520%2528SAEs%2529%2520have%2520become%2520a%2520popular%2520tool%2520in%250Ainterpretability.%2520However%252C%2520recent%2520work%2520has%2520demonstrated%2520phenomenology%2520of%2520model%250Arepresentations%2520that%2520lies%2520outside%2520the%2520scope%2520of%2520this%2520hypothesis%252C%2520showing%250Asignatures%2520of%2520hierarchical%252C%2520nonlinear%252C%2520and%2520multi-dimensional%2520features.%2520This%250Araises%2520the%2520question%253A%2520do%2520SAEs%2520represent%2520features%2520that%2520possess%2520structure%2520at%2520odds%250Awith%2520their%2520motivating%2520hypothesis%253F%2520If%2520not%252C%2520does%2520avoiding%2520this%2520mismatch%2520help%250Aidentify%2520said%2520features%2520and%2520gain%2520further%2520insights%2520into%2520neural%2520network%250Arepresentations%253F%2520To%2520answer%2520these%2520questions%252C%2520we%2520take%2520a%2520construction-based%250Aapproach%2520and%2520re-contextualize%2520the%2520popular%2520matching%2520pursuits%2520%2528MP%2529%2520algorithm%2520from%250Asparse%2520coding%2520to%2520design%2520MP-SAE%2520--%2520an%2520SAE%2520that%2520unrolls%2520its%2520encoder%2520into%2520a%250Asequence%2520of%2520residual-guided%2520steps%252C%2520allowing%2520it%2520to%2520capture%2520hierarchical%2520and%250Anonlinearly%2520accessible%2520features.%2520Comparing%2520this%2520architecture%2520with%2520existing%2520SAEs%250Aon%2520a%2520mixture%2520of%2520synthetic%2520and%2520natural%2520data%2520settings%252C%2520we%2520show%253A%2520%2528i%2529%2520hierarchical%250Aconcepts%2520induce%2520conditionally%2520orthogonal%2520features%252C%2520which%2520existing%2520SAEs%2520are%250Aunable%2520to%2520faithfully%2520capture%252C%2520and%2520%2528ii%2529%2520the%2520nonlinear%2520encoding%2520step%2520of%2520MP-SAE%250Arecovers%2520highly%2520meaningful%2520features%252C%2520helping%2520us%2520unravel%2520shared%2520structure%2520in%2520the%250Aseemingly%2520dichotomous%2520representation%2520spaces%2520of%2520different%2520modalities%2520in%2520a%250Avision-language%2520model%252C%2520hence%2520demonstrating%2520the%2520assumption%2520that%2520useful%2520features%250Aare%2520solely%2520linearly%2520accessible%2520is%2520insufficient.%2520We%2520also%2520show%2520that%2520the%250Asequential%2520encoder%2520principle%2520of%2520MP-SAE%2520affords%2520an%2520additional%2520benefit%2520of%250Aadaptive%2520sparsity%2520at%2520inference%2520time%252C%2520which%2520may%2520be%2520of%2520independent%2520interest.%250AOverall%252C%2520we%2520argue%2520our%2520results%2520provide%2520credence%2520to%2520the%2520idea%2520that%250Ainterpretability%2520should%2520begin%2520with%2520the%2520phenomenology%2520of%2520representations%252C%2520with%250Amethods%2520emerging%2520from%2520assumptions%2520that%2520fit%2520it.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03093v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Flat%20to%20Hierarchical%3A%20Extracting%20Sparse%20Representations%20with%0A%20%20Matching%20Pursuit&entry.906535625=Val%C3%A9rie%20Costa%20and%20Thomas%20Fel%20and%20Ekdeep%20Singh%20Lubana%20and%20Bahareh%20Tolooshams%20and%20Demba%20Ba&entry.1292438233=%20%20Motivated%20by%20the%20hypothesis%20that%20neural%20network%20representations%20encode%0Aabstract%2C%20interpretable%20features%20as%20linearly%20accessible%2C%20approximately%0Aorthogonal%20directions%2C%20sparse%20autoencoders%20%28SAEs%29%20have%20become%20a%20popular%20tool%20in%0Ainterpretability.%20However%2C%20recent%20work%20has%20demonstrated%20phenomenology%20of%20model%0Arepresentations%20that%20lies%20outside%20the%20scope%20of%20this%20hypothesis%2C%20showing%0Asignatures%20of%20hierarchical%2C%20nonlinear%2C%20and%20multi-dimensional%20features.%20This%0Araises%20the%20question%3A%20do%20SAEs%20represent%20features%20that%20possess%20structure%20at%20odds%0Awith%20their%20motivating%20hypothesis%3F%20If%20not%2C%20does%20avoiding%20this%20mismatch%20help%0Aidentify%20said%20features%20and%20gain%20further%20insights%20into%20neural%20network%0Arepresentations%3F%20To%20answer%20these%20questions%2C%20we%20take%20a%20construction-based%0Aapproach%20and%20re-contextualize%20the%20popular%20matching%20pursuits%20%28MP%29%20algorithm%20from%0Asparse%20coding%20to%20design%20MP-SAE%20--%20an%20SAE%20that%20unrolls%20its%20encoder%20into%20a%0Asequence%20of%20residual-guided%20steps%2C%20allowing%20it%20to%20capture%20hierarchical%20and%0Anonlinearly%20accessible%20features.%20Comparing%20this%20architecture%20with%20existing%20SAEs%0Aon%20a%20mixture%20of%20synthetic%20and%20natural%20data%20settings%2C%20we%20show%3A%20%28i%29%20hierarchical%0Aconcepts%20induce%20conditionally%20orthogonal%20features%2C%20which%20existing%20SAEs%20are%0Aunable%20to%20faithfully%20capture%2C%20and%20%28ii%29%20the%20nonlinear%20encoding%20step%20of%20MP-SAE%0Arecovers%20highly%20meaningful%20features%2C%20helping%20us%20unravel%20shared%20structure%20in%20the%0Aseemingly%20dichotomous%20representation%20spaces%20of%20different%20modalities%20in%20a%0Avision-language%20model%2C%20hence%20demonstrating%20the%20assumption%20that%20useful%20features%0Aare%20solely%20linearly%20accessible%20is%20insufficient.%20We%20also%20show%20that%20the%0Asequential%20encoder%20principle%20of%20MP-SAE%20affords%20an%20additional%20benefit%20of%0Aadaptive%20sparsity%20at%20inference%20time%2C%20which%20may%20be%20of%20independent%20interest.%0AOverall%2C%20we%20argue%20our%20results%20provide%20credence%20to%20the%20idea%20that%0Ainterpretability%20should%20begin%20with%20the%20phenomenology%20of%20representations%2C%20with%0Amethods%20emerging%20from%20assumptions%20that%20fit%20it.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03093v1&entry.124074799=Read"},
{"title": "Scene Structure Guidance Network: Unfolding Graph Partitioning into\n  Pixel-Wise Feature Learning", "author": "Jisu Shin and Seunghyun Shin and Hae-Gon Jeon", "abstract": "  Understanding the informative structures of scenes is essential for low-level\nvision tasks. Unfortunately, it is difficult to obtain a concrete visual\ndefinition of the informative structures because influences of visual features\nare task-specific. In this paper, we propose a single general neural network\narchitecture for extracting task-specific structure guidance for scenes. To do\nthis, we first analyze traditional spectral clustering methods, which computes\na set of eigenvectors to model a segmented graph forming small compact\nstructures on image domains. We then unfold the traditional graph-partitioning\nproblem into a learnable network, named \\textit{Scene Structure Guidance\nNetwork (SSGNet)}, to represent the task-specific informative structures. The\nSSGNet yields a set of coefficients of eigenvectors that produces explicit\nfeature representations of image structures. In addition, our SSGNet is\nlight-weight ($\\sim$ 56K parameters), and can be used as a plug-and-play module\nfor off-the-shelf architectures. We optimize the SSGNet without any supervision\nby proposing two novel training losses that enforce task-specific scene\nstructure generation during training. Our main contribution is to show that\nsuch a simple network can achieve state-of-the-art results for several\nlow-level vision applications. We also demonstrate that our network generalizes\nwell on unseen datasets, compared to existing methods which use structural\nembedding frameworks. We further propose a lighter version of SSGNet ($\\sim$\n29K parameters) for depth computation, SSGNet-D, and successfully execute it on\nedge computing devices like Jetson AGX Orin, improving the performance of\nbaseline network, even in the wild, with little computational delay.\n", "link": "http://arxiv.org/abs/2301.00555v2", "date": "2025-06-03", "relevancy": 2.8885, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5819}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5756}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5756}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scene%20Structure%20Guidance%20Network%3A%20Unfolding%20Graph%20Partitioning%20into%0A%20%20Pixel-Wise%20Feature%20Learning&body=Title%3A%20Scene%20Structure%20Guidance%20Network%3A%20Unfolding%20Graph%20Partitioning%20into%0A%20%20Pixel-Wise%20Feature%20Learning%0AAuthor%3A%20Jisu%20Shin%20and%20Seunghyun%20Shin%20and%20Hae-Gon%20Jeon%0AAbstract%3A%20%20%20Understanding%20the%20informative%20structures%20of%20scenes%20is%20essential%20for%20low-level%0Avision%20tasks.%20Unfortunately%2C%20it%20is%20difficult%20to%20obtain%20a%20concrete%20visual%0Adefinition%20of%20the%20informative%20structures%20because%20influences%20of%20visual%20features%0Aare%20task-specific.%20In%20this%20paper%2C%20we%20propose%20a%20single%20general%20neural%20network%0Aarchitecture%20for%20extracting%20task-specific%20structure%20guidance%20for%20scenes.%20To%20do%0Athis%2C%20we%20first%20analyze%20traditional%20spectral%20clustering%20methods%2C%20which%20computes%0Aa%20set%20of%20eigenvectors%20to%20model%20a%20segmented%20graph%20forming%20small%20compact%0Astructures%20on%20image%20domains.%20We%20then%20unfold%20the%20traditional%20graph-partitioning%0Aproblem%20into%20a%20learnable%20network%2C%20named%20%5Ctextit%7BScene%20Structure%20Guidance%0ANetwork%20%28SSGNet%29%7D%2C%20to%20represent%20the%20task-specific%20informative%20structures.%20The%0ASSGNet%20yields%20a%20set%20of%20coefficients%20of%20eigenvectors%20that%20produces%20explicit%0Afeature%20representations%20of%20image%20structures.%20In%20addition%2C%20our%20SSGNet%20is%0Alight-weight%20%28%24%5Csim%24%2056K%20parameters%29%2C%20and%20can%20be%20used%20as%20a%20plug-and-play%20module%0Afor%20off-the-shelf%20architectures.%20We%20optimize%20the%20SSGNet%20without%20any%20supervision%0Aby%20proposing%20two%20novel%20training%20losses%20that%20enforce%20task-specific%20scene%0Astructure%20generation%20during%20training.%20Our%20main%20contribution%20is%20to%20show%20that%0Asuch%20a%20simple%20network%20can%20achieve%20state-of-the-art%20results%20for%20several%0Alow-level%20vision%20applications.%20We%20also%20demonstrate%20that%20our%20network%20generalizes%0Awell%20on%20unseen%20datasets%2C%20compared%20to%20existing%20methods%20which%20use%20structural%0Aembedding%20frameworks.%20We%20further%20propose%20a%20lighter%20version%20of%20SSGNet%20%28%24%5Csim%24%0A29K%20parameters%29%20for%20depth%20computation%2C%20SSGNet-D%2C%20and%20successfully%20execute%20it%20on%0Aedge%20computing%20devices%20like%20Jetson%20AGX%20Orin%2C%20improving%20the%20performance%20of%0Abaseline%20network%2C%20even%20in%20the%20wild%2C%20with%20little%20computational%20delay.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.00555v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScene%2520Structure%2520Guidance%2520Network%253A%2520Unfolding%2520Graph%2520Partitioning%2520into%250A%2520%2520Pixel-Wise%2520Feature%2520Learning%26entry.906535625%3DJisu%2520Shin%2520and%2520Seunghyun%2520Shin%2520and%2520Hae-Gon%2520Jeon%26entry.1292438233%3D%2520%2520Understanding%2520the%2520informative%2520structures%2520of%2520scenes%2520is%2520essential%2520for%2520low-level%250Avision%2520tasks.%2520Unfortunately%252C%2520it%2520is%2520difficult%2520to%2520obtain%2520a%2520concrete%2520visual%250Adefinition%2520of%2520the%2520informative%2520structures%2520because%2520influences%2520of%2520visual%2520features%250Aare%2520task-specific.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520single%2520general%2520neural%2520network%250Aarchitecture%2520for%2520extracting%2520task-specific%2520structure%2520guidance%2520for%2520scenes.%2520To%2520do%250Athis%252C%2520we%2520first%2520analyze%2520traditional%2520spectral%2520clustering%2520methods%252C%2520which%2520computes%250Aa%2520set%2520of%2520eigenvectors%2520to%2520model%2520a%2520segmented%2520graph%2520forming%2520small%2520compact%250Astructures%2520on%2520image%2520domains.%2520We%2520then%2520unfold%2520the%2520traditional%2520graph-partitioning%250Aproblem%2520into%2520a%2520learnable%2520network%252C%2520named%2520%255Ctextit%257BScene%2520Structure%2520Guidance%250ANetwork%2520%2528SSGNet%2529%257D%252C%2520to%2520represent%2520the%2520task-specific%2520informative%2520structures.%2520The%250ASSGNet%2520yields%2520a%2520set%2520of%2520coefficients%2520of%2520eigenvectors%2520that%2520produces%2520explicit%250Afeature%2520representations%2520of%2520image%2520structures.%2520In%2520addition%252C%2520our%2520SSGNet%2520is%250Alight-weight%2520%2528%2524%255Csim%2524%252056K%2520parameters%2529%252C%2520and%2520can%2520be%2520used%2520as%2520a%2520plug-and-play%2520module%250Afor%2520off-the-shelf%2520architectures.%2520We%2520optimize%2520the%2520SSGNet%2520without%2520any%2520supervision%250Aby%2520proposing%2520two%2520novel%2520training%2520losses%2520that%2520enforce%2520task-specific%2520scene%250Astructure%2520generation%2520during%2520training.%2520Our%2520main%2520contribution%2520is%2520to%2520show%2520that%250Asuch%2520a%2520simple%2520network%2520can%2520achieve%2520state-of-the-art%2520results%2520for%2520several%250Alow-level%2520vision%2520applications.%2520We%2520also%2520demonstrate%2520that%2520our%2520network%2520generalizes%250Awell%2520on%2520unseen%2520datasets%252C%2520compared%2520to%2520existing%2520methods%2520which%2520use%2520structural%250Aembedding%2520frameworks.%2520We%2520further%2520propose%2520a%2520lighter%2520version%2520of%2520SSGNet%2520%2528%2524%255Csim%2524%250A29K%2520parameters%2529%2520for%2520depth%2520computation%252C%2520SSGNet-D%252C%2520and%2520successfully%2520execute%2520it%2520on%250Aedge%2520computing%2520devices%2520like%2520Jetson%2520AGX%2520Orin%252C%2520improving%2520the%2520performance%2520of%250Abaseline%2520network%252C%2520even%2520in%2520the%2520wild%252C%2520with%2520little%2520computational%2520delay.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.00555v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scene%20Structure%20Guidance%20Network%3A%20Unfolding%20Graph%20Partitioning%20into%0A%20%20Pixel-Wise%20Feature%20Learning&entry.906535625=Jisu%20Shin%20and%20Seunghyun%20Shin%20and%20Hae-Gon%20Jeon&entry.1292438233=%20%20Understanding%20the%20informative%20structures%20of%20scenes%20is%20essential%20for%20low-level%0Avision%20tasks.%20Unfortunately%2C%20it%20is%20difficult%20to%20obtain%20a%20concrete%20visual%0Adefinition%20of%20the%20informative%20structures%20because%20influences%20of%20visual%20features%0Aare%20task-specific.%20In%20this%20paper%2C%20we%20propose%20a%20single%20general%20neural%20network%0Aarchitecture%20for%20extracting%20task-specific%20structure%20guidance%20for%20scenes.%20To%20do%0Athis%2C%20we%20first%20analyze%20traditional%20spectral%20clustering%20methods%2C%20which%20computes%0Aa%20set%20of%20eigenvectors%20to%20model%20a%20segmented%20graph%20forming%20small%20compact%0Astructures%20on%20image%20domains.%20We%20then%20unfold%20the%20traditional%20graph-partitioning%0Aproblem%20into%20a%20learnable%20network%2C%20named%20%5Ctextit%7BScene%20Structure%20Guidance%0ANetwork%20%28SSGNet%29%7D%2C%20to%20represent%20the%20task-specific%20informative%20structures.%20The%0ASSGNet%20yields%20a%20set%20of%20coefficients%20of%20eigenvectors%20that%20produces%20explicit%0Afeature%20representations%20of%20image%20structures.%20In%20addition%2C%20our%20SSGNet%20is%0Alight-weight%20%28%24%5Csim%24%2056K%20parameters%29%2C%20and%20can%20be%20used%20as%20a%20plug-and-play%20module%0Afor%20off-the-shelf%20architectures.%20We%20optimize%20the%20SSGNet%20without%20any%20supervision%0Aby%20proposing%20two%20novel%20training%20losses%20that%20enforce%20task-specific%20scene%0Astructure%20generation%20during%20training.%20Our%20main%20contribution%20is%20to%20show%20that%0Asuch%20a%20simple%20network%20can%20achieve%20state-of-the-art%20results%20for%20several%0Alow-level%20vision%20applications.%20We%20also%20demonstrate%20that%20our%20network%20generalizes%0Awell%20on%20unseen%20datasets%2C%20compared%20to%20existing%20methods%20which%20use%20structural%0Aembedding%20frameworks.%20We%20further%20propose%20a%20lighter%20version%20of%20SSGNet%20%28%24%5Csim%24%0A29K%20parameters%29%20for%20depth%20computation%2C%20SSGNet-D%2C%20and%20successfully%20execute%20it%20on%0Aedge%20computing%20devices%20like%20Jetson%20AGX%20Orin%2C%20improving%20the%20performance%20of%0Abaseline%20network%2C%20even%20in%20the%20wild%2C%20with%20little%20computational%20delay.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.00555v2&entry.124074799=Read"},
{"title": "Enriching Location Representation with Detailed Semantic Information", "author": "Junyuan Liu and Xinglei Wang and Tao Cheng", "abstract": "  Spatial representations that capture both structural and semantic\ncharacteristics of urban environments are essential for urban modeling.\nTraditional spatial embeddings often prioritize spatial proximity while\nunderutilizing fine-grained contextual information from places. To address this\nlimitation, we introduce CaLLiPer+, an extension of the CaLLiPer model that\nsystematically integrates Point-of-Interest (POI) names alongside categorical\nlabels within a multimodal contrastive learning framework. We evaluate its\neffectiveness on two downstream tasks, land use classification and\nsocioeconomic status distribution mapping, demonstrating consistent performance\ngains of 4% to 11% over baseline methods. Additionally, we show that\nincorporating POI names enhances location retrieval, enabling models to capture\ncomplex urban concepts with greater precision. Ablation studies further reveal\nthe complementary role of POI names and the advantages of leveraging pretrained\ntext encoders for spatial representations. Overall, our findings highlight the\npotential of integrating fine-grained semantic attributes and multimodal\nlearning techniques to advance the development of urban foundation models.\n", "link": "http://arxiv.org/abs/2506.02744v1", "date": "2025-06-03", "relevancy": 2.8503, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.592}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5591}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5591}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enriching%20Location%20Representation%20with%20Detailed%20Semantic%20Information&body=Title%3A%20Enriching%20Location%20Representation%20with%20Detailed%20Semantic%20Information%0AAuthor%3A%20Junyuan%20Liu%20and%20Xinglei%20Wang%20and%20Tao%20Cheng%0AAbstract%3A%20%20%20Spatial%20representations%20that%20capture%20both%20structural%20and%20semantic%0Acharacteristics%20of%20urban%20environments%20are%20essential%20for%20urban%20modeling.%0ATraditional%20spatial%20embeddings%20often%20prioritize%20spatial%20proximity%20while%0Aunderutilizing%20fine-grained%20contextual%20information%20from%20places.%20To%20address%20this%0Alimitation%2C%20we%20introduce%20CaLLiPer%2B%2C%20an%20extension%20of%20the%20CaLLiPer%20model%20that%0Asystematically%20integrates%20Point-of-Interest%20%28POI%29%20names%20alongside%20categorical%0Alabels%20within%20a%20multimodal%20contrastive%20learning%20framework.%20We%20evaluate%20its%0Aeffectiveness%20on%20two%20downstream%20tasks%2C%20land%20use%20classification%20and%0Asocioeconomic%20status%20distribution%20mapping%2C%20demonstrating%20consistent%20performance%0Agains%20of%204%25%20to%2011%25%20over%20baseline%20methods.%20Additionally%2C%20we%20show%20that%0Aincorporating%20POI%20names%20enhances%20location%20retrieval%2C%20enabling%20models%20to%20capture%0Acomplex%20urban%20concepts%20with%20greater%20precision.%20Ablation%20studies%20further%20reveal%0Athe%20complementary%20role%20of%20POI%20names%20and%20the%20advantages%20of%20leveraging%20pretrained%0Atext%20encoders%20for%20spatial%20representations.%20Overall%2C%20our%20findings%20highlight%20the%0Apotential%20of%20integrating%20fine-grained%20semantic%20attributes%20and%20multimodal%0Alearning%20techniques%20to%20advance%20the%20development%20of%20urban%20foundation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02744v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnriching%2520Location%2520Representation%2520with%2520Detailed%2520Semantic%2520Information%26entry.906535625%3DJunyuan%2520Liu%2520and%2520Xinglei%2520Wang%2520and%2520Tao%2520Cheng%26entry.1292438233%3D%2520%2520Spatial%2520representations%2520that%2520capture%2520both%2520structural%2520and%2520semantic%250Acharacteristics%2520of%2520urban%2520environments%2520are%2520essential%2520for%2520urban%2520modeling.%250ATraditional%2520spatial%2520embeddings%2520often%2520prioritize%2520spatial%2520proximity%2520while%250Aunderutilizing%2520fine-grained%2520contextual%2520information%2520from%2520places.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520introduce%2520CaLLiPer%252B%252C%2520an%2520extension%2520of%2520the%2520CaLLiPer%2520model%2520that%250Asystematically%2520integrates%2520Point-of-Interest%2520%2528POI%2529%2520names%2520alongside%2520categorical%250Alabels%2520within%2520a%2520multimodal%2520contrastive%2520learning%2520framework.%2520We%2520evaluate%2520its%250Aeffectiveness%2520on%2520two%2520downstream%2520tasks%252C%2520land%2520use%2520classification%2520and%250Asocioeconomic%2520status%2520distribution%2520mapping%252C%2520demonstrating%2520consistent%2520performance%250Agains%2520of%25204%2525%2520to%252011%2525%2520over%2520baseline%2520methods.%2520Additionally%252C%2520we%2520show%2520that%250Aincorporating%2520POI%2520names%2520enhances%2520location%2520retrieval%252C%2520enabling%2520models%2520to%2520capture%250Acomplex%2520urban%2520concepts%2520with%2520greater%2520precision.%2520Ablation%2520studies%2520further%2520reveal%250Athe%2520complementary%2520role%2520of%2520POI%2520names%2520and%2520the%2520advantages%2520of%2520leveraging%2520pretrained%250Atext%2520encoders%2520for%2520spatial%2520representations.%2520Overall%252C%2520our%2520findings%2520highlight%2520the%250Apotential%2520of%2520integrating%2520fine-grained%2520semantic%2520attributes%2520and%2520multimodal%250Alearning%2520techniques%2520to%2520advance%2520the%2520development%2520of%2520urban%2520foundation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02744v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enriching%20Location%20Representation%20with%20Detailed%20Semantic%20Information&entry.906535625=Junyuan%20Liu%20and%20Xinglei%20Wang%20and%20Tao%20Cheng&entry.1292438233=%20%20Spatial%20representations%20that%20capture%20both%20structural%20and%20semantic%0Acharacteristics%20of%20urban%20environments%20are%20essential%20for%20urban%20modeling.%0ATraditional%20spatial%20embeddings%20often%20prioritize%20spatial%20proximity%20while%0Aunderutilizing%20fine-grained%20contextual%20information%20from%20places.%20To%20address%20this%0Alimitation%2C%20we%20introduce%20CaLLiPer%2B%2C%20an%20extension%20of%20the%20CaLLiPer%20model%20that%0Asystematically%20integrates%20Point-of-Interest%20%28POI%29%20names%20alongside%20categorical%0Alabels%20within%20a%20multimodal%20contrastive%20learning%20framework.%20We%20evaluate%20its%0Aeffectiveness%20on%20two%20downstream%20tasks%2C%20land%20use%20classification%20and%0Asocioeconomic%20status%20distribution%20mapping%2C%20demonstrating%20consistent%20performance%0Agains%20of%204%25%20to%2011%25%20over%20baseline%20methods.%20Additionally%2C%20we%20show%20that%0Aincorporating%20POI%20names%20enhances%20location%20retrieval%2C%20enabling%20models%20to%20capture%0Acomplex%20urban%20concepts%20with%20greater%20precision.%20Ablation%20studies%20further%20reveal%0Athe%20complementary%20role%20of%20POI%20names%20and%20the%20advantages%20of%20leveraging%20pretrained%0Atext%20encoders%20for%20spatial%20representations.%20Overall%2C%20our%20findings%20highlight%20the%0Apotential%20of%20integrating%20fine-grained%20semantic%20attributes%20and%20multimodal%0Alearning%20techniques%20to%20advance%20the%20development%20of%20urban%20foundation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02744v1&entry.124074799=Read"},
{"title": "Linear Spatial World Models Emerge in Large Language Models", "author": "Matthieu Tehenan and Christian Bolivar Moya and Tenghai Long and Guang Lin", "abstract": "  Large language models (LLMs) have demonstrated emergent abilities across\ndiverse tasks, raising the question of whether they acquire internal world\nmodels. In this work, we investigate whether LLMs implicitly encode linear\nspatial world models, which we define as linear representations of physical\nspace and object configurations. We introduce a formal framework for spatial\nworld models and assess whether such structure emerges in contextual\nembeddings. Using a synthetic dataset of object positions, we train probes to\ndecode object positions and evaluate geometric consistency of the underlying\nspace. We further conduct causal interventions to test whether these spatial\nrepresentations are functionally used by the model. Our results provide\nempirical evidence that LLMs encode linear spatial world models.\n", "link": "http://arxiv.org/abs/2506.02996v1", "date": "2025-06-03", "relevancy": 2.8373, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6026}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6026}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4973}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Linear%20Spatial%20World%20Models%20Emerge%20in%20Large%20Language%20Models&body=Title%3A%20Linear%20Spatial%20World%20Models%20Emerge%20in%20Large%20Language%20Models%0AAuthor%3A%20Matthieu%20Tehenan%20and%20Christian%20Bolivar%20Moya%20and%20Tenghai%20Long%20and%20Guang%20Lin%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20emergent%20abilities%20across%0Adiverse%20tasks%2C%20raising%20the%20question%20of%20whether%20they%20acquire%20internal%20world%0Amodels.%20In%20this%20work%2C%20we%20investigate%20whether%20LLMs%20implicitly%20encode%20linear%0Aspatial%20world%20models%2C%20which%20we%20define%20as%20linear%20representations%20of%20physical%0Aspace%20and%20object%20configurations.%20We%20introduce%20a%20formal%20framework%20for%20spatial%0Aworld%20models%20and%20assess%20whether%20such%20structure%20emerges%20in%20contextual%0Aembeddings.%20Using%20a%20synthetic%20dataset%20of%20object%20positions%2C%20we%20train%20probes%20to%0Adecode%20object%20positions%20and%20evaluate%20geometric%20consistency%20of%20the%20underlying%0Aspace.%20We%20further%20conduct%20causal%20interventions%20to%20test%20whether%20these%20spatial%0Arepresentations%20are%20functionally%20used%20by%20the%20model.%20Our%20results%20provide%0Aempirical%20evidence%20that%20LLMs%20encode%20linear%20spatial%20world%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02996v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLinear%2520Spatial%2520World%2520Models%2520Emerge%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DMatthieu%2520Tehenan%2520and%2520Christian%2520Bolivar%2520Moya%2520and%2520Tenghai%2520Long%2520and%2520Guang%2520Lin%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520emergent%2520abilities%2520across%250Adiverse%2520tasks%252C%2520raising%2520the%2520question%2520of%2520whether%2520they%2520acquire%2520internal%2520world%250Amodels.%2520In%2520this%2520work%252C%2520we%2520investigate%2520whether%2520LLMs%2520implicitly%2520encode%2520linear%250Aspatial%2520world%2520models%252C%2520which%2520we%2520define%2520as%2520linear%2520representations%2520of%2520physical%250Aspace%2520and%2520object%2520configurations.%2520We%2520introduce%2520a%2520formal%2520framework%2520for%2520spatial%250Aworld%2520models%2520and%2520assess%2520whether%2520such%2520structure%2520emerges%2520in%2520contextual%250Aembeddings.%2520Using%2520a%2520synthetic%2520dataset%2520of%2520object%2520positions%252C%2520we%2520train%2520probes%2520to%250Adecode%2520object%2520positions%2520and%2520evaluate%2520geometric%2520consistency%2520of%2520the%2520underlying%250Aspace.%2520We%2520further%2520conduct%2520causal%2520interventions%2520to%2520test%2520whether%2520these%2520spatial%250Arepresentations%2520are%2520functionally%2520used%2520by%2520the%2520model.%2520Our%2520results%2520provide%250Aempirical%2520evidence%2520that%2520LLMs%2520encode%2520linear%2520spatial%2520world%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02996v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Linear%20Spatial%20World%20Models%20Emerge%20in%20Large%20Language%20Models&entry.906535625=Matthieu%20Tehenan%20and%20Christian%20Bolivar%20Moya%20and%20Tenghai%20Long%20and%20Guang%20Lin&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20emergent%20abilities%20across%0Adiverse%20tasks%2C%20raising%20the%20question%20of%20whether%20they%20acquire%20internal%20world%0Amodels.%20In%20this%20work%2C%20we%20investigate%20whether%20LLMs%20implicitly%20encode%20linear%0Aspatial%20world%20models%2C%20which%20we%20define%20as%20linear%20representations%20of%20physical%0Aspace%20and%20object%20configurations.%20We%20introduce%20a%20formal%20framework%20for%20spatial%0Aworld%20models%20and%20assess%20whether%20such%20structure%20emerges%20in%20contextual%0Aembeddings.%20Using%20a%20synthetic%20dataset%20of%20object%20positions%2C%20we%20train%20probes%20to%0Adecode%20object%20positions%20and%20evaluate%20geometric%20consistency%20of%20the%20underlying%0Aspace.%20We%20further%20conduct%20causal%20interventions%20to%20test%20whether%20these%20spatial%0Arepresentations%20are%20functionally%20used%20by%20the%20model.%20Our%20results%20provide%0Aempirical%20evidence%20that%20LLMs%20encode%20linear%20spatial%20world%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02996v1&entry.124074799=Read"},
{"title": "Unified Attention Modeling for Efficient Free-Viewing and Visual Search\n  via Shared Representations", "author": "Fatma Youssef Mohammed and Kostas Alexis", "abstract": "  Computational human attention modeling in free-viewing and task-specific\nsettings is often studied separately, with limited exploration of whether a\ncommon representation exists between them. This work investigates this question\nand proposes a neural network architecture that builds upon the Human Attention\ntransformer (HAT) to test the hypothesis. Our results demonstrate that\nfree-viewing and visual search can efficiently share a common representation,\nallowing a model trained in free-viewing attention to transfer its knowledge to\ntask-driven visual search with a performance drop of only 3.86% in the\npredicted fixation scanpaths, measured by the semantic sequence score (SemSS)\nmetric which reflects the similarity between predicted and human scanpaths.\nThis transfer reduces computational costs by 92.29% in terms of GFLOPs and\n31.23% in terms of trainable parameters.\n", "link": "http://arxiv.org/abs/2506.02764v1", "date": "2025-06-03", "relevancy": 2.827, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5657}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5657}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5649}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unified%20Attention%20Modeling%20for%20Efficient%20Free-Viewing%20and%20Visual%20Search%0A%20%20via%20Shared%20Representations&body=Title%3A%20Unified%20Attention%20Modeling%20for%20Efficient%20Free-Viewing%20and%20Visual%20Search%0A%20%20via%20Shared%20Representations%0AAuthor%3A%20Fatma%20Youssef%20Mohammed%20and%20Kostas%20Alexis%0AAbstract%3A%20%20%20Computational%20human%20attention%20modeling%20in%20free-viewing%20and%20task-specific%0Asettings%20is%20often%20studied%20separately%2C%20with%20limited%20exploration%20of%20whether%20a%0Acommon%20representation%20exists%20between%20them.%20This%20work%20investigates%20this%20question%0Aand%20proposes%20a%20neural%20network%20architecture%20that%20builds%20upon%20the%20Human%20Attention%0Atransformer%20%28HAT%29%20to%20test%20the%20hypothesis.%20Our%20results%20demonstrate%20that%0Afree-viewing%20and%20visual%20search%20can%20efficiently%20share%20a%20common%20representation%2C%0Aallowing%20a%20model%20trained%20in%20free-viewing%20attention%20to%20transfer%20its%20knowledge%20to%0Atask-driven%20visual%20search%20with%20a%20performance%20drop%20of%20only%203.86%25%20in%20the%0Apredicted%20fixation%20scanpaths%2C%20measured%20by%20the%20semantic%20sequence%20score%20%28SemSS%29%0Ametric%20which%20reflects%20the%20similarity%20between%20predicted%20and%20human%20scanpaths.%0AThis%20transfer%20reduces%20computational%20costs%20by%2092.29%25%20in%20terms%20of%20GFLOPs%20and%0A31.23%25%20in%20terms%20of%20trainable%20parameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02764v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnified%2520Attention%2520Modeling%2520for%2520Efficient%2520Free-Viewing%2520and%2520Visual%2520Search%250A%2520%2520via%2520Shared%2520Representations%26entry.906535625%3DFatma%2520Youssef%2520Mohammed%2520and%2520Kostas%2520Alexis%26entry.1292438233%3D%2520%2520Computational%2520human%2520attention%2520modeling%2520in%2520free-viewing%2520and%2520task-specific%250Asettings%2520is%2520often%2520studied%2520separately%252C%2520with%2520limited%2520exploration%2520of%2520whether%2520a%250Acommon%2520representation%2520exists%2520between%2520them.%2520This%2520work%2520investigates%2520this%2520question%250Aand%2520proposes%2520a%2520neural%2520network%2520architecture%2520that%2520builds%2520upon%2520the%2520Human%2520Attention%250Atransformer%2520%2528HAT%2529%2520to%2520test%2520the%2520hypothesis.%2520Our%2520results%2520demonstrate%2520that%250Afree-viewing%2520and%2520visual%2520search%2520can%2520efficiently%2520share%2520a%2520common%2520representation%252C%250Aallowing%2520a%2520model%2520trained%2520in%2520free-viewing%2520attention%2520to%2520transfer%2520its%2520knowledge%2520to%250Atask-driven%2520visual%2520search%2520with%2520a%2520performance%2520drop%2520of%2520only%25203.86%2525%2520in%2520the%250Apredicted%2520fixation%2520scanpaths%252C%2520measured%2520by%2520the%2520semantic%2520sequence%2520score%2520%2528SemSS%2529%250Ametric%2520which%2520reflects%2520the%2520similarity%2520between%2520predicted%2520and%2520human%2520scanpaths.%250AThis%2520transfer%2520reduces%2520computational%2520costs%2520by%252092.29%2525%2520in%2520terms%2520of%2520GFLOPs%2520and%250A31.23%2525%2520in%2520terms%2520of%2520trainable%2520parameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02764v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unified%20Attention%20Modeling%20for%20Efficient%20Free-Viewing%20and%20Visual%20Search%0A%20%20via%20Shared%20Representations&entry.906535625=Fatma%20Youssef%20Mohammed%20and%20Kostas%20Alexis&entry.1292438233=%20%20Computational%20human%20attention%20modeling%20in%20free-viewing%20and%20task-specific%0Asettings%20is%20often%20studied%20separately%2C%20with%20limited%20exploration%20of%20whether%20a%0Acommon%20representation%20exists%20between%20them.%20This%20work%20investigates%20this%20question%0Aand%20proposes%20a%20neural%20network%20architecture%20that%20builds%20upon%20the%20Human%20Attention%0Atransformer%20%28HAT%29%20to%20test%20the%20hypothesis.%20Our%20results%20demonstrate%20that%0Afree-viewing%20and%20visual%20search%20can%20efficiently%20share%20a%20common%20representation%2C%0Aallowing%20a%20model%20trained%20in%20free-viewing%20attention%20to%20transfer%20its%20knowledge%20to%0Atask-driven%20visual%20search%20with%20a%20performance%20drop%20of%20only%203.86%25%20in%20the%0Apredicted%20fixation%20scanpaths%2C%20measured%20by%20the%20semantic%20sequence%20score%20%28SemSS%29%0Ametric%20which%20reflects%20the%20similarity%20between%20predicted%20and%20human%20scanpaths.%0AThis%20transfer%20reduces%20computational%20costs%20by%2092.29%25%20in%20terms%20of%20GFLOPs%20and%0A31.23%25%20in%20terms%20of%20trainable%20parameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02764v1&entry.124074799=Read"},
{"title": "Explicitly Modeling Subcortical Vision with a Neuro-Inspired Front-End\n  Improves CNN Robustness", "author": "Lucas Piper and Arlindo L. Oliveira and Tiago Marques", "abstract": "  Convolutional neural networks (CNNs) trained on object recognition achieve\nhigh task performance but continue to exhibit vulnerability under a range of\nvisual perturbations and out-of-domain images, when compared with biological\nvision. Prior work has demonstrated that coupling a standard CNN with a\nfront-end block (VOneBlock) that mimics the primate primary visual cortex (V1)\ncan improve overall model robustness. Expanding on this, we introduce Early\nVision Networks (EVNets), a new class of hybrid CNNs that combine the VOneBlock\nwith a novel SubcorticalBlock, whose architecture draws from computational\nmodels in neuroscience and is parameterized to maximize alignment with\nsubcortical responses reported across multiple experimental studies. Without\nbeing optimized to do so, the assembly of the SubcorticalBlock with the\nVOneBlock improved V1 alignment across most standard V1 benchmarks, and better\nmodeled extra-classical receptive field phenomena. In addition, EVNets exhibit\nstronger emergent shape bias and overperform the base CNN architecture by 8.5%\non an aggregate benchmark of robustness evaluations, including adversarial\nperturbations, common corruptions, and domain shifts. Finally, we show that\nEVNets can be further improved when paired with a state-of-the-art data\naugmentation technique, surpassing the performance of the isolated data\naugmentation approach by 7.3% on our robustness benchmark. This result reveals\ncomplementary benefits between changes in architecture to better mimic biology\nand training-based machine learning approaches.\n", "link": "http://arxiv.org/abs/2506.03089v1", "date": "2025-06-03", "relevancy": 2.8251, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5845}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explicitly%20Modeling%20Subcortical%20Vision%20with%20a%20Neuro-Inspired%20Front-End%0A%20%20Improves%20CNN%20Robustness&body=Title%3A%20Explicitly%20Modeling%20Subcortical%20Vision%20with%20a%20Neuro-Inspired%20Front-End%0A%20%20Improves%20CNN%20Robustness%0AAuthor%3A%20Lucas%20Piper%20and%20Arlindo%20L.%20Oliveira%20and%20Tiago%20Marques%0AAbstract%3A%20%20%20Convolutional%20neural%20networks%20%28CNNs%29%20trained%20on%20object%20recognition%20achieve%0Ahigh%20task%20performance%20but%20continue%20to%20exhibit%20vulnerability%20under%20a%20range%20of%0Avisual%20perturbations%20and%20out-of-domain%20images%2C%20when%20compared%20with%20biological%0Avision.%20Prior%20work%20has%20demonstrated%20that%20coupling%20a%20standard%20CNN%20with%20a%0Afront-end%20block%20%28VOneBlock%29%20that%20mimics%20the%20primate%20primary%20visual%20cortex%20%28V1%29%0Acan%20improve%20overall%20model%20robustness.%20Expanding%20on%20this%2C%20we%20introduce%20Early%0AVision%20Networks%20%28EVNets%29%2C%20a%20new%20class%20of%20hybrid%20CNNs%20that%20combine%20the%20VOneBlock%0Awith%20a%20novel%20SubcorticalBlock%2C%20whose%20architecture%20draws%20from%20computational%0Amodels%20in%20neuroscience%20and%20is%20parameterized%20to%20maximize%20alignment%20with%0Asubcortical%20responses%20reported%20across%20multiple%20experimental%20studies.%20Without%0Abeing%20optimized%20to%20do%20so%2C%20the%20assembly%20of%20the%20SubcorticalBlock%20with%20the%0AVOneBlock%20improved%20V1%20alignment%20across%20most%20standard%20V1%20benchmarks%2C%20and%20better%0Amodeled%20extra-classical%20receptive%20field%20phenomena.%20In%20addition%2C%20EVNets%20exhibit%0Astronger%20emergent%20shape%20bias%20and%20overperform%20the%20base%20CNN%20architecture%20by%208.5%25%0Aon%20an%20aggregate%20benchmark%20of%20robustness%20evaluations%2C%20including%20adversarial%0Aperturbations%2C%20common%20corruptions%2C%20and%20domain%20shifts.%20Finally%2C%20we%20show%20that%0AEVNets%20can%20be%20further%20improved%20when%20paired%20with%20a%20state-of-the-art%20data%0Aaugmentation%20technique%2C%20surpassing%20the%20performance%20of%20the%20isolated%20data%0Aaugmentation%20approach%20by%207.3%25%20on%20our%20robustness%20benchmark.%20This%20result%20reveals%0Acomplementary%20benefits%20between%20changes%20in%20architecture%20to%20better%20mimic%20biology%0Aand%20training-based%20machine%20learning%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03089v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplicitly%2520Modeling%2520Subcortical%2520Vision%2520with%2520a%2520Neuro-Inspired%2520Front-End%250A%2520%2520Improves%2520CNN%2520Robustness%26entry.906535625%3DLucas%2520Piper%2520and%2520Arlindo%2520L.%2520Oliveira%2520and%2520Tiago%2520Marques%26entry.1292438233%3D%2520%2520Convolutional%2520neural%2520networks%2520%2528CNNs%2529%2520trained%2520on%2520object%2520recognition%2520achieve%250Ahigh%2520task%2520performance%2520but%2520continue%2520to%2520exhibit%2520vulnerability%2520under%2520a%2520range%2520of%250Avisual%2520perturbations%2520and%2520out-of-domain%2520images%252C%2520when%2520compared%2520with%2520biological%250Avision.%2520Prior%2520work%2520has%2520demonstrated%2520that%2520coupling%2520a%2520standard%2520CNN%2520with%2520a%250Afront-end%2520block%2520%2528VOneBlock%2529%2520that%2520mimics%2520the%2520primate%2520primary%2520visual%2520cortex%2520%2528V1%2529%250Acan%2520improve%2520overall%2520model%2520robustness.%2520Expanding%2520on%2520this%252C%2520we%2520introduce%2520Early%250AVision%2520Networks%2520%2528EVNets%2529%252C%2520a%2520new%2520class%2520of%2520hybrid%2520CNNs%2520that%2520combine%2520the%2520VOneBlock%250Awith%2520a%2520novel%2520SubcorticalBlock%252C%2520whose%2520architecture%2520draws%2520from%2520computational%250Amodels%2520in%2520neuroscience%2520and%2520is%2520parameterized%2520to%2520maximize%2520alignment%2520with%250Asubcortical%2520responses%2520reported%2520across%2520multiple%2520experimental%2520studies.%2520Without%250Abeing%2520optimized%2520to%2520do%2520so%252C%2520the%2520assembly%2520of%2520the%2520SubcorticalBlock%2520with%2520the%250AVOneBlock%2520improved%2520V1%2520alignment%2520across%2520most%2520standard%2520V1%2520benchmarks%252C%2520and%2520better%250Amodeled%2520extra-classical%2520receptive%2520field%2520phenomena.%2520In%2520addition%252C%2520EVNets%2520exhibit%250Astronger%2520emergent%2520shape%2520bias%2520and%2520overperform%2520the%2520base%2520CNN%2520architecture%2520by%25208.5%2525%250Aon%2520an%2520aggregate%2520benchmark%2520of%2520robustness%2520evaluations%252C%2520including%2520adversarial%250Aperturbations%252C%2520common%2520corruptions%252C%2520and%2520domain%2520shifts.%2520Finally%252C%2520we%2520show%2520that%250AEVNets%2520can%2520be%2520further%2520improved%2520when%2520paired%2520with%2520a%2520state-of-the-art%2520data%250Aaugmentation%2520technique%252C%2520surpassing%2520the%2520performance%2520of%2520the%2520isolated%2520data%250Aaugmentation%2520approach%2520by%25207.3%2525%2520on%2520our%2520robustness%2520benchmark.%2520This%2520result%2520reveals%250Acomplementary%2520benefits%2520between%2520changes%2520in%2520architecture%2520to%2520better%2520mimic%2520biology%250Aand%2520training-based%2520machine%2520learning%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03089v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explicitly%20Modeling%20Subcortical%20Vision%20with%20a%20Neuro-Inspired%20Front-End%0A%20%20Improves%20CNN%20Robustness&entry.906535625=Lucas%20Piper%20and%20Arlindo%20L.%20Oliveira%20and%20Tiago%20Marques&entry.1292438233=%20%20Convolutional%20neural%20networks%20%28CNNs%29%20trained%20on%20object%20recognition%20achieve%0Ahigh%20task%20performance%20but%20continue%20to%20exhibit%20vulnerability%20under%20a%20range%20of%0Avisual%20perturbations%20and%20out-of-domain%20images%2C%20when%20compared%20with%20biological%0Avision.%20Prior%20work%20has%20demonstrated%20that%20coupling%20a%20standard%20CNN%20with%20a%0Afront-end%20block%20%28VOneBlock%29%20that%20mimics%20the%20primate%20primary%20visual%20cortex%20%28V1%29%0Acan%20improve%20overall%20model%20robustness.%20Expanding%20on%20this%2C%20we%20introduce%20Early%0AVision%20Networks%20%28EVNets%29%2C%20a%20new%20class%20of%20hybrid%20CNNs%20that%20combine%20the%20VOneBlock%0Awith%20a%20novel%20SubcorticalBlock%2C%20whose%20architecture%20draws%20from%20computational%0Amodels%20in%20neuroscience%20and%20is%20parameterized%20to%20maximize%20alignment%20with%0Asubcortical%20responses%20reported%20across%20multiple%20experimental%20studies.%20Without%0Abeing%20optimized%20to%20do%20so%2C%20the%20assembly%20of%20the%20SubcorticalBlock%20with%20the%0AVOneBlock%20improved%20V1%20alignment%20across%20most%20standard%20V1%20benchmarks%2C%20and%20better%0Amodeled%20extra-classical%20receptive%20field%20phenomena.%20In%20addition%2C%20EVNets%20exhibit%0Astronger%20emergent%20shape%20bias%20and%20overperform%20the%20base%20CNN%20architecture%20by%208.5%25%0Aon%20an%20aggregate%20benchmark%20of%20robustness%20evaluations%2C%20including%20adversarial%0Aperturbations%2C%20common%20corruptions%2C%20and%20domain%20shifts.%20Finally%2C%20we%20show%20that%0AEVNets%20can%20be%20further%20improved%20when%20paired%20with%20a%20state-of-the-art%20data%0Aaugmentation%20technique%2C%20surpassing%20the%20performance%20of%20the%20isolated%20data%0Aaugmentation%20approach%20by%207.3%25%20on%20our%20robustness%20benchmark.%20This%20result%20reveals%0Acomplementary%20benefits%20between%20changes%20in%20architecture%20to%20better%20mimic%20biology%0Aand%20training-based%20machine%20learning%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03089v1&entry.124074799=Read"},
{"title": "InterMamba: Efficient Human-Human Interaction Generation with Adaptive\n  Spatio-Temporal Mamba", "author": "Zizhao Wu and Yingying Sun and Yiming Chen and Xiaoling Gu and Ruyu Liu and Jiazhou Chen", "abstract": "  Human-human interaction generation has garnered significant attention in\nmotion synthesis due to its vital role in understanding humans as social\nbeings. However, existing methods typically rely on transformer-based\narchitectures, which often face challenges related to scalability and\nefficiency. To address these issues, we propose a novel, efficient human-human\ninteraction generation method based on the Mamba framework, designed to meet\nthe demands of effectively capturing long-sequence dependencies while providing\nreal-time feedback. Specifically, we introduce an adaptive spatio-temporal\nMamba framework that utilizes two parallel SSM branches with an adaptive\nmechanism to integrate the spatial and temporal features of motion sequences.\nTo further enhance the model's ability to capture dependencies within\nindividual motion sequences and the interactions between different individual\nsequences, we develop two key modules: the self-adaptive spatio-temporal Mamba\nmodule and the cross-adaptive spatio-temporal Mamba module, enabling efficient\nfeature learning. Extensive experiments demonstrate that our method achieves\nstate-of-the-art results on two interaction datasets with remarkable quality\nand efficiency. Compared to the baseline method InterGen, our approach not only\nimproves accuracy but also requires a minimal parameter size of just 66M ,only\n36% of InterGen's, while achieving an average inference speed of 0.57 seconds,\nwhich is 46% of InterGen's execution time.\n", "link": "http://arxiv.org/abs/2506.03084v1", "date": "2025-06-03", "relevancy": 2.8242, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.609}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5434}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InterMamba%3A%20Efficient%20Human-Human%20Interaction%20Generation%20with%20Adaptive%0A%20%20Spatio-Temporal%20Mamba&body=Title%3A%20InterMamba%3A%20Efficient%20Human-Human%20Interaction%20Generation%20with%20Adaptive%0A%20%20Spatio-Temporal%20Mamba%0AAuthor%3A%20Zizhao%20Wu%20and%20Yingying%20Sun%20and%20Yiming%20Chen%20and%20Xiaoling%20Gu%20and%20Ruyu%20Liu%20and%20Jiazhou%20Chen%0AAbstract%3A%20%20%20Human-human%20interaction%20generation%20has%20garnered%20significant%20attention%20in%0Amotion%20synthesis%20due%20to%20its%20vital%20role%20in%20understanding%20humans%20as%20social%0Abeings.%20However%2C%20existing%20methods%20typically%20rely%20on%20transformer-based%0Aarchitectures%2C%20which%20often%20face%20challenges%20related%20to%20scalability%20and%0Aefficiency.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%2C%20efficient%20human-human%0Ainteraction%20generation%20method%20based%20on%20the%20Mamba%20framework%2C%20designed%20to%20meet%0Athe%20demands%20of%20effectively%20capturing%20long-sequence%20dependencies%20while%20providing%0Areal-time%20feedback.%20Specifically%2C%20we%20introduce%20an%20adaptive%20spatio-temporal%0AMamba%20framework%20that%20utilizes%20two%20parallel%20SSM%20branches%20with%20an%20adaptive%0Amechanism%20to%20integrate%20the%20spatial%20and%20temporal%20features%20of%20motion%20sequences.%0ATo%20further%20enhance%20the%20model%27s%20ability%20to%20capture%20dependencies%20within%0Aindividual%20motion%20sequences%20and%20the%20interactions%20between%20different%20individual%0Asequences%2C%20we%20develop%20two%20key%20modules%3A%20the%20self-adaptive%20spatio-temporal%20Mamba%0Amodule%20and%20the%20cross-adaptive%20spatio-temporal%20Mamba%20module%2C%20enabling%20efficient%0Afeature%20learning.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20results%20on%20two%20interaction%20datasets%20with%20remarkable%20quality%0Aand%20efficiency.%20Compared%20to%20the%20baseline%20method%20InterGen%2C%20our%20approach%20not%20only%0Aimproves%20accuracy%20but%20also%20requires%20a%20minimal%20parameter%20size%20of%20just%2066M%20%2Conly%0A36%25%20of%20InterGen%27s%2C%20while%20achieving%20an%20average%20inference%20speed%20of%200.57%20seconds%2C%0Awhich%20is%2046%25%20of%20InterGen%27s%20execution%20time.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03084v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterMamba%253A%2520Efficient%2520Human-Human%2520Interaction%2520Generation%2520with%2520Adaptive%250A%2520%2520Spatio-Temporal%2520Mamba%26entry.906535625%3DZizhao%2520Wu%2520and%2520Yingying%2520Sun%2520and%2520Yiming%2520Chen%2520and%2520Xiaoling%2520Gu%2520and%2520Ruyu%2520Liu%2520and%2520Jiazhou%2520Chen%26entry.1292438233%3D%2520%2520Human-human%2520interaction%2520generation%2520has%2520garnered%2520significant%2520attention%2520in%250Amotion%2520synthesis%2520due%2520to%2520its%2520vital%2520role%2520in%2520understanding%2520humans%2520as%2520social%250Abeings.%2520However%252C%2520existing%2520methods%2520typically%2520rely%2520on%2520transformer-based%250Aarchitectures%252C%2520which%2520often%2520face%2520challenges%2520related%2520to%2520scalability%2520and%250Aefficiency.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%252C%2520efficient%2520human-human%250Ainteraction%2520generation%2520method%2520based%2520on%2520the%2520Mamba%2520framework%252C%2520designed%2520to%2520meet%250Athe%2520demands%2520of%2520effectively%2520capturing%2520long-sequence%2520dependencies%2520while%2520providing%250Areal-time%2520feedback.%2520Specifically%252C%2520we%2520introduce%2520an%2520adaptive%2520spatio-temporal%250AMamba%2520framework%2520that%2520utilizes%2520two%2520parallel%2520SSM%2520branches%2520with%2520an%2520adaptive%250Amechanism%2520to%2520integrate%2520the%2520spatial%2520and%2520temporal%2520features%2520of%2520motion%2520sequences.%250ATo%2520further%2520enhance%2520the%2520model%2527s%2520ability%2520to%2520capture%2520dependencies%2520within%250Aindividual%2520motion%2520sequences%2520and%2520the%2520interactions%2520between%2520different%2520individual%250Asequences%252C%2520we%2520develop%2520two%2520key%2520modules%253A%2520the%2520self-adaptive%2520spatio-temporal%2520Mamba%250Amodule%2520and%2520the%2520cross-adaptive%2520spatio-temporal%2520Mamba%2520module%252C%2520enabling%2520efficient%250Afeature%2520learning.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520method%2520achieves%250Astate-of-the-art%2520results%2520on%2520two%2520interaction%2520datasets%2520with%2520remarkable%2520quality%250Aand%2520efficiency.%2520Compared%2520to%2520the%2520baseline%2520method%2520InterGen%252C%2520our%2520approach%2520not%2520only%250Aimproves%2520accuracy%2520but%2520also%2520requires%2520a%2520minimal%2520parameter%2520size%2520of%2520just%252066M%2520%252Conly%250A36%2525%2520of%2520InterGen%2527s%252C%2520while%2520achieving%2520an%2520average%2520inference%2520speed%2520of%25200.57%2520seconds%252C%250Awhich%2520is%252046%2525%2520of%2520InterGen%2527s%2520execution%2520time.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03084v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InterMamba%3A%20Efficient%20Human-Human%20Interaction%20Generation%20with%20Adaptive%0A%20%20Spatio-Temporal%20Mamba&entry.906535625=Zizhao%20Wu%20and%20Yingying%20Sun%20and%20Yiming%20Chen%20and%20Xiaoling%20Gu%20and%20Ruyu%20Liu%20and%20Jiazhou%20Chen&entry.1292438233=%20%20Human-human%20interaction%20generation%20has%20garnered%20significant%20attention%20in%0Amotion%20synthesis%20due%20to%20its%20vital%20role%20in%20understanding%20humans%20as%20social%0Abeings.%20However%2C%20existing%20methods%20typically%20rely%20on%20transformer-based%0Aarchitectures%2C%20which%20often%20face%20challenges%20related%20to%20scalability%20and%0Aefficiency.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%2C%20efficient%20human-human%0Ainteraction%20generation%20method%20based%20on%20the%20Mamba%20framework%2C%20designed%20to%20meet%0Athe%20demands%20of%20effectively%20capturing%20long-sequence%20dependencies%20while%20providing%0Areal-time%20feedback.%20Specifically%2C%20we%20introduce%20an%20adaptive%20spatio-temporal%0AMamba%20framework%20that%20utilizes%20two%20parallel%20SSM%20branches%20with%20an%20adaptive%0Amechanism%20to%20integrate%20the%20spatial%20and%20temporal%20features%20of%20motion%20sequences.%0ATo%20further%20enhance%20the%20model%27s%20ability%20to%20capture%20dependencies%20within%0Aindividual%20motion%20sequences%20and%20the%20interactions%20between%20different%20individual%0Asequences%2C%20we%20develop%20two%20key%20modules%3A%20the%20self-adaptive%20spatio-temporal%20Mamba%0Amodule%20and%20the%20cross-adaptive%20spatio-temporal%20Mamba%20module%2C%20enabling%20efficient%0Afeature%20learning.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20achieves%0Astate-of-the-art%20results%20on%20two%20interaction%20datasets%20with%20remarkable%20quality%0Aand%20efficiency.%20Compared%20to%20the%20baseline%20method%20InterGen%2C%20our%20approach%20not%20only%0Aimproves%20accuracy%20but%20also%20requires%20a%20minimal%20parameter%20size%20of%20just%2066M%20%2Conly%0A36%25%20of%20InterGen%27s%2C%20while%20achieving%20an%20average%20inference%20speed%20of%200.57%20seconds%2C%0Awhich%20is%2046%25%20of%20InterGen%27s%20execution%20time.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03084v1&entry.124074799=Read"},
{"title": "Social Genome: Grounded Social Reasoning Abilities of Multimodal Models", "author": "Leena Mathur and Marian Qian and Paul Pu Liang and Louis-Philippe Morency", "abstract": "  Social reasoning abilities are crucial for AI systems to effectively\ninterpret and respond to multimodal human communication and interaction within\nsocial contexts. We introduce SOCIAL GENOME, the first benchmark for\nfine-grained, grounded social reasoning abilities of multimodal models. SOCIAL\nGENOME contains 272 videos of interactions and 1,486 human-annotated reasoning\ntraces related to inferences about these interactions. These traces contain\n5,777 reasoning steps that reference evidence from visual cues, verbal cues,\nvocal cues, and external knowledge (contextual knowledge external to videos).\nSOCIAL GENOME is also the first modeling challenge to study external knowledge\nin social reasoning. SOCIAL GENOME computes metrics to holistically evaluate\nsemantic and structural qualities of model-generated social reasoning traces.\nWe demonstrate the utility of SOCIAL GENOME through experiments with\nstate-of-the-art models, identifying performance gaps and opportunities for\nfuture research to improve the grounded social reasoning abilities of\nmultimodal models.\n", "link": "http://arxiv.org/abs/2502.15109v3", "date": "2025-06-03", "relevancy": 2.8177, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5666}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5666}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Social%20Genome%3A%20Grounded%20Social%20Reasoning%20Abilities%20of%20Multimodal%20Models&body=Title%3A%20Social%20Genome%3A%20Grounded%20Social%20Reasoning%20Abilities%20of%20Multimodal%20Models%0AAuthor%3A%20Leena%20Mathur%20and%20Marian%20Qian%20and%20Paul%20Pu%20Liang%20and%20Louis-Philippe%20Morency%0AAbstract%3A%20%20%20Social%20reasoning%20abilities%20are%20crucial%20for%20AI%20systems%20to%20effectively%0Ainterpret%20and%20respond%20to%20multimodal%20human%20communication%20and%20interaction%20within%0Asocial%20contexts.%20We%20introduce%20SOCIAL%20GENOME%2C%20the%20first%20benchmark%20for%0Afine-grained%2C%20grounded%20social%20reasoning%20abilities%20of%20multimodal%20models.%20SOCIAL%0AGENOME%20contains%20272%20videos%20of%20interactions%20and%201%2C486%20human-annotated%20reasoning%0Atraces%20related%20to%20inferences%20about%20these%20interactions.%20These%20traces%20contain%0A5%2C777%20reasoning%20steps%20that%20reference%20evidence%20from%20visual%20cues%2C%20verbal%20cues%2C%0Avocal%20cues%2C%20and%20external%20knowledge%20%28contextual%20knowledge%20external%20to%20videos%29.%0ASOCIAL%20GENOME%20is%20also%20the%20first%20modeling%20challenge%20to%20study%20external%20knowledge%0Ain%20social%20reasoning.%20SOCIAL%20GENOME%20computes%20metrics%20to%20holistically%20evaluate%0Asemantic%20and%20structural%20qualities%20of%20model-generated%20social%20reasoning%20traces.%0AWe%20demonstrate%20the%20utility%20of%20SOCIAL%20GENOME%20through%20experiments%20with%0Astate-of-the-art%20models%2C%20identifying%20performance%20gaps%20and%20opportunities%20for%0Afuture%20research%20to%20improve%20the%20grounded%20social%20reasoning%20abilities%20of%0Amultimodal%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.15109v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSocial%2520Genome%253A%2520Grounded%2520Social%2520Reasoning%2520Abilities%2520of%2520Multimodal%2520Models%26entry.906535625%3DLeena%2520Mathur%2520and%2520Marian%2520Qian%2520and%2520Paul%2520Pu%2520Liang%2520and%2520Louis-Philippe%2520Morency%26entry.1292438233%3D%2520%2520Social%2520reasoning%2520abilities%2520are%2520crucial%2520for%2520AI%2520systems%2520to%2520effectively%250Ainterpret%2520and%2520respond%2520to%2520multimodal%2520human%2520communication%2520and%2520interaction%2520within%250Asocial%2520contexts.%2520We%2520introduce%2520SOCIAL%2520GENOME%252C%2520the%2520first%2520benchmark%2520for%250Afine-grained%252C%2520grounded%2520social%2520reasoning%2520abilities%2520of%2520multimodal%2520models.%2520SOCIAL%250AGENOME%2520contains%2520272%2520videos%2520of%2520interactions%2520and%25201%252C486%2520human-annotated%2520reasoning%250Atraces%2520related%2520to%2520inferences%2520about%2520these%2520interactions.%2520These%2520traces%2520contain%250A5%252C777%2520reasoning%2520steps%2520that%2520reference%2520evidence%2520from%2520visual%2520cues%252C%2520verbal%2520cues%252C%250Avocal%2520cues%252C%2520and%2520external%2520knowledge%2520%2528contextual%2520knowledge%2520external%2520to%2520videos%2529.%250ASOCIAL%2520GENOME%2520is%2520also%2520the%2520first%2520modeling%2520challenge%2520to%2520study%2520external%2520knowledge%250Ain%2520social%2520reasoning.%2520SOCIAL%2520GENOME%2520computes%2520metrics%2520to%2520holistically%2520evaluate%250Asemantic%2520and%2520structural%2520qualities%2520of%2520model-generated%2520social%2520reasoning%2520traces.%250AWe%2520demonstrate%2520the%2520utility%2520of%2520SOCIAL%2520GENOME%2520through%2520experiments%2520with%250Astate-of-the-art%2520models%252C%2520identifying%2520performance%2520gaps%2520and%2520opportunities%2520for%250Afuture%2520research%2520to%2520improve%2520the%2520grounded%2520social%2520reasoning%2520abilities%2520of%250Amultimodal%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.15109v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Social%20Genome%3A%20Grounded%20Social%20Reasoning%20Abilities%20of%20Multimodal%20Models&entry.906535625=Leena%20Mathur%20and%20Marian%20Qian%20and%20Paul%20Pu%20Liang%20and%20Louis-Philippe%20Morency&entry.1292438233=%20%20Social%20reasoning%20abilities%20are%20crucial%20for%20AI%20systems%20to%20effectively%0Ainterpret%20and%20respond%20to%20multimodal%20human%20communication%20and%20interaction%20within%0Asocial%20contexts.%20We%20introduce%20SOCIAL%20GENOME%2C%20the%20first%20benchmark%20for%0Afine-grained%2C%20grounded%20social%20reasoning%20abilities%20of%20multimodal%20models.%20SOCIAL%0AGENOME%20contains%20272%20videos%20of%20interactions%20and%201%2C486%20human-annotated%20reasoning%0Atraces%20related%20to%20inferences%20about%20these%20interactions.%20These%20traces%20contain%0A5%2C777%20reasoning%20steps%20that%20reference%20evidence%20from%20visual%20cues%2C%20verbal%20cues%2C%0Avocal%20cues%2C%20and%20external%20knowledge%20%28contextual%20knowledge%20external%20to%20videos%29.%0ASOCIAL%20GENOME%20is%20also%20the%20first%20modeling%20challenge%20to%20study%20external%20knowledge%0Ain%20social%20reasoning.%20SOCIAL%20GENOME%20computes%20metrics%20to%20holistically%20evaluate%0Asemantic%20and%20structural%20qualities%20of%20model-generated%20social%20reasoning%20traces.%0AWe%20demonstrate%20the%20utility%20of%20SOCIAL%20GENOME%20through%20experiments%20with%0Astate-of-the-art%20models%2C%20identifying%20performance%20gaps%20and%20opportunities%20for%0Afuture%20research%20to%20improve%20the%20grounded%20social%20reasoning%20abilities%20of%0Amultimodal%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.15109v3&entry.124074799=Read"},
{"title": "EgoVLM: Policy Optimization for Egocentric Video Understanding", "author": "Ashwin Vinod and Shrey Pandit and Aditya Vavre and Linshen Liu", "abstract": "  Emerging embodied AI applications, such as wearable cameras and autonomous\nagents, have underscored the need for robust reasoning from first person video\nstreams. We introduce EgoVLM, a vision-language model specifically designed to\nintegrate visual comprehension and spatial-temporal reasoning within egocentric\nvideo contexts. EgoVLM is fine-tuned via Group Relative Policy Optimization\n(GRPO), a reinforcement learning method adapted to align model outputs with\nhuman-like reasoning steps. Following DeepSeek R1-Zero's approach, we directly\ntune using RL without any supervised fine-tuning phase on chain-of-thought\n(CoT) data. We evaluate EgoVLM on egocentric video question answering\nbenchmarks and show that domain-specific training substantially improves\nperformance over general-purpose VLMs. Our EgoVLM-3B, trained exclusively on\nnon-CoT egocentric data, outperforms the base Qwen2.5-VL 3B and 7B models by\n14.33 and 13.87 accuracy points on the EgoSchema benchmark, respectively. By\nexplicitly generating reasoning traces, EgoVLM enhances interpretability,\nmaking it well-suited for downstream applications. Furthermore, we introduce a\nnovel keyframe-based reward that incorporates salient frame selection to guide\nreinforcement learning optimization. This reward formulation opens a promising\navenue for future exploration in temporally grounded egocentric reasoning.\n", "link": "http://arxiv.org/abs/2506.03097v1", "date": "2025-06-03", "relevancy": 2.8041, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5685}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5685}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EgoVLM%3A%20Policy%20Optimization%20for%20Egocentric%20Video%20Understanding&body=Title%3A%20EgoVLM%3A%20Policy%20Optimization%20for%20Egocentric%20Video%20Understanding%0AAuthor%3A%20Ashwin%20Vinod%20and%20Shrey%20Pandit%20and%20Aditya%20Vavre%20and%20Linshen%20Liu%0AAbstract%3A%20%20%20Emerging%20embodied%20AI%20applications%2C%20such%20as%20wearable%20cameras%20and%20autonomous%0Aagents%2C%20have%20underscored%20the%20need%20for%20robust%20reasoning%20from%20first%20person%20video%0Astreams.%20We%20introduce%20EgoVLM%2C%20a%20vision-language%20model%20specifically%20designed%20to%0Aintegrate%20visual%20comprehension%20and%20spatial-temporal%20reasoning%20within%20egocentric%0Avideo%20contexts.%20EgoVLM%20is%20fine-tuned%20via%20Group%20Relative%20Policy%20Optimization%0A%28GRPO%29%2C%20a%20reinforcement%20learning%20method%20adapted%20to%20align%20model%20outputs%20with%0Ahuman-like%20reasoning%20steps.%20Following%20DeepSeek%20R1-Zero%27s%20approach%2C%20we%20directly%0Atune%20using%20RL%20without%20any%20supervised%20fine-tuning%20phase%20on%20chain-of-thought%0A%28CoT%29%20data.%20We%20evaluate%20EgoVLM%20on%20egocentric%20video%20question%20answering%0Abenchmarks%20and%20show%20that%20domain-specific%20training%20substantially%20improves%0Aperformance%20over%20general-purpose%20VLMs.%20Our%20EgoVLM-3B%2C%20trained%20exclusively%20on%0Anon-CoT%20egocentric%20data%2C%20outperforms%20the%20base%20Qwen2.5-VL%203B%20and%207B%20models%20by%0A14.33%20and%2013.87%20accuracy%20points%20on%20the%20EgoSchema%20benchmark%2C%20respectively.%20By%0Aexplicitly%20generating%20reasoning%20traces%2C%20EgoVLM%20enhances%20interpretability%2C%0Amaking%20it%20well-suited%20for%20downstream%20applications.%20Furthermore%2C%20we%20introduce%20a%0Anovel%20keyframe-based%20reward%20that%20incorporates%20salient%20frame%20selection%20to%20guide%0Areinforcement%20learning%20optimization.%20This%20reward%20formulation%20opens%20a%20promising%0Aavenue%20for%20future%20exploration%20in%20temporally%20grounded%20egocentric%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03097v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEgoVLM%253A%2520Policy%2520Optimization%2520for%2520Egocentric%2520Video%2520Understanding%26entry.906535625%3DAshwin%2520Vinod%2520and%2520Shrey%2520Pandit%2520and%2520Aditya%2520Vavre%2520and%2520Linshen%2520Liu%26entry.1292438233%3D%2520%2520Emerging%2520embodied%2520AI%2520applications%252C%2520such%2520as%2520wearable%2520cameras%2520and%2520autonomous%250Aagents%252C%2520have%2520underscored%2520the%2520need%2520for%2520robust%2520reasoning%2520from%2520first%2520person%2520video%250Astreams.%2520We%2520introduce%2520EgoVLM%252C%2520a%2520vision-language%2520model%2520specifically%2520designed%2520to%250Aintegrate%2520visual%2520comprehension%2520and%2520spatial-temporal%2520reasoning%2520within%2520egocentric%250Avideo%2520contexts.%2520EgoVLM%2520is%2520fine-tuned%2520via%2520Group%2520Relative%2520Policy%2520Optimization%250A%2528GRPO%2529%252C%2520a%2520reinforcement%2520learning%2520method%2520adapted%2520to%2520align%2520model%2520outputs%2520with%250Ahuman-like%2520reasoning%2520steps.%2520Following%2520DeepSeek%2520R1-Zero%2527s%2520approach%252C%2520we%2520directly%250Atune%2520using%2520RL%2520without%2520any%2520supervised%2520fine-tuning%2520phase%2520on%2520chain-of-thought%250A%2528CoT%2529%2520data.%2520We%2520evaluate%2520EgoVLM%2520on%2520egocentric%2520video%2520question%2520answering%250Abenchmarks%2520and%2520show%2520that%2520domain-specific%2520training%2520substantially%2520improves%250Aperformance%2520over%2520general-purpose%2520VLMs.%2520Our%2520EgoVLM-3B%252C%2520trained%2520exclusively%2520on%250Anon-CoT%2520egocentric%2520data%252C%2520outperforms%2520the%2520base%2520Qwen2.5-VL%25203B%2520and%25207B%2520models%2520by%250A14.33%2520and%252013.87%2520accuracy%2520points%2520on%2520the%2520EgoSchema%2520benchmark%252C%2520respectively.%2520By%250Aexplicitly%2520generating%2520reasoning%2520traces%252C%2520EgoVLM%2520enhances%2520interpretability%252C%250Amaking%2520it%2520well-suited%2520for%2520downstream%2520applications.%2520Furthermore%252C%2520we%2520introduce%2520a%250Anovel%2520keyframe-based%2520reward%2520that%2520incorporates%2520salient%2520frame%2520selection%2520to%2520guide%250Areinforcement%2520learning%2520optimization.%2520This%2520reward%2520formulation%2520opens%2520a%2520promising%250Aavenue%2520for%2520future%2520exploration%2520in%2520temporally%2520grounded%2520egocentric%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03097v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EgoVLM%3A%20Policy%20Optimization%20for%20Egocentric%20Video%20Understanding&entry.906535625=Ashwin%20Vinod%20and%20Shrey%20Pandit%20and%20Aditya%20Vavre%20and%20Linshen%20Liu&entry.1292438233=%20%20Emerging%20embodied%20AI%20applications%2C%20such%20as%20wearable%20cameras%20and%20autonomous%0Aagents%2C%20have%20underscored%20the%20need%20for%20robust%20reasoning%20from%20first%20person%20video%0Astreams.%20We%20introduce%20EgoVLM%2C%20a%20vision-language%20model%20specifically%20designed%20to%0Aintegrate%20visual%20comprehension%20and%20spatial-temporal%20reasoning%20within%20egocentric%0Avideo%20contexts.%20EgoVLM%20is%20fine-tuned%20via%20Group%20Relative%20Policy%20Optimization%0A%28GRPO%29%2C%20a%20reinforcement%20learning%20method%20adapted%20to%20align%20model%20outputs%20with%0Ahuman-like%20reasoning%20steps.%20Following%20DeepSeek%20R1-Zero%27s%20approach%2C%20we%20directly%0Atune%20using%20RL%20without%20any%20supervised%20fine-tuning%20phase%20on%20chain-of-thought%0A%28CoT%29%20data.%20We%20evaluate%20EgoVLM%20on%20egocentric%20video%20question%20answering%0Abenchmarks%20and%20show%20that%20domain-specific%20training%20substantially%20improves%0Aperformance%20over%20general-purpose%20VLMs.%20Our%20EgoVLM-3B%2C%20trained%20exclusively%20on%0Anon-CoT%20egocentric%20data%2C%20outperforms%20the%20base%20Qwen2.5-VL%203B%20and%207B%20models%20by%0A14.33%20and%2013.87%20accuracy%20points%20on%20the%20EgoSchema%20benchmark%2C%20respectively.%20By%0Aexplicitly%20generating%20reasoning%20traces%2C%20EgoVLM%20enhances%20interpretability%2C%0Amaking%20it%20well-suited%20for%20downstream%20applications.%20Furthermore%2C%20we%20introduce%20a%0Anovel%20keyframe-based%20reward%20that%20incorporates%20salient%20frame%20selection%20to%20guide%0Areinforcement%20learning%20optimization.%20This%20reward%20formulation%20opens%20a%20promising%0Aavenue%20for%20future%20exploration%20in%20temporally%20grounded%20egocentric%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03097v1&entry.124074799=Read"},
{"title": "SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation", "author": "Siqi Chen and Xinyu Dong and Haolei Xu and Xingyu Wu and Fei Tang and Hang Zhang and Yuchen Yan and Linjuan Wu and Wenqi Zhang and Guiyang Hou and Yongliang Shen and Weiming Lu and Yueting Zhuang", "abstract": "  Large Language Models (LLMs) and Multimodal LLMs have shown promising\ncapabilities for SVG processing, yet existing benchmarks suffer from limited\nreal-world coverage, lack of complexity stratification, and fragmented\nevaluation paradigms. We introduce SVGenius, a comprehensive benchmark\ncomprising 2,377 queries across three progressive dimensions: understanding,\nediting, and generation. Built on real-world data from 24 application domains\nwith systematic complexity stratification, SVGenius evaluates models through 8\ntask categories and 18 metrics. We assess 22 mainstream models spanning\ndifferent scales, architectures, training paradigms, and accessibility levels.\nOur analysis reveals that while proprietary models significantly outperform\nopen-source counterparts, all models exhibit systematic performance degradation\nwith increasing complexity, indicating fundamental limitations in current\napproaches; however, reasoning-enhanced training proves more effective than\npure scaling for overcoming these limitations, though style transfer remains\nthe most challenging capability across all model types. SVGenius establishes\nthe first systematic evaluation framework for SVG processing, providing crucial\ninsights for developing more capable vector graphics models and advancing\nautomated graphic design applications. Appendix and supplementary materials\n(including all data and code) are available at\nhttps://zju-real.github.io/SVGenius.\n", "link": "http://arxiv.org/abs/2506.03139v1", "date": "2025-06-03", "relevancy": 2.7817, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.574}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.574}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5211}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SVGenius%3A%20Benchmarking%20LLMs%20in%20SVG%20Understanding%2C%20Editing%20and%20Generation&body=Title%3A%20SVGenius%3A%20Benchmarking%20LLMs%20in%20SVG%20Understanding%2C%20Editing%20and%20Generation%0AAuthor%3A%20Siqi%20Chen%20and%20Xinyu%20Dong%20and%20Haolei%20Xu%20and%20Xingyu%20Wu%20and%20Fei%20Tang%20and%20Hang%20Zhang%20and%20Yuchen%20Yan%20and%20Linjuan%20Wu%20and%20Wenqi%20Zhang%20and%20Guiyang%20Hou%20and%20Yongliang%20Shen%20and%20Weiming%20Lu%20and%20Yueting%20Zhuang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20and%20Multimodal%20LLMs%20have%20shown%20promising%0Acapabilities%20for%20SVG%20processing%2C%20yet%20existing%20benchmarks%20suffer%20from%20limited%0Areal-world%20coverage%2C%20lack%20of%20complexity%20stratification%2C%20and%20fragmented%0Aevaluation%20paradigms.%20We%20introduce%20SVGenius%2C%20a%20comprehensive%20benchmark%0Acomprising%202%2C377%20queries%20across%20three%20progressive%20dimensions%3A%20understanding%2C%0Aediting%2C%20and%20generation.%20Built%20on%20real-world%20data%20from%2024%20application%20domains%0Awith%20systematic%20complexity%20stratification%2C%20SVGenius%20evaluates%20models%20through%208%0Atask%20categories%20and%2018%20metrics.%20We%20assess%2022%20mainstream%20models%20spanning%0Adifferent%20scales%2C%20architectures%2C%20training%20paradigms%2C%20and%20accessibility%20levels.%0AOur%20analysis%20reveals%20that%20while%20proprietary%20models%20significantly%20outperform%0Aopen-source%20counterparts%2C%20all%20models%20exhibit%20systematic%20performance%20degradation%0Awith%20increasing%20complexity%2C%20indicating%20fundamental%20limitations%20in%20current%0Aapproaches%3B%20however%2C%20reasoning-enhanced%20training%20proves%20more%20effective%20than%0Apure%20scaling%20for%20overcoming%20these%20limitations%2C%20though%20style%20transfer%20remains%0Athe%20most%20challenging%20capability%20across%20all%20model%20types.%20SVGenius%20establishes%0Athe%20first%20systematic%20evaluation%20framework%20for%20SVG%20processing%2C%20providing%20crucial%0Ainsights%20for%20developing%20more%20capable%20vector%20graphics%20models%20and%20advancing%0Aautomated%20graphic%20design%20applications.%20Appendix%20and%20supplementary%20materials%0A%28including%20all%20data%20and%20code%29%20are%20available%20at%0Ahttps%3A//zju-real.github.io/SVGenius.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03139v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSVGenius%253A%2520Benchmarking%2520LLMs%2520in%2520SVG%2520Understanding%252C%2520Editing%2520and%2520Generation%26entry.906535625%3DSiqi%2520Chen%2520and%2520Xinyu%2520Dong%2520and%2520Haolei%2520Xu%2520and%2520Xingyu%2520Wu%2520and%2520Fei%2520Tang%2520and%2520Hang%2520Zhang%2520and%2520Yuchen%2520Yan%2520and%2520Linjuan%2520Wu%2520and%2520Wenqi%2520Zhang%2520and%2520Guiyang%2520Hou%2520and%2520Yongliang%2520Shen%2520and%2520Weiming%2520Lu%2520and%2520Yueting%2520Zhuang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520Multimodal%2520LLMs%2520have%2520shown%2520promising%250Acapabilities%2520for%2520SVG%2520processing%252C%2520yet%2520existing%2520benchmarks%2520suffer%2520from%2520limited%250Areal-world%2520coverage%252C%2520lack%2520of%2520complexity%2520stratification%252C%2520and%2520fragmented%250Aevaluation%2520paradigms.%2520We%2520introduce%2520SVGenius%252C%2520a%2520comprehensive%2520benchmark%250Acomprising%25202%252C377%2520queries%2520across%2520three%2520progressive%2520dimensions%253A%2520understanding%252C%250Aediting%252C%2520and%2520generation.%2520Built%2520on%2520real-world%2520data%2520from%252024%2520application%2520domains%250Awith%2520systematic%2520complexity%2520stratification%252C%2520SVGenius%2520evaluates%2520models%2520through%25208%250Atask%2520categories%2520and%252018%2520metrics.%2520We%2520assess%252022%2520mainstream%2520models%2520spanning%250Adifferent%2520scales%252C%2520architectures%252C%2520training%2520paradigms%252C%2520and%2520accessibility%2520levels.%250AOur%2520analysis%2520reveals%2520that%2520while%2520proprietary%2520models%2520significantly%2520outperform%250Aopen-source%2520counterparts%252C%2520all%2520models%2520exhibit%2520systematic%2520performance%2520degradation%250Awith%2520increasing%2520complexity%252C%2520indicating%2520fundamental%2520limitations%2520in%2520current%250Aapproaches%253B%2520however%252C%2520reasoning-enhanced%2520training%2520proves%2520more%2520effective%2520than%250Apure%2520scaling%2520for%2520overcoming%2520these%2520limitations%252C%2520though%2520style%2520transfer%2520remains%250Athe%2520most%2520challenging%2520capability%2520across%2520all%2520model%2520types.%2520SVGenius%2520establishes%250Athe%2520first%2520systematic%2520evaluation%2520framework%2520for%2520SVG%2520processing%252C%2520providing%2520crucial%250Ainsights%2520for%2520developing%2520more%2520capable%2520vector%2520graphics%2520models%2520and%2520advancing%250Aautomated%2520graphic%2520design%2520applications.%2520Appendix%2520and%2520supplementary%2520materials%250A%2528including%2520all%2520data%2520and%2520code%2529%2520are%2520available%2520at%250Ahttps%253A//zju-real.github.io/SVGenius.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03139v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SVGenius%3A%20Benchmarking%20LLMs%20in%20SVG%20Understanding%2C%20Editing%20and%20Generation&entry.906535625=Siqi%20Chen%20and%20Xinyu%20Dong%20and%20Haolei%20Xu%20and%20Xingyu%20Wu%20and%20Fei%20Tang%20and%20Hang%20Zhang%20and%20Yuchen%20Yan%20and%20Linjuan%20Wu%20and%20Wenqi%20Zhang%20and%20Guiyang%20Hou%20and%20Yongliang%20Shen%20and%20Weiming%20Lu%20and%20Yueting%20Zhuang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20and%20Multimodal%20LLMs%20have%20shown%20promising%0Acapabilities%20for%20SVG%20processing%2C%20yet%20existing%20benchmarks%20suffer%20from%20limited%0Areal-world%20coverage%2C%20lack%20of%20complexity%20stratification%2C%20and%20fragmented%0Aevaluation%20paradigms.%20We%20introduce%20SVGenius%2C%20a%20comprehensive%20benchmark%0Acomprising%202%2C377%20queries%20across%20three%20progressive%20dimensions%3A%20understanding%2C%0Aediting%2C%20and%20generation.%20Built%20on%20real-world%20data%20from%2024%20application%20domains%0Awith%20systematic%20complexity%20stratification%2C%20SVGenius%20evaluates%20models%20through%208%0Atask%20categories%20and%2018%20metrics.%20We%20assess%2022%20mainstream%20models%20spanning%0Adifferent%20scales%2C%20architectures%2C%20training%20paradigms%2C%20and%20accessibility%20levels.%0AOur%20analysis%20reveals%20that%20while%20proprietary%20models%20significantly%20outperform%0Aopen-source%20counterparts%2C%20all%20models%20exhibit%20systematic%20performance%20degradation%0Awith%20increasing%20complexity%2C%20indicating%20fundamental%20limitations%20in%20current%0Aapproaches%3B%20however%2C%20reasoning-enhanced%20training%20proves%20more%20effective%20than%0Apure%20scaling%20for%20overcoming%20these%20limitations%2C%20though%20style%20transfer%20remains%0Athe%20most%20challenging%20capability%20across%20all%20model%20types.%20SVGenius%20establishes%0Athe%20first%20systematic%20evaluation%20framework%20for%20SVG%20processing%2C%20providing%20crucial%0Ainsights%20for%20developing%20more%20capable%20vector%20graphics%20models%20and%20advancing%0Aautomated%20graphic%20design%20applications.%20Appendix%20and%20supplementary%20materials%0A%28including%20all%20data%20and%20code%29%20are%20available%20at%0Ahttps%3A//zju-real.github.io/SVGenius.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03139v1&entry.124074799=Read"},
{"title": "MultiDLO: Simultaneous Shape Tracking of Multiple Deformable Linear\n  Objects with Global-Local Topology Preservation", "author": "Jingyi Xiang and Holly Dinkel", "abstract": "  MultiDLO is a real-time algorithm for estimating the shapes of multiple,\nintertwining deformable linear objects (DLOs) from RGB-D image sequences.\nUnlike prior methods that track only a single DLO, MultiDLO simultaneously\nhandles several objects. It uses the geodesic distance in the Global-Local\nTopology Preservation algorithm to define both inter-object identity and\nintra-object topology, ensuring entangled DLOs remain distinct with accurate\nlocal geometry. The MultiDLO algorithm is demonstrated on two challenging\nscenarios involving three entangling ropes, and the implementation is\nopen-source and available for the community.\n", "link": "http://arxiv.org/abs/2310.13245v3", "date": "2025-06-03", "relevancy": 2.7793, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5739}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5574}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MultiDLO%3A%20Simultaneous%20Shape%20Tracking%20of%20Multiple%20Deformable%20Linear%0A%20%20Objects%20with%20Global-Local%20Topology%20Preservation&body=Title%3A%20MultiDLO%3A%20Simultaneous%20Shape%20Tracking%20of%20Multiple%20Deformable%20Linear%0A%20%20Objects%20with%20Global-Local%20Topology%20Preservation%0AAuthor%3A%20Jingyi%20Xiang%20and%20Holly%20Dinkel%0AAbstract%3A%20%20%20MultiDLO%20is%20a%20real-time%20algorithm%20for%20estimating%20the%20shapes%20of%20multiple%2C%0Aintertwining%20deformable%20linear%20objects%20%28DLOs%29%20from%20RGB-D%20image%20sequences.%0AUnlike%20prior%20methods%20that%20track%20only%20a%20single%20DLO%2C%20MultiDLO%20simultaneously%0Ahandles%20several%20objects.%20It%20uses%20the%20geodesic%20distance%20in%20the%20Global-Local%0ATopology%20Preservation%20algorithm%20to%20define%20both%20inter-object%20identity%20and%0Aintra-object%20topology%2C%20ensuring%20entangled%20DLOs%20remain%20distinct%20with%20accurate%0Alocal%20geometry.%20The%20MultiDLO%20algorithm%20is%20demonstrated%20on%20two%20challenging%0Ascenarios%20involving%20three%20entangling%20ropes%2C%20and%20the%20implementation%20is%0Aopen-source%20and%20available%20for%20the%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.13245v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultiDLO%253A%2520Simultaneous%2520Shape%2520Tracking%2520of%2520Multiple%2520Deformable%2520Linear%250A%2520%2520Objects%2520with%2520Global-Local%2520Topology%2520Preservation%26entry.906535625%3DJingyi%2520Xiang%2520and%2520Holly%2520Dinkel%26entry.1292438233%3D%2520%2520MultiDLO%2520is%2520a%2520real-time%2520algorithm%2520for%2520estimating%2520the%2520shapes%2520of%2520multiple%252C%250Aintertwining%2520deformable%2520linear%2520objects%2520%2528DLOs%2529%2520from%2520RGB-D%2520image%2520sequences.%250AUnlike%2520prior%2520methods%2520that%2520track%2520only%2520a%2520single%2520DLO%252C%2520MultiDLO%2520simultaneously%250Ahandles%2520several%2520objects.%2520It%2520uses%2520the%2520geodesic%2520distance%2520in%2520the%2520Global-Local%250ATopology%2520Preservation%2520algorithm%2520to%2520define%2520both%2520inter-object%2520identity%2520and%250Aintra-object%2520topology%252C%2520ensuring%2520entangled%2520DLOs%2520remain%2520distinct%2520with%2520accurate%250Alocal%2520geometry.%2520The%2520MultiDLO%2520algorithm%2520is%2520demonstrated%2520on%2520two%2520challenging%250Ascenarios%2520involving%2520three%2520entangling%2520ropes%252C%2520and%2520the%2520implementation%2520is%250Aopen-source%2520and%2520available%2520for%2520the%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.13245v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MultiDLO%3A%20Simultaneous%20Shape%20Tracking%20of%20Multiple%20Deformable%20Linear%0A%20%20Objects%20with%20Global-Local%20Topology%20Preservation&entry.906535625=Jingyi%20Xiang%20and%20Holly%20Dinkel&entry.1292438233=%20%20MultiDLO%20is%20a%20real-time%20algorithm%20for%20estimating%20the%20shapes%20of%20multiple%2C%0Aintertwining%20deformable%20linear%20objects%20%28DLOs%29%20from%20RGB-D%20image%20sequences.%0AUnlike%20prior%20methods%20that%20track%20only%20a%20single%20DLO%2C%20MultiDLO%20simultaneously%0Ahandles%20several%20objects.%20It%20uses%20the%20geodesic%20distance%20in%20the%20Global-Local%0ATopology%20Preservation%20algorithm%20to%20define%20both%20inter-object%20identity%20and%0Aintra-object%20topology%2C%20ensuring%20entangled%20DLOs%20remain%20distinct%20with%20accurate%0Alocal%20geometry.%20The%20MultiDLO%20algorithm%20is%20demonstrated%20on%20two%20challenging%0Ascenarios%20involving%20three%20entangling%20ropes%2C%20and%20the%20implementation%20is%0Aopen-source%20and%20available%20for%20the%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.13245v3&entry.124074799=Read"},
{"title": "METok: Multi-Stage Event-based Token Compression for Efficient Long\n  Video Understanding", "author": "Mengyue Wang and Shuo Chen and Kristian Kersting and Volker Tresp and Yunpu Ma", "abstract": "  Recent advances in Video Large Language Models (VLLMs) have significantly\nenhanced their ability to understand video content. Nonetheless, processing\nlong videos remains challenging due to high computational demands and the\nredundancy present in the visual data. In this work, we propose METok, a\ntraining-free, Multi-stage Event-based Token compression framework designed to\naccelerate VLLMs' inference while preserving accuracy. METok progressively\neliminates redundant visual tokens across three critical stages: (1)\nevent-aware compression during vision encoding, (2) hierarchical token pruning\nin the prefilling stage based on semantic alignment and event importance, and\n(3) a decoding-stage KV Cache optimization that further reduces memory\nconsumption. Our experiments on diverse video benchmarks demonstrate that METok\nachieves an optimal trade-off between efficiency and accuracy by dynamically\nselecting informative visual tokens. For instance, equipping LongVA-7B with\nMETok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all\nwhile maintaining comparable or even superior accuracy.\n", "link": "http://arxiv.org/abs/2506.02850v1", "date": "2025-06-03", "relevancy": 2.761, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5596}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5485}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20METok%3A%20Multi-Stage%20Event-based%20Token%20Compression%20for%20Efficient%20Long%0A%20%20Video%20Understanding&body=Title%3A%20METok%3A%20Multi-Stage%20Event-based%20Token%20Compression%20for%20Efficient%20Long%0A%20%20Video%20Understanding%0AAuthor%3A%20Mengyue%20Wang%20and%20Shuo%20Chen%20and%20Kristian%20Kersting%20and%20Volker%20Tresp%20and%20Yunpu%20Ma%0AAbstract%3A%20%20%20Recent%20advances%20in%20Video%20Large%20Language%20Models%20%28VLLMs%29%20have%20significantly%0Aenhanced%20their%20ability%20to%20understand%20video%20content.%20Nonetheless%2C%20processing%0Along%20videos%20remains%20challenging%20due%20to%20high%20computational%20demands%20and%20the%0Aredundancy%20present%20in%20the%20visual%20data.%20In%20this%20work%2C%20we%20propose%20METok%2C%20a%0Atraining-free%2C%20Multi-stage%20Event-based%20Token%20compression%20framework%20designed%20to%0Aaccelerate%20VLLMs%27%20inference%20while%20preserving%20accuracy.%20METok%20progressively%0Aeliminates%20redundant%20visual%20tokens%20across%20three%20critical%20stages%3A%20%281%29%0Aevent-aware%20compression%20during%20vision%20encoding%2C%20%282%29%20hierarchical%20token%20pruning%0Ain%20the%20prefilling%20stage%20based%20on%20semantic%20alignment%20and%20event%20importance%2C%20and%0A%283%29%20a%20decoding-stage%20KV%20Cache%20optimization%20that%20further%20reduces%20memory%0Aconsumption.%20Our%20experiments%20on%20diverse%20video%20benchmarks%20demonstrate%20that%20METok%0Aachieves%20an%20optimal%20trade-off%20between%20efficiency%20and%20accuracy%20by%20dynamically%0Aselecting%20informative%20visual%20tokens.%20For%20instance%2C%20equipping%20LongVA-7B%20with%0AMETok%20realizes%20an%2080.6%25%20FLOPs%20reduction%20and%2093.5%25%20KV%20Cache%20memory%20savings%2C%20all%0Awhile%20maintaining%20comparable%20or%20even%20superior%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02850v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMETok%253A%2520Multi-Stage%2520Event-based%2520Token%2520Compression%2520for%2520Efficient%2520Long%250A%2520%2520Video%2520Understanding%26entry.906535625%3DMengyue%2520Wang%2520and%2520Shuo%2520Chen%2520and%2520Kristian%2520Kersting%2520and%2520Volker%2520Tresp%2520and%2520Yunpu%2520Ma%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520Video%2520Large%2520Language%2520Models%2520%2528VLLMs%2529%2520have%2520significantly%250Aenhanced%2520their%2520ability%2520to%2520understand%2520video%2520content.%2520Nonetheless%252C%2520processing%250Along%2520videos%2520remains%2520challenging%2520due%2520to%2520high%2520computational%2520demands%2520and%2520the%250Aredundancy%2520present%2520in%2520the%2520visual%2520data.%2520In%2520this%2520work%252C%2520we%2520propose%2520METok%252C%2520a%250Atraining-free%252C%2520Multi-stage%2520Event-based%2520Token%2520compression%2520framework%2520designed%2520to%250Aaccelerate%2520VLLMs%2527%2520inference%2520while%2520preserving%2520accuracy.%2520METok%2520progressively%250Aeliminates%2520redundant%2520visual%2520tokens%2520across%2520three%2520critical%2520stages%253A%2520%25281%2529%250Aevent-aware%2520compression%2520during%2520vision%2520encoding%252C%2520%25282%2529%2520hierarchical%2520token%2520pruning%250Ain%2520the%2520prefilling%2520stage%2520based%2520on%2520semantic%2520alignment%2520and%2520event%2520importance%252C%2520and%250A%25283%2529%2520a%2520decoding-stage%2520KV%2520Cache%2520optimization%2520that%2520further%2520reduces%2520memory%250Aconsumption.%2520Our%2520experiments%2520on%2520diverse%2520video%2520benchmarks%2520demonstrate%2520that%2520METok%250Aachieves%2520an%2520optimal%2520trade-off%2520between%2520efficiency%2520and%2520accuracy%2520by%2520dynamically%250Aselecting%2520informative%2520visual%2520tokens.%2520For%2520instance%252C%2520equipping%2520LongVA-7B%2520with%250AMETok%2520realizes%2520an%252080.6%2525%2520FLOPs%2520reduction%2520and%252093.5%2525%2520KV%2520Cache%2520memory%2520savings%252C%2520all%250Awhile%2520maintaining%2520comparable%2520or%2520even%2520superior%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02850v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=METok%3A%20Multi-Stage%20Event-based%20Token%20Compression%20for%20Efficient%20Long%0A%20%20Video%20Understanding&entry.906535625=Mengyue%20Wang%20and%20Shuo%20Chen%20and%20Kristian%20Kersting%20and%20Volker%20Tresp%20and%20Yunpu%20Ma&entry.1292438233=%20%20Recent%20advances%20in%20Video%20Large%20Language%20Models%20%28VLLMs%29%20have%20significantly%0Aenhanced%20their%20ability%20to%20understand%20video%20content.%20Nonetheless%2C%20processing%0Along%20videos%20remains%20challenging%20due%20to%20high%20computational%20demands%20and%20the%0Aredundancy%20present%20in%20the%20visual%20data.%20In%20this%20work%2C%20we%20propose%20METok%2C%20a%0Atraining-free%2C%20Multi-stage%20Event-based%20Token%20compression%20framework%20designed%20to%0Aaccelerate%20VLLMs%27%20inference%20while%20preserving%20accuracy.%20METok%20progressively%0Aeliminates%20redundant%20visual%20tokens%20across%20three%20critical%20stages%3A%20%281%29%0Aevent-aware%20compression%20during%20vision%20encoding%2C%20%282%29%20hierarchical%20token%20pruning%0Ain%20the%20prefilling%20stage%20based%20on%20semantic%20alignment%20and%20event%20importance%2C%20and%0A%283%29%20a%20decoding-stage%20KV%20Cache%20optimization%20that%20further%20reduces%20memory%0Aconsumption.%20Our%20experiments%20on%20diverse%20video%20benchmarks%20demonstrate%20that%20METok%0Aachieves%20an%20optimal%20trade-off%20between%20efficiency%20and%20accuracy%20by%20dynamically%0Aselecting%20informative%20visual%20tokens.%20For%20instance%2C%20equipping%20LongVA-7B%20with%0AMETok%20realizes%20an%2080.6%25%20FLOPs%20reduction%20and%2093.5%25%20KV%20Cache%20memory%20savings%2C%20all%0Awhile%20maintaining%20comparable%20or%20even%20superior%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02850v1&entry.124074799=Read"},
{"title": "OpenFace 3.0: A Lightweight Multitask System for Comprehensive Facial\n  Behavior Analysis", "author": "Jiewen Hu and Leena Mathur and Paul Pu Liang and Louis-Philippe Morency", "abstract": "  In recent years, there has been increasing interest in automatic facial\nbehavior analysis systems from computing communities such as vision, multimodal\ninteraction, robotics, and affective computing. Building upon the widespread\nutility of prior open-source facial analysis systems, we introduce OpenFace\n3.0, an open-source toolkit capable of facial landmark detection, facial action\nunit detection, eye-gaze estimation, and facial emotion recognition. OpenFace\n3.0 contributes a lightweight unified model for facial analysis, trained with a\nmulti-task architecture across diverse populations, head poses, lighting\nconditions, video resolutions, and facial analysis tasks. By leveraging the\nbenefits of parameter sharing through a unified model and training paradigm,\nOpenFace 3.0 exhibits improvements in prediction performance, inference speed,\nand memory efficiency over similar toolkits and rivals state-of-the-art models.\nOpenFace 3.0 can be installed and run with a single line of code and operate in\nreal-time without specialized hardware. OpenFace 3.0 code for training models\nand running the system is freely available for research purposes and supports\ncontributions from the community.\n", "link": "http://arxiv.org/abs/2506.02891v1", "date": "2025-06-03", "relevancy": 2.757, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5574}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5574}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5394}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenFace%203.0%3A%20A%20Lightweight%20Multitask%20System%20for%20Comprehensive%20Facial%0A%20%20Behavior%20Analysis&body=Title%3A%20OpenFace%203.0%3A%20A%20Lightweight%20Multitask%20System%20for%20Comprehensive%20Facial%0A%20%20Behavior%20Analysis%0AAuthor%3A%20Jiewen%20Hu%20and%20Leena%20Mathur%20and%20Paul%20Pu%20Liang%20and%20Louis-Philippe%20Morency%0AAbstract%3A%20%20%20In%20recent%20years%2C%20there%20has%20been%20increasing%20interest%20in%20automatic%20facial%0Abehavior%20analysis%20systems%20from%20computing%20communities%20such%20as%20vision%2C%20multimodal%0Ainteraction%2C%20robotics%2C%20and%20affective%20computing.%20Building%20upon%20the%20widespread%0Autility%20of%20prior%20open-source%20facial%20analysis%20systems%2C%20we%20introduce%20OpenFace%0A3.0%2C%20an%20open-source%20toolkit%20capable%20of%20facial%20landmark%20detection%2C%20facial%20action%0Aunit%20detection%2C%20eye-gaze%20estimation%2C%20and%20facial%20emotion%20recognition.%20OpenFace%0A3.0%20contributes%20a%20lightweight%20unified%20model%20for%20facial%20analysis%2C%20trained%20with%20a%0Amulti-task%20architecture%20across%20diverse%20populations%2C%20head%20poses%2C%20lighting%0Aconditions%2C%20video%20resolutions%2C%20and%20facial%20analysis%20tasks.%20By%20leveraging%20the%0Abenefits%20of%20parameter%20sharing%20through%20a%20unified%20model%20and%20training%20paradigm%2C%0AOpenFace%203.0%20exhibits%20improvements%20in%20prediction%20performance%2C%20inference%20speed%2C%0Aand%20memory%20efficiency%20over%20similar%20toolkits%20and%20rivals%20state-of-the-art%20models.%0AOpenFace%203.0%20can%20be%20installed%20and%20run%20with%20a%20single%20line%20of%20code%20and%20operate%20in%0Areal-time%20without%20specialized%20hardware.%20OpenFace%203.0%20code%20for%20training%20models%0Aand%20running%20the%20system%20is%20freely%20available%20for%20research%20purposes%20and%20supports%0Acontributions%20from%20the%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02891v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenFace%25203.0%253A%2520A%2520Lightweight%2520Multitask%2520System%2520for%2520Comprehensive%2520Facial%250A%2520%2520Behavior%2520Analysis%26entry.906535625%3DJiewen%2520Hu%2520and%2520Leena%2520Mathur%2520and%2520Paul%2520Pu%2520Liang%2520and%2520Louis-Philippe%2520Morency%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520there%2520has%2520been%2520increasing%2520interest%2520in%2520automatic%2520facial%250Abehavior%2520analysis%2520systems%2520from%2520computing%2520communities%2520such%2520as%2520vision%252C%2520multimodal%250Ainteraction%252C%2520robotics%252C%2520and%2520affective%2520computing.%2520Building%2520upon%2520the%2520widespread%250Autility%2520of%2520prior%2520open-source%2520facial%2520analysis%2520systems%252C%2520we%2520introduce%2520OpenFace%250A3.0%252C%2520an%2520open-source%2520toolkit%2520capable%2520of%2520facial%2520landmark%2520detection%252C%2520facial%2520action%250Aunit%2520detection%252C%2520eye-gaze%2520estimation%252C%2520and%2520facial%2520emotion%2520recognition.%2520OpenFace%250A3.0%2520contributes%2520a%2520lightweight%2520unified%2520model%2520for%2520facial%2520analysis%252C%2520trained%2520with%2520a%250Amulti-task%2520architecture%2520across%2520diverse%2520populations%252C%2520head%2520poses%252C%2520lighting%250Aconditions%252C%2520video%2520resolutions%252C%2520and%2520facial%2520analysis%2520tasks.%2520By%2520leveraging%2520the%250Abenefits%2520of%2520parameter%2520sharing%2520through%2520a%2520unified%2520model%2520and%2520training%2520paradigm%252C%250AOpenFace%25203.0%2520exhibits%2520improvements%2520in%2520prediction%2520performance%252C%2520inference%2520speed%252C%250Aand%2520memory%2520efficiency%2520over%2520similar%2520toolkits%2520and%2520rivals%2520state-of-the-art%2520models.%250AOpenFace%25203.0%2520can%2520be%2520installed%2520and%2520run%2520with%2520a%2520single%2520line%2520of%2520code%2520and%2520operate%2520in%250Areal-time%2520without%2520specialized%2520hardware.%2520OpenFace%25203.0%2520code%2520for%2520training%2520models%250Aand%2520running%2520the%2520system%2520is%2520freely%2520available%2520for%2520research%2520purposes%2520and%2520supports%250Acontributions%2520from%2520the%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02891v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenFace%203.0%3A%20A%20Lightweight%20Multitask%20System%20for%20Comprehensive%20Facial%0A%20%20Behavior%20Analysis&entry.906535625=Jiewen%20Hu%20and%20Leena%20Mathur%20and%20Paul%20Pu%20Liang%20and%20Louis-Philippe%20Morency&entry.1292438233=%20%20In%20recent%20years%2C%20there%20has%20been%20increasing%20interest%20in%20automatic%20facial%0Abehavior%20analysis%20systems%20from%20computing%20communities%20such%20as%20vision%2C%20multimodal%0Ainteraction%2C%20robotics%2C%20and%20affective%20computing.%20Building%20upon%20the%20widespread%0Autility%20of%20prior%20open-source%20facial%20analysis%20systems%2C%20we%20introduce%20OpenFace%0A3.0%2C%20an%20open-source%20toolkit%20capable%20of%20facial%20landmark%20detection%2C%20facial%20action%0Aunit%20detection%2C%20eye-gaze%20estimation%2C%20and%20facial%20emotion%20recognition.%20OpenFace%0A3.0%20contributes%20a%20lightweight%20unified%20model%20for%20facial%20analysis%2C%20trained%20with%20a%0Amulti-task%20architecture%20across%20diverse%20populations%2C%20head%20poses%2C%20lighting%0Aconditions%2C%20video%20resolutions%2C%20and%20facial%20analysis%20tasks.%20By%20leveraging%20the%0Abenefits%20of%20parameter%20sharing%20through%20a%20unified%20model%20and%20training%20paradigm%2C%0AOpenFace%203.0%20exhibits%20improvements%20in%20prediction%20performance%2C%20inference%20speed%2C%0Aand%20memory%20efficiency%20over%20similar%20toolkits%20and%20rivals%20state-of-the-art%20models.%0AOpenFace%203.0%20can%20be%20installed%20and%20run%20with%20a%20single%20line%20of%20code%20and%20operate%20in%0Areal-time%20without%20specialized%20hardware.%20OpenFace%203.0%20code%20for%20training%20models%0Aand%20running%20the%20system%20is%20freely%20available%20for%20research%20purposes%20and%20supports%0Acontributions%20from%20the%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02891v1&entry.124074799=Read"},
{"title": "Open-PMC-18M: A High-Fidelity Large Scale Medical Dataset for Multimodal\n  Representation Learning", "author": "Negin Baghbanzadeh and Sajad Ashkezari and Elham Dolatabadi and Arash Afkanpour", "abstract": "  Compound figures, which are multi-panel composites containing diverse\nsubfigures, are ubiquitous in biomedical literature, yet large-scale subfigure\nextraction remains largely unaddressed. Prior work on subfigure extraction has\nbeen limited in both dataset size and generalizability, leaving a critical open\nquestion: How does high-fidelity image-text alignment via large-scale subfigure\nextraction impact representation learning in vision-language models? We address\nthis gap by introducing a scalable subfigure extraction pipeline based on\ntransformer-based object detection, trained on a synthetic corpus of 500,000\ncompound figures, and achieving state-of-the-art performance on both ImageCLEF\n2016 and synthetic benchmarks. Using this pipeline, we release OPEN-PMC-18M, a\nlarge-scale high quality biomedical vision-language dataset comprising 18\nmillion clinically relevant subfigure-caption pairs spanning radiology,\nmicroscopy, and visible light photography. We train and evaluate\nvision-language models on our curated datasets and show improved performance\nacross retrieval, zero-shot classification, and robustness benchmarks,\noutperforming existing baselines. We release our dataset, models, and code to\nsupport reproducible benchmarks and further study into biomedical\nvision-language modeling and representation learning.\n", "link": "http://arxiv.org/abs/2506.02738v1", "date": "2025-06-03", "relevancy": 2.7419, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5485}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5485}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Open-PMC-18M%3A%20A%20High-Fidelity%20Large%20Scale%20Medical%20Dataset%20for%20Multimodal%0A%20%20Representation%20Learning&body=Title%3A%20Open-PMC-18M%3A%20A%20High-Fidelity%20Large%20Scale%20Medical%20Dataset%20for%20Multimodal%0A%20%20Representation%20Learning%0AAuthor%3A%20Negin%20Baghbanzadeh%20and%20Sajad%20Ashkezari%20and%20Elham%20Dolatabadi%20and%20Arash%20Afkanpour%0AAbstract%3A%20%20%20Compound%20figures%2C%20which%20are%20multi-panel%20composites%20containing%20diverse%0Asubfigures%2C%20are%20ubiquitous%20in%20biomedical%20literature%2C%20yet%20large-scale%20subfigure%0Aextraction%20remains%20largely%20unaddressed.%20Prior%20work%20on%20subfigure%20extraction%20has%0Abeen%20limited%20in%20both%20dataset%20size%20and%20generalizability%2C%20leaving%20a%20critical%20open%0Aquestion%3A%20How%20does%20high-fidelity%20image-text%20alignment%20via%20large-scale%20subfigure%0Aextraction%20impact%20representation%20learning%20in%20vision-language%20models%3F%20We%20address%0Athis%20gap%20by%20introducing%20a%20scalable%20subfigure%20extraction%20pipeline%20based%20on%0Atransformer-based%20object%20detection%2C%20trained%20on%20a%20synthetic%20corpus%20of%20500%2C000%0Acompound%20figures%2C%20and%20achieving%20state-of-the-art%20performance%20on%20both%20ImageCLEF%0A2016%20and%20synthetic%20benchmarks.%20Using%20this%20pipeline%2C%20we%20release%20OPEN-PMC-18M%2C%20a%0Alarge-scale%20high%20quality%20biomedical%20vision-language%20dataset%20comprising%2018%0Amillion%20clinically%20relevant%20subfigure-caption%20pairs%20spanning%20radiology%2C%0Amicroscopy%2C%20and%20visible%20light%20photography.%20We%20train%20and%20evaluate%0Avision-language%20models%20on%20our%20curated%20datasets%20and%20show%20improved%20performance%0Aacross%20retrieval%2C%20zero-shot%20classification%2C%20and%20robustness%20benchmarks%2C%0Aoutperforming%20existing%20baselines.%20We%20release%20our%20dataset%2C%20models%2C%20and%20code%20to%0Asupport%20reproducible%20benchmarks%20and%20further%20study%20into%20biomedical%0Avision-language%20modeling%20and%20representation%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02738v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpen-PMC-18M%253A%2520A%2520High-Fidelity%2520Large%2520Scale%2520Medical%2520Dataset%2520for%2520Multimodal%250A%2520%2520Representation%2520Learning%26entry.906535625%3DNegin%2520Baghbanzadeh%2520and%2520Sajad%2520Ashkezari%2520and%2520Elham%2520Dolatabadi%2520and%2520Arash%2520Afkanpour%26entry.1292438233%3D%2520%2520Compound%2520figures%252C%2520which%2520are%2520multi-panel%2520composites%2520containing%2520diverse%250Asubfigures%252C%2520are%2520ubiquitous%2520in%2520biomedical%2520literature%252C%2520yet%2520large-scale%2520subfigure%250Aextraction%2520remains%2520largely%2520unaddressed.%2520Prior%2520work%2520on%2520subfigure%2520extraction%2520has%250Abeen%2520limited%2520in%2520both%2520dataset%2520size%2520and%2520generalizability%252C%2520leaving%2520a%2520critical%2520open%250Aquestion%253A%2520How%2520does%2520high-fidelity%2520image-text%2520alignment%2520via%2520large-scale%2520subfigure%250Aextraction%2520impact%2520representation%2520learning%2520in%2520vision-language%2520models%253F%2520We%2520address%250Athis%2520gap%2520by%2520introducing%2520a%2520scalable%2520subfigure%2520extraction%2520pipeline%2520based%2520on%250Atransformer-based%2520object%2520detection%252C%2520trained%2520on%2520a%2520synthetic%2520corpus%2520of%2520500%252C000%250Acompound%2520figures%252C%2520and%2520achieving%2520state-of-the-art%2520performance%2520on%2520both%2520ImageCLEF%250A2016%2520and%2520synthetic%2520benchmarks.%2520Using%2520this%2520pipeline%252C%2520we%2520release%2520OPEN-PMC-18M%252C%2520a%250Alarge-scale%2520high%2520quality%2520biomedical%2520vision-language%2520dataset%2520comprising%252018%250Amillion%2520clinically%2520relevant%2520subfigure-caption%2520pairs%2520spanning%2520radiology%252C%250Amicroscopy%252C%2520and%2520visible%2520light%2520photography.%2520We%2520train%2520and%2520evaluate%250Avision-language%2520models%2520on%2520our%2520curated%2520datasets%2520and%2520show%2520improved%2520performance%250Aacross%2520retrieval%252C%2520zero-shot%2520classification%252C%2520and%2520robustness%2520benchmarks%252C%250Aoutperforming%2520existing%2520baselines.%2520We%2520release%2520our%2520dataset%252C%2520models%252C%2520and%2520code%2520to%250Asupport%2520reproducible%2520benchmarks%2520and%2520further%2520study%2520into%2520biomedical%250Avision-language%2520modeling%2520and%2520representation%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02738v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Open-PMC-18M%3A%20A%20High-Fidelity%20Large%20Scale%20Medical%20Dataset%20for%20Multimodal%0A%20%20Representation%20Learning&entry.906535625=Negin%20Baghbanzadeh%20and%20Sajad%20Ashkezari%20and%20Elham%20Dolatabadi%20and%20Arash%20Afkanpour&entry.1292438233=%20%20Compound%20figures%2C%20which%20are%20multi-panel%20composites%20containing%20diverse%0Asubfigures%2C%20are%20ubiquitous%20in%20biomedical%20literature%2C%20yet%20large-scale%20subfigure%0Aextraction%20remains%20largely%20unaddressed.%20Prior%20work%20on%20subfigure%20extraction%20has%0Abeen%20limited%20in%20both%20dataset%20size%20and%20generalizability%2C%20leaving%20a%20critical%20open%0Aquestion%3A%20How%20does%20high-fidelity%20image-text%20alignment%20via%20large-scale%20subfigure%0Aextraction%20impact%20representation%20learning%20in%20vision-language%20models%3F%20We%20address%0Athis%20gap%20by%20introducing%20a%20scalable%20subfigure%20extraction%20pipeline%20based%20on%0Atransformer-based%20object%20detection%2C%20trained%20on%20a%20synthetic%20corpus%20of%20500%2C000%0Acompound%20figures%2C%20and%20achieving%20state-of-the-art%20performance%20on%20both%20ImageCLEF%0A2016%20and%20synthetic%20benchmarks.%20Using%20this%20pipeline%2C%20we%20release%20OPEN-PMC-18M%2C%20a%0Alarge-scale%20high%20quality%20biomedical%20vision-language%20dataset%20comprising%2018%0Amillion%20clinically%20relevant%20subfigure-caption%20pairs%20spanning%20radiology%2C%0Amicroscopy%2C%20and%20visible%20light%20photography.%20We%20train%20and%20evaluate%0Avision-language%20models%20on%20our%20curated%20datasets%20and%20show%20improved%20performance%0Aacross%20retrieval%2C%20zero-shot%20classification%2C%20and%20robustness%20benchmarks%2C%0Aoutperforming%20existing%20baselines.%20We%20release%20our%20dataset%2C%20models%2C%20and%20code%20to%0Asupport%20reproducible%20benchmarks%20and%20further%20study%20into%20biomedical%0Avision-language%20modeling%20and%20representation%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02738v1&entry.124074799=Read"},
{"title": "A Comparative Study of Scanpath Models in Graph-Based Visualization", "author": "Angela Lopez-Cardona and Parvin Emami and Sebastian Idesis and Saravanakumar Duraisamy and Luis A. Leiva and Ioannis Arapakis", "abstract": "  Information Visualization (InfoVis) systems utilize visual representations to\nenhance data interpretation. Understanding how visual attention is allocated is\nessential for optimizing interface design. However, collecting Eye-tracking\n(ET) data presents challenges related to cost, privacy, and scalability.\nComputational models provide alternatives for predicting gaze patterns, thereby\nadvancing InfoVis research. In our study, we conducted an ET experiment with 40\nparticipants who analyzed graphs while responding to questions of varying\ncomplexity within the context of digital forensics. We compared human scanpaths\nwith synthetic ones generated by models such as DeepGaze, UMSS, and Gazeformer.\nOur research evaluates the accuracy of these models and examines how question\ncomplexity and number of nodes influence performance. This work contributes to\nthe development of predictive modeling in visual analytics, offering insights\nthat can enhance the design and effectiveness of InfoVis systems.\n", "link": "http://arxiv.org/abs/2503.24160v3", "date": "2025-06-03", "relevancy": 2.7319, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5593}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5593}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comparative%20Study%20of%20Scanpath%20Models%20in%20Graph-Based%20Visualization&body=Title%3A%20A%20Comparative%20Study%20of%20Scanpath%20Models%20in%20Graph-Based%20Visualization%0AAuthor%3A%20Angela%20Lopez-Cardona%20and%20Parvin%20Emami%20and%20Sebastian%20Idesis%20and%20Saravanakumar%20Duraisamy%20and%20Luis%20A.%20Leiva%20and%20Ioannis%20Arapakis%0AAbstract%3A%20%20%20Information%20Visualization%20%28InfoVis%29%20systems%20utilize%20visual%20representations%20to%0Aenhance%20data%20interpretation.%20Understanding%20how%20visual%20attention%20is%20allocated%20is%0Aessential%20for%20optimizing%20interface%20design.%20However%2C%20collecting%20Eye-tracking%0A%28ET%29%20data%20presents%20challenges%20related%20to%20cost%2C%20privacy%2C%20and%20scalability.%0AComputational%20models%20provide%20alternatives%20for%20predicting%20gaze%20patterns%2C%20thereby%0Aadvancing%20InfoVis%20research.%20In%20our%20study%2C%20we%20conducted%20an%20ET%20experiment%20with%2040%0Aparticipants%20who%20analyzed%20graphs%20while%20responding%20to%20questions%20of%20varying%0Acomplexity%20within%20the%20context%20of%20digital%20forensics.%20We%20compared%20human%20scanpaths%0Awith%20synthetic%20ones%20generated%20by%20models%20such%20as%20DeepGaze%2C%20UMSS%2C%20and%20Gazeformer.%0AOur%20research%20evaluates%20the%20accuracy%20of%20these%20models%20and%20examines%20how%20question%0Acomplexity%20and%20number%20of%20nodes%20influence%20performance.%20This%20work%20contributes%20to%0Athe%20development%20of%20predictive%20modeling%20in%20visual%20analytics%2C%20offering%20insights%0Athat%20can%20enhance%20the%20design%20and%20effectiveness%20of%20InfoVis%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.24160v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comparative%2520Study%2520of%2520Scanpath%2520Models%2520in%2520Graph-Based%2520Visualization%26entry.906535625%3DAngela%2520Lopez-Cardona%2520and%2520Parvin%2520Emami%2520and%2520Sebastian%2520Idesis%2520and%2520Saravanakumar%2520Duraisamy%2520and%2520Luis%2520A.%2520Leiva%2520and%2520Ioannis%2520Arapakis%26entry.1292438233%3D%2520%2520Information%2520Visualization%2520%2528InfoVis%2529%2520systems%2520utilize%2520visual%2520representations%2520to%250Aenhance%2520data%2520interpretation.%2520Understanding%2520how%2520visual%2520attention%2520is%2520allocated%2520is%250Aessential%2520for%2520optimizing%2520interface%2520design.%2520However%252C%2520collecting%2520Eye-tracking%250A%2528ET%2529%2520data%2520presents%2520challenges%2520related%2520to%2520cost%252C%2520privacy%252C%2520and%2520scalability.%250AComputational%2520models%2520provide%2520alternatives%2520for%2520predicting%2520gaze%2520patterns%252C%2520thereby%250Aadvancing%2520InfoVis%2520research.%2520In%2520our%2520study%252C%2520we%2520conducted%2520an%2520ET%2520experiment%2520with%252040%250Aparticipants%2520who%2520analyzed%2520graphs%2520while%2520responding%2520to%2520questions%2520of%2520varying%250Acomplexity%2520within%2520the%2520context%2520of%2520digital%2520forensics.%2520We%2520compared%2520human%2520scanpaths%250Awith%2520synthetic%2520ones%2520generated%2520by%2520models%2520such%2520as%2520DeepGaze%252C%2520UMSS%252C%2520and%2520Gazeformer.%250AOur%2520research%2520evaluates%2520the%2520accuracy%2520of%2520these%2520models%2520and%2520examines%2520how%2520question%250Acomplexity%2520and%2520number%2520of%2520nodes%2520influence%2520performance.%2520This%2520work%2520contributes%2520to%250Athe%2520development%2520of%2520predictive%2520modeling%2520in%2520visual%2520analytics%252C%2520offering%2520insights%250Athat%2520can%2520enhance%2520the%2520design%2520and%2520effectiveness%2520of%2520InfoVis%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.24160v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comparative%20Study%20of%20Scanpath%20Models%20in%20Graph-Based%20Visualization&entry.906535625=Angela%20Lopez-Cardona%20and%20Parvin%20Emami%20and%20Sebastian%20Idesis%20and%20Saravanakumar%20Duraisamy%20and%20Luis%20A.%20Leiva%20and%20Ioannis%20Arapakis&entry.1292438233=%20%20Information%20Visualization%20%28InfoVis%29%20systems%20utilize%20visual%20representations%20to%0Aenhance%20data%20interpretation.%20Understanding%20how%20visual%20attention%20is%20allocated%20is%0Aessential%20for%20optimizing%20interface%20design.%20However%2C%20collecting%20Eye-tracking%0A%28ET%29%20data%20presents%20challenges%20related%20to%20cost%2C%20privacy%2C%20and%20scalability.%0AComputational%20models%20provide%20alternatives%20for%20predicting%20gaze%20patterns%2C%20thereby%0Aadvancing%20InfoVis%20research.%20In%20our%20study%2C%20we%20conducted%20an%20ET%20experiment%20with%2040%0Aparticipants%20who%20analyzed%20graphs%20while%20responding%20to%20questions%20of%20varying%0Acomplexity%20within%20the%20context%20of%20digital%20forensics.%20We%20compared%20human%20scanpaths%0Awith%20synthetic%20ones%20generated%20by%20models%20such%20as%20DeepGaze%2C%20UMSS%2C%20and%20Gazeformer.%0AOur%20research%20evaluates%20the%20accuracy%20of%20these%20models%20and%20examines%20how%20question%0Acomplexity%20and%20number%20of%20nodes%20influence%20performance.%20This%20work%20contributes%20to%0Athe%20development%20of%20predictive%20modeling%20in%20visual%20analytics%2C%20offering%20insights%0Athat%20can%20enhance%20the%20design%20and%20effectiveness%20of%20InfoVis%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.24160v3&entry.124074799=Read"},
{"title": "Focal-SAM: Focal Sharpness-Aware Minimization for Long-Tailed\n  Classification", "author": "Sicong Li and Qianqian Xu and Zhiyong Yang and Zitai Wang and Linchao Zhang and Xiaochun Cao and Qingming Huang", "abstract": "  Real-world datasets often follow a long-tailed distribution, making\ngeneralization to tail classes difficult. Recent methods resorted to long-tail\nvariants of Sharpness-Aware Minimization (SAM), such as ImbSAM and CC-SAM, to\nimprove generalization by flattening the loss landscape. However, these\nattempts face a trade-off between computational efficiency and control over the\nloss landscape. On the one hand, ImbSAM is efficient but offers only coarse\ncontrol as it excludes head classes from the SAM process. On the other hand,\nCC-SAM provides fine-grained control through class-dependent perturbations but\nat the cost of efficiency due to multiple backpropagations. Seeing this\ndilemma, we introduce Focal-SAM, which assigns different penalties to\nclass-wise sharpness, achieving fine-grained control without extra\nbackpropagations, thus maintaining efficiency. Furthermore, we theoretically\nanalyze Focal-SAM's generalization ability and derive a sharper generalization\nbound. Extensive experiments on both traditional and foundation models validate\nthe effectiveness of Focal-SAM.\n", "link": "http://arxiv.org/abs/2505.01660v2", "date": "2025-06-03", "relevancy": 2.7318, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.618}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5232}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4979}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Focal-SAM%3A%20Focal%20Sharpness-Aware%20Minimization%20for%20Long-Tailed%0A%20%20Classification&body=Title%3A%20Focal-SAM%3A%20Focal%20Sharpness-Aware%20Minimization%20for%20Long-Tailed%0A%20%20Classification%0AAuthor%3A%20Sicong%20Li%20and%20Qianqian%20Xu%20and%20Zhiyong%20Yang%20and%20Zitai%20Wang%20and%20Linchao%20Zhang%20and%20Xiaochun%20Cao%20and%20Qingming%20Huang%0AAbstract%3A%20%20%20Real-world%20datasets%20often%20follow%20a%20long-tailed%20distribution%2C%20making%0Ageneralization%20to%20tail%20classes%20difficult.%20Recent%20methods%20resorted%20to%20long-tail%0Avariants%20of%20Sharpness-Aware%20Minimization%20%28SAM%29%2C%20such%20as%20ImbSAM%20and%20CC-SAM%2C%20to%0Aimprove%20generalization%20by%20flattening%20the%20loss%20landscape.%20However%2C%20these%0Aattempts%20face%20a%20trade-off%20between%20computational%20efficiency%20and%20control%20over%20the%0Aloss%20landscape.%20On%20the%20one%20hand%2C%20ImbSAM%20is%20efficient%20but%20offers%20only%20coarse%0Acontrol%20as%20it%20excludes%20head%20classes%20from%20the%20SAM%20process.%20On%20the%20other%20hand%2C%0ACC-SAM%20provides%20fine-grained%20control%20through%20class-dependent%20perturbations%20but%0Aat%20the%20cost%20of%20efficiency%20due%20to%20multiple%20backpropagations.%20Seeing%20this%0Adilemma%2C%20we%20introduce%20Focal-SAM%2C%20which%20assigns%20different%20penalties%20to%0Aclass-wise%20sharpness%2C%20achieving%20fine-grained%20control%20without%20extra%0Abackpropagations%2C%20thus%20maintaining%20efficiency.%20Furthermore%2C%20we%20theoretically%0Aanalyze%20Focal-SAM%27s%20generalization%20ability%20and%20derive%20a%20sharper%20generalization%0Abound.%20Extensive%20experiments%20on%20both%20traditional%20and%20foundation%20models%20validate%0Athe%20effectiveness%20of%20Focal-SAM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.01660v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFocal-SAM%253A%2520Focal%2520Sharpness-Aware%2520Minimization%2520for%2520Long-Tailed%250A%2520%2520Classification%26entry.906535625%3DSicong%2520Li%2520and%2520Qianqian%2520Xu%2520and%2520Zhiyong%2520Yang%2520and%2520Zitai%2520Wang%2520and%2520Linchao%2520Zhang%2520and%2520Xiaochun%2520Cao%2520and%2520Qingming%2520Huang%26entry.1292438233%3D%2520%2520Real-world%2520datasets%2520often%2520follow%2520a%2520long-tailed%2520distribution%252C%2520making%250Ageneralization%2520to%2520tail%2520classes%2520difficult.%2520Recent%2520methods%2520resorted%2520to%2520long-tail%250Avariants%2520of%2520Sharpness-Aware%2520Minimization%2520%2528SAM%2529%252C%2520such%2520as%2520ImbSAM%2520and%2520CC-SAM%252C%2520to%250Aimprove%2520generalization%2520by%2520flattening%2520the%2520loss%2520landscape.%2520However%252C%2520these%250Aattempts%2520face%2520a%2520trade-off%2520between%2520computational%2520efficiency%2520and%2520control%2520over%2520the%250Aloss%2520landscape.%2520On%2520the%2520one%2520hand%252C%2520ImbSAM%2520is%2520efficient%2520but%2520offers%2520only%2520coarse%250Acontrol%2520as%2520it%2520excludes%2520head%2520classes%2520from%2520the%2520SAM%2520process.%2520On%2520the%2520other%2520hand%252C%250ACC-SAM%2520provides%2520fine-grained%2520control%2520through%2520class-dependent%2520perturbations%2520but%250Aat%2520the%2520cost%2520of%2520efficiency%2520due%2520to%2520multiple%2520backpropagations.%2520Seeing%2520this%250Adilemma%252C%2520we%2520introduce%2520Focal-SAM%252C%2520which%2520assigns%2520different%2520penalties%2520to%250Aclass-wise%2520sharpness%252C%2520achieving%2520fine-grained%2520control%2520without%2520extra%250Abackpropagations%252C%2520thus%2520maintaining%2520efficiency.%2520Furthermore%252C%2520we%2520theoretically%250Aanalyze%2520Focal-SAM%2527s%2520generalization%2520ability%2520and%2520derive%2520a%2520sharper%2520generalization%250Abound.%2520Extensive%2520experiments%2520on%2520both%2520traditional%2520and%2520foundation%2520models%2520validate%250Athe%2520effectiveness%2520of%2520Focal-SAM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.01660v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Focal-SAM%3A%20Focal%20Sharpness-Aware%20Minimization%20for%20Long-Tailed%0A%20%20Classification&entry.906535625=Sicong%20Li%20and%20Qianqian%20Xu%20and%20Zhiyong%20Yang%20and%20Zitai%20Wang%20and%20Linchao%20Zhang%20and%20Xiaochun%20Cao%20and%20Qingming%20Huang&entry.1292438233=%20%20Real-world%20datasets%20often%20follow%20a%20long-tailed%20distribution%2C%20making%0Ageneralization%20to%20tail%20classes%20difficult.%20Recent%20methods%20resorted%20to%20long-tail%0Avariants%20of%20Sharpness-Aware%20Minimization%20%28SAM%29%2C%20such%20as%20ImbSAM%20and%20CC-SAM%2C%20to%0Aimprove%20generalization%20by%20flattening%20the%20loss%20landscape.%20However%2C%20these%0Aattempts%20face%20a%20trade-off%20between%20computational%20efficiency%20and%20control%20over%20the%0Aloss%20landscape.%20On%20the%20one%20hand%2C%20ImbSAM%20is%20efficient%20but%20offers%20only%20coarse%0Acontrol%20as%20it%20excludes%20head%20classes%20from%20the%20SAM%20process.%20On%20the%20other%20hand%2C%0ACC-SAM%20provides%20fine-grained%20control%20through%20class-dependent%20perturbations%20but%0Aat%20the%20cost%20of%20efficiency%20due%20to%20multiple%20backpropagations.%20Seeing%20this%0Adilemma%2C%20we%20introduce%20Focal-SAM%2C%20which%20assigns%20different%20penalties%20to%0Aclass-wise%20sharpness%2C%20achieving%20fine-grained%20control%20without%20extra%0Abackpropagations%2C%20thus%20maintaining%20efficiency.%20Furthermore%2C%20we%20theoretically%0Aanalyze%20Focal-SAM%27s%20generalization%20ability%20and%20derive%20a%20sharper%20generalization%0Abound.%20Extensive%20experiments%20on%20both%20traditional%20and%20foundation%20models%20validate%0Athe%20effectiveness%20of%20Focal-SAM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.01660v2&entry.124074799=Read"},
{"title": "A Novel Benchmark for Few-Shot Semantic Segmentation in the Era of\n  Foundation Models", "author": "Reda Bensaid and Vincent Gripon and Fran\u00e7ois Leduc-Primeau and Lukas Mauch and Ghouthi Boukli Hacene and Fabien Cardinaux", "abstract": "  Few-shot semantic segmentation (FSS) is a crucial challenge in computer\nvision, driving extensive research into a diverse range of methods, from\nadvanced meta-learning techniques to simple transfer learning baselines. With\nthe emergence of vision foundation models (VFM) serving as generalist feature\nextractors, we seek to explore the adaptation of these models for FSS. While\ncurrent FSS benchmarks focus on adapting pre-trained models to new tasks with\nfew images, they emphasize in-domain generalization, making them less suitable\nfor VFM trained on large-scale web datasets. To address this, we propose a\nnovel realistic benchmark with a simple and straightforward adaptation process\ntailored for this task. Using this benchmark, we conduct a comprehensive\ncomparative analysis of prominent VFM and semantic segmentation models. To\nevaluate their effectiveness, we leverage various adaption methods, ranging\nfrom linear probing to parameter efficient fine-tuning (PEFT) and full\nfine-tuning. Our findings show that models designed for segmentation can be\noutperformed by self-supervised (SSL) models. On the other hand, while PEFT\nmethods yields competitive performance, they provide little discrepancy in the\nobtained results compared to other methods, highlighting the critical role of\nthe feature extractor in determining results. To our knowledge, this is the\nfirst study on the adaptation of VFM for FSS.\n", "link": "http://arxiv.org/abs/2401.11311v3", "date": "2025-06-03", "relevancy": 2.7302, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5694}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5694}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4992}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Novel%20Benchmark%20for%20Few-Shot%20Semantic%20Segmentation%20in%20the%20Era%20of%0A%20%20Foundation%20Models&body=Title%3A%20A%20Novel%20Benchmark%20for%20Few-Shot%20Semantic%20Segmentation%20in%20the%20Era%20of%0A%20%20Foundation%20Models%0AAuthor%3A%20Reda%20Bensaid%20and%20Vincent%20Gripon%20and%20Fran%C3%A7ois%20Leduc-Primeau%20and%20Lukas%20Mauch%20and%20Ghouthi%20Boukli%20Hacene%20and%20Fabien%20Cardinaux%0AAbstract%3A%20%20%20Few-shot%20semantic%20segmentation%20%28FSS%29%20is%20a%20crucial%20challenge%20in%20computer%0Avision%2C%20driving%20extensive%20research%20into%20a%20diverse%20range%20of%20methods%2C%20from%0Aadvanced%20meta-learning%20techniques%20to%20simple%20transfer%20learning%20baselines.%20With%0Athe%20emergence%20of%20vision%20foundation%20models%20%28VFM%29%20serving%20as%20generalist%20feature%0Aextractors%2C%20we%20seek%20to%20explore%20the%20adaptation%20of%20these%20models%20for%20FSS.%20While%0Acurrent%20FSS%20benchmarks%20focus%20on%20adapting%20pre-trained%20models%20to%20new%20tasks%20with%0Afew%20images%2C%20they%20emphasize%20in-domain%20generalization%2C%20making%20them%20less%20suitable%0Afor%20VFM%20trained%20on%20large-scale%20web%20datasets.%20To%20address%20this%2C%20we%20propose%20a%0Anovel%20realistic%20benchmark%20with%20a%20simple%20and%20straightforward%20adaptation%20process%0Atailored%20for%20this%20task.%20Using%20this%20benchmark%2C%20we%20conduct%20a%20comprehensive%0Acomparative%20analysis%20of%20prominent%20VFM%20and%20semantic%20segmentation%20models.%20To%0Aevaluate%20their%20effectiveness%2C%20we%20leverage%20various%20adaption%20methods%2C%20ranging%0Afrom%20linear%20probing%20to%20parameter%20efficient%20fine-tuning%20%28PEFT%29%20and%20full%0Afine-tuning.%20Our%20findings%20show%20that%20models%20designed%20for%20segmentation%20can%20be%0Aoutperformed%20by%20self-supervised%20%28SSL%29%20models.%20On%20the%20other%20hand%2C%20while%20PEFT%0Amethods%20yields%20competitive%20performance%2C%20they%20provide%20little%20discrepancy%20in%20the%0Aobtained%20results%20compared%20to%20other%20methods%2C%20highlighting%20the%20critical%20role%20of%0Athe%20feature%20extractor%20in%20determining%20results.%20To%20our%20knowledge%2C%20this%20is%20the%0Afirst%20study%20on%20the%20adaptation%20of%20VFM%20for%20FSS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.11311v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Novel%2520Benchmark%2520for%2520Few-Shot%2520Semantic%2520Segmentation%2520in%2520the%2520Era%2520of%250A%2520%2520Foundation%2520Models%26entry.906535625%3DReda%2520Bensaid%2520and%2520Vincent%2520Gripon%2520and%2520Fran%25C3%25A7ois%2520Leduc-Primeau%2520and%2520Lukas%2520Mauch%2520and%2520Ghouthi%2520Boukli%2520Hacene%2520and%2520Fabien%2520Cardinaux%26entry.1292438233%3D%2520%2520Few-shot%2520semantic%2520segmentation%2520%2528FSS%2529%2520is%2520a%2520crucial%2520challenge%2520in%2520computer%250Avision%252C%2520driving%2520extensive%2520research%2520into%2520a%2520diverse%2520range%2520of%2520methods%252C%2520from%250Aadvanced%2520meta-learning%2520techniques%2520to%2520simple%2520transfer%2520learning%2520baselines.%2520With%250Athe%2520emergence%2520of%2520vision%2520foundation%2520models%2520%2528VFM%2529%2520serving%2520as%2520generalist%2520feature%250Aextractors%252C%2520we%2520seek%2520to%2520explore%2520the%2520adaptation%2520of%2520these%2520models%2520for%2520FSS.%2520While%250Acurrent%2520FSS%2520benchmarks%2520focus%2520on%2520adapting%2520pre-trained%2520models%2520to%2520new%2520tasks%2520with%250Afew%2520images%252C%2520they%2520emphasize%2520in-domain%2520generalization%252C%2520making%2520them%2520less%2520suitable%250Afor%2520VFM%2520trained%2520on%2520large-scale%2520web%2520datasets.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%250Anovel%2520realistic%2520benchmark%2520with%2520a%2520simple%2520and%2520straightforward%2520adaptation%2520process%250Atailored%2520for%2520this%2520task.%2520Using%2520this%2520benchmark%252C%2520we%2520conduct%2520a%2520comprehensive%250Acomparative%2520analysis%2520of%2520prominent%2520VFM%2520and%2520semantic%2520segmentation%2520models.%2520To%250Aevaluate%2520their%2520effectiveness%252C%2520we%2520leverage%2520various%2520adaption%2520methods%252C%2520ranging%250Afrom%2520linear%2520probing%2520to%2520parameter%2520efficient%2520fine-tuning%2520%2528PEFT%2529%2520and%2520full%250Afine-tuning.%2520Our%2520findings%2520show%2520that%2520models%2520designed%2520for%2520segmentation%2520can%2520be%250Aoutperformed%2520by%2520self-supervised%2520%2528SSL%2529%2520models.%2520On%2520the%2520other%2520hand%252C%2520while%2520PEFT%250Amethods%2520yields%2520competitive%2520performance%252C%2520they%2520provide%2520little%2520discrepancy%2520in%2520the%250Aobtained%2520results%2520compared%2520to%2520other%2520methods%252C%2520highlighting%2520the%2520critical%2520role%2520of%250Athe%2520feature%2520extractor%2520in%2520determining%2520results.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%250Afirst%2520study%2520on%2520the%2520adaptation%2520of%2520VFM%2520for%2520FSS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.11311v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Novel%20Benchmark%20for%20Few-Shot%20Semantic%20Segmentation%20in%20the%20Era%20of%0A%20%20Foundation%20Models&entry.906535625=Reda%20Bensaid%20and%20Vincent%20Gripon%20and%20Fran%C3%A7ois%20Leduc-Primeau%20and%20Lukas%20Mauch%20and%20Ghouthi%20Boukli%20Hacene%20and%20Fabien%20Cardinaux&entry.1292438233=%20%20Few-shot%20semantic%20segmentation%20%28FSS%29%20is%20a%20crucial%20challenge%20in%20computer%0Avision%2C%20driving%20extensive%20research%20into%20a%20diverse%20range%20of%20methods%2C%20from%0Aadvanced%20meta-learning%20techniques%20to%20simple%20transfer%20learning%20baselines.%20With%0Athe%20emergence%20of%20vision%20foundation%20models%20%28VFM%29%20serving%20as%20generalist%20feature%0Aextractors%2C%20we%20seek%20to%20explore%20the%20adaptation%20of%20these%20models%20for%20FSS.%20While%0Acurrent%20FSS%20benchmarks%20focus%20on%20adapting%20pre-trained%20models%20to%20new%20tasks%20with%0Afew%20images%2C%20they%20emphasize%20in-domain%20generalization%2C%20making%20them%20less%20suitable%0Afor%20VFM%20trained%20on%20large-scale%20web%20datasets.%20To%20address%20this%2C%20we%20propose%20a%0Anovel%20realistic%20benchmark%20with%20a%20simple%20and%20straightforward%20adaptation%20process%0Atailored%20for%20this%20task.%20Using%20this%20benchmark%2C%20we%20conduct%20a%20comprehensive%0Acomparative%20analysis%20of%20prominent%20VFM%20and%20semantic%20segmentation%20models.%20To%0Aevaluate%20their%20effectiveness%2C%20we%20leverage%20various%20adaption%20methods%2C%20ranging%0Afrom%20linear%20probing%20to%20parameter%20efficient%20fine-tuning%20%28PEFT%29%20and%20full%0Afine-tuning.%20Our%20findings%20show%20that%20models%20designed%20for%20segmentation%20can%20be%0Aoutperformed%20by%20self-supervised%20%28SSL%29%20models.%20On%20the%20other%20hand%2C%20while%20PEFT%0Amethods%20yields%20competitive%20performance%2C%20they%20provide%20little%20discrepancy%20in%20the%0Aobtained%20results%20compared%20to%20other%20methods%2C%20highlighting%20the%20critical%20role%20of%0Athe%20feature%20extractor%20in%20determining%20results.%20To%20our%20knowledge%2C%20this%20is%20the%0Afirst%20study%20on%20the%20adaptation%20of%20VFM%20for%20FSS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.11311v3&entry.124074799=Read"},
{"title": "FORLA:Federated Object-centric Representation Learning with Slot\n  Attention", "author": "Guiqiu Liao and Matjaz Jogan and Eric Eaton and Daniel A. Hashimoto", "abstract": "  Learning efficient visual representations across heterogeneous unlabeled\ndatasets remains a central challenge in federated learning. Effective federated\nrepresentations require features that are jointly informative across clients\nwhile disentangling domain-specific factors without supervision. We introduce\nFORLA, a novel framework for federated object-centric representation learning\nand feature adaptation across clients using unsupervised slot attention. At the\ncore of our method is a shared feature adapter, trained collaboratively across\nclients to adapt features from foundation models, and a shared slot attention\nmodule that learns to reconstruct the adapted features. To optimize this\nadapter, we design a two-branch student-teacher architecture. In each client, a\nstudent decoder learns to reconstruct full features from foundation models,\nwhile a teacher decoder reconstructs their adapted, low-dimensional\ncounterpart. The shared slot attention module bridges cross-domain learning by\naligning object-level representations across clients. Experiments in multiple\nreal-world datasets show that our framework not only outperforms centralized\nbaselines on object discovery but also learns a compact, universal\nrepresentation that generalizes well across domains. This work highlights\nfederated slot attention as an effective tool for scalable, unsupervised visual\nrepresentation learning from cross-domain data with distributed concepts.\n", "link": "http://arxiv.org/abs/2506.02964v1", "date": "2025-06-03", "relevancy": 2.7293, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5661}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5636}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5078}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FORLA%3AFederated%20Object-centric%20Representation%20Learning%20with%20Slot%0A%20%20Attention&body=Title%3A%20FORLA%3AFederated%20Object-centric%20Representation%20Learning%20with%20Slot%0A%20%20Attention%0AAuthor%3A%20Guiqiu%20Liao%20and%20Matjaz%20Jogan%20and%20Eric%20Eaton%20and%20Daniel%20A.%20Hashimoto%0AAbstract%3A%20%20%20Learning%20efficient%20visual%20representations%20across%20heterogeneous%20unlabeled%0Adatasets%20remains%20a%20central%20challenge%20in%20federated%20learning.%20Effective%20federated%0Arepresentations%20require%20features%20that%20are%20jointly%20informative%20across%20clients%0Awhile%20disentangling%20domain-specific%20factors%20without%20supervision.%20We%20introduce%0AFORLA%2C%20a%20novel%20framework%20for%20federated%20object-centric%20representation%20learning%0Aand%20feature%20adaptation%20across%20clients%20using%20unsupervised%20slot%20attention.%20At%20the%0Acore%20of%20our%20method%20is%20a%20shared%20feature%20adapter%2C%20trained%20collaboratively%20across%0Aclients%20to%20adapt%20features%20from%20foundation%20models%2C%20and%20a%20shared%20slot%20attention%0Amodule%20that%20learns%20to%20reconstruct%20the%20adapted%20features.%20To%20optimize%20this%0Aadapter%2C%20we%20design%20a%20two-branch%20student-teacher%20architecture.%20In%20each%20client%2C%20a%0Astudent%20decoder%20learns%20to%20reconstruct%20full%20features%20from%20foundation%20models%2C%0Awhile%20a%20teacher%20decoder%20reconstructs%20their%20adapted%2C%20low-dimensional%0Acounterpart.%20The%20shared%20slot%20attention%20module%20bridges%20cross-domain%20learning%20by%0Aaligning%20object-level%20representations%20across%20clients.%20Experiments%20in%20multiple%0Areal-world%20datasets%20show%20that%20our%20framework%20not%20only%20outperforms%20centralized%0Abaselines%20on%20object%20discovery%20but%20also%20learns%20a%20compact%2C%20universal%0Arepresentation%20that%20generalizes%20well%20across%20domains.%20This%20work%20highlights%0Afederated%20slot%20attention%20as%20an%20effective%20tool%20for%20scalable%2C%20unsupervised%20visual%0Arepresentation%20learning%20from%20cross-domain%20data%20with%20distributed%20concepts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02964v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFORLA%253AFederated%2520Object-centric%2520Representation%2520Learning%2520with%2520Slot%250A%2520%2520Attention%26entry.906535625%3DGuiqiu%2520Liao%2520and%2520Matjaz%2520Jogan%2520and%2520Eric%2520Eaton%2520and%2520Daniel%2520A.%2520Hashimoto%26entry.1292438233%3D%2520%2520Learning%2520efficient%2520visual%2520representations%2520across%2520heterogeneous%2520unlabeled%250Adatasets%2520remains%2520a%2520central%2520challenge%2520in%2520federated%2520learning.%2520Effective%2520federated%250Arepresentations%2520require%2520features%2520that%2520are%2520jointly%2520informative%2520across%2520clients%250Awhile%2520disentangling%2520domain-specific%2520factors%2520without%2520supervision.%2520We%2520introduce%250AFORLA%252C%2520a%2520novel%2520framework%2520for%2520federated%2520object-centric%2520representation%2520learning%250Aand%2520feature%2520adaptation%2520across%2520clients%2520using%2520unsupervised%2520slot%2520attention.%2520At%2520the%250Acore%2520of%2520our%2520method%2520is%2520a%2520shared%2520feature%2520adapter%252C%2520trained%2520collaboratively%2520across%250Aclients%2520to%2520adapt%2520features%2520from%2520foundation%2520models%252C%2520and%2520a%2520shared%2520slot%2520attention%250Amodule%2520that%2520learns%2520to%2520reconstruct%2520the%2520adapted%2520features.%2520To%2520optimize%2520this%250Aadapter%252C%2520we%2520design%2520a%2520two-branch%2520student-teacher%2520architecture.%2520In%2520each%2520client%252C%2520a%250Astudent%2520decoder%2520learns%2520to%2520reconstruct%2520full%2520features%2520from%2520foundation%2520models%252C%250Awhile%2520a%2520teacher%2520decoder%2520reconstructs%2520their%2520adapted%252C%2520low-dimensional%250Acounterpart.%2520The%2520shared%2520slot%2520attention%2520module%2520bridges%2520cross-domain%2520learning%2520by%250Aaligning%2520object-level%2520representations%2520across%2520clients.%2520Experiments%2520in%2520multiple%250Areal-world%2520datasets%2520show%2520that%2520our%2520framework%2520not%2520only%2520outperforms%2520centralized%250Abaselines%2520on%2520object%2520discovery%2520but%2520also%2520learns%2520a%2520compact%252C%2520universal%250Arepresentation%2520that%2520generalizes%2520well%2520across%2520domains.%2520This%2520work%2520highlights%250Afederated%2520slot%2520attention%2520as%2520an%2520effective%2520tool%2520for%2520scalable%252C%2520unsupervised%2520visual%250Arepresentation%2520learning%2520from%2520cross-domain%2520data%2520with%2520distributed%2520concepts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02964v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FORLA%3AFederated%20Object-centric%20Representation%20Learning%20with%20Slot%0A%20%20Attention&entry.906535625=Guiqiu%20Liao%20and%20Matjaz%20Jogan%20and%20Eric%20Eaton%20and%20Daniel%20A.%20Hashimoto&entry.1292438233=%20%20Learning%20efficient%20visual%20representations%20across%20heterogeneous%20unlabeled%0Adatasets%20remains%20a%20central%20challenge%20in%20federated%20learning.%20Effective%20federated%0Arepresentations%20require%20features%20that%20are%20jointly%20informative%20across%20clients%0Awhile%20disentangling%20domain-specific%20factors%20without%20supervision.%20We%20introduce%0AFORLA%2C%20a%20novel%20framework%20for%20federated%20object-centric%20representation%20learning%0Aand%20feature%20adaptation%20across%20clients%20using%20unsupervised%20slot%20attention.%20At%20the%0Acore%20of%20our%20method%20is%20a%20shared%20feature%20adapter%2C%20trained%20collaboratively%20across%0Aclients%20to%20adapt%20features%20from%20foundation%20models%2C%20and%20a%20shared%20slot%20attention%0Amodule%20that%20learns%20to%20reconstruct%20the%20adapted%20features.%20To%20optimize%20this%0Aadapter%2C%20we%20design%20a%20two-branch%20student-teacher%20architecture.%20In%20each%20client%2C%20a%0Astudent%20decoder%20learns%20to%20reconstruct%20full%20features%20from%20foundation%20models%2C%0Awhile%20a%20teacher%20decoder%20reconstructs%20their%20adapted%2C%20low-dimensional%0Acounterpart.%20The%20shared%20slot%20attention%20module%20bridges%20cross-domain%20learning%20by%0Aaligning%20object-level%20representations%20across%20clients.%20Experiments%20in%20multiple%0Areal-world%20datasets%20show%20that%20our%20framework%20not%20only%20outperforms%20centralized%0Abaselines%20on%20object%20discovery%20but%20also%20learns%20a%20compact%2C%20universal%0Arepresentation%20that%20generalizes%20well%20across%20domains.%20This%20work%20highlights%0Afederated%20slot%20attention%20as%20an%20effective%20tool%20for%20scalable%2C%20unsupervised%20visual%0Arepresentation%20learning%20from%20cross-domain%20data%20with%20distributed%20concepts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02964v1&entry.124074799=Read"},
{"title": "Pan-Arctic Permafrost Landform and Human-built Infrastructure Feature\n  Detection with Vision Transformers and Location Embeddings", "author": "Amal S. Perera and David Fernandez and Chandi Witharana and Elias Manos and Michael Pimenta and Anna K. Liljedahl and Ingmar Nitze and Yili Yang and Todd Nicholson and Chia-Yu Hsu and Wenwen Li and Guido Grosse", "abstract": "  Accurate mapping of permafrost landforms, thaw disturbances, and human-built\ninfrastructure at pan-Arctic scale using sub-meter satellite imagery is\nincreasingly critical. Handling petabyte-scale image data requires\nhigh-performance computing and robust feature detection models. While\nconvolutional neural network (CNN)-based deep learning approaches are widely\nused for remote sensing (RS),similar to the success in transformer based large\nlanguage models, Vision Transformers (ViTs) offer advantages in capturing\nlong-range dependencies and global context via attention mechanisms. ViTs\nsupport pretraining via self-supervised learning-addressing the common\nlimitation of labeled data in Arctic feature detection and outperform CNNs on\nbenchmark datasets. Arctic also poses challenges for model generalization,\nespecially when features with the same semantic class exhibit diverse spectral\ncharacteristics. To address these issues for Arctic feature detection, we\nintegrate geospatial location embeddings into ViTs to improve adaptation across\nregions. This work investigates: (1) the suitability of pre-trained ViTs as\nfeature extractors for high-resolution Arctic remote sensing tasks, and (2) the\nbenefit of combining image and location embeddings. Using previously published\ndatasets for Arctic feature detection, we evaluate our models on three\ntasks-detecting ice-wedge polygons (IWP), retrogressive thaw slumps (RTS), and\nhuman-built infrastructure. We empirically explore multiple configurations to\nfuse image embeddings and location embeddings. Results show that ViTs with\nlocation embeddings outperform prior CNN-based models on two of the three tasks\nincluding F1 score increase from 0.84 to 0.92 for RTS detection, demonstrating\nthe potential of transformer-based models with spatial awareness for Arctic RS\napplications.\n", "link": "http://arxiv.org/abs/2506.02868v1", "date": "2025-06-03", "relevancy": 2.7115, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5581}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5344}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pan-Arctic%20Permafrost%20Landform%20and%20Human-built%20Infrastructure%20Feature%0A%20%20Detection%20with%20Vision%20Transformers%20and%20Location%20Embeddings&body=Title%3A%20Pan-Arctic%20Permafrost%20Landform%20and%20Human-built%20Infrastructure%20Feature%0A%20%20Detection%20with%20Vision%20Transformers%20and%20Location%20Embeddings%0AAuthor%3A%20Amal%20S.%20Perera%20and%20David%20Fernandez%20and%20Chandi%20Witharana%20and%20Elias%20Manos%20and%20Michael%20Pimenta%20and%20Anna%20K.%20Liljedahl%20and%20Ingmar%20Nitze%20and%20Yili%20Yang%20and%20Todd%20Nicholson%20and%20Chia-Yu%20Hsu%20and%20Wenwen%20Li%20and%20Guido%20Grosse%0AAbstract%3A%20%20%20Accurate%20mapping%20of%20permafrost%20landforms%2C%20thaw%20disturbances%2C%20and%20human-built%0Ainfrastructure%20at%20pan-Arctic%20scale%20using%20sub-meter%20satellite%20imagery%20is%0Aincreasingly%20critical.%20Handling%20petabyte-scale%20image%20data%20requires%0Ahigh-performance%20computing%20and%20robust%20feature%20detection%20models.%20While%0Aconvolutional%20neural%20network%20%28CNN%29-based%20deep%20learning%20approaches%20are%20widely%0Aused%20for%20remote%20sensing%20%28RS%29%2Csimilar%20to%20the%20success%20in%20transformer%20based%20large%0Alanguage%20models%2C%20Vision%20Transformers%20%28ViTs%29%20offer%20advantages%20in%20capturing%0Along-range%20dependencies%20and%20global%20context%20via%20attention%20mechanisms.%20ViTs%0Asupport%20pretraining%20via%20self-supervised%20learning-addressing%20the%20common%0Alimitation%20of%20labeled%20data%20in%20Arctic%20feature%20detection%20and%20outperform%20CNNs%20on%0Abenchmark%20datasets.%20Arctic%20also%20poses%20challenges%20for%20model%20generalization%2C%0Aespecially%20when%20features%20with%20the%20same%20semantic%20class%20exhibit%20diverse%20spectral%0Acharacteristics.%20To%20address%20these%20issues%20for%20Arctic%20feature%20detection%2C%20we%0Aintegrate%20geospatial%20location%20embeddings%20into%20ViTs%20to%20improve%20adaptation%20across%0Aregions.%20This%20work%20investigates%3A%20%281%29%20the%20suitability%20of%20pre-trained%20ViTs%20as%0Afeature%20extractors%20for%20high-resolution%20Arctic%20remote%20sensing%20tasks%2C%20and%20%282%29%20the%0Abenefit%20of%20combining%20image%20and%20location%20embeddings.%20Using%20previously%20published%0Adatasets%20for%20Arctic%20feature%20detection%2C%20we%20evaluate%20our%20models%20on%20three%0Atasks-detecting%20ice-wedge%20polygons%20%28IWP%29%2C%20retrogressive%20thaw%20slumps%20%28RTS%29%2C%20and%0Ahuman-built%20infrastructure.%20We%20empirically%20explore%20multiple%20configurations%20to%0Afuse%20image%20embeddings%20and%20location%20embeddings.%20Results%20show%20that%20ViTs%20with%0Alocation%20embeddings%20outperform%20prior%20CNN-based%20models%20on%20two%20of%20the%20three%20tasks%0Aincluding%20F1%20score%20increase%20from%200.84%20to%200.92%20for%20RTS%20detection%2C%20demonstrating%0Athe%20potential%20of%20transformer-based%20models%20with%20spatial%20awareness%20for%20Arctic%20RS%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02868v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPan-Arctic%2520Permafrost%2520Landform%2520and%2520Human-built%2520Infrastructure%2520Feature%250A%2520%2520Detection%2520with%2520Vision%2520Transformers%2520and%2520Location%2520Embeddings%26entry.906535625%3DAmal%2520S.%2520Perera%2520and%2520David%2520Fernandez%2520and%2520Chandi%2520Witharana%2520and%2520Elias%2520Manos%2520and%2520Michael%2520Pimenta%2520and%2520Anna%2520K.%2520Liljedahl%2520and%2520Ingmar%2520Nitze%2520and%2520Yili%2520Yang%2520and%2520Todd%2520Nicholson%2520and%2520Chia-Yu%2520Hsu%2520and%2520Wenwen%2520Li%2520and%2520Guido%2520Grosse%26entry.1292438233%3D%2520%2520Accurate%2520mapping%2520of%2520permafrost%2520landforms%252C%2520thaw%2520disturbances%252C%2520and%2520human-built%250Ainfrastructure%2520at%2520pan-Arctic%2520scale%2520using%2520sub-meter%2520satellite%2520imagery%2520is%250Aincreasingly%2520critical.%2520Handling%2520petabyte-scale%2520image%2520data%2520requires%250Ahigh-performance%2520computing%2520and%2520robust%2520feature%2520detection%2520models.%2520While%250Aconvolutional%2520neural%2520network%2520%2528CNN%2529-based%2520deep%2520learning%2520approaches%2520are%2520widely%250Aused%2520for%2520remote%2520sensing%2520%2528RS%2529%252Csimilar%2520to%2520the%2520success%2520in%2520transformer%2520based%2520large%250Alanguage%2520models%252C%2520Vision%2520Transformers%2520%2528ViTs%2529%2520offer%2520advantages%2520in%2520capturing%250Along-range%2520dependencies%2520and%2520global%2520context%2520via%2520attention%2520mechanisms.%2520ViTs%250Asupport%2520pretraining%2520via%2520self-supervised%2520learning-addressing%2520the%2520common%250Alimitation%2520of%2520labeled%2520data%2520in%2520Arctic%2520feature%2520detection%2520and%2520outperform%2520CNNs%2520on%250Abenchmark%2520datasets.%2520Arctic%2520also%2520poses%2520challenges%2520for%2520model%2520generalization%252C%250Aespecially%2520when%2520features%2520with%2520the%2520same%2520semantic%2520class%2520exhibit%2520diverse%2520spectral%250Acharacteristics.%2520To%2520address%2520these%2520issues%2520for%2520Arctic%2520feature%2520detection%252C%2520we%250Aintegrate%2520geospatial%2520location%2520embeddings%2520into%2520ViTs%2520to%2520improve%2520adaptation%2520across%250Aregions.%2520This%2520work%2520investigates%253A%2520%25281%2529%2520the%2520suitability%2520of%2520pre-trained%2520ViTs%2520as%250Afeature%2520extractors%2520for%2520high-resolution%2520Arctic%2520remote%2520sensing%2520tasks%252C%2520and%2520%25282%2529%2520the%250Abenefit%2520of%2520combining%2520image%2520and%2520location%2520embeddings.%2520Using%2520previously%2520published%250Adatasets%2520for%2520Arctic%2520feature%2520detection%252C%2520we%2520evaluate%2520our%2520models%2520on%2520three%250Atasks-detecting%2520ice-wedge%2520polygons%2520%2528IWP%2529%252C%2520retrogressive%2520thaw%2520slumps%2520%2528RTS%2529%252C%2520and%250Ahuman-built%2520infrastructure.%2520We%2520empirically%2520explore%2520multiple%2520configurations%2520to%250Afuse%2520image%2520embeddings%2520and%2520location%2520embeddings.%2520Results%2520show%2520that%2520ViTs%2520with%250Alocation%2520embeddings%2520outperform%2520prior%2520CNN-based%2520models%2520on%2520two%2520of%2520the%2520three%2520tasks%250Aincluding%2520F1%2520score%2520increase%2520from%25200.84%2520to%25200.92%2520for%2520RTS%2520detection%252C%2520demonstrating%250Athe%2520potential%2520of%2520transformer-based%2520models%2520with%2520spatial%2520awareness%2520for%2520Arctic%2520RS%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02868v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pan-Arctic%20Permafrost%20Landform%20and%20Human-built%20Infrastructure%20Feature%0A%20%20Detection%20with%20Vision%20Transformers%20and%20Location%20Embeddings&entry.906535625=Amal%20S.%20Perera%20and%20David%20Fernandez%20and%20Chandi%20Witharana%20and%20Elias%20Manos%20and%20Michael%20Pimenta%20and%20Anna%20K.%20Liljedahl%20and%20Ingmar%20Nitze%20and%20Yili%20Yang%20and%20Todd%20Nicholson%20and%20Chia-Yu%20Hsu%20and%20Wenwen%20Li%20and%20Guido%20Grosse&entry.1292438233=%20%20Accurate%20mapping%20of%20permafrost%20landforms%2C%20thaw%20disturbances%2C%20and%20human-built%0Ainfrastructure%20at%20pan-Arctic%20scale%20using%20sub-meter%20satellite%20imagery%20is%0Aincreasingly%20critical.%20Handling%20petabyte-scale%20image%20data%20requires%0Ahigh-performance%20computing%20and%20robust%20feature%20detection%20models.%20While%0Aconvolutional%20neural%20network%20%28CNN%29-based%20deep%20learning%20approaches%20are%20widely%0Aused%20for%20remote%20sensing%20%28RS%29%2Csimilar%20to%20the%20success%20in%20transformer%20based%20large%0Alanguage%20models%2C%20Vision%20Transformers%20%28ViTs%29%20offer%20advantages%20in%20capturing%0Along-range%20dependencies%20and%20global%20context%20via%20attention%20mechanisms.%20ViTs%0Asupport%20pretraining%20via%20self-supervised%20learning-addressing%20the%20common%0Alimitation%20of%20labeled%20data%20in%20Arctic%20feature%20detection%20and%20outperform%20CNNs%20on%0Abenchmark%20datasets.%20Arctic%20also%20poses%20challenges%20for%20model%20generalization%2C%0Aespecially%20when%20features%20with%20the%20same%20semantic%20class%20exhibit%20diverse%20spectral%0Acharacteristics.%20To%20address%20these%20issues%20for%20Arctic%20feature%20detection%2C%20we%0Aintegrate%20geospatial%20location%20embeddings%20into%20ViTs%20to%20improve%20adaptation%20across%0Aregions.%20This%20work%20investigates%3A%20%281%29%20the%20suitability%20of%20pre-trained%20ViTs%20as%0Afeature%20extractors%20for%20high-resolution%20Arctic%20remote%20sensing%20tasks%2C%20and%20%282%29%20the%0Abenefit%20of%20combining%20image%20and%20location%20embeddings.%20Using%20previously%20published%0Adatasets%20for%20Arctic%20feature%20detection%2C%20we%20evaluate%20our%20models%20on%20three%0Atasks-detecting%20ice-wedge%20polygons%20%28IWP%29%2C%20retrogressive%20thaw%20slumps%20%28RTS%29%2C%20and%0Ahuman-built%20infrastructure.%20We%20empirically%20explore%20multiple%20configurations%20to%0Afuse%20image%20embeddings%20and%20location%20embeddings.%20Results%20show%20that%20ViTs%20with%0Alocation%20embeddings%20outperform%20prior%20CNN-based%20models%20on%20two%20of%20the%20three%20tasks%0Aincluding%20F1%20score%20increase%20from%200.84%20to%200.92%20for%20RTS%20detection%2C%20demonstrating%0Athe%20potential%20of%20transformer-based%20models%20with%20spatial%20awareness%20for%20Arctic%20RS%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02868v1&entry.124074799=Read"},
{"title": "VolTex: Food Volume Estimation using Text-Guided Segmentation and Neural\n  Surface Reconstruction", "author": "Ahmad AlMughrabi and Umair Haroon and Ricardo Marques and Petia Radeva", "abstract": "  Accurate food volume estimation is crucial for dietary monitoring, medical\nnutrition management, and food intake analysis. Existing 3D Food Volume\nestimation methods accurately compute the food volume but lack for food\nportions selection. We present VolTex, a framework that improves \\change{the\nfood object selection} in food volume estimation. Allowing users to specify a\ntarget food item via text input to be segmented, our method enables the precise\nselection of specific food objects in real-world scenes. The segmented object\nis then reconstructed using the Neural Surface Reconstruction method to\ngenerate high-fidelity 3D meshes for volume computation. Extensive evaluations\non the MetaFood3D dataset demonstrate the effectiveness of our approach in\nisolating and reconstructing food items for accurate volume estimation. The\nsource code is accessible at https://github.com/GCVCG/VolTex.\n", "link": "http://arxiv.org/abs/2506.02895v1", "date": "2025-06-03", "relevancy": 2.6934, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.557}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5295}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5295}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VolTex%3A%20Food%20Volume%20Estimation%20using%20Text-Guided%20Segmentation%20and%20Neural%0A%20%20Surface%20Reconstruction&body=Title%3A%20VolTex%3A%20Food%20Volume%20Estimation%20using%20Text-Guided%20Segmentation%20and%20Neural%0A%20%20Surface%20Reconstruction%0AAuthor%3A%20Ahmad%20AlMughrabi%20and%20Umair%20Haroon%20and%20Ricardo%20Marques%20and%20Petia%20Radeva%0AAbstract%3A%20%20%20Accurate%20food%20volume%20estimation%20is%20crucial%20for%20dietary%20monitoring%2C%20medical%0Anutrition%20management%2C%20and%20food%20intake%20analysis.%20Existing%203D%20Food%20Volume%0Aestimation%20methods%20accurately%20compute%20the%20food%20volume%20but%20lack%20for%20food%0Aportions%20selection.%20We%20present%20VolTex%2C%20a%20framework%20that%20improves%20%5Cchange%7Bthe%0Afood%20object%20selection%7D%20in%20food%20volume%20estimation.%20Allowing%20users%20to%20specify%20a%0Atarget%20food%20item%20via%20text%20input%20to%20be%20segmented%2C%20our%20method%20enables%20the%20precise%0Aselection%20of%20specific%20food%20objects%20in%20real-world%20scenes.%20The%20segmented%20object%0Ais%20then%20reconstructed%20using%20the%20Neural%20Surface%20Reconstruction%20method%20to%0Agenerate%20high-fidelity%203D%20meshes%20for%20volume%20computation.%20Extensive%20evaluations%0Aon%20the%20MetaFood3D%20dataset%20demonstrate%20the%20effectiveness%20of%20our%20approach%20in%0Aisolating%20and%20reconstructing%20food%20items%20for%20accurate%20volume%20estimation.%20The%0Asource%20code%20is%20accessible%20at%20https%3A//github.com/GCVCG/VolTex.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02895v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVolTex%253A%2520Food%2520Volume%2520Estimation%2520using%2520Text-Guided%2520Segmentation%2520and%2520Neural%250A%2520%2520Surface%2520Reconstruction%26entry.906535625%3DAhmad%2520AlMughrabi%2520and%2520Umair%2520Haroon%2520and%2520Ricardo%2520Marques%2520and%2520Petia%2520Radeva%26entry.1292438233%3D%2520%2520Accurate%2520food%2520volume%2520estimation%2520is%2520crucial%2520for%2520dietary%2520monitoring%252C%2520medical%250Anutrition%2520management%252C%2520and%2520food%2520intake%2520analysis.%2520Existing%25203D%2520Food%2520Volume%250Aestimation%2520methods%2520accurately%2520compute%2520the%2520food%2520volume%2520but%2520lack%2520for%2520food%250Aportions%2520selection.%2520We%2520present%2520VolTex%252C%2520a%2520framework%2520that%2520improves%2520%255Cchange%257Bthe%250Afood%2520object%2520selection%257D%2520in%2520food%2520volume%2520estimation.%2520Allowing%2520users%2520to%2520specify%2520a%250Atarget%2520food%2520item%2520via%2520text%2520input%2520to%2520be%2520segmented%252C%2520our%2520method%2520enables%2520the%2520precise%250Aselection%2520of%2520specific%2520food%2520objects%2520in%2520real-world%2520scenes.%2520The%2520segmented%2520object%250Ais%2520then%2520reconstructed%2520using%2520the%2520Neural%2520Surface%2520Reconstruction%2520method%2520to%250Agenerate%2520high-fidelity%25203D%2520meshes%2520for%2520volume%2520computation.%2520Extensive%2520evaluations%250Aon%2520the%2520MetaFood3D%2520dataset%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520in%250Aisolating%2520and%2520reconstructing%2520food%2520items%2520for%2520accurate%2520volume%2520estimation.%2520The%250Asource%2520code%2520is%2520accessible%2520at%2520https%253A//github.com/GCVCG/VolTex.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02895v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VolTex%3A%20Food%20Volume%20Estimation%20using%20Text-Guided%20Segmentation%20and%20Neural%0A%20%20Surface%20Reconstruction&entry.906535625=Ahmad%20AlMughrabi%20and%20Umair%20Haroon%20and%20Ricardo%20Marques%20and%20Petia%20Radeva&entry.1292438233=%20%20Accurate%20food%20volume%20estimation%20is%20crucial%20for%20dietary%20monitoring%2C%20medical%0Anutrition%20management%2C%20and%20food%20intake%20analysis.%20Existing%203D%20Food%20Volume%0Aestimation%20methods%20accurately%20compute%20the%20food%20volume%20but%20lack%20for%20food%0Aportions%20selection.%20We%20present%20VolTex%2C%20a%20framework%20that%20improves%20%5Cchange%7Bthe%0Afood%20object%20selection%7D%20in%20food%20volume%20estimation.%20Allowing%20users%20to%20specify%20a%0Atarget%20food%20item%20via%20text%20input%20to%20be%20segmented%2C%20our%20method%20enables%20the%20precise%0Aselection%20of%20specific%20food%20objects%20in%20real-world%20scenes.%20The%20segmented%20object%0Ais%20then%20reconstructed%20using%20the%20Neural%20Surface%20Reconstruction%20method%20to%0Agenerate%20high-fidelity%203D%20meshes%20for%20volume%20computation.%20Extensive%20evaluations%0Aon%20the%20MetaFood3D%20dataset%20demonstrate%20the%20effectiveness%20of%20our%20approach%20in%0Aisolating%20and%20reconstructing%20food%20items%20for%20accurate%20volume%20estimation.%20The%0Asource%20code%20is%20accessible%20at%20https%3A//github.com/GCVCG/VolTex.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02895v1&entry.124074799=Read"},
{"title": "Learning from True-False Labels via Multi-modal Prompt Retrieving", "author": "Zhongnian Li and Jinghao Xu and Peng Ying and Meng Wei and Xinzheng Xu", "abstract": "  Pre-trained Vision-Language Models (VLMs) exhibit strong zero-shot\nclassification abilities, demonstrating great potential for generating weakly\nsupervised labels. Unfortunately, existing weakly supervised learning methods\nare short of ability in generating accurate labels via VLMs. In this paper, we\npropose a novel weakly supervised labeling setting, namely True-False Labels\n(TFLs) which can achieve high accuracy when generated by VLMs. The TFL\nindicates whether an instance belongs to the label, which is randomly and\nuniformly sampled from the candidate label set. Specifically, we theoretically\nderive a risk-consistent estimator to explore and utilize the conditional\nprobability distribution information of TFLs. Besides, we propose a\nconvolutional-based Multi-modal Prompt Retrieving (MRP) method to bridge the\ngap between the knowledge of VLMs and target learning tasks. Experimental\nresults demonstrate the effectiveness of the proposed TFL setting and MRP\nlearning method. The code to reproduce the experiments is at\nhttps://github.com/Tranquilxu/TMP.\n", "link": "http://arxiv.org/abs/2405.15228v2", "date": "2025-06-03", "relevancy": 2.6752, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5752}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5196}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20True-False%20Labels%20via%20Multi-modal%20Prompt%20Retrieving&body=Title%3A%20Learning%20from%20True-False%20Labels%20via%20Multi-modal%20Prompt%20Retrieving%0AAuthor%3A%20Zhongnian%20Li%20and%20Jinghao%20Xu%20and%20Peng%20Ying%20and%20Meng%20Wei%20and%20Xinzheng%20Xu%0AAbstract%3A%20%20%20Pre-trained%20Vision-Language%20Models%20%28VLMs%29%20exhibit%20strong%20zero-shot%0Aclassification%20abilities%2C%20demonstrating%20great%20potential%20for%20generating%20weakly%0Asupervised%20labels.%20Unfortunately%2C%20existing%20weakly%20supervised%20learning%20methods%0Aare%20short%20of%20ability%20in%20generating%20accurate%20labels%20via%20VLMs.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20weakly%20supervised%20labeling%20setting%2C%20namely%20True-False%20Labels%0A%28TFLs%29%20which%20can%20achieve%20high%20accuracy%20when%20generated%20by%20VLMs.%20The%20TFL%0Aindicates%20whether%20an%20instance%20belongs%20to%20the%20label%2C%20which%20is%20randomly%20and%0Auniformly%20sampled%20from%20the%20candidate%20label%20set.%20Specifically%2C%20we%20theoretically%0Aderive%20a%20risk-consistent%20estimator%20to%20explore%20and%20utilize%20the%20conditional%0Aprobability%20distribution%20information%20of%20TFLs.%20Besides%2C%20we%20propose%20a%0Aconvolutional-based%20Multi-modal%20Prompt%20Retrieving%20%28MRP%29%20method%20to%20bridge%20the%0Agap%20between%20the%20knowledge%20of%20VLMs%20and%20target%20learning%20tasks.%20Experimental%0Aresults%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20TFL%20setting%20and%20MRP%0Alearning%20method.%20The%20code%20to%20reproduce%20the%20experiments%20is%20at%0Ahttps%3A//github.com/Tranquilxu/TMP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15228v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520True-False%2520Labels%2520via%2520Multi-modal%2520Prompt%2520Retrieving%26entry.906535625%3DZhongnian%2520Li%2520and%2520Jinghao%2520Xu%2520and%2520Peng%2520Ying%2520and%2520Meng%2520Wei%2520and%2520Xinzheng%2520Xu%26entry.1292438233%3D%2520%2520Pre-trained%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520exhibit%2520strong%2520zero-shot%250Aclassification%2520abilities%252C%2520demonstrating%2520great%2520potential%2520for%2520generating%2520weakly%250Asupervised%2520labels.%2520Unfortunately%252C%2520existing%2520weakly%2520supervised%2520learning%2520methods%250Aare%2520short%2520of%2520ability%2520in%2520generating%2520accurate%2520labels%2520via%2520VLMs.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520novel%2520weakly%2520supervised%2520labeling%2520setting%252C%2520namely%2520True-False%2520Labels%250A%2528TFLs%2529%2520which%2520can%2520achieve%2520high%2520accuracy%2520when%2520generated%2520by%2520VLMs.%2520The%2520TFL%250Aindicates%2520whether%2520an%2520instance%2520belongs%2520to%2520the%2520label%252C%2520which%2520is%2520randomly%2520and%250Auniformly%2520sampled%2520from%2520the%2520candidate%2520label%2520set.%2520Specifically%252C%2520we%2520theoretically%250Aderive%2520a%2520risk-consistent%2520estimator%2520to%2520explore%2520and%2520utilize%2520the%2520conditional%250Aprobability%2520distribution%2520information%2520of%2520TFLs.%2520Besides%252C%2520we%2520propose%2520a%250Aconvolutional-based%2520Multi-modal%2520Prompt%2520Retrieving%2520%2528MRP%2529%2520method%2520to%2520bridge%2520the%250Agap%2520between%2520the%2520knowledge%2520of%2520VLMs%2520and%2520target%2520learning%2520tasks.%2520Experimental%250Aresults%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520proposed%2520TFL%2520setting%2520and%2520MRP%250Alearning%2520method.%2520The%2520code%2520to%2520reproduce%2520the%2520experiments%2520is%2520at%250Ahttps%253A//github.com/Tranquilxu/TMP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15228v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20True-False%20Labels%20via%20Multi-modal%20Prompt%20Retrieving&entry.906535625=Zhongnian%20Li%20and%20Jinghao%20Xu%20and%20Peng%20Ying%20and%20Meng%20Wei%20and%20Xinzheng%20Xu&entry.1292438233=%20%20Pre-trained%20Vision-Language%20Models%20%28VLMs%29%20exhibit%20strong%20zero-shot%0Aclassification%20abilities%2C%20demonstrating%20great%20potential%20for%20generating%20weakly%0Asupervised%20labels.%20Unfortunately%2C%20existing%20weakly%20supervised%20learning%20methods%0Aare%20short%20of%20ability%20in%20generating%20accurate%20labels%20via%20VLMs.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20weakly%20supervised%20labeling%20setting%2C%20namely%20True-False%20Labels%0A%28TFLs%29%20which%20can%20achieve%20high%20accuracy%20when%20generated%20by%20VLMs.%20The%20TFL%0Aindicates%20whether%20an%20instance%20belongs%20to%20the%20label%2C%20which%20is%20randomly%20and%0Auniformly%20sampled%20from%20the%20candidate%20label%20set.%20Specifically%2C%20we%20theoretically%0Aderive%20a%20risk-consistent%20estimator%20to%20explore%20and%20utilize%20the%20conditional%0Aprobability%20distribution%20information%20of%20TFLs.%20Besides%2C%20we%20propose%20a%0Aconvolutional-based%20Multi-modal%20Prompt%20Retrieving%20%28MRP%29%20method%20to%20bridge%20the%0Agap%20between%20the%20knowledge%20of%20VLMs%20and%20target%20learning%20tasks.%20Experimental%0Aresults%20demonstrate%20the%20effectiveness%20of%20the%20proposed%20TFL%20setting%20and%20MRP%0Alearning%20method.%20The%20code%20to%20reproduce%20the%20experiments%20is%20at%0Ahttps%3A//github.com/Tranquilxu/TMP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15228v2&entry.124074799=Read"},
{"title": "T-FAKE: Synthesizing Thermal Images for Facial Landmarking", "author": "Philipp Flotho and Moritz Piening and Anna Kukleva and Gabriele Steidl", "abstract": "  Facial analysis is a key component in a wide range of applications such as\nhealthcare, autonomous driving, and entertainment. Despite the availability of\nvarious facial RGB datasets, the thermal modality, which plays a crucial role\nin life sciences, medicine, and biometrics, has been largely overlooked. To\naddress this gap, we introduce the T-FAKE dataset, a new large-scale synthetic\nthermal dataset with sparse and dense landmarks. To facilitate the creation of\nthe dataset, we propose a novel RGB2Thermal loss function, which enables the\ndomain-adaptive transfer of RGB faces to thermal style. By utilizing the\nWasserstein distance between thermal and RGB patches and the statistical\nanalysis of clinical temperature distributions on faces, we ensure that the\ngenerated thermal images closely resemble real samples. Using RGB2Thermal style\ntransfer based on our RGB2Thermal loss function, we create the large-scale\nsynthetic thermal T-FAKE dataset with landmark and segmentation annotations.\nLeveraging our novel T-FAKE dataset, probabilistic landmark prediction, and\nlabel adaptation networks, we demonstrate significant improvements in landmark\ndetection methods on thermal images across different landmark conventions. Our\nmodels show excellent performance with both sparse 70-point landmarks and dense\n478-point landmark annotations. Moreover, our RGB2Thermal loss leads to notable\nresults in terms of perceptual evaluation and temperature prediction.\n", "link": "http://arxiv.org/abs/2408.15127v3", "date": "2025-06-03", "relevancy": 2.6609, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5404}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5301}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T-FAKE%3A%20Synthesizing%20Thermal%20Images%20for%20Facial%20Landmarking&body=Title%3A%20T-FAKE%3A%20Synthesizing%20Thermal%20Images%20for%20Facial%20Landmarking%0AAuthor%3A%20Philipp%20Flotho%20and%20Moritz%20Piening%20and%20Anna%20Kukleva%20and%20Gabriele%20Steidl%0AAbstract%3A%20%20%20Facial%20analysis%20is%20a%20key%20component%20in%20a%20wide%20range%20of%20applications%20such%20as%0Ahealthcare%2C%20autonomous%20driving%2C%20and%20entertainment.%20Despite%20the%20availability%20of%0Avarious%20facial%20RGB%20datasets%2C%20the%20thermal%20modality%2C%20which%20plays%20a%20crucial%20role%0Ain%20life%20sciences%2C%20medicine%2C%20and%20biometrics%2C%20has%20been%20largely%20overlooked.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20the%20T-FAKE%20dataset%2C%20a%20new%20large-scale%20synthetic%0Athermal%20dataset%20with%20sparse%20and%20dense%20landmarks.%20To%20facilitate%20the%20creation%20of%0Athe%20dataset%2C%20we%20propose%20a%20novel%20RGB2Thermal%20loss%20function%2C%20which%20enables%20the%0Adomain-adaptive%20transfer%20of%20RGB%20faces%20to%20thermal%20style.%20By%20utilizing%20the%0AWasserstein%20distance%20between%20thermal%20and%20RGB%20patches%20and%20the%20statistical%0Aanalysis%20of%20clinical%20temperature%20distributions%20on%20faces%2C%20we%20ensure%20that%20the%0Agenerated%20thermal%20images%20closely%20resemble%20real%20samples.%20Using%20RGB2Thermal%20style%0Atransfer%20based%20on%20our%20RGB2Thermal%20loss%20function%2C%20we%20create%20the%20large-scale%0Asynthetic%20thermal%20T-FAKE%20dataset%20with%20landmark%20and%20segmentation%20annotations.%0ALeveraging%20our%20novel%20T-FAKE%20dataset%2C%20probabilistic%20landmark%20prediction%2C%20and%0Alabel%20adaptation%20networks%2C%20we%20demonstrate%20significant%20improvements%20in%20landmark%0Adetection%20methods%20on%20thermal%20images%20across%20different%20landmark%20conventions.%20Our%0Amodels%20show%20excellent%20performance%20with%20both%20sparse%2070-point%20landmarks%20and%20dense%0A478-point%20landmark%20annotations.%20Moreover%2C%20our%20RGB2Thermal%20loss%20leads%20to%20notable%0Aresults%20in%20terms%20of%20perceptual%20evaluation%20and%20temperature%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15127v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT-FAKE%253A%2520Synthesizing%2520Thermal%2520Images%2520for%2520Facial%2520Landmarking%26entry.906535625%3DPhilipp%2520Flotho%2520and%2520Moritz%2520Piening%2520and%2520Anna%2520Kukleva%2520and%2520Gabriele%2520Steidl%26entry.1292438233%3D%2520%2520Facial%2520analysis%2520is%2520a%2520key%2520component%2520in%2520a%2520wide%2520range%2520of%2520applications%2520such%2520as%250Ahealthcare%252C%2520autonomous%2520driving%252C%2520and%2520entertainment.%2520Despite%2520the%2520availability%2520of%250Avarious%2520facial%2520RGB%2520datasets%252C%2520the%2520thermal%2520modality%252C%2520which%2520plays%2520a%2520crucial%2520role%250Ain%2520life%2520sciences%252C%2520medicine%252C%2520and%2520biometrics%252C%2520has%2520been%2520largely%2520overlooked.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520introduce%2520the%2520T-FAKE%2520dataset%252C%2520a%2520new%2520large-scale%2520synthetic%250Athermal%2520dataset%2520with%2520sparse%2520and%2520dense%2520landmarks.%2520To%2520facilitate%2520the%2520creation%2520of%250Athe%2520dataset%252C%2520we%2520propose%2520a%2520novel%2520RGB2Thermal%2520loss%2520function%252C%2520which%2520enables%2520the%250Adomain-adaptive%2520transfer%2520of%2520RGB%2520faces%2520to%2520thermal%2520style.%2520By%2520utilizing%2520the%250AWasserstein%2520distance%2520between%2520thermal%2520and%2520RGB%2520patches%2520and%2520the%2520statistical%250Aanalysis%2520of%2520clinical%2520temperature%2520distributions%2520on%2520faces%252C%2520we%2520ensure%2520that%2520the%250Agenerated%2520thermal%2520images%2520closely%2520resemble%2520real%2520samples.%2520Using%2520RGB2Thermal%2520style%250Atransfer%2520based%2520on%2520our%2520RGB2Thermal%2520loss%2520function%252C%2520we%2520create%2520the%2520large-scale%250Asynthetic%2520thermal%2520T-FAKE%2520dataset%2520with%2520landmark%2520and%2520segmentation%2520annotations.%250ALeveraging%2520our%2520novel%2520T-FAKE%2520dataset%252C%2520probabilistic%2520landmark%2520prediction%252C%2520and%250Alabel%2520adaptation%2520networks%252C%2520we%2520demonstrate%2520significant%2520improvements%2520in%2520landmark%250Adetection%2520methods%2520on%2520thermal%2520images%2520across%2520different%2520landmark%2520conventions.%2520Our%250Amodels%2520show%2520excellent%2520performance%2520with%2520both%2520sparse%252070-point%2520landmarks%2520and%2520dense%250A478-point%2520landmark%2520annotations.%2520Moreover%252C%2520our%2520RGB2Thermal%2520loss%2520leads%2520to%2520notable%250Aresults%2520in%2520terms%2520of%2520perceptual%2520evaluation%2520and%2520temperature%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15127v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T-FAKE%3A%20Synthesizing%20Thermal%20Images%20for%20Facial%20Landmarking&entry.906535625=Philipp%20Flotho%20and%20Moritz%20Piening%20and%20Anna%20Kukleva%20and%20Gabriele%20Steidl&entry.1292438233=%20%20Facial%20analysis%20is%20a%20key%20component%20in%20a%20wide%20range%20of%20applications%20such%20as%0Ahealthcare%2C%20autonomous%20driving%2C%20and%20entertainment.%20Despite%20the%20availability%20of%0Avarious%20facial%20RGB%20datasets%2C%20the%20thermal%20modality%2C%20which%20plays%20a%20crucial%20role%0Ain%20life%20sciences%2C%20medicine%2C%20and%20biometrics%2C%20has%20been%20largely%20overlooked.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20the%20T-FAKE%20dataset%2C%20a%20new%20large-scale%20synthetic%0Athermal%20dataset%20with%20sparse%20and%20dense%20landmarks.%20To%20facilitate%20the%20creation%20of%0Athe%20dataset%2C%20we%20propose%20a%20novel%20RGB2Thermal%20loss%20function%2C%20which%20enables%20the%0Adomain-adaptive%20transfer%20of%20RGB%20faces%20to%20thermal%20style.%20By%20utilizing%20the%0AWasserstein%20distance%20between%20thermal%20and%20RGB%20patches%20and%20the%20statistical%0Aanalysis%20of%20clinical%20temperature%20distributions%20on%20faces%2C%20we%20ensure%20that%20the%0Agenerated%20thermal%20images%20closely%20resemble%20real%20samples.%20Using%20RGB2Thermal%20style%0Atransfer%20based%20on%20our%20RGB2Thermal%20loss%20function%2C%20we%20create%20the%20large-scale%0Asynthetic%20thermal%20T-FAKE%20dataset%20with%20landmark%20and%20segmentation%20annotations.%0ALeveraging%20our%20novel%20T-FAKE%20dataset%2C%20probabilistic%20landmark%20prediction%2C%20and%0Alabel%20adaptation%20networks%2C%20we%20demonstrate%20significant%20improvements%20in%20landmark%0Adetection%20methods%20on%20thermal%20images%20across%20different%20landmark%20conventions.%20Our%0Amodels%20show%20excellent%20performance%20with%20both%20sparse%2070-point%20landmarks%20and%20dense%0A478-point%20landmark%20annotations.%20Moreover%2C%20our%20RGB2Thermal%20loss%20leads%20to%20notable%0Aresults%20in%20terms%20of%20perceptual%20evaluation%20and%20temperature%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15127v3&entry.124074799=Read"},
{"title": "SubTrack++ : Gradient Subspace Tracking for Scalable LLM Training", "author": "Sahar Rajabi and Nayeema Nonta and Sirisha Rambhatla", "abstract": "  Training large language models (LLMs) is highly resource-intensive due to\ntheir massive number of parameters and the overhead of optimizer states. While\nrecent work has aimed to reduce memory consumption, such efforts often entail\ntrade-offs among memory efficiency, training time, and model performance. Yet,\ntrue democratization of LLMs requires simultaneous progress across all three\ndimensions. To this end, we propose SubTrack++ that leverages Grassmannian\ngradient subspace tracking combined with projection-aware optimizers, enabling\nAdam's internal statistics to adapt to changes in the optimization subspace.\nAdditionally, employing recovery scaling, a technique that restores information\nlost through low-rank projections, further enhances model performance. Our\nmethod demonstrates SOTA convergence by exploiting Grassmannian geometry and\nachieves lowest evaluation loss, outperforming the current SOTA while reducing\npretraining wall time by 43% and maintaining the memory footprint on a\n1B-parameter Llama model.\n", "link": "http://arxiv.org/abs/2502.01586v2", "date": "2025-06-03", "relevancy": 2.657, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5539}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5248}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SubTrack%2B%2B%20%3A%20Gradient%20Subspace%20Tracking%20for%20Scalable%20LLM%20Training&body=Title%3A%20SubTrack%2B%2B%20%3A%20Gradient%20Subspace%20Tracking%20for%20Scalable%20LLM%20Training%0AAuthor%3A%20Sahar%20Rajabi%20and%20Nayeema%20Nonta%20and%20Sirisha%20Rambhatla%0AAbstract%3A%20%20%20Training%20large%20language%20models%20%28LLMs%29%20is%20highly%20resource-intensive%20due%20to%0Atheir%20massive%20number%20of%20parameters%20and%20the%20overhead%20of%20optimizer%20states.%20While%0Arecent%20work%20has%20aimed%20to%20reduce%20memory%20consumption%2C%20such%20efforts%20often%20entail%0Atrade-offs%20among%20memory%20efficiency%2C%20training%20time%2C%20and%20model%20performance.%20Yet%2C%0Atrue%20democratization%20of%20LLMs%20requires%20simultaneous%20progress%20across%20all%20three%0Adimensions.%20To%20this%20end%2C%20we%20propose%20SubTrack%2B%2B%20that%20leverages%20Grassmannian%0Agradient%20subspace%20tracking%20combined%20with%20projection-aware%20optimizers%2C%20enabling%0AAdam%27s%20internal%20statistics%20to%20adapt%20to%20changes%20in%20the%20optimization%20subspace.%0AAdditionally%2C%20employing%20recovery%20scaling%2C%20a%20technique%20that%20restores%20information%0Alost%20through%20low-rank%20projections%2C%20further%20enhances%20model%20performance.%20Our%0Amethod%20demonstrates%20SOTA%20convergence%20by%20exploiting%20Grassmannian%20geometry%20and%0Aachieves%20lowest%20evaluation%20loss%2C%20outperforming%20the%20current%20SOTA%20while%20reducing%0Apretraining%20wall%20time%20by%2043%25%20and%20maintaining%20the%20memory%20footprint%20on%20a%0A1B-parameter%20Llama%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.01586v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSubTrack%252B%252B%2520%253A%2520Gradient%2520Subspace%2520Tracking%2520for%2520Scalable%2520LLM%2520Training%26entry.906535625%3DSahar%2520Rajabi%2520and%2520Nayeema%2520Nonta%2520and%2520Sirisha%2520Rambhatla%26entry.1292438233%3D%2520%2520Training%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520highly%2520resource-intensive%2520due%2520to%250Atheir%2520massive%2520number%2520of%2520parameters%2520and%2520the%2520overhead%2520of%2520optimizer%2520states.%2520While%250Arecent%2520work%2520has%2520aimed%2520to%2520reduce%2520memory%2520consumption%252C%2520such%2520efforts%2520often%2520entail%250Atrade-offs%2520among%2520memory%2520efficiency%252C%2520training%2520time%252C%2520and%2520model%2520performance.%2520Yet%252C%250Atrue%2520democratization%2520of%2520LLMs%2520requires%2520simultaneous%2520progress%2520across%2520all%2520three%250Adimensions.%2520To%2520this%2520end%252C%2520we%2520propose%2520SubTrack%252B%252B%2520that%2520leverages%2520Grassmannian%250Agradient%2520subspace%2520tracking%2520combined%2520with%2520projection-aware%2520optimizers%252C%2520enabling%250AAdam%2527s%2520internal%2520statistics%2520to%2520adapt%2520to%2520changes%2520in%2520the%2520optimization%2520subspace.%250AAdditionally%252C%2520employing%2520recovery%2520scaling%252C%2520a%2520technique%2520that%2520restores%2520information%250Alost%2520through%2520low-rank%2520projections%252C%2520further%2520enhances%2520model%2520performance.%2520Our%250Amethod%2520demonstrates%2520SOTA%2520convergence%2520by%2520exploiting%2520Grassmannian%2520geometry%2520and%250Aachieves%2520lowest%2520evaluation%2520loss%252C%2520outperforming%2520the%2520current%2520SOTA%2520while%2520reducing%250Apretraining%2520wall%2520time%2520by%252043%2525%2520and%2520maintaining%2520the%2520memory%2520footprint%2520on%2520a%250A1B-parameter%2520Llama%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.01586v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SubTrack%2B%2B%20%3A%20Gradient%20Subspace%20Tracking%20for%20Scalable%20LLM%20Training&entry.906535625=Sahar%20Rajabi%20and%20Nayeema%20Nonta%20and%20Sirisha%20Rambhatla&entry.1292438233=%20%20Training%20large%20language%20models%20%28LLMs%29%20is%20highly%20resource-intensive%20due%20to%0Atheir%20massive%20number%20of%20parameters%20and%20the%20overhead%20of%20optimizer%20states.%20While%0Arecent%20work%20has%20aimed%20to%20reduce%20memory%20consumption%2C%20such%20efforts%20often%20entail%0Atrade-offs%20among%20memory%20efficiency%2C%20training%20time%2C%20and%20model%20performance.%20Yet%2C%0Atrue%20democratization%20of%20LLMs%20requires%20simultaneous%20progress%20across%20all%20three%0Adimensions.%20To%20this%20end%2C%20we%20propose%20SubTrack%2B%2B%20that%20leverages%20Grassmannian%0Agradient%20subspace%20tracking%20combined%20with%20projection-aware%20optimizers%2C%20enabling%0AAdam%27s%20internal%20statistics%20to%20adapt%20to%20changes%20in%20the%20optimization%20subspace.%0AAdditionally%2C%20employing%20recovery%20scaling%2C%20a%20technique%20that%20restores%20information%0Alost%20through%20low-rank%20projections%2C%20further%20enhances%20model%20performance.%20Our%0Amethod%20demonstrates%20SOTA%20convergence%20by%20exploiting%20Grassmannian%20geometry%20and%0Aachieves%20lowest%20evaluation%20loss%2C%20outperforming%20the%20current%20SOTA%20while%20reducing%0Apretraining%20wall%20time%20by%2043%25%20and%20maintaining%20the%20memory%20footprint%20on%20a%0A1B-parameter%20Llama%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.01586v2&entry.124074799=Read"},
{"title": "PBR-SR: Mesh PBR Texture Super Resolution from 2D Image Priors", "author": "Yujin Chen and Yinyu Nie and Benjamin Ummenhofer and Reiner Birkl and Michael Paulitsch and Matthias Nie\u00dfner", "abstract": "  We present PBR-SR, a novel method for physically based rendering (PBR)\ntexture super resolution (SR). It outputs high-resolution, high-quality PBR\ntextures from low-resolution (LR) PBR input in a zero-shot manner. PBR-SR\nleverages an off-the-shelf super-resolution model trained on natural images,\nand iteratively minimizes the deviations between super-resolution priors and\ndifferentiable renderings. These enhancements are then back-projected into the\nPBR map space in a differentiable manner to produce refined, high-resolution\ntextures. To mitigate view inconsistencies and lighting sensitivity, which is\ncommon in view-based super-resolution, our method applies 2D prior constraints\nacross multi-view renderings, iteratively refining the shared, upscaled\ntextures. In parallel, we incorporate identity constraints directly in the PBR\ntexture domain to ensure the upscaled textures remain faithful to the LR input.\nPBR-SR operates without any additional training or data requirements, relying\nentirely on pretrained image priors. We demonstrate that our approach produces\nhigh-fidelity PBR textures for both artist-designed and AI-generated meshes,\noutperforming both direct SR models application and prior texture optimization\nmethods. Our results show high-quality outputs in both PBR and rendering\nevaluations, supporting advanced applications such as relighting.\n", "link": "http://arxiv.org/abs/2506.02846v1", "date": "2025-06-03", "relevancy": 2.6554, "topK": [{"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5406}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5371}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PBR-SR%3A%20Mesh%20PBR%20Texture%20Super%20Resolution%20from%202D%20Image%20Priors&body=Title%3A%20PBR-SR%3A%20Mesh%20PBR%20Texture%20Super%20Resolution%20from%202D%20Image%20Priors%0AAuthor%3A%20Yujin%20Chen%20and%20Yinyu%20Nie%20and%20Benjamin%20Ummenhofer%20and%20Reiner%20Birkl%20and%20Michael%20Paulitsch%20and%20Matthias%20Nie%C3%9Fner%0AAbstract%3A%20%20%20We%20present%20PBR-SR%2C%20a%20novel%20method%20for%20physically%20based%20rendering%20%28PBR%29%0Atexture%20super%20resolution%20%28SR%29.%20It%20outputs%20high-resolution%2C%20high-quality%20PBR%0Atextures%20from%20low-resolution%20%28LR%29%20PBR%20input%20in%20a%20zero-shot%20manner.%20PBR-SR%0Aleverages%20an%20off-the-shelf%20super-resolution%20model%20trained%20on%20natural%20images%2C%0Aand%20iteratively%20minimizes%20the%20deviations%20between%20super-resolution%20priors%20and%0Adifferentiable%20renderings.%20These%20enhancements%20are%20then%20back-projected%20into%20the%0APBR%20map%20space%20in%20a%20differentiable%20manner%20to%20produce%20refined%2C%20high-resolution%0Atextures.%20To%20mitigate%20view%20inconsistencies%20and%20lighting%20sensitivity%2C%20which%20is%0Acommon%20in%20view-based%20super-resolution%2C%20our%20method%20applies%202D%20prior%20constraints%0Aacross%20multi-view%20renderings%2C%20iteratively%20refining%20the%20shared%2C%20upscaled%0Atextures.%20In%20parallel%2C%20we%20incorporate%20identity%20constraints%20directly%20in%20the%20PBR%0Atexture%20domain%20to%20ensure%20the%20upscaled%20textures%20remain%20faithful%20to%20the%20LR%20input.%0APBR-SR%20operates%20without%20any%20additional%20training%20or%20data%20requirements%2C%20relying%0Aentirely%20on%20pretrained%20image%20priors.%20We%20demonstrate%20that%20our%20approach%20produces%0Ahigh-fidelity%20PBR%20textures%20for%20both%20artist-designed%20and%20AI-generated%20meshes%2C%0Aoutperforming%20both%20direct%20SR%20models%20application%20and%20prior%20texture%20optimization%0Amethods.%20Our%20results%20show%20high-quality%20outputs%20in%20both%20PBR%20and%20rendering%0Aevaluations%2C%20supporting%20advanced%20applications%20such%20as%20relighting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02846v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPBR-SR%253A%2520Mesh%2520PBR%2520Texture%2520Super%2520Resolution%2520from%25202D%2520Image%2520Priors%26entry.906535625%3DYujin%2520Chen%2520and%2520Yinyu%2520Nie%2520and%2520Benjamin%2520Ummenhofer%2520and%2520Reiner%2520Birkl%2520and%2520Michael%2520Paulitsch%2520and%2520Matthias%2520Nie%25C3%259Fner%26entry.1292438233%3D%2520%2520We%2520present%2520PBR-SR%252C%2520a%2520novel%2520method%2520for%2520physically%2520based%2520rendering%2520%2528PBR%2529%250Atexture%2520super%2520resolution%2520%2528SR%2529.%2520It%2520outputs%2520high-resolution%252C%2520high-quality%2520PBR%250Atextures%2520from%2520low-resolution%2520%2528LR%2529%2520PBR%2520input%2520in%2520a%2520zero-shot%2520manner.%2520PBR-SR%250Aleverages%2520an%2520off-the-shelf%2520super-resolution%2520model%2520trained%2520on%2520natural%2520images%252C%250Aand%2520iteratively%2520minimizes%2520the%2520deviations%2520between%2520super-resolution%2520priors%2520and%250Adifferentiable%2520renderings.%2520These%2520enhancements%2520are%2520then%2520back-projected%2520into%2520the%250APBR%2520map%2520space%2520in%2520a%2520differentiable%2520manner%2520to%2520produce%2520refined%252C%2520high-resolution%250Atextures.%2520To%2520mitigate%2520view%2520inconsistencies%2520and%2520lighting%2520sensitivity%252C%2520which%2520is%250Acommon%2520in%2520view-based%2520super-resolution%252C%2520our%2520method%2520applies%25202D%2520prior%2520constraints%250Aacross%2520multi-view%2520renderings%252C%2520iteratively%2520refining%2520the%2520shared%252C%2520upscaled%250Atextures.%2520In%2520parallel%252C%2520we%2520incorporate%2520identity%2520constraints%2520directly%2520in%2520the%2520PBR%250Atexture%2520domain%2520to%2520ensure%2520the%2520upscaled%2520textures%2520remain%2520faithful%2520to%2520the%2520LR%2520input.%250APBR-SR%2520operates%2520without%2520any%2520additional%2520training%2520or%2520data%2520requirements%252C%2520relying%250Aentirely%2520on%2520pretrained%2520image%2520priors.%2520We%2520demonstrate%2520that%2520our%2520approach%2520produces%250Ahigh-fidelity%2520PBR%2520textures%2520for%2520both%2520artist-designed%2520and%2520AI-generated%2520meshes%252C%250Aoutperforming%2520both%2520direct%2520SR%2520models%2520application%2520and%2520prior%2520texture%2520optimization%250Amethods.%2520Our%2520results%2520show%2520high-quality%2520outputs%2520in%2520both%2520PBR%2520and%2520rendering%250Aevaluations%252C%2520supporting%2520advanced%2520applications%2520such%2520as%2520relighting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02846v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PBR-SR%3A%20Mesh%20PBR%20Texture%20Super%20Resolution%20from%202D%20Image%20Priors&entry.906535625=Yujin%20Chen%20and%20Yinyu%20Nie%20and%20Benjamin%20Ummenhofer%20and%20Reiner%20Birkl%20and%20Michael%20Paulitsch%20and%20Matthias%20Nie%C3%9Fner&entry.1292438233=%20%20We%20present%20PBR-SR%2C%20a%20novel%20method%20for%20physically%20based%20rendering%20%28PBR%29%0Atexture%20super%20resolution%20%28SR%29.%20It%20outputs%20high-resolution%2C%20high-quality%20PBR%0Atextures%20from%20low-resolution%20%28LR%29%20PBR%20input%20in%20a%20zero-shot%20manner.%20PBR-SR%0Aleverages%20an%20off-the-shelf%20super-resolution%20model%20trained%20on%20natural%20images%2C%0Aand%20iteratively%20minimizes%20the%20deviations%20between%20super-resolution%20priors%20and%0Adifferentiable%20renderings.%20These%20enhancements%20are%20then%20back-projected%20into%20the%0APBR%20map%20space%20in%20a%20differentiable%20manner%20to%20produce%20refined%2C%20high-resolution%0Atextures.%20To%20mitigate%20view%20inconsistencies%20and%20lighting%20sensitivity%2C%20which%20is%0Acommon%20in%20view-based%20super-resolution%2C%20our%20method%20applies%202D%20prior%20constraints%0Aacross%20multi-view%20renderings%2C%20iteratively%20refining%20the%20shared%2C%20upscaled%0Atextures.%20In%20parallel%2C%20we%20incorporate%20identity%20constraints%20directly%20in%20the%20PBR%0Atexture%20domain%20to%20ensure%20the%20upscaled%20textures%20remain%20faithful%20to%20the%20LR%20input.%0APBR-SR%20operates%20without%20any%20additional%20training%20or%20data%20requirements%2C%20relying%0Aentirely%20on%20pretrained%20image%20priors.%20We%20demonstrate%20that%20our%20approach%20produces%0Ahigh-fidelity%20PBR%20textures%20for%20both%20artist-designed%20and%20AI-generated%20meshes%2C%0Aoutperforming%20both%20direct%20SR%20models%20application%20and%20prior%20texture%20optimization%0Amethods.%20Our%20results%20show%20high-quality%20outputs%20in%20both%20PBR%20and%20rendering%0Aevaluations%2C%20supporting%20advanced%20applications%20such%20as%20relighting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02846v1&entry.124074799=Read"},
{"title": "Emergent Abilities of Large Language Models under Continued Pretraining\n  for Language Adaptation", "author": "Ahmed Elhady and Eneko Agirre and Mikel Artetxe", "abstract": "  Continued pretraining (CPT) is a popular approach to adapt existing large\nlanguage models (LLMs) to new languages. When doing so, it is common practice\nto include a portion of English data in the mixture, but its role has not been\ncarefully studied to date. In this work, we show that including English does\nnot impact validation perplexity, yet it is critical for the emergence of\ndownstream capabilities in the target language. We introduce a\nlanguage-agnostic benchmark for in-context learning (ICL), which reveals\ncatastrophic forgetting early on CPT when English is not included. This in turn\ndamages the ability of the model to generalize to downstream prompts in the\ntarget language as measured by perplexity, even if it does not manifest in\nterms of accuracy until later in training, and can be tied to a big shift in\nthe model parameters. Based on these insights, we introduce curriculum learning\nand exponential moving average (EMA) of weights as effective alternatives to\nmitigate the need for English. All in all, our work sheds light into the\ndynamics by which emergent abilities arise when doing CPT for language\nadaptation, and can serve as a foundation to design more effective methods in\nthe future.\n", "link": "http://arxiv.org/abs/2506.00288v2", "date": "2025-06-03", "relevancy": 2.653, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5504}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5207}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5207}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emergent%20Abilities%20of%20Large%20Language%20Models%20under%20Continued%20Pretraining%0A%20%20for%20Language%20Adaptation&body=Title%3A%20Emergent%20Abilities%20of%20Large%20Language%20Models%20under%20Continued%20Pretraining%0A%20%20for%20Language%20Adaptation%0AAuthor%3A%20Ahmed%20Elhady%20and%20Eneko%20Agirre%20and%20Mikel%20Artetxe%0AAbstract%3A%20%20%20Continued%20pretraining%20%28CPT%29%20is%20a%20popular%20approach%20to%20adapt%20existing%20large%0Alanguage%20models%20%28LLMs%29%20to%20new%20languages.%20When%20doing%20so%2C%20it%20is%20common%20practice%0Ato%20include%20a%20portion%20of%20English%20data%20in%20the%20mixture%2C%20but%20its%20role%20has%20not%20been%0Acarefully%20studied%20to%20date.%20In%20this%20work%2C%20we%20show%20that%20including%20English%20does%0Anot%20impact%20validation%20perplexity%2C%20yet%20it%20is%20critical%20for%20the%20emergence%20of%0Adownstream%20capabilities%20in%20the%20target%20language.%20We%20introduce%20a%0Alanguage-agnostic%20benchmark%20for%20in-context%20learning%20%28ICL%29%2C%20which%20reveals%0Acatastrophic%20forgetting%20early%20on%20CPT%20when%20English%20is%20not%20included.%20This%20in%20turn%0Adamages%20the%20ability%20of%20the%20model%20to%20generalize%20to%20downstream%20prompts%20in%20the%0Atarget%20language%20as%20measured%20by%20perplexity%2C%20even%20if%20it%20does%20not%20manifest%20in%0Aterms%20of%20accuracy%20until%20later%20in%20training%2C%20and%20can%20be%20tied%20to%20a%20big%20shift%20in%0Athe%20model%20parameters.%20Based%20on%20these%20insights%2C%20we%20introduce%20curriculum%20learning%0Aand%20exponential%20moving%20average%20%28EMA%29%20of%20weights%20as%20effective%20alternatives%20to%0Amitigate%20the%20need%20for%20English.%20All%20in%20all%2C%20our%20work%20sheds%20light%20into%20the%0Adynamics%20by%20which%20emergent%20abilities%20arise%20when%20doing%20CPT%20for%20language%0Aadaptation%2C%20and%20can%20serve%20as%20a%20foundation%20to%20design%20more%20effective%20methods%20in%0Athe%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.00288v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmergent%2520Abilities%2520of%2520Large%2520Language%2520Models%2520under%2520Continued%2520Pretraining%250A%2520%2520for%2520Language%2520Adaptation%26entry.906535625%3DAhmed%2520Elhady%2520and%2520Eneko%2520Agirre%2520and%2520Mikel%2520Artetxe%26entry.1292438233%3D%2520%2520Continued%2520pretraining%2520%2528CPT%2529%2520is%2520a%2520popular%2520approach%2520to%2520adapt%2520existing%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520to%2520new%2520languages.%2520When%2520doing%2520so%252C%2520it%2520is%2520common%2520practice%250Ato%2520include%2520a%2520portion%2520of%2520English%2520data%2520in%2520the%2520mixture%252C%2520but%2520its%2520role%2520has%2520not%2520been%250Acarefully%2520studied%2520to%2520date.%2520In%2520this%2520work%252C%2520we%2520show%2520that%2520including%2520English%2520does%250Anot%2520impact%2520validation%2520perplexity%252C%2520yet%2520it%2520is%2520critical%2520for%2520the%2520emergence%2520of%250Adownstream%2520capabilities%2520in%2520the%2520target%2520language.%2520We%2520introduce%2520a%250Alanguage-agnostic%2520benchmark%2520for%2520in-context%2520learning%2520%2528ICL%2529%252C%2520which%2520reveals%250Acatastrophic%2520forgetting%2520early%2520on%2520CPT%2520when%2520English%2520is%2520not%2520included.%2520This%2520in%2520turn%250Adamages%2520the%2520ability%2520of%2520the%2520model%2520to%2520generalize%2520to%2520downstream%2520prompts%2520in%2520the%250Atarget%2520language%2520as%2520measured%2520by%2520perplexity%252C%2520even%2520if%2520it%2520does%2520not%2520manifest%2520in%250Aterms%2520of%2520accuracy%2520until%2520later%2520in%2520training%252C%2520and%2520can%2520be%2520tied%2520to%2520a%2520big%2520shift%2520in%250Athe%2520model%2520parameters.%2520Based%2520on%2520these%2520insights%252C%2520we%2520introduce%2520curriculum%2520learning%250Aand%2520exponential%2520moving%2520average%2520%2528EMA%2529%2520of%2520weights%2520as%2520effective%2520alternatives%2520to%250Amitigate%2520the%2520need%2520for%2520English.%2520All%2520in%2520all%252C%2520our%2520work%2520sheds%2520light%2520into%2520the%250Adynamics%2520by%2520which%2520emergent%2520abilities%2520arise%2520when%2520doing%2520CPT%2520for%2520language%250Aadaptation%252C%2520and%2520can%2520serve%2520as%2520a%2520foundation%2520to%2520design%2520more%2520effective%2520methods%2520in%250Athe%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.00288v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emergent%20Abilities%20of%20Large%20Language%20Models%20under%20Continued%20Pretraining%0A%20%20for%20Language%20Adaptation&entry.906535625=Ahmed%20Elhady%20and%20Eneko%20Agirre%20and%20Mikel%20Artetxe&entry.1292438233=%20%20Continued%20pretraining%20%28CPT%29%20is%20a%20popular%20approach%20to%20adapt%20existing%20large%0Alanguage%20models%20%28LLMs%29%20to%20new%20languages.%20When%20doing%20so%2C%20it%20is%20common%20practice%0Ato%20include%20a%20portion%20of%20English%20data%20in%20the%20mixture%2C%20but%20its%20role%20has%20not%20been%0Acarefully%20studied%20to%20date.%20In%20this%20work%2C%20we%20show%20that%20including%20English%20does%0Anot%20impact%20validation%20perplexity%2C%20yet%20it%20is%20critical%20for%20the%20emergence%20of%0Adownstream%20capabilities%20in%20the%20target%20language.%20We%20introduce%20a%0Alanguage-agnostic%20benchmark%20for%20in-context%20learning%20%28ICL%29%2C%20which%20reveals%0Acatastrophic%20forgetting%20early%20on%20CPT%20when%20English%20is%20not%20included.%20This%20in%20turn%0Adamages%20the%20ability%20of%20the%20model%20to%20generalize%20to%20downstream%20prompts%20in%20the%0Atarget%20language%20as%20measured%20by%20perplexity%2C%20even%20if%20it%20does%20not%20manifest%20in%0Aterms%20of%20accuracy%20until%20later%20in%20training%2C%20and%20can%20be%20tied%20to%20a%20big%20shift%20in%0Athe%20model%20parameters.%20Based%20on%20these%20insights%2C%20we%20introduce%20curriculum%20learning%0Aand%20exponential%20moving%20average%20%28EMA%29%20of%20weights%20as%20effective%20alternatives%20to%0Amitigate%20the%20need%20for%20English.%20All%20in%20all%2C%20our%20work%20sheds%20light%20into%20the%0Adynamics%20by%20which%20emergent%20abilities%20arise%20when%20doing%20CPT%20for%20language%0Aadaptation%2C%20and%20can%20serve%20as%20a%20foundation%20to%20design%20more%20effective%20methods%20in%0Athe%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.00288v2&entry.124074799=Read"},
{"title": "Targeted Forgetting of Image Subgroups in CLIP Models", "author": "Zeliang Zhang and Gaowen Liu and Charles Fleming and Ramana Rao Kompella and Chenliang Xu", "abstract": "  Foundation models (FMs) such as CLIP have demonstrated impressive zero-shot\nperformance across various tasks by leveraging large-scale, unsupervised\npre-training. However, they often inherit harmful or unwanted knowledge from\nnoisy internet-sourced datasets, compromising their reliability in real-world\napplications. Existing model unlearning methods either rely on access to\npre-trained datasets or focus on coarse-grained unlearning (e.g., entire\nclasses), leaving a critical gap for fine-grained unlearning. In this paper, we\naddress the challenging scenario of selectively forgetting specific portions of\nknowledge within a class, without access to pre-trained data, while preserving\nthe model's overall performance. We propose a novel three-stage approach that\nprogressively unlearns targeted knowledge while mitigating over-forgetting. It\nconsists of (1) a forgetting stage to fine-tune the CLIP on samples to be\nforgotten, (2) a reminding stage to restore performance on retained samples,\nand (3) a restoring stage to recover zero-shot capabilities using model\nsouping. Additionally, we introduce knowledge distillation to handle the\ndistribution disparity between forgetting, retaining samples, and unseen\npre-trained data. Extensive experiments on CIFAR-10, ImageNet-1K, and style\ndatasets demonstrate that our approach effectively unlearns specific subgroups\nwhile maintaining strong zero-shot performance on semantically similar\nsubgroups and other categories, significantly outperforming baseline unlearning\nmethods, which lose effectiveness under the CLIP unlearning setting.\n", "link": "http://arxiv.org/abs/2506.03117v1", "date": "2025-06-03", "relevancy": 2.6516, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5658}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5299}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Targeted%20Forgetting%20of%20Image%20Subgroups%20in%20CLIP%20Models&body=Title%3A%20Targeted%20Forgetting%20of%20Image%20Subgroups%20in%20CLIP%20Models%0AAuthor%3A%20Zeliang%20Zhang%20and%20Gaowen%20Liu%20and%20Charles%20Fleming%20and%20Ramana%20Rao%20Kompella%20and%20Chenliang%20Xu%0AAbstract%3A%20%20%20Foundation%20models%20%28FMs%29%20such%20as%20CLIP%20have%20demonstrated%20impressive%20zero-shot%0Aperformance%20across%20various%20tasks%20by%20leveraging%20large-scale%2C%20unsupervised%0Apre-training.%20However%2C%20they%20often%20inherit%20harmful%20or%20unwanted%20knowledge%20from%0Anoisy%20internet-sourced%20datasets%2C%20compromising%20their%20reliability%20in%20real-world%0Aapplications.%20Existing%20model%20unlearning%20methods%20either%20rely%20on%20access%20to%0Apre-trained%20datasets%20or%20focus%20on%20coarse-grained%20unlearning%20%28e.g.%2C%20entire%0Aclasses%29%2C%20leaving%20a%20critical%20gap%20for%20fine-grained%20unlearning.%20In%20this%20paper%2C%20we%0Aaddress%20the%20challenging%20scenario%20of%20selectively%20forgetting%20specific%20portions%20of%0Aknowledge%20within%20a%20class%2C%20without%20access%20to%20pre-trained%20data%2C%20while%20preserving%0Athe%20model%27s%20overall%20performance.%20We%20propose%20a%20novel%20three-stage%20approach%20that%0Aprogressively%20unlearns%20targeted%20knowledge%20while%20mitigating%20over-forgetting.%20It%0Aconsists%20of%20%281%29%20a%20forgetting%20stage%20to%20fine-tune%20the%20CLIP%20on%20samples%20to%20be%0Aforgotten%2C%20%282%29%20a%20reminding%20stage%20to%20restore%20performance%20on%20retained%20samples%2C%0Aand%20%283%29%20a%20restoring%20stage%20to%20recover%20zero-shot%20capabilities%20using%20model%0Asouping.%20Additionally%2C%20we%20introduce%20knowledge%20distillation%20to%20handle%20the%0Adistribution%20disparity%20between%20forgetting%2C%20retaining%20samples%2C%20and%20unseen%0Apre-trained%20data.%20Extensive%20experiments%20on%20CIFAR-10%2C%20ImageNet-1K%2C%20and%20style%0Adatasets%20demonstrate%20that%20our%20approach%20effectively%20unlearns%20specific%20subgroups%0Awhile%20maintaining%20strong%20zero-shot%20performance%20on%20semantically%20similar%0Asubgroups%20and%20other%20categories%2C%20significantly%20outperforming%20baseline%20unlearning%0Amethods%2C%20which%20lose%20effectiveness%20under%20the%20CLIP%20unlearning%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03117v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTargeted%2520Forgetting%2520of%2520Image%2520Subgroups%2520in%2520CLIP%2520Models%26entry.906535625%3DZeliang%2520Zhang%2520and%2520Gaowen%2520Liu%2520and%2520Charles%2520Fleming%2520and%2520Ramana%2520Rao%2520Kompella%2520and%2520Chenliang%2520Xu%26entry.1292438233%3D%2520%2520Foundation%2520models%2520%2528FMs%2529%2520such%2520as%2520CLIP%2520have%2520demonstrated%2520impressive%2520zero-shot%250Aperformance%2520across%2520various%2520tasks%2520by%2520leveraging%2520large-scale%252C%2520unsupervised%250Apre-training.%2520However%252C%2520they%2520often%2520inherit%2520harmful%2520or%2520unwanted%2520knowledge%2520from%250Anoisy%2520internet-sourced%2520datasets%252C%2520compromising%2520their%2520reliability%2520in%2520real-world%250Aapplications.%2520Existing%2520model%2520unlearning%2520methods%2520either%2520rely%2520on%2520access%2520to%250Apre-trained%2520datasets%2520or%2520focus%2520on%2520coarse-grained%2520unlearning%2520%2528e.g.%252C%2520entire%250Aclasses%2529%252C%2520leaving%2520a%2520critical%2520gap%2520for%2520fine-grained%2520unlearning.%2520In%2520this%2520paper%252C%2520we%250Aaddress%2520the%2520challenging%2520scenario%2520of%2520selectively%2520forgetting%2520specific%2520portions%2520of%250Aknowledge%2520within%2520a%2520class%252C%2520without%2520access%2520to%2520pre-trained%2520data%252C%2520while%2520preserving%250Athe%2520model%2527s%2520overall%2520performance.%2520We%2520propose%2520a%2520novel%2520three-stage%2520approach%2520that%250Aprogressively%2520unlearns%2520targeted%2520knowledge%2520while%2520mitigating%2520over-forgetting.%2520It%250Aconsists%2520of%2520%25281%2529%2520a%2520forgetting%2520stage%2520to%2520fine-tune%2520the%2520CLIP%2520on%2520samples%2520to%2520be%250Aforgotten%252C%2520%25282%2529%2520a%2520reminding%2520stage%2520to%2520restore%2520performance%2520on%2520retained%2520samples%252C%250Aand%2520%25283%2529%2520a%2520restoring%2520stage%2520to%2520recover%2520zero-shot%2520capabilities%2520using%2520model%250Asouping.%2520Additionally%252C%2520we%2520introduce%2520knowledge%2520distillation%2520to%2520handle%2520the%250Adistribution%2520disparity%2520between%2520forgetting%252C%2520retaining%2520samples%252C%2520and%2520unseen%250Apre-trained%2520data.%2520Extensive%2520experiments%2520on%2520CIFAR-10%252C%2520ImageNet-1K%252C%2520and%2520style%250Adatasets%2520demonstrate%2520that%2520our%2520approach%2520effectively%2520unlearns%2520specific%2520subgroups%250Awhile%2520maintaining%2520strong%2520zero-shot%2520performance%2520on%2520semantically%2520similar%250Asubgroups%2520and%2520other%2520categories%252C%2520significantly%2520outperforming%2520baseline%2520unlearning%250Amethods%252C%2520which%2520lose%2520effectiveness%2520under%2520the%2520CLIP%2520unlearning%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03117v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Targeted%20Forgetting%20of%20Image%20Subgroups%20in%20CLIP%20Models&entry.906535625=Zeliang%20Zhang%20and%20Gaowen%20Liu%20and%20Charles%20Fleming%20and%20Ramana%20Rao%20Kompella%20and%20Chenliang%20Xu&entry.1292438233=%20%20Foundation%20models%20%28FMs%29%20such%20as%20CLIP%20have%20demonstrated%20impressive%20zero-shot%0Aperformance%20across%20various%20tasks%20by%20leveraging%20large-scale%2C%20unsupervised%0Apre-training.%20However%2C%20they%20often%20inherit%20harmful%20or%20unwanted%20knowledge%20from%0Anoisy%20internet-sourced%20datasets%2C%20compromising%20their%20reliability%20in%20real-world%0Aapplications.%20Existing%20model%20unlearning%20methods%20either%20rely%20on%20access%20to%0Apre-trained%20datasets%20or%20focus%20on%20coarse-grained%20unlearning%20%28e.g.%2C%20entire%0Aclasses%29%2C%20leaving%20a%20critical%20gap%20for%20fine-grained%20unlearning.%20In%20this%20paper%2C%20we%0Aaddress%20the%20challenging%20scenario%20of%20selectively%20forgetting%20specific%20portions%20of%0Aknowledge%20within%20a%20class%2C%20without%20access%20to%20pre-trained%20data%2C%20while%20preserving%0Athe%20model%27s%20overall%20performance.%20We%20propose%20a%20novel%20three-stage%20approach%20that%0Aprogressively%20unlearns%20targeted%20knowledge%20while%20mitigating%20over-forgetting.%20It%0Aconsists%20of%20%281%29%20a%20forgetting%20stage%20to%20fine-tune%20the%20CLIP%20on%20samples%20to%20be%0Aforgotten%2C%20%282%29%20a%20reminding%20stage%20to%20restore%20performance%20on%20retained%20samples%2C%0Aand%20%283%29%20a%20restoring%20stage%20to%20recover%20zero-shot%20capabilities%20using%20model%0Asouping.%20Additionally%2C%20we%20introduce%20knowledge%20distillation%20to%20handle%20the%0Adistribution%20disparity%20between%20forgetting%2C%20retaining%20samples%2C%20and%20unseen%0Apre-trained%20data.%20Extensive%20experiments%20on%20CIFAR-10%2C%20ImageNet-1K%2C%20and%20style%0Adatasets%20demonstrate%20that%20our%20approach%20effectively%20unlearns%20specific%20subgroups%0Awhile%20maintaining%20strong%20zero-shot%20performance%20on%20semantically%20similar%0Asubgroups%20and%20other%20categories%2C%20significantly%20outperforming%20baseline%20unlearning%0Amethods%2C%20which%20lose%20effectiveness%20under%20the%20CLIP%20unlearning%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03117v1&entry.124074799=Read"},
{"title": "How to Connect Speech Foundation Models and Large Language Models? What\n  Matters and What Does Not", "author": "Francesco Verdini and Pierfrancesco Melucci and Stefano Perna and Francesco Cariaggi and Marco Gaido and Sara Papi and Szymon Mazurek and Marek Kasztelnik and Luisa Bentivogli and S\u00e9bastien Brati\u00e8res and Paolo Merialdo and Simone Scardapane", "abstract": "  The remarkable performance achieved by Large Language Models (LLM) has driven\nresearch efforts to leverage them for a wide range of tasks and input\nmodalities. In speech-to-text (S2T) tasks, the emerging solution consists of\nprojecting the output of the encoder of a Speech Foundational Model (SFM) into\nthe LLM embedding space through an adapter module. However, no work has yet\ninvestigated how much the downstream-task performance depends on each component\n(SFM, adapter, LLM) nor whether the best design of the adapter depends on the\nchosen SFM and LLM. To fill this gap, we evaluate the combination of 5 adapter\nmodules, 2 LLMs (Mistral and Llama), and 2 SFMs (Whisper and SeamlessM4T) on\ntwo widespread S2T tasks, namely Automatic Speech Recognition and Speech\nTranslation. Our results demonstrate that the SFM plays a pivotal role in\ndownstream performance, while the adapter choice has moderate impact and\ndepends on the SFM and LLM.\n", "link": "http://arxiv.org/abs/2409.17044v3", "date": "2025-06-03", "relevancy": 2.6511, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5347}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5347}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20to%20Connect%20Speech%20Foundation%20Models%20and%20Large%20Language%20Models%3F%20What%0A%20%20Matters%20and%20What%20Does%20Not&body=Title%3A%20How%20to%20Connect%20Speech%20Foundation%20Models%20and%20Large%20Language%20Models%3F%20What%0A%20%20Matters%20and%20What%20Does%20Not%0AAuthor%3A%20Francesco%20Verdini%20and%20Pierfrancesco%20Melucci%20and%20Stefano%20Perna%20and%20Francesco%20Cariaggi%20and%20Marco%20Gaido%20and%20Sara%20Papi%20and%20Szymon%20Mazurek%20and%20Marek%20Kasztelnik%20and%20Luisa%20Bentivogli%20and%20S%C3%A9bastien%20Brati%C3%A8res%20and%20Paolo%20Merialdo%20and%20Simone%20Scardapane%0AAbstract%3A%20%20%20The%20remarkable%20performance%20achieved%20by%20Large%20Language%20Models%20%28LLM%29%20has%20driven%0Aresearch%20efforts%20to%20leverage%20them%20for%20a%20wide%20range%20of%20tasks%20and%20input%0Amodalities.%20In%20speech-to-text%20%28S2T%29%20tasks%2C%20the%20emerging%20solution%20consists%20of%0Aprojecting%20the%20output%20of%20the%20encoder%20of%20a%20Speech%20Foundational%20Model%20%28SFM%29%20into%0Athe%20LLM%20embedding%20space%20through%20an%20adapter%20module.%20However%2C%20no%20work%20has%20yet%0Ainvestigated%20how%20much%20the%20downstream-task%20performance%20depends%20on%20each%20component%0A%28SFM%2C%20adapter%2C%20LLM%29%20nor%20whether%20the%20best%20design%20of%20the%20adapter%20depends%20on%20the%0Achosen%20SFM%20and%20LLM.%20To%20fill%20this%20gap%2C%20we%20evaluate%20the%20combination%20of%205%20adapter%0Amodules%2C%202%20LLMs%20%28Mistral%20and%20Llama%29%2C%20and%202%20SFMs%20%28Whisper%20and%20SeamlessM4T%29%20on%0Atwo%20widespread%20S2T%20tasks%2C%20namely%20Automatic%20Speech%20Recognition%20and%20Speech%0ATranslation.%20Our%20results%20demonstrate%20that%20the%20SFM%20plays%20a%20pivotal%20role%20in%0Adownstream%20performance%2C%20while%20the%20adapter%20choice%20has%20moderate%20impact%20and%0Adepends%20on%20the%20SFM%20and%20LLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17044v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520to%2520Connect%2520Speech%2520Foundation%2520Models%2520and%2520Large%2520Language%2520Models%253F%2520What%250A%2520%2520Matters%2520and%2520What%2520Does%2520Not%26entry.906535625%3DFrancesco%2520Verdini%2520and%2520Pierfrancesco%2520Melucci%2520and%2520Stefano%2520Perna%2520and%2520Francesco%2520Cariaggi%2520and%2520Marco%2520Gaido%2520and%2520Sara%2520Papi%2520and%2520Szymon%2520Mazurek%2520and%2520Marek%2520Kasztelnik%2520and%2520Luisa%2520Bentivogli%2520and%2520S%25C3%25A9bastien%2520Brati%25C3%25A8res%2520and%2520Paolo%2520Merialdo%2520and%2520Simone%2520Scardapane%26entry.1292438233%3D%2520%2520The%2520remarkable%2520performance%2520achieved%2520by%2520Large%2520Language%2520Models%2520%2528LLM%2529%2520has%2520driven%250Aresearch%2520efforts%2520to%2520leverage%2520them%2520for%2520a%2520wide%2520range%2520of%2520tasks%2520and%2520input%250Amodalities.%2520In%2520speech-to-text%2520%2528S2T%2529%2520tasks%252C%2520the%2520emerging%2520solution%2520consists%2520of%250Aprojecting%2520the%2520output%2520of%2520the%2520encoder%2520of%2520a%2520Speech%2520Foundational%2520Model%2520%2528SFM%2529%2520into%250Athe%2520LLM%2520embedding%2520space%2520through%2520an%2520adapter%2520module.%2520However%252C%2520no%2520work%2520has%2520yet%250Ainvestigated%2520how%2520much%2520the%2520downstream-task%2520performance%2520depends%2520on%2520each%2520component%250A%2528SFM%252C%2520adapter%252C%2520LLM%2529%2520nor%2520whether%2520the%2520best%2520design%2520of%2520the%2520adapter%2520depends%2520on%2520the%250Achosen%2520SFM%2520and%2520LLM.%2520To%2520fill%2520this%2520gap%252C%2520we%2520evaluate%2520the%2520combination%2520of%25205%2520adapter%250Amodules%252C%25202%2520LLMs%2520%2528Mistral%2520and%2520Llama%2529%252C%2520and%25202%2520SFMs%2520%2528Whisper%2520and%2520SeamlessM4T%2529%2520on%250Atwo%2520widespread%2520S2T%2520tasks%252C%2520namely%2520Automatic%2520Speech%2520Recognition%2520and%2520Speech%250ATranslation.%2520Our%2520results%2520demonstrate%2520that%2520the%2520SFM%2520plays%2520a%2520pivotal%2520role%2520in%250Adownstream%2520performance%252C%2520while%2520the%2520adapter%2520choice%2520has%2520moderate%2520impact%2520and%250Adepends%2520on%2520the%2520SFM%2520and%2520LLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17044v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20to%20Connect%20Speech%20Foundation%20Models%20and%20Large%20Language%20Models%3F%20What%0A%20%20Matters%20and%20What%20Does%20Not&entry.906535625=Francesco%20Verdini%20and%20Pierfrancesco%20Melucci%20and%20Stefano%20Perna%20and%20Francesco%20Cariaggi%20and%20Marco%20Gaido%20and%20Sara%20Papi%20and%20Szymon%20Mazurek%20and%20Marek%20Kasztelnik%20and%20Luisa%20Bentivogli%20and%20S%C3%A9bastien%20Brati%C3%A8res%20and%20Paolo%20Merialdo%20and%20Simone%20Scardapane&entry.1292438233=%20%20The%20remarkable%20performance%20achieved%20by%20Large%20Language%20Models%20%28LLM%29%20has%20driven%0Aresearch%20efforts%20to%20leverage%20them%20for%20a%20wide%20range%20of%20tasks%20and%20input%0Amodalities.%20In%20speech-to-text%20%28S2T%29%20tasks%2C%20the%20emerging%20solution%20consists%20of%0Aprojecting%20the%20output%20of%20the%20encoder%20of%20a%20Speech%20Foundational%20Model%20%28SFM%29%20into%0Athe%20LLM%20embedding%20space%20through%20an%20adapter%20module.%20However%2C%20no%20work%20has%20yet%0Ainvestigated%20how%20much%20the%20downstream-task%20performance%20depends%20on%20each%20component%0A%28SFM%2C%20adapter%2C%20LLM%29%20nor%20whether%20the%20best%20design%20of%20the%20adapter%20depends%20on%20the%0Achosen%20SFM%20and%20LLM.%20To%20fill%20this%20gap%2C%20we%20evaluate%20the%20combination%20of%205%20adapter%0Amodules%2C%202%20LLMs%20%28Mistral%20and%20Llama%29%2C%20and%202%20SFMs%20%28Whisper%20and%20SeamlessM4T%29%20on%0Atwo%20widespread%20S2T%20tasks%2C%20namely%20Automatic%20Speech%20Recognition%20and%20Speech%0ATranslation.%20Our%20results%20demonstrate%20that%20the%20SFM%20plays%20a%20pivotal%20role%20in%0Adownstream%20performance%2C%20while%20the%20adapter%20choice%20has%20moderate%20impact%20and%0Adepends%20on%20the%20SFM%20and%20LLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17044v3&entry.124074799=Read"},
{"title": "Effective Dual-Region Augmentation for Reduced Reliance on Large Amounts\n  of Labeled Data", "author": "Prasanna Reddy Pulakurthi and Majid Rabbani and Celso M. de Melo and Sohail A. Dianat and Raghuveer M. Rao", "abstract": "  This paper introduces a novel dual-region augmentation approach designed to\nreduce reliance on large-scale labeled datasets while improving model\nrobustness and adaptability across diverse computer vision tasks, including\nsource-free domain adaptation (SFDA) and person re-identification (ReID). Our\nmethod performs targeted data transformations by applying random noise\nperturbations to foreground objects and spatially shuffling background patches.\nThis effectively increases the diversity of the training data, improving model\nrobustness and generalization. Evaluations on the PACS dataset for SFDA\ndemonstrate that our augmentation strategy consistently outperforms existing\nmethods, achieving significant accuracy improvements in both single-target and\nmulti-target adaptation settings. By augmenting training data through\nstructured transformations, our method enables model generalization across\ndomains, providing a scalable solution for reducing reliance on manually\nannotated datasets. Furthermore, experiments on Market-1501 and DukeMTMC-reID\ndatasets validate the effectiveness of our approach for person ReID, surpassing\ntraditional augmentation techniques. The code is available at\nhttps://github.com/PrasannaPulakurthi/Foreground-Background-Augmentation\n", "link": "http://arxiv.org/abs/2504.13077v2", "date": "2025-06-03", "relevancy": 2.6469, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5329}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5291}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.526}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Effective%20Dual-Region%20Augmentation%20for%20Reduced%20Reliance%20on%20Large%20Amounts%0A%20%20of%20Labeled%20Data&body=Title%3A%20Effective%20Dual-Region%20Augmentation%20for%20Reduced%20Reliance%20on%20Large%20Amounts%0A%20%20of%20Labeled%20Data%0AAuthor%3A%20Prasanna%20Reddy%20Pulakurthi%20and%20Majid%20Rabbani%20and%20Celso%20M.%20de%20Melo%20and%20Sohail%20A.%20Dianat%20and%20Raghuveer%20M.%20Rao%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20dual-region%20augmentation%20approach%20designed%20to%0Areduce%20reliance%20on%20large-scale%20labeled%20datasets%20while%20improving%20model%0Arobustness%20and%20adaptability%20across%20diverse%20computer%20vision%20tasks%2C%20including%0Asource-free%20domain%20adaptation%20%28SFDA%29%20and%20person%20re-identification%20%28ReID%29.%20Our%0Amethod%20performs%20targeted%20data%20transformations%20by%20applying%20random%20noise%0Aperturbations%20to%20foreground%20objects%20and%20spatially%20shuffling%20background%20patches.%0AThis%20effectively%20increases%20the%20diversity%20of%20the%20training%20data%2C%20improving%20model%0Arobustness%20and%20generalization.%20Evaluations%20on%20the%20PACS%20dataset%20for%20SFDA%0Ademonstrate%20that%20our%20augmentation%20strategy%20consistently%20outperforms%20existing%0Amethods%2C%20achieving%20significant%20accuracy%20improvements%20in%20both%20single-target%20and%0Amulti-target%20adaptation%20settings.%20By%20augmenting%20training%20data%20through%0Astructured%20transformations%2C%20our%20method%20enables%20model%20generalization%20across%0Adomains%2C%20providing%20a%20scalable%20solution%20for%20reducing%20reliance%20on%20manually%0Aannotated%20datasets.%20Furthermore%2C%20experiments%20on%20Market-1501%20and%20DukeMTMC-reID%0Adatasets%20validate%20the%20effectiveness%20of%20our%20approach%20for%20person%20ReID%2C%20surpassing%0Atraditional%20augmentation%20techniques.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/PrasannaPulakurthi/Foreground-Background-Augmentation%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.13077v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEffective%2520Dual-Region%2520Augmentation%2520for%2520Reduced%2520Reliance%2520on%2520Large%2520Amounts%250A%2520%2520of%2520Labeled%2520Data%26entry.906535625%3DPrasanna%2520Reddy%2520Pulakurthi%2520and%2520Majid%2520Rabbani%2520and%2520Celso%2520M.%2520de%2520Melo%2520and%2520Sohail%2520A.%2520Dianat%2520and%2520Raghuveer%2520M.%2520Rao%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520dual-region%2520augmentation%2520approach%2520designed%2520to%250Areduce%2520reliance%2520on%2520large-scale%2520labeled%2520datasets%2520while%2520improving%2520model%250Arobustness%2520and%2520adaptability%2520across%2520diverse%2520computer%2520vision%2520tasks%252C%2520including%250Asource-free%2520domain%2520adaptation%2520%2528SFDA%2529%2520and%2520person%2520re-identification%2520%2528ReID%2529.%2520Our%250Amethod%2520performs%2520targeted%2520data%2520transformations%2520by%2520applying%2520random%2520noise%250Aperturbations%2520to%2520foreground%2520objects%2520and%2520spatially%2520shuffling%2520background%2520patches.%250AThis%2520effectively%2520increases%2520the%2520diversity%2520of%2520the%2520training%2520data%252C%2520improving%2520model%250Arobustness%2520and%2520generalization.%2520Evaluations%2520on%2520the%2520PACS%2520dataset%2520for%2520SFDA%250Ademonstrate%2520that%2520our%2520augmentation%2520strategy%2520consistently%2520outperforms%2520existing%250Amethods%252C%2520achieving%2520significant%2520accuracy%2520improvements%2520in%2520both%2520single-target%2520and%250Amulti-target%2520adaptation%2520settings.%2520By%2520augmenting%2520training%2520data%2520through%250Astructured%2520transformations%252C%2520our%2520method%2520enables%2520model%2520generalization%2520across%250Adomains%252C%2520providing%2520a%2520scalable%2520solution%2520for%2520reducing%2520reliance%2520on%2520manually%250Aannotated%2520datasets.%2520Furthermore%252C%2520experiments%2520on%2520Market-1501%2520and%2520DukeMTMC-reID%250Adatasets%2520validate%2520the%2520effectiveness%2520of%2520our%2520approach%2520for%2520person%2520ReID%252C%2520surpassing%250Atraditional%2520augmentation%2520techniques.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/PrasannaPulakurthi/Foreground-Background-Augmentation%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.13077v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Effective%20Dual-Region%20Augmentation%20for%20Reduced%20Reliance%20on%20Large%20Amounts%0A%20%20of%20Labeled%20Data&entry.906535625=Prasanna%20Reddy%20Pulakurthi%20and%20Majid%20Rabbani%20and%20Celso%20M.%20de%20Melo%20and%20Sohail%20A.%20Dianat%20and%20Raghuveer%20M.%20Rao&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20dual-region%20augmentation%20approach%20designed%20to%0Areduce%20reliance%20on%20large-scale%20labeled%20datasets%20while%20improving%20model%0Arobustness%20and%20adaptability%20across%20diverse%20computer%20vision%20tasks%2C%20including%0Asource-free%20domain%20adaptation%20%28SFDA%29%20and%20person%20re-identification%20%28ReID%29.%20Our%0Amethod%20performs%20targeted%20data%20transformations%20by%20applying%20random%20noise%0Aperturbations%20to%20foreground%20objects%20and%20spatially%20shuffling%20background%20patches.%0AThis%20effectively%20increases%20the%20diversity%20of%20the%20training%20data%2C%20improving%20model%0Arobustness%20and%20generalization.%20Evaluations%20on%20the%20PACS%20dataset%20for%20SFDA%0Ademonstrate%20that%20our%20augmentation%20strategy%20consistently%20outperforms%20existing%0Amethods%2C%20achieving%20significant%20accuracy%20improvements%20in%20both%20single-target%20and%0Amulti-target%20adaptation%20settings.%20By%20augmenting%20training%20data%20through%0Astructured%20transformations%2C%20our%20method%20enables%20model%20generalization%20across%0Adomains%2C%20providing%20a%20scalable%20solution%20for%20reducing%20reliance%20on%20manually%0Aannotated%20datasets.%20Furthermore%2C%20experiments%20on%20Market-1501%20and%20DukeMTMC-reID%0Adatasets%20validate%20the%20effectiveness%20of%20our%20approach%20for%20person%20ReID%2C%20surpassing%0Atraditional%20augmentation%20techniques.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/PrasannaPulakurthi/Foreground-Background-Augmentation%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.13077v2&entry.124074799=Read"},
{"title": "SASP: Strip-Aware Spatial Perception for Fine-Grained Bird Image\n  Classification", "author": "Zheng Wang", "abstract": "  Fine-grained bird image classification (FBIC) is not only of great\nsignificance for ecological monitoring and species identification, but also\nholds broad research value in the fields of image recognition and fine-grained\nvisual modeling. Compared with general image classification tasks, FBIC poses\nmore formidable challenges: 1) the differences in species size and imaging\ndistance result in the varying sizes of birds presented in the images; 2)\ncomplex natural habitats often introduce strong background interference; 3) and\nhighly flexible poses such as flying, perching, or foraging result in\nsubstantial intra-class variability. These factors collectively make it\ndifficult for traditional methods to stably extract discriminative features,\nthereby limiting the generalizability and interpretability of models in\nreal-world applications. To address these challenges, this paper proposes a\nfine-grained bird classification framework based on strip-aware spatial\nperception, which aims to capture long-range spatial dependencies across entire\nrows or columns in bird images, thereby enhancing the model's robustness and\ninterpretability. The proposed method incorporates two novel modules:\nextensional perception aggregator (EPA) and channel semantic weaving (CSW).\nSpecifically, EPA integrates local texture details with global structural cues\nby aggregating information across horizontal and vertical spatial directions.\nCSW further refines the semantic representations by adaptively fusing\nlong-range and short-range information along the channel dimension. Built upon\na ResNet-50 backbone, the model enables jump-wise connection of extended\nstructural features across the spatial domain. Experimental results on the\nCUB-200-2011 dataset demonstrate that our framework achieves significant\nperformance improvements while maintaining architectural efficiency.\n", "link": "http://arxiv.org/abs/2505.24380v2", "date": "2025-06-03", "relevancy": 2.6256, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5316}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5266}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SASP%3A%20Strip-Aware%20Spatial%20Perception%20for%20Fine-Grained%20Bird%20Image%0A%20%20Classification&body=Title%3A%20SASP%3A%20Strip-Aware%20Spatial%20Perception%20for%20Fine-Grained%20Bird%20Image%0A%20%20Classification%0AAuthor%3A%20Zheng%20Wang%0AAbstract%3A%20%20%20Fine-grained%20bird%20image%20classification%20%28FBIC%29%20is%20not%20only%20of%20great%0Asignificance%20for%20ecological%20monitoring%20and%20species%20identification%2C%20but%20also%0Aholds%20broad%20research%20value%20in%20the%20fields%20of%20image%20recognition%20and%20fine-grained%0Avisual%20modeling.%20Compared%20with%20general%20image%20classification%20tasks%2C%20FBIC%20poses%0Amore%20formidable%20challenges%3A%201%29%20the%20differences%20in%20species%20size%20and%20imaging%0Adistance%20result%20in%20the%20varying%20sizes%20of%20birds%20presented%20in%20the%20images%3B%202%29%0Acomplex%20natural%20habitats%20often%20introduce%20strong%20background%20interference%3B%203%29%20and%0Ahighly%20flexible%20poses%20such%20as%20flying%2C%20perching%2C%20or%20foraging%20result%20in%0Asubstantial%20intra-class%20variability.%20These%20factors%20collectively%20make%20it%0Adifficult%20for%20traditional%20methods%20to%20stably%20extract%20discriminative%20features%2C%0Athereby%20limiting%20the%20generalizability%20and%20interpretability%20of%20models%20in%0Areal-world%20applications.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%20a%0Afine-grained%20bird%20classification%20framework%20based%20on%20strip-aware%20spatial%0Aperception%2C%20which%20aims%20to%20capture%20long-range%20spatial%20dependencies%20across%20entire%0Arows%20or%20columns%20in%20bird%20images%2C%20thereby%20enhancing%20the%20model%27s%20robustness%20and%0Ainterpretability.%20The%20proposed%20method%20incorporates%20two%20novel%20modules%3A%0Aextensional%20perception%20aggregator%20%28EPA%29%20and%20channel%20semantic%20weaving%20%28CSW%29.%0ASpecifically%2C%20EPA%20integrates%20local%20texture%20details%20with%20global%20structural%20cues%0Aby%20aggregating%20information%20across%20horizontal%20and%20vertical%20spatial%20directions.%0ACSW%20further%20refines%20the%20semantic%20representations%20by%20adaptively%20fusing%0Along-range%20and%20short-range%20information%20along%20the%20channel%20dimension.%20Built%20upon%0Aa%20ResNet-50%20backbone%2C%20the%20model%20enables%20jump-wise%20connection%20of%20extended%0Astructural%20features%20across%20the%20spatial%20domain.%20Experimental%20results%20on%20the%0ACUB-200-2011%20dataset%20demonstrate%20that%20our%20framework%20achieves%20significant%0Aperformance%20improvements%20while%20maintaining%20architectural%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.24380v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSASP%253A%2520Strip-Aware%2520Spatial%2520Perception%2520for%2520Fine-Grained%2520Bird%2520Image%250A%2520%2520Classification%26entry.906535625%3DZheng%2520Wang%26entry.1292438233%3D%2520%2520Fine-grained%2520bird%2520image%2520classification%2520%2528FBIC%2529%2520is%2520not%2520only%2520of%2520great%250Asignificance%2520for%2520ecological%2520monitoring%2520and%2520species%2520identification%252C%2520but%2520also%250Aholds%2520broad%2520research%2520value%2520in%2520the%2520fields%2520of%2520image%2520recognition%2520and%2520fine-grained%250Avisual%2520modeling.%2520Compared%2520with%2520general%2520image%2520classification%2520tasks%252C%2520FBIC%2520poses%250Amore%2520formidable%2520challenges%253A%25201%2529%2520the%2520differences%2520in%2520species%2520size%2520and%2520imaging%250Adistance%2520result%2520in%2520the%2520varying%2520sizes%2520of%2520birds%2520presented%2520in%2520the%2520images%253B%25202%2529%250Acomplex%2520natural%2520habitats%2520often%2520introduce%2520strong%2520background%2520interference%253B%25203%2529%2520and%250Ahighly%2520flexible%2520poses%2520such%2520as%2520flying%252C%2520perching%252C%2520or%2520foraging%2520result%2520in%250Asubstantial%2520intra-class%2520variability.%2520These%2520factors%2520collectively%2520make%2520it%250Adifficult%2520for%2520traditional%2520methods%2520to%2520stably%2520extract%2520discriminative%2520features%252C%250Athereby%2520limiting%2520the%2520generalizability%2520and%2520interpretability%2520of%2520models%2520in%250Areal-world%2520applications.%2520To%2520address%2520these%2520challenges%252C%2520this%2520paper%2520proposes%2520a%250Afine-grained%2520bird%2520classification%2520framework%2520based%2520on%2520strip-aware%2520spatial%250Aperception%252C%2520which%2520aims%2520to%2520capture%2520long-range%2520spatial%2520dependencies%2520across%2520entire%250Arows%2520or%2520columns%2520in%2520bird%2520images%252C%2520thereby%2520enhancing%2520the%2520model%2527s%2520robustness%2520and%250Ainterpretability.%2520The%2520proposed%2520method%2520incorporates%2520two%2520novel%2520modules%253A%250Aextensional%2520perception%2520aggregator%2520%2528EPA%2529%2520and%2520channel%2520semantic%2520weaving%2520%2528CSW%2529.%250ASpecifically%252C%2520EPA%2520integrates%2520local%2520texture%2520details%2520with%2520global%2520structural%2520cues%250Aby%2520aggregating%2520information%2520across%2520horizontal%2520and%2520vertical%2520spatial%2520directions.%250ACSW%2520further%2520refines%2520the%2520semantic%2520representations%2520by%2520adaptively%2520fusing%250Along-range%2520and%2520short-range%2520information%2520along%2520the%2520channel%2520dimension.%2520Built%2520upon%250Aa%2520ResNet-50%2520backbone%252C%2520the%2520model%2520enables%2520jump-wise%2520connection%2520of%2520extended%250Astructural%2520features%2520across%2520the%2520spatial%2520domain.%2520Experimental%2520results%2520on%2520the%250ACUB-200-2011%2520dataset%2520demonstrate%2520that%2520our%2520framework%2520achieves%2520significant%250Aperformance%2520improvements%2520while%2520maintaining%2520architectural%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24380v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SASP%3A%20Strip-Aware%20Spatial%20Perception%20for%20Fine-Grained%20Bird%20Image%0A%20%20Classification&entry.906535625=Zheng%20Wang&entry.1292438233=%20%20Fine-grained%20bird%20image%20classification%20%28FBIC%29%20is%20not%20only%20of%20great%0Asignificance%20for%20ecological%20monitoring%20and%20species%20identification%2C%20but%20also%0Aholds%20broad%20research%20value%20in%20the%20fields%20of%20image%20recognition%20and%20fine-grained%0Avisual%20modeling.%20Compared%20with%20general%20image%20classification%20tasks%2C%20FBIC%20poses%0Amore%20formidable%20challenges%3A%201%29%20the%20differences%20in%20species%20size%20and%20imaging%0Adistance%20result%20in%20the%20varying%20sizes%20of%20birds%20presented%20in%20the%20images%3B%202%29%0Acomplex%20natural%20habitats%20often%20introduce%20strong%20background%20interference%3B%203%29%20and%0Ahighly%20flexible%20poses%20such%20as%20flying%2C%20perching%2C%20or%20foraging%20result%20in%0Asubstantial%20intra-class%20variability.%20These%20factors%20collectively%20make%20it%0Adifficult%20for%20traditional%20methods%20to%20stably%20extract%20discriminative%20features%2C%0Athereby%20limiting%20the%20generalizability%20and%20interpretability%20of%20models%20in%0Areal-world%20applications.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%20a%0Afine-grained%20bird%20classification%20framework%20based%20on%20strip-aware%20spatial%0Aperception%2C%20which%20aims%20to%20capture%20long-range%20spatial%20dependencies%20across%20entire%0Arows%20or%20columns%20in%20bird%20images%2C%20thereby%20enhancing%20the%20model%27s%20robustness%20and%0Ainterpretability.%20The%20proposed%20method%20incorporates%20two%20novel%20modules%3A%0Aextensional%20perception%20aggregator%20%28EPA%29%20and%20channel%20semantic%20weaving%20%28CSW%29.%0ASpecifically%2C%20EPA%20integrates%20local%20texture%20details%20with%20global%20structural%20cues%0Aby%20aggregating%20information%20across%20horizontal%20and%20vertical%20spatial%20directions.%0ACSW%20further%20refines%20the%20semantic%20representations%20by%20adaptively%20fusing%0Along-range%20and%20short-range%20information%20along%20the%20channel%20dimension.%20Built%20upon%0Aa%20ResNet-50%20backbone%2C%20the%20model%20enables%20jump-wise%20connection%20of%20extended%0Astructural%20features%20across%20the%20spatial%20domain.%20Experimental%20results%20on%20the%0ACUB-200-2011%20dataset%20demonstrate%20that%20our%20framework%20achieves%20significant%0Aperformance%20improvements%20while%20maintaining%20architectural%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.24380v2&entry.124074799=Read"},
{"title": "MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition\n  Query", "author": "Wei Chow and Yuan Gao and Linfeng Li and Xian Wang and Qi Xu and Hang Song and Lingdong Kong and Ran Zhou and Yi Zeng and Yidong Cai and Botian Jiang and Shilin Xu and Jiajun Zhang and Minghui Qiu and Xiangtai Li and Tianshu Yang and Siliang Tang and Juncheng Li", "abstract": "  Semantic retrieval is crucial for modern applications yet remains\nunderexplored in current research. Existing datasets are limited to single\nlanguages, single images, or singular retrieval conditions, often failing to\nfully exploit the expressive capacity of visual information as evidenced by\nmaintained performance when images are replaced with captions. However,\npractical retrieval scenarios frequently involve interleaved multi-condition\nqueries with multiple images. Hence, this paper introduces MERIT, the first\nmultilingual dataset for interleaved multi-condition semantic retrieval,\ncomprising 320,000 queries with 135,000 products in 5 languages, covering 7\ndistinct product categories. Extensive experiments on MERIT identify existing\nmodels's limitation: focusing solely on global semantic information while\nneglecting specific conditional elements in queries. Consequently, we propose\nCoral, a novel fine-tuning framework that adapts pre-trained MLLMs by\nintegrating embedding reconstruction to preserve fine-grained conditional\nelements and contrastive learning to extract comprehensive global semantics.\nExperiments demonstrate that Coral achieves a 45.9% performance improvement\nover conventional approaches on MERIT, with strong generalization capabilities\nvalidated across 8 established retrieval benchmarks. Collectively, our\ncontributions - a novel dataset, identification of critical limitations in\nexisting approaches, and an innovative fine-tuning framework - establish a\nfoundation for future research in interleaved multi-condition semantic\nretrieval.\n", "link": "http://arxiv.org/abs/2506.03144v1", "date": "2025-06-03", "relevancy": 2.6141, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5345}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.517}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MERIT%3A%20Multilingual%20Semantic%20Retrieval%20with%20Interleaved%20Multi-Condition%0A%20%20Query&body=Title%3A%20MERIT%3A%20Multilingual%20Semantic%20Retrieval%20with%20Interleaved%20Multi-Condition%0A%20%20Query%0AAuthor%3A%20Wei%20Chow%20and%20Yuan%20Gao%20and%20Linfeng%20Li%20and%20Xian%20Wang%20and%20Qi%20Xu%20and%20Hang%20Song%20and%20Lingdong%20Kong%20and%20Ran%20Zhou%20and%20Yi%20Zeng%20and%20Yidong%20Cai%20and%20Botian%20Jiang%20and%20Shilin%20Xu%20and%20Jiajun%20Zhang%20and%20Minghui%20Qiu%20and%20Xiangtai%20Li%20and%20Tianshu%20Yang%20and%20Siliang%20Tang%20and%20Juncheng%20Li%0AAbstract%3A%20%20%20Semantic%20retrieval%20is%20crucial%20for%20modern%20applications%20yet%20remains%0Aunderexplored%20in%20current%20research.%20Existing%20datasets%20are%20limited%20to%20single%0Alanguages%2C%20single%20images%2C%20or%20singular%20retrieval%20conditions%2C%20often%20failing%20to%0Afully%20exploit%20the%20expressive%20capacity%20of%20visual%20information%20as%20evidenced%20by%0Amaintained%20performance%20when%20images%20are%20replaced%20with%20captions.%20However%2C%0Apractical%20retrieval%20scenarios%20frequently%20involve%20interleaved%20multi-condition%0Aqueries%20with%20multiple%20images.%20Hence%2C%20this%20paper%20introduces%20MERIT%2C%20the%20first%0Amultilingual%20dataset%20for%20interleaved%20multi-condition%20semantic%20retrieval%2C%0Acomprising%20320%2C000%20queries%20with%20135%2C000%20products%20in%205%20languages%2C%20covering%207%0Adistinct%20product%20categories.%20Extensive%20experiments%20on%20MERIT%20identify%20existing%0Amodels%27s%20limitation%3A%20focusing%20solely%20on%20global%20semantic%20information%20while%0Aneglecting%20specific%20conditional%20elements%20in%20queries.%20Consequently%2C%20we%20propose%0ACoral%2C%20a%20novel%20fine-tuning%20framework%20that%20adapts%20pre-trained%20MLLMs%20by%0Aintegrating%20embedding%20reconstruction%20to%20preserve%20fine-grained%20conditional%0Aelements%20and%20contrastive%20learning%20to%20extract%20comprehensive%20global%20semantics.%0AExperiments%20demonstrate%20that%20Coral%20achieves%20a%2045.9%25%20performance%20improvement%0Aover%20conventional%20approaches%20on%20MERIT%2C%20with%20strong%20generalization%20capabilities%0Avalidated%20across%208%20established%20retrieval%20benchmarks.%20Collectively%2C%20our%0Acontributions%20-%20a%20novel%20dataset%2C%20identification%20of%20critical%20limitations%20in%0Aexisting%20approaches%2C%20and%20an%20innovative%20fine-tuning%20framework%20-%20establish%20a%0Afoundation%20for%20future%20research%20in%20interleaved%20multi-condition%20semantic%0Aretrieval.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03144v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMERIT%253A%2520Multilingual%2520Semantic%2520Retrieval%2520with%2520Interleaved%2520Multi-Condition%250A%2520%2520Query%26entry.906535625%3DWei%2520Chow%2520and%2520Yuan%2520Gao%2520and%2520Linfeng%2520Li%2520and%2520Xian%2520Wang%2520and%2520Qi%2520Xu%2520and%2520Hang%2520Song%2520and%2520Lingdong%2520Kong%2520and%2520Ran%2520Zhou%2520and%2520Yi%2520Zeng%2520and%2520Yidong%2520Cai%2520and%2520Botian%2520Jiang%2520and%2520Shilin%2520Xu%2520and%2520Jiajun%2520Zhang%2520and%2520Minghui%2520Qiu%2520and%2520Xiangtai%2520Li%2520and%2520Tianshu%2520Yang%2520and%2520Siliang%2520Tang%2520and%2520Juncheng%2520Li%26entry.1292438233%3D%2520%2520Semantic%2520retrieval%2520is%2520crucial%2520for%2520modern%2520applications%2520yet%2520remains%250Aunderexplored%2520in%2520current%2520research.%2520Existing%2520datasets%2520are%2520limited%2520to%2520single%250Alanguages%252C%2520single%2520images%252C%2520or%2520singular%2520retrieval%2520conditions%252C%2520often%2520failing%2520to%250Afully%2520exploit%2520the%2520expressive%2520capacity%2520of%2520visual%2520information%2520as%2520evidenced%2520by%250Amaintained%2520performance%2520when%2520images%2520are%2520replaced%2520with%2520captions.%2520However%252C%250Apractical%2520retrieval%2520scenarios%2520frequently%2520involve%2520interleaved%2520multi-condition%250Aqueries%2520with%2520multiple%2520images.%2520Hence%252C%2520this%2520paper%2520introduces%2520MERIT%252C%2520the%2520first%250Amultilingual%2520dataset%2520for%2520interleaved%2520multi-condition%2520semantic%2520retrieval%252C%250Acomprising%2520320%252C000%2520queries%2520with%2520135%252C000%2520products%2520in%25205%2520languages%252C%2520covering%25207%250Adistinct%2520product%2520categories.%2520Extensive%2520experiments%2520on%2520MERIT%2520identify%2520existing%250Amodels%2527s%2520limitation%253A%2520focusing%2520solely%2520on%2520global%2520semantic%2520information%2520while%250Aneglecting%2520specific%2520conditional%2520elements%2520in%2520queries.%2520Consequently%252C%2520we%2520propose%250ACoral%252C%2520a%2520novel%2520fine-tuning%2520framework%2520that%2520adapts%2520pre-trained%2520MLLMs%2520by%250Aintegrating%2520embedding%2520reconstruction%2520to%2520preserve%2520fine-grained%2520conditional%250Aelements%2520and%2520contrastive%2520learning%2520to%2520extract%2520comprehensive%2520global%2520semantics.%250AExperiments%2520demonstrate%2520that%2520Coral%2520achieves%2520a%252045.9%2525%2520performance%2520improvement%250Aover%2520conventional%2520approaches%2520on%2520MERIT%252C%2520with%2520strong%2520generalization%2520capabilities%250Avalidated%2520across%25208%2520established%2520retrieval%2520benchmarks.%2520Collectively%252C%2520our%250Acontributions%2520-%2520a%2520novel%2520dataset%252C%2520identification%2520of%2520critical%2520limitations%2520in%250Aexisting%2520approaches%252C%2520and%2520an%2520innovative%2520fine-tuning%2520framework%2520-%2520establish%2520a%250Afoundation%2520for%2520future%2520research%2520in%2520interleaved%2520multi-condition%2520semantic%250Aretrieval.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03144v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MERIT%3A%20Multilingual%20Semantic%20Retrieval%20with%20Interleaved%20Multi-Condition%0A%20%20Query&entry.906535625=Wei%20Chow%20and%20Yuan%20Gao%20and%20Linfeng%20Li%20and%20Xian%20Wang%20and%20Qi%20Xu%20and%20Hang%20Song%20and%20Lingdong%20Kong%20and%20Ran%20Zhou%20and%20Yi%20Zeng%20and%20Yidong%20Cai%20and%20Botian%20Jiang%20and%20Shilin%20Xu%20and%20Jiajun%20Zhang%20and%20Minghui%20Qiu%20and%20Xiangtai%20Li%20and%20Tianshu%20Yang%20and%20Siliang%20Tang%20and%20Juncheng%20Li&entry.1292438233=%20%20Semantic%20retrieval%20is%20crucial%20for%20modern%20applications%20yet%20remains%0Aunderexplored%20in%20current%20research.%20Existing%20datasets%20are%20limited%20to%20single%0Alanguages%2C%20single%20images%2C%20or%20singular%20retrieval%20conditions%2C%20often%20failing%20to%0Afully%20exploit%20the%20expressive%20capacity%20of%20visual%20information%20as%20evidenced%20by%0Amaintained%20performance%20when%20images%20are%20replaced%20with%20captions.%20However%2C%0Apractical%20retrieval%20scenarios%20frequently%20involve%20interleaved%20multi-condition%0Aqueries%20with%20multiple%20images.%20Hence%2C%20this%20paper%20introduces%20MERIT%2C%20the%20first%0Amultilingual%20dataset%20for%20interleaved%20multi-condition%20semantic%20retrieval%2C%0Acomprising%20320%2C000%20queries%20with%20135%2C000%20products%20in%205%20languages%2C%20covering%207%0Adistinct%20product%20categories.%20Extensive%20experiments%20on%20MERIT%20identify%20existing%0Amodels%27s%20limitation%3A%20focusing%20solely%20on%20global%20semantic%20information%20while%0Aneglecting%20specific%20conditional%20elements%20in%20queries.%20Consequently%2C%20we%20propose%0ACoral%2C%20a%20novel%20fine-tuning%20framework%20that%20adapts%20pre-trained%20MLLMs%20by%0Aintegrating%20embedding%20reconstruction%20to%20preserve%20fine-grained%20conditional%0Aelements%20and%20contrastive%20learning%20to%20extract%20comprehensive%20global%20semantics.%0AExperiments%20demonstrate%20that%20Coral%20achieves%20a%2045.9%25%20performance%20improvement%0Aover%20conventional%20approaches%20on%20MERIT%2C%20with%20strong%20generalization%20capabilities%0Avalidated%20across%208%20established%20retrieval%20benchmarks.%20Collectively%2C%20our%0Acontributions%20-%20a%20novel%20dataset%2C%20identification%20of%20critical%20limitations%20in%0Aexisting%20approaches%2C%20and%20an%20innovative%20fine-tuning%20framework%20-%20establish%20a%0Afoundation%20for%20future%20research%20in%20interleaved%20multi-condition%20semantic%0Aretrieval.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03144v1&entry.124074799=Read"},
{"title": "Iterative Methods for Full-Scale Gaussian Process Approximations for\n  Large Spatial Data", "author": "Tim Gyger and Reinhard Furrer and Fabio Sigrist", "abstract": "  Gaussian processes are flexible probabilistic regression models which are\nwidely used in statistics and machine learning. However, a drawback is their\nlimited scalability to large data sets. To alleviate this, full-scale\napproximations (FSAs) combine predictive process methods and covariance\ntapering, thus approximating both global and local structures. We show how\niterative methods can be used to reduce computational costs in calculating\nlikelihoods, gradients, and predictive distributions with FSAs. In particular,\nwe introduce a novel preconditioner and show theoretically and empirically that\nit accelerates the conjugate gradient method's convergence speed and mitigates\nits sensitivity with respect to the FSA parameters and the eigenvalue structure\nof the original covariance matrix, and we demonstrate empirically that it\noutperforms a state-of-the-art pivoted Cholesky preconditioner. Furthermore, we\nintroduce an accurate and fast way to calculate predictive variances using\nstochastic simulation and iterative methods. In addition, we show how our newly\nproposed FITC preconditioner can also be used in iterative methods for Vecchia\napproximations. In our experiments, it outperforms existing state-of-the-art\npreconditioners for Vecchia approximations. All methods are implemented in a\nfree C++ software library with high-level Python and R packages.\n", "link": "http://arxiv.org/abs/2405.14492v3", "date": "2025-06-03", "relevancy": 2.6079, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.541}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5187}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5051}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Iterative%20Methods%20for%20Full-Scale%20Gaussian%20Process%20Approximations%20for%0A%20%20Large%20Spatial%20Data&body=Title%3A%20Iterative%20Methods%20for%20Full-Scale%20Gaussian%20Process%20Approximations%20for%0A%20%20Large%20Spatial%20Data%0AAuthor%3A%20Tim%20Gyger%20and%20Reinhard%20Furrer%20and%20Fabio%20Sigrist%0AAbstract%3A%20%20%20Gaussian%20processes%20are%20flexible%20probabilistic%20regression%20models%20which%20are%0Awidely%20used%20in%20statistics%20and%20machine%20learning.%20However%2C%20a%20drawback%20is%20their%0Alimited%20scalability%20to%20large%20data%20sets.%20To%20alleviate%20this%2C%20full-scale%0Aapproximations%20%28FSAs%29%20combine%20predictive%20process%20methods%20and%20covariance%0Atapering%2C%20thus%20approximating%20both%20global%20and%20local%20structures.%20We%20show%20how%0Aiterative%20methods%20can%20be%20used%20to%20reduce%20computational%20costs%20in%20calculating%0Alikelihoods%2C%20gradients%2C%20and%20predictive%20distributions%20with%20FSAs.%20In%20particular%2C%0Awe%20introduce%20a%20novel%20preconditioner%20and%20show%20theoretically%20and%20empirically%20that%0Ait%20accelerates%20the%20conjugate%20gradient%20method%27s%20convergence%20speed%20and%20mitigates%0Aits%20sensitivity%20with%20respect%20to%20the%20FSA%20parameters%20and%20the%20eigenvalue%20structure%0Aof%20the%20original%20covariance%20matrix%2C%20and%20we%20demonstrate%20empirically%20that%20it%0Aoutperforms%20a%20state-of-the-art%20pivoted%20Cholesky%20preconditioner.%20Furthermore%2C%20we%0Aintroduce%20an%20accurate%20and%20fast%20way%20to%20calculate%20predictive%20variances%20using%0Astochastic%20simulation%20and%20iterative%20methods.%20In%20addition%2C%20we%20show%20how%20our%20newly%0Aproposed%20FITC%20preconditioner%20can%20also%20be%20used%20in%20iterative%20methods%20for%20Vecchia%0Aapproximations.%20In%20our%20experiments%2C%20it%20outperforms%20existing%20state-of-the-art%0Apreconditioners%20for%20Vecchia%20approximations.%20All%20methods%20are%20implemented%20in%20a%0Afree%20C%2B%2B%20software%20library%20with%20high-level%20Python%20and%20R%20packages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.14492v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIterative%2520Methods%2520for%2520Full-Scale%2520Gaussian%2520Process%2520Approximations%2520for%250A%2520%2520Large%2520Spatial%2520Data%26entry.906535625%3DTim%2520Gyger%2520and%2520Reinhard%2520Furrer%2520and%2520Fabio%2520Sigrist%26entry.1292438233%3D%2520%2520Gaussian%2520processes%2520are%2520flexible%2520probabilistic%2520regression%2520models%2520which%2520are%250Awidely%2520used%2520in%2520statistics%2520and%2520machine%2520learning.%2520However%252C%2520a%2520drawback%2520is%2520their%250Alimited%2520scalability%2520to%2520large%2520data%2520sets.%2520To%2520alleviate%2520this%252C%2520full-scale%250Aapproximations%2520%2528FSAs%2529%2520combine%2520predictive%2520process%2520methods%2520and%2520covariance%250Atapering%252C%2520thus%2520approximating%2520both%2520global%2520and%2520local%2520structures.%2520We%2520show%2520how%250Aiterative%2520methods%2520can%2520be%2520used%2520to%2520reduce%2520computational%2520costs%2520in%2520calculating%250Alikelihoods%252C%2520gradients%252C%2520and%2520predictive%2520distributions%2520with%2520FSAs.%2520In%2520particular%252C%250Awe%2520introduce%2520a%2520novel%2520preconditioner%2520and%2520show%2520theoretically%2520and%2520empirically%2520that%250Ait%2520accelerates%2520the%2520conjugate%2520gradient%2520method%2527s%2520convergence%2520speed%2520and%2520mitigates%250Aits%2520sensitivity%2520with%2520respect%2520to%2520the%2520FSA%2520parameters%2520and%2520the%2520eigenvalue%2520structure%250Aof%2520the%2520original%2520covariance%2520matrix%252C%2520and%2520we%2520demonstrate%2520empirically%2520that%2520it%250Aoutperforms%2520a%2520state-of-the-art%2520pivoted%2520Cholesky%2520preconditioner.%2520Furthermore%252C%2520we%250Aintroduce%2520an%2520accurate%2520and%2520fast%2520way%2520to%2520calculate%2520predictive%2520variances%2520using%250Astochastic%2520simulation%2520and%2520iterative%2520methods.%2520In%2520addition%252C%2520we%2520show%2520how%2520our%2520newly%250Aproposed%2520FITC%2520preconditioner%2520can%2520also%2520be%2520used%2520in%2520iterative%2520methods%2520for%2520Vecchia%250Aapproximations.%2520In%2520our%2520experiments%252C%2520it%2520outperforms%2520existing%2520state-of-the-art%250Apreconditioners%2520for%2520Vecchia%2520approximations.%2520All%2520methods%2520are%2520implemented%2520in%2520a%250Afree%2520C%252B%252B%2520software%2520library%2520with%2520high-level%2520Python%2520and%2520R%2520packages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.14492v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Iterative%20Methods%20for%20Full-Scale%20Gaussian%20Process%20Approximations%20for%0A%20%20Large%20Spatial%20Data&entry.906535625=Tim%20Gyger%20and%20Reinhard%20Furrer%20and%20Fabio%20Sigrist&entry.1292438233=%20%20Gaussian%20processes%20are%20flexible%20probabilistic%20regression%20models%20which%20are%0Awidely%20used%20in%20statistics%20and%20machine%20learning.%20However%2C%20a%20drawback%20is%20their%0Alimited%20scalability%20to%20large%20data%20sets.%20To%20alleviate%20this%2C%20full-scale%0Aapproximations%20%28FSAs%29%20combine%20predictive%20process%20methods%20and%20covariance%0Atapering%2C%20thus%20approximating%20both%20global%20and%20local%20structures.%20We%20show%20how%0Aiterative%20methods%20can%20be%20used%20to%20reduce%20computational%20costs%20in%20calculating%0Alikelihoods%2C%20gradients%2C%20and%20predictive%20distributions%20with%20FSAs.%20In%20particular%2C%0Awe%20introduce%20a%20novel%20preconditioner%20and%20show%20theoretically%20and%20empirically%20that%0Ait%20accelerates%20the%20conjugate%20gradient%20method%27s%20convergence%20speed%20and%20mitigates%0Aits%20sensitivity%20with%20respect%20to%20the%20FSA%20parameters%20and%20the%20eigenvalue%20structure%0Aof%20the%20original%20covariance%20matrix%2C%20and%20we%20demonstrate%20empirically%20that%20it%0Aoutperforms%20a%20state-of-the-art%20pivoted%20Cholesky%20preconditioner.%20Furthermore%2C%20we%0Aintroduce%20an%20accurate%20and%20fast%20way%20to%20calculate%20predictive%20variances%20using%0Astochastic%20simulation%20and%20iterative%20methods.%20In%20addition%2C%20we%20show%20how%20our%20newly%0Aproposed%20FITC%20preconditioner%20can%20also%20be%20used%20in%20iterative%20methods%20for%20Vecchia%0Aapproximations.%20In%20our%20experiments%2C%20it%20outperforms%20existing%20state-of-the-art%0Apreconditioners%20for%20Vecchia%20approximations.%20All%20methods%20are%20implemented%20in%20a%0Afree%20C%2B%2B%20software%20library%20with%20high-level%20Python%20and%20R%20packages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.14492v3&entry.124074799=Read"},
{"title": "FlySearch: Exploring how vision-language models explore", "author": "Adam Pardyl and Dominik Matuszek and Mateusz Przebieracz and Marek Cygan and Bartosz Zieli\u0144ski and Maciej Wo\u0142czyk", "abstract": "  The real world is messy and unstructured. Uncovering critical information\noften requires active, goal-driven exploration. It remains to be seen whether\nVision-Language Models (VLMs), which recently emerged as a popular zero-shot\ntool in many difficult tasks, can operate effectively in such conditions. In\nthis paper, we answer this question by introducing FlySearch, a 3D, outdoor,\nphotorealistic environment for searching and navigating to objects in complex\nscenes. We define three sets of scenarios with varying difficulty and observe\nthat state-of-the-art VLMs cannot reliably solve even the simplest exploration\ntasks, with the gap to human performance increasing as the tasks get harder. We\nidentify a set of central causes, ranging from vision hallucination, through\ncontext misunderstanding, to task planning failures, and we show that some of\nthem can be addressed by finetuning. We publicly release the benchmark,\nscenarios, and the underlying codebase.\n", "link": "http://arxiv.org/abs/2506.02896v1", "date": "2025-06-03", "relevancy": 2.6057, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6663}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6663}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlySearch%3A%20Exploring%20how%20vision-language%20models%20explore&body=Title%3A%20FlySearch%3A%20Exploring%20how%20vision-language%20models%20explore%0AAuthor%3A%20Adam%20Pardyl%20and%20Dominik%20Matuszek%20and%20Mateusz%20Przebieracz%20and%20Marek%20Cygan%20and%20Bartosz%20Zieli%C5%84ski%20and%20Maciej%20Wo%C5%82czyk%0AAbstract%3A%20%20%20The%20real%20world%20is%20messy%20and%20unstructured.%20Uncovering%20critical%20information%0Aoften%20requires%20active%2C%20goal-driven%20exploration.%20It%20remains%20to%20be%20seen%20whether%0AVision-Language%20Models%20%28VLMs%29%2C%20which%20recently%20emerged%20as%20a%20popular%20zero-shot%0Atool%20in%20many%20difficult%20tasks%2C%20can%20operate%20effectively%20in%20such%20conditions.%20In%0Athis%20paper%2C%20we%20answer%20this%20question%20by%20introducing%20FlySearch%2C%20a%203D%2C%20outdoor%2C%0Aphotorealistic%20environment%20for%20searching%20and%20navigating%20to%20objects%20in%20complex%0Ascenes.%20We%20define%20three%20sets%20of%20scenarios%20with%20varying%20difficulty%20and%20observe%0Athat%20state-of-the-art%20VLMs%20cannot%20reliably%20solve%20even%20the%20simplest%20exploration%0Atasks%2C%20with%20the%20gap%20to%20human%20performance%20increasing%20as%20the%20tasks%20get%20harder.%20We%0Aidentify%20a%20set%20of%20central%20causes%2C%20ranging%20from%20vision%20hallucination%2C%20through%0Acontext%20misunderstanding%2C%20to%20task%20planning%20failures%2C%20and%20we%20show%20that%20some%20of%0Athem%20can%20be%20addressed%20by%20finetuning.%20We%20publicly%20release%20the%20benchmark%2C%0Ascenarios%2C%20and%20the%20underlying%20codebase.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02896v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlySearch%253A%2520Exploring%2520how%2520vision-language%2520models%2520explore%26entry.906535625%3DAdam%2520Pardyl%2520and%2520Dominik%2520Matuszek%2520and%2520Mateusz%2520Przebieracz%2520and%2520Marek%2520Cygan%2520and%2520Bartosz%2520Zieli%25C5%2584ski%2520and%2520Maciej%2520Wo%25C5%2582czyk%26entry.1292438233%3D%2520%2520The%2520real%2520world%2520is%2520messy%2520and%2520unstructured.%2520Uncovering%2520critical%2520information%250Aoften%2520requires%2520active%252C%2520goal-driven%2520exploration.%2520It%2520remains%2520to%2520be%2520seen%2520whether%250AVision-Language%2520Models%2520%2528VLMs%2529%252C%2520which%2520recently%2520emerged%2520as%2520a%2520popular%2520zero-shot%250Atool%2520in%2520many%2520difficult%2520tasks%252C%2520can%2520operate%2520effectively%2520in%2520such%2520conditions.%2520In%250Athis%2520paper%252C%2520we%2520answer%2520this%2520question%2520by%2520introducing%2520FlySearch%252C%2520a%25203D%252C%2520outdoor%252C%250Aphotorealistic%2520environment%2520for%2520searching%2520and%2520navigating%2520to%2520objects%2520in%2520complex%250Ascenes.%2520We%2520define%2520three%2520sets%2520of%2520scenarios%2520with%2520varying%2520difficulty%2520and%2520observe%250Athat%2520state-of-the-art%2520VLMs%2520cannot%2520reliably%2520solve%2520even%2520the%2520simplest%2520exploration%250Atasks%252C%2520with%2520the%2520gap%2520to%2520human%2520performance%2520increasing%2520as%2520the%2520tasks%2520get%2520harder.%2520We%250Aidentify%2520a%2520set%2520of%2520central%2520causes%252C%2520ranging%2520from%2520vision%2520hallucination%252C%2520through%250Acontext%2520misunderstanding%252C%2520to%2520task%2520planning%2520failures%252C%2520and%2520we%2520show%2520that%2520some%2520of%250Athem%2520can%2520be%2520addressed%2520by%2520finetuning.%2520We%2520publicly%2520release%2520the%2520benchmark%252C%250Ascenarios%252C%2520and%2520the%2520underlying%2520codebase.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02896v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlySearch%3A%20Exploring%20how%20vision-language%20models%20explore&entry.906535625=Adam%20Pardyl%20and%20Dominik%20Matuszek%20and%20Mateusz%20Przebieracz%20and%20Marek%20Cygan%20and%20Bartosz%20Zieli%C5%84ski%20and%20Maciej%20Wo%C5%82czyk&entry.1292438233=%20%20The%20real%20world%20is%20messy%20and%20unstructured.%20Uncovering%20critical%20information%0Aoften%20requires%20active%2C%20goal-driven%20exploration.%20It%20remains%20to%20be%20seen%20whether%0AVision-Language%20Models%20%28VLMs%29%2C%20which%20recently%20emerged%20as%20a%20popular%20zero-shot%0Atool%20in%20many%20difficult%20tasks%2C%20can%20operate%20effectively%20in%20such%20conditions.%20In%0Athis%20paper%2C%20we%20answer%20this%20question%20by%20introducing%20FlySearch%2C%20a%203D%2C%20outdoor%2C%0Aphotorealistic%20environment%20for%20searching%20and%20navigating%20to%20objects%20in%20complex%0Ascenes.%20We%20define%20three%20sets%20of%20scenarios%20with%20varying%20difficulty%20and%20observe%0Athat%20state-of-the-art%20VLMs%20cannot%20reliably%20solve%20even%20the%20simplest%20exploration%0Atasks%2C%20with%20the%20gap%20to%20human%20performance%20increasing%20as%20the%20tasks%20get%20harder.%20We%0Aidentify%20a%20set%20of%20central%20causes%2C%20ranging%20from%20vision%20hallucination%2C%20through%0Acontext%20misunderstanding%2C%20to%20task%20planning%20failures%2C%20and%20we%20show%20that%20some%20of%0Athem%20can%20be%20addressed%20by%20finetuning.%20We%20publicly%20release%20the%20benchmark%2C%0Ascenarios%2C%20and%20the%20underlying%20codebase.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02896v1&entry.124074799=Read"},
{"title": "GaRA-SAM: Robustifying Segment Anything Model with Gated-Rank Adaptation", "author": "Sohyun Lee and Yeho Kwon and Lukas Hoyer and Suha Kwak", "abstract": "  Improving robustness of the Segment Anything Model (SAM) to input\ndegradations is critical for its deployment in high-stakes applications such as\nautonomous driving and robotics. Our approach to this challenge prioritizes\nthree key aspects: first, parameter efficiency to maintain the inherent\ngeneralization capability of SAM; second, fine-grained and input-aware\nrobustification to precisely address the input corruption; and third, adherence\nto standard training protocols for ease of training. To this end, we propose\ngated-rank adaptation (GaRA). GaRA introduces lightweight adapters into\nintermediate layers of the frozen SAM, where each adapter dynamically adjusts\nthe effective rank of its weight matrix based on the input by selectively\nactivating (rank-1) components of the matrix using a learned gating module.\nThis adjustment enables fine-grained and input-aware robustification without\ncompromising the generalization capability of SAM. Our model, GaRA-SAM,\nsignificantly outperforms prior work on all robust segmentation benchmarks. In\nparticular, it surpasses the previous best IoU score by up to 21.3\\%p on ACDC,\na challenging real corrupted image dataset.\n", "link": "http://arxiv.org/abs/2506.02882v1", "date": "2025-06-03", "relevancy": 2.6005, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5457}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5079}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GaRA-SAM%3A%20Robustifying%20Segment%20Anything%20Model%20with%20Gated-Rank%20Adaptation&body=Title%3A%20GaRA-SAM%3A%20Robustifying%20Segment%20Anything%20Model%20with%20Gated-Rank%20Adaptation%0AAuthor%3A%20Sohyun%20Lee%20and%20Yeho%20Kwon%20and%20Lukas%20Hoyer%20and%20Suha%20Kwak%0AAbstract%3A%20%20%20Improving%20robustness%20of%20the%20Segment%20Anything%20Model%20%28SAM%29%20to%20input%0Adegradations%20is%20critical%20for%20its%20deployment%20in%20high-stakes%20applications%20such%20as%0Aautonomous%20driving%20and%20robotics.%20Our%20approach%20to%20this%20challenge%20prioritizes%0Athree%20key%20aspects%3A%20first%2C%20parameter%20efficiency%20to%20maintain%20the%20inherent%0Ageneralization%20capability%20of%20SAM%3B%20second%2C%20fine-grained%20and%20input-aware%0Arobustification%20to%20precisely%20address%20the%20input%20corruption%3B%20and%20third%2C%20adherence%0Ato%20standard%20training%20protocols%20for%20ease%20of%20training.%20To%20this%20end%2C%20we%20propose%0Agated-rank%20adaptation%20%28GaRA%29.%20GaRA%20introduces%20lightweight%20adapters%20into%0Aintermediate%20layers%20of%20the%20frozen%20SAM%2C%20where%20each%20adapter%20dynamically%20adjusts%0Athe%20effective%20rank%20of%20its%20weight%20matrix%20based%20on%20the%20input%20by%20selectively%0Aactivating%20%28rank-1%29%20components%20of%20the%20matrix%20using%20a%20learned%20gating%20module.%0AThis%20adjustment%20enables%20fine-grained%20and%20input-aware%20robustification%20without%0Acompromising%20the%20generalization%20capability%20of%20SAM.%20Our%20model%2C%20GaRA-SAM%2C%0Asignificantly%20outperforms%20prior%20work%20on%20all%20robust%20segmentation%20benchmarks.%20In%0Aparticular%2C%20it%20surpasses%20the%20previous%20best%20IoU%20score%20by%20up%20to%2021.3%5C%25p%20on%20ACDC%2C%0Aa%20challenging%20real%20corrupted%20image%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02882v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaRA-SAM%253A%2520Robustifying%2520Segment%2520Anything%2520Model%2520with%2520Gated-Rank%2520Adaptation%26entry.906535625%3DSohyun%2520Lee%2520and%2520Yeho%2520Kwon%2520and%2520Lukas%2520Hoyer%2520and%2520Suha%2520Kwak%26entry.1292438233%3D%2520%2520Improving%2520robustness%2520of%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520to%2520input%250Adegradations%2520is%2520critical%2520for%2520its%2520deployment%2520in%2520high-stakes%2520applications%2520such%2520as%250Aautonomous%2520driving%2520and%2520robotics.%2520Our%2520approach%2520to%2520this%2520challenge%2520prioritizes%250Athree%2520key%2520aspects%253A%2520first%252C%2520parameter%2520efficiency%2520to%2520maintain%2520the%2520inherent%250Ageneralization%2520capability%2520of%2520SAM%253B%2520second%252C%2520fine-grained%2520and%2520input-aware%250Arobustification%2520to%2520precisely%2520address%2520the%2520input%2520corruption%253B%2520and%2520third%252C%2520adherence%250Ato%2520standard%2520training%2520protocols%2520for%2520ease%2520of%2520training.%2520To%2520this%2520end%252C%2520we%2520propose%250Agated-rank%2520adaptation%2520%2528GaRA%2529.%2520GaRA%2520introduces%2520lightweight%2520adapters%2520into%250Aintermediate%2520layers%2520of%2520the%2520frozen%2520SAM%252C%2520where%2520each%2520adapter%2520dynamically%2520adjusts%250Athe%2520effective%2520rank%2520of%2520its%2520weight%2520matrix%2520based%2520on%2520the%2520input%2520by%2520selectively%250Aactivating%2520%2528rank-1%2529%2520components%2520of%2520the%2520matrix%2520using%2520a%2520learned%2520gating%2520module.%250AThis%2520adjustment%2520enables%2520fine-grained%2520and%2520input-aware%2520robustification%2520without%250Acompromising%2520the%2520generalization%2520capability%2520of%2520SAM.%2520Our%2520model%252C%2520GaRA-SAM%252C%250Asignificantly%2520outperforms%2520prior%2520work%2520on%2520all%2520robust%2520segmentation%2520benchmarks.%2520In%250Aparticular%252C%2520it%2520surpasses%2520the%2520previous%2520best%2520IoU%2520score%2520by%2520up%2520to%252021.3%255C%2525p%2520on%2520ACDC%252C%250Aa%2520challenging%2520real%2520corrupted%2520image%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02882v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaRA-SAM%3A%20Robustifying%20Segment%20Anything%20Model%20with%20Gated-Rank%20Adaptation&entry.906535625=Sohyun%20Lee%20and%20Yeho%20Kwon%20and%20Lukas%20Hoyer%20and%20Suha%20Kwak&entry.1292438233=%20%20Improving%20robustness%20of%20the%20Segment%20Anything%20Model%20%28SAM%29%20to%20input%0Adegradations%20is%20critical%20for%20its%20deployment%20in%20high-stakes%20applications%20such%20as%0Aautonomous%20driving%20and%20robotics.%20Our%20approach%20to%20this%20challenge%20prioritizes%0Athree%20key%20aspects%3A%20first%2C%20parameter%20efficiency%20to%20maintain%20the%20inherent%0Ageneralization%20capability%20of%20SAM%3B%20second%2C%20fine-grained%20and%20input-aware%0Arobustification%20to%20precisely%20address%20the%20input%20corruption%3B%20and%20third%2C%20adherence%0Ato%20standard%20training%20protocols%20for%20ease%20of%20training.%20To%20this%20end%2C%20we%20propose%0Agated-rank%20adaptation%20%28GaRA%29.%20GaRA%20introduces%20lightweight%20adapters%20into%0Aintermediate%20layers%20of%20the%20frozen%20SAM%2C%20where%20each%20adapter%20dynamically%20adjusts%0Athe%20effective%20rank%20of%20its%20weight%20matrix%20based%20on%20the%20input%20by%20selectively%0Aactivating%20%28rank-1%29%20components%20of%20the%20matrix%20using%20a%20learned%20gating%20module.%0AThis%20adjustment%20enables%20fine-grained%20and%20input-aware%20robustification%20without%0Acompromising%20the%20generalization%20capability%20of%20SAM.%20Our%20model%2C%20GaRA-SAM%2C%0Asignificantly%20outperforms%20prior%20work%20on%20all%20robust%20segmentation%20benchmarks.%20In%0Aparticular%2C%20it%20surpasses%20the%20previous%20best%20IoU%20score%20by%20up%20to%2021.3%5C%25p%20on%20ACDC%2C%0Aa%20challenging%20real%20corrupted%20image%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02882v1&entry.124074799=Read"},
{"title": "Dense Match Summarization for Faster Two-view Estimation", "author": "Jonathan Astermark and Anders Heyden and Viktor Larsson", "abstract": "  In this paper, we speed up robust two-view relative pose from dense\ncorrespondences. Previous work has shown that dense matchers can significantly\nimprove both accuracy and robustness in the resulting pose. However, the large\nnumber of matches comes with a significantly increased runtime during robust\nestimation in RANSAC. To avoid this, we propose an efficient match\nsummarization scheme which provides comparable accuracy to using the full set\nof dense matches, while having 10-100x faster runtime. We validate our approach\non standard benchmark datasets together with multiple state-of-the-art dense\nmatchers.\n", "link": "http://arxiv.org/abs/2506.02893v1", "date": "2025-06-03", "relevancy": 2.582, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5218}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5173}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dense%20Match%20Summarization%20for%20Faster%20Two-view%20Estimation&body=Title%3A%20Dense%20Match%20Summarization%20for%20Faster%20Two-view%20Estimation%0AAuthor%3A%20Jonathan%20Astermark%20and%20Anders%20Heyden%20and%20Viktor%20Larsson%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20speed%20up%20robust%20two-view%20relative%20pose%20from%20dense%0Acorrespondences.%20Previous%20work%20has%20shown%20that%20dense%20matchers%20can%20significantly%0Aimprove%20both%20accuracy%20and%20robustness%20in%20the%20resulting%20pose.%20However%2C%20the%20large%0Anumber%20of%20matches%20comes%20with%20a%20significantly%20increased%20runtime%20during%20robust%0Aestimation%20in%20RANSAC.%20To%20avoid%20this%2C%20we%20propose%20an%20efficient%20match%0Asummarization%20scheme%20which%20provides%20comparable%20accuracy%20to%20using%20the%20full%20set%0Aof%20dense%20matches%2C%20while%20having%2010-100x%20faster%20runtime.%20We%20validate%20our%20approach%0Aon%20standard%20benchmark%20datasets%20together%20with%20multiple%20state-of-the-art%20dense%0Amatchers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02893v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDense%2520Match%2520Summarization%2520for%2520Faster%2520Two-view%2520Estimation%26entry.906535625%3DJonathan%2520Astermark%2520and%2520Anders%2520Heyden%2520and%2520Viktor%2520Larsson%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520speed%2520up%2520robust%2520two-view%2520relative%2520pose%2520from%2520dense%250Acorrespondences.%2520Previous%2520work%2520has%2520shown%2520that%2520dense%2520matchers%2520can%2520significantly%250Aimprove%2520both%2520accuracy%2520and%2520robustness%2520in%2520the%2520resulting%2520pose.%2520However%252C%2520the%2520large%250Anumber%2520of%2520matches%2520comes%2520with%2520a%2520significantly%2520increased%2520runtime%2520during%2520robust%250Aestimation%2520in%2520RANSAC.%2520To%2520avoid%2520this%252C%2520we%2520propose%2520an%2520efficient%2520match%250Asummarization%2520scheme%2520which%2520provides%2520comparable%2520accuracy%2520to%2520using%2520the%2520full%2520set%250Aof%2520dense%2520matches%252C%2520while%2520having%252010-100x%2520faster%2520runtime.%2520We%2520validate%2520our%2520approach%250Aon%2520standard%2520benchmark%2520datasets%2520together%2520with%2520multiple%2520state-of-the-art%2520dense%250Amatchers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02893v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dense%20Match%20Summarization%20for%20Faster%20Two-view%20Estimation&entry.906535625=Jonathan%20Astermark%20and%20Anders%20Heyden%20and%20Viktor%20Larsson&entry.1292438233=%20%20In%20this%20paper%2C%20we%20speed%20up%20robust%20two-view%20relative%20pose%20from%20dense%0Acorrespondences.%20Previous%20work%20has%20shown%20that%20dense%20matchers%20can%20significantly%0Aimprove%20both%20accuracy%20and%20robustness%20in%20the%20resulting%20pose.%20However%2C%20the%20large%0Anumber%20of%20matches%20comes%20with%20a%20significantly%20increased%20runtime%20during%20robust%0Aestimation%20in%20RANSAC.%20To%20avoid%20this%2C%20we%20propose%20an%20efficient%20match%0Asummarization%20scheme%20which%20provides%20comparable%20accuracy%20to%20using%20the%20full%20set%0Aof%20dense%20matches%2C%20while%20having%2010-100x%20faster%20runtime.%20We%20validate%20our%20approach%0Aon%20standard%20benchmark%20datasets%20together%20with%20multiple%20state-of-the-art%20dense%0Amatchers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02893v1&entry.124074799=Read"},
{"title": "RACE-Align: Retrieval-Augmented and Chain-of-Thought Enhanced Preference\n  Alignment for Large Language Models", "author": "Qihang Yan and Xinyu Zhang and Luming Guo and Qi Zhang and Feifan Liu", "abstract": "  Large Language Models (LLMs) struggle with accuracy, domain-specific\nreasoning, and interpretability in vertical domains. Traditional preference\nalignment methods like Reinforcement Learning from Human Feedback (RLHF) and\nDirect Preference Optimization (DPO) often overlook the underlying knowledge\nsources and reasoning logic. This paper introduces RACE-Align\n(Retrieval-Augmented and Chain-of-Thought Enhanced Alignment), a novel\nframework designed to address these limitations. RACE-Align systematically\nconstructs a binary preference dataset incorporating external knowledge support\nand explicit Chain-of-Thought (CoT) reasoning, then aligns LLMs using the DPO\nalgorithm. The core innovation lies in its preference data construction\nstrategy: it integrates AI-driven retrieval for factual grounding, enhancing\nknowledgeability and accuracy, and emphasizes the optimization of\ndomain-specific CoT, treating the reasoning process itself as a key preference\ndimension. A multi-stage, AI-driven refinement pipeline cost-effectively\ngenerates these preference pairs. Experimental validation in Traditional\nChinese Medicine (TCM) using Qwen3-1.7B as the base model demonstrates that\nRACE-Align significantly outperforms the original base model and a model\nfine-tuned only with Supervised Fine-Tuning (SFT). Improvements were observed\nacross multiple dimensions, including answer accuracy, information richness,\napplication of TCM thinking patterns, logicality and depth of reasoning, and\ninterpretability. These findings suggest RACE-Align offers an effective pathway\nto enhance LLMs' knowledge application, reasoning reliability, and process\ntransparency in complex vertical domains.\n", "link": "http://arxiv.org/abs/2506.02726v1", "date": "2025-06-03", "relevancy": 2.579, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5261}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5107}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5107}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RACE-Align%3A%20Retrieval-Augmented%20and%20Chain-of-Thought%20Enhanced%20Preference%0A%20%20Alignment%20for%20Large%20Language%20Models&body=Title%3A%20RACE-Align%3A%20Retrieval-Augmented%20and%20Chain-of-Thought%20Enhanced%20Preference%0A%20%20Alignment%20for%20Large%20Language%20Models%0AAuthor%3A%20Qihang%20Yan%20and%20Xinyu%20Zhang%20and%20Luming%20Guo%20and%20Qi%20Zhang%20and%20Feifan%20Liu%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20struggle%20with%20accuracy%2C%20domain-specific%0Areasoning%2C%20and%20interpretability%20in%20vertical%20domains.%20Traditional%20preference%0Aalignment%20methods%20like%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20and%0ADirect%20Preference%20Optimization%20%28DPO%29%20often%20overlook%20the%20underlying%20knowledge%0Asources%20and%20reasoning%20logic.%20This%20paper%20introduces%20RACE-Align%0A%28Retrieval-Augmented%20and%20Chain-of-Thought%20Enhanced%20Alignment%29%2C%20a%20novel%0Aframework%20designed%20to%20address%20these%20limitations.%20RACE-Align%20systematically%0Aconstructs%20a%20binary%20preference%20dataset%20incorporating%20external%20knowledge%20support%0Aand%20explicit%20Chain-of-Thought%20%28CoT%29%20reasoning%2C%20then%20aligns%20LLMs%20using%20the%20DPO%0Aalgorithm.%20The%20core%20innovation%20lies%20in%20its%20preference%20data%20construction%0Astrategy%3A%20it%20integrates%20AI-driven%20retrieval%20for%20factual%20grounding%2C%20enhancing%0Aknowledgeability%20and%20accuracy%2C%20and%20emphasizes%20the%20optimization%20of%0Adomain-specific%20CoT%2C%20treating%20the%20reasoning%20process%20itself%20as%20a%20key%20preference%0Adimension.%20A%20multi-stage%2C%20AI-driven%20refinement%20pipeline%20cost-effectively%0Agenerates%20these%20preference%20pairs.%20Experimental%20validation%20in%20Traditional%0AChinese%20Medicine%20%28TCM%29%20using%20Qwen3-1.7B%20as%20the%20base%20model%20demonstrates%20that%0ARACE-Align%20significantly%20outperforms%20the%20original%20base%20model%20and%20a%20model%0Afine-tuned%20only%20with%20Supervised%20Fine-Tuning%20%28SFT%29.%20Improvements%20were%20observed%0Aacross%20multiple%20dimensions%2C%20including%20answer%20accuracy%2C%20information%20richness%2C%0Aapplication%20of%20TCM%20thinking%20patterns%2C%20logicality%20and%20depth%20of%20reasoning%2C%20and%0Ainterpretability.%20These%20findings%20suggest%20RACE-Align%20offers%20an%20effective%20pathway%0Ato%20enhance%20LLMs%27%20knowledge%20application%2C%20reasoning%20reliability%2C%20and%20process%0Atransparency%20in%20complex%20vertical%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02726v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRACE-Align%253A%2520Retrieval-Augmented%2520and%2520Chain-of-Thought%2520Enhanced%2520Preference%250A%2520%2520Alignment%2520for%2520Large%2520Language%2520Models%26entry.906535625%3DQihang%2520Yan%2520and%2520Xinyu%2520Zhang%2520and%2520Luming%2520Guo%2520and%2520Qi%2520Zhang%2520and%2520Feifan%2520Liu%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520struggle%2520with%2520accuracy%252C%2520domain-specific%250Areasoning%252C%2520and%2520interpretability%2520in%2520vertical%2520domains.%2520Traditional%2520preference%250Aalignment%2520methods%2520like%2520Reinforcement%2520Learning%2520from%2520Human%2520Feedback%2520%2528RLHF%2529%2520and%250ADirect%2520Preference%2520Optimization%2520%2528DPO%2529%2520often%2520overlook%2520the%2520underlying%2520knowledge%250Asources%2520and%2520reasoning%2520logic.%2520This%2520paper%2520introduces%2520RACE-Align%250A%2528Retrieval-Augmented%2520and%2520Chain-of-Thought%2520Enhanced%2520Alignment%2529%252C%2520a%2520novel%250Aframework%2520designed%2520to%2520address%2520these%2520limitations.%2520RACE-Align%2520systematically%250Aconstructs%2520a%2520binary%2520preference%2520dataset%2520incorporating%2520external%2520knowledge%2520support%250Aand%2520explicit%2520Chain-of-Thought%2520%2528CoT%2529%2520reasoning%252C%2520then%2520aligns%2520LLMs%2520using%2520the%2520DPO%250Aalgorithm.%2520The%2520core%2520innovation%2520lies%2520in%2520its%2520preference%2520data%2520construction%250Astrategy%253A%2520it%2520integrates%2520AI-driven%2520retrieval%2520for%2520factual%2520grounding%252C%2520enhancing%250Aknowledgeability%2520and%2520accuracy%252C%2520and%2520emphasizes%2520the%2520optimization%2520of%250Adomain-specific%2520CoT%252C%2520treating%2520the%2520reasoning%2520process%2520itself%2520as%2520a%2520key%2520preference%250Adimension.%2520A%2520multi-stage%252C%2520AI-driven%2520refinement%2520pipeline%2520cost-effectively%250Agenerates%2520these%2520preference%2520pairs.%2520Experimental%2520validation%2520in%2520Traditional%250AChinese%2520Medicine%2520%2528TCM%2529%2520using%2520Qwen3-1.7B%2520as%2520the%2520base%2520model%2520demonstrates%2520that%250ARACE-Align%2520significantly%2520outperforms%2520the%2520original%2520base%2520model%2520and%2520a%2520model%250Afine-tuned%2520only%2520with%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529.%2520Improvements%2520were%2520observed%250Aacross%2520multiple%2520dimensions%252C%2520including%2520answer%2520accuracy%252C%2520information%2520richness%252C%250Aapplication%2520of%2520TCM%2520thinking%2520patterns%252C%2520logicality%2520and%2520depth%2520of%2520reasoning%252C%2520and%250Ainterpretability.%2520These%2520findings%2520suggest%2520RACE-Align%2520offers%2520an%2520effective%2520pathway%250Ato%2520enhance%2520LLMs%2527%2520knowledge%2520application%252C%2520reasoning%2520reliability%252C%2520and%2520process%250Atransparency%2520in%2520complex%2520vertical%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02726v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RACE-Align%3A%20Retrieval-Augmented%20and%20Chain-of-Thought%20Enhanced%20Preference%0A%20%20Alignment%20for%20Large%20Language%20Models&entry.906535625=Qihang%20Yan%20and%20Xinyu%20Zhang%20and%20Luming%20Guo%20and%20Qi%20Zhang%20and%20Feifan%20Liu&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20struggle%20with%20accuracy%2C%20domain-specific%0Areasoning%2C%20and%20interpretability%20in%20vertical%20domains.%20Traditional%20preference%0Aalignment%20methods%20like%20Reinforcement%20Learning%20from%20Human%20Feedback%20%28RLHF%29%20and%0ADirect%20Preference%20Optimization%20%28DPO%29%20often%20overlook%20the%20underlying%20knowledge%0Asources%20and%20reasoning%20logic.%20This%20paper%20introduces%20RACE-Align%0A%28Retrieval-Augmented%20and%20Chain-of-Thought%20Enhanced%20Alignment%29%2C%20a%20novel%0Aframework%20designed%20to%20address%20these%20limitations.%20RACE-Align%20systematically%0Aconstructs%20a%20binary%20preference%20dataset%20incorporating%20external%20knowledge%20support%0Aand%20explicit%20Chain-of-Thought%20%28CoT%29%20reasoning%2C%20then%20aligns%20LLMs%20using%20the%20DPO%0Aalgorithm.%20The%20core%20innovation%20lies%20in%20its%20preference%20data%20construction%0Astrategy%3A%20it%20integrates%20AI-driven%20retrieval%20for%20factual%20grounding%2C%20enhancing%0Aknowledgeability%20and%20accuracy%2C%20and%20emphasizes%20the%20optimization%20of%0Adomain-specific%20CoT%2C%20treating%20the%20reasoning%20process%20itself%20as%20a%20key%20preference%0Adimension.%20A%20multi-stage%2C%20AI-driven%20refinement%20pipeline%20cost-effectively%0Agenerates%20these%20preference%20pairs.%20Experimental%20validation%20in%20Traditional%0AChinese%20Medicine%20%28TCM%29%20using%20Qwen3-1.7B%20as%20the%20base%20model%20demonstrates%20that%0ARACE-Align%20significantly%20outperforms%20the%20original%20base%20model%20and%20a%20model%0Afine-tuned%20only%20with%20Supervised%20Fine-Tuning%20%28SFT%29.%20Improvements%20were%20observed%0Aacross%20multiple%20dimensions%2C%20including%20answer%20accuracy%2C%20information%20richness%2C%0Aapplication%20of%20TCM%20thinking%20patterns%2C%20logicality%20and%20depth%20of%20reasoning%2C%20and%0Ainterpretability.%20These%20findings%20suggest%20RACE-Align%20offers%20an%20effective%20pathway%0Ato%20enhance%20LLMs%27%20knowledge%20application%2C%20reasoning%20reliability%2C%20and%20process%0Atransparency%20in%20complex%20vertical%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02726v1&entry.124074799=Read"},
{"title": "ToothForge: Automatic Dental Shape Generation using Synchronized\n  Spectral Embeddings", "author": "Tibor Kub\u00edk and Fran\u00e7ois Guibault and Michal \u0160pan\u011bl and Herv\u00e9 Lombaert", "abstract": "  We introduce ToothForge, a spectral approach for automatically generating\nnovel 3D teeth, effectively addressing the sparsity of dental shape datasets.\nBy operating in the spectral domain, our method enables compact machine\nlearning modeling, allowing the generation of high-resolution tooth meshes in\nmilliseconds. However, generating shape spectra comes with the instability of\nthe decomposed harmonics. To address this, we propose modeling the latent\nmanifold on synchronized frequential embeddings. Spectra of all data samples\nare aligned to a common basis prior to the training procedure, effectively\neliminating biases introduced by the decomposition instability. Furthermore,\nsynchronized modeling removes the limiting factor imposed by previous methods,\nwhich require all shapes to share a common fixed connectivity. Using a private\ndataset of real dental crowns, we observe a greater reconstruction quality of\nthe synthetized shapes, exceeding those of models trained on unaligned\nembeddings. We also explore additional applications of spectral analysis in\ndigital dentistry, such as shape compression and interpolation. ToothForge\nfacilitates a range of approaches at the intersection of spectral analysis and\nmachine learning, with fewer restrictions on mesh structure. This makes it\napplicable for shape analysis not only in dentistry, but also in broader\nmedical applications, where guaranteeing consistent connectivity across shapes\nfrom various clinics is unrealistic. The code is available at\nhttps://github.com/tiborkubik/toothForge.\n", "link": "http://arxiv.org/abs/2506.02702v1", "date": "2025-06-03", "relevancy": 2.5695, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.526}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5168}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4989}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ToothForge%3A%20Automatic%20Dental%20Shape%20Generation%20using%20Synchronized%0A%20%20Spectral%20Embeddings&body=Title%3A%20ToothForge%3A%20Automatic%20Dental%20Shape%20Generation%20using%20Synchronized%0A%20%20Spectral%20Embeddings%0AAuthor%3A%20Tibor%20Kub%C3%ADk%20and%20Fran%C3%A7ois%20Guibault%20and%20Michal%20%C5%A0pan%C4%9Bl%20and%20Herv%C3%A9%20Lombaert%0AAbstract%3A%20%20%20We%20introduce%20ToothForge%2C%20a%20spectral%20approach%20for%20automatically%20generating%0Anovel%203D%20teeth%2C%20effectively%20addressing%20the%20sparsity%20of%20dental%20shape%20datasets.%0ABy%20operating%20in%20the%20spectral%20domain%2C%20our%20method%20enables%20compact%20machine%0Alearning%20modeling%2C%20allowing%20the%20generation%20of%20high-resolution%20tooth%20meshes%20in%0Amilliseconds.%20However%2C%20generating%20shape%20spectra%20comes%20with%20the%20instability%20of%0Athe%20decomposed%20harmonics.%20To%20address%20this%2C%20we%20propose%20modeling%20the%20latent%0Amanifold%20on%20synchronized%20frequential%20embeddings.%20Spectra%20of%20all%20data%20samples%0Aare%20aligned%20to%20a%20common%20basis%20prior%20to%20the%20training%20procedure%2C%20effectively%0Aeliminating%20biases%20introduced%20by%20the%20decomposition%20instability.%20Furthermore%2C%0Asynchronized%20modeling%20removes%20the%20limiting%20factor%20imposed%20by%20previous%20methods%2C%0Awhich%20require%20all%20shapes%20to%20share%20a%20common%20fixed%20connectivity.%20Using%20a%20private%0Adataset%20of%20real%20dental%20crowns%2C%20we%20observe%20a%20greater%20reconstruction%20quality%20of%0Athe%20synthetized%20shapes%2C%20exceeding%20those%20of%20models%20trained%20on%20unaligned%0Aembeddings.%20We%20also%20explore%20additional%20applications%20of%20spectral%20analysis%20in%0Adigital%20dentistry%2C%20such%20as%20shape%20compression%20and%20interpolation.%20ToothForge%0Afacilitates%20a%20range%20of%20approaches%20at%20the%20intersection%20of%20spectral%20analysis%20and%0Amachine%20learning%2C%20with%20fewer%20restrictions%20on%20mesh%20structure.%20This%20makes%20it%0Aapplicable%20for%20shape%20analysis%20not%20only%20in%20dentistry%2C%20but%20also%20in%20broader%0Amedical%20applications%2C%20where%20guaranteeing%20consistent%20connectivity%20across%20shapes%0Afrom%20various%20clinics%20is%20unrealistic.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/tiborkubik/toothForge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02702v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToothForge%253A%2520Automatic%2520Dental%2520Shape%2520Generation%2520using%2520Synchronized%250A%2520%2520Spectral%2520Embeddings%26entry.906535625%3DTibor%2520Kub%25C3%25ADk%2520and%2520Fran%25C3%25A7ois%2520Guibault%2520and%2520Michal%2520%25C5%25A0pan%25C4%259Bl%2520and%2520Herv%25C3%25A9%2520Lombaert%26entry.1292438233%3D%2520%2520We%2520introduce%2520ToothForge%252C%2520a%2520spectral%2520approach%2520for%2520automatically%2520generating%250Anovel%25203D%2520teeth%252C%2520effectively%2520addressing%2520the%2520sparsity%2520of%2520dental%2520shape%2520datasets.%250ABy%2520operating%2520in%2520the%2520spectral%2520domain%252C%2520our%2520method%2520enables%2520compact%2520machine%250Alearning%2520modeling%252C%2520allowing%2520the%2520generation%2520of%2520high-resolution%2520tooth%2520meshes%2520in%250Amilliseconds.%2520However%252C%2520generating%2520shape%2520spectra%2520comes%2520with%2520the%2520instability%2520of%250Athe%2520decomposed%2520harmonics.%2520To%2520address%2520this%252C%2520we%2520propose%2520modeling%2520the%2520latent%250Amanifold%2520on%2520synchronized%2520frequential%2520embeddings.%2520Spectra%2520of%2520all%2520data%2520samples%250Aare%2520aligned%2520to%2520a%2520common%2520basis%2520prior%2520to%2520the%2520training%2520procedure%252C%2520effectively%250Aeliminating%2520biases%2520introduced%2520by%2520the%2520decomposition%2520instability.%2520Furthermore%252C%250Asynchronized%2520modeling%2520removes%2520the%2520limiting%2520factor%2520imposed%2520by%2520previous%2520methods%252C%250Awhich%2520require%2520all%2520shapes%2520to%2520share%2520a%2520common%2520fixed%2520connectivity.%2520Using%2520a%2520private%250Adataset%2520of%2520real%2520dental%2520crowns%252C%2520we%2520observe%2520a%2520greater%2520reconstruction%2520quality%2520of%250Athe%2520synthetized%2520shapes%252C%2520exceeding%2520those%2520of%2520models%2520trained%2520on%2520unaligned%250Aembeddings.%2520We%2520also%2520explore%2520additional%2520applications%2520of%2520spectral%2520analysis%2520in%250Adigital%2520dentistry%252C%2520such%2520as%2520shape%2520compression%2520and%2520interpolation.%2520ToothForge%250Afacilitates%2520a%2520range%2520of%2520approaches%2520at%2520the%2520intersection%2520of%2520spectral%2520analysis%2520and%250Amachine%2520learning%252C%2520with%2520fewer%2520restrictions%2520on%2520mesh%2520structure.%2520This%2520makes%2520it%250Aapplicable%2520for%2520shape%2520analysis%2520not%2520only%2520in%2520dentistry%252C%2520but%2520also%2520in%2520broader%250Amedical%2520applications%252C%2520where%2520guaranteeing%2520consistent%2520connectivity%2520across%2520shapes%250Afrom%2520various%2520clinics%2520is%2520unrealistic.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/tiborkubik/toothForge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02702v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ToothForge%3A%20Automatic%20Dental%20Shape%20Generation%20using%20Synchronized%0A%20%20Spectral%20Embeddings&entry.906535625=Tibor%20Kub%C3%ADk%20and%20Fran%C3%A7ois%20Guibault%20and%20Michal%20%C5%A0pan%C4%9Bl%20and%20Herv%C3%A9%20Lombaert&entry.1292438233=%20%20We%20introduce%20ToothForge%2C%20a%20spectral%20approach%20for%20automatically%20generating%0Anovel%203D%20teeth%2C%20effectively%20addressing%20the%20sparsity%20of%20dental%20shape%20datasets.%0ABy%20operating%20in%20the%20spectral%20domain%2C%20our%20method%20enables%20compact%20machine%0Alearning%20modeling%2C%20allowing%20the%20generation%20of%20high-resolution%20tooth%20meshes%20in%0Amilliseconds.%20However%2C%20generating%20shape%20spectra%20comes%20with%20the%20instability%20of%0Athe%20decomposed%20harmonics.%20To%20address%20this%2C%20we%20propose%20modeling%20the%20latent%0Amanifold%20on%20synchronized%20frequential%20embeddings.%20Spectra%20of%20all%20data%20samples%0Aare%20aligned%20to%20a%20common%20basis%20prior%20to%20the%20training%20procedure%2C%20effectively%0Aeliminating%20biases%20introduced%20by%20the%20decomposition%20instability.%20Furthermore%2C%0Asynchronized%20modeling%20removes%20the%20limiting%20factor%20imposed%20by%20previous%20methods%2C%0Awhich%20require%20all%20shapes%20to%20share%20a%20common%20fixed%20connectivity.%20Using%20a%20private%0Adataset%20of%20real%20dental%20crowns%2C%20we%20observe%20a%20greater%20reconstruction%20quality%20of%0Athe%20synthetized%20shapes%2C%20exceeding%20those%20of%20models%20trained%20on%20unaligned%0Aembeddings.%20We%20also%20explore%20additional%20applications%20of%20spectral%20analysis%20in%0Adigital%20dentistry%2C%20such%20as%20shape%20compression%20and%20interpolation.%20ToothForge%0Afacilitates%20a%20range%20of%20approaches%20at%20the%20intersection%20of%20spectral%20analysis%20and%0Amachine%20learning%2C%20with%20fewer%20restrictions%20on%20mesh%20structure.%20This%20makes%20it%0Aapplicable%20for%20shape%20analysis%20not%20only%20in%20dentistry%2C%20but%20also%20in%20broader%0Amedical%20applications%2C%20where%20guaranteeing%20consistent%20connectivity%20across%20shapes%0Afrom%20various%20clinics%20is%20unrealistic.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/tiborkubik/toothForge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02702v1&entry.124074799=Read"},
{"title": "TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via\n  Autoregressive Diffusion Models", "author": "Chetwin Low and Weimin Wang", "abstract": "  In this paper, we present TalkingMachines -- an efficient framework that\ntransforms pretrained video generation models into real-time, audio-driven\ncharacter animators. TalkingMachines enables natural conversational experiences\nby integrating an audio large language model (LLM) with our video generation\nfoundation model. Our primary contributions include: (1) We adapt a pretrained\nSOTA image-to-video DiT into an audio-driven avatar generation model of 18\nbillion parameters; (2) We enable infinite video streaming without error\naccumulation through asymmetric knowledge distillation from a bidirectional\nteacher model into a sparse causal, autoregressive student model; (3) We design\na high-throughput, low-latency inference pipeline incorporating several key\nengineering optimizations such as: (a) disaggregation of the DiT and VAE\ndecoder across separate devices, (b) efficient overlap of inter-device\ncommunication and computation using CUDA streams, (c) elimination of redundant\nrecomputations to maximize frame-generation throughput. Please see demo videos\nhere - https://aaxwaz.github.io/TalkingMachines/\n", "link": "http://arxiv.org/abs/2506.03099v1", "date": "2025-06-03", "relevancy": 2.5645, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6658}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6604}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TalkingMachines%3A%20Real-Time%20Audio-Driven%20FaceTime-Style%20Video%20via%0A%20%20Autoregressive%20Diffusion%20Models&body=Title%3A%20TalkingMachines%3A%20Real-Time%20Audio-Driven%20FaceTime-Style%20Video%20via%0A%20%20Autoregressive%20Diffusion%20Models%0AAuthor%3A%20Chetwin%20Low%20and%20Weimin%20Wang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20present%20TalkingMachines%20--%20an%20efficient%20framework%20that%0Atransforms%20pretrained%20video%20generation%20models%20into%20real-time%2C%20audio-driven%0Acharacter%20animators.%20TalkingMachines%20enables%20natural%20conversational%20experiences%0Aby%20integrating%20an%20audio%20large%20language%20model%20%28LLM%29%20with%20our%20video%20generation%0Afoundation%20model.%20Our%20primary%20contributions%20include%3A%20%281%29%20We%20adapt%20a%20pretrained%0ASOTA%20image-to-video%20DiT%20into%20an%20audio-driven%20avatar%20generation%20model%20of%2018%0Abillion%20parameters%3B%20%282%29%20We%20enable%20infinite%20video%20streaming%20without%20error%0Aaccumulation%20through%20asymmetric%20knowledge%20distillation%20from%20a%20bidirectional%0Ateacher%20model%20into%20a%20sparse%20causal%2C%20autoregressive%20student%20model%3B%20%283%29%20We%20design%0Aa%20high-throughput%2C%20low-latency%20inference%20pipeline%20incorporating%20several%20key%0Aengineering%20optimizations%20such%20as%3A%20%28a%29%20disaggregation%20of%20the%20DiT%20and%20VAE%0Adecoder%20across%20separate%20devices%2C%20%28b%29%20efficient%20overlap%20of%20inter-device%0Acommunication%20and%20computation%20using%20CUDA%20streams%2C%20%28c%29%20elimination%20of%20redundant%0Arecomputations%20to%20maximize%20frame-generation%20throughput.%20Please%20see%20demo%20videos%0Ahere%20-%20https%3A//aaxwaz.github.io/TalkingMachines/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03099v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTalkingMachines%253A%2520Real-Time%2520Audio-Driven%2520FaceTime-Style%2520Video%2520via%250A%2520%2520Autoregressive%2520Diffusion%2520Models%26entry.906535625%3DChetwin%2520Low%2520and%2520Weimin%2520Wang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520TalkingMachines%2520--%2520an%2520efficient%2520framework%2520that%250Atransforms%2520pretrained%2520video%2520generation%2520models%2520into%2520real-time%252C%2520audio-driven%250Acharacter%2520animators.%2520TalkingMachines%2520enables%2520natural%2520conversational%2520experiences%250Aby%2520integrating%2520an%2520audio%2520large%2520language%2520model%2520%2528LLM%2529%2520with%2520our%2520video%2520generation%250Afoundation%2520model.%2520Our%2520primary%2520contributions%2520include%253A%2520%25281%2529%2520We%2520adapt%2520a%2520pretrained%250ASOTA%2520image-to-video%2520DiT%2520into%2520an%2520audio-driven%2520avatar%2520generation%2520model%2520of%252018%250Abillion%2520parameters%253B%2520%25282%2529%2520We%2520enable%2520infinite%2520video%2520streaming%2520without%2520error%250Aaccumulation%2520through%2520asymmetric%2520knowledge%2520distillation%2520from%2520a%2520bidirectional%250Ateacher%2520model%2520into%2520a%2520sparse%2520causal%252C%2520autoregressive%2520student%2520model%253B%2520%25283%2529%2520We%2520design%250Aa%2520high-throughput%252C%2520low-latency%2520inference%2520pipeline%2520incorporating%2520several%2520key%250Aengineering%2520optimizations%2520such%2520as%253A%2520%2528a%2529%2520disaggregation%2520of%2520the%2520DiT%2520and%2520VAE%250Adecoder%2520across%2520separate%2520devices%252C%2520%2528b%2529%2520efficient%2520overlap%2520of%2520inter-device%250Acommunication%2520and%2520computation%2520using%2520CUDA%2520streams%252C%2520%2528c%2529%2520elimination%2520of%2520redundant%250Arecomputations%2520to%2520maximize%2520frame-generation%2520throughput.%2520Please%2520see%2520demo%2520videos%250Ahere%2520-%2520https%253A//aaxwaz.github.io/TalkingMachines/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03099v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TalkingMachines%3A%20Real-Time%20Audio-Driven%20FaceTime-Style%20Video%20via%0A%20%20Autoregressive%20Diffusion%20Models&entry.906535625=Chetwin%20Low%20and%20Weimin%20Wang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20present%20TalkingMachines%20--%20an%20efficient%20framework%20that%0Atransforms%20pretrained%20video%20generation%20models%20into%20real-time%2C%20audio-driven%0Acharacter%20animators.%20TalkingMachines%20enables%20natural%20conversational%20experiences%0Aby%20integrating%20an%20audio%20large%20language%20model%20%28LLM%29%20with%20our%20video%20generation%0Afoundation%20model.%20Our%20primary%20contributions%20include%3A%20%281%29%20We%20adapt%20a%20pretrained%0ASOTA%20image-to-video%20DiT%20into%20an%20audio-driven%20avatar%20generation%20model%20of%2018%0Abillion%20parameters%3B%20%282%29%20We%20enable%20infinite%20video%20streaming%20without%20error%0Aaccumulation%20through%20asymmetric%20knowledge%20distillation%20from%20a%20bidirectional%0Ateacher%20model%20into%20a%20sparse%20causal%2C%20autoregressive%20student%20model%3B%20%283%29%20We%20design%0Aa%20high-throughput%2C%20low-latency%20inference%20pipeline%20incorporating%20several%20key%0Aengineering%20optimizations%20such%20as%3A%20%28a%29%20disaggregation%20of%20the%20DiT%20and%20VAE%0Adecoder%20across%20separate%20devices%2C%20%28b%29%20efficient%20overlap%20of%20inter-device%0Acommunication%20and%20computation%20using%20CUDA%20streams%2C%20%28c%29%20elimination%20of%20redundant%0Arecomputations%20to%20maximize%20frame-generation%20throughput.%20Please%20see%20demo%20videos%0Ahere%20-%20https%3A//aaxwaz.github.io/TalkingMachines/%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03099v1&entry.124074799=Read"},
{"title": "Zero-Shot Tree Detection and Segmentation from Aerial Forest Imagery", "author": "Michelle Chen and David Russell and Amritha Pallavoor and Derek Young and Jane Wu", "abstract": "  Large-scale delineation of individual trees from remote sensing imagery is\ncrucial to the advancement of ecological research, particularly as climate\nchange and other environmental factors rapidly transform forest landscapes\nacross the world. Current RGB tree segmentation methods rely on training\nspecialized machine learning models with labeled tree datasets. While these\nlearning-based approaches can outperform manual data collection when accurate,\nthe existing models still depend on training data that's hard to scale. In this\npaper, we investigate the efficacy of using a state-of-the-art image\nsegmentation model, Segment Anything Model 2 (SAM2), in a zero-shot manner for\nindividual tree detection and segmentation. We evaluate a pretrained SAM2 model\non two tasks in this domain: (1) zero-shot segmentation and (2) zero-shot\ntransfer by using predictions from an existing tree detection model as prompts.\nOur results suggest that SAM2 not only has impressive generalization\ncapabilities, but also can form a natural synergy with specialized methods\ntrained on in-domain labeled data. We find that applying large pretrained\nmodels to problems in remote sensing is a promising avenue for future progress.\nWe make our code available at:\nhttps://github.com/open-forest-observatory/tree-detection-framework.\n", "link": "http://arxiv.org/abs/2506.03114v1", "date": "2025-06-03", "relevancy": 2.5436, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5117}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5117}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5028}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-Shot%20Tree%20Detection%20and%20Segmentation%20from%20Aerial%20Forest%20Imagery&body=Title%3A%20Zero-Shot%20Tree%20Detection%20and%20Segmentation%20from%20Aerial%20Forest%20Imagery%0AAuthor%3A%20Michelle%20Chen%20and%20David%20Russell%20and%20Amritha%20Pallavoor%20and%20Derek%20Young%20and%20Jane%20Wu%0AAbstract%3A%20%20%20Large-scale%20delineation%20of%20individual%20trees%20from%20remote%20sensing%20imagery%20is%0Acrucial%20to%20the%20advancement%20of%20ecological%20research%2C%20particularly%20as%20climate%0Achange%20and%20other%20environmental%20factors%20rapidly%20transform%20forest%20landscapes%0Aacross%20the%20world.%20Current%20RGB%20tree%20segmentation%20methods%20rely%20on%20training%0Aspecialized%20machine%20learning%20models%20with%20labeled%20tree%20datasets.%20While%20these%0Alearning-based%20approaches%20can%20outperform%20manual%20data%20collection%20when%20accurate%2C%0Athe%20existing%20models%20still%20depend%20on%20training%20data%20that%27s%20hard%20to%20scale.%20In%20this%0Apaper%2C%20we%20investigate%20the%20efficacy%20of%20using%20a%20state-of-the-art%20image%0Asegmentation%20model%2C%20Segment%20Anything%20Model%202%20%28SAM2%29%2C%20in%20a%20zero-shot%20manner%20for%0Aindividual%20tree%20detection%20and%20segmentation.%20We%20evaluate%20a%20pretrained%20SAM2%20model%0Aon%20two%20tasks%20in%20this%20domain%3A%20%281%29%20zero-shot%20segmentation%20and%20%282%29%20zero-shot%0Atransfer%20by%20using%20predictions%20from%20an%20existing%20tree%20detection%20model%20as%20prompts.%0AOur%20results%20suggest%20that%20SAM2%20not%20only%20has%20impressive%20generalization%0Acapabilities%2C%20but%20also%20can%20form%20a%20natural%20synergy%20with%20specialized%20methods%0Atrained%20on%20in-domain%20labeled%20data.%20We%20find%20that%20applying%20large%20pretrained%0Amodels%20to%20problems%20in%20remote%20sensing%20is%20a%20promising%20avenue%20for%20future%20progress.%0AWe%20make%20our%20code%20available%20at%3A%0Ahttps%3A//github.com/open-forest-observatory/tree-detection-framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03114v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-Shot%2520Tree%2520Detection%2520and%2520Segmentation%2520from%2520Aerial%2520Forest%2520Imagery%26entry.906535625%3DMichelle%2520Chen%2520and%2520David%2520Russell%2520and%2520Amritha%2520Pallavoor%2520and%2520Derek%2520Young%2520and%2520Jane%2520Wu%26entry.1292438233%3D%2520%2520Large-scale%2520delineation%2520of%2520individual%2520trees%2520from%2520remote%2520sensing%2520imagery%2520is%250Acrucial%2520to%2520the%2520advancement%2520of%2520ecological%2520research%252C%2520particularly%2520as%2520climate%250Achange%2520and%2520other%2520environmental%2520factors%2520rapidly%2520transform%2520forest%2520landscapes%250Aacross%2520the%2520world.%2520Current%2520RGB%2520tree%2520segmentation%2520methods%2520rely%2520on%2520training%250Aspecialized%2520machine%2520learning%2520models%2520with%2520labeled%2520tree%2520datasets.%2520While%2520these%250Alearning-based%2520approaches%2520can%2520outperform%2520manual%2520data%2520collection%2520when%2520accurate%252C%250Athe%2520existing%2520models%2520still%2520depend%2520on%2520training%2520data%2520that%2527s%2520hard%2520to%2520scale.%2520In%2520this%250Apaper%252C%2520we%2520investigate%2520the%2520efficacy%2520of%2520using%2520a%2520state-of-the-art%2520image%250Asegmentation%2520model%252C%2520Segment%2520Anything%2520Model%25202%2520%2528SAM2%2529%252C%2520in%2520a%2520zero-shot%2520manner%2520for%250Aindividual%2520tree%2520detection%2520and%2520segmentation.%2520We%2520evaluate%2520a%2520pretrained%2520SAM2%2520model%250Aon%2520two%2520tasks%2520in%2520this%2520domain%253A%2520%25281%2529%2520zero-shot%2520segmentation%2520and%2520%25282%2529%2520zero-shot%250Atransfer%2520by%2520using%2520predictions%2520from%2520an%2520existing%2520tree%2520detection%2520model%2520as%2520prompts.%250AOur%2520results%2520suggest%2520that%2520SAM2%2520not%2520only%2520has%2520impressive%2520generalization%250Acapabilities%252C%2520but%2520also%2520can%2520form%2520a%2520natural%2520synergy%2520with%2520specialized%2520methods%250Atrained%2520on%2520in-domain%2520labeled%2520data.%2520We%2520find%2520that%2520applying%2520large%2520pretrained%250Amodels%2520to%2520problems%2520in%2520remote%2520sensing%2520is%2520a%2520promising%2520avenue%2520for%2520future%2520progress.%250AWe%2520make%2520our%2520code%2520available%2520at%253A%250Ahttps%253A//github.com/open-forest-observatory/tree-detection-framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03114v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-Shot%20Tree%20Detection%20and%20Segmentation%20from%20Aerial%20Forest%20Imagery&entry.906535625=Michelle%20Chen%20and%20David%20Russell%20and%20Amritha%20Pallavoor%20and%20Derek%20Young%20and%20Jane%20Wu&entry.1292438233=%20%20Large-scale%20delineation%20of%20individual%20trees%20from%20remote%20sensing%20imagery%20is%0Acrucial%20to%20the%20advancement%20of%20ecological%20research%2C%20particularly%20as%20climate%0Achange%20and%20other%20environmental%20factors%20rapidly%20transform%20forest%20landscapes%0Aacross%20the%20world.%20Current%20RGB%20tree%20segmentation%20methods%20rely%20on%20training%0Aspecialized%20machine%20learning%20models%20with%20labeled%20tree%20datasets.%20While%20these%0Alearning-based%20approaches%20can%20outperform%20manual%20data%20collection%20when%20accurate%2C%0Athe%20existing%20models%20still%20depend%20on%20training%20data%20that%27s%20hard%20to%20scale.%20In%20this%0Apaper%2C%20we%20investigate%20the%20efficacy%20of%20using%20a%20state-of-the-art%20image%0Asegmentation%20model%2C%20Segment%20Anything%20Model%202%20%28SAM2%29%2C%20in%20a%20zero-shot%20manner%20for%0Aindividual%20tree%20detection%20and%20segmentation.%20We%20evaluate%20a%20pretrained%20SAM2%20model%0Aon%20two%20tasks%20in%20this%20domain%3A%20%281%29%20zero-shot%20segmentation%20and%20%282%29%20zero-shot%0Atransfer%20by%20using%20predictions%20from%20an%20existing%20tree%20detection%20model%20as%20prompts.%0AOur%20results%20suggest%20that%20SAM2%20not%20only%20has%20impressive%20generalization%0Acapabilities%2C%20but%20also%20can%20form%20a%20natural%20synergy%20with%20specialized%20methods%0Atrained%20on%20in-domain%20labeled%20data.%20We%20find%20that%20applying%20large%20pretrained%0Amodels%20to%20problems%20in%20remote%20sensing%20is%20a%20promising%20avenue%20for%20future%20progress.%0AWe%20make%20our%20code%20available%20at%3A%0Ahttps%3A//github.com/open-forest-observatory/tree-detection-framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03114v1&entry.124074799=Read"},
{"title": "Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate\n  Video Diffusion Transformers", "author": "Pengtao Chen and Xianfang Zeng and Maosen Zhao and Peng Ye and Mingzhu Shen and Wei Cheng and Gang Yu and Tao Chen", "abstract": "  While Diffusion Transformers (DiTs) have achieved breakthroughs in video\ngeneration, this long sequence generation task remains constrained by the\nquadratic complexity of attention mechanisms, resulting in significant\ninference latency. Through detailed analysis of attention maps in Video\nDiffusion Transformer (vDiT), we identify three recurring sparsity patterns:\ndiagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\\%\nattention heads can be skipped. Crucially, these patterns exhibit strong\nlayer-depth and head-position correlations but show limited dependence on the\ninput content. Leveraging these findings, we propose Sparse-vDiT, a sparsity\nacceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels\nthat replace dense attention with computationally efficient implementations for\neach identified sparsity pattern. 2) An offline sparse diffusion search\nalgorithm that selects the optimal sparse computation strategy per layer and\nhead via hardware-aware cost modeling. After determining the optimal\nconfiguration, we fuse heads within the same layer that share the same\nattention strategy, enhancing inference efficiency. Integrated into\nstate-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1),\nSparse-vDiT achieves 2.09$\\times$, 2.38$\\times$, and 1.67$\\times$ theoretical\nFLOP reduction, and actual inference speedups of 1.76$\\times$, 1.85$\\times$,\nand 1.58$\\times$, respectively, while maintaining high visual fidelity, with\nPSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent\nstructural sparsity in vDiTs can be systematically exploited for long video\nsynthesis.\n", "link": "http://arxiv.org/abs/2506.03065v1", "date": "2025-06-03", "relevancy": 2.5388, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6519}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.645}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sparse-vDiT%3A%20Unleashing%20the%20Power%20of%20Sparse%20Attention%20to%20Accelerate%0A%20%20Video%20Diffusion%20Transformers&body=Title%3A%20Sparse-vDiT%3A%20Unleashing%20the%20Power%20of%20Sparse%20Attention%20to%20Accelerate%0A%20%20Video%20Diffusion%20Transformers%0AAuthor%3A%20Pengtao%20Chen%20and%20Xianfang%20Zeng%20and%20Maosen%20Zhao%20and%20Peng%20Ye%20and%20Mingzhu%20Shen%20and%20Wei%20Cheng%20and%20Gang%20Yu%20and%20Tao%20Chen%0AAbstract%3A%20%20%20While%20Diffusion%20Transformers%20%28DiTs%29%20have%20achieved%20breakthroughs%20in%20video%0Ageneration%2C%20this%20long%20sequence%20generation%20task%20remains%20constrained%20by%20the%0Aquadratic%20complexity%20of%20attention%20mechanisms%2C%20resulting%20in%20significant%0Ainference%20latency.%20Through%20detailed%20analysis%20of%20attention%20maps%20in%20Video%0ADiffusion%20Transformer%20%28vDiT%29%2C%20we%20identify%20three%20recurring%20sparsity%20patterns%3A%0Adiagonal%2C%20multi-diagonal%2C%20and%20vertical-stripe%20structures.%20And%20even%203-6%5C%25%0Aattention%20heads%20can%20be%20skipped.%20Crucially%2C%20these%20patterns%20exhibit%20strong%0Alayer-depth%20and%20head-position%20correlations%20but%20show%20limited%20dependence%20on%20the%0Ainput%20content.%20Leveraging%20these%20findings%2C%20we%20propose%20Sparse-vDiT%2C%20a%20sparsity%0Aacceleration%20framework%20for%20vDiT%20comprising%3A%201%29%20Pattern-optimized%20sparse%20kernels%0Athat%20replace%20dense%20attention%20with%20computationally%20efficient%20implementations%20for%0Aeach%20identified%20sparsity%20pattern.%202%29%20An%20offline%20sparse%20diffusion%20search%0Aalgorithm%20that%20selects%20the%20optimal%20sparse%20computation%20strategy%20per%20layer%20and%0Ahead%20via%20hardware-aware%20cost%20modeling.%20After%20determining%20the%20optimal%0Aconfiguration%2C%20we%20fuse%20heads%20within%20the%20same%20layer%20that%20share%20the%20same%0Aattention%20strategy%2C%20enhancing%20inference%20efficiency.%20Integrated%20into%0Astate-of-the-art%20vDiT%20models%20%28CogVideoX1.5%2C%20HunyuanVideo%2C%20and%20Wan2.1%29%2C%0ASparse-vDiT%20achieves%202.09%24%5Ctimes%24%2C%202.38%24%5Ctimes%24%2C%20and%201.67%24%5Ctimes%24%20theoretical%0AFLOP%20reduction%2C%20and%20actual%20inference%20speedups%20of%201.76%24%5Ctimes%24%2C%201.85%24%5Ctimes%24%2C%0Aand%201.58%24%5Ctimes%24%2C%20respectively%2C%20while%20maintaining%20high%20visual%20fidelity%2C%20with%0APSNR%20values%20reaching%2024.13%2C%2027.09%2C%20and%2022.59.%20Our%20work%20demonstrates%20that%20latent%0Astructural%20sparsity%20in%20vDiTs%20can%20be%20systematically%20exploited%20for%20long%20video%0Asynthesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03065v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparse-vDiT%253A%2520Unleashing%2520the%2520Power%2520of%2520Sparse%2520Attention%2520to%2520Accelerate%250A%2520%2520Video%2520Diffusion%2520Transformers%26entry.906535625%3DPengtao%2520Chen%2520and%2520Xianfang%2520Zeng%2520and%2520Maosen%2520Zhao%2520and%2520Peng%2520Ye%2520and%2520Mingzhu%2520Shen%2520and%2520Wei%2520Cheng%2520and%2520Gang%2520Yu%2520and%2520Tao%2520Chen%26entry.1292438233%3D%2520%2520While%2520Diffusion%2520Transformers%2520%2528DiTs%2529%2520have%2520achieved%2520breakthroughs%2520in%2520video%250Ageneration%252C%2520this%2520long%2520sequence%2520generation%2520task%2520remains%2520constrained%2520by%2520the%250Aquadratic%2520complexity%2520of%2520attention%2520mechanisms%252C%2520resulting%2520in%2520significant%250Ainference%2520latency.%2520Through%2520detailed%2520analysis%2520of%2520attention%2520maps%2520in%2520Video%250ADiffusion%2520Transformer%2520%2528vDiT%2529%252C%2520we%2520identify%2520three%2520recurring%2520sparsity%2520patterns%253A%250Adiagonal%252C%2520multi-diagonal%252C%2520and%2520vertical-stripe%2520structures.%2520And%2520even%25203-6%255C%2525%250Aattention%2520heads%2520can%2520be%2520skipped.%2520Crucially%252C%2520these%2520patterns%2520exhibit%2520strong%250Alayer-depth%2520and%2520head-position%2520correlations%2520but%2520show%2520limited%2520dependence%2520on%2520the%250Ainput%2520content.%2520Leveraging%2520these%2520findings%252C%2520we%2520propose%2520Sparse-vDiT%252C%2520a%2520sparsity%250Aacceleration%2520framework%2520for%2520vDiT%2520comprising%253A%25201%2529%2520Pattern-optimized%2520sparse%2520kernels%250Athat%2520replace%2520dense%2520attention%2520with%2520computationally%2520efficient%2520implementations%2520for%250Aeach%2520identified%2520sparsity%2520pattern.%25202%2529%2520An%2520offline%2520sparse%2520diffusion%2520search%250Aalgorithm%2520that%2520selects%2520the%2520optimal%2520sparse%2520computation%2520strategy%2520per%2520layer%2520and%250Ahead%2520via%2520hardware-aware%2520cost%2520modeling.%2520After%2520determining%2520the%2520optimal%250Aconfiguration%252C%2520we%2520fuse%2520heads%2520within%2520the%2520same%2520layer%2520that%2520share%2520the%2520same%250Aattention%2520strategy%252C%2520enhancing%2520inference%2520efficiency.%2520Integrated%2520into%250Astate-of-the-art%2520vDiT%2520models%2520%2528CogVideoX1.5%252C%2520HunyuanVideo%252C%2520and%2520Wan2.1%2529%252C%250ASparse-vDiT%2520achieves%25202.09%2524%255Ctimes%2524%252C%25202.38%2524%255Ctimes%2524%252C%2520and%25201.67%2524%255Ctimes%2524%2520theoretical%250AFLOP%2520reduction%252C%2520and%2520actual%2520inference%2520speedups%2520of%25201.76%2524%255Ctimes%2524%252C%25201.85%2524%255Ctimes%2524%252C%250Aand%25201.58%2524%255Ctimes%2524%252C%2520respectively%252C%2520while%2520maintaining%2520high%2520visual%2520fidelity%252C%2520with%250APSNR%2520values%2520reaching%252024.13%252C%252027.09%252C%2520and%252022.59.%2520Our%2520work%2520demonstrates%2520that%2520latent%250Astructural%2520sparsity%2520in%2520vDiTs%2520can%2520be%2520systematically%2520exploited%2520for%2520long%2520video%250Asynthesis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03065v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse-vDiT%3A%20Unleashing%20the%20Power%20of%20Sparse%20Attention%20to%20Accelerate%0A%20%20Video%20Diffusion%20Transformers&entry.906535625=Pengtao%20Chen%20and%20Xianfang%20Zeng%20and%20Maosen%20Zhao%20and%20Peng%20Ye%20and%20Mingzhu%20Shen%20and%20Wei%20Cheng%20and%20Gang%20Yu%20and%20Tao%20Chen&entry.1292438233=%20%20While%20Diffusion%20Transformers%20%28DiTs%29%20have%20achieved%20breakthroughs%20in%20video%0Ageneration%2C%20this%20long%20sequence%20generation%20task%20remains%20constrained%20by%20the%0Aquadratic%20complexity%20of%20attention%20mechanisms%2C%20resulting%20in%20significant%0Ainference%20latency.%20Through%20detailed%20analysis%20of%20attention%20maps%20in%20Video%0ADiffusion%20Transformer%20%28vDiT%29%2C%20we%20identify%20three%20recurring%20sparsity%20patterns%3A%0Adiagonal%2C%20multi-diagonal%2C%20and%20vertical-stripe%20structures.%20And%20even%203-6%5C%25%0Aattention%20heads%20can%20be%20skipped.%20Crucially%2C%20these%20patterns%20exhibit%20strong%0Alayer-depth%20and%20head-position%20correlations%20but%20show%20limited%20dependence%20on%20the%0Ainput%20content.%20Leveraging%20these%20findings%2C%20we%20propose%20Sparse-vDiT%2C%20a%20sparsity%0Aacceleration%20framework%20for%20vDiT%20comprising%3A%201%29%20Pattern-optimized%20sparse%20kernels%0Athat%20replace%20dense%20attention%20with%20computationally%20efficient%20implementations%20for%0Aeach%20identified%20sparsity%20pattern.%202%29%20An%20offline%20sparse%20diffusion%20search%0Aalgorithm%20that%20selects%20the%20optimal%20sparse%20computation%20strategy%20per%20layer%20and%0Ahead%20via%20hardware-aware%20cost%20modeling.%20After%20determining%20the%20optimal%0Aconfiguration%2C%20we%20fuse%20heads%20within%20the%20same%20layer%20that%20share%20the%20same%0Aattention%20strategy%2C%20enhancing%20inference%20efficiency.%20Integrated%20into%0Astate-of-the-art%20vDiT%20models%20%28CogVideoX1.5%2C%20HunyuanVideo%2C%20and%20Wan2.1%29%2C%0ASparse-vDiT%20achieves%202.09%24%5Ctimes%24%2C%202.38%24%5Ctimes%24%2C%20and%201.67%24%5Ctimes%24%20theoretical%0AFLOP%20reduction%2C%20and%20actual%20inference%20speedups%20of%201.76%24%5Ctimes%24%2C%201.85%24%5Ctimes%24%2C%0Aand%201.58%24%5Ctimes%24%2C%20respectively%2C%20while%20maintaining%20high%20visual%20fidelity%2C%20with%0APSNR%20values%20reaching%2024.13%2C%2027.09%2C%20and%2022.59.%20Our%20work%20demonstrates%20that%20latent%0Astructural%20sparsity%20in%20vDiTs%20can%20be%20systematically%20exploited%20for%20long%20video%0Asynthesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03065v1&entry.124074799=Read"},
{"title": "Why do AI agents communicate in human language?", "author": "Pengcheng Zhou and Yinglun Feng and Halimulati Julaiti and Zhongliang Yang", "abstract": "  Large Language Models (LLMs) have become foundational to modern AI agent\nsystems, enabling autonomous agents to reason and plan. In most existing\nsystems, inter-agent communication relies primarily on natural language. While\nthis design supports interpretability and human oversight, we argue that it\nintroduces fundamental limitations in agent-to-agent coordination. The semantic\nspace of natural language is structurally misaligned with the high-dimensional\nvector spaces in which LLMs operate, resulting in information loss and\nbehavioral drift. Beyond surface-level inefficiencies, we highlight a deeper\narchitectural limitation: current LLMs were not trained with the objective of\nsupporting agentic behavior. As such, they lack mechanisms for modeling role\ncontinuity, task boundaries, and multi-agent dependencies. The standard\nnext-token prediction paradigm fails to support the structural alignment\nrequired for robust, scalable agent coordination. Based on this, we argue that\ntwo core questions deserve careful examination: first, given that AI agents\nfundamentally operate in high-dimensional vector spaces, should they rely on a\nlanguage system originally designed for human cognition as their communication\nmedium? Second, should we consider developing a new model construction paradigm\nthat builds models from the ground up to natively support structured\ncommunication, shared intentionality, and task alignment in multi-role,\nmulti-agent environments? This paper calls for a reconsideration not only of\nhow agents should communicate, but also of what it fundamentally means to train\na model that natively supports multi-agent coordination and communication.\n", "link": "http://arxiv.org/abs/2506.02739v1", "date": "2025-06-03", "relevancy": 2.5243, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5112}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5112}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Why%20do%20AI%20agents%20communicate%20in%20human%20language%3F&body=Title%3A%20Why%20do%20AI%20agents%20communicate%20in%20human%20language%3F%0AAuthor%3A%20Pengcheng%20Zhou%20and%20Yinglun%20Feng%20and%20Halimulati%20Julaiti%20and%20Zhongliang%20Yang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20foundational%20to%20modern%20AI%20agent%0Asystems%2C%20enabling%20autonomous%20agents%20to%20reason%20and%20plan.%20In%20most%20existing%0Asystems%2C%20inter-agent%20communication%20relies%20primarily%20on%20natural%20language.%20While%0Athis%20design%20supports%20interpretability%20and%20human%20oversight%2C%20we%20argue%20that%20it%0Aintroduces%20fundamental%20limitations%20in%20agent-to-agent%20coordination.%20The%20semantic%0Aspace%20of%20natural%20language%20is%20structurally%20misaligned%20with%20the%20high-dimensional%0Avector%20spaces%20in%20which%20LLMs%20operate%2C%20resulting%20in%20information%20loss%20and%0Abehavioral%20drift.%20Beyond%20surface-level%20inefficiencies%2C%20we%20highlight%20a%20deeper%0Aarchitectural%20limitation%3A%20current%20LLMs%20were%20not%20trained%20with%20the%20objective%20of%0Asupporting%20agentic%20behavior.%20As%20such%2C%20they%20lack%20mechanisms%20for%20modeling%20role%0Acontinuity%2C%20task%20boundaries%2C%20and%20multi-agent%20dependencies.%20The%20standard%0Anext-token%20prediction%20paradigm%20fails%20to%20support%20the%20structural%20alignment%0Arequired%20for%20robust%2C%20scalable%20agent%20coordination.%20Based%20on%20this%2C%20we%20argue%20that%0Atwo%20core%20questions%20deserve%20careful%20examination%3A%20first%2C%20given%20that%20AI%20agents%0Afundamentally%20operate%20in%20high-dimensional%20vector%20spaces%2C%20should%20they%20rely%20on%20a%0Alanguage%20system%20originally%20designed%20for%20human%20cognition%20as%20their%20communication%0Amedium%3F%20Second%2C%20should%20we%20consider%20developing%20a%20new%20model%20construction%20paradigm%0Athat%20builds%20models%20from%20the%20ground%20up%20to%20natively%20support%20structured%0Acommunication%2C%20shared%20intentionality%2C%20and%20task%20alignment%20in%20multi-role%2C%0Amulti-agent%20environments%3F%20This%20paper%20calls%20for%20a%20reconsideration%20not%20only%20of%0Ahow%20agents%20should%20communicate%2C%20but%20also%20of%20what%20it%20fundamentally%20means%20to%20train%0Aa%20model%20that%20natively%20supports%20multi-agent%20coordination%20and%20communication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02739v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhy%2520do%2520AI%2520agents%2520communicate%2520in%2520human%2520language%253F%26entry.906535625%3DPengcheng%2520Zhou%2520and%2520Yinglun%2520Feng%2520and%2520Halimulati%2520Julaiti%2520and%2520Zhongliang%2520Yang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520become%2520foundational%2520to%2520modern%2520AI%2520agent%250Asystems%252C%2520enabling%2520autonomous%2520agents%2520to%2520reason%2520and%2520plan.%2520In%2520most%2520existing%250Asystems%252C%2520inter-agent%2520communication%2520relies%2520primarily%2520on%2520natural%2520language.%2520While%250Athis%2520design%2520supports%2520interpretability%2520and%2520human%2520oversight%252C%2520we%2520argue%2520that%2520it%250Aintroduces%2520fundamental%2520limitations%2520in%2520agent-to-agent%2520coordination.%2520The%2520semantic%250Aspace%2520of%2520natural%2520language%2520is%2520structurally%2520misaligned%2520with%2520the%2520high-dimensional%250Avector%2520spaces%2520in%2520which%2520LLMs%2520operate%252C%2520resulting%2520in%2520information%2520loss%2520and%250Abehavioral%2520drift.%2520Beyond%2520surface-level%2520inefficiencies%252C%2520we%2520highlight%2520a%2520deeper%250Aarchitectural%2520limitation%253A%2520current%2520LLMs%2520were%2520not%2520trained%2520with%2520the%2520objective%2520of%250Asupporting%2520agentic%2520behavior.%2520As%2520such%252C%2520they%2520lack%2520mechanisms%2520for%2520modeling%2520role%250Acontinuity%252C%2520task%2520boundaries%252C%2520and%2520multi-agent%2520dependencies.%2520The%2520standard%250Anext-token%2520prediction%2520paradigm%2520fails%2520to%2520support%2520the%2520structural%2520alignment%250Arequired%2520for%2520robust%252C%2520scalable%2520agent%2520coordination.%2520Based%2520on%2520this%252C%2520we%2520argue%2520that%250Atwo%2520core%2520questions%2520deserve%2520careful%2520examination%253A%2520first%252C%2520given%2520that%2520AI%2520agents%250Afundamentally%2520operate%2520in%2520high-dimensional%2520vector%2520spaces%252C%2520should%2520they%2520rely%2520on%2520a%250Alanguage%2520system%2520originally%2520designed%2520for%2520human%2520cognition%2520as%2520their%2520communication%250Amedium%253F%2520Second%252C%2520should%2520we%2520consider%2520developing%2520a%2520new%2520model%2520construction%2520paradigm%250Athat%2520builds%2520models%2520from%2520the%2520ground%2520up%2520to%2520natively%2520support%2520structured%250Acommunication%252C%2520shared%2520intentionality%252C%2520and%2520task%2520alignment%2520in%2520multi-role%252C%250Amulti-agent%2520environments%253F%2520This%2520paper%2520calls%2520for%2520a%2520reconsideration%2520not%2520only%2520of%250Ahow%2520agents%2520should%2520communicate%252C%2520but%2520also%2520of%2520what%2520it%2520fundamentally%2520means%2520to%2520train%250Aa%2520model%2520that%2520natively%2520supports%2520multi-agent%2520coordination%2520and%2520communication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02739v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Why%20do%20AI%20agents%20communicate%20in%20human%20language%3F&entry.906535625=Pengcheng%20Zhou%20and%20Yinglun%20Feng%20and%20Halimulati%20Julaiti%20and%20Zhongliang%20Yang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20become%20foundational%20to%20modern%20AI%20agent%0Asystems%2C%20enabling%20autonomous%20agents%20to%20reason%20and%20plan.%20In%20most%20existing%0Asystems%2C%20inter-agent%20communication%20relies%20primarily%20on%20natural%20language.%20While%0Athis%20design%20supports%20interpretability%20and%20human%20oversight%2C%20we%20argue%20that%20it%0Aintroduces%20fundamental%20limitations%20in%20agent-to-agent%20coordination.%20The%20semantic%0Aspace%20of%20natural%20language%20is%20structurally%20misaligned%20with%20the%20high-dimensional%0Avector%20spaces%20in%20which%20LLMs%20operate%2C%20resulting%20in%20information%20loss%20and%0Abehavioral%20drift.%20Beyond%20surface-level%20inefficiencies%2C%20we%20highlight%20a%20deeper%0Aarchitectural%20limitation%3A%20current%20LLMs%20were%20not%20trained%20with%20the%20objective%20of%0Asupporting%20agentic%20behavior.%20As%20such%2C%20they%20lack%20mechanisms%20for%20modeling%20role%0Acontinuity%2C%20task%20boundaries%2C%20and%20multi-agent%20dependencies.%20The%20standard%0Anext-token%20prediction%20paradigm%20fails%20to%20support%20the%20structural%20alignment%0Arequired%20for%20robust%2C%20scalable%20agent%20coordination.%20Based%20on%20this%2C%20we%20argue%20that%0Atwo%20core%20questions%20deserve%20careful%20examination%3A%20first%2C%20given%20that%20AI%20agents%0Afundamentally%20operate%20in%20high-dimensional%20vector%20spaces%2C%20should%20they%20rely%20on%20a%0Alanguage%20system%20originally%20designed%20for%20human%20cognition%20as%20their%20communication%0Amedium%3F%20Second%2C%20should%20we%20consider%20developing%20a%20new%20model%20construction%20paradigm%0Athat%20builds%20models%20from%20the%20ground%20up%20to%20natively%20support%20structured%0Acommunication%2C%20shared%20intentionality%2C%20and%20task%20alignment%20in%20multi-role%2C%0Amulti-agent%20environments%3F%20This%20paper%20calls%20for%20a%20reconsideration%20not%20only%20of%0Ahow%20agents%20should%20communicate%2C%20but%20also%20of%20what%20it%20fundamentally%20means%20to%20train%0Aa%20model%20that%20natively%20supports%20multi-agent%20coordination%20and%20communication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02739v1&entry.124074799=Read"},
{"title": "Adaptive Guidance for Local Training in Heterogeneous Federated Learning", "author": "Jianqing Zhang and Yang Liu and Yang Hua and Jian Cao and Qiang Yang", "abstract": "  Model heterogeneity poses a significant challenge in Heterogeneous Federated\nLearning (HtFL). In scenarios with diverse model architectures, directly\naggregating model parameters is impractical, leading HtFL methods to\nincorporate an extra objective alongside the original local objective on each\nclient to facilitate collaboration. However, this often results in a mismatch\nbetween the extra and local objectives. To resolve this, we propose Federated\nLearning-to-Guide (FedL2G), a method that adaptively learns to guide local\ntraining in a federated manner, ensuring the added objective aligns with each\nclient's original goal. With theoretical guarantees, FedL2G utilizes only\nfirst-order derivatives w.r.t. model parameters, achieving a non-convex\nconvergence rate of O(1/T). We conduct extensive experiments across two data\nheterogeneity and six model heterogeneity settings, using 14 heterogeneous\nmodel architectures (e.g., CNNs and ViTs). The results show that FedL2G\nsignificantly outperforms seven state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2410.06490v3", "date": "2025-06-03", "relevancy": 2.5125, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5198}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4954}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4923}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Guidance%20for%20Local%20Training%20in%20Heterogeneous%20Federated%20Learning&body=Title%3A%20Adaptive%20Guidance%20for%20Local%20Training%20in%20Heterogeneous%20Federated%20Learning%0AAuthor%3A%20Jianqing%20Zhang%20and%20Yang%20Liu%20and%20Yang%20Hua%20and%20Jian%20Cao%20and%20Qiang%20Yang%0AAbstract%3A%20%20%20Model%20heterogeneity%20poses%20a%20significant%20challenge%20in%20Heterogeneous%20Federated%0ALearning%20%28HtFL%29.%20In%20scenarios%20with%20diverse%20model%20architectures%2C%20directly%0Aaggregating%20model%20parameters%20is%20impractical%2C%20leading%20HtFL%20methods%20to%0Aincorporate%20an%20extra%20objective%20alongside%20the%20original%20local%20objective%20on%20each%0Aclient%20to%20facilitate%20collaboration.%20However%2C%20this%20often%20results%20in%20a%20mismatch%0Abetween%20the%20extra%20and%20local%20objectives.%20To%20resolve%20this%2C%20we%20propose%20Federated%0ALearning-to-Guide%20%28FedL2G%29%2C%20a%20method%20that%20adaptively%20learns%20to%20guide%20local%0Atraining%20in%20a%20federated%20manner%2C%20ensuring%20the%20added%20objective%20aligns%20with%20each%0Aclient%27s%20original%20goal.%20With%20theoretical%20guarantees%2C%20FedL2G%20utilizes%20only%0Afirst-order%20derivatives%20w.r.t.%20model%20parameters%2C%20achieving%20a%20non-convex%0Aconvergence%20rate%20of%20O%281/T%29.%20We%20conduct%20extensive%20experiments%20across%20two%20data%0Aheterogeneity%20and%20six%20model%20heterogeneity%20settings%2C%20using%2014%20heterogeneous%0Amodel%20architectures%20%28e.g.%2C%20CNNs%20and%20ViTs%29.%20The%20results%20show%20that%20FedL2G%0Asignificantly%20outperforms%20seven%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.06490v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Guidance%2520for%2520Local%2520Training%2520in%2520Heterogeneous%2520Federated%2520Learning%26entry.906535625%3DJianqing%2520Zhang%2520and%2520Yang%2520Liu%2520and%2520Yang%2520Hua%2520and%2520Jian%2520Cao%2520and%2520Qiang%2520Yang%26entry.1292438233%3D%2520%2520Model%2520heterogeneity%2520poses%2520a%2520significant%2520challenge%2520in%2520Heterogeneous%2520Federated%250ALearning%2520%2528HtFL%2529.%2520In%2520scenarios%2520with%2520diverse%2520model%2520architectures%252C%2520directly%250Aaggregating%2520model%2520parameters%2520is%2520impractical%252C%2520leading%2520HtFL%2520methods%2520to%250Aincorporate%2520an%2520extra%2520objective%2520alongside%2520the%2520original%2520local%2520objective%2520on%2520each%250Aclient%2520to%2520facilitate%2520collaboration.%2520However%252C%2520this%2520often%2520results%2520in%2520a%2520mismatch%250Abetween%2520the%2520extra%2520and%2520local%2520objectives.%2520To%2520resolve%2520this%252C%2520we%2520propose%2520Federated%250ALearning-to-Guide%2520%2528FedL2G%2529%252C%2520a%2520method%2520that%2520adaptively%2520learns%2520to%2520guide%2520local%250Atraining%2520in%2520a%2520federated%2520manner%252C%2520ensuring%2520the%2520added%2520objective%2520aligns%2520with%2520each%250Aclient%2527s%2520original%2520goal.%2520With%2520theoretical%2520guarantees%252C%2520FedL2G%2520utilizes%2520only%250Afirst-order%2520derivatives%2520w.r.t.%2520model%2520parameters%252C%2520achieving%2520a%2520non-convex%250Aconvergence%2520rate%2520of%2520O%25281/T%2529.%2520We%2520conduct%2520extensive%2520experiments%2520across%2520two%2520data%250Aheterogeneity%2520and%2520six%2520model%2520heterogeneity%2520settings%252C%2520using%252014%2520heterogeneous%250Amodel%2520architectures%2520%2528e.g.%252C%2520CNNs%2520and%2520ViTs%2529.%2520The%2520results%2520show%2520that%2520FedL2G%250Asignificantly%2520outperforms%2520seven%2520state-of-the-art%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.06490v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Guidance%20for%20Local%20Training%20in%20Heterogeneous%20Federated%20Learning&entry.906535625=Jianqing%20Zhang%20and%20Yang%20Liu%20and%20Yang%20Hua%20and%20Jian%20Cao%20and%20Qiang%20Yang&entry.1292438233=%20%20Model%20heterogeneity%20poses%20a%20significant%20challenge%20in%20Heterogeneous%20Federated%0ALearning%20%28HtFL%29.%20In%20scenarios%20with%20diverse%20model%20architectures%2C%20directly%0Aaggregating%20model%20parameters%20is%20impractical%2C%20leading%20HtFL%20methods%20to%0Aincorporate%20an%20extra%20objective%20alongside%20the%20original%20local%20objective%20on%20each%0Aclient%20to%20facilitate%20collaboration.%20However%2C%20this%20often%20results%20in%20a%20mismatch%0Abetween%20the%20extra%20and%20local%20objectives.%20To%20resolve%20this%2C%20we%20propose%20Federated%0ALearning-to-Guide%20%28FedL2G%29%2C%20a%20method%20that%20adaptively%20learns%20to%20guide%20local%0Atraining%20in%20a%20federated%20manner%2C%20ensuring%20the%20added%20objective%20aligns%20with%20each%0Aclient%27s%20original%20goal.%20With%20theoretical%20guarantees%2C%20FedL2G%20utilizes%20only%0Afirst-order%20derivatives%20w.r.t.%20model%20parameters%2C%20achieving%20a%20non-convex%0Aconvergence%20rate%20of%20O%281/T%29.%20We%20conduct%20extensive%20experiments%20across%20two%20data%0Aheterogeneity%20and%20six%20model%20heterogeneity%20settings%2C%20using%2014%20heterogeneous%0Amodel%20architectures%20%28e.g.%2C%20CNNs%20and%20ViTs%29.%20The%20results%20show%20that%20FedL2G%0Asignificantly%20outperforms%20seven%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.06490v3&entry.124074799=Read"},
{"title": "ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal\n  Transformations", "author": "Ekaterina Grishina and Mikhail Gorbunov and Maxim Rakhuba", "abstract": "  Large language models (LLMs) demonstrate impressive results in natural\nlanguage processing tasks but require a significant amount of computational and\nmemory resources. Structured matrix representations are a promising way for\nreducing the number of parameters of these models. However, it seems\nunrealistic to expect that weight matrices of pretrained models can be\naccurately represented by structured matrices without any fine-tuning. To\novercome this issue, we utilize the fact that LLM output is invariant under\ncertain orthogonal transformations of weight matrices. This insight can be\nleveraged to identify transformations that significantly improve the\ncompressibility of weights within structured classes. The proposed approach is\napplicable to various types of structured matrices that support efficient\nprojection operations. Code is available at\nhttps://github.com/GrishKate/ProcrustesGPT\n", "link": "http://arxiv.org/abs/2506.02818v1", "date": "2025-06-03", "relevancy": 2.5057, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5255}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4925}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4855}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProcrustesGPT%3A%20Compressing%20LLMs%20with%20Structured%20Matrices%20and%20Orthogonal%0A%20%20Transformations&body=Title%3A%20ProcrustesGPT%3A%20Compressing%20LLMs%20with%20Structured%20Matrices%20and%20Orthogonal%0A%20%20Transformations%0AAuthor%3A%20Ekaterina%20Grishina%20and%20Mikhail%20Gorbunov%20and%20Maxim%20Rakhuba%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20demonstrate%20impressive%20results%20in%20natural%0Alanguage%20processing%20tasks%20but%20require%20a%20significant%20amount%20of%20computational%20and%0Amemory%20resources.%20Structured%20matrix%20representations%20are%20a%20promising%20way%20for%0Areducing%20the%20number%20of%20parameters%20of%20these%20models.%20However%2C%20it%20seems%0Aunrealistic%20to%20expect%20that%20weight%20matrices%20of%20pretrained%20models%20can%20be%0Aaccurately%20represented%20by%20structured%20matrices%20without%20any%20fine-tuning.%20To%0Aovercome%20this%20issue%2C%20we%20utilize%20the%20fact%20that%20LLM%20output%20is%20invariant%20under%0Acertain%20orthogonal%20transformations%20of%20weight%20matrices.%20This%20insight%20can%20be%0Aleveraged%20to%20identify%20transformations%20that%20significantly%20improve%20the%0Acompressibility%20of%20weights%20within%20structured%20classes.%20The%20proposed%20approach%20is%0Aapplicable%20to%20various%20types%20of%20structured%20matrices%20that%20support%20efficient%0Aprojection%20operations.%20Code%20is%20available%20at%0Ahttps%3A//github.com/GrishKate/ProcrustesGPT%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02818v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProcrustesGPT%253A%2520Compressing%2520LLMs%2520with%2520Structured%2520Matrices%2520and%2520Orthogonal%250A%2520%2520Transformations%26entry.906535625%3DEkaterina%2520Grishina%2520and%2520Mikhail%2520Gorbunov%2520and%2520Maxim%2520Rakhuba%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520demonstrate%2520impressive%2520results%2520in%2520natural%250Alanguage%2520processing%2520tasks%2520but%2520require%2520a%2520significant%2520amount%2520of%2520computational%2520and%250Amemory%2520resources.%2520Structured%2520matrix%2520representations%2520are%2520a%2520promising%2520way%2520for%250Areducing%2520the%2520number%2520of%2520parameters%2520of%2520these%2520models.%2520However%252C%2520it%2520seems%250Aunrealistic%2520to%2520expect%2520that%2520weight%2520matrices%2520of%2520pretrained%2520models%2520can%2520be%250Aaccurately%2520represented%2520by%2520structured%2520matrices%2520without%2520any%2520fine-tuning.%2520To%250Aovercome%2520this%2520issue%252C%2520we%2520utilize%2520the%2520fact%2520that%2520LLM%2520output%2520is%2520invariant%2520under%250Acertain%2520orthogonal%2520transformations%2520of%2520weight%2520matrices.%2520This%2520insight%2520can%2520be%250Aleveraged%2520to%2520identify%2520transformations%2520that%2520significantly%2520improve%2520the%250Acompressibility%2520of%2520weights%2520within%2520structured%2520classes.%2520The%2520proposed%2520approach%2520is%250Aapplicable%2520to%2520various%2520types%2520of%2520structured%2520matrices%2520that%2520support%2520efficient%250Aprojection%2520operations.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/GrishKate/ProcrustesGPT%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02818v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProcrustesGPT%3A%20Compressing%20LLMs%20with%20Structured%20Matrices%20and%20Orthogonal%0A%20%20Transformations&entry.906535625=Ekaterina%20Grishina%20and%20Mikhail%20Gorbunov%20and%20Maxim%20Rakhuba&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20demonstrate%20impressive%20results%20in%20natural%0Alanguage%20processing%20tasks%20but%20require%20a%20significant%20amount%20of%20computational%20and%0Amemory%20resources.%20Structured%20matrix%20representations%20are%20a%20promising%20way%20for%0Areducing%20the%20number%20of%20parameters%20of%20these%20models.%20However%2C%20it%20seems%0Aunrealistic%20to%20expect%20that%20weight%20matrices%20of%20pretrained%20models%20can%20be%0Aaccurately%20represented%20by%20structured%20matrices%20without%20any%20fine-tuning.%20To%0Aovercome%20this%20issue%2C%20we%20utilize%20the%20fact%20that%20LLM%20output%20is%20invariant%20under%0Acertain%20orthogonal%20transformations%20of%20weight%20matrices.%20This%20insight%20can%20be%0Aleveraged%20to%20identify%20transformations%20that%20significantly%20improve%20the%0Acompressibility%20of%20weights%20within%20structured%20classes.%20The%20proposed%20approach%20is%0Aapplicable%20to%20various%20types%20of%20structured%20matrices%20that%20support%20efficient%0Aprojection%20operations.%20Code%20is%20available%20at%0Ahttps%3A//github.com/GrishKate/ProcrustesGPT%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02818v1&entry.124074799=Read"},
{"title": "Benchmarking and Advancing Large Language Models for Local Life Services", "author": "Xiaochong Lan and Jie Feng and Jiahuan Lei and Xinlei Shi and Yong Li", "abstract": "  Large language models (LLMs) have exhibited remarkable capabilities and\nachieved significant breakthroughs across various domains, leading to their\nwidespread adoption in recent years. Building on this progress, we investigate\ntheir potential in the realm of local life services. In this study, we\nestablish a comprehensive benchmark and systematically evaluate the performance\nof diverse LLMs across a wide range of tasks relevant to local life services.\nTo further enhance their effectiveness, we explore two key approaches: model\nfine-tuning and agent-based workflows. Our findings reveal that even a\nrelatively compact 7B model can attain performance levels comparable to a much\nlarger 72B model, effectively balancing inference cost and model capability.\nThis optimization greatly enhances the feasibility and efficiency of deploying\nLLMs in real-world online services, making them more practical and accessible\nfor local life applications.\n", "link": "http://arxiv.org/abs/2506.02720v1", "date": "2025-06-03", "relevancy": 2.5051, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5167}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5167}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20and%20Advancing%20Large%20Language%20Models%20for%20Local%20Life%20Services&body=Title%3A%20Benchmarking%20and%20Advancing%20Large%20Language%20Models%20for%20Local%20Life%20Services%0AAuthor%3A%20Xiaochong%20Lan%20and%20Jie%20Feng%20and%20Jiahuan%20Lei%20and%20Xinlei%20Shi%20and%20Yong%20Li%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20exhibited%20remarkable%20capabilities%20and%0Aachieved%20significant%20breakthroughs%20across%20various%20domains%2C%20leading%20to%20their%0Awidespread%20adoption%20in%20recent%20years.%20Building%20on%20this%20progress%2C%20we%20investigate%0Atheir%20potential%20in%20the%20realm%20of%20local%20life%20services.%20In%20this%20study%2C%20we%0Aestablish%20a%20comprehensive%20benchmark%20and%20systematically%20evaluate%20the%20performance%0Aof%20diverse%20LLMs%20across%20a%20wide%20range%20of%20tasks%20relevant%20to%20local%20life%20services.%0ATo%20further%20enhance%20their%20effectiveness%2C%20we%20explore%20two%20key%20approaches%3A%20model%0Afine-tuning%20and%20agent-based%20workflows.%20Our%20findings%20reveal%20that%20even%20a%0Arelatively%20compact%207B%20model%20can%20attain%20performance%20levels%20comparable%20to%20a%20much%0Alarger%2072B%20model%2C%20effectively%20balancing%20inference%20cost%20and%20model%20capability.%0AThis%20optimization%20greatly%20enhances%20the%20feasibility%20and%20efficiency%20of%20deploying%0ALLMs%20in%20real-world%20online%20services%2C%20making%20them%20more%20practical%20and%20accessible%0Afor%20local%20life%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02720v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520and%2520Advancing%2520Large%2520Language%2520Models%2520for%2520Local%2520Life%2520Services%26entry.906535625%3DXiaochong%2520Lan%2520and%2520Jie%2520Feng%2520and%2520Jiahuan%2520Lei%2520and%2520Xinlei%2520Shi%2520and%2520Yong%2520Li%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520exhibited%2520remarkable%2520capabilities%2520and%250Aachieved%2520significant%2520breakthroughs%2520across%2520various%2520domains%252C%2520leading%2520to%2520their%250Awidespread%2520adoption%2520in%2520recent%2520years.%2520Building%2520on%2520this%2520progress%252C%2520we%2520investigate%250Atheir%2520potential%2520in%2520the%2520realm%2520of%2520local%2520life%2520services.%2520In%2520this%2520study%252C%2520we%250Aestablish%2520a%2520comprehensive%2520benchmark%2520and%2520systematically%2520evaluate%2520the%2520performance%250Aof%2520diverse%2520LLMs%2520across%2520a%2520wide%2520range%2520of%2520tasks%2520relevant%2520to%2520local%2520life%2520services.%250ATo%2520further%2520enhance%2520their%2520effectiveness%252C%2520we%2520explore%2520two%2520key%2520approaches%253A%2520model%250Afine-tuning%2520and%2520agent-based%2520workflows.%2520Our%2520findings%2520reveal%2520that%2520even%2520a%250Arelatively%2520compact%25207B%2520model%2520can%2520attain%2520performance%2520levels%2520comparable%2520to%2520a%2520much%250Alarger%252072B%2520model%252C%2520effectively%2520balancing%2520inference%2520cost%2520and%2520model%2520capability.%250AThis%2520optimization%2520greatly%2520enhances%2520the%2520feasibility%2520and%2520efficiency%2520of%2520deploying%250ALLMs%2520in%2520real-world%2520online%2520services%252C%2520making%2520them%2520more%2520practical%2520and%2520accessible%250Afor%2520local%2520life%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02720v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20and%20Advancing%20Large%20Language%20Models%20for%20Local%20Life%20Services&entry.906535625=Xiaochong%20Lan%20and%20Jie%20Feng%20and%20Jiahuan%20Lei%20and%20Xinlei%20Shi%20and%20Yong%20Li&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20exhibited%20remarkable%20capabilities%20and%0Aachieved%20significant%20breakthroughs%20across%20various%20domains%2C%20leading%20to%20their%0Awidespread%20adoption%20in%20recent%20years.%20Building%20on%20this%20progress%2C%20we%20investigate%0Atheir%20potential%20in%20the%20realm%20of%20local%20life%20services.%20In%20this%20study%2C%20we%0Aestablish%20a%20comprehensive%20benchmark%20and%20systematically%20evaluate%20the%20performance%0Aof%20diverse%20LLMs%20across%20a%20wide%20range%20of%20tasks%20relevant%20to%20local%20life%20services.%0ATo%20further%20enhance%20their%20effectiveness%2C%20we%20explore%20two%20key%20approaches%3A%20model%0Afine-tuning%20and%20agent-based%20workflows.%20Our%20findings%20reveal%20that%20even%20a%0Arelatively%20compact%207B%20model%20can%20attain%20performance%20levels%20comparable%20to%20a%20much%0Alarger%2072B%20model%2C%20effectively%20balancing%20inference%20cost%20and%20model%20capability.%0AThis%20optimization%20greatly%20enhances%20the%20feasibility%20and%20efficiency%20of%20deploying%0ALLMs%20in%20real-world%20online%20services%2C%20making%20them%20more%20practical%20and%20accessible%0Afor%20local%20life%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02720v1&entry.124074799=Read"},
{"title": "Controllable Human-centric Keyframe Interpolation with Generative Prior", "author": "Zujin Guo and Size Wu and Zhongang Cai and Wei Li and Chen Change Loy", "abstract": "  Existing interpolation methods use pre-trained video diffusion priors to\ngenerate intermediate frames between sparsely sampled keyframes. In the absence\nof 3D geometric guidance, these methods struggle to produce plausible results\nfor complex, articulated human motions and offer limited control over the\nsynthesized dynamics. In this paper, we introduce PoseFuse3D Keyframe\nInterpolator (PoseFuse3D-KI), a novel framework that integrates 3D human\nguidance signals into the diffusion process for Controllable Human-centric\nKeyframe Interpolation (CHKI). To provide rich spatial and structural cues for\ninterpolation, our PoseFuse3D, a 3D-informed control model, features a novel\nSMPL-X encoder that transforms 3D geometry and shape into the 2D latent\nconditioning space, alongside a fusion network that integrates these 3D cues\nwith 2D pose embeddings. For evaluation, we build CHKI-Video, a new dataset\nannotated with both 2D poses and 3D SMPL-X parameters. We show that\nPoseFuse3D-KI consistently outperforms state-of-the-art baselines on\nCHKI-Video, achieving a 9% improvement in PSNR and a 38% reduction in LPIPS.\nComprehensive ablations demonstrate that our PoseFuse3D model improves\ninterpolation fidelity.\n", "link": "http://arxiv.org/abs/2506.03119v1", "date": "2025-06-03", "relevancy": 2.5043, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6781}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5973}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Controllable%20Human-centric%20Keyframe%20Interpolation%20with%20Generative%20Prior&body=Title%3A%20Controllable%20Human-centric%20Keyframe%20Interpolation%20with%20Generative%20Prior%0AAuthor%3A%20Zujin%20Guo%20and%20Size%20Wu%20and%20Zhongang%20Cai%20and%20Wei%20Li%20and%20Chen%20Change%20Loy%0AAbstract%3A%20%20%20Existing%20interpolation%20methods%20use%20pre-trained%20video%20diffusion%20priors%20to%0Agenerate%20intermediate%20frames%20between%20sparsely%20sampled%20keyframes.%20In%20the%20absence%0Aof%203D%20geometric%20guidance%2C%20these%20methods%20struggle%20to%20produce%20plausible%20results%0Afor%20complex%2C%20articulated%20human%20motions%20and%20offer%20limited%20control%20over%20the%0Asynthesized%20dynamics.%20In%20this%20paper%2C%20we%20introduce%20PoseFuse3D%20Keyframe%0AInterpolator%20%28PoseFuse3D-KI%29%2C%20a%20novel%20framework%20that%20integrates%203D%20human%0Aguidance%20signals%20into%20the%20diffusion%20process%20for%20Controllable%20Human-centric%0AKeyframe%20Interpolation%20%28CHKI%29.%20To%20provide%20rich%20spatial%20and%20structural%20cues%20for%0Ainterpolation%2C%20our%20PoseFuse3D%2C%20a%203D-informed%20control%20model%2C%20features%20a%20novel%0ASMPL-X%20encoder%20that%20transforms%203D%20geometry%20and%20shape%20into%20the%202D%20latent%0Aconditioning%20space%2C%20alongside%20a%20fusion%20network%20that%20integrates%20these%203D%20cues%0Awith%202D%20pose%20embeddings.%20For%20evaluation%2C%20we%20build%20CHKI-Video%2C%20a%20new%20dataset%0Aannotated%20with%20both%202D%20poses%20and%203D%20SMPL-X%20parameters.%20We%20show%20that%0APoseFuse3D-KI%20consistently%20outperforms%20state-of-the-art%20baselines%20on%0ACHKI-Video%2C%20achieving%20a%209%25%20improvement%20in%20PSNR%20and%20a%2038%25%20reduction%20in%20LPIPS.%0AComprehensive%20ablations%20demonstrate%20that%20our%20PoseFuse3D%20model%20improves%0Ainterpolation%20fidelity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03119v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DControllable%2520Human-centric%2520Keyframe%2520Interpolation%2520with%2520Generative%2520Prior%26entry.906535625%3DZujin%2520Guo%2520and%2520Size%2520Wu%2520and%2520Zhongang%2520Cai%2520and%2520Wei%2520Li%2520and%2520Chen%2520Change%2520Loy%26entry.1292438233%3D%2520%2520Existing%2520interpolation%2520methods%2520use%2520pre-trained%2520video%2520diffusion%2520priors%2520to%250Agenerate%2520intermediate%2520frames%2520between%2520sparsely%2520sampled%2520keyframes.%2520In%2520the%2520absence%250Aof%25203D%2520geometric%2520guidance%252C%2520these%2520methods%2520struggle%2520to%2520produce%2520plausible%2520results%250Afor%2520complex%252C%2520articulated%2520human%2520motions%2520and%2520offer%2520limited%2520control%2520over%2520the%250Asynthesized%2520dynamics.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520PoseFuse3D%2520Keyframe%250AInterpolator%2520%2528PoseFuse3D-KI%2529%252C%2520a%2520novel%2520framework%2520that%2520integrates%25203D%2520human%250Aguidance%2520signals%2520into%2520the%2520diffusion%2520process%2520for%2520Controllable%2520Human-centric%250AKeyframe%2520Interpolation%2520%2528CHKI%2529.%2520To%2520provide%2520rich%2520spatial%2520and%2520structural%2520cues%2520for%250Ainterpolation%252C%2520our%2520PoseFuse3D%252C%2520a%25203D-informed%2520control%2520model%252C%2520features%2520a%2520novel%250ASMPL-X%2520encoder%2520that%2520transforms%25203D%2520geometry%2520and%2520shape%2520into%2520the%25202D%2520latent%250Aconditioning%2520space%252C%2520alongside%2520a%2520fusion%2520network%2520that%2520integrates%2520these%25203D%2520cues%250Awith%25202D%2520pose%2520embeddings.%2520For%2520evaluation%252C%2520we%2520build%2520CHKI-Video%252C%2520a%2520new%2520dataset%250Aannotated%2520with%2520both%25202D%2520poses%2520and%25203D%2520SMPL-X%2520parameters.%2520We%2520show%2520that%250APoseFuse3D-KI%2520consistently%2520outperforms%2520state-of-the-art%2520baselines%2520on%250ACHKI-Video%252C%2520achieving%2520a%25209%2525%2520improvement%2520in%2520PSNR%2520and%2520a%252038%2525%2520reduction%2520in%2520LPIPS.%250AComprehensive%2520ablations%2520demonstrate%2520that%2520our%2520PoseFuse3D%2520model%2520improves%250Ainterpolation%2520fidelity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03119v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Controllable%20Human-centric%20Keyframe%20Interpolation%20with%20Generative%20Prior&entry.906535625=Zujin%20Guo%20and%20Size%20Wu%20and%20Zhongang%20Cai%20and%20Wei%20Li%20and%20Chen%20Change%20Loy&entry.1292438233=%20%20Existing%20interpolation%20methods%20use%20pre-trained%20video%20diffusion%20priors%20to%0Agenerate%20intermediate%20frames%20between%20sparsely%20sampled%20keyframes.%20In%20the%20absence%0Aof%203D%20geometric%20guidance%2C%20these%20methods%20struggle%20to%20produce%20plausible%20results%0Afor%20complex%2C%20articulated%20human%20motions%20and%20offer%20limited%20control%20over%20the%0Asynthesized%20dynamics.%20In%20this%20paper%2C%20we%20introduce%20PoseFuse3D%20Keyframe%0AInterpolator%20%28PoseFuse3D-KI%29%2C%20a%20novel%20framework%20that%20integrates%203D%20human%0Aguidance%20signals%20into%20the%20diffusion%20process%20for%20Controllable%20Human-centric%0AKeyframe%20Interpolation%20%28CHKI%29.%20To%20provide%20rich%20spatial%20and%20structural%20cues%20for%0Ainterpolation%2C%20our%20PoseFuse3D%2C%20a%203D-informed%20control%20model%2C%20features%20a%20novel%0ASMPL-X%20encoder%20that%20transforms%203D%20geometry%20and%20shape%20into%20the%202D%20latent%0Aconditioning%20space%2C%20alongside%20a%20fusion%20network%20that%20integrates%20these%203D%20cues%0Awith%202D%20pose%20embeddings.%20For%20evaluation%2C%20we%20build%20CHKI-Video%2C%20a%20new%20dataset%0Aannotated%20with%20both%202D%20poses%20and%203D%20SMPL-X%20parameters.%20We%20show%20that%0APoseFuse3D-KI%20consistently%20outperforms%20state-of-the-art%20baselines%20on%0ACHKI-Video%2C%20achieving%20a%209%25%20improvement%20in%20PSNR%20and%20a%2038%25%20reduction%20in%20LPIPS.%0AComprehensive%20ablations%20demonstrate%20that%20our%20PoseFuse3D%20model%20improves%0Ainterpolation%20fidelity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03119v1&entry.124074799=Read"},
{"title": "Improving Heart Rejection Detection in XPCI Images Using Synthetic Data\n  Augmentation", "author": "Jakov Samard\u017eija and Donik Vr\u0161nak and Sven Lon\u010dari\u0107", "abstract": "  Accurate identification of acute cellular rejection (ACR) in endomyocardial\nbiopsies is essential for effective management of heart transplant patients.\nHowever, the rarity of high-grade rejection cases (3R) presents a significant\nchallenge for training robust deep learning models. This work addresses the\nclass imbalance problem by leveraging synthetic data generation using StyleGAN\nto augment the limited number of real 3R images. Prior to GAN training,\nhistogram equalization was applied to standardize image appearance and improve\nthe consistency of tissue representation. StyleGAN was trained on available 3R\nbiopsy patches and subsequently used to generate 10,000 realistic synthetic\nimages. These were combined with real 0R samples, that is samples without\nrejection, in various configurations to train ResNet-18 classifiers for binary\nrejection classification.\n  Three classifier variants were evaluated: one trained on real 0R and\nsynthetic 3R images, another using both synthetic and additional real samples,\nand a third trained solely on real data. All models were tested on an\nindependent set of real biopsy images. Results demonstrate that synthetic data\nimproves classification performance, particularly when used in combination with\nreal samples. The highest-performing model, which used both real and synthetic\nimages, achieved strong precision and recall for both classes. These findings\nunderscore the value of hybrid training strategies and highlight the potential\nof GAN-based data augmentation in biomedical image analysis, especially in\ndomains constrained by limited annotated datasets.\n", "link": "http://arxiv.org/abs/2505.19746v2", "date": "2025-06-03", "relevancy": 2.4892, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5051}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4989}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4895}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Heart%20Rejection%20Detection%20in%20XPCI%20Images%20Using%20Synthetic%20Data%0A%20%20Augmentation&body=Title%3A%20Improving%20Heart%20Rejection%20Detection%20in%20XPCI%20Images%20Using%20Synthetic%20Data%0A%20%20Augmentation%0AAuthor%3A%20Jakov%20Samard%C5%BEija%20and%20Donik%20Vr%C5%A1nak%20and%20Sven%20Lon%C4%8Dari%C4%87%0AAbstract%3A%20%20%20Accurate%20identification%20of%20acute%20cellular%20rejection%20%28ACR%29%20in%20endomyocardial%0Abiopsies%20is%20essential%20for%20effective%20management%20of%20heart%20transplant%20patients.%0AHowever%2C%20the%20rarity%20of%20high-grade%20rejection%20cases%20%283R%29%20presents%20a%20significant%0Achallenge%20for%20training%20robust%20deep%20learning%20models.%20This%20work%20addresses%20the%0Aclass%20imbalance%20problem%20by%20leveraging%20synthetic%20data%20generation%20using%20StyleGAN%0Ato%20augment%20the%20limited%20number%20of%20real%203R%20images.%20Prior%20to%20GAN%20training%2C%0Ahistogram%20equalization%20was%20applied%20to%20standardize%20image%20appearance%20and%20improve%0Athe%20consistency%20of%20tissue%20representation.%20StyleGAN%20was%20trained%20on%20available%203R%0Abiopsy%20patches%20and%20subsequently%20used%20to%20generate%2010%2C000%20realistic%20synthetic%0Aimages.%20These%20were%20combined%20with%20real%200R%20samples%2C%20that%20is%20samples%20without%0Arejection%2C%20in%20various%20configurations%20to%20train%20ResNet-18%20classifiers%20for%20binary%0Arejection%20classification.%0A%20%20Three%20classifier%20variants%20were%20evaluated%3A%20one%20trained%20on%20real%200R%20and%0Asynthetic%203R%20images%2C%20another%20using%20both%20synthetic%20and%20additional%20real%20samples%2C%0Aand%20a%20third%20trained%20solely%20on%20real%20data.%20All%20models%20were%20tested%20on%20an%0Aindependent%20set%20of%20real%20biopsy%20images.%20Results%20demonstrate%20that%20synthetic%20data%0Aimproves%20classification%20performance%2C%20particularly%20when%20used%20in%20combination%20with%0Areal%20samples.%20The%20highest-performing%20model%2C%20which%20used%20both%20real%20and%20synthetic%0Aimages%2C%20achieved%20strong%20precision%20and%20recall%20for%20both%20classes.%20These%20findings%0Aunderscore%20the%20value%20of%20hybrid%20training%20strategies%20and%20highlight%20the%20potential%0Aof%20GAN-based%20data%20augmentation%20in%20biomedical%20image%20analysis%2C%20especially%20in%0Adomains%20constrained%20by%20limited%20annotated%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.19746v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Heart%2520Rejection%2520Detection%2520in%2520XPCI%2520Images%2520Using%2520Synthetic%2520Data%250A%2520%2520Augmentation%26entry.906535625%3DJakov%2520Samard%25C5%25BEija%2520and%2520Donik%2520Vr%25C5%25A1nak%2520and%2520Sven%2520Lon%25C4%258Dari%25C4%2587%26entry.1292438233%3D%2520%2520Accurate%2520identification%2520of%2520acute%2520cellular%2520rejection%2520%2528ACR%2529%2520in%2520endomyocardial%250Abiopsies%2520is%2520essential%2520for%2520effective%2520management%2520of%2520heart%2520transplant%2520patients.%250AHowever%252C%2520the%2520rarity%2520of%2520high-grade%2520rejection%2520cases%2520%25283R%2529%2520presents%2520a%2520significant%250Achallenge%2520for%2520training%2520robust%2520deep%2520learning%2520models.%2520This%2520work%2520addresses%2520the%250Aclass%2520imbalance%2520problem%2520by%2520leveraging%2520synthetic%2520data%2520generation%2520using%2520StyleGAN%250Ato%2520augment%2520the%2520limited%2520number%2520of%2520real%25203R%2520images.%2520Prior%2520to%2520GAN%2520training%252C%250Ahistogram%2520equalization%2520was%2520applied%2520to%2520standardize%2520image%2520appearance%2520and%2520improve%250Athe%2520consistency%2520of%2520tissue%2520representation.%2520StyleGAN%2520was%2520trained%2520on%2520available%25203R%250Abiopsy%2520patches%2520and%2520subsequently%2520used%2520to%2520generate%252010%252C000%2520realistic%2520synthetic%250Aimages.%2520These%2520were%2520combined%2520with%2520real%25200R%2520samples%252C%2520that%2520is%2520samples%2520without%250Arejection%252C%2520in%2520various%2520configurations%2520to%2520train%2520ResNet-18%2520classifiers%2520for%2520binary%250Arejection%2520classification.%250A%2520%2520Three%2520classifier%2520variants%2520were%2520evaluated%253A%2520one%2520trained%2520on%2520real%25200R%2520and%250Asynthetic%25203R%2520images%252C%2520another%2520using%2520both%2520synthetic%2520and%2520additional%2520real%2520samples%252C%250Aand%2520a%2520third%2520trained%2520solely%2520on%2520real%2520data.%2520All%2520models%2520were%2520tested%2520on%2520an%250Aindependent%2520set%2520of%2520real%2520biopsy%2520images.%2520Results%2520demonstrate%2520that%2520synthetic%2520data%250Aimproves%2520classification%2520performance%252C%2520particularly%2520when%2520used%2520in%2520combination%2520with%250Areal%2520samples.%2520The%2520highest-performing%2520model%252C%2520which%2520used%2520both%2520real%2520and%2520synthetic%250Aimages%252C%2520achieved%2520strong%2520precision%2520and%2520recall%2520for%2520both%2520classes.%2520These%2520findings%250Aunderscore%2520the%2520value%2520of%2520hybrid%2520training%2520strategies%2520and%2520highlight%2520the%2520potential%250Aof%2520GAN-based%2520data%2520augmentation%2520in%2520biomedical%2520image%2520analysis%252C%2520especially%2520in%250Adomains%2520constrained%2520by%2520limited%2520annotated%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19746v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Heart%20Rejection%20Detection%20in%20XPCI%20Images%20Using%20Synthetic%20Data%0A%20%20Augmentation&entry.906535625=Jakov%20Samard%C5%BEija%20and%20Donik%20Vr%C5%A1nak%20and%20Sven%20Lon%C4%8Dari%C4%87&entry.1292438233=%20%20Accurate%20identification%20of%20acute%20cellular%20rejection%20%28ACR%29%20in%20endomyocardial%0Abiopsies%20is%20essential%20for%20effective%20management%20of%20heart%20transplant%20patients.%0AHowever%2C%20the%20rarity%20of%20high-grade%20rejection%20cases%20%283R%29%20presents%20a%20significant%0Achallenge%20for%20training%20robust%20deep%20learning%20models.%20This%20work%20addresses%20the%0Aclass%20imbalance%20problem%20by%20leveraging%20synthetic%20data%20generation%20using%20StyleGAN%0Ato%20augment%20the%20limited%20number%20of%20real%203R%20images.%20Prior%20to%20GAN%20training%2C%0Ahistogram%20equalization%20was%20applied%20to%20standardize%20image%20appearance%20and%20improve%0Athe%20consistency%20of%20tissue%20representation.%20StyleGAN%20was%20trained%20on%20available%203R%0Abiopsy%20patches%20and%20subsequently%20used%20to%20generate%2010%2C000%20realistic%20synthetic%0Aimages.%20These%20were%20combined%20with%20real%200R%20samples%2C%20that%20is%20samples%20without%0Arejection%2C%20in%20various%20configurations%20to%20train%20ResNet-18%20classifiers%20for%20binary%0Arejection%20classification.%0A%20%20Three%20classifier%20variants%20were%20evaluated%3A%20one%20trained%20on%20real%200R%20and%0Asynthetic%203R%20images%2C%20another%20using%20both%20synthetic%20and%20additional%20real%20samples%2C%0Aand%20a%20third%20trained%20solely%20on%20real%20data.%20All%20models%20were%20tested%20on%20an%0Aindependent%20set%20of%20real%20biopsy%20images.%20Results%20demonstrate%20that%20synthetic%20data%0Aimproves%20classification%20performance%2C%20particularly%20when%20used%20in%20combination%20with%0Areal%20samples.%20The%20highest-performing%20model%2C%20which%20used%20both%20real%20and%20synthetic%0Aimages%2C%20achieved%20strong%20precision%20and%20recall%20for%20both%20classes.%20These%20findings%0Aunderscore%20the%20value%20of%20hybrid%20training%20strategies%20and%20highlight%20the%20potential%0Aof%20GAN-based%20data%20augmentation%20in%20biomedical%20image%20analysis%2C%20especially%20in%0Adomains%20constrained%20by%20limited%20annotated%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.19746v2&entry.124074799=Read"},
{"title": "PoLAR: Polar-Decomposed Low-Rank Adapter Representation", "author": "Kai Lion and Liang Zhang and Bingcong Li and Niao He", "abstract": "  We show that low-rank adaptation of large-scale models suffers from a low\nstable rank that is well below the linear algebraic rank of the subspace,\ndegrading fine-tuning performance. To mitigate the underutilization of the\nallocated subspace, we propose PoLAR, a parameterization inspired by the polar\ndecomposition that factorizes the low-rank update into two direction matrices\nconstrained to Stiefel manifolds and an unconstrained scale matrix. Our theory\nshows that PoLAR yields an exponentially faster convergence rate on a canonical\nlow-rank adaptation problem. Pairing the parameterization with Riemannian\noptimization leads to consistent gains on three different benchmarks testing\ngeneral language understanding, commonsense reasoning, and mathematical problem\nsolving with base model sizes ranging from 350M to 27B.\n", "link": "http://arxiv.org/abs/2506.03133v1", "date": "2025-06-03", "relevancy": 2.4845, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.516}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4944}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PoLAR%3A%20Polar-Decomposed%20Low-Rank%20Adapter%20Representation&body=Title%3A%20PoLAR%3A%20Polar-Decomposed%20Low-Rank%20Adapter%20Representation%0AAuthor%3A%20Kai%20Lion%20and%20Liang%20Zhang%20and%20Bingcong%20Li%20and%20Niao%20He%0AAbstract%3A%20%20%20We%20show%20that%20low-rank%20adaptation%20of%20large-scale%20models%20suffers%20from%20a%20low%0Astable%20rank%20that%20is%20well%20below%20the%20linear%20algebraic%20rank%20of%20the%20subspace%2C%0Adegrading%20fine-tuning%20performance.%20To%20mitigate%20the%20underutilization%20of%20the%0Aallocated%20subspace%2C%20we%20propose%20PoLAR%2C%20a%20parameterization%20inspired%20by%20the%20polar%0Adecomposition%20that%20factorizes%20the%20low-rank%20update%20into%20two%20direction%20matrices%0Aconstrained%20to%20Stiefel%20manifolds%20and%20an%20unconstrained%20scale%20matrix.%20Our%20theory%0Ashows%20that%20PoLAR%20yields%20an%20exponentially%20faster%20convergence%20rate%20on%20a%20canonical%0Alow-rank%20adaptation%20problem.%20Pairing%20the%20parameterization%20with%20Riemannian%0Aoptimization%20leads%20to%20consistent%20gains%20on%20three%20different%20benchmarks%20testing%0Ageneral%20language%20understanding%2C%20commonsense%20reasoning%2C%20and%20mathematical%20problem%0Asolving%20with%20base%20model%20sizes%20ranging%20from%20350M%20to%2027B.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03133v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPoLAR%253A%2520Polar-Decomposed%2520Low-Rank%2520Adapter%2520Representation%26entry.906535625%3DKai%2520Lion%2520and%2520Liang%2520Zhang%2520and%2520Bingcong%2520Li%2520and%2520Niao%2520He%26entry.1292438233%3D%2520%2520We%2520show%2520that%2520low-rank%2520adaptation%2520of%2520large-scale%2520models%2520suffers%2520from%2520a%2520low%250Astable%2520rank%2520that%2520is%2520well%2520below%2520the%2520linear%2520algebraic%2520rank%2520of%2520the%2520subspace%252C%250Adegrading%2520fine-tuning%2520performance.%2520To%2520mitigate%2520the%2520underutilization%2520of%2520the%250Aallocated%2520subspace%252C%2520we%2520propose%2520PoLAR%252C%2520a%2520parameterization%2520inspired%2520by%2520the%2520polar%250Adecomposition%2520that%2520factorizes%2520the%2520low-rank%2520update%2520into%2520two%2520direction%2520matrices%250Aconstrained%2520to%2520Stiefel%2520manifolds%2520and%2520an%2520unconstrained%2520scale%2520matrix.%2520Our%2520theory%250Ashows%2520that%2520PoLAR%2520yields%2520an%2520exponentially%2520faster%2520convergence%2520rate%2520on%2520a%2520canonical%250Alow-rank%2520adaptation%2520problem.%2520Pairing%2520the%2520parameterization%2520with%2520Riemannian%250Aoptimization%2520leads%2520to%2520consistent%2520gains%2520on%2520three%2520different%2520benchmarks%2520testing%250Ageneral%2520language%2520understanding%252C%2520commonsense%2520reasoning%252C%2520and%2520mathematical%2520problem%250Asolving%2520with%2520base%2520model%2520sizes%2520ranging%2520from%2520350M%2520to%252027B.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03133v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PoLAR%3A%20Polar-Decomposed%20Low-Rank%20Adapter%20Representation&entry.906535625=Kai%20Lion%20and%20Liang%20Zhang%20and%20Bingcong%20Li%20and%20Niao%20He&entry.1292438233=%20%20We%20show%20that%20low-rank%20adaptation%20of%20large-scale%20models%20suffers%20from%20a%20low%0Astable%20rank%20that%20is%20well%20below%20the%20linear%20algebraic%20rank%20of%20the%20subspace%2C%0Adegrading%20fine-tuning%20performance.%20To%20mitigate%20the%20underutilization%20of%20the%0Aallocated%20subspace%2C%20we%20propose%20PoLAR%2C%20a%20parameterization%20inspired%20by%20the%20polar%0Adecomposition%20that%20factorizes%20the%20low-rank%20update%20into%20two%20direction%20matrices%0Aconstrained%20to%20Stiefel%20manifolds%20and%20an%20unconstrained%20scale%20matrix.%20Our%20theory%0Ashows%20that%20PoLAR%20yields%20an%20exponentially%20faster%20convergence%20rate%20on%20a%20canonical%0Alow-rank%20adaptation%20problem.%20Pairing%20the%20parameterization%20with%20Riemannian%0Aoptimization%20leads%20to%20consistent%20gains%20on%20three%20different%20benchmarks%20testing%0Ageneral%20language%20understanding%2C%20commonsense%20reasoning%2C%20and%20mathematical%20problem%0Asolving%20with%20base%20model%20sizes%20ranging%20from%20350M%20to%2027B.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03133v1&entry.124074799=Read"},
{"title": "SG2VID: Scene Graphs Enable Fine-Grained Control for Video Synthesis", "author": "Ssharvien Kumar Sivakumar and Yannik Frisch and Ghazal Ghazaei and Anirban Mukhopadhyay", "abstract": "  Surgical simulation plays a pivotal role in training novice surgeons,\naccelerating their learning curve and reducing intra-operative errors. However,\nconventional simulation tools fall short in providing the necessary\nphotorealism and the variability of human anatomy. In response, current methods\nare shifting towards generative model-based simulators. Yet, these approaches\nprimarily focus on using increasingly complex conditioning for precise\nsynthesis while neglecting the fine-grained human control aspect. To address\nthis gap, we introduce SG2VID, the first diffusion-based video model that\nleverages Scene Graphs for both precise video synthesis and fine-grained human\ncontrol. We demonstrate SG2VID's capabilities across three public datasets\nfeaturing cataract and cholecystectomy surgery. While SG2VID outperforms\nprevious methods both qualitatively and quantitatively, it also enables precise\nsynthesis, providing accurate control over tool and anatomy's size and\nmovement, entrance of new tools, as well as the overall scene layout. We\nqualitatively motivate how SG2VID can be used for generative augmentation and\npresent an experiment demonstrating its ability to improve a downstream phase\ndetection task when the training set is extended with our synthetic videos.\nFinally, to showcase SG2VID's ability to retain human control, we interact with\nthe Scene Graphs to generate new video samples depicting major yet rare\nintra-operative irregularities.\n", "link": "http://arxiv.org/abs/2506.03082v1", "date": "2025-06-03", "relevancy": 2.4778, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6287}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6135}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6112}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SG2VID%3A%20Scene%20Graphs%20Enable%20Fine-Grained%20Control%20for%20Video%20Synthesis&body=Title%3A%20SG2VID%3A%20Scene%20Graphs%20Enable%20Fine-Grained%20Control%20for%20Video%20Synthesis%0AAuthor%3A%20Ssharvien%20Kumar%20Sivakumar%20and%20Yannik%20Frisch%20and%20Ghazal%20Ghazaei%20and%20Anirban%20Mukhopadhyay%0AAbstract%3A%20%20%20Surgical%20simulation%20plays%20a%20pivotal%20role%20in%20training%20novice%20surgeons%2C%0Aaccelerating%20their%20learning%20curve%20and%20reducing%20intra-operative%20errors.%20However%2C%0Aconventional%20simulation%20tools%20fall%20short%20in%20providing%20the%20necessary%0Aphotorealism%20and%20the%20variability%20of%20human%20anatomy.%20In%20response%2C%20current%20methods%0Aare%20shifting%20towards%20generative%20model-based%20simulators.%20Yet%2C%20these%20approaches%0Aprimarily%20focus%20on%20using%20increasingly%20complex%20conditioning%20for%20precise%0Asynthesis%20while%20neglecting%20the%20fine-grained%20human%20control%20aspect.%20To%20address%0Athis%20gap%2C%20we%20introduce%20SG2VID%2C%20the%20first%20diffusion-based%20video%20model%20that%0Aleverages%20Scene%20Graphs%20for%20both%20precise%20video%20synthesis%20and%20fine-grained%20human%0Acontrol.%20We%20demonstrate%20SG2VID%27s%20capabilities%20across%20three%20public%20datasets%0Afeaturing%20cataract%20and%20cholecystectomy%20surgery.%20While%20SG2VID%20outperforms%0Aprevious%20methods%20both%20qualitatively%20and%20quantitatively%2C%20it%20also%20enables%20precise%0Asynthesis%2C%20providing%20accurate%20control%20over%20tool%20and%20anatomy%27s%20size%20and%0Amovement%2C%20entrance%20of%20new%20tools%2C%20as%20well%20as%20the%20overall%20scene%20layout.%20We%0Aqualitatively%20motivate%20how%20SG2VID%20can%20be%20used%20for%20generative%20augmentation%20and%0Apresent%20an%20experiment%20demonstrating%20its%20ability%20to%20improve%20a%20downstream%20phase%0Adetection%20task%20when%20the%20training%20set%20is%20extended%20with%20our%20synthetic%20videos.%0AFinally%2C%20to%20showcase%20SG2VID%27s%20ability%20to%20retain%20human%20control%2C%20we%20interact%20with%0Athe%20Scene%20Graphs%20to%20generate%20new%20video%20samples%20depicting%20major%20yet%20rare%0Aintra-operative%20irregularities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03082v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSG2VID%253A%2520Scene%2520Graphs%2520Enable%2520Fine-Grained%2520Control%2520for%2520Video%2520Synthesis%26entry.906535625%3DSsharvien%2520Kumar%2520Sivakumar%2520and%2520Yannik%2520Frisch%2520and%2520Ghazal%2520Ghazaei%2520and%2520Anirban%2520Mukhopadhyay%26entry.1292438233%3D%2520%2520Surgical%2520simulation%2520plays%2520a%2520pivotal%2520role%2520in%2520training%2520novice%2520surgeons%252C%250Aaccelerating%2520their%2520learning%2520curve%2520and%2520reducing%2520intra-operative%2520errors.%2520However%252C%250Aconventional%2520simulation%2520tools%2520fall%2520short%2520in%2520providing%2520the%2520necessary%250Aphotorealism%2520and%2520the%2520variability%2520of%2520human%2520anatomy.%2520In%2520response%252C%2520current%2520methods%250Aare%2520shifting%2520towards%2520generative%2520model-based%2520simulators.%2520Yet%252C%2520these%2520approaches%250Aprimarily%2520focus%2520on%2520using%2520increasingly%2520complex%2520conditioning%2520for%2520precise%250Asynthesis%2520while%2520neglecting%2520the%2520fine-grained%2520human%2520control%2520aspect.%2520To%2520address%250Athis%2520gap%252C%2520we%2520introduce%2520SG2VID%252C%2520the%2520first%2520diffusion-based%2520video%2520model%2520that%250Aleverages%2520Scene%2520Graphs%2520for%2520both%2520precise%2520video%2520synthesis%2520and%2520fine-grained%2520human%250Acontrol.%2520We%2520demonstrate%2520SG2VID%2527s%2520capabilities%2520across%2520three%2520public%2520datasets%250Afeaturing%2520cataract%2520and%2520cholecystectomy%2520surgery.%2520While%2520SG2VID%2520outperforms%250Aprevious%2520methods%2520both%2520qualitatively%2520and%2520quantitatively%252C%2520it%2520also%2520enables%2520precise%250Asynthesis%252C%2520providing%2520accurate%2520control%2520over%2520tool%2520and%2520anatomy%2527s%2520size%2520and%250Amovement%252C%2520entrance%2520of%2520new%2520tools%252C%2520as%2520well%2520as%2520the%2520overall%2520scene%2520layout.%2520We%250Aqualitatively%2520motivate%2520how%2520SG2VID%2520can%2520be%2520used%2520for%2520generative%2520augmentation%2520and%250Apresent%2520an%2520experiment%2520demonstrating%2520its%2520ability%2520to%2520improve%2520a%2520downstream%2520phase%250Adetection%2520task%2520when%2520the%2520training%2520set%2520is%2520extended%2520with%2520our%2520synthetic%2520videos.%250AFinally%252C%2520to%2520showcase%2520SG2VID%2527s%2520ability%2520to%2520retain%2520human%2520control%252C%2520we%2520interact%2520with%250Athe%2520Scene%2520Graphs%2520to%2520generate%2520new%2520video%2520samples%2520depicting%2520major%2520yet%2520rare%250Aintra-operative%2520irregularities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03082v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SG2VID%3A%20Scene%20Graphs%20Enable%20Fine-Grained%20Control%20for%20Video%20Synthesis&entry.906535625=Ssharvien%20Kumar%20Sivakumar%20and%20Yannik%20Frisch%20and%20Ghazal%20Ghazaei%20and%20Anirban%20Mukhopadhyay&entry.1292438233=%20%20Surgical%20simulation%20plays%20a%20pivotal%20role%20in%20training%20novice%20surgeons%2C%0Aaccelerating%20their%20learning%20curve%20and%20reducing%20intra-operative%20errors.%20However%2C%0Aconventional%20simulation%20tools%20fall%20short%20in%20providing%20the%20necessary%0Aphotorealism%20and%20the%20variability%20of%20human%20anatomy.%20In%20response%2C%20current%20methods%0Aare%20shifting%20towards%20generative%20model-based%20simulators.%20Yet%2C%20these%20approaches%0Aprimarily%20focus%20on%20using%20increasingly%20complex%20conditioning%20for%20precise%0Asynthesis%20while%20neglecting%20the%20fine-grained%20human%20control%20aspect.%20To%20address%0Athis%20gap%2C%20we%20introduce%20SG2VID%2C%20the%20first%20diffusion-based%20video%20model%20that%0Aleverages%20Scene%20Graphs%20for%20both%20precise%20video%20synthesis%20and%20fine-grained%20human%0Acontrol.%20We%20demonstrate%20SG2VID%27s%20capabilities%20across%20three%20public%20datasets%0Afeaturing%20cataract%20and%20cholecystectomy%20surgery.%20While%20SG2VID%20outperforms%0Aprevious%20methods%20both%20qualitatively%20and%20quantitatively%2C%20it%20also%20enables%20precise%0Asynthesis%2C%20providing%20accurate%20control%20over%20tool%20and%20anatomy%27s%20size%20and%0Amovement%2C%20entrance%20of%20new%20tools%2C%20as%20well%20as%20the%20overall%20scene%20layout.%20We%0Aqualitatively%20motivate%20how%20SG2VID%20can%20be%20used%20for%20generative%20augmentation%20and%0Apresent%20an%20experiment%20demonstrating%20its%20ability%20to%20improve%20a%20downstream%20phase%0Adetection%20task%20when%20the%20training%20set%20is%20extended%20with%20our%20synthetic%20videos.%0AFinally%2C%20to%20showcase%20SG2VID%27s%20ability%20to%20retain%20human%20control%2C%20we%20interact%20with%0Athe%20Scene%20Graphs%20to%20generate%20new%20video%20samples%20depicting%20major%20yet%20rare%0Aintra-operative%20irregularities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03082v1&entry.124074799=Read"},
{"title": "Exploiting the English Vocabulary Profile for L2 word-level vocabulary\n  assessment with LLMs", "author": "Stefano Bann\u00f2 and Kate Knill and Mark Gales", "abstract": "  Vocabulary use is a fundamental aspect of second language (L2) proficiency.\nTo date, its assessment by automated systems has typically examined the\ncontext-independent, or part-of-speech (PoS) related use of words. This paper\nintroduces a novel approach to enable fine-grained vocabulary evaluation\nexploiting the precise use of words within a sentence. The scheme combines\nlarge language models (LLMs) with the English Vocabulary Profile (EVP). The EVP\nis a standard lexical resource that enables in-context vocabulary use to be\nlinked with proficiency level. We evaluate the ability of LLMs to assign\nproficiency levels to individual words as they appear in L2 learner writing,\naddressing key challenges such as polysemy, contextual variation, and\nmulti-word expressions. We compare LLMs to a PoS-based baseline. LLMs appear to\nexploit additional semantic information that yields improved performance. We\nalso explore correlations between word-level proficiency and essay-level\nproficiency. Finally, the approach is applied to examine the consistency of the\nEVP proficiency levels. Results show that LLMs are well-suited for the task of\nvocabulary assessment.\n", "link": "http://arxiv.org/abs/2506.02758v1", "date": "2025-06-03", "relevancy": 2.4761, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5256}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5256}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20the%20English%20Vocabulary%20Profile%20for%20L2%20word-level%20vocabulary%0A%20%20assessment%20with%20LLMs&body=Title%3A%20Exploiting%20the%20English%20Vocabulary%20Profile%20for%20L2%20word-level%20vocabulary%0A%20%20assessment%20with%20LLMs%0AAuthor%3A%20Stefano%20Bann%C3%B2%20and%20Kate%20Knill%20and%20Mark%20Gales%0AAbstract%3A%20%20%20Vocabulary%20use%20is%20a%20fundamental%20aspect%20of%20second%20language%20%28L2%29%20proficiency.%0ATo%20date%2C%20its%20assessment%20by%20automated%20systems%20has%20typically%20examined%20the%0Acontext-independent%2C%20or%20part-of-speech%20%28PoS%29%20related%20use%20of%20words.%20This%20paper%0Aintroduces%20a%20novel%20approach%20to%20enable%20fine-grained%20vocabulary%20evaluation%0Aexploiting%20the%20precise%20use%20of%20words%20within%20a%20sentence.%20The%20scheme%20combines%0Alarge%20language%20models%20%28LLMs%29%20with%20the%20English%20Vocabulary%20Profile%20%28EVP%29.%20The%20EVP%0Ais%20a%20standard%20lexical%20resource%20that%20enables%20in-context%20vocabulary%20use%20to%20be%0Alinked%20with%20proficiency%20level.%20We%20evaluate%20the%20ability%20of%20LLMs%20to%20assign%0Aproficiency%20levels%20to%20individual%20words%20as%20they%20appear%20in%20L2%20learner%20writing%2C%0Aaddressing%20key%20challenges%20such%20as%20polysemy%2C%20contextual%20variation%2C%20and%0Amulti-word%20expressions.%20We%20compare%20LLMs%20to%20a%20PoS-based%20baseline.%20LLMs%20appear%20to%0Aexploit%20additional%20semantic%20information%20that%20yields%20improved%20performance.%20We%0Aalso%20explore%20correlations%20between%20word-level%20proficiency%20and%20essay-level%0Aproficiency.%20Finally%2C%20the%20approach%20is%20applied%20to%20examine%20the%20consistency%20of%20the%0AEVP%20proficiency%20levels.%20Results%20show%20that%20LLMs%20are%20well-suited%20for%20the%20task%20of%0Avocabulary%20assessment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02758v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520the%2520English%2520Vocabulary%2520Profile%2520for%2520L2%2520word-level%2520vocabulary%250A%2520%2520assessment%2520with%2520LLMs%26entry.906535625%3DStefano%2520Bann%25C3%25B2%2520and%2520Kate%2520Knill%2520and%2520Mark%2520Gales%26entry.1292438233%3D%2520%2520Vocabulary%2520use%2520is%2520a%2520fundamental%2520aspect%2520of%2520second%2520language%2520%2528L2%2529%2520proficiency.%250ATo%2520date%252C%2520its%2520assessment%2520by%2520automated%2520systems%2520has%2520typically%2520examined%2520the%250Acontext-independent%252C%2520or%2520part-of-speech%2520%2528PoS%2529%2520related%2520use%2520of%2520words.%2520This%2520paper%250Aintroduces%2520a%2520novel%2520approach%2520to%2520enable%2520fine-grained%2520vocabulary%2520evaluation%250Aexploiting%2520the%2520precise%2520use%2520of%2520words%2520within%2520a%2520sentence.%2520The%2520scheme%2520combines%250Alarge%2520language%2520models%2520%2528LLMs%2529%2520with%2520the%2520English%2520Vocabulary%2520Profile%2520%2528EVP%2529.%2520The%2520EVP%250Ais%2520a%2520standard%2520lexical%2520resource%2520that%2520enables%2520in-context%2520vocabulary%2520use%2520to%2520be%250Alinked%2520with%2520proficiency%2520level.%2520We%2520evaluate%2520the%2520ability%2520of%2520LLMs%2520to%2520assign%250Aproficiency%2520levels%2520to%2520individual%2520words%2520as%2520they%2520appear%2520in%2520L2%2520learner%2520writing%252C%250Aaddressing%2520key%2520challenges%2520such%2520as%2520polysemy%252C%2520contextual%2520variation%252C%2520and%250Amulti-word%2520expressions.%2520We%2520compare%2520LLMs%2520to%2520a%2520PoS-based%2520baseline.%2520LLMs%2520appear%2520to%250Aexploit%2520additional%2520semantic%2520information%2520that%2520yields%2520improved%2520performance.%2520We%250Aalso%2520explore%2520correlations%2520between%2520word-level%2520proficiency%2520and%2520essay-level%250Aproficiency.%2520Finally%252C%2520the%2520approach%2520is%2520applied%2520to%2520examine%2520the%2520consistency%2520of%2520the%250AEVP%2520proficiency%2520levels.%2520Results%2520show%2520that%2520LLMs%2520are%2520well-suited%2520for%2520the%2520task%2520of%250Avocabulary%2520assessment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02758v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20the%20English%20Vocabulary%20Profile%20for%20L2%20word-level%20vocabulary%0A%20%20assessment%20with%20LLMs&entry.906535625=Stefano%20Bann%C3%B2%20and%20Kate%20Knill%20and%20Mark%20Gales&entry.1292438233=%20%20Vocabulary%20use%20is%20a%20fundamental%20aspect%20of%20second%20language%20%28L2%29%20proficiency.%0ATo%20date%2C%20its%20assessment%20by%20automated%20systems%20has%20typically%20examined%20the%0Acontext-independent%2C%20or%20part-of-speech%20%28PoS%29%20related%20use%20of%20words.%20This%20paper%0Aintroduces%20a%20novel%20approach%20to%20enable%20fine-grained%20vocabulary%20evaluation%0Aexploiting%20the%20precise%20use%20of%20words%20within%20a%20sentence.%20The%20scheme%20combines%0Alarge%20language%20models%20%28LLMs%29%20with%20the%20English%20Vocabulary%20Profile%20%28EVP%29.%20The%20EVP%0Ais%20a%20standard%20lexical%20resource%20that%20enables%20in-context%20vocabulary%20use%20to%20be%0Alinked%20with%20proficiency%20level.%20We%20evaluate%20the%20ability%20of%20LLMs%20to%20assign%0Aproficiency%20levels%20to%20individual%20words%20as%20they%20appear%20in%20L2%20learner%20writing%2C%0Aaddressing%20key%20challenges%20such%20as%20polysemy%2C%20contextual%20variation%2C%20and%0Amulti-word%20expressions.%20We%20compare%20LLMs%20to%20a%20PoS-based%20baseline.%20LLMs%20appear%20to%0Aexploit%20additional%20semantic%20information%20that%20yields%20improved%20performance.%20We%0Aalso%20explore%20correlations%20between%20word-level%20proficiency%20and%20essay-level%0Aproficiency.%20Finally%2C%20the%20approach%20is%20applied%20to%20examine%20the%20consistency%20of%20the%0AEVP%20proficiency%20levels.%20Results%20show%20that%20LLMs%20are%20well-suited%20for%20the%20task%20of%0Avocabulary%20assessment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02758v1&entry.124074799=Read"},
{"title": "ORV: 4D Occupancy-centric Robot Video Generation", "author": "Xiuyu Yang and Bohan Li and Shaocong Xu and Nan Wang and Chongjie Ye and Zhaoxi Chen and Minghan Qin and Yikang Ding and Xin Jin and Hang Zhao and Hao Zhao", "abstract": "  Acquiring real-world robotic simulation data through teleoperation is\nnotoriously time-consuming and labor-intensive. Recently, action-driven\ngenerative models have gained widespread adoption in robot learning and\nsimulation, as they eliminate safety concerns and reduce maintenance efforts.\nHowever, the action sequences used in these methods often result in limited\ncontrol precision and poor generalization due to their globally coarse\nalignment. To address these limitations, we propose ORV, an Occupancy-centric\nRobot Video generation framework, which utilizes 4D semantic occupancy\nsequences as a fine-grained representation to provide more accurate semantic\nand geometric guidance for video generation. By leveraging occupancy-based\nrepresentations, ORV enables seamless translation of simulation data into\nphotorealistic robot videos, while ensuring high temporal consistency and\nprecise controllability. Furthermore, our framework supports the simultaneous\ngeneration of multi-view videos of robot gripping operations - an important\ncapability for downstream robotic learning tasks. Extensive experimental\nresults demonstrate that ORV consistently outperforms existing baseline methods\nacross various datasets and sub-tasks. Demo, Code and Model:\nhttps://orangesodahub.github.io/ORV\n", "link": "http://arxiv.org/abs/2506.03079v1", "date": "2025-06-03", "relevancy": 2.4739, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6272}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6191}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6144}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ORV%3A%204D%20Occupancy-centric%20Robot%20Video%20Generation&body=Title%3A%20ORV%3A%204D%20Occupancy-centric%20Robot%20Video%20Generation%0AAuthor%3A%20Xiuyu%20Yang%20and%20Bohan%20Li%20and%20Shaocong%20Xu%20and%20Nan%20Wang%20and%20Chongjie%20Ye%20and%20Zhaoxi%20Chen%20and%20Minghan%20Qin%20and%20Yikang%20Ding%20and%20Xin%20Jin%20and%20Hang%20Zhao%20and%20Hao%20Zhao%0AAbstract%3A%20%20%20Acquiring%20real-world%20robotic%20simulation%20data%20through%20teleoperation%20is%0Anotoriously%20time-consuming%20and%20labor-intensive.%20Recently%2C%20action-driven%0Agenerative%20models%20have%20gained%20widespread%20adoption%20in%20robot%20learning%20and%0Asimulation%2C%20as%20they%20eliminate%20safety%20concerns%20and%20reduce%20maintenance%20efforts.%0AHowever%2C%20the%20action%20sequences%20used%20in%20these%20methods%20often%20result%20in%20limited%0Acontrol%20precision%20and%20poor%20generalization%20due%20to%20their%20globally%20coarse%0Aalignment.%20To%20address%20these%20limitations%2C%20we%20propose%20ORV%2C%20an%20Occupancy-centric%0ARobot%20Video%20generation%20framework%2C%20which%20utilizes%204D%20semantic%20occupancy%0Asequences%20as%20a%20fine-grained%20representation%20to%20provide%20more%20accurate%20semantic%0Aand%20geometric%20guidance%20for%20video%20generation.%20By%20leveraging%20occupancy-based%0Arepresentations%2C%20ORV%20enables%20seamless%20translation%20of%20simulation%20data%20into%0Aphotorealistic%20robot%20videos%2C%20while%20ensuring%20high%20temporal%20consistency%20and%0Aprecise%20controllability.%20Furthermore%2C%20our%20framework%20supports%20the%20simultaneous%0Ageneration%20of%20multi-view%20videos%20of%20robot%20gripping%20operations%20-%20an%20important%0Acapability%20for%20downstream%20robotic%20learning%20tasks.%20Extensive%20experimental%0Aresults%20demonstrate%20that%20ORV%20consistently%20outperforms%20existing%20baseline%20methods%0Aacross%20various%20datasets%20and%20sub-tasks.%20Demo%2C%20Code%20and%20Model%3A%0Ahttps%3A//orangesodahub.github.io/ORV%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DORV%253A%25204D%2520Occupancy-centric%2520Robot%2520Video%2520Generation%26entry.906535625%3DXiuyu%2520Yang%2520and%2520Bohan%2520Li%2520and%2520Shaocong%2520Xu%2520and%2520Nan%2520Wang%2520and%2520Chongjie%2520Ye%2520and%2520Zhaoxi%2520Chen%2520and%2520Minghan%2520Qin%2520and%2520Yikang%2520Ding%2520and%2520Xin%2520Jin%2520and%2520Hang%2520Zhao%2520and%2520Hao%2520Zhao%26entry.1292438233%3D%2520%2520Acquiring%2520real-world%2520robotic%2520simulation%2520data%2520through%2520teleoperation%2520is%250Anotoriously%2520time-consuming%2520and%2520labor-intensive.%2520Recently%252C%2520action-driven%250Agenerative%2520models%2520have%2520gained%2520widespread%2520adoption%2520in%2520robot%2520learning%2520and%250Asimulation%252C%2520as%2520they%2520eliminate%2520safety%2520concerns%2520and%2520reduce%2520maintenance%2520efforts.%250AHowever%252C%2520the%2520action%2520sequences%2520used%2520in%2520these%2520methods%2520often%2520result%2520in%2520limited%250Acontrol%2520precision%2520and%2520poor%2520generalization%2520due%2520to%2520their%2520globally%2520coarse%250Aalignment.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520ORV%252C%2520an%2520Occupancy-centric%250ARobot%2520Video%2520generation%2520framework%252C%2520which%2520utilizes%25204D%2520semantic%2520occupancy%250Asequences%2520as%2520a%2520fine-grained%2520representation%2520to%2520provide%2520more%2520accurate%2520semantic%250Aand%2520geometric%2520guidance%2520for%2520video%2520generation.%2520By%2520leveraging%2520occupancy-based%250Arepresentations%252C%2520ORV%2520enables%2520seamless%2520translation%2520of%2520simulation%2520data%2520into%250Aphotorealistic%2520robot%2520videos%252C%2520while%2520ensuring%2520high%2520temporal%2520consistency%2520and%250Aprecise%2520controllability.%2520Furthermore%252C%2520our%2520framework%2520supports%2520the%2520simultaneous%250Ageneration%2520of%2520multi-view%2520videos%2520of%2520robot%2520gripping%2520operations%2520-%2520an%2520important%250Acapability%2520for%2520downstream%2520robotic%2520learning%2520tasks.%2520Extensive%2520experimental%250Aresults%2520demonstrate%2520that%2520ORV%2520consistently%2520outperforms%2520existing%2520baseline%2520methods%250Aacross%2520various%2520datasets%2520and%2520sub-tasks.%2520Demo%252C%2520Code%2520and%2520Model%253A%250Ahttps%253A//orangesodahub.github.io/ORV%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ORV%3A%204D%20Occupancy-centric%20Robot%20Video%20Generation&entry.906535625=Xiuyu%20Yang%20and%20Bohan%20Li%20and%20Shaocong%20Xu%20and%20Nan%20Wang%20and%20Chongjie%20Ye%20and%20Zhaoxi%20Chen%20and%20Minghan%20Qin%20and%20Yikang%20Ding%20and%20Xin%20Jin%20and%20Hang%20Zhao%20and%20Hao%20Zhao&entry.1292438233=%20%20Acquiring%20real-world%20robotic%20simulation%20data%20through%20teleoperation%20is%0Anotoriously%20time-consuming%20and%20labor-intensive.%20Recently%2C%20action-driven%0Agenerative%20models%20have%20gained%20widespread%20adoption%20in%20robot%20learning%20and%0Asimulation%2C%20as%20they%20eliminate%20safety%20concerns%20and%20reduce%20maintenance%20efforts.%0AHowever%2C%20the%20action%20sequences%20used%20in%20these%20methods%20often%20result%20in%20limited%0Acontrol%20precision%20and%20poor%20generalization%20due%20to%20their%20globally%20coarse%0Aalignment.%20To%20address%20these%20limitations%2C%20we%20propose%20ORV%2C%20an%20Occupancy-centric%0ARobot%20Video%20generation%20framework%2C%20which%20utilizes%204D%20semantic%20occupancy%0Asequences%20as%20a%20fine-grained%20representation%20to%20provide%20more%20accurate%20semantic%0Aand%20geometric%20guidance%20for%20video%20generation.%20By%20leveraging%20occupancy-based%0Arepresentations%2C%20ORV%20enables%20seamless%20translation%20of%20simulation%20data%20into%0Aphotorealistic%20robot%20videos%2C%20while%20ensuring%20high%20temporal%20consistency%20and%0Aprecise%20controllability.%20Furthermore%2C%20our%20framework%20supports%20the%20simultaneous%0Ageneration%20of%20multi-view%20videos%20of%20robot%20gripping%20operations%20-%20an%20important%0Acapability%20for%20downstream%20robotic%20learning%20tasks.%20Extensive%20experimental%0Aresults%20demonstrate%20that%20ORV%20consistently%20outperforms%20existing%20baseline%20methods%0Aacross%20various%20datasets%20and%20sub-tasks.%20Demo%2C%20Code%20and%20Model%3A%0Ahttps%3A//orangesodahub.github.io/ORV%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03079v1&entry.124074799=Read"},
{"title": "Hierarchical Self-Prompting SAM: A Prompt-Free Medical Image\n  Segmentation Framework", "author": "Mengmeng Zhang and Xingyuan Dai and Yicheng Sun and Jing Wang and Yueyang Yao and Xiaoyan Gong and Fuze Cong and Feiyue Wang and Yisheng Lv", "abstract": "  Although the Segment Anything Model (SAM) is highly effective in natural\nimage segmentation, it requires dependencies on prompts, which limits its\napplicability to medical imaging where manual prompts are often unavailable.\nExisting efforts to fine-tune SAM for medical segmentation typically struggle\nto remove this dependency. We propose Hierarchical Self-Prompting SAM\n(HSP-SAM), a novel self-prompting framework that enables SAM to achieve strong\nperformance in prompt-free medical image segmentation. Unlike previous\nself-prompting methods that remain limited to positional prompts similar to\nvanilla SAM, we are the first to introduce learning abstract prompts during the\nself-prompting process. This simple and intuitive self-prompting framework\nachieves superior performance on classic segmentation tasks such as polyp and\nskin lesion segmentation, while maintaining robustness across diverse medical\nimaging modalities. Furthermore, it exhibits strong generalization to unseen\ndatasets, achieving improvements of up to 14.04% over previous state-of-the-art\nmethods on some challenging benchmarks. These results suggest that abstract\nprompts encapsulate richer and higher-dimensional semantic information compared\nto positional prompts, thereby enhancing the model's robustness and\ngeneralization performance. All models and codes will be released upon\nacceptance.\n", "link": "http://arxiv.org/abs/2506.02854v1", "date": "2025-06-03", "relevancy": 2.4691, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4996}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4918}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Self-Prompting%20SAM%3A%20A%20Prompt-Free%20Medical%20Image%0A%20%20Segmentation%20Framework&body=Title%3A%20Hierarchical%20Self-Prompting%20SAM%3A%20A%20Prompt-Free%20Medical%20Image%0A%20%20Segmentation%20Framework%0AAuthor%3A%20Mengmeng%20Zhang%20and%20Xingyuan%20Dai%20and%20Yicheng%20Sun%20and%20Jing%20Wang%20and%20Yueyang%20Yao%20and%20Xiaoyan%20Gong%20and%20Fuze%20Cong%20and%20Feiyue%20Wang%20and%20Yisheng%20Lv%0AAbstract%3A%20%20%20Although%20the%20Segment%20Anything%20Model%20%28SAM%29%20is%20highly%20effective%20in%20natural%0Aimage%20segmentation%2C%20it%20requires%20dependencies%20on%20prompts%2C%20which%20limits%20its%0Aapplicability%20to%20medical%20imaging%20where%20manual%20prompts%20are%20often%20unavailable.%0AExisting%20efforts%20to%20fine-tune%20SAM%20for%20medical%20segmentation%20typically%20struggle%0Ato%20remove%20this%20dependency.%20We%20propose%20Hierarchical%20Self-Prompting%20SAM%0A%28HSP-SAM%29%2C%20a%20novel%20self-prompting%20framework%20that%20enables%20SAM%20to%20achieve%20strong%0Aperformance%20in%20prompt-free%20medical%20image%20segmentation.%20Unlike%20previous%0Aself-prompting%20methods%20that%20remain%20limited%20to%20positional%20prompts%20similar%20to%0Avanilla%20SAM%2C%20we%20are%20the%20first%20to%20introduce%20learning%20abstract%20prompts%20during%20the%0Aself-prompting%20process.%20This%20simple%20and%20intuitive%20self-prompting%20framework%0Aachieves%20superior%20performance%20on%20classic%20segmentation%20tasks%20such%20as%20polyp%20and%0Askin%20lesion%20segmentation%2C%20while%20maintaining%20robustness%20across%20diverse%20medical%0Aimaging%20modalities.%20Furthermore%2C%20it%20exhibits%20strong%20generalization%20to%20unseen%0Adatasets%2C%20achieving%20improvements%20of%20up%20to%2014.04%25%20over%20previous%20state-of-the-art%0Amethods%20on%20some%20challenging%20benchmarks.%20These%20results%20suggest%20that%20abstract%0Aprompts%20encapsulate%20richer%20and%20higher-dimensional%20semantic%20information%20compared%0Ato%20positional%20prompts%2C%20thereby%20enhancing%20the%20model%27s%20robustness%20and%0Ageneralization%20performance.%20All%20models%20and%20codes%20will%20be%20released%20upon%0Aacceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02854v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Self-Prompting%2520SAM%253A%2520A%2520Prompt-Free%2520Medical%2520Image%250A%2520%2520Segmentation%2520Framework%26entry.906535625%3DMengmeng%2520Zhang%2520and%2520Xingyuan%2520Dai%2520and%2520Yicheng%2520Sun%2520and%2520Jing%2520Wang%2520and%2520Yueyang%2520Yao%2520and%2520Xiaoyan%2520Gong%2520and%2520Fuze%2520Cong%2520and%2520Feiyue%2520Wang%2520and%2520Yisheng%2520Lv%26entry.1292438233%3D%2520%2520Although%2520the%2520Segment%2520Anything%2520Model%2520%2528SAM%2529%2520is%2520highly%2520effective%2520in%2520natural%250Aimage%2520segmentation%252C%2520it%2520requires%2520dependencies%2520on%2520prompts%252C%2520which%2520limits%2520its%250Aapplicability%2520to%2520medical%2520imaging%2520where%2520manual%2520prompts%2520are%2520often%2520unavailable.%250AExisting%2520efforts%2520to%2520fine-tune%2520SAM%2520for%2520medical%2520segmentation%2520typically%2520struggle%250Ato%2520remove%2520this%2520dependency.%2520We%2520propose%2520Hierarchical%2520Self-Prompting%2520SAM%250A%2528HSP-SAM%2529%252C%2520a%2520novel%2520self-prompting%2520framework%2520that%2520enables%2520SAM%2520to%2520achieve%2520strong%250Aperformance%2520in%2520prompt-free%2520medical%2520image%2520segmentation.%2520Unlike%2520previous%250Aself-prompting%2520methods%2520that%2520remain%2520limited%2520to%2520positional%2520prompts%2520similar%2520to%250Avanilla%2520SAM%252C%2520we%2520are%2520the%2520first%2520to%2520introduce%2520learning%2520abstract%2520prompts%2520during%2520the%250Aself-prompting%2520process.%2520This%2520simple%2520and%2520intuitive%2520self-prompting%2520framework%250Aachieves%2520superior%2520performance%2520on%2520classic%2520segmentation%2520tasks%2520such%2520as%2520polyp%2520and%250Askin%2520lesion%2520segmentation%252C%2520while%2520maintaining%2520robustness%2520across%2520diverse%2520medical%250Aimaging%2520modalities.%2520Furthermore%252C%2520it%2520exhibits%2520strong%2520generalization%2520to%2520unseen%250Adatasets%252C%2520achieving%2520improvements%2520of%2520up%2520to%252014.04%2525%2520over%2520previous%2520state-of-the-art%250Amethods%2520on%2520some%2520challenging%2520benchmarks.%2520These%2520results%2520suggest%2520that%2520abstract%250Aprompts%2520encapsulate%2520richer%2520and%2520higher-dimensional%2520semantic%2520information%2520compared%250Ato%2520positional%2520prompts%252C%2520thereby%2520enhancing%2520the%2520model%2527s%2520robustness%2520and%250Ageneralization%2520performance.%2520All%2520models%2520and%2520codes%2520will%2520be%2520released%2520upon%250Aacceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02854v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Self-Prompting%20SAM%3A%20A%20Prompt-Free%20Medical%20Image%0A%20%20Segmentation%20Framework&entry.906535625=Mengmeng%20Zhang%20and%20Xingyuan%20Dai%20and%20Yicheng%20Sun%20and%20Jing%20Wang%20and%20Yueyang%20Yao%20and%20Xiaoyan%20Gong%20and%20Fuze%20Cong%20and%20Feiyue%20Wang%20and%20Yisheng%20Lv&entry.1292438233=%20%20Although%20the%20Segment%20Anything%20Model%20%28SAM%29%20is%20highly%20effective%20in%20natural%0Aimage%20segmentation%2C%20it%20requires%20dependencies%20on%20prompts%2C%20which%20limits%20its%0Aapplicability%20to%20medical%20imaging%20where%20manual%20prompts%20are%20often%20unavailable.%0AExisting%20efforts%20to%20fine-tune%20SAM%20for%20medical%20segmentation%20typically%20struggle%0Ato%20remove%20this%20dependency.%20We%20propose%20Hierarchical%20Self-Prompting%20SAM%0A%28HSP-SAM%29%2C%20a%20novel%20self-prompting%20framework%20that%20enables%20SAM%20to%20achieve%20strong%0Aperformance%20in%20prompt-free%20medical%20image%20segmentation.%20Unlike%20previous%0Aself-prompting%20methods%20that%20remain%20limited%20to%20positional%20prompts%20similar%20to%0Avanilla%20SAM%2C%20we%20are%20the%20first%20to%20introduce%20learning%20abstract%20prompts%20during%20the%0Aself-prompting%20process.%20This%20simple%20and%20intuitive%20self-prompting%20framework%0Aachieves%20superior%20performance%20on%20classic%20segmentation%20tasks%20such%20as%20polyp%20and%0Askin%20lesion%20segmentation%2C%20while%20maintaining%20robustness%20across%20diverse%20medical%0Aimaging%20modalities.%20Furthermore%2C%20it%20exhibits%20strong%20generalization%20to%20unseen%0Adatasets%2C%20achieving%20improvements%20of%20up%20to%2014.04%25%20over%20previous%20state-of-the-art%0Amethods%20on%20some%20challenging%20benchmarks.%20These%20results%20suggest%20that%20abstract%0Aprompts%20encapsulate%20richer%20and%20higher-dimensional%20semantic%20information%20compared%0Ato%20positional%20prompts%2C%20thereby%20enhancing%20the%20model%27s%20robustness%20and%0Ageneralization%20performance.%20All%20models%20and%20codes%20will%20be%20released%20upon%0Aacceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02854v1&entry.124074799=Read"},
{"title": "TestDG: Test-time Domain Generalization for Continual Test-time\n  Adaptation", "author": "Sohyun Lee and Nayeong Kim and Juwon Kang and Seong Joon Oh and Suha Kwak", "abstract": "  This paper studies continual test-time adaptation (CTTA), the task of\nadapting a model to constantly changing unseen domains in testing while\npreserving previously learned knowledge. Existing CTTA methods mostly focus on\nadaptation to the current test domain only, overlooking generalization to\narbitrary test domains a model may face in the future. To tackle this\nlimitation, we present a novel online test-time domain generalization framework\nfor CTTA, dubbed TestDG. TestDG aims to learn features invariant to both\ncurrent and previous test domains on the fly during testing, improving the\npotential for effective generalization to future domains. To this end, we\npropose a new model architecture and a test-time adaptation strategy dedicated\nto learning domain-invariant features, along with a new data structure and\noptimization algorithm for effectively managing information from previous test\ndomains. TestDG achieved state of the art on four public CTTA benchmarks.\nMoreover, it showed superior generalization to unseen test domains.\n", "link": "http://arxiv.org/abs/2504.04981v2", "date": "2025-06-03", "relevancy": 2.4531, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.515}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4812}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4757}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TestDG%3A%20Test-time%20Domain%20Generalization%20for%20Continual%20Test-time%0A%20%20Adaptation&body=Title%3A%20TestDG%3A%20Test-time%20Domain%20Generalization%20for%20Continual%20Test-time%0A%20%20Adaptation%0AAuthor%3A%20Sohyun%20Lee%20and%20Nayeong%20Kim%20and%20Juwon%20Kang%20and%20Seong%20Joon%20Oh%20and%20Suha%20Kwak%0AAbstract%3A%20%20%20This%20paper%20studies%20continual%20test-time%20adaptation%20%28CTTA%29%2C%20the%20task%20of%0Aadapting%20a%20model%20to%20constantly%20changing%20unseen%20domains%20in%20testing%20while%0Apreserving%20previously%20learned%20knowledge.%20Existing%20CTTA%20methods%20mostly%20focus%20on%0Aadaptation%20to%20the%20current%20test%20domain%20only%2C%20overlooking%20generalization%20to%0Aarbitrary%20test%20domains%20a%20model%20may%20face%20in%20the%20future.%20To%20tackle%20this%0Alimitation%2C%20we%20present%20a%20novel%20online%20test-time%20domain%20generalization%20framework%0Afor%20CTTA%2C%20dubbed%20TestDG.%20TestDG%20aims%20to%20learn%20features%20invariant%20to%20both%0Acurrent%20and%20previous%20test%20domains%20on%20the%20fly%20during%20testing%2C%20improving%20the%0Apotential%20for%20effective%20generalization%20to%20future%20domains.%20To%20this%20end%2C%20we%0Apropose%20a%20new%20model%20architecture%20and%20a%20test-time%20adaptation%20strategy%20dedicated%0Ato%20learning%20domain-invariant%20features%2C%20along%20with%20a%20new%20data%20structure%20and%0Aoptimization%20algorithm%20for%20effectively%20managing%20information%20from%20previous%20test%0Adomains.%20TestDG%20achieved%20state%20of%20the%20art%20on%20four%20public%20CTTA%20benchmarks.%0AMoreover%2C%20it%20showed%20superior%20generalization%20to%20unseen%20test%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.04981v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTestDG%253A%2520Test-time%2520Domain%2520Generalization%2520for%2520Continual%2520Test-time%250A%2520%2520Adaptation%26entry.906535625%3DSohyun%2520Lee%2520and%2520Nayeong%2520Kim%2520and%2520Juwon%2520Kang%2520and%2520Seong%2520Joon%2520Oh%2520and%2520Suha%2520Kwak%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520continual%2520test-time%2520adaptation%2520%2528CTTA%2529%252C%2520the%2520task%2520of%250Aadapting%2520a%2520model%2520to%2520constantly%2520changing%2520unseen%2520domains%2520in%2520testing%2520while%250Apreserving%2520previously%2520learned%2520knowledge.%2520Existing%2520CTTA%2520methods%2520mostly%2520focus%2520on%250Aadaptation%2520to%2520the%2520current%2520test%2520domain%2520only%252C%2520overlooking%2520generalization%2520to%250Aarbitrary%2520test%2520domains%2520a%2520model%2520may%2520face%2520in%2520the%2520future.%2520To%2520tackle%2520this%250Alimitation%252C%2520we%2520present%2520a%2520novel%2520online%2520test-time%2520domain%2520generalization%2520framework%250Afor%2520CTTA%252C%2520dubbed%2520TestDG.%2520TestDG%2520aims%2520to%2520learn%2520features%2520invariant%2520to%2520both%250Acurrent%2520and%2520previous%2520test%2520domains%2520on%2520the%2520fly%2520during%2520testing%252C%2520improving%2520the%250Apotential%2520for%2520effective%2520generalization%2520to%2520future%2520domains.%2520To%2520this%2520end%252C%2520we%250Apropose%2520a%2520new%2520model%2520architecture%2520and%2520a%2520test-time%2520adaptation%2520strategy%2520dedicated%250Ato%2520learning%2520domain-invariant%2520features%252C%2520along%2520with%2520a%2520new%2520data%2520structure%2520and%250Aoptimization%2520algorithm%2520for%2520effectively%2520managing%2520information%2520from%2520previous%2520test%250Adomains.%2520TestDG%2520achieved%2520state%2520of%2520the%2520art%2520on%2520four%2520public%2520CTTA%2520benchmarks.%250AMoreover%252C%2520it%2520showed%2520superior%2520generalization%2520to%2520unseen%2520test%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.04981v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TestDG%3A%20Test-time%20Domain%20Generalization%20for%20Continual%20Test-time%0A%20%20Adaptation&entry.906535625=Sohyun%20Lee%20and%20Nayeong%20Kim%20and%20Juwon%20Kang%20and%20Seong%20Joon%20Oh%20and%20Suha%20Kwak&entry.1292438233=%20%20This%20paper%20studies%20continual%20test-time%20adaptation%20%28CTTA%29%2C%20the%20task%20of%0Aadapting%20a%20model%20to%20constantly%20changing%20unseen%20domains%20in%20testing%20while%0Apreserving%20previously%20learned%20knowledge.%20Existing%20CTTA%20methods%20mostly%20focus%20on%0Aadaptation%20to%20the%20current%20test%20domain%20only%2C%20overlooking%20generalization%20to%0Aarbitrary%20test%20domains%20a%20model%20may%20face%20in%20the%20future.%20To%20tackle%20this%0Alimitation%2C%20we%20present%20a%20novel%20online%20test-time%20domain%20generalization%20framework%0Afor%20CTTA%2C%20dubbed%20TestDG.%20TestDG%20aims%20to%20learn%20features%20invariant%20to%20both%0Acurrent%20and%20previous%20test%20domains%20on%20the%20fly%20during%20testing%2C%20improving%20the%0Apotential%20for%20effective%20generalization%20to%20future%20domains.%20To%20this%20end%2C%20we%0Apropose%20a%20new%20model%20architecture%20and%20a%20test-time%20adaptation%20strategy%20dedicated%0Ato%20learning%20domain-invariant%20features%2C%20along%20with%20a%20new%20data%20structure%20and%0Aoptimization%20algorithm%20for%20effectively%20managing%20information%20from%20previous%20test%0Adomains.%20TestDG%20achieved%20state%20of%20the%20art%20on%20four%20public%20CTTA%20benchmarks.%0AMoreover%2C%20it%20showed%20superior%20generalization%20to%20unseen%20test%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.04981v2&entry.124074799=Read"},
{"title": "LLM-Guided Taxonomy and Hierarchical Uncertainty for 3D Point Cloud\n  Active Learning", "author": "Chenxi Li and Nuo Chen and Fengyun Tan and Yantong Chen and Bochun Yuan and Tianrui Li and Chongshou Li", "abstract": "  We present a novel active learning framework for 3D point cloud semantic\nsegmentation that, for the first time, integrates large language models (LLMs)\nto construct hierarchical label structures and guide uncertainty-based sample\nselection. Unlike prior methods that treat labels as flat and independent, our\napproach leverages LLM prompting to automatically generate multi-level semantic\ntaxonomies and introduces a recursive uncertainty projection mechanism that\npropagates uncertainty across hierarchy levels. This enables spatially diverse,\nlabel-aware point selection that respects the inherent semantic structure of 3D\nscenes. Experiments on S3DIS and ScanNet v2 show that our method achieves up to\n4% mIoU improvement under extremely low annotation budgets (e.g., 0.02%),\nsubstantially outperforming existing baselines. Our results highlight the\nuntapped potential of LLMs as knowledge priors in 3D vision and establish\nhierarchical uncertainty modeling as a powerful paradigm for efficient point\ncloud annotation.\n", "link": "http://arxiv.org/abs/2505.18924v2", "date": "2025-06-03", "relevancy": 2.4476, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6734}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6149}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-Guided%20Taxonomy%20and%20Hierarchical%20Uncertainty%20for%203D%20Point%20Cloud%0A%20%20Active%20Learning&body=Title%3A%20LLM-Guided%20Taxonomy%20and%20Hierarchical%20Uncertainty%20for%203D%20Point%20Cloud%0A%20%20Active%20Learning%0AAuthor%3A%20Chenxi%20Li%20and%20Nuo%20Chen%20and%20Fengyun%20Tan%20and%20Yantong%20Chen%20and%20Bochun%20Yuan%20and%20Tianrui%20Li%20and%20Chongshou%20Li%0AAbstract%3A%20%20%20We%20present%20a%20novel%20active%20learning%20framework%20for%203D%20point%20cloud%20semantic%0Asegmentation%20that%2C%20for%20the%20first%20time%2C%20integrates%20large%20language%20models%20%28LLMs%29%0Ato%20construct%20hierarchical%20label%20structures%20and%20guide%20uncertainty-based%20sample%0Aselection.%20Unlike%20prior%20methods%20that%20treat%20labels%20as%20flat%20and%20independent%2C%20our%0Aapproach%20leverages%20LLM%20prompting%20to%20automatically%20generate%20multi-level%20semantic%0Ataxonomies%20and%20introduces%20a%20recursive%20uncertainty%20projection%20mechanism%20that%0Apropagates%20uncertainty%20across%20hierarchy%20levels.%20This%20enables%20spatially%20diverse%2C%0Alabel-aware%20point%20selection%20that%20respects%20the%20inherent%20semantic%20structure%20of%203D%0Ascenes.%20Experiments%20on%20S3DIS%20and%20ScanNet%20v2%20show%20that%20our%20method%20achieves%20up%20to%0A4%25%20mIoU%20improvement%20under%20extremely%20low%20annotation%20budgets%20%28e.g.%2C%200.02%25%29%2C%0Asubstantially%20outperforming%20existing%20baselines.%20Our%20results%20highlight%20the%0Auntapped%20potential%20of%20LLMs%20as%20knowledge%20priors%20in%203D%20vision%20and%20establish%0Ahierarchical%20uncertainty%20modeling%20as%20a%20powerful%20paradigm%20for%20efficient%20point%0Acloud%20annotation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.18924v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-Guided%2520Taxonomy%2520and%2520Hierarchical%2520Uncertainty%2520for%25203D%2520Point%2520Cloud%250A%2520%2520Active%2520Learning%26entry.906535625%3DChenxi%2520Li%2520and%2520Nuo%2520Chen%2520and%2520Fengyun%2520Tan%2520and%2520Yantong%2520Chen%2520and%2520Bochun%2520Yuan%2520and%2520Tianrui%2520Li%2520and%2520Chongshou%2520Li%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520active%2520learning%2520framework%2520for%25203D%2520point%2520cloud%2520semantic%250Asegmentation%2520that%252C%2520for%2520the%2520first%2520time%252C%2520integrates%2520large%2520language%2520models%2520%2528LLMs%2529%250Ato%2520construct%2520hierarchical%2520label%2520structures%2520and%2520guide%2520uncertainty-based%2520sample%250Aselection.%2520Unlike%2520prior%2520methods%2520that%2520treat%2520labels%2520as%2520flat%2520and%2520independent%252C%2520our%250Aapproach%2520leverages%2520LLM%2520prompting%2520to%2520automatically%2520generate%2520multi-level%2520semantic%250Ataxonomies%2520and%2520introduces%2520a%2520recursive%2520uncertainty%2520projection%2520mechanism%2520that%250Apropagates%2520uncertainty%2520across%2520hierarchy%2520levels.%2520This%2520enables%2520spatially%2520diverse%252C%250Alabel-aware%2520point%2520selection%2520that%2520respects%2520the%2520inherent%2520semantic%2520structure%2520of%25203D%250Ascenes.%2520Experiments%2520on%2520S3DIS%2520and%2520ScanNet%2520v2%2520show%2520that%2520our%2520method%2520achieves%2520up%2520to%250A4%2525%2520mIoU%2520improvement%2520under%2520extremely%2520low%2520annotation%2520budgets%2520%2528e.g.%252C%25200.02%2525%2529%252C%250Asubstantially%2520outperforming%2520existing%2520baselines.%2520Our%2520results%2520highlight%2520the%250Auntapped%2520potential%2520of%2520LLMs%2520as%2520knowledge%2520priors%2520in%25203D%2520vision%2520and%2520establish%250Ahierarchical%2520uncertainty%2520modeling%2520as%2520a%2520powerful%2520paradigm%2520for%2520efficient%2520point%250Acloud%2520annotation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18924v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-Guided%20Taxonomy%20and%20Hierarchical%20Uncertainty%20for%203D%20Point%20Cloud%0A%20%20Active%20Learning&entry.906535625=Chenxi%20Li%20and%20Nuo%20Chen%20and%20Fengyun%20Tan%20and%20Yantong%20Chen%20and%20Bochun%20Yuan%20and%20Tianrui%20Li%20and%20Chongshou%20Li&entry.1292438233=%20%20We%20present%20a%20novel%20active%20learning%20framework%20for%203D%20point%20cloud%20semantic%0Asegmentation%20that%2C%20for%20the%20first%20time%2C%20integrates%20large%20language%20models%20%28LLMs%29%0Ato%20construct%20hierarchical%20label%20structures%20and%20guide%20uncertainty-based%20sample%0Aselection.%20Unlike%20prior%20methods%20that%20treat%20labels%20as%20flat%20and%20independent%2C%20our%0Aapproach%20leverages%20LLM%20prompting%20to%20automatically%20generate%20multi-level%20semantic%0Ataxonomies%20and%20introduces%20a%20recursive%20uncertainty%20projection%20mechanism%20that%0Apropagates%20uncertainty%20across%20hierarchy%20levels.%20This%20enables%20spatially%20diverse%2C%0Alabel-aware%20point%20selection%20that%20respects%20the%20inherent%20semantic%20structure%20of%203D%0Ascenes.%20Experiments%20on%20S3DIS%20and%20ScanNet%20v2%20show%20that%20our%20method%20achieves%20up%20to%0A4%25%20mIoU%20improvement%20under%20extremely%20low%20annotation%20budgets%20%28e.g.%2C%200.02%25%29%2C%0Asubstantially%20outperforming%20existing%20baselines.%20Our%20results%20highlight%20the%0Auntapped%20potential%20of%20LLMs%20as%20knowledge%20priors%20in%203D%20vision%20and%20establish%0Ahierarchical%20uncertainty%20modeling%20as%20a%20powerful%20paradigm%20for%20efficient%20point%0Acloud%20annotation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.18924v2&entry.124074799=Read"},
{"title": "Exemplar-condensed Federated Class-incremental Learning", "author": "Rui Sun and Yumin Zhang and Varun Ojha and Tejal Shah and Haoran Duan and Bo Wei and Rajiv Ranjan", "abstract": "  We propose Exemplar-Condensed federated class-incremental learning (ECoral)\nto distil the training characteristics of real images from streaming data into\ninformative rehearsal exemplars. The proposed method eliminates the limitations\nof exemplar selection in replay-based approaches for mitigating catastrophic\nforgetting in federated continual learning (FCL). The limitations particularly\nrelated to the heterogeneity of information density of each summarized data.\nOur approach maintains the consistency of training gradients and the\nrelationship to past tasks for the summarized exemplars to represent the\nstreaming data compared to the original images effectively. Additionally, our\napproach reduces the information-level heterogeneity of the summarized data by\ninter-client sharing of the disentanglement generative model. Extensive\nexperiments show that our ECoral outperforms several state-of-the-art methods\nand can be seamlessly integrated with many existing approaches to enhance\nperformance.\n", "link": "http://arxiv.org/abs/2412.18926v2", "date": "2025-06-03", "relevancy": 2.4448, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4944}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4938}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4787}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exemplar-condensed%20Federated%20Class-incremental%20Learning&body=Title%3A%20Exemplar-condensed%20Federated%20Class-incremental%20Learning%0AAuthor%3A%20Rui%20Sun%20and%20Yumin%20Zhang%20and%20Varun%20Ojha%20and%20Tejal%20Shah%20and%20Haoran%20Duan%20and%20Bo%20Wei%20and%20Rajiv%20Ranjan%0AAbstract%3A%20%20%20We%20propose%20Exemplar-Condensed%20federated%20class-incremental%20learning%20%28ECoral%29%0Ato%20distil%20the%20training%20characteristics%20of%20real%20images%20from%20streaming%20data%20into%0Ainformative%20rehearsal%20exemplars.%20The%20proposed%20method%20eliminates%20the%20limitations%0Aof%20exemplar%20selection%20in%20replay-based%20approaches%20for%20mitigating%20catastrophic%0Aforgetting%20in%20federated%20continual%20learning%20%28FCL%29.%20The%20limitations%20particularly%0Arelated%20to%20the%20heterogeneity%20of%20information%20density%20of%20each%20summarized%20data.%0AOur%20approach%20maintains%20the%20consistency%20of%20training%20gradients%20and%20the%0Arelationship%20to%20past%20tasks%20for%20the%20summarized%20exemplars%20to%20represent%20the%0Astreaming%20data%20compared%20to%20the%20original%20images%20effectively.%20Additionally%2C%20our%0Aapproach%20reduces%20the%20information-level%20heterogeneity%20of%20the%20summarized%20data%20by%0Ainter-client%20sharing%20of%20the%20disentanglement%20generative%20model.%20Extensive%0Aexperiments%20show%20that%20our%20ECoral%20outperforms%20several%20state-of-the-art%20methods%0Aand%20can%20be%20seamlessly%20integrated%20with%20many%20existing%20approaches%20to%20enhance%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.18926v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExemplar-condensed%2520Federated%2520Class-incremental%2520Learning%26entry.906535625%3DRui%2520Sun%2520and%2520Yumin%2520Zhang%2520and%2520Varun%2520Ojha%2520and%2520Tejal%2520Shah%2520and%2520Haoran%2520Duan%2520and%2520Bo%2520Wei%2520and%2520Rajiv%2520Ranjan%26entry.1292438233%3D%2520%2520We%2520propose%2520Exemplar-Condensed%2520federated%2520class-incremental%2520learning%2520%2528ECoral%2529%250Ato%2520distil%2520the%2520training%2520characteristics%2520of%2520real%2520images%2520from%2520streaming%2520data%2520into%250Ainformative%2520rehearsal%2520exemplars.%2520The%2520proposed%2520method%2520eliminates%2520the%2520limitations%250Aof%2520exemplar%2520selection%2520in%2520replay-based%2520approaches%2520for%2520mitigating%2520catastrophic%250Aforgetting%2520in%2520federated%2520continual%2520learning%2520%2528FCL%2529.%2520The%2520limitations%2520particularly%250Arelated%2520to%2520the%2520heterogeneity%2520of%2520information%2520density%2520of%2520each%2520summarized%2520data.%250AOur%2520approach%2520maintains%2520the%2520consistency%2520of%2520training%2520gradients%2520and%2520the%250Arelationship%2520to%2520past%2520tasks%2520for%2520the%2520summarized%2520exemplars%2520to%2520represent%2520the%250Astreaming%2520data%2520compared%2520to%2520the%2520original%2520images%2520effectively.%2520Additionally%252C%2520our%250Aapproach%2520reduces%2520the%2520information-level%2520heterogeneity%2520of%2520the%2520summarized%2520data%2520by%250Ainter-client%2520sharing%2520of%2520the%2520disentanglement%2520generative%2520model.%2520Extensive%250Aexperiments%2520show%2520that%2520our%2520ECoral%2520outperforms%2520several%2520state-of-the-art%2520methods%250Aand%2520can%2520be%2520seamlessly%2520integrated%2520with%2520many%2520existing%2520approaches%2520to%2520enhance%250Aperformance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.18926v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exemplar-condensed%20Federated%20Class-incremental%20Learning&entry.906535625=Rui%20Sun%20and%20Yumin%20Zhang%20and%20Varun%20Ojha%20and%20Tejal%20Shah%20and%20Haoran%20Duan%20and%20Bo%20Wei%20and%20Rajiv%20Ranjan&entry.1292438233=%20%20We%20propose%20Exemplar-Condensed%20federated%20class-incremental%20learning%20%28ECoral%29%0Ato%20distil%20the%20training%20characteristics%20of%20real%20images%20from%20streaming%20data%20into%0Ainformative%20rehearsal%20exemplars.%20The%20proposed%20method%20eliminates%20the%20limitations%0Aof%20exemplar%20selection%20in%20replay-based%20approaches%20for%20mitigating%20catastrophic%0Aforgetting%20in%20federated%20continual%20learning%20%28FCL%29.%20The%20limitations%20particularly%0Arelated%20to%20the%20heterogeneity%20of%20information%20density%20of%20each%20summarized%20data.%0AOur%20approach%20maintains%20the%20consistency%20of%20training%20gradients%20and%20the%0Arelationship%20to%20past%20tasks%20for%20the%20summarized%20exemplars%20to%20represent%20the%0Astreaming%20data%20compared%20to%20the%20original%20images%20effectively.%20Additionally%2C%20our%0Aapproach%20reduces%20the%20information-level%20heterogeneity%20of%20the%20summarized%20data%20by%0Ainter-client%20sharing%20of%20the%20disentanglement%20generative%20model.%20Extensive%0Aexperiments%20show%20that%20our%20ECoral%20outperforms%20several%20state-of-the-art%20methods%0Aand%20can%20be%20seamlessly%20integrated%20with%20many%20existing%20approaches%20to%20enhance%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.18926v2&entry.124074799=Read"},
{"title": "OASST-ETC Dataset: Alignment Signals from Eye-tracking Analysis of LLM\n  Responses", "author": "Angela Lopez-Cardona and Sebastian Idesis and Miguel Barreda-\u00c1ngeles and Sergi Abadal and Ioannis Arapakis", "abstract": "  While Large Language Models (LLMs) have significantly advanced natural\nlanguage processing, aligning them with human preferences remains an open\nchallenge. Although current alignment methods rely primarily on explicit\nfeedback, eye-tracking (ET) data offers insights into real-time cognitive\nprocessing during reading. In this paper, we present OASST-ETC, a novel\neye-tracking corpus capturing reading patterns from 24 participants, while\nevaluating LLM-generated responses from the OASST1 dataset. Our analysis\nreveals distinct reading patterns between preferred and non-preferred\nresponses, which we compare with synthetic eye-tracking data. Furthermore, we\nexamine the correlation between human reading measures and attention patterns\nfrom various transformer-based models, discovering stronger correlations in\npreferred responses. This work introduces a unique resource for studying human\ncognitive processing in LLM evaluation and suggests promising directions for\nincorporating eye-tracking data into alignment methods. The dataset and\nanalysis code are publicly available.\n", "link": "http://arxiv.org/abs/2503.10927v3", "date": "2025-06-03", "relevancy": 2.4391, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4892}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4892}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4851}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OASST-ETC%20Dataset%3A%20Alignment%20Signals%20from%20Eye-tracking%20Analysis%20of%20LLM%0A%20%20Responses&body=Title%3A%20OASST-ETC%20Dataset%3A%20Alignment%20Signals%20from%20Eye-tracking%20Analysis%20of%20LLM%0A%20%20Responses%0AAuthor%3A%20Angela%20Lopez-Cardona%20and%20Sebastian%20Idesis%20and%20Miguel%20Barreda-%C3%81ngeles%20and%20Sergi%20Abadal%20and%20Ioannis%20Arapakis%0AAbstract%3A%20%20%20While%20Large%20Language%20Models%20%28LLMs%29%20have%20significantly%20advanced%20natural%0Alanguage%20processing%2C%20aligning%20them%20with%20human%20preferences%20remains%20an%20open%0Achallenge.%20Although%20current%20alignment%20methods%20rely%20primarily%20on%20explicit%0Afeedback%2C%20eye-tracking%20%28ET%29%20data%20offers%20insights%20into%20real-time%20cognitive%0Aprocessing%20during%20reading.%20In%20this%20paper%2C%20we%20present%20OASST-ETC%2C%20a%20novel%0Aeye-tracking%20corpus%20capturing%20reading%20patterns%20from%2024%20participants%2C%20while%0Aevaluating%20LLM-generated%20responses%20from%20the%20OASST1%20dataset.%20Our%20analysis%0Areveals%20distinct%20reading%20patterns%20between%20preferred%20and%20non-preferred%0Aresponses%2C%20which%20we%20compare%20with%20synthetic%20eye-tracking%20data.%20Furthermore%2C%20we%0Aexamine%20the%20correlation%20between%20human%20reading%20measures%20and%20attention%20patterns%0Afrom%20various%20transformer-based%20models%2C%20discovering%20stronger%20correlations%20in%0Apreferred%20responses.%20This%20work%20introduces%20a%20unique%20resource%20for%20studying%20human%0Acognitive%20processing%20in%20LLM%20evaluation%20and%20suggests%20promising%20directions%20for%0Aincorporating%20eye-tracking%20data%20into%20alignment%20methods.%20The%20dataset%20and%0Aanalysis%20code%20are%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.10927v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOASST-ETC%2520Dataset%253A%2520Alignment%2520Signals%2520from%2520Eye-tracking%2520Analysis%2520of%2520LLM%250A%2520%2520Responses%26entry.906535625%3DAngela%2520Lopez-Cardona%2520and%2520Sebastian%2520Idesis%2520and%2520Miguel%2520Barreda-%25C3%2581ngeles%2520and%2520Sergi%2520Abadal%2520and%2520Ioannis%2520Arapakis%26entry.1292438233%3D%2520%2520While%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520significantly%2520advanced%2520natural%250Alanguage%2520processing%252C%2520aligning%2520them%2520with%2520human%2520preferences%2520remains%2520an%2520open%250Achallenge.%2520Although%2520current%2520alignment%2520methods%2520rely%2520primarily%2520on%2520explicit%250Afeedback%252C%2520eye-tracking%2520%2528ET%2529%2520data%2520offers%2520insights%2520into%2520real-time%2520cognitive%250Aprocessing%2520during%2520reading.%2520In%2520this%2520paper%252C%2520we%2520present%2520OASST-ETC%252C%2520a%2520novel%250Aeye-tracking%2520corpus%2520capturing%2520reading%2520patterns%2520from%252024%2520participants%252C%2520while%250Aevaluating%2520LLM-generated%2520responses%2520from%2520the%2520OASST1%2520dataset.%2520Our%2520analysis%250Areveals%2520distinct%2520reading%2520patterns%2520between%2520preferred%2520and%2520non-preferred%250Aresponses%252C%2520which%2520we%2520compare%2520with%2520synthetic%2520eye-tracking%2520data.%2520Furthermore%252C%2520we%250Aexamine%2520the%2520correlation%2520between%2520human%2520reading%2520measures%2520and%2520attention%2520patterns%250Afrom%2520various%2520transformer-based%2520models%252C%2520discovering%2520stronger%2520correlations%2520in%250Apreferred%2520responses.%2520This%2520work%2520introduces%2520a%2520unique%2520resource%2520for%2520studying%2520human%250Acognitive%2520processing%2520in%2520LLM%2520evaluation%2520and%2520suggests%2520promising%2520directions%2520for%250Aincorporating%2520eye-tracking%2520data%2520into%2520alignment%2520methods.%2520The%2520dataset%2520and%250Aanalysis%2520code%2520are%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.10927v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OASST-ETC%20Dataset%3A%20Alignment%20Signals%20from%20Eye-tracking%20Analysis%20of%20LLM%0A%20%20Responses&entry.906535625=Angela%20Lopez-Cardona%20and%20Sebastian%20Idesis%20and%20Miguel%20Barreda-%C3%81ngeles%20and%20Sergi%20Abadal%20and%20Ioannis%20Arapakis&entry.1292438233=%20%20While%20Large%20Language%20Models%20%28LLMs%29%20have%20significantly%20advanced%20natural%0Alanguage%20processing%2C%20aligning%20them%20with%20human%20preferences%20remains%20an%20open%0Achallenge.%20Although%20current%20alignment%20methods%20rely%20primarily%20on%20explicit%0Afeedback%2C%20eye-tracking%20%28ET%29%20data%20offers%20insights%20into%20real-time%20cognitive%0Aprocessing%20during%20reading.%20In%20this%20paper%2C%20we%20present%20OASST-ETC%2C%20a%20novel%0Aeye-tracking%20corpus%20capturing%20reading%20patterns%20from%2024%20participants%2C%20while%0Aevaluating%20LLM-generated%20responses%20from%20the%20OASST1%20dataset.%20Our%20analysis%0Areveals%20distinct%20reading%20patterns%20between%20preferred%20and%20non-preferred%0Aresponses%2C%20which%20we%20compare%20with%20synthetic%20eye-tracking%20data.%20Furthermore%2C%20we%0Aexamine%20the%20correlation%20between%20human%20reading%20measures%20and%20attention%20patterns%0Afrom%20various%20transformer-based%20models%2C%20discovering%20stronger%20correlations%20in%0Apreferred%20responses.%20This%20work%20introduces%20a%20unique%20resource%20for%20studying%20human%0Acognitive%20processing%20in%20LLM%20evaluation%20and%20suggests%20promising%20directions%20for%0Aincorporating%20eye-tracking%20data%20into%20alignment%20methods.%20The%20dataset%20and%0Aanalysis%20code%20are%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.10927v3&entry.124074799=Read"},
{"title": "Dialz: A Python Toolkit for Steering Vectors", "author": "Zara Siddique and Liam D. Turner and Luis Espinosa-Anke", "abstract": "  We introduce Dialz, a framework for advancing research on steering vectors\nfor open-source LLMs, implemented in Python. Steering vectors allow users to\nmodify activations at inference time to amplify or weaken a 'concept', e.g.\nhonesty or positivity, providing a more powerful alternative to prompting or\nfine-tuning. Dialz supports a diverse set of tasks, including creating\ncontrastive pair datasets, computing and applying steering vectors, and\nvisualizations. Unlike existing libraries, Dialz emphasizes modularity and\nusability, enabling both rapid prototyping and in-depth analysis. We\ndemonstrate how Dialz can be used to reduce harmful outputs such as\nstereotypes, while also providing insights into model behaviour across\ndifferent layers. We release Dialz with full documentation, tutorials, and\nsupport for popular open-source models to encourage further research in safe\nand controllable language generation. Dialz enables faster research cycles and\nfacilitates insights into model interpretability, paving the way for safer,\nmore transparent, and more reliable AI systems.\n", "link": "http://arxiv.org/abs/2505.06262v2", "date": "2025-06-03", "relevancy": 2.4379, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4963}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4963}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dialz%3A%20A%20Python%20Toolkit%20for%20Steering%20Vectors&body=Title%3A%20Dialz%3A%20A%20Python%20Toolkit%20for%20Steering%20Vectors%0AAuthor%3A%20Zara%20Siddique%20and%20Liam%20D.%20Turner%20and%20Luis%20Espinosa-Anke%0AAbstract%3A%20%20%20We%20introduce%20Dialz%2C%20a%20framework%20for%20advancing%20research%20on%20steering%20vectors%0Afor%20open-source%20LLMs%2C%20implemented%20in%20Python.%20Steering%20vectors%20allow%20users%20to%0Amodify%20activations%20at%20inference%20time%20to%20amplify%20or%20weaken%20a%20%27concept%27%2C%20e.g.%0Ahonesty%20or%20positivity%2C%20providing%20a%20more%20powerful%20alternative%20to%20prompting%20or%0Afine-tuning.%20Dialz%20supports%20a%20diverse%20set%20of%20tasks%2C%20including%20creating%0Acontrastive%20pair%20datasets%2C%20computing%20and%20applying%20steering%20vectors%2C%20and%0Avisualizations.%20Unlike%20existing%20libraries%2C%20Dialz%20emphasizes%20modularity%20and%0Ausability%2C%20enabling%20both%20rapid%20prototyping%20and%20in-depth%20analysis.%20We%0Ademonstrate%20how%20Dialz%20can%20be%20used%20to%20reduce%20harmful%20outputs%20such%20as%0Astereotypes%2C%20while%20also%20providing%20insights%20into%20model%20behaviour%20across%0Adifferent%20layers.%20We%20release%20Dialz%20with%20full%20documentation%2C%20tutorials%2C%20and%0Asupport%20for%20popular%20open-source%20models%20to%20encourage%20further%20research%20in%20safe%0Aand%20controllable%20language%20generation.%20Dialz%20enables%20faster%20research%20cycles%20and%0Afacilitates%20insights%20into%20model%20interpretability%2C%20paving%20the%20way%20for%20safer%2C%0Amore%20transparent%2C%20and%20more%20reliable%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.06262v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDialz%253A%2520A%2520Python%2520Toolkit%2520for%2520Steering%2520Vectors%26entry.906535625%3DZara%2520Siddique%2520and%2520Liam%2520D.%2520Turner%2520and%2520Luis%2520Espinosa-Anke%26entry.1292438233%3D%2520%2520We%2520introduce%2520Dialz%252C%2520a%2520framework%2520for%2520advancing%2520research%2520on%2520steering%2520vectors%250Afor%2520open-source%2520LLMs%252C%2520implemented%2520in%2520Python.%2520Steering%2520vectors%2520allow%2520users%2520to%250Amodify%2520activations%2520at%2520inference%2520time%2520to%2520amplify%2520or%2520weaken%2520a%2520%2527concept%2527%252C%2520e.g.%250Ahonesty%2520or%2520positivity%252C%2520providing%2520a%2520more%2520powerful%2520alternative%2520to%2520prompting%2520or%250Afine-tuning.%2520Dialz%2520supports%2520a%2520diverse%2520set%2520of%2520tasks%252C%2520including%2520creating%250Acontrastive%2520pair%2520datasets%252C%2520computing%2520and%2520applying%2520steering%2520vectors%252C%2520and%250Avisualizations.%2520Unlike%2520existing%2520libraries%252C%2520Dialz%2520emphasizes%2520modularity%2520and%250Ausability%252C%2520enabling%2520both%2520rapid%2520prototyping%2520and%2520in-depth%2520analysis.%2520We%250Ademonstrate%2520how%2520Dialz%2520can%2520be%2520used%2520to%2520reduce%2520harmful%2520outputs%2520such%2520as%250Astereotypes%252C%2520while%2520also%2520providing%2520insights%2520into%2520model%2520behaviour%2520across%250Adifferent%2520layers.%2520We%2520release%2520Dialz%2520with%2520full%2520documentation%252C%2520tutorials%252C%2520and%250Asupport%2520for%2520popular%2520open-source%2520models%2520to%2520encourage%2520further%2520research%2520in%2520safe%250Aand%2520controllable%2520language%2520generation.%2520Dialz%2520enables%2520faster%2520research%2520cycles%2520and%250Afacilitates%2520insights%2520into%2520model%2520interpretability%252C%2520paving%2520the%2520way%2520for%2520safer%252C%250Amore%2520transparent%252C%2520and%2520more%2520reliable%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.06262v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dialz%3A%20A%20Python%20Toolkit%20for%20Steering%20Vectors&entry.906535625=Zara%20Siddique%20and%20Liam%20D.%20Turner%20and%20Luis%20Espinosa-Anke&entry.1292438233=%20%20We%20introduce%20Dialz%2C%20a%20framework%20for%20advancing%20research%20on%20steering%20vectors%0Afor%20open-source%20LLMs%2C%20implemented%20in%20Python.%20Steering%20vectors%20allow%20users%20to%0Amodify%20activations%20at%20inference%20time%20to%20amplify%20or%20weaken%20a%20%27concept%27%2C%20e.g.%0Ahonesty%20or%20positivity%2C%20providing%20a%20more%20powerful%20alternative%20to%20prompting%20or%0Afine-tuning.%20Dialz%20supports%20a%20diverse%20set%20of%20tasks%2C%20including%20creating%0Acontrastive%20pair%20datasets%2C%20computing%20and%20applying%20steering%20vectors%2C%20and%0Avisualizations.%20Unlike%20existing%20libraries%2C%20Dialz%20emphasizes%20modularity%20and%0Ausability%2C%20enabling%20both%20rapid%20prototyping%20and%20in-depth%20analysis.%20We%0Ademonstrate%20how%20Dialz%20can%20be%20used%20to%20reduce%20harmful%20outputs%20such%20as%0Astereotypes%2C%20while%20also%20providing%20insights%20into%20model%20behaviour%20across%0Adifferent%20layers.%20We%20release%20Dialz%20with%20full%20documentation%2C%20tutorials%2C%20and%0Asupport%20for%20popular%20open-source%20models%20to%20encourage%20further%20research%20in%20safe%0Aand%20controllable%20language%20generation.%20Dialz%20enables%20faster%20research%20cycles%20and%0Afacilitates%20insights%20into%20model%20interpretability%2C%20paving%20the%20way%20for%20safer%2C%0Amore%20transparent%2C%20and%20more%20reliable%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.06262v2&entry.124074799=Read"},
{"title": "Leveraging Information Retrieval to Enhance Spoken Language\n  Understanding Prompts in Few-Shot Learning", "author": "Pierre Lepagnol and Sahar Ghannay and Thomas Gerald and Christophe Servan and Sophie Rosset", "abstract": "  Understanding user queries is fundamental in many applications, such as home\nassistants, booking systems, or recommendations. Accordingly, it is crucial to\ndevelop accurate Spoken Language Understanding (SLU) approaches to ensure the\nreliability of the considered system. Current State-of-the-Art SLU techniques\nrely on large amounts of training data; however, only limited annotated\nexamples are available for specific tasks or languages.\n  In the meantime, instruction-tuned large language models (LLMs) have shown\nexceptional performance on unseen tasks in a few-shot setting when provided\nwith adequate prompts. In this work, we propose to explore example selection by\nleveraging Information retrieval (IR) approaches to build an enhanced prompt\nthat is applied to an SLU task. We evaluate the effectiveness of the proposed\nmethod on several SLU benchmarks. Experimental results show that lexical IR\nmethods significantly enhance performance without increasing prompt length.\n", "link": "http://arxiv.org/abs/2506.03035v1", "date": "2025-06-03", "relevancy": 2.4239, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.501}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.501}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4522}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Information%20Retrieval%20to%20Enhance%20Spoken%20Language%0A%20%20Understanding%20Prompts%20in%20Few-Shot%20Learning&body=Title%3A%20Leveraging%20Information%20Retrieval%20to%20Enhance%20Spoken%20Language%0A%20%20Understanding%20Prompts%20in%20Few-Shot%20Learning%0AAuthor%3A%20Pierre%20Lepagnol%20and%20Sahar%20Ghannay%20and%20Thomas%20Gerald%20and%20Christophe%20Servan%20and%20Sophie%20Rosset%0AAbstract%3A%20%20%20Understanding%20user%20queries%20is%20fundamental%20in%20many%20applications%2C%20such%20as%20home%0Aassistants%2C%20booking%20systems%2C%20or%20recommendations.%20Accordingly%2C%20it%20is%20crucial%20to%0Adevelop%20accurate%20Spoken%20Language%20Understanding%20%28SLU%29%20approaches%20to%20ensure%20the%0Areliability%20of%20the%20considered%20system.%20Current%20State-of-the-Art%20SLU%20techniques%0Arely%20on%20large%20amounts%20of%20training%20data%3B%20however%2C%20only%20limited%20annotated%0Aexamples%20are%20available%20for%20specific%20tasks%20or%20languages.%0A%20%20In%20the%20meantime%2C%20instruction-tuned%20large%20language%20models%20%28LLMs%29%20have%20shown%0Aexceptional%20performance%20on%20unseen%20tasks%20in%20a%20few-shot%20setting%20when%20provided%0Awith%20adequate%20prompts.%20In%20this%20work%2C%20we%20propose%20to%20explore%20example%20selection%20by%0Aleveraging%20Information%20retrieval%20%28IR%29%20approaches%20to%20build%20an%20enhanced%20prompt%0Athat%20is%20applied%20to%20an%20SLU%20task.%20We%20evaluate%20the%20effectiveness%20of%20the%20proposed%0Amethod%20on%20several%20SLU%20benchmarks.%20Experimental%20results%20show%20that%20lexical%20IR%0Amethods%20significantly%20enhance%20performance%20without%20increasing%20prompt%20length.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03035v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Information%2520Retrieval%2520to%2520Enhance%2520Spoken%2520Language%250A%2520%2520Understanding%2520Prompts%2520in%2520Few-Shot%2520Learning%26entry.906535625%3DPierre%2520Lepagnol%2520and%2520Sahar%2520Ghannay%2520and%2520Thomas%2520Gerald%2520and%2520Christophe%2520Servan%2520and%2520Sophie%2520Rosset%26entry.1292438233%3D%2520%2520Understanding%2520user%2520queries%2520is%2520fundamental%2520in%2520many%2520applications%252C%2520such%2520as%2520home%250Aassistants%252C%2520booking%2520systems%252C%2520or%2520recommendations.%2520Accordingly%252C%2520it%2520is%2520crucial%2520to%250Adevelop%2520accurate%2520Spoken%2520Language%2520Understanding%2520%2528SLU%2529%2520approaches%2520to%2520ensure%2520the%250Areliability%2520of%2520the%2520considered%2520system.%2520Current%2520State-of-the-Art%2520SLU%2520techniques%250Arely%2520on%2520large%2520amounts%2520of%2520training%2520data%253B%2520however%252C%2520only%2520limited%2520annotated%250Aexamples%2520are%2520available%2520for%2520specific%2520tasks%2520or%2520languages.%250A%2520%2520In%2520the%2520meantime%252C%2520instruction-tuned%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520shown%250Aexceptional%2520performance%2520on%2520unseen%2520tasks%2520in%2520a%2520few-shot%2520setting%2520when%2520provided%250Awith%2520adequate%2520prompts.%2520In%2520this%2520work%252C%2520we%2520propose%2520to%2520explore%2520example%2520selection%2520by%250Aleveraging%2520Information%2520retrieval%2520%2528IR%2529%2520approaches%2520to%2520build%2520an%2520enhanced%2520prompt%250Athat%2520is%2520applied%2520to%2520an%2520SLU%2520task.%2520We%2520evaluate%2520the%2520effectiveness%2520of%2520the%2520proposed%250Amethod%2520on%2520several%2520SLU%2520benchmarks.%2520Experimental%2520results%2520show%2520that%2520lexical%2520IR%250Amethods%2520significantly%2520enhance%2520performance%2520without%2520increasing%2520prompt%2520length.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03035v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Information%20Retrieval%20to%20Enhance%20Spoken%20Language%0A%20%20Understanding%20Prompts%20in%20Few-Shot%20Learning&entry.906535625=Pierre%20Lepagnol%20and%20Sahar%20Ghannay%20and%20Thomas%20Gerald%20and%20Christophe%20Servan%20and%20Sophie%20Rosset&entry.1292438233=%20%20Understanding%20user%20queries%20is%20fundamental%20in%20many%20applications%2C%20such%20as%20home%0Aassistants%2C%20booking%20systems%2C%20or%20recommendations.%20Accordingly%2C%20it%20is%20crucial%20to%0Adevelop%20accurate%20Spoken%20Language%20Understanding%20%28SLU%29%20approaches%20to%20ensure%20the%0Areliability%20of%20the%20considered%20system.%20Current%20State-of-the-Art%20SLU%20techniques%0Arely%20on%20large%20amounts%20of%20training%20data%3B%20however%2C%20only%20limited%20annotated%0Aexamples%20are%20available%20for%20specific%20tasks%20or%20languages.%0A%20%20In%20the%20meantime%2C%20instruction-tuned%20large%20language%20models%20%28LLMs%29%20have%20shown%0Aexceptional%20performance%20on%20unseen%20tasks%20in%20a%20few-shot%20setting%20when%20provided%0Awith%20adequate%20prompts.%20In%20this%20work%2C%20we%20propose%20to%20explore%20example%20selection%20by%0Aleveraging%20Information%20retrieval%20%28IR%29%20approaches%20to%20build%20an%20enhanced%20prompt%0Athat%20is%20applied%20to%20an%20SLU%20task.%20We%20evaluate%20the%20effectiveness%20of%20the%20proposed%0Amethod%20on%20several%20SLU%20benchmarks.%20Experimental%20results%20show%20that%20lexical%20IR%0Amethods%20significantly%20enhance%20performance%20without%20increasing%20prompt%20length.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03035v1&entry.124074799=Read"},
{"title": "GeneA-SLAM2: Dynamic SLAM with AutoEncoder-Preprocessed Genetic\n  Keypoints Resampling and Depth Variance-Guided Dynamic Region Removal", "author": "Shufan Qing and Anzhen Li and Qiandi Wang and Yuefeng Niu and Mingchen Feng and Guoliang Hu and Jinqiao Wu and Fengtao Nan and Yingchun Fan", "abstract": "  Existing semantic SLAM in dynamic environments mainly identify dynamic\nregions through object detection or semantic segmentation methods. However, in\ncertain highly dynamic scenarios, the detection boxes or segmentation masks\ncannot fully cover dynamic regions. Therefore, this paper proposes a robust and\nefficient GeneA-SLAM2 system that leverages depth variance constraints to\nhandle dynamic scenes. Our method extracts dynamic pixels via depth variance\nand creates precise depth masks to guide the removal of dynamic objects.\nSimultaneously, an autoencoder is used to reconstruct keypoints, improving the\ngenetic resampling keypoint algorithm to obtain more uniformly distributed\nkeypoints and enhance the accuracy of pose estimation. Our system was evaluated\non multiple highly dynamic sequences. The results demonstrate that GeneA-SLAM2\nmaintains high accuracy in dynamic scenes compared to current methods. Code is\navailable at: https://github.com/qingshufan/GeneA-SLAM2.\n", "link": "http://arxiv.org/abs/2506.02736v1", "date": "2025-06-03", "relevancy": 2.4146, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6327}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5894}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeneA-SLAM2%3A%20Dynamic%20SLAM%20with%20AutoEncoder-Preprocessed%20Genetic%0A%20%20Keypoints%20Resampling%20and%20Depth%20Variance-Guided%20Dynamic%20Region%20Removal&body=Title%3A%20GeneA-SLAM2%3A%20Dynamic%20SLAM%20with%20AutoEncoder-Preprocessed%20Genetic%0A%20%20Keypoints%20Resampling%20and%20Depth%20Variance-Guided%20Dynamic%20Region%20Removal%0AAuthor%3A%20Shufan%20Qing%20and%20Anzhen%20Li%20and%20Qiandi%20Wang%20and%20Yuefeng%20Niu%20and%20Mingchen%20Feng%20and%20Guoliang%20Hu%20and%20Jinqiao%20Wu%20and%20Fengtao%20Nan%20and%20Yingchun%20Fan%0AAbstract%3A%20%20%20Existing%20semantic%20SLAM%20in%20dynamic%20environments%20mainly%20identify%20dynamic%0Aregions%20through%20object%20detection%20or%20semantic%20segmentation%20methods.%20However%2C%20in%0Acertain%20highly%20dynamic%20scenarios%2C%20the%20detection%20boxes%20or%20segmentation%20masks%0Acannot%20fully%20cover%20dynamic%20regions.%20Therefore%2C%20this%20paper%20proposes%20a%20robust%20and%0Aefficient%20GeneA-SLAM2%20system%20that%20leverages%20depth%20variance%20constraints%20to%0Ahandle%20dynamic%20scenes.%20Our%20method%20extracts%20dynamic%20pixels%20via%20depth%20variance%0Aand%20creates%20precise%20depth%20masks%20to%20guide%20the%20removal%20of%20dynamic%20objects.%0ASimultaneously%2C%20an%20autoencoder%20is%20used%20to%20reconstruct%20keypoints%2C%20improving%20the%0Agenetic%20resampling%20keypoint%20algorithm%20to%20obtain%20more%20uniformly%20distributed%0Akeypoints%20and%20enhance%20the%20accuracy%20of%20pose%20estimation.%20Our%20system%20was%20evaluated%0Aon%20multiple%20highly%20dynamic%20sequences.%20The%20results%20demonstrate%20that%20GeneA-SLAM2%0Amaintains%20high%20accuracy%20in%20dynamic%20scenes%20compared%20to%20current%20methods.%20Code%20is%0Aavailable%20at%3A%20https%3A//github.com/qingshufan/GeneA-SLAM2.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02736v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneA-SLAM2%253A%2520Dynamic%2520SLAM%2520with%2520AutoEncoder-Preprocessed%2520Genetic%250A%2520%2520Keypoints%2520Resampling%2520and%2520Depth%2520Variance-Guided%2520Dynamic%2520Region%2520Removal%26entry.906535625%3DShufan%2520Qing%2520and%2520Anzhen%2520Li%2520and%2520Qiandi%2520Wang%2520and%2520Yuefeng%2520Niu%2520and%2520Mingchen%2520Feng%2520and%2520Guoliang%2520Hu%2520and%2520Jinqiao%2520Wu%2520and%2520Fengtao%2520Nan%2520and%2520Yingchun%2520Fan%26entry.1292438233%3D%2520%2520Existing%2520semantic%2520SLAM%2520in%2520dynamic%2520environments%2520mainly%2520identify%2520dynamic%250Aregions%2520through%2520object%2520detection%2520or%2520semantic%2520segmentation%2520methods.%2520However%252C%2520in%250Acertain%2520highly%2520dynamic%2520scenarios%252C%2520the%2520detection%2520boxes%2520or%2520segmentation%2520masks%250Acannot%2520fully%2520cover%2520dynamic%2520regions.%2520Therefore%252C%2520this%2520paper%2520proposes%2520a%2520robust%2520and%250Aefficient%2520GeneA-SLAM2%2520system%2520that%2520leverages%2520depth%2520variance%2520constraints%2520to%250Ahandle%2520dynamic%2520scenes.%2520Our%2520method%2520extracts%2520dynamic%2520pixels%2520via%2520depth%2520variance%250Aand%2520creates%2520precise%2520depth%2520masks%2520to%2520guide%2520the%2520removal%2520of%2520dynamic%2520objects.%250ASimultaneously%252C%2520an%2520autoencoder%2520is%2520used%2520to%2520reconstruct%2520keypoints%252C%2520improving%2520the%250Agenetic%2520resampling%2520keypoint%2520algorithm%2520to%2520obtain%2520more%2520uniformly%2520distributed%250Akeypoints%2520and%2520enhance%2520the%2520accuracy%2520of%2520pose%2520estimation.%2520Our%2520system%2520was%2520evaluated%250Aon%2520multiple%2520highly%2520dynamic%2520sequences.%2520The%2520results%2520demonstrate%2520that%2520GeneA-SLAM2%250Amaintains%2520high%2520accuracy%2520in%2520dynamic%2520scenes%2520compared%2520to%2520current%2520methods.%2520Code%2520is%250Aavailable%2520at%253A%2520https%253A//github.com/qingshufan/GeneA-SLAM2.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02736v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeneA-SLAM2%3A%20Dynamic%20SLAM%20with%20AutoEncoder-Preprocessed%20Genetic%0A%20%20Keypoints%20Resampling%20and%20Depth%20Variance-Guided%20Dynamic%20Region%20Removal&entry.906535625=Shufan%20Qing%20and%20Anzhen%20Li%20and%20Qiandi%20Wang%20and%20Yuefeng%20Niu%20and%20Mingchen%20Feng%20and%20Guoliang%20Hu%20and%20Jinqiao%20Wu%20and%20Fengtao%20Nan%20and%20Yingchun%20Fan&entry.1292438233=%20%20Existing%20semantic%20SLAM%20in%20dynamic%20environments%20mainly%20identify%20dynamic%0Aregions%20through%20object%20detection%20or%20semantic%20segmentation%20methods.%20However%2C%20in%0Acertain%20highly%20dynamic%20scenarios%2C%20the%20detection%20boxes%20or%20segmentation%20masks%0Acannot%20fully%20cover%20dynamic%20regions.%20Therefore%2C%20this%20paper%20proposes%20a%20robust%20and%0Aefficient%20GeneA-SLAM2%20system%20that%20leverages%20depth%20variance%20constraints%20to%0Ahandle%20dynamic%20scenes.%20Our%20method%20extracts%20dynamic%20pixels%20via%20depth%20variance%0Aand%20creates%20precise%20depth%20masks%20to%20guide%20the%20removal%20of%20dynamic%20objects.%0ASimultaneously%2C%20an%20autoencoder%20is%20used%20to%20reconstruct%20keypoints%2C%20improving%20the%0Agenetic%20resampling%20keypoint%20algorithm%20to%20obtain%20more%20uniformly%20distributed%0Akeypoints%20and%20enhance%20the%20accuracy%20of%20pose%20estimation.%20Our%20system%20was%20evaluated%0Aon%20multiple%20highly%20dynamic%20sequences.%20The%20results%20demonstrate%20that%20GeneA-SLAM2%0Amaintains%20high%20accuracy%20in%20dynamic%20scenes%20compared%20to%20current%20methods.%20Code%20is%0Aavailable%20at%3A%20https%3A//github.com/qingshufan/GeneA-SLAM2.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02736v1&entry.124074799=Read"},
{"title": "Large language models for crowd decision making based on prompt design\n  strategies using ChatGPT: models, analysis and challenges", "author": "David Herrera-Poyatos and Cristina Zuheros and Rosana Montes and Francisco Herrera", "abstract": "  Social Media and Internet have the potential to be exploited as a source of\nopinion to enrich Decision Making solutions. Crowd Decision Making (CDM) is a\nmethodology able to infer opinions and decisions from plain texts, such as\nreviews published in social media platforms, by means of Sentiment Analysis.\nCurrently, the emergence and potential of Large Language Models (LLMs) lead us\nto explore new scenarios of automatically understand written texts, also known\nas natural language processing. This paper analyzes the use of ChatGPT based on\nprompt design strategies to assist in CDM processes to extract opinions and\nmake decisions. We integrate ChatGPT in CDM processes as a flexible tool that\ninfer the opinions expressed in texts, providing numerical or linguistic\nevaluations where the decision making models are based on the prompt design\nstrategies. We include a multi-criteria decision making scenario with a\ncategory ontology for criteria. We also consider ChatGPT as an end-to-end CDM\nmodel able to provide a general opinion and score on the alternatives. We\nconduct empirical experiments on real data extracted from TripAdvisor, the\nTripR-2020Large dataset. The analysis of results show a promising branch for\ndeveloping quality decision making models using ChatGPT. Finally, we discuss\nthe challenges of consistency, sensitivity and explainability associated to the\nuse of LLMs in CDM processes, raising open questions for future studies.\n", "link": "http://arxiv.org/abs/2403.15587v2", "date": "2025-06-03", "relevancy": 2.4099, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4922}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4769}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20language%20models%20for%20crowd%20decision%20making%20based%20on%20prompt%20design%0A%20%20strategies%20using%20ChatGPT%3A%20models%2C%20analysis%20and%20challenges&body=Title%3A%20Large%20language%20models%20for%20crowd%20decision%20making%20based%20on%20prompt%20design%0A%20%20strategies%20using%20ChatGPT%3A%20models%2C%20analysis%20and%20challenges%0AAuthor%3A%20David%20Herrera-Poyatos%20and%20Cristina%20Zuheros%20and%20Rosana%20Montes%20and%20Francisco%20Herrera%0AAbstract%3A%20%20%20Social%20Media%20and%20Internet%20have%20the%20potential%20to%20be%20exploited%20as%20a%20source%20of%0Aopinion%20to%20enrich%20Decision%20Making%20solutions.%20Crowd%20Decision%20Making%20%28CDM%29%20is%20a%0Amethodology%20able%20to%20infer%20opinions%20and%20decisions%20from%20plain%20texts%2C%20such%20as%0Areviews%20published%20in%20social%20media%20platforms%2C%20by%20means%20of%20Sentiment%20Analysis.%0ACurrently%2C%20the%20emergence%20and%20potential%20of%20Large%20Language%20Models%20%28LLMs%29%20lead%20us%0Ato%20explore%20new%20scenarios%20of%20automatically%20understand%20written%20texts%2C%20also%20known%0Aas%20natural%20language%20processing.%20This%20paper%20analyzes%20the%20use%20of%20ChatGPT%20based%20on%0Aprompt%20design%20strategies%20to%20assist%20in%20CDM%20processes%20to%20extract%20opinions%20and%0Amake%20decisions.%20We%20integrate%20ChatGPT%20in%20CDM%20processes%20as%20a%20flexible%20tool%20that%0Ainfer%20the%20opinions%20expressed%20in%20texts%2C%20providing%20numerical%20or%20linguistic%0Aevaluations%20where%20the%20decision%20making%20models%20are%20based%20on%20the%20prompt%20design%0Astrategies.%20We%20include%20a%20multi-criteria%20decision%20making%20scenario%20with%20a%0Acategory%20ontology%20for%20criteria.%20We%20also%20consider%20ChatGPT%20as%20an%20end-to-end%20CDM%0Amodel%20able%20to%20provide%20a%20general%20opinion%20and%20score%20on%20the%20alternatives.%20We%0Aconduct%20empirical%20experiments%20on%20real%20data%20extracted%20from%20TripAdvisor%2C%20the%0ATripR-2020Large%20dataset.%20The%20analysis%20of%20results%20show%20a%20promising%20branch%20for%0Adeveloping%20quality%20decision%20making%20models%20using%20ChatGPT.%20Finally%2C%20we%20discuss%0Athe%20challenges%20of%20consistency%2C%20sensitivity%20and%20explainability%20associated%20to%20the%0Ause%20of%20LLMs%20in%20CDM%20processes%2C%20raising%20open%20questions%20for%20future%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15587v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520language%2520models%2520for%2520crowd%2520decision%2520making%2520based%2520on%2520prompt%2520design%250A%2520%2520strategies%2520using%2520ChatGPT%253A%2520models%252C%2520analysis%2520and%2520challenges%26entry.906535625%3DDavid%2520Herrera-Poyatos%2520and%2520Cristina%2520Zuheros%2520and%2520Rosana%2520Montes%2520and%2520Francisco%2520Herrera%26entry.1292438233%3D%2520%2520Social%2520Media%2520and%2520Internet%2520have%2520the%2520potential%2520to%2520be%2520exploited%2520as%2520a%2520source%2520of%250Aopinion%2520to%2520enrich%2520Decision%2520Making%2520solutions.%2520Crowd%2520Decision%2520Making%2520%2528CDM%2529%2520is%2520a%250Amethodology%2520able%2520to%2520infer%2520opinions%2520and%2520decisions%2520from%2520plain%2520texts%252C%2520such%2520as%250Areviews%2520published%2520in%2520social%2520media%2520platforms%252C%2520by%2520means%2520of%2520Sentiment%2520Analysis.%250ACurrently%252C%2520the%2520emergence%2520and%2520potential%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520lead%2520us%250Ato%2520explore%2520new%2520scenarios%2520of%2520automatically%2520understand%2520written%2520texts%252C%2520also%2520known%250Aas%2520natural%2520language%2520processing.%2520This%2520paper%2520analyzes%2520the%2520use%2520of%2520ChatGPT%2520based%2520on%250Aprompt%2520design%2520strategies%2520to%2520assist%2520in%2520CDM%2520processes%2520to%2520extract%2520opinions%2520and%250Amake%2520decisions.%2520We%2520integrate%2520ChatGPT%2520in%2520CDM%2520processes%2520as%2520a%2520flexible%2520tool%2520that%250Ainfer%2520the%2520opinions%2520expressed%2520in%2520texts%252C%2520providing%2520numerical%2520or%2520linguistic%250Aevaluations%2520where%2520the%2520decision%2520making%2520models%2520are%2520based%2520on%2520the%2520prompt%2520design%250Astrategies.%2520We%2520include%2520a%2520multi-criteria%2520decision%2520making%2520scenario%2520with%2520a%250Acategory%2520ontology%2520for%2520criteria.%2520We%2520also%2520consider%2520ChatGPT%2520as%2520an%2520end-to-end%2520CDM%250Amodel%2520able%2520to%2520provide%2520a%2520general%2520opinion%2520and%2520score%2520on%2520the%2520alternatives.%2520We%250Aconduct%2520empirical%2520experiments%2520on%2520real%2520data%2520extracted%2520from%2520TripAdvisor%252C%2520the%250ATripR-2020Large%2520dataset.%2520The%2520analysis%2520of%2520results%2520show%2520a%2520promising%2520branch%2520for%250Adeveloping%2520quality%2520decision%2520making%2520models%2520using%2520ChatGPT.%2520Finally%252C%2520we%2520discuss%250Athe%2520challenges%2520of%2520consistency%252C%2520sensitivity%2520and%2520explainability%2520associated%2520to%2520the%250Ause%2520of%2520LLMs%2520in%2520CDM%2520processes%252C%2520raising%2520open%2520questions%2520for%2520future%2520studies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15587v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20language%20models%20for%20crowd%20decision%20making%20based%20on%20prompt%20design%0A%20%20strategies%20using%20ChatGPT%3A%20models%2C%20analysis%20and%20challenges&entry.906535625=David%20Herrera-Poyatos%20and%20Cristina%20Zuheros%20and%20Rosana%20Montes%20and%20Francisco%20Herrera&entry.1292438233=%20%20Social%20Media%20and%20Internet%20have%20the%20potential%20to%20be%20exploited%20as%20a%20source%20of%0Aopinion%20to%20enrich%20Decision%20Making%20solutions.%20Crowd%20Decision%20Making%20%28CDM%29%20is%20a%0Amethodology%20able%20to%20infer%20opinions%20and%20decisions%20from%20plain%20texts%2C%20such%20as%0Areviews%20published%20in%20social%20media%20platforms%2C%20by%20means%20of%20Sentiment%20Analysis.%0ACurrently%2C%20the%20emergence%20and%20potential%20of%20Large%20Language%20Models%20%28LLMs%29%20lead%20us%0Ato%20explore%20new%20scenarios%20of%20automatically%20understand%20written%20texts%2C%20also%20known%0Aas%20natural%20language%20processing.%20This%20paper%20analyzes%20the%20use%20of%20ChatGPT%20based%20on%0Aprompt%20design%20strategies%20to%20assist%20in%20CDM%20processes%20to%20extract%20opinions%20and%0Amake%20decisions.%20We%20integrate%20ChatGPT%20in%20CDM%20processes%20as%20a%20flexible%20tool%20that%0Ainfer%20the%20opinions%20expressed%20in%20texts%2C%20providing%20numerical%20or%20linguistic%0Aevaluations%20where%20the%20decision%20making%20models%20are%20based%20on%20the%20prompt%20design%0Astrategies.%20We%20include%20a%20multi-criteria%20decision%20making%20scenario%20with%20a%0Acategory%20ontology%20for%20criteria.%20We%20also%20consider%20ChatGPT%20as%20an%20end-to-end%20CDM%0Amodel%20able%20to%20provide%20a%20general%20opinion%20and%20score%20on%20the%20alternatives.%20We%0Aconduct%20empirical%20experiments%20on%20real%20data%20extracted%20from%20TripAdvisor%2C%20the%0ATripR-2020Large%20dataset.%20The%20analysis%20of%20results%20show%20a%20promising%20branch%20for%0Adeveloping%20quality%20decision%20making%20models%20using%20ChatGPT.%20Finally%2C%20we%20discuss%0Athe%20challenges%20of%20consistency%2C%20sensitivity%20and%20explainability%20associated%20to%20the%0Ause%20of%20LLMs%20in%20CDM%20processes%2C%20raising%20open%20questions%20for%20future%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15587v2&entry.124074799=Read"},
{"title": "FreeScene: Mixed Graph Diffusion for 3D Scene Synthesis from Free\n  Prompts", "author": "Tongyuan Bai and Wangyuanfan Bai and Dong Chen and Tieru Wu and Manyi Li and Rui Ma", "abstract": "  Controllability plays a crucial role in the practical applications of 3D\nindoor scene synthesis. Existing works either allow rough language-based\ncontrol, that is convenient but lacks fine-grained scene customization, or\nemploy graph based control, which offers better controllability but demands\nconsiderable knowledge for the cumbersome graph design process. To address\nthese challenges, we present FreeScene, a user-friendly framework that enables\nboth convenient and effective control for indoor scene synthesis.Specifically,\nFreeScene supports free-form user inputs including text description and/or\nreference images, allowing users to express versatile design intentions. The\nuser inputs are adequately analyzed and integrated into a graph representation\nby a VLM-based Graph Designer. We then propose MG-DiT, a Mixed Graph Diffusion\nTransformer, which performs graph-aware denoising to enhance scene generation.\nOur MG-DiT not only excels at preserving graph structure but also offers broad\napplicability to various tasks, including, but not limited to, text-to-scene,\ngraph-to-scene, and rearrangement, all within a single model. Extensive\nexperiments demonstrate that FreeScene provides an efficient and user-friendly\nsolution that unifies text-based and graph based scene synthesis, outperforming\nstate-of-the-art methods in terms of both generation quality and\ncontrollability in a range of applications.\n", "link": "http://arxiv.org/abs/2506.02781v1", "date": "2025-06-03", "relevancy": 2.4054, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6116}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6046}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5898}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FreeScene%3A%20Mixed%20Graph%20Diffusion%20for%203D%20Scene%20Synthesis%20from%20Free%0A%20%20Prompts&body=Title%3A%20FreeScene%3A%20Mixed%20Graph%20Diffusion%20for%203D%20Scene%20Synthesis%20from%20Free%0A%20%20Prompts%0AAuthor%3A%20Tongyuan%20Bai%20and%20Wangyuanfan%20Bai%20and%20Dong%20Chen%20and%20Tieru%20Wu%20and%20Manyi%20Li%20and%20Rui%20Ma%0AAbstract%3A%20%20%20Controllability%20plays%20a%20crucial%20role%20in%20the%20practical%20applications%20of%203D%0Aindoor%20scene%20synthesis.%20Existing%20works%20either%20allow%20rough%20language-based%0Acontrol%2C%20that%20is%20convenient%20but%20lacks%20fine-grained%20scene%20customization%2C%20or%0Aemploy%20graph%20based%20control%2C%20which%20offers%20better%20controllability%20but%20demands%0Aconsiderable%20knowledge%20for%20the%20cumbersome%20graph%20design%20process.%20To%20address%0Athese%20challenges%2C%20we%20present%20FreeScene%2C%20a%20user-friendly%20framework%20that%20enables%0Aboth%20convenient%20and%20effective%20control%20for%20indoor%20scene%20synthesis.Specifically%2C%0AFreeScene%20supports%20free-form%20user%20inputs%20including%20text%20description%20and/or%0Areference%20images%2C%20allowing%20users%20to%20express%20versatile%20design%20intentions.%20The%0Auser%20inputs%20are%20adequately%20analyzed%20and%20integrated%20into%20a%20graph%20representation%0Aby%20a%20VLM-based%20Graph%20Designer.%20We%20then%20propose%20MG-DiT%2C%20a%20Mixed%20Graph%20Diffusion%0ATransformer%2C%20which%20performs%20graph-aware%20denoising%20to%20enhance%20scene%20generation.%0AOur%20MG-DiT%20not%20only%20excels%20at%20preserving%20graph%20structure%20but%20also%20offers%20broad%0Aapplicability%20to%20various%20tasks%2C%20including%2C%20but%20not%20limited%20to%2C%20text-to-scene%2C%0Agraph-to-scene%2C%20and%20rearrangement%2C%20all%20within%20a%20single%20model.%20Extensive%0Aexperiments%20demonstrate%20that%20FreeScene%20provides%20an%20efficient%20and%20user-friendly%0Asolution%20that%20unifies%20text-based%20and%20graph%20based%20scene%20synthesis%2C%20outperforming%0Astate-of-the-art%20methods%20in%20terms%20of%20both%20generation%20quality%20and%0Acontrollability%20in%20a%20range%20of%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02781v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreeScene%253A%2520Mixed%2520Graph%2520Diffusion%2520for%25203D%2520Scene%2520Synthesis%2520from%2520Free%250A%2520%2520Prompts%26entry.906535625%3DTongyuan%2520Bai%2520and%2520Wangyuanfan%2520Bai%2520and%2520Dong%2520Chen%2520and%2520Tieru%2520Wu%2520and%2520Manyi%2520Li%2520and%2520Rui%2520Ma%26entry.1292438233%3D%2520%2520Controllability%2520plays%2520a%2520crucial%2520role%2520in%2520the%2520practical%2520applications%2520of%25203D%250Aindoor%2520scene%2520synthesis.%2520Existing%2520works%2520either%2520allow%2520rough%2520language-based%250Acontrol%252C%2520that%2520is%2520convenient%2520but%2520lacks%2520fine-grained%2520scene%2520customization%252C%2520or%250Aemploy%2520graph%2520based%2520control%252C%2520which%2520offers%2520better%2520controllability%2520but%2520demands%250Aconsiderable%2520knowledge%2520for%2520the%2520cumbersome%2520graph%2520design%2520process.%2520To%2520address%250Athese%2520challenges%252C%2520we%2520present%2520FreeScene%252C%2520a%2520user-friendly%2520framework%2520that%2520enables%250Aboth%2520convenient%2520and%2520effective%2520control%2520for%2520indoor%2520scene%2520synthesis.Specifically%252C%250AFreeScene%2520supports%2520free-form%2520user%2520inputs%2520including%2520text%2520description%2520and/or%250Areference%2520images%252C%2520allowing%2520users%2520to%2520express%2520versatile%2520design%2520intentions.%2520The%250Auser%2520inputs%2520are%2520adequately%2520analyzed%2520and%2520integrated%2520into%2520a%2520graph%2520representation%250Aby%2520a%2520VLM-based%2520Graph%2520Designer.%2520We%2520then%2520propose%2520MG-DiT%252C%2520a%2520Mixed%2520Graph%2520Diffusion%250ATransformer%252C%2520which%2520performs%2520graph-aware%2520denoising%2520to%2520enhance%2520scene%2520generation.%250AOur%2520MG-DiT%2520not%2520only%2520excels%2520at%2520preserving%2520graph%2520structure%2520but%2520also%2520offers%2520broad%250Aapplicability%2520to%2520various%2520tasks%252C%2520including%252C%2520but%2520not%2520limited%2520to%252C%2520text-to-scene%252C%250Agraph-to-scene%252C%2520and%2520rearrangement%252C%2520all%2520within%2520a%2520single%2520model.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520FreeScene%2520provides%2520an%2520efficient%2520and%2520user-friendly%250Asolution%2520that%2520unifies%2520text-based%2520and%2520graph%2520based%2520scene%2520synthesis%252C%2520outperforming%250Astate-of-the-art%2520methods%2520in%2520terms%2520of%2520both%2520generation%2520quality%2520and%250Acontrollability%2520in%2520a%2520range%2520of%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02781v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreeScene%3A%20Mixed%20Graph%20Diffusion%20for%203D%20Scene%20Synthesis%20from%20Free%0A%20%20Prompts&entry.906535625=Tongyuan%20Bai%20and%20Wangyuanfan%20Bai%20and%20Dong%20Chen%20and%20Tieru%20Wu%20and%20Manyi%20Li%20and%20Rui%20Ma&entry.1292438233=%20%20Controllability%20plays%20a%20crucial%20role%20in%20the%20practical%20applications%20of%203D%0Aindoor%20scene%20synthesis.%20Existing%20works%20either%20allow%20rough%20language-based%0Acontrol%2C%20that%20is%20convenient%20but%20lacks%20fine-grained%20scene%20customization%2C%20or%0Aemploy%20graph%20based%20control%2C%20which%20offers%20better%20controllability%20but%20demands%0Aconsiderable%20knowledge%20for%20the%20cumbersome%20graph%20design%20process.%20To%20address%0Athese%20challenges%2C%20we%20present%20FreeScene%2C%20a%20user-friendly%20framework%20that%20enables%0Aboth%20convenient%20and%20effective%20control%20for%20indoor%20scene%20synthesis.Specifically%2C%0AFreeScene%20supports%20free-form%20user%20inputs%20including%20text%20description%20and/or%0Areference%20images%2C%20allowing%20users%20to%20express%20versatile%20design%20intentions.%20The%0Auser%20inputs%20are%20adequately%20analyzed%20and%20integrated%20into%20a%20graph%20representation%0Aby%20a%20VLM-based%20Graph%20Designer.%20We%20then%20propose%20MG-DiT%2C%20a%20Mixed%20Graph%20Diffusion%0ATransformer%2C%20which%20performs%20graph-aware%20denoising%20to%20enhance%20scene%20generation.%0AOur%20MG-DiT%20not%20only%20excels%20at%20preserving%20graph%20structure%20but%20also%20offers%20broad%0Aapplicability%20to%20various%20tasks%2C%20including%2C%20but%20not%20limited%20to%2C%20text-to-scene%2C%0Agraph-to-scene%2C%20and%20rearrangement%2C%20all%20within%20a%20single%20model.%20Extensive%0Aexperiments%20demonstrate%20that%20FreeScene%20provides%20an%20efficient%20and%20user-friendly%0Asolution%20that%20unifies%20text-based%20and%20graph%20based%20scene%20synthesis%2C%20outperforming%0Astate-of-the-art%20methods%20in%20terms%20of%20both%20generation%20quality%20and%0Acontrollability%20in%20a%20range%20of%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02781v1&entry.124074799=Read"},
{"title": "Gaussian mixture models as a proxy for interacting language models", "author": "Edward L. Wang and Tianyu Wang and Avanti Athreya and Vince Lyzinski and Carey E. Priebe", "abstract": "  Large language models (LLMs) are a powerful tool with the ability to match\nhuman capabilities and behavior in many settings. Retrieval-augmented\ngeneration (RAG) further allows LLMs to generate diverse output depending on\nthe contents of their RAG database. This motivates their use in the social\nsciences to study human behavior between individuals when large-scale\nexperiments are infeasible. However, LLMs depend on complex, computationally\nexpensive algorithms. In this paper, we introduce interacting Gaussian mixture\nmodels (GMMs) as an alternative to similar frameworks using LLMs. We compare a\nsimplified model of GMMs to select experimental simulations of LLMs whose\nupdating and response depend on feedback from other LLMs. We find that\ninteracting GMMs capture important features of the dynamics in interacting\nLLMs, and we investigate key similarities and differences between interacting\nLLMs and GMMs. We conclude by discussing the benefits of Gaussian mixture\nmodels, potential modifications, and future research directions.\n", "link": "http://arxiv.org/abs/2506.00077v2", "date": "2025-06-03", "relevancy": 2.3933, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4928}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4734}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4697}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20mixture%20models%20as%20a%20proxy%20for%20interacting%20language%20models&body=Title%3A%20Gaussian%20mixture%20models%20as%20a%20proxy%20for%20interacting%20language%20models%0AAuthor%3A%20Edward%20L.%20Wang%20and%20Tianyu%20Wang%20and%20Avanti%20Athreya%20and%20Vince%20Lyzinski%20and%20Carey%20E.%20Priebe%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20are%20a%20powerful%20tool%20with%20the%20ability%20to%20match%0Ahuman%20capabilities%20and%20behavior%20in%20many%20settings.%20Retrieval-augmented%0Ageneration%20%28RAG%29%20further%20allows%20LLMs%20to%20generate%20diverse%20output%20depending%20on%0Athe%20contents%20of%20their%20RAG%20database.%20This%20motivates%20their%20use%20in%20the%20social%0Asciences%20to%20study%20human%20behavior%20between%20individuals%20when%20large-scale%0Aexperiments%20are%20infeasible.%20However%2C%20LLMs%20depend%20on%20complex%2C%20computationally%0Aexpensive%20algorithms.%20In%20this%20paper%2C%20we%20introduce%20interacting%20Gaussian%20mixture%0Amodels%20%28GMMs%29%20as%20an%20alternative%20to%20similar%20frameworks%20using%20LLMs.%20We%20compare%20a%0Asimplified%20model%20of%20GMMs%20to%20select%20experimental%20simulations%20of%20LLMs%20whose%0Aupdating%20and%20response%20depend%20on%20feedback%20from%20other%20LLMs.%20We%20find%20that%0Ainteracting%20GMMs%20capture%20important%20features%20of%20the%20dynamics%20in%20interacting%0ALLMs%2C%20and%20we%20investigate%20key%20similarities%20and%20differences%20between%20interacting%0ALLMs%20and%20GMMs.%20We%20conclude%20by%20discussing%20the%20benefits%20of%20Gaussian%20mixture%0Amodels%2C%20potential%20modifications%2C%20and%20future%20research%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.00077v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520mixture%2520models%2520as%2520a%2520proxy%2520for%2520interacting%2520language%2520models%26entry.906535625%3DEdward%2520L.%2520Wang%2520and%2520Tianyu%2520Wang%2520and%2520Avanti%2520Athreya%2520and%2520Vince%2520Lyzinski%2520and%2520Carey%2520E.%2520Priebe%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520are%2520a%2520powerful%2520tool%2520with%2520the%2520ability%2520to%2520match%250Ahuman%2520capabilities%2520and%2520behavior%2520in%2520many%2520settings.%2520Retrieval-augmented%250Ageneration%2520%2528RAG%2529%2520further%2520allows%2520LLMs%2520to%2520generate%2520diverse%2520output%2520depending%2520on%250Athe%2520contents%2520of%2520their%2520RAG%2520database.%2520This%2520motivates%2520their%2520use%2520in%2520the%2520social%250Asciences%2520to%2520study%2520human%2520behavior%2520between%2520individuals%2520when%2520large-scale%250Aexperiments%2520are%2520infeasible.%2520However%252C%2520LLMs%2520depend%2520on%2520complex%252C%2520computationally%250Aexpensive%2520algorithms.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520interacting%2520Gaussian%2520mixture%250Amodels%2520%2528GMMs%2529%2520as%2520an%2520alternative%2520to%2520similar%2520frameworks%2520using%2520LLMs.%2520We%2520compare%2520a%250Asimplified%2520model%2520of%2520GMMs%2520to%2520select%2520experimental%2520simulations%2520of%2520LLMs%2520whose%250Aupdating%2520and%2520response%2520depend%2520on%2520feedback%2520from%2520other%2520LLMs.%2520We%2520find%2520that%250Ainteracting%2520GMMs%2520capture%2520important%2520features%2520of%2520the%2520dynamics%2520in%2520interacting%250ALLMs%252C%2520and%2520we%2520investigate%2520key%2520similarities%2520and%2520differences%2520between%2520interacting%250ALLMs%2520and%2520GMMs.%2520We%2520conclude%2520by%2520discussing%2520the%2520benefits%2520of%2520Gaussian%2520mixture%250Amodels%252C%2520potential%2520modifications%252C%2520and%2520future%2520research%2520directions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.00077v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20mixture%20models%20as%20a%20proxy%20for%20interacting%20language%20models&entry.906535625=Edward%20L.%20Wang%20and%20Tianyu%20Wang%20and%20Avanti%20Athreya%20and%20Vince%20Lyzinski%20and%20Carey%20E.%20Priebe&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20are%20a%20powerful%20tool%20with%20the%20ability%20to%20match%0Ahuman%20capabilities%20and%20behavior%20in%20many%20settings.%20Retrieval-augmented%0Ageneration%20%28RAG%29%20further%20allows%20LLMs%20to%20generate%20diverse%20output%20depending%20on%0Athe%20contents%20of%20their%20RAG%20database.%20This%20motivates%20their%20use%20in%20the%20social%0Asciences%20to%20study%20human%20behavior%20between%20individuals%20when%20large-scale%0Aexperiments%20are%20infeasible.%20However%2C%20LLMs%20depend%20on%20complex%2C%20computationally%0Aexpensive%20algorithms.%20In%20this%20paper%2C%20we%20introduce%20interacting%20Gaussian%20mixture%0Amodels%20%28GMMs%29%20as%20an%20alternative%20to%20similar%20frameworks%20using%20LLMs.%20We%20compare%20a%0Asimplified%20model%20of%20GMMs%20to%20select%20experimental%20simulations%20of%20LLMs%20whose%0Aupdating%20and%20response%20depend%20on%20feedback%20from%20other%20LLMs.%20We%20find%20that%0Ainteracting%20GMMs%20capture%20important%20features%20of%20the%20dynamics%20in%20interacting%0ALLMs%2C%20and%20we%20investigate%20key%20similarities%20and%20differences%20between%20interacting%0ALLMs%20and%20GMMs.%20We%20conclude%20by%20discussing%20the%20benefits%20of%20Gaussian%20mixture%0Amodels%2C%20potential%20modifications%2C%20and%20future%20research%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.00077v2&entry.124074799=Read"},
{"title": "FocalPO: Enhancing Preference Optimizing by Focusing on Correct\n  Preference Rankings", "author": "Tong Liu and Xiao Yu and Wenxuan Zhou and Jindong Gu and Volker Tresp", "abstract": "  Efficient preference optimization algorithms such as Direct Preference\nOptimization (DPO) have become a popular approach in aligning large language\nmodels (LLMs) with human preferences. These algorithms implicitly treat the LLM\nas a reward model, and focus on training it to correct misranked preference\npairs. However, recent work~\\citep{chen2024preference} empirically finds that\nDPO training \\textit{rarely improves these misranked preference pairs}, despite\nits gradient emphasizing on these cases. We introduce FocalPO, a DPO variant\nthat instead \\textit{down-weighs} misranked preference pairs and prioritizes\nenhancing the model's understanding of pairs that it can already rank\ncorrectly. Inspired by Focal Loss used in vision tasks, FocalPO achieves this\nby adding a modulating factor to dynamically scale DPO loss. Our experiment\ndemonstrates that FocalPO surpasses DPO and its variants on popular benchmarks\nlike Alpaca Eval 2.0 using Mistral-Base-7B and Llama-3-Instruct-8B, with the\nintroduced hyperparameter fixed. Additionally, we empirically reveals how\nFocalPO affects training on correct and incorrect sample groups, further\nunderscoring its effectiveness.\n", "link": "http://arxiv.org/abs/2501.06645v2", "date": "2025-06-03", "relevancy": 2.3882, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4842}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4842}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FocalPO%3A%20Enhancing%20Preference%20Optimizing%20by%20Focusing%20on%20Correct%0A%20%20Preference%20Rankings&body=Title%3A%20FocalPO%3A%20Enhancing%20Preference%20Optimizing%20by%20Focusing%20on%20Correct%0A%20%20Preference%20Rankings%0AAuthor%3A%20Tong%20Liu%20and%20Xiao%20Yu%20and%20Wenxuan%20Zhou%20and%20Jindong%20Gu%20and%20Volker%20Tresp%0AAbstract%3A%20%20%20Efficient%20preference%20optimization%20algorithms%20such%20as%20Direct%20Preference%0AOptimization%20%28DPO%29%20have%20become%20a%20popular%20approach%20in%20aligning%20large%20language%0Amodels%20%28LLMs%29%20with%20human%20preferences.%20These%20algorithms%20implicitly%20treat%20the%20LLM%0Aas%20a%20reward%20model%2C%20and%20focus%20on%20training%20it%20to%20correct%20misranked%20preference%0Apairs.%20However%2C%20recent%20work~%5Ccitep%7Bchen2024preference%7D%20empirically%20finds%20that%0ADPO%20training%20%5Ctextit%7Brarely%20improves%20these%20misranked%20preference%20pairs%7D%2C%20despite%0Aits%20gradient%20emphasizing%20on%20these%20cases.%20We%20introduce%20FocalPO%2C%20a%20DPO%20variant%0Athat%20instead%20%5Ctextit%7Bdown-weighs%7D%20misranked%20preference%20pairs%20and%20prioritizes%0Aenhancing%20the%20model%27s%20understanding%20of%20pairs%20that%20it%20can%20already%20rank%0Acorrectly.%20Inspired%20by%20Focal%20Loss%20used%20in%20vision%20tasks%2C%20FocalPO%20achieves%20this%0Aby%20adding%20a%20modulating%20factor%20to%20dynamically%20scale%20DPO%20loss.%20Our%20experiment%0Ademonstrates%20that%20FocalPO%20surpasses%20DPO%20and%20its%20variants%20on%20popular%20benchmarks%0Alike%20Alpaca%20Eval%202.0%20using%20Mistral-Base-7B%20and%20Llama-3-Instruct-8B%2C%20with%20the%0Aintroduced%20hyperparameter%20fixed.%20Additionally%2C%20we%20empirically%20reveals%20how%0AFocalPO%20affects%20training%20on%20correct%20and%20incorrect%20sample%20groups%2C%20further%0Aunderscoring%20its%20effectiveness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.06645v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFocalPO%253A%2520Enhancing%2520Preference%2520Optimizing%2520by%2520Focusing%2520on%2520Correct%250A%2520%2520Preference%2520Rankings%26entry.906535625%3DTong%2520Liu%2520and%2520Xiao%2520Yu%2520and%2520Wenxuan%2520Zhou%2520and%2520Jindong%2520Gu%2520and%2520Volker%2520Tresp%26entry.1292438233%3D%2520%2520Efficient%2520preference%2520optimization%2520algorithms%2520such%2520as%2520Direct%2520Preference%250AOptimization%2520%2528DPO%2529%2520have%2520become%2520a%2520popular%2520approach%2520in%2520aligning%2520large%2520language%250Amodels%2520%2528LLMs%2529%2520with%2520human%2520preferences.%2520These%2520algorithms%2520implicitly%2520treat%2520the%2520LLM%250Aas%2520a%2520reward%2520model%252C%2520and%2520focus%2520on%2520training%2520it%2520to%2520correct%2520misranked%2520preference%250Apairs.%2520However%252C%2520recent%2520work~%255Ccitep%257Bchen2024preference%257D%2520empirically%2520finds%2520that%250ADPO%2520training%2520%255Ctextit%257Brarely%2520improves%2520these%2520misranked%2520preference%2520pairs%257D%252C%2520despite%250Aits%2520gradient%2520emphasizing%2520on%2520these%2520cases.%2520We%2520introduce%2520FocalPO%252C%2520a%2520DPO%2520variant%250Athat%2520instead%2520%255Ctextit%257Bdown-weighs%257D%2520misranked%2520preference%2520pairs%2520and%2520prioritizes%250Aenhancing%2520the%2520model%2527s%2520understanding%2520of%2520pairs%2520that%2520it%2520can%2520already%2520rank%250Acorrectly.%2520Inspired%2520by%2520Focal%2520Loss%2520used%2520in%2520vision%2520tasks%252C%2520FocalPO%2520achieves%2520this%250Aby%2520adding%2520a%2520modulating%2520factor%2520to%2520dynamically%2520scale%2520DPO%2520loss.%2520Our%2520experiment%250Ademonstrates%2520that%2520FocalPO%2520surpasses%2520DPO%2520and%2520its%2520variants%2520on%2520popular%2520benchmarks%250Alike%2520Alpaca%2520Eval%25202.0%2520using%2520Mistral-Base-7B%2520and%2520Llama-3-Instruct-8B%252C%2520with%2520the%250Aintroduced%2520hyperparameter%2520fixed.%2520Additionally%252C%2520we%2520empirically%2520reveals%2520how%250AFocalPO%2520affects%2520training%2520on%2520correct%2520and%2520incorrect%2520sample%2520groups%252C%2520further%250Aunderscoring%2520its%2520effectiveness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06645v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FocalPO%3A%20Enhancing%20Preference%20Optimizing%20by%20Focusing%20on%20Correct%0A%20%20Preference%20Rankings&entry.906535625=Tong%20Liu%20and%20Xiao%20Yu%20and%20Wenxuan%20Zhou%20and%20Jindong%20Gu%20and%20Volker%20Tresp&entry.1292438233=%20%20Efficient%20preference%20optimization%20algorithms%20such%20as%20Direct%20Preference%0AOptimization%20%28DPO%29%20have%20become%20a%20popular%20approach%20in%20aligning%20large%20language%0Amodels%20%28LLMs%29%20with%20human%20preferences.%20These%20algorithms%20implicitly%20treat%20the%20LLM%0Aas%20a%20reward%20model%2C%20and%20focus%20on%20training%20it%20to%20correct%20misranked%20preference%0Apairs.%20However%2C%20recent%20work~%5Ccitep%7Bchen2024preference%7D%20empirically%20finds%20that%0ADPO%20training%20%5Ctextit%7Brarely%20improves%20these%20misranked%20preference%20pairs%7D%2C%20despite%0Aits%20gradient%20emphasizing%20on%20these%20cases.%20We%20introduce%20FocalPO%2C%20a%20DPO%20variant%0Athat%20instead%20%5Ctextit%7Bdown-weighs%7D%20misranked%20preference%20pairs%20and%20prioritizes%0Aenhancing%20the%20model%27s%20understanding%20of%20pairs%20that%20it%20can%20already%20rank%0Acorrectly.%20Inspired%20by%20Focal%20Loss%20used%20in%20vision%20tasks%2C%20FocalPO%20achieves%20this%0Aby%20adding%20a%20modulating%20factor%20to%20dynamically%20scale%20DPO%20loss.%20Our%20experiment%0Ademonstrates%20that%20FocalPO%20surpasses%20DPO%20and%20its%20variants%20on%20popular%20benchmarks%0Alike%20Alpaca%20Eval%202.0%20using%20Mistral-Base-7B%20and%20Llama-3-Instruct-8B%2C%20with%20the%0Aintroduced%20hyperparameter%20fixed.%20Additionally%2C%20we%20empirically%20reveals%20how%0AFocalPO%20affects%20training%20on%20correct%20and%20incorrect%20sample%20groups%2C%20further%0Aunderscoring%20its%20effectiveness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.06645v2&entry.124074799=Read"},
{"title": "Sheaves Reloaded: A Directional Awakening", "author": "Stefano Fiorini and Hakan Aktas and Iulia Duta and Stefano Coniglio and Pietro Morerio and Alessio Del Bue and Pietro Li\u00f2", "abstract": "  Sheaf Neural Networks (SNNs) represent a powerful generalization of Graph\nNeural Networks (GNNs) that significantly improve our ability to model complex\nrelational data. While directionality has been shown to substantially boost\nperformance in graph learning tasks and is key to many real-world applications,\nexisting SNNs fall short in representing it. To address this limitation, we\nintroduce the Directed Cellular Sheaf, a special type of cellular sheaf\ndesigned to explicitly account for edge orientation. Building on this\nstructure, we define a new sheaf Laplacian, the Directed Sheaf Laplacian, which\ncaptures both the graph's topology and its directional information. This\noperator serves as the backbone of the Directed Sheaf Neural Network (DSNN),\nthe first SNN model to embed a directional bias into its architecture.\nExtensive experiments on nine real-world benchmarks show that DSNN consistently\noutperforms baseline methods.\n", "link": "http://arxiv.org/abs/2506.02842v1", "date": "2025-06-03", "relevancy": 2.3823, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5128}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4677}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sheaves%20Reloaded%3A%20A%20Directional%20Awakening&body=Title%3A%20Sheaves%20Reloaded%3A%20A%20Directional%20Awakening%0AAuthor%3A%20Stefano%20Fiorini%20and%20Hakan%20Aktas%20and%20Iulia%20Duta%20and%20Stefano%20Coniglio%20and%20Pietro%20Morerio%20and%20Alessio%20Del%20Bue%20and%20Pietro%20Li%C3%B2%0AAbstract%3A%20%20%20Sheaf%20Neural%20Networks%20%28SNNs%29%20represent%20a%20powerful%20generalization%20of%20Graph%0ANeural%20Networks%20%28GNNs%29%20that%20significantly%20improve%20our%20ability%20to%20model%20complex%0Arelational%20data.%20While%20directionality%20has%20been%20shown%20to%20substantially%20boost%0Aperformance%20in%20graph%20learning%20tasks%20and%20is%20key%20to%20many%20real-world%20applications%2C%0Aexisting%20SNNs%20fall%20short%20in%20representing%20it.%20To%20address%20this%20limitation%2C%20we%0Aintroduce%20the%20Directed%20Cellular%20Sheaf%2C%20a%20special%20type%20of%20cellular%20sheaf%0Adesigned%20to%20explicitly%20account%20for%20edge%20orientation.%20Building%20on%20this%0Astructure%2C%20we%20define%20a%20new%20sheaf%20Laplacian%2C%20the%20Directed%20Sheaf%20Laplacian%2C%20which%0Acaptures%20both%20the%20graph%27s%20topology%20and%20its%20directional%20information.%20This%0Aoperator%20serves%20as%20the%20backbone%20of%20the%20Directed%20Sheaf%20Neural%20Network%20%28DSNN%29%2C%0Athe%20first%20SNN%20model%20to%20embed%20a%20directional%20bias%20into%20its%20architecture.%0AExtensive%20experiments%20on%20nine%20real-world%20benchmarks%20show%20that%20DSNN%20consistently%0Aoutperforms%20baseline%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02842v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSheaves%2520Reloaded%253A%2520A%2520Directional%2520Awakening%26entry.906535625%3DStefano%2520Fiorini%2520and%2520Hakan%2520Aktas%2520and%2520Iulia%2520Duta%2520and%2520Stefano%2520Coniglio%2520and%2520Pietro%2520Morerio%2520and%2520Alessio%2520Del%2520Bue%2520and%2520Pietro%2520Li%25C3%25B2%26entry.1292438233%3D%2520%2520Sheaf%2520Neural%2520Networks%2520%2528SNNs%2529%2520represent%2520a%2520powerful%2520generalization%2520of%2520Graph%250ANeural%2520Networks%2520%2528GNNs%2529%2520that%2520significantly%2520improve%2520our%2520ability%2520to%2520model%2520complex%250Arelational%2520data.%2520While%2520directionality%2520has%2520been%2520shown%2520to%2520substantially%2520boost%250Aperformance%2520in%2520graph%2520learning%2520tasks%2520and%2520is%2520key%2520to%2520many%2520real-world%2520applications%252C%250Aexisting%2520SNNs%2520fall%2520short%2520in%2520representing%2520it.%2520To%2520address%2520this%2520limitation%252C%2520we%250Aintroduce%2520the%2520Directed%2520Cellular%2520Sheaf%252C%2520a%2520special%2520type%2520of%2520cellular%2520sheaf%250Adesigned%2520to%2520explicitly%2520account%2520for%2520edge%2520orientation.%2520Building%2520on%2520this%250Astructure%252C%2520we%2520define%2520a%2520new%2520sheaf%2520Laplacian%252C%2520the%2520Directed%2520Sheaf%2520Laplacian%252C%2520which%250Acaptures%2520both%2520the%2520graph%2527s%2520topology%2520and%2520its%2520directional%2520information.%2520This%250Aoperator%2520serves%2520as%2520the%2520backbone%2520of%2520the%2520Directed%2520Sheaf%2520Neural%2520Network%2520%2528DSNN%2529%252C%250Athe%2520first%2520SNN%2520model%2520to%2520embed%2520a%2520directional%2520bias%2520into%2520its%2520architecture.%250AExtensive%2520experiments%2520on%2520nine%2520real-world%2520benchmarks%2520show%2520that%2520DSNN%2520consistently%250Aoutperforms%2520baseline%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02842v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sheaves%20Reloaded%3A%20A%20Directional%20Awakening&entry.906535625=Stefano%20Fiorini%20and%20Hakan%20Aktas%20and%20Iulia%20Duta%20and%20Stefano%20Coniglio%20and%20Pietro%20Morerio%20and%20Alessio%20Del%20Bue%20and%20Pietro%20Li%C3%B2&entry.1292438233=%20%20Sheaf%20Neural%20Networks%20%28SNNs%29%20represent%20a%20powerful%20generalization%20of%20Graph%0ANeural%20Networks%20%28GNNs%29%20that%20significantly%20improve%20our%20ability%20to%20model%20complex%0Arelational%20data.%20While%20directionality%20has%20been%20shown%20to%20substantially%20boost%0Aperformance%20in%20graph%20learning%20tasks%20and%20is%20key%20to%20many%20real-world%20applications%2C%0Aexisting%20SNNs%20fall%20short%20in%20representing%20it.%20To%20address%20this%20limitation%2C%20we%0Aintroduce%20the%20Directed%20Cellular%20Sheaf%2C%20a%20special%20type%20of%20cellular%20sheaf%0Adesigned%20to%20explicitly%20account%20for%20edge%20orientation.%20Building%20on%20this%0Astructure%2C%20we%20define%20a%20new%20sheaf%20Laplacian%2C%20the%20Directed%20Sheaf%20Laplacian%2C%20which%0Acaptures%20both%20the%20graph%27s%20topology%20and%20its%20directional%20information.%20This%0Aoperator%20serves%20as%20the%20backbone%20of%20the%20Directed%20Sheaf%20Neural%20Network%20%28DSNN%29%2C%0Athe%20first%20SNN%20model%20to%20embed%20a%20directional%20bias%20into%20its%20architecture.%0AExtensive%20experiments%20on%20nine%20real-world%20benchmarks%20show%20that%20DSNN%20consistently%0Aoutperforms%20baseline%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02842v1&entry.124074799=Read"},
{"title": "Knowledge Graph Completion by Intermediate Variables Regularization", "author": "Changyi Xiao and Yixin Cao", "abstract": "  Knowledge graph completion (KGC) can be framed as a 3-order binary tensor\ncompletion task. Tensor decomposition-based (TDB) models have demonstrated\nstrong performance in KGC. In this paper, we provide a summary of existing TDB\nmodels and derive a general form for them, serving as a foundation for further\nexploration of TDB models. Despite the expressiveness of TDB models, they are\nprone to overfitting. Existing regularization methods merely minimize the norms\nof embeddings to regularize the model, leading to suboptimal performance.\nTherefore, we propose a novel regularization method for TDB models that\naddresses this limitation. The regularization is applicable to most TDB models\nand ensures tractable computation. Our method minimizes the norms of\nintermediate variables involved in the different ways of computing the\npredicted tensor. To support our regularization method, we provide a\ntheoretical analysis that proves its effect in promoting low trace norm of the\npredicted tensor to reduce overfitting. Finally, we conduct experiments to\nverify the effectiveness of our regularization technique as well as the\nreliability of our theoretical analysis. The code is available at\nhttps://github.com/changyi7231/IVR.\n", "link": "http://arxiv.org/abs/2506.02749v1", "date": "2025-06-03", "relevancy": 2.3803, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4972}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4671}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4639}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Graph%20Completion%20by%20Intermediate%20Variables%20Regularization&body=Title%3A%20Knowledge%20Graph%20Completion%20by%20Intermediate%20Variables%20Regularization%0AAuthor%3A%20Changyi%20Xiao%20and%20Yixin%20Cao%0AAbstract%3A%20%20%20Knowledge%20graph%20completion%20%28KGC%29%20can%20be%20framed%20as%20a%203-order%20binary%20tensor%0Acompletion%20task.%20Tensor%20decomposition-based%20%28TDB%29%20models%20have%20demonstrated%0Astrong%20performance%20in%20KGC.%20In%20this%20paper%2C%20we%20provide%20a%20summary%20of%20existing%20TDB%0Amodels%20and%20derive%20a%20general%20form%20for%20them%2C%20serving%20as%20a%20foundation%20for%20further%0Aexploration%20of%20TDB%20models.%20Despite%20the%20expressiveness%20of%20TDB%20models%2C%20they%20are%0Aprone%20to%20overfitting.%20Existing%20regularization%20methods%20merely%20minimize%20the%20norms%0Aof%20embeddings%20to%20regularize%20the%20model%2C%20leading%20to%20suboptimal%20performance.%0ATherefore%2C%20we%20propose%20a%20novel%20regularization%20method%20for%20TDB%20models%20that%0Aaddresses%20this%20limitation.%20The%20regularization%20is%20applicable%20to%20most%20TDB%20models%0Aand%20ensures%20tractable%20computation.%20Our%20method%20minimizes%20the%20norms%20of%0Aintermediate%20variables%20involved%20in%20the%20different%20ways%20of%20computing%20the%0Apredicted%20tensor.%20To%20support%20our%20regularization%20method%2C%20we%20provide%20a%0Atheoretical%20analysis%20that%20proves%20its%20effect%20in%20promoting%20low%20trace%20norm%20of%20the%0Apredicted%20tensor%20to%20reduce%20overfitting.%20Finally%2C%20we%20conduct%20experiments%20to%0Averify%20the%20effectiveness%20of%20our%20regularization%20technique%20as%20well%20as%20the%0Areliability%20of%20our%20theoretical%20analysis.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/changyi7231/IVR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02749v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520Graph%2520Completion%2520by%2520Intermediate%2520Variables%2520Regularization%26entry.906535625%3DChangyi%2520Xiao%2520and%2520Yixin%2520Cao%26entry.1292438233%3D%2520%2520Knowledge%2520graph%2520completion%2520%2528KGC%2529%2520can%2520be%2520framed%2520as%2520a%25203-order%2520binary%2520tensor%250Acompletion%2520task.%2520Tensor%2520decomposition-based%2520%2528TDB%2529%2520models%2520have%2520demonstrated%250Astrong%2520performance%2520in%2520KGC.%2520In%2520this%2520paper%252C%2520we%2520provide%2520a%2520summary%2520of%2520existing%2520TDB%250Amodels%2520and%2520derive%2520a%2520general%2520form%2520for%2520them%252C%2520serving%2520as%2520a%2520foundation%2520for%2520further%250Aexploration%2520of%2520TDB%2520models.%2520Despite%2520the%2520expressiveness%2520of%2520TDB%2520models%252C%2520they%2520are%250Aprone%2520to%2520overfitting.%2520Existing%2520regularization%2520methods%2520merely%2520minimize%2520the%2520norms%250Aof%2520embeddings%2520to%2520regularize%2520the%2520model%252C%2520leading%2520to%2520suboptimal%2520performance.%250ATherefore%252C%2520we%2520propose%2520a%2520novel%2520regularization%2520method%2520for%2520TDB%2520models%2520that%250Aaddresses%2520this%2520limitation.%2520The%2520regularization%2520is%2520applicable%2520to%2520most%2520TDB%2520models%250Aand%2520ensures%2520tractable%2520computation.%2520Our%2520method%2520minimizes%2520the%2520norms%2520of%250Aintermediate%2520variables%2520involved%2520in%2520the%2520different%2520ways%2520of%2520computing%2520the%250Apredicted%2520tensor.%2520To%2520support%2520our%2520regularization%2520method%252C%2520we%2520provide%2520a%250Atheoretical%2520analysis%2520that%2520proves%2520its%2520effect%2520in%2520promoting%2520low%2520trace%2520norm%2520of%2520the%250Apredicted%2520tensor%2520to%2520reduce%2520overfitting.%2520Finally%252C%2520we%2520conduct%2520experiments%2520to%250Averify%2520the%2520effectiveness%2520of%2520our%2520regularization%2520technique%2520as%2520well%2520as%2520the%250Areliability%2520of%2520our%2520theoretical%2520analysis.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/changyi7231/IVR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02749v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Graph%20Completion%20by%20Intermediate%20Variables%20Regularization&entry.906535625=Changyi%20Xiao%20and%20Yixin%20Cao&entry.1292438233=%20%20Knowledge%20graph%20completion%20%28KGC%29%20can%20be%20framed%20as%20a%203-order%20binary%20tensor%0Acompletion%20task.%20Tensor%20decomposition-based%20%28TDB%29%20models%20have%20demonstrated%0Astrong%20performance%20in%20KGC.%20In%20this%20paper%2C%20we%20provide%20a%20summary%20of%20existing%20TDB%0Amodels%20and%20derive%20a%20general%20form%20for%20them%2C%20serving%20as%20a%20foundation%20for%20further%0Aexploration%20of%20TDB%20models.%20Despite%20the%20expressiveness%20of%20TDB%20models%2C%20they%20are%0Aprone%20to%20overfitting.%20Existing%20regularization%20methods%20merely%20minimize%20the%20norms%0Aof%20embeddings%20to%20regularize%20the%20model%2C%20leading%20to%20suboptimal%20performance.%0ATherefore%2C%20we%20propose%20a%20novel%20regularization%20method%20for%20TDB%20models%20that%0Aaddresses%20this%20limitation.%20The%20regularization%20is%20applicable%20to%20most%20TDB%20models%0Aand%20ensures%20tractable%20computation.%20Our%20method%20minimizes%20the%20norms%20of%0Aintermediate%20variables%20involved%20in%20the%20different%20ways%20of%20computing%20the%0Apredicted%20tensor.%20To%20support%20our%20regularization%20method%2C%20we%20provide%20a%0Atheoretical%20analysis%20that%20proves%20its%20effect%20in%20promoting%20low%20trace%20norm%20of%20the%0Apredicted%20tensor%20to%20reduce%20overfitting.%20Finally%2C%20we%20conduct%20experiments%20to%0Averify%20the%20effectiveness%20of%20our%20regularization%20technique%20as%20well%20as%20the%0Areliability%20of%20our%20theoretical%20analysis.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/changyi7231/IVR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02749v1&entry.124074799=Read"},
{"title": "WeightLoRA: Keep Only Necessary Adapters", "author": "Andrey Veprikov and Vladimir Solodkin and Alexander Zyl and Andrey Savchenko and Aleksandr Beznosikov", "abstract": "  The widespread utilization of language models in modern applications is\ninconceivable without Parameter-Efficient Fine-Tuning techniques, such as\nlow-rank adaptation ($\\texttt{LoRA}$), which adds trainable adapters to\nselected layers. Although $\\texttt{LoRA}$ may obtain accurate solutions, it\nrequires significant memory to train large models and intuition on which layers\nto add adapters. In this paper, we propose a novel method,\n$\\texttt{WeightLoRA}$, which overcomes this issue by adaptive selection of the\nmost critical $\\texttt{LoRA}$ heads throughout the optimization process. As a\nresult, we can significantly reduce the number of trainable parameters while\nmaintaining the capability to obtain consistent or even superior metric values.\nWe conduct experiments for a series of competitive benchmarks and DeBERTa,\nBART, and Llama models, comparing our method with different adaptive\napproaches. The experimental results demonstrate the efficacy of\n$\\texttt{WeightLoRA}$ and the superior performance of $\\texttt{WeightLoRA+}$ in\nalmost all cases.\n", "link": "http://arxiv.org/abs/2506.02724v1", "date": "2025-06-03", "relevancy": 2.3557, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4783}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4723}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4628}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WeightLoRA%3A%20Keep%20Only%20Necessary%20Adapters&body=Title%3A%20WeightLoRA%3A%20Keep%20Only%20Necessary%20Adapters%0AAuthor%3A%20Andrey%20Veprikov%20and%20Vladimir%20Solodkin%20and%20Alexander%20Zyl%20and%20Andrey%20Savchenko%20and%20Aleksandr%20Beznosikov%0AAbstract%3A%20%20%20The%20widespread%20utilization%20of%20language%20models%20in%20modern%20applications%20is%0Ainconceivable%20without%20Parameter-Efficient%20Fine-Tuning%20techniques%2C%20such%20as%0Alow-rank%20adaptation%20%28%24%5Ctexttt%7BLoRA%7D%24%29%2C%20which%20adds%20trainable%20adapters%20to%0Aselected%20layers.%20Although%20%24%5Ctexttt%7BLoRA%7D%24%20may%20obtain%20accurate%20solutions%2C%20it%0Arequires%20significant%20memory%20to%20train%20large%20models%20and%20intuition%20on%20which%20layers%0Ato%20add%20adapters.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20method%2C%0A%24%5Ctexttt%7BWeightLoRA%7D%24%2C%20which%20overcomes%20this%20issue%20by%20adaptive%20selection%20of%20the%0Amost%20critical%20%24%5Ctexttt%7BLoRA%7D%24%20heads%20throughout%20the%20optimization%20process.%20As%20a%0Aresult%2C%20we%20can%20significantly%20reduce%20the%20number%20of%20trainable%20parameters%20while%0Amaintaining%20the%20capability%20to%20obtain%20consistent%20or%20even%20superior%20metric%20values.%0AWe%20conduct%20experiments%20for%20a%20series%20of%20competitive%20benchmarks%20and%20DeBERTa%2C%0ABART%2C%20and%20Llama%20models%2C%20comparing%20our%20method%20with%20different%20adaptive%0Aapproaches.%20The%20experimental%20results%20demonstrate%20the%20efficacy%20of%0A%24%5Ctexttt%7BWeightLoRA%7D%24%20and%20the%20superior%20performance%20of%20%24%5Ctexttt%7BWeightLoRA%2B%7D%24%20in%0Aalmost%20all%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02724v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeightLoRA%253A%2520Keep%2520Only%2520Necessary%2520Adapters%26entry.906535625%3DAndrey%2520Veprikov%2520and%2520Vladimir%2520Solodkin%2520and%2520Alexander%2520Zyl%2520and%2520Andrey%2520Savchenko%2520and%2520Aleksandr%2520Beznosikov%26entry.1292438233%3D%2520%2520The%2520widespread%2520utilization%2520of%2520language%2520models%2520in%2520modern%2520applications%2520is%250Ainconceivable%2520without%2520Parameter-Efficient%2520Fine-Tuning%2520techniques%252C%2520such%2520as%250Alow-rank%2520adaptation%2520%2528%2524%255Ctexttt%257BLoRA%257D%2524%2529%252C%2520which%2520adds%2520trainable%2520adapters%2520to%250Aselected%2520layers.%2520Although%2520%2524%255Ctexttt%257BLoRA%257D%2524%2520may%2520obtain%2520accurate%2520solutions%252C%2520it%250Arequires%2520significant%2520memory%2520to%2520train%2520large%2520models%2520and%2520intuition%2520on%2520which%2520layers%250Ato%2520add%2520adapters.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520method%252C%250A%2524%255Ctexttt%257BWeightLoRA%257D%2524%252C%2520which%2520overcomes%2520this%2520issue%2520by%2520adaptive%2520selection%2520of%2520the%250Amost%2520critical%2520%2524%255Ctexttt%257BLoRA%257D%2524%2520heads%2520throughout%2520the%2520optimization%2520process.%2520As%2520a%250Aresult%252C%2520we%2520can%2520significantly%2520reduce%2520the%2520number%2520of%2520trainable%2520parameters%2520while%250Amaintaining%2520the%2520capability%2520to%2520obtain%2520consistent%2520or%2520even%2520superior%2520metric%2520values.%250AWe%2520conduct%2520experiments%2520for%2520a%2520series%2520of%2520competitive%2520benchmarks%2520and%2520DeBERTa%252C%250ABART%252C%2520and%2520Llama%2520models%252C%2520comparing%2520our%2520method%2520with%2520different%2520adaptive%250Aapproaches.%2520The%2520experimental%2520results%2520demonstrate%2520the%2520efficacy%2520of%250A%2524%255Ctexttt%257BWeightLoRA%257D%2524%2520and%2520the%2520superior%2520performance%2520of%2520%2524%255Ctexttt%257BWeightLoRA%252B%257D%2524%2520in%250Aalmost%2520all%2520cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02724v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WeightLoRA%3A%20Keep%20Only%20Necessary%20Adapters&entry.906535625=Andrey%20Veprikov%20and%20Vladimir%20Solodkin%20and%20Alexander%20Zyl%20and%20Andrey%20Savchenko%20and%20Aleksandr%20Beznosikov&entry.1292438233=%20%20The%20widespread%20utilization%20of%20language%20models%20in%20modern%20applications%20is%0Ainconceivable%20without%20Parameter-Efficient%20Fine-Tuning%20techniques%2C%20such%20as%0Alow-rank%20adaptation%20%28%24%5Ctexttt%7BLoRA%7D%24%29%2C%20which%20adds%20trainable%20adapters%20to%0Aselected%20layers.%20Although%20%24%5Ctexttt%7BLoRA%7D%24%20may%20obtain%20accurate%20solutions%2C%20it%0Arequires%20significant%20memory%20to%20train%20large%20models%20and%20intuition%20on%20which%20layers%0Ato%20add%20adapters.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20method%2C%0A%24%5Ctexttt%7BWeightLoRA%7D%24%2C%20which%20overcomes%20this%20issue%20by%20adaptive%20selection%20of%20the%0Amost%20critical%20%24%5Ctexttt%7BLoRA%7D%24%20heads%20throughout%20the%20optimization%20process.%20As%20a%0Aresult%2C%20we%20can%20significantly%20reduce%20the%20number%20of%20trainable%20parameters%20while%0Amaintaining%20the%20capability%20to%20obtain%20consistent%20or%20even%20superior%20metric%20values.%0AWe%20conduct%20experiments%20for%20a%20series%20of%20competitive%20benchmarks%20and%20DeBERTa%2C%0ABART%2C%20and%20Llama%20models%2C%20comparing%20our%20method%20with%20different%20adaptive%0Aapproaches.%20The%20experimental%20results%20demonstrate%20the%20efficacy%20of%0A%24%5Ctexttt%7BWeightLoRA%7D%24%20and%20the%20superior%20performance%20of%20%24%5Ctexttt%7BWeightLoRA%2B%7D%24%20in%0Aalmost%20all%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02724v1&entry.124074799=Read"},
{"title": "MoBluRF: Motion Deblurring Neural Radiance Fields for Blurry Monocular\n  Video", "author": "Minh-Quan Viet Bui and Jongmin Park and Jihyong Oh and Munchurl Kim", "abstract": "  Neural Radiance Fields (NeRF), initially developed for static scenes, have\ninspired many video novel view synthesis techniques. However, the challenge for\nvideo view synthesis arises from motion blur, a consequence of object or camera\nmovements during exposure, which hinders the precise synthesis of sharp\nspatio-temporal views. In response, we propose a novel motion deblurring NeRF\nframework for blurry monocular video, called MoBluRF, consisting of a Base Ray\nInitialization (BRI) stage and a Motion Decomposition-based Deblurring (MDD)\nstage. In the BRI stage, we coarsely reconstruct dynamic 3D scenes and jointly\ninitialize the base rays which are further used to predict latent sharp rays,\nusing the inaccurate camera pose information from the given blurry frames. In\nthe MDD stage, we introduce a novel Incremental Latent Sharp-rays Prediction\n(ILSP) approach for the blurry monocular video frames by decomposing the latent\nsharp rays into global camera motion and local object motion components. We\nfurther propose two loss functions for effective geometry regularization and\ndecomposition of static and dynamic scene components without any mask\nsupervision. Experiments show that MoBluRF outperforms qualitatively and\nquantitatively the recent state-of-the-art methods with large margins.\n", "link": "http://arxiv.org/abs/2312.13528v3", "date": "2025-06-03", "relevancy": 2.3525, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6048}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5932}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoBluRF%3A%20Motion%20Deblurring%20Neural%20Radiance%20Fields%20for%20Blurry%20Monocular%0A%20%20Video&body=Title%3A%20MoBluRF%3A%20Motion%20Deblurring%20Neural%20Radiance%20Fields%20for%20Blurry%20Monocular%0A%20%20Video%0AAuthor%3A%20Minh-Quan%20Viet%20Bui%20and%20Jongmin%20Park%20and%20Jihyong%20Oh%20and%20Munchurl%20Kim%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20%28NeRF%29%2C%20initially%20developed%20for%20static%20scenes%2C%20have%0Ainspired%20many%20video%20novel%20view%20synthesis%20techniques.%20However%2C%20the%20challenge%20for%0Avideo%20view%20synthesis%20arises%20from%20motion%20blur%2C%20a%20consequence%20of%20object%20or%20camera%0Amovements%20during%20exposure%2C%20which%20hinders%20the%20precise%20synthesis%20of%20sharp%0Aspatio-temporal%20views.%20In%20response%2C%20we%20propose%20a%20novel%20motion%20deblurring%20NeRF%0Aframework%20for%20blurry%20monocular%20video%2C%20called%20MoBluRF%2C%20consisting%20of%20a%20Base%20Ray%0AInitialization%20%28BRI%29%20stage%20and%20a%20Motion%20Decomposition-based%20Deblurring%20%28MDD%29%0Astage.%20In%20the%20BRI%20stage%2C%20we%20coarsely%20reconstruct%20dynamic%203D%20scenes%20and%20jointly%0Ainitialize%20the%20base%20rays%20which%20are%20further%20used%20to%20predict%20latent%20sharp%20rays%2C%0Ausing%20the%20inaccurate%20camera%20pose%20information%20from%20the%20given%20blurry%20frames.%20In%0Athe%20MDD%20stage%2C%20we%20introduce%20a%20novel%20Incremental%20Latent%20Sharp-rays%20Prediction%0A%28ILSP%29%20approach%20for%20the%20blurry%20monocular%20video%20frames%20by%20decomposing%20the%20latent%0Asharp%20rays%20into%20global%20camera%20motion%20and%20local%20object%20motion%20components.%20We%0Afurther%20propose%20two%20loss%20functions%20for%20effective%20geometry%20regularization%20and%0Adecomposition%20of%20static%20and%20dynamic%20scene%20components%20without%20any%20mask%0Asupervision.%20Experiments%20show%20that%20MoBluRF%20outperforms%20qualitatively%20and%0Aquantitatively%20the%20recent%20state-of-the-art%20methods%20with%20large%20margins.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13528v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoBluRF%253A%2520Motion%2520Deblurring%2520Neural%2520Radiance%2520Fields%2520for%2520Blurry%2520Monocular%250A%2520%2520Video%26entry.906535625%3DMinh-Quan%2520Viet%2520Bui%2520and%2520Jongmin%2520Park%2520and%2520Jihyong%2520Oh%2520and%2520Munchurl%2520Kim%26entry.1292438233%3D%2520%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%252C%2520initially%2520developed%2520for%2520static%2520scenes%252C%2520have%250Ainspired%2520many%2520video%2520novel%2520view%2520synthesis%2520techniques.%2520However%252C%2520the%2520challenge%2520for%250Avideo%2520view%2520synthesis%2520arises%2520from%2520motion%2520blur%252C%2520a%2520consequence%2520of%2520object%2520or%2520camera%250Amovements%2520during%2520exposure%252C%2520which%2520hinders%2520the%2520precise%2520synthesis%2520of%2520sharp%250Aspatio-temporal%2520views.%2520In%2520response%252C%2520we%2520propose%2520a%2520novel%2520motion%2520deblurring%2520NeRF%250Aframework%2520for%2520blurry%2520monocular%2520video%252C%2520called%2520MoBluRF%252C%2520consisting%2520of%2520a%2520Base%2520Ray%250AInitialization%2520%2528BRI%2529%2520stage%2520and%2520a%2520Motion%2520Decomposition-based%2520Deblurring%2520%2528MDD%2529%250Astage.%2520In%2520the%2520BRI%2520stage%252C%2520we%2520coarsely%2520reconstruct%2520dynamic%25203D%2520scenes%2520and%2520jointly%250Ainitialize%2520the%2520base%2520rays%2520which%2520are%2520further%2520used%2520to%2520predict%2520latent%2520sharp%2520rays%252C%250Ausing%2520the%2520inaccurate%2520camera%2520pose%2520information%2520from%2520the%2520given%2520blurry%2520frames.%2520In%250Athe%2520MDD%2520stage%252C%2520we%2520introduce%2520a%2520novel%2520Incremental%2520Latent%2520Sharp-rays%2520Prediction%250A%2528ILSP%2529%2520approach%2520for%2520the%2520blurry%2520monocular%2520video%2520frames%2520by%2520decomposing%2520the%2520latent%250Asharp%2520rays%2520into%2520global%2520camera%2520motion%2520and%2520local%2520object%2520motion%2520components.%2520We%250Afurther%2520propose%2520two%2520loss%2520functions%2520for%2520effective%2520geometry%2520regularization%2520and%250Adecomposition%2520of%2520static%2520and%2520dynamic%2520scene%2520components%2520without%2520any%2520mask%250Asupervision.%2520Experiments%2520show%2520that%2520MoBluRF%2520outperforms%2520qualitatively%2520and%250Aquantitatively%2520the%2520recent%2520state-of-the-art%2520methods%2520with%2520large%2520margins.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.13528v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoBluRF%3A%20Motion%20Deblurring%20Neural%20Radiance%20Fields%20for%20Blurry%20Monocular%0A%20%20Video&entry.906535625=Minh-Quan%20Viet%20Bui%20and%20Jongmin%20Park%20and%20Jihyong%20Oh%20and%20Munchurl%20Kim&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRF%29%2C%20initially%20developed%20for%20static%20scenes%2C%20have%0Ainspired%20many%20video%20novel%20view%20synthesis%20techniques.%20However%2C%20the%20challenge%20for%0Avideo%20view%20synthesis%20arises%20from%20motion%20blur%2C%20a%20consequence%20of%20object%20or%20camera%0Amovements%20during%20exposure%2C%20which%20hinders%20the%20precise%20synthesis%20of%20sharp%0Aspatio-temporal%20views.%20In%20response%2C%20we%20propose%20a%20novel%20motion%20deblurring%20NeRF%0Aframework%20for%20blurry%20monocular%20video%2C%20called%20MoBluRF%2C%20consisting%20of%20a%20Base%20Ray%0AInitialization%20%28BRI%29%20stage%20and%20a%20Motion%20Decomposition-based%20Deblurring%20%28MDD%29%0Astage.%20In%20the%20BRI%20stage%2C%20we%20coarsely%20reconstruct%20dynamic%203D%20scenes%20and%20jointly%0Ainitialize%20the%20base%20rays%20which%20are%20further%20used%20to%20predict%20latent%20sharp%20rays%2C%0Ausing%20the%20inaccurate%20camera%20pose%20information%20from%20the%20given%20blurry%20frames.%20In%0Athe%20MDD%20stage%2C%20we%20introduce%20a%20novel%20Incremental%20Latent%20Sharp-rays%20Prediction%0A%28ILSP%29%20approach%20for%20the%20blurry%20monocular%20video%20frames%20by%20decomposing%20the%20latent%0Asharp%20rays%20into%20global%20camera%20motion%20and%20local%20object%20motion%20components.%20We%0Afurther%20propose%20two%20loss%20functions%20for%20effective%20geometry%20regularization%20and%0Adecomposition%20of%20static%20and%20dynamic%20scene%20components%20without%20any%20mask%0Asupervision.%20Experiments%20show%20that%20MoBluRF%20outperforms%20qualitatively%20and%0Aquantitatively%20the%20recent%20state-of-the-art%20methods%20with%20large%20margins.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13528v3&entry.124074799=Read"},
{"title": "Towards Auto-Annotation from Annotation Guidelines: A Benchmark through\n  3D LiDAR Detection", "author": "Yechi Ma and Wei Hua and Shu Kong", "abstract": "  A crucial yet under-appreciated prerequisite in machine learning solutions\nfor real-applications is data annotation: human annotators are hired to\nmanually label data according to detailed, expert-crafted guidelines. This is\noften a laborious, tedious, and costly process. To study methods for\nfacilitating data annotation, we introduce a new benchmark AnnoGuide:\nAuto-Annotation from Annotation Guidelines. It aims to evaluate automated\nmethods for data annotation directly from expert-defined annotation guidelines,\neliminating the need for manual labeling. As a case study, we repurpose the\nwell-established nuScenes dataset, commonly used in autonomous driving\nresearch, which provides comprehensive annotation guidelines for labeling LiDAR\npoint clouds with 3D cuboids across 18 object classes. These guidelines include\na few visual examples and textual descriptions, but no labeled 3D cuboids in\nLiDAR data, making this a novel task of multi-modal few-shot 3D detection\nwithout 3D annotations. The advances of powerful foundation models (FMs) make\nAnnoGuide especially timely, as FMs offer promising tools to tackle its\nchallenges. We employ a conceptually straightforward pipeline that (1) utilizes\nopen-source FMs for object detection and segmentation in RGB images, (2)\nprojects 2D detections into 3D using known camera poses, and (3) clusters LiDAR\npoints within the frustum of each 2D detection to generate a 3D cuboid.\nStarting with a non-learned solution that leverages off-the-shelf FMs, we\nprogressively refine key components and achieve significant performance\nimprovements, boosting 3D detection mAP from 12.1 to 21.9! Nevertheless, our\nresults highlight that AnnoGuide remains an open and challenging problem,\nunderscoring the urgent need for developing LiDAR-based FMs. We release our\ncode and models at GitHub: https://annoguide.github.io/annoguide3Dbenchmark\n", "link": "http://arxiv.org/abs/2506.02914v1", "date": "2025-06-03", "relevancy": 2.3451, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6341}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5772}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Auto-Annotation%20from%20Annotation%20Guidelines%3A%20A%20Benchmark%20through%0A%20%203D%20LiDAR%20Detection&body=Title%3A%20Towards%20Auto-Annotation%20from%20Annotation%20Guidelines%3A%20A%20Benchmark%20through%0A%20%203D%20LiDAR%20Detection%0AAuthor%3A%20Yechi%20Ma%20and%20Wei%20Hua%20and%20Shu%20Kong%0AAbstract%3A%20%20%20A%20crucial%20yet%20under-appreciated%20prerequisite%20in%20machine%20learning%20solutions%0Afor%20real-applications%20is%20data%20annotation%3A%20human%20annotators%20are%20hired%20to%0Amanually%20label%20data%20according%20to%20detailed%2C%20expert-crafted%20guidelines.%20This%20is%0Aoften%20a%20laborious%2C%20tedious%2C%20and%20costly%20process.%20To%20study%20methods%20for%0Afacilitating%20data%20annotation%2C%20we%20introduce%20a%20new%20benchmark%20AnnoGuide%3A%0AAuto-Annotation%20from%20Annotation%20Guidelines.%20It%20aims%20to%20evaluate%20automated%0Amethods%20for%20data%20annotation%20directly%20from%20expert-defined%20annotation%20guidelines%2C%0Aeliminating%20the%20need%20for%20manual%20labeling.%20As%20a%20case%20study%2C%20we%20repurpose%20the%0Awell-established%20nuScenes%20dataset%2C%20commonly%20used%20in%20autonomous%20driving%0Aresearch%2C%20which%20provides%20comprehensive%20annotation%20guidelines%20for%20labeling%20LiDAR%0Apoint%20clouds%20with%203D%20cuboids%20across%2018%20object%20classes.%20These%20guidelines%20include%0Aa%20few%20visual%20examples%20and%20textual%20descriptions%2C%20but%20no%20labeled%203D%20cuboids%20in%0ALiDAR%20data%2C%20making%20this%20a%20novel%20task%20of%20multi-modal%20few-shot%203D%20detection%0Awithout%203D%20annotations.%20The%20advances%20of%20powerful%20foundation%20models%20%28FMs%29%20make%0AAnnoGuide%20especially%20timely%2C%20as%20FMs%20offer%20promising%20tools%20to%20tackle%20its%0Achallenges.%20We%20employ%20a%20conceptually%20straightforward%20pipeline%20that%20%281%29%20utilizes%0Aopen-source%20FMs%20for%20object%20detection%20and%20segmentation%20in%20RGB%20images%2C%20%282%29%0Aprojects%202D%20detections%20into%203D%20using%20known%20camera%20poses%2C%20and%20%283%29%20clusters%20LiDAR%0Apoints%20within%20the%20frustum%20of%20each%202D%20detection%20to%20generate%20a%203D%20cuboid.%0AStarting%20with%20a%20non-learned%20solution%20that%20leverages%20off-the-shelf%20FMs%2C%20we%0Aprogressively%20refine%20key%20components%20and%20achieve%20significant%20performance%0Aimprovements%2C%20boosting%203D%20detection%20mAP%20from%2012.1%20to%2021.9%21%20Nevertheless%2C%20our%0Aresults%20highlight%20that%20AnnoGuide%20remains%20an%20open%20and%20challenging%20problem%2C%0Aunderscoring%20the%20urgent%20need%20for%20developing%20LiDAR-based%20FMs.%20We%20release%20our%0Acode%20and%20models%20at%20GitHub%3A%20https%3A//annoguide.github.io/annoguide3Dbenchmark%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02914v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Auto-Annotation%2520from%2520Annotation%2520Guidelines%253A%2520A%2520Benchmark%2520through%250A%2520%25203D%2520LiDAR%2520Detection%26entry.906535625%3DYechi%2520Ma%2520and%2520Wei%2520Hua%2520and%2520Shu%2520Kong%26entry.1292438233%3D%2520%2520A%2520crucial%2520yet%2520under-appreciated%2520prerequisite%2520in%2520machine%2520learning%2520solutions%250Afor%2520real-applications%2520is%2520data%2520annotation%253A%2520human%2520annotators%2520are%2520hired%2520to%250Amanually%2520label%2520data%2520according%2520to%2520detailed%252C%2520expert-crafted%2520guidelines.%2520This%2520is%250Aoften%2520a%2520laborious%252C%2520tedious%252C%2520and%2520costly%2520process.%2520To%2520study%2520methods%2520for%250Afacilitating%2520data%2520annotation%252C%2520we%2520introduce%2520a%2520new%2520benchmark%2520AnnoGuide%253A%250AAuto-Annotation%2520from%2520Annotation%2520Guidelines.%2520It%2520aims%2520to%2520evaluate%2520automated%250Amethods%2520for%2520data%2520annotation%2520directly%2520from%2520expert-defined%2520annotation%2520guidelines%252C%250Aeliminating%2520the%2520need%2520for%2520manual%2520labeling.%2520As%2520a%2520case%2520study%252C%2520we%2520repurpose%2520the%250Awell-established%2520nuScenes%2520dataset%252C%2520commonly%2520used%2520in%2520autonomous%2520driving%250Aresearch%252C%2520which%2520provides%2520comprehensive%2520annotation%2520guidelines%2520for%2520labeling%2520LiDAR%250Apoint%2520clouds%2520with%25203D%2520cuboids%2520across%252018%2520object%2520classes.%2520These%2520guidelines%2520include%250Aa%2520few%2520visual%2520examples%2520and%2520textual%2520descriptions%252C%2520but%2520no%2520labeled%25203D%2520cuboids%2520in%250ALiDAR%2520data%252C%2520making%2520this%2520a%2520novel%2520task%2520of%2520multi-modal%2520few-shot%25203D%2520detection%250Awithout%25203D%2520annotations.%2520The%2520advances%2520of%2520powerful%2520foundation%2520models%2520%2528FMs%2529%2520make%250AAnnoGuide%2520especially%2520timely%252C%2520as%2520FMs%2520offer%2520promising%2520tools%2520to%2520tackle%2520its%250Achallenges.%2520We%2520employ%2520a%2520conceptually%2520straightforward%2520pipeline%2520that%2520%25281%2529%2520utilizes%250Aopen-source%2520FMs%2520for%2520object%2520detection%2520and%2520segmentation%2520in%2520RGB%2520images%252C%2520%25282%2529%250Aprojects%25202D%2520detections%2520into%25203D%2520using%2520known%2520camera%2520poses%252C%2520and%2520%25283%2529%2520clusters%2520LiDAR%250Apoints%2520within%2520the%2520frustum%2520of%2520each%25202D%2520detection%2520to%2520generate%2520a%25203D%2520cuboid.%250AStarting%2520with%2520a%2520non-learned%2520solution%2520that%2520leverages%2520off-the-shelf%2520FMs%252C%2520we%250Aprogressively%2520refine%2520key%2520components%2520and%2520achieve%2520significant%2520performance%250Aimprovements%252C%2520boosting%25203D%2520detection%2520mAP%2520from%252012.1%2520to%252021.9%2521%2520Nevertheless%252C%2520our%250Aresults%2520highlight%2520that%2520AnnoGuide%2520remains%2520an%2520open%2520and%2520challenging%2520problem%252C%250Aunderscoring%2520the%2520urgent%2520need%2520for%2520developing%2520LiDAR-based%2520FMs.%2520We%2520release%2520our%250Acode%2520and%2520models%2520at%2520GitHub%253A%2520https%253A//annoguide.github.io/annoguide3Dbenchmark%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02914v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Auto-Annotation%20from%20Annotation%20Guidelines%3A%20A%20Benchmark%20through%0A%20%203D%20LiDAR%20Detection&entry.906535625=Yechi%20Ma%20and%20Wei%20Hua%20and%20Shu%20Kong&entry.1292438233=%20%20A%20crucial%20yet%20under-appreciated%20prerequisite%20in%20machine%20learning%20solutions%0Afor%20real-applications%20is%20data%20annotation%3A%20human%20annotators%20are%20hired%20to%0Amanually%20label%20data%20according%20to%20detailed%2C%20expert-crafted%20guidelines.%20This%20is%0Aoften%20a%20laborious%2C%20tedious%2C%20and%20costly%20process.%20To%20study%20methods%20for%0Afacilitating%20data%20annotation%2C%20we%20introduce%20a%20new%20benchmark%20AnnoGuide%3A%0AAuto-Annotation%20from%20Annotation%20Guidelines.%20It%20aims%20to%20evaluate%20automated%0Amethods%20for%20data%20annotation%20directly%20from%20expert-defined%20annotation%20guidelines%2C%0Aeliminating%20the%20need%20for%20manual%20labeling.%20As%20a%20case%20study%2C%20we%20repurpose%20the%0Awell-established%20nuScenes%20dataset%2C%20commonly%20used%20in%20autonomous%20driving%0Aresearch%2C%20which%20provides%20comprehensive%20annotation%20guidelines%20for%20labeling%20LiDAR%0Apoint%20clouds%20with%203D%20cuboids%20across%2018%20object%20classes.%20These%20guidelines%20include%0Aa%20few%20visual%20examples%20and%20textual%20descriptions%2C%20but%20no%20labeled%203D%20cuboids%20in%0ALiDAR%20data%2C%20making%20this%20a%20novel%20task%20of%20multi-modal%20few-shot%203D%20detection%0Awithout%203D%20annotations.%20The%20advances%20of%20powerful%20foundation%20models%20%28FMs%29%20make%0AAnnoGuide%20especially%20timely%2C%20as%20FMs%20offer%20promising%20tools%20to%20tackle%20its%0Achallenges.%20We%20employ%20a%20conceptually%20straightforward%20pipeline%20that%20%281%29%20utilizes%0Aopen-source%20FMs%20for%20object%20detection%20and%20segmentation%20in%20RGB%20images%2C%20%282%29%0Aprojects%202D%20detections%20into%203D%20using%20known%20camera%20poses%2C%20and%20%283%29%20clusters%20LiDAR%0Apoints%20within%20the%20frustum%20of%20each%202D%20detection%20to%20generate%20a%203D%20cuboid.%0AStarting%20with%20a%20non-learned%20solution%20that%20leverages%20off-the-shelf%20FMs%2C%20we%0Aprogressively%20refine%20key%20components%20and%20achieve%20significant%20performance%0Aimprovements%2C%20boosting%203D%20detection%20mAP%20from%2012.1%20to%2021.9%21%20Nevertheless%2C%20our%0Aresults%20highlight%20that%20AnnoGuide%20remains%20an%20open%20and%20challenging%20problem%2C%0Aunderscoring%20the%20urgent%20need%20for%20developing%20LiDAR-based%20FMs.%20We%20release%20our%0Acode%20and%20models%20at%20GitHub%3A%20https%3A//annoguide.github.io/annoguide3Dbenchmark%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02914v1&entry.124074799=Read"},
{"title": "SAMJ: Fast Image Annotation on ImageJ/Fiji via Segment Anything Model", "author": "Carlos Garcia-Lopez-de-Haro and Caterina Fuster-Barcelo and Curtis T. Rueden and Jonathan Heras and Vladimir Ulman and Daniel Franco-Barranco and Adrian Ines and Kevin W. Eliceiri and Jean-Christophe Olivo-Marin and Jean-Yves Tinevez and Daniel Sage and Arrate Munoz-Barrutia", "abstract": "  Mask annotation remains a significant bottleneck in AI-driven biomedical\nimage analysis due to its labor-intensive nature. To address this challenge, we\nintroduce SAMJ, a user-friendly ImageJ/Fiji plugin leveraging the Segment\nAnything Model (SAM). SAMJ enables seamless, interactive annotations with\none-click installation on standard computers. Designed for real-time object\ndelineation in large scientific images, SAMJ is an easy-to-use solution that\nsimplifies and accelerates the creation of labeled image datasets.\n", "link": "http://arxiv.org/abs/2506.02783v1", "date": "2025-06-03", "relevancy": 2.3417, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4738}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4656}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4656}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAMJ%3A%20Fast%20Image%20Annotation%20on%20ImageJ/Fiji%20via%20Segment%20Anything%20Model&body=Title%3A%20SAMJ%3A%20Fast%20Image%20Annotation%20on%20ImageJ/Fiji%20via%20Segment%20Anything%20Model%0AAuthor%3A%20Carlos%20Garcia-Lopez-de-Haro%20and%20Caterina%20Fuster-Barcelo%20and%20Curtis%20T.%20Rueden%20and%20Jonathan%20Heras%20and%20Vladimir%20Ulman%20and%20Daniel%20Franco-Barranco%20and%20Adrian%20Ines%20and%20Kevin%20W.%20Eliceiri%20and%20Jean-Christophe%20Olivo-Marin%20and%20Jean-Yves%20Tinevez%20and%20Daniel%20Sage%20and%20Arrate%20Munoz-Barrutia%0AAbstract%3A%20%20%20Mask%20annotation%20remains%20a%20significant%20bottleneck%20in%20AI-driven%20biomedical%0Aimage%20analysis%20due%20to%20its%20labor-intensive%20nature.%20To%20address%20this%20challenge%2C%20we%0Aintroduce%20SAMJ%2C%20a%20user-friendly%20ImageJ/Fiji%20plugin%20leveraging%20the%20Segment%0AAnything%20Model%20%28SAM%29.%20SAMJ%20enables%20seamless%2C%20interactive%20annotations%20with%0Aone-click%20installation%20on%20standard%20computers.%20Designed%20for%20real-time%20object%0Adelineation%20in%20large%20scientific%20images%2C%20SAMJ%20is%20an%20easy-to-use%20solution%20that%0Asimplifies%20and%20accelerates%20the%20creation%20of%20labeled%20image%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02783v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAMJ%253A%2520Fast%2520Image%2520Annotation%2520on%2520ImageJ/Fiji%2520via%2520Segment%2520Anything%2520Model%26entry.906535625%3DCarlos%2520Garcia-Lopez-de-Haro%2520and%2520Caterina%2520Fuster-Barcelo%2520and%2520Curtis%2520T.%2520Rueden%2520and%2520Jonathan%2520Heras%2520and%2520Vladimir%2520Ulman%2520and%2520Daniel%2520Franco-Barranco%2520and%2520Adrian%2520Ines%2520and%2520Kevin%2520W.%2520Eliceiri%2520and%2520Jean-Christophe%2520Olivo-Marin%2520and%2520Jean-Yves%2520Tinevez%2520and%2520Daniel%2520Sage%2520and%2520Arrate%2520Munoz-Barrutia%26entry.1292438233%3D%2520%2520Mask%2520annotation%2520remains%2520a%2520significant%2520bottleneck%2520in%2520AI-driven%2520biomedical%250Aimage%2520analysis%2520due%2520to%2520its%2520labor-intensive%2520nature.%2520To%2520address%2520this%2520challenge%252C%2520we%250Aintroduce%2520SAMJ%252C%2520a%2520user-friendly%2520ImageJ/Fiji%2520plugin%2520leveraging%2520the%2520Segment%250AAnything%2520Model%2520%2528SAM%2529.%2520SAMJ%2520enables%2520seamless%252C%2520interactive%2520annotations%2520with%250Aone-click%2520installation%2520on%2520standard%2520computers.%2520Designed%2520for%2520real-time%2520object%250Adelineation%2520in%2520large%2520scientific%2520images%252C%2520SAMJ%2520is%2520an%2520easy-to-use%2520solution%2520that%250Asimplifies%2520and%2520accelerates%2520the%2520creation%2520of%2520labeled%2520image%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02783v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAMJ%3A%20Fast%20Image%20Annotation%20on%20ImageJ/Fiji%20via%20Segment%20Anything%20Model&entry.906535625=Carlos%20Garcia-Lopez-de-Haro%20and%20Caterina%20Fuster-Barcelo%20and%20Curtis%20T.%20Rueden%20and%20Jonathan%20Heras%20and%20Vladimir%20Ulman%20and%20Daniel%20Franco-Barranco%20and%20Adrian%20Ines%20and%20Kevin%20W.%20Eliceiri%20and%20Jean-Christophe%20Olivo-Marin%20and%20Jean-Yves%20Tinevez%20and%20Daniel%20Sage%20and%20Arrate%20Munoz-Barrutia&entry.1292438233=%20%20Mask%20annotation%20remains%20a%20significant%20bottleneck%20in%20AI-driven%20biomedical%0Aimage%20analysis%20due%20to%20its%20labor-intensive%20nature.%20To%20address%20this%20challenge%2C%20we%0Aintroduce%20SAMJ%2C%20a%20user-friendly%20ImageJ/Fiji%20plugin%20leveraging%20the%20Segment%0AAnything%20Model%20%28SAM%29.%20SAMJ%20enables%20seamless%2C%20interactive%20annotations%20with%0Aone-click%20installation%20on%20standard%20computers.%20Designed%20for%20real-time%20object%0Adelineation%20in%20large%20scientific%20images%2C%20SAMJ%20is%20an%20easy-to-use%20solution%20that%0Asimplifies%20and%20accelerates%20the%20creation%20of%20labeled%20image%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02783v1&entry.124074799=Read"},
{"title": "Armijo Line-search Can Make (Stochastic) Gradient Descent Provably\n  Faster", "author": "Sharan Vaswani and Reza Babanezhad", "abstract": "  Armijo line-search (Armijo-LS) is a standard method to set the step-size for\ngradient descent (GD). For smooth functions, Armijo-LS alleviates the need to\nknow the global smoothness constant L and adapts to the ``local'' smoothness,\nenabling GD to converge faster. Existing theoretical analyses show that GD with\nArmijo-LS (GD-LS) can result in constant factor improvements over GD with a 1/L\nstep-size (denoted as GD(1/L)). We strengthen these results and show that if\nthe objective function satisfies a certain non-uniform smoothness condition,\nGD-LS can result in a faster convergence rate than GD(1/L). In particular, we\nprove that for convex objectives corresponding to logistic regression and\nmulti-class classification, GD-LS can converge to the optimum at a linear rate,\nand hence improves over the sublinear convergence of GD(1/L). Furthermore, for\nnon-convex objectives satisfying gradient domination (e.g., those corresponding\nto the softmax policy gradient in RL or generalized linear models with a\nlogistic link function), GD-LS can match the fast convergence of algorithms\ntailored for these specific settings. Finally, we prove that under the\ninterpolation assumption, for convex losses, stochastic GD with a stochastic\nline-search can match the fast convergence of GD-LS\n", "link": "http://arxiv.org/abs/2503.00229v2", "date": "2025-06-03", "relevancy": 2.3277, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4861}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4563}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Armijo%20Line-search%20Can%20Make%20%28Stochastic%29%20Gradient%20Descent%20Provably%0A%20%20Faster&body=Title%3A%20Armijo%20Line-search%20Can%20Make%20%28Stochastic%29%20Gradient%20Descent%20Provably%0A%20%20Faster%0AAuthor%3A%20Sharan%20Vaswani%20and%20Reza%20Babanezhad%0AAbstract%3A%20%20%20Armijo%20line-search%20%28Armijo-LS%29%20is%20a%20standard%20method%20to%20set%20the%20step-size%20for%0Agradient%20descent%20%28GD%29.%20For%20smooth%20functions%2C%20Armijo-LS%20alleviates%20the%20need%20to%0Aknow%20the%20global%20smoothness%20constant%20L%20and%20adapts%20to%20the%20%60%60local%27%27%20smoothness%2C%0Aenabling%20GD%20to%20converge%20faster.%20Existing%20theoretical%20analyses%20show%20that%20GD%20with%0AArmijo-LS%20%28GD-LS%29%20can%20result%20in%20constant%20factor%20improvements%20over%20GD%20with%20a%201/L%0Astep-size%20%28denoted%20as%20GD%281/L%29%29.%20We%20strengthen%20these%20results%20and%20show%20that%20if%0Athe%20objective%20function%20satisfies%20a%20certain%20non-uniform%20smoothness%20condition%2C%0AGD-LS%20can%20result%20in%20a%20faster%20convergence%20rate%20than%20GD%281/L%29.%20In%20particular%2C%20we%0Aprove%20that%20for%20convex%20objectives%20corresponding%20to%20logistic%20regression%20and%0Amulti-class%20classification%2C%20GD-LS%20can%20converge%20to%20the%20optimum%20at%20a%20linear%20rate%2C%0Aand%20hence%20improves%20over%20the%20sublinear%20convergence%20of%20GD%281/L%29.%20Furthermore%2C%20for%0Anon-convex%20objectives%20satisfying%20gradient%20domination%20%28e.g.%2C%20those%20corresponding%0Ato%20the%20softmax%20policy%20gradient%20in%20RL%20or%20generalized%20linear%20models%20with%20a%0Alogistic%20link%20function%29%2C%20GD-LS%20can%20match%20the%20fast%20convergence%20of%20algorithms%0Atailored%20for%20these%20specific%20settings.%20Finally%2C%20we%20prove%20that%20under%20the%0Ainterpolation%20assumption%2C%20for%20convex%20losses%2C%20stochastic%20GD%20with%20a%20stochastic%0Aline-search%20can%20match%20the%20fast%20convergence%20of%20GD-LS%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.00229v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArmijo%2520Line-search%2520Can%2520Make%2520%2528Stochastic%2529%2520Gradient%2520Descent%2520Provably%250A%2520%2520Faster%26entry.906535625%3DSharan%2520Vaswani%2520and%2520Reza%2520Babanezhad%26entry.1292438233%3D%2520%2520Armijo%2520line-search%2520%2528Armijo-LS%2529%2520is%2520a%2520standard%2520method%2520to%2520set%2520the%2520step-size%2520for%250Agradient%2520descent%2520%2528GD%2529.%2520For%2520smooth%2520functions%252C%2520Armijo-LS%2520alleviates%2520the%2520need%2520to%250Aknow%2520the%2520global%2520smoothness%2520constant%2520L%2520and%2520adapts%2520to%2520the%2520%2560%2560local%2527%2527%2520smoothness%252C%250Aenabling%2520GD%2520to%2520converge%2520faster.%2520Existing%2520theoretical%2520analyses%2520show%2520that%2520GD%2520with%250AArmijo-LS%2520%2528GD-LS%2529%2520can%2520result%2520in%2520constant%2520factor%2520improvements%2520over%2520GD%2520with%2520a%25201/L%250Astep-size%2520%2528denoted%2520as%2520GD%25281/L%2529%2529.%2520We%2520strengthen%2520these%2520results%2520and%2520show%2520that%2520if%250Athe%2520objective%2520function%2520satisfies%2520a%2520certain%2520non-uniform%2520smoothness%2520condition%252C%250AGD-LS%2520can%2520result%2520in%2520a%2520faster%2520convergence%2520rate%2520than%2520GD%25281/L%2529.%2520In%2520particular%252C%2520we%250Aprove%2520that%2520for%2520convex%2520objectives%2520corresponding%2520to%2520logistic%2520regression%2520and%250Amulti-class%2520classification%252C%2520GD-LS%2520can%2520converge%2520to%2520the%2520optimum%2520at%2520a%2520linear%2520rate%252C%250Aand%2520hence%2520improves%2520over%2520the%2520sublinear%2520convergence%2520of%2520GD%25281/L%2529.%2520Furthermore%252C%2520for%250Anon-convex%2520objectives%2520satisfying%2520gradient%2520domination%2520%2528e.g.%252C%2520those%2520corresponding%250Ato%2520the%2520softmax%2520policy%2520gradient%2520in%2520RL%2520or%2520generalized%2520linear%2520models%2520with%2520a%250Alogistic%2520link%2520function%2529%252C%2520GD-LS%2520can%2520match%2520the%2520fast%2520convergence%2520of%2520algorithms%250Atailored%2520for%2520these%2520specific%2520settings.%2520Finally%252C%2520we%2520prove%2520that%2520under%2520the%250Ainterpolation%2520assumption%252C%2520for%2520convex%2520losses%252C%2520stochastic%2520GD%2520with%2520a%2520stochastic%250Aline-search%2520can%2520match%2520the%2520fast%2520convergence%2520of%2520GD-LS%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.00229v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Armijo%20Line-search%20Can%20Make%20%28Stochastic%29%20Gradient%20Descent%20Provably%0A%20%20Faster&entry.906535625=Sharan%20Vaswani%20and%20Reza%20Babanezhad&entry.1292438233=%20%20Armijo%20line-search%20%28Armijo-LS%29%20is%20a%20standard%20method%20to%20set%20the%20step-size%20for%0Agradient%20descent%20%28GD%29.%20For%20smooth%20functions%2C%20Armijo-LS%20alleviates%20the%20need%20to%0Aknow%20the%20global%20smoothness%20constant%20L%20and%20adapts%20to%20the%20%60%60local%27%27%20smoothness%2C%0Aenabling%20GD%20to%20converge%20faster.%20Existing%20theoretical%20analyses%20show%20that%20GD%20with%0AArmijo-LS%20%28GD-LS%29%20can%20result%20in%20constant%20factor%20improvements%20over%20GD%20with%20a%201/L%0Astep-size%20%28denoted%20as%20GD%281/L%29%29.%20We%20strengthen%20these%20results%20and%20show%20that%20if%0Athe%20objective%20function%20satisfies%20a%20certain%20non-uniform%20smoothness%20condition%2C%0AGD-LS%20can%20result%20in%20a%20faster%20convergence%20rate%20than%20GD%281/L%29.%20In%20particular%2C%20we%0Aprove%20that%20for%20convex%20objectives%20corresponding%20to%20logistic%20regression%20and%0Amulti-class%20classification%2C%20GD-LS%20can%20converge%20to%20the%20optimum%20at%20a%20linear%20rate%2C%0Aand%20hence%20improves%20over%20the%20sublinear%20convergence%20of%20GD%281/L%29.%20Furthermore%2C%20for%0Anon-convex%20objectives%20satisfying%20gradient%20domination%20%28e.g.%2C%20those%20corresponding%0Ato%20the%20softmax%20policy%20gradient%20in%20RL%20or%20generalized%20linear%20models%20with%20a%0Alogistic%20link%20function%29%2C%20GD-LS%20can%20match%20the%20fast%20convergence%20of%20algorithms%0Atailored%20for%20these%20specific%20settings.%20Finally%2C%20we%20prove%20that%20under%20the%0Ainterpolation%20assumption%2C%20for%20convex%20losses%2C%20stochastic%20GD%20with%20a%20stochastic%0Aline-search%20can%20match%20the%20fast%20convergence%20of%20GD-LS%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.00229v2&entry.124074799=Read"},
{"title": "PECANN: Parallel Efficient Clustering with Graph-Based Approximate\n  Nearest Neighbor Search", "author": "Shangdi Yu and Joshua Engels and Yihao Huang and Julian Shun", "abstract": "  This paper studies density-based clustering of point sets. These methods use\ndense regions of points to detect clusters of arbitrary shapes. In particular,\nwe study variants of density peaks clustering, a popular type of algorithm that\nhas been shown to work well in practice. Our goal is to cluster large\nhigh-dimensional datasets, which are prevalent in practice. Prior solutions are\neither sequential, and cannot scale to large data, or are specialized for\nlow-dimensional data.\n  This paper unifies the different variants of density peaks clustering into a\nsingle framework, PECANN, by abstracting out several key steps common to this\nclass of algorithms. One such key step is to find nearest neighbors that\nsatisfy a predicate function, and one of the main contributions of this paper\nis an efficient way to do this predicate search using graph-based approximate\nnearest neighbor search (ANNS). To provide ample parallelism, we propose a\ndoubling search technique that enables points to find an approximate nearest\nneighbor satisfying the predicate in a small number of rounds. Our technique\ncan be applied to many existing graph-based ANNS algorithms, which can all be\nplugged into PECANN.\n  We implement five clustering algorithms with PECANN and evaluate them on\nsynthetic and real-world datasets with up to 1.28 million points and up to 1024\ndimensions on a 30-core machine with two-way hyper-threading. Compared to the\nstate-of-the-art FASTDP algorithm for high-dimensional density peaks\nclustering, which is sequential, our best algorithm is 45x-734x faster while\nachieving competitive ARI scores. Compared to the state-of-the-art parallel\nDPC-based algorithm, which is optimized for low dimensions, we show that PECANN\nis two orders of magnitude faster. As far as we know, our work is the first to\nevaluate DPC variants on large high-dimensional real-world image and text\nembedding datasets.\n", "link": "http://arxiv.org/abs/2312.03940v3", "date": "2025-06-03", "relevancy": 2.3173, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4692}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4613}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PECANN%3A%20Parallel%20Efficient%20Clustering%20with%20Graph-Based%20Approximate%0A%20%20Nearest%20Neighbor%20Search&body=Title%3A%20PECANN%3A%20Parallel%20Efficient%20Clustering%20with%20Graph-Based%20Approximate%0A%20%20Nearest%20Neighbor%20Search%0AAuthor%3A%20Shangdi%20Yu%20and%20Joshua%20Engels%20and%20Yihao%20Huang%20and%20Julian%20Shun%0AAbstract%3A%20%20%20This%20paper%20studies%20density-based%20clustering%20of%20point%20sets.%20These%20methods%20use%0Adense%20regions%20of%20points%20to%20detect%20clusters%20of%20arbitrary%20shapes.%20In%20particular%2C%0Awe%20study%20variants%20of%20density%20peaks%20clustering%2C%20a%20popular%20type%20of%20algorithm%20that%0Ahas%20been%20shown%20to%20work%20well%20in%20practice.%20Our%20goal%20is%20to%20cluster%20large%0Ahigh-dimensional%20datasets%2C%20which%20are%20prevalent%20in%20practice.%20Prior%20solutions%20are%0Aeither%20sequential%2C%20and%20cannot%20scale%20to%20large%20data%2C%20or%20are%20specialized%20for%0Alow-dimensional%20data.%0A%20%20This%20paper%20unifies%20the%20different%20variants%20of%20density%20peaks%20clustering%20into%20a%0Asingle%20framework%2C%20PECANN%2C%20by%20abstracting%20out%20several%20key%20steps%20common%20to%20this%0Aclass%20of%20algorithms.%20One%20such%20key%20step%20is%20to%20find%20nearest%20neighbors%20that%0Asatisfy%20a%20predicate%20function%2C%20and%20one%20of%20the%20main%20contributions%20of%20this%20paper%0Ais%20an%20efficient%20way%20to%20do%20this%20predicate%20search%20using%20graph-based%20approximate%0Anearest%20neighbor%20search%20%28ANNS%29.%20To%20provide%20ample%20parallelism%2C%20we%20propose%20a%0Adoubling%20search%20technique%20that%20enables%20points%20to%20find%20an%20approximate%20nearest%0Aneighbor%20satisfying%20the%20predicate%20in%20a%20small%20number%20of%20rounds.%20Our%20technique%0Acan%20be%20applied%20to%20many%20existing%20graph-based%20ANNS%20algorithms%2C%20which%20can%20all%20be%0Aplugged%20into%20PECANN.%0A%20%20We%20implement%20five%20clustering%20algorithms%20with%20PECANN%20and%20evaluate%20them%20on%0Asynthetic%20and%20real-world%20datasets%20with%20up%20to%201.28%20million%20points%20and%20up%20to%201024%0Adimensions%20on%20a%2030-core%20machine%20with%20two-way%20hyper-threading.%20Compared%20to%20the%0Astate-of-the-art%20FASTDP%20algorithm%20for%20high-dimensional%20density%20peaks%0Aclustering%2C%20which%20is%20sequential%2C%20our%20best%20algorithm%20is%2045x-734x%20faster%20while%0Aachieving%20competitive%20ARI%20scores.%20Compared%20to%20the%20state-of-the-art%20parallel%0ADPC-based%20algorithm%2C%20which%20is%20optimized%20for%20low%20dimensions%2C%20we%20show%20that%20PECANN%0Ais%20two%20orders%20of%20magnitude%20faster.%20As%20far%20as%20we%20know%2C%20our%20work%20is%20the%20first%20to%0Aevaluate%20DPC%20variants%20on%20large%20high-dimensional%20real-world%20image%20and%20text%0Aembedding%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.03940v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPECANN%253A%2520Parallel%2520Efficient%2520Clustering%2520with%2520Graph-Based%2520Approximate%250A%2520%2520Nearest%2520Neighbor%2520Search%26entry.906535625%3DShangdi%2520Yu%2520and%2520Joshua%2520Engels%2520and%2520Yihao%2520Huang%2520and%2520Julian%2520Shun%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520density-based%2520clustering%2520of%2520point%2520sets.%2520These%2520methods%2520use%250Adense%2520regions%2520of%2520points%2520to%2520detect%2520clusters%2520of%2520arbitrary%2520shapes.%2520In%2520particular%252C%250Awe%2520study%2520variants%2520of%2520density%2520peaks%2520clustering%252C%2520a%2520popular%2520type%2520of%2520algorithm%2520that%250Ahas%2520been%2520shown%2520to%2520work%2520well%2520in%2520practice.%2520Our%2520goal%2520is%2520to%2520cluster%2520large%250Ahigh-dimensional%2520datasets%252C%2520which%2520are%2520prevalent%2520in%2520practice.%2520Prior%2520solutions%2520are%250Aeither%2520sequential%252C%2520and%2520cannot%2520scale%2520to%2520large%2520data%252C%2520or%2520are%2520specialized%2520for%250Alow-dimensional%2520data.%250A%2520%2520This%2520paper%2520unifies%2520the%2520different%2520variants%2520of%2520density%2520peaks%2520clustering%2520into%2520a%250Asingle%2520framework%252C%2520PECANN%252C%2520by%2520abstracting%2520out%2520several%2520key%2520steps%2520common%2520to%2520this%250Aclass%2520of%2520algorithms.%2520One%2520such%2520key%2520step%2520is%2520to%2520find%2520nearest%2520neighbors%2520that%250Asatisfy%2520a%2520predicate%2520function%252C%2520and%2520one%2520of%2520the%2520main%2520contributions%2520of%2520this%2520paper%250Ais%2520an%2520efficient%2520way%2520to%2520do%2520this%2520predicate%2520search%2520using%2520graph-based%2520approximate%250Anearest%2520neighbor%2520search%2520%2528ANNS%2529.%2520To%2520provide%2520ample%2520parallelism%252C%2520we%2520propose%2520a%250Adoubling%2520search%2520technique%2520that%2520enables%2520points%2520to%2520find%2520an%2520approximate%2520nearest%250Aneighbor%2520satisfying%2520the%2520predicate%2520in%2520a%2520small%2520number%2520of%2520rounds.%2520Our%2520technique%250Acan%2520be%2520applied%2520to%2520many%2520existing%2520graph-based%2520ANNS%2520algorithms%252C%2520which%2520can%2520all%2520be%250Aplugged%2520into%2520PECANN.%250A%2520%2520We%2520implement%2520five%2520clustering%2520algorithms%2520with%2520PECANN%2520and%2520evaluate%2520them%2520on%250Asynthetic%2520and%2520real-world%2520datasets%2520with%2520up%2520to%25201.28%2520million%2520points%2520and%2520up%2520to%25201024%250Adimensions%2520on%2520a%252030-core%2520machine%2520with%2520two-way%2520hyper-threading.%2520Compared%2520to%2520the%250Astate-of-the-art%2520FASTDP%2520algorithm%2520for%2520high-dimensional%2520density%2520peaks%250Aclustering%252C%2520which%2520is%2520sequential%252C%2520our%2520best%2520algorithm%2520is%252045x-734x%2520faster%2520while%250Aachieving%2520competitive%2520ARI%2520scores.%2520Compared%2520to%2520the%2520state-of-the-art%2520parallel%250ADPC-based%2520algorithm%252C%2520which%2520is%2520optimized%2520for%2520low%2520dimensions%252C%2520we%2520show%2520that%2520PECANN%250Ais%2520two%2520orders%2520of%2520magnitude%2520faster.%2520As%2520far%2520as%2520we%2520know%252C%2520our%2520work%2520is%2520the%2520first%2520to%250Aevaluate%2520DPC%2520variants%2520on%2520large%2520high-dimensional%2520real-world%2520image%2520and%2520text%250Aembedding%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.03940v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PECANN%3A%20Parallel%20Efficient%20Clustering%20with%20Graph-Based%20Approximate%0A%20%20Nearest%20Neighbor%20Search&entry.906535625=Shangdi%20Yu%20and%20Joshua%20Engels%20and%20Yihao%20Huang%20and%20Julian%20Shun&entry.1292438233=%20%20This%20paper%20studies%20density-based%20clustering%20of%20point%20sets.%20These%20methods%20use%0Adense%20regions%20of%20points%20to%20detect%20clusters%20of%20arbitrary%20shapes.%20In%20particular%2C%0Awe%20study%20variants%20of%20density%20peaks%20clustering%2C%20a%20popular%20type%20of%20algorithm%20that%0Ahas%20been%20shown%20to%20work%20well%20in%20practice.%20Our%20goal%20is%20to%20cluster%20large%0Ahigh-dimensional%20datasets%2C%20which%20are%20prevalent%20in%20practice.%20Prior%20solutions%20are%0Aeither%20sequential%2C%20and%20cannot%20scale%20to%20large%20data%2C%20or%20are%20specialized%20for%0Alow-dimensional%20data.%0A%20%20This%20paper%20unifies%20the%20different%20variants%20of%20density%20peaks%20clustering%20into%20a%0Asingle%20framework%2C%20PECANN%2C%20by%20abstracting%20out%20several%20key%20steps%20common%20to%20this%0Aclass%20of%20algorithms.%20One%20such%20key%20step%20is%20to%20find%20nearest%20neighbors%20that%0Asatisfy%20a%20predicate%20function%2C%20and%20one%20of%20the%20main%20contributions%20of%20this%20paper%0Ais%20an%20efficient%20way%20to%20do%20this%20predicate%20search%20using%20graph-based%20approximate%0Anearest%20neighbor%20search%20%28ANNS%29.%20To%20provide%20ample%20parallelism%2C%20we%20propose%20a%0Adoubling%20search%20technique%20that%20enables%20points%20to%20find%20an%20approximate%20nearest%0Aneighbor%20satisfying%20the%20predicate%20in%20a%20small%20number%20of%20rounds.%20Our%20technique%0Acan%20be%20applied%20to%20many%20existing%20graph-based%20ANNS%20algorithms%2C%20which%20can%20all%20be%0Aplugged%20into%20PECANN.%0A%20%20We%20implement%20five%20clustering%20algorithms%20with%20PECANN%20and%20evaluate%20them%20on%0Asynthetic%20and%20real-world%20datasets%20with%20up%20to%201.28%20million%20points%20and%20up%20to%201024%0Adimensions%20on%20a%2030-core%20machine%20with%20two-way%20hyper-threading.%20Compared%20to%20the%0Astate-of-the-art%20FASTDP%20algorithm%20for%20high-dimensional%20density%20peaks%0Aclustering%2C%20which%20is%20sequential%2C%20our%20best%20algorithm%20is%2045x-734x%20faster%20while%0Aachieving%20competitive%20ARI%20scores.%20Compared%20to%20the%20state-of-the-art%20parallel%0ADPC-based%20algorithm%2C%20which%20is%20optimized%20for%20low%20dimensions%2C%20we%20show%20that%20PECANN%0Ais%20two%20orders%20of%20magnitude%20faster.%20As%20far%20as%20we%20know%2C%20our%20work%20is%20the%20first%20to%0Aevaluate%20DPC%20variants%20on%20large%20high-dimensional%20real-world%20image%20and%20text%0Aembedding%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.03940v3&entry.124074799=Read"},
{"title": "Online Performance Assessment of Multi-Source-Localization for\n  Autonomous Driving Systems Using Subjective Logic", "author": "Stefan Orf and Sven Ochs and Marc Ren\u00e9 Zofka and J. Marius Z\u00f6llner", "abstract": "  Autonomous driving (AD) relies heavily on high precision localization as a\ncrucial part of all driving related software components. The precise\npositioning is necessary for the utilization of high-definition maps,\nprediction of other road participants and the controlling of the vehicle\nitself. Due to this reason, the localization is absolutely safety relevant.\nTypical errors of the localization systems, which are long term drifts, jumps\nand false localization, that must be detected to enhance safety. An online\nassessment and evaluation of the current localization performance is a\nchallenging task, which is usually done by Kalman filtering for single\nlocalization systems. Current autonomous vehicles cope with these challenges by\nfusing multiple individual localization methods into an overall state\nestimation. Such approaches need expert knowledge for a competitive performance\nin challenging environments. This expert knowledge is based on the trust and\nthe prioritization of distinct localization methods in respect to the current\nsituation and environment.\n  This work presents a novel online performance assessment technique of\nmultiple localization systems by using subjective logic (SL). In our research\nvehicles, three different systems for localization are available, namely\nodometry-, Simultaneous Localization And Mapping (SLAM)- and Global Navigation\nSatellite System (GNSS)-based. Our performance assessment models the behavior\nof these three localization systems individually and puts them into reference\nof each other. The experiments were carried out using the CoCar NextGen, which\nis based on an Audi A6. The vehicle's localization system was evaluated under\nchallenging conditions, specifically within a tunnel environment. The overall\nevaluation shows the feasibility of our approach.\n", "link": "http://arxiv.org/abs/2506.02932v1", "date": "2025-06-03", "relevancy": 2.3159, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6195}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5799}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Performance%20Assessment%20of%20Multi-Source-Localization%20for%0A%20%20Autonomous%20Driving%20Systems%20Using%20Subjective%20Logic&body=Title%3A%20Online%20Performance%20Assessment%20of%20Multi-Source-Localization%20for%0A%20%20Autonomous%20Driving%20Systems%20Using%20Subjective%20Logic%0AAuthor%3A%20Stefan%20Orf%20and%20Sven%20Ochs%20and%20Marc%20Ren%C3%A9%20Zofka%20and%20J.%20Marius%20Z%C3%B6llner%0AAbstract%3A%20%20%20Autonomous%20driving%20%28AD%29%20relies%20heavily%20on%20high%20precision%20localization%20as%20a%0Acrucial%20part%20of%20all%20driving%20related%20software%20components.%20The%20precise%0Apositioning%20is%20necessary%20for%20the%20utilization%20of%20high-definition%20maps%2C%0Aprediction%20of%20other%20road%20participants%20and%20the%20controlling%20of%20the%20vehicle%0Aitself.%20Due%20to%20this%20reason%2C%20the%20localization%20is%20absolutely%20safety%20relevant.%0ATypical%20errors%20of%20the%20localization%20systems%2C%20which%20are%20long%20term%20drifts%2C%20jumps%0Aand%20false%20localization%2C%20that%20must%20be%20detected%20to%20enhance%20safety.%20An%20online%0Aassessment%20and%20evaluation%20of%20the%20current%20localization%20performance%20is%20a%0Achallenging%20task%2C%20which%20is%20usually%20done%20by%20Kalman%20filtering%20for%20single%0Alocalization%20systems.%20Current%20autonomous%20vehicles%20cope%20with%20these%20challenges%20by%0Afusing%20multiple%20individual%20localization%20methods%20into%20an%20overall%20state%0Aestimation.%20Such%20approaches%20need%20expert%20knowledge%20for%20a%20competitive%20performance%0Ain%20challenging%20environments.%20This%20expert%20knowledge%20is%20based%20on%20the%20trust%20and%0Athe%20prioritization%20of%20distinct%20localization%20methods%20in%20respect%20to%20the%20current%0Asituation%20and%20environment.%0A%20%20This%20work%20presents%20a%20novel%20online%20performance%20assessment%20technique%20of%0Amultiple%20localization%20systems%20by%20using%20subjective%20logic%20%28SL%29.%20In%20our%20research%0Avehicles%2C%20three%20different%20systems%20for%20localization%20are%20available%2C%20namely%0Aodometry-%2C%20Simultaneous%20Localization%20And%20Mapping%20%28SLAM%29-%20and%20Global%20Navigation%0ASatellite%20System%20%28GNSS%29-based.%20Our%20performance%20assessment%20models%20the%20behavior%0Aof%20these%20three%20localization%20systems%20individually%20and%20puts%20them%20into%20reference%0Aof%20each%20other.%20The%20experiments%20were%20carried%20out%20using%20the%20CoCar%20NextGen%2C%20which%0Ais%20based%20on%20an%20Audi%20A6.%20The%20vehicle%27s%20localization%20system%20was%20evaluated%20under%0Achallenging%20conditions%2C%20specifically%20within%20a%20tunnel%20environment.%20The%20overall%0Aevaluation%20shows%20the%20feasibility%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02932v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Performance%2520Assessment%2520of%2520Multi-Source-Localization%2520for%250A%2520%2520Autonomous%2520Driving%2520Systems%2520Using%2520Subjective%2520Logic%26entry.906535625%3DStefan%2520Orf%2520and%2520Sven%2520Ochs%2520and%2520Marc%2520Ren%25C3%25A9%2520Zofka%2520and%2520J.%2520Marius%2520Z%25C3%25B6llner%26entry.1292438233%3D%2520%2520Autonomous%2520driving%2520%2528AD%2529%2520relies%2520heavily%2520on%2520high%2520precision%2520localization%2520as%2520a%250Acrucial%2520part%2520of%2520all%2520driving%2520related%2520software%2520components.%2520The%2520precise%250Apositioning%2520is%2520necessary%2520for%2520the%2520utilization%2520of%2520high-definition%2520maps%252C%250Aprediction%2520of%2520other%2520road%2520participants%2520and%2520the%2520controlling%2520of%2520the%2520vehicle%250Aitself.%2520Due%2520to%2520this%2520reason%252C%2520the%2520localization%2520is%2520absolutely%2520safety%2520relevant.%250ATypical%2520errors%2520of%2520the%2520localization%2520systems%252C%2520which%2520are%2520long%2520term%2520drifts%252C%2520jumps%250Aand%2520false%2520localization%252C%2520that%2520must%2520be%2520detected%2520to%2520enhance%2520safety.%2520An%2520online%250Aassessment%2520and%2520evaluation%2520of%2520the%2520current%2520localization%2520performance%2520is%2520a%250Achallenging%2520task%252C%2520which%2520is%2520usually%2520done%2520by%2520Kalman%2520filtering%2520for%2520single%250Alocalization%2520systems.%2520Current%2520autonomous%2520vehicles%2520cope%2520with%2520these%2520challenges%2520by%250Afusing%2520multiple%2520individual%2520localization%2520methods%2520into%2520an%2520overall%2520state%250Aestimation.%2520Such%2520approaches%2520need%2520expert%2520knowledge%2520for%2520a%2520competitive%2520performance%250Ain%2520challenging%2520environments.%2520This%2520expert%2520knowledge%2520is%2520based%2520on%2520the%2520trust%2520and%250Athe%2520prioritization%2520of%2520distinct%2520localization%2520methods%2520in%2520respect%2520to%2520the%2520current%250Asituation%2520and%2520environment.%250A%2520%2520This%2520work%2520presents%2520a%2520novel%2520online%2520performance%2520assessment%2520technique%2520of%250Amultiple%2520localization%2520systems%2520by%2520using%2520subjective%2520logic%2520%2528SL%2529.%2520In%2520our%2520research%250Avehicles%252C%2520three%2520different%2520systems%2520for%2520localization%2520are%2520available%252C%2520namely%250Aodometry-%252C%2520Simultaneous%2520Localization%2520And%2520Mapping%2520%2528SLAM%2529-%2520and%2520Global%2520Navigation%250ASatellite%2520System%2520%2528GNSS%2529-based.%2520Our%2520performance%2520assessment%2520models%2520the%2520behavior%250Aof%2520these%2520three%2520localization%2520systems%2520individually%2520and%2520puts%2520them%2520into%2520reference%250Aof%2520each%2520other.%2520The%2520experiments%2520were%2520carried%2520out%2520using%2520the%2520CoCar%2520NextGen%252C%2520which%250Ais%2520based%2520on%2520an%2520Audi%2520A6.%2520The%2520vehicle%2527s%2520localization%2520system%2520was%2520evaluated%2520under%250Achallenging%2520conditions%252C%2520specifically%2520within%2520a%2520tunnel%2520environment.%2520The%2520overall%250Aevaluation%2520shows%2520the%2520feasibility%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02932v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Performance%20Assessment%20of%20Multi-Source-Localization%20for%0A%20%20Autonomous%20Driving%20Systems%20Using%20Subjective%20Logic&entry.906535625=Stefan%20Orf%20and%20Sven%20Ochs%20and%20Marc%20Ren%C3%A9%20Zofka%20and%20J.%20Marius%20Z%C3%B6llner&entry.1292438233=%20%20Autonomous%20driving%20%28AD%29%20relies%20heavily%20on%20high%20precision%20localization%20as%20a%0Acrucial%20part%20of%20all%20driving%20related%20software%20components.%20The%20precise%0Apositioning%20is%20necessary%20for%20the%20utilization%20of%20high-definition%20maps%2C%0Aprediction%20of%20other%20road%20participants%20and%20the%20controlling%20of%20the%20vehicle%0Aitself.%20Due%20to%20this%20reason%2C%20the%20localization%20is%20absolutely%20safety%20relevant.%0ATypical%20errors%20of%20the%20localization%20systems%2C%20which%20are%20long%20term%20drifts%2C%20jumps%0Aand%20false%20localization%2C%20that%20must%20be%20detected%20to%20enhance%20safety.%20An%20online%0Aassessment%20and%20evaluation%20of%20the%20current%20localization%20performance%20is%20a%0Achallenging%20task%2C%20which%20is%20usually%20done%20by%20Kalman%20filtering%20for%20single%0Alocalization%20systems.%20Current%20autonomous%20vehicles%20cope%20with%20these%20challenges%20by%0Afusing%20multiple%20individual%20localization%20methods%20into%20an%20overall%20state%0Aestimation.%20Such%20approaches%20need%20expert%20knowledge%20for%20a%20competitive%20performance%0Ain%20challenging%20environments.%20This%20expert%20knowledge%20is%20based%20on%20the%20trust%20and%0Athe%20prioritization%20of%20distinct%20localization%20methods%20in%20respect%20to%20the%20current%0Asituation%20and%20environment.%0A%20%20This%20work%20presents%20a%20novel%20online%20performance%20assessment%20technique%20of%0Amultiple%20localization%20systems%20by%20using%20subjective%20logic%20%28SL%29.%20In%20our%20research%0Avehicles%2C%20three%20different%20systems%20for%20localization%20are%20available%2C%20namely%0Aodometry-%2C%20Simultaneous%20Localization%20And%20Mapping%20%28SLAM%29-%20and%20Global%20Navigation%0ASatellite%20System%20%28GNSS%29-based.%20Our%20performance%20assessment%20models%20the%20behavior%0Aof%20these%20three%20localization%20systems%20individually%20and%20puts%20them%20into%20reference%0Aof%20each%20other.%20The%20experiments%20were%20carried%20out%20using%20the%20CoCar%20NextGen%2C%20which%0Ais%20based%20on%20an%20Audi%20A6.%20The%20vehicle%27s%20localization%20system%20was%20evaluated%20under%0Achallenging%20conditions%2C%20specifically%20within%20a%20tunnel%20environment.%20The%20overall%0Aevaluation%20shows%20the%20feasibility%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02932v1&entry.124074799=Read"},
{"title": "Predictable Reinforcement Learning Dynamics through Entropy Rate\n  Minimization", "author": "Daniel Jarne Ornia and Giannis Delimpaltadakis and Jens Kober and Javier Alonso-Mora", "abstract": "  In Reinforcement Learning (RL), agents have no incentive to exhibit\npredictable behaviors, and are often pushed (through e.g. policy entropy\nregularisation) to randomise their actions in favor of exploration. This often\nmakes it challenging for other agents and humans to predict an agent's\nbehavior, triggering unsafe scenarios (e.g. in human-robot interaction). We\npropose a novel method to induce predictable behavior in RL agents, termed\nPredictability-Aware RL (PARL), employing the agent's trajectory entropy rate\nto quantify predictability. Our method maximizes a linear combination of a\nstandard discounted reward and the negative entropy rate, thus trading off\noptimality with predictability. We show how the entropy rate can be formally\ncast as an average reward, how entropy-rate value functions can be estimated\nfrom a learned model and incorporate this in policy-gradient algorithms, and\ndemonstrate how this approach produces predictable (near-optimal) policies in\ntasks inspired by human-robot use-cases.\n", "link": "http://arxiv.org/abs/2311.18703v5", "date": "2025-06-03", "relevancy": 1.4368, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5497}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4599}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predictable%20Reinforcement%20Learning%20Dynamics%20through%20Entropy%20Rate%0A%20%20Minimization&body=Title%3A%20Predictable%20Reinforcement%20Learning%20Dynamics%20through%20Entropy%20Rate%0A%20%20Minimization%0AAuthor%3A%20Daniel%20Jarne%20Ornia%20and%20Giannis%20Delimpaltadakis%20and%20Jens%20Kober%20and%20Javier%20Alonso-Mora%0AAbstract%3A%20%20%20In%20Reinforcement%20Learning%20%28RL%29%2C%20agents%20have%20no%20incentive%20to%20exhibit%0Apredictable%20behaviors%2C%20and%20are%20often%20pushed%20%28through%20e.g.%20policy%20entropy%0Aregularisation%29%20to%20randomise%20their%20actions%20in%20favor%20of%20exploration.%20This%20often%0Amakes%20it%20challenging%20for%20other%20agents%20and%20humans%20to%20predict%20an%20agent%27s%0Abehavior%2C%20triggering%20unsafe%20scenarios%20%28e.g.%20in%20human-robot%20interaction%29.%20We%0Apropose%20a%20novel%20method%20to%20induce%20predictable%20behavior%20in%20RL%20agents%2C%20termed%0APredictability-Aware%20RL%20%28PARL%29%2C%20employing%20the%20agent%27s%20trajectory%20entropy%20rate%0Ato%20quantify%20predictability.%20Our%20method%20maximizes%20a%20linear%20combination%20of%20a%0Astandard%20discounted%20reward%20and%20the%20negative%20entropy%20rate%2C%20thus%20trading%20off%0Aoptimality%20with%20predictability.%20We%20show%20how%20the%20entropy%20rate%20can%20be%20formally%0Acast%20as%20an%20average%20reward%2C%20how%20entropy-rate%20value%20functions%20can%20be%20estimated%0Afrom%20a%20learned%20model%20and%20incorporate%20this%20in%20policy-gradient%20algorithms%2C%20and%0Ademonstrate%20how%20this%20approach%20produces%20predictable%20%28near-optimal%29%20policies%20in%0Atasks%20inspired%20by%20human-robot%20use-cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.18703v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredictable%2520Reinforcement%2520Learning%2520Dynamics%2520through%2520Entropy%2520Rate%250A%2520%2520Minimization%26entry.906535625%3DDaniel%2520Jarne%2520Ornia%2520and%2520Giannis%2520Delimpaltadakis%2520and%2520Jens%2520Kober%2520and%2520Javier%2520Alonso-Mora%26entry.1292438233%3D%2520%2520In%2520Reinforcement%2520Learning%2520%2528RL%2529%252C%2520agents%2520have%2520no%2520incentive%2520to%2520exhibit%250Apredictable%2520behaviors%252C%2520and%2520are%2520often%2520pushed%2520%2528through%2520e.g.%2520policy%2520entropy%250Aregularisation%2529%2520to%2520randomise%2520their%2520actions%2520in%2520favor%2520of%2520exploration.%2520This%2520often%250Amakes%2520it%2520challenging%2520for%2520other%2520agents%2520and%2520humans%2520to%2520predict%2520an%2520agent%2527s%250Abehavior%252C%2520triggering%2520unsafe%2520scenarios%2520%2528e.g.%2520in%2520human-robot%2520interaction%2529.%2520We%250Apropose%2520a%2520novel%2520method%2520to%2520induce%2520predictable%2520behavior%2520in%2520RL%2520agents%252C%2520termed%250APredictability-Aware%2520RL%2520%2528PARL%2529%252C%2520employing%2520the%2520agent%2527s%2520trajectory%2520entropy%2520rate%250Ato%2520quantify%2520predictability.%2520Our%2520method%2520maximizes%2520a%2520linear%2520combination%2520of%2520a%250Astandard%2520discounted%2520reward%2520and%2520the%2520negative%2520entropy%2520rate%252C%2520thus%2520trading%2520off%250Aoptimality%2520with%2520predictability.%2520We%2520show%2520how%2520the%2520entropy%2520rate%2520can%2520be%2520formally%250Acast%2520as%2520an%2520average%2520reward%252C%2520how%2520entropy-rate%2520value%2520functions%2520can%2520be%2520estimated%250Afrom%2520a%2520learned%2520model%2520and%2520incorporate%2520this%2520in%2520policy-gradient%2520algorithms%252C%2520and%250Ademonstrate%2520how%2520this%2520approach%2520produces%2520predictable%2520%2528near-optimal%2529%2520policies%2520in%250Atasks%2520inspired%2520by%2520human-robot%2520use-cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.18703v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predictable%20Reinforcement%20Learning%20Dynamics%20through%20Entropy%20Rate%0A%20%20Minimization&entry.906535625=Daniel%20Jarne%20Ornia%20and%20Giannis%20Delimpaltadakis%20and%20Jens%20Kober%20and%20Javier%20Alonso-Mora&entry.1292438233=%20%20In%20Reinforcement%20Learning%20%28RL%29%2C%20agents%20have%20no%20incentive%20to%20exhibit%0Apredictable%20behaviors%2C%20and%20are%20often%20pushed%20%28through%20e.g.%20policy%20entropy%0Aregularisation%29%20to%20randomise%20their%20actions%20in%20favor%20of%20exploration.%20This%20often%0Amakes%20it%20challenging%20for%20other%20agents%20and%20humans%20to%20predict%20an%20agent%27s%0Abehavior%2C%20triggering%20unsafe%20scenarios%20%28e.g.%20in%20human-robot%20interaction%29.%20We%0Apropose%20a%20novel%20method%20to%20induce%20predictable%20behavior%20in%20RL%20agents%2C%20termed%0APredictability-Aware%20RL%20%28PARL%29%2C%20employing%20the%20agent%27s%20trajectory%20entropy%20rate%0Ato%20quantify%20predictability.%20Our%20method%20maximizes%20a%20linear%20combination%20of%20a%0Astandard%20discounted%20reward%20and%20the%20negative%20entropy%20rate%2C%20thus%20trading%20off%0Aoptimality%20with%20predictability.%20We%20show%20how%20the%20entropy%20rate%20can%20be%20formally%0Acast%20as%20an%20average%20reward%2C%20how%20entropy-rate%20value%20functions%20can%20be%20estimated%0Afrom%20a%20learned%20model%20and%20incorporate%20this%20in%20policy-gradient%20algorithms%2C%20and%0Ademonstrate%20how%20this%20approach%20produces%20predictable%20%28near-optimal%29%20policies%20in%0Atasks%20inspired%20by%20human-robot%20use-cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.18703v5&entry.124074799=Read"},
{"title": "Solving the Pod Repositioning Problem with Deep Reinforced Adaptive\n  Large Neighborhood Search", "author": "Lin Xie and Hanyi Li", "abstract": "  The Pod Repositioning Problem (PRP) in Robotic Mobile Fulfillment Systems\n(RMFS) involves selecting optimal storage locations for pods returning from\npick stations. This work presents an improved solution method that integrates\nAdaptive Large Neighborhood Search (ALNS) with Deep Reinforcement Learning\n(DRL). A DRL agent dynamically selects destroy and repair operators and adjusts\nkey parameters such as destruction degree and acceptance thresholds during the\nsearch. Specialized heuristics for both operators are designed to reflect\nPRP-specific characteristics, including pod usage frequency and movement costs.\nComputational results show that this DRL-guided ALNS outperforms traditional\napproaches such as cheapest-place, fixed-place, binary integer programming, and\nstatic heuristics. The method demonstrates strong solution quality and\nillustrating the benefit of learning-driven control within combinatorial\noptimization for warehouse systems.\n", "link": "http://arxiv.org/abs/2506.02746v1", "date": "2025-06-03", "relevancy": 1.522, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5317}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5194}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4927}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solving%20the%20Pod%20Repositioning%20Problem%20with%20Deep%20Reinforced%20Adaptive%0A%20%20Large%20Neighborhood%20Search&body=Title%3A%20Solving%20the%20Pod%20Repositioning%20Problem%20with%20Deep%20Reinforced%20Adaptive%0A%20%20Large%20Neighborhood%20Search%0AAuthor%3A%20Lin%20Xie%20and%20Hanyi%20Li%0AAbstract%3A%20%20%20The%20Pod%20Repositioning%20Problem%20%28PRP%29%20in%20Robotic%20Mobile%20Fulfillment%20Systems%0A%28RMFS%29%20involves%20selecting%20optimal%20storage%20locations%20for%20pods%20returning%20from%0Apick%20stations.%20This%20work%20presents%20an%20improved%20solution%20method%20that%20integrates%0AAdaptive%20Large%20Neighborhood%20Search%20%28ALNS%29%20with%20Deep%20Reinforcement%20Learning%0A%28DRL%29.%20A%20DRL%20agent%20dynamically%20selects%20destroy%20and%20repair%20operators%20and%20adjusts%0Akey%20parameters%20such%20as%20destruction%20degree%20and%20acceptance%20thresholds%20during%20the%0Asearch.%20Specialized%20heuristics%20for%20both%20operators%20are%20designed%20to%20reflect%0APRP-specific%20characteristics%2C%20including%20pod%20usage%20frequency%20and%20movement%20costs.%0AComputational%20results%20show%20that%20this%20DRL-guided%20ALNS%20outperforms%20traditional%0Aapproaches%20such%20as%20cheapest-place%2C%20fixed-place%2C%20binary%20integer%20programming%2C%20and%0Astatic%20heuristics.%20The%20method%20demonstrates%20strong%20solution%20quality%20and%0Aillustrating%20the%20benefit%20of%20learning-driven%20control%20within%20combinatorial%0Aoptimization%20for%20warehouse%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02746v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolving%2520the%2520Pod%2520Repositioning%2520Problem%2520with%2520Deep%2520Reinforced%2520Adaptive%250A%2520%2520Large%2520Neighborhood%2520Search%26entry.906535625%3DLin%2520Xie%2520and%2520Hanyi%2520Li%26entry.1292438233%3D%2520%2520The%2520Pod%2520Repositioning%2520Problem%2520%2528PRP%2529%2520in%2520Robotic%2520Mobile%2520Fulfillment%2520Systems%250A%2528RMFS%2529%2520involves%2520selecting%2520optimal%2520storage%2520locations%2520for%2520pods%2520returning%2520from%250Apick%2520stations.%2520This%2520work%2520presents%2520an%2520improved%2520solution%2520method%2520that%2520integrates%250AAdaptive%2520Large%2520Neighborhood%2520Search%2520%2528ALNS%2529%2520with%2520Deep%2520Reinforcement%2520Learning%250A%2528DRL%2529.%2520A%2520DRL%2520agent%2520dynamically%2520selects%2520destroy%2520and%2520repair%2520operators%2520and%2520adjusts%250Akey%2520parameters%2520such%2520as%2520destruction%2520degree%2520and%2520acceptance%2520thresholds%2520during%2520the%250Asearch.%2520Specialized%2520heuristics%2520for%2520both%2520operators%2520are%2520designed%2520to%2520reflect%250APRP-specific%2520characteristics%252C%2520including%2520pod%2520usage%2520frequency%2520and%2520movement%2520costs.%250AComputational%2520results%2520show%2520that%2520this%2520DRL-guided%2520ALNS%2520outperforms%2520traditional%250Aapproaches%2520such%2520as%2520cheapest-place%252C%2520fixed-place%252C%2520binary%2520integer%2520programming%252C%2520and%250Astatic%2520heuristics.%2520The%2520method%2520demonstrates%2520strong%2520solution%2520quality%2520and%250Aillustrating%2520the%2520benefit%2520of%2520learning-driven%2520control%2520within%2520combinatorial%250Aoptimization%2520for%2520warehouse%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02746v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20the%20Pod%20Repositioning%20Problem%20with%20Deep%20Reinforced%20Adaptive%0A%20%20Large%20Neighborhood%20Search&entry.906535625=Lin%20Xie%20and%20Hanyi%20Li&entry.1292438233=%20%20The%20Pod%20Repositioning%20Problem%20%28PRP%29%20in%20Robotic%20Mobile%20Fulfillment%20Systems%0A%28RMFS%29%20involves%20selecting%20optimal%20storage%20locations%20for%20pods%20returning%20from%0Apick%20stations.%20This%20work%20presents%20an%20improved%20solution%20method%20that%20integrates%0AAdaptive%20Large%20Neighborhood%20Search%20%28ALNS%29%20with%20Deep%20Reinforcement%20Learning%0A%28DRL%29.%20A%20DRL%20agent%20dynamically%20selects%20destroy%20and%20repair%20operators%20and%20adjusts%0Akey%20parameters%20such%20as%20destruction%20degree%20and%20acceptance%20thresholds%20during%20the%0Asearch.%20Specialized%20heuristics%20for%20both%20operators%20are%20designed%20to%20reflect%0APRP-specific%20characteristics%2C%20including%20pod%20usage%20frequency%20and%20movement%20costs.%0AComputational%20results%20show%20that%20this%20DRL-guided%20ALNS%20outperforms%20traditional%0Aapproaches%20such%20as%20cheapest-place%2C%20fixed-place%2C%20binary%20integer%20programming%2C%20and%0Astatic%20heuristics.%20The%20method%20demonstrates%20strong%20solution%20quality%20and%0Aillustrating%20the%20benefit%20of%20learning-driven%20control%20within%20combinatorial%0Aoptimization%20for%20warehouse%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02746v1&entry.124074799=Read"},
{"title": "TestAgent: An Adaptive and Intelligent Expert for Human Assessment", "author": "Junhao Yu and Yan Zhuang and YuXuan Sun and Weibo Gao and Qi Liu and Mingyue Cheng and Zhenya Huang and Enhong Chen", "abstract": "  Accurately assessing internal human states is key to understanding\npreferences, offering personalized services, and identifying challenges in\nreal-world applications. Originating from psychometrics, adaptive testing has\nbecome the mainstream method for human measurement and has now been widely\napplied in education, healthcare, sports, and sociology. It customizes\nassessments by selecting the fewest test questions . However, current adaptive\ntesting methods face several challenges. The mechanized nature of most\nalgorithms leads to guessing behavior and difficulties with open-ended\nquestions. Additionally, subjective assessments suffer from noisy response data\nand coarse-grained test outputs, further limiting their effectiveness. To move\ncloser to an ideal adaptive testing process, we propose TestAgent, a large\nlanguage model (LLM)-powered agent designed to enhance adaptive testing through\ninteractive engagement. This is the first application of LLMs in adaptive\ntesting. TestAgent supports personalized question selection, captures\ntest-takers' responses and anomalies, and provides precise outcomes through\ndynamic, conversational interactions. Experiments on psychological,\neducational, and lifestyle assessments show our approach achieves more accurate\nresults with 20% fewer questions than state-of-the-art baselines, and testers\npreferred it in speed, smoothness, and other dimensions.\n", "link": "http://arxiv.org/abs/2506.03032v1", "date": "2025-06-03", "relevancy": 1.5059, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5125}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4913}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TestAgent%3A%20An%20Adaptive%20and%20Intelligent%20Expert%20for%20Human%20Assessment&body=Title%3A%20TestAgent%3A%20An%20Adaptive%20and%20Intelligent%20Expert%20for%20Human%20Assessment%0AAuthor%3A%20Junhao%20Yu%20and%20Yan%20Zhuang%20and%20YuXuan%20Sun%20and%20Weibo%20Gao%20and%20Qi%20Liu%20and%20Mingyue%20Cheng%20and%20Zhenya%20Huang%20and%20Enhong%20Chen%0AAbstract%3A%20%20%20Accurately%20assessing%20internal%20human%20states%20is%20key%20to%20understanding%0Apreferences%2C%20offering%20personalized%20services%2C%20and%20identifying%20challenges%20in%0Areal-world%20applications.%20Originating%20from%20psychometrics%2C%20adaptive%20testing%20has%0Abecome%20the%20mainstream%20method%20for%20human%20measurement%20and%20has%20now%20been%20widely%0Aapplied%20in%20education%2C%20healthcare%2C%20sports%2C%20and%20sociology.%20It%20customizes%0Aassessments%20by%20selecting%20the%20fewest%20test%20questions%20.%20However%2C%20current%20adaptive%0Atesting%20methods%20face%20several%20challenges.%20The%20mechanized%20nature%20of%20most%0Aalgorithms%20leads%20to%20guessing%20behavior%20and%20difficulties%20with%20open-ended%0Aquestions.%20Additionally%2C%20subjective%20assessments%20suffer%20from%20noisy%20response%20data%0Aand%20coarse-grained%20test%20outputs%2C%20further%20limiting%20their%20effectiveness.%20To%20move%0Acloser%20to%20an%20ideal%20adaptive%20testing%20process%2C%20we%20propose%20TestAgent%2C%20a%20large%0Alanguage%20model%20%28LLM%29-powered%20agent%20designed%20to%20enhance%20adaptive%20testing%20through%0Ainteractive%20engagement.%20This%20is%20the%20first%20application%20of%20LLMs%20in%20adaptive%0Atesting.%20TestAgent%20supports%20personalized%20question%20selection%2C%20captures%0Atest-takers%27%20responses%20and%20anomalies%2C%20and%20provides%20precise%20outcomes%20through%0Adynamic%2C%20conversational%20interactions.%20Experiments%20on%20psychological%2C%0Aeducational%2C%20and%20lifestyle%20assessments%20show%20our%20approach%20achieves%20more%20accurate%0Aresults%20with%2020%25%20fewer%20questions%20than%20state-of-the-art%20baselines%2C%20and%20testers%0Apreferred%20it%20in%20speed%2C%20smoothness%2C%20and%20other%20dimensions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03032v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTestAgent%253A%2520An%2520Adaptive%2520and%2520Intelligent%2520Expert%2520for%2520Human%2520Assessment%26entry.906535625%3DJunhao%2520Yu%2520and%2520Yan%2520Zhuang%2520and%2520YuXuan%2520Sun%2520and%2520Weibo%2520Gao%2520and%2520Qi%2520Liu%2520and%2520Mingyue%2520Cheng%2520and%2520Zhenya%2520Huang%2520and%2520Enhong%2520Chen%26entry.1292438233%3D%2520%2520Accurately%2520assessing%2520internal%2520human%2520states%2520is%2520key%2520to%2520understanding%250Apreferences%252C%2520offering%2520personalized%2520services%252C%2520and%2520identifying%2520challenges%2520in%250Areal-world%2520applications.%2520Originating%2520from%2520psychometrics%252C%2520adaptive%2520testing%2520has%250Abecome%2520the%2520mainstream%2520method%2520for%2520human%2520measurement%2520and%2520has%2520now%2520been%2520widely%250Aapplied%2520in%2520education%252C%2520healthcare%252C%2520sports%252C%2520and%2520sociology.%2520It%2520customizes%250Aassessments%2520by%2520selecting%2520the%2520fewest%2520test%2520questions%2520.%2520However%252C%2520current%2520adaptive%250Atesting%2520methods%2520face%2520several%2520challenges.%2520The%2520mechanized%2520nature%2520of%2520most%250Aalgorithms%2520leads%2520to%2520guessing%2520behavior%2520and%2520difficulties%2520with%2520open-ended%250Aquestions.%2520Additionally%252C%2520subjective%2520assessments%2520suffer%2520from%2520noisy%2520response%2520data%250Aand%2520coarse-grained%2520test%2520outputs%252C%2520further%2520limiting%2520their%2520effectiveness.%2520To%2520move%250Acloser%2520to%2520an%2520ideal%2520adaptive%2520testing%2520process%252C%2520we%2520propose%2520TestAgent%252C%2520a%2520large%250Alanguage%2520model%2520%2528LLM%2529-powered%2520agent%2520designed%2520to%2520enhance%2520adaptive%2520testing%2520through%250Ainteractive%2520engagement.%2520This%2520is%2520the%2520first%2520application%2520of%2520LLMs%2520in%2520adaptive%250Atesting.%2520TestAgent%2520supports%2520personalized%2520question%2520selection%252C%2520captures%250Atest-takers%2527%2520responses%2520and%2520anomalies%252C%2520and%2520provides%2520precise%2520outcomes%2520through%250Adynamic%252C%2520conversational%2520interactions.%2520Experiments%2520on%2520psychological%252C%250Aeducational%252C%2520and%2520lifestyle%2520assessments%2520show%2520our%2520approach%2520achieves%2520more%2520accurate%250Aresults%2520with%252020%2525%2520fewer%2520questions%2520than%2520state-of-the-art%2520baselines%252C%2520and%2520testers%250Apreferred%2520it%2520in%2520speed%252C%2520smoothness%252C%2520and%2520other%2520dimensions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03032v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TestAgent%3A%20An%20Adaptive%20and%20Intelligent%20Expert%20for%20Human%20Assessment&entry.906535625=Junhao%20Yu%20and%20Yan%20Zhuang%20and%20YuXuan%20Sun%20and%20Weibo%20Gao%20and%20Qi%20Liu%20and%20Mingyue%20Cheng%20and%20Zhenya%20Huang%20and%20Enhong%20Chen&entry.1292438233=%20%20Accurately%20assessing%20internal%20human%20states%20is%20key%20to%20understanding%0Apreferences%2C%20offering%20personalized%20services%2C%20and%20identifying%20challenges%20in%0Areal-world%20applications.%20Originating%20from%20psychometrics%2C%20adaptive%20testing%20has%0Abecome%20the%20mainstream%20method%20for%20human%20measurement%20and%20has%20now%20been%20widely%0Aapplied%20in%20education%2C%20healthcare%2C%20sports%2C%20and%20sociology.%20It%20customizes%0Aassessments%20by%20selecting%20the%20fewest%20test%20questions%20.%20However%2C%20current%20adaptive%0Atesting%20methods%20face%20several%20challenges.%20The%20mechanized%20nature%20of%20most%0Aalgorithms%20leads%20to%20guessing%20behavior%20and%20difficulties%20with%20open-ended%0Aquestions.%20Additionally%2C%20subjective%20assessments%20suffer%20from%20noisy%20response%20data%0Aand%20coarse-grained%20test%20outputs%2C%20further%20limiting%20their%20effectiveness.%20To%20move%0Acloser%20to%20an%20ideal%20adaptive%20testing%20process%2C%20we%20propose%20TestAgent%2C%20a%20large%0Alanguage%20model%20%28LLM%29-powered%20agent%20designed%20to%20enhance%20adaptive%20testing%20through%0Ainteractive%20engagement.%20This%20is%20the%20first%20application%20of%20LLMs%20in%20adaptive%0Atesting.%20TestAgent%20supports%20personalized%20question%20selection%2C%20captures%0Atest-takers%27%20responses%20and%20anomalies%2C%20and%20provides%20precise%20outcomes%20through%0Adynamic%2C%20conversational%20interactions.%20Experiments%20on%20psychological%2C%0Aeducational%2C%20and%20lifestyle%20assessments%20show%20our%20approach%20achieves%20more%20accurate%0Aresults%20with%2020%25%20fewer%20questions%20than%20state-of-the-art%20baselines%2C%20and%20testers%0Apreferred%20it%20in%20speed%2C%20smoothness%2C%20and%20other%20dimensions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03032v1&entry.124074799=Read"},
{"title": "How do Pre-Trained Models Support Software Engineering? An Empirical\n  Study in Hugging Face", "author": "Alexandra Gonz\u00e1lez and Xavier Franch and David Lo and Silverio Mart\u00ednez-Fern\u00e1ndez", "abstract": "  Open-Source Pre-Trained Models (PTMs) provide extensive resources for various\nMachine Learning (ML) tasks, yet these resources lack a classification tailored\nto Software Engineering (SE) needs. To address this gap, we derive a taxonomy\nencompassing 147 SE tasks and apply an SE-oriented classification to PTMs in a\npopular open-source ML repository, Hugging Face (HF). Our repository mining\nstudy began with a systematically gathered database of PTMs from the HF API,\nconsidering their model card descriptions and metadata, and the abstract of the\nassociated arXiv papers. We confirmed SE relevance through multiple filtering\nsteps: detecting outliers, identifying near-identical PTMs, and the use of\nGemini 2.0 Flash, which was validated with five pilot studies involving three\nhuman annotators. This approach uncovered 2,205 SE PTMs. We find that code\ngeneration is the most common SE task among PTMs, primarily focusing on\nsoftware implementation, while requirements engineering and software design\nactivities receive limited attention. In terms of ML tasks, text generation\ndominates within SE PTMs. Notably, the number of SE PTMs has increased markedly\nsince 2023 Q2. Our classification provides a solid foundation for future\nautomated SE scenarios, such as the sampling and selection of suitable PTMs.\n", "link": "http://arxiv.org/abs/2506.03013v1", "date": "2025-06-03", "relevancy": 1.9013, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4833}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4737}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20do%20Pre-Trained%20Models%20Support%20Software%20Engineering%3F%20An%20Empirical%0A%20%20Study%20in%20Hugging%20Face&body=Title%3A%20How%20do%20Pre-Trained%20Models%20Support%20Software%20Engineering%3F%20An%20Empirical%0A%20%20Study%20in%20Hugging%20Face%0AAuthor%3A%20Alexandra%20Gonz%C3%A1lez%20and%20Xavier%20Franch%20and%20David%20Lo%20and%20Silverio%20Mart%C3%ADnez-Fern%C3%A1ndez%0AAbstract%3A%20%20%20Open-Source%20Pre-Trained%20Models%20%28PTMs%29%20provide%20extensive%20resources%20for%20various%0AMachine%20Learning%20%28ML%29%20tasks%2C%20yet%20these%20resources%20lack%20a%20classification%20tailored%0Ato%20Software%20Engineering%20%28SE%29%20needs.%20To%20address%20this%20gap%2C%20we%20derive%20a%20taxonomy%0Aencompassing%20147%20SE%20tasks%20and%20apply%20an%20SE-oriented%20classification%20to%20PTMs%20in%20a%0Apopular%20open-source%20ML%20repository%2C%20Hugging%20Face%20%28HF%29.%20Our%20repository%20mining%0Astudy%20began%20with%20a%20systematically%20gathered%20database%20of%20PTMs%20from%20the%20HF%20API%2C%0Aconsidering%20their%20model%20card%20descriptions%20and%20metadata%2C%20and%20the%20abstract%20of%20the%0Aassociated%20arXiv%20papers.%20We%20confirmed%20SE%20relevance%20through%20multiple%20filtering%0Asteps%3A%20detecting%20outliers%2C%20identifying%20near-identical%20PTMs%2C%20and%20the%20use%20of%0AGemini%202.0%20Flash%2C%20which%20was%20validated%20with%20five%20pilot%20studies%20involving%20three%0Ahuman%20annotators.%20This%20approach%20uncovered%202%2C205%20SE%20PTMs.%20We%20find%20that%20code%0Ageneration%20is%20the%20most%20common%20SE%20task%20among%20PTMs%2C%20primarily%20focusing%20on%0Asoftware%20implementation%2C%20while%20requirements%20engineering%20and%20software%20design%0Aactivities%20receive%20limited%20attention.%20In%20terms%20of%20ML%20tasks%2C%20text%20generation%0Adominates%20within%20SE%20PTMs.%20Notably%2C%20the%20number%20of%20SE%20PTMs%20has%20increased%20markedly%0Asince%202023%20Q2.%20Our%20classification%20provides%20a%20solid%20foundation%20for%20future%0Aautomated%20SE%20scenarios%2C%20such%20as%20the%20sampling%20and%20selection%20of%20suitable%20PTMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03013v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520do%2520Pre-Trained%2520Models%2520Support%2520Software%2520Engineering%253F%2520An%2520Empirical%250A%2520%2520Study%2520in%2520Hugging%2520Face%26entry.906535625%3DAlexandra%2520Gonz%25C3%25A1lez%2520and%2520Xavier%2520Franch%2520and%2520David%2520Lo%2520and%2520Silverio%2520Mart%25C3%25ADnez-Fern%25C3%25A1ndez%26entry.1292438233%3D%2520%2520Open-Source%2520Pre-Trained%2520Models%2520%2528PTMs%2529%2520provide%2520extensive%2520resources%2520for%2520various%250AMachine%2520Learning%2520%2528ML%2529%2520tasks%252C%2520yet%2520these%2520resources%2520lack%2520a%2520classification%2520tailored%250Ato%2520Software%2520Engineering%2520%2528SE%2529%2520needs.%2520To%2520address%2520this%2520gap%252C%2520we%2520derive%2520a%2520taxonomy%250Aencompassing%2520147%2520SE%2520tasks%2520and%2520apply%2520an%2520SE-oriented%2520classification%2520to%2520PTMs%2520in%2520a%250Apopular%2520open-source%2520ML%2520repository%252C%2520Hugging%2520Face%2520%2528HF%2529.%2520Our%2520repository%2520mining%250Astudy%2520began%2520with%2520a%2520systematically%2520gathered%2520database%2520of%2520PTMs%2520from%2520the%2520HF%2520API%252C%250Aconsidering%2520their%2520model%2520card%2520descriptions%2520and%2520metadata%252C%2520and%2520the%2520abstract%2520of%2520the%250Aassociated%2520arXiv%2520papers.%2520We%2520confirmed%2520SE%2520relevance%2520through%2520multiple%2520filtering%250Asteps%253A%2520detecting%2520outliers%252C%2520identifying%2520near-identical%2520PTMs%252C%2520and%2520the%2520use%2520of%250AGemini%25202.0%2520Flash%252C%2520which%2520was%2520validated%2520with%2520five%2520pilot%2520studies%2520involving%2520three%250Ahuman%2520annotators.%2520This%2520approach%2520uncovered%25202%252C205%2520SE%2520PTMs.%2520We%2520find%2520that%2520code%250Ageneration%2520is%2520the%2520most%2520common%2520SE%2520task%2520among%2520PTMs%252C%2520primarily%2520focusing%2520on%250Asoftware%2520implementation%252C%2520while%2520requirements%2520engineering%2520and%2520software%2520design%250Aactivities%2520receive%2520limited%2520attention.%2520In%2520terms%2520of%2520ML%2520tasks%252C%2520text%2520generation%250Adominates%2520within%2520SE%2520PTMs.%2520Notably%252C%2520the%2520number%2520of%2520SE%2520PTMs%2520has%2520increased%2520markedly%250Asince%25202023%2520Q2.%2520Our%2520classification%2520provides%2520a%2520solid%2520foundation%2520for%2520future%250Aautomated%2520SE%2520scenarios%252C%2520such%2520as%2520the%2520sampling%2520and%2520selection%2520of%2520suitable%2520PTMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03013v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20do%20Pre-Trained%20Models%20Support%20Software%20Engineering%3F%20An%20Empirical%0A%20%20Study%20in%20Hugging%20Face&entry.906535625=Alexandra%20Gonz%C3%A1lez%20and%20Xavier%20Franch%20and%20David%20Lo%20and%20Silverio%20Mart%C3%ADnez-Fern%C3%A1ndez&entry.1292438233=%20%20Open-Source%20Pre-Trained%20Models%20%28PTMs%29%20provide%20extensive%20resources%20for%20various%0AMachine%20Learning%20%28ML%29%20tasks%2C%20yet%20these%20resources%20lack%20a%20classification%20tailored%0Ato%20Software%20Engineering%20%28SE%29%20needs.%20To%20address%20this%20gap%2C%20we%20derive%20a%20taxonomy%0Aencompassing%20147%20SE%20tasks%20and%20apply%20an%20SE-oriented%20classification%20to%20PTMs%20in%20a%0Apopular%20open-source%20ML%20repository%2C%20Hugging%20Face%20%28HF%29.%20Our%20repository%20mining%0Astudy%20began%20with%20a%20systematically%20gathered%20database%20of%20PTMs%20from%20the%20HF%20API%2C%0Aconsidering%20their%20model%20card%20descriptions%20and%20metadata%2C%20and%20the%20abstract%20of%20the%0Aassociated%20arXiv%20papers.%20We%20confirmed%20SE%20relevance%20through%20multiple%20filtering%0Asteps%3A%20detecting%20outliers%2C%20identifying%20near-identical%20PTMs%2C%20and%20the%20use%20of%0AGemini%202.0%20Flash%2C%20which%20was%20validated%20with%20five%20pilot%20studies%20involving%20three%0Ahuman%20annotators.%20This%20approach%20uncovered%202%2C205%20SE%20PTMs.%20We%20find%20that%20code%0Ageneration%20is%20the%20most%20common%20SE%20task%20among%20PTMs%2C%20primarily%20focusing%20on%0Asoftware%20implementation%2C%20while%20requirements%20engineering%20and%20software%20design%0Aactivities%20receive%20limited%20attention.%20In%20terms%20of%20ML%20tasks%2C%20text%20generation%0Adominates%20within%20SE%20PTMs.%20Notably%2C%20the%20number%20of%20SE%20PTMs%20has%20increased%20markedly%0Asince%202023%20Q2.%20Our%20classification%20provides%20a%20solid%20foundation%20for%20future%0Aautomated%20SE%20scenarios%2C%20such%20as%20the%20sampling%20and%20selection%20of%20suitable%20PTMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03013v1&entry.124074799=Read"},
{"title": "DPO Learning with LLMs-Judge Signal for Computer Use Agents", "author": "Man Luo and David Cobbley and Xin Su and Shachar Rosenman and Vasudev Lal and Shao-Yen Tseng and Phillip Howard", "abstract": "  Computer use agents (CUA) are systems that automatically interact with\ngraphical user interfaces (GUIs) to complete tasks. CUA have made significant\nprogress with the advent of large vision-language models (VLMs). However, these\nagents typically rely on cloud-based inference with substantial compute\ndemands, raising critical privacy and scalability concerns, especially when\noperating on personal devices. In this work, we take a step toward\nprivacy-preserving and resource-efficient agents by developing a lightweight\nvision-language model that runs entirely on local machines. To train this\ncompact agent, we introduce an LLM-as-Judge framework that automatically\nevaluates and filters synthetic interaction trajectories, producing\nhigh-quality data for reinforcement learning without human annotation.\nExperiments on the OS-World benchmark demonstrate that our fine-tuned local\nmodel outperforms existing baselines, highlighting a promising path toward\nprivate, efficient, and generalizable GUI agents.\n", "link": "http://arxiv.org/abs/2506.03095v1", "date": "2025-06-03", "relevancy": 1.5863, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5411}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5337}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DPO%20Learning%20with%20LLMs-Judge%20Signal%20for%20Computer%20Use%20Agents&body=Title%3A%20DPO%20Learning%20with%20LLMs-Judge%20Signal%20for%20Computer%20Use%20Agents%0AAuthor%3A%20Man%20Luo%20and%20David%20Cobbley%20and%20Xin%20Su%20and%20Shachar%20Rosenman%20and%20Vasudev%20Lal%20and%20Shao-Yen%20Tseng%20and%20Phillip%20Howard%0AAbstract%3A%20%20%20Computer%20use%20agents%20%28CUA%29%20are%20systems%20that%20automatically%20interact%20with%0Agraphical%20user%20interfaces%20%28GUIs%29%20to%20complete%20tasks.%20CUA%20have%20made%20significant%0Aprogress%20with%20the%20advent%20of%20large%20vision-language%20models%20%28VLMs%29.%20However%2C%20these%0Aagents%20typically%20rely%20on%20cloud-based%20inference%20with%20substantial%20compute%0Ademands%2C%20raising%20critical%20privacy%20and%20scalability%20concerns%2C%20especially%20when%0Aoperating%20on%20personal%20devices.%20In%20this%20work%2C%20we%20take%20a%20step%20toward%0Aprivacy-preserving%20and%20resource-efficient%20agents%20by%20developing%20a%20lightweight%0Avision-language%20model%20that%20runs%20entirely%20on%20local%20machines.%20To%20train%20this%0Acompact%20agent%2C%20we%20introduce%20an%20LLM-as-Judge%20framework%20that%20automatically%0Aevaluates%20and%20filters%20synthetic%20interaction%20trajectories%2C%20producing%0Ahigh-quality%20data%20for%20reinforcement%20learning%20without%20human%20annotation.%0AExperiments%20on%20the%20OS-World%20benchmark%20demonstrate%20that%20our%20fine-tuned%20local%0Amodel%20outperforms%20existing%20baselines%2C%20highlighting%20a%20promising%20path%20toward%0Aprivate%2C%20efficient%2C%20and%20generalizable%20GUI%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03095v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDPO%2520Learning%2520with%2520LLMs-Judge%2520Signal%2520for%2520Computer%2520Use%2520Agents%26entry.906535625%3DMan%2520Luo%2520and%2520David%2520Cobbley%2520and%2520Xin%2520Su%2520and%2520Shachar%2520Rosenman%2520and%2520Vasudev%2520Lal%2520and%2520Shao-Yen%2520Tseng%2520and%2520Phillip%2520Howard%26entry.1292438233%3D%2520%2520Computer%2520use%2520agents%2520%2528CUA%2529%2520are%2520systems%2520that%2520automatically%2520interact%2520with%250Agraphical%2520user%2520interfaces%2520%2528GUIs%2529%2520to%2520complete%2520tasks.%2520CUA%2520have%2520made%2520significant%250Aprogress%2520with%2520the%2520advent%2520of%2520large%2520vision-language%2520models%2520%2528VLMs%2529.%2520However%252C%2520these%250Aagents%2520typically%2520rely%2520on%2520cloud-based%2520inference%2520with%2520substantial%2520compute%250Ademands%252C%2520raising%2520critical%2520privacy%2520and%2520scalability%2520concerns%252C%2520especially%2520when%250Aoperating%2520on%2520personal%2520devices.%2520In%2520this%2520work%252C%2520we%2520take%2520a%2520step%2520toward%250Aprivacy-preserving%2520and%2520resource-efficient%2520agents%2520by%2520developing%2520a%2520lightweight%250Avision-language%2520model%2520that%2520runs%2520entirely%2520on%2520local%2520machines.%2520To%2520train%2520this%250Acompact%2520agent%252C%2520we%2520introduce%2520an%2520LLM-as-Judge%2520framework%2520that%2520automatically%250Aevaluates%2520and%2520filters%2520synthetic%2520interaction%2520trajectories%252C%2520producing%250Ahigh-quality%2520data%2520for%2520reinforcement%2520learning%2520without%2520human%2520annotation.%250AExperiments%2520on%2520the%2520OS-World%2520benchmark%2520demonstrate%2520that%2520our%2520fine-tuned%2520local%250Amodel%2520outperforms%2520existing%2520baselines%252C%2520highlighting%2520a%2520promising%2520path%2520toward%250Aprivate%252C%2520efficient%252C%2520and%2520generalizable%2520GUI%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03095v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DPO%20Learning%20with%20LLMs-Judge%20Signal%20for%20Computer%20Use%20Agents&entry.906535625=Man%20Luo%20and%20David%20Cobbley%20and%20Xin%20Su%20and%20Shachar%20Rosenman%20and%20Vasudev%20Lal%20and%20Shao-Yen%20Tseng%20and%20Phillip%20Howard&entry.1292438233=%20%20Computer%20use%20agents%20%28CUA%29%20are%20systems%20that%20automatically%20interact%20with%0Agraphical%20user%20interfaces%20%28GUIs%29%20to%20complete%20tasks.%20CUA%20have%20made%20significant%0Aprogress%20with%20the%20advent%20of%20large%20vision-language%20models%20%28VLMs%29.%20However%2C%20these%0Aagents%20typically%20rely%20on%20cloud-based%20inference%20with%20substantial%20compute%0Ademands%2C%20raising%20critical%20privacy%20and%20scalability%20concerns%2C%20especially%20when%0Aoperating%20on%20personal%20devices.%20In%20this%20work%2C%20we%20take%20a%20step%20toward%0Aprivacy-preserving%20and%20resource-efficient%20agents%20by%20developing%20a%20lightweight%0Avision-language%20model%20that%20runs%20entirely%20on%20local%20machines.%20To%20train%20this%0Acompact%20agent%2C%20we%20introduce%20an%20LLM-as-Judge%20framework%20that%20automatically%0Aevaluates%20and%20filters%20synthetic%20interaction%20trajectories%2C%20producing%0Ahigh-quality%20data%20for%20reinforcement%20learning%20without%20human%20annotation.%0AExperiments%20on%20the%20OS-World%20benchmark%20demonstrate%20that%20our%20fine-tuned%20local%0Amodel%20outperforms%20existing%20baselines%2C%20highlighting%20a%20promising%20path%20toward%0Aprivate%2C%20efficient%2C%20and%20generalizable%20GUI%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03095v1&entry.124074799=Read"},
{"title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback", "author": "Xiaoying Zhang and Hao Sun and Yipeng Zhang and Kaituo Feng and Chao Yang and Helen Meng", "abstract": "  Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of self-reflection, and persistent\nfailures. We then demonstrate that RL-finetuned models, even after exhibiting\nperformance plateaus, can generate correct refinements on persistently failed\nproblems by leveraging natural language feedback in the form of critiques.\nBuilding on this insight, we propose Critique-GRPO, an online RL framework that\nintegrates both natural language and numerical feedback for effective policy\noptimization. Critique-GRPO enables LLMs to learn from initial responses and\ncritique-guided refinements simultaneously while maintaining exploration.\nExtensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that\nCritique-GRPO consistently outperforms supervised learning-based and RL-based\nfine-tuning approaches across eight challenging mathematical, STEM, and general\nreasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,\nrespectively. Notably, Critique-GRPO surpasses a strong baseline that\nincorporates expert demonstrations within online RL. Further analysis reveals\ntwo critical insights about policy exploration: (1) higher entropy does not\nalways guarantee efficient learning from exploration, and (2) longer responses\ndo not necessarily lead to more effective exploration.\n", "link": "http://arxiv.org/abs/2506.03106v1", "date": "2025-06-03", "relevancy": 1.9586, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5144}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4847}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Critique-GRPO%3A%20Advancing%20LLM%20Reasoning%20with%20Natural%20Language%20and%0A%20%20Numerical%20Feedback&body=Title%3A%20Critique-GRPO%3A%20Advancing%20LLM%20Reasoning%20with%20Natural%20Language%20and%0A%20%20Numerical%20Feedback%0AAuthor%3A%20Xiaoying%20Zhang%20and%20Hao%20Sun%20and%20Yipeng%20Zhang%20and%20Kaituo%20Feng%20and%20Chao%20Yang%20and%20Helen%20Meng%0AAbstract%3A%20%20%20Recent%20advances%20in%20reinforcement%20learning%20%28RL%29%20with%20numerical%20feedback%2C%20such%0Aas%20scalar%20rewards%2C%20have%20significantly%20enhanced%20the%20complex%20reasoning%0Acapabilities%20of%20large%20language%20models%20%28LLMs%29.%20Despite%20this%20success%2C%20we%20identify%0Athree%20key%20challenges%20encountered%20by%20RL%20with%20solely%20numerical%20feedback%3A%0Aperformance%20plateaus%2C%20limited%20effectiveness%20of%20self-reflection%2C%20and%20persistent%0Afailures.%20We%20then%20demonstrate%20that%20RL-finetuned%20models%2C%20even%20after%20exhibiting%0Aperformance%20plateaus%2C%20can%20generate%20correct%20refinements%20on%20persistently%20failed%0Aproblems%20by%20leveraging%20natural%20language%20feedback%20in%20the%20form%20of%20critiques.%0ABuilding%20on%20this%20insight%2C%20we%20propose%20Critique-GRPO%2C%20an%20online%20RL%20framework%20that%0Aintegrates%20both%20natural%20language%20and%20numerical%20feedback%20for%20effective%20policy%0Aoptimization.%20Critique-GRPO%20enables%20LLMs%20to%20learn%20from%20initial%20responses%20and%0Acritique-guided%20refinements%20simultaneously%20while%20maintaining%20exploration.%0AExtensive%20experiments%20using%20Qwen2.5-7B-Base%20and%20Qwen3-8B-Base%20show%20that%0ACritique-GRPO%20consistently%20outperforms%20supervised%20learning-based%20and%20RL-based%0Afine-tuning%20approaches%20across%20eight%20challenging%20mathematical%2C%20STEM%2C%20and%20general%0Areasoning%20tasks%2C%20improving%20average%20pass%401%20scores%20by%20approximately%204.5%25%20and%205%25%2C%0Arespectively.%20Notably%2C%20Critique-GRPO%20surpasses%20a%20strong%20baseline%20that%0Aincorporates%20expert%20demonstrations%20within%20online%20RL.%20Further%20analysis%20reveals%0Atwo%20critical%20insights%20about%20policy%20exploration%3A%20%281%29%20higher%20entropy%20does%20not%0Aalways%20guarantee%20efficient%20learning%20from%20exploration%2C%20and%20%282%29%20longer%20responses%0Ado%20not%20necessarily%20lead%20to%20more%20effective%20exploration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.03106v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCritique-GRPO%253A%2520Advancing%2520LLM%2520Reasoning%2520with%2520Natural%2520Language%2520and%250A%2520%2520Numerical%2520Feedback%26entry.906535625%3DXiaoying%2520Zhang%2520and%2520Hao%2520Sun%2520and%2520Yipeng%2520Zhang%2520and%2520Kaituo%2520Feng%2520and%2520Chao%2520Yang%2520and%2520Helen%2520Meng%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520reinforcement%2520learning%2520%2528RL%2529%2520with%2520numerical%2520feedback%252C%2520such%250Aas%2520scalar%2520rewards%252C%2520have%2520significantly%2520enhanced%2520the%2520complex%2520reasoning%250Acapabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Despite%2520this%2520success%252C%2520we%2520identify%250Athree%2520key%2520challenges%2520encountered%2520by%2520RL%2520with%2520solely%2520numerical%2520feedback%253A%250Aperformance%2520plateaus%252C%2520limited%2520effectiveness%2520of%2520self-reflection%252C%2520and%2520persistent%250Afailures.%2520We%2520then%2520demonstrate%2520that%2520RL-finetuned%2520models%252C%2520even%2520after%2520exhibiting%250Aperformance%2520plateaus%252C%2520can%2520generate%2520correct%2520refinements%2520on%2520persistently%2520failed%250Aproblems%2520by%2520leveraging%2520natural%2520language%2520feedback%2520in%2520the%2520form%2520of%2520critiques.%250ABuilding%2520on%2520this%2520insight%252C%2520we%2520propose%2520Critique-GRPO%252C%2520an%2520online%2520RL%2520framework%2520that%250Aintegrates%2520both%2520natural%2520language%2520and%2520numerical%2520feedback%2520for%2520effective%2520policy%250Aoptimization.%2520Critique-GRPO%2520enables%2520LLMs%2520to%2520learn%2520from%2520initial%2520responses%2520and%250Acritique-guided%2520refinements%2520simultaneously%2520while%2520maintaining%2520exploration.%250AExtensive%2520experiments%2520using%2520Qwen2.5-7B-Base%2520and%2520Qwen3-8B-Base%2520show%2520that%250ACritique-GRPO%2520consistently%2520outperforms%2520supervised%2520learning-based%2520and%2520RL-based%250Afine-tuning%2520approaches%2520across%2520eight%2520challenging%2520mathematical%252C%2520STEM%252C%2520and%2520general%250Areasoning%2520tasks%252C%2520improving%2520average%2520pass%25401%2520scores%2520by%2520approximately%25204.5%2525%2520and%25205%2525%252C%250Arespectively.%2520Notably%252C%2520Critique-GRPO%2520surpasses%2520a%2520strong%2520baseline%2520that%250Aincorporates%2520expert%2520demonstrations%2520within%2520online%2520RL.%2520Further%2520analysis%2520reveals%250Atwo%2520critical%2520insights%2520about%2520policy%2520exploration%253A%2520%25281%2529%2520higher%2520entropy%2520does%2520not%250Aalways%2520guarantee%2520efficient%2520learning%2520from%2520exploration%252C%2520and%2520%25282%2529%2520longer%2520responses%250Ado%2520not%2520necessarily%2520lead%2520to%2520more%2520effective%2520exploration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.03106v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Critique-GRPO%3A%20Advancing%20LLM%20Reasoning%20with%20Natural%20Language%20and%0A%20%20Numerical%20Feedback&entry.906535625=Xiaoying%20Zhang%20and%20Hao%20Sun%20and%20Yipeng%20Zhang%20and%20Kaituo%20Feng%20and%20Chao%20Yang%20and%20Helen%20Meng&entry.1292438233=%20%20Recent%20advances%20in%20reinforcement%20learning%20%28RL%29%20with%20numerical%20feedback%2C%20such%0Aas%20scalar%20rewards%2C%20have%20significantly%20enhanced%20the%20complex%20reasoning%0Acapabilities%20of%20large%20language%20models%20%28LLMs%29.%20Despite%20this%20success%2C%20we%20identify%0Athree%20key%20challenges%20encountered%20by%20RL%20with%20solely%20numerical%20feedback%3A%0Aperformance%20plateaus%2C%20limited%20effectiveness%20of%20self-reflection%2C%20and%20persistent%0Afailures.%20We%20then%20demonstrate%20that%20RL-finetuned%20models%2C%20even%20after%20exhibiting%0Aperformance%20plateaus%2C%20can%20generate%20correct%20refinements%20on%20persistently%20failed%0Aproblems%20by%20leveraging%20natural%20language%20feedback%20in%20the%20form%20of%20critiques.%0ABuilding%20on%20this%20insight%2C%20we%20propose%20Critique-GRPO%2C%20an%20online%20RL%20framework%20that%0Aintegrates%20both%20natural%20language%20and%20numerical%20feedback%20for%20effective%20policy%0Aoptimization.%20Critique-GRPO%20enables%20LLMs%20to%20learn%20from%20initial%20responses%20and%0Acritique-guided%20refinements%20simultaneously%20while%20maintaining%20exploration.%0AExtensive%20experiments%20using%20Qwen2.5-7B-Base%20and%20Qwen3-8B-Base%20show%20that%0ACritique-GRPO%20consistently%20outperforms%20supervised%20learning-based%20and%20RL-based%0Afine-tuning%20approaches%20across%20eight%20challenging%20mathematical%2C%20STEM%2C%20and%20general%0Areasoning%20tasks%2C%20improving%20average%20pass%401%20scores%20by%20approximately%204.5%25%20and%205%25%2C%0Arespectively.%20Notably%2C%20Critique-GRPO%20surpasses%20a%20strong%20baseline%20that%0Aincorporates%20expert%20demonstrations%20within%20online%20RL.%20Further%20analysis%20reveals%0Atwo%20critical%20insights%20about%20policy%20exploration%3A%20%281%29%20higher%20entropy%20does%20not%0Aalways%20guarantee%20efficient%20learning%20from%20exploration%2C%20and%20%282%29%20longer%20responses%0Ado%20not%20necessarily%20lead%20to%20more%20effective%20exploration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.03106v1&entry.124074799=Read"},
{"title": "Towards Efficient Online Tuning of VLM Agents via Counterfactual Soft\n  Reinforcement Learning", "author": "Lang Feng and Weihao Tan and Zhiyi Lyu and Longtao Zheng and Haiyang Xu and Ming Yan and Fei Huang and Bo An", "abstract": "  Online fine-tuning vision-language model (VLM) agents with reinforcement\nlearning (RL) has shown promise for equipping agents with multi-step,\ngoal-oriented capabilities in dynamic environments. However, their open-ended\ntextual action space and non-end-to-end nature of action generation present\nsignificant challenges to effective online exploration in RL, e.g., explosion\nof the exploration space. We propose a novel online fine-tuning method,\nCounterfactual Soft Reinforcement Learning (CoSo), better suited to the textual\noutput space of VLM agents. Compared to prior methods that assign uniform\nuncertainty to all tokens, CoSo leverages counterfactual reasoning to\ndynamically assess the causal influence of individual tokens on post-processed\nactions. By prioritizing the exploration of action-critical tokens while\nreducing the impact of semantically redundant or low-impact tokens, CoSo\nenables a more targeted and efficient online rollout process. We provide\ntheoretical analysis proving CoSo's convergence and policy improvement\nguarantees, and extensive empirical evaluations supporting CoSo's\neffectiveness. Our results across a diverse set of agent tasks, including\nAndroid device control, card gaming, and embodied AI, highlight its remarkable\nability to enhance exploration efficiency and deliver consistent performance\ngains. The code is available at https://github.com/langfengQ/CoSo.\n", "link": "http://arxiv.org/abs/2505.03792v2", "date": "2025-06-03", "relevancy": 1.6402, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.559}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5345}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Efficient%20Online%20Tuning%20of%20VLM%20Agents%20via%20Counterfactual%20Soft%0A%20%20Reinforcement%20Learning&body=Title%3A%20Towards%20Efficient%20Online%20Tuning%20of%20VLM%20Agents%20via%20Counterfactual%20Soft%0A%20%20Reinforcement%20Learning%0AAuthor%3A%20Lang%20Feng%20and%20Weihao%20Tan%20and%20Zhiyi%20Lyu%20and%20Longtao%20Zheng%20and%20Haiyang%20Xu%20and%20Ming%20Yan%20and%20Fei%20Huang%20and%20Bo%20An%0AAbstract%3A%20%20%20Online%20fine-tuning%20vision-language%20model%20%28VLM%29%20agents%20with%20reinforcement%0Alearning%20%28RL%29%20has%20shown%20promise%20for%20equipping%20agents%20with%20multi-step%2C%0Agoal-oriented%20capabilities%20in%20dynamic%20environments.%20However%2C%20their%20open-ended%0Atextual%20action%20space%20and%20non-end-to-end%20nature%20of%20action%20generation%20present%0Asignificant%20challenges%20to%20effective%20online%20exploration%20in%20RL%2C%20e.g.%2C%20explosion%0Aof%20the%20exploration%20space.%20We%20propose%20a%20novel%20online%20fine-tuning%20method%2C%0ACounterfactual%20Soft%20Reinforcement%20Learning%20%28CoSo%29%2C%20better%20suited%20to%20the%20textual%0Aoutput%20space%20of%20VLM%20agents.%20Compared%20to%20prior%20methods%20that%20assign%20uniform%0Auncertainty%20to%20all%20tokens%2C%20CoSo%20leverages%20counterfactual%20reasoning%20to%0Adynamically%20assess%20the%20causal%20influence%20of%20individual%20tokens%20on%20post-processed%0Aactions.%20By%20prioritizing%20the%20exploration%20of%20action-critical%20tokens%20while%0Areducing%20the%20impact%20of%20semantically%20redundant%20or%20low-impact%20tokens%2C%20CoSo%0Aenables%20a%20more%20targeted%20and%20efficient%20online%20rollout%20process.%20We%20provide%0Atheoretical%20analysis%20proving%20CoSo%27s%20convergence%20and%20policy%20improvement%0Aguarantees%2C%20and%20extensive%20empirical%20evaluations%20supporting%20CoSo%27s%0Aeffectiveness.%20Our%20results%20across%20a%20diverse%20set%20of%20agent%20tasks%2C%20including%0AAndroid%20device%20control%2C%20card%20gaming%2C%20and%20embodied%20AI%2C%20highlight%20its%20remarkable%0Aability%20to%20enhance%20exploration%20efficiency%20and%20deliver%20consistent%20performance%0Agains.%20The%20code%20is%20available%20at%20https%3A//github.com/langfengQ/CoSo.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.03792v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Efficient%2520Online%2520Tuning%2520of%2520VLM%2520Agents%2520via%2520Counterfactual%2520Soft%250A%2520%2520Reinforcement%2520Learning%26entry.906535625%3DLang%2520Feng%2520and%2520Weihao%2520Tan%2520and%2520Zhiyi%2520Lyu%2520and%2520Longtao%2520Zheng%2520and%2520Haiyang%2520Xu%2520and%2520Ming%2520Yan%2520and%2520Fei%2520Huang%2520and%2520Bo%2520An%26entry.1292438233%3D%2520%2520Online%2520fine-tuning%2520vision-language%2520model%2520%2528VLM%2529%2520agents%2520with%2520reinforcement%250Alearning%2520%2528RL%2529%2520has%2520shown%2520promise%2520for%2520equipping%2520agents%2520with%2520multi-step%252C%250Agoal-oriented%2520capabilities%2520in%2520dynamic%2520environments.%2520However%252C%2520their%2520open-ended%250Atextual%2520action%2520space%2520and%2520non-end-to-end%2520nature%2520of%2520action%2520generation%2520present%250Asignificant%2520challenges%2520to%2520effective%2520online%2520exploration%2520in%2520RL%252C%2520e.g.%252C%2520explosion%250Aof%2520the%2520exploration%2520space.%2520We%2520propose%2520a%2520novel%2520online%2520fine-tuning%2520method%252C%250ACounterfactual%2520Soft%2520Reinforcement%2520Learning%2520%2528CoSo%2529%252C%2520better%2520suited%2520to%2520the%2520textual%250Aoutput%2520space%2520of%2520VLM%2520agents.%2520Compared%2520to%2520prior%2520methods%2520that%2520assign%2520uniform%250Auncertainty%2520to%2520all%2520tokens%252C%2520CoSo%2520leverages%2520counterfactual%2520reasoning%2520to%250Adynamically%2520assess%2520the%2520causal%2520influence%2520of%2520individual%2520tokens%2520on%2520post-processed%250Aactions.%2520By%2520prioritizing%2520the%2520exploration%2520of%2520action-critical%2520tokens%2520while%250Areducing%2520the%2520impact%2520of%2520semantically%2520redundant%2520or%2520low-impact%2520tokens%252C%2520CoSo%250Aenables%2520a%2520more%2520targeted%2520and%2520efficient%2520online%2520rollout%2520process.%2520We%2520provide%250Atheoretical%2520analysis%2520proving%2520CoSo%2527s%2520convergence%2520and%2520policy%2520improvement%250Aguarantees%252C%2520and%2520extensive%2520empirical%2520evaluations%2520supporting%2520CoSo%2527s%250Aeffectiveness.%2520Our%2520results%2520across%2520a%2520diverse%2520set%2520of%2520agent%2520tasks%252C%2520including%250AAndroid%2520device%2520control%252C%2520card%2520gaming%252C%2520and%2520embodied%2520AI%252C%2520highlight%2520its%2520remarkable%250Aability%2520to%2520enhance%2520exploration%2520efficiency%2520and%2520deliver%2520consistent%2520performance%250Agains.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/langfengQ/CoSo.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.03792v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Efficient%20Online%20Tuning%20of%20VLM%20Agents%20via%20Counterfactual%20Soft%0A%20%20Reinforcement%20Learning&entry.906535625=Lang%20Feng%20and%20Weihao%20Tan%20and%20Zhiyi%20Lyu%20and%20Longtao%20Zheng%20and%20Haiyang%20Xu%20and%20Ming%20Yan%20and%20Fei%20Huang%20and%20Bo%20An&entry.1292438233=%20%20Online%20fine-tuning%20vision-language%20model%20%28VLM%29%20agents%20with%20reinforcement%0Alearning%20%28RL%29%20has%20shown%20promise%20for%20equipping%20agents%20with%20multi-step%2C%0Agoal-oriented%20capabilities%20in%20dynamic%20environments.%20However%2C%20their%20open-ended%0Atextual%20action%20space%20and%20non-end-to-end%20nature%20of%20action%20generation%20present%0Asignificant%20challenges%20to%20effective%20online%20exploration%20in%20RL%2C%20e.g.%2C%20explosion%0Aof%20the%20exploration%20space.%20We%20propose%20a%20novel%20online%20fine-tuning%20method%2C%0ACounterfactual%20Soft%20Reinforcement%20Learning%20%28CoSo%29%2C%20better%20suited%20to%20the%20textual%0Aoutput%20space%20of%20VLM%20agents.%20Compared%20to%20prior%20methods%20that%20assign%20uniform%0Auncertainty%20to%20all%20tokens%2C%20CoSo%20leverages%20counterfactual%20reasoning%20to%0Adynamically%20assess%20the%20causal%20influence%20of%20individual%20tokens%20on%20post-processed%0Aactions.%20By%20prioritizing%20the%20exploration%20of%20action-critical%20tokens%20while%0Areducing%20the%20impact%20of%20semantically%20redundant%20or%20low-impact%20tokens%2C%20CoSo%0Aenables%20a%20more%20targeted%20and%20efficient%20online%20rollout%20process.%20We%20provide%0Atheoretical%20analysis%20proving%20CoSo%27s%20convergence%20and%20policy%20improvement%0Aguarantees%2C%20and%20extensive%20empirical%20evaluations%20supporting%20CoSo%27s%0Aeffectiveness.%20Our%20results%20across%20a%20diverse%20set%20of%20agent%20tasks%2C%20including%0AAndroid%20device%20control%2C%20card%20gaming%2C%20and%20embodied%20AI%2C%20highlight%20its%20remarkable%0Aability%20to%20enhance%20exploration%20efficiency%20and%20deliver%20consistent%20performance%0Agains.%20The%20code%20is%20available%20at%20https%3A//github.com/langfengQ/CoSo.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.03792v2&entry.124074799=Read"},
{"title": "COMPKE: Complex Question Answering under Knowledge Editing", "author": "Keyuan Cheng and Zijian Kan and Zhixian He and Zhuoran Zhang and Muhammad Asif Ali and Ke Xu and Lijie Hu and Di Wang", "abstract": "  Knowledge Editing, which efficiently modifies the knowledge in large language\nmodels, has gathered great attention. Current benchmarks primarily use\nmulti-hop question answering to assess and analyze newly injected or updated\nknowledge. However, we argue that these benchmarks fail to effectively evaluate\nhow well the updated models apply this knowledge in real-life scenarios,\nparticularly when questions require complex reasoning, involving one-to-many\nrelationships or multi-step logical intersections. To fill in this gap, we\nintroduce a new benchmark, COMPKE: Complex Question Answering under Knowledge\nEditing, which includes 11,924 complex questions that reflect real-life\nsituations. We conduct an extensive evaluation of four knowledge editing\nmethods on COMPKE, revealing that their effectiveness varies notably across\ndifferent models. For instance, MeLLo attains an accuracy of 39.47 on\nGPT-4O-MINI, but this drops sharply to 3.83 on QWEN2.5-3B. We further\ninvestigate the underlying causes of these disparities from both methodological\nand model-specific perspectives. The datasets are available at\nhttps://github.com/kzjkzj666/CompKE.\n", "link": "http://arxiv.org/abs/2506.00829v2", "date": "2025-06-03", "relevancy": 2.0708, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5258}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5258}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20COMPKE%3A%20Complex%20Question%20Answering%20under%20Knowledge%20Editing&body=Title%3A%20COMPKE%3A%20Complex%20Question%20Answering%20under%20Knowledge%20Editing%0AAuthor%3A%20Keyuan%20Cheng%20and%20Zijian%20Kan%20and%20Zhixian%20He%20and%20Zhuoran%20Zhang%20and%20Muhammad%20Asif%20Ali%20and%20Ke%20Xu%20and%20Lijie%20Hu%20and%20Di%20Wang%0AAbstract%3A%20%20%20Knowledge%20Editing%2C%20which%20efficiently%20modifies%20the%20knowledge%20in%20large%20language%0Amodels%2C%20has%20gathered%20great%20attention.%20Current%20benchmarks%20primarily%20use%0Amulti-hop%20question%20answering%20to%20assess%20and%20analyze%20newly%20injected%20or%20updated%0Aknowledge.%20However%2C%20we%20argue%20that%20these%20benchmarks%20fail%20to%20effectively%20evaluate%0Ahow%20well%20the%20updated%20models%20apply%20this%20knowledge%20in%20real-life%20scenarios%2C%0Aparticularly%20when%20questions%20require%20complex%20reasoning%2C%20involving%20one-to-many%0Arelationships%20or%20multi-step%20logical%20intersections.%20To%20fill%20in%20this%20gap%2C%20we%0Aintroduce%20a%20new%20benchmark%2C%20COMPKE%3A%20Complex%20Question%20Answering%20under%20Knowledge%0AEditing%2C%20which%20includes%2011%2C924%20complex%20questions%20that%20reflect%20real-life%0Asituations.%20We%20conduct%20an%20extensive%20evaluation%20of%20four%20knowledge%20editing%0Amethods%20on%20COMPKE%2C%20revealing%20that%20their%20effectiveness%20varies%20notably%20across%0Adifferent%20models.%20For%20instance%2C%20MeLLo%20attains%20an%20accuracy%20of%2039.47%20on%0AGPT-4O-MINI%2C%20but%20this%20drops%20sharply%20to%203.83%20on%20QWEN2.5-3B.%20We%20further%0Ainvestigate%20the%20underlying%20causes%20of%20these%20disparities%20from%20both%20methodological%0Aand%20model-specific%20perspectives.%20The%20datasets%20are%20available%20at%0Ahttps%3A//github.com/kzjkzj666/CompKE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.00829v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCOMPKE%253A%2520Complex%2520Question%2520Answering%2520under%2520Knowledge%2520Editing%26entry.906535625%3DKeyuan%2520Cheng%2520and%2520Zijian%2520Kan%2520and%2520Zhixian%2520He%2520and%2520Zhuoran%2520Zhang%2520and%2520Muhammad%2520Asif%2520Ali%2520and%2520Ke%2520Xu%2520and%2520Lijie%2520Hu%2520and%2520Di%2520Wang%26entry.1292438233%3D%2520%2520Knowledge%2520Editing%252C%2520which%2520efficiently%2520modifies%2520the%2520knowledge%2520in%2520large%2520language%250Amodels%252C%2520has%2520gathered%2520great%2520attention.%2520Current%2520benchmarks%2520primarily%2520use%250Amulti-hop%2520question%2520answering%2520to%2520assess%2520and%2520analyze%2520newly%2520injected%2520or%2520updated%250Aknowledge.%2520However%252C%2520we%2520argue%2520that%2520these%2520benchmarks%2520fail%2520to%2520effectively%2520evaluate%250Ahow%2520well%2520the%2520updated%2520models%2520apply%2520this%2520knowledge%2520in%2520real-life%2520scenarios%252C%250Aparticularly%2520when%2520questions%2520require%2520complex%2520reasoning%252C%2520involving%2520one-to-many%250Arelationships%2520or%2520multi-step%2520logical%2520intersections.%2520To%2520fill%2520in%2520this%2520gap%252C%2520we%250Aintroduce%2520a%2520new%2520benchmark%252C%2520COMPKE%253A%2520Complex%2520Question%2520Answering%2520under%2520Knowledge%250AEditing%252C%2520which%2520includes%252011%252C924%2520complex%2520questions%2520that%2520reflect%2520real-life%250Asituations.%2520We%2520conduct%2520an%2520extensive%2520evaluation%2520of%2520four%2520knowledge%2520editing%250Amethods%2520on%2520COMPKE%252C%2520revealing%2520that%2520their%2520effectiveness%2520varies%2520notably%2520across%250Adifferent%2520models.%2520For%2520instance%252C%2520MeLLo%2520attains%2520an%2520accuracy%2520of%252039.47%2520on%250AGPT-4O-MINI%252C%2520but%2520this%2520drops%2520sharply%2520to%25203.83%2520on%2520QWEN2.5-3B.%2520We%2520further%250Ainvestigate%2520the%2520underlying%2520causes%2520of%2520these%2520disparities%2520from%2520both%2520methodological%250Aand%2520model-specific%2520perspectives.%2520The%2520datasets%2520are%2520available%2520at%250Ahttps%253A//github.com/kzjkzj666/CompKE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.00829v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=COMPKE%3A%20Complex%20Question%20Answering%20under%20Knowledge%20Editing&entry.906535625=Keyuan%20Cheng%20and%20Zijian%20Kan%20and%20Zhixian%20He%20and%20Zhuoran%20Zhang%20and%20Muhammad%20Asif%20Ali%20and%20Ke%20Xu%20and%20Lijie%20Hu%20and%20Di%20Wang&entry.1292438233=%20%20Knowledge%20Editing%2C%20which%20efficiently%20modifies%20the%20knowledge%20in%20large%20language%0Amodels%2C%20has%20gathered%20great%20attention.%20Current%20benchmarks%20primarily%20use%0Amulti-hop%20question%20answering%20to%20assess%20and%20analyze%20newly%20injected%20or%20updated%0Aknowledge.%20However%2C%20we%20argue%20that%20these%20benchmarks%20fail%20to%20effectively%20evaluate%0Ahow%20well%20the%20updated%20models%20apply%20this%20knowledge%20in%20real-life%20scenarios%2C%0Aparticularly%20when%20questions%20require%20complex%20reasoning%2C%20involving%20one-to-many%0Arelationships%20or%20multi-step%20logical%20intersections.%20To%20fill%20in%20this%20gap%2C%20we%0Aintroduce%20a%20new%20benchmark%2C%20COMPKE%3A%20Complex%20Question%20Answering%20under%20Knowledge%0AEditing%2C%20which%20includes%2011%2C924%20complex%20questions%20that%20reflect%20real-life%0Asituations.%20We%20conduct%20an%20extensive%20evaluation%20of%20four%20knowledge%20editing%0Amethods%20on%20COMPKE%2C%20revealing%20that%20their%20effectiveness%20varies%20notably%20across%0Adifferent%20models.%20For%20instance%2C%20MeLLo%20attains%20an%20accuracy%20of%2039.47%20on%0AGPT-4O-MINI%2C%20but%20this%20drops%20sharply%20to%203.83%20on%20QWEN2.5-3B.%20We%20further%0Ainvestigate%20the%20underlying%20causes%20of%20these%20disparities%20from%20both%20methodological%0Aand%20model-specific%20perspectives.%20The%20datasets%20are%20available%20at%0Ahttps%3A//github.com/kzjkzj666/CompKE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.00829v2&entry.124074799=Read"},
{"title": "Theoretical Performance Guarantees for Partial Domain Adaptation via\n  Partial Optimal Transport", "author": "Jayadev Naram and Fredrik Hellstr\u00f6m and Ziming Wang and Rebecka J\u00f6rnsten and Giuseppe Durisi", "abstract": "  In many scenarios of practical interest, labeled data from a target\ndistribution are scarce while labeled data from a related source distribution\nare abundant. One particular setting of interest arises when the target label\nspace is a subset of the source label space, leading to the framework of\npartial domain adaptation (PDA). Typical approaches to PDA involve minimizing a\ndomain alignment term and a weighted empirical loss on the source data, with\nthe aim of transferring knowledge between domains. However, a theoretical basis\nfor this procedure is lacking, and in particular, most existing weighting\nschemes are heuristic. In this work, we derive generalization bounds for the\nPDA problem based on partial optimal transport. These bounds corroborate the\nuse of the partial Wasserstein distance as a domain alignment term, and lead to\ntheoretically motivated explicit expressions for the empirical source loss\nweights. Inspired by these bounds, we devise a practical algorithm for PDA,\ntermed WARMPOT. Through extensive numerical experiments, we show that WARMPOT\nis competitive with recent approaches, and that our proposed weights improve on\nexisting schemes.\n", "link": "http://arxiv.org/abs/2506.02712v1", "date": "2025-06-03", "relevancy": 1.9725, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5035}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4888}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4779}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Theoretical%20Performance%20Guarantees%20for%20Partial%20Domain%20Adaptation%20via%0A%20%20Partial%20Optimal%20Transport&body=Title%3A%20Theoretical%20Performance%20Guarantees%20for%20Partial%20Domain%20Adaptation%20via%0A%20%20Partial%20Optimal%20Transport%0AAuthor%3A%20Jayadev%20Naram%20and%20Fredrik%20Hellstr%C3%B6m%20and%20Ziming%20Wang%20and%20Rebecka%20J%C3%B6rnsten%20and%20Giuseppe%20Durisi%0AAbstract%3A%20%20%20In%20many%20scenarios%20of%20practical%20interest%2C%20labeled%20data%20from%20a%20target%0Adistribution%20are%20scarce%20while%20labeled%20data%20from%20a%20related%20source%20distribution%0Aare%20abundant.%20One%20particular%20setting%20of%20interest%20arises%20when%20the%20target%20label%0Aspace%20is%20a%20subset%20of%20the%20source%20label%20space%2C%20leading%20to%20the%20framework%20of%0Apartial%20domain%20adaptation%20%28PDA%29.%20Typical%20approaches%20to%20PDA%20involve%20minimizing%20a%0Adomain%20alignment%20term%20and%20a%20weighted%20empirical%20loss%20on%20the%20source%20data%2C%20with%0Athe%20aim%20of%20transferring%20knowledge%20between%20domains.%20However%2C%20a%20theoretical%20basis%0Afor%20this%20procedure%20is%20lacking%2C%20and%20in%20particular%2C%20most%20existing%20weighting%0Aschemes%20are%20heuristic.%20In%20this%20work%2C%20we%20derive%20generalization%20bounds%20for%20the%0APDA%20problem%20based%20on%20partial%20optimal%20transport.%20These%20bounds%20corroborate%20the%0Ause%20of%20the%20partial%20Wasserstein%20distance%20as%20a%20domain%20alignment%20term%2C%20and%20lead%20to%0Atheoretically%20motivated%20explicit%20expressions%20for%20the%20empirical%20source%20loss%0Aweights.%20Inspired%20by%20these%20bounds%2C%20we%20devise%20a%20practical%20algorithm%20for%20PDA%2C%0Atermed%20WARMPOT.%20Through%20extensive%20numerical%20experiments%2C%20we%20show%20that%20WARMPOT%0Ais%20competitive%20with%20recent%20approaches%2C%20and%20that%20our%20proposed%20weights%20improve%20on%0Aexisting%20schemes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.02712v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTheoretical%2520Performance%2520Guarantees%2520for%2520Partial%2520Domain%2520Adaptation%2520via%250A%2520%2520Partial%2520Optimal%2520Transport%26entry.906535625%3DJayadev%2520Naram%2520and%2520Fredrik%2520Hellstr%25C3%25B6m%2520and%2520Ziming%2520Wang%2520and%2520Rebecka%2520J%25C3%25B6rnsten%2520and%2520Giuseppe%2520Durisi%26entry.1292438233%3D%2520%2520In%2520many%2520scenarios%2520of%2520practical%2520interest%252C%2520labeled%2520data%2520from%2520a%2520target%250Adistribution%2520are%2520scarce%2520while%2520labeled%2520data%2520from%2520a%2520related%2520source%2520distribution%250Aare%2520abundant.%2520One%2520particular%2520setting%2520of%2520interest%2520arises%2520when%2520the%2520target%2520label%250Aspace%2520is%2520a%2520subset%2520of%2520the%2520source%2520label%2520space%252C%2520leading%2520to%2520the%2520framework%2520of%250Apartial%2520domain%2520adaptation%2520%2528PDA%2529.%2520Typical%2520approaches%2520to%2520PDA%2520involve%2520minimizing%2520a%250Adomain%2520alignment%2520term%2520and%2520a%2520weighted%2520empirical%2520loss%2520on%2520the%2520source%2520data%252C%2520with%250Athe%2520aim%2520of%2520transferring%2520knowledge%2520between%2520domains.%2520However%252C%2520a%2520theoretical%2520basis%250Afor%2520this%2520procedure%2520is%2520lacking%252C%2520and%2520in%2520particular%252C%2520most%2520existing%2520weighting%250Aschemes%2520are%2520heuristic.%2520In%2520this%2520work%252C%2520we%2520derive%2520generalization%2520bounds%2520for%2520the%250APDA%2520problem%2520based%2520on%2520partial%2520optimal%2520transport.%2520These%2520bounds%2520corroborate%2520the%250Ause%2520of%2520the%2520partial%2520Wasserstein%2520distance%2520as%2520a%2520domain%2520alignment%2520term%252C%2520and%2520lead%2520to%250Atheoretically%2520motivated%2520explicit%2520expressions%2520for%2520the%2520empirical%2520source%2520loss%250Aweights.%2520Inspired%2520by%2520these%2520bounds%252C%2520we%2520devise%2520a%2520practical%2520algorithm%2520for%2520PDA%252C%250Atermed%2520WARMPOT.%2520Through%2520extensive%2520numerical%2520experiments%252C%2520we%2520show%2520that%2520WARMPOT%250Ais%2520competitive%2520with%2520recent%2520approaches%252C%2520and%2520that%2520our%2520proposed%2520weights%2520improve%2520on%250Aexisting%2520schemes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.02712v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Theoretical%20Performance%20Guarantees%20for%20Partial%20Domain%20Adaptation%20via%0A%20%20Partial%20Optimal%20Transport&entry.906535625=Jayadev%20Naram%20and%20Fredrik%20Hellstr%C3%B6m%20and%20Ziming%20Wang%20and%20Rebecka%20J%C3%B6rnsten%20and%20Giuseppe%20Durisi&entry.1292438233=%20%20In%20many%20scenarios%20of%20practical%20interest%2C%20labeled%20data%20from%20a%20target%0Adistribution%20are%20scarce%20while%20labeled%20data%20from%20a%20related%20source%20distribution%0Aare%20abundant.%20One%20particular%20setting%20of%20interest%20arises%20when%20the%20target%20label%0Aspace%20is%20a%20subset%20of%20the%20source%20label%20space%2C%20leading%20to%20the%20framework%20of%0Apartial%20domain%20adaptation%20%28PDA%29.%20Typical%20approaches%20to%20PDA%20involve%20minimizing%20a%0Adomain%20alignment%20term%20and%20a%20weighted%20empirical%20loss%20on%20the%20source%20data%2C%20with%0Athe%20aim%20of%20transferring%20knowledge%20between%20domains.%20However%2C%20a%20theoretical%20basis%0Afor%20this%20procedure%20is%20lacking%2C%20and%20in%20particular%2C%20most%20existing%20weighting%0Aschemes%20are%20heuristic.%20In%20this%20work%2C%20we%20derive%20generalization%20bounds%20for%20the%0APDA%20problem%20based%20on%20partial%20optimal%20transport.%20These%20bounds%20corroborate%20the%0Ause%20of%20the%20partial%20Wasserstein%20distance%20as%20a%20domain%20alignment%20term%2C%20and%20lead%20to%0Atheoretically%20motivated%20explicit%20expressions%20for%20the%20empirical%20source%20loss%0Aweights.%20Inspired%20by%20these%20bounds%2C%20we%20devise%20a%20practical%20algorithm%20for%20PDA%2C%0Atermed%20WARMPOT.%20Through%20extensive%20numerical%20experiments%2C%20we%20show%20that%20WARMPOT%0Ais%20competitive%20with%20recent%20approaches%2C%20and%20that%20our%20proposed%20weights%20improve%20on%0Aexisting%20schemes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.02712v1&entry.124074799=Read"},
{"title": "X-Driver: Explainable Autonomous Driving with Vision-Language Models", "author": "Wei Liu and Jiyuan Zhang and Binxiong Zheng and Yufeng Hu and Yingzhan Lin and Zengfeng Zeng", "abstract": "  End-to-end autonomous driving has advanced significantly, offering benefits\nsuch as system simplicity and stronger driving performance in both open-loop\nand closed-loop settings than conventional pipelines. However, existing\nframeworks still suffer from low success rates in closed-loop evaluations,\nhighlighting their limitations in real-world deployment. In this paper, we\nintroduce X-Driver, a unified multi-modal large language models(MLLMs)\nframework designed for closed-loop autonomous driving, leveraging\nChain-of-Thought(CoT) and autoregressive modeling to enhance perception and\ndecision-making. We validate X-Driver across multiple autonomous driving tasks\nusing public benchmarks in CARLA simulation environment, including\nBench2Drive[6]. Our experimental results demonstrate superior closed-loop\nperformance, surpassing the current state-of-the-art(SOTA) while improving the\ninterpretability of driving decisions. These findings underscore the importance\nof structured reasoning in end-to-end driving and establish X-Driver as a\nstrong baseline for future research in closed-loop autonomous driving.\n", "link": "http://arxiv.org/abs/2505.05098v2", "date": "2025-06-03", "relevancy": 2.3122, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5855}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5855}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20X-Driver%3A%20Explainable%20Autonomous%20Driving%20with%20Vision-Language%20Models&body=Title%3A%20X-Driver%3A%20Explainable%20Autonomous%20Driving%20with%20Vision-Language%20Models%0AAuthor%3A%20Wei%20Liu%20and%20Jiyuan%20Zhang%20and%20Binxiong%20Zheng%20and%20Yufeng%20Hu%20and%20Yingzhan%20Lin%20and%20Zengfeng%20Zeng%0AAbstract%3A%20%20%20End-to-end%20autonomous%20driving%20has%20advanced%20significantly%2C%20offering%20benefits%0Asuch%20as%20system%20simplicity%20and%20stronger%20driving%20performance%20in%20both%20open-loop%0Aand%20closed-loop%20settings%20than%20conventional%20pipelines.%20However%2C%20existing%0Aframeworks%20still%20suffer%20from%20low%20success%20rates%20in%20closed-loop%20evaluations%2C%0Ahighlighting%20their%20limitations%20in%20real-world%20deployment.%20In%20this%20paper%2C%20we%0Aintroduce%20X-Driver%2C%20a%20unified%20multi-modal%20large%20language%20models%28MLLMs%29%0Aframework%20designed%20for%20closed-loop%20autonomous%20driving%2C%20leveraging%0AChain-of-Thought%28CoT%29%20and%20autoregressive%20modeling%20to%20enhance%20perception%20and%0Adecision-making.%20We%20validate%20X-Driver%20across%20multiple%20autonomous%20driving%20tasks%0Ausing%20public%20benchmarks%20in%20CARLA%20simulation%20environment%2C%20including%0ABench2Drive%5B6%5D.%20Our%20experimental%20results%20demonstrate%20superior%20closed-loop%0Aperformance%2C%20surpassing%20the%20current%20state-of-the-art%28SOTA%29%20while%20improving%20the%0Ainterpretability%20of%20driving%20decisions.%20These%20findings%20underscore%20the%20importance%0Aof%20structured%20reasoning%20in%20end-to-end%20driving%20and%20establish%20X-Driver%20as%20a%0Astrong%20baseline%20for%20future%20research%20in%20closed-loop%20autonomous%20driving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.05098v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DX-Driver%253A%2520Explainable%2520Autonomous%2520Driving%2520with%2520Vision-Language%2520Models%26entry.906535625%3DWei%2520Liu%2520and%2520Jiyuan%2520Zhang%2520and%2520Binxiong%2520Zheng%2520and%2520Yufeng%2520Hu%2520and%2520Yingzhan%2520Lin%2520and%2520Zengfeng%2520Zeng%26entry.1292438233%3D%2520%2520End-to-end%2520autonomous%2520driving%2520has%2520advanced%2520significantly%252C%2520offering%2520benefits%250Asuch%2520as%2520system%2520simplicity%2520and%2520stronger%2520driving%2520performance%2520in%2520both%2520open-loop%250Aand%2520closed-loop%2520settings%2520than%2520conventional%2520pipelines.%2520However%252C%2520existing%250Aframeworks%2520still%2520suffer%2520from%2520low%2520success%2520rates%2520in%2520closed-loop%2520evaluations%252C%250Ahighlighting%2520their%2520limitations%2520in%2520real-world%2520deployment.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520X-Driver%252C%2520a%2520unified%2520multi-modal%2520large%2520language%2520models%2528MLLMs%2529%250Aframework%2520designed%2520for%2520closed-loop%2520autonomous%2520driving%252C%2520leveraging%250AChain-of-Thought%2528CoT%2529%2520and%2520autoregressive%2520modeling%2520to%2520enhance%2520perception%2520and%250Adecision-making.%2520We%2520validate%2520X-Driver%2520across%2520multiple%2520autonomous%2520driving%2520tasks%250Ausing%2520public%2520benchmarks%2520in%2520CARLA%2520simulation%2520environment%252C%2520including%250ABench2Drive%255B6%255D.%2520Our%2520experimental%2520results%2520demonstrate%2520superior%2520closed-loop%250Aperformance%252C%2520surpassing%2520the%2520current%2520state-of-the-art%2528SOTA%2529%2520while%2520improving%2520the%250Ainterpretability%2520of%2520driving%2520decisions.%2520These%2520findings%2520underscore%2520the%2520importance%250Aof%2520structured%2520reasoning%2520in%2520end-to-end%2520driving%2520and%2520establish%2520X-Driver%2520as%2520a%250Astrong%2520baseline%2520for%2520future%2520research%2520in%2520closed-loop%2520autonomous%2520driving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.05098v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=X-Driver%3A%20Explainable%20Autonomous%20Driving%20with%20Vision-Language%20Models&entry.906535625=Wei%20Liu%20and%20Jiyuan%20Zhang%20and%20Binxiong%20Zheng%20and%20Yufeng%20Hu%20and%20Yingzhan%20Lin%20and%20Zengfeng%20Zeng&entry.1292438233=%20%20End-to-end%20autonomous%20driving%20has%20advanced%20significantly%2C%20offering%20benefits%0Asuch%20as%20system%20simplicity%20and%20stronger%20driving%20performance%20in%20both%20open-loop%0Aand%20closed-loop%20settings%20than%20conventional%20pipelines.%20However%2C%20existing%0Aframeworks%20still%20suffer%20from%20low%20success%20rates%20in%20closed-loop%20evaluations%2C%0Ahighlighting%20their%20limitations%20in%20real-world%20deployment.%20In%20this%20paper%2C%20we%0Aintroduce%20X-Driver%2C%20a%20unified%20multi-modal%20large%20language%20models%28MLLMs%29%0Aframework%20designed%20for%20closed-loop%20autonomous%20driving%2C%20leveraging%0AChain-of-Thought%28CoT%29%20and%20autoregressive%20modeling%20to%20enhance%20perception%20and%0Adecision-making.%20We%20validate%20X-Driver%20across%20multiple%20autonomous%20driving%20tasks%0Ausing%20public%20benchmarks%20in%20CARLA%20simulation%20environment%2C%20including%0ABench2Drive%5B6%5D.%20Our%20experimental%20results%20demonstrate%20superior%20closed-loop%0Aperformance%2C%20surpassing%20the%20current%20state-of-the-art%28SOTA%29%20while%20improving%20the%0Ainterpretability%20of%20driving%20decisions.%20These%20findings%20underscore%20the%20importance%0Aof%20structured%20reasoning%20in%20end-to-end%20driving%20and%20establish%20X-Driver%20as%20a%0Astrong%20baseline%20for%20future%20research%20in%20closed-loop%20autonomous%20driving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.05098v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


